- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:06:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1906.06113] A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1906.06113] 关于基于深度学习的图像深度重建架构的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1906.06113](https://ar5iv.labs.arxiv.org/html/1906.06113)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1906.06113](https://ar5iv.labs.arxiv.org/html/1906.06113)
- en: A Survey on Deep Learning Architectures for Image-based Depth Reconstruction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于基于深度学习的图像深度重建架构的综述
- en: 'Hamid Laga    Hamid Laga Hamid Laga is with Murdoch University (Australia),
    and with the Phenomics and Bioinformatics Research Centre, University of South
    Australia. E-mail: H.Laga@murdoch.edu.au Manuscript received April 19, 2005; revised
    December 27, 2012.(September 2018)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 哈米德·拉加    哈米德·拉加 哈米德·拉加现任教于穆尔多克大学（澳大利亚），并且在南澳大学的表型学和生物信息学研究中心工作。电子邮件：H.Laga@murdoch.edu.au
    手稿接收日期：2005年4月19日；修订日期：2012年12月27日。（2018年9月）
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Estimating depth from RGB images is a long-standing ill-posed problem, which
    has been explored for decades by the computer vision, graphics, and machine learning
    communities. In this article, we provide a comprehension survey of the recent
    developments in this field. We will focus on the works which use deep learning
    techniques to estimate depth from one or multiple images. Deep learning, coupled
    with the availability of large training datasets, have revolutionized the way
    the depth reconstruction problem is being approached by the research community.
    In this article, we survey more than $100$ key contributions that appeared in
    the past five years, summarize the most commonly used pipelines, and discuss their
    benefits and limitations. In retrospect of what has been achieved so far, we also
    conjecture what the future may hold for learning-based depth reconstruction research.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从RGB图像中估计深度是一个长期存在的病态问题，计算机视觉、图形学和机器学习领域已对其进行了数十年的探索。在本文中，我们提供了对该领域最新发展的全面综述。我们将重点关注那些使用深度学习技术从一张或多张图像中估计深度的工作。深度学习以及大量训练数据集的可用性，已经彻底改变了研究社区处理深度重建问题的方式。在本文中，我们综述了过去五年出现的超过$100$项关键贡献，总结了最常用的流程，并讨论了它们的优点和局限性。回顾迄今为止取得的成就，我们还推测了未来基于学习的深度重建研究可能的发展方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Stereo matching, Disparity, CNN, Convolutional Neural Networks, 3D Video, 3D
    Reconstruction
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 立体匹配、视差、CNN、卷积神经网络、三维视频、三维重建
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The goal of image-based 3D reconstruction is to infer the 3D geometry and structure
    of real objects and scenes from one or multiple RGB images. This long standing
    ill-posed problem is fundamental to many applications such as robot navigation,
    object recognition and scene understanding, 3D modeling and animation, industrial
    control, and medical diagnosis.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图像的三维重建的目标是从一张或多张RGB图像中推断真实物体和场景的三维几何形状和结构。这个长期存在的病态问题对许多应用至关重要，如机器人导航、物体识别和场景理解、三维建模和动画、工业控制以及医学诊断。
- en: Recovering the lost dimension from 2D images has been the goal of multiview
    stereo and Structure-from-X methods, which have been extensively investigated
    for many decades. The first generation of methods has focused on understanding
    and formalizing the 3D to 2D projection process, with the aim to devise solutions
    to the ill-posed inverse problem. Effective solutions typically require multiple
    images, captured using accurately calibrated cameras. Although these techniques
    can achieve remarkable results, they are still limited in many aspects. For instance,
    they are not suitable when dealing with occlusions, featureless regions, or highly
    textured regions with repetitive features.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从二维图像中恢复丢失的维度一直是多视角立体和结构自X方法的目标，这些方法已被广泛研究了几十年。第一代方法专注于理解和形式化三维到二维投影过程，旨在为这个病态的逆问题设计解决方案。有效的解决方案通常需要多张图像，这些图像使用经过准确标定的相机拍摄。尽管这些技术可以取得显著成果，但在许多方面仍然有限。例如，它们在处理遮挡、特征较少的区域或具有重复特征的高纹理区域时并不适用。
- en: Interestingly, we, as humans, are good at solving such ill-posed inverse problems
    by leveraging prior knowledge. For example, we can easily infer the approximate
    size and rough geometry of objects using only one eye. We can even guess what
    it would look like from another view. We can do this because all the previously
    seen objects and scenes have enabled us to build a prior knowledge and develop
    mental models of what objects, and the 3D world in general, look like. The second
    generation of depth reconstruction methods try to leverage this prior knowledge
    by formulating the problem as a recognition task. The avenue of deep learning
    techniques, and more importantly, the increasing availability of large training
    data sets, have lead to a new generation of methods that are able to recover the
    lost dimension even from a single image. Despite being recent, these methods have
    demonstrated exciting and promising results on various tasks related to computer
    vision and graphics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，作为人类，我们善于利用先验知识解决这些不适定的逆问题。例如，我们仅用一只眼睛就能轻松推测出物体的大致大小和粗略几何形状。我们甚至可以猜测从另一个视角看起来会是什么样子。我们之所以能够做到这一点，是因为之前看到的所有物体和场景使我们能够建立先验知识，并发展出关于物体及整个3D世界的心理模型。第二代深度重建方法试图通过将问题形式化为识别任务来利用这些先验知识。深度学习技术的崛起，以及更重要的是大规模训练数据集的日益增加，催生了一代新方法，这些方法即使从单张图像中也能恢复丢失的维度。尽管这些方法较为新颖，但在计算机视觉和图形学相关的各种任务中已经展现出了令人兴奋和有前景的结果。
- en: In this article, we provide a comprehensive and structured review of the recent
    advances in image-based depth reconstruction using deep learning techniques. We
    have gathered more than $100$ papers, which appeared from $2014$ to December $2018$
    in leading computer vision, computer graphics, and machine learning conferences
    and journals dealing specifically with this problem¹¹1This number is continuously
    increasing even at the time we are writing this article and during the review
    process.. The goal is to help the reader navigate in this emerging field, which
    gained a significant momentum in the past few years. Compared to the existing
    literature, the main contributions of this article are as follows;
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了关于使用深度学习技术进行基于图像的深度重建的最新进展的全面且结构化的综述。我们收集了超过$100$篇论文，这些论文发表于$2014$年至2018年12月期间的领先计算机视觉、计算机图形学和机器学习会议及期刊，专门讨论这一问题¹¹1这个数字在我们撰写本文和审稿过程中还在不断增加..
    目标是帮助读者在这个新兴领域中导航，该领域在过去几年中获得了显著的动能。与现有文献相比，本文的主要贡献如下；
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first survey paper in the literature
    which focuses on image-based depth reconstruction using deep learning techniques.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是文献中首篇关注于使用深度学习技术进行基于图像的深度重建的综述论文。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We adequately cover the contemporary literature with respect to this area. We
    present a comprehensive review of more than $100$ articles, which appeared from
    $2014$ to December $2018$.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们充分覆盖了这一领域的现代文献。我们对超过$100$篇从$2014$年至2018年12月发表的文章进行了全面综述。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This article also provides a comprehensive review and an insightful analysis
    on all aspects of depth reconstruction using deep learning, including the training
    data, the choice of network architectures and their effect on the reconstruction
    results, the training strategies, and the application scenarios.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文还提供了关于深度学习进行深度重建的所有方面的全面综述和深刻分析，包括训练数据、网络架构选择及其对重建结果的影响、训练策略以及应用场景。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comparative summary of the properties and performances of the reviewed
    methods for different scenarios including depth reconstruction from stereo pairs,
    depth reconstruction from multiple images, and depth reconstruction from a single
    RGB image.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了所评审方法在不同场景下的属性和性能的比较总结，包括从立体对、多个图像和单张RGB图像进行深度重建。
- en: The rest of this article is organized as follows; Section [2](#S2 "2 Scope and
    taxonomy ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    fomulates the problem and lays down the taxonomy. Section [3](#S3 "3 Depth by
    stereo matching ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction") focuses on the recent papers that use deep learning architectures
    for stereo matching. Section [4](#S4 "4 Depth estimation by regression ‣ A Survey
    on Deep Learning Architectures for Image-based Depth Reconstruction") reviews
    the methods that directly regress depth maps from one or multiple images without
    explicitly matching features across the input images. Section [5](#S5 "5 Training
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    focuses on the training procedures including the choice of training datasets and
    loss functions. Section [6](#S6 "6 Discussion and comparison ‣ A Survey on Deep
    Learning Architectures for Image-based Depth Reconstruction") discuss the performance
    of some key methods. Finally, Sections [7](#S7 "7 Future research directions ‣
    A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    and [8](#S8 "8 Conclusion ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction") discuss potential future research directions, and summarize
    the paper.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：第 [2](#S2 "2 Scope and taxonomy ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") 节提出问题并制定分类法。第 [3](#S3 "3 Depth by stereo
    matching ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    节重点介绍了使用深度学习架构进行立体匹配的最新论文。第 [4](#S4 "4 Depth estimation by regression ‣ A Survey
    on Deep Learning Architectures for Image-based Depth Reconstruction") 节回顾了从一个或多个图像中直接回归深度图而不显式匹配输入图像特征的方法。第
    [5](#S5 "5 Training ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction") 节重点介绍了训练程序，包括训练数据集的选择和损失函数。第 [6](#S6 "6 Discussion and
    comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    节讨论了一些关键方法的性能。最后，第 [7](#S7 "7 Future research directions ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction") 和第 [8](#S8 "8 Conclusion
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    节讨论了潜在的未来研究方向，并总结了本文。
- en: 2 Scope and taxonomy
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 领域和分类
- en: Let $\textbf{I}=\{I_{k},k=1,\dots,n\}$ be a set of $n\geq 1$ RGB images of the
    same 3D scene, captured using cameras whose intrinsic and extrinsic parameters
    can be *known* or *unknown*. The images can be captured by multiple cameras placed
    around the 3D scene, and thus they are spatially correlated, or with a single
    camera moving around the scene producing images that are temporally correlated.
    The goal is to estimate one or multiple depth maps, which can be from the same
    viewpoint as the input [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)],
    or from a new arbitrary viewpoint [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)]. In this article focuses on methods that estimate
    depth from one or multiple images with known or unknown camera parameters. Structure-from-Motion
    (SfM) and Simultaneous Localization and Mapping (SLAM) techniques, which estimate
    at the same time depth and (relative) camera pose from multiple images or a video
    stream, are beyond the scope of this article and require a separate survey.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\textbf{I}=\{I_{k},k=1,\dots,n\}$ 为一组 $n\geq 1$ 张相同 3D 场景的 RGB 图像，这些图像使用的相机的内部和外部参数可以是*已知*的，也可以是*未知*的。图像可以由多个相机在
    3D 场景周围拍摄，从而空间上相关，或者由单个相机在场景中移动拍摄，从而在时间上相关。目标是估计一个或多个深度图，这些深度图可以与输入图像具有相同的视角[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)]，也可以是新的任意视角[[5](#bib.bib5), [6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]。本文重点讨论从一个或多个图像中估计深度的方法，这些图像的相机参数可能是已知的，也可能是未知的。结构光（SfM）和同步定位与地图构建（SLAM）技术，它们同时估计深度和（相对）相机位姿，超出了本文的范围，需要单独的调查。
- en: Learning-based depth reconstruction can be summarized as the process of learning
    a predictor $f_{\theta}$ that can infer a depth map $\hat{D}$ that is as close
    as possible to the unknown depth map $D$. In other words, we seek to find a function
    $f_{\theta}$ such that $\mathcal{L}(\textbf{I})=d\left(f_{\theta}(\textbf{I}),D\right)$
    is minimized. Here, $\theta$ is a set of parameters, and $d(\cdot,\cdot)$ is a
    certain measure of distance between the real depth map $D$ and the reconstructed
    depth map $f_{\theta}(\textbf{I})$. The reconstruction objective $\mathcal{L}$
    is also known as the *loss function* in the deep learning jargon.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的深度重建可以总结为学习一个预测器 $f_{\theta}$ 的过程，该预测器可以推断出一个尽可能接近未知深度图 $D$ 的深度图 $\hat{D}$。换句话说，我们寻求找到一个函数
    $f_{\theta}$，使得 $\mathcal{L}(\textbf{I})=d\left(f_{\theta}(\textbf{I}),D\right)$
    最小化。在这里，$\theta$ 是一组参数，而 $d(\cdot,\cdot)$ 是真实深度图 $D$ 和重建深度图 $f_{\theta}(\textbf{I})$
    之间的某种距离度量。重建目标 $\mathcal{L}$ 在深度学习术语中也被称为 *损失函数*。
- en: 'We can distinguish two main categories of methods. Methods in the first class
    (Section [3](#S3 "3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")) mimic the traditional stereo-matching
    techniques by explicitly learning how to match, or put in correspondence, pixels
    across the input images. Such correspondences can then be converted into an optical
    flow or a disparity map, which in turn can be converted into depth at each pixel
    in the input image. The predictor $f$ is composed of three modules: a feature
    extraction module, a feature matching and cost aggregation module, and a disparity/depth
    estimation module.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以区分两种主要的类别。第一类方法（第[3节](#S3 "3 Depth by stereo matching ‣ A Survey on Deep
    Learning Architectures for Image-based Depth Reconstruction")）通过明确学习如何匹配或对应输入图像中的像素，模仿传统的立体匹配技术。这些对应关系随后可以转换为光流或视差图，进而可以转换为输入图像中每个像素的深度。预测器
    $f$ 由三个模块组成：特征提取模块、特征匹配和成本聚合模块，以及视差/深度估计模块。
- en: The second class of methods (Section [4](#S4 "4 Depth estimation by regression
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction"))
    do not explicitly learn the matching function. Instead, they learn a function
    that directly predicts depth (or disparity) at each pixel in the input image(s).
    These methods are very general and have been used to estimate depth from a single
    image as well as from multiple images taken from arbitrary view points. The predicted
    depth map $D$ can be from the same viewpoint as the input, or from a new arbitrary
    viewpoint $v$. We refer to these methods as *regression-based* depth estimation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类方法（第[4节](#S4 "4 Depth estimation by regression ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")）并不明确学习匹配函数。相反，它们学习一个函数，该函数直接预测输入图像中每个像素的深度（或视差）。这些方法非常通用，已被用来从单张图像以及从多个从任意视点拍摄的图像中估计深度。预测的深度图
    $D$ 可以来自与输入相同的视点，也可以来自新的任意视点 $v$。我们将这些方法称为 *基于回归* 的深度估计。
- en: In all methods, the estimated depth maps can be further refined using refinement
    modules [[10](#bib.bib10), [1](#bib.bib1), [11](#bib.bib11), [2](#bib.bib2)] and/or
    progressive reconstruction strategies where the reconstruction is refined every
    time new images become available (Section [3.1.4](#S3.SS1.SSS4 "3.1.4 Refinement
    ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有方法中，估计的深度图可以进一步通过细化模块[[10](#bib.bib10), [1](#bib.bib1), [11](#bib.bib11),
    [2](#bib.bib2)]和/或渐进重建策略进行精细化，每当新图像可用时，重建过程会进行精细化（第[3.1.4节](#S3.SS1.SSS4 "3.1.4
    Refinement ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep
    Learning Architectures for Image-based Depth Reconstruction")）。
- en: The subsequent sections will review the state-of-the-art techniques. Within
    each class of methods, we will first review how the different modules within the
    common pipeline have been implemented using deep learning techniques. We will
    then discuss the different methods based on their input and output, the network
    architecture, the training procedures including the loss functions they use and
    the degree of supervision they require, and their performances on standard benchmarks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 后续章节将回顾最先进的技术。在每种方法类别中，我们将首先回顾在共同管道中的不同模块是如何使用深度学习技术实现的。然后，我们将讨论不同的方法，基于它们的输入和输出、网络架构、训练过程（包括所使用的损失函数和所需的监督程度）以及它们在标准基准上的表现。
- en: 3 Depth by stereo matching
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 通过立体匹配进行深度估计
- en: Stereo-based depth reconstruction methods take $n>1$ RGB images and produce
    a depth map, a disparity map, or an optical flow [[12](#bib.bib12), [13](#bib.bib13)],
    by matching features across the images. The input images may be captured with
    calibrated [[14](#bib.bib14)] or uncalibrated [[15](#bib.bib15)] cameras.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于立体的深度重建方法采用 $n>1$ 张 RGB 图像，生成深度图、视差图或光流[[12](#bib.bib12), [13](#bib.bib13)]，通过匹配图像中的特征。这些输入图像可以通过校准[[14](#bib.bib14)]
    或未校准[[15](#bib.bib15)] 相机拍摄。
- en: This section focuses on deep learning-based methods that mimic the traditional
    stereo-matching pipeline, *i.e.,* methods that learn how to explicitly match patches
    across stereo images for disparity/depth map estimation. We will first review
    how individual blocks of the stereo-matching pipeline have been implemented using
    deep learning (Section [3.1](#S3.SS1 "3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")),
    and then discuss how these blocks are put together and trained for depth reconstruction
    (Section [3.2](#S3.SS2 "3.2 Stereo matching networks ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节关注基于深度学习的方法，这些方法模仿传统的立体匹配管道，*即*，那些学习如何显式地匹配立体图像中的块以进行视差/深度图估计的方法。我们将首先回顾如何利用深度学习实现立体匹配管道的各个模块（第[3.1节](#S3.SS1
    "3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")），然后讨论这些模块如何组合在一起并训练以进行深度重建（第[3.2节](#S3.SS2
    "3.2 Stereo matching networks ‣ 3 Depth by stereo matching ‣ A Survey on Deep
    Learning Architectures for Image-based Depth Reconstruction")）。
- en: 3.1 The pipeline
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 管道
- en: 'The stereo-based depth reconstruction process can be formulated as the problem
    of estimating a map $D$ ($D$ can be a depth/disparity map, or an optical flow)
    which minimizes an energy function of the form:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于立体的深度重建过程可以被公式化为估计一个地图 $D$（$D$ 可以是深度/视差图，或光流）的过程，该过程最小化一种形式的能量函数：
- en: '|  | $E(D)=\sum_{x}C(x,0pt_{x})+\sum_{x}\sum_{y\in\mathcal{N}_{x}}E_{s}(0pt_{x},0pt_{y}).$
    |  | (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(D)=\sum_{x}C(x,0pt_{x})+\sum_{x}\sum_{y\in\mathcal{N}_{x}}E_{s}(0pt_{x},0pt_{y}).$
    |  | (1) |'
- en: Here, $x$ and $y$ are image pixels, $0pt_{x}=D(x)$ is the depth / disparity
    at $x$, $C$ is a 3D cost volume where $C(x,0pt_{x})$ is the cost of pixel $x$
    having depth or disparity equal to $0pt_{x}$, $\mathcal{N}_{x}$ is the set of
    pixels that are within the neighborhood of $x$, and $E_{s}$ is a regularization
    term, which is used to impose various constraints, *e.g.,* smoothness and left-right
    depth/disparity consistency, to the final solution. The first term of Equation ([1](#S3.E1
    "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")) is the matching cost. In
    the case of rectified stereo pairs, it measures the cost of matching the pixel
    $x=(i,j)$ of the left image with the pixel $y=(i,j-0pt_{x})$ of the right image.
    In the more general multiview stereo case, it measures the inverse likelihood
    of $x$ on the reference image having depth $0pt_{x}$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$x$ 和 $y$ 是图像像素，$0pt_{x}=D(x)$ 是 $x$ 处的深度/视差，$C$ 是一个三维代价体，其中 $C(x,0pt_{x})$
    是像素 $x$ 拥有深度或视差等于 $0pt_{x}$ 的代价，$\mathcal{N}_{x}$ 是位于 $x$ 邻域内的像素集合，$E_{s}$ 是一个正则化项，用于对最终解施加各种约束，*例如*，平滑性和左右视差/深度一致性。方程
    ([1](#S3.E1 "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep
    Learning Architectures for Image-based Depth Reconstruction")) 的第一个项是匹配代价。在校正立体对的情况下，它测量将左图像的像素
    $x=(i,j)$ 与右图像的像素 $y=(i,j-0pt_{x})$ 匹配的代价。在更一般的多视图立体情况下，它测量参考图像上 $x$ 的逆似然性，其深度为
    $0pt_{x}$。
- en: 'In general, this problem is solved with a pipeline of four building blocks [[16](#bib.bib16)]:
    (1) feature extraction, (2) matching cost calculation and aggregation, (3) disparity/depth
    calculation, and (4) disparity/depth refinement. The first two blocks construct
    the cost volume $C$. The third and fourth blocks define the regularization term
    and find the depth/disparity map $\tilde{D}$ that minimizes Equation ([1](#S3.E1
    "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")). In this section, we review
    the recent methods that implement these individual blocks using deep learning
    techniques.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这个问题是通过四个构建块的管道来解决的[[16](#bib.bib16)]： (1) 特征提取， (2) 匹配代价计算和聚合， (3) 视差/深度计算，以及
    (4) 视差/深度优化。前两个块构建代价体 $C$。第三和第四个块定义正则化项，并找到最小化方程 ([1](#S3.E1 "In 3.1 The pipeline
    ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction")) 的深度/视差图 $\tilde{D}$。在本节中，我们回顾了利用深度学习技术实现这些单独模块的最新方法。
- en: 'TABLE I: Taxonomy of deep learning-based stereo matching algorithms. ”Arch.”
    refers to architecture. ”CV” refers to cost volume. ”corr.” refers to correlation.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 基于深度学习的立体匹配算法分类。“Arch.”指的是架构。“CV”指的是代价体积。“corr.”指的是相关性。'
- en: '|   |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: '| Method | (1) Feature extraction | (2) Matching cost computation | (3) Cost
    volume regularization | Depth estimation |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | (1) 特征提取 | (2) 匹配代价计算 | (3) 代价体积正则化 | 深度估计 |'
- en: '| Scale | Arch. | Hand-crafted | Learned similarity | Input | Approach / Net
    arch. | Output |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 尺度 | 架构 | 手工设计 | 学习相似性 | 输入 | 方法 / 网络架构 | 输出 |'
- en: '| Feature | Similarity learning |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 相似性学习 |'
- en: '| aggregation | Network | Output |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 聚合 | 网络 | 输出 |'
- en: '|  | Fixed vs. | ConvNet vs. | $L_{2}$ | Pooling, | FC, CNN | Matching score,
    | Cost volume (CV) | Standard stereo | Regularized CV | argmin, argmax |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | 固定 vs. | ConvNet vs. | $L_{2}$ | 池化， | FC，CNN | 匹配分数， | 代价体积 (CV) | 标准立体
    | 正则化 CV | argmin, argmax |'
- en: '|  | multiscale | ResNet | correlation | Concatenation |  | matching features
    | CV+ features (CVF) | encoder | Disparity/depth | soft argmin, soft argmax |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | 多尺度 | ResNet | 相关 | 连接 |  | 匹配特征 | CV + 特征 (CVF) | 编码器 | 差异/深度 | soft
    argmin, soft argmax |'
- en: '|  |  |  |  |  |  |  | CVF + segmentation (CVFS) | encoder + decoder |  | subpixel
    MAP |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  | CVF + 分割 (CVFS) | 编码器 + 解码器 |  | 亚像素 MAP |'
- en: '|  |  |  |  |  |  |  | CVF + edge map (CVFE) |  |  |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  | CVF + 边缘图 (CVFE) |  |  |  |'
- en: '|   |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: '| MC-CNN Accr [[17](#bib.bib17), [18](#bib.bib18)] | fixed | CNN | $-$ | concatenation
    | 4 FC layers | matching score | cost volume | Standard stereo | Regularized CV
    | argmin |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| MC-CNN Accr [[17](#bib.bib17), [18](#bib.bib18)] | 固定 | CNN | $-$ | 连接 |
    4 FC 层 | 匹配分数 | 代价体积 | 标准立体 | 正则化 CV | argmin |'
- en: '| Luo *et al.* [[19](#bib.bib19)] | fixed | ConvNet | correlation | $-$ | $-$
    | matching score | cost volume | Standard stereo | Regularized CV | argmin |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Luo *等*[[19](#bib.bib19)] | 固定 | ConvNet | 相关 | $-$ | $-$ | 匹配分数 | 代价体积 |
    标准立体 | 正则化 CV | argmin |'
- en: '| Chen *et al.* [[20](#bib.bib20)] | multiscale | ConvNet | corr. + voting
    | $-$ | $-$ | matching score | cost volume | Standard stereo | Regularized CV
    | argmin |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Chen *等*[[20](#bib.bib20)] | 多尺度 | ConvNet | 相关 + 投票 | $-$ | $-$ | 匹配分数 |
    代价体积 | 标准立体 | 正则化 CV | argmin |'
- en: '| L-ResMatch [[21](#bib.bib21)] | fixed | ResNet | $-$ | concatenation | 4
    (FC+ReLu) + FC | matching score | cost volume | standard stereo + | Regularized
    CV | argmin |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| L-ResMatch [[21](#bib.bib21)] | 固定 | ResNet | $-$ | 连接 | 4 (FC+ReLu) + FC
    | 匹配分数 | 代价体积 | 标准立体 + | 正则化 CV | argmin |'
- en: '|  |  |  |  |  |  |  |  | 4 Conv + 5 FC |  |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  | 4 Conv + 5 FC |  |  |'
- en: '| Han *et al.*[[22](#bib.bib22)] | fixed | ConvNet | $-$ | concatenation |
    2 (FC + Relu), FC | matching score | $-$ | $-$ | $-$ | softmax |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Han *等*[[22](#bib.bib22)] | 固定 | ConvNet | $-$ | 连接 | 2 (FC + Relu)，FC |
    匹配分数 | $-$ | $-$ | $-$ | softmax |'
- en: '| DispNetCorr [[13](#bib.bib13)] | fixed | ConvNet | 1D correlation | $-$ |
    $-$ | matching score | CV + features | encoder + decoder | disparity | $-$ |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| DispNetCorr [[13](#bib.bib13)] | 固定 | ConvNet | 1D 相关 | $-$ | $-$ | 匹配分数
    | CV + 特征 | 编码器 + 解码器 | 差异 | $-$ |'
- en: '| Pang *et al.*[[23](#bib.bib23)] | fixed | ConvNet | 1D correlation | $-$
    | $-$ | matching score | CV + features | encoder + decoder | disparity |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Pang *等*[[23](#bib.bib23)] | 固定 | ConvNet | 1D 相关 | $-$ | $-$ | 匹配分数 | CV
    + 特征 | 编码器 + 解码器 | 差异 |  |'
- en: '| Yu *et al.*[[24](#bib.bib24)] | fixed | ResNet | $-$ | concatenation | encoder+decoder
    | matching scores | cost volume | 3D Conv | Regularized CV | softargmin |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Yu *等*[[24](#bib.bib24)] | 固定 | ResNet | $-$ | 连接 | 编码器+解码器 | 匹配分数 | 代价体积
    | 3D Conv | 正则化 CV | softargmin |'
- en: '| Yang *et al.* [[25](#bib.bib25)] (un)sup. | fixed | ResNet | correlation
    | $-$ | $-$ | matching scores | CVF + segmentation | encoder-decoder | depth |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Yang *等*[[25](#bib.bib25)] (未) | 固定 | ResNet | 相关 | $-$ | $-$ | 匹配分数 | CVF
    + 分割 | 编码器-解码器 | 深度 |  |'
- en: '| Liang *et al.* [[26](#bib.bib26)] | multiscale | ConvNet | correlation |
    $-$ | $-$ | matching score | CV + features | encder-decoder | depth | $-$ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Liang *等*[[26](#bib.bib26)] | 多尺度 | ConvNet | 相关 | $-$ | $-$ | 匹配分数 | CV
    + 特征 | 编码器-解码器 | 深度 | $-$ |'
- en: '| Khamis *et al.* [[27](#bib.bib27)] | fixed | ResNet | $L_{2}$ | $-$ | $-$
    | $-$ | cost volume | encoder | regularized volume | soft argmin/max |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Khamis *等*[[27](#bib.bib27)] | 固定 | ResNet | $L_{2}$ | $-$ | $-$ | $-$ |
    代价体积 | 编码器 | 正则化体积 | soft argmin/max |'
- en: '| Chang & Chen [[28](#bib.bib28)] | multiscale | ResNet | $-$ | concatenate
    | $-$ | $-$ | cost volume | 12 conv layers, residual | regularized volume | regression
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Chang & Chen [[28](#bib.bib28)] | 多尺度 | ResNet | $-$ | 连接 | $-$ | $-$ | 代价体积
    | 12 个卷积层，残差 | 正则化体积 | 回归 |'
- en: '| (basic) |  |  |  |  |  |  |  | blocks, upsampling |  |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| (基本) |  |  |  |  |  |  |  | 块，上采样 |  |  |'
- en: '| Chang & Chen [[28](#bib.bib28)] | multiscale | ResNet | $-$ | concatenation
    | $-$ | $-$ | cost volume | stacked encoder-decorder blocks, | regularized volume
    | regression |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Chang & Chen [[28](#bib.bib28)] | 多尺度 | ResNet | $-$ | 拼接 | $-$ | $-$ | 代价体积
    | 堆叠的编码器-解码器块 | 正则化体积 | 回归 |'
- en: '| (stacked) |  |  |  |  |  |  |  | residual connections, upsampling |  |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| （堆叠的） |  |  |  |  |  |  |  | 残差连接，上采样 |  |  |'
- en: '| Zhong *et al.*[[29](#bib.bib29)] | fixed | ResNet | $-$ | concatenation |
    encoder-decoder | matching scores | cost volume | $-$ | regularized volume | soft
    argmin |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Zhong *等人*[[29](#bib.bib29)] | 固定 | ResNet | $-$ | 拼接 | 编码器-解码器 | 匹配分数 |
    代价体积 | $-$ | 正则化体积 | soft argmin |'
- en: '| SGM-Net [[30](#bib.bib30)] | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | cost volume
    | MRF + SGM-Net | regularized volume | $-$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| SGM-Net [[30](#bib.bib30)] | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | 代价体积 | MRF
    + SGM-Net | 正则化体积 | $-$ |'
- en: '| EdgeStereo [[31](#bib.bib31)] | fixed | VGG-16 | correlation | $-$ |  | matching
    scores | CVF + edge map | encoder-decoder (res. pyramid) | depth | $-$ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| EdgeStereo [[31](#bib.bib31)] | 固定 | VGG-16 | 相关性 | $-$ |  | 匹配分数 | CVF +
    边缘图 | 编码器-解码器（分辨率金字塔） | 深度 | $-$ |'
- en: '| Tulyakov *et al.*[[32](#bib.bib32)] | fixed | ConvNet | $-$ | concatenation
    | encoder-decoder | matching signatures | matching signatures | encoder + decoder
    | regularized volume | subpixel MAP |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Tulyakov *等人*[[32](#bib.bib32)] | 固定 | ConvNet | $-$ | 拼接 | 编码器-解码器 | 匹配签名
    | 匹配签名 | 编码器 + 解码器 | 正则化体积 | 子像素 MAP |'
- en: '|  |  |  |  | at each disparity |  |  |  |  |  |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 每个视差 |  |  |  |  |  |  |'
- en: '| Jie *et al.*[[33](#bib.bib33)] | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | Left
    and right CVs | Recurrent ConvLSTM | disparity | $-$ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Jie *等人*[[33](#bib.bib33)] | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | 左右视差体积 |
    循环 ConvLSTM | 视差 | $-$ |'
- en: '|  |  |  |  |  |  |  |  | with left-right consistency |  |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  | 具有左右一致性 |  |  |'
- en: '| Zagoruyko *et al.*[[34](#bib.bib34)] | multiscale | ConvNet | $-$ | concatenation
    | FC | matching scores | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Zagoruyko *等人*[[34](#bib.bib34)] | 多尺度 | ConvNet | $-$ | 拼接 | FC | 匹配分数 |
    $-$ | $-$ | $-$ | $-$ |'
- en: '| Hartmann *et al.* [[35](#bib.bib35)] | fixed | ConvNet |  | Avg pooling |
    CNN | matching score | $-$ | $-$ | $-$ | softmax |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Hartmann *等人* [[35](#bib.bib35)] | 固定 | ConvNet |  | 平均池化 | CNN | 匹配分数 |
    $-$ | $-$ | $-$ | softmax |'
- en: '| Huang *et al.* [[36](#bib.bib36)] | fixed | ConvNet | $-$ | Max pooling |
    CNN | matching features | cost volume | encoder | regularized volume | argmin
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Huang *等人* [[36](#bib.bib36)] | 固定 | ConvNet | $-$ | 最大池化 | CNN | 匹配特征 |
    代价体积 | 编码器 | 正则化体积 | argmin |'
- en: '| Yao *et al.* [[37](#bib.bib37)] | fixed | ConvNet | $-$ | Var. pooling |
    $-$ | matching features | cost volume | encoder-decoder | regularized volume |
    softmax |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Yao *等人* [[37](#bib.bib37)] | 固定 | ConvNet | $-$ | 变量池化 | $-$ | 匹配特征 | 代价体积
    | 编码器-解码器 | 正则化体积 | softmax |'
- en: '| Flynn *et al.* [[14](#bib.bib14)] | fixed | 2D Conv | $-$ | Conv across |
    CNN | matching score | cost volume | encoder | regularized volume | soft argmin/max
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Flynn *等人* [[14](#bib.bib14)] | 固定 | 2D Conv | $-$ | 跨卷积 | CNN | 匹配分数 | 代价体积
    | 编码器 | 正则化体积 | soft argmin/max |'
- en: '|  |  |  |  | depth layers |  |  |  |  |  |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 深度层 |  |  |  |  |  |  |'
- en: '| Kar *et al.* [[38](#bib.bib38)] | fixed | ConvNet | $-$ | feature unprojection
    + | CNN | matching score | cost volume | encoder-decoder | 3D occupancy | projection
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Kar *等人* [[38](#bib.bib38)] | 固定 | ConvNet | $-$ | 特征反投影 + | CNN | 匹配分数 |
    代价体积 | 编码器-解码器 | 3D 占用 | 投影 |'
- en: '|  |  |  |  | recurrent fusion |  |  |  |  | grid |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 循环融合 |  |  |  |  | 网格 |  |'
- en: '| Kendall *et al.* [[39](#bib.bib39)] | fixed | ConvNet | $-$ | concatenation
    | CNN | matching features | cost volume | encoder-decoder | regularized volume
    | soft argmin/max |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Kendall *等人* [[39](#bib.bib39)] | 固定 | ConvNet | $-$ | 拼接 | CNN | 匹配特征 |
    代价体积 | 编码器-解码器 | 正则化体积 | soft argmin/max |'
- en: '|   |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: 3.1.1 Feature extraction
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 特征提取
- en: '![Refer to caption](img/ad1cd76c83adebd58a8f8f0008bfd7eb.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ad1cd76c83adebd58a8f8f0008bfd7eb.png)'
- en: 'Figure 1: Feature extraction networks.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 特征提取网络。'
- en: '![Refer to caption](img/3f3c5c3a13365d9a866b917f9cf004d7.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3f3c5c3a13365d9a866b917f9cf004d7.png)'
- en: 'Figure 2: Multiscale feature extraction networks.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 多尺度特征提取网络。'
- en: The first step is to compute a good set of features to match across images.
    This has been modelled using CNN architectures where the encoder takes either
    patches around pixels of interests or entire images, and produces dense feature
    maps in the 2D image space. These features can be of fixed scale (Section [3.1.1.1](#S3.SS1.SSS1.P1
    "3.1.1.1 Fixed-scale features ‣ 3.1.1 Feature extraction ‣ 3.1 The pipeline ‣
    3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction")) or multiscale (Section [3.1.1.2](#S3.SS1.SSS1.P2 "3.1.1.2
    Multiscale features ‣ 3.1.1 Feature extraction ‣ 3.1 The pipeline ‣ 3 Depth by
    stereo matching ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction")).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是计算一组适用于图像匹配的良好特征。这已通过CNN架构进行建模，其中编码器处理感兴趣像素周围的补丁或整个图像，并在2D图像空间中生成密集特征图。这些特征可以是固定尺度的（第[3.1.1.1节](#S3.SS1.SSS1.P1
    "3.1.1.1 固定尺度特征 ‣ 3.1.1 特征提取 ‣ 3.1 管道 ‣ 3 通过立体匹配进行深度估计 ‣ 关于基于图像的深度重建的深度学习架构的调查")）或多尺度的（第[3.1.1.2节](#S3.SS1.SSS1.P2
    "3.1.1.2 多尺度特征 ‣ 3.1.1 特征提取 ‣ 3.1 管道 ‣ 3 通过立体匹配进行深度估计 ‣ 关于基于图像的深度重建的深度学习架构的调查")）。
- en: 3.1.1.1 Fixed-scale features
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.1.1 固定尺度特征
- en: The main type of network architectures that have been used in the literature
    is the multi-branch network with shared weights [[34](#bib.bib34), [17](#bib.bib17),
    [12](#bib.bib12), [18](#bib.bib18), [19](#bib.bib19), [13](#bib.bib13), [38](#bib.bib38),
    [39](#bib.bib39), [23](#bib.bib23), [26](#bib.bib26)], see also Figure [1](#S3.F1
    "Figure 1 ‣ 3.1.1 Feature extraction ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction").
    It is composed of $n\geq 2$ encoding branches, one for each input image, which
    act as descriptor computation modules. Each branch is a Convolutional Neural Networks
    (CNN), which takes a patch around a pixel $i$ and outputs a feature vector that
    characterizes that patch [[34](#bib.bib34), [17](#bib.bib17), [12](#bib.bib12),
    [40](#bib.bib40), [18](#bib.bib18), [19](#bib.bib19), [13](#bib.bib13), [14](#bib.bib14),
    [38](#bib.bib38), [39](#bib.bib39), [23](#bib.bib23), [36](#bib.bib36), [37](#bib.bib37),
    [26](#bib.bib26)]. It is generally composed of convolutional layers, spatial normalizations,
    pooling layers, and rectified linear units (ReLU). The scale of the features that
    are extracted is controlled by the size of the convolutional filters used in each
    layer as well as by the number of convolutional and pooling layers. Increasing
    the size of the filters and/or the number of layers increases the scale of the
    features that will be extracted. It has also the advantage of capturing more interactions
    between the image pixels. However, this comes with a high computation cost. To
    reduce the computational cost while increasing the field of view of the network,
    some techniques, *e.g.,*  [[41](#bib.bib41)], use dilated convolutions, *i.e.,*
    large convolutional filters but with holes and thus they are computationally efficient.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中使用的主要网络架构类型是具有共享权重的多分支网络[[34](#bib.bib34), [17](#bib.bib17), [12](#bib.bib12),
    [18](#bib.bib18), [19](#bib.bib19), [13](#bib.bib13), [38](#bib.bib38), [39](#bib.bib39),
    [23](#bib.bib23), [26](#bib.bib26)]，另见图[1](#S3.F1 "图1 ‣ 3.1.1 特征提取 ‣ 3.1 管道 ‣
    3 通过立体匹配进行深度估计 ‣ 关于基于图像的深度重建的深度学习架构的调查")。它由$n\geq 2$个编码分支组成，每个分支对应一个输入图像，作为描述符计算模块。每个分支是一个卷积神经网络（CNN），它处理像素$i$周围的补丁，并输出一个描述该补丁的特征向量[[34](#bib.bib34),
    [17](#bib.bib17), [12](#bib.bib12), [40](#bib.bib40), [18](#bib.bib18), [19](#bib.bib19),
    [13](#bib.bib13), [14](#bib.bib14), [38](#bib.bib38), [39](#bib.bib39), [23](#bib.bib23),
    [36](#bib.bib36), [37](#bib.bib37), [26](#bib.bib26)]。它通常由卷积层、空间归一化、池化层和修正线性单元（ReLU）组成。提取特征的尺度由每层中使用的卷积滤波器的大小以及卷积层和池化层的数量控制。增加滤波器的大小和/或层数会增加提取特征的尺度。这也有利于捕捉图像像素之间的更多交互。然而，这会带来较高的计算成本。为了在增加网络视野的同时减少计算成本，一些技术，如[[41](#bib.bib41)]，使用扩张卷积，即具有孔的较大卷积滤波器，从而在计算上更为高效。
- en: Instead of using fully convolutional networks, some techniques [[25](#bib.bib25)]
    use residual networks, *e.g.,* ResNet [[42](#bib.bib42)], *i.e.,* CNNs with residual
    blocks. A residual block takes an input and estimates the residual that needs
    to be added to that input. They are used to ease the training of substantially
    deep networks since learning the residual of a signal is much easier than learning
    to predict the signal itself. Various types of residual blocks have been used
    in the literature. For example, Shaked and Wolf [[21](#bib.bib21)] proposed appending
    residual blocks with multilevel connections. Its particularity is that the network
    learns by itself how to adjust the contribution of the added skip connections.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有些技术 [[25](#bib.bib25)] 采用残差网络，而不是使用全卷积网络，*例如* ResNet [[42](#bib.bib42)]，*即*
    带有残差块的CNN。残差块接收一个输入并估计需要添加到该输入中的残差。它们用于简化非常深的网络的训练，因为学习信号的残差比学习预测信号本身要容易得多。文献中使用了各种类型的残差块。例如，Shaked
    和 Wolf [[21](#bib.bib21)] 提出了附加具有多级连接的残差块。其特点是网络可以自我学习如何调整添加的跳跃连接的贡献。
- en: Table [II](#S3.T2 "TABLE II ‣ 3.1.1.1 Fixed-scale features ‣ 3.1.1 Feature extraction
    ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") summarises the detailed architecture (number
    of layers, filter sizes, and stride at each layer) of various methods and the
    size of the features they produce. Note that, one advantage of convolutional networks
    is that the convolutional operations within one level are independent from each
    other, and thus they are parallelizable. As such, all the features of an entire
    image can be computed with a single forward pass.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Table [II](#S3.T2 "TABLE II ‣ 3.1.1.1 固定尺度特征 ‣ 3.1.1 特征提取 ‣ 3.1 流程 ‣ 3 通过立体匹配的深度
    ‣ 关于图像深度重建的深度学习架构的调查") 总结了各种方法的详细架构（每层的层数、滤波器大小和步幅）及其产生的特征大小。请注意，卷积网络的一个优点是同一层内的卷积操作是相互独立的，因此可以并行计算。这样，整个图像的所有特征可以通过一次前向传递计算完成。
- en: 'TABLE II: Network architectures for feature extraction. Each layer of the network
    is described in the following format: (filter size, type, stride, output feature
    size, scaling). Scaling refers to upscaling or downscaling of the resolution of
    the output with respect to the input. SPP refers to Spatial Pyramid Pooling. The
    last column refers to the feature size as produced by the last layer.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE II: 特征提取的网络结构。网络的每一层以以下格式描述：（滤波器大小，类型，步幅，输出特征大小，缩放）。缩放指的是相对于输入的输出分辨率的放大或缩小。SPP
    指的是空间金字塔池化。最后一列表示最后一层产生的特征大小。'
- en: '| Method | Input | Type | Architecture | Feature size |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 输入 | 类型 | 结构 | 特征大小 |'
- en: '| Dosovitskiy *et al.* [[12](#bib.bib12)] | $512\times 384$ | CNN | $(7\times
    7,conv,2,64,-),(5\times 5,conv,2,128,-),(5\times 5,conv,2,256,-)$ | $64\times
    48\times 256$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Dosovitskiy *等* [[12](#bib.bib12)] | $512\times 384$ | CNN | $(7\times 7,conv,2,64,-),(5\times
    5,conv,2,128,-),(5\times 5,conv,2,256,-)$ | $64\times 48\times 256$ |'
- en: '| Chen *et al.* [[20](#bib.bib20)] | $13\times 13$ | CNN | $(3\times 3,conv,-,32,-)_{1,2}$,
    $(5\times 5,conv,-,200,-)_{3,4}$ | $1\times 200$ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Chen *等* [[20](#bib.bib20)] | $13\times 13$ | CNN | $(3\times 3,conv,-,32,-)_{1,2}$,
    $(5\times 5,conv,-,200,-)_{3,4}$ | $1\times 200$ |'
- en: '| Zagoruyko [[34](#bib.bib34)] | patches of varying sizes | CNN + SPP | $(-,conv+ReLu,-,-,-)_{1,2},(-,conv+SPP,-,-,-)$
    | $1\times c$ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Zagoruyko [[34](#bib.bib34)] | 不同大小的补丁 | CNN + SPP | $(-,conv+ReLu,-,-,-)_{1,2},(-,conv+SPP,-,-,-)$
    | $1\times c$ |'
- en: '| Zbontar & LeCun [[18](#bib.bib18)] (fast) | patches $9\times 9$ | CNN | $(3\times
    3,conv+ReLu,-,64,-)_{1,2,3},(3\times 3,conv,-,64,-)$ | $1\times 64$ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Zbontar & LeCun [[18](#bib.bib18)] (快速) | 补丁 $9\times 9$ | CNN | $(3\times
    3,conv+ReLu,-,64,-)_{1,2,3},(3\times 3,conv,-,64,-)$ | $1\times 64$ |'
- en: '| Zbontar & LeCun [[18](#bib.bib18)] (accr) | patches $9\times 9$ | CNN | $(3\times
    3,conv+ReLu,-,112,-)_{1,2,3},(3\times 3,conv,-,112,-)$ | $1\times 112$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Zbontar & LeCun [[18](#bib.bib18)] (准确) | 补丁 $9\times 9$ | CNN | $(3\times
    3,conv+ReLu,-,112,-)_{1,2,3},(3\times 3,conv,-,112,-)$ | $1\times 112$ |'
- en: '| Luo *et al.* [[19](#bib.bib19)] | Small patch | CNN | $(3\times 3\text{ or
    }5\times 5,conv+ReLu,-,32\text{ or }64,-)_{1,2,3},\text{or}$ | $1\times 32$ or
    $1\times 64$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Luo *等* [[19](#bib.bib19)] | 小补丁 | CNN | $(3\times 3\text{ 或 }5\times 5,conv+ReLu,-,32\text{
    或 }64,-)_{1,2,3},\text{或}$ | $1\times 32$ 或 $1\times 64$ |'
- en: '|  |  |  | $(5\times 5,conv,-,32\text{ or }64,-)$ |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | $(5\times 5,conv,-,32\text{ 或 }64,-)$ |  |'
- en: '| DispNetC [[13](#bib.bib13)], Pang *et al.* [[23](#bib.bib23)] | $768\times
    364$ | CNN | $(7\times 7,conv,2,64,-),(5\times 5,conv,2,128,-)$ | $192\times 96\times
    128$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| DispNetC [[13](#bib.bib13)], Pang *等* [[23](#bib.bib23)] | $768\times 364$
    | CNN | $(7\times 7,conv,2,64,-),(5\times 5,conv,2,128,-)$ | $192\times 96\times
    128$ |'
- en: '| Kendall *et al.* [[39](#bib.bib39)] | $0pt\times 0pt$ | CNN (2D conv) + |
    $(5\times 5,conv,2,32,/2),[(3\times 3,conv,-,32,-),(3\times 3,res,-,32,-)]_{1,\dots,7},$
    | $\frac{1}{2}0pt\times\frac{1}{2}0pt\times 32$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Kendall *等* [[39](#bib.bib39)] | $0pt\times 0pt$ | CNN (2D conv) + | $(5\times
    5,conv,2,32,/2),[(3\times 3,conv,-,32,-),(3\times 3,res,-,32,-)]_{1,\dots,7},$
    | $\frac{1}{2}0pt\times\frac{1}{2}0pt\times 32$ |'
- en: '|  |  | Residual connections | $(3\times 3,conv,-,32,-)$, No RLu or BN on the
    last layer |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 残差连接 | $(3\times 3,conv,-,32,-)$, 最后一层没有 RLu 或 BN |  |'
- en: '| Liang *et al.* [[26](#bib.bib26)] | $0pt\times 0pt$ | CNN | $(7\times 7,conv,2,64,/2),(5\times
    5,conv,2,128,/2)$. | $\frac{1}{4}0pt\times\frac{1}{4}0pt\times 128$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Liang *等* [[26](#bib.bib26)] | $0pt\times 0pt$ | CNN | $(7\times 7,conv,2,64,/2),(5\times
    5,conv,2,128,/2)$。 | $\frac{1}{4}0pt\times\frac{1}{4}0pt\times 128$ |'
- en: '| Kar *et al.* [[38](#bib.bib38)] | $224\times 224$ | CNN | $(3\times 3,conv,-,64,-),(3\times
    3,conv,-,64,-),(2\times 2,\text{maxpool},-,64,-)$ | $32\times 32\times 1024$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 卡尔 *等* [[38](#bib.bib38)] | $224\times 224$ | CNN | $(3\times 3,conv,-,64,-),(3\times
    3,conv,-,64,-),(2\times 2,\text{maxpool},-,64,-)$ | $32\times 32\times 1024$ |'
- en: '|  |  |  | $(3\times 3,conv,-,128,-),(3\times 3,conv,-,128,-),(2\times 2,\text{maxpool},-,128,-)$
    |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | $(3\times 3,conv,-,128,-),(3\times 3,conv,-,128,-),(2\times 2,\text{maxpool},-,128,-)$
    |  |'
- en: '|  |  |  | $(3\times 3,conv,-,512,-),(3\times 3,conv,-,512,-),(2\times 2,\text{maxpool},-,512,-)$
    |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | $(3\times 3,conv,-,512,-),(3\times 3,conv,-,512,-),(2\times 2,\text{maxpool},-,512,-)$
    |  |'
- en: '|  |  |  | $(3\times 3,conv,-,1024,-),(3\times 3,conv,-,1024,-)$ |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | $(3\times 3,conv,-,1024,-),(3\times 3,conv,-,1024,-)$ |  |'
- en: '| Yang *et al.* [[25](#bib.bib25)] | $0pt\times 0pt$ | CNN + Residual blocks
    | $(3\times 3,conv,2,64,/2),(3\times 3,conv,1,64,/1),(3\times 3,conv,1,128,/1)$
    |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 杨 *等* [[25](#bib.bib25)] | $0pt\times 0pt$ | CNN + 残差块 | $(3\times 3,conv,2,64,/2),(3\times
    3,conv,1,64,/1),(3\times 3,conv,1,128,/1)$ |  |'
- en: '|  |  |  | $(3\times 3,maxpool,2,128,/2),(3\times 3,res\_block,1,256,/2)$,
    |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | $(3\times 3,maxpool,2,128,/2),(3\times 3,res\_block,1,256,/2)$,
    |  |'
- en: '|  |  |  | $(3\times 3,res\_block,1,256,/1)_{1,2}$, $(3\times 3,res\_blcok,1,512,/2)$
    | $-$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | $(3\times 3,res\_block,1,256,/1)_{1,2}$, $(3\times 3,res\_blcok,1,512,/2)$
    | $-$ |'
- en: '| Shaked and Wolf [[21](#bib.bib21)] | $11\times 11$ | CNN + | $conv_{1},ReLU,$
    Outer $\lambda-$ residual block, $ReLU,conv_{2\cdots 5},ReLU,$ | $1\times 1\times
    112$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Shaked and Wolf [[21](#bib.bib21)] | $11\times 11$ | CNN + | $conv_{1},ReLU,$
    外部 $\lambda-$ 残差块, $ReLU,conv_{2\cdots 5},ReLU,$ | $1\times 1\times 112$ |'
- en: '|  |  | Outer $\lambda-$residual blocks | Outer $\lambda-$ residual block |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 外部 $\lambda-$残差块 | 外部 $\lambda-$ 残差块 |  |'
- en: 3.1.1.2 Multiscale features
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.1.2 多尺度特征
- en: The methods described in Section [3.1.1.1](#S3.SS1.SSS1.P1 "3.1.1.1 Fixed-scale
    features ‣ 3.1.1 Feature extraction ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    can be extended to extract features at multiple scales, see Figure [2](#S3.F2
    "Figure 2 ‣ 3.1.1 Feature extraction ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction").
    This is done either by feeding the network with patches of different sizes centered
    at the same pixel [[34](#bib.bib34), [20](#bib.bib20), [28](#bib.bib28)], or by
    using the features computed by the intermediate layers [[26](#bib.bib26)]. Note
    that the deeper is a layer in the network, the larger is the scale of the features
    it computes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第 [3.1.1.1](#S3.SS1.SSS1.P1 "3.1.1.1 固定尺度特征 ‣ 3.1.1 特征提取 ‣ 3.1 流水线 ‣ 3 深度立体匹配
    ‣ 图像深度重建的深度学习架构调查") 节中描述的方法可以扩展到多个尺度提取特征，见图 [2](#S3.F2 "图 2 ‣ 3.1.1 特征提取 ‣ 3.1
    流水线 ‣ 3 深度立体匹配 ‣ 图像深度重建的深度学习架构调查")。这可以通过向网络提供以同一像素为中心的不同大小的补丁 [[34](#bib.bib34),
    [20](#bib.bib20), [28](#bib.bib28)]，或使用中间层计算的特征实现 [[26](#bib.bib26)]。注意，网络中的某一层越深，计算的特征尺度就越大。
- en: Liang *et al.* [[26](#bib.bib26)] compute multiscale features using a two-layer
    convolutional network. The output of the two layers are then concatenated and
    fused, using a convolutional layer, which results in *multi-scale fusion features*.
    Zagoruyko and Komodakis [[34](#bib.bib34)] proposed a central-surround two-stream
    network which is essentially a network composed of two siamese networks combined
    at the output by a top network. The first siamese network, called central high-resolution
    stream, receives as input two $32\times 32$ patches that are generated by cropping
    (at the original resolution) the central $32\times 32$ part of each $64\times
    64$-input patch. The second network, called surround low-resolution stream, receives
    as input two $32\times 32$ patches generated by downsampling at half the original
    input. Chen *et al.* [[20](#bib.bib20)] also used a similar approach but each
    network processes patches of size $13\times 13$. The main advantage of this architecture
    is that it can compute the features at two different resolutions in a single forward
    pass. It, however, requires one stream by scale, which is not practical if more
    than two scales are needed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Liang *et al.* [[26](#bib.bib26)] 使用一个两层卷积网络计算多尺度特征。这两层的输出被连接并融合，使用卷积层，这样就得到了
    *多尺度融合特征*。Zagoruyko 和 Komodakis [[34](#bib.bib34)] 提出了一个中心-周围的双流网络，本质上是由两个在输出处由顶层网络结合的孪生网络组成。第一个孪生网络称为中心高分辨率流，它接收两个
    $32\times 32$ 的图像块，这些图像块是通过裁剪（在原始分辨率下）每个 $64\times 64$ 输入图像块的中心 $32\times 32$
    部分生成的。第二个网络称为周围低分辨率流，它接收两个通过将原始输入下采样一半生成的 $32\times 32$ 图像块。Chen *et al.* [[20](#bib.bib20)]
    也使用了类似的方法，但每个网络处理的是 $13\times 13$ 大小的图像块。这种架构的主要优点是可以在一次前向传播中计算两个不同分辨率的特征。然而，它需要每种尺度一个流，如果需要更多尺度则不够实用。
- en: Chang and Chen [[28](#bib.bib28)] used Spatial Pyramid Pooling (SPP) module
    to aggregate context in different scales and location. More precisely, the feature
    extraction module is composed of a CNN of seven layers, and an SPP module followed
    by convolutional layers. The CNN produces a feature map of size $\frac{1}{4}0pt\times\frac{1}{4}0pt\times
    128$. The SPP module then takes a patch around each pixel but at four different
    sizes ($8\times 8\times 128$, $16\times 16\times 128$, $32\times 32\times 128$,
    and $64\times 64\times 128$), and converts them into one-channel by mean pooling
    followed by a $1\times 1$ convolution. These are then upsampled to the desired
    size and concatenated with features from different layers of the CNN, and further
    processed with additional convolutional layers to produce the features that will
    be fed to the subsequent modules for matching and disparity computation. Chang
    and Chen [[28](#bib.bib28)] showed that the SPP module enables estimating disparity
    for inherently ill-posed regions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Chang 和 Chen [[28](#bib.bib28)] 使用了空间金字塔池化（SPP）模块来聚合不同尺度和位置的上下文。更具体地说，特征提取模块由一个七层的
    CNN 和一个 SPP 模块加上卷积层组成。CNN 生成一个大小为 $\frac{1}{4}0pt\times\frac{1}{4}0pt\times 128$
    的特征图。然后，SPP 模块对每个像素周围的图像块进行四种不同尺寸（$8\times 8\times 128$、$16\times 16\times 128$、$32\times
    32\times 128$ 和 $64\times 64\times 128$）的均值池化，并通过 $1\times 1$ 卷积将其转换为单通道。这些特征图被上采样到所需尺寸，并与
    CNN 不同层的特征连接，然后通过额外的卷积层进一步处理，生成将用于后续模块的特征，以进行匹配和视差计算。Chang 和 Chen [[28](#bib.bib28)]
    表明，SPP 模块使得对固有难以解决的区域进行视差估计成为可能。
- en: In general, Spatial Pyramid Pooling (SPP) are convenient for processing patches
    of arbitrary sizes. For instance, Zaogoruyko and Komodakis [[34](#bib.bib34)]
    append an SPP layer at the end of the feature computation network. Such a layer
    aggregates the features of the last convolutional layer through spatial pooling,
    where the size of the pooling regions dependents on the size of the input. By
    doing so, one will be able to feed the network with patches of arbitrary sizes
    and compute feature vectors of the same dimension.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，空间金字塔池化（SPP）对于处理任意大小的图像块很方便。例如，Zaogoruyko 和 Komodakis [[34](#bib.bib34)]
    在特征计算网络的末尾附加了一个 SPP 层。这样的层通过空间池化聚合最后一层卷积层的特征，其中池化区域的大小取决于输入的大小。通过这样做，可以将任意大小的图像块输入网络，并计算出相同维度的特征向量。
- en: 3.1.2 Matching cost computation
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 匹配成本计算
- en: '![Refer to caption](img/cc671ee69ea9824e8e15b2265dec606e.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/cc671ee69ea9824e8e15b2265dec606e.png)'
- en: 'Figure 3: Taxonomy of similarity computation networks.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：相似性计算网络的分类。
- en: This module takes the features computed on each of the input images, and computes
    the matching scores of Equation ([1](#S3.E1 "In 3.1 The pipeline ‣ 3 Depth by
    stereo matching ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction")). The matching scores form a 3D volume, called *Disparity Space
    Image (DSI)* [[16](#bib.bib16)], of the form $C(x,d_{x})$ where $x=(i,j)$ is the
    image coordinates of pixel $x$ and $d_{x}\in[0,n_{d}]$ is the candidate disparity/depth
    value. It is of size $\tilde{0pt}\times\tilde{0pt}\times(n_{d}+1)$, where $\tilde{0pt}\times\tilde{0pt}$
    is the resolution at which we want to compute the depth map and $n_{d}$ is the
    number of depth/disparity values. In stereo matching, if the left and right images
    have been rectified so that the epipolar lines are horizontal then $C(x,d_{x})$
    is the similarity between the pixel $x=(i,j)$ on the rectified left image and
    the pixel $y=(i,j-d_{x})$ on the rectified right image. Otherwise, $C(x,d_{x})$
    indicates the likelihood, or probability, of the pixel $x$ having depth $d_{x}$.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本模块利用输入图像上计算得到的特征，并计算方程 ([1](#S3.E1 "在 3.1 流程 ‣ 3 通过立体匹配获取深度 ‣ 关于基于图像的深度重建的深度学习架构的综述"))
    的匹配分数。这些匹配分数形成一个 3D 体积，称为*视差空间图像 (DSI)* [[16](#bib.bib16)]，形式为 $C(x,d_{x})$，其中
    $x=(i,j)$ 是像素 $x$ 的图像坐标，$d_{x}\in[0,n_{d}]$ 是候选视差/深度值。其大小为 $\tilde{0pt}\times\tilde{0pt}\times(n_{d}+1)$，其中
    $\tilde{0pt}\times\tilde{0pt}$ 是我们想要计算深度图的分辨率，$n_{d}$ 是深度/视差值的数量。在立体匹配中，如果左图和右图已经进行了校正，使得极线为水平线，则
    $C(x,d_{x})$ 表示在校正后的左图中像素 $x=(i,j)$ 和在校正后的右图中像素 $y=(i,j-d_{x})$ 之间的相似度。否则，$C(x,d_{x})$
    表示像素 $x$ 具有深度 $d_{x}$ 的可能性或概率。
- en: Similar to traditional stereo-matching methods [[16](#bib.bib16)], the cost
    volume is computing by comparing the deep features of the input images using standard
    metrics such as the $L_{2}$ distance, the cosine distance, and the (normalized)
    correlation distance (Section [3.1.2.1](#S3.SS1.SSS2.P1 "3.1.2.1 Using distance
    measures ‣ 3.1.2 Matching cost computation ‣ 3.1 The pipeline ‣ 3 Depth by stereo
    matching ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")).
    With the avenue of deep neural networks, several new mechanisms have been proposed
    (Section [3.1.2.2](#S3.SS1.SSS2.P2 "3.1.2.2 Using similarity-learning networks
    ‣ 3.1.2 Matching cost computation ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")).
    Figure [3](#S3.F3 "Figure 3 ‣ 3.1.2 Matching cost computation ‣ 3.1 The pipeline
    ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction") shows the main similarity computation architectures. Below,
    we discuss them in details.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于传统的立体匹配方法 [[16](#bib.bib16)]，代价体积通过使用标准度量（如 $L_{2}$ 距离、余弦距离和（归一化的）相关距离）来比较输入图像的深度特征进行计算（第
    [3.1.2.1](#S3.SS1.SSS2.P1 "3.1.2.1 使用距离度量 ‣ 3.1.2 匹配代价计算 ‣ 3.1 流程 ‣ 3 通过立体匹配获取深度
    ‣ 关于基于图像的深度重建的深度学习架构的综述") 节）。随着深度神经网络的兴起，提出了几种新的机制（第 [3.1.2.2](#S3.SS1.SSS2.P2
    "3.1.2.2 使用相似性学习网络 ‣ 3.1.2 匹配代价计算 ‣ 3.1 流程 ‣ 3 通过立体匹配获取深度 ‣ 关于基于图像的深度重建的深度学习架构的综述")
    节）。图 [3](#S3.F3 "图 3 ‣ 3.1.2 匹配代价计算 ‣ 3.1 流程 ‣ 3 通过立体匹配获取深度 ‣ 关于基于图像的深度重建的深度学习架构的综述")
    显示了主要的相似性计算架构。接下来，我们将详细讨论这些架构。
- en: 3.1.2.1 Using distance measures
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.2.1 使用距离度量
- en: The simplest way to form a cost volume is by taking the distance between the
    feature vector of a pixel and the feature vectors of the matching candidates,
    *i.e.,* the pixels on the other image that are within a pre-defined disparity
    range. There are several distance measures that can be used. Khamis *et al.* [[27](#bib.bib27)],
    for example, used the $L_{2}$ distance. Other techniques, *e.g.,* [[12](#bib.bib12),
    [20](#bib.bib20), [18](#bib.bib18), [19](#bib.bib19), [13](#bib.bib13), [26](#bib.bib26)],
    used correlation, *i.e.,* the inner product between feature vectors. The main
    advantage of correlation over the $L_{2}$ distance is that it can be implemented
    using a layer of 2D [[12](#bib.bib12)] or 1D [[13](#bib.bib13)] convolutional
    operations, called correlation layer. 1D correlations are computationally more
    efficient than their 2D counterpart. They, however, require rectified images so
    that the search for correspondences is restricted to pixels within the same raw.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 形成成本体积的最简单方法是通过计算像素的特征向量与匹配候选的特征向量之间的距离，即其他图像中在预定义视差范围内的像素。可以使用几种距离度量方法。例如，Khamis
    *et al.* [[27](#bib.bib27)] 使用了 $L_{2}$ 距离。其他技术，例如 [[12](#bib.bib12), [20](#bib.bib20),
    [18](#bib.bib18), [19](#bib.bib19), [13](#bib.bib13), [26](#bib.bib26)]，使用了相关性，即特征向量之间的内积。相关性的主要优势在于它可以通过
    2D [[12](#bib.bib12)] 或 1D [[13](#bib.bib13)] 卷积操作层实现，称为相关层。1D 相关性在计算上比其 2D 对应物更高效。然而，它们需要经过校正的图像，以便将匹配搜索限制在同一行的像素中。
- en: Compared to the two other methods that will be described below, the main advantage
    of the correlation layer is that it does not require training since the filters
    are in fact the features computed by the second branch of the network.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与下面将描述的其他两种方法相比，相关层的主要优势在于它不需要训练，因为滤波器实际上是由网络第二分支计算出的特征。
- en: 3.1.2.2 Using similarity-learning networks
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.2.2 使用相似性学习网络
- en: These methods aggregate the features produced by the different branches, and
    process them with a top network, which produces a matching score. The rational
    is to let the network learn from data the appropriate similarity measure.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法聚合不同分支产生的特征，并通过顶层网络处理，生成匹配分数。其合理性在于让网络从数据中学习适当的相似性度量。
- en: '(1) Feature aggregation. Some stereo reconstruction methods first aggregate
    the features computed by the different branches of the network before passing
    them through further processing layers. The aggregation can be done in two different
    ways:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 特征聚合。一些立体重建方法在通过进一步处理层之前，首先聚合网络不同分支计算出的特征。聚合可以通过两种不同的方法完成：
- en: Aggregation by concatenation. The simplest way is to just concatenate the learned
    features computed by the different branches of the network and feed them to the
    similarity computation network [[34](#bib.bib34), [17](#bib.bib17), [18](#bib.bib18),
    [39](#bib.bib39)]. Kendall *et al.* [[39](#bib.bib39)] concatenate each feature
    with their corresponding feature from the opposite stereo image across each disparity
    level, and pack these into a 4D volume of dimensionality $0pt\times 0pt\times(n_{d}+1)\times
    c$ ($c$ here is the dimension of the features). Huang *et al.* [[36](#bib.bib36)],
    on the other hand, concatenate the $64^{3}$ feature volume, computed for the $64\times
    64\times 3$ reference image, and another volume of the same size from the plane-sweep
    volume plane that corresponds to the $n-$th input image at the $d-$th disparity
    level, to form a $64\times 64\times 128$ volume. Zhong *et al.* [[29](#bib.bib29)]
    followed the same approach but concatenate the features in an interleaved manner.
    That is, if $\textbf{f}_{L}$ is the feature map of the left image and $\textbf{f}_{R}$
    the feature map of the right image then the final feature volume is assembled
    in such a way that its $2i-$th slice holds the left feature map while the $(2i+1)-$th
    slice holds the right feature map but at disparity $d=i$. That is,
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接进行聚合。最简单的方法是直接连接由网络的不同分支计算出的特征，并将其输入到相似度计算网络中[[34](#bib.bib34), [17](#bib.bib17),
    [18](#bib.bib18), [39](#bib.bib39)]。Kendall *等人* [[39](#bib.bib39)] 将每个特征与来自对侧立体图像的相应特征在每个视差级别上进行连接，并将这些特征打包成一个尺寸为
    $0pt\times 0pt\times(n_{d}+1)\times c$ 的 4D 体积（此处 $c$ 是特征的维度）。另一方面，Huang *等人*
    [[36](#bib.bib36)] 将为 $64\times 64\times 3$ 参考图像计算出的 $64^{3}$ 特征体积与来自平面扫掠体积平面、对应于第
    $d-$ 视差级别的第 $n-$ 输入图像的另一体积连接，形成一个 $64\times 64\times 128$ 体积。Zhong *等人* [[29](#bib.bib29)]
    采用了相同的方法，但以交错的方式连接特征。即，如果 $\textbf{f}_{L}$ 是左图像的特征图，而 $\textbf{f}_{R}$ 是右图像的特征图，则最终的特征体积的
    $2i-$ 层包含左特征图，而 $(2i+1)-$ 层包含右特征图，但在视差 $d=i$。
- en: '|  | $\textbf{f}_{LR}(u,v,d)=\textbf{f}_{L}(u,v)\&#124;\textbf{f}_{R}(u-d,v),$
    |  | (2) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{f}_{LR}(u,v,d)=\textbf{f}_{L}(u,v)\&#124;\textbf{f}_{R}(u-d,v),$
    |  | (2) |'
- en: where $\|$ denotes the vector concatenation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\|$ 表示向量连接。
- en: Aggregation by pooling. Another approach is to use pooling layers to aggregate
    the feature maps. For instance, Hartmann *et al.* [[35](#bib.bib35)] used average
    pooling. Huang *et al.* [[36](#bib.bib36)] used max-pooling, while Yao *et al.* [[37](#bib.bib37)]
    take their variance, which is equivalent to first computing the average feature
    vector and then taking the average distance of the other features to the mean.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过池化进行聚合。另一种方法是使用池化层来聚合特征图。例如，Hartmann *等人* [[35](#bib.bib35)] 使用了平均池化。Huang
    *等人* [[36](#bib.bib36)] 使用了最大池化，而 Yao *等人* [[37](#bib.bib37)] 则取其方差，相当于先计算平均特征向量，然后计算其他特征与均值的平均距离。
- en: The main advantage of pooling over concatenation is three-fold; First, it does
    not increase the dimensionality of the data that is fed to the top similarity
    computation network, which facilitates the training. Second, it makes it possible
    to input a varying number of views without retraining the network. This is particularly
    suitable for multiview stereo (MVS) approaches, especially when dealing with an
    arbitrary number of input images and when the number of images at runtime may
    be different from the number of images at training. Finally, pooling ensures that
    the results are invariant with respect to the order in which the images are fed
    to the network.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 池化相对于连接的主要优点有三；首先，它不会增加输入到顶部相似度计算网络的数据的维度，从而有利于训练。其次，它使得在不重新训练网络的情况下输入不同数量的视图成为可能。这对于多视图立体（MVS）方法尤其合适，特别是当处理任意数量的输入图像时，且运行时图像数量可能与训练时的图像数量不同。最后，池化确保结果对输入图像的顺序保持不变。
- en: '(2) Similarity computation. There are two types of networks that have been
    used in the literature: fully connected networks and convolutional networks.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 相似度计算。文献中使用了两种类型的网络：全连接网络和卷积网络。
- en: Using fully-connected networks. In these methods, the similarity computation
    network is composed of fully connected layers [[34](#bib.bib34), [17](#bib.bib17),
    [18](#bib.bib18)]. The last layer produces the probability of the input feature
    vectors being a good or a bad match. Zagoruyko and Komodakis [[34](#bib.bib34)],
    for example, used a network composed of two fully connected layers (each with
    $512$ hidden units) that are separated by a ReLU activation layer. Zbontar and
    LeCun [[17](#bib.bib17), [18](#bib.bib18)] used five fully connected layers with
    $300$ neurones each except for the last layer, which projects the output to two
    real numbers that are fed through a softmax function, which in turn produces the
    probability of the two input feature vectors being a good match.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用全连接网络。在这些方法中，相似度计算网络由全连接层组成[[34](#bib.bib34)、[17](#bib.bib17)、[18](#bib.bib18)]。最后一层生成输入特征向量是好匹配还是差匹配的概率。例如，Zagoruyko和Komodakis[[34](#bib.bib34)]使用了由两个全连接层（每层有$512$个隐藏单元）组成的网络，这些层之间由ReLU激活层隔开。Zbontar和LeCun[[17](#bib.bib17)、[18](#bib.bib18)]使用了五个全连接层，每个层有$300$个神经元，最后一层将输出映射到两个实数，通过softmax函数计算概率，进而得到两个输入特征向量的匹配概率。
- en: Using convolutional networks. Another approach is to aggregate the features
    and further post-process them using convolutional networks, which output either
    matching scores [[14](#bib.bib14), [38](#bib.bib38), [35](#bib.bib35)] (similar
    to correlation layers), or matching features [[39](#bib.bib39), [36](#bib.bib36)].
    The most commonly used CNNs include max-pooling layers, which provide invariance
    in spatial transformation. Pooling layers also widen the receptive field area
    of a CNN without increasing the number of parameters. The drawback is that the
    network loses fine details. To overcome this limitation, Park and Lee [[43](#bib.bib43)]
    introduced a pixel-wise pyramid pooling layer to enlarge the receptive field during
    the comparison of two input patches. This method produced more accurate matching
    cost than [[17](#bib.bib17)].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积网络。另一种方法是聚合特征，并通过卷积网络进一步后处理这些特征，卷积网络输出匹配分数[[14](#bib.bib14)、[38](#bib.bib38)、[35](#bib.bib35)]（类似于相关层），或匹配特征[[39](#bib.bib39)、[36](#bib.bib36)]。最常用的CNN包括最大池化层，这些层提供空间变换的不变性。池化层还扩展了CNN的感受野，而不增加参数数量。缺点是网络丧失了细节。为克服这一限制，Park和Lee[[43](#bib.bib43)]引入了像素级金字塔池化层，在比较两个输入补丁时扩大感受野。这种方法产生了比[[17](#bib.bib17)]更准确的匹配成本。
- en: One limitation of correlation layers and convolutional networks that produce
    a single cost value is that they decimate the feature dimension. Thus, they restrict
    the network to only learning relative representations between features, and cannot
    carry absolute feature representations. Instead, a matching feature can be seen
    as a descriptor, or a feature vector, that characterizes the similarity between
    two given patches. The simplest way of computing matching features is by aggregating
    the feature maps produced by the descriptor computation branches of the network [[44](#bib.bib44),
    [39](#bib.bib39)], or by using an encoder that takes the concatenated features
    and produces another volume of matching features [[36](#bib.bib36)]. For instance,
    Huang *et al.* [[36](#bib.bib36)] take the $64\times 64\times 128$ volume, formed
    by features, and process it using three convolutional layers to produce a $64\times
    64\times 4$ volume of matching features. Since the approach computes $n_{d}+1$
    matching features, one for each disparity level ($n_{d}=100$ in [[36](#bib.bib36)]),
    these need to be aggregated into a single matching feature. This is done using
    another encoder-decoder network with skip connections [[36](#bib.bib36)]. Each
    level of the encoder is formed by a stride-2 convolution layer followed by an
    ordinary convolution layer. Each level of the decoder is formed by two convolution
    layers followed by a bilinear upsampling layer. It produces a volume of matching
    features of size $64\times 64\times 800$.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 相关层和卷积网络的一个限制是它们生成单一成本值，这会减少特征维度。因此，它们限制网络只能学习特征之间的相对表示，而不能承载绝对特征表示。相反，匹配特征可以被视为描述符或特征向量，表征两个给定块之间的相似性。计算匹配特征的最简单方法是通过汇总由网络描述符计算分支生成的特征图
    [[44](#bib.bib44), [39](#bib.bib39)]，或使用一个编码器，该编码器接受拼接的特征并生成另一个匹配特征体积 [[36](#bib.bib36)]。例如，Huang
    *et al.* [[36](#bib.bib36)] 采用由特征组成的 $64\times 64\times 128$ 体积，并通过三个卷积层处理，生成一个
    $64\times 64\times 4$ 的匹配特征体积。由于该方法计算了 $n_{d}+1$ 个匹配特征，每个视差级别一个 ($n_{d}=100$ 在
    [[36](#bib.bib36)] 中)，这些需要被聚合成一个单一的匹配特征。这通过另一个具有跳跃连接的编码器-解码器网络 [[36](#bib.bib36)]
    完成。每一层编码器由一个 stride-2 卷积层和一个普通卷积层组成。每一层解码器由两个卷积层和一个双线性上采样层组成。它生成一个大小为 $64\times
    64\times 800$ 的匹配特征体积。
- en: (3) Cost volume aggregation. In general, multiview stereo methods, which take
    $n$ input images, compute $n-1$ cost or feature matching volumes, one for each
    pair $(I_{0},I_{i})$, where $I_{0}$ is the reference image. These need to be aggregated
    into a single cost/feature matching volume before feeding it into the disparity/depth
    calculation module. This has been done either by using (max, average) pooling
    or pooling followed by an encoder [[36](#bib.bib36), [37](#bib.bib37)], which
    produces the final cost/feature matching volume $C$.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 成本体积聚合。一般来说，多视角立体方法，取 $n$ 张输入图像，计算 $n-1$ 个成本或特征匹配体积，每对 $(I_{0},I_{i})$ 计算一个，其中
    $I_{0}$ 是参考图像。这些需要在送入视差/深度计算模块之前被聚合成一个单一的成本/特征匹配体积。这通常通过使用 (max, average) 池化或池化加编码器
    [[36](#bib.bib36), [37](#bib.bib37)] 来完成，从而生成最终的成本/特征匹配体积 $C$。
- en: 3.1.3 Disparity and depth computation
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 视差和深度计算
- en: We have seen so far the various deep learning techniques that have been used
    to estimate the cost volume $C$, *i.e.,* the first term of Equation ([1](#S3.E1
    "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")). The goal now is to estimate
    the depth/disparity map $\tilde{D}$ that minimizes the energy function $E(D)$
    of Equation ([1](#S3.E1 "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A
    Survey on Deep Learning Architectures for Image-based Depth Reconstruction")).
    This is done in two steps; (1) cost volume regularization, and (2) disparity/depth
    estimation from the regularized cost volume.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到各种深度学习技术被用于估计成本体积 $C$，*即* 方程式 ([1](#S3.E1 "在 3.1 流程 ‣ 3 通过立体匹配获得深度
    ‣ 基于图像的深度重建深度学习架构调查")) 的第一项。现在的目标是估计深度/视差图 $\tilde{D}$，使方程式 ([1](#S3.E1 "在 3.1
    流程 ‣ 3 通过立体匹配获得深度 ‣ 基于图像的深度重建深度学习架构调查")) 的能量函数 $E(D)$ 最小化。这分为两个步骤： (1) 成本体积正则化，和
    (2) 从正则化的成本体积中估计视差/深度。
- en: '![Refer to caption](img/e468721ddd00b86c5c949931f3ed0a8e.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e468721ddd00b86c5c949931f3ed0a8e.png)'
- en: 'Figure 4: Cost volume regularization and disparity/depth map estimation.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：成本体积正则化和视差/深度图估计。
- en: 3.1.3.1 Cost volume regularization
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.3.1 成本体积正则化
- en: Once ta raw cost volume is estimated, one can estimate disparity/depth by dropping
    the smoothness term of Equation ([1](#S3.E1 "In 3.1 The pipeline ‣ 3 Depth by
    stereo matching ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction")) and taking the argmin, the softargmin, or the subpixel MAP approximation
    (see Section [3.1.3.2](#S3.SS1.SSS3.P2 "3.1.3.2 Disparity/depth estimation ‣ 3.1.3
    Disparity and depth computation ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")).
    In general, however, the raw cost volume computed from image features could be
    noise-contaminated (*e.g.,* due to the existence of non-Lambertian surfaces, object
    occlusions, and repetitive patterns). Thus, the estimated depth maps can be noisy.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦估计出原始成本体积，就可以通过去掉方程 ([1](#S3.E1 "在 3.1 流程 ‣ 3 通过立体匹配获取深度 ‣ 深度学习架构在图像深度重建中的调查"))
    的平滑度项并采用 argmin、softargmin 或亚像素 MAP 近似（见第 [3.1.3.2](#S3.SS1.SSS3.P2 "3.1.3.2 差异/深度估计
    ‣ 3.1.3 差异和深度计算 ‣ 3.1 流程 ‣ 3 通过立体匹配获取深度 ‣ 深度学习架构在图像深度重建中的调查") 节）来估计差异/深度。然而，一般来说，由图像特征计算得到的原始成本体积可能会受到噪声污染（*例如*，由于非兰伯特表面、物体遮挡和重复模式的存在）。因此，估计的深度图可能会很嘈杂。
- en: Several deep learning-based regularization techniques have been proposed to
    estimate accurate depth maps from the cost volume, see Figure [4](#S3.F4 "Figure
    4 ‣ 3.1.3 Disparity and depth computation ‣ 3.1 The pipeline ‣ 3 Depth by stereo
    matching ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    for an illustration of the taxonomy. Their input can be the cost volume [[14](#bib.bib14),
    [27](#bib.bib27), [39](#bib.bib39), [29](#bib.bib29), [37](#bib.bib37), [38](#bib.bib38),
    [28](#bib.bib28), [36](#bib.bib36)], the cost volume concatenated with the features
    of the reference image [[12](#bib.bib12), [26](#bib.bib26)] and/or with semantic
    features such as the segmentation mask [[25](#bib.bib25)] or the edge map [[31](#bib.bib31)].
    The produced volume is then processed with either an encoder-decoder network with
    skip connections [[39](#bib.bib39), [29](#bib.bib29), [37](#bib.bib37), [38](#bib.bib38),
    [28](#bib.bib28), [12](#bib.bib12), [26](#bib.bib26), [25](#bib.bib25), [31](#bib.bib31)],
    or just an encoder [[14](#bib.bib14), [27](#bib.bib27), [36](#bib.bib36)], to
    produce either a regularized cost volume [[39](#bib.bib39), [29](#bib.bib29),
    [37](#bib.bib37), [38](#bib.bib38), [28](#bib.bib28), [14](#bib.bib14), [27](#bib.bib27),
    [36](#bib.bib36)], or directly the disparity/depth map [[12](#bib.bib12), [25](#bib.bib25),
    [31](#bib.bib31), [26](#bib.bib26)]. In the former case, the regularized volume
    is processed using argmin, softargmin, or subpixel MAP approximation (Section [3.1.3.2](#S3.SS1.SSS3.P2
    "3.1.3.2 Disparity/depth estimation ‣ 3.1.3 Disparity and depth computation ‣
    3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")) to produce the final disparity/depth map.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 几种基于深度学习的正则化技术已经被提出，用于从成本体积中估计准确的深度图，见图 [4](#S3.F4 "图 4 ‣ 3.1.3 差异和深度计算 ‣ 3.1
    流程 ‣ 3 通过立体匹配获取深度 ‣ 深度学习架构在图像深度重建中的调查") 了解分类法的示例。它们的输入可以是成本体积 [[14](#bib.bib14),
    [27](#bib.bib27), [39](#bib.bib39), [29](#bib.bib29), [37](#bib.bib37), [38](#bib.bib38),
    [28](#bib.bib28), [36](#bib.bib36)]，也可以是与参考图像特征 [[12](#bib.bib12), [26](#bib.bib26)]
    以及/或语义特征（例如分割掩码 [[25](#bib.bib25)] 或边缘图 [[31](#bib.bib31)] 连接的成本体积。然后，通过带有跳跃连接的编码器-解码器网络 [[39](#bib.bib39),
    [29](#bib.bib29), [37](#bib.bib37), [38](#bib.bib38), [28](#bib.bib28), [12](#bib.bib12),
    [26](#bib.bib26), [25](#bib.bib25), [31](#bib.bib31)]，或者仅使用编码器 [[14](#bib.bib14),
    [27](#bib.bib27), [36](#bib.bib36)] 来处理生成的体积，以产生正则化的成本体积 [[39](#bib.bib39), [29](#bib.bib29),
    [37](#bib.bib37), [38](#bib.bib38), [28](#bib.bib28), [14](#bib.bib14), [27](#bib.bib27),
    [36](#bib.bib36)]，或者直接生成差异/深度图 [[12](#bib.bib12), [25](#bib.bib25), [31](#bib.bib31),
    [26](#bib.bib26)]。在前一种情况下，正则化的体积使用 argmin、softargmin 或亚像素 MAP 近似（见第 [3.1.3.2](#S3.SS1.SSS3.P2
    "3.1.3.2 差异/深度估计 ‣ 3.1.3 差异和深度计算 ‣ 3.1 流程 ‣ 3 通过立体匹配获取深度 ‣ 深度学习架构在图像深度重建中的调查")
    节）来生成最终的差异/深度图。
- en: Note that some methods adopt an MRF-based stereo framework for cost volume regularization [[20](#bib.bib20),
    [17](#bib.bib17), [19](#bib.bib19)]. In these methods, the initial cost volume
    $C$ is fed to a global [[16](#bib.bib16)] or a semi-global [[45](#bib.bib45)]
    matcher to compute the disparity map. Semi-global methods define the smoothness
    term as
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，某些方法采用基于 MRF 的立体框架进行成本体积正则化 [[20](#bib.bib20), [17](#bib.bib17), [19](#bib.bib19)]。在这些方法中，初始成本体积
    $C$ 被输入到全局 [[16](#bib.bib16)] 或半全局 [[45](#bib.bib45)] 匹配器中以计算差异图。半全局方法定义平滑度项为
- en: '|  | $\small{E_{s}(0pt_{x},0pt_{y})=\alpha_{1}\delta(&#124;0pt_{x}-0pt_{y}&#124;=1)+\alpha_{2}\delta(&#124;0pt_{x}-0pt_{y}&#124;>1),}$
    |  | (3) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{E_{s}(0pt_{x},0pt_{y})=\alpha_{1}\delta(&#124;0pt_{x}-0pt_{y}&#124;=1)+\alpha_{2}\delta(&#124;0pt_{x}-0pt_{y}&#124;>1),}$
    |  | (3) |'
- en: where $\alpha_{1}$ and $\alpha_{2}$ are positive weights chosen such that $\alpha_{2}>\alpha_{1}$.
    Instead of manually setting these two parameters, Seki *et al.* [[30](#bib.bib30)]
    proposed SGM-Net, a neural network trained to provide these parameters at each
    image pixel. They obtained better penalties than hand-tuned methods as in [[17](#bib.bib17)].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha_{1}$ 和 $\alpha_{2}$ 是正权重，选择使得 $\alpha_{2}>\alpha_{1}$。Seki *et al.*
    [[30](#bib.bib30)] 提出了 SGM-Net，这是一种神经网络，训练来为每个图像像素提供这些参数。他们获得了比手动调整方法 [[17](#bib.bib17)]
    更好的惩罚效果。
- en: 3.1.3.2 Disparity/depth estimation
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.3.2 差异/深度估计
- en: 'The simplest way to estimate disparity/depth from the (regularized) cost volume
    $C$ is by using the pixel-wise argmin, *i.e.,* $0pt_{x}=\arg\min_{0pt}C(x,0pt)$
    (or equivalently $\arg\max$ if the volume $C$ encodes likelihood) [[36](#bib.bib36)].
    However, the agrmin/argmax operatir is unable to produce sub-pixel accuracy and
    cannot be trained with back-propagation due to its non-differentiability. Another
    approach is to process the cost volume using a layer of per-pixel softmin, also
    called soft argmin (or equivalently softmax), over disparity/depth [[14](#bib.bib14),
    [39](#bib.bib39), [27](#bib.bib27)]:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 从（正则化）成本体积 $C$ 中估计差异/深度的最简单方法是使用逐像素的 argmin，*即，* $0pt_{x}=\arg\min_{0pt}C(x,0pt)$（或如果体积
    $C$ 编码了可能性，则等效于 $\arg\max$） [[36](#bib.bib36)]。然而，argmin/argmax 操作符无法产生子像素精度，并且由于其不可微性，无法通过反向传播进行训练。另一种方法是使用逐像素的
    softmin 层处理成本体积，也称为 soft argmin（或等效的 softmax），对差异/深度进行处理 [[14](#bib.bib14), [39](#bib.bib39),
    [27](#bib.bib27)]：
- en: '|  | $d^{*}=\frac{1}{\sum_{j=0}^{n_{d}}e^{-C(x,j)}}\sum_{d=0}^{n_{d}}d\times
    e^{-C(x,d)}.$ |  | (4) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $d^{*}=\frac{1}{\sum_{j=0}^{n_{d}}e^{-C(x,j)}}\sum_{d=0}^{n_{d}}d\times
    e^{-C(x,d)}.$ |  | (4) |'
- en: 'It approximates sub-pixel MAP solution when the distribution is unimodal and
    symmetric [[32](#bib.bib32)]. When this assumption is not fulfilled, the softargmin
    blends the modes and may produce a solution that is far from all the modes. Also,
    the network only learns for the disparity range used during training. If the disparity
    range changes at runtime, then the network needs to be re-trained. To address
    these issues, Tulyakov *et al.* [[32](#bib.bib32)] introduced the sub-pixel MAP
    approximation that computes a weighted mean around the disparity with maximum
    posterior probability as:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当分布为单峰且对称时，它逼近子像素 MAP 解 [[32](#bib.bib32)]。当这一假设不成立时，softargmin 会混合模式，可能产生远离所有模式的解。此外，网络只对训练过程中使用的差异范围进行学习。如果在运行时差异范围发生变化，则需要重新训练网络。为了解决这些问题，Tulyakov
    *et al.* [[32](#bib.bib32)] 引入了子像素 MAP 近似，通过计算最大后验概率的差异的加权均值来实现：
- en: '|  | $d^{*}=\sum_{d:&#124;\hat{d}-d&#124;\leq\delta}d\cdot\sigma(C(x,d)),$
    |  | (5) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $d^{*}=\sum_{d:&#124;\hat{d}-d&#124;\leq\delta}d\cdot\sigma(C(x,d)),$
    |  | (5) |'
- en: where $\delta$ is a meta parameter set to $4$ in [[32](#bib.bib32)], and $\displaystyle\hat{d}=\arg\max_{d}C(x,d)$.
    Note that, in Tulyakov *et al.* [[32](#bib.bib32)], the sub-pixel MAP is only
    used for inference.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\delta$ 是一个元参数，在 [[32](#bib.bib32)] 中设置为 $4$，而 $\displaystyle\hat{d}=\arg\max_{d}C(x,d)$。请注意，在
    Tulyakov *et al.* [[32](#bib.bib32)] 中，子像素 MAP 仅用于推断。
- en: 3.1.4 Refinement
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 精细化
- en: 'TABLE III: Taxonomy of disparity/depth refinement techniques. ”reco error”:
    reconstruction error. ”CSPN”: Convolutional Spatial Propagation Networks.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：差异/深度精细化技术的分类。 “reco error”：重建误差。 “CSPN”：卷积空间传播网络。
- en: '|   |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: '| Traditional methods | Deep learning-based methods |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 传统方法 | 基于深度学习的方法 |'
- en: '|  | Input | Approach | Other cues |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入 | 方法 | 其他提示 |'
- en: '|  |  | Bottom-up | Top-down | Guided |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 自下而上 | 自上而下 | 引导 |  |'
- en: '|   |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: '| Variational [[12](#bib.bib12)] | Raw depth | Split and merge [[46](#bib.bib46)]
    | Decoder [[10](#bib.bib10)] | Detect - Replace - Refine [[47](#bib.bib47)] |
    Joint depth and normal [[48](#bib.bib48)] |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 变分 [[12](#bib.bib12)] | 原始深度 | 分割与合并 [[46](#bib.bib46)] | 解码器 [[10](#bib.bib10)]
    | 检测 - 替换 - 精细化 [[47](#bib.bib47)] | 结合深度与法线 [[48](#bib.bib48)] |'
- en: '| Fully-connected CRF [[36](#bib.bib36)] | Depth + Ref. Image [[37](#bib.bib37)]
    | Sliding window [[11](#bib.bib11)] | Encoder + decoder [[15](#bib.bib15), [27](#bib.bib27),
    [49](#bib.bib49)] | Depth-balanced loss [[46](#bib.bib46)] | Left-Right consistency [[33](#bib.bib33)]
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 完全连接 CRF [[36](#bib.bib36)] | 深度 + 参考图像 [[37](#bib.bib37)] | 滑动窗口 [[11](#bib.bib11)]
    | 编码器 + 解码器 [[15](#bib.bib15), [27](#bib.bib27), [49](#bib.bib49)] | 深度平衡损失 [[46](#bib.bib46)]
    | 左右一致性 [[33](#bib.bib33)] |'
- en: '| Hierarchical CRF [[1](#bib.bib1)] | Depth + CV + Ref. Image + rec. error [[26](#bib.bib26)]
    | Diffusion using CSPN [[50](#bib.bib50)] | Encoder + decoder with residual learning
    [[23](#bib.bib23), [37](#bib.bib37), [51](#bib.bib51), [26](#bib.bib26), [49](#bib.bib49)]
    |  |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 层次 CRF [[1](#bib.bib1)] | 深度 + CV + 参考图像 + 重建误差 [[26](#bib.bib26)] | 使用 CSPN
    的扩散 [[50](#bib.bib50)] | 带有残差学习的编码器 + 解码器 [[23](#bib.bib23), [37](#bib.bib37),
    [51](#bib.bib51), [26](#bib.bib26), [49](#bib.bib49)] |  |  |'
- en: '| Depth propagation [[20](#bib.bib20)] | Depth + Rewarped right image [[52](#bib.bib52)]
    | Diffusion using recurrent convolutional operation [[53](#bib.bib53)] | Progressive
    upsampling [[51](#bib.bib51)] |  |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 深度传播 [[20](#bib.bib20)] | 深度 + 重新变形的右图像 [[52](#bib.bib52)] | 使用递归卷积操作的扩散
    [[53](#bib.bib53)] | 渐进上采样 [[51](#bib.bib51)] |  |  |'
- en: '| CRF energy minimized with CNN [[4](#bib.bib4)] | Depth + Learned features [[10](#bib.bib10)].
    |  |  |  |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 使用 CNN 的 CRF 能量最小化 [[4](#bib.bib4)] | 深度 + 学习特征 [[10](#bib.bib10)] |  |  |  |  |'
- en: '|   |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: In general, the predicted disparity/depth maps are of low resolution, miss fine
    details, and may suffer from over-smoothing especially at object boundaries. Some
    methods also output incomplete and/or sparse maps. Deep-learning networks that
    directly predict high resolution and high quality maps would require a large number
    of parameters and thus are usually difficult to train. Instead, an additional
    refinement block is added to the pipeline. Its goal is to (1) improve the resolution
    of the estimated disparity/depth map, (2) refine the reconstruction of the fine
    details, and (3) perform depth/disparity completion. Such refinement block can
    be implemented using traditional approaches. For instance, Dosovitskiy *et al.* [[12](#bib.bib12)]
    use the variational approach from [[54](#bib.bib54)]. Huang *et al.* [[36](#bib.bib36)]
    apply the Fully-Connected Conditional Random Field (DenseCRF) of [[55](#bib.bib55)]
    to the predicted raw disparities. Li *et al.* [[1](#bib.bib1)] refine the predicted
    depth (or surface normals) from the super-pixel level to pixel level using a hierarchical
    Conditional Random Field (CRF). The use of DenseCRF or hierarchical CRF encourages
    the pixels that are spatially close and with similar colors to have closer disparity
    predictions. Also, this step removes unreliable matches via left-right check.
    Chen *et al.* [[20](#bib.bib20)] compute the final disparity map from the raw
    one, after removing unreliable matches, by propagating reliable disparities to
    non-reliable areas [[56](#bib.bib56)]. Note that the use of CRF for depth estimation
    has been also explored by Liu *et al.* [[4](#bib.bib4)]. However, unlike Li *et
    al.* [[1](#bib.bib1)], Liu *et al.* [[4](#bib.bib4)] used a CNN to minimize the
    CRF energy.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，预测的视差/深度图分辨率较低，缺少细节，并且在物体边界处可能会出现过度平滑的现象。一些方法还会输出不完整和/或稀疏的图。直接预测高分辨率和高质量图的深度学习网络需要大量的参数，因此通常训练起来比较困难。相反，通常会在管道中添加一个额外的细化块。其目标是（1）提高估计视差/深度图的分辨率，（2）细化细节的重建，以及（3）执行深度/视差补全。这种细化块可以使用传统方法来实现。例如，Dosovitskiy
    *等人* [[12](#bib.bib12)] 使用来自 [[54](#bib.bib54)] 的变分方法。Huang *等人* [[36](#bib.bib36)]
    将 [[55](#bib.bib55)] 的全连接条件随机场（DenseCRF）应用于预测的原始视差。Li *等人* [[1](#bib.bib1)] 使用层次条件随机场（CRF）将预测的深度（或表面法线）从超像素级别细化到像素级别。使用
    DenseCRF 或层次 CRF 可以使空间上接近且颜色相似的像素具有更接近的视差预测。此外，这一步还通过左-右检查去除不可靠的匹配。Chen *等人* [[20](#bib.bib20)]
    在去除不可靠匹配后，通过将可靠的视差传播到不可靠区域来计算最终的视差图 [[56](#bib.bib56)]。值得注意的是，Liu *等人* [[4](#bib.bib4)]
    也探索了 CRF 在深度估计中的应用。然而，与 Li *等人* [[1](#bib.bib1)] 不同的是，Liu *等人* [[4](#bib.bib4)]
    使用 CNN 来最小化 CRF 能量。
- en: 'In this section, we will look at how the refinement block has been implemented
    using deep learning, see Table [III](#S3.T3 "TABLE III ‣ 3.1.4 Refinement ‣ 3.1
    The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") for a taxonomy of these methods. In general,
    the input to the refinement module can be: (1) the estimated depth/disparity map,
    (2) the estimated depth/disparity map concatenated with the reference image scaled
    to the resolution of the estimated depth/disparity map [[37](#bib.bib37)], (3)
    the initially-estimated disparity map, the cost volume, and the reconstruction
    error, which is calculated as the absolute difference between the multi-scale
    fusion features of the left image and the multi-scale fusion features of the right
    image but back-warped using the initial disparity map to the left image [[26](#bib.bib26)],
    (4) the raw disparity/depth map, and the right image but warped into the view
    of the left image using the estimated initial disparity map [[52](#bib.bib52)],
    and (5) the estimated depth/disparity map concatenated with the feature map of
    the reference image, *e.g.,* the output of a first convolutional layer [[10](#bib.bib10)].'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看一下如何使用深度学习实现了细化块，详情请见表[III](#S3.T3 "表III  ‣  3.1.4 细化  ‣  3.1 流水线  ‣  3
    通过立体匹配获得深度  ‣  基于图像的深度重建的深度学习架构概述")，其中对这些方法进行了分类。总的来说，细化模块的输入可以是：(1) 估计的深度/视差图，(2)
    估计的深度/视差图与按照估计的深度/视差图分辨率缩放的参考图像的拼接[[37](#bib.bib37)], (3) 最初估计的视差图，代价体积和重建误差，重建误差计算为左图像和右图像的多尺度融合特征的绝对差，但使用初始视差图反向变换到左图像[[26](#bib.bib26)],
    (4) 原始视差/深度图和通过使用估计的初始视差图将右图像向左图像视图翘曲[[52](#bib.bib52)]，和 (5) 估计的深度/视差图与参考图像的特征图拼接，*例如*，第一个卷积层的输出[[10](#bib.bib10)]。
- en: Note that refinement can be hierarchical by cascading several refinement modules [[23](#bib.bib23),
    [49](#bib.bib49)].
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过级联多个细化模块[[23](#bib.bib23), [49](#bib.bib49)]，细化可以是分层的。
- en: 3.1.4.1 Bottom-up approaches
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.4.1 自底向上方法
- en: A bottom-up network operates in a sliding window-like approach. It takes small
    patches and estimates the refined depth at the center of the patch [[11](#bib.bib11)].
    Lee *et al.* [[46](#bib.bib46)] follow a split-and-merge approach. The input image
    is split into regions, and a depth is estimated for each region. The estimates
    are then merged using a fusion network, which operates in the Fourier domain so
    that depth maps with different cropping ratios can be handled. The rational is
    that inferring accurate depth at the desired resolution would require large networks
    with a large number of parameters to estimate. By making the network focus on
    small regions, fine details can be recovered with less parameters. However, obtaining
    the entire refined map will require multiple forward passes, which is not suitable
    for realtime applications.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 自底向上网络以类似滑动窗口的方式运作，它获取小补丁，然后估计补丁中心的细化深度[[11](#bib.bib11)]。Lee等人[[46](#bib.bib46)]采用分离和合并方法。将输入图像分割成区域，并为每个区域估计深度。然后使用融合网络将这些估计值合并，该网络在傅立叶域中运作，以便处理具有不同裁剪比例的深度图。其原则是，要在所需分辨率上推断精确的深度，需要具有大量参数的大网络。通过使网络关注小区域，可以以更少的参数恢复细节。然而，获取整个细化地图将需要多次正向传递，这对于实时应用来说并不合适。
- en: Another bottom-up refinement strategy is based on diffusion processes. The idea
    is to start with an incomplete depth map and use anisotropic diffusion to propagate
    the known depth to the regions where depth is missing. Convolutional Spatial Propagation
    Networks (CSPN) [[50](#bib.bib50)], which implement an anisotropic diffusion process,
    are particularly suitable this task. They take as input the original image and
    a sparse depth map, which can be the output of a depth estimation network, and
    predict, using a deep CNN, the diffusion tensor. This is then applied to the initial
    map to obtain the refined one. Cheng *et al.* [[53](#bib.bib53)] used this approach
    in their proposed refinement module. It takes an initial depth estimate and performs
    linear propagation, in which the propagation is performed with a manner of recurrent
    convolutional operation, and the affinity among neighboring pixels is learned
    through a deep CNN.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种自下而上的精细化策略基于扩散过程。其思想是从不完整的深度图开始，使用各向异性扩散将已知深度传播到深度缺失的区域。卷积空间传播网络（CSPN） [[50](#bib.bib50)]，实现了各向异性扩散过程，非常适合这个任务。它们以原始图像和稀疏深度图作为输入，稀疏深度图可以是深度估计网络的输出，并通过深度CNN预测扩散张量。然后将其应用于初始图，以获得精细化图。Cheng
    *et al.* [[53](#bib.bib53)] 在他们提出的精细化模块中使用了这种方法。它以初始深度估计为基础，执行线性传播，其中传播通过递归卷积操作进行，并通过深度CNN学习邻近像素之间的相似性。
- en: 3.1.4.2 Top-down approaches
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.4.2 自顶向下的方法
- en: Another approach is to use a top-down network that processes the entire raw
    disparity/depth map. It can be implemented as (1) a decoder, which consists of
    unpooling units to extend the resolution of its input, as opposed to pooling,
    and convolution layers [[10](#bib.bib10)], or with (2) a encoder-decoder network [[15](#bib.bib15)].
    In the latter case, the encoder is to map the input into a latent space. The decoder
    then predicts the high resolution map from the latent variable. These networks
    also use skip connections from the contracting part to the expanding part so that
    fine details can be preserved. To avoid the checkboard artifacts produced by the
    deconvolutions and upconvolutions [[57](#bib.bib57), [27](#bib.bib27), [49](#bib.bib49)],
    several papers first upsample the initial map, *e.g.,* using bilinear upsampling,
    and then apply convolutions [[27](#bib.bib27), [49](#bib.bib49)].
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用一个自顶向下的网络处理整个原始视差/深度图。它可以实现为（1）一个解码器，由反池化单元组成，以扩展其输入的分辨率，与池化相对，并包含卷积层 [[10](#bib.bib10)]，或者（2）一个编码-解码网络 [[15](#bib.bib15)]。在后一种情况下，编码器将输入映射到潜在空间。解码器然后从潜在变量中预测高分辨率图。这些网络还使用从收缩部分到扩展部分的跳跃连接，以便保留细节。为了避免由反卷积和上卷积产生的棋盘格伪影 [[57](#bib.bib57),
    [27](#bib.bib27), [49](#bib.bib49)]，一些论文首先对初始图进行上采样，例如，使用双线性上采样，然后应用卷积 [[27](#bib.bib27),
    [49](#bib.bib49)]。
- en: These architectures can be used to directly predict the high resolution maps
    but also to predict the residuals [[37](#bib.bib37), [26](#bib.bib26), [49](#bib.bib49)].
    As opposed to directly learning the refined disparity map, residual learning provides
    a more effective refinement. In this approach, the estimated map and the resized
    reference image are concatenated and used as a 4-channel input to a refinement
    network, which learns the disparity/depth residual. The estimated residual is
    then added to the originally estimated map to generate the refined map.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构可以直接预测高分辨率图，也可以预测残差 [[37](#bib.bib37), [26](#bib.bib26), [49](#bib.bib49)]。与直接学习精细化视差图相对，残差学习提供了更有效的精细化。在这种方法中，将估计的图和调整大小后的参考图连接起来，作为4通道输入送入一个精细化网络，该网络学习视差/深度残差。然后将估计的残差加到原始估计图中以生成精细化图。
- en: Pang *et al.* [[23](#bib.bib23)] refine the raW disparity map using a cascade
    of two CNNs. The first stage advances the DispNet of [[13](#bib.bib13)] by adding
    extra up-convolution modules, leading to disparity images with more details. The
    second stage, initialized by the output of the first stage, explicitly rectifies
    the disparity; it couples with the first-stage and generates residual signals
    across multiple scales. The summation of the outputs from the two stages gives
    the final disparity.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Pang *et al.* [[23](#bib.bib23)] 使用两个CNN的级联来精细化原始视差图。第一阶段通过添加额外的上卷积模块来改进 [[13](#bib.bib13)]
    的DispNet，生成更多细节的视差图。第二阶段由第一阶段的输出初始化，显式地修正视差；它与第一阶段耦合，并在多个尺度上生成残差信号。两个阶段的输出求和即为最终视差。
- en: Jeon and Lee [[51](#bib.bib51)] proposed a deep Laplacian Pyramid Network to
    spatially varying noise and holes. By considering local and global contexts, the
    network progressively reduces the noise and fills the holes from coarse to fine
    scales. It first predicts, using residual learning, a clean complete depth image
    at a coarse scale (quarter of the original resolution). The prediction is then
    progressively upsampled through the pyramid to predict the half and original sized
    clean depth image. The network is trained with 3D supervision using a loss that
    is a combination of a data loss and a structure-preserving loss. The data loss
    is a weighted sum of $L_{1}$ distance between the ground-truth depth and the estimated
    depth, the $L_{1}$ distance between the gradient of the ground-truth depth and
    the estimated depth, and the $L_{1}$ distance between the normal vectors of the
    estimated and ground-truth depths. The structure-preserving loss is gradient-based
    to preserve the original structures and discontinuities. It is defined as the
    $L_{2}$ distance between the maximum gradient around a pixel in the ground-truth
    depth map and the maximum gradient around that pixel in the estimate depth map.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Jeon 和 Lee [[51](#bib.bib51)] 提出了一个深度拉普拉斯金字塔网络，以处理空间变化的噪声和孔洞。通过考虑局部和全局上下文，网络逐步减少噪声并填补孔洞，从粗到细进行处理。它首先使用残差学习预测一个粗略尺度（原始分辨率的四分之一）的干净完整深度图。然后，该预测通过金字塔逐步上采样，以预测半尺寸和原始尺寸的干净深度图。网络使用3D监督进行训练，损失函数是数据损失和结构保持损失的组合。数据损失是基于地面真实深度和估计深度之间的
    $L_{1}$ 距离、地面真实深度梯度和估计深度梯度之间的 $L_{1}$ 距离以及估计深度和地面真实深度的法向量之间的 $L_{1}$ 距离的加权和。结构保持损失是基于梯度的，以保持原始结构和不连续性。它定义为地面真实深度图中像素周围最大梯度与估计深度图中该像素周围最大梯度之间的
    $L_{2}$ 距离。
- en: 3.1.4.3 Guided refinement
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.4.3 指导性精细化
- en: 'Gidaris and Komodakis [[47](#bib.bib47)] argue that the approaches that predict
    either new depth estimates or residual corrections are sub-optimal. Instead, they
    propose a generic CNN architecture that decomposes the refinement task into three
    steps: (1) detecting the incorrect initial estimates, (2) replacing the incorrect
    labels with new ones, and (3) refining the renewed labels by predicting residual
    corrections with respect to them. Since the approach is generic, it can be used
    to refine the raw depth map produced by any other method, *e.g.,* [[19](#bib.bib19)].'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Gidaris 和 Komodakis [[47](#bib.bib47)] 认为预测新深度估计或残差修正的方法是不够优化的。相反，他们提出了一种通用的
    CNN 架构，将精细化任务分解为三个步骤：（1）检测不正确的初始估计，（2）用新的标签替换不正确的标签，以及（3）通过预测相对于这些标签的残差修正来精细化更新的标签。由于该方法是通用的，因此可以用来精细化任何其他方法生成的原始深度图，例如
    [[19](#bib.bib19)]。
- en: In general, the predictions of the baseline backbone, which is composed of an
    encoder-decoder, are coarse and smooth due to the lack of depth details. To overcome
    this, Zhang *et al.* [[58](#bib.bib58)] introduced a hierarchical guidance strategy,
    which guides the estimation process to predict fine-grained details. They perform
    this by attaching refinement networks (composed of 5 conv-residual blocks and
    several following $1\times 1$ convolution layers) to the last three layers of
    the encoder (one per layer). Its role is to predict predict depth maps at these
    levels. The features learned by these refinement networks are used as input to
    their corresponding layers on the decoder part of the backbone network. This is
    similar to using skip connection. However, instead of feeding directly the features
    of the encoder, these are further processed to predict depth map at that level.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，由编码器-解码器组成的基线骨干网络的预测由于缺乏深度细节而显得粗糙和平滑。为了克服这一点，张*等* [[58](#bib.bib58)] 引入了一种分层引导策略，该策略指导估计过程以预测细粒度的细节。他们通过将精细化网络（由5个卷积残差块和若干个
    $1\times 1$ 卷积层组成）附加到编码器的最后三层（每层一个）来实现这一点。其作用是在这些层次上预测深度图。这些精细化网络学到的特征被用作其在骨干网络解码器部分相应层的输入。这类似于使用跳跃连接。然而，与直接输入编码器特征不同，这些特征经过进一步处理，以在该层次上预测深度图。
- en: Finally, to handle equally close and far depths, Li *et al.* [[46](#bib.bib46)]
    introduced depth-balanced Euclidean loss to reliably train the network on a wide
    range of depths.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了处理同样接近和远离的深度，李*等* [[46](#bib.bib46)] 引入了深度平衡欧几里得损失，以可靠地训练网络在广泛的深度范围内。
- en: 3.1.4.4 Leveraging other cues
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.4.4 利用其他线索
- en: 'Qi *et al.* [[48](#bib.bib48)] proposed a mechanism that uses the depth map
    to refine the quality of the normal estimates, and the normal map to refine the
    quality of the depth estimates. This is done using a two-stream CNN, one for estimating
    an initial depth map and another for estimating an initial normal map. Then, it
    uses another two-stream networks: a depth-to-normal network and a normal-to-depth
    network. The former is used to refine the normal map using the initial depth map.
    The latter is used to refine the depth map using the estimated normal map.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Qi *等人* [[48](#bib.bib48)] 提出了一个机制，利用深度图细化法线估计的质量，以及利用法线图细化深度估计的质量。这是通过一个双流CNN来完成的，一个用于估计初始深度图，另一个用于估计初始法线图。然后，使用另一个双流网络：深度到法线网络和法线到深度网络。前者用于使用初始深度图细化法线图，后者用于使用估计的法线图细化深度图。
- en: •
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The depth-to-normal network first takes the initial depth map and generates
    a rough normal map using PCA analysis. This is then fed into a 3-layer CNN, which
    estimates the residual. The residual is then added to the rough normal map, concatenated
    with the initial raw normal map, and further processed with one convolutional
    layer to output the refined normal map.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度到法线网络首先接受初始深度图，并使用PCA分析生成粗略的法线图。然后将其输入到一个三层CNN中，估计残差。残差然后被加到粗略的法线图上，与初始原始法线图连接，并通过一个卷积层进一步处理，输出细化的法线图。
- en: •
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The normal-to-depth network uses kernel regression process, which takes the
    initial normal and depth maps, and regresses the refined depth map.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 法线到深度网络使用核回归过程，它接受初始法线和深度图，并回归出细化的深度图。
- en: Instead of estimating a single depth map from the reference image, one can estimate
    multiple depth maps, one per input image, check the consistency of the estimates,
    and use the consistency maps to (recursively) refine the estimates. In the case
    of stereo matching, this process is referred to as the left-right consistency
    check, which traditionally was an isolated post-processing step and heavily hand-crafted.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 与其从参考图像估计单一深度图，不如估计多个深度图，每个输入图像一个，检查估计的一致性，并使用一致性图来（递归地）细化估计。在立体匹配的情况下，这个过程被称为左右一致性检查，传统上是一个孤立的后处理步骤且极其手工制作。
- en: The standard approach for implementing the left-right consistency check is as
    follows;
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 实施左右一致性检查的标准方法如下；
- en: •
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compute two disparity maps $D_{l}$ and $D_{r}$, one for the left image and another
    for the right image.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算两个视差图 $D_{l}$ 和 $D_{r}$，分别用于左图像和右图像。
- en: •
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reproject the right disparity map onto the coordinates of the left image, obtaining
    $\tilde{D}_{r}$.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将右侧视差图重投影到左图像的坐标上，获得 $\tilde{D}_{r}$。
- en: •
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compute the error or confidence map indicating whether the estimated disparity
    is correct or not.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算误差或置信度图，指示估计的视差是否正确。
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Finally, use the computed confidence map to refine the disparity estimate.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，使用计算得到的置信度图来细化视差估计。
- en: A simple way of computing the confidence map is by taking pixel-wise difference.
    Seki *et al.* [[59](#bib.bib59)], on the other hand, used a CNN trained in a classifier
    manner. It outputs a label per pixel indicating whether the estimated disparity
    is correct or not. This confidence map is then incorporated into a Semi-Global
    Matching (SGM) for dense disparity estimation.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 计算置信度图的简单方法是逐像素差异。另一方面，Seki *等人* [[59](#bib.bib59)] 使用了以分类器方式训练的CNN。它为每个像素输出一个标签，指示估计的视差是否正确。然后将这个置信度图纳入半全局匹配（SGM）中，以进行密集视差估计。
- en: Jie *et al.* [[33](#bib.bib33)] perform left-right consistency check jointly
    with disparity estimation, using a Left-Right Comparative Recurrent (LRCR) model.
    It consists of two parallely stacked convolutional LSTM networks. The left network
    takes the cost volume and generates a disparity map for the left image. Similarly,
    the right network generates, independently of the left network, a disparity map
    for the right image. The two maps are converted to the opposite coordinates (using
    the known camera parameters) for comparison with each other. Such comparison produces
    two error maps, one for the left disparity and another for the right disparity.
    Finally, the error map for each image is concatenated with its associated cost
    volume and used as input at the next step to the convolutional LSTM. This will
    allow the LRCR model to selectively focus on the left-right mismatched regions
    at the next step.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Jie *等*[[33](#bib.bib33)] 通过左-右一致性检查与视差估计共同进行，使用了左-右比较递归（LRCR）模型。它由两个平行堆叠的卷积LSTM网络组成。左侧网络接收代价体积并生成左图的视差图。类似地，右侧网络独立于左侧网络生成右图的视差图。这两个视差图被转换到对方坐标（使用已知的相机参数）进行比较。这样的比较产生两个误差图，一个用于左侧视差，另一个用于右侧视差。最后，每张图像的误差图与其相关的代价体积拼接在一起，并作为输入用于下一步的卷积LSTM。这将允许LRCR模型在下一步中选择性地关注左右不匹配的区域。
- en: 3.2 Stereo matching networks
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 立体匹配网络
- en: In the previous section, we have discussed how the different blocks of the stereo
    matching pipeline have been implemented using deep learning. This section discusses
    how different state-of-the-art techniques used these blocks and put them together
    to solve the pairwise stereo matching-based depth reconstruction problem.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讨论了如何使用深度学习实现立体匹配管道的不同块。本节讨论了不同的最先进技术如何使用这些块并将它们组合在一起以解决基于配对立体匹配的深度重建问题。
- en: 3.2.1 Early methods
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 早期方法
- en: 'Early methods, *e.g.,* [[17](#bib.bib17), [20](#bib.bib20), [34](#bib.bib34),
    [22](#bib.bib22), [19](#bib.bib19), [60](#bib.bib60)], replace the hand-crafted
    features and similarity computation with deep learning architectures. The basic
    architecture is composed of a stack of the modules described in Section [3.1](#S3.SS1
    "3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction"). The feature extraction module is implemented
    as a multi-branch network, with shared weights. Each branch computes features
    from its input. These are then matched using:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的方法，例如[[17](#bib.bib17), [20](#bib.bib20), [34](#bib.bib34), [22](#bib.bib22),
    [19](#bib.bib19), [60](#bib.bib60)]，用深度学习架构取代了手工设计的特征和相似性计算。基本架构由第[3.1](#S3.SS1
    "3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")节中描述的模块堆叠组成。特征提取模块实现为多分支网络，具有共享权重。每个分支从其输入中计算特征。这些特征然后使用以下方式进行匹配：
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a fixed correlation layer (implemented as a convolutional layer) [[17](#bib.bib17),
    [19](#bib.bib19)],
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个固定的相关层（实现为卷积层）[[17](#bib.bib17), [19](#bib.bib19)]，
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a fully connected neural network [[22](#bib.bib22), [34](#bib.bib34), [21](#bib.bib21),
    [18](#bib.bib18)], which takes as input the concatenated features of the patches
    from the left and right images and produces a matching score.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个全连接神经网络[[22](#bib.bib22), [34](#bib.bib34), [21](#bib.bib21), [18](#bib.bib18)]，它以左图和右图的拼接特征作为输入，并产生一个匹配分数。
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: convolutional networks composed of convolutional layers followed by ReLU [[35](#bib.bib35)].
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卷积网络由卷积层和后续的ReLU[[35](#bib.bib35)]组成。
- en: Using convolutional and/or fully-connected layers enables the network to learn
    from data the appropriate similarity measure, instead of imposing one at the outset.
    It is more accurate than using a correlation layer but is significantly slower.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积和/或全连接层使网络能够从数据中学习适当的相似性度量，而不是一开始就强加一个。这比使用相关层更准确，但显著更慢。
- en: Note that while Zbontar *et al.* [[17](#bib.bib17), [18](#bib.bib18)] and Han
    *et al.* [[22](#bib.bib22)] use standard convolutional layers in the feature extraction
    block, Shaked and Wolf [[21](#bib.bib21)] add residual blocks with multilevel
    weighted residual connections to facilitate the training of very deep networks.
    It was demonstrated that this architecture outperformed the base network of Zbontar
    *et al.* [[17](#bib.bib17)]. To enable multiscale features, Chen *et al.* [[20](#bib.bib20)]
    replicate twice the feature extraction module and the correlation layer. The two
    instances take patches around the same pixel but of different sizes, and produce
    two matching scores. These are then merged using voting. Chen *et al.*’s approach
    shares some similarities with the central-surround two-stream network of [[34](#bib.bib34)].
    The main difference is that in [[34](#bib.bib34)], the output of the four branches
    of the descriptor computation module is given as input to a top decision network
    for fusion and similarity computation, instead of using voting. Zagoruyko and
    Komodakis [[34](#bib.bib34)] add at the end of each feature computation branch
    a Spatial Pyramid Pooling so that patches of arbitrary sizes can be compared.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，虽然 Zbontar *等人* [[17](#bib.bib17), [18](#bib.bib18)] 和 Han *等人* [[22](#bib.bib22)]
    在特征提取块中使用了标准的卷积层，但 Shaked 和 Wolf [[21](#bib.bib21)] 则通过添加具有多级加权残差连接的残差块来促进非常深网络的训练。研究表明，这种架构的表现优于
    Zbontar *等人* [[17](#bib.bib17)] 的基础网络。为了实现多尺度特征，Chen *等人* [[20](#bib.bib20)] 复制了两次特征提取模块和相关层。这两个实例围绕相同像素但大小不同的补丁进行处理，并生成两个匹配分数。然后通过投票将它们合并。Chen
    *等人* 的方法与 [[34](#bib.bib34)] 的中心-环绕双流网络有一些相似之处。主要区别在于 [[34](#bib.bib34)] 中，描述符计算模块的四个分支的输出作为输入提供给一个顶层决策网络进行融合和相似性计算，而不是使用投票。Zagoruyko
    和 Komodakis [[34](#bib.bib34)] 在每个特征计算分支的末尾添加了空间金字塔池化，以便能够比较任意大小的补丁。
- en: Using these approaches, inferring the raw cost volume from a pair of stereo
    images is performed using a moving window-like approach, which would require multiple
    forward passes ($n_{d}$ forward passes per pixel). However, since correlations
    are highly parallelizable, the number of forward passes can be significantly reduced.
    For instance, Luo *et al.* [[19](#bib.bib19)] reduce the number of forward passes
    to one pass per pixel by using a siamese network where the first branch takes
    a patch around a pixel while the second branch takes a larger patch that expands
    over all possible disparities. The output is a single 64D representation for the
    left branch, and $n_{d}\times 64$ for the right branch. A correlation layer then
    computes a vector of length $n_{d}$ where its $0pt-$th element is the cost of
    matching the pixel $x$ on the left image with the pixel $x-0pt$ on the rectified
    right image. Other papers, *e.g.,* [[29](#bib.bib29), [39](#bib.bib39), [21](#bib.bib21),
    [35](#bib.bib35), [27](#bib.bib27), [49](#bib.bib49), [25](#bib.bib25)], compute
    the feature maps of the left and right images in a single forward pass. These,
    however, have a high memory footprint at runtime and thus the feature map is usually
    computed at a resolution that is lower than the resolution of the input images.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些方法，从一对立体图像推断原始代价体积是通过类似移动窗口的方法进行的，这将需要多次前向传递（每个像素 $n_{d}$ 次前向传递）。然而，由于相关性高度并行化，前向传递的次数可以显著减少。例如，Luo
    *等人* [[19](#bib.bib19)] 通过使用一个西阿摩斯网络将前向传递的次数减少到每个像素一次，其中第一个分支处理围绕像素的补丁，而第二个分支处理扩展到所有可能差异的较大补丁。输出是左分支的单个
    64D 表示，而右分支的输出是 $n_{d}\times 64$。然后，相关层计算一个长度为 $n_{d}$ 的向量，其中其 $0pt-$ 元素是将左图像中的像素
    $x$ 与校正右图像中的像素 $x-0pt$ 匹配的代价。其他文献，例如 [[29](#bib.bib29), [39](#bib.bib39), [21](#bib.bib21),
    [35](#bib.bib35), [27](#bib.bib27), [49](#bib.bib49), [25](#bib.bib25)]，在一次前向传递中计算左右图像的特征图。然而，这些方法在运行时具有较高的内存占用，因此特征图通常在低于输入图像分辨率的分辨率下计算。
- en: These early methods produce matching scores that can be aggregated into a cost
    volume, which corresponds to the data term of Equation ([1](#S3.E1 "In 3.1 The
    pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")). They then extensively rely on hand-engineered
    post-processing steps, which are not jointly trained with the feature computation
    and feature matching networks, to regularize the cost volume and refine the disparity/depth
    estimation [[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [30](#bib.bib30)].
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这些早期方法生成的匹配得分可以汇总成一个代价体积，这与方程式 ([1](#S3.E1 "在 3.1 流水线 ‣ 3 通过立体匹配获得深度 ‣ 关于基于图像的深度重建的深度学习架构的调查"))
    的数据项相对应。然后，它们大量依赖手工设计的后处理步骤，这些步骤与特征计算和特征匹配网络并未共同训练，用于规范化代价体积和细化视差/深度估计 [[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [30](#bib.bib30)]。
- en: 3.2.2 End-to-end methods
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 端到端方法
- en: Recent works solve the stereo matching problem using a pipeline that is trained
    end-to-end without post-processing. For instance, Knöbelreiter *et al.* [[61](#bib.bib61)]
    proposed a hybrid CNN-CRF. The CNN part computes the matching term of Equation ([1](#S3.E1
    "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")). This then becomes the unary
    term of a Conditional Random Field (CRF) module, which performs the regularization.
    The pairwise term of the CRF is parameterized by edge weights and is computed
    using another CNN. Using the learned unary and pairwise costs, the CRF tries to
    find a joint solution optimizing the total sum of all unary and pairwise costs
    in a 4-connected graph. The whole CNN-CRF hybrid pipeline, which is trained end-to-end,
    could achieve a competitive performance using much fewer parameters (and thus
    a better utilization of the training data) than the earlier methods.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究通过一个端到端训练的流水线来解决立体匹配问题，而无需后处理。例如，Knöbelreiter *et al.* [[61](#bib.bib61)]
    提出了一个混合的 CNN-CRF。CNN 部分计算方程式 ([1](#S3.E1 "在 3.1 流水线 ‣ 3 通过立体匹配获得深度 ‣ 关于基于图像的深度重建的深度学习架构的调查"))
    的匹配项。这随后成为条件随机场 (CRF) 模块的单项，这个模块执行规范化。CRF 的成对项由边权参数化，并使用另一个 CNN 计算。利用学习到的单项和成对代价，CRF
    尝试找到一个联合解，优化 4 连接图中所有单项和成对代价的总和。整个 CNN-CRF 混合流水线经过端到端训练，可以用更少的参数（从而更好地利用训练数据）实现比早期方法更具竞争力的性能。
- en: Others papers [[12](#bib.bib12), [13](#bib.bib13), [29](#bib.bib29), [39](#bib.bib39),
    [21](#bib.bib21), [35](#bib.bib35), [27](#bib.bib27), [49](#bib.bib49), [25](#bib.bib25)]
    implement the entire pipeline using convolutional networks. In these approaches,
    the cost volume is computed in a single forward pass, which results in a high
    memory footprint. To reduce the memory footprint, some methods such as [[12](#bib.bib12),
    [13](#bib.bib13)] compute a lower resolution raw cost volume, *e.g.,* one half
    or one fourth of the size of the input images. Some methods, *e.g.,*  [[29](#bib.bib29),
    [39](#bib.bib39), [21](#bib.bib21), [28](#bib.bib28)], ommit the matching module.
    The left-right features, concatenated across the disparity range, are directly
    fed to the regularization and depth computation module. This, however, results
    in even larger memory footprint. Tulyakov *et al.* [[32](#bib.bib32)] reduce the
    memory use, without sacrificing accuracy, by introducing a matching module that
    compresses the concatenated features into compact matching signatures. The approach
    uses mean pooling instead of feature concatenation. This also reduces the memory
    footprint. More importantly, it allows the network to handle arbitrary number
    of multiview images, and to vary the number of input at runtime without re-training
    the network. Note that pooling layers have been used to aggregate features of
    different scales [[28](#bib.bib28)].
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 其他论文 [[12](#bib.bib12), [13](#bib.bib13), [29](#bib.bib29), [39](#bib.bib39),
    [21](#bib.bib21), [35](#bib.bib35), [27](#bib.bib27), [49](#bib.bib49), [25](#bib.bib25)]
    使用卷积网络实现整个流程。在这些方法中，成本体积在一次前向传播中计算，这导致了较高的内存占用。为了减少内存占用，一些方法如 [[12](#bib.bib12),
    [13](#bib.bib13)] 计算较低分辨率的原始成本体积，*例如，* 输入图像尺寸的一半或四分之一。一些方法，*例如，* [[29](#bib.bib29),
    [39](#bib.bib39), [21](#bib.bib21), [28](#bib.bib28)]，省略了匹配模块。左右特征在视差范围内串联后，直接输入到正则化和深度计算模块。然而，这会导致更大的内存占用。Tulyakov
    *et al.* [[32](#bib.bib32)] 通过引入一个匹配模块将串联特征压缩为紧凑的匹配签名，减少了内存使用，而不牺牲精度。该方法使用均值池化而非特征串联，这也减少了内存占用。更重要的是，它允许网络处理任意数量的多视角图像，并在运行时变更输入数量而无需重新训练网络。请注意，池化层已被用于聚合不同尺度的特征
    [[28](#bib.bib28)]。
- en: The regularization module takes the cost volume, the concatenated features,
    or the cost volume concatenated with the reference image [[12](#bib.bib12)], with
    the features of the reference image [[12](#bib.bib12), [26](#bib.bib26)], and/or
    with semantic features such as the segmentation mask [[25](#bib.bib25)] or the
    edge map [[31](#bib.bib31)], which serve as semantic priors. It then regularizes
    it and outputs either a depth/disparity map [[12](#bib.bib12), [13](#bib.bib13),
    [23](#bib.bib23), [31](#bib.bib31), [26](#bib.bib26)] or a distribution over depth/disparities [[39](#bib.bib39),
    [29](#bib.bib29), [28](#bib.bib28), [33](#bib.bib33)]. Both the segmentation mask [[25](#bib.bib25)]
    and the edge map [[31](#bib.bib31)] can be computed using deep networks that are
    trained jointly and end-to-end with the disparity/depth estimation networks. Appending
    semantic features to the cost volume improves the reconstruction of fine details,
    especially near object boundaries.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化模块接收成本体积、串联特征，或与参考图像 [[12](#bib.bib12)] 串联的成本体积，或与参考图像的特征 [[12](#bib.bib12),
    [26](#bib.bib26)]，以及/或语义特征如分割掩模 [[25](#bib.bib25)] 或边缘图 [[31](#bib.bib31)]，作为语义先验。然后它对其进行正则化，并输出深度/视差图
    [[12](#bib.bib12), [13](#bib.bib13), [23](#bib.bib23), [31](#bib.bib31), [26](#bib.bib26)]
    或深度/视差分布 [[39](#bib.bib39), [29](#bib.bib29), [28](#bib.bib28), [33](#bib.bib33)]。分割掩模
    [[25](#bib.bib25)] 和边缘图 [[31](#bib.bib31)] 可以使用与视差/深度估计网络共同训练和端到端训练的深度网络计算。将语义特征附加到成本体积中可以改善细节的重建，尤其是在物体边界附近。
- en: The regularization module is usually implemented as convolution-deconvolution
    (hourglass) deep network with skip connections between the contracting and expanding
    parts [[13](#bib.bib13), [12](#bib.bib12), [23](#bib.bib23), [39](#bib.bib39),
    [29](#bib.bib29), [28](#bib.bib28), [26](#bib.bib26)], or as a convolutional network [[14](#bib.bib14),
    [27](#bib.bib27)]. It can use 2D convolutions [[13](#bib.bib13), [12](#bib.bib12),
    [23](#bib.bib23), [26](#bib.bib26)] or 3D convolutions [[39](#bib.bib39), [29](#bib.bib29),
    [33](#bib.bib33), [28](#bib.bib28)]. The latter has less parameters. In both cases,
    their disparity range is fixed in advance and cannot be re-adjusted without re-training.
    Tulyakov *et al.* [[32](#bib.bib32)] introduced the sub-pixel MAP approximation
    for inference, which computes a weighted mean around the disparity with MAP probability.
    They showed that it is more robust to erroneous modes in the distribution and
    allows to modify the disparity range without re-training.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化模块通常实现为带有跳跃连接的卷积-反卷积（沙漏）深度网络，这些连接在收缩部分和扩展部分之间 [[13](#bib.bib13), [12](#bib.bib12),
    [23](#bib.bib23), [39](#bib.bib39), [29](#bib.bib29), [28](#bib.bib28), [26](#bib.bib26)]，或作为卷积网络
    [[14](#bib.bib14), [27](#bib.bib27)]。它可以使用 2D 卷积 [[13](#bib.bib13), [12](#bib.bib12),
    [23](#bib.bib23), [26](#bib.bib26)] 或 3D 卷积 [[39](#bib.bib39), [29](#bib.bib29),
    [33](#bib.bib33), [28](#bib.bib28)]。后者具有更少的参数。在这两种情况下，它们的视差范围是提前固定的，且不能在不重新训练的情况下调整。Tulyakov
    *等人* [[32](#bib.bib32)] 引入了用于推断的亚像素 MAP 近似，该方法计算视差的加权平均值，并使用 MAP 概率。他们表明，它对分布中的错误模式更具鲁棒性，并允许在不重新训练的情况下修改视差范围。
- en: Depth can be computed from the regularized cost volume using (1) the softargmin
    operator [[14](#bib.bib14), [39](#bib.bib39), [49](#bib.bib49), [27](#bib.bib27)],
    which is differentiable and allows sub-pixel accuracy but limited to network outputs
    that are unimodal, or (2) sub-pixel MAP approximation [[32](#bib.bib32)], which
    can handle multi-modal distributions.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用 (1) 软argmin 操作符 [[14](#bib.bib14), [39](#bib.bib39), [49](#bib.bib49),
    [27](#bib.bib27)] 从规则化的成本体积中计算深度，该操作符可微分并允许亚像素精度，但仅限于单峰的网络输出，或 (2) 亚像素 MAP 近似
    [[32](#bib.bib32)]，它可以处理多峰分布。
- en: Some papers, *e.g.,* [[39](#bib.bib39)], directly regress high-resolution map
    without an explicit refinement module. This is done by adding a final upconvolutinal
    layer to the regression module in order to upscale the cost volume to the resolution
    of the input images. In general, however, inferring high resolution depth maps
    would require large networks, which are expensive in terms of memory storage but
    also hard to train given the large number of free parameters. As such, some methods
    first estimate a low-resolution depth map and then refine it using a refinement
    module [[23](#bib.bib23), [26](#bib.bib26), [33](#bib.bib33)]. The refinement
    module as well as the early modules are trained jointly and end-to-end.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一些论文，*例如* [[39](#bib.bib39)]，直接回归高分辨率地图而没有明确的精炼模块。这是通过在回归模块中添加一个最终的上卷积层，以将成本体积上采样到输入图像的分辨率。然而，一般而言，推断高分辨率深度图需要大型网络，这在内存存储方面很昂贵，同时由于大量自由参数而难以训练。因此，一些方法首先估计低分辨率深度图，然后使用精炼模块进行精炼
    [[23](#bib.bib23), [26](#bib.bib26), [33](#bib.bib33)]。精炼模块以及早期模块是联合训练和端到端的。
- en: 3.3 Multiview stereo (MVS) matching networks
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 多视图立体匹配网络
- en: '| ![Refer to caption](img/aca83e4cf79dabd0e08dbdf2a91ea559.png) | ![Refer to
    caption](img/f2e95b16ba051754931a167cfc5f334f.png) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/aca83e4cf79dabd0e08dbdf2a91ea559.png) | ![参见说明](img/f2e95b16ba051754931a167cfc5f334f.png)
    |'
- en: '| (a) Hartmann *et al.* [[35](#bib.bib35)]. | (b) Flynn *et al.* [[14](#bib.bib14)].
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| (a) Hartmann *等人* [[35](#bib.bib35)]。 | (b) Flynn *等人* [[14](#bib.bib14)]。
    |'
- en: '| ![Refer to caption](img/4615460dadd96454e62c601b5c2ebb38.png) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/4615460dadd96454e62c601b5c2ebb38.png) |'
- en: '| (c) Kar *et al.* [[38](#bib.bib38)] and Yao *et al.* [[37](#bib.bib37)].
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| (c) Kar *等人* [[38](#bib.bib38)] 和 Yao *等人* [[37](#bib.bib37)]。 |'
- en: '| ![Refer to caption](img/767e96c2b576cc9ddc21d843f4ee16e1.png) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/767e96c2b576cc9ddc21d843f4ee16e1.png) |'
- en: '| (d) Huang *et al.* [[36](#bib.bib36)]. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| (d) Huang *等人* [[36](#bib.bib36)]。 |'
- en: 'Figure 5: Taxonomy of multivewstereo methods. (a), (b), and (c) perform early
    fusion, while (d) performs early fusion by aggregating features across depth plans,
    and late fusion by aggregating cost volumes across views.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 多视图立体方法的分类。 (a)、(b) 和 (c) 进行早期融合，而 (d) 通过在深度平面之间聚合特征来执行早期融合，并通过在视图之间聚合成本体积来执行晚期融合。'
- en: The methods described in Section [3.2](#S3.SS2 "3.2 Stereo matching networks
    ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction") have been designed to reconstruct depth/disparity maps
    from a pair of stereo images. These methods can be extended to the multiview stereo
    (MVS) case, *i.e.,* $n>2$, by replicating the feature computation branch $n$ times.
    The features computed by the different branches can then be aggregated using,
    for example, pooling [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)] or
    a recurrent fusion unit [[38](#bib.bib38)] before feeding the aggregated features
    into a top network, which regresses the depth map (Figures [5](#S3.F5 "Figure
    5 ‣ 3.3 Multiview stereo (MVS) matching networks ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")-(a),
    (b), and (c)). Alternatively, one can sample pairs of views, estimate the cost
    volume from each pair, and then merge the cost volumes either by voting or pooling [[36](#bib.bib36)]
    (Figure [5](#S3.F5 "Figure 5 ‣ 3.3 Multiview stereo (MVS) matching networks ‣
    3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction")-(d)). The former called *early fusion* while the latter
    is called *late fusion*.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 第[3.2](#S3.SS2 "3.2 立体匹配网络 ‣ 3 通过立体匹配进行深度估计 ‣ 基于深度学习的图像深度重建架构调查")节中描述的方法旨在从一对立体图像中重建深度/视差图。这些方法可以扩展到多视图立体（MVS）情况，即$n>2$，通过将特征计算分支复制$n$次来实现。然后，可以使用例如池化[[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37)]或递归融合单元[[38](#bib.bib38)]来聚合由不同分支计算出的特征，然后将聚合的特征输入到一个顶层网络，该网络回归深度图（图[5](#S3.F5
    "图5 ‣ 3.3 多视图立体（MVS）匹配网络 ‣ 3 通过立体匹配进行深度估计 ‣ 基于深度学习的图像深度重建架构调查")-(a)、(b)和(c)）。另外，也可以对视图对进行采样，从每对视图中估计成本体积，然后通过投票或池化[[36](#bib.bib36)]来合并成本体积（图[5](#S3.F5
    "图5 ‣ 3.3 多视图立体（MVS）匹配网络 ‣ 3 通过立体匹配进行深度估计 ‣ 基于深度学习的图像深度重建架构调查")-(d)）。前者称为*早期融合*，而后者称为*晚期融合*。
- en: The early work of Hartmann *et al.* [[35](#bib.bib35)] introduced a mechanism
    to learn multi-patch similarity, which replaces the correlation layer used in
    stereo matching. The approach uses pooling to aggregate the features computed
    on the different patches before feeding them to the subsequent blocks of the standard
    stereo matching pipeline. Recent techniques use Plane-Sweep Volumes (PSV) [[14](#bib.bib14),
    [36](#bib.bib36)], feature unprojection to the 3D space [[38](#bib.bib38), [37](#bib.bib37)],
    and image unprojection to the 3D space resulting in the Colored Voxel Cube (CVC) [[62](#bib.bib62)].
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Hartmann *et al.* [[35](#bib.bib35)] 的早期工作引入了一种机制来学习多补丁相似性，这取代了立体匹配中使用的相关层。这种方法使用池化来聚合在不同补丁上计算的特征，然后将这些特征输入到标准立体匹配管道的后续块中。近年来的技术使用平面扫描体积（PSV）[[14](#bib.bib14),
    [36](#bib.bib36)]、特征反投影到三维空间[[38](#bib.bib38), [37](#bib.bib37)]和图像反投影到三维空间，从而得到彩色体素立方体（CVC）[[62](#bib.bib62)]。
- en: Flynn *et al.* [[14](#bib.bib14)] and Huang *et al.* [[36](#bib.bib36)] use
    the camera parameters to unproject the input images into Plane-Sweep Volumes (PSV)
    and feed them into the subsequent feature extraction and feature matching networks.
    Flynn *et al.* [[14](#bib.bib14)]’s network is composed of $n_{d}$ branches, one
    for each depth plane (or depth value). The $0pt-$th branch of the network takes
    as input the reference image and the planes of the Plane-Sweep Volumes of the
    other images and which are located at depth $0pt$. These are packed together and
    fed to a two-stages network. The first stage, which consists of 2D convolutional
    rectified linear layers that share weights across all depth planes, computes matching
    features between the reference image and the PSV planes located at depth $0pt$.
    The second stage is composed of convolutional layers that are connected across
    depth planes in order to model interactions between them. The final layer of the
    network is a per-pixel softmax over depth, which returns the most probable depth
    value per pixel. The approach, which has been used for pairwise and multiview
    stereo matching, requires that the number of views and the camera parameters of
    each view to be known. It also requires setting in advance the disparity range.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Flynn *et al.* [[14](#bib.bib14)] 和 Huang *et al.* [[36](#bib.bib36)] 使用相机参数将输入图像反投影到平面扫描体积（PSV）中，并将其输入到随后的特征提取和特征匹配网络中。Flynn
    *et al.* [[14](#bib.bib14)] 的网络由 $n_{d}$ 个分支组成，每个深度平面（或深度值）一个。网络的 $0pt-$ 分支将参考图像和其他图像的平面扫描体积中的平面作为输入，这些平面位于深度
    $0pt$。这些被打包在一起并输入到一个两阶段网络中。第一阶段由在所有深度平面上共享权重的 2D 卷积整流线性层组成，计算参考图像与位于深度 $0pt$ 的
    PSV 平面之间的匹配特征。第二阶段由在深度平面之间连接的卷积层组成，以建模它们之间的交互。网络的最后一层是对深度的逐像素 softmax，返回每个像素最可能的深度值。该方法已用于对偶视图和多视图立体匹配，要求已知视图数量和每个视图的相机参数。还需要提前设置视差范围。
- en: 'Huang *et al.* [[36](#bib.bib36)]’s approach, which also operates on the plane-sweep
    volumes, uses a network composed of three parts: the patch matching part, the
    intra-volume feature aggregation part, and the inter-volume feature aggregation
    part:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Huang *et al.* [[36](#bib.bib36)] 的方法也在平面扫描体积上操作，使用一个由三个部分组成的网络：补丁匹配部分、体内特征聚合部分和体间特征聚合部分：
- en: •
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The patch matching part is a siamese network. Its first branch extracts features
    from a patch in the reference image and the second one from the plane-sweep volume
    that corresponds to the $i-$th input image at the $0pt-$th disparity level. ($100$
    disparity values have been used.) The features are then concatenated and passed
    to the subsequent convolutional layers. This process is repeated for all the plane-swept
    images.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 补丁匹配部分是一个孪生网络。它的第一个分支从参考图像中的一个补丁提取特征，第二个分支从与第 $i-$ 个输入图像在 $0pt-$ 视差层级对应的平面扫描体积中提取特征。（使用了
    $100$ 个视差值。）这些特征然后被连接并传递到随后的卷积层。这一过程对所有平面扫描图像重复进行。
- en: •
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The output from the patch matching module of the $0pt-$th plane-sweep volume
    are concatenated and fed into another encoder-decoder which produces a feature
    vector $F_{0}pt$ of size $64\times 64\times 800$.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 $0pt-$ 平面扫描体积的补丁匹配模块输出的特征被连接并输入到另一个编码器-解码器中，该编码器-解码器生成大小为 $64\times 64\times
    800$ 的特征向量 $F_{0}pt$。
- en: •
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All the feature vectors $F_{i},i=1,\dots,n$ (one for each input image), are
    aggregated using a max-pooling layer followed by convolutional layers, which produce
    a depth map of size $64\times 64$.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有特征向量 $F_{i},i=1,\dots,n$（每个输入图像一个）通过最大池化层和随后卷积层进行聚合，生成大小为 $64\times 64$ 的深度图。
- en: Unlike Flynn *et al.* [[14](#bib.bib14)], Huang *et al.* [[36](#bib.bib36)]’s
    approach does not require a fixed number of input views since aggregation is performed
    using pooling. In fact, the number of views at runtime can be different from the
    number of views used during training.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Flynn *et al.* [[14](#bib.bib14)] 不同，Huang *et al.* [[36](#bib.bib36)] 的方法不需要固定数量的输入视图，因为通过池化进行聚合。事实上，运行时的视图数量可以与训练时使用的视图数量不同。
- en: The main advantage of using PSVs is that they eliminate the need to supply rectified
    images. In other words, the camera parameters are implicitly encoded. However,
    in order to compute the PSVs, the intrinsic and extrinsic camera parameters need
    to be either provided in advance or estimated using, for example, Structure-from-Motion
    techniques as in [[36](#bib.bib36)]. Also, these methods require setting in advance
    the disparity range and its discretisation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PSV 的主要优点是它们消除了提供校正图像的需求。换句话说，相机参数是隐式编码的。然而，为了计算 PSV，需要提前提供内参和外参相机参数，或者使用例如结构光技术进行估计，如
    [[36](#bib.bib36)] 中所示。此外，这些方法需要提前设置视差范围及其离散化。
- en: Instead of using PSVs, other methods use the camera parameters to unproject
    either the input images [[62](#bib.bib62)] or the learned features [[38](#bib.bib38),
    [37](#bib.bib37)] into either a regular 3D feature grid, by rasterizing the viewing
    rays with the known camera poses, a 3D frustum of a reference camera [[37](#bib.bib37)],
    or by warping of the features into different parallel frontal planes of the reference
    camera, each one located at a specific depth. This unprojection aligns the features
    along epipolar lines, enabling efficient local matching by using either some distance
    measures such as the Euclidean or cosine distances [[38](#bib.bib38)], using a
    recurrent network [[38](#bib.bib38)], or using an encoder composed of multiple
    convolutional layers producing the probability of each voxel being on the surface
    of the 3D shape.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用 PSV 不同，其他方法使用相机参数将输入图像 [[62](#bib.bib62)] 或学习到的特征 [[38](#bib.bib38), [37](#bib.bib37)]
    进行反投影到一个规则的 3D 特征网格中，通过已知的相机姿态对视线进行光栅化，参考相机的 3D 截锥 [[37](#bib.bib37)]，或通过将特征扭曲到参考相机的不同平行前平面中，每个平面位于特定深度。这种反投影使得特征沿极线对齐，能够通过使用一些距离度量，如欧氏距离或余弦距离
    [[38](#bib.bib38)]、使用递归网络 [[38](#bib.bib38)] 或使用由多个卷积层组成的编码器生成每个体素在 3D 形状表面上的概率来实现高效的局部匹配。
- en: Note that the approaches of Kar *et al.* [[38](#bib.bib38)] and Ji *et al.* [[62](#bib.bib62)]
    perform volumetric reconstruction and use 3D convolutions. Thus, due to the memory
    requirements, only a coarse volume of size $32^{3}$ could be estimated. Huang
    *et al.* [[36](#bib.bib36)] overcome this limitation by directly regressing depth
    from different reference images. Similarly, Yao *et al.* [[37](#bib.bib37)] focus
    on producing the depth map for one reference image at each time. Thus, it can
    directly reconstruct a large scene.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Kar *et al.* [[38](#bib.bib38)] 和 Ji *et al.* [[62](#bib.bib62)] 的方法进行体积重建并使用
    3D 卷积。因此，由于内存要求，只能估计大小为 $32^{3}$ 的粗略体积。Huang *et al.* [[36](#bib.bib36)] 通过直接从不同参考图像中回归深度克服了这一限制。类似地，Yao
    *et al.* [[37](#bib.bib37)] 专注于为每次一个参考图像生成深度图。因此，它可以直接重建一个大场景。
- en: Table [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy and depth range ‣ 6.3 Multiview
    stereo techniques ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") summarizes the performance of these techniques.
    Note that most of them do not achieve sub-pixel accuracy, require the depth range
    to be specified in advance and cannot vary it at runtime without re-adjusting
    the network architecture and retraining it. Also, these methods fail in reconstructing
    tiny features such as those present in vegetation.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy and depth range ‣ 6.3 Multiview stereo
    techniques ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") 总结了这些技术的性能。注意，大多数技术没有实现亚像素精度，需要提前指定深度范围，并且在运行时无法在不重新调整网络架构和重新训练的情况下进行变化。此外，这些方法在重建诸如植被中存在的微小特征时表现不佳。
- en: 4 Depth estimation by regression
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 回归深度估计
- en: Instead of trying to match features across images, methods in this class directly
    regress disparity/depth from the input images or their learned features [[12](#bib.bib12),
    [13](#bib.bib13), [15](#bib.bib15), [63](#bib.bib63)]. These methods have no direct
    notion of descriptor matching. They consider a learned, view-based representation
    for depth reconstruction from either $n$ predefined viewpoints $\{v_{1},\dots,v_{n}\}$,
    or from any arbitrary viewpoint specified by the user. Their goal is to learn
    a predictor $f$ (see Section [2](#S2 "2 Scope and taxonomy ‣ A Survey on Deep
    Learning Architectures for Image-based Depth Reconstruction")), which predicts
    depth map from an input I.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 与尝试跨图像匹配特征不同，这一类方法直接从输入图像或其学习的特征回归视差/深度[[12](#bib.bib12), [13](#bib.bib13),
    [15](#bib.bib15), [63](#bib.bib63)]。这些方法没有直接的描述符匹配概念。它们考虑了一种学习的、基于视图的表示，用于从$n$个预定义的视点$\{v_{1},\dots,v_{n}\}$，或从用户指定的任意视点中进行深度重建。它们的目标是学习一个预测器$f$（见第[2](#S2
    "2 Scope and taxonomy ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction")节），该预测器从输入I预测深度图。
- en: 4.1 Network architectures
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 网络架构
- en: We classify the state-of-the-art into two classes, based on the type of network
    architectures they use. In the first class of methods, the predictor $f$ is an
    encoder which directly regresses the depth map [[3](#bib.bib3)].
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最先进的技术分为两类，基于它们所使用的网络架构类型。在第一类方法中，预测器$f$是一个编码器，它直接回归深度图[[3](#bib.bib3)]。
- en: In the second class of methods, the predictor $f$ is composed of an encoder
    and a top network. The encoder, which learns, using a convolutional network, a
    function $h$ that maps the input I into a compact latent representation $\textbf{x}=h(\textbf{I})\in\mathcal{X}$.
    The space $\mathcal{X}$ is referred to as *the latent space*. The encoder can
    be designed following any of the architectures discussed in Section [3.1](#S3.SS1
    "3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction"). The top network $g$ takes the compact
    representation, and eventually the target viewpoint $v$, and generates the estimated
    depth map $\hat{D}=g\left(h(\textbf{I}),v\right)=(g\circ h)(\textbf{I},v)$. Some
    methods use a top network composed of fully connected layers [[10](#bib.bib10),
    [1](#bib.bib1), [11](#bib.bib11), [4](#bib.bib4)]. Others use a decoder composed
    of upconvolutional layers [[2](#bib.bib2), [13](#bib.bib13), [44](#bib.bib44),
    [64](#bib.bib64), [63](#bib.bib63)].
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二类方法中，预测器$f$由编码器和一个顶层网络组成。编码器使用卷积网络学习一个函数$h$，将输入I映射到一个紧凑的潜在表示$\textbf{x}=h(\textbf{I})\in\mathcal{X}$。空间$\mathcal{X}$被称为*潜在空间*。编码器可以按照第[3.1](#S3.SS1
    "3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")节中讨论的任何架构进行设计。顶层网络$g$接收紧凑的表示以及目标视点$v$，生成估计的深度图$\hat{D}=g\left(h(\textbf{I}),v\right)=(g\circ
    h)(\textbf{I},v)$。一些方法使用由全连接层组成的顶层网络[[10](#bib.bib10), [1](#bib.bib1), [11](#bib.bib11),
    [4](#bib.bib4)]。其他方法使用由上卷积层组成的解码器[[2](#bib.bib2), [13](#bib.bib13), [44](#bib.bib44),
    [64](#bib.bib64), [63](#bib.bib63)]。
- en: The advantage of fully-connected layers is that they aggregate information coming
    from the entire image, and thus enable the network to infer depth at each pixel
    using global information. Convolutional operations, on the other hand, can only
    see local regions. To capture larger spatial relations, one needs to increase
    the number of convolution layers or use dilated convolutions, *i.e.,* large convolutional
    filters but with holes
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层的优点在于它们聚合来自整个图像的信息，从而使网络能够使用全局信息推断每个像素的深度。另一方面，卷积操作只能看到局部区域。为了捕捉更大的空间关系，需要增加卷积层的数量或使用扩张卷积，即具有孔的大卷积滤波器。
- en: 4.1.1 Input encoding networks
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 输入编码网络
- en: In general, the encoder stage is composed of convolutional layers, which capture
    local interactions between image features, followed by a number of fully-connected
    layers, which capture global interactions. Some layers are followed by spatial
    pooling operations to reduce the resolution of the output. For instance, Eigen
    *et al.* [[10](#bib.bib10)], one of the early works that tried to regress depth
    directly from a single input image, used an encoder composed of five feature extraction
    layers of convolution and max-pooling followed by one fully-connected layer. This
    maps the input image of size $304\times 228$ or $576\times 172$, depending on
    the dataset, to a latent representation of dimension $1\times 4096$. Liu *et al.* [[4](#bib.bib4)],
    on the other hand, used 7 convolutional layers to map the input into a low resolution
    feature map of dimension $512$.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，编码器阶段由卷积层组成，这些卷积层捕捉图像特征之间的局部交互，后跟若干个全连接层，这些层捕捉全局交互。一些层后跟有空间池化操作，以减少输出的分辨率。例如，Eigen
    *等人* [[10](#bib.bib10)]，作为早期尝试直接从单个输入图像回归深度的工作之一，使用了一个由五个特征提取层（卷积和最大池化）和一个全连接层组成的编码器。这将大小为
    $304\times 228$ 或 $576\times 172$ 的输入图像（取决于数据集）映射到一个 $1\times 4096$ 的潜在表示。另一方面，Liu
    *等人* [[4](#bib.bib4)] 使用了 7 个卷积层将输入映射到一个维度为 $512$ 的低分辨率特征图。
- en: Garg *et al.* [[3](#bib.bib3)] used this architecture to directly regress depth
    map from an input RGB images of size $188\times 620$. The encoder is composed
    of $7$ convolutional layers. The second, third, and fifth layers are followed
    by pooling layers to reduce the size of the output and thus the number of parameters
    of the network. The output of the sixth layer is a feature vector, which can be
    seen as a latent representation of size $16\times 16\times 2048$. The last convolutional
    layer maps this latent representation into a depth map of size $17\times 17$,
    which is upsampled using two fully connected layers and three upconvolutional
    layers, into a depth map of size $176\times 608$. Since it only relies on convolutional
    operation to regress depth, the approach does not capture global interactions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Garg *等人* [[3](#bib.bib3)] 使用这种架构直接从输入的 RGB 图像（大小为 $188\times 620$）回归深度图。编码器由
    $7$ 个卷积层组成。第二、第三和第五层后跟有池化层，以减少输出的尺寸，从而减少网络的参数数量。第六层的输出是一个特征向量，可以看作是一个 $16\times
    16\times 2048$ 的潜在表示。最后一个卷积层将这个潜在表示映射到一个 $17\times 17$ 的深度图，通过两个全连接层和三个上卷积层上采样成
    $176\times 608$ 的深度图。由于该方法仅依赖于卷积操作来回归深度，因此该方法没有捕捉到全局交互。
- en: Li *et al.* [[1](#bib.bib1)] extended the approach of Eigen *et al.* [[10](#bib.bib10)]
    to operate on superpixels and at multiple scales. Given an image, super-pixels
    are obtained and multi-scale image patches (at five different sizes) are extracted
    around the super-pixel centers. All patches of a super-pixel are resized to $227\times
    227$ pixels to form a multiscale input to a pre-trained multi-branch deep network
    (AlexNet or VGGNet). Each branch generates a latent representation of size $1\times
    4096$. The latent representations from the different branches are concatenated
    together and fed to the top network. Since the networks process patches, obtaining
    the entire depth map requires multiple forward passes.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Li *等人* [[1](#bib.bib1)] 扩展了 Eigen *等人* [[10](#bib.bib10)] 的方法，将其应用于超像素和多个尺度。给定一幅图像，超像素被提取出来，并且在超像素中心周围提取出多尺度图像块（共五种不同尺寸）。所有超像素的图像块被调整为
    $227\times 227$ 像素，以形成对预训练的多分支深度网络（AlexNet 或 VGGNet）的多尺度输入。每个分支生成一个 $1\times 4096$
    的潜在表示。来自不同分支的潜在表示被串联在一起，并输入到顶层网络。由于网络处理的是图像块，因此获得整个深度图需要多次前向传递。
- en: Since its introduction, this approach has been extended in many ways. For instance,
    Eigen and Fergus [[2](#bib.bib2)] showed a substantial improvement by switching
    from AlexNet (used in [[10](#bib.bib10)]) to VGG, which has a higher disciminative
    power. Also, instead of using fully convolutional layers, Laina *et al.* [[64](#bib.bib64)]
    incorporate residual blocks to ease the training. The encoder is implemented following
    the same architecture as ResNet50 but without the fully connected layers.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 自引入以来，这种方法已经在许多方面得到扩展。例如，Eigen 和 Fergus [[2](#bib.bib2)] 通过将 AlexNet（在 [[10](#bib.bib10)]
    中使用）切换到具有更高判别能力的 VGG 显著提高了性能。此外，Laina *等人* [[64](#bib.bib64)] 采用残差块来简化训练，而不是使用全卷积层。编码器按照与
    ResNet50 相同的架构实现，但没有全连接层。
- en: Using repeated spatial pooling reduces the spatial resolution of the feature
    maps. Although high-resolution maps can be obtained using the refinement techniques
    of Section  [3.1.4](#S3.SS1.SSS4 "3.1.4 Refinement ‣ 3.1 The pipeline ‣ 3 Depth
    by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction"), this would require additional computational and memory costs.
    To overcome this problem, Fu *et al.* [[41](#bib.bib41)], removed some pooling
    layers and replaced some convolutions with dilated convolutions. In fact, convolutional
    operations are local, and thus, they do not capture the global structure. To enlarge
    their receptive field, one can increase the size of the filters, or increase the
    number of convolutional and pooling layers. This, however, would require additional
    computational and memory costs, and will complicate the network architecture and
    the training procedure. One way to solve this problem is by using dilated convolutions,
    *i.e.,* convolutions with filters that have holes [[41](#bib.bib41)]. This allows
    to enlarge the receptive field of the filters without decreasing the spatial resolution
    or increasing the number of parameters and computation time.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用重复的空间池化会降低特征图的空间分辨率。尽管可以通过第 [3.1.4](#S3.SS1.SSS4 "3.1.4 Refinement ‣ 3.1 The
    pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") 节中的精炼技术获得高分辨率图像，但这会增加额外的计算和内存开销。为了解决这个问题，Fu
    *et al.* [[41](#bib.bib41)] 移除了一些池化层，并将部分卷积替换为膨胀卷积。实际上，卷积操作是局部的，因此它们无法捕捉全局结构。为了扩大其感受野，可以增加滤波器的大小，或者增加卷积和池化层的数量。然而，这将需要额外的计算和内存开销，并且会使网络架构和训练过程变得复杂。解决这一问题的一种方法是使用膨胀卷积，即具有孔的卷积
    [[41](#bib.bib41)]。这可以在不降低空间分辨率或增加参数和计算时间的情况下扩大滤波器的感受野。
- en: Using this principle, Fu *et al.* [[41](#bib.bib41)] proposed an encoding module
    that operates in two stages. The first stage extracts a dense feature map using
    an encoder whose last few downsampling operators (pooling, strides) are replaced
    with dilated convolutions in order to enlarge the receptive field of the filters.
    The second stage processes the dense feature map using three parallel modules;
    a full image encoder, a cross channel leaner, and an atrous spatial pyramid pooling
    (ASPP). The full image encoder maps the dense feature map into a latent representation.
    It uses an average pooling layer with a small kernel size and stride to reduce
    the spatial dimension. It is then followed by a fully connected layer to obtain
    a feature vector, then add a convolutional layer with $1\times 1$ kernel and copy
    the resultant feature vector into a feature map where each entry has the same
    feature vector. The ASPP module extracts features from multiple large receptive
    fields via dilated convolutions, with three different dilation rates. The output
    of the three modules are concatenated to form the latent representation.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 运用这一原则，Fu *et al.* [[41](#bib.bib41)] 提出了一个分为两个阶段的编码模块。第一阶段使用编码器提取密集特征图，其中最后几个下采样操作（池化、步幅）被膨胀卷积替代，以扩大滤波器的感受野。第二阶段使用三个并行模块处理密集特征图：全图像编码器、跨通道学习器和膨胀空间金字塔池化（ASPP）。全图像编码器将密集特征图映射为潜在表示。它使用一个小内核和步幅的平均池化层来减少空间维度，然后跟随一个全连接层以获得特征向量，接着添加一个$1\times
    1$内核的卷积层，并将结果特征向量复制到特征图中，其中每个条目都具有相同的特征向量。ASPP模块通过膨胀卷积从多个大感受野中提取特征，具有三种不同的膨胀率。三个模块的输出被拼接在一起形成潜在表示。
- en: These encoding techniques extract absolute features, ignoring the depth constraints
    of neighboring pixels, *i.e.,* relative features. To overcome this limitation,
    Gan *et al.* [[65](#bib.bib65)] explicitly model the relationships of different
    image locations using an affinity layer. They also combine absolute and relative
    features in an end-to-end network. In this approach, the input image is first
    processed by a ResNet50 encoder. The produced absolute feature map is fed into
    a context network, which captures both neighboring and global context information.
    It is composed of an affinity layer, which computes correlations between the features
    of neighboring pixels, followed by a fully-connected layer, which combines absolute
    and relative features. The output is fed into a depth estimator, which produces
    a coarse depth map.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这些编码技术提取绝对特征，忽略了邻近像素的深度约束，*即*相对特征。为了克服这一限制，Gan *等*[[65](#bib.bib65)]通过使用亲和层显式建模不同图像位置的关系。他们还在端到端网络中结合了绝对特征和相对特征。在这种方法中，输入图像首先由ResNet50编码器处理。生成的绝对特征图被送入一个上下文网络，该网络捕捉邻近和全局上下文信息。它由一个亲和层组成，该层计算邻近像素特征之间的相关性，然后是一个全连接层，该层结合了绝对特征和相对特征。输出被送入深度估计器，生成一个粗略的深度图。
- en: 4.1.2 Decoding networks
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 解码网络
- en: Many techniques first compute a latent representation of the input and then
    use a top network to decode the latent representation into a coarse depth map.
    In general, the decoding process can be done with either a series of fully-connected
    layers [[10](#bib.bib10), [1](#bib.bib1), [11](#bib.bib11), [4](#bib.bib4)], or
    upconvolutional layers [[2](#bib.bib2), [13](#bib.bib13), [44](#bib.bib44), [64](#bib.bib64),
    [63](#bib.bib63)].
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 许多技术首先计算输入的潜在表示，然后使用顶层网络将潜在表示解码为粗略的深度图。一般来说，解码过程可以通过一系列全连接层[[10](#bib.bib10)、[1](#bib.bib1)、[11](#bib.bib11)、[4](#bib.bib4)]或上卷积层[[2](#bib.bib2)、[13](#bib.bib13)、[44](#bib.bib44)、[64](#bib.bib64)、[63](#bib.bib63)]来完成。
- en: 4.1.2.1 Using fully-connected layers
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.1 使用全连接层
- en: Eigen *et al.* [[10](#bib.bib10)], Li *et al.* [[1](#bib.bib1)], Eigen and Fergus [[2](#bib.bib2)]
    and Liu *et al.* [[4](#bib.bib4)] use a top network composed of two fully-connected
    layers. The main advantage of using fully connected layers is that their receptive
    field is global. As such, they aggregate information from the entire image in
    the process of estimating the depth map. By doing so, however, the number of parameters
    in the network is high, and subsequently the memory requirement and computation
    time increase substantially. As such, these methods only estimate low-resolution
    coarse depth maps, which are then refined using some refinement blocks. For example,
    Eigen and Fergus [[2](#bib.bib2)] use two refinement blocks similar to those used
    in [[10](#bib.bib10)]. The first one produces predictions at a mid-level resolution,
    while the last one produces high resolution depth maps, at half the resolution
    of the output. In this approach, the coarse depth prediction network and the first
    refinement stage are trained jointly, with 3D supervision.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Eigen *等*[[10](#bib.bib10)]、Li *等*[[1](#bib.bib1)]、Eigen和Fergus[[2](#bib.bib2)]以及Liu
    *等*[[4](#bib.bib4)]使用由两个全连接层组成的顶层网络。使用全连接层的主要优点是它们的感受野是全局的。因此，在估计深度图的过程中，它们汇总了整个图像的信息。然而，这样做会导致网络中的参数数量很高，因此内存需求和计算时间显著增加。因此，这些方法仅估计低分辨率的粗略深度图，然后使用一些细化块进行细化。例如，Eigen和Fergus[[2](#bib.bib2)]使用两个类似于[[10](#bib.bib10)]中使用的细化块。第一个块在中级分辨率下生成预测，而最后一个块在输出分辨率的一半下生成高分辨率深度图。在这种方法中，粗略深度预测网络和第一个细化阶段是共同训练的，并且具有3D监督。
- en: 4.1.2.2 Using up-convolutional layers
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.2 使用上卷积层
- en: Dosovitskiy *et al.* [[12](#bib.bib12)] extended the approach of Eigen *et al.* [[10](#bib.bib10)]
    by removing the fully-connected layers. Instead, they pass the feature map, *i.e.,*
    the latent representation (of size $6\times 8\times 1024$), directly into a decoder
    to regress the optical flow in the case of the FlowNetSimple of [[12](#bib.bib12)],
    and a depth map in the case of [[2](#bib.bib2)]. In general, the decoder mirrors
    the encoder. It also includes skip connections, *i.e.,* connections from some
    layers of the encoder to their corresponding counterpart in the decoder. Dosovitskiy
    *et al.* [[12](#bib.bib12)] use variational refinement to refine the coarse optical
    flow.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Dosovitskiy*等*[[12](#bib.bib12)]通过去除全连接层扩展了Eigen*等*[[10](#bib.bib10)]的方法。相反，他们将特征图，即潜在表示（大小为$6\times
    8\times 1024$），直接传递到解码器，以回归光流（在[[12](#bib.bib12)]的FlowNetSimple中）和深度图（在[[2](#bib.bib2)]中）。通常，解码器镜像编码器。它还包括跳跃连接，即从编码器的某些层到解码器中对应层的连接。Dosovitskiy*等*[[12](#bib.bib12)]使用变分细化来细化粗略的光流。
- en: 'Chen *et al.* [[66](#bib.bib66)] used a similar approach to produce dense depth
    maps given an RGB image with known depth at a few pixels. At training, the approach
    takes the ground-truth depth map and a binary mask indicating valid ground-truth
    depth pixels, and generates two other maps: the nearest-neighbor fill of the sparse
    depth map, and the Euclidean distance transform of the binary mask. These two
    maps are then concatenated together and with the input image and used as input
    to an encoder-decoder, which learns the residual that will be added to the sparse
    depth map. The network follows the same architecture as in [[67](#bib.bib67)].
    Note that the same approach has been also used to infer other properties, *e.g.,*
    the optical flow as in Zhou *et al.* [[44](#bib.bib44)].'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 陈*等*[[66](#bib.bib66)]使用了类似的方法，从已知深度的RGB图像中生成密集深度图。在训练时，该方法将真实深度图和一个表示有效真实深度像素的二进制掩模作为输入，生成两个其他图：稀疏深度图的最近邻填充和二进制掩模的欧几里得距离变换。然后将这两个图与输入图像一起连接，并作为输入送入编码器-解码器，该编码器-解码器学习将添加到稀疏深度图的残差。该网络遵循与[[67](#bib.bib67)]相同的架构。注意，同样的方法也用于推断其他属性，*例如*，光流，如周*等*[[44](#bib.bib44)]所述。
- en: 4.1.3 Combining and stacking multiple networks
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 结合和堆叠多个网络
- en: Several previous papers showed that stacking and combining multiple networks
    can lead to significantly improved performance. For example, Ummenhofer and Zhou
    [[15](#bib.bib15)] introduced DeMoN, which takes an image pair as input and predicts
    the depth map of the left image and the relative pose (egomotion) of the right
    image with respect to the left. The network consists of a chain of three blocks
    that iterate over optical flow, depth, and relative camera pose estimation. The
    first block in the chain, called bootstrap net, is composed of two encoder-decoder
    networks. It gets the image pair as input and then estimates, using the first
    encoder-decoder, the optical flow and a confidence map. These, along with the
    original pair of images, are fed to the second encoder-decoder, which outputs
    the initial depth and egomotion estimates. The second component, called iterative
    net, is trained to improve, in a recursive manner, the depth, normal, and motion
    estimates. Finally, the last component, called refinement net, upsamples, using
    and encoder-decoder network, the output of the iterative net to obtain high resolution
    depth maps.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 一些早期的论文显示，堆叠和结合多个网络可以显著提高性能。例如，Ummenhofer和周[[15](#bib.bib15)]介绍了DeMoN，它以一对图像为输入，预测左图像的深度图和右图像相对于左图像的相对位姿（自运动）。该网络由三个块链组成，这些块依次遍历光流、深度和相对相机位姿估计。链中的第一个块，称为bootstrap网，包含两个编码器-解码器网络。它以图像对作为输入，然后使用第一个编码器-解码器估计光流和置信度图。这些数据连同原始图像对一起送入第二个编码器-解码器，后者输出初步深度和自运动估计。第二个组件，称为迭代网，经过训练以递归方式改进深度、法线和运动估计。最后，最后一个组件，称为细化网，使用编码器-解码器网络对迭代网的输出进行上采样，以获得高分辨率深度图。
- en: Note that, unlike other techniques such as FlowNetSimple of [[12](#bib.bib12)]
    which require a calibrated pair of images, Ummenhofer and Zhou [[15](#bib.bib15)]
    estimates jointly the relative camera motion and the depth map.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与像[[12](#bib.bib12)]的FlowNetSimple等需要校准图像对的其他技术不同，Ummenhofer和周[[15](#bib.bib15)]联合估计了相对相机运动和深度图。
- en: FlowNetSimple of [[12](#bib.bib12)] has been later extended by Ilg *et al.* [[68](#bib.bib68)]
    to FlowNet2.0, which achieved results that are competitive with the traditional
    methods, but with an order of magnitude faster. The idea is to combine multiple
    FLowNetSimple networks to compute large displacement optical flow. It (1) stacks
    multiple FlowNetSimple and FlowNetC networks [[12](#bib.bib12)]. The flow estimated
    by each network is used to warp, using a warping operator, the right image onto
    the left image, and feed the concatenated left image, warped image, estimated
    flow, and the brightness error, into the next network. This way, the next network
    in the stack can focus on learning the remaining increment between the left and
    right images, (2) adds another FlowNetSimple network, called FlowNet-SD, which
    focuses on small subpixel motion, and (3) uses a learning schedule consisting
    of multiple datasets. The output of the FlowNet-SD and the stack of multiple FlowNetSimple
    modules are merged together and processed using a fusion network, which provides
    the final flow estimation.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: FlowNetSimple的[[12](#bib.bib12)]后来由Ilg *et al.* [[68](#bib.bib68)] 扩展为FlowNet2.0，其取得的结果与传统方法竞争，但速度快了一个数量级。其思路是将多个FlowNetSimple网络结合起来，以计算大位移光流。它
    (1) 堆叠多个FlowNetSimple和FlowNetC网络[[12](#bib.bib12)]。每个网络估算的光流用于通过变形操作将右图像变形到左图像上，并将拼接的左图像、变形图像、估算的光流和亮度误差输入到下一个网络。这样，堆叠中的下一个网络可以专注于学习左图像和右图像之间剩余的增量，(2)
    添加了另一个FlowNetSimple网络，称为FlowNet-SD，它专注于小的亚像素运动，(3) 使用由多个数据集组成的学习计划。FlowNet-SD的输出和多个FlowNetSimple模块的堆叠被合并在一起，并使用融合网络进行处理，从而提供最终的光流估计。
- en: Roy *et al.* [[69](#bib.bib69)] observed that among all the training data sets
    currently available, there is limited training data for some depths. As a consequence,
    deep learning techniques trained with these datasets will naturally achieve low
    performance in the depth ranges that are under-represented in the training data.
    Roy *et al.* [[69](#bib.bib69)] mitigate the problem by combining CNN with a Neural
    Regression Forest. A patch around a pixel is processed with an ensemble of binary
    regression tree, called Convolutional Regression Tree (CRT). At every node of
    the CRT, the patch is processed with a shallow CNN associated to that node, and
    then passed to the left or right child node with a Bernouli probability for further
    convolutional processing. The process is repeated until the patch reaches the
    leaves of the tree. Depth estimates made by every leaf are weighted with the corresponding
    path probability. The regression results of every CRT are then fused into a final
    depth estimation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Roy *et al.* [[69](#bib.bib69)] 观察到，在当前可用的所有训练数据集中，一些深度的训练数据非常有限。因此，使用这些数据集训练的深度学习技术在训练数据中表现不足的深度范围自然会表现出低性能。Roy
    *et al.* [[69](#bib.bib69)] 通过将CNN与神经回归森林相结合来缓解这个问题。围绕一个像素的补丁通过一组二元回归树进行处理，称为卷积回归树（CRT）。在CRT的每个节点上，补丁通过与该节点关联的浅层CNN进行处理，然后以伯努利概率传递到左子节点或右子节点进行进一步的卷积处理。这个过程会重复，直到补丁到达树的叶子节点。每个叶子节点做出的深度估计会根据相应的路径概率加权。每个CRT的回归结果随后被融合成最终的深度估计。
- en: Chakrabarti [[70](#bib.bib70)] combine global and local methods. The method
    first maps an input image into a latent representation of size $1\times 1096$,
    which is then reshaped into a feature map of size $427\times 562\times 64$. In
    other words, each pixel is represented with a global descriptor (of size $1\times
    64$) that characterizes the entire scene. A parallel path takes patches of size
    $97\times 97$ around each pixel and computes a local feature vector of size $1\times
    1024$. This one is then concatenated with the global descriptor of that pixel
    and fed into a top network. The whole network is trained to predict, at every
    image location, depth derivatives of different orders, orientations, and scales.
    However, instead of a single estimate for each derivative, the network outputs
    probability distributions that allow it to express confidence about some coefficients,
    and ambiguity about others. Scene depth is then estimated by harmonizing this
    overcomplete set of network predictions, using a globalization procedure that
    finds a single consistent depth map that best matches all the local derivative
    distributions.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Chakrabarti [[70](#bib.bib70)] 结合了全局方法和局部方法。该方法首先将输入图像映射到大小为 $1\times 1096$
    的潜在表示，然后将其重塑为大小为 $427\times 562\times 64$ 的特征图。换句话说，每个像素用一个全局描述符（大小为 $1\times
    64$）来表示，该描述符表征整个场景。一个并行路径会在每个像素周围提取大小为 $97\times 97$ 的补丁，并计算出一个大小为 $1\times 1024$
    的局部特征向量。然后将该局部特征向量与该像素的全局描述符连接，并输入到上层网络。整个网络被训练以预测每个图像位置的不同阶数、方向和尺度的深度导数。然而，网络输出的是概率分布，而不是每个导数的单一估计，这使其能够对某些系数表达信心，而对其他系数表达不确定性。场景深度随后通过协调这些过完整的网络预测集来估计，使用一种全球化过程，找到一个与所有局部导数分布最匹配的单一一致深度图。
- en: 4.1.4 Joint task learning
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 联合任务学习
- en: Depth estimation and many other visual image understanding problems, such as
    segmentation, semantic labelling, and scene parsing, are strongly correlated and
    mutually beneficial. Leveraging on the complementarity properties of these tasks,
    many recent papers proposed to either jointly solve these tasks so that one boosts
    the performance of another.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 深度估计以及许多其他视觉图像理解问题，如分割、语义标注和场景解析，彼此之间有很强的关联性，并且互相促进。利用这些任务的互补特性，许多最新的研究提出了要么联合解决这些任务，以便一个任务可以提升另一个任务的性能。
- en: To this end, Wang *et al.* [[71](#bib.bib71)] follow the CNN structure in [[10](#bib.bib10)]
    but adds additional semantic nodes, in the final layer, to predict the semantic
    label. Both depth estimation and semantic label prediction are trained jointly
    using a loss function that is a weighted sum of the depth error and the semantic
    loss. The overall network is composed of a joint global CNN, which predicts, from
    the entire image, a coarse depth and segmentation maps, and a regional CNN, which
    operates on image segments (obtained by over segmenting the input image) and predicts
    a more accurate depth and segmentation labels within each segment. These two predictions
    form unary terms to a hierarchical CRF, which produces the final depth and semantic
    labels. The CRF includes additional pairwise terms such as pairwise edges between
    neighboring pixels, and pairwise edges between neighboring segments.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，Wang *et al.* [[71](#bib.bib71)] 采用了[[10](#bib.bib10)]中的CNN结构，但在最终层添加了额外的语义节点，以预测语义标签。深度估计和语义标签预测使用一个损失函数进行联合训练，该损失函数是深度误差和语义损失的加权和。整个网络由一个联合的全局CNN组成，该CNN从整个图像中预测粗略的深度和分割图，以及一个区域CNN，该CNN对图像片段（通过对输入图像进行过度分割获得）进行操作，并在每个片段中预测更准确的深度和分割标签。这两个预测形成了分层CRF的单项项，该CRF生成最终的深度和语义标签。CRF还包括额外的配对项，如邻近像素之间的配对边缘，以及邻近片段之间的配对边缘。
- en: 'Zhou *et al.* [[72](#bib.bib72)] follow the same idea to jointly estimate,
    from two successive images and in a non-supervised manner, the depth map at each
    of them, the 6D relative camera pose, and the forward and backward optical flows.
    The approach uses three separate network, but jointly trained using a cross-task
    consistency loss: a DepthNet, which estimates depth from two successive frames,
    PoseNet, which estimates the relative camera pose, and a FlowNet, which estimates
    the optical flow between the two frames. To handle non-rigid transformations that
    cannot be explained by the camera motion, the paper exploits the forward-backward
    consistency check to identify valid regions, *i.e.,* regions that moved in a rigid
    manner, and avoid enforcing the cross-task consistency in the non-valid regions.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Zhou *et al.* [[72](#bib.bib72)] 遵循相同的思路，以无监督的方式从两幅连续图像中共同估计每幅图像的深度图、6D相机姿态以及前向和后向光流。该方法使用三个独立的网络，但通过一个跨任务一致性损失函数进行联合训练：一个DepthNet用于从两幅连续帧中估计深度，PoseNet用于估计相对相机姿态，FlowNet用于估计两帧之间的光流。为了处理不能通过相机运动解释的非刚性变换，论文利用前向-后向一致性检查来识别有效区域，即以刚性方式移动的区域，并避免在无效区域强制执行跨任务一致性。
- en: 'In the approaches of Wang *et al.* [[71](#bib.bib71)] and Zhou *et al.* [[72](#bib.bib72)],
    the networks (or network components) that estimate each modality do not directly
    share knowledge. Collaboration between them is only through a joint loss function.
    To enable information exchange between the different task, Xu *et al.* [[73](#bib.bib73)]
    proposed an approach that first maps the input image into a latent representation
    using a CNN. The latent representation is then decoded, using four decoding streams,
    into a depth map, a normal map, an edge map, and a semantic label map. These multi-modal
    intermediate information is aggregated using a multi-model distillation module,
    and t hen passed into two decoders, one estimates the refined depth map and the
    other one estimates the refined semantic label map. For the multi-model distillation
    module, Xu *et al.* [[73](#bib.bib73)] investigated three architectures:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在Wang *et al.* [[71](#bib.bib71)] 和Zhou *et al.* [[72](#bib.bib72)] 的方法中，估计每种模态的网络（或网络组件）并不直接共享知识。它们之间的协作仅通过一个联合损失函数来实现。为了在不同任务之间实现信息交换，Xu
    *et al.* [[73](#bib.bib73)] 提出了一个方法，该方法首先使用CNN将输入图像映射到潜在表示。然后，使用四个解码流将潜在表示解码为深度图、法线图、边缘图和语义标签图。这些多模态中间信息通过一个多模态蒸馏模块进行聚合，然后传递到两个解码器中，一个估计优化后的深度图，另一个估计优化后的语义标签图。对于多模态蒸馏模块，Xu
    *et al.* [[73](#bib.bib73)] 研究了三种架构：
- en: •
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Simple concatenation of the four modalities.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单地将四种模态进行拼接。
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Concatenating the four modalities and feeding them to two different encoders.
    One encoder learns the features that are appropriate for inferring depth while
    the second learns features that are appropriate for inferring semantic labels.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将四种模态拼接后输入到两个不同的编码器中。一个编码器学习适用于深度推断的特征，另一个编码器学习适用于语义标签推断的特征。
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Using an attention mechanism to guide the message passing between the multi-modal
    features, before concatenating them and feeding them to the encoder that learns
    the features for depth estimation or the one which learns the features for semantic
    labels estimation.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用注意力机制来指导多模态特征之间的信息传递，然后将这些特征拼接在一起，输入到学习深度估计特征的编码器或学习语义标签估计特征的编码器中。
- en: Xu *et al.* [[73](#bib.bib73)] showed that the third option obtains remarkably
    better performance than the others.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Xu *et al.* [[73](#bib.bib73)] 表明第三种选项的性能明显优于其他方法。
- en: Instead of a distillation module, Jiao *et al.* [[74](#bib.bib74)] proposed
    a a synergy network whose backbone is a shared encoder. The network then splits
    into two branches, one for depth estimation and another for semantic labelling.
    These two branches share knowledge through lateral sharing units . The network
    is trained with a attention-driven loss, which guides the network to pay more
    attention to the distant depth regions during training.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Jiao *et al.* [[74](#bib.bib74)] 提出了一个协同网络，其主干是共享编码器。网络随后分为两个分支，一个用于深度估计，另一个用于语义标注。这两个分支通过横向共享单元共享知识。该网络使用一个基于注意力的损失函数进行训练，该损失函数引导网络在训练过程中更加关注远处的深度区域。
- en: Finally, instead of estimating depth and semantic segmentation in a single iteration,
    Zhang *et al.* [[52](#bib.bib52)] performed it recursively, using Task-Recursive
    learning. It is composed of an encoder and a decoder network, with a series of
    residual blocks (ResNet), upsampling blocks, and Task-Attention Modules. The input
    image is first fed into the encoder and then into the task-recursive decoding
    to estimate depth and semantic segmentation. In the decoder, the two tasks (depth
    estimation and segmentation) are alternately processed by adaptively evolving
    previous experiences of both tasks to benefit each other. A task-attention module
    is used before each residual block and takes depth and segmentation features from
    the previous residual block as input. It is composed of a balance unit, to balance
    the contribution of the features of the two sources. The balanced output is fed
    into a series of convolutional-deconvolutional layers designed to get different
    spatial attentions by using receptive field variation. The output is an attention
    map, which is used to generate the gated depth and segmentation features. These
    are fused by concatenation followed by one convolutional layer.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Zhang *等人* [[52](#bib.bib52)] 通过使用任务递归学习递归地进行深度和语义分割的估计。它由一个编码器和一个解码器网络组成，包含一系列残差块（ResNet）、上采样块和任务注意模块。输入图像首先被送入编码器，然后进入任务递归解码，以估计深度和语义分割。在解码器中，通过自适应地演变两个任务的先前经验来交替处理这两个任务（深度估计和分割），以便相互受益。在每个残差块之前使用任务注意模块，并将来自上一个残差块的深度和分割特征作为输入。它由一个平衡单元组成，用于平衡两个来源特征的贡献。平衡后的输出被送入一系列卷积-反卷积层，设计用于通过使用感受野变化获得不同的空间注意。输出是一个注意图，用于生成门控深度和分割特征。通过连接然后经过一层卷积层来融合这些特征。
- en: While jointly estimating depth and other cues, *e.g.,* semantic labels, significantly
    improves the performance of both tasks, it requires a large amount of training
    data annotated with depth and semantic labels.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然同时估计深度和其他线索，*例如*，语义标签，显著提高了两个任务的性能，但它需要大量带有深度和语义标签的训练数据。
- en: 5 Training
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 训练
- en: 'The training process aims to find the network parameters $W^{*}$ that minimize
    a loss function $\mathcal{L}$, *i.e.,* :'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程的目标是找到网络参数 $W^{*}$，以最小化损失函数 $\mathcal{L}$，*即*：
- en: '|  | $W^{*}=\arg\min_{W}\mathcal{L}(\hat{D},\Theta,W).$ |  | (6) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $W^{*}=\arg\min_{W}\mathcal{L}(\hat{D},\Theta,W).$ |  | (6) |'
- en: Here, $\Theta$ is the training data, which can be composed of input images,
    their associated camera parameters, and/or their corresponding ground-truth depth.
    We will review in Section [5.1](#S5.SS1 "5.1 Datasets and data augmentation ‣
    5 Training ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    the different different datasets that have been used for training deep learning-based
    depth reconstruction algorithms, and for evaluating their performances. We will
    then review the different loss functions (Section [5.2](#S5.SS2 "5.2 Loss functions
    ‣ 5 Training ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")),
    the degree of supervision required in various methods (Section [5.3](#S5.SS3 "5.3
    Degree of supervision ‣ 5 Training ‣ A Survey on Deep Learning Architectures for
    Image-based Depth Reconstruction")), and the domain adaptation and transfer learning
    techniques (Section [5.3.4](#S5.SS3.SSS4 "5.3.4 Domain adaptation and transfer
    learning ‣ 5.3 Degree of supervision ‣ 5 Training ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\Theta$ 是训练数据，可以由输入图像、相关的相机参数和/或对应的真实深度组成。我们将在第[5.1节](#S5.SS1 "5.1 数据集和数据增强
    ‣ 5 训练 ‣ 基于图像的深度重建的深度学习架构调查")回顾用于训练基于深度学习的深度重建算法和评估其性能的不同数据集。接着，我们将回顾不同的损失函数（第[5.2节](#S5.SS2
    "5.2 损失函数 ‣ 5 训练 ‣ 基于图像的深度重建的深度学习架构调查")），各种方法所需的监督程度（第[5.3节](#S5.SS3 "5.3 监督程度
    ‣ 5 训练 ‣ 基于图像的深度重建的深度学习架构调查")），以及领域适应和迁移学习技术（第[5.3.4节](#S5.SS3.SSS4 "5.3.4 领域适应和迁移学习
    ‣ 5.3 监督程度 ‣ 5 训练 ‣ 基于图像的深度重建的深度学习架构调查")）。
- en: 5.1 Datasets and data augmentation
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据集和数据增强
- en: 'TABLE IV: Datasets for depth/disparity estimation. ”$\#f$” refers to the number
    of frames per video. ”#img./scene” refers to the number of images per scenes.
    We also refer the reader to [[75](#bib.bib75)] for more 3D datasets.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：深度/视差估计的数据集。“$\#f$”指的是每个视频的帧数。“#img./scene”指的是每个场景的图像数量。我们还建议读者参考[[75](#bib.bib75)]以获取更多3D数据集。
- en: '| Name | Type | Frame size | Number of (pairs of) images |  | Video (scene)
    |  | Camera params | Disp/Depth |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 类型 | 帧大小 | 图像（对）数量 |  | 视频（场景） |  | 相机参数 | 视差/深度 |'
- en: '|  |  |  | Total | #train | #val | #test |  | $\#f$ | #train | #test | #img./scene
    |  |  |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 总计 | 训练集数量 | 验证集数量 | 测试集数量 |  | $\#f$ | 训练集数量 | 测试集数量 | 每场景图像数量
    |  |  |  |'
- en: '| CityScapes [[76](#bib.bib76)] | real | $2048\times 1024$ | $5000$ | $2975$
    | $500$ | $1525$ |  | $-$ | $-$ | $-$ | $-$ |  | $-$ | $-$ |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| CityScapes [[76](#bib.bib76)] | 实景 | $2048\times 1024$ | $5000$ | $2975$
    | $500$ | $1525$ |  | $-$ | $-$ | $-$ | $-$ |  | $-$ | $-$ |'
- en: '| KITTI 2015 [[77](#bib.bib77)] | real | $1242\times 375$ | $-$ | $-$ | $-$
    | $-$ |  | $400$ | $200$ | $200$ | $4$ |  | int+ext | sparse |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| KITTI 2015 [[77](#bib.bib77)] | 实景 | $1242\times 375$ | $-$ | $-$ | $-$ |
    $-$ |  | $400$ | $200$ | $200$ | $4$ |  | 内部+外部 | 稀疏 |'
- en: '| KITTI 2012 [[78](#bib.bib78)] | real | $1240\times 376$ | 389 | $194$ | $-$
    | $195$ |  | $-$ | $-$ | $-$ | $-$ |  | int+ext | sparse |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| KITTI 2012 [[78](#bib.bib78)] | 实景 | $1240\times 376$ | 389 | $194$ | $-$
    | $195$ |  | $-$ | $-$ | $-$ | $-$ |  | 内部+外部 | 稀疏 |'
- en: '| FlyingThings3D [[13](#bib.bib13)] | synth. | $960\times 540$ | $26066$ |
    $21818$ | $-$ | $4248$ |  | $-$ | $2247$ | $-$ | $-$ |  | int+ext | per-pixel
    |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| FlyingThings3D [[13](#bib.bib13)] | 合成 | $960\times 540$ | $26066$ | $21818$
    | $-$ | $4248$ |  | $-$ | $2247$ | $-$ | $-$ |  | 内部+外部 | 每像素 |'
- en: '| Monkaa [[13](#bib.bib13)] | synth. | $960\times 540$ | $8591$ | $8591$ |
    $-$ | $-$ |  | $-$ | $8$ | $-$ | $-$ |  | int+ext | per-pixel |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Monkaa [[13](#bib.bib13)] | 合成 | $960\times 540$ | $8591$ | $8591$ | $-$
    | $-$ |  | $-$ | $8$ | $-$ | $-$ |  | 内部+外部 | 每像素 |'
- en: '| Driving [[13](#bib.bib13)] | synth. | $960\times 540$ | $4392$ | $4392$ |
    $-$ | $-$ |  | $-$ | $1$ | $-$ | $-$ |  | int+ext | per-pixel |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Driving [[13](#bib.bib13)] | 合成 | $960\times 540$ | $4392$ | $4392$ | $-$
    | $-$ |  | $-$ | $1$ | $-$ | $-$ |  | 内部+外部 | 每像素 |'
- en: '| MPI Sintel [[79](#bib.bib79)] | synth. | $1024\times 436$ | $1041$ | $1041$
    | $-$ | $-$ |  | $35$ | $23$ | $12$ | $50$ |  | $-$ | per-pixel |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| MPI Sintel [[79](#bib.bib79)] | 合成 | $1024\times 436$ | $1041$ | $1041$ |
    $-$ | $-$ |  | $35$ | $23$ | $12$ | $50$ |  | $-$ | 每像素 |'
- en: '| SUN3D [[80](#bib.bib80)] | rooms | $640\times 480$ | $-$ | $2.5$M | $-$ |
    $-$ |  | $-$ | $415$ | $-$ | $-$ |  | ext. | per-pixel |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| SUN3D [[80](#bib.bib80)] | 房间 | $640\times 480$ | $-$ | $2.5$M | $-$ | $-$
    |  | $-$ | $415$ | $-$ | $-$ |  | 外部 | 每像素 |'
- en: '| NYU2 [[81](#bib.bib81)] | indoor | $640\times 480$ | $-$ | $1449$ | $-$ |
    $-$ |  | $-$ | $464$ | $-$ | $-$ |  | no | per-pixel |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| NYU2 [[81](#bib.bib81)] | 室内 | $640\times 480$ | $-$ | $1449$ | $-$ | $-$
    |  | $-$ | $464$ | $-$ | $-$ |  | 无 | 每像素 |'
- en: '| RGB-D SLAM [[82](#bib.bib82)] | real | $640\times 480$ | $-$ | $-$ | $-$
    | $-$ |  | $-$ | $15$ | $4$ | variable |  | int+ext | per-pixel |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| RGB-D SLAM [[82](#bib.bib82)] | 实景 | $640\times 480$ | $-$ | $-$ | $-$ |
    $-$ |  | $-$ | $15$ | $4$ | 可变 |  | 内部+外部 | 每像素 |'
- en: '| MVS-Synth [[36](#bib.bib36)] | Urban | $1920\times 1080$ | $-$ | $-$ | $-$
    | $-$ |  | $120$ | $-$ | $-$ | $100$ |  | int+ext | per-pixel |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| MVS-Synth [[36](#bib.bib36)] | 城市 | $1920\times 1080$ | $-$ | $-$ | $-$ |
    $-$ |  | $120$ | $-$ | $-$ | $100$ |  | 内部+外部 | 每像素 |'
- en: '| ETH3D [[83](#bib.bib83)] | in/outdoor | $713\times 438$ |  | $27$ | $-$ |
    $20$ |  |  | $5$ | $5$ | $-$ |  | int+ext | point cloud |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| ETH3D [[83](#bib.bib83)] | 室内/室外 | $713\times 438$ |  | $27$ | $-$ | $20$
    |  |  | $5$ | $5$ | $-$ |  | 内部+外部 | 点云 |'
- en: '| DTU [[84](#bib.bib84)] | MVS | $1200\times 1600$ |  |  |  |  |  | $80$ |  |  |
    $49-64$ |  | int+ext |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| DTU [[84](#bib.bib84)] | MVS | $1200\times 1600$ |  |  |  |  |  | $80$ |  |  |
    $49-64$ |  | 内部+外部 |  |'
- en: '| MVS KITTI2015 [[77](#bib.bib77)] | MVS |  |  |  |  |  |  | $200+200$ |  |  |
    $20$ |  | $-$ |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| MVS KITTI2015 [[77](#bib.bib77)] | MVS |  |  |  |  |  |  | $200+200$ |  |  |
    $20$ |  | $-$ |  |'
- en: '| ETH3D [[83](#bib.bib83)] | MVS | $6048\times 4032$ |  |  |  |  |  | $13+12$
    |  |  | variable |  | $-$ |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| ETH3D [[83](#bib.bib83)] | MVS | $6048\times 4032$ |  |  |  |  |  | $13+12$
    |  |  | 可变 |  | $-$ |  |'
- en: '| Make3D [[85](#bib.bib85)] | Single view | $2272\times 1704$ | $534$ | $400$
    | $-$ | $134$ |  | $-$ | $-$ | $-$ | $-$ |  | $-$ | $55\times 305$ |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Make3D [[85](#bib.bib85)] | 单视图 | $2272\times 1704$ | $534$ | $400$ | $-$
    | $134$ |  | $-$ | $-$ | $-$ | $-$ |  | $-$ | $55\times 305$ |'
- en: '| MegaDepth [[86](#bib.bib86)] | Single, MVS | $-$ | $130$K | $-$ | $-$ | $-$
    |  | $196$ | $-$ | $-$ | $-$ |  | $-$ | Eucl., ordinal |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| MegaDepth [[86](#bib.bib86)] | 单视图，MVS | $-$ | $130$K | $-$ | $-$ | $-$ |  |
    $196$ | $-$ | $-$ | $-$ |  | $-$ | 欧几里得，顺序 |'
- en: Unlike traditional 3D reconstruction techniques, training and evaluating deep-learning
    architectures for depth reconstruction require large amounts of annotated data.
    This annotated data should be in the form of natural images and their corresponding
    depth maps, which is very challenging to obtain. Tables [IV](#S5.T4 "TABLE IV
    ‣ 5.1 Datasets and data augmentation ‣ 5 Training ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction") summarizes some of the datasets
    that have been used in the literature. Some of them have been specifically designed
    to train, test, and benchmark stereo-based depth reconstruction algorithms. They
    usually contain pairs of stereo images of real or synthesized scenes, captured
    with calibrated cameras, and their corresponding disparity/depth information as
    ground truth. The disparity/depth information can be either in the form of maps
    at the same or lower esolution as the input images, or in the form of sparse depth
    values at some locations in the reference image. Some of these datasets contain
    video sequences and thus are suitable for benchmarking Structure from Motion (SfM)
    and Simultaneous Localisation and Mapping (SLAM) algorithms.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的 3D 重建技术不同，为深度重建训练和评估深度学习架构需要大量的标注数据。这些标注数据应为自然图像及其对应的深度图，这些数据非常难以获得。表 [IV](#S5.T4
    "TABLE IV ‣ 5.1 数据集和数据增强 ‣ 5 训练 ‣ 深度学习架构在基于图像的深度重建中的调查") 总结了一些在文献中使用的数据集。其中一些数据集专门设计用于训练、测试和基准测试基于立体的深度重建算法。它们通常包含真实或合成场景的立体图像对，这些图像由标定的相机拍摄，并且具有对应的视差/深度信息作为真实值。视差/深度信息可以是与输入图像相同或更低分辨率的图像，或者是参考图像中某些位置的稀疏深度值。其中一些数据集包含视频序列，因此适合于基准测试运动结构（SfM）和同时定位与地图构建（SLAM）算法。
- en: The datasets that were particularly designed to train and benchmark multiview
    stereo and single view-based reconstruction algorithms (MVS) are composed of multiple
    scenes with $n\geq 1$ images per scene. Each image is captured from a different
    viewpoint.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 特别设计用于训练和基准测试多视角立体和单视角重建算法 (MVS) 的数据集由多个场景组成，每个场景中有 $n\geq 1$ 张图像。每张图像都从不同的视角拍摄。
- en: 'In general, deep-learning models achieve good results if trained on large datasets.
    Obtaining the ground-truth depth maps is, however, time-consuming and resource
    intensive. To overcome this limitation, many papers collect data from some existing
    datasets and augment them with suitable information and annotations to make them
    suitable for training and testing deep learning-based depth reconstruction techniques.
    In general, they use the four following strategies:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，深度学习模型在大数据集上训练时可以取得良好的结果。然而，获取真实深度图是耗时且资源密集的。为了克服这一限制，许多论文从一些现有数据集中收集数据，并用适当的信息和注释对其进行扩充，使其适合于训练和测试基于深度学习的深度重建技术。通常，他们使用以下四种策略：
- en: •
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 3D data augmentation. To introduce more diversity to the training datasets,
    one can apply to the existing datasets some geometric and photometric transformations,
    *e.g.,* translation, rotation, and scaling, as well as additive Gaussian noise
    and changes in brightness, contrast, gamma, and color. Although some transformations
    are similarity preserving, they still enrich the datasets. One advantage of this
    approach is that it reduces the network’s generalization error. Also statistical
    shape analysis techniques [[87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89)]
    can be used to synthesize more 3D shapes from existing ones.
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3D 数据增强。为了向训练数据集中引入更多多样性，可以对现有数据集应用一些几何和光度变换，例如，平移、旋转和缩放，以及添加高斯噪声和亮度、对比度、伽马和颜色的变化。尽管某些变换保持了相似性，但它们仍然丰富了数据集。这种方法的一个优点是它减少了网络的泛化误差。此外，统计形状分析技术 [[87](#bib.bib87),
    [88](#bib.bib88), [89](#bib.bib89)] 可以用来从现有形状中合成更多的 3D 形状。
- en: •
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Using synthesized 3D models and scenes. One approach to generate image-depth
    annotations is by synthetically rendering from 3D CAD models 2D and 2.5D views
    from various (random) viewpoints, poses, and lighting conditions. They can also
    be overlayed with random textures.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用合成的 3D 模型和场景。一种生成图像深度标注的方法是通过合成渲染从 3D CAD 模型中获取各种（随机）视角、姿势和光照条件下的 2D 和 2.5D
    视图。它们也可以覆盖随机纹理。
- en: •
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Natural image - 3D shape/scene pairs. Another approach is to synthesize training
    data by overlaying images rendered from large 3D model collections on the top
    of real images such as those in the SUN [[90](#bib.bib90)], ShapeNet [[91](#bib.bib91)],
    ModelNet [[92](#bib.bib92)], IKEA [[93](#bib.bib93)], and PASCAL 3D+ [[94](#bib.bib94)]
    datasets.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自然图像 - 3D 形状/场景对。另一种方法是通过将从大型3D模型集合渲染的图像叠加到真实图像上来合成训练数据，例如SUN [[90](#bib.bib90)]、ShapeNet
    [[91](#bib.bib91)]、ModelNet [[92](#bib.bib92)]、IKEA [[93](#bib.bib93)] 和 PASCAL
    3D+ [[94](#bib.bib94)] 数据集中的图像。
- en: While the last two techniques allow enriching existing training datasets, they
    suffer from domain bias and thus require using domain adaptation techniques, see
    Section [5.3.4](#S5.SS3.SSS4 "5.3.4 Domain adaptation and transfer learning ‣
    5.3 Degree of supervision ‣ 5 Training ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction"). Finally, some papers overcome the need
    for ground-truth depth information by training their deep networks without 3D
    supervision, see Section [5.3](#S5.SS3 "5.3 Degree of supervision ‣ 5 Training
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction").
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最后两种技术允许丰富现有的训练数据集，但它们受到领域偏差的影响，因此需要使用领域适应技术，见第[5.3.4节](#S5.SS3.SSS4 "5.3.4
    领域适应与迁移学习 ‣ 5.3 监督程度 ‣ 5 训练 ‣ 基于图像的深度重建的深度学习架构调查")。最后，一些论文通过在没有3D监督的情况下训练深度网络，克服了对真实深度信息的需求，见第[5.3节](#S5.SS3
    "5.3 监督程度 ‣ 5 训练 ‣ 基于图像的深度重建的深度学习架构调查")。
- en: 5.2 Loss functions
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 损失函数
- en: 'The role of the loss function is to measure at each iteration how far the estimated
    disparity/depth map $\hat{D}$ is from the real map $D$, and use it to guide the
    update of the network weights. In general, the loss function is defined as the
    sum of two terms:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的作用是每次迭代时衡量估计的视差/深度图 $\hat{D}$ 距离真实图 $D$ 的远近，并用其指导网络权重的更新。通常，损失函数被定义为两个项的总和：
- en: '|  | $\mathcal{L}(\hat{D},\Theta,W)=\mathcal{L}_{1}(\hat{D},\Theta,W)+\mathcal{L}_{2}(\hat{D},\Theta,W).$
    |  | (7) |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\hat{D},\Theta,W)=\mathcal{L}_{1}(\hat{D},\Theta,W)+\mathcal{L}_{2}(\hat{D},\Theta,W).$
    |  | (7) |'
- en: The data term $\mathcal{L}_{1}$ measures the error between the ground truth
    and the estimated depth while the regularization term $\mathcal{L}_{2}$ is used
    to incorporate various constraints, *e.g.,* smoothness. To ensure robustness to
    spurious outliers, some techniques, *e.g.,* [[95](#bib.bib95)], use a truncated
    loss, which is defined at each pixel $x$ as $\min(\mathcal{L}_{x},\psi)$. Here,
    $\mathcal{L}_{x}$ denotes the non-truncated loss at pixel $x$, and $\psi$ is a
    pre-defined threshold.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 数据项 $\mathcal{L}_{1}$ 衡量真实值与估计深度之间的误差，而正则化项 $\mathcal{L}_{2}$ 用于引入各种约束，*例如，*
    平滑性。为了确保对虚假离群值的鲁棒性，一些技术，*例如，* [[95](#bib.bib95)]，使用了截断损失，其在每个像素 $x$ 处定义为 $\min(\mathcal{L}_{x},\psi)$。这里，$\mathcal{L}_{x}$
    表示像素 $x$ 处的非截断损失，$\psi$ 是一个预定义的阈值。
- en: There are various loss functions that have been used in the literature. Below,
    we list the most popular ones. Tables [V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo
    matching techniques ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction"), [VI](#S6.T6 "TABLE VI ‣
    6.3.2 Accuracy and depth range ‣ 6.3 Multiview stereo techniques ‣ 6 Discussion
    and comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction"), and [VII](#S6.T7 "TABLE VII ‣ 6.4 Depth regression techniques
    ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction") show how these terms have been used to train depth estimation
    pipelines.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中使用了各种损失函数。下面，我们列出了最流行的几种。表[V](#S6.T5 "TABLE V ‣ 6.2 对称立体匹配技术 ‣ 6 讨论与比较 ‣
    基于图像的深度重建的深度学习架构调查")、[VI](#S6.T6 "TABLE VI ‣ 6.3.2 准确性和深度范围 ‣ 6.3 多视图立体技术 ‣ 6
    讨论与比较 ‣ 基于图像的深度重建的深度学习架构调查") 和 [VII](#S6.T7 "TABLE VII ‣ 6.4 深度回归技术 ‣ 6 讨论与比较
    ‣ 基于图像的深度重建的深度学习架构调查") 展示了这些术语在训练深度估计管道中的应用。
- en: 5.2.1 The data term
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 数据项
- en: The data term of Equation ([7](#S5.E7 "In 5.2 Loss functions ‣ 5 Training ‣
    A Survey on Deep Learning Architectures for Image-based Depth Reconstruction"))
    measures the error between the ground truth and the estimated depth. Such error
    can be quantified using one or a weighted sum of two or more of the error measures
    described below. The $L_{2}$, the mean absolute difference, the cross-entropy
    loss, and the Hinge loss require 3D supervision while re-projection based losses
    can be used without 3D supervision since they do not depend on ground truth depth/disparity.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 ([7](#S5.E7 "In 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction"))中的数据项衡量真实值与估计深度之间的误差。此类误差可以通过以下错误测量中的一个或两个以上的加权和来量化。$L_{2}$、平均绝对差异、交叉熵损失和铰链损失需要3D监督，而基于重投影的损失可以在没有3D监督的情况下使用，因为它们不依赖于真实深度/视差。
- en: (1) The $L_{2}$ loss is defined as
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: (1) $L_{2}$ 损失定义为
- en: '|  | $\mathcal{L}_{1}^{0}=\frac{1}{N}\sum_{x}\&#124;D(x)-\hat{D}(x)\&#124;^{2},$
    |  | (8) |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{1}^{0}=\frac{1}{N}\sum_{x}\&#124;D(x)-\hat{D}(x)\&#124;^{2},$
    |  | (8) |'
- en: where $N$ is the number of pixels being considered.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$N$ 是考虑的像素数量。
- en: (2) The mean absolute difference (mAD) between the ground truth and the predicted
    disparity/depth maps [[39](#bib.bib39), [15](#bib.bib15), [23](#bib.bib23), [53](#bib.bib53)]
    is defined as follows;
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 真实值与预测的视差/深度图之间的平均绝对差异（mAD） [[39](#bib.bib39), [15](#bib.bib15), [23](#bib.bib23),
    [53](#bib.bib53)] 定义如下：
- en: '|  | $\mathcal{L}_{1}^{1}=\frac{1}{N}\sum_{x}\&#124;D(x)-\hat{D}(x)\&#124;_{1}.$
    |  | (9) |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{1}^{1}=\frac{1}{N}\sum_{x}\&#124;D(x)-\hat{D}(x)\&#124;_{1}.$
    |  | (9) |'
- en: 'Many variants of this loss function have been used. For instance, Tonioni *et
    al.* [[96](#bib.bib96)] avoid explicit 3D supervision by taking $d_{x}$ as the
    disparity/depth at pixel $x$ computed using traditional stereo matching techniques,
    $\hat{D}(x)$ as the estimated disparity, and $c_{x}$ as the confidence of the
    estimate at $x$. They then define a *confidence-guided loss* as:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 已经使用了许多这种损失函数的变体。例如，Tonioni *等* [[96](#bib.bib96)] 通过将$d_{x}$作为使用传统立体匹配技术计算的像素$x$的视差/深度，将$\hat{D}(x)$作为估计的视差，将$c_{x}$作为$x$处估计的置信度，从而避免了显式的3D监督。然后，他们定义了*置信度引导的损失*为：
- en: '|  | $\small{\mathcal{L}_{1}^{2}=\frac{1}{N}\sum_{x}L(x),\text{ }L(x)=\left\{\begin{tabular}[]{@{}l@{}l@{}}$c_{x}&#124;d_{x}-\hat{d}_{x}&#124;$&amp;{
    if} $c_{x}\geq\epsilon$,\\ $0$&amp;{ }otherwise.\end{tabular}\right.}$ |  | (10)
    |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{\mathcal{L}_{1}^{2}=\frac{1}{N}\sum_{x}L(x),\text{ }L(x)=\left\{\begin{tabular}[]{@{}l@{}l@{}}$c_{x}&#124;d_{x}-\hat{d}_{x}&#124;$&amp;{
    如果} $c_{x}\geq\epsilon$,\\ $0$&amp;{ 否则。\end{tabular}\right.}$ |  | (10) |'
- en: Here, $\epsilon$ is a user-defined threshold.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\epsilon$ 是用户定义的阈值。
- en: 'Yao *et al.* [[37](#bib.bib37)], which first estimate an initial depth map
    $\hat{D}_{0}$ and then the refined one $\hat{D}$, define the overall loss as the
    weighted sum of the mean absolute difference between the ground truth $D$ and
    $\hat{D}_{0}$, and the ground truth $D$ and $\hat{D}$:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 姚*等* [[37](#bib.bib37)]，首先估计初始深度图$\hat{D}_{0}$，然后是精细化后的$\hat{D}$，将整体损失定义为真实值$D$与$\hat{D}_{0}$之间的平均绝对差异和真实值$D$与$\hat{D}$之间的平均绝对差异的加权和：
- en: '|  | $\small{\mathcal{L}_{1}^{3}=\frac{1}{N}\sum_{x}\left\{\&#124;d(x)-\hat{d}_{0}(x)\&#124;_{1}+\lambda\&#124;d(x)-\hat{d}(x)\&#124;_{1}\right\}}.$
    |  | (11) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{\mathcal{L}_{1}^{3}=\frac{1}{N}\sum_{x}\left\{\&#124;d(x)-\hat{d}_{0}(x)\&#124;_{1}+\lambda\&#124;d(x)-\hat{d}(x)\&#124;_{1}\right\}}.$
    |  | (11) |'
- en: 'Here, $d_{x}=D(x)$, and $\lambda$ is a weight factor, which is set to one in [[37](#bib.bib37)].
    Khamis *et al.* [[27](#bib.bib27)], on the other hand, used the two-parameter
    robust function $\rho(\cdot)$, proposed in [[97](#bib.bib97)], to approximate
    a smoothed $L_{1}$ loss. It is defined as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$d_{x}=D(x)$，$\lambda$ 是一个权重因子，在 [[37](#bib.bib37)] 中设置为1。另一方面，Khamis *等* [[27](#bib.bib27)]
    使用了在 [[97](#bib.bib97)] 中提出的两参数鲁棒函数 $\rho(\cdot)$ 来近似平滑的$L_{1}$ 损失。其定义如下：
- en: '|  |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; $\mathcal{L}_{1}^{4}=\frac{1}{N}\sum_{x}\rho(d_{x}-\hat{d}_{x},\alpha,c),\text{
    where }\alpha=1,c=2,\text{ and }$ &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{L}_{1}^{4}=\frac{1}{N}\sum_{x}\rho(d_{x}-\hat{d}_{x},\alpha,c),\text{
    其中 }\alpha=1,c=2,\text{ 和 }$ &#124;'
- en: '&#124; $\rho(x,\alpha,c)=\frac{&#124;2-\alpha&#124;}{\alpha}\left(\left(\frac{x^{2}}{c^{2}&#124;2-\alpha&#124;}+1\right)^{\frac{\alpha}{2}}-1\right).$
    &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\rho(x,\alpha,c)=\frac{&#124;2-\alpha&#124;}{\alpha}\left(\left(\frac{x^{2}}{c^{2}&#124;2-\alpha&#124;}+1\right)^{\frac{\alpha}{2}}-1\right).$
    &#124;'
- en: '|  | (12) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | (12) |'
- en: 'Other papers, *e.g.,*  [[28](#bib.bib28)], use the smooth $L_{1}$ loss, which
    is widely used in bounding box regression for object detection because of its
    robustness and low sensitivity to outliers. It is defined as:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 其他论文，如 [[28](#bib.bib28)]，使用平滑的$L_{1}$ 损失，该损失在物体检测中的边界框回归中被广泛使用，因为它具有鲁棒性且对离群值的敏感度低。其定义如下：
- en: '|  | $\mathcal{L}_{1}^{5}=\frac{1}{N}\sum_{x}\text{smooth}_{L_{1}}(d_{x}-\hat{d}_{x}),$
    |  | (13) |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{1}^{5}=\frac{1}{N}\sum_{x}\text{smooth}_{L_{1}}(d_{x}-\hat{d}_{x}),$
    |  | (13) |'
- en: where <math   alttext="\text{smooth}_{L_{1}}(x)=\left\{\begin{tabular}[]{ll}$0.5x^{2}$&amp;$\text{
    if }|x|<1$,\\
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="\text{smooth}_{L_{1}}(x)=\left\{\begin{tabular}[]{ll}$0.5x^{2}$&amp;$\text{
    如果 }|x|<1$,\\
- en: $|x|-0.5$&amp;$\text{ otherwise}$.\end{tabular}\right." display="inline"><semantics
    ><mrow ><msub ><mtext >smooth</mtext><msub ><mi >L</mi><mn >1</mn></msub></msub><mrow
    ><mo stretchy="false" >(</mo><mi >x</mi><mo stretchy="false" >)</mo></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd
    columnalign="left" ><mn >0.5</mn><mi >x</mi><msup ><mn >2</mn></msup></mtd><mtd
    columnalign="left" ><mtext >if </mtext><mo fence="false" rspace="0.167em" stretchy="false"
    >|</mo><mi >x</mi><mo fence="false" stretchy="false" >|</mo><mo lspace="0.167em"
    ><</mo><mn >1</mn><mtext >,</mtext></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mo fence="false" rspace="0.167em" stretchy="false" >|</mo><mi >x</mi><mo fence="false"
    stretchy="false" >|</mo><mo lspace="0em" >−</mo><mn >0.5</mn></mtd><mtd columnalign="left"
    ><mtext >otherwise</mtext><mtext >.</mtext></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\text{smooth}_{L_{1}}(x)=\left\{\begin{tabular}[]{ll}$0.5x^{2}><$\text{
    if }|x|<1$,\\ $|x|-0.5><$\text{ otherwise}$.\end{tabular}\right.</annotation></semantics></math>
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: $|x|-0.5$&amp;$\text{ 否则}$.\end{tabular}\right." display="inline"><semantics
    ><mrow ><msub ><mtext >smooth</mtext><msub ><mi >L</mi><mn >1</mn></msub></msub><mrow
    ><mo stretchy="false" >(</mo><mi >x</mi><mo stretchy="false" >)</mo></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd
    columnalign="left" ><mn >0.5</mn><mi >x</mi><msup ><mn >2</mn></msup></mtd><mtd
    columnalign="left" ><mtext >如果 </mtext><mo fence="false" rspace="0.167em" stretchy="false"
    >|</mo><mi >x</mi><mo fence="false" stretchy="false" >|</mo><mo lspace="0.167em"
    ><</mo><mn >1</mn><mtext >,</mtext></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mo fence="false" rspace="0.167em" stretchy="false" >|</mo><mi >x</mi><mo fence="false"
    stretchy="false" >|</mo><mo lspace="0em" >−</mo><mn >0.5</mn></mtd><mtd columnalign="left"
    ><mtext >否则</mtext><mtext >。</mtext></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\text{smooth}_{L_{1}}(x)=\left\{\begin{tabular}[]{ll}$0.5x^{2}><$\text{
    如果 }|x|<1$,\\ $|x|-0.5><$\text{ 否则}$.\end{tabular}\right.</annotation></semantics></math>
- en: Note that some papers restrict the sum to be over valid pixels in order to avoid
    outliers, or over regions of interests, *e.g.,* foreground or visible pixels [[44](#bib.bib44)].
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一些论文限制求和范围在有效像素上，以避免异常值，或在感兴趣的区域上，*例如，*前景或可见像素 [[44](#bib.bib44)]。
- en: '(3) The cross-entropy loss [[19](#bib.bib19), [36](#bib.bib36)]. It is defined
    as:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 交叉熵损失 [[19](#bib.bib19), [36](#bib.bib36)]。它定义为：
- en: '|  | $\mathcal{L}_{1}^{6}=-\sum_{x}Q(d_{x},\hat{d}_{x})\log\left(P(x,\hat{d}_{x})\right).$
    |  | (14) |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{1}^{6}=-\sum_{x}Q(d_{x},\hat{d}_{x})\log\left(P(x,\hat{d}_{x})\right).$
    |  | (14) |'
- en: 'Here, $P(x,\hat{d}_{x})$ is the likelihood, as computed by the network, of
    pixel $x$ having the disparity/depth $\hat{d}_{x}$. It is defined in [[19](#bib.bib19),
    [36](#bib.bib36)] as the 3-pixel error:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$P(x,\hat{d}_{x})$ 是网络计算的像素 $x$ 具有视差/深度 $\hat{d}_{x}$ 的可能性。它在 [[19](#bib.bib19),
    [36](#bib.bib36)] 中定义为 3 像素误差：
- en: '|  | <math   alttext="Q(d_{x},\hat{d}_{x})=\left\{\begin{tabular}[]{ll}$\lambda_{1}$&amp;\text{if
    } $d_{x}=\hat{d}_{x}$\\ $\lambda_{2}$&amp;\text{if } $&#124;d_{x}-\hat{d}_{x}&#124;=1$\\'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="Q(d_{x},\hat{d}_{x})=\left\{\begin{tabular}[]{ll}$\lambda_{1}$&amp;\text{如果
    } $d_{x}=\hat{d}_{x}$\\ $\lambda_{2}$&amp;\text{如果 } $&#124;d_{x}-\hat{d}_{x}&#124;=1$\\'
- en: $\lambda_{3}$&amp;\text{if } $&#124;d_{x}-\hat{d}_{x}&#124;=2$\\
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: $\lambda_{3}$&amp;\text{如果 } $&#124;d_{x}-\hat{d}_{x}&#124;=2$\\
- en: $0$&amp;\text{otherwise. }\\
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: $0$&amp;\text{否则。}\\
- en: \end{tabular}\right." display="block"><semantics ><mrow ><mi >Q</mi><mrow ><mo
    stretchy="false" >(</mo><msub ><mi >d</mi><mi >x</mi></msub><mo >,</mo><msub ><mover
    accent="true" ><mi >d</mi><mo >^</mo></mover><mi >x</mi></msub><mo stretchy="false"
    >)</mo></mrow><mo >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd  columnalign="left" ><mi >λ</mi><msub ><mn >1</mn></msub></mtd><mtd  columnalign="left"
    ><mtext >if </mtext><mi >d</mi><msub ><mi >x</mi></msub><mo >=</mo><mover accent="true"
    ><mi >d</mi><mo >^</mo></mover><msub ><mi >x</mi></msub></mtd></mtr><mtr ><mtd
    columnalign="left" ><mi >λ</mi><msub ><mn >2</mn></msub></mtd><mtd columnalign="left"
    ><mtext >if </mtext><mo fence="false" rspace="0.167em" stretchy="false" >&#124;</mo><mi
    >d</mi><msub ><mi >x</mi></msub><mo >−</mo><mover accent="true" ><mi >d</mi><mo
    >^</mo></mover><msub ><mi >x</mi></msub><mo fence="false" stretchy="false" >&#124;</mo><mo
    lspace="0.167em" >=</mo><mn >1</mn></mtd></mtr><mtr ><mtd columnalign="left" ><mi
    >λ</mi><msub ><mn >3</mn></msub></mtd><mtd columnalign="left" ><mtext >if </mtext><mo
    fence="false" rspace="0.167em" stretchy="false" >&#124;</mo><mi >d</mi><msub ><mi
    >x</mi></msub><mo >−</mo><mover accent="true" ><mi >d</mi><mo >^</mo></mover><msub
    ><mi >x</mi></msub><mo fence="false" stretchy="false" >&#124;</mo><mo lspace="0.167em"
    >=</mo><mn >2</mn></mtd></mtr><mtr ><mtd columnalign="left" ><mn >0</mn></mtd><mtd
    columnalign="left" ><mtext >otherwise.</mtext></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >Q(d_{x},\hat{d}_{x})=\left\{\begin{tabular}[]{ll}$\lambda_{1}><\text{if
    } $d_{x}=\hat{d}_{x}$\\ $\lambda_{2}><\text{if } $&#124;d_{x}-\hat{d}_{x}&#124;=1$\\
    $\lambda_{3}><\text{if } $&#124;d_{x}-\hat{d}_{x}&#124;=2$\\ $0><\text{otherwise.
    }\\ \end{tabular}\right.</annotation></semantics></math> |  | (15) |
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: \end{tabular}\right." display="block"><semantics ><mrow ><mi >Q</mi><mrow ><mo
    stretchy="false" >(</mo><msub ><mi >d</mi><mi >x</mi></msub><mo >,</mo><msub ><mover
    accent="true" ><mi >d</mi><mo >^</mo></mover><mi >x</mi></msub><mo stretchy="false"
    >)</mo></mrow><mo >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd  columnalign="left" ><mi >λ</mi><msub ><mn >1</mn></msub></mtd><mtd  columnalign="left"
    ><mtext >如果 </mtext><mi >d</mi><msub ><mi >x</mi></msub><mo >=</mo><mover accent="true"
    ><mi >d</mi><mo >^</mo></mover><msub ><mi >x</mi></msub></mtd></mtr><mtr ><mtd
    columnalign="left" ><mi >λ</mi><msub ><mn >2</mn></msub></mtd><mtd columnalign="left"
    ><mtext >如果 </mtext><mo fence="false" rspace="0.167em" stretchy="false" >&#124;</mo><mi
    >d</mi><msub ><mi >x</mi></msub><mo >−</mo><mover accent="true" ><mi >d</mi><mo
    >^</mo></mover><msub ><mi >x</mi></msub><mo fence="false" stretchy="false" >&#124;</mo><mo
    lspace="0.167em" >=</mo><mn >1</mn></mtd></mtr><mtr ><mtd columnalign="left" ><mi
    >λ</mi><msub ><mn >3</mn></msub></mtd><mtd columnalign="left" ><mtext >如果 </mtext><mo
    fence="false" rspace="0.167em" stretchy="false" >&#124;</mo><mi >d</mi><msub ><mi
    >x</mi></msub><mo >−</mo><mover accent="true" ><mi >d</mi><mo >^</mo></mover><msub
    ><mi >x</mi></msub><mo fence="false" stretchy="false" >&#124;</mo><mo lspace="0.167em"
    >=</mo><mn >2</mn></mtd></mtr><mtr ><mtd columnalign="left" ><mn >0</mn></mtd><mtd
    columnalign="left" ><mtext >其他情况。</mtext></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >Q(d_{x},\hat{d}_{x})=\left\{\begin{tabular}[]{ll}$\lambda_{1}><\text{如果
    } $d_{x}=\hat{d}_{x}$\\ $\lambda_{2}><\text{如果 } $&#124;d_{x}-\hat{d}_{x}&#124;=1$\\
    $\lambda_{3}><\text{如果 } $&#124;d_{x}-\hat{d}_{x}&#124;=2$\\ $0><\text{其他情况。 }\\
    \end{tabular}\right.</annotation></semantics></math> |  | (15) |
- en: Luo *et al.* [[19](#bib.bib19)] set $\lambda_{1}=0.5$, $\lambda_{2}=0.2$, and
    $\lambda_{3}=0.05$.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Luo *等人* [[19](#bib.bib19)] 设置 $\lambda_{1}=0.5$，$\lambda_{2}=0.2$ 和 $\lambda_{3}=0.05$。
- en: '(4) The sub-pixel cross-entropy loss $\mathcal{L}_{1}^{7}$. This loss, introduced
    by Tulyakov *et al.* [[60](#bib.bib60)], enables faster convergence and better
    accuracy at the sub-pixel level. It is defined using a discretized Laplace distribution
    centered at the ground-truth disparity:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 子像素交叉熵损失 $\mathcal{L}_{1}^{7}$。这一损失函数由 Tulyakov *等人* [[60](#bib.bib60)]
    提出，能够在子像素级别上实现更快的收敛和更好的精度。它通过一个以真实视差为中心的离散拉普拉斯分布来定义：
- en: '|  | $\small{Q(d_{x},\hat{d}_{x})=\frac{1}{Z}e^{-\frac{1}{b}&#124;d_{x}-\hat{d}_{x}&#124;},\text{
    }Z=\sum_{d}e^{-\frac{1}{b}&#124;d_{x}-d&#124;}.}$ |  | (16) |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{Q(d_{x},\hat{d}_{x})=\frac{1}{Z}e^{-\frac{1}{b}&#124;d_{x}-\hat{d}_{x}&#124;},\text{
    }Z=\sum_{d}e^{-\frac{1}{b}&#124;d_{x}-d&#124;}.}$ |  | (16) |'
- en: Here, $d_{x}$ is the ground-truth disparity at pixel $x$ and $\hat{d}_{x}$ is
    the estimated disparity at the same pixel.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$d_{x}$ 是像素 $x$ 处的真实视差，而 $\hat{d}_{x}$ 是同一像素的估计视差。
- en: '(5) The hinge loss criterion [[18](#bib.bib18), [21](#bib.bib21)]. It is computed
    by considering pairs of examples centered around the same image position where
    one example belongs to the positive and one to the negative class. Let $s_{+}$
    be the output of the network for the positive example, $s_{-}$ be the output of
    the network for the negative example, and let $m$, the margin, be a positive real
    number. The hinge loss for that pair of examples is defined as:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 合页损失准则 [[18](#bib.bib18), [21](#bib.bib21)]。它是通过考虑围绕相同图像位置的样本对来计算的，其中一个样本属于正类，另一个样本属于负类。设
    $s_{+}$ 为网络对正例的输出，$s_{-}$ 为网络对负例的输出，$m$ 为正实数的边际。该对样本的合页损失定义为：
- en: '|  | $\mathcal{L}_{1}^{8}=max(0,m+s_{-}-s_{+}).$ |  | (17) |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{1}^{8}=max(0,m+s_{-}-s_{+}).$ |  | (17) |'
- en: It is zero when the similarity of the positive example is greater than the similarity
    of the negative example by at least the margin $m$, which is set to $0.2$ in [[18](#bib.bib18)].
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 当正例的相似性大于负例的相似性至少 $m$ 时，该损失为零，其中 $m$ 在 [[18](#bib.bib18)] 中设置为 $0.2$。
- en: '(6) The re-projection (inverse warping) loss. Obtaining 3D ground truth data
    is very expensive. To overcome this issue, some techniques measure the loss based
    on the re-projection error. The rational is that if the estimated disparity/depth
    map is as close as possible to the ground truth, then the discrepancy between
    the reference image and any of the other images but unprojected using the estimated
    depth map onto the reference image, is also minimized. It can be defined in terms
    of the photometric error [[98](#bib.bib98), [95](#bib.bib95)], also called per-pixel
    $L_{1}$ loss [[14](#bib.bib14)], or image reconstruction error [[29](#bib.bib29)].
    It is defined as the $L_{1}$ norm between the reference image $I_{ref}$ and $\tilde{I}_{t}$,
    which is $I_{t}$ but unwarped onto $I_{ref}$ using the camera parameters:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 重新投影（逆变换）损失。获取 3D 地面真实数据非常昂贵。为了解决这个问题，一些技术基于重新投影误差来衡量损失。其原理是，如果估计的视差/深度图尽可能接近地面真实值，那么参考图像和任何其他图像之间的差异，除了通过估计的深度图未投影到参考图像上的部分，也会最小化。它可以通过光度误差
    [[98](#bib.bib98), [95](#bib.bib95)] 定义，也称为每像素 $L_{1}$ 损失 [[14](#bib.bib14)]，或图像重建误差
    [[29](#bib.bib29)]。它定义为参考图像 $I_{ref}$ 和 $\tilde{I}_{t}$ 之间的 $L_{1}$ 范数，其中 $I_{t}$
    是未通过相机参数逆变换到 $I_{ref}$ 上的：
- en: '|  | $\mathcal{L}_{1}^{9}=\frac{1}{N}\sum_{x}\&#124;I_{ref}(x)-\tilde{I}_{t}(x)\&#124;_{1}.$
    |  | (18) |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{1}^{9}=\frac{1}{N}\sum_{x}\&#124;I_{ref}(x)-\tilde{I}_{t}(x)\&#124;_{1}.$
    |  | (18) |'
- en: 'It can also be defined using the distance between the features f of the reference
    image and the features $\tilde{\textbf{f}}_{t}$ of any of the other images but
    unwarped onto the view of the reference image using the camera parameters and
    the computed depth map [[25](#bib.bib25)]:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 它也可以通过参考图像的特征 f 和任何其他图像的特征 $\tilde{\textbf{f}}_{t}$ 之间的距离来定义，但这些其他图像需要通过相机参数和计算得到的深度图将其逆变换到参考图像的视图上
    [[25](#bib.bib25)]：
- en: '|  | $\mathcal{L}_{1}^{10}=\frac{1}{N}\sum_{x}\&#124;\textbf{f}_{(}x)-\tilde{\textbf{f}}_{t}(x)\&#124;_{1}.$
    |  | (19) |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{1}^{10}=\frac{1}{N}\sum_{x}\&#124;\textbf{f}_{(}x)-\tilde{\textbf{f}}_{t}(x)\&#124;_{1}.$
    |  | (19) |'
- en: 'Other terms can be added to the re-projection loss. Examples include the $L_{1}$
    difference between the gradients of $I_{ref}$ and the gradient of $\tilde{I}_{t}$ [[29](#bib.bib29)]:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 其他项可以添加到重新投影损失中。例如，$I_{ref}$ 的梯度与 $\tilde{I}_{t}$ 的梯度之间的 $L_{1}$ 差异 [[29](#bib.bib29)]：
- en: '|  | $\mathcal{L}_{1}^{11}=\frac{1}{N}\sum_{x}\&#124;\nabla I_{ref}(x)-\tilde{I}_{t}(x)\&#124;_{1},$
    |  | (20) |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{1}^{11}=\frac{1}{N}\sum_{x}\&#124;\nabla I_{ref}(x)-\tilde{I}_{t}(x)\&#124;_{1},$
    |  | (20) |'
- en: and the structural dissimilarity between patches in $I_{ref}$ and in $\tilde{I}_{t}$ [[29](#bib.bib29),
    [99](#bib.bib99)]. We denote this loss by $\mathcal{L}_{1}^{12}$.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 以及 $I_{ref}$ 和 $\tilde{I}_{t}$ 中补丁之间的结构差异 [[29](#bib.bib29), [99](#bib.bib99)]。我们用
    $\mathcal{L}_{1}^{12}$ 来表示这个损失。
- en: '(7) Matching loss. Some methods, *e.g.,* [[18](#bib.bib18), [20](#bib.bib20),
    [21](#bib.bib21)], train the feature matching network separately from the subsequent
    disparity computation and refinement blocks, using different loss functions. Chen
    *et al.* [[20](#bib.bib20)] use a loss that measures the $L_{2}$ distance between
    the predicted matching score and the ground-truth score:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: (7) 匹配损失。一些方法，例如 [[18](#bib.bib18), [20](#bib.bib20), [21](#bib.bib21)]，将特征匹配网络与随后的视差计算和细化块分开训练，使用不同的损失函数。Chen
    *et al.* [[20](#bib.bib20)] 使用一种损失函数来衡量预测匹配分数与真实分数之间的 $L_{2}$ 距离：
- en: '|  | $\mathcal{L}_{1}^{13}=\&#124;\text{predicted\_score}(x,d)-\text{label}(x,d)\&#124;,$
    |  | (21) |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{1}^{13}=\&#124;\text{predicted\_score}(x,d)-\text{label}(x,d)\&#124;,$
    |  | (21) |'
- en: where $\text{label}(x,d)\in\{0,1\}$ is the ground-truth label indicating whether
    the pixel $x(i,j)$ on the left image corresponds to the pixel $(i-d,j)$ on the
    right image, and $\text{predicted\_score}(x,d)$ is the predicted matching score
    for the same pair of pixels.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\text{label}(x,d)\in\{0,1\}$ 是表示像素 $x(i,j)$ 是否对应于右图像上的像素 $(i-d,j)$ 的真实标签，而
    $\text{predicted\_score}(x,d)$ 是相同像素对的预测匹配分数。
- en: (8) The semantic loss. Some papers incorporate semantic cues, *e.g.,* segmentation [[25](#bib.bib25)]
    and edge [[31](#bib.bib31)] maps, to guide the depth/disparity estimation. These
    can be either provided at the outset, *e.g.,* estimated with a separate method
    as in [[31](#bib.bib31)], or estimated jointly with the depth/disparity map using
    the same network trained end-to-end. The latter case requires a semantic loss.
    For instance, Yang *et al.* [[25](#bib.bib25)], which use segmentation as semantics,
    define the semantic loss $\mathcal{L}_{1}^{13}$ as the distance between the classified
    warped maps and ground-truth labels. Song *et al.* [[31](#bib.bib31)], which use
    edge probability map as semantics, define the semantic loss as follows;
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: (8) 语义损失。一些论文结合了语义提示，如分割 [[25](#bib.bib25)] 和边缘 [[31](#bib.bib31)] 图，以指导深度/视差估计。这些可以在开始时提供，例如，像
    [[31](#bib.bib31)] 中使用单独的方法估计，或者使用同一网络端到端联合估计深度/视差图。后一种情况需要语义损失。例如，Yang *等* [[25](#bib.bib25)]
    使用分割作为语义，将语义损失 $\mathcal{L}_{1}^{13}$ 定义为分类后的扭曲图与真实标签之间的距离。Song *等* [[31](#bib.bib31)]
    使用边缘概率图作为语义，将语义损失定义如下：
- en: '|  | $\small{\mathcal{L}_{1}^{14}=\frac{1}{N}\sum_{x}\left\{&#124;\partial_{u}d_{x}&#124;e^{-&#124;\partial_{u}\xi_{x}&#124;}+&#124;\partial_{v}d_{x}&#124;e^{-&#124;\partial_{v}\xi_{x}&#124;}\right\},}$
    |  | (22) |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{\mathcal{L}_{1}^{14}=\frac{1}{N}\sum_{x}\left\{&#124;\partial_{u}d_{x}&#124;e^{-&#124;\partial_{u}\xi_{x}&#124;}+&#124;\partial_{v}d_{x}&#124;e^{-&#124;\partial_{v}\xi_{x}&#124;}\right\},}$
    |  | (22) |'
- en: where $x=(u,v)$ and $\xi$ is the edge probability map.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x=(u,v)$ 且 $\xi$ 是边缘概率图。
- en: 5.2.2 The regularization term
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 正则化项
- en: 'In general, one can make many assumptions about the disparity/depth map and
    incorporate them into the regularization term of Equation ([7](#S5.E7 "In 5.2
    Loss functions ‣ 5 Training ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction")). Examples of constraints include: smoothness [[29](#bib.bib29)],
    left-right consistency [[29](#bib.bib29)], maximum depth [[29](#bib.bib29)], and
    scale-invariant gradient loss [[15](#bib.bib15)]. The regularization term can
    then be formed using a weighted sum of these losses.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，可以对视差/深度图做出许多假设，并将其纳入方程 ([7](#S5.E7 "In 5.2 Loss functions ‣ 5 Training ‣
    A Survey on Deep Learning Architectures for Image-based Depth Reconstruction"))
    的正则化项中。约束的例子包括：平滑性 [[29](#bib.bib29)]、左右一致性 [[29](#bib.bib29)]、最大深度 [[29](#bib.bib29)]
    和尺度不变梯度损失 [[15](#bib.bib15)]。然后，可以使用这些损失的加权和来形成正则化项。
- en: '(1) Smoothness. It can be measured using the magnitude of the first or second-order
    gradient of the estimated disparity/depth map. For instance, Yang *et al.* [[25](#bib.bib25)]
    used the $L_{1}$ norm of the first-order gradient:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 平滑性。可以通过估计的视差/深度图的第一阶或第二阶梯度的大小来测量。例如，Yang *等* [[25](#bib.bib25)] 使用了第一阶梯度的
    $L_{1}$ 范数：
- en: '|  | $\small{\mathcal{L}_{2}^{1}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}d_{x})+(\nabla_{v}d_{x})\right\},x=(u,v).}$
    |  | (23) |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{\mathcal{L}_{2}^{1}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}d_{x})+(\nabla_{v}d_{x})\right\},x=(u,v).}$
    |  | (23) |'
- en: 'Here $\nabla$ is the gradient operator. Zhou *et al.* [[95](#bib.bib95)] and
    Vijayanarasimhan *et al.* [[100](#bib.bib100)] define smoothness as the $L_{2}$
    norm of the second-order gradient:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $\nabla$ 是梯度算子。Zhou *等* [[95](#bib.bib95)] 和 Vijayanarasimhan *等* [[100](#bib.bib100)]
    将平滑性定义为二阶梯度的 $L_{2}$ 范数：
- en: '|  | $\mathcal{L}_{2}^{2}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}^{2}d_{x})^{2}+(\nabla_{v}^{2}d_{x})^{2}\right\}.$
    |  | (24) |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{2}^{2}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}^{2}d_{x})^{2}+(\nabla_{v}^{2}d_{x})^{2}\right\}.$
    |  | (24) |'
- en: 'Zhong *et al.* [[29](#bib.bib29)] used the second-order gradient but weighted
    with the image’s second-order gradients:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: Zhong *等* [[29](#bib.bib29)] 使用了二阶梯度，但加权的是图像的二阶梯度：
- en: '|  | $\small{\mathcal{L}_{2}^{3}=\frac{1}{N}\sum\left\{&#124;\nabla_{u}^{2}d_{x}&#124;e^{-&#124;\nabla_{u}^{2}I_{left}(x)&#124;}+&#124;\nabla_{v}^{2}d_{x}&#124;e^{-&#124;\nabla_{v}^{2}I_{left}(x)&#124;}\right\}.}$
    |  | (25) |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{\mathcal{L}_{2}^{3}=\frac{1}{N}\sum\left\{&#124;\nabla_{u}^{2}d_{x}&#124;e^{-&#124;\nabla_{u}^{2}I_{left}(x)&#124;}+&#124;\nabla_{v}^{2}d_{x}&#124;e^{-&#124;\nabla_{v}^{2}I_{left}(x)&#124;}\right\}.}$
    |  | (25) |'
- en: 'Finally, Tonioni *et al.* [[96](#bib.bib96)] define the smoothness at a pixel
    $x$ as the absolute difference between the disparity predicted at $x$ and those
    predicted at each pixel $y$ within a certain predefined neighborhood $\mathcal{N}_{x}$
    around the pixel $x$. This is then averaged over all pixels:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Tonioni *等人* [[96](#bib.bib96)] 将像素 $x$ 的平滑度定义为在 $x$ 处预测的视差与在像素 $x$ 周围某个预定义邻域
    $\mathcal{N}_{x}$ 内每个像素 $y$ 预测的视差之间的绝对差异。然后对所有像素取平均：
- en: '|  | $\mathcal{L}_{2}^{4}=\frac{1}{N}\sum_{x}\sum_{y\in\mathcal{N}_{x}}&#124;d_{x}-d_{y}&#124;.$
    |  | (26) |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{2}^{4}=\frac{1}{N}\sum_{x}\sum_{y\in\mathcal{N}_{x}}|d_{x}-d_{y}|.$
    |  | (26) |'
- en: '(2) Consistency. Zhong *et al.* [[29](#bib.bib29)] introduced the loop-consistency
    loss, which is constructed as follows; Consider the left image $I_{left}$ and
    the synthesized image $\tilde{I}_{left}$ obtained by warping the right image to
    the left image coordinate with the disparity map defined on the right image. A
    second synthesized left image $\tilde{\tilde{I}}$ is generated by warping the
    left image to the right image coordinates by using the disparities at the left
    and right images, respectively. The loop consistency loss consistency term is
    then defined as:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 一致性。钟*等人* [[29](#bib.bib29)] 提出了循环一致性损失，其构造如下：考虑左侧图像 $I_{left}$ 和通过将右侧图像变形到左侧图像坐标的合成图像
    $\tilde{I}_{left}$，该变形使用定义在右侧图像上的视差图。通过使用左侧和右侧图像上的视差，生成第二个合成左侧图像 $\tilde{\tilde{I}}$。然后，循环一致性损失的一致性项定义为：
- en: '|  | $\mathcal{L}_{2}^{5}=&#124;\tilde{\tilde{I}}-I_{left}&#124;.$ |  | (27)
    |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{2}^{5}=|\tilde{\tilde{I}}-I_{left}|.$ |  | (27) |'
- en: Godard *et al.* [[63](#bib.bib63)] introduced the left-right consistency term,
    which attempts to make the left-view disparity map be equal to the projected right-view
    disparity map. It can be seen as a linear approximation of the loop consistency
    and is defined as follows;
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: Godard *等人* [[63](#bib.bib63)] 提出了左右一致性项，它试图使左视图的视差图等于投影的右视图的视差图。这可以视为循环一致性的线性近似，其定义如下：
- en: '|  | $\mathcal{L}_{2}^{6}=\frac{1}{N}\sum_{x}&#124;d_{x}-\tilde{d}_{x}&#124;,$
    |  | (28) |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{2}^{6}=\frac{1}{N}\sum_{x}|d_{x}-\tilde{d}_{x}|,$ |  | (28)
    |'
- en: where $\tilde{d}$ is the disparity at the right image but reprojected onto the
    coordinates of the left image.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{d}$ 是右侧图像上的视差，但重新投影到左侧图像的坐标上。
- en: '(3) Maximum-depth heuristic. There may be multiple warping functions that achieve
    similar warping loss, especially for textureless areas. To provide strong regularization
    in these areas, Zhong *et al.* [[29](#bib.bib29)] use the Maximum-Depth Heuristic
    (MDH) [[101](#bib.bib101)], which is defined as the sum of all depths/disparities:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 最大深度启发式。可能存在多个实现类似变形损失的变形函数，尤其是在无纹理区域。为在这些区域提供强有力的正则化，钟*等人* [[29](#bib.bib29)]
    使用最大深度启发式（MDH） [[101](#bib.bib101)]，其定义为所有深度/视差的总和：
- en: '|  | $\mathcal{L}_{2}^{7}=\frac{1}{N}\sum_{x}&#124;d_{x}&#124;.$ |  | (29)
    |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{2}^{7}=\frac{1}{N}\sum_{x}|d_{x}|.$ |  | (29) |'
- en: '(4) Scale-invariant gradient loss [[15](#bib.bib15)], defined as:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 尺度不变梯度损失 [[15](#bib.bib15)]，定义为：
- en: '|  | $\mathcal{L}_{2}^{8}=\sum_{h\in A}\sum_{x}\&#124;g_{h}[D](x)-g_{h}[\hat{D}](x)\&#124;_{2},$
    |  | (30) |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{2}^{8}=\sum_{h\in A}\sum_{x}|g_{h}[D](x)-g_{h}[\hat{D}](x)|_{2},$
    |  | (30) |'
- en: where $A=\{1,2,4,8,16\}$, $x=(i,j)$, $f_{i,j}\equiv f(i,j)$, and
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A=\{1,2,4,8,16\}$，$x=(i,j)$，$f_{i,j}\equiv f(i,j)$，并且
- en: '|  | $\small{g_{h}[f](i,j)=\left(\frac{f_{i+h,j}-f_{i,j}}{&#124;f_{i+h,j}-f_{i,j}&#124;},\frac{f_{i,j+h}-f_{i,j}}{&#124;f_{i,j+h}-f_{i,j}&#124;}\right)^{\top}.}$
    |  | (31) |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{g_{h}[f](i,j)=\left(\frac{f_{i+h,j}-f_{i,j}}{|f_{i+h,j}-f_{i,j}|},\frac{f_{i,j+h}-f_{i,j}}{|f_{i,j+h}-f_{i,j}|}\right)^{\top}.}$
    |  | (31) |'
- en: 5.3 Degree of supervision
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 监督程度
- en: Supervised methods for depth estimation, which have achieved promising results,
    rely on large quantities of ground truth depth data. However, obtaining ground-truth
    depth data, either manually or using traditional stereo matching algorithms or
    3D scanning devices, *e.g.,* Kinect, is extremely difficult and expensive, and
    is prune to noise and inaccuracies. Several mechanisms have been recently proposed
    in the literature to make the 3D supervision as light as possible. Below, we discuss
    the most important ones.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 深度估计的监督方法已经取得了令人满意的结果，但它们依赖于大量的真实深度数据。然而，无论是手动获取、使用传统立体匹配算法还是3D扫描设备，如 Kinect，都非常困难且昂贵，并且容易受到噪声和不准确性的影响。最近文献中提出了几种机制，以尽可能减轻3D监督的负担。下面，我们讨论了其中最重要的几种。
- en: 5.3.1 Supervision with stereo images
  id: totrans-403
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 使用立体图像的监督
- en: Godard *et al.* [[63](#bib.bib63)] exploit the left-right consistency to perform
    unsupervised depth estimation from a monocular image. The approach is trained,
    without 3D supervision, using stereo pairs. At runtime, it only requires one input
    image and returns the disparity map from the same view as the input image. For
    this, the approach uses the left-right consistency loss of Equation ([28](#S5.E28
    "In 5.2.2 The regularization term ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey
    on Deep Learning Architectures for Image-based Depth Reconstruction")), which
    attempts to make the left-view disparity map be equal to the projected right-view
    disparity map.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: Godard *等人* [[63](#bib.bib63)] 利用左右一致性从单目图像中进行无监督深度估计。这种方法在没有 3D 监督的情况下，使用立体对进行训练。在运行时，它只需要一张输入图像，并返回与输入图像相同视角的视差图。为此，该方法使用方程
    ([28](#S5.E28 "在 5.2.2 正则化项 ‣ 5.2 损失函数 ‣ 5 训练 ‣ 关于基于图像的深度重建的深度学习架构综述")) 中的左右一致性损失，尝试使左视图的视差图等于投影的右视图视差图。
- en: 'Tonioni *et al.*[[96](#bib.bib96)] fine-tune pre-trained networks without any
    3D supervision by using stereo pairs. The idea is leverage on traditional stereo
    algorithms and state-of-the-art confidence measures in order to fine-tune a deep
    stereo model based on disparities provided by standard stereo algorithms that
    are deemed as highly reliable by the confidence measure. This is done by minimizing
    a loss function made out of two terms: a confidence-guided loss and a smoothing
    term.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: Tonioni *等人* [[96](#bib.bib96)] 在没有任何 3D 监督的情况下，通过使用立体对微调预训练网络。其思想是利用传统的立体算法和最先进的置信度测量，以基于标准立体算法提供的视差来微调深度立体模型，这些视差被置信度测量认为是高度可靠的。这是通过最小化由两个项组成的损失函数来完成的：置信度引导的损失和光滑项。
- en: Note that while stereo-based supervision does not require ground-truth 3D labels,
    these techniques usually rely on the availability of calibrated stereo pairs during
    training.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然基于立体的监督不需要真实的 3D 标签，但这些技术通常依赖于训练过程中使用的标定立体对。
- en: 5.3.2 Supervision with camera’s aperture
  id: totrans-407
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 使用相机光圈进行监督
- en: Srinivasan *et al.* [[102](#bib.bib102)]’s approach uses as supervision the
    information provided by a camera’s aperture. It introduces two differentiable
    aperture rendering functions that use the input image and the predicted depths
    to simulate depth-of-field effects caused by real camera apertures. The depth
    estimation network is trained end-to-end to predict the scene depths that best
    explain these finite aperture images as defocus-blurred renderings of the input
    all-in-focus image.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: Srinivasan *等人* [[102](#bib.bib102)] 的方法使用相机光圈提供的信息作为监督。它引入了两个可微分的光圈渲染函数，这些函数利用输入图像和预测深度来模拟由真实相机光圈引起的景深效果。深度估计网络进行端到端训练，以预测最能解释这些有限光圈图像作为输入全景图像的失焦模糊渲染的场景深度。
- en: 5.3.3 Training with relative/ordinal depth annotation
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 使用相对/序深度注释进行训练
- en: 'People, in general, are better at judging relative depth [[103](#bib.bib103)],
    *i.e.,* assessing whether a point $A$ is closer than point $B$. Chen *et al.* [[67](#bib.bib67)]
    introduced an algorithm for learning to estimate metric depth using only annotations
    of relative depths. In this approach each image is annotated with only the ordinal
    relation between a pair of pixels, *i.e.,* point $A$ is closer to point $B$, further
    than $B$, or it is hard to tell. To train the network, a ConvNet in this case,
    using such ordinal relations, Chen *et al.* [[67](#bib.bib67)] introduced an improved
    ranking loss, which encourages the predicted depth to agree with the ground-truth
    ordinal relations. It is defined as:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，人们在判断相对深度上更擅长[[103](#bib.bib103)]，*即*，评估点 $A$ 是否比点 $B$ 更接近。Chen *等人* [[67](#bib.bib67)]
    提出了一个仅使用相对深度注释来学习估计度量深度的算法。在这种方法中，每张图像仅通过一对像素之间的序关系进行注释，*即*，点 $A$ 比点 $B$ 更接近、远离
    $B$，或者很难判断。为了使用这些序关系训练网络（在这种情况下是 ConvNet），Chen *等人* [[67](#bib.bib67)] 引入了一种改进的排名损失，旨在使预测的深度与真实的序关系一致。其定义为：
- en: '|  | $\mathcal{L}_{1}^{15}(\hat{D},\Theta,W)=\sum_{i=1}{N}\omega_{k}\mathcal{L}_{k}(I,x_{k},y_{k},l_{k},\hat{d}),$
    |  | (32) |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{1}^{15}(\hat{D},\Theta,W)=\sum_{i=1}^{N}\omega_{k}\mathcal{L}_{k}(I,x_{k},y_{k},l_{k},\hat{d}),$
    |  | (32) |'
- en: 'where $\omega_{k}$, $l_{k}$, and $\mathcal{L}_{k}$ are, respectively, the weight,
    the label, and loss of the $k-$th pair $(x_{k},y_{i})$ defined as:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\omega_{k}$、$l_{k}$ 和 $\mathcal{L}_{k}$ 分别是第 $k-$ 对 $(x_{k},y_{i})$ 的权重、标签和损失，其定义为：
- en: '|  | $\small{\mathcal{L}_{k}=\left\{\begin{tabular}[]{ll}$\log(1+\text{exp}((-\hat{d}_{xk}+\hat{d}_{yk})l_{k})$,&amp;if
    $l_{k}\neq 0$,\\ $(\hat{d}_{xk}-\hat{d}_{yk})^{2}$,&amp;otherwise.\end{tabular}\right.}$
    |  | (33) |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{\mathcal{L}_{k}=\left\{\begin{tabular}[]{ll}$\log(1+\text{exp}((-\hat{d}_{xk}+\hat{d}_{yk})l_{k})$,&amp;if
    $l_{k}\neq 0$,\\ $(\hat{d}_{xk}-\hat{d}_{yk})^{2}$,&amp;otherwise.\end{tabular}\right.}$
    |  | (33) |'
- en: Xian *et al.* [[104](#bib.bib104)] showed that training with only one pair of
    ordinal relation for each image is not sufficient to get satisfactory results.
    They then extended this approach by annotating each image with 3K pairs and showed
    that they can achieve substantial improvement accuracy. The challenge, however,
    is how to cheaply get such large number of relative ordinal annotations. They
    propose to use optical flow maps from web stereo images. Note that at runtime,
    both methods take a single image of size $384\times 384$ and output a dense depth
    map of the same size.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: Xian *et al.* [[104](#bib.bib104)] 发现仅使用一对顺序关系进行训练不足以获得令人满意的结果。他们通过为每张图像标注 3K
    对关系来扩展了这种方法，并显示出显著的准确性提升。然而，挑战在于如何以低成本获得如此大量的相对顺序注释。他们建议使用来自网络立体图像的光流图。请注意，在运行时，这两种方法都接受尺寸为
    $384\times 384$ 的单张图像，并输出相同尺寸的密集深度图。
- en: 5.3.4 Domain adaptation and transfer learning
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.4 领域适应和迁移学习
- en: Supervised deep learning often suffers from the lack of sufficient training
    data. Also, when using range sensors, noise is often present and the measurements
    can be very sparse. Kuznietsov *et al.* [[105](#bib.bib105)] propose an approach
    to depth map prediction from monocular images that learns in a semi-supervised
    way. The ida is to use sparse ground-truth depth for supervised learning, while
    enforcing the deep network to produce photoconsistent dense depth maps in a stereo
    setup using a direct image alignment / reprojection loss.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 监督式深度学习通常受限于训练数据的不足。此外，当使用范围传感器时，噪声经常存在，测量可能非常稀疏。Kuznietsov *et al.* [[105](#bib.bib105)]
    提出了一个从单目图像中预测深度图的方法，该方法以半监督的方式进行学习。其思路是使用稀疏的地面真实深度进行监督学习，同时通过直接图像对齐/重投影损失来强制深度网络在立体设置中生成光一致性的密集深度图。
- en: While obtaining ground-truth depth annotations of real images is challenging
    and time consuming, synthetic images with their corresponding depth maps can be
    easily generated using computer graphics techniques. However, the domain of real
    images is different from the domain of graphics-generated images. Recently, several
    domain adaptation strategies have been proposed to solve this domain bias issue.
    These allow training on synthetic data and transfer what has been learned to the
    domain of real images.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管获取真实图像的地面真实深度注释既具挑战性又耗时，但可以使用计算机图形技术轻松生成对应深度图的合成图像。然而，真实图像的领域与图形生成图像的领域不同。最近，提出了几种领域适应策略来解决这种领域偏差问题。这些策略允许在合成数据上进行训练，并将学到的知识转移到真实图像的领域。
- en: Domain adaptation methods for depth estimation can be classified into two categories.
    Methods in the first category transform the data of one domain to look similar
    in style to the data in the other domain. For example, Atapour-Abarghoue *et al.* [[106](#bib.bib106)]
    proposed a two-staged approach. The first stage includes training a depth estimation
    model using synthetic data. The second stage is trained to transfer the style
    of synthetic images to real-world images. By doing so, the style of real images
    is first transformed to match the style of synthetic data and then fed into the
    depth estimation network, which has been trained on synthetic data. Zheng *et
    al.* [[107](#bib.bib107)] performed the opposite; it transforms the synthetic
    images to become more realistic and use them to train the depth estimation network.
    Guo *et al.* [[108](#bib.bib108)], on the other hand, trains a stereo matching
    network using synthetic data to predict occlusion maps and disparity maps of stereo
    image pairs. In a second step, a monocular depth estimation network is trained
    on real data by distilling the knowledge of the stereo network [[109](#bib.bib109)].
    These methods use adversarial learning.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 深度估计的领域适应方法可以分为两类。第一类方法将一个领域的数据转换为与另一个领域的数据风格相似。例如，Atapour-Abarghoue *et al.* [[106](#bib.bib106)]
    提出了一个两阶段的方法。第一阶段包括使用合成数据训练深度估计模型。第二阶段训练以将合成图像的风格转移到真实图像中。通过这样做，首先将真实图像的风格转换为匹配合成数据的风格，然后将其输入到已在合成数据上训练的深度估计网络中。Zheng
    *et al.* [[107](#bib.bib107)] 进行相反的操作；它将合成图像转换为更具现实感的图像，并使用这些图像来训练深度估计网络。另一方面，Guo
    *et al.* [[108](#bib.bib108)] 使用合成数据训练立体匹配网络，以预测立体图像对的遮挡图和视差图。在第二步中，通过提炼立体网络的知识，在真实数据上训练单目深度估计网络 [[109](#bib.bib109)]。这些方法使用对抗学习。
- en: Methods in the second class operate on the network architecture and the loss
    functions used for their training. Kundu *et al.* [[110](#bib.bib110)] introduced
    AdaDepth, an unsupervised mechanism for domain adaptation. The approach uses an
    encoder-decoder architecture of the form $\hat{D}=g\left(h(\textbf{I}_{s})\right)$.
    It is first trained, in a supervised manner, using synthetic data $\textbf{I}_{s}$.
    Let $g_{s}$ and $h_{s}$ be the decoding and encoding functions learned from synthetic
    data. Let $g_{t}$ and $h_{t}$ be the decoding and encoding functions that correspond
    to real data $\textbf{I}_{t}$. Kundu *et al.* [[110](#bib.bib110)] assume that
    $g_{t}=g_{s}=g$. Its goal is to match the distributions of the latent representations
    generated by $h_{s}$ and $h_{t}$. This is done by initializing the network with
    the weights that have been learned using synthetic data. It then uses adversarial
    learning to minimize an objective function that discriminates between $g_{t}(\textbf{I}_{t})$
    and $g_{s}(\textbf{I}_{s})$, and another objective function that discriminates
    between $\hat{0}pt_{s}$ and $g(h_{t}(\textbf{I}_{t}))$. The former ensures that
    real data, when fed to the encoder, are mapped to the same latent space as the
    one learned during training. The latter ensures that inferences through the corresponding
    transformation functions $g(h_{s}(\cdot))$ and $g(h_{t}(\cdot))$ are directed
    towards the same output density function.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类方法作用于网络架构和用于训练的损失函数。Kundu *et al.* [[110](#bib.bib110)] 引入了 AdaDepth，这是一种用于领域适应的无监督机制。该方法使用形式为
    $\hat{D}=g\left(h(\textbf{I}_{s})\right)$ 的编码器-解码器架构。它首先使用合成数据 $\textbf{I}_{s}$
    以监督方式进行训练。设 $g_{s}$ 和 $h_{s}$ 为从合成数据中学习到的解码和编码函数。设 $g_{t}$ 和 $h_{t}$ 为对应于真实数据
    $\textbf{I}_{t}$ 的解码和编码函数。Kundu *et al.* [[110](#bib.bib110)] 假设 $g_{t}=g_{s}=g$。其目标是使由
    $h_{s}$ 和 $h_{t}$ 生成的潜在表示的分布匹配。这是通过使用从合成数据中学习到的权重初始化网络来完成的。然后使用对抗学习来最小化区分 $g_{t}(\textbf{I}_{t})$
    和 $g_{s}(\textbf{I}_{s})$ 的目标函数，以及另一个区分 $\hat{0}pt_{s}$ 和 $g(h_{t}(\textbf{I}_{t}))$
    的目标函数。前者确保真实数据在输入到编码器时被映射到与训练期间学习的潜在空间相同的空间中。后者确保通过相应的变换函数 $g(h_{s}(\cdot))$ 和
    $g(h_{t}(\cdot))$ 进行的推断指向相同的输出密度函数。
- en: 6 Discussion and comparison
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论与比较
- en: This section discusses some state-of-the-art techniques using quantitative and
    qualitative performance criteria.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了一些最先进的技术，使用定量和定性性能标准。
- en: 6.1 Evaluation metrics and criteria
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 评估指标和标准
- en: The most commonly used quantitative metrics for evaluating the performance of
    a depth estimation algorithm include (the lower these metrics are the better);
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 评估深度估计算法性能时最常用的定量指标包括（这些指标值越低越好）；
- en: •
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Computation time at training and runtime. While one can afford large computation
    time during training, some applications may require realtime performance at runtime.
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练和运行时的计算时间。虽然在训练期间可以承受较长的计算时间，但一些应用可能要求在运行时具有实时性能。
- en: •
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Memory footprint. In general deep neural networks have a large number of parameters.
    Some of them operate on volumes using 3D convolutions. This would require large
    memory storage, which can affect their performance at runtime.
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存占用。在一般情况下，深度神经网络具有大量的参数。其中一些通过 3D 卷积在体积上操作。这将需要大量的内存存储，这可能会影响其在运行时的性能。
- en: •
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The End-Point Error (EPE). Called also geometric error, it is defined as the
    distance between the ground truth $D$ and the predicted disparity/depth $\hat{D}$,
    *i.e.,* $EPE(D,\hat{D})=\|D-\hat{D}\|$. This metric has two variants: Avg-Noc
    and Avg-ll. The former is measured in non-occluded areas while the latter is measured
    over the entire image.'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 端点误差（EPE）。也称为几何误差，它被定义为真实深度 $D$ 和预测的视差/深度 $\hat{D}$ 之间的距离，*即*， $EPE(D,\hat{D})=\|D-\hat{D}\|$。这个指标有两个变体：Avg-Noc
    和 Avg-ll。前者在非遮挡区域测量，而后者在整个图像上测量。
- en: •
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Percentage of Erroneous pixels (PE). It is defined as the percentage of pixels
    where the true and predicted disparity/depth differ with more than a predefined
    threshold $\epsilon$. Similar to the EPE, this error can be measured on non-occluded
    ares (Out-Noc) and over the entire image (Out-All).
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 错误像素的百分比（PE）。它被定义为真实和预测的视差/深度差异超过预定义阈值 $\epsilon$ 的像素百分比。类似于 EPE，这种误差可以在非遮挡区域（Out-Noc）和整个图像（Out-All）上进行测量。
- en: •
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The bad pixel error (D1). It is defined as the percentage of disparity/depth
    errors below a threshold. This metric is computed in non-occluded (Noc) and in
    all pixels (All), in background (bg) and in foreground (fg) pixels.
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 坏像素误差（D1）。它被定义为视差/深度误差低于阈值的百分比。这个指标在非遮挡（Noc）和所有像素（All）、背景（bg）和前景（fg）像素中进行计算。
- en: •
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Absolute relative difference. It is defined as the average over all the image
    pixels of the $L_{1}$ distance between the groud-truth and the estimate depth/disparity,
    but scaled by the estimated depth/disparity:'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绝对相对差异。它被定义为所有图像像素上真实深度和估计深度/视差之间的 $L_{1}$ 距离的平均值，但以估计深度/视差进行缩放：
- en: '|  | $\text{Abs rel. diff}=\frac{1}{N}\sum_{N}\frac{&#124;0pt_{i}-\hat{0}pt_{i}&#124;}{\hat{0}pt_{i}}.$
    |  | (34) |'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{Abs rel. diff}=\frac{1}{N}\sum_{N}\frac{\|0pt_{i}-\hat{0}pt_{i}\|}{\hat{0}pt_{i}}.$
    |  | (34) |'
- en: •
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Squared relative difference. It is defined as the average over all the image
    pixels of the $L_{2}$ distance between the groud-truth and the estimate depth
    / disparity, but scaled by the estimated depth/disparity:'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平方相对差异。它被定义为所有图像像素上真实深度和估计深度/视差之间的 $L_{2}$ 距离的平均值，但以估计深度/视差进行缩放：
- en: '|  | $\text{Abs rel. diff}=\frac{1}{N}\sum_{N}\frac{&#124;0pt_{i}-\hat{0}pt_{i}&#124;^{2}}{\hat{0}pt_{i}}.$
    |  | (35) |'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{Abs rel. diff}=\frac{1}{N}\sum_{N}\frac{\|0pt_{i}-\hat{0}pt_{i}\|^{2}}{\hat{0}pt_{i}}.$
    |  | (35) |'
- en: •
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The linear Root Mean Square Error. It is defined as follows:'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性均方根误差。它被定义如下：
- en: '|  | $\text{RMSE(linear)}=\sqrt{\frac{1}{N}\sum_{N}{&#124;0pt_{i}-\hat{0}pt_{i}&#124;}^{2}}.$
    |  | (36) |'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{RMSE(linear)}=\sqrt{\frac{1}{N}\sum_{N}{\|0pt_{i}-\hat{0}pt_{i}\|^{2}}}.$
    |  | (36) |'
- en: •
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The log Root Mean Square Error. It is defined as follows:'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对数均方根误差。它被定义如下：
- en: '|  | $\text{RMSE(log)}=\sqrt{\frac{1}{N}\sum_{N}{&#124;\log 0pt_{i}-\log\hat{0}pt_{i}&#124;}^{2}}.$
    |  | (37) |'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{RMSE(log)}=\sqrt{\frac{1}{N}\sum_{N}{\|\log 0pt_{i}-\log\hat{0}pt_{i}\|^{2}}}.$
    |  | (37) |'
- en: The accuracy is generally evaluated using the following metrics (the higher
    these metrics are the better);
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性通常使用以下指标进行评估（这些指标越高越好）；
- en: •
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Maximum relative error. It is defined as the percentage of pixels $i$ such that
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大相对误差。它被定义为像素 $i$ 的百分比，其中
- en: '|  | $\max\left(\frac{0pt_{i}}{\hat{0}pt_{i}},\frac{\hat{0}pt_{i}}{0pt_{i}}\right)<\epsilon,$
    |  | (38) |'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\max\left(\frac{0pt_{i}}{\hat{0}pt_{i}},\frac{\hat{0}pt_{i}}{0pt_{i}}\right)<\epsilon,$
    |  | (38) |'
- en: where $\epsilon$ is a user-defined threshold. It is generally set to $1.25$,
    $1.25^{2}$, and $1.25^{3}$.
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\epsilon$ 是用户定义的阈值。通常设置为 $1.25$、$1.25^{2}$ 和 $1.25^{3}$。
- en: •
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Density. It is defined as the percentage of pixels for which depth has been
    estimated.
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 密度。它被定义为已估计深度的像素百分比。
- en: In addition to these quantitative metrics, there are several qualitative aspects
    to consider. Examples include;
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些定量指标外，还有几个定性方面需要考虑。例如：
- en: •
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Degree of 3D supervision. One important aspect of deep learning-based depth
    reconstruction methods is the degree of 3D supervision they require during training.
    In fact, while obtaining multiview stereo images is easy, obtaining their corresponding
    ground-truth depth maps and/or pixel-wise correspondences is quite challenging.
    As such, techniques that require minimal or no 3D supervision are usually preferred
    over those that require ground-truth depth maps during training.
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3D监督的程度。基于深度学习的深度重建方法的一个重要方面是它们在训练过程中所需的3D监督程度。实际上，尽管获取多视角立体图像很容易，但获取相应的真实深度图和/或逐像素对应关系却非常具有挑战性。因此，通常更倾向于那些在训练过程中需要最少或没有3D监督的技术，而不是那些需要真实深度图的技术。
- en: •
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: End-to-end training. In general, the depth estimation pipeline is composed of
    multiple blocks. In methods, these blocks are trained separately. Others train
    them jointly in an end-to-end fashion. Some these techniques include a deep learning-based
    refinement module. Others directly regress the final high resolution map without
    additional post-processing or regularization.
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 端到端训练。一般来说，深度估计管道由多个模块组成。在某些方法中，这些模块是分别训练的。其他方法则以端到端的方式联合训练这些模块。一些技术包括一个基于深度学习的细化模块。其他方法则直接回归最终的高分辨率图像，而无需额外的后处理或正则化。
- en: •
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sub-pixel accuracy. In general, its is desirable to achieve sub-pixel accuracy
    without any additional post-processing or regularization.
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 亚像素精度。一般来说，希望在没有额外的后处理或正则化的情况下实现亚像素精度。
- en: •
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Change in disparity range. This may require changing the network structure as
    well as re-training.
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视差范围的变化。这可能需要更改网络结构以及重新训练。
- en: We will use these metrics and criteria to compare and discuss existing methods.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些指标和标准来比较和讨论现有的方法。
- en: 6.2 Pairwise stereo matching techniques
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 对偶立体匹配技术
- en: Table [V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo matching techniques ‣ 6 Discussion
    and comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction") compares the properties and performance of deep learning-based
    depth estimation methods from stereo images. Below, we discuss some of them.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo matching techniques ‣ 6 Discussion
    and comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction") 比较了基于深度学习的立体图像深度估计方法的属性和性能。以下，我们讨论其中的一些。
- en: 'TABLE V: Performance comparison of deep learning-based stereo matching algorithms
    on the test set of KITTI 2015 benchmark (as of 2019/01/05). PE: percentage of
    erroneous pixels. EPE: End-Point Error. S1: the bad pixel error. Non-Occ: non-occluded
    pixels. All: all pixels. fg: foreground pixels. bg: background pixels. Noc: non-occluded
    pixels only. The bad pixel metric (D1) considers the disparity/depth at a pixel
    to be correctly estimated if the error is less than $3$ pixels or less than $5\%$
    of its value.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：基于深度学习的立体匹配算法在KITTI 2015基准测试集上的性能比较（截至2019/01/05）。PE：错误像素的百分比。EPE：端点误差。S1：坏像素误差。Non-Occ：非遮挡像素。All：所有像素。fg：前景像素。bg：背景像素。Noc：仅非遮挡像素。坏像素指标（D1）考虑到如果一个像素的误差小于$3$像素或其值的$5\%$，则认为该像素的视差/深度估计是正确的。
- en: '| Method | Description | Training | Avg D1 Non-Occ / Est | Avg D1 All / Est
    | Avg D1 Non-Occ / All | Avg D1 All / All | Time (s) | Environment |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 描述 | 训练 | 平均 D1 非遮挡 / 估计 | 平均 D1 全部 / 估计 | 平均 D1 非遮挡 / 全部 | 平均 D1 全部
    / 全部 | 时间（秒） | 环境 |'
- en: '|  |  | 3D Sup | Loss | End-to-end | D1-fg | D1-bg | D1-all | D1-fg | D1-bg
    | D1-all | D1-fg | D1-bg | D1-all | D1-fg | D1-bg | D1-all |  |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 3D Sup | 损失 | 端到端 | D1-fg | D1-bg | D1-all | D1-fg | D1-bg | D1-all
    | D1-fg | D1-bg | D1-all | D1-fg | D1-bg | D1-all |  |'
- en: '| MC-CNN Accr [[17](#bib.bib17)] | Raw disparity $+$ classic refinement | ✓
    | $\mathcal{L}_{1}^{13}$ | ✕ | $7.64$ | $2.48$ | $3.33$ | $8.88$ | $2.89$ | $3.89$
    | $7.64$ | $2.48$ | $3.33$ | $8.88$ | $2.89$ | $3.89$ | $67$ | Nvidia GTX Titan
    X (CUDA, Lua/Torch7) |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| MC-CNN Accr [[17](#bib.bib17)] | 原始视差 $+$ 经典细化 | ✓ | $\mathcal{L}_{1}^{13}$
    | ✕ | $7.64$ | $2.48$ | $3.33$ | $8.88$ | $2.89$ | $3.89$ | $7.64$ | $2.48$ |
    $3.33$ | $8.88$ | $2.89$ | $3.89$ | $67$ | Nvidia GTX Titan X (CUDA, Lua/Torch7)
    |'
- en: '| Luo *et al.* [[19](#bib.bib19)] | Raw disparity $+$ classic refinement |
    ✓ | $\mathcal{L}_{1}^{6}$ | ✕ | $7.44$ | $3.32$ | $4.00$ | $8.58$ | $3.73$ | $4.54$
    | $7.44$ | $3.32$ | $4.00$ | $8.58$ | $3.73$ | $4.54$ | $1$ | Nvidia GTX Titan
    X (Torch) |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| Luo *et al.* [[19](#bib.bib19)] | 原始视差 $+$ 经典细化 | ✓ | $\mathcal{L}_{1}^{6}$
    | ✕ | $7.44$ | $3.32$ | $4.00$ | $8.58$ | $3.73$ | $4.54$ | $7.44$ | $3.32$ |
    $4.00$ | $8.58$ | $3.73$ | $4.54$ | $1$ | Nvidia GTX Titan X (Torch) |'
- en: '| Chen *et al.* [[20](#bib.bib20)] | Raw disparity $+$ classic refinement |
    ✓ | $\mathcal{L}_{1}^{13}$ | ✕ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ |  |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| Chen *et al.* [[20](#bib.bib20)] | 原始视差 $+$ 经典细化 | ✓ | $\mathcal{L}_{1}^{13}$
    | ✕ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ |  |'
- en: '| L-ResMatch [[21](#bib.bib21)] | Raw disparity $+$ confidence score $+$ classic
    refinement | ✓ |  | $\circ$ | $5.74$ | $2.35$ | $2.91$ | $6.95$ | $2.72$ | $3.42$
    | $5.74$ | $2.35$ | $2.91$ | $6.95$ | $2.72$ | $3.42$ | $48$ | Nvidia Titan-X
    |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| L-ResMatch [[21](#bib.bib21)] | 原始视差 $+$ 置信度评分 $+$ 经典细化 | ✓ |  | $\circ$
    | $5.74$ | $2.35$ | $2.91$ | $6.95$ | $2.72$ | $3.42$ | $5.74$ | $2.35$ | $2.91$
    | $6.95$ | $2.72$ | $3.42$ | $48$ | Nvidia Titan-X |'
- en: '| Han *et al.*[[22](#bib.bib22)] | Matching network | ✓ | $\mathcal{L}_{1}^{6}$
    | ✕ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | Nvidia GTX Titan Xp |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| Han *et al.*[[22](#bib.bib22)] | 匹配网络 | ✓ | $\mathcal{L}_{1}^{6}$ | ✕ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | Nvidia
    GTX Titan Xp |'
- en: '| Tulyakov *et al.*[[60](#bib.bib60)] | MC-CNN fast [[17](#bib.bib17)] + weakly-supervised
    learning |  | $-$ | ✕ | $9.42$ | $3.06$ | $4.11$ | $10.93$ | $3.78$ | $4.93$ |
    $9.42$ | $3.06$ | $4.11$ | $10.93$ | $3.78$ | $4.93$ | $1.35$ | 1 core 2.5 Ghz
    + K40 NVIDIA, Lua-Torch |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| Tulyakov *et al.*[[60](#bib.bib60)] | MC-CNN 快速 [[17](#bib.bib17)] + 弱监督学习
    |  | $-$ | ✕ | $9.42$ | $3.06$ | $4.11$ | $10.93$ | $3.78$ | $4.93$ | $9.42$ |
    $3.06$ | $4.11$ | $10.93$ | $3.78$ | $4.93$ | $1.35$ | 1 核 2.5 GHz + K40 NVIDIA,
    Lua-Torch |'
- en: '| FlowNetCorr [[12](#bib.bib12)] | Refined disparity | ✓ | $\mathcal{L}_{1}^{1}$
    | ✓ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $1.12$ | Nvidia GTX Titan |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| FlowNetCorr [[12](#bib.bib12)] | 细化视差 | ✓ | $\mathcal{L}_{1}^{1}$ | ✓ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $1.12$ | Nvidia
    GTX Titan |'
- en: '| DispNetCorr [[13](#bib.bib13)] | Raw disparity | ✓ |  | $\circ$ | $3.72$
    | $4.11$ | $4.05$ | $4.41$ | $4.32$ | $4.34$ | $3.72$ | $4.11$ | $4.05$ | $4.41$
    | $4.32$ | $4.34$ | $0.06$ | Nvidia Titan-X |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| DispNetCorr [[13](#bib.bib13)] | 原始视差 | ✓ |  | $\circ$ | $3.72$ | $4.11$
    | $4.05$ | $4.41$ | $4.32$ | $4.34$ | $3.72$ | $4.11$ | $4.05$ | $4.41$ | $4.32$
    | $4.34$ | $0.06$ | Nvidia Titan-X |'
- en: '| Pang *et al.*[[23](#bib.bib23)] | Raw disparity | ✓ | $\mathcal{L}_{1}^{1}$
    | $\circ$ | $3.12$ | $2.32$ | $2.45$ | $3.59$ | $2.48$ | $2.67$ | $3.12$ | $2.32$
    | $2.45$ | $3.59$ | $2.48$ | $2.67$ | $0.47$ | $-$ |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| Pang *et al.* [[23](#bib.bib23)] | 原始视差 | ✓ | $\mathcal{L}_{1}^{1}$ | $\circ$
    | $3.12$ | $2.32$ | $2.45$ | $3.59$ | $2.48$ | $2.67$ | $3.12$ | $2.32$ | $2.45$
    | $3.59$ | $2.48$ | $2.67$ | $0.47$ | $-$ |'
- en: '| Yu *et al.*[[24](#bib.bib24)] | Raw disparity map | ✓ | $\mathcal{L}_{1}^{1}$
    | $\circ$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $5.32$ | $2.06$ | $2.32$ | $5.46$
    | $2.17$ | $2.79$ | $1.13$ | Nvidia 1080Ti |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| Yu *et al.*[[24](#bib.bib24)] | 原始视差图 | ✓ | $\mathcal{L}_{1}^{1}$ | $\circ$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $5.32$ | $2.06$ | $2.32$ | $5.46$ | $2.17$
    | $2.79$ | $1.13$ | Nvidia 1080Ti |'
- en: '| Yang *et al.* [[25](#bib.bib25)] - supp. | Raw disparity | ✓ | $\mathcal{L}_{1}^{1}+\mathcal{L}_{1}^{13}+\mathcal{L}_{2}^{1}$
    | ✓ | $3.70$ | $1.76$ | $2.08$ | $4.07$ | $1.88$ | $2.25$ | $3.70$ | $1.76$ |
    $2.08$ | $4.07$ | $1.88$ | $2.25$ | $0.6$ |  |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| Yang *et al.* [[25](#bib.bib25)] - 附录 | 原始视差 | ✓ | $\mathcal{L}_{1}^{1}+\mathcal{L}_{1}^{13}+\mathcal{L}_{2}^{1}$
    | ✓ | $3.70$ | $1.76$ | $2.08$ | $4.07$ | $1.88$ | $2.25$ | $3.70$ | $1.76$ |
    $2.08$ | $4.07$ | $1.88$ | $2.25$ | $0.6$ |  |'
- en: '| Yang *et al.* [[25](#bib.bib25)] - unsup. | Raw disparity | ✕ | $\mathcal{L}_{1}^{9}+\mathcal{L}_{1}^{13}+\mathcal{L}_{2}^{1}$
    | ✓ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $7.70$ | $-$ | $-$ | $8.79$
    | $0.6$ |  |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| Yang *et al.* [[25](#bib.bib25)] - 无监督 | 原始视差 | ✕ | $\mathcal{L}_{1}^{9}+\mathcal{L}_{1}^{13}+\mathcal{L}_{2}^{1}$
    | ✓ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $7.70$ | $-$ | $-$ | $8.79$
    | $0.6$ |  |'
- en: '| Liang *et al.* [[26](#bib.bib26)] | Refined disparity | ✓ | $\mathcal{L}_{1}^{1}$
    | ✓ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $2.76$ | $2.07$ | $2.19$ | $3.40$ |
    $2.25$ | $2.44$ | $0.12$ | Nvidia Titan-X |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| Liang *et al.* [[26](#bib.bib26)] | 细化视差 | ✓ | $\mathcal{L}_{1}^{1}$ | ✓
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $2.76$ | $2.07$ | $2.19$ | $3.40$ | $2.25$
    | $2.44$ | $0.12$ | Nvidia Titan-X |'
- en: '| Khamis *et al.* [[27](#bib.bib27)] | Raw disparity $+$ hierarchical refinement
    | ✓ | $\mathcal{L}_{1}^{4}$ with $\alpha=1,c=2$ | ✓ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $7.45$ | $4.30$ | $4.83$ | $0.015$ | Nvidia Titan-X
    |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| Khamis *et al.* [[27](#bib.bib27)] | 原始视差 $+$ 分层细化 | ✓ | $\mathcal{L}_{1}^{4}$
    with $\alpha=1,c=2$ | ✓ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $7.45$ | $4.30$ | $4.83$ | $0.015$ | Nvidia Titan-X |'
- en: '| Gidaris & Komodakis [[47](#bib.bib47)] | Refinement only | ✓ | $\mathcal{L}_{1}^{1}$
    | $\circ$ | $4.87$ | $2.34$ | $2.76$ | $6.04$ | $2.58$ | $3.16$ | $4.87$ | $2.34$
    | $2.76$ | $6.04$ | $2.58$ | $3.16$ | $0.4$ | Nvidia Titan-X |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| Gidaris & Komodakis [[47](#bib.bib47)] | 仅细化 | ✓ | $\mathcal{L}_{1}^{1}$
    | $\circ$ | $4.87$ | $2.34$ | $2.76$ | $6.04$ | $2.58$ | $3.16$ | $4.87$ | $2.34$
    | $2.76$ | $6.04$ | $2.58$ | $3.16$ | $0.4$ | Nvidia Titan-X |'
- en: '| Chang & Chen [[28](#bib.bib28)] | Raw disparity | ✓ | $\mathcal{L}_{1}^{5}$
    | $\circ$ | $4.31$ | $1.71$ | $2.14$ | $4.62$ | $1.86$ | $2.32$ | $4.31$ | $1.71$
    | $2.14$ | $4.62$ | $1.86$ | $2.32$ | $0.41$ | Nvidia GTX Titan Xp |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| Chang & Chen [[28](#bib.bib28)] | 原始视差 | ✓ | $\mathcal{L}_{1}^{5}$ | $\circ$
    | $4.31$ | $1.71$ | $2.14$ | $4.62$ | $1.86$ | $2.32$ | $4.31$ | $1.71$ | $2.14$
    | $4.62$ | $1.86$ | $2.32$ | $0.41$ | Nvidia GTX Titan Xp |'
- en: '| Zhong *et al.*[[29](#bib.bib29)] | Raw disparity map | ✕ | $\alpha_{1}\mathcal{L}_{1}^{12}+\alpha_{2}\mathcal{L}_{1}^{9}+\alpha_{3}\mathcal{L}_{1}^{7}+\alpha_{4}\mathcal{L}_{2}^{3}+\alpha_{5}\mathcal{L}_{2}^{5}+\alpha_{6}\mathcal{L}_{2}^{7}$
    | $\circ$ | $6.13$ | $2.46$ | $3.06$ | $7.12$ | $2.86$ | $3.57$ | $6.13$ | $2.46$
    | $3.06$ | $7.12$ | $2.86$ | $3.57$ | $0.8$ | P100 |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| Zhong *et al.*[[29](#bib.bib29)] | 原始视差图 | ✕ | $\alpha_{1}\mathcal{L}_{1}^{12}+\alpha_{2}\mathcal{L}_{1}^{9}+\alpha_{3}\mathcal{L}_{1}^{7}+\alpha_{4}\mathcal{L}_{2}^{3}+\alpha_{5}\mathcal{L}_{2}^{5}+\alpha_{6}\mathcal{L}_{2}^{7}$
    | $\circ$ | $6.13$ | $2.46$ | $3.06$ | $7.12$ | $2.86$ | $3.57$ | $6.13$ | $2.46$
    | $3.06$ | $7.12$ | $2.86$ | $3.57$ | $0.8$ | P100 |'
- en: '| Kendall *et al.*[[39](#bib.bib39)] | Refiend disparity map without refinement
    module | ✓ | $\mathcal{L}_{1}^{1}$ | ✓ | $5.58$ | $2.02$ | $2.61$ | $6.16$ | $2.21$
    | $2.87$ | $5.58$ | $2.02$ | $2.61$ | $6.16$ | $2.21$ | $2.87$ | $0.9$ | Nvidia
    GTX Titan X |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| Kendall *et al.*[[39](#bib.bib39)] | 无精化模块的视差图 | ✓ | $\mathcal{L}_{1}^{1}$
    | ✓ | $5.58$ | $2.02$ | $2.61$ | $6.16$ | $2.21$ | $2.87$ | $5.58$ | $2.02$ |
    $2.61$ | $6.16$ | $2.21$ | $2.87$ | $0.9$ | Nvidia GTX Titan X |'
- en: '| Standard SGM-Net [[30](#bib.bib30)] | Refinement with CNN-based SGM | ✓ |
    Weighted sum of path cost and neighbor cost | $\circ$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $7.44$ | $2.23$ | $3.09$ | $-$ | $-$ | $-$ | $67$ | Nvidia Titan-X
    |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| Standard SGM-Net [[30](#bib.bib30)] | 基于 CNN 的 SGM 精化 | ✓ | 路径成本和邻居成本的加权和
    | $\circ$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $7.44$ | $2.23$ | $3.09$ | $-$
    | $-$ | $-$ | $67$ | Nvidia Titan-X |'
- en: '| Signed SGM-Net [[30](#bib.bib30)] | Refinement with CNN-based SGM | ✓ | Weighted
    sum of path cost and neighbor cost | $\circ$ | $7.43$ | $2.23$ | $3.09$ | $8.64$
    | $2.66$ | $3.66$ | $7.43$ | $2.23$ | $3.09$ | $8.64$ | $2.66$ | $3.66$ | $67$
    | Nvidia Titan-X |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| Signed SGM-Net [[30](#bib.bib30)] | 基于 CNN 的 SGM 精化 | ✓ | 路径成本和邻居成本的加权和 |
    $\circ$ | $7.43$ | $2.23$ | $3.09$ | $8.64$ | $2.66$ | $3.66$ | $7.43$ | $2.23$
    | $3.09$ | $8.64$ | $2.66$ | $3.66$ | $67$ | Nvidia Titan-X |'
- en: '| Cheng *et al.*[[53](#bib.bib53)] | Refinement | ✓ | $\mathcal{L}_{1}^{1}$
    | $\circ$ | $2.67$ | $1.40$ | $1.61$ | $2.88$ | $1.51$ | $1.74$ | $2.67$ | $1.40$
    | $1.61$ | $2.88$ | $1.51$ | $1.74$ | $0.5$ | GPU @ 2.5 Ghz (C/C++) |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| Cheng *et al.*[[53](#bib.bib53)] | 精化 | ✓ | $\mathcal{L}_{1}^{1}$ | $\circ$
    | $2.67$ | $1.40$ | $1.61$ | $2.88$ | $1.51$ | $1.74$ | $2.67$ | $1.40$ | $1.61$
    | $2.88$ | $1.51$ | $1.74$ | $0.5$ | GPU @ 2.5 Ghz (C/C++) |'
- en: '| EdgeStereo [[31](#bib.bib31)] | Raw disparity | ✓ | $\displaystyle\sum_{sc=1}^{\text{nscales}}\left(\mathcal{L}_{1}^{1}+\alpha\mathcal{L}_{1}^{14}\right)_{sc}$
    | ✕ | $3.04$ | $1.70$ | $1.92$ | $3.39$ | $1.85$ | $2.10$ | $3.04$ | $1.70$ |
    $1.92$ | $3.39$ | $1.85$ | $2.10$ | $0.32$ | Nvidia GTX Titan Xp |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| EdgeStereo [[31](#bib.bib31)] | 原始视差 | ✓ | $\displaystyle\sum_{sc=1}^{\text{nscales}}\left(\mathcal{L}_{1}^{1}+\alpha\mathcal{L}_{1}^{14}\right)_{sc}$
    | ✕ | $3.04$ | $1.70$ | $1.92$ | $3.39$ | $1.85$ | $2.10$ | $3.04$ | $1.70$ |
    $1.92$ | $3.39$ | $1.85$ | $2.10$ | $0.32$ | Nvidia GTX Titan Xp |'
- en: '| Tulyakov *et al.*[[32](#bib.bib32)] | Disparity with sub-pixel accuracy |
    ✓ | $\mathcal{L}_{1}^{7}$ | $\circ$ | $3.63$ | $2.09$ | $2.36$ | $4.05$ | $2.25$
    | $2.58$ | $3.63$ | $2.09$ | $2.36$ | $4.05$ | $2.25$ | $2.58$ | $0.5$ | 1 core
    @ 2.5 Ghz (Python) |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| Tulyakov *et al.*[[32](#bib.bib32)] | 亚像素精度的视差 | ✓ | $\mathcal{L}_{1}^{7}$
    | $\circ$ | $3.63$ | $2.09$ | $2.36$ | $4.05$ | $2.25$ | $2.58$ | $3.63$ | $2.09$
    | $2.36$ | $4.05$ | $2.25$ | $2.58$ | $0.5$ | 1 核 @ 2.5 Ghz (Python) |'
- en: '| Jie *et al.*[[33](#bib.bib33)] | Refined disparity with DL | ✓ | $\mathcal{L}_{1}^{1}$
    | ✓ | $4.19$ | $2.23$ | $2.55$ | $5.42$ | $2.55$ | $3.03$ | $4.19$ | $2.23$ |
    $2.55$ | $5.42$ | $2.55$ | $3.03$ | $49.2$ | Nvidia GTX Titan X |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| Jie *et al.*[[33](#bib.bib33)] | 基于 DL 的精化视差 | ✓ | $\mathcal{L}_{1}^{1}$
    | ✓ | $4.19$ | $2.23$ | $2.55$ | $5.42$ | $2.55$ | $3.03$ | $4.19$ | $2.23$ |
    $2.55$ | $5.42$ | $2.55$ | $3.03$ | $49.2$ | Nvidia GTX Titan X |'
- en: '| Seki *et al.*[[59](#bib.bib59)] | Raw disparity, confidence map, SGM-based
    refinement | ✓ | $\mathcal{L}_{1}^{6}$ | $\circ$ | $7.71$ | $2.27$ | $3.17$ |
    $8.74$ | $2.58$ | $3.61$ | $7.71$ | $2.27$ | $3.17$ | $8.74$ | $2.58$ | $3.61$
    | $68$ | Nvidia GTX Titan X |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| Seki *et al.*[[59](#bib.bib59)] | 原始视差、置信度图、基于 SGM 的精化 | ✓ | $\mathcal{L}_{1}^{6}$
    | $\circ$ | $7.71$ | $2.27$ | $3.17$ | $8.74$ | $2.58$ | $3.61$ | $7.71$ | $2.27$
    | $3.17$ | $8.74$ | $2.58$ | $3.61$ | $68$ | Nvidia GTX Titan X |'
- en: '| Kuzmin *et al.*[[111](#bib.bib111)] | Only aggregated cost volume | ✓ | $\mathcal{L}_{1}^{6}$
    | $\circ$ | $10.11$ | $4.81$ | $5.68$ | $11.35$ | $5.32$ | $6.32$ | $10.11$ |
    $4.82$ | $5.69$ | $11.35$ | $5.34$ | $6.34$ | $0.03$ | GPU @ 2.5 Ghz (C/C++) |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| Kuzmin *et al.*[[111](#bib.bib111)] | 仅聚合成本体积 | ✓ | $\mathcal{L}_{1}^{6}$
    | $\circ$ | $10.11$ | $4.81$ | $5.68$ | $11.35$ | $5.32$ | $6.32$ | $10.11$ |
    $4.82$ | $5.69$ | $11.35$ | $5.34$ | $6.34$ | $0.03$ | GPU @ 2.5 Ghz (C/C++) |'
- en: '| Tonioni *et al.*[[96](#bib.bib96)] | Unsupervised adaptation - DispNetCorr1D [[13](#bib.bib13)]
    + CENSUS [[112](#bib.bib112)] | ✓ | $\mathcal{L}_{1}^{2}+\alpha\mathcal{L}_{2}^{4}$
    | NA | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.76$
    | $-$ | GPU @ 2.5 Ghz (Python) |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| Tonioni *et al.*[[96](#bib.bib96)] | 无监督适应 - DispNetCorr1D [[13](#bib.bib13)]
    + CENSUS [[112](#bib.bib112)] | ✓ | $\mathcal{L}_{1}^{2}+\alpha\mathcal{L}_{2}^{4}$
    | NA | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.76$ | $-$
    | GPU @ 2.5 Ghz (Python) |'
- en: 6.2.1 Degree of supervision
  id: totrans-495
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 监督程度
- en: Most of the state-of-the-art methods require ground-truth depth maps to train
    their deep learning models. This is reflected in the loss functions they use to
    train the networks. For instance, Flynn *et al.* [[14](#bib.bib14)], Kendall *et
    al.* [[39](#bib.bib39)], Pang *et al.* [[23](#bib.bib23)], Cheng *et al.* [[53](#bib.bib53)],
    and Liang *et al.* [[26](#bib.bib26)] minimize the $L_{1}$ distance between the
    estimated disparity/depth and the ground truth (Equation ([9](#S5.E9 "In 5.2.1
    The data term ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction"))), while Luo *et al.* [[19](#bib.bib19)]
    minimize the cross-entropy loss of Equation ([14](#S5.E14 "In 5.2.1 The data term
    ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning Architectures for
    Image-based Depth Reconstruction")). Khamis *et al.* [[27](#bib.bib27)] used the
    same approach but by using the two-parameter robust function (Equation ([12](#S5.E12
    "In 5.2.1 The data term ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction"))). Chen *et al.* [[20](#bib.bib20)],
    which formulated the stereo matching problem as a classification problem, trained
    their network to classify whether pixel $x$ on the left image and pixel $x-d$
    on the right image are in correspondence (positive class) or not (negative class).
    The loss is then defined as the $L_{2}$ distance between the output of the network
    for the pixel pair $(x,x-d)$ and the ground-truth label (0 or 1) of this pair
    of pixels.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数最先进的方法需要真实的深度图来训练其深度学习模型。这在他们用于训练网络的损失函数中有所体现。例如，Flynn *et al.* [[14](#bib.bib14)]、Kendall
    *et al.* [[39](#bib.bib39)]、Pang *et al.* [[23](#bib.bib23)]、Cheng *et al.* [[53](#bib.bib53)]
    和 Liang *et al.* [[26](#bib.bib26)] 最小化了估计的视差/深度与真实值之间的 $L_{1}$ 距离（方程 ([9](#S5.E9
    "In 5.2.1 The data term ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction"))），而 Luo *et al.* [[19](#bib.bib19)]
    最小化了方程 ([14](#S5.E14 "In 5.2.1 The data term ‣ 5.2 Loss functions ‣ 5 Training
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction"))的交叉熵损失。Khamis
    *et al.* [[27](#bib.bib27)] 采用了相同的方法，但使用了两个参数的鲁棒函数（方程 ([12](#S5.E12 "In 5.2.1
    The data term ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction"))）。Chen *et al.* [[20](#bib.bib20)] 将立体匹配问题形式化为分类问题，训练他们的网络来分类左图像上的像素
    $x$ 和右图像上的像素 $x-d$ 是否对应（正类）或不对应（负类）。损失定义为网络对像素对 $(x,x-d)$ 的输出与该像素对的真实标签（0 或 1）之间的
    $L_{2}$ 距离。
- en: In general, obtaining ground-truth disparity/depth maps is very challenging.
    As such, techniques that do not require 3D supervision are more attractive. The
    key to training without 3D supervision is the use of loss functions that are based
    on the reprojection error, *e.g.,* Equation ([18](#S5.E18 "In 5.2.1 The data term
    ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning Architectures for
    Image-based Depth Reconstruction")). This approach has been adopted in recent
    techniques, *e.g.,* Zhou *et al.* [[95](#bib.bib95)] and Yang *et al.* [[25](#bib.bib25)].
    One limitation of these techniques is that they assume that the camera parameters
    are known so that the unwarping or re-projection onto the coordinates of the other
    image can be calculated. Some techniques, *e.g.,* [[15](#bib.bib15)], assume that
    the camera parameters are unknown and regress them at the same time as depth/disparity
    in the same spirit as Structure from Motion (SfM) or visual SLAM.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，获得真实的视差/深度图是非常具有挑战性的。因此，不需要 3D 监督的技术更具吸引力。无 3D 监督训练的关键在于使用基于重投影误差的损失函数，*例如*，方程 ([18](#S5.E18
    "In 5.2.1 The data term ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction"))。这种方法已经被最近的技术采纳，*例如*，Zhou
    *et al.* [[95](#bib.bib95)] 和 Yang *et al.* [[25](#bib.bib25)]。这些技术的一个限制是，它们假设相机参数已知，以便可以计算到另一图像坐标的去畸变或重投影。一些技术，*例如*，[[15](#bib.bib15)]，假设相机参数未知，并在与深度/视差一起回归，具有类似于结构光束（SfM）或视觉
    SLAM 的精神。
- en: As shown in Table [V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo matching techniques
    ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction"), methods that are trained with 3D supervision achieve a
    better performance at runtime than those without 3D supervision. For example,
    Yang *et al.* [[25](#bib.bib25)] evaluated their networks in both modes and showed
    that the supervised one achieved and average D1-all (All/All) of $2.25\%$ compared
    to $8.79\%$ for the unsupervised one.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo matching techniques ‣ 6 Discussion
    and comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction")所示，使用3D监督训练的方法在运行时的性能优于未使用3D监督的方法。例如，Yang *et al.* [[25](#bib.bib25)]在两种模式下评估了他们的网络，并显示出有监督模式的平均D1-all
    (All/All)为$2.25\%$，而无监督模式为$8.79\%$。
- en: 6.2.2 Accuracy and disparity range
  id: totrans-499
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 精度和视差范围
- en: Based on the metrics of Section [6](#S6 "6 Discussion and comparison ‣ A Survey
    on Deep Learning Architectures for Image-based Depth Reconstruction"), the unsupervised
    adaptation method of Tonioni *et al.* [[96](#bib.bib96)] boosted significantly
    the performance of DispNetCorr1D [[13](#bib.bib13)] (from $4.34$ on Avg D1 All/All
    (D1-all) to $0.76$). This suggests that such adaptation module can be used to
    boost the performance of the other methods.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第[6](#S6 "6 Discussion and comparison ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")节的指标，Tonioni *et al.* [[96](#bib.bib96)]
    的无监督适应方法显著提高了DispNetCorr1D [[13](#bib.bib13)]的性能（从$4.34$在Avg D1 All/All (D1-all)降低到$0.76$）。这表明此类适应模块可以用于提升其他方法的性能。
- en: Note that only a few methods could achieve sub-pixel accuracy. Examples include
    the approach of Tulyakov *et al.* [[32](#bib.bib32)], which uses the sub-pixel
    MAP approximation instead of the softargmin. Also, Tulyakov *et al.* [[32](#bib.bib32)]’s
    approach allows changing the disparity range at runtime without retraining the
    network.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到只有少数方法可以实现亚像素级精度。例如，Tulyakov *et al.* [[32](#bib.bib32)] 的方法使用亚像素MAP近似而不是softargmin。此外，Tulyakov
    *et al.* [[32](#bib.bib32)] 的方法允许在运行时更改视差范围而无需重新训练网络。
- en: 6.2.3 Computation time and memory footprint
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 计算时间和内存占用
- en: Computation time and memory footprint, which in general are interrelated, are
    very important especially at runtime. Based on Table [V](#S6.T5 "TABLE V ‣ 6.2
    Pairwise stereo matching techniques ‣ 6 Discussion and comparison ‣ A Survey on
    Deep Learning Architectures for Image-based Depth Reconstruction"), we can distinguish
    three types of methods; Slow methods, average-speed methods, which produce a depth
    map in around one seconds, and fast methods, which require less than $0.1$ seconds
    to estimate a single depth map.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 计算时间和内存占用通常是相互关联的，尤其在运行时非常重要。根据表[V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo matching
    techniques ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")，我们可以区分三种类型的方法：慢速方法、平均速度方法（大约在一秒钟内生成深度图）和快速方法（估算单个深度图的时间少于$0.1$秒）。
- en: Slow methods require more than $40$ seconds to estimate one single depth map.
    There are multiple design aspects that make these method slow. For instance, some
    of them perform multiple forward passes as in [[30](#bib.bib30)]. Others either
    deepen the network by using a large number of layers including many fully connected
    layers, especially in the similarity computation block as in the MC-CNN Acc of [[17](#bib.bib17)]
    and the L-ResMatch of [[21](#bib.bib21)]), or use multiple subnetworks. Other
    methods estimate the depth map in a recurrent fashion, *e.g.,* by using multiple
    convLSTM blocks as in Jie *et al.* [[33](#bib.bib33)].
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 慢速方法需要超过$40$秒来估算一个单一的深度图。这些方法变慢的原因有多个设计方面。例如，其中一些执行多个前向传递，如[[30](#bib.bib30)]。其他方法通过使用大量层，包括许多全连接层（尤其是在相似性计算块中，如MC-CNN
    Acc [[17](#bib.bib17)]和L-ResMatch [[21](#bib.bib21)]），或使用多个子网络来加深网络。其他方法以递归方式估算深度图，例如，使用多个convLSTM块，如Jie
    *et al.* [[33](#bib.bib33)]。
- en: According to Table [V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo matching techniques
    ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction"), the approach of Khamis *et al.* [[27](#bib.bib27)] is
    the fastest one as it produces, at run time, a disparity map in $15$ms, with a
    subpixel accuracy of $0.03$, which corresponds to an error of less than $3$cm
    at $3$m distance from the camera. In fact, Khamis *et al.* [[27](#bib.bib27)]
    observed that most of the time and compute is spent matching features at higher
    resolutions, while most of the performance gain comes from matching at lower resolutions.
    Thus, they compute the cost volume by matching features at low resolution. An
    efficient refinement module is then used to upsample the low resolution depth
    map to the input resolution.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表 [V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo matching techniques ‣ 6 Discussion
    and comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction")，Khamis *等人* [[27](#bib.bib27)] 的方法是最快的，因为它在运行时产生一个视差图，耗时 $15$ms，子像素精度为
    $0.03$，这对应于距离相机 $3$m 处的误差小于 $3$cm。事实上，Khamis *等人* [[27](#bib.bib27)] 观察到，大多数时间和计算都花在了高分辨率下匹配特征上，而大多数性能提升来自于低分辨率下的匹配。因此，他们通过低分辨率下的特征匹配计算成本体积。然后使用高效的精修模块将低分辨率深度图上采样到输入分辨率。
- en: Finally, t Mayer *et al.* [[13](#bib.bib13)] and Kendall *et al.* [[39](#bib.bib39)]
    can run very fast, with $0.06$s and $0.9$s consumed on a single Nvidia GTX Titan
    X GPU, respectively. However, disparity refinement is not included in these networks,
    which limits their performance.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，Mayer *等人* [[13](#bib.bib13)] 和 Kendall *等人* [[39](#bib.bib39)] 的算法运行非常快速，分别在单个
    Nvidia GTX Titan X GPU 上消耗 $0.06$s 和 $0.9$s。然而，这些网络不包括视差精修，这限制了它们的性能。
- en: 6.3 Multiview stereo techniques
  id: totrans-507
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 多视角立体技术
- en: Table [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy and depth range ‣ 6.3 Multiview
    stereo techniques ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") compares the properties and performance
    of five deep learning-based multiview stereo reconstruction algorithms. Note that
    the related papers have reported performance results using different datasets.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy and depth range ‣ 6.3 Multiview stereo
    techniques ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") 比较了五种基于深度学习的多视角立体重建算法的属性和性能。注意相关论文使用了不同的数据集报告了性能结果。
- en: 6.3.1 Degree of supervision
  id: totrans-509
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1 监督程度
- en: Most of the methods described in Table [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy
    and depth range ‣ 6.3 Multiview stereo techniques ‣ 6 Discussion and comparison
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    are trained using 3D supervision. The only exception is the approach of Flynn
    *et al.* [[14](#bib.bib14)], which is trained using a posed set of images, *i.e.,*
    images with known camera parameters. At training, the approach takes a set of
    images, leaves one image out, and learns how to predict it from the remaining
    ones. The rational is that providing a set of posed images is much simpler than
    providing depth values at each pixel in every reference image.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy and depth range ‣ 6.3 Multiview stereo
    techniques ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") 中描述的大多数方法是使用 3D 监督训练的。唯一的例外是 Flynn *等人*
    [[14](#bib.bib14)] 的方法，该方法使用一组已知相机参数的图像进行训练。在训练过程中，该方法取一组图像，留下一张图像，并学习如何从其余图像中预测这张图像。其合理性在于提供一组已知姿态的图像比在每个参考图像中提供每个像素的深度值要简单得多。
- en: 6.3.2 Accuracy and depth range
  id: totrans-511
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2 精度和深度范围
- en: In terms of accuracy, the approach of Huang *et al.* [[36](#bib.bib36)] seems
    to outperform the state-of-the art, see Table [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy
    and depth range ‣ 6.3 Multiview stereo techniques ‣ 6 Discussion and comparison
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction").
    However, since these methods have been evaluated on different datasets, it is
    not clear whether they would achieve the same level of accuracy on other datasets.
    As such, the accuracy results reported in Table [VI](#S6.T6 "TABLE VI ‣ 6.3.2
    Accuracy and depth range ‣ 6.3 Multiview stereo techniques ‣ 6 Discussion and
    comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    are just indicative not conclusive.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在精度方面，Huang *et al.* [[36](#bib.bib36)] 的方法似乎优于现有技术，参见表 [VI](#S6.T6 "TABLE VI
    ‣ 6.3.2 Accuracy and depth range ‣ 6.3 Multiview stereo techniques ‣ 6 Discussion
    and comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction")。然而，由于这些方法在不同的数据集上进行了评估，目前尚不清楚它们是否能在其他数据集上实现相同的精度水平。因此，表 [VI](#S6.T6
    "TABLE VI ‣ 6.3.2 Accuracy and depth range ‣ 6.3 Multiview stereo techniques ‣
    6 Discussion and comparison ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction") 中报告的精度结果仅为指示性，并非结论性。
- en: Finally, since most of the MVS methods rely on Plane Sweep Volumes or image/feature
    umprojection onto depth planes, the depth range needs to be set in advance. Changing
    the depth range and its discretization at runtime would require re-training the
    methods.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于大多数MVS方法依赖于平面扫掠体积或图像/特征反投影到深度平面，因此需要提前设置深度范围。在运行时更改深度范围及其离散化会要求重新训练这些方法。
- en: 'TABLE VI: Performance comparison of deep learning-based multiview stereo matching
    algorithms on the KITTI 2015 benchmark. Accuracy and completeness refer, respectively,
    to the mean accuracy and mean completeness (the lower the better).'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：深度学习基于的多视图立体匹配算法在 KITTI 2015 基准上的性能比较。精度和完整性分别指平均精度和平均完整性（数值越低越好）。
- en: '| Method | Description | Depth range | Training | Testing | Performance |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 描述 | 深度范围 | 训练 | 测试 | 性能 |'
- en: '|  |  |  | #views | 3D Sup | Loss | End-to-end | Time (s) | Memory | #views
    | Time (s) | Memory | Dataset | Accuracy | Completeness |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | #视图 | 3D Sup | 损失 | 端到端 | 时间（秒） | 内存 | #视图 | 时间（秒） | 内存 | 数据集 |
    精度 | 完整性 |'
- en: '| [[14](#bib.bib14)] | Feature projection and unprojection. | $96$ | $4+1$
    | ✕ | $L_{1}$ color loss | ✓ | $-$ | $-$ | $4$ | $12$ min | $-$ | KITTI 2012 |
    $-$ | $-$ |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| [[14](#bib.bib14)] | 特征投影和反投影。 | $96$ | $4+1$ | ✕ | $L_{1}$ 色彩损失 | ✓ | $-$
    | $-$ | $4$ | $12$ min | $-$ | KITTI 2012 | $-$ | $-$ |'
- en: '| [[35](#bib.bib35)] | Multi-patch similarity | $256$ | variable | ✓ | softmax
    loss | $\circ$ | $-$ | $-$ | variable | $0.07$ | $-$ | DTU | $1.336$ | $2.126$
    |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| [[35](#bib.bib35)] | 多补丁相似性 | $256$ | 变量 | ✓ | softmax 损失 | $\circ$ | $-$
    | $-$ | 变量 | $0.07$ | $-$ | DTU | $1.336$ | $2.126$ |'
- en: '| [[38](#bib.bib38)] | Feature unprojection. Volumetric reconstruction followed
    by projection to generate depth | $300$ | $5$ or $10$ | ✓ | $L_{1}$ loss | ✓ |
    $-$ | $-$ | $5$ or $10$ | $0.033$ | $-$ | ShapeNet | $-$ | $-$ |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| [[38](#bib.bib38)] | 特征反投影。体积重建后投影以生成深度 | $300$ | $5$ 或 $10$ | ✓ | $L_{1}$
    损失 | ✓ | $-$ | $-$ | $5$ 或 $10$ | $0.033$ | $-$ | ShapeNet | $-$ | $-$ |'
- en: '| [[36](#bib.bib36)] | Plane Sweep Volumes | $100$ | $6$ | ✓ | class-balanced
    cross entropy | ✓ | $5$ days | $-$ | variable | $0.05$ for $32^{3}$ grid, $0.40$
    for $64^{3}$ grid | $-$ | ETH3D | $0.036$ | $-$ |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| [[36](#bib.bib36)] | 平面扫掠体积 | $100$ | $6$ | ✓ | 类别平衡交叉熵 | ✓ | $5$ 天 | $-$
    | 变量 | $0.05$ 对于 $32^{3}$ 网格, $0.40$ 对于 $64^{3}$ 网格 | $-$ | ETH3D | $0.036$ |
    $-$ |'
- en: '| [[37](#bib.bib37)] | Feature unprojection | $256$ | $3$ | ✓ | $L_{1}$ | ✓
    | $-$ | $-$ | $5$ | $4.7$ per view | $-$ | DTU | $0.396$ | $0.527$ |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| [[37](#bib.bib37)] | 特征反投影 | $256$ | $3$ | ✓ | $L_{1}$ | ✓ | $-$ | $-$ |
    $5$ | $4.7$ 每视图 | $-$ | DTU | $0.396$ | $0.527$ |'
- en: 6.4 Depth regression techniques
  id: totrans-522
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 深度回归技术
- en: 'TABLE VII: Comparison of some deep learning-based depth regression techniques.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：一些基于深度学习的深度回归技术的比较。
- en: '| Method | Description | Training | Runtime | KITTI 2012 | Make3D | NYUDv2
    |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 描述 | 训练 | 运行时间 | KITTI 2012 | Make3D | NYUDv2 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '|  |  | #views | 3D sup | #views | time | Performance ($\downarrow$ the better)
    | Accuracy ($\uparrow$ the better) | Performance ($\downarrow$ the better) | Performance
    ($\downarrow$ the better) | Accuracy ($\uparrow$ the better) |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '|  |  | #视图 | 3D sup | #视图 | 时间 | 性能（$\downarrow$ 越好越低） | 精度（$\uparrow$ 越好越高）
    | 性能（$\downarrow$ 越好越低） | 性能（$\downarrow$ 越好越低） | 精度（$\uparrow$ 越好越高） |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  |  |  |  |  |  | abs rel. | sqr rel. | RMSE | RMSE | $<1.25$ | $<1.25^{2}$
    | $<1.25^{3}$ | abs rel. | sqr rel. | RMSE | RMSE | abs rel. | sqr rel. | RMSE
    | RMSE | $<1.25$ | $<1.25^{2}$ | $<1.25^{3}$ |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  | 绝对相对误差 | 平方相对误差 | RMSE | RMSE | $<1.25$ | $<1.25^{2}$ |
    $<1.25^{3}$ | 绝对相对误差 | 平方相对误差 | RMSE | RMSE | 绝对相对误差 | 平方相对误差 | RMSE | RMSE |
    $<1.25$ | $<1.25^{2}$ | $<1.25^{3}$ |'
- en: '|  |  |  |  |  |  |  |  | (lin) | (log) |  |  |  |  |  | (lin) | (log) |  |  |
    (lin) | (log) |  |  |  |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  | (线性) | (对数) |  |  |  |  |  | (线性) | (对数) |  |  |
    (线性) | (对数) |  |  |  |'
- en: '| Chakrabarti *et al.* [[70](#bib.bib70)] | Probability that model confidence
    and ambiguities | $1$ | ✓ | $1$ | $24$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $0.149$ | $0.118$ | $0.620$ | $0.205$ | $0.806$ | $0.958$
    | $0.987$ |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| Chakrabarti *等人* [[70](#bib.bib70)] | 模型置信度和模糊性的概率 | $1$ | ✓ | $1$ | $24$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.149$ |
    $0.118$ | $0.620$ | $0.205$ | $0.806$ | $0.958$ | $0.987$ |'
- en: '| Kuznietsov *et al.* [[105](#bib.bib105)] | Supervised followed by unsupervised
    | $2$ | 3D + Stereo | $1$ | $-$ | $0.113$ | $0.741$ | $4.621$ | $0.189$ | 0.862
    | 0.960 | 0.986 | $0.157$ | $-$ | $3.97$ | $0.062$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| Kuznietsov *等人* [[105](#bib.bib105)] | 有监督随后无监督 | $2$ | 3D + 立体 | $1$ | $-$
    | $0.113$ | $0.741$ | $4.621$ | $0.189$ | 0.862 | 0.960 | 0.986 | $0.157$ | $-$
    | $3.97$ | $0.062$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
- en: '| Zhan *et al.* [[113](#bib.bib113)] | visual odometry | Stereo seq. + cam.
    motion | ✕ | $1$ |  | $0.144$ | 1.391 | 5.869 | 0.241 | 0.803 | 0.928 | 0.969
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| Zhan *等人* [[113](#bib.bib113)] | 视觉里程计 | 立体序列 + 相机运动 | ✕ | $1$ |  | $0.144$
    | 1.391 | 5.869 | 0.241 | 0.803 | 0.928 | 0.969 | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
- en: '| Eigen [[10](#bib.bib10)] | Multi-scale | $1$ | ✓ | $1$ | $-$ | $0.1904$ |
    $1.515$ | $7.156$ | $0.270$ | 0.702 | 0.890 | 0.958 | $-$ | $-$ | $-$ | $-$ |
    $0.214$ | $0.204$ | $0.877$ | $0.283$ | $0.614$ | $0.888$ | $0.972$ |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| Eigen [[10](#bib.bib10)] | 多尺度 | $1$ | ✓ | $1$ | $-$ | $0.1904$ | $1.515$
    | $7.156$ | $0.270$ | 0.702 | 0.890 | 0.958 | $-$ | $-$ | $-$ | $-$ | $0.214$
    | $0.204$ | $0.877$ | $0.283$ | $0.614$ | $0.888$ | $0.972$ |'
- en: '| Eigen *et al.* [[2](#bib.bib2)] | VGG - multi-sclae CNN for depth, normals,
    and labeling | $1$ | ✓ | $1$ | $0.033$ |  |  |  |  | $-$ |  |  | $-$ | $-$ | $-$
    | $-$ | $0.158$ | $0.121$ | $0.641$ | $0.214$ | $0.769$ | $0.950$ | $0.988$ |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| Eigen *等人* [[2](#bib.bib2)] | VGG - 多尺度CNN用于深度、法线和标记 | $1$ | ✓ | $1$ | $0.033$
    |  |  |  |  | $-$ |  |  | $-$ | $-$ | $-$ | $-$ | $0.158$ | $0.121$ | $0.641$
    | $0.214$ | $0.769$ | $0.950$ | $0.988$ |'
- en: '| Fu *et al.* [[41](#bib.bib41)] | ResNet - Ordinal regression | $1$ | ✓ |
    $1$ | $-$ | $0.072$ | $0.307$ | $2.727$ | $0.120$ | $0.932$ | $0.984$ | $0.994$
    | $0.157$ | $-$ | $3.97$ | $0.062$ | $0.115$ | $-$ | $0.509$ | $0.051$ | $-$ |
    $-$ | $-$ |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| Fu *等人* [[41](#bib.bib41)] | ResNet - 序数回归 | $1$ | ✓ | $1$ | $-$ | $0.072$
    | $0.307$ | $2.727$ | $0.120$ | $0.932$ | $0.984$ | $0.994$ | $0.157$ | $-$ |
    $3.97$ | $0.062$ | $0.115$ | $-$ | $0.509$ | $0.051$ | $-$ | $-$ | $-$ |'
- en: '| Gan *et al.* [[65](#bib.bib65)] | Uses Affinity, Vertical Pooling, and Label
    Enhancement | $1$ | ✓ | $1$ | $0.07$ | $0.098$ | $0.666$ | $3.933$ | $0.173$ |
    $0.890$ | $0.964$ | $0.985$ | $-$ | $-$ | $-$ | $-$ | $0.158$ | $-$ | $0.631$
    | $0.066$ | $0.756$ | $0.934$ | $0.980$ |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| Gan *等人* [[65](#bib.bib65)] | 使用亲和力、垂直池化和标签增强 | $1$ | ✓ | $1$ | $0.07$ |
    $0.098$ | $0.666$ | $3.933$ | $0.173$ | $0.890$ | $0.964$ | $0.985$ | $-$ | $-$
    | $-$ | $-$ | $0.158$ | $-$ | $0.631$ | $0.066$ | $0.756$ | $0.934$ | $0.980$
    |'
- en: '| Garg [[3](#bib.bib3)] | variable-size input | $2$ | ✕ | $1$ | $-$ | $0.177$
    | $1.169$ | $5.285$ | $0.282$ | $0.727$ | $0.896$ | $0.958$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| Garg [[3](#bib.bib3)] | 可变大小输入 | $2$ | ✕ | $1$ | $-$ | $0.177$ | $1.169$
    | $5.285$ | $0.282$ | $0.727$ | $0.896$ | $0.958$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
- en: '| Godard [[63](#bib.bib63)] | Training with a stereo pair | $2$ (calibrated)
    | ✕ | $1$ | $-$ | $0.114$ | $0.898$ | $4.935$ | $0.206$ | 0.830 | 0.936 | 0.970
    | $0.535$ | $11.990$ | $11.513$ | $0.156$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| Godard [[63](#bib.bib63)] | 使用立体对进行训练 | $2$ (已校准) | ✕ | $1$ | $-$ | $0.114$
    | $0.898$ | $4.935$ | $0.206$ | 0.830 | 0.936 | 0.970 | $0.535$ | $11.990$ | $11.513$
    | $0.156$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
- en: '| Jiao [[74](#bib.bib74)] | $40$ categories - pay more attention to distant
    regions | $1$ | ✓ | $1$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $0.098$ |  | $0.329$ | $0.125$ | $0.917$ | $0.983$ | $0.996$
    |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| Jiao [[74](#bib.bib74)] | $40$ 类别 - 更多关注远处区域 | $1$ | ✓ | $1$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.098$ |  | $0.329$
    | $0.125$ | $0.917$ | $0.983$ | $0.996$ |'
- en: '| Laina [[64](#bib.bib64)] | VGG - feature map up-sampling | $1$ | ✓ | $1$
    | $0.055$ |  |  |  |  | $-$ |  |  | $0.176$ | $-$ | $4.6$ | $0.072$ | $0.194$
    | $-$ | $0.79$ | $0.083$ | $-$ | $-$ | $-$ |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| Laina [[64](#bib.bib64)] | VGG - 特征图上采样 | $1$ | ✓ | $1$ | $0.055$ |  |  |  |  |
    $-$ |  |  | $0.176$ | $-$ | $4.6$ | $0.072$ | $0.194$ | $-$ | $0.79$ | $0.083$
    | $-$ | $-$ | $-$ |'
- en: '| Laina [[64](#bib.bib64)] | ResNet - feature map up-sampling | $1$ | ✓ | $1$
    | $0.055$ |  |  |  |  | $-$ |  |  |  | $-$ |  |  | $0.127$ | $-$ | $0.573$ | $0.055$
    | $0.811$ | $0.953$ | $0.988$ |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| Laina [[64](#bib.bib64)] | ResNet - 特征图上采样 | $1$ | ✓ | $1$ | $0.055$ |  |  |  |  |
    $-$ |  |  |  | $-$ |  |  | $0.127$ | $-$ | $0.573$ | $0.055$ | $0.811$ | $0.953$
    | $0.988$ |'
- en: '| Lee [[46](#bib.bib46)] | Split and merge | $1$ | ✓ | $1$ | $-$ |  |  |  |  |
    $-$ |  |  | $-$ | $-$ | $-$ | $-$ | $0.139$ | $0.096$ | $0.572$ | $0.193$ | $0.815$
    | $0.963$ | $0.991$ |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| Lee [[46](#bib.bib46)] | 分割与合并 | $1$ | ✓ | $1$ | $-$ |  |  |  |  | $-$ |  |  |
    $-$ | $-$ | $-$ | $-$ | $0.139$ | $0.096$ | $0.572$ | $0.193$ | $0.815$ | $0.963$
    | $0.991$ |'
- en: '| Li [[1](#bib.bib1)] | Multiscale patches, refinement with CRF | $1$ | ✓ |
    $1$ | $-$ |  |  |  |  | $-$ |  |  | $0.278$ | $-$ | $7.188$ | $-$ | $0.232$ |
    $-$ | $0.821$ | $-$ | $0.6395$ | $0.9003$ | $0.9741$ |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| Li [[1](#bib.bib1)] | 多尺度补丁，使用CRF进行优化 | $1$ | ✓ | $1$ | $-$ |  |  |  |  |
    $-$ |  |  | $0.278$ | $-$ | $7.188$ | $-$ | $0.232$ | $-$ | $0.821$ | $-$ | $0.6395$
    | $0.9003$ | $0.9741$ |'
- en: '| Qi [[48](#bib.bib48)] | Joint depth and normal maps | $1$ | ✓ | $1$ | $0.87$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.128$ |  |
    $0.569$ |  | $0.834$ | $0.960$ | $0.990$ |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| Qi [[48](#bib.bib48)] | 关节深度和法线图 | $1$ | ✓ | $1$ | $0.87$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.128$ |  | $0.569$ |  | $0.834$
    | $0.960$ | $0.990$ |'
- en: '| Roy [[69](#bib.bib69)] | CNN + Random Forests | $1$ | ✓ | $1$ | $-$ |  |  |  |  |
    $-$ |  |  | $0.260$ | $-$ | $12.40$ | $0.119$ | $0.187$ | $-$ | $0.74$ | $-$ |
    $-$ | $-$ | $-$ |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| Roy [[69](#bib.bib69)] | CNN + 随机森林 | $1$ | ✓ | $1$ | $-$ |  |  |  |  | $-$
    |  |  | $0.260$ | $-$ | $12.40$ | $0.119$ | $0.187$ | $-$ | $0.74$ | $-$ | $-$
    | $-$ | $-$ |'
- en: '| Xian [[104](#bib.bib104)] | Training with $3$K ordinal relations per image
    | $1$ | $3$K ordinal | $1$ | $-$ |  |  |  |  | $-$ |  | $-$ | $-$ | $-$ | $-$
    | $-$ | $0.155$ | $-$ | $0.660$ | $0.066$ | $0.781$ | $0.950$ | $0.987$ |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| Xian [[104](#bib.bib104)] | 每张图像训练$3$K序关系 | $1$ | $3$K序关系 | $1$ | $-$ |  |  |  |  |
    $-$ |  | $-$ | $-$ | $-$ | $-$ | $-$ | $0.155$ | $-$ | $0.660$ | $0.066$ | $0.781$
    | $0.950$ | $0.987$ |'
- en: '| Xu [[114](#bib.bib114)] | Integration with continuous CRF | $1$ | ✓ | $1$
    |  | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.184$ | $-$ | $4.386$ | $0.065$
    | $0.121$ | $-$ | $0.586$ | $0.052$ | $0.706$ | $0.925$ | $0.981$ |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| Xu [[114](#bib.bib114)] | 与连续CRF的集成 | $1$ | ✓ | $1$ |  | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $0.184$ | $-$ | $4.386$ | $0.065$ | $0.121$ | $-$ |
    $0.586$ | $0.052$ | $0.706$ | $0.925$ | $0.981$ |'
- en: '| Xu [[73](#bib.bib73)] | Joint depth estimation and scene parsing | $1$ |
    ✓ | $1$ | $-$ |  |  |  |  | $-$ |  |  | $-$ | $-$ | $-$ | $-$ | $0.214$ | $-$
    | $0.792$ | $-$ | $0.643$ | $0.902$ | $0.977$ |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| Xu [[73](#bib.bib73)] | 关节深度估计与场景解析 | $1$ | ✓ | $1$ | $-$ |  |  |  |  | $-$
    |  |  | $-$ | $-$ | $-$ | $-$ | $0.214$ | $-$ | $0.792$ | $-$ | $0.643$ | $0.902$
    | $0.977$ |'
- en: '| Wang [[11](#bib.bib11)] | Depth and semantic prediction | $1$ | ✓ | $1$ |
    $-$ |  |  |  |  | $-$ |  |  | $-$ | $-$ | $-$ | $-$ | $0.220$ | $-$ | $0.745$
    | $0.262$ | $0.605$ | $0.890$ | $0.970$ |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| Wang [[11](#bib.bib11)] | 深度和语义预测 | $1$ | ✓ | $1$ | $-$ |  |  |  |  | $-$
    |  |  | $-$ | $-$ | $-$ | $-$ | $0.220$ | $-$ | $0.745$ | $0.262$ | $0.605$ |
    $0.890$ | $0.970$ |'
- en: '| Zhang *et al.* [[58](#bib.bib58)] | Hierarchical guidance strategy for depth
    refinement | $1$ | ✓ | $1$ | $0.2$ | $0.136$ | $-$ | $4.310$ | $-$ | $0.833$ |
    $0.957$ | $0.987$ | $0.181$ | $-$ | $4.360$ | $-$ | $0.134$ | $-$ | $0.540$ |
    $-$ | $0.830$ | $0.964$ | $0.992$ |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| Zhang *et al.* [[58](#bib.bib58)] | 层次引导策略用于深度优化 | $1$ | ✓ | $1$ | $0.2$
    | $0.136$ | $-$ | $4.310$ | $-$ | $0.833$ | $0.957$ | $0.987$ | $0.181$ | $-$
    | $4.360$ | $-$ | $0.134$ | $-$ | $0.540$ | $-$ | $0.830$ | $0.964$ | $0.992$
    |'
- en: '| Zhang *et al.* [[52](#bib.bib52)] | ResNet50 - Joint segmentation and depth
    estimation | $1$ | ✓ | $1$ | $0.2$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $0.156$ | $-$ | $0.510$ | $0.187$ | $0.140$ | $-$ | $0.468$ | $-$ | $0.815$ |
    $0.962$ | $0.992$ |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| Zhang *et al.* [[52](#bib.bib52)] | ResNet50 - 关节分割与深度估计 | $1$ | ✓ | $1$
    | $0.2$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.156$ | $-$ | $0.510$ |
    $0.187$ | $0.140$ | $-$ | $0.468$ | $-$ | $0.815$ | $0.962$ | $0.992$ |'
- en: '| Zou [[72](#bib.bib72)] | Joint depth and flow | $2$ | ✕ | $2$ | $-$ | $0.150$
    | $1.124$ | $5.507$ | $0.223$ | $0.806$ | $0.933$ | $0.973$ | $0.331$ | $2.698-$
    | $0.416$ | $6.89$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| Zou [[72](#bib.bib72)] | 关节深度与流动 | $2$ | ✕ | $2$ | $-$ | $0.150$ | $1.124$
    | $5.507$ | $0.223$ | $0.806$ | $0.933$ | $0.973$ | $0.331$ | $2.698-$ | $0.416$
    | $6.89$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
- en: '| Zhou [[95](#bib.bib95)] | Depth + pose | $\geq 2$ | ✕ | $1$ | $-$ | $0.183$
    | $1.595$ | $6.709$ | $0.270$ | $0.734$ | $0.902$ | $0.959$ | $0.383$ | $5.321$
    | $10.47$ | $0.478$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| Zhou [[95](#bib.bib95)] | 深度 + 位姿 | $\geq 2$ | ✕ | $1$ | $-$ | $0.183$ |
    $1.595$ | $6.709$ | $0.270$ | $0.734$ | $0.902$ | $0.959$ | $0.383$ | $5.321$
    | $10.47$ | $0.478$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
- en: '| Zhou [[44](#bib.bib44)] | 3D-guided cycle consistency | $2$ | ✕ | $2$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| Zhou [[44](#bib.bib44)] | 3D引导的循环一致性 | $2$ | ✕ | $2$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ |'
- en: '| Dosovitski [[12](#bib.bib12)] | Regression from calibrated stereo images
    | $2$ (calibrated) | ✓ | $2$ (calibrated) | $1.05$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| Dosovitski [[12](#bib.bib12)] | 从标定立体图像回归 | $2$（标定） | ✓ | $2$（标定） | $1.05$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ |'
- en: '| [[15](#bib.bib15)] | Regression from a pair of images | $2$ | ✓ | $2$ | $0.11$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| [[15](#bib.bib15)] | 从一对图像回归 | $2$ | ✓ | $2$ | $0.11$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ |'
- en: '| Pang [[23](#bib.bib23)] | Cascade residual learning | $2$ (calibrated) |
    ✓ | $2$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| Pang [[23](#bib.bib23)] | 级联残差学习 | $2$（标定） | ✓ | $2$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ |'
- en: '| Ilg [[68](#bib.bib68)] | Extension of FlowNet [[12](#bib.bib12)] | $2$ |
    ✓ | $2$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| Ilg [[68](#bib.bib68)] | FlowNet 的扩展 [[12](#bib.bib12)] | $2$ | ✓ | $2$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
- en: '| Li [[1](#bib.bib1)] | Depth from multiscale patches, refinement with CRF
    | $1$ | ✓ | $1$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.278$ | $-$
    | $7.188$ | $-$ | $0.232$ | $-$ | $0.821$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| Li [[1](#bib.bib1)] | 从多尺度补丁中提取深度，使用 CRF 进行优化 | $1$ | ✓ | $1$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.278$ | $-$ | $7.188$ | $-$ | $0.232$
    | $-$ | $0.821$ | $-$ | $-$ | $-$ | $-$ |'
- en: '| Liu [[4](#bib.bib4)] | Refinement with continuous CRF | $1$ | ✓ | $1$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.314$ | $-$ | $0.314$ | $-$ | $0.230$
    | $-$ | $0.824$ | $-$ | $0.614$ | $0.883$ | $0.971$ |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| Liu [[4](#bib.bib4)] | 使用连续 CRF 进行优化 | $1$ | ✓ | $1$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $0.314$ | $-$ | $0.314$ | $-$ | $0.230$ | $-$ |
    $0.824$ | $-$ | $0.614$ | $0.883$ | $0.971$ |'
- en: '| Xie [[115](#bib.bib115)] | Predict one view from another using estimated
    depth | $2$ stereo | ✕ | $1$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| Xie [[115](#bib.bib115)] | 使用估计深度从一个视图预测另一个视图 | $2$ 立体 | ✕ | $1$ | $-$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $-$ |'
- en: '| Chen [[67](#bib.bib67)] | Training with one ordinal relation per image |
    $1$ | 1 ordinal | $1$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $0.34$ | $0.42$ | $1.10$ | $0.38$ | $-$ | $-$ | $-$ |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[67](#bib.bib67)] | 每张图像训练一个有序关系 | $1$ | 1 有序 | $1$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.34$ | $0.42$ | $1.10$
    | $0.38$ | $-$ | $-$ | $-$ |'
- en: '| Mayer [[13](#bib.bib13)] | A dataset for training | $1$ | ✓ | $0.06$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| Mayer [[13](#bib.bib13)] | 用于训练的数据集 | $1$ | ✓ | $0.06$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ |'
- en: Table [VII](#S6.T7 "TABLE VII ‣ 6.4 Depth regression techniques ‣ 6 Discussion
    and comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction") summarizes the properties and compares the performance of some
    of the state-of-the-art methods for deep learning-based depth regression on KITTI2012,
    Make3D, and NYUDv2 datasets.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [VII](#S6.T7 "TABLE VII ‣ 6.4 Depth regression techniques ‣ 6 Discussion and
    comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    总结了用于 KITTI2012、Make3D 和 NYUDv2 数据集的深度学习基础的深度回归技术的一些最先进方法的属性并进行了性能比较。
- en: As shown in this table, the methods of Jiao *et al.* [[74](#bib.bib74)] and
    Fu *et al.* [[41](#bib.bib41)] seem to achieve the best accuracy on NYUDv2 dataset.
    Their common property is the way they handle different depth ranges. In fact,
    previous methods treat near and far depth values equally. As such, most of them
    achieve good accuracy in near depth values but their accuracy drops for distant
    depth values. Jiao *et al.* [[74](#bib.bib74)] proposed a new loss function that
    pays more attention to distant depths. Fu *et al.* [[41](#bib.bib41)], on the
    other hand, formulated depth estimation as a classification problem. The depth
    range is first discretized into intervals. The network then learn to classify
    each image pixel into one of the depth intervals. However, instead of using uniform
    discretization, Fu *et al.* [[41](#bib.bib41)] used a spacing increasing discretization.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 如表所示，Jiao *et al.* [[74](#bib.bib74)] 和 Fu *et al.* [[41](#bib.bib41)] 方法似乎在
    NYUDv2 数据集上实现了最佳准确性。他们的共同特点是处理不同深度范围的方式。事实上，之前的方法对近距离和远距离深度值的处理是一样的。因此，大多数方法在近距离深度值上准确性较高，但在远距离深度值上准确性下降。Jiao
    *et al.* [[74](#bib.bib74)] 提出了一个新的损失函数，更关注远处的深度。另一方面，Fu *et al.* [[41](#bib.bib41)]
    将深度估计公式化为分类问题。深度范围首先被离散化为区间。然后，网络学习将每个图像像素分类到一个深度区间。然而，Fu *et al.* [[41](#bib.bib41)]
    使用了间隔增加的离散化，而不是均匀离散化。
- en: Another observation from Table LABEL:tab:performance_depthregerssion is that
    recent techniques trained without 3D supervision, *e.g.,* by using stereo images
    and re-projection loss, are becoming very competitive since their performances
    are close to techniques that use 3D supervision. Kuznietsov *et al.* [[105](#bib.bib105)]
    showed that the performance can be even further improved using semi-supervised
    techniques where the network is first trained with 3D supervision and then fine-tuned
    with stereo supervision. Also, training with ordinal relations seems to improve
    the performance of depth estimation. In fact, while the early work of Chen *et
    al.* [[67](#bib.bib67)], which used one ordinal relation per image, achieved relatively
    low performance, the recent work of Xian *et al.* [[104](#bib.bib104)], which
    used $3$K ordinal relations per image, achieved an accuracy that is very close
    to supervised techniques. Their main benefit is that, in general metric depths
    obtained by stereo matching or depth sensor are noisy. However, their corresponding
    ordinal depths are accurate. Thus, obtaining reliable ordinal depths for training
    is significantly easier.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 从表 LABEL:tab:performance_depthregerssion 另一个观察结果是，最近那些没有 3D 监督的技术，例如通过使用立体图像和重投影损失，正变得非常有竞争力，因为它们的表现接近使用
    3D 监督的技术。Kuznietsov *et al.* [[105](#bib.bib105)] 表明，通过使用半监督技术，性能甚至可以进一步提升，其中网络首先使用
    3D 监督进行训练，然后用立体监督进行微调。此外，使用序关系的训练似乎提高了深度估计的性能。事实上，虽然 Chen *et al.* [[67](#bib.bib67)]
    的早期工作（每幅图像使用一个序关系）表现相对较低，但 Xian *et al.* [[104](#bib.bib104)] 的最新工作（每幅图像使用 $3$K
    个序关系）达到了非常接近监督技术的准确性。他们的主要优势在于，一般通过立体匹配或深度传感器获得的度量深度是嘈杂的。然而，它们相应的序深度是准确的。因此，获得可靠的序深度用于训练要容易得多。
- en: Finally, methods which jointly estimate depth and normals maps [[48](#bib.bib48)]
    or depth and semantic segmentation [[52](#bib.bib52)] outperform many methods
    that estimate depth alone. Their performance can be further improved by using
    the loss function of [[74](#bib.bib74)], which pays more attention to distant
    depths, or by using the spacing increasing depth range discretization of [[41](#bib.bib41)].
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，那些同时估计深度和法线图[[48](#bib.bib48)]或深度和语义分割[[52](#bib.bib52)]的方法优于许多仅估计深度的方法。通过使用[[74](#bib.bib74)]的损失函数（该函数更关注远处深度），或者通过使用[[41](#bib.bib41)]的间隔增加的深度范围离散化，这些方法的性能可以进一步提升。
- en: 7 Future research directions
  id: totrans-568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来研究方向
- en: Despite the extensive research undertaken in the past five years, deep learning-based
    depth reconstruction achieved promising results. The topic, however, is still
    in its infancy and further developments are yet to be expected. In this section,
    we present some of the current issues and highlight directions for future research.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在过去五年中进行了广泛的研究，基于深度学习的深度重建仍取得了令人满意的结果。然而，这一主题仍处于起步阶段，预计还会有进一步的发展。在这一部分，我们介绍了一些当前的问题，并突出未来研究的方向。
- en: •
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input: Most of the current techniques do not handle high resolution input,
    require calibrated images, and cannot vary the number of input images at training
    and testing without re-training. The former is mainly due to the computation and
    memory requirements of most of the deep learning techniques. Developments in high
    performance computing can address this issue in the future. However, developing
    lighter deep architectures remains desirable especially if to be deployed in mobile
    and portable platforms.'
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入：目前的大多数技术无法处理高分辨率输入，需要校准图像，并且在训练和测试时无法改变输入图像的数量而不重新训练。这主要是由于大多数深度学习技术的计算和内存需求。未来，高性能计算的发展可以解决这个问题。然而，开发更轻量的深度架构仍然是值得期望的，尤其是要在移动和便携平台上部署时。
- en: •
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Accuracy: Although refinement modules can improve the resolution of the estimated
    depth maps, it is still small compared to the resolution of the images that can
    be recovered. As such, deep learning techniques find it difficult to recover small
    details, *e.g.,* vegetation and hair. Also, most of the techniques discretize
    the depth range. Although some methods can achieve sub-pixel accuracy, changing
    the depth range, and the discretization frequency, requires retraining the networks.
    Another issue is the accuracy, which, in general, varies for different depth ranges.
    Some of the recent works, *e.g.,*  [[74](#bib.bib74)], tried to address this problem,
    but it still remains an open and challenging problem since it is highly related
    to the data bias issue and the type of loss functions used to train the network.
    Accuracy of existing methods is also affected by complex scenarios, *e.g.,* occlusions
    and highly cluttered scenes, and objects with complex material properties.'
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确性：尽管精细化模块可以提高估计深度图的分辨率，但与可以恢复的图像分辨率相比仍然较小。因此，深度学习技术在恢复小细节方面存在困难，例如*植物*和*头发*。此外，大多数技术对深度范围进行离散化。虽然一些方法可以实现亚像素精度，但改变深度范围和离散化频率需要重新训练网络。另一个问题是准确性，一般在不同深度范围内有所变化。一些近期的工作，例如[[74](#bib.bib74)]，尝试解决这个问题，但由于它与数据偏差问题和用于训练网络的损失函数类型高度相关，因此仍然是一个开放且具有挑战性的问题。现有方法的准确性也受到复杂场景的影响，例如*遮挡*和*高度混乱的场景*，以及具有复杂材料属性的物体。
- en: •
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Performance: Complex deep networks are very expensive in terms of memory requirements.
    Memory footprint is even a major issue when dealing with high resolution images
    and when aiming to reconstruct high resolution depth maps. While this can be mitigated
    by using multi-scale and part-based reconstruction techniques, it can result in
    high computation time.'
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能：复杂的深度网络在内存需求方面非常昂贵。当处理高分辨率图像以及重建高分辨率深度图时，内存占用甚至是一个主要问题。虽然可以通过使用多尺度和基于部件的重建技术来缓解这一问题，但这可能会导致较高的计算时间。
- en: •
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training: Deep learning techniques rely heavily on the availability of training
    datasets annotated with ground-truth labels. Obtaining ground-truth labels for
    depth reconstruction is very expensive. Existing techniques mitigate this problem
    by either designing loss functions that do not require 3D annotations, or use
    domain adaptation and transfer learning strategies. The former, however, requires
    calibrated cameras. Domain adaptation techniques are recently attracting more
    attention since, with these techniques, one can train with synthetic data, which
    are easy to obtain, and real-world data.'
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练：深度学习技术在很大程度上依赖于带有真实标签的训练数据集。获取深度重建的真实标签非常昂贵。现有技术通过设计不需要三维注释的损失函数或使用领域适应和迁移学习策略来缓解这一问题。然而，前者需要校准相机。领域适应技术最近受到更多关注，因为使用这些技术，可以使用易于获取的合成数据和现实世界数据进行训练。
- en: •
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data bias and generalization: Most of the recent deep learning-based depth
    reconstruction techniques have been trained and tested on publicly available benchmarks.
    While this gives an indication on their performances, it is not clear yet how
    do they generalize and perform on completely unseen images, from a completely
    different category. Thus, we expect in the future to see the emergence of large
    datasets, similar to ImageNet but for 3D reconstruction. Developing self-adaptation
    techniques, *i.e.,* techniques that can adapt themselves to new scenarios in real
    time or with minimum supervision, is one promising direction for future research.'
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据偏差和泛化：大多数近期基于深度学习的深度重建技术已在公开可用的基准数据上进行训练和测试。虽然这能提供它们性能的指示，但目前尚不清楚它们如何在完全不同类别的全新图像上泛化和表现。因此，我们期望未来会出现类似于
    ImageNet 的大规模数据集，但用于 3D 重建。开发自适应技术，*即*，能够实时或在最小监督下适应新场景的技术，是未来研究的一个有前途的方向。
- en: 8 Conclusion
  id: totrans-580
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: This paper provided a comprehensive survey of the recent developments in depth
    reconstruction using deep learning techniques. Despite its infancy, these techniques
    are achieving acceptable results, and some recent developments are even competing,
    in terms of accuracy of the results, with traditional techniques. We have seen
    that, since 2014, more than $100$ papers on the topic have been published in major
    computer vision and machine learning conferences and journals, and more new papers
    are being published even during the final stage of this submission. We believe
    that since 2014, we entered a new era where data-driven and machine learning techniques
    play a central role in image-based depth reconstruction.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了深度学习技术在深度重建领域的最新发展的全面调查。尽管这些技术还处于初期阶段，但它们已经取得了可接受的结果，并且一些最新的发展在结果的准确性上甚至与传统技术竞争。我们看到，自
    2014 年以来，已有超过 $100$ 篇关于该主题的论文发表在主要计算机视觉和机器学习会议及期刊中，并且即使在提交的最终阶段，新的论文仍在不断发表。我们相信，自
    2014 年以来，我们进入了一个新纪元，在这个纪元中，数据驱动和机器学习技术在基于图像的深度重建中发挥了核心作用。
- en: 'Finally, there are several related topics that have not been covered in this
    survey. Examples include:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有一些相关主题在本次调查中没有涵盖。例如：
- en: •
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Synthesis of novel 2D views from one or multiple images, which use similar formulations
    as depth estimation, see for example [[6](#bib.bib6), [5](#bib.bib5), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)].
  id: totrans-584
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从一张或多张图像中合成新颖的二维视图，这些方法采用与深度估计类似的公式，参见例如 [[6](#bib.bib6), [5](#bib.bib5), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)]。
- en: •
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM),
    which aim to recover at the same time depth maps and (relative) camera pose (or
    camera motion). Interested readers can refer to recent papers such as [[13](#bib.bib13),
    [100](#bib.bib100), [116](#bib.bib116)].
  id: totrans-586
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结构光束法（SfM）和同时定位与建图（SLAM），旨在同时恢复深度图和（相对）相机姿态（或相机运动）。有兴趣的读者可以参考最近的论文，如 [[13](#bib.bib13),
    [100](#bib.bib100), [116](#bib.bib116)]。
- en: •
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Image-based object reconstruction algorithms, which aim to recover the entire
    3D geometry of objects from one or multiple images.
  id: totrans-588
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于图像的物体重建算法，旨在从一张或多张图像中恢复物体的整个三维几何形状。
- en: While these topics are strongly related to depth estimation, they require a
    separate survey given the large amount of work that has been dedicated to them
    in the past 4 to 5 years.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些主题与深度估计密切相关，但鉴于过去 4 到 5 年中为这些主题投入的大量工作，它们需要单独的调查。
- en: References
  id: totrans-590
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] B. Li, C. Shen, Y. Dai, A. Van Den Hengel, and M. He, “Depth and surface
    normal estimation from monocular images using regression on deep features and
    hierarchical CRFs,” in *IEEE CVPR*, 2015, pp. 1119–1127.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] B. Li, C. Shen, Y. Dai, A. Van Den Hengel, 和 M. He，“通过对深度特征和分层 CRF 的回归进行单目图像的深度和表面法线估计”，在
    *IEEE CVPR* 中，2015, 页 1119–1127。'
- en: '[2] D. Eigen and R. Fergus, “Predicting depth, surface normals and semantic
    labels with a common multi-scale convolutional architecture,” in *IEEE ICCV*,
    2015, pp. 2650–2658.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Eigen 和 R. Fergus，“使用通用多尺度卷积结构预测深度、表面法线和语义标签”，在 *IEEE ICCV* 中，2015,
    页 2650–2658。'
- en: '[3] R. Garg, V. K. BG, G. Carneiro, and I. Reid, “Unsupervised CNN for single
    view depth estimation: Geometry to the rescue,” in *ECCV*.   Springer, 2016, pp.
    740–756.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] R. Garg, V. K. BG, G. Carneiro, 和 I. Reid，“用于单视图深度估计的无监督 CNN：几何学的救助”，在
    *ECCV* 中。 Springer, 2016, 页 740–756。'
- en: '[4] F. Liu, C. Shen, G. Lin, and I. D. Reid, “Learning depth from single monocular
    images using deep convolutional neural fields,” *IEEE PAMI*, vol. 38, no. 10,
    pp. 2024–2039, 2016.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] F. Liu, C. Shen, G. Lin, 和 I. D. Reid, “通过深度卷积神经场从单张单目图像中学习深度，” *IEEE PAMI*，第38卷，第10期，页2024–2039，2016。'
- en: '[5] J. Yang, S. E. Reed, M.-H. Yang, and H. Lee, “Weakly-supervised disentangling
    with recurrent transformations for 3D view synthesis,” in *NIPS*, 2015, pp. 1099–1107.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Yang, S. E. Reed, M.-H. Yang, 和 H. Lee, “用于3D视图合成的弱监督解耦与递归变换，” 载于 *NIPS*，2015，页1099–1107。'
- en: '[6] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum, “Deep convolutional
    inverse graphics network,” in *Advances in Neural Information Processing Systems*,
    2015, pp. 2539–2547.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. D. Kulkarni, W. F. Whitney, P. Kohli, 和 J. Tenenbaum, “深度卷积逆图形网络，” 载于
    *Advances in Neural Information Processing Systems*，2015，页2539–2547。'
- en: '[7] M. Tatarchenko, A. Dosovitskiy, and T. Brox, “Multi-view 3D models from
    single images with a convolutional network,” in *ECCV*.   Springer, 2016, pp.
    322–337.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Tatarchenko, A. Dosovitskiy, 和 T. Brox, “通过卷积网络从单张图像中生成多视角3D模型，” 载于
    *ECCV*。 Springer，2016，页322–337。'
- en: '[8] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros, “View synthesis
    by appearance flow,” in *ECCV*.   Springer, 2016, pp. 286–301.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] T. Zhou, S. Tulsiani, W. Sun, J. Malik, 和 A. A. Efros, “通过外观流进行视图合成，” 载于
    *ECCV*。 Springer，2016，页286–301。'
- en: '[9] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C. Berg, “Transformation-grounded
    image generation network for novel 3D view synthesis,” in *IEEE CVPR*.   IEEE,
    2017, pp. 702–711.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] E. Park, J. Yang, E. Yumer, D. Ceylan, 和 A. C. Berg, “用于新颖3D视图合成的基于变换的图像生成网络，”
    载于 *IEEE CVPR*。 IEEE，2017，页702–711。'
- en: '[10] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a single
    image using a multi-scale deep network,” in *Advances in neural information processing
    systems*, 2014, pp. 2366–2374.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] D. Eigen, C. Puhrsch, 和 R. Fergus, “使用多尺度深度网络从单张图像中预测深度图，” 载于 *Advances
    in neural information processing systems*，2014，页2366–2374。'
- en: '[11] X. Wang, D. Fouhey, and A. Gupta, “Designing deep networks for surface
    normal estimation,” in *IEEE CVPR*, 2015, pp. 539–547.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] X. Wang, D. Fouhey, 和 A. Gupta, “为表面法线估计设计深度网络，” 载于 *IEEE CVPR*，2015，页539–547。'
- en: '[12] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
    P. Van Der Smagt, D. Cremers, and T. Brox, “FlowNet: Learning optical flow with
    convolutional networks,” in *IEEE ICCV*, 2015, pp. 2758–2766.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
    P. Van Der Smagt, D. Cremers, 和 T. Brox, “FlowNet: 使用卷积网络学习光流，” 载于 *IEEE ICCV*，2015，页2758–2766。'
- en: '[13] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
    and T. Brox, “A large dataset to train convolutional networks for disparity, optical
    flow, and scene flow estimation,” in *IEEE CVPR*, 2016, pp. 4040–4048.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
    和 T. Brox, “用于视差、光流和场景流估计的卷积网络的大型数据集，” 载于 *IEEE CVPR*，2016，页4040–4048。'
- en: '[14] J. Flynn, I. Neulander, J. Philbin, and N. Snavely, “DeepStereo: Learning
    to predict new views from the world’s imagery,” in *IEEE CVPR*, 2016, pp. 5515–5524.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. Flynn, I. Neulander, J. Philbin, 和 N. Snavely, “DeepStereo: 学习从世界图像中预测新视图，”
    载于 *IEEE CVPR*，2016，页5515–5524。'
- en: '[15] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and
    T. Brox, “Demon: Depth and motion network for learning monocular stereo,” in *IEEE
    CVPR*, vol. 5, 2017, p. 6.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, 和
    T. Brox, “Demon: 深度与运动网络用于学习单目立体，” 载于 *IEEE CVPR*，第5卷，2017，第6页。'
- en: '[16] D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame
    stereo correspondence algorithms,” *IJCV*, vol. 47, no. 1-3, pp. 7–42, 2002.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] D. Scharstein 和 R. Szeliski, “密集双帧立体对应算法的分类与评估，” *IJCV*，第47卷，第1-3期，页7–42，2002。'
- en: '[17] J. Zbontar and Y. LeCun, “Computing the stereo matching cost with a convolutional
    neural network,” in *IEEE CVPR*, 2015, pp. 1592–1599.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. Zbontar 和 Y. LeCun, “利用卷积神经网络计算立体匹配成本，” 载于 *IEEE CVPR*，2015，页1592–1599。'
- en: '[18] ——, “Stereo matching by training a convolutional neural network to compare
    image patches,” *Journal of Machine Learning Research*, vol. 17, no. 1-32, p. 2,
    2016.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] ——, “通过训练卷积神经网络比较图像块进行立体匹配，” *Journal of Machine Learning Research*，第17卷，第1-32期，第2页，2016。'
- en: '[19] W. Luo, A. G. Schwing, and R. Urtasun, “Efficient deep learning for stereo
    matching,” in *IEEE CVPR*, 2016, pp. 5695–5703.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] W. Luo, A. G. Schwing, 和 R. Urtasun, “高效的深度学习立体匹配，” 载于 *IEEE CVPR*，2016，页5695–5703。'
- en: '[20] w. Chen, X. Sun, L. Wang, Y. Yu, and C. Huang, “A deep visual correspondence
    embedding model for stereo matching costs,” in *IEEE ICCV*, 2015, pp. 972–980.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] w. Chen, X. Sun, L. Wang, Y. Yu, 和 C. Huang, “用于立体匹配成本的深度视觉对应嵌入模型，” 载于
    *IEEE ICCV*，2015，页972–980。'
- en: '[21] A. Shaked and L. Wolf, “Improved stereo matching with constant highway
    networks and reflective confidence learning,” in *IEEE CVPR*, 2017, pp. 4641–4650.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Shaked 和 L. Wolf，“通过常量高速公路网络和反射置信度学习改进立体匹配”，在*IEEE CVPR*，2017，第4641–4650页。'
- en: '[22] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg, “MatchNet: Unifying
    feature and metric learning for patch-based matching,” in *IEEE CVPR*, 2015, pp.
    3279–3286.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] X. Han, T. Leung, Y. Jia, R. Sukthankar, 和 A. C. Berg，“MatchNet: 统一特征和度量学习用于基于补丁的匹配”，在*IEEE
    CVPR*，2015，第3279–3286页。'
- en: '[23] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan, “Cascade residual learning:
    A two-stage convolutional neural network for stereo matching,” in *ICCV Workshops*,
    vol. 7, no. 8, 2017.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Pang, W. Sun, J. S. Ren, C. Yang, 和 Q. Yan，“级联残差学习: 用于立体匹配的两阶段卷积神经网络”，在*ICCV
    Workshops*，第7卷，第8期，2017。'
- en: '[24] L. Yu, Y. Wang, Y. Wu, and Y. Jia, “Deep stereo matching with explicit
    cost aggregation sub-architecture,” *AAAI*, 2018.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] L. Yu, Y. Wang, Y. Wu, 和 Y. Jia，“具有显式成本聚合子架构的深度立体匹配”，*AAAI*，2018。'
- en: '[25] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia, “SegStereo: Exploiting
    Semantic Information for Disparity Estimation,” *arXiv preprint arXiv:1807.11699*,
    2018.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] G. Yang, H. Zhao, J. Shi, Z. Deng, 和 J. Jia，“SegStereo: 利用语义信息进行视差估计”，*arXiv
    预印本 arXiv:1807.11699*，2018。'
- en: '[26] Z. Liang, Y. Feng, Y. G. H. L. W. Chen, and L. Q. L. Z. J. Zhang, “Learning
    for disparity estimation through feature constancy,” in *IEEE CVPR*, 2018, pp.
    2811–2820.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Z. Liang, Y. Feng, Y. G. H. L. W. Chen, 和 L. Q. L. Z. J. Zhang，“通过特征一致性进行视差估计的学习”，在*IEEE
    CVPR*，2018，第2811–2820页。'
- en: '[27] S. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin, and S. Izadi,
    “Stereonet: Guided hierarchical refinement for real-time edge-aware depth prediction,”
    *ECCV*, 2018.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin, 和 S. Izadi，“Stereonet:
    实时边缘感知深度预测的引导层次细化”，*ECCV*，2018。'
- en: '[28] J. Chang and Y. Chen, “Pyramid Stereo Matching Network,” *CVPR*, 2018.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Chang 和 Y. Chen，“金字塔立体匹配网络”，*CVPR*，2018。'
- en: '[29] Y. Zhong, Y. Dai, and H. Li, “Self-supervised learning for stereo matching
    with self-improving ability,” *arXiv preprint arXiv:1709.00930*, 2017.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Y. Zhong, Y. Dai, 和 H. Li，“具有自我改进能力的自监督学习立体匹配”，*arXiv 预印本 arXiv:1709.00930*，2017。'
- en: '[30] A. Seki and M. Pollefeys, “SGM-Nets: Semi-global matching with neural
    networks,” in *IEEE CVPR Workshops*, 2017, pp. 21–26.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] A. Seki 和 M. Pollefeys，“SGM-Nets: 使用神经网络的半全局匹配”，在*IEEE CVPR Workshops*，2017，第21–26页。'
- en: '[31] X. Song, X. Zhao, H. Hu, and L. Fang, “EdgeStereo: A Context Integrated
    Residual Pyramid Network for Stereo Matching,” *ACCV*, 2018.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] X. Song, X. Zhao, H. Hu, 和 L. Fang，“EdgeStereo: 一种集成上下文的残差金字塔网络用于立体匹配”，*ACCV*，2018。'
- en: '[32] S. Tulyakov, A. Ivanov, and F. Fleuret, “Practical Deep Stereo (PDS):
    Toward applications-friendly deep stereo matching,” *NIPS*, 2018.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. Tulyakov, A. Ivanov, 和 F. Fleuret，“实用深度立体（PDS）：面向应用的深度立体匹配”，*NIPS*，2018。'
- en: '[33] Z. Jie, P. Wang, Y. Ling, B. Zhao, Y. Wei, J. Feng, and W. Liu, “Left-right
    comparative recurrent model for stereo matching,” in *IEEE CVPR*, 2018, pp. 3838–3846.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Z. Jie, P. Wang, Y. Ling, B. Zhao, Y. Wei, J. Feng, 和 W. Liu，“左-右比较递归模型用于立体匹配”，在*IEEE
    CVPR*，2018，第3838–3846页。'
- en: '[34] S. Zagoruyko and N. Komodakis, “Learning to compare image patches via
    convolutional neural networks,” in *IEEE CVPR*, 2015, pp. 4353–4361.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Zagoruyko 和 N. Komodakis，“通过卷积神经网络学习比较图像补丁”，在*IEEE CVPR*，2015，第4353–4361页。'
- en: '[35] W. Hartmann, S. Galliani, M. Havlena, L. Van Gool, and K. Schindler, “Learned
    multi-patch similarity,” in *IEEE ICCV*.   IEEE, 2017, pp. 1595–1603.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] W. Hartmann, S. Galliani, M. Havlena, L. Van Gool, 和 K. Schindler，“学习的多补丁相似性”，在*IEEE
    ICCV*。IEEE，2017，第1595–1603页。'
- en: '[36] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang, “DeepMVS:
    Learning Multi-view Stereopsis,” in *IEEE CVPR*, 2018, pp. 2821–2830.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, 和 J.-B. Huang，“DeepMVS: 学习多视角立体视觉”，在*IEEE
    CVPR*，2018，第2821–2830页。'
- en: '[37] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan, “MVSNet: Depth Inference
    for Unstructured Multi-view Stereo,” *ECCV*, 2018.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Y. Yao, Z. Luo, S. Li, T. Fang, 和 L. Quan，“MVSNet: 非结构化多视角立体深度推断”，*ECCV*，2018。'
- en: '[38] A. Kar, C. Häne, and J. Malik, “Learning a multi-view stereo machine,”
    in *NIPS*, 2017, pp. 364–375.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Kar, C. Häne, 和 J. Malik，“学习多视角立体机器”，在*NIPS*，2017，第364–375页。'
- en: '[39] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach,
    and A. Bry, “End-to-end learning of geometry and context for deep stereo regression,”
    *CoRR, vol. abs/1703.04309*, 2017.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach,
    和 A. Bry，“端到端学习几何和上下文以进行深度立体回归”，*CoRR，第abs/1703.04309卷*，2017。'
- en: '[40] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer,
    “Discriminative learning of deep convolutional feature point descriptors,” in
    *IEEE ICCV*, 2015, pp. 118–126.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, 和 F. Moreno-Noguer，“深度卷积特征点描述符的判别学习，”发表于*IEEE
    ICCV*，2015年，第118–126页。'
- en: '[41] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao, “Deep ordinal regression
    network for monocular depth estimation,” in *IEEE CVPR*, June 2018.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Fu, M. Gong, C. Wang, K. Batmanghelich, 和 D. Tao，“用于单目深度估计的深度序数回归网络，”发表于*IEEE
    CVPR*，2018年6月。'
- en: '[42] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *IEEE CVPR*, 2016, pp. 770–778.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，”发表于*IEEE CVPR*，2016年，第770–778页。'
- en: '[43] H. Park and K. M. Lee, “Look wider to match image patches with convolutional
    neural networks,” *IEEE Signal Processing Letters*, vol. 24, no. 12, pp. 1788–1792,
    2017.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] H. Park 和 K. M. Lee，“通过卷积神经网络扩大视野以匹配图像块，”*IEEE信号处理快报*，第24卷，第12期，第1788–1792页，2017年。'
- en: '[44] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang, and A. A. Efros, “Learning
    dense correspondence via 3D-guided cycle consistency,” in *IEEE CVPR*, 2016, pp.
    117–126.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang, 和 A. A. Efros，“通过3D引导的循环一致性学习密集对应关系，”发表于*IEEE
    CVPR*，2016年，第117–126页。'
- en: '[45] H. Hirschmuller, “Stereo processing by semiglobal matching and mutual
    information,” *IEEE PAMI*, vol. 30, no. 2, pp. 328–341, 2008.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] H. Hirschmuller，“通过半全局匹配和互信息进行立体处理，”*IEEE PAMI*，第30卷，第2期，第328–341页，2008年。'
- en: '[46] J.-H. Lee, M. Heo, K.-R. Kim, and C.-S. Kim, “Single-image depth estimation
    based on fourier domain analysis,” in *IEEE CVPR*, June 2018.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J.-H. Lee, M. Heo, K.-R. Kim, 和 C.-S. Kim，“基于傅里叶域分析的单幅图像深度估计，”发表于*IEEE
    CVPR*，2018年6月。'
- en: '[47] S. Gidaris and N. Komodakis, “Detect, replace, refine: Deep structured
    prediction for pixel wise labeling,” in *Proc. of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017, pp. 5248–5257.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] S. Gidaris 和 N. Komodakis，“检测、替换、优化：用于像素级标记的深度结构预测，”发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，第5248–5257页。'
- en: '[48] X. Qi, R. Liao, Z. Liu, R. Urtasun, and J. Jia, “GeoNet: Geometric Neural
    Network for Joint Depth and Surface Normal Estimation,” in *IEEE CVPR*, June 2018.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] X. Qi, R. Liao, Z. Liu, R. Urtasun, 和 J. Jia，“GeoNet：用于联合深度和表面法线估计的几何神经网络，”发表于*IEEE
    CVPR*，2018年6月。'
- en: '[49] Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle, V. Tankovich,
    M. Schoenberg, S. Izadi, T. Funkhouser, and S. Fanello, “ActiveStereoNet: end-to-end
    self-supervised learning for active stereo systems,” *ECCV*, 2018.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle, V. Tankovich,
    M. Schoenberg, S. Izadi, T. Funkhouser, 和 S. Fanello，“ActiveStereoNet：用于主动立体系统的端到端自监督学习，”*ECCV*，2018年。'
- en: '[50] S. Liu, S. De Mello, J. Gu, G. Zhong, M.-H. Yang, and J. Kautz, “Learning
    affinity via spatial propagation networks,” in *NIPS*, 2017, pp. 1520–1530.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Liu, S. De Mello, J. Gu, G. Zhong, M.-H. Yang, 和 J. Kautz，“通过空间传播网络学习亲和力，”发表于*NIPS*，2017年，第1520–1530页。'
- en: '[51] J. Jeon and S. Lee, “Reconstruction-based pairwise depth dataset for depth
    image enhancement using cnn,” in *ECCV*, September 2018.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] J. Jeon 和 S. Lee，“基于重建的配对深度数据集，用于使用CNN的深度图像增强，”发表于*ECCV*，2018年9月。'
- en: '[52] Z. Zhang, Z. Cui, C. Xu, Z. Jie, X. Li, and J. Yang, “Joint task-recursive
    learning for semantic segmentation and depth estimation,” in *ECCV*, September
    2018.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Z. Zhang, Z. Cui, C. Xu, Z. Jie, X. Li, 和 J. Yang，“用于语义分割和深度估计的联合任务递归学习，”发表于*ECCV*，2018年9月。'
- en: '[53] X. Cheng, P. Wang, and R. Yang, “Depth estimation via affinity learned
    with convolutional spatial propagation network,” in *ECCV*, September 2018.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] X. Cheng, P. Wang, 和 R. Yang，“通过卷积空间传播网络学习的亲和力深度估计，”发表于*ECCV*，2018年9月。'
- en: '[54] T. Brox and J. Malik, “Large displacement optical flow: descriptor matching
    in variational motion estimation,” *IEEE PAMI*, vol. 33, no. 3, pp. 500–513, 2011.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] T. Brox 和 J. Malik，“大位移光流：变分运动估计中的描述符匹配，”*IEEE PAMI*，第33卷，第3期，第500–513页，2011年。'
- en: '[55] P. Krähenbühl and V. Koltun, “Efficient inference in fully connected crfs
    with gaussian edge potentials,” in *NIPS*, 2011, pp. 109–117.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] P. Krähenbühl 和 V. Koltun，“在具有高斯边缘势的全连接CRF中高效推理，”发表于*NIPS*，2011年，第109–117页。'
- en: '[56] X. Sun, X. Mei, S. Jiao, M. Zhou, Z. Liu, and H. Wang, “Real-time local
    stereo via edge-aware disparity propagation,” *Pattern Recognition Letters*, vol. 49,
    pp. 201–206, 2014.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] X. Sun, X. Mei, S. Jiao, M. Zhou, Z. Liu, 和 H. Wang，“实时局部立体视图通过边缘感知视差传播，”*模式识别快报*，第49卷，第201–206页，2014年。'
- en: '[57] A. Odena, V. Dumoulin, and C. Olah, “Deconvolution and checkerboard artifacts,”
    *Distill*, vol. 1, no. 10, p. e3, 2016.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] A. Odena, V. Dumoulin, 和 C. Olah，“反卷积和棋盘格伪影，”*Distill*，第1卷，第10期，第e3页，2016年。'
- en: '[58] Z. Zhang, C. Xu, J. Yang, Y. Tai, and L. Chen, “Deep hierarchical guidance
    and regularization learning for end-to-end depth estimation,” *Pattern Recognition*,
    vol. 83, pp. 430–442, 2018.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Z. Zhang、C. Xu、J. Yang、Y. Tai 和 L. Chen，“用于端到端深度估计的深层次指导和正则化学习”，发表于 *模式识别*，第
    83 卷，第 430–442 页，2018 年。'
- en: '[59] A. Seki and M. Pollefeys, “Patch based confidence prediction for dense
    disparity map,” in *BMVC*, vol. 2, no. 3, 2016, p. 4.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] A. Seki 和 M. Pollefeys，“基于补丁的置信度预测以生成稠密视差图”，发表于 *BMVC*，第 2 卷，第 3 期，2016
    年，第 4 页。'
- en: '[60] S. Tulyakov, A. Ivanov, and F. Fleuret, “Weakly supervised learning of
    deep metrics for stereo reconstruction,” in *IEEE ICCV*, no. EPFL-CONF-233588,
    2017.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. Tulyakov、A. Ivanov 和 F. Fleuret，“用于立体重建的弱监督深度度量学习”，发表于 *IEEE ICCV*，编号
    EPFL-CONF-233588，2017 年。'
- en: '[61] P. Knöbelreiter, C. Reinbacher, A. Shekhovtsov, and T. Pock, “End-to-end
    training of hybrid cnn-crf models for stereo,” in *IEEE CVPR*.   IEEE, 2017, pp.
    1456–1465.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] P. Knöbelreiter、C. Reinbacher、A. Shekhovtsov 和 T. Pock，“用于立体的混合 CNN-CRF
    模型的端到端训练”，发表于 *IEEE CVPR*。IEEE，2017 年，第 1456–1465 页。'
- en: '[62] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang, “SurfaceNet: an end-to-end
    3D neural network for multiview stereopsis,” *IEEE ICCV*, 2017.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] M. Ji、J. Gall、H. Zheng、Y. Liu 和 L. Fang，“SurfaceNet：用于多视角立体视觉的端到端 3D 神经网络”，发表于
    *IEEE ICCV*，2017 年。'
- en: '[63] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth
    estimation with left-right consistency,” in *CVPR*, vol. 2, no. 6, 2017, p. 7.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] C. Godard、O. Mac Aodha 和 G. J. Brostow，“具有左右一致性的无监督单目深度估计”，发表于 *CVPR*，第
    2 卷，第 6 期，2017 年，第 7 页。'
- en: '[64] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab, “Deeper
    depth prediction with fully convolutional residual networks,” in *3D Vision (3DV)*.   IEEE,
    2016, pp. 239–248.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] I. Laina、C. Rupprecht、V. Belagiannis、F. Tombari 和 N. Navab，“通过全卷积残差网络进行更深层次的深度预测”，发表于
    *3D Vision (3DV)*。IEEE，2016 年，第 239–248 页。'
- en: '[65] Y. Gan, X. Xu, W. Sun, and L. Lin, “Monocular depth estimation with affinity,
    vertical pooling, and label enhancement,” in *ECCV*, September 2018.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Y. Gan、X. Xu、W. Sun 和 L. Lin，“具有亲和力、垂直池化和标签增强的单目深度估计”，发表于 *ECCV*，2018
    年 9 月。'
- en: '[66] Z. Chen, V. Badrinarayanan, G. Drozdov, and A. Rabinovich, “Estimating
    depth from rgb and sparse sensing,” in *ECCV*, September 2018.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Z. Chen、V. Badrinarayanan、G. Drozdov 和 A. Rabinovich，“从 RGB 和稀疏传感中估计深度”，发表于
    *ECCV*，2018 年 9 月。'
- en: '[67] W. Chen, Z. Fu, D. Yang, and J. Deng, “Single-image depth perception in
    the wild,” in *Advances in Neural Information Processing Systems*, 2016, pp. 730–738.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] W. Chen、Z. Fu、D. Yang 和 J. Deng，“在野外的单幅图像深度感知”，发表于 *神经信息处理系统进展*，2016 年，第
    730–738 页。'
- en: '[68] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox, “Flownet
    2.0: Evolution of optical flow estimation with deep networks,” in *IEEE CVPR*,
    vol. 2, 2017, p. 6.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] E. Ilg、N. Mayer、T. Saikia、M. Keuper、A. Dosovitskiy 和 T. Brox，“Flownet
    2.0：利用深度网络演变的光流估计”，发表于 *IEEE CVPR*，第 2 卷，2017 年，第 6 页。'
- en: '[69] A. Roy and S. Todorovic, “Monocular depth estimation using neural regression
    forest,” in *IEEE CVPR*, 2016, pp. 5506–5514.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Roy 和 S. Todorovic，“使用神经回归森林进行单目深度估计”，发表于 *IEEE CVPR*，2016 年，第 5506–5514
    页。'
- en: '[70] A. Chakrabarti, J. Shao, and G. Shakhnarovich, “Depth from a single image
    by harmonizing overcomplete local network predictions,” in *Advances in Neural
    Information Processing Systems*, 2016, pp. 2658–2666.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] A. Chakrabarti、J. Shao 和 G. Shakhnarovich，“通过协调过度完整的局部网络预测从单幅图像中获取深度”，发表于
    *神经信息处理系统进展*，2016 年，第 2658–2666 页。'
- en: '[71] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L. Yuille, “Towards
    unified depth and semantic prediction from a single image,” in *IEEE CVPR*, 2015,
    pp. 2800–2809.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] P. Wang、X. Shen、Z. Lin、S. Cohen、B. Price 和 A. L. Yuille，“从单幅图像中统一深度和语义预测”，发表于
    *IEEE CVPR*，2015 年，第 2800–2809 页。'
- en: '[72] Y. Zou, Z. Luo, and J.-B. Huang, “DF-Net: Unsupervised Joint Learning
    of Depth and Flow using Cross-Task Consistency,” in *ECCV*, September 2018.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Zou、Z. Luo 和 J.-B. Huang，“DF-Net：使用跨任务一致性进行无监督联合深度和光流学习”，发表于 *ECCV*，2018
    年 9 月。'
- en: '[73] D. Xu, W. Wang, H. Tang, H. Liu, N. Sebe, and E. Ricci, “Structured attention
    guided convolutional neural fields for monocular depth estimation,” in *The IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2018.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] D. Xu、W. Wang、H. Tang、H. Liu、N. Sebe 和 E. Ricci，“用于单目深度估计的结构化注意力引导卷积神经场”，发表于
    *IEEE 计算机视觉与模式识别会议 (CVPR)*，2018 年 6 月。'
- en: '[74] J. Jiao, Y. Cao, Y. Song, and R. Lau, “Look deeper into depth: Monocular
    depth estimation with semantic booster and attention-driven loss,” in *ECCV*,
    September 2018.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Jiao、Y. Cao、Y. Song 和 R. Lau，“深入探究深度：结合语义增强器和注意力驱动损失的单目深度估计”，发表于 *ECCV*，2018
    年 9 月。'
- en: '[75] H. Laga, Y. Guo, H. Tabia, R. B. Fisher, and M. Bennamoun, *3D Shape Analysis:
    Fundamentals, Theory, and Applications*.   John Wiley & Sons, 2018.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] H. Laga, Y. Guo, H. Tabia, R. B. Fisher, 和 M. Bennamoun，*3D 形状分析：基础、理论与应用*。
    John Wiley & Sons，2018 年。'
- en: '[76] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *IEEE CVPR*, 2016, pp. 3213–3223.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, 和 B. Schiele，“用于语义城市场景理解的 Cityscapes 数据集，”在 *IEEE CVPR* 中，2016
    年，第 3213–3223 页。'
- en: '[77] M. Menze and A. Geiger, “Object scene flow for autonomous vehicles,” in
    *IEEE CVPR*, 2015, pp. 3061–3070.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] M. Menze 和 A. Geiger，“用于自动驾驶车辆的对象场景流，”在 *IEEE CVPR* 中，2015 年，第 3061–3070
    页。'
- en: '[78] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *IEEE CVPR*.   IEEE, 2012, pp. 3354–3361.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] A. Geiger, P. Lenz, 和 R. Urtasun，“我们为自动驾驶做好准备了吗？KITTI 视觉基准套件，”在 *IEEE
    CVPR* 中。 IEEE，2012 年，第 3354–3361 页。'
- en: '[79] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A naturalistic
    open source movie for optical flow evaluation,” in *ECCV*.   Springer, 2012, pp.
    611–625.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] D. J. Butler, J. Wulff, G. B. Stanley, 和 M. J. Black，“用于光流评估的自然主义开源电影，”在
    *ECCV* 中。 Springer，2012 年，第 611–625 页。'
- en: '[80] J. Xiao, A. Owens, and A. Torralba, “Sun3d: A database of big spaces reconstructed
    using sfm and object labels,” in *IEEE ICCV*, 2013, pp. 1625–1632.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] J. Xiao, A. Owens, 和 A. Torralba，“Sun3d：一个使用 SFM 和物体标签重建的大空间数据库，”在 *IEEE
    ICCV* 中，2013 年，第 1625–1632 页。'
- en: '[81] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
    and support inference from rgbd images,” *ECCV*, pp. 746–760, 2012.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] N. Silberman, D. Hoiem, P. Kohli, 和 R. Fergus，“从 RGBD 图像进行室内分割和支撑推断，”
    *ECCV*，第 746–760 页，2012 年。'
- en: '[82] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A benchmark
    for the evaluation of rgb-d slam systems,” in *IEEE/RSJ IROS*, 2012, pp. 573–580.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Sturm, N. Engelhard, F. Endres, W. Burgard, 和 D. Cremers，“RGB-D SLAM
    系统评估基准测试，”在 *IEEE/RSJ IROS* 中，2012 年，第 573–580 页。'
- en: '[83] T. Schöps, J. L. Schönberger, S. Galliani, T. Sattler, K. Schindler, M. Pollefeys,
    and A. Geiger, “A multi-view stereo benchmark with high-resolution images and
    multi-camera videos,” in *IEEE CVPR*, vol. 2017, 2017.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] T. Schöps, J. L. Schönberger, S. Galliani, T. Sattler, K. Schindler, M.
    Pollefeys, 和 A. Geiger，“具有高分辨率图像和多相机视频的多视角立体基准测试，”在 *IEEE CVPR* 中，第 2017 卷，2017
    年。'
- en: '[84] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl, “Large-scale
    data for multiple-view stereopsis,” *IJCV*, vol. 120, no. 2, pp. 153–168, 2016.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, 和 A. B. Dahl，“大规模多视角立体视觉数据，”
    *IJCV*，第 120 卷，第 2 期，第 153–168 页，2016 年。'
- en: '[85] A. Saxena, M. Sun, and A. Y. Ng, “Make3d: Learning 3d scene structure
    from a single still image,” *IEEE PAMI*, vol. 31, no. 5, pp. 824–840, 2009.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] A. Saxena, M. Sun, 和 A. Y. Ng，“Make3d：从单幅静态图像学习 3D 场景结构，” *IEEE PAMI*，第
    31 卷，第 5 期，第 824–840 页，2009 年。'
- en: '[86] Z. Li and N. Snavely, “Megadepth: Learning single-view depth prediction
    from internet photos,” in *IEEE CVPR*, June 2018.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Z. Li 和 N. Snavely，“Megadepth：从互联网照片中学习单视图深度预测，”在 *IEEE CVPR* 中，2018 年
    6 月。'
- en: '[87] H. Laga, Q. Xie, I. H. Jermyn, A. Srivastava *et al.*, “Numerical inversion
    of srnf maps for elastic shape analysis of genus-zero surfaces,” *IEEE transactions
    on pattern analysis and machine intelligence*, vol. 39, no. 12, pp. 2451–2464,
    2017.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. Laga, Q. Xie, I. H. Jermyn, A. Srivastava *等*，“用于弹性形状分析的 SRNF 图的数值反演，”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*，第 39 卷，第 12 期，第
    2451–2464 页，2017 年。'
- en: '[88] I. H. Jermyn, S. Kurtek, H. Laga, and A. Srivastava, “Elastic shape analysis
    of three-dimensional objects,” *Synthesis Lectures on Computer Vision*, vol. 12,
    no. 1, pp. 1–185, 2017.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] I. H. Jermyn, S. Kurtek, H. Laga, 和 A. Srivastava，“三维物体的弹性形状分析，” *Synthesis
    Lectures on Computer Vision*，第 12 卷，第 1 期，第 1–185 页，2017 年。'
- en: '[89] G. Wang, H. Laga, N. Xie, J. Jia, and H. Tabia, “The shape space of 3d
    botanical tree models,” *ACM Transactions on Graphics (TOG)*, vol. 37, no. 1,
    p. 7, 2018.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] G. Wang, H. Laga, N. Xie, J. Jia, 和 H. Tabia，“3D 植物树模型的形状空间，” *ACM Transactions
    on Graphics (TOG)*，第 37 卷，第 1 期，第 7 页，2018 年。'
- en: '[90] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, “Sun database:
    Large-scale scene recognition from abbey to zoo,” in *IEEE CVPR*.   IEEE, 2010,
    pp. 3485–3492.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, 和 A. Torralba，“Sun 数据库：从修道院到动物园的大规模场景识别，”在
    *IEEE CVPR* 中。 IEEE，2010 年，第 3485–3492 页。'
- en: '[91] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su *et al.*, “Shapenet: An information-rich 3D model repository,”
    *arXiv preprint arXiv:1512.03012*, 2015.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S.
    Savarese, M. Savva, S. Song, H. Su *等*，“Shapenet：一个信息丰富的 3D 模型库，” *arXiv 预印本 arXiv:1512.03012*，2015
    年。'
- en: '[92] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3D
    shapenets: A deep representation for volumetric shapes,” in *IEEE CVPR*, 2015,
    pp. 1912–1920.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, 和 J. Xiao，“3D shapenets：一种用于体积形状的深度表示，”发表在*IEEE
    CVPR*，2015年，第1912–1920页。'
- en: '[93] J. J. Lim, H. Pirsiavash, and A. Torralba, “Parsing ikea objects: Fine
    pose estimation,” in *IEEE ICCV*, 2013, pp. 2992–2999.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. J. Lim, H. Pirsiavash, 和 A. Torralba，“解析IKEA物体：精细姿态估计，”发表在*IEEE ICCV*，2013年，第2992–2999页。'
- en: '[94] Y. Xiang, R. Mottaghi, and S. Savarese, “Beyond pascal: A benchmark for
    3d object detection in the wild,” in *IEEE WACV*.   IEEE, 2014, pp. 75–82.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. Xiang, R. Mottaghi, 和 S. Savarese，“超越Pascal：野外3D物体检测基准，”发表在*IEEE WACV*。IEEE，2014年，第75–82页。'
- en: '[95] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised learning
    of depth and ego-motion from video,” in *IEEE CVPR*, vol. 2, no. 6, 2017, p. 7.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] T. Zhou, M. Brown, N. Snavely, 和 D. G. Lowe，“从视频中无监督学习深度和自我运动，”发表在*IEEE
    CVPR*，第2卷，第6期，2017年，第7页。'
- en: '[96] A. Tonioni, M. Poggi, S. Mattoccia, and L. Di Stefano, “Unsupervised adaptation
    for deep stereo,” in *IEEE ICCV*.   IEEE, 2017, pp. 1614–1622.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] A. Tonioni, M. Poggi, S. Mattoccia, 和 L. Di Stefano，“用于深度立体的无监督适应，”发表在*IEEE
    ICCV*。IEEE，2017年，第1614–1622页。'
- en: '[97] J. T. Barron, “A more general robust loss function,” *arXiv preprint arXiv:1701.03077*,
    2017.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. T. Barron，“一种更通用的鲁棒损失函数，” *arXiv预印本 arXiv:1701.03077*，2017年。'
- en: '[98] M. Bai, W. Luo, K. Kundu, and R. Urtasun, “Exploiting semantic information
    and deep matching for optical flow,” in *ECCV*.   Springer, 2016, pp. 154–170.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Bai, W. Luo, K. Kundu, 和 R. Urtasun，“利用语义信息和深度匹配进行光流估计，”发表在*ECCV*。Springer，2016年，第154–170页。'
- en: '[99] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE transactions
    on image processing*, vol. 13, no. 4, pp. 600–612, 2004.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Z. Wang, A. C. Bovik, H. R. Sheikh, 和 E. P. Simoncelli，“图像质量评估：从误差可见性到结构相似性，”
    *IEEE图像处理交易*，第13卷，第4期，第600–612页，2004年。'
- en: '[100] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki,
    “Sfm-net: Learning of structure and motion from video,” *arXiv preprint arXiv:1704.07804*,
    2017.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, 和 K. Fragkiadaki，“Sfm-net：从视频中学习结构和运动，”*arXiv预印本
    arXiv:1704.07804*，2017年。'
- en: '[101] M. Perriollat, R. Hartley, and A. Bartoli, “Monocular template-based
    reconstruction of inextensible surfaces,” *IJCV*, vol. 95, no. 2, pp. 124–137,
    2011.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] M. Perriollat, R. Hartley, 和 A. Bartoli，“基于模板的单目不可延展表面重建，” *IJCV*，第95卷，第2期，第124–137页，2011年。'
- en: '[102] P. P. Srinivasan, R. Garg, N. Wadhwa, R. Ng, and J. T. Barron, “Aperture
    supervision for monocular depth estimation,” in *IEEE CVPR*, June 2018.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] P. P. Srinivasan, R. Garg, N. Wadhwa, R. Ng, 和 J. T. Barron，“用于单目深度估计的孔径监督，”发表在*IEEE
    CVPR*，2018年6月。'
- en: '[103] J. T. Todd and J. F. Norman, “The visual perception of 3-d shape from
    multiple cues: Are observers capable of perceiving metric structure?” *Perception
    & Psychophysics*, vol. 65, no. 1, pp. 31–47, 2003.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. T. Todd 和 J. F. Norman，“从多重线索中感知3D形状：观察者是否能够感知度量结构？” *Perception &
    Psychophysics*，第65卷，第1期，第31–47页，2003年。'
- en: '[104] K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, and Z. Luo, “Monocular
    relative depth perception with web stereo data supervision,” in *IEEE CVPR*, June
    2018.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, 和 Z. Luo，“使用网络立体数据监督的单目相对深度感知，”发表在*IEEE
    CVPR*，2018年6月。'
- en: '[105] Y. Kuznietsov, J. Stuckler, and B. Leibe, “Semi-supervised deep learning
    for monocular depth map prediction,” in *IEEE CVPR*, 2017, pp. 6647–6655.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Y. Kuznietsov, J. Stuckler, 和 B. Leibe，“用于单目深度图预测的半监督深度学习，”发表在*IEEE CVPR*，2017年，第6647–6655页。'
- en: '[106] A. Atapour-Abarghouei and T. P. Breckon, “Real-time monocular depth estimation
    using synthetic data with domain adaptation via image style transfer,” in *IEEE
    CVPR*, June 2018.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Atapour-Abarghouei 和 T. P. Breckon，“使用合成数据和通过图像风格迁移的领域适应进行实时单目深度估计，”发表在*IEEE
    CVPR*，2018年6月。'
- en: '[107] C. Zheng, T.-J. Cham, and J. Cai, “T2net: Synthetic-to-realistic translation
    for solving single-image depth estimation tasks,” in *ECCV*, September 2018.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] C. Zheng, T.-J. Cham, 和 J. Cai，“T2net：用于解决单图像深度估计任务的合成到真实翻译，”发表在*ECCV*，2018年9月。'
- en: '[108] X. Guo, H. Li, S. Yi, J. Ren, and X. Wang, “Learning monocular depth
    by distilling cross-domain stereo networks,” in *ECCV*, September 2018.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] X. Guo, H. Li, S. Yi, J. Ren, 和 X. Wang，“通过提取跨域立体网络学习单目深度，”发表在*ECCV*，2018年9月。'
- en: '[109] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv preprint arXiv:1503.02531*, 2015.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] G. Hinton, O. Vinyals, 和 J. Dean，“在神经网络中提取知识，” *arXiv预印本 arXiv:1503.02531*，2015年。'
- en: '[110] J. Nath Kundu, P. Krishna Uppala, A. Pahuja, and R. Venkatesh Babu, “Adadepth:
    Unsupervised content congruent adaptation for depth estimation,” in *IEEE CVPR*,
    June 2018.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. Nath Kundu、P. Krishna Uppala、A. Pahuja 和 R. Venkatesh Babu，“Adadepth:
    用于深度估计的无监督内容一致适应，”发表于 *IEEE CVPR*，2018年6月。'
- en: '[111] A. Kuzmin, D. Mikushin, and V. Lempitsky, “End-to-end learning of cost-volume
    aggregation for real-time dense stereo,” in *International Workshop on Machine
    Learning for Signal Processing (MLSP)*.   IEEE, 2017, pp. 1–6.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] A. Kuzmin、D. Mikushin 和 V. Lempitsky，“用于实时密集立体的端到端成本体积聚合学习，”发表于 *国际机器学习信号处理研讨会（MLSP）*。
    IEEE，2017年，第1–6页。'
- en: '[112] R. Zabih and J. Woodfill, “Non-parametric local transforms for computing
    visual correspondence,” in *ECCV*.   Springer, 1994, pp. 151–158.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] R. Zabih 和 J. Woodfill，“计算视觉对应的非参数局部变换，”发表于 *ECCV*。 Springer，1994年，第151–158页。'
- en: '[113] H. Zhan, R. Garg, C. Saroj Weerasekera, K. Li, H. Agarwal, and I. Reid,
    “Unsupervised learning of monocular depth estimation and visual odometry with
    deep feature reconstruction,” in *IEEE CVPR*, June 2018.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] H. Zhan、R. Garg、C. Saroj Weerasekera、K. Li、H. Agarwal 和 I. Reid，“通过深度特征重建的无监督单目深度估计和视觉里程计学习，”发表于
    *IEEE CVPR*，2018年6月。'
- en: '[114] D. Xu, E. Ricci, W. Ouyang, X. Wang, and N. Sebe, “Multi-scale continuous
    CRFs as sequential deep networks for monocular depth estimation,” in *IEEE CVPR*,
    2017, pp. 5354–5362.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] D. Xu、E. Ricci、W. Ouyang、X. Wang 和 N. Sebe，“多尺度连续CRFs作为单目深度估计的序列深度网络，”发表于
    *IEEE CVPR*，2017年，第5354–5362页。'
- en: '[115] J. Xie, R. Girshick, and A. Farhadi, “Deep3d: Fully automatic 2D-to-3D
    video conversion with deep convolutional neural networks,” in *ECCV*.   Springer,
    2016, pp. 842–857.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. Xie、R. Girshick 和 A. Farhadi，“Deep3d: 完全自动的2D到3D视频转换，使用深度卷积神经网络，”发表于
    *ECCV*。  Springer，2016年，第842–857页。'
- en: '[116] R. Wang, J.-M. Frahm, and S. M. Pizer, “Recurrent neural network for
    learning densedepth and ego-motion from video,” *arXiv preprint arXiv:1805.06558*,
    2018.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] R. Wang、J.-M. Frahm 和 S. M. Pizer，“用于从视频中学习深度和自我运动的递归神经网络，” *arXiv 预印本
    arXiv:1805.06558*，2018年。'
