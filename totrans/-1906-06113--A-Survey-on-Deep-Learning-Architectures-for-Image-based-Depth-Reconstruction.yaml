- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:06:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1906.06113] A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1906.06113](https://ar5iv.labs.arxiv.org/html/1906.06113)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Deep Learning Architectures for Image-based Depth Reconstruction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hamid Laga    Hamid Laga Hamid Laga is with Murdoch University (Australia),
    and with the Phenomics and Bioinformatics Research Centre, University of South
    Australia. E-mail: H.Laga@murdoch.edu.au Manuscript received April 19, 2005; revised
    December 27, 2012.(September 2018)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Estimating depth from RGB images is a long-standing ill-posed problem, which
    has been explored for decades by the computer vision, graphics, and machine learning
    communities. In this article, we provide a comprehension survey of the recent
    developments in this field. We will focus on the works which use deep learning
    techniques to estimate depth from one or multiple images. Deep learning, coupled
    with the availability of large training datasets, have revolutionized the way
    the depth reconstruction problem is being approached by the research community.
    In this article, we survey more than $100$ key contributions that appeared in
    the past five years, summarize the most commonly used pipelines, and discuss their
    benefits and limitations. In retrospect of what has been achieved so far, we also
    conjecture what the future may hold for learning-based depth reconstruction research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Stereo matching, Disparity, CNN, Convolutional Neural Networks, 3D Video, 3D
    Reconstruction
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of image-based 3D reconstruction is to infer the 3D geometry and structure
    of real objects and scenes from one or multiple RGB images. This long standing
    ill-posed problem is fundamental to many applications such as robot navigation,
    object recognition and scene understanding, 3D modeling and animation, industrial
    control, and medical diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: Recovering the lost dimension from 2D images has been the goal of multiview
    stereo and Structure-from-X methods, which have been extensively investigated
    for many decades. The first generation of methods has focused on understanding
    and formalizing the 3D to 2D projection process, with the aim to devise solutions
    to the ill-posed inverse problem. Effective solutions typically require multiple
    images, captured using accurately calibrated cameras. Although these techniques
    can achieve remarkable results, they are still limited in many aspects. For instance,
    they are not suitable when dealing with occlusions, featureless regions, or highly
    textured regions with repetitive features.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, we, as humans, are good at solving such ill-posed inverse problems
    by leveraging prior knowledge. For example, we can easily infer the approximate
    size and rough geometry of objects using only one eye. We can even guess what
    it would look like from another view. We can do this because all the previously
    seen objects and scenes have enabled us to build a prior knowledge and develop
    mental models of what objects, and the 3D world in general, look like. The second
    generation of depth reconstruction methods try to leverage this prior knowledge
    by formulating the problem as a recognition task. The avenue of deep learning
    techniques, and more importantly, the increasing availability of large training
    data sets, have lead to a new generation of methods that are able to recover the
    lost dimension even from a single image. Despite being recent, these methods have
    demonstrated exciting and promising results on various tasks related to computer
    vision and graphics.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we provide a comprehensive and structured review of the recent
    advances in image-based depth reconstruction using deep learning techniques. We
    have gathered more than $100$ papers, which appeared from $2014$ to December $2018$
    in leading computer vision, computer graphics, and machine learning conferences
    and journals dealing specifically with this problem¹¹1This number is continuously
    increasing even at the time we are writing this article and during the review
    process.. The goal is to help the reader navigate in this emerging field, which
    gained a significant momentum in the past few years. Compared to the existing
    literature, the main contributions of this article are as follows;
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, this is the first survey paper in the literature
    which focuses on image-based depth reconstruction using deep learning techniques.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We adequately cover the contemporary literature with respect to this area. We
    present a comprehensive review of more than $100$ articles, which appeared from
    $2014$ to December $2018$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This article also provides a comprehensive review and an insightful analysis
    on all aspects of depth reconstruction using deep learning, including the training
    data, the choice of network architectures and their effect on the reconstruction
    results, the training strategies, and the application scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a comparative summary of the properties and performances of the reviewed
    methods for different scenarios including depth reconstruction from stereo pairs,
    depth reconstruction from multiple images, and depth reconstruction from a single
    RGB image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The rest of this article is organized as follows; Section [2](#S2 "2 Scope and
    taxonomy ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    fomulates the problem and lays down the taxonomy. Section [3](#S3 "3 Depth by
    stereo matching ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction") focuses on the recent papers that use deep learning architectures
    for stereo matching. Section [4](#S4 "4 Depth estimation by regression ‣ A Survey
    on Deep Learning Architectures for Image-based Depth Reconstruction") reviews
    the methods that directly regress depth maps from one or multiple images without
    explicitly matching features across the input images. Section [5](#S5 "5 Training
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    focuses on the training procedures including the choice of training datasets and
    loss functions. Section [6](#S6 "6 Discussion and comparison ‣ A Survey on Deep
    Learning Architectures for Image-based Depth Reconstruction") discuss the performance
    of some key methods. Finally, Sections [7](#S7 "7 Future research directions ‣
    A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    and [8](#S8 "8 Conclusion ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction") discuss potential future research directions, and summarize
    the paper.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Scope and taxonomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let $\textbf{I}=\{I_{k},k=1,\dots,n\}$ be a set of $n\geq 1$ RGB images of the
    same 3D scene, captured using cameras whose intrinsic and extrinsic parameters
    can be *known* or *unknown*. The images can be captured by multiple cameras placed
    around the 3D scene, and thus they are spatially correlated, or with a single
    camera moving around the scene producing images that are temporally correlated.
    The goal is to estimate one or multiple depth maps, which can be from the same
    viewpoint as the input [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)],
    or from a new arbitrary viewpoint [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)]. In this article focuses on methods that estimate
    depth from one or multiple images with known or unknown camera parameters. Structure-from-Motion
    (SfM) and Simultaneous Localization and Mapping (SLAM) techniques, which estimate
    at the same time depth and (relative) camera pose from multiple images or a video
    stream, are beyond the scope of this article and require a separate survey.
  prefs: []
  type: TYPE_NORMAL
- en: Learning-based depth reconstruction can be summarized as the process of learning
    a predictor $f_{\theta}$ that can infer a depth map $\hat{D}$ that is as close
    as possible to the unknown depth map $D$. In other words, we seek to find a function
    $f_{\theta}$ such that $\mathcal{L}(\textbf{I})=d\left(f_{\theta}(\textbf{I}),D\right)$
    is minimized. Here, $\theta$ is a set of parameters, and $d(\cdot,\cdot)$ is a
    certain measure of distance between the real depth map $D$ and the reconstructed
    depth map $f_{\theta}(\textbf{I})$. The reconstruction objective $\mathcal{L}$
    is also known as the *loss function* in the deep learning jargon.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can distinguish two main categories of methods. Methods in the first class
    (Section [3](#S3 "3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")) mimic the traditional stereo-matching
    techniques by explicitly learning how to match, or put in correspondence, pixels
    across the input images. Such correspondences can then be converted into an optical
    flow or a disparity map, which in turn can be converted into depth at each pixel
    in the input image. The predictor $f$ is composed of three modules: a feature
    extraction module, a feature matching and cost aggregation module, and a disparity/depth
    estimation module.'
  prefs: []
  type: TYPE_NORMAL
- en: The second class of methods (Section [4](#S4 "4 Depth estimation by regression
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction"))
    do not explicitly learn the matching function. Instead, they learn a function
    that directly predicts depth (or disparity) at each pixel in the input image(s).
    These methods are very general and have been used to estimate depth from a single
    image as well as from multiple images taken from arbitrary view points. The predicted
    depth map $D$ can be from the same viewpoint as the input, or from a new arbitrary
    viewpoint $v$. We refer to these methods as *regression-based* depth estimation.
  prefs: []
  type: TYPE_NORMAL
- en: In all methods, the estimated depth maps can be further refined using refinement
    modules [[10](#bib.bib10), [1](#bib.bib1), [11](#bib.bib11), [2](#bib.bib2)] and/or
    progressive reconstruction strategies where the reconstruction is refined every
    time new images become available (Section [3.1.4](#S3.SS1.SSS4 "3.1.4 Refinement
    ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")).
  prefs: []
  type: TYPE_NORMAL
- en: The subsequent sections will review the state-of-the-art techniques. Within
    each class of methods, we will first review how the different modules within the
    common pipeline have been implemented using deep learning techniques. We will
    then discuss the different methods based on their input and output, the network
    architecture, the training procedures including the loss functions they use and
    the degree of supervision they require, and their performances on standard benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Depth by stereo matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stereo-based depth reconstruction methods take $n>1$ RGB images and produce
    a depth map, a disparity map, or an optical flow [[12](#bib.bib12), [13](#bib.bib13)],
    by matching features across the images. The input images may be captured with
    calibrated [[14](#bib.bib14)] or uncalibrated [[15](#bib.bib15)] cameras.
  prefs: []
  type: TYPE_NORMAL
- en: This section focuses on deep learning-based methods that mimic the traditional
    stereo-matching pipeline, *i.e.,* methods that learn how to explicitly match patches
    across stereo images for disparity/depth map estimation. We will first review
    how individual blocks of the stereo-matching pipeline have been implemented using
    deep learning (Section [3.1](#S3.SS1 "3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")),
    and then discuss how these blocks are put together and trained for depth reconstruction
    (Section [3.2](#S3.SS2 "3.2 Stereo matching networks ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The stereo-based depth reconstruction process can be formulated as the problem
    of estimating a map $D$ ($D$ can be a depth/disparity map, or an optical flow)
    which minimizes an energy function of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $E(D)=\sum_{x}C(x,0pt_{x})+\sum_{x}\sum_{y\in\mathcal{N}_{x}}E_{s}(0pt_{x},0pt_{y}).$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Here, $x$ and $y$ are image pixels, $0pt_{x}=D(x)$ is the depth / disparity
    at $x$, $C$ is a 3D cost volume where $C(x,0pt_{x})$ is the cost of pixel $x$
    having depth or disparity equal to $0pt_{x}$, $\mathcal{N}_{x}$ is the set of
    pixels that are within the neighborhood of $x$, and $E_{s}$ is a regularization
    term, which is used to impose various constraints, *e.g.,* smoothness and left-right
    depth/disparity consistency, to the final solution. The first term of Equation ([1](#S3.E1
    "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")) is the matching cost. In
    the case of rectified stereo pairs, it measures the cost of matching the pixel
    $x=(i,j)$ of the left image with the pixel $y=(i,j-0pt_{x})$ of the right image.
    In the more general multiview stereo case, it measures the inverse likelihood
    of $x$ on the reference image having depth $0pt_{x}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, this problem is solved with a pipeline of four building blocks [[16](#bib.bib16)]:
    (1) feature extraction, (2) matching cost calculation and aggregation, (3) disparity/depth
    calculation, and (4) disparity/depth refinement. The first two blocks construct
    the cost volume $C$. The third and fourth blocks define the regularization term
    and find the depth/disparity map $\tilde{D}$ that minimizes Equation ([1](#S3.E1
    "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")). In this section, we review
    the recent methods that implement these individual blocks using deep learning
    techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Taxonomy of deep learning-based stereo matching algorithms. ”Arch.”
    refers to architecture. ”CV” refers to cost volume. ”corr.” refers to correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   |'
  prefs: []
  type: TYPE_TB
- en: '| Method | (1) Feature extraction | (2) Matching cost computation | (3) Cost
    volume regularization | Depth estimation |'
  prefs: []
  type: TYPE_TB
- en: '| Scale | Arch. | Hand-crafted | Learned similarity | Input | Approach / Net
    arch. | Output |'
  prefs: []
  type: TYPE_TB
- en: '| Feature | Similarity learning |'
  prefs: []
  type: TYPE_TB
- en: '| aggregation | Network | Output |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fixed vs. | ConvNet vs. | $L_{2}$ | Pooling, | FC, CNN | Matching score,
    | Cost volume (CV) | Standard stereo | Regularized CV | argmin, argmax |'
  prefs: []
  type: TYPE_TB
- en: '|  | multiscale | ResNet | correlation | Concatenation |  | matching features
    | CV+ features (CVF) | encoder | Disparity/depth | soft argmin, soft argmax |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  | CVF + segmentation (CVFS) | encoder + decoder |  | subpixel
    MAP |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  | CVF + edge map (CVFE) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|   |'
  prefs: []
  type: TYPE_TB
- en: '| MC-CNN Accr [[17](#bib.bib17), [18](#bib.bib18)] | fixed | CNN | $-$ | concatenation
    | 4 FC layers | matching score | cost volume | Standard stereo | Regularized CV
    | argmin |'
  prefs: []
  type: TYPE_TB
- en: '| Luo *et al.* [[19](#bib.bib19)] | fixed | ConvNet | correlation | $-$ | $-$
    | matching score | cost volume | Standard stereo | Regularized CV | argmin |'
  prefs: []
  type: TYPE_TB
- en: '| Chen *et al.* [[20](#bib.bib20)] | multiscale | ConvNet | corr. + voting
    | $-$ | $-$ | matching score | cost volume | Standard stereo | Regularized CV
    | argmin |'
  prefs: []
  type: TYPE_TB
- en: '| L-ResMatch [[21](#bib.bib21)] | fixed | ResNet | $-$ | concatenation | 4
    (FC+ReLu) + FC | matching score | cost volume | standard stereo + | Regularized
    CV | argmin |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  | 4 Conv + 5 FC |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Han *et al.*[[22](#bib.bib22)] | fixed | ConvNet | $-$ | concatenation |
    2 (FC + Relu), FC | matching score | $-$ | $-$ | $-$ | softmax |'
  prefs: []
  type: TYPE_TB
- en: '| DispNetCorr [[13](#bib.bib13)] | fixed | ConvNet | 1D correlation | $-$ |
    $-$ | matching score | CV + features | encoder + decoder | disparity | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pang *et al.*[[23](#bib.bib23)] | fixed | ConvNet | 1D correlation | $-$
    | $-$ | matching score | CV + features | encoder + decoder | disparity |  |'
  prefs: []
  type: TYPE_TB
- en: '| Yu *et al.*[[24](#bib.bib24)] | fixed | ResNet | $-$ | concatenation | encoder+decoder
    | matching scores | cost volume | 3D Conv | Regularized CV | softargmin |'
  prefs: []
  type: TYPE_TB
- en: '| Yang *et al.* [[25](#bib.bib25)] (un)sup. | fixed | ResNet | correlation
    | $-$ | $-$ | matching scores | CVF + segmentation | encoder-decoder | depth |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liang *et al.* [[26](#bib.bib26)] | multiscale | ConvNet | correlation |
    $-$ | $-$ | matching score | CV + features | encder-decoder | depth | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Khamis *et al.* [[27](#bib.bib27)] | fixed | ResNet | $L_{2}$ | $-$ | $-$
    | $-$ | cost volume | encoder | regularized volume | soft argmin/max |'
  prefs: []
  type: TYPE_TB
- en: '| Chang & Chen [[28](#bib.bib28)] | multiscale | ResNet | $-$ | concatenate
    | $-$ | $-$ | cost volume | 12 conv layers, residual | regularized volume | regression
    |'
  prefs: []
  type: TYPE_TB
- en: '| (basic) |  |  |  |  |  |  |  | blocks, upsampling |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Chang & Chen [[28](#bib.bib28)] | multiscale | ResNet | $-$ | concatenation
    | $-$ | $-$ | cost volume | stacked encoder-decorder blocks, | regularized volume
    | regression |'
  prefs: []
  type: TYPE_TB
- en: '| (stacked) |  |  |  |  |  |  |  | residual connections, upsampling |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zhong *et al.*[[29](#bib.bib29)] | fixed | ResNet | $-$ | concatenation |
    encoder-decoder | matching scores | cost volume | $-$ | regularized volume | soft
    argmin |'
  prefs: []
  type: TYPE_TB
- en: '| SGM-Net [[30](#bib.bib30)] | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | cost volume
    | MRF + SGM-Net | regularized volume | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| EdgeStereo [[31](#bib.bib31)] | fixed | VGG-16 | correlation | $-$ |  | matching
    scores | CVF + edge map | encoder-decoder (res. pyramid) | depth | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Tulyakov *et al.*[[32](#bib.bib32)] | fixed | ConvNet | $-$ | concatenation
    | encoder-decoder | matching signatures | matching signatures | encoder + decoder
    | regularized volume | subpixel MAP |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  | at each disparity |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Jie *et al.*[[33](#bib.bib33)] | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | Left
    and right CVs | Recurrent ConvLSTM | disparity | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  | with left-right consistency |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zagoruyko *et al.*[[34](#bib.bib34)] | multiscale | ConvNet | $-$ | concatenation
    | FC | matching scores | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Hartmann *et al.* [[35](#bib.bib35)] | fixed | ConvNet |  | Avg pooling |
    CNN | matching score | $-$ | $-$ | $-$ | softmax |'
  prefs: []
  type: TYPE_TB
- en: '| Huang *et al.* [[36](#bib.bib36)] | fixed | ConvNet | $-$ | Max pooling |
    CNN | matching features | cost volume | encoder | regularized volume | argmin
    |'
  prefs: []
  type: TYPE_TB
- en: '| Yao *et al.* [[37](#bib.bib37)] | fixed | ConvNet | $-$ | Var. pooling |
    $-$ | matching features | cost volume | encoder-decoder | regularized volume |
    softmax |'
  prefs: []
  type: TYPE_TB
- en: '| Flynn *et al.* [[14](#bib.bib14)] | fixed | 2D Conv | $-$ | Conv across |
    CNN | matching score | cost volume | encoder | regularized volume | soft argmin/max
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  | depth layers |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Kar *et al.* [[38](#bib.bib38)] | fixed | ConvNet | $-$ | feature unprojection
    + | CNN | matching score | cost volume | encoder-decoder | 3D occupancy | projection
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  | recurrent fusion |  |  |  |  | grid |  |'
  prefs: []
  type: TYPE_TB
- en: '| Kendall *et al.* [[39](#bib.bib39)] | fixed | ConvNet | $-$ | concatenation
    | CNN | matching features | cost volume | encoder-decoder | regularized volume
    | soft argmin/max |'
  prefs: []
  type: TYPE_TB
- en: '|   |'
  prefs: []
  type: TYPE_TB
- en: 3.1.1 Feature extraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad1cd76c83adebd58a8f8f0008bfd7eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Feature extraction networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f3c5c3a13365d9a866b917f9cf004d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Multiscale feature extraction networks.'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to compute a good set of features to match across images.
    This has been modelled using CNN architectures where the encoder takes either
    patches around pixels of interests or entire images, and produces dense feature
    maps in the 2D image space. These features can be of fixed scale (Section [3.1.1.1](#S3.SS1.SSS1.P1
    "3.1.1.1 Fixed-scale features ‣ 3.1.1 Feature extraction ‣ 3.1 The pipeline ‣
    3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction")) or multiscale (Section [3.1.1.2](#S3.SS1.SSS1.P2 "3.1.1.2
    Multiscale features ‣ 3.1.1 Feature extraction ‣ 3.1 The pipeline ‣ 3 Depth by
    stereo matching ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction")).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1.1 Fixed-scale features
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The main type of network architectures that have been used in the literature
    is the multi-branch network with shared weights [[34](#bib.bib34), [17](#bib.bib17),
    [12](#bib.bib12), [18](#bib.bib18), [19](#bib.bib19), [13](#bib.bib13), [38](#bib.bib38),
    [39](#bib.bib39), [23](#bib.bib23), [26](#bib.bib26)], see also Figure [1](#S3.F1
    "Figure 1 ‣ 3.1.1 Feature extraction ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction").
    It is composed of $n\geq 2$ encoding branches, one for each input image, which
    act as descriptor computation modules. Each branch is a Convolutional Neural Networks
    (CNN), which takes a patch around a pixel $i$ and outputs a feature vector that
    characterizes that patch [[34](#bib.bib34), [17](#bib.bib17), [12](#bib.bib12),
    [40](#bib.bib40), [18](#bib.bib18), [19](#bib.bib19), [13](#bib.bib13), [14](#bib.bib14),
    [38](#bib.bib38), [39](#bib.bib39), [23](#bib.bib23), [36](#bib.bib36), [37](#bib.bib37),
    [26](#bib.bib26)]. It is generally composed of convolutional layers, spatial normalizations,
    pooling layers, and rectified linear units (ReLU). The scale of the features that
    are extracted is controlled by the size of the convolutional filters used in each
    layer as well as by the number of convolutional and pooling layers. Increasing
    the size of the filters and/or the number of layers increases the scale of the
    features that will be extracted. It has also the advantage of capturing more interactions
    between the image pixels. However, this comes with a high computation cost. To
    reduce the computational cost while increasing the field of view of the network,
    some techniques, *e.g.,*  [[41](#bib.bib41)], use dilated convolutions, *i.e.,*
    large convolutional filters but with holes and thus they are computationally efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using fully convolutional networks, some techniques [[25](#bib.bib25)]
    use residual networks, *e.g.,* ResNet [[42](#bib.bib42)], *i.e.,* CNNs with residual
    blocks. A residual block takes an input and estimates the residual that needs
    to be added to that input. They are used to ease the training of substantially
    deep networks since learning the residual of a signal is much easier than learning
    to predict the signal itself. Various types of residual blocks have been used
    in the literature. For example, Shaked and Wolf [[21](#bib.bib21)] proposed appending
    residual blocks with multilevel connections. Its particularity is that the network
    learns by itself how to adjust the contribution of the added skip connections.
  prefs: []
  type: TYPE_NORMAL
- en: Table [II](#S3.T2 "TABLE II ‣ 3.1.1.1 Fixed-scale features ‣ 3.1.1 Feature extraction
    ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") summarises the detailed architecture (number
    of layers, filter sizes, and stride at each layer) of various methods and the
    size of the features they produce. Note that, one advantage of convolutional networks
    is that the convolutional operations within one level are independent from each
    other, and thus they are parallelizable. As such, all the features of an entire
    image can be computed with a single forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Network architectures for feature extraction. Each layer of the network
    is described in the following format: (filter size, type, stride, output feature
    size, scaling). Scaling refers to upscaling or downscaling of the resolution of
    the output with respect to the input. SPP refers to Spatial Pyramid Pooling. The
    last column refers to the feature size as produced by the last layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Input | Type | Architecture | Feature size |'
  prefs: []
  type: TYPE_TB
- en: '| Dosovitskiy *et al.* [[12](#bib.bib12)] | $512\times 384$ | CNN | $(7\times
    7,conv,2,64,-),(5\times 5,conv,2,128,-),(5\times 5,conv,2,256,-)$ | $64\times
    48\times 256$ |'
  prefs: []
  type: TYPE_TB
- en: '| Chen *et al.* [[20](#bib.bib20)] | $13\times 13$ | CNN | $(3\times 3,conv,-,32,-)_{1,2}$,
    $(5\times 5,conv,-,200,-)_{3,4}$ | $1\times 200$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zagoruyko [[34](#bib.bib34)] | patches of varying sizes | CNN + SPP | $(-,conv+ReLu,-,-,-)_{1,2},(-,conv+SPP,-,-,-)$
    | $1\times c$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zbontar & LeCun [[18](#bib.bib18)] (fast) | patches $9\times 9$ | CNN | $(3\times
    3,conv+ReLu,-,64,-)_{1,2,3},(3\times 3,conv,-,64,-)$ | $1\times 64$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zbontar & LeCun [[18](#bib.bib18)] (accr) | patches $9\times 9$ | CNN | $(3\times
    3,conv+ReLu,-,112,-)_{1,2,3},(3\times 3,conv,-,112,-)$ | $1\times 112$ |'
  prefs: []
  type: TYPE_TB
- en: '| Luo *et al.* [[19](#bib.bib19)] | Small patch | CNN | $(3\times 3\text{ or
    }5\times 5,conv+ReLu,-,32\text{ or }64,-)_{1,2,3},\text{or}$ | $1\times 32$ or
    $1\times 64$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | $(5\times 5,conv,-,32\text{ or }64,-)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| DispNetC [[13](#bib.bib13)], Pang *et al.* [[23](#bib.bib23)] | $768\times
    364$ | CNN | $(7\times 7,conv,2,64,-),(5\times 5,conv,2,128,-)$ | $192\times 96\times
    128$ |'
  prefs: []
  type: TYPE_TB
- en: '| Kendall *et al.* [[39](#bib.bib39)] | $0pt\times 0pt$ | CNN (2D conv) + |
    $(5\times 5,conv,2,32,/2),[(3\times 3,conv,-,32,-),(3\times 3,res,-,32,-)]_{1,\dots,7},$
    | $\frac{1}{2}0pt\times\frac{1}{2}0pt\times 32$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Residual connections | $(3\times 3,conv,-,32,-)$, No RLu or BN on the
    last layer |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liang *et al.* [[26](#bib.bib26)] | $0pt\times 0pt$ | CNN | $(7\times 7,conv,2,64,/2),(5\times
    5,conv,2,128,/2)$. | $\frac{1}{4}0pt\times\frac{1}{4}0pt\times 128$ |'
  prefs: []
  type: TYPE_TB
- en: '| Kar *et al.* [[38](#bib.bib38)] | $224\times 224$ | CNN | $(3\times 3,conv,-,64,-),(3\times
    3,conv,-,64,-),(2\times 2,\text{maxpool},-,64,-)$ | $32\times 32\times 1024$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | $(3\times 3,conv,-,128,-),(3\times 3,conv,-,128,-),(2\times 2,\text{maxpool},-,128,-)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | $(3\times 3,conv,-,512,-),(3\times 3,conv,-,512,-),(2\times 2,\text{maxpool},-,512,-)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | $(3\times 3,conv,-,1024,-),(3\times 3,conv,-,1024,-)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Yang *et al.* [[25](#bib.bib25)] | $0pt\times 0pt$ | CNN + Residual blocks
    | $(3\times 3,conv,2,64,/2),(3\times 3,conv,1,64,/1),(3\times 3,conv,1,128,/1)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | $(3\times 3,maxpool,2,128,/2),(3\times 3,res\_block,1,256,/2)$,
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | $(3\times 3,res\_block,1,256,/1)_{1,2}$, $(3\times 3,res\_blcok,1,512,/2)$
    | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Shaked and Wolf [[21](#bib.bib21)] | $11\times 11$ | CNN + | $conv_{1},ReLU,$
    Outer $\lambda-$ residual block, $ReLU,conv_{2\cdots 5},ReLU,$ | $1\times 1\times
    112$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Outer $\lambda-$residual blocks | Outer $\lambda-$ residual block |  |'
  prefs: []
  type: TYPE_TB
- en: 3.1.1.2 Multiscale features
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The methods described in Section [3.1.1.1](#S3.SS1.SSS1.P1 "3.1.1.1 Fixed-scale
    features ‣ 3.1.1 Feature extraction ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    can be extended to extract features at multiple scales, see Figure [2](#S3.F2
    "Figure 2 ‣ 3.1.1 Feature extraction ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction").
    This is done either by feeding the network with patches of different sizes centered
    at the same pixel [[34](#bib.bib34), [20](#bib.bib20), [28](#bib.bib28)], or by
    using the features computed by the intermediate layers [[26](#bib.bib26)]. Note
    that the deeper is a layer in the network, the larger is the scale of the features
    it computes.
  prefs: []
  type: TYPE_NORMAL
- en: Liang *et al.* [[26](#bib.bib26)] compute multiscale features using a two-layer
    convolutional network. The output of the two layers are then concatenated and
    fused, using a convolutional layer, which results in *multi-scale fusion features*.
    Zagoruyko and Komodakis [[34](#bib.bib34)] proposed a central-surround two-stream
    network which is essentially a network composed of two siamese networks combined
    at the output by a top network. The first siamese network, called central high-resolution
    stream, receives as input two $32\times 32$ patches that are generated by cropping
    (at the original resolution) the central $32\times 32$ part of each $64\times
    64$-input patch. The second network, called surround low-resolution stream, receives
    as input two $32\times 32$ patches generated by downsampling at half the original
    input. Chen *et al.* [[20](#bib.bib20)] also used a similar approach but each
    network processes patches of size $13\times 13$. The main advantage of this architecture
    is that it can compute the features at two different resolutions in a single forward
    pass. It, however, requires one stream by scale, which is not practical if more
    than two scales are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Chang and Chen [[28](#bib.bib28)] used Spatial Pyramid Pooling (SPP) module
    to aggregate context in different scales and location. More precisely, the feature
    extraction module is composed of a CNN of seven layers, and an SPP module followed
    by convolutional layers. The CNN produces a feature map of size $\frac{1}{4}0pt\times\frac{1}{4}0pt\times
    128$. The SPP module then takes a patch around each pixel but at four different
    sizes ($8\times 8\times 128$, $16\times 16\times 128$, $32\times 32\times 128$,
    and $64\times 64\times 128$), and converts them into one-channel by mean pooling
    followed by a $1\times 1$ convolution. These are then upsampled to the desired
    size and concatenated with features from different layers of the CNN, and further
    processed with additional convolutional layers to produce the features that will
    be fed to the subsequent modules for matching and disparity computation. Chang
    and Chen [[28](#bib.bib28)] showed that the SPP module enables estimating disparity
    for inherently ill-posed regions.
  prefs: []
  type: TYPE_NORMAL
- en: In general, Spatial Pyramid Pooling (SPP) are convenient for processing patches
    of arbitrary sizes. For instance, Zaogoruyko and Komodakis [[34](#bib.bib34)]
    append an SPP layer at the end of the feature computation network. Such a layer
    aggregates the features of the last convolutional layer through spatial pooling,
    where the size of the pooling regions dependents on the size of the input. By
    doing so, one will be able to feed the network with patches of arbitrary sizes
    and compute feature vectors of the same dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Matching cost computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc671ee69ea9824e8e15b2265dec606e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Taxonomy of similarity computation networks.'
  prefs: []
  type: TYPE_NORMAL
- en: This module takes the features computed on each of the input images, and computes
    the matching scores of Equation ([1](#S3.E1 "In 3.1 The pipeline ‣ 3 Depth by
    stereo matching ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction")). The matching scores form a 3D volume, called *Disparity Space
    Image (DSI)* [[16](#bib.bib16)], of the form $C(x,d_{x})$ where $x=(i,j)$ is the
    image coordinates of pixel $x$ and $d_{x}\in[0,n_{d}]$ is the candidate disparity/depth
    value. It is of size $\tilde{0pt}\times\tilde{0pt}\times(n_{d}+1)$, where $\tilde{0pt}\times\tilde{0pt}$
    is the resolution at which we want to compute the depth map and $n_{d}$ is the
    number of depth/disparity values. In stereo matching, if the left and right images
    have been rectified so that the epipolar lines are horizontal then $C(x,d_{x})$
    is the similarity between the pixel $x=(i,j)$ on the rectified left image and
    the pixel $y=(i,j-d_{x})$ on the rectified right image. Otherwise, $C(x,d_{x})$
    indicates the likelihood, or probability, of the pixel $x$ having depth $d_{x}$.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to traditional stereo-matching methods [[16](#bib.bib16)], the cost
    volume is computing by comparing the deep features of the input images using standard
    metrics such as the $L_{2}$ distance, the cosine distance, and the (normalized)
    correlation distance (Section [3.1.2.1](#S3.SS1.SSS2.P1 "3.1.2.1 Using distance
    measures ‣ 3.1.2 Matching cost computation ‣ 3.1 The pipeline ‣ 3 Depth by stereo
    matching ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")).
    With the avenue of deep neural networks, several new mechanisms have been proposed
    (Section [3.1.2.2](#S3.SS1.SSS2.P2 "3.1.2.2 Using similarity-learning networks
    ‣ 3.1.2 Matching cost computation ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")).
    Figure [3](#S3.F3 "Figure 3 ‣ 3.1.2 Matching cost computation ‣ 3.1 The pipeline
    ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction") shows the main similarity computation architectures. Below,
    we discuss them in details.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2.1 Using distance measures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The simplest way to form a cost volume is by taking the distance between the
    feature vector of a pixel and the feature vectors of the matching candidates,
    *i.e.,* the pixels on the other image that are within a pre-defined disparity
    range. There are several distance measures that can be used. Khamis *et al.* [[27](#bib.bib27)],
    for example, used the $L_{2}$ distance. Other techniques, *e.g.,* [[12](#bib.bib12),
    [20](#bib.bib20), [18](#bib.bib18), [19](#bib.bib19), [13](#bib.bib13), [26](#bib.bib26)],
    used correlation, *i.e.,* the inner product between feature vectors. The main
    advantage of correlation over the $L_{2}$ distance is that it can be implemented
    using a layer of 2D [[12](#bib.bib12)] or 1D [[13](#bib.bib13)] convolutional
    operations, called correlation layer. 1D correlations are computationally more
    efficient than their 2D counterpart. They, however, require rectified images so
    that the search for correspondences is restricted to pixels within the same raw.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the two other methods that will be described below, the main advantage
    of the correlation layer is that it does not require training since the filters
    are in fact the features computed by the second branch of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2.2 Using similarity-learning networks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These methods aggregate the features produced by the different branches, and
    process them with a top network, which produces a matching score. The rational
    is to let the network learn from data the appropriate similarity measure.
  prefs: []
  type: TYPE_NORMAL
- en: '(1) Feature aggregation. Some stereo reconstruction methods first aggregate
    the features computed by the different branches of the network before passing
    them through further processing layers. The aggregation can be done in two different
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation by concatenation. The simplest way is to just concatenate the learned
    features computed by the different branches of the network and feed them to the
    similarity computation network [[34](#bib.bib34), [17](#bib.bib17), [18](#bib.bib18),
    [39](#bib.bib39)]. Kendall *et al.* [[39](#bib.bib39)] concatenate each feature
    with their corresponding feature from the opposite stereo image across each disparity
    level, and pack these into a 4D volume of dimensionality $0pt\times 0pt\times(n_{d}+1)\times
    c$ ($c$ here is the dimension of the features). Huang *et al.* [[36](#bib.bib36)],
    on the other hand, concatenate the $64^{3}$ feature volume, computed for the $64\times
    64\times 3$ reference image, and another volume of the same size from the plane-sweep
    volume plane that corresponds to the $n-$th input image at the $d-$th disparity
    level, to form a $64\times 64\times 128$ volume. Zhong *et al.* [[29](#bib.bib29)]
    followed the same approach but concatenate the features in an interleaved manner.
    That is, if $\textbf{f}_{L}$ is the feature map of the left image and $\textbf{f}_{R}$
    the feature map of the right image then the final feature volume is assembled
    in such a way that its $2i-$th slice holds the left feature map while the $(2i+1)-$th
    slice holds the right feature map but at disparity $d=i$. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textbf{f}_{LR}(u,v,d)=\textbf{f}_{L}(u,v)\&#124;\textbf{f}_{R}(u-d,v),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\|$ denotes the vector concatenation.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation by pooling. Another approach is to use pooling layers to aggregate
    the feature maps. For instance, Hartmann *et al.* [[35](#bib.bib35)] used average
    pooling. Huang *et al.* [[36](#bib.bib36)] used max-pooling, while Yao *et al.* [[37](#bib.bib37)]
    take their variance, which is equivalent to first computing the average feature
    vector and then taking the average distance of the other features to the mean.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of pooling over concatenation is three-fold; First, it does
    not increase the dimensionality of the data that is fed to the top similarity
    computation network, which facilitates the training. Second, it makes it possible
    to input a varying number of views without retraining the network. This is particularly
    suitable for multiview stereo (MVS) approaches, especially when dealing with an
    arbitrary number of input images and when the number of images at runtime may
    be different from the number of images at training. Finally, pooling ensures that
    the results are invariant with respect to the order in which the images are fed
    to the network.
  prefs: []
  type: TYPE_NORMAL
- en: '(2) Similarity computation. There are two types of networks that have been
    used in the literature: fully connected networks and convolutional networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Using fully-connected networks. In these methods, the similarity computation
    network is composed of fully connected layers [[34](#bib.bib34), [17](#bib.bib17),
    [18](#bib.bib18)]. The last layer produces the probability of the input feature
    vectors being a good or a bad match. Zagoruyko and Komodakis [[34](#bib.bib34)],
    for example, used a network composed of two fully connected layers (each with
    $512$ hidden units) that are separated by a ReLU activation layer. Zbontar and
    LeCun [[17](#bib.bib17), [18](#bib.bib18)] used five fully connected layers with
    $300$ neurones each except for the last layer, which projects the output to two
    real numbers that are fed through a softmax function, which in turn produces the
    probability of the two input feature vectors being a good match.
  prefs: []
  type: TYPE_NORMAL
- en: Using convolutional networks. Another approach is to aggregate the features
    and further post-process them using convolutional networks, which output either
    matching scores [[14](#bib.bib14), [38](#bib.bib38), [35](#bib.bib35)] (similar
    to correlation layers), or matching features [[39](#bib.bib39), [36](#bib.bib36)].
    The most commonly used CNNs include max-pooling layers, which provide invariance
    in spatial transformation. Pooling layers also widen the receptive field area
    of a CNN without increasing the number of parameters. The drawback is that the
    network loses fine details. To overcome this limitation, Park and Lee [[43](#bib.bib43)]
    introduced a pixel-wise pyramid pooling layer to enlarge the receptive field during
    the comparison of two input patches. This method produced more accurate matching
    cost than [[17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: One limitation of correlation layers and convolutional networks that produce
    a single cost value is that they decimate the feature dimension. Thus, they restrict
    the network to only learning relative representations between features, and cannot
    carry absolute feature representations. Instead, a matching feature can be seen
    as a descriptor, or a feature vector, that characterizes the similarity between
    two given patches. The simplest way of computing matching features is by aggregating
    the feature maps produced by the descriptor computation branches of the network [[44](#bib.bib44),
    [39](#bib.bib39)], or by using an encoder that takes the concatenated features
    and produces another volume of matching features [[36](#bib.bib36)]. For instance,
    Huang *et al.* [[36](#bib.bib36)] take the $64\times 64\times 128$ volume, formed
    by features, and process it using three convolutional layers to produce a $64\times
    64\times 4$ volume of matching features. Since the approach computes $n_{d}+1$
    matching features, one for each disparity level ($n_{d}=100$ in [[36](#bib.bib36)]),
    these need to be aggregated into a single matching feature. This is done using
    another encoder-decoder network with skip connections [[36](#bib.bib36)]. Each
    level of the encoder is formed by a stride-2 convolution layer followed by an
    ordinary convolution layer. Each level of the decoder is formed by two convolution
    layers followed by a bilinear upsampling layer. It produces a volume of matching
    features of size $64\times 64\times 800$.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Cost volume aggregation. In general, multiview stereo methods, which take
    $n$ input images, compute $n-1$ cost or feature matching volumes, one for each
    pair $(I_{0},I_{i})$, where $I_{0}$ is the reference image. These need to be aggregated
    into a single cost/feature matching volume before feeding it into the disparity/depth
    calculation module. This has been done either by using (max, average) pooling
    or pooling followed by an encoder [[36](#bib.bib36), [37](#bib.bib37)], which
    produces the final cost/feature matching volume $C$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Disparity and depth computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have seen so far the various deep learning techniques that have been used
    to estimate the cost volume $C$, *i.e.,* the first term of Equation ([1](#S3.E1
    "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")). The goal now is to estimate
    the depth/disparity map $\tilde{D}$ that minimizes the energy function $E(D)$
    of Equation ([1](#S3.E1 "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A
    Survey on Deep Learning Architectures for Image-based Depth Reconstruction")).
    This is done in two steps; (1) cost volume regularization, and (2) disparity/depth
    estimation from the regularized cost volume.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e468721ddd00b86c5c949931f3ed0a8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Cost volume regularization and disparity/depth map estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3.1 Cost volume regularization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once ta raw cost volume is estimated, one can estimate disparity/depth by dropping
    the smoothness term of Equation ([1](#S3.E1 "In 3.1 The pipeline ‣ 3 Depth by
    stereo matching ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction")) and taking the argmin, the softargmin, or the subpixel MAP approximation
    (see Section [3.1.3.2](#S3.SS1.SSS3.P2 "3.1.3.2 Disparity/depth estimation ‣ 3.1.3
    Disparity and depth computation ‣ 3.1 The pipeline ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")).
    In general, however, the raw cost volume computed from image features could be
    noise-contaminated (*e.g.,* due to the existence of non-Lambertian surfaces, object
    occlusions, and repetitive patterns). Thus, the estimated depth maps can be noisy.
  prefs: []
  type: TYPE_NORMAL
- en: Several deep learning-based regularization techniques have been proposed to
    estimate accurate depth maps from the cost volume, see Figure [4](#S3.F4 "Figure
    4 ‣ 3.1.3 Disparity and depth computation ‣ 3.1 The pipeline ‣ 3 Depth by stereo
    matching ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    for an illustration of the taxonomy. Their input can be the cost volume [[14](#bib.bib14),
    [27](#bib.bib27), [39](#bib.bib39), [29](#bib.bib29), [37](#bib.bib37), [38](#bib.bib38),
    [28](#bib.bib28), [36](#bib.bib36)], the cost volume concatenated with the features
    of the reference image [[12](#bib.bib12), [26](#bib.bib26)] and/or with semantic
    features such as the segmentation mask [[25](#bib.bib25)] or the edge map [[31](#bib.bib31)].
    The produced volume is then processed with either an encoder-decoder network with
    skip connections [[39](#bib.bib39), [29](#bib.bib29), [37](#bib.bib37), [38](#bib.bib38),
    [28](#bib.bib28), [12](#bib.bib12), [26](#bib.bib26), [25](#bib.bib25), [31](#bib.bib31)],
    or just an encoder [[14](#bib.bib14), [27](#bib.bib27), [36](#bib.bib36)], to
    produce either a regularized cost volume [[39](#bib.bib39), [29](#bib.bib29),
    [37](#bib.bib37), [38](#bib.bib38), [28](#bib.bib28), [14](#bib.bib14), [27](#bib.bib27),
    [36](#bib.bib36)], or directly the disparity/depth map [[12](#bib.bib12), [25](#bib.bib25),
    [31](#bib.bib31), [26](#bib.bib26)]. In the former case, the regularized volume
    is processed using argmin, softargmin, or subpixel MAP approximation (Section [3.1.3.2](#S3.SS1.SSS3.P2
    "3.1.3.2 Disparity/depth estimation ‣ 3.1.3 Disparity and depth computation ‣
    3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")) to produce the final disparity/depth map.
  prefs: []
  type: TYPE_NORMAL
- en: Note that some methods adopt an MRF-based stereo framework for cost volume regularization [[20](#bib.bib20),
    [17](#bib.bib17), [19](#bib.bib19)]. In these methods, the initial cost volume
    $C$ is fed to a global [[16](#bib.bib16)] or a semi-global [[45](#bib.bib45)]
    matcher to compute the disparity map. Semi-global methods define the smoothness
    term as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{E_{s}(0pt_{x},0pt_{y})=\alpha_{1}\delta(&#124;0pt_{x}-0pt_{y}&#124;=1)+\alpha_{2}\delta(&#124;0pt_{x}-0pt_{y}&#124;>1),}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha_{1}$ and $\alpha_{2}$ are positive weights chosen such that $\alpha_{2}>\alpha_{1}$.
    Instead of manually setting these two parameters, Seki *et al.* [[30](#bib.bib30)]
    proposed SGM-Net, a neural network trained to provide these parameters at each
    image pixel. They obtained better penalties than hand-tuned methods as in [[17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3.2 Disparity/depth estimation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The simplest way to estimate disparity/depth from the (regularized) cost volume
    $C$ is by using the pixel-wise argmin, *i.e.,* $0pt_{x}=\arg\min_{0pt}C(x,0pt)$
    (or equivalently $\arg\max$ if the volume $C$ encodes likelihood) [[36](#bib.bib36)].
    However, the agrmin/argmax operatir is unable to produce sub-pixel accuracy and
    cannot be trained with back-propagation due to its non-differentiability. Another
    approach is to process the cost volume using a layer of per-pixel softmin, also
    called soft argmin (or equivalently softmax), over disparity/depth [[14](#bib.bib14),
    [39](#bib.bib39), [27](#bib.bib27)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d^{*}=\frac{1}{\sum_{j=0}^{n_{d}}e^{-C(x,j)}}\sum_{d=0}^{n_{d}}d\times
    e^{-C(x,d)}.$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'It approximates sub-pixel MAP solution when the distribution is unimodal and
    symmetric [[32](#bib.bib32)]. When this assumption is not fulfilled, the softargmin
    blends the modes and may produce a solution that is far from all the modes. Also,
    the network only learns for the disparity range used during training. If the disparity
    range changes at runtime, then the network needs to be re-trained. To address
    these issues, Tulyakov *et al.* [[32](#bib.bib32)] introduced the sub-pixel MAP
    approximation that computes a weighted mean around the disparity with maximum
    posterior probability as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d^{*}=\sum_{d:&#124;\hat{d}-d&#124;\leq\delta}d\cdot\sigma(C(x,d)),$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\delta$ is a meta parameter set to $4$ in [[32](#bib.bib32)], and $\displaystyle\hat{d}=\arg\max_{d}C(x,d)$.
    Note that, in Tulyakov *et al.* [[32](#bib.bib32)], the sub-pixel MAP is only
    used for inference.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Refinement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'TABLE III: Taxonomy of disparity/depth refinement techniques. ”reco error”:
    reconstruction error. ”CSPN”: Convolutional Spatial Propagation Networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional methods | Deep learning-based methods |'
  prefs: []
  type: TYPE_TB
- en: '|  | Input | Approach | Other cues |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Bottom-up | Top-down | Guided |  |'
  prefs: []
  type: TYPE_TB
- en: '|   |'
  prefs: []
  type: TYPE_TB
- en: '| Variational [[12](#bib.bib12)] | Raw depth | Split and merge [[46](#bib.bib46)]
    | Decoder [[10](#bib.bib10)] | Detect - Replace - Refine [[47](#bib.bib47)] |
    Joint depth and normal [[48](#bib.bib48)] |'
  prefs: []
  type: TYPE_TB
- en: '| Fully-connected CRF [[36](#bib.bib36)] | Depth + Ref. Image [[37](#bib.bib37)]
    | Sliding window [[11](#bib.bib11)] | Encoder + decoder [[15](#bib.bib15), [27](#bib.bib27),
    [49](#bib.bib49)] | Depth-balanced loss [[46](#bib.bib46)] | Left-Right consistency [[33](#bib.bib33)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hierarchical CRF [[1](#bib.bib1)] | Depth + CV + Ref. Image + rec. error [[26](#bib.bib26)]
    | Diffusion using CSPN [[50](#bib.bib50)] | Encoder + decoder with residual learning
    [[23](#bib.bib23), [37](#bib.bib37), [51](#bib.bib51), [26](#bib.bib26), [49](#bib.bib49)]
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Depth propagation [[20](#bib.bib20)] | Depth + Rewarped right image [[52](#bib.bib52)]
    | Diffusion using recurrent convolutional operation [[53](#bib.bib53)] | Progressive
    upsampling [[51](#bib.bib51)] |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CRF energy minimized with CNN [[4](#bib.bib4)] | Depth + Learned features [[10](#bib.bib10)].
    |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|   |'
  prefs: []
  type: TYPE_TB
- en: In general, the predicted disparity/depth maps are of low resolution, miss fine
    details, and may suffer from over-smoothing especially at object boundaries. Some
    methods also output incomplete and/or sparse maps. Deep-learning networks that
    directly predict high resolution and high quality maps would require a large number
    of parameters and thus are usually difficult to train. Instead, an additional
    refinement block is added to the pipeline. Its goal is to (1) improve the resolution
    of the estimated disparity/depth map, (2) refine the reconstruction of the fine
    details, and (3) perform depth/disparity completion. Such refinement block can
    be implemented using traditional approaches. For instance, Dosovitskiy *et al.* [[12](#bib.bib12)]
    use the variational approach from [[54](#bib.bib54)]. Huang *et al.* [[36](#bib.bib36)]
    apply the Fully-Connected Conditional Random Field (DenseCRF) of [[55](#bib.bib55)]
    to the predicted raw disparities. Li *et al.* [[1](#bib.bib1)] refine the predicted
    depth (or surface normals) from the super-pixel level to pixel level using a hierarchical
    Conditional Random Field (CRF). The use of DenseCRF or hierarchical CRF encourages
    the pixels that are spatially close and with similar colors to have closer disparity
    predictions. Also, this step removes unreliable matches via left-right check.
    Chen *et al.* [[20](#bib.bib20)] compute the final disparity map from the raw
    one, after removing unreliable matches, by propagating reliable disparities to
    non-reliable areas [[56](#bib.bib56)]. Note that the use of CRF for depth estimation
    has been also explored by Liu *et al.* [[4](#bib.bib4)]. However, unlike Li *et
    al.* [[1](#bib.bib1)], Liu *et al.* [[4](#bib.bib4)] used a CNN to minimize the
    CRF energy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will look at how the refinement block has been implemented
    using deep learning, see Table [III](#S3.T3 "TABLE III ‣ 3.1.4 Refinement ‣ 3.1
    The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") for a taxonomy of these methods. In general,
    the input to the refinement module can be: (1) the estimated depth/disparity map,
    (2) the estimated depth/disparity map concatenated with the reference image scaled
    to the resolution of the estimated depth/disparity map [[37](#bib.bib37)], (3)
    the initially-estimated disparity map, the cost volume, and the reconstruction
    error, which is calculated as the absolute difference between the multi-scale
    fusion features of the left image and the multi-scale fusion features of the right
    image but back-warped using the initial disparity map to the left image [[26](#bib.bib26)],
    (4) the raw disparity/depth map, and the right image but warped into the view
    of the left image using the estimated initial disparity map [[52](#bib.bib52)],
    and (5) the estimated depth/disparity map concatenated with the feature map of
    the reference image, *e.g.,* the output of a first convolutional layer [[10](#bib.bib10)].'
  prefs: []
  type: TYPE_NORMAL
- en: Note that refinement can be hierarchical by cascading several refinement modules [[23](#bib.bib23),
    [49](#bib.bib49)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4.1 Bottom-up approaches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A bottom-up network operates in a sliding window-like approach. It takes small
    patches and estimates the refined depth at the center of the patch [[11](#bib.bib11)].
    Lee *et al.* [[46](#bib.bib46)] follow a split-and-merge approach. The input image
    is split into regions, and a depth is estimated for each region. The estimates
    are then merged using a fusion network, which operates in the Fourier domain so
    that depth maps with different cropping ratios can be handled. The rational is
    that inferring accurate depth at the desired resolution would require large networks
    with a large number of parameters to estimate. By making the network focus on
    small regions, fine details can be recovered with less parameters. However, obtaining
    the entire refined map will require multiple forward passes, which is not suitable
    for realtime applications.
  prefs: []
  type: TYPE_NORMAL
- en: Another bottom-up refinement strategy is based on diffusion processes. The idea
    is to start with an incomplete depth map and use anisotropic diffusion to propagate
    the known depth to the regions where depth is missing. Convolutional Spatial Propagation
    Networks (CSPN) [[50](#bib.bib50)], which implement an anisotropic diffusion process,
    are particularly suitable this task. They take as input the original image and
    a sparse depth map, which can be the output of a depth estimation network, and
    predict, using a deep CNN, the diffusion tensor. This is then applied to the initial
    map to obtain the refined one. Cheng *et al.* [[53](#bib.bib53)] used this approach
    in their proposed refinement module. It takes an initial depth estimate and performs
    linear propagation, in which the propagation is performed with a manner of recurrent
    convolutional operation, and the affinity among neighboring pixels is learned
    through a deep CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4.2 Top-down approaches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another approach is to use a top-down network that processes the entire raw
    disparity/depth map. It can be implemented as (1) a decoder, which consists of
    unpooling units to extend the resolution of its input, as opposed to pooling,
    and convolution layers [[10](#bib.bib10)], or with (2) a encoder-decoder network [[15](#bib.bib15)].
    In the latter case, the encoder is to map the input into a latent space. The decoder
    then predicts the high resolution map from the latent variable. These networks
    also use skip connections from the contracting part to the expanding part so that
    fine details can be preserved. To avoid the checkboard artifacts produced by the
    deconvolutions and upconvolutions [[57](#bib.bib57), [27](#bib.bib27), [49](#bib.bib49)],
    several papers first upsample the initial map, *e.g.,* using bilinear upsampling,
    and then apply convolutions [[27](#bib.bib27), [49](#bib.bib49)].
  prefs: []
  type: TYPE_NORMAL
- en: These architectures can be used to directly predict the high resolution maps
    but also to predict the residuals [[37](#bib.bib37), [26](#bib.bib26), [49](#bib.bib49)].
    As opposed to directly learning the refined disparity map, residual learning provides
    a more effective refinement. In this approach, the estimated map and the resized
    reference image are concatenated and used as a 4-channel input to a refinement
    network, which learns the disparity/depth residual. The estimated residual is
    then added to the originally estimated map to generate the refined map.
  prefs: []
  type: TYPE_NORMAL
- en: Pang *et al.* [[23](#bib.bib23)] refine the raW disparity map using a cascade
    of two CNNs. The first stage advances the DispNet of [[13](#bib.bib13)] by adding
    extra up-convolution modules, leading to disparity images with more details. The
    second stage, initialized by the output of the first stage, explicitly rectifies
    the disparity; it couples with the first-stage and generates residual signals
    across multiple scales. The summation of the outputs from the two stages gives
    the final disparity.
  prefs: []
  type: TYPE_NORMAL
- en: Jeon and Lee [[51](#bib.bib51)] proposed a deep Laplacian Pyramid Network to
    spatially varying noise and holes. By considering local and global contexts, the
    network progressively reduces the noise and fills the holes from coarse to fine
    scales. It first predicts, using residual learning, a clean complete depth image
    at a coarse scale (quarter of the original resolution). The prediction is then
    progressively upsampled through the pyramid to predict the half and original sized
    clean depth image. The network is trained with 3D supervision using a loss that
    is a combination of a data loss and a structure-preserving loss. The data loss
    is a weighted sum of $L_{1}$ distance between the ground-truth depth and the estimated
    depth, the $L_{1}$ distance between the gradient of the ground-truth depth and
    the estimated depth, and the $L_{1}$ distance between the normal vectors of the
    estimated and ground-truth depths. The structure-preserving loss is gradient-based
    to preserve the original structures and discontinuities. It is defined as the
    $L_{2}$ distance between the maximum gradient around a pixel in the ground-truth
    depth map and the maximum gradient around that pixel in the estimate depth map.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4.3 Guided refinement
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Gidaris and Komodakis [[47](#bib.bib47)] argue that the approaches that predict
    either new depth estimates or residual corrections are sub-optimal. Instead, they
    propose a generic CNN architecture that decomposes the refinement task into three
    steps: (1) detecting the incorrect initial estimates, (2) replacing the incorrect
    labels with new ones, and (3) refining the renewed labels by predicting residual
    corrections with respect to them. Since the approach is generic, it can be used
    to refine the raw depth map produced by any other method, *e.g.,* [[19](#bib.bib19)].'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the predictions of the baseline backbone, which is composed of an
    encoder-decoder, are coarse and smooth due to the lack of depth details. To overcome
    this, Zhang *et al.* [[58](#bib.bib58)] introduced a hierarchical guidance strategy,
    which guides the estimation process to predict fine-grained details. They perform
    this by attaching refinement networks (composed of 5 conv-residual blocks and
    several following $1\times 1$ convolution layers) to the last three layers of
    the encoder (one per layer). Its role is to predict predict depth maps at these
    levels. The features learned by these refinement networks are used as input to
    their corresponding layers on the decoder part of the backbone network. This is
    similar to using skip connection. However, instead of feeding directly the features
    of the encoder, these are further processed to predict depth map at that level.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to handle equally close and far depths, Li *et al.* [[46](#bib.bib46)]
    introduced depth-balanced Euclidean loss to reliably train the network on a wide
    range of depths.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4.4 Leveraging other cues
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Qi *et al.* [[48](#bib.bib48)] proposed a mechanism that uses the depth map
    to refine the quality of the normal estimates, and the normal map to refine the
    quality of the depth estimates. This is done using a two-stream CNN, one for estimating
    an initial depth map and another for estimating an initial normal map. Then, it
    uses another two-stream networks: a depth-to-normal network and a normal-to-depth
    network. The former is used to refine the normal map using the initial depth map.
    The latter is used to refine the depth map using the estimated normal map.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The depth-to-normal network first takes the initial depth map and generates
    a rough normal map using PCA analysis. This is then fed into a 3-layer CNN, which
    estimates the residual. The residual is then added to the rough normal map, concatenated
    with the initial raw normal map, and further processed with one convolutional
    layer to output the refined normal map.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The normal-to-depth network uses kernel regression process, which takes the
    initial normal and depth maps, and regresses the refined depth map.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Instead of estimating a single depth map from the reference image, one can estimate
    multiple depth maps, one per input image, check the consistency of the estimates,
    and use the consistency maps to (recursively) refine the estimates. In the case
    of stereo matching, this process is referred to as the left-right consistency
    check, which traditionally was an isolated post-processing step and heavily hand-crafted.
  prefs: []
  type: TYPE_NORMAL
- en: The standard approach for implementing the left-right consistency check is as
    follows;
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute two disparity maps $D_{l}$ and $D_{r}$, one for the left image and another
    for the right image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproject the right disparity map onto the coordinates of the left image, obtaining
    $\tilde{D}_{r}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the error or confidence map indicating whether the estimated disparity
    is correct or not.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, use the computed confidence map to refine the disparity estimate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A simple way of computing the confidence map is by taking pixel-wise difference.
    Seki *et al.* [[59](#bib.bib59)], on the other hand, used a CNN trained in a classifier
    manner. It outputs a label per pixel indicating whether the estimated disparity
    is correct or not. This confidence map is then incorporated into a Semi-Global
    Matching (SGM) for dense disparity estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Jie *et al.* [[33](#bib.bib33)] perform left-right consistency check jointly
    with disparity estimation, using a Left-Right Comparative Recurrent (LRCR) model.
    It consists of two parallely stacked convolutional LSTM networks. The left network
    takes the cost volume and generates a disparity map for the left image. Similarly,
    the right network generates, independently of the left network, a disparity map
    for the right image. The two maps are converted to the opposite coordinates (using
    the known camera parameters) for comparison with each other. Such comparison produces
    two error maps, one for the left disparity and another for the right disparity.
    Finally, the error map for each image is concatenated with its associated cost
    volume and used as input at the next step to the convolutional LSTM. This will
    allow the LRCR model to selectively focus on the left-right mismatched regions
    at the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Stereo matching networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we have discussed how the different blocks of the stereo
    matching pipeline have been implemented using deep learning. This section discusses
    how different state-of-the-art techniques used these blocks and put them together
    to solve the pairwise stereo matching-based depth reconstruction problem.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Early methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Early methods, *e.g.,* [[17](#bib.bib17), [20](#bib.bib20), [34](#bib.bib34),
    [22](#bib.bib22), [19](#bib.bib19), [60](#bib.bib60)], replace the hand-crafted
    features and similarity computation with deep learning architectures. The basic
    architecture is composed of a stack of the modules described in Section [3.1](#S3.SS1
    "3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction"). The feature extraction module is implemented
    as a multi-branch network, with shared weights. Each branch computes features
    from its input. These are then matched using:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a fixed correlation layer (implemented as a convolutional layer) [[17](#bib.bib17),
    [19](#bib.bib19)],
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a fully connected neural network [[22](#bib.bib22), [34](#bib.bib34), [21](#bib.bib21),
    [18](#bib.bib18)], which takes as input the concatenated features of the patches
    from the left and right images and produces a matching score.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: convolutional networks composed of convolutional layers followed by ReLU [[35](#bib.bib35)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using convolutional and/or fully-connected layers enables the network to learn
    from data the appropriate similarity measure, instead of imposing one at the outset.
    It is more accurate than using a correlation layer but is significantly slower.
  prefs: []
  type: TYPE_NORMAL
- en: Note that while Zbontar *et al.* [[17](#bib.bib17), [18](#bib.bib18)] and Han
    *et al.* [[22](#bib.bib22)] use standard convolutional layers in the feature extraction
    block, Shaked and Wolf [[21](#bib.bib21)] add residual blocks with multilevel
    weighted residual connections to facilitate the training of very deep networks.
    It was demonstrated that this architecture outperformed the base network of Zbontar
    *et al.* [[17](#bib.bib17)]. To enable multiscale features, Chen *et al.* [[20](#bib.bib20)]
    replicate twice the feature extraction module and the correlation layer. The two
    instances take patches around the same pixel but of different sizes, and produce
    two matching scores. These are then merged using voting. Chen *et al.*’s approach
    shares some similarities with the central-surround two-stream network of [[34](#bib.bib34)].
    The main difference is that in [[34](#bib.bib34)], the output of the four branches
    of the descriptor computation module is given as input to a top decision network
    for fusion and similarity computation, instead of using voting. Zagoruyko and
    Komodakis [[34](#bib.bib34)] add at the end of each feature computation branch
    a Spatial Pyramid Pooling so that patches of arbitrary sizes can be compared.
  prefs: []
  type: TYPE_NORMAL
- en: Using these approaches, inferring the raw cost volume from a pair of stereo
    images is performed using a moving window-like approach, which would require multiple
    forward passes ($n_{d}$ forward passes per pixel). However, since correlations
    are highly parallelizable, the number of forward passes can be significantly reduced.
    For instance, Luo *et al.* [[19](#bib.bib19)] reduce the number of forward passes
    to one pass per pixel by using a siamese network where the first branch takes
    a patch around a pixel while the second branch takes a larger patch that expands
    over all possible disparities. The output is a single 64D representation for the
    left branch, and $n_{d}\times 64$ for the right branch. A correlation layer then
    computes a vector of length $n_{d}$ where its $0pt-$th element is the cost of
    matching the pixel $x$ on the left image with the pixel $x-0pt$ on the rectified
    right image. Other papers, *e.g.,* [[29](#bib.bib29), [39](#bib.bib39), [21](#bib.bib21),
    [35](#bib.bib35), [27](#bib.bib27), [49](#bib.bib49), [25](#bib.bib25)], compute
    the feature maps of the left and right images in a single forward pass. These,
    however, have a high memory footprint at runtime and thus the feature map is usually
    computed at a resolution that is lower than the resolution of the input images.
  prefs: []
  type: TYPE_NORMAL
- en: These early methods produce matching scores that can be aggregated into a cost
    volume, which corresponds to the data term of Equation ([1](#S3.E1 "In 3.1 The
    pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction")). They then extensively rely on hand-engineered
    post-processing steps, which are not jointly trained with the feature computation
    and feature matching networks, to regularize the cost volume and refine the disparity/depth
    estimation [[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 End-to-end methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recent works solve the stereo matching problem using a pipeline that is trained
    end-to-end without post-processing. For instance, Knöbelreiter *et al.* [[61](#bib.bib61)]
    proposed a hybrid CNN-CRF. The CNN part computes the matching term of Equation ([1](#S3.E1
    "In 3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")). This then becomes the unary
    term of a Conditional Random Field (CRF) module, which performs the regularization.
    The pairwise term of the CRF is parameterized by edge weights and is computed
    using another CNN. Using the learned unary and pairwise costs, the CRF tries to
    find a joint solution optimizing the total sum of all unary and pairwise costs
    in a 4-connected graph. The whole CNN-CRF hybrid pipeline, which is trained end-to-end,
    could achieve a competitive performance using much fewer parameters (and thus
    a better utilization of the training data) than the earlier methods.
  prefs: []
  type: TYPE_NORMAL
- en: Others papers [[12](#bib.bib12), [13](#bib.bib13), [29](#bib.bib29), [39](#bib.bib39),
    [21](#bib.bib21), [35](#bib.bib35), [27](#bib.bib27), [49](#bib.bib49), [25](#bib.bib25)]
    implement the entire pipeline using convolutional networks. In these approaches,
    the cost volume is computed in a single forward pass, which results in a high
    memory footprint. To reduce the memory footprint, some methods such as [[12](#bib.bib12),
    [13](#bib.bib13)] compute a lower resolution raw cost volume, *e.g.,* one half
    or one fourth of the size of the input images. Some methods, *e.g.,*  [[29](#bib.bib29),
    [39](#bib.bib39), [21](#bib.bib21), [28](#bib.bib28)], ommit the matching module.
    The left-right features, concatenated across the disparity range, are directly
    fed to the regularization and depth computation module. This, however, results
    in even larger memory footprint. Tulyakov *et al.* [[32](#bib.bib32)] reduce the
    memory use, without sacrificing accuracy, by introducing a matching module that
    compresses the concatenated features into compact matching signatures. The approach
    uses mean pooling instead of feature concatenation. This also reduces the memory
    footprint. More importantly, it allows the network to handle arbitrary number
    of multiview images, and to vary the number of input at runtime without re-training
    the network. Note that pooling layers have been used to aggregate features of
    different scales [[28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: The regularization module takes the cost volume, the concatenated features,
    or the cost volume concatenated with the reference image [[12](#bib.bib12)], with
    the features of the reference image [[12](#bib.bib12), [26](#bib.bib26)], and/or
    with semantic features such as the segmentation mask [[25](#bib.bib25)] or the
    edge map [[31](#bib.bib31)], which serve as semantic priors. It then regularizes
    it and outputs either a depth/disparity map [[12](#bib.bib12), [13](#bib.bib13),
    [23](#bib.bib23), [31](#bib.bib31), [26](#bib.bib26)] or a distribution over depth/disparities [[39](#bib.bib39),
    [29](#bib.bib29), [28](#bib.bib28), [33](#bib.bib33)]. Both the segmentation mask [[25](#bib.bib25)]
    and the edge map [[31](#bib.bib31)] can be computed using deep networks that are
    trained jointly and end-to-end with the disparity/depth estimation networks. Appending
    semantic features to the cost volume improves the reconstruction of fine details,
    especially near object boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: The regularization module is usually implemented as convolution-deconvolution
    (hourglass) deep network with skip connections between the contracting and expanding
    parts [[13](#bib.bib13), [12](#bib.bib12), [23](#bib.bib23), [39](#bib.bib39),
    [29](#bib.bib29), [28](#bib.bib28), [26](#bib.bib26)], or as a convolutional network [[14](#bib.bib14),
    [27](#bib.bib27)]. It can use 2D convolutions [[13](#bib.bib13), [12](#bib.bib12),
    [23](#bib.bib23), [26](#bib.bib26)] or 3D convolutions [[39](#bib.bib39), [29](#bib.bib29),
    [33](#bib.bib33), [28](#bib.bib28)]. The latter has less parameters. In both cases,
    their disparity range is fixed in advance and cannot be re-adjusted without re-training.
    Tulyakov *et al.* [[32](#bib.bib32)] introduced the sub-pixel MAP approximation
    for inference, which computes a weighted mean around the disparity with MAP probability.
    They showed that it is more robust to erroneous modes in the distribution and
    allows to modify the disparity range without re-training.
  prefs: []
  type: TYPE_NORMAL
- en: Depth can be computed from the regularized cost volume using (1) the softargmin
    operator [[14](#bib.bib14), [39](#bib.bib39), [49](#bib.bib49), [27](#bib.bib27)],
    which is differentiable and allows sub-pixel accuracy but limited to network outputs
    that are unimodal, or (2) sub-pixel MAP approximation [[32](#bib.bib32)], which
    can handle multi-modal distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Some papers, *e.g.,* [[39](#bib.bib39)], directly regress high-resolution map
    without an explicit refinement module. This is done by adding a final upconvolutinal
    layer to the regression module in order to upscale the cost volume to the resolution
    of the input images. In general, however, inferring high resolution depth maps
    would require large networks, which are expensive in terms of memory storage but
    also hard to train given the large number of free parameters. As such, some methods
    first estimate a low-resolution depth map and then refine it using a refinement
    module [[23](#bib.bib23), [26](#bib.bib26), [33](#bib.bib33)]. The refinement
    module as well as the early modules are trained jointly and end-to-end.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Multiview stereo (MVS) matching networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/aca83e4cf79dabd0e08dbdf2a91ea559.png) | ![Refer to
    caption](img/f2e95b16ba051754931a167cfc5f334f.png) |'
  prefs: []
  type: TYPE_TB
- en: '| (a) Hartmann *et al.* [[35](#bib.bib35)]. | (b) Flynn *et al.* [[14](#bib.bib14)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/4615460dadd96454e62c601b5c2ebb38.png) |'
  prefs: []
  type: TYPE_TB
- en: '| (c) Kar *et al.* [[38](#bib.bib38)] and Yao *et al.* [[37](#bib.bib37)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/767e96c2b576cc9ddc21d843f4ee16e1.png) |'
  prefs: []
  type: TYPE_TB
- en: '| (d) Huang *et al.* [[36](#bib.bib36)]. |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5: Taxonomy of multivewstereo methods. (a), (b), and (c) perform early
    fusion, while (d) performs early fusion by aggregating features across depth plans,
    and late fusion by aggregating cost volumes across views.'
  prefs: []
  type: TYPE_NORMAL
- en: The methods described in Section [3.2](#S3.SS2 "3.2 Stereo matching networks
    ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction") have been designed to reconstruct depth/disparity maps
    from a pair of stereo images. These methods can be extended to the multiview stereo
    (MVS) case, *i.e.,* $n>2$, by replicating the feature computation branch $n$ times.
    The features computed by the different branches can then be aggregated using,
    for example, pooling [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)] or
    a recurrent fusion unit [[38](#bib.bib38)] before feeding the aggregated features
    into a top network, which regresses the depth map (Figures [5](#S3.F5 "Figure
    5 ‣ 3.3 Multiview stereo (MVS) matching networks ‣ 3 Depth by stereo matching
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")-(a),
    (b), and (c)). Alternatively, one can sample pairs of views, estimate the cost
    volume from each pair, and then merge the cost volumes either by voting or pooling [[36](#bib.bib36)]
    (Figure [5](#S3.F5 "Figure 5 ‣ 3.3 Multiview stereo (MVS) matching networks ‣
    3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction")-(d)). The former called *early fusion* while the latter
    is called *late fusion*.
  prefs: []
  type: TYPE_NORMAL
- en: The early work of Hartmann *et al.* [[35](#bib.bib35)] introduced a mechanism
    to learn multi-patch similarity, which replaces the correlation layer used in
    stereo matching. The approach uses pooling to aggregate the features computed
    on the different patches before feeding them to the subsequent blocks of the standard
    stereo matching pipeline. Recent techniques use Plane-Sweep Volumes (PSV) [[14](#bib.bib14),
    [36](#bib.bib36)], feature unprojection to the 3D space [[38](#bib.bib38), [37](#bib.bib37)],
    and image unprojection to the 3D space resulting in the Colored Voxel Cube (CVC) [[62](#bib.bib62)].
  prefs: []
  type: TYPE_NORMAL
- en: Flynn *et al.* [[14](#bib.bib14)] and Huang *et al.* [[36](#bib.bib36)] use
    the camera parameters to unproject the input images into Plane-Sweep Volumes (PSV)
    and feed them into the subsequent feature extraction and feature matching networks.
    Flynn *et al.* [[14](#bib.bib14)]’s network is composed of $n_{d}$ branches, one
    for each depth plane (or depth value). The $0pt-$th branch of the network takes
    as input the reference image and the planes of the Plane-Sweep Volumes of the
    other images and which are located at depth $0pt$. These are packed together and
    fed to a two-stages network. The first stage, which consists of 2D convolutional
    rectified linear layers that share weights across all depth planes, computes matching
    features between the reference image and the PSV planes located at depth $0pt$.
    The second stage is composed of convolutional layers that are connected across
    depth planes in order to model interactions between them. The final layer of the
    network is a per-pixel softmax over depth, which returns the most probable depth
    value per pixel. The approach, which has been used for pairwise and multiview
    stereo matching, requires that the number of views and the camera parameters of
    each view to be known. It also requires setting in advance the disparity range.
  prefs: []
  type: TYPE_NORMAL
- en: 'Huang *et al.* [[36](#bib.bib36)]’s approach, which also operates on the plane-sweep
    volumes, uses a network composed of three parts: the patch matching part, the
    intra-volume feature aggregation part, and the inter-volume feature aggregation
    part:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The patch matching part is a siamese network. Its first branch extracts features
    from a patch in the reference image and the second one from the plane-sweep volume
    that corresponds to the $i-$th input image at the $0pt-$th disparity level. ($100$
    disparity values have been used.) The features are then concatenated and passed
    to the subsequent convolutional layers. This process is repeated for all the plane-swept
    images.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output from the patch matching module of the $0pt-$th plane-sweep volume
    are concatenated and fed into another encoder-decoder which produces a feature
    vector $F_{0}pt$ of size $64\times 64\times 800$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the feature vectors $F_{i},i=1,\dots,n$ (one for each input image), are
    aggregated using a max-pooling layer followed by convolutional layers, which produce
    a depth map of size $64\times 64$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unlike Flynn *et al.* [[14](#bib.bib14)], Huang *et al.* [[36](#bib.bib36)]’s
    approach does not require a fixed number of input views since aggregation is performed
    using pooling. In fact, the number of views at runtime can be different from the
    number of views used during training.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of using PSVs is that they eliminate the need to supply rectified
    images. In other words, the camera parameters are implicitly encoded. However,
    in order to compute the PSVs, the intrinsic and extrinsic camera parameters need
    to be either provided in advance or estimated using, for example, Structure-from-Motion
    techniques as in [[36](#bib.bib36)]. Also, these methods require setting in advance
    the disparity range and its discretisation.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using PSVs, other methods use the camera parameters to unproject
    either the input images [[62](#bib.bib62)] or the learned features [[38](#bib.bib38),
    [37](#bib.bib37)] into either a regular 3D feature grid, by rasterizing the viewing
    rays with the known camera poses, a 3D frustum of a reference camera [[37](#bib.bib37)],
    or by warping of the features into different parallel frontal planes of the reference
    camera, each one located at a specific depth. This unprojection aligns the features
    along epipolar lines, enabling efficient local matching by using either some distance
    measures such as the Euclidean or cosine distances [[38](#bib.bib38)], using a
    recurrent network [[38](#bib.bib38)], or using an encoder composed of multiple
    convolutional layers producing the probability of each voxel being on the surface
    of the 3D shape.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the approaches of Kar *et al.* [[38](#bib.bib38)] and Ji *et al.* [[62](#bib.bib62)]
    perform volumetric reconstruction and use 3D convolutions. Thus, due to the memory
    requirements, only a coarse volume of size $32^{3}$ could be estimated. Huang
    *et al.* [[36](#bib.bib36)] overcome this limitation by directly regressing depth
    from different reference images. Similarly, Yao *et al.* [[37](#bib.bib37)] focus
    on producing the depth map for one reference image at each time. Thus, it can
    directly reconstruct a large scene.
  prefs: []
  type: TYPE_NORMAL
- en: Table [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy and depth range ‣ 6.3 Multiview
    stereo techniques ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") summarizes the performance of these techniques.
    Note that most of them do not achieve sub-pixel accuracy, require the depth range
    to be specified in advance and cannot vary it at runtime without re-adjusting
    the network architecture and retraining it. Also, these methods fail in reconstructing
    tiny features such as those present in vegetation.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Depth estimation by regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of trying to match features across images, methods in this class directly
    regress disparity/depth from the input images or their learned features [[12](#bib.bib12),
    [13](#bib.bib13), [15](#bib.bib15), [63](#bib.bib63)]. These methods have no direct
    notion of descriptor matching. They consider a learned, view-based representation
    for depth reconstruction from either $n$ predefined viewpoints $\{v_{1},\dots,v_{n}\}$,
    or from any arbitrary viewpoint specified by the user. Their goal is to learn
    a predictor $f$ (see Section [2](#S2 "2 Scope and taxonomy ‣ A Survey on Deep
    Learning Architectures for Image-based Depth Reconstruction")), which predicts
    depth map from an input I.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Network architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We classify the state-of-the-art into two classes, based on the type of network
    architectures they use. In the first class of methods, the predictor $f$ is an
    encoder which directly regresses the depth map [[3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: In the second class of methods, the predictor $f$ is composed of an encoder
    and a top network. The encoder, which learns, using a convolutional network, a
    function $h$ that maps the input I into a compact latent representation $\textbf{x}=h(\textbf{I})\in\mathcal{X}$.
    The space $\mathcal{X}$ is referred to as *the latent space*. The encoder can
    be designed following any of the architectures discussed in Section [3.1](#S3.SS1
    "3.1 The pipeline ‣ 3 Depth by stereo matching ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction"). The top network $g$ takes the compact
    representation, and eventually the target viewpoint $v$, and generates the estimated
    depth map $\hat{D}=g\left(h(\textbf{I}),v\right)=(g\circ h)(\textbf{I},v)$. Some
    methods use a top network composed of fully connected layers [[10](#bib.bib10),
    [1](#bib.bib1), [11](#bib.bib11), [4](#bib.bib4)]. Others use a decoder composed
    of upconvolutional layers [[2](#bib.bib2), [13](#bib.bib13), [44](#bib.bib44),
    [64](#bib.bib64), [63](#bib.bib63)].
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of fully-connected layers is that they aggregate information coming
    from the entire image, and thus enable the network to infer depth at each pixel
    using global information. Convolutional operations, on the other hand, can only
    see local regions. To capture larger spatial relations, one needs to increase
    the number of convolution layers or use dilated convolutions, *i.e.,* large convolutional
    filters but with holes
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Input encoding networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In general, the encoder stage is composed of convolutional layers, which capture
    local interactions between image features, followed by a number of fully-connected
    layers, which capture global interactions. Some layers are followed by spatial
    pooling operations to reduce the resolution of the output. For instance, Eigen
    *et al.* [[10](#bib.bib10)], one of the early works that tried to regress depth
    directly from a single input image, used an encoder composed of five feature extraction
    layers of convolution and max-pooling followed by one fully-connected layer. This
    maps the input image of size $304\times 228$ or $576\times 172$, depending on
    the dataset, to a latent representation of dimension $1\times 4096$. Liu *et al.* [[4](#bib.bib4)],
    on the other hand, used 7 convolutional layers to map the input into a low resolution
    feature map of dimension $512$.
  prefs: []
  type: TYPE_NORMAL
- en: Garg *et al.* [[3](#bib.bib3)] used this architecture to directly regress depth
    map from an input RGB images of size $188\times 620$. The encoder is composed
    of $7$ convolutional layers. The second, third, and fifth layers are followed
    by pooling layers to reduce the size of the output and thus the number of parameters
    of the network. The output of the sixth layer is a feature vector, which can be
    seen as a latent representation of size $16\times 16\times 2048$. The last convolutional
    layer maps this latent representation into a depth map of size $17\times 17$,
    which is upsampled using two fully connected layers and three upconvolutional
    layers, into a depth map of size $176\times 608$. Since it only relies on convolutional
    operation to regress depth, the approach does not capture global interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Li *et al.* [[1](#bib.bib1)] extended the approach of Eigen *et al.* [[10](#bib.bib10)]
    to operate on superpixels and at multiple scales. Given an image, super-pixels
    are obtained and multi-scale image patches (at five different sizes) are extracted
    around the super-pixel centers. All patches of a super-pixel are resized to $227\times
    227$ pixels to form a multiscale input to a pre-trained multi-branch deep network
    (AlexNet or VGGNet). Each branch generates a latent representation of size $1\times
    4096$. The latent representations from the different branches are concatenated
    together and fed to the top network. Since the networks process patches, obtaining
    the entire depth map requires multiple forward passes.
  prefs: []
  type: TYPE_NORMAL
- en: Since its introduction, this approach has been extended in many ways. For instance,
    Eigen and Fergus [[2](#bib.bib2)] showed a substantial improvement by switching
    from AlexNet (used in [[10](#bib.bib10)]) to VGG, which has a higher disciminative
    power. Also, instead of using fully convolutional layers, Laina *et al.* [[64](#bib.bib64)]
    incorporate residual blocks to ease the training. The encoder is implemented following
    the same architecture as ResNet50 but without the fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: Using repeated spatial pooling reduces the spatial resolution of the feature
    maps. Although high-resolution maps can be obtained using the refinement techniques
    of Section  [3.1.4](#S3.SS1.SSS4 "3.1.4 Refinement ‣ 3.1 The pipeline ‣ 3 Depth
    by stereo matching ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction"), this would require additional computational and memory costs.
    To overcome this problem, Fu *et al.* [[41](#bib.bib41)], removed some pooling
    layers and replaced some convolutions with dilated convolutions. In fact, convolutional
    operations are local, and thus, they do not capture the global structure. To enlarge
    their receptive field, one can increase the size of the filters, or increase the
    number of convolutional and pooling layers. This, however, would require additional
    computational and memory costs, and will complicate the network architecture and
    the training procedure. One way to solve this problem is by using dilated convolutions,
    *i.e.,* convolutions with filters that have holes [[41](#bib.bib41)]. This allows
    to enlarge the receptive field of the filters without decreasing the spatial resolution
    or increasing the number of parameters and computation time.
  prefs: []
  type: TYPE_NORMAL
- en: Using this principle, Fu *et al.* [[41](#bib.bib41)] proposed an encoding module
    that operates in two stages. The first stage extracts a dense feature map using
    an encoder whose last few downsampling operators (pooling, strides) are replaced
    with dilated convolutions in order to enlarge the receptive field of the filters.
    The second stage processes the dense feature map using three parallel modules;
    a full image encoder, a cross channel leaner, and an atrous spatial pyramid pooling
    (ASPP). The full image encoder maps the dense feature map into a latent representation.
    It uses an average pooling layer with a small kernel size and stride to reduce
    the spatial dimension. It is then followed by a fully connected layer to obtain
    a feature vector, then add a convolutional layer with $1\times 1$ kernel and copy
    the resultant feature vector into a feature map where each entry has the same
    feature vector. The ASPP module extracts features from multiple large receptive
    fields via dilated convolutions, with three different dilation rates. The output
    of the three modules are concatenated to form the latent representation.
  prefs: []
  type: TYPE_NORMAL
- en: These encoding techniques extract absolute features, ignoring the depth constraints
    of neighboring pixels, *i.e.,* relative features. To overcome this limitation,
    Gan *et al.* [[65](#bib.bib65)] explicitly model the relationships of different
    image locations using an affinity layer. They also combine absolute and relative
    features in an end-to-end network. In this approach, the input image is first
    processed by a ResNet50 encoder. The produced absolute feature map is fed into
    a context network, which captures both neighboring and global context information.
    It is composed of an affinity layer, which computes correlations between the features
    of neighboring pixels, followed by a fully-connected layer, which combines absolute
    and relative features. The output is fed into a depth estimator, which produces
    a coarse depth map.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Decoding networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many techniques first compute a latent representation of the input and then
    use a top network to decode the latent representation into a coarse depth map.
    In general, the decoding process can be done with either a series of fully-connected
    layers [[10](#bib.bib10), [1](#bib.bib1), [11](#bib.bib11), [4](#bib.bib4)], or
    upconvolutional layers [[2](#bib.bib2), [13](#bib.bib13), [44](#bib.bib44), [64](#bib.bib64),
    [63](#bib.bib63)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2.1 Using fully-connected layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Eigen *et al.* [[10](#bib.bib10)], Li *et al.* [[1](#bib.bib1)], Eigen and Fergus [[2](#bib.bib2)]
    and Liu *et al.* [[4](#bib.bib4)] use a top network composed of two fully-connected
    layers. The main advantage of using fully connected layers is that their receptive
    field is global. As such, they aggregate information from the entire image in
    the process of estimating the depth map. By doing so, however, the number of parameters
    in the network is high, and subsequently the memory requirement and computation
    time increase substantially. As such, these methods only estimate low-resolution
    coarse depth maps, which are then refined using some refinement blocks. For example,
    Eigen and Fergus [[2](#bib.bib2)] use two refinement blocks similar to those used
    in [[10](#bib.bib10)]. The first one produces predictions at a mid-level resolution,
    while the last one produces high resolution depth maps, at half the resolution
    of the output. In this approach, the coarse depth prediction network and the first
    refinement stage are trained jointly, with 3D supervision.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2.2 Using up-convolutional layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Dosovitskiy *et al.* [[12](#bib.bib12)] extended the approach of Eigen *et al.* [[10](#bib.bib10)]
    by removing the fully-connected layers. Instead, they pass the feature map, *i.e.,*
    the latent representation (of size $6\times 8\times 1024$), directly into a decoder
    to regress the optical flow in the case of the FlowNetSimple of [[12](#bib.bib12)],
    and a depth map in the case of [[2](#bib.bib2)]. In general, the decoder mirrors
    the encoder. It also includes skip connections, *i.e.,* connections from some
    layers of the encoder to their corresponding counterpart in the decoder. Dosovitskiy
    *et al.* [[12](#bib.bib12)] use variational refinement to refine the coarse optical
    flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chen *et al.* [[66](#bib.bib66)] used a similar approach to produce dense depth
    maps given an RGB image with known depth at a few pixels. At training, the approach
    takes the ground-truth depth map and a binary mask indicating valid ground-truth
    depth pixels, and generates two other maps: the nearest-neighbor fill of the sparse
    depth map, and the Euclidean distance transform of the binary mask. These two
    maps are then concatenated together and with the input image and used as input
    to an encoder-decoder, which learns the residual that will be added to the sparse
    depth map. The network follows the same architecture as in [[67](#bib.bib67)].
    Note that the same approach has been also used to infer other properties, *e.g.,*
    the optical flow as in Zhou *et al.* [[44](#bib.bib44)].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Combining and stacking multiple networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several previous papers showed that stacking and combining multiple networks
    can lead to significantly improved performance. For example, Ummenhofer and Zhou
    [[15](#bib.bib15)] introduced DeMoN, which takes an image pair as input and predicts
    the depth map of the left image and the relative pose (egomotion) of the right
    image with respect to the left. The network consists of a chain of three blocks
    that iterate over optical flow, depth, and relative camera pose estimation. The
    first block in the chain, called bootstrap net, is composed of two encoder-decoder
    networks. It gets the image pair as input and then estimates, using the first
    encoder-decoder, the optical flow and a confidence map. These, along with the
    original pair of images, are fed to the second encoder-decoder, which outputs
    the initial depth and egomotion estimates. The second component, called iterative
    net, is trained to improve, in a recursive manner, the depth, normal, and motion
    estimates. Finally, the last component, called refinement net, upsamples, using
    and encoder-decoder network, the output of the iterative net to obtain high resolution
    depth maps.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, unlike other techniques such as FlowNetSimple of [[12](#bib.bib12)]
    which require a calibrated pair of images, Ummenhofer and Zhou [[15](#bib.bib15)]
    estimates jointly the relative camera motion and the depth map.
  prefs: []
  type: TYPE_NORMAL
- en: FlowNetSimple of [[12](#bib.bib12)] has been later extended by Ilg *et al.* [[68](#bib.bib68)]
    to FlowNet2.0, which achieved results that are competitive with the traditional
    methods, but with an order of magnitude faster. The idea is to combine multiple
    FLowNetSimple networks to compute large displacement optical flow. It (1) stacks
    multiple FlowNetSimple and FlowNetC networks [[12](#bib.bib12)]. The flow estimated
    by each network is used to warp, using a warping operator, the right image onto
    the left image, and feed the concatenated left image, warped image, estimated
    flow, and the brightness error, into the next network. This way, the next network
    in the stack can focus on learning the remaining increment between the left and
    right images, (2) adds another FlowNetSimple network, called FlowNet-SD, which
    focuses on small subpixel motion, and (3) uses a learning schedule consisting
    of multiple datasets. The output of the FlowNet-SD and the stack of multiple FlowNetSimple
    modules are merged together and processed using a fusion network, which provides
    the final flow estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Roy *et al.* [[69](#bib.bib69)] observed that among all the training data sets
    currently available, there is limited training data for some depths. As a consequence,
    deep learning techniques trained with these datasets will naturally achieve low
    performance in the depth ranges that are under-represented in the training data.
    Roy *et al.* [[69](#bib.bib69)] mitigate the problem by combining CNN with a Neural
    Regression Forest. A patch around a pixel is processed with an ensemble of binary
    regression tree, called Convolutional Regression Tree (CRT). At every node of
    the CRT, the patch is processed with a shallow CNN associated to that node, and
    then passed to the left or right child node with a Bernouli probability for further
    convolutional processing. The process is repeated until the patch reaches the
    leaves of the tree. Depth estimates made by every leaf are weighted with the corresponding
    path probability. The regression results of every CRT are then fused into a final
    depth estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Chakrabarti [[70](#bib.bib70)] combine global and local methods. The method
    first maps an input image into a latent representation of size $1\times 1096$,
    which is then reshaped into a feature map of size $427\times 562\times 64$. In
    other words, each pixel is represented with a global descriptor (of size $1\times
    64$) that characterizes the entire scene. A parallel path takes patches of size
    $97\times 97$ around each pixel and computes a local feature vector of size $1\times
    1024$. This one is then concatenated with the global descriptor of that pixel
    and fed into a top network. The whole network is trained to predict, at every
    image location, depth derivatives of different orders, orientations, and scales.
    However, instead of a single estimate for each derivative, the network outputs
    probability distributions that allow it to express confidence about some coefficients,
    and ambiguity about others. Scene depth is then estimated by harmonizing this
    overcomplete set of network predictions, using a globalization procedure that
    finds a single consistent depth map that best matches all the local derivative
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Joint task learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Depth estimation and many other visual image understanding problems, such as
    segmentation, semantic labelling, and scene parsing, are strongly correlated and
    mutually beneficial. Leveraging on the complementarity properties of these tasks,
    many recent papers proposed to either jointly solve these tasks so that one boosts
    the performance of another.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, Wang *et al.* [[71](#bib.bib71)] follow the CNN structure in [[10](#bib.bib10)]
    but adds additional semantic nodes, in the final layer, to predict the semantic
    label. Both depth estimation and semantic label prediction are trained jointly
    using a loss function that is a weighted sum of the depth error and the semantic
    loss. The overall network is composed of a joint global CNN, which predicts, from
    the entire image, a coarse depth and segmentation maps, and a regional CNN, which
    operates on image segments (obtained by over segmenting the input image) and predicts
    a more accurate depth and segmentation labels within each segment. These two predictions
    form unary terms to a hierarchical CRF, which produces the final depth and semantic
    labels. The CRF includes additional pairwise terms such as pairwise edges between
    neighboring pixels, and pairwise edges between neighboring segments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhou *et al.* [[72](#bib.bib72)] follow the same idea to jointly estimate,
    from two successive images and in a non-supervised manner, the depth map at each
    of them, the 6D relative camera pose, and the forward and backward optical flows.
    The approach uses three separate network, but jointly trained using a cross-task
    consistency loss: a DepthNet, which estimates depth from two successive frames,
    PoseNet, which estimates the relative camera pose, and a FlowNet, which estimates
    the optical flow between the two frames. To handle non-rigid transformations that
    cannot be explained by the camera motion, the paper exploits the forward-backward
    consistency check to identify valid regions, *i.e.,* regions that moved in a rigid
    manner, and avoid enforcing the cross-task consistency in the non-valid regions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the approaches of Wang *et al.* [[71](#bib.bib71)] and Zhou *et al.* [[72](#bib.bib72)],
    the networks (or network components) that estimate each modality do not directly
    share knowledge. Collaboration between them is only through a joint loss function.
    To enable information exchange between the different task, Xu *et al.* [[73](#bib.bib73)]
    proposed an approach that first maps the input image into a latent representation
    using a CNN. The latent representation is then decoded, using four decoding streams,
    into a depth map, a normal map, an edge map, and a semantic label map. These multi-modal
    intermediate information is aggregated using a multi-model distillation module,
    and t hen passed into two decoders, one estimates the refined depth map and the
    other one estimates the refined semantic label map. For the multi-model distillation
    module, Xu *et al.* [[73](#bib.bib73)] investigated three architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple concatenation of the four modalities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concatenating the four modalities and feeding them to two different encoders.
    One encoder learns the features that are appropriate for inferring depth while
    the second learns features that are appropriate for inferring semantic labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an attention mechanism to guide the message passing between the multi-modal
    features, before concatenating them and feeding them to the encoder that learns
    the features for depth estimation or the one which learns the features for semantic
    labels estimation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Xu *et al.* [[73](#bib.bib73)] showed that the third option obtains remarkably
    better performance than the others.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of a distillation module, Jiao *et al.* [[74](#bib.bib74)] proposed
    a a synergy network whose backbone is a shared encoder. The network then splits
    into two branches, one for depth estimation and another for semantic labelling.
    These two branches share knowledge through lateral sharing units . The network
    is trained with a attention-driven loss, which guides the network to pay more
    attention to the distant depth regions during training.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, instead of estimating depth and semantic segmentation in a single iteration,
    Zhang *et al.* [[52](#bib.bib52)] performed it recursively, using Task-Recursive
    learning. It is composed of an encoder and a decoder network, with a series of
    residual blocks (ResNet), upsampling blocks, and Task-Attention Modules. The input
    image is first fed into the encoder and then into the task-recursive decoding
    to estimate depth and semantic segmentation. In the decoder, the two tasks (depth
    estimation and segmentation) are alternately processed by adaptively evolving
    previous experiences of both tasks to benefit each other. A task-attention module
    is used before each residual block and takes depth and segmentation features from
    the previous residual block as input. It is composed of a balance unit, to balance
    the contribution of the features of the two sources. The balanced output is fed
    into a series of convolutional-deconvolutional layers designed to get different
    spatial attentions by using receptive field variation. The output is an attention
    map, which is used to generate the gated depth and segmentation features. These
    are fused by concatenation followed by one convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: While jointly estimating depth and other cues, *e.g.,* semantic labels, significantly
    improves the performance of both tasks, it requires a large amount of training
    data annotated with depth and semantic labels.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training process aims to find the network parameters $W^{*}$ that minimize
    a loss function $\mathcal{L}$, *i.e.,* :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W^{*}=\arg\min_{W}\mathcal{L}(\hat{D},\Theta,W).$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\Theta$ is the training data, which can be composed of input images,
    their associated camera parameters, and/or their corresponding ground-truth depth.
    We will review in Section [5.1](#S5.SS1 "5.1 Datasets and data augmentation ‣
    5 Training ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    the different different datasets that have been used for training deep learning-based
    depth reconstruction algorithms, and for evaluating their performances. We will
    then review the different loss functions (Section [5.2](#S5.SS2 "5.2 Loss functions
    ‣ 5 Training ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")),
    the degree of supervision required in various methods (Section [5.3](#S5.SS3 "5.3
    Degree of supervision ‣ 5 Training ‣ A Survey on Deep Learning Architectures for
    Image-based Depth Reconstruction")), and the domain adaptation and transfer learning
    techniques (Section [5.3.4](#S5.SS3.SSS4 "5.3.4 Domain adaptation and transfer
    learning ‣ 5.3 Degree of supervision ‣ 5 Training ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction")).
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Datasets and data augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE IV: Datasets for depth/disparity estimation. ”$\#f$” refers to the number
    of frames per video. ”#img./scene” refers to the number of images per scenes.
    We also refer the reader to [[75](#bib.bib75)] for more 3D datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Frame size | Number of (pairs of) images |  | Video (scene)
    |  | Camera params | Disp/Depth |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Total | #train | #val | #test |  | $\#f$ | #train | #test | #img./scene
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CityScapes [[76](#bib.bib76)] | real | $2048\times 1024$ | $5000$ | $2975$
    | $500$ | $1525$ |  | $-$ | $-$ | $-$ | $-$ |  | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI 2015 [[77](#bib.bib77)] | real | $1242\times 375$ | $-$ | $-$ | $-$
    | $-$ |  | $400$ | $200$ | $200$ | $4$ |  | int+ext | sparse |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI 2012 [[78](#bib.bib78)] | real | $1240\times 376$ | 389 | $194$ | $-$
    | $195$ |  | $-$ | $-$ | $-$ | $-$ |  | int+ext | sparse |'
  prefs: []
  type: TYPE_TB
- en: '| FlyingThings3D [[13](#bib.bib13)] | synth. | $960\times 540$ | $26066$ |
    $21818$ | $-$ | $4248$ |  | $-$ | $2247$ | $-$ | $-$ |  | int+ext | per-pixel
    |'
  prefs: []
  type: TYPE_TB
- en: '| Monkaa [[13](#bib.bib13)] | synth. | $960\times 540$ | $8591$ | $8591$ |
    $-$ | $-$ |  | $-$ | $8$ | $-$ | $-$ |  | int+ext | per-pixel |'
  prefs: []
  type: TYPE_TB
- en: '| Driving [[13](#bib.bib13)] | synth. | $960\times 540$ | $4392$ | $4392$ |
    $-$ | $-$ |  | $-$ | $1$ | $-$ | $-$ |  | int+ext | per-pixel |'
  prefs: []
  type: TYPE_TB
- en: '| MPI Sintel [[79](#bib.bib79)] | synth. | $1024\times 436$ | $1041$ | $1041$
    | $-$ | $-$ |  | $35$ | $23$ | $12$ | $50$ |  | $-$ | per-pixel |'
  prefs: []
  type: TYPE_TB
- en: '| SUN3D [[80](#bib.bib80)] | rooms | $640\times 480$ | $-$ | $2.5$M | $-$ |
    $-$ |  | $-$ | $415$ | $-$ | $-$ |  | ext. | per-pixel |'
  prefs: []
  type: TYPE_TB
- en: '| NYU2 [[81](#bib.bib81)] | indoor | $640\times 480$ | $-$ | $1449$ | $-$ |
    $-$ |  | $-$ | $464$ | $-$ | $-$ |  | no | per-pixel |'
  prefs: []
  type: TYPE_TB
- en: '| RGB-D SLAM [[82](#bib.bib82)] | real | $640\times 480$ | $-$ | $-$ | $-$
    | $-$ |  | $-$ | $15$ | $4$ | variable |  | int+ext | per-pixel |'
  prefs: []
  type: TYPE_TB
- en: '| MVS-Synth [[36](#bib.bib36)] | Urban | $1920\times 1080$ | $-$ | $-$ | $-$
    | $-$ |  | $120$ | $-$ | $-$ | $100$ |  | int+ext | per-pixel |'
  prefs: []
  type: TYPE_TB
- en: '| ETH3D [[83](#bib.bib83)] | in/outdoor | $713\times 438$ |  | $27$ | $-$ |
    $20$ |  |  | $5$ | $5$ | $-$ |  | int+ext | point cloud |'
  prefs: []
  type: TYPE_TB
- en: '| DTU [[84](#bib.bib84)] | MVS | $1200\times 1600$ |  |  |  |  |  | $80$ |  |  |
    $49-64$ |  | int+ext |  |'
  prefs: []
  type: TYPE_TB
- en: '| MVS KITTI2015 [[77](#bib.bib77)] | MVS |  |  |  |  |  |  | $200+200$ |  |  |
    $20$ |  | $-$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| ETH3D [[83](#bib.bib83)] | MVS | $6048\times 4032$ |  |  |  |  |  | $13+12$
    |  |  | variable |  | $-$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Make3D [[85](#bib.bib85)] | Single view | $2272\times 1704$ | $534$ | $400$
    | $-$ | $134$ |  | $-$ | $-$ | $-$ | $-$ |  | $-$ | $55\times 305$ |'
  prefs: []
  type: TYPE_TB
- en: '| MegaDepth [[86](#bib.bib86)] | Single, MVS | $-$ | $130$K | $-$ | $-$ | $-$
    |  | $196$ | $-$ | $-$ | $-$ |  | $-$ | Eucl., ordinal |'
  prefs: []
  type: TYPE_TB
- en: Unlike traditional 3D reconstruction techniques, training and evaluating deep-learning
    architectures for depth reconstruction require large amounts of annotated data.
    This annotated data should be in the form of natural images and their corresponding
    depth maps, which is very challenging to obtain. Tables [IV](#S5.T4 "TABLE IV
    ‣ 5.1 Datasets and data augmentation ‣ 5 Training ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction") summarizes some of the datasets
    that have been used in the literature. Some of them have been specifically designed
    to train, test, and benchmark stereo-based depth reconstruction algorithms. They
    usually contain pairs of stereo images of real or synthesized scenes, captured
    with calibrated cameras, and their corresponding disparity/depth information as
    ground truth. The disparity/depth information can be either in the form of maps
    at the same or lower esolution as the input images, or in the form of sparse depth
    values at some locations in the reference image. Some of these datasets contain
    video sequences and thus are suitable for benchmarking Structure from Motion (SfM)
    and Simultaneous Localisation and Mapping (SLAM) algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The datasets that were particularly designed to train and benchmark multiview
    stereo and single view-based reconstruction algorithms (MVS) are composed of multiple
    scenes with $n\geq 1$ images per scene. Each image is captured from a different
    viewpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, deep-learning models achieve good results if trained on large datasets.
    Obtaining the ground-truth depth maps is, however, time-consuming and resource
    intensive. To overcome this limitation, many papers collect data from some existing
    datasets and augment them with suitable information and annotations to make them
    suitable for training and testing deep learning-based depth reconstruction techniques.
    In general, they use the four following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D data augmentation. To introduce more diversity to the training datasets,
    one can apply to the existing datasets some geometric and photometric transformations,
    *e.g.,* translation, rotation, and scaling, as well as additive Gaussian noise
    and changes in brightness, contrast, gamma, and color. Although some transformations
    are similarity preserving, they still enrich the datasets. One advantage of this
    approach is that it reduces the network’s generalization error. Also statistical
    shape analysis techniques [[87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89)]
    can be used to synthesize more 3D shapes from existing ones.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using synthesized 3D models and scenes. One approach to generate image-depth
    annotations is by synthetically rendering from 3D CAD models 2D and 2.5D views
    from various (random) viewpoints, poses, and lighting conditions. They can also
    be overlayed with random textures.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural image - 3D shape/scene pairs. Another approach is to synthesize training
    data by overlaying images rendered from large 3D model collections on the top
    of real images such as those in the SUN [[90](#bib.bib90)], ShapeNet [[91](#bib.bib91)],
    ModelNet [[92](#bib.bib92)], IKEA [[93](#bib.bib93)], and PASCAL 3D+ [[94](#bib.bib94)]
    datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While the last two techniques allow enriching existing training datasets, they
    suffer from domain bias and thus require using domain adaptation techniques, see
    Section [5.3.4](#S5.SS3.SSS4 "5.3.4 Domain adaptation and transfer learning ‣
    5.3 Degree of supervision ‣ 5 Training ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction"). Finally, some papers overcome the need
    for ground-truth depth information by training their deep networks without 3D
    supervision, see Section [5.3](#S5.SS3 "5.3 Degree of supervision ‣ 5 Training
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction").
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Loss functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The role of the loss function is to measure at each iteration how far the estimated
    disparity/depth map $\hat{D}$ is from the real map $D$, and use it to guide the
    update of the network weights. In general, the loss function is defined as the
    sum of two terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}(\hat{D},\Theta,W)=\mathcal{L}_{1}(\hat{D},\Theta,W)+\mathcal{L}_{2}(\hat{D},\Theta,W).$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: The data term $\mathcal{L}_{1}$ measures the error between the ground truth
    and the estimated depth while the regularization term $\mathcal{L}_{2}$ is used
    to incorporate various constraints, *e.g.,* smoothness. To ensure robustness to
    spurious outliers, some techniques, *e.g.,* [[95](#bib.bib95)], use a truncated
    loss, which is defined at each pixel $x$ as $\min(\mathcal{L}_{x},\psi)$. Here,
    $\mathcal{L}_{x}$ denotes the non-truncated loss at pixel $x$, and $\psi$ is a
    pre-defined threshold.
  prefs: []
  type: TYPE_NORMAL
- en: There are various loss functions that have been used in the literature. Below,
    we list the most popular ones. Tables [V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo
    matching techniques ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction"), [VI](#S6.T6 "TABLE VI ‣
    6.3.2 Accuracy and depth range ‣ 6.3 Multiview stereo techniques ‣ 6 Discussion
    and comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction"), and [VII](#S6.T7 "TABLE VII ‣ 6.4 Depth regression techniques
    ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction") show how these terms have been used to train depth estimation
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 The data term
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The data term of Equation ([7](#S5.E7 "In 5.2 Loss functions ‣ 5 Training ‣
    A Survey on Deep Learning Architectures for Image-based Depth Reconstruction"))
    measures the error between the ground truth and the estimated depth. Such error
    can be quantified using one or a weighted sum of two or more of the error measures
    described below. The $L_{2}$, the mean absolute difference, the cross-entropy
    loss, and the Hinge loss require 3D supervision while re-projection based losses
    can be used without 3D supervision since they do not depend on ground truth depth/disparity.
  prefs: []
  type: TYPE_NORMAL
- en: (1) The $L_{2}$ loss is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{1}^{0}=\frac{1}{N}\sum_{x}\&#124;D(x)-\hat{D}(x)\&#124;^{2},$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $N$ is the number of pixels being considered.
  prefs: []
  type: TYPE_NORMAL
- en: (2) The mean absolute difference (mAD) between the ground truth and the predicted
    disparity/depth maps [[39](#bib.bib39), [15](#bib.bib15), [23](#bib.bib23), [53](#bib.bib53)]
    is defined as follows;
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{1}^{1}=\frac{1}{N}\sum_{x}\&#124;D(x)-\hat{D}(x)\&#124;_{1}.$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'Many variants of this loss function have been used. For instance, Tonioni *et
    al.* [[96](#bib.bib96)] avoid explicit 3D supervision by taking $d_{x}$ as the
    disparity/depth at pixel $x$ computed using traditional stereo matching techniques,
    $\hat{D}(x)$ as the estimated disparity, and $c_{x}$ as the confidence of the
    estimate at $x$. They then define a *confidence-guided loss* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{L}_{1}^{2}=\frac{1}{N}\sum_{x}L(x),\text{ }L(x)=\left\{\begin{tabular}[]{@{}l@{}l@{}}$c_{x}&#124;d_{x}-\hat{d}_{x}&#124;$&amp;{
    if} $c_{x}\geq\epsilon$,\\ $0$&amp;{ }otherwise.\end{tabular}\right.}$ |  | (10)
    |'
  prefs: []
  type: TYPE_TB
- en: Here, $\epsilon$ is a user-defined threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yao *et al.* [[37](#bib.bib37)], which first estimate an initial depth map
    $\hat{D}_{0}$ and then the refined one $\hat{D}$, define the overall loss as the
    weighted sum of the mean absolute difference between the ground truth $D$ and
    $\hat{D}_{0}$, and the ground truth $D$ and $\hat{D}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{L}_{1}^{3}=\frac{1}{N}\sum_{x}\left\{\&#124;d(x)-\hat{d}_{0}(x)\&#124;_{1}+\lambda\&#124;d(x)-\hat{d}(x)\&#124;_{1}\right\}}.$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $d_{x}=D(x)$, and $\lambda$ is a weight factor, which is set to one in [[37](#bib.bib37)].
    Khamis *et al.* [[27](#bib.bib27)], on the other hand, used the two-parameter
    robust function $\rho(\cdot)$, proposed in [[97](#bib.bib97)], to approximate
    a smoothed $L_{1}$ loss. It is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $\mathcal{L}_{1}^{4}=\frac{1}{N}\sum_{x}\rho(d_{x}-\hat{d}_{x},\alpha,c),\text{
    where }\alpha=1,c=2,\text{ and }$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\rho(x,\alpha,c)=\frac{&#124;2-\alpha&#124;}{\alpha}\left(\left(\frac{x^{2}}{c^{2}&#124;2-\alpha&#124;}+1\right)^{\frac{\alpha}{2}}-1\right).$
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 'Other papers, *e.g.,*  [[28](#bib.bib28)], use the smooth $L_{1}$ loss, which
    is widely used in bounding box regression for object detection because of its
    robustness and low sensitivity to outliers. It is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{1}^{5}=\frac{1}{N}\sum_{x}\text{smooth}_{L_{1}}(d_{x}-\hat{d}_{x}),$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where <math  class="ltx_math_unparsed" alttext="\text{smooth}_{L_{1}}(x)=\left\{\begin{tabular}[]{ll}$0.5x^{2}$&amp;$\text{
    if }|x|<1$,\\
  prefs: []
  type: TYPE_NORMAL
- en: $|x|-0.5$&amp;$\text{ otherwise}$.\end{tabular}\right." display="inline"><semantics
    ><mrow ><msub ><mtext
    >smooth</mtext><msub ><mi
    >L</mi><mn >1</mn></msub></msub><mrow
    ><mo stretchy="false" >(</mo><mi
    >x</mi><mo stretchy="false" >)</mo></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd
    class="ltx_align_left" columnalign="left" ><mn >0.5</mn><mi
    >x</mi><msup ><mn
    >2</mn></msup></mtd><mtd class="ltx_align_left"
    columnalign="left" ><mtext >if </mtext><mo
    fence="false" rspace="0.167em" stretchy="false" >|</mo><mi
    >x</mi><mo fence="false" stretchy="false"
    >|</mo><mo lspace="0.167em" ><</mo><mn
    >1</mn><mtext >,</mtext></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left" ><mo
    fence="false" rspace="0.167em" stretchy="false" >|</mo><mi
    >x</mi><mo fence="false" stretchy="false"
    >|</mo><mo lspace="0em" >−</mo><mn
    >0.5</mn></mtd><mtd class="ltx_align_left"
    columnalign="left" ><mtext >otherwise</mtext><mtext
    >.</mtext></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\text{smooth}_{L_{1}}(x)=\left\{\begin{tabular}[]{ll}$0.5x^{2}><$\text{
    if }|x|<1$,\\ $|x|-0.5><$\text{ otherwise}$.\end{tabular}\right.</annotation></semantics></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that some papers restrict the sum to be over valid pixels in order to avoid
    outliers, or over regions of interests, *e.g.,* foreground or visible pixels [[44](#bib.bib44)].
  prefs: []
  type: TYPE_NORMAL
- en: '(3) The cross-entropy loss [[19](#bib.bib19), [36](#bib.bib36)]. It is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{1}^{6}=-\sum_{x}Q(d_{x},\hat{d}_{x})\log\left(P(x,\hat{d}_{x})\right).$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $P(x,\hat{d}_{x})$ is the likelihood, as computed by the network, of
    pixel $x$ having the disparity/depth $\hat{d}_{x}$. It is defined in [[19](#bib.bib19),
    [36](#bib.bib36)] as the 3-pixel error:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_math_unparsed" alttext="Q(d_{x},\hat{d}_{x})=\left\{\begin{tabular}[]{ll}$\lambda_{1}$&amp;\text{if
    } $d_{x}=\hat{d}_{x}$\\ $\lambda_{2}$&amp;\text{if } $&#124;d_{x}-\hat{d}_{x}&#124;=1$\\'
  prefs: []
  type: TYPE_NORMAL
- en: $\lambda_{3}$&amp;\text{if } $&#124;d_{x}-\hat{d}_{x}&#124;=2$\\
  prefs: []
  type: TYPE_NORMAL
- en: $0$&amp;\text{otherwise. }\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{tabular}\right." display="block"><semantics ><mrow ><mi
    >Q</mi><mrow ><mo stretchy="false" >(</mo><msub
    ><mi >d</mi><mi >x</mi></msub><mo
    >,</mo><msub ><mover accent="true" ><mi
    >d</mi><mo >^</mo></mover><mi
    >x</mi></msub><mo stretchy="false" >)</mo></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable
    class="ltx_guessed_headers" columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd class="ltx_th_row" columnalign="left"
    ><mi >λ</mi><msub ><mn
    >1</mn></msub></mtd><mtd class="ltx_align_left" columnalign="left"
    ><mtext >if </mtext><mi >d</mi><msub
    ><mi >x</mi></msub><mo >=</mo><mover
    accent="true" ><mi >d</mi><mo
    >^</mo></mover><msub ><mi
    >x</mi></msub></mtd></mtr><mtr ><mtd
    class="ltx_th_row" columnalign="left" ><mi >λ</mi><msub
    ><mn >2</mn></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mtext >if </mtext><mo
    fence="false" rspace="0.167em" stretchy="false" >&#124;</mo><mi
    >d</mi><msub ><mi >x</mi></msub><mo
    >−</mo><mover accent="true" ><mi
    >d</mi><mo >^</mo></mover><msub
    ><mi >x</mi></msub><mo fence="false"
    stretchy="false" >&#124;</mo><mo lspace="0.167em" >=</mo><mn
    >1</mn></mtd></mtr><mtr ><mtd class="ltx_th_row"
    columnalign="left" ><mi >λ</mi><msub
    ><mn >3</mn></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mtext >if </mtext><mo
    fence="false" rspace="0.167em" stretchy="false" >&#124;</mo><mi
    >d</mi><msub ><mi >x</mi></msub><mo
    >−</mo><mover accent="true" ><mi
    >d</mi><mo >^</mo></mover><msub
    ><mi >x</mi></msub><mo fence="false"
    stretchy="false" >&#124;</mo><mo lspace="0.167em" >=</mo><mn
    >2</mn></mtd></mtr><mtr ><mtd class="ltx_th_row"
    columnalign="left" ><mn >0</mn></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mtext >otherwise.</mtext></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >Q(d_{x},\hat{d}_{x})=\left\{\begin{tabular}[]{ll}$\lambda_{1}><\text{if
    } $d_{x}=\hat{d}_{x}$\\ $\lambda_{2}><\text{if } $&#124;d_{x}-\hat{d}_{x}&#124;=1$\\
    $\lambda_{3}><\text{if } $&#124;d_{x}-\hat{d}_{x}&#124;=2$\\ $0><\text{otherwise.
    }\\ \end{tabular}\right.</annotation></semantics></math> |  | (15) |
  prefs: []
  type: TYPE_NORMAL
- en: Luo *et al.* [[19](#bib.bib19)] set $\lambda_{1}=0.5$, $\lambda_{2}=0.2$, and
    $\lambda_{3}=0.05$.
  prefs: []
  type: TYPE_NORMAL
- en: '(4) The sub-pixel cross-entropy loss $\mathcal{L}_{1}^{7}$. This loss, introduced
    by Tulyakov *et al.* [[60](#bib.bib60)], enables faster convergence and better
    accuracy at the sub-pixel level. It is defined using a discretized Laplace distribution
    centered at the ground-truth disparity:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{Q(d_{x},\hat{d}_{x})=\frac{1}{Z}e^{-\frac{1}{b}&#124;d_{x}-\hat{d}_{x}&#124;},\text{
    }Z=\sum_{d}e^{-\frac{1}{b}&#124;d_{x}-d&#124;}.}$ |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: Here, $d_{x}$ is the ground-truth disparity at pixel $x$ and $\hat{d}_{x}$ is
    the estimated disparity at the same pixel.
  prefs: []
  type: TYPE_NORMAL
- en: '(5) The hinge loss criterion [[18](#bib.bib18), [21](#bib.bib21)]. It is computed
    by considering pairs of examples centered around the same image position where
    one example belongs to the positive and one to the negative class. Let $s_{+}$
    be the output of the network for the positive example, $s_{-}$ be the output of
    the network for the negative example, and let $m$, the margin, be a positive real
    number. The hinge loss for that pair of examples is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{1}^{8}=max(0,m+s_{-}-s_{+}).$ |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: It is zero when the similarity of the positive example is greater than the similarity
    of the negative example by at least the margin $m$, which is set to $0.2$ in [[18](#bib.bib18)].
  prefs: []
  type: TYPE_NORMAL
- en: '(6) The re-projection (inverse warping) loss. Obtaining 3D ground truth data
    is very expensive. To overcome this issue, some techniques measure the loss based
    on the re-projection error. The rational is that if the estimated disparity/depth
    map is as close as possible to the ground truth, then the discrepancy between
    the reference image and any of the other images but unprojected using the estimated
    depth map onto the reference image, is also minimized. It can be defined in terms
    of the photometric error [[98](#bib.bib98), [95](#bib.bib95)], also called per-pixel
    $L_{1}$ loss [[14](#bib.bib14)], or image reconstruction error [[29](#bib.bib29)].
    It is defined as the $L_{1}$ norm between the reference image $I_{ref}$ and $\tilde{I}_{t}$,
    which is $I_{t}$ but unwarped onto $I_{ref}$ using the camera parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{1}^{9}=\frac{1}{N}\sum_{x}\&#124;I_{ref}(x)-\tilde{I}_{t}(x)\&#124;_{1}.$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'It can also be defined using the distance between the features f of the reference
    image and the features $\tilde{\textbf{f}}_{t}$ of any of the other images but
    unwarped onto the view of the reference image using the camera parameters and
    the computed depth map [[25](#bib.bib25)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{1}^{10}=\frac{1}{N}\sum_{x}\&#124;\textbf{f}_{(}x)-\tilde{\textbf{f}}_{t}(x)\&#124;_{1}.$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: 'Other terms can be added to the re-projection loss. Examples include the $L_{1}$
    difference between the gradients of $I_{ref}$ and the gradient of $\tilde{I}_{t}$ [[29](#bib.bib29)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{1}^{11}=\frac{1}{N}\sum_{x}\&#124;\nabla I_{ref}(x)-\tilde{I}_{t}(x)\&#124;_{1},$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: and the structural dissimilarity between patches in $I_{ref}$ and in $\tilde{I}_{t}$ [[29](#bib.bib29),
    [99](#bib.bib99)]. We denote this loss by $\mathcal{L}_{1}^{12}$.
  prefs: []
  type: TYPE_NORMAL
- en: '(7) Matching loss. Some methods, *e.g.,* [[18](#bib.bib18), [20](#bib.bib20),
    [21](#bib.bib21)], train the feature matching network separately from the subsequent
    disparity computation and refinement blocks, using different loss functions. Chen
    *et al.* [[20](#bib.bib20)] use a loss that measures the $L_{2}$ distance between
    the predicted matching score and the ground-truth score:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{1}^{13}=\&#124;\text{predicted\_score}(x,d)-\text{label}(x,d)\&#124;,$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: where $\text{label}(x,d)\in\{0,1\}$ is the ground-truth label indicating whether
    the pixel $x(i,j)$ on the left image corresponds to the pixel $(i-d,j)$ on the
    right image, and $\text{predicted\_score}(x,d)$ is the predicted matching score
    for the same pair of pixels.
  prefs: []
  type: TYPE_NORMAL
- en: (8) The semantic loss. Some papers incorporate semantic cues, *e.g.,* segmentation [[25](#bib.bib25)]
    and edge [[31](#bib.bib31)] maps, to guide the depth/disparity estimation. These
    can be either provided at the outset, *e.g.,* estimated with a separate method
    as in [[31](#bib.bib31)], or estimated jointly with the depth/disparity map using
    the same network trained end-to-end. The latter case requires a semantic loss.
    For instance, Yang *et al.* [[25](#bib.bib25)], which use segmentation as semantics,
    define the semantic loss $\mathcal{L}_{1}^{13}$ as the distance between the classified
    warped maps and ground-truth labels. Song *et al.* [[31](#bib.bib31)], which use
    edge probability map as semantics, define the semantic loss as follows;
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{L}_{1}^{14}=\frac{1}{N}\sum_{x}\left\{&#124;\partial_{u}d_{x}&#124;e^{-&#124;\partial_{u}\xi_{x}&#124;}+&#124;\partial_{v}d_{x}&#124;e^{-&#124;\partial_{v}\xi_{x}&#124;}\right\},}$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: where $x=(u,v)$ and $\xi$ is the edge probability map.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 The regularization term
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In general, one can make many assumptions about the disparity/depth map and
    incorporate them into the regularization term of Equation ([7](#S5.E7 "In 5.2
    Loss functions ‣ 5 Training ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction")). Examples of constraints include: smoothness [[29](#bib.bib29)],
    left-right consistency [[29](#bib.bib29)], maximum depth [[29](#bib.bib29)], and
    scale-invariant gradient loss [[15](#bib.bib15)]. The regularization term can
    then be formed using a weighted sum of these losses.'
  prefs: []
  type: TYPE_NORMAL
- en: '(1) Smoothness. It can be measured using the magnitude of the first or second-order
    gradient of the estimated disparity/depth map. For instance, Yang *et al.* [[25](#bib.bib25)]
    used the $L_{1}$ norm of the first-order gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{L}_{2}^{1}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}d_{x})+(\nabla_{v}d_{x})\right\},x=(u,v).}$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: 'Here $\nabla$ is the gradient operator. Zhou *et al.* [[95](#bib.bib95)] and
    Vijayanarasimhan *et al.* [[100](#bib.bib100)] define smoothness as the $L_{2}$
    norm of the second-order gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{2}^{2}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}^{2}d_{x})^{2}+(\nabla_{v}^{2}d_{x})^{2}\right\}.$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: 'Zhong *et al.* [[29](#bib.bib29)] used the second-order gradient but weighted
    with the image’s second-order gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{L}_{2}^{3}=\frac{1}{N}\sum\left\{&#124;\nabla_{u}^{2}d_{x}&#124;e^{-&#124;\nabla_{u}^{2}I_{left}(x)&#124;}+&#124;\nabla_{v}^{2}d_{x}&#124;e^{-&#124;\nabla_{v}^{2}I_{left}(x)&#124;}\right\}.}$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, Tonioni *et al.* [[96](#bib.bib96)] define the smoothness at a pixel
    $x$ as the absolute difference between the disparity predicted at $x$ and those
    predicted at each pixel $y$ within a certain predefined neighborhood $\mathcal{N}_{x}$
    around the pixel $x$. This is then averaged over all pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{2}^{4}=\frac{1}{N}\sum_{x}\sum_{y\in\mathcal{N}_{x}}&#124;d_{x}-d_{y}&#124;.$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: '(2) Consistency. Zhong *et al.* [[29](#bib.bib29)] introduced the loop-consistency
    loss, which is constructed as follows; Consider the left image $I_{left}$ and
    the synthesized image $\tilde{I}_{left}$ obtained by warping the right image to
    the left image coordinate with the disparity map defined on the right image. A
    second synthesized left image $\tilde{\tilde{I}}$ is generated by warping the
    left image to the right image coordinates by using the disparities at the left
    and right images, respectively. The loop consistency loss consistency term is
    then defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{2}^{5}=&#124;\tilde{\tilde{I}}-I_{left}&#124;.$ |  | (27)
    |'
  prefs: []
  type: TYPE_TB
- en: Godard *et al.* [[63](#bib.bib63)] introduced the left-right consistency term,
    which attempts to make the left-view disparity map be equal to the projected right-view
    disparity map. It can be seen as a linear approximation of the loop consistency
    and is defined as follows;
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{2}^{6}=\frac{1}{N}\sum_{x}&#124;d_{x}-\tilde{d}_{x}&#124;,$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: where $\tilde{d}$ is the disparity at the right image but reprojected onto the
    coordinates of the left image.
  prefs: []
  type: TYPE_NORMAL
- en: '(3) Maximum-depth heuristic. There may be multiple warping functions that achieve
    similar warping loss, especially for textureless areas. To provide strong regularization
    in these areas, Zhong *et al.* [[29](#bib.bib29)] use the Maximum-Depth Heuristic
    (MDH) [[101](#bib.bib101)], which is defined as the sum of all depths/disparities:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{2}^{7}=\frac{1}{N}\sum_{x}&#124;d_{x}&#124;.$ |  | (29)
    |'
  prefs: []
  type: TYPE_TB
- en: '(4) Scale-invariant gradient loss [[15](#bib.bib15)], defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{2}^{8}=\sum_{h\in A}\sum_{x}\&#124;g_{h}[D](x)-g_{h}[\hat{D}](x)\&#124;_{2},$
    |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: where $A=\{1,2,4,8,16\}$, $x=(i,j)$, $f_{i,j}\equiv f(i,j)$, and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{g_{h}[f](i,j)=\left(\frac{f_{i+h,j}-f_{i,j}}{&#124;f_{i+h,j}-f_{i,j}&#124;},\frac{f_{i,j+h}-f_{i,j}}{&#124;f_{i,j+h}-f_{i,j}&#124;}\right)^{\top}.}$
    |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: 5.3 Degree of supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised methods for depth estimation, which have achieved promising results,
    rely on large quantities of ground truth depth data. However, obtaining ground-truth
    depth data, either manually or using traditional stereo matching algorithms or
    3D scanning devices, *e.g.,* Kinect, is extremely difficult and expensive, and
    is prune to noise and inaccuracies. Several mechanisms have been recently proposed
    in the literature to make the 3D supervision as light as possible. Below, we discuss
    the most important ones.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Supervision with stereo images
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Godard *et al.* [[63](#bib.bib63)] exploit the left-right consistency to perform
    unsupervised depth estimation from a monocular image. The approach is trained,
    without 3D supervision, using stereo pairs. At runtime, it only requires one input
    image and returns the disparity map from the same view as the input image. For
    this, the approach uses the left-right consistency loss of Equation ([28](#S5.E28
    "In 5.2.2 The regularization term ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey
    on Deep Learning Architectures for Image-based Depth Reconstruction")), which
    attempts to make the left-view disparity map be equal to the projected right-view
    disparity map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tonioni *et al.*[[96](#bib.bib96)] fine-tune pre-trained networks without any
    3D supervision by using stereo pairs. The idea is leverage on traditional stereo
    algorithms and state-of-the-art confidence measures in order to fine-tune a deep
    stereo model based on disparities provided by standard stereo algorithms that
    are deemed as highly reliable by the confidence measure. This is done by minimizing
    a loss function made out of two terms: a confidence-guided loss and a smoothing
    term.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that while stereo-based supervision does not require ground-truth 3D labels,
    these techniques usually rely on the availability of calibrated stereo pairs during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Supervision with camera’s aperture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Srinivasan *et al.* [[102](#bib.bib102)]’s approach uses as supervision the
    information provided by a camera’s aperture. It introduces two differentiable
    aperture rendering functions that use the input image and the predicted depths
    to simulate depth-of-field effects caused by real camera apertures. The depth
    estimation network is trained end-to-end to predict the scene depths that best
    explain these finite aperture images as defocus-blurred renderings of the input
    all-in-focus image.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Training with relative/ordinal depth annotation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'People, in general, are better at judging relative depth [[103](#bib.bib103)],
    *i.e.,* assessing whether a point $A$ is closer than point $B$. Chen *et al.* [[67](#bib.bib67)]
    introduced an algorithm for learning to estimate metric depth using only annotations
    of relative depths. In this approach each image is annotated with only the ordinal
    relation between a pair of pixels, *i.e.,* point $A$ is closer to point $B$, further
    than $B$, or it is hard to tell. To train the network, a ConvNet in this case,
    using such ordinal relations, Chen *et al.* [[67](#bib.bib67)] introduced an improved
    ranking loss, which encourages the predicted depth to agree with the ground-truth
    ordinal relations. It is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{1}^{15}(\hat{D},\Theta,W)=\sum_{i=1}{N}\omega_{k}\mathcal{L}_{k}(I,x_{k},y_{k},l_{k},\hat{d}),$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\omega_{k}$, $l_{k}$, and $\mathcal{L}_{k}$ are, respectively, the weight,
    the label, and loss of the $k-$th pair $(x_{k},y_{i})$ defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{L}_{k}=\left\{\begin{tabular}[]{ll}$\log(1+\text{exp}((-\hat{d}_{xk}+\hat{d}_{yk})l_{k})$,&amp;if
    $l_{k}\neq 0$,\\ $(\hat{d}_{xk}-\hat{d}_{yk})^{2}$,&amp;otherwise.\end{tabular}\right.}$
    |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: Xian *et al.* [[104](#bib.bib104)] showed that training with only one pair of
    ordinal relation for each image is not sufficient to get satisfactory results.
    They then extended this approach by annotating each image with 3K pairs and showed
    that they can achieve substantial improvement accuracy. The challenge, however,
    is how to cheaply get such large number of relative ordinal annotations. They
    propose to use optical flow maps from web stereo images. Note that at runtime,
    both methods take a single image of size $384\times 384$ and output a dense depth
    map of the same size.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4 Domain adaptation and transfer learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Supervised deep learning often suffers from the lack of sufficient training
    data. Also, when using range sensors, noise is often present and the measurements
    can be very sparse. Kuznietsov *et al.* [[105](#bib.bib105)] propose an approach
    to depth map prediction from monocular images that learns in a semi-supervised
    way. The ida is to use sparse ground-truth depth for supervised learning, while
    enforcing the deep network to produce photoconsistent dense depth maps in a stereo
    setup using a direct image alignment / reprojection loss.
  prefs: []
  type: TYPE_NORMAL
- en: While obtaining ground-truth depth annotations of real images is challenging
    and time consuming, synthetic images with their corresponding depth maps can be
    easily generated using computer graphics techniques. However, the domain of real
    images is different from the domain of graphics-generated images. Recently, several
    domain adaptation strategies have been proposed to solve this domain bias issue.
    These allow training on synthetic data and transfer what has been learned to the
    domain of real images.
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptation methods for depth estimation can be classified into two categories.
    Methods in the first category transform the data of one domain to look similar
    in style to the data in the other domain. For example, Atapour-Abarghoue *et al.* [[106](#bib.bib106)]
    proposed a two-staged approach. The first stage includes training a depth estimation
    model using synthetic data. The second stage is trained to transfer the style
    of synthetic images to real-world images. By doing so, the style of real images
    is first transformed to match the style of synthetic data and then fed into the
    depth estimation network, which has been trained on synthetic data. Zheng *et
    al.* [[107](#bib.bib107)] performed the opposite; it transforms the synthetic
    images to become more realistic and use them to train the depth estimation network.
    Guo *et al.* [[108](#bib.bib108)], on the other hand, trains a stereo matching
    network using synthetic data to predict occlusion maps and disparity maps of stereo
    image pairs. In a second step, a monocular depth estimation network is trained
    on real data by distilling the knowledge of the stereo network [[109](#bib.bib109)].
    These methods use adversarial learning.
  prefs: []
  type: TYPE_NORMAL
- en: Methods in the second class operate on the network architecture and the loss
    functions used for their training. Kundu *et al.* [[110](#bib.bib110)] introduced
    AdaDepth, an unsupervised mechanism for domain adaptation. The approach uses an
    encoder-decoder architecture of the form $\hat{D}=g\left(h(\textbf{I}_{s})\right)$.
    It is first trained, in a supervised manner, using synthetic data $\textbf{I}_{s}$.
    Let $g_{s}$ and $h_{s}$ be the decoding and encoding functions learned from synthetic
    data. Let $g_{t}$ and $h_{t}$ be the decoding and encoding functions that correspond
    to real data $\textbf{I}_{t}$. Kundu *et al.* [[110](#bib.bib110)] assume that
    $g_{t}=g_{s}=g$. Its goal is to match the distributions of the latent representations
    generated by $h_{s}$ and $h_{t}$. This is done by initializing the network with
    the weights that have been learned using synthetic data. It then uses adversarial
    learning to minimize an objective function that discriminates between $g_{t}(\textbf{I}_{t})$
    and $g_{s}(\textbf{I}_{s})$, and another objective function that discriminates
    between $\hat{0}pt_{s}$ and $g(h_{t}(\textbf{I}_{t}))$. The former ensures that
    real data, when fed to the encoder, are mapped to the same latent space as the
    one learned during training. The latter ensures that inferences through the corresponding
    transformation functions $g(h_{s}(\cdot))$ and $g(h_{t}(\cdot))$ are directed
    towards the same output density function.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion and comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section discusses some state-of-the-art techniques using quantitative and
    qualitative performance criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Evaluation metrics and criteria
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most commonly used quantitative metrics for evaluating the performance of
    a depth estimation algorithm include (the lower these metrics are the better);
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computation time at training and runtime. While one can afford large computation
    time during training, some applications may require realtime performance at runtime.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory footprint. In general deep neural networks have a large number of parameters.
    Some of them operate on volumes using 3D convolutions. This would require large
    memory storage, which can affect their performance at runtime.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The End-Point Error (EPE). Called also geometric error, it is defined as the
    distance between the ground truth $D$ and the predicted disparity/depth $\hat{D}$,
    *i.e.,* $EPE(D,\hat{D})=\|D-\hat{D}\|$. This metric has two variants: Avg-Noc
    and Avg-ll. The former is measured in non-occluded areas while the latter is measured
    over the entire image.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of Erroneous pixels (PE). It is defined as the percentage of pixels
    where the true and predicted disparity/depth differ with more than a predefined
    threshold $\epsilon$. Similar to the EPE, this error can be measured on non-occluded
    ares (Out-Noc) and over the entire image (Out-All).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bad pixel error (D1). It is defined as the percentage of disparity/depth
    errors below a threshold. This metric is computed in non-occluded (Noc) and in
    all pixels (All), in background (bg) and in foreground (fg) pixels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Absolute relative difference. It is defined as the average over all the image
    pixels of the $L_{1}$ distance between the groud-truth and the estimate depth/disparity,
    but scaled by the estimated depth/disparity:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{Abs rel. diff}=\frac{1}{N}\sum_{N}\frac{&#124;0pt_{i}-\hat{0}pt_{i}&#124;}{\hat{0}pt_{i}}.$
    |  | (34) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Squared relative difference. It is defined as the average over all the image
    pixels of the $L_{2}$ distance between the groud-truth and the estimate depth
    / disparity, but scaled by the estimated depth/disparity:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{Abs rel. diff}=\frac{1}{N}\sum_{N}\frac{&#124;0pt_{i}-\hat{0}pt_{i}&#124;^{2}}{\hat{0}pt_{i}}.$
    |  | (35) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The linear Root Mean Square Error. It is defined as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{RMSE(linear)}=\sqrt{\frac{1}{N}\sum_{N}{&#124;0pt_{i}-\hat{0}pt_{i}&#124;}^{2}}.$
    |  | (36) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The log Root Mean Square Error. It is defined as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{RMSE(log)}=\sqrt{\frac{1}{N}\sum_{N}{&#124;\log 0pt_{i}-\log\hat{0}pt_{i}&#124;}^{2}}.$
    |  | (37) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: The accuracy is generally evaluated using the following metrics (the higher
    these metrics are the better);
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum relative error. It is defined as the percentage of pixels $i$ such that
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\max\left(\frac{0pt_{i}}{\hat{0}pt_{i}},\frac{\hat{0}pt_{i}}{0pt_{i}}\right)<\epsilon,$
    |  | (38) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $\epsilon$ is a user-defined threshold. It is generally set to $1.25$,
    $1.25^{2}$, and $1.25^{3}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density. It is defined as the percentage of pixels for which depth has been
    estimated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition to these quantitative metrics, there are several qualitative aspects
    to consider. Examples include;
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Degree of 3D supervision. One important aspect of deep learning-based depth
    reconstruction methods is the degree of 3D supervision they require during training.
    In fact, while obtaining multiview stereo images is easy, obtaining their corresponding
    ground-truth depth maps and/or pixel-wise correspondences is quite challenging.
    As such, techniques that require minimal or no 3D supervision are usually preferred
    over those that require ground-truth depth maps during training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-to-end training. In general, the depth estimation pipeline is composed of
    multiple blocks. In methods, these blocks are trained separately. Others train
    them jointly in an end-to-end fashion. Some these techniques include a deep learning-based
    refinement module. Others directly regress the final high resolution map without
    additional post-processing or regularization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sub-pixel accuracy. In general, its is desirable to achieve sub-pixel accuracy
    without any additional post-processing or regularization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change in disparity range. This may require changing the network structure as
    well as re-training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will use these metrics and criteria to compare and discuss existing methods.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Pairwise stereo matching techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo matching techniques ‣ 6 Discussion
    and comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction") compares the properties and performance of deep learning-based
    depth estimation methods from stereo images. Below, we discuss some of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Performance comparison of deep learning-based stereo matching algorithms
    on the test set of KITTI 2015 benchmark (as of 2019/01/05). PE: percentage of
    erroneous pixels. EPE: End-Point Error. S1: the bad pixel error. Non-Occ: non-occluded
    pixels. All: all pixels. fg: foreground pixels. bg: background pixels. Noc: non-occluded
    pixels only. The bad pixel metric (D1) considers the disparity/depth at a pixel
    to be correctly estimated if the error is less than $3$ pixels or less than $5\%$
    of its value.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Description | Training | Avg D1 Non-Occ / Est | Avg D1 All / Est
    | Avg D1 Non-Occ / All | Avg D1 All / All | Time (s) | Environment |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 3D Sup | Loss | End-to-end | D1-fg | D1-bg | D1-all | D1-fg | D1-bg
    | D1-all | D1-fg | D1-bg | D1-all | D1-fg | D1-bg | D1-all |  |'
  prefs: []
  type: TYPE_TB
- en: '| MC-CNN Accr [[17](#bib.bib17)] | Raw disparity $+$ classic refinement | ✓
    | $\mathcal{L}_{1}^{13}$ | ✕ | $7.64$ | $2.48$ | $3.33$ | $8.88$ | $2.89$ | $3.89$
    | $7.64$ | $2.48$ | $3.33$ | $8.88$ | $2.89$ | $3.89$ | $67$ | Nvidia GTX Titan
    X (CUDA, Lua/Torch7) |'
  prefs: []
  type: TYPE_TB
- en: '| Luo *et al.* [[19](#bib.bib19)] | Raw disparity $+$ classic refinement |
    ✓ | $\mathcal{L}_{1}^{6}$ | ✕ | $7.44$ | $3.32$ | $4.00$ | $8.58$ | $3.73$ | $4.54$
    | $7.44$ | $3.32$ | $4.00$ | $8.58$ | $3.73$ | $4.54$ | $1$ | Nvidia GTX Titan
    X (Torch) |'
  prefs: []
  type: TYPE_TB
- en: '| Chen *et al.* [[20](#bib.bib20)] | Raw disparity $+$ classic refinement |
    ✓ | $\mathcal{L}_{1}^{13}$ | ✕ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| L-ResMatch [[21](#bib.bib21)] | Raw disparity $+$ confidence score $+$ classic
    refinement | ✓ |  | $\circ$ | $5.74$ | $2.35$ | $2.91$ | $6.95$ | $2.72$ | $3.42$
    | $5.74$ | $2.35$ | $2.91$ | $6.95$ | $2.72$ | $3.42$ | $48$ | Nvidia Titan-X
    |'
  prefs: []
  type: TYPE_TB
- en: '| Han *et al.*[[22](#bib.bib22)] | Matching network | ✓ | $\mathcal{L}_{1}^{6}$
    | ✕ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | Nvidia GTX Titan Xp |'
  prefs: []
  type: TYPE_TB
- en: '| Tulyakov *et al.*[[60](#bib.bib60)] | MC-CNN fast [[17](#bib.bib17)] + weakly-supervised
    learning |  | $-$ | ✕ | $9.42$ | $3.06$ | $4.11$ | $10.93$ | $3.78$ | $4.93$ |
    $9.42$ | $3.06$ | $4.11$ | $10.93$ | $3.78$ | $4.93$ | $1.35$ | 1 core 2.5 Ghz
    + K40 NVIDIA, Lua-Torch |'
  prefs: []
  type: TYPE_TB
- en: '| FlowNetCorr [[12](#bib.bib12)] | Refined disparity | ✓ | $\mathcal{L}_{1}^{1}$
    | ✓ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $1.12$ | Nvidia GTX Titan |'
  prefs: []
  type: TYPE_TB
- en: '| DispNetCorr [[13](#bib.bib13)] | Raw disparity | ✓ |  | $\circ$ | $3.72$
    | $4.11$ | $4.05$ | $4.41$ | $4.32$ | $4.34$ | $3.72$ | $4.11$ | $4.05$ | $4.41$
    | $4.32$ | $4.34$ | $0.06$ | Nvidia Titan-X |'
  prefs: []
  type: TYPE_TB
- en: '| Pang *et al.*[[23](#bib.bib23)] | Raw disparity | ✓ | $\mathcal{L}_{1}^{1}$
    | $\circ$ | $3.12$ | $2.32$ | $2.45$ | $3.59$ | $2.48$ | $2.67$ | $3.12$ | $2.32$
    | $2.45$ | $3.59$ | $2.48$ | $2.67$ | $0.47$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Yu *et al.*[[24](#bib.bib24)] | Raw disparity map | ✓ | $\mathcal{L}_{1}^{1}$
    | $\circ$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $5.32$ | $2.06$ | $2.32$ | $5.46$
    | $2.17$ | $2.79$ | $1.13$ | Nvidia 1080Ti |'
  prefs: []
  type: TYPE_TB
- en: '| Yang *et al.* [[25](#bib.bib25)] - supp. | Raw disparity | ✓ | $\mathcal{L}_{1}^{1}+\mathcal{L}_{1}^{13}+\mathcal{L}_{2}^{1}$
    | ✓ | $3.70$ | $1.76$ | $2.08$ | $4.07$ | $1.88$ | $2.25$ | $3.70$ | $1.76$ |
    $2.08$ | $4.07$ | $1.88$ | $2.25$ | $0.6$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Yang *et al.* [[25](#bib.bib25)] - unsup. | Raw disparity | ✕ | $\mathcal{L}_{1}^{9}+\mathcal{L}_{1}^{13}+\mathcal{L}_{2}^{1}$
    | ✓ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $7.70$ | $-$ | $-$ | $8.79$
    | $0.6$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liang *et al.* [[26](#bib.bib26)] | Refined disparity | ✓ | $\mathcal{L}_{1}^{1}$
    | ✓ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $2.76$ | $2.07$ | $2.19$ | $3.40$ |
    $2.25$ | $2.44$ | $0.12$ | Nvidia Titan-X |'
  prefs: []
  type: TYPE_TB
- en: '| Khamis *et al.* [[27](#bib.bib27)] | Raw disparity $+$ hierarchical refinement
    | ✓ | $\mathcal{L}_{1}^{4}$ with $\alpha=1,c=2$ | ✓ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $7.45$ | $4.30$ | $4.83$ | $0.015$ | Nvidia Titan-X
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gidaris & Komodakis [[47](#bib.bib47)] | Refinement only | ✓ | $\mathcal{L}_{1}^{1}$
    | $\circ$ | $4.87$ | $2.34$ | $2.76$ | $6.04$ | $2.58$ | $3.16$ | $4.87$ | $2.34$
    | $2.76$ | $6.04$ | $2.58$ | $3.16$ | $0.4$ | Nvidia Titan-X |'
  prefs: []
  type: TYPE_TB
- en: '| Chang & Chen [[28](#bib.bib28)] | Raw disparity | ✓ | $\mathcal{L}_{1}^{5}$
    | $\circ$ | $4.31$ | $1.71$ | $2.14$ | $4.62$ | $1.86$ | $2.32$ | $4.31$ | $1.71$
    | $2.14$ | $4.62$ | $1.86$ | $2.32$ | $0.41$ | Nvidia GTX Titan Xp |'
  prefs: []
  type: TYPE_TB
- en: '| Zhong *et al.*[[29](#bib.bib29)] | Raw disparity map | ✕ | $\alpha_{1}\mathcal{L}_{1}^{12}+\alpha_{2}\mathcal{L}_{1}^{9}+\alpha_{3}\mathcal{L}_{1}^{7}+\alpha_{4}\mathcal{L}_{2}^{3}+\alpha_{5}\mathcal{L}_{2}^{5}+\alpha_{6}\mathcal{L}_{2}^{7}$
    | $\circ$ | $6.13$ | $2.46$ | $3.06$ | $7.12$ | $2.86$ | $3.57$ | $6.13$ | $2.46$
    | $3.06$ | $7.12$ | $2.86$ | $3.57$ | $0.8$ | P100 |'
  prefs: []
  type: TYPE_TB
- en: '| Kendall *et al.*[[39](#bib.bib39)] | Refiend disparity map without refinement
    module | ✓ | $\mathcal{L}_{1}^{1}$ | ✓ | $5.58$ | $2.02$ | $2.61$ | $6.16$ | $2.21$
    | $2.87$ | $5.58$ | $2.02$ | $2.61$ | $6.16$ | $2.21$ | $2.87$ | $0.9$ | Nvidia
    GTX Titan X |'
  prefs: []
  type: TYPE_TB
- en: '| Standard SGM-Net [[30](#bib.bib30)] | Refinement with CNN-based SGM | ✓ |
    Weighted sum of path cost and neighbor cost | $\circ$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $7.44$ | $2.23$ | $3.09$ | $-$ | $-$ | $-$ | $67$ | Nvidia Titan-X
    |'
  prefs: []
  type: TYPE_TB
- en: '| Signed SGM-Net [[30](#bib.bib30)] | Refinement with CNN-based SGM | ✓ | Weighted
    sum of path cost and neighbor cost | $\circ$ | $7.43$ | $2.23$ | $3.09$ | $8.64$
    | $2.66$ | $3.66$ | $7.43$ | $2.23$ | $3.09$ | $8.64$ | $2.66$ | $3.66$ | $67$
    | Nvidia Titan-X |'
  prefs: []
  type: TYPE_TB
- en: '| Cheng *et al.*[[53](#bib.bib53)] | Refinement | ✓ | $\mathcal{L}_{1}^{1}$
    | $\circ$ | $2.67$ | $1.40$ | $1.61$ | $2.88$ | $1.51$ | $1.74$ | $2.67$ | $1.40$
    | $1.61$ | $2.88$ | $1.51$ | $1.74$ | $0.5$ | GPU @ 2.5 Ghz (C/C++) |'
  prefs: []
  type: TYPE_TB
- en: '| EdgeStereo [[31](#bib.bib31)] | Raw disparity | ✓ | $\displaystyle\sum_{sc=1}^{\text{nscales}}\left(\mathcal{L}_{1}^{1}+\alpha\mathcal{L}_{1}^{14}\right)_{sc}$
    | ✕ | $3.04$ | $1.70$ | $1.92$ | $3.39$ | $1.85$ | $2.10$ | $3.04$ | $1.70$ |
    $1.92$ | $3.39$ | $1.85$ | $2.10$ | $0.32$ | Nvidia GTX Titan Xp |'
  prefs: []
  type: TYPE_TB
- en: '| Tulyakov *et al.*[[32](#bib.bib32)] | Disparity with sub-pixel accuracy |
    ✓ | $\mathcal{L}_{1}^{7}$ | $\circ$ | $3.63$ | $2.09$ | $2.36$ | $4.05$ | $2.25$
    | $2.58$ | $3.63$ | $2.09$ | $2.36$ | $4.05$ | $2.25$ | $2.58$ | $0.5$ | 1 core
    @ 2.5 Ghz (Python) |'
  prefs: []
  type: TYPE_TB
- en: '| Jie *et al.*[[33](#bib.bib33)] | Refined disparity with DL | ✓ | $\mathcal{L}_{1}^{1}$
    | ✓ | $4.19$ | $2.23$ | $2.55$ | $5.42$ | $2.55$ | $3.03$ | $4.19$ | $2.23$ |
    $2.55$ | $5.42$ | $2.55$ | $3.03$ | $49.2$ | Nvidia GTX Titan X |'
  prefs: []
  type: TYPE_TB
- en: '| Seki *et al.*[[59](#bib.bib59)] | Raw disparity, confidence map, SGM-based
    refinement | ✓ | $\mathcal{L}_{1}^{6}$ | $\circ$ | $7.71$ | $2.27$ | $3.17$ |
    $8.74$ | $2.58$ | $3.61$ | $7.71$ | $2.27$ | $3.17$ | $8.74$ | $2.58$ | $3.61$
    | $68$ | Nvidia GTX Titan X |'
  prefs: []
  type: TYPE_TB
- en: '| Kuzmin *et al.*[[111](#bib.bib111)] | Only aggregated cost volume | ✓ | $\mathcal{L}_{1}^{6}$
    | $\circ$ | $10.11$ | $4.81$ | $5.68$ | $11.35$ | $5.32$ | $6.32$ | $10.11$ |
    $4.82$ | $5.69$ | $11.35$ | $5.34$ | $6.34$ | $0.03$ | GPU @ 2.5 Ghz (C/C++) |'
  prefs: []
  type: TYPE_TB
- en: '| Tonioni *et al.*[[96](#bib.bib96)] | Unsupervised adaptation - DispNetCorr1D [[13](#bib.bib13)]
    + CENSUS [[112](#bib.bib112)] | ✓ | $\mathcal{L}_{1}^{2}+\alpha\mathcal{L}_{2}^{4}$
    | NA | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.76$
    | $-$ | GPU @ 2.5 Ghz (Python) |'
  prefs: []
  type: TYPE_TB
- en: 6.2.1 Degree of supervision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most of the state-of-the-art methods require ground-truth depth maps to train
    their deep learning models. This is reflected in the loss functions they use to
    train the networks. For instance, Flynn *et al.* [[14](#bib.bib14)], Kendall *et
    al.* [[39](#bib.bib39)], Pang *et al.* [[23](#bib.bib23)], Cheng *et al.* [[53](#bib.bib53)],
    and Liang *et al.* [[26](#bib.bib26)] minimize the $L_{1}$ distance between the
    estimated disparity/depth and the ground truth (Equation ([9](#S5.E9 "In 5.2.1
    The data term ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction"))), while Luo *et al.* [[19](#bib.bib19)]
    minimize the cross-entropy loss of Equation ([14](#S5.E14 "In 5.2.1 The data term
    ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning Architectures for
    Image-based Depth Reconstruction")). Khamis *et al.* [[27](#bib.bib27)] used the
    same approach but by using the two-parameter robust function (Equation ([12](#S5.E12
    "In 5.2.1 The data term ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning
    Architectures for Image-based Depth Reconstruction"))). Chen *et al.* [[20](#bib.bib20)],
    which formulated the stereo matching problem as a classification problem, trained
    their network to classify whether pixel $x$ on the left image and pixel $x-d$
    on the right image are in correspondence (positive class) or not (negative class).
    The loss is then defined as the $L_{2}$ distance between the output of the network
    for the pixel pair $(x,x-d)$ and the ground-truth label (0 or 1) of this pair
    of pixels.
  prefs: []
  type: TYPE_NORMAL
- en: In general, obtaining ground-truth disparity/depth maps is very challenging.
    As such, techniques that do not require 3D supervision are more attractive. The
    key to training without 3D supervision is the use of loss functions that are based
    on the reprojection error, *e.g.,* Equation ([18](#S5.E18 "In 5.2.1 The data term
    ‣ 5.2 Loss functions ‣ 5 Training ‣ A Survey on Deep Learning Architectures for
    Image-based Depth Reconstruction")). This approach has been adopted in recent
    techniques, *e.g.,* Zhou *et al.* [[95](#bib.bib95)] and Yang *et al.* [[25](#bib.bib25)].
    One limitation of these techniques is that they assume that the camera parameters
    are known so that the unwarping or re-projection onto the coordinates of the other
    image can be calculated. Some techniques, *e.g.,* [[15](#bib.bib15)], assume that
    the camera parameters are unknown and regress them at the same time as depth/disparity
    in the same spirit as Structure from Motion (SfM) or visual SLAM.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Table [V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo matching techniques
    ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction"), methods that are trained with 3D supervision achieve a
    better performance at runtime than those without 3D supervision. For example,
    Yang *et al.* [[25](#bib.bib25)] evaluated their networks in both modes and showed
    that the supervised one achieved and average D1-all (All/All) of $2.25\%$ compared
    to $8.79\%$ for the unsupervised one.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Accuracy and disparity range
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Based on the metrics of Section [6](#S6 "6 Discussion and comparison ‣ A Survey
    on Deep Learning Architectures for Image-based Depth Reconstruction"), the unsupervised
    adaptation method of Tonioni *et al.* [[96](#bib.bib96)] boosted significantly
    the performance of DispNetCorr1D [[13](#bib.bib13)] (from $4.34$ on Avg D1 All/All
    (D1-all) to $0.76$). This suggests that such adaptation module can be used to
    boost the performance of the other methods.
  prefs: []
  type: TYPE_NORMAL
- en: Note that only a few methods could achieve sub-pixel accuracy. Examples include
    the approach of Tulyakov *et al.* [[32](#bib.bib32)], which uses the sub-pixel
    MAP approximation instead of the softargmin. Also, Tulyakov *et al.* [[32](#bib.bib32)]’s
    approach allows changing the disparity range at runtime without retraining the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Computation time and memory footprint
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Computation time and memory footprint, which in general are interrelated, are
    very important especially at runtime. Based on Table [V](#S6.T5 "TABLE V ‣ 6.2
    Pairwise stereo matching techniques ‣ 6 Discussion and comparison ‣ A Survey on
    Deep Learning Architectures for Image-based Depth Reconstruction"), we can distinguish
    three types of methods; Slow methods, average-speed methods, which produce a depth
    map in around one seconds, and fast methods, which require less than $0.1$ seconds
    to estimate a single depth map.
  prefs: []
  type: TYPE_NORMAL
- en: Slow methods require more than $40$ seconds to estimate one single depth map.
    There are multiple design aspects that make these method slow. For instance, some
    of them perform multiple forward passes as in [[30](#bib.bib30)]. Others either
    deepen the network by using a large number of layers including many fully connected
    layers, especially in the similarity computation block as in the MC-CNN Acc of [[17](#bib.bib17)]
    and the L-ResMatch of [[21](#bib.bib21)]), or use multiple subnetworks. Other
    methods estimate the depth map in a recurrent fashion, *e.g.,* by using multiple
    convLSTM blocks as in Jie *et al.* [[33](#bib.bib33)].
  prefs: []
  type: TYPE_NORMAL
- en: According to Table [V](#S6.T5 "TABLE V ‣ 6.2 Pairwise stereo matching techniques
    ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures for Image-based
    Depth Reconstruction"), the approach of Khamis *et al.* [[27](#bib.bib27)] is
    the fastest one as it produces, at run time, a disparity map in $15$ms, with a
    subpixel accuracy of $0.03$, which corresponds to an error of less than $3$cm
    at $3$m distance from the camera. In fact, Khamis *et al.* [[27](#bib.bib27)]
    observed that most of the time and compute is spent matching features at higher
    resolutions, while most of the performance gain comes from matching at lower resolutions.
    Thus, they compute the cost volume by matching features at low resolution. An
    efficient refinement module is then used to upsample the low resolution depth
    map to the input resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, t Mayer *et al.* [[13](#bib.bib13)] and Kendall *et al.* [[39](#bib.bib39)]
    can run very fast, with $0.06$s and $0.9$s consumed on a single Nvidia GTX Titan
    X GPU, respectively. However, disparity refinement is not included in these networks,
    which limits their performance.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Multiview stereo techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy and depth range ‣ 6.3 Multiview
    stereo techniques ‣ 6 Discussion and comparison ‣ A Survey on Deep Learning Architectures
    for Image-based Depth Reconstruction") compares the properties and performance
    of five deep learning-based multiview stereo reconstruction algorithms. Note that
    the related papers have reported performance results using different datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Degree of supervision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most of the methods described in Table [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy
    and depth range ‣ 6.3 Multiview stereo techniques ‣ 6 Discussion and comparison
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    are trained using 3D supervision. The only exception is the approach of Flynn
    *et al.* [[14](#bib.bib14)], which is trained using a posed set of images, *i.e.,*
    images with known camera parameters. At training, the approach takes a set of
    images, leaves one image out, and learns how to predict it from the remaining
    ones. The rational is that providing a set of posed images is much simpler than
    providing depth values at each pixel in every reference image.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Accuracy and depth range
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In terms of accuracy, the approach of Huang *et al.* [[36](#bib.bib36)] seems
    to outperform the state-of-the art, see Table [VI](#S6.T6 "TABLE VI ‣ 6.3.2 Accuracy
    and depth range ‣ 6.3 Multiview stereo techniques ‣ 6 Discussion and comparison
    ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction").
    However, since these methods have been evaluated on different datasets, it is
    not clear whether they would achieve the same level of accuracy on other datasets.
    As such, the accuracy results reported in Table [VI](#S6.T6 "TABLE VI ‣ 6.3.2
    Accuracy and depth range ‣ 6.3 Multiview stereo techniques ‣ 6 Discussion and
    comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth Reconstruction")
    are just indicative not conclusive.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, since most of the MVS methods rely on Plane Sweep Volumes or image/feature
    umprojection onto depth planes, the depth range needs to be set in advance. Changing
    the depth range and its discretization at runtime would require re-training the
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Performance comparison of deep learning-based multiview stereo matching
    algorithms on the KITTI 2015 benchmark. Accuracy and completeness refer, respectively,
    to the mean accuracy and mean completeness (the lower the better).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Description | Depth range | Training | Testing | Performance |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | #views | 3D Sup | Loss | End-to-end | Time (s) | Memory | #views
    | Time (s) | Memory | Dataset | Accuracy | Completeness |'
  prefs: []
  type: TYPE_TB
- en: '| [[14](#bib.bib14)] | Feature projection and unprojection. | $96$ | $4+1$
    | ✕ | $L_{1}$ color loss | ✓ | $-$ | $-$ | $4$ | $12$ min | $-$ | KITTI 2012 |
    $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[35](#bib.bib35)] | Multi-patch similarity | $256$ | variable | ✓ | softmax
    loss | $\circ$ | $-$ | $-$ | variable | $0.07$ | $-$ | DTU | $1.336$ | $2.126$
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[38](#bib.bib38)] | Feature unprojection. Volumetric reconstruction followed
    by projection to generate depth | $300$ | $5$ or $10$ | ✓ | $L_{1}$ loss | ✓ |
    $-$ | $-$ | $5$ or $10$ | $0.033$ | $-$ | ShapeNet | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[36](#bib.bib36)] | Plane Sweep Volumes | $100$ | $6$ | ✓ | class-balanced
    cross entropy | ✓ | $5$ days | $-$ | variable | $0.05$ for $32^{3}$ grid, $0.40$
    for $64^{3}$ grid | $-$ | ETH3D | $0.036$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[37](#bib.bib37)] | Feature unprojection | $256$ | $3$ | ✓ | $L_{1}$ | ✓
    | $-$ | $-$ | $5$ | $4.7$ per view | $-$ | DTU | $0.396$ | $0.527$ |'
  prefs: []
  type: TYPE_TB
- en: 6.4 Depth regression techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE VII: Comparison of some deep learning-based depth regression techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Description | Training | Runtime | KITTI 2012 | Make3D | NYUDv2
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | #views | 3D sup | #views | time | Performance ($\downarrow$ the better)
    | Accuracy ($\uparrow$ the better) | Performance ($\downarrow$ the better) | Performance
    ($\downarrow$ the better) | Accuracy ($\uparrow$ the better) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  | abs rel. | sqr rel. | RMSE | RMSE | $<1.25$ | $<1.25^{2}$
    | $<1.25^{3}$ | abs rel. | sqr rel. | RMSE | RMSE | abs rel. | sqr rel. | RMSE
    | RMSE | $<1.25$ | $<1.25^{2}$ | $<1.25^{3}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  | (lin) | (log) |  |  |  |  |  | (lin) | (log) |  |  |
    (lin) | (log) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Chakrabarti *et al.* [[70](#bib.bib70)] | Probability that model confidence
    and ambiguities | $1$ | ✓ | $1$ | $24$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $0.149$ | $0.118$ | $0.620$ | $0.205$ | $0.806$ | $0.958$
    | $0.987$ |'
  prefs: []
  type: TYPE_TB
- en: '| Kuznietsov *et al.* [[105](#bib.bib105)] | Supervised followed by unsupervised
    | $2$ | 3D + Stereo | $1$ | $-$ | $0.113$ | $0.741$ | $4.621$ | $0.189$ | 0.862
    | 0.960 | 0.986 | $0.157$ | $-$ | $3.97$ | $0.062$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zhan *et al.* [[113](#bib.bib113)] | visual odometry | Stereo seq. + cam.
    motion | ✕ | $1$ |  | $0.144$ | 1.391 | 5.869 | 0.241 | 0.803 | 0.928 | 0.969
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Eigen [[10](#bib.bib10)] | Multi-scale | $1$ | ✓ | $1$ | $-$ | $0.1904$ |
    $1.515$ | $7.156$ | $0.270$ | 0.702 | 0.890 | 0.958 | $-$ | $-$ | $-$ | $-$ |
    $0.214$ | $0.204$ | $0.877$ | $0.283$ | $0.614$ | $0.888$ | $0.972$ |'
  prefs: []
  type: TYPE_TB
- en: '| Eigen *et al.* [[2](#bib.bib2)] | VGG - multi-sclae CNN for depth, normals,
    and labeling | $1$ | ✓ | $1$ | $0.033$ |  |  |  |  | $-$ |  |  | $-$ | $-$ | $-$
    | $-$ | $0.158$ | $0.121$ | $0.641$ | $0.214$ | $0.769$ | $0.950$ | $0.988$ |'
  prefs: []
  type: TYPE_TB
- en: '| Fu *et al.* [[41](#bib.bib41)] | ResNet - Ordinal regression | $1$ | ✓ |
    $1$ | $-$ | $0.072$ | $0.307$ | $2.727$ | $0.120$ | $0.932$ | $0.984$ | $0.994$
    | $0.157$ | $-$ | $3.97$ | $0.062$ | $0.115$ | $-$ | $0.509$ | $0.051$ | $-$ |
    $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Gan *et al.* [[65](#bib.bib65)] | Uses Affinity, Vertical Pooling, and Label
    Enhancement | $1$ | ✓ | $1$ | $0.07$ | $0.098$ | $0.666$ | $3.933$ | $0.173$ |
    $0.890$ | $0.964$ | $0.985$ | $-$ | $-$ | $-$ | $-$ | $0.158$ | $-$ | $0.631$
    | $0.066$ | $0.756$ | $0.934$ | $0.980$ |'
  prefs: []
  type: TYPE_TB
- en: '| Garg [[3](#bib.bib3)] | variable-size input | $2$ | ✕ | $1$ | $-$ | $0.177$
    | $1.169$ | $5.285$ | $0.282$ | $0.727$ | $0.896$ | $0.958$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Godard [[63](#bib.bib63)] | Training with a stereo pair | $2$ (calibrated)
    | ✕ | $1$ | $-$ | $0.114$ | $0.898$ | $4.935$ | $0.206$ | 0.830 | 0.936 | 0.970
    | $0.535$ | $11.990$ | $11.513$ | $0.156$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Jiao [[74](#bib.bib74)] | $40$ categories - pay more attention to distant
    regions | $1$ | ✓ | $1$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $0.098$ |  | $0.329$ | $0.125$ | $0.917$ | $0.983$ | $0.996$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Laina [[64](#bib.bib64)] | VGG - feature map up-sampling | $1$ | ✓ | $1$
    | $0.055$ |  |  |  |  | $-$ |  |  | $0.176$ | $-$ | $4.6$ | $0.072$ | $0.194$
    | $-$ | $0.79$ | $0.083$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Laina [[64](#bib.bib64)] | ResNet - feature map up-sampling | $1$ | ✓ | $1$
    | $0.055$ |  |  |  |  | $-$ |  |  |  | $-$ |  |  | $0.127$ | $-$ | $0.573$ | $0.055$
    | $0.811$ | $0.953$ | $0.988$ |'
  prefs: []
  type: TYPE_TB
- en: '| Lee [[46](#bib.bib46)] | Split and merge | $1$ | ✓ | $1$ | $-$ |  |  |  |  |
    $-$ |  |  | $-$ | $-$ | $-$ | $-$ | $0.139$ | $0.096$ | $0.572$ | $0.193$ | $0.815$
    | $0.963$ | $0.991$ |'
  prefs: []
  type: TYPE_TB
- en: '| Li [[1](#bib.bib1)] | Multiscale patches, refinement with CRF | $1$ | ✓ |
    $1$ | $-$ |  |  |  |  | $-$ |  |  | $0.278$ | $-$ | $7.188$ | $-$ | $0.232$ |
    $-$ | $0.821$ | $-$ | $0.6395$ | $0.9003$ | $0.9741$ |'
  prefs: []
  type: TYPE_TB
- en: '| Qi [[48](#bib.bib48)] | Joint depth and normal maps | $1$ | ✓ | $1$ | $0.87$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.128$ |  |
    $0.569$ |  | $0.834$ | $0.960$ | $0.990$ |'
  prefs: []
  type: TYPE_TB
- en: '| Roy [[69](#bib.bib69)] | CNN + Random Forests | $1$ | ✓ | $1$ | $-$ |  |  |  |  |
    $-$ |  |  | $0.260$ | $-$ | $12.40$ | $0.119$ | $0.187$ | $-$ | $0.74$ | $-$ |
    $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Xian [[104](#bib.bib104)] | Training with $3$K ordinal relations per image
    | $1$ | $3$K ordinal | $1$ | $-$ |  |  |  |  | $-$ |  | $-$ | $-$ | $-$ | $-$
    | $-$ | $0.155$ | $-$ | $0.660$ | $0.066$ | $0.781$ | $0.950$ | $0.987$ |'
  prefs: []
  type: TYPE_TB
- en: '| Xu [[114](#bib.bib114)] | Integration with continuous CRF | $1$ | ✓ | $1$
    |  | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.184$ | $-$ | $4.386$ | $0.065$
    | $0.121$ | $-$ | $0.586$ | $0.052$ | $0.706$ | $0.925$ | $0.981$ |'
  prefs: []
  type: TYPE_TB
- en: '| Xu [[73](#bib.bib73)] | Joint depth estimation and scene parsing | $1$ |
    ✓ | $1$ | $-$ |  |  |  |  | $-$ |  |  | $-$ | $-$ | $-$ | $-$ | $0.214$ | $-$
    | $0.792$ | $-$ | $0.643$ | $0.902$ | $0.977$ |'
  prefs: []
  type: TYPE_TB
- en: '| Wang [[11](#bib.bib11)] | Depth and semantic prediction | $1$ | ✓ | $1$ |
    $-$ |  |  |  |  | $-$ |  |  | $-$ | $-$ | $-$ | $-$ | $0.220$ | $-$ | $0.745$
    | $0.262$ | $0.605$ | $0.890$ | $0.970$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.* [[58](#bib.bib58)] | Hierarchical guidance strategy for depth
    refinement | $1$ | ✓ | $1$ | $0.2$ | $0.136$ | $-$ | $4.310$ | $-$ | $0.833$ |
    $0.957$ | $0.987$ | $0.181$ | $-$ | $4.360$ | $-$ | $0.134$ | $-$ | $0.540$ |
    $-$ | $0.830$ | $0.964$ | $0.992$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.* [[52](#bib.bib52)] | ResNet50 - Joint segmentation and depth
    estimation | $1$ | ✓ | $1$ | $0.2$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $0.156$ | $-$ | $0.510$ | $0.187$ | $0.140$ | $-$ | $0.468$ | $-$ | $0.815$ |
    $0.962$ | $0.992$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zou [[72](#bib.bib72)] | Joint depth and flow | $2$ | ✕ | $2$ | $-$ | $0.150$
    | $1.124$ | $5.507$ | $0.223$ | $0.806$ | $0.933$ | $0.973$ | $0.331$ | $2.698-$
    | $0.416$ | $6.89$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou [[95](#bib.bib95)] | Depth + pose | $\geq 2$ | ✕ | $1$ | $-$ | $0.183$
    | $1.595$ | $6.709$ | $0.270$ | $0.734$ | $0.902$ | $0.959$ | $0.383$ | $5.321$
    | $10.47$ | $0.478$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou [[44](#bib.bib44)] | 3D-guided cycle consistency | $2$ | ✕ | $2$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Dosovitski [[12](#bib.bib12)] | Regression from calibrated stereo images
    | $2$ (calibrated) | ✓ | $2$ (calibrated) | $1.05$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[15](#bib.bib15)] | Regression from a pair of images | $2$ | ✓ | $2$ | $0.11$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pang [[23](#bib.bib23)] | Cascade residual learning | $2$ (calibrated) |
    ✓ | $2$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Ilg [[68](#bib.bib68)] | Extension of FlowNet [[12](#bib.bib12)] | $2$ |
    ✓ | $2$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Li [[1](#bib.bib1)] | Depth from multiscale patches, refinement with CRF
    | $1$ | ✓ | $1$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.278$ | $-$
    | $7.188$ | $-$ | $0.232$ | $-$ | $0.821$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Liu [[4](#bib.bib4)] | Refinement with continuous CRF | $1$ | ✓ | $1$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $0.314$ | $-$ | $0.314$ | $-$ | $0.230$
    | $-$ | $0.824$ | $-$ | $0.614$ | $0.883$ | $0.971$ |'
  prefs: []
  type: TYPE_TB
- en: '| Xie [[115](#bib.bib115)] | Predict one view from another using estimated
    depth | $2$ stereo | ✕ | $1$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Chen [[67](#bib.bib67)] | Training with one ordinal relation per image |
    $1$ | 1 ordinal | $1$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ |
    $-$ | $-$ | $-$ | $0.34$ | $0.42$ | $1.10$ | $0.38$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mayer [[13](#bib.bib13)] | A dataset for training | $1$ | ✓ | $0.06$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$ | $-$
    | $-$ | $-$ | $-$ | $-$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: Table [VII](#S6.T7 "TABLE VII ‣ 6.4 Depth regression techniques ‣ 6 Discussion
    and comparison ‣ A Survey on Deep Learning Architectures for Image-based Depth
    Reconstruction") summarizes the properties and compares the performance of some
    of the state-of-the-art methods for deep learning-based depth regression on KITTI2012,
    Make3D, and NYUDv2 datasets.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in this table, the methods of Jiao *et al.* [[74](#bib.bib74)] and
    Fu *et al.* [[41](#bib.bib41)] seem to achieve the best accuracy on NYUDv2 dataset.
    Their common property is the way they handle different depth ranges. In fact,
    previous methods treat near and far depth values equally. As such, most of them
    achieve good accuracy in near depth values but their accuracy drops for distant
    depth values. Jiao *et al.* [[74](#bib.bib74)] proposed a new loss function that
    pays more attention to distant depths. Fu *et al.* [[41](#bib.bib41)], on the
    other hand, formulated depth estimation as a classification problem. The depth
    range is first discretized into intervals. The network then learn to classify
    each image pixel into one of the depth intervals. However, instead of using uniform
    discretization, Fu *et al.* [[41](#bib.bib41)] used a spacing increasing discretization.
  prefs: []
  type: TYPE_NORMAL
- en: Another observation from Table LABEL:tab:performance_depthregerssion is that
    recent techniques trained without 3D supervision, *e.g.,* by using stereo images
    and re-projection loss, are becoming very competitive since their performances
    are close to techniques that use 3D supervision. Kuznietsov *et al.* [[105](#bib.bib105)]
    showed that the performance can be even further improved using semi-supervised
    techniques where the network is first trained with 3D supervision and then fine-tuned
    with stereo supervision. Also, training with ordinal relations seems to improve
    the performance of depth estimation. In fact, while the early work of Chen *et
    al.* [[67](#bib.bib67)], which used one ordinal relation per image, achieved relatively
    low performance, the recent work of Xian *et al.* [[104](#bib.bib104)], which
    used $3$K ordinal relations per image, achieved an accuracy that is very close
    to supervised techniques. Their main benefit is that, in general metric depths
    obtained by stereo matching or depth sensor are noisy. However, their corresponding
    ordinal depths are accurate. Thus, obtaining reliable ordinal depths for training
    is significantly easier.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, methods which jointly estimate depth and normals maps [[48](#bib.bib48)]
    or depth and semantic segmentation [[52](#bib.bib52)] outperform many methods
    that estimate depth alone. Their performance can be further improved by using
    the loss function of [[74](#bib.bib74)], which pays more attention to distant
    depths, or by using the spacing increasing depth range discretization of [[41](#bib.bib41)].
  prefs: []
  type: TYPE_NORMAL
- en: 7 Future research directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the extensive research undertaken in the past five years, deep learning-based
    depth reconstruction achieved promising results. The topic, however, is still
    in its infancy and further developments are yet to be expected. In this section,
    we present some of the current issues and highlight directions for future research.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input: Most of the current techniques do not handle high resolution input,
    require calibrated images, and cannot vary the number of input images at training
    and testing without re-training. The former is mainly due to the computation and
    memory requirements of most of the deep learning techniques. Developments in high
    performance computing can address this issue in the future. However, developing
    lighter deep architectures remains desirable especially if to be deployed in mobile
    and portable platforms.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accuracy: Although refinement modules can improve the resolution of the estimated
    depth maps, it is still small compared to the resolution of the images that can
    be recovered. As such, deep learning techniques find it difficult to recover small
    details, *e.g.,* vegetation and hair. Also, most of the techniques discretize
    the depth range. Although some methods can achieve sub-pixel accuracy, changing
    the depth range, and the discretization frequency, requires retraining the networks.
    Another issue is the accuracy, which, in general, varies for different depth ranges.
    Some of the recent works, *e.g.,*  [[74](#bib.bib74)], tried to address this problem,
    but it still remains an open and challenging problem since it is highly related
    to the data bias issue and the type of loss functions used to train the network.
    Accuracy of existing methods is also affected by complex scenarios, *e.g.,* occlusions
    and highly cluttered scenes, and objects with complex material properties.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance: Complex deep networks are very expensive in terms of memory requirements.
    Memory footprint is even a major issue when dealing with high resolution images
    and when aiming to reconstruct high resolution depth maps. While this can be mitigated
    by using multi-scale and part-based reconstruction techniques, it can result in
    high computation time.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training: Deep learning techniques rely heavily on the availability of training
    datasets annotated with ground-truth labels. Obtaining ground-truth labels for
    depth reconstruction is very expensive. Existing techniques mitigate this problem
    by either designing loss functions that do not require 3D annotations, or use
    domain adaptation and transfer learning strategies. The former, however, requires
    calibrated cameras. Domain adaptation techniques are recently attracting more
    attention since, with these techniques, one can train with synthetic data, which
    are easy to obtain, and real-world data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data bias and generalization: Most of the recent deep learning-based depth
    reconstruction techniques have been trained and tested on publicly available benchmarks.
    While this gives an indication on their performances, it is not clear yet how
    do they generalize and perform on completely unseen images, from a completely
    different category. Thus, we expect in the future to see the emergence of large
    datasets, similar to ImageNet but for 3D reconstruction. Developing self-adaptation
    techniques, *i.e.,* techniques that can adapt themselves to new scenarios in real
    time or with minimum supervision, is one promising direction for future research.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper provided a comprehensive survey of the recent developments in depth
    reconstruction using deep learning techniques. Despite its infancy, these techniques
    are achieving acceptable results, and some recent developments are even competing,
    in terms of accuracy of the results, with traditional techniques. We have seen
    that, since 2014, more than $100$ papers on the topic have been published in major
    computer vision and machine learning conferences and journals, and more new papers
    are being published even during the final stage of this submission. We believe
    that since 2014, we entered a new era where data-driven and machine learning techniques
    play a central role in image-based depth reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there are several related topics that have not been covered in this
    survey. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthesis of novel 2D views from one or multiple images, which use similar formulations
    as depth estimation, see for example [[6](#bib.bib6), [5](#bib.bib5), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM),
    which aim to recover at the same time depth maps and (relative) camera pose (or
    camera motion). Interested readers can refer to recent papers such as [[13](#bib.bib13),
    [100](#bib.bib100), [116](#bib.bib116)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image-based object reconstruction algorithms, which aim to recover the entire
    3D geometry of objects from one or multiple images.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While these topics are strongly related to depth estimation, they require a
    separate survey given the large amount of work that has been dedicated to them
    in the past 4 to 5 years.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] B. Li, C. Shen, Y. Dai, A. Van Den Hengel, and M. He, “Depth and surface
    normal estimation from monocular images using regression on deep features and
    hierarchical CRFs,” in *IEEE CVPR*, 2015, pp. 1119–1127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] D. Eigen and R. Fergus, “Predicting depth, surface normals and semantic
    labels with a common multi-scale convolutional architecture,” in *IEEE ICCV*,
    2015, pp. 2650–2658.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] R. Garg, V. K. BG, G. Carneiro, and I. Reid, “Unsupervised CNN for single
    view depth estimation: Geometry to the rescue,” in *ECCV*.   Springer, 2016, pp.
    740–756.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] F. Liu, C. Shen, G. Lin, and I. D. Reid, “Learning depth from single monocular
    images using deep convolutional neural fields,” *IEEE PAMI*, vol. 38, no. 10,
    pp. 2024–2039, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. Yang, S. E. Reed, M.-H. Yang, and H. Lee, “Weakly-supervised disentangling
    with recurrent transformations for 3D view synthesis,” in *NIPS*, 2015, pp. 1099–1107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum, “Deep convolutional
    inverse graphics network,” in *Advances in Neural Information Processing Systems*,
    2015, pp. 2539–2547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Tatarchenko, A. Dosovitskiy, and T. Brox, “Multi-view 3D models from
    single images with a convolutional network,” in *ECCV*.   Springer, 2016, pp.
    322–337.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros, “View synthesis
    by appearance flow,” in *ECCV*.   Springer, 2016, pp. 286–301.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C. Berg, “Transformation-grounded
    image generation network for novel 3D view synthesis,” in *IEEE CVPR*.   IEEE,
    2017, pp. 702–711.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a single
    image using a multi-scale deep network,” in *Advances in neural information processing
    systems*, 2014, pp. 2366–2374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] X. Wang, D. Fouhey, and A. Gupta, “Designing deep networks for surface
    normal estimation,” in *IEEE CVPR*, 2015, pp. 539–547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
    P. Van Der Smagt, D. Cremers, and T. Brox, “FlowNet: Learning optical flow with
    convolutional networks,” in *IEEE ICCV*, 2015, pp. 2758–2766.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
    and T. Brox, “A large dataset to train convolutional networks for disparity, optical
    flow, and scene flow estimation,” in *IEEE CVPR*, 2016, pp. 4040–4048.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Flynn, I. Neulander, J. Philbin, and N. Snavely, “DeepStereo: Learning
    to predict new views from the world’s imagery,” in *IEEE CVPR*, 2016, pp. 5515–5524.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and
    T. Brox, “Demon: Depth and motion network for learning monocular stereo,” in *IEEE
    CVPR*, vol. 5, 2017, p. 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame
    stereo correspondence algorithms,” *IJCV*, vol. 47, no. 1-3, pp. 7–42, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. Zbontar and Y. LeCun, “Computing the stereo matching cost with a convolutional
    neural network,” in *IEEE CVPR*, 2015, pp. 1592–1599.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] ——, “Stereo matching by training a convolutional neural network to compare
    image patches,” *Journal of Machine Learning Research*, vol. 17, no. 1-32, p. 2,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] W. Luo, A. G. Schwing, and R. Urtasun, “Efficient deep learning for stereo
    matching,” in *IEEE CVPR*, 2016, pp. 5695–5703.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] w. Chen, X. Sun, L. Wang, Y. Yu, and C. Huang, “A deep visual correspondence
    embedding model for stereo matching costs,” in *IEEE ICCV*, 2015, pp. 972–980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] A. Shaked and L. Wolf, “Improved stereo matching with constant highway
    networks and reflective confidence learning,” in *IEEE CVPR*, 2017, pp. 4641–4650.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg, “MatchNet: Unifying
    feature and metric learning for patch-based matching,” in *IEEE CVPR*, 2015, pp.
    3279–3286.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan, “Cascade residual learning:
    A two-stage convolutional neural network for stereo matching,” in *ICCV Workshops*,
    vol. 7, no. 8, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Yu, Y. Wang, Y. Wu, and Y. Jia, “Deep stereo matching with explicit
    cost aggregation sub-architecture,” *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia, “SegStereo: Exploiting
    Semantic Information for Disparity Estimation,” *arXiv preprint arXiv:1807.11699*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Z. Liang, Y. Feng, Y. G. H. L. W. Chen, and L. Q. L. Z. J. Zhang, “Learning
    for disparity estimation through feature constancy,” in *IEEE CVPR*, 2018, pp.
    2811–2820.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] S. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin, and S. Izadi,
    “Stereonet: Guided hierarchical refinement for real-time edge-aware depth prediction,”
    *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Chang and Y. Chen, “Pyramid Stereo Matching Network,” *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Y. Zhong, Y. Dai, and H. Li, “Self-supervised learning for stereo matching
    with self-improving ability,” *arXiv preprint arXiv:1709.00930*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] A. Seki and M. Pollefeys, “SGM-Nets: Semi-global matching with neural
    networks,” in *IEEE CVPR Workshops*, 2017, pp. 21–26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] X. Song, X. Zhao, H. Hu, and L. Fang, “EdgeStereo: A Context Integrated
    Residual Pyramid Network for Stereo Matching,” *ACCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. Tulyakov, A. Ivanov, and F. Fleuret, “Practical Deep Stereo (PDS):
    Toward applications-friendly deep stereo matching,” *NIPS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Z. Jie, P. Wang, Y. Ling, B. Zhao, Y. Wei, J. Feng, and W. Liu, “Left-right
    comparative recurrent model for stereo matching,” in *IEEE CVPR*, 2018, pp. 3838–3846.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Zagoruyko and N. Komodakis, “Learning to compare image patches via
    convolutional neural networks,” in *IEEE CVPR*, 2015, pp. 4353–4361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] W. Hartmann, S. Galliani, M. Havlena, L. Van Gool, and K. Schindler, “Learned
    multi-patch similarity,” in *IEEE ICCV*.   IEEE, 2017, pp. 1595–1603.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang, “DeepMVS:
    Learning Multi-view Stereopsis,” in *IEEE CVPR*, 2018, pp. 2821–2830.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan, “MVSNet: Depth Inference
    for Unstructured Multi-view Stereo,” *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] A. Kar, C. Häne, and J. Malik, “Learning a multi-view stereo machine,”
    in *NIPS*, 2017, pp. 364–375.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach,
    and A. Bry, “End-to-end learning of geometry and context for deep stereo regression,”
    *CoRR, vol. abs/1703.04309*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer,
    “Discriminative learning of deep convolutional feature point descriptors,” in
    *IEEE ICCV*, 2015, pp. 118–126.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao, “Deep ordinal regression
    network for monocular depth estimation,” in *IEEE CVPR*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *IEEE CVPR*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] H. Park and K. M. Lee, “Look wider to match image patches with convolutional
    neural networks,” *IEEE Signal Processing Letters*, vol. 24, no. 12, pp. 1788–1792,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang, and A. A. Efros, “Learning
    dense correspondence via 3D-guided cycle consistency,” in *IEEE CVPR*, 2016, pp.
    117–126.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] H. Hirschmuller, “Stereo processing by semiglobal matching and mutual
    information,” *IEEE PAMI*, vol. 30, no. 2, pp. 328–341, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J.-H. Lee, M. Heo, K.-R. Kim, and C.-S. Kim, “Single-image depth estimation
    based on fourier domain analysis,” in *IEEE CVPR*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] S. Gidaris and N. Komodakis, “Detect, replace, refine: Deep structured
    prediction for pixel wise labeling,” in *Proc. of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017, pp. 5248–5257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] X. Qi, R. Liao, Z. Liu, R. Urtasun, and J. Jia, “GeoNet: Geometric Neural
    Network for Joint Depth and Surface Normal Estimation,” in *IEEE CVPR*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle, V. Tankovich,
    M. Schoenberg, S. Izadi, T. Funkhouser, and S. Fanello, “ActiveStereoNet: end-to-end
    self-supervised learning for active stereo systems,” *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] S. Liu, S. De Mello, J. Gu, G. Zhong, M.-H. Yang, and J. Kautz, “Learning
    affinity via spatial propagation networks,” in *NIPS*, 2017, pp. 1520–1530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] J. Jeon and S. Lee, “Reconstruction-based pairwise depth dataset for depth
    image enhancement using cnn,” in *ECCV*, September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Z. Zhang, Z. Cui, C. Xu, Z. Jie, X. Li, and J. Yang, “Joint task-recursive
    learning for semantic segmentation and depth estimation,” in *ECCV*, September
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] X. Cheng, P. Wang, and R. Yang, “Depth estimation via affinity learned
    with convolutional spatial propagation network,” in *ECCV*, September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] T. Brox and J. Malik, “Large displacement optical flow: descriptor matching
    in variational motion estimation,” *IEEE PAMI*, vol. 33, no. 3, pp. 500–513, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] P. Krähenbühl and V. Koltun, “Efficient inference in fully connected crfs
    with gaussian edge potentials,” in *NIPS*, 2011, pp. 109–117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] X. Sun, X. Mei, S. Jiao, M. Zhou, Z. Liu, and H. Wang, “Real-time local
    stereo via edge-aware disparity propagation,” *Pattern Recognition Letters*, vol. 49,
    pp. 201–206, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Odena, V. Dumoulin, and C. Olah, “Deconvolution and checkerboard artifacts,”
    *Distill*, vol. 1, no. 10, p. e3, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Z. Zhang, C. Xu, J. Yang, Y. Tai, and L. Chen, “Deep hierarchical guidance
    and regularization learning for end-to-end depth estimation,” *Pattern Recognition*,
    vol. 83, pp. 430–442, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Seki and M. Pollefeys, “Patch based confidence prediction for dense
    disparity map,” in *BMVC*, vol. 2, no. 3, 2016, p. 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] S. Tulyakov, A. Ivanov, and F. Fleuret, “Weakly supervised learning of
    deep metrics for stereo reconstruction,” in *IEEE ICCV*, no. EPFL-CONF-233588,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] P. Knöbelreiter, C. Reinbacher, A. Shekhovtsov, and T. Pock, “End-to-end
    training of hybrid cnn-crf models for stereo,” in *IEEE CVPR*.   IEEE, 2017, pp.
    1456–1465.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang, “SurfaceNet: an end-to-end
    3D neural network for multiview stereopsis,” *IEEE ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth
    estimation with left-right consistency,” in *CVPR*, vol. 2, no. 6, 2017, p. 7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab, “Deeper
    depth prediction with fully convolutional residual networks,” in *3D Vision (3DV)*.   IEEE,
    2016, pp. 239–248.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Y. Gan, X. Xu, W. Sun, and L. Lin, “Monocular depth estimation with affinity,
    vertical pooling, and label enhancement,” in *ECCV*, September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Z. Chen, V. Badrinarayanan, G. Drozdov, and A. Rabinovich, “Estimating
    depth from rgb and sparse sensing,” in *ECCV*, September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] W. Chen, Z. Fu, D. Yang, and J. Deng, “Single-image depth perception in
    the wild,” in *Advances in Neural Information Processing Systems*, 2016, pp. 730–738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox, “Flownet
    2.0: Evolution of optical flow estimation with deep networks,” in *IEEE CVPR*,
    vol. 2, 2017, p. 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. Roy and S. Todorovic, “Monocular depth estimation using neural regression
    forest,” in *IEEE CVPR*, 2016, pp. 5506–5514.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] A. Chakrabarti, J. Shao, and G. Shakhnarovich, “Depth from a single image
    by harmonizing overcomplete local network predictions,” in *Advances in Neural
    Information Processing Systems*, 2016, pp. 2658–2666.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L. Yuille, “Towards
    unified depth and semantic prediction from a single image,” in *IEEE CVPR*, 2015,
    pp. 2800–2809.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Y. Zou, Z. Luo, and J.-B. Huang, “DF-Net: Unsupervised Joint Learning
    of Depth and Flow using Cross-Task Consistency,” in *ECCV*, September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] D. Xu, W. Wang, H. Tang, H. Liu, N. Sebe, and E. Ricci, “Structured attention
    guided convolutional neural fields for monocular depth estimation,” in *The IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. Jiao, Y. Cao, Y. Song, and R. Lau, “Look deeper into depth: Monocular
    depth estimation with semantic booster and attention-driven loss,” in *ECCV*,
    September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] H. Laga, Y. Guo, H. Tabia, R. B. Fisher, and M. Bennamoun, *3D Shape Analysis:
    Fundamentals, Theory, and Applications*.   John Wiley & Sons, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *IEEE CVPR*, 2016, pp. 3213–3223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] M. Menze and A. Geiger, “Object scene flow for autonomous vehicles,” in
    *IEEE CVPR*, 2015, pp. 3061–3070.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *IEEE CVPR*.   IEEE, 2012, pp. 3354–3361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A naturalistic
    open source movie for optical flow evaluation,” in *ECCV*.   Springer, 2012, pp.
    611–625.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] J. Xiao, A. Owens, and A. Torralba, “Sun3d: A database of big spaces reconstructed
    using sfm and object labels,” in *IEEE ICCV*, 2013, pp. 1625–1632.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
    and support inference from rgbd images,” *ECCV*, pp. 746–760, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A benchmark
    for the evaluation of rgb-d slam systems,” in *IEEE/RSJ IROS*, 2012, pp. 573–580.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] T. Schöps, J. L. Schönberger, S. Galliani, T. Sattler, K. Schindler, M. Pollefeys,
    and A. Geiger, “A multi-view stereo benchmark with high-resolution images and
    multi-camera videos,” in *IEEE CVPR*, vol. 2017, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl, “Large-scale
    data for multiple-view stereopsis,” *IJCV*, vol. 120, no. 2, pp. 153–168, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] A. Saxena, M. Sun, and A. Y. Ng, “Make3d: Learning 3d scene structure
    from a single still image,” *IEEE PAMI*, vol. 31, no. 5, pp. 824–840, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Z. Li and N. Snavely, “Megadepth: Learning single-view depth prediction
    from internet photos,” in *IEEE CVPR*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] H. Laga, Q. Xie, I. H. Jermyn, A. Srivastava *et al.*, “Numerical inversion
    of srnf maps for elastic shape analysis of genus-zero surfaces,” *IEEE transactions
    on pattern analysis and machine intelligence*, vol. 39, no. 12, pp. 2451–2464,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] I. H. Jermyn, S. Kurtek, H. Laga, and A. Srivastava, “Elastic shape analysis
    of three-dimensional objects,” *Synthesis Lectures on Computer Vision*, vol. 12,
    no. 1, pp. 1–185, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] G. Wang, H. Laga, N. Xie, J. Jia, and H. Tabia, “The shape space of 3d
    botanical tree models,” *ACM Transactions on Graphics (TOG)*, vol. 37, no. 1,
    p. 7, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, “Sun database:
    Large-scale scene recognition from abbey to zoo,” in *IEEE CVPR*.   IEEE, 2010,
    pp. 3485–3492.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su *et al.*, “Shapenet: An information-rich 3D model repository,”
    *arXiv preprint arXiv:1512.03012*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3D
    shapenets: A deep representation for volumetric shapes,” in *IEEE CVPR*, 2015,
    pp. 1912–1920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. J. Lim, H. Pirsiavash, and A. Torralba, “Parsing ikea objects: Fine
    pose estimation,” in *IEEE ICCV*, 2013, pp. 2992–2999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Y. Xiang, R. Mottaghi, and S. Savarese, “Beyond pascal: A benchmark for
    3d object detection in the wild,” in *IEEE WACV*.   IEEE, 2014, pp. 75–82.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised learning
    of depth and ego-motion from video,” in *IEEE CVPR*, vol. 2, no. 6, 2017, p. 7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A. Tonioni, M. Poggi, S. Mattoccia, and L. Di Stefano, “Unsupervised adaptation
    for deep stereo,” in *IEEE ICCV*.   IEEE, 2017, pp. 1614–1622.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. T. Barron, “A more general robust loss function,” *arXiv preprint arXiv:1701.03077*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Bai, W. Luo, K. Kundu, and R. Urtasun, “Exploiting semantic information
    and deep matching for optical flow,” in *ECCV*.   Springer, 2016, pp. 154–170.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE transactions
    on image processing*, vol. 13, no. 4, pp. 600–612, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki,
    “Sfm-net: Learning of structure and motion from video,” *arXiv preprint arXiv:1704.07804*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] M. Perriollat, R. Hartley, and A. Bartoli, “Monocular template-based
    reconstruction of inextensible surfaces,” *IJCV*, vol. 95, no. 2, pp. 124–137,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] P. P. Srinivasan, R. Garg, N. Wadhwa, R. Ng, and J. T. Barron, “Aperture
    supervision for monocular depth estimation,” in *IEEE CVPR*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] J. T. Todd and J. F. Norman, “The visual perception of 3-d shape from
    multiple cues: Are observers capable of perceiving metric structure?” *Perception
    & Psychophysics*, vol. 65, no. 1, pp. 31–47, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, and Z. Luo, “Monocular
    relative depth perception with web stereo data supervision,” in *IEEE CVPR*, June
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Y. Kuznietsov, J. Stuckler, and B. Leibe, “Semi-supervised deep learning
    for monocular depth map prediction,” in *IEEE CVPR*, 2017, pp. 6647–6655.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] A. Atapour-Abarghouei and T. P. Breckon, “Real-time monocular depth estimation
    using synthetic data with domain adaptation via image style transfer,” in *IEEE
    CVPR*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] C. Zheng, T.-J. Cham, and J. Cai, “T2net: Synthetic-to-realistic translation
    for solving single-image depth estimation tasks,” in *ECCV*, September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] X. Guo, H. Li, S. Yi, J. Ren, and X. Wang, “Learning monocular depth
    by distilling cross-domain stereo networks,” in *ECCV*, September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv preprint arXiv:1503.02531*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] J. Nath Kundu, P. Krishna Uppala, A. Pahuja, and R. Venkatesh Babu, “Adadepth:
    Unsupervised content congruent adaptation for depth estimation,” in *IEEE CVPR*,
    June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] A. Kuzmin, D. Mikushin, and V. Lempitsky, “End-to-end learning of cost-volume
    aggregation for real-time dense stereo,” in *International Workshop on Machine
    Learning for Signal Processing (MLSP)*.   IEEE, 2017, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] R. Zabih and J. Woodfill, “Non-parametric local transforms for computing
    visual correspondence,” in *ECCV*.   Springer, 1994, pp. 151–158.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] H. Zhan, R. Garg, C. Saroj Weerasekera, K. Li, H. Agarwal, and I. Reid,
    “Unsupervised learning of monocular depth estimation and visual odometry with
    deep feature reconstruction,” in *IEEE CVPR*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] D. Xu, E. Ricci, W. Ouyang, X. Wang, and N. Sebe, “Multi-scale continuous
    CRFs as sequential deep networks for monocular depth estimation,” in *IEEE CVPR*,
    2017, pp. 5354–5362.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] J. Xie, R. Girshick, and A. Farhadi, “Deep3d: Fully automatic 2D-to-3D
    video conversion with deep convolutional neural networks,” in *ECCV*.   Springer,
    2016, pp. 842–857.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] R. Wang, J.-M. Frahm, and S. M. Pizer, “Recurrent neural network for
    learning densedepth and ego-motion from video,” *arXiv preprint arXiv:1805.06558*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
