- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:07:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 20:07:08'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1811.01238] Deep Learning based Computer-Aided Diagnosis Systems for Diabetic
    Retinopathy: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1811.01238] 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1811.01238](https://ar5iv.labs.arxiv.org/html/1811.01238)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1811.01238](https://ar5iv.labs.arxiv.org/html/1811.01238)
- en: '¹¹institutetext: Community College, Imam Abdulrahman Bin Faisal University,
    Dammam, Saudi Arabia'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹institutetext: 社区学院，沙特阿拉伯达曼伊曼·阿卜杜勒拉赫曼·本·费萨尔大学'
- en: Computer and Information Science College, King Saud University, Riyadh, Saudi
    Arabia
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机与信息科学学院，沙特阿拉伯利雅得国王苏德大学
- en: Department of Ophthalmology, College of Medicine, Princess Nourah bint Abdulrahman
    University, Riyadh, Saudi Arabia
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 眼科，医学院，沙特阿拉伯利雅得努拉公主大学
- en: Vitreoretinal Diseases and Surgery, King Khalid Eye Specialist Hospital, Riyadh,
    Saudi Arabia
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 玻璃体视网膜疾病与手术，沙特阿拉伯利雅得国王哈立德眼科医院
- en: '¹¹email: norah.m.asiri@outlook.com'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹email: norah.m.asiri@outlook.com'
- en: '¹¹email: mhussian@ksu.edu.sa'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹email: mhussian@ksu.edu.sa'
- en: '¹¹email: ffaladel@pnu.edu.sa'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹email: ffaladel@pnu.edu.sa'
- en: '¹¹email: nzaidi@kkesh.med.sa'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹email: nzaidi@kkesh.med.sa'
- en: 'Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述
- en: Norah Asiri 1122    Muhammad Hussain 22    Fadwa Al Adel 33    Nazih Alzaidi
    4411223344
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 诺拉·阿西里 1122    穆罕默德·侯赛因 22    法德瓦·阿尔·阿德尔 33    纳齐赫·阿尔扎伊迪 4411223344
- en: Abstract
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Diabetic retinopathy (DR) results in vision loss if not treated early. A computer-aided
    diagnosis (CAD) system based on retinal fundus images is an efficient and effective
    method for early DR diagnosis and assisting experts. A computer-aided diagnosis
    (CAD) system involves various stages like detection, segmentation and classification
    of lesions in fundus images. Many traditional machine-learning (ML) techniques
    based on hand-engineered features have been introduced. The recent emergence of
    deep learning (DL) and its decisive victory over traditional ML methods for various
    applications motivated the researchers to employ it for DR diagnosis, and many
    deep-learning-based methods have been introduced. In this paper, we review these
    methods, highlighting their pros and cons. In addition, we point out the challenges
    to be addressed in designing and learning about efficient, effective and robust
    deep-learning algorithms for various problems in DR diagnosis and draw attention
    to directions for future research.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病视网膜病变（DR）如果不及时治疗会导致视力丧失。基于视网膜底部图像的计算机辅助诊断（CAD）系统是一种有效且高效的早期DR诊断方法，并可以辅助专家。计算机辅助诊断（CAD）系统涉及检测、分割和分类视网膜底部图像中的病变等多个阶段。已引入许多基于手工设计特征的传统机器学习（ML）技术。深度学习（DL）的最新出现及其在各种应用中对传统ML方法的决定性胜利，激励了研究人员将其应用于DR诊断，并引入了许多基于深度学习的方法。在本文中，我们回顾了这些方法，突出了它们的优缺点。此外，我们指出了在设计和学习高效、有效且鲁棒的深度学习算法以应对DR诊断中的各种问题时需要解决的挑战，并引起对未来研究方向的关注。
- en: 'Keywords:'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Diabetic Retinpoathy, Lesion, Exudate, Macula, Diabetic Macular Edema, Optic
    Disc, Microaneurysms, Hemorrhages, CNN, Autoencoder, RNN, DBN
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病视网膜病变，病变，渗出物，黄斑，糖尿病性黄斑水肿，视盘，微动脉瘤，出血，CNN，自编码器，RNN，DBN
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Diabetic retinopathy (DR) is one of the main causes of blindness among the working-age
    population. It is one of the most feared complications of diabetes. The fundamental
    problem of DR is that it becomes incurable at advanced stages, so early diagnosis
    is important. However, this involves remarkable difficulty in the health care
    system due to a large number of potential patients and the small number of experienced
    technicians. This has motivated the need to develop automated diagnosis systems
    to assist in early diagnosis of DR. Several attempts have been made in this direction,
    and several approaches based on hand-engineered features have been proposed, which
    have shown promising efficiency in recognizing DR regions in retinal fundus images.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病视网膜病变（DR）是工作年龄人群失明的主要原因之一。它是糖尿病最令人担忧的并发症之一。DR的根本问题在于它在晚期阶段变得不可治愈，因此早期诊断非常重要。然而，由于潜在患者数量庞大和经验丰富的技术人员稀少，这给医疗系统带来了极大难度。这促使了开发自动化诊断系统以协助早期诊断DR的需求。在这方面已有若干尝试，并提出了基于手工设计特征的几种方法，这些方法在识别视网膜底部图像中的DR区域方面表现出了良好的效率。
- en: Hand-engineered features are commonly used with traditional machine-learning
    (ML) methods for DR diagnosis. Different surveys have reviewed these traditional
    methods [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)]. For example, Mookiah et al. [[1](#bib.bib1)],
    Mansour [[4](#bib.bib4)] categorized DR diagnosis according to the adopted methodologies,
    such as mathematical morphology, retinal lesion tracking, thresholding and deformable
    models, clustering-based models, matched filtering models and hybrid approaches.
    Faust et al. [[2](#bib.bib2)] reviewed algorithms that extract lesion features
    from fundus images, such as the blood vessel area, exudes, hemorrhages, microaneurysms
    and texture. Joshi and Karule [[3](#bib.bib3)] reviewed the early research on
    exudate detection. Almotiri et al. [[5](#bib.bib5)] provided an overview of algorithms
    to segment retinal vessels. Almazroa et al. [[6](#bib.bib6)] and Thakur and Juneja
    [[7](#bib.bib7)] reviewed several methods for optic disc segmentation and diagnosis
    of glaucoma. However, expert knowledge is a prerequisite for hand-engineered features,
    and choosing the appropriate features requires intensive investigation of various
    options and tedious parameter settings. Moreover, techniques based on hand-engineered
    features do not generalize well.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 手工设计的特征通常与传统的机器学习（ML）方法一起用于糖尿病视网膜病变（DR）诊断。不同的调查已审查了这些传统方法[[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]。例如，Mookiah
    等人[[1](#bib.bib1)]、Mansour [[4](#bib.bib4)] 根据采用的方法对DR诊断进行了分类，如数学形态学、视网膜病变跟踪、阈值处理和可变形模型、基于聚类的模型、匹配滤波模型和混合方法。Faust
    等人[[2](#bib.bib2)] 综述了从眼底图像中提取病变特征的算法，如血管区域、渗出物、出血、微动脉瘤和纹理。Joshi 和 Karule [[3](#bib.bib3)]
    回顾了早期的渗出物检测研究。Almotiri 等人[[5](#bib.bib5)] 提供了分割视网膜血管的算法概述。Almazroa 等人[[6](#bib.bib6)]
    和 Thakur 及 Juneja [[7](#bib.bib7)] 回顾了几种视盘分割和青光眼诊断的方法。然而，手工设计特征的前提是专家知识，选择合适的特征需要对各种选项进行深入调查和繁琐的参数设置。此外，基于手工设计特征的方法泛化能力较差。
- en: 'In recent years, the availability of huge datasets and the tremendous computing
    power offered by graphics processing units (GPUs) have motivated research on deep-learning
    algorithms, which have shown outstanding performance in various computer vision
    tasks and have gained a decisive victory over traditional hand-engineered-based
    methods. Many deep-learning (DL)-based algorithms have also been developed for
    various tasks to analyze retinal fundus images to develop automatic computer-aided
    diagnosis systems for DR. This paper reviews the latest DL algorithms used in
    DR detection, highlighting the contributions and challenges of recent research
    papers. First, we provide an overview of various DL approaches and then review
    the DL-based techniques for DR diagnosis. Finally, we summarize future directions,
    gaps and challenges in designing and training deep neural networks for DR diagnosis.
    The remainder of the paper is organized as follows: automatic detection of DR,
    types of lesions, DR stages, grading of DR, detection tasks and the detection
    framework are presented in Section [2](#S2 "2 Automatic Diabetic Retinopathy Detection
    ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey"). After that, public-domain DR datasets and common performance metrics
    are briefly described in Section [3](#S3 "3 Datasets and Performance Metrics ‣
    Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey"). An overview of DL techniques used in DR diagnosis is given in Section
    [4](#S4 "4 Overview of Deep Learning ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey"). The most recent research based on
    DL for DR diagnosis are reviewed in Section [5](#S5 "5 Literature Survey ‣ Deep
    Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey").
    This research is discussed in Section [6](#S6 "6 Discussion ‣ Deep Learning based
    Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey"). Finally,
    research gaps and future directions with conclusion are presented in Sections
    [7](#S7 "7 Gaps and Future Directions ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey") and [8](#S8 "8 Conclusion ‣ Deep
    Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，大规模数据集的可用性和图形处理单元（GPUs）提供的强大计算能力促使了对深度学习算法的研究，这些算法在各种计算机视觉任务中表现出色，并在传统的手工设计方法中取得了决定性的胜利。许多基于深度学习（DL）的算法也已被开发用于各种任务，以分析视网膜眼底图像，从而开发自动计算机辅助诊断系统用于糖尿病视网膜病变（DR）。本文回顾了最新的用于DR检测的DL算法，重点介绍了近期研究论文的贡献和挑战。首先，我们提供了各种DL方法的概述，然后回顾了用于DR诊断的DL技术。最后，我们总结了在设计和训练深度神经网络用于DR诊断方面的未来方向、空白和挑战。本文的其余部分组织如下：第[2](#S2
    "2 自动糖尿病视网膜病变检测 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")节介绍了DR的自动检测、病变类型、DR阶段、DR分级、检测任务和检测框架。之后，第[3](#S3
    "3 数据集和性能指标 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")节简要描述了公共领域的DR数据集和常见的性能指标。第[4](#S4 "4
    深度学习概述 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")节提供了用于DR诊断的DL技术的概述。第[5](#S5 "5 文献综述 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")节回顾了基于DL的最新DR诊断研究。第[6](#S6
    "6 讨论 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")节讨论了这些研究。最后，第[7](#S7 "7 空白与未来方向 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")和第[8](#S8
    "8 结论 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")节呈现了研究空白、未来方向以及结论。
- en: 2 Automatic Diabetic Retinopathy Detection
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 自动糖尿病视网膜病变检测
- en: In this section, for the sake of clarity, we give an overview of DR detection,
    types of DR lesions, stages of DR, grading of DR, DR-detection tasks and the general
    framework for detection. Automatic computer-aided solutions for DR characterization
    are still an open field of research [[4](#bib.bib4)]. Automatic image-based DR
    detection systems are intended to perform rapid retinal evaluations and early
    detection of DR to indicate whether DR complications are present.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，为了清晰起见，我们概述了DR检测、DR病变类型、DR阶段、DR分级、DR检测任务和检测的一般框架。自动计算机辅助解决方案用于DR特征描述仍然是一个开放的研究领域[[4](#bib.bib4)]。自动基于图像的DR检测系统旨在快速进行视网膜评估和早期检测DR，以指示是否存在DR并发症。
- en: 2.1 Types of Lesions
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 病变类型
- en: 'The earliest clinical signs of DR and retinal damage are microaneurysms (MAs),
    which are a dilation of microvasculature formed due to disruption of the internal
    elastic lamina. Retinal microaneurysms reduce vision due to local loss of endothelial
    barrier function, causing leakage and retinal edema. MAs are small (usually less
    than 125 microns in diameter) and appear as red spots with sharp margins. When
    walls of weak capillaries are broken, bleeding causes hemorrhages (HMs), which
    are similar to MAs but larger [[8](#bib.bib8)] and have an irregular margin, they
    have different appearances according to which retinal layer they leak in. Splinter
    hemorrhages occur in the superficial surface layers of the retina and cause a
    superficial flame-shaped bleeding. Whereas dot and blot hemorrhages occur in the
    deeper layers of the retina. More leakage of damaged capillaries can cause exudates
    (EXs), which usually appear yellow and irregularly shaped in the retina. There
    are two types of EXs: hard and soft. Hard exudates (HEs) are lipoproteins and
    other proteins escaping from abnormal retinal vessels. They are white or white-yellow
    with sharp margins. They are often organized in blocks or circular rings [[9](#bib.bib9)]
    and are located in the outer layer of the retina. On the other hand, soft exudates
    (SEs) or cotton wool spots (CWS) are small, whitish-grey cloud-like shapes that
    occur when an arteriole is occluded [[10](#bib.bib10)]. EXs are different from
    MAs and HMs in terms of brightness. MAs and HMs are dark lesions, while EXs are
    bright [[11](#bib.bib11)]. Variations in the diameter of the retinal veins is
    called Venous beading (VB) [[12](#bib.bib12)] this usually happens in advanced
    stages of non-proliferative diabetic retinopathy. Due to the inability to use
    glucose by normal routes, alternate blood pathways are activated, which causes
    the synthesis of elements such as sorbitols and favors the development of alterations
    in the microvasculature. Intraretinal microvascular abnormalities (IRMA) is an
    example, it represents either a dilation of pre-existing capillaries or an actual
    growth of new blood vessels within the retina. When the retinal vessels stand
    out and grow towards the vitreous they are called neovascularization (NV) [[13](#bib.bib13)].
    Macular edema (ME) occurs when the retinal capillaries become permeable and leakage
    occurs around macula [[14](#bib.bib14)]. This can lead to retinal thickening or
    hard exudates developing either within one disk diameter of the center of the
    macula (the fovea) [[15](#bib.bib15)] or involving the fovea, which is responsible
    for the central vision.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DR 和视网膜损伤的最早临床迹象是微动脉瘤（MAs），它们是由于内弹力膜破裂而形成的微血管扩张。视网膜微动脉瘤由于内皮屏障功能局部丧失，导致渗漏和视网膜水肿，从而降低视力。MAs
    较小（通常直径小于 125 微米），呈现为红色斑点，边缘锐利。当脆弱毛细血管的壁破裂时，出血会导致出血（HMs），它们与 MAs 类似但更大[[8](#bib.bib8)]，边缘不规则，根据其渗漏的视网膜层有不同的外观。碎片状出血发生在视网膜的浅表层，导致浅表的火焰状出血。而点状和斑块状出血发生在视网膜的深层。更多的毛细血管损伤渗漏会导致渗出物（EXs），这些通常在视网膜中呈黄色和不规则形状。EXs
    有两种类型：硬性和软性。硬性渗出物（HEs）是从异常视网膜血管中逃逸的脂蛋白和其他蛋白质。它们呈白色或白黄，边缘锐利。它们通常以块状或圆环状排列[[9](#bib.bib9)]，位于视网膜的外层。另一方面，软性渗出物（SEs）或棉絮样斑点（CWS）是小的、灰白色的云状形状，发生在小动脉被阻塞时[[10](#bib.bib10)]。EXs
    在亮度上与 MAs 和 HMs 不同。MAs 和 HMs 是暗色病变，而 EXs 是亮色[[11](#bib.bib11)]。视网膜静脉直径的变化称为静脉珠串（VB）[[12](#bib.bib12)]，通常发生在非增殖性糖尿病视网膜病变的晚期。由于正常途径无法使用葡萄糖，激活了替代血液通道，导致如山梨醇等元素的合成，并促使微血管出现变化。视网膜内微血管异常（IRMA）就是一个例子，它代表了既有毛细血管的扩张或视网膜内新血管的实际生长。当视网膜血管突出并向玻璃体生长时，称为新生血管形成（NV）[[13](#bib.bib13)]。黄斑水肿（ME）发生在视网膜毛细血管变得通透，渗漏发生在黄斑周围[[14](#bib.bib14)]。这可能导致视网膜增厚或硬性渗出物发展，可能在黄斑中心的一盘直径内（即中央凹）[[15](#bib.bib15)]，或涉及中央凹，它负责中央视力。
- en: 'An important object that plays an essential role in detecting DR is the optic
    disc (OD), which characterized by the highest contrast between the circular-shaped
    regions [[16](#bib.bib16)]. The optic disc is used as a landmark and frame of
    reference to diagnose serious eye pathologies such as glaucoma, optic disc pit,
    optic disc drusen and to check for any neovascularization at the disc [[17](#bib.bib17),
    [18](#bib.bib18)]. The OD is also used to pinpoint other structures such as the
    fovea. In normal retina, the edges of the OD are clear and well-defined, as shown
    in Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Types of Lesions ‣ 2 Automatic Diabetic Retinopathy
    Detection ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic
    Retinopathy: A Survey").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '在检测糖尿病视网膜病变中扮演重要角色的一个关键对象是视神经盘（OD），其特征是圆形区域之间的对比度最高[[16](#bib.bib16)]。视神经盘被用作标志物和参考框架，以诊断严重的眼病如青光眼、视神经盘凹陷、视神经盘乳头以及检查视神经盘是否有新生血管[[17](#bib.bib17),
    [18](#bib.bib18)]。视神经盘还用于定位其他结构，如中央凹。在正常视网膜中，视神经盘的边缘清晰且定义良好，如图[1](#S2.F1 "Figure
    1 ‣ 2.1 Types of Lesions ‣ 2 Automatic Diabetic Retinopathy Detection ‣ Deep Learning
    based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey")所示。'
- en: '![Refer to caption](img/e5f6a9cb9a1d687d3142da0484f4d49d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e5f6a9cb9a1d687d3142da0484f4d49d.png)'
- en: 'Figure 1: Optic disc and abnormal findings in the eye fundus caused by the
    diabetic retinopathy.Left: MA, EX and HM. Right: new blood vessel routes(PDR).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：视神经盘和由糖尿病视网膜病变引起的眼底异常发现。左侧：MA、EX 和 HM。右侧：新血管路径（PDR）。
- en: 2.2 Stages of DR
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 糖尿病视网膜病变的阶段
- en: 'DR can be classified into two main classes based on its severity: non-proliferative
    (NPDR) and proliferative (PDR) [[8](#bib.bib8), [19](#bib.bib19)]. NPDR is an
    early stage, during which diabetes starts to damage small blood vessels within
    the retina; it is very common in people with diabetes [[14](#bib.bib14)]. These
    vessels start to discharge fluid and blood, causing the retina to swell. As time
    passes, the swelling or edema thickens the retina, causing blurry vision. The
    clinical feature of this stage is at least one microaneurysm or hemorrhage with
    or without hard exudates [[20](#bib.bib20)]. Proliferative DR is an advanced stage
    that leads to the growth of new blood vessels; as such, it is characterized by
    by abnormal vascular proliferation within the retina towards the vitreous cavity.
    These fragile new blood vessels can bleed into the vitreous cavity and cause severe
    visual loss due to vitreous hemorrhage. They can also further cause traction on
    the retina as they usually grow with a fibro vascular network around them that
    may lead to tractional retinal detachment.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病视网膜病变可以根据其严重程度分为两大类：非增殖性（NPDR）和增殖性（PDR）[[8](#bib.bib8), [19](#bib.bib19)]。NPDR是早期阶段，此时糖尿病开始损害视网膜内的小血管；这种情况在糖尿病患者中非常常见[[14](#bib.bib14)]。这些血管开始渗漏液体和血液，导致视网膜肿胀。随着时间的推移，肿胀或水肿使视网膜变厚，导致视力模糊。此阶段的临床特征是至少有一个微动脉瘤或出血，伴或不伴有硬性渗出物[[20](#bib.bib20)]。增殖性DR是一个高级阶段，导致新血管的生长；因此，其特征是视网膜内的异常血管增生，向玻璃体腔延伸。这些脆弱的新血管可能会出血到玻璃体腔，并因玻璃体出血导致严重的视觉损失。它们还可能进一步对视网膜产生牵引，因为它们通常会在周围生长一张纤维血管网络，这可能导致牵引性视网膜脱离。
- en: 2.3 Grading of DR
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 糖尿病视网膜病变的分级
- en: 'Examination and screening of the retina by ophthalmoscopy usually requires
    dilated pupils, a skilled examiner and a visit to an eye care provider such as
    optometrist to grade and classify pathology [[21](#bib.bib21)]. Grading is a vital
    activity in DR screening programme to diagnose retinal diseases. It is an intensive
    procedure that needs a trained workforce and an adequately sized computer screens[2](#S2.F2
    "Figure 2 ‣ 2.3 Grading of DR ‣ 2 Automatic Diabetic Retinopathy Detection ‣ Deep
    Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '通过眼底镜检查视网膜通常需要扩张瞳孔、一位熟练的检查员和一次到眼科医生如验光师的访问，以评估和分类病理[[21](#bib.bib21)]。分级是糖尿病视网膜病变筛查计划中的一项重要活动，用于诊断视网膜疾病。这是一个需要经过培训的专业人员和适当大小计算机屏幕的密集过程[2](#S2.F2
    "Figure 2 ‣ 2.3 Grading of DR ‣ 2 Automatic Diabetic Retinopathy Detection ‣ Deep
    Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey")。'
- en: '![Refer to caption](img/2522500112a6ca1af59decaecbfb40ec.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2522500112a6ca1af59decaecbfb40ec.png)'
- en: 'Figure 2: Graders need appropriate environment to maintain high-quality performance.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：分级员需要适当的环境以维持高质量的表现。
- en: 'Graders such as optometrists or well-trained technicians perform an essential
    task to treat and recover potentially blinding eye conditions to treat and recover
    potentially blinding eye conditions such as age-related macular degeneration (AMD)
    and diabetic eye diseases [[21](#bib.bib21)]. Non-mydriatic fundus images are
    usually acquired but if the image is unclear due to any media opacity then mydriatic
    drops are used to dilate the pupil in an attempt to improve the quality of the
    image. All graders must receive a special training based on screening protocol
    to ensure the fundus images are graded in standardized manner. They should spend
    time on training to identify and confirm cases as having pathology abnormality
    or not and differentiate the levels of pathology seen and make referral decision
    or return for recall based on agreed interval. There are various systems to grade
    DR vascular changes such as American academy of ophthalmology (AAO), the classification
    which was introduced by the early treatment of diabetic retinopathy study (ETDRS)
    [[22](#bib.bib22)] and Scottish DR grading protocol where only one field is taken
    per eye, which is centered on the fovea [[23](#bib.bib23)]. Scottish protocol
    is represented in Table [1](#S2.T1 "Table 1 ‣ 2.3 Grading of DR ‣ 2 Automatic
    Diabetic Retinopathy Detection ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey").'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 评估人员如验光师或经过良好培训的技术人员在治疗和恢复潜在致盲的眼科疾病（如年龄相关性黄斑变性（AMD）和糖尿病眼病）中扮演了重要角色[[21](#bib.bib21)]。通常会获取非散瞳眼底图像，但如果图像由于任何介质不透明而不清晰，则会使用散瞳药水扩张瞳孔以提高图像质量。所有评估人员必须接受基于筛查协议的特别培训，以确保眼底图像按照标准化的方式进行评分。他们应花时间进行培训，以识别和确认病例是否具有病理异常，并区分观察到的病理水平，基于商定的间隔做出转诊决定或返回复查。对DR血管变化的评分有多种系统，如美国眼科学会（AAO）、早期糖尿病视网膜病变研究（ETDRS）引入的分类[[22](#bib.bib22)]以及苏格兰DR评分协议，其中每只眼睛只拍摄一个以黄斑为中心的视野[[23](#bib.bib23)]。苏格兰协议见表[1](#S2.T1
    "表1 ‣ 2.3 DR评分 ‣ 2 自动化糖尿病视网膜病变检测 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")。
- en: 'Table 1: DR Scottish grading protocol[[24](#bib.bib24)]'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：DR苏格兰评分协议[[24](#bib.bib24)]
- en: '| Grade | Features | Decision |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 级别 | 特征 | 决策 |'
- en: '| R0: No DR | No abnormalities | Rescreen in 12 months |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| R0：无糖尿病视网膜病变 | 无异常 | 12个月复查 |'
- en: '| R1: Mild NPDR | Only MAs | Rescreen in 12 months |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| R1：轻度非增殖性糖尿病视网膜病变 | 仅有MAs | 12个月复查 |'
- en: '| R2: Moderate NPDR |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| R2：中度非增殖性糖尿病视网膜病变 |'
- en: '&#124; More than just MAs but less than severe NPDR &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超过黄斑异常但低于严重增殖性视网膜病变 &#124;'
- en: '| Rescreen in 6 months |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 6个月复查 |'
- en: '| R3: Severe NPDR |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| R3：严重非增殖性糖尿病视网膜病变 |'
- en: '&#124; -More than 20 HMs in each quadrant &#124;'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -每个象限超过20个HMs &#124;'
- en: '&#124; -Venous beading in two quadrants &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -两个象限中的静脉珠状扩张 &#124;'
- en: '&#124; -Intraretinal microvascular abnormalities &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -视网膜内微血管异常 &#124;'
- en: '| Refer |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 转诊 |'
- en: '| R4: PDR |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| R4：增殖性糖尿病视网膜病变 |'
- en: '&#124; -Any new vessels at OD or elsewhere &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -OD或其他地方任何新血管 &#124;'
- en: '&#124; -Vitreous/ pre-retinal HM &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -玻璃体/视网膜前HM &#124;'
- en: '| Refer |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 转诊 |'
- en: '| M0: No ME | No EX or retinal thickening in posterior pole | 12 month rescreening
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| M0：无黄斑水肿 | 无EXs或视网膜增厚在后极部 | 12个月复查 |'
- en: '| M1: Mild ME | EXs or retinal thickening at posterior pole, $>$1 disc diameters
    from fovea | 6 month rescreening |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| M1：轻度黄斑水肿 | 视网膜增厚或在后极部，$>$1个视盘直径距离黄斑 | 6个月复查 |'
- en: '| M2: Moderate ME |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| M2：中度黄斑水肿 |'
- en: '&#124; Same signs of mild ME but with 1 disc diameters or less from fovea,
    &#124;'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轻度黄斑水肿的相同迹象，但距离黄斑1个视盘直径或更少 &#124;'
- en: '&#124; but not affecting fovea &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 但不影响黄斑中央 &#124;'
- en: '| Refer for laser treatment |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 需转诊以进行激光治疗 |'
- en: '| M3: Severe ME | EXs or retinal thickening affecting center of fovea | Refer
    for laser treatment |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| M3：严重黄斑水肿 | 影响黄斑中央的EXs或视网膜增厚 | 需转诊以进行激光治疗 |'
- en: 2.4 Detection Tasks and General Framework
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 检测任务和一般框架
- en: 'At a high level, DR detection is categorized into two tasks: lesion-level-based
    detection and image-level-based detection. In lesion-level-based detection, every
    lesion is detected and their locations are determined because the number of lesions
    and their locations are crucial to assessing the severity level of DR [[25](#bib.bib25)].
    On the other hand, image–based detection focuses on assessment based on image
    levels and is more interesting from the screening point of view because it evaluates
    only whether there are signs of DR [[25](#bib.bib25)]. Lesion-based detection
    usually involves two phases: (i) lesion detection and/or segmentation and (ii)
    lesion classification. First, lesions such as microaneurysms, hemorrhages, hard
    exudates and soft exudates are detected from fundus images, and the exact area
    of the lesion is localized. This is a challenging task because retinal fundus
    images contain other objects with similar appearances, such as red dots and blood
    vessels. For this task, the global and local context are usually needed to perform
    accurate localization and segmentation. The detection phase yields potential regions
    of interest, but they include false positives as well. The lesion-classification
    phase is used to remove false positives. Image-based detection is an image-screening
    task that classifies a given fundus image as being normal or having DR signs.
    This is one of the first areas of medical diagnosis to which DL has made a significant
    contributions [[26](#bib.bib26)].'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，DR 检测被分为两种任务：病灶级别检测和图像级别检测。在病灶级别检测中，每个病灶都被检测到，并确定其位置，因为病灶的数量及其位置对评估 DR
    的严重程度至关重要[[25](#bib.bib25)]。另一方面，基于图像的检测侧重于基于图像级别的评估，从筛查的角度来看更有趣，因为它仅评估是否存在 DR
    的迹象[[25](#bib.bib25)]。病灶级别检测通常涉及两个阶段：(i) 病灶检测和/或分割，以及 (ii) 病灶分类。首先，从眼底图像中检测病灶，如微动脉瘤、出血、硬性渗出物和软性渗出物，并确定病灶的确切区域。这是一个具有挑战性的任务，因为视网膜眼底图像中包含其他具有类似外观的物体，如红点和血管。对于这项任务，通常需要全局和局部上下文以进行准确的定位和分割。检测阶段产生潜在的兴趣区域，但也包括误报。病灶分类阶段用于去除误报。基于图像的检测是一项图像筛查任务，将给定的眼底图像分类为正常或有
    DR 迹象。这是 DL 在医学诊断中做出重大贡献的首个领域之一[[26](#bib.bib26)]。
- en: 'The general framework for detection, segmentation and classification involves
    the specific steps of preprocessing, feature extraction/selection, choice of a
    suitable classification method and finally assessment of the results. DR classification
    systems can be divided into two types according to learning procedure: supervised
    and unsupervised learning. In supervised learning, the system is taught using
    labeled data to infer functional mapping [[27](#bib.bib27), [28](#bib.bib28)].
    On the other hand, unsupervised learning methods tend to discover hidden patterns
    on their own from the properties of the unlabeled examples according to their
    similarity [[29](#bib.bib29)]. Unlike hand-engineered feature-based approaches,
    DL approaches integrate all of the steps into a unified framework and automatically
    learns the features and trains the system in an end-to-end manner.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 检测、分割和分类的一般框架包括预处理、特征提取/选择、选择合适的分类方法以及最终结果评估的具体步骤。DR 分类系统根据学习过程可以分为两种类型：监督学习和无监督学习。在监督学习中，系统使用标记的数据进行训练以推断功能映射[[27](#bib.bib27),
    [28](#bib.bib28)]。另一方面，无监督学习方法倾向于根据未标记示例的相似性发现隐藏模式[[29](#bib.bib29)]。与手工工程特征方法不同，深度学习（DL）方法将所有步骤集成到一个统一框架中，并以端到端的方式自动学习特征并训练系统。
- en: 3 Datasets and Performance Metrics
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集和性能指标
- en: In this section, we give an overview of the benchmark datasets and performance
    metrics that are commonly used for DR research.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了在糖尿病视网膜病变（DR）研究中常用的基准数据集和性能指标。
- en: 3.1 Retinal Fundus Image Datasets
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 视网膜眼底图像数据集
- en: 'Several datasets consisting of retinal fundus images have been produced to
    teach and test the algorithms for different DR detection tasks. In the following
    paragraphs, we give an overview of the following public domain benchmark datasets:
    MESSIDOR [[30](#bib.bib30)], e-ophtha [[31](#bib.bib31)], Kaggle [[32](#bib.bib32)],
    DRIVE [[33](#bib.bib33)], STARE [[34](#bib.bib34)], DIARETDB1 [[35](#bib.bib35),
    [36](#bib.bib36)], CHASE [[37](#bib.bib37)] , DRiDB [[38](#bib.bib38)], ORIGA
    [[39](#bib.bib39)], SCES [[40](#bib.bib40)] , AREDS [[41](#bib.bib41)], REVIEW
    [[42](#bib.bib42)], EyePACS-1 [[43](#bib.bib43)], RIM-ONE [[44](#bib.bib44)],
    DRISHTI-GS [[45](#bib.bib45)], ARIA [[46](#bib.bib46)], DRIONS-DB [[47](#bib.bib47)]
    and SEED-DB [[48](#bib.bib48)]. Table [2](#S3.T2 "Table 2 ‣ 3.1.18 SEED ‣ 3.1
    Retinal Fundus Image Datasets ‣ 3 Datasets and Performance Metrics ‣ Deep Learning
    based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey") summarizes
    these datasets.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '已经产生了几个包含视网膜眼底图像的数据集，用于教授和测试不同糖尿病视网膜病变（DR）检测任务的算法。在以下段落中，我们概述了以下公共领域基准数据集：MESSIDOR
    [[30](#bib.bib30)]、e-ophtha [[31](#bib.bib31)]、Kaggle [[32](#bib.bib32)]、DRIVE
    [[33](#bib.bib33)]、STARE [[34](#bib.bib34)]、DIARETDB1 [[35](#bib.bib35)、[36](#bib.bib36)]、CHASE
    [[37](#bib.bib37)]、DRiDB [[38](#bib.bib38)]、ORIGA [[39](#bib.bib39)]、SCES [[40](#bib.bib40)]、AREDS
    [[41](#bib.bib41)]、REVIEW [[42](#bib.bib42)]、EyePACS-1 [[43](#bib.bib43)]、RIM-ONE
    [[44](#bib.bib44)]、DRISHTI-GS [[45](#bib.bib45)]、ARIA [[46](#bib.bib46)]、DRIONS-DB
    [[47](#bib.bib47)]和SEED-DB [[48](#bib.bib48)]。表[2](#S3.T2 "Table 2 ‣ 3.1.18 SEED
    ‣ 3.1 Retinal Fundus Image Datasets ‣ 3 Datasets and Performance Metrics ‣ Deep
    Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey")总结了这些数据集。'
- en: 3.1.1 MESSIDOR
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 MESSIDOR
- en: 'It was developed under MESSIDOR research program funded by the French ministry
    of research and defense [[30](#bib.bib30)]. It was acquired by three ophthalmology
    departments using colored video 3CCD camera mounted on a Topcon TRC NW6 non-mydriatic
    retinograph with a 45^∘ field of view (FOV). Two types of image level annotation
    were provided by expert ophthalmologists: DR grades and risk levels of macular
    edema. The DR grades are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 它是在MESSIDOR研究计划下开发的，该计划由法国研究和国防部资助[[30](#bib.bib30)]。它由三个眼科部门使用装有45^∘视野（FOV）的Topcon
    TRC NW6非散瞳眼底照相机通过彩色视频3CCD相机获得。专家眼科医生提供了两种类型的图像级注释：DR等级和黄斑水肿的风险等级。DR等级如下：
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '0: No risk: (#MA = 0) AND (#HM = 0)'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '0: 无风险: (#MA = 0) AND (#HM = 0)'
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '1: (0 $<$ #MA $\leq$ 5) AND (#HM = 0)'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '1: (0 $<$ #MA $\leq$ 5) AND (#HM = 0)'
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '2: ((5 $<$ #MA $<$ 15) OR (0 $<$ #HM $<$ 5)) AND (NV = 0)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '2: ((5 $<$ #MA $<$ 15) OR (0 $<$ #HM $<$ 5)) AND (NV = 0)'
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '3: (#MA $\leq$ 15) OR (#HM $\leq$ 5) OR (NV = 1)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '3: (#MA $\leq$ 15) OR (#HM $\leq$ 5) OR (NV = 1)'
- en: 'The risk levels of macular edema are as follows :'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 黄斑水肿的风险等级如下：
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '0: No risk'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '0: 无风险'
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '1: Shortest distance between macula and hard EX $>$ one papilla diameter'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '1: 黄斑和硬性外周病变之间的最短距离$>$一个乳头直径'
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '2: Shortest distance between macula and hard EX $\leq$ one papilla diameter'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '2: 黄斑和硬性外周病变之间的最短距离$\leq$一个乳头直径'
- en: 3.1.2 e-ophtha
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 e-ophtha
- en: 'It was introduced by e-ophtha project funded by the French research agency
    [[31](#bib.bib31)]. It provides the locations of MAs and EXs, which were identified
    by two ophthalmologists. The first ophthalmologist outlined the locations, which
    were checked and examined by the second ophthalmologist. The database consists
    of two datasets: e-ophtha EX and e-ophtha MA. The e-ophtha EX set contains 47
    images with 12,278 EXs and 35 healthy images. Several images of healthy controls
    contain structures which can easily mislead EX detection methods, such as reflections
    and optical artifacts. On the other hand, e-ophtha MA contains 148 images with
    1306 MAs and 233 healthy images.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 它由e-ophtha项目引入，该项目由法国研究机构资助[[31](#bib.bib31)]。它提供了黄斑病变（MA）和硬性外周病变（EX）的定位，这些定位由两位眼科医生确认。第一位眼科医生勾勒出位置，第二位眼科医生检查和审查。数据库包括两个数据集：e-ophtha
    EX和e-ophtha MA。e-ophtha EX集包含47张图像，其中有12,278个EX和35张健康图像。几张健康对照图像包含易于误导EX检测方法的结构，如反射和光学伪影。另一方面，e-ophtha
    MA包含148张图像，其中有1306个MA和233张健康图像。
- en: 3.1.3 Kaggle
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 Kaggle
- en: 'It consists of a large set of high-resolution retinal images taken under different
    conditions and was provided by EyePACS clinics [[32](#bib.bib32)]. The image level
    annotation was provided by expert ophthalmologists, and each image has been assigned
    a DR grade on the scale of 0 to 4 as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集由EyePACS诊所提供，包括大量在不同条件下拍摄的高分辨率视网膜图像[[32](#bib.bib32)]。图像级注释由专家眼科医生提供，每张图像被分配了0到4的DR等级，具体如下：
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '0: No risk'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '0: 无风险'
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '1: Mild'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '1: 轻度'
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '2: Moderate'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '2: 中度'
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '3: Severe'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '3: 严重'
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '4: PDR'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '4: PDR'
- en: 3.1.4 DRIVE
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 DRIVE
- en: Digital retinal images for vessel extraction (DRIVE) [[33](#bib.bib33)] was
    collected under DR screening program in Netherlands for comparative studies of
    vascular segmentation in retinal images using Canon CR5 non-mydriatic 3CCD camera
    . It consists of 40 fundus images, which were randomly selected; among them 33
    do not show any sign of DR whereas 7 show signs of mild early DR; it is divided
    into a test and training sets, each containing 20 images. It provides pixel level
    annotation; a pixel is annotated as a vessel pixel with 70% confidence.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 用于血管提取的数字视网膜图像（DRIVE）[[33](#bib.bib33)]是在荷兰的DR筛查程序下收集的，用于比较视网膜图像中的血管分割，使用了Canon
    CR5非散瞳3CCD相机。它包含40张眼底图像，这些图像是随机选择的；其中33张没有显示DR的迹象，而7张显示轻度早期DR的迹象；它被分为测试集和训练集，每个集包含20张图像。它提供了像素级注释；像素被注释为血管像素的置信度为70%。
- en: 3.1.5 STARE
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 STARE
- en: Structured analysis of the retina (STARE) [[34](#bib.bib34)] program was funded
    by the U.S. national institutes of health (NIH). It includes fundus images showing
    13 diseases associated with human eye. It provides the list of disease codes and
    names for each image. Blood vessels and optic nerve have the pixel level annotation
    but without grading. Two observers manually segmented all the images. On average,
    the first person labeled 32,200 pixels in each image as vessel, while the second
    person labeled 46,100 pixels in each image as vessel. This dataset offers a challenging
    OD detection problem due to the appearance of retinal diseases.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 视网膜结构分析（STARE）[[34](#bib.bib34)] 程序由美国国家卫生研究院（NIH）资助。它包括显示13种与人眼相关的疾病的眼底图像。它提供了每张图像的疾病代码和名称列表。血管和视神经具有像素级注释，但没有分级。两名观察者手动分割了所有图像。平均而言，第一个人将每张图像中的32,200个像素标记为血管，而第二个人将每张图像中的46,100个像素标记为血管。由于视网膜疾病的出现，该数据集提供了一个具有挑战性的OD检测问题。
- en: 3.1.6 DIARETDB1
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.6 DIARETDB1
- en: This dataset contains 89 color fundus images, which were taken under varying
    imaging settings and FOV of 50^∘, and were captured in Kuopio university hospital
    in Finland [[35](#bib.bib35)]. Four independent experts annotated the images.
    These experts delineated the regions where MAs and HMs can be found, and provided
    a map for each type of lesion. This dataset is referred to as “calibration level
    1 fundus images”. It is divided into training and test sets containing 28 and
    61 images, respectively.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含89张彩色眼底图像，这些图像是在不同的成像设置和50^∘视场下拍摄的，拍摄地点位于芬兰的库奥皮奥大学医院[[35](#bib.bib35)]。四位独立专家对这些图像进行了注释。这些专家划定了可以找到MAs和HMs的区域，并提供了每种病变类型的地图。该数据集被称为“校准级别1眼底图像”。它被分为训练集和测试集，分别包含28张和61张图像。
- en: 3.1.7 CHASE
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.7 CHASE
- en: It was acquired under the program child heart and health study in England (CHASE)
    [[37](#bib.bib37)] from children of different ethnic origin and ages from 9 and
    10 years. It consists of 28 fundus images taken from 14 children and the annotation
    contains ground truths for blood vessels collected using Top Con TRV-50 camera
    with 35 FOV. Unlike DRIVE and STARE, it contains images with uneven and non-uniform
    background illumination, poor contrast of blood vessels and wider arteries that
    have a bright strip running down the center, known as the central vessel reflex.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在英格兰的儿童心脏与健康研究计划（CHASE）[[37](#bib.bib37)]下获得的，来自不同种族和年龄在9岁和10岁的儿童。它包含28张来自14名儿童的眼底图像，注释包含使用Top
    Con TRV-50相机以35视场拍摄的血管的真实情况。与DRIVE和STARE不同，它包含背景照明不均匀和非均匀的图像，血管对比度较差，以及有明亮条纹贯穿中心的较宽动脉，这被称为中心血管反射。
- en: 3.1.8 DRiDB
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.8 DRiDB
- en: Diabetic retinopathy image database (DRiDB) [[38](#bib.bib38)] was obtained
    at university hospital in Zagreb and was created to overcome the shortcomings
    in previous datasets such as grading and limited number of observers. Images were
    taken and selected by experts with 45 FOV and shown DR symptoms vary from almost
    normal to cases where new fragile vessels are visible. In this dataset, each image
    was evaluated by five independent experts to mark DR findings. These experts annotated
    pixels of findings and related areas of MAs, HMs, hard and soft EXs, blood vessels,
    ODs and macula.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病视网膜病变图像数据库（DRiDB）[[38](#bib.bib38)]是在萨格勒布的大学医院获得的，旨在克服先前数据集中的不足，如分级和观察者数量有限。图像由专家拍摄并选择，具有45视场，并且DR症状从几乎正常到可以看到新生脆弱血管的病例各不相同。在该数据集中，每张图像由五位独立专家评估，以标记DR发现。这些专家注释了发现的像素及相关区域，包括MAs、HMs、硬性和软性EXs、血管、ODs和黄斑。
- en: 3.1.9 ORIGA
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.9 ORIGA
- en: Online retinal fundus image database for glaucoma analysis and research (ORIGA)
    [[39](#bib.bib39)] is an online repository which shares fundus images and their
    ground truths as benchmarks for researchers to share retinal image analysis results
    and the corresponding diagnosis. It was collected over a period of 3 years from
    2004 to 2007 at Singapore eye research institute. It focuses on OD and optic cup
    (OC) segmentation and Cup-to-Disc Ratio (CDR) to diagnosis glaucoma.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在线视网膜眼底图像数据库用于青光眼分析和研究（ORIGA）[[39](#bib.bib39)]是一个在线库，分享眼底图像及其真实数据作为基准，供研究人员共享视网膜图像分析结果及相应诊断。该数据集在2004年至2007年间在新加坡眼科研究所收集，重点关注视盘（OD）和视神经杯（OC）分割以及杯盘比（CDR）用于青光眼诊断。
- en: 3.1.10 SCES
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.10 SCES
- en: It was acquired under Singapore Chinese eye study (SCES) [[40](#bib.bib40)]
    conducted on 1,060 Chinese participants and was graded by one senior professional
    grader and one retinal specialist. The study was conducted to identify the determinants
    of anterior chamber depth (ACD) and to ascertain the relative importance of these
    determinants in Chinese persons in Singapore.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 它是在新加坡华人眼科研究（SCES）[[40](#bib.bib40)]下获得的，研究对象为1060名华人参与者，由一位高级专业分级员和一位视网膜专家进行分级。该研究旨在识别前房深度（ACD）的决定因素，并确定这些因素在新加坡华人中的相对重要性。
- en: 3.1.11 AREDS
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.11 AREDS
- en: It was developed under Age-related eye disease study (AREDS) [[41](#bib.bib41)],
    which was funded by NIH. It is long-term multicenter, prospective study of 595
    participants with ages from 55 to 80 years, which was designed to assess the clinical
    course of both AMD and cataract. Participants were of any illness or condition
    that would make long-term follow-up. On the basis of fundus photographs graded
    by a central reading center, best corrected visual acuity, and ophthalmologic
    evaluations, participants were enrolled in one of several AMD categories.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 它是在年龄相关眼病研究（AREDS）[[41](#bib.bib41)]下开发的，该研究由NIH资助。这是一项长期多中心前瞻性研究，涉及595名年龄在55至80岁之间的参与者，旨在评估AMD和白内障的临床过程。参与者没有任何会影响长期跟踪的疾病或状况。根据由中央阅片中心分级的眼底照片、最佳矫正视力和眼科评估，参与者被纳入若干AMD类别之一。
- en: 3.1.12 REVIEW
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.12 REVIEW
- en: 'Retinal vessel image set for estimation of widths (REVIEW) [[42](#bib.bib42)]
    was made available online in 2008 by the department of computing and informatics
    at the university of Lincoln, UK. The dataset contains 16 mydriatic images with
    193 annotated vessel segments consisting of 5066 profile points manually marked
    by three independent experts. Unlike DRIVE and STARE, REVIEW dataset includes
    width measurements. The images were chosen to evaluate the accuracy and precision
    of the vessel width measurement algorithms in the presence of pathology and central
    light reflex. The 16 images are subdivided into four sets: the high resolution
    image set (8 images), the vascular disease image set (4 images), the central light
    reflex image set (2 images) and the kickpoint image set (2 images).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 估计宽度的视网膜血管图像集（REVIEW）[[42](#bib.bib42)]由英国林肯大学计算机与信息学系于2008年在线发布。数据集包含16张散瞳图像和193个标注的血管段，总共有5066个轮廓点由三位独立专家手动标记。与DRIVE和STARE不同，REVIEW数据集包含宽度测量。这些图像被选中以评估血管宽度测量算法在存在病理和中央光反射情况下的准确性和精确性。16张图像被划分为四组：高分辨率图像组（8张图像）、血管疾病图像组（4张图像）、中央光反射图像组（2张图像）和起点图像组（2张图像）。
- en: 3.1.13 EyePACS-1
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.13 EyePACS-1
- en: Eye picture archive and communication system (EyePACS) [[43](#bib.bib43)] is
    a flexible protocol and web based telemedicine system for DR screening and collaboration
    among clinicians. Patients’ fundus images can be easily uploaded to EyePACS web.
    The protocol evaluates the presence and severity of discrete retinal lesions associated
    with DR. The protocol uses the Canon CR-DGi and Canon CR-1 nonmydriatic cameras
    can be accessed on the EyePACS Web site. The lesions are graded as MAs, HMs with
    or without MAs, cotton wool spots, intraretinal microvascular abnormalities, venous
    beading, new vessels (new vessels on the disk and new vessels elsewhere), fibrous
    proliferation, vitreous HMs or preretinal HMs and HEs. In addition, the presence
    or absence of laser scars. Graders grade each lesion type separately in each image
    using an online grading template that records a choice for each lesion type among
    no (absent), yes (present) or cannot grade.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 眼底图像归档与通信系统（EyePACS）[[43](#bib.bib43)]是一个灵活的协议和基于网络的远程医疗系统，用于DR筛查和临床医生之间的协作。患者的眼底图像可以轻松上传到EyePACS网站。该协议评估与DR相关的离散视网膜病变的存在和严重程度。该协议使用Canon
    CR-DGi和Canon CR-1非散瞳相机，这些相机可以在EyePACS网站上访问。病变被分为MAs、HMs（有或没有MAs）、棉花状斑点、视网膜内微血管异常、静脉珠串、新生血管（视盘上的新生血管和其他地方的新生血管）、纤维增生、玻璃体HMs或前膜HMs以及HEs。此外，还包括激光瘢痕的存在与否。评分者使用在线评分模板单独为每种病变类型评分，记录每种病变类型的选择，包括无（缺失）、有（存在）或无法评分。
- en: 3.1.14 RIM-ONE
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.14 RIM-ONE
- en: 'It is an open retinal image database for optic nerve evaluation (RIM-ONE) [[44](#bib.bib44)]
    captured by non-mydriatic Nidek AFC-210 with a body of a Canon EOS 5D Mark II.
    It was designed for glaucoma diagnosis and consists of 169 optic nerve head regions,
    which were cropped manually from full fundus images. These images were annotated
    by 5 glaucoma experts: 4 ophthalmologists and 1 optometrist.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个开放的视网膜图像数据库，用于视神经评估（RIM-ONE）[[44](#bib.bib44)]，由非散瞳的Nidek AFC-210与Canon
    EOS 5D Mark II机身拍摄。旨在用于青光眼诊断，包含169个视神经头区域，这些区域是从完整眼底图像中手动裁剪的。这些图像由5位青光眼专家注释：4位眼科医生和1位验光师。
- en: 3.1.15 DRISHTI-GS
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.15 DRISHTI-GS
- en: It consists of a total of 101 fundus images of healthy controls and glaucoma
    patients with almost 25 FOV that were collected at Aravind eye hospital in India
    [[45](#bib.bib45)]. It is divided into training and test sets consisting of 50
    and 51 images, respectively. All images were annotated by 4 ophthalmologists with
    clinical experiences of 3, 5, 9 and 20 years, respectively. The manual segmentation
    of OD and OC boundaries, and CDR are provided as ground truths. Also two other
    expert opinions were included about whether an image represents healthy control
    or glaucomatous eye and presence or absence of notching in the inferior and/or
    superior sectors of the image.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 包含了101张来自印度Aravind眼科医院的健康对照组和青光眼患者的眼底图像，几乎覆盖25 FOV。数据集分为训练集和测试集，分别包含50张和51张图像。所有图像由4位拥有3年、5年、9年和20年临床经验的眼科医生注释。手动标记了OD和OC边界以及CDR作为真实值。此外，还包括了关于图像是否代表健康对照或青光眼眼以及图像的下方和/或上方区域是否有凹陷的其他两个专家意见。
- en: 3.1.16 ARIA
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.16 ARIA
- en: Automated retinal image analyzer (ARIA) [[46](#bib.bib46)] was collected and
    designed to trace blood vessels, ODs and fovea locations. It was marked by two
    image analysis experts. This dataset was collected at St Paul’s eye unit and the
    university of Liverpool to diagnosis AMD and DR using a Zeiss FF450+ fundus camera
    at a 50 FOV.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化视网膜图像分析器（ARIA）[[46](#bib.bib46)]旨在跟踪血管、OD和中央凹的位置。由两位图像分析专家标记。该数据集在St Paul’s眼科单位和利物浦大学使用Zeiss
    FF450+眼底相机在50 FOV下收集，用于诊断AMD和DR。
- en: 3.1.17 DRIONS-DB
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.17 DRIONS-DB
- en: Digital retinal images for optic nerve segmentation database (DRIONS-DB) [[47](#bib.bib47)]
    was collected at a university hospital in Spain. It was designed to segment optic
    nerve head and its related pathologies. It was annotated by 2 independent medical
    experts. Images were centered on the optic nerve head and are stored in slide
    format.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 光神经分割数据库的数字视网膜图像（DRIONS-DB）[[47](#bib.bib47)]在西班牙的一家大学医院收集。该数据库旨在分割视神经头及其相关病理。由2位独立的医学专家注释。图像以视神经头为中心，并以幻灯片格式存储。
- en: 3.1.18 SEED
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.18 SEED
- en: Singapore epidemiology of eye diseases (SEED) [[48](#bib.bib48)] was composed
    of 235 fundus images with a focus on studying major eye diseases, including DR,
    AMD, glaucoma, refractive errors and cataract. Each image has OD and OC regions
    marked by a trained grader, which serves as a ground truth for segmentation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 新加坡眼病流行病学（SEED）[[48](#bib.bib48)]由235张眼底图像组成，重点研究主要眼病，包括DR、AMD、青光眼、屈光差错和白内障。每张图像由训练有素的评分员标记OD和OC区域，作为分割的基础真相。
- en: 'Table 2: Datasets for DR Detection'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：DR检测数据集
- en: '| Dataset | #Images | Resolution | Format | Tasks |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | #图像 | 分辨率 | 格式 | 任务 |'
- en: '| Images level annotation |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 图像级别标注 |'
- en: '| MESSIDOR [[49](#bib.bib49)] | 1,200 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| MESSIDOR [[49](#bib.bib49)] | 1,200 |'
- en: '&#124; 1,440$\times$960, &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1,440$\times$960, &#124;'
- en: '&#124; 2,240$\times$1,488, &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2,240$\times$1,488, &#124;'
- en: '&#124; 2,304$\times$1,536 &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2,304$\times$1,536 &#124;'
- en: '|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Images: TIFF &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像：TIFF &#124;'
- en: '&#124; Diagnosis: excel file &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 诊断：excel文件 &#124;'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; -DR grading &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -DR分级 &#124;'
- en: '&#124; -Risk of DME &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -DME风险 &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Kaggle &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Kaggle &#124;'
- en: '&#124; [[32](#bib.bib32)] &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[32](#bib.bib32)] &#124;'
- en: '| 80,000 | - | JPEG |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 80,000 | - | JPEG |'
- en: '&#124; -No DR &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -无DR &#124;'
- en: '&#124; -Mild &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -轻度 &#124;'
- en: '&#124; -Moderate &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -中度 &#124;'
- en: '&#124; -Severe &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -严重 &#124;'
- en: '&#124; -PDR &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -PDR &#124;'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| AREDS[[41](#bib.bib41)] | 72,000 | - | - | -AMD stages |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| AREDS[[41](#bib.bib41)] | 72,000 | - | - | -AMD阶段 |'
- en: '| EyePACS-1[[43](#bib.bib43)] | 9,963 | - | - |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| EyePACS-1[[43](#bib.bib43)] | 9,963 | - | - |'
- en: '&#124; -Referable DR &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -可参考DR &#124;'
- en: '&#124; -MA &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -MA &#124;'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Pixel level annotation |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 像素级别标注 |'
- en: '|'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; e-ophtha[[31](#bib.bib31)] &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; e-ophtha[[31](#bib.bib31)] &#124;'
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 148 MAs, &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 148 MAs, &#124;'
- en: '&#124; 233 normal non-MA &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 233正常非MA &#124;'
- en: '&#124; 47 EXs, &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 47 EXs, &#124;'
- en: '&#124; 35 normal non-EX &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 35正常非EX &#124;'
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2,544 $\times$ 1,696 &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2,544 $\times$ 1,696 &#124;'
- en: '&#124; 1440$\times$960 &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1440$\times$960 &#124;'
- en: '- |'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '- |'
- en: '&#124; Images: JPEG &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像：JPEG &#124;'
- en: '&#124; GT: PNG &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GT：PNG &#124;'
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; -MA small HM detection &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -MA小HM检测 &#124;'
- en: '&#124; -EX detection &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -EX检测 &#124;'
- en: '|'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DRIVE [[33](#bib.bib33)] |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| DRIVE [[33](#bib.bib33)] |'
- en: '&#124; 33 normal &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 33正常 &#124;'
- en: '&#124; 7 mild to early &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 7轻度到早期 &#124;'
- en: '&#124; DR stage &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DR阶段 &#124;'
- en: '| 584$\times$565 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 584$\times$565 |'
- en: '&#124; Images: TIFF &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像：TIFF &#124;'
- en: '&#124; GT, masks: GIF &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GT，遮罩：GIF'
- en: '| -Vessels extraction |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| -血管提取 |'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; STARE &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; STARE &#124;'
- en: '&#124; [[34](#bib.bib34)] &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[34](#bib.bib34)] &#124;'
- en: '| 402 | 605$\times$700 | PPM |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 402 | 605$\times$700 | PPM |'
- en: '&#124; -13 retinal diseases &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -13种视网膜疾病 &#124;'
- en: '&#124; -Vessels extraction &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -血管提取 &#124;'
- en: '&#124; -Optic nerve &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -视神经 &#124;'
- en: '|'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DIARETDB1 &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DIARETDB1 &#124;'
- en: '&#124; [[35](#bib.bib35), [36](#bib.bib36)] &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[35](#bib.bib35), [36](#bib.bib36)] &#124;'
- en: '|'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 5 normal &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5正常 &#124;'
- en: '&#124; 84 with at least one &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 84至少有一个 &#124;'
- en: '&#124; NPDR sign &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NPDR标志 &#124;'
- en: '| 1,500$\times$1,152 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 1,500$\times$1,152 |'
- en: '&#124; Images, &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像，&#124;'
- en: '&#124; masks, &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 遮罩，&#124;'
- en: '&#124; GT: PNG &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GT：PNG &#124;'
- en: '|'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; -MAs &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -MAs &#124;'
- en: '&#124; -HMs &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -HMs &#124;'
- en: '&#124; -SEs &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -SEs &#124;'
- en: '&#124; -HEs &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -HEs &#124;'
- en: '|'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CHASE [[37](#bib.bib37)] | 28 | 1,280 $\times$ 960 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| CHASE [[37](#bib.bib37)] | 28 | 1,280 $\times$ 960 |'
- en: '&#124; Images: JPEG &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像：JPEG &#124;'
- en: '&#124; GT: PNG &#124;'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GT：PNG &#124;'
- en: '| -Vessels extraction |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| -血管提取 |'
- en: '| DRiDB [[38](#bib.bib38)] | 50 | 720$\times$576 | BMP |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| DRiDB [[38](#bib.bib38)] | 50 | 720$\times$576 | BMP |'
- en: '&#124; -MAs &#124;'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -MAs &#124;'
- en: '&#124; -HMs &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -HMs &#124;'
- en: '&#124; -HEs &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -HEs &#124;'
- en: '&#124; -SEs &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -SEs &#124;'
- en: '&#124; -Vessels extraction &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -血管提取 &#124;'
- en: '&#124; -OD &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -OD &#124;'
- en: '&#124; -Macula &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -黄斑 &#124;'
- en: '|'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ORIGA[[39](#bib.bib39)] |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| ORIGA[[39](#bib.bib39)] |'
- en: '&#124; 482 normal &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 482正常 &#124;'
- en: '&#124; 168 glaucomatous &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 168青光眼性&#124;'
- en: '| 720$\times$576 | - |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 720$\times$576 | - |'
- en: '&#124; -OD &#124;'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -OD &#124;'
- en: '&#124; -Optic cup &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -视盘杯 &#124;'
- en: '&#124; -Cup-to-Disc Ratio (CDR) &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -杯盘比 (CDR) &#124;'
- en: '|'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SCES[[40](#bib.bib40)] |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| SCES[[40](#bib.bib40)] |'
- en: '&#124; 1,630 normal &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1,630正常 &#124;'
- en: '&#124; 46 glaucomatous &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 46青光眼性 &#124;'
- en: '| - | - | -CDR |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| - | - | -CDR |'
- en: '| REVIEW[[42](#bib.bib42)] | 16 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| REVIEW[[42](#bib.bib42)] | 16 |'
- en: '&#124; 3,584$\times$2,438 &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3,584$\times$2,438 &#124;'
- en: '&#124; 1,360$\times$1,024 &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1,360$\times$1,024 &#124;'
- en: '&#124; 2,160$\times$1,440 &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2,160$\times$1,440 &#124;'
- en: '&#124; 3,300$\times$2,600 &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3,300$\times$2,600 &#124;'
- en: '| - | -Vessels extraction |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| - | -血管提取 |'
- en: '| RIM-ONE[[44](#bib.bib44)] |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| RIM-ONE[[44](#bib.bib44)] |'
- en: '&#124; 118 normal &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 118正常 &#124;'
- en: '&#124; 12 early glaucoma &#124;'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 12 早期青光眼 &#124;'
- en: '&#124; 14 moderate glaucoma &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 14 中度青光眼 &#124;'
- en: '&#124; 14 deep glaucoma &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 14 深度青光眼 &#124;'
- en: '&#124; 11 ocular hypertension &#124;'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 11 眼内高压 &#124;'
- en: '| - | - | -Optic nerve |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| - | - | -视神经 |'
- en: '| DRISHTI-GS[[45](#bib.bib45)] |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| DRISHTI-GS[[45](#bib.bib45)] |'
- en: '&#124; 31 normal &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 31 正常 &#124;'
- en: '&#124; 70 glaucomatous &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 70 青光眼性 &#124;'
- en: '| 2,896 $\times$ 1,944 | PNG |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 2,896 $\times$ 1,944 | PNG |'
- en: '&#124; -OD segmentation &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -OD 分割 &#124;'
- en: '&#124; -OC segmentation &#124;'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -OC 分割 &#124;'
- en: '|'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ARIA [[46](#bib.bib46)] |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| ARIA [[46](#bib.bib46)] |'
- en: '&#124; 16 normal &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 16 正常 &#124;'
- en: '&#124; 92 AMD &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 92 AMD &#124;'
- en: '&#124; 59 DR &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 59 糖尿病视网膜病变 &#124;'
- en: '| 768$\times$576 | TIFF |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 768$\times$576 | TIFF |'
- en: '&#124; -OD &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -OD &#124;'
- en: '&#124; -Fovea location &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -中心凹位置 &#124;'
- en: '&#124; -Vessel extraction &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -血管提取 &#124;'
- en: '|'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DRIONS-DB[[47](#bib.bib47)] | 110 | 600$\times$400 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| DRIONS-DB[[47](#bib.bib47)] | 110 | 600$\times$400 |'
- en: '&#124; Images: JPEG &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像：JPEG &#124;'
- en: '&#124; GT: txt file &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GT: txt 文件 &#124;'
- en: '| -OD |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| -OD |'
- en: '| SEED-DB[[48](#bib.bib48)] |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| SEED-DB[[48](#bib.bib48)] |'
- en: '&#124; 192 normal &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 192 正常 &#124;'
- en: '&#124; 43 glaucomatous &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 43 青光眼性 &#124;'
- en: '| 3,504$\times$ 2,336 | - |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 3,504$\times$ 2,336 | - |'
- en: '&#124; -OD &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -OD &#124;'
- en: '&#124; -OC &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -OC &#124;'
- en: '|'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 3.2 Performance Metrics
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 性能指标
- en: In this section, we define the performance metrics that are commonly used to
    assess DR detection algorithms. Common metrics for measuring the performance of
    classification algorithms include accuracy, sensitivity (recall), specificity,
    precision, F-score, ROC curve, logloss, IOU, overlapping error, boundary-based
    evaluation and the dice similarity coefficient.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们定义了常用的性能指标，以评估糖尿病视网膜病变检测算法。用于衡量分类算法性能的常见指标包括准确率、敏感性（召回率）、特异性、精确度、F-得分、ROC
    曲线、对数损失、IOU、重叠误差、基于边界的评估和骰子相似系数。
- en: 'Accuracy is defined as the ratio of the correctly classified instances over
    the total number of instances [[50](#bib.bib50)]. It is formally defined as:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率定义为正确分类的实例与总实例数的比率[[50](#bib.bib50)]。它的正式定义如下：
- en: '|  | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN},$ |  | (1) |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN},$ |  | (1) |'
- en: 'where $TP$ (true positive) is the number of positive instances (e.g., having
    DR) in the considered dataset that are correctly classified, $TN$ (true negative)
    is the number of negative instances (e.g., normal cases) in the considered dataset
    that are correctly classified, and $FP$ (false positive) and $FN$ (false negative)
    are the numbers of positive and negative instances that are incorrectly classified,
    respectively. Note that in detecting DR, an instance is either a fundus image,
    a patch or a pixel of a fundus image, depending on the task. Sensitivity(SN),
    or the true positive rate or recall, measures the fraction of correctly classified
    positive instances; specificity(SP), or the true negative rate, measures the fraction
    of correctly classified negative instances; and precision, or positive predictive
    value, measures the fraction of positive instances that are correctly classified.
    They are formally defined as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $TP$（真正例）是指在考虑的数据集中被正确分类的正实例数量（例如，具有糖尿病视网膜病变），$TN$（真负例）是指在考虑的数据集中被正确分类的负实例数量（例如，正常病例），$FP$（假正例）和
    $FN$（假负例）分别是指被错误分类的正实例和负实例的数量。请注意，在检测糖尿病视网膜病变时，实例可以是视网膜图像、视网膜图像的补丁或视网膜图像的像素，具体取决于任务。敏感性（SN），即真正例率或召回率，衡量正确分类的正实例的比例；特异性（SP），即真负例率，衡量正确分类的负实例的比例；而精确度，即正预测值，衡量正确分类的正实例的比例。它们的正式定义如下：
- en: '|  | $Sensitivity(Recall)=\frac{TP}{TP+FN}$ |  | (2) |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | $Sensitivity(Recall)=\frac{TP}{TP+FN}$ |  | (2) |'
- en: '|  | $Specificity=\frac{TN}{TN+FP}$ |  | (3) |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $Specificity=\frac{TN}{TN+FP}$ |  | (3) |'
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (4) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (4) |'
- en: '$\textit{F-score}(F)$ combines precision and recall as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: $\textit{F-score}(F)$ 结合了精确度和召回率，如下所示：
- en: '|  | $F=2\frac{Precision\times Recall}{Precision+Recall}$ |  | (5) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $F=2\frac{Precision\times Recall}{Precision+Recall}$ |  | (5) |'
- en: 'The receiver operating characteristic (ROC) curve represents the plot of the
    true positive rate against the false positive rate. It shows the relationship
    between sensitivity and specificity. The area under the ROC curve (AUC) is also
    used as a performance metric and takes values between 0 and 1; the closer the
    AUC is to 1, the better the performance. Logarithmic loss (log loss) determines
    a classifier’s accuracy by penalizing false classifications. To find log loss,
    the classifier must assign a probability to each class, instead of presenting
    the most likely class. It is given by:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 接收器操作特征 (ROC) 曲线表示真正例率与假正例率的图示。它显示了灵敏度与特异性的关系。ROC 曲线下面积 (AUC) 也用作性能指标，取值范围在
    0 到 1 之间；AUC 越接近 1，性能越好。对数损失 (log loss) 通过惩罚错误分类来确定分类器的准确性。要计算对数损失，分类器必须为每个类别分配一个概率，而不是仅仅呈现最可能的类别。其计算公式为：
- en: '|  | $logloss=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M}y_{ij}logp_{ij},$ |  |
    (6) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | $logloss=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M}y_{ij}logp_{ij},$ |  |
    (6) |'
- en: 'where N is the number of samples, $M$ is the number of labels, $y_{ij}$ is
    a binary indicator of whether label $j$ is the correct classification for instance
    $i$, and $p_{ij}$ is the model’s probability of assigning label $j$ to instance
    $i$. As segmentation is also a kind of classification at the pixel level, the
    metrics defined for classification can be used for segmentation. Additional metrics
    used for measuring the performance of segmentation algorithms include overlapping
    error, intersection over union and the dice similarity coefficient. Intersection
    over union (IOU) is defined as follows [[51](#bib.bib51)]:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 是样本数量，$M$ 是标签数量，$y_{ij}$ 是标签 $j$ 是否为实例 $i$ 的正确分类的二值指示器，而 $p_{ij}$ 是模型将标签
    $j$ 分配给实例 $i$ 的概率。由于分割在像素级别上也是一种分类，因此定义用于分类的指标可以用于分割。用于衡量分割算法性能的附加指标包括重叠误差、交并比和骰子相似系数。交并比（IOU）定义如下
    [[51](#bib.bib51)]：
- en: '|  | $IOU=\frac{Area(A\cap G)}{Area(A\cup G)}$ |  | (7) |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | $IOU=\frac{Area(A\cap G)}{Area(A\cup G)}$ |  | (7) |'
- en: 'Overlapping error is obtained by:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 重叠误差通过以下方式获得：
- en: '|  | $E=1-IOU$ |  | (8) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | $E=1-IOU$ |  | (8) |'
- en: where $A$ is the notation for segmentation of the output and $G$ indicates the
    manual ground truth segmentation [[52](#bib.bib52)].
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 是输出的分割符号，$G$ 表示手动标注的真实分割 [[52](#bib.bib52)]。
- en: 'Boundary-based evaluation (B) is the absolute pointwise localization error
    obtained by measuring the distance between two closed boundary curves. Let $C_{g}$
    be the boundary of ground truth and $C_{a}$ be the boundary obtained from a method.
    The distance $D$ between two curves is defined as (in pixels):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 基于边界的评估 (B) 是通过测量两个闭合边界曲线之间的距离得到的绝对逐点定位误差。设 $C_{g}$ 为真实边界，$C_{a}$ 为从某方法得到的边界。两个曲线之间的距离
    $D$ 定义为（以像素为单位）：
- en: '|  | $B=\frac{1}{n}\sum_{\theta=1}^{\theta_{n}}\sqrt{(d_{g}^{\theta})^{2}-(d_{a}^{\theta})^{2}},$
    |  | (9) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | $B=\frac{1}{n}\sum_{\theta=1}^{\theta_{n}}\sqrt{(d_{g}^{\theta})^{2}-(d_{a}^{\theta})^{2}},$
    |  | (9) |'
- en: where $d_{g}^{\theta}$ and $d_{a}^{\theta}$ are the distance from the centroid
    of the curve to points on $C_{g}$ and $C_{a}$ in the direction of $\theta$ and
    $n$ is the total number of angular samples. The distance between the calculated
    boundary and ground truth should ideally be close to zero [[18](#bib.bib18)].
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{g}^{\theta}$ 和 $d_{a}^{\theta}$ 是从曲线中心到 $C_{g}$ 和 $C_{a}$ 上的点在 $\theta$
    方向的距离，而 $n$ 是角度样本的总数。计算得到的边界与真实边界之间的距离理想情况下应接近零 [[18](#bib.bib18)]。
- en: 'An alternative to overlapping error that is used for DR detection is the dice
    similarity coefficient (DSC) or overlap index, which is defined by [[53](#bib.bib53)]:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 DR 检测的重叠误差的替代方法是骰子相似系数 (DSC) 或重叠指数，其定义如下 [[53](#bib.bib53)]：
- en: '|  | $DSC=\frac{2TP}{2TP+FP+FN}$ |  | (10) |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | $DSC=\frac{2TP}{2TP+FP+FN}$ |  | (10) |'
- en: The DSC takes values between 0 and 1; the closer the DSC is to 1, the better
    the segmentation results are. Region precision recall (RPR) is commonly used to
    assess edge or boundary detection outcomes based on region overlapping. It refers
    to the segmentation quality in a precision recall space [[54](#bib.bib54)].
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: DSC 的值在 0 和 1 之间；DSC 越接近 1，分割结果越好。区域精度召回 (RPR) 通常用于基于区域重叠评估边缘或边界检测结果。它指的是在精度召回空间中的分割质量
    [[54](#bib.bib54)]。
- en: 4 Overview of Deep Learning
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习概述
- en: Various DL-based architectures have been introduced. Some commonly employed
    deep architectures for various DR-detection include convolutional neural networks
    (CNNs), autoencoders (AEs), recurrent neural networks (RNNs) and deep belief networks
    (DBNs). In the following paragraphs, we give an overview of these architectures.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 各种基于深度学习的架构已经被引入。用于各种糖尿病视网膜病变（DR）检测的常见深度架构包括卷积神经网络（CNNs）、自编码器（AEs）、递归神经网络（RNNs）和深度置信网络（DBNs）。在接下来的段落中，我们将概述这些架构。
- en: 4.1 Convolutional Neural Networks
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 卷积神经网络
- en: 'CNNs simulate the human visual system and have been widely employed for various
    computer vision tasks. They mainly consist of three types of layers: convolutional,
    pooling and fully connected (FC). Convolutional layers employ a convolution operation
    to encode local spatial information and then FC layers to encode the global information.
    Commonly used CNN models include AlexNet, VGGNet, GoogLeNet and ResNet. A CNN
    model is taught in an end-to-end manner; it learns the hierarchy of features automatically
    and results in outstanding classification performance. Initial CNN models such
    as LeNet [[55](#bib.bib55)] and AlexNet [[56](#bib.bib56)] contain few layers.
    In 2014, Simonyan and Zisserman [[57](#bib.bib57)] explored a deeper CNN model
    called VGGNet, which contains 19 layers, and found that depth is crucial for better
    performance. Motivated by these findings, deeper models such as GoogLeNet, Inception
    [[58](#bib.bib58)] and ResNet [[59](#bib.bib59)] have been proposed, which have
    shown amazing performance in many computer vision tasks. An end-to-end model usually
    means a deep model that takes inputs and gives outputs. Transfer learning means
    that a model is first taught in an end-to-end fashion using a dataset from a related
    domain and then fine-tuned using the dataset from the domain. Learning a CNN model
    requires a very large amount of data to overcome overfitting problems and ensure
    proper convergence [[60](#bib.bib60)], but large amounts of data are not available
    in the medical domain, particularly for DR detection. The solution is to use transfer
    learning [[61](#bib.bib61)]. Generally, two strategies of transfer learning are
    used: (i) using a pre-trained CNN model as a feature extractor and (ii) fine-tuning
    a pre-trained CNN model using data from the relevant domain. A fully convolutional
    network (FCN) is a version of a CNN model in which FC layers are converted into
    convolutional layers and deconvolution (or transposed convolution) layers are
    added to undo the effect of down-sampling during the convolutional layers and
    to obtain an output map of the same size as the input image [[62](#bib.bib62)].
    This model is commonly used for segmentation.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 模拟人类视觉系统，已广泛应用于各种计算机视觉任务。它们主要由三种类型的层组成：卷积层、池化层和全连接（FC）层。卷积层使用卷积操作来编码局部空间信息，然后使用
    FC 层来编码全局信息。常用的 CNN 模型包括 AlexNet、VGGNet、GoogLeNet 和 ResNet。CNN 模型采用端到端的方式进行训练；它自动学习特征层次结构，从而实现卓越的分类性能。最初的
    CNN 模型，如 LeNet [[55](#bib.bib55)] 和 AlexNet [[56](#bib.bib56)] 包含较少的层。2014 年，Simonyan
    和 Zisserman [[57](#bib.bib57)] 探索了一个更深的 CNN 模型 VGGNet，该模型包含 19 层，发现深度对更好的性能至关重要。受这些发现的启发，提出了更深的模型，如
    GoogLeNet、Inception [[58](#bib.bib58)] 和 ResNet [[59](#bib.bib59)]，这些模型在许多计算机视觉任务中表现出色。端到端模型通常意味着一个深度模型，它接收输入并给出输出。迁移学习意味着模型首先使用来自相关领域的数据集进行端到端的训练，然后使用来自该领域的数据集进行微调。学习一个
    CNN 模型需要大量的数据以克服过拟合问题并确保适当的收敛 [[60](#bib.bib60)]，但在医学领域，特别是糖尿病视网膜病变检测中，大量数据并不可用。解决方案是使用迁移学习
    [[61](#bib.bib61)]。一般来说，迁移学习使用两种策略：（i）使用预训练的 CNN 模型作为特征提取器，（ii）使用相关领域的数据对预训练的
    CNN 模型进行微调。全卷积网络（FCN）是 CNN 模型的一个版本，其中 FC 层被转换为卷积层，并添加了反卷积（或转置卷积）层，以撤销卷积层中的下采样效果，并获得与输入图像相同大小的输出图
    [[62](#bib.bib62)]。该模型通常用于分割。
- en: 4.2 Autoencoder-based and Stacked Autoencoder Methods
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于自编码器和堆叠自编码器的方法
- en: 'An autoencoder (AE) is a single hidden layer neural network with the same input
    and output [[63](#bib.bib63)] and is used to build a stacked-autoencoder (SAE),
    a deep architecture [[64](#bib.bib64)]. The training of an SAE model consists
    of two phases: pre-training and fine-tuning. In the pre-training phase, an SAE
    is trained layer by layer in an unsupervised way. In the fine-tuning phase, the
    pre-trained SAE model is fine-tuned using gradient descent and backpropagation
    algorithms in a supervised way. An autoencoder is the basic building block of
    an SAE. There two main types of autoencoders: sparse and denoising. Sparse autoencoders
    are a type of autoencoder that tends to force the extracting of sparse features
    from raw data. The sparsity of the representation can either be achieved by penalizing
    hidden unit biases or by directly penalizing the output of hidden unit activations.
    Denoising autoencoders (DAEs) have also been used in DR detection Maji et al.
    [[65](#bib.bib65)] due its robustness in recovering the corrupted input and force
    the model to capture the correct version.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（AE）是一个具有相同输入和输出的单隐藏层神经网络 [[63](#bib.bib63)]，用于构建堆叠自编码器（SAE），一种深度架构 [[64](#bib.bib64)]。SAE
    模型的训练包括两个阶段：预训练和微调。在预训练阶段，SAE 以无监督的方式逐层训练。在微调阶段，预训练的 SAE 模型通过监督方式使用梯度下降和反向传播算法进行微调。自编码器是
    SAE 的基本构建块。自编码器主要有两种类型：稀疏自编码器和去噪自编码器。稀疏自编码器是一种自编码器，倾向于从原始数据中提取稀疏特征。表示的稀疏性可以通过惩罚隐藏单元偏置或直接惩罚隐藏单元激活的输出来实现。去噪自编码器（DAE）也已被用于
    DR 检测 Maji 等人 [[65](#bib.bib65)]，由于其在恢复损坏输入方面的鲁棒性，并强制模型捕捉正确的版本。
- en: 4.3 Recurrent Neural Networks
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 循环神经网络
- en: RNNs are a type of neural network that learns the context as well along with
    input patterns. It learns the output of the previous iterations and combines it
    with the current input to yield an output; in this way, an RNN is able to influence
    itself through recurrences. An RNN model usually contains three sets of parameters—input
    to hidden weights $W$, hidden weights $U$ and hidden weights—to output$V$ where
    weights are shared across position/time of input sequence [[66](#bib.bib66)].
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 是一种学习上下文及输入模式的神经网络。它学习先前迭代的输出，并将其与当前输入结合以产生输出；通过这种方式，RNN 能够通过递归影响自身。RNN
    模型通常包含三组参数——输入到隐藏层的权重 $W$，隐藏层权重 $U$ 和隐藏层到输出的权重 $V$，其中权重在输入序列的位置/时间上是共享的 [[66](#bib.bib66)]。
- en: 4.4 Deep Belief Networks
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 深度置信网络
- en: A DBN [[67](#bib.bib67)] is a deep network architecture that is built with cascading
    restricted Boltzmann machines (RBMs). An RBM is taught using a contrastive divergence
    algorithm in such a way that maximizes the similarity (in the sense of probability)
    between the input and its projection. The involvement of probability as a similarity
    measure prevents degenerate solutions and makes DBNs a probabilistic model. Just
    like SAEs, DBNs are first pre-trained in an unsupervised way using a layer-by-layer
    greedy learning strategy; then, it is fine-tuned using gradient descent and backpropagation
    algorithms.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: DBN [[67](#bib.bib67)] 是一种深度网络架构，由级联的限制玻尔兹曼机（RBM）构建而成。RBM 通过对比散度算法进行训练，以最大化输入与其投影之间的相似性（在概率的意义上）。概率作为相似性度量的参与防止了退化解，并使
    DBN 成为一种概率模型。与 SAE 类似，DBN 首先以无监督的方式通过逐层贪婪学习策略进行预训练；然后，使用梯度下降和反向传播算法进行微调。
- en: 5 Literature Survey
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 文献综述
- en: 'Based on the clinical importance of DR detection tasks, we categorize them
    into four categories: (i) retinal blood vessel segmentation, (ii) optic disk localization
    and segmentation, (iii) lesion detection and classification, and (iv) image-level
    DR diagnosis for referral. In the following sub-sections, we review the state-of-the-art
    DL-based algorithms for these tasks.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 DR 检测任务的临床重要性，我们将其分为四类：（i）视网膜血管分割，（ii）视盘定位和分割，（iii）病变检测和分类，以及（iv）图像级 DR 诊断以供转诊。在以下子章节中，我们回顾了这些任务的最先进的基于深度学习的算法。
- en: 5.1 Retinal Blood Vessel Segmentation
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 视网膜血管分割
- en: It is very important to identify changes in fine retinal blood vessels for preventing
    vision impairment due to pathological retinal damage. The segmentation of retinal
    blood vessels is challenging due to their low contrast, variations in their morphology
    against a noisy background and the presence of pathologies like MAs and HMs. Different
    learning approaches have been applied to segment retinal blood vessels. In the
    following paragraphs, we review these methods based on DL approaches.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 识别细小视网膜血管的变化对于防止由于病理性视网膜损伤导致的视力损害是非常重要的。由于血管的对比度低、形态在噪声背景下的变化以及如MA和HM等病理存在，视网膜血管的分割具有挑战性。不同的学习方法已被应用于视网膜血管的分割。在接下来的段落中，我们将回顾这些基于深度学习的方法。
- en: 5.1.1 Convolutional Neural Networks
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 卷积神经网络
- en: Many retinal blood vessel segmentation algorithms based on CNN models have been
    proposed. Maji et al. [[68](#bib.bib68)] employed 12 CNN models to segment vessel
    and non-vessel pixels. Each CNN model consists of three convolutional layers and
    two fully connected layers. For evaluation, they used the DRIVE dataset.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN模型的许多视网膜血管分割算法已经被提出。Maji等人[[68](#bib.bib68)]使用了12个CNN模型来分割血管和非血管像素。每个CNN模型包括三个卷积层和两个全连接层。在评估中，他们使用了DRIVE数据集。
- en: Liskowski and Krawiec [[69](#bib.bib69)] proposed a pixel-wise supervised vessel-segmentation
    method based on deep CNN, which is trained using fundus images that have been
    pre-processed with global contrast normalization and zero-phase whitening, and
    augmented using geometric transformations and gamma corrections. They used the
    DRIVE, STARE and CHASE datasets to evaluate the system. It is robust against the
    central vessel reflex and sensitive in detecting fine vessels.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Liskowski和Krawiec[[69](#bib.bib69)]提出了一种基于深度CNN的像素级监督血管分割方法，该方法使用经过全局对比度归一化和零相位白化处理的眼底图像进行训练，并使用几何变换和伽马校正进行了增强。他们使用DRIVE、STARE和CHASE数据集来评估该系统。该方法对中心血管反射具有鲁棒性，并在检测细小血管方面敏感。
- en: Maninis et al. [[70](#bib.bib70)] formulated the retinal blood vessel segmentation
    problem as an image-to-image regression task, for which they employed pre-trained
    VGG, which was modified by removing FC layers and incorporating additional convolutional
    layers after the first four convolution blocks of VGG before pooling the layers.
    The additional convolutional layers are upsampled to the same size as the image,
    trained and concatenated into a volume. They used DRIVE and STARE for evaluation.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Maninis等人[[70](#bib.bib70)]将视网膜血管分割问题制定为图像到图像的回归任务，为此他们采用了经过预训练的VGG，并通过去除全连接层和在VGG的前四个卷积块后加入额外的卷积层来对VGG进行修改。在池化层之前，这些额外的卷积层会被上采样到与图像相同的尺寸，经过训练后与体积连接起来。他们使用了DRIVE和STARE进行评估。
- en: Wu et al. [[71](#bib.bib71)] first extracted discriminative features using a
    CNN and then used nearest neighbor search based on principal component analysis
    (PCA) to estimate the local structure distribution, which was finally employed
    by the generalized probabilistic tracking framework to segment blood vessels.
    This method was evaluated using the DRIVE dataset.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: Wu等人[[71](#bib.bib71)]首先使用CNN提取判别特征，然后基于主成分分析（PCA）进行最近邻搜索，以估计局部结构分布，最后由广义概率跟踪框架用于分割血管。这种方法使用DRIVE数据集进行了评估。
- en: Dasgupta and Singh [[72](#bib.bib72)] used an FCN combined with structured prediction
    to segment blood vessels, which they assumed to be a multi-label inference task.
    The green channel of the images was preprocessed by normalization, contrast, gamma
    adjustment and scaling the intensity value between 0 and 1\. They used DRIVE to
    evaluate the method’s performance.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Dasgupta和Singh[[72](#bib.bib72)]结合了FCN和结构化预测来分割血管，他们将其假设为一个多标签推断任务。图像的绿色通道经过了归一化、对比度调整、伽马校正和将强度值缩放到0和1之间的预处理。他们使用DRIVE来评估该方法的性能。
- en: Tan et al. [[73](#bib.bib73)] proposed a seven-layer CNN model to simultaneously
    segment blood vessels, OD and fovea. After normalizing the colored images, they
    formulated the segmentation problem as a classification problem assuming four
    classes—blood vessels, OD, fovea and background—and classified each pixel by taking
    a neighborhood of 25$\times$25 pixels. This is very time consuming because each
    pixel is classified independently, with as many passes made through the net as
    the number of pixels. Its performance was evaluated with the DRIVE dataset.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Tan 等人 [[73](#bib.bib73)] 提出了一个七层 CNN 模型，用于同时分割血管、视盘（OD）和黄斑（fovea）。在对彩色图像进行归一化之后，他们将分割问题形式化为一个分类问题，假设有四个类别——血管、视盘、黄斑和背景——并通过取
    25$\times$25 像素的邻域对每个像素进行分类。这种方法非常耗时，因为每个像素是独立分类的，每个像素都需要经过与像素数量相同次数的网络传递。该方法的性能通过
    DRIVE 数据集进行了评估。
- en: Fu et al. [[74](#bib.bib74)] similarly formulated the blood vessel segmentation
    problem as a boundary-detection task and proposed a method for this task by integrating
    FCN and fully connected conditional random field (CRF). First, a vessel probability
    map is created using FCN, and then the vessels are segmented by combining the
    vessel probability map and long-range interactions between pixels using CRF. This
    method was validated on the DRIVE and STARE datasets.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Fu 等人 [[74](#bib.bib74)] 同样将血管分割问题形式化为边界检测任务，并通过结合 FCN 和全连接条件随机场（CRF）提出了该任务的方法。首先，使用
    FCN 创建一个血管概率图，然后通过结合血管概率图和 CRF 中像素之间的长程交互来分割血管。该方法在 DRIVE 和 STARE 数据集上进行了验证。
- en: Mo and Zhang [[75](#bib.bib75)] used an FCN and incorporated some auxiliary
    classifiers in intermediate layers to make the features more discriminative in
    lower layers. To overcome the small number of available samples, they used transfer
    learning to train the FCN model. They evaluated the system on DRIVE, STARE and
    CHASE.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Mo 和 Zhang [[75](#bib.bib75)] 使用了一个 FCN，并在中间层中加入了一些辅助分类器，以使低层特征更具判别性。为了克服可用样本数量少的问题，他们使用了迁移学习来训练
    FCN 模型。他们在 DRIVE、STARE 和 CHASE 数据集上对系统进行了评估。
- en: 'The performance analysis of all the aforementioned methods is given in Table
    [3](#S5.T3 "Table 3 ‣ 5.1.3 Recurrent Neural Network-Based Methods ‣ 5.1 Retinal
    Blood Vessel Segmentation ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided
    Diagnosis Systems for Diabetic Retinopathy: A Survey"). This analysis indicates
    that among all CNN-based methods, the one by Liskowski and Krawiec [[69](#bib.bib69)]
    performed better than all other methods in terms of accuracy, AUC and sensitivity.
    This method may outperform due to the preprocessing of fundus images and training
    of the CNN model using an augmented dataset. All of the other methods use pre-trained
    CNN models without preprocessing or augmentation. Against expectations, the ensemble
    of CNN models by Maji et al. [[68](#bib.bib68)] did not perform better than the
    other CNN-based methods because there is no preprocessing and augmentation of
    the training dataset.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S5.T3 "Table 3 ‣ 5.1.3 Recurrent Neural Network-Based Methods ‣ 5.1
    Retinal Blood Vessel Segmentation ‣ 5 Literature Survey ‣ Deep Learning based
    Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey") 给出了上述所有方法的性能分析。该分析表明，在所有基于
    CNN 的方法中，Liskowski 和 Krawiec [[69](#bib.bib69)] 的方法在准确性、AUC 和灵敏度方面表现优于所有其他方法。这种方法可能由于视网膜图像的预处理和使用增强数据集训练
    CNN 模型而表现更好。所有其他方法都使用了没有预处理或增强的预训练 CNN 模型。与预期相反，Maji 等人 [[68](#bib.bib68)] 的 CNN
    模型集成方法由于没有对训练数据集进行预处理和增强，未能优于其他基于 CNN 的方法。'
- en: 5.1.2 Stacked Autoencoder-Based Methods
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 基于堆叠自编码器的方法
- en: Some methods employ SAEs in different ways to segment vessels. The method proposed
    by Maji et al. [[65](#bib.bib65)] uses a hybrid DL architecture, which consists
    of unsupervised stacked denoising autoencoders (SDAEs), to segment vessels in
    fundus images. The structure of the first DAE consists of 400 hidden neurons,
    and the second DAE contains 100 hidden neurons. SDAE learns features, which are
    classified using random forest (RF). This approach segments vessels using patches
    of size k × k around each pixel in the green channel. They used DRIVE to assess
    the method.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法以不同的方式使用 SAE 来分割血管。Maji 等人 [[65](#bib.bib65)] 提出的 方法使用了一种混合的深度学习架构，包括无监督的堆叠去噪自编码器（SDAEs），用于在视网膜图像中分割血管。第一个
    DAE 的结构包括 400 个隐藏神经元，第二个 DAE 包含 100 个隐藏神经元。SDAE 学习特征，这些特征使用随机森林（RF）进行分类。该方法通过在绿色通道中围绕每个像素的
    k × k 大小的补丁来分割血管。他们使用 DRIVE 来评估该方法。
- en: 'Roy and Sheet [[76](#bib.bib76)] introduced an SAE-based deep neural network
    (SAE-DNN) model for vessel segmentation that employs the domain adaptation (DA)
    approach for its training. SAE-DNN consists of two hidden layers, which are trained
    using the source domain (DRIVE dataset), using an auto-encoding mechanism and
    supervised learning. Then, DA is applied in two stages: unsupervised weight adaptation
    and supervised fine-tuning. In unsupervised weight adaptation, hidden nodes of
    the SAE-DNN are re-trained using unlabeled samples from the target domain (STARE
    dataset) with the auto-encoding mechanism using systematic node dropouts, whereas
    in supervised fine-tuning, the SAE-DNN is fine-tuned using a small number of labeled
    samples from the target domain. The results show that domain DA improves the performance
    of the SAE-DNN.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: Roy 和 Sheet [[76](#bib.bib76)] 介绍了一种基于 SAE 的深度神经网络（SAE-DNN）模型用于血管分割，该模型采用领域适应（DA）方法进行训练。SAE-DNN
    由两层隐藏层组成，这些层使用源领域（DRIVE 数据集）进行训练，采用自编码机制和监督学习。然后，DA 分两个阶段应用：无监督权重适应和监督微调。在无监督权重适应阶段，SAE-DNN
    的隐藏节点使用来自目标领域（STARE 数据集）的未标记样本进行重新训练，采用系统的节点丢弃机制；在监督微调阶段，SAE-DNN 使用来自目标领域的小量标记样本进行微调。结果显示领域
    DA 改善了 SAE-DNN 的性能。
- en: Li et al. [[28](#bib.bib28)] proposed segmenting retinal vessels from the green
    channel using a supervised DL approach that labels the patch of a pixel instead
    of a single pixel. In this approach, the vessel-segmentation problem is modeled
    as a cross-modality data transformation that transforms a retinal image to a vessel
    map and is defined using a deep neural network consisting of DAEs. They assessed
    the performance on DRIVE, STARE and CHASE (28 images).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 [[28](#bib.bib28)] 提出了使用监督式深度学习方法从绿色通道分割视网膜血管，该方法对像素块而非单个像素进行标记。在这种方法中，血管分割问题被建模为一种跨模态数据转换，将视网膜图像转换为血管图，并使用由深度自编码器组成的深度神经网络来定义。他们在
    DRIVE、STARE 和 CHASE（28 张图像）上评估了性能。
- en: Lahiri et al. [[77](#bib.bib77)] used a two-level ensemble of stacked denoised
    autoencoder networks (SDAEs). In the first-level ensemble, a network (E-net) consists
    of $n$ SDAEs composed of the same structure; each SDAE contains two hidden layers
    and is followed by a Softmax classifier; SDAEs are trained on bootstrap training
    samples using an auto-encoding mechanism in parallel, to produce probabilistic
    image maps, which are conglomerated using a fusion strategy. In the second level
    of the ensemble, to introduce further diversity, decisions from two E-nets having
    different architectures are merged using the convex weighted average. The authors
    used the DRIVE dataset to evaluate the method.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: Lahiri 等人 [[77](#bib.bib77)] 使用了一个由堆叠去噪自编码器网络（SDAEs）组成的两级集成。在第一层集成中，一个网络（E-net）包含
    $n$ 个具有相同结构的 SDAEs；每个 SDAE 包含两个隐藏层，并跟随一个 Softmax 分类器；SDAEs 在引导训练样本上使用自编码机制并行训练，以生成概率图像图，这些图像图通过融合策略进行合并。在第二层集成中，为了引入更多的多样性，将两个具有不同架构的
    E-nets 的决策通过凸加权平均合并。作者使用 DRIVE 数据集评估了该方法。
- en: 5.1.3 Recurrent Neural Network-Based Methods
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 基于递归神经网络的方法
- en: Fu et al. [[78](#bib.bib78)] formulated the blood vessel segmentation problem
    as a boundary detection task and proposed the DeepVessel method by integrating
    CNN and CRF as an RNN and evaluated it on the DRIVE, STARE, and CHASE datasets.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: Fu 等人 [[78](#bib.bib78)] 将血管分割问题形式化为边界检测任务，并通过将 CNN 和 CRF 作为 RNN 集成提出了 DeepVessel
    方法，并在 DRIVE、STARE 和 CHASE 数据集上进行了评估。
- en: 'A performance analysis of the aforementioned methods is given in Table [3](#S5.T3
    "Table 3 ‣ 5.1.3 Recurrent Neural Network-Based Methods ‣ 5.1 Retinal Blood Vessel
    Segmentation ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey"). This analysis indicates that among
    all SAE-based methods, the methods based on cross-modality transformation [[28](#bib.bib28)]
    and two-level ensemble of SAEs [[77](#bib.bib77)] outperform SAE-based methods
    in terms of accuracy. Although the method based on two-level ensemble of SAEs
    [[77](#bib.bib77)] performed slightly better than the method based on cross-modality
    transformation [[28](#bib.bib28)], the difference was not significant. Interestingly,
    there was no noticeable difference in the performance of methods based on CNNs
    and on SAEs in terms of accuracy. CNN models involve much more learnable parameters
    than SAE models and as such are prone to overfitting. CNN models can do better
    provided a huge labeled dataset is available or novel augmentation techniques
    are introduced.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 对上述方法的性能分析见表 [3](#S5.T3 "表 3 ‣ 5.1.3 基于递归神经网络的方法 ‣ 5.1 视网膜血管分割 ‣ 5 文献综述 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")。这一分析表明，在所有
    SAE 基于的方法中，基于跨模态转换 [[28](#bib.bib28)] 和 SAE 的两级集成 [[77](#bib.bib77)] 的方法在准确性方面优于
    SAE 基于的方法。尽管基于 SAE 两级集成 [[77](#bib.bib77)] 的方法的表现略优于基于跨模态转换 [[28](#bib.bib28)]
    的方法，但差异不显著。有趣的是，基于 CNN 和 SAE 的方法在准确性方面没有明显差异。CNN 模型涉及的可学习参数比 SAE 模型多得多，因此更容易过拟合。CNN
    模型在有大量标记数据集或引入新的增强技术时可以表现得更好。
- en: 'Table 3: Representative of works in diabetic retinopathy (DR) vessels detection'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：糖尿病视网膜病变（DR）血管检测的代表性工作
- en: '| Research study | Segmentation Method | Training | Dataset | Performance |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 研究研究 | 分割方法 | 训练 | 数据集 | 性能 |'
- en: '| CNN-Based Methods |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 基于 CNN 的方法 |'
- en: '| Maji et al. [[68](#bib.bib68)] | Patch-based ensemble of CNN models | End-to-end
    | DRIVE |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| Maji 等 [[68](#bib.bib68)] | 基于补丁的 CNN 模型集成 | 端到端 | DRIVE |'
- en: '&#124; AUC=0.9283 &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.9283 &#124;'
- en: '&#124; ACC= 94.7 &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC= 94.7 &#124;'
- en: '|'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Liskowski and Krawiec [[69](#bib.bib69)] | Patch-based CNN | End-to-end |
    DRIVE |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| Liskowski 和 Krawiec [[69](#bib.bib69)] | 基于补丁的 CNN | 端到端 | DRIVE |'
- en: '&#124; SN= 98.07, SP=78.11 &#124;'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN= 98.07, SP=78.11 &#124;'
- en: '&#124; AUC=0.9790 &#124;'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.9790 &#124;'
- en: '&#124; ACC=95.35 &#124;'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=95.35 &#124;'
- en: '|'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| STARE |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| STARE |'
- en: '&#124; SN=85.54, SP=98.62 &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=85.54, SP=98.62 &#124;'
- en: '&#124; AUC=0.9928 &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.9928 &#124;'
- en: '&#124; ACC=97.29 &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=97.29 &#124;'
- en: '|'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CHASE |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| CHASE |'
- en: '&#124; SN=81.54, SP=98.66 &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=81.54, SP=98.66 &#124;'
- en: '&#124; AUC=0.988 &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.988 &#124;'
- en: '&#124; ACC=96.96 &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=96.96 &#124;'
- en: '|'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Maninis et al. [[70](#bib.bib70)] | FCN | Transfer learning | DRIVE | RPR=0.822
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| Maninis 等 [[70](#bib.bib70)] | FCN | 迁移学习 | DRIVE | RPR=0.822 |'
- en: '| STARE | RPR=0.831 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| STARE | RPR=0.831 |'
- en: '| Wu et al. [[71](#bib.bib71)] | Vessel tracking/patch-based CNN/PCA as classifier
    | End-to-end | DRIVE | AUC=0.9701 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等 [[71](#bib.bib71)] | 血管跟踪/基于补丁的 CNN/PCA 作为分类器 | 端到端 | DRIVE | AUC=0.9701
    |'
- en: '| Dasgupta and Singh [[72](#bib.bib72)] | Patch-based FCN | End-to-end | DRIVE
    |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Dasgupta 和 Singh [[72](#bib.bib72)] | 基于补丁的 FCN | 端到端 | DRIVE |'
- en: '&#124; SN=76.91 &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=76.91 &#124;'
- en: '&#124; SP=98.01 &#124;'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SP=98.01 &#124;'
- en: '&#124; AUC=0.974 &#124;'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.974 &#124;'
- en: '&#124; ACC=95.33 &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=95.33 &#124;'
- en: '|'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Tan et al. [[73](#bib.bib73)] | Patch-based seven-layers CNN | End-to-end
    | DRIVE |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Tan 等 [[73](#bib.bib73)] | 基于补丁的七层 CNN | 端到端 | DRIVE |'
- en: '&#124; SN=75.37 &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=75.37 &#124;'
- en: '&#124; SP=96.94 &#124;'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SP=96.94 &#124;'
- en: '|'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Fu et al. [[74](#bib.bib74)] | FCN/CRF | End-to-end | DRIVE |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| Fu 等 [[74](#bib.bib74)] | FCN/CRF | 端到端 | DRIVE |'
- en: '&#124; SN=72.94, ACC=94.70 &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=72.94, ACC=94.70 &#124;'
- en: '|'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| STARE |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| STARE |'
- en: '&#124; SN=71.40, ACC=95.45 &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=71.40, ACC=95.45 &#124;'
- en: '|'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Mo and Zhang [[75](#bib.bib75)] | Multi-level FCN | End-to-end | DRIVE |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Mo 和 Zhang [[75](#bib.bib75)] | 多级 FCN | 端到端 | DRIVE |'
- en: '&#124; SN=77.79, SP=97.80 &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=77.79, SP=97.80 &#124;'
- en: '&#124; AUC=0.9782 &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.9782 &#124;'
- en: '&#124; ACC=95.21 &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=95.21 &#124;'
- en: '|'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| STARE |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| STARE |'
- en: '&#124; SN=81.47, SP=98.44 &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=81.47, SP=98.44 &#124;'
- en: '&#124; AUC=0.9885 &#124;'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.9885 &#124;'
- en: '&#124; ACC=96.76 &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=96.76 &#124;'
- en: '|'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CHASE |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| CHASE |'
- en: '&#124; SN=76.61, SP=98.16 &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=76.61, SP=98.16 &#124;'
- en: '&#124; AUC=0.9812 &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.9812 &#124;'
- en: '&#124; ACC=95.99 &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=95.99 &#124;'
- en: '|'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| AE-Based Methods |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 基于 AE 的方法 |'
- en: '| Maji et al. [[65](#bib.bib65)] | Patch-based SDAE/RF | Transfer learning
    | DRIVE |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| Maji 等 [[65](#bib.bib65)] | 基于补丁的 SDAE/RF | 迁移学习 | DRIVE |'
- en: '&#124; AUC=0.9195, ACC=93.27 &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.9195, ACC=93.27 &#124;'
- en: '|'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Roy and Sheet [[76](#bib.bib76)] | Patch-based SAE | Transfer learning |
    STARE |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| Roy 和 Sheet [[76](#bib.bib76)] | 基于块的SAE | 迁移学习 | STARE |'
- en: '&#124; AUC=0.92 &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.92 &#124;'
- en: '&#124; logloss=0.18 &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; logloss=0.18 &#124;'
- en: '|'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Li et al. [[28](#bib.bib28)] | Patch-based SDAE | End-to-end | DRIVE |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人 [[28](#bib.bib28)] | 基于块的SDAE | 端到端 | DRIVE |'
- en: '&#124; SN=75.6, SP=98 &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=75.6, SP=98 &#124;'
- en: '&#124; AUC=0.9738 &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.9738 &#124;'
- en: '&#124; ACC=95.27 &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=95.27 &#124;'
- en: '|'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| STARE |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| STARE |'
- en: '&#124; SN=77.26, SP=98.79 &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=77.26, SP=98.79 &#124;'
- en: '&#124; ACC=96.28 &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=96.28 &#124;'
- en: '|'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CHASE (28 images) |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| CHASE (28 张图像) |'
- en: '&#124; SN=75.07, SP=97.93 &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=75.07, SP=97.93 &#124;'
- en: '&#124; AUC=0.9716 &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.9716 &#124;'
- en: '&#124; ACC=95.81 &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=95.81 &#124;'
- en: '|'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Lahiri et al. [[77](#bib.bib77)] | Patch-based DSAE | Transfer learning |
    DRIVE | ACC=95.3 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| Lahiri 等人 [[77](#bib.bib77)] | 基于块的DSAE | 迁移学习 | DRIVE | ACC=95.3 |'
- en: '| RNN-Based Methods |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 基于RNN的方法 |'
- en: '| Fu et al. [[78](#bib.bib78)] | Patch-based CNN/CRF as RNN | End-to-end |
    DRIVE |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| Fu 等人 [[78](#bib.bib78)] | 基于块的CNN/CRF作为RNN | 端到端 | DRIVE |'
- en: '&#124; SP=76.03 &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SP=76.03 &#124;'
- en: '&#124; ACC=95.23 &#124;'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=95.23 &#124;'
- en: '|'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| STARE |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| STARE |'
- en: '&#124; SP=74.12 &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SP=74.12 &#124;'
- en: '&#124; ACC=95.85 &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=95.85 &#124;'
- en: '|'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CHASE |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| CHASE |'
- en: '&#124; SP=71.30 &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SP=71.30 &#124;'
- en: '&#124; ACC=94.89 &#124;'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=94.89 &#124;'
- en: '|'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 5.2 Optic Disc Feature
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 视盘特征
- en: 'Detecting the OD can enhance DR detection and classification because its bright
    appearance can create confusion for other bright lesions such as EXs. OD detection
    involves two operations: (i) localizing and (ii) segmenting the OD. Both CNN and
    SAE models have been employed for OD detection. Some methods only localize the
    OD, which is basically an object-detection problem, and others localize and segment
    the OD, which is a segmentation problem, to identify the area of the OD along
    with its boundaries.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 检测OD可以增强DR的检测和分类，因为其明亮的外观可能会与其他明亮的病变（如EXs）产生混淆。OD检测包括两个操作：（i）定位和（ii）分割OD。CNN和SAE模型都已用于OD检测。一些方法仅定位OD，这基本上是一个物体检测问题，而其他方法则定位和分割OD，这是一种分割问题，以识别OD的区域及其边界。
- en: 5.2.1 Convolutional Neural Networks
  id: totrans-423
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 卷积神经网络
- en: 'Lim et al. [[79](#bib.bib79)] method was one of the earliest proposals to employ
    a nine-layer CNN model to segment the OD and OC. It involves four main phases:
    localizing the region around the OD, enhancing this region by exaggerating the
    relevant visual features, classifying the enhanced region at pixel-level using
    a CNN model to produce a probability map and finally segmenting this map to predict
    the disc and cup boundaries. It was assessed on MESSIDOR and SEED-DB.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: Lim 等人 [[79](#bib.bib79)] 的方法是最早采用九层CNN模型来分割OD和OC的提案之一。它包括四个主要阶段：定位OD周围的区域，通过放大相关视觉特征来增强该区域，使用CNN模型在像素级别分类增强区域以生成概率图，最后分割此图以预测盘和杯的边界。该方法在MESSIDOR和SEED-DB上进行了评估。
- en: Guo et al. [[80](#bib.bib80)] used a large pixel patch-based CNN in which the
    OC was segmented by classification of each pixel patch and postprocessing. They
    used the DRISHTI-GS dataset for training and testing. Similarly, Tan et al. [[73](#bib.bib73)]
    segmented the OD and vessels jointly; it has been reviewed in section vessel segmentation.
    Sevastopolsky [[81](#bib.bib81)] used the modified U-net convolutional network
    presented in [[82](#bib.bib82)] to segment both the OD and OC.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: Guo 等人 [[80](#bib.bib80)] 使用了基于大像素块的CNN，其中OC通过对每个像素块的分类和后处理进行分割。他们使用了DRISHTI-GS数据集进行训练和测试。类似地，Tan
    等人 [[73](#bib.bib73)] 共同分割了OD和血管；这在血管分割部分有回顾。Sevastopolsky [[81](#bib.bib81)]
    使用了在 [[82](#bib.bib82)] 中提出的改进版U-net卷积网络来分割OD和OC。
- en: 'Zilly et al. [[83](#bib.bib83)] ] introduced an OD and OC segmentation method
    based on a multi-scale two-layer CNN model that is trained with boosting. First,
    the region around the OD is cropped, down-sampled by a factor of 4, converted
    to L*a*b color space and normalized. Then, the region is processed by entropy
    filtering to identify the most discriminative points and is passed to the CNN
    model, which is trained using the gentle AdaBoost method. The logistic regression
    classifier produces a probability map from the output of the CNN model, and finally
    the graph cut method and convex-hull fitting are applied to get the segmented
    OD and OC regions. This method was evaluated with the DRISHTI-GS dataset using
    three performance metrics: F-score, overlap measure (IOU) and boundary error (B).'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: Zilly等人[[83](#bib.bib83)]引入了一种基于多尺度双层CNN模型的OD和OC分割方法，该模型经过提升训练。首先，将OD周围的区域裁剪、下采样4倍，转换为L*a*b颜色空间并归一化。然后，通过熵过滤处理该区域以识别最具辨别力的点，并传递到CNN模型中，该模型使用温和的AdaBoost方法进行训练。逻辑回归分类器从CNN模型的输出中生成概率图，最后应用图切割方法和凸包拟合以获得分割后的OD和OC区域。该方法使用DRISHTI-GS数据集进行评估，采用了三种性能指标：F-score、重叠度量（IOU）和边界误差（B）。
- en: An extended version of this method is presented in [[84](#bib.bib84)], Zilly
    et al. [[84](#bib.bib84)] used ensemble CNN with entropy sampling to select informative
    points. These points were used to create a novel learning approach for convolutional
    filters based on boosting.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 本方法的扩展版本在[[84](#bib.bib84)]中提出，Zilly等人[[84](#bib.bib84)]使用了具有熵采样的集成CNN来选择信息量大的点。这些点用于创建一种基于提升的卷积滤波器的新学习方法。
- en: Maninis et al. [[70](#bib.bib70)] used the same FCN to segment both blood vessels
    and the OD from retinal, images as mentioned in the Vessel Segmentation section.
    The method was validated for OD and OC segmentation on the DRIONS-DB and RIM-ONE
    datasets.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: Maninis等人[[70](#bib.bib70)]使用相同的FCN对视网膜图像中的血管和OD进行分割，如血管分割部分所述。该方法在DRIONS-DB和RIM-ONE数据集上验证了OD和OC的分割效果。
- en: Shankaranarayana et al. [[51](#bib.bib51)] proposed a method for joint segmentation
    of the OC and OD using residual learning-based, fully convolutional networks (ResU-Net)
    that is similar to U-net [[82](#bib.bib82)] , which contains an encoder on the
    left side of the net involving down-sampling operations and a decoder on the right
    side employing up-sampling operations. A mapping between the retinal image and
    its segmentation map for OD and OC detection is trained using ResU-Net and generative
    adversarial networks (GANs). This method (ResU-GAN) does not involve any preprocessing
    and is efficient, compared with other pixel-segmentation methods [[79](#bib.bib79)].
    This method was tested using 159 images from the RIM-ONE dataset.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Shankaranarayana等人[[51](#bib.bib51)]提出了一种联合分割OC和OD的方法，该方法使用基于残差学习的全卷积网络（ResU-Net），类似于U-net[[82](#bib.bib82)]，该网络的左侧包含下采样操作的编码器，右侧包含上采样操作的解码器。使用ResU-Net和生成对抗网络（GANs）训练视网膜图像与OD和OC检测分割图之间的映射。该方法（ResU-GAN）不涉及任何预处理，相比于其他像素分割方法[[79](#bib.bib79)]更为高效。该方法使用了RIM-ONE数据集中的159张图像进行测试。
- en: Zhang et al. [[85](#bib.bib85)] used a faster region convolutional neural network
    (faster RCNN) with ZF net as the base CNN model to localize the OD. After localizing
    the OD, blood vessels in its bounding box are removed by using a Hessian matrix,
    and a shape-constrained level set is used to cut the OD’s boundaries. They used
    4,000 images selected from Kaggle to train the CNN model and MESSIDOR for testing.
    This method is fast and gives very good localization results.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang等人[[85](#bib.bib85)]使用了更快的区域卷积神经网络（faster RCNN），以ZF网作为基础CNN模型来定位OD。在定位OD后，通过使用Hessian矩阵去除其边界框中的血管，并使用形状约束的水平集来切割OD的边界。他们使用从Kaggle中选择的4,000张图像训练CNN模型，并使用MESSIDOR进行测试。该方法快速且提供了非常好的定位结果。
- en: Fu et al. [[86](#bib.bib86)] used a U-shape CNN model (M-Net) to simultaneously
    segment the OD and OC in one stage and find the cup-to-disc ratio (CDR). The input
    layer of M-Net is a multi-scale layer consisting of an image pyramid. It involves
    a U-shaped CNN with a side-output layer to produce a local prediction map for
    different scale layers and a multi-label loss function output layer. First, the
    OD region is localized and transformed into a polar domain; then, it is passed
    through M-Net to generate a multi-label map, which is inverse transformed into
    the Cartesian domain to segment the OD. The ORIGA and SCES datasets were used
    to assess the method, which gave state-of-the-art results.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: Fu 等人 [[86](#bib.bib86)] 使用了一个 U 型 CNN 模型（M-Net）来在一个阶段同时分割视盘和视杯，并找到杯盘比（CDR）。M-Net
    的输入层是一个由图像金字塔组成的多尺度层。它涉及一个带有侧输出层的 U 形 CNN，用于产生不同尺度层的局部预测图和一个多标签损失函数输出层。首先，视盘区域被定位并转换为极坐标域；然后，通过
    M-Net 生成一个多标签图，该图被逆向转换为笛卡尔坐标域以分割视盘。使用 ORIGA 和 SCES 数据集评估了该方法，取得了最先进的结果。
- en: The method by Niu et al. [[87](#bib.bib87)] used saliency map region proposal
    generation and a seven-layer-based CNN model to detect the OD. Using the saliency-based
    visual attention model, salient regions of a fundus image are identified, and
    a CNN model is used to classify these regions to locate the OD. This method is
    a validated DL approach that used cascading localization with feedback to localize
    the OD on preprocessed images using mean subtraction. The algorithm ends only
    when it finds a region containing the OD. The authors tested the performance on
    ORIGA, MESSIDOR and these datasets together.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: Niu 等人 [[87](#bib.bib87)] 的方法使用了显著性图区域提议生成和一个七层的 CNN 模型来检测视盘。通过使用基于显著性的视觉注意模型，识别眼底图像中的显著区域，并利用
    CNN 模型对这些区域进行分类以定位视盘。这种方法是一种经过验证的深度学习方法，使用级联定位与反馈来在经过预处理的图像上定位视盘，并通过均值减法来实现。算法仅在找到包含视盘的区域时结束。作者在
    ORIGA、MESSIDOR 和这些数据集上测试了性能。
- en: Alghamdi et al. [[88](#bib.bib88)] proposed a method for detecting abnormal
    ODs using a cascade of CNN models. First, candidate OD regions are extracted,
    preprocessed and normalized using whitening. Then, these regions are classified
    using the first module as the OD or non-OD. Finally, the detected OD regions are
    classified as normal, suspicious or abnormal by the second CNN module. This method
    was evaluated on DRIVE, DIARETDB1, MESSIDOR, STARE and a local dataset.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: Alghamdi 等人 [[88](#bib.bib88)] 提出了一个使用级联 CNN 模型检测异常视盘的方法。首先，提取候选视盘区域，经过预处理和归一化处理。然后，使用第一个模块对这些区域进行分类为视盘或非视盘。最后，通过第二个
    CNN 模块将检测到的视盘区域分类为正常、可疑或异常。该方法在 DRIVE、DIARETDB1、MESSIDOR、STARE 和本地数据集上进行了评估。
- en: Xu et al. [[89](#bib.bib89)] employed a pre-trained VGG model without the last
    FC layers and deconvolution layers connected to the last three pooling layers
    of a VGG model to calculate the probability map of pixels. The probability map
    is thresholded, and finally, the center of gravity of the pixels above the threshold
    is obtained to locate the OD. The authors used the ORIGA, MESSIDOR and STARE datasets
    for evaluation. This method is efficient in correctly localizing the OD.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人 [[89](#bib.bib89)] 使用了一个经过预训练的 VGG 模型，去除了最后的全连接层和反卷积层，并将其连接到 VGG 模型的最后三层池化层，以计算像素的概率图。概率图经过阈值处理，最后获得阈值以上像素的质心以定位视盘。作者使用了
    ORIGA、MESSIDOR 和 STARE 数据集进行评估。这种方法在正确定位视盘方面非常高效。
- en: 'Table [4](#S5.T4 "Table 4 ‣ 5.2.2 Stacked Autoencoder-Based Methods ‣ 5.2 Optic
    Disc Feature ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey") presents an aggregated view of the
    OD segmentation and localization methods. For OD segmentation, it is difficult
    to determine which method gives the best performance because all of the methods
    were evaluated on different databases using different metrics. Among the OD-localization
    methods, the method by Zhang et al. [[85](#bib.bib85)] based on faster RCNN gives
    the best localization results for the MESSIDOR dataset.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S5.T4 "Table 4 ‣ 5.2.2 Stacked Autoencoder-Based Methods ‣ 5.2 Optic
    Disc Feature ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey") 展示了视盘分割和定位方法的汇总视图。在视盘分割方面，很难确定哪种方法表现最佳，因为所有方法都是在不同的数据库上使用不同的指标进行评估的。在视盘定位方法中，Zhang
    等人 [[85](#bib.bib85)] 基于更快的 RCNN 的方法在 MESSIDOR 数据集上取得了最佳的定位结果。'
- en: 5.2.2 Stacked Autoencoder-Based Methods
  id: totrans-436
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 堆叠自编码器方法
- en: 'We found just one method based on SAEs used to segment OD. Srivastava et al.
    [[52](#bib.bib52)]’s idea is to distinguish parapapillary atrophy (PPA) from OD.
    This method crops the region of interest (ROI) after detecting the OD’s center
    and enhances its contrast using CLAHE. Features of each pixel are computed assuming
    a window of size 25x25 around it, which are then passed to a deep SAE consisting
    of one input layer with 626 units; seven hidden layers with 500, 400, 300, 200,
    100, 50, and 20 units; and an output layer, to classify it as an OD or non-OD
    pixel. The binary map of the ROI obtained using the SAE is further refined for
    OD segmentation using an active shape model (ASM). The least mean overlapping
    error (LMOE) was used for evaluation on the dataset containing 230 images taken
    from ref. [[90](#bib.bib90)].. Table [4](#S5.T4 "Table 4 ‣ 5.2.2 Stacked Autoencoder-Based
    Methods ‣ 5.2 Optic Disc Feature ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided
    Diagnosis Systems for Diabetic Retinopathy: A Survey") provides a general view
    showing that CNN-based methods performed better than SAE-based methods.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '我们发现只有一种基于 SAE 的方法用于分割 OD。Srivastava 等 [[52](#bib.bib52)] 的想法是区分视盘旁萎缩 (PPA)
    和 OD。该方法在检测到 OD 的中心后裁剪感兴趣区域 (ROI) 并使用 CLAHE 增强其对比度。计算每个像素的特征，假设其周围窗口大小为 25x25，然后传递给一个深度
    SAE，该 SAE 由一个输入层（626 个单元）、七个隐藏层（500、400、300、200、100、50 和 20 个单元）和一个输出层组成，用于将其分类为
    OD 或非 OD 像素。使用 SAE 获得的 ROI 的二进制图进一步通过主动形状模型 (ASM) 进行 OD 分割。使用最小均值重叠误差 (LMOE) 对包含
    230 张图像的数据集进行评估。表 [4](#S5.T4 "Table 4 ‣ 5.2.2 Stacked Autoencoder-Based Methods
    ‣ 5.2 Optic Disc Feature ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided
    Diagnosis Systems for Diabetic Retinopathy: A Survey") 提供了一个总体视图，显示 CNN 基于的方法比
    SAE 基于的方法表现更好。'
- en: 'Table 4: OD detection works'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: OD 检测工作'
- en: '| Research study | Method | Training | Type(s) | Dataset | Performance |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 研究 | 方法 | 训练 | 类型 | 数据集 | 性能 |'
- en: '| CNN-Based Methods |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 基于 CNN 的方法 |'
- en: '| Lim et al. [[79](#bib.bib79)] | Nine-layer CNN with exaggeration | End-to-end
    | OD segmentation | MESSIDOR | E=0.112, IOU=0.888 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| Lim 等 [[79](#bib.bib79)] | 九层 CNN 带夸张 | 端到端 | OD 分割 | MESSIDOR | E=0.112,
    IOU=0.888 |'
- en: '| SEED-DB | E= 0.0843, IOU=0.916 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| SEED-DB | E= 0.0843, IOU=0.916 |'
- en: '| Guo et al. [[80](#bib.bib80)] | Large pixel patch-based CNN | End-to-end
    | OC segmentation | DRISHTI-GS | F=93.73, E=0.1225 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| Guo 等 [[80](#bib.bib80)] | 大像素补丁基于的 CNN | 端到端 | OC 分割 | DRISHTI-GS | F=93.73,
    E=0.1225 |'
- en: '| Tan et al. [[73](#bib.bib73)] | Seven-layers CNN | End-to-end | OD segmentation
    | DRIVE | ACC=87.90 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| Tan 等 [[73](#bib.bib73)] | 七层 CNN | 端到端 | OD 分割 | DRIVE | ACC=87.90 |'
- en: '| Sevastopolsky [[81](#bib.bib81)] | Modified U-Net CNN | Transfer learning
    | OD segmentation | DRION-DB | IOU=0.98, Dice=0.94 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| Sevastopolsky [[81](#bib.bib81)] | 修改版 U-Net CNN | 转移学习 | OD 分割 | DRION-DB
    | IOU=0.98, Dice=0.94 |'
- en: '| RIM-ONE | IOU=0.98, Dice=0.95 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| RIM-ONE | IOU=0.98, Dice=0.95 |'
- en: '| OC segmentation | DRION-DB | IOU=0.75, Dice=0.85 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| OC 分割 | DRION-DB | IOU=0.75, Dice=0.85 |'
- en: '| RIM-ONE | IOU=0.69, Dice=0.82 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| RIM-ONE | IOU=0.69, Dice=0.82 |'
- en: '| Zilly et al. [[83](#bib.bib83)] | Multi-scale two-layers CNN | End-to-end
    | OD segmentation | DRISHTI-GS | F=94.7, IOU=0.895, B=9.1 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| Zilly 等 [[83](#bib.bib83)] | 多尺度两层 CNN | 端到端 | OD 分割 | DRISHTI-GS | F=94.7,
    IOU=0.895, B=9.1 |'
- en: '| OC segmentation | F=83, IOU=0.864, B=16.5 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| OC 分割 | F=83, IOU=0.864, B=16.5 |'
- en: '| Zilly et al. [[84](#bib.bib84)] | Ensemble learning-based CNN | End-to-end
    | OD segmentation | DRISHTI-GS | F=97.3, IOU=0.914, B=9.9 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| Zilly 等 [[84](#bib.bib84)] | 基于集成学习的 CNN | 端到端 | OD 分割 | DRISHTI-GS | F=97.3,
    IOU=0.914, B=9.9 |'
- en: '| OC segmentation | F=87.1, IOU=0.85, B=10.2 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| OC 分割 | F=87.1, IOU=0.85, B=10.2 |'
- en: '| Maninis et al. [[70](#bib.bib70)] | FCN based on VGG-16 | Transfer learning
    | OD segmentation | DRIONS-DB | RPR=0.971 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| Maninis 等 [[70](#bib.bib70)] | 基于 VGG-16 的 FCN | 转移学习 | OD 分割 | DRIONS-DB
    | RPR=0.971 |'
- en: '| RIM-ONE | RPR=0.959 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| RIM-ONE | RPR=0.959 |'
- en: '| Shankaranarayana et al. [[51](#bib.bib51)] | ResU-Net and GANs | Transfer
    learning | OD segmentation | RIM-ONE | F=98.7, IOU= 0.961 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| Shankaranarayana 等 [[51](#bib.bib51)] | ResU-Net 和 GANs | 转移学习 | OD 分割 |
    RIM-ONE | F=98.7, IOU= 0.961 |'
- en: '| OC segmentation | F=90.6, IOU=0.739 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| OC 分割 | F=90.6, IOU=0.739 |'
- en: '| Zhang et al. [[85](#bib.bib85)] | Faster RCNN | Transfer learning | OD localization
    | MESSIDOR | Mean average precision=99.9 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等 [[85](#bib.bib85)] | Faster RCNN | 转移学习 | OD 定位 | MESSIDOR | 平均精确度=99.9
    |'
- en: '| OD segmentation | MESSIDOR (120 images) | Average matching score of 85.4
    |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| OD 分割 | MESSIDOR (120 张图像) | 平均匹配分数 85.4 |'
- en: '| Fu et al. [[86](#bib.bib86)] | U-shaped CNN and polar transformation | Transfer
    learning | OD segmentation | ORIGA | E=0.071, IOU=0.929 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| Fu 等 [[86](#bib.bib86)] | U 形 CNN 和极坐标变换 | 转移学习 | OD 分割 | ORIGA | E=0.071,
    IOU=0.929 |'
- en: '| Niu et al. [[87](#bib.bib87)] | Saliency map, CNN based on AlexNet | Transfer
    learning | OD localization | ORIGA | ACC=99.33 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| Niu 等人 [[87](#bib.bib87)] | 显著性图，基于 AlexNet 的 CNN | 迁移学习 | OD 定位 | ORIGA
    | ACC=99.33 |'
- en: '| MESSIDOR | ACC=98.75 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| MESSIDOR | ACC=98.75 |'
- en: '| ORIGA+ MESSIDOR | ACC=99.04 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| ORIGA+ MESSIDOR | ACC=99.04 |'
- en: '| Alghamdi et al. [[88](#bib.bib88)] | Cascade CNN, each model with 10-layers
    | End-to-end | OD localization | DRIVE | ACC=100 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| Alghamdi 等人 [[88](#bib.bib88)] | Cascade CNN，每个模型有 10 层 | 端到端 | OD 定位 | DRIVE
    | ACC=100 |'
- en: '| DIARETDB1 | ACC=98.88 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| DIARETDB1 | ACC=98.88 |'
- en: '| MESSIDOR | ACC=99.20 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| MESSIDOR | ACC=99.20 |'
- en: '| STARE | ACC=86.71 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| STARE | ACC=86.71 |'
- en: '| Xu et al. [[89](#bib.bib89)] | CNN based on VGG and deconvolution | Transfer
    learning | OD localization | ORIGA | ACC=100 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| Xu 等人 [[89](#bib.bib89)] | 基于 VGG 和解卷积的 CNN | 迁移学习 | OD 定位 | ORIGA | ACC=100
    |'
- en: '| MESSIDOR | ACC=99.43 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| MESSIDOR | ACC=99.43 |'
- en: '| STARE | ACC=89 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| STARE | ACC=89 |'
- en: '| AE-Based Methods |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| AE 基于方法 |'
- en: '| Srivastava et al. [[52](#bib.bib52)] | SAE with ASM | End-to-end | OD segmentation
    | Local dataset used by Foong et al. [[90](#bib.bib90)] | E=0.097 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| Srivastava 等人 [[52](#bib.bib52)] | SAE 与 ASM | 端到端 | OD 分割 | Foong 等人 [[90](#bib.bib90)]
    使用的本地数据集 | E=0.097 |'
- en: 5.3 Lesion Detection and Classification
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 病变检测与分类
- en: Many DL methods have been proposed for detecting and classifying different types
    of DR lesions such as macular edema, exudates, microaneurysms and hemorrhages.
    In this section, we review these methods.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度学习方法已被提出用于检测和分类不同类型的 DR 病变，如黄斑水肿、渗出物、微动脉瘤和出血。在这一部分，我们将回顾这些方法。
- en: 5.3.1 Macula Edema as a Clinical Feature
  id: totrans-474
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 黄斑水肿作为临床特征
- en: The macula is the central part of retina, which consists of a thin layer of
    cells and light-sensitive nerve fibers at the back of eye, and is responsible
    for clear vision. Diabetic macula edema (DME) is a DR complication that occurs
    when the retinal capillaries become permeable and leakage occurs around the macula
    [[14](#bib.bib14)]; when vessels’ fluid and blood enter the retina, the macula
    swells and thickens. The DL methods for DME mainly can be categorized as CNN-based
    and AE-based methods.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 黄斑是视网膜的中央部分，由一层薄薄的细胞和光敏神经纤维组成，负责清晰的视觉。糖尿病黄斑水肿（DME）是 DR 并发症之一，当视网膜毛细血管变得通透并在黄斑周围发生泄漏时
    [[14](#bib.bib14)]；当血管的液体和血液进入视网膜时，黄斑会肿胀和增厚。DME 的深度学习方法主要可以分为基于 CNN 的方法和基于 AE
    的方法。
- en: 5.3.1.1 Convolutional Neural Networks
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.3.1.1 卷积神经网络
- en: Abràmoff et al. [[91](#bib.bib91)] proposed a supervised end-to-end CNN-based
    method to recognize DME. Perdomo et al. [[92](#bib.bib92)] proposed a method that
    combines EX localization and segmentation with DME detection. EX localization
    consists of two stages. In the first stage, an eight-layer CNN model, which takes
    a 488$\times$48 patch as its input, is used to localize EXs. It is trained on
    e-ophtha. In the second stage, using this CNN model as a predictor as well as
    the MESSIDOR dataset, grayscale mask images are produced. The DME detection model
    is based on the AlexNet architecture, which takes a fundus image together with
    a corresponding grayscale mask image as the inputs and predicts the class as normal,
    mild, moderate or severe DME. Preprocessing is used to extract the EXs’ ROIs,
    and data augmentation is applied to generate more samples to train the CNN model.
    The authors used MESSIDOR for testing.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: Abràmoff 等人 [[91](#bib.bib91)] 提出了一个基于监督的端到端 CNN 方法来识别 DME。Perdomo 等人 [[92](#bib.bib92)]
    提出了一个将 EX 定位和分割与 DME 检测相结合的方法。EX 定位包括两个阶段。在第一阶段，使用一个八层 CNN 模型，该模型以 488$\times$48
    的补丁作为输入，用于定位 EX。该模型在 e-ophtha 数据集上进行训练。在第二阶段，使用这个 CNN 模型作为预测器，以及 MESSIDOR 数据集，生成灰度掩模图像。DME
    检测模型基于 AlexNet 架构，输入为视网膜图像和相应的灰度掩模图像，预测类别为正常、轻度、中度或重度 DME。预处理用于提取 EX 的 ROI，并且应用数据增强以生成更多样本来训练
    CNN 模型。作者使用了 MESSIDOR 进行测试。
- en: Burlina et al. [[93](#bib.bib93)] used a deep convolutional neural network for
    feature extraction and a linear support vector machine (LSVM) for classification,
    for age-related macular degeneration (AMD). After cropping and resizing a fundus
    image to 231×231 pixels, the OverFeat CNN model pre-trained on the ImageNet dataset
    is used for feature extraction. The dataset NIH AREDS [[41](#bib.bib41)], which
    is divided into four categories according to AMD severity, was used for validation.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: Burlina 等人 [[93](#bib.bib93)] 使用深度卷积神经网络进行特征提取，并使用线性支持向量机（LSVM）进行分类，应用于年龄相关性黄斑变性（AMD）。在将视网膜图像裁剪并调整为
    231×231 像素后，使用在 ImageNet 数据集上预训练的 OverFeat CNN 模型进行特征提取。用于验证的数据集 NIH AREDS [[41](#bib.bib41)]
    按照 AMD 严重程度分为四类。
- en: Al-Bander et al. [[94](#bib.bib94)] proposed an end-to-end CNN model for grading
    DME severity. After cropping and resizing a fundus image, red, green and blue
    channels are scaled to have zero mean and unit variance. The proposed CNN model
    consists of three convolution blocks and one block of FC layers. Data augmentation
    is applied to increase the number of samples for training. The model was evaluated
    using the MESSIDOR dataset.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: Al-Bander 等人[[94](#bib.bib94)]提出了一种端到端的CNN模型来评估DME严重程度。在裁剪和调整眼底图像的大小后，红色、绿色和蓝色通道被缩放为零均值和单位方差。所提出的CNN模型由三个卷积块和一个全连接层块组成。应用数据增强来增加训练样本的数量。该模型使用了MESSIDOR数据集进行评估。
- en: Ting et al. [[95](#bib.bib95)] evaluated the performance of a CNN model to diagnose
    AMD and other DR complications and concluded that their CNN was are effective
    in diagnosing DR complications but cannot identify all DME cases using fundus
    images. The CNN model for AMD detection was trained using 72,610 fundus images
    and was tested on 35,948 images from different ethnicities.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: Ting 等人[[95](#bib.bib95)]评估了使用卷积神经网络模型诊断AMD和其他DR并发症的性能，并得出他们的卷积神经网络在诊断DR并发症方面是有效的，但不能使用眼底图像识别所有DME病例。这个AMD检测的卷积神经网络模型使用了72,610张眼底图像进行训练，并在来自不同族裔的35,948张图像上进行了测试。
- en: Mo et al. [[96](#bib.bib96)] proposed a two-stage method to classify DME. In
    the first stage, a cascaded fully convolutional residual network (FCRN) with fused
    multi-level hierarchical information is used to create a probability map and segment
    EXs. In the second stage, using the segmented regions, the pixels with maximum
    probability are cropped and fed into another residual network to classify DME.
    They used the HEI-MED [[97](#bib.bib97)] and e-ophtha datasets to assess the method.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: Mo 等人[[96](#bib.bib96)]提出了一种二阶段的方法来分类DME。在第一阶段，使用级联全卷积残差网络（FCRN）和融合的多级分层信息来创建概率图和分割EX。在第二阶段，使用分割区域，裁剪具有最大概率的像素，并将其输入到另一个残差网络中以分类DME。他们使用了HEI-MED[[97](#bib.bib97)]和e-ophtha数据集来评估这种方法。
- en: 5.3.1.2 Deep Belief Networks
  id: totrans-482
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.3.1.2 深度置信网络
- en: have also been employed for image-level DME diagnosis. Arunkumar and Karthigaikumar
    [[29](#bib.bib29)] used a DBN for feature extraction and a multiclass SVM for
    classification to diagnose AMD together with other DR complications. In this method,
    fundus images first undergo a preprocessing procedure that includes normalization,
    contrast adjustment or histogram equalization. Then, features are extracted using
    unsupervised DBN, the dimensions of the feature space are reduced with a generalized
    regression neural network (GRNN) and finally classification is performed using
    a multiclass SVM. They used the ARIA dataset to assess the method.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 也已经对图像级别的DME诊断进行了应用。Arunkumar 和 Karthigaikumar[[29](#bib.bib29)] 使用DBN进行特征提取，并使用多类SVM进行分类，以诊断AMD和其他DR并发症。在这种方法中，眼底图像首先经过预处理过程，包括归一化、对比度调整或直方图均衡化。然后，使用无监督的DBN提取特征，使用广义回归神经网络（GRNN）对特征空间的维度进行降维，并最后使用多类SVM进行分类。他们使用ARIA数据集来评估这种方法。
- en: 'The comparison of these CNN and DBN-based methods given in Table [5](#S5.T5
    "Table 5 ‣ 5.3.1.2 Deep Belief Networks ‣ 5.3.1 Macula Edema as a Clinical Feature
    ‣ 5.3 Lesion Detection and Classification ‣ 5 Literature Survey ‣ Deep Learning
    based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey") shows
    that CNN-based methods outperform DBN-based methods. Among the CNN-based methods,
    the one by Abràmoff et al. [[91](#bib.bib91)] achieved the better performance,
    probably because it is based on an Alexnet-like model. DBNs have not been used
    in an end-to-end way; thus, they must be explored further using end-to-end learning.
    Interestingly, DBNs involve significantly fewer learnable parameters than CNN
    models.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S5.T5 "表5 ‣ 5.3.1.2 Deep Belief Networks ‣ 5.3.1 Macula Edema as a Clinical
    Feature ‣ 5.3 Lesion Detection and Classification ‣ 5 Literature Survey ‣ Deep
    Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey")中对这些基于CNN和DBN的方法进行的比较表明，基于CNN的方法优于基于DBN的方法。在基于CNN的方法中，Abràmoff等人[[91](#bib.bib91)]的方法取得了更好的性能，可能是因为它基于一个类似于Alexnet的模型。DBN尚未以端到端的方式使用，因此必须进一步探索其在端到端学习中的应用。有趣的是，DBN比CNN模型具有显著较少的可学习参数。'
- en: 'Table 5: Representative works for DME detection'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：DME检测的代表性工作
- en: '| Research Study | Method | Training | Lesion Type(s) | Dataset | Performance
    |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| 研究论文 | 方法 | 训练 | 损伤类型 | 数据集 | 性能 |'
- en: '| CNN-Based Methods |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| 基于CNN的方法 |'
- en: '| Abràmoff et al. [[91](#bib.bib91)] | CNN inspired by AlexNet | End-to-end
    | Multistage DR/ME | MESSIDOR-2 | SN=100 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| Abràmoff 等人[[91](#bib.bib91)] | 受AlexNet启发的CNN | 端到端 | 多阶段DR/ME | MESSIDOR-2
    | SN=100 |'
- en: '| Mo et al. [[96](#bib.bib96)] | Cascaded FCRN | End-to-end | ME | HEI-MED
    | SN=92.55,F=84.99 |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| Mo 等 [[96](#bib.bib96)] | 级联 FCRN | 端到端 | ME | HEI-MED | SN=92.55, F=84.99
    |'
- en: '| e-ophtha | SN=92.27, F=90.53 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| e-ophtha | SN=92.27, F=90.53 |'
- en: '| Perdomo et al. [[92](#bib.bib92)] | Patches based CNN model | Transfer learning
    | Multistage DR/ME | MESSIDOR |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| Perdomo 等 [[92](#bib.bib92)] | 基于补丁的 CNN 模型 | 迁移学习 | 多阶段 DR/ME | MESSIDOR
    |'
- en: '&#124; SN=56.5, SP=92.8 &#124;'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=56.5, SP=92.8 &#124;'
- en: '&#124; DME ACC=77 &#124;'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DME ACC=77 &#124;'
- en: '&#124; DME loss=0.78 &#124;'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DME 损失=0.78 &#124;'
- en: '|'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Burlina et al. [[93](#bib.bib93)] | CNN-based on OverFeat | Transfer learning
    | Multistage AMD | NIH AREDS |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| Burlina 等 [[93](#bib.bib93)] | 基于 OverFeat 的 CNN | 迁移学习 | 多阶段 AMD | NIH AREDS
    |'
- en: '&#124; SN=90.9-93.4 &#124;'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=90.9-93.4 &#124;'
- en: '&#124; SP=89.9-95.6 &#124;'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SP=89.9-95.6 &#124;'
- en: '&#124; ACC=92-95 &#124;'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=92-95 &#124;'
- en: '|'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Al-Bander et al. [[94](#bib.bib94)] | CNN model with three conv. blocks and
    one FC block | End-to-end | Multistage DR/ME | MESSIDOR |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| Al-Bander 等 [[94](#bib.bib94)] | 具有三个卷积块和一个全连接块的 CNN 模型 | 端到端 | 多阶段 DR/ME
    | MESSIDOR |'
- en: '&#124; SN=74.7, SP=95 &#124;'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=74.7, SP=95 &#124;'
- en: '&#124; ACC=88.8 &#124;'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=88.8 &#124;'
- en: '|'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Ting et al. [[95](#bib.bib95)] | CNN | End-to-end | AMD | 35948 images |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| Ting 等 [[95](#bib.bib95)] | CNN | 端到端 | AMD | 35948 张图像 |'
- en: '&#124; SN=93.2, SP=88.7 &#124;'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=93.2, SP=88.7 &#124;'
- en: '&#124; AUC=0.931 &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AUC=0.931 &#124;'
- en: '|'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DBN-Based Methods |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 基于 DBN 的方法 |'
- en: '| Arunkumar and Karthigaikumar [[29](#bib.bib29)] | DBN for training and multiclass
    SVM as classifier | End-to-end | AMD | ARIA |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| Arunkumar 和 Karthigaikumar [[29](#bib.bib29)] | 用于训练的 DBN 和作为分类器的多类 SVM |
    端到端 | AMD | ARIA |'
- en: '&#124; SN=79.32, SP=97.89 &#124;'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SN=79.32, SP=97.89 &#124;'
- en: '&#124; ACC=96.73 &#124;'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACC=96.73 &#124;'
- en: '|'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 5.3.2 Exudate as a Clinical Feature
  id: totrans-514
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 渗出物作为临床特征
- en: The detection of EX is necessary for automatic early DR diagnosis, but it is
    challenging because of significant variation in their size, shape and contrast
    levels. In this section, we review DL-based methods for EX detection. According
    to our best knowledge, all of the methods are based on CNNs.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: EX 的检测对自动早期 DR 诊断是必要的，但由于其大小、形状和对比度水平的显著变化，这一任务具有挑战性。在本节中，我们回顾了基于深度学习的 EX 检测方法。根据我们所知，所有这些方法都基于
    CNN。
- en: 5.3.2.1 Convolutional Neural Networks
  id: totrans-516
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.3.2.1 卷积神经网络
- en: Prentašić and Lončarić [[98](#bib.bib98)] proposed a CNN-based method for EX
    detection in color fundus images. First, they detect the OD, create an OD probability
    map and fit a parabola. Then they create vessel probability and bright-border
    probability maps. Finally, using an 11-layer CNN model, they create an EX probability
    map and combine it with the OD, vessel and bright-border probability maps and
    the fitted parabola to generate the final EX probability map. They assessed the
    model’s performance using the DRiDB database. It significantly outperformed the
    traditional methods based on hand-engineered features.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: Prentašić 和 Lončarić [[98](#bib.bib98)] 提出了一个基于 CNN 的 EX 检测方法，用于彩色视网膜图像。首先，他们检测
    OD，创建 OD 概率图并拟合抛物线。然后，他们创建血管概率图和亮边界概率图。最后，使用 11 层 CNN 模型，生成 EX 概率图，并将其与 OD、血管和亮边界概率图及拟合的抛物线结合，生成最终的
    EX 概率图。他们使用 DRiDB 数据库评估了模型的性能。该模型显著优于基于手工设计特征的传统方法。
- en: Perdomo et al. [[99](#bib.bib99)] proposed a patch-level method based on the
    LeNet model to discriminate EX regions from healthy regions on fundus images.
    In this method, potential EX patches are first cropped manually or automatically;
    then, these patches are passed to the LeNet model for classification. To train
    LeNet, extra patches are created using a data-augmentation technique based on
    flipping and rotation operations. The e-ophtha dataset was used for validation;
    20,148 EXs and healthy patches were extracted, and 40% of these patches was used
    as testing data.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: Perdomo 等 [[99](#bib.bib99)] 提出了一个基于 LeNet 模型的补丁级方法，用于区分视网膜图像中的 EX 区域和健康区域。在该方法中，潜在的
    EX 补丁首先通过手动或自动方式裁剪；然后，这些补丁被送入 LeNet 模型进行分类。为了训练 LeNet，使用基于翻转和旋转操作的数据增强技术创建了额外的补丁。e-ophtha
    数据集用于验证；提取了 20,148 个 EX 和健康补丁，其中 40% 的补丁被用作测试数据。
- en: Gondal et al. [[27](#bib.bib27)] introduced a method for detecting EXs together
    with other DR lesions based on the award-winning $o\_O$ CNN architecture [[100](#bib.bib100)].
    To localize DR lesions, including hard EX (HE) and soft EX (SE), the dense layers
    are removed from the CNN model. A global average pooling (GAP) layer is introduced
    on top of the last convolutional layer and is followed by a classification layer,
    which are used to learn the class-specific importance of each feature map of the
    last convolution layer. The feature maps are combined with class-specific importance
    to generate a class activation map (CAM) [[101](#bib.bib101)], which is up-sampled
    to the size of the original image to localize the lesion regions. The authors
    used Kaggle for training and DIARETDB1 for validation. This method not only performs
    image-level detection but also lesion-level detection
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: Gondal 等人 [[27](#bib.bib27)] 提出了一个基于获奖的$o\_O$ CNN架构[[100](#bib.bib100)]检测EX及其他DR病变的方法。为了定位DR病变，包括硬EX
    (HE)和软EX (SE)，从CNN模型中移除了密集层。在最后的卷积层上引入了全局平均池化（GAP）层，并跟随一个分类层，这些层用于学习每个最后卷积层特征图的类别特定重要性。这些特征图与类别特定的重要性结合，生成类别激活图（CAM）[[101](#bib.bib101)]，该图被上采样到原始图像的大小以定位病变区域。作者使用Kaggle进行训练，并使用DIARETDB1进行验证。这种方法不仅执行图像级检测，还进行病变级检测。
- en: Quellec et al. [[102](#bib.bib102)] addressed the problem of jointly detecting
    referable DR at the image level and detecting DR lesions such as EXs at the pixel
    level, and they proposed a solution that relies on CNN visualization methods.
    The heatmaps generated by CNN visualization techniques are not optimized for computer-aided
    diagnosis of DR lesions. Based on the sensitivity analysis by Simonyan and Zisserman
    [[57](#bib.bib57)], they proposed modifications to generate heatmaps, which help
    in jointly detecting referable DR and lesions by jointly optimizing CNN predictions
    and the produced heatmaps. They employed the $o\_O$ architecture as the CNN base
    model. The authors used the Kaggle dataset for training at the image level and
    DIARETDB1 for testing at both the lesion and image levels for EX detection.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: Quellec 等人 [[102](#bib.bib102)] 解决了在图像级别联合检测可参考的DR和在像素级别检测DR病变（如EX）的问题，他们提出了依赖于CNN可视化方法的解决方案。CNN可视化技术生成的热图并未针对DR病变的计算机辅助诊断进行优化。基于Simonyan和Zisserman的敏感性分析[[57](#bib.bib57)]，他们提出了生成热图的修改方法，这有助于通过联合优化CNN预测和生成的热图来共同检测可参考的DR和病变。他们采用了$o\_O$架构作为CNN基础模型。作者使用Kaggle数据集进行图像级训练，并使用DIARETDB1进行EX检测的病变级和图像级测试。
- en: Khojasteh et al. [[103](#bib.bib103)] compared several DL-patch-based methods
    to detect EX. They concluded that pre-trained ResNet-50 with SVM outperformed
    other methods. They assessed their method on DIARETDB1 and e-ophtha.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: Khojasteh 等人 [[103](#bib.bib103)] 比较了几种基于DL-patch的方法来检测EX。他们得出结论，预训练的ResNet-50与SVM的组合优于其他方法。他们在DIARETDB1和e-ophtha上评估了他们的方法。
- en: 'Table [6](#S5.T6 "Table 6 ‣ 5.3.2.1 Convolutional Neural Networks ‣ 5.3.2 Exudate
    as a Clinical Feature ‣ 5.3 Lesion Detection and Classification ‣ 5 Literature
    Survey ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey") presents a comparative analysis of the aforementioned methods for EX
    detection using a DL approach. The methods of Quellec et al. [[102](#bib.bib102)],
    Gondal et al. [[27](#bib.bib27)], which jointly detect referable DR and lesions,
    show good performance for both lesion and image-base detection. The method inKhojasteh
    et al. [[103](#bib.bib103)] is computationally more efficient and produces comparable
    results due to using the deep pre-trained ResNet.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#S5.T6 "表6 ‣ 5.3.2.1 卷积神经网络 ‣ 5.3.2 病变作为临床特征 ‣ 5.3 病变检测与分类 ‣ 5 文献综述 ‣
    基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述") 提供了上述基于DL的方法进行EX检测的比较分析。Quellec 等人[[102](#bib.bib102)]和Gondal
    等人[[27](#bib.bib27)]的方法，在同时检测可参考的DR和病变方面表现良好。Khojasteh 等人[[103](#bib.bib103)]的方法在计算上更高效，由于使用了深度预训练的ResNet，因此产生了可比的结果。
- en: 'Table 6: Representative of works in diabetic retinopathy (DR) EX detection'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：糖尿病视网膜病变（DR）EX检测的代表性工作
- en: '| Research Study | Method | Training | Lesion Type(s) | Dataset |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| 研究研究 | 方法 | 训练 | 病变类型 | 数据集 |'
- en: '&#124; Segment/ &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割/ &#124;'
- en: '&#124; localize? &#124;'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本地化？ &#124;'
- en: '| Performance |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 性能 |'
- en: '| CNN-Based Methods |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 基于CNN的方法 |'
- en: '| Prentašić and Lončarić [[98](#bib.bib98)] | 11-layer CNN, OD and vessel maps
    | End-to-end | EX | DRiDB | ✓ | SN=78, F=78 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| Prentašić 和 Lončarić [[98](#bib.bib98)] | 11层CNN，OD和血管图 | 端到端 | EX | DRiDB
    | ✓ | SN=78，F=78 |'
- en: '| Perdomo et al. [[99](#bib.bib99)] | Patches-based LeNet CNN | Transfer learning
    | EX | e-ophtha(40% of patches) | ✓ | SN=99.8, SP=99.6 ACC=99.6 |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| Perdomo 等 [[99](#bib.bib99)] | 基于补丁的 LeNet CNN | 迁移学习 | EX | e-ophtha（40%
    补丁） | ✓ | SN=99.8, SP=99.6 ACC=99.6 |'
- en: '| Gondal et al. [[27](#bib.bib27)] | $o\_O$ CNN model with CAM | Transfer learning
    | HE/SE | DIARETDB1 | ✓ | SN:HE=87, SE=80 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| Gondal 等 [[27](#bib.bib27)] | $o\_O$ CNN 模型与 CAM | 迁移学习 | HE/SE | DIARETDB1
    | ✓ | SN:HE=87, SE=80 |'
- en: '| HE/SE | ✗ | SN:HE =100, SE=90.0 AUC=0.954 |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| HE/SE | ✗ | SN:HE =100, SE=90.0 AUC=0.954 |'
- en: '| Quellec et al. [[102](#bib.bib102)] | $o\_O$ CNN(net A) | Transfer learning
    | HE/SE | DIARETDB1 | ✓ | AUC:HE=0.735, SE=0.809 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| Quellec 等 [[102](#bib.bib102)] | $o\_O$ CNN（网络 A） | 迁移学习 | HE/SE | DIARETDB1
    | ✓ | AUC:HE=0.735, SE=0.809 |'
- en: '| $o\_O$ CNN(net B) | HE/SE | ✗ | AUC:HE=0.974, SE=0.963 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| $o\_O$ CNN（网络 B） | HE/SE | ✗ | AUC:HE=0.974, SE=0.963 |'
- en: '| Khojasteh et al. [[103](#bib.bib103)] | Patch-based ResNet/SVM as classifier
    | Transfer learning | EX | DIARETDB1 | ✗ | SN=99, SP=96, ACC=98.2 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| Khojasteh 等 [[103](#bib.bib103)] | 基于补丁的 ResNet/SVM 作为分类器 | 迁移学习 | EX | DIARETDB1
    | ✗ | SN=99, SP=96, ACC=98.2 |'
- en: '|  | e-ophtha | ✗ | SN=98, SP=95, ACC=97.6 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '|  | e-ophtha | ✗ | SN=98, SP=95, ACC=97.6 |'
- en: 5.3.3 Microaneurysms and Hemorrhages as Clinical Features
  id: totrans-537
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 微动脉瘤和出血作为临床特征
- en: MAs and HMs are also have been investigated using DL approaches as a sign of
    DR, as presented in this section.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: MAs 和 HMs 也已经使用深度学习方法作为 DR 的标志进行了研究，如本节所述。
- en: 5.3.3.1 Convolutional Neural Networks (CNN)
  id: totrans-539
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.3.3.1 卷积神经网络（CNN）
- en: Haloi [[104](#bib.bib104)] employed a nine-layer CNN model with a dropout training
    procedure to classify each pixel as MA or non-MA. Each pixel is classified by
    taking a window of size 129$\times$129 around it and passing the window to the
    CNN model. For training, the author employed a data-augmentation technique to
    generate six windows around each pixel. He graded the severity from no DR to severe
    DR according to the number of MAs. The method was tested on the MESSIDOR and Retinopathy
    Online Challenge (ROC) datasets.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: Haloi [[104](#bib.bib104)] 采用了一个九层 CNN 模型，并使用 dropout 训练程序将每个像素分类为 MA 或非 MA。每个像素通过取其周围
    129$\times$129 的窗口并将其传递给 CNN 模型进行分类。为了训练，作者使用了数据增强技术来生成每个像素周围的六个窗口。他根据 MA 的数量对
    DR 的严重程度进行了评分，从无 DR 到严重 DR。该方法在 MESSIDOR 和 Retinopathy Online Challenge (ROC)
    数据集上进行了测试。
- en: The method introduced by van Grinsven et al. [[105](#bib.bib105)] was aimed
    at detecting HMs. The main contribution of this method is to address the over-represented
    normal samples created for training a CNN model. To overcome this problem, the
    authors proposed a dynamic selective sampling strategy that selects informative
    training samples. First, they extract patches of size 41$\times$41 around HM pixels
    from positive images only and non-HM pixels from positive images only, and each
    patch is labeled according to the central pixel. The CNN is trained using a dynamic
    selective sampling strategy. They used a 10-layer CNN model and tested their system
    on Kaggle and MESSIDOR.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: van Grinsven 等 [[105](#bib.bib105)] 提出的这种方法旨在检测 HM。该方法的主要贡献是解决为训练 CNN 模型而创建的过多正常样本的问题。为了克服这个问题，作者提出了一种动态选择性采样策略，用于选择信息丰富的训练样本。首先，他们从正图像中提取大小为
    41$\times$41 的 HM 像素周围的补丁，仅从正图像中提取非 HM 像素，每个补丁根据中心像素进行标记。CNN 使用动态选择性采样策略进行训练。他们使用了一个
    10 层的 CNN 模型，并在 Kaggle 和 MESSIDOR 上测试了他们的系统。
- en: The methods of Gondal et al. [[27](#bib.bib27)] and Quellec et al. [[102](#bib.bib102)]
    discussed in the Exudate section, which jointly detect referable DR and lesions,
    also detect HMs and small red dots. Another similar method was proposed by Orlando
    et al. [[106](#bib.bib106)]. In this method, they first extract candidate red
    lesions using morphological operations and crop patches of size 32×32 around the
    candidates. Next, they extract CNN features and hand-engineered features (HEFs)
    such as intensity and shape features from each candidate patch, fuse them and
    pass the fused feature vector to random forest (RF) to create a probability map,
    which is used to make lesion- and image-level decisions about red lesions. They
    employed a six-layer CNN model. For lesion-based evaluation, they used as a competition
    metric (CPM) the average per lesion sensitivity at the reference false positive
    detections per image value. They used the DIARETDB1 and e-ophtha datasets for
    per lesion evaluation. They used MESSIODR for detecting referable DR.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: Gondal et al. [[27](#bib.bib27)] 和Quellec et al. [[102](#bib.bib102)] 在排除液体部分讨论中提到的方法，这些方法共同检测可参考的DR和病变，也检测HMs和小红点。Orlando
    et al. [[106](#bib.bib106)] 提出了另一种类似的方法。在该方法中，他们首先使用形态学操作提取候选红色病变，并在候选区域周围裁剪32×32大小的补丁。接着，他们从每个候选补丁中提取CNN特征和手工设计特征（HEFs），如强度和形状特征，将它们融合，然后将融合的特征向量传递给随机森林（RF）以创建概率图，用于对红色病变进行病变级和图像级决策。他们采用了六层CNN模型。在病变级评估中，他们使用每图像参考假阳性检测值的每病变敏感性的平均值作为竞争指标（CPM）。他们使用DIARETDB1和e-ophtha数据集进行每病变评估。他们使用MESSIODR来检测可参考的DR。
- en: 5.3.3.2 Stacked Autoencoder-Based Methods
  id: totrans-543
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.3.3.2 堆叠自编码器方法
- en: Shan and Li [[107](#bib.bib107)] used the stacked sparse autoencoder (SSAE)
    to detect MA lesions. A patch is passed to SSAE, which extracts features, and
    the Softmax classifier labels it as a MA or non-MA patch. They trained and fine-tuned
    the SSAE on MA and non-MA patches taken from 89 fundus retinal images selected
    from the DIARETDB dataset. The patches were extracted without any preprocessing
    procedure, and Shan and Li evaluated them using 10-fold cross validation.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: Shan和Li [[107](#bib.bib107)] 使用堆叠稀疏自编码器（SSAE）来检测MA病变。一个补丁被传递给SSAE，SSAE提取特征，然后Softmax分类器将其标记为MA或非MA补丁。他们在DIARETDB数据集中从89幅视网膜图像中选取的MA和非MA补丁上训练和微调了SSAE。这些补丁在没有任何预处理的情况下提取，Shan和Li使用10折交叉验证对其进行了评估。
- en: 'A summary of the above-reviewed methods is given in Table [7](#S5.T7 "Table
    7 ‣ 5.3.3.2 Stacked Autoencoder-Based Methods ‣ 5.3.3 Microaneurysms and Hemorrhages
    as Clinical Features ‣ 5.3 Lesion Detection and Classification ‣ 5 Literature
    Survey ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey"). In terms of sensitivity, specificity, AUC and accuracy, the CNN-based
    technique by Haloi [[104](#bib.bib104)] seems to outperform other methods for
    MA detection due to using pixel augmentation instead of image-based augmentation.
    The performance of the stacked sparse autoencoder based-method by Shan and Li
    [[107](#bib.bib107)] is not better than those based on CNN. Among CNN-based methods,
    those used by Gondal et al. [[27](#bib.bib27)] and Quellec et al. [[102](#bib.bib102)]
    are computationally efficient and jointly detect referable DR and red lesions.'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '上述评审方法的总结见表[7](#S5.T7 "Table 7 ‣ 5.3.3.2 Stacked Autoencoder-Based Methods
    ‣ 5.3.3 Microaneurysms and Hemorrhages as Clinical Features ‣ 5.3 Lesion Detection
    and Classification ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided
    Diagnosis Systems for Diabetic Retinopathy: A Survey")。在敏感性、特异性、AUC和准确度方面，Haloi
    [[104](#bib.bib104)] 的基于CNN的技术似乎优于其他方法，因为它使用了像素增强而非基于图像的增强。Shan和Li [[107](#bib.bib107)]
    的基于堆叠稀疏自编码器的方法性能不如基于CNN的方法。在基于CNN的方法中，Gondal et al. [[27](#bib.bib27)] 和Quellec
    et al. [[102](#bib.bib102)] 使用的方法计算效率高，并且可以共同检测可参考的DR和红色病变。'
- en: 'Table 7: Representative of works in diabetic retinopathy (DR) detection based
    on MA and HM'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：基于MA和HM的糖尿病视网膜病变（DR）检测的代表性工作
- en: '| Research study | Method | Training | Lesion Type(s) | Dataset |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| 研究研究 | 方法 | 训练 | 病变类型 | 数据集 |'
- en: '&#124; Segment/ &#124;'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Segment/ &#124;'
- en: '&#124; localize? &#124;'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; localize? &#124;'
- en: '| Performance |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 性能 |'
- en: '| CNN Based Methods |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| 基于CNN的方法 |'
- en: '| Haloi [[104](#bib.bib104)] | 9-layer CNN | End-to-end | MA | MESSIDOR | ✓
    | SN=97, SP=95 AUC=0.982 ACC=95.4 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| Haloi [[104](#bib.bib104)] | 9层CNN | 端到端 | MA | MESSIDOR | ✓ | SN=97, SP=95
    AUC=0.982 ACC=95.4 |'
- en: '| ROC | ✓ | AUC=0.98 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| ROC | ✓ | AUC=0.98 |'
- en: '| van Grinsven et al. [[105](#bib.bib105)] | Patches based selective sampling
    | End-to-end | HM | Kaggle | ✓ | SN=84.8, SP=90.4 AUC=0.917 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| van Grinsven et al. [[105](#bib.bib105)] | 基于补丁的选择性采样 | 端到端 | HM | Kaggle
    | ✓ | SN=84.8, SP=90.4 AUC=0.917 |'
- en: '| MESSIDOR | ✓ | SN=93.1, SP=91.5 AUC=0.979 |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| MESSIDOR | ✓ | SN=93.1, SP=91.5 AUC=0.979 |'
- en: '| Gondal et al. [[27](#bib.bib27)] | o_O CNN model | Transfer learning |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| Gondal 等人 [[27](#bib.bib27)] | o_O CNN 模型 | 迁移学习 |'
- en: '&#124; -HM &#124;'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -HM &#124;'
- en: '&#124; -Small red dots &#124;'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -小红点 &#124;'
- en: '| DIARETDB1 | ✓ | SN: -HM=91 -Small red dots=52 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| DIARETDB1 | ✓ | SN: -HM=91 -小红点=52 |'
- en: '|'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; -HM &#124;'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -HM &#124;'
- en: '&#124; -Small red dots &#124;'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -小红点 &#124;'
- en: '| ✗ | SN: -HM=97.2 -Red small dots=50 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | SN: -HM=97.2 -红色小点=50 |'
- en: '| Quellec et al. [[102](#bib.bib102)] | o_O CNN (net B) | Transfer learning
    |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| Quellec 等人 [[102](#bib.bib102)] | o_O CNN (net B) | 迁移学习 |'
- en: '&#124; -HM &#124;'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -HM &#124;'
- en: '&#124; -Small red dots &#124;'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -小红点 &#124;'
- en: '| DIARETDB1 | ✓ | AUC: -HM=0.614 -Small red dots=0.50 |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| DIARETDB1 | ✓ | AUC: -HM=0.614 -小红点=0.50 |'
- en: '|'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; -HM &#124;'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -HM &#124;'
- en: '&#124; -Small red dots &#124;'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -小红点 &#124;'
- en: '| ✗ | AUC: -HM=0.999 -Small red dots=0.912 -Red small dots +HM=0.97 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | AUC: -HM=0.999 -小红点=0.912 -红色小点 +HM=0.97 |'
- en: '| Orlando et al. [[106](#bib.bib106)] | HEF + CNN features and RF classifier
    | End-to-end | MA | DIARETDB1 | ✓ | CPM=0.3301, SN=48.83 |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| Orlando 等人 [[106](#bib.bib106)] | HEF + CNN 特征和 RF 分类器 | 端到端 | MA | DIARETDB1
    | ✓ | CPM=0.3301, SN=48.83 |'
- en: '| HM | ✓ | CPM=0.4884, SN=48.83 |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| HM | ✓ | CPM=0.4884, SN=48.83 |'
- en: '|'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MA &#124;'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MA &#124;'
- en: '| e-ophtha | ✓ | CPM=0.3683, SN=36.80 |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| e-ophtha | ✓ | CPM=0.3683, SN=36.80 |'
- en: '|  |  |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '&#124; Red lesion &#124;'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 红色病变 &#124;'
- en: '| MESSIDOR | ✗ | SN=91.09, SP=50 AUC=0.8932 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| MESSIDOR | ✗ | SN=91.09, SP=50 AUC=0.8932 |'
- en: '| AE Based Methods |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| AE 基于的方法 |'
- en: '| Shan and Li [[107](#bib.bib107)] | Patches based SSAE | Transfer learning
    | MA | DIARETDB | ✓ | SP=91.6 F=91.3 ACC=91.38 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| Shan 和 Li [[107](#bib.bib107)] | 基于补丁的 SSAE | 迁移学习 | MA | DIARETDB | ✓ |
    SP=91.6 F=91.3 ACC=91.38 |'
- en: 5.4 Classification of Fundus Images for Referral
  id: totrans-582
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 视网膜图像的转诊分类
- en: This section focuses on methods that deal with referable DR detection and use
    only image-level annotation. The main purpose of these methods is to grade DR
    levels for referral. Some methods in this category also detect lesions jointly
    with referable DR detection, but without using pixel- or lesion-level annotation
    [[102](#bib.bib102)]. To the best of our knowledge, only CNN models have been
    employed for this problem.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍处理可转诊 DR 检测且仅使用图像级标注的方法。这些方法的主要目的是对 DR 级别进行分级以便转诊。本类别中的一些方法还联合检测病变与可转诊
    DR，但不使用像素或病变级别的标注 [[102](#bib.bib102)]。据我们所知，目前仅有 CNN 模型被应用于此问题。
- en: Gulshan et al. [[108](#bib.bib108)] used the Inception-v3 CNN architecture to
    detect referable DR on a fundus image. They assessed the system using the EyePACS-1
    dataset, which consists of 9,963 images taken from 4,997 patients and MESSIDOR-2;
    both were graded by at least seven US licensed ophthalmologists and ophthalmology
    senior residents. This evaluation study concluded that an algorithm based on CNN
    has high sensitivity and specificity for detecting referable DR.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: Gulshan 等人 [[108](#bib.bib108)] 使用 Inception-v3 CNN 架构来检测视网膜图像上的可转诊 DR。他们使用
    EyePACS-1 数据集评估了该系统，该数据集包含来自 4,997 名患者的 9,963 张图像和 MESSIDOR-2；这些图像都由至少七名美国执业眼科医生和眼科高级住院医师进行分级。这项评估研究得出结论，基于
    CNN 的算法在检测可转诊 DR 方面具有很高的敏感性和特异性。
- en: Colas et al. [[109](#bib.bib109)] proposed a method based on deep learning,
    which jointly detects referable DR and lesion location. They trained the deep
    model on 70,000 labeled images and tested 10,000 images taken from Kaggle dataset,
    where each patient has two images of right and left eyes. Each image is graded
    by ophthalmologists into five main stages that vary from no retinopathy to proliferative
    retinopathy.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: Colas 等人 [[109](#bib.bib109)] 提出了一种基于深度学习的方法，该方法联合检测可转诊的糖尿病视网膜病变（DR）和病变位置。他们在
    70,000 张标注图像上训练了深度模型，并在来自 Kaggle 数据集的 10,000 张图像上进行了测试，每位患者都有一对左右眼图像。每张图像由眼科医生分为五个主要阶段，这些阶段从没有视网膜病变到增殖性视网膜病变。
- en: 'Similarly, as discussed in the MAs and HMs section, the method used by Quellec
    et al. [[102](#bib.bib102)] jointly detects referable DR and lesions; its performance
    was evaluated on three datasets: Kaggle, e-ophtha and DIRETDB1.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 MAs 和 HMs 部分讨论的方法，Quellec 等人 [[102](#bib.bib102)] 使用的方法联合检测可转诊 DR 和病变；其性能在三个数据集上进行了评估：Kaggle、e-ophtha
    和 DIRETDB1。
- en: 'Costa and Campilho [[110](#bib.bib110)] used a different approach and introduced
    a method for detecting referable DR by generalizing the idea of bag-of-visual-words
    (BoVW). First, they extract sparse local features with speeded-up robust features
    (SURF) and encode them using convolution operation or encoded dense features with
    a CNN model, and then use neural network for classification. They evaluated the
    proposed methods on three different datasets: DR1 and DR2 from [[111](#bib.bib111)]
    and MESSIDOR. DR1 and DR2 consist of grayscale images. The authors show that the
    SURF-based method outperforms the CNN-based method, probably because the CNN architecture
    is not deep enough.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: Costa和Campilho[[110](#bib.bib110)]使用了不同的方法，提出了一种通过概括视觉单词包（BoVW）思想来检测可参考DR的方法。首先，他们使用加速稳健特征（SURF）提取稀疏局部特征，并通过卷积操作对其进行编码，或者使用CNN模型编码密集特征，然后使用神经网络进行分类。他们在三个不同的数据集上评估了所提出的方法：来自[[111](#bib.bib111)]的DR1和DR2，以及MESSIDOR。DR1和DR2由灰度图像组成。作者展示了基于SURF的方法优于基于CNN的方法，可能是因为CNN架构不够深。
- en: 'Pratt et al. [[112](#bib.bib112)] used CNN structure for grading fundus images
    into one of the five stages: no DR, mild DR, moderate DR, severe DR and proliferated
    DR. They addressed the issues of overfitting and skewed datasets, and proposed
    a technique to solve these issues. For training, they enhanced the volume of data
    using a data augmentation technique. The employed CNN model consists of 10 convolutional
    layers and three fully connected layers. For training, they used 80,000 images
    taken from the Kaggle dataset, and 5,000 images for testing.'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: Pratt等人[[112](#bib.bib112)]使用CNN结构将眼底图像分为五个阶段：无DR、轻度DR、中度DR、严重DR和增殖性DR。他们解决了过拟合和数据集偏斜的问题，并提出了一种解决这些问题的技术。在训练中，他们通过数据增强技术来增加数据量。使用的CNN模型包含10个卷积层和三个全连接层。训练时，他们使用了从Kaggle数据集中获取的80,000张图像，并使用5,000张图像进行测试。
- en: Gargeya and Leng [[113](#bib.bib113)] used the ResNet CNN model consisting of
    five residual blocks of four, six, eight, 10, and six layers, respectively, and
    a gradient boosting classifier for grading a fundus image as normal or referable
    DR. Additionally, they introduced a convolutional visualization layer at the end
    of ResNet for visualizing its learning procedure. For training, they used 75,137
    images selected from the EyePACS dataset and evaluated independently on MESSIDOR-2
    and e-ophtha datasets.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: Gargeya和Leng[[113](#bib.bib113)]使用了包含五个残差块的ResNet CNN模型，分别为四层、六层、八层、10层和六层，并使用梯度提升分类器对眼底图像进行正常或可参考DR的分级。此外，他们在ResNet的末尾引入了卷积可视化层，以可视化其学习过程。在训练中，他们使用了从EyePACS数据集中选择的75,137张图像，并在MESSIDOR-2和e-ophtha数据集上进行了独立评估。
- en: Abràmoff et al. [[91](#bib.bib91)] also proposed a method based on supervised
    end-to-end CNN models, discussed in the ME section, to grade a fundus image as
    normal or referable DR; a fundus image is taken to be referable DR if it is moderate
    DR, severe non-proliferative DR (NPDR) or proliferative DR (PDR).
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: Abràmoff等人[[91](#bib.bib91)]也提出了一种基于监督式端到端CNN模型的方法，讨论了ME部分，用于将眼底图像分级为正常或可参考DR；如果眼底图像为中度DR、严重非增殖性DR（NPDR）或增殖性DR（PDR），则被认为是可参考DR。
- en: Similarly, Ting et al. [[95](#bib.bib95)] addressed the problem of detecting
    referable DR using 76,370 images for training. This method is discussed in the
    mecula edema section.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Ting等人[[95](#bib.bib95)]解决了使用76,370张图像检测可参考DR的问题。该方法在mecula edema部分进行了讨论。
- en: 'Wang et al. [[114](#bib.bib114)] proposed a supervised image-level CNN-based
    approach that diagnosed DR and highlighted suspicious patches regions. They used
    a network called Zoom-in, which mimics the zoomin procedure of retinal clinical
    examination. The architecture of the network consisted of three parts: main network
    (M-Net), which was pre-trained on ImageNet, a sub-network; attention network (A-Net)
    to generate attention maps; and another sub-network, crop-network (C-Net). They
    used EyePACS and MESSIDOR to evaluate the system.'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: Wang等人[[114](#bib.bib114)]提出了一种监督式图像级CNN方法，用于诊断DR并突出显示可疑的斑块区域。他们使用了一个名为Zoom-in的网络，该网络模拟了视网膜临床检查的放大程序。网络的架构包括三部分：主要网络（M-Net），在ImageNet上进行预训练，一个子网络；注意力网络（A-Net）生成注意力图；以及另一个子网络，裁剪网络（C-Net）。他们使用EyePACS和MESSIDOR来评估该系统。
- en: The method by Mansour [[115](#bib.bib115)] used the AlexNet model in conjunction
    with a preprocessing, Gaussian mixture model for background subtraction and connected
    component analysis to localize blood vessels; then linear discriminant analysis
    is used for dimensionality reduction. Finally, SVM is employed for classification
    and 10-fold cross validation is used for evaluation. Also, as discussed in the
    MAs and HMs section, the method by Orlando et al. [[106](#bib.bib106)], jointly
    detects referable DR and lesions; its performance was evaluated on MESSIDOR.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: Mansour [[115](#bib.bib115)] 的方法使用了 AlexNet 模型，并结合了高斯混合模型进行背景去除和连通组件分析以定位血管；然后使用线性判别分析进行降维。最后，采用
    SVM 进行分类，并使用 10 倍交叉验证进行评估。此外，如 MAs 和 HMs 部分所述，Orlando 等 [[106](#bib.bib106)] 的方法联合检测可转诊的
    DR 和病变；其性能在 MESSIDOR 上进行了评估。
- en: Chen et al. [[116](#bib.bib116)] built a model called SI2DRNet-v1 to detect
    referral DR that consisted of 20 layers. After applying preprocessing in their
    model, such as a Gaussian filter, they used global average pooling instead of
    FC layers and 1$\times$1 filters to reduce parameters and regularize the model;
    they also scaled the kernel size after each pooling layer from 3$\times$3 to 5$\times$5\.
    Finally, they extracted 5 probability values from the Softmax layer to grade DR
    severity.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等 [[116](#bib.bib116)] 构建了一个名为 SI2DRNet-v1 的模型，用于检测转诊 DR，该模型包含 20 层。在他们的模型中应用了预处理，如高斯滤波器后，他们使用全局平均池化代替
    FC 层和 1$\times$1 滤波器以减少参数并正则化模型；他们还在每个池化层后将内核大小从 3$\times$3 扩展到 5$\times$5。最后，他们从
    Softmax 层中提取了 5 个概率值以对 DR 严重程度进行分级。
- en: 'Table [8](#S5.T8 "Table 8 ‣ 5.4 Classification of Fundus Images for Referral
    ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided Diagnosis Systems for
    Diabetic Retinopathy: A Survey") summarizes the works presented in this section.
    Apparently, the method by Gulshan et al. [[108](#bib.bib108)] outperforms other
    methods in terms of sensitivity, specificity and AUC; the performance of this
    method is comparable to a panel of seven certified ophthalmologists. However,
    it is difficult to compare the performance of the methods because different datasets
    were used for training and testing. The method by Gargeya and Leng [[113](#bib.bib113)]
    seems to be robust because it is based on ResNet architecture, which has been
    shown to outperform most of the CNN architectures; it was trained and tested on
    different datasets, and its cross-dataset performance is quite good. A CNN model
    is like a black box and does not give any insight into pathology. This method
    also incorporates the visualization of pathologic regions, which can aid real-time
    clinical validation of automated diagnoses.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [8](#S5.T8 "Table 8 ‣ 5.4 Classification of Fundus Images for Referral ‣
    5 Literature Survey ‣ Deep Learning based Computer-Aided Diagnosis Systems for
    Diabetic Retinopathy: A Survey") 总结了本节中提出的工作。显然，Gulshan 等 [[108](#bib.bib108)]
    的方法在敏感性、特异性和 AUC 上优于其他方法；该方法的表现可与七名认证眼科医生的结果相媲美。然而，由于使用了不同的数据集进行训练和测试，比较方法的性能比较困难。Gargeya
    和 Leng [[113](#bib.bib113)] 的方法似乎具有鲁棒性，因为它基于 ResNet 架构，已被证明优于大多数 CNN 架构；它在不同的数据集上进行了训练和测试，并且其跨数据集的表现相当好。CNN
    模型如同黑箱，无法提供对病理的深入了解。该方法还结合了病理区域的可视化，这有助于实时临床验证自动诊断结果。'
- en: 'Table 8: Representative of works in diabetic retinopathy (DR) for referral'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 糖尿病视网膜病变 (DR) 转诊的代表性工作'
- en: '| Research Study | Method | Training | Dataset | SN% | SP% | AUC | ACC% |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| 研究 | 方法 | 训练 | 数据集 | SN% | SP% | AUC | ACC% |'
- en: '| Gulshan et al. [[108](#bib.bib108)] | Inception-v3 CNN | Transfer learning
    | EyePACS-1 | 90.3 | 90 | 0.991 | - |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| Gulshan 等 [[108](#bib.bib108)] | Inception-v3 CNN | 转移学习 | EyePACS-1 | 90.3
    | 90 | 0.991 | - |'
- en: '| MESSIDOR-2 | 87 | 98.5 | 0.990 | - |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| MESSIDOR-2 | 87 | 98.5 | 0.990 | - |'
- en: '| Colas et al. [[109](#bib.bib109)] | CNN model | End-to-end | Kaggle | 96.2
    | 66.6 | 0.946 | - |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| Colas 等 [[109](#bib.bib109)] | CNN 模型 | 端到端 | Kaggle | 96.2 | 66.6 | 0.946
    | - |'
- en: '| Quellec et al. [[102](#bib.bib102)] | $o\_O$ CNN(net B) | Transfer learning
    | DIARETDB1 | - | - | 0.954 | - |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| Quellec 等 [[102](#bib.bib102)] | $o\_O$ CNN（net B） | 转移学习 | DIARETDB1 | -
    | - | 0.954 | - |'
- en: '| Ensemble net A, net B, AlexNet | Kaggle | - | - | 0.955 | - |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| 集成网 A、网 B、AlexNet | Kaggle | - | - | 0.955 | - |'
- en: '| e-ophtha | - | - | 0.949 | - |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| e-ophtha | - | - | 0.949 | - |'
- en: '| Costa and Campilho [[110](#bib.bib110)] | Sparse SURF/CNN | End-to-end |
    MESSIDOR(20% of images ) | - | - | 0.90 | - |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| Costa 和 Campilho [[110](#bib.bib110)] | 稀疏 SURF/CNN | 端到端 | MESSIDOR（20%
    的图像） | - | - | 0.90 | - |'
- en: '| DR1(20% of images ) | - | - | 0.93 | - |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| DR1（20% 的图像） | - | - | 0.93 | - |'
- en: '| DR2(20% of images) | - | - | 0.97 | - |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| DR2（20% 的图像） | - | - | 0.97 | - |'
- en: '| Pratt et al. [[112](#bib.bib112)] | 13-layers CNN | End-to-end | Kaggle |
    95 | - | - | 75 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| 普拉特等人 [[112](#bib.bib112)] | 13层 CNN | 端到端 | Kaggle | 95 | - | - | 75 |'
- en: '| Gargeya and Leng [[113](#bib.bib113)] | ResNet+Gradient boosting tree | End-to-end
    | MESSIDOR-2 | - | - | 0.94 | - |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| Gargeya 和 Leng [[113](#bib.bib113)] | ResNet+梯度提升树 | 端到端 | MESSIDOR-2 | -
    | - | 0.94 | - |'
- en: '|'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; e-ophtha &#124;'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; e-ophtha &#124;'
- en: '| - | - | 0.95 | - |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| - | - | 0.95 | - |'
- en: '| Ting et al. [[95](#bib.bib95)] | CNN | End-to-end | (71896 images) | 90.5
    | 91.6 | 0.936 | - |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| 丁等人 [[95](#bib.bib95)] | CNN | 端到端 | （71896 图像） | 90.5 | 91.6 | 0.936 | -
    |'
- en: '| Abràmoff et al. [[91](#bib.bib91)] | CNN | End-to-end |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| Abràmoff 等人 [[91](#bib.bib91)] | CNN | 端到端 |'
- en: '&#124; MESSIDOR-2 &#124;'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MESSIDOR-2 &#124;'
- en: '| 96.8 | 87 | 0.980 | - |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| 96.8 | 87 | 0.980 | - |'
- en: '| Wang et al. [[114](#bib.bib114)] | Zoom-in network | Transfer learning |
    EyePACS | - | - | 0.825 | - |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| 王等人 [[114](#bib.bib114)] | 放大网络 | 迁移学习 | EyePACS | - | - | 0.825 | - |'
- en: '| MESSIDOR | - | - | 0.957 | 91.1 |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| MESSIDOR | - | - | 0.957 | 91.1 |'
- en: '| Chen et al. [[116](#bib.bib116)] | SI2DRNet-v1(20 layers) | End-to-end |
    MESSIDOR | - | - | 0.965 | 91.2 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| 陈等人 [[116](#bib.bib116)] | SI2DRNet-v1（20层） | 端到端 | MESSIDOR | - | - | 0.965
    | 91.2 |'
- en: '| Mansour [[115](#bib.bib115)] | AlexNet/SVM as classifier | Transfer learning
    | Kaggle | 100 | 93 | - | 97.93 |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| 曼苏尔 [[115](#bib.bib115)] | AlexNet/SVM 作为分类器 | 迁移学习 | Kaggle | 100 | 93 |
    - | 97.93 |'
- en: '| Orlando et al. [[106](#bib.bib106)] | HEF + CNN features and RF classifier
    | End-to-end | MESSIDOR | 97.21 | 50 | 0.9347 | - |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| 奥兰多等人 [[106](#bib.bib106)] | HEF + CNN 特征和 RF 分类器 | 端到端 | MESSIDOR | 97.21
    | 50 | 0.9347 | - |'
- en: 6 Discussion
  id: totrans-621
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: 'The previous section gives a detailed account of techniques related to DR diagnosis
    based on a deep-learning approach. The studies listed in this survey use four
    main deep learning architectures: CNN, AE, DBN and RNN. Each of these architectures
    has several variations that have been used in DR diagnosis. Deep-learning-based
    techniques have been proposed for retinal vessels segmentation, OD detection and
    segmentation, DR lesion detection and classification, and referable DR detection.
    A review of these methods indicates that most deep-learning-based techniques for
    the above problems use CNN architecture, and that it outperforms other deep architectures.
    Despite these improvements, there are still challenges to improve deep learning
    techniques for more robust and accurate detection, localization and diagnosis
    of different DR biomarkers and complications. All reviewed methods were tested
    and evaluated on public domain datasets, except two methods [[52](#bib.bib52),
    [95](#bib.bib95)] that used fundus images collected from medical organizations
    and hospitals. For the most part, methods addressing the same problem were evaluated
    on different datasets using different metrics, and as such, it is difficult to
    precisely compare them and grade them based on their performance. Most of the
    methods were evaluated using the same dataset for training and testing, and performance
    of the same method is different for different datasets; this raises questions
    about their robustness and how these methods will perform when deployed in a real
    clinical setting.'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 前一部分详细介绍了基于深度学习方法的 DR 诊断技术。本调查中列出的研究使用了四种主要的深度学习架构：CNN、AE、DBN 和 RNN。这些架构都有几种变体在
    DR 诊断中得到了应用。基于深度学习的方法已被提出用于视网膜血管分割、OD 检测与分割、DR 病变检测与分类，以及可参考 DR 检测。对这些方法的回顾表明，大多数针对上述问题的深度学习技术使用了
    CNN 架构，并且它优于其他深度架构。尽管有这些改进，但仍然存在挑战，需要改进深度学习技术，以便更强健、准确地检测、定位和诊断不同的 DR 生物标志物和并发症。所有评审的方法都在公共数据集上进行了测试和评估，除了两种方法
    [[52](#bib.bib52)、[95](#bib.bib95)] 使用了从医疗机构和医院收集的眼底图像。大多数情况下，解决相同问题的方法在不同的数据集上使用了不同的度量标准进行评估，因此很难准确比较它们并根据其性能进行评分。大多数方法在相同数据集上进行了训练和测试，并且相同方法在不同数据集上的表现不同；这引发了对其鲁棒性以及这些方法在实际临床环境中部署时的表现的疑问。
- en: There is a serious difficulty in interpreting and comparing the results of different
    methods in terms different performance metrics when different datasets are used
    for evaluation. For example, method by Liskowski and Krawiec [[69](#bib.bib69)]
    is tested on STARE consisting of 402 images (38 test negatives and 364 test positives)
    and method by Dasgupta and Singh [[72](#bib.bib72)] is tested on the DRIVE consisting
    of 40 images (33 test negatives and 7 test negatives). Both methods are similar
    in terms of specificity (SP), area under ROC curve (AUC) and accuracy (ACC) i.e.
    both have SP = 98%, AUC = 0.97-0.99 and ACC = 95%-97%., but the method by Liskowski
    and Krawiec [[69](#bib.bib69)] is far better (with SN = 85%) than the method by
    Dasgupta and Singh [[72](#bib.bib72)] (with SN = 76%) when sensitivity is used
    for evaluation. It indicates that the methods must be evaluated on the same datasets
    to estimate their real performance gains.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用不同数据集进行评估时，解释和比较不同方法在各种性能指标上的结果存在严重困难。例如，Liskowski和Krawiec [[69](#bib.bib69)]
    的方法在STARE数据集上进行了测试，该数据集包含402张图像（38张测试阴性和364张测试阳性）；Dasgupta和Singh [[72](#bib.bib72)]
    的方法在DRIVE数据集上进行了测试，该数据集包含40张图像（33张测试阴性和7张测试阳性）。这两种方法在特异性（SP）、ROC曲线下面积（AUC）和准确率（ACC）方面相似，即两者均有SP
    = 98%、AUC = 0.97-0.99和ACC = 95%-97%。然而，Liskowski和Krawiec [[69](#bib.bib69)] 的方法在灵敏度方面（SN
    = 85%）远优于Dasgupta和Singh [[72](#bib.bib72)] 的方法（SN = 76%）。这表明，必须在相同的数据集上评估这些方法以估算其实际性能提升。
- en: Though CNN architecture results in better performance, CNN models involve a
    huge number of parameters, and requires a huge volume of annotated datasets; however,
    the available datasets consist of a small number of annotated images. As such,
    when CNN is used to detect and diagnose different DR complications, there is a
    high risk of overfitting. One solution to deal this problem is data augmentation,
    but the data augmentation techniques that have been used so far do not create
    real samples. More data augmentation techniques are needed in order to create
    new samples from existing ones. Another solution is to use transfer learning,
    i.e. first train a CNN model using a dataset from a related domain and then fine-tune
    it with the dataset from the domain of the problem. Most of the reviewed methods
    use CNN models pre-trained on natural images [[70](#bib.bib70), [87](#bib.bib87),
    [89](#bib.bib89), [93](#bib.bib93)], e.g. the ImageNet dataset for transfer learning;
    only a few methods used CNN models pre-trained on fundus images [[102](#bib.bib102),
    [27](#bib.bib27)]. Another alternative to deal with the overfitting problem is
    to introduce CNN models that are expressive but involve fewer learnable parameters.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管卷积神经网络（CNN）架构能带来更好的性能，但CNN模型涉及大量参数，并且需要大量标注数据集；然而，现有的数据集仅包含少量标注图像。因此，当使用CNN检测和诊断不同的糖尿病视网膜病变（DR）并发症时，有较高的过拟合风险。解决这个问题的一种方法是数据增强，但迄今为止使用的数据增强技术无法创建真实样本。需要更多的数据增强技术来从现有样本中创建新样本。另一种解决方案是使用迁移学习，即首先使用相关领域的数据集训练CNN模型，然后用问题领域的数据集进行微调。大多数被审查的方法使用在自然图像上预训练的CNN模型[[70](#bib.bib70),
    [87](#bib.bib87), [89](#bib.bib89), [93](#bib.bib93)]，例如用于迁移学习的ImageNet数据集；只有少数方法使用在眼底图像上预训练的CNN模型[[102](#bib.bib102),
    [27](#bib.bib27)]。另一种应对过拟合问题的替代方法是引入表达能力强但涉及较少可学习参数的CNN模型。
- en: 6.1 Comparison of Deep-Learning-Based and Hand-Engineered Methods
  id: totrans-625
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 基于深度学习方法与人工设计方法的比较
- en: 'In this section, we compare traditional methods based on hand-engineered features
    and deep-learning-based methods. For comparison, we selected the methods evaluated
    using the same datasets and performance metrics. The selected traditional methods
    are the state-of-the-art methods reported in references [[117](#bib.bib117), [5](#bib.bib5)]
    for vessels segmentation and [[6](#bib.bib6)] for OD and [[1](#bib.bib1)] for
    MAs. The deep learning methods that give the best performance in this review are
    selected for comparison. Although hand-engineered features have been dominant
    for long time, the deep learning approach is a state-of-the-art technique and
    has shown impressive performance compared with traditional approaches. Table [9](#S6.T9
    "Table 9 ‣ 6.1 Comparison of Deep-Learning-Based and Hand-Engineered Methods ‣
    6 Discussion ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic
    Retinopathy: A Survey") presents a comparison between traditional and deep-learning-based
    methods.'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '本节中，我们比较了基于手工设计特征的传统方法和基于深度学习的方法。为了比较，我们选择了在相同数据集和性能指标上进行评估的方法。所选的传统方法是文献[[117](#bib.bib117),
    [5](#bib.bib5)]中报告的用于血管分割的最先进方法，以及[[6](#bib.bib6)]用于OD和[[1](#bib.bib1)]用于MAs的最先进方法。本文回顾中表现最佳的深度学习方法也被选入比较。尽管手工设计特征长期占据主导地位，但深度学习方法作为一种最先进的技术，与传统方法相比表现出色。表[9](#S6.T9
    "Table 9 ‣ 6.1 Comparison of Deep-Learning-Based and Hand-Engineered Methods ‣
    6 Discussion ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic
    Retinopathy: A Survey")展示了传统方法和基于深度学习的方法之间的比较。'
- en: 'For retinal blood vessel segmentation, the traditional method by Villalobos-Castaldi
    et al. [[118](#bib.bib118)] seems to give better sensitivity (96.48%) and accuracy
    (97.59%) than the deep learning method used by Liskowski and Krawiec [[69](#bib.bib69)]
    (sensitivity: 78.11% and accuracy: 95.35%) on the DRIVE dataset; however, the
    latter method gives overall better performance than another traditional method
    by Condurache and Mertins [[119](#bib.bib119)] on the STARE and CHASE datasets.
    This indicates that deep learning based methods outperform traditional methods
    overall, but in spite of this fact, even deep-learning-based methods are not robust:
    their performance is different for different datasets. Figure [3](#S6.F3 "Figure
    3 ‣ 6.1 Comparison of Deep-Learning-Based and Hand-Engineered Methods ‣ 6 Discussion
    ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey") shows a plot of the performance of traditional and DL-based methods.'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '对于视网膜血管分割，Villalobos-Castaldi等人[[118](#bib.bib118)]提出的传统方法在DRIVE数据集上的灵敏度（96.48%）和准确率（97.59%）优于Liskowski和Krawiec[[69](#bib.bib69)]使用的深度学习方法（灵敏度：78.11%，准确率：95.35%）；然而，后者方法在STARE和CHASE数据集上的整体表现优于Condurache和Mertins[[119](#bib.bib119)]的另一种传统方法。这表明，基于深度学习的方法总体上优于传统方法，但即使是深度学习方法也不够稳健：它们在不同数据集上的表现有所不同。图[3](#S6.F3
    "Figure 3 ‣ 6.1 Comparison of Deep-Learning-Based and Hand-Engineered Methods
    ‣ 6 Discussion ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic
    Retinopathy: A Survey")显示了传统方法和基于深度学习的方法的性能图。'
- en: For OD localization, the deep-learning-based method by Zhang et al. [[85](#bib.bib85)],
    with an accuracy of 99.9%, outperforms the traditional method used by Aquino et al.
    [[120](#bib.bib120)], with an accuracy of 99% on the MESSIDOR dataset. For the
    DRIVE dataset, the learning-based method by Alghamdi et al. [[88](#bib.bib88)]
    and traditional method by Zhang et al. [[121](#bib.bib121)] achieved the same
    performance with an accuracy of 100%. However, for DIARETDB1, the traditional
    method by Sinha and Babu [[122](#bib.bib122)], with an accuracy of 100%, outperforms
    the deep-learning accuracy of 98.88% achieved by Alghamdi et al. [[88](#bib.bib88)].
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 对于OD定位，Zhang等人[[85](#bib.bib85)]提出的基于深度学习的方法以99.9%的准确率优于Aquino等人[[120](#bib.bib120)]使用的传统方法，其准确率为99%，在MESSIDOR数据集上表现更佳。对于DRIVE数据集，Alghamdi等人[[88](#bib.bib88)]的学习方法和Zhang等人[[121](#bib.bib121)]的传统方法在准确率上均为100%。然而，对于DIARETDB1，Sinha和Babu[[122](#bib.bib122)]提出的传统方法以100%的准确率优于Alghamdi等人[[88](#bib.bib88)]取得的98.88%的深度学习准确率。
- en: 'For OD segmentation, deep-learning-based methods show significantly higher
    accuracy than traditional methods. For example, on the MESSIDOR dataset, the traditional
    method used by Aquino et al. [[120](#bib.bib120)] showed less accuracy (86%) than
    that of the deep-learning-based method of Lim et al. [[79](#bib.bib79)], achieving
    significant higher accuracy (96.4%). Similarly, on the DRIVE dataset, the traditional
    method used by Tjandrasa et al. [[123](#bib.bib123)] gave lower accuracy (75.56%)
    than that yielded by Tan et al.’s deep-learning-based method. (92.68%) Tan et al.
    [[73](#bib.bib73)]. Figure [4](#S6.F4 "Figure 4 ‣ 6.1 Comparison of Deep-Learning-Based
    and Hand-Engineered Methods ‣ 6 Discussion ‣ Deep Learning based Computer-Aided
    Diagnosis Systems for Diabetic Retinopathy: A Survey") summarizes the performance
    of OD localization and segmentation in traditional and DL methods.'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视盘分割，基于深度学习的方法显示出明显高于传统方法的准确性。例如，在 MESSIDOR 数据集上，Aquino 等人使用的传统方法 [[120](#bib.bib120)]
    的准确性（86%）低于 Lim 等人使用的深度学习方法 [[79](#bib.bib79)] 的准确性（96.4%）。同样，在 DRIVE 数据集上，Tjandrasa
    等人使用的传统方法 [[123](#bib.bib123)] 的准确性（75.56%）低于 Tan 等人基于深度学习的方法 [[73](#bib.bib73)]
    的准确性（92.68%）。图 [4](#S6.F4 "图 4 ‣ 6.1 深度学习与手工设计方法的比较 ‣ 6 讨论 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")
    总结了传统方法和深度学习方法在视盘定位和分割方面的性能。
- en: Exudate detection review by Joshi and Karule [[3](#bib.bib3)] that published
    2018 reported the maximum EX detection performance on private datasets. However,
    we compared the best DL method with the one that has maximum number of images
    and higher performance. Massey et al. [[124](#bib.bib124)] showed slightly better
    accuracy (98.87%) whereas the DL based method by Khojasteh et al. [[103](#bib.bib103)]
    achieved 98.2% but with higher sensitivity (99%).
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: Joshi 和 Karule [[3](#bib.bib3)] 在 2018 年发布的外渗物检测回顾报告了在私有数据集上的最高 EX 检测性能。然而，我们将最佳深度学习方法与拥有最多图像和更高性能的方法进行了比较。Massey
    等人 [[124](#bib.bib124)] 展现了略高的准确率（98.87%），而 Khojasteh 等人 [[103](#bib.bib103)]
    的深度学习方法则实现了 98.2% 的准确率，但具有更高的敏感性（99%）。
- en: 'For MAs detection, on the MESSIDOR dataset, Haloi’s deep-learning-based method
    [[104](#bib.bib104)] achieved a higher performance with sensitivity, specificity,
    AUC and accuracy of 97%, 95%, 0.982 and 95.4%, respectively, whereas the traditional
    method by Antal and Hajdu [[125](#bib.bib125)] equaled 94%, 90%, 0.942 and 90%,
    respectively. Figure [5](#S6.F5 "Figure 5 ‣ 6.1 Comparison of Deep-Learning-Based
    and Hand-Engineered Methods ‣ 6 Discussion ‣ Deep Learning based Computer-Aided
    Diagnosis Systems for Diabetic Retinopathy: A Survey") presents both EX and MA
    detection the performance in traditional and DL methods.'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MA 检测，在 MESSIDOR 数据集上，Haloi 的深度学习方法 [[104](#bib.bib104)] 的表现更佳，其敏感性、特异性、AUC
    和准确率分别为 97%、95%、0.982 和 95.4%，而 Antal 和 Hajdu [[125](#bib.bib125)] 使用的传统方法的敏感性、特异性、AUC
    和准确率分别为 94%、90%、0.942 和 90%。图 [5](#S6.F5 "图 5 ‣ 6.1 深度学习与手工设计方法的比较 ‣ 6 讨论 ‣ 基于深度学习的糖尿病视网膜病变计算机辅助诊断系统：综述")
    展示了传统方法和深度学习方法在 EX 和 MA 检测中的性能。
- en: Overall, CNN-based methods for retinal vessel segmentation, OD detection and
    segmentation and DR lesion detection outperform traditional methods. However,
    deep-learning methods are not robust, not interpretable and suffer from overfitting,
    and further research is needed to overcome these issues.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，基于 CNN 的视网膜血管分割、视盘检测与分割以及糖尿病视网膜病变检测的方法表现优于传统方法。然而，深度学习方法并不稳健，难以解释，并且容易过拟合，需要进一步研究以克服这些问题。
- en: '![Refer to caption](img/673f7b752d95ee2451f85a5261d442b7.png)'
  id: totrans-633
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/673f7b752d95ee2451f85a5261d442b7.png)'
- en: 'Figure 3: The plot for maximum performance of vessel segmentation in traditional
    and DL-based methods'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：传统方法与基于深度学习的方法在血管分割最大性能的对比图
- en: '![Refer to caption](img/4432d4cfbd1979a4cba223d8befe7ac6.png)'
  id: totrans-635
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4432d4cfbd1979a4cba223d8befe7ac6.png)'
- en: 'Figure 4: The plot for maximum performance of OD localization and segmentation
    in traditional and DL-based methods'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：传统方法与基于深度学习的方法在视盘定位和分割最大性能的对比图
- en: '![Refer to caption](img/8d822476e8d9be861b36cc7f4424d11a.png)'
  id: totrans-637
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8d822476e8d9be861b36cc7f4424d11a.png)'
- en: 'Figure 5: The plot for maximum performance of MA and EX detection in traditional
    and DL-based methods'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：传统方法与基于深度学习的方法在 MA 和 EX 检测最大性能的对比图
- en: 'Table 9: Comparison between state-of-the-art traditional methods and best-performance
    deep-learning methods'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：最先进的传统方法与最佳性能的深度学习方法之间的比较
- en: '| Features | Approach | Study | Dataset | SN% | SP% | AUC | ACC% |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 方法 | 研究 | 数据集 | SN% | SP% | AUC | ACC% |'
- en: '| Vessels | Traditional | Villalobos-Castaldi et al. [[118](#bib.bib118)] |
    DRIVE | 96.48 | 94.80 | - | 97.59 |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| 血管 | 传统方法 | Villalobos-Castaldi 等 [[118](#bib.bib118)] | DRIVE | 96.48 |
    94.80 | - | 97.59 |'
- en: '| Deep learning | Liskowski and Krawiec [[69](#bib.bib69)] | 78.11 | 98.07
    | 0.9790 | 95.35 |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | Liskowski 和 Krawiec [[69](#bib.bib69)] | 78.11 | 98.07 | 0.9790 |
    95.35 |'
- en: '| Traditional | Condurache and Mertins [[119](#bib.bib119)] | STARE | 89.02
    | 96.73 | 0.9791 | 95.95 |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| 传统方法 | Condurache 和 Mertins [[119](#bib.bib119)] | STARE | 89.02 | 96.73
    | 0.9791 | 95.95 |'
- en: '| Deep learning | Liskowski and Krawiec [[69](#bib.bib69)] | 85.54 | 98.62
    | 0.9928 | 97.29 |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | Liskowski 和 Krawiec [[69](#bib.bib69)] | 85.54 | 98.62 | 0.9928 |
    97.29 |'
- en: '| Traditional | Condurache and Mertins [[119](#bib.bib119)] | CHASE | 72.24
    | 97.11 | 0.9712 | 94.69 |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| 传统方法 | Condurache 和 Mertins [[119](#bib.bib119)] | CHASE | 72.24 | 97.11
    | 0.9712 | 94.69 |'
- en: '| Deep learning | Liskowski and Krawiec [[69](#bib.bib69)] | 81.54 | 98.66
    | 0.988 | 96.96 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | Liskowski 和 Krawiec [[69](#bib.bib69)] | 81.54 | 98.66 | 0.988 | 96.96
    |'
- en: '| OD localization | Traditional | Aquino et al. [[120](#bib.bib120)] | MESSIDOR
    | - | - | - | 99 |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| OD 定位 | 传统方法 | Aquino 等 [[120](#bib.bib120)] | MESSIDOR | - | - | - | 99
    |'
- en: '| Deep learning | Zhang et al. [[85](#bib.bib85)] | - | - | - | 99.9 |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | Zhang 等 [[85](#bib.bib85)] | - | - | - | 99.9 |'
- en: '| Traditional | Zhang et al. [[121](#bib.bib121)] | DRIVE | - | - | - | 100
    |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| 传统方法 | Zhang 等 [[121](#bib.bib121)] | DRIVE | - | - | - | 100 |'
- en: '| Deep learning | Alghamdi et al. [[88](#bib.bib88)] | - | - | - | 100 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | Alghamdi 等 [[88](#bib.bib88)] | - | - | - | 100 |'
- en: '| Traditional | Sinha and Babu [[122](#bib.bib122)] | DIARETDB1 | - | - | -
    | 100 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| 传统方法 | Sinha 和 Babu [[122](#bib.bib122)] | DIARETDB1 | - | - | - | 100 |'
- en: '| Deep learning | Alghamdi et al. [[88](#bib.bib88)] | - | - | - | 98.88 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | Alghamdi 等 [[88](#bib.bib88)] | - | - | - | 98.88 |'
- en: '| OD segm entation | Traditional | Aquino et al. [[120](#bib.bib120)] | MESSIDOR
    | - | - | - | 86 |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| OD 分割 | 传统方法 | Aquino 等 [[120](#bib.bib120)] | MESSIDOR | - | - | - | 86
    |'
- en: '| Deep learning | Lim et al. [[79](#bib.bib79)] | - | - | - | 96.4 |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | Lim 等 [[79](#bib.bib79)] | - | - | - | 96.4 |'
- en: '| Traditional | Tjandrasa et al. [[123](#bib.bib123)] | DRIVE | - | - | - |
    75.56 |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| 传统方法 | Tjandrasa 等 [[123](#bib.bib123)] | DRIVE | - | - | - | 75.56 |'
- en: '| Deep learning | Tan et al. [[73](#bib.bib73)] | 87.90 | 99.27 | - | 92.68
    |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | Tan 等 [[73](#bib.bib73)] | 87.90 | 99.27 | - | 92.68 |'
- en: '| EX | Traditional | Massey et al. [[124](#bib.bib124)] | 50 images | 96.9
    | 98.9 | - | 98.87 |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| EX | 传统方法 | Massey 等 [[124](#bib.bib124)] | 50 张图像 | 96.9 | 98.9 | - | 98.87
    |'
- en: '| Deep learning | Khojasteh et al. [[103](#bib.bib103)] | DIARETDB1 | 99 |
    96 | - | 98.2 |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | Khojasteh 等 [[103](#bib.bib103)] | DIARETDB1 | 99 | 96 | - | 98.2
    |'
- en: '| MA | Traditional | Antal and Hajdu [[125](#bib.bib125)] | MESSIODR ,R0vsR1
    | 94 | 90 | 0.942 | 90 |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| MA | 传统方法 | Antal 和 Hajdu [[125](#bib.bib125)] | MESSIODR ,R0vsR1 | 94 |
    90 | 0.942 | 90 |'
- en: '| Deep learning | Haloi [[104](#bib.bib104)] | 97 | 95 | 0.982 | 95.4 |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | Haloi [[104](#bib.bib104)] | 97 | 95 | 0.982 | 95.4 |'
- en: 7 Gaps and Future Directions
  id: totrans-661
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 差距与未来方向
- en: This review of methods related to DR diagnosis reveals that deep learning helped
    to design better methods for DR diagnosis and moved state-of-the-art techniques
    forward, but it is still an open problem, and more research is needed. There are
    not many methods based on deep learning, and advanced deep-learning techniques
    must be developed in order to solve this problem. Deep-learning-based models are
    mostly black boxes and do not provide interpretations of diagnostic value that
    could help validate their usefulness in a real clinical setting. Most of the methods
    in this review do not provide any interpretation of their outcomes.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 这项关于 DR 诊断方法的综述表明，深度学习有助于设计更好的 DR 诊断方法，并推动了最先进技术的发展，但这仍然是一个开放的问题，需要更多的研究。目前基于深度学习的方法不多，必须开发先进的深度学习技术以解决这一问题。基于深度学习的模型大多是黑箱，不提供诊断价值的解释，这些解释可以帮助验证其在实际临床环境中的有效性。此综述中的大多数方法未提供其结果的任何解释。
- en: One of the most challenging problems in designing robust deep-learning methods,
    especially based on CNN models with deeper architectures, is the acquisition of
    huge volumes of labeled fundus images with pixel- and image-level annotations.
    The main issue is not the availability of huge datasets, but the annotation of
    these images, which is expensive and requires the services of expert ophthalmologists.
    The solution is to design learning algorithms that can learn a deep model from
    limited data; this is an important area of research not only for DR diagnosis,
    but also in medical image analysis; one possible direction to explore Generative
    Adversarial Networks (GANs) [[126](#bib.bib126)]. Another alternative is to introduce
    augmentation techniques, as the data augmentation techniques used thus far do
    not create real samples. Therefore, new data augmentation techniques must be developed
    that create new samples from existing samples that represent real samples. Another
    alternative to deal with the problem is to introduce CNN models that are expressive
    but involve fewer learnable parameters.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 设计鲁棒的深度学习方法，尤其是基于更深层架构的CNN模型，面临的最具挑战性的问题之一是获取大量带有像素级和图像级注释的标注眼底图像。主要的问题不是数据集的可用性，而是这些图像的注释，这不仅昂贵，而且需要专家眼科医生的服务。解决方案是设计能够从有限数据中学习深度模型的学习算法；这不仅是DR诊断的重要研究领域，也是医学图像分析中的一个重要方向；一个可能的方向是探索生成对抗网络（GANs）[[126](#bib.bib126)]。另一种替代方法是引入数据增强技术，因为迄今为止使用的数据增强技术并未生成真实样本。因此，必须开发新的数据增强技术，从现有样本中生成代表真实样本的新样本。解决这一问题的另一个替代方案是引入表达能力强但涉及较少可学习参数的CNN模型。
- en: Moreover, class imbalance is another challenge in datasets; in medical imaging,
    in general, and fundus images, in particular, the number of DR cases is much less
    than in normal cases. Furthermore, the quantities of images with different DR
    complications and DR lesions are different, and this difference in some cases
    is significant and adds bias for specific classes during the training of deep
    models. Large-scale retinal screening processes around the world lead to huge
    datasets of fundus images; however, most of the images are normal and do not contain
    any suspicious symptoms or lesions. Developing deep-learning strategies in dealing
    with this class imbalance is another essential area of research. Data augmentation
    has been used in some studies – such as [[112](#bib.bib112), [105](#bib.bib105),
    [127](#bib.bib127), [128](#bib.bib128)] – to tackle the class imbalance problem,
    but these data augmentation techniques mostly use geometric transformations and
    only create rotated and scaled samples, and do not introduce samples with lesions
    having morphological variations. More sophisticated data augmentation techniques
    that create heterogeneous samples while preserving prognostic characteristics
    of fundus images must be introduced, and one possible direction is to explore
    Generative Adversarial Networks (GANs) [[129](#bib.bib129)].
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，类别不平衡是数据集中的另一个挑战；在医学成像领域，尤其是在眼底图像中，DR病例的数量远少于正常病例。此外，不同DR并发症和DR病灶的图像数量也不同，在某些情况下这种差异很大，且在深度模型训练过程中为特定类别增加了偏差。全球范围的大规模视网膜筛查过程导致了大量的眼底图像数据集；然而，大多数图像是正常的，不包含任何可疑症状或病灶。在处理这种类别不平衡问题时，开发深度学习策略是另一个重要的研究领域。数据增强已经在一些研究中使用——如[[112](#bib.bib112),
    [105](#bib.bib105), [127](#bib.bib127), [128](#bib.bib128)]——来解决类别不平衡问题，但这些数据增强技术主要使用几何变换，仅生成旋转和缩放样本，而没有引入具有形态变化的病灶样本。必须引入更复杂的数据增强技术，在保留眼底图像预后特征的同时生成异质样本，一个可能的方向是探索生成对抗网络（GANs）[[129](#bib.bib129)]。
- en: A principal issue in fundoscopy is the lack of uniformity among fundus images,
    i.e. the images being captured under different conditions. Fundus images usually
    suffer from the problem of illumination variation due to non-uniform diffusion
    of light in the retina; the shape of a retina is close to a sphere, which prevents
    the retina’s light incident from being reflected uniformly. Another common problem
    with respect to illumination is related to the angle at which light is incident
    on the retina; the angle at which the image is taken is not always the same. This
    can be confirmed by observing that the optic nerve does not maintain a specific
    position in the entire database. Another problem related to capturing the fundus
    image is that in some cases, the image is out of focus. In addition, fundus images
    are not always captured with the same resolution and camera. There is also the
    problem of pigmentation reflected by the iris.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 眼底镜检查中的一个主要问题是视网膜图像的缺乏统一性，即在不同条件下捕捉到的图像。视网膜图像通常会出现光照变化的问题，因为光线在视网膜上的扩散不均匀；视网膜的形状接近于球体，这导致视网膜上的光线无法均匀反射。另一个与光照相关的常见问题是光线照射在视网膜上的角度；拍摄图像的角度并不总是相同的。这可以通过观察视神经在整个数据库中并没有保持特定位置来确认。与捕捉眼底图像相关的另一个问题是图像在某些情况下可能会失焦。此外，眼底图像的拍摄分辨率和相机也不总是相同。还有虹膜反射的色素问题。
- en: One way to deal with these problems is to add a preprocessing stage in deep-learning
    methods. Alternatively, a robust approach is to design deep models such as Generative
    Adversarial Networks (GANs) [[130](#bib.bib130)] so that they automatically detect
    and correct these image artifacts. The bottleneck herein is to develop a huge
    annotated dataset that captures all different types of image artifacts. Alternatively,
    sophisticated data augmentation techniques must be introduced, augmenting an existing
    dataset with images having different artifacts.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的一种方法是将预处理阶段添加到深度学习方法中。或者，一种稳健的方法是设计深度模型，如生成对抗网络（GANs）[[130](#bib.bib130)]，使其能够自动检测和纠正这些图像伪影。瓶颈在于开发一个包含所有不同类型图像伪影的大型注释数据集。或者，必须引入复杂的数据增强技术，通过包含不同伪影的图像来扩充现有数据集。
- en: The methods developed so far are based on the assumption that the input image
    is a retinal image. However, the input image might not be a true retinal image
    or tempered retinal image; with the development of user-friendly image editing
    software it is easy to tamper an image. As such first of all an intelligent computer
    aided method must first of all identify whether the input image is a real and
    authentic retinal image.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止开发的方法都是基于输入图像是视网膜图像的假设。然而，输入图像可能不是一个真实的视网膜图像或伪造的视网膜图像；随着用户友好的图像编辑软件的发展，伪造图像变得很容易。因此，首先必须有一种智能计算机辅助方法来识别输入图像是否是一个真实和可信的视网膜图像。
- en: Ophthalmologists usually prefer to use pupil dilating drops (mydriasis) for
    better view of the retina and more field to evaluate if there is peripheral DR.
    Using fundus photography can be with mydriasis or without depending on the field
    of camera lens used. Optos camera can capture up to 200^∘ of the retina in one
    shot and without mydriasis, while other cameras like Topcon and Nikon (which is
    used usually to screen DR as it has better image quality and color) can capture
    30-50^∘ which is a limitation of the system if we capture one image. Lawrence
    [[131](#bib.bib131)] conducted a study to compare mydriasis and non-mydriasis
    with single or multiple shots to clarify how sensitive is mydriasis in DR screening.
    He showed that mydriasis with multiple shots is more sensitive and specific compared
    to in non-mydriasis group with single shot. For that reason, the datasets must
    be developed with mydriasis and multiple shots to avoid the ungradable image.
    The rate of ungradable images in the general ranges from 7-17% [[132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136)].
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 眼科医生通常倾向于使用瞳孔扩张药水（**瞳孔散大**）来更好地查看视网膜，并且有更多的视野来评估是否存在外周糖尿病视网膜病（DR）。使用眼底摄影可以根据相机镜头的视野来决定是否使用瞳孔散大。Optos
    相机可以在一次拍摄中捕捉到高达200^∘的视网膜，并且无需瞳孔散大，而其他相机如Topcon和Nikon（通常用于筛查DR，因为它们具有更好的图像质量和色彩）则只能捕捉到30-50^∘，这是系统在捕捉单张图像时的限制。Lawrence
    [[131](#bib.bib131)] 进行了一项研究，比较了瞳孔散大和非瞳孔散大的单次或多次拍摄，以明确瞳孔散大在DR筛查中的敏感度。他展示了与非瞳孔散大的单次拍摄相比，瞳孔散大的多次拍摄在敏感性和特异性上更高。因此，数据集必须在瞳孔散大和多次拍摄的条件下开发，以避免无法评价的图像。一般来说，无法评价的图像率在7-17%之间
    [[132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)]。
- en: For translational effect, AI and DL techniques should be developed in consultation
    with practicing ophthalmologists and must be validated in “real-world” DR screening
    programs where fundus images have different qualities (e.g. cataract, poor pupil
    dilation, poor contrast/focus), in patient samples of different ethnicity (i.e.
    different fundi pigmentation) and systemic control (poor and good control) [[137](#bib.bib137)].
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现转化效果，人工智能和深度学习技术应与在职眼科医生协商开发，并且必须在“真实世界”的DR筛查项目中进行验证，这些项目中的眼底图像具有不同的质量（例如白内障、瞳孔扩张不足、对比度/焦点差），涉及不同种族的患者样本（即不同的视网膜色素沉着）和系统控制（控制差和良好）
    [[137](#bib.bib137)]。
- en: The benchmark datasets have been used for the evaluation of different methods
    reviewed here. However, there is a variability in the grading by the human graders
    and different screening programs differ in local protocols. The benchmark datasets
    used in the reviewed methods does not follow a standard and as such the methods
    developed and tested using these datasets might not work in the clinical settings.
    As the performance of an intelligent computer aided method depends on the dataset
    that is used to train it, it necessitates the need of the development of new datasets
    keeping in view the procedures which are adopted in DR screening programs. Because
    of the possibility of the variability in grading by human graders while using
    different classification systems, it is advocated to standardize the use of one
    classification system for the development of the datasets and to use images that
    are graded and agreed on by at least 3 different graders. Further, the datasets
    must be statistically analyzed to determine the accuracy of grading [[138](#bib.bib138)].
    In addition, there is a classification introduced by the early treatment of diabetic
    retinopathy study (ETDRS) [[22](#bib.bib22)] for diabetic maculopathy. It must
    be taken into account for developing dataset for diabetic maculopathy.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 基准数据集已用于评估这里审查的不同方法。然而，人为评分者的评分存在变异，不同筛查程序在本地协议上有所不同。审查方法中使用的基准数据集并不遵循标准，因此使用这些数据集开发和测试的方法可能在临床环境中不起作用。由于智能计算机辅助方法的性能取决于用于训练的数据集，因此需要开发新的数据集，考虑到DR筛查程序中采用的程序。由于使用不同分类系统时人为评分者评分的变异性，建议对数据集的开发采用一种分类系统，并使用至少3位不同评分者评分并一致同意的图像。此外，数据集必须进行统计分析以确定评分的准确性
    [[138](#bib.bib138)]。此外，早期糖尿病视网膜病研究（ETDRS） [[22](#bib.bib22)] 引入了一种用于糖尿病黄斑病的分类。这必须在开发糖尿病黄斑病的数据集时加以考虑。
- en: Screening different races is a limitation of DR screening systems. Ting et al.
    [[95](#bib.bib95)] addressed this issue. However, it is not enough, further research
    is needed on this issue. From our personal experience, we observed that some DL
    algorithms developed on western populations were not able to detect some significant
    lesions in Saudi population. We noticed that as middle eastern or darkly skinned
    people may have more melanocytes in their retinal pigmented epithelium (RPE),
    which forms the most outer layer of the retina. Darker retina obscures some vascular
    changes as compared to the light colored retina. This limitation was noticed when
    we started using DL system which was developed using a dataset from light colored
    retinas. An important challenge to adoption of an algorithm is that it must be
    validated on larger patient cohorts under different settings and conditions. The
    performance of a screening software varies with the prevalence of the condition
    being screened. The prevalence of DR varies and is low in some communities and
    ethnic groups and higher in others (e.g. Hispanics, African Americans). It is
    important to understand the performance characteristics in these populations.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 筛选不同种族是一种DR筛查系统的局限性。Ting等人[[95](#bib.bib95)]解决了这个问题。然而，这还不够，需要进一步研究这个问题。从我们的个人经验来看，我们观察到一些在西方人群中开发的DL算法未能检测到沙特人群中的一些显著病变。我们注意到，中东或肤色较深的人群的视网膜色素上皮（RPE）中可能有更多的黑色素细胞，这形成了视网膜的最外层。较暗的视网膜掩盖了与浅色视网膜相比的一些血管变化。当我们开始使用一个基于浅色视网膜数据集开发的DL系统时，就发现了这一局限性。一个算法被采纳的一个重要挑战是必须在不同设置和条件下的大型患者群体中验证其有效性。筛查软件的性能随着筛查条件的流行程度而变化。DR的流行程度在一些社区和族群中较低，而在其他群体中则较高（例如，西班牙裔、非洲裔美国人）。了解这些人群的性能特征非常重要。
- en: Due to the reasons explained above, different datasets of fundus images created
    for benchmarking DR diagnosis methods are heterogeneous, and a deep-learning-based
    method gives good performance when trained and tested using the same dataset.
    For robustness, it is necessary that a deep-learning-based method gives satisfactory
    performance across different datasets. There are very few methods that have been
    tested across datasets. As real clinical settings can be forced to match the conditions
    under which a particular dataset was captured for developing a DR diagnosis method,
    robust deep-learning methods must be developed to give satisfactory performance
    in cross-database evaluation, i.e. trained with one dataset and tested with another.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述原因，为DR诊断方法创建的不同眼底图像数据集是异质的，基于深度学习的方法在使用相同数据集进行训练和测试时表现良好。为了提高鲁棒性，基于深度学习的方法必须在不同数据集上表现令人满意。目前，跨数据集测试的方法非常少。由于真实的临床环境可能会被迫匹配特定数据集捕获的条件，因此必须开发鲁棒的深度学习方法，以在跨数据库评估中提供令人满意的性能，即用一个数据集进行训练并用另一个数据集进行测试。
- en: 8 Conclusion
  id: totrans-673
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: Diabetic retinopathy is a complication of diabetes that damages the retina,
    causing vision problems. Diabetes harms the retinal blood vessels and leads to
    dangerous consequences, such as blindness. DR is preventable, and to avoid vision
    loss, early detection is important. Conventional methods for detecting DR biomarkers
    and lesions are based on hand-engineered features. The advent of deep learning
    has opened the way to design and develop more robust and accurate methods for
    detecting and diagnosing different DR complications, and deep learning has been
    employed to develop many methods for retinal blood vessel segmentation, OD detection
    and segmentation, detection and classification of different DR lesions, and the
    detection of referable DR. First, we have given an overview of different DR biomarkers
    and lesions, different tasks related to DR diagnosis, and the general framework
    of these tasks. Then we have given an overview of datasets that have been developed
    for research on DR diagnosis and performance metrics commonly used for evaluation.
    After that, we give an overview of deep-learning architectures that have been
    employed for designing DR diagnosis methods. After providing the necessary background,
    we then reviewed deep-learning-based methods that have been proposed for retinal
    blood vessels segmentation, OD detection and segmentation, detection of various
    DR lesions such as EXs, MAs, HMs and referable DR, highlighted their pro and cones
    and discussed their overall performance and compared them with state-of-the-art
    traditional methods based on hand–engineered features. In general, the deep-learning
    approach outperforms the traditional approach based on hand-engineered feature
    extraction techniques. At the end, we highlighted the gaps and weakness of the
    existing deep-learning-based DR diagnosis methods and presented potential future
    directions for research. This review gives a comprehensive view of state-of-the-art
    deep-learning-based methods related to DR diagnosis and will help researchers
    to conduct further research on this problem.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病视网膜病变是糖尿病的一种并发症，会损害视网膜，导致视力问题。糖尿病会损害视网膜血管，导致如失明等危险后果。糖尿病视网膜病变是可以预防的，为了避免视力丧失，早期检测非常重要。传统的糖尿病视网膜病变生物标志物和病变检测方法基于手工设计的特征。深度学习的出现为设计和开发更强大、更准确的糖尿病视网膜病变检测和诊断方法打开了大门，深度学习已经被用于开发许多视网膜血管分割、视盘检测和分割、各种糖尿病视网膜病变的检测和分类以及可转诊糖尿病视网膜病变的检测方法。首先，我们概述了不同的糖尿病视网膜病变生物标志物和病变、与糖尿病视网膜病变诊断相关的任务以及这些任务的一般框架。然后，我们概述了为糖尿病视网膜病变诊断研究开发的数据集以及常用于评估的性能指标。之后，我们概述了用于设计糖尿病视网膜病变诊断方法的深度学习架构。在提供必要背景后，我们回顾了提出的基于深度学习的方法，这些方法用于视网膜血管分割、视盘检测和分割、各种糖尿病视网膜病变的检测，如EXs、MAs、HMs和可转诊糖尿病视网膜病变，突出了它们的优缺点，并讨论了它们的整体性能，并与基于手工设计特征的最新传统方法进行了比较。一般而言，深度学习方法优于基于手工设计特征提取技术的传统方法。最后，我们强调了现有基于深度学习的糖尿病视网膜病变诊断方法的不足和弱点，并提出了未来研究的潜在方向。这篇综述提供了有关糖尿病视网膜病变诊断的最新深度学习方法的全面视角，将有助于研究人员进一步研究这一问题。
- en: References
  id: totrans-675
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Mookiah et al. [2013] Muthu Rama Krishnan Mookiah, U Rajendra Acharya, Chua Kuang
    Chua, Choo Min Lim, EYK Ng, and Augustinus Laude. Computer-aided diagnosis of
    diabetic retinopathy: A review. *Computers in biology and medicine*, 43(12):2136–2155,
    2013.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mookiah 等人 [2013] 穆图·拉玛·克里希南·穆基亚，U·拉詹德拉·阿查里亚，蔡光财，林敏，EYK Ng 和 奥古斯丁·劳德。《计算机辅助诊断糖尿病视网膜病变：综述》。*生物医学计算机*，43(12):2136–2155，2013。
- en: 'Faust et al. [2012] Oliver Faust, Rajendra Acharya, Eddie Yin-Kwee Ng, Kwan-Hoong
    Ng, and Jasjit S Suri. Algorithms for the automated detection of diabetic retinopathy
    using digital fundus images: a review. *Journal of medical systems*, 36(1):145–157,
    2012.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faust 等人 [2012] 奥利弗·福斯特，拉詹德拉·阿查里亚，埃迪·尹奎·吴，关宏和贾斯吉·S·苏里。《基于数字眼底图像的糖尿病视网膜病变自动检测算法：综述》。*医学系统杂志*，36(1):145–157，2012。
- en: Joshi and Karule [2018] Shilpa Joshi and PT Karule. A review on exudates detection
    methods for diabetic retinopathy. *Biomedicine & Pharmacotherapy*, 97:1454–1460,
    2018.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi 和 Karule [2018] 希尔帕·乔希 和 PT·卡鲁勒。《糖尿病视网膜病变渗出物检测方法的综述》。*生物医学与药物治疗*，97:1454–1460，2018。
- en: 'Mansour [2017] Romany F Mansour. Evolutionary computing enriched computer-aided
    diagnosis system for diabetic retinopathy: A survey. *IEEE reviews in biomedical
    engineering*, 10:334–349, 2017.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mansour [2017] 罗曼尼·F·曼苏尔。《进化计算增强的糖尿病视网膜病变计算机辅助诊断系统：调查》。*IEEE生物医学工程评论*，10:334–349，2017。
- en: 'Almotiri et al. [2018] Jasem Almotiri, Khaled Elleithy, and Abdelrahman Elleithy.
    Retinal vessels segmentation techniques and algorithms: A survey. *Applied Sciences*,
    8(2):155, 2018.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almotiri 等人 [2018] Jasem Almotiri、Khaled Elleithy 和 Abdelrahman Elleithy。视网膜血管分割技术与算法：综述。*应用科学*，8(2):155，2018年。
- en: 'Almazroa et al. [2015] Ahmed Almazroa, Ritambhar Burman, Kaamran Raahemifar,
    and Vasudevan Lakshminarayanan. Optic disc and optic cup segmentation methodologies
    for glaucoma image detection: a survey. *Journal of ophthalmology*, 2015, 2015.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazroa 等人 [2015] Ahmed Almazroa、Ritambhar Burman、Kaamran Raahemifar 和 Vasudevan
    Lakshminarayanan。青光眼图像检测中的视神经盘和视神经杯分割方法：综述。*眼科杂志*，2015年，2015年。
- en: Thakur and Juneja [2018] Niharika Thakur and Mamta Juneja. Survey on segmentation
    and classification approaches of optic cup and optic disc for diagnosis of glaucoma.
    *Biomedical Signal Processing and Control*, 42:162–189, 2018.
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thakur 和 Juneja [2018] Niharika Thakur 和 Mamta Juneja。关于青光眼诊断中视神经杯和视神经盘分割与分类方法的调查。*生物医学信号处理与控制*，42:162–189，2018年。
- en: 'Group et al. [1991] Early Treatment Diabetic Retinopathy Study Research Group
    et al. Grading diabetic retinopathy from stereoscopic color fundus photographs—an
    extension of the modified airlie house classification: Etdrs report number 10.
    *Ophthalmology*, 98(5):786–806, 1991.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Group 等人 [1991] 早期治疗糖尿病视网膜病变研究组等。从立体彩色眼底照片中对糖尿病视网膜病变进行分级——改进版 Airlie House 分类的扩展：ETDRS
    报告第 10 号。*眼科*，98(5):786–806，1991年。
- en: Harney [2006] Fiona Harney. Diabetic retinopathy. *Medicine*, 34(3):95–98, 2006.
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harney [2006] Fiona Harney。糖尿病视网膜病变。*医学*，34(3):95–98，2006年。
- en: McLeod [2005] D McLeod. Why cotton wool spots should not be regarded as retinal
    nerve fibre layer infarcts. *British journal of ophthalmology*, 89(2):229–237,
    2005.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McLeod [2005] D McLeod。为什么棉絮状斑点不应被视为视网膜神经纤维层梗塞。*英国眼科杂志*，89(2):229–237，2005年。
- en: Akram et al. [2014] M Usman Akram, Shehzad Khalid, Anam Tariq, Shoab A Khan,
    and Farooque Azam. Detection and classification of retinal lesions for grading
    of diabetic retinopathy. *Computers in biology and medicine*, 45:161–171, 2014.
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akram 等人 [2014] M Usman Akram、Shehzad Khalid、Anam Tariq、Shoab A Khan 和 Farooque
    Azam。视网膜病变检测与分类用于糖尿病视网膜病变的分级。*生物医学计算机与医学*，45:161–171，2014年。
- en: Lee and Cheng [1994] Tien-You Lee and HD Cheng. Parallel grading of venous beading
    on transputer. In *Proceedings of 1994 20th Annual Northeast Bioengineering Conference*,
    pages 54–58\. IEEE, 1994.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 和 Cheng [1994] Tien-You Lee 和 HD Cheng。在转置机上并行分级静脉珠。在 *1994年第20届东北生物工程会议论文集*，第54–58页。IEEE，1994年。
- en: Patz [1980] Arnall Patz. Studies on retinal neovascularization. friedenwald
    lecture. *Investigative ophthalmology & visual science*, 19(10):1133–1138, 1980.
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patz [1980] Arnall Patz。视网膜新生血管化研究。Friedenwald 讲座。*眼科与视觉科学研究*，19(10):1133–1138，1980年。
- en: '[14] Diabetic retinopathy. [https://www.nhs.uk/conditions/diabetic-retinopathy/stages/](https://www.nhs.uk/conditions/diabetic-retinopathy/stages/).
    Accessed: 2018-01-08.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[糖尿病视网膜病变](https://www.nhs.uk/conditions/diabetic-retinopathy/stages/)。访问日期：2018-01-08。'
- en: 'Group et al. [1987] Early Treatment Diabetic Retinopathy Study Research Group
    et al. Treatment techniques and clinical guidelines for photocoagulation of diabetic
    macular edema: Early treatment diabetic retinopathy study report number 2. *Ophthalmology*,
    94(7):761–774, 1987.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Group 等人 [1987] 早期治疗糖尿病视网膜病变研究组等。糖尿病黄斑水肿光凝治疗技术与临床指南：早期治疗糖尿病视网膜病变研究报告第 2 号。*眼科*，94(7):761–774，1987年。
- en: Sopharak et al. [2008] Akara Sopharak, Bunyarit Uyyanonvara, Sarah Barman, and
    Thomas H Williamson. Automatic detection of diabetic retinopathy exudates from
    non-dilated retinal images using mathematical morphology methods. *Computerized
    medical imaging and graphics*, 32(8):720–727, 2008.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sopharak 等人 [2008] Akara Sopharak、Bunyarit Uyyanonvara、Sarah Barman 和 Thomas
    H Williamson。使用数学形态学方法从非扩张的视网膜图像中自动检测糖尿病视网膜病变渗出物。*计算机医学影像与图形*，32(8):720–727，2008年。
- en: Jonas et al. [1988] Jost B Jonas, Gabriele Ch Gusek, and Gottfried OH Naumann.
    Optic disk morphometry in high myopia. *Graefe’s archive for clinical and experimental
    ophthalmology*, 226(6):587–590, 1988.
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jonas 等人 [1988] Jost B Jonas、Gabriele Ch Gusek 和 Gottfried OH Naumann。高近视的视神经盘形态测量。*格雷费眼科临床与实验档案*，226(6):587–590，1988年。
- en: Joshi et al. [2011] Gopal Datt Joshi, Jayanthi Sivaswamy, and SR Krishnadas.
    Optic disk and cup segmentation from monocular color retinal images for glaucoma
    assessment. *IEEE transactions on medical imaging*, 30(6):1192–1205, 2011.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi 等人 [2011] Gopal Datt Joshi、Jayanthi Sivaswamy 和 SR Krishnadas。从单眼彩色视网膜图像中分割视神经盘和杯子以评估青光眼。*IEEE医学影像学报*，30(6):1192–1205，2011年。
- en: 'Acharya et al. [2009] Udyavara R Acharya, Choo M Lim, E Yin Kwee Ng, Caroline
    Chee, and Toshiyo Tamura. Computer-based detection of diabetes retinopathy stages
    using digital fundus images. *Proceedings of the institution of mechanical engineers,
    part H: journal of engineering in medicine*, 223(5):545–553, 2009.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Acharya et al. [2009] 乌迪亚瓦拉·R·阿查雅、周·M·林、E·尹·奎·吴、卡罗琳·齐和田代·田村。基于计算机的糖尿病视网膜病阶段检测，使用数字眼底图像。*机械工程师学会会刊，第H部分：医学工程学报*,
    223(5):545–553, 2009。
- en: Fleming et al. [2010] Alan D Fleming, Keith A Goatman, Sam Philip, Graeme J
    Williams, Gordon J Prescott, Graham S Scotland, Paul McNamee, Graham P Leese,
    William N Wykes, Peter F Sharp, et al. The role of haemorrhage and exudate detection
    in automated grading of diabetic retinopathy. *British Journal of Ophthalmology*,
    94(6):706–711, 2010.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleming et al. [2010] 阿兰·D·弗莱明、基思·A·戈特曼、萨姆·菲利普、格雷姆·J·威廉姆斯、戈登·J·普雷斯科特、格雷厄姆·S·苏格兰、保罗·麦克内梅、格雷厄姆·P·李斯、威廉·N·怀克斯、彼得·F·夏普等。出血和渗出物检测在糖尿病视网膜病自动分级中的作用。*英国眼科杂志*,
    94(6):706–711, 2010。
- en: '[21] Diabetic retinal screening, grading, monitoring and referral guidance.
    [https://www.health.govt.nz/publication/diabetic-retinal-screening-grading-monitoring-and-referral-guidance](https://www.health.govt.nz/publication/diabetic-retinal-screening-grading-monitoring-and-referral-guidance).
    Accessed: 2019-05-01.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 糖尿病视网膜筛查、分级、监测和转诊指南。 [https://www.health.govt.nz/publication/diabetic-retinal-screening-grading-monitoring-and-referral-guidance](https://www.health.govt.nz/publication/diabetic-retinal-screening-grading-monitoring-and-referral-guidance)。访问时间：2019-05-01。'
- en: 'Kanski [2009] Jack J Kanski. *Clinical ophthalmology: a synopsis*. Elsevier
    Health Sciences, 2009.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanski [2009] 杰克·J·坎斯基。*临床眼科学：概述*。爱思唯尔健康科学, 2009。
- en: Zachariah et al. [2015] Sonia Zachariah, William Wykes, and David Yorston. Grading
    diabetic retinopathy (dr) using the scottish grading protocol. *Community eye
    health*, 28(92):72, 2015.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zachariah et al. [2015] 索尼娅·扎卡里亚、威廉·怀克斯和大卫·约斯顿。使用苏格兰分级协议对糖尿病视网膜病（dr）进行分级。*社区眼健康*,
    28(92):72, 2015。
- en: 'dr- [2015] Diabetic retinopathy (dr): management and referral. In *Community
    Eye Health*, pages 70–71, 2015.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dr- [2015] 糖尿病视网膜病（dr）：管理和转诊。见于 *社区眼健康*, 页70–71, 2015。
- en: Seoud et al. [2016] Lama Seoud, Thomas Hurtut, Jihed Chelbi, Farida Cheriet,
    and JM Pierre Langlois. Red lesion detection using dynamic shape features for
    diabetic retinopathy screening. *IEEE transactions on medical imaging*, 35(4):1116–1126,
    2016.
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seoud et al. [2016] 拉马·塞乌德、托马斯·赫尔图、吉赫德·切尔比、法里达·谢里特和JM·皮埃尔·朗格瓦。使用动态形状特征检测红斑，用于糖尿病视网膜病筛查。*IEEE医学影像交易*,
    35(4):1116–1126, 2016。
- en: Litjens et al. [2017] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud
    Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen AWM van der
    Laak, Bram van Ginneken, and Clara I Sánchez. A survey on deep learning in medical
    image analysis. *Medical image analysis*, 42:60–88, 2017.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Litjens et al. [2017] 吉尔特·利特延斯、泰斯·库伊、巴巴克·厄赫特沙米·贝伊诺尔迪、阿尔诺·阿林德拉·阿迪约索·塞蒂奥、弗朗切斯科·乔姆皮、莫赫森·加霍里安、杰龙·AWM·范·德·拉克、布拉姆·范·吉尼肯和克拉拉·I·桑切斯。关于医学图像分析中的深度学习的调查。*医学图像分析*,
    42:60–88, 2017。
- en: Gondal et al. [2017] W. Gondal, J. M. Köhler, R. Grzeszick, G. Fink, and M. Hirsch.
    Weakly-supervised localization of diabetic retinopathy lesions in retinal fundus
    images. In *IEEE International Conference on Image Processing (ICIP 207)*, 2017.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gondal et al. [2017] W·贡达尔、J·M·科赫勒、R·格雷瑟克、G·芬克和M·赫希。视网膜眼底图像中糖尿病视网膜病病变的弱监督定位。见于
    *IEEE国际图像处理会议（ICIP 207）*, 2017。
- en: Li et al. [2016] Qiaoliang Li, Bowei Feng, LinPei Xie, Ping Liang, Huisheng
    Zhang, and Tianfu Wang. A cross-modality learning approach for vessel segmentation
    in retinal images. *IEEE transactions on medical imaging*, 35(1):109–118, 2016.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2016] 乔利昂·李、博伟·冯、林佩·谢、平·梁、惠生·张和天赋·王。用于视网膜图像血管分割的跨模态学习方法。*IEEE医学影像交易*,
    35(1):109–118, 2016。
- en: Arunkumar and Karthigaikumar [2017] R Arunkumar and P Karthigaikumar. Multi-retinal
    disease classification by reduced deep learning features. *Neural Computing and
    Applications*, 28(2):329–334, 2017.
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arunkumar 和 Karthigaikumar [2017] R·阿伦库马尔和P·卡尔提凯库马尔。通过减少的深度学习特征进行多视网膜疾病分类。*神经计算与应用*,
    28(2):329–334, 2017。
- en: '[30] Messidor dataset. [http://www.adcis.net/en/Download-Third-Party/Messidor.html](http://www.adcis.net/en/Download-Third-Party/Messidor.html).
    Accessed: 2018-01-08.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Messidor 数据集。 [http://www.adcis.net/en/Download-Third-Party/Messidor.html](http://www.adcis.net/en/Download-Third-Party/Messidor.html)。访问时间：2018-01-08。'
- en: '[31] E-ophtha. [http://www.adcis.net/en/Download-Third-Party/E-Ophtha.html](http://www.adcis.net/en/Download-Third-Party/E-Ophtha.html).
    Accessed: 2018-01-08.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] E-ophtha. [http://www.adcis.net/en/Download-Third-Party/E-Ophtha.html](http://www.adcis.net/en/Download-Third-Party/E-Ophtha.html)。访问时间：2018-01-08。'
- en: '[32] Kaggle dataset. [https://www.kaggle.com/c/diabetic-retinopathy-detection/data](https://www.kaggle.com/c/diabetic-retinopathy-detection/data).
    Accessed: 2018-01-08.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Kaggle 数据集。 [https://www.kaggle.com/c/diabetic-retinopathy-detection/data](https://www.kaggle.com/c/diabetic-retinopathy-detection/data)。访问时间：2018-01-08。'
- en: 'dri [a] Drive dataset. [https://www.isi.uu.nl/Research/Databases/DRIVE/](https://www.isi.uu.nl/Research/Databases/DRIVE/),
    a. Accessed: 2018-01-08.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dri [a] Drive 数据集。 [https://www.isi.uu.nl/Research/Databases/DRIVE/](https://www.isi.uu.nl/Research/Databases/DRIVE/)，a。访问时间：2018-01-08。
- en: Hoover et al. [2000] AD Hoover, Valentina Kouznetsova, and Michael Goldbaum.
    Locating blood vessels in retinal images by piecewise threshold probing of a matched
    filter response. *IEEE Transactions on Medical imaging*, 19(3):203–210, 2000.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoover 等人 [2000] AD Hoover、Valentina Kouznetsova 和 Michael Goldbaum。通过逐段阈值探测匹配滤波响应来定位视网膜图像中的血管。*IEEE
    医学成像汇刊*，19(3)：203–210，2000年。
- en: Kälviäinen and Uusitalo [2007] RVJPH Kälviäinen and H Uusitalo. Diaretdb1 diabetic
    retinopathy database and evaluation protocol. In *Medical Image Understanding
    and Analysis*, volume 2007, page 61\. Citeseer, 2007.
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kälviäinen 和 Uusitalo [2007] RVJPH Kälviäinen 和 H Uusitalo。Diaretdb1 糖尿病视网膜病变数据库及评估协议。在
    *医学图像理解与分析*，卷2007，第61页。Citeseer，2007年。
- en: '[36] Diaretdb1 dataset. [http://www.it.lut.fi/project/imageret/diaretdb1/](http://www.it.lut.fi/project/imageret/diaretdb1/).
    Accessed: 2018-01-08.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Diaretdb1 数据集。 [http://www.it.lut.fi/project/imageret/diaretdb1/](http://www.it.lut.fi/project/imageret/diaretdb1/)。访问时间：2018-01-08。'
- en: '[37] Chase dataset. [http://www.chasestudy.ac.uk/](http://www.chasestudy.ac.uk/).
    Accessed: 2018-02-01.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Chase 数据集。 [http://www.chasestudy.ac.uk/](http://www.chasestudy.ac.uk/)。访问时间：2018-02-01。'
- en: 'Prentasic et al. [2013] Pavle Prentasic, Sven Loncaric, Zoran Vatavuk, Goran
    Bencic, Marko Subasic, Tomislav Petkovic, Lana Dujmovic, Maja Malenica-Ravlic,
    Nikolina Budimlija, and Raseljka Tadic. Diabetic retinopathy image database (dridb):
    a new database for diabetic retinopathy screening programs research. In *Image
    and Signal Processing and Analysis (ISPA), 2013 8th International Symposium on*,
    pages 711–716\. IEEE, 2013.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prentasic 等人 [2013] Pavle Prentasic、Sven Loncaric、Zoran Vatavuk、Goran Bencic、Marko
    Subasic、Tomislav Petkovic、Lana Dujmovic、Maja Malenica-Ravlic、Nikolina Budimlija
    和 Raseljka Tadic。糖尿病视网膜病变图像数据库 (dridb)：一个用于糖尿病视网膜病变筛查程序研究的新数据库。在 *图像与信号处理与分析 (ISPA)，2013年第八届国际研讨会*，第711–716页。IEEE，2013年。
- en: 'Zhang et al. [2010] Zhuo Zhang, Feng Shou Yin, Jiang Liu, Wing Kee Wong, Ngan Meng
    Tan, Beng Hai Lee, Jun Cheng, and Tien Yin Wong. Origa-light: An online retinal
    fundus image database for glaucoma analysis and research. In *Engineering in Medicine
    and Biology Society (EMBC), 2010 Annual International Conference of the IEEE*,
    pages 3065–3068\. IEEE, 2010.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2010] Zhuo Zhang、Feng Shou Yin、Jiang Liu、Wing Kee Wong、Ngan Meng Tan、Beng
    Hai Lee、Jun Cheng 和 Tien Yin Wong。Origa-light：一个用于青光眼分析和研究的在线视网膜底片图像数据库。在 *医学与生物工程学会
    (EMBC)，2010年IEEE年会*，第3065–3068页。IEEE，2010年。
- en: 'Sng et al. [2012] Chelvin C Sng, Li-Lian Foo, Ching-Yu Cheng, John C Allen,
    Mingguang He, Gita Krishnaswamy, Monisha E Nongpiur, David S Friedman, Tien Y
    Wong, and Tin Aung. Determinants of anterior chamber depth: the singapore chinese
    eye study. *Ophthalmology*, 119(6):1143–1150, 2012.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sng 等人 [2012] Chelvin C Sng、Li-Lian Foo、Ching-Yu Cheng、John C Allen、Mingguang
    He、Gita Krishnaswamy、Monisha E Nongpiur、David S Friedman、Tien Y Wong 和 Tin Aung。前房深度的决定因素：新加坡华人眼科研究。*眼科学*，119(6)：1143–1150，2012年。
- en: '[41] Nih areds dataset. [https://www.nih.gov/news-events/news-releases/nih-adds-first-images-major-research-database](https://www.nih.gov/news-events/news-releases/nih-adds-first-images-major-research-database).
    Accessed: 2018-02-01.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Nih areds 数据集。 [https://www.nih.gov/news-events/news-releases/nih-adds-first-images-major-research-database](https://www.nih.gov/news-events/news-releases/nih-adds-first-images-major-research-database)。访问时间：2018-02-01。'
- en: Al-Diri et al. [2008] Bashir Al-Diri, Andrew Hunter, David Steel, Maged Habib,
    Taghread Hudaib, and Simon Berry. A reference data set for retinal vessel profiles.
    In *Engineering in Medicine and Biology Society, 2008\. EMBS 2008\. 30th Annual
    International Conference of the IEEE*, pages 2262–2265. IEEE, 2008.
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Diri 等人 [2008] Bashir Al-Diri、Andrew Hunter、David Steel、Maged Habib、Taghread
    Hudaib 和 Simon Berry。视网膜血管轮廓的参考数据集。在 *医学与生物工程学会，2008年\. EMBS 2008\. 第30届IEEE年会*，第2262–2265页。IEEE，2008年。
- en: '[43] Eyepacs dataset. [http://www.eyepacs.com/eyepacssystem/](http://www.eyepacs.com/eyepacssystem/).
    Accessed: 2018-03-01.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Eyepacs 数据集。 [http://www.eyepacs.com/eyepacssystem/](http://www.eyepacs.com/eyepacssystem/)。访问时间：2018-03-01。'
- en: 'Fumero et al. [2011] Francisco Fumero, Silvia Alayón, JL Sanchez, J Sigut,
    and M Gonzalez-Hernandez. Rim-one: An open retinal image database for optic nerve
    evaluation. In *Computer-Based Medical Systems (CBMS), 2011 24th International
    Symposium on*, pages 1–6\. IEEE, 2011.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fumero et al. [2011] Francisco Fumero, Silvia Alayón, JL Sanchez, J Sigut,
    和 M Gonzalez-Hernandez。Rim-one: 一个用于视神经评估的开放视网膜图像数据库。在*计算机基础医疗系统 (CBMS)，2011年第24届国际研讨会*上，第1–6页。IEEE，2011年。'
- en: 'Sivaswamy et al. [2014] Jayanthi Sivaswamy, SR Krishnadas, Gopal Datt Joshi,
    Madhulika Jain, and A Ujjwaft Syed Tabish. Drishti-gs: Retinal image dataset for
    optic nerve head (onh) segmentation. In *Biomedical Imaging (ISBI), 2014 IEEE
    11th International Symposium on*, pages 53–56\. IEEE, 2014.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sivaswamy et al. [2014] Jayanthi Sivaswamy, SR Krishnadas, Gopal Datt Joshi,
    Madhulika Jain, 和 A Ujjwaft Syed Tabish。Drishti-gs: 用于视神经头 (onh) 分割的视网膜图像数据集。在*生物医学成像
    (ISBI)，2014年IEEE第11届国际研讨会*上，第53–56页。IEEE，2014年。'
- en: '[46] Aria dataset. [http://www.eyecharity.com/aria_online.html](http://www.eyecharity.com/aria_online.html).
    Accessed: 2018-02-28.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Aria 数据集。 [http://www.eyecharity.com/aria_online.html](http://www.eyecharity.com/aria_online.html)。访问日期：2018-02-28。'
- en: 'dri [b] drion dataset. [http://www.ia.uned.es/~ejcarmona/DRIONS-DB.html](http://www.ia.uned.es/~ejcarmona/DRIONS-DB.html),
    b. Accessed: 2018-04-30.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dri [b] drion 数据集。 [http://www.ia.uned.es/~ejcarmona/DRIONS-DB.html](http://www.ia.uned.es/~ejcarmona/DRIONS-DB.html)，b。访问日期：2018-04-30。
- en: '[48] Seed-db. [https://www.seri.com.sg/key-programmes/singapore-epidemiology-of-eye-diseases-seed/](https://www.seri.com.sg/key-programmes/singapore-epidemiology-of-eye-diseases-seed/).
    Assessed on 2018-08-08.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Seed-db。 [https://www.seri.com.sg/key-programmes/singapore-epidemiology-of-eye-diseases-seed/](https://www.seri.com.sg/key-programmes/singapore-epidemiology-of-eye-diseases-seed/)。评估日期：2018-08-08。'
- en: 'Decencière et al. [2014] Etienne Decencière, Xiwei Zhang, Guy Cazuguel, Bruno
    Lay, Béatrice Cochener, Caroline Trone, Philippe Gain, Richard Ordonez, Pascale
    Massin, Ali Erginay, et al. Feedback on a publicly distributed image database:
    the messidor database. *Image Analysis & Stereology*, 33(3):231–234, 2014.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Decencière et al. [2014] Etienne Decencière, Xiwei Zhang, Guy Cazuguel, Bruno
    Lay, Béatrice Cochener, Caroline Trone, Philippe Gain, Richard Ordonez, Pascale
    Massin, Ali Erginay, 等。关于公开分发图像数据库的反馈: messidor 数据库。*图像分析与立体学*，33(3):231–234，2014年。'
- en: Mitchell [2004] Tom M Mitchell. The role of unlabeled data in supervised learning.
    In *Language, Knowledge, and Representation*, pages 103–111. Springer, 2004.
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell [2004] Tom M Mitchell。未标记数据在监督学习中的作用。在*语言、知识与表征*，第103–111页。Springer，2004年。
- en: Shankaranarayana et al. [2017] Sharath M Shankaranarayana, Keerthi Ram, Kaushik
    Mitra, and Mohanasankar Sivaprakasam. Joint optic disc and cup segmentation using
    fully convolutional and adversarial networks. In *Fetal, Infant and Ophthalmic
    Medical Image Analysis*, pages 168–176\. Springer, 2017.
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shankaranarayana et al. [2017] Sharath M Shankaranarayana, Keerthi Ram, Kaushik
    Mitra, 和 Mohanasankar Sivaprakasam。使用全卷积和对抗网络进行联合视盘和杯分割。在*胎儿、婴儿及眼科医学图像分析*，第168–176页。Springer，2017年。
- en: Srivastava et al. [2015] Ruchir Srivastava, Jun Cheng, Damon WK Wong, and Jiang
    Liu. Using deep learning for robustness to parapapillary atrophy in optic disc
    segmentation. In *Biomedical Imaging (ISBI), 2015 IEEE 12th International Symposium
    on*, pages 768–771\. IEEE, 2015.
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava et al. [2015] Ruchir Srivastava, Jun Cheng, Damon WK Wong, 和 Jiang
    Liu。利用深度学习提高视盘分割对视盘周围萎缩的鲁棒性。在*生物医学成像 (ISBI)，2015年IEEE第12届国际研讨会*上，第768–771页。IEEE，2015年。
- en: 'Zou et al. [2004] Kelly H Zou, Simon K Warfield, Aditya Bharatha, Clare MC
    Tempany, Michael R Kaus, Steven J Haker, William M Wells, Ferenc A Jolesz, and
    Ron Kikinis. Statistical validation of image segmentation quality based on a spatial
    overlap index1: scientific reports. *Academic radiology*, 11(2):178–189, 2004.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zou et al. [2004] Kelly H Zou, Simon K Warfield, Aditya Bharatha, Clare MC
    Tempany, Michael R Kaus, Steven J Haker, William M Wells, Ferenc A Jolesz, 和 Ron
    Kikinis。基于空间重叠指数的图像分割质量的统计验证1: 科学报告。*学术放射学*，11(2):178–189，2004年。'
- en: Zhang et al. [2015] Xueliang Zhang, Xuezhi Feng, Pengfeng Xiao, Guangjun He,
    and Liujun Zhu. Segmentation quality evaluation using region-based precision and
    recall measures for remote sensing images. *ISPRS Journal of Photogrammetry and
    Remote Sensing*, 102:73–84, 2015.
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2015] Xueliang Zhang, Xuezhi Feng, Pengfeng Xiao, Guangjun He,
    和 Liujun Zhu。使用基于区域的精度和召回度量评估遥感图像的分割质量。*ISPRS摄影测量与遥感期刊*，102:73–84，2015年。
- en: LeCun et al. [1998] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    Gradient-based learning applied to document recognition. *Proceedings of the IEEE*,
    86(11):2278–2324, 1998.
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. [1998] Yann LeCun, Léon Bottou, Yoshua Bengio, 和 Patrick Haffner。应用于文档识别的基于梯度的学习。*IEEE学报*，86(11):2278–2324，1998年。
- en: Krizhevsky et al. [2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    Imagenet classification with deep convolutional neural networks. In *Advances
    in neural information processing systems*, pages 1097–1105, 2012.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等人 [2012] Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E Hinton。使用深度卷积神经网络进行
    Imagenet 分类。发表于 *神经信息处理系统进展*，第1097–1105页，2012年。
- en: Simonyan and Zisserman [2014] Karen Simonyan and Andrew Zisserman. Very deep
    convolutional networks for large-scale image recognition. *arXiv preprint arXiv:1409.1556*,
    2014.
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman [2014] Karen Simonyan 和 Andrew Zisserman。用于大规模图像识别的非常深的卷积网络。
    *arXiv 预印本 arXiv:1409.1556*，2014年。
- en: Szegedy et al. [2015] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich,
    et al. Going deeper with convolutions. Cvpr, 2015.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等人 [2015] Christian Szegedy、Wei Liu、Yangqing Jia、Pierre Sermanet、Scott
    Reed、Dragomir Anguelov、Dumitru Erhan、Vincent Vanhoucke、Andrew Rabinovich 等。通过卷积进一步深入。Cvpr，2015年。
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 770–778, 2016.
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2016] Kaiming He、Xiangyu Zhang、Shaoqing Ren 和 Jian Sun。用于图像识别的深度残差学习。发表于
    *IEEE计算机视觉与模式识别会议论文集*，第770–778页，2016年。
- en: 'Tajbakhsh et al. [2016] Nima Tajbakhsh, Jae Y Shin, Suryakanth R Gurudu, R Todd
    Hurst, Christopher B Kendall, Michael B Gotway, and Jianming Liang. Convolutional
    neural networks for medical image analysis: Full training or fine tuning? *IEEE
    transactions on medical imaging*, 35(5):1299–1312, 2016.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tajbakhsh 等人 [2016] Nima Tajbakhsh、Jae Y Shin、Suryakanth R Gurudu、R Todd Hurst、Christopher
    B Kendall、Michael B Gotway 和 Jianming Liang。用于医学图像分析的卷积神经网络：完全训练还是微调？ *IEEE医学影像学汇刊*，35(5):1299–1312，2016年。
- en: Erhan et al. [2010] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine
    Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training
    help deep learning? *Journal of Machine Learning Research*, 11(Feb):625–660, 2010.
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erhan 等人 [2010] Dumitru Erhan、Yoshua Bengio、Aaron Courville、Pierre-Antoine Manzagol、Pascal
    Vincent 和 Samy Bengio。为什么无监督预训练有助于深度学习？ *机器学习研究期刊*，11(Feb):625–660，2010年。
- en: Long et al. [2015] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
    convolutional networks for semantic segmentation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 3431–3440, 2015.
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等人 [2015] Jonathan Long、Evan Shelhamer 和 Trevor Darrell。用于语义分割的全卷积网络。发表于
    *IEEE计算机视觉与模式识别会议论文集*，第3431–3440页，2015年。
- en: Hinton and Salakhutdinov [2006] Geoffrey E Hinton and Ruslan R Salakhutdinov.
    Reducing the dimensionality of data with neural networks. *science*, 313(5786):504–507,
    2006.
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 和 Salakhutdinov [2006] Geoffrey E Hinton 和 Ruslan R Salakhutdinov。利用神经网络减少数据的维度。
    *科学*，313(5786):504–507，2006年。
- en: Liou et al. [2014] Cheng-Yuan Liou, Wei-Chen Cheng, Jiun-Wei Liou, and Daw-Ran
    Liou. Autoencoder for words. *Neurocomputing*, 139:84–96, 2014.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liou 等人 [2014] Cheng-Yuan Liou、Wei-Chen Cheng、Jiun-Wei Liou 和 Daw-Ran Liou。用于词汇的自编码器。
    *神经计算*，139:84–96，2014年。
- en: Maji et al. [2015] Debapriya Maji, Anirban Santara, Sambuddha Ghosh, Debdoot
    Sheet, and Pabitra Mitra. Deep neural network and random forest hybrid architecture
    for learning to detect retinal vessels in fundus images. In *Engineering in Medicine
    and Biology Society (EMBC), 2015 37th Annual International Conference of the IEEE*,
    pages 3029–3032\. IEEE, 2015.
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maji 等人 [2015] Debapriya Maji、Anirban Santara、Sambuddha Ghosh、Debdoot Sheet
    和 Pabitra Mitra。用于学习检测视网膜血管的深度神经网络与随机森林混合架构。发表于 *工程医学与生物学学会（EMBC），2015年第37届IEEE国际会议*，第3029–3032页。IEEE，2015年。
- en: Mikolov et al. [2010] Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černockỳ,
    and Sanjeev Khudanpur. Recurrent neural network based language model. In *Eleventh
    Annual Conference of the International Speech Communication Association*, 2010.
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等人 [2010] Tomáš Mikolov、Martin Karafiát、Lukáš Burget、Jan Černockỳ 和
    Sanjeev Khudanpur。基于递归神经网络的语言模型。发表于 *第十一届国际语音通信协会年会*，2010年。
- en: 'Vinyals et al. [2017] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru
    Erhan. Show and tell: Lessons learned from the 2015 mscoco image captioning challenge.
    *IEEE transactions on pattern analysis and machine intelligence*, 39(4):652–663,
    2017.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等人 [2017] Oriol Vinyals、Alexander Toshev、Samy Bengio 和 Dumitru Erhan。展示与讲述：从2015年mscoco图像字幕挑战赛中学到的经验。
    *IEEE模式分析与机器智能汇刊*，39(4):652–663，2017年。
- en: Maji et al. [2016] Debapriya Maji, Anirban Santara, Pabitra Mitra, and Debdoot
    Sheet. Ensemble of deep convolutional neural networks for learning to detect retinal
    vessels in fundus images. *arXiv preprint arXiv:1603.04833*, 2016.
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maji 等 [2016] Debapriya Maji, Anirban Santara, Pabitra Mitra, 和 Debdoot Sheet.
    用于学习检测视网膜血管的深度卷积神经网络集成。*arXiv 预印本 arXiv:1603.04833*，2016年。
- en: Liskowski and Krawiec [2016] Paweł Liskowski and Krzysztof Krawiec. Segmenting
    retinal blood vessels with deep neural networks. *IEEE transactions on medical
    imaging*, 35(11):2369–2380, 2016.
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liskowski 和 Krawiec [2016] Paweł Liskowski 和 Krzysztof Krawiec. 使用深度神经网络分割视网膜血管。*IEEE
    医学成像交易*，35(11):2369–2380，2016年。
- en: Maninis et al. [2016] Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbeláez,
    and Luc Van Gool. Deep retinal image understanding. In *International Conference
    on Medical Image Computing and Computer-Assisted Intervention*, pages 140–148\.
    Springer, 2016.
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maninis 等 [2016] Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbeláez, 和
    Luc Van Gool. 深度视网膜图像理解。见 *医学图像计算与计算机辅助干预国际会议*，页码140–148。Springer，2016年。
- en: 'Wu et al. [2016] Aaron Wu, Ziyue Xu, Mingchen Gao, Mario Buty, and Daniel J
    Mollura. Deep vessel tracking: A generalized probabilistic approach via deep learning.
    In *Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium on*, pages
    1363–1367\. IEEE, 2016.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 [2016] Aaron Wu, Ziyue Xu, Mingchen Gao, Mario Buty, 和 Daniel J Mollura.
    深度血管跟踪：一种通过深度学习的广义概率方法。见 *生物医学成像（ISBI），2016 IEEE 第十三届国际研讨会*，页码1363–1367。IEEE，2016年。
- en: Dasgupta and Singh [2017] Avijit Dasgupta and Sonam Singh. A fully convolutional
    neural network based structured prediction approach towards the retinal vessel
    segmentation. In *Biomedical Imaging (ISBI 2017), 2017 IEEE 14th International
    Symposium on*, pages 248–251\. IEEE, 2017.
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasgupta 和 Singh [2017] Avijit Dasgupta 和 Sonam Singh. 基于完全卷积神经网络的结构化预测方法用于视网膜血管分割。见
    *生物医学成像（ISBI 2017），2017 IEEE 第十四届国际研讨会*，页码248–251。IEEE，2017年。
- en: Tan et al. [2017] Jen Hong Tan, U Rajendra Acharya, Sulatha V Bhandary, Kuang Chua
    Chua, and Sobha Sivaprasad. Segmentation of optic disc, fovea and retinal vasculature
    using a single convolutional neural network. *Journal of Computational Science*,
    20:70–79, 2017.
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等 [2017] Jen Hong Tan, U Rajendra Acharya, Sulatha V Bhandary, Kuang Chua
    Chua, 和 Sobha Sivaprasad. 使用单一卷积神经网络分割视神经盘、中央凹和视网膜血管。*计算科学杂志*，20:70–79，2017年。
- en: Fu et al. [2016a] Huazhu Fu, Yanwu Xu, Damon Wing Kee Wong, and Jiang Liu. Retinal
    vessel segmentation via deep learning network and fully-connected conditional
    random fields. In *Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium
    on*, pages 698–701\. IEEE, 2016a.
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 [2016a] Huazhu Fu, Yanwu Xu, Damon Wing Kee Wong, 和 Jiang Liu. 通过深度学习网络和全连接条件随机场进行视网膜血管分割。见
    *生物医学成像（ISBI），2016 IEEE 第十三届国际研讨会*，页码698–701。IEEE，2016年。
- en: Mo and Zhang [2017] Juan Mo and Lei Zhang. Multi-level deep supervised networks
    for retinal vessel segmentation. *International journal of computer assisted radiology
    and surgery*, 12(12):2181–2193, 2017.
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mo 和 Zhang [2017] Juan Mo 和 Lei Zhang. 多层次深度监督网络用于视网膜血管分割。*计算机辅助放射学与外科国际杂志*，12(12):2181–2193，2017年。
- en: 'Roy and Sheet [2015] Abhijit Guha Roy and Debdoot Sheet. Dasa: Domain adaptation
    in stacked autoencoders using systematic dropout. In *Pattern Recognition (ACPR),
    2015 3rd IAPR Asian Conference on*, pages 735–739\. IEEE, 2015.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy 和 Sheet [2015] Abhijit Guha Roy 和 Debdoot Sheet. Dasa：在堆叠自编码器中使用系统化丢弃的领域适应。见
    *模式识别（ACPR），2015 第三届 IAPR 亚洲会议*，页码735–739。IEEE，2015年。
- en: Lahiri et al. [2016] Avisek Lahiri, Abhijit Guha Roy, Debdoot Sheet, and Prabir Kumar
    Biswas. Deep neural ensemble for retinal vessel segmentation in fundus images
    towards achieving label-free angiography. In *Engineering in Medicine and Biology
    Society (EMBC), 2016 IEEE 38th Annual International Conference of the*, pages
    1340–1343\. IEEE, 2016.
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lahiri 等 [2016] Avisek Lahiri, Abhijit Guha Roy, Debdoot Sheet, 和 Prabir Kumar
    Biswas. 用于视网膜血管分割的深度神经集成，旨在实现无标签血管造影。见 *医学与生物学工程学会（EMBC），2016 IEEE 第三十八届年度国际会议*，页码1340–1343。IEEE，2016年。
- en: 'Fu et al. [2016b] Huazhu Fu, Yanwu Xu, Stephen Lin, Damon Wing Kee Wong, and
    Jiang Liu. Deepvessel: Retinal vessel segmentation via deep learning and conditional
    random field. In *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, pages 132–139\. Springer, 2016b.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 [2016b] Huazhu Fu, Yanwu Xu, Stephen Lin, Damon Wing Kee Wong, 和 Jiang
    Liu. Deepvessel：通过深度学习和条件随机场进行视网膜血管分割。见 *医学图像计算与计算机辅助干预国际会议*，页码132–139。Springer，2016年。
- en: Lim et al. [2015] Gilbert Lim, Yuan Cheng, Wynne Hsu, and Mong Li Lee. Integrated
    optic disc and cup segmentation with deep learning. pages 162–169, 2015.
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lim等人 [2015] Gilbert Lim, Yuan Cheng, Wynne Hsu, 和 Mong Li Lee。基于深度学习的综合视盘和视杯分割。第162–169页，2015年。
- en: Guo et al. [2016] Yundi Guo, Beiji Zou, Zailiang Chen, Qi He, Qing Liu, and
    Rongchang Zhao. Optic cup segmentation using large pixel patch based cnns. 2016.
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人 [2016] Yundi Guo, Beiji Zou, Zailiang Chen, Qi He, Qing Liu, 和 Rongchang
    Zhao。使用大像素补丁基于CNN的视杯分割。2016年。
- en: Sevastopolsky [2017] Artem Sevastopolsky. Optic disc and cup segmentation methods
    for glaucoma detection with modification of u-net convolutional neural network.
    *Pattern Recognition and Image Analysis*, 27(3):618–624, 2017.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sevastopolsky [2017] Artem Sevastopolsky。用于青光眼检测的视盘和视杯分割方法，修改的U-Net卷积神经网络。*模式识别与图像分析*，27(3)：618–624，2017年。
- en: 'Ronneberger et al. [2015] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    U-net: Convolutional networks for biomedical image segmentation. In *International
    Conference on Medical image computing and computer-assisted intervention*, pages
    234–241\. Springer, 2015.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ronneberger等人 [2015] Olaf Ronneberger, Philipp Fischer, 和 Thomas Brox。U-net：用于生物医学图像分割的卷积网络。见于*国际医学图像计算与计算机辅助干预会议*，第234–241页。Springer，2015年。
- en: Zilly et al. [2015] Julian G Zilly, Joachim M Buhmann, and Dwarikanath Mahapatra.
    Boosting convolutional filters with entropy sampling for optic cup and disc image
    segmentation from fundus images. pages 136–143, 2015.
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zilly等人 [2015] Julian G Zilly, Joachim M Buhmann, 和 Dwarikanath Mahapatra。通过熵采样增强卷积滤波器，用于眼底图像的视杯和视盘分割。第136–143页，2015年。
- en: Zilly et al. [2017] Julian Zilly, Joachim M Buhmann, and Dwarikanath Mahapatra.
    Glaucoma detection using entropy sampling and ensemble learning for automatic
    optic cup and disc segmentation. *Computerized Medical Imaging and Graphics*,
    55:28–41, 2017.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zilly等人 [2017] Julian Zilly, Joachim M Buhmann, 和 Dwarikanath Mahapatra。使用熵采样和集成学习的青光眼检测，自动视杯和视盘分割。*计算机化医学成像与图形*，55：28–41，2017年。
- en: 'Zhang et al. [2018] Defeng Zhang, Weifang Zhu, Heming Zhao, Fei Shi, and Xinjian
    Chen. Automatic localization and segmentation of optical disk based on faster
    r-cnn and level set in fundus image. In *Medical Imaging 2018: Image Processing*,
    volume 10574, page 105741U. International Society for Optics and Photonics, 2018.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人 [2018] Defeng Zhang, Weifang Zhu, Heming Zhao, Fei Shi, 和 Xinjian Chen。基于Faster
    R-CNN和Level Set的眼底图像光盘的自动定位和分割。见于*医学成像2018：图像处理*，第10574卷，第105741U页。国际光学与光子学学会，2018年。
- en: Fu et al. [2018] Huazhu Fu, Jun Cheng, Yanwu Xu, Damon Wing Kee Wong, Jiang
    Liu, and Xiaochun Cao. Joint optic disc and cup segmentation based on multi-label
    deep network and polar transformation. *arXiv preprint arXiv:1801.00926*, 2018.
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu等人 [2018] Huazhu Fu, Jun Cheng, Yanwu Xu, Damon Wing Kee Wong, Jiang Liu,
    和 Xiaochun Cao。基于多标签深度网络和极坐标变换的联合视盘和视杯分割。*arXiv预印本arXiv:1801.00926*，2018年。
- en: Niu et al. [2017] Di Niu, Peiyuan Xu, Cheng Wan, Jun Cheng, and Jiang Liu. Automatic
    localization of optic disc based on deep learning in fundus images. In *Signal
    and Image Processing (ICSIP), 2017 IEEE 2nd International Conference on*, pages
    208–212\. IEEE, 2017.
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu等人 [2017] Di Niu, Peiyuan Xu, Cheng Wan, Jun Cheng, 和 Jiang Liu。基于深度学习的眼底图像视盘自动定位。见于*信号与图像处理（ICSIP），2017年IEEE第二届国际会议*，第208–212页。IEEE，2017年。
- en: 'Alghamdi et al. [2016] Hanan S Alghamdi, Hongying Lilian Tang, Saad A Waheeb,
    and Tunde Peto. Automatic optic disc abnormality detection in fundus images: a
    deep learning approach. 2016.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alghamdi等人 [2016] Hanan S Alghamdi, Hongying Lilian Tang, Saad A Waheeb, 和 Tunde
    Peto。基于深度学习的眼底图像自动视盘异常检测。2016年。
- en: Xu et al. [2017] Peiyuan Xu, Cheng Wan, Jun Cheng, Di Niu, and Jiang Liu. Optic
    disc detection via deep learning in fundus images. In *Fetal, Infant and Ophthalmic
    Medical Image Analysis*, pages 134–141\. Springer, 2017.
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等人 [2017] Peiyuan Xu, Cheng Wan, Jun Cheng, Di Niu, 和 Jiang Liu。通过深度学习在眼底图像中检测视盘。见于*胎儿、婴儿和眼科医学图像分析*，第134–141页。Springer，2017年。
- en: 'Foong et al. [2007] Athena WP Foong, Seang-Mei Saw, Jing-Liang Loo, Sunny Shen,
    Seng-Chee Loon, Mohamad Rosman, Tin Aung, Donald TH Tan, E Shyong Tai, and Tien Y
    Wong. Rationale and methodology for a population-based study of eye diseases in
    malay people: The singapore malay eye study (simes). *Ophthalmic epidemiology*,
    14(1):25–35, 2007.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foong等人 [2007] Athena WP Foong, Seang-Mei Saw, Jing-Liang Loo, Sunny Shen, Seng-Chee
    Loon, Mohamad Rosman, Tin Aung, Donald TH Tan, E Shyong Tai, 和 Tien Y Wong。马来人群眼病的流行病学研究的理论依据和方法：新加坡马来眼研究（SIMES）。*眼科学流行病学*，14(1)：25–35，2007年。
- en: Abràmoff et al. [2016] Michael David Abràmoff, Yiyue Lou, Ali Erginay, Warren
    Clarida, Ryan Amelon, James C Folk, and Meindert Niemeijer. Improved automated
    detection of diabetic retinopathy on a publicly available dataset through integration
    of deep learning. *Investigative ophthalmology & visual science*, 57(13):5200–5206,
    2016.
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abràmoff 等人 [2016] Michael David Abràmoff, Yiyue Lou, Ali Erginay, Warren Clarida,
    Ryan Amelon, James C Folk 和 Meindert Niemeijer。通过深度学习集成改进了对公开数据集中糖尿病视网膜病的自动检测。*Investigative
    ophthalmology & visual science*，57(13):5200–5206，2016年。
- en: Perdomo et al. [2016] Oscar Perdomo, Sebastian Otalora, Francisco Rodríguez,
    John Arevalo, and Fabio A González. A novel machine learning model based on exudate
    localization to detect diabetic macular edema. 2016.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perdomo 等人 [2016] Oscar Perdomo, Sebastian Otalora, Francisco Rodríguez, John
    Arevalo 和 Fabio A González。基于渗出物定位的新型机器学习模型检测糖尿病性黄斑水肿。2016年。
- en: Burlina et al. [2016] Philippe Burlina, David E Freund, Neil Joshi, Y Wolfson,
    and Neil M Bressler. Detection of age-related macular degeneration via deep learning.
    In *Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium on*, pages
    184–188\. IEEE, 2016.
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burlina 等人 [2016] Philippe Burlina, David E Freund, Neil Joshi, Y Wolfson 和
    Neil M Bressler。通过深度学习检测年龄相关性黄斑变性。在*生物医学成像 (ISBI)，2016 IEEE 第十三届国际研讨会*，页184–188。IEEE，2016年。
- en: Al-Bander et al. [2016] Baidaa Al-Bander, Waleed Al-Nuaimy, Majid A Al-Taee,
    Bryan M Williams, and Yalin Zheng. Diabetic macular edema grading based on deep
    neural networks. 2016.
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Bander 等人 [2016] Baidaa Al-Bander, Waleed Al-Nuaimy, Majid A Al-Taee, Bryan
    M Williams 和 Yalin Zheng。基于深度神经网络的糖尿病性黄斑水肿分级。2016年。
- en: Ting et al. [2017] Daniel Shu Wei Ting, Carol Yim-Lui Cheung, Gilbert Lim, Gavin
    Siew Wei Tan, Nguyen D Quang, Alfred Gan, Haslina Hamzah, Renata Garcia-Franco,
    Ian Yew San Yeo, Shu Yen Lee, et al. Development and validation of a deep learning
    system for diabetic retinopathy and related eye diseases using retinal images
    from multiethnic populations with diabetes. *Jama*, 318(22):2211–2223, 2017.
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ting 等人 [2017] Daniel Shu Wei Ting, Carol Yim-Lui Cheung, Gilbert Lim, Gavin
    Siew Wei Tan, Nguyen D Quang, Alfred Gan, Haslina Hamzah, Renata Garcia-Franco,
    Ian Yew San Yeo, Shu Yen Lee 等人。基于多种族糖尿病人群视网膜图像的深度学习系统开发与验证，用于糖尿病视网膜病及相关眼病。*Jama*，318(22):2211–2223，2017年。
- en: Mo et al. [2018] Juan Mo, Lei Zhang, and Yangqin Feng. Exudate-based diabetic
    macular edema recognition in retinal images using cascaded deep residual networks.
    *Neurocomputing*, 290:161–171, 2018.
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mo 等人 [2018] Juan Mo, Lei Zhang 和 Yangqin Feng。使用级联深度残差网络在视网膜图像中识别基于渗出的糖尿病性黄斑水肿。*Neurocomputing*，290:161–171，2018年。
- en: '[97] Hei-med dataset. [http://www.genenetwork.org/dbdoc/Eye_M2_0908_R.html](http://www.genenetwork.org/dbdoc/Eye_M2_0908_R.html).
    Accessed: 2018-11-02.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Hei-med 数据集。 [http://www.genenetwork.org/dbdoc/Eye_M2_0908_R.html](http://www.genenetwork.org/dbdoc/Eye_M2_0908_R.html)。访问时间：2018-11-02。'
- en: Prentašić and Lončarić [2016] Pavle Prentašić and Sven Lončarić. Detection of
    exudates in fundus photographs using deep neural networks and anatomical landmark
    detection fusion. *Computer methods and programs in biomedicine*, 137:281–292,
    2016.
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prentašić 和 Lončarić [2016] Pavle Prentašić 和 Sven Lončarić。利用深度神经网络和解剖标志检测融合检测眼底照片中的渗出物。*Computer
    methods and programs in biomedicine*，137:281–292，2016年。
- en: Perdomo et al. [2017] Oscar Perdomo, John Arevalo, and Fabio A González. Convolutional
    network to detect exudates in eye fundus images of diabetic subjects. In *12th
    International Symposium on Medical Information Processing and Analysis*, volume
    10160, page 101600T. International Society for Optics and Photonics, 2017.
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perdomo 等人 [2017] Oscar Perdomo, John Arevalo 和 Fabio A González。卷积网络用于检测糖尿病患者眼底图像中的渗出物。在*第十二届医学信息处理与分析国际研讨会*，第10160卷，第101600T页。国际光学和光子学学会，2017年。
- en: '[100] o o cnn solution. [https://www.kaggle.com/c/diabetic-retinopathydetection/discussion/15617](https://www.kaggle.com/c/diabetic-retinopathydetection/discussion/15617).
    Accessed: 2017-01-16.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] o o cnn 解决方案。 [https://www.kaggle.com/c/diabetic-retinopathydetection/discussion/15617](https://www.kaggle.com/c/diabetic-retinopathydetection/discussion/15617)。访问时间：2017-01-16。'
- en: Zhou et al. [2016] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. Learning deep features for discriminative localization. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 2921–2929,
    2016.
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2016] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva 和 Antonio
    Torralba。学习深度特征以进行判别定位。在*IEEE计算机视觉与模式识别会议论文集*，页2921–2929，2016年。
- en: Quellec et al. [2017] Gwenolé Quellec, Katia Charrière, Yassine Boudi, Béatrice
    Cochener, and Mathieu Lamard. Deep image mining for diabetic retinopathy screening.
    *Medical Image Analysis*, 39:178–193, 2017.
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quellec 等人 [2017] Gwenolé Quellec, Katia Charrière, Yassine Boudi, Béatrice
    Cochener 和 Mathieu Lamard。用于糖尿病视网膜病筛查的深度图像挖掘。*Medical Image Analysis*，39:178–193，2017年。
- en: Khojasteh et al. [2018] Parham Khojasteh, Leandro Aparecido Passos Júnior, Tiago
    Carvalho, Edmar Rezende, Behzad Aliahmad, João Paulo Papa, and Dinesh Kant Kumar.
    Exudate detection in fundus images using deeply-learnable features. *Computers
    in biology and medicine*, 2018.
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khojasteh 等人 [2018] Parham Khojasteh、Leandro Aparecido Passos Júnior、Tiago Carvalho、Edmar
    Rezende、Behzad Aliahmad、João Paulo Papa 和 Dinesh Kant Kumar。使用深度可学习特征进行眼底图像中的渗出物检测。*计算机生物医学*，2018年。
- en: Haloi [2015] Mrinal Haloi. Improved microaneurysm detection using deep neural
    networks. *arXiv preprint arXiv:1505.04424*, 2015.
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haloi [2015] Mrinal Haloi。使用深度神经网络改进微动脉瘤检测。*arXiv预印本 arXiv:1505.04424*，2015年。
- en: 'van Grinsven et al. [2016] Mark JJP van Grinsven, Bram van Ginneken, Carel B
    Hoyng, Thomas Theelen, and Clara I Sánchez. Fast convolutional neural network
    training using selective data sampling: Application to hemorrhage detection in
    color fundus images. *IEEE transactions on medical imaging*, 35(5):1273–1284,
    2016.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Grinsven 等人 [2016] Mark JJP van Grinsven、Bram van Ginneken、Carel B Hoyng、Thomas
    Theelen 和 Clara I Sánchez。利用选择性数据采样进行快速卷积神经网络训练：应用于彩色眼底图像中的出血检测。*IEEE医学成像学报*，35(5):1273–1284，2016年。
- en: Orlando et al. [2018] José Ignacio Orlando, Elena Prokofyeva, Mariana del Fresno,
    and Matthew B Blaschko. An ensemble deep learning based approach for red lesion
    detection in fundus images. *Computer methods and programs in biomedicine*, 153:115–127,
    2018.
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Orlando 等人 [2018] José Ignacio Orlando、Elena Prokofyeva、Mariana del Fresno 和
    Matthew B Blaschko。一种基于集成深度学习的眼底图像红色病变检测方法。*计算机方法与生物医学程序*，153:115–127，2018年。
- en: 'Shan and Li [2016] Juan Shan and Lin Li. A deep learning method for microaneurysm
    detection in fundus images. In *Connected Health: Applications, Systems and Engineering
    Technologies (CHASE), 2016 IEEE First International Conference on*, pages 357–358\.
    IEEE, 2016.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shan 和 Li [2016] Juan Shan 和 Lin Li。用于眼底图像中微动脉瘤检测的深度学习方法。见于 *2016年IEEE首届国际健康互联会议
    (CHASE)*，页357–358。IEEE，2016年。
- en: Gulshan et al. [2016] Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe,
    Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom
    Madams, Jorge Cuadros, et al. Development and validation of a deep learning algorithm
    for detection of diabetic retinopathy in retinal fundus photographs. *Jama*, 316(22):2402–2410,
    2016.
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulshan 等人 [2016] Varun Gulshan、Lily Peng、Marc Coram、Martin C Stumpe、Derek Wu、Arunachalam
    Narayanaswamy、Subhashini Venugopalan、Kasumi Widner、Tom Madams、Jorge Cuadros 等。开发和验证用于检测视网膜眼底照片中糖尿病视网膜病变的深度学习算法。*Jama*，316(22):2402–2410，2016年。
- en: Colas et al. [2016] E Colas, A Besse, A Orgogozo, B Schmauch, N Meric, and E Besse.
    Deep learning approach for diabetic retinopathy screening. *Acta Ophthalmologica*,
    94(S256), 2016.
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colas 等人 [2016] E Colas、A Besse、A Orgogozo、B Schmauch、N Meric 和 E Besse。糖尿病视网膜病变筛查的深度学习方法。*眼科学报*，94(S256)，2016年。
- en: Costa and Campilho [2017] Pedro Costa and Aurélio Campilho. Convolutional bag
    of words for diabetic retinopathy detection from eye fundus images. *IPSJ Transactions
    on Computer Vision and Applications*, 9(1):10, 2017.
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costa 和 Campilho [2017] Pedro Costa 和 Aurélio Campilho。用于从眼底图像中检测糖尿病视网膜病变的卷积词袋。*IPSJ计算机视觉与应用事务*，9(1):10，2017年。
- en: Pires et al. [2014] Ramon Pires, Herbert F Jelinek, Jacques Wainer, Eduardo
    Valle, and Anderson Rocha. Advancing bag-of-visual-words representations for lesion
    classification in retinal images. *PloS one*, 9(6):e96814, 2014.
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pires 等人 [2014] Ramon Pires、Herbert F Jelinek、Jacques Wainer、Eduardo Valle 和
    Anderson Rocha。推进视觉词袋表示法用于视网膜图像中的病变分类。*PloS one*，9(6):e96814，2014年。
- en: Pratt et al. [2016] Harry Pratt, Frans Coenen, Deborah M Broadbent, Simon P
    Harding, and Yalin Zheng. Convolutional neural networks for diabetic retinopathy.
    *Procedia Computer Science*, 90:200–205, 2016.
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pratt 等人 [2016] Harry Pratt、Frans Coenen、Deborah M Broadbent、Simon P Harding
    和 Yalin Zheng。用于糖尿病视网膜病变的卷积神经网络。*计算机科学程序集*，90:200–205，2016年。
- en: Gargeya and Leng [2017] Rishab Gargeya and Theodore Leng. Automated identification
    of diabetic retinopathy using deep learning. *Ophthalmology*, 124(7):962–969,
    2017.
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gargeya 和 Leng [2017] Rishab Gargeya 和 Theodore Leng。使用深度学习自动识别糖尿病视网膜病变。*眼科学*，124(7):962–969，2017年。
- en: 'Wang et al. [2017] Zhe Wang, Yanxin Yin, Jianping Shi, Wei Fang, Hongsheng
    Li, and Xiaogang Wang. Zoom-in-net: Deep mining lesions for diabetic retinopathy
    detection. In *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, pages 267–275\. Springer, 2017.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2017] Zhe Wang、Yanxin Yin、Jianping Shi、Wei Fang、Hongsheng Li 和 Xiaogang
    Wang。Zoom-in-net：深度挖掘病变以进行糖尿病视网膜病变检测。见于 *国际医学图像计算与计算机辅助干预会议*，页267–275。Springer，2017年。
- en: Mansour [2018] Romany F Mansour. Deep-learning-based automatic computer-aided
    diagnosis system for diabetic retinopathy. *Biomedical Engineering Letters*, 8(1):41–57,
    2018.
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mansour [2018] Romany F Mansour。基于深度学习的自动计算机辅助诊断系统用于糖尿病视网膜病变。*生物医学工程信函*，8(1)：41–57，2018年。
- en: Chen et al. [2018] Yi-Wei Chen, Tung-Yu Wu, Wing-Hung Wong, and Chen-Yi Lee.
    Diabetic retinopathy detection based on deep convolutional neural networks. In
    *2018 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, pages 1030–1034\. IEEE, 2018.
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2018] 陈奕伟、吴宗宇、黄荣宏和李振仪。基于深度卷积神经网络的糖尿病视网膜病变检测。在*2018 IEEE国际声学、语音与信号处理会议 (ICASSP)*，第1030–1034页。IEEE，2018年。
- en: Srinidhi et al. [2017] Chetan L Srinidhi, P Aparna, and Jeny Rajan. Recent advancements
    in retinal vessel segmentation. *Journal of medical systems*, 41(4):70, 2017.
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srinidhi 等人 [2017] Chetan L Srinidhi、P Aparna 和 Jeny Rajan。视网膜血管分割的最新进展。*医学系统杂志*，41(4)：70，2017年。
- en: Villalobos-Castaldi et al. [2010] Fabiola M Villalobos-Castaldi, Edgardo M Felipe-Riverón,
    and Luis P Sánchez-Fernández. A fast, efficient and automated method to extract
    vessels from fundus images. *Journal of Visualization*, 13(3):263–270, 2010.
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Villalobos-Castaldi 等人 [2010] Fabiola M Villalobos-Castaldi、Edgardo M Felipe-Riverón
    和 Luis P Sánchez-Fernández。提取眼底图像中的血管的快速、高效和自动化方法。*可视化杂志*，13(3)：263–270，2010年。
- en: Condurache and Mertins [2012] Alexandru Paul Condurache and Alfred Mertins.
    Segmentation of retinal vessels with a hysteresis binary-classification paradigm.
    *Computerized Medical Imaging and Graphics*, 36(4):325–335, 2012.
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Condurache 和 Mertins [2012] Alexandru Paul Condurache 和 Alfred Mertins。使用滞后二分类范式的视网膜血管分割。*计算机医学成像与图形学*，36(4)：325–335，2012年。
- en: Aquino et al. [2010] Arturo Aquino, Manuel Emilio Gegúndez-Arias, and Diego
    Marín. Detecting the optic disc boundary in digital fundus images using morphological,
    edge detection, and feature extraction techniques. *IEEE transactions on medical
    imaging*, 29(11):1860–1869, 2010.
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aquino 等人 [2010] Arturo Aquino、Manuel Emilio Gegúndez-Arias 和 Diego Marín。使用形态学、边缘检测和特征提取技术检测数字眼底图像中的视盘边界。*IEEE医学影像学报*，29(11)：1860–1869，2010年。
- en: Zhang et al. [2012] Dongbo Zhang, Yao Yi, Xingyu Shang, and Yinghui Peng. Optic
    disc localization by projection with vessel distribution and appearance characteristics.
    In *Pattern Recognition (ICPR), 2012 21st International Conference on*, pages
    3176–3179\. IEEE, 2012.
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2012] Dongbo Zhang、Yao Yi、Xingyu Shang 和 Yinghui Peng。通过投影结合血管分布和外观特征进行视盘定位。在*模式识别
    (ICPR)，2012年第21届国际会议*，第3176–3179页。IEEE，2012年。
- en: Sinha and Babu [2012] Neelam Sinha and R Venkatesh Babu. Optic disk localization
    using l 1 minimization. In *Image Processing (ICIP), 2012 19th IEEE International
    Conference on*, pages 2829–2832\. IEEE, 2012.
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha 和 Babu [2012] Neelam Sinha 和 R Venkatesh Babu。使用l 1最小化的视盘定位。在*图像处理 (ICIP)，2012年第19届IEEE国际会议*，第2829–2832页。IEEE，2012年。
- en: Tjandrasa et al. [2012] Handayani Tjandrasa, Ari Wijayanti, and Nanik Suciati.
    Optic nerve head segmentation using hough transform and active contours. *Indonesian
    Journal of Electrical Engineering and Computer Science*, 10(3):531–536, 2012.
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tjandrasa 等人 [2012] Handayani Tjandrasa、Ari Wijayanti 和 Nanik Suciati。使用霍夫变换和活动轮廓的视神经头分割。*印度尼西亚电气工程与计算机科学杂志*，10(3)：531–536，2012年。
- en: Massey et al. [2009] EM Massey, Andrew Hunter, James Lowell, and DH Steel. A
    robust lesion boundary segmentation algorithm using level set methods. In *World
    Congress on Medical Physics and Biomedical Engineering, September 7-12, 2009,
    Munich, Germany*, pages 304–307. Springer, 2009.
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Massey 等人 [2009] EM Massey、Andrew Hunter、James Lowell 和 DH Steel。使用水平集方法的稳健病变边界分割算法。在*2009年9月7-12日医学物理与生物医学工程世界大会，德国慕尼黑*，第304–307页。Springer，2009年。
- en: Antal and Hajdu [2012] Balint Antal and Andras Hajdu. An ensemble-based system
    for microaneurysm detection and diabetic retinopathy grading. *IEEE transactions
    on biomedical engineering*, 59(6):1720–1726, 2012.
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antal 和 Hajdu [2012] Balint Antal 和 Andras Hajdu。基于集成的微动脉瘤检测和糖尿病视网膜病变分级系统。*IEEE生物医学工程学报*，59(6)：1720–1726，2012年。
- en: Neff et al. [2017] Thomas Neff, Christian Payer, Darko Štern, and Martin Urschler.
    Generative adversarial network based synthesis for supervised medical image segmentation.
    In *OAGM & ARW Joint Workshop 2017 on “Vision, Automation & Robotics”*. Verlag
    der Technischen Universität Graz, 2017.
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neff 等人 [2017] Thomas Neff、Christian Payer、Darko Štern 和 Martin Urschler。基于生成对抗网络的监督医学图像分割合成。在*OAGM
    & ARW 2017年“视觉、自动化与机器人”联合研讨会*。格拉茨技术大学出版社，2017年。
- en: Worrall et al. [2016] Daniel E Worrall, Clare M Wilson, and Gabriel J Brostow.
    Automated retinopathy of prematurity case detection with convolutional neural
    networks. In *International Workshop on Large-Scale Annotation of Biomedical Data
    and Expert Label Synthesis*, pages 68–76\. Springer, 2016.
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Worrall等人 [2016] Daniel E Worrall, Clare M Wilson, 和 Gabriel J Brostow。使用卷积神经网络进行早产儿视网膜病变案例检测。在*大规模生物医学数据注释与专家标签合成国际研讨会*，页码68–76。Springer，2016年。
- en: Chen et al. [2015] Xiangyu Chen, Yanwu Xu, Damon Wing Kee Wong, Tien Yin Wong,
    and Jiang Liu. Glaucoma detection based on deep convolutional neural network.
    In *Engineering in Medicine and Biology Society (EMBC), 2015 37th Annual International
    Conference of the IEEE*, pages 715–718\. IEEE, 2015.
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人 [2015] Xiangyu Chen, Yanwu Xu, Damon Wing Kee Wong, Tien Yin Wong, 和
    Jiang Liu。基于深度卷积神经网络的青光眼检测。在*医学与生物工程学会（EMBC），2015年第37届IEEE国际会议*，页码715–718。IEEE，2015年。
- en: Frid-Adar et al. [2018] Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai,
    Jacob Goldberger, and Hayit Greenspan. Gan-based synthetic medical image augmentation
    for increased cnn performance in liver lesion classification. *arXiv preprint
    arXiv:1803.01229*, 2018.
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frid-Adar等人 [2018] Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai,
    Jacob Goldberger, 和 Hayit Greenspan。基于GAN的合成医学图像增强以提高CNN在肝脏病变分类中的表现。*arXiv预印本
    arXiv:1803.01229*，2018年。
- en: 'Schawinski et al. [2017] Kevin Schawinski, Ce Zhang, Hantian Zhang, Lucas Fowler,
    and Gokula Krishnan Santhanam. Generative adversarial networks recover features
    in astrophysical images of galaxies beyond the deconvolution limit. *Monthly Notices
    of the Royal Astronomical Society: Letters*, 467(1):L110–L114, 2017.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schawinski等人 [2017] Kevin Schawinski, Ce Zhang, Hantian Zhang, Lucas Fowler,
    和 Gokula Krishnan Santhanam。生成对抗网络在天文图像中的特征恢复超越了解卷积限制。*皇家天文学会月刊：通讯*，467(1):L110–L114，2017年。
- en: 'Lawrence [2004] Mary Gilbert Lawrence. The accuracy of digital-video retinal
    imaging to screen for diabetic retinopathy: an analysis of two digital-video retinal
    imaging systems using standard stereoscopic seven-field photography and dilated
    clinical examination as reference standards. *Transactions of the American Ophthalmological
    Society*, 102:321, 2004.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lawrence [2004] Mary Gilbert Lawrence。数字视频视网膜成像筛查糖尿病视网膜病变的准确性：使用标准立体七字段摄影和散瞳临床检查作为参考标准的两个数字视频视网膜成像系统的分析。*美国眼科学会会刊*，102:321，2004年。
- en: Massin et al. [2003] P Massin, A Erginay, A Ben Mehidi, E Vicaut, G Quentel,
    Z Victor, M Marre, PJ Guillausseau, and A Gaudric. Evaluation of a new non-mydriatic
    digital camera for detection of diabetic retinopathy. *Diabetic medicine*, 20(8):635–641,
    2003.
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Massin等人 [2003] P Massin, A Erginay, A Ben Mehidi, E Vicaut, G Quentel, Z Victor,
    M Marre, PJ Guillausseau, 和 A Gaudric。评估一种新型非散瞳数字相机用于检测糖尿病视网膜病变。*糖尿病医学*，20(8):635–641，2003年。
- en: 'Szabó et al. [2015] Dorottya Szabó, Orsolya Fiedler, Anikó Somogyi, Gábor Márk
    Somfai, Zsolt Bíró, Veronika Ölvedy, Zsófia Hargitai, and János Németh. Telemedical
    diabetic retinopathy screening in hungary: a pilot programme. *Journal of telemedicine
    and telecare*, 21(3):167–173, 2015.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szabó等人 [2015] Dorottya Szabó, Orsolya Fiedler, Anikó Somogyi, Gábor Márk Somfai,
    Zsolt Bíró, Veronika Ölvedy, Zsófia Hargitai, 和 János Németh。匈牙利的远程糖尿病视网膜病变筛查：一个试点项目。*远程医疗与远程护理杂志*，21(3):167–173，2015年。
- en: Abdellaoui et al. [2016] M Abdellaoui, M Marrakchi, IA Benatiya, and H Tahri.
    Screening for diabetic retinopathy by non-mydriatic retinal camera in the region
    of fez. *Journal francais d’ophtalmologie*, 39(1):48–54, 2016.
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdellaoui等人 [2016] M Abdellaoui, M Marrakchi, IA Benatiya, 和 H Tahri。在费斯地区通过非散瞳视网膜相机筛查糖尿病视网膜病变。*法国眼科学杂志*，39(1):48–54，2016年。
- en: Siu et al. [1998] SC Siu, TC Ko, KW Wong, WN Chan, et al. Effectiveness of non-mydriatic
    retinal photography and direct ophthalmoscopy in detecting diabetic retinopathy.
    *Hong Kong Medical Journal*, pages 367–370, 1998.
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siu等人 [1998] SC Siu, TC Ko, KW Wong, WN Chan等人。非散瞳视网膜摄影与直接眼底镜检查在检测糖尿病视网膜病变中的效果。*香港医学杂志*，页码367–370，1998年。
- en: Chow et al. [2006] Sing-Pey Chow, Lloyd M Aiello, Jerry D Cavallerano, Paula
    Katalinic, Kristen Hock, Ann Tolson, Rita Kirby, Sven-Erik Bursell, and Lloyd Paul
    Aiello. Comparison of nonmydriatic digital retinal imaging versus dilated ophthalmic
    examination for nondiabetic eye disease in persons with diabetes. *Ophthalmology*,
    113(5):833–840, 2006.
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chow等人 [2006] Sing-Pey Chow, Lloyd M Aiello, Jerry D Cavallerano, Paula Katalinic,
    Kristen Hock, Ann Tolson, Rita Kirby, Sven-Erik Bursell, 和 Lloyd Paul Aiello。非散瞳数字视网膜成像与散瞳眼科检查在糖尿病患者中非糖尿病眼病的比较。*眼科学*，113(5):833–840，2006年。
- en: Wong and Bressler [2016] Tien Yin Wong and Neil M Bressler. Artificial intelligence
    with deep learning technology looks into diabetic retinopathy screening. *Jama*,
    316(22):2366–2367, 2016.
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong 和 Bressler [2016] Tien Yin Wong 和 Neil M Bressler。利用深度学习技术的人工智能探讨糖尿病视网膜病筛查。*美国医学会杂志*，316(22):2366–2367，2016年。
- en: Oke et al. [2016] JL Oke, IM Stratton, SJ Aldington, RJ Stevens, and Peter H
    Scanlon. The use of statistical methodology to determine the accuracy of grading
    within a diabetic retinopathy screening programme. *Diabetic Medicine*, 33(7):896–903,
    2016.
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oke 等人 [2016] JL Oke, IM Stratton, SJ Aldington, RJ Stevens 和 Peter H Scanlon。使用统计方法来确定糖尿病视网膜病筛查项目中的评分准确性。*糖尿病医学*，33(7):896–903，2016年。
