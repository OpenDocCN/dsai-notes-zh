- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:07:08'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1811.01238] Deep Learning based Computer-Aided Diagnosis Systems for Diabetic
    Retinopathy: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1811.01238](https://ar5iv.labs.arxiv.org/html/1811.01238)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '¹¹institutetext: Community College, Imam Abdulrahman Bin Faisal University,
    Dammam, Saudi Arabia'
  prefs: []
  type: TYPE_NORMAL
- en: Computer and Information Science College, King Saud University, Riyadh, Saudi
    Arabia
  prefs: []
  type: TYPE_NORMAL
- en: Department of Ophthalmology, College of Medicine, Princess Nourah bint Abdulrahman
    University, Riyadh, Saudi Arabia
  prefs: []
  type: TYPE_NORMAL
- en: Vitreoretinal Diseases and Surgery, King Khalid Eye Specialist Hospital, Riyadh,
    Saudi Arabia
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹email: norah.m.asiri@outlook.com'
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹email: mhussian@ksu.edu.sa'
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹email: ffaladel@pnu.edu.sa'
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹email: nzaidi@kkesh.med.sa'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Norah Asiri 1122    Muhammad Hussain 22    Fadwa Al Adel 33    Nazih Alzaidi
    4411223344
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Diabetic retinopathy (DR) results in vision loss if not treated early. A computer-aided
    diagnosis (CAD) system based on retinal fundus images is an efficient and effective
    method for early DR diagnosis and assisting experts. A computer-aided diagnosis
    (CAD) system involves various stages like detection, segmentation and classification
    of lesions in fundus images. Many traditional machine-learning (ML) techniques
    based on hand-engineered features have been introduced. The recent emergence of
    deep learning (DL) and its decisive victory over traditional ML methods for various
    applications motivated the researchers to employ it for DR diagnosis, and many
    deep-learning-based methods have been introduced. In this paper, we review these
    methods, highlighting their pros and cons. In addition, we point out the challenges
    to be addressed in designing and learning about efficient, effective and robust
    deep-learning algorithms for various problems in DR diagnosis and draw attention
    to directions for future research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Diabetic Retinpoathy, Lesion, Exudate, Macula, Diabetic Macular Edema, Optic
    Disc, Microaneurysms, Hemorrhages, CNN, Autoencoder, RNN, DBN
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Diabetic retinopathy (DR) is one of the main causes of blindness among the working-age
    population. It is one of the most feared complications of diabetes. The fundamental
    problem of DR is that it becomes incurable at advanced stages, so early diagnosis
    is important. However, this involves remarkable difficulty in the health care
    system due to a large number of potential patients and the small number of experienced
    technicians. This has motivated the need to develop automated diagnosis systems
    to assist in early diagnosis of DR. Several attempts have been made in this direction,
    and several approaches based on hand-engineered features have been proposed, which
    have shown promising efficiency in recognizing DR regions in retinal fundus images.
  prefs: []
  type: TYPE_NORMAL
- en: Hand-engineered features are commonly used with traditional machine-learning
    (ML) methods for DR diagnosis. Different surveys have reviewed these traditional
    methods [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)]. For example, Mookiah et al. [[1](#bib.bib1)],
    Mansour [[4](#bib.bib4)] categorized DR diagnosis according to the adopted methodologies,
    such as mathematical morphology, retinal lesion tracking, thresholding and deformable
    models, clustering-based models, matched filtering models and hybrid approaches.
    Faust et al. [[2](#bib.bib2)] reviewed algorithms that extract lesion features
    from fundus images, such as the blood vessel area, exudes, hemorrhages, microaneurysms
    and texture. Joshi and Karule [[3](#bib.bib3)] reviewed the early research on
    exudate detection. Almotiri et al. [[5](#bib.bib5)] provided an overview of algorithms
    to segment retinal vessels. Almazroa et al. [[6](#bib.bib6)] and Thakur and Juneja
    [[7](#bib.bib7)] reviewed several methods for optic disc segmentation and diagnosis
    of glaucoma. However, expert knowledge is a prerequisite for hand-engineered features,
    and choosing the appropriate features requires intensive investigation of various
    options and tedious parameter settings. Moreover, techniques based on hand-engineered
    features do not generalize well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, the availability of huge datasets and the tremendous computing
    power offered by graphics processing units (GPUs) have motivated research on deep-learning
    algorithms, which have shown outstanding performance in various computer vision
    tasks and have gained a decisive victory over traditional hand-engineered-based
    methods. Many deep-learning (DL)-based algorithms have also been developed for
    various tasks to analyze retinal fundus images to develop automatic computer-aided
    diagnosis systems for DR. This paper reviews the latest DL algorithms used in
    DR detection, highlighting the contributions and challenges of recent research
    papers. First, we provide an overview of various DL approaches and then review
    the DL-based techniques for DR diagnosis. Finally, we summarize future directions,
    gaps and challenges in designing and training deep neural networks for DR diagnosis.
    The remainder of the paper is organized as follows: automatic detection of DR,
    types of lesions, DR stages, grading of DR, detection tasks and the detection
    framework are presented in Section [2](#S2 "2 Automatic Diabetic Retinopathy Detection
    ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey"). After that, public-domain DR datasets and common performance metrics
    are briefly described in Section [3](#S3 "3 Datasets and Performance Metrics ‣
    Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey"). An overview of DL techniques used in DR diagnosis is given in Section
    [4](#S4 "4 Overview of Deep Learning ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey"). The most recent research based on
    DL for DR diagnosis are reviewed in Section [5](#S5 "5 Literature Survey ‣ Deep
    Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey").
    This research is discussed in Section [6](#S6 "6 Discussion ‣ Deep Learning based
    Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey"). Finally,
    research gaps and future directions with conclusion are presented in Sections
    [7](#S7 "7 Gaps and Future Directions ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey") and [8](#S8 "8 Conclusion ‣ Deep
    Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Automatic Diabetic Retinopathy Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, for the sake of clarity, we give an overview of DR detection,
    types of DR lesions, stages of DR, grading of DR, DR-detection tasks and the general
    framework for detection. Automatic computer-aided solutions for DR characterization
    are still an open field of research [[4](#bib.bib4)]. Automatic image-based DR
    detection systems are intended to perform rapid retinal evaluations and early
    detection of DR to indicate whether DR complications are present.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Types of Lesions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The earliest clinical signs of DR and retinal damage are microaneurysms (MAs),
    which are a dilation of microvasculature formed due to disruption of the internal
    elastic lamina. Retinal microaneurysms reduce vision due to local loss of endothelial
    barrier function, causing leakage and retinal edema. MAs are small (usually less
    than 125 microns in diameter) and appear as red spots with sharp margins. When
    walls of weak capillaries are broken, bleeding causes hemorrhages (HMs), which
    are similar to MAs but larger [[8](#bib.bib8)] and have an irregular margin, they
    have different appearances according to which retinal layer they leak in. Splinter
    hemorrhages occur in the superficial surface layers of the retina and cause a
    superficial flame-shaped bleeding. Whereas dot and blot hemorrhages occur in the
    deeper layers of the retina. More leakage of damaged capillaries can cause exudates
    (EXs), which usually appear yellow and irregularly shaped in the retina. There
    are two types of EXs: hard and soft. Hard exudates (HEs) are lipoproteins and
    other proteins escaping from abnormal retinal vessels. They are white or white-yellow
    with sharp margins. They are often organized in blocks or circular rings [[9](#bib.bib9)]
    and are located in the outer layer of the retina. On the other hand, soft exudates
    (SEs) or cotton wool spots (CWS) are small, whitish-grey cloud-like shapes that
    occur when an arteriole is occluded [[10](#bib.bib10)]. EXs are different from
    MAs and HMs in terms of brightness. MAs and HMs are dark lesions, while EXs are
    bright [[11](#bib.bib11)]. Variations in the diameter of the retinal veins is
    called Venous beading (VB) [[12](#bib.bib12)] this usually happens in advanced
    stages of non-proliferative diabetic retinopathy. Due to the inability to use
    glucose by normal routes, alternate blood pathways are activated, which causes
    the synthesis of elements such as sorbitols and favors the development of alterations
    in the microvasculature. Intraretinal microvascular abnormalities (IRMA) is an
    example, it represents either a dilation of pre-existing capillaries or an actual
    growth of new blood vessels within the retina. When the retinal vessels stand
    out and grow towards the vitreous they are called neovascularization (NV) [[13](#bib.bib13)].
    Macular edema (ME) occurs when the retinal capillaries become permeable and leakage
    occurs around macula [[14](#bib.bib14)]. This can lead to retinal thickening or
    hard exudates developing either within one disk diameter of the center of the
    macula (the fovea) [[15](#bib.bib15)] or involving the fovea, which is responsible
    for the central vision.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An important object that plays an essential role in detecting DR is the optic
    disc (OD), which characterized by the highest contrast between the circular-shaped
    regions [[16](#bib.bib16)]. The optic disc is used as a landmark and frame of
    reference to diagnose serious eye pathologies such as glaucoma, optic disc pit,
    optic disc drusen and to check for any neovascularization at the disc [[17](#bib.bib17),
    [18](#bib.bib18)]. The OD is also used to pinpoint other structures such as the
    fovea. In normal retina, the edges of the OD are clear and well-defined, as shown
    in Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Types of Lesions ‣ 2 Automatic Diabetic Retinopathy
    Detection ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic
    Retinopathy: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5f6a9cb9a1d687d3142da0484f4d49d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Optic disc and abnormal findings in the eye fundus caused by the
    diabetic retinopathy.Left: MA, EX and HM. Right: new blood vessel routes(PDR).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Stages of DR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DR can be classified into two main classes based on its severity: non-proliferative
    (NPDR) and proliferative (PDR) [[8](#bib.bib8), [19](#bib.bib19)]. NPDR is an
    early stage, during which diabetes starts to damage small blood vessels within
    the retina; it is very common in people with diabetes [[14](#bib.bib14)]. These
    vessels start to discharge fluid and blood, causing the retina to swell. As time
    passes, the swelling or edema thickens the retina, causing blurry vision. The
    clinical feature of this stage is at least one microaneurysm or hemorrhage with
    or without hard exudates [[20](#bib.bib20)]. Proliferative DR is an advanced stage
    that leads to the growth of new blood vessels; as such, it is characterized by
    by abnormal vascular proliferation within the retina towards the vitreous cavity.
    These fragile new blood vessels can bleed into the vitreous cavity and cause severe
    visual loss due to vitreous hemorrhage. They can also further cause traction on
    the retina as they usually grow with a fibro vascular network around them that
    may lead to tractional retinal detachment.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Grading of DR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Examination and screening of the retina by ophthalmoscopy usually requires
    dilated pupils, a skilled examiner and a visit to an eye care provider such as
    optometrist to grade and classify pathology [[21](#bib.bib21)]. Grading is a vital
    activity in DR screening programme to diagnose retinal diseases. It is an intensive
    procedure that needs a trained workforce and an adequately sized computer screens[2](#S2.F2
    "Figure 2 ‣ 2.3 Grading of DR ‣ 2 Automatic Diabetic Retinopathy Detection ‣ Deep
    Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2522500112a6ca1af59decaecbfb40ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Graders need appropriate environment to maintain high-quality performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Graders such as optometrists or well-trained technicians perform an essential
    task to treat and recover potentially blinding eye conditions to treat and recover
    potentially blinding eye conditions such as age-related macular degeneration (AMD)
    and diabetic eye diseases [[21](#bib.bib21)]. Non-mydriatic fundus images are
    usually acquired but if the image is unclear due to any media opacity then mydriatic
    drops are used to dilate the pupil in an attempt to improve the quality of the
    image. All graders must receive a special training based on screening protocol
    to ensure the fundus images are graded in standardized manner. They should spend
    time on training to identify and confirm cases as having pathology abnormality
    or not and differentiate the levels of pathology seen and make referral decision
    or return for recall based on agreed interval. There are various systems to grade
    DR vascular changes such as American academy of ophthalmology (AAO), the classification
    which was introduced by the early treatment of diabetic retinopathy study (ETDRS)
    [[22](#bib.bib22)] and Scottish DR grading protocol where only one field is taken
    per eye, which is centered on the fovea [[23](#bib.bib23)]. Scottish protocol
    is represented in Table [1](#S2.T1 "Table 1 ‣ 2.3 Grading of DR ‣ 2 Automatic
    Diabetic Retinopathy Detection ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: DR Scottish grading protocol[[24](#bib.bib24)]'
  prefs: []
  type: TYPE_NORMAL
- en: '| Grade | Features | Decision |'
  prefs: []
  type: TYPE_TB
- en: '| R0: No DR | No abnormalities | Rescreen in 12 months |'
  prefs: []
  type: TYPE_TB
- en: '| R1: Mild NPDR | Only MAs | Rescreen in 12 months |'
  prefs: []
  type: TYPE_TB
- en: '| R2: Moderate NPDR |'
  prefs: []
  type: TYPE_TB
- en: '&#124; More than just MAs but less than severe NPDR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Rescreen in 6 months |'
  prefs: []
  type: TYPE_TB
- en: '| R3: Severe NPDR |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -More than 20 HMs in each quadrant &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Venous beading in two quadrants &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Intraretinal microvascular abnormalities &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Refer |'
  prefs: []
  type: TYPE_TB
- en: '| R4: PDR |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -Any new vessels at OD or elsewhere &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Vitreous/ pre-retinal HM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Refer |'
  prefs: []
  type: TYPE_TB
- en: '| M0: No ME | No EX or retinal thickening in posterior pole | 12 month rescreening
    |'
  prefs: []
  type: TYPE_TB
- en: '| M1: Mild ME | EXs or retinal thickening at posterior pole, $>$1 disc diameters
    from fovea | 6 month rescreening |'
  prefs: []
  type: TYPE_TB
- en: '| M2: Moderate ME |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Same signs of mild ME but with 1 disc diameters or less from fovea,
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; but not affecting fovea &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Refer for laser treatment |'
  prefs: []
  type: TYPE_TB
- en: '| M3: Severe ME | EXs or retinal thickening affecting center of fovea | Refer
    for laser treatment |'
  prefs: []
  type: TYPE_TB
- en: 2.4 Detection Tasks and General Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At a high level, DR detection is categorized into two tasks: lesion-level-based
    detection and image-level-based detection. In lesion-level-based detection, every
    lesion is detected and their locations are determined because the number of lesions
    and their locations are crucial to assessing the severity level of DR [[25](#bib.bib25)].
    On the other hand, image–based detection focuses on assessment based on image
    levels and is more interesting from the screening point of view because it evaluates
    only whether there are signs of DR [[25](#bib.bib25)]. Lesion-based detection
    usually involves two phases: (i) lesion detection and/or segmentation and (ii)
    lesion classification. First, lesions such as microaneurysms, hemorrhages, hard
    exudates and soft exudates are detected from fundus images, and the exact area
    of the lesion is localized. This is a challenging task because retinal fundus
    images contain other objects with similar appearances, such as red dots and blood
    vessels. For this task, the global and local context are usually needed to perform
    accurate localization and segmentation. The detection phase yields potential regions
    of interest, but they include false positives as well. The lesion-classification
    phase is used to remove false positives. Image-based detection is an image-screening
    task that classifies a given fundus image as being normal or having DR signs.
    This is one of the first areas of medical diagnosis to which DL has made a significant
    contributions [[26](#bib.bib26)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The general framework for detection, segmentation and classification involves
    the specific steps of preprocessing, feature extraction/selection, choice of a
    suitable classification method and finally assessment of the results. DR classification
    systems can be divided into two types according to learning procedure: supervised
    and unsupervised learning. In supervised learning, the system is taught using
    labeled data to infer functional mapping [[27](#bib.bib27), [28](#bib.bib28)].
    On the other hand, unsupervised learning methods tend to discover hidden patterns
    on their own from the properties of the unlabeled examples according to their
    similarity [[29](#bib.bib29)]. Unlike hand-engineered feature-based approaches,
    DL approaches integrate all of the steps into a unified framework and automatically
    learns the features and trains the system in an end-to-end manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Datasets and Performance Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we give an overview of the benchmark datasets and performance
    metrics that are commonly used for DR research.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Retinal Fundus Image Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Several datasets consisting of retinal fundus images have been produced to
    teach and test the algorithms for different DR detection tasks. In the following
    paragraphs, we give an overview of the following public domain benchmark datasets:
    MESSIDOR [[30](#bib.bib30)], e-ophtha [[31](#bib.bib31)], Kaggle [[32](#bib.bib32)],
    DRIVE [[33](#bib.bib33)], STARE [[34](#bib.bib34)], DIARETDB1 [[35](#bib.bib35),
    [36](#bib.bib36)], CHASE [[37](#bib.bib37)] , DRiDB [[38](#bib.bib38)], ORIGA
    [[39](#bib.bib39)], SCES [[40](#bib.bib40)] , AREDS [[41](#bib.bib41)], REVIEW
    [[42](#bib.bib42)], EyePACS-1 [[43](#bib.bib43)], RIM-ONE [[44](#bib.bib44)],
    DRISHTI-GS [[45](#bib.bib45)], ARIA [[46](#bib.bib46)], DRIONS-DB [[47](#bib.bib47)]
    and SEED-DB [[48](#bib.bib48)]. Table [2](#S3.T2 "Table 2 ‣ 3.1.18 SEED ‣ 3.1
    Retinal Fundus Image Datasets ‣ 3 Datasets and Performance Metrics ‣ Deep Learning
    based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey") summarizes
    these datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 MESSIDOR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It was developed under MESSIDOR research program funded by the French ministry
    of research and defense [[30](#bib.bib30)]. It was acquired by three ophthalmology
    departments using colored video 3CCD camera mounted on a Topcon TRC NW6 non-mydriatic
    retinograph with a 45^∘ field of view (FOV). Two types of image level annotation
    were provided by expert ophthalmologists: DR grades and risk levels of macular
    edema. The DR grades are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '0: No risk: (#MA = 0) AND (#HM = 0)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1: (0 $<$ #MA $\leq$ 5) AND (#HM = 0)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2: ((5 $<$ #MA $<$ 15) OR (0 $<$ #HM $<$ 5)) AND (NV = 0)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3: (#MA $\leq$ 15) OR (#HM $\leq$ 5) OR (NV = 1)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The risk levels of macular edema are as follows :'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '0: No risk'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1: Shortest distance between macula and hard EX $>$ one papilla diameter'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2: Shortest distance between macula and hard EX $\leq$ one papilla diameter'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.1.2 e-ophtha
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It was introduced by e-ophtha project funded by the French research agency
    [[31](#bib.bib31)]. It provides the locations of MAs and EXs, which were identified
    by two ophthalmologists. The first ophthalmologist outlined the locations, which
    were checked and examined by the second ophthalmologist. The database consists
    of two datasets: e-ophtha EX and e-ophtha MA. The e-ophtha EX set contains 47
    images with 12,278 EXs and 35 healthy images. Several images of healthy controls
    contain structures which can easily mislead EX detection methods, such as reflections
    and optical artifacts. On the other hand, e-ophtha MA contains 148 images with
    1306 MAs and 233 healthy images.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Kaggle
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It consists of a large set of high-resolution retinal images taken under different
    conditions and was provided by EyePACS clinics [[32](#bib.bib32)]. The image level
    annotation was provided by expert ophthalmologists, and each image has been assigned
    a DR grade on the scale of 0 to 4 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '0: No risk'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1: Mild'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2: Moderate'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3: Severe'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4: PDR'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.1.4 DRIVE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Digital retinal images for vessel extraction (DRIVE) [[33](#bib.bib33)] was
    collected under DR screening program in Netherlands for comparative studies of
    vascular segmentation in retinal images using Canon CR5 non-mydriatic 3CCD camera
    . It consists of 40 fundus images, which were randomly selected; among them 33
    do not show any sign of DR whereas 7 show signs of mild early DR; it is divided
    into a test and training sets, each containing 20 images. It provides pixel level
    annotation; a pixel is annotated as a vessel pixel with 70% confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 STARE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Structured analysis of the retina (STARE) [[34](#bib.bib34)] program was funded
    by the U.S. national institutes of health (NIH). It includes fundus images showing
    13 diseases associated with human eye. It provides the list of disease codes and
    names for each image. Blood vessels and optic nerve have the pixel level annotation
    but without grading. Two observers manually segmented all the images. On average,
    the first person labeled 32,200 pixels in each image as vessel, while the second
    person labeled 46,100 pixels in each image as vessel. This dataset offers a challenging
    OD detection problem due to the appearance of retinal diseases.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6 DIARETDB1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset contains 89 color fundus images, which were taken under varying
    imaging settings and FOV of 50^∘, and were captured in Kuopio university hospital
    in Finland [[35](#bib.bib35)]. Four independent experts annotated the images.
    These experts delineated the regions where MAs and HMs can be found, and provided
    a map for each type of lesion. This dataset is referred to as “calibration level
    1 fundus images”. It is divided into training and test sets containing 28 and
    61 images, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.7 CHASE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It was acquired under the program child heart and health study in England (CHASE)
    [[37](#bib.bib37)] from children of different ethnic origin and ages from 9 and
    10 years. It consists of 28 fundus images taken from 14 children and the annotation
    contains ground truths for blood vessels collected using Top Con TRV-50 camera
    with 35 FOV. Unlike DRIVE and STARE, it contains images with uneven and non-uniform
    background illumination, poor contrast of blood vessels and wider arteries that
    have a bright strip running down the center, known as the central vessel reflex.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.8 DRiDB
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Diabetic retinopathy image database (DRiDB) [[38](#bib.bib38)] was obtained
    at university hospital in Zagreb and was created to overcome the shortcomings
    in previous datasets such as grading and limited number of observers. Images were
    taken and selected by experts with 45 FOV and shown DR symptoms vary from almost
    normal to cases where new fragile vessels are visible. In this dataset, each image
    was evaluated by five independent experts to mark DR findings. These experts annotated
    pixels of findings and related areas of MAs, HMs, hard and soft EXs, blood vessels,
    ODs and macula.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.9 ORIGA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Online retinal fundus image database for glaucoma analysis and research (ORIGA)
    [[39](#bib.bib39)] is an online repository which shares fundus images and their
    ground truths as benchmarks for researchers to share retinal image analysis results
    and the corresponding diagnosis. It was collected over a period of 3 years from
    2004 to 2007 at Singapore eye research institute. It focuses on OD and optic cup
    (OC) segmentation and Cup-to-Disc Ratio (CDR) to diagnosis glaucoma.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.10 SCES
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It was acquired under Singapore Chinese eye study (SCES) [[40](#bib.bib40)]
    conducted on 1,060 Chinese participants and was graded by one senior professional
    grader and one retinal specialist. The study was conducted to identify the determinants
    of anterior chamber depth (ACD) and to ascertain the relative importance of these
    determinants in Chinese persons in Singapore.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.11 AREDS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It was developed under Age-related eye disease study (AREDS) [[41](#bib.bib41)],
    which was funded by NIH. It is long-term multicenter, prospective study of 595
    participants with ages from 55 to 80 years, which was designed to assess the clinical
    course of both AMD and cataract. Participants were of any illness or condition
    that would make long-term follow-up. On the basis of fundus photographs graded
    by a central reading center, best corrected visual acuity, and ophthalmologic
    evaluations, participants were enrolled in one of several AMD categories.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.12 REVIEW
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Retinal vessel image set for estimation of widths (REVIEW) [[42](#bib.bib42)]
    was made available online in 2008 by the department of computing and informatics
    at the university of Lincoln, UK. The dataset contains 16 mydriatic images with
    193 annotated vessel segments consisting of 5066 profile points manually marked
    by three independent experts. Unlike DRIVE and STARE, REVIEW dataset includes
    width measurements. The images were chosen to evaluate the accuracy and precision
    of the vessel width measurement algorithms in the presence of pathology and central
    light reflex. The 16 images are subdivided into four sets: the high resolution
    image set (8 images), the vascular disease image set (4 images), the central light
    reflex image set (2 images) and the kickpoint image set (2 images).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.13 EyePACS-1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Eye picture archive and communication system (EyePACS) [[43](#bib.bib43)] is
    a flexible protocol and web based telemedicine system for DR screening and collaboration
    among clinicians. Patients’ fundus images can be easily uploaded to EyePACS web.
    The protocol evaluates the presence and severity of discrete retinal lesions associated
    with DR. The protocol uses the Canon CR-DGi and Canon CR-1 nonmydriatic cameras
    can be accessed on the EyePACS Web site. The lesions are graded as MAs, HMs with
    or without MAs, cotton wool spots, intraretinal microvascular abnormalities, venous
    beading, new vessels (new vessels on the disk and new vessels elsewhere), fibrous
    proliferation, vitreous HMs or preretinal HMs and HEs. In addition, the presence
    or absence of laser scars. Graders grade each lesion type separately in each image
    using an online grading template that records a choice for each lesion type among
    no (absent), yes (present) or cannot grade.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.14 RIM-ONE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is an open retinal image database for optic nerve evaluation (RIM-ONE) [[44](#bib.bib44)]
    captured by non-mydriatic Nidek AFC-210 with a body of a Canon EOS 5D Mark II.
    It was designed for glaucoma diagnosis and consists of 169 optic nerve head regions,
    which were cropped manually from full fundus images. These images were annotated
    by 5 glaucoma experts: 4 ophthalmologists and 1 optometrist.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.15 DRISHTI-GS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It consists of a total of 101 fundus images of healthy controls and glaucoma
    patients with almost 25 FOV that were collected at Aravind eye hospital in India
    [[45](#bib.bib45)]. It is divided into training and test sets consisting of 50
    and 51 images, respectively. All images were annotated by 4 ophthalmologists with
    clinical experiences of 3, 5, 9 and 20 years, respectively. The manual segmentation
    of OD and OC boundaries, and CDR are provided as ground truths. Also two other
    expert opinions were included about whether an image represents healthy control
    or glaucomatous eye and presence or absence of notching in the inferior and/or
    superior sectors of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.16 ARIA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Automated retinal image analyzer (ARIA) [[46](#bib.bib46)] was collected and
    designed to trace blood vessels, ODs and fovea locations. It was marked by two
    image analysis experts. This dataset was collected at St Paul’s eye unit and the
    university of Liverpool to diagnosis AMD and DR using a Zeiss FF450+ fundus camera
    at a 50 FOV.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.17 DRIONS-DB
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Digital retinal images for optic nerve segmentation database (DRIONS-DB) [[47](#bib.bib47)]
    was collected at a university hospital in Spain. It was designed to segment optic
    nerve head and its related pathologies. It was annotated by 2 independent medical
    experts. Images were centered on the optic nerve head and are stored in slide
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.18 SEED
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Singapore epidemiology of eye diseases (SEED) [[48](#bib.bib48)] was composed
    of 235 fundus images with a focus on studying major eye diseases, including DR,
    AMD, glaucoma, refractive errors and cataract. Each image has OD and OC regions
    marked by a trained grader, which serves as a ground truth for segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Datasets for DR Detection'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | #Images | Resolution | Format | Tasks |'
  prefs: []
  type: TYPE_TB
- en: '| Images level annotation |'
  prefs: []
  type: TYPE_TB
- en: '| MESSIDOR [[49](#bib.bib49)] | 1,200 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 1,440$\times$960, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2,240$\times$1,488, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2,304$\times$1,536 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Images: TIFF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Diagnosis: excel file &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -DR grading &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Risk of DME &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Kaggle &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 80,000 | - | JPEG |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -No DR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Mild &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Moderate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Severe &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -PDR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| AREDS[[41](#bib.bib41)] | 72,000 | - | - | -AMD stages |'
  prefs: []
  type: TYPE_TB
- en: '| EyePACS-1[[43](#bib.bib43)] | 9,963 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -Referable DR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -MA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pixel level annotation |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; e-ophtha[[31](#bib.bib31)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 148 MAs, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 233 normal non-MA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 EXs, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 35 normal non-EX &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2,544 $\times$ 1,696 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1440$\times$960 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '- |'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Images: JPEG &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GT: PNG &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -MA small HM detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -EX detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DRIVE [[33](#bib.bib33)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 33 normal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 7 mild to early &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DR stage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 584$\times$565 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Images: TIFF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GT, masks: GIF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| -Vessels extraction |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; STARE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[34](#bib.bib34)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 402 | 605$\times$700 | PPM |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -13 retinal diseases &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Vessels extraction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Optic nerve &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DIARETDB1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[35](#bib.bib35), [36](#bib.bib36)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 5 normal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 84 with at least one &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NPDR sign &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1,500$\times$1,152 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Images, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; masks, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GT: PNG &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -MAs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -HMs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -SEs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -HEs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CHASE [[37](#bib.bib37)] | 28 | 1,280 $\times$ 960 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Images: JPEG &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GT: PNG &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| -Vessels extraction |'
  prefs: []
  type: TYPE_TB
- en: '| DRiDB [[38](#bib.bib38)] | 50 | 720$\times$576 | BMP |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -MAs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -HMs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -HEs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -SEs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Vessels extraction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -OD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Macula &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ORIGA[[39](#bib.bib39)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 482 normal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 168 glaucomatous &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 720$\times$576 | - |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -OD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Optic cup &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Cup-to-Disc Ratio (CDR) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SCES[[40](#bib.bib40)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 1,630 normal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 46 glaucomatous &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| - | - | -CDR |'
  prefs: []
  type: TYPE_TB
- en: '| REVIEW[[42](#bib.bib42)] | 16 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 3,584$\times$2,438 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1,360$\times$1,024 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2,160$\times$1,440 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3,300$\times$2,600 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| - | -Vessels extraction |'
  prefs: []
  type: TYPE_TB
- en: '| RIM-ONE[[44](#bib.bib44)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 118 normal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 12 early glaucoma &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 14 moderate glaucoma &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 14 deep glaucoma &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 11 ocular hypertension &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| - | - | -Optic nerve |'
  prefs: []
  type: TYPE_TB
- en: '| DRISHTI-GS[[45](#bib.bib45)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 31 normal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 70 glaucomatous &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2,896 $\times$ 1,944 | PNG |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -OD segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -OC segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ARIA [[46](#bib.bib46)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 16 normal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 92 AMD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 59 DR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 768$\times$576 | TIFF |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -OD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Fovea location &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Vessel extraction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DRIONS-DB[[47](#bib.bib47)] | 110 | 600$\times$400 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Images: JPEG &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GT: txt file &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| -OD |'
  prefs: []
  type: TYPE_TB
- en: '| SEED-DB[[48](#bib.bib48)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 192 normal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 43 glaucomatous &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3,504$\times$ 2,336 | - |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -OD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -OC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Performance Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we define the performance metrics that are commonly used to
    assess DR detection algorithms. Common metrics for measuring the performance of
    classification algorithms include accuracy, sensitivity (recall), specificity,
    precision, F-score, ROC curve, logloss, IOU, overlapping error, boundary-based
    evaluation and the dice similarity coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy is defined as the ratio of the correctly classified instances over
    the total number of instances [[50](#bib.bib50)]. It is formally defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN},$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $TP$ (true positive) is the number of positive instances (e.g., having
    DR) in the considered dataset that are correctly classified, $TN$ (true negative)
    is the number of negative instances (e.g., normal cases) in the considered dataset
    that are correctly classified, and $FP$ (false positive) and $FN$ (false negative)
    are the numbers of positive and negative instances that are incorrectly classified,
    respectively. Note that in detecting DR, an instance is either a fundus image,
    a patch or a pixel of a fundus image, depending on the task. Sensitivity(SN),
    or the true positive rate or recall, measures the fraction of correctly classified
    positive instances; specificity(SP), or the true negative rate, measures the fraction
    of correctly classified negative instances; and precision, or positive predictive
    value, measures the fraction of positive instances that are correctly classified.
    They are formally defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Sensitivity(Recall)=\frac{TP}{TP+FN}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $Specificity=\frac{TN}{TN+FP}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '$\textit{F-score}(F)$ combines precision and recall as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F=2\frac{Precision\times Recall}{Precision+Recall}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'The receiver operating characteristic (ROC) curve represents the plot of the
    true positive rate against the false positive rate. It shows the relationship
    between sensitivity and specificity. The area under the ROC curve (AUC) is also
    used as a performance metric and takes values between 0 and 1; the closer the
    AUC is to 1, the better the performance. Logarithmic loss (log loss) determines
    a classifier’s accuracy by penalizing false classifications. To find log loss,
    the classifier must assign a probability to each class, instead of presenting
    the most likely class. It is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $logloss=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M}y_{ij}logp_{ij},$ |  |
    (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where N is the number of samples, $M$ is the number of labels, $y_{ij}$ is
    a binary indicator of whether label $j$ is the correct classification for instance
    $i$, and $p_{ij}$ is the model’s probability of assigning label $j$ to instance
    $i$. As segmentation is also a kind of classification at the pixel level, the
    metrics defined for classification can be used for segmentation. Additional metrics
    used for measuring the performance of segmentation algorithms include overlapping
    error, intersection over union and the dice similarity coefficient. Intersection
    over union (IOU) is defined as follows [[51](#bib.bib51)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $IOU=\frac{Area(A\cap G)}{Area(A\cup G)}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'Overlapping error is obtained by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $E=1-IOU$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $A$ is the notation for segmentation of the output and $G$ indicates the
    manual ground truth segmentation [[52](#bib.bib52)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Boundary-based evaluation (B) is the absolute pointwise localization error
    obtained by measuring the distance between two closed boundary curves. Let $C_{g}$
    be the boundary of ground truth and $C_{a}$ be the boundary obtained from a method.
    The distance $D$ between two curves is defined as (in pixels):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $B=\frac{1}{n}\sum_{\theta=1}^{\theta_{n}}\sqrt{(d_{g}^{\theta})^{2}-(d_{a}^{\theta})^{2}},$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $d_{g}^{\theta}$ and $d_{a}^{\theta}$ are the distance from the centroid
    of the curve to points on $C_{g}$ and $C_{a}$ in the direction of $\theta$ and
    $n$ is the total number of angular samples. The distance between the calculated
    boundary and ground truth should ideally be close to zero [[18](#bib.bib18)].
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative to overlapping error that is used for DR detection is the dice
    similarity coefficient (DSC) or overlap index, which is defined by [[53](#bib.bib53)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $DSC=\frac{2TP}{2TP+FP+FN}$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: The DSC takes values between 0 and 1; the closer the DSC is to 1, the better
    the segmentation results are. Region precision recall (RPR) is commonly used to
    assess edge or boundary detection outcomes based on region overlapping. It refers
    to the segmentation quality in a precision recall space [[54](#bib.bib54)].
  prefs: []
  type: TYPE_NORMAL
- en: 4 Overview of Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Various DL-based architectures have been introduced. Some commonly employed
    deep architectures for various DR-detection include convolutional neural networks
    (CNNs), autoencoders (AEs), recurrent neural networks (RNNs) and deep belief networks
    (DBNs). In the following paragraphs, we give an overview of these architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Convolutional Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'CNNs simulate the human visual system and have been widely employed for various
    computer vision tasks. They mainly consist of three types of layers: convolutional,
    pooling and fully connected (FC). Convolutional layers employ a convolution operation
    to encode local spatial information and then FC layers to encode the global information.
    Commonly used CNN models include AlexNet, VGGNet, GoogLeNet and ResNet. A CNN
    model is taught in an end-to-end manner; it learns the hierarchy of features automatically
    and results in outstanding classification performance. Initial CNN models such
    as LeNet [[55](#bib.bib55)] and AlexNet [[56](#bib.bib56)] contain few layers.
    In 2014, Simonyan and Zisserman [[57](#bib.bib57)] explored a deeper CNN model
    called VGGNet, which contains 19 layers, and found that depth is crucial for better
    performance. Motivated by these findings, deeper models such as GoogLeNet, Inception
    [[58](#bib.bib58)] and ResNet [[59](#bib.bib59)] have been proposed, which have
    shown amazing performance in many computer vision tasks. An end-to-end model usually
    means a deep model that takes inputs and gives outputs. Transfer learning means
    that a model is first taught in an end-to-end fashion using a dataset from a related
    domain and then fine-tuned using the dataset from the domain. Learning a CNN model
    requires a very large amount of data to overcome overfitting problems and ensure
    proper convergence [[60](#bib.bib60)], but large amounts of data are not available
    in the medical domain, particularly for DR detection. The solution is to use transfer
    learning [[61](#bib.bib61)]. Generally, two strategies of transfer learning are
    used: (i) using a pre-trained CNN model as a feature extractor and (ii) fine-tuning
    a pre-trained CNN model using data from the relevant domain. A fully convolutional
    network (FCN) is a version of a CNN model in which FC layers are converted into
    convolutional layers and deconvolution (or transposed convolution) layers are
    added to undo the effect of down-sampling during the convolutional layers and
    to obtain an output map of the same size as the input image [[62](#bib.bib62)].
    This model is commonly used for segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Autoencoder-based and Stacked Autoencoder Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An autoencoder (AE) is a single hidden layer neural network with the same input
    and output [[63](#bib.bib63)] and is used to build a stacked-autoencoder (SAE),
    a deep architecture [[64](#bib.bib64)]. The training of an SAE model consists
    of two phases: pre-training and fine-tuning. In the pre-training phase, an SAE
    is trained layer by layer in an unsupervised way. In the fine-tuning phase, the
    pre-trained SAE model is fine-tuned using gradient descent and backpropagation
    algorithms in a supervised way. An autoencoder is the basic building block of
    an SAE. There two main types of autoencoders: sparse and denoising. Sparse autoencoders
    are a type of autoencoder that tends to force the extracting of sparse features
    from raw data. The sparsity of the representation can either be achieved by penalizing
    hidden unit biases or by directly penalizing the output of hidden unit activations.
    Denoising autoencoders (DAEs) have also been used in DR detection Maji et al.
    [[65](#bib.bib65)] due its robustness in recovering the corrupted input and force
    the model to capture the correct version.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Recurrent Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNs are a type of neural network that learns the context as well along with
    input patterns. It learns the output of the previous iterations and combines it
    with the current input to yield an output; in this way, an RNN is able to influence
    itself through recurrences. An RNN model usually contains three sets of parameters—input
    to hidden weights $W$, hidden weights $U$ and hidden weights—to output$V$ where
    weights are shared across position/time of input sequence [[66](#bib.bib66)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Deep Belief Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A DBN [[67](#bib.bib67)] is a deep network architecture that is built with cascading
    restricted Boltzmann machines (RBMs). An RBM is taught using a contrastive divergence
    algorithm in such a way that maximizes the similarity (in the sense of probability)
    between the input and its projection. The involvement of probability as a similarity
    measure prevents degenerate solutions and makes DBNs a probabilistic model. Just
    like SAEs, DBNs are first pre-trained in an unsupervised way using a layer-by-layer
    greedy learning strategy; then, it is fine-tuned using gradient descent and backpropagation
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Literature Survey
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the clinical importance of DR detection tasks, we categorize them
    into four categories: (i) retinal blood vessel segmentation, (ii) optic disk localization
    and segmentation, (iii) lesion detection and classification, and (iv) image-level
    DR diagnosis for referral. In the following sub-sections, we review the state-of-the-art
    DL-based algorithms for these tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Retinal Blood Vessel Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is very important to identify changes in fine retinal blood vessels for preventing
    vision impairment due to pathological retinal damage. The segmentation of retinal
    blood vessels is challenging due to their low contrast, variations in their morphology
    against a noisy background and the presence of pathologies like MAs and HMs. Different
    learning approaches have been applied to segment retinal blood vessels. In the
    following paragraphs, we review these methods based on DL approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Convolutional Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many retinal blood vessel segmentation algorithms based on CNN models have been
    proposed. Maji et al. [[68](#bib.bib68)] employed 12 CNN models to segment vessel
    and non-vessel pixels. Each CNN model consists of three convolutional layers and
    two fully connected layers. For evaluation, they used the DRIVE dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Liskowski and Krawiec [[69](#bib.bib69)] proposed a pixel-wise supervised vessel-segmentation
    method based on deep CNN, which is trained using fundus images that have been
    pre-processed with global contrast normalization and zero-phase whitening, and
    augmented using geometric transformations and gamma corrections. They used the
    DRIVE, STARE and CHASE datasets to evaluate the system. It is robust against the
    central vessel reflex and sensitive in detecting fine vessels.
  prefs: []
  type: TYPE_NORMAL
- en: Maninis et al. [[70](#bib.bib70)] formulated the retinal blood vessel segmentation
    problem as an image-to-image regression task, for which they employed pre-trained
    VGG, which was modified by removing FC layers and incorporating additional convolutional
    layers after the first four convolution blocks of VGG before pooling the layers.
    The additional convolutional layers are upsampled to the same size as the image,
    trained and concatenated into a volume. They used DRIVE and STARE for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Wu et al. [[71](#bib.bib71)] first extracted discriminative features using a
    CNN and then used nearest neighbor search based on principal component analysis
    (PCA) to estimate the local structure distribution, which was finally employed
    by the generalized probabilistic tracking framework to segment blood vessels.
    This method was evaluated using the DRIVE dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Dasgupta and Singh [[72](#bib.bib72)] used an FCN combined with structured prediction
    to segment blood vessels, which they assumed to be a multi-label inference task.
    The green channel of the images was preprocessed by normalization, contrast, gamma
    adjustment and scaling the intensity value between 0 and 1\. They used DRIVE to
    evaluate the method’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Tan et al. [[73](#bib.bib73)] proposed a seven-layer CNN model to simultaneously
    segment blood vessels, OD and fovea. After normalizing the colored images, they
    formulated the segmentation problem as a classification problem assuming four
    classes—blood vessels, OD, fovea and background—and classified each pixel by taking
    a neighborhood of 25$\times$25 pixels. This is very time consuming because each
    pixel is classified independently, with as many passes made through the net as
    the number of pixels. Its performance was evaluated with the DRIVE dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Fu et al. [[74](#bib.bib74)] similarly formulated the blood vessel segmentation
    problem as a boundary-detection task and proposed a method for this task by integrating
    FCN and fully connected conditional random field (CRF). First, a vessel probability
    map is created using FCN, and then the vessels are segmented by combining the
    vessel probability map and long-range interactions between pixels using CRF. This
    method was validated on the DRIVE and STARE datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Mo and Zhang [[75](#bib.bib75)] used an FCN and incorporated some auxiliary
    classifiers in intermediate layers to make the features more discriminative in
    lower layers. To overcome the small number of available samples, they used transfer
    learning to train the FCN model. They evaluated the system on DRIVE, STARE and
    CHASE.
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance analysis of all the aforementioned methods is given in Table
    [3](#S5.T3 "Table 3 ‣ 5.1.3 Recurrent Neural Network-Based Methods ‣ 5.1 Retinal
    Blood Vessel Segmentation ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided
    Diagnosis Systems for Diabetic Retinopathy: A Survey"). This analysis indicates
    that among all CNN-based methods, the one by Liskowski and Krawiec [[69](#bib.bib69)]
    performed better than all other methods in terms of accuracy, AUC and sensitivity.
    This method may outperform due to the preprocessing of fundus images and training
    of the CNN model using an augmented dataset. All of the other methods use pre-trained
    CNN models without preprocessing or augmentation. Against expectations, the ensemble
    of CNN models by Maji et al. [[68](#bib.bib68)] did not perform better than the
    other CNN-based methods because there is no preprocessing and augmentation of
    the training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Stacked Autoencoder-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some methods employ SAEs in different ways to segment vessels. The method proposed
    by Maji et al. [[65](#bib.bib65)] uses a hybrid DL architecture, which consists
    of unsupervised stacked denoising autoencoders (SDAEs), to segment vessels in
    fundus images. The structure of the first DAE consists of 400 hidden neurons,
    and the second DAE contains 100 hidden neurons. SDAE learns features, which are
    classified using random forest (RF). This approach segments vessels using patches
    of size k × k around each pixel in the green channel. They used DRIVE to assess
    the method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Roy and Sheet [[76](#bib.bib76)] introduced an SAE-based deep neural network
    (SAE-DNN) model for vessel segmentation that employs the domain adaptation (DA)
    approach for its training. SAE-DNN consists of two hidden layers, which are trained
    using the source domain (DRIVE dataset), using an auto-encoding mechanism and
    supervised learning. Then, DA is applied in two stages: unsupervised weight adaptation
    and supervised fine-tuning. In unsupervised weight adaptation, hidden nodes of
    the SAE-DNN are re-trained using unlabeled samples from the target domain (STARE
    dataset) with the auto-encoding mechanism using systematic node dropouts, whereas
    in supervised fine-tuning, the SAE-DNN is fine-tuned using a small number of labeled
    samples from the target domain. The results show that domain DA improves the performance
    of the SAE-DNN.'
  prefs: []
  type: TYPE_NORMAL
- en: Li et al. [[28](#bib.bib28)] proposed segmenting retinal vessels from the green
    channel using a supervised DL approach that labels the patch of a pixel instead
    of a single pixel. In this approach, the vessel-segmentation problem is modeled
    as a cross-modality data transformation that transforms a retinal image to a vessel
    map and is defined using a deep neural network consisting of DAEs. They assessed
    the performance on DRIVE, STARE and CHASE (28 images).
  prefs: []
  type: TYPE_NORMAL
- en: Lahiri et al. [[77](#bib.bib77)] used a two-level ensemble of stacked denoised
    autoencoder networks (SDAEs). In the first-level ensemble, a network (E-net) consists
    of $n$ SDAEs composed of the same structure; each SDAE contains two hidden layers
    and is followed by a Softmax classifier; SDAEs are trained on bootstrap training
    samples using an auto-encoding mechanism in parallel, to produce probabilistic
    image maps, which are conglomerated using a fusion strategy. In the second level
    of the ensemble, to introduce further diversity, decisions from two E-nets having
    different architectures are merged using the convex weighted average. The authors
    used the DRIVE dataset to evaluate the method.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Recurrent Neural Network-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fu et al. [[78](#bib.bib78)] formulated the blood vessel segmentation problem
    as a boundary detection task and proposed the DeepVessel method by integrating
    CNN and CRF as an RNN and evaluated it on the DRIVE, STARE, and CHASE datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'A performance analysis of the aforementioned methods is given in Table [3](#S5.T3
    "Table 3 ‣ 5.1.3 Recurrent Neural Network-Based Methods ‣ 5.1 Retinal Blood Vessel
    Segmentation ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey"). This analysis indicates that among
    all SAE-based methods, the methods based on cross-modality transformation [[28](#bib.bib28)]
    and two-level ensemble of SAEs [[77](#bib.bib77)] outperform SAE-based methods
    in terms of accuracy. Although the method based on two-level ensemble of SAEs
    [[77](#bib.bib77)] performed slightly better than the method based on cross-modality
    transformation [[28](#bib.bib28)], the difference was not significant. Interestingly,
    there was no noticeable difference in the performance of methods based on CNNs
    and on SAEs in terms of accuracy. CNN models involve much more learnable parameters
    than SAE models and as such are prone to overfitting. CNN models can do better
    provided a huge labeled dataset is available or novel augmentation techniques
    are introduced.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Representative of works in diabetic retinopathy (DR) vessels detection'
  prefs: []
  type: TYPE_NORMAL
- en: '| Research study | Segmentation Method | Training | Dataset | Performance |'
  prefs: []
  type: TYPE_TB
- en: '| CNN-Based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Maji et al. [[68](#bib.bib68)] | Patch-based ensemble of CNN models | End-to-end
    | DRIVE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AUC=0.9283 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC= 94.7 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Liskowski and Krawiec [[69](#bib.bib69)] | Patch-based CNN | End-to-end |
    DRIVE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN= 98.07, SP=78.11 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AUC=0.9790 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=95.35 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| STARE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=85.54, SP=98.62 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AUC=0.9928 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=97.29 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CHASE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=81.54, SP=98.66 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AUC=0.988 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=96.96 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Maninis et al. [[70](#bib.bib70)] | FCN | Transfer learning | DRIVE | RPR=0.822
    |'
  prefs: []
  type: TYPE_TB
- en: '| STARE | RPR=0.831 |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[71](#bib.bib71)] | Vessel tracking/patch-based CNN/PCA as classifier
    | End-to-end | DRIVE | AUC=0.9701 |'
  prefs: []
  type: TYPE_TB
- en: '| Dasgupta and Singh [[72](#bib.bib72)] | Patch-based FCN | End-to-end | DRIVE
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=76.91 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SP=98.01 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AUC=0.974 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=95.33 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tan et al. [[73](#bib.bib73)] | Patch-based seven-layers CNN | End-to-end
    | DRIVE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=75.37 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SP=96.94 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Fu et al. [[74](#bib.bib74)] | FCN/CRF | End-to-end | DRIVE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=72.94, ACC=94.70 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| STARE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=71.40, ACC=95.45 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Mo and Zhang [[75](#bib.bib75)] | Multi-level FCN | End-to-end | DRIVE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=77.79, SP=97.80 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AUC=0.9782 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=95.21 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| STARE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=81.47, SP=98.44 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AUC=0.9885 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=96.76 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CHASE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=76.61, SP=98.16 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AUC=0.9812 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=95.99 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| AE-Based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Maji et al. [[65](#bib.bib65)] | Patch-based SDAE/RF | Transfer learning
    | DRIVE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AUC=0.9195, ACC=93.27 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Roy and Sheet [[76](#bib.bib76)] | Patch-based SAE | Transfer learning |
    STARE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AUC=0.92 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; logloss=0.18 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[28](#bib.bib28)] | Patch-based SDAE | End-to-end | DRIVE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=75.6, SP=98 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AUC=0.9738 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=95.27 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| STARE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=77.26, SP=98.79 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=96.28 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CHASE (28 images) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=75.07, SP=97.93 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AUC=0.9716 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=95.81 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Lahiri et al. [[77](#bib.bib77)] | Patch-based DSAE | Transfer learning |
    DRIVE | ACC=95.3 |'
  prefs: []
  type: TYPE_TB
- en: '| RNN-Based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Fu et al. [[78](#bib.bib78)] | Patch-based CNN/CRF as RNN | End-to-end |
    DRIVE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SP=76.03 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=95.23 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| STARE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SP=74.12 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=95.85 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CHASE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SP=71.30 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=94.89 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Optic Disc Feature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Detecting the OD can enhance DR detection and classification because its bright
    appearance can create confusion for other bright lesions such as EXs. OD detection
    involves two operations: (i) localizing and (ii) segmenting the OD. Both CNN and
    SAE models have been employed for OD detection. Some methods only localize the
    OD, which is basically an object-detection problem, and others localize and segment
    the OD, which is a segmentation problem, to identify the area of the OD along
    with its boundaries.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Convolutional Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Lim et al. [[79](#bib.bib79)] method was one of the earliest proposals to employ
    a nine-layer CNN model to segment the OD and OC. It involves four main phases:
    localizing the region around the OD, enhancing this region by exaggerating the
    relevant visual features, classifying the enhanced region at pixel-level using
    a CNN model to produce a probability map and finally segmenting this map to predict
    the disc and cup boundaries. It was assessed on MESSIDOR and SEED-DB.'
  prefs: []
  type: TYPE_NORMAL
- en: Guo et al. [[80](#bib.bib80)] used a large pixel patch-based CNN in which the
    OC was segmented by classification of each pixel patch and postprocessing. They
    used the DRISHTI-GS dataset for training and testing. Similarly, Tan et al. [[73](#bib.bib73)]
    segmented the OD and vessels jointly; it has been reviewed in section vessel segmentation.
    Sevastopolsky [[81](#bib.bib81)] used the modified U-net convolutional network
    presented in [[82](#bib.bib82)] to segment both the OD and OC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zilly et al. [[83](#bib.bib83)] ] introduced an OD and OC segmentation method
    based on a multi-scale two-layer CNN model that is trained with boosting. First,
    the region around the OD is cropped, down-sampled by a factor of 4, converted
    to L*a*b color space and normalized. Then, the region is processed by entropy
    filtering to identify the most discriminative points and is passed to the CNN
    model, which is trained using the gentle AdaBoost method. The logistic regression
    classifier produces a probability map from the output of the CNN model, and finally
    the graph cut method and convex-hull fitting are applied to get the segmented
    OD and OC regions. This method was evaluated with the DRISHTI-GS dataset using
    three performance metrics: F-score, overlap measure (IOU) and boundary error (B).'
  prefs: []
  type: TYPE_NORMAL
- en: An extended version of this method is presented in [[84](#bib.bib84)], Zilly
    et al. [[84](#bib.bib84)] used ensemble CNN with entropy sampling to select informative
    points. These points were used to create a novel learning approach for convolutional
    filters based on boosting.
  prefs: []
  type: TYPE_NORMAL
- en: Maninis et al. [[70](#bib.bib70)] used the same FCN to segment both blood vessels
    and the OD from retinal, images as mentioned in the Vessel Segmentation section.
    The method was validated for OD and OC segmentation on the DRIONS-DB and RIM-ONE
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Shankaranarayana et al. [[51](#bib.bib51)] proposed a method for joint segmentation
    of the OC and OD using residual learning-based, fully convolutional networks (ResU-Net)
    that is similar to U-net [[82](#bib.bib82)] , which contains an encoder on the
    left side of the net involving down-sampling operations and a decoder on the right
    side employing up-sampling operations. A mapping between the retinal image and
    its segmentation map for OD and OC detection is trained using ResU-Net and generative
    adversarial networks (GANs). This method (ResU-GAN) does not involve any preprocessing
    and is efficient, compared with other pixel-segmentation methods [[79](#bib.bib79)].
    This method was tested using 159 images from the RIM-ONE dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. [[85](#bib.bib85)] used a faster region convolutional neural network
    (faster RCNN) with ZF net as the base CNN model to localize the OD. After localizing
    the OD, blood vessels in its bounding box are removed by using a Hessian matrix,
    and a shape-constrained level set is used to cut the OD’s boundaries. They used
    4,000 images selected from Kaggle to train the CNN model and MESSIDOR for testing.
    This method is fast and gives very good localization results.
  prefs: []
  type: TYPE_NORMAL
- en: Fu et al. [[86](#bib.bib86)] used a U-shape CNN model (M-Net) to simultaneously
    segment the OD and OC in one stage and find the cup-to-disc ratio (CDR). The input
    layer of M-Net is a multi-scale layer consisting of an image pyramid. It involves
    a U-shaped CNN with a side-output layer to produce a local prediction map for
    different scale layers and a multi-label loss function output layer. First, the
    OD region is localized and transformed into a polar domain; then, it is passed
    through M-Net to generate a multi-label map, which is inverse transformed into
    the Cartesian domain to segment the OD. The ORIGA and SCES datasets were used
    to assess the method, which gave state-of-the-art results.
  prefs: []
  type: TYPE_NORMAL
- en: The method by Niu et al. [[87](#bib.bib87)] used saliency map region proposal
    generation and a seven-layer-based CNN model to detect the OD. Using the saliency-based
    visual attention model, salient regions of a fundus image are identified, and
    a CNN model is used to classify these regions to locate the OD. This method is
    a validated DL approach that used cascading localization with feedback to localize
    the OD on preprocessed images using mean subtraction. The algorithm ends only
    when it finds a region containing the OD. The authors tested the performance on
    ORIGA, MESSIDOR and these datasets together.
  prefs: []
  type: TYPE_NORMAL
- en: Alghamdi et al. [[88](#bib.bib88)] proposed a method for detecting abnormal
    ODs using a cascade of CNN models. First, candidate OD regions are extracted,
    preprocessed and normalized using whitening. Then, these regions are classified
    using the first module as the OD or non-OD. Finally, the detected OD regions are
    classified as normal, suspicious or abnormal by the second CNN module. This method
    was evaluated on DRIVE, DIARETDB1, MESSIDOR, STARE and a local dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Xu et al. [[89](#bib.bib89)] employed a pre-trained VGG model without the last
    FC layers and deconvolution layers connected to the last three pooling layers
    of a VGG model to calculate the probability map of pixels. The probability map
    is thresholded, and finally, the center of gravity of the pixels above the threshold
    is obtained to locate the OD. The authors used the ORIGA, MESSIDOR and STARE datasets
    for evaluation. This method is efficient in correctly localizing the OD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S5.T4 "Table 4 ‣ 5.2.2 Stacked Autoencoder-Based Methods ‣ 5.2 Optic
    Disc Feature ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided Diagnosis
    Systems for Diabetic Retinopathy: A Survey") presents an aggregated view of the
    OD segmentation and localization methods. For OD segmentation, it is difficult
    to determine which method gives the best performance because all of the methods
    were evaluated on different databases using different metrics. Among the OD-localization
    methods, the method by Zhang et al. [[85](#bib.bib85)] based on faster RCNN gives
    the best localization results for the MESSIDOR dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Stacked Autoencoder-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We found just one method based on SAEs used to segment OD. Srivastava et al.
    [[52](#bib.bib52)]’s idea is to distinguish parapapillary atrophy (PPA) from OD.
    This method crops the region of interest (ROI) after detecting the OD’s center
    and enhances its contrast using CLAHE. Features of each pixel are computed assuming
    a window of size 25x25 around it, which are then passed to a deep SAE consisting
    of one input layer with 626 units; seven hidden layers with 500, 400, 300, 200,
    100, 50, and 20 units; and an output layer, to classify it as an OD or non-OD
    pixel. The binary map of the ROI obtained using the SAE is further refined for
    OD segmentation using an active shape model (ASM). The least mean overlapping
    error (LMOE) was used for evaluation on the dataset containing 230 images taken
    from ref. [[90](#bib.bib90)].. Table [4](#S5.T4 "Table 4 ‣ 5.2.2 Stacked Autoencoder-Based
    Methods ‣ 5.2 Optic Disc Feature ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided
    Diagnosis Systems for Diabetic Retinopathy: A Survey") provides a general view
    showing that CNN-based methods performed better than SAE-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: OD detection works'
  prefs: []
  type: TYPE_NORMAL
- en: '| Research study | Method | Training | Type(s) | Dataset | Performance |'
  prefs: []
  type: TYPE_TB
- en: '| CNN-Based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Lim et al. [[79](#bib.bib79)] | Nine-layer CNN with exaggeration | End-to-end
    | OD segmentation | MESSIDOR | E=0.112, IOU=0.888 |'
  prefs: []
  type: TYPE_TB
- en: '| SEED-DB | E= 0.0843, IOU=0.916 |'
  prefs: []
  type: TYPE_TB
- en: '| Guo et al. [[80](#bib.bib80)] | Large pixel patch-based CNN | End-to-end
    | OC segmentation | DRISHTI-GS | F=93.73, E=0.1225 |'
  prefs: []
  type: TYPE_TB
- en: '| Tan et al. [[73](#bib.bib73)] | Seven-layers CNN | End-to-end | OD segmentation
    | DRIVE | ACC=87.90 |'
  prefs: []
  type: TYPE_TB
- en: '| Sevastopolsky [[81](#bib.bib81)] | Modified U-Net CNN | Transfer learning
    | OD segmentation | DRION-DB | IOU=0.98, Dice=0.94 |'
  prefs: []
  type: TYPE_TB
- en: '| RIM-ONE | IOU=0.98, Dice=0.95 |'
  prefs: []
  type: TYPE_TB
- en: '| OC segmentation | DRION-DB | IOU=0.75, Dice=0.85 |'
  prefs: []
  type: TYPE_TB
- en: '| RIM-ONE | IOU=0.69, Dice=0.82 |'
  prefs: []
  type: TYPE_TB
- en: '| Zilly et al. [[83](#bib.bib83)] | Multi-scale two-layers CNN | End-to-end
    | OD segmentation | DRISHTI-GS | F=94.7, IOU=0.895, B=9.1 |'
  prefs: []
  type: TYPE_TB
- en: '| OC segmentation | F=83, IOU=0.864, B=16.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Zilly et al. [[84](#bib.bib84)] | Ensemble learning-based CNN | End-to-end
    | OD segmentation | DRISHTI-GS | F=97.3, IOU=0.914, B=9.9 |'
  prefs: []
  type: TYPE_TB
- en: '| OC segmentation | F=87.1, IOU=0.85, B=10.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Maninis et al. [[70](#bib.bib70)] | FCN based on VGG-16 | Transfer learning
    | OD segmentation | DRIONS-DB | RPR=0.971 |'
  prefs: []
  type: TYPE_TB
- en: '| RIM-ONE | RPR=0.959 |'
  prefs: []
  type: TYPE_TB
- en: '| Shankaranarayana et al. [[51](#bib.bib51)] | ResU-Net and GANs | Transfer
    learning | OD segmentation | RIM-ONE | F=98.7, IOU= 0.961 |'
  prefs: []
  type: TYPE_TB
- en: '| OC segmentation | F=90.6, IOU=0.739 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[85](#bib.bib85)] | Faster RCNN | Transfer learning | OD localization
    | MESSIDOR | Mean average precision=99.9 |'
  prefs: []
  type: TYPE_TB
- en: '| OD segmentation | MESSIDOR (120 images) | Average matching score of 85.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fu et al. [[86](#bib.bib86)] | U-shaped CNN and polar transformation | Transfer
    learning | OD segmentation | ORIGA | E=0.071, IOU=0.929 |'
  prefs: []
  type: TYPE_TB
- en: '| Niu et al. [[87](#bib.bib87)] | Saliency map, CNN based on AlexNet | Transfer
    learning | OD localization | ORIGA | ACC=99.33 |'
  prefs: []
  type: TYPE_TB
- en: '| MESSIDOR | ACC=98.75 |'
  prefs: []
  type: TYPE_TB
- en: '| ORIGA+ MESSIDOR | ACC=99.04 |'
  prefs: []
  type: TYPE_TB
- en: '| Alghamdi et al. [[88](#bib.bib88)] | Cascade CNN, each model with 10-layers
    | End-to-end | OD localization | DRIVE | ACC=100 |'
  prefs: []
  type: TYPE_TB
- en: '| DIARETDB1 | ACC=98.88 |'
  prefs: []
  type: TYPE_TB
- en: '| MESSIDOR | ACC=99.20 |'
  prefs: []
  type: TYPE_TB
- en: '| STARE | ACC=86.71 |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[89](#bib.bib89)] | CNN based on VGG and deconvolution | Transfer
    learning | OD localization | ORIGA | ACC=100 |'
  prefs: []
  type: TYPE_TB
- en: '| MESSIDOR | ACC=99.43 |'
  prefs: []
  type: TYPE_TB
- en: '| STARE | ACC=89 |'
  prefs: []
  type: TYPE_TB
- en: '| AE-Based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Srivastava et al. [[52](#bib.bib52)] | SAE with ASM | End-to-end | OD segmentation
    | Local dataset used by Foong et al. [[90](#bib.bib90)] | E=0.097 |'
  prefs: []
  type: TYPE_TB
- en: 5.3 Lesion Detection and Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many DL methods have been proposed for detecting and classifying different types
    of DR lesions such as macular edema, exudates, microaneurysms and hemorrhages.
    In this section, we review these methods.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Macula Edema as a Clinical Feature
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The macula is the central part of retina, which consists of a thin layer of
    cells and light-sensitive nerve fibers at the back of eye, and is responsible
    for clear vision. Diabetic macula edema (DME) is a DR complication that occurs
    when the retinal capillaries become permeable and leakage occurs around the macula
    [[14](#bib.bib14)]; when vessels’ fluid and blood enter the retina, the macula
    swells and thickens. The DL methods for DME mainly can be categorized as CNN-based
    and AE-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1.1 Convolutional Neural Networks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Abràmoff et al. [[91](#bib.bib91)] proposed a supervised end-to-end CNN-based
    method to recognize DME. Perdomo et al. [[92](#bib.bib92)] proposed a method that
    combines EX localization and segmentation with DME detection. EX localization
    consists of two stages. In the first stage, an eight-layer CNN model, which takes
    a 488$\times$48 patch as its input, is used to localize EXs. It is trained on
    e-ophtha. In the second stage, using this CNN model as a predictor as well as
    the MESSIDOR dataset, grayscale mask images are produced. The DME detection model
    is based on the AlexNet architecture, which takes a fundus image together with
    a corresponding grayscale mask image as the inputs and predicts the class as normal,
    mild, moderate or severe DME. Preprocessing is used to extract the EXs’ ROIs,
    and data augmentation is applied to generate more samples to train the CNN model.
    The authors used MESSIDOR for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Burlina et al. [[93](#bib.bib93)] used a deep convolutional neural network for
    feature extraction and a linear support vector machine (LSVM) for classification,
    for age-related macular degeneration (AMD). After cropping and resizing a fundus
    image to 231×231 pixels, the OverFeat CNN model pre-trained on the ImageNet dataset
    is used for feature extraction. The dataset NIH AREDS [[41](#bib.bib41)], which
    is divided into four categories according to AMD severity, was used for validation.
  prefs: []
  type: TYPE_NORMAL
- en: Al-Bander et al. [[94](#bib.bib94)] proposed an end-to-end CNN model for grading
    DME severity. After cropping and resizing a fundus image, red, green and blue
    channels are scaled to have zero mean and unit variance. The proposed CNN model
    consists of three convolution blocks and one block of FC layers. Data augmentation
    is applied to increase the number of samples for training. The model was evaluated
    using the MESSIDOR dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Ting et al. [[95](#bib.bib95)] evaluated the performance of a CNN model to diagnose
    AMD and other DR complications and concluded that their CNN was are effective
    in diagnosing DR complications but cannot identify all DME cases using fundus
    images. The CNN model for AMD detection was trained using 72,610 fundus images
    and was tested on 35,948 images from different ethnicities.
  prefs: []
  type: TYPE_NORMAL
- en: Mo et al. [[96](#bib.bib96)] proposed a two-stage method to classify DME. In
    the first stage, a cascaded fully convolutional residual network (FCRN) with fused
    multi-level hierarchical information is used to create a probability map and segment
    EXs. In the second stage, using the segmented regions, the pixels with maximum
    probability are cropped and fed into another residual network to classify DME.
    They used the HEI-MED [[97](#bib.bib97)] and e-ophtha datasets to assess the method.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1.2 Deep Belief Networks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: have also been employed for image-level DME diagnosis. Arunkumar and Karthigaikumar
    [[29](#bib.bib29)] used a DBN for feature extraction and a multiclass SVM for
    classification to diagnose AMD together with other DR complications. In this method,
    fundus images first undergo a preprocessing procedure that includes normalization,
    contrast adjustment or histogram equalization. Then, features are extracted using
    unsupervised DBN, the dimensions of the feature space are reduced with a generalized
    regression neural network (GRNN) and finally classification is performed using
    a multiclass SVM. They used the ARIA dataset to assess the method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The comparison of these CNN and DBN-based methods given in Table [5](#S5.T5
    "Table 5 ‣ 5.3.1.2 Deep Belief Networks ‣ 5.3.1 Macula Edema as a Clinical Feature
    ‣ 5.3 Lesion Detection and Classification ‣ 5 Literature Survey ‣ Deep Learning
    based Computer-Aided Diagnosis Systems for Diabetic Retinopathy: A Survey") shows
    that CNN-based methods outperform DBN-based methods. Among the CNN-based methods,
    the one by Abràmoff et al. [[91](#bib.bib91)] achieved the better performance,
    probably because it is based on an Alexnet-like model. DBNs have not been used
    in an end-to-end way; thus, they must be explored further using end-to-end learning.
    Interestingly, DBNs involve significantly fewer learnable parameters than CNN
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Representative works for DME detection'
  prefs: []
  type: TYPE_NORMAL
- en: '| Research Study | Method | Training | Lesion Type(s) | Dataset | Performance
    |'
  prefs: []
  type: TYPE_TB
- en: '| CNN-Based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Abràmoff et al. [[91](#bib.bib91)] | CNN inspired by AlexNet | End-to-end
    | Multistage DR/ME | MESSIDOR-2 | SN=100 |'
  prefs: []
  type: TYPE_TB
- en: '| Mo et al. [[96](#bib.bib96)] | Cascaded FCRN | End-to-end | ME | HEI-MED
    | SN=92.55,F=84.99 |'
  prefs: []
  type: TYPE_TB
- en: '| e-ophtha | SN=92.27, F=90.53 |'
  prefs: []
  type: TYPE_TB
- en: '| Perdomo et al. [[92](#bib.bib92)] | Patches based CNN model | Transfer learning
    | Multistage DR/ME | MESSIDOR |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=56.5, SP=92.8 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DME ACC=77 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DME loss=0.78 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Burlina et al. [[93](#bib.bib93)] | CNN-based on OverFeat | Transfer learning
    | Multistage AMD | NIH AREDS |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=90.9-93.4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SP=89.9-95.6 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=92-95 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Al-Bander et al. [[94](#bib.bib94)] | CNN model with three conv. blocks and
    one FC block | End-to-end | Multistage DR/ME | MESSIDOR |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=74.7, SP=95 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=88.8 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ting et al. [[95](#bib.bib95)] | CNN | End-to-end | AMD | 35948 images |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=93.2, SP=88.7 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AUC=0.931 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DBN-Based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Arunkumar and Karthigaikumar [[29](#bib.bib29)] | DBN for training and multiclass
    SVM as classifier | End-to-end | AMD | ARIA |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SN=79.32, SP=97.89 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACC=96.73 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Exudate as a Clinical Feature
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The detection of EX is necessary for automatic early DR diagnosis, but it is
    challenging because of significant variation in their size, shape and contrast
    levels. In this section, we review DL-based methods for EX detection. According
    to our best knowledge, all of the methods are based on CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2.1 Convolutional Neural Networks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Prentašić and Lončarić [[98](#bib.bib98)] proposed a CNN-based method for EX
    detection in color fundus images. First, they detect the OD, create an OD probability
    map and fit a parabola. Then they create vessel probability and bright-border
    probability maps. Finally, using an 11-layer CNN model, they create an EX probability
    map and combine it with the OD, vessel and bright-border probability maps and
    the fitted parabola to generate the final EX probability map. They assessed the
    model’s performance using the DRiDB database. It significantly outperformed the
    traditional methods based on hand-engineered features.
  prefs: []
  type: TYPE_NORMAL
- en: Perdomo et al. [[99](#bib.bib99)] proposed a patch-level method based on the
    LeNet model to discriminate EX regions from healthy regions on fundus images.
    In this method, potential EX patches are first cropped manually or automatically;
    then, these patches are passed to the LeNet model for classification. To train
    LeNet, extra patches are created using a data-augmentation technique based on
    flipping and rotation operations. The e-ophtha dataset was used for validation;
    20,148 EXs and healthy patches were extracted, and 40% of these patches was used
    as testing data.
  prefs: []
  type: TYPE_NORMAL
- en: Gondal et al. [[27](#bib.bib27)] introduced a method for detecting EXs together
    with other DR lesions based on the award-winning $o\_O$ CNN architecture [[100](#bib.bib100)].
    To localize DR lesions, including hard EX (HE) and soft EX (SE), the dense layers
    are removed from the CNN model. A global average pooling (GAP) layer is introduced
    on top of the last convolutional layer and is followed by a classification layer,
    which are used to learn the class-specific importance of each feature map of the
    last convolution layer. The feature maps are combined with class-specific importance
    to generate a class activation map (CAM) [[101](#bib.bib101)], which is up-sampled
    to the size of the original image to localize the lesion regions. The authors
    used Kaggle for training and DIARETDB1 for validation. This method not only performs
    image-level detection but also lesion-level detection
  prefs: []
  type: TYPE_NORMAL
- en: Quellec et al. [[102](#bib.bib102)] addressed the problem of jointly detecting
    referable DR at the image level and detecting DR lesions such as EXs at the pixel
    level, and they proposed a solution that relies on CNN visualization methods.
    The heatmaps generated by CNN visualization techniques are not optimized for computer-aided
    diagnosis of DR lesions. Based on the sensitivity analysis by Simonyan and Zisserman
    [[57](#bib.bib57)], they proposed modifications to generate heatmaps, which help
    in jointly detecting referable DR and lesions by jointly optimizing CNN predictions
    and the produced heatmaps. They employed the $o\_O$ architecture as the CNN base
    model. The authors used the Kaggle dataset for training at the image level and
    DIARETDB1 for testing at both the lesion and image levels for EX detection.
  prefs: []
  type: TYPE_NORMAL
- en: Khojasteh et al. [[103](#bib.bib103)] compared several DL-patch-based methods
    to detect EX. They concluded that pre-trained ResNet-50 with SVM outperformed
    other methods. They assessed their method on DIARETDB1 and e-ophtha.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [6](#S5.T6 "Table 6 ‣ 5.3.2.1 Convolutional Neural Networks ‣ 5.3.2 Exudate
    as a Clinical Feature ‣ 5.3 Lesion Detection and Classification ‣ 5 Literature
    Survey ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey") presents a comparative analysis of the aforementioned methods for EX
    detection using a DL approach. The methods of Quellec et al. [[102](#bib.bib102)],
    Gondal et al. [[27](#bib.bib27)], which jointly detect referable DR and lesions,
    show good performance for both lesion and image-base detection. The method inKhojasteh
    et al. [[103](#bib.bib103)] is computationally more efficient and produces comparable
    results due to using the deep pre-trained ResNet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Representative of works in diabetic retinopathy (DR) EX detection'
  prefs: []
  type: TYPE_NORMAL
- en: '| Research Study | Method | Training | Lesion Type(s) | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Segment/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; localize? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Performance |'
  prefs: []
  type: TYPE_TB
- en: '| CNN-Based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Prentašić and Lončarić [[98](#bib.bib98)] | 11-layer CNN, OD and vessel maps
    | End-to-end | EX | DRiDB | ✓ | SN=78, F=78 |'
  prefs: []
  type: TYPE_TB
- en: '| Perdomo et al. [[99](#bib.bib99)] | Patches-based LeNet CNN | Transfer learning
    | EX | e-ophtha(40% of patches) | ✓ | SN=99.8, SP=99.6 ACC=99.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Gondal et al. [[27](#bib.bib27)] | $o\_O$ CNN model with CAM | Transfer learning
    | HE/SE | DIARETDB1 | ✓ | SN:HE=87, SE=80 |'
  prefs: []
  type: TYPE_TB
- en: '| HE/SE | ✗ | SN:HE =100, SE=90.0 AUC=0.954 |'
  prefs: []
  type: TYPE_TB
- en: '| Quellec et al. [[102](#bib.bib102)] | $o\_O$ CNN(net A) | Transfer learning
    | HE/SE | DIARETDB1 | ✓ | AUC:HE=0.735, SE=0.809 |'
  prefs: []
  type: TYPE_TB
- en: '| $o\_O$ CNN(net B) | HE/SE | ✗ | AUC:HE=0.974, SE=0.963 |'
  prefs: []
  type: TYPE_TB
- en: '| Khojasteh et al. [[103](#bib.bib103)] | Patch-based ResNet/SVM as classifier
    | Transfer learning | EX | DIARETDB1 | ✗ | SN=99, SP=96, ACC=98.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | e-ophtha | ✗ | SN=98, SP=95, ACC=97.6 |'
  prefs: []
  type: TYPE_TB
- en: 5.3.3 Microaneurysms and Hemorrhages as Clinical Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MAs and HMs are also have been investigated using DL approaches as a sign of
    DR, as presented in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3.1 Convolutional Neural Networks (CNN)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Haloi [[104](#bib.bib104)] employed a nine-layer CNN model with a dropout training
    procedure to classify each pixel as MA or non-MA. Each pixel is classified by
    taking a window of size 129$\times$129 around it and passing the window to the
    CNN model. For training, the author employed a data-augmentation technique to
    generate six windows around each pixel. He graded the severity from no DR to severe
    DR according to the number of MAs. The method was tested on the MESSIDOR and Retinopathy
    Online Challenge (ROC) datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The method introduced by van Grinsven et al. [[105](#bib.bib105)] was aimed
    at detecting HMs. The main contribution of this method is to address the over-represented
    normal samples created for training a CNN model. To overcome this problem, the
    authors proposed a dynamic selective sampling strategy that selects informative
    training samples. First, they extract patches of size 41$\times$41 around HM pixels
    from positive images only and non-HM pixels from positive images only, and each
    patch is labeled according to the central pixel. The CNN is trained using a dynamic
    selective sampling strategy. They used a 10-layer CNN model and tested their system
    on Kaggle and MESSIDOR.
  prefs: []
  type: TYPE_NORMAL
- en: The methods of Gondal et al. [[27](#bib.bib27)] and Quellec et al. [[102](#bib.bib102)]
    discussed in the Exudate section, which jointly detect referable DR and lesions,
    also detect HMs and small red dots. Another similar method was proposed by Orlando
    et al. [[106](#bib.bib106)]. In this method, they first extract candidate red
    lesions using morphological operations and crop patches of size 32×32 around the
    candidates. Next, they extract CNN features and hand-engineered features (HEFs)
    such as intensity and shape features from each candidate patch, fuse them and
    pass the fused feature vector to random forest (RF) to create a probability map,
    which is used to make lesion- and image-level decisions about red lesions. They
    employed a six-layer CNN model. For lesion-based evaluation, they used as a competition
    metric (CPM) the average per lesion sensitivity at the reference false positive
    detections per image value. They used the DIARETDB1 and e-ophtha datasets for
    per lesion evaluation. They used MESSIODR for detecting referable DR.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3.2 Stacked Autoencoder-Based Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Shan and Li [[107](#bib.bib107)] used the stacked sparse autoencoder (SSAE)
    to detect MA lesions. A patch is passed to SSAE, which extracts features, and
    the Softmax classifier labels it as a MA or non-MA patch. They trained and fine-tuned
    the SSAE on MA and non-MA patches taken from 89 fundus retinal images selected
    from the DIARETDB dataset. The patches were extracted without any preprocessing
    procedure, and Shan and Li evaluated them using 10-fold cross validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of the above-reviewed methods is given in Table [7](#S5.T7 "Table
    7 ‣ 5.3.3.2 Stacked Autoencoder-Based Methods ‣ 5.3.3 Microaneurysms and Hemorrhages
    as Clinical Features ‣ 5.3 Lesion Detection and Classification ‣ 5 Literature
    Survey ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey"). In terms of sensitivity, specificity, AUC and accuracy, the CNN-based
    technique by Haloi [[104](#bib.bib104)] seems to outperform other methods for
    MA detection due to using pixel augmentation instead of image-based augmentation.
    The performance of the stacked sparse autoencoder based-method by Shan and Li
    [[107](#bib.bib107)] is not better than those based on CNN. Among CNN-based methods,
    those used by Gondal et al. [[27](#bib.bib27)] and Quellec et al. [[102](#bib.bib102)]
    are computationally efficient and jointly detect referable DR and red lesions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Representative of works in diabetic retinopathy (DR) detection based
    on MA and HM'
  prefs: []
  type: TYPE_NORMAL
- en: '| Research study | Method | Training | Lesion Type(s) | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Segment/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; localize? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Performance |'
  prefs: []
  type: TYPE_TB
- en: '| CNN Based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Haloi [[104](#bib.bib104)] | 9-layer CNN | End-to-end | MA | MESSIDOR | ✓
    | SN=97, SP=95 AUC=0.982 ACC=95.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ROC | ✓ | AUC=0.98 |'
  prefs: []
  type: TYPE_TB
- en: '| van Grinsven et al. [[105](#bib.bib105)] | Patches based selective sampling
    | End-to-end | HM | Kaggle | ✓ | SN=84.8, SP=90.4 AUC=0.917 |'
  prefs: []
  type: TYPE_TB
- en: '| MESSIDOR | ✓ | SN=93.1, SP=91.5 AUC=0.979 |'
  prefs: []
  type: TYPE_TB
- en: '| Gondal et al. [[27](#bib.bib27)] | o_O CNN model | Transfer learning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -HM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Small red dots &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| DIARETDB1 | ✓ | SN: -HM=91 -Small red dots=52 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -HM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Small red dots &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ✗ | SN: -HM=97.2 -Red small dots=50 |'
  prefs: []
  type: TYPE_TB
- en: '| Quellec et al. [[102](#bib.bib102)] | o_O CNN (net B) | Transfer learning
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; -HM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Small red dots &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| DIARETDB1 | ✓ | AUC: -HM=0.614 -Small red dots=0.50 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -HM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Small red dots &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ✗ | AUC: -HM=0.999 -Small red dots=0.912 -Red small dots +HM=0.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Orlando et al. [[106](#bib.bib106)] | HEF + CNN features and RF classifier
    | End-to-end | MA | DIARETDB1 | ✓ | CPM=0.3301, SN=48.83 |'
  prefs: []
  type: TYPE_TB
- en: '| HM | ✓ | CPM=0.4884, SN=48.83 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| e-ophtha | ✓ | CPM=0.3683, SN=36.80 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Red lesion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| MESSIDOR | ✗ | SN=91.09, SP=50 AUC=0.8932 |'
  prefs: []
  type: TYPE_TB
- en: '| AE Based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Shan and Li [[107](#bib.bib107)] | Patches based SSAE | Transfer learning
    | MA | DIARETDB | ✓ | SP=91.6 F=91.3 ACC=91.38 |'
  prefs: []
  type: TYPE_TB
- en: 5.4 Classification of Fundus Images for Referral
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section focuses on methods that deal with referable DR detection and use
    only image-level annotation. The main purpose of these methods is to grade DR
    levels for referral. Some methods in this category also detect lesions jointly
    with referable DR detection, but without using pixel- or lesion-level annotation
    [[102](#bib.bib102)]. To the best of our knowledge, only CNN models have been
    employed for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Gulshan et al. [[108](#bib.bib108)] used the Inception-v3 CNN architecture to
    detect referable DR on a fundus image. They assessed the system using the EyePACS-1
    dataset, which consists of 9,963 images taken from 4,997 patients and MESSIDOR-2;
    both were graded by at least seven US licensed ophthalmologists and ophthalmology
    senior residents. This evaluation study concluded that an algorithm based on CNN
    has high sensitivity and specificity for detecting referable DR.
  prefs: []
  type: TYPE_NORMAL
- en: Colas et al. [[109](#bib.bib109)] proposed a method based on deep learning,
    which jointly detects referable DR and lesion location. They trained the deep
    model on 70,000 labeled images and tested 10,000 images taken from Kaggle dataset,
    where each patient has two images of right and left eyes. Each image is graded
    by ophthalmologists into five main stages that vary from no retinopathy to proliferative
    retinopathy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, as discussed in the MAs and HMs section, the method used by Quellec
    et al. [[102](#bib.bib102)] jointly detects referable DR and lesions; its performance
    was evaluated on three datasets: Kaggle, e-ophtha and DIRETDB1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Costa and Campilho [[110](#bib.bib110)] used a different approach and introduced
    a method for detecting referable DR by generalizing the idea of bag-of-visual-words
    (BoVW). First, they extract sparse local features with speeded-up robust features
    (SURF) and encode them using convolution operation or encoded dense features with
    a CNN model, and then use neural network for classification. They evaluated the
    proposed methods on three different datasets: DR1 and DR2 from [[111](#bib.bib111)]
    and MESSIDOR. DR1 and DR2 consist of grayscale images. The authors show that the
    SURF-based method outperforms the CNN-based method, probably because the CNN architecture
    is not deep enough.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pratt et al. [[112](#bib.bib112)] used CNN structure for grading fundus images
    into one of the five stages: no DR, mild DR, moderate DR, severe DR and proliferated
    DR. They addressed the issues of overfitting and skewed datasets, and proposed
    a technique to solve these issues. For training, they enhanced the volume of data
    using a data augmentation technique. The employed CNN model consists of 10 convolutional
    layers and three fully connected layers. For training, they used 80,000 images
    taken from the Kaggle dataset, and 5,000 images for testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Gargeya and Leng [[113](#bib.bib113)] used the ResNet CNN model consisting of
    five residual blocks of four, six, eight, 10, and six layers, respectively, and
    a gradient boosting classifier for grading a fundus image as normal or referable
    DR. Additionally, they introduced a convolutional visualization layer at the end
    of ResNet for visualizing its learning procedure. For training, they used 75,137
    images selected from the EyePACS dataset and evaluated independently on MESSIDOR-2
    and e-ophtha datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Abràmoff et al. [[91](#bib.bib91)] also proposed a method based on supervised
    end-to-end CNN models, discussed in the ME section, to grade a fundus image as
    normal or referable DR; a fundus image is taken to be referable DR if it is moderate
    DR, severe non-proliferative DR (NPDR) or proliferative DR (PDR).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Ting et al. [[95](#bib.bib95)] addressed the problem of detecting
    referable DR using 76,370 images for training. This method is discussed in the
    mecula edema section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wang et al. [[114](#bib.bib114)] proposed a supervised image-level CNN-based
    approach that diagnosed DR and highlighted suspicious patches regions. They used
    a network called Zoom-in, which mimics the zoomin procedure of retinal clinical
    examination. The architecture of the network consisted of three parts: main network
    (M-Net), which was pre-trained on ImageNet, a sub-network; attention network (A-Net)
    to generate attention maps; and another sub-network, crop-network (C-Net). They
    used EyePACS and MESSIDOR to evaluate the system.'
  prefs: []
  type: TYPE_NORMAL
- en: The method by Mansour [[115](#bib.bib115)] used the AlexNet model in conjunction
    with a preprocessing, Gaussian mixture model for background subtraction and connected
    component analysis to localize blood vessels; then linear discriminant analysis
    is used for dimensionality reduction. Finally, SVM is employed for classification
    and 10-fold cross validation is used for evaluation. Also, as discussed in the
    MAs and HMs section, the method by Orlando et al. [[106](#bib.bib106)], jointly
    detects referable DR and lesions; its performance was evaluated on MESSIDOR.
  prefs: []
  type: TYPE_NORMAL
- en: Chen et al. [[116](#bib.bib116)] built a model called SI2DRNet-v1 to detect
    referral DR that consisted of 20 layers. After applying preprocessing in their
    model, such as a Gaussian filter, they used global average pooling instead of
    FC layers and 1$\times$1 filters to reduce parameters and regularize the model;
    they also scaled the kernel size after each pooling layer from 3$\times$3 to 5$\times$5\.
    Finally, they extracted 5 probability values from the Softmax layer to grade DR
    severity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [8](#S5.T8 "Table 8 ‣ 5.4 Classification of Fundus Images for Referral
    ‣ 5 Literature Survey ‣ Deep Learning based Computer-Aided Diagnosis Systems for
    Diabetic Retinopathy: A Survey") summarizes the works presented in this section.
    Apparently, the method by Gulshan et al. [[108](#bib.bib108)] outperforms other
    methods in terms of sensitivity, specificity and AUC; the performance of this
    method is comparable to a panel of seven certified ophthalmologists. However,
    it is difficult to compare the performance of the methods because different datasets
    were used for training and testing. The method by Gargeya and Leng [[113](#bib.bib113)]
    seems to be robust because it is based on ResNet architecture, which has been
    shown to outperform most of the CNN architectures; it was trained and tested on
    different datasets, and its cross-dataset performance is quite good. A CNN model
    is like a black box and does not give any insight into pathology. This method
    also incorporates the visualization of pathologic regions, which can aid real-time
    clinical validation of automated diagnoses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Representative of works in diabetic retinopathy (DR) for referral'
  prefs: []
  type: TYPE_NORMAL
- en: '| Research Study | Method | Training | Dataset | SN% | SP% | AUC | ACC% |'
  prefs: []
  type: TYPE_TB
- en: '| Gulshan et al. [[108](#bib.bib108)] | Inception-v3 CNN | Transfer learning
    | EyePACS-1 | 90.3 | 90 | 0.991 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MESSIDOR-2 | 87 | 98.5 | 0.990 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Colas et al. [[109](#bib.bib109)] | CNN model | End-to-end | Kaggle | 96.2
    | 66.6 | 0.946 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Quellec et al. [[102](#bib.bib102)] | $o\_O$ CNN(net B) | Transfer learning
    | DIARETDB1 | - | - | 0.954 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Ensemble net A, net B, AlexNet | Kaggle | - | - | 0.955 | - |'
  prefs: []
  type: TYPE_TB
- en: '| e-ophtha | - | - | 0.949 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Costa and Campilho [[110](#bib.bib110)] | Sparse SURF/CNN | End-to-end |
    MESSIDOR(20% of images ) | - | - | 0.90 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DR1(20% of images ) | - | - | 0.93 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DR2(20% of images) | - | - | 0.97 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Pratt et al. [[112](#bib.bib112)] | 13-layers CNN | End-to-end | Kaggle |
    95 | - | - | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| Gargeya and Leng [[113](#bib.bib113)] | ResNet+Gradient boosting tree | End-to-end
    | MESSIDOR-2 | - | - | 0.94 | - |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; e-ophtha &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| - | - | 0.95 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Ting et al. [[95](#bib.bib95)] | CNN | End-to-end | (71896 images) | 90.5
    | 91.6 | 0.936 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Abràmoff et al. [[91](#bib.bib91)] | CNN | End-to-end |'
  prefs: []
  type: TYPE_TB
- en: '&#124; MESSIDOR-2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 96.8 | 87 | 0.980 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[114](#bib.bib114)] | Zoom-in network | Transfer learning |
    EyePACS | - | - | 0.825 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MESSIDOR | - | - | 0.957 | 91.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[116](#bib.bib116)] | SI2DRNet-v1(20 layers) | End-to-end |
    MESSIDOR | - | - | 0.965 | 91.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Mansour [[115](#bib.bib115)] | AlexNet/SVM as classifier | Transfer learning
    | Kaggle | 100 | 93 | - | 97.93 |'
  prefs: []
  type: TYPE_TB
- en: '| Orlando et al. [[106](#bib.bib106)] | HEF + CNN features and RF classifier
    | End-to-end | MESSIDOR | 97.21 | 50 | 0.9347 | - |'
  prefs: []
  type: TYPE_TB
- en: 6 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous section gives a detailed account of techniques related to DR diagnosis
    based on a deep-learning approach. The studies listed in this survey use four
    main deep learning architectures: CNN, AE, DBN and RNN. Each of these architectures
    has several variations that have been used in DR diagnosis. Deep-learning-based
    techniques have been proposed for retinal vessels segmentation, OD detection and
    segmentation, DR lesion detection and classification, and referable DR detection.
    A review of these methods indicates that most deep-learning-based techniques for
    the above problems use CNN architecture, and that it outperforms other deep architectures.
    Despite these improvements, there are still challenges to improve deep learning
    techniques for more robust and accurate detection, localization and diagnosis
    of different DR biomarkers and complications. All reviewed methods were tested
    and evaluated on public domain datasets, except two methods [[52](#bib.bib52),
    [95](#bib.bib95)] that used fundus images collected from medical organizations
    and hospitals. For the most part, methods addressing the same problem were evaluated
    on different datasets using different metrics, and as such, it is difficult to
    precisely compare them and grade them based on their performance. Most of the
    methods were evaluated using the same dataset for training and testing, and performance
    of the same method is different for different datasets; this raises questions
    about their robustness and how these methods will perform when deployed in a real
    clinical setting.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a serious difficulty in interpreting and comparing the results of different
    methods in terms different performance metrics when different datasets are used
    for evaluation. For example, method by Liskowski and Krawiec [[69](#bib.bib69)]
    is tested on STARE consisting of 402 images (38 test negatives and 364 test positives)
    and method by Dasgupta and Singh [[72](#bib.bib72)] is tested on the DRIVE consisting
    of 40 images (33 test negatives and 7 test negatives). Both methods are similar
    in terms of specificity (SP), area under ROC curve (AUC) and accuracy (ACC) i.e.
    both have SP = 98%, AUC = 0.97-0.99 and ACC = 95%-97%., but the method by Liskowski
    and Krawiec [[69](#bib.bib69)] is far better (with SN = 85%) than the method by
    Dasgupta and Singh [[72](#bib.bib72)] (with SN = 76%) when sensitivity is used
    for evaluation. It indicates that the methods must be evaluated on the same datasets
    to estimate their real performance gains.
  prefs: []
  type: TYPE_NORMAL
- en: Though CNN architecture results in better performance, CNN models involve a
    huge number of parameters, and requires a huge volume of annotated datasets; however,
    the available datasets consist of a small number of annotated images. As such,
    when CNN is used to detect and diagnose different DR complications, there is a
    high risk of overfitting. One solution to deal this problem is data augmentation,
    but the data augmentation techniques that have been used so far do not create
    real samples. More data augmentation techniques are needed in order to create
    new samples from existing ones. Another solution is to use transfer learning,
    i.e. first train a CNN model using a dataset from a related domain and then fine-tune
    it with the dataset from the domain of the problem. Most of the reviewed methods
    use CNN models pre-trained on natural images [[70](#bib.bib70), [87](#bib.bib87),
    [89](#bib.bib89), [93](#bib.bib93)], e.g. the ImageNet dataset for transfer learning;
    only a few methods used CNN models pre-trained on fundus images [[102](#bib.bib102),
    [27](#bib.bib27)]. Another alternative to deal with the overfitting problem is
    to introduce CNN models that are expressive but involve fewer learnable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Comparison of Deep-Learning-Based and Hand-Engineered Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we compare traditional methods based on hand-engineered features
    and deep-learning-based methods. For comparison, we selected the methods evaluated
    using the same datasets and performance metrics. The selected traditional methods
    are the state-of-the-art methods reported in references [[117](#bib.bib117), [5](#bib.bib5)]
    for vessels segmentation and [[6](#bib.bib6)] for OD and [[1](#bib.bib1)] for
    MAs. The deep learning methods that give the best performance in this review are
    selected for comparison. Although hand-engineered features have been dominant
    for long time, the deep learning approach is a state-of-the-art technique and
    has shown impressive performance compared with traditional approaches. Table [9](#S6.T9
    "Table 9 ‣ 6.1 Comparison of Deep-Learning-Based and Hand-Engineered Methods ‣
    6 Discussion ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic
    Retinopathy: A Survey") presents a comparison between traditional and deep-learning-based
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For retinal blood vessel segmentation, the traditional method by Villalobos-Castaldi
    et al. [[118](#bib.bib118)] seems to give better sensitivity (96.48%) and accuracy
    (97.59%) than the deep learning method used by Liskowski and Krawiec [[69](#bib.bib69)]
    (sensitivity: 78.11% and accuracy: 95.35%) on the DRIVE dataset; however, the
    latter method gives overall better performance than another traditional method
    by Condurache and Mertins [[119](#bib.bib119)] on the STARE and CHASE datasets.
    This indicates that deep learning based methods outperform traditional methods
    overall, but in spite of this fact, even deep-learning-based methods are not robust:
    their performance is different for different datasets. Figure [3](#S6.F3 "Figure
    3 ‣ 6.1 Comparison of Deep-Learning-Based and Hand-Engineered Methods ‣ 6 Discussion
    ‣ Deep Learning based Computer-Aided Diagnosis Systems for Diabetic Retinopathy:
    A Survey") shows a plot of the performance of traditional and DL-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: For OD localization, the deep-learning-based method by Zhang et al. [[85](#bib.bib85)],
    with an accuracy of 99.9%, outperforms the traditional method used by Aquino et al.
    [[120](#bib.bib120)], with an accuracy of 99% on the MESSIDOR dataset. For the
    DRIVE dataset, the learning-based method by Alghamdi et al. [[88](#bib.bib88)]
    and traditional method by Zhang et al. [[121](#bib.bib121)] achieved the same
    performance with an accuracy of 100%. However, for DIARETDB1, the traditional
    method by Sinha and Babu [[122](#bib.bib122)], with an accuracy of 100%, outperforms
    the deep-learning accuracy of 98.88% achieved by Alghamdi et al. [[88](#bib.bib88)].
  prefs: []
  type: TYPE_NORMAL
- en: 'For OD segmentation, deep-learning-based methods show significantly higher
    accuracy than traditional methods. For example, on the MESSIDOR dataset, the traditional
    method used by Aquino et al. [[120](#bib.bib120)] showed less accuracy (86%) than
    that of the deep-learning-based method of Lim et al. [[79](#bib.bib79)], achieving
    significant higher accuracy (96.4%). Similarly, on the DRIVE dataset, the traditional
    method used by Tjandrasa et al. [[123](#bib.bib123)] gave lower accuracy (75.56%)
    than that yielded by Tan et al.’s deep-learning-based method. (92.68%) Tan et al.
    [[73](#bib.bib73)]. Figure [4](#S6.F4 "Figure 4 ‣ 6.1 Comparison of Deep-Learning-Based
    and Hand-Engineered Methods ‣ 6 Discussion ‣ Deep Learning based Computer-Aided
    Diagnosis Systems for Diabetic Retinopathy: A Survey") summarizes the performance
    of OD localization and segmentation in traditional and DL methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Exudate detection review by Joshi and Karule [[3](#bib.bib3)] that published
    2018 reported the maximum EX detection performance on private datasets. However,
    we compared the best DL method with the one that has maximum number of images
    and higher performance. Massey et al. [[124](#bib.bib124)] showed slightly better
    accuracy (98.87%) whereas the DL based method by Khojasteh et al. [[103](#bib.bib103)]
    achieved 98.2% but with higher sensitivity (99%).
  prefs: []
  type: TYPE_NORMAL
- en: 'For MAs detection, on the MESSIDOR dataset, Haloi’s deep-learning-based method
    [[104](#bib.bib104)] achieved a higher performance with sensitivity, specificity,
    AUC and accuracy of 97%, 95%, 0.982 and 95.4%, respectively, whereas the traditional
    method by Antal and Hajdu [[125](#bib.bib125)] equaled 94%, 90%, 0.942 and 90%,
    respectively. Figure [5](#S6.F5 "Figure 5 ‣ 6.1 Comparison of Deep-Learning-Based
    and Hand-Engineered Methods ‣ 6 Discussion ‣ Deep Learning based Computer-Aided
    Diagnosis Systems for Diabetic Retinopathy: A Survey") presents both EX and MA
    detection the performance in traditional and DL methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, CNN-based methods for retinal vessel segmentation, OD detection and
    segmentation and DR lesion detection outperform traditional methods. However,
    deep-learning methods are not robust, not interpretable and suffer from overfitting,
    and further research is needed to overcome these issues.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/673f7b752d95ee2451f85a5261d442b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The plot for maximum performance of vessel segmentation in traditional
    and DL-based methods'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4432d4cfbd1979a4cba223d8befe7ac6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The plot for maximum performance of OD localization and segmentation
    in traditional and DL-based methods'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d822476e8d9be861b36cc7f4424d11a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The plot for maximum performance of MA and EX detection in traditional
    and DL-based methods'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Comparison between state-of-the-art traditional methods and best-performance
    deep-learning methods'
  prefs: []
  type: TYPE_NORMAL
- en: '| Features | Approach | Study | Dataset | SN% | SP% | AUC | ACC% |'
  prefs: []
  type: TYPE_TB
- en: '| Vessels | Traditional | Villalobos-Castaldi et al. [[118](#bib.bib118)] |
    DRIVE | 96.48 | 94.80 | - | 97.59 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | Liskowski and Krawiec [[69](#bib.bib69)] | 78.11 | 98.07
    | 0.9790 | 95.35 |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional | Condurache and Mertins [[119](#bib.bib119)] | STARE | 89.02
    | 96.73 | 0.9791 | 95.95 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | Liskowski and Krawiec [[69](#bib.bib69)] | 85.54 | 98.62
    | 0.9928 | 97.29 |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional | Condurache and Mertins [[119](#bib.bib119)] | CHASE | 72.24
    | 97.11 | 0.9712 | 94.69 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | Liskowski and Krawiec [[69](#bib.bib69)] | 81.54 | 98.66
    | 0.988 | 96.96 |'
  prefs: []
  type: TYPE_TB
- en: '| OD localization | Traditional | Aquino et al. [[120](#bib.bib120)] | MESSIDOR
    | - | - | - | 99 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | Zhang et al. [[85](#bib.bib85)] | - | - | - | 99.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional | Zhang et al. [[121](#bib.bib121)] | DRIVE | - | - | - | 100
    |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | Alghamdi et al. [[88](#bib.bib88)] | - | - | - | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional | Sinha and Babu [[122](#bib.bib122)] | DIARETDB1 | - | - | -
    | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | Alghamdi et al. [[88](#bib.bib88)] | - | - | - | 98.88 |'
  prefs: []
  type: TYPE_TB
- en: '| OD segm entation | Traditional | Aquino et al. [[120](#bib.bib120)] | MESSIDOR
    | - | - | - | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | Lim et al. [[79](#bib.bib79)] | - | - | - | 96.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional | Tjandrasa et al. [[123](#bib.bib123)] | DRIVE | - | - | - |
    75.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | Tan et al. [[73](#bib.bib73)] | 87.90 | 99.27 | - | 92.68
    |'
  prefs: []
  type: TYPE_TB
- en: '| EX | Traditional | Massey et al. [[124](#bib.bib124)] | 50 images | 96.9
    | 98.9 | - | 98.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | Khojasteh et al. [[103](#bib.bib103)] | DIARETDB1 | 99 |
    96 | - | 98.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MA | Traditional | Antal and Hajdu [[125](#bib.bib125)] | MESSIODR ,R0vsR1
    | 94 | 90 | 0.942 | 90 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | Haloi [[104](#bib.bib104)] | 97 | 95 | 0.982 | 95.4 |'
  prefs: []
  type: TYPE_TB
- en: 7 Gaps and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This review of methods related to DR diagnosis reveals that deep learning helped
    to design better methods for DR diagnosis and moved state-of-the-art techniques
    forward, but it is still an open problem, and more research is needed. There are
    not many methods based on deep learning, and advanced deep-learning techniques
    must be developed in order to solve this problem. Deep-learning-based models are
    mostly black boxes and do not provide interpretations of diagnostic value that
    could help validate their usefulness in a real clinical setting. Most of the methods
    in this review do not provide any interpretation of their outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most challenging problems in designing robust deep-learning methods,
    especially based on CNN models with deeper architectures, is the acquisition of
    huge volumes of labeled fundus images with pixel- and image-level annotations.
    The main issue is not the availability of huge datasets, but the annotation of
    these images, which is expensive and requires the services of expert ophthalmologists.
    The solution is to design learning algorithms that can learn a deep model from
    limited data; this is an important area of research not only for DR diagnosis,
    but also in medical image analysis; one possible direction to explore Generative
    Adversarial Networks (GANs) [[126](#bib.bib126)]. Another alternative is to introduce
    augmentation techniques, as the data augmentation techniques used thus far do
    not create real samples. Therefore, new data augmentation techniques must be developed
    that create new samples from existing samples that represent real samples. Another
    alternative to deal with the problem is to introduce CNN models that are expressive
    but involve fewer learnable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, class imbalance is another challenge in datasets; in medical imaging,
    in general, and fundus images, in particular, the number of DR cases is much less
    than in normal cases. Furthermore, the quantities of images with different DR
    complications and DR lesions are different, and this difference in some cases
    is significant and adds bias for specific classes during the training of deep
    models. Large-scale retinal screening processes around the world lead to huge
    datasets of fundus images; however, most of the images are normal and do not contain
    any suspicious symptoms or lesions. Developing deep-learning strategies in dealing
    with this class imbalance is another essential area of research. Data augmentation
    has been used in some studies – such as [[112](#bib.bib112), [105](#bib.bib105),
    [127](#bib.bib127), [128](#bib.bib128)] – to tackle the class imbalance problem,
    but these data augmentation techniques mostly use geometric transformations and
    only create rotated and scaled samples, and do not introduce samples with lesions
    having morphological variations. More sophisticated data augmentation techniques
    that create heterogeneous samples while preserving prognostic characteristics
    of fundus images must be introduced, and one possible direction is to explore
    Generative Adversarial Networks (GANs) [[129](#bib.bib129)].
  prefs: []
  type: TYPE_NORMAL
- en: A principal issue in fundoscopy is the lack of uniformity among fundus images,
    i.e. the images being captured under different conditions. Fundus images usually
    suffer from the problem of illumination variation due to non-uniform diffusion
    of light in the retina; the shape of a retina is close to a sphere, which prevents
    the retina’s light incident from being reflected uniformly. Another common problem
    with respect to illumination is related to the angle at which light is incident
    on the retina; the angle at which the image is taken is not always the same. This
    can be confirmed by observing that the optic nerve does not maintain a specific
    position in the entire database. Another problem related to capturing the fundus
    image is that in some cases, the image is out of focus. In addition, fundus images
    are not always captured with the same resolution and camera. There is also the
    problem of pigmentation reflected by the iris.
  prefs: []
  type: TYPE_NORMAL
- en: One way to deal with these problems is to add a preprocessing stage in deep-learning
    methods. Alternatively, a robust approach is to design deep models such as Generative
    Adversarial Networks (GANs) [[130](#bib.bib130)] so that they automatically detect
    and correct these image artifacts. The bottleneck herein is to develop a huge
    annotated dataset that captures all different types of image artifacts. Alternatively,
    sophisticated data augmentation techniques must be introduced, augmenting an existing
    dataset with images having different artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: The methods developed so far are based on the assumption that the input image
    is a retinal image. However, the input image might not be a true retinal image
    or tempered retinal image; with the development of user-friendly image editing
    software it is easy to tamper an image. As such first of all an intelligent computer
    aided method must first of all identify whether the input image is a real and
    authentic retinal image.
  prefs: []
  type: TYPE_NORMAL
- en: Ophthalmologists usually prefer to use pupil dilating drops (mydriasis) for
    better view of the retina and more field to evaluate if there is peripheral DR.
    Using fundus photography can be with mydriasis or without depending on the field
    of camera lens used. Optos camera can capture up to 200^∘ of the retina in one
    shot and without mydriasis, while other cameras like Topcon and Nikon (which is
    used usually to screen DR as it has better image quality and color) can capture
    30-50^∘ which is a limitation of the system if we capture one image. Lawrence
    [[131](#bib.bib131)] conducted a study to compare mydriasis and non-mydriasis
    with single or multiple shots to clarify how sensitive is mydriasis in DR screening.
    He showed that mydriasis with multiple shots is more sensitive and specific compared
    to in non-mydriasis group with single shot. For that reason, the datasets must
    be developed with mydriasis and multiple shots to avoid the ungradable image.
    The rate of ungradable images in the general ranges from 7-17% [[132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136)].
  prefs: []
  type: TYPE_NORMAL
- en: For translational effect, AI and DL techniques should be developed in consultation
    with practicing ophthalmologists and must be validated in “real-world” DR screening
    programs where fundus images have different qualities (e.g. cataract, poor pupil
    dilation, poor contrast/focus), in patient samples of different ethnicity (i.e.
    different fundi pigmentation) and systemic control (poor and good control) [[137](#bib.bib137)].
  prefs: []
  type: TYPE_NORMAL
- en: The benchmark datasets have been used for the evaluation of different methods
    reviewed here. However, there is a variability in the grading by the human graders
    and different screening programs differ in local protocols. The benchmark datasets
    used in the reviewed methods does not follow a standard and as such the methods
    developed and tested using these datasets might not work in the clinical settings.
    As the performance of an intelligent computer aided method depends on the dataset
    that is used to train it, it necessitates the need of the development of new datasets
    keeping in view the procedures which are adopted in DR screening programs. Because
    of the possibility of the variability in grading by human graders while using
    different classification systems, it is advocated to standardize the use of one
    classification system for the development of the datasets and to use images that
    are graded and agreed on by at least 3 different graders. Further, the datasets
    must be statistically analyzed to determine the accuracy of grading [[138](#bib.bib138)].
    In addition, there is a classification introduced by the early treatment of diabetic
    retinopathy study (ETDRS) [[22](#bib.bib22)] for diabetic maculopathy. It must
    be taken into account for developing dataset for diabetic maculopathy.
  prefs: []
  type: TYPE_NORMAL
- en: Screening different races is a limitation of DR screening systems. Ting et al.
    [[95](#bib.bib95)] addressed this issue. However, it is not enough, further research
    is needed on this issue. From our personal experience, we observed that some DL
    algorithms developed on western populations were not able to detect some significant
    lesions in Saudi population. We noticed that as middle eastern or darkly skinned
    people may have more melanocytes in their retinal pigmented epithelium (RPE),
    which forms the most outer layer of the retina. Darker retina obscures some vascular
    changes as compared to the light colored retina. This limitation was noticed when
    we started using DL system which was developed using a dataset from light colored
    retinas. An important challenge to adoption of an algorithm is that it must be
    validated on larger patient cohorts under different settings and conditions. The
    performance of a screening software varies with the prevalence of the condition
    being screened. The prevalence of DR varies and is low in some communities and
    ethnic groups and higher in others (e.g. Hispanics, African Americans). It is
    important to understand the performance characteristics in these populations.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the reasons explained above, different datasets of fundus images created
    for benchmarking DR diagnosis methods are heterogeneous, and a deep-learning-based
    method gives good performance when trained and tested using the same dataset.
    For robustness, it is necessary that a deep-learning-based method gives satisfactory
    performance across different datasets. There are very few methods that have been
    tested across datasets. As real clinical settings can be forced to match the conditions
    under which a particular dataset was captured for developing a DR diagnosis method,
    robust deep-learning methods must be developed to give satisfactory performance
    in cross-database evaluation, i.e. trained with one dataset and tested with another.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Diabetic retinopathy is a complication of diabetes that damages the retina,
    causing vision problems. Diabetes harms the retinal blood vessels and leads to
    dangerous consequences, such as blindness. DR is preventable, and to avoid vision
    loss, early detection is important. Conventional methods for detecting DR biomarkers
    and lesions are based on hand-engineered features. The advent of deep learning
    has opened the way to design and develop more robust and accurate methods for
    detecting and diagnosing different DR complications, and deep learning has been
    employed to develop many methods for retinal blood vessel segmentation, OD detection
    and segmentation, detection and classification of different DR lesions, and the
    detection of referable DR. First, we have given an overview of different DR biomarkers
    and lesions, different tasks related to DR diagnosis, and the general framework
    of these tasks. Then we have given an overview of datasets that have been developed
    for research on DR diagnosis and performance metrics commonly used for evaluation.
    After that, we give an overview of deep-learning architectures that have been
    employed for designing DR diagnosis methods. After providing the necessary background,
    we then reviewed deep-learning-based methods that have been proposed for retinal
    blood vessels segmentation, OD detection and segmentation, detection of various
    DR lesions such as EXs, MAs, HMs and referable DR, highlighted their pro and cones
    and discussed their overall performance and compared them with state-of-the-art
    traditional methods based on hand–engineered features. In general, the deep-learning
    approach outperforms the traditional approach based on hand-engineered feature
    extraction techniques. At the end, we highlighted the gaps and weakness of the
    existing deep-learning-based DR diagnosis methods and presented potential future
    directions for research. This review gives a comprehensive view of state-of-the-art
    deep-learning-based methods related to DR diagnosis and will help researchers
    to conduct further research on this problem.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mookiah et al. [2013] Muthu Rama Krishnan Mookiah, U Rajendra Acharya, Chua Kuang
    Chua, Choo Min Lim, EYK Ng, and Augustinus Laude. Computer-aided diagnosis of
    diabetic retinopathy: A review. *Computers in biology and medicine*, 43(12):2136–2155,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Faust et al. [2012] Oliver Faust, Rajendra Acharya, Eddie Yin-Kwee Ng, Kwan-Hoong
    Ng, and Jasjit S Suri. Algorithms for the automated detection of diabetic retinopathy
    using digital fundus images: a review. *Journal of medical systems*, 36(1):145–157,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joshi and Karule [2018] Shilpa Joshi and PT Karule. A review on exudates detection
    methods for diabetic retinopathy. *Biomedicine & Pharmacotherapy*, 97:1454–1460,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mansour [2017] Romany F Mansour. Evolutionary computing enriched computer-aided
    diagnosis system for diabetic retinopathy: A survey. *IEEE reviews in biomedical
    engineering*, 10:334–349, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Almotiri et al. [2018] Jasem Almotiri, Khaled Elleithy, and Abdelrahman Elleithy.
    Retinal vessels segmentation techniques and algorithms: A survey. *Applied Sciences*,
    8(2):155, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Almazroa et al. [2015] Ahmed Almazroa, Ritambhar Burman, Kaamran Raahemifar,
    and Vasudevan Lakshminarayanan. Optic disc and optic cup segmentation methodologies
    for glaucoma image detection: a survey. *Journal of ophthalmology*, 2015, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thakur and Juneja [2018] Niharika Thakur and Mamta Juneja. Survey on segmentation
    and classification approaches of optic cup and optic disc for diagnosis of glaucoma.
    *Biomedical Signal Processing and Control*, 42:162–189, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Group et al. [1991] Early Treatment Diabetic Retinopathy Study Research Group
    et al. Grading diabetic retinopathy from stereoscopic color fundus photographs—an
    extension of the modified airlie house classification: Etdrs report number 10.
    *Ophthalmology*, 98(5):786–806, 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harney [2006] Fiona Harney. Diabetic retinopathy. *Medicine*, 34(3):95–98, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McLeod [2005] D McLeod. Why cotton wool spots should not be regarded as retinal
    nerve fibre layer infarcts. *British journal of ophthalmology*, 89(2):229–237,
    2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Akram et al. [2014] M Usman Akram, Shehzad Khalid, Anam Tariq, Shoab A Khan,
    and Farooque Azam. Detection and classification of retinal lesions for grading
    of diabetic retinopathy. *Computers in biology and medicine*, 45:161–171, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee and Cheng [1994] Tien-You Lee and HD Cheng. Parallel grading of venous beading
    on transputer. In *Proceedings of 1994 20th Annual Northeast Bioengineering Conference*,
    pages 54–58\. IEEE, 1994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patz [1980] Arnall Patz. Studies on retinal neovascularization. friedenwald
    lecture. *Investigative ophthalmology & visual science*, 19(10):1133–1138, 1980.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Diabetic retinopathy. [https://www.nhs.uk/conditions/diabetic-retinopathy/stages/](https://www.nhs.uk/conditions/diabetic-retinopathy/stages/).
    Accessed: 2018-01-08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Group et al. [1987] Early Treatment Diabetic Retinopathy Study Research Group
    et al. Treatment techniques and clinical guidelines for photocoagulation of diabetic
    macular edema: Early treatment diabetic retinopathy study report number 2. *Ophthalmology*,
    94(7):761–774, 1987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sopharak et al. [2008] Akara Sopharak, Bunyarit Uyyanonvara, Sarah Barman, and
    Thomas H Williamson. Automatic detection of diabetic retinopathy exudates from
    non-dilated retinal images using mathematical morphology methods. *Computerized
    medical imaging and graphics*, 32(8):720–727, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jonas et al. [1988] Jost B Jonas, Gabriele Ch Gusek, and Gottfried OH Naumann.
    Optic disk morphometry in high myopia. *Graefe’s archive for clinical and experimental
    ophthalmology*, 226(6):587–590, 1988.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joshi et al. [2011] Gopal Datt Joshi, Jayanthi Sivaswamy, and SR Krishnadas.
    Optic disk and cup segmentation from monocular color retinal images for glaucoma
    assessment. *IEEE transactions on medical imaging*, 30(6):1192–1205, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Acharya et al. [2009] Udyavara R Acharya, Choo M Lim, E Yin Kwee Ng, Caroline
    Chee, and Toshiyo Tamura. Computer-based detection of diabetes retinopathy stages
    using digital fundus images. *Proceedings of the institution of mechanical engineers,
    part H: journal of engineering in medicine*, 223(5):545–553, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fleming et al. [2010] Alan D Fleming, Keith A Goatman, Sam Philip, Graeme J
    Williams, Gordon J Prescott, Graham S Scotland, Paul McNamee, Graham P Leese,
    William N Wykes, Peter F Sharp, et al. The role of haemorrhage and exudate detection
    in automated grading of diabetic retinopathy. *British Journal of Ophthalmology*,
    94(6):706–711, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Diabetic retinal screening, grading, monitoring and referral guidance.
    [https://www.health.govt.nz/publication/diabetic-retinal-screening-grading-monitoring-and-referral-guidance](https://www.health.govt.nz/publication/diabetic-retinal-screening-grading-monitoring-and-referral-guidance).
    Accessed: 2019-05-01.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kanski [2009] Jack J Kanski. *Clinical ophthalmology: a synopsis*. Elsevier
    Health Sciences, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zachariah et al. [2015] Sonia Zachariah, William Wykes, and David Yorston. Grading
    diabetic retinopathy (dr) using the scottish grading protocol. *Community eye
    health*, 28(92):72, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'dr- [2015] Diabetic retinopathy (dr): management and referral. In *Community
    Eye Health*, pages 70–71, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seoud et al. [2016] Lama Seoud, Thomas Hurtut, Jihed Chelbi, Farida Cheriet,
    and JM Pierre Langlois. Red lesion detection using dynamic shape features for
    diabetic retinopathy screening. *IEEE transactions on medical imaging*, 35(4):1116–1126,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Litjens et al. [2017] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud
    Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen AWM van der
    Laak, Bram van Ginneken, and Clara I Sánchez. A survey on deep learning in medical
    image analysis. *Medical image analysis*, 42:60–88, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gondal et al. [2017] W. Gondal, J. M. Köhler, R. Grzeszick, G. Fink, and M. Hirsch.
    Weakly-supervised localization of diabetic retinopathy lesions in retinal fundus
    images. In *IEEE International Conference on Image Processing (ICIP 207)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2016] Qiaoliang Li, Bowei Feng, LinPei Xie, Ping Liang, Huisheng
    Zhang, and Tianfu Wang. A cross-modality learning approach for vessel segmentation
    in retinal images. *IEEE transactions on medical imaging*, 35(1):109–118, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arunkumar and Karthigaikumar [2017] R Arunkumar and P Karthigaikumar. Multi-retinal
    disease classification by reduced deep learning features. *Neural Computing and
    Applications*, 28(2):329–334, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Messidor dataset. [http://www.adcis.net/en/Download-Third-Party/Messidor.html](http://www.adcis.net/en/Download-Third-Party/Messidor.html).
    Accessed: 2018-01-08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] E-ophtha. [http://www.adcis.net/en/Download-Third-Party/E-Ophtha.html](http://www.adcis.net/en/Download-Third-Party/E-Ophtha.html).
    Accessed: 2018-01-08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Kaggle dataset. [https://www.kaggle.com/c/diabetic-retinopathy-detection/data](https://www.kaggle.com/c/diabetic-retinopathy-detection/data).
    Accessed: 2018-01-08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'dri [a] Drive dataset. [https://www.isi.uu.nl/Research/Databases/DRIVE/](https://www.isi.uu.nl/Research/Databases/DRIVE/),
    a. Accessed: 2018-01-08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoover et al. [2000] AD Hoover, Valentina Kouznetsova, and Michael Goldbaum.
    Locating blood vessels in retinal images by piecewise threshold probing of a matched
    filter response. *IEEE Transactions on Medical imaging*, 19(3):203–210, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kälviäinen and Uusitalo [2007] RVJPH Kälviäinen and H Uusitalo. Diaretdb1 diabetic
    retinopathy database and evaluation protocol. In *Medical Image Understanding
    and Analysis*, volume 2007, page 61\. Citeseer, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Diaretdb1 dataset. [http://www.it.lut.fi/project/imageret/diaretdb1/](http://www.it.lut.fi/project/imageret/diaretdb1/).
    Accessed: 2018-01-08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Chase dataset. [http://www.chasestudy.ac.uk/](http://www.chasestudy.ac.uk/).
    Accessed: 2018-02-01.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prentasic et al. [2013] Pavle Prentasic, Sven Loncaric, Zoran Vatavuk, Goran
    Bencic, Marko Subasic, Tomislav Petkovic, Lana Dujmovic, Maja Malenica-Ravlic,
    Nikolina Budimlija, and Raseljka Tadic. Diabetic retinopathy image database (dridb):
    a new database for diabetic retinopathy screening programs research. In *Image
    and Signal Processing and Analysis (ISPA), 2013 8th International Symposium on*,
    pages 711–716\. IEEE, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2010] Zhuo Zhang, Feng Shou Yin, Jiang Liu, Wing Kee Wong, Ngan Meng
    Tan, Beng Hai Lee, Jun Cheng, and Tien Yin Wong. Origa-light: An online retinal
    fundus image database for glaucoma analysis and research. In *Engineering in Medicine
    and Biology Society (EMBC), 2010 Annual International Conference of the IEEE*,
    pages 3065–3068\. IEEE, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sng et al. [2012] Chelvin C Sng, Li-Lian Foo, Ching-Yu Cheng, John C Allen,
    Mingguang He, Gita Krishnaswamy, Monisha E Nongpiur, David S Friedman, Tien Y
    Wong, and Tin Aung. Determinants of anterior chamber depth: the singapore chinese
    eye study. *Ophthalmology*, 119(6):1143–1150, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Nih areds dataset. [https://www.nih.gov/news-events/news-releases/nih-adds-first-images-major-research-database](https://www.nih.gov/news-events/news-releases/nih-adds-first-images-major-research-database).
    Accessed: 2018-02-01.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-Diri et al. [2008] Bashir Al-Diri, Andrew Hunter, David Steel, Maged Habib,
    Taghread Hudaib, and Simon Berry. A reference data set for retinal vessel profiles.
    In *Engineering in Medicine and Biology Society, 2008\. EMBS 2008\. 30th Annual
    International Conference of the IEEE*, pages 2262–2265. IEEE, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Eyepacs dataset. [http://www.eyepacs.com/eyepacssystem/](http://www.eyepacs.com/eyepacssystem/).
    Accessed: 2018-03-01.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fumero et al. [2011] Francisco Fumero, Silvia Alayón, JL Sanchez, J Sigut,
    and M Gonzalez-Hernandez. Rim-one: An open retinal image database for optic nerve
    evaluation. In *Computer-Based Medical Systems (CBMS), 2011 24th International
    Symposium on*, pages 1–6\. IEEE, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sivaswamy et al. [2014] Jayanthi Sivaswamy, SR Krishnadas, Gopal Datt Joshi,
    Madhulika Jain, and A Ujjwaft Syed Tabish. Drishti-gs: Retinal image dataset for
    optic nerve head (onh) segmentation. In *Biomedical Imaging (ISBI), 2014 IEEE
    11th International Symposium on*, pages 53–56\. IEEE, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Aria dataset. [http://www.eyecharity.com/aria_online.html](http://www.eyecharity.com/aria_online.html).
    Accessed: 2018-02-28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'dri [b] drion dataset. [http://www.ia.uned.es/~ejcarmona/DRIONS-DB.html](http://www.ia.uned.es/~ejcarmona/DRIONS-DB.html),
    b. Accessed: 2018-04-30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Seed-db. [https://www.seri.com.sg/key-programmes/singapore-epidemiology-of-eye-diseases-seed/](https://www.seri.com.sg/key-programmes/singapore-epidemiology-of-eye-diseases-seed/).
    Assessed on 2018-08-08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decencière et al. [2014] Etienne Decencière, Xiwei Zhang, Guy Cazuguel, Bruno
    Lay, Béatrice Cochener, Caroline Trone, Philippe Gain, Richard Ordonez, Pascale
    Massin, Ali Erginay, et al. Feedback on a publicly distributed image database:
    the messidor database. *Image Analysis & Stereology*, 33(3):231–234, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitchell [2004] Tom M Mitchell. The role of unlabeled data in supervised learning.
    In *Language, Knowledge, and Representation*, pages 103–111. Springer, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shankaranarayana et al. [2017] Sharath M Shankaranarayana, Keerthi Ram, Kaushik
    Mitra, and Mohanasankar Sivaprakasam. Joint optic disc and cup segmentation using
    fully convolutional and adversarial networks. In *Fetal, Infant and Ophthalmic
    Medical Image Analysis*, pages 168–176\. Springer, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srivastava et al. [2015] Ruchir Srivastava, Jun Cheng, Damon WK Wong, and Jiang
    Liu. Using deep learning for robustness to parapapillary atrophy in optic disc
    segmentation. In *Biomedical Imaging (ISBI), 2015 IEEE 12th International Symposium
    on*, pages 768–771\. IEEE, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou et al. [2004] Kelly H Zou, Simon K Warfield, Aditya Bharatha, Clare MC
    Tempany, Michael R Kaus, Steven J Haker, William M Wells, Ferenc A Jolesz, and
    Ron Kikinis. Statistical validation of image segmentation quality based on a spatial
    overlap index1: scientific reports. *Academic radiology*, 11(2):178–189, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2015] Xueliang Zhang, Xuezhi Feng, Pengfeng Xiao, Guangjun He,
    and Liujun Zhu. Segmentation quality evaluation using region-based precision and
    recall measures for remote sensing images. *ISPRS Journal of Photogrammetry and
    Remote Sensing*, 102:73–84, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [1998] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    Gradient-based learning applied to document recognition. *Proceedings of the IEEE*,
    86(11):2278–2324, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. [2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    Imagenet classification with deep convolutional neural networks. In *Advances
    in neural information processing systems*, pages 1097–1105, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman [2014] Karen Simonyan and Andrew Zisserman. Very deep
    convolutional networks for large-scale image recognition. *arXiv preprint arXiv:1409.1556*,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. [2015] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich,
    et al. Going deeper with convolutions. Cvpr, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 770–778, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tajbakhsh et al. [2016] Nima Tajbakhsh, Jae Y Shin, Suryakanth R Gurudu, R Todd
    Hurst, Christopher B Kendall, Michael B Gotway, and Jianming Liang. Convolutional
    neural networks for medical image analysis: Full training or fine tuning? *IEEE
    transactions on medical imaging*, 35(5):1299–1312, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erhan et al. [2010] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine
    Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training
    help deep learning? *Journal of Machine Learning Research*, 11(Feb):625–660, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. [2015] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
    convolutional networks for semantic segmentation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 3431–3440, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton and Salakhutdinov [2006] Geoffrey E Hinton and Ruslan R Salakhutdinov.
    Reducing the dimensionality of data with neural networks. *science*, 313(5786):504–507,
    2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liou et al. [2014] Cheng-Yuan Liou, Wei-Chen Cheng, Jiun-Wei Liou, and Daw-Ran
    Liou. Autoencoder for words. *Neurocomputing*, 139:84–96, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maji et al. [2015] Debapriya Maji, Anirban Santara, Sambuddha Ghosh, Debdoot
    Sheet, and Pabitra Mitra. Deep neural network and random forest hybrid architecture
    for learning to detect retinal vessels in fundus images. In *Engineering in Medicine
    and Biology Society (EMBC), 2015 37th Annual International Conference of the IEEE*,
    pages 3029–3032\. IEEE, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. [2010] Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černockỳ,
    and Sanjeev Khudanpur. Recurrent neural network based language model. In *Eleventh
    Annual Conference of the International Speech Communication Association*, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. [2017] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru
    Erhan. Show and tell: Lessons learned from the 2015 mscoco image captioning challenge.
    *IEEE transactions on pattern analysis and machine intelligence*, 39(4):652–663,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maji et al. [2016] Debapriya Maji, Anirban Santara, Pabitra Mitra, and Debdoot
    Sheet. Ensemble of deep convolutional neural networks for learning to detect retinal
    vessels in fundus images. *arXiv preprint arXiv:1603.04833*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liskowski and Krawiec [2016] Paweł Liskowski and Krzysztof Krawiec. Segmenting
    retinal blood vessels with deep neural networks. *IEEE transactions on medical
    imaging*, 35(11):2369–2380, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maninis et al. [2016] Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbeláez,
    and Luc Van Gool. Deep retinal image understanding. In *International Conference
    on Medical Image Computing and Computer-Assisted Intervention*, pages 140–148\.
    Springer, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2016] Aaron Wu, Ziyue Xu, Mingchen Gao, Mario Buty, and Daniel J
    Mollura. Deep vessel tracking: A generalized probabilistic approach via deep learning.
    In *Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium on*, pages
    1363–1367\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dasgupta and Singh [2017] Avijit Dasgupta and Sonam Singh. A fully convolutional
    neural network based structured prediction approach towards the retinal vessel
    segmentation. In *Biomedical Imaging (ISBI 2017), 2017 IEEE 14th International
    Symposium on*, pages 248–251\. IEEE, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. [2017] Jen Hong Tan, U Rajendra Acharya, Sulatha V Bhandary, Kuang Chua
    Chua, and Sobha Sivaprasad. Segmentation of optic disc, fovea and retinal vasculature
    using a single convolutional neural network. *Journal of Computational Science*,
    20:70–79, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. [2016a] Huazhu Fu, Yanwu Xu, Damon Wing Kee Wong, and Jiang Liu. Retinal
    vessel segmentation via deep learning network and fully-connected conditional
    random fields. In *Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium
    on*, pages 698–701\. IEEE, 2016a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mo and Zhang [2017] Juan Mo and Lei Zhang. Multi-level deep supervised networks
    for retinal vessel segmentation. *International journal of computer assisted radiology
    and surgery*, 12(12):2181–2193, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roy and Sheet [2015] Abhijit Guha Roy and Debdoot Sheet. Dasa: Domain adaptation
    in stacked autoencoders using systematic dropout. In *Pattern Recognition (ACPR),
    2015 3rd IAPR Asian Conference on*, pages 735–739\. IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lahiri et al. [2016] Avisek Lahiri, Abhijit Guha Roy, Debdoot Sheet, and Prabir Kumar
    Biswas. Deep neural ensemble for retinal vessel segmentation in fundus images
    towards achieving label-free angiography. In *Engineering in Medicine and Biology
    Society (EMBC), 2016 IEEE 38th Annual International Conference of the*, pages
    1340–1343\. IEEE, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2016b] Huazhu Fu, Yanwu Xu, Stephen Lin, Damon Wing Kee Wong, and
    Jiang Liu. Deepvessel: Retinal vessel segmentation via deep learning and conditional
    random field. In *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, pages 132–139\. Springer, 2016b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lim et al. [2015] Gilbert Lim, Yuan Cheng, Wynne Hsu, and Mong Li Lee. Integrated
    optic disc and cup segmentation with deep learning. pages 162–169, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. [2016] Yundi Guo, Beiji Zou, Zailiang Chen, Qi He, Qing Liu, and
    Rongchang Zhao. Optic cup segmentation using large pixel patch based cnns. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sevastopolsky [2017] Artem Sevastopolsky. Optic disc and cup segmentation methods
    for glaucoma detection with modification of u-net convolutional neural network.
    *Pattern Recognition and Image Analysis*, 27(3):618–624, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. [2015] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    U-net: Convolutional networks for biomedical image segmentation. In *International
    Conference on Medical image computing and computer-assisted intervention*, pages
    234–241\. Springer, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zilly et al. [2015] Julian G Zilly, Joachim M Buhmann, and Dwarikanath Mahapatra.
    Boosting convolutional filters with entropy sampling for optic cup and disc image
    segmentation from fundus images. pages 136–143, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zilly et al. [2017] Julian Zilly, Joachim M Buhmann, and Dwarikanath Mahapatra.
    Glaucoma detection using entropy sampling and ensemble learning for automatic
    optic cup and disc segmentation. *Computerized Medical Imaging and Graphics*,
    55:28–41, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2018] Defeng Zhang, Weifang Zhu, Heming Zhao, Fei Shi, and Xinjian
    Chen. Automatic localization and segmentation of optical disk based on faster
    r-cnn and level set in fundus image. In *Medical Imaging 2018: Image Processing*,
    volume 10574, page 105741U. International Society for Optics and Photonics, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. [2018] Huazhu Fu, Jun Cheng, Yanwu Xu, Damon Wing Kee Wong, Jiang
    Liu, and Xiaochun Cao. Joint optic disc and cup segmentation based on multi-label
    deep network and polar transformation. *arXiv preprint arXiv:1801.00926*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niu et al. [2017] Di Niu, Peiyuan Xu, Cheng Wan, Jun Cheng, and Jiang Liu. Automatic
    localization of optic disc based on deep learning in fundus images. In *Signal
    and Image Processing (ICSIP), 2017 IEEE 2nd International Conference on*, pages
    208–212\. IEEE, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alghamdi et al. [2016] Hanan S Alghamdi, Hongying Lilian Tang, Saad A Waheeb,
    and Tunde Peto. Automatic optic disc abnormality detection in fundus images: a
    deep learning approach. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2017] Peiyuan Xu, Cheng Wan, Jun Cheng, Di Niu, and Jiang Liu. Optic
    disc detection via deep learning in fundus images. In *Fetal, Infant and Ophthalmic
    Medical Image Analysis*, pages 134–141\. Springer, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Foong et al. [2007] Athena WP Foong, Seang-Mei Saw, Jing-Liang Loo, Sunny Shen,
    Seng-Chee Loon, Mohamad Rosman, Tin Aung, Donald TH Tan, E Shyong Tai, and Tien Y
    Wong. Rationale and methodology for a population-based study of eye diseases in
    malay people: The singapore malay eye study (simes). *Ophthalmic epidemiology*,
    14(1):25–35, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abràmoff et al. [2016] Michael David Abràmoff, Yiyue Lou, Ali Erginay, Warren
    Clarida, Ryan Amelon, James C Folk, and Meindert Niemeijer. Improved automated
    detection of diabetic retinopathy on a publicly available dataset through integration
    of deep learning. *Investigative ophthalmology & visual science*, 57(13):5200–5206,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perdomo et al. [2016] Oscar Perdomo, Sebastian Otalora, Francisco Rodríguez,
    John Arevalo, and Fabio A González. A novel machine learning model based on exudate
    localization to detect diabetic macular edema. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burlina et al. [2016] Philippe Burlina, David E Freund, Neil Joshi, Y Wolfson,
    and Neil M Bressler. Detection of age-related macular degeneration via deep learning.
    In *Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium on*, pages
    184–188\. IEEE, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-Bander et al. [2016] Baidaa Al-Bander, Waleed Al-Nuaimy, Majid A Al-Taee,
    Bryan M Williams, and Yalin Zheng. Diabetic macular edema grading based on deep
    neural networks. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ting et al. [2017] Daniel Shu Wei Ting, Carol Yim-Lui Cheung, Gilbert Lim, Gavin
    Siew Wei Tan, Nguyen D Quang, Alfred Gan, Haslina Hamzah, Renata Garcia-Franco,
    Ian Yew San Yeo, Shu Yen Lee, et al. Development and validation of a deep learning
    system for diabetic retinopathy and related eye diseases using retinal images
    from multiethnic populations with diabetes. *Jama*, 318(22):2211–2223, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mo et al. [2018] Juan Mo, Lei Zhang, and Yangqin Feng. Exudate-based diabetic
    macular edema recognition in retinal images using cascaded deep residual networks.
    *Neurocomputing*, 290:161–171, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Hei-med dataset. [http://www.genenetwork.org/dbdoc/Eye_M2_0908_R.html](http://www.genenetwork.org/dbdoc/Eye_M2_0908_R.html).
    Accessed: 2018-11-02.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prentašić and Lončarić [2016] Pavle Prentašić and Sven Lončarić. Detection of
    exudates in fundus photographs using deep neural networks and anatomical landmark
    detection fusion. *Computer methods and programs in biomedicine*, 137:281–292,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perdomo et al. [2017] Oscar Perdomo, John Arevalo, and Fabio A González. Convolutional
    network to detect exudates in eye fundus images of diabetic subjects. In *12th
    International Symposium on Medical Information Processing and Analysis*, volume
    10160, page 101600T. International Society for Optics and Photonics, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] o o cnn solution. [https://www.kaggle.com/c/diabetic-retinopathydetection/discussion/15617](https://www.kaggle.com/c/diabetic-retinopathydetection/discussion/15617).
    Accessed: 2017-01-16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2016] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. Learning deep features for discriminative localization. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 2921–2929,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quellec et al. [2017] Gwenolé Quellec, Katia Charrière, Yassine Boudi, Béatrice
    Cochener, and Mathieu Lamard. Deep image mining for diabetic retinopathy screening.
    *Medical Image Analysis*, 39:178–193, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khojasteh et al. [2018] Parham Khojasteh, Leandro Aparecido Passos Júnior, Tiago
    Carvalho, Edmar Rezende, Behzad Aliahmad, João Paulo Papa, and Dinesh Kant Kumar.
    Exudate detection in fundus images using deeply-learnable features. *Computers
    in biology and medicine*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haloi [2015] Mrinal Haloi. Improved microaneurysm detection using deep neural
    networks. *arXiv preprint arXiv:1505.04424*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'van Grinsven et al. [2016] Mark JJP van Grinsven, Bram van Ginneken, Carel B
    Hoyng, Thomas Theelen, and Clara I Sánchez. Fast convolutional neural network
    training using selective data sampling: Application to hemorrhage detection in
    color fundus images. *IEEE transactions on medical imaging*, 35(5):1273–1284,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orlando et al. [2018] José Ignacio Orlando, Elena Prokofyeva, Mariana del Fresno,
    and Matthew B Blaschko. An ensemble deep learning based approach for red lesion
    detection in fundus images. *Computer methods and programs in biomedicine*, 153:115–127,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shan and Li [2016] Juan Shan and Lin Li. A deep learning method for microaneurysm
    detection in fundus images. In *Connected Health: Applications, Systems and Engineering
    Technologies (CHASE), 2016 IEEE First International Conference on*, pages 357–358\.
    IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulshan et al. [2016] Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe,
    Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom
    Madams, Jorge Cuadros, et al. Development and validation of a deep learning algorithm
    for detection of diabetic retinopathy in retinal fundus photographs. *Jama*, 316(22):2402–2410,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Colas et al. [2016] E Colas, A Besse, A Orgogozo, B Schmauch, N Meric, and E Besse.
    Deep learning approach for diabetic retinopathy screening. *Acta Ophthalmologica*,
    94(S256), 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Costa and Campilho [2017] Pedro Costa and Aurélio Campilho. Convolutional bag
    of words for diabetic retinopathy detection from eye fundus images. *IPSJ Transactions
    on Computer Vision and Applications*, 9(1):10, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pires et al. [2014] Ramon Pires, Herbert F Jelinek, Jacques Wainer, Eduardo
    Valle, and Anderson Rocha. Advancing bag-of-visual-words representations for lesion
    classification in retinal images. *PloS one*, 9(6):e96814, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pratt et al. [2016] Harry Pratt, Frans Coenen, Deborah M Broadbent, Simon P
    Harding, and Yalin Zheng. Convolutional neural networks for diabetic retinopathy.
    *Procedia Computer Science*, 90:200–205, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gargeya and Leng [2017] Rishab Gargeya and Theodore Leng. Automated identification
    of diabetic retinopathy using deep learning. *Ophthalmology*, 124(7):962–969,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2017] Zhe Wang, Yanxin Yin, Jianping Shi, Wei Fang, Hongsheng
    Li, and Xiaogang Wang. Zoom-in-net: Deep mining lesions for diabetic retinopathy
    detection. In *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, pages 267–275\. Springer, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mansour [2018] Romany F Mansour. Deep-learning-based automatic computer-aided
    diagnosis system for diabetic retinopathy. *Biomedical Engineering Letters*, 8(1):41–57,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2018] Yi-Wei Chen, Tung-Yu Wu, Wing-Hung Wong, and Chen-Yi Lee.
    Diabetic retinopathy detection based on deep convolutional neural networks. In
    *2018 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, pages 1030–1034\. IEEE, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinidhi et al. [2017] Chetan L Srinidhi, P Aparna, and Jeny Rajan. Recent advancements
    in retinal vessel segmentation. *Journal of medical systems*, 41(4):70, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Villalobos-Castaldi et al. [2010] Fabiola M Villalobos-Castaldi, Edgardo M Felipe-Riverón,
    and Luis P Sánchez-Fernández. A fast, efficient and automated method to extract
    vessels from fundus images. *Journal of Visualization*, 13(3):263–270, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Condurache and Mertins [2012] Alexandru Paul Condurache and Alfred Mertins.
    Segmentation of retinal vessels with a hysteresis binary-classification paradigm.
    *Computerized Medical Imaging and Graphics*, 36(4):325–335, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aquino et al. [2010] Arturo Aquino, Manuel Emilio Gegúndez-Arias, and Diego
    Marín. Detecting the optic disc boundary in digital fundus images using morphological,
    edge detection, and feature extraction techniques. *IEEE transactions on medical
    imaging*, 29(11):1860–1869, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2012] Dongbo Zhang, Yao Yi, Xingyu Shang, and Yinghui Peng. Optic
    disc localization by projection with vessel distribution and appearance characteristics.
    In *Pattern Recognition (ICPR), 2012 21st International Conference on*, pages
    3176–3179\. IEEE, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sinha and Babu [2012] Neelam Sinha and R Venkatesh Babu. Optic disk localization
    using l 1 minimization. In *Image Processing (ICIP), 2012 19th IEEE International
    Conference on*, pages 2829–2832\. IEEE, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tjandrasa et al. [2012] Handayani Tjandrasa, Ari Wijayanti, and Nanik Suciati.
    Optic nerve head segmentation using hough transform and active contours. *Indonesian
    Journal of Electrical Engineering and Computer Science*, 10(3):531–536, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Massey et al. [2009] EM Massey, Andrew Hunter, James Lowell, and DH Steel. A
    robust lesion boundary segmentation algorithm using level set methods. In *World
    Congress on Medical Physics and Biomedical Engineering, September 7-12, 2009,
    Munich, Germany*, pages 304–307. Springer, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Antal and Hajdu [2012] Balint Antal and Andras Hajdu. An ensemble-based system
    for microaneurysm detection and diabetic retinopathy grading. *IEEE transactions
    on biomedical engineering*, 59(6):1720–1726, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neff et al. [2017] Thomas Neff, Christian Payer, Darko Štern, and Martin Urschler.
    Generative adversarial network based synthesis for supervised medical image segmentation.
    In *OAGM & ARW Joint Workshop 2017 on “Vision, Automation & Robotics”*. Verlag
    der Technischen Universität Graz, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worrall et al. [2016] Daniel E Worrall, Clare M Wilson, and Gabriel J Brostow.
    Automated retinopathy of prematurity case detection with convolutional neural
    networks. In *International Workshop on Large-Scale Annotation of Biomedical Data
    and Expert Label Synthesis*, pages 68–76\. Springer, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2015] Xiangyu Chen, Yanwu Xu, Damon Wing Kee Wong, Tien Yin Wong,
    and Jiang Liu. Glaucoma detection based on deep convolutional neural network.
    In *Engineering in Medicine and Biology Society (EMBC), 2015 37th Annual International
    Conference of the IEEE*, pages 715–718\. IEEE, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frid-Adar et al. [2018] Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai,
    Jacob Goldberger, and Hayit Greenspan. Gan-based synthetic medical image augmentation
    for increased cnn performance in liver lesion classification. *arXiv preprint
    arXiv:1803.01229*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schawinski et al. [2017] Kevin Schawinski, Ce Zhang, Hantian Zhang, Lucas Fowler,
    and Gokula Krishnan Santhanam. Generative adversarial networks recover features
    in astrophysical images of galaxies beyond the deconvolution limit. *Monthly Notices
    of the Royal Astronomical Society: Letters*, 467(1):L110–L114, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lawrence [2004] Mary Gilbert Lawrence. The accuracy of digital-video retinal
    imaging to screen for diabetic retinopathy: an analysis of two digital-video retinal
    imaging systems using standard stereoscopic seven-field photography and dilated
    clinical examination as reference standards. *Transactions of the American Ophthalmological
    Society*, 102:321, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Massin et al. [2003] P Massin, A Erginay, A Ben Mehidi, E Vicaut, G Quentel,
    Z Victor, M Marre, PJ Guillausseau, and A Gaudric. Evaluation of a new non-mydriatic
    digital camera for detection of diabetic retinopathy. *Diabetic medicine*, 20(8):635–641,
    2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szabó et al. [2015] Dorottya Szabó, Orsolya Fiedler, Anikó Somogyi, Gábor Márk
    Somfai, Zsolt Bíró, Veronika Ölvedy, Zsófia Hargitai, and János Németh. Telemedical
    diabetic retinopathy screening in hungary: a pilot programme. *Journal of telemedicine
    and telecare*, 21(3):167–173, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abdellaoui et al. [2016] M Abdellaoui, M Marrakchi, IA Benatiya, and H Tahri.
    Screening for diabetic retinopathy by non-mydriatic retinal camera in the region
    of fez. *Journal francais d’ophtalmologie*, 39(1):48–54, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siu et al. [1998] SC Siu, TC Ko, KW Wong, WN Chan, et al. Effectiveness of non-mydriatic
    retinal photography and direct ophthalmoscopy in detecting diabetic retinopathy.
    *Hong Kong Medical Journal*, pages 367–370, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chow et al. [2006] Sing-Pey Chow, Lloyd M Aiello, Jerry D Cavallerano, Paula
    Katalinic, Kristen Hock, Ann Tolson, Rita Kirby, Sven-Erik Bursell, and Lloyd Paul
    Aiello. Comparison of nonmydriatic digital retinal imaging versus dilated ophthalmic
    examination for nondiabetic eye disease in persons with diabetes. *Ophthalmology*,
    113(5):833–840, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wong and Bressler [2016] Tien Yin Wong and Neil M Bressler. Artificial intelligence
    with deep learning technology looks into diabetic retinopathy screening. *Jama*,
    316(22):2366–2367, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oke et al. [2016] JL Oke, IM Stratton, SJ Aldington, RJ Stevens, and Peter H
    Scanlon. The use of statistical methodology to determine the accuracy of grading
    within a diabetic retinopathy screening programme. *Diabetic Medicine*, 33(7):896–903,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
