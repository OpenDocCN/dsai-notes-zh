- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:36:42'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2310.06557] Data efficient deep learning for medical image analysis: A survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06557](https://ar5iv.labs.arxiv.org/html/2310.06557)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Data efficient deep learning for medical image analysis: A survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suruchi Kumari [suruchi_k@cs.iitr.ac.in](mailto:suruchi_k@cs.iitr.ac.in) Pravendra Singh
    [pravendra.singh@cs.iitr.ac.in](mailto:pravendra.singh@cs.iitr.ac.in) Department
    of Computer Science and Engineering, Indian Institute of Technology Roorkee, India
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The rapid evolution of deep learning has significantly advanced the field of
    medical image analysis. However, despite these achievements, the further enhancement
    of deep learning models for medical image analysis faces a significant challenge
    due to the scarcity of large, well-annotated datasets. To address this issue,
    recent years have witnessed a growing emphasis on the development of data-efficient
    deep learning methods. This paper conducts a thorough review of data-efficient
    deep learning methods for medical image analysis. To this end, we categorize these
    methods based on the level of supervision they rely on, encompassing categories
    such as *no supervision*, *inexact supervision*, *incomplete supervision*, *inaccurate
    supervision*, and *only limited supervision*. We further divide these categories
    into finer subcategories. For example, we categorize *inexact supervision* into
    *multiple instance learning* and *learning with weak annotations*. Similarly,
    we categorize *incomplete supervision* into *semi-supervised learning*, *active
    learning*, and *domain-adaptive learning* and so on. Furthermore, we systematically
    summarize commonly used datasets for data efficient deep learning in medical image
    analysis and investigate future research directions to conclude this survey.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data efficient deep learning , Medical image analysis , Inexact supervision
    , Incomplete supervision , Inaccurate supervision , Only limited supervision ,
    No supervision.\UseRawInputEncoding
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning has significantly influenced various medical fields, particularly
    medical imaging, with its influence expected to further expand [[1](#bib.bib1)].
    In the context of medical image analysis (MIA), deep learning methods have demonstrated
    remarkable performance across various tasks, including disease classification
    [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)], medical object
    detection [[6](#bib.bib6), [7](#bib.bib7)], ROI segmentation [[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)], and image registration [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]. Initially, supervised learning was widely
    adopted in MIA. Despite its success in numerous applications, the broader use
    of supervised models faces a significant challenge due to the typically small
    size of most medical datasets. Medical image datasets are often considerably smaller
    than standard computer vision datasets. The initial amount of available data is
    limited, and obtaining additional data is hindered by factors such as patient
    confidentiality and institutional policies. Furthermore, in many instances, only
    a small fraction of the images are annotated by domain experts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/798d7d9d633ce5ba0f9fcc4755341549.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Taxonomy of data efficient deep learning approaches for medical image
    analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, researchers rely on domain experts, such as radiologists or pathologists,
    to create task-specific annotations for image data. Labeling a sufficiently large
    dataset can be time-consuming [[15](#bib.bib15)]. For example, training deep learning
    systems for radiology, especially when involving 3D data, requires meticulous
    slice-by-slice annotations, which can be particularly time-intensive [[12](#bib.bib12)].
    Some research efforts have involved numerous experts in annotating extensive medical
    image datasets [[16](#bib.bib16), [17](#bib.bib17)]. However, such initiatives
    demand substantial financial and logistical resources, which are often not readily
    available across various domains. Other investigations have resorted to crowd-sourcing
    approaches for obtaining labels from non-experts [[18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20)]. Although this method may have potential in specific cases,
    its applicability is limited because non-experts typically cannot provide meaningful
    labels for most medical applications. To overcome these limitations, there is
    a growing trend among researchers to develop data-efficient deep learning approaches
    for medical image analysis. We broadly categorize these approaches into the following
    groups: no supervision, inexact supervision, incomplete supervision, inaccurate
    supervision, and limited supervision, as shown in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Data efficient deep learning for medical image analysis: A
    survey").'
  prefs: []
  type: TYPE_NORMAL
- en: This survey covers more than 250 papers, with the majority published in recent
    years (2020-2023). These papers span a diverse range of applications of deep learning
    in medical image analysis and have been presented in conference proceedings for
    MICCAI, EMBC, and ISBI, as well as various journals such as TMI, Medical Image
    Analysis, and Computers in Biology and Medicine, among others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several related review articles have already been published summarizing a few
    specific categories of data efficient learning in the domain of medical image
    analysis. Cheplygina et al. [[21](#bib.bib21)] provided an overview of semi-supervised
    learning, multiple instance learning, and transfer learning within the context
    of medical imaging, addressing both diagnostic and segmentation tasks. Meanwhile,
    Tajbakhsh et al. [[22](#bib.bib22)] explored numerous strategies for handling
    dataset limitations, such as cases involving scarce or weak annotations, with
    a particular focus on medical image segmentation. Chen et al. [[13](#bib.bib13)]
    present a summary of the latest developments in deep learning, encompassing supervised,
    unsupervised, and semi-supervised methodologies. More recently, Jin et al. [[23](#bib.bib23)]
    provide an overview of semi-supervised, self-supervised, multi-instance learning,
    active learning, and annotation-efficient techniques. However, it’s worth noting
    that their review does not delve into subjects such as domain-adaptive learning
    or few-shot learning, among others. Also, in the previously discussed surveys,
    their coverage is either restricted concerning data-efficient methods in MIA or
    not up to date with the current trends. To tackle this challenge, we undertake
    a systematic review of recent data-efficient methodologies, as outlined in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Data efficient deep learning for medical image analysis:
    A survey"). Our goal is to offer a thorough review of data-efficient learning
    methods in medical image analysis and outline future challenges. We also provide
    an overview of several widely used available datasets in the field of medical
    imaging, as illustrated in Table [1](#S1.T1b "Table 1 ‣ 1 Introduction ‣ Data
    efficient deep learning for medical image analysis: A survey"). The major contributions
    of our work can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the first survey paper that summarizes recent advances in data efficient
    deep learning for medical image analysis. Specifically, we present a comprehensive
    overview of more than 250 relevant papers to cover the recent progress.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We systematically categorize these methods into five distinct groups: incomplete
    supervision, no supervision, inaccurate supervision, inexact supervision, and
    only limited supervision.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we explore several potential future directions for further research
    and development for data-efficient deep learning methods in MIA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remainder of this survey is organized as follows. In Section [2](#S2 "2
    No supervision ‣ Data efficient deep learning for medical image analysis: A survey"),
    we delve into techniques falling under the category of No Supervision, which we
    further subdivide into Predictive Self-Supervision (Subsection [2.1](#S2.SS1 "2.1
    Predictive self-supervision ‣ 2 No supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")), Generative Self-Supervision (Subsection [2.2](#S2.SS2
    "2.2 Generative self-supervision ‣ 2 No supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")), Contrastive Self-Supervision (Subsection [2.3](#S2.SS3
    "2.3 Contrastive self-supervision ‣ 2 No supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")), and Multi-Self Supervised Learning (Subsection [2.4](#S2.SS4
    "2.4 Multi-self supervised learning: combining multiple SSL pretext tasks into
    one framework ‣ 2 No supervision ‣ Data efficient deep learning for medical image
    analysis: A survey")). In Section [3](#S3 "3 Inexact supervision ‣ Data efficient
    deep learning for medical image analysis: A survey"), we explore Inexact Supervision
    techniques, further classified into Multiple Instance Learning (Subsection [3.1](#S3.SS1
    "3.1 Multiple instance learning ‣ 3 Inexact supervision ‣ Data efficient deep
    learning for medical image analysis: A survey")) and Learning with Weak Annotations
    (Subsection [3.2](#S3.SS2 "3.2 Learning with weak annotations ‣ 3 Inexact supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). Section [4](#S4
    "4 Incomplete supervision ‣ Data efficient deep learning for medical image analysis:
    A survey") is dedicated to Incomplete Supervision methods, which we further categorize
    as Semi-Supervised Learning (Subsection [4.1](#S4.SS1 "4.1 Semi-supervised learning
    ‣ 4 Incomplete supervision ‣ Data efficient deep learning for medical image analysis:
    A survey")), Active Learning (Subsection [4.2](#S4.SS2 "4.2 Active learning ‣
    4 Incomplete supervision ‣ Data efficient deep learning for medical image analysis:
    A survey")), and Domain-Adaptive Learning (Subsection [4.3](#S4.SS3 "4.3 Domain-adaptive
    learning ‣ 4 Incomplete supervision ‣ Data efficient deep learning for medical
    image analysis: A survey")). Similarly, Section [5](#S5 "5 Inaccurate supervision
    ‣ Data efficient deep learning for medical image analysis: A survey") deals with
    Inaccurate Supervision techniques, which we further categorize as Robust Loss
    Design (Subsection [5.1](#S5.SS1 "5.1 Robust loss design ‣ 5 Inaccurate supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")), Data reweighting
    (Subsection [5.2](#S5.SS2 "5.2 Data re-weighting ‣ 5 Inaccurate supervision ‣
    Data efficient deep learning for medical image analysis: A survey")), and Training
    procedures (Subsection [5.3](#S5.SS3 "5.3 Training procedures ‣ 5 Inaccurate supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). Moving
    on to Section [6](#S6 "6 Only limited supervision ‣ Data efficient deep learning
    for medical image analysis: A survey"), we focus on Only Limited Supervision techniques,
    which are classified into Data Augmentation (Subsection [6.1](#S6.SS1 "6.1 Data
    Augmentation ‣ 6 Only limited supervision ‣ Data efficient deep learning for medical
    image analysis: A survey")), Few-Shot Learning (Subsection [6.2](#S6.SS2 "6.2
    Few shot learning ‣ 6 Only limited supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")), and Transfer Learning (Subsection [6.3](#S6.SS3
    "6.3 Transfer learning ‣ 6 Only limited supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")). Additionally, we outline potential future
    research directions in Section [7](#S7 "7 Future research scope ‣ Data efficient
    deep learning for medical image analysis: A survey") before concluding this survey
    in Section [8](#S8 "8 Conclusion ‣ Data efficient deep learning for medical image
    analysis: A survey"). The structural overview of this survey is presented in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Data efficient deep learning for medical image analysis:
    A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Commonly used datasets for data efficient deep learning in medical
    image analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Organ | Types | Task | Description | Link |'
  prefs: []
  type: TYPE_TB
- en: '| JSRT Database (2000) [[24](#bib.bib24)] | Brain | Chest radiographs | Classification
    | The database includes 154 conventional chest radiographs with a lung nodule
    (100 malignant and 54 benign nodules) and 93 radiographs without a nodule. | [http://db.jsrt.or.jp/eng.php](http://db.jsrt.or.jp/eng.php)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ADNI-3 dataset [[25](#bib.bib25), [26](#bib.bib26)] | Brain | MRI, PET, fMRI,
    etc.. | Alzheimer’s Disease identification | 697 subjects from ADNI-2 and additional
    133 CN, 151 amnestic MCI and 87 AD subjects were added (371 total new subjects)
    | [https://adni.loni.usc.edu/adni-3/](https://adni.loni.usc.edu/adni-3/) |'
  prefs: []
  type: TYPE_TB
- en: '| BraTS 2012 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: 30 datasets(pre- and post-therapy images) Synthetic data: 50 simulated
    datasets; Test: 15 clinical and 15 simulated datasets | [http://www.imm.dtu.dk/projects/BRATS2012/data.html](http://www.imm.dtu.dk/projects/BRATS2012/data.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BraTS 2013 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: Clinical dataset from BraTS12 training data; Test: 15 clinical test
    images from BraTS12 and 10 new test dataset | [https://www.smir.ch/BRATS/Start2013#!#download](https://www.smir.ch/BRATS/Start2013#!#download)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BraTS 2014 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: 200 datasets from both BraTS12 and BraTS13 and TCIA [16] including
    longitudinal datasets; Test: 38 unseen datasets from both BraTS12 and BraTS13
    test datasets and TCIA | [https://www.smir.ch/BRATS/Start2014](https://www.smir.ch/BRATS/Start2014)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BraTS 2015 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: Identical to the BraTS14 training dataset; Test: 53 unseen datasets
    from both BraTS12 and BraTS13 test datasets and TCIA | [https://www.smir.ch/BRATS/Start2015](https://www.smir.ch/BRATS/Start2015)
    |'
  prefs: []
  type: TYPE_TB
- en: '| TCIA (2015) | Brain | MR images | Segmentation | 20 subjects with primary
    newly diagnosed glioblastoma who were treated with surgery and standard concomitant
    chemo-radiation therapy (CRT) followed by adjuvant chemotherapy. | [https://www.cancerimagingarchive.net/](https://www.cancerimagingarchive.net/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BraTS 2016 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: Identical to the BraTS14 training dataset; Test: 191 unseen datasets
    from both BraTS12 and BraTS13 test datasets and TCIA | [https://www.smir.ch/BRATS/Start2016](https://www.smir.ch/BRATS/Start2016)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ABIDE-II (2016) | Brain | fMRI sequences | Autism spectrum disorder classification
    | 1114 datasets from 521 individuals with ASD and 593 controls | [https://fcon1000.projects.nitrc.org/indi/abide/](https://fcon1000.projects.nitrc.org/indi/abide/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BraTS 2017 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: 285 training datasets from BraTS12 and BraTS13 + pre-operative MRI
    scans from 19 institution; Validation: 6 unseen datasets from different institution;
    Test: 146 unseen datasets from both BraTS13 test datasets and different institutions
    | [https://sites.google.com/site/braintumorsegmentation/](https://sites.google.com/site/braintumorsegmentation/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BraTS 2018 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: Identical to the BraTS17 dataset; Validation: 6 unseen datasets from
    different institution; Test: 191 unseen datasets from both BraTS13 test datasets
    and different institutions | [https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=37224922](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=37224922)
    |'
  prefs: []
  type: TYPE_TB
- en: '| dHCP 2018 [[28](#bib.bib28)] | Brain | MRI | Cortical and sub-cortical volume
    segmentation, cortical surface extraction, and inflation | 465 subjects ranging
    from 28 to 45 weeks post-menstrual age. | [http://www.developingconnectome.org/data-release/](http://www.developingconnectome.org/data-release/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Calgary-Campinas-359 (CC-359) [[29](#bib.bib29)] | Brain | MR images | Skull
    stripping or Brain segmentation | 359 subjects on scanners from three different
    vendors (GE, Philips, and Siemens) and at two magnetic field strengths (1.5 T
    and 3 T) | [https://www.ccdataset.com/download](https://www.ccdataset.com/download)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MICCAI WMH Challenge [[30](#bib.bib30)] | Brain | MR images | White matter
    hyperintensities (WMH) segmentation | Training: 60 images; Test: 110 images |
    [https://wmh.isi.uu.nl/#_Toc122355662](https://wmh.isi.uu.nl/#_Toc122355662) |'
  prefs: []
  type: TYPE_TB
- en: '| REST-meta-MDD Consortium [[31](#bib.bib31)] | Brain | Resting-state functional
    MRI (R-fMRI) | Major Depressive Disorder (MDD) classification | Neuroimaging data
    of 1,300 depressed patients and 1,128 normal controls from 25 research groups
    | [http://rfmri.org/REST-meta-MDD](http://rfmri.org/REST-meta-MDD) |'
  prefs: []
  type: TYPE_TB
- en: '| BraTS (2021) | Brain | MR images | Segmentation; Classification | 2,000 cases
    (8,000 mpMRI scans) | [http://braintumorsegmentation.org/](http://braintumorsegmentation.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MM-WHS challenge dataset (2017) [[32](#bib.bib32), [33](#bib.bib33)] | Heart
    | MR and CT images | Whole heart segmentation | 20 labeled and 40 unlabeled CT
    volumes; 20 labeled and 40 unlabeled MR volumes. | [https://zmiclab.github.io/zxh/0/mmwhs](https://zmiclab.github.io/zxh/0/mmwhs)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ACDC (2018) [[34](#bib.bib34)] | Heart | Cine MR images | Classification
    and segmentation | Training: 100 patients; Test: 50 patients | [https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html](https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Atrial LGE-MRI dataset (2018) [[35](#bib.bib35)] | Heart | Cardiac (LA) segmentation
    | Late gadolinium-enhanced magnetic resonance images (LGE-MRI) | Training: 100
    LGE-MRI; Test: 54 LGE-MRI | [http://atriaseg2018.cardiacatlas.org](http://atriaseg2018.cardiacatlas.org)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSCMRseg (2019) [[36](#bib.bib36)] | Heart | MR images | Cardiac(MYO, RV
    and LV) segmentation | Data was collected from 45 patients, who underwent cardiomyopathy.
    | [https://zmiclab.github.io/zxh/0/mscmrseg19](https://zmiclab.github.io/zxh/0/mscmrseg19)
    |'
  prefs: []
  type: TYPE_TB
- en: '| M&Ms (2020) [[37](#bib.bib37)] | Heart | MR images | Cardiac segmentation
    | Training: 175; Validation: 40; Test: 160 MR images | [https://www.ub.edu/mnms/](https://www.ub.edu/mnms/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| STARE | Eye | Fundus images | Blood vessel segmentation | 20 equal-sized
    (700×605) color fundus images | [https://cecas.clemson.edu/~ahoover/stare/](https://cecas.clemson.edu/~ahoover/stare/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DRIVE (2004) | Eye | Images captured withCanon CR5 non-mydriatic 3CCD camera
    | Vasculature segmentation | Training: 20 images; Test: 20 images | [https://drive.grand-challenge.org/](https://drive.grand-challenge.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DRISHTI-GS (2014) [[38](#bib.bib38)] | Eye | Fundus images | Optic disc (OD)
    and (OC) cup segmentation | Training: 50 images; Test: 51 images | [https://ieeexplore.ieee.org/document/6867807](https://ieeexplore.ieee.org/document/6867807)
    |'
  prefs: []
  type: TYPE_TB
- en: '| continued on the next page |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Commonly used datasets for data efficient deep learning in medical
    image analysis (continued).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Organ | Types | Task | Description | Link |'
  prefs: []
  type: TYPE_TB
- en: '| ReTOUCH (2017) [[39](#bib.bib39)] | Eye | OCT volumes | Fluid detection and
    fluid segmentation | Training: 70 OCT volumes; Test: 42 OCT volumes | [https://retouch.grand-challenge.org](https://retouch.grand-challenge.org)
    |'
  prefs: []
  type: TYPE_TB
- en: '| RetinalOCT (2018) [[40](#bib.bib40)] | Eye | Optical Coherence Tomography
    (OCT) Images | Classification | 207,130 OCT images | [https://www.kaggle.com/datasets/paultimothymooney/kermany2018](https://www.kaggle.com/datasets/paultimothymooney/kermany2018)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LDLOCTCXR (2018) [[40](#bib.bib40)] | Eye | OCT and Chest X-Ray images |
    Classification | 108,312 images(37,206 with choroidal neovascularization, 11,349
    with diabetic macular edema, 8,617 with drusen, and 51,140 normal) from 4,686
    patient | [https://data.mendeley.com/datasets/rscbjbr9sj/3](https://data.mendeley.com/datasets/rscbjbr9sj/3)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PALM (2019) [[41](#bib.bib41)] | Eye | Images captured with Zeiss Visucam
    500 | Classification of normal and myopia fundus; lesion segmentation in pathologic
    myopia. | Training: 400 images, Validation: 400 images; Test: 400 images | [https://palm.grand-challenge.org](https://palm.grand-challenge.org)
    |'
  prefs: []
  type: TYPE_TB
- en: '| REFUGE challenge dataset [[42](#bib.bib42)] | Eye | Fundus images | Classification
    of clinical Glaucoma; OD and OC segmentation; Localization of Fovea | 1200 fundus
    images with ground truth segmentations and clinical glaucoma labels | [https://refuge.grand-challenge.org/](https://refuge.grand-challenge.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ADAM (2020) [[43](#bib.bib43)] | Eye | Fundus images captured using a Zeiss
    Visucam 500 fundus camera | Classification; Optic disc detection and segmentation;
    Fovea localization and Lesion detection and segmentation | 1200 retinal fundus
    images | [https://amd.grand-challenge.org/](https://amd.grand-challenge.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| RIGA+ dataset (2022) [[44](#bib.bib44)] | Eye | Fundus images | Segmentation
    of Optic Disc (OD) and Cup (OC) | 744 labeled samples and 717 Unlabeled samples
    | [https://zenodo.org/record/6325549](https://zenodo.org/record/6325549) |'
  prefs: []
  type: TYPE_TB
- en: '| ISIC (2016) | Skin | Dermoscopic lesion images | 1.Lesion Segmentation; 2.Dermoscopic
    Feature Classification and segmentation; 3.Disease Classification | 1.Training:900,
    Test:379 images; 2.Training:807, Test:335 images; 3.Training:900, Test:379 images
    | [https://challenge.isic-archive.com/data/#2016](https://challenge.isic-archive.com/data/#2016)
    |'
  prefs: []
  type: TYPE_TB
- en: '| HAM10000 (2018) | Skin | Dermatoscopic images | Lesion classification and
    segmentation | 10000 training images | [https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MITOS12 [[45](#bib.bib45)] | Breast | Histological Images | Breast cancer
    grading | 50 high power fields (HPF) coming from 5 different slides scanned at
    ×40 magnification | [http://ludo17.free.fr/mitos_2012/dataset.html](http://ludo17.free.fr/mitos_2012/dataset.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MITOS14 | Breast | Histological Images | Breast cancer grading | Training
    data set there are 284 frames at X20 magnification and 1,136 frames at X40 magnification.
    | [https://mitos-atypia-14.grand-challenge.org/Dataset/](https://mitos-atypia-14.grand-challenge.org/Dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MIAS (2015) | Breast | Mammograms | Detection; Classification | 322 images
    (161 pairs) at 50 micron resolution in *Portable Gray Map* format | [https://www.kaggle.com/datasets/kmader/mias-mammography](https://www.kaggle.com/datasets/kmader/mias-mammography)
    |'
  prefs: []
  type: TYPE_TB
- en: '| TUPAC (2016) [[46](#bib.bib46)] | Breast | Whole-slide histopathology images
    | Automatic prediction of tumor proliferation scores of breast tumors | Training:
    500 WSIs; Test: 321 WSIs | [https://github.com/CODAIT/deep-histopath](https://github.com/CODAIT/deep-histopath)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CAMELYON (2016) [[47](#bib.bib47)] | Breast | Whole-slide images (WSIs) |
    Detection and classification of breast cancer metastases | Training: 270 WSI;
    Test: 130 WSI | [https://camelyon16.grand-challenge.org/Data](https://camelyon16.grand-challenge.org/Data)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CAMELYON (2017) [[47](#bib.bib47)] | Breast | Whole-slide images (WSIs) |
    Detection and classification of breast cancer metastases | Training: 500 WSI;
    Test: 500 WSI | [https://camelyon17.grand-challenge.org/Data](https://camelyon17.grand-challenge.org/Data)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CBIS-DDSM (2017) | Breast | Mammograms | Segmentation | Data set contains
    753 calcification cases and 891 mass cases | [https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset](https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BACH (2018) [[48](#bib.bib48)] | Breast | Microscopy and Whole-slide images
    | Breast cancer classification | Microscopy: 400 images; WSI: 30 images | [https://iciar2018-challenge.grand-challenge.org/Dataset/](https://iciar2018-challenge.grand-challenge.org/Dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| TNBC (2018) | Breast | Histopathology images stained with H&E | Nuclei segmentation
    | Data Set1: 50 images with a total of 4022 annotated cells; Data Set2: 30 images
    from 7 different organs with a total of 21 623 annotated nuclei | [https://ega-archive.org/datasets/EGAD00001000063](https://ega-archive.org/datasets/EGAD00001000063)
    |'
  prefs: []
  type: TYPE_TB
- en: '| FNAC (2019) [[49](#bib.bib49)] | Breast | Cytology images | Classification
    | 212 images in two classes: benign (99) and malignant (113) | [https://1drv.ms/u/s!Al-T6d-_ENf6axsEbvhbEc2gUFs](https://1drv.ms/u/s!Al-T6d-_ENf6axsEbvhbEc2gUFs)
    |'
  prefs: []
  type: TYPE_TB
- en: '| NYUBCS (2019) | Breast | Mammograms | Segmentation | 29,426 digital screening
    mammography exams (1,001,093 images) from 141,473 patients | [https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf](https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BreastPathQ (2019) [[50](#bib.bib50)] | Breast | Whole slide images stained
    with H&E | Estimation of tumor cellularity (TC) | Training: 2,579 patches extracted
    from 69 WSIs; Test: 1,121 patches extracted from 25 WSIs | [https://breastpathq.grand-challenge.org/Overview/](https://breastpathq.grand-challenge.org/Overview/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CERVIX93 (2018) [[51](#bib.bib51)] | Cervix | Cytology images | Classification;
    detection | 93 stacks of images (2705 nuclei) | [https://github.com/parhamap/cytology_dataset](https://github.com/parhamap/cytology_dataset)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LBC (2020) [[52](#bib.bib52)] | Cervix | Cytology images | Classification
    | 963 LBC images in classes of NILM, LSIL, HSIL, and SCC | [https://data.mendeley.com/datasets/zddtpgzv63/4](https://data.mendeley.com/datasets/zddtpgzv63/4)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CHAOS (2021) [[53](#bib.bib53)] | Abdomen | CT and MR images | Liver and
    Abdominal segmentation | CT: 40 images; MRI: 120 DICOM data sets | [https://chaos.grand-challenge.org/](https://chaos.grand-challenge.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| KiTS (2023) | Kidney | CT scan | Kidney Tumor Segmentation | Training: 489
    cases; Test: 110 cases | [https://kits-challenge.org/kits23/](https://kits-challenge.org/kits23/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LiTS (2017) | Liver | CT scans | Liver lesions segmentation | Training: 130
    CT scans; Test: 70 CT scans | [https://competitions.codalab.org/competitions/17094](https://competitions.codalab.org/competitions/17094)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Asciteps (2020) [[54](#bib.bib54)] | Stomach | Classification; detection
    | Cytology images | 487 images for classification: malignant(18,558) and benign(6089);
    176 images for detection (6573 bounding boxes) | [https://pan.baidu.com/s/1r0cd0PVm5DiUmaNozMSxgg](https://pan.baidu.com/s/1r0cd0PVm5DiUmaNozMSxgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| continued on the next page |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Commonly used datasets for data efficient deep learning in medical
    image analysis (continued).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Organ | Types | Task | Description | Link |'
  prefs: []
  type: TYPE_TB
- en: '| MoNuSeg (2017) [[55](#bib.bib55)] | Multi-organ | H&E stained tissue images
    | Nuclei segmentation | Training: 30 images and around 22,000 nuclear boundary
    annotations; Test: 7000 nuclear boundary annotations | [https://monuseg.grand-challenge.org/](https://monuseg.grand-challenge.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BTCV (2017) [[56](#bib.bib56)] | Multi-organ | CT images | Multi-organ segmentation
    | 90 abdominal CT images | [nhttps://zenodo.org/record/1169361#.Y8Ud-OxBwUE](nhttps://zenodo.org/record/1169361#.Y8Ud-OxBwUE)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DeepLesion (2018) [[57](#bib.bib57)] | Multi-organ | CT slices | For different
    applications | 32,735 lesions in 32,120 CT slices | [https://nihcc.app.box.com/v/DeepLesion](https://nihcc.app.box.com/v/DeepLesion)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DECATHLON (2019) | Multi-organ | CT and MRI | Segmentation | Brain: 750 MRI;
    Heart: 30 MRI; Liver: 201 CT images; Hippocampus: 195 MRI; Prostate: 48 MRI; Lung:
    96 CT scans; Pancreas: 420 CT scans; HepaticVessel: 443 CT scans; Spleen: 61 CT
    scans; Colon: 190 CT scans | [http://medicaldecathlon.com/](http://medicaldecathlon.com/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MIDOG [[58](#bib.bib58)] | Multi-organ | Whole Slide Images | Segmentation
    | Canine Lung Cancer: 44 cases; Human Breast Cancer: 150 cases; Canine Lymphoma:
    55 cases; Human neuroendocrine tumor: 55 cases; Canine Cutaneous Mast Cell Tumor:
    50 cases; Human melanoma: 49 cases | [https://imig.science/midog/the-dataset/](https://imig.science/midog/the-dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CRCHistoPhenotypes (2016) [[59](#bib.bib59)] | Colon | Histology images |
    Cancer classification | 100 H&E stained histology images of colorectal adenocarcinomas
    | [https://warwick.ac.uk/fac/crossfac/tia/data/crchistolabelednucleihe](https://warwick.ac.uk/fac/crossfac/tia/data/crchistolabelednucleihe)
    |'
  prefs: []
  type: TYPE_TB
- en: '| KATHER (2018) [[60](#bib.bib60)] | Colon | Histological images | Cancer classification
    | 100,000 histological images of human colorectal cancer and healthy tissue |
    [https://zenodo.org/record/1214456#.Y8fgV-zP1hE](https://zenodo.org/record/1214456#.Y8fgV-zP1hE)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PROMISE12 challenge dataset [[61](#bib.bib61)] | Prostate | MR images | Prostate
    segmentation | Training: 50; Test: 30; Live challenge: 20 datasets | [promise12.grand-challenge.org/](promise12.grand-challenge.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| TMA-Zurich (2018) [[62](#bib.bib62)] | Prostate | Histopathology images |
    Gleason grading of prostate cancer | Training: 641 patients; Test: 245 patients
    | [https://www.nature.com/articles/s41598-018-30535-1?source=app#data-availability](https://www.nature.com/articles/s41598-018-30535-1?source=app#data-availability)
    |'
  prefs: []
  type: TYPE_TB
- en: '| The Cancer Genome Atlas (TCGA) dataset | Prostate | Histopathology WSIs |
    Cancer tumour classification based on gleason scores | 20,000 patient samples
    spanning 33 cancer types | [https://portal.gdc.cancer.gov/repository](https://portal.gdc.cancer.gov/repository)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PANDA (2020) [[63](#bib.bib63)] | Prostate | Whole-slide images | Gleason
    grading of prostate cancer | Development set: 10,616 biopsies; Tuning set: 393;
    Internal validation set: 545; External validation: 1071 | [https://www.kaggle.com/c/prostate-cancer-grade-assessment/data](https://www.kaggle.com/c/prostate-cancer-grade-assessment/data)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SCGM dataset [[64](#bib.bib64)] | Spinal Cord | MRI images | Spinal cord
    gray matter segmentation | Training: 40 images; Test: 40 images | [http://niftyweb.cs.ucl.ac.uk/program.php?p=CHALLENGE](http://niftyweb.cs.ucl.ac.uk/program.php?p=CHALLENGE)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Montgomery (2014) [[65](#bib.bib65)] | Chest | Chest X-rays | Segmentation
    | 138 images in two classes: normal (80) and manifestations of TB (58) | [https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-montgomery](https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-montgomery)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Shenzhen (2014) [[65](#bib.bib65)] | Chest | Chest X-rays | Segmentation
    | 662 images in two classes: normal (326) and manifestations of TB (336) | [https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-shenzhen](https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-shenzhen)
    |'
  prefs: []
  type: TYPE_TB
- en: '| NIH Chest X-ray (2017) [[66](#bib.bib66)] | Chest | Chest X-rays | Classification
    | 112,120 X-ray images with disease labels from 30,805 unique patients. | [https://www.kaggle.com/datasets/nih-chest-xrays/data](https://www.kaggle.com/datasets/nih-chest-xrays/data)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ChestX-ray8 (2017) [[66](#bib.bib66)] | Chest | Chest x-ray images | Classification
    and Localization of Common Thorax Diseases | 108,948 frontal-view X-ray images
    of 32,717 unique patients with the text-mined eight disease image labels | [https://nihcc.app.box.com/v/ChestXray-NIHCC/](https://nihcc.app.box.com/v/ChestXray-NIHCC/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MIMIC-CXR (2019) [[67](#bib.bib67)] | Chest | Chest x-ray images | Detection
    | Total of 377,110 images with semi-structured free-text radiology report that
    describes the radiological findings of the images | [https://physionet.org/content/mimic-cxr/2.0.0/](https://physionet.org/content/mimic-cxr/2.0.0/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ChestX-ray14 (2019) | Chest | Chest x-ray images | Classification and Localization
    of Common Thorax Diseases | 112,120 frontal chest radiographs from 30,805 distinct
    patients with 14 binary labels | [https://stanfordmlgroup.github.io/competitions/chexpert/](https://stanfordmlgroup.github.io/competitions/chexpert/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CC-COVID (2020) [[68](#bib.bib68)] | Chest | CT images | Lung-lesion segmentation
    | 532,506 CT images from NCP, common pneumonia, and normal controls | [https://ncov-ai.big.ac.cn/download?lang=en](https://ncov-ai.big.ac.cn/download?lang=en)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SegTHOR (2020) [[69](#bib.bib69)] | Chest | CT images | Segmentation of Thoracic
    Organs | Training: 40 CT scans; Test: 20 CT scans | [https://competitions.codalab.org/competitions/21145](https://competitions.codalab.org/competitions/21145)
    |'
  prefs: []
  type: TYPE_TB
- en: '| VinDr-CXR (2021) [[70](#bib.bib70)] | Chest | Chest x-ray images | Classification;
    Detection | Training: 15000 scans; Test: 3000 scans | [https://vindr.ai/datasets/cxr](https://vindr.ai/datasets/cxr)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ChestXR (2021) | Chest | Chest x-ray images | Classification | 20,000+ images
    and 3 classes: COVID-19, Pneumonia and Normal cases | [https://cxr-covid19.grand-challenge.org/Dataset/](https://cxr-covid19.grand-challenge.org/Dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MICCAI2018 IVDM3Seg dataset | Intervertebral Disc | MRI images | Intervertebral
    discs (IVD) localization and segmentation | 24 3D multi-modality MRI data sets
    each data set contains four aligned high-resolution 3D volumes, so total 96 high-resolution
    3D MRI volume data | [https://ivdm3seg.weebly.com/data.html](https://ivdm3seg.weebly.com/data.html)
    |'
  prefs: []
  type: TYPE_TB
- en: 2 No supervision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Learning with no supervision, commonly referred to as unsupervised learning,
    involves the challenge of obtaining supervision signals in the absence of explicit
    guidance. One primary technique used for this purpose is self-supervised learning
    (SSL). In SSL, representations are acquired by training on an auxiliary pretext
    task and later transferred to a target downstream task of interest. The effectiveness
    of SSL relies significantly on the design of well-crafted pretext tasks. These
    pretext tasks introduce implicit inductive biases into the model, making it crucial
    to select them thoughtfully to ensure their relevance to the specific domain of
    interest. Self-supervised learning can be divided into four broad categories:
    predictive, generative, contrastive, and multi self-supervision [[71](#bib.bib71)].
    A summary of recent methods for learning with no supervision is provided in Table [2](#S2.T2
    "Table 2 ‣ 2.4 Multi-self supervised learning: combining multiple SSL pretext
    tasks into one framework ‣ 2 No supervision ‣ Data efficient deep learning for
    medical image analysis: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Predictive self-supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we explore predictive self-supervision, where the pretext
    task is cast as either a classification or regression task. Specifically, each
    unlabeled image is assigned a pseudo label, which is generated directly from the
    data itself. These pseudo labels can take on categorical or numerical values,
    depending on the design specifications of the pretext task. Common transformation-based
    predictive tasks involve aspects such as assessing relative position [[72](#bib.bib72)],
    solving jigsaw puzzles [[73](#bib.bib73)], and determining rotation angles [[74](#bib.bib74)],
    among others. These traditional pretext tasks, and their variations, have been
    explored in MIA and have demonstrated their effectiveness. For instance, Bai et
    al. [[75](#bib.bib75)] introduced an approach for segmenting cardiac MRI scans
    by proposing a pretext task focused on predicting anatomical positions. This pretext
    task aimed to utilize the various cardiac views available in the MRI scans, such
    as short-axis, 2CH long-axis, and 4CH long-axis, to represent different cardiac
    anatomical regions, including the left and right atrium and ventricle. To accomplish
    this, the authors defined a series of bounding boxes corresponding to specific
    anatomical positions within a given view and trained their network to predict
    these positions. Taleb et al. [[76](#bib.bib76)] introduced a novel approach inspired
    by Jigsaw puzzle-solving, which makes use of multiple imaging modalities. In this
    method, an input image is composed of disordered patches from different modalities,
    and the model’s task is to reconstruct the original image by correctly assembling
    these patches. Their work represents a notable enhancement over the traditional
    Jigsaw puzzle approach. Zhuang et al. [[77](#bib.bib77)] proposed a self-supervised
    task called Rubik cube recovery, inspired by the early work on Jigsaw puzzle solving
    for 2D natural images. The task involves two operations: cube rearrangement and
    cube rotation, as shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Predictive self-supervision
    ‣ 2 No supervision ‣ Data efficient deep learning for medical image analysis:
    A survey"). The Rubik cube recovery task uses 3D input, where a Rubik cube is
    divided into a 3D grid of 2×2×2 sub-cubes. The addition of the cube rotation task
    ensures learning of rotation invariant features, going beyond the original Jigsaw
    puzzle task, which only focuses on learning translation-invariant features. Rubik
    cube+ [[78](#bib.bib78)] improves upon the Rubik cube recovery pretext task by
    using cube masking operation along with both cube rearrangement and cube rotation
    operations. Nguyen et al. [[79](#bib.bib79)] introduced a spatial awareness pretext
    task with the aim of acquiring semantic and spatial representations from volumetric
    images. This concept of a spatial pretext task was influenced by Chen et al.’s
    [[80](#bib.bib80)] context restoration framework; however, it was formulated here
    into a classification problem. Recently, Zhou et al. [[81](#bib.bib81)] performed
    multi-scale pixel restoration and siamese feature comparison within the feature
    pyramid. This approach effectively retains semantic, pixel-level, and scale information
    all at once.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a823e006cf90f1a49ca0991edc59689.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of the Rubik’s Cube pretext task: A Siamese network
    with M (representing the number of cubes) shared-weight branches, referred to
    as Siamese-Octad, is employed to solve the Rubik’s Cube. The backbone network
    for each branch can be any well-known 3D CNN. The feature maps derived from the
    final fully-connected or convolutional layer of all branches are concatenated
    and used as input for separate tasks’ fully-connected layers, namely cube ordering
    and orientation. These tasks are supervised by the permutation loss ($L_{P}$)
    and rotation loss ($L_{R}$), respectively (image from [[77](#bib.bib77)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Generative self-supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The generative self-supervised learning approach seeks to learn underlying
    features in the input data by framing pretext tasks as generative problems [[71](#bib.bib71)].
    The idea behind generative pretext tasks is that the model can acquire valuable
    representations from unlabeled data by either learning to reconstruct the input
    data itself or by generating new examples that follow the same distribution as
    the input data. Ross et al. [[82](#bib.bib82)] utilized the image colorization
    pretext task to address the segmentation of endoscopic medical instruments in
    endoscopic video data. However, instead of using the original architecture employed
    in the colorization task, they opted for a conditional Generative Adversarial
    Network (GAN) architecture. This choice aimed to promote the generation of more
    realistic colored images. The authors evaluated their approach on six datasets
    from both medical and natural domains to assess its effectiveness in downstream
    tasks. Chen et al. [[80](#bib.bib80)] introduced a new generative pretext task
    that involves randomly selecting two isolated patches from an input image and
    swapping their positions. This swapping process is repeated iteratively, resulting
    in a corrupted version of the original image while preserving its overall distribution.
    Subsequently, a generative model is used to restore the corrupted image back to
    its original version (see Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Generative self-supervision
    ‣ 2 No supervision ‣ Data efficient deep learning for medical image analysis:
    A survey")). Building upon earlier context-restoration-based studies, Zhou et
    al. [[83](#bib.bib83)] incorporated four data transformations (non-linear transformation,
    local-shuffling, outer-cutout, and inner-cutout) into a cohesive reconstruction
    model called *Model Genesis*. Harvella et al. [[10](#bib.bib10)] introduced a
    self-supervised multi-modal reconstruction task for retinal anatomy learning.
    They assumed that distinct modalities of the same organ could offer complementary
    knowledge, leading to valuable representations for subsequent tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the medical domain, conventional pretext tasks that heavily rely on the
    existence of bigger objects in natural images are inadequate because disease-related
    features are usually found in smaller regions of the medical image. To address
    this, Holmberg et al. [[84](#bib.bib84)] introduced a pretext task, cross-modal
    self-supervised retinal thickness prediction, for ophthalmic disease diagnosis.
    This task involves the utilization of two distinct modalities: infrared fundus
    images and optical coherence tomography scans (OCT). Initially, they extracted
    retinal thickness maps from OCT scans by training a segmentation model with the
    limited annotated dataset, which served as ground-truth annotations for the preliminary
    task. Then, a model was trained to predict the thickness maps utilizing unlabeled
    fundus images and the previously predicted thickness maps as labels. Other examples
    of generative self-supervised pretext tasks include the image denoising method
    proposed by Prakash et al. [[85](#bib.bib85)] and the Rubik cube++ (introduced
    by Tao et al. [[86](#bib.bib86)]). In the Rubik cube++ approach, significant modifications
    were made to the earlier Rubik cube method [[77](#bib.bib77)]. Instead of treating
    it as a classification task, they approached it as a generative problem using
    a GAN-based framework. The generator’s task was to bring back the initial arrangement
    of the Rubik cube before applying transformations, whereas the discriminator was
    responsible for distinguishing between correct and incorrect arrangements of the
    generated cubes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/242038d660542e7e5b10dae4949632bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: CNN architecture for self-supervised context restoration learning,
    where blue, green, and orange strides indicate convolutional units, downsampling
    units, and upsampling units, respectively. The specific structure of the CNN in
    the reconstruction part may vary based on the subsequent task (image from [[80](#bib.bib80)]).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/044bac0f629c564d3a0eeed267225fa3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustration of the enhanced SimCLR for 3D medical image segmentation:
    (i) Outline of the global contrastive loss employed for pre-training the encoder
    $e$ using dense layers $g_{1}$. (ii) Outline of the local contrastive loss utilized
    for pre-training the decoder $d_{l}$ with 1 × 1 convolutional layers $g_{2}$,
    with frozen weights of encoder $e$ obtained from the previous training stage (image
    from [[87](#bib.bib87)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Contrastive self-supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrastive learning is designed to maximize the mutual information between
    positive image pairs and, if needed, minimize the representation similarity of
    negative image pairs. Positive pairs consist of two augmented views of the same
    instance, whereas negative pairs come from different instances. This allows the
    network to learn discriminative representations of instances, which are beneficial
    for pattern recognition tasks. In contrastive learning, the effectiveness of learned
    representations heavily depends on the choice of positive and negative pairs.
    However, the conventional pair generation methods used for natural images might
    not be suitable for medical images with intricate semantic concepts, leading to
    potentially meaningless representations. To tackle this challenge, researchers
    have dedicated considerable effort to meticulously devising pair selection strategies
    within widely used contrastive learning frameworks [[88](#bib.bib88)]. These strategies
    aim to retain the pathological semantics present in medical images, resulting
    in significant performance enhancements for medical datasets compared to traditional
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contig [[89](#bib.bib89)] employs a contrastive loss to align images and various
    genetic modalities within the feature space. The approach is devised to seamlessly
    incorporate multiple modalities from each individual into a single end-to-end
    model, even when the modalities available may differ among individuals. Sowrirajan
    et al. [[90](#bib.bib90)] asserted that the augmentations used in MOCO [[91](#bib.bib91)]
    are not suitable for gray-scale medical images. Specifically, blurring and random
    crop could potentially remove important lesions. To address this issue, they introduced
    MoCo-CXR, a modified version of MOCO, specifically tailored for chest X-ray images
    by adapting the augmentations to better suit this medical imaging context. Vu
    et al. [[92](#bib.bib92)] introduced a SSL technique called MedAug, inspired by
    MoCo-CXR. In their method, positive pairs are generated from diverse images of
    a single patient based on their metadata. Azizi et al. [[5](#bib.bib5)] presented
    a similar work to MedAug, which was based on the SimCLR framework [[93](#bib.bib93)].
    They introduced a method called *Multi-Instance Contrastive Learning* to create
    more informative positive pairs from various images of a similar patient. Chaitanya
    et al. [[87](#bib.bib87)] enhanced SimCLR for 3D medical image segmentation (see
    Figure [4](#S2.F4 "Figure 4 ‣ 2.2 Generative self-supervision ‣ 2 No supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). They introduced
    a novel contrasting strategy that leveraged the structural similarity of volumetric
    medical images. Additionally, they introduced a local contrastive loss to facilitate
    the learning of more detailed and fine-grained representations. Ciga et al. [[94](#bib.bib94)]
    introduce a contrastive SSL approach for digital histopathology. They conducted
    training on 57 unlabeled histopathology datasets. Their findings reveal that enhancing
    the feature quality is achievable by combining multiple multi-organ datasets with
    diverse staining and resolution characteristics. Some techniques leverage anatomical
    priors within contrastive methods to further enhance performance across various
    tasks [[6](#bib.bib6), [95](#bib.bib95)]. Specifically, He et al. [[95](#bib.bib95)]
    introduce Geometric Visual Similarity Learning (GVSL). GVSL incorporates the concept
    of topological invariance into the metric, ensuring a dependable assessment of
    inter-image similarity. This approach aims to learn a consistent representation
    for equivalent semantic regions across different images.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.4 Multi-self supervised learning: combining multiple SSL pretext tasks into
    one framework'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multi-SSL integrates various types of pretext tasks, including predictive,
    generative, and contrastive tasks. By doing so, it aims to overcome the limitation
    of single pretext tasks, which might learn task-specific features. By employing
    different self-supervision signals during network training, multi-SSL aims to
    extract more robust and generalizable representations. Taleb et al. [[9](#bib.bib9)]
    proposed that medical images with a 3D nature offer the potential to learn rich
    representations compared to 2D images. To accommodate this, they employed five
    predesigned pretext tasks, namely contrastive predictive coding (CPC), exemplar
    CNN, rotation prediction, relative position prediction, and Jigsaw puzzle, to
    adapt to the characteristics of 3D medical images. Haghighi et al. [[96](#bib.bib96)]
    introduced Semantic Genesis, building upon the Model Genesis approach [[83](#bib.bib83)].
    This framework comprises three modules: self-classification, self-restoration,
    and self-discovery, aimed at learning semantics-enriched representations. In a
    further extension of Model Genesis, Zhang et al. [[97](#bib.bib97)] incorporated
    a scale-aware proxy task for predicting the input’s scale. This addition allows
    for the learning of multi-level representations. Zhou et al. [[98](#bib.bib98)]
    combined generative and contrastive SSL into a Preservational Contrastive Representation
    Learning (PCRL) framework, where preservational learning is introduced for the
    generative SSL to keep more information. Tang et al. [[99](#bib.bib99)] introduce
    a novel 3D transformer-based architecture known as Swin UNEt TRansformers (Swin
    UNETR), with a hierarchical encoder for self-supervised pre-training. In their
    proposed pre-training framework, input CT images undergo random cropping into
    sub-volumes and are augmented with random inner cutout and rotation operations.
    Subsequently, they are inputted into the Swin UNETR encoder. The authors employ
    masked volume inpainting, contrastive learning, and rotation prediction as proxy
    tasks to facilitate the learning of contextual representations from input images,
    as shown in Figure [5](#S2.F5 "Figure 5 ‣ 2.4 Multi-self supervised learning:
    combining multiple SSL pretext tasks into one framework ‣ 2 No supervision ‣ Data
    efficient deep learning for medical image analysis: A survey"). CS-CO [[100](#bib.bib100)],
    designed specifically for histopathological images, combines the strengths of
    generative and discriminative approaches. This method comprises two self-supervised
    learning phases: cross-stain prediction (CS) and contrastive learning (CO). Yan
    et al. [[101](#bib.bib101)] employ Masked Autoencoders (MAE) but demonstrate that
    directly applying MAE is suboptimal for dense downstream prediction tasks such
    as multi-organ segmentation. To address this limitation, they propose a self-supervised
    pre-training approach on large-scale unlabeled medical datasets, leveraging both
    contrastive and generative modeling techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Yan et al. [[101](#bib.bib101)] used Masked Autoencoders (MAE) but demonstrated
    that directly applying MAE is suboptimal for dense downstream prediction tasks,
    such as multi-organ segmentation. To address this limitation, they proposed a
    self-supervised pre-training approach on large-scale unlabeled medical datasets,
    leveraging both contrastive and generative modeling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4717c35abc7c02900da61c9ff0b7be39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The pre-training framework’s [[99](#bib.bib99)] outline begins with
    the random cropping of input CT images into sub-volumes, followed by the application
    of random inner cutout and rotation augmentations. These processed images are
    then utilized as input for the Swin UNETR encoder. The framework leverages masked
    volume inpainting, contrastive learning, and rotation prediction as proxy tasks
    aimed at acquiring contextual representations from the input images (image from
    [[99](#bib.bib99)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Overview of recent methods in *No Supervision* category.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Task | Pretext task | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| [[75](#bib.bib75)] | Cardiac segmentation | Anatomical Position Prediction
    | Private Dataset: 3825 Subjects | DSC: 0.93 |'
  prefs: []
  type: TYPE_TB
- en: '| [[77](#bib.bib77)] | Brain tumor segmentation Brain hemorrhage classification
    | Rubik’s Cube Recovery | BraTS 2018; Private Dataset: 1,486 Images | BraTS 2018:
    mIoU: 0.773; Private: Acc: 0.838 |'
  prefs: []
  type: TYPE_TB
- en: '| [[78](#bib.bib78)] | Brain tumor segmentation Brain hemorrhage classification
    | Rubik cube+ (cube ordering, cube orientation and masking identification) | BraTS-2018;
    Private Dataset: 1,486 CT volumes | BraTS 2018: Mean Dice: 81.70; Private: Acc:
    87.84 |'
  prefs: []
  type: TYPE_TB
- en: '| [[80](#bib.bib80)] | Fetal image classification Abdominal multi-organ localization
    Brain tumour segmentation | Image Context Restoration | Private Fetus Dataset:
    2,694 Images; Private Multi-organ Dataset: 150 Images; BraTS 2017 | Private Fetus
    Dataset: F1: 0.8942; Private Multi-organ Dataset: Mean Distance: 2.90; BraTS 2017:
    DSC: 0.8557 |'
  prefs: []
  type: TYPE_TB
- en: '| [[10](#bib.bib10)] | Optic disc segmentation | Multi-modal Reconstruction
    | Isfahan MISP | AUC: 0.818 |'
  prefs: []
  type: TYPE_TB
- en: '| [[86](#bib.bib86)] | Pancreas and Brain Tissue segmentation | Rubik cube
    ++ | NIH PCT; MRBrainS18 | NIH PCT: DSC: 0.8408; MRBrainS18: DSC: 0.7756 |'
  prefs: []
  type: TYPE_TB
- en: '| [[5](#bib.bib5)] | Chest X-ray classification Skin lesions classification
    | Multi-Instance Contrastive Learning (SimCLR) | Priavte Dermatology Dataset;
    CheXpert | Private: Top-1 Acc: 0.7002; CheXpert: AUC: 0.772 |'
  prefs: []
  type: TYPE_TB
- en: '| [[102](#bib.bib102)] | Lung | Contrastive Learning | CheXpert | AUC: 0.889
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[6](#bib.bib6)] | 2D and 3D landmark detection; 3D Lesion matching | Global
    and Local Contrastive Learning | DeepLesion; NIH LN; Private Dataset: 94 Patients
    | Mean Radial Error: 4.3; Maximum Radial Error: 16.4 |'
  prefs: []
  type: TYPE_TB
- en: '| [[96](#bib.bib96)] | Lung | Self-Discovery + Self-Classification + Self-Restoration
    | LUNA; LiTS; CAD-PE; BraTS 2018; ChestX-ray14; LIDC-IDRI; SIIM-ACR | Classification:
    LUNA: AUC: 0.9847; Segmentation: IoU: LiTS: 0.8560; BraTS 2018: 0.6882 |'
  prefs: []
  type: TYPE_TB
- en: '| [[9](#bib.bib9)] | Brain tumors segmentation pancreas tumor segmentation
    | CPC Jigsaw puzzle Exemplar CNN Rotation Prediction Relative position prediction
    | BraTS 2018; DECATHLON; DRD | BraTS 2018: DSC: 0.9080; DECATHLON: DSC $\approx$
    0.635; DRD DRD: DSC $\approx$ 0.80 |'
  prefs: []
  type: TYPE_TB
- en: '| [[101](#bib.bib101)] | Multi-organ segmentation | Masked Autoencoders + contrastive
    and generative modeling | Pre-training Dataset: Abdomen-1K; Fine-tuning Dataset:
    ABD-110; Thorax-85; HaN | ABD-110: Dice score: 84.67; Thorax-85: Dice score: 90.37;
    HaN: Dice score: 77.31 |'
  prefs: []
  type: TYPE_TB
- en: 3 Inexact supervision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inexact supervision pertains to situations where some form of supervision information
    is available but lacks the exactness desired for the task. In this context, we
    classify inexact supervision into two categories: Multiple Instance Learning (MIL)
    and learning with weak annotations (Figure [6](#S3.F6 "Figure 6 ‣ 3 Inexact supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). In the
    MIL framework (Subsection [3.1](#S3.SS1 "3.1 Multiple instance learning ‣ 3 Inexact
    supervision ‣ Data efficient deep learning for medical image analysis: A survey")),
    each image is treated as a *bag*, and the patches extracted from it are regarded
    as *instances*. When a bag is labeled as negative, it implies that all instances
    within it are also considered negative. Conversely, if a bag is labeled as positive,
    it indicates the presence of at least one positive instance within it. This labeling
    strategy at the bag level significantly reduces the labeling burden compared to
    labeling each individual instance separately, which proves advantageous across
    various tasks. Learning with weak annotations (Subsection [3.2](#S3.SS2 "3.2 Learning
    with weak annotations ‣ 3 Inexact supervision ‣ Data efficient deep learning for
    medical image analysis: A survey")) refers to a scenario in which the available
    training data is annotated with labels that are less detailed or less precise
    than what might be ideal for a particular task. In many medical imaging tasks,
    obtaining precise annotations at a fine-grained level (such as pixel-level annotations)
    can be highly valuable but also costly and time-consuming. Weak annotations offer
    an alternative approach where the labels provided for the training data are of
    a coarser or less specific nature, making them easier and more cost-effective
    to obtain. These weak annotations can take various forms, including image-level,
    point-level, scribble-level, or box-level. In all of these scenarios, the provided
    annotations are less detailed and precise compared to comprehensive pixel-level
    annotations. A summary of recent methods for learning with inexact supervision
    is provided in Table [3](#S3.T3 "Table 3 ‣ 3.2.2 Learning with point annotation
    ‣ 3.2 Learning with weak annotations ‣ 3 Inexact supervision ‣ Data efficient
    deep learning for medical image analysis: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/02d325496f39cbe142ed18298548a993.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Taxonomy of *Inexact Supervision* methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Multiple instance learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multiple-instance learning (MIL) [[103](#bib.bib103)] arises when obtaining
    detailed annotations for individual pixels or patches in an image becomes impractical,
    time-consuming, or infeasible. Instead, global labels representing the overall
    image condition are more readily available. However, these global labels do not
    directly correspond to every pixel or patch within the image. MIL extends supervised
    learning to train classifiers using weakly labeled data. In MIL, every image is
    viewed as a *bag* containing numerous patches, also referred to as *instances*.
    If an image, or *bag*, is classified as disease-positive, it implies that at least
    one patch, or *instance*, within that image is disease-positive. Conversely, if
    an image is labeled as disease-negative, it signifies that all patches, or *instances*,
    in that image are negative instances. The current approaches within deep MIL can
    be classified into two categories: instance-based methods and bag-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Instance-based methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main concept behind the instance-based method is to train an effective instance
    classifier to predict the possible labels for individual instances (e.g., image
    patches) within each bag. Subsequently, the MIL-pooling (the aggregation process
    is commonly referred to as MIL-pooling) method is applied to combine the predictions
    of all instances within each bag, ultimately generating the bag’s prediction.
    Given that the actual labels of individual instances are unknown, these approaches
    typically begin by assigning pseudo-labels to each instance based on their respective
    bags (i.e., all instances within a positive bag are assigned positive labels,
    and all instances within a negative bag are assigned negative labels). Subsequently,
    the instance classifier is trained using pseudo-labels in a supervised manner
    until it converges [[103](#bib.bib103)]. Various MIL pooling techniques are employed
    in this process, including Mean-pooling [[104](#bib.bib104)], Max-pooling [[104](#bib.bib104)],
    Average-pooling [[105](#bib.bib105)] log-sum-exp-pooling [[106](#bib.bib106)],
    Noisy-or-pooling [[107](#bib.bib107)], Noisy-and-pooling [[108](#bib.bib108)],
    and Dynamic pooling [[109](#bib.bib109)], among others. Couture et al. [[110](#bib.bib110)]
    propose an improved MIL aggregation approach that employs a quantile function
    as the pooling mechanism. This innovative technique allows for a comprehensive
    representation of the variations within each sample, leading to improved global
    classification accuracy. In the recent study by Qu et al. [[111](#bib.bib111)],
    they applied instance-level contrastive learning to aggregate various tumor features
    for the purpose of diagnosing pancreatic cancer.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Bag-based methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bag-based methods rely on shared instance-level feature extractors to capture
    the features of each instance within a bag. These features are then aggregated
    using MIL-pooling to obtain bag-level features, followed by supervised training
    of the bag classifier until convergence is achieved. In bag-based methods, MIL-pooling
    aggregates instance features rather than instance predictions, as is the case
    in instance-based methods. Bag-based methods excel in bag classification because
    they have access to true bag labels, making their training process free from noise
    and more accurate than instance-based methods. However, they are less suitable
    for localization tasks, and their instance feature aggregation lacks flexibility
    in showcasing the contributions of individual instances to bag classification.
    These methods are suitable when the target pattern is expected to be visible at
    the whole-bag level rather than being localized to specific instances within the
    bag [[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e1e3c037a40d1d871b9c2032c4a2c2fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Illustration of the framework from Zhao et al.’s work [[112](#bib.bib112)]:
    VAE-GAN functions as the instance-level feature extractor. The feature selection
    process identifies and selects discriminative instance-level features. A Graph
    Convolutional Network (GCN) is employed to synthesize the selected instance-level
    features, responsible for generating bag representations and performing the final
    classification (image adapted from [[112](#bib.bib112)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bag-based methods primarily vary in three key components: the first being the
    instance-level feature extraction module, the second involving instance-level
    feature selection, and lastly, the method by which the instance features are aggregated
    to produce bag-level features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concerning the instance-level feature extractor, the majority of methods utilize
    CNNs to automatically extract robust features from patches or employ pre-trained
    models [[3](#bib.bib3)]. Recently, there has been an emergence of methods that
    utilize unsupervised learning to extract features at the patch level. In this
    context, [[112](#bib.bib112)] train the feature extractor using a combination
    model that includes both a variational autoencoder and a generative adversarial
    network (VAE-GAN) as shown in Figure [7](#S3.F7 "Figure 7 ‣ 3.1.2 Bag-based methods
    ‣ 3.1 Multiple instance learning ‣ 3 Inexact supervision ‣ Data efficient deep
    learning for medical image analysis: A survey"). Various methods employ a self-supervised
    contrastive learning approach to obtain instance-level feature representations.
    For instance, [[113](#bib.bib113)] uses contrastive predictive coding (CPC) from
    [[114](#bib.bib114)], while [[115](#bib.bib115)] utilizes SimCLR from [[93](#bib.bib93)].
    Additionally, Chikontwe et al. [[116](#bib.bib116)] integrate an unsupervised
    contrastive loss with their proposed MIL method to enhance the learning of instance-level
    features.'
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the feature selection, the high resolution of medical images poses
    a challenge when applying deep Multiple Instance Learning (MIL) methods since
    only a limited number of patches can be selected from these images for MIL. To
    address this, some approaches use techniques such as random patch selection [[117](#bib.bib117)],
    intelligent sampling using weakly supervised discriminator [[118](#bib.bib118)]
    and discriminative patch selection [[119](#bib.bib119), [112](#bib.bib112)]. Additionally,
    patch clustering methods [[120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122)]
    have been employed. Patch clustering serves the purpose of ensuring the representativeness
    of the selected patches to a certain degree, as a few patches chosen from a cluster
    can approximately represent the entire cluster. Ultimately, representative clusters
    are utilized to make the final prediction. Sharma et al. [[120](#bib.bib120)]
    employ clustering and sampling on the patch features extracted through the feature
    extractor. Subsequently, they integrate these features using an adaptive attention
    mechanism to facilitate end-to-end training. To enhance the feature space learning,
    Lu et al. [[121](#bib.bib121)] select instances with the highest and lowest attention
    scores within the current bag for clustering. To advance upon these prior techniques,
    Yan et al. [[122](#bib.bib122)] introduce a patch clustering approach based on
    unsupervised and self-supervised learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: For the Bag level representation, pooling methods such as max pooling, average
    pooling, and log-sum-exp pooling [[106](#bib.bib106)] are typically adopted in
    this step. However, these pooling methods are not trainable, which can restrict
    their usefulness. To address this limitation, Ilse et al. [[123](#bib.bib123)]
    introduced a fully trainable approach that uses the attention mechanism to assign
    weights to instances, thus indicating the contribution of individual instances
    to bag classification. This work has spurred a wave of research into attention-based
    aggregation methods [[113](#bib.bib113), [124](#bib.bib124), [2](#bib.bib2), [125](#bib.bib125),
    [115](#bib.bib115)]. Hashimoto et al. [[2](#bib.bib2)] utilized the attention
    mechanism to combine instance features at various resolutions. Li et al. [[115](#bib.bib115)]
    introduced a dual-stream aggregator that relies on masked non-local operations
    for conducting instance-level classification as well as bag-level classification.
    In contrast to the methods mentioned earlier, their model computes attention explicitly
    using a trainable distance measurement. It’s not just important to consider the
    contribution of various instances to bag classification; the relationships among
    these instances should also be fully explored. To address this, several methods
    proposed to use Transformer to aggregate instance features [[126](#bib.bib126),
    [3](#bib.bib3)]. Shao et al. [[3](#bib.bib3)] introduced Vision Transformer (ViT)
    into MIL for gigapixel Whole Slide Images (WSIs) because ViT offers significant
    benefits in capturing long-distance information and correlations among instances
    in a sequence. Wang et al. [[126](#bib.bib126)] aimed to improve lymph node metastasis
    prediction by incorporating a pruned Transformer model into MIL. To address the
    issue of limited samples in the original dataset and prevent overfitting, they
    also developed a knowledge distillation mechanism using data from similar datasets.
    Different from the approaches mentioned above, [[112](#bib.bib112)] work builds
    the bag representation with a Graph Convolutional Network.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Learning with weak annotations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Learning with weak annotations refers to a scenario where the available training
    data is annotated with labels that are less detailed or less precise than what
    might be ideal for a particular medical imaging task. In many MIA applications,
    obtaining precise annotations at a fine-grained level, such as pixel-level annotations,
    can be challenging, or expensive. Weak annotations provide a cost-effective alternative
    with coarser labels. These weak annotations can take various forms, including:
    ([3.2.1](#S3.SS2.SSS1 "3.2.1 Learning with image-level supervision ‣ 3.2 Learning
    with weak annotations ‣ 3 Inexact supervision ‣ Data efficient deep learning for
    medical image analysis: A survey")) Image-level annotations: Only category labels
    are provided for each training image, lacking precise instance-level information.
    ([3.2.2](#S3.SS2.SSS2 "3.2.2 Learning with point annotation ‣ 3.2 Learning with
    weak annotations ‣ 3 Inexact supervision ‣ Data efficient deep learning for medical
    image analysis: A survey")) Point-level annotations: A single specific location
    or coordinate within an image is marked to highlight a key feature. ([3.2.3](#S3.SS2.SSS3
    "3.2.3 Learning with scribble-level supervision ‣ 3.2 Learning with weak annotations
    ‣ 3 Inexact supervision ‣ Data efficient deep learning for medical image analysis:
    A survey")) Scribble-level annotations: A subset of pixels within each training
    image is annotated. ([3.2.4](#S3.SS2.SSS4 "3.2.4 Learning with box-level supervision
    ‣ 3.2 Learning with weak annotations ‣ 3 Inexact supervision ‣ Data efficient
    deep learning for medical image analysis: A survey")) Box-level annotations: Object
    bounding boxes are annotated for each training image, offering coarse localization
    information but not pixel-level accuracy (see Figure [8](#S3.F8 "Figure 8 ‣ 3.2
    Learning with weak annotations ‣ 3 Inexact supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")). In each of these cases, the annotations
    are less detailed or less precise than full pixel-level annotations, which presents
    challenges but also reduces the labeling effort compared to exhaustive pixel-level
    annotation requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9a715b4ca428ef3d567b97d831b8b85b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Illustration of fully supervised mask annotation, weakly supervised
    box annotation, scribble annotation and point annotation (image from [[127](#bib.bib127)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Learning with image-level supervision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we examine approaches that exclusively rely on image-level
    supervision for tasks like image detection and segmentation. It’s worth noting
    that image-level supervision is commonly employed to train models for image classification.
    The challenge here arises from the substantial gap in supervision between the
    high-level information provided by image-level labels and the detailed pixel-level
    predictions required for tasks like detection and segmentation [[128](#bib.bib128)].
    In most cases, the Class Activation Maps (CAMs) [[129](#bib.bib129)] are commonly
    used as the standard approach for producing initial regions of interest using
    classification models. Essentially, CAMs leverage prior of cross-label constraints
    to identify these initial regions within an image based on the information derived
    from a classification model. Nonetheless, the accuracy of localizing using CAMs
    is relatively limited. To tackle this challenge, researchers have devised multiple
    strategies aimed at enhancing CAMs to enable tasks such as segmentation with only
    image-level supervision. For example, Li et al. [[130](#bib.bib130)] introduce
    an approach named CAM-deep level set (CAM-DLS). In this method, they integrate
    the DLS loss into the classification loss during the training of the classification
    network. This DLS loss leverages CAMs to emphasize regions within breast tumors.
    Similarly, Chen et al. [[131](#bib.bib131)] present a causal CAM approach for
    organ segmentation. This method employs the concept of causal inference, incorporating
    a category-causality chain and an anatomy-causality chain.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Learning with point annotation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Point annotation involves marking a single specific location or coordinate
    within an image to indicate a key feature or point of interest. Some works [[132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134)] concentrate on employing extreme points
    as annotations for accomplishing pixel-level segmentation. Specifically, Khan
    et al. [[132](#bib.bib132)] investigate a method designed to extract information
    from extreme points and create a confidence map. This map serves as a guide for
    neural networks to comprehend the precise object location within the boundaries
    set by the extreme points. Similarly, Roth et al. [[133](#bib.bib133)] utilize
    a network that takes two types of input: an image channel and a point channel
    representing user-defined extreme points. This point channel is subsequently integrated
    into the network to provide additional guidance during segmentation training.
    Specifically, it is used as an extra input for attention gates and is incorporated
    into the loss function, effectively enhancing the segmentation process. Nevertheless,
    these methods demand annotators to identify the object’s boundary, a task that
    remains labor-intensive in practical applications. In comparison, some methods
    [[135](#bib.bib135), [136](#bib.bib136), [127](#bib.bib127)] employ center point
    annotation to accomplish pixel-level segmentation. To achieve this, certain studies
    employ the Voronoi diagram [[137](#bib.bib137)] and clustering algorithms to create
    initial coarse pixel-level labels. Subsequently, various techniques are applied
    to enhance the segmentation outcomes, including iterative optimization [[135](#bib.bib135)]
    and co-training [[136](#bib.bib136), [138](#bib.bib138)]. Zhao et al. [[127](#bib.bib127)]
    employ a framework that combines self-training and co-training to address cell
    segmentation. They introduce a divergence loss to mitigate overfitting and a consistency
    loss to ensure agreement among multiple co-trained networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Overview of recent methods in *Inexact supervision* category.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Task | Algorithm Design | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| [[2](#bib.bib2)] | Cancer subtype classification | Domain Adversarial + Multi-scale
    MIL | Private Dataset: 196 Images | Acc: 0.871 |'
  prefs: []
  type: TYPE_TB
- en: '| [[117](#bib.bib117)] | Colorectal cancer staging, | Graph Attention MIL |
    MCO | Acc: 0.811; F1: 0.798 |'
  prefs: []
  type: TYPE_TB
- en: '| [[3](#bib.bib3)] | Whole slide image classification | Transformer-based MIL
    | CAMELYON 2016; TCGA-NSCLC; TCGA-RCC | Acc: CAMELYON: 0.8837; TCGA-NSCLC: 0.8835;
    TCGA-RCC: 0.9466 |'
  prefs: []
  type: TYPE_TB
- en: '| [[126](#bib.bib126)] | Lymph node metastasis prediction | Transformer-based
    MIL + Knowledge Distillation | Private Dataset: 595 Images | AUC: 0.9835; P: 0.9482;
    R: 0.9151; F1: 0.9297 |'
  prefs: []
  type: TYPE_TB
- en: '| [[139](#bib.bib139)] | Histopathology whole slide image classification |
    Double-Tier Feature Distillation MIL | CAMELYON 2016; TCGA-Lung | CAMELYON 2016:
    AUC: 0.946; TCGA-Lung: AUC: 0.961 |'
  prefs: []
  type: TYPE_TB
- en: '| [[105](#bib.bib105)] | Chest X-rays classification | Jointly Classification
    and Localization | RSNA-Lung; MIMIC-CXR; Private Dataset: 1,003 Images | AUC:
    0.93 |'
  prefs: []
  type: TYPE_TB
- en: '| [[110](#bib.bib110)] | Breast cancer classification | Quantile Function-based
    MIL | CBCS3 | Acc: 0.952 |'
  prefs: []
  type: TYPE_TB
- en: '| [[123](#bib.bib123)] | Cancer classification | Attention-based MIL | TMA-UCSB;
    CRCHistoPhenotypes | TMA-UCSB: Acc: 0.755; CRCHistoPhenotypes: Acc: 0.898 |'
  prefs: []
  type: TYPE_TB
- en: '| [[124](#bib.bib124)] | Pancreatic ductal adenocarcinoma classification and
    segmentation | Jointly Global-level Classification and Local-level Segmentation
    | Private Dataset: 800 Images | DSC: 0.6029; Sens: 0.9975 |'
  prefs: []
  type: TYPE_TB
- en: '| [[7](#bib.bib7)] | Detection of lymph node metastases | Hybrid MIL | MSK
    breast cancer | AUC: 0.965 |'
  prefs: []
  type: TYPE_TB
- en: '| [[140](#bib.bib140)] | Breast Cancer (HER2 scoring: negative, equivocal and
    positive) | Hybrid MIL | Private dataset: 1105 cases | Accuracy: 0.8970 |'
  prefs: []
  type: TYPE_TB
- en: '| [[130](#bib.bib130)] | Breast tumor segmentation | CAM + Level-Set | Private
    dataset: 3062 BUS images | DSC: fat 0.830 ± 0.118; mammary gland 0.843 ± 0.100;
    muscle 0.807 ± 0.154; thorax layers 0.910 ± 0.114 |'
  prefs: []
  type: TYPE_TB
- en: '| [[131](#bib.bib131)] | Segmentation | Causal Inference; CAM | ACDC; ProMRI;
    CHAOS | ProMRI DSC: 0.864±0.004; ASD: 3.86±1.20; MSD: 3.85±1.33 Abdominal Organ
    ACDC DSC: 0.875±0.008; ASD: 1.62±0.41; MSD: 1.17±0.24 CHAOS DSC: 0.781 |'
  prefs: []
  type: TYPE_TB
- en: '| [[132](#bib.bib132)] | Multi-organ segmentation | Confidence Map Supervision
    | SegTHOR | DSC Aorta: 0.9441 ± 0.0187; Esophagus 0.8983 ± 0.0416 |'
  prefs: []
  type: TYPE_TB
- en: '| [[133](#bib.bib133)] | Multi-organ segmentation | Random Walker + Iterative
    Training | BTCV; MSD; CT-ORG | MO-Liver 0.956 ± 0.010; MO-Pancreas 0.747 ± 0.082;
    DSC: MSD-spleen 0.958 ± 0.007; MO-Spleen 0.954 ± 0.027 |'
  prefs: []
  type: TYPE_TB
- en: '| [[134](#bib.bib134)] | Brain tumor segmentation | CNN + CRF | Vestibular-Schwannoma-SEG
    | DSC: 0.819±0.080; HD95: 3.7±7.4; P: 0.929±0.059 |'
  prefs: []
  type: TYPE_TB
- en: '| [[136](#bib.bib136)] | Multi-organ segmentation | Co-/Self-Training | MoNuSeg;
    CPM | MoNuSeg DSC: 0.7441; AJI: 0.5620; CPM DSC: 0.7337; AJI: 0.5132 |'
  prefs: []
  type: TYPE_TB
- en: '| [[127](#bib.bib127)] | Cell segmentation | Self-/Co-/Hybrid-Training | PHC;
    Phase100 | DSC PHC: 0.871; Phase 100: 0.811 |'
  prefs: []
  type: TYPE_TB
- en: 3.2.3 Learning with scribble-level supervision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we examine techniques related to scribble-based supervision,
    where annotations are given for a limited number of pixels, often in the form
    of manually drawn scribbles. These scribbles essentially act as seed regions.
    The key challenge is to extend semantic information from these sparsely annotated
    scribbles to all other pixels that lack labels. Some approaches address this challenge
    by aiming to expand the scribbles or reconstruct the complete mask for model training
    [[141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143)]. Nevertheless, the
    iterative training necessary for the pixel-relabeling process is time-consuming
    and susceptible to the introduction of noisy labels. To eliminate the necessity
    for relabeling, several approaches have utilized conditional random fields for
    refining segmentation results, either in post-processing [[144](#bib.bib144)]
    or as a trainable layer [[145](#bib.bib145)]. Specifically, Can et al. [[144](#bib.bib144)]
    use region growing to create seed areas. They apply a random walk-based segmentation
    method that generates per-pixel probability maps for each label, assigning values
    only when the probability exceeds a specific threshold. However, these methods
    failed to provide more effective guidance for model training. Conversely, alternative
    techniques [[146](#bib.bib146), [147](#bib.bib147)] introduced new modules to
    assess the quality of segmentation masks, thereby encouraging the generation of
    realistic predictions. For instance, Gabriele et al. [[147](#bib.bib147)] proposed
    an adversarial training and an attention gating mechanism to produce segmentation
    masks, leading to enhanced object localization across multiple resolutions, while
    Zhang et al. [[148](#bib.bib148)] leveraged the PatchGAN discriminator to incorporate
    shape priors. However, these methods required additional data source of complete
    masks. On the other hand, Zhang et al. [[149](#bib.bib149)] utilize mix augmentation
    and cycle consistency within the Scribble-Pixel approach. This demonstrates enhancements
    in both weakly and fully supervised segmentation methodologies. Several studies
    utilize consistency learning for scribble-based supervision [[150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152)]. Scribble2Label [[152](#bib.bib152)] combines
    guidance signals from scribble annotations and pseudo labels using exponential
    moving averages for cell segmentation. Based on the teacher-student framework,
    Gao et al. [[150](#bib.bib150)] propose SOUSA, where the student model receives
    weak supervision through scribbles and a Geodesic distance map created from those
    scribbles. Simultaneously, a substantial volume of unlabeled data containing different
    forms of perturbations is provided to both the student and teacher models. The
    alignment of their output predictions is enforced using a combination of Mean
    Square Error (MSE) loss and a Multi-angle Projection Reconstruction (MPR) loss.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Learning with box-level supervision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we evaluate approaches for semantic segmentation guided by
    box-level supervision. Utilizing box-level supervision proves to be a more robust
    substitute for image-level guidance, as it inherently reduces the exploration
    area for object detection. For object segmentation, Rajchl et al. [[153](#bib.bib153)]
    recover pixel-wise annotations given a database of images with corresponding bounding
    boxes. To achieve this goal, they devise an iterative energy minimization problem
    within a densely connected conditional random field framework to adjust and refine
    the parameters of a CNN model throughout the iterative process. Wang et al. [[154](#bib.bib154)]
    utilize MIL and a smooth maximum approximation method based on the concept of
    bounding box tightness. In this context, bounding box tightness implies that an
    object instance should have contact with all four sides of its bounding box. Consequently,
    if there is a vertical or horizontal crossing line within the box, it results
    in a positive bag classification because it covers at least one foreground pixel.
    In the work presented by [[155](#bib.bib155)], they introduce a fusion filter
    sampling (FFS) module designed to create pixel-level pseudo labels from box annotations
    while minimizing noise.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Incomplete supervision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Incomplete supervision refers to a scenario where we have access to a limited
    quantity of labeled data, which is inadequate for training an effective learner,
    while there exists a large pool of unlabeled data. We categorize incomplete supervision
    into three broad subcategories: Semi-supervised Learning, Active Learning, and
    Domain-adaptive Learning (Figure [9](#S4.F9 "Figure 9 ‣ 4 Incomplete supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). Semi-supervised
    learning aims to enhance learning performance by leveraging both labeled and unlabeled
    data automatically. In Domain-adaptive Learning, a domain shift occurs between
    labeled and unlabeled data. Conversely, Active learning operates on the assumption
    that there is an *oracle*, like a human expert, who can be consulted to obtain
    ground-truth labels for specific unlabeled instances. A summary of recent methods
    for learning with incomplete supervision is provided in Table [4](#S4.T4 "Table
    4 ‣ 4.3.6 Hybrid methods ‣ 4.3 Domain-adaptive learning ‣ 4 Incomplete supervision
    ‣ Data efficient deep learning for medical image analysis: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7596483b93ff04cf9c229c59220c2e44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Taxonomy of *Incomplete Supervision* methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Semi-supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will examine techniques used in semi-supervised learning
    (Semi-SL). In this approach, only a small portion of the training images have
    annotations, while the majority of training images remain unannotated. The goal
    of semi-supervised learning is to incorporate the vast number of unlabeled training
    images into the training process in order to enhance model performance [[156](#bib.bib156),
    [157](#bib.bib157)]. Semi-supervised Learning can be categorized into Consistency
    regularization, Generative, Pseudo-labeling, and Hybrid methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Consistency regularization methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consistency regularization methods rely on the concept of smoothness or manifold
    assumption, suggesting that perturbing data points should not alter the model’s
    predictions. Importantly, this approach does not rely on label information, making
    it an effective constraint for learning from unlabeled data. Within this framework,
    various perturbations are available and can be classified into two categories:
    input perturbations and feature map perturbations. These perturbations must be
    relevant and meaningful for the specific task at hand. Commonly employed input
    perturbations encompass random rotation, Gaussian blurring, Gaussian noise, contrast
    variations, and scaling. Notably, Bortsova et al. [[158](#bib.bib158)] and Li
    et al. [[159](#bib.bib159)] employ consistency learning by applying different
    transformations to input images. Another widely adopted form of consistency is
    mix-up consistency [[160](#bib.bib160), [161](#bib.bib161)], where the segmentation
    of interpolation of two inputs is encouraged to remain consistent with the interpolation
    of segmentation results for those inputs. Moreover, recent investigations by [[162](#bib.bib162)]
    and [[163](#bib.bib163)] delve into perturbations at the feature map level. Zheng
    et al. [[162](#bib.bib162)] propose a method that introduces random noise into
    the parameter calculations of the teacher model. Li et al. [[163](#bib.bib163)]
    introduce seven distinct feature perturbations, each associated with an additional
    decoder, all conditioned on maintaining consistency with the primary decoder.
    Furthermore, there are studies that simultaneously apply perturbations at both
    the input and feature map levels [[164](#bib.bib164), [165](#bib.bib165)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to incorporating perturbations, alternative consistency learning
    techniques are also available. For instance, the $\pi$-model [[166](#bib.bib166)]
    is a straightforward yet powerful approach that utilizes a shared encoder to generate
    various views of the input sample through augmentation. It enforces the classifier
    to provide consistent predictions for different augmentations of the same input.
    Simultaneously, the training process incorporates label information to enhance
    the classifier’s overall performance. Li et al. [[167](#bib.bib167)] developed
    a semi-supervised algorithm for skin lesion segmentation based on the $\pi$-model
    approach. Temporal ensembling [[168](#bib.bib168)] was created with the aim of
    enhancing the prediction stability of the $\pi$-model. This is achieved by incorporating
    an exponentially moving average module to update predictions. Several researchers
    have adopted this module to tackle MIA related challenges [[169](#bib.bib169),
    [170](#bib.bib170)]. To achieve precise breast mass segmentation, Cao et al. [[169](#bib.bib169)]
    incorporate uncertainty into the temporal ensembling model. They utilize uncertainty
    maps as guidance for the neural network to ensure the reliability of the generated
    predictions. Likewise, Luo et al. [[170](#bib.bib170)] suggest an uncertainty-aware
    temporal ensembling method for chest X-ray disease screening. In the training
    process of temporal ensembling, the activation of each training sample is updated
    only once in one epoch. Mean teacher (MT) [[171](#bib.bib171)] overcomes this
    limitation by applying exponentially moving average on model parameters instead
    of network activations. Several methods enhance the MT framework for its application
    in MIA contexts [[172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175)].
    To enhance the performance of the MT, Yu et al. [[172](#bib.bib172)] introduced
    the Uncertainty-Aware Mean Teacher (UA-MT) framework (see Figure [10](#S4.F10
    "Figure 10 ‣ 4.1.1 Consistency regularization methods ‣ 4.1 Semi-supervised learning
    ‣ 4 Incomplete supervision ‣ Data efficient deep learning for medical image analysis:
    A survey")) for 3D left atrium segmentation. In this approach, the teacher model,
    in addition to producing target outputs, also assesses the uncertainty associated
    with each target prediction using Monte Carlo sampling. This allows the removal
    of unreliable predictions, retaining only those with low uncertainty for consistency
    loss calculations. This process offers more reliable guidance to the student model,
    promoting the teacher model to produce higher-quality target predictions. Wang
    et al. [[174](#bib.bib174)] incorporated multi-task learning into the mean teacher
    framework including segmentation, reconstruction, and SDF prediction tasks to
    enhance data, model, and task consistency. Additionally, they introduced an uncertainty-weighted
    integration (UWI) approach to assess uncertainty across all tasks and created
    a triple-uncertainty method to guide the student model to learn reliable information
    from the teacher.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Xu et al. [[176](#bib.bib176)] present a dual uncertainty-guided mixing
    consistency network for precise 3D semi-supervised segmentation, emphasizing the
    consideration of context information at the volume level. To segment surgical
    images, Lou et al. [[177](#bib.bib177)] propose a Min-Max Similarity (MMS) method.
    This approach adopts a dual-view training strategy, utilizing classifiers and
    projectors to construct pairs of all-negative features and positive/negative feature
    pairs. This formulation transforms the learning process into solving an MMS problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26fed374465422e319bf966f5be17d4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Illustration of the Uncertainty-Aware Mean Teacher (UA-MT) framework.
    The student model is trained by minimizing the supervised loss $L_{s}$ on labeled
    data and the consistency loss $L_{c}$ on both unlabeled and labeled data. The
    teacher model’s estimated uncertainty is used to instruct the student in learning
    from the more dependable teacher-provided targets (image courtesy of Yu et al.
    [[172](#bib.bib172)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Generative methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The generative adversarial network (GAN) has shown potential performance on
    semi-supervised learning [[178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180)].
    GANs consist of two main parts: a generator and a discriminator. The generator’s
    goal is to deceive the discriminator by producing fake data that appears real,
    while the discriminator aims to distinguish between real and synthetic data (see
    Figure [11](#S4.F11 "Figure 11 ‣ 4.1.2 Generative methods ‣ 4.1 Semi-supervised
    learning ‣ 4 Incomplete supervision ‣ Data efficient deep learning for medical
    image analysis: A survey")(B)). These two networks engage in a zero-sum game,
    where any gain made by one network comes at the expense of the other. There are
    different ways to use GANs in Semi-SL settings. One such approach involves employing
    adversarial techniques to encourage the outputs of unlabeled images to closely
    resemble those of the labeled images [[181](#bib.bib181), [182](#bib.bib182)].
    Peiris et al. [[182](#bib.bib182)] incorporate a critic network into their segmentation
    architecture. This network engages in a min-max game by distinguishing between
    the predicted masks and the actual ground truth masks. The outcomes of their experiments
    indicate that this approach can enhance the definition of boundaries in the prediction
    masks. Additionally, the discriminator can be employed to generate pixel-wise
    confidence maps, facilitating the selection of reliable pixel predictions for
    consistency learning. The study by Wu et al. [[179](#bib.bib179)] introduces a
    pair of discriminators to anticipate confidence maps and differentiate between
    segmentation outcomes originating from labeled or unlabeled data. Constrained
    Adversarial Training (CAT) [[180](#bib.bib180)] focuses on generating anatomically
    accurate segmentations. This method incorporates unlabeled samples into an adversarial
    training framework, which serves to regularize the network and facilitate constraint
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hou et al. [[183](#bib.bib183)] use a GAN-based framework with three enhancements:
    First, a U-Net style network is employed as the discriminator. Second, a *polluted
    discriminator* is introduced, incorporating auxiliary *leaking links* from the
    generator to encourage the generation of moderate, though unrealistic, samples,
    thereby enhancing semi-supervised learning. Third, the discriminator undergoes
    regularization via the mean-teacher mechanism, enhancing segmentation generalization
    through input and weight perturbations. Certain approaches employ GANs as a method
    for data augmentation within the context of Semi-SL. For instance, Chaitanya et
    al. [[184](#bib.bib184)] integrate unlabeled data directly into GAN’s adversarial
    training process to enhance the generator’s performance for improving medical
    data augmentation. They assert that incorporating unlabeled samples enables greater
    diversity in terms of shape and intensity, thereby enhancing the model’s robustness
    and guiding the optimization process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Variational Autoencoder (VAE) [[185](#bib.bib185)] consists of two main components:
    an encoder that transforms input data into a latent representation and a decoder
    that reconstructs the latent representation into the original data space. In order
    to regularize the encoder of the VAE, a prior over the latent distribution is
    commonly introduced (see Figure [11](#S4.F11 "Figure 11 ‣ 4.1.2 Generative methods
    ‣ 4.1 Semi-supervised learning ‣ 4 Incomplete supervision ‣ Data efficient deep
    learning for medical image analysis: A survey")(A)). As one of the initial attempts
    to apply VAE to semi-supervised segmentation tasks, Sedai et al. [[178](#bib.bib178)]
    employed a dual-VAE approach for segmenting the optic cup in retinal fundus images.
    This method involved two VAEs, where one VAE learned the data distribution from
    unlabeled data and transferred its acquired knowledge to the other VAE responsible
    for segmentation using labeled data. Wang et al. [[186](#bib.bib186)] extended
    the VAE architecture to 3D medical image segmentation by introducing a mean vector
    and covariance matrix to account for correlations across different slices within
    an input volume.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2c6d142286bf8133c4d977a61207f2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Illustration of VAE and GAN architectures: (A) In the VAE architecture,
    there is an encoder-decoder structure. Here, $Z_{\mu}$ represents the mean vector,
    $Z_{\sigma}$ denotes the standard deviation vector, and $Z$ is the sampled latent
    vector. (B) In the GAN architecture, there are both a generator and a discriminator
    (image courtesy of [[187](#bib.bib187)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Pseudo-labeling methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In pseudo-labeling, a model is trained on the available labeled data. It then
    predicts labels for unlabeled samples with high confidence, effectively creating
    pseudo-labels. Finally, the model is retrained using both the labeled data and
    these newly generated pseudo-labeled samples, improving its performance through
    the utilization of additional unlabeled data. Pseudo-labeling methods can be mainly
    categorized into two sub-categories: Self-training methods and Co-training learning
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/acd2bb5dbf73c41d177ef62f6da9235f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Illustration of the Inf-net framework: CT images are initially processed
    through two convolutional layers for the extraction of high-resolution (i.e.,
    low-level) features. An edge attention module is incorporated to enhance the representation
    of boundaries within the region of interest. Subsequently, the obtained low-level
    features, denoted as $f_{2}$, undergo three convolutional layers to extract high-level
    features. These high-level features serve two primary purposes. Firstly, they
    are used to feed a parallel partial decoder (PPD), which aggregates these features
    and generates a global map denoted as $S_{g}$. This global map aids in the coarse
    localization of lung infections. Secondly, these high-level features, along with
    $f_{2}$, are directed through multiple cascaded reverse attention (RA) modules
    under the guidance of $S_{g}$. The RA module $R_{4}$ depends on the output of
    another RA module, $R_{5}$. Finally, the output of the last RA module, denoted
    as $S_{3}$, is passed through a sigmoid activation function for the final prediction
    of lung infection regions (image from [[188](#bib.bib188)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-training models: In the self-training framework, an initial model is trained
    using limited labeled data. Then, this initial model is utilized to generate pseudo
    labels for the unlabeled data. Subsequently, the labeled dataset is combined with
    the pseudo-labeled dataset to update the initial model. The training process iteratively
    alternates between these two steps until a predetermined number of iterations
    is reached. Self-training approaches primarily vary in terms of model initialization,
    pseudo label generation, and their strategies for addressing pseudo label noise.
    According to the study by [[189](#bib.bib189)], pseudo labels with higher confidence
    tend to be more effective. Consequently, various methods that take into account
    confidence or uncertainty in pseudo labels have been introduced to generate more
    consistent and reliable pseudo labels, such as refining pseudo labels through
    conditional random fields [[190](#bib.bib190)], uncertainty-aware confidence evaluation
    [[191](#bib.bib191)]. Similarly, Ke et al. [[192](#bib.bib192)] proposed a three-stage
    self-training framework to refine pseudo labels in a stage-wise manner. It reduces
    the uncertainty in the predicted probability for the pseudo-masks using a multi-task
    model. Inf-net [[188](#bib.bib188)] addresses the shortage of well-annotated data
    for segmentation of COVID-19 lung infections in CT images. Further, a parallel
    partial decoder (PPD), reverse attention (RA), and edge attention were further
    added to improve the performance of the model, as shown in Figure [12](#S4.F12
    "Figure 12 ‣ 4.1.3 Pseudo-labeling methods ‣ 4.1 Semi-supervised learning ‣ 4
    Incomplete supervision ‣ Data efficient deep learning for medical image analysis:
    A survey"). In contrast to conventional pseudo-labeling techniques, which rely
    on a threshold to pick confidently classified samples, Liu et al. [[193](#bib.bib193)]
    propose the Anti-Curriculum Pseudo-labeling (ACPL) method. ACPL utilizes a mechanism
    known as *cross-distribution sample informativeness* to identify highly informative
    unlabeled samples for pseudo-labeling. It also employs an ensemble of classifiers
    to generate precise pseudo-labels. This approach enables ACPL to effectively handle
    multi-class and multi-label imbalanced classification issues in the field of MIA.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Chen et al. [[194](#bib.bib194)] introduced a teacher-student framework
    for multi-organ segmentation in CT scans. They proposed a learning paradigm involving
    $N^{3}$ small cubes extracted from each CT scan, called magic-cubes. Two data
    augmentation strategies were designed. First, labeled and unlabeled data cubes
    were mixed to teach unlabeled data organ semantics in their relative positions.
    Second, for smaller organs, data cubes were shuffled and fed into the student
    network. Finally, the original magic-cubes were reconstructed to align with the
    ground-truth or teacher’s supervision. Further, the teacher network’s predicted
    pseudo labels are improved by blending them with the learned representations of
    the small cubes. This blending strategy considers local attributes like texture,
    luster, and boundary smoothness, addressing the lower performance observed for
    smaller organs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Co-training models: In the Co-training framework [[195](#bib.bib195)], a model
    is trained on a dataset with two or more views or representations of the data.
    These views are typically different but complementary. The key idea is that if
    each view provides unique information about the data, the model can learn more
    effectively from the combined knowledge of all views. In contrast to the self-training
    framework, which expands the labeled dataset based on a single model’s confidence,
    co-training iteratively selects instances on which the model is confident based
    on different views, expanding the labeled dataset with complementary information.
    The essence of co-training lies in the process of creating two or more deep models
    that can effectively capture distinct and nearly independent perspectives. These
    approaches typically involve utilizing diverse data sources, implementing various
    network architectures, and applying specialized training techniques to acquire
    a range of diverse deep models [[156](#bib.bib156)]. In the context of medical
    images, data can originate from various modalities or medical centers, resulting
    in distinct distributions. In this regard, [[196](#bib.bib196)] and [[197](#bib.bib197)]
    make use of different views derived from diverse modalities within the co-training
    framework. Some approaches employ different network architectures as distinct
    views. For instance, Luo et al. [[198](#bib.bib198)] propose cross-teaching between
    CNN and Transformer models, which implicitly promotes consistency and complementarity
    between these distinct networks. Peng et al. [[199](#bib.bib199)] generate adversarial
    examples as an alternative view. Similarly, for 3D images, Zhao et al. [[200](#bib.bib200)]
    utilize coronal, sagittal, and axial views of images as diverse input views. Recently,
    Wang et al. [[201](#bib.bib201)] address the issue of imbalanced class distribution
    in Semi-SL methods using the Dual-debiased Heterogeneous Co-training (DHC) framework.
    They introduce two loss weighting techniques called Distribution-aware Debiased
    Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW). These strategies
    utilize pseudo labels dynamically to help the model address data and learning
    biases effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Hybrid models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An emerging area of research in Semi-SL involves integrating the previously
    mentioned methods into a unified framework to achieve improved performance. These
    combined approaches are referred to as hybrid methods [[202](#bib.bib202), [203](#bib.bib203),
    [204](#bib.bib204)]. Several studies have explored the combination of pseudo-labeling
    and contrastive learning methods [[205](#bib.bib205), [206](#bib.bib206), [207](#bib.bib207),
    [208](#bib.bib208)] for different tasks. Specifically, both Chaitanya et al. [[205](#bib.bib205)]
    and Basak et al. [[206](#bib.bib206)] introduce a self-training method based on
    local contrastive learning, guided by pseudo-labels, and demonstrate its effectiveness
    across various medical segmentation datasets. For COVID-19 Screening and Lesion
    Segmentation, Zeng et al. [[208](#bib.bib208)] present a double-threshold pseudo-labeling
    approach and a novel inter-slice consistency regularization technique designed
    specifically for CT images. Wang et al. [[202](#bib.bib202)] utilize self-training
    with consistency regularization to efficiently extract valuable information from
    unlabeled data, and they incorporate virtual adversarial training to enhance the
    model’s generalization capability. ASE-Net [[209](#bib.bib209)] comprises segmentation
    networks and a discriminator network. The segmentation network is constructed
    using the MT framework, while the discriminator network employs an adversarial
    consistency training strategy (ACTS) with two discriminators focused on consistency
    learning. This strategy helps establish prior relationships between labeled and
    unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Active learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Active learning (AL) [[210](#bib.bib210)] operates on the assumption that the
    ground-truth labels of unlabeled instances can be obtained by querying an expert
    annotators (see Figure [13](#S4.F13 "Figure 13 ‣ 4.2 Active learning ‣ 4 Incomplete
    supervision ‣ Data efficient deep learning for medical image analysis: A survey")).
    Assuming that the labeling cost is solely based on the number of queries, the
    objective of active learning is to minimize the number of queries required while
    still achieving effective model training with minimized labeling costs. In situations
    where there is a limited set of labeled data but an abundance of unlabeled data,
    active learning aims to identify the most valuable unlabeled instance for querying.
    There are two commonly used selection criteria: informativeness and representativeness.
    Informativeness assesses how effectively an unlabeled instance reduces the uncertainty
    of a statistical model, while representativeness calculates how well an instance
    represents the structure of input patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8aebfa067709b452a760b2f9cdb002a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Overview of the active learning paradigm: In a cycle, a deep learning
    model is trained on a labeled medical dataset. Then, active sampling strategies
    are implemented to select the data that is most valuable to the model from an
    unlabeled medical dataset. Finally, oracles are used to annotate the selected
    data. Image courtesy of Peng and Wang [[211](#bib.bib211)].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Evaluating informativeness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The primary category of informativeness measures revolves around calculating
    uncertainty. The key idea is that including the ground truth for samples with
    higher uncertainty in the training set can provide more valuable information.
    In the deep learning area, uncertainty-based sampling has seen widespread usage
    in recent active learning methods [[212](#bib.bib212), [213](#bib.bib213), [214](#bib.bib214)].
    Specifically, Wen et al. [[213](#bib.bib213)] introduce an active learning approach
    that employs uncertainty sampling to facilitate quality control of nucleus segmentation
    in pathology images. Wu et al. [[214](#bib.bib214)] use both the network loss
    and diversity condition as the uncertainty metric for sampling from a loss prediction
    network. They apply this method to the COVID-19 classification task. Zhou et al.
    [[215](#bib.bib215)] introduce the concept of active selection policies, where
    the highest confidence is determined based on the entropy and diversity of the
    sampled data in the mean prediction outcomes. For classifying radiology images,
    Balram et al. [[216](#bib.bib216)] introduce an integrated end-to-end solution
    that merges consistency-driven semi-supervised learning with uncertainty-guided
    active learning, aiming to alleviate the need for extensive manual annotations.
    Another prevalent approach for estimating informativeness involves assessing the
    agreement among various models executing the same task. The reasoning is that
    greater disagreement observed between predictions on similar data points indicates
    a higher degree of uncertainty. These techniques are commonly employed in situations
    where ensembling is utilized to enhance performance. Ensembling involves training
    multiple models to execute the same task with slight variations in parameters
    or settings [[217](#bib.bib217), [218](#bib.bib218)]. Beluch Bcai et al. [[218](#bib.bib218)]
    showcase the effectiveness of ensembles in active learning and compare them to
    alternative approaches. Kuo et al. [[217](#bib.bib217)] employed an ensemble technique
    to assess uncertainty in the context of intracranial hemorrhage segmentation,
    utilizing the Jensen-Shannon divergence. Additionally, they made an effort to
    predict the time required for manual delineation using a log-linear model. Their
    approach involved selecting examples for manual segmentation based on maximizing
    the cumulative uncertainty within a specified time constraint. Atzeni et al. [[219](#bib.bib219)]
    adopt an iterative method, requesting manual delineation for a single Region of
    Interest (ROI) on a single slice per iteration rather than labeling all structures
    within a slice or volume. They update a segmentation CNN that generates dense
    segmentations for all slices using mixed-cross entropy loss, effectively utilizing
    partially annotated images. Similar to Kuo et al. [[217](#bib.bib217)], they use
    tracing time, based on boundary length, as a practical measure of effort. However,
    in contrast to Kuo et al. [[217](#bib.bib217)], they also account for multiple
    ROIs and their spatial relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian neural networks have gained significant interest due to their capacity
    to represent and propagate the probability of deep learning models. Gal et al.
    [[220](#bib.bib220)] introduce the concept of using Bayesian CNNs for AL, specifically
    employing *Bayesian Active Learning by Disagreement* (BALD). Their study demonstrates
    the superior performance of Bayesian CNNs compared to deterministic CNNs within
    the context of AL. Mahapatra et al. [[221](#bib.bib221)] employ a conditional
    GAN to generate chest X-ray images based on a real image. Additionally, they use
    a Bayesian neural network to assess the informativeness of each generated sample,
    determining whether it should be utilized as training data. If selected, the sample
    is used to fine-tune the network. Their study demonstrates that this method achieves
    comparable performance to training on fully labeled data, even when working with
    a dataset where only 33 % of the pixels in the training set have annotations.
    This provides significant time, effort, and cost savings for annotators. Dai et
    al. [[222](#bib.bib222)] proposed a distinctive method for brain tumor segmentation.
    Instead of traditional approaches, they adopted a novel strategy to select the
    most informative example. This involved moving through the image space along the
    gradient direction of the Dice loss and identifying the nearest neighbor of this
    image within a lower-dimensional latent space, which was learned using a variational
    autoencoder. Certain studies address the challenge of the cold start problem in
    Active Learning, which pertains to the initial selection of images for labeling
    when no labeled data is available as a starting point [[223](#bib.bib223), [224](#bib.bib224)].
    Nath et al. [[223](#bib.bib223)] address the issue of cold start by introducing
    a proxy task and subsequently leveraging the uncertainty generated from this proxy
    task to prioritize the annotation of unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Representativeness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'These approaches go beyond relying solely on uncertainty-based methods and
    instead focus on evaluating the diversity within chosen samples to minimize repetitive
    annotations. By introducing a representativeness measure, these strategies aim
    to promote the selection of samples from various areas of the distribution, leading
    to greater sample diversity and ultimately enhancing the performance of AL. To
    this end, Yang et al. [[225](#bib.bib225)] introduce Suggestive Annotation, a
    deep active learning framework designed for medical image segmentation. This framework
    utilizes a different approach to uncertainty sampling and incorporates a form
    of representativeness density weighting. The method involves training multiple
    models, each of which excludes a portion of the training data. These models are
    then leveraged to calculate an ensemble-based uncertainty measure. Ozdemir et
    al. [[226](#bib.bib226)] create a Bayesian network and utilize Monte Carlo dropout
    to derive variance information as a measure of model uncertainty. In addition,
    they employ infoVAE [[227](#bib.bib227)] to build a representativeness metric,
    which aids in the selection of samples through maximum likelihood sampling within
    the latent space. Li et al. [[228](#bib.bib228)] adopt k-means clustering and
    curriculum classification (CC) techniques, leveraging CurriculumNet [[229](#bib.bib229)],
    to estimate uncertainty and representativeness in their approach. Li et al. [[224](#bib.bib224)]
    tackle the challenge of the cold start problem by employing representativeness
    sampling that relies on the distance matrix to choose an initial dataset that
    is representative. They also introduce a hybrid sample selection approach that
    incorporates pixel entropy, region consistency, and image diversity scores to
    filter the samples. These three scores reflect informativeness at different levels:
    pixel, region, and image. This strategy, which combines these three levels of
    scores, proves to be more effective in selecting the most valuable samples compared
    to using a simple pixel uncertainty score alone. Wang et al. [[230](#bib.bib230)]
    utilize model ensembles to guide user labeling, focusing on cells that optimize
    a blend of uncertainty, diversity (evaluated using a clustering algorithm), and
    representativeness assessed through cosine similarity of features.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Domain-adaptive learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Domain adaptive learning, also known as domain adaptation [[231](#bib.bib231)],
    is a learning paradigm focused on improving the performance of a model on a target
    domain by leveraging knowledge learned from a source domain. In this context,
    a domain refers to a specific distribution of data, which can vary in terms of
    characteristics like data collection settings, sensor types, lighting conditions,
    or other factors that affect the data’s distribution. The main challenge addressed
    by domain adaptive learning is the domain shift problem. This problem arises when
    there is a mismatch between the source domain (where the model is trained) and
    the target domain (where the model needs to perform well). Due to this mismatch,
    a model trained on one domain may not generalize effectively to another domain.
    Domain adaptive learning methods aim to bridge this gap by adapting the model
    to the target domain. Unsupervised Domain Adaptation (UDA) is a specific case
    of domain adaptation where you only have labeled data in the source domain and
    no labeled data in the target domain. The adaptation process is entirely unsupervised,
    meaning it relies solely on unlabeled data in the target domain. These methods
    encompass various approaches, including feature alignment, image translation-based
    methods, learning disentangled representations, pseudo-labeling approaches, self-supervision,
    and hybrid methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Feature alignment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The fundamental idea behind feature alignment in UDA is to lessen the distinction
    between the source and target domains by learning domain-invariant representations.
    Various UDA approaches map images from both domains onto a common latent space
    to mitigate disparities. This can be accomplished directly by reducing a disparity
    measure that quantifies domain dissimilarities. Alternatively, it can be realized
    implicitly through adversarial learning techniques. The objective is to align
    the feature distributions of both the source and target domains, ensuring that
    the learned representations can be smoothly transferred and effectively utilized
    in diverse domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Explicit discrepancy minimization: Methods focused on explicitly minimizing
    discrepancies usually create a measure or loss function that calculates how different
    the source and target distributions are from each other. This measure is then
    reduced during training to encourage the development of features that work well
    in both domains. Different measures, like Maximum Mean Discrepancy (MMD) [[232](#bib.bib232),
    [233](#bib.bib233)], Kullback-Leibler (KL) divergence [[234](#bib.bib234)], and
    Contrastive Loss (CL) [[235](#bib.bib235), [236](#bib.bib236)], can be employed
    for this purpose. Specifically, Yu et al. [[232](#bib.bib232)] use two separate
    feature encoders for both the target and source domains. They integrate an attention
    technique to focus on particular brain regions and employ MMD to acquire features
    that work well across domains for the prediction of subjective cognitive decline.
    Another explicit measurement used in UDA is the Characteristic Function (CF) distance
    [[237](#bib.bib237)]. This metric calculates the distinction between the distributions
    of latent features in the frequency domain instead of the spatial domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implicit discrepancy minimization: Implicit methods for reducing differences
    in UDA mainly rely on the concepts of adversarial learning. To ensure that feature
    distributions are comparable between different domains, a technique called domain-adversarial
    neural network (DANN) [[238](#bib.bib238)] is used. This approach involves incorporating
    a gradient reversal layer (GRL) into the framework of Generative Adversarial Networks
    (GANs), as illustrated in Figure [14](#S4.F14 "Figure 14 ‣ 4.3.1 Feature alignment
    ‣ 4.3 Domain-adaptive learning ‣ 4 Incomplete supervision ‣ Data efficient deep
    learning for medical image analysis: A survey"). The network comprises two classifiers
    and shared feature extraction layers. With the help of GRL, DANN aims to maximize
    the loss due to domain confusion while minimizing the loss associated with label
    prediction for source samples and domain confusion loss for all samples. DANN
    serves as a foundational model for different UDA methods that are built upon adversarial
    learning principles.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d2139cc2cab0a276970c455bddb02613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The figure demonstrates the Domain Adversarial Neural Network (DANN)
    framework, a classic and effective model designed for learning domain-invariant
    features using adversarial training (image courtesy of Ganin [[238](#bib.bib238)]).'
  prefs: []
  type: TYPE_NORMAL
- en: Different research studies employ implicit techniques for a range of classification
    issues. For instance, Ren et al. [[239](#bib.bib239)] utilize an adversarial loss
    along with siamese architecture for whole slide images. Zhang et al. [[240](#bib.bib240)]
    leverage adversarial learning and introduce focal loss to tackle the problem of
    class imbalance in histopathology images. More recently, Feng et al. [[241](#bib.bib241)]
    engage in binary and multi-class classification tasks related to diagnosing pneumonia.
    They make use of a conditional domain adversarial network to narrow the domain
    discrepancy and implement a contrastive loss to address the challenge of limited
    data in the target domain. Certain investigations have combined self-training
    and adversarial learning for the task of medical image segmentation [[242](#bib.bib242),
    [243](#bib.bib243), [244](#bib.bib244)]. Specifically, Liu et al. [[243](#bib.bib243)]
    proposed the Self-cleansing UDA (S-cuda) technique, which is specifically designed
    to address the issue of domain shift and handle noisy labels in the source domain.
    This method utilizes self-training to produce accurate pseudo-labels for both
    the noisy source and unlabeled target domains. Beyond image classification and
    segmentation, various other applications also make use of implicit discrepancy
    methods. For instance, these methods are applied in bronchoscopic depth estimation
    [[245](#bib.bib245)], reconstructing precise high-resolution (HR) representations
    from low-resolution (LR) OCTA images [[246](#bib.bib246)], and automating sleep
    staging [[247](#bib.bib247)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Image translation based methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Image translation techniques achieve domain alignment by altering the pixel-level
    appearance of source data to match the characteristics of a target domain. Generative
    Adversarial Networks (GANs) are often used for tasks involving direct mapping
    between pixels for image translation. A widely used approach in this category
    is CycleGAN [[248](#bib.bib248)], which operates as an image-to-image translation
    architecture (see Figure [15](#S4.F15 "Figure 15 ‣ 4.3.2 Image translation based
    methods ‣ 4.3 Domain-adaptive learning ‣ 4 Incomplete supervision ‣ Data efficient
    deep learning for medical image analysis: A survey")). It transforms features
    from one image domain into another without relying on paired training examples.
    In the medical field, several approaches apply CycleGAN for unsupervised domain
    adaptation (UDA). However, CycleGAN’s emphasis on pixel-level mapping might not
    consistently ensure the preservation of semantic information in medical images.
    To overcome this limitation, multiple studies have integrated semantic understanding
    into the framework. Various works [[249](#bib.bib249), [250](#bib.bib250), [251](#bib.bib251)]
    have incorporated task-specific losses within the UDA context. These task-specific
    losses are designed to enhance the UDA procedure by introducing extra constraints
    aligned with the unique requirements of the task at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59735d8cfe0ebe235dc6c9e98cf8df78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Illustration of CycleGAN framework: (a) CycleGAN comprises two mapping
    functions $G:X\rightarrow Y$ and $F:Y\rightarrow X$, accompanied by adversarial
    discriminators $D_{Y}$ and $D_{X}$. $D_{Y}$ encourages $G$ to translate $X$ into
    outputs indistinguishable from domain $Y$, while $D_{X}$ performs the reverse
    task. (b) The forward cycle-consistency loss is represented as: $x\rightarrow
    G(x)\rightarrow F(G(x))\approx x$. (c) The backward cycle-consistency loss is
    represented as: $y\rightarrow F(y)\rightarrow G(F(y))\approx y$ (image courtesy
    of Zhu et al. [[248](#bib.bib248)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Certain works employ attention mechanisms to capture distant relationships
    [[252](#bib.bib252), [253](#bib.bib253)]. In the context of cross-modality domain
    adaptation, Tomar et al. [[252](#bib.bib252)] employ a dual cycle consistency
    loss to maintain semantic content while performing image translation. They propose
    a self-attentive spatial adaptive normalization technique that comprises two components:
    the synthesis module and the attention module. The synthesis module’s intermediate
    layers receive semantic layout information from the attention module, aiding in
    the learning of the translation process. Certain studies exploring UDA in image
    detection also employ image translation techniques. For instance, Xing et al.
    [[254](#bib.bib254)] delve into UDA for cell detection across different data modalities.
    They leverage the CycleGAN framework to adjust source images to align with the
    target domain. Their methodology involves training a structured regression-based
    object detector using these adapted source images. Furthermore, they refine the
    detector by incorporating pseudo-labels derived from the target training dataset.
    Extending their earlier study, Xing et al. [[255](#bib.bib255)] enhance their
    method by introducing bidirectional mapping. This involves translating images
    both from the source to the target and vice versa. They also expand this framework
    to address the semi-supervised scenario. In a subsequent extension of their research,
    Xing and Cornish [[256](#bib.bib256)] tackle not just the UDA challenges in cell/nucleus
    detection but also address the challenge of having scarce training data in the
    target domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Learning disentangled representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rather than imposing the demanding requirement of making the entire model or
    features domain-invariant, an alternative approach is to ease this constraint
    by permitting certain components to be domain-specific [[257](#bib.bib257)]. This
    essentially involves acquiring disentangled representations. The key idea of disentangled
    representation is to differentiate between the content and style of an image.
    The underlying premise is that the content, which refers to anatomical information,
    remains uniform across domains, whereas the style, encompassing attributes like
    texture and lighting, is specific to each domain. In the process of achieving
    disentangled representation [[258](#bib.bib258)], initial steps involve extracting
    style and content codes from both the source and target images using specialized
    encoders. Subsequently, generators are employed to create images in the opposite
    domains by combining content codes from one domain with style codes from the other.
    This interplay of generators aims to deceive discriminators by generating images
    that confuse the domains’ distinguishing features, leading to the desired disentangled
    representation. Wang et al. [[259](#bib.bib259)] incorporated the segmentation
    stage and diverse image translation stage into a cohesive end-to-end approach.
  prefs: []
  type: TYPE_NORMAL
- en: Sun et al. [[260](#bib.bib260)] employ a combination of the attention mechanism
    and disentanglement to further mitigate the disparities between domains. Specifically,
    they adopt a preliminary alignment phase to address issues like variations in
    brightness between MRI and CT images. Following this, they introduce an improved
    approach to disentanglement that leverages the Hilbert-Schmidt independence criterion
    to encourage independence and complementary characteristics between content and
    style attributes. Lastly, they integrate an attention bias mechanism to emphasize
    the alignment of regions relevant to the task of cardiac segmentation. Several
    studies enhance disentanglement learning by employing various approaches. For
    example, Xie et al. [[261](#bib.bib261)] utilize a zero loss to ensure that the
    domain-specific encoder only captures information from its corresponding domain.
    Similarly, Yang et al. [[262](#bib.bib262)] implement a coarse-to-fine prototype
    alignment process before feature disentanglement to enhance the separation of
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Pseudo-labeling approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pseudo-labeling is a widely used strategy in UDA to make use of unlabeled data
    in the target domain. This method involves generating pseudo-labels to unlabeled
    data in the target domain using a model trained on labeled data from the source
    domain. Nevertheless, these pseudo-labels can be inaccurate due to the domain
    gap, leading to noise. Thus, a crucial aspect of pseudo-labeling is how various
    networks reduce the uncertainty and eliminate noise from the pseudo-labels to
    enhance their precision. To reduce the uncertainty of pseudo labels, Wu et al.
    [[263](#bib.bib263)] introduce an uncertainty-aware model that integrates Monte
    Carlo dropout layers into a U-Net architecture. Likewise, the Strudel approach
    [[264](#bib.bib264)] involves incorporating uncertainty details into the training
    process through an uncertainty-guided loss function. This aids in eliminating
    labels with low level of certainty. Some studies adopt a curriculum learning strategy,
    beginning with simpler instances to facilitate the model’s learning process and
    gradually introducing more complex cases over time [[265](#bib.bib265), [266](#bib.bib266)].
    When facing a situation where classes are imbalanced, pseudo-labels frequently
    demonstrate an uneven distribution because the model tends to have greater confidence
    in dominant or less complex classes. To address this, Mottaghi et al. [[267](#bib.bib267)]
    introduce a new strategy for pseudo-label selection. This involves using a subset
    of pseudo-labels based on the reciprocal of class frequency, favoring less common
    or challenging classes. This technique effectively addresses label distribution
    imbalance, boosting the surgical activity recognition model’s reliability and
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 Self-supervision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Certain studies address UDA by employing the self-supervision strategy. In this
    approach, alignment is achieved by concurrently conducting auxiliary self-supervised
    tasks in both domains. Each self-supervised task aims to bring the domains closer
    by focusing on relevant directions. Successfully training these self-supervised
    tasks alongside the primary task in the source domain has proven effective in
    generalizing to the unlabeled target domain [[268](#bib.bib268)]. Various auxiliary
    self-supervised tasks are available, but not all are suitable for UDA. Consequently,
    the primary challenge in the self-supervision method is to identify an appropriate
    self-supervised task that enables the model to learn valuable representations
    from the data and promote alignment between the domains. Koohbanani et al. [[269](#bib.bib269)]
    present a method called Self-Path for histology image classification. In this
    approach, they propose three innovative domain-specific self-supervision tasks.
    These tasks involve predicting the magnification level, solving a magnification
    jigsaw puzzle, and predicting the Hematoxylin channel. These tasks are strategically
    designed to utilize the contextual, multi-resolution, and semantic features inherent
    in histopathology images. The Self-rule to multi-adapt (SRMA) technique [[270](#bib.bib270)]
    is applied in the detection of cancer tissue. This method uses a limited set of
    labeled images from the source domain and integrates structural details from both
    domains by identifying visual similarities using self-supervision within each
    domain and across domains. Additional self-supervised tasks include the jigsaw
    puzzle auxiliary task, where the spatial correlation in an image is learned by
    reconstructing a CT scan from shuffled patches [[271](#bib.bib271)], and an auxiliary
    task focused on edge generation [[272](#bib.bib272)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.6 Hybrid methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Various works employ feature alignment and image translation methods together
    to enhance the performance of UDA. These are called hybrid methods. Hybrid methods
    encompass a two-step procedure: initially, image transformation modifies the source
    images to align them with the target domain’s appearance, and subsequently, feature
    adaptation is applied to narrow the remaining disparity between the generated
    target-like images and the real target images [[273](#bib.bib273)]. The benefit
    of employing hybrid techniques lies in their ability to retain pixel-level, feature-level,
    and semantic information. The Cycle-Consistent Adversarial Domain Adaptation technique
    (CyCADA), introduced by Hoffman et al. [[274](#bib.bib274)], is a hybrid learning
    method developed for natural images. It consists of two stages: image adaptation
    and feature adaptation, both of which undergo sequential training without direct
    interactions. CyCADA has found extensive application as a fundamental model in
    different medical imaging scenarios [[275](#bib.bib275), [276](#bib.bib276)].'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to CyCADA, Chen et al. [[11](#bib.bib11), [273](#bib.bib273)] introduce
    an alternative technique known as Synergistic Image and Feature Alignment (SIFA),
    which facilitates concurrent image and feature translation. In particular, the
    feature encoder is shared, enabling it to simultaneously alter the image’s appearance
    and extract domain-invariant representations for the segmentation task. To enhance
    domain adaptation accuracy further, certain research studies incorporate attention
    mechanisms alongside image and feature alignment techniques [[277](#bib.bib277),
    [278](#bib.bib278)]. Chen et al. [[278](#bib.bib278)] employ the same framework
    as SIFA. However, in their approach, the alignment of the feature space is directed
    by the dual adversarial attention mechanism. This mechanism concentrates on specific
    regions identified by the spatial and class attention mechanisms rather than treating
    all semantic feature components uniformly. Label-efficient UDA (LE-UDA) [[279](#bib.bib279)]
    tackles both domain shift and source label scarcity. The approach utilizes a hybrid
    method to handle domain shift, while for source label scarcity, it incorporates
    two teacher models. These models leverage information within domains as well as
    across domains from diverse datasets. In a recent study, Li et al. [[280](#bib.bib280)]
    introduce a self-training adversarial learning framework for retinal OCT fluid
    segmentation tasks that utilize a hybrid approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Overview of recent methods in *Incomplete Supervision* category.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Task | Algorithm Design | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| [[181](#bib.bib181)] | Gland segmentation | Deep adversarial network | 2015
    MICCAI Gland Challenge dataset | F1: 0.916; ObjectDice: 0.903 |'
  prefs: []
  type: TYPE_TB
- en: '| [[179](#bib.bib179)] | Polyp segmentation | Adversarial learning | Kvasir-SEG;
    CVC-Clinic DB | Kvasir-SEG: Dice: 15% label: 0.7676, 30% label: 0.8095; CVC-Clinic
    DB: Dice: 15% label: 0.8218, 30% label: 0.8929 |'
  prefs: []
  type: TYPE_TB
- en: '| [[184](#bib.bib184)] | Heart; Prostate; Pancreas segmentation | Semi-supervised
    GAN | ACDC; DECATHLON | ACDC: DSC (Dice coefficient): 0.834; DECATHLON: DSC: 0.529
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[183](#bib.bib183)] | Fundus segmentation | Leaking GAN | DRIVE, STARE,
    CHASE DB1 | DRIVE: Acc: 95.74 Sp: 86.72 Se: 97.50; STARE: Acc: 95.65 Sp: 91.86
    Se: 91.02; CHASE DB1: Acc: 96.83 Sp:92.21 Se:94.72 |'
  prefs: []
  type: TYPE_TB
- en: '| [[178](#bib.bib178)] | Optic cup segmentation | Teacher-student VAE | DRD
    | DSC: 0.80 |'
  prefs: []
  type: TYPE_TB
- en: '| [[186](#bib.bib186)] | Kidney; Heart; Liver | Generative Bayesian Deep Learning
    | KiTS; ASG; DECATHLON | DSC: KiTS: 0.898; ASG: 0.884; DECATHLON: 0.935 |'
  prefs: []
  type: TYPE_TB
- en: '| [[167](#bib.bib167)] | Skin lesion segmentation | $\Pi$-model | ISIC 2017
    | DSC: 0.874; Acc: 0.943 |'
  prefs: []
  type: TYPE_TB
- en: '| [[158](#bib.bib158)] | Chest X-ray segmentation | Elastic deformations perturbations
    for CL | JSRT dataset | MeanIOU: 5 labeled samples: 85.0 $\pm$ 2.8; 10 labeled
    samples: 87.9 $\pm$ 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| [[169](#bib.bib169)] | Breast | Uncertainty-aware Temporal Ensembling | Private
    Dataset: 170 Volumes; ISIC 2017 | Private Dataset: DSC: 0.7287; ISIC 2017: DSC:
    0.8178 |'
  prefs: []
  type: TYPE_TB
- en: '| [[176](#bib.bib176)] | Brain Tumor and Left Atrial Segmentation | Dual Uncertainty-Guided
    Mixing Consistency | BraTS2020; LA2018 | BraTS: Dice: 85.94 %; LA2018: Dice: 89.28
    % |'
  prefs: []
  type: TYPE_TB
- en: '| [[190](#bib.bib190)] | Heart | CRF-based Self-training | Private Dataset:
    8050 | Images DSC: 0.920 |'
  prefs: []
  type: TYPE_TB
- en: '| [[193](#bib.bib193)] | Thorax Disease and Skin lesion classification | Anti-Curriculum
    Pseudo-labeling | Chest X-Ray14; ISIC 2018 | Chest X-Ray14: AUC: 81.77; ISIC 2018:
    AUC: 94.36 Sensitivity: 72.14 F1: 62.23 |'
  prefs: []
  type: TYPE_TB
- en: '| [[197](#bib.bib197)] | Multi-organ abdominal segmentation | Co-training using
    different modalities | BTCV; CHAOS | BTCV: Mean Dice score: 10 % labels: 81.3;
    CHAOS: Mean Dice score: 10 % labels: 82.1 |'
  prefs: []
  type: TYPE_TB
- en: '| [[198](#bib.bib198)] | Cardiac segmentation | Co-training using different
    network architectures | ACDC dataset | Mean DSC: 0.848 (0.085); Mean HD95: 7.6
    (10.8) |'
  prefs: []
  type: TYPE_TB
- en: '| [[200](#bib.bib200)] | Cardiac segmentation | Co-training using different
    transformations | MM-WHS dataset | 10% labeled data: Dice: 0.743, mIOU: 0.601,
    PixAcc: 0.973; 20% labeled data: Dice: 0.828, mIOU: 0.714, PixAcc: 0.979; 40%
    labeled data: Dice: 0.849, mIOU: 0.746, PixAcc: 0.985; |'
  prefs: []
  type: TYPE_TB
- en: '| [[202](#bib.bib202)] | Breast; Retina | Self-training + Virtual Adversarial
    Training | RetinalOCT; Private Dataset: 39,904 Images | Acc: 0.9513; Macro-R (Macro-Recall):
    0.9330 |'
  prefs: []
  type: TYPE_TB
- en: '| [[203](#bib.bib203)] | Lung detection | MixMatch + Focal Loss | LUNA; NLST
    | LUNA: CPM: 0.872 |'
  prefs: []
  type: TYPE_TB
- en: '| [[204](#bib.bib204)] | Metastatic epidural spinal cord classification | Consistency
    Regularization + Pseudo-labeling + Active Learning | Private Dataset: 7,295 Images;
    | Acc: 0.9582; Macro-P (Macro-Precision): 0.8609 |'
  prefs: []
  type: TYPE_TB
- en: '| [[205](#bib.bib205)] | Cardiac and Prostate segmentation | Self-training
    + Contrastive loss | ACDC; Prostate; MMWHS dataset | ACDC: DSC: 0.881; Prostate:
    DSC: 0.693; MMWHS: DSC: 0.803 |'
  prefs: []
  type: TYPE_TB
- en: '| [[206](#bib.bib206)] | Cardiac, Tumour and histopathology images segmentation
    | Self-training + Contrastive loss | ACDC; KiTS19; CRAG | ACDC: DSC: 0.891; KiTS19:
    DSC: 0.919; CRAG: 0.882 |'
  prefs: []
  type: TYPE_TB
- en: '| [[215](#bib.bib215)] | Colon | Traditional Data Augmentation Entropy + Diversity
    | Private Dataset: 6 colonoscopy videos 38 polyp videos + 121 CTPA datasets |
    Classification: 4 % input: AUC: 0.9204; Detection: 2.04 % input: AUC: 0.9615 |'
  prefs: []
  type: TYPE_TB
- en: '| [[214](#bib.bib214)] | Lung Classification | Loss Prediction Network | CC-CCII
    Dataset | 42 % Chest X-Ray input: Acc: 86.6% |'
  prefs: []
  type: TYPE_TB
- en: '| [[220](#bib.bib220)] | Skin disease classification | BALD + KL-divergence
    | ISIC 2016 | 22 % image input: AUC: 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| [[221](#bib.bib221)] | Chest | Bayesian Neural Network + cGAN Data Augmentation
    | JSRT Database; ChestX-ray8 | Classification: 35 % input: AUC: 0.953; Segmentation:
    35 % input: DSC: 0.910 |'
  prefs: []
  type: TYPE_TB
- en: '| [[225](#bib.bib225)] | Gland; Lymph | Cosine Similarity + Bootstrapping +
    FCN | GlaS 2015; Private Dataset: 80 US images | MICCAI 2015: 50 % input: F1:
    0.921; Private Dataset: 50 % input: F1: 0.871 |'
  prefs: []
  type: TYPE_TB
- en: '| [[226](#bib.bib226)] | Shoulder | BNN + MMD Divergence | Private Dataset:
    36 Volume of MRIs | 48 % MRI input: DSC $\approx$ 0.85 |'
  prefs: []
  type: TYPE_TB
- en: '| [[233](#bib.bib233)] | Major depressive order identification | Feature (MMD)
    | REST-meta-MDD Consortium | Site/Hospital - 20 → Site/Hospital - 1: ACC (%):
    59.73 $\pm$ 1.63; AUC (%): 62.50 $\pm$ 2.50; SEN (%): 69.46 $\pm$ 6.43; SPE (%):
    50.00 $\pm$ 9.63; PRE (%): 58.49 $\pm$ 2.58 |'
  prefs: []
  type: TYPE_TB
- en: '| [[234](#bib.bib234)] | 3D Medical Image Synthesis | KL divergence | BraTS
    2019 dataset (2 subsets are used CBICA and TCIA) | CBICA → TCIA: Dice: 0.773;
    TCIA → CBICA: Dice: 0.874 |'
  prefs: []
  type: TYPE_TB
- en: '| [[236](#bib.bib236)] | Segmentation of retinal fluids in 3D OCT images |
    Contrastive and supervised loss | Two large OCT datasets (Spectralis and Cirrus)
    | Spectralis → Cirrus: Dice: 62.33; UVD: 10.88 |'
  prefs: []
  type: TYPE_TB
- en: '| continued on the next page |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Overview of recent methods in *Incomplete Supervision* category (continued).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Task | Algorithm Design | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| [[240](#bib.bib240)] | Histopathology cancer classification | Adversarial
    learning + Entropy loss + Focal loss | Private cross-modality dataset | WSI →
    Microscopy images(MSIs): Accuracy: 90.48; Precision: 90.67; Recall: 90.35; F1-measure:
    90.50 |'
  prefs: []
  type: TYPE_TB
- en: '| [[241](#bib.bib241)] | Automated pneumonia diagnosis | Conditional domain
    adversarial network + Contrastive loss | RSNA dataset (Stage I); Child X-ray dataset
    | RSNA dataset → Child X-ray: AUC score: 90.57; RSNA + COVID → TTSH dataset: weighted
    AUC: 88.27 |'
  prefs: []
  type: TYPE_TB
- en: '| [[245](#bib.bib245)] | Bronchoscopic Depth Estimation | Adversarial learning
    | Synthetic dataset and human pulmonary dataset | Mean abs. rel. diff: 0.379;
    RMSE: 7.532; Accuracy: 0.856 |'
  prefs: []
  type: TYPE_TB
- en: '| [[250](#bib.bib250)] | Lung Cancer Segmentation | CycleGan + Tumor-aware
    loss | The Cancer Imaging Archive (TCIA) CT dataset and Private MRI dataset |
    CT → MRI: Validation set (Unsupervised): DSC: 0.62 $\pm$ 0.26 HD95: 7.47 $\pm$
    4.66; Test set (Unsupervised): DSC: 0.74 $\pm$ 0.15 HD95: 8.88 $\pm$ 4.8 |'
  prefs: []
  type: TYPE_TB
- en: '| [[252](#bib.bib252)] | Brain tumor and cardiac segmentation | Dual CycleGan
    + Self-attentive spatial adaptive normalization | MM-WHS challenge; BraTS | MM-WHS:
    MRI → CT: Mean Dice: 0.78 $\pm$ 0.10, Mean ASSD: 4.9 $\pm$ 1.5 CT → MR: Mean Dice:
    0.70 $\pm$ 0.11, Mean ASSD: 9.5 $\pm$ 3.2; BraTS: MRI-T2 → MRI-T1: 0.50 $\pm$
    0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| [[258](#bib.bib258)] | Liver segmentation | Disentangled Representation |
    LiTS challenge 2017 dataset (CT slices) and multi-phasic (MRI slices) | CT → MR:
    Dice: 0.81 |'
  prefs: []
  type: TYPE_TB
- en: '| [[259](#bib.bib259)] | Cardiac (LV and MYO) segmentation | Disentangled Representation
    + Semantic consistency loss | MS-CMRSeg; MM-WHS challenge | MS-CMRSeg: bSSFP CMR
    → LGE CMR images: DSC: 79.08, ASSD: 1.68; MM-WHS: CT → MRI: DSC: 84.51, ASSD:
    1.00, MRI → CT: DSC: 84.77 ASSD: 0.98 |'
  prefs: []
  type: TYPE_TB
- en: '| [[260](#bib.bib260)] | Cardiac segmentation | Disentangled Representation
    + HSIC + Attention bias | MMWHS challenge 2017 dataset | MRI → CT: Dice: 80.2,
    ASD: 5.1; CT → MRI: Dice: 66.3, ASD: 4.9 |'
  prefs: []
  type: TYPE_TB
- en: '| [[264](#bib.bib264)] | White Matter Hyperintensity Segmentation | Self training
    + Uncertainty-guided loss | WMH; ADNI-2 | WMH → ADNI-2: DSC: 0.69 $\pm$ 0.18,
    H95: 11.2 $\pm$ 14.5 |'
  prefs: []
  type: TYPE_TB
- en: '| [[265](#bib.bib265)] | Epithelial-stroma (ES) classi- fication | Curriculum
    learning | Netherland Cancer Institute’s (NKI) dataset, Vancouver General Hospital’s
    (VGH) and IHC dataset | Accuracy: VGH → NKI: 91.50, IHC → NKI: 82.51, NKI → VGH:
    92.62, IHC → VGH: 80.49, VGH → IHC: 88.15, NKI → IHC: 81.90 |'
  prefs: []
  type: TYPE_TB
- en: '| [[267](#bib.bib267)] | Pseudo labeling | Surgical activity recognition models
    across operating rooms | Dataset of full-length surgery videos from two robotic
    ORs (OR1 and OR2) | OR1 → OR2: Accuracy: 70.76 mAP: 83.71, OR2 → OR1: Accuracy:
    73.53, mAP: 89.96 |'
  prefs: []
  type: TYPE_TB
- en: '| [[269](#bib.bib269)] | Classification of Pathology Images | Prediction of
    magnification level and Hematoxylin channel + Solving jigsaw puzzle | WSIs: Camelyon16
    and In house dataset(LNM-OSCC) | Camelyon16: AUC-ROC: 93.7 % LNM-OSCC: 97.4 %
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[270](#bib.bib270)] | Colorectal tissue type classification; Multi-source
    patch classification | Intra-domain and cross-domain self-supervision | Kather-16
    [242], Kather-19 [243], Colorectal cancer tissue phenotyping dataset (CRC-TP)[244]
    and In-house dataset | Kather-19 → Kather-16: overall weighted F1 score: 87.7;
    Kather-19 + Kather-16 → CRC-TP: weighted F1 score: 83.6 |'
  prefs: []
  type: TYPE_TB
- en: '| [[272](#bib.bib272)] | Cardiac segmentation | Edge generation task + Adversarial
    learning | MM-WHS challenge dataset (2017) | MRI $\rightarrow$ CT: Dice: 76.98,
    ASD: 4.6 |'
  prefs: []
  type: TYPE_TB
- en: '| [[275](#bib.bib275)] | Cone-beam computed tomography (CBCT) segmentation
    | Image and feature alignment | Private Dataset: 90 patients | CT $\rightarrow$
    CBCT: DSC: 83.6 % |'
  prefs: []
  type: TYPE_TB
- en: '| [[11](#bib.bib11)] | Cardiac segmentation | Synergistic image and feature
    alignment (SIFA) | MM-WHS challenge dataset | MRI $\rightarrow$ CT: Dice: 73.0,
    ASD: 8.1 |'
  prefs: []
  type: TYPE_TB
- en: '| [[278](#bib.bib278)] | Skull segmentation and Cardiac segmentation | CycleGan
    + Feature space alignment is led by the dual adversarial attention mechanism |
    CQ500; ADNI; MM-WHS challenge | Skull: CT $\rightarrow$ MRI: DSC: 84.07 %, ASSD:
    1.18; Cardiac: MRI $\rightarrow$ CT: DSC: 76.7 %, ASSD: 5.1 |'
  prefs: []
  type: TYPE_TB
- en: '| [[279](#bib.bib279)] | Multi-organ and Cardiac segmentation | CycleGan and
    feature alignment | MICCAI 2015 Multi-Atlas Abdomen Labeling(CT images), ISBI
    2019 CHAOS Challenge (MR images); MM-WHS | Cardiac: MRI $\rightarrow$ CT: Dice:
    70.8, ASD: 9.6; CT $\rightarrow$ MRI: Dice: 66.5, ASD: 4.0; Multi-organ: MRI $\rightarrow$
    CT: Dice: 82.8, ASD: 2.3; CT $\rightarrow$ MRI: Dice: 87.7, ASD: 1.0 |'
  prefs: []
  type: TYPE_TB
- en: 5 Inaccurate supervision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inaccurate supervision refers to a scenario where the provided supervision
    information isn’t always entirely accurate, meaning that errors may be present
    in some of the label information. Such noisy labels [[281](#bib.bib281)] can originate
    from various sources, including human errors in the labeling process, inter-observer
    variability among medical experts, or reliance on non-experts or automated systems
    for data labeling (see Figure [16](#S5.F16 "Figure 16 ‣ 5 Inaccurate supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). Since
    noisy labels can significantly harm the generalization capabilities of deep neural
    networks, it is imperative to develop robust techniques to handle and mitigate
    the impact of noisy labels. This is particularly vital in the field of MIA, where
    precision and accuracy are critical for medical diagnosis and treatment. A summary
    of recent approaches for learning with inaccurate supervision is provided in Table [5](#S5.T5
    "Table 5 ‣ 5.3 Training procedures ‣ 5 Inaccurate supervision ‣ Data efficient
    deep learning for medical image analysis: A survey"). We categorize inaccurate
    supervision approaches into three broad groups: Robust loss Design, data re-weighting
    and training procedures.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7728fd0a766114b0715785a3dfb49b35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The major sources of label noise encompass variations among different
    observers, mistakes made by human annotators, and inaccuracies in computer-generated
    labels. The impact of label noise in medical datasets is expected to grow as larger
    datasets are curated for deep learning purposes (image courtesy of Karimi [[281](#bib.bib281)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Robust loss design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Certain investigations modify the loss function as a strategy to mitigate the
    impact of noisy labels. Researchers have introduced novel loss functions like
    Mean Absolute Error (MAE) [[282](#bib.bib282)] and Generalized Cross Entropy [[283](#bib.bib283)]
    to tackle the issue of label noise in natural images. In addition, some works
    choose to adapt loss functions specifically for tasks in medical imaging [[284](#bib.bib284),
    [285](#bib.bib285), [286](#bib.bib286)]. For COVID-19 Pneumonia Lesion segmentation,
    Wang et al. [[285](#bib.bib285)] proposed an enhanced Dice loss that addresses
    noise-related challenges. This improved loss function is an extended version of
    the traditional Dice loss, tailored for segmentation tasks, and incorporates the
    Mean Absolute Error (MAE) loss to enhance its robustness against noisy data. Chen
    et al. [[286](#bib.bib286)] introduce a new and versatile loss function called
    Adaptive Cross Entropy (ACE), designed to handle noise in labels without requiring
    hyperparameter fine-tuning during training. They provide both theoretical and
    practical evaluations of the ACE loss and demonstrate its efficacy across various
    publicly available datasets. Previous segmentation methods that handle noisy labels
    have typically focused on preserving semantics in a pixel-wise manner, which involves
    actions like pixel-wise label correction. However, they often overlook the potential
    benefits of considering pairwise relationships between pixels [[287](#bib.bib287)].
    Notably, it has been observed that capturing these pairwise affinities can significantly
    decrease label noise. Building on this insight, Guo et al. [[287](#bib.bib287)]
    introduce a joint class-affinity segmentation model that takes into account both
    pixel-wise label correction and pairwise pixel relationships in order to reduce
    label noise. To further reduce the impact of label noise, they introduce a strategy
    called class-affinity loss correction (CALC), which includes class-level and affinity-level
    loss correction.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Data re-weighting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Broadly speaking, these methods aim at down-weighting those training samples
    that are more likely to have incorrect labels. In this context, Xue et al. [[4](#bib.bib4)]
    introduced an approach for classifying skin lesions with noisy labels. Their method
    involved a data re-weighting technique, effectively excluding data samples with
    significant loss values in each training batch. To predict pancreatic cancer regions
    in whole-slide images (WSIs), Le et al. [[288](#bib.bib288)] utilized a noisy
    label classification technique. This approach incorporates a limited set of clean
    training samples and dynamically assigns weights to training samples to address
    sample noise. These weights are assigned in real time to align the network loss
    with that of the clean samples. For multi-organ segmentation, Zhu et al. [[8](#bib.bib8)]
    introduced an approach known as *pick and learn*. In this method, a deep learning
    model is trained to identify incorrect labels and assign weights to each sample
    within a training batch. The goal is to reduce the impact of samples with inaccurate
    labels. Simultaneously, the primary segmentation model is trained in parallel,
    incorporating these weights into its loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies involving resampling and reweighting at the pixel level are intended
    to focus the segmentation model on learning from reliable pixels. For example,
    Mirikharaji et al. [[289](#bib.bib289)] introduced a method for skin lesion segmentation
    that incorporates pixel-wise weighting. This approach learns weight maps that
    adapt spatially and adjusts the influence of each pixel using a meta-reweighting
    framework. The Tri-network approach by Zhang et al. [[290](#bib.bib290)] employs
    three cooperating networks and dynamically identifies informative samples based
    on the consensus among predictions generated by these distinct networks. Meanwhile,
    Wang et al. [[291](#bib.bib291)] employ meta-learning techniques to automatically
    estimate an importance map, allowing them to extract reliable information from
    crucial pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Training procedures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The methods in this category are very diverse. Several approaches in this category
    rely on Multi-network Learning, which frequently employs techniques like collaborative
    learning and co-training to train multiple networks simultaneously. Meanwhile,
    others follow the Multi-round Learning paradigm, which iteratively enhances the
    chosen set of clean examples without the need for maintaining additional DNNs.
    This improvement is achieved by repeating the training process in multiple rounds.
    Min et al. [[292](#bib.bib292)] adapted concepts from Malach and Shalev-Shwartz
    [[293](#bib.bib293)] to create label-noise-resistant techniques for medical image
    segmentation. They simultaneously trained two distinct models and exclusively
    updated these models using data samples where the predictions of the two models
    disagreed. Rather than solely relying on final layer predictions, Min et al. [[292](#bib.bib292)]
    incorporated attention modules at different network depths, allowing them to utilize
    gradient information from various feature maps to identify and reduce the influence
    of samples with incorrect labels. They demonstrated encouraging outcomes in MRI-based
    cardiac and glioma segmentation tasks. For medical image classification, Xue et
    al. [[294](#bib.bib294)] utilize a self-ensemble model along with a noisy label
    filter to effectively identify clean and noisy samples. Subsequently, they employ
    a collaborative training approach to train the clean samples, aiming to mitigate
    the impact of imperfect labels. Additionally, they introduce an innovative global
    and local representation learning scheme, which serves as an implicit regularization
    method for enabling the networks to make use of noisy samples in a self-supervised
    fashion. For COVID-19 pneumonia lesion segmentation, Yang et al. [[295](#bib.bib295)]
    propose a dual-branch network that learns from both accurate and noisy annotations
    separately. They introduce the Divergence-Aware Selective Training (DAST) strategy
    to distinguish between severely noisy and slightly noisy annotations. For severely
    noisy samples, they apply regularization through dual-branch consistency between
    predictions from the two branches. Additionally, they refine slightly noisy samples
    and incorporate them as supplementary data for the clean branch to prevent overfitting.
    Li et al. [[296](#bib.bib296)] focus on selecting training pixels with reliable
    annotations from pixels with uncertain network predictions. They introduce the
    online prototypical soft label correction (PSLC) method to estimate pseudo-labels
    for label-unreliable pixels. They then calibrate the total segmentation loss using
    the segmentation loss of label-reliable and label-unreliable pixels. For hepatic
    vessel segmentation, Xu et al. [[297](#bib.bib297), [298](#bib.bib298)] utilized
    a small set of accurately labeled data alongside a larger set of noisily labeled
    data. They employed confident learning with the help of a weight-averaged teacher
    model. This strategy involved progressively refining the noisy labels in the low-quality
    dataset through pixel-wise soft correction. Shi et al. [[299](#bib.bib299)] present
    a framework designed to address noisy labels by extracting valuable supervision
    information from both pixel-level and image-level sources. Specifically, they
    make explicit estimations of pixel-wise uncertainty, treating it as a measure
    of noise at the pixel level. They then propose a robust learning approach at the
    pixel level, utilizing both the original labels and pseudo-labels. Additionally,
    they present a complementary image-level robust learning method to incorporate
    more information alongside pixel-level learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Overview of recent methods in *Inaccurate Supervision* category.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Task | Algorithm Design | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| [[8](#bib.bib8)] | Multi-organ segmentation | Loss Re-weighting | JSRT |
    25 % noise: Dice: 0.895; 50 % noise: Dice: 0.898; 75 % noise: Dice: 0.895 |'
  prefs: []
  type: TYPE_TB
- en: '| [[289](#bib.bib289)] | Skin Lesions segmentation | Example reweighting |
    ISIC 2017 | Unsupervised noise: Dice: 73.55 % |'
  prefs: []
  type: TYPE_TB
- en: '| [[4](#bib.bib4)] | Skin lesion classification | Sample reweighting + Online
    Uncertainty Sample Mining | ISIC 2017 | 5% noise: Acc: 84.5; 10%: Acc: 83.6; 20%:
    ;Acc: 80.7; 40%: Acc: 80.7; |'
  prefs: []
  type: TYPE_TB
- en: '| [[290](#bib.bib290)] | Clinical stroke lesion and multi organ segmentation
    | Tri-teaching network | Private clinical stroke dataset; JSRT | Clinical stroke:
    Dice: 68.12 %; JSRT: Dice: 80.43 |'
  prefs: []
  type: TYPE_TB
- en: '| [[292](#bib.bib292)] | Cardiac and Brain tumour segmentation | Two-Stream
    Mutual Attention Network | HVSMR 2016; BRATS 2015 | HVSMR 2016: Myocardium: Dice:
    0.820 ADB: 0.824 HDD: 4.73, Blood Pool: Dice: 0.926 ADB: 0.957 HDD: 8.81; BRATS
    2015: Mean Dice: 0.792 |'
  prefs: []
  type: TYPE_TB
- en: '| [[297](#bib.bib297)] | Hepatic Vessel Segmentation | Mean-Teacher-assisted
    Confident Learning | 3DIRCADb; MSD8 (Used for training) | 3DIRCADb: Dice: 0.7245
    PRE: 0.7570 ASD: 1.1718 HD: 7.2111 |'
  prefs: []
  type: TYPE_TB
- en: '| [[299](#bib.bib299)] | Left Atrial(LA) and cervical cancer Segmentation |
    Pixel-wise and Image-level Noise Tolerant learning | Left Atrial(LA); Private
    dataset | LA: 25 % Noise : Dice(%): ASD: 1.60 50 % Noise: Dice(%): 89.04 ASD:
    1.92 75 % Noise: Dice(%): 76.25 ASD: 4.56; Private dataset: Dice(%): 75.31 ASD:
    1.76 |'
  prefs: []
  type: TYPE_TB
- en: '| [[287](#bib.bib287)] | Surgical instrument segmentation | Pixel-wise label
    correction and pairwise pixel relationships in order to reduce label noise. |
    Endovis18 | Average: Ellipse noise: Dice (%): 71.384 Jac (%): 58.452; Symmetric
    noise: Dice (%): 74.058 Jac (%): 62.667; Asymmetric noise: Dice (%): 74.410 Jac
    (%): 63.029 |'
  prefs: []
  type: TYPE_TB
- en: 6 Only limited supervision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Only Limited Supervision* refers to a set of methods in which the available
    supervision or labeling for training data is constrained or limited in nature.
    Furthermore, unlabeled data is also unavailable. These methods are typically applied
    in scenarios where acquiring extensive or detailed annotations is challenging
    or resource-intensive. Instead, they employ alternative strategies such as few-shot
    learning, transfer learning, and data augmentation to maximize the utility of
    the limited available supervision (Figure [17](#S6.F17 "Figure 17 ‣ 6 Only limited
    supervision ‣ Data efficient deep learning for medical image analysis: A survey")).
    These approaches enhance model performance and facilitate tasks like segmentation,
    classification, or detection with minimal labeled data. A summary of recent approaches
    for learning with only limited supervision is provided in Table [6](#S6.T6 "Table
    6 ‣ 6.3 Transfer learning ‣ 6 Only limited supervision ‣ Data efficient deep learning
    for medical image analysis: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a2b2a9190722310ce00af0d3b4b91ea0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Taxonomy of *Only Limited Supervision* methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Data Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data augmentation offers a means to significantly increase the quantity and
    diversity of training data, all while avoiding the need for additional sample
    collection. These augmentation techniques encompass both straightforward yet remarkably
    impactful transformations like cropping, padding, and flipping, as well as more
    intricate generative models [[300](#bib.bib300)]. The efficacy of data augmentation
    strategies varies based on factors like input nature and visual tasks. Therefore,
    the field of medical imaging might necessitate distinct augmentation approaches
    that yield plausible data instances and effectively enhance the regularization
    of deep neural networks. Moreover, data augmentation can also address the issue
    of underrepresented classes by generating additional instances, such as generating
    synthetic lesions. Following earlier surveys [[300](#bib.bib300), [301](#bib.bib301)],
    we have categorized data augmentation methods into three broad groups: transformation
    of original data, generation of artificial data and other categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Transformation of original data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first data augmentation category involves applying various image manipulation
    techniques to existing samples. This can be divided into three subcategories:
    (1) Affine Transformations are geometric changes that retain lines and parallelism,
    though not necessarily distances and angles. This is ensured by transformation
    constraints, typically preserving the image’s aspect ratio along axes of symmetry.
    The transformations include translation, rotation, flipping, scaling, cropping,
    and shearing [[302](#bib.bib302), [303](#bib.bib303)]. (2) Elastic transformations
    involve applying a spatial deformation field to an image. Unlike affine transformations,
    they don’t enforce the preservation of collinearity or aspect ratio. As a result,
    elastic transformations can introduce shape variations and can be employed to
    enhance the robustness of segmentation algorithms [[301](#bib.bib301)]. and (3)
    Pixel-Level Transformations alter pixel values to modify characteristics like
    saturation, contrast, noise, and brightness [[304](#bib.bib304)]. Given that medical
    imaging is often grayscale, color-based changes are rare. Pixel-level transformations
    aid deep neural networks’ robustness across different scanners and protocols that
    might affect pixel distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The majority of transformations within this category are quite straightforward
    to apply and are either readily available in deep learning frameworks or can be
    easily incorporated using versatile libraries [[305](#bib.bib305)]. Recently,
    there has been a surge in frameworks and libraries tailored for the medical field,
    like the Medical Open Network for AI (MONAI) [[306](#bib.bib306)]. However, it’s
    important to note that since these techniques rely on altering the original samples,
    they can’t enhance the network’s ability to generalize beyond its initial training
    data. They often generate samples that are highly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Generation of artificial data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Creating artificial or synthesized samples can provide a broader range of diverse
    and complex examples, effectively addressing the limitations of methods based
    on transformations. The prevalent method for medical image synthesis is through
    generative networks, particularly generative adversarial networks (GANs) [[307](#bib.bib307)].
    However, generating synthetic images can also involve techniques like combining
    features or employing specialized modeling approaches designed for specific medical
    imaging tasks or modalities. While these approaches offer increased diversity,
    they often require higher computational resources and introduce complexity. Additionally,
    artificially generated samples might not fully capture the visual attributes or
    distribution of genuine data instances.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the specific domain, dataset, and task at hand, certain families
    of GANs may be more suitable, while others may be entirely impractical. Translation-based
    GANs, which encompass models like CGAN [[308](#bib.bib308)], pix2pix [[309](#bib.bib309)],
    CycleGan [[248](#bib.bib248)], and SPADE [[310](#bib.bib310)], specialize in learning
    how to transform various types of images. For instance, they can convert a segmentation
    mask into a newly synthesized input or transform a non-contrast CT-scan into a
    contrast CT-scan. In contrast, noise-based generation models like DC-GAN [[311](#bib.bib311)],
    StyleGAN2 [[312](#bib.bib312)], and PGAN [[313](#bib.bib313)] offer greater flexibility.
    However, noise-based generation techniques may encounter challenges when dealing
    with small training datasets, necessitating mitigation strategies such as patch
    extractions and traditional data augmentation. Although translation based models
    are known for producing images of exceptionally high quality, they are restricted
    in terms of the quantity of images they can create because they rely on the use
    of segmentation masks or different image modalities as input requirements. Noise-based
    approaches, on the other hand, do not face such limitations but often yield images
    with lower visual quality and run the risk of reproducing artifacts (such as vignettes
    or rulers) that could reinforce biases present in the dataset [[314](#bib.bib314)].
    These frameworks and other extended versions of GANs have been widely employed
    for augmenting various types of organ images, including liver [[315](#bib.bib315),
    [316](#bib.bib316)], skin [[317](#bib.bib317)], chest [[318](#bib.bib318)], eye
    [[319](#bib.bib319)], lung [[320](#bib.bib320)], breast [[321](#bib.bib321), [322](#bib.bib322)],
    brain [[323](#bib.bib323), [324](#bib.bib324)], and more. Readers interested in
    a comprehensive review of GANs for medical image augmentation can refer to the
    work of Chen et al. [[325](#bib.bib325)].
  prefs: []
  type: TYPE_NORMAL
- en: Other than GANs, Copy-paste methods have been utilized to generate artificial
    data. Copy-paste is a simple yet effective data augmentation technique and, it
    has demonstrated the potential to amplify the generalization power of deep neural
    networks. In essence, copy-paste involves copying portions of one image and pasting
    them onto another. Notably, the mix-up technique by [[326](#bib.bib326)], and
    CutMix [[327](#bib.bib327)] are well-known approaches for mixing entire images
    and mixing image crops, respectively. Several studies have extended these methods
    to address specific objectives in MIA. For example, TumorCP [[328](#bib.bib328)]
    employs lesion masks to extract lesions from scans and paste them onto another
    scan at appropriate locations, guided by the lesion masks in the target scan.
    In the context of nuclei segmentation, InsMix [[329](#bib.bib329)] follows a Copy-Smooth-Paste
    principle and conducts morphology-constrained generative instance augmentation.
    SelfMix [[330](#bib.bib330)] leverages both tumor and non-tumor information for
    lesion segmentation. Given a pair of annotated training images, CarveMix [[331](#bib.bib331)]
    combines a region of interest (ROI) based on lesion location and geometry, replacing
    the corresponding voxels in other labeled images. TensorMixup [[332](#bib.bib332)]
    is a method that merges two image patches using a tensor and has been utilized
    to enhance the precision of tumor segmentation. Certain alternative approaches
    concentrate on medical datasets that frequently exhibit skewness towards negative
    cases. These methods encompass the creation and incorporation of fabricated lesions
    into individuals who are otherwise healthy. For example, these methods have been
    applied to simulate lesions resembling multiple sclerosis in brain MR images [[333](#bib.bib333)]
    or to introduce cancer indicators into breast mammography images [[334](#bib.bib334)].
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Others
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Apart from the aforementioned categories, there exist augmentation techniques
    designed for specific purposes. Notably, modalities such as CT and MRI possess
    a volumetric nature. Leveraging 3D convolutions presents the advantage of incorporating
    information from neighboring slices, leading to enhanced performance given a large
    dataset. While several image transformations, particularly affine transformations,
    are well-established for 2D images, extending them to 3D settings may pose challenges
    in terms of computational efficiency. Examples of augmentations designed with
    3D data augmentation techniques include 3D GANs [[335](#bib.bib335)], 3D affine
    transformations [[336](#bib.bib336)], and multiplanar image synthesis [[337](#bib.bib337)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Learnable data augmentation, also known as neural data augmentation, is an
    advanced technique in deep learning where the augmentation parameters are learned
    by the neural network during the training process. Unlike traditional data augmentation
    methods that apply fixed transformations to input data, learnable data augmentation
    allows the model to adaptively determine the augmentation parameters based on
    the data and the task at hand. Methods following this strategy involve training
    two networks simultaneously: one network learns to solve a specific task, while
    the second network learns how to augment the data for the first one. One of the
    most common approaches for learnable data augmentation is autoaugment [[338](#bib.bib338)].
    This method aims to optimize network performance by identifying the most effective
    combination of established transformations (like affine transformations, pixel-level
    modifications, etc.). The ideal augmentation policy is determined through a neural
    network, which can be trained using adversarial training [[339](#bib.bib339)],
    evolutionary algorithms [[340](#bib.bib340)], or reinforcement learning [[338](#bib.bib338)].'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Few shot learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Few-shot learning (FSL) takes inspiration from human-like robust reasoning
    and analytical abilities. Wang et al. [[341](#bib.bib341)] provided a standard
    definition for machine learning based on experience (E), task (T), and performance
    (P): A computer program is considered to learn from experience E with respect
    to certain classes of task T and performance measure P if its performance can
    enhance with E on T as measured by P. It’s important to note that E, the experience
    in FSL, is quite limited. Formally, within each few-shot task, we are given three
    sets: a support set denoted as S, a query set referred to as Q, and an auxiliary
    set labeled as A. The support set S encompasses C distinct categories, with each
    category comprising K training samples, essentially forming a C-way K-shot configuration.
    The query set Q comprises unlabeled query data. We categorize the current deep
    FSL methods into enlarging the training data, metric-learning based methods, meta-learning
    based methods and others.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enlarging the training data: These approaches augment the training data to
    enlarge the number of samples, enabling the utilization of standard deep learning
    models and algorithms on the augmented dataset to attain a more accurate model.
    For multi-modal medical image segmentation, Mondal et al. [[342](#bib.bib342)]
    expand the training set through the use of GANs. On the other hand, Zhao et al.
    [[343](#bib.bib343)] present a learning-based technique for data augmentation.
    Specifically, their approach starts with a single labeled image and a set of unlabeled
    examples. By employing learning-based registration methods, they model the spatial
    and appearance transformations between the labeled and unlabeled examples. These
    transformations encompass effects such as non-linear deformations and variations
    in imaging intensity. Subsequently, they generate new labeled examples by sampling
    these transformations and applying them to the labeled example, resulting in a
    diverse range of realistic images. These synthesized examples are then used to
    train a supervised segmentation model. For the segmentation of new WM tracts in
    a few-shot scenario, [[344](#bib.bib344), [345](#bib.bib345)] have proposed efficient
    data augmentation techniques. Specifically, Lu et al. [[344](#bib.bib344)] introduce
    an efficient data augmentation technique that creates synthetic annotated images
    through tract-aware image mixing. Additionally, they employ a transfer learning
    method for few-shot segmentation. [[346](#bib.bib346)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metric-learning based approaches: These approaches offer a straightforward
    and adaptable framework where they directly assess the similarities or dissimilarities
    between query images in the query set and labeled images in the support set. A
    classical metric-learning technique, known as the Prototypical Network (ProtoNet)
    by Snell et al. [[347](#bib.bib347)], illustrates this concept. ProtoNet computes
    prototype representations for each base class by averaging the feature vectors
    and subsequently measures the distances between these prototype representations
    and each query image. Importantly, metric-learning based methods do not involve
    data-independent parameters in their classifiers, which means fine-tuning is unnecessary
    during the testing phase. Moreover, some researchers, such as Ali et al. [[348](#bib.bib348)],
    have introduced an innovative additive angular margin metric to enhance the original
    ProtoNet’s ability to classify challenging samples, especially in scenarios involving
    multi-center, underrepresented, and difficult-to-classify endoscopy data. To enhance
    prototype-based few-shot segmentation model for abdominal organs, Wang et al.
    [[349](#bib.bib349)] introduce a regularization technique. This enhancement involves
    two key elements: self-reference and contrastive learning. Self-reference regularization
    ensures that a class prototype accurately represents the entire organ within a
    support image. Contrastive learning aids in the understanding of similarity between
    foreground and background features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to natural images, there is a lack of extensive publicly available
    datasets for pre-training medical image segmentation models. Consequently, some
    self-supervised learning approaches have emerged in the domain of medical image
    few-shot segmentation, relying on unlabeled data. To this end, Ouyang et al. [[350](#bib.bib350),
    [351](#bib.bib351)] presented SSL-ALPNet, a self-supervised learning method based
    on superpixels. In this approach, for every unlabeled image, pseudolabels are
    created at the superpixel level. During each training iteration, a randomly chosen
    pseudolabel, along with the original image, is used as both the support and query.
    Random transformations are introduced between the support and query images. The
    primary objective of this self-supervision task is to segment the pseudolabel
    on the query image, using the support image as a reference, despite the applied
    transformations. Additionally, they incorporated an adaptive local prototype pooling
    module into prototypical networks to address the prevalent issue of foreground-background
    class imbalance in medical image segmentation, as shown in Figure [18](#S6.F18
    "Figure 18 ‣ 6.2 Few shot learning ‣ 6 Only limited supervision ‣ Data efficient
    deep learning for medical image analysis: A survey"). Hansen et al. [[352](#bib.bib352)]
    expanded on this concept by extending the self-supervision task to supervoxels,
    effectively incorporating 3D information from image volumes. They introduced ADNet,
    a prototypical segmentation network inspired by anomaly detection, which avoids
    modeling the large and diverse background class with prototypes. Building on this
    work, their recent contribution, ADNet++ [[353](#bib.bib353)], presents a one-step
    multi-class medical image segmentation framework. The model notably enhances the
    current 3D FSS model for MRI and CT-based abdominal organ and cardiac segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8590d1749f6ec8d68a0412e4875470ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Illustration of the SSL-ALPNet framework [[350](#bib.bib350)]: (a)
    The self-supervision task involves segmenting the pseudolabel on the query image
    with reference to the support image, despite the applied transformations (shown
    in blue boxes). (b) The ALPNet method addresses the challenge of class imbalance
    by adaptively extracting multiple local representations of the large background
    class (in blue), each of which represents a distinct local background region (image
    from [[350](#bib.bib350)]).'
  prefs: []
  type: TYPE_NORMAL
- en: In medical images, a significant imbalance exists between the foreground and
    background. Medical images typically feature a diverse background comprising numerous
    tissues and organs, whereas the foreground is typically uniform and occupies a
    relatively small area. Applying the same global operation, like masked average
    pooling, directly to both foreground and background, as is commonly done in the
    processing of natural images, can result in the loss of local information. To
    address this issue, recent studies in the field of prototypical FSS have introduced
    more adaptive prototype extraction modules to mitigate the impact of complex backgrounds.
    Specifically, these studies have incorporated additional priors, such as spatial
    location [[354](#bib.bib354)], and neighborhood correlations [[355](#bib.bib355)],
    into the prototypes to preserve spatial and shape information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meta-learning based approaches: Meta-learning methods typically employ a meta-training
    approach, where they train a model on a sequence of few-shot tasks derived from
    the base classes during the training phase. The objective is to equip the pre-trained
    model with the capability to quickly adapt to entirely new tasks during the testing
    phase. One well-known representative of meta-learning methods is Model-Agnostic
    Meta-Learning (MAML) [[356](#bib.bib356)]. MAML achieves this adaptation by pre-training
    the initial model parameters using second-order gradients, enabling the model
    to rapidly adapt to new tasks with only a limited number of gradient steps. Several
    studies in medical imaging employ MAML and its extensions for various few-shot
    learning tasks. These tasks include rare disease classification [[357](#bib.bib357)],
    classifying whole-genome doubling (WGD) across 17 cancer types using digitized
    histopathology slide images [[358](#bib.bib358)], brain tumor segmentation [[359](#bib.bib359)],
    and more. Further, To address issues related to vanishing high-order meta-gradients
    in MAML, Khadka et al. [[360](#bib.bib360)] utilize the Implicit Model Agnostic
    Meta-Learning (iMAML) optimization strategy [[361](#bib.bib361)] for few-shot
    lesion segmentation. In this approach, inner optimization focuses on computing
    weights using a CNN model, while an analytic solution is used to estimate the
    outer meta-gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhao et al. [[362](#bib.bib362)] suggest that meta-learning can enhance the
    generator’s ability to *learn to hallucinate* meaningful images, leading to improved
    segmentation models in few-shot unsupervised domain adaptation. In this regard,
    they introduce a meta-hallucinator to generate valuable samples, enhancing model
    adaptability on the target domain with limited source annotations. For diagnosis
    of glioblastoma multiforme progression, Song et al. [[363](#bib.bib363)] propose
    an interpretable structure-constrained graph neural network (ISGNN). The ISGNN
    used a meta-learning strategy for aggregating class-specific graph nodes to enhance
    classification performance on small-scale datasets while maintaining interpretability.
    Recently, Gao et al. [[364](#bib.bib364)] introduced a discriminative ensemble
    meta-learning approach for the diagnosis of rare fundus diseases. They introduced
    a co-regulation loss during the pre-training of the meta-learning backbone. Subsequently,
    ensemble-learning techniques were employed to improve performance, taking advantage
    of the hierarchical features within the backbone network. They explored three
    ensemble strategies: uniform averaging, majority voting, and stacking, to identify
    low-shot rare fundus diseases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Others: One approach to accomplishing few-shot learning involves utilizing
    the encoder-decoder framework to explore the connection between a query and a
    support set. In this regard, Roy et al. [[365](#bib.bib365)] introduce the Squeeze
    and Excitep framework, which was the first implementation of a two-branch architecture
    for medical image few-shot segmentation. One branch, referred to as the conditioner
    arm, focuses on extracting foreground information from the support set. The other
    branch, known as the segmenter arm, engages with the conditioner arm through the
    spatial SE module to rectify the query feature. MprNet [[366](#bib.bib366)], as
    an enhancement of SENet, introduces a fusion module based on cosine similarity
    to facilitate information exchange between these two branches. Similarly, Kim
    et al. [[367](#bib.bib367)] introduce a U-Net like network tailored for segmentation
    tasks. This network is designed to predict segmentation by capturing the relationship
    between 2D slices from the support data and a query image. It incorporates a bidirectional
    gated recurrent unit (GRU) to learn the coherence of encoded features among adjacent
    slices. Recently, Feng et al. [[368](#bib.bib368)] employ a hybrid method in which
    they introduce a segmenter built upon the encoder-decoder architecture. They incorporate
    spatial and prototypical priors as extra sources of supervisory information. Experimental
    results in multi-modalities and multi-organs segmentation showcase that the method
    they propose significantly surpasses previous state-of-the-art techniques. FSL
    is typically trained using the episode training method. Zhu et al. [[369](#bib.bib369)]
    introduced a Query-Relative (QR) loss, which is more effective when combined with
    the episode training approach than the Cross-Entropy loss for FSL.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To address the challenge of limited training data and enhance model performance,
    another common approach known as transfer learning is frequently employed. In
    this scenario, the goal is to harness knowledge acquired from similar learning
    tasks. The supervised transfer learning technique [[370](#bib.bib370)] has proven
    valuable in addressing various issues in medical image analysis. These approaches
    typically involve initial pre-training of standard architectures like ResNet [[371](#bib.bib371)]
    or VGG [[372](#bib.bib372)] on a source domain containing abundant data, such
    as natural images from sources like ImageNet [[373](#bib.bib373)] or medical images.
    Subsequently, these pre-trained models are transferred to the target domain and
    fine-tuned using a significantly smaller set of training examples. Tajbakhsh et
    al. [[370](#bib.bib370)] demonstrated that pre-trained CNNs, when appropriately
    fine-tuned, achieved performance levels at least comparable to CNNs trained entirely
    from the beginning. This has established transfer learning as a fundamental technique
    for image classification tasks across diverse modalities, spanning CT [[374](#bib.bib374)],
    mammography [[375](#bib.bib375)] MRI [[376](#bib.bib376)], X-ray [[377](#bib.bib377)],
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from classification, the use of transfer learning in addressing different
    medical image challenges, such as image segmentation and localization, has been
    limited [[22](#bib.bib22), [378](#bib.bib378), [211](#bib.bib211)]. This trend
    can be attributed, in part, to the inherent 3D characteristics of medical images,
    which present challenges when adapting 2D models trained on natural images. Additionally,
    it is influenced by the effective performance of shallower segmentation networks
    in medical imaging, which may not gain significant advantages from fine-tuning
    in contrast to deep models. However, certain studies have attempted transfer learning
    for image segmentation. For instance, Ma et al. [[379](#bib.bib379)] performed
    fine-tuning on an autoencoder that was originally pre-trained for image segmentation
    tasks in natural images. Similarly, other researchers like Qin et al. [[380](#bib.bib380)]
    utilized an encoder pre-trained for the task of image classification in natural
    images and added a randomly initialized decoder to it to address the task of prostate
    MRI segmentation. Liu et al. [[381](#bib.bib381)] perform a two-stage transfer
    learning framework for segmenting COVID-19 lung infections from CT images. Nguyen
    et al. [[382](#bib.bib382)] perform task agnostic transfer learning for skin attribute
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: Some studies have attempted to transfer knowledge from 2D models pre-trained
    on natural images to models intended for 3D medical applications. For example,
    Yu et al. [[383](#bib.bib383)] adapted models trained on natural scene videos,
    treating the third dimension of medical scans as a temporal axis. However, this
    approach may not effectively capture the 3D context of medical scans. In contrast,
    Liu et al. [[384](#bib.bib384)] proposed a method to transform a 2D model into
    a 3D network by expanding 2D convolution filters into 3D separable anisotropic
    filters. Recently, Messaoudi et al. [[385](#bib.bib385)] introduced two transfer
    learning strategies. Firstly, they introduced weight transfer learning, an effective
    method for leveraging the weights of a pre-trained 2D classifier network by incorporating
    it into a network of the same or higher dimension. The second approach they proposed
    is dimensional transfer learning, which relies on extrapolating 3D weights from
    a pre-trained 2D network. Empirical evidence demonstrates that their methods outperform
    current state-of-the-art techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Overview of recent methods in *Only Limited Supervision* category.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Task | Algorithm Design | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| [[328](#bib.bib328)] | kidney tumor segmentation | Data Augmentation | KiTS19
    | Mean Dice: 77.44 |'
  prefs: []
  type: TYPE_TB
- en: '| [[343](#bib.bib343)] | Brain tumour segmentation | Learning-based technique
    for data augmentation | T1-weighted MRI brain scans described in [[386](#bib.bib386)]
    | Dice score: 0.815 |'
  prefs: []
  type: TYPE_TB
- en: '| [[348](#bib.bib348)] | Clinical endoscopy image classification | Angular
    margin metric to ProtoNet | miniEndoGI classification datase | 5-way: 1-shot:
    58.76 $\pm$ 1.64, 5-shot: 66.72 $\pm$ 1.35; 3-way: 1-shot: 75.06 $\pm$ 1.87, 5-shot:
    81.20 $\pm$ 1.72; 2-way: 1-shot: 85.60 $\pm$ 2.21, 5-shot: 90.60 $\pm$ 1.70 |'
  prefs: []
  type: TYPE_TB
- en: '| [[351](#bib.bib351)] | Cardiac and organ segmentation | Prototype-based network
    + Self-supervision | Abdominal CT; Abdominal T2-SPIR MRI; Cardiac bSSFP MRI |
    Abdominal CT: 1-shot: Dice: 67.62, 5-shot: Dice: 75.91; Abdominal MRI: 1-shot:
    Dice: 76.81, 5-shot: Dice: 80.16; Cardiac: 1-shot: Dice: 77.94, 5-shot: Dice:
    81.66 |'
  prefs: []
  type: TYPE_TB
- en: '| [[352](#bib.bib352)] | Abdomen and cardiac segmentation | Anomaly detection-inspired
    FS + Self-supervision | MS-CMRSeg; CHAOS | CHAOS: Mean DSC: 72.41; MS-CMRSeg:
    Mean DSC: 69.62 |'
  prefs: []
  type: TYPE_TB
- en: '| [[353](#bib.bib353)] | Abdomen and cardiac segmentation | Prototype-based
    network + Self-supervision | MS-CMRSeg; CHAOS; BTCV | CHAOS: Mean 95 HD: 12.5;
    Mean DSC: 80.99; BTCV: Mean 95 HD: 23.60; Mean DSC: 60.94; MS-CMRSeg: Mean 95
    HD: 6.08; Mean DSC: 69.68 |'
  prefs: []
  type: TYPE_TB
- en: '| [[355](#bib.bib355)] | Abdomen segmentation | Context relation encoder +
    Recurrent mask refine- ment module + Prototypical network | ABD-110; BTCV; CHAOS
    | ABD-110: DSC: 81.91; BTCV: DSC: 72.48; CHAOS: DSC: 79.26 |'
  prefs: []
  type: TYPE_TB
- en: '| [[369](#bib.bib369)] | Skin Disease Classification | FSL with Query-Relative
    loss | Dermatology images | 5-way 1-shot: ACC%: 52.41 Precision%: 53.21 F1%: 49.52;
    5-way 5-shot: ACC%: 71.99 Precision%: 74.23 F1%: 70.30 |'
  prefs: []
  type: TYPE_TB
- en: '| [[359](#bib.bib359)] | Brain tumor segmentation | Meta-learning | BraTS2021
    | 1-way 1-shot: DSC($\mu$ ± std)% : 0.57 ± 0.19; 1-way 1-shot: DSC($\mu$ ± std)%
    : 0.63 ± 0.16; 1-way 1-shot: DSC($\mu$ ± std)% : 0.65 ± 0.17 |'
  prefs: []
  type: TYPE_TB
- en: '| [[358](#bib.bib358)] | Classification of whole-genome doubling across 17
    cancer types | Model-Agnostic Meta-Learning | TCGA | AUC (average ± 1 standard
    deviation): 0.6944 ± 0.0773 |'
  prefs: []
  type: TYPE_TB
- en: '| [[363](#bib.bib363)] | Tumour classification | Interpretable structure- constrained
    graph neural network | Private dataset: 150 patients | ACC: 83.3, AUC: 81.9, SEN:
    67.2 and SPE: 85.7 |'
  prefs: []
  type: TYPE_TB
- en: '| [[362](#bib.bib362)] | Cardiac segmentation | Gradient-based meta-hallucination
    learning | MM-WHS 2017 | 4-shots: Average Dice: 75.6, Average ASD: 4.8; 1-shots:
    Average Dice: 51.8, Average ASD: 14.1 |'
  prefs: []
  type: TYPE_TB
- en: '| [[364](#bib.bib364)] | Rare fundus diseases diagnosis | Meta learning + Co-regularization
    loss + Ensemble-learning | FundusData-FS [[364](#bib.bib364)] | Accuracy(%): 2-way:
    1-shot: 71.53, 3-shot: 78.20, 5-shot: 81.47; Accuracy(%): 3-way: 1-shot: 56.69,
    3-shot: 62.62, 5-shot: 66.78; Accuracy(%): 4-way: 1-shot: 48.17, 3-shot: 56.65,
    5-shot: 58.60 |'
  prefs: []
  type: TYPE_TB
- en: '| [[365](#bib.bib365)] | Multi-organ segmentation | Squeeze and excitep framework
    | Visceral dataset | Mean Dice score on validation set: 0.567 |'
  prefs: []
  type: TYPE_TB
- en: '| [[368](#bib.bib368)] | Multi-organ segmentation | Encoder-decoder architecture
    + Spatial and prototypical priors | CHAOS | Left atrium (LA): 1-shot: Mean DSC:
    86.37; 5-shot: Mean DSC: 88.02 Left ventricle (LV): 1-shot: Mean DSC: 87.06; 5-shot:
    Mean DSC: 87.87 |'
  prefs: []
  type: TYPE_TB
- en: 7 Future research scope
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Continual/lifelong learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In healthcare, most intelligent diagnosis systems are limited in their scope,
    often capable of diagnosing only a few diseases. Expanding their capabilities
    after deployment is challenging, preventing them from achieving the breadth of
    diagnoses that medical specialists can. Collecting data for all diseases poses
    significant challenges due to privacy concerns and data sharing constraints. Consequently,
    training a single system to diagnose all diseases simultaneously is impractical.
    One potential solution is to make the system with the ability for continual learning.
    This would allow the system to progressively acquire the capacity to diagnose
    more diseases over time without needing extensive new data for previously learned
    diseases. Continual learning, also known as lifelong learning or incremental learning,
    is a learning paradigm in which a model learns and adapts to new information and
    tasks over time without forgetting previously acquired knowledge [[387](#bib.bib387)].
    Unlike traditional deep learning approaches that assume a fixed dataset and task,
    continual learning addresses scenarios where data arrives continuously, and the
    nature of tasks can evolve over time, including the possibility of introducing
    new classes.
  prefs: []
  type: TYPE_NORMAL
- en: Despite its potential, there has been a limited exploration of continual learning
    in medical contexts. Current research has primarily focused on this paradigm in
    specific areas such as image segmentation [[388](#bib.bib388), [389](#bib.bib389)],
    disease classification [[390](#bib.bib390), [391](#bib.bib391), [392](#bib.bib392),
    [393](#bib.bib393)], and domain adaptation [[394](#bib.bib394), [395](#bib.bib395)].
    Moreover, there is currently no unified framework in continual learning capable
    of accommodating diverse types of annotations in medical applications. We look
    forward to the development of an integrated framework for continual learning that
    can encompass the various settings and challenges highlighted in this paper. Such
    a framework would significantly advance the application of continual learning
    in the medical field, providing a more comprehensive approach to managing evolving
    datasets and diverse annotations.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Incorporating domain knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Incorporating additional information beyond the existing medical datasets has
    emerged as a more promising strategy to tackle the issue of limited-sized medical
    datasets. Within this context, domain knowledge plays a vital role in guiding
    the development of effective deep learning algorithms for MIA. While many models
    used in medical vision are adapted from those designed for natural images, it’s
    worth noting that medical images typically present more complex challenges, such
    as high inter-class similarity, a scarcity of labeled data, and label noise. When
    applied effectively, domain knowledge can mitigate these challenges with reduced
    time and computational requirements. Integrating domain knowledge into deep learning
    algorithms can be achieved by leveraging anatomical details from MRI and CT images
    [[83](#bib.bib83)], exploiting multi-instance data from the same patient [[5](#bib.bib5)],
    incorporating patient metadata [[92](#bib.bib92)], utilizing radiomic features,
    and considering textual reports that accompany the images [[396](#bib.bib396)].
  prefs: []
  type: TYPE_NORMAL
- en: While the utilization of medical domain knowledge in deep learning models is
    a prevalent practice, it is not without its challenges. These challenges involve
    the selection, representation, and integration methods for medical domain knowledge
    [[397](#bib.bib397)]. Identifying such knowledge is a complex task primarily because
    the experiences of medical professionals tend to be subjective and ambiguous.
    It’s often difficult for medical practitioners to provide precise and objective
    descriptions of the experiences they draw upon to complete specific tasks. Currently,
    the identification of medical domain knowledge relies on manual processes, and
    there is no existing method for automatically identifying medical domain knowledge
    within a given field. Medical professionals typically draw from various types
    of domain knowledge simultaneously. Furthermore, Most existing approaches, however,
    incorporate only a single type or a few types of medical domain knowledge, often
    from the same modality. Consequently, simultaneously integrating multiple forms
    of medical domain knowledge has the potential to provide more robust support for
    deep learning models across various medical applications.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Label-efficient learning by vision transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Current label-efficient segmentation techniques primarily rely on convolutional
    neural networks (CNNs). However, there has been a recent transformation in computer
    vision, driven by the introduction of the transformer module [[398](#bib.bib398)].
    This innovation has given rise to vision transformers (ViT) [[399](#bib.bib399)]
    and their adaptations [[400](#bib.bib400)], leading to significant advancements
    in numerous medical applications, including segmentation, detection, and classification
    tasks. Transformer-based models can achieve higher performance when trained on
    extensive datasets, but their effectiveness diminishes when data or annotations
    are scarce. To overcome this challenge, self-supervised transformers offer a promising
    solution. By utilizing unlabeled data and employing proxy tasks like contrastive
    learning and reconstruction, these transformers can enhance their representation
    learning capabilities [[401](#bib.bib401)]. For instance, the Self-Supervised
    SwinUNETR [[99](#bib.bib99)] and unified pre-training [[402](#bib.bib402)] frameworks
    in the medical domain demonstrate that training with large-scale unlabeled 2D
    or 3D images is advantageous for fine-tuning models with smaller datasets. However,
    it’s worth noting that the utilization of pre-training can be computationally
    demanding. Future research directions may aim to simplify and assess the efficiency
    of the pre-training framework, especially regarding its applicability to smaller
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Flexible target model design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To develop better architectures for data-efficient deep learning architectures,
    one promising avenue is the field of automated architectural engineering. Presently,
    the architectures predominantly in use are crafted by human experts through iterative
    processes that are susceptible to errors. To circumvent the need for manual design,
    researchers have put forward the concept of automating architectural engineering,
    with one relevant domain being neural architecture search (NAS), introduced by
    Zoph and Le [[403](#bib.bib403)]. However, it’s essential to note that the majority
    of NAS investigations have been concentrated on image classification tasks [[404](#bib.bib404)].
    Regrettably, this focus has yet to yield truly transformative models capable of
    instigating fundamental shifts [[405](#bib.bib405)]. Nevertheless, the exploration
    of NAS for data-efficient learning in MIA remains a promising avenue.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Federated learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Modern healthcare systems collect significant amounts of medical data, yet
    the complete utilization of this data by deep learning is hindered. This limitation
    stems from the data being isolated within silos and privacy concerns that limit
    data access [[20](#bib.bib20)]. To tackle this challenge, federated learning (FL)
    emerges as a learning paradigm seeking to address the problem of data governance
    and privacy. It achieves this by training algorithms collaboratively without the
    need to exchange the data itself [[406](#bib.bib406)]. FL preserves data privacy
    while collectively improving model efficiency, making it a valuable tool for data-efficient
    deep learning in MIA. FL has produced valuable outcomes in the MIA domain [[407](#bib.bib407),
    [408](#bib.bib408), [409](#bib.bib409)]. Nevertheless, existing FL algorithms
    are predominantly trained using supervised methods. When implementing FL in real-world
    MIA situations, a critical issue arises: *label scarcity* can occur in local healthcare
    datasets. Different medical centers may have varying degrees of missing labels,
    or the label granularity may differ. A potential avenue for research is the development
    of label-efficient federated learning techniques to tackle this challenge.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Data-efficient learning with text supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text supervision involves using textual descriptions or labels as additional
    sources of information during training. This can include utilizing clinical reports,
    medical terminology, or textual metadata associated with images or patient records.
    By incorporating text supervision, MIA models can learn to associate medical text
    with visual patterns in images, facilitating improved generalization and a deeper
    understanding of medical data. Some studies have explored this approach [[396](#bib.bib396),
    [410](#bib.bib410)]. We encourage future investigations to further explore and
    expand upon this area of study.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The challenge of acquiring high-quality labels remains a significant hurdle
    for supervised learning in Medical Image Analysis (MIA). This challenge has fueled
    interest in alternative approaches that enhance labeling efficiency to reduce
    the labeled data requirement. In recent years, extensive research efforts have
    been dedicated to advancing data-efficient learning within the realm of medical
    images, resulting in the development of numerous techniques applicable across
    diverse application domains. In this paper, we have provided a comprehensive review
    that explores the recent progress in data-efficient deep learning for MIA. Specifically,
    we conducted a thorough examination of deep learning-based data-efficient methodologies
    and categorized them into five distinct groups. These categorizations are rooted
    in the varying degrees of supervision they depend on, covering a spectrum from
    scenarios with no supervision to those involving inexact, incomplete, inaccurate,
    and only limited supervision. Finally, we highlight several potential future directions
    for research and development in this area. We hope that this survey serves as
    a valuable resource, offering insights into the current state of data-efficient
    deep learning in medical imaging and inspiring further progress in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] E. J. Topol, High-performance medicine: the convergence of human and artificial
    intelligence, Nature medicine 25 (1) (2019) 44–56.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] N. Hashimoto, D. Fukushima, R. Koga, Y. Takagi, K. Ko, K. Kohno, M. Nakaguro,
    S. Nakamura, H. Hontani, I. Takeuchi, Multi-scale domain-adversarial multiple-instance
    cnn for cancer subtype classification with unannotated histopathological images,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2020, pp. 3852–3861.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Z. Shao, H. Bian, Y. Chen, Y. Wang, J. Zhang, X. Ji, et al., Transmil:
    Transformer based correlated multiple instance learning for whole slide image
    classification, Advances in neural information processing systems 34 (2021) 2136–2147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Xue, Q. Dou, X. Shi, H. Chen, P.-A. Heng, Robust learning at noisy labeled
    medical images: Applied to skin lesion classification, in: 2019 IEEE 16th International
    Symposium on Biomedical Imaging (ISBI 2019), IEEE, 2019, pp. 1280–1283.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] S. Azizi, B. Mustafa, F. Ryan, Z. Beaver, J. Freyberg, J. Deaton, A. Loh,
    A. Karthikesalingam, S. Kornblith, T. Chen, et al., Big self-supervised models
    advance medical image classification, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2021, pp. 3478–3488.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] K. Yan, J. Cai, D. Jin, S. Miao, D. Guo, A. P. Harrison, Y. Tang, J. Xiao,
    J. Lu, L. Lu, Sam: Self-supervised learning of pixel-wise anatomical embeddings
    in radiological images, IEEE Transactions on Medical Imaging 41 (10) (2022) 2658–2669.
    [doi:10.1109/TMI.2022.3169003](https://doi.org/10.1109/TMI.2022.3169003).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] G. Campanella, M. G. Hanna, L. Geneslaw, A. Miraflor, V. Werneck Krauss Silva,
    K. J. Busam, E. Brogi, V. E. Reuter, D. S. Klimstra, T. J. Fuchs, Clinical-grade
    computational pathology using weakly supervised deep learning on whole slide images,
    Nature medicine 25 (8) (2019) 1301–1309.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] H. Zhu, J. Shi, J. Wu, Pick-and-learn: Automatic quality evaluation for
    noisy-labeled image segmentation, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October
    13–17, 2019, Proceedings, Part VI 22, Springer, 2019, pp. 576–584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Taleb, W. Loetzsch, N. Danz, J. Severin, T. Gaertner, B. Bergner, C. Lippert,
    3d self-supervised methods for medical imaging, Advances in neural information
    processing systems 33 (2020) 18158–18172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Á. S. Hervella, J. Rouco, J. Novo, M. Ortega, Learning the retinal anatomy
    from scarce annotated data using self-supervised multimodal reconstruction, Applied
    Soft Computing 91 (2020) 106210.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] C. Chen, Q. Dou, H. Chen, J. Qin, P.-A. Heng, Synergistic image and feature
    adaptation: Towards cross-modality domain adaptation for medical image segmentation,
    in: Proceedings of the AAAI conference on artificial intelligence, Vol. 33, 2019,
    pp. 865–872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, C. I. Sánchez, A survey on deep learning
    in medical image analysis, Medical image analysis 42 (2017) 60–88.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] X. Chen, X. Wang, K. Zhang, K.-M. Fung, T. C. Thai, K. Moore, R. S. Mannel,
    H. Liu, B. Zheng, Y. Qiu, Recent advances and clinical applications of deep learning
    in medical image analysis, Medical Image Analysis 79 (2022) 102444.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for
    biomedical image segmentation, in: Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18, Springer, 2015, pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. J. Willemink, W. A. Koszek, C. Hardell, J. Wu, D. Fleischmann, H. Harvey,
    L. R. Folio, R. M. Summers, D. L. Rubin, M. P. Lungren, Preparing medical imaging
    data for machine learning, Radiology 295 (1) (2020) 4–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,
    S. Venugopalan, K. Widner, T. Madams, J. Cuadros, et al., Development and validation
    of a deep learning algorithm for detection of diabetic retinopathy in retinal
    fundus photographs, jama 316 (22) (2016) 2402–2410.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, S. Thrun,
    Dermatologist-level classification of skin cancer with deep neural networks, nature
    542 (7639) (2017) 115–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] D. Gurari, D. Theriault, M. Sameki, B. Isenberg, T. A. Pham, A. Purwada,
    P. Solski, M. Walker, C. Zhang, J. Y. Wong, M. Betke, How to collect segmentations
    for biomedical images? a benchmark evaluating the performance of experts, crowdsourced
    non-experts, and algorithms, in: 2015 IEEE Winter Conference on Applications of
    Computer Vision, 2015, pp. 1169–1176. [doi:10.1109/WACV.2015.160](https://doi.org/10.1109/WACV.2015.160).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S. Albarqouni, C. Baur, F. Achilles, V. Belagiannis, S. Demirci, N. Navab,
    Aggnet: deep learning from crowds for mitosis detection in breast cancer histology
    images, IEEE transactions on medical imaging 35 (5) (2016) 1313–1321.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] P. Rajpurkar, E. Chen, O. Banerjee, E. J. Topol, Ai in health and medicine,
    Nature medicine 28 (1) (2022) 31–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] V. Cheplygina, M. de Bruijne, J. P. Pluim, Not-so-supervised: a survey
    of semi-supervised, multi-instance, and transfer learning in medical image analysis,
    Medical image analysis 54 (2019) 280–296.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, X. Ding, Embracing
    imperfect datasets: A review of deep learning solutions for medical image segmentation,
    Medical Image Analysis 63 (2020) 101693.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] C. Jin, Z. Guo, Y. Lin, L. Luo, H. Chen, Label-efficient deep learning
    in medical image analysis: Challenges and future directions, arXiv preprint arXiv:2303.12484
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Shiraishi, S. Katsuragawa, J. Ikezoe, T. Matsumoto, T. Kobayashi, K.-i.
    Komatsu, M. Matsui, H. Fujita, Y. Kodera, K. Doi, Development of a digital image
    database for chest radiographs with and without a lung nodule: receiver operating
    characteristic analysis of radiologists’ detection of pulmonary nodules, American
    Journal of Roentgenology 174 (1) (2000) 71–74.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] C. R. Jack Jr, M. A. Bernstein, N. C. Fox, P. Thompson, G. Alexander,
    D. Harvey, B. Borowski, P. J. Britson, J. L. Whitwell, C. Ward, et al., The alzheimer’s
    disease neuroimaging initiative (adni): Mri methods, Journal of Magnetic Resonance
    Imaging: An Official Journal of the International Society for Magnetic Resonance
    in Medicine 27 (4) (2008) 685–691.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. W. Weiner, D. P. Veitch, P. S. Aisen, L. A. Beckett, N. J. Cairns,
    R. C. Green, D. Harvey, C. R. Jack Jr, W. Jagust, J. C. Morris, et al., The alzheimer’s
    disease neuroimaging initiative 3: Continued innovation for clinical trial improvement,
    Alzheimer’s & Dementia 13 (5) (2017) 561–571.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby,
    Y. Burren, N. Porz, J. Slotboom, R. Wiest, et al., The multimodal brain tumor
    image segmentation benchmark (brats), IEEE transactions on medical imaging 34 (10)
    (2014) 1993–2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] A. Makropoulos, E. C. Robinson, A. Schuh, R. Wright, S. Fitzgibbon, J. Bozek,
    S. J. Counsell, J. Steinweg, K. Vecchiato, J. Passerat-Palmbach, et al., The developing
    human connectome project: A minimal processing pipeline for neonatal cortical
    surface reconstruction, Neuroimage 173 (2018) 88–112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] R. Souza, O. Lucena, J. Garrafa, D. Gobbi, M. Saluzzi, S. Appenzeller,
    L. Rittner, R. Frayne, R. Lotufo, An open, multi-vendor, multi-field-strength
    brain mr dataset and analysis of publicly available skull stripping methods agreement,
    NeuroImage 170 (2018) 482–494.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] H. J. Kuijf, J. M. Biesbroek, J. De Bresser, R. Heinen, S. Andermatt,
    M. Bento, M. Berseth, M. Belyaev, M. J. Cardoso, A. Casamitjana, et al., Standardized
    assessment of automatic segmentation of white matter hyperintensities and results
    of the wmh segmentation challenge, IEEE transactions on medical imaging 38 (11)
    (2019) 2556–2568.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] C.-G. Yan, X. Chen, L. Li, F. X. Castellanos, T.-J. Bai, Q.-J. Bo, J. Cao,
    G.-M. Chen, N.-X. Chen, W. Chen, et al., Reduced default mode network functional
    connectivity in patients with recurrent major depressive disorder, Proceedings
    of the National Academy of Sciences 116 (18) (2019) 9078–9083.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] X. Zhuang, J. Shen, Multi-scale patch and multi-modality atlases for whole
    heart segmentation of mri, Medical image analysis 31 (2016) 77–87.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] X. Zhuang, L. Li, C. Payer, D. vStern, M. Urschler, M. P. Heinrich, J. Oster,
    C. Wang, Ö. Smedby, C. Bian, et al., Evaluation of algorithms for multi-modality
    whole heart segmentation: an open-access grand challenge, Medical image analysis
    58 (2019) 101537.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng,
    I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester, et al., Deep learning techniques
    for automatic mri cardiac multi-structures segmentation and diagnosis: is the
    problem solved?, IEEE transactions on medical imaging 37 (11) (2018) 2514–2525.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Z. Xiong, Q. Xia, Z. Hu, N. Huang, C. Bian, Y. Zheng, S. Vesal, N. Ravikumar,
    A. Maier, X. Yang, et al., A global benchmark of algorithms for segmenting the
    left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging,
    Medical image analysis 67 (2021) 101832.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] X. Zhuang, Multivariate mixture model for cardiac segmentation from multi-sequence
    mri, in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2016, pp. 581–588.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] V. M. Campello, P. Gkontra, C. Izquierdo, C. Martin-Isla, A. Sojoudi,
    P. M. Full, K. Maier-Hein, Y. Zhang, Z. He, J. Ma, et al., Multi-centre, multi-vendor
    and multi-disease cardiac segmentation: the m&ms challenge, IEEE Transactions
    on Medical Imaging 40 (12) (2021) 3543–3554.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] J. Sivaswamy, S. Krishnadas, G. D. Joshi, M. Jain, A. U. S. Tabish, Drishti-gs:
    Retinal image dataset for optic nerve head (onh) segmentation, in: 2014 IEEE 11th
    international symposium on biomedical imaging (ISBI), IEEE, 2014, pp. 53–56.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] H. Bogunović, F. Venhuizen, S. Klimscha, S. Apostolopoulos, A. Bab-Hadiashar,
    U. Bagci, M. F. Beg, L. Bekalo, Q. Chen, C. Ciller, et al., Retouch: The retinal
    oct fluid detection and segmentation benchmark and challenge, IEEE transactions
    on medical imaging 38 (8) (2019) 1858–1874.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] D. S. Kermany, M. Goldbaum, W. Cai, C. C. Valentim, H. Liang, S. L. Baxter,
    A. McKeown, G. Yang, X. Wu, F. Yan, et al., Identifying medical diagnoses and
    treatable diseases by image-based deep learning, cell 172 (5) (2018) 1122–1131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] H. Fu, F. Li, J. I. Orlando, H. Bogunović, X. Sun, J. Liao, Y. Xu, S. Zhang,
    X. Zhang, [Palm: Pathologic myopia challenge](https://dx.doi.org/10.21227/55pk-8z03)
    (2019). [doi:10.21227/55pk-8z03](https://doi.org/10.21227/55pk-8z03).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://dx.doi.org/10.21227/55pk-8z03](https://dx.doi.org/10.21227/55pk-8z03)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[42] J. I. Orlando, H. Fu, J. B. Breda, K. Van Keer, D. R. Bathula, A. Diaz-Pinto,
    R. Fang, P.-A. Heng, J. Kim, J. Lee, et al., Refuge challenge: A unified framework
    for evaluating automated methods for glaucoma assessment from fundus photographs,
    Medical image analysis 59 (2020) 101570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] H. Fang, F. Li, H. Fu, X. Sun, X. Cao, F. Lin, J. Son, S. Kim, G. Quellec,
    S. Matta, et al., Adam challenge: Detecting age-related macular degeneration from
    fundus images, IEEE Transactions on Medical Imaging 41 (10) (2022) 2828–2847.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. Hu, Z. Liao, Y. Xia, Domain specific convolution and high frequency
    reconstruction based unsupervised domain adaptation for medical image segmentation,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2022, pp. 650–659.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] R. Ludovic, R. Daniel, L. Nicolas, K. Maria, I. Humayun, K. Jacques, C. Frédérique,
    G. Catherine, et al., Mitosis detection in breast cancer histological images an
    icpr 2012 contest, Journal of pathology informatics 4 (1) (2013) 8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] M. Veta, Y. J. Heng, N. Stathonikos, B. E. Bejnordi, F. Beca, T. Wollmann,
    K. Rohr, M. A. Shah, D. Wang, M. Rousson, et al., Predicting breast tumor proliferation
    from whole-slide images: the tupac16 challenge, Medical image analysis 54 (2019)
    111–121.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] G. Litjens, P. Bandi, B. Ehteshami Bejnordi, O. Geessink, M. Balkenhol,
    P. Bult, A. Halilovic, M. Hermsen, R. van de Loo, R. Vogels, et al., 1399 h&e-stained
    sentinel lymph node sections of breast cancer patients: the camelyon dataset,
    GigaScience 7 (6) (2018) giy065.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] G. Aresta, T. Araújo, S. Kwok, S. S. Chennamsetty, M. Safwan, V. Alex,
    B. Marami, M. Prastawa, M. Chan, M. Donovan, et al., Bach: Grand challenge on
    breast cancer histology images, Medical image analysis 56 (2019) 122–139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] A. R. Saikia, K. Bora, L. B. Mahanta, A. K. Das, Comparative assessment
    of cnn architectures for classification of breast fnac images, Tissue and Cell
    57 (2019) 8–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Petrick, S. Akbar, K. H. Cha, S. Nofech-Mozes, B. Sahiner, M. A. Gavrielides,
    J. Kalpathy-Cramer, K. Drukker, A. L. Martel, f. t. BreastPathQ Challenge Group,
    Spie-aapm-nci breastpathq challenge: an image analysis challenge for quantitative
    tumor cellularity assessment in breast cancer histology images following neoadjuvant
    treatment, Journal of Medical Imaging 8 (3) (2021) 034501–034501.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] H. A. Phoulady, P. R. Mouton, A new cervical cytology dataset for nucleus
    detection and image classification (cervix93) and methods for cervical nucleus
    detection, arXiv preprint arXiv:1811.09651 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] E. Hussain, L. B. Mahanta, H. Borah, C. R. Das, Liquid based-cytology
    pap smear dataset for automated multi-class diagnosis of pre-cancerous and cervical
    cancer lesions, Data in brief 30 (2020) 105589.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A. E. Kavur, N. S. Gezer, M. Barıcs, S. Aslan, P.-H. Conze, V. Groza,
    D. D. Pham, S. Chatterjee, P. Ernst, S. Özkan, et al., Chaos challenge-combined
    (ct-mr) healthy abdominal organ segmentation, Medical Image Analysis 69 (2021)
    101950.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] F. Su, Y. Sun, Y. Hu, P. Yuan, X. Wang, Q. Wang, J. Li, J.-F. Ji, Development
    and validation of a deep learning system for ascites cytopathology interpretation,
    Gastric Cancer 23 (2020) 1041–1050.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane, A. Sethi, A dataset
    and a technique for generalized nuclear segmentation for computational pathology,
    IEEE transactions on medical imaging 36 (7) (2017) 1550–1560.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. Davidson,
    S. P. Pereira, M. J. Clarkson, D. C. Barratt, Automatic multi-organ segmentation
    on abdominal ct with dense v-networks, IEEE transactions on medical imaging 37 (8)
    (2018) 1822–1834.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] K. Yan, X. Wang, L. Lu, R. M. Summers, Deeplesion: automated mining of
    large-scale lesion annotations and universal lesion detection with deep learning,
    Journal of medical imaging 5 (3) (2018) 036501–036501.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] M. Aubreville, N. Stathonikos, C. A. Bertram, R. Klopfleisch, N. Ter Hoeve,
    F. Ciompi, F. Wilm, C. Marzahl, T. A. Donovan, A. Maier, et al., Mitosis domain
    generalization in histopathology images—the midog challenge, Medical Image Analysis
    84 (2023) 102699.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] K. Sirinukunwattana, S. E. A. Raza, Y.-W. Tsang, D. R. Snead, I. A. Cree,
    N. M. Rajpoot, Locality sensitive deep learning for detection and classification
    of nuclei in routine colon cancer histology images, IEEE transactions on medical
    imaging 35 (5) (2016) 1196–1206.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] J. N. Kather, J. Krisam, P. Charoentong, T. Luedde, E. Herpel, C.-A. Weis,
    T. Gaiser, A. Marx, N. A. Valous, D. Ferber, et al., Predicting survival from
    colorectal cancer histology slides using deep learning: A retrospective multicenter
    study, PLoS medicine 16 (1) (2019) e1002730.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] G. Litjens, R. Toth, W. Van De Ven, C. Hoeks, S. Kerkstra, B. Van Ginneken,
    G. Vincent, G. Guillard, N. Birbeck, J. Zhang, et al., Evaluation of prostate
    segmentation algorithms for mri: the promise12 challenge, Medical image analysis
    18 (2) (2014) 359–373.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] E. Arvaniti, K. S. Fricker, M. Moret, N. Rupp, T. Hermanns, C. Fankhauser,
    N. Wey, P. J. Wild, J. H. Rueschoff, M. Claassen, Automated gleason grading of
    prostate cancer tissue microarrays via deep learning, Scientific reports 8 (1)
    (2018) 12054.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] W. Bulten, K. Kartasalo, P.-H. C. Chen, P. Ström, H. Pinckaers, K. Nagpal,
    Y. Cai, D. F. Steiner, H. van Boven, R. Vink, et al., Artificial intelligence
    for diagnosis and gleason grading of prostate cancer: the panda challenge, Nature
    medicine 28 (1) (2022) 154–163.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] F. Prados, J. Ashburner, C. Blaiotta, T. Brosch, J. Carballido-Gamio,
    M. J. Cardoso, B. N. Conrad, E. Datta, G. Dávid, B. De Leener, et al., Spinal
    cord grey matter segmentation challenge, Neuroimage 152 (2017) 312–329.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. Wáng, P.-X. Lu, G. Thoma,
    Two public chest x-ray datasets for computer-aided screening of pulmonary diseases,
    Quantitative imaging in medicine and surgery 4 (6) (2014) 475.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Summers, Chestx-ray8:
    Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification
    and localization of common thorax diseases, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, 2017, pp. 2097–2106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] A. E. Johnson, T. J. Pollard, S. J. Berkowitz, N. R. Greenbaum, M. P.
    Lungren, C.-y. Deng, R. G. Mark, S. Horng, Mimic-cxr, a de-identified publicly
    available database of chest radiographs with free-text reports, Scientific data
    6 (1) (2019) 317.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Zha, W. Liang, C. Wang,
    K. Wang, et al., Clinically applicable ai system for accurate diagnosis, quantitative
    measurements, and prognosis of covid-19 pneumonia using computed tomography, Cell
    181 (6) (2020) 1423–1433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Z. Lambert, C. Petitjean, B. Dubray, S. Kuan, Segthor: Segmentation of
    thoracic organs at risk in ct images, in: 2020 Tenth International Conference
    on Image Processing Theory, Tools and Applications (IPTA), IEEE, 2020, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] H. Q. Nguyen, K. Lam, L. T. Le, H. H. Pham, D. Q. Tran, D. B. Nguyen,
    D. D. Le, C. M. Pham, H. T. Tong, D. H. Dinh, et al., Vindr-cxr: An open dataset
    of chest x-rays with radiologist’s annotations, Scientific Data 9 (1) (2022) 429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] S. Shurrab, R. Duwairi, Self-supervised learning methods and applications
    in medical imaging analysis: A survey, PeerJ Computer Science 8 (2022) e1045.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] C. Doersch, A. Gupta, A. A. Efros, Unsupervised visual representation
    learning by context prediction, in: Proceedings of the IEEE international conference
    on computer vision, 2015, pp. 1422–1430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] M. Noroozi, P. Favaro, Unsupervised learning of visual representations
    by solving jigsaw puzzles, in: European conference on computer vision, Springer,
    2016, pp. 69–84.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] S. Gidaris, P. Singh, N. Komodakis, Unsupervised representation learning
    by predicting image rotations, in: International Conference on Learning Representations,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] W. Bai, C. Chen, G. Tarroni, J. Duan, F. Guitton, S. E. Petersen, Y. Guo,
    P. M. Matthews, D. Rueckert, Self-supervised learning for cardiac mr image segmentation
    by anatomical position prediction, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October
    13–17, 2019, Proceedings, Part II 22, Springer, 2019, pp. 541–549.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] A. Taleb, C. Lippert, T. Klein, M. Nabi, Multimodal self-supervised learning
    for medical image analysis, in: International conference on information processing
    in medical imaging, Springer, 2021, pp. 661–673.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] X. Zhuang, Y. Li, Y. Hu, K. Ma, Y. Yang, Y. Zheng, Self-supervised feature
    learning for 3d medical images by playing a rubik’s cube, in: Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference,
    Shenzhen, China, October 13–17, 2019, Proceedings, Part IV 22, Springer, 2019,
    pp. 420–428.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] J. Zhu, Y. Li, Y. Hu, K. Ma, S. K. Zhou, Y. Zheng, Rubik’s cube+: A self-supervised
    feature learning framework for 3d medical image analysis, Medical image analysis
    64 (2020) 101746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] X.-B. Nguyen, G. S. Lee, S. H. Kim, H. J. Yang, Self-supervised learning
    based on spatial awareness for medical image analysis, IEEE Access 8 (2020) 162973–162981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] L. Chen, P. Bentley, K. Mori, K. Misawa, M. Fujiwara, D. Rueckert, Self-supervised
    learning for medical image analysis using image context restoration, Medical image
    analysis 58 (2019) 101539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] H.-Y. Zhou, C. Lu, C. Chen, S. Yang, Y. Yu, A unified visual information
    preservation framework for self-supervised pre-training in medical image analysis,
    IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] T. Ross, D. Zimmerer, A. Vemuri, F. Isensee, M. Wiesenfarth, S. Bodenstedt,
    F. Both, P. Kessler, M. Wagner, B. Müller, et al., Exploiting the potential of
    unlabeled endoscopic video data with self-supervised learning, International journal
    of computer assisted radiology and surgery 13 (2018) 925–933.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Z. Zhou, V. Sodha, M. M. Rahman Siddiquee, R. Feng, N. Tajbakhsh, M. B.
    Gotway, J. Liang, Models genesis: Generic autodidactic models for 3d medical image
    analysis, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings,
    Part IV 22, Springer, 2019, pp. 384–393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] O. G. Holmberg, N. D. Köhler, T. Martins, J. Siedlecki, T. Herold, L. Keidel,
    B. Asani, J. Schiefelbein, S. Priglinger, K. U. Kortuem, et al., Self-supervised
    retinal thickness prediction enables deep learning from unlabelled data to boost
    classification of diabetic retinopathy, Nature Machine Intelligence 2 (11) (2020)
    719–726.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] M. Prakash, T.-O. Buchholz, M. Lalit, P. Tomancak, F. Jug, A. Krull, Leveraging
    self-supervised denoising for image segmentation, in: 2020 IEEE 17th international
    symposium on biomedical imaging (ISBI), IEEE, 2020, pp. 428–432.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] X. Tao, Y. Li, W. Zhou, K. Ma, Y. Zheng, Revisiting rubik’s cube: self-supervised
    learning with volume-wise transformation for 3d medical image segmentation, in:
    Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International
    Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part IV 23, Springer,
    2020, pp. 238–248.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] K. Chaitanya, E. Erdil, N. Karani, E. Konukoglu, Contrastive learning
    of global and local features for medical image segmentation with limited annotations,
    Advances in neural information processing systems 33 (2020) 12546–12558.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C. Zhang, H. Zheng, Y. Gu, Dive into the details of self-supervised learning
    for medical image analysis, Medical Image Analysis 89 (2023) 102879.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] A. Taleb, M. Kirchler, R. Monti, C. Lippert, Contig: Self-supervised multimodal
    contrastive learning for medical imaging with genetics, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 20908–20921.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] H. Sowrirajan, J. Yang, A. Y. Ng, P. Rajpurkar, Moco pretraining improves
    representation and transferability of chest x-ray models, in: Medical Imaging
    with Deep Learning, PMLR, 2021, pp. 728–744.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] K. He, H. Fan, Y. Wu, S. Xie, R. Girshick, Momentum contrast for unsupervised
    visual representation learning, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, 2020, pp. 9729–9738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Y. N. T. Vu, R. Wang, N. Balachandar, C. Liu, A. Y. Ng, P. Rajpurkar,
    Medaug: Contrastive learning leveraging patient metadata improves representations
    for chest x-ray interpretation, in: Machine Learning for Healthcare Conference,
    PMLR, 2021, pp. 755–769.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] T. Chen, S. Kornblith, M. Norouzi, G. Hinton, A simple framework for contrastive
    learning of visual representations, in: International conference on machine learning,
    PMLR, 2020, pp. 1597–1607.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] O. Ciga, T. Xu, A. L. Martel, Self supervised contrastive learning for
    digital histopathology, Machine Learning with Applications 7 (2022) 100198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Y. He, G. Yang, R. Ge, Y. Chen, J.-L. Coatrieux, B. Wang, S. Li, Geometric
    visual similarity learning in 3d medical image self-supervised pre-training, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2023, pp. 9538–9547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] F. Haghighi, M. R. Hosseinzadeh Taher, Z. Zhou, M. B. Gotway, J. Liang,
    Learning semantics-enriched representation via self-discovery, self-classification,
    and self-restoration, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings,
    Part I 23, Springer, 2020, pp. 137–147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] X. Zhang, S. Feng, Y. Zhou, Y. Zhang, Y. Wang, Sar: Scale-aware restoration
    learning for 3d tumor segmentation, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part II 24, Springer, 2021, pp. 124–133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] H.-Y. Zhou, C. Lu, S. Yang, X. Han, Y. Yu, Preservational learning improves
    self-supervised medical image models by reconstructing diverse contexts, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 3499–3509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Y. Tang, D. Yang, W. Li, H. R. Roth, B. Landman, D. Xu, V. Nath, A. Hatamizadeh,
    Self-supervised pre-training of swin transformers for 3d medical image analysis,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2022, pp. 20730–20740.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] P. Yang, X. Yin, H. Lu, Z. Hu, X. Zhang, R. Jiang, H. Lv, Cs-co: A hybrid
    self-supervised visual representation learning method for h&e-stained histopathological
    images, Medical Image Analysis 81 (2022) 102539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X. Yan, J. Naushad, S. Sun, K. Han, H. Tang, D. Kong, H. Ma, C. You,
    X. Xie, Representation recovering for self-supervised pre-training on medical
    images, in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision, 2023, pp. 2685–2695.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] E. Tiu, E. Talius, P. Patel, C. P. Langlotz, A. Y. Ng, P. Rajpurkar,
    Expert-level detection of pathologies from unannotated chest x-ray images via
    self-supervised learning, Nature Biomedical Engineering 6 (12) (2022) 1399–1406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] L. Qu, S. Liu, X. Liu, M. Wang, Z. Song, Towards label-efficient automatic
    diagnosis and analysis: a comprehensive survey of advanced deep learning-based
    weakly-supervised, semi-supervised and self-supervised techniques in histopathological
    image analysis, Physics in Medicine & Biology (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] X. Wang, Y. Yan, P. Tang, X. Bai, W. Liu, Revisiting multiple instance
    neural networks, Pattern Recognition 74 (2018) 15–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] E. Schwab, A. Gooßen, H. Deshpande, A. Saalbach, Localization of critical
    findings in chest x-ray without local annotations using multi-instance learning,
    in: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), IEEE,
    2020, pp. 1879–1882.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. Ramon, L. De Raedt, Multi instance neural networks, in: Proceedings
    of the ICML-2000 workshop on attribute-value and relational learning, 2000, pp.
    53–60.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] O. Maron, T. Lozano-Pérez, A framework for multiple-instance learning,
    Advances in neural information processing systems 10 (1997).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] O. Z. Kraus, J. L. Ba, B. J. Frey, Classifying and segmenting microscopy
    images with deep multiple instance learning, Bioinformatics 32 (12) (2016) i52–i59.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Y. Yan, X. Wang, X. Guo, J. Fang, W. Liu, J. Huang, Deep multi-instance
    learning with dynamic pooling, in: Asian Conference on Machine Learning, PMLR,
    2018, pp. 662–677.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] H. D. Couture, J. S. Marron, C. M. Perou, M. A. Troester, M. Niethammer,
    Multiple instance learning for heterogeneous images: Training a cnn for histopathology,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st
    International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
    Part II 11, Springer, 2018, pp. 254–262.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] J. Qu, X. Wei, X. Qian, Generalized pancreatic cancer diagnosis via multiple
    instance learning and anatomically-guided shape normalization, Medical Image Analysis
    86 (2023) 102774.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. Zhao, F. Yang, Y. Fang, H. Liu, N. Zhou, J. Zhang, J. Sun, S. Yang,
    B. Menze, X. Fan, et al., Predicting lymph node metastasis using histopathological
    images based on multiple instance learning with deep graph convolution, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.
    4837–4846.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] M. Y. Lu, R. J. Chen, J. Wang, D. Dillon, F. Mahmood, Semi-supervised
    histology classification using deep multiple instance learning and contrastive
    predictive coding, arXiv preprint arXiv:1910.10825 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] A. v. d. Oord, Y. Li, O. Vinyals, Representation learning with contrastive
    predictive coding, arXiv preprint arXiv:1807.03748 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] B. Li, Y. Li, K. W. Eliceiri, Dual-stream multiple instance learning
    network for whole slide image classification with self-supervised contrastive
    learning, in: Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition, 2021, pp. 14318–14328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] P. Chikontwe, M. Luna, M. Kang, K. S. Hong, J. H. Ahn, S. H. Park, Dual
    attention multiple instance learning with unsupervised complementary loss for
    covid-19 screening, Medical Image Analysis 72 (2021) 102105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. Raju, J. Yao, M. M. Haq, J. Jonnagaddala, J. Huang, Graph attention
    multi-instance learning for accurate colorectal cancer staging, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part V 23, Springer, 2020, pp. 529–539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Z. Su, T. E. Tavolara, G. Carreno-Galeano, S. J. Lee, M. N. Gurcan, M. Niazi,
    Attention2majority: Weak multiple instance learning for regenerative kidney grading
    on whole slide images, Medical Image Analysis 79 (2022) 102462.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] M. Adnan, S. Kalra, H. R. Tizhoosh, Representation learning of histopathology
    images using graph neural networks, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Workshops, 2020, pp. 988–989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Y. Sharma, A. Shrivastava, L. Ehsan, C. A. Moskaluk, S. Syed, D. Brown,
    Cluster-to-conquer: A framework for end-to-end multi-instance learning for whole
    slide image classification, in: Medical Imaging with Deep Learning, PMLR, 2021,
    pp. 682–698.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M. Y. Lu, D. F. Williamson, T. Y. Chen, R. J. Chen, M. Barbieri, F. Mahmood,
    Data-efficient and weakly supervised computational pathology on whole-slide images,
    Nature biomedical engineering 5 (6) (2021) 555–570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] R. Yan, Y. Shen, X. Zhang, P. Xu, J. Wang, J. Li, F. Ren, D. Ye, S. K.
    Zhou, Histopathological bladder cancer gene mutation prediction with hierarchical
    deep multiple-instance learning, Medical Image Analysis 87 (2023) 102824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] M. Ilse, J. Tomczak, M. Welling, Attention-based deep multiple instance
    learning, in: International conference on machine learning, PMLR, 2018, pp. 2127–2136.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Y. Wang, P. Tang, Y. Zhou, W. Shen, E. K. Fishman, A. L. Yuille, Learning
    inductive attention guidance for partially supervised pancreatic ductal adenocarcinoma
    prediction, IEEE transactions on medical imaging 40 (10) (2021) 2723–2735.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Z. Li, W. Zhao, F. Shi, L. Qi, X. Xie, Y. Wei, Z. Ding, Y. Gao, S. Wu,
    J. Liu, et al., A novel multiple instance learning framework for covid-19 severity
    assessment via data augmentation and self-supervised learning, Medical Image Analysis
    69 (2021) 101978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Z. Wang, L. Yu, X. Ding, X. Liao, L. Wang, Lymph node metastasis prediction
    from whole slide images with transformer-guided multiinstance learning and knowledge
    transfer, IEEE Transactions on Medical Imaging 41 (10) (2022) 2777–2787.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] T. Zhao, Z. Yin, Weakly supervised cell segmentation by point annotation,
    IEEE Transactions on Medical Imaging 40 (10) (2020) 2736–2747.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] W. Shen, Z. Peng, X. Wang, H. Wang, J. Cen, D. Jiang, L. Xie, X. Yang,
    Q. Tian, A survey on label-efficient deep image segmentation: Bridging the gap
    between weak supervision and dense prediction, IEEE Transactions on Pattern Analysis
    and Machine Intelligence (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep
    features for discriminative localization, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, 2016, pp. 2921–2929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Y. Li, Y. Liu, L. Huang, Z. Wang, J. Luo, Deep weakly-supervised breast
    tumor segmentation in ultrasound images with explicit anatomical constraints,
    Medical image analysis 76 (2022) 102315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Z. Chen, Z. Tian, J. Zhu, C. Li, S. Du, C-cam: Causal cam for weakly
    supervised semantic segmentation on medical image, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2022, pp. 11676–11685.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] S. Khan, A. H. Shahin, J. Villafruela, J. Shen, L. Shao, Extreme points
    derived confidence map as a cue for class-agnostic interactive segmentation using
    deep neural network, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings,
    Part II 22, Springer, 2019, pp. 66–73.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] H. R. Roth, D. Yang, Z. Xu, X. Wang, D. Xu, Going to extremes: weakly
    supervised medical image segmentation, Machine Learning and Knowledge Extraction
    3 (2) (2021) 507–524.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] R. Dorent, S. Joutard, J. Shapey, A. Kujawa, M. Modat, S. Ourselin, T. Vercauteren,
    Inter extreme points geodesics for end-to-end weakly supervised image segmentation,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part II 24, Springer, 2021, pp. 615–624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] H. Qu, P. Wu, Q. Huang, J. Yi, Z. Yan, K. Li, G. M. Riedlinger, S. De,
    S. Zhang, D. N. Metaxas, Weakly supervised deep nuclei segmentation using partial
    points annotation in histopathology images, IEEE transactions on medical imaging
    39 (11) (2020) 3655–3666.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Y. Lin, Z. Qu, H. Chen, Z. Gao, Y. Li, L. Xia, K. Ma, Y. Zheng, K.-T.
    Cheng, Label propagation for annotation-efficient nuclei segmentation from pathology
    images, arXiv preprint arXiv:2202.08195 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] K. Kise, A. Sato, M. Iwata, Segmentation of page images using the area
    voronoi diagram, Computer Vision and Image Understanding 70 (3) (1998) 370–382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Y. Lin, Z. Qu, H. Chen, Z. Gao, Y. Li, L. Xia, K. Ma, Y. Zheng, K.-T.
    Cheng, Nuclei segmentation with point annotations from pathology images via self-supervised
    learning and co-training, Medical Image Analysis (2023) 102933.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] H. Zhang, Y. Meng, Y. Zhao, Y. Qiao, X. Yang, S. E. Coupland, Y. Zheng,
    Dtfd-mil: Double-tier feature distillation multiple instance learning for histopathology
    whole slide image classification, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, 2022, pp. 18802–18812.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Z. Chen, J. Zhang, S. Che, J. Huang, X. Han, Y. Yuan, Diagnose like a
    pathologist: Weakly-supervised pathologist-tree network for slide-level immunohistochemical
    scoring, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35,
    2021, pp. 47–54.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] W. Bai, H. Suzuki, C. Qin, G. Tarroni, O. Oktay, P. M. Matthews, D. Rueckert,
    Recurrent neural networks for aortic image sequence segmentation with sparse annotations,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st
    International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
    Part IV 11, Springer, 2018, pp. 586–594.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Z. Ji, Y. Shen, C. Ma, M. Gao, Scribble-based hierarchical weakly supervised
    learning for brain tumor segmentation, in: Medical Image Computing and Computer
    Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China,
    October 13–17, 2019, Proceedings, Part III 22, Springer, 2019, pp. 175–183.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Q. Chen, Y. Hong, Scribble2d5: Weakly-supervised volumetric image segmentation
    via scribble annotations, in: International Conference on Medical Image Computing
    and Computer-Assisted Intervention, Springer, 2022, pp. 234–243.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Y. B. Can, K. Chaitanya, B. Mustafa, L. M. Koch, E. Konukoglu, C. F.
    Baumgartner, Learning to segment medical images with scribble-supervision alone,
    in: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical
    Decision Support: 4th International Workshop, DLMIA 2018, and 8th International
    Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September
    20, 2018, Proceedings 4, Springer, 2018, pp. 236–244.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] M. Tang, F. Perazzi, A. Djelouah, I. Ben Ayed, C. Schroers, Y. Boykov,
    On regularized losses for weakly-supervised cnn segmentation, in: Proceedings
    of the European Conference on Computer Vision (ECCV), 2018, pp. 507–522.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] X. Luo, M. Hu, W. Liao, S. Zhai, T. Song, G. Wang, S. Zhang, Scribble-supervised
    medical image segmentation via dual-branch network and dynamically mixed pseudo
    labels supervision, in: International Conference on Medical Image Computing and
    Computer-Assisted Intervention, Springer, 2022, pp. 528–538.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] G. Valvano, A. Leo, S. A. Tsaftaris, Learning to segment from scribbles
    using multi-scale adversarial attention gates, IEEE Transactions on Medical Imaging
    40 (8) (2021) 1990–2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] P. Zhang, Y. Zhong, X. Li, Accl: Adversarial constrained-cnn loss for
    weakly supervised medical image segmentation, arXiv preprint arXiv:2005.00328
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] K. Zhang, X. Zhuang, Cyclemix: A holistic strategy for medical image
    segmentation from scribble supervision, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 11656–11665.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] F. Gao, M. Hu, M.-E. Zhong, S. Feng, X. Tian, X. Meng, Z. Huang, M. Lv,
    T. Song, X. Zhang, et al., Segmentation only uses sparse annotations: Unified
    weakly and semi-supervised learning in medical images, Medical Image Analysis
    80 (2022) 102515.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] K. Zhang, X. Zhuang, Shapepu: A new pu learning framework regularized
    by global consistency for scribble supervised cardiac segmentation, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer,
    2022, pp. 162–172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] H. Lee, W.-K. Jeong, Scribble2label: Scribble-supervised cell segmentation
    via self-generating pseudo-labels with consistency, in: Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part I 23, Springer, 2020, pp. 14–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] M. Rajchl, M. C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach, W. Bai,
    M. Damodaram, M. A. Rutherford, J. V. Hajnal, B. Kainz, et al., Deepcut: Object
    segmentation from bounding box annotations using convolutional neural networks,
    IEEE transactions on medical imaging 36 (2) (2016) 674–683.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] J. Wang, B. Xia, Bounding box tightness prior for weakly supervised image
    segmentation, in: International conference on medical image computing and computer-assisted
    intervention, Springer, 2021, pp. 526–536.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] J. Wei, Y. Hu, G. Li, S. Cui, S. Kevin Zhou, Z. Li, Boxpolyp: Boost generalized
    polyp segmentation using extra coarse bounding box annotations, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer,
    2022, pp. 67–77.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] R. Jiao, Y. Zhang, L. Ding, R. Cai, J. Zhang, Learning with limited annotations:
    a survey on deep semi-supervised learning for medical image segmentation, arXiv
    preprint arXiv:2207.14191 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] X. Yang, Z. Song, I. King, Z. Xu, A survey on deep semi-supervised learning,
    IEEE Transactions on Knowledge and Data Engineering 35 (9) (2023) 8934–8954. [doi:10.1109/TKDE.2022.3220219](https://doi.org/10.1109/TKDE.2022.3220219).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] G. Bortsova, F. Dubost, L. Hogeweg, I. Katramados, M. De Bruijne, Semi-supervised
    medical image segmentation via learning consistency under transformations, in:
    Medical Image Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International
    Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part VI 22, Springer,
    2019, pp. 810–818.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] X. Li, L. Yu, H. Chen, C.-W. Fu, L. Xing, P.-A. Heng, Transformation-consistent
    self-ensembling model for semisupervised medical image segmentation, IEEE Transactions
    on Neural Networks and Learning Systems 32 (2) (2020) 523–534.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, C. A.
    Raffel, Mixmatch: A holistic approach to semi-supervised learning, Advances in
    neural information processing systems 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] H. Basak, R. Bhattacharya, R. Hussain, A. Chatterjee, An embarrassingly
    simple consistency regularization method for semi-supervised medical image segmentation,
    arXiv preprint arXiv:2202.00677 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] K. Zheng, J. Xu, J. Wei, Double noise mean teacher self-ensembling model
    for semi-supervised tumor segmentation, in: ICASSP 2022-2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2022, pp.
    1446–1450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Y. Li, L. Luo, H. Lin, H. Chen, P.-A. Heng, Dual-consistency semi-supervised
    learning with uncertainty quantification for covid-19 lesion segmentation from
    ct images, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2021: 24th International Conference, Strasbourg, France, September 27–October
    1, 2021, Proceedings, Part II 24, Springer, 2021, pp. 199–209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] X. Xu, T. Sanford, B. Turkbey, S. Xu, B. J. Wood, P. Yan, Shadow-consistent
    semi-supervised learning for prostate ultrasound segmentation, IEEE Transactions
    on Medical Imaging 41 (6) (2021) 1331–1345.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Y. Shu, H. Li, B. Xiao, X. Bi, W. Li, Cross-mix monitoring for medical
    image segmentation with limited supervision, IEEE Transactions on Multimedia (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] M. Sajjadi, M. Javanmardi, T. Tasdizen, Regularization with stochastic
    transformations and perturbations for deep semi-supervised learning, Advances
    in neural information processing systems 29 (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] X. Li, L. Yu, H. Chen, C.-W. Fu, P.-A. Heng, Semi-supervised skin lesion
    segmentation via transformation consistent self-ensembling model, arXiv preprint
    arXiv:1808.03887 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] S. Laine, T. Aila, Temporal ensembling for semi-supervised learning,
    in: International Conference on Learning Representations, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] X. Cao, H. Chen, Y. Li, Y. Peng, S. Wang, L. Cheng, Uncertainty aware
    temporal-ensembling model for semi-supervised abus mass segmentation, IEEE transactions
    on medical imaging 40 (1) (2020) 431–443.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] L. Luo, L. Yu, H. Chen, Q. Liu, X. Wang, J. Xu, P.-A. Heng, Deep mining
    external imperfect data for chest x-ray disease screening, IEEE transactions on
    medical imaging 39 (11) (2020) 3583–3594.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] A. Tarvainen, H. Valpola, Mean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results, Advances in
    neural information processing systems 30 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] L. Yu, S. Wang, X. Li, C.-W. Fu, P.-A. Heng, Uncertainty-aware self-ensembling
    model for semi-supervised 3d left atrium segmentation, in: Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference,
    Shenzhen, China, October 13–17, 2019, Proceedings, Part II 22, Springer, 2019,
    pp. 605–613.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Z. Xu, Y. Wang, D. Lu, X. Luo, J. Yan, Y. Zheng, R. K.-y. Tong, Ambiguity-selective
    consistency regularization for mean-teacher semi-supervised medical image segmentation,
    Medical Image Analysis 88 (2023) 102880.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] K. Wang, B. Zhan, C. Zu, X. Wu, J. Zhou, L. Zhou, Y. Wang, Tripled-uncertainty
    guided mean teacher model for semi-supervised medical image segmentation, in:
    Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International
    Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part
    II 24, Springer, 2021, pp. 450–460.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] J. Zhu, B. Bolsterlee, B. V. Chow, Y. Song, E. Meijering, Hybrid dual
    mean-teacher network with double-uncertainty guidance for semi-supervised segmentation
    of mri scans, arXiv preprint arXiv:2303.05126 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] C. Xu, Y. Yang, Z. Xia, B. Wang, D. Zhang, Y. Zhang, S. Zhao, Dual uncertainty-guided
    mixing consistency for semi-supervised 3d medical image segmentation, IEEE Transactions
    on Big Data (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] A. Lou, K. Tawfik, X. Yao, Z. Liu, J. Noble, Min-max similarity: A contrastive
    semi-supervised deep learning network for surgical tools segmentation, IEEE Transactions
    on Medical Imaging (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] S. Sedai, D. Mahapatra, S. Hewavitharanage, S. Maetschke, R. Garnavi,
    Semi-supervised segmentation of optic cup in retinal fundus images using variational
    autoencoder, in: Medical Image Computing and Computer-Assisted Intervention- MICCAI
    2017: 20th International Conference, Quebec City, QC, Canada, September 11-13,
    2017, Proceedings, Part II 20, Springer, 2017, pp. 75–82.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] H. Wu, G. Chen, Z. Wen, J. Qin, Collaborative and adversarial learning
    of focused and dispersive representations for semi-supervised polyp segmentation,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021,
    pp. 3489–3498.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] P. Wang, J. Peng, M. Pedersoli, Y. Zhou, C. Zhang, C. Desrosiers, Cat:
    Constrained adversarial training for anatomically-plausible semi-supervised segmentation,
    IEEE Transactions on Medical Imaging (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Y. Zhang, L. Yang, J. Chen, M. Fredericksen, D. P. Hughes, D. Z. Chen,
    Deep adversarial networks for biomedical image segmentation utilizing unannotated
    images, in: Medical Image Computing and Computer Assisted Intervention- MICCAI
    2017: 20th International Conference, Quebec City, QC, Canada, September 11-13,
    2017, Proceedings, Part III 20, Springer, 2017, pp. 408–416.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] H. Peiris, Z. Chen, G. Egan, M. Harandi, Duo-segnet: adversarial dual-views
    for semi-supervised medical image segmentation, in: Medical Image Computing and
    Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg,
    France, September 27–October 1, 2021, Proceedings, Part II 24, Springer, 2021,
    pp. 428–438.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] J. Hou, X. Ding, J. D. Deng, Semi-supervised semantic segmentation of
    vessel images using leaking perturbations, in: Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, 2022, pp. 2625–2634.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] K. Chaitanya, N. Karani, C. F. Baumgartner, E. Erdil, A. Becker, O. Donati,
    E. Konukoglu, Semi-supervised task-driven data augmentation for medical image
    segmentation, Medical Image Analysis 68 (2021) 101934.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] D. P. Kingma, M. Welling, Auto-encoding variational bayes, arXiv preprint
    arXiv:1312.6114 (2013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] J. Wang, T. Lukasiewicz, Rethinking bayesian deep learning methods for
    semi-supervised volumetric medical image segmentation, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 182–190.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] H. Jiang, Y. Zhou, Y. Lin, R. C. Chan, J. Liu, H. Chen, Deep learning
    for computational cytology: A survey, Medical Image Analysis (2022) 102691.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] D.-P. Fan, T. Zhou, G.-P. Ji, Y. Zhou, G. Chen, H. Fu, J. Shen, L. Shao,
    Inf-net: Automatic covid-19 lung infection segmentation from ct images, IEEE transactions
    on medical imaging 39 (8) (2020) 2626–2637.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] D.-H. Lee, et al., Pseudo-label: The simple and efficient semi-supervised
    learning method for deep neural networks, in: Workshop on challenges in representation
    learning, ICML, Vol. 3, Atlanta, 2013, p. 896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] W. Bai, O. Oktay, M. Sinclair, H. Suzuki, M. Rajchl, G. Tarroni, B. Glocker,
    A. King, P. M. Matthews, D. Rueckert, Semi-supervised learning for network-based
    cardiac mr image segmentation, in: Medical Image Computing and Computer-Assisted
    Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada,
    September 11-13, 2017, Proceedings, Part II 20, Springer, 2017, pp. 253–260.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] G. Wang, S. Zhai, G. Lasio, B. Zhang, B. Yi, S. Chen, T. J. Macvittie,
    D. Metaxas, J. Zhou, S. Zhang, Semi-supervised segmentation of radiation-induced
    pulmonary fibrosis from lung ct scans with multi-scale guided dense attention,
    IEEE transactions on medical imaging 41 (3) (2021) 531–542.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] R. Ke, A. I. Aviles-Rivero, S. Pandey, S. Reddy, C.-B. Schönlieb, A three-stage
    self-training framework for semi-supervised semantic segmentation, IEEE Transactions
    on Image Processing 31 (2022) 1805–1815.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] F. Liu, Y. Tian, Y. Chen, Y. Liu, V. Belagiannis, G. Carneiro, Acpl:
    Anti-curriculum pseudo-labelling for semi-supervised medical image classification,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2022, pp. 20697–20706.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] D. Chen, Y. Bai, W. Shen, Q. Li, L. Yu, Y. Wang, Magicnet: Semi-supervised
    multi-organ segmentation via magic-cube partition and recovery, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
    23869–23878.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] A. Blum, T. Mitchell, Combining labeled and unlabeled data with co-training,
    in: Proceedings of the eleventh annual conference on Computational learning theory,
    1998, pp. 92–100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] L. Zhu, K. Yang, M. Zhang, L. L. Chan, T. K. Ng, B. C. Ooi, Semi-supervised
    unpaired multi-modal learning for label-efficient medical image segmentation,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part II 24, Springer, 2021, pp. 394–404.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] X. Chen, H.-Y. Zhou, F. Liu, J. Guo, L. Wang, Y. Yu, Mass: Modality-collaborative
    semi-supervised segmentation by exploiting cross-modal consistency from unpaired
    ct and mri images, Medical Image Analysis 80 (2022) 102506.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] X. Luo, M. Hu, T. Song, G. Wang, S. Zhang, Semi-supervised medical image
    segmentation via cross teaching between cnn and transformer, in: International
    Conference on Medical Imaging with Deep Learning, PMLR, 2022, pp. 820–833.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] J. Peng, G. Estrada, M. Pedersoli, C. Desrosiers, Deep co-training for
    semi-supervised image segmentation, Pattern Recognition 107 (2020) 107269.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Z. Zhao, J. Hu, Z. Zeng, X. Yang, P. Qian, B. Veeravalli, C. Guan, Mmgl:
    Multi-scale multi-view global-local contrastive learning for semi-supervised cardiac
    image segmentation, in: 2022 IEEE international conference on image processing
    (ICIP), IEEE, 2022, pp. 401–405.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] H. Wang, X. Li, Dhc: Dual-debiased heterogeneous co-training framework
    for class-imbalanced semi-supervised medical image segmentation, arXiv preprint
    arXiv:2307.11960 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] X. Wang, H. Chen, H. Xiang, H. Lin, X. Lin, P.-A. Heng, Deep virtual
    adversarial self-training with consistency regularization for semi-supervised
    medical image classification, Medical image analysis 70 (2021) 102010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] D. Wang, Y. Zhang, K. Zhang, L. Wang, Focalmix: Semi-supervised learning
    for 3d medical image detection, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, 2020, pp. 3951–3960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] W. Zhang, L. Zhu, J. Hallinan, S. Zhang, A. Makmur, Q. Cai, B. C. Ooi,
    Boostmis: Boosting medical image semi-supervised learning with adaptive pseudo
    labeling and informative active annotation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 20666–20676.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] K. Chaitanya, E. Erdil, N. Karani, E. Konukoglu, Local contrastive loss
    with pseudo-label based self-training for semi-supervised medical image segmentation,
    Medical Image Analysis 87 (2023) 102792.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] H. Basak, Z. Yin, Pseudo-label guided contrastive learning for semi-supervised
    medical image segmentation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2023, pp. 19786–19797.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] S. Zhang, J. Zhang, B. Tian, T. Lukasiewicz, Z. Xu, Multi-modal contrastive
    mutual learning and pseudo-label re-learning for semi-supervised medical image
    segmentation, Medical Image Analysis 83 (2023) 102656.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] L.-L. Zeng, K. Gao, D. Hu, Z. Feng, C. Hou, P. Rong, W. Wang, Ss-tbn:
    A semi-supervised tri-branch network for covid-19 screening and lesion segmentation,
    IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] T. Lei, D. Zhang, X. Du, X. Wang, Y. Wan, A. K. Nandi, Semi-supervised
    medical image segmentation using adversarial consistency learning and dynamic
    convolution network, IEEE Transactions on Medical Imaging (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] S. Budd, E. C. Robinson, B. Kainz, A survey on active learning and human-in-the-loop
    deep learning for medical image analysis, Medical Image Analysis 71 (2021) 102062.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] J. Peng, Y. Wang, Medical image segmentation with limited supervision:
    A review of deep network models, IEEE Access 9 (2021) 36827–36851. [doi:10.1109/ACCESS.2021.3062380](https://doi.org/10.1109/ACCESS.2021.3062380).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] J. Liu, L. Cao, Y. Tian, Deep active learning for effective pulmonary
    nodule detection, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings,
    Part VI 23, Springer, 2020, pp. 609–618.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] S. Wen, T. M. Kurc, L. Hou, J. H. Saltz, R. R. Gupta, R. Batiste, T. Zhao,
    V. Nguyen, D. Samaras, W. Zhu, Comparison of different classifiers with active
    learning to support quality control in nucleus segmentation in pathology images,
    AMIA Summits on Translational Science Proceedings 2018 (2018) 227.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] X. Wu, C. Chen, M. Zhong, J. Wang, J. Shi, Covid-al: The diagnosis of
    covid-19 with deep active learning, Medical Image Analysis 68 (2021) 101913.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] Z. Zhou, J. Y. Shin, S. R. Gurudu, M. B. Gotway, J. Liang, Active, continual
    fine tuning of convolutional neural networks for reducing annotation efforts,
    Medical image analysis 71 (2021) 101997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] S. Balaram, C. M. Nguyen, A. Kassim, P. Krishnaswamy, Consistency-based
    semi-supervised evidential active learning for diagnostic radiograph classification,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2022, pp. 675–685.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] W. Kuo, C. Häne, E. Yuh, P. Mukherjee, J. Malik, Cost-sensitive active
    learning for intracranial hemorrhage detection, in: Medical Image Computing and
    Computer Assisted Intervention–MICCAI 2018: 21st International Conference, Granada,
    Spain, September 16-20, 2018, Proceedings, Part III 11, Springer, 2018, pp. 715–723.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] W. H. Beluch, T. Genewein, A. Nürnberger, J. M. Köhler, The power of
    ensembles for active learning in image classification, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, 2018, pp. 9368–9377.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] A. Atzeni, L. Peter, E. Robinson, E. Blackburn, J. Althonayan, D. C.
    Alexander, J. E. Iglesias, Deep active learning for suggestive segmentation of
    biomedical image stacks via optimisation of dice scores and traced boundary length,
    Medical Image Analysis 81 (2022) 102549.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Y. Gal, R. Islam, Z. Ghahramani, Deep bayesian active learning with image
    data, in: International conference on machine learning, PMLR, 2017, pp. 1183–1192.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] D. Mahapatra, B. Bozorgtabar, J.-P. Thiran, M. Reyes, Efficient active
    learning for image classification and segmentation using a sample selection and
    conditional generative adversarial network, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer, 2018, pp. 580–588.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] C. Dai, S. Wang, Y. Mo, K. Zhou, E. Angelini, Y. Guo, W. Bai, Suggestive
    annotation of brain tumour images with gradient-guided sampling, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part IV 23, Springer, 2020, pp. 156–165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] V. Nath, D. Yang, H. R. Roth, D. Xu, Warm start active learning with
    proxy labels and selection via semi-supervised fine-tuning, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer,
    2022, pp. 297–308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] X. Li, M. Xia, J. Jiao, S. Zhou, C. Chang, Y. Wang, Y. Guo, Hal-ia: A
    hybrid active learning framework using interactive annotation for medical image
    segmentation, Medical Image Analysis (2023) 102862.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] L. Yang, Y. Zhang, J. Chen, S. Zhang, D. Z. Chen, Suggestive annotation:
    A deep active learning framework for biomedical image segmentation, in: Medical
    Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International
    Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part
    III 20, Springer, 2017, pp. 399–407.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] F. Ozdemir, Z. Peng, P. Fuernstahl, C. Tanner, O. Goksel, Active learning
    for segmentation based on bayesian sample queries, Knowledge-Based Systems 214
    (2021) 106531.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] S. Zhao, J. Song, S. Ermon, Infovae: Information maximizing variational
    autoencoders, arXiv preprint arXiv:1706.02262 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] W. Li, J. Li, Z. Wang, J. Polson, A. E. Sisk, D. P. Sajed, W. Speier,
    C. W. Arnold, Pathal: An active learning framework for histopathology image analysis,
    IEEE Transactions on Medical Imaging 41 (5) (2021) 1176–1187.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] S. Guo, W. Huang, H. Zhang, C. Zhuang, D. Dong, M. R. Scott, D. Huang,
    Curriculumnet: Weakly supervised learning from large-scale web images, in: Proceedings
    of the European conference on computer vision (ECCV), 2018, pp. 135–150.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Z. Wang, Z. Yin, Annotation-efficient cell counting, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference,
    Strasbourg, France, September 27–October 1, 2021, Proceedings, Part VIII 24, Springer,
    2021, pp. 405–414.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] S. Kumari, P. Singh, Deep learning for unsupervised domain adaptation
    in medical imaging: Recent advancements and future perspectives, arXiv preprint
    arXiv:2308.01265 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] M. Yu, H. Guan, Y. Fang, L. Yue, M. Liu, Domain-prior-induced structural
    mri adaptation for clinical progression prediction of subjective cognitive decline,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2022, pp. 24–33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] Y. Fang, M. Wang, G. G. Potter, M. Liu, Unsupervised cross-domain functional
    mri adaptation for automated major depressive disorder identification, Medical
    Image Analysis 84 (2023) 102707.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Q. Hu, H. Li, J. Zhang, Domain-adaptive 3d medical image synthesis: An
    efficient unsupervised approach, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2022: 25th International Conference, Singapore, September
    18–22, 2022, Proceedings, Part VI, Springer, 2022, pp. 495–504.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] M. Sahu, R. Strömsdörfer, A. Mukhopadhyay, S. Zachow, Endo-sim2real:
    Consistency learning-based domain adaptation for instrument segmentation, in:
    International Conference on Medical Image Computing and Computer-Assisted Intervention,
    Springer, 2020, pp. 784–794.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] A. Gomariz, H. Lu, Y. Y. Li, T. Albrecht, A. Maunz, F. Benmansour, A. M.
    Valcarcel, J. Luu, D. Ferrara, O. Goksel, Unsupervised domain adaptation with
    contrastive learning for oct segmentation, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer, 2022, pp. 351–361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] F. Wu, X. Zhuang, Cf distance: a new domain discrepancy metric and application
    to explicit domain adaptation for cross-modality cardiac image segmentation, IEEE
    Transactions on Medical Imaging 39 (12) (2020) 4274–4285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, V. Lempitsky, Domain-adversarial training of neural networks, The
    journal of machine learning research 17 (1) (2016) 2096–2030.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] J. Ren, I. Hacihaliloglu, E. A. Singer, D. J. Foran, X. Qi, Adversarial
    domain adaptation for classification of prostate histopathology whole-slide images,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st
    International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
    Part II 11, Springer, 2018, pp. 201–209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Y. Zhang, H. Chen, Y. Wei, P. Zhao, J. Cao, X. Fan, X. Lou, H. Liu, J. Hou,
    X. Han, et al., From whole slide imaging to microscopy: Deep microscopy adaptation
    network for histopathology cancer image classification, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2019,
    pp. 360–368.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Y. Feng, Z. Wang, X. Xu, Y. Wang, H. Fu, S. Li, L. Zhen, X. Lei, Y. Cui,
    J. S. Z. Ting, et al., Contrastive domain adaptation with consistency match for
    automated pneumonia diagnosis, Medical Image Analysis 83 (2023) 102664.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] C. Bian, C. Yuan, J. Wang, M. Li, X. Yang, S. Yu, K. Ma, J. Yuan, Y. Zheng,
    Uncertainty-aware domain alignment for anatomical structure segmentation, Medical
    Image Analysis 64 (2020) 101732.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] L. Liu, Z. Zhang, S. Li, K. Ma, Y. Zheng, S-cuda: Self-cleansing unsupervised
    domain adaptation for medical image segmentation, Medical Image Analysis 74 (2021)
    102214.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] W. Huang, X. Liu, Z. Cheng, Y. Zhang, Z. Xiong, Domain adaptive mitochondria
    segmentation via enforcing inter-section consistency, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 89–98.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] M. A. Karaoglu, N. Brasch, M. Stollenga, W. Wein, N. Navab, F. Tombari,
    A. Ladikos, Adversarial domain feature adaptation for bronchoscopic depth estimation,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part IV 24, Springer, 2021, pp. 300–310.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] H. Hao, C. Xu, D. Zhang, Q. Yan, J. Zhang, Y. Liu, Y. Zhao, Sparse-based
    domain adaptation network for octa image super-resolution reconstruction, IEEE
    Journal of Biomedical and Health Informatics 26 (9) (2022) 4402–4413.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] C. Yoo, H. W. Lee, J.-W. Kang, Transferring structured knowledge in unsupervised
    domain adaptation of a sleep staging network, IEEE journal of biomedical and health
    informatics 26 (3) (2021) 1273–1284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image translation
    using cycle-consistent adversarial networks, in: Proceedings of the IEEE international
    conference on computer vision, 2017, pp. 2223–2232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] Y. Tang, Y. Tang, V. Sandfort, J. Xiao, R. M. Summers, Tuna-net: Task-oriented
    unsupervised adversarial network for disease recognition in cross-domain chest
    x-rays, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings,
    Part VI 22, Springer, 2019, pp. 431–440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] J. Jiang, Y.-C. Hu, N. Tyagi, P. Zhang, A. Rimner, G. S. Mageras, J. O.
    Deasy, H. Veeraraghavan, Tumor-aware, adversarial domain adaptation from ct to
    mri for lung cancer segmentation, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2018: 21st International Conference, Granada, Spain, September
    16-20, 2018, Proceedings, Part II 11, Springer, 2018, pp. 777–785.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] J. Jiang, Y.-C. Hu, N. Tyagi, A. Rimner, N. Lee, J. O. Deasy, S. Berry,
    H. Veeraraghavan, Psigan: Joint probabilistic segmentation and image distribution
    matching for unpaired cross-modality adaptation-based mri segmentation, IEEE transactions
    on medical imaging 39 (12) (2020) 4071–4084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] D. Tomar, M. Lortkipanidze, G. Vray, B. Bozorgtabar, J.-P. Thiran, Self-attentive
    spatial adaptive normalization for cross-modality domain adaptation, IEEE Transactions
    on Medical Imaging 40 (10) (2021) 2926–2938.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] A. Kapil, A. Meier, K. Steele, M. Rebelatto, K. Nekolla, A. Haragan,
    A. Silva, A. Zuraw, C. Barker, M. L. Scott, et al., Domain adaptation-based deep
    learning for automated tumor cell (tc) scoring and survival analysis on pd-l1
    stained tissue images, IEEE Transactions on Medical Imaging 40 (9) (2021) 2513–2523.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] F. Xing, T. Bennett, D. Ghosh, Adversarial domain adaptation and pseudo-labeling
    for cross-modality microscopy image quantification, in: Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference,
    Shenzhen, China, October 13–17, 2019, Proceedings, Part I 22, Springer, 2019,
    pp. 740–749.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] F. Xing, T. C. Cornish, T. D. Bennett, D. Ghosh, Bidirectional mapping-based
    domain adaptation for nucleus detection in cross-modality microscopy images, IEEE
    Transactions on Medical Imaging 40 (10) (2020) 2880–2896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] F. Xing, T. C. Cornish, Low-resource adversarial domain adaptation for
    cross-modality nucleus detection, in: International Conference on Medical Image
    Computing and Computer-Assisted Intervention, Springer, 2022, pp. 639–649.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C. C. Loy, Domain generalization:
    A survey, IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] J. Yang, N. C. Dvornek, F. Zhang, J. Chapiro, M. Lin, J. S. Duncan, Unsupervised
    domain adaptation via disentangled representations: Application to cross-modality
    liver segmentation, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings,
    Part II 22, Springer, 2019, pp. 255–263.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] R. Wang, G. Zheng, Cycmis: Cycle-consistent cross-domain medical image
    segmentation via diverse image augmentation, Medical Image Analysis 76 (2022)
    102328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] X. Sun, Z. Liu, S. Zheng, C. Lin, Z. Zhu, Y. Zhao, Attention-enhanced
    disentangled representation learning for unsupervised domain adaptation in cardiac
    segmentation, in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2022, pp. 745–754.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] Q. Xie, Y. Li, N. He, M. Ning, K. Ma, G. Wang, Y. Lian, Y. Zheng, Unsupervised
    domain adaptation for medical image segmentation by disentanglement learning and
    self-training, IEEE Transactions on Medical Imaging (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] C. Yang, X. Guo, M. Zhu, B. Ibragimov, Y. Yuan, Mutual-prototype adaptation
    for cross-domain polyp segmentation, IEEE Journal of Biomedical and Health Informatics
    25 (10) (2021) 3886–3897.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] S. Wu, C. Chen, Z. Xiong, X. Chen, X. Sun, Uncertainty-aware label rectification
    for domain adaptive mitochondria segmentation, in: Medical Image Computing and
    Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg,
    France, September 27–October 1, 2021, Proceedings, Part III 24, Springer, 2021,
    pp. 191–200.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] F. Gröger, A.-M. Rickmann, C. Wachinger, Strudel: Self-training with
    uncertainty dependent label refinement across domains, in: Machine Learning in
    Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with
    MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings 12, Springer,
    2021, pp. 306–316.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] Q. Qi, X. Lin, C. Chen, W. Xie, Y. Huang, X. Ding, X. Liu, Y. Yu, Curriculum
    feature alignment domain adaptation for epithelium-stroma classification in histopathological
    images, IEEE Journal of Biomedical and Health Informatics 25 (4) (2020) 1163–1172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] H. Cho, K. Nishimura, K. Watanabe, R. Bise, Effective pseudo-labeling
    based on heatmap for unsupervised domain adaptation in cell detection, Medical
    Image Analysis 79 (2022) 102436.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] A. Mottaghi, A. Sharghi, S. Yeung, O. Mohareri, Adaptation of surgical
    activity recognition models across operating rooms, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 530–540.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] Y. Sun, E. Tzeng, T. Darrell, A. A. Efros, Unsupervised domain adaptation
    through self-supervision, arXiv preprint arXiv:1909.11825 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] N. A. Koohbanani, B. Unnikrishnan, S. A. Khurram, P. Krishnaswamy, N. Rajpoot,
    Self-path: Self-supervision for classification of pathology images with limited
    annotations, IEEE Transactions on Medical Imaging 40 (10) (2021) 2845–2856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] C. Abbet, L. Studer, A. Fischer, H. Dawson, I. Zlobec, B. Bozorgtabar,
    J.-P. Thiran, Self-rule to multi-adapt: Generalized multi-source feature learning
    using unsupervised domain adaptation for colorectal cancer tissue detection, Medical
    image analysis 79 (2022) 102473.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, A. Yuille, Domain
    adaptive relational reasoning for 3d multi-organ segmentation, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part I 23, Springer, 2020, pp. 656–666.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] Y. Xue, S. Feng, Y. Zhang, X. Zhang, Y. Wang, Dual-task self-supervision
    for cross-modality domain adaptation, in: Medical Image Computing and Computer
    Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru,
    October 4–8, 2020, Proceedings, Part I 23, Springer, 2020, pp. 408–417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] C. Chen, Q. Dou, H. Chen, J. Qin, P. A. Heng, Unsupervised bidirectional
    cross-modality adaptation via deeply synergistic image and feature alignment for
    medical image segmentation, IEEE transactions on medical imaging 39 (7) (2020)
    2494–2505.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    T. Darrell, Cycada: Cycle-consistent adversarial domain adaptation, in: International
    conference on machine learning, Pmlr, 2018, pp. 1989–1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] X. Jia, S. Wang, X. Liang, A. Balagopal, D. Nguyen, M. Yang, Z. Wang,
    J. X. Ji, X. Qian, S. Jiang, Cone-beam computed tomography (cbct) segmentation
    by adversarial learning domain adaptation, in: Medical Image Computing and Computer
    Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China,
    October 13–17, 2019, Proceedings, Part VI 22, Springer, 2019, pp. 567–575.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] D. Liu, D. Zhang, Y. Song, F. Zhang, L. O’Donnell, H. Huang, M. Chen,
    W. Cai, Pdam: A panoptic-level feature alignment framework for unsupervised domain
    adaptive instance segmentation in microscopy images, IEEE Transactions on Medical
    Imaging 40 (1) (2020) 154–165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] H. Cui, C. Yuwen, L. Jiang, Y. Xia, Y. Zhang, Bidirectional cross-modality
    unsupervised domain adaptation using generative adversarial networks for cardiac
    image segmentation, Computers in Biology and Medicine 136 (2021) 104726.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] X. Chen, T. Kuang, H. Deng, S. H. Fung, J. Gateno, J. J. Xia, P.-T. Yap,
    Dual adversarial attention mechanism for unsupervised domain adaptive medical
    image segmentation, IEEE Transactions on Medical Imaging 41 (11) (2022) 3445–3453.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] Z. Zhao, F. Zhou, K. Xu, Z. Zeng, C. Guan, S. K. Zhou, Le-uda: Label-efficient
    unsupervised domain adaptation for medical image segmentation, IEEE transactions
    on medical imaging 42 (3) (2023) 633–646.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] X. Li, S. Niu, X. Gao, X. Zhou, J. Dong, H. Zhao, Self-training adversarial
    learning for cross-domain retinal oct fluid segmentation, Computers in Biology
    and Medicine 155 (2023) 106650.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] D. Karimi, H. Dou, S. K. Warfield, A. Gholipour, Deep learning with noisy
    labels: Exploring techniques and remedies in medical image analysis, Medical image
    analysis 65 (2020) 101759.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] A. Ghosh, H. Kumar, P. S. Sastry, Robust loss functions under label noise
    for deep neural networks, in: Proceedings of the AAAI conference on artificial
    intelligence, Vol. 31, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] Z. Zhang, M. Sabuncu, Generalized cross entropy loss for training deep
    neural networks with noisy labels, Advances in neural information processing systems
    31 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] D. J. Matuszewski, I.-M. Sintorn, Minimal annotation training for segmentation
    of microscopy images, in: 2018 IEEE 15th International Symposium on Biomedical
    Imaging (ISBI 2018), IEEE, 2018, pp. 387–390.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] G. Wang, X. Liu, C. Li, Z. Xu, J. Ruan, H. Zhu, T. Meng, K. Li, N. Huang,
    S. Zhang, A noise-robust framework for automatic segmentation of covid-19 pneumonia
    lesions from ct images, IEEE Transactions on Medical Imaging 39 (8) (2020) 2653–2663.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] H. Chen, W. Tan, J. Li, P. Guan, L. Wu, B. Yan, J. Li, Y. Wang, Adaptive
    cross entropy for ultrasmall object detection in computed tomography with noisy
    labels, Computers in Biology and Medicine 147 (2022) 105763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] X. Guo, Y. Yuan, Joint class-affinity loss correction for robust medical
    image segmentation with noisy labels, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer, 2022, pp. 588–598.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] H. Le, D. Samaras, T. Kurc, R. Gupta, K. Shroyer, J. Saltz, Pancreatic
    cancer detection in whole slide images using noisy label annotations, in: Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International
    Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part I 22, Springer,
    2019, pp. 541–549.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] Z. Mirikharaji, Y. Yan, G. Hamarneh, Learning to segment skin lesions
    from noisy annotations, in: Domain Adaptation and Representation Transfer and
    Medical Image Learning with Less Labels and Imperfect Data: First MICCAI Workshop,
    DART 2019, and First International Workshop, MIL3ID 2019, Shenzhen, Held in Conjunction
    with MICCAI 2019, Shenzhen, China, October 13 and 17, 2019, Proceedings 1, Springer,
    2019, pp. 207–215.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] T. Zhang, L. Yu, N. Hu, S. Lv, S. Gu, Robust medical image segmentation
    from non-expert annotations with tri-network, in: Medical Image Computing and
    Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima,
    Peru, October 4–8, 2020, Proceedings, Part IV 23, Springer, 2020, pp. 249–258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] J. Wang, S. Zhou, C. Fang, L. Wang, J. Wang, Meta corrupted pixels mining
    for medical image segmentation, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8,
    2020, Proceedings, Part I 23, Springer, 2020, pp. 335–345.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] S. Min, X. Chen, Z.-J. Zha, F. Wu, Y. Zhang, A two-stream mutual attention
    network for semi-supervised biomedical segmentation with noisy labels, in: Proceedings
    of the AAAI Conference on Artificial Intelligence, Vol. 33, 2019, pp. 4578–4585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] E. Malach, S. Shalev-Shwartz, Decoupling” when to update” from” how to
    update”, Advances in neural information processing systems 30 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] C. Xue, L. Yu, P. Chen, Q. Dou, P.-A. Heng, Robust medical image classification
    from noisy labeled data with global and local representation guided co-training,
    IEEE Transactions on Medical Imaging 41 (6) (2022) 1371–1382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] S. Yang, G. Wang, H. Sun, X. Luo, P. Sun, K. Li, Q. Wang, S. Zhang, Learning
    covid-19 pneumonia lesion segmentation from imperfect annotations via divergence-aware
    selective training, IEEE Journal of Biomedical and Health Informatics 26 (8) (2022)
    3673–3684.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] X. Li, Y. Wei, Q. Hu, C. Wang, J. Yang, Learning to segment subcortical
    structures from noisy annotations with a novel uncertainty-reliability aware learning
    framework, Computers in Biology and Medicine 151 (2022) 106326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] Z. Xu, D. Lu, Y. Wang, J. Luo, J. Jayender, K. Ma, Y. Zheng, X. Li, Noisy
    labels are treasure: mean-teacher-assisted confident learning for hepatic vessel
    segmentation, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2021: 24th International Conference, Strasbourg, France, September 27–October
    1, 2021, Proceedings, Part I 24, Springer, 2021, pp. 3–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] Z. Xu, D. Lu, J. Luo, Y. Wang, J. Yan, K. Ma, Y. Zheng, R. K.-Y. Tong,
    Anti-interference from noisy labels: Mean-teacher-assisted confident learning
    for medical image segmentation, IEEE Transactions on Medical Imaging 41 (11) (2022)
    3062–3073.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] J. Shi, J. Wu, Distilling effective supervision for robust medical image
    segmentation with noisy labels, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part I 24, Springer, 2021, pp. 668–677.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] F. Garcea, A. Serra, F. Lamberti, L. Morra, Data augmentation for medical
    imaging: A systematic literature review, Computers in Biology and Medicine (2022)
    106391.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] J. Nalepa, M. Marcinkiewicz, M. Kawulok, Data augmentation for brain-tumor
    segmentation: a review, Frontiers in computational neuroscience 13 (2019) 83.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] S. Pereira, A. Pinto, V. Alves, C. A. Silva, Brain tumor segmentation
    using convolutional neural networks in mri images, IEEE transactions on medical
    imaging 35 (5) (2016) 1240–1251.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] Y. Liu, S. Stojadinovic, B. Hrycushko, Z. Wardak, S. Lau, W. Lu, Y. Yan,
    S. B. Jiang, X. Zhen, R. Timmerman, et al., A deep convolutional neural network-based
    automatic delineation strategy for multiple brain metastases stereotactic radiosurgery,
    PloS one 12 (10) (2017) e0185844.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] A. Galdran, A. Alvarez-Gila, M. I. Meyer, C. L. Saratxaga, T. Araújo,
    E. Garrote, G. Aresta, P. Costa, A. M. Mendoncca, A. Campilho, Data-driven color
    augmentation techniques for deep skin image analysis, arXiv preprint arXiv:1703.03702
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M. Druzhinin,
    A. A. Kalinin, Albumentations: fast and flexible image augmentations, Information
    11 (2) (2020) 125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] F. Pérez-García, R. Sparks, S. Ourselin, Torchio: a python library for
    efficient loading, preprocessing, augmentation and patch-based sampling of medical
    images in deep learning, Computer Methods and Programs in Biomedicine 208 (2021)
    106236.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, Y. Bengio, Generative adversarial nets, Advances in neural information
    processing systems 27 (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] M. Mirza, S. Osindero, Conditional generative adversarial nets, arXiv
    preprint arXiv:1411.1784 (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] P. Isola, J.-Y. Zhu, T. Zhou, A. A. Efros, Image-to-image translation
    with conditional adversarial networks, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, 2017, pp. 1125–1134.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] T. Park, M.-Y. Liu, T.-C. Wang, J.-Y. Zhu, Semantic image synthesis with
    spatially-adaptive normalization, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, 2019, pp. 2337–2346.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] A. Radford, L. Metz, S. Chintala, Unsupervised representation learning
    with deep convolutional generative adversarial networks, arXiv preprint arXiv:1511.06434
    (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, T. Aila, Analyzing
    and improving the image quality of stylegan, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2020, pp. 8110–8119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] T. Karras, T. Aila, S. Laine, J. Lehtinen, Progressive growing of gans
    for improved quality, stability, and variation, in: International Conference on
    Learning Representations, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] A. Bissoto, E. Valle, S. Avila, Gan-based data augmentation and anonymization
    for skin-lesion analysis: A critical review, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2021, pp. 1847–1856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger, H. Greenspan, Synthetic
    data augmentation using gan for improved liver lesion classification, in: 2018
    IEEE 15th international symposium on biomedical imaging (ISBI 2018), IEEE, 2018,
    pp. 289–293.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] M. Frid-Adar, I. Diamant, E. Klang, M. Amitai, J. Goldberger, H. Greenspan,
    Gan-based synthetic medical image augmentation for increased cnn performance in
    liver lesion classification, Neurocomputing 321 (2018) 321–331.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] H. Rashid, M. A. Tanveer, H. A. Khan, Skin lesion classification using
    gan based data augmentation, in: 2019 41St annual international conference of
    the IEEE engineering in medicine and biology society (EMBC), IEEE, 2019, pp. 916–919.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] A. Waheed, M. Goyal, D. Gupta, A. Khanna, F. Al-Turjman, P. R. Pinheiro,
    Covidgan: data augmentation using auxiliary classifier gan for improved covid-19
    detection, Ieee Access 8 (2020) 91916–91923.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] H. Zhao, H. Li, S. Maurer-Stroh, L. Cheng, Synthesizing retinal and neuronal
    images with generative adversarial nets, Medical image analysis 49 (2018) 14–26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] M. Nishio, C. Muramatsu, S. Noguchi, H. Nakai, K. Fujimoto, R. Sakamoto,
    H. Fujita, Attribute-guided image generation of three-dimensional computed tomography
    images of lung nodules using a generative adversarial network, Computers in Biology
    and Medicine 126 (2020) 104032.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] K. Chen, Y. Guo, C. Yang, Y. Xu, R. Zhang, C. Li, R. Wu, Enhanced breast
    lesion classification via knowledge guided cross-modal and semantic data augmentation,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part V 24, Springer, 2021, pp. 53–63.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] C. Muramatsu, M. Nishio, T. Goto, M. Oiwa, T. Morita, M. Yakami, T. Kubo,
    K. Togashi, H. Fujita, Improving breast mass classification by shared data with
    domain transformation using a generative adversarial network, Computers in biology
    and medicine 119 (2020) 103698.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] P. Guo, P. Wang, J. Zhou, V. M. Patel, S. Jiang, Lesion mask-based simultaneous
    synthesis of anatomic and molecular mr images using a gan, in: Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part II 23, Springer, 2020, pp. 104–113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] M. Rezaei, K. Harmuth, W. Gierke, T. Kellermeier, M. Fischer, H. Yang,
    C. Meinel, A conditional adversarial network for semantic segmentation of brain
    tumor, in: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain
    Injuries: Third International Workshop, BrainLes 2017, Held in Conjunction with
    MICCAI 2017, Quebec City, QC, Canada, September 14, 2017, Revised Selected Papers
    3, Springer, 2018, pp. 241–252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] Y. Chen, X.-H. Yang, Z. Wei, A. A. Heidari, N. Zheng, Z. Li, H. Chen,
    H. Hu, Q. Zhou, Q. Guan, Generative adversarial networks in medical image augmentation:
    A review, Computers in Biology and Medicine 144 (2022) 105382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, mixup: Beyond empirical
    risk minimization, in: International Conference on Learning Representations, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, Y. Yoo, Cutmix: Regularization
    strategy to train strong classifiers with localizable features, in: Proceedings
    of the IEEE/CVF international conference on computer vision, 2019, pp. 6023–6032.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] J. Yang, Y. Zhang, Y. Liang, Y. Zhang, L. He, Z. He, Tumorcp: A simple
    but effective object-level data augmentation for tumor segmentation, in: Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International
    Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part
    I 24, Springer, 2021, pp. 579–588.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] Y. Lin, Z. Wang, K.-T. Cheng, H. Chen, Insmix: Towards realistic generative
    data augmentation for nuclei instance segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 140–149.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] Q. Zhu, Y. Wang, L. Yin, J. Yang, F. Liao, S. Li, Selfmix: a self-adaptive
    data augmentation method for lesion segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 683–692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] X. Zhang, C. Liu, N. Ou, X. Zeng, Z. Zhuo, Y. Duan, X. Xiong, Y. Yu,
    Z. Liu, Y. Liu, et al., Carvemix: a simple data augmentation method for brain
    lesion segmentation, NeuroImage 271 (2023) 120041.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] Y. Wang, Y. Ji, H. Xiao, A data augmentation method for fully automatic
    brain tumor segmentation, Computers in Biology and Medicine 149 (2022) 106039.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] M. Salem, S. Valverde, M. Cabezas, D. Pareto, A. Oliver, J. Salvi, À. Rovira,
    X. Lladó, Multiple sclerosis lesion synthesis in mri using an encoder-decoder
    u-net, IEEE Access 7 (2019) 25171–25184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] K. H. Cha, N. Petrick, A. Pezeshk, C. G. Graff, D. Sharma, A. Badal,
    B. Sahiner, Evaluation of data augmentation via synthetic images for improved
    breast mass detection on mammograms using deep learning, Journal of Medical Imaging
    7 (1) (2020) 012703–012703.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] Y. Sun, P. Yuan, Y. Sun, Mm-gan: 3d mri data augmentation for medical
    image segmentation via generative adversarial networks, in: 2020 IEEE International
    conference on knowledge graph (ICKG), IEEE, 2020, pp. 227–234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] Y. Onishi, A. Teramoto, M. Tsujimoto, T. Tsukamoto, K. Saito, H. Toyama,
    K. Imaizumi, H. Fujita, Multiplanar analysis for pulmonary nodule classification
    in ct images using deep convolutional neural network and generative adversarial
    networks, International journal of computer assisted radiology and surgery 15
    (2020) 173–178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] Y. Chen, D. Ruan, J. Xiao, L. Wang, B. Sun, R. Saouaf, W. Yang, D. Li,
    Z. Fan, Fully automated multiorgan segmentation in abdominal magnetic resonance
    imaging with deep neural networks, Medical physics 47 (10) (2020) 4971–4982.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, Q. V. Le, Autoaugment: Learning
    augmentation policies from data, arXiv preprint arXiv:1805.09501 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] C. Chen, C. Qin, C. Ouyang, Z. Li, S. Wang, H. Qiu, L. Chen, G. Tarroni,
    W. Bai, D. Rueckert, Enhancing mr image segmentation with realistic adversarial
    data augmentation, Medical Image Analysis 82 (2022) 102597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] K. Fujita, M. Kobayashi, T. Nagao, Data augmentation using evolutionary
    image processing, in: 2018 Digital Image Computing: Techniques and Applications
    (DICTA), IEEE, 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] Y. Wang, Q. Yao, J. T. Kwok, L. M. Ni, Generalizing from a few examples:
    A survey on few-shot learning, ACM computing surveys (csur) 53 (3) (2020) 1–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] A. K. Mondal, J. Dolz, C. Desrosiers, Few-shot 3d multi-modal medical
    image segmentation using generative adversarial learning, arXiv preprint arXiv:1810.12241
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, A. V. Dalca, Data
    augmentation using learned transformations for one-shot medical image segmentation,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2019, pp. 8543–8553.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] Q. Lu, W. Liu, Z. Zhuo, Y. Li, Y. Duan, P. Yu, L. Qu, C. Ye, Y. Liu,
    A transfer learning approach to few-shot segmentation of novel white matter tracts,
    Medical Image Analysis 79 (2022) 102454.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] W. Liu, Q. Lu, Z. Zhuo, Y. Liu, C. Ye, One-shot segmentation of novel
    white matter tracts via extensive data augmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 133–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[346] M. Fischer, T. Hepp, S. Gatidis, B. Yang, Self-supervised contrastive
    learning with random walks for medical image segmentation with limited annotations,
    Computerized Medical Imaging and Graphics 104 (2023) 102174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learning,
    Advances in neural information processing systems 30 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] S. Ali, B. Bhattarai, T.-K. Kim, J. Rittscher, Additive angular margin
    for few shot learning to classify clinical endoscopy images, in: Machine Learning
    in Medical Imaging: 11th International Workshop, MLMI 2020, Held in Conjunction
    with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings 11, Springer, 2020,
    pp. 494–503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] R. Wang, Q. Zhou, G. Zheng, Few-shot medical image segmentation regularized
    with self-reference and contrastive learning, in: International Conference on
    Medical Image Computing and Computer-Assisted Intervention, Springer, 2022, pp.
    514–523.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] C. Ouyang, C. Biffi, C. Chen, T. Kart, H. Qiu, D. Rueckert, Self-supervision
    with superpixels: Training few-shot medical image segmentation without annotation,
    in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XXIX 16, Springer, 2020, pp. 762–780.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] C. Ouyang, C. Biffi, C. Chen, T. Kart, H. Qiu, D. Rueckert, Self-supervised
    learning for few-shot medical image segmentation, IEEE Transactions on Medical
    Imaging 41 (7) (2022) 1837–1848.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] S. Hansen, S. Gautam, R. Jenssen, M. Kampffmeyer, Anomaly detection-inspired
    few-shot medical image segmentation through self-supervision with supervoxels,
    Medical Image Analysis 78 (2022) 102385.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] S. Hansen, S. Gautam, S. A. Salahuddin, M. Kampffmeyer, R. Jenssen, Adnet++:
    A few-shot learning framework for multi-class medical image volume segmentation
    with uncertainty-guided feature refinement, Medical Image Analysis (2023) 102870.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] Q. Yu, K. Dang, N. Tajbakhsh, D. Terzopoulos, X. Ding, A location-sensitive
    local prototype network for few-shot medical image segmentation, in: 2021 IEEE
    18th international symposium on biomedical imaging (ISBI), IEEE, 2021, pp. 262–266.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] H. Tang, X. Liu, S. Sun, X. Yan, X. Xie, Recurrent mask refinement for
    few-shot medical image segmentation, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2021, pp. 3918–3928.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] C. Finn, P. Abbeel, S. Levine, Model-agnostic meta-learning for fast
    adaptation of deep networks, in: International conference on machine learning,
    PMLR, 2017, pp. 1126–1135.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] R. Singh, V. Bharti, V. Purohit, A. Kumar, A. K. Singh, S. K. Singh,
    Metamed: Few-shot medical image classification using gradient-based meta-learning,
    Pattern Recognition 120 (2021) 108111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] S. Chao, D. Belanger, Generalizing few-shot classification of whole-genome
    doubling across cancer types, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2021, pp. 3382–3392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] A. Achmamad, F. Ghazouani, S. Ruan, Few-shot learning for brain tumor
    segmentation from mri images, in: 2022 16th IEEE International Conference on Signal
    Processing (ICSP), Vol. 1, IEEE, 2022, pp. 489–494.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] R. Khadka, D. Jha, S. Hicks, V. Thambawita, M. A. Riegler, S. Ali, P. Halvorsen,
    Meta-learning with implicit gradients in a few-shot setting for medical image
    segmentation, Computers in Biology and Medicine 143 (2022) 105227.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] A. Rajeswaran, C. Finn, S. M. Kakade, S. Levine, Meta-learning with implicit
    gradients, Advances in neural information processing systems 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[362] Z. Zhao, F. Zhou, Z. Zeng, C. Guan, S. K. Zhou, Meta-hallucinator: Towards
    few-shot cross-modality cardiac image segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 128–139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[363] X. Song, J. Li, X. Qian, Diagnosis of glioblastoma multiforme progression
    via interpretable structure-constrained graph neural networks, IEEE Transactions
    on Medical Imaging 42 (2) (2022) 380–390.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[364] M. Gao, H. Jiang, L. Zhu, Z. Jiang, M. Geng, Q. Ren, Y. Lu, Discriminative
    ensemble meta-learning with co-regularization for rare fundus diseases diagnosis,
    Medical Image Analysis 89 (2023) 102884.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] A. G. Roy, S. Siddiqui, S. Pölsterl, N. Navab, C. Wachinger, ‘squeeze
    & excite’guided few-shot segmentation of volumetric images, Medical image analysis
    59 (2020) 101587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] R. Feng, X. Zheng, T. Gao, J. Chen, W. Wang, D. Z. Chen, J. Wu, Interactive
    few-shot learning: Limited supervision, better medical image segmentation, IEEE
    Transactions on Medical Imaging 40 (10) (2021) 2575–2588.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] S. Kim, S. An, P. Chikontwe, S. H. Park, Bidirectional rnn-based few
    shot learning for 3d medical image segmentation, in: Proceedings of the AAAI conference
    on artificial intelligence, Vol. 35, 2021, pp. 1808–1816.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] Y. Feng, Y. Wang, H. Li, M. Qu, J. Yang, Learning what and where to segment:
    A new perspective on medical image few-shot segmentation, Medical Image Analysis
    87 (2023) 102834.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[369] W. Zhu, H. Liao, W. Li, W. Li, J. Luo, Alleviating the incompatibility
    between cross entropy loss and episode training for few-shot skin disease classification,
    in: Medical Image Computing and Computer Assisted Intervention – MICCAI 2020,
    Springer International Publishing, Cham, 2020, pp. 330–339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[370] N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M. B.
    Gotway, J. Liang, Convolutional neural networks for medical image analysis: Full
    training or fine tuning?, IEEE transactions on medical imaging 35 (5) (2016) 1299–1312.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[371] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[372] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale
    image recognition, arXiv preprint arXiv:1409.1556 (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[373] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A
    large-scale hierarchical image database, in: 2009 IEEE conference on computer
    vision and pattern recognition, Ieee, 2009, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[374] H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura,
    R. M. Summers, Deep convolutional neural networks for computer-aided detection:
    Cnn architectures, dataset characteristics and transfer learning, IEEE transactions
    on medical imaging 35 (5) (2016) 1285–1298.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[375] B. Q. Huynh, H. Li, M. L. Giger, Digital mammographic tumor classification
    using transfer learning from deep convolutional neural networks, Journal of Medical
    Imaging 3 (3) (2016) 034501–034501.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[376] Y. Yuan, W. Qin, M. Buyyounouski, B. Ibragimov, S. Hancock, B. Han, L. Xing,
    Prostate cancer classification with multiparametric mri transfer learning model,
    Medical physics 46 (2) (2019) 756–765.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[377] S. Minaee, R. Kafieh, M. Sonka, S. Yazdani, G. J. Soufi, Deep-covid:
    Predicting covid-19 from chest x-ray images using deep transfer learning, Medical
    image analysis 65 (2020) 101794.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[378] P. Kora, C. P. Ooi, O. Faust, U. Raghavendra, A. Gudigar, W. Y. Chan,
    K. Meenakshi, K. Swaraja, P. Plawiak, U. R. Acharya, Transfer learning techniques
    for medical image analysis: A review, Biocybernetics and Biomedical Engineering
    42 (1) (2022) 79–107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[379] C. Ma, Z. Ji, M. Gao, Neural style transfer improves 3d cardiovascular
    mr image segmentation on inconsistent data, in: Medical Image Computing and Computer
    Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China,
    October 13–17, 2019, Proceedings, Part II 22, Springer, 2019, pp. 128–136.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[380] X. Qin, Transfer learning with edge attention for prostate mri segmentation,
    arXiv preprint arXiv:1912.09847 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[381] J. Liu, B. Dong, S. Wang, H. Cui, D.-P. Fan, J. Ma, G. Chen, Covid-19
    lung infection segmentation with a novel two-stage cross-domain transfer learning
    framework, Medical image analysis 74 (2021) 102205.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[382] D. M. Nguyen, T. T. Nguyen, H. Vu, Q. Pham, M.-D. Nguyen, B. T. Nguyen,
    D. Sonntag, Tatl: Task agnostic transfer learning for skin attributes detection,
    Medical Image Analysis 78 (2022) 102359.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[383] Q. Yu, L. Xie, Y. Wang, Y. Zhou, E. K. Fishman, A. L. Yuille, Recurrent
    saliency transformation network: Incorporating multi-stage visual cues for small
    organ segmentation, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2018, pp. 8280–8289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[384] S. Liu, D. Xu, S. K. Zhou, O. Pauly, S. Grbic, T. Mertelmeier, J. Wicklein,
    A. Jerebko, W. Cai, D. Comaniciu, 3d anisotropic hybrid network: Transferring
    convolutional features from 2d images to 3d anisotropic volumes, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2018: 21st International Conference,
    Granada, Spain, September 16-20, 2018, Proceedings, Part II 11, Springer, 2018,
    pp. 851–858.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[385] H. Messaoudi, A. Belaid, D. B. Salem, P.-H. Conze, Cross-dimensional
    transfer learning in medical image segmentation with deep learning, Medical Image
    Analysis (2023) 102868.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[386] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, A. V. Dalca, An unsupervised
    learning model for deformable medical image registration, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, 2018, pp. 9252–9260.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[387] C. V. Nguyen, Y. Li, T. D. Bui, R. E. Turner, Variational continual learning,
    in: International Conference on Learning Representations, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[388] E. Zheng, Q. Yu, R. Li, P. Shi, A. Haake, A continual learning framework
    for uncertainty-aware interactive image segmentation, in: Proceedings of the AAAI
    Conference on Artificial Intelligence, Vol. 35, 2021, pp. 6030–6038.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[389] J. Zhang, R. Gu, G. Wang, L. Gu, Comprehensive importance-based selective
    regularization for continual segmentation across multiple sites, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference,
    Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24, Springer,
    2021, pp. 389–399.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[390] Z. Li, C. Zhong, R. Wang, W.-S. Zheng, Continual learning of new diseases
    with dual distillation and ensemble strategy, in: Medical Image Computing and
    Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima,
    Peru, October 4–8, 2020, Proceedings, Part I 23, Springer, 2020, pp. 169–178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[391] M. M. Derakhshani, I. Najdenkoska, T. van Sonsbeek, X. Zhen, D. Mahapatra,
    M. Worring, C. G. Snoek, Lifelonger: A benchmark for continual disease classification,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2022, pp. 314–324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[392] Y. Yang, Z. Cui, J. Xu, C. Zhong, W.-S. Zheng, R. Wang, Continual learning
    with bayesian model based on a fixed pre-trained feature extractor, Visual Intelligence
    1 (1) (2023) 5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[393] N. Bayasi, G. Hamarneh, R. Garbi, Culprit-prune-net: Efficient continual
    sequential multi-domain learning with application to skin lesion classification,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2021, pp. 165–175.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[394] M. Lenga, H. Schulz, A. Saalbach, Continual learning for domain adaptation
    in chest x-ray classification, in: Medical Imaging with Deep Learning, PMLR, 2020,
    pp. 413–423.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[395] B. Chen, K. Thandiackal, P. Pati, O. Goksel, Generative appearance replay
    for continual unsupervised domain adaptation, arXiv preprint arXiv:2301.01211
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[396] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, C. P. Langlotz, Contrastive
    learning of medical visual representations from paired images and text, in: Machine
    Learning for Healthcare Conference, PMLR, 2022, pp. 2–25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[397] X. Xie, J. Niu, X. Liu, Z. Chen, S. Tang, S. Yu, A survey on incorporating
    domain knowledge into deep learning for medical image analysis, Medical Image
    Analysis 69 (2021) 101985.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[398] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information
    processing systems 30 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[399] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16
    words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[400] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, C. Feichtenhofer,
    Multiscale vision transformers, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2021, pp. 6824–6835.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[401] J. Li, J. Chen, Y. Tang, C. Wang, B. A. Landman, S. K. Zhou, Transforming
    medical imaging with transformers? a comparative review of key properties, current
    progresses, and future perspectives, Medical image analysis (2023) 102762.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[402] Y. Xie, J. Zhang, Y. Xia, Q. Wu, Unified 2d and 3d pre-training for medical
    image classification and segmentation, arXiv preprint arXiv:2112.09356 1 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[403] B. Zoph, Q. Le, Neural architecture search with reinforcement learning,
    in: International Conference on Learning Representations, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[404] T. Elsken, J. H. Metzen, F. Hutter, Neural architecture search: A survey,
    The Journal of Machine Learning Research 20 (1) (2019) 1997–2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[405] A. Yuille, C. Liu, Deep nets: What have they ever done for vision?.,
    Int J Comput Vis 129 (2021) 781–802.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[406] D. B. Larson, D. C. Magnus, M. P. Lungren, N. H. Shah, C. P. Langlotz,
    Ethics of using and sharing clinical imaging data for artificial intelligence:
    a proposed framework, Radiology 295 (3) (2020) 675–682.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[407] N. Rieke, J. Hancox, W. Li, F. Milletari, H. R. Roth, S. Albarqouni,
    S. Bakas, M. N. Galtier, B. A. Landman, K. Maier-Hein, et al., The future of digital
    health with federated learning, NPJ digital medicine 3 (1) (2020) 119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[408] R. S. Antunes, C. André da Costa, A. Küderle, I. A. Yari, B. Eskofier,
    Federated learning for healthcare: Systematic review and architecture proposal,
    ACM Transactions on Intelligent Systems and Technology (TIST) 13 (4) (2022) 1–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[409] M. J. Sheller, B. Edwards, G. A. Reina, J. Martin, S. Pati, A. Kotrotsou,
    M. Milchenko, W. Xu, D. Marcus, R. R. Colen, et al., Federated learning in medicine:
    facilitating multi-institutional collaborations without sharing patient data,
    Scientific reports 10 (1) (2020) 12598.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[410] Z. Li, Y. Li, Q. Li, P. Wang, D. Guo, L. Lu, D. Jin, Y. Zhang, Q. Hong,
    Lvit: language meets vision transformer in medical image segmentation, IEEE Transactions
    on Medical Imaging (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
