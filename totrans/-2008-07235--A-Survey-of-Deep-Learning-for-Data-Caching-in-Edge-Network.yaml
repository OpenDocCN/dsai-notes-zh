- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:59:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[2008.07235] A Survey of Deep Learning for Data Caching in Edge Network'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2008.07235](https://ar5iv.labs.arxiv.org/html/2008.07235)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Learning for Data Caching in Edge Network
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yantong Wang ¹ and Vasilis Friderikos ²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Center for Telecommunications Research
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Department of Engineering, King’s College London
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: London, WC2R 2LS, U.K.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: ¹ yantong.wang@kcl.ac.uk
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: ² vasilis.friderikos@kcl.ac.uk
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The concept of edge caching provision in emerging 5G and beyond mobile networks
    is a promising method to deal both with the traffic congestion problem in the
    core network as well as reducing latency to access popular content. In that respect
    end user demand for popular content can be satisfied by proactively caching it
    at the network edge, i.e, at close proximity to the users. In addition to model
    based caching schemes learning-based edge caching optimizations has recently attracted
    significant attention and the aim hereafter is to capture these recent advances
    for both model based and data driven techniques in the area of proactive caching.
    This paper summarizes the utilization of deep learning for data caching in edge
    network. We first outline the typical research topics in content caching and formulate
    a taxonomy based on network hierarchical structure. Then, a number of key types
    of deep learning algorithms are presented, ranging from supervised learning to
    unsupervised learning as well as reinforcement learning. Furthermore, a comparison
    of state-of-the-art literature is provided from the aspects of caching topics
    and deep learning methods. Finally, we discuss research challenges and future
    directions of applying deep learning for caching.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '*K*eywords deep learning  $\cdot$ content caching  $\cdot$ network optimization
     $\cdot$ edge network'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Undoubtedly, future 5G and beyond mobile communication networks will have to
    address stringent requirements of delivering popular content at ultra high speeds
    and low latency due to the proliferation of advanced mobile devices and data rich
    applications. In that ecosystem, edge-caching has received significant research
    attention over the last decade as an efficient technique to reduce delivery latency
    and network congestion especially during peak-traffic times or during unexpected
    network congestion episodes by bringing popular data closer to the end users.
    One of the main reasons of enabling edge caching in the network is to reduce the
    number of requests that traverse the access and core mobile network as well as
    reducing the load at the origin servers that would have to, otherwise, respond
    to all requests directly in absence of edge caching. In that case popular content
    and objects can be stored and served from edge locations, which are closer to
    the end users. This operation is also beneficial from the end user perspective
    since edge caching can dramatically reduce the overall latency to access the content
    and increase in the sense overall user experience. It is also important to note
    that the notion of popular content means that the requests of top 10% of video
    content on the Internet account for almost 80% of all traffic; which relates to
    multiple requests from different end users of the same content [[1](#bib.bib1)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，未来的5G及更高级的移动通信网络必须满足严格的要求，以超高速和低延迟传输热门内容，这是由于先进移动设备和数据丰富的应用程序的普及。在这种生态系统中，边缘缓存作为一种有效的技术，得到了过去十年的显著研究关注，它可以通过将热门数据更接近终端用户，从而减少交付延迟和网络拥堵，特别是在高峰流量时段或意外网络拥堵时。启用边缘缓存的主要原因之一是减少穿越接入和核心移动网络的请求数量，同时减少原始服务器的负载，否则这些服务器必须直接响应所有请求，如果没有边缘缓存的话。在这种情况下，热门内容和对象可以存储并从接近终端用户的边缘位置提供。这种操作对于终端用户来说也是有益的，因为边缘缓存可以显著减少访问内容的整体延迟，并提高整体用户体验感。还需要注意的是，热门内容的概念意味着，互联网上前10%的视频内容请求占据了几乎80%的所有流量；这涉及到来自不同终端用户对相同内容的多个请求[[1](#bib.bib1)]。
- en: 'Recently, deep learning (DL) has attracted significant attention from both
    academia and industry and has been applied to diverse domains like self-driving,
    medical diagnosis, playing complex games such as Go [[2](#bib.bib2)]. DL has also
    made their way into communication areas [[3](#bib.bib3)]. In this paper, we pay
    attention to the application of DL in caching policy. Though there are some earlier
    surveys related to machine learning applications, they either focus on general
    machine learning techniques for caching [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)],
    or concentrate on overall wireless applications [[7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]. The work [[3](#bib.bib3)] provides a big picture of applying
    machine learning in wireless communications. In [[4](#bib.bib4)], the authors
    consider the machine learning on both caching and routing strategy. A comprehensive
    survey on machine learning applications for caching content in edge networks is
    provided in [[5](#bib.bib5)]. The researchers [[6](#bib.bib6)] provide a survey
    about machine learning on mobile edge caching and communication resources. On
    the other hand, [[7](#bib.bib7)] overviews how artificial neural networks can
    be employed for various wireless network problems. The authors in [[8](#bib.bib8)]
    detail a survey on deep reinforcement learning (DRL) for issues in communications
    and networking. [[9](#bib.bib9)] presents a comprehensive on deep learning applications
    and edge computing paradigm. Our work can be distinguished from the aforementioned
    papers based on the fact that we focus on the deep learning techniques on content
    caching and both wired and wireless caching are taken into account. Our main contributions
    are listed as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习（DL）引起了学术界和工业界的广泛关注，并已应用于自动驾驶、医学诊断、围棋等复杂游戏 [[2](#bib.bib2)]。深度学习还进入了通信领域
    [[3](#bib.bib3)]。在本文中，我们关注深度学习在缓存策略中的应用。尽管有一些早期的关于机器学习应用的调查，但它们要么关注于用于缓存的一般机器学习技术
    [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]，要么集中于整体无线应用 [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)]。工作 [[3](#bib.bib3)] 提供了机器学习在无线通信中应用的整体视图。在 [[4](#bib.bib4)]
    中，作者考虑了机器学习在缓存和路由策略中的应用。 [[5](#bib.bib5)] 提供了关于机器学习在边缘网络中缓存内容的应用的全面调查。研究人员 [[6](#bib.bib6)]
    提供了关于移动边缘缓存和通信资源的机器学习调查。另一方面， [[7](#bib.bib7)] 概述了人工神经网络如何用于各种无线网络问题。 [[8](#bib.bib8)]
    详细介绍了深度强化学习（DRL）在通信和网络问题中的应用。 [[9](#bib.bib9)] 提出了深度学习应用和边缘计算范式的全面概述。我们的工作与上述论文不同，因为我们专注于深度学习技术在内容缓存中的应用，并考虑了有线和无线缓存。我们的主要贡献如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We classify the content caching problem into Layer 1 Caching and Layer 2 Caching.
    Each layer caching consists of four tightly coupled subproblems: where to cache,
    what to cache, cache dimensioning and content delivery. Related researches are
    provided accordingly.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将内容缓存问题分为第1层缓存和第2层缓存。每一层缓存包含四个紧密相关的子问题：缓存位置、缓存内容、缓存维度和内容传递。相关的研究也随之提供。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present the fundamentals of DL techniques which are widely used in content
    caching, such as convolutional neural network, recurrent neural network, actor-critic
    model based deep reinforcement learning, etc.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了在内容缓存中广泛使用的深度学习（DL）技术的基础知识，如卷积神经网络、递归神经网络、基于演员-评论家模型的深度强化学习等。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We analyze a broad range of state-of-the-art literature which use DL to content
    caching. These papers are compared based on the DL structure, layer caching coupled
    subproblems and the objective of DL in each scenarios. Then we discuss research
    challenges and potential directions for the utilization of DL in caching.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了广泛的前沿文献，这些文献使用深度学习进行内容缓存。这些论文基于深度学习结构、层缓存相关子问题和每种场景中深度学习的目标进行比较。然后我们讨论了深度学习在缓存中应用的研究挑战和潜在方向。
- en: '![Refer to caption](img/9493f7dba2702916c3de1d1b5d24b77e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9493f7dba2702916c3de1d1b5d24b77e.png)'
- en: 'Figure 1: Survey Architecture'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：调查架构
- en: The rest of this survey is organized as follows (as illustrated in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning for Data Caching in Edge
    Network") ). Section [2](#S2 "2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") presents the categories of content caching
    problem. Section [3](#S3 "3 Deep Learning Outline ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") reviews typical deep neural network structures.
    In Section [4](#S4 "4 Deep Learning for Data Caching ‣ A Survey of Deep Learning
    for Data Caching in Edge Network"), we list state-of-the-art DL-based caching
    strategies and their comparison. Section [5](#S5 "5 Research Challenges and Future
    Directions ‣ A Survey of Deep Learning for Data Caching in Edge Network") debates
    challenges as well as potential research directions. In the end, Section [6](#S6
    "6 Conclusions ‣ A Survey of Deep Learning for Data Caching in Edge Network")
    concludes this paper. For better readability, the abbreviations in this paper
    is listed as Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") shows.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的其余部分组织如下（如图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 边缘网络数据缓存深度学习调查") 所示）。第 [2](#S2 "2 数据缓存综述
    ‣ 边缘网络数据缓存深度学习调查") 节介绍了内容缓存问题的类别。第 [3](#S3 "3 深度学习概要 ‣ 边缘网络数据缓存深度学习调查") 节回顾了典型的深度神经网络结构。在第
    [4](#S4 "4 数据缓存的深度学习 ‣ 边缘网络数据缓存深度学习调查") 节中，我们列出了最先进的基于深度学习的缓存策略及其比较。第 [5](#S5
    "5 研究挑战与未来方向 ‣ 边缘网络数据缓存深度学习调查") 节讨论了挑战以及潜在的研究方向。最后，第 [6](#S6 "6 结论 ‣ 边缘网络数据缓存深度学习调查")
    节总结了本文。为了更好的可读性，本文中的缩写列表见表 [1](#S1.T1 "表 1 ‣ 1 介绍 ‣ 边缘网络数据缓存深度学习调查")。
- en: 'Table 1: List of Abbreviations.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 缩写列表。'
- en: '| Abbr. | Description | Abbr. | Description |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 缩写 | 描述 | 缩写 | 描述 |'
- en: '| 3C | Computing, Caching and Communication | A3C | Asynchronous Advantage
    Actor-Critic |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 3C | 计算、缓存与通信 | A3C | 异步优势演员-评论家 |'
- en: '| BBU | Baseband Unit | CCN | Content-Centric Network |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| BBU | 基带单元 | CCN | 内容中心网络 |'
- en: '| CNN | Convolutional Neural Network | CoMP-JT | Coordinated Multi Point Joint
    Transmission |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 卷积神经网络 | CoMP-JT | 协调多点联合传输 |'
- en: '| CR | Content Router | C-RAN | Cloud-Radio Access Network |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| CR | 内容路由器 | C-RAN | 云无线接入网络 |'
- en: '| CSI | Channel State Information | D2D | Device to Device |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| CSI | 信道状态信息 | D2D | 设备到设备 |'
- en: '| DDPG | Deep Deterministic Policy Gradient | DL | Deep Learning |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| DDPG | 深度确定性策略梯度 | DL | 深度学习 |'
- en: '| DNN | Deep Neural Network | DQN | Deep Q Network |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| DNN | 深度神经网络 | DQN | 深度Q网络 |'
- en: '| DRL | Deep Reinforcement Learning | DT | Digital Twin |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| DRL | 深度强化学习 | DT | 数字双胞胎 |'
- en: '| ED | End Device | ES | Edge Server |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| ED | 终端设备 | ES | 边缘服务器 |'
- en: '| ETSI | European Telecommunication Standardization Institute |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| ETSI | 欧洲电信标准化协会 |'
- en: '| ESN | Echo-State Network | FIFO | First In First Out |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| ESN | 回声状态网络 | FIFO | 先进先出 |'
- en: '| FNN | Feedforward Neural Network | FBS | Femto Base Station |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| FNN | 前馈神经网络 | FBS | 雌基站 |'
- en: '| ICN | Information-Centric Network | LFU | Least Frequently Used |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| ICN | 信息中心网络 | LFU | 最少频繁使用 |'
- en: '| LP | Linear Programming | LRU | Least Recently Used |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| LP | 线性规划 | LRU | 最少使用 |'
- en: '| LSTM | Long Short-Term Memory | MAR | Mobile Augmented Reality |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 长短期记忆 | MAR | 移动增强现实 |'
- en: '| MD | Mobile Device | MILP | Mixed Integer Linear Programming |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| MD | 移动设备 | MILP | 混合整数线性规划 |'
- en: '| MBS | Macro Base Station | NFV | Network Function Virtualization |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| MBS | 宏基站 | NFV | 网络功能虚拟化 |'
- en: '| PNF | Physical Network Function | PPO | Proximal Policy Optimization |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| PNF | 物理网络功能 | PPO | 近端策略优化 |'
- en: '| QoE | Quality of Experience | RL | Reinforcement Learning |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| QoE | 体验质量 | RL | 强化学习 |'
- en: '| RNN | Recurrent Neural Network | RRH | Remote Radio Head |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 循环神经网络 | RRH | 遥控无线头 |'
- en: '| SAE | Sparse Auto Encoder | SDN | Software Defined Network |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| SAE | 稀疏自编码器 | SDN | 软件定义网络 |'
- en: '| seq2seq | Sequence to Sequence | SNM | Shot Noise Model |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| seq2seq | 序列到序列 | SNM | 射线噪声模型 |'
- en: '| TRPO | Trust Region Policy Optimization | TTL | Time to Live |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| TRPO | 信任区域策略优化 | TTL | 生存时间 |'
- en: '| VNF | Virtual Network Function | WSN | Wireless Sensor Network |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| VNF | 虚拟网络功能 | WSN | 无线传感器网络 |'
- en: 2 Data Caching Review
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据缓存综述
- en: 'The paradigm of data caching in edge networks is illustrated in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Data Caching Review ‣ A Survey of Deep Learning for Data Caching
    in Edge Network"). Similarity to [[10](#bib.bib10)], the scope of edge in this
    paper is along the path between end user and data server, which contains Content
    Router (CR), Macro Base Station (MSB), Femto Base Station (FSB) and End Device
    (ED). In the context of Cloud-Radio Access Network (C-RAN)[[11](#bib.bib11)],
    both baseband unit (BBU) and remote radio head (RRH) are considered as potential
    caching candidates to hosting content, where the BBUs are clustered as a BBU pool
    centrally and RRHs are deployed near BS’s antenna distributively. According to
    the hierarchical structure of edge network, the data caching is classified into
    two categories: Layer 1 Caching and Layer 2 Caching. In this section we illustrate
    the typical research topics in these two areas.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘网络中的数据缓存模式在图[2](#S2.F2 "Figure 2 ‣ 2 Data Caching Review ‣ A Survey of Deep
    Learning for Data Caching in Edge Network")中有所说明。与[[10](#bib.bib10)]类似，本文中边缘的范围包括在最终用户和数据服务器之间的路径，其中包含内容路由器（CR）、宏基站（MSB）、微基站（FSB）和终端设备（ED）。在云无线接入网络（C-RAN）[[11](#bib.bib11)]的背景下，基带单元（BBU）和远程无线头（RRH）都被视为潜在的缓存候选者来托管内容，其中BBU集中成一个BBU池，而RRH则分布在基站的天线附近。根据边缘网络的层级结构，数据缓存分为两类：层级
    1 缓存和层级 2 缓存。本节我们将说明这两个领域中的典型研究课题。
- en: '![Refer to caption](img/6f5024e4294f7d10008b934d8a03e5fc.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6f5024e4294f7d10008b934d8a03e5fc.png)'
- en: 'Figure 2: Data Caching in Edge Network'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 边缘网络中的数据缓存'
- en: 2.1 Layer 1 Caching
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 层级 1 缓存
- en: 'In Layer 1 Caching, the popular content is considered to be hosted in CRs.
    In the context of Information-Centric Network (ICN), CR plays dual roles both
    as a typical router (i.e. data flow forwarding) and content store (i.e. local
    area data caching facility). Generally, the CR is connected via wired networks.
    Layer 1 Caching consists of four tightly coupled problems: where to cache, what
    to cache, cache dimensioning and content delivery[[12](#bib.bib12)].'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在层级 1 缓存中，热门内容被认为应托管在CR中。在信息中心网络（ICN）的背景下，CR既作为典型路由器（即数据流转发），又作为内容存储（即本地数据缓存设施）。通常，CR通过有线网络连接。层级
    1 缓存包括四个紧密相关的问题：缓存位置、缓存内容、缓存规模和内容传递[[12](#bib.bib12)]。
- en: 'Where to cache focuses on selecting the proper CRs to host the content. For
    instance, in Figure [2](#S2.F2 "Figure 2 ‣ 2 Data Caching Review ‣ A Survey of
    Deep Learning for Data Caching in Edge Network"), contents replicas can be placed
    in lower hierarchical level CRs, such as router B and C, as the mean of reducing
    transmission cost but with extra cost pay for hosting contents; reversely, consolidating
    caching in CR A can be adopted to saving caching cost at the expense of more transmission
    cost and has the risk of expiring end users’ delay requirement. Here the caching/hosting
    cost is the cost to deploy the content, which could be measured by space utilization,
    energy consumption or other metrics. The transmission cost represents the price
    for delivering the content from cached CR (or data server) to end user and is
    basically estimated via the number of hops. Where to cache problem usually has
    been modelled as a Mixed Integer Linear Programming (MILP):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存的位置集中于选择合适的内容路由器（CR）来托管内容。例如，在图[2](#S2.F2 "Figure 2 ‣ 2 Data Caching Review
    ‣ A Survey of Deep Learning for Data Caching in Edge Network")中，内容副本可以放置在较低层次的CR中，如路由器B和C，以减少传输成本，但需要额外的托管成本；相反，集中缓存于CR
    A可以节省缓存成本，但会增加传输成本，并有超出最终用户延迟要求的风险。在这里，缓存/托管成本是部署内容的成本，可以通过空间利用率、能源消耗或其他指标来衡量。传输成本表示将内容从缓存CR（或数据服务器）传递到最终用户的费用，通常通过跳数来估算。缓存位置问题通常被建模为混合整数线性规划（MILP）：
- en: '|  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\mathop{\min}_{\begin{subarray}{c}x\end{subarray}}\;$ |
    $\displaystyle c^{T}x$ |  | (1a) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathop{\min}_{\begin{subarray}{c}x\end{subarray}}\;$ |
    $\displaystyle c^{T}x$ |  | (1a) |'
- en: '|  | s.t. | $\displaystyle Ax\leq b$ |  | (1b) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | 满足 | $\displaystyle Ax\leq b$ |  | (1b) |'
- en: '|  |  | $\displaystyle x\in\{0,1\}$ |  | (1c) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle x\in\{0,1\}$ |  | (1c) |'
- en: '|  | or | $\displaystyle x\geq 0$ |  | (1d) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | 或 | $\displaystyle x\geq 0$ |  | (1d) |'
- en: where $x$ is the decision variable. Normally it is a binary variable indicating
    the CR assignment. In special cases, with the aim of modelling or linearization,
    some non-binary auxiliary variables are introduced as constraint ([1d](#S2.E1.4
    "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network")) shows. If taking caching a part of a file
    not the complete into consideration, the decision variable $x$ is a continuous
    variable representing the segments host in the CR, then constraint ([1c](#S2.E1.3
    "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network")) becomes $x\in[0,1]$ and MILP model turns to
    linear programming (LP). There are many work allocating contents via MILP with
    different objectives and limitations. The authors in [[13](#bib.bib13)] propose
    a model to minimize the user delay and load balancing level of CRs with the satisfaction
    of cache space. The work in [[14](#bib.bib14)] considers a trade-off between caching
    and transmission cost with cache space, link bandwidth and user latency constraints.
    In [[15](#bib.bib15)], an energy efficient optimization model is constructed consisting
    of caching energy and transport energy. [[16](#bib.bib16)] provides more details
    of mathematical model and related heuristic algorithms in caching deployment of
    wired networks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x$ 是决策变量。通常它是一个二进制变量，用于指示 CR 分配。在特殊情况下，为了建模或线性化，引入了一些非二进制的辅助变量作为约束，如[1d](#S2.E1.4
    "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network")所示。如果考虑缓存文件的部分而非完整文件，决策变量 $x$ 就是一个连续变量，表示 CR
    中的段，那么约束条件[1c](#S2.E1.3 "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣
    A Survey of Deep Learning for Data Caching in Edge Network") 变为 $x\in[0,1]$，MILP
    模型转化为线性规划 (LP)。许多工作通过 MILP 分配内容，具有不同的目标和限制。[[13](#bib.bib13)] 的作者提出了一种模型，以在满足缓存空间的条件下，最小化用户延迟和
    CR 的负载均衡水平。[[14](#bib.bib14)] 的工作考虑了缓存与传输成本之间的权衡，同时满足缓存空间、链路带宽和用户延迟的约束条件。在[[15](#bib.bib15)]中，构建了一个能源效率优化模型，包括缓存能源和传输能源。[[16](#bib.bib16)]
    提供了有关有线网络缓存部署的数学模型和相关启发式算法的更多细节。
- en: 'What to cache concentrates on selecting the proper contents in CRs for the
    purpose of maximizing the cache hit ratio. Via exploiting the statistical patterns
    of user requests, the popularity of requested information and user preference
    can be forecasted and play a very significant role in determining caching content.
    On the one hand, from the view of aggregated request contents, researchers propose
    many different models and algorithms for popularity estimation. One widely used
    model in web caching is the Zipf model based the assumption that the content popularity
    is static and each users’ request is independent[[17](#bib.bib17)]. However, this
    method fails to reflect the temporal and spatial correlations of the content,
    where the temporal correlation reflects the popularity varies over time and the
    spatial correlation represents the content preference is different on the geographical
    area and social cultural media. A temporal model named the shot noise model (SNM)
    is built in [[18](#bib.bib18)] which enables users to estimate the content popularity
    dynamically. Inspired by SNM, the work in [[19](#bib.bib19)] considers both spatial
    and temporal characteristics during caching decision. On the other hand, from
    the view of a specific end user during a certain period, caching his/her preference
    content (may not be the popular in network) can also help to reduce the traffic
    flow. Many approaches in recommendation systems can be applied in this case[[20](#bib.bib20)].
    Another aspect of what to cache problem is the designing of cache eviction strategies
    when storage space faces the risk of overflow. Depending on the life of caching
    contents, these policies can be divided into two categories roughly: one is like
    first in first out (FIFO), least frequently used (LFU), least recently used (LRU)
    and randomized replacement, the contents would not be removed until no more memory
    is available; the other one is called time to live (TTL) strategy, where the eviction
    happens once the related timer expires. [[21](#bib.bib21)] presents analytic model
    for hit ratio in TTL-based cache requested by independent and identically distributed
    flows. It worth noting that in [[21](#bib.bib21)], the TTL-based cache policy
    is used for the consistency of dynamic contents instead of contents replacement.
    In [[22](#bib.bib22)], the authors introduce a TTL model for cache eviction and
    the timer is reset once related content cache hit happens.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 关于缓存内容的选择集中在选择适当的 CR 内容，以最大化缓存命中率。通过利用用户请求的统计模式，可以预测请求信息的流行程度和用户偏好，这在确定缓存内容中发挥着非常重要的作用。一方面，从汇总请求内容的角度来看，研究人员提出了许多不同的流行度估计模型和算法。在
    Web 缓存中，广泛使用的一种模型是基于内容流行度静态且每个用户请求独立的假设的 Zipf 模型[[17](#bib.bib17)]。然而，这种方法未能反映内容的时间和空间相关性，其中时间相关性反映了流行度随时间变化，而空间相关性表示地理区域和社会文化媒体上的内容偏好不同。[[18](#bib.bib18)]
    中建立了一种称为短噪声模型（SNM）的时间模型，它使用户能够动态估计内容流行度。受到 SNM 启发，[[19](#bib.bib19)] 的工作在缓存决策过程中考虑了空间和时间特征。另一方面，从特定终端用户在某一时间段的角度来看，缓存其/她的偏好内容（可能在网络上不流行）也有助于减少流量。在这种情况下，许多推荐系统中的方法可以应用[[20](#bib.bib20)]。缓存内容面对存储空间溢出风险时，另一个方面是设计缓存驱逐策略。根据缓存内容的生命周期，这些策略大致可以分为两类：一种类似于先进先出（FIFO）、最少使用（LFU）、最近最少使用（LRU）和随机替换，内容不会被移除，直到没有更多内存可用；另一种称为生存时间（TTL）策略，其中一旦相关定时器到期即发生驱逐。[[21](#bib.bib21)]
    提出了 TTL 基于缓存的命中率的分析模型，这些缓存由独立同分布的流请求。值得注意的是，在 [[21](#bib.bib21)] 中，TTL 基于缓存策略用于动态内容的一致性，而不是内容替换。在
    [[22](#bib.bib22)] 中，作者介绍了一个用于缓存驱逐的 TTL 模型，并且一旦相关内容缓存命中，定时器会被重置。
- en: 'Cache dimensioning highlights how much storage space to be allocated. Benefit
    from the softwarization and virtualization technologies, the cache size in each
    CR or edge cloud can be managed in a more flexible and dynamical way, which makes
    the cache dimensioning decisions become an important feature in data caching.
    Technically, the cache hit ratio rises with the increasing of cache memory, and
    consequently eases the traffic congestion in the core network. However, excessive
    space allocation would waste the resource like energy to support the caching function.
    Hence there is a trade-off between cache size cost and network congestion. Economically,
    taking such scenario into consideration: a small content provider wants to rent
    service from a CDN provider such as Akamai or Huawei Cloud, and there is also
    a balance between investment saving and network performance. In [[23](#bib.bib23)],
    the proper cache size of individual CR in Content-Centric Network (CCN) is investigated
    via exploiting the network topology. In [[24](#bib.bib24)], the authors consider
    the effect of network traffic distribution and user behaviours when designing
    cache size.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存维度化突出需要分配多少存储空间。得益于软件化和虚拟化技术，每个CR或边缘云中的缓存大小可以以更灵活和动态的方式进行管理，这使得缓存维度化决策成为数据缓存中的一个重要特征。在技术上，缓存命中率随着缓存内存的增加而上升，从而缓解了核心网络中的流量拥堵。然而，过度分配空间会浪费资源，如能源，以支持缓存功能。因此，缓存大小成本和网络拥堵之间存在权衡。从经济角度来看，考虑到这样的情景：一个小型内容提供商希望从CDN提供商（如Akamai或华为云）租赁服务，在投资节省和网络性能之间也存在平衡。在[[23](#bib.bib23)]中，通过利用网络拓扑调查了内容中心网络（CCN）中每个CR的合适缓存大小。在[[24](#bib.bib24)]中，作者在设计缓存大小时考虑了网络流量分布和用户行为的影响。
- en: Content delivery considers how to transform the caching content to the requested
    user. The delivery traffic embraces single cache file downloading and video content
    steaming and the metrics for these two scenarios vary. Regarding file downloading,
    the content cannot be consumed until the delivery is completed. Therefore the
    downloading time of the entire file is viewed as a metric to reflect the quality
    of experience (QoE). For video steaming, especially for those large video splitted
    into several chunks, the delay limitation only works on the first chunk. In that
    case, delivering the first chunk in time and keep the smooth transmission of the
    rest chunks are the key aims [[25](#bib.bib25)]. Apart from those measuring metrics,
    another problem in content delivery is the routing policy. In CCN [[26](#bib.bib26)],
    one implementation of ICN architecture, employs a flooding-based name routing
    protocol to publish the request among cached CRs. On one hand, flooding strategy
    simplifies the designing complexity and reduce the maintaining cost particularly
    in an unstable scenario; on the other hand, it costly wastes bandwidth resources.
    In [[27](#bib.bib27)], the authors discuss the optimal radius in scoped flooding.
    The deliver route is often considered jointly with where to cache problem, in
    which the objective function ([1a](#S2.E1.1 "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data
    Caching Review ‣ A Survey of Deep Learning for Data Caching in Edge Network"))
    includes both deployment and routing cost.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 内容交付考虑如何将缓存内容转换为用户所请求的内容。交付流量包括单一缓存文件下载和视频内容流媒体，这两种情况的指标有所不同。对于文件下载，内容在交付完成之前无法被使用。因此，整个文件的下载时间被视为反映体验质量（QoE）的指标。对于视频流媒体，特别是那些被分成多个块的大型视频，延迟限制仅对第一个块有效。在这种情况下，按时交付第一个块并保持其余块的顺畅传输是关键目标[[25](#bib.bib25)]。除了这些测量指标，内容交付中的另一个问题是路由策略。在CCN
    [[26](#bib.bib26)]中，ICN架构的一种实现，采用基于泛洪的名称路由协议来在缓存的CR之间发布请求。一方面，泛洪策略简化了设计复杂性，并减少了特别是在不稳定情况下的维护成本；另一方面，它会浪费带宽资源。在[[27](#bib.bib27)]中，作者讨论了范围泛洪中的最佳半径。传递路径通常与缓存问题共同考虑，其中目标函数
    ([1a](#S2.E1.1 "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣ A Survey
    of Deep Learning for Data Caching in Edge Network")) 包括部署和路由成本。
- en: 2.2 Layer 2 Caching
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 第二层缓存
- en: 'Contrast to Layer 1 caching in wired connection, Layer 2 caching considers
    implementing caching techniques in wireless network. Though both of them need
    solve where to cache, what to cache, cache dimensioning and content delivery problems,
    wireless caching is more challenging and some mature strategies in wired caching
    cannot be migrated directly to wireless case. Some reasons come from the listed
    aspects: the resources in wireless environment, such as caching storage and spectrum,
    are limited compared with CRs in Layer 1 Caching; the mobility of end users and
    dynamic network typologies are also required to be considered during the design
    of caching strategies; moreover, the wireless channels are uncertain since they
    can be effected by fading and interference.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与有线连接中的第1层缓存相比，第2层缓存考虑在无线网络中实施缓存技术。尽管两者都需要解决缓存的位置、缓存的内容、缓存的维度以及内容传输等问题，但无线缓存面临的挑战更大，一些成熟的有线缓存策略不能直接迁移到无线场景中。原因包括以下几个方面：无线环境中的资源，如缓存存储和频谱，相较于第1层缓存中的CR（缓存资源）是有限的；用户的移动性和动态的网络拓扑也需要在缓存策略设计中加以考虑；此外，无线信道是不确定的，因为它们可能受到衰落和干扰的影响。
- en: In wireless caching, where to cache focus on finding the proper candidates among
    MBS, FBS, ED, even BBU pool and RRU in C-RAN to host the content. Caching at MBS
    and FBS can alleviate backhaul congestion since end users obtain the requested
    content from BS directly instead of from CR via backhaul links. Compared with
    FBS, MBS has wider coverage and typically, there is no overlap among different
    MBSs [[28](#bib.bib28)]. As mentioned above, the caching space in BSs is limited
    and it is impractical to cache all popular content. With the aim of improving
    cache-hit ratio, a MILP-modelled collaborative caching strategy among MBSs is
    proposed in [[29](#bib.bib29)]. If the accessed MBS does not host the content,
    the request will be served by a neighbour MBS which cache the file rather than
    by the data server. For FBS caching, a distributed caching method is presented
    in [[30](#bib.bib30)] and the main idea is that the ED locating in the FBS coverage
    overlap is able to obtain contents from multiple hosters. Caching at ED can not
    only ease backhaul congestion but also improve the area spectral efficiency [[28](#bib.bib28)].
    When the end user requests a content, he/she would be severed by the local storage
    if the content is precached in his/her ED or by adjacent ED via D2D communication
    if the content is host accordingly. In [[31](#bib.bib31)], the authors model the
    cache-enabled D2D network as a Poisson cluster process, where end users are grouped
    into several clusters and the collective performance is improved. Individually,
    caching the interested contents for other users affects personal benefit. In [[32](#bib.bib32)],
    a Stackelberg game model is applied to formulate the conflict among end users
    and a related incentive mechanism is designed to encourage content sharing. For
    the case of cache-enabled C-RAN, caching at BBU can ease the traffic congestion
    in the backhaul while caching at RRH can reduce the fronthaul communication cost.
    On the other hand, caching all at BBU raises the signaling overhead of BBU pool
    while at RRH weakens the processing capability. Therefore, where to cache the
    content in C-RAN makes a substantial contribution to balancing the signal processing
    capability at the BBU pool and the backhaul/fronthaul costs [[28](#bib.bib28)].
    The work in [[33](#bib.bib33)] investigates caching at RRHs with jointly considering
    cell outage probability and fronthaul utilization. Due to the end users’ mobility,
    the prediction/awareness of user moving behaviour also influence the proper hoster
    selection. There are some researches exploiting user mobility in cache strategy
    designing like [[34](#bib.bib34)] and [[35](#bib.bib35)].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Similar with Layer 1, what to cache decision as well as eviction policy of layer
    2 depends on the accurate prediction on content popularity or user preference
    in proactive caching method. The content popularity contains the feature of temporal
    and spatial correlations, which has already been described in Layer 1 Caching.
    In Layer 2 caching, the proper spatial granularity in popular contents estimation
    needs to take special attentions [[36](#bib.bib36)]. For example, the coverage
    of MBS and FBS are different, which makes the popularity in MBS and FBS are different
    as well. Because the former based on a large number of users’ behaviors but the
    individual may prefer specific content categories. For small cells, the preference
    estimation requires more accurate information like historical data [[28](#bib.bib28)].
    In order to capture the temporal and spatial dynamics of user preference, many
    different deep learning based algorithms are proposed, which will be illustrated
    in Section [4](#S4 "4 Deep Learning for Data Caching ‣ A Survey of Deep Learning
    for Data Caching in Edge Network").
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与第1层类似，第2层的缓存决策以及驱逐策略依赖于对内容受欢迎程度或用户偏好的准确预测，这些都在前向缓存方法中体现。内容受欢迎程度包含了时间和空间相关性的特征，这在第1层缓存中已经描述过。在第2层缓存中，估算热门内容时需要特别注意适当的空间粒度[[36](#bib.bib36)]。例如，MBS和FBS的覆盖范围不同，这使得MBS和FBS的受欢迎程度也有所不同。前者基于大量用户行为，而个体可能偏好特定内容类别。对于小型蜂窝网络，偏好估算需要更准确的信息，如历史数据[[28](#bib.bib28)]。为了捕捉用户偏好的时间和空间动态，提出了许多不同的基于深度学习的算法，这将在第[4](#S4
    "4 Deep Learning for Data Caching ‣ A Survey of Deep Learning for Data Caching
    in Edge Network")节中说明。
- en: Cache dimensioning in Layer 2 Caching has more complicated factors need to be
    considered, not only including the network topology and content popularity as
    Layer 1 Caching, but also containing backhaul transmission status and wireless
    channel features. The proper cache size assignment is studied in the scenario
    of backhaul limited cellular network [[37](#bib.bib37)]. It also provides the
    closed-form boundary of minimum cache size in one cell case. In the case of dense
    wireless network, the work in [[38](#bib.bib38)] quantifies the minimum required
    cache to achieve the linear capacity scaling of network throughput. The authors
    of [[39](#bib.bib39)] also consider the scenario of dense networks. They derive
    the closed-form of the optimal memory size which can reduce the consumption of
    backhaul capacity as well as guarantee wireless QoS.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第2层缓存的缓存维度设计涉及更多复杂因素，需要考虑的不仅包括网络拓扑和内容受欢迎程度，还包括回传传输状态和无线信道特性。研究了在回传有限的蜂窝网络场景下的适当缓存大小分配[[37](#bib.bib37)]。它还提供了单个小区情况下最小缓存大小的封闭形式边界。在密集无线网络的情况下，[[38](#bib.bib38)]中的工作量化了实现网络吞吐量线性容量扩展所需的最小缓存量。[[39](#bib.bib39)]的作者也考虑了密集网络的场景。他们推导出了最佳内存大小的封闭形式，这可以减少回传容量的消耗，同时保证无线服务质量。
- en: 'According to the number of transmitters and receivers, we divide the content
    delivery in Layer 2 caching into three categories: one candidate serves one end
    user, such as unicast and D2D transmission; one candidate serves multiple users
    like multicast; and coordinated delivery including multiple transmitters serve
    one or more receivers like coordinated multi-point joint transmission (CoMP-JT).
    Once the requested content is cached locally, BS can serve the end user via unicast
    or the adjacent device shares the contents by implementing D2D transmission. Concurrent
    transmission has the risk of co-channel interference in dense deployed networks.
    In D2D network, link scheduling is introduced to select subsets of links to transmit
    simultaneously [[28](#bib.bib28)]. With the aim of improving the spectral efficiency,
    multicast is applied in content delivery when serving multiple requests simultaneously
    with the same content. Therefore there is a trade off between spectral efficiency
    and service delay. For the aim of serving more users in one transmission as well
    as higher spectral efficiency, the BS will wait to collect enough requirement
    for the same content which makes the first request a long waiting time. An optimal
    dynamic multicast scheduling is proposed in [[40](#bib.bib40)] to balance these
    two factors. Multicast can also serve multiple requests with different contents.
    In [[41](#bib.bib41)], the authors provides a coded caching scheme which requires
    the communication link is error free and each user caches a part of its own content
    and partial of other users. Then BS multicasts the coded data to all users. Each
    user can decode his own requested content by XOR operation between the received
    data and the precached other users’ file. However, the coding complexity increases
    exponentially as the quantity of end users grows. The CoMP-JT can improve the
    spectral efficiency as well via sharing channel state information (CSI) and contents
    among BSs but it also needs high-capacity backhaul consumption for exchanging
    data. In C-RAN, the BBUs are centralized in the BBU pool, which makes communication
    among BSs very efficiency. [[42](#bib.bib42)] designs CoMP-JT in C-RAN for the
    purpose of minimizing power consumption with limitations of transmission energy,
    link capacity and requested QoS.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据发射器和接收器的数量，我们将第2层缓存中的内容传输分为三类：一个候选者服务于一个终端用户，例如单播和D2D传输；一个候选者服务于多个用户，如多播；以及协调传输，包括多个发射器服务于一个或多个接收器，如协调多点联合传输（CoMP-JT）。一旦请求的内容被本地缓存，基站（BS）可以通过单播服务终端用户，或通过实现D2D传输，邻近设备共享内容。并发传输在密集部署网络中存在同频干扰的风险。在D2D网络中，引入链路调度以选择同时传输的链路子集
    [[28](#bib.bib28)]。为了提高频谱效率，当同时处理多个请求时，多播被应用于内容传输。因此，频谱效率和服务延迟之间存在权衡。为了在一次传输中服务更多用户以及提高频谱效率，基站将等待收集足够的相同内容的需求，这使得首次请求的等待时间较长。[[40](#bib.bib40)]
    提出了一个优化的动态多播调度方案，以平衡这两个因素。多播还可以服务于不同内容的多个请求。在 [[41](#bib.bib41)] 中，作者提供了一种编码缓存方案，该方案要求通信链路无误差，每个用户缓存自己的一部分内容以及其他用户的部分内容。然后基站将编码数据广播给所有用户。每个用户可以通过对接收的数据和预缓存的其他用户文件进行
    XOR 操作来解码自己请求的内容。然而，随着终端用户数量的增长，编码复杂性呈指数级增加。CoMP-JT 通过在基站之间共享频道状态信息（CSI）和内容，也可以提高频谱效率，但它也需要高容量的回传链路来交换数据。在
    C-RAN 中，BBU 被集中在 BBU 池中，这使得基站之间的通信非常高效。[[42](#bib.bib42)] 在 C-RAN 中设计了 CoMP-JT，旨在最小化功耗，并考虑传输能量、链路容量和请求的
    QoS 限制。
- en: 3 Deep Learning Outline
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习概述
- en: 'As Figure [3](#S3.F3 "Figure 3 ‣ 3 Deep Learning Outline ‣ A Survey of Deep
    Learning for Data Caching in Edge Network") shows, some typical deep neural network
    (DNN) methods are stated. These models are classified into three categories depending
    on the training methods: supervised learning, unsupervised learning and reinforcement
    learning.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3](#S3.F3 "Figure 3 ‣ 3 Deep Learning Outline ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") 所示，列出了几种典型的深度神经网络（DNN）方法。这些模型根据训练方法的不同被分为三类：监督学习、无监督学习和强化学习。
- en: '![Refer to caption](img/436e8d028fa2a7f5ceeb406c762bdd24.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/436e8d028fa2a7f5ceeb406c762bdd24.png)'
- en: 'Figure 3: Typical DNN Structures'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：典型的 DNN 结构
- en: 3.1 Feedforward Neural Network (FNN)
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 前馈神经网络（FNN）
- en: FNN is a kind of DNNs whose information propagation direction is forward and
    there is no cycle in neurons. In this paper, the term FNN is used to represent
    fully connected neural network, which indicates the connection between two adjacent
    layers is filled. According to the Universal Approximation Theorem, FNN has the
    ability to approximate any closed and bounded function with enough neurons in
    hidden layer [[43](#bib.bib43)]. The hidden layer is applied to extract features
    of input vector, and then feed the output layer, which works as a classifier.
    Though FNN is very powerful, it gets into trouble when dealing with real-world
    task such as image recognition due to enormous weight parameters (because of fully
    connected) and lack of data augmentation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Convolutional Neural Network (CNN)
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the aim of overcoming the aforementioned drawback of FNN, CNN employs convolution
    and pooling operations, where the former applies sliding convolutional filters
    to the input vector and the later does down sampling, usually via maximum or mean
    pooling. Generally, CNN tends to contain deeper layers and smaller convolutional
    filters, and the structure becomes fully convolutional network [[44](#bib.bib44)],
    reducing the ratio of pooling layers as well as fully connected layers. Taxonomically,
    CNN belongs to FNN and has been broadly employed in image recognition, video analysis,
    natural language processing, etc. Including CNN, one of the limitations of FNN
    is that the output only depends on current input vectors. So it is hard to deal
    with sequential tasks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Recurrent Neural Network (RNN)
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to deal with sequential tasks and using historical information, RNN
    employs neurons with self feedback in hidden layers. Unlike the hidden neuron
    in FNN, the output of recurrent neuron depends on both current output of precious
    layer and last hidden state. Compared with FNN approximates any continues functions,
    RNN with Sigmoid activation function can simulate a universal Turing Machine and
    has the ability to solve all computational problems [[45](#bib.bib45)]. It is
    worth noting that RNN has the risk to suffer from long-term dependencies problem
    [[43](#bib.bib43)] including gradient exploding and vanishing. Additionally, RNN
    has more parameters waiting to be trained due to adding recurrent weights. In
    the following, we introduce some RNN variants as Figure [4](#S3.F4 "Figure 4 ‣
    3.3 Recurrent Neural Network (RNN) ‣ 3 Deep Learning Outline ‣ A Survey of Deep
    Learning for Data Caching in Edge Network") shows.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/92efa15b66cb14c37adb80f9d051ac85.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: RNN Variants'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Echo-State Network (ESN)
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As aforementioned, simple RNN contains more parameters in training step, where
    the recurrent weights and input weights are difficult to learn [[43](#bib.bib43)].
    The basic idea of ESN is fixing these two kinds of weights and only learn the
    output weights (as links highlighted in Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Recurrent
    Neural Network (RNN) ‣ 3 Deep Learning Outline ‣ A Survey of Deep Learning for
    Data Caching in Edge Network")). The hidden layer is renamed as reservoir in ESN,
    where the neurons are sparsely connected and the weights are randomly assigned.
    The recurrent weights keep constant so the information of previous moments is
    stored in the reservoir with constant weight like voice echoing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Long Short-Term Memory (LSTM)
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recently, an efficient way to cope with long-term dependencies in practical
    is employing gated RNN, including LSTM [[43](#bib.bib43)]. Then we compare with
    the recurrent neuron in simple RNN: Internally LSTM introduces three gates to
    control signal propagation, where input gate $I$ decides the partition of input
    signal to be stored, forget gate $F$ controls ratio of last moment memory to be
    kept until next period (the name "forget gate" may be a little misleading because
    it actually represents the ratio to be remembered) and output gate $O$ influences
    the proportion of current state to be delivered; Externally LSTM has four inputs
    embracing one input signal and three control signals for three gates. All these
    four signals are derived via the calculation of current network input and last
    moment delivered state.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Pointer Network
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A typical application of RNN is converting one sequence to another sequence
    (seq2seq) such as machine translation. Conventionally, the output of seq2seq architecture
    is a probability distribution of output dictionary. However, it cannot deal with
    the problem that the size of output relies on the length of input due to fixed
    output dictionary. In [[46](#bib.bib46)], the authors modify the output to be
    the distribution of input sequence, which is analogous to pointers in C/C++. Pointer
    network has been widely used in text condensation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Auto Encoder
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Auto Encoder is a stack of two NNs named encoder and decoder respectively, where
    the former tries to learn the representative characteristics of input and generate
    a related code, and the later reads the code and reconstructs the original input.
    In order to avoid the auto encoder simply copying the input, some restrictions
    are considered like the dimension of code is smaller than input vector [[43](#bib.bib43)].
    The quality of auto encoder can be measured via reconstruction error, which estimates
    the similarity between input and output. In most cases, the auto encoder is used
    for the proper representation of input vector so the decoder part is removed after
    unsupervised training. The code can be employed as input for further deep learning
    models.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Deep Reinforcement Learning (DRL)
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) is a Markov Decision Process represented by a quintuple
    $\{\mathcal{S,A,P,R},\gamma\}$, where $\mathcal{S}$ is the state space controlled
    by environment; $\mathcal{A}$ is the action space determined by agent; $\mathcal{P}$
    is the state transition function measuring the probability of moving to a new
    state $s_{t+1}$ given previous state $s_{t}$ and action $a_{t}$; $R$ is reward
    function calculated by environment considering state and action; $\gamma$ is a
    discount factor for estimating total reward. During the interaction between agent
    and environment, agent observes current state $s_{t}$ from environment, and then
    takes action $a_{t}$ following its policy $\pi$. The environment moves to a new
    state $s_{t+1}$ stochastically based on $\mathcal{P}(s_{t},a_{t})$ and returns
    a reward $r_{t}$ to agent. The RL’s aim is finding the policy $\pi$ to maximum
    accumulated reward $\sum_{t}\gamma^{t}r_{t}$. In the early stage, RL focuses on
    scenarios whose $\mathcal{S}$ and $\mathcal{A}$ are discrete and limited. So the
    agent can use a table to record these information. Recently, some tasks have enormous
    discrete states and actions such as playing go and even continuous value such
    as self-driving, which makes table recording impractical. In order to solve this,
    DRL combines RL and DL, where RL defines the problem and optimization object;
    DL models the policy and the reward expectation. Depending on the roles of DNN
    in DRL, we classify the DRL into 3 categories as Figure shows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一个由五元组 $\{\mathcal{S,A,P,R},\gamma\}$ 表示的马尔可夫决策过程，其中 $\mathcal{S}$
    是由环境控制的状态空间；$\mathcal{A}$ 是由代理确定的动作空间；$\mathcal{P}$ 是状态转移函数，测量给定先前状态 $s_{t}$ 和动作
    $a_{t}$ 的情况下，转移到新状态 $s_{t+1}$ 的概率；$R$ 是由环境根据状态和动作计算的奖励函数；$\gamma$ 是用于估计总奖励的折扣因子。在代理和环境的交互过程中，代理从环境中观察当前状态
    $s_{t}$，然后按照其策略 $\pi$ 采取动作 $a_{t}$。环境基于 $\mathcal{P}(s_{t},a_{t})$ 随机转移到新状态 $s_{t+1}$
    并返回奖励 $r_{t}$ 给代理。RL 的目标是找到使累计奖励 $\sum_{t}\gamma^{t}r_{t}$ 最大化的策略 $\pi$。在早期阶段，RL
    关注于 $\mathcal{S}$ 和 $\mathcal{A}$ 是离散且有限的场景。因此，代理可以使用表格记录这些信息。最近，一些任务具有大量离散状态和动作，例如围棋，甚至有连续值任务如自动驾驶，这使得表格记录变得不切实际。为了解决这个问题，DRL
    结合了 RL 和 DL，其中 RL 定义问题和优化目标；DL 则对策略和奖励期望进行建模。根据 DNN 在 DRL 中的角色，我们将 DRL 分为 3 类，如图所示。
- en: 3.5.1 DNN as Critic (Value-Based)
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 DNN 作为批评者（基于价值）
- en: 'In value-based method, DNN does not get involved with policy decision but estimates
    the policy performance. Two functions are introduced for the measurement: $V^{\pi}(s)$
    represents the reward expectation of policy $\pi$ starting from state $s$; $Q^{\pi}(s,a)$
    illustrates the reward expectation of policy $\pi$ starting from state $s$ and
    taking action $a$. In addition, $V^{\pi}(s)$ is the expected value of $Q^{\pi}(s,a)$.
    If we can estimate $Q^{\pi}(s,a)$, the policy $\pi$ can also be improved by choosing
    the action $a^{*}$ hold $Q^{\pi}(s,a^{*})\geq V^{\pi}(s)$. So the DNN employed
    in agent is approximating function $Q^{\pi}(s,a)$, where the inputs are state
    $s$ and action $a$ and output is the estimated value $Q^{\pi}(s,a)$. There are
    some representative critic methods like Deep Q Networks (DQN) [[47](#bib.bib47)]
    and its variants Double DQN [[48](#bib.bib48)], Dueling DQN [[49](#bib.bib49)],
    etc.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于价值的方法中，DNN 不涉及策略决策，而是评估策略的性能。为了进行测量，引入了两个函数：$V^{\pi}(s)$ 代表从状态 $s$ 开始的策略
    $\pi$ 的奖励期望；$Q^{\pi}(s,a)$ 说明从状态 $s$ 开始并采取动作 $a$ 的策略 $\pi$ 的奖励期望。此外，$V^{\pi}(s)$
    是 $Q^{\pi}(s,a)$ 的期望值。如果我们能够估计 $Q^{\pi}(s,a)$，则可以通过选择动作 $a^{*}$ 使得 $Q^{\pi}(s,a^{*})\geq
    V^{\pi}(s)$ 来改进策略 $\pi$。因此，代理中使用的 DNN 近似函数 $Q^{\pi}(s,a)$，其中输入是状态 $s$ 和动作 $a$，输出是估计值
    $Q^{\pi}(s,a)$。一些具有代表性的批评方法包括 Deep Q Networks (DQN) [[47](#bib.bib47)] 及其变体 Double
    DQN [[48](#bib.bib48)]、Dueling DQN [[49](#bib.bib49)] 等。
- en: 3.5.2 DNN as Actor (Policy-Based)
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 DNN 作为行动者（基于策略）
- en: In policy-based method, DNN gets involved in the action selection directly instead
    of via $Q^{\pi}(s,a)$. The policy can be viewed as an optimization problem, where
    the objective function is maximizing reward expectation and the search space is
    policy space. The input of DNN is current state and output is the probability
    distribution of potential actions. By employing gradient ascent, we can update
    the DNN to provide better action then maximize total reward. Some popular algorithms
    include Trust Region Policy Optimization (TRPO) [[50](#bib.bib50)], Proximal Policy
    Optimization (PPO) [[51](#bib.bib51)].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于策略的方法中，DNN直接参与动作选择，而不是通过$Q^{\pi}(s,a)$。策略可以视为一个优化问题，其中目标函数是最大化奖励期望，搜索空间是策略空间。DNN的输入是当前状态，输出是潜在动作的概率分布。通过使用梯度上升，我们可以更新DNN以提供更好的动作，从而最大化总奖励。一些流行的算法包括信任区域策略优化（TRPO）[[50](#bib.bib50)]，近端策略优化（PPO）[[51](#bib.bib51)]。
- en: 3.5.3 Actor-Critic Model
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 Actor-Critic模型
- en: Generally, compared with policy-based approach, the value-based method is less
    stable and suffer from poor convergence since the policy is derived based on $Q^{\pi}(s,a)$
    approximation. But value-based method is more sample efficient, while policy-based
    method is easier to fall into local optimal solution because the search space
    is vast. The actor-critic model combines these two approaches, i.e. the agent
    contains two DNNs named actor and critic respectively. In each training iteration,
    the actor considers current state $s$ and policy $\pi$ for deciding action $a$.
    Then the environment changes to state $s^{\prime}$ and returns reward $r$. The
    critic updates its own parameters based on the feedback from environment and output
    a mark for the actor’s action. The actor updates the policy $\pi$ depending on
    critic’s mark. Some typical algorithms are proposed recent years like Deep Deterministic
    Policy Gradient (DDPG) [[52](#bib.bib52)] and Asynchronous Advantage Actor-Critic
    (A3C) [[53](#bib.bib53)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，与基于策略的方法相比，基于价值的方法不够稳定且收敛性差，因为策略是基于$Q^{\pi}(s,a)$近似得出的。但基于价值的方法样本效率更高，而基于策略的方法则更容易陷入局部最优解，因为搜索空间很大。Actor-Critic模型结合了这两种方法，即代理包含两个DNN，分别称为actor和critic。在每次训练迭代中，actor考虑当前状态$s$和策略$\pi$以决定动作$a$。然后环境变化到状态$s^{\prime}$并返回奖励$r$。critic基于来自环境的反馈更新自身参数，并为actor的动作输出评分。actor根据critic的评分更新策略$\pi$。近年来提出了一些典型算法，如深度确定性策略梯度（DDPG）[[52](#bib.bib52)]和异步优势actor-critic（A3C）[[53](#bib.bib53)]。
- en: 4 Deep Learning for Data Caching
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习用于数据缓存
- en: 'We divide the studies regarding deep learning for data caching in edge networks
    into four categories depending on the DL tools employed: FNN and CNN; RNN; Auto
    Encoder; DRL. Recently many works utilize more than one DL techniques for jointly
    considered caching problems. For instance, at the beginning we applies a RNN to
    predict content popularity, and then a DRL to find suboptimal solutions of content
    placement for the purpose of reducing time complexity. In such case, we classify
    the related work into DRL since it represents the caching allocation policy. Unless
    mention the caching location (such as CRs, MBSs, FBSs, EDs and BBUs) otherwise,
    the approaches in this section can be utilized for both Layer 1 and Layer 2 caching.
    Table [2](#S4.T2 "Table 2 ‣ 4 Deep Learning for Data Caching ‣ A Survey of Deep
    Learning for Data Caching in Edge Network") summarize some studies of DL for caching.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将边缘网络中关于数据缓存的深度学习研究分为四类，依据所使用的DL工具：FNN 和 CNN；RNN；自动编码器；DRL。最近许多研究结合了多种DL技术来共同解决缓存问题。例如，开始时我们使用RNN预测内容流行度，然后使用DRL寻找内容放置的次优解，以减少时间复杂度。在这种情况下，我们将相关工作归入DRL，因为它代表了缓存分配策略。除非提及缓存位置（如CRs、MBSs、FBSs、EDs
    和 BBUs），否则本节的方法可用于层1和层2缓存。表[2](#S4.T2 "表 2 ‣ 4 深度学习用于数据缓存 ‣ 边缘网络中深度学习用于数据缓存的调查")总结了部分DL缓存研究。
- en: 'Table 2: Summary of Deep Learning for Data Caching'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：深度学习用于数据缓存的总结
- en: '| method | Study | Caching Problem | DL Objective |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 研究 | 缓存问题 | DL 目标 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| FNN and CNN | [[54](#bib.bib54)] | content delivery | reduce feasible region
    of time slot allocation |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| FNN 和 CNN | [[54](#bib.bib54)] | 内容交付 | 减少时间槽分配的可行区域 |'
- en: '| [[55](#bib.bib55)] | where to cache, content delivery | determine MBSs for
    caching & delivery duration |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| [[55](#bib.bib55)] | 缓存位置，内容交付 | 确定缓存和交付持续时间的MBSs |'
- en: '| [[56](#bib.bib56)] | where to cache, content delivery | nominate proper CRs
    for caching |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| [[57](#bib.bib57)] | where to cache, content delivery | reduce feasible region
    for caching |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| [[58](#bib.bib58)] | what to cache | extract video features |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| [[59](#bib.bib59)] | what to cache | predict requested content & frequency
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| [[60](#bib.bib60)] | what to cache | predict requested content |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| [[61](#bib.bib61)] | what to cache | predict content popularity |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| RNN | [[62](#bib.bib62), [63](#bib.bib63)] | what to cache | predict requested
    content & user mobility |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] | what to cache | predict content popularity |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] | content delivery | reduce traffic load, select optimal
    BS subset |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| Auto Encoder | [[70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73),
    [74](#bib.bib74)] | what to cache | predict content popularity |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| [[75](#bib.bib75)] | what to cache | predict top popular contents |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| DRL | [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84),
    [85](#bib.bib85)] | what to cache | decide cache placement |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| [[86](#bib.bib86)] | what to cache | decide cache replacement & power allocation
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| [[87](#bib.bib87)] | what to cache | predict popularity & searching best
    NN model |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| [[88](#bib.bib88)] | where to cache | decide cache location |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| [[89](#bib.bib89)] | content delivery | users grouping |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| [[90](#bib.bib90)] | where to cache, content delivery | decide BS connection,
    computation offloading & caching location |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| [[91](#bib.bib91)] | what to cache, content delivery | decide caching & bandwidth
    allocation |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| [[92](#bib.bib92)] | what to cache, content delivery | decide caching, computing
    offloading & radio resource allocation |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| [[93](#bib.bib93)] | what to cache, content delivery | decide multicast scheduling
    & caching replacement |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| [[94](#bib.bib94)] | where & what to cache | predict popularity, decide caching
    & task offloading |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| [[95](#bib.bib95)] | where & what to cache, content delivery | predict user
    mobility & content popularity, determine D2D link |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: 4.1 FNN and CNN
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [[54](#bib.bib54)], the content delivery problem in wireless network is formulated
    as two MILP optimization models with the aims of minimum delivery time slot and
    energy consumption respectively. Both models consider the data rate for content
    delivery. Considering the computational complexity of solving MILP, a CNN is introduced
    to reduce the feasible region of decision variables, where the input is channel
    coefficients matrix. The FNN in paper [[55](#bib.bib55)] plays a similar role
    as [[54](#bib.bib54)] to simplify the searching space of the content delivery
    optimization model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: For resource allocation problem, the authors of [[96](#bib.bib96)] model it
    as linear sum assignment problems then utilize CNN and FNN to solve the model.
    The idea is extended in [[56](#bib.bib56)] and [[57](#bib.bib57)], where the authors
    consider where to cache problem among potential CRs and content delivery jointly,
    which is modeled as MILP with the aim of balancing caching and transmission cost
    by considering the user mobility, space utilization and bandwidth limitations.
    The cache allocation is viewed as multi-label classification problem and is decomposed
    into several independent sub-problems, where each one correlates with a CNN to
    predict assignment. The input of CNN is a grey-scale image which combines the
    information of user mobility, space and link utilization level. In [[56](#bib.bib56)],
    a hill climbing local search algorithm is provided to improve the performance
    of CNN while in [[57](#bib.bib57)], the prediction of CNN is used to feed a smaller
    MILP model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: For these above works [[54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57), [96](#bib.bib96)], the FNN or CNN input is extracted from the
    optimization model. The work in [[97](#bib.bib97)] trains a CNN via original graph
    instead of parameters matrix/image, which makes the process human recognizable
    and interpretable. Though the authors take traveling salesman problem not data
    caching as an example, the method can be viewed as a potential research direction.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: In [[58](#bib.bib58)], an ILP model is proposed to minimize the backhaul video-data
    type load by determining the portion of cached content in BSs. Considering the
    fact that the mobile users covered by a BS change frequently, therefore predicting
    user preference is unnecessary. Instead, the authors concentrate on the popular
    content in general. At the beginning, a 3D CNN is introduced to extract spatio-temporal
    features of videos. The popularity of new contents without historical information
    is determined via comparing similar video features. The authors of [[59](#bib.bib59)]
    also considers the spatio-temporal features among visiting contents in a mobile
    bus WiFI environment. By exploiting the previous 9 days collecting data, the content
    that the user may visit on the last day and corresponding visiting frequency can
    be forecast. The social property is taken into account in [[60](#bib.bib60)].
    By observing users interests on tweets during 2016 U.S. election, a CNN based
    predicted model can foresee the content category that is most likely to be requested.
    Such kind of content would be cached in MBSs and FBSs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The work of [[61](#bib.bib61)] examines the role of DNN in caching from another
    aspect. The authors propose a FNN to predict content popularity as a regression
    problem. The results show that FNN outperforms RNN, though the later is believed
    to be effective to solve sequential predictions. Moreover, replacing the FNN by
    a linear estimator does not devalue the performance significantly. The author
    provides explanation that FNN would work better than linear predictor in the case
    of incomplete information, and RNN has more advantages to model the popularity
    prediction as a classification rather than a regression problem.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 RNN
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Considering RNN is superior in dealing with sequential tasks, the work [[64](#bib.bib64)]
    applies a bidirectional RNN for online content popularity prediction in mobile
    edge network. Simple RNN’s output depends on previous and current storage, but
    the bidirectional can also take future information into account. The forecast
    model consists three blocks cascadingly: a CNN reads user requests and extract
    features; bidirectional LTSM learns association of requests over time step; FNN
    is added in the end to improve the prediction performance. Then content eviction
    is based on the popularity prediction.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[62](#bib.bib62)] utilize ESN to predict both content request
    distribution and end user mobility pattern. The user’s preference is viewed as
    context which links with personal information combining gender, age, job, location,
    etc.. For the request prediction, the input of ESN is user’s information vector
    and the output represent the probability distribution of content. For mobility
    prediction, the input includes historical and present user’s location and the
    output is the expected position for next time duration. Eventually, the prediction
    influences the caching content decisions in BBUs and RRHs for the purpose of minimizing
    traffic load and delay in CRAN. The authors extend their work in [[63](#bib.bib63)]
    by introducing conceptor-based ESN which can split users’ context into different
    patterns and learn them independently. Therefore a more accurate prediction is
    achieved.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In [[65](#bib.bib65)], a caching decision policy named PA-Cache is proposed
    to predict time-variant video popularity for cache eviction when the space is
    full. The temporal content popularity is exploited by attaching every hidden layer
    representation of RNN to an output regression. In order to improve the accuracy,
    hedge backpropagation is introduced during training process which decides when
    and how to adapt the depth of the DNN in an evolving manner. Similarly, the work
    in [[66](#bib.bib66)] also considers caching replacement of video content. A deep
    LSTM network is utilized for popularity prediction consisting of stacking multiple
    LSTM layers and one softmax layer, where the input of the network is request sequence
    data (device, timestamp, location, title of video) without any prepossessing and
    the output is estimated content popularity. Another work concentrates on prediction
    and interactions between user mobility and content popularity can be found [[67](#bib.bib67)].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The work of [[68](#bib.bib68)] recognizes the popularity prediction as a seq2seq
    modeling problem and proposes LSTM Encoder-Decoder model. The input vector consists
    of past probabilities where each vectors are calculated during a predefined time
    window. In [[69](#bib.bib69)], the authors focus on caching content delivery with
    the aim of minimizing BSs to cover all requested users, i.e. set cover problem,
    via coded caching. Unlike [[68](#bib.bib68)], an auto encoder is introduced in
    coded caching stage for file conversion to reduce transmission load. In addition,
    a RNN model is employed to select BSs for broadcasting.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The paper [[98](#bib.bib98)] shows the potential of RNN in solving where to
    cache problem. In [[98](#bib.bib98)], a task allocation model is formulated as
    a knapsack problem and the decision variables represent the task is processed
    locally in mobile devices (MDs) or remotely in edge servers (ESs). The authors
    design a multi-pointer network structure of 3 RNNs, where 2 encoders encode MDs
    and ESs respectively, 1 decoder demonstrates ES and MD pairing. Considering the
    similarity of where to cache optimization model and knapsack problem, the multi-pointer
    network can be transferred for caching location decision after according parameter
    modifications.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Auto Encoder
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally, auto encoder is utilized to learn efficient representation or extract
    features of raw data in an unsupervised manner. The work in [[70](#bib.bib70)]
    considers the cache replacement in wireless sensor network (WSN) based on content
    popularity. Considering sparse auto encoder (SAE) can extract representative expression
    of input data, the authors employ a SAE followed by a classifier where the input
    contains collecting user content requests and the output represents the contents
    popularity level. The authors also think about the implementation in a distributed
    way by SDN/NFV technical, i.e. the input layer is deployed on sink node, while
    the rest layers are implemented on the main controller. A related work applying
    auto encoder in 5G network proactive caching can be found in [[71](#bib.bib71)].
    In [[72](#bib.bib72)], two auto encoders are utilized for extracting the features
    of users and content respectively. Then the extracted information is explored
    to estimate popularity at the core network. Similarly, the auto encoder in [[73](#bib.bib73)]
    is for spatio-temporal popularity features extraction and auto encoders work collaboratively
    in [[75](#bib.bib75)] to predict top K popular videos.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 DRL
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The work in [[78](#bib.bib78)] focuses on the cooperative caching policy at
    FBSs with maximum distance separable coding in ultra dense networks. A value-based
    model is utilized to determine caching categories and the content quantity at
    FBSs during off peak duration. The authors in [[79](#bib.bib79)] study the problem
    of caching 360°videos and virtual viewports in FBSs with unknown content popularity.
    The virtual viewport represents the most popular tiles of a 360°video over users’
    population. A DQN is introduced to decide which tiles of a video to be hosted
    and in which quality. Additionally, [[80](#bib.bib80)] employs DQN for content
    eviction decision offering a satisfactory quality of experience and [[81](#bib.bib81)]
    is for the purpose of minimizing energy consumption. In [[82](#bib.bib82)], the
    authors also apply DQN to decide cache eviction in a single BS. Moreover, the
    critic is generated with stacking LSTM and FNN to evaluate Q value and an external
    memory is added for recording learned knowledge. For the purpose of improving
    the prediction accuracy, the Q value update is determined by the similarity of
    estimated value of critic and recording information in the external memory, instead
    of critic domination. The paper [[84](#bib.bib84)] puts forth DQN a two-level
    network caching, where a parent node links with multiple leaf nodes to cache content
    instead of a single BS. In [[76](#bib.bib76)], a DRL framework with Wolpertinger
    architecture [[99](#bib.bib99)] is presented for content caching at BSs. The Wolpertinger
    architecture is based on actor-critic model and performs efficiently in large
    discrete action space. [[76](#bib.bib76)] employs two FNNs working as actor and
    critic respectively, where the former determines requested content is cached or
    not and the later estimates the reward. The whole framework consists two phases:
    in offline phase, these two FNNs are trained in supervised learning; in online
    phase, the critic and actor update via the interaction with environment. The authors
    extend their work to a multi agent actor-critic model for decentralized cooperative
    caching at multiple BSs [[77](#bib.bib77)]. In [[83](#bib.bib83)], an actor-critic
    model is used for solving cache replacement problem, which balance the data freshness
    and communication cost. The aforementioned papers put attention on the network
    performance while ignore the influence of caching on information processing and
    resource consumption. Therefore, authors of [[85](#bib.bib85)] design cache policy
    considering both network performance during content transmission and processing
    efficiency during data consumption. A DQN is employed to determine the number
    of chunks of the requested file to be updated. The paper [[86](#bib.bib86)] investigates
    a joint cache replacement and power allocation optimization problem to minimize
    latency in a downlink F-RAN. A DQN is proposed for finding a suboptimal solution.
    Though [[87](#bib.bib87)] is regarded as solving what to cache problem like [[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86)],
    the reinforcement learning approach plays a different role. In [[87](#bib.bib87)],
    a DNN is utilized for content popularity prediction and then a RL is used for
    DNN hyperparameters tuning instead of determining caching content. Therefore the
    action space consists of choosing model architectures (i.e. CNN, LSTM, etc.),
    number of layers and layer configurations.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: In [[88](#bib.bib88)], the authors generate an optimization model with the aim
    of maximizing network operator’s utility in mobile social networks under the framework
    of mobile edge computing, in network caching and D2D communications (3C). The
    trust value which if estimated through social relationships among users are also
    considered. Then a DQN model is utilized for solving optimization problem, including
    determine video provider and subscriber association, video transcoding offloading
    and the video cache allocation for video providers.. The DQN employs two CNNs
    for training process, where one generates target Q value and the other is for
    estimated Q value. Unlike the conventional DQN, the authors in [[88](#bib.bib88)]
    introduces a dueling structure, i.e. the Q value is not computed in the final
    fully connected layer, but is decomposed into two components and use the summary
    as estimated Q value, which helps achieve a more robust result. The authors also
    consider utilizing dueling DQN model in different scenarios like cache-enabled
    opportunistic interference alignment [[89](#bib.bib89)] and orchestrating 3C in
    vehicular network [[90](#bib.bib90)]. The work [[91](#bib.bib91)] provides a DDPG
    model to cope with continuous valued control decision for 3C in vehicular edge
    networks, which is combined with the idea of DQN and actor-critic model. The DDPG
    structure can be divided into two parts as DQN, one is for estimated Q value and
    the other for target Q value. Each part consists of two DNNs, which play the role
    of actor and critic respectively. The critic updates its parameters like DQN while
    the actor learns policy via deterministic policy gradient approach. The proposed
    DRL is used for deciding content caching/replacement, vehicle organization and
    bandwidth resource assignment on different duration.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The paper [[92](#bib.bib92)] provides an optimization model which takes what
    to cache and content delivery into consideration in the fog-enabled IoT network
    in order to minimize service latency. Since the wireless signals and user requests
    are stochastic, a actor-critic model is engaged where the actor makes decision
    for requesting contents while critic estimates the reward. Specially, the action
    space $S$ consists of decision variables and reward function is a variant of the
    objective function. A caching replacement strategy and dynamic multicast scheduling
    strategy are studied in [[93](#bib.bib93)]. In order to get a suboptimal result,
    an auto encoder is used to approximate the state. Further, a weighted double DQN
    scheme is utilized for avoiding overestimation of Q value. [[94](#bib.bib94)]
    applies a RNN to predict content popularity by collecting historical requests
    and the output represents the popularity in the near future. Then the prediction
    is employed for cooperative caching and computation offloading among MEC servers,
    which is modelled as a ILP problem. For the purpose of solving it efficiently,
    a multi-agent DQN is applied where each user is viewed as an agent. The action
    space consists of task local computing and offloading decision as well as local
    caching and cooperative caching determination. The reward is measured by accumulated
    latency. The agent choose its own action based on current state without cooperation.
    The where to cache, what to cache and content delivery decision of D2D network
    are jointly modelled in [[95](#bib.bib95)]. Two RNNs, ESN and LSTM, are considered
    to predict mobile users’ location and requested content popularity. Then the prediction
    result is used for determining content categories and cache locations. The content
    delivery is formulated as the actor-critic based DRL framework. The state spaces
    include CSI, transmission distances and communication power between requested
    user and other available candidates. The function of DRL is determining the communication
    link among users with the aim of minimizing power consumption and content delay.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: We notice that most papers prefer to use value-based model (critic) and value-policy-based
    (actor-critic) model in DRL framework, but rare paper considers only policy-based
    model to solve data caching problem. One proper reason is that the search space
    of caching problem is enormous so policy-based model is easier to fall into local
    optimal solution, resulting in poor performance. Though the value-based model
    is less stable, some variant structures are utilized like Double DQN in [[93](#bib.bib93)]
    to avoid value overestimation and dueling DQN in [[88](#bib.bib88), [89](#bib.bib89),
    [90](#bib.bib90)] to improve robust.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 5 Research Challenges and Future Directions
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A serious of open issues on content caching and potential research directions
    are discussed in this section. We first extend the idea of content caching to
    virtual network function chain since caching can be viewed as a specific network
    function. Then we consider the caching for augmented reality applications. Moreover,
    we notice that the cache dimensioning has not been covered yet by DL methods.
    Finally, we debate the addition cost introduced by DL.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Caching as a Virtual Network Function Chain
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of Network Function Virtualization (NFV) has been firstly discussed
    and proposed within the realms of the European Telecommunication Standardization
    Institute (ETSI)¹¹1Network Functions Virtualisation, An Introduction, Benefits,
    Enablers, Challenges and Call for Action, ETSI, 2012 https://portal.etsi.org/NFV/NFV_White_Paper.pdf.
    The rational is to facilitate the dynamic provisioning of network services through
    virtualization technologies to decouple the service creation process form the
    underlying hardware. The framework allows network services to be implemented by
    a specific chaining and ordering of a set of functions which can be implemented
    either on a more traditional dedicated hardware which in this this case are called
    Physical Network Functions (PNFs), or alternatively as Virtual Network Functions
    (VNFs) which is a software running on top of virtualized general-purpose hardware.
    The decoupling between the hardware and the software is one of the important considerations
    the other – equally important – is that a virtualized service lend itself naturally
    to a dynamic programmable service creation where VNF resources can be deployed
    as required. Hence, edge cloud and network resource usage can be adapted to the
    instantaneous user demand whilst avoiding a more static over-provisioned configurations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Within that framework, the incoming network service requests include the specification
    of the service function chain that need to be created in the form of an ordered
    sequence of VNFs. For example different type of VNFs such as a firewall or a NAT
    mechanism need to be visited in a specific order. In such constructed service
    chain each independent VNF requires specific underlying resources in terms for
    example of CPU cycles and/or memory.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Under this framework, caching of popular content can be considered as a specialized
    VNF chain function since inevitably delivery of the cached popular content to
    users will require a set of other functions to be supported related to security,
    optimization of the content etc. However, the issue of data caching and VNF chaining
    have evolved rather independently in the literature and the issue on how to optimize
    data caching when seeing it as part of VNF chain is still an interesting open
    ended issue.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Caching for Mobile Augmented Reality (MAR) applications and Digital Twins
    (DTs)
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mobile augmented reality (MAR) applications can be considered as a way to augment
    the physical real-world environment with artificial computer-based generated information
    and is an area that has received significant research attention recently. In order
    to successfully superimpose different digital object in the physical world MAR
    applications include several computationally and storage complex concepts such
    as image recognition, mobile camera calibration, and also the use of advanced
    2D and 3D graphics rendering. These functionalities are highly computationally
    intensive and as such require support from an edge cloud, in addition the virtual
    objects to be embedded in the physical world are expected to be proactively cached
    closer to the end user so that latency is minimized. Ultra low latency in these
    type of applications is of paramount importance so that to provide a photorealistic
    embedding of virtual objects in the video view of the end user. However, since
    computational and augmented reality objects need to be readily available, the
    caching of those objects should be considered in conjunction with the computational
    capabilities of he edge cloud. In addition to the above when MAR is considered
    under the lenses of an NFV environment the application might inherently require
    access to some VNFs and therefore the above discussion on VNF chaining for MAR
    applications is also valid in this case.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Recently the concept of Digital Twin (DT) [[100](#bib.bib100)], [[101](#bib.bib101)]
    has received significant research attention due to the plethora of applications
    ranging from industrial manufacturing and health to smart cities. In a nutshell,
    a DT can be defined as an accurate digital replica of a real world object across
    multiple granularity levels; and this real world object could be a machine, a
    robot or an industrial process or (sub) system. By reflecting the physical status
    of the system under consideration in a virtual space open up a plethora of optimization,
    prediction, fault tolerance and automation process that cannot be done using solely
    the physical object. At the core of DT applications is the requirement of stringent
    two–way real time communication between the digital replica and the physical object.
    This requirement inevitably require support from edge clouds to minimize latency
    and efficient storage and computational resources including caching. In that setting,
    the use of the aforementioned deep learning technologies will have a key role
    to play in order to provide high quality real time decision making to avoid misalignment
    between the digital replica of the physical object under consideration. Efficient
    machine-to-DT connectivity would require capabilities similar to the above mentioned
    augmented reality application but due to the continuous real-time control-loop
    operation DTs will require a complete new set of network optimization capabilities
    and in that frontier efficient caching and data-driven techniques will have a
    central role to play. Hence, as the research regarding the inter-play between
    low latency communications and DTs is still in embryonic stage there is significant
    scope in the investigation of suitable data driven deep learning techniques to
    be utilized for distributed allocation of caching and computing resources.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Deep Learning for Cache Dimensioning
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As introduced in Section [2](#S2 "2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network"), cache dimensioning explores the appropriate
    cache size allocation for content host such as CRs and BSs. Disappointingly, there
    is rare paper applies DL on cache dimensioning decisions. One proper reason is
    lack of training data set in contrast to content popular prediction, where we
    have historical user request log to train a DNN. In addition, the caching size
    allocation affects the network performance and economic investment. Recently,
    network slicing is identified as an important tool to enable 5G to provide multi-services
    with diverse characteristics. The slice is established on physical infrastructure
    including network storage. Therefore it is a very interesting topic to consider
    the allocation of the memory space to support content caching and other storage
    services, which guarantees QoE and satisfies task requirements. Furthermore, for
    the case lack of training data set, DRL can be viewed as a promising technology
    to configure slicing settings as well as cache dimensioning. For the action space
    designing, it can be either discrete by setting storage levels, or continuous
    which is allocate the memory space directly. However, there are requirements to
    design caching-enabled network slicing model especially for dynamic allocation
    as well as associated DRL framework including state space, detailed action space,
    reward function and agent structure.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 The Cost of Deep Learning
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though the application of DL brings performance efficiency for caching policy,
    additional cost introduced by DL is unneglected, since training and deploying
    DL model require not only network resources but also time duration. Naturally
    there is a trade-off between the cost which DL-assisted caching policy saved and
    the consumption which supports DL itself running, which indicates the trading
    with DL results in either profit, loss, or break even. Therefore, where and when
    to apply DL should be carefully investigated. In addition, for the purpose of
    reducing resource consumption and accelerating training process, some knowledge
    transfer methods like transfer learning [[102](#bib.bib102)] can be utilized,
    which can transform the knowledge already learnt from the source domain to a relevant
    target domain.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article presents a comprehensive study for the application of deep learning
    methods in the area of content caching. Particularly, the data caching is divided
    into two classifications according to the caching location of edge network. Each
    category contains where to cache, what to cache, cache dimensioning and content
    delivery. Then we introduce typical DNN methods which are categorised via training
    process into supervised learning, unsupervised learning and RL. Further, this
    paper critically compares and analyzes state-of-the-art papers on parameters,
    such as DL methods employed, the caching problems solved and the objective of
    applying DL. The challenges and research directions of DL on caching is also examined
    on the topic of extending caching to VNF chains, the application of caching for
    MAR as well as DTs, DL for cache size allocation and the additional cost of employing
    DL. Undoubtedly, DL is playing a significant role in 5G and beyond. We hope this
    paper will increase discussions and interests on DL for caching policy design
    and relevant applications, which will advance future network communications.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] X. Wang, M. Chen, Z. Han, D. O. Wu, and T. T. Kwon. Toss: Traffic offloading
    by social network service-based opportunistic sharing in mobile social networks.
    In IEEE INFOCOM 2014 - IEEE Conference on Computer Communications, pages 2346–2354,
    2014.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing
    of deep neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295–2329,
    2017.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Yaohua Sun, Mugen Peng, Yangcheng Zhou, Yuzhe Huang, and Shiwen Mao. Application
    of machine learning in wireless networks: Key techniques and open issues. IEEE
    Communications Surveys & Tutorials, 21(4):3072–3108, 2019.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Adita Kulkarni and Anand Seetharam. Model and machine learning based caching
    and routing algorithms for cache-enabled networks. arXiv preprint arXiv:2004.06787,
    2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Junaid Shuja, Kashif Bilal, Eisa Alanazi, Waleed Alasmary, and Abdulaziz
    Alashaikh. Applying machine learning techniques for caching in edge networks:
    A comprehensive survey. arXiv preprint arXiv:2006.16864, 2020.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Stephen ANOKYE, SEID Mohammed, and SUN Guolin. A survey on machine learning
    based proactive caching. ZTE Communications, 17(4):46–55, 2020.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Mingzhe Chen, Ursula Challita, Walid Saad, Changchuan Yin, and Mérouane
    Debbah. Artificial neural networks-based machine learning for wireless networks:
    A tutorial. IEEE Communications Surveys & Tutorials, 21(4):3039–3071, 2019.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang,
    Ying-Chang Liang, and Dong In Kim. Applications of deep reinforcement learning
    in communications and networking: A survey. IEEE Communications Surveys & Tutorials,
    21(4):3133–3174, 2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Xiaofei Wang, Yiwen Han, Victor CM Leung, Dusit Niyato, Xueqiang Yan, and
    Xu Chen. Convergence of edge computing and deep learning: A comprehensive survey.
    IEEE Communications Surveys & Tutorials, 22(2):869–904, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, and Lanyu Xu. Edge computing:
    Vision and challenges. IEEE internet of things journal, 3(5):637–646, 2016.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Mugen Peng, Yaohua Sun, Xuelong Li, Zhendong Mao, and Chonggang Wang.
    Recent advances in cloud radio access networks: System architectures, key techniques,
    and open issues. IEEE Communications Surveys & Tutorials, 18(3):2282–2308, 2016.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Georgios S Paschos, George Iosifidis, Meixia Tao, Don Towsley, and Giuseppe
    Caire. The role of caching in future communication systems and networks. IEEE
    Journal on Selected Areas in Communications, 36(6):1111–1125, 2018.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Siyang Shan, Chunyan Feng, Tiankui Zhang, and Jonathan Loo. Proactive
    caching placement for arbitrary topology with multi-hop forwarding in icn. IEEE
    Access, 7:149117–149131, 2019.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Yantong Wang, Gao Zheng, and Vasilis Friderikos. Proactive caching in
    mobile networks with delay guarantees. In ICC 2019-2019 IEEE International Conference
    on Communications (ICC), pages 1–6\. IEEE, 2019.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Chao Fang, F Richard Yu, Tao Huang, Jiang Liu, and Yunjie Liu. An energy-efficient
    distributed in-network caching scheme for green content-centric networks. Computer
    Networks, 78:119–129, 2015.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Jagruti Sahoo, Mohammad A Salahuddin, Roch Glitho, Halima Elbiaze, and
    Wessam Ajib. A survey on replica server placement algorithms for content delivery
    networks. IEEE Communications Surveys & Tutorials, 19(2):1002–1026, 2016.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Asif Kabir, Gohar Rehman, Syed Mushhad Gilani, Edvin J Kitindi, Zain Ul Abidin Jaffri,
    and Khurrum Mustafa Abbasi. The role of caching in next generation cellular networks:
    A survey and research outlook. Transactions on Emerging Telecommunications Technologies,
    31(2):e3702, 2020.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Stefano Traverso, Mohamed Ahmed, Michele Garetto, Paolo Giaccone, Emilio
    Leonardi, and Saverio Niccolini. Temporal locality in today’s content caching:
    why it matters and how to model it. ACM SIGCOMM Computer Communication Review,
    43(5):5–12, 2013.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Ali Dabirmoghaddam, Maziar Mirzazad Barijough, and JJ Garcia-Luna-Aceves.
    Understanding optimal caching and opportunistic caching at" the edge" of information-centric
    networks. In Proceedings of the 1st ACM conference on information-centric networking,
    pages 47–56, 2014.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Yue Shi, Martha Larson, and Alan Hanjalic. Collaborative filtering beyond
    the user-item matrix: A survey of the state of the art and future challenges.
    ACM Computing Surveys (CSUR), 47(1):1–45, 2014.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Jaeyeon Jung, Arthur W Berger, and Hari Balakrishnan. Modeling ttl-based
    internet caches. In IEEE INFOCOM 2003\. Twenty-second Annual Joint Conference
    of the IEEE Computer and Communications Societies (IEEE Cat. No. 03CH37428), volume 1,
    pages 417–426\. IEEE, 2003.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Nicaise Choungmo Fofack, Philippe Nain, Giovanni Neglia, and Don Towsley.
    Performance evaluation of hierarchical ttl-based cache networks. Computer Networks,
    65:212–231, 2014.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Dario Rossi and Giuseppe Rossini. On sizing ccn content stores by exploiting
    topological information. In 2012 Proceedings IEEE INFOCOM Workshops, pages 280–285.
    IEEE, 2012.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Yuemei Xu, Yang Li, Tao Lin, Zihou Wang, Wenjia Niu, Hui Tang, and Song
    Ci. A novel cache size optimization scheme based on manifold learning in content
    centric networking. Journal of Network and Computer Applications, 37:273–281,
    2014.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Georgios Paschos, George Iosifidis, and Giuseppe Caire. Cache optimization
    models and algorithms. arXiv preprint arXiv:1912.12339, 2019.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Van Jacobson, Diana K Smetters, James D Thornton, Michael F Plass, Nicholas H
    Briggs, and Rebecca L Braynard. Networking named content. In Proceedings of the
    5th international conference on Emerging networking experiments and technologies,
    pages 1–12, 2009.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Liang Wang, Suzan Bayhan, Jörg Ott, Jussi Kangasharju, and Jon Crowcroft.
    Understanding scoped-flooding for content discovery and caching in content networks.
    IEEE Journal on Selected Areas in Communications, 36(8):1887–1900, 2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Liying Li, Guodong Zhao, and Rick S Blum. A survey of caching techniques
    in cellular networks: Research issues and challenges in content placement and
    delivery strategies. IEEE Communications Surveys & Tutorials, 20(3):1710–1732,
    2018.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Ammar Gharaibeh, Abdallah Khreishah, Bo Ji, and Moussa Ayyash. A provably
    efficient online collaborative caching algorithm for multicell-coordinated systems.
    IEEE Transactions on Mobile Computing, 15(8):1863–1876, 2015.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Negin Golrezaei, Andreas F Molisch, Alexandros G Dimakis, and Giuseppe
    Caire. Femtocaching and device-to-device collaboration: A new architecture for
    wireless video distribution. IEEE Communications Magazine, 51(4):142–149, 2013.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Mehrnaz Afshang, Harpreet S Dhillon, and Peter Han Joo Chong. Fundamentals
    of cluster-centric content placement in cache-enabled device-to-device networks.
    IEEE Transactions on Communications, 64(6):2511–2526, 2016.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Zhuoqun Chen, Yangyang Liu, Bo Zhou, and Meixia Tao. Caching incentive
    design in wireless d2d networks: A stackelberg game approach. In 2016 IEEE International
    Conference on Communications (ICC), pages 1–6\. IEEE, 2016.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Zhun Ye, Cunhua Pan, Huiling Zhu, and Jiangzhou Wang. Tradeoff caching
    strategy of the outage probability and fronthaul usage in a cloud-ran. IEEE Transactions
    on Vehicular Technology, 67(7):6383–6397, 2018.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Dewang Ren, Xiaolin Gui, Kaiyuan Zhang, and Jie Wu. Mobility-aware traffic
    offloading via cooperative coded edge caching. IEEE Access, 8:43427–43442, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Jaeyoung Song and Wan Choi. Mobility-aware content placement for device-to-device
    caching systems. IEEE Transactions on Wireless Communications, 18(7):3658–3668,
    2019.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Dong Liu, Binqiang Chen, Chenyang Yang, and Andreas F Molisch. Caching
    at the wireless edge: design aspects, challenges, and future directions. IEEE
    Communications Magazine, 54(9):22–28, 2016.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Xi Peng, Jun Zhang, SH Song, and Khaled B Letaief. Cache size allocation
    in backhaul limited wireless networks. In 2016 IEEE International Conference on
    Communications (ICC), pages 1–6\. IEEE, 2016.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] An Liu and Vincent KN Lau. How much cache is needed to achieve linear
    capacity scaling in backhaul-limited dense wireless networks? IEEE/ACM Transactions
    on Networking, 25(1):179–188, 2016.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Jaeyoung Song and Wan Choi. Minimum cache size and backhaul capacity for
    cache-enabled small cell networks. IEEE Wireless Communications Letters, 7(4):490–493,
    2017.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Bo Zhou, Ying Cui, and Meixia Tao. Optimal dynamic multicast scheduling
    for cache-enabled content-centric wireless networks. IEEE Transactions on Communications,
    65(7):2956–2970, 2017.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Mohammad Ali Maddah-Ali and Urs Niesen. Fundamental limits of caching.
    IEEE Transactions on Information Theory, 60(5):2856–2867, 2014.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Vu Nguyen Ha, Long Bao Le, et al. Coordinated multipoint transmission
    design for cloud-rans with limited fronthaul capacity constraints. IEEE Transactions
    on Vehicular Technology, 65(9):7432–7447, 2015.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT
    Press, 2016. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431–3440, 2015.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Hava T Siegelmann and Eduardo D Sontag. Turing computability with neural
    nets. Applied Mathematics Letters, 4(6):77–80, 1991.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks.
    In Advances in neural information processing systems, pages 2692–2700, 2015.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
    Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,
    et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533,
    2015.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning
    with double q-learning. In Thirtieth AAAI conference on artificial intelligence,
    2016.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and
    Nando Freitas. Dueling network architectures for deep reinforcement learning.
    In International conference on machine learning, pages 1995–2003, 2016.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
    Moritz. Trust region policy optimization. In International conference on machine
    learning, pages 1889–1897, 2015.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In International conference on machine learning,
    pages 1928–1937, 2016.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Lei Lei, Yaxiong Yuan, Thang X Vu, Symeon Chatzinotas, and Björn Ottersten.
    Learning-based resource allocation: Efficient content delivery enabled by convolutional
    neural network. In 2019 IEEE 20th International Workshop on Signal Processing
    Advances in Wireless Communications (SPAWC), pages 1–5\. IEEE, 2019.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Lei Lei, Lei You, Gaoyang Dai, Thang Xuan Vu, Di Yuan, and Symeon Chatzinotas.
    A deep learning approach for optimizing content delivering in cache-enabled hetnet.
    In 2017 international symposium on wireless communication systems (ISWCS), pages
    449–453\. IEEE, 2017.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Yantong Wang and Vasilis Friderikos. Caching as an image characterization
    problem using deep convolutional neural networks. arXiv preprint arXiv:1907.07263,
    2019.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Yantong Wang and Vasilis Friderikos. Network orchestration in mobile networks
    via a synergy of model-driven and ai-based techniques. arXiv preprint arXiv:2004.00660,
    2020.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Khai Nguyen Doan, Thang Van Nguyen, Tony QS Quek, and Hyundong Shin. Content-aware
    proactive caching for backhaul offloading in cellular network. IEEE Transactions
    on Wireless Communications, 17(5):3128–3140, 2018.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Zhou Qin, Yikun Xian, and Desheng Zhang. A neural networks based caching
    scheme for mobile edge networks. In Proceedings of the 17th Conference on Embedded
    Networked Sensor Systems, pages 408–409, 2019.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Kuo Chun Tsai, Li Wang, and Zhu Han. Mobile social media networks caching
    with convolutional neural network. In 2018 IEEE wireless communications and networking
    conference workshops (WCNCW), pages 83–88\. IEEE, 2018.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Vladyslav Fedchenko, Giovanni Neglia, and Bruno Ribeiro. Feedforward neural
    networks for caching: n enough or too much? ACM SIGMETRICS Performance Evaluation
    Review, 46(3):139–142, 2019.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Mingzhe Chen, Walid Saad, Changchuan Yin, and Mérouane Debbah. Echo state
    networks for proactive caching in cloud-based radio access networks with mobile
    users. IEEE Transactions on Wireless Communications, 16(6):3520–3535, 2017.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Mingzhe Chen, Mohammad Mozaffari, Walid Saad, Changchuan Yin, Mérouane
    Debbah, and Choong Seon Hong. Caching in the sky: Proactive deployment of cache-enabled
    unmanned aerial vehicles for optimized quality-of-experience. IEEE Journal on
    Selected Areas in Communications, 35(5):1046–1061, 2017.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Laha Ale, Ning Zhang, Huici Wu, Dajiang Chen, and Tao Han. Online proactive
    caching in mobile edge computing using bidirectional deep recurrent neural network.
    IEEE Internet of Things Journal, 6(3):5520–5530, 2019.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Qilin Fan, Jian Li, Xiuhua Li, Qiang He, Shu Fu, and Sen Wang. Pa-cache:
    Learning-based popularity-aware content caching in edge networks. arXiv preprint
    arXiv:2002.08805, 2020.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Cong Zhang, Haitian Pang, Jiangchuan Liu, Shizhi Tang, Ruixiao Zhang,
    Dan Wang, and Lifeng Sun. Toward edge-assisted video content intelligent caching
    with long short-term memory learning. IEEE access, 7:152832–152846, 2019.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Hanlin Mou, Yuhong Liu, and Li Wang. Lstm for mobility based content popularity
    prediction in wireless caching networks. In 2019 IEEE Globecom Workshops (GC Wkshps),
    pages 1–6\. IEEE, 2019.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Arvind Narayanan, Saurabh Verma, Eman Ramadan, Pariya Babaie, and Zhi-Li
    Zhang. Deepcache: A deep learning based framework for content caching. In Proceedings
    of the 2018 Workshop on Network Meets AI & ML, pages 48–53, 2018.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Zhengming Zhang, Yaru Zheng, Chunguo Li, Yongming Huang, and Luxi Yang.
    On the cover problem for coded caching in wireless networks via deep neural network.
    In 2019 IEEE Global Communications Conference (GLOBECOM), pages 1–6\. IEEE, 2019.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Fangyuan Lei, Jun Cai, Qingyun Dai, and Huimin Zhao. Deep learning based
    proactive caching for effective wsn-enabled vision applications. Complexity, 2019,
    2019.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] FangYuan Lei, QinYun Dai, Jun Cai, HuiMin Zhao, Xun Liu, and Yan Liu.
    A proactive caching strategy based on deep learning in epc of 5g. In International
    Conference on Brain Inspired Cognitive Systems, pages 738–747\. Springer, 2018.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Shailendra Rathore, Jung Hyun Ryu, Pradip Kumar Sharma, and Jong Hyuk
    Park. Deepcachnet: A proactive caching framework based on deep learning in cellular
    networks. IEEE Network, 33(3):130–138, 2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Wai-Xi Liu, Jie Zhang, Zhong-Wei Liang, Ling-Xi Peng, and Jun Cai. Content
    popularity prediction and caching for icn: A deep learning approach with sdn.
    IEEE access, 6:5075–5089, 2017.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Wei Li, Jun Wang, Guoyong Zhang, Li Li, Ze Dang, and Shaoqian Li. A reinforcement
    learning based smart cache strategy for cache-aided ultra-dense network. IEEE
    Access, 7:39390–39401, 2019.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Yu-Tai Lin, Chia-Cheng Yen, and Jia-Shung Wang. Video popularity prediction:
    An autoencoder approach with clustering. IEEE Access, 8:129285–129299, 2020.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Chen Zhong, M Cenk Gursoy, and Senem Velipasalar. A deep reinforcement
    learning-based framework for content caching. In 2018 52nd Annual Conference on
    Information Sciences and Systems (CISS), pages 1–6\. IEEE, 2018.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Chen Zhong, M Cenk Gursoy, and Senem Velipasalar. Deep reinforcement learning-based
    edge caching in wireless networks. IEEE Transactions on Cognitive Communications
    and Networking, 6(1):48–61, 2020.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Shen Gao, Peihao Dong, Zhiwen Pan, and Geoffrey Ye Li. Reinforcement learning
    based cooperative coded caching under dynamic popularities in ultra-dense networks.
    IEEE Transactions on Vehicular Technology, 69(5):5442–5456, 2020.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Pantelis Maniotis and Nikolaos Thomos. Viewport-aware deep reinforcement
    learning approach for 360$\^{o}$ video caching. arXiv preprint arXiv:2003.08473,
    2020.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Xiaoming He, Kun Wang, and Wenyao Xu. Qoe-driven content-centric caching
    with deep reinforcement learning in edge-enabled iot. IEEE Computational Intelligence
    Magazine, 14(4):12–20, 2019.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Jie Tang, Hengbin Tang, Xiuyin Zhang, Kanapathippillai Cumanan, Gaojie
    Chen, Kai-Kit Wong, and Jonathon A Chambers. Energy minimization in d2d-assisted
    cache-enabled internet of things: A deep reinforcement learning approach. IEEE
    Transactions on Industrial Informatics, 16(8):5412–5423, 2019.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Pingyang Wu, Jun Li, Long Shi, Ming Ding, Kui Cai, and Fuli Yang. Dynamic
    content update for wireless edge caching via deep reinforcement learning. IEEE
    Communications Letters, 23(10):1773–1777, 2019.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Hao Zhu, Yang Cao, Xiao Wei, Wei Wang, Tao Jiang, and Shi Jin. Caching
    transient data for internet of things: A deep reinforcement learning approach.
    IEEE Internet of Things Journal, 6(2):2074–2083, 2018.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Alireza Sadeghi, Gang Wang, and Georgios B Giannakis. Deep reinforcement
    learning for adaptive caching in hierarchical content delivery networks. IEEE
    Transactions on Cognitive Communications and Networking, 5(4):1024–1033, 2019.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Yimeng Wang, Yongbo Li, Tian Lan, and Vaneet Aggarwal. Deepchunk: Deep
    q-learning for chunk-based caching in wireless data processing networks. IEEE
    Transactions on Cognitive Communications and Networking, 5(4):1034–1045, 2019.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] GM Shafiqur Rahman, Mugen Peng, Shi Yan, and Tian Dang. Learning based
    joint cache and power allocation in fog radio access networks. IEEE Transactions
    on Vehicular Technology, 69(4):4401–4411, 2020.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Kyi Thar, Thant Zin Oo, Yan Kyaw Tun, Ki Tae Kim, Choong Seon Hong, et al.
    A deep learning model generation framework for virtualized multi-access edge cache
    management. IEEE Access, 7:62734–62749, 2019.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Ying He, Chengchao Liang, Richard Yu, and Zhu Han. Trust-based social
    networks with computing, caching and communications: A deep reinforcement learning
    approach. IEEE Transactions on Network Science and Engineering, 2018.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Ying He, Zheng Zhang, F Richard Yu, Nan Zhao, Hongxi Yin, Victor CM Leung,
    and Yanhua Zhang. Deep-reinforcement-learning-based optimization for cache-enabled
    opportunistic interference alignment wireless networks. IEEE Transactions on Vehicular
    Technology, 66(11):10433–10445, 2017.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Ying He, Nan Zhao, and Hongxi Yin. Integrated networking, caching, and
    computing for connected vehicles: A deep reinforcement learning approach. IEEE
    Transactions on Vehicular Technology, 67(1):44–55, 2017.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Guanhua Qiao, Supeng Leng, Sabita Maharjan, Yan Zhang, and Nirwan Ansari.
    Deep reinforcement learning for cooperative content caching in vehicular edge
    computing and networks. IEEE Internet of Things Journal, 7(1):247–257, 2019.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Yifei Wei, F Richard Yu, Mei Song, and Zhu Han. Joint optimization of
    caching, computing, and radio resources for fog-enabled iot using natural actor–critic
    deep reinforcement learning. IEEE Internet of Things Journal, 6(2):2061–2073,
    2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Zhengming Zhang, Hongyang Chen, Meng Hua, Chunguo Li, Yongming Huang,
    and Luxi Yang. Double coded caching in ultra dense networks: Caching and multicast
    scheduling via deep reinforcement learning. IEEE Transactions on Communications,
    68(2):1071–1086, 2019.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Shilu Li, Baogang Li, and Wei Zhao. Joint optimization of caching and
    computation in multi-server noma-mec system via reinforcement learning. IEEE Access,
    2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Lixin Li, Yang Xu, Jiaying Yin, Wei Liang, Xu Li, Wei Chen, and Zhu Han.
    Deep reinforcement learning approaches for content caching in cache-enabled d2d
    networks. IEEE Internet of Things Journal, 7(1):544–557, 2019.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Mengyuan Lee, Yuanhao Xiong, Guanding Yu, and Geoffrey Ye Li. Deep neural
    networks for linear sum assignment problems. IEEE Wireless Communications Letters,
    7(6):962–965, 2018.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Zhengxuan Ling, Xinyu Tao, Yu Zhang, and Xi Chen. Solving optimization
    problems through fully convolutional networks: An application to the traveling
    salesman problem. IEEE Transactions on Systems, Man, and Cybernetics: Systems,
    2020.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Qingmiao Jiang, Yuan Zhang, and Jinyao Yan. Neural combinatorial optimization
    for energy-efficient offloading in mobile edge computing. IEEE Access, 8:35077–35089,
    2020.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag,
    Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris,
    and Ben Coppin. Deep reinforcement learning in large discrete action spaces. arXiv
    preprint arXiv:1512.07679, 2015.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] A. El Saddik. Digital twins: The convergence of multimedia technologies.
    IEEE MultiMedia, 25(2):87–92, 2018.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] K. M. Alam and A. El Saddik. C2ps: A digital twin architecture reference
    model for the cloud-based cyber-physical systems. IEEE Access, 5:2050–2062, 2017.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer
    learning. Journal of Big data, 3(1):9, 2016.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
