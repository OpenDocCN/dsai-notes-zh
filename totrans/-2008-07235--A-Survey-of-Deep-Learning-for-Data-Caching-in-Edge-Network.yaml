- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:59:50'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2008.07235] A Survey of Deep Learning for Data Caching in Edge Network'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2008.07235](https://ar5iv.labs.arxiv.org/html/2008.07235)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Learning for Data Caching in Edge Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yantong Wang ¹ and Vasilis Friderikos ²
  prefs: []
  type: TYPE_NORMAL
- en: Center for Telecommunications Research
  prefs: []
  type: TYPE_NORMAL
- en: Department of Engineering, King’s College London
  prefs: []
  type: TYPE_NORMAL
- en: London, WC2R 2LS, U.K.
  prefs: []
  type: TYPE_NORMAL
- en: ¹ yantong.wang@kcl.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: ² vasilis.friderikos@kcl.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The concept of edge caching provision in emerging 5G and beyond mobile networks
    is a promising method to deal both with the traffic congestion problem in the
    core network as well as reducing latency to access popular content. In that respect
    end user demand for popular content can be satisfied by proactively caching it
    at the network edge, i.e, at close proximity to the users. In addition to model
    based caching schemes learning-based edge caching optimizations has recently attracted
    significant attention and the aim hereafter is to capture these recent advances
    for both model based and data driven techniques in the area of proactive caching.
    This paper summarizes the utilization of deep learning for data caching in edge
    network. We first outline the typical research topics in content caching and formulate
    a taxonomy based on network hierarchical structure. Then, a number of key types
    of deep learning algorithms are presented, ranging from supervised learning to
    unsupervised learning as well as reinforcement learning. Furthermore, a comparison
    of state-of-the-art literature is provided from the aspects of caching topics
    and deep learning methods. Finally, we discuss research challenges and future
    directions of applying deep learning for caching.
  prefs: []
  type: TYPE_NORMAL
- en: '*K*eywords deep learning  $\cdot$ content caching  $\cdot$ network optimization
     $\cdot$ edge network'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Undoubtedly, future 5G and beyond mobile communication networks will have to
    address stringent requirements of delivering popular content at ultra high speeds
    and low latency due to the proliferation of advanced mobile devices and data rich
    applications. In that ecosystem, edge-caching has received significant research
    attention over the last decade as an efficient technique to reduce delivery latency
    and network congestion especially during peak-traffic times or during unexpected
    network congestion episodes by bringing popular data closer to the end users.
    One of the main reasons of enabling edge caching in the network is to reduce the
    number of requests that traverse the access and core mobile network as well as
    reducing the load at the origin servers that would have to, otherwise, respond
    to all requests directly in absence of edge caching. In that case popular content
    and objects can be stored and served from edge locations, which are closer to
    the end users. This operation is also beneficial from the end user perspective
    since edge caching can dramatically reduce the overall latency to access the content
    and increase in the sense overall user experience. It is also important to note
    that the notion of popular content means that the requests of top 10% of video
    content on the Internet account for almost 80% of all traffic; which relates to
    multiple requests from different end users of the same content [[1](#bib.bib1)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, deep learning (DL) has attracted significant attention from both
    academia and industry and has been applied to diverse domains like self-driving,
    medical diagnosis, playing complex games such as Go [[2](#bib.bib2)]. DL has also
    made their way into communication areas [[3](#bib.bib3)]. In this paper, we pay
    attention to the application of DL in caching policy. Though there are some earlier
    surveys related to machine learning applications, they either focus on general
    machine learning techniques for caching [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)],
    or concentrate on overall wireless applications [[7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]. The work [[3](#bib.bib3)] provides a big picture of applying
    machine learning in wireless communications. In [[4](#bib.bib4)], the authors
    consider the machine learning on both caching and routing strategy. A comprehensive
    survey on machine learning applications for caching content in edge networks is
    provided in [[5](#bib.bib5)]. The researchers [[6](#bib.bib6)] provide a survey
    about machine learning on mobile edge caching and communication resources. On
    the other hand, [[7](#bib.bib7)] overviews how artificial neural networks can
    be employed for various wireless network problems. The authors in [[8](#bib.bib8)]
    detail a survey on deep reinforcement learning (DRL) for issues in communications
    and networking. [[9](#bib.bib9)] presents a comprehensive on deep learning applications
    and edge computing paradigm. Our work can be distinguished from the aforementioned
    papers based on the fact that we focus on the deep learning techniques on content
    caching and both wired and wireless caching are taken into account. Our main contributions
    are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We classify the content caching problem into Layer 1 Caching and Layer 2 Caching.
    Each layer caching consists of four tightly coupled subproblems: where to cache,
    what to cache, cache dimensioning and content delivery. Related researches are
    provided accordingly.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present the fundamentals of DL techniques which are widely used in content
    caching, such as convolutional neural network, recurrent neural network, actor-critic
    model based deep reinforcement learning, etc.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We analyze a broad range of state-of-the-art literature which use DL to content
    caching. These papers are compared based on the DL structure, layer caching coupled
    subproblems and the objective of DL in each scenarios. Then we discuss research
    challenges and potential directions for the utilization of DL in caching.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9493f7dba2702916c3de1d1b5d24b77e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Survey Architecture'
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this survey is organized as follows (as illustrated in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning for Data Caching in Edge
    Network") ). Section [2](#S2 "2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") presents the categories of content caching
    problem. Section [3](#S3 "3 Deep Learning Outline ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") reviews typical deep neural network structures.
    In Section [4](#S4 "4 Deep Learning for Data Caching ‣ A Survey of Deep Learning
    for Data Caching in Edge Network"), we list state-of-the-art DL-based caching
    strategies and their comparison. Section [5](#S5 "5 Research Challenges and Future
    Directions ‣ A Survey of Deep Learning for Data Caching in Edge Network") debates
    challenges as well as potential research directions. In the end, Section [6](#S6
    "6 Conclusions ‣ A Survey of Deep Learning for Data Caching in Edge Network")
    concludes this paper. For better readability, the abbreviations in this paper
    is listed as Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") shows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: List of Abbreviations.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Abbr. | Description | Abbr. | Description |'
  prefs: []
  type: TYPE_TB
- en: '| 3C | Computing, Caching and Communication | A3C | Asynchronous Advantage
    Actor-Critic |'
  prefs: []
  type: TYPE_TB
- en: '| BBU | Baseband Unit | CCN | Content-Centric Network |'
  prefs: []
  type: TYPE_TB
- en: '| CNN | Convolutional Neural Network | CoMP-JT | Coordinated Multi Point Joint
    Transmission |'
  prefs: []
  type: TYPE_TB
- en: '| CR | Content Router | C-RAN | Cloud-Radio Access Network |'
  prefs: []
  type: TYPE_TB
- en: '| CSI | Channel State Information | D2D | Device to Device |'
  prefs: []
  type: TYPE_TB
- en: '| DDPG | Deep Deterministic Policy Gradient | DL | Deep Learning |'
  prefs: []
  type: TYPE_TB
- en: '| DNN | Deep Neural Network | DQN | Deep Q Network |'
  prefs: []
  type: TYPE_TB
- en: '| DRL | Deep Reinforcement Learning | DT | Digital Twin |'
  prefs: []
  type: TYPE_TB
- en: '| ED | End Device | ES | Edge Server |'
  prefs: []
  type: TYPE_TB
- en: '| ETSI | European Telecommunication Standardization Institute |'
  prefs: []
  type: TYPE_TB
- en: '| ESN | Echo-State Network | FIFO | First In First Out |'
  prefs: []
  type: TYPE_TB
- en: '| FNN | Feedforward Neural Network | FBS | Femto Base Station |'
  prefs: []
  type: TYPE_TB
- en: '| ICN | Information-Centric Network | LFU | Least Frequently Used |'
  prefs: []
  type: TYPE_TB
- en: '| LP | Linear Programming | LRU | Least Recently Used |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM | Long Short-Term Memory | MAR | Mobile Augmented Reality |'
  prefs: []
  type: TYPE_TB
- en: '| MD | Mobile Device | MILP | Mixed Integer Linear Programming |'
  prefs: []
  type: TYPE_TB
- en: '| MBS | Macro Base Station | NFV | Network Function Virtualization |'
  prefs: []
  type: TYPE_TB
- en: '| PNF | Physical Network Function | PPO | Proximal Policy Optimization |'
  prefs: []
  type: TYPE_TB
- en: '| QoE | Quality of Experience | RL | Reinforcement Learning |'
  prefs: []
  type: TYPE_TB
- en: '| RNN | Recurrent Neural Network | RRH | Remote Radio Head |'
  prefs: []
  type: TYPE_TB
- en: '| SAE | Sparse Auto Encoder | SDN | Software Defined Network |'
  prefs: []
  type: TYPE_TB
- en: '| seq2seq | Sequence to Sequence | SNM | Shot Noise Model |'
  prefs: []
  type: TYPE_TB
- en: '| TRPO | Trust Region Policy Optimization | TTL | Time to Live |'
  prefs: []
  type: TYPE_TB
- en: '| VNF | Virtual Network Function | WSN | Wireless Sensor Network |'
  prefs: []
  type: TYPE_TB
- en: 2 Data Caching Review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The paradigm of data caching in edge networks is illustrated in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Data Caching Review ‣ A Survey of Deep Learning for Data Caching
    in Edge Network"). Similarity to [[10](#bib.bib10)], the scope of edge in this
    paper is along the path between end user and data server, which contains Content
    Router (CR), Macro Base Station (MSB), Femto Base Station (FSB) and End Device
    (ED). In the context of Cloud-Radio Access Network (C-RAN)[[11](#bib.bib11)],
    both baseband unit (BBU) and remote radio head (RRH) are considered as potential
    caching candidates to hosting content, where the BBUs are clustered as a BBU pool
    centrally and RRHs are deployed near BS’s antenna distributively. According to
    the hierarchical structure of edge network, the data caching is classified into
    two categories: Layer 1 Caching and Layer 2 Caching. In this section we illustrate
    the typical research topics in these two areas.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f5024e4294f7d10008b934d8a03e5fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Data Caching in Edge Network'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Layer 1 Caching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Layer 1 Caching, the popular content is considered to be hosted in CRs.
    In the context of Information-Centric Network (ICN), CR plays dual roles both
    as a typical router (i.e. data flow forwarding) and content store (i.e. local
    area data caching facility). Generally, the CR is connected via wired networks.
    Layer 1 Caching consists of four tightly coupled problems: where to cache, what
    to cache, cache dimensioning and content delivery[[12](#bib.bib12)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where to cache focuses on selecting the proper CRs to host the content. For
    instance, in Figure [2](#S2.F2 "Figure 2 ‣ 2 Data Caching Review ‣ A Survey of
    Deep Learning for Data Caching in Edge Network"), contents replicas can be placed
    in lower hierarchical level CRs, such as router B and C, as the mean of reducing
    transmission cost but with extra cost pay for hosting contents; reversely, consolidating
    caching in CR A can be adopted to saving caching cost at the expense of more transmission
    cost and has the risk of expiring end users’ delay requirement. Here the caching/hosting
    cost is the cost to deploy the content, which could be measured by space utilization,
    energy consumption or other metrics. The transmission cost represents the price
    for delivering the content from cached CR (or data server) to end user and is
    basically estimated via the number of hops. Where to cache problem usually has
    been modelled as a Mixed Integer Linear Programming (MILP):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathop{\min}_{\begin{subarray}{c}x\end{subarray}}\;$ |
    $\displaystyle c^{T}x$ |  | (1a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle Ax\leq b$ |  | (1b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle x\in\{0,1\}$ |  | (1c) |'
  prefs: []
  type: TYPE_TB
- en: '|  | or | $\displaystyle x\geq 0$ |  | (1d) |'
  prefs: []
  type: TYPE_TB
- en: where $x$ is the decision variable. Normally it is a binary variable indicating
    the CR assignment. In special cases, with the aim of modelling or linearization,
    some non-binary auxiliary variables are introduced as constraint ([1d](#S2.E1.4
    "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network")) shows. If taking caching a part of a file
    not the complete into consideration, the decision variable $x$ is a continuous
    variable representing the segments host in the CR, then constraint ([1c](#S2.E1.3
    "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network")) becomes $x\in[0,1]$ and MILP model turns to
    linear programming (LP). There are many work allocating contents via MILP with
    different objectives and limitations. The authors in [[13](#bib.bib13)] propose
    a model to minimize the user delay and load balancing level of CRs with the satisfaction
    of cache space. The work in [[14](#bib.bib14)] considers a trade-off between caching
    and transmission cost with cache space, link bandwidth and user latency constraints.
    In [[15](#bib.bib15)], an energy efficient optimization model is constructed consisting
    of caching energy and transport energy. [[16](#bib.bib16)] provides more details
    of mathematical model and related heuristic algorithms in caching deployment of
    wired networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'What to cache concentrates on selecting the proper contents in CRs for the
    purpose of maximizing the cache hit ratio. Via exploiting the statistical patterns
    of user requests, the popularity of requested information and user preference
    can be forecasted and play a very significant role in determining caching content.
    On the one hand, from the view of aggregated request contents, researchers propose
    many different models and algorithms for popularity estimation. One widely used
    model in web caching is the Zipf model based the assumption that the content popularity
    is static and each users’ request is independent[[17](#bib.bib17)]. However, this
    method fails to reflect the temporal and spatial correlations of the content,
    where the temporal correlation reflects the popularity varies over time and the
    spatial correlation represents the content preference is different on the geographical
    area and social cultural media. A temporal model named the shot noise model (SNM)
    is built in [[18](#bib.bib18)] which enables users to estimate the content popularity
    dynamically. Inspired by SNM, the work in [[19](#bib.bib19)] considers both spatial
    and temporal characteristics during caching decision. On the other hand, from
    the view of a specific end user during a certain period, caching his/her preference
    content (may not be the popular in network) can also help to reduce the traffic
    flow. Many approaches in recommendation systems can be applied in this case[[20](#bib.bib20)].
    Another aspect of what to cache problem is the designing of cache eviction strategies
    when storage space faces the risk of overflow. Depending on the life of caching
    contents, these policies can be divided into two categories roughly: one is like
    first in first out (FIFO), least frequently used (LFU), least recently used (LRU)
    and randomized replacement, the contents would not be removed until no more memory
    is available; the other one is called time to live (TTL) strategy, where the eviction
    happens once the related timer expires. [[21](#bib.bib21)] presents analytic model
    for hit ratio in TTL-based cache requested by independent and identically distributed
    flows. It worth noting that in [[21](#bib.bib21)], the TTL-based cache policy
    is used for the consistency of dynamic contents instead of contents replacement.
    In [[22](#bib.bib22)], the authors introduce a TTL model for cache eviction and
    the timer is reset once related content cache hit happens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cache dimensioning highlights how much storage space to be allocated. Benefit
    from the softwarization and virtualization technologies, the cache size in each
    CR or edge cloud can be managed in a more flexible and dynamical way, which makes
    the cache dimensioning decisions become an important feature in data caching.
    Technically, the cache hit ratio rises with the increasing of cache memory, and
    consequently eases the traffic congestion in the core network. However, excessive
    space allocation would waste the resource like energy to support the caching function.
    Hence there is a trade-off between cache size cost and network congestion. Economically,
    taking such scenario into consideration: a small content provider wants to rent
    service from a CDN provider such as Akamai or Huawei Cloud, and there is also
    a balance between investment saving and network performance. In [[23](#bib.bib23)],
    the proper cache size of individual CR in Content-Centric Network (CCN) is investigated
    via exploiting the network topology. In [[24](#bib.bib24)], the authors consider
    the effect of network traffic distribution and user behaviours when designing
    cache size.'
  prefs: []
  type: TYPE_NORMAL
- en: Content delivery considers how to transform the caching content to the requested
    user. The delivery traffic embraces single cache file downloading and video content
    steaming and the metrics for these two scenarios vary. Regarding file downloading,
    the content cannot be consumed until the delivery is completed. Therefore the
    downloading time of the entire file is viewed as a metric to reflect the quality
    of experience (QoE). For video steaming, especially for those large video splitted
    into several chunks, the delay limitation only works on the first chunk. In that
    case, delivering the first chunk in time and keep the smooth transmission of the
    rest chunks are the key aims [[25](#bib.bib25)]. Apart from those measuring metrics,
    another problem in content delivery is the routing policy. In CCN [[26](#bib.bib26)],
    one implementation of ICN architecture, employs a flooding-based name routing
    protocol to publish the request among cached CRs. On one hand, flooding strategy
    simplifies the designing complexity and reduce the maintaining cost particularly
    in an unstable scenario; on the other hand, it costly wastes bandwidth resources.
    In [[27](#bib.bib27)], the authors discuss the optimal radius in scoped flooding.
    The deliver route is often considered jointly with where to cache problem, in
    which the objective function ([1a](#S2.E1.1 "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data
    Caching Review ‣ A Survey of Deep Learning for Data Caching in Edge Network"))
    includes both deployment and routing cost.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Layer 2 Caching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Contrast to Layer 1 caching in wired connection, Layer 2 caching considers
    implementing caching techniques in wireless network. Though both of them need
    solve where to cache, what to cache, cache dimensioning and content delivery problems,
    wireless caching is more challenging and some mature strategies in wired caching
    cannot be migrated directly to wireless case. Some reasons come from the listed
    aspects: the resources in wireless environment, such as caching storage and spectrum,
    are limited compared with CRs in Layer 1 Caching; the mobility of end users and
    dynamic network typologies are also required to be considered during the design
    of caching strategies; moreover, the wireless channels are uncertain since they
    can be effected by fading and interference.'
  prefs: []
  type: TYPE_NORMAL
- en: In wireless caching, where to cache focus on finding the proper candidates among
    MBS, FBS, ED, even BBU pool and RRU in C-RAN to host the content. Caching at MBS
    and FBS can alleviate backhaul congestion since end users obtain the requested
    content from BS directly instead of from CR via backhaul links. Compared with
    FBS, MBS has wider coverage and typically, there is no overlap among different
    MBSs [[28](#bib.bib28)]. As mentioned above, the caching space in BSs is limited
    and it is impractical to cache all popular content. With the aim of improving
    cache-hit ratio, a MILP-modelled collaborative caching strategy among MBSs is
    proposed in [[29](#bib.bib29)]. If the accessed MBS does not host the content,
    the request will be served by a neighbour MBS which cache the file rather than
    by the data server. For FBS caching, a distributed caching method is presented
    in [[30](#bib.bib30)] and the main idea is that the ED locating in the FBS coverage
    overlap is able to obtain contents from multiple hosters. Caching at ED can not
    only ease backhaul congestion but also improve the area spectral efficiency [[28](#bib.bib28)].
    When the end user requests a content, he/she would be severed by the local storage
    if the content is precached in his/her ED or by adjacent ED via D2D communication
    if the content is host accordingly. In [[31](#bib.bib31)], the authors model the
    cache-enabled D2D network as a Poisson cluster process, where end users are grouped
    into several clusters and the collective performance is improved. Individually,
    caching the interested contents for other users affects personal benefit. In [[32](#bib.bib32)],
    a Stackelberg game model is applied to formulate the conflict among end users
    and a related incentive mechanism is designed to encourage content sharing. For
    the case of cache-enabled C-RAN, caching at BBU can ease the traffic congestion
    in the backhaul while caching at RRH can reduce the fronthaul communication cost.
    On the other hand, caching all at BBU raises the signaling overhead of BBU pool
    while at RRH weakens the processing capability. Therefore, where to cache the
    content in C-RAN makes a substantial contribution to balancing the signal processing
    capability at the BBU pool and the backhaul/fronthaul costs [[28](#bib.bib28)].
    The work in [[33](#bib.bib33)] investigates caching at RRHs with jointly considering
    cell outage probability and fronthaul utilization. Due to the end users’ mobility,
    the prediction/awareness of user moving behaviour also influence the proper hoster
    selection. There are some researches exploiting user mobility in cache strategy
    designing like [[34](#bib.bib34)] and [[35](#bib.bib35)].
  prefs: []
  type: TYPE_NORMAL
- en: Similar with Layer 1, what to cache decision as well as eviction policy of layer
    2 depends on the accurate prediction on content popularity or user preference
    in proactive caching method. The content popularity contains the feature of temporal
    and spatial correlations, which has already been described in Layer 1 Caching.
    In Layer 2 caching, the proper spatial granularity in popular contents estimation
    needs to take special attentions [[36](#bib.bib36)]. For example, the coverage
    of MBS and FBS are different, which makes the popularity in MBS and FBS are different
    as well. Because the former based on a large number of users’ behaviors but the
    individual may prefer specific content categories. For small cells, the preference
    estimation requires more accurate information like historical data [[28](#bib.bib28)].
    In order to capture the temporal and spatial dynamics of user preference, many
    different deep learning based algorithms are proposed, which will be illustrated
    in Section [4](#S4 "4 Deep Learning for Data Caching ‣ A Survey of Deep Learning
    for Data Caching in Edge Network").
  prefs: []
  type: TYPE_NORMAL
- en: Cache dimensioning in Layer 2 Caching has more complicated factors need to be
    considered, not only including the network topology and content popularity as
    Layer 1 Caching, but also containing backhaul transmission status and wireless
    channel features. The proper cache size assignment is studied in the scenario
    of backhaul limited cellular network [[37](#bib.bib37)]. It also provides the
    closed-form boundary of minimum cache size in one cell case. In the case of dense
    wireless network, the work in [[38](#bib.bib38)] quantifies the minimum required
    cache to achieve the linear capacity scaling of network throughput. The authors
    of [[39](#bib.bib39)] also consider the scenario of dense networks. They derive
    the closed-form of the optimal memory size which can reduce the consumption of
    backhaul capacity as well as guarantee wireless QoS.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the number of transmitters and receivers, we divide the content
    delivery in Layer 2 caching into three categories: one candidate serves one end
    user, such as unicast and D2D transmission; one candidate serves multiple users
    like multicast; and coordinated delivery including multiple transmitters serve
    one or more receivers like coordinated multi-point joint transmission (CoMP-JT).
    Once the requested content is cached locally, BS can serve the end user via unicast
    or the adjacent device shares the contents by implementing D2D transmission. Concurrent
    transmission has the risk of co-channel interference in dense deployed networks.
    In D2D network, link scheduling is introduced to select subsets of links to transmit
    simultaneously [[28](#bib.bib28)]. With the aim of improving the spectral efficiency,
    multicast is applied in content delivery when serving multiple requests simultaneously
    with the same content. Therefore there is a trade off between spectral efficiency
    and service delay. For the aim of serving more users in one transmission as well
    as higher spectral efficiency, the BS will wait to collect enough requirement
    for the same content which makes the first request a long waiting time. An optimal
    dynamic multicast scheduling is proposed in [[40](#bib.bib40)] to balance these
    two factors. Multicast can also serve multiple requests with different contents.
    In [[41](#bib.bib41)], the authors provides a coded caching scheme which requires
    the communication link is error free and each user caches a part of its own content
    and partial of other users. Then BS multicasts the coded data to all users. Each
    user can decode his own requested content by XOR operation between the received
    data and the precached other users’ file. However, the coding complexity increases
    exponentially as the quantity of end users grows. The CoMP-JT can improve the
    spectral efficiency as well via sharing channel state information (CSI) and contents
    among BSs but it also needs high-capacity backhaul consumption for exchanging
    data. In C-RAN, the BBUs are centralized in the BBU pool, which makes communication
    among BSs very efficiency. [[42](#bib.bib42)] designs CoMP-JT in C-RAN for the
    purpose of minimizing power consumption with limitations of transmission energy,
    link capacity and requested QoS.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning Outline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As Figure [3](#S3.F3 "Figure 3 ‣ 3 Deep Learning Outline ‣ A Survey of Deep
    Learning for Data Caching in Edge Network") shows, some typical deep neural network
    (DNN) methods are stated. These models are classified into three categories depending
    on the training methods: supervised learning, unsupervised learning and reinforcement
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/436e8d028fa2a7f5ceeb406c762bdd24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Typical DNN Structures'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Feedforward Neural Network (FNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FNN is a kind of DNNs whose information propagation direction is forward and
    there is no cycle in neurons. In this paper, the term FNN is used to represent
    fully connected neural network, which indicates the connection between two adjacent
    layers is filled. According to the Universal Approximation Theorem, FNN has the
    ability to approximate any closed and bounded function with enough neurons in
    hidden layer [[43](#bib.bib43)]. The hidden layer is applied to extract features
    of input vector, and then feed the output layer, which works as a classifier.
    Though FNN is very powerful, it gets into trouble when dealing with real-world
    task such as image recognition due to enormous weight parameters (because of fully
    connected) and lack of data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Convolutional Neural Network (CNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the aim of overcoming the aforementioned drawback of FNN, CNN employs convolution
    and pooling operations, where the former applies sliding convolutional filters
    to the input vector and the later does down sampling, usually via maximum or mean
    pooling. Generally, CNN tends to contain deeper layers and smaller convolutional
    filters, and the structure becomes fully convolutional network [[44](#bib.bib44)],
    reducing the ratio of pooling layers as well as fully connected layers. Taxonomically,
    CNN belongs to FNN and has been broadly employed in image recognition, video analysis,
    natural language processing, etc. Including CNN, one of the limitations of FNN
    is that the output only depends on current input vectors. So it is hard to deal
    with sequential tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Recurrent Neural Network (RNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to deal with sequential tasks and using historical information, RNN
    employs neurons with self feedback in hidden layers. Unlike the hidden neuron
    in FNN, the output of recurrent neuron depends on both current output of precious
    layer and last hidden state. Compared with FNN approximates any continues functions,
    RNN with Sigmoid activation function can simulate a universal Turing Machine and
    has the ability to solve all computational problems [[45](#bib.bib45)]. It is
    worth noting that RNN has the risk to suffer from long-term dependencies problem
    [[43](#bib.bib43)] including gradient exploding and vanishing. Additionally, RNN
    has more parameters waiting to be trained due to adding recurrent weights. In
    the following, we introduce some RNN variants as Figure [4](#S3.F4 "Figure 4 ‣
    3.3 Recurrent Neural Network (RNN) ‣ 3 Deep Learning Outline ‣ A Survey of Deep
    Learning for Data Caching in Edge Network") shows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/92efa15b66cb14c37adb80f9d051ac85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: RNN Variants'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Echo-State Network (ESN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As aforementioned, simple RNN contains more parameters in training step, where
    the recurrent weights and input weights are difficult to learn [[43](#bib.bib43)].
    The basic idea of ESN is fixing these two kinds of weights and only learn the
    output weights (as links highlighted in Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Recurrent
    Neural Network (RNN) ‣ 3 Deep Learning Outline ‣ A Survey of Deep Learning for
    Data Caching in Edge Network")). The hidden layer is renamed as reservoir in ESN,
    where the neurons are sparsely connected and the weights are randomly assigned.
    The recurrent weights keep constant so the information of previous moments is
    stored in the reservoir with constant weight like voice echoing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Long Short-Term Memory (LSTM)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recently, an efficient way to cope with long-term dependencies in practical
    is employing gated RNN, including LSTM [[43](#bib.bib43)]. Then we compare with
    the recurrent neuron in simple RNN: Internally LSTM introduces three gates to
    control signal propagation, where input gate $I$ decides the partition of input
    signal to be stored, forget gate $F$ controls ratio of last moment memory to be
    kept until next period (the name "forget gate" may be a little misleading because
    it actually represents the ratio to be remembered) and output gate $O$ influences
    the proportion of current state to be delivered; Externally LSTM has four inputs
    embracing one input signal and three control signals for three gates. All these
    four signals are derived via the calculation of current network input and last
    moment delivered state.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Pointer Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A typical application of RNN is converting one sequence to another sequence
    (seq2seq) such as machine translation. Conventionally, the output of seq2seq architecture
    is a probability distribution of output dictionary. However, it cannot deal with
    the problem that the size of output relies on the length of input due to fixed
    output dictionary. In [[46](#bib.bib46)], the authors modify the output to be
    the distribution of input sequence, which is analogous to pointers in C/C++. Pointer
    network has been widely used in text condensation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Auto Encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Auto Encoder is a stack of two NNs named encoder and decoder respectively, where
    the former tries to learn the representative characteristics of input and generate
    a related code, and the later reads the code and reconstructs the original input.
    In order to avoid the auto encoder simply copying the input, some restrictions
    are considered like the dimension of code is smaller than input vector [[43](#bib.bib43)].
    The quality of auto encoder can be measured via reconstruction error, which estimates
    the similarity between input and output. In most cases, the auto encoder is used
    for the proper representation of input vector so the decoder part is removed after
    unsupervised training. The code can be employed as input for further deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Deep Reinforcement Learning (DRL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) is a Markov Decision Process represented by a quintuple
    $\{\mathcal{S,A,P,R},\gamma\}$, where $\mathcal{S}$ is the state space controlled
    by environment; $\mathcal{A}$ is the action space determined by agent; $\mathcal{P}$
    is the state transition function measuring the probability of moving to a new
    state $s_{t+1}$ given previous state $s_{t}$ and action $a_{t}$; $R$ is reward
    function calculated by environment considering state and action; $\gamma$ is a
    discount factor for estimating total reward. During the interaction between agent
    and environment, agent observes current state $s_{t}$ from environment, and then
    takes action $a_{t}$ following its policy $\pi$. The environment moves to a new
    state $s_{t+1}$ stochastically based on $\mathcal{P}(s_{t},a_{t})$ and returns
    a reward $r_{t}$ to agent. The RL’s aim is finding the policy $\pi$ to maximum
    accumulated reward $\sum_{t}\gamma^{t}r_{t}$. In the early stage, RL focuses on
    scenarios whose $\mathcal{S}$ and $\mathcal{A}$ are discrete and limited. So the
    agent can use a table to record these information. Recently, some tasks have enormous
    discrete states and actions such as playing go and even continuous value such
    as self-driving, which makes table recording impractical. In order to solve this,
    DRL combines RL and DL, where RL defines the problem and optimization object;
    DL models the policy and the reward expectation. Depending on the roles of DNN
    in DRL, we classify the DRL into 3 categories as Figure shows.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 DNN as Critic (Value-Based)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In value-based method, DNN does not get involved with policy decision but estimates
    the policy performance. Two functions are introduced for the measurement: $V^{\pi}(s)$
    represents the reward expectation of policy $\pi$ starting from state $s$; $Q^{\pi}(s,a)$
    illustrates the reward expectation of policy $\pi$ starting from state $s$ and
    taking action $a$. In addition, $V^{\pi}(s)$ is the expected value of $Q^{\pi}(s,a)$.
    If we can estimate $Q^{\pi}(s,a)$, the policy $\pi$ can also be improved by choosing
    the action $a^{*}$ hold $Q^{\pi}(s,a^{*})\geq V^{\pi}(s)$. So the DNN employed
    in agent is approximating function $Q^{\pi}(s,a)$, where the inputs are state
    $s$ and action $a$ and output is the estimated value $Q^{\pi}(s,a)$. There are
    some representative critic methods like Deep Q Networks (DQN) [[47](#bib.bib47)]
    and its variants Double DQN [[48](#bib.bib48)], Dueling DQN [[49](#bib.bib49)],
    etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 DNN as Actor (Policy-Based)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In policy-based method, DNN gets involved in the action selection directly instead
    of via $Q^{\pi}(s,a)$. The policy can be viewed as an optimization problem, where
    the objective function is maximizing reward expectation and the search space is
    policy space. The input of DNN is current state and output is the probability
    distribution of potential actions. By employing gradient ascent, we can update
    the DNN to provide better action then maximize total reward. Some popular algorithms
    include Trust Region Policy Optimization (TRPO) [[50](#bib.bib50)], Proximal Policy
    Optimization (PPO) [[51](#bib.bib51)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.3 Actor-Critic Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally, compared with policy-based approach, the value-based method is less
    stable and suffer from poor convergence since the policy is derived based on $Q^{\pi}(s,a)$
    approximation. But value-based method is more sample efficient, while policy-based
    method is easier to fall into local optimal solution because the search space
    is vast. The actor-critic model combines these two approaches, i.e. the agent
    contains two DNNs named actor and critic respectively. In each training iteration,
    the actor considers current state $s$ and policy $\pi$ for deciding action $a$.
    Then the environment changes to state $s^{\prime}$ and returns reward $r$. The
    critic updates its own parameters based on the feedback from environment and output
    a mark for the actor’s action. The actor updates the policy $\pi$ depending on
    critic’s mark. Some typical algorithms are proposed recent years like Deep Deterministic
    Policy Gradient (DDPG) [[52](#bib.bib52)] and Asynchronous Advantage Actor-Critic
    (A3C) [[53](#bib.bib53)].
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep Learning for Data Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We divide the studies regarding deep learning for data caching in edge networks
    into four categories depending on the DL tools employed: FNN and CNN; RNN; Auto
    Encoder; DRL. Recently many works utilize more than one DL techniques for jointly
    considered caching problems. For instance, at the beginning we applies a RNN to
    predict content popularity, and then a DRL to find suboptimal solutions of content
    placement for the purpose of reducing time complexity. In such case, we classify
    the related work into DRL since it represents the caching allocation policy. Unless
    mention the caching location (such as CRs, MBSs, FBSs, EDs and BBUs) otherwise,
    the approaches in this section can be utilized for both Layer 1 and Layer 2 caching.
    Table [2](#S4.T2 "Table 2 ‣ 4 Deep Learning for Data Caching ‣ A Survey of Deep
    Learning for Data Caching in Edge Network") summarize some studies of DL for caching.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of Deep Learning for Data Caching'
  prefs: []
  type: TYPE_NORMAL
- en: '| method | Study | Caching Problem | DL Objective |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FNN and CNN | [[54](#bib.bib54)] | content delivery | reduce feasible region
    of time slot allocation |'
  prefs: []
  type: TYPE_TB
- en: '| [[55](#bib.bib55)] | where to cache, content delivery | determine MBSs for
    caching & delivery duration |'
  prefs: []
  type: TYPE_TB
- en: '| [[56](#bib.bib56)] | where to cache, content delivery | nominate proper CRs
    for caching |'
  prefs: []
  type: TYPE_TB
- en: '| [[57](#bib.bib57)] | where to cache, content delivery | reduce feasible region
    for caching |'
  prefs: []
  type: TYPE_TB
- en: '| [[58](#bib.bib58)] | what to cache | extract video features |'
  prefs: []
  type: TYPE_TB
- en: '| [[59](#bib.bib59)] | what to cache | predict requested content & frequency
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[60](#bib.bib60)] | what to cache | predict requested content |'
  prefs: []
  type: TYPE_TB
- en: '| [[61](#bib.bib61)] | what to cache | predict content popularity |'
  prefs: []
  type: TYPE_TB
- en: '| RNN | [[62](#bib.bib62), [63](#bib.bib63)] | what to cache | predict requested
    content & user mobility |'
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] | what to cache | predict content popularity |'
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] | content delivery | reduce traffic load, select optimal
    BS subset |'
  prefs: []
  type: TYPE_TB
- en: '| Auto Encoder | [[70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73),
    [74](#bib.bib74)] | what to cache | predict content popularity |'
  prefs: []
  type: TYPE_TB
- en: '| [[75](#bib.bib75)] | what to cache | predict top popular contents |'
  prefs: []
  type: TYPE_TB
- en: '| DRL | [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84),
    [85](#bib.bib85)] | what to cache | decide cache placement |'
  prefs: []
  type: TYPE_TB
- en: '| [[86](#bib.bib86)] | what to cache | decide cache replacement & power allocation
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[87](#bib.bib87)] | what to cache | predict popularity & searching best
    NN model |'
  prefs: []
  type: TYPE_TB
- en: '| [[88](#bib.bib88)] | where to cache | decide cache location |'
  prefs: []
  type: TYPE_TB
- en: '| [[89](#bib.bib89)] | content delivery | users grouping |'
  prefs: []
  type: TYPE_TB
- en: '| [[90](#bib.bib90)] | where to cache, content delivery | decide BS connection,
    computation offloading & caching location |'
  prefs: []
  type: TYPE_TB
- en: '| [[91](#bib.bib91)] | what to cache, content delivery | decide caching & bandwidth
    allocation |'
  prefs: []
  type: TYPE_TB
- en: '| [[92](#bib.bib92)] | what to cache, content delivery | decide caching, computing
    offloading & radio resource allocation |'
  prefs: []
  type: TYPE_TB
- en: '| [[93](#bib.bib93)] | what to cache, content delivery | decide multicast scheduling
    & caching replacement |'
  prefs: []
  type: TYPE_TB
- en: '| [[94](#bib.bib94)] | where & what to cache | predict popularity, decide caching
    & task offloading |'
  prefs: []
  type: TYPE_TB
- en: '| [[95](#bib.bib95)] | where & what to cache, content delivery | predict user
    mobility & content popularity, determine D2D link |'
  prefs: []
  type: TYPE_TB
- en: 4.1 FNN and CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [[54](#bib.bib54)], the content delivery problem in wireless network is formulated
    as two MILP optimization models with the aims of minimum delivery time slot and
    energy consumption respectively. Both models consider the data rate for content
    delivery. Considering the computational complexity of solving MILP, a CNN is introduced
    to reduce the feasible region of decision variables, where the input is channel
    coefficients matrix. The FNN in paper [[55](#bib.bib55)] plays a similar role
    as [[54](#bib.bib54)] to simplify the searching space of the content delivery
    optimization model.
  prefs: []
  type: TYPE_NORMAL
- en: For resource allocation problem, the authors of [[96](#bib.bib96)] model it
    as linear sum assignment problems then utilize CNN and FNN to solve the model.
    The idea is extended in [[56](#bib.bib56)] and [[57](#bib.bib57)], where the authors
    consider where to cache problem among potential CRs and content delivery jointly,
    which is modeled as MILP with the aim of balancing caching and transmission cost
    by considering the user mobility, space utilization and bandwidth limitations.
    The cache allocation is viewed as multi-label classification problem and is decomposed
    into several independent sub-problems, where each one correlates with a CNN to
    predict assignment. The input of CNN is a grey-scale image which combines the
    information of user mobility, space and link utilization level. In [[56](#bib.bib56)],
    a hill climbing local search algorithm is provided to improve the performance
    of CNN while in [[57](#bib.bib57)], the prediction of CNN is used to feed a smaller
    MILP model.
  prefs: []
  type: TYPE_NORMAL
- en: For these above works [[54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57), [96](#bib.bib96)], the FNN or CNN input is extracted from the
    optimization model. The work in [[97](#bib.bib97)] trains a CNN via original graph
    instead of parameters matrix/image, which makes the process human recognizable
    and interpretable. Though the authors take traveling salesman problem not data
    caching as an example, the method can be viewed as a potential research direction.
  prefs: []
  type: TYPE_NORMAL
- en: In [[58](#bib.bib58)], an ILP model is proposed to minimize the backhaul video-data
    type load by determining the portion of cached content in BSs. Considering the
    fact that the mobile users covered by a BS change frequently, therefore predicting
    user preference is unnecessary. Instead, the authors concentrate on the popular
    content in general. At the beginning, a 3D CNN is introduced to extract spatio-temporal
    features of videos. The popularity of new contents without historical information
    is determined via comparing similar video features. The authors of [[59](#bib.bib59)]
    also considers the spatio-temporal features among visiting contents in a mobile
    bus WiFI environment. By exploiting the previous 9 days collecting data, the content
    that the user may visit on the last day and corresponding visiting frequency can
    be forecast. The social property is taken into account in [[60](#bib.bib60)].
    By observing users interests on tweets during 2016 U.S. election, a CNN based
    predicted model can foresee the content category that is most likely to be requested.
    Such kind of content would be cached in MBSs and FBSs.
  prefs: []
  type: TYPE_NORMAL
- en: The work of [[61](#bib.bib61)] examines the role of DNN in caching from another
    aspect. The authors propose a FNN to predict content popularity as a regression
    problem. The results show that FNN outperforms RNN, though the later is believed
    to be effective to solve sequential predictions. Moreover, replacing the FNN by
    a linear estimator does not devalue the performance significantly. The author
    provides explanation that FNN would work better than linear predictor in the case
    of incomplete information, and RNN has more advantages to model the popularity
    prediction as a classification rather than a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 RNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Considering RNN is superior in dealing with sequential tasks, the work [[64](#bib.bib64)]
    applies a bidirectional RNN for online content popularity prediction in mobile
    edge network. Simple RNN’s output depends on previous and current storage, but
    the bidirectional can also take future information into account. The forecast
    model consists three blocks cascadingly: a CNN reads user requests and extract
    features; bidirectional LTSM learns association of requests over time step; FNN
    is added in the end to improve the prediction performance. Then content eviction
    is based on the popularity prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[62](#bib.bib62)] utilize ESN to predict both content request
    distribution and end user mobility pattern. The user’s preference is viewed as
    context which links with personal information combining gender, age, job, location,
    etc.. For the request prediction, the input of ESN is user’s information vector
    and the output represent the probability distribution of content. For mobility
    prediction, the input includes historical and present user’s location and the
    output is the expected position for next time duration. Eventually, the prediction
    influences the caching content decisions in BBUs and RRHs for the purpose of minimizing
    traffic load and delay in CRAN. The authors extend their work in [[63](#bib.bib63)]
    by introducing conceptor-based ESN which can split users’ context into different
    patterns and learn them independently. Therefore a more accurate prediction is
    achieved.
  prefs: []
  type: TYPE_NORMAL
- en: In [[65](#bib.bib65)], a caching decision policy named PA-Cache is proposed
    to predict time-variant video popularity for cache eviction when the space is
    full. The temporal content popularity is exploited by attaching every hidden layer
    representation of RNN to an output regression. In order to improve the accuracy,
    hedge backpropagation is introduced during training process which decides when
    and how to adapt the depth of the DNN in an evolving manner. Similarly, the work
    in [[66](#bib.bib66)] also considers caching replacement of video content. A deep
    LSTM network is utilized for popularity prediction consisting of stacking multiple
    LSTM layers and one softmax layer, where the input of the network is request sequence
    data (device, timestamp, location, title of video) without any prepossessing and
    the output is estimated content popularity. Another work concentrates on prediction
    and interactions between user mobility and content popularity can be found [[67](#bib.bib67)].
  prefs: []
  type: TYPE_NORMAL
- en: The work of [[68](#bib.bib68)] recognizes the popularity prediction as a seq2seq
    modeling problem and proposes LSTM Encoder-Decoder model. The input vector consists
    of past probabilities where each vectors are calculated during a predefined time
    window. In [[69](#bib.bib69)], the authors focus on caching content delivery with
    the aim of minimizing BSs to cover all requested users, i.e. set cover problem,
    via coded caching. Unlike [[68](#bib.bib68)], an auto encoder is introduced in
    coded caching stage for file conversion to reduce transmission load. In addition,
    a RNN model is employed to select BSs for broadcasting.
  prefs: []
  type: TYPE_NORMAL
- en: The paper [[98](#bib.bib98)] shows the potential of RNN in solving where to
    cache problem. In [[98](#bib.bib98)], a task allocation model is formulated as
    a knapsack problem and the decision variables represent the task is processed
    locally in mobile devices (MDs) or remotely in edge servers (ESs). The authors
    design a multi-pointer network structure of 3 RNNs, where 2 encoders encode MDs
    and ESs respectively, 1 decoder demonstrates ES and MD pairing. Considering the
    similarity of where to cache optimization model and knapsack problem, the multi-pointer
    network can be transferred for caching location decision after according parameter
    modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Auto Encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally, auto encoder is utilized to learn efficient representation or extract
    features of raw data in an unsupervised manner. The work in [[70](#bib.bib70)]
    considers the cache replacement in wireless sensor network (WSN) based on content
    popularity. Considering sparse auto encoder (SAE) can extract representative expression
    of input data, the authors employ a SAE followed by a classifier where the input
    contains collecting user content requests and the output represents the contents
    popularity level. The authors also think about the implementation in a distributed
    way by SDN/NFV technical, i.e. the input layer is deployed on sink node, while
    the rest layers are implemented on the main controller. A related work applying
    auto encoder in 5G network proactive caching can be found in [[71](#bib.bib71)].
    In [[72](#bib.bib72)], two auto encoders are utilized for extracting the features
    of users and content respectively. Then the extracted information is explored
    to estimate popularity at the core network. Similarly, the auto encoder in [[73](#bib.bib73)]
    is for spatio-temporal popularity features extraction and auto encoders work collaboratively
    in [[75](#bib.bib75)] to predict top K popular videos.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 DRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The work in [[78](#bib.bib78)] focuses on the cooperative caching policy at
    FBSs with maximum distance separable coding in ultra dense networks. A value-based
    model is utilized to determine caching categories and the content quantity at
    FBSs during off peak duration. The authors in [[79](#bib.bib79)] study the problem
    of caching 360°videos and virtual viewports in FBSs with unknown content popularity.
    The virtual viewport represents the most popular tiles of a 360°video over users’
    population. A DQN is introduced to decide which tiles of a video to be hosted
    and in which quality. Additionally, [[80](#bib.bib80)] employs DQN for content
    eviction decision offering a satisfactory quality of experience and [[81](#bib.bib81)]
    is for the purpose of minimizing energy consumption. In [[82](#bib.bib82)], the
    authors also apply DQN to decide cache eviction in a single BS. Moreover, the
    critic is generated with stacking LSTM and FNN to evaluate Q value and an external
    memory is added for recording learned knowledge. For the purpose of improving
    the prediction accuracy, the Q value update is determined by the similarity of
    estimated value of critic and recording information in the external memory, instead
    of critic domination. The paper [[84](#bib.bib84)] puts forth DQN a two-level
    network caching, where a parent node links with multiple leaf nodes to cache content
    instead of a single BS. In [[76](#bib.bib76)], a DRL framework with Wolpertinger
    architecture [[99](#bib.bib99)] is presented for content caching at BSs. The Wolpertinger
    architecture is based on actor-critic model and performs efficiently in large
    discrete action space. [[76](#bib.bib76)] employs two FNNs working as actor and
    critic respectively, where the former determines requested content is cached or
    not and the later estimates the reward. The whole framework consists two phases:
    in offline phase, these two FNNs are trained in supervised learning; in online
    phase, the critic and actor update via the interaction with environment. The authors
    extend their work to a multi agent actor-critic model for decentralized cooperative
    caching at multiple BSs [[77](#bib.bib77)]. In [[83](#bib.bib83)], an actor-critic
    model is used for solving cache replacement problem, which balance the data freshness
    and communication cost. The aforementioned papers put attention on the network
    performance while ignore the influence of caching on information processing and
    resource consumption. Therefore, authors of [[85](#bib.bib85)] design cache policy
    considering both network performance during content transmission and processing
    efficiency during data consumption. A DQN is employed to determine the number
    of chunks of the requested file to be updated. The paper [[86](#bib.bib86)] investigates
    a joint cache replacement and power allocation optimization problem to minimize
    latency in a downlink F-RAN. A DQN is proposed for finding a suboptimal solution.
    Though [[87](#bib.bib87)] is regarded as solving what to cache problem like [[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86)],
    the reinforcement learning approach plays a different role. In [[87](#bib.bib87)],
    a DNN is utilized for content popularity prediction and then a RL is used for
    DNN hyperparameters tuning instead of determining caching content. Therefore the
    action space consists of choosing model architectures (i.e. CNN, LSTM, etc.),
    number of layers and layer configurations.'
  prefs: []
  type: TYPE_NORMAL
- en: In [[88](#bib.bib88)], the authors generate an optimization model with the aim
    of maximizing network operator’s utility in mobile social networks under the framework
    of mobile edge computing, in network caching and D2D communications (3C). The
    trust value which if estimated through social relationships among users are also
    considered. Then a DQN model is utilized for solving optimization problem, including
    determine video provider and subscriber association, video transcoding offloading
    and the video cache allocation for video providers.. The DQN employs two CNNs
    for training process, where one generates target Q value and the other is for
    estimated Q value. Unlike the conventional DQN, the authors in [[88](#bib.bib88)]
    introduces a dueling structure, i.e. the Q value is not computed in the final
    fully connected layer, but is decomposed into two components and use the summary
    as estimated Q value, which helps achieve a more robust result. The authors also
    consider utilizing dueling DQN model in different scenarios like cache-enabled
    opportunistic interference alignment [[89](#bib.bib89)] and orchestrating 3C in
    vehicular network [[90](#bib.bib90)]. The work [[91](#bib.bib91)] provides a DDPG
    model to cope with continuous valued control decision for 3C in vehicular edge
    networks, which is combined with the idea of DQN and actor-critic model. The DDPG
    structure can be divided into two parts as DQN, one is for estimated Q value and
    the other for target Q value. Each part consists of two DNNs, which play the role
    of actor and critic respectively. The critic updates its parameters like DQN while
    the actor learns policy via deterministic policy gradient approach. The proposed
    DRL is used for deciding content caching/replacement, vehicle organization and
    bandwidth resource assignment on different duration.
  prefs: []
  type: TYPE_NORMAL
- en: The paper [[92](#bib.bib92)] provides an optimization model which takes what
    to cache and content delivery into consideration in the fog-enabled IoT network
    in order to minimize service latency. Since the wireless signals and user requests
    are stochastic, a actor-critic model is engaged where the actor makes decision
    for requesting contents while critic estimates the reward. Specially, the action
    space $S$ consists of decision variables and reward function is a variant of the
    objective function. A caching replacement strategy and dynamic multicast scheduling
    strategy are studied in [[93](#bib.bib93)]. In order to get a suboptimal result,
    an auto encoder is used to approximate the state. Further, a weighted double DQN
    scheme is utilized for avoiding overestimation of Q value. [[94](#bib.bib94)]
    applies a RNN to predict content popularity by collecting historical requests
    and the output represents the popularity in the near future. Then the prediction
    is employed for cooperative caching and computation offloading among MEC servers,
    which is modelled as a ILP problem. For the purpose of solving it efficiently,
    a multi-agent DQN is applied where each user is viewed as an agent. The action
    space consists of task local computing and offloading decision as well as local
    caching and cooperative caching determination. The reward is measured by accumulated
    latency. The agent choose its own action based on current state without cooperation.
    The where to cache, what to cache and content delivery decision of D2D network
    are jointly modelled in [[95](#bib.bib95)]. Two RNNs, ESN and LSTM, are considered
    to predict mobile users’ location and requested content popularity. Then the prediction
    result is used for determining content categories and cache locations. The content
    delivery is formulated as the actor-critic based DRL framework. The state spaces
    include CSI, transmission distances and communication power between requested
    user and other available candidates. The function of DRL is determining the communication
    link among users with the aim of minimizing power consumption and content delay.
  prefs: []
  type: TYPE_NORMAL
- en: We notice that most papers prefer to use value-based model (critic) and value-policy-based
    (actor-critic) model in DRL framework, but rare paper considers only policy-based
    model to solve data caching problem. One proper reason is that the search space
    of caching problem is enormous so policy-based model is easier to fall into local
    optimal solution, resulting in poor performance. Though the value-based model
    is less stable, some variant structures are utilized like Double DQN in [[93](#bib.bib93)]
    to avoid value overestimation and dueling DQN in [[88](#bib.bib88), [89](#bib.bib89),
    [90](#bib.bib90)] to improve robust.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Research Challenges and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A serious of open issues on content caching and potential research directions
    are discussed in this section. We first extend the idea of content caching to
    virtual network function chain since caching can be viewed as a specific network
    function. Then we consider the caching for augmented reality applications. Moreover,
    we notice that the cache dimensioning has not been covered yet by DL methods.
    Finally, we debate the addition cost introduced by DL.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Caching as a Virtual Network Function Chain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of Network Function Virtualization (NFV) has been firstly discussed
    and proposed within the realms of the European Telecommunication Standardization
    Institute (ETSI)¹¹1Network Functions Virtualisation, An Introduction, Benefits,
    Enablers, Challenges and Call for Action, ETSI, 2012 https://portal.etsi.org/NFV/NFV_White_Paper.pdf.
    The rational is to facilitate the dynamic provisioning of network services through
    virtualization technologies to decouple the service creation process form the
    underlying hardware. The framework allows network services to be implemented by
    a specific chaining and ordering of a set of functions which can be implemented
    either on a more traditional dedicated hardware which in this this case are called
    Physical Network Functions (PNFs), or alternatively as Virtual Network Functions
    (VNFs) which is a software running on top of virtualized general-purpose hardware.
    The decoupling between the hardware and the software is one of the important considerations
    the other – equally important – is that a virtualized service lend itself naturally
    to a dynamic programmable service creation where VNF resources can be deployed
    as required. Hence, edge cloud and network resource usage can be adapted to the
    instantaneous user demand whilst avoiding a more static over-provisioned configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Within that framework, the incoming network service requests include the specification
    of the service function chain that need to be created in the form of an ordered
    sequence of VNFs. For example different type of VNFs such as a firewall or a NAT
    mechanism need to be visited in a specific order. In such constructed service
    chain each independent VNF requires specific underlying resources in terms for
    example of CPU cycles and/or memory.
  prefs: []
  type: TYPE_NORMAL
- en: Under this framework, caching of popular content can be considered as a specialized
    VNF chain function since inevitably delivery of the cached popular content to
    users will require a set of other functions to be supported related to security,
    optimization of the content etc. However, the issue of data caching and VNF chaining
    have evolved rather independently in the literature and the issue on how to optimize
    data caching when seeing it as part of VNF chain is still an interesting open
    ended issue.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Caching for Mobile Augmented Reality (MAR) applications and Digital Twins
    (DTs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mobile augmented reality (MAR) applications can be considered as a way to augment
    the physical real-world environment with artificial computer-based generated information
    and is an area that has received significant research attention recently. In order
    to successfully superimpose different digital object in the physical world MAR
    applications include several computationally and storage complex concepts such
    as image recognition, mobile camera calibration, and also the use of advanced
    2D and 3D graphics rendering. These functionalities are highly computationally
    intensive and as such require support from an edge cloud, in addition the virtual
    objects to be embedded in the physical world are expected to be proactively cached
    closer to the end user so that latency is minimized. Ultra low latency in these
    type of applications is of paramount importance so that to provide a photorealistic
    embedding of virtual objects in the video view of the end user. However, since
    computational and augmented reality objects need to be readily available, the
    caching of those objects should be considered in conjunction with the computational
    capabilities of he edge cloud. In addition to the above when MAR is considered
    under the lenses of an NFV environment the application might inherently require
    access to some VNFs and therefore the above discussion on VNF chaining for MAR
    applications is also valid in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Recently the concept of Digital Twin (DT) [[100](#bib.bib100)], [[101](#bib.bib101)]
    has received significant research attention due to the plethora of applications
    ranging from industrial manufacturing and health to smart cities. In a nutshell,
    a DT can be defined as an accurate digital replica of a real world object across
    multiple granularity levels; and this real world object could be a machine, a
    robot or an industrial process or (sub) system. By reflecting the physical status
    of the system under consideration in a virtual space open up a plethora of optimization,
    prediction, fault tolerance and automation process that cannot be done using solely
    the physical object. At the core of DT applications is the requirement of stringent
    two–way real time communication between the digital replica and the physical object.
    This requirement inevitably require support from edge clouds to minimize latency
    and efficient storage and computational resources including caching. In that setting,
    the use of the aforementioned deep learning technologies will have a key role
    to play in order to provide high quality real time decision making to avoid misalignment
    between the digital replica of the physical object under consideration. Efficient
    machine-to-DT connectivity would require capabilities similar to the above mentioned
    augmented reality application but due to the continuous real-time control-loop
    operation DTs will require a complete new set of network optimization capabilities
    and in that frontier efficient caching and data-driven techniques will have a
    central role to play. Hence, as the research regarding the inter-play between
    low latency communications and DTs is still in embryonic stage there is significant
    scope in the investigation of suitable data driven deep learning techniques to
    be utilized for distributed allocation of caching and computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Deep Learning for Cache Dimensioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As introduced in Section [2](#S2 "2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network"), cache dimensioning explores the appropriate
    cache size allocation for content host such as CRs and BSs. Disappointingly, there
    is rare paper applies DL on cache dimensioning decisions. One proper reason is
    lack of training data set in contrast to content popular prediction, where we
    have historical user request log to train a DNN. In addition, the caching size
    allocation affects the network performance and economic investment. Recently,
    network slicing is identified as an important tool to enable 5G to provide multi-services
    with diverse characteristics. The slice is established on physical infrastructure
    including network storage. Therefore it is a very interesting topic to consider
    the allocation of the memory space to support content caching and other storage
    services, which guarantees QoE and satisfies task requirements. Furthermore, for
    the case lack of training data set, DRL can be viewed as a promising technology
    to configure slicing settings as well as cache dimensioning. For the action space
    designing, it can be either discrete by setting storage levels, or continuous
    which is allocate the memory space directly. However, there are requirements to
    design caching-enabled network slicing model especially for dynamic allocation
    as well as associated DRL framework including state space, detailed action space,
    reward function and agent structure.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 The Cost of Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though the application of DL brings performance efficiency for caching policy,
    additional cost introduced by DL is unneglected, since training and deploying
    DL model require not only network resources but also time duration. Naturally
    there is a trade-off between the cost which DL-assisted caching policy saved and
    the consumption which supports DL itself running, which indicates the trading
    with DL results in either profit, loss, or break even. Therefore, where and when
    to apply DL should be carefully investigated. In addition, for the purpose of
    reducing resource consumption and accelerating training process, some knowledge
    transfer methods like transfer learning [[102](#bib.bib102)] can be utilized,
    which can transform the knowledge already learnt from the source domain to a relevant
    target domain.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article presents a comprehensive study for the application of deep learning
    methods in the area of content caching. Particularly, the data caching is divided
    into two classifications according to the caching location of edge network. Each
    category contains where to cache, what to cache, cache dimensioning and content
    delivery. Then we introduce typical DNN methods which are categorised via training
    process into supervised learning, unsupervised learning and RL. Further, this
    paper critically compares and analyzes state-of-the-art papers on parameters,
    such as DL methods employed, the caching problems solved and the objective of
    applying DL. The challenges and research directions of DL on caching is also examined
    on the topic of extending caching to VNF chains, the application of caching for
    MAR as well as DTs, DL for cache size allocation and the additional cost of employing
    DL. Undoubtedly, DL is playing a significant role in 5G and beyond. We hope this
    paper will increase discussions and interests on DL for caching policy design
    and relevant applications, which will advance future network communications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] X. Wang, M. Chen, Z. Han, D. O. Wu, and T. T. Kwon. Toss: Traffic offloading
    by social network service-based opportunistic sharing in mobile social networks.
    In IEEE INFOCOM 2014 - IEEE Conference on Computer Communications, pages 2346–2354,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing
    of deep neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295–2329,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Yaohua Sun, Mugen Peng, Yangcheng Zhou, Yuzhe Huang, and Shiwen Mao. Application
    of machine learning in wireless networks: Key techniques and open issues. IEEE
    Communications Surveys & Tutorials, 21(4):3072–3108, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Adita Kulkarni and Anand Seetharam. Model and machine learning based caching
    and routing algorithms for cache-enabled networks. arXiv preprint arXiv:2004.06787,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Junaid Shuja, Kashif Bilal, Eisa Alanazi, Waleed Alasmary, and Abdulaziz
    Alashaikh. Applying machine learning techniques for caching in edge networks:
    A comprehensive survey. arXiv preprint arXiv:2006.16864, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Stephen ANOKYE, SEID Mohammed, and SUN Guolin. A survey on machine learning
    based proactive caching. ZTE Communications, 17(4):46–55, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Mingzhe Chen, Ursula Challita, Walid Saad, Changchuan Yin, and Mérouane
    Debbah. Artificial neural networks-based machine learning for wireless networks:
    A tutorial. IEEE Communications Surveys & Tutorials, 21(4):3039–3071, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang,
    Ying-Chang Liang, and Dong In Kim. Applications of deep reinforcement learning
    in communications and networking: A survey. IEEE Communications Surveys & Tutorials,
    21(4):3133–3174, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Xiaofei Wang, Yiwen Han, Victor CM Leung, Dusit Niyato, Xueqiang Yan, and
    Xu Chen. Convergence of edge computing and deep learning: A comprehensive survey.
    IEEE Communications Surveys & Tutorials, 22(2):869–904, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, and Lanyu Xu. Edge computing:
    Vision and challenges. IEEE internet of things journal, 3(5):637–646, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Mugen Peng, Yaohua Sun, Xuelong Li, Zhendong Mao, and Chonggang Wang.
    Recent advances in cloud radio access networks: System architectures, key techniques,
    and open issues. IEEE Communications Surveys & Tutorials, 18(3):2282–2308, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Georgios S Paschos, George Iosifidis, Meixia Tao, Don Towsley, and Giuseppe
    Caire. The role of caching in future communication systems and networks. IEEE
    Journal on Selected Areas in Communications, 36(6):1111–1125, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Siyang Shan, Chunyan Feng, Tiankui Zhang, and Jonathan Loo. Proactive
    caching placement for arbitrary topology with multi-hop forwarding in icn. IEEE
    Access, 7:149117–149131, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Yantong Wang, Gao Zheng, and Vasilis Friderikos. Proactive caching in
    mobile networks with delay guarantees. In ICC 2019-2019 IEEE International Conference
    on Communications (ICC), pages 1–6\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Chao Fang, F Richard Yu, Tao Huang, Jiang Liu, and Yunjie Liu. An energy-efficient
    distributed in-network caching scheme for green content-centric networks. Computer
    Networks, 78:119–129, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Jagruti Sahoo, Mohammad A Salahuddin, Roch Glitho, Halima Elbiaze, and
    Wessam Ajib. A survey on replica server placement algorithms for content delivery
    networks. IEEE Communications Surveys & Tutorials, 19(2):1002–1026, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Asif Kabir, Gohar Rehman, Syed Mushhad Gilani, Edvin J Kitindi, Zain Ul Abidin Jaffri,
    and Khurrum Mustafa Abbasi. The role of caching in next generation cellular networks:
    A survey and research outlook. Transactions on Emerging Telecommunications Technologies,
    31(2):e3702, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Stefano Traverso, Mohamed Ahmed, Michele Garetto, Paolo Giaccone, Emilio
    Leonardi, and Saverio Niccolini. Temporal locality in today’s content caching:
    why it matters and how to model it. ACM SIGCOMM Computer Communication Review,
    43(5):5–12, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Ali Dabirmoghaddam, Maziar Mirzazad Barijough, and JJ Garcia-Luna-Aceves.
    Understanding optimal caching and opportunistic caching at" the edge" of information-centric
    networks. In Proceedings of the 1st ACM conference on information-centric networking,
    pages 47–56, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Yue Shi, Martha Larson, and Alan Hanjalic. Collaborative filtering beyond
    the user-item matrix: A survey of the state of the art and future challenges.
    ACM Computing Surveys (CSUR), 47(1):1–45, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Jaeyeon Jung, Arthur W Berger, and Hari Balakrishnan. Modeling ttl-based
    internet caches. In IEEE INFOCOM 2003\. Twenty-second Annual Joint Conference
    of the IEEE Computer and Communications Societies (IEEE Cat. No. 03CH37428), volume 1,
    pages 417–426\. IEEE, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Nicaise Choungmo Fofack, Philippe Nain, Giovanni Neglia, and Don Towsley.
    Performance evaluation of hierarchical ttl-based cache networks. Computer Networks,
    65:212–231, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Dario Rossi and Giuseppe Rossini. On sizing ccn content stores by exploiting
    topological information. In 2012 Proceedings IEEE INFOCOM Workshops, pages 280–285.
    IEEE, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Yuemei Xu, Yang Li, Tao Lin, Zihou Wang, Wenjia Niu, Hui Tang, and Song
    Ci. A novel cache size optimization scheme based on manifold learning in content
    centric networking. Journal of Network and Computer Applications, 37:273–281,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Georgios Paschos, George Iosifidis, and Giuseppe Caire. Cache optimization
    models and algorithms. arXiv preprint arXiv:1912.12339, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Van Jacobson, Diana K Smetters, James D Thornton, Michael F Plass, Nicholas H
    Briggs, and Rebecca L Braynard. Networking named content. In Proceedings of the
    5th international conference on Emerging networking experiments and technologies,
    pages 1–12, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Liang Wang, Suzan Bayhan, Jörg Ott, Jussi Kangasharju, and Jon Crowcroft.
    Understanding scoped-flooding for content discovery and caching in content networks.
    IEEE Journal on Selected Areas in Communications, 36(8):1887–1900, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Liying Li, Guodong Zhao, and Rick S Blum. A survey of caching techniques
    in cellular networks: Research issues and challenges in content placement and
    delivery strategies. IEEE Communications Surveys & Tutorials, 20(3):1710–1732,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Ammar Gharaibeh, Abdallah Khreishah, Bo Ji, and Moussa Ayyash. A provably
    efficient online collaborative caching algorithm for multicell-coordinated systems.
    IEEE Transactions on Mobile Computing, 15(8):1863–1876, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Negin Golrezaei, Andreas F Molisch, Alexandros G Dimakis, and Giuseppe
    Caire. Femtocaching and device-to-device collaboration: A new architecture for
    wireless video distribution. IEEE Communications Magazine, 51(4):142–149, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Mehrnaz Afshang, Harpreet S Dhillon, and Peter Han Joo Chong. Fundamentals
    of cluster-centric content placement in cache-enabled device-to-device networks.
    IEEE Transactions on Communications, 64(6):2511–2526, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Zhuoqun Chen, Yangyang Liu, Bo Zhou, and Meixia Tao. Caching incentive
    design in wireless d2d networks: A stackelberg game approach. In 2016 IEEE International
    Conference on Communications (ICC), pages 1–6\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Zhun Ye, Cunhua Pan, Huiling Zhu, and Jiangzhou Wang. Tradeoff caching
    strategy of the outage probability and fronthaul usage in a cloud-ran. IEEE Transactions
    on Vehicular Technology, 67(7):6383–6397, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Dewang Ren, Xiaolin Gui, Kaiyuan Zhang, and Jie Wu. Mobility-aware traffic
    offloading via cooperative coded edge caching. IEEE Access, 8:43427–43442, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Jaeyoung Song and Wan Choi. Mobility-aware content placement for device-to-device
    caching systems. IEEE Transactions on Wireless Communications, 18(7):3658–3668,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Dong Liu, Binqiang Chen, Chenyang Yang, and Andreas F Molisch. Caching
    at the wireless edge: design aspects, challenges, and future directions. IEEE
    Communications Magazine, 54(9):22–28, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Xi Peng, Jun Zhang, SH Song, and Khaled B Letaief. Cache size allocation
    in backhaul limited wireless networks. In 2016 IEEE International Conference on
    Communications (ICC), pages 1–6\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] An Liu and Vincent KN Lau. How much cache is needed to achieve linear
    capacity scaling in backhaul-limited dense wireless networks? IEEE/ACM Transactions
    on Networking, 25(1):179–188, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Jaeyoung Song and Wan Choi. Minimum cache size and backhaul capacity for
    cache-enabled small cell networks. IEEE Wireless Communications Letters, 7(4):490–493,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Bo Zhou, Ying Cui, and Meixia Tao. Optimal dynamic multicast scheduling
    for cache-enabled content-centric wireless networks. IEEE Transactions on Communications,
    65(7):2956–2970, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Mohammad Ali Maddah-Ali and Urs Niesen. Fundamental limits of caching.
    IEEE Transactions on Information Theory, 60(5):2856–2867, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Vu Nguyen Ha, Long Bao Le, et al. Coordinated multipoint transmission
    design for cloud-rans with limited fronthaul capacity constraints. IEEE Transactions
    on Vehicular Technology, 65(9):7432–7447, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT
    Press, 2016. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431–3440, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Hava T Siegelmann and Eduardo D Sontag. Turing computability with neural
    nets. Applied Mathematics Letters, 4(6):77–80, 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks.
    In Advances in neural information processing systems, pages 2692–2700, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
    Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,
    et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning
    with double q-learning. In Thirtieth AAAI conference on artificial intelligence,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and
    Nando Freitas. Dueling network architectures for deep reinforcement learning.
    In International conference on machine learning, pages 1995–2003, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
    Moritz. Trust region policy optimization. In International conference on machine
    learning, pages 1889–1897, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In International conference on machine learning,
    pages 1928–1937, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Lei Lei, Yaxiong Yuan, Thang X Vu, Symeon Chatzinotas, and Björn Ottersten.
    Learning-based resource allocation: Efficient content delivery enabled by convolutional
    neural network. In 2019 IEEE 20th International Workshop on Signal Processing
    Advances in Wireless Communications (SPAWC), pages 1–5\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Lei Lei, Lei You, Gaoyang Dai, Thang Xuan Vu, Di Yuan, and Symeon Chatzinotas.
    A deep learning approach for optimizing content delivering in cache-enabled hetnet.
    In 2017 international symposium on wireless communication systems (ISWCS), pages
    449–453\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Yantong Wang and Vasilis Friderikos. Caching as an image characterization
    problem using deep convolutional neural networks. arXiv preprint arXiv:1907.07263,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Yantong Wang and Vasilis Friderikos. Network orchestration in mobile networks
    via a synergy of model-driven and ai-based techniques. arXiv preprint arXiv:2004.00660,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Khai Nguyen Doan, Thang Van Nguyen, Tony QS Quek, and Hyundong Shin. Content-aware
    proactive caching for backhaul offloading in cellular network. IEEE Transactions
    on Wireless Communications, 17(5):3128–3140, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Zhou Qin, Yikun Xian, and Desheng Zhang. A neural networks based caching
    scheme for mobile edge networks. In Proceedings of the 17th Conference on Embedded
    Networked Sensor Systems, pages 408–409, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Kuo Chun Tsai, Li Wang, and Zhu Han. Mobile social media networks caching
    with convolutional neural network. In 2018 IEEE wireless communications and networking
    conference workshops (WCNCW), pages 83–88\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Vladyslav Fedchenko, Giovanni Neglia, and Bruno Ribeiro. Feedforward neural
    networks for caching: n enough or too much? ACM SIGMETRICS Performance Evaluation
    Review, 46(3):139–142, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Mingzhe Chen, Walid Saad, Changchuan Yin, and Mérouane Debbah. Echo state
    networks for proactive caching in cloud-based radio access networks with mobile
    users. IEEE Transactions on Wireless Communications, 16(6):3520–3535, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Mingzhe Chen, Mohammad Mozaffari, Walid Saad, Changchuan Yin, Mérouane
    Debbah, and Choong Seon Hong. Caching in the sky: Proactive deployment of cache-enabled
    unmanned aerial vehicles for optimized quality-of-experience. IEEE Journal on
    Selected Areas in Communications, 35(5):1046–1061, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Laha Ale, Ning Zhang, Huici Wu, Dajiang Chen, and Tao Han. Online proactive
    caching in mobile edge computing using bidirectional deep recurrent neural network.
    IEEE Internet of Things Journal, 6(3):5520–5530, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Qilin Fan, Jian Li, Xiuhua Li, Qiang He, Shu Fu, and Sen Wang. Pa-cache:
    Learning-based popularity-aware content caching in edge networks. arXiv preprint
    arXiv:2002.08805, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Cong Zhang, Haitian Pang, Jiangchuan Liu, Shizhi Tang, Ruixiao Zhang,
    Dan Wang, and Lifeng Sun. Toward edge-assisted video content intelligent caching
    with long short-term memory learning. IEEE access, 7:152832–152846, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Hanlin Mou, Yuhong Liu, and Li Wang. Lstm for mobility based content popularity
    prediction in wireless caching networks. In 2019 IEEE Globecom Workshops (GC Wkshps),
    pages 1–6\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Arvind Narayanan, Saurabh Verma, Eman Ramadan, Pariya Babaie, and Zhi-Li
    Zhang. Deepcache: A deep learning based framework for content caching. In Proceedings
    of the 2018 Workshop on Network Meets AI & ML, pages 48–53, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Zhengming Zhang, Yaru Zheng, Chunguo Li, Yongming Huang, and Luxi Yang.
    On the cover problem for coded caching in wireless networks via deep neural network.
    In 2019 IEEE Global Communications Conference (GLOBECOM), pages 1–6\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Fangyuan Lei, Jun Cai, Qingyun Dai, and Huimin Zhao. Deep learning based
    proactive caching for effective wsn-enabled vision applications. Complexity, 2019,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] FangYuan Lei, QinYun Dai, Jun Cai, HuiMin Zhao, Xun Liu, and Yan Liu.
    A proactive caching strategy based on deep learning in epc of 5g. In International
    Conference on Brain Inspired Cognitive Systems, pages 738–747\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Shailendra Rathore, Jung Hyun Ryu, Pradip Kumar Sharma, and Jong Hyuk
    Park. Deepcachnet: A proactive caching framework based on deep learning in cellular
    networks. IEEE Network, 33(3):130–138, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Wai-Xi Liu, Jie Zhang, Zhong-Wei Liang, Ling-Xi Peng, and Jun Cai. Content
    popularity prediction and caching for icn: A deep learning approach with sdn.
    IEEE access, 6:5075–5089, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Wei Li, Jun Wang, Guoyong Zhang, Li Li, Ze Dang, and Shaoqian Li. A reinforcement
    learning based smart cache strategy for cache-aided ultra-dense network. IEEE
    Access, 7:39390–39401, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Yu-Tai Lin, Chia-Cheng Yen, and Jia-Shung Wang. Video popularity prediction:
    An autoencoder approach with clustering. IEEE Access, 8:129285–129299, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Chen Zhong, M Cenk Gursoy, and Senem Velipasalar. A deep reinforcement
    learning-based framework for content caching. In 2018 52nd Annual Conference on
    Information Sciences and Systems (CISS), pages 1–6\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Chen Zhong, M Cenk Gursoy, and Senem Velipasalar. Deep reinforcement learning-based
    edge caching in wireless networks. IEEE Transactions on Cognitive Communications
    and Networking, 6(1):48–61, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Shen Gao, Peihao Dong, Zhiwen Pan, and Geoffrey Ye Li. Reinforcement learning
    based cooperative coded caching under dynamic popularities in ultra-dense networks.
    IEEE Transactions on Vehicular Technology, 69(5):5442–5456, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Pantelis Maniotis and Nikolaos Thomos. Viewport-aware deep reinforcement
    learning approach for 360$\^{o}$ video caching. arXiv preprint arXiv:2003.08473,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Xiaoming He, Kun Wang, and Wenyao Xu. Qoe-driven content-centric caching
    with deep reinforcement learning in edge-enabled iot. IEEE Computational Intelligence
    Magazine, 14(4):12–20, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Jie Tang, Hengbin Tang, Xiuyin Zhang, Kanapathippillai Cumanan, Gaojie
    Chen, Kai-Kit Wong, and Jonathon A Chambers. Energy minimization in d2d-assisted
    cache-enabled internet of things: A deep reinforcement learning approach. IEEE
    Transactions on Industrial Informatics, 16(8):5412–5423, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Pingyang Wu, Jun Li, Long Shi, Ming Ding, Kui Cai, and Fuli Yang. Dynamic
    content update for wireless edge caching via deep reinforcement learning. IEEE
    Communications Letters, 23(10):1773–1777, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Hao Zhu, Yang Cao, Xiao Wei, Wei Wang, Tao Jiang, and Shi Jin. Caching
    transient data for internet of things: A deep reinforcement learning approach.
    IEEE Internet of Things Journal, 6(2):2074–2083, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Alireza Sadeghi, Gang Wang, and Georgios B Giannakis. Deep reinforcement
    learning for adaptive caching in hierarchical content delivery networks. IEEE
    Transactions on Cognitive Communications and Networking, 5(4):1024–1033, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Yimeng Wang, Yongbo Li, Tian Lan, and Vaneet Aggarwal. Deepchunk: Deep
    q-learning for chunk-based caching in wireless data processing networks. IEEE
    Transactions on Cognitive Communications and Networking, 5(4):1034–1045, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] GM Shafiqur Rahman, Mugen Peng, Shi Yan, and Tian Dang. Learning based
    joint cache and power allocation in fog radio access networks. IEEE Transactions
    on Vehicular Technology, 69(4):4401–4411, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Kyi Thar, Thant Zin Oo, Yan Kyaw Tun, Ki Tae Kim, Choong Seon Hong, et al.
    A deep learning model generation framework for virtualized multi-access edge cache
    management. IEEE Access, 7:62734–62749, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Ying He, Chengchao Liang, Richard Yu, and Zhu Han. Trust-based social
    networks with computing, caching and communications: A deep reinforcement learning
    approach. IEEE Transactions on Network Science and Engineering, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Ying He, Zheng Zhang, F Richard Yu, Nan Zhao, Hongxi Yin, Victor CM Leung,
    and Yanhua Zhang. Deep-reinforcement-learning-based optimization for cache-enabled
    opportunistic interference alignment wireless networks. IEEE Transactions on Vehicular
    Technology, 66(11):10433–10445, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Ying He, Nan Zhao, and Hongxi Yin. Integrated networking, caching, and
    computing for connected vehicles: A deep reinforcement learning approach. IEEE
    Transactions on Vehicular Technology, 67(1):44–55, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Guanhua Qiao, Supeng Leng, Sabita Maharjan, Yan Zhang, and Nirwan Ansari.
    Deep reinforcement learning for cooperative content caching in vehicular edge
    computing and networks. IEEE Internet of Things Journal, 7(1):247–257, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Yifei Wei, F Richard Yu, Mei Song, and Zhu Han. Joint optimization of
    caching, computing, and radio resources for fog-enabled iot using natural actor–critic
    deep reinforcement learning. IEEE Internet of Things Journal, 6(2):2061–2073,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Zhengming Zhang, Hongyang Chen, Meng Hua, Chunguo Li, Yongming Huang,
    and Luxi Yang. Double coded caching in ultra dense networks: Caching and multicast
    scheduling via deep reinforcement learning. IEEE Transactions on Communications,
    68(2):1071–1086, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Shilu Li, Baogang Li, and Wei Zhao. Joint optimization of caching and
    computation in multi-server noma-mec system via reinforcement learning. IEEE Access,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Lixin Li, Yang Xu, Jiaying Yin, Wei Liang, Xu Li, Wei Chen, and Zhu Han.
    Deep reinforcement learning approaches for content caching in cache-enabled d2d
    networks. IEEE Internet of Things Journal, 7(1):544–557, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Mengyuan Lee, Yuanhao Xiong, Guanding Yu, and Geoffrey Ye Li. Deep neural
    networks for linear sum assignment problems. IEEE Wireless Communications Letters,
    7(6):962–965, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Zhengxuan Ling, Xinyu Tao, Yu Zhang, and Xi Chen. Solving optimization
    problems through fully convolutional networks: An application to the traveling
    salesman problem. IEEE Transactions on Systems, Man, and Cybernetics: Systems,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Qingmiao Jiang, Yuan Zhang, and Jinyao Yan. Neural combinatorial optimization
    for energy-efficient offloading in mobile edge computing. IEEE Access, 8:35077–35089,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag,
    Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris,
    and Ben Coppin. Deep reinforcement learning in large discrete action spaces. arXiv
    preprint arXiv:1512.07679, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] A. El Saddik. Digital twins: The convergence of multimedia technologies.
    IEEE MultiMedia, 25(2):87–92, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] K. M. Alam and A. El Saddik. C2ps: A digital twin architecture reference
    model for the cloud-based cyber-physical systems. IEEE Access, 5:2050–2062, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer
    learning. Journal of Big data, 3(1):9, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
