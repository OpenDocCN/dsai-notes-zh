- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:44:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:44:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2210.04675] A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2210.04675] 解决基于深度学习的自然语言处理中的类别不平衡问题的方法调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2210.04675](https://ar5iv.labs.arxiv.org/html/2210.04675)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2210.04675](https://ar5iv.labs.arxiv.org/html/2210.04675)
- en: A Survey of Methods for Addressing Class Imbalance
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决类别不平衡的方法调查
- en: in Deep-Learning Based Natural Language Processing
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于深度学习的自然语言处理
- en: Sophie Henning^(1,2)    William Beluch¹    Alexander Fraser²    Annemarie Friedrich¹
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Sophie Henning^(1,2)    William Beluch¹    Alexander Fraser²    Annemarie Friedrich¹
- en: ¹ Bosch Center for Artificial Intelligence, Renningen, Germany
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 博世人工智能中心，德国伦宁根
- en: ² Center for Information and Language Processing, LMU Munich, Germany
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ² 信息与语言处理中心，德国慕尼黑大学
- en: sophieelisabeth.henning|william.beluch@de.bosch.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: sophieelisabeth.henning|william.beluch@de.bosch.com
- en: fraser@cis.lmu.de
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: fraser@cis.lmu.de
- en: annemarie.friedrich@de.bosch.com
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: annemarie.friedrich@de.bosch.com
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Many natural language processing (NLP) tasks are naturally imbalanced, as some
    target categories occur much more frequently than others in the real world. In
    such scenarios, current NLP models tend to perform poorly on less frequent classes.
    Addressing class imbalance in NLP is an active research topic, yet, finding a
    good approach for a particular task and imbalance scenario is difficult.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多自然语言处理（NLP）任务本质上是不平衡的，因为在现实世界中，一些目标类别的出现频率远高于其他类别。在这种情况下，当前的NLP模型往往在较少出现的类别上表现较差。解决NLP中的类别不平衡问题是一个活跃的研究课题，但为特定任务和不平衡场景找到有效的方法仍然困难重重。
- en: In this survey, the first overview on class imbalance in deep-learning based
    NLP, we first discuss various types of controlled and real-world class imbalance.
    Our survey then covers approaches that have been explicitly proposed for class-imbalanced
    NLP tasks or, originating in the computer vision community, have been evaluated
    on them. We organize the methods by whether they are based on sampling, data augmentation,
    choice of loss function, staged learning, or model design. Finally, we discuss
    open problems and how to move forward.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们首次概述了基于深度学习的NLP中的类别不平衡问题，我们首先讨论了各种受控和现实世界的类别不平衡类型。我们的调查接着涵盖了专门为类别不平衡的NLP任务提出的或来源于计算机视觉社区并在这些任务上进行评估的方法。我们按方法是否基于采样、数据增强、损失函数选择、阶段性学习或模型设计来组织这些方法。最后，我们讨论了开放问题以及如何推进研究。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: '![Refer to caption](img/ebd6a8a2fa52d7bbe71c05fff9f2ac6c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/ebd6a8a2fa52d7bbe71c05fff9f2ac6c.png)'
- en: (a) Single-label relation     classification on TACRED
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 单标签关系     分类在TACRED上
- en: (Zhou and Chen, [2021](#bib.bib117))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (Zhou and Chen, [2021](#bib.bib117))
- en: '![Refer to caption](img/43e764c6e87f2f056920b28136245f4f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/43e764c6e87f2f056920b28136245f4f.png)'
- en: (b) Hierarchical multi-label
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 层次多标签
- en: patent classification      (Pujari et al., [2021](#bib.bib70))
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 专利分类      (Pujari et al., [2021](#bib.bib70))
- en: '![Refer to caption](img/52575f9dbcef57bfb787837510a98dc6.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/52575f9dbcef57bfb787837510a98dc6.png)'
- en: (c) Implicit discourse rela-      tion classification (PDTB)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 隐含话语关系分类（PDTB）
- en: (Shi and Demberg, [2019](#bib.bib79))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (Shi and Demberg, [2019](#bib.bib79))
- en: '![Refer to caption](img/73ce2b8c8cfc076435ddafa5b02691c6.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/73ce2b8c8cfc076435ddafa5b02691c6.png)'
- en: (d) UD dependency parsing      using RoBERTa on EWT
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 使用RoBERTa进行UD依赖解析在EWT上
- en: (Grünewald et al., [2021](#bib.bib27))
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (Grünewald et al., [2021](#bib.bib27))
- en: 'Figure 1: Class imbalance has a negative effect on performance especially for
    minority classes in a variety of NLP tasks. Upper charts show label count distributions,
    lower part show test/dev F1 by training instance count (lighter colors indicate
    fewer test/dev instances). All models are based on transformers.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：类别不平衡对性能的负面影响，尤其是在各种NLP任务中的少数类别。上部图表显示标签计数分布，下部显示测试/开发F1分数与训练实例计数的关系（较浅的颜色表示测试/开发实例较少）。所有模型均基于变换器。
- en: Class imbalance is a major problem in natural language processing (NLP), because
    target category distributions are almost always skewed in NLP tasks. As illustrated
    by Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Methods for Addressing
    Class Imbalance in Deep-Learning Based Natural Language Processing"), this often
    leads to poor performance on minority classes. Which categories matter is highly
    task-specific and may even depend on the intended downstream use. Developing methods
    that improve model performance in imbalanced data settings has been an active
    area for decades (e.g., Bruzzone and Serpico, [1997](#bib.bib9); Japkowicz et al.,
    [2000](#bib.bib37); Estabrooks and Japkowicz, [2001](#bib.bib21); Park and Zhang,
    [2002](#bib.bib67); Tan, [2005](#bib.bib83)), and is recently gaining momentum
    in the context of maturing neural approaches (e.g., Buda et al., [2018](#bib.bib10);
    Kang et al., [2020](#bib.bib43); Li et al., [2020](#bib.bib52); Yang et al., [2020](#bib.bib105);
    Jiang et al., [2021](#bib.bib38); Spangher et al., [2021](#bib.bib80)). The problem
    is exacerbated when classes overlap in the feature space (Lin et al., [2019](#bib.bib54);
    Tian et al., [2020](#bib.bib88)). For example, in patent classification, technical
    categories differ largely in frequency, and the concepts mentioned in the different
    categories can be very similar.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡是自然语言处理（NLP）中的一个主要问题，因为在 NLP 任务中，目标类别分布几乎总是偏斜的。如图 [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing") 所示，这通常导致少数类的表现不佳。哪些类别重要高度依赖于具体任务，甚至可能取决于预期的下游使用。在不平衡数据环境中提高模型性能的方法开发已经是一个活跃的研究领域几十年了（例如，Bruzzone
    和 Serpico，[1997](#bib.bib9)；Japkowicz 等人，[2000](#bib.bib37)；Estabrooks 和 Japkowicz，[2001](#bib.bib21)；Park
    和 Zhang，[2002](#bib.bib67)；Tan，[2005](#bib.bib83)），最近在成熟的神经方法背景下获得了更多关注（例如，Buda
    等人，[2018](#bib.bib10)；Kang 等人，[2020](#bib.bib43)；Li 等人，[2020](#bib.bib52)；Yang
    等人，[2020](#bib.bib105)；Jiang 等人，[2021](#bib.bib38)；Spangher 等人，[2021](#bib.bib80)）。当类别在特征空间中重叠时，问题会更加严重（Lin
    等人，[2019](#bib.bib54)；Tian 等人，[2020](#bib.bib88)）。例如，在专利分类中，技术类别的频率差异很大，而且不同类别中提到的概念可能非常相似。
- en: '![Refer to caption](img/86655417b53d563cf3981d0887faabbc.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/86655417b53d563cf3981d0887faabbc.png)'
- en: (a) Step imbalance, $\mu=0.4$, $\rho=10$
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 步骤不平衡，$\mu=0.4$，$\rho=10$
- en: '![Refer to caption](img/0451941012cadd4eeb206950f81713f0.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0451941012cadd4eeb206950f81713f0.png)'
- en: (b) Linear imbalance, $\rho=10$
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 线性不平衡，$\rho=10$
- en: '![Refer to caption](img/60bd8f46e1874b00813051562d6e909b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/60bd8f46e1874b00813051562d6e909b.png)'
- en: (c) Long-tailed distribution
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 长尾分布
- en: 'Figure 2: Instance counts per label follow different distributions: examples
    of class imbalance types.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：每个标签的实例数量遵循不同的分布：类别不平衡类型的示例。
- en: On a large variety of NLP tasks, transformer models such as BERT (Vaswani et al.,
    [2017](#bib.bib93); Devlin et al., [2019](#bib.bib17)) outperform both their neural
    predecessors and traditional models (Liu et al., [2019](#bib.bib57); Xie et al.,
    [2020](#bib.bib104); Mathew et al., [2021](#bib.bib60)). Performance for minority
    classes is also often higher when using self-supervised pre-trained models (e.g.,
    Li and Scarton, [2020](#bib.bib53); Niklaus et al., [2021](#bib.bib65)), which
    parallels findings from computer vision (Liu et al., [2022](#bib.bib56)). However,
    the advent of BERT has not solved the class imbalance problem in NLP, as illustrated
    by Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Methods for Addressing
    Class Imbalance in Deep-Learning Based Natural Language Processing"). Tänzer et al.
    ([2022](#bib.bib85)) find that on synthetically imbalanced named entity datasets
    with majority classes having thousands of examples, at least 25 instances are
    required to predict a class at all, and 100 examples to learn to predict it with
    some accuracy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种自然语言处理（NLP）任务中，像 BERT（Vaswani 等人，[2017](#bib.bib93)；Devlin 等人，[2019](#bib.bib17)）这样的变换器模型表现优于它们的神经前身和传统模型（Liu
    等人，[2019](#bib.bib57)；Xie 等人，[2020](#bib.bib104)；Mathew 等人，[2021](#bib.bib60)）。使用自监督预训练模型（例如，Li
    和 Scarton，[2020](#bib.bib53)；Niklaus 等人，[2021](#bib.bib65)）时，少数类的表现通常也较高，这与计算机视觉中的发现相一致（Liu
    等人，[2022](#bib.bib56)）。然而，BERT 的出现并没有解决 NLP 中的类别不平衡问题，如图 [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing") 所示。Tänzer 等人（[2022](#bib.bib85)）发现，在合成不平衡的命名实体数据集上，主要类别的样本数达到数千时，至少需要
    25 个实例才能预测一个类别，并且需要 100 个示例才能学会以一定准确率进行预测。
- en: Despite the relevance of class imbalance to NLP, related surveys only exist
    in the computer vision domain (Johnson and Khoshgoftaar, [2019b](#bib.bib40);
    Zhang et al., [2021b](#bib.bib116)). Incorporating methods addressing class imbalance
    can lead to performance gains of up to 20%. Yet, NLP research often overlooks
    how important this is in practical applications, where minority classes may be
    of special interest.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管类别不平衡在NLP中的相关性，但相关的调查仅存在于计算机视觉领域（Johnson 和 Khoshgoftaar，[2019b](#bib.bib40)；Zhang
    等，[2021b](#bib.bib116)）。纳入解决类别不平衡的方法可以带来高达20%的性能提升。然而，NLP研究往往忽视了这在实际应用中的重要性，特别是当少数类可能特别重要时。
- en: Our contribution is to draw a clear landscape of approaches applicable to deep-learning
    (DL) based NLP. We set out with a problem definition (Sec. [2](#S2 "2 Problem
    Definition ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing")), and then organize approaches by whether
    they are based on sampling, data augmentation, choice of loss function, staged
    learning, or model design (Sec. [3](#S3 "3 Methods for Addressing Class Imbalance
    in NLP ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based
    Natural Language Processing")). Our extensive survey finds that re-sampling, data
    augmentation, and changing the loss function can be relatively simple ways to
    increase performance in class-imbalanced settings and are thus straightforward
    choices for NLP practitioners.¹¹1We provide practical advice on identifying potentially
    applicable class imbalance methods in the Appendix (Figure [3](#A2.F3 "Figure
    3 ‣ Appendix B Practical advice ‣ A Survey of Methods for Addressing Class Imbalance
    in Deep-Learning Based Natural Language Processing")). While promising research
    directions, staged learning or model modifications often are implementation-wise
    and/or computationally costlier. Moreover, we discuss particular challenges of
    non-standard classification settings, e.g., imbalanced multi-label classification
    and catch-all classes, and provide useful connections to related computer vision
    work. Finally, we outline promising directions for future research (Sec. [4](#S4
    "4 Insights and Future Directions ‣ A Survey of Methods for Addressing Class Imbalance
    in Deep-Learning Based Natural Language Processing")).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献是绘制一个适用于深度学习（DL）基础自然语言处理（NLP）的方法全景图。我们从问题定义开始（见第[2](#S2 "2 Problem Definition
    ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural
    Language Processing)节"），然后根据方法是否基于采样、数据增强、损失函数选择、分阶段学习或模型设计进行组织（见第[3](#S3 "3 Methods
    for Addressing Class Imbalance in NLP ‣ A Survey of Methods for Addressing Class
    Imbalance in Deep-Learning Based Natural Language Processing)节"）。我们广泛的调查发现，重新采样、数据增强以及更改损失函数可以在类别不平衡的情况下相对简单地提高性能，因此是NLP从业者的直接选择。¹¹1我们在附录中提供了识别潜在适用类别不平衡方法的实用建议（见图[3](#A2.F3
    "Figure 3 ‣ Appendix B Practical advice ‣ A Survey of Methods for Addressing Class
    Imbalance in Deep-Learning Based Natural Language Processing)节"）。尽管分阶段学习或模型修改是有前景的研究方向，但它们往往在实现和/或计算上更为昂贵。此外，我们讨论了非标准分类设置的特定挑战，例如不平衡的多标签分类和“
    catch-all” 类别，并提供了与相关计算机视觉工作的有用联系。最后，我们概述了未来研究的有前景方向（见第[4](#S4 "4 Insights and
    Future Directions ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing)节"）。
- en: Scope of this survey. We focus on approaches evaluated on or developed for neural
    methods. Work from “traditional” NLP (e.g., Tomanek and Hahn, [2009](#bib.bib90);
    Li et al., [2011](#bib.bib51); Li and Nenkova, [2014](#bib.bib50); Kunchukuttan
    and Bhattacharyya, [2015](#bib.bib47)) as well as Natural Language Generation
    (e.g., Nishino et al., [2020](#bib.bib66)) and Automatic Speech Recognition (e.g.,
    Winata et al., [2020](#bib.bib101); Deng et al., [2022](#bib.bib16)) are not addressed
    in this survey. Other types of imbalances such as differently sized data sets
    of subtasks in continual learning (Ahrens et al., [2021](#bib.bib2)) or imbalanced
    regression (Yang et al., [2021](#bib.bib106)) are also beyond the scope of this
    survey. In Sec. [3.5](#S3.SS5 "3.5 Model Design ‣ 3 Methods for Addressing Class
    Imbalance in NLP ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing"), we briefly touch upon the related area of
    few-shot learning (Wang et al., [2020c](#bib.bib96)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的范围。我们关注的是针对神经方法进行评估或开发的方法。来自“传统”NLP 的工作（例如，Tomanek 和 Hahn，[2009](#bib.bib90)；Li
    等人，[2011](#bib.bib51)；Li 和 Nenkova，[2014](#bib.bib50)；Kunchukuttan 和 Bhattacharyya，[2015](#bib.bib47)）以及自然语言生成（例如，Nishino
    等人，[2020](#bib.bib66)）和自动语音识别（例如，Winata 等人，[2020](#bib.bib101)；Deng 等人，[2022](#bib.bib16)）的方法未在本调查中涉及。其他类型的不平衡，例如在持续学习中子任务的数据集大小不同（Ahrens
    等人，[2021](#bib.bib2)）或不平衡回归（Yang 等人，[2021](#bib.bib106)），也超出了本调查的范围。在第[3.5节](#S3.SS5
    "3.5 模型设计 ‣ 3 解决 NLP 中类不平衡的方法 ‣ 解决深度学习自然语言处理中的类不平衡的方法调查")中，我们简要提及了相关的少样本学习领域（Wang
    等人，[2020c](#bib.bib96)）。
- en: Related surveys. We review imbalance-specific data augmentation approaches in
    Sec. [3.2](#S3.SS2 "3.2 Data Augmentation ‣ 3 Methods for Addressing Class Imbalance
    in NLP ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based
    Natural Language Processing"). Feng et al. ([2021](#bib.bib22)) give a broader
    overview of data augmentation in NLP, Hedderich et al. ([2021](#bib.bib33)) provide
    an overview of low-resource NLP, and Ramponi and Plank ([2020](#bib.bib72)) discuss
    neural domain adaptation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 相关调查。我们在第[3.2节](#S3.SS2 "3.2 数据增强 ‣ 3 解决 NLP 中类不平衡的方法 ‣ 解决深度学习自然语言处理中的类不平衡的方法调查")中回顾了针对不平衡的特定数据增强方法。Feng
    等人 ([2021](#bib.bib22)) 对 NLP 中的数据增强进行了更广泛的概述，Hedderich 等人 ([2021](#bib.bib33))
    提供了低资源 NLP 的概述，而 Ramponi 和 Plank ([2020](#bib.bib72)) 讨论了神经领域适应。
- en: 2 Problem Definition
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题定义
- en: Class imbalance refers to a classification setting in which one or multiple
    classes (minority classes) are considerably less frequent than others (majority
    classes). More concrete definitions, e.g., regarding the relative share up to
    which a class is seen as a minority class, depend on the task, dataset and labelset
    size. Much research focuses on improving all minority classes equally while maintaining
    or at least monitoring majority class performance (e.g., Huang et al., [2021](#bib.bib35);
    Yang et al., [2020](#bib.bib105); Spangher et al., [2021](#bib.bib80)). We next
    discuss prototypical types of imbalance (Sec. [2.1](#S2.SS1 "2.1 Types of Imbalance
    ‣ 2 Problem Definition ‣ A Survey of Methods for Addressing Class Imbalance in
    Deep-Learning Based Natural Language Processing")) and then compare controlled
    and real-world settings (Sec. [2.2](#S2.SS2 "2.2 Controlled vs. Real-World Class
    Imbalance ‣ 2 Problem Definition ‣ A Survey of Methods for Addressing Class Imbalance
    in Deep-Learning Based Natural Language Processing")).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡指的是一种分类设置，其中一个或多个类别（少数类别）的出现频率明显低于其他类别（多数类别）。更具体的定义，例如关于将哪个类别视为少数类别的相对份额，取决于任务、数据集和标签集的大小。许多研究集中于在保持或至少监控多数类别性能的同时，平等地改善所有少数类别（例如，Huang
    等人，[2021](#bib.bib35)；Yang 等人，[2020](#bib.bib105)；Spangher 等人，[2021](#bib.bib80)）。我们接下来讨论典型的不平衡类型（第[2.1节](#S2.SS1
    "2.1 不平衡类型 ‣ 2 问题定义 ‣ 解决深度学习自然语言处理中的类不平衡的方法调查")），然后比较受控和实际场景（第[2.2节](#S2.SS2 "2.2
    受控 vs. 真实世界类不平衡 ‣ 2 问题定义 ‣ 解决深度学习自然语言处理中的类不平衡的方法调查")）。
- en: 2.1 Types of Imbalance
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 不平衡类型
- en: To systematically investigate the effect of imbalance, Buda et al. ([2018](#bib.bib10))
    define two prototypical types of label distributions, which we explain next.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了系统地研究不平衡的影响，Buda 等人 ([2018](#bib.bib10)) 定义了两种典型的标签分布类型，我们将在下文中解释。
- en: Step imbalance is characterized by the fraction of minority classes, $\mu$,
    and the size ratio between majority and minority classes, $\rho$. Larger $\rho$
    values indicate more imbalanced data sets. In prototypical step imbalance, if
    there are multiple minority classes, all of them are equally sized; if there are
    several majority classes, they also have equal size. Figure [2(a)](#S1.F2.sf1
    "In Figure 2 ‣ 1 Introduction ‣ A Survey of Methods for Addressing Class Imbalance
    in Deep-Learning Based Natural Language Processing") shows a step-imbalanced distribution
    with $40\%$ of the classes being minority classes and an imbalance ratio of $\rho=10$.
    NLP datasets with a large catch-all class as they often arise in sequence tagging
    (see Sec. [2.2](#S2.SS2 "2.2 Controlled vs. Real-World Class Imbalance ‣ 2 Problem
    Definition ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing")) or in relevance judgments in retrieval models
    frequently resemble step-imbalanced distributions. The $\rho$ ratio has also been
    reported in NLP, e.g., by Li et al. ([2020](#bib.bib52)), although more task-specific
    imbalance measures have been proposed, e.g., for single-label text classification
    (Tian et al., [2020](#bib.bib88)). In linear imbalance, class size grows linearly
    with imbalance ratio $\rho$ (see Figure [2(b)](#S1.F2.sf2 "In Figure 2 ‣ 1 Introduction
    ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural
    Language Processing")), as, e.g., in the naturally imbalanced SICK dataset for
    natural language inference (Marelli et al., [2014](#bib.bib59)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤失衡的特点是少数类的比例$\mu$和多数类与少数类之间的大小比$\rho$。较大的$\rho$值表示数据集的不平衡程度更高。在原型步骤失衡中，如果存在多个少数类，它们的大小相同；如果存在几个多数类，它们也具有相同的大小。图 [2(a)](#S1.F2.sf1
    "在图 2 ‣ 1 引言 ‣ 处理深度学习自然语言处理中的类别不平衡方法的综述")展示了一个步骤失衡的分布，其中$40\%$的类别为少数类，失衡比为$\rho=10$。NLP数据集中，大的通用类通常出现在序列标注（参见第[2.2节](#S2.SS2
    "2.2 受控与现实世界中的类别不平衡 ‣ 2 问题定义 ‣ 处理深度学习自然语言处理中的类别不平衡方法的综述")）或检索模型中的相关性判断中，往往类似于步骤失衡分布。$\rho$比率在NLP中也有所报道，例如Li等人（[2020](#bib.bib52)），尽管也提出了更多特定任务的不平衡度量，例如用于单标签文本分类（Tian等人，[2020](#bib.bib88)）。在线性失衡中，类别大小随着失衡比$\rho$线性增长（见图 [2(b)](#S1.F2.sf2
    "在图 2 ‣ 1 引言 ‣ 处理深度学习自然语言处理中的类别不平衡方法的综述")），例如在自然语言推理的SICK数据集中（Marelli等人，[2014](#bib.bib59)）。
- en: Long-tailed label distributions (Figure [2(c)](#S1.F2.sf3 "In Figure 2 ‣ 1 Introduction
    ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural
    Language Processing")) are conceptually similar to linear imbalance. They contain
    many data points for a small number of classes (head classes), but only very few
    for the rest of the classes (tail classes). These distributions are common in
    computer vision tasks like instance segmentation (e.g., Gupta et al., [2019a](#bib.bib28)),
    but also in multi-label text classification, for example with the goal of assigning
    clinical codes (Mullenbach et al., [2018](#bib.bib63)), patent categories (Pujari
    et al., [2021](#bib.bib70)), or news and research topics (Huang et al., [2021](#bib.bib35)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 长尾标签分布（图 [2(c)](#S1.F2.sf3 "在图 2 ‣ 1 引言 ‣ 处理深度学习自然语言处理中的类别不平衡方法的综述")）在概念上类似于线性失衡。它们包含少数类别（头部类别）的大量数据点，但其余类别（尾部类别）只有非常少的数据点。这些分布在计算机视觉任务中很常见，如实例分割（例如，Gupta等人，[2019a](#bib.bib28)），但在多标签文本分类中也常见，例如目标是分配临床编码（Mullenbach等人，[2018](#bib.bib63)）、专利类别（Pujari等人，[2021](#bib.bib70)）或新闻和研究主题（Huang等人，[2021](#bib.bib35)）。
- en: 2.2 Controlled vs. Real-World Class Imbalance
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 受控与现实世界中的类别不平衡
- en: Most real-world label distributions in NLP tasks do not perfectly match the
    prototypical distributions proposed by Buda et al. ([2018](#bib.bib10)). Yet,
    awareness of these settings helps practitioners to select appropriate methods
    for their data set or problem by comparing distribution plots. Using synthetically
    imbalanced data sets, researchers can control for more experimental factors and
    investigate several scenarios at once. However, evaluating on naturally imbalanced
    data provides evidence of a method’s real-world effectiveness. Some recent studies
    combine both types of evaluation (e.g., Tian et al., [2021](#bib.bib89); Subramanian
    et al., [2021](#bib.bib81); Jang et al., [2021](#bib.bib36)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现实世界中的 NLP 任务标签分布并不完全匹配 Buda 等人（[2018](#bib.bib10)）提出的典型分布。然而，了解这些设置有助于从业者通过比较分布图来选择适合他们数据集或问题的方法。使用合成不平衡数据集，研究人员可以控制更多实验因素并一次性研究多种情况。然而，在自然不平衡数据上进行评估提供了方法在实际世界中的有效性证据。一些近期研究结合了这两种评估类型（例如，Tian
    等，[2021](#bib.bib89)；Subramanian 等，[2021](#bib.bib81)；Jang 等，[2021](#bib.bib36)）。
- en: Many NLP tasks require treating a large, often heterogenous catch-all class
    that contains all instances that are not of interest to the task, while the remaining
    (minority) classes are approximately same-sized. Examples include the “Outside”
    label in IOB sequence tagging, or tweets that mention products in contexts that
    are irrelevant to the annotated categories (Adel et al., [2017](#bib.bib1)). Such
    real-world settings often roughly follow a step imbalance distribution, with the
    additional difficulty of the catch-all class.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 NLP 任务需要处理一个包含所有与任务无关的实例的大型、通常是异质的类别，而剩余（少数）类别的大小大致相同。例如，IOB 序列标注中的“Outside”标签，或提及产品但与注释类别无关的推文（Adel
    等，[2017](#bib.bib1)）。这些真实世界的设置通常大致遵循一个步骤不平衡分布，并且还面临着处理大类的额外困难。
- en: 2.3 Evaluation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 评估
- en: 'As accuracy and micro-averages mostly reflect majority class performance, choosing
    a good evaluation setting and metric is non-trivial. It is also highly task-dependent:
    in many NLP tasks, recognizing one or all minority classes well is at least equally
    important as majority class performance. For instance, non-hateful tweets are
    much more frequent in Twitter (Waseem and Hovy, [2016](#bib.bib97)), but recognizing
    hateful content is the key motivation of hate speech detection. Which classes
    matter may even depend on downstream considerations, i.e., the same named entity
    tagger might be used in one application where a majority class matters, and another
    where minority classes matter more. Several evaluation metrics exist that have
    been designed to account for class-imbalanced settings, but no de facto standard
    exists. For example, balanced accuracy (Brodersen et al., [2010](#bib.bib8)) corresponds
    to the average of per-class recall scores. It is often useful to record performance
    on all classes and to report macro-averages, which treat all classes equally.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于准确性和微平均值主要反映了大类的表现，选择一个好的评估设置和指标并非易事。这也高度依赖于任务：在许多 NLP 任务中，识别一个或所有少数类别的能力至少与大类的表现同样重要。例如，Twitter
    上非仇恨言论的频率远高于仇恨言论（Waseem 和 Hovy，[2016](#bib.bib97)），但识别仇恨内容是仇恨言论检测的关键动机。哪些类别重要甚至可能取决于下游考虑，即同一个命名实体标注器可能在一个对大类重要的应用中使用，而在另一个对少数类别更重要的应用中使用。有几个评估指标被设计用来处理类别不平衡的设置，但没有事实上的标准。例如，平衡准确性（Brodersen
    等，[2010](#bib.bib8)）对应于每个类别召回率的平均值。记录所有类别的表现并报告宏平均值通常很有用，因为它们将所有类别视为平等。
- en: 3 Methods for Addressing Class Imbalance in NLP
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 解决 NLP 中类别不平衡的方法
- en: In this section, we survey methods that either have been explicitly proposed
    to address class-imbalance issues in NLP or that have been empirically shown to
    be applicable for NLP problems. We provide an overview of which methods are applicable
    to a selection of NLP tasks in Appendix [A](#A1 "Appendix A Method Overview ‣
    A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural
    Language Processing").
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们调查了那些明确提出以解决 NLP 中类别不平衡问题的方法，或那些在 NLP 问题中经过实证证明适用的方法。我们在附录 [A](#A1 "附录
    A 方法概述 ‣ 解决深度学习自然语言处理中的类别不平衡问题的方法调查") 中提供了适用于一系列 NLP 任务的方法概述。
- en: 3.1 Re-Sampling
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 重新采样
- en: To increase the importance of minority instances in training, the label distribution
    can be changed by various sampling strategies. Sampling can either be executed
    once or repeatedly during training (Pouyanfar et al., [2018](#bib.bib68)). In
    random oversampling (ROS), a random choice of minority instances are duplicated,
    whereas in random undersampling (RUS), a random choice of majority instances are
    removed from the dataset. ROS can lead to overfitting and increases training times.
    RUS, however, discards potentially valuable data, but has been shown to work well
    in language-modeling objectives (Mikolov et al., [2013](#bib.bib61)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在训练中增加少数实例的重要性，可以通过各种采样策略来改变标签分布。采样可以在训练过程中执行一次或重复执行（Pouyanfar 等， [2018](#bib.bib68)）。在随机过采样（ROS）中，少数实例被随机选择并重复，而在随机欠采样（RUS）中，随机选择的多数实例被从数据集中删除。ROS可能导致过拟合并增加训练时间。然而，RUS则丢弃了潜在有价值的数据，但在语言建模目标中已被证明效果良好（Mikolov
    等， [2013](#bib.bib61)）。
- en: When applied in DL, ROS outperforms RUS both in synthetic step and linear imbalance
    (Buda et al., [2018](#bib.bib10)) and in binary and multi-class English and Korean
    text classification (Juuti et al., [2020](#bib.bib42); Akhbardeh et al., [2021](#bib.bib3);
    Jang et al., [2021](#bib.bib36)). More flexible variants, e.g., re-sampling only
    a tunable share of classes (Tepper et al., [2020](#bib.bib87)) or interpolating
    between the (imbalanced) data distribution and an almost perfectly balanced distribution
    (Arivazhagan et al., [2019](#bib.bib4)), can also further improve results. Class-aware
    sampling (CAS, Shen et al., [2016](#bib.bib78)), also referred to as class-balanced
    sampling, first chooses a class, and then an instance from this class. Performance-based
    re-sampling during training, following the idea of Pouyanfar et al. ([2018](#bib.bib68)),
    works well in multi-class text classification (Akhbardeh et al., [2021](#bib.bib3)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习（DL）中，随机过采样（ROS）在合成步骤和线性不平衡（Buda 等， [2018](#bib.bib10)）以及在二分类和多分类的英语和韩语文本分类（Juuti
    等， [2020](#bib.bib42)；Akhbardeh 等， [2021](#bib.bib3)；Jang 等， [2021](#bib.bib36)）中都优于随机欠采样（RUS）。更灵活的变体，例如，仅对一部分类别进行重采样（Tepper
    等， [2020](#bib.bib87)）或在（不平衡的）数据分布和几乎完全平衡的分布之间进行插值（Arivazhagan 等， [2019](#bib.bib4)），也能进一步提高结果。基于类别的采样（CAS，Shen
    等， [2016](#bib.bib78)），也称为类别平衡采样，首先选择一个类别，然后从这个类别中选择一个实例。基于性能的重采样，在训练过程中遵循Pouyanfar
    等（ [2018](#bib.bib68)）的思想，在多分类文本分类中表现良好（Akhbardeh 等， [2021](#bib.bib3)）。
- en: Issues in multi-label classification. In multi-label classification, label dependencies
    between majority and minority classes complicate sampling approaches, as over-sampling
    an instance with a minority label may simultaneously amplify the majority class
    count (Charte et al., [2015](#bib.bib12); Huang et al., [2021](#bib.bib35)). CAS
    also suffers from this issue, and additionally introduces within-class imbalance,
    as instances of one class are selected with different probabilities depending
    on the co-assigned labels (Wu et al., [2020](#bib.bib102)). Effective sampling
    in such settings is still an open issue. Existing approaches monitor the class
    distributions during sampling (Charte et al., [2015](#bib.bib12)) or assign instance-based
    sampling probabilities (Gupta et al., [2019b](#bib.bib29); Wu et al., [2020](#bib.bib102)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类中的问题。在多标签分类中，多数类和少数类之间的标签依赖性使得采样方法复杂化，因为对一个少数类标签的实例进行过采样可能同时会放大多数类的数量（Charte
    等， [2015](#bib.bib12)；Huang 等， [2021](#bib.bib35)）。CAS也会受到这个问题的影响，并且还会引入类内不平衡，因为一个类的实例根据共同分配的标签被以不同的概率选择（Wu
    等， [2020](#bib.bib102)）。在这种设置中，如何有效采样仍然是一个未解的问题。现有的方法在采样过程中监控类别分布（Charte 等， [2015](#bib.bib12)）或分配基于实例的采样概率（Gupta
    等， [2019b](#bib.bib29)；Wu 等， [2020](#bib.bib102)）。
- en: 3.2 Data Augmentation
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据增强
- en: Increasing the amount of minority class data during corpus construction, e.g.,
    by writing additional examples or selecting examples to be labeled using Active
    Learning, can mitigate the class imbalance problem to some extent (Cho et al.,
    [2020](#bib.bib15); Ein-Dor et al., [2020](#bib.bib19)). However, this is particularly
    laborious in naturally imbalanced settings as it may require finding “the needle
    in the haystack,” or may lead to biased minority class examples, e.g., due to
    collection via keyword queries. Synthetically generating additional minority instances
    thus is a promising direction. In this section, we survey data augmentation methods
    that have been explicitly proposed to mitigate class imbalance and that have been
    evaluated in combination with DL.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在语料库构建过程中增加少数类数据量，例如，通过编写额外示例或使用主动学习选择需要标记的示例，可以在一定程度上缓解类别不平衡问题（Cho 等人，[2020](#bib.bib15)；Ein-Dor
    等人，[2020](#bib.bib19)）。然而，这在自然不平衡的设置中尤其繁琐，因为这可能需要寻找“稻草中的针”，或者可能导致偏向少数类的示例，例如，由于通过关键词查询收集的示例。因此，合成生成额外的少数类实例是一个有前景的方向。在本节中，我们回顾了那些明确提出用于缓解类别不平衡的、并且与深度学习结合评估过的数据增强方法。
- en: Text augmentation generates new natural language instances of minority classes,
    ranging from simple string-based manipulations such as synonym replacements to
    Transformer-based generation. Easy Data Augmentation (EDA, Wei and Zou, [2019](#bib.bib99)),
    which uses dictionary-based synonym replacements, random insertion, random swap,
    and random deletion, has been shown to work well in class-imbalanced settings
    (Jiang et al., [2021](#bib.bib38); Jang et al., [2021](#bib.bib36); Juuti et al.,
    [2020](#bib.bib42)). Juuti et al. ([2020](#bib.bib42)) generate new minority class
    instances for English binary text classification using EDA and embedding-based
    synonym replacements, and by adding a random majority class sentence to a minority
    class document. They also prompt the pre-trained language model GPT-2 (Radford
    et al., [2019](#bib.bib71)) with a minority class instance to generate new minority
    class samples. Tepper et al. ([2020](#bib.bib87)) evaluate generation with GPT-2
    on English multi-class text classification datasets, coupled with a flexible balancing
    policy (see Sec. [3.1](#S3.SS1 "3.1 Re-Sampling ‣ 3 Methods for Addressing Class
    Imbalance in NLP ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing")).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 文本增强生成了少数类的新自然语言实例，从简单的字符串操作（如同义词替换）到基于 Transformer 的生成。Easy Data Augmentation
    (EDA，Wei 和 Zou，[2019](#bib.bib99))，该方法使用基于词典的同义词替换、随机插入、随机交换和随机删除，已在类别不平衡的设置中显示出良好的效果（Jiang
    等人，[2021](#bib.bib38)；Jang 等人，[2021](#bib.bib36)；Juuti 等人，[2020](#bib.bib42)）。Juuti
    等人（[2020](#bib.bib42)）使用 EDA 和基于嵌入的同义词替换为英语二分类文本分类生成新的少数类实例，并通过将随机的多数类句子添加到少数类文档中来实现。他们还使用少数类实例提示预训练语言模型
    GPT-2（Radford 等人，[2019](#bib.bib71)）生成新的少数类样本。Tepper 等人（[2020](#bib.bib87)）评估了在英语多类文本分类数据集上使用
    GPT-2 生成的效果，并结合了灵活的平衡策略（见第 [3.1](#S3.SS1 "3.1 Re-Sampling ‣ 3 Methods for Addressing
    Class Imbalance in NLP ‣ A Survey of Methods for Addressing Class Imbalance in
    Deep-Learning Based Natural Language Processing") 节）。
- en: Similarly, Gaspers et al. ([2020](#bib.bib26)) combine machine-translation based
    text augmentation with dataset balancing to build a multi-task model. Both the
    main and auxiliary tasks are German intent classification. Only the training data
    for the latter is balanced and enriched with synthetic minority instances. In
    a long-tailed multi-label setting, Zhang et al. ([2022](#bib.bib112)) learn an
    attention-based text augmentation that augments instances with text segments that
    are relevant to tail classes, leading to small improvements. In general, transferring
    methods such as EDA or backtranslation to multi-label settings is difficult (Zhang
    et al., [2022](#bib.bib112), [2020](#bib.bib111); Tang et al., [2020](#bib.bib84)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Gaspers 等人（[2020](#bib.bib26)）将基于机器翻译的文本增强与数据集平衡结合起来，构建了一个多任务模型。主要任务和辅助任务都是德语意图分类。只有后者的训练数据经过平衡和用合成少数类实例进行丰富。在长尾多标签设置中，Zhang
    等人（[2022](#bib.bib112)）学习了一种基于注意力的文本增强方法，该方法通过与尾部类别相关的文本片段来增强实例，从而带来了小幅改进。一般而言，将诸如
    EDA 或反向翻译的方法转移到多标签设置中是困难的（Zhang 等人，[2022](#bib.bib112)，[2020](#bib.bib111)；Tang
    等人，[2020](#bib.bib84)）。
- en: Hidden space augmentation generates new instance vectors that are not directly
    associated with a particular natural language string, leveraging the representations
    of real examples. Using representation-based augmentations to tackle class imbalance
    is not tied to DL. SMOTE (Chawla et al., [2002](#bib.bib13)), which interpolates
    minority instances with randomly chosen examples from their K-nearest neighbours,
    is popular in traditional machine learning (Fernández et al., [2018](#bib.bib23)),
    but leads to mixed results in DL-based NLP (Ek and Ghanimifard, [2019](#bib.bib20);
    Tran and Litman, [2021](#bib.bib91); Wei et al., [2022](#bib.bib100)). Inspired
    by CutMix (Yun et al., [2019](#bib.bib110)), which cuts and pastes a single pixel
    region in an image, TextCut (Jiang et al., [2021](#bib.bib38)) randomly replaces
    small parts of the BERT representation of one instance with those of the other.
    In binary and multi-class text classification experiments, TextCut improves over
    non-augmented BERT and EDA.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 隐空间增强生成的新实例向量与特定自然语言字符串没有直接关联，利用了真实示例的表示。利用基于表示的增强方法来应对类别不平衡并不局限于深度学习。SMOTE（Chawla
    et al., [2002](#bib.bib13)），通过与其 K 最近邻的随机选择样本进行插值，广泛应用于传统机器学习（Fernández et al.,
    [2018](#bib.bib23)），但在深度学习的自然语言处理（Ek and Ghanimifard, [2019](#bib.bib20); Tran
    and Litman, [2021](#bib.bib91); Wei et al., [2022](#bib.bib100)）中效果不一。受 CutMix（Yun
    et al., [2019](#bib.bib110)）的启发，CutMix 在图像中剪切并粘贴单个像素区域，TextCut（Jiang et al., [2021](#bib.bib38)）随机替换一个实例的
    BERT 表示的部分。实验表明，TextCut 在二分类和多分类文本分类中优于未增强的 BERT 和 EDA。
- en: Good-enough example extrapolation (GE3, Wei, [2021](#bib.bib98)) and Reprint
    (Wei et al., [2022](#bib.bib100)) also operate in the original representation
    space. To synthesize a new minority instance, GE3 adds the vector representing
    the difference between a majority class instance and the centroid of the respective
    majority class to the mean of a minority class. Evaluations on synthetically step-imbalanced
    English multi-class text classification datasets show improvements over oversampling
    and hidden space augmentation baselines. GE3 assumes that the distribution of
    data points of a class around its mean can be extrapolated to other classes, an
    assumption potentially hurting performance if the minority class distribution
    differs. To account for this when subtracting out majority characteristics, Reprint
    performs a principal component analysis (PCA) for each class, leveraging the information
    on relevant dimensions during sample generation. This method usually outperforms
    GE3, with the cost of an additional hyperparameter (subspace dimensionality).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 好的示例外推（GE3, Wei, [2021](#bib.bib98)）和重印（Wei et al., [2022](#bib.bib100)）也在原始表示空间中操作。为了合成新的少数类实例，GE3
    将表示多数类实例与相应多数类质心之间差异的向量加到少数类的均值上。对合成步骤不平衡的英文多类别文本分类数据集的评估显示，相较于过采样和隐空间增强基准，GE3
    有所改进。GE3 假设某一类的数据点分布可以外推到其他类别，如果少数类分布不同，这一假设可能会影响性能。为了解决这个问题，重印在减去多数特征时，对每一类执行主成分分析（PCA），在样本生成过程中利用相关维度的信息。这种方法通常优于
    GE3，但需要额外的超参数（子空间维度）。
- en: MISO (Tian et al., [2021](#bib.bib89)) generates new instances by transforming
    the representations of minority class instances that are located nearby majority
    class instances. They learn a mapping from minority instance vectors to “disentangled”
    representations, making use of mutual information estimators (Belghazi et al.,
    [2018](#bib.bib5)) to push these representations away from the majority class
    and closer to the minority class. An adversarially-trained generator then generates
    minority instances using these disentangled representations. [Tian et al.](#bib.bib89)
    apply MISO in naturally and synthetically imbalanced English and Chinese binary
    and multi-class text classification with a single minority class.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MISO（Tian et al., [2021](#bib.bib89)）通过转换靠近多数类实例的少数类实例的表示来生成新实例。他们学习从少数实例向量到“解耦”表示的映射，利用互信息估计器（Belghazi
    et al., [2018](#bib.bib5)）将这些表示推离多数类，更接近少数类。一个对抗训练的生成器随后利用这些解耦的表示生成少数实例。[Tian
    et al.](#bib.bib89) 将 MISO 应用于自然和合成不平衡的英文和中文二分类及多分类文本分类任务中，针对单一少数类。
- en: ECRT (Chen et al., [2021](#bib.bib14)) learns to map encoder representations
    (feature space) to a new space (source space) whose components are independent
    of each other given the class, assuming an invariant causal mechanism from source
    to feature space. The independence enables them to generate new meaningful minority
    examples by permuting or sampling components in the source space, resulting in
    medium improvements on a large multi-label text classification dataset with many
    labels.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ECRT（Chen 等，[2021](#bib.bib14)）学习将编码器表示（特征空间）映射到一个新的空间（源空间），其组件在给定类别的条件下相互独立，假设从源到特征空间的因果机制是不变的。独立性使得他们能够通过在源空间中置换或采样组件生成新的有意义的少数类示例，从而在具有多个标签的大型多标签文本分类数据集上取得中等改进。
- en: Further related work exists in the area of transfer learning (Ruder et al.,
    [2019](#bib.bib74)), e.g., from additional datasets that provide complementary
    information on minority classes. For instance, Spangher et al. ([2021](#bib.bib80))
    achieve small gains by manually selecting auxiliary datasets to improve imbalanced
    sentence-based discourse classification. However, complementary datasets have
    to be retrieved for each application, and task loss coefficients have to be tuned.
    Adapting methods to predict useful transfer sources (Lange et al., [2021](#bib.bib48))
    might help alleviate these problems.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习领域存在进一步相关的研究（Ruder 等，[2019](#bib.bib74)），例如，从额外的数据集提供对少数类的补充信息。例如，Spangher
    等（[2021](#bib.bib80)）通过手动选择辅助数据集来改善不平衡的基于句子的语篇分类，取得了小幅提升。然而，补充数据集需要为每个应用检索，并且任务损失系数需要调节。适应方法以预测有用的迁移来源（Lange
    等，[2021](#bib.bib48)）可能有助于缓解这些问题。
- en: 3.3 Loss Functions
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 损失函数
- en: Standard cross-entropy loss (CE) is composed from the predictions for instances
    that carry the label in the gold standard, which is why the resulting classifiers
    fit the minority classes less well. In this section, we summarize loss functions
    designed for imbalanced scenarios. They either re-weight instances by class membership
    or prediction difficulty, or explicitly model class margins to change the decision
    boundary. Throughout this section, we use the variables and terms as shown in
    Table [1](#S3.T1 "Table 1 ‣ Losses for Single-Label Scenarios. ‣ 3.3 Loss Functions
    ‣ 3 Methods for Addressing Class Imbalance in NLP ‣ A Survey of Methods for Addressing
    Class Imbalance in Deep-Learning Based Natural Language Processing").
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 标准交叉熵损失（CE）由带有金标准标签的实例的预测组成，这就是为什么结果分类器对少数类的适应较差的原因。在这一部分，我们总结了为不平衡场景设计的损失函数。它们要么通过类别成员资格或预测难度重新加权实例，要么显式建模类别边际以改变决策边界。在本节中，我们使用表
    [1](#S3.T1 "Table 1 ‣ Losses for Single-Label Scenarios. ‣ 3.3 Loss Functions
    ‣ 3 Methods for Addressing Class Imbalance in NLP ‣ A Survey of Methods for Addressing
    Class Imbalance in Deep-Learning Based Natural Language Processing") 中显示的变量和术语。
- en: Losses for Single-Label Scenarios.
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单标签场景的损失。
- en: Weighted cross-entropy (WCE) uses class-specific weights $\alpha_{j}$ that are
    tuned as hyperparameters or set to the inverse class frequency (e.g., Adel et al.,
    [2017](#bib.bib1); Tayyar Madabushi et al., [2019](#bib.bib86); Li and Xiao, [2020](#bib.bib49)).
    While WCE treats all instances of one class in the same way, focal loss (FL, Lin
    et al., [2017](#bib.bib55)) down-weights instances for which the model is already
    confident (implemented with the $\left(1-p_{j}\right)^{\beta}$ coefficient). FL
    can of course also be used with class weights. Instead of mimicking accuracy like
    CE, dice loss (Dice, Milletari et al., [2016](#bib.bib62)) tries to capture class-wise
    F1 score, with the predicted probability $p_{j}$ proxying precision and the ground
    truth indicator $y_{j}$ proxying recall. Self-adjusting dice loss (ADL, Li et al.,
    [2020](#bib.bib52)) combines confidence-based down-weighting via $1-p_{j}$ with
    Dice loss. For sequence labeling, QA and matching on English and Chinese datasets,
    Dice performs better than FL and ADL.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 加权交叉熵（WCE）使用类特定权重 $\alpha_{j}$，这些权重作为超参数进行调节，或设置为逆类频率（例如，Adel 等，[2017](#bib.bib1)；Tayyar
    Madabushi 等，[2019](#bib.bib86)；Li 和 Xiao，[2020](#bib.bib49)）。虽然 WCE 对同一类别的所有实例采取相同处理方式，但焦点损失（FL，Lin
    等，[2017](#bib.bib55)）则通过 $\left(1-p_{j}\right)^{\beta}$ 系数对模型已经自信的实例进行下调权重。FL
    当然也可以与类别权重一起使用。与 CE 模拟准确率不同，Dice 损失（Dice，Milletari 等，[2016](#bib.bib62)）试图捕捉类别
    F1 分数，预测概率 $p_{j}$ 代理精度，真实值指示符 $y_{j}$ 代理召回率。自适应 Dice 损失（ADL，Li 等，[2020](#bib.bib52)）结合了通过
    $1-p_{j}$ 进行的基于置信度的下调权重和 Dice 损失。在序列标注、问答和英中数据集上的匹配中，Dice 表现优于 FL 和 ADL。
- en: Rather than re-weighting instances, label-distribution-aware margin loss (LDAM,
    Cao et al., [2019](#bib.bib11)), essentially a smooth hinge loss with label-dependent
    margins $\Delta_{j}$, aims to increase the distance of the minority class instances
    to the decision boundary with the aim of better generalization for these classes.
    [Cao et al.](#bib.bib11)’s evaluation largely focuses on computer vision, but
    they also report results for LDAM on a synthetically imbalanced version of the
    IMDB review dataset (Maas et al., [2011](#bib.bib58)), achieving a much lower
    error on the minority class than vanilla CE or CE with re-sampling or re-weighting.
    Subramanian et al. ([2021](#bib.bib81)) propose LDAM variants that consider bias
    related to socially salient groups (e.g., gender-based bias) in addition to class
    imbalance, evaluating them on binary text classification.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与其重新加权实例，不如使用标签分布感知边际损失 (LDAM, Cao 等，[2019](#bib.bib11))，这本质上是一种具有标签依赖边际 $\Delta_{j}$
    的平滑铰链损失，旨在增加少数类实例与决策边界之间的距离，以便更好地对这些类别进行泛化。[Cao 等](#bib.bib11) 的评估主要集中在计算机视觉上，但他们还报告了在
    IMDB 评论数据集（Maas 等，[2011](#bib.bib58)）的合成不平衡版本上 LDAM 的结果，相比于原始 CE 或 CE 与重采样或重加权，少数类的错误率显著降低。Subramanian
    等 ([2021](#bib.bib81)) 提出了考虑与社会显著群体（如性别偏差）相关的偏差的 LDAM 变体，并在二分类文本分类中进行了评估。
- en: '| Single-label | CE | $-\sum_{j=1}^{C}y_{j}\log p_{j}$                 WCE
    $-\sum_{j=1}^{C}\alpha_{j}y_{j}\log p_{j}$ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 单标签 | CE | $-\sum_{j=1}^{C}y_{j}\log p_{j}$                 WCE $-\sum_{j=1}^{C}\alpha_{j}y_{j}\log
    p_{j}$ |'
- en: '| FL | $-\sum_{j=1}^{C}y_{j}(1-p_{j})^{\beta}\log p_{j}$ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| FL | $-\sum_{j=1}^{C}y_{j}(1-p_{j})^{\beta}\log p_{j}$ |'
- en: '| Dice | $\sum_{j=1}^{C}1-\displaystyle\frac{2p_{j}y_{j}+\gamma}{p_{j}^{2}+y_{j}^{2}+\gamma}$
             ADL $\sum_{j=1}^{C}1-\displaystyle\frac{2(1-p_{j})p_{j}y_{j}+\gamma}{(1-p_{j})p_{j}+y_{j}+\gamma}$
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Dice | $\sum_{j=1}^{C}1-\displaystyle\frac{2p_{j}y_{j}+\gamma}{p_{j}^{2}+y_{j}^{2}+\gamma}$
             ADL $\sum_{j=1}^{C}1-\displaystyle\frac{2(1-p_{j})p_{j}y_{j}+\gamma}{(1-p_{j})p_{j}+y_{j}+\gamma}$
    |'
- en: '|  | LDAM | $-\sum_{j=1}^{C}y_{j}\log\displaystyle\frac{\exp(z_{j}-\Delta_{j})}{\exp(z_{j}-\Delta_{j})+\sum_{l\neq
    j}\exp(z_{l})}$ with $\Delta_{j}=K/n_{j}^{1/4}$ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | LDAM | $-\sum_{j=1}^{C}y_{j}\log\displaystyle\frac{\exp(z_{j}-\Delta_{j})}{\exp(z_{j}-\Delta_{j})+\sum_{l\neq
    j}\exp(z_{l})}$ 其中 $\Delta_{j}=K/n_{j}^{1/4}$ |'
- en: '|  | RL | $\mathds{1}(gt\neq A)\log(1+\exp(\rho(m^{+}-z_{gt})))+\log(1+\exp(\rho(m^{-}-z_{c^{-}})))$
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | RL | $\mathds{1}(gt\neq A)\log(1+\exp(\rho(m^{+}-z_{gt})))+\log(1+\exp(\rho(m^{-}-z_{c^{-}})))$
    |'
- en: '| Multi-label | BCE | $-\sum_{j=1}^{C}[y_{j}\log p_{j}+(1-y_{j})\log(1-p_{j})]$
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 多标签 | BCE | $-\sum_{j=1}^{C}[y_{j}\log p_{j}+(1-y_{j})\log(1-p_{j})]$ |'
- en: '| WBCE | $-\sum_{j=1}^{C}\alpha_{j}[y_{j}\log p_{j}+(1-y_{j})\log(1-p_{j})]$
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| WBCE | $-\sum_{j=1}^{C}\alpha_{j}[y_{j}\log p_{j}+(1-y_{j})\log(1-p_{j})]$
    |'
- en: '| FL | $-\sum_{j=1}^{C}[y_{j}(1-p_{j})^{\beta}\log p_{j}+(1-y_{j})p_{j}^{\beta}\log(1-p_{j})]$
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| FL | $-\sum_{j=1}^{C}[y_{j}(1-p_{j})^{\beta}\log p_{j}+(1-y_{j})p_{j}^{\beta}\log(1-p_{j})]$
    |'
- en: '|  | DB | $-\sum_{j=1}^{C}[y_{j}\hat{\alpha}_{j}(1-q_{j})^{\beta}\log q_{j}+(1-y_{j})\hat{\alpha}_{j}\frac{1}{\lambda}q_{j}^{\beta}\log(1-q_{j})]$
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | DB | $-\sum_{j=1}^{C}[y_{j}\hat{\alpha}_{j}(1-q_{j})^{\beta}\log q_{j}+(1-y_{j})\hat{\alpha}_{j}\frac{1}{\lambda}q_{j}^{\beta}\log(1-q_{j})]$
    |'
- en: '|  |  | with $q_{j}=y_{j}\sigma(z_{j}-v_{j})+(1-y_{j})\sigma(\lambda(z_{j}-v_{j}))$
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 其中 $q_{j}=y_{j}\sigma(z_{j}-v_{j})+(1-y_{j})\sigma(\lambda(z_{j}-v_{j}))$
    |'
- en: '| $C$ | number of classes |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| $C$ | 类别数 |'
- en: '| $y$ | target vector |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| $y$ | 目标向量 |'
- en: '| $p$ | model prediction vector |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| $p$ | 模型预测向量 |'
- en: '| $\alpha$ | class weights |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$ | 类别权重 |'
- en: '| $\beta$ | tunable focusing parameter |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| $\beta$ | 可调焦点参数 |'
- en: '| $z$ | model logits vector |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| $z$ | 模型 logits 向量 |'
- en: '| $\gamma$ | smoothing constant |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma$ | 平滑常数 |'
- en: '| $n_{j}$ | size of class $j$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| $n_{j}$ | 类别 $j$ 的大小 |'
- en: '| $K$ | label-independent constant |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| $K$ | 标签无关常数 |'
- en: '| $gt$ | index of ground-truth class |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| $gt$ | 真实类别的索引 |'
- en: '| $m^{+}$ | margin to correct class |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| $m^{+}$ | 到正确类别的边际 |'
- en: '| $m^{-}$ | … to most competitive incorrect class |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $m^{-}$ | … 到最具竞争力的错误类别 |'
- en: '| $A$ | special catch-all class |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| $A$ | 特殊的包容类别 |'
- en: '| $c^{-}$ | index of largest non-$gt$ logit |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| $c^{-}$ | 最大的非 $gt$ logits 的索引 |'
- en: '| $\lambda$ | scaling factor |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda$ | 缩放因子 |'
- en: '| $\hat{\alpha}_{j}$ | instance-specific class weights |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| $\hat{\alpha}_{j}$ | 实例特定的类别权重 |'
- en: '| $v_{j}$ | class-specific bias |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| $v_{j}$ | 类别特定的偏差 |'
- en: 'Table 1: Overview of loss functions formulated for one instance. See Appendix [A](#A1
    "Appendix A Method Overview ‣ A Survey of Methods for Addressing Class Imbalance
    in Deep-Learning Based Natural Language Processing") for references/implementations.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 针对单实例制定的损失函数概述。有关参考文献/实现，请参见附录 [A](#A1 "附录 A 方法概述 ‣ 处理深度学习自然语言处理中的类别不平衡的方法调查")。'
- en: In settings with a large artificial and potentially heterogeneous catch-all
    class (see Sec. [2.2](#S2.SS2 "2.2 Controlled vs. Real-World Class Imbalance ‣
    2 Problem Definition ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing")), many areas of the space contain representations
    of the catch-all class. Here, vanilla LDAM might be an appropriate loss function
    as it encourages larger margins for minority classes. In such cases, ranking losses
    (RL) can also be effective to incentivize the model to only pay attention to “real”
    classes. On an imbalanced English multi-class dataset with a large catch-all class,
    Adel et al. ([2017](#bib.bib1)) find a ranking loss introduced by dos Santos et al.
    ([2015](#bib.bib18)) improves over CE and WCE. For minority classes, this loss
    function maximizes the score of the correct label $z_{gt}$ while at the same time
    minimizing the score of the highest-scoring incorrect label $z_{c^{-}}$. For the
    catch-all class $A$, only $z_{c^{-}}$ is minimized; $z_{gt}$ is ignored. Similarly,
    Hu et al. ([2022](#bib.bib34)) apply class weights only to non-catch-all classes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有大型人工和潜在异质的通用类别的设置中（见第[2.2节](#S2.SS2 "2.2 Controlled vs. Real-World Class
    Imbalance ‣ 2 Problem Definition ‣ A Survey of Methods for Addressing Class Imbalance
    in Deep-Learning Based Natural Language Processing")），空间中的许多区域包含通用类别的表示。在这种情况下，原始LDAM可能是一个合适的损失函数，因为它鼓励少数类有更大的间隔。在这种情况下，排名损失（RL）也可以有效地激励模型仅关注“真实”类别。在一个不平衡的英语多类别数据集上，具有大型通用类别，Adel等人（[2017](#bib.bib1)）发现由dos
    Santos等人（[2015](#bib.bib18)）引入的排名损失相较于CE和WCE有改进。对于少数类，这个损失函数最大化正确标签$z_{gt}$的分数，同时最小化最高得分的错误标签$z_{c^{-}}$的分数。对于通用类别$A$，仅最小化$z_{c^{-}}$；$z_{gt}$被忽略。同样，Hu等人（[2022](#bib.bib34)）将类权重仅应用于非通用类别。
- en: Losses for Multi-Label Scenarios.
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多标签场景中的损失函数。
- en: In multi-label classification, each label assignment can be viewed as a binary
    decision, hence binary cross-entropy (BCE) is often used here. Under imbalance,
    two issues arise. First, although class-specific weights have been used with BCE
    (e.g., Yang et al., [2020](#bib.bib105)), their effect on minority classes is
    less clear than in the single-label case. For each instance, all classes contribute
    to BCE, with the labels not assigned to the instance (called negative classes)
    included via $(1-y_{j})\log(1-p_{j})$. Thus, if weighted binary cross-entropy
    (WBCE) uses a high weight for a class, it also increases the importance of negative
    instances for a minority class, which may further encourage the model to not predict
    this minority class.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在多标签分类中，每个标签分配可以视为一个二分类决策，因此通常使用二分类交叉熵（BCE）。在类别不平衡的情况下，会出现两个问题。首先，尽管在BCE中使用了类特定的权重（例如，Yang等人，[2020](#bib.bib105)），但这些权重对少数类的影响比单标签情况要不明确。对于每个实例，所有类别都对BCE有贡献，未分配给该实例的标签（称为负类）通过$(1-y_{j})\log(1-p_{j})$包含在内。因此，如果加权二分类交叉熵（WBCE）对某个类别使用较高的权重，它也会增加少数类负实例的重要性，这可能进一步促使模型不预测这个少数类。
- en: To leverage class weights more effectively in BCE, one option is to only apply
    them to the loss of positive instances as proposed for multi-label image classification
    (Kumar et al., [2018](#bib.bib46)). Related work includes uniformly upweighting
    positive instances of all classes in hierarchical multi-label text classification
    (e.g., Rathnayaka et al., [2019](#bib.bib73)). An approach to multi-label emotion
    classification by Yilmaz et al. ([2021](#bib.bib108)) performs training time balancing
    by adapting FL such that for a given mini-batch the loss over all instances in
    this mini-batch has exactly the same value for every class.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在BCE中更有效地利用类权重，一个选项是仅将它们应用于正实例的损失，如在多标签图像分类中所提议的（Kumar等人，[2018](#bib.bib46)）。相关工作包括在层次多标签文本分类中均匀地增加所有类别的正实例权重（例如，Rathnayaka等人，[2019](#bib.bib73)）。Yilmaz等人（[2021](#bib.bib108)）提出了一种多标签情感分类的方法，通过调整FL进行训练时间平衡，使得给定的小批量中所有实例的损失在每个类别上具有完全相同的值。
- en: If a classifier already correctly predicts a negative class for an instance,
    the loss can be further decreased by reducing the respective label’s logits. In
    CE, due to the softmax that uses the logits of all classes, the impact of this
    effect becomes minor once the logit for the correct class is much larger than
    those of the other classes. However, the problem is more severe in BCE (Wu et al.,
    [2020](#bib.bib102)), as logits are treated independently. As minority labels
    mostly occur as negative classes, this logit suppression leads to a bias in the
    decision boundary, making it less likely for minority classes to be predicted.
    To tackle this issue and based on a multi-label version of FL, Wu et al. ([2020](#bib.bib102))
    propose distribution-balanced loss (DB) for object detection, adding Negative
    Tolerant Regularization for the loss for negative classes by transforming the
    logits of positive and negative classes differently (see $q_{j}$ in Table [1](#S3.T1
    "Table 1 ‣ Losses for Single-Label Scenarios. ‣ 3.3 Loss Functions ‣ 3 Methods
    for Addressing Class Imbalance in NLP ‣ A Survey of Methods for Addressing Class
    Imbalance in Deep-Learning Based Natural Language Processing")). This regularization
    imposes a sharp drop in the loss function for negative classes once the respective
    logit is below a threshold. Moreover, DB introduces instance-specific class weights
    $\hat{\alpha}$ to account for imbalances caused by class-aware sampling (see Sec. [3.1](#S3.SS1
    "3.1 Re-Sampling ‣ 3 Methods for Addressing Class Imbalance in NLP ‣ A Survey
    of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language
    Processing")) in multi-label scenarios. These weights reflect the frequency of
    a class and the quantity and frequency of the positive labels of the instance.
    Huang et al. ([2021](#bib.bib35)) have shown large improvements of DB over BCE
    even when using uniform sampling on two long-tailed multi-label English text classification
    datasets.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分类器已经正确预测了一个实例的负类，通过降低相应标签的logits，损失可以进一步减少。在CE中，由于softmax使用所有类别的logits，一旦正确类别的logit远大于其他类别的logit，这种效果的影响变得较小。然而，在BCE中（Wu
    et al., [2020](#bib.bib102)），由于logits被独立处理，这个问题更加严重。由于少数标签大多作为负类出现，这种logit压制导致决策边界偏差，使得少数类被预测的可能性降低。为了解决这个问题，Wu
    et al. ([2020](#bib.bib102)) 基于FL的多标签版本，提出了分布平衡损失（DB）用于目标检测，通过以不同方式转换正类和负类的logits，添加负类的负容忍正则化（见表 [1](#S3.T1
    "Table 1 ‣ Losses for Single-Label Scenarios. ‣ 3.3 Loss Functions ‣ 3 Methods
    for Addressing Class Imbalance in NLP ‣ A Survey of Methods for Addressing Class
    Imbalance in Deep-Learning Based Natural Language Processing")）。这种正则化一旦相应的logit低于阈值，会在负类的损失函数中施加一个急剧的下降。此外，DB引入了实例特定的类别权重$\hat{\alpha}$，以考虑多标签场景中由类别感知采样造成的不平衡（见第[3.1节](#S3.SS1
    "3.1 Re-Sampling ‣ 3 Methods for Addressing Class Imbalance in NLP ‣ A Survey
    of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language
    Processing")）。这些权重反映了类别的频率以及实例的正标签的数量和频率。Huang et al. ([2021](#bib.bib35)) 已经展示了即使在对两个长尾多标签英语文本分类数据集使用均匀采样时，DB相较于BCE也有显著改进。
- en: While Cao et al. ([2019](#bib.bib11)) propose and theoretically justify LDAM
    for single-label classification only, it has been successfully applied to multi-label
    text classification as well (Biswas et al., [2021](#bib.bib6)). Ferreira and Vlachos
    ([2019](#bib.bib24)) show that applying a cross-label dependency loss (Yeh et al.,
    [2017](#bib.bib107); Zhang and Zhou, [2006](#bib.bib113)) can be helpful for multi-label
    stance classification. Similarly, Lin et al. ([2019](#bib.bib54)) introduce a
    label-confusion aware cost factor into their loss function. The adaptive loss
    of Suresh and Ong ([2021](#bib.bib82)) integrates inter-label relationships into
    a contrastive loss (Khosla et al., [2020](#bib.bib44)), which compares the score
    of a positive example with the distance to that of other positive and negative
    examples in order to push its representation closer to the correct class and further
    away from the wrong class(es). The resulting loss function learns how to increase
    the weight of confusable negative labels relative to other negative labels. Combining
    label-confusion aware loss functions with class weighting techniques is a promising
    research direction.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Cao等人（[2019](#bib.bib11)）仅提出并理论上证明了LDAM在单标签分类中的有效性，但它也已成功应用于多标签文本分类（Biswas等，[2021](#bib.bib6)）。Ferreira和Vlachos（[2019](#bib.bib24)）展示了应用跨标签依赖损失（Yeh等，[2017](#bib.bib107)；Zhang和Zhou，[2006](#bib.bib113)）对多标签立场分类有帮助。同样，Lin等人（[2019](#bib.bib54)）在其损失函数中引入了标签混淆感知成本因子。Suresh和Ong（[2021](#bib.bib82)）的自适应损失将标签间关系集成到对比损失（Khosla等，[2020](#bib.bib44)）中，该对比损失比较了正样本的得分与其他正样本和负样本的距离，以推动其表示更接近正确类别，远离错误类别。最终的损失函数学习如何相对于其他负标签增加混淆负标签的权重。将标签混淆感知损失函数与类别加权技术相结合是一个有前途的研究方向。
- en: Re-Sampling vs. Loss Functions.
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重采样与损失函数。
- en: Re-sampling and loss functions that are specifically designed for class-imbalanced
    settings are based on the same idea of increasing the importance of minority instances.
    Re-sampling is conceptually simpler and has a direct impact on training time,
    e.g., oversampling may cause a considerable increase. By contrast, the loss functions
    explained above are more flexible, e.g., by modeling desirable properties of margins,
    but also mostly harder to interpret.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 专门为类别不平衡设置设计的重采样和损失函数基于相同的理念，即增加少数实例的重要性。重采样在概念上更简单，对训练时间有直接影响，例如，过采样可能会导致时间显著增加。相比之下，上述解释的损失函数更具灵活性，例如，通过建模边际的期望属性，但通常也更难以解释。
- en: 3.4 Staged Learning
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 阶段性学习
- en: One approach to finding a good trade-off between learning features that are
    representative of the underlying data distribution and reducing the classifier’s
    bias towards the majority class(es) is to perform the training in several stages.
    Two-staged training is common in imbalanced or data-scarce computer vision tasks
    (e.g., Wang et al., [2020b](#bib.bib95), [a](#bib.bib94); Zhang et al., [2021a](#bib.bib115)).
    The first stage usually performs standard training in order to train or fine-tune
    the feature extraction network. Later stages may freeze the feature extractor
    and re-train the classifier layers using special methods to address class imbalance,
    e.g., using more balanced data distributions or specific losses. For example,
    Cao et al. ([2019](#bib.bib11)) find their LDAM loss to be most effective when
    the training happens in two stages. In NLP, deep-learning models are usually based
    on pre-trained neural text encoders or word embeddings. Further domain-specific
    pre-training before starting the fine-tuning stage(s) can also be effective (Gururangan
    et al., [2020](#bib.bib30)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找在学习代表底层数据分布的特征和减少分类器对多数类的偏倚之间良好权衡的一种方法是将训练分为多个阶段。两阶段训练在不平衡或数据稀缺的计算机视觉任务中很常见（例如，Wang等，[2020b](#bib.bib95)，[a](#bib.bib94)；Zhang等，[2021a](#bib.bib115)）。第一阶段通常进行标准训练，以训练或微调特征提取网络。后续阶段可能会冻结特征提取器，并使用特殊方法重新训练分类器层以解决类别不平衡问题，例如，使用更平衡的数据分布或特定的损失函数。例如，Cao等人（[2019](#bib.bib11)）发现他们的LDAM损失在两阶段训练时最为有效。在自然语言处理领域，深度学习模型通常基于预训练的神经文本编码器或词嵌入。在开始微调阶段之前的进一步领域特定的预训练也可能有效（Gururangan等，[2020](#bib.bib30)）。
- en: Several NLP approaches that fall under staged learning are directly inspired
    by computer vision research. In the context of long-tailed image classification,
    Kang et al. ([2020](#bib.bib43)) find that class-balanced sampling (see Sec. [3.1](#S3.SS1
    "3.1 Re-Sampling ‣ 3 Methods for Addressing Class Imbalance in NLP ‣ A Survey
    of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language
    Processing")) helps when performing single-stage training, but that in their two-stage
    classifier re-training (cRT) method, using the original distribution in the first
    stage is more effective than class-balanced sampling. cRT employs the latter only
    in the second stage after freezing the representation weights. Yu et al. ([2020](#bib.bib109))
    perform a similar decoupling analysis on long-tailed relation classification,
    essentially confirming Kang et al. ([2020](#bib.bib43))’s results on this NLP
    task with respect to the re-sampling strategies. Additionally, they find that
    loss re-weighting under this analysis behaves similar to re-sampling, i.e., it
    leads to worse performance when applied during representation learning, but boosts
    performance when re-training the classifier. Hu et al. ([2022](#bib.bib34)) successfully
    leverage [Kang et al.](#bib.bib43)’s ideas for event detection, where both trigger
    detection and trigger classification suffer from class imbalance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 归属于阶段学习的几个自然语言处理（NLP）方法直接受到计算机视觉研究的启发。在长尾图像分类的背景下，Kang 等人（[2020](#bib.bib43)）发现类平衡采样（参见第[3.1节](#S3.SS1
    "3.1 Re-Sampling ‣ 3 Methods for Addressing Class Imbalance in NLP ‣ A Survey
    of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language
    Processing")）在进行单阶段训练时有帮助，但在他们的两阶段分类器重训练（cRT）方法中，第一阶段使用原始分布比类平衡采样更有效。cRT 仅在冻结表示权重后的第二阶段中使用后者。Yu
    等人（[2020](#bib.bib109)）对长尾关系分类进行类似的解耦分析，本质上确认了Kang 等人（[2020](#bib.bib43)）在这个NLP任务中的重新采样策略结果。此外，他们发现这种分析下的损失重新加权行为类似于重新采样，即在表示学习期间应用时会导致性能变差，但在重新训练分类器时会提升性能。Hu
    等人（[2022](#bib.bib34)）成功地利用了[Kang 等人](#bib.bib43)的想法进行事件检测，其中触发器检测和触发器分类都受到类不平衡的困扰。
- en: Jang et al. ([2021](#bib.bib36)) model imbalanced classification as a continual
    learning task with $k$ stages where the data gradually becomes more balanced (sequential
    targeting, ST). The first stage contains the most imbalanced subset, and then
    the degree of imbalance decreases, with the last stage presenting the most balanced
    subset. The training objective encourages both good performance on the current
    stage and keeping information learnt in previous stages. Their experiments include
    binary and ternary English and Korean text classification. Active Learning (AL),
    which contains several stages by definition, has also been shown to boost performance
    of BERT models for minority classes (Ein-Dor et al., [2020](#bib.bib19)). For
    a discussion about AL and deep learning, see Schröder and Niekler ([2020](#bib.bib77)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Jang 等人（[2021](#bib.bib36)）将不平衡分类建模为一个具有 $k$ 个阶段的持续学习任务，其中数据逐渐变得更加平衡（序列目标，ST）。第一阶段包含最不平衡的子集，然后不平衡的程度减少，最后阶段呈现最平衡的子集。训练目标既鼓励当前阶段的良好表现，也保持在前面阶段中学习到的信息。他们的实验包括二元和三元的英语和韩语文本分类。主动学习（AL）定义上包含多个阶段，已被证明可以提升BERT模型对少数类的性能（Ein-Dor
    等人，[2020](#bib.bib19)）。有关主动学习和深度学习的讨论，请参见Schröder 和 Niekler（[2020](#bib.bib77)）。
- en: 3.5 Model Design
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 模型设计
- en: The methods described so far are largely independent of model architecture.
    This section describes model modifications that aim at improving performance in
    imbalanced settings.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止描述的方法在很大程度上与模型架构无关。本节描述了旨在改善不平衡设置中性能的模型修改。
- en: Observing that the weight vectors for smaller classes have smaller norms in
    standard joint training compared to staged-learning based cRT (see Sec. [3.4](#S3.SS4
    "3.4 Staged Learning ‣ 3 Methods for Addressing Class Imbalance in NLP ‣ A Survey
    of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language
    Processing")), Kang et al. ([2020](#bib.bib43)) normalize the classifier weights
    directly in one-staged training using a hyperparameter $\tau$ to control the normalization
    “temperature” ($\tau$-norm). $\tau$-norm achieves similar or better performance
    than cRT in long-tailed image classification and outperforms cRT in relation extraction,
    but cRT works better for named entity recognition and event detection (Nan et al.,
    [2021](#bib.bib64)).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到在标准联合训练中，较小类别的权重向量的范数较小，相比于基于阶段学习的cRT（见第[3.4节](#S3.SS4 "3.4 Staged Learning
    ‣ 3 Methods for Addressing Class Imbalance in NLP ‣ A Survey of Methods for Addressing
    Class Imbalance in Deep-Learning Based Natural Language Processing")），Kang等人（[2020](#bib.bib43)）在单阶段训练中直接对分类器权重进行归一化，使用超参数$\tau$来控制归一化的“温度”（$\tau$-norm）。$\tau$-norm在长尾图像分类中取得了类似或更好的性能，并在关系抽取中优于cRT，但cRT在命名实体识别和事件检测中表现更好（Nan等人，[2021](#bib.bib64)）。
- en: SetConv (Gao et al., [2020](#bib.bib25)) and ProtoBERT (Tänzer et al., [2022](#bib.bib85))
    learn representatives for each class using support sets and classify an input
    (the query) based on its similarity to these representatives. SetConv applies
    convolution kernels that capture intra- and inter-class correlations to extract
    class representatives. ProtoBERT uses class centroids in a learned BERT-based
    feature space, treating the distance of any instance to the catch-all class as
    just another learnable parameter. At each training step, SetConv uses only one
    instance per class in the query set, but preserves the original class imbalance
    in the support set, whereas ProtoBERT uses fixed ratios. In the respective experimental
    studies, ProtoBERT performs better than using a standard classification layer
    on top of BERT for minority classes in NER if less than 100 examples are seen
    by the model, while SetConv excels in binary text classification with higher degrees
    of imbalance and in multi-class text classification.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: SetConv（Gao等人，[2020](#bib.bib25)）和ProtoBERT（Tänzer等人，[2022](#bib.bib85)）通过支持集为每个类别学习代表，并根据输入（查询）与这些代表的相似性进行分类。SetConv应用卷积核以捕捉类内和类间相关性来提取类代表。ProtoBERT在学习的基于BERT的特征空间中使用类质心，将任何实例到通用类的距离视为另一个可学习的参数。在每次训练步骤中，SetConv在查询集中仅使用每个类别的一个实例，但保留了支持集中原始的类别不平衡，而ProtoBERT使用固定比例。在各自的实验研究中，如果模型看到的少于100个样本，ProtoBERT在NER中的表现优于在BERT上使用标准分类层，而SetConv在高不平衡度的二元文本分类和多类文本分类中表现更佳。
- en: The HSCNN model (Yang et al., [2020](#bib.bib105)) uses class representatives
    only for the classification of tail classes, while head classes are assigned using
    a standard text CNN (Kim, [2014](#bib.bib45)). HSCNN learns label-specific similarity
    functions, extracting instance representations from the pre-final layers of two
    copies of the original CNN, and assigns a tail class if the similarity to the
    class representative (computed as the mean of 5 random support instances) exceeds
    0.5. On tail classes, HSCNN consistently improves over the vanilla CNN.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: HSCNN模型（Yang等人，[2020](#bib.bib105)）仅在尾类分类中使用类别代表，而头类则使用标准文本CNN（Kim，[2014](#bib.bib45)）。HSCNN学习标签特定的相似性函数，从原始CNN的两个副本的倒数第二层提取实例表示，如果与类别代表（计算为5个随机支持实例的均值）的相似性超过0.5，则分配为尾类。在尾类上，HSCNN相较于原始CNN表现始终更佳。
- en: In addition, there exist a number of task-specific solutions. Prange et al.
    ([2021](#bib.bib69)) propose to construct CCG supertags from predicted tree structures
    rather than treating the problem as a standard classification task. In order to
    recognize implicit positive interpretations in negated statements in a class-imbalanced
    dataset, van Son et al. ([2018](#bib.bib92)) argue that leveraging information
    structure could be one way to improve inference. Structural causal models (SCMs)
    have been applied to imbalanced NLP tasks, encoding task-specific causal graphs
    (e.g., Nan et al., [2021](#bib.bib64)). Similarly, Wu et al. ([2021](#bib.bib103))
    causally model how bias in long-tailed corpora affects topic modeling (Blei et al.,
    [2003](#bib.bib7)) and use this to improve training of a variational autoencoder.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，存在一些特定任务的解决方案。Prange等人（[2021](#bib.bib69)）建议从预测的树结构中构造CCG超级标记，而不是将问题视为标准分类任务。为了在类不平衡的数据集中识别否定陈述中的隐性正面解释，van
    Son等人（[2018](#bib.bib92)）认为利用信息结构可能是一种改善推理的方法。结构性因果模型（SCMs）已被应用于不平衡的NLP任务中，编码任务特定的因果图（例如，Nan等人，[2021](#bib.bib64)）。类似地，Wu等人（[2021](#bib.bib103)）因果地建模了长尾语料库中的偏差如何影响主题建模（Blei等人，[2003](#bib.bib7)），并利用这一点改善变分自编码器的训练。
- en: A research area closely related to class imbalance is few-shot learning (FSL,
    Wang et al., [2020c](#bib.bib96)), which aims to learn classes based on only very
    few training examples. Model ideas from FSL can be leveraged for long-tailed settings,
    e.g., by making use of relational information about class labels in the form of
    knowledge graph embeddings or other forms of embedding hierarchical relationships
    between labels (Han et al., [2018](#bib.bib31); Zhang et al., [2019](#bib.bib114)),
    or computing label-specific representations (Mullenbach et al., [2018](#bib.bib63)).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与类不平衡密切相关的研究领域是少样本学习（FSL，Wang等人，[2020c](#bib.bib96)），其目标是仅基于极少的训练样本学习类别。FSL中的模型思想可以用于长尾设置，例如，通过利用关于类别标签的关系信息，如知识图谱嵌入或其他形式的标签层次关系嵌入（Han等人，[2018](#bib.bib31)；Zhang等人，[2019](#bib.bib114)），或计算特定标签的表示（Mullenbach等人，[2018](#bib.bib63)）。
- en: 4 Insights and Future Directions
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 见解与未来方向
- en: We have provided a comprehensive, concise and structured overview of current
    approaches to dealing with class imbalance in DL-based NLP.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了当前应对DL-based NLP中类不平衡的全面、简明且结构化的概述。
- en: What works (best)? As there is no established benchmark for class-imbalanced
    settings, evaluation results are hard to compare across papers. In general, re-sampling
    or changing the loss function may lead to small to moderate gains. For data augmentation
    approaches, the reported performance increases tend to be larger than for re-sampling
    or new loss functions. The effects of staged training or modifications of the
    model vary drastically, ranging from detrimental to very large performance gains.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 什么方法效果最佳？由于尚无建立的类不平衡设置的基准，评估结果在各论文之间难以比较。一般而言，重新采样或改变损失函数可能会带来小到中等的提升。对于数据增强方法，报告的性能提升往往大于重新采样或新损失函数。阶段性训练或模型修改的效果差异很大，从有害到非常大的性能提升不等。
- en: Hence, re-sampling, data augmentation, and changing the loss function are straightforward
    choices in class-imbalanced settings. Approaches based on staged learning or model
    design may sometimes outperform them, but often come with a higher implementation
    or computational cost. For a practical decision aid and potential application
    settings of some class imbalance methods, see Figure [3](#A2.F3 "Figure 3 ‣ Appendix
    B Practical advice ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing") in Appendix [B](#A2 "Appendix B Practical
    advice ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based
    Natural Language Processing") and Table [3](#A2.T3 "Table 3 ‣ Appendix B Practical
    advice ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based
    Natural Language Processing") in Appendix [A](#A1 "Appendix A Method Overview
    ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural
    Language Processing").
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在类别不平衡的设置中，重新采样、数据增强和更改损失函数是直接的选择。基于分阶段学习或模型设计的方法有时可能优于这些方法，但通常伴随更高的实现或计算成本。有关一些类别不平衡方法的实际决策辅助和潜在应用设置，请参见附录[B](#A2
    "Appendix B Practical advice ‣ A Survey of Methods for Addressing Class Imbalance
    in Deep-Learning Based Natural Language Processing")中的图[3](#A2.F3 "Figure 3 ‣
    Appendix B Practical advice ‣ A Survey of Methods for Addressing Class Imbalance
    in Deep-Learning Based Natural Language Processing")和附录[A](#A1 "Appendix A Method
    Overview ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing")中的表[3](#A2.T3 "Table 3 ‣ Appendix B Practical
    advice ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based
    Natural Language Processing")。
- en: How should we report results? Much NLP research only reports aggregate statistics
    (Harbecke et al., [2022](#bib.bib32)), making it hard to judge the impact on improvements
    by class, which is often important in practice. We thus argue that NLP researchers
    should always report per-class statistics, e.g., as in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing"). Open-sourcing spreadsheets with the exact
    numbers would enable the community to compare systems more flexibly from multiple
    angles, i.e., with respect to whichever class(es) matter in a particular application
    scenario, and to re-use this data in research on class imbalance. Reviewers should
    also value works that analyze performance for relevant minority classes rather
    than focusing largely only on overall accuracy improvements.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何报告结果？许多自然语言处理研究仅报告**汇总统计数据**（Harbecke 等，[2022](#bib.bib32)），这使得很难判断按类别改进的影响，而这在实践中通常很重要。因此，我们认为自然语言处理研究人员应该始终报告每类的统计数据，例如，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey of Methods for Addressing Class Imbalance
    in Deep-Learning Based Natural Language Processing")所示。开源包含确切数字的电子表格将使社区能够从多个角度更灵活地比较系统，即根据特定应用场景中重要的类别，并在类不平衡研究中重复使用这些数据。审稿人也应该重视分析相关少数类别性能的工作，而不仅仅是关注整体准确度的提高。
- en: A main hindrance to making progress on class imbalance in computer vision and
    NLP alike is that experimental results are often hard to compare (Johnson and
    Khoshgoftaar, [2019a](#bib.bib39), [2020](#bib.bib41)). A first important step
    would be to not restrict baselines to methods of the same type, e.g., a new data
    augmentation approach should not only compare to other data augmentation methods,
    but also to using loss functions for class imbalance. Establishing a shared and
    systematic benchmark of a diverse set of class-imbalanced NLP tasks would be highly
    beneficial for both researchers and practitioners.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉和自然语言处理领域在类不平衡问题上取得进展的一个主要障碍是实验结果往往难以比较（Johnson 和 Khoshgoftaar，[2019a](#bib.bib39)，[2020](#bib.bib41)）。一个重要的第一步是不要将基线限制为同类型的方法，例如，新的数据增强方法不仅应与其他数据增强方法进行比较，还应与用于类不平衡的损失函数进行比较。建立一套共享和系统化的多样化类别不平衡自然语言处理任务的基准将对研究人员和从业者都非常有利。
- en: How can we move forward? Most work on class-imbalanced NLP has focused on single-label
    text classification. Finding good solutions for multi-label settings is still
    an open research challenge. Class imbalance also poses problems in NLP tasks such
    as sequence labeling or parsing, and we believe that the interaction of structured
    prediction models with methods to address class imbalance is a promising area
    for future research. Moreover, we need to study how class imbalance methods affect
    prediction calibration in order to provide reliable confidence estimates.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何前进？大多数关于类不平衡的 NLP 工作集中于单标签文本分类。寻找多标签设置的良好解决方案仍然是一个未解的研究挑战。类不平衡在 NLP 任务中也带来了问题，如序列标注或解析，我们相信结构化预测模型与解决类不平衡方法的结合是未来研究的一个有前景的领域。此外，我们还需要研究类不平衡方法如何影响预测校准，以提供可靠的置信度估计。
- en: In general, inspiration for new model architectures could for example be drawn
    from approaches developed for few-shot learning (Wang et al., [2020c](#bib.bib96)).
    Recently, prompting (Radford et al., [2019](#bib.bib71)) has emerged as a new
    paradigm in NLP, which performs strongly in real-world few-shot settings (Schick
    and Schütze, [2022](#bib.bib76)). Methods that improve worst-case performance
    under distribution shift (e.g., Sagawa et al., [2020](#bib.bib75)) might also
    be applied to improve minority-class performance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，新模型架构的灵感可以从为少样本学习（Wang et al., [2020c](#bib.bib96)）开发的方法中获得。最近，提示（Radford
    et al., [2019](#bib.bib71)）作为自然语言处理中的一种新范式出现，在实际少样本环境中表现强劲（Schick and Schütze,
    [2022](#bib.bib76)）。改进分布变化下最坏情况性能的方法（例如，Sagawa et al., [2020](#bib.bib75)）也可能用于提升少数类的表现。
- en: Acknowledgements
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank the anonymous reviewers for their valuable comments, and Heike Adel,
    Stefan Grünewald, Subhash Pujari, and Timo Schrader for providing data for our
    teaser image. We thank them and Talita Anthonio, Mohamed Gad-Elrab, Lukas Lange,
    Stefan Ott, Robert Schmier, Hendrik Schuff, Daria Stepanova, Jannik Strötgen,
    Thang Vu, and Dan Zhang for helpful discussions and feedback on the writing. We
    also thank Jason Wei and Jiaqi Zeng for answering questions about their work.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢匿名评审员提供的宝贵意见，以及 Heike Adel、Stefan Grünewald、Subhash Pujari 和 Timo Schrader
    为我们的预览图提供的数据。我们感谢他们以及 Talita Anthonio、Mohamed Gad-Elrab、Lukas Lange、Stefan Ott、Robert
    Schmier、Hendrik Schuff、Daria Stepanova、Jannik Strötgen、Thang Vu 和 Dan Zhang 对写作过程中的有益讨论和反馈。我们还感谢
    Jason Wei 和 Jiaqi Zeng 解答了有关他们工作的疑问。
- en: Limitations
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: This paper is a survey, structuring, organizing and describing works and concepts
    to address class imbalance including long-tailed learning. While we touch upon
    data augmentation and few-shot learning, we do not comprehensively review those
    areas. Details on the scope of this review have also been described in Sec. [1](#S1
    "1 Introduction ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing").
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是一项综述，旨在构建、组织和描述解决类不平衡问题的工作和概念，包括长尾学习。虽然我们涉及了数据增强和少样本学习，但未对这些领域进行全面评审。有关本综述范围的详细信息也已在第[1](#S1
    "1 Introduction ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing")节中描述。
- en: The search process for the survey included searching for the keywords class
    imbalance and long tail in Google Scholar and the ACL Anthology, as well as carefully
    checking the papers that cite relevant papers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 调查过程包括在 Google Scholar 和 ACL Anthology 中搜索关键词类不平衡和长尾，并仔细检查引用相关论文的文献。
- en: Finally, the paper only constitutes a literature review, it does not yet provide
    a comprehensive empirical study which is much needed in this research area, but
    it will be of use in carrying out such a study.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本文仅为文献综述，并未提供在该研究领域非常需要的全面实证研究，但对于开展这样的研究将有所帮助。
- en: References
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Adel et al. (2017) Heike Adel, Francine Chen, and Yan-Ying Chen. 2017. [Ranking
    convolutional recurrent neural networks for purchase stage identification on imbalanced
    Twitter data](https://aclanthology.org/E17-2094). In *Proceedings of the 15th
    Conference of the European Chapter of the Association for Computational Linguistics:
    Volume 2, Short Papers*, pages 592–598, Valencia, Spain. Association for Computational
    Linguistics.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adel et al. (2017) Heike Adel, Francine Chen, 和 Yan-Ying Chen. 2017. [在不平衡的
    Twitter 数据上进行购买阶段识别的卷积递归神经网络排名](https://aclanthology.org/E17-2094)。在 *第十五届欧洲计算语言学协会会议：卷2，简短论文*，第
    592–598 页，西班牙瓦伦西亚。计算语言学协会。
- en: 'Ahrens et al. (2021) Kyra Ahrens, Fares Abawi, and Stefan Wermter. 2021. Drill:
    Dynamic representations for imbalanced lifelong learning. In *Artificial Neural
    Networks and Machine Learning – ICANN 2021*, pages 409–420, Cham. Springer International
    Publishing.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ahrens 等（2021）Kyra Ahrens, Fares Abawi, 和 Stefan Wermter. 2021. Drill: 不平衡终身学习的动态表示。
    在 *人工神经网络与机器学习 - ICANN 2021* 中，页码 409–420，Cham。Springer 国际出版社。'
- en: 'Akhbardeh et al. (2021) Farhad Akhbardeh, Cecilia Ovesdotter Alm, Marcos Zampieri,
    and Travis Desell. 2021. [Handling extreme class imbalance in technical logbook
    datasets](https://doi.org/10.18653/v1/2021.acl-long.312). In *Proceedings of the
    59th Annual Meeting of the Association for Computational Linguistics and the 11th
    International Joint Conference on Natural Language Processing (Volume 1: Long
    Papers)*, pages 4034–4045, Online. Association for Computational Linguistics.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akhbardeh 等（2021）Farhad Akhbardeh, Cecilia Ovesdotter Alm, Marcos Zampieri,
    和 Travis Desell. 2021. [处理技术日志数据集中的极端类别不平衡](https://doi.org/10.18653/v1/2021.acl-long.312)。在
    *第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第1卷：长篇论文）论文集* 中，页码 4034–4045，在线。计算语言学协会。
- en: 'Arivazhagan et al. (2019) Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry
    Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George F. Foster,
    Colin Cherry, Wolfgang Macherey, Zhifeng Chen, and Yonghui Wu. 2019. [Massively
    multilingual neural machine translation in the wild: Findings and challenges](http://arxiv.org/abs/1907.05019).
    *CoRR*, abs/1907.05019.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arivazhagan 等（2019）Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin,
    Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George F. Foster, Colin Cherry,
    Wolfgang Macherey, Zhifeng Chen, 和 Yonghui Wu. 2019. [大规模多语言神经机器翻译中的发现与挑战](http://arxiv.org/abs/1907.05019)。*CoRR*，abs/1907.05019。
- en: Belghazi et al. (2018) Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar,
    Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. 2018. [Mutual
    information neural estimation](https://proceedings.mlr.press/v80/belghazi18a.html).
    In *Proceedings of the 35th International Conference on Machine Learning*, volume 80
    of *Proceedings of Machine Learning Research*, pages 531–540\. PMLR.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belghazi 等（2018）Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil
    Ozair, Yoshua Bengio, Aaron Courville, 和 Devon Hjelm. 2018. [互信息神经估计](https://proceedings.mlr.press/v80/belghazi18a.html)。在
    *第35届国际机器学习会议论文集* 中，*机器学习研究论文集* 第80卷，页码 531–540\. PMLR。
- en: 'Biswas et al. (2021) Biplob Biswas, Thai-Hoang Pham, and Ping Zhang. 2021.
    [Transicd: Transformer based code-wise attention model for explainable ICD coding](https://doi.org/10.1007/978-3-030-77211-6_56).
    In *Artificial Intelligence in Medicine - 19th International Conference on Artificial
    Intelligence in Medicine, AIME 2021, Virtual Event, June 15-18, 2021, Proceedings*,
    volume 12721 of *Lecture Notes in Computer Science*, pages 469–478\. Springer.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Biswas 等（2021）Biplob Biswas, Thai-Hoang Pham, 和 Ping Zhang. 2021. [Transicd:
    基于 Transformer 的代码级注意力模型用于可解释的 ICD 编码](https://doi.org/10.1007/978-3-030-77211-6_56)。在
    *人工智能医学 - 第19届人工智能医学国际会议，AIME 2021，虚拟会议，2021年6月15-18日，论文集* 中，*计算机科学讲义笔记* 第12721卷，页码
    469–478\. Springer。'
- en: Blei et al. (2003) David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
    [Latent dirichlet allocation](http://jmlr.org/papers/v3/blei03a.html). *J. Mach.
    Learn. Res.*, 3:993–1022.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blei 等（2003）David M. Blei, Andrew Y. Ng, 和 Michael I. Jordan. 2003. [潜在狄利克雷分配](http://jmlr.org/papers/v3/blei03a.html)。*机器学习研究杂志*，3:993–1022。
- en: Brodersen et al. (2010) Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan,
    and Joachim M. Buhmann. 2010. [The balanced accuracy and its posterior distribution](https://doi.org/10.1109/ICPR.2010.764).
    In *2010 20th International Conference on Pattern Recognition*, pages 3121–3124.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brodersen 等（2010）Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan,
    和 Joachim M. Buhmann. 2010. [平衡准确率及其后验分布](https://doi.org/10.1109/ICPR.2010.764)。在
    *2010年第20届国际模式识别大会* 中，页码 3121–3124。
- en: Bruzzone and Serpico (1997) L. Bruzzone and S.B. Serpico. 1997. [Classification
    of imbalanced remote-sensing data by neural networks](https://doi.org/https://doi.org/10.1016/S0167-8655(97)00109-8).
    *Pattern Recognition Letters*, 18(11):1323–1328.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruzzone 和 Serpico（1997）L. Bruzzone 和 S.B. Serpico. 1997. [通过神经网络对不平衡遥感数据进行分类](https://doi.org/https://doi.org/10.1016/S0167-8655(97)00109-8)。*模式识别通讯*，18(11):1323–1328。
- en: Buda et al. (2018) Mateusz Buda, Atsuto Maki, and Maciej A. Mazurowski. 2018.
    [A systematic study of the class imbalance problem in convolutional neural networks](https://doi.org/https://doi.org/10.1016/j.neunet.2018.07.011).
    *Neural Networks*, 106:249–259.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buda 等（2018）Mateusz Buda, Atsuto Maki, 和 Maciej A. Mazurowski. 2018. [卷积神经网络中的类别不平衡问题的系统研究](https://doi.org/https://doi.org/10.1016/j.neunet.2018.07.011)。*神经网络*，106:249–259。
- en: Cao et al. (2019) Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu
    Ma. 2019. [Learning imbalanced datasets with label-distribution-aware margin loss](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 32\. Curran Associates,
    Inc.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2019) Kaidi Cao、Colin Wei、Adrien Gaidon、Nikos Arechiga 和 Tengyu
    Ma。2019年。 [通过标签分布感知的边际损失学习不平衡数据集](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf)。在
    *神经信息处理系统进展*，第32卷。Curran Associates, Inc.
- en: 'Charte et al. (2015) Francisco Charte, Antonio J. Rivera, María J. del Jesus,
    and Francisco Herrera. 2015. [Addressing imbalance in multilabel classification:
    Measures and random resampling algorithms](https://doi.org/https://doi.org/10.1016/j.neucom.2014.08.091).
    *Neurocomputing*, 163:3–16. Recent Advancements in Hybrid Artificial Intelligence
    Systems and its Application to Real-World Problems Progress in Intelligent Systems
    Mining Humanistic Data.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charte et al. (2015) Francisco Charte、Antonio J. Rivera、María J. del Jesus 和
    Francisco Herrera。2015年。 [处理多标签分类中的不平衡：度量和随机重采样算法](https://doi.org/https://doi.org/10.1016/j.neucom.2014.08.091)。*神经计算*，163:3–16。混合人工智能系统的最新进展及其在实际问题中的应用
    智能系统中的进展 挖掘人文学科数据。
- en: 'Chawla et al. (2002) Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and
    W Philip Kegelmeyer. 2002. Smote: Synthetic minority over-sampling technique.
    *Journal of Artificial Intelligence Research*, 16:321–357.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chawla et al. (2002) Nitesh V Chawla、Kevin W Bowyer、Lawrence O Hall 和 W Philip
    Kegelmeyer。2002年。Smote：合成少数类过采样技术。*人工智能研究期刊*，16:321–357。
- en: Chen et al. (2021) Junya Chen, Zidi Xiu, Benjamin Goldstein, Ricardo Henao,
    Lawrence Carin, and Chenyang Tao. 2021. Supercharging imbalanced data learning
    with energy-based contrastive representation transfer. *Advances in Neural Information
    Processing Systems*, 34.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021) Junya Chen、Zidi Xiu、Benjamin Goldstein、Ricardo Henao、Lawrence
    Carin 和 Chenyang Tao。2021年。通过基于能量的对比表征转移来超充不平衡数据学习。*神经信息处理系统进展*，34。
- en: 'Cho et al. (2020) Won Ik Cho, Youngki Moon, Sangwhan Moon, Seok Min Kim, and
    Nam Soo Kim. 2020. [Machines getting with the program: Understanding intent arguments
    of non-canonical directives](https://doi.org/10.18653/v1/2020.findings-emnlp.31).
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages
    329–339, Online. Association for Computational Linguistics.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho et al. (2020) Won Ik Cho、Youngki Moon、Sangwhan Moon、Seok Min Kim 和 Nam Soo
    Kim。2020年。 [机器适应程序：理解非标准指令的意图参数](https://doi.org/10.18653/v1/2020.findings-emnlp.31)。在
    *计算语言学协会发现：EMNLP 2020*，第329–339页，在线。计算语言学协会。
- en: Deng et al. (2022) Keqi Deng, Gaofeng Cheng, Runyan Yang, and Yonghong Yan.
    2022. [Alleviating asr long-tailed problem by decoupling the learning of representation
    and classification](https://doi.org/10.1109/TASLP.2021.3138707). *IEEE/ACM Transactions
    on Audio, Speech, and Language Processing*, 30:340–354.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2022) Keqi Deng、Gaofeng Cheng、Runyan Yang 和 Yonghong Yan。2022年。
    [通过解耦表征和分类的学习来缓解ASR长尾问题](https://doi.org/10.1109/TASLP.2021.3138707)。*IEEE/ACM音频、语音和语言处理汇刊*，30:340–354。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2019) Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2019年。
    [BERT：用于语言理解的深度双向变换器的预训练](https://doi.org/10.18653/v1/N19-1423)。在 *2019年北美计算语言学协会会议：人类语言技术，第1卷（长篇和短篇论文）*，第4171–4186页，明尼阿波利斯，明尼苏达州。计算语言学协会。
- en: 'dos Santos et al. (2015) Cícero dos Santos, Bing Xiang, and Bowen Zhou. 2015.
    [Classifying relations by ranking with convolutional neural networks](https://doi.org/10.3115/v1/P15-1061).
    In *Proceedings of the 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 626–634, Beijing, China. Association for Computational
    Linguistics.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dos Santos et al. (2015) Cícero dos Santos、Bing Xiang 和 Bowen Zhou。2015年。 [通过卷积神经网络对关系进行排名分类](https://doi.org/10.3115/v1/P15-1061)。在
    *第53届计算语言学协会年会暨第七届国际自然语言处理联合会议（卷1：长篇论文）*，第626–634页，北京，中国。计算语言学协会。
- en: 'Ein-Dor et al. (2020) Liat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,
    Lena Dankin, Leshem Choshen, Marina Danilevsky, Ranit Aharonov, Yoav Katz, and
    Noam Slonim. 2020. [Active Learning for BERT: An Empirical Study](https://doi.org/10.18653/v1/2020.emnlp-main.638).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 7949–7962, Online. Association for Computational Linguistics.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ein-Dor 等 (2020) Liat Ein-Dor、Alon Halfon、Ariel Gera、Eyal Shnarch、Lena Dankin、Leshem
    Choshen、Marina Danilevsky、Ranit Aharonov、Yoav Katz 和 Noam Slonim。2020. [BERT 的主动学习：一项实证研究](https://doi.org/10.18653/v1/2020.emnlp-main.638)。发表于
    *2020 年自然语言处理经验方法会议（EMNLP）论文集*，第7949–7962页，在线。计算语言学协会。
- en: 'Ek and Ghanimifard (2019) Adam Ek and Mehdi Ghanimifard. 2019. [Synthetic propaganda
    embeddings to train a linear projection](https://doi.org/10.18653/v1/D19-5023).
    In *Proceedings of the Second Workshop on Natural Language Processing for Internet
    Freedom: Censorship, Disinformation, and Propaganda*, pages 155–161, Hong Kong,
    China. Association for Computational Linguistics.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ek 和 Ghanimifard (2019) Adam Ek 和 Mehdi Ghanimifard。2019. [合成宣传嵌入用于训练线性投影](https://doi.org/10.18653/v1/D19-5023)。发表于
    *第二届互联网自由自然语言处理研讨会：审查、虚假信息与宣传*，第155–161页，中国香港。计算语言学协会。
- en: Estabrooks and Japkowicz (2001) Andrew Estabrooks and Nathalie Japkowicz. 2001.
    [A mixture-of-experts framework for text classification](https://aclanthology.org/W01-0709).
    In *Proceedings of the ACL 2001 Workshop on Computational Natural Language Learning
    (ConLL)*.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Estabrooks 和 Japkowicz (2001) Andrew Estabrooks 和 Nathalie Japkowicz. 2001.
    [一种用于文本分类的专家混合框架](https://aclanthology.org/W01-0709)。发表于 *ACL 2001 计算自然语言学习研讨会（ConLL）论文集*。
- en: 'Feng et al. (2021) Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar,
    Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. [A survey of data augmentation
    approaches for NLP](https://doi.org/10.18653/v1/2021.findings-acl.84). In *Findings
    of the Association for Computational Linguistics: ACL-IJCNLP 2021*, pages 968–988,
    Online. Association for Computational Linguistics.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等 (2021) Steven Y. Feng、Varun Gangal、Jason Wei、Sarath Chandar、Soroush Vosoughi、Teruko
    Mitamura 和 Eduard Hovy。2021. [NLP 数据增强方法综述](https://doi.org/10.18653/v1/2021.findings-acl.84)。发表于
    *计算语言学协会：ACL-IJCNLP 2021 发现*，第968–988页，在线。计算语言学协会。
- en: 'Fernández et al. (2018) Alberto Fernández, Salvador Garcia, Francisco Herrera,
    and Nitesh V Chawla. 2018. Smote for learning from imbalanced data: Progress and
    challenges, marking the 15-year anniversary. *Journal of Artificial Intelligence
    Researc*, 61:863–905.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernández 等 (2018) Alberto Fernández、Salvador Garcia、Francisco Herrera 和 Nitesh
    V Chawla。2018. SMOTE 在不平衡数据学习中的进展与挑战，纪念15周年。*人工智能研究期刊*，61:863–905。
- en: Ferreira and Vlachos (2019) William Ferreira and Andreas Vlachos. 2019. [Incorporating
    label dependencies in multilabel stance detection](https://doi.org/10.18653/v1/D19-1665).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 6350–6354, Hong Kong, China. Association for Computational
    Linguistics.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira 和 Vlachos (2019) William Ferreira 和 Andreas Vlachos。2019. [在多标签态度检测中融入标签依赖](https://doi.org/10.18653/v1/D19-1665)。发表于
    *2019 年自然语言处理经验方法会议暨第九届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集*，第6350–6354页，中国香港。计算语言学协会。
- en: 'Gao et al. (2020) Yang Gao, Yi-Fan Li, Yu Lin, Charu Aggarwal, and Latifur
    Khan. 2020. [SetConv: A New Approach for Learning from Imbalanced Data](https://doi.org/10.18653/v1/2020.emnlp-main.98).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 1284–1294, Online. Association for Computational Linguistics.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等 (2020) Yang Gao、Yi-Fan Li、Yu Lin、Charu Aggarwal 和 Latifur Khan。2020. [SetConv：一种从不平衡数据中学习的新方法](https://doi.org/10.18653/v1/2020.emnlp-main.98)。发表于
    *2020 年自然语言处理经验方法会议（EMNLP）论文集*，第1284–1294页，在线。计算语言学协会。
- en: Gaspers et al. (2020) Judith Gaspers, Quynh Do, and Fabian Triefenbach. 2020.
    [Data Balancing for Boosting Performance of Low-Frequency Classes in Spoken Language
    Understanding](https://doi.org/10.21437/Interspeech.2020-1676). In *Proc. Interspeech
    2020*, pages 1560–1564.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaspers 等 (2020) Judith Gaspers、Quynh Do 和 Fabian Triefenbach。2020. [数据平衡用于提升语音语言理解中低频类别的性能](https://doi.org/10.21437/Interspeech.2020-1676)。发表于
    *Interspeech 2020 论文集*，第1560–1564页。
- en: 'Grünewald et al. (2021) Stefan Grünewald, Annemarie Friedrich, and Jonas Kuhn.
    2021. [Applying occam’s razor to transformer-based dependency parsing: What works,
    what doesn’t, and what is really necessary](https://doi.org/10.18653/v1/2021.iwpt-1.13).
    In *Proceedings of the 17th International Conference on Parsing Technologies and
    the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT
    2021)*, pages 131–144, Online. Association for Computational Linguistics.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grünewald 等 (2021) Stefan Grünewald, Annemarie Friedrich, 和 Jonas Kuhn. 2021.
    [应用奥卡姆剃刀于基于变换器的依存句法分析：哪些有效，哪些无效，以及哪些是真正必要的](https://doi.org/10.18653/v1/2021.iwpt-1.13)。见于
    *第 17 届国际句法分析技术会议及 IWPT 2021 增强通用依存关系解析共享任务*，第 131–144 页，在线。计算语言学协会。
- en: 'Gupta et al. (2019a) Agrim Gupta, Piotr Dollar, and Ross Girshick. 2019a. Lvis:
    A dataset for large vocabulary instance segmentation. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gupta 等 (2019a) Agrim Gupta, Piotr Dollar, 和 Ross Girshick. 2019a. Lvis: 一个用于大词汇量实例分割的数据集。见于
    *IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集*。'
- en: 'Gupta et al. (2019b) Agrim Gupta, Piotr Dollár, and Ross B. Girshick. 2019b.
    [LVIS: A dataset for large vocabulary instance segmentation](http://arxiv.org/abs/1908.03195).
    *CoRR*, abs/1908.03195.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等 (2019b) Agrim Gupta, Piotr Dollár, 和 Ross B. Girshick. 2019b. [LVIS：一个用于大词汇量实例分割的数据集](http://arxiv.org/abs/1908.03195)。*CoRR*，abs/1908.03195。
- en: 'Gururangan et al. (2020) Suchin Gururangan, Ana Marasović, Swabha Swayamdipta,
    Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. [Don’t stop pretraining:
    Adapt language models to domains and tasks](https://doi.org/10.18653/v1/2020.acl-main.740).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8342–8360, Online. Association for Computational Linguistics.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gururangan 等 (2020) Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle
    Lo, Iz Beltagy, Doug Downey, 和 Noah A. Smith. 2020. [不要停止预训练：将语言模型适配于领域和任务](https://doi.org/10.18653/v1/2020.acl-main.740)。见于
    *第 58 届计算语言学协会年会论文集*，第 8342–8360 页，在线。计算语言学协会。
- en: Han et al. (2018) Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, and Peng Li.
    2018. [Hierarchical relation extraction with coarse-to-fine grained attention](https://doi.org/10.18653/v1/D18-1247).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pages 2236–2245, Brussels, Belgium. Association for Computational
    Linguistics.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2018) Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, 和 Peng Li. 2018.
    [层次关系抽取的粗到细关注机制](https://doi.org/10.18653/v1/D18-1247)。见于 *2018 年自然语言处理经验方法会议论文集*，第
    2236–2245 页，比利时布鲁塞尔。计算语言学协会。
- en: Harbecke et al. (2022) David Harbecke, Yuxuan Chen, Leonhard Hennig, and Christoph
    Alt. 2022. [Why only micro-f1? class weighting of measures for relation classification](https://doi.org/10.18653/v1/2022.nlppower-1.4).
    In *Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in
    NLP*, pages 32–41, Dublin, Ireland. Association for Computational Linguistics.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harbecke 等 (2022) David Harbecke, Yuxuan Chen, Leonhard Hennig, 和 Christoph
    Alt. 2022. [为什么只有微平均 F1？关系分类的度量加权](https://doi.org/10.18653/v1/2022.nlppower-1.4)。见于
    *NLP Power! 第一次高效基准测试研讨会*，第 32–41 页，爱尔兰都柏林。计算语言学协会。
- en: 'Hedderich et al. (2021) Michael A. Hedderich, Lukas Lange, Heike Adel, Jannik
    Strötgen, and Dietrich Klakow. 2021. [A survey on recent approaches for natural
    language processing in low-resource scenarios](https://doi.org/10.18653/v1/2021.naacl-main.201).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2545–2568,
    Online. Association for Computational Linguistics.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hedderich 等 (2021) Michael A. Hedderich, Lukas Lange, Heike Adel, Jannik Strötgen,
    和 Dietrich Klakow. 2021. [关于低资源场景下自然语言处理的近期方法综述](https://doi.org/10.18653/v1/2021.naacl-main.201)。见于
    *2021 年北美计算语言学协会：人类语言技术会议论文集*，第 2545–2568 页，在线。计算语言学协会。
- en: 'Hu et al. (2022) Bo Hu, Yun Liu, Naiyue Chen, Lifu Wang, Ning Liu, and Xing
    Cao. 2022. [Segcn-dcr: A syntax-enhanced event detection framework with decoupled
    classification rebalance](https://doi.org/https://doi.org/10.1016/j.neucom.2022.01.069).
    *Neurocomputing*, 481:55–66.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 (2022) Bo Hu, Yun Liu, Naiyue Chen, Lifu Wang, Ning Liu, 和 Xing Cao. 2022.
    [Segcn-dcr：一种通过解耦分类重平衡的语法增强事件检测框架](https://doi.org/https://doi.org/10.1016/j.neucom.2022.01.069)。*Neurocomputing*，481:55–66。
- en: Huang et al. (2021) Yi Huang, Buse Giledereli, Abdullatif Köksal, Arzucan Özgür,
    and Elif Ozkirimli. 2021. [Balancing methods for multi-label text classification
    with long-tailed class distribution](https://doi.org/10.18653/v1/2021.emnlp-main.643).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 8153–8161, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2021）Yi Huang、Buse Giledereli、Abdullatif Köksal、Arzucan Özgür 和 Elif
    Ozkirimli. 2021. [针对长尾类别分布的多标签文本分类平衡方法](https://doi.org/10.18653/v1/2021.emnlp-main.643)。在
    *2021年自然语言处理实证方法会议论文集*，页码8153–8161，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Jang et al. (2021) Joel Jang, Yoonjeon Kim, Kyoungho Choi, and Sungho Suh.
    2021. [Sequential targeting: A continual learning approach for data imbalance
    in text classification](https://doi.org/https://doi.org/10.1016/j.eswa.2021.115067).
    *Expert Systems with Applications*, 179:115067.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang 等人（2021）Joel Jang、Yoonjeon Kim、Kyoungho Choi 和 Sungho Suh. 2021. [顺序目标：一种用于文本分类数据不平衡的持续学习方法](https://doi.org/https://doi.org/10.1016/j.eswa.2021.115067)。*应用专家系统*，179:115067。
- en: 'Japkowicz et al. (2000) Nathalie Japkowicz et al. 2000. Learning from imbalanced
    data sets: a comparison of various strategies. In *AAAI workshop on learning from
    imbalanced data sets*, volume 68, pages 10–15\. AAAI Press Menlo Park, CA.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Japkowicz 等人（2000）Nathalie Japkowicz 等人. 2000. 从不平衡数据集中学习：各种策略的比较。在 *AAAI关于从不平衡数据集中学习的研讨会*，第68卷，页码10–15。AAAI
    Press Menlo Park, CA。
- en: 'Jiang et al. (2021) Wanrong Jiang, Ya Chen, Hao Fu, and Guiquan Liu. 2021.
    Textcut: A multi-region replacement data augmentation approach for text imbalance
    classification. In *Neural Information Processing*, pages 427–439, Cham. Springer
    International Publishing.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人（2021）Wanrong Jiang、Ya Chen、Hao Fu 和 Guiquan Liu. 2021. Textcut：一种用于文本不平衡分类的多区域替换数据增强方法。在
    *神经信息处理*，页码427–439，Cham。Springer International Publishing。
- en: Johnson and Khoshgoftaar (2019a) Justin M. Johnson and Taghi M. Khoshgoftaar.
    2019a. Deep learning and thresholding with class-imbalanced big data. *2019 18th
    IEEE International Conference On Machine Learning And Applications (ICMLA)*, pages
    755–762.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 和 Khoshgoftaar（2019a）Justin M. Johnson 和 Taghi M. Khoshgoftaar. 2019a.
    深度学习与类别不平衡的大数据阈值化。*2019年第18届IEEE国际机器学习与应用会议（ICMLA）*，页码755–762。
- en: Johnson and Khoshgoftaar (2019b) Justin M. Johnson and Taghi M. Khoshgoftaar.
    2019b. [Survey on deep learning with class imbalance](https://doi.org/10.1186/s40537-019-0192-5).
    *Journal of Big Data*, 6(1).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 和 Khoshgoftaar（2019b）Justin M. Johnson 和 Taghi M. Khoshgoftaar. 2019b.
    [关于类别不平衡的深度学习综述](https://doi.org/10.1186/s40537-019-0192-5)。*大数据期刊*，6(1)。
- en: Johnson and Khoshgoftaar (2020) Justin M. Johnson and Taghi M. Khoshgoftaar.
    2020. [The Effects of Data Sampling with Deep Learning and Highly Imbalanced Big
    Data](https://doi.org/10.1007/s10796-020-10022-7). *Information Systems Frontiers*,
    22(5):1113–1131.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 和 Khoshgoftaar（2020）Justin M. Johnson 和 Taghi M. Khoshgoftaar. 2020.
    [深度学习和高度不平衡大数据的数据采样效果](https://doi.org/10.1007/s10796-020-10022-7)。*信息系统前沿*，22(5):1113–1131。
- en: 'Juuti et al. (2020) Mika Juuti, Tommi Gröndahl, Adrian Flanagan, and N. Asokan.
    2020. [A little goes a long way: Improving toxic language classification despite
    data scarcity](https://doi.org/10.18653/v1/2020.findings-emnlp.269). In *Findings
    of the Association for Computational Linguistics: EMNLP 2020*, pages 2991–3009,
    Online. Association for Computational Linguistics.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juuti 等人（2020）Mika Juuti、Tommi Gröndahl、Adrian Flanagan 和 N. Asokan. 2020. [小投入大回报：尽管数据稀缺仍能改善有毒语言分类](https://doi.org/10.18653/v1/2020.findings-emnlp.269)。在
    *计算语言学协会会议成果：EMNLP 2020*，页码2991–3009，在线。计算语言学协会。
- en: Kang et al. (2020) Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan,
    Albert Gordo, Jiashi Feng, and Yannis Kalantidis. 2020. [Decoupling representation
    and classifier for long-tailed recognition](https://openreview.net/forum?id=r1gRTCVFvB).
    In *8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等人（2020）Bingyi Kang、Saining Xie、Marcus Rohrbach、Zhicheng Yan、Albert Gordo、Jiashi
    Feng 和 Yannis Kalantidis. 2020. [长尾识别中的表示和分类器解耦](https://openreview.net/forum?id=r1gRTCVFvB)。在
    *第8届国际学习表征会议，ICLR 2020，亚的斯亚贝巴，埃塞俄比亚，2020年4月26-30日*。OpenReview.net。
- en: Khosla et al. (2020) Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
    Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020.
    Supervised contrastive learning. *Advances in Neural Information Processing Systems*,
    33:18661–18673.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosla 等人（2020）Prannay Khosla、Piotr Teterwak、Chen Wang、Aaron Sarna、Yonglong
    Tian、Phillip Isola、Aaron Maschinot、Ce Liu 和 Dilip Krishnan. 2020. 监督对比学习。*神经信息处理系统进展*，33:18661–18673。
- en: Kim (2014) Yoon Kim. 2014. [Convolutional neural networks for sentence classification](https://doi.org/10.3115/v1/D14-1181).
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 1746–1751, Doha, Qatar. Association for Computational
    Linguistics.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim (2014) Yoon Kim。2014年。[用于句子分类的卷积神经网络](https://doi.org/10.3115/v1/D14-1181)。在*2014年自然语言处理经验方法会议论文集
    (EMNLP)*，页码 1746–1751，卡塔尔多哈。计算语言学协会。
- en: Kumar et al. (2018) Pulkit Kumar, Monika Grewal, and Muktabh Mayank Srivastava.
    2018. Boosted cascaded convnets for multilabel classification of thoracic diseases
    in chest radiographs. In *International conference image analysis and recognition*,
    pages 546–552\. Springer.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 (2018) Pulkit Kumar, Monika Grewal 和 Muktabh Mayank Srivastava。2018年。*胸部X射线中的多标签分类的增强级联卷积网络*。在*国际图像分析与识别会议*上，页码
    546–552。Springer。
- en: Kunchukuttan and Bhattacharyya (2015) Anoop Kunchukuttan and Pushpak Bhattacharyya.
    2015. [Addressing class imbalance in grammatical error detection with evaluation
    metric optimization](https://aclanthology.org/W15-5902). In *Proceedings of the
    12th International Conference on Natural Language Processing*, pages 2–10, Trivandrum,
    India. NLP Association of India.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kunchukuttan 和 Bhattacharyya (2015) Anoop Kunchukuttan 和 Pushpak Bhattacharyya。2015年。[通过评估指标优化解决语法错误检测中的类别不平衡问题](https://aclanthology.org/W15-5902)。在*第12届国际自然语言处理会议论文集*，页码
    2–10，印度 Trivandrum。印度自然语言处理协会。
- en: 'Lange et al. (2021) Lukas Lange, Jannik Strötgen, Heike Adel, and Dietrich
    Klakow. 2021. [To share or not to share: Predicting sets of sources for model
    transfer learning](https://doi.org/10.18653/v1/2021.emnlp-main.689). In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    8744–8753, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lange 等人 (2021) Lukas Lange, Jannik Strötgen, Heike Adel 和 Dietrich Klakow。2021年。[分享还是不分享：预测模型迁移学习的源集合](https://doi.org/10.18653/v1/2021.emnlp-main.689)。在*2021年自然语言处理经验方法会议论文集*，页码
    8744–8753，在线及多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Li and Xiao (2020) Jinfen Li and Lu Xiao. 2020. [syrapropa at SemEval-2020
    task 11: BERT-based models design for propagandistic technique and span detection](https://doi.org/10.18653/v1/2020.semeval-1.237).
    In *Proceedings of the Fourteenth Workshop on Semantic Evaluation*, pages 1808–1816,
    Barcelona (online). International Committee for Computational Linguistics.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Xiao (2020) Jinfen Li 和 Lu Xiao。2020年。[syrapropa 在 SemEval-2020 任务 11：基于
    BERT 的模型设计用于宣传技术和范围检测](https://doi.org/10.18653/v1/2020.semeval-1.237)。在*第十四届语义评估研讨会论文集*，页码
    1808–1816，西班牙巴塞罗那（在线）。国际计算语言学委员会。
- en: Li and Nenkova (2014) Junyi Jessy Li and Ani Nenkova. 2014. [Addressing class
    imbalance for improved recognition of implicit discourse relations](https://doi.org/10.3115/v1/W14-4320).
    In *Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse
    and Dialogue (SIGDIAL)*, pages 142–150, Philadelphia, PA, U.S.A. Association for
    Computational Linguistics.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Nenkova (2014) Junyi Jessy Li 和 Ani Nenkova。2014年。[解决类别不平衡以改进隐含话语关系的识别](https://doi.org/10.3115/v1/W14-4320)。在*第15届话语与对话特别兴趣小组年会会议论文集
    (SIGDIAL)*，页码 142–150，美国宾夕法尼亚州费城。计算语言学协会。
- en: Li et al. (2011) Shoushan Li, Guodong Zhou, Zhongqing Wang, Sophia Yat Mei Lee,
    and Rangyang Wang. 2011. [Imbalanced sentiment classification](https://doi.org/10.1145/2063576.2063994).
    In *Proceedings of the 20th ACM International Conference on Information and Knowledge
    Management*, CIKM ’11, page 2469–2472, New York, NY, USA. Association for Computing
    Machinery.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2011) Shoushan Li, Guodong Zhou, Zhongqing Wang, Sophia Yat Mei Lee 和
    Rangyang Wang。2011年。[不平衡情感分类](https://doi.org/10.1145/2063576.2063994)。在*第20届ACM国际信息与知识管理会议论文集*，CIKM
    ’11，页码 2469–2472，美国纽约。计算机协会。
- en: Li et al. (2020) Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu,
    and Jiwei Li. 2020. [Dice loss for data-imbalanced NLP tasks](https://doi.org/10.18653/v1/2020.acl-main.45).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 465–476, Online. Association for Computational Linguistics.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2020) Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu 和 Jiwei
    Li。2020年。[用于数据不平衡NLP任务的Dice损失](https://doi.org/10.18653/v1/2020.acl-main.45)。在*第58届计算语言学协会年会论文集*，页码
    465–476，在线。计算语言学协会。
- en: 'Li and Scarton (2020) Yue Li and Carolina Scarton. 2020. [Revisiting rumour
    stance classification: Dealing with imbalanced data](https://aclanthology.org/2020.rdsm-1.4).
    In *Proceedings of the 3rd International Workshop on Rumours and Deception in
    Social Media (RDSM)*, pages 38–44, Barcelona, Spain (Online). Association for
    Computational Linguistics.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李和斯卡顿（2020）岳李和卡罗利娜·斯卡顿。2020年。[重新审视谣言立场分类：处理不平衡数据](https://aclanthology.org/2020.rdsm-1.4)。发表于*第三届社交媒体谣言与欺骗国际研讨会（RDSM）*，第38–44页，西班牙巴塞罗那（线上）。计算语言学协会。
- en: Lin et al. (2019) Hongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun. 2019. [Cost-sensitive
    regularization for label confusion-aware event detection](https://doi.org/10.18653/v1/P19-1521).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 5278–5283, Florence, Italy. Association for Computational
    Linguistics.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2019）林宏宇、姚杰·卢、韩显培和孙乐。2019年。[面向标签混淆的代价敏感正则化事件检测](https://doi.org/10.18653/v1/P19-1521)。发表于*第57届计算语言学协会年会*，第5278–5283页，意大利佛罗伦萨。计算语言学协会。
- en: Lin et al. (2017) Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
    Piotr Dollar. 2017. Focal loss for dense object detection. In *Proceedings of
    the IEEE International Conference on Computer Vision (ICCV)*.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2017）林宗义、普里亚·戈亚尔、罗斯·吉尔什克、何恺明和皮奥特·美元。2017年。密集目标检测的焦点损失。发表于*IEEE国际计算机视觉会议（ICCV）*。
- en: Liu et al. (2022) Hong Liu, Jeff Z. HaoChen, Adrien Gaidon, and Tengyu Ma. 2022.
    [Self-supervised learning is more robust to dataset imbalance](https://openreview.net/forum?id=4AZz9osqrar).
    In *International Conference on Learning Representations*.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2022）洪刘、杰夫·Z·郝晨、阿德里安·盖登和滕宇·马。2022年。[自监督学习对数据集不平衡的鲁棒性更强](https://openreview.net/forum?id=4AZz9osqrar)。发表于*国际学习表征会议*。
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    [Roberta: A robustly optimized BERT pretraining approach](http://arxiv.org/abs/1907.11692).
    *CoRR*, abs/1907.11692.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2019）刘银汉、迈尔·奥特、纳曼·戈亚尔、景飞·杜、曼达尔·乔希、陈丹琪、奥梅尔·莱维、迈克·刘易斯、卢克·泽特尔莫耶和维塞林·斯托亚诺夫。2019年。[Roberta：一种稳健优化的BERT预训练方法](http://arxiv.org/abs/1907.11692)。*CoRR*，abs/1907.11692。
- en: 'Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, and Christopher Potts. 2011. [Learning word vectors for sentiment
    analysis](https://aclanthology.org/P11-1015). In *Proceedings of the 49th Annual
    Meeting of the Association for Computational Linguistics: Human Language Technologies*,
    pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马斯等（2011）安德鲁·L·马斯、雷蒙德·E·戴利、彼得·T·范、丹·黄、安德鲁·Y·吴和克里斯托弗·波茨。2011年。[用于情感分析的词向量学习](https://aclanthology.org/P11-1015)。发表于*第49届计算语言学协会年会：人类语言技术论文集*，第142–150页，美国俄勒冈州波特兰。计算语言学协会。
- en: Marelli et al. (2014) Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli,
    Raffaella Bernardi, and Roberto Zamparelli. 2014. [A SICK cure for the evaluation
    of compositional distributional semantic models](http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf).
    In *Proceedings of the Ninth International Conference on Language Resources and
    Evaluation (LREC’14)*, pages 216–223, Reykjavik, Iceland. European Language Resources
    Association (ELRA).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马雷利等（2014）马尔科·马雷利、斯特凡诺·梅尼尼、马尔科·巴罗尼、路易莎·本蒂沃利、拉法埃拉·贝尔纳尔迪和罗伯托·赞帕雷利。2014年。[SICK：用于评估组合分布语义模型的工具](http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf)。发表于*第九届语言资源与评估国际会议（LREC’14）*，第216–223页，冰岛雷克雅未克。欧洲语言资源协会（ELRA）。
- en: 'Mathew et al. (2021) Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann,
    Pawan Goyal, and Animesh Mukherjee. 2021. Hatexplain: A benchmark dataset for
    explainable hate speech detection. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 35, pages 14867–14875.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马修等（2021）宾尼·马修、普尼亚乔伊·萨哈、塞义德·穆赫耶·伊马姆、克里斯·比曼、帕万·戈亚尔和阿尼梅什·穆克吉。2021年。Hatexplain：用于可解释仇恨言论检测的基准数据集。发表于*AAAI人工智能会议论文集*，第35卷，第14867–14875页。
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. [Distributed representations of words and phrases and their
    compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 26\. Curran Associates,
    Inc.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 米科洛夫等（2013）托马斯·米科洛夫、伊利亚·苏茨克弗、凯·陈、格雷戈·S·科拉多和杰夫·迪恩。2013年。[词汇和短语的分布式表示及其组合性](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)。发表于*神经信息处理系统进展*，第26卷。卡伦协会。
- en: 'Milletari et al. (2016) Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
    2016. [V-net: Fully convolutional neural networks for volumetric medical image
    segmentation](https://doi.org/10.1109/3DV.2016.79). In *2016 Fourth International
    Conference on 3D Vision (3DV)*, pages 565–571.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Milletari et al. (2016) Fausto Milletari、Nassir Navab 和 Seyed-Ahmad Ahmadi.
    2016. [V-net：用于体积医学图像分割的全卷积神经网络](https://doi.org/10.1109/3DV.2016.79)。在 *2016
    第四届三维视觉国际会议 (3DV)*，第 565–571 页。
- en: 'Mullenbach et al. (2018) James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng
    Sun, and Jacob Eisenstein. 2018. [Explainable prediction of medical codes from
    clinical text](https://doi.org/10.18653/v1/N18-1100). In *Proceedings of the 2018
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1101–1111,
    New Orleans, Louisiana. Association for Computational Linguistics.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mullenbach et al. (2018) James Mullenbach、Sarah Wiegreffe、Jon Duke、Jimeng Sun
    和 Jacob Eisenstein. 2018. [从临床文本中解释性预测医学编码](https://doi.org/10.18653/v1/N18-1100)。在
    *2018 年计算语言学协会北美章节会议：人类语言技术，第 1 卷（长篇论文）*，第 1101–1111 页，美国路易斯安那州新奥尔良。计算语言学协会。
- en: Nan et al. (2021) Guoshun Nan, Jiaqi Zeng, Rui Qiao, Zhijiang Guo, and Wei Lu.
    2021. [Uncovering main causalities for long-tailed information extraction](https://doi.org/10.18653/v1/2021.emnlp-main.763).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 9683–9695, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nan et al. (2021) Guoshun Nan、Jiaqi Zeng、Rui Qiao、Zhijiang Guo 和 Wei Lu. 2021.
    [揭示长尾信息提取的主要因果关系](https://doi.org/10.18653/v1/2021.emnlp-main.763)。在 *2021 年自然语言处理经验方法会议论文集*，第
    9683–9695 页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Niklaus et al. (2021) Joel Niklaus, Ilias Chalkidis, and Matthias Stürmer.
    2021. [Swiss-judgment-prediction: A multilingual legal judgment prediction benchmark](https://doi.org/10.18653/v1/2021.nllp-1.3).
    In *Proceedings of the Natural Legal Language Processing Workshop 2021*, pages
    19–35, Punta Cana, Dominican Republic. Association for Computational Linguistics.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niklaus et al. (2021) Joel Niklaus、Ilias Chalkidis 和 Matthias Stürmer. 2021.
    [瑞士判决预测：多语言法律判决预测基准](https://doi.org/10.18653/v1/2021.nllp-1.3)。在 *2021 年自然法律语言处理研讨会论文集*，第
    19–35 页，多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Nishino et al. (2020) Toru Nishino, Ryota Ozaki, Yohei Momoki, Tomoki Taniguchi,
    Ryuji Kano, Norihisa Nakano, Yuki Tagawa, Motoki Taniguchi, Tomoko Ohkuma, and
    Keigo Nakamura. 2020. [Reinforcement learning with imbalanced dataset for data-to-text
    medical report generation](https://doi.org/10.18653/v1/2020.findings-emnlp.202).
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages
    2223–2236, Online. Association for Computational Linguistics.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nishino et al. (2020) Toru Nishino、Ryota Ozaki、Yohei Momoki、Tomoki Taniguchi、Ryuji
    Kano、Norihisa Nakano、Yuki Tagawa、Motoki Taniguchi、Tomoko Ohkuma 和 Keigo Nakamura.
    2020. [使用不平衡数据集的强化学习进行数据到文本的医学报告生成](https://doi.org/10.18653/v1/2020.findings-emnlp.202)。在
    *计算语言学协会：EMNLP 2020 发现*，第 2223–2236 页，在线。计算语言学协会。
- en: Park and Zhang (2002) Seong-Bae Park and Byoung-Tak Zhang. 2002. [A boosted
    maximum entropy model for learning text chunking](https://bi.snu.ac.kr/Publications/Conferences/International/ICML2002_Park.pdf).
    In *Proceedings of the Nineteenth International Conference on Machine Learning*,
    pages 482–489.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park and Zhang (2002) Seong-Bae Park 和 Byoung-Tak Zhang. 2002. [用于学习文本切块的增强最大熵模型](https://bi.snu.ac.kr/Publications/Conferences/International/ICML2002_Park.pdf)。在
    *第十九届国际机器学习大会论文集*，第 482–489 页。
- en: Pouyanfar et al. (2018) Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian,
    Ahmed S. Kaseb, Kent Gauen, Ryan Dailey, Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching
    Chen, and Mei-Ling Shyu. 2018. [Dynamic sampling in convolutional neural networks
    for imbalanced data classification](https://doi.org/10.1109/MIPR.2018.00027).
    In *2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)*,
    pages 112–117.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pouyanfar et al. (2018) Samira Pouyanfar、Yudong Tao、Anup Mohan、Haiman Tian、Ahmed
    S. Kaseb、Kent Gauen、Ryan Dailey、Sarah Aghajanzadeh、Yung-Hsiang Lu、Shu-Ching Chen
    和 Mei-Ling Shyu. 2018. [卷积神经网络中的动态采样用于不平衡数据分类](https://doi.org/10.1109/MIPR.2018.00027)。在
    *2018 IEEE 多媒体信息处理与检索会议 (MIPR)*，第 112–117 页。
- en: Prange et al. (2021) Jakob Prange, Nathan Schneider, and Vivek Srikumar. 2021.
    [Supertagging the long tail with tree-structured decoding of complex categories](https://doi.org/10.1162/tacl_a_00364).
    *Transactions of the Association for Computational Linguistics*, 9:243–260.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prange et al. (2021) Jakob Prange、Nathan Schneider 和 Vivek Srikumar. 2021. [使用树状解码复杂类别的超标记](https://doi.org/10.1162/tacl_a_00364)。*计算语言学协会会刊*，9:243–260。
- en: Pujari et al. (2021) Subhash Chandra Pujari, Annemarie Friedrich, and Jannik
    Strötgen. 2021. A multi-task approach to neural multi-label hierarchical patent
    classification using transformers. In *European Conference on Information Retrieval*,
    pages 513–528\. Springer.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pujari 等（2021）Subhash Chandra Pujari、Annemarie Friedrich 和 Jannik Strötgen.
    2021. 一种多任务的神经网络多标签层次专利分类方法，使用变压器。在 *欧洲信息检索会议*，第 513–528 页。Springer。
- en: Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. [Language models are unsupervised multitask
    learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
    Technical report, OpenAI.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019）Alec Radford、Jeff Wu、Rewon Child、David Luan、Dario Amodei 和 Ilya
    Sutskever. 2019. [语言模型是无监督的多任务学习者](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)。技术报告，OpenAI。
- en: Ramponi and Plank (2020) Alan Ramponi and Barbara Plank. 2020. [Neural unsupervised
    domain adaptation in NLP—A survey](https://doi.org/10.18653/v1/2020.coling-main.603).
    In *Proceedings of the 28th International Conference on Computational Linguistics*,
    pages 6838–6855, Barcelona, Spain (Online). International Committee on Computational
    Linguistics.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramponi 和 Plank（2020）Alan Ramponi 和 Barbara Plank. 2020. [神经无监督领域适应在 NLP 中的调查](https://doi.org/10.18653/v1/2020.coling-main.603)。在
    *第 28 届国际计算语言学会议论文集*，第 6838–6855 页，西班牙巴塞罗那（在线）。国际计算语言学委员会。
- en: Rathnayaka et al. (2019) Prabod Rathnayaka, Supun Abeysinghe, Chamod Samarajeewa,
    Isura Manchanayake, Malaka J. Walpola, Rashmika Nawaratne, Tharindu R. Bandaragoda,
    and Damminda Alahakoon. 2019. [Gated recurrent neural network approach for multilabel
    emotion detection in microblogs](http://arxiv.org/abs/1907.07653). *CoRR*, abs/1907.07653.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rathnayaka 等（2019）Prabod Rathnayaka、Supun Abeysinghe、Chamod Samarajeewa、Isura
    Manchanayake、Malaka J. Walpola、Rashmika Nawaratne、Tharindu R. Bandaragoda 和 Damminda
    Alahakoon. 2019. [用于微博客中的多标签情感检测的门控递归神经网络方法](http://arxiv.org/abs/1907.07653)。*CoRR*，abs/1907.07653。
- en: 'Ruder et al. (2019) Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta,
    and Thomas Wolf. 2019. [Transfer learning in natural language processing](https://doi.org/10.18653/v1/N19-5004).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Tutorials*, pages 15–18, Minneapolis, Minnesota.
    Association for Computational Linguistics.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder 等（2019）Sebastian Ruder、Matthew E. Peters、Swabha Swayamdipta 和 Thomas Wolf.
    2019. [自然语言处理中的迁移学习](https://doi.org/10.18653/v1/N19-5004)。在 *2019 年北美计算语言学协会会议：教程*，第
    15–18 页，明尼阿波利斯，明尼苏达州。计算语言学协会。
- en: Sagawa et al. (2020) Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and
    Percy Liang. 2020. [Distributionally robust neural networks](https://openreview.net/forum?id=ryxGuJrFvS).
    In *International Conference on Learning Representations*.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sagawa 等（2020）Shiori Sagawa、Pang Wei Koh、Tatsunori B. Hashimoto 和 Percy Liang.
    2020. [分布式鲁棒神经网络](https://openreview.net/forum?id=ryxGuJrFvS)。在 *国际学习表征会议*。
- en: Schick and Schütze (2022) Timo Schick and Hinrich Schütze. 2022. [True few-shot
    learning with Prompts—A real-world perspective](https://doi.org/10.1162/tacl_a_00485).
    *Transactions of the Association for Computational Linguistics*, 10:716–731.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick 和 Schütze（2022）Timo Schick 和 Hinrich Schütze. 2022. [真正的少样本学习与提示—现实世界的视角](https://doi.org/10.1162/tacl_a_00485)。*计算语言学协会交易*，10:716–731。
- en: Schröder and Niekler (2020) Christopher Schröder and Andreas Niekler. 2020.
    [A survey of active learning for text classification using deep neural networks](http://arxiv.org/abs/2008.07267).
    *CoRR*, abs/2008.07267.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schröder 和 Niekler（2020）Christopher Schröder 和 Andreas Niekler. 2020. [使用深度神经网络进行文本分类的主动学习调查](http://arxiv.org/abs/2008.07267)。*CoRR*，abs/2008.07267。
- en: Shen et al. (2016) Li Shen, Zhouchen Lin, and Qingming Huang. 2016. Relay backpropagation
    for effective learning of deep convolutional neural networks. In *Computer Vision
    – ECCV 2016*, pages 467–482, Cham. Springer International Publishing.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等（2016）Li Shen、Zhouchen Lin 和 Qingming Huang. 2016. 用于深度卷积神经网络有效学习的继电反向传播。在
    *计算机视觉 – ECCV 2016*，第 467–482 页，Cham。Springer International Publishing。
- en: Shi and Demberg (2019) Wei Shi and Vera Demberg. 2019. [Next sentence prediction
    helps implicit discourse relation classification within and across domains](https://doi.org/10.18653/v1/D19-1586).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 5790–5796, Hong Kong, China. Association for Computational
    Linguistics.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 和 Demberg（2019）Wei Shi 和 Vera Demberg. 2019. [下一句预测有助于隐性语篇关系分类，跨领域应用](https://doi.org/10.18653/v1/D19-1586)。在
    *2019年自然语言处理经验方法会议暨第9届国际联合自然语言处理会议（EMNLP-IJCNLP）*，第5790–5796页，香港，中国。计算语言学协会。
- en: Spangher et al. (2021) Alexander Spangher, Jonathan May, Sz-Rung Shiang, and
    Lingjia Deng. 2021. [Multitask semi-supervised learning for class-imbalanced discourse
    classification](https://doi.org/10.18653/v1/2021.emnlp-main.40). In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    498–517, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spangher 等（2021）Alexander Spangher, Jonathan May, Sz-Rung Shiang, 和 Lingjia
    Deng. 2021. [多任务半监督学习用于类别不平衡的语篇分类](https://doi.org/10.18653/v1/2021.emnlp-main.40)。在
    *2021年自然语言处理经验方法会议*，第498–517页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Subramanian et al. (2021) Shivashankar Subramanian, Afshin Rahimi, Timothy Baldwin,
    Trevor Cohn, and Lea Frermann. 2021. [Fairness-aware class imbalanced learning](https://doi.org/10.18653/v1/2021.emnlp-main.155).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 2045–2051, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subramanian 等（2021）Shivashankar Subramanian, Afshin Rahimi, Timothy Baldwin,
    Trevor Cohn, 和 Lea Frermann. 2021. [公平意识的类别不平衡学习](https://doi.org/10.18653/v1/2021.emnlp-main.155)。在
    *2021年自然语言处理经验方法会议*，第2045–2051页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Suresh and Ong (2021) Varsha Suresh and Desmond Ong. 2021. [Not all negatives
    are equal: Label-aware contrastive loss for fine-grained text classification](https://doi.org/10.18653/v1/2021.emnlp-main.359).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 4381–4394, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suresh 和 Ong（2021）Varsha Suresh 和 Desmond Ong. 2021. [并非所有负样本都是平等的：标签感知对比损失用于细粒度文本分类](https://doi.org/10.18653/v1/2021.emnlp-main.359)。在
    *2021年自然语言处理经验方法会议*，第4381–4394页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Tan (2005) Songbo Tan. 2005. [Neighbor-weighted k-nearest neighbor for unbalanced
    text corpus](https://doi.org/https://doi.org/10.1016/j.eswa.2004.12.023). *Expert
    Systems with Applications*, 28(4):667–671.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan（2005）Songbo Tan. 2005. [邻居加权的 k 最近邻用于不平衡文本语料库](https://doi.org/https://doi.org/10.1016/j.eswa.2004.12.023)。*专家系统与应用*，28(4):667–671。
- en: Tang et al. (2020) Tiancheng Tang, Xinhuai Tang, and Tianyi Yuan. 2020. [Fine-tuning
    bert for multi-label sentiment analysis in unbalanced code-switching text](https://doi.org/10.1109/ACCESS.2020.3030468).
    *IEEE Access*, 8:193248–193256.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等（2020）Tiancheng Tang, Xinhuai Tang, 和 Tianyi Yuan. 2020. [在不平衡代码切换文本中微调
    BERT 用于多标签情感分析](https://doi.org/10.1109/ACCESS.2020.3030468)。*IEEE Access*，8:193248–193256。
- en: 'Tänzer et al. (2022) Michael Tänzer, Sebastian Ruder, and Marek Rei. 2022.
    [Memorisation versus generalisation in pre-trained language models](https://doi.org/10.18653/v1/2022.acl-long.521).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 7564–7578, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tänzer 等（2022）Michael Tänzer, Sebastian Ruder, 和 Marek Rei. 2022. [预训练语言模型中的记忆与泛化](https://doi.org/10.18653/v1/2022.acl-long.521)。在
    *第60届计算语言学协会年会（第1卷：长篇论文）*，第7564–7578页，都柏林，爱尔兰。计算语言学协会。
- en: 'Tayyar Madabushi et al. (2019) Harish Tayyar Madabushi, Elena Kochkina, and
    Michael Castelle. 2019. [Cost-sensitive BERT for generalisable sentence classification
    on imbalanced data](https://doi.org/10.18653/v1/D19-5018). In *Proceedings of
    the Second Workshop on Natural Language Processing for Internet Freedom: Censorship,
    Disinformation, and Propaganda*, pages 125–134, Hong Kong, China. Association
    for Computational Linguistics.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tayyar Madabushi 等（2019）Harish Tayyar Madabushi, Elena Kochkina, 和 Michael Castelle.
    2019. [成本敏感的 BERT 用于不平衡数据上的可泛化句子分类](https://doi.org/10.18653/v1/D19-5018)。在 *第二届互联网自由自然语言处理研讨会：审查、虚假信息与宣传*，第125–134页，香港，中国。计算语言学协会。
- en: 'Tepper et al. (2020) Naama Tepper, Esther Goldbraich, Naama Zwerdling, George
    Kour, Ateret Anaby Tavor, and Boaz Carmeli. 2020. [Balancing via generation for
    multi-class text classification improvement](https://doi.org/10.18653/v1/2020.findings-emnlp.130).
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages
    1440–1452, Online. Association for Computational Linguistics.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tepper 等（2020）Naama Tepper, Esther Goldbraich, Naama Zwerdling, George Kour,
    Ateret Anaby Tavor 和 Boaz Carmeli。2020年。[通过生成平衡多类文本分类的改进](https://doi.org/10.18653/v1/2020.findings-emnlp.130)。在
    *计算语言学协会：EMNLP 2020的发现*，第1440–1452页，在线。计算语言学协会。
- en: Tian et al. (2020) Jiachen Tian, Shizhan Chen, Xiaowang Zhang, and Zhiyong Feng.
    2020. A graph-based measurement for text imbalance classification. In *ECAI 2020*,
    pages 2188–2195\. IOS Press.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等（2020）Jiachen Tian, Shizhan Chen, Xiaowang Zhang 和 Zhiyong Feng。2020年。基于图的文本不平衡分类测量方法。在
    *ECAI 2020*，第2188–2195页。IOS Press。
- en: Tian et al. (2021) Jiachen Tian, Shizhan Chen, Xiaowang Zhang, Zhiyong Feng,
    Deyi Xiong, Shaojuan Wu, and Chunliu Dou. 2021. [Re-embedding difficult samples
    via mutual information constrained semantically oversampling for imbalanced text
    classification](https://doi.org/10.18653/v1/2021.emnlp-main.252). In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    3148–3161, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等（2021）Jiachen Tian, Shizhan Chen, Xiaowang Zhang, Zhiyong Feng, Deyi Xiong,
    Shaojuan Wu 和 Chunliu Dou。2021年。[通过互信息约束的语义过采样重新嵌入困难样本用于不平衡文本分类](https://doi.org/10.18653/v1/2021.emnlp-main.252)。在
    *2021年自然语言处理实证方法会议论文集*，第3148–3161页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Tomanek and Hahn (2009) Katrin Tomanek and Udo Hahn. 2009. [Reducing class imbalance
    during active learning for named entity annotation](https://doi.org/10.1145/1597735.1597754).
    In *Proceedings of the Fifth International Conference on Knowledge Capture*, K-CAP
    ’09, page 105–112, New York, NY, USA. Association for Computing Machinery.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tomanek 和 Hahn（2009）Katrin Tomanek 和 Udo Hahn。2009年。[在主动学习中减少类不平衡以进行命名实体注释](https://doi.org/10.1145/1597735.1597754)。在
    *第五届国际知识捕获会议论文集*，K-CAP ’09，第105–112页，美国纽约。计算机协会。
- en: Tran and Litman (2021) Nhat Tran and Diane Litman. 2021. [Multi-task learning
    in argument mining for persuasive online discussions](https://doi.org/10.18653/v1/2021.argmining-1.15).
    In *Proceedings of the 8th Workshop on Argument Mining*, pages 148–153, Punta
    Cana, Dominican Republic. Association for Computational Linguistics.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 和 Litman（2021）Nhat Tran 和 Diane Litman。2021年。[多任务学习在说服性在线讨论中的应用](https://doi.org/10.18653/v1/2021.argmining-1.15)。在
    *第8届论证挖掘研讨会论文集*，第148–153页，多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'van Son et al. (2018) Chantal van Son, Roser Morante, Lora Aroyo, and Piek
    Vossen. 2018. [Scoring and classifying implicit positive interpretations: A challenge
    of class imbalance](https://aclanthology.org/C18-1191). In *Proceedings of the
    27th International Conference on Computational Linguistics*, pages 2253–2264,
    Santa Fe, New Mexico, USA. Association for Computational Linguistics.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Son 等（2018）Chantal van Son, Roser Morante, Lora Aroyo 和 Piek Vossen。2018年。[评分和分类隐含正面解释：类不平衡的挑战](https://aclanthology.org/C18-1191)。在
    *第27届国际计算语言学会议论文集*，第2253–2264页，美国新墨西哥州圣菲。计算语言学协会。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. [Attention
    is all you need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 30\. Curran Associates,
    Inc.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N Gomez, Ł ukasz Kaiser 和 Illia Polosukhin。2017年。[注意力机制是你所需要的一切](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)。在
    *神经信息处理系统进展*，第30卷。Curran Associates, Inc.
- en: 'Wang et al. (2020a) Tao Wang, Yu Li, Bingyi Kang, Junnan Li, Junhao Liew, Sheng
    Tang, Steven Hoi, and Jiashi Feng. 2020a. The devil is in classification: A simple
    framework for long-tail instance segmentation. In *Computer Vision – ECCV 2020*,
    pages 728–744, Cham. Springer International Publishing.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020a）Tao Wang, Yu Li, Bingyi Kang, Junnan Li, Junhao Liew, Sheng Tang,
    Steven Hoi 和 Jiashi Feng。2020a年。分类中的难题：一个简单的长尾实例分割框架。在 *计算机视觉 – ECCV 2020*，第728–744页，德国香农。Springer国际出版公司。
- en: Wang et al. (2020b) Xin Wang, Thomas Huang, Joseph Gonzalez, Trevor Darrell,
    and Fisher Yu. 2020b. [Frustratingly simple few-shot object detection](https://proceedings.mlr.press/v119/wang20j.html).
    In *Proceedings of the 37th International Conference on Machine Learning*, volume
    119 of *Proceedings of Machine Learning Research*, pages 9919–9928\. PMLR.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020b) Xin Wang, Thomas Huang, Joseph Gonzalez, Trevor Darrell,
    和 Fisher Yu. 2020b. [令人沮丧的简单少样本目标检测](https://proceedings.mlr.press/v119/wang20j.html)。在*第37届国际机器学习大会论文集*，第119卷，*机器学习研究论文集*，第9919–9928页。PMLR。
- en: 'Wang et al. (2020c) Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni.
    2020c. Generalizing from a few examples: A survey on few-shot learning. *ACM computing
    surveys (csur)*, 53(3):1–34.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020c) Yaqing Wang, Quanming Yao, James T Kwok, 和 Lionel M Ni.
    2020c. 从少量示例中推广：少样本学习的综述。*ACM计算调查 (csur)*，53(3):1–34。
- en: Waseem and Hovy (2016) Zeerak Waseem and Dirk Hovy. 2016. [Hateful symbols or
    hateful people? predictive features for hate speech detection on Twitter](https://doi.org/10.18653/v1/N16-2013).
    In *Proceedings of the NAACL Student Research Workshop*, pages 88–93, San Diego,
    California. Association for Computational Linguistics.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Waseem and Hovy (2016) Zeerak Waseem 和 Dirk Hovy. 2016. [仇恨符号还是仇恨的人？用于Twitter仇恨言论检测的预测特征](https://doi.org/10.18653/v1/N16-2013)。在*NAACL学生研究研讨会论文集*，第88–93页，加利福尼亚州圣地亚哥。计算语言学协会。
- en: Wei (2021) Jason Wei. 2021. [Good-enough example extrapolation](https://doi.org/10.18653/v1/2021.emnlp-main.479).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 5923–5929, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei (2021) Jason Wei. 2021. [足够好的示例外推](https://doi.org/10.18653/v1/2021.emnlp-main.479)。在*2021年自然语言处理实证方法会议论文集*，第5923–5929页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Wei and Zou (2019) Jason Wei and Kai Zou. 2019. [EDA: Easy data augmentation
    techniques for boosting performance on text classification tasks](https://doi.org/10.18653/v1/D19-1670).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 6382–6388, Hong Kong, China. Association for Computational
    Linguistics.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei and Zou (2019) Jason Wei 和 Kai Zou. 2019. [EDA：用于提升文本分类任务性能的简易数据增强技术](https://doi.org/10.18653/v1/D19-1670)。在*2019年自然语言处理实证方法会议和第9届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集*，第6382–6388页，中国香港。计算语言学协会。
- en: 'Wei et al. (2022) Jiale Wei, Qiyuan Chen, Pai Peng, Benjamin Guedj, and Le Li.
    2022. [Reprint: a randomized extrapolation based on principal components for data
    augmentation](https://doi.org/10.48550/arXiv.2204.12024). *CoRR*, abs/2204.12024.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jiale Wei, Qiyuan Chen, Pai Peng, Benjamin Guedj, 和 Le Li.
    2022. [重印：基于主成分的随机化外推用于数据增强](https://doi.org/10.48550/arXiv.2204.12024)。*CoRR*，abs/2204.12024。
- en: 'Winata et al. (2020) Genta Indra Winata, Guangsen Wang, Caiming Xiong, and
    Steven C. H. Hoi. 2020. [Adapt-and-adjust: Overcoming the long-tail problem of
    multilingual speech recognition](http://arxiv.org/abs/2012.01687). *CoRR*, abs/2012.01687.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Winata et al. (2020) Genta Indra Winata, Guangsen Wang, Caiming Xiong, 和 Steven
    C. H. Hoi. 2020. [适应与调整：克服多语言语音识别中的长尾问题](http://arxiv.org/abs/2012.01687)。*CoRR*，abs/2012.01687。
- en: Wu et al. (2020) Tong Wu, Qingqiu Huang, Ziwei Liu, Yu Wang, and Dahua Lin.
    2020. Distribution-balanced loss for multi-label classification in long-tailed
    datasets. In *Computer Vision – ECCV 2020*, pages 162–178, Cham. Springer International
    Publishing.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2020) Tong Wu, Qingqiu Huang, Ziwei Liu, Yu Wang, 和 Dahua Lin. 2020.
    长尾数据集中的多标签分类的分布平衡损失。在*计算机视觉 – ECCV 2020*，第162–178页，Cham。施普林格国际出版公司。
- en: 'Wu et al. (2021) Xiaobao Wu, Chunping Li, and Yishu Miao. 2021. [Discovering
    topics in long-tailed corpora with causal intervention](https://doi.org/10.18653/v1/2021.findings-acl.15).
    In *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 175–185, Online. Association for Computational Linguistics.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2021) Xiaobao Wu, Chunping Li, 和 Yishu Miao. 2021. [通过因果干预发现长尾语料中的主题](https://doi.org/10.18653/v1/2021.findings-acl.15)。在*计算语言学协会发现：ACL-IJCNLP
    2021*，第175–185页，在线。计算语言学协会。
- en: Xie et al. (2020) Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc
    Le. 2020. Unsupervised data augmentation for consistency training. *Advances in
    Neural Information Processing Systems*, 33:6256–6268.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2020) Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, 和 Quoc Le.
    2020. 用于一致性训练的无监督数据增强。*神经信息处理系统进展*，33:6256–6268。
- en: 'Yang et al. (2020) Wenshuo Yang, Jiyi Li, Fumiyo Fukumoto, and Yanming Ye.
    2020. [HSCNN: A hybrid-Siamese convolutional neural network for extremely imbalanced
    multi-label text classification](https://doi.org/10.18653/v1/2020.emnlp-main.545).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 6716–6722, Online. Association for Computational Linguistics.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等（2020）Wenshuo Yang, Jiyi Li, Fumiyo Fukumoto和Yanming Ye. 2020. [HSCNN:
    一种用于极度不平衡多标签文本分类的混合Siamese卷积神经网络](https://doi.org/10.18653/v1/2020.emnlp-main.545)。收录于*2020年自然语言处理经验方法会议（EMNLP）论文集*，页码6716–6722，在线。计算语言学协会。'
- en: Yang et al. (2021) Yuzhe Yang, Kaiwen Zha, Ying-Cong Chen, Hao Wang, and Dina
    Katabi. 2021. [Delving into deep imbalanced regression](http://proceedings.mlr.press/v139/yang21m.html).
    In *Proceedings of the 38th International Conference on Machine Learning, ICML
    2021, 18-24 July 2021, Virtual Event*, volume 139 of *Proceedings of Machine Learning
    Research*, pages 11842–11851\. PMLR.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2021）Yuzhe Yang, Kaiwen Zha, Ying-Cong Chen, Hao Wang和Dina Katabi. 2021.
    [深入探讨深度不平衡回归](http://proceedings.mlr.press/v139/yang21m.html)。收录于*第38届国际机器学习会议，ICML
    2021, 2021年7月18-24日，虚拟会议*，*机器学习研究论文集*第139卷，页码11842–11851。PMLR。
- en: Yeh et al. (2017) Chih-Kuan Yeh, Wei-Chieh Wu, Wei-Jen Ko, and Yu-Chiang Frank
    Wang. 2017. Learning deep latent space for multi-label classification. In *Thirty-first
    AAAI conference on artificial intelligence*.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeh等（2017）Chih-Kuan Yeh, Wei-Chieh Wu, Wei-Jen Ko和Yu-Chiang Frank Wang. 2017.
    学习用于多标签分类的深层潜在空间。收录于*第31届人工智能AAAI会议*。
- en: Yilmaz et al. (2021) Selim F. Yilmaz, E. Batuhan Kaynak, Aykut Koç, Hamdi Dibeklioğlu,
    and Suleyman Serdar Kozat. 2021. [Multi-label sentiment analysis on 100 languages
    with dynamic weighting for label imbalance](https://doi.org/10.1109/TNNLS.2021.3094304).
    *IEEE Transactions on Neural Networks and Learning Systems*, pages 1–13.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yilmaz等（2021）Selim F. Yilmaz, E. Batuhan Kaynak, Aykut Koç, Hamdi Dibeklioğlu和Suleyman
    Serdar Kozat. 2021. [在100种语言上进行多标签情感分析，具有动态加权以应对标签不平衡](https://doi.org/10.1109/TNNLS.2021.3094304)。*IEEE神经网络与学习系统汇刊*，页码1–13。
- en: 'Yu et al. (2020) Haiyang Yu, Ningyu Zhang, Shumin Deng, Zonggang Yuan, Yantao
    Jia, and Huajun Chen. 2020. [The devil is the classifier: Investigating long tail
    relation classification with decoupling analysis](http://arxiv.org/abs/2009.07022).
    *CoRR*, abs/2009.07022.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等（2020）Haiyang Yu, Ningyu Zhang, Shumin Deng, Zonggang Yuan, Yantao Jia和Huajun
    Chen. 2020. [魔鬼在于分类器：通过解耦分析研究长尾关系分类](http://arxiv.org/abs/2009.07022)。*CoRR*，abs/2009.07022。
- en: 'Yun et al. (2019) Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun,
    Junsuk Choe, and Youngjoon Yoo. 2019. Cutmix: Regularization strategy to train
    strong classifiers with localizable features. In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision (ICCV)*.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yun等（2019）Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe和Youngjoon
    Yoo. 2019. Cutmix: 用于训练具有可局部化特征的强分类器的正则化策略。收录于*IEEE/CVF国际计算机视觉会议（ICCV）论文集*。'
- en: Zhang et al. (2020) Danqing Zhang, Tao Li, Haiyang Zhang, and Bing Yin. 2020.
    [On data augmentation for extreme multi-label classification](http://arxiv.org/abs/2009.10778).
    *CoRR*, abs/2009.10778.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2020）Danqing Zhang, Tao Li, Haiyang Zhang和Bing Yin. 2020. [关于极端多标签分类的数据增强](http://arxiv.org/abs/2009.10778)。*CoRR*，abs/2009.10778。
- en: 'Zhang et al. (2022) Jiaxin Zhang, Jie Liu, Shaowei Chen, Shaoxin Lin, Bingquan
    Wang, and Shanpeng Wang. 2022. Adam: An attentional data augmentation method for
    extreme multi-label text classification. In *Advances in Knowledge Discovery and
    Data Mining*, pages 131–142, Cham. Springer International Publishing.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等（2022）Jiaxin Zhang, Jie Liu, Shaowei Chen, Shaoxin Lin, Bingquan Wang和Shanpeng
    Wang. 2022. Adam: 一种用于极端多标签文本分类的注意力数据增强方法。收录于*知识发现与数据挖掘进展*，页码131–142，Cham。Springer国际出版公司。'
- en: Zhang and Zhou (2006) Min-Ling Zhang and Zhi-Hua Zhou. 2006. [Multilabel neural
    networks with applications to functional genomics and text categorization](https://doi.org/10.1109/TKDE.2006.162).
    *IEEE Transactions on Knowledge and Data Engineering*, 18(10):1338–1351.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang和Zhou（2006）Min-Ling Zhang和Zhi-Hua Zhou. 2006. [用于功能基因组学和文本分类的多标签神经网络](https://doi.org/10.1109/TKDE.2006.162)。*IEEE知识与数据工程汇刊*，18(10):1338–1351。
- en: 'Zhang et al. (2019) Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang,
    Xi Chen, Wei Zhang, and Huajun Chen. 2019. [Long-tail relation extraction via
    knowledge graph embeddings and graph convolution networks](https://doi.org/10.18653/v1/N19-1306).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pages 3016–3025, Minneapolis, Minnesota. Association for Computational
    Linguistics.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021a) Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, and
    Jian Sun. 2021a. Distribution alignment: A unified framework for long-tail visual
    recognition. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*, pages 2361–2370.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021b) Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and
    Jiashi Feng. 2021b. [Deep long-tailed learning: A survey](http://arxiv.org/abs/2110.04596).
    *CoRR*, abs/2110.04596.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou and Chen (2021) Wenxuan Zhou and Muhao Chen. 2021. [An improved baseline
    for sentence-level relation extraction](http://arxiv.org/abs/2102.01373). *CoRR*,
    abs/2102.01373.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Method Overview
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We here provide details on a selection of methods surveyed in this paper. Table [3](#A2.T3
    "Table 3 ‣ Appendix B Practical advice ‣ A Survey of Methods for Addressing Class
    Imbalance in Deep-Learning Based Natural Language Processing") shows whether they
    have been applied respectively whether they are applicable in binary, multi-class,
    and multi-label classification. Moreover, it contains information on whether authors
    open-sourced their implementation. For links to open-sourced code, see Table [2](#A2.T2
    "Table 2 ‣ Appendix B Practical advice ‣ A Survey of Methods for Addressing Class
    Imbalance in Deep-Learning Based Natural Language Processing").
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Practical advice
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Figure [3](#A2.F3 "Figure 3 ‣ Appendix B Practical advice ‣ A Survey of Methods
    for Addressing Class Imbalance in Deep-Learning Based Natural Language Processing"),
    we provide practical advice which class imbalance methods might be beneficial
    under which circumstances. Due to the lack of an established benchmark, we can
    only give rough guidance.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Link |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| Data Augmentation |  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| EDA (Wei and Zou, [2019](#bib.bib99)) | [GitHub](https://github.com/jasonwei20/eda_nlp)
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| GE3 (Wei, [2021](#bib.bib98)) | [ACL Anthology](https://aclanthology.org/attachments/2021.emnlp-main.479.Software.zip)
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| ECRT (Chen et al., [2021](#bib.bib14)) | [GitHub](https://github.com/ZidiXiu/ECRT)
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| Loss Functions |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| FL (Lin et al., [2017](#bib.bib55)) | [GitHub](https://github.com/facebookresearch/fvcore/blob/main/fvcore/nn/focal_loss.py)
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| ADL (Li et al., [2020](#bib.bib52)) | [GitHub](https://github.com/ShannonAI/dice_loss_for_NLP)
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| LDAM (Cao et al., [2019](#bib.bib11)) | [GitHub](https://github.com/kaidic/LDAM-DRW)
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| DB (Wu et al., [2020](#bib.bib102)) | [GitHub](https://github.com/wutong16/DistributionBalancedLoss)
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| Staged Learning |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| cRT (Kang et al., [2020](#bib.bib43)) | [GitHub](https://github.com/facebookresearch/classifier-balancing)
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| ST (Jang et al., [2021](#bib.bib36)) | [GitHub](https://github.com/joeljang/Sequential-Targeting)
    |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| Model Design |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| $\tau$-norm (Kang et al., [2020](#bib.bib43)) | [GitHub](https://github.com/facebookresearch/classifier-balancing)
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| ProtoBERT (Tänzer et al., [2022](#bib.bib85)) | [GitHub](https://github.com/Michael-Tanzer/BERT-mem-lowres)
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Open-sourced implementations of examples of class imbalance methods.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Binary classification | Multi-class classification | Multi-label
    classification | Code |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| Re-Sampling |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| ROS/RUS (Sec. [3.1](#S3.SS1 "3.1 Re-Sampling ‣ 3 Methods for Addressing Class
    Imbalance in NLP ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing")) | ✓ | ✓ | ? | N/A |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| CAS (Shen et al., [2016](#bib.bib78)) | ✓ | ✓ | ? | × |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| Data Augmentation |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| EDA (Wei and Zou, [2019](#bib.bib99)) | Juuti et al. ([2020](#bib.bib42))
    | Jiang et al. ([2021](#bib.bib38)) | Zhang et al. ([2022](#bib.bib112), [2020](#bib.bib111))
    | ✓ |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '|  | Jiang et al. ([2021](#bib.bib38)) |  |  |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| TextCut (Jiang et al., [2021](#bib.bib38)) | Jiang et al. ([2021](#bib.bib38))
    | Jiang et al. ([2021](#bib.bib38)) | ✓ | × |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| GE3 (Wei, [2021](#bib.bib98)) | ✓ | Wei ([2021](#bib.bib98)) | ? | ✓ |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '|  |  | Wei et al. ([2022](#bib.bib100)) |  |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| MISO (Tian et al., [2021](#bib.bib89)) | Tian et al. ([2021](#bib.bib89))
    | Tian et al. ([2021](#bib.bib89))* | ? | × |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| ECRT (Chen et al., [2021](#bib.bib14)) | ✓ | ✓ | Chen et al. ([2021](#bib.bib14))
    | ✓ |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| Loss Functions |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| WCE (Sec. [3.3](#S3.SS3 "3.3 Loss Functions ‣ 3 Methods for Addressing Class
    Imbalance in NLP ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing")) | [Tayyar Madabushi et al.](#bib.bib86) |
    Adel et al. ([2017](#bib.bib1)) | N/A | N/A |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '|  | ([2019](#bib.bib86)) | Li and Xiao ([2020](#bib.bib49)) |  |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| FL (Lin et al., [2017](#bib.bib55)) | ✓ | Li et al. ([2020](#bib.bib52))
    | ✓ | ✓ |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '|  |  | Nan et al. ([2021](#bib.bib64)) |  |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| ADL (Li et al., [2020](#bib.bib52)) | ✓ | Li et al. ([2020](#bib.bib52))
    | ✓ | ✓ |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '|  |  | Spangher et al. ([2021](#bib.bib80)) |  |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| LDAM (Cao et al., [2019](#bib.bib11)) | Cao et al. ([2019](#bib.bib11)) |
    ✓ | Biswas et al. ([2021](#bib.bib6)) | ✓ |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '|  | Subramanian et al. ([2021](#bib.bib81)) |  |  |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| WBCE (Sec. [3.3](#S3.SS3 "3.3 Loss Functions ‣ 3 Methods for Addressing Class
    Imbalance in NLP ‣ A Survey of Methods for Addressing Class Imbalance in Deep-Learning
    Based Natural Language Processing")) | ✓ | × | Yang et al. ([2020](#bib.bib105))
    | N/A |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| RL (dos Santos et al., [2015](#bib.bib18)) | ✓ | Adel et al. ([2017](#bib.bib1))
    | × | × |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| DB (Wu et al., [2020](#bib.bib102)) | × | × | Huang et al. ([2021](#bib.bib35))
    | ✓ |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| DB (Wu et al., [2020](#bib.bib102)) | × | × | Huang et al. ([2021](#bib.bib35))
    | ✓ |'
- en: '| Staged Learning |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 分阶段学习 |'
- en: '| cRT (Kang et al., [2020](#bib.bib43)) | ✓ | Nan et al. ([2021](#bib.bib64))
    | ✓ | ✓ |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| cRT (Kang et al., [2020](#bib.bib43)) | ✓ | Nan et al. ([2021](#bib.bib64))
    | ✓ | ✓ |'
- en: '|  |  | Hu et al. ([2022](#bib.bib34)) |  |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Hu et al. ([2022](#bib.bib34)) |  |  |'
- en: '| ST (Jang et al., [2021](#bib.bib36)) | Jang et al. ([2021](#bib.bib36)) |
    Jang et al. ([2021](#bib.bib36)) | ✓ | ✓ |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| ST (Jang et al., [2021](#bib.bib36)) | Jang et al. ([2021](#bib.bib36)) |
    Jang et al. ([2021](#bib.bib36)) | ✓ | ✓ |'
- en: '| Model Design |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 模型设计 |'
- en: '| $\tau$-norm (Kang et al., [2020](#bib.bib43)) | ✓ | Nan et al. ([2021](#bib.bib64))
    | ✓ | ✓ |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| $\tau$-范数 (Kang et al., [2020](#bib.bib43)) | ✓ | Nan et al. ([2021](#bib.bib64))
    | ✓ | ✓ |'
- en: '| SetConv (Gao et al., [2020](#bib.bib25)) | Gao et al. ([2020](#bib.bib25))
    | Gao et al. ([2020](#bib.bib25)) | ✓ | × |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| SetConv (Gao et al., [2020](#bib.bib25)) | Gao et al. ([2020](#bib.bib25))
    | Gao et al. ([2020](#bib.bib25)) | ✓ | × |'
- en: '| ProtoBERT (Tänzer et al., [2022](#bib.bib85)) | ✓ | Tänzer et al. ([2022](#bib.bib85))
    | ✓ | ✓ |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| ProtoBERT (Tänzer et al., [2022](#bib.bib85)) | ✓ | Tänzer et al. ([2022](#bib.bib85))
    | ✓ | ✓ |'
- en: '| HSCNN (Yang et al., [2020](#bib.bib105)) | ✓ | ✓ | Yang et al. ([2020](#bib.bib105))
    | × |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| HSCNN (Yang et al., [2020](#bib.bib105)) | ✓ | ✓ | Yang et al. ([2020](#bib.bib105))
    | × |'
- en: 'Table 3: Examples of class imbalance methods and NLP application settings.
    ✓: method applicable (but no particular reference reporting experimental results
    exists)/code: authors open-sourced their implementation, ?: application not straightforward
    / open research issues. *: The authors select only one class as the minority class
    in their experiments. For links to open-sourced code, see Table [2](#A2.T2 "Table
    2 ‣ Appendix B Practical advice ‣ A Survey of Methods for Addressing Class Imbalance
    in Deep-Learning Based Natural Language Processing"). Methods for binary and multi-class
    classification are in general applicable to classification-based relation extraction
    approaches; applying class-imbalance techniques to sequence labeling methods in
    general is similar to the case of multi-label classification. For example, if
    sampling for a particular category, the whole sequence sample may contain additional
    annotations for other categories.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：类别不平衡方法和 NLP 应用设置示例。✓：方法适用（但没有特定参考文献报告实验结果）/代码：作者开源了他们的实现，？：应用不直接/开放研究问题。*：作者在实验中只选择一个类别作为少数类别。有关开源代码的链接，请参见表[2](#A2.T2
    "表 2 ‣ 附录 B 实用建议 ‣ 深度学习基础自然语言处理中的类别不平衡方法调查")。对于二分类和多分类任务的方法，通常适用于基于分类的关系抽取方法；将类别不平衡技术应用于序列标注方法通常类似于多标签分类的情况。例如，如果对特定类别进行采样，则整个序列样本可能包含其他类别的附加注释。
- en: <svg   height="457.42" overflow="visible" version="1.1" width="542.26"><g transform="translate(0,457.42)
    matrix(1 0 0 -1 0 0) translate(227.19,0) translate(0,403.37)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -31.13 4.77)" fill="#000000"
    stroke="#000000"><foreignobject width="62.27" height="28.9" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">classifica-tion type? <g fill="#CCCCFF"><path
    d="M 211.25 0 L 157.48 53.77 L 103.71 0 L 157.48 -53.77 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 126.35 13.07)" fill="#000000" stroke="#000000"><foreignobject width="62.27"
    height="45.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">all classes
    equally important?</foreignobject></g> <g fill="#E6E6FF"><path d="M 309.26 27.67
    L 241.92 27.67 C 238.86 27.67 236.39 25.2 236.39 22.14 L 236.39 -22.14 C 236.39
    -25.2 238.86 -27.67 241.92 -27.67 L 309.26 -27.67 C 312.32 -27.67 314.79 -25.2
    314.79 -22.14 L 314.79 22.14 C 314.79 25.2 312.32 27.67 309.26 27.67 Z M 236.39
    -27.67"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 241 9.9)" fill="#000000"
    stroke="#000000"><foreignobject width="69.19" height="39.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">distribution-balanced loss</foreignobject></g>
    <g fill="#E6E6FF"><path d="M 274.67 -33.14 L 207.33 -33.14 C 204.27 -33.14 201.79
    -35.62 201.79 -38.67 L 201.79 -128.36 C 201.79 -131.42 204.27 -133.89 207.33 -133.89
    L 274.67 -133.89 C 277.72 -133.89 280.2 -131.42 280.2 -128.36 L 280.2 -38.67 C
    280.2 -35.62 277.72 -33.14 274.67 -33.14 Z M 201.79 -133.89"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 206.4 -47.44)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="91.53" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">weighted
    (binary) CE loss upweighting only classes of interest</foreignobject></g> <g fill="#CCCCFF"><path
    d="M 52.43 -118.11 L 0 -65.68 L -52.43 -118.11 L 0 -170.54 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -31.13 -106.39)" fill="#000000" stroke="#000000"><foreignobject width="62.27"
    height="42.82" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">limited
    computational resources?</foreignobject></g> <g fill="#CCCCFF"><path d="M 123.37
    -187.71 L 69.6 -133.93 L 15.82 -187.71 L 69.6 -241.48 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 38.46 -174.64)" fill="#000000" stroke="#000000"><foreignobject width="62.27"
    height="45.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">all classes
    equally important?</foreignobject></g> <g fill="#CCCCFF"><path d="M 113.72 -305.82
    L 69.6 -261.69 L 25.47 -305.82 L 69.6 -349.94 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 38.46 -302.4)" fill="#000000" stroke="#000000"><foreignobject width="62.27"
    height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">catch-all
    class?</foreignobject></g> <g fill="#E6E6FF"><path d="M 172.86 -347.74 L 105.52
    -347.74 C 102.47 -347.74 99.99 -350.22 99.99 -353.28 L 99.99 -397.55 C 99.99 -400.61
    102.47 -403.09 105.52 -403.09 L 172.86 -403.09 C 175.92 -403.09 178.4 -400.61
    178.4 -397.55 L 178.4 -353.28 C 178.4 -350.22 175.92 -347.74 172.86 -347.74 Z
    M 99.99 -403.09"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 104.6 -378.95)"
    fill="#000000" stroke="#000000"><foreignobject width="69.19" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">ranking loss</foreignobject></g> <g fill="#E6E6FF"><path
    d="M 201.69 -278.14 L 134.35 -278.14 C 131.3 -278.14 128.82 -280.62 128.82 -283.68
    L 128.82 -327.96 C 128.82 -331.01 131.3 -333.49 134.35 -333.49 L 201.69 -333.49
    C 204.75 -333.49 207.23 -331.01 207.23 -327.96 L 207.23 -283.68 C 207.23 -280.62
    204.75 -278.14 201.69 -278.14 Z M 128.82 -333.49"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 133.43 -302.4)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">weighted
    CE loss</foreignobject></g> <g fill="#CCCCFF"><path d="M -8.87 -187.71 L -69.6
    -126.98 L -130.33 -187.71 L -69.6 -248.44 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -100.73 -167.68)" fill="#000000" stroke="#000000"><foreignobject width="62.27"
    height="59.42" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">limited
    implementation capacities?</foreignobject></g> <g fill="#E6E6FF"><path d="M -154.04
    -160.03 L -221.38 -160.03 C -224.43 -160.03 -226.91 -162.51 -226.91 -165.57 L
    -226.91 -209.85 C -226.91 -212.9 -224.43 -215.38 -221.38 -215.38 L -154.04 -215.38
    C -150.98 -215.38 -148.5 -212.9 -148.5 -209.85 L -148.5 -165.57 C -148.5 -162.51
    -150.98 -160.03 -154.04 -160.03 Z M -226.91 -215.38"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -222.3 -182.94)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="28.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">random oversampling</foreignobject></g>
    <g fill="#CCCCFF"><path d="M -17.17 -305.82 L -69.6 -253.39 L -122.02 -305.82
    L -69.6 -358.25 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -100.73 -294.09)"
    fill="#000000" stroke="#000000"><foreignobject width="62.27" height="42.82" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">access to good representations?</foreignobject></g>
    <g fill="#E6E6FF"><path d="M -105.52 -347.74 L -172.86 -347.74 C -175.92 -347.74
    -178.4 -350.22 -178.4 -353.28 L -178.4 -397.55 C -178.4 -400.61 -175.92 -403.09
    -172.86 -403.09 L -105.52 -403.09 C -102.47 -403.09 -99.99 -400.61 -99.99 -397.55
    L -99.99 -353.28 C -99.99 -350.22 -102.47 -347.74 -105.52 -347.74 Z M -178.4 -403.09"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -173.79 -380.26)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GE3</foreignobject></g>
    <g fill="#E6E6FF"><path d="M 33.67 -347.74 L -33.67 -347.74 C -36.73 -347.74 -39.2
    -350.22 -39.2 -353.28 L -39.2 -397.55 C -39.2 -400.61 -36.73 -403.09 -33.67 -403.09
    L 33.67 -403.09 C 36.73 -403.09 39.2 -400.61 39.2 -397.55 L 39.2 -353.28 C 39.2
    -350.22 36.73 -347.74 33.67 -347.74 Z M -39.2 -403.09"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -34.59 -380.26)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">EDA</foreignobject></g>
    <g stroke-width="1.2pt"><g stroke="#808080" fill="#808080"><path d="M 45.86 0
    L 98 0" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 98 0)" color="#808080"><path
    d="M 5.31 0 C 3.1 0.44 -0.89 1.33 -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32
    C -0.89 -1.33 3.1 -0.44 5.31 0 Z" style="stroke:none"></path></g></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 41.92 5.44)" fill="#000000" stroke="#000000"
    color="#808080"><foreignobject width="64.96" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">multi-label</foreignobject></g></g><g stroke-width="1.2pt"><g
    stroke="#808080" fill="#808080"><path d="M 211.64 0 L 230.8 0" style="fill:none"><g
    transform="matrix(1.0 0.0 0.0 1.0 230.8 0)" color="#808080"><path d="M 5.31 0
    C 3.1 0.44 -0.89 1.33 -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32 C -0.89
    -1.33 3.1 -0.44 5.31 0 Z" style="stroke:none"></path></g></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 214.61 8.13)" fill="#000000" stroke="#000000" color="#808080"><foreignobject
    width="18.53" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">yes</foreignobject></g></g><g
    stroke-width="1.2pt"><g stroke="#808080" fill="#808080"><path d="M 184.55 -27.09
    L 197.76 -40.31" style="fill:none"><g transform="matrix(0.70686 -0.70734 0.70734
    0.70686 197.76 -40.31)" color="#808080"><path d="M 5.31 0 C 3.1 0.44 -0.89 1.33
    -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32 C -0.89 -1.33 3.1 -0.44 5.31 0
    Z" style="stroke:none"></path></g></path></g><g transform="matrix(1.0 0.0 0.0
    1.0 198.48 -30.14)" fill="#000000" stroke="#000000" color="#808080"><foreignobject
    width="14.61" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">no</foreignobject></g></g><g
    stroke-width="1.2pt"><g stroke="#808080" fill="#808080"><path d="M 0 -45.86 L
    0 -59.98" style="fill:none"><g transform="matrix(0.0 -1.0 1.0 0.0 0 -59.98)" color="#808080"><path
    d="M 5.31 0 C 3.1 0.44 -0.89 1.33 -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32
    C -0.89 -1.33 3.1 -0.44 5.31 0 Z" style="stroke:none"></path></g></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 5.44 -59.04)" fill="#000000" stroke="#000000"
    color="#808080"><foreignobject width="66.96" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">single-label</foreignobject></g></g><g stroke-width="1.2pt"><g
    stroke="#808080" fill="#808080"><path d="M 26.41 -144.52 L 38.76 -156.87" style="fill:none"><g
    transform="matrix(0.7071 -0.7071 0.7071 0.7071 38.76 -156.87)" color="#808080"><path
    d="M 5.31 0 C 3.1 0.44 -0.89 1.33 -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32
    C -0.89 -1.33 3.1 -0.44 5.31 0 Z" style="stroke:none"></path></g></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 39.9 -144.44)" fill="#000000" stroke="#000000"
    color="#808080"><foreignobject width="18.53" height="8.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">yes</foreignobject></g></g><g stroke-width="1.2pt"><g
    stroke="#808080" fill="#808080"><path d="M 103.28 -167.23 L 196.98 -110.27" style="fill:none"><g
    transform="matrix(0.8545 0.51945 -0.51945 0.8545 196.98 -110.27)" color="#808080"><path
    d="M 5.31 0 C 3.1 0.44 -0.89 1.33 -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32
    C -0.89 -1.33 3.1 -0.44 5.31 0 Z" style="stroke:none"></path></g></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 132.35 -131.93)" fill="#000000" stroke="#000000"
    color="#808080"><foreignobject width="14.61" height="5.96" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">no</foreignobject></g></g><g stroke-width="1.2pt"><g
    stroke="#808080" fill="#808080"><path d="M 69.6 -241.87 L 69.6 -255.99" style="fill:none"><g
    transform="matrix(0.0 -1.0 1.0 0.0 69.6 -255.99)" color="#808080"><path d="M 5.31
    0 C 3.1 0.44 -0.89 1.33 -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32 C -0.89
    -1.33 3.1 -0.44 5.31 0 Z" style="stroke:none"></path></g></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 75.04 -253.22)" fill="#000000" stroke="#000000" color="#808080"><foreignobject
    width="18.53" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">yes</foreignobject></g></g><g
    stroke-width="1.2pt"><g stroke="#808080" fill="#808080"><path d="M 91.86 -328.07
    L 107.51 -343.71" style="fill:none"><g transform="matrix(0.70728 -0.70694 0.70694
    0.70728 107.51 -343.71)" color="#808080"><path d="M 5.31 0 C 3.1 0.44 -0.89 1.33
    -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32 C -0.89 -1.33 3.1 -0.44 5.31 0
    Z" style="stroke:none"></path></g></path></g><g transform="matrix(1.0 0.0 0.0
    1.0 107.01 -329.63)" fill="#000000" stroke="#000000" color="#808080"><foreignobject
    width="18.53" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">yes</foreignobject></g></g><g
    stroke-width="1.2pt"><g stroke="#808080" fill="#808080"><path d="M 114.11 -305.82
    L 123.23 -305.82" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 123.23
    -305.82)" color="#808080"><path d="M 5.31 0 C 3.1 0.44 -0.89 1.33 -3.54 3.32 C
    -1.33 0.89 -1.33 -0.89 -3.54 -3.32 C -0.89 -1.33 3.1 -0.44 5.31 0 Z" style="stroke:none"></path></g></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 114.02 -300.38)" fill="#000000" stroke="#000000"
    color="#808080"><foreignobject width="14.61" height="5.96" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">no</foreignobject></g></g><g stroke-width="1.2pt"><g
    stroke="#808080" fill="#808080"><path d="M -26.41 -144.52 L -35.28 -153.39" style="fill:none"><g
    transform="matrix(-0.7071 -0.7071 0.7071 -0.7071 -35.28 -153.39)" color="#808080"><path
    d="M 5.31 0 C 3.1 0.44 -0.89 1.33 -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32
    C -0.89 -1.33 3.1 -0.44 5.31 0 Z" style="stroke:none"></path></g></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -27.28 -162.23)" fill="#000000" stroke="#000000"
    color="#808080"><foreignobject width="14.61" height="5.96" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">no</foreignobject></g></g><g stroke-width="1.2pt"><g
    stroke="#808080" fill="#808080"><path d="M -130.72 -187.71 L -142.91 -187.71"
    style="fill:none"><g transform="matrix(-1.0 0.0 0.0 -1.0 -142.91 -187.71)" color="#808080"><path
    d="M 5.31 0 C 3.1 0.44 -0.89 1.33 -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32
    C -0.89 -1.33 3.1 -0.44 5.31 0 Z" style="stroke:none"></path></g></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -148.74 -199.11)" fill="#000000" stroke="#000000"
    color="#808080"><foreignobject width="18.53" height="8.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">yes</foreignobject></g></g><g stroke-width="1.2pt"><g
    stroke="#808080" fill="#808080"><path d="M -69.6 -248.83 L -69.6 -247.68" style="fill:none"><g
    transform="matrix(0.0 -1.0 1.0 0.0 -69.6 -247.68)" color="#808080"><path d="M
    5.31 0 C 3.1 0.44 -0.89 1.33 -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32 C
    -0.89 -1.33 3.1 -0.44 5.31 0 Z" style="stroke:none"></path></g></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -64.16 -253.89)" fill="#000000" stroke="#000000" color="#808080"><foreignobject
    width="14.61" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">no</foreignobject></g></g><g
    stroke-width="1.2pt"><g stroke="#808080" fill="#808080"><path d="M -96.01 -332.22
    L -107.51 -343.71" style="fill:none"><g transform="matrix(-0.70728 -0.70694 0.70694
    -0.70728 -107.51 -343.71)" color="#808080"><path d="M 5.31 0 C 3.1 0.44 -0.89
    1.33 -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32 C -0.89 -1.33 3.1 -0.44 5.31
    0 Z" style="stroke:none"></path></g></path></g><g transform="matrix(1.0 0.0 0.0
    1.0 -98.2 -351.24)" fill="#000000" stroke="#000000" color="#808080"><foreignobject
    width="18.53" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">yes</foreignobject></g></g><g
    stroke-width="1.2pt"><g stroke="#808080" fill="#808080"><path d="M -43.18 -332.22
    L -31.69 -343.71" style="fill:none"><g transform="matrix(0.70728 -0.70694 0.70694
    0.70728 -31.69 -343.71)" color="#808080"><path d="M 5.31 0 C 3.1 0.44 -0.89 1.33
    -3.54 3.32 C -1.33 0.89 -1.33 -0.89 -3.54 -3.32 C -0.89 -1.33 3.1 -0.44 5.31 0
    Z" style="stroke:none"></path></g></path></g><g transform="matrix(1.0 0.0 0.0
    1.0 -30.11 -334.4)" fill="#000000" stroke="#000000" color="#808080"><foreignobject
    width="14.61" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">no</foreignobject></g></g>
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Practical advice which methods to try under which circumstances.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：实用建议，在哪些情况下尝试哪些方法。
