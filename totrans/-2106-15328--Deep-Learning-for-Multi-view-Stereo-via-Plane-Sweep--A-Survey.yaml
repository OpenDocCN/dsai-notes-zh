- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:53:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[2106.15328] Deep Learning for Multi-view Stereo via Plane Sweep: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.15328](https://ar5iv.labs.arxiv.org/html/2106.15328)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Multi-view Stereo via Plane Sweep: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Qingtian Zhu¹, Chen Min^(1,2), Zizhuang Wei¹, Yisong Chen¹, and Guoping Wang¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: ¹Peking University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: ²National Innovation Institute of Defense Technology
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: zqt@stu.pku.edu.cn
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 3D reconstruction has lately attracted increasing attention due to its wide
    application in many areas, such as autonomous driving, robotics and virtual reality.
    As a dominant technique in artificial intelligence, deep learning has been successfully
    adopted to solve various computer vision problems. However, deep learning for
    3D reconstruction is still at its infancy due to its unique challenges and varying
    pipelines. To stimulate future research, this paper presents a review of recent
    progress in deep learning methods for Multi-view Stereo (MVS), which is considered
    as a crucial task of image-based 3D reconstruction. It also presents comparative
    results on several publicly available datasets, with insightful observations and
    inspiring future research directions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the rapid development of 3D acquisition techniques, depth sensors are becoming
    increasingly affordable and reliable, such as LiDARs. These sensors have been
    widely equipped for real-time tasks to obtain a rough estimation of surrounding
    environment, e.g., in simultaneous localization and mapping (SLAM). However, depth
    maps captured by depth sensors are usually sparse due to hardware and power limitations
    at edge devices, so that delicate details are abandoned in exchange for computational
    efficiency. Another pipeline is to reconstruct 3D models from a series of images.
    In this case, depth values are computed by matching 2D images and the whole reconstruction
    is done off-line. Provided that capturing images is more economical and available
    than acquiring depth maps via depth sensors, image-based 3D reconstruction is
    a better option for time-insensitive tasks. Besides, images actually contain information
    that depth sensors cannot capture, such as texture and lighting. These clues are
    crucial for reconstructing more delicate and detailed 3D models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Multi-view Stereo (MVS) is a computationally expensive procedure of image-based
    3D reconstruction. The most universal definition of the task is stated as follows.
    Given a series of images with their respective calibrated camera parameters, MVS
    aims to estimate a depth map for each image and then reconstruct a dense point
    cloud of the scene. Most previous attempts [[26](#bib.bib26), [27](#bib.bib27)]
    adopt this definition. Deep learning has shown its effectiveness in many computer
    vision tasks. For binocular stereo, [[18](#bib.bib18)] discretizes the depth space
    and turns the task of stereo into a classification problem. Plane sweep algorithm [[4](#bib.bib4)]
    extends this pattern to multi-image matching, whose pattern is suitable for deep
    CNNs to handle.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角立体视觉（MVS）是一种计算开销大的基于图像的3D重建过程。任务的最普遍定义如下：给定一系列图像及其相应的标定相机参数，MVS旨在为每张图像估计深度图，然后重建场景的稠密点云。大多数以前的尝试[[26](#bib.bib26),
    [27](#bib.bib27)]采用了这一定义。深度学习在许多计算机视觉任务中已显示出其有效性。对于双目立体视觉，[[18](#bib.bib18)]将深度空间离散化，将立体视觉任务转化为分类问题。平面扫描算法[[4](#bib.bib4)]将这一模式扩展到多图像匹配，这种模式适合深度CNN处理。
- en: 'To restate, this paper surveys learning-based MVS methods that build matching
    cost volumes via plane sweep algorithm and yield per-view depth maps as intermediate
    representation to reconstruct dense 3D point clouds. Each image within a scan
    takes turns to be the reference image to estimate depth maps and its $N-1$ neighboring
    images as source images. In total $N$ images are sent into the network as inputs
    to produce one depth map and corresponding confidence map. The dense point cloud
    is then obtained by filtering and fusing depth maps for all images, The pipeline
    is also shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for
    Multi-view Stereo via Plane Sweep: A Survey").'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '重申一下，本文综述了基于学习的MVS方法，这些方法通过平面扫描算法构建匹配成本体积，并产生每视图深度图作为中间表示，以重建稠密的3D点云。扫描中的每张图像轮流作为参考图像来估计深度图，并以其$N-1$个邻近图像作为源图像。总共$N$张图像被送入网络作为输入，以生成一个深度图和相应的置信度图。然后，通过过滤和融合所有图像的深度图来获得稠密点云。该流程也显示在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Multi-view Stereo via Plane Sweep:
    A Survey")中。'
- en: '![Refer to caption](img/1a1d5fb4827bc7231fae18c797a07bb0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1a1d5fb4827bc7231fae18c797a07bb0.png)'
- en: 'Figure 1: Input, intermediate outputs and final output from a typical MVS pipeline.
    Reference image (a), together with its $N-1$ neighboring images, are the inputs
    of network and its depth map (b) and confidence map (c) are produced accordingly.
    Depth maps of all images are filtered and fused into the reconstructed dense point
    cloud (d).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：典型的MVS流程中的输入、中间输出和最终输出。参考图像(a)以及其$N-1$个邻近图像是网络的输入，其深度图(b)和置信度图(c)会相应生成。所有图像的深度图被过滤并融合成重建的稠密点云(d)。
- en: 2 Background
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 'MVS plays as a key component of image-based 3D reconstruction pipeline. In
    this section, background knowledge is presented. Sec. [2.1](#S2.SS1 "2.1 Structure
    from Motion ‣ 2 Background ‣ Deep Learning for Multi-view Stereo via Plane Sweep:
    A Survey") introduces preproposed Structure from Motion (SfM) as the source of
    camera calibration. Sec. [2.2](#S2.SS2 "2.2 Plane Sweep ‣ 2 Background ‣ Deep
    Learning for Multi-view Stereo via Plane Sweep: A Survey") explains how to build
    cost volumes in learning-based MVS methods, which is a key step to enable CNNs
    to predict depth. Post-processes after obtaining depth maps, including depth filter
    and fusion, are presented in Sec. [2.3](#S2.SS3 "2.3 Depth Filtering & Fusion
    ‣ 2 Background ‣ Deep Learning for Multi-view Stereo via Plane Sweep: A Survey").
    Sec. [2.4](#S2.SS4 "2.4 Datasets ‣ 2 Background ‣ Deep Learning for Multi-view
    Stereo via Plane Sweep: A Survey") lists several well-known open datasets for
    MVS and Sec. [2.5](#S2.SS5 "2.5 Evaluation Metrics ‣ 2 Background ‣ Deep Learning
    for Multi-view Stereo via Plane Sweep: A Survey") lists metrics used for evaluation.
    Sec. [2.6](#S2.SS6 "2.6 Loss Function ‣ 2 Background ‣ Deep Learning for Multi-view
    Stereo via Plane Sweep: A Survey") covers loss functions used for learning-based
    MVS. For clarification, this survey only cover learning-based MVS methods taking
    advantage of plane sweep algorithm.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MVS 作为基于图像的 3D 重建管道的关键组件。在本节中，介绍了背景知识。第 [2.1](#S2.SS1 "2.1 从运动中重建 ‣ 2 背景 ‣ 基于平面扫描的多视图立体深度学习：综述")
    节介绍了预先提出的从运动中重建（SfM）作为相机校准的来源。第 [2.2](#S2.SS2 "2.2 平面扫描 ‣ 2 背景 ‣ 基于平面扫描的多视图立体深度学习：综述")
    节解释了如何在基于学习的 MVS 方法中构建成本体积，这是使 CNN 预测深度的关键步骤。获得深度图后的后处理，包括深度滤波和融合，见第 [2.3](#S2.SS3
    "2.3 深度滤波与融合 ‣ 2 背景 ‣ 基于平面扫描的多视图立体深度学习：综述") 节。第 [2.4](#S2.SS4 "2.4 数据集 ‣ 2 背景
    ‣ 基于平面扫描的多视图立体深度学习：综述") 节列出了几个著名的 MVS 开放数据集，第 [2.5](#S2.SS5 "2.5 评估指标 ‣ 2 背景 ‣
    基于平面扫描的多视图立体深度学习：综述") 节列出了用于评估的指标。第 [2.6](#S2.SS6 "2.6 损失函数 ‣ 2 背景 ‣ 基于平面扫描的多视图立体深度学习：综述")
    节涵盖了用于基于学习的 MVS 的损失函数。为澄清，本综述仅涵盖利用平面扫描算法的基于学习的 MVS 方法。
- en: 2.1 Structure from Motion
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 从运动中重建
- en: MVS requires calibrated camera parameter to obtain image-wise adjacency, which
    is usually achieved by Structure from Motion algorithms. SfM is usually categorized
    into incremental and global ones. Generally speaking, incremental pipelines solve
    the optimization problem locally and merge new cameras into known tracks. Thus
    incremental methods are slower but more robust and accurate. Global SfM is more
    scalable and can often converge to a pretty good solution but is more susceptible
    to outliers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MVS 需要校准的相机参数来获取图像间的邻接关系，这通常通过从运动中重建算法实现。SfM 通常分为增量式和全局式。一般来说，增量式管道在局部解决优化问题，并将新相机合并到已知轨迹中。因此，增量式方法较慢，但更稳健和准确。全局
    SfM 更具可扩展性，通常可以收敛到相当好的解决方案，但更容易受到离群点的影响。
- en: Specifically for MVS, camera calibration means for each image, a camera extrinsic
    matrix $\mathbf{T}$, a camera intrinsic matrix $\mathbf{K}$, a depth range $[d_{min},d_{max}]$
    are acquired by SfM. For most MVS methods, COLMAP [[20](#bib.bib20)] provides
    a good enough estimation of cameras.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于 MVS，相机校准意味着对于每张图像，通过 SfM 获得一个相机外参矩阵 $\mathbf{T}$、一个相机内参矩阵 $\mathbf{K}$
    以及一个深度范围 $[d_{min},d_{max}]$。对于大多数 MVS 方法，COLMAP [[20](#bib.bib20)] 提供了足够好的相机估计。
- en: 2.2 Plane Sweep
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 平面扫描
- en: '![Refer to caption](img/ba0e0061831f211365d7c8f4ae6d21d1.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ba0e0061831f211365d7c8f4ae6d21d1.png)'
- en: 'Figure 2: Illustration of plane sweep algorithm. To estimate the depth map
    for reference camera, neighboring source images are projected by homography to
    fronto-parallel planes of the frustum of the reference camera.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：平面扫描算法的示意图。为了估算参考相机的深度图，通过单应性将相邻源图像投影到参考相机视锥体的前平行平面上。
- en: The main principle of plane sweep stereo [[4](#bib.bib4)] is that for each depth,
    source images are projected to fronto-parallel planes of the reference camera
    frustum and those depth hypotheses with high similarity of projected images are
    more reliable. Most learning-based MVS methods rely on plane sweep algorithm to
    generate cost volumes. This practice is deeply inspired by binocular stereo. In
    learning-based binocular stereo methods, instead of regressing depth values directly,
    disparity values, which describe the pixel-level distance between the two views,
    are estimated. With knowledge of epipolar geometry, depth values can be computed
    from estimated disparity values. Besides, since the unit of disparity values are
    pixels, this task becomes a classification task where each class represents a
    discretized disparity. This common practice has two underlying advantages. First,
    the estimation of depth is now scale-irrelevant since the unit is pixels, not
    meters or other measurement of actual distance. Second, CNNs are considered as
    better at classification than regression, so this helps to yield more reliable
    results. This discretization relies on plane sweep algorithm.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 平面扫描立体的主要原理[[4](#bib.bib4)]是，对于每个深度，源图像被投影到参考相机视锥的前平行平面上，投影图像的高相似度的深度假设更为可靠。大多数基于学习的
    MVS 方法依赖于平面扫描算法来生成代价体积。这一做法深受双目立体的启发。在基于学习的双目立体方法中，不是直接回归深度值，而是估计视差值，即描述两个视图之间像素级距离的值。通过已知的视差几何，可以从估计的视差值计算深度值。此外，由于视差值的单位是像素，这项任务变成了一个分类任务，其中每个类别代表一个离散的视差。这种常见做法有两个潜在优点。首先，由于单位是像素，而不是米或其他实际距离的测量单位，深度估计现在与尺度无关。其次，卷积神经网络（CNN）被认为在分类方面优于回归，因此这有助于产生更可靠的结果。这种离散化依赖于平面扫描算法。
- en: The core of plane sweep algorithm is to verify depth hypotheses. After projecting
    pixels into space by a hypothetical depth, plane sweep algorithm says that if
    a hypothetical point in space is captured by different cameras with a similar
    photometry, this point is likely to be a real point, which is to say, the hypothesis
    of depth ($z$ value) is valid. In this case, we can divide the depth interval
    into discretized values and make hypotheses with these values. The final depth
    is estimated by choosing the most valid depth among all hypotheses. When it comes
    to implementation, there are two problems remaining. One is to match pixels across
    different images or to build a homography between views; the other is to measure
    the similarity of photometry. Note that considering individual pixel RGB color
    is not robust enough for matching, photometry is usually replaced by a feature
    map extracted from the original image.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 平面扫描算法的核心是验证深度假设。在通过假设深度将像素投影到空间后，平面扫描算法指出，如果一个假设的空间点被不同的相机以相似的光度捕捉到，则该点可能是一个真实点，也就是说，深度（$z$
    值）的假设是有效的。在这种情况下，我们可以将深度区间划分为离散值，并用这些值进行假设。最终深度通过选择所有假设中最有效的深度来估计。实现过程中，还有两个剩余问题。一个是匹配不同图像中的像素或在视图之间建立单应性；另一个是测量光度的相似性。注意，考虑单个像素的
    RGB 颜色在匹配中不够稳健，光度通常用从原始图像中提取的特征图替代。
- en: For a pair of calibrated binocular images, since the two main optical axes are
    always parallel, we only need to shift one view to another by disparity hypotheses.
    Things get a little more complicated for MVS as cameras are distributed over the
    space without epipolar constraint. At depth hypothesis $d$, we first project all
    pixels of a source image into space with $d$ and then back-warp these points through
    the reference camera. Thus the homography between the $i$-th source image and
    the reference image is
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一对经过校准的双目图像，由于两个主要光轴始终平行，我们只需要通过视差假设将一个视图移到另一个视图上。对于多视图立体视觉（MVS），情况就复杂一些，因为相机分布在空间中，没有视差约束。在深度假设
    $d$ 下，我们首先将源图像的所有像素投影到深度为 $d$ 的空间中，然后通过参考相机将这些点反向扭曲。因此，第 $i$ 张源图像和参考图像之间的单应性为
- en: '|  | $H_{i}(d)=d\mathbf{K}_{0}\mathbf{T}_{0}\mathbf{T}^{-1}_{i}\mathbf{K}^{-1}_{i},$
    |  | (1) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{i}(d)=d\mathbf{K}_{0}\mathbf{T}_{0}\mathbf{T}^{-1}_{i}\mathbf{K}^{-1}_{i},$
    |  | (1) |'
- en: where $\mathbf{K}_{0}$ and $\mathbf{T}_{0}$ are camera intrinsics and extrinsics
    of reference image.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{K}_{0}$ 和 $\mathbf{T}_{0}$ 是参考图像的相机内参和外参。
- en: Measurement of photometric similarity varies from one method to another. For
    binocular stereo, [[5](#bib.bib5), [17](#bib.bib17)] introduce a correlation layer
    to compute inner product of feature vectors; GC-Net [[13](#bib.bib13)] concatenates
    feature vectors together. For MVS where the number of cameras is larger than two,
    there are two main options. MVSNet [[26](#bib.bib26)] applies variance for all
    feature vectors; DPSNet [[11](#bib.bib11)] concatenates features pairwise and
    a final cost volume is obtained by averaging all $N-1$ volumes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 计算光度相似性的测量方法因方法而异。对于双目立体，[[5](#bib.bib5)、[17](#bib.bib17)] 引入了一个相关层来计算特征向量的内积；GC-Net
    [[13](#bib.bib13)] 将特征向量连接在一起。对于数量大于两个的MVS，相主要有两个选项。MVSNet [[26](#bib.bib26)]
    对所有特征向量应用方差；DPSNet [[11](#bib.bib11)] 将特征成对连接，并通过对所有 $N-1$ 个体积进行平均来获得最终的成本体积。
- en: 'It is worth noting that the division of depth space is a crucial problem to
    yield good results, which will be covered later in Sec. [2.6](#S2.SS6 "2.6 Loss
    Function ‣ 2 Background ‣ Deep Learning for Multi-view Stereo via Plane Sweep:
    A Survey").'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，深度空间的划分是获得良好结果的关键问题，这将在后面的第[2.6节](#S2.SS6 "2.6 损失函数 ‣ 2 背景 ‣ 基于平面扫描的多视角立体深度学习综述")中讨论。
- en: To restate with an example, provided that the image resolution is $H\times W$
    and the number of total depth hypotheses is $D$, assuming the dimension of per-pixel
    feature vectors is $F$, MVS methods build a cost volume of $H\times W\times D\times
    F$ from image features and this cost volume is then regularized by a neural network
    to obtain a depth map.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个例子重新陈述，假设图像分辨率为 $H\times W$，总深度假设的数量为 $D$，每像素特征向量的维度为 $F$，MVS方法从图像特征中构建一个
    $H\times W\times D\times F$ 的成本体积，然后通过神经网络对该成本体积进行正则化，以获得深度图。
- en: 2.3 Depth Filtering & Fusion
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 深度过滤与融合
- en: Assuming that all depth maps have been obtained by MVS methods, the next step
    is to filter and fuse depth maps into a dense point cloud. Since image-based 3D
    reconstruction is scale-irrelevant, the estimated depth values are actually the
    $z$ values for pixels in the local camera coordinate system. Thus the fusion of
    depth maps are rather straightforward that all we need to do is to project all
    pixels into 3D space through cameras. The transformation between image coordinate
    and world coordinate is
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有深度图已通过MVS方法获得，下一步是过滤和融合深度图，生成稠密点云。由于基于图像的3D重建与尺度无关，估计的深度值实际上是局部相机坐标系中像素的
    $z$ 值。因此，深度图的融合非常简单，我们只需通过相机将所有像素投影到3D空间中即可。图像坐标与世界坐标之间的变换是
- en: '|  | $\mathbf{P}_{w}=d\mathbf{T}^{-1}\mathbf{K}^{-1}\mathbf{P}_{x},$ |  | (2)
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{P}_{w}=d\mathbf{T}^{-1}\mathbf{K}^{-1}\mathbf{P}_{x},$ |  | (2)
    |'
- en: where $\mathbf{P}_{x}$ and $\mathbf{P}_{w}$ denote pixel coordinates in image
    coordinate and world coordinate respectively.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{P}_{x}$ 和 $\mathbf{P}_{w}$ 分别表示图像坐标和世界坐标中的像素坐标。
- en: However, not all pixels are suitable to be preserved in the final point cloud,
    e.g., those with low confidence and those at infinity, such as sky. To overcome
    this problem, depth maps are filtered before fused. Since learning-based MVS methods
    adopt a classification fashion, each depth map is yielded together with a confidence
    map correspondingly. So naturally, a threshold can be set to filter depth values
    with low confidence. Besides, depth values can be across-checked among neighboring
    views. This strategy of filtering is based on reprojection error, which is commonly
    used in Bundle Adjustment of SfM.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有像素都适合保留在最终的点云中，例如，低置信度的像素和无限远处的像素，如天空。为解决此问题，深度图在融合之前需要进行过滤。由于基于学习的多视角立体（MVS）方法采用分类方式，每个深度图都会生成一个相应的置信度图。因此，自然可以设置一个阈值来过滤低置信度的深度值。此外，可以在相邻视图之间交叉检查深度值。这种过滤策略基于重投影误差，通常用于SfM的束束调整。
- en: 'Taking the strategy in [[29](#bib.bib29)] as an example, which is also shown
    in Fig. [3](#S2.F3 "Figure 3 ‣ 2.3 Depth Filtering & Fusion ‣ 2 Background ‣ Deep
    Learning for Multi-view Stereo via Plane Sweep: A Survey"), by mapping a pixel
    $\mathbf{P}$ in image $\mathbf{I}_{i}$ to its neighboring view $\mathbf{I}_{j}$
    through estimated depth $D_{i}(\mathbf{P})$, we obtain a new pixel $\mathbf{P}^{\prime}$.
    As $\mathbf{I}_{j}$ also has its depth map, we can get $D_{j}(\mathbf{P}^{\prime})$
    accordingly. In turn, $\mathbf{P}^{\prime}$ can be projected to $\mathbf{I}_{i}$
    at $\mathbf{P}^{\prime\prime}$ with depth $D_{j}(\mathbf{P}^{\prime})$. Depth
    estimation of $\mathbf{P}^{\prime\prime}$ in $\mathbf{I}_{i}$ is denoted as $D_{i}(\mathbf{P}^{\prime\prime})$.
    The constraints for depth filtering are'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\&#124;\mathbf{P}-\mathbf{P}^{\prime\prime}\&#124;_{2}\leq\tau_{1},$
    |  | (3) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '|  | $\frac{\&#124;D_{i}(\mathbf{P}^{\prime\prime})-D_{i}(\mathbf{P})\&#124;_{1}}{D_{i}(\mathbf{P})}\leq\tau_{2},$
    |  | (4) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: where $\tau_{1}$ and $\tau_{2}$ are threshold values. As for [[29](#bib.bib29)],
    pixels satisfying these constraints under at least 3 neighboring views are considered
    as valid enough to remain.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8203c1b259795bba8d43facf5014920b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of checking geometric consistency among neighboring
    views by measuring reprojection error.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that depth filtering and fusion methods are often uncovered
    in papers though they might be of great importance to obtain good results.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Datasets
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tab. [1](#S2.T1 "Table 1 ‣ 2.4 Datasets ‣ 2 Background ‣ Deep Learning for
    Multi-view Stereo via Plane Sweep: A Survey") is a brief summary of released MVS
    datasets. Note that for MVS training, depth is required while evaluation is based
    on point clouds. Surface reconstruction is required to render depth maps from
    point clouds and depth fusion is required to evaluate reconstruction quality.
    Besides, if a dataset does not consist of ground truth camera calibration or uses
    an open-source software to obtain ground truth calibration, then it might not
    be suitable for training since plane sweep is sensitive to noises in camera calibration.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: An overview of public datasets for MVS.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Provided Ground Truth¹ | Synthetic | Online | Evaluation |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| Camera | Depth | Point Cloud | Benchmark | Target |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| DTU [[1](#bib.bib1)] | ✓ |  | ✓ |  |  | Point Cloud |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| Tanks and Temples [[14](#bib.bib14)] |  |  | ✓ |  | ✓ | Point Cloud |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| ETH3D [[22](#bib.bib22)] | ✓ |  | ✓ |  | ✓ | Point Cloud |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| BlendedMVS [[28](#bib.bib28)] | ✓ | ✓ |  | ✓ |  | Depth Map |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '1'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For datasets with online benchmark, ground truth of test set (excluding camera
    parameters) is not released.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: DTU
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: DTU dataset [[1](#bib.bib1)] is an indoor MVS dataset collected under well-controlled
    laboratory conditions with accurate camera trajectory. It contains 128 scans with
    49 views under 7 different lighting conditions and is split into 79 training scans,
    18 validation scans and 22 evaluation scans. By setting each image as reference,
    there are 27097 training samples in total. DTU dataset officially provides ground
    truth point clouds, rather than depth maps, which means surface reconstruction
    is required to generate mesh models and render depth maps. Normally, screened
    Poisson surface reconstruction algorithm [[12](#bib.bib12)] is adopted.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Tanks and Temples
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Tanks and Temples [[14](#bib.bib14)] is a large-scale online benchmark captured
    in more complex real indoor and outdoor scenarios. It contains an intermediate
    set and an advanced set. Different scenes have different scales, surface reflection
    and exposure conditions. Evaluation of Tanks and Temples is done online by uploading
    reconstructed points to its official website. Note that Tanks and Temples does
    not provide ground truth camera parameters. A training set with ground truth point
    clouds is available, which is usually used for local off-line validation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: ETH3D
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: ETH3D [[22](#bib.bib22)] is a comprehensive benchmark for both SLAM and stereo
    tasks. Considering MVS, it contains 25 high-resolution scenes and 10 low-resolution
    scenes. ETH3D is widely acknowledged as the most difficult MVS task since it contains
    many low-textured regions such as white walls and reflective floor. Traditional
    MVS methods based on broadcasting valid depth values perform better in this case.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: BlendedMVS
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: BlendedMVS dataset [[28](#bib.bib28)] is a recently published large-scale synthetic
    dataset for MVS training that contains a variety of scenes, such as cities, sculptures
    and shoes. The dataset consists of over 17k high-resolution images rendered with
    reconstructed models and is split into 106 training scenes and 7 validation scenes.
    Since BlendedMVS is obtained through virtual cameras, its provided camera calibration
    is reliable enough for MVS training.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Evaluation Metrics
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As is mentioned in Tab. [1](#S2.T1 "Table 1 ‣ 2.4 Datasets ‣ 2 Background ‣
    Deep Learning for Multi-view Stereo via Plane Sweep: A Survey"), most datasets
    provide point clouds as ground truth instead of depth maps and evaluation metrics
    are usually based on quality of reconstructed dense point clouds. Since point
    clouds are actually unordered points with permutation invariance, before comparison,
    reconstructed point clouds should be aligned to ground truth point clouds through
    per-view camera parameters.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Absolute Error
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Though benchmarks adopt point-cloud-based metrics for ranking, depth-based metrics
    can still be used for validation during network training. Absolution error is
    commonly used to measure the quality of depth maps. A common practice is to use
    multiple thresholds to show a more overall performance of networks, e.g., 2-px
    absolute error, 4-px absolute error, 6-px absolute error and 8-px absolute error.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基准测试采用基于点云的度量进行排名，但在网络训练期间仍然可以使用基于深度的度量进行验证。绝对误差通常用于测量深度图的质量。一个常见的做法是使用多个阈值来显示网络的整体性能，例如
    2-px 绝对误差、4-px 绝对误差、6-px 绝对误差和 8-px 绝对误差。
- en: Precision/Accuracy
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 精度/准确率
- en: Precision/Accuracy is a measurement of what percentage of predicted points can
    get matched in the ground truth point cloud. Considering a point $\mathbf{P}_{p}$
    in the predicted point cloud, it is considered to have a good match in the ground
    truth point cloud $\{\mathbf{P}_{g}\}$ if
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 精度/准确率是衡量预测点与真实点云中匹配的百分比。考虑预测点云中的一个点 $\mathbf{P}_{p}$，如果它在真实点云 $\{\mathbf{P}_{g}\}$
    中有一个良好的匹配，则满足
- en: '|  | $\&#124;\mathbf{P}_{p}-\mathop{\arg\min}\limits_{\mathbf{P}\in\{\mathbf{P}_{g}\}}\&#124;\mathbf{P}-\mathbf{P}_{p}\&#124;_{2}\&#124;_{2}\leq\lambda,$
    |  | (5) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\&#124;\mathbf{P}_{p}-\mathop{\arg\min}\limits_{\mathbf{P}\in\{\mathbf{P}_{g}\}}\&#124;\mathbf{P}-\mathbf{P}_{p}\&#124;_{2}\&#124;_{2}\leq\lambda,$
    |  | (5) |'
- en: where $\lambda$ is a scene-dependent parameter assigned by datasets. $\lambda$
    is set to a larger value for larger scenes. The definition of distance is the
    same as Chamfer distance. Precision/Accuracy is the number of points in the predicted
    point cloud satisfying the requirement over the total number of points in the
    predicted point cloud. Note that in some datasets, precision/accuracy is not measured
    by proportion (percentage) but by mean or median absolute distance instead.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是由数据集分配的场景相关参数。对于较大的场景，$\lambda$ 设置为较大的值。距离的定义与Chamfer距离相同。精度/准确率是满足要求的预测点云中的点数占预测点云总点数的比例。请注意，在某些数据集中，精度/准确率不是通过比例（百分比）来衡量，而是通过均值或中位绝对距离来衡量。
- en: Recall/Completeness
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 召回率/完整性
- en: Recall/Completeness measures what percentage of ground truth points can get
    matched in the predicted point cloud. The computation is simply swapping the ground
    truth and the prediction. For a point $\mathbf{P}_{g}$ in the ground truth point
    cloud, it is considered to have a good match in the predicted point cloud $\{\mathbf{P}_{p}\}$
    if
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率/完整性衡量真实点云中有多少百分比的点能够匹配到预测点云中。计算时只需交换真实点云和预测点云。对于真实点云中的一个点 $\mathbf{P}_{g}$，如果它在预测点云
    $\{\mathbf{P}_{p}\}$ 中有一个良好的匹配，则满足
- en: '|  | $\&#124;\mathbf{P}_{g}-\mathop{\arg\min}\limits_{\mathbf{P}\in\{\mathbf{P}_{p}\}}\&#124;\mathbf{P}-\mathbf{P}_{g}\&#124;_{2}\&#124;_{2}\leq\lambda,$
    |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\&#124;\mathbf{P}_{g}-\mathop{\arg\min}\limits_{\mathbf{P}\in\{\mathbf{P}_{p}\}}\&#124;\mathbf{P}-\mathbf{P}_{g}\&#124;_{2}\&#124;_{2}\leq\lambda,$
    |  | (6) |'
- en: and recall/completeness is the number of points in the ground truth point cloud
    satisfying the requirement over the total number of points in the ground truth
    point cloud. Similar to precision/accuracy, recall/completeness is sometimes measured
    by mean or median absolute distance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率/完整性是满足要求的真实点云中的点数占真实点云总点数的比例。与精度/准确率类似，召回率/完整性有时通过均值或中位绝对距离来衡量。
- en: F-Score
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: F-分数
- en: The two aforementioned metrics measure the accuracy and completeness of predicted
    point clouds. However, each of these metrics alone cannot present the overall
    performance since different MVS methods use different a prior assumptions. A stronger
    assumption usually leads to higher accuracy but lower completeness. Both measures
    are needed for a fair comparison. If only precision/accuracy is reported, it would
    favor MVS algorithms that only include estimated points of high certainty. On
    the other hand, if only recall/completeness is reported it would favor MVS algorithms
    that include everything, regardless of point quality. Therefore, an integrated
    metric is introduced. F-score is the harmonic mean of precision and recall. Harmonic
    mean is sensitive to extremely small values and tends to get more affected by
    smaller values, which is to say, F-score does not encourage imbalanced results.
    However, in most cases, F-score still suffers from unfairness due to limitations
    of ground truth. Since the representation of point clouds is unstructured and
    overall sparse, this problem remains unsolved.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Loss Function
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Loss functions for learning-based MVS can be categorized into regression-like
    and classification-like ones. To briefly recap, for a cost volume of $H\times
    W\times D\times F$, a probability volume $H\times W\times D$ is generated after
    cost volume regularization. Different loss functions correspond to different ways
    of determining the final prediction.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: If the final ground truth prediction is determined by an argmax operation. It
    has already been turned into a pure classification task, cross entropy loss is
    naturally suitable to be the loss function, where the ground truth depth maps
    are also discretized in the same way of plane sweep and one-hot encoded. The cross
    entropy loss function is stated as
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L=\sum_{d}^{D}-G(d)\log[P(d)],$ |  | (7) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: where $G(d)$ is the ground truth one-hot distribution w.r.t. depth and $P(d)$
    is the predicted distribution. An important advantage of classification-like loss
    functions are actually insensitive to depth division, which means the division
    can be arbitrary and not necessarily to be uniform. [[27](#bib.bib27), [24](#bib.bib24)]
    use cross entropy as their loss functions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Some methods adopt a regression-like pattern to determine the prediction that
    the mathematical expectation of depth is calculated instead. In this case, a L1
    loss is adopted as the loss function. This practice helps to predict smoother
    depth maps. MVSNet [[26](#bib.bib26)], along with later coarse-to-fine methods,
    adopts this pattern. The loss is stated as
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L=\&#124;d_{0}-Ex[P(d)]\&#124;_{1}$ |  | (8) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: where $d_{0}$ denotes the ground truth depth map and $Ex[\cdot]$ denotes expectation
    of a distribution. However, mathematical expectation is valid if and only if the
    division of space is uniform. To boost the scalability, R-MVSNet [[27](#bib.bib27)]
    adopts an inverse depth sampling strategy where the level of depth planes and
    actual depth value are in inverse proportion. In this case, plane sweep is finer
    at distant areas but the regression-like loss function is no longer valid.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{0}$ 表示真实深度图，$Ex[\cdot]$ 表示分布的期望。然而，数学期望仅在空间划分均匀的情况下才有效。为了提升可扩展性，R-MVSNet [[27](#bib.bib27)]
    采用了一个逆深度采样策略，其中深度平面级别与实际深度值成反比。在这种情况下，平面扫描在远离区域较精细，但类似回归的损失函数不再有效。
- en: Empirical results show that classification-like loss functions help to predict
    accurate depth values since all candidates lower than maximum probability are
    compressed. However, it usually leads to incontinuity of depth values. While regression-like
    ones consider mathematical expectation is a differentiable way to do argmax, which
    helps to predict smooth depth maps but loses sharpness on edges.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实证结果表明，类似分类的损失函数有助于预测准确的深度值，因为所有低于最大概率的候选值都会被压缩。然而，这通常会导致深度值的不连续性。而类似回归的方法考虑到数学期望是做argmax的可微分方式，这有助于预测平滑的深度图，但会丧失边缘的锐利度。
- en: 3 Method
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'This section introduces learning-based MVS networks that yield depth maps for
    further post-processing. A typical MVS network mainly contains three parts, namely
    a feature extraction network (Sec. [3.1](#S3.SS1 "3.1 Feature Extraction ‣ 3 Method
    ‣ Deep Learning for Multi-view Stereo via Plane Sweep: A Survey")), a cost volume
    constructor (Sec. [3.2](#S3.SS2 "3.2 Cost Volume Construction ‣ 3 Method ‣ Deep
    Learning for Multi-view Stereo via Plane Sweep: A Survey")) and a cost volume
    regularization network (Sec. [3.3](#S3.SS3 "3.3 Cost Volume Regularization ‣ 3
    Method ‣ Deep Learning for Multi-view Stereo via Plane Sweep: A Survey")). Tab. [2](#S3.T2
    "Table 2 ‣ 3 Method ‣ Deep Learning for Multi-view Stereo via Plane Sweep: A Survey")
    is an overview of typical learning-based MVS methods.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了基于学习的MVS网络，这些网络生成深度图用于进一步的后处理。一个典型的MVS网络主要包括三个部分，即特征提取网络（第 [3.1](#S3.SS1
    "3.1 特征提取 ‣ 3 方法 ‣ 通过平面扫描的深度学习多视图立体：概述") 节），成本体积构造器（第 [3.2](#S3.SS2 "3.2 成本体积构造
    ‣ 3 方法 ‣ 通过平面扫描的深度学习多视图立体：概述") 节）和成本体积正则化网络（第 [3.3](#S3.SS3 "3.3 成本体积正则化 ‣ 3 方法
    ‣ 通过平面扫描的深度学习多视图立体：概述") 节）。表 [2](#S3.T2 "表2 ‣ 3 方法 ‣ 通过平面扫描的深度学习多视图立体：概述") 是基于学习的典型MVS方法的概述。
- en: 'Table 2: An overview of typical learning-based MVS methods via plane sweep
    algorithm.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：基于学习的典型MVS方法概述（通过平面扫描算法）。
- en: '| Method | Regularization Scheme¹ | Visibility | Loss Function |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 正则化方案¹ | 可见性 | 损失函数 |'
- en: '| 3D CNN | RNN | Coarse to Fine | Classification | Regression |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 3D CNN | RNN | 粗到精 | 分类 | 回归 |'
- en: '| MVSNet [[26](#bib.bib26)] | ✓ |  |  |  |  | ✓ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| MVSNet [[26](#bib.bib26)] | ✓ |  |  |  |  | ✓ |'
- en: '| R-MVSNet [[27](#bib.bib27)] |  | ✓ |  |  | ✓ |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| R-MVSNet [[27](#bib.bib27)] |  | ✓ |  |  | ✓ |  |'
- en: '| CasMVSNet [[8](#bib.bib8)] |  |  | ✓ |  |  | ✓ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| CasMVSNet [[8](#bib.bib8)] |  |  | ✓ |  |  | ✓ |'
- en: '| CVP-MVSNet [[25](#bib.bib25)] |  |  | ✓ |  |  | ✓ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| CVP-MVSNet [[25](#bib.bib25)] |  |  | ✓ |  |  | ✓ |'
- en: '| UCS-Net [[3](#bib.bib3)] |  |  | ✓ |  |  | ✓ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| UCS-Net [[3](#bib.bib3)] |  |  | ✓ |  |  | ✓ |'
- en: '| Vis-MVSNet [[31](#bib.bib31)] |  |  | ✓ | ✓ | ✓ |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Vis-MVSNet [[31](#bib.bib31)] |  |  | ✓ | ✓ | ✓ |  |'
- en: '| PVA-MVSNet [[29](#bib.bib29)] | ✓ |  |  | ✓ |  | ✓ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| PVA-MVSNet [[29](#bib.bib29)] | ✓ |  |  | ✓ |  | ✓ |'
- en: '| $D^{2}$HC-RMVSNet [[24](#bib.bib24)] |  | ✓ |  |  | ✓ |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| $D^{2}$HC-RMVSNet [[24](#bib.bib24)] |  | ✓ |  |  | ✓ |  |'
- en: '| AA-RMVSNet [[23](#bib.bib23)] |  | ✓ |  | ✓ | ✓ |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| AA-RMVSNet [[23](#bib.bib23)] |  | ✓ |  | ✓ | ✓ |  |'
- en: '1'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: Strictly speaking, all coarse-to-fine methods are all based on 3D CNNs. Therefore,
    the definition of using 3D CNN excludes those with a coarse-to-fine pattern.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 严格来说，所有的粗到精方法都基于3D CNN。因此，使用3D CNN的定义排除了那些具有粗到精模式的方法。
- en: 3.1 Feature Extraction
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 特征提取
- en: Feature extraction for MVS still remains unstudied and most methods apply a
    common CNN backbone methods to extract features, e.g., ResNet [[9](#bib.bib9)]
    and U-Net [[19](#bib.bib19)]. The main novelty of learning-basedlies at feature
    extraction, e.g., $D^{2}$HC-RMVSNet [[24](#bib.bib24)] applies multi-scale features
    with dilation to aggregate features.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: MVS的特征提取仍然未得到充分研究，大多数方法应用了通用的CNN骨干网络来提取特征，例如，ResNet [[9](#bib.bib9)] 和 U-Net [[19](#bib.bib19)]。基于学习的方法的主要新颖性在于特征提取，例如，$D^{2}$HC-RMVSNet [[24](#bib.bib24)]
    应用了多尺度特征和扩张来聚合特征。
- en: We can compare feature extraction of different computer vision tasks. For image
    classification where each image is assigned with one label, global features are
    more important since an overall perception of the whole image is required. For
    object detection, locality is more significant than global context. As for stereo
    matching, which is rather similar to MVS, the best matching should be semi-global [[10](#bib.bib10)].
    For high-frequency regions whose texture information is rich, we expect a more
    local receptive field; while those weak-textured areas should be matched in a
    wider range.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Cost Volume Construction
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Plane sweep is done to construct the cost volume, whose details have been presented
    in Sec. [2.2](#S2.SS2 "2.2 Plane Sweep ‣ 2 Background ‣ Deep Learning for Multi-view
    Stereo via Plane Sweep: A Survey"). Since cost volume construction can be pairwise,
    there occurs another procedure to aggregate all $N-1$ cost volumes into one. DPSNet [[11](#bib.bib11)]
    simply aggregates all cost volumes by addition and the underlying principle is
    that all views are considered equally. Practically speaking, occlusion is common
    in a MVS system and it usually causes invalid matching. As a result, an increasing
    number of input views will lead to even worse prediction. In this way, views that
    are closer to the reference view should be given higher priority since it is less
    likely to suffer from occlusion.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: To alleviate this problem, PVA-MVSNet [[29](#bib.bib29)] applies gated convolution [[30](#bib.bib30)]
    to adaptively aggregate cost volumes. View aggregation tends to give occluded
    areas smaller weights and the reweighting map is yielded according to the volume
    itself. This practice actually follows the fashion of self-attention. Vis-MVSNet [[31](#bib.bib31)]
    explicitly introduces a measure for cost volumes treated as visibility by examining
    the uncertainty or confidence of probability distribution.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Cost Volume Regularization
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main differences between different MVS networks lie in the way of doing
    cost volume regularization, which will be categorized and introduced in the following
    sections. Fig. [4](#S3.F4 "Figure 4 ‣ 3.3 Cost Volume Regularization ‣ 3 Method
    ‣ Deep Learning for Multi-view Stereo via Plane Sweep: A Survey") illustrates
    the three regularization schemes covered in this paper.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cff9f57037818510a75ea0a0293627ab.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustration of typical cost regularization schemes. (a) 3D CNN regularization
    simply applies a 3D CNN to aggregate spatial context of all dimensions; (b) RNN
    regularization models each depth hypothesis similarly to a time node and adopts
    a 2D CNN with shared weights to aggregate context of $H$ and $W$; (c) in coarse-to-fine
    regularization, finer cost volumes are built according to coarser prediction and
    3D CNNs are used for different stages.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 3D CNN
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 3D CNNs are a straightforward choice for cost volume regularization. Literally,
    3D CNNs consist of 3D convolution operations, whose convolution kernels are 3
    dimensional and move among all dimensions of cost volumes. MVSNet [[26](#bib.bib26)]
    adopts a 3D U-Net to regularize cost volumes. Similar to 2D U-Net [[19](#bib.bib19)],
    3D U-Net contains an encoder, which does downsampling 3D convolutions, and a decoder,
    that recovers the original feature resolution gradually. MVSNet [[26](#bib.bib26)],
    which is the first MVS method that leverages deep learning, uses a 3D U-Net for
    cost volume regularization.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 3D CNNs是成本体积正则化的直接选择。字面上，3D CNNs由3D卷积操作组成，其卷积核是三维的，并在成本体积的所有维度之间移动。MVSNet[[26](#bib.bib26)]采用3D
    U-Net来正则化成本体积。类似于2D U-Net[[19](#bib.bib19)]，3D U-Net包含一个编码器，它执行下采样3D卷积，以及一个解码器，它逐渐恢复原始特征分辨率。MVSNet[[26](#bib.bib26)]是第一个利用深度学习的MVS方法，使用3D
    U-Net进行成本体积正则化。
- en: The purpose of cost volume regularization is to aggregate features and predict
    relatively valid depth values according to aggregated features. In this way, 3D
    CNNs are a universal method for its capability of aggregating local and global
    features in all dimensions. However, CNNs are operated on regular grids and a
    underlying assumption is that the division of space is uniform. For completed
    cases, a uniform division is not good enough to predict reliable depth values.
    Besides, 3D CNNs are computationally expensive and consumes massive memory, which
    limits the value of $D$. A finer (large $D$) and suitable (not uniform) division
    is usually crucial to obtain high quality depth maps.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 成本体积正则化的目的是根据聚合特征来聚合特征并预测相对有效的深度值。这样，3D CNNs因其在所有维度上聚合局部和全局特征的能力而成为一种通用方法。然而，CNNs是在规则网格上操作的，基本假设是空间的划分是均匀的。对于完整的情况，均匀的划分不足以预测可靠的深度值。此外，3D
    CNNs计算开销大且消耗大量内存，这限制了$D$的值。通常，细化的（大$D$）和合适的（非均匀的）划分对于获得高质量深度图至关重要。
- en: 3.3.2 RNN
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 RNN
- en: A major disadvantage of using 3D CNNs for cost volume regularization is massive
    memory consumption and in order to reduce the amount of demanded memory, some
    attempts [[27](#bib.bib27), [24](#bib.bib24)] replace 3D CNNs by sequential 2D
    CNNs along the dimension of $D$. In this way, there will always be one cost slice
    being processed in the GPU memory and a RNN is used to thread all depth hypotheses,
    passing context information on dimension $D$.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用3D CNNs进行成本体积正则化的一个主要缺点是内存消耗巨大。为了减少所需的内存量，一些尝试[[27](#bib.bib27), [24](#bib.bib24)]通过沿着$D$维度使用顺序2D
    CNNs来替代3D CNNs。这样，GPU内存中将始终处理一个成本切片，同时使用RNN来线程所有深度假设，传递$D$维度上的上下文信息。
- en: A great advantage of using recurrent regularization is that it improves the
    scalability of MVS methods since the division of space can be finer so farther
    objects can be reconstructed. But correspondingly, this scheme trades running
    efficiency for space since RNN’s parallelizability is poorer than CNNs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用递归正则化的一个巨大优势是，它改善了MVS方法的可扩展性，因为空间的划分可以更精细，从而可以重建更远的物体。但相应地，这种方案在空间上交换了运行效率，因为RNN的并行性比CNNs差。
- en: 3.3.3 Coarse to Fine
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 粗到细
- en: 'Making prediction in a coarse-to-fine pattern is another solution of reducing
    massive memory consumption. Literally, the network predicts a coarse depth map
    and then finer results are yielded based on former ones. Coarse predictions are
    usually based on downsampled images of lower resolution and fine predictions are
    on higher resolution images. This practice adopts encoder-decoder architectures
    where low-res feature maps contain more low frequency components and high frequency
    is more found in high-res feature maps. The ways of making use of coarse prediction
    are different. Cas-MVSNet [[8](#bib.bib8)] regenerates cost volumes by warping
    feature maps with a smaller depth range around previous coarse prediction. Cascade
    rewarping at stage $k+1$ is based on Eq. [1](#S2.E1 "In 2.2 Plane Sweep ‣ 2 Background
    ‣ Deep Learning for Multi-view Stereo via Plane Sweep: A Survey") by setting $d=d^{(k)}+\Delta^{(k+1)}$,
    whose homography is'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '采用从粗到精的模式进行预测是减少大量内存消耗的另一种解决方案。字面上，网络先预测一个粗略的深度图，然后基于之前的结果生成更精细的结果。粗略预测通常基于较低分辨率的下采样图像，而精细预测则基于较高分辨率的图像。这种做法采用了编码器-解码器架构，其中低分辨率特征图包含更多的低频成分，高频成分则更多地存在于高分辨率特征图中。利用粗略预测的方法各不相同。Cas-MVSNet [[8](#bib.bib8)]通过对之前粗略预测的特征图进行小范围深度的扭曲来再生代价体积。阶段$k+1$的级联重扭曲基于公式[1](#S2.E1
    "In 2.2 Plane Sweep ‣ 2 Background ‣ Deep Learning for Multi-view Stereo via Plane
    Sweep: A Survey")，其中$d=d^{(k)}+\Delta^{(k+1)}$，其单应性是'
- en: '|  | $H_{i}^{(k+1)}(d^{(k)}+\Delta^{(k+1)})=(d^{(k)}+\Delta^{(k+1)})\mathbf{K}_{0}\mathbf{T}_{0}\mathbf{T}^{-1}_{i}\mathbf{K}^{-1}_{i},$
    |  | (9) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{i}^{(k+1)}(d^{(k)}+\Delta^{(k+1)})=(d^{(k)}+\Delta^{(k+1)})\mathbf{K}_{0}\mathbf{T}_{0}\mathbf{T}^{-1}_{i}\mathbf{K}^{-1}_{i},$
    |  | (9) |'
- en: where $k$ denotes the number of stages, $d^{(k)}$ is the estimated depth value
    at stage $k$ and $\Delta^{(k+1)}$ is the residual depth to be determined in the
    current stage.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$k$表示阶段的数量，$d^{(k)}$是阶段$k$的估计深度值，$\Delta^{(k+1)}$是当前阶段需要确定的残差深度。
- en: UCS-Net [[3](#bib.bib3)] obtains uncertainty from coarse prediction to aid finer
    prediction. Note that both RNN and coarse-to-fine regularization methods allow
    finer division of depth but they focus on different cases. RNN regularization
    allows larger $D$ so there can be more hypothetical depth planes; coarse-to-fine
    regularization enables an adaptive subdivision of depth interval for finer prediction,
    leading to ability of constructing delicate details.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: UCS-Net [[3](#bib.bib3)]从粗略预测中获得不确定性以辅助更精细的预测。请注意，RNN和粗到细的正则化方法都允许更精细的深度划分，但它们关注的情况不同。RNN正则化允许较大的$D$，因此可以有更多的假设深度平面；粗到细正则化使得深度间隔可以进行自适应细分，从而具备构建精细细节的能力。
- en: 4 Discussions
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: 4.1 Results & Analysis
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 结果与分析
- en: 'Tab. [3](#S4.T3 "Table 3 ‣ 4.1 Results & Analysis ‣ 4 Discussions ‣ Deep Learning
    for Multi-view Stereo via Plane Sweep: A Survey") and Tab. [4](#S4.T4 "Table 4
    ‣ 4.1 Results & Analysis ‣ 4 Discussions ‣ Deep Learning for Multi-view Stereo
    via Plane Sweep: A Survey") are quantitative results of typical learning-based
    MVS methods. Furu [[6](#bib.bib6)], Gipuma [[7](#bib.bib7)] and COLMAP [[21](#bib.bib21)]
    are non-learning methods shown for comparison.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '表[3](#S4.T3 "Table 3 ‣ 4.1 Results & Analysis ‣ 4 Discussions ‣ Deep Learning
    for Multi-view Stereo via Plane Sweep: A Survey")和表[4](#S4.T4 "Table 4 ‣ 4.1 Results
    & Analysis ‣ 4 Discussions ‣ Deep Learning for Multi-view Stereo via Plane Sweep:
    A Survey")是典型的基于学习的MVS方法的定量结果。Furu [[6](#bib.bib6)]、Gipuma [[7](#bib.bib7)]和COLMAP [[21](#bib.bib21)]是展示的非学习方法用于比较。'
- en: As for DTU dataset, it is apparent that learning-based MVS methods outperform
    traditional methods especially in terms of completeness, while Gipuma [[7](#bib.bib7)],
    a non-learning method, achieves best accuracy. A straightforward insight is that
    deep learning can adopt a data-driven statistical pattern to predict complete
    depth values, while traditional methods depend on stricter constraints so that
    these regions are omitted due to poor geometric consistency. Among learning-based
    methods, coarse-to-fine ones perform better than others. Considering DTU is an
    ideal indoor dataset with delicate details, coarse-to-fine methods are able to
    distinguish subtle depth differences.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 关于DTU数据集，显然基于学习的方法在完整性方面优于传统方法，而Gipuma [[7](#bib.bib7)]，一种非学习方法，则实现了最佳准确度。一个直观的见解是，深度学习可以采用数据驱动的统计模式来预测完整的深度值，而传统方法依赖于更严格的约束，因此由于几何一致性差而忽略了这些区域。在基于学习的方法中，从粗到精的方法表现优于其他方法。考虑到DTU是一个具有精细细节的理想室内数据集，从粗到精的方法能够区分微妙的深度差异。
- en: While for Tanks and Temples, whose scale is larger, methods using RNN regularization
    scheme demonstrate their dominance, e.g., $D^{2}$HC-RMVSNet [[24](#bib.bib24)]
    and AA-RMVSNet [[23](#bib.bib23)]. For real-world scenes, whose lighting and scale
    can be varying, it is more important to reconstruct relatively complete point
    clouds rather than accurate ones. Therefore, RNN regularization, which allows
    a larger number of $D$ and inverse depth sampling, is better at predicting complete
    scenes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Tanks 和 Temples 这类规模较大的数据集，采用 RNN 正则化方案的方法表现出其优势，例如 $D^{2}$HC-RMVSNet [[24](#bib.bib24)]
    和 AA-RMVSNet [[23](#bib.bib23)]。对于真实场景，其光照和规模可能有所不同，因此重建相对完整的点云比准确度更为重要。因此，RNN
    正则化方法可以进行更多的 $D$ 和逆深度采样，能够更好地预测完整的场景。
- en: This performance gap between DTU and Tanks and Temples can be explained from
    the perspective of regularization. 3D CNN (including coarse-to-fine regularization
    pattern) is a rather stronger regularization since it aggregates spatial context
    information in all dimensions, while in RNNs, the constraints in the dimension
    of $D$ are relaxed. The relaxation leads to better generalizability and robustness
    but takes more time for prediction.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: DTU 和 Tanks 与 Temples 之间的性能差异可以从正则化的角度解释。3D CNN（包括粗到细的正则化模式）是一种较强的正则化，因为它在所有维度中聚合空间上下文信息，而
    RNN 中的 $D$ 维度的约束则被放宽。这种放宽导致了更好的泛化性和鲁棒性，但预测所需的时间更长。
- en: 'Table 3: Quantitative results on DTU evaluation set [[1](#bib.bib1)] (lower
    is better). Acc. stand for accuracy and Comp. stand for completeness. Note that
    DTU dataset measures absolute Chamfer distance between point clouds instead of
    percentage for evaluation. Furu [[6](#bib.bib6)], Gipuma [[7](#bib.bib7)] and
    COLMAP [[21](#bib.bib21)] are non-learning methods listed for comparison.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：DTU评估集上的定量结果 [[1](#bib.bib1)]（值越低越好）。Acc. 代表准确度，Comp. 代表完整度。请注意，DTU 数据集使用绝对
    Chamfer 距离而非百分比来进行评估。Furu [[6](#bib.bib6)]、Gipuma [[7](#bib.bib7)] 和 COLMAP [[21](#bib.bib21)]
    是列出的非学习方法供对比。
- en: '| Method | Acc.(mm) | Comp.(mm) | Overall(mm) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 准确度（mm） | 完整度（mm） | 总体（mm） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Furu [[6](#bib.bib6)] | 0.613 | 0.941 | 0.777 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Furu [[6](#bib.bib6)] | 0.613 | 0.941 | 0.777 |'
- en: '| Gipuma [[7](#bib.bib7)] | 0.283 | 0.873 | 0.578 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Gipuma [[7](#bib.bib7)] | 0.283 | 0.873 | 0.578 |'
- en: '| COLMAP [[21](#bib.bib21)] | 0.400 | 0.664 | 0.532 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| COLMAP [[21](#bib.bib21)] | 0.400 | 0.664 | 0.532 |'
- en: '| MVSNet [[26](#bib.bib26)] | 0.396 | 0.527 | 0.462 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| MVSNet [[26](#bib.bib26)] | 0.396 | 0.527 | 0.462 |'
- en: '| R-MVSNet [[27](#bib.bib27)] | 0.385 | 0.459 | 0.422 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| R-MVSNet [[27](#bib.bib27)] | 0.385 | 0.459 | 0.422 |'
- en: '| $D^{2}$HC-RMVSNet [[24](#bib.bib24)] | 0.395 | 0.378 | 0.386 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| $D^{2}$HC-RMVSNet [[24](#bib.bib24)] | 0.395 | 0.378 | 0.386 |'
- en: '| Vis-MVSNet [[31](#bib.bib31)] | 0.369 | 0.361 | 0.365 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Vis-MVSNet [[31](#bib.bib31)] | 0.369 | 0.361 | 0.365 |'
- en: '| CasMVSNet [[8](#bib.bib8)] | 0.325 | 0.385 | 0.355 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| CasMVSNet [[8](#bib.bib8)] | 0.325 | 0.385 | 0.355 |'
- en: '| CVP-MVSNet [[25](#bib.bib25)] | 0.296 | 0.406 | 0.351 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| CVP-MVSNet [[25](#bib.bib25)] | 0.296 | 0.406 | 0.351 |'
- en: '| UCS-Net [[3](#bib.bib3)] | 0.338 | 0.349 | 0.344 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| UCS-Net [[3](#bib.bib3)] | 0.338 | 0.349 | 0.344 |'
- en: '| AA-RMVSNet [[23](#bib.bib23)] | 0.376 | 0.339 | 0.357 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| AA-RMVSNet [[23](#bib.bib23)] | 0.376 | 0.339 | 0.357 |'
- en: 'Table 4: Quantitative results of typical learning-based MVS methods on the
    Tanks and Temples benchmark [[14](#bib.bib14)]. The evaluation metric is F-score
    (higher is better). L.H. stands for Lighthouse and P.G. stands for Playground.
    COLMAP [[21](#bib.bib21)] is a non-learning baseline for comparison.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：Tanks 和 Temples 基准测试 [[14](#bib.bib14)] 上典型学习型 MVS 方法的定量结果。评价指标为 F-score（值越高越好）。L.H.
    代表 Lighthouse，P.G. 代表 Playground。COLMAP [[21](#bib.bib21)] 是非学习基线方法用于对比。
- en: '| Method | Mean | Family | Francis | Horse | L.H. | M60 | Panther | P.G. |
    Train |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 平均 | 家庭 | Francis | Horse | L.H. | M60 | Panther | P.G. | 火车 |'
- en: '| COLMAP [[21](#bib.bib21)] | 42.14 | 50.41 | 22.25 | 25.63 | 56.43 | 44.83
    | 46.97 | 48.53 | 42.04 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| COLMAP [[21](#bib.bib21)] | 42.14 | 50.41 | 22.25 | 25.63 | 56.43 | 44.83
    | 46.97 | 48.53 | 42.04 |'
- en: '| MVSNet [[26](#bib.bib26)] | 43.48 | 55.99 | 28.55 | 25.07 | 50.79 | 53.96
    | 50.86 | 47.90 | 34.69 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| MVSNet [[26](#bib.bib26)] | 43.48 | 55.99 | 28.55 | 25.07 | 50.79 | 53.96
    | 50.86 | 47.90 | 34.69 |'
- en: '| R-MVSNet [[27](#bib.bib27)] | 50.55 | 73.01 | 54.46 | 43.42 | 43.88 | 46.80
    | 46.69 | 50.87 | 45.25 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| R-MVSNet [[27](#bib.bib27)] | 50.55 | 73.01 | 54.46 | 43.42 | 43.88 | 46.80
    | 46.69 | 50.87 | 45.25 |'
- en: '| PVA-MVSNet [[29](#bib.bib29)] | 54.46 | 69.36 | 46.80 | 46.01 | 55.74 | 57.23
    | 54.75 | 56.70 | 49.06 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| PVA-MVSNet [[29](#bib.bib29)] | 54.46 | 69.36 | 46.80 | 46.01 | 55.74 | 57.23
    | 54.75 | 56.70 | 49.06 |'
- en: '| CVP-MVSNet [[25](#bib.bib25)] | 54.03 | 76.50 | 47.74 | 36.34 | 55.12 | 57.28
    | 54.28 | 57.43 | 47.54 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| CVP-MVSNet [[25](#bib.bib25)] | 54.03 | 76.50 | 47.74 | 36.34 | 55.12 | 57.28
    | 54.28 | 57.43 | 47.54 |'
- en: '| CasMVSNet [[8](#bib.bib8)] | 56.84 | 76.37 | 58.45 | 46.26 | 55.81 | 56.11
    | 54.06 | 58.18 | 49.51 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| CasMVSNet [[8](#bib.bib8)] | 56.84 | 76.37 | 58.45 | 46.26 | 55.81 | 56.11
    | 54.06 | 58.18 | 49.51 |'
- en: '| UCS-Net [[3](#bib.bib3)] | 54.83 | 76.09 | 53.16 | 43.03 | 54.00 | 55.60
    | 51.49 | 57.38 | 47.89 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| UCS-Net [[3](#bib.bib3)] | 54.83 | 76.09 | 53.16 | 43.03 | 54.00 | 55.60
    | 51.49 | 57.38 | 47.89 |'
- en: '| $D^{2}$HC-RMVSNet [[24](#bib.bib24)] | 59.20 | 74.69 | 56.04 | 49.42 | 60.08
    | 59.81 | 59.61 | 60.04 | 53.92 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| $D^{2}$HC-RMVSNet [[24](#bib.bib24)] | 59.20 | 74.69 | 56.04 | 49.42 | 60.08
    | 59.81 | 59.61 | 60.04 | 53.92 |'
- en: '| Vis-MVSNet [[31](#bib.bib31)] | 60.03 | 77.40 | 60.23 | 47.07 | 63.44 | 62.21
    | 57.28 | 60.54 | 52.07 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Vis-MVSNet [[31](#bib.bib31)] | 60.03 | 77.40 | 60.23 | 47.07 | 63.44 | 62.21
    | 57.28 | 60.54 | 52.07 |'
- en: '| AA-RMVSNet [[23](#bib.bib23)] | 61.51 | 77.77 | 59.53 | 51.53 | 64.02 | 64.05
    | 59.47 | 60.85 | 54.90 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| AA-RMVSNet [[23](#bib.bib23)] | 61.51 | 77.77 | 59.53 | 51.53 | 64.02 | 64.05
    | 59.47 | 60.85 | 54.90 |'
- en: 4.2 Topics Remaining Unstudied
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 仍未研究的话题
- en: So far, learning-based MVS is still a niche field of computer vision and many
    well-known and widely used commercial softwares still apply traditional non-learning
    algorithms. A very typical problem for learning-based MVS algorithms is lack of
    valid depth values at low-textured regions and that is the reason why SOTA methods
    of ETH3D are still non-learning ones. Some attempts use different a priori information,
    such as surface normal [[15](#bib.bib15), [16](#bib.bib16)] to overcome planar
    areas. But these attempts highly rely on post-process and are still far from end-to-end.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，基于学习的 MVS 仍然是计算机视觉的一个小众领域，许多知名且广泛使用的商业软件仍然应用传统的非学习算法。基于学习的 MVS 算法的一个典型问题是在低纹理区域缺乏有效的深度值，这也是
    ETH3D 的 SOTA 方法仍然是非学习方法的原因。一些尝试使用不同的先验信息，例如表面法线 [[15](#bib.bib15), [16](#bib.bib16)]
    来克服平面区域。但这些尝试高度依赖后处理，仍然远未达到端到端。
- en: 'One unstudied topic is what kind of feature extractors are suitable for MVS,
    as have been mentioned in Sec. [3.1](#S3.SS1 "3.1 Feature Extraction ‣ 3 Method
    ‣ Deep Learning for Multi-view Stereo via Plane Sweep: A Survey"). MVS relies
    on a relatively tricky size of receptive fields.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '一个未研究的话题是适合 MVS 的特征提取器种类，如第 [3.1](#S3.SS1 "3.1 Feature Extraction ‣ 3 Method
    ‣ Deep Learning for Multi-view Stereo via Plane Sweep: A Survey")节中提到的。MVS 依赖于相对复杂的感受野尺寸。'
- en: Evaluation metrics are not reasonable. Since MVS serves as a step of image-based
    3D reconstruction, whose final purpose is to construct a mesh model and point
    clouds are intermediate representations. If a good surface reconstruction algorithm
    could properly estimate faces, then the lack of points during MVS reconstruction
    is acceptable.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标并不合理。由于 MVS 是基于图像的 3D 重建的一个步骤，其最终目的是构建网格模型，而点云是中间表示。如果一个好的表面重建算法能够正确估计面，那么在
    MVS 重建过程中缺乏点是可以接受的。
- en: Some researchers have noticed the gap between depth maps and point clouds and
    want to construct a unified framework for training and evaluation. For example,
    merging depth fusion into end-to-end training so the loss is directly computed
    from point clouds. A major problem preventing doing so is how to turn geometric
    consistency checking differentiable. Some attempts are done, such as Point-MVSNet [[2](#bib.bib2)],
    but the results are not satisfying enough.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员已经注意到深度图与点云之间的差距，并希望构建一个统一的训练和评估框架。例如，将深度融合合并到端到端的训练中，以便直接从点云中计算损失。一个主要的问题是如何使几何一致性检查具有可微分性。一些尝试已经进行，例如
    Point-MVSNet [[2](#bib.bib2)]，但结果仍不够令人满意。
- en: Another task is to encode multimodal information into MVS networks, e.g., semantics.
    In this way, some challenging areas can be dealt individually, such as omitting
    points with the semantic label of sky and interpolating points within the region
    labelled as ground. A suitable dataset is also required.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个任务是将多模态信息编码到 MVS 网络中，例如语义信息。这样，一些挑战性的领域可以单独处理，例如省略带有天空语义标签的点，并在标记为地面的区域内插值点。还需要一个合适的数据集。
- en: Last but not least, the number of available datasets and the diversity of data
    are also quite limited.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，可用的数据集数量和数据的多样性也非常有限。
- en: 5 Conclusion
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, several aspects of learning-based MVS algorithms are covered,
    including post-processes, plane sweep, relevant datasets and network modules.
    Besides, comparative results and observations are presented. Generally speaking,
    deep-learning-based MVS is still under development and the community is rather
    tiny compared to other computer vision tasks.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了学习基础的MVS算法的几个方面，包括后处理、平面扫描、相关数据集和网络模块。此外，还展示了比较结果和观察。总体而言，基于深度学习的MVS仍在发展中，与其他计算机视觉任务相比，社区相对较小。
- en: References
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Henrik Aanæs, Rasmus Ramsbøl Jensen, George Vogiatzis, Engin Tola, and
    Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. International
    Journal of Computer Vision, 120(2):153–168, 2016.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 亨里克·安纳斯、拉斯穆斯·拉姆斯博尔·詹森、乔治·沃吉亚齐斯、恩金·托拉 和 安德斯·比约霍尔姆·达尔。大规模数据用于多视图立体视觉。国际计算机视觉期刊，120(2):153–168，2016年。'
- en: '[2] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo
    network. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
    pages 1538–1547, 2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 陈睿、韩松芳、徐晶 和 苏浩。基于点的多视图立体网络。在IEEE/CVF国际计算机视觉会议论文集中，页码1538–1547，2019年。'
- en: '[3] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi,
    and Hao Su. Deep stereo using adaptive thin volume representation with uncertainty
    awareness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pages 2524–2534, 2020.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 程硕、徐泽翔、朱世林、李卓文、李尔然、拉维·拉马穆尔西 和 苏浩。使用适应性薄体积表示和不确定性感知的深度立体。在IEEE/CVF计算机视觉与模式识别会议论文集中，页码2524–2534，2020年。'
- en: '[4] Robert T Collins. A space-sweep approach to true multi-image matching.
    In Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern
    Recognition, pages 358–363\. IEEE, 1996.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] 罗伯特·T·柯林斯。空间扫描法用于真实的多图像匹配。在CVPR IEEE计算机视觉与模式识别会议论文集中，页码358–363。IEEE，1996年。'
- en: '[5] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
    Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
    Learning optical flow with convolutional networks. In Proceedings of the IEEE
    international conference on computer vision, pages 2758–2766, 2015.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 阿列克谢·多索维茨基、菲利普·费舍尔、埃迪·伊尔格、菲利普·豪瑟、卡内尔·哈兹尔巴斯、弗拉基米尔·戈尔科夫、帕特里克·范德·斯马赫特、丹尼尔·克雷默斯
    和 托马斯·布罗克斯。Flownet：使用卷积网络学习光流。在IEEE国际计算机视觉会议论文集中，页码2758–2766，2015年。'
- en: '[6] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview
    stereopsis. IEEE transactions on pattern analysis and machine intelligence, 32(8):1362–1376,
    2009.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 古川康高 和 让·蓬斯。准确、密集且稳健的多视图立体视觉。IEEE模式分析与机器智能期刊，32(8):1362–1376, 2009年。'
- en: '[7] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel
    multiview stereopsis by surface normal diffusion. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 873–881, 2015.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 西尔瓦诺·加利亚尼、卡特琳·拉辛格 和 康拉德·辛德勒。通过表面法线扩散的高度并行多视图立体视觉。在IEEE国际计算机视觉会议论文集中，页码873–881，2015年。'
- en: '[8] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan.
    Cascade cost volume for high-resolution multi-view stereo and stereo matching.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 2495–2504, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 顾晓东、范志文、朱思宇、戴左卓、谭飞通 和 谭平。用于高分辨率多视图立体视觉和立体匹配的级联成本体积。在IEEE/CVF计算机视觉与模式识别会议论文集中，页码2495–2504，2020年。'
- en: '[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 770–778, 2016.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 贺凯明、张翔宇、任少卿 和 孙剑。用于图像识别的深度残差学习。在IEEE计算机视觉与模式识别会议论文集中，页码770–778，2016年。'
- en: '[10] Heiko Hirschmuller. Stereo processing by semiglobal matching and mutual
    information. IEEE Transactions on pattern analysis and machine intelligence, 30(2):328–341,
    2007.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 海科·赫希穆勒。通过半全局匹配和互信息进行立体处理。IEEE模式分析与机器智能期刊，30(2):328–341，2007年。'
- en: '[11] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. Dpsnet: End-to-end
    deep plane sweep stereo. In International Conference on Learning Representations,
    2018.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 任胜勋、全海坤、林思敏 和 崔恩洙。Dpsnet：端到端深度平面扫描立体。国际学习表征会议，2018年。'
- en: '[12] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction.
    ACM Transactions on Graphics (ToG), 32(3):1–13, 2013.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 迈克尔·卡兹丹 和 于格斯·霍普。筛选泊松表面重建。ACM图形学交易（ToG），32(3):1–13，2013年。'
- en: '[13] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy,
    Abraham Bachrach, and Adam Bry. End-to-end learning of geometry and context for
    deep stereo regression. In Proceedings of the IEEE International Conference on
    Computer Vision, pages 66–75, 2017.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and
    temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics
    (ToG), 36(4):1–13, 2017.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Uday Kusupati, Shuo Cheng, Rui Chen, and Hao Su. Normal assisted stereo
    depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 2189–2199, 2020.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Hongmin Liu, Xincheng Tang, and Shuhan Shen. Depth-map completion for
    large indoor scene reconstruction. Pattern Recognition, 99:107112, 2020.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers,
    Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks
    for disparity, optical flow, and scene flow estimation. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 4040–4048, 2016.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Masatoshi Okutomi and Takeo Kanade. A multiple-baseline stereo. IEEE Transactions
    on pattern analysis and machine intelligence, 15(4):353–363, 1993.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional
    networks for biomedical image segmentation. In International Conference on Medical
    image computing and computer-assisted intervention, pages 234–241\. Springer,
    2015.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Johannes L Schönberger and Jan-Michael Frahm. Structure-from-motion revisited.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 4104–4113, 2016.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Johannes L Schönberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys.
    Pixelwise view selection for unstructured multi-view stereo. In European Conference
    on Computer Vision, pages 501–518. Springer, 2016.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Thomas Schöps, Johannes L Schönberger, Silvano Galliani, Torsten Sattler,
    Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark
    with high-resolution images and multi-camera videos. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 3260–3269, 2017.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet:
    Adaptive aggregation recurrent multi-view stereo network. In IEEE International
    Conference on Computer Vision, 2021.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong
    Chen, Guoping Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-view stereo
    net with dynamic consistency checking. In European Conference on Computer Vision,
    pages 674–689. Springer, 2020.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid
    based depth inference for multi-view stereo. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 4877–4886, 2020.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth
    inference for unstructured multi-view stereo. In Proceedings of the European Conference
    on Computer Vision (ECCV), pages 767–783, 2018.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan.
    Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5525–5534,
    2019.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian
    Fang, and Long Quan. Blendedmvs: A large-scale dataset for generalized multi-view
    stereo networks. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 1790–1799, 2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Hongwei Yi, Zizhuang Wei, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping
    Wang, and Yu-Wing Tai. Pyramid multi-view stereo net with self-adaptive view aggregation.
    In European Conference on Computer Vision, pages 766–782. Springer, 2020.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang.
    Free-form image inpainting with gated convolution. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pages 4471–4480, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware
    multi-view stereo network. British Machine Vision Conference (BMVC), 2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
