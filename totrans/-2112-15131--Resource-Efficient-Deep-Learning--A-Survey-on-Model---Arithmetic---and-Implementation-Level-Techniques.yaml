- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:48:46'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2112.15131] Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2112.15131](https://ar5iv.labs.arxiv.org/html/2112.15131)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JunKyu Lee
  prefs: []
  type: TYPE_NORMAL
- en: Queen’s University Belfast
  prefs: []
  type: TYPE_NORMAL
- en: Belfast, Northern Ireland, UK
  prefs: []
  type: TYPE_NORMAL
- en: junkyu.lee@qub.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: '&Lev Mukhanov'
  prefs: []
  type: TYPE_NORMAL
- en: Queen’s University Belfast
  prefs: []
  type: TYPE_NORMAL
- en: Belfast, Northern Ireland, UK
  prefs: []
  type: TYPE_NORMAL
- en: l.mukhanov@qub.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: '&Amir Sabbagh Molahosseini'
  prefs: []
  type: TYPE_NORMAL
- en: Queen’s University Belfast
  prefs: []
  type: TYPE_NORMAL
- en: Belfast, Northern Ireland, UK
  prefs: []
  type: TYPE_NORMAL
- en: a.sabbaghmolahosseini@qub.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: '&Umar Minhas'
  prefs: []
  type: TYPE_NORMAL
- en: Queen’s University Belfast
  prefs: []
  type: TYPE_NORMAL
- en: Belfast, Northern Ireland, UK
  prefs: []
  type: TYPE_NORMAL
- en: u.minhas@qub.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: '&Yang Hua'
  prefs: []
  type: TYPE_NORMAL
- en: Queen’s University Belfast
  prefs: []
  type: TYPE_NORMAL
- en: Belfast, Northern Ireland, UK
  prefs: []
  type: TYPE_NORMAL
- en: y.hua@qub.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: '&Jesus Martinez del Rincon'
  prefs: []
  type: TYPE_NORMAL
- en: Queen’s University Belfast
  prefs: []
  type: TYPE_NORMAL
- en: Belfast, Northern Ireland, UK
  prefs: []
  type: TYPE_NORMAL
- en: j.martinez-del-rincon@qub.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: '&Kiril Dichev'
  prefs: []
  type: TYPE_NORMAL
- en: University of Cambridge
  prefs: []
  type: TYPE_NORMAL
- en: Cambridge, England, UK
  prefs: []
  type: TYPE_NORMAL
- en: kiril.dichev@gmail.com &Cheol-Ho Hong
  prefs: []
  type: TYPE_NORMAL
- en: Chung-Ang University
  prefs: []
  type: TYPE_NORMAL
- en: Seoul, South Korea
  prefs: []
  type: TYPE_NORMAL
- en: cheolhohong@cau.ac.kr
  prefs: []
  type: TYPE_NORMAL
- en: '&Hans Vandierendonck'
  prefs: []
  type: TYPE_NORMAL
- en: Queen’s University Belfast
  prefs: []
  type: TYPE_NORMAL
- en: Belfast, Northern Ireland, UK
  prefs: []
  type: TYPE_NORMAL
- en: h.vandierendonck@qub.ac.uk Corresponding Author
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning is pervasive in our daily life, including self-driving cars, virtual
    assistants, social network services, healthcare services, face recognition, etc.
    However, deep neural networks demand substantial compute resources during training
    and inference. The machine learning community has mainly focused on model-level
    optimizations such as architectural compression of deep learning models, while
    the system community has focused on implementation-level optimization. In between,
    various arithmetic-level optimization techniques have been proposed in the arithmetic
    community. This article provides a survey on resource-efficient deep learning
    techniques in terms of model-, arithmetic-, and implementation-level techniques
    and identifies the research gaps for resource-efficient deep learning techniques
    across the three different level techniques. Our survey clarifies the influence
    from higher to lower-level techniques based on our resource-efficiency metric
    definition and discusses the future trend for resource-efficient deep learning
    research.
  prefs: []
  type: TYPE_NORMAL
- en: '*Keywords* deep learning, neural networks, resource efficiency, arithmetic
    utilization'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent improvements in network and storage devices have provided the machine
    learning community with the opportunity to utilize immense data sources, leading
    to the golden age of AI and deep learning [[22](#bib.bib22)]. Since modern deep
    neural networks (DNNs) require considerable computing resources and are deployed
    in a variety of compute devices, ranging from high-end servers to mobile devices
    with limited computational resources, there is a strong need to realize economical
    DNNs that fit within the resource constraints [[118](#bib.bib118), [150](#bib.bib150),
    [151](#bib.bib151)]. Resource-efficient deep learning research has vividly been
    carried out independently in various research communities including the machine
    learning, computer arithmetic, and computing system communities. Recently, DeepMind
    proposed the resource-efficient deep learning benchmark metric which is the accuracy
    along with the required memory footprint and number of operations [[78](#bib.bib78)].
  prefs: []
  type: TYPE_NORMAL
- en: 'With this regard, this article surveys resource-efficient techniques for deep
    learning based on the three-level categorization: the model-, arithmetic-, and
    implementation-level techniques along with various resource efficiency metrics
    as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").
    Our resource efficiency metrics include the accuracy per parameter, operation,
    memory footprint, core utilization, memory access, and Joule. For the resource-efficiency
    comparison between the baseline DNN and a DNN utilizing resource-efficient techniques,
    the accuracy should be equivalent between the two DNNs. In other words, it is
    not fair to compare the resource efficiency between a DNN producing a high accuracy
    and a DNN producing a low accuracy since the resource efficiency is significantly
    higher in a low performing DNN based on our resource metrics. We categorize the
    resource-efficient techniques into the model-level resource-efficient techniques
    if they compress the DNN model sizes; the arithmetic-level resource-efficient
    techniques if they utilize reduced precision arithmetic and/or customized arithmetic
    rules; the implementation-level resource-efficient techniques if they apply hardware
    optimization techniques to the DNNs (e.g., locating local memory near to processing
    elements) to improve physical resource efficiency such as the accuracy per compute
    resource and per Joule.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient Deep Learning:
    A Survey on Model-, Arithmetic-, and Implementation-Level Techniques"), Convolutional
    Neural Networks (CNNs) can be considered as a resource-efficient technique since
    they improve the accuracy per parameter, per operation, and per memory footprint,
    compared to fully connected neural networks. The resource-efficiency from CNNs
    can be further improved by applying the model-, arithmetic-, and implementation-level
    techniques. The model- and arithmetic-level techniques can affect the accuracy
    since they affect either the DNN model structure or the arithmetic rule, while
    the implementation-level techniques generally do not affect the accuracy. The
    model-level techniques mostly contribute to improving abstract resource efficiency,
    while the implementation-level techniques contribute to improve physical resource
    efficiency. Without careful consideration at the intersection between the model-
    and the implementation-level techniques, a DNN model compressed by the model-level
    techniques might require significant runtime compute resources, incurring longer
    training time and inference latency than the original model [[108](#bib.bib108),
    [31](#bib.bib31)]. Thus, to optimize the performance and energy efficiency on
    a particular hardware, it is essential to consider the joint effect of the model,
    arithmetic and implementation-level optimizations. Our survey focuses on the three
    different level resource-efficient techniques for CNN architectures, since CNN
    is one of the most widely used deep learning architectures [[91](#bib.bib91)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f4dbfa19be843d95900334910a2bf262.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Survey on resource-efficient deep learning techniques based on resource
    efficiency metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: Related survey works are as follows. Sze et al. [[145](#bib.bib145)] provided
    a comprehensive tutorial and survey towards efficient processing of DNNs, discussing
    DNN architectures, software frameworks (e.g., PyTorch, TensorFlow, Keras, etc.),
    and the implementation methods optimizing Multiply-and-Accumulate Computations
    (MACs) of DNNs on given compute platforms. Cheng et al. [[35](#bib.bib35)] conducted
    a survey on the model compression techniques including pruning, low-rank factorization,
    compact convolution, and knowledge distillation. Deng et al. [[42](#bib.bib42)]
    discussed joint model-compression methods which combined multiple model-level
    compression techniques, and their efficient implementation on particular computing
    platforms. Wang et al. [[157](#bib.bib157)] provided a survey on custom hardware
    implementations of DNNs and evaluated their performances using the Roofline model
    of [[162](#bib.bib162)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the previous survey works, we conduct a comprehensive survey on resource-efficient
    deep learning techniques in terms of the model-, arithmetic-, and implementation-level
    techniques by clarifying which resource-efficiency can be improved with particular
    techniques according to our resource-efficiency metrics as defined in Section [2.2](#S2.SS2
    "2.2 Resource Efficiency Metrics for Deep Learning ‣ 2 Background on Deep Learning
    and Resource-Efficiency ‣ Resource-Efficient Deep Learning: A Survey on Model-,
    Arithmetic-, and Implementation-Level Techniques"). Such clarification would provide
    machine learning engineers, computer arithmetic designers, software developers,
    and hardware manufacturers with useful information to improve particular resource
    efficiency for their DNN applications. Besides, since we notice that fast wireless
    communication and edge computing development affects deep learning applications
    [[180](#bib.bib180)], our survey also includes cutting-edge resource-efficient
    techniques for distributed AI such as early exiting techniques [[150](#bib.bib150),
    [151](#bib.bib151)]. The holistic and multi-facet view for resource-efficient
    techniques for deep learning from our survey would allow for a better understanding
    of the available techniques and, as consequence, a better global optimization,
    compared to previous survey works. The main contributions of our paper include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This paper first provides a comprehensive survey coverage of the recent resource-efficient
    techniques for DNNs in terms of the model-, arithmetic-, and implementation-level
    techniques.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, our paper is the first to provide comprehensive
    survey on arithmetic-level utilization techniques for deep learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This paper utilizes multiple resource efficiency metrics to clarify which resource
    efficiency metrics each technique improves.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This paper provides the influence of resource-efficient deep learning techniques
    from higher to lower level techniques (refer to Fig. [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss the future trend for the resource-efficient deep learning techniques.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We discuss our resource efficiency metrics for deep learning in Section [2](#S2
    "2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques"),
    the model-level resource-efficient techniques in Section [3](#S3 "3 Model-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques"), the arithmetic-level
    techniques in Section [4](#S4 "4 Arithmetic-Level resource-efficient Techniques
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques"), the implementation-level techniques in Section [5](#S5 "5 Implementation-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques"), the influences between
    different-level techniques in Section [6](#S6 "6 Interrelated Influences ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques"),
    the future trend in Section [7](#S7 "7 Future Trend for Resource-Efficient Deep
    Learning ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques"), and conclusion in Section [8](#S8 "8 Conclusion
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques"). Our paper excludes higher-level training procedure manipulation
    techniques such as one-pass ImageNet [[78](#bib.bib78)], bag of freebies [[20](#bib.bib20)],
    data augmentation, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background on Deep Learning and Resource-Efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section describes deep learning overview and resource efficiency metric,
    as preparatory to the description of resource-efficient techniques via the three
    different levels.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Deep Learning Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning is defined as “learning multiple levels of representation” [[17](#bib.bib17)]
    and often utilizes DNNs to learn the multiple levels of representation. DNNs are
    trained using the training data set, and their prediction accuracy is evaluated
    using the test dataset [[5](#bib.bib5)]. In this section, we describe the perceptron
    model (i.e., artificial neuron) first and then DNNs later.
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.1 Perceptron Model:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The McCulloch and Pitts’s neuron (a.k.a. M-P neuron) [[112](#bib.bib112)],
    proposed in 1943, was a system mimicking the neuron in the nervous system, receiving
    multiple binary inputs and producing one binary output based on a threshold. Inspired
    by the work of [[112](#bib.bib112)], Rosenblatt [[128](#bib.bib128)] proposed
    the “perceptron” model consisting of multiple weights, a summation, and an activation
    function as shown in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1.1 Perceptron Model: ‣ 2.1
    Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques").(a).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8b5d86588b45eebc84a58efedce56962.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Perceptron and neural network model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Eq. ([1](#S2.E1 "In 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview ‣
    2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques"))
    describes a perceptron’s firing activity $y_{out}$ using the inputs $x_{i}$ associated
    with their weights $w_{i}$, where the $i$ represents an index to indicate one
    of multiple inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{out}=\begin{cases}1,&amp;\text{if }(\Sigma_{i=1}^{n_{in}}w_{i}\times
    x_{i}>threshold)\text{ or }(\Sigma_{i=1}^{n_{in}}w_{i}\times x_{i}+bias>0)\\ 0,&amp;\text{if
    }(\Sigma_{i=1}^{n_{in}}w_{i}\times x_{i}\leq threshold)\text{ or }(\Sigma_{i=1}^{n_{in}}w_{i}\times
    x_{i}+bias\leq 0),\end{cases}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $n_{in}$ is the number of the inputs. The function that determines the
    firing activity is referred to as the activation function, and the bias is in
    proportion to the probability of the firing activation [[116](#bib.bib116)]. Since
    single perceptron model is suitable only for linearly separable problems, a Multi-Layer
    Perceptron (MLP) model can be used for non-linearly separable problems as shown
    in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview
    ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").(b),
    where $w_{j,k,(i)}$ represents a weight linking the $j$^(th) neuron in the $(i-1)$^(th)
    layer to the $k$^(th) neuron in the $i$^(th) layer. The signal $s_{j,(i)}$ in
    Fig. [2](#S2.F2 "Figure 2 ‣ 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview
    ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")
    follows Eq. ([2](#S2.E2 "In 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview
    ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{j,(l)}=\Sigma_{i=1}^{n_{in}^{(l-1)}}(w_{i,j,(l)}\times x_{i,(l-1)})=(\mathbf{W}_{(l)}^{T}\mathbf{x}_{(l-1)})_{j},$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'and $x_{j,(l)}=\theta_{P}(s_{j,(l)})$, where $\theta_{P}(s)$ is a perceptron’s
    activation function that follows Eq. ([1](#S2.E1 "In 2.1.1 Perceptron Model: ‣
    2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques")) (i.e., step function), and $\mathbf{W}_{(l)}$ consists of the matrix
    elements, $w_{i,j,(l)}$s, for the $i^{th}$ row and the $j^{th}$ column.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.2 Deep Neural Network:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since it requires tremendous efforts for human to optimize MLPs manually, neural
    networks that adopts a soft threshold activation function $\theta_{N}$ (e.g.,
    $sigmoid$, $ReLU$, etc.) were proposed to train the weights according to the training
    data [[163](#bib.bib163), [160](#bib.bib160)]. [[116](#bib.bib116)] notice that
    neural network is sometimes interchangeably used with MLP. For clarity, we name
    an algorithm as an MLP if it utilizes a step function for its activation functions
    and as a neural network if it utilizes a soft threshold function. In Fig. [2](#S2.F2
    "Figure 2 ‣ 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview ‣ 2 Background
    on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep Learning: A
    Survey on Model-, Arithmetic-, and Implementation-Level Techniques").(b), the
    output from the $i$^(th) neuron at the $l$^(th) layer in a neural network employing
    a soft threshold activation function, $\theta_{N}(\cdot)$, can be represented
    as Eq. ([3](#S2.E3 "In 2.1.2 Deep Neural Network: ‣ 2.1 Deep Learning Overview
    ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{i,(l)}=\theta_{N}(s_{i,(l)}).$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'A neural network allows the weights and the biases to be trained using the
    backpropagation [[5](#bib.bib5)]. A neural network model is often referred to
    as a feed-forward model in that the weights always link the neurons in the current
    layer to the neurons in the very next layer. In a neural network, the middle layers
    located between the input and output layer, are often referred to as hidden layers
    (e.g., two hidden layers in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1.1 Perceptron Model:
    ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques").(b)). A neural network with multiple hidden layers is referred to
    as a DNN [[145](#bib.bib145)].'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.3 Training - Backpropagation:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the forward pass, the neuron outputs are propagated in forward direction
    based on the matrix-vector multiplications as shown in Eq. ([2](#S2.E2 "In 2.1.1
    Perceptron Model: ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning
    and Resource-Efficiency ‣ Resource-Efficient Deep Learning: A Survey on Model-,
    Arithmetic-, and Implementation-Level Techniques")). Likewise, the weights and
    the biases can be trained in backward direction using matrix-vector multiplications.
    This method is called as the backpropagation. The backpropagation method consists
    of the three steps, allowing a gradient descent algorithm to be implemented efficiently
    on computers. It finds the activation gradients, $\delta_{j,(l)}$s (i.e., the
    gradients with respect to all the signals, $s_{j,(l)}$s, in Eq ([2](#S2.E2 "In
    2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning
    and Resource-Efficiency ‣ Resource-Efficient Deep Learning: A Survey on Model-,
    Arithmetic-, and Implementation-Level Techniques"))), in step 1, finds the weight
    gradients (i.e., the gradients with respect to all the weights) using the activation
    gradients in step 2, and finally updates the weights using the weight gradients
    in step 3\. All $\delta_{j,(l-1)}$s are found in backward direction using the
    matrix-vector multiplications by replacing $\mathbf{W}_{(l)}^{T}$ to $\mathbf{W}_{(l)}$
    and $x_{j,(l-1)}$ to $\delta_{j,(l)}$ in Eq. ([3](#S2.E3 "In 2.1.2 Deep Neural
    Network: ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques")). After all activation gradients have been found, the weight gradients
    can be found. Finally, the weights are updated using the weight gradients. The
    backpropagation requires additional storage to store all the weights and activation
    values. Once a DNN is trained, the DNN is used for the inference task using the
    trained weights. Please refer to [[5](#bib.bib5)] for the further details for
    the backprogation method. After a DNN being trained, the DNN’s accuracy is evaluated
    using the validation dataset which is unseen from the training.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.1.4 Convolutional Neural Network:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since CNN is one of the most successful and widely used deep learning architectures
    [[91](#bib.bib91)], we exemplify CNN as a representative deep learning architecture.
    CNN employs multiple convolutional layers, and each convolutional layer utilizes
    multiple filters to perform convolutions independently with respect to each filter
    as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional Neural Network: ‣
    2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques"), where a filter at a convolutional layer consists of as many kernels
    as the number of the channels at the input layer (e.g., 3 kernels per filter in
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional Neural Network: ‣ 2.1 Deep Learning
    Overview ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")).
    For example, each $3\times 3$ filter has 9 weight parameters and slides from the
    top-left to the bottom-right position, generating $4$ output values with respect
    to each position (e.g., top-left, top-right, bottom-left, and bottom-right position)
    in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional Neural Network: ‣ 2.1 Deep
    Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").
    The outputs from the convolutions are often called feature maps and are fed into
    activation functions. Modern CNNs such as ResNet [[67](#bib.bib67)] often employ
    a batch normalization layer [[83](#bib.bib83)] between the convolutional layer
    and the ReLU layer to improve the accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/89f5521a9fb6c0991e23bdbc1a342752.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Convolution operations in a CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: The CNN is a resource-efficient deep learning architecture in terms of the accuracy
    per parameter and per operation by leveraging the two properties, local receptive
    field and shared weights [[93](#bib.bib93)]. For example, performing convolutions
    using multiple small kernels extracts multiple local receptive features from the
    input image during training, and each kernel contains some meaningful pattern
    from the input image after being trained [[179](#bib.bib179)]. Thus, CNN utilizes
    much fewer weights than fully connected DNN, since the kernel’s height and weight
    are generally much smaller than the height and the width at the input layer, leading
    to the improved resource efficiency. Notice that a convolutional layer becomes
    a fully connected layer if the height and the width at the input layer are matched
    with each kernel’s height and width. The number of total weights in a layer in
    a CNN is much less than used in a fully connected neural network, since the local
    receptive weights are shared over the entire feature on a layer.
  prefs: []
  type: TYPE_NORMAL
- en: Training CNN also utilizes the backprogation using the transpose of kernel matrices
    in a filter to update the weights in the filter. The mini-batch gradient descent
    algorithm is widely used to train CNNs, which utilizes part of training data to
    update the weights per iteration. The number of data used per iteration is often
    referred to as the batch size $B$ (e.g., $B=64$ or $128$). Each $epoch$ consumes
    the entire training data, consisting of $N/B$ iterations, where $N$ is the number
    of the entire training data. The mini-batch gradient descent method is a resource-efficient
    training algorithm in terms of the accuracy per operation, compared to the batch
    gradient descent method that utilizes entire training dataset per iteration (i.e.,
    the batch gradient descent method updates the weights per epoch). For parallel
    backpropagation implementation with respect to $B$ data samples in one mini-batch,
    all the weights and all the activation values using $B$ training samples should
    be stored to update the weights per the mini-batch iteration, requiring $B\times$
    additional storage, compared to the backpropagation using a stochastic gradient
    descent algorithm which updates the weights per training sample. Our paper refers
    to the term, DNN, as any neural network with several hidden layers, including
    CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Resource Efficiency Metrics for Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recently, researchers from DeepMind [[78](#bib.bib78)] proposed the metrics
    for resource-efficient deep learning benchmarks, including the top-1 accuracy,
    the required memory footprint for training, and the required number of floating
    operations for training, and evaluated the resource-efficiency for deep learning
    applications with jointly considering the three metrics. The Roofline model [[162](#bib.bib162)]
    discussed attainable performance in terms of the operational intensity defined
    as the number of floating point operations per DRAM access. Motivated by [[78](#bib.bib78),
    [162](#bib.bib162)], our resource efficiency metrics include the accuracy per
    parameter, per operation, per memory footprint, per core utilization, per memory
    access, and per Joule as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques").'
  prefs: []
  type: TYPE_NORMAL
- en: '2.2.1 Accuracy per Parameter:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We consider the accuracy per parameter (i.e., weight) for a resource-efficiency
    metric. The accuracy per parameter is an abstract resource efficiency metric since
    higher accuracy per parameter does not always imply higher physical resource efficiency
    after its implementation [[78](#bib.bib78), [1](#bib.bib1)].
  prefs: []
  type: TYPE_NORMAL
- en: '2.2.2 Accuracy per Operation:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We consider the accuracy per arithmetic operation for a resource-efficiency
    metric. This is also an abstract metric, since it can be evaluated prior to the
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '2.2.3 Accuracy per Compute Resource:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The instruction-driven architecture such as CPU or GPU requires substantial
    memory accesses due to instruction fetch and decode operations, while the data-driven
    architecture such as ASIC or FPGA can minimize the number of memory accesses,
    resulting in energy efficiency. We further categorize such compute resource into
    core utilization, memory footprint, and memory access, required to operate a DNN
    on given computing platforms. For example, the memory access can be interpreted
    as GPU DRAM access (or off-chip memory) for a GPU and as FPGA on-chip memory access
    (or off-chip memory) for a FPGA.
  prefs: []
  type: TYPE_NORMAL
- en: 'a. Accuracy per Core Utilization: The core utilization in this paper represents
    the utilization percentage of the processing cores or processing elements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'b. Accuracy per Memory Footprint: The accuracy per memory footprint is related
    to both physical and abstract resource efficiency as shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient Deep Learning: A Survey on Model-,
    Arithmetic-, and Implementation-Level Techniques"). The memory footprint is in
    proportion to the number of the parameters, but it can be varied according to
    a precision-level applied for arithmetic. For example, if a half precision arithmetic
    is applied for a deep learning, the memory footprint can be saved by $2\times$,
    compared to a single precision arithmetic deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'c. Accuracy per Memory Access: A computing kernel having a low operational
    intensity cannot approach a peak performance defined by hardware specification
    since the data supply rate from DRAM to CPU cannot catch up with the data consumption
    rate by arithmetic operations. Such kernels are called “memory bound kernels”
    in [[162](#bib.bib162)]. Other type kernels are named “compute bound kernels”
    that can approach a peak performance defined by hardware specification. Utilizing
    reduced precision arithmetic can improve the performance for both memory bound
    kernels by improving the data supply rate from DRAM to CPU and compute bound kernels
    by increasing word-level parallelism on SIMD architectures [[95](#bib.bib95)].'
  prefs: []
  type: TYPE_NORMAL
- en: '2.2.4 Accuracy per Joule:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The dynamic power consumption is the main factor to determine energy consumption
    required for computationally intensive tasks (e.g., DNN training/inference tasks).
    The dynamic power consumption, $P_{D}$, follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P_{D}=\#_{TTR}\times C_{CP}\times V^{2}_{CP}\times f_{CP},$ |  | (4)
    |'
  prefs: []
  type: TYPE_TB
- en: 'where $\#_{TTR}$ is the number of toggled transistors, $C_{CP}$ is an effective
    capacitance, $V_{CP}$ is an operational voltage, and $f_{CP}$ is an operational
    frequency for a given computing platform $CP$. Generally, the required minimum
    operational voltage is in proportion to the operational frequency. Therefore,
    adapting the frequency to the voltage scaling can save power cubically (a.k.a.
    Dynamic Voltage Frequency Scaling [[137](#bib.bib137)]). For example, minimizing
    the operations required to operate DNN during runtime contributes to minimizing
    $\#_{TTR}$, resulting in power reduction and energy saving; we discuss further
    the resource-efficient techniques leveraging this in Section [5.2.1](#S5.SS2.SSS1
    "5.2.1 Skipping Operations during Runtime: ‣ 5.2 Leveraging Sparsity of Weights
    and Activations ‣ 5 Implementation-Level Resource-Efficient Techniques ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Model-Level Resource-Efficient Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The model-level resource-efficient techniques, mostly developed from machine
    learning community, aim at reducing the DNN model size to fit the models to resource-constrained
    systems such as mobile devices, IoTs, etc. We categorize the model-level resource-efficient
    techniques as shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3 Model-Level Resource-Efficient
    Techniques ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/77bbc80aa3290d39beb1cd36421182e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Categorization for model-level resource-efficient techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Weight Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The weight quantization techniques quantize the weights with a smaller number
    of bits, improving the accuracy per memory footprint. The training procedure should
    be amended according to the weight quantization schemes.
  prefs: []
  type: TYPE_NORMAL
- en: '3.1.1 Binary Weight Quantization:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: BinaryConnect training scheme [[38](#bib.bib38)] allowed a DNN to represent
    the weights using one bit. In step 1, the weights are encoded to $\{-1,1\}$ using
    a stochastic clipping function. In step 2, the forward pass is performed using
    the encoded binary weights. In step 3, backpropagation seeks all activation gradients
    using full precision. In step 4, the weights are updated using full precision,
    and the training procedure goes back to step 1 for the training using the next
    mini-batch. This method required only one bit to represent the weights, thus improving
    the accuracy per memory footprint. In addition, the binary weight quantization
    also removed the need for multiplication arithmetic operations for MAC operations,
    improving the accuracy per operation. Moreover, if the activations are also quantized
    to the binary value, all MAC operations in DNN can be implemented only with XNOR
    gates and a counter [[39](#bib.bib39), [124](#bib.bib124)].
  prefs: []
  type: TYPE_NORMAL
- en: '3.1.2 Ternary Weight Quantization:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Li et al. [[98](#bib.bib98)] proposed ternary weight networks that utilized
    ternary weights, improving accuracy, compared to the binary weight networks. All
    the weights on each layer were quantized into three values, requiring only two
    bits to represent the quantized weights. Overall training procedure was similar
    to [[38](#bib.bib38)], but with ternary valued weights instead of the binary weights.
    The ternary weight network showed equivalent accuracy to various single precision
    networks with MNIST, CIFAR-10 and ImageNet, while the binary weight quantization
    [[38](#bib.bib38)] showed minor accuracy loss. Zhu et al. [[185](#bib.bib185)]
    scaled the ternary weights independently for each layer with a layer-wise scaling
    approach, improving the accuracy further, compared to [[98](#bib.bib98)].
  prefs: []
  type: TYPE_NORMAL
- en: '3.1.3 Mixed Quantization:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[[81](#bib.bib81)] proposed “Quantized Neural Network” that quantizes the activations
    and the weights to arbitrary lower precision format. For example, quantizing the
    weights to 1 bit and the activations to 2 bits improved the accuracy, compared
    to the binarized DNN of [[39](#bib.bib39)].'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pruning unimportant neurons, filters, and channels can save computational resources
    for deep learning applications without sacrificing accuracy, improving the accuracy
    per parameter and per operation. Coarse-grained pruning methods such as pruning
    filters or channels are not flexible to achieve a prescribed accuracy, but can
    be implemented efficiently on hardware [[104](#bib.bib104)], implying higher physical
    resource efficiency than fine-grained pruning such as pruning weights. Notice
    that such pruning methods can degrade confidence scores without careful re-training,
    even though they did not affect top-1 accuracy [[174](#bib.bib174)].
  prefs: []
  type: TYPE_NORMAL
- en: '3.2.1 Pruning Weights:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 1990, LeCun et al. proposed a weight pruning method to generate sparse DNNs
    with fewer weights without losing accuracy [[94](#bib.bib94)]. In 2015, the weight
    pruning approach was revisited [[66](#bib.bib66)], and the weights were pruned
    based on their magnitudes after training – the pruned DNNs were retrained to regain
    the lost accuracy. The pruning and re-training procedures could be performed iteratively
    to prune the weights further. This method reduced the number of weights of AlexNet
    by $9\times$ without losing accuracy. In 2016, Guo et al. [[58](#bib.bib58)] noticed
    that pruning wrong weights could not be revived, and proposed to prune and splice
    the weights per mini-batch training to minimize the risk from pruning wrong weights
    from previous mini-batch training. For example, the pruned weights were also participated
    in the weight update procedure during the backpropagation and were restored when
    they were re-considered as the important weights. In 2017, Yang et. al [[170](#bib.bib170)]
    proposed an energy-aware weight pruning method in which the energy consumption
    of a CNN was directly measured to guide the pruning process. In 2019, Frankly
    et al. [[50](#bib.bib50)] demonstrated that some of pruned models outperformed
    the original model by retraining the pruned models with replacing the survived
    weights with the initial random weights used for training the original model.
  prefs: []
  type: TYPE_NORMAL
- en: '3.2.2 Pruning Neurons:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of pruning individual weights, pruning a neuron can remove a group of
    the weights belonging to the neuron [[143](#bib.bib143), [110](#bib.bib110), [77](#bib.bib77),
    [178](#bib.bib178)]. In 2015, [[143](#bib.bib143)] pruned the redundant neurons
    having similar weight values in a trained DNN model. For example, the weights
    in a baseline neuron were compared to the weights in other neurons at the same
    layer, and the neurons having similar weights to the baseline neuron were fused
    to the baseline neuron based on a Euclidean distance metric in the weight values
    between the two neurons. In 2016, [[110](#bib.bib110)] pruned the redundant neurons
    based on the “determinantal point process” metric. Hu et. al [[77](#bib.bib77)]
    measured the average percentage of zero activations per neuron and pruned the
    neurons having a high percentage of zero activations according to a given compression
    rate. Yu et al. [[178](#bib.bib178)] pruned unimportant neurons based on the effect
    of the pruning error propagation on the final response layer (e.g., the neurons
    were pruned backward from the final layer to the first layer). The methods of
    pruning neurons improved the resource efficiency such as the accuracy per parameter
    and per operation.
  prefs: []
  type: TYPE_NORMAL
- en: '3.2.3 Pruning Filters:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pruning insignificant filters after training can improve the accuracy per parameter
    and per operation. The feature maps associated with the pruned filters and the
    next kernels associated with the pruned feature maps should be also pruned. Pruning
    filters can maintain the dense structure of DNN unlike pruning weights, implying
    that it is highly probable to improve physical resource efficiency further, compared
    to pruning weights. Li et al. [[99](#bib.bib99)] pruned unimportant filters based
    on the summation of absolute weight values in the filter. The pruned DNNs were
    retrained with the survived filters to regain the lost accuracy. Yang et al. [[171](#bib.bib171)]
    pruned filters based on a platform-aware magnitude-based metric depending on the
    resource-constrained devices. ThiNet [[107](#bib.bib107)] calculated the significance
    of the filters using the outputs of the next layer and pruned the insignificant
    filters based on this significance measurement.
  prefs: []
  type: TYPE_NORMAL
- en: '3.2.4 Pruning Channels:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike pruning filters, pruning channels removes the filters at the current
    layer and the kernels at the next layer associated with the pruned channels. The
    network slimming approach [[104](#bib.bib104)] pruned insignificant channels,
    producing compact models while keeping equivalent accuracy, compared to the models
    prior to pruning. For example, insignificant channels were identified based on
    scaling factors generated from the batch normalization of [[83](#bib.bib83)],
    and the channels associated with lower scaling factors were pruned. After the
    initial training, the channels associated with relatively low scaling factors
    were first pruned, and retraining was then performed to refine the network. He
    et al. [[69](#bib.bib69)] identified unimportant channels using LASSO regression
    from a pre-trained CNN model and pruned them. The channel pruning brought 5 $\times$
    speed-up on VGG-16 with minor accuracy loss. Lin et al. [[102](#bib.bib102)] pruned
    unimportant channels during runtime based on a decision maker trained by reinforcement
    learning. Gao et al. [[53](#bib.bib53)] proposed another dynamic channel pruning
    method that dynamically skipped the convolution operations associated with unimportant
    channels.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Compact Convolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To improve resource-efficiency such as the accuracy per operation and per parameter
    from computationally intensive convolution operations, many compact convolution
    methods were proposed.
  prefs: []
  type: TYPE_NORMAL
- en: '3.3.1 Squeezing Channel:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2016, Iandola et al. [[82](#bib.bib82)] proposed SqueezeNet in which each
    network block utilized the number of 1$\times$1 filters less than the number of
    the input channels to reduce the network width in the squeezing stage and then
    utilized multiple 1$\times$1 and 3$\times$3 kernels in the expansion stage. The
    computational complexity was significantly reduced by squeezing the width, while
    compensating the accuracy in the expansion stage. SqueezeNet reduced the number
    of parameters by $50\times$, compared to AlexNet on ImageNet without losing accuracy,
    improving accuracy per parameter. Gholami et al. [[55](#bib.bib55)] proposed SqueezeNext
    that utilized separable convolutions in the expansion stage; a $k\times k$ filter
    was divided into a $k\times 1$ and a $1\times k$ filter. Such separable convolutions
    reduced the number of parameters further, compared to SqueezeNet while maintaining
    AlexNet’s accuracy on ImageNet, improving accuracy per parameter further, compared
    to SqueezeNet.
  prefs: []
  type: TYPE_NORMAL
- en: '3.3.2 Depth-Wise Separable Convolution:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Xception [[36](#bib.bib36)] utilized the depth-wise separable convolutions,
    that replace 3D convolutions with 2D separable convolutions followed by 1D convolutions
    (i.e., point-wise convolutions) as shown in Fig. [5](#S3.F5 "Figure 5 ‣ 3.3.2
    Depth-Wise Separable Convolution: ‣ 3.3 Compact Convolution ‣ 3 Model-Level Resource-Efficient
    Techniques ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques"), to reduce computational complexity. The
    2D separable convolutions are performed separately with respect to different channels.
    Howard et al. [[76](#bib.bib76)] proposed MobileNet v1 that utilizes the depth-wise
    separable convolutions with the two hyperparameters, “width multiplier and resolution
    multiplier", to fit DNNs to resource-constrained devices by fully leveraging the
    accuracy and resource trade-off in the DNNs. MobileNet v1 showed equivalent accuracy
    to GoogleNet and VGG16 on ImageNet dataset with less computational complexity,
    improving the accuracy per parameter and per operation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4e8b524db07de6f9ad5dc76e880381df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Depth-wise convolution used in [[76](#bib.bib76)].'
  prefs: []
  type: TYPE_NORMAL
- en: '3.3.3 Linear Bottleneck Layer:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In general, the manifold of interest (i.e., the subspace formed by the set
    of activations at each layer) could be embedded in low-dimensional subspaces in
    deep learning. Inspired by this, Sandler et al. [[132](#bib.bib132)] proposed
    MobileNet v2 consisting of a series of bottleneck layer blocks. Each bottleneck
    layer block as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3.3 Linear Bottleneck Layer:
    ‣ 3.3 Compact Convolution ‣ 3 Model-Level Resource-Efficient Techniques ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")
    received lower dimensional input, expanded the input to high dimensional intermediate
    feature maps, and projected the high dimensional intermediate features onto low
    dimensional features. Keeping linearity for the output feature maps was crucial
    to avoid destroying information from non-linear activations, so linear activation
    functions were used at the end of each bottleneck block.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b901f0aa4a691d1efbaf78ea5cf00ed4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Bottleneck layer block used in [[132](#bib.bib132)].'
  prefs: []
  type: TYPE_NORMAL
- en: '3.3.4 Group Convolution:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a group convolution method, the input channels are divided into several groups,
    and the channels in each group are separately participated in convolution with
    other groups. For example, the input channels with three groups required three
    separate convolutions. Since group convolution does not communicate with the channels
    in other groups, communication between different groups is performed after the
    separate convolutions. Group convolution methods of [[181](#bib.bib181), [108](#bib.bib108),
    [80](#bib.bib80), [79](#bib.bib79)] reduced the number of MAC operations, improving
    the accuracy per operation, compared to DNNs using regular convolution. In 2012,
    AlexNet utilized group convolution to train the DNNs effectively using the two
    NVIDIA GTX580 GPUs [[91](#bib.bib91)]. Surprisingly, an AlexNet using the group
    convolution showed superior accuracy to an AlexNet using regular convolution,
    improving the accuracy per operation. In 2017, ResNext [[167](#bib.bib167)] utilized
    group convolution based on ResNet [[67](#bib.bib67)] using a cardinality parameter
    (i.e., the number of groups). In 2018, Zhang et al. [[181](#bib.bib181)] noticed
    that the point-wise convolutions were computationally intensive in practice in
    the depth-wise convolutions and proposed ShuffleNet that applied group convolution
    to every point-wise convolution to reduce compute complexity further, compared
    to MobileNet v1\. ShuffleNet shuffled the output channels from the grouped point-wise
    convolution to communicate with different grouped convolutions, demonstrating
    superior accuracy to MobileNet v1 on ImageNet and COCO datasets, given the same
    arithmetic operation cost budget. Ma et al. [[108](#bib.bib108)] proposed ShuffleNet
    v2 that improved physical resource efficiency further, compared to ShuffleNet
    [[181](#bib.bib181)] by employing equal channel width for input and output channels
    where applicable and minimizing the number of operations required for $1\times
    1$ convolutions. Rather than choosing each group randomly and shuffling them,
    Huang et al. [[80](#bib.bib80)] proposed to learn each group for a group convolution
    during training. The “learned group convolution” was applied in Densenet [[79](#bib.bib79)],
    and Densenet improved the accuracy per parameter and per operation, compared to
    ShuffleNet, given a prescribed accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '3.3.5 Octave Convolution:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Chen et al. [[34](#bib.bib34)] decomposed feature maps into a higher and a lower
    frequency part to save the feature maps’ memory footprint and reduce the computational
    cost. The decomposed feature maps were used by specific convolution called “Octave
    Convolution” that performs a convolution between the higher and lower frequency
    part. The application of the octave convolution to ResNet-152 architecture achieved
    higher accuracy using ImageNet dataset than the regular convolution, improving
    the accuracy per operation and per memory footprint.
  prefs: []
  type: TYPE_NORMAL
- en: '3.3.6 Downsampling:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Qin et al. [[122](#bib.bib122)] applied a downsampling approach (e.g., a larger
    stride size for a convolution) to MobileNet v1, improving the top-1 accuracy by
    5.5% over MobileNet v1 on the ILSVRC 2012 dataset, given a 12M arithmetic operations
    budget.
  prefs: []
  type: TYPE_NORMAL
- en: '3.3.7 Low Rank Approximation:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Denton et al. [[44](#bib.bib44)] proposed a low rank approximation that compresses
    the kernel tensors in the convolutional layers and the weight matrices in the
    fully connected layers by using singular value decomposition. Another low rank
    approximation [[89](#bib.bib89)] used Tucker decomposition to compress the feature
    maps, resulting in significant reductions in the model size, the inference latency,
    and the energy consumption. Such low rank approximation methods improve the accuracy
    per parameter, per operation, and per memory footprint.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Knowledge Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The knowledge from a large-scale high performing model (teacher network) could
    be transferred to a compact neural network (student network) to improve resource
    efficiency such as accuracy per parameter and per operation for inference tasks
    [[23](#bib.bib23), [127](#bib.bib127), [29](#bib.bib29)]. Buciluă et al. [[23](#bib.bib23)]
    utilized data with the labels generated from the teacher model (i.e., a large
    scale ensemble model) to train a compact neural network. The compact model was
    trained with the pseudo training data generated from the teacher model, demonstrating
    equivalent accuracy to the teacher model. Ba and Caruana [[14](#bib.bib14)] noticed
    that the softmax outputs often resulted in the student network ignoring the information
    of the other categorizations than the one with the highest probability, and utilized
    the values prior to the softmax layer, from the teacher network, for the training
    labels to allow the student network to learn the teacher network more efficiently.
    Hinton et al. [[72](#bib.bib72)] added a “temperature” term for the labels to
    enrich the information from the teacher network and train the student network
    more efficiently, compared to [[14](#bib.bib14)]. Romeo et. al [[127](#bib.bib127)]
    utilized both labels and intermediate representations from a wider teacher network
    to compress it to a thinner and deeper student network. The “hint layer” was chosen
    from the teacher network and the “guided layer” was chosen from the student network.
    The student network was then trained so that the intermediate representation deviation
    between the outputs from the hint layers and guided layers could be minimized.
    A thinner student network employed $10.4\times$ less weight parameters, compared
    to a wider teacher network, while improving accuracy. This technique is also known
    as “hint learning”. The hint learning was applied to both the region proposal
    and classification components for object detection applications [[29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Neural Architecture Search for Compressed Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Zoph et al. [[187](#bib.bib187)] proposed Neural Architectural Search (NAS)
    technique to seek optimal DNN models in the space of hyperparameters of network
    width, depth, and resolution. In case that compute resource budget was limited
    (e.g., mobile devices), many NAS variants exploited the trade-off between accuracy
    and latency to maximize resource efficiency given compute resource budget [[70](#bib.bib70),
    [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149), [164](#bib.bib164),
    [13](#bib.bib13)]. He et al. [[70](#bib.bib70)] proposed a NAS employing reinforcement
    learning, AutoML, that sampled the least sufficient candidate design space to
    compress the DNN models. MnasNet [[147](#bib.bib147)] utilized reinforcement learning
    with a balanced reward function between the accuracy and the latency to seek a
    compact neural network model. Wu et. al [[164](#bib.bib164)] proposed a gradient-based
    NAS that produced a DNN model with $2.4\times$ model size reduction, compared
    to a MobileNet v2 without losing accuracy on ImageNet dataset. Florian et. al
    [[133](#bib.bib133)] proposed a narrow-space NAS to generate low-resource DNNs
    satisfying strict memory budget and inference time requirement for IoT applications.
    [[13](#bib.bib13)] noticed that conventional NAS might improve abstract resource
    efficiency rather than physical resource-efficiency, and utilized the hardware
    information including the inference latency for a NAS to ensure that the candidate
    models could improve the physical resource-efficiency in practice. Efficientnet
    [[148](#bib.bib148)] utilized a NAS with compound scaling of depth, width, and
    resolution to seek optimal DNN models given fixed compute resource budgets. Another
    NAS utilizing compound scaling, Efficientdet, was proposed for object detection
    applications [[149](#bib.bib149)]. Efficientdet improved the accuracy using COCO
    dataset with $4-9\times$ model size reduction, compared to state-of-the-art object
    detectors, improving the accuracy per parameters. Recently, [[25](#bib.bib25)]
    proposed a feed-forward NAS approach that produced a customized DNN, given compute
    resource and latency constraint.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Arithmetic-Level resource-efficient Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Utilizing lower precision arithmetic reduces the memory footprint and the time
    spent transferring data across buses and interconnections [[49](#bib.bib49), [114](#bib.bib114),
    [186](#bib.bib186), [172](#bib.bib172)]. Employing least sufficient arithmetic
    precision for DNN applications can improve the accuracy per memory footprint and
    the accuracy per memory access. We categorize the arithmetic-level resource-efficient
    techniques into the two categories as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4 Arithmetic-Level
    resource-efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques"), Arithmetic-Level Techniques
    for Inference and Arithmetic-Level Techniques for Training. We discuss different
    number formats first and the deployment of such number formats on DNNs later.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f645d4e9b0cfbe8d677086ae5fe9270f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Categorization of arithmetic-level resource-efficient techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Number Formats for Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection describes various number formats for deep learning applications,
    as preparatory to explaining the arithmetic-level resource-efficient techniques.
    Fixed-Point (FiP) number format utilizes a binary fixed point between fraction
    and integer part. For example, an 8-bit FiP format, “01.100000”, represents 1.5
    (i.e., $...0\times 2^{1}+1\times 2^{0}+1\times 2^{-1}+0\times 2^{-2}...$) for
    the decimal representation, and the point between integer part and fraction part
    is fixed for arithmetic operations. Therefore, it could be implemented with simple
    circuits, but the available data range is very limited [[120](#bib.bib120)].
  prefs: []
  type: TYPE_NORMAL
- en: 'We exemplify the IEEE-754 general-purpose floating-point standard [[4](#bib.bib4)]
    to explain Floating-Point (FP) format and its arithmetic, since this standard
    is used for most commercially available CPUs and GPUs. The IEEE 754 Floating-Point
    (IFP) data format [[4](#bib.bib4)] consists of sign, exponent, and significand
    as shown in Eq. ([5](#S4.E5 "In 4.1 Number Formats for Deep Learning ‣ 4 Arithmetic-Level
    resource-efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques")). For example, a floating
    point number has a $(p+1)$-bit significand (including the hidden one), an $e$-bit
    exponent, and an $1$ sign bit. The machine epsilon $\epsilon_{mach}$ is defined
    as $2^{-(p+1)}$. The value represented by FP is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{out}=\begin{cases}\text{normal mode:}&amp;(-1)^{sign}\times(1\times
    2^{0}+d_{1}\times 2^{-1}+...+d_{p}\times 2^{-p})\times 2^{exponent-bias}\\ \text{subnormal
    mode:}&amp;(-1)^{sign}\times(d_{1}\times 2^{-1}+...+d_{p}\times 2^{-p})\times
    2^{1-bias},\end{cases}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $d_{1}$, … , $d_{p}$ represent binary digits, the ‘$1$’ associated with
    the coefficient $2^{0}$ is referred to as the hidden ‘$1$’, the $exponent$ is
    stored in offset notation, and the $bias$ is a positive constant. If the absolute
    value of exponent is zero, the floating-point value is represented by the subnormal
    mode. IEEE 754 standard requires exact rounding for addition, subtraction, multiplication,
    and division; the floating point arithmetic result should be identical to the
    one obtained from the final rounding after exact calculation. For example, based
    on the IEEE 754 rounding to nearest mode standard, floating point arithmetic should
    follow Eq. ([6](#S4.E6 "In 4.1 Number Formats for Deep Learning ‣ 4 Arithmetic-Level
    resource-efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $fl(x_{1}\odot x_{2})=(x_{1}\odot x_{2})(1+\epsilon_{r}),$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $|\epsilon_{r}|\leq\epsilon_{mach}$, $\odot$ is one of the four arithmetic
    operations, and $fl(\cdot)$ represents the result from the floating point arithmetic.
    Notice that quantization quantizes data to lower precision, while arithmetic is
    a rule applied to arithmetic operations between the two operands. For example,
    the quantization affects the values for the two operands, $x_{1}$ and $x_{2}$
    in Eq. ([6](#S4.E6 "In 4.1 Number Formats for Deep Learning ‣ 4 Arithmetic-Level
    resource-efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques")), while arithmetic
    affects the rounding error, $\epsilon_{r}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.1.1 Half, Single, and Double Precision:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The IEEE Floating-Point 32- (IFP32 or single precision) and 64-bit (IFP64 or
    double precision) versions are available on most of off-the-shelf conventional
    processors. Besides, IEEE-754 standard includes a 16-bit FP format (IFP16 or half
    precision) [[4](#bib.bib4)]. $p=52$, $e=11$, and $bias=1023$ for IFP64, $p=23$,
    $e=8$, and $bias=127$ for IFP32, and $p=10$, $e=5$, and $bias=15$ for IFP16\.
    IFP16 is currently supported in hardware on some of modern GPUs to accelerate
    DNN applications [[73](#bib.bib73), [37](#bib.bib37)].
  prefs: []
  type: TYPE_NORMAL
- en: '4.1.2 Brain Float-Point Format using 16 Bits (BFloat16):'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2018, a 16-bit Brain Floating-Point format [[176](#bib.bib176), [24](#bib.bib24)]
    was proposed that was tailored to deep learning applications. The BFloat16 consists
    of an 8-bit exponent and a 7-bit significand, supporting a wider dynamic data
    range than IFP16. BFloat16 is currently supported in hardware in the Intel Cooper
    Lake Xeon processors, the NVIDIA A100 GPUs, and the Google TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '4.1.3 DLFloat:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the race of designing specific FP formats for DNNs, [[6](#bib.bib6), [158](#bib.bib158)]
    proposed another 16-bit precision format, DLFloat, consisting of 6-bit exponent
    and 9-bit significand to provide better balance between dynamic data range and
    precision than IFP16 and BFloat16 formats for some of deep learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: '4.1.4 TensorFloat32 (TF32):'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NVIDIA proposed a 19-bit data format, TF32, consisting of 1-bit sign, 8-bit
    exponent and 10-bit significand to accelerate deep learning applications on A100
    GPUs with the same dynamic range support as IFP32 [[48](#bib.bib48)]. TF32 Tensor
    cores in an A100 truncates IFP32 operands to 19-bit TF32 format but accumulates
    them using IFP32 arithmetic for MAC operations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Arithmetic-Level Techniques for Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection discusses various resource-efficient arithmetic-level techniques
    based on the pre-trained DNNs for the inference tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.1 Lower Precision Arithmetic:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lower precision FiP arithmetic has been widely used to deploy DNNs on edge devices
    [[169](#bib.bib169)]. [[165](#bib.bib165), [159](#bib.bib159)] analyzed the effect
    of deploying various lower precision arithmetic on the DNN inference tasks in
    terms of accuracy and latency. The BitFusion method accelerated DNN inference
    tasks by employing variable bit-width FiP formats dynamically depending on the
    different layers [[138](#bib.bib138)]. Similarly, Tambe et al. [[146](#bib.bib146)]
    proposed AdaptiveFloat that adjusted dynamic ranges of FP numbers depending on
    the different layers, resulting in higher energy efficiency than FiP-based methods,
    given the same accuracy requirement.
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.2 Encoding Weights and Using Lookup Table:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[[21](#bib.bib21)] leveraged the fact that the exponent values of most of the
    weights were located within a narrow range and encoded the frequent exponent values
    of the weights with fewer bits using Huffman coding scheme, improving the accuracy
    per memory footprint for natural language processing applications. A lookup table,
    located between the memory and FP arithmetic units, is used to convert the encoded
    exponent values into FP exponent values.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.3 Applying Various Number Format Quantizations to DNNs:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Residue Number System (RNS) is a parallel and carry-limited number system
    that transforms a big natural number to several smaller residues. Therefore, RNS
    was often used to perform parallel and independent calculations on residues without
    carry-propagation. [[28](#bib.bib28)] exploited such parallelism to accelerate
    DNN computation. In a RNS-based DNN, the weights of a pre-trained model were transformed
    to RNS presentation. Recently, RNS was used to replace costly multiplication operations
    with simple logical operations such as multiplexing and shifting, accelerating
    DNN applications [[131](#bib.bib131), [105](#bib.bib105), [130](#bib.bib130)].
    The Logarithmic Number System (LNS) applies the logarithm to the absolute values
    of the real numbers [[120](#bib.bib120)]. The main advantage of LNS is in the
    capability of transforming multiplications into additions and divisions into subtractions.
    In 2018, [[156](#bib.bib156)] utilized a 5-bit logarithmic format using arbitrary
    log bases to improve the resource efficiency such as accuracy per memory footprint
    and per operations by replacing costly multiplication arithmetic operations to
    simple bit-shift operations [[156](#bib.bib156)]. The Posit number format [[61](#bib.bib61)]
    employs multiple separate exponent fields to represent dynamic range effectively.
    Recently, DNNs utilizing Posit showed higher accuracy than various FP8 formats
    using Mushroom and Iris datasets [[26](#bib.bib26), [27](#bib.bib27)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Arithmetic-Level Techniques for Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection discusses arithmetic-level resource-efficient techniques used
    for DNN training tasks. Training DNNs generally requires a higher precision arithmetic
    due to extremely small weight gradient values [[38](#bib.bib38), [185](#bib.bib185)].
    Adjusting arithmetic precision according to different training procedures such
    as forward propagation, activation gradient updates, and weight updates can accelerate
    DNN training [[59](#bib.bib59)]. Training quantized DNNs often required stochastic
    rounding schemes [[60](#bib.bib60), [166](#bib.bib166), [172](#bib.bib172), [184](#bib.bib184)].
  prefs: []
  type: TYPE_NORMAL
- en: '4.3.1 Mixed-Precision Training:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A conventional mixed precision training applied lower precision arithmetic to
    the multiplications in MACs, including both forward and backward path, and higher
    precision arithmetic to the accumulations in the MACs using the lower precision
    quantized operands [[86](#bib.bib86), [114](#bib.bib114)]. The higher precision
    outcomes from MACs were quantized to a lower precision format to be used for consequent
    operations. In the following (X + Y) formats, X represents the data format used
    for MAC operations, and Y represents the arithmetic applied for the accumulations
    in MAC operations (refer to [[59](#bib.bib59)] for the details for the lower and
    higher precision arithmetic usage.).
  prefs: []
  type: TYPE_NORMAL
- en: 'a. IFP16 + IFP32: In 2018, [[114](#bib.bib114)] noticed that the weights were
    updated using very small weight gradient values, and applied a lower precision
    arithmetic IFP16 to the multiplications and a higher precision IFP32 to the accumulations
    for the weight updates. For example, in the mixed-precision training approach
    in [[114](#bib.bib114)], IFP16 was used to store weights, activations, activation
    gradients and weight gradients, while IFP32 was used to keep the weight copies
    for their updates. Along with accumulating IFP16 operands using IFP32 arithmetic,
    the use of loss scaling allowed the mixed precision training to achieve equivalent
    accuracy to the IFP32 training while reducing the memory footprint.'
  prefs: []
  type: TYPE_NORMAL
- en: 'b. BFloat16 + IFP32: In 2018, the mixed-precision DNN training using (BFloat16
    + IFP32) was explored in [[176](#bib.bib176)]. In 2019, [[86](#bib.bib86)] studied
    the BFloat16’s feasibility for mixed precision training for various DNNs including
    AlexNet, ResNet, GAN, etc., and concluded that (BFloat16 + IFP32) scheme outperformed
    the (IFP16 + IFP32) scheme since BFloat16 could represent the same dynamic range
    of data as IFP32 while using fewer bits.'
  prefs: []
  type: TYPE_NORMAL
- en: e. FP8 + DLFloat In 2018, [[158](#bib.bib158)] proposed a mixed-precision training
    method that applies the 5eFP8 format (1 sign-bit, 5-bit exponent, and 2-bit for
    significand) to the multiplications and DLFloat to the accumulations in MAC operations.
    The mixed precision method improved resource efficiency such as accuracy per memory
    footprint and accuracy per memory access, compared to various (FP16 + IFP32) schemes
    with respect to different FP16 formats. Compared to the previous (FP16 + IFP32)
    methods, the chunk-accumulation and stochastic rounding schemes were additionally
    used to minimize the accuracy loss in [[158](#bib.bib158)], The chunk-based accumulation
    utilized 64 data per chunk instead of one long sequential accumulation to reduce
    rounding errors. Utilizing stochastic rounding scheme with limited precision format
    for deep learning was proposed earlier in [[60](#bib.bib60)]. [[144](#bib.bib144)]
    noted that (5eFP8 + DLFloat) training degraded accuracy for DNNs utilizing depth-wise
    convolutions such as MobileNets. To overcome this issue, [[144](#bib.bib144)]
    proposed to employ two different 8 bit floating-point formats each for forward
    and backward propagation to minimize the accuracy degradation for compressed DNNs.
    The mixed-precision training utilized 5eFP8 for backpropagation and another 8-bit
    floating-point format with (Sign, Exponent, significand) = (1, 4, 3), 4eFP8, for
    forward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: 'c. DLFloat only: In 2019, [[6](#bib.bib6)] employed the DLFloat for entire
    training procedure, removing the necessity of data conversions between the multiplications
    and the accumulations and found that DLFloat could provide better balance between
    dynamic range and precision than IFP16 and BFloat16 for LSTM networks [[74](#bib.bib74)]
    using Penn Treebank dataset. The DLFloat arithmetic units removed subnormal mode
    and supported the round-to-nearest up mode to minimize computational complexity.
    In [[6](#bib.bib6)], the DLFloat arithmetic showed equivalent performance to IFP32
    on ResNet-32 using CIFAR10 and ResNet-50 using ImageNet, while using a half of
    IFP32 bit width.'
  prefs: []
  type: TYPE_NORMAL
- en: 'f. INT8-based: Yang et al. [[172](#bib.bib172)] noticed that previously proposed
    mixed-precision training schemes did not quantize the data in the batch normalization
    layer, requiring high floating-point arithmetic in some parts of the data paths.
    To overcome this issue, [[172](#bib.bib172)] proposed a unified INT8-based quantization
    framework that quantizes all data paths in DNN including weights, activation,
    gradient, batch normalization, weight update, etc. into INT8-based data. However,
    this training method degraded the accuracy to some extent. In 2020, Zhu et al. [[186](#bib.bib186)]
    improved the accuracy, compared to the work of [[172](#bib.bib172)] while keeping
    unified INT8-based quantization framework. [[186](#bib.bib186)] minimized the
    deviation of the activation gradient direction between before and after quantization
    by measuring the distance during runtime based on the inner product between the
    two normalized gradient vectors generated before and after quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'g. Layer-Wise Adaptive Fixed-Point Training: In 2020, Zhang et al. [[182](#bib.bib182)]
    proposed a layer-wise adaptive quantization scheme. For example, activation gradient
    distributions at fully connected layers followed a narrower distribution, requiring
    more bit-width for the quantizations. [[182](#bib.bib182)] quantized AlexNet using
    INT8 for all the weights and activations and both INT8 (22%) and INT16 (78%) for
    the activation gradients. The quantized AlexNet achieved equivalent accuracy to
    the one using IFP32 for entire training on ImageNet dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Block Floating-Point Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Block Floating-Point (BFP) format utilizes a shared exponent for a series of
    numbers in a data block in order to reduce data-size [[161](#bib.bib161)]. Applying
    BFP to DNNs can improve the resource efficiency in terms of accuracy per memory
    footprint and per memory access. In addition, BFP utilizes less transistors for
    simpler adders and multipliers than FP adders and multipliers, resulting in improving
    accuracy per Joule. Various versions of DNN training methods using BFP were proposed
    to improve resource efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'a. Flexpoint: A DNN-optimized BFP format, Flexpoint [[90](#bib.bib90)], was
    proposed by Intel, and it was used with the Nervana neural processors. The BFP
    format used 5 bits for a shared exponent and 16 bits for the significand for the
    data in a data block. Flexpoint utilized the format of (Flex N)+M, where Flex
    N represents variable number of bits for the shared exponent according to the
    different epochs, and M represents the number of bits for the separated significand.
    For example, the number of exponent bits is adapted based on the dynamic range
    of the weight values depending on the number of iterations; the dynamic range
    of the weight values at the current iteration was predicted at the previous iteration.
    The (Flex N + 16) format produced equivalent accuracy to IFP32 in AlexNet using
    ImageNet dataset and a ResNet using CIFAR-10 dataset, resulting in significant
    resource efficiency improvement in terms of accuracy per memory footprint and
    accuracy per memory access.'
  prefs: []
  type: TYPE_NORMAL
- en: 'b. BFP + FP training: Drumond et al. [[45](#bib.bib45)] proposed a hybrid use
    of BFP and FP for DNN training that uses BFP only for MAC operations and FP for
    the other operations. Such hybrid training method brought 8.5 $\times$ potential
    throughput improvement with minor accuracy loss in WideResNet28-10 using CIFAR-100
    dataset on a Stratix V FPGA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'c. Block MiniFloat: [[49](#bib.bib49)] noticed that ordinary BFP formats were
    limited in minimizing original data loss with fewer bits and improving arithmetic
    density per memory access for deep learning applications. To address the two issues,
    Fox et al. [[49](#bib.bib49)] proposed the Block Minifloat (BM) along with customized
    hardware circuit design. The BM<e,m> format follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{out}=\begin{cases}\text{normal mode:}&amp;(-1)^{sign}\times(1\times
    2^{0}+d_{1}\times 2^{-1}+...+d_{m}\times 2^{-m})\times 2^{exponent-bias-BIAS_{SE}}\\
    \text{subnormal mode:}&amp;(-1)^{sign}\times(d_{1}\times 2^{-1}+...+d_{m}\times
    2^{-m})\times 2^{1-bias-BIAS_{SE}},\end{cases}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $bias=2^{e-1}-1$ and $BIAS_{SE}$ is a shared exponent value. $BIAS_{SE}$
    is scaled according to the maximum value of the data for dot-product operations.
    For example, BM<2,3> represents a 6-bit data format having 1 sign bit, 2-bit exponent,
    and 3-bit significand. Such BM variant formats were applied for training. Utilizing
    these 6-bit BM formats produced equivalent accuracy to IFP32 formats but with
    fewer bits using CIFAR 10 and 100 dataset with ResNet-18, resulting in reduced
    memory-traffic and low energy consumption. Therefore, BM improved the resource
    efficiency in term of accuracy per memory access.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Implementation-Level Resource-Efficient Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fig. [8](#S5.F8 "Figure 8 ‣ 5 Implementation-Level Resource-Efficient Techniques
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques") classifies the implementation-level resource-efficient techniques.
    Most implementation-level techniques have focused on improving energy efficiency
    and computational speed for MAC operations, since MACs generally occupy more than
     90% of computational workload for both training and inference tasks in DNNs [[145](#bib.bib145)].
    The implementation-level resource-efficient techniques exploited the characteristics
    of MACs in DNN including data reuse, sparsity of weights and activations, and
    weight repetition from quantized DNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/67533633a9419a48b6bc90999bef3957.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Implementation-level resource-efficient techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Leveraging Data Reuse from Convolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The weights and the activations are heavily reused in convolution operations.
    For example, the weights of a filter are reused $((H-k_{H}+1)\times(W-k_{W}+1))/stride$
    times, where $H=W=4$ (height and width at input channel) and $k_{H}=k_{W}=3$ (height
    and width for a kernel) in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional Neural
    Network: ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques"). Generally, $H$ and $W$ are three orders of magnitude (e.g., 128,
    256, etc), $k_{H}$ and $k_{W}$ are one order of magnitude (e.g., 3, 5, etc), and
    $stride$ is either $1$ or $2$. For example, if $H=W=128$, $k_{H}=k_{W}=3$, and
    $stride=1$, each filter is reused $16129$ times for convolutional operations.
    Each input element at a covolutional layer is also reused approximately $M\times
    k_{H}\times k_{W}$ times, where $M$ is the number of the total kernels used in
    the layer. Fig. [9](#S5.F9 "Figure 9 ‣ 5.1 Leveraging Data Reuse from Convolution
    ‣ 5 Implementation-Level Resource-Efficient Techniques ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")
    describes the data access patterns for MAC operations used for convolutional layers.
    In each MAC computation in Fig. [9](#S5.F9 "Figure 9 ‣ 5.1 Leveraging Data Reuse
    from Convolution ‣ 5 Implementation-Level Resource-Efficient Techniques ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").(a),
    the data, a, b, and c, are read from the memory for multiply and add computation,
    and the result d is written back to the memory, where c contains a partial sum
    for the MAC. To save energy consumption, highly reused data for MAC computations
    can be stored in small local memory as shown in Fig. [9](#S5.F9 "Figure 9 ‣ 5.1
    Leveraging Data Reuse from Convolution ‣ 5 Implementation-Level Resource-Efficient
    Techniques ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques").(b). For example, the power consumption
    required to access data depends on where the data are located – accessing data
    from off-chip memory, DRAM, generally requires two orders of magnitude more than
    from on-chip memory [[31](#bib.bib31)]. For commercially available CPUs or GPUs,
    transforming convolutional operations into matrix multiplications can leverage
    such data reuse properties to accelerate the convolution operations by utilizing
    highly optimized BLAS libraries [[155](#bib.bib155)]. Many research works presented
    how to leverage such data reuse properties to improve the resource efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f7ab022d096db7755ca2882841d5bcd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Multiply-and-accumulate dataflow.'
  prefs: []
  type: TYPE_NORMAL
- en: '5.1.1 Employing SRAM Local Memory near to Processing Elements:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The use of SRAM buffers reduces the energy consumed by DNNs by up to two orders
    of magnitude, compared to DRAM. Similar to Fig. [9](#S5.F9 "Figure 9 ‣ 5.1 Leveraging
    Data Reuse from Convolution ‣ 5 Implementation-Level Resource-Efficient Techniques
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques").(b), Dianao architecture [[30](#bib.bib30)] employed one Neural Functional
    Unit (NFU) integrated with three separated local buffers, each for holding $16$
    input neurons, $16\times 16$ weights, and $16$ output neurons, in order to optimize
    circuitry for MAC operations. The weights and the activations stored to the local
    memories were reused efficiently by additionally using internal registers to store
    the partial sums and the circular buffer. The NFU is a three-stage pipelined architecture
    consisting of the multiplication, the adder-tree, and the activation stage. In
    the multiplication stage, $256$ multipliers support the multiplications based
    on the weight connections between $16$ input and $16$ output neurons. In the adder-tree
    stage, $16$ feature maps are generated from the multiplications based on adder-tree
    structure. In the activation stage, the $16$ feature maps are approximated for
    the $16$ activations by using piece-wise linear function approximation. DianNao
    with $65nm$ ASIC layout brought up to $118\times$ speedup and reduced the energy
    consumption by $21\times$, compared to a 128-bit 2GHz SIMD processor over the
    benchmarks used in [[30](#bib.bib30)]. One of the following studies adapted DianNao
    to deploy it on a supercomputer and named it as DaDianNao [[33](#bib.bib33)].
    Since the number of weights is generally larger than the number of input activations
    for convolution operations, DaDianNao stored a big chunk of weights and shared
    them to multiple NFUs by using a central embedded DRAM to reduce the data movement
    cost in delivering the weights associated to each NFU.'
  prefs: []
  type: TYPE_NORMAL
- en: '5.1.2 Leveraging Spatial Architecture:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Designing PEs and their local memory according to data reuse properties of
    MAC operations improved energy efficiency on FPGAs and ASIC [[31](#bib.bib31),
    [32](#bib.bib32)]. For example, Google TPU employs a systolic array architecture
    to send the data directly to an adjacent Processing Element (PE) as shown in Fig. [9](#S5.F9
    "Figure 9 ‣ 5.1 Leveraging Data Reuse from Convolution ‣ 5 Implementation-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques").(c) [[3](#bib.bib3)].
    Chen et al. [[31](#bib.bib31)] noticed that the computational throughput and energy
    consumption of CNNs mainly depended on data movement rather than computation and
    proposed a “row-stationary” spatial architecture (a variant of Fig. [9](#S5.F9
    "Figure 9 ‣ 5.1 Leveraging Data Reuse from Convolution ‣ 5 Implementation-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques").(c)), Eyeriss, to support
    parallel processing with minimal data movement energy cost by fully exploiting
    the data reuse property. For example, the three PEs in the first column in Fig [9](#S5.F9
    "Figure 9 ‣ 5.1 Leveraging Data Reuse from Convolution ‣ 5 Implementation-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques").(c) can be assigned
    to compute the first row of the convolution output using a $3\times 3$ filter
    - the three elements on each row of the kernel are stored to the local memory
    on each PE (i.e., “row-stationary” structure in [[145](#bib.bib145)]), and all
    the elements in the kernel are reused during convolution, generating the first
    row of the output. In this case, the partial summation values are stored back
    to the local memory on each PE.'
  prefs: []
  type: TYPE_NORMAL
- en: '5.1.3 Circuit Optimization:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exploring binary weights [[38](#bib.bib38)] with binary inputs offered the opportunity
    to explore XNOR gates for the efficient implementation of CNN [[123](#bib.bib123)],
    improving the accuracy per memory foot print and per Joule. In 2021, [[183](#bib.bib183)]
    proposed hardware-friendly statistic-based quantization units and near data processing
    engines to accelerate mixed precision training schemes by minimizing the number
    of accesses to higher precision data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Leveraging Sparsity of Weights and Activations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the forward pass, negative feature map values are converted to zeros after
    ReLU activation functions, making the activation data structure sparse. In addition,
    the trained weight values follow a sharp Gaussian distribution centered at zero,
    locating most of the weights near to zero. Quantizing such weights makes the weight
    data structure sparse, so the sparse weights can be fully exploited on the quantized
    networks such as binarized DNNs [[39](#bib.bib39), [38](#bib.bib38)] and ternary
    weight DNNs [[98](#bib.bib98), [185](#bib.bib185)].
  prefs: []
  type: TYPE_NORMAL
- en: '5.2.1 Skipping Operations during Runtime:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2016, several methods to conditionally skip MAC operations were proposed
    simultaneously [[31](#bib.bib31), [10](#bib.bib10), [103](#bib.bib103)]. Eyeriss [[31](#bib.bib31)]
    employed clock gating to block the convolution operations during runtime when
    either the weight or the activation was detected as zero in order to save computational
    power. Cnvlutin [[10](#bib.bib10)] skipped MAC operations associated with zero
    activations by employing separated “neuron lanes” according to different channels.
    Similarly, [[103](#bib.bib103)] proposed Cambricon-X that fetches the activations
    associated with any non-zero weights for convolutions by using the “step indexing”
    in order to skip the MAC operations associated with the zero weights. Cambricon-X
    brought 6 $\times$ resource-efficiency improvement in terms of accuracy per Joule,
    compared to the original DianNao architecture. In 2017, Kim et al. [[87](#bib.bib87)]
    proposed ZeNa that performs MAC operations only if both the weights and the activations
    are non-zero values. In 2018, Akhlaghi et al. [[8](#bib.bib8)] proposed a runtime
    technique, SnaPEA, that performs MAC operations associated with positive weights
    first and then negative weights later while monitoring the sign of the partial
    sum value. Since the activation values from ReLU are always greater or equal to
    zero, the convolution operation can be terminated once the partial sum value becomes
    negative. Notice that such decision should be performed during runtime, since
    the zero valued activation patterns depend on the test images. In 2021, another
    method skipping zero operations, GoSPA [[41](#bib.bib41)], was proposed, which
    is similar to ZeNa in that MAC operations were performed only when both input
    activations and weights were non-zero values. [[41](#bib.bib41)] constructed “Static
    Sparsity Filter" module by leveraging the property that the weight values are
    static while the activation values are dynamic to filter out zero activations
    associated with non-zero weights on the fly before MAC operations. Such skipping
    operation optimization techniques improved the accuracy per Joule, since the transistors
    associated with skipped operations were not toggled during runtime, saving dynamic
    power consumption.
  prefs: []
  type: TYPE_NORMAL
- en: '5.2.2 Encoding Sparse Weights/Activations/Feature Maps:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since memory access operations dominate the power consumption in deep learning
    applications, fetching the weights less frequently from memory by encoding and
    compressing the weights and the activations can improve the resource efficiency
    such as the accuracy per memory footprint, per memory access, and per Joule. Han
    et al. [[65](#bib.bib65), [64](#bib.bib64)] utilized the Huffman encoding scheme
    to compress the weights. The quantized DNN reduced the memory footprint of AlexNet
    on ImageNet dataset by $35$ times without losing accuracy. In [[65](#bib.bib65),
    [64](#bib.bib64)], a three stage pipelined operation was performed in order to
    reduce the memory footprint of DNNs as follows. The pruned sparse quantized weights
    were stored with Compressed Sparse Row (CSR) format and then divided into several
    groups. The weights in the same group were shared with the average value over
    the weights in the group, and they were re-trained thereafter. Huffman coding
    was used to compress the weights further. Parashar et al. [[119](#bib.bib119)]
    employed an encoding scheme to compress sparse weights and activations and designed
    an associated dataflow, "Compressed-Sparse Convolutional Neural Networks (SCNN)",
    to minimise data transfer and reduce memory footprint. Aimar et al. [[7](#bib.bib7)]
    proposed NullHop that encodes the sparse feature maps by using two sequentially
    ordered (i.e., internally indexed) additional storage, one for a 3D mask to indicate
    the positions of non-zero values and the other for storing the non-zero data sequentially
    in the feature map. For example, ‘0’s are marked at the position of zero values
    in the 3D mask, otherwise ‘1’s are marked. Decoding refers to both the 3D mask
    and the non-zero value list. Rhu et al. [[125](#bib.bib125)] presented HashedNet
    that utilizes a low cost hash function to compress sparse activations. The virtualized
    DNN (vDNN) [[126](#bib.bib126)] compressed sparse activation units using the “zero-value
    compression” technique to minimize the data transfer cost between GPU and CPU.
    The vDNN allowed users to utilize both GPU and CPU memory for DNN training.
  prefs: []
  type: TYPE_NORMAL
- en: '5.2.3 Decomposing Kernel Matrix:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Li et al. [[100](#bib.bib100)] proposed SqueezeFlow that reduces the number
    of operations for convolutions by decomposing the kernel matrix into non-zero
    valued sub-matrices and zero-valued sub-matrices. This method can improve the
    accuracy per Joule.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Leveraging Weight Repetition in Quantized DNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hedge et al. [[71](#bib.bib71)] noticed that the identical weight values were
    often repeated in quantized DNNs such as binary weight DNNs [[39](#bib.bib39),
    [38](#bib.bib38)] and ternary weight DNNs [[98](#bib.bib98), [185](#bib.bib185)]
    and proposed the Unique Weight CNN Accelerator (UCNN) that reduces the number
    of memory accesses and the number of operations by leveraging the repeated weight
    values in the quantized DNNs. For example, if a $2\times 2$ kernel consisting
    of $\{k_{1,1},k_{1,2},k_{2,1},k_{2,2}\}$ performs a convolution with the activation
    maps, $\{a_{1,1},a_{1,2},a_{2,1},a_{2,2}\}$. The conventional covolutional operation,
    $\Sigma_{i=1,j=1}^{i=2,j=2}k_{i,j}\times a_{i,j}$, requires $8$ read memory accesses,
    $4$ multiplications, and $3$ additions. If two of the weights in the kernel are
    identical (e.g., $k_{1,1}=k_{2,2}$ and $k_{1,2}=k_{2,1}$), the covolutional operation
    can be performed using: $k_{1,1}(a_{1,1}+a_{2,2})+k_{1,2}(a_{1,2}+a_{2,1})$, requiring
    $6$ read memory accesses, $2$ multiplications, and $3$ additions. The UCNN improved
    the accuracy per Joule by $1.2-4\times$ in AlexNet and LeNet on Eyeriss architecture
    using ImageNet dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Leveraging Innovative Technology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many research attempts have leveraged innovative computing architecture technologies
    such as neuromorphic computing and in-memory processing as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '5.4.1 Neuromorphic Computing:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neuromorphic computing mimics the brain, including brain components such as
    neurons and synapses; furthermore, biological neural systems include axons, dendrites,
    glial cells, and spiking signal transferring mechanisms [[84](#bib.bib84)]. The
    memristor, ‘memory resistor’, is one of the most widely used devices in neuromorphic
    computing. ISAAC [[136](#bib.bib136)] replaced MAC operation units with the memristor
    crossbars based on DaDianNao architecture. In the crossbars, every wire in horizontal
    wire array was connected to every wire in vertical wire array with a resistor.
    Different level voltages, $V=[v_{1},v_{2},...,v_{m}]$, were applied to the horizontal
    wires connected to a vertical wire by the different resistors, $R=[1/g_{1},1/g_{2},...,1/g_{m}]$.
    With mapping $v_{i}$ to input elements and $g_{i}$ to weights, where $i=1,...,m$,
    the output current, $I$, from the vertical wire can be represented as MAC operations
    in a layer, $I=\Sigma_{i}^{m}(v_{i}\times g_{i})$, based on the Kirchhoff’s law.
    Multiple MAC operations can be performed by collecting the currents from multiple
    vertical wires. ISAAC employed digital-to-analog converters to receive the input
    elements and covert them into the appropriate voltages and analog-to-digital converters
    to convert the current values into digitized feature map values. Due to lack of
    re-programability of resistors in the crossbars, ISAAC architecture was only available
    for the inference tasks. ISAAC improved $5.5\times$ accuracy per Joule, compared
    to full-fledged DaDianNao. As another neuromorphic computing approaches, many
    research attempts implemented Hodgkin-Huxley and Morris Lecar models [[75](#bib.bib75)]
    that describes the activity of neurons using nonlinear differential equations
    in hardware simulators [[109](#bib.bib109), [92](#bib.bib92), [129](#bib.bib129),
    [139](#bib.bib139), [142](#bib.bib142), [140](#bib.bib140), [141](#bib.bib141),
    [56](#bib.bib56), [16](#bib.bib16), [15](#bib.bib15), [46](#bib.bib46)]. Several
    studies implemented neuromorphic architectures in ASIC, including TrueNorth [[9](#bib.bib9)],
    SpiNNaker [[117](#bib.bib117)], Neurogrid [[18](#bib.bib18)], BrainScaleS [[134](#bib.bib134)],
    and IFAT [[121](#bib.bib121)]. Please refer to [[135](#bib.bib135)] for a comprehensive
    survey of neuromorphic computing.
  prefs: []
  type: TYPE_NORMAL
- en: '5.4.2 In-Memory Processing:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Scaling down the size of transistors enables energy efficiency and high performance
    for Von-Neumann computing systems. However, it became very challenging in the
    era of sub-10nm technologies due to physical limitations [[51](#bib.bib51), [113](#bib.bib113),
    [11](#bib.bib11)]. To address this challenge, researchers proposed a paradigm
    of in-memory processing to improve performance and energy efficiency by integrating
    computations units into memory devices [[63](#bib.bib63), [168](#bib.bib168),
    [101](#bib.bib101)]. Several studies proposed to enable in-memory processing to
    accelerate DNNs  [[12](#bib.bib12), [43](#bib.bib43), [68](#bib.bib68), [88](#bib.bib88),
    [52](#bib.bib52), [175](#bib.bib175)]. For example, XNOR-SRAM [[175](#bib.bib175)]
    integrated the XNOR gates and accumulation logic into SRAM to fetch data from
    SRAM and perform MAC operation in one cycle. Notice that this approach was applicable
    for binarized DNNs such as [[39](#bib.bib39)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Adaptive Compute Resource Assignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection comprises the methods assigning runtime compute resources adaptively
    to the DNN inference workload to improve resource efficiency. The implementation
    of the DNNs can be adapted to the accuracy requirements of the applications by
    using various runtime implementation techniques as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '5.5.1 Early Exiting:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The required depth of DNN depends on the problem complexity. The “early exiting”
    technique allows a DNN to classify an object as early as possible by having multiple
    exit classifier points in a single DNN [[118](#bib.bib118), [150](#bib.bib150),
    [151](#bib.bib151)]. The early exiting technique was applied to distributed computing
    systems, addressing concern about privacy, response time, and higher quality of
    experience [[151](#bib.bib151)]. Such early exiting methods minimized the compute
    resources and the inference latency, improving the accuracy per Joule, per operation,
    and per core utilization. Please refer to [[111](#bib.bib111)] for the details
    on the early exiting techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '5.5.2 Runtime Channel Width Adaptation:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The runtime channel width adaptation pruned unimportant filters during runtime.
    In 2018, Fang et. al [[47](#bib.bib47)] presented a single DNN model, NestDNN,
    being able to switch between multiple capacities of the DNN during runtime according
    to the accuracy and inference latency requirement. During training, unimportant
    filters from the original model were pruned to generate the smallest possible
    model, “seed model”. Each re-training, some of pruned filters were added to the
    seed model while fixing the filter parameters from the previous training. Since
    the seed model was descended from the original model, the accuracy for each capacity
    in NestDNN was higher than the model having the identical capacity trained from
    the scratch. Similarly, Yu et. al [[177](#bib.bib177)] proposed another runtime
    switchable DNN model, Slimmable Neural Network, in which a larger capacity model
    shared the filter parameters from a smaller capacity model.
  prefs: []
  type: TYPE_NORMAL
- en: '5.5.3 Runtime Model Switching:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lee et al. [[96](#bib.bib96)] selected the best performing object detectors
    between multiple DNN detectors during runtime according to dynamic video content
    to improve the accuracy per core utilization and per Joule. Lou et al. [[106](#bib.bib106)]
    switched between multiple DNNs, generated from the Once-For-All NAS of [[25](#bib.bib25)],
    during runtime according to dynamic workload. For example, when the inference
    latency of a DNN was increased due to a newly assigned workload, a runtime decision
    maker downgraded the current DNN during runtime to meet a latency constraint.
    Such runtime model switching approaches were appropriate when memory resources
    were sufficient, since the multiple DNNs should be pre-loaded in DRAM.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Interrelated Influences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section discusses the influence from higher- to lower-level techniques
    as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Influences of Model-Level Techniques on Arithmetic-Level Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Weight quantization [[38](#bib.bib38), [98](#bib.bib98), [185](#bib.bib185)]
    in model-level techniques influenced arithmetic-level techniques as shown in Fig. [7](#S4.F7
    "Figure 7 ‣ 4 Arithmetic-Level resource-efficient Techniques ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").
    The multiplications using the quantized binary weights can be replaced with multiplexers,
    removing multiplication arithmetic operations. The resource efficiency from the
    model-level techniques can be further improved by utilizing the arithmetic-level
    techniques. For example, quantized DNNs such as ternary weight and binarized DNNs
    allowed INT8 arithmetic to be used in training [[166](#bib.bib166), [172](#bib.bib172)].
    When reduced precision DNNs suffered from zero gradients, the reduced precision
    arithmetic was replaced with a hybrid version arithmetic using both BFP and FP
    [[45](#bib.bib45)] or the Block MiniFloat format [[49](#bib.bib49)].'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Influences of Model-Level Techniques on Implementation-Level Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Weight quantization in model-level techniques influenced the implementation-level
    techniques as shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5 Implementation-Level Resource-Efficient
    Techniques ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques"). Pruning weights can bring sparsity in the
    hardware architecture while pruning filters [[107](#bib.bib107), [99](#bib.bib99)]
    maintains dense structure. Weight quantization in the model-level techniques allows
    a DNN to utilize fewer bits for weights in order to save memory resource usage,
    requiring customized hardware. For example, EIE [[64](#bib.bib64)] is an inference
    accelerator with weights quantized by 4 bits. To implement the weight quantization
    method effectively, EIE utilized weight sharing to reduce the model size further
    and fit the compressed DNN into the on-chip SRAM. Exploring binary weights [[39](#bib.bib39)]
    with binary inputs offered the opportunity to explore XNOR gates for the efficient
    implementation of CNNs [[123](#bib.bib123)], improving the accuracy per memory
    footprint and per Joule. In [[153](#bib.bib153)], ternary neural networks [[98](#bib.bib98),
    [185](#bib.bib185)] were implemented on FPGAs by unrolling convolution operations.
    Since quantized DNNs [[98](#bib.bib98), [185](#bib.bib185)] increased the number
    of repeated weights in DNNs, UCNN [[71](#bib.bib71)] leveraged the property of
    the repeated weight values in quantized DNNs to improve resource efficiency such
    as accuracy per memory access and per operation. As main limitation, the weight
    quantization methods such as [[38](#bib.bib38), [185](#bib.bib185), [98](#bib.bib98)]
    were not suitable for commercially available CPUs and GPUs, since such computing
    platforms do not support binary and ternary weights in hardware. Therefore, the
    implementation of weight quantization methods on CPUs or GPUs might not improve
    accuracy per Joule as higher precision arithmetic still was required in part of
    data path in training and inference. The bottleneck structures generated by compact
    convolutions in [[132](#bib.bib132), [82](#bib.bib82)] can be used to reduce the
    data size transferred between a local device and an edge server for the efficient
    implementation of edge-based AI [[111](#bib.bib111)].'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Influences of Arithmetic-Level Techniques on Implementation-Level Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The arithmetic-level techniques influenced the implementation-level techniques
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: First, the research in arithmetic utilization acted as a catalyst for commodity
    CPUs and GPUs. For example, the mixed precision research [[114](#bib.bib114)]
    laid a foundation for tensor cores in latest NVIDIA GPUs, which can accelerate
    the performance of deep learning workloads by supporting a fused multiply–add
    operation and the mixed precision training capability in hardware [[19](#bib.bib19)].
    The BFloat16 format [[24](#bib.bib24)] designed by Google overcomes the limited
    accuracy issue of the IFP16 format by providing the same dynamic range as IFP32,
    and it is supported in hardware in Intel Cooper Lake Xeon processors, NVIDIA A100
    GPUs, and Google TPUs. In 2016, NVIDIA Pascal GPUs supported IFP16 arithmetic
    in hardware to accelerate DNN applications. In 2017, NVIDIA Volta GPUs supported
    IFP16 tensor cores. In 2020, the NVIDIA Ampere architecture supported tensor cores,
    TF32, BFloat16, and sparsity acceleration in hardware to accelerate MACs [[2](#bib.bib2)].
    The Graphcore company developed the Intelligent Processing Unit (IPU), which employs
    local memory assigned to each processing unit with support for a large number
    of independently operating hardware threads [[85](#bib.bib85)]. The IPU is an
    efficient computing architecture customized to “fine-grained, irregular computation
    that exhibits irregular data access”.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the arithmetic-level techniques led to specialized custom accelerators
    for deep learning. There is ample evidence in the arithmetic-level literature
    such as [[49](#bib.bib49), [45](#bib.bib45), [21](#bib.bib21), [158](#bib.bib158),
    [170](#bib.bib170)] that even smaller operators (e.g., 16 bits or even less) have
    almost no impact on the accuracy of DNNs. For example, DianNao [[30](#bib.bib30)]
    and DaDianNao [[33](#bib.bib33)] were customized to 16-bit fixed-point arithmetic
    operators instead of word-size (e.g., 32-bit) floating-point operators. ISAAC [[136](#bib.bib136)]
    is a fully-fledged crossbar-based CNN accelerator architecture, which implemented
    a memristor-based logic based on resistive memory, suitable for 16-bit arithmetic
    for DNN workloads. Wang et al. [[158](#bib.bib158)] designed their customized
    8-bit floating point arithmetic multiplications with 16-bit accumulations on an
    ASIC-based hardware platform with a $14nm$ silicon technology to support energy-efficient
    deep learning training. The Eyeriss [[31](#bib.bib31)] and SnaPEA [[8](#bib.bib8)]
    accelerators were customized to 16-bit arithmetic. UCNN [[71](#bib.bib71)] utilized
    8-bit and 16-bit fixed point configurations. SCNN [[119](#bib.bib119)] utilized
    16-bit multiplication and 24-bit accumulation.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the mixed precision training schemes were accelerated in hardware by
    minimizing the data conversion overhead between lower and higher precision formats
    in updating weights and activations [[183](#bib.bib183)]. Also, the stochastic
    rounding scheme was supported in hardware in Intel Loihi processor [[40](#bib.bib40)]
    and Graphcore IPU [[85](#bib.bib85)], since it was often required for quantizing
    weights and activations during training [[166](#bib.bib166), [60](#bib.bib60),
    [172](#bib.bib172)].
  prefs: []
  type: TYPE_NORMAL
- en: 7 Future Trend for Resource-Efficient Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open research issues in resource-efficient deep learning emerge in an attempt
    to improve the resource efficiency further, compared to the state-of-the-art resource-efficient
    techniques discussed in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Future Trend for Model-Level Resource-Efficient Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, edge-based computing has become pervasive, and fitting DNN models
    into such resource-constrained devices for inference tasks has become extremely
    challenging.
  prefs: []
  type: TYPE_NORMAL
- en: '7.1.1 Improving Physical Resource Efficiency under Very Low Compute Resource
    Budget:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many researchers considered keeping dense network structures after pruning parameters,
    including pruning channels [[104](#bib.bib104), [53](#bib.bib53)], filters [[99](#bib.bib99),
    [107](#bib.bib107)], etc., to implement the pruned networks efficiently on commercially
    available CPUs and GPUs. Since then, various budget-aware network pruning methods
    were proposed, given a resource budget such as the number of floating point operations
    [[57](#bib.bib57)] and the number of neurons [[97](#bib.bib97)] for the inference
    task. NetAdapt [[171](#bib.bib171)] pruned the filters as it measured physical
    resources such as latency, energy, memory footprint, etc. to improve the physical
    resource efficiency directly rather than abstract resource efficiency. Along with
    the fast technology development in computer networks and wireless communications,
    research attempts to improve physical resource efficiency are expected to continue
    to deploy appropriate DNN models on extremely low resource devices such as mobile,
    IoT, and edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: '7.1.2 Neural Network Search Methods Combined with Domain Specific Knowledge:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2016 and 2017, handcrafted compressed DNNs were presented such as SqueezeNet
    [[82](#bib.bib82)], MobileNet [[76](#bib.bib76)], ShuffleNet [[181](#bib.bib181)],
    and DenseNet [[79](#bib.bib79)], and they improved both abstract and physical
    resource efficiency. Various NAS methods [[148](#bib.bib148), [149](#bib.bib149),
    [147](#bib.bib147), [70](#bib.bib70)] assisted to seek the optimized DNN models
    (e.g., least sufficient models) by searching candidate spaces according to the
    training dataset, and the compressed models found by the NAS methods generally
    showed superior physical resource efficiency to the handcrafted compressed DNNs.
    As mobile and edge devices become prevalent, we expect that automatic search methods
    integrating with domain specific model compression methods are expected to be
    paid attention in the future. For example, performance-aware NAS methods for resource-constrained
    devices have been vividly paid attention [[13](#bib.bib13), [147](#bib.bib147),
    [149](#bib.bib149), [148](#bib.bib148), [171](#bib.bib171)] since 2019\. Such
    performance-aware NAS methods can be enhanced by adopting recent domain specific
    model-level resource-efficient techniques such as [[65](#bib.bib65), [82](#bib.bib82),
    [76](#bib.bib76), [99](#bib.bib99), [50](#bib.bib50)].
  prefs: []
  type: TYPE_NORMAL
- en: '7.1.3 Theoretical Studies Behind Model-Level Resource-Efficient Techniques:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The bias-variance trade-off [[54](#bib.bib54)] is behind the model-level resource-efficient
    techniques. For example, compressed models having fewer parameters increase the
    regularization effect on the accuracy, minimizing overfitting issues [[65](#bib.bib65),
    [54](#bib.bib54)]. For example, [[50](#bib.bib50)] proposed the lottery ticket
    hypothesis in that better (or equivalent) performing sub-DNNs using fewer weights
    exist inside a dense, randomly-initialized, feed-forward DNN. In order to seek
    the better performing sparse sub-DNNs, “winning tickets”, the survived weights
    from weight pruning were re-trained by replacing the survived weights with the
    random weights initially used to train the original dense DNN. The lottery ticket
    hypothesis implies that such sparse DNNs could be found in even compressed dense
    DNNs. We expect that such theoretical studies supporting model-level resource-efficient
    techniques can be paid attention in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Future Trend for Arithmetic-Level Resource-Efficient Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As edge- and mobile-based devices becomes pervasive for AI applications, open
    research issues emerge in the attempts to improve further physical resource efficiency
    on such resource-constrained devices, compared to state-of-the-art arithmetic-level
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Adapting Arithmetic Precision Level to Numerical Properties of DNNs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2011, Vanhoucke et al. [[155](#bib.bib155)] demonstrated the feasibility
    of INT8 arithmetic for inference tasks using a shallow depth neural network on
    an Intel x86 architecture. In 2015, Gupta et al. [[60](#bib.bib60)] demonstrated
    that employing FiP16 with a stochastic rounding scheme for a shallow depth neural
    network produced equivalent accuracy using MNIST and CIFAR10 to that using IFP32\.
    In 2018, [[114](#bib.bib114)] presented the guidelines for mixed precision training.
    The guidelines contained the information on how to deploy different-level arithmetic
    precision on different computing components in MAC operations. The guidelines
    led to further research attempts in hardware optimization for mixed precision
    training [[183](#bib.bib183)]. We expect that the research attempts in adapting
    an arithmetic precision to DNN computing components according to their numerical
    stability characteristics will continue in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '7.2.2 Adapting Arithmetic Format to Problem Complexity:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since floating point arithmetic is computationally intensive, several studies
    have removed floating point arithmetic in training tasks. For example, Wu et al. [[166](#bib.bib166)]
    demonstrated that quantized networks such as binary or ternary weight networks
    can be trained using INT8 arithmetic along with a scaling and a stochastic rounding
    scheme. In 2020, Yang et al. [[172](#bib.bib172)] demonstrated that quantizing
    weights and activations with INT8 format while applying INT24 arithmetic to the
    weight updates could accelerate training and inference tasks for various ResNet
    models using ImageNet with minor accuracy loss, compared to those using IFP32\.
    Recently, RNS-based quantization was applied to various DNNs [[131](#bib.bib131),
    [130](#bib.bib130)]. It will continue in the future to explore how to adapt a
    number format to given DNN structures for inference and training tasks in order
    to improve resource efficiency further on resource-constrained devices.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Future Trend for Implementation-Level Resource-Efficient Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, there are two ways to accelerate DNN computations. One is to optimize
    DNN computations on given compute architecture such as CPUs and GPUs. The other
    is to customize dataflow on FPGAs and ASIC.
  prefs: []
  type: TYPE_NORMAL
- en: '7.3.1 Leveraging Spatial and Temporal Data Access Pattern with Lower Precision
    Arithmetic on CPUs and GPUs:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A decade ago, [[154](#bib.bib154)] accelerated DNN computations on a SIMD CPU
    by leveraging the data reuse property from MAC operations using SSE instructions
    and fast fixed point arithmetic. NVIDIA Tensor cores and Google TPUs support a
    customized arithmetic precision format such as IFP16, BFloat16, etc. and customized
    datapath in hardware for deep learning applications [[2](#bib.bib2), [3](#bib.bib3)].
    The research attempts to leverage spatial and temporal data access patterns with
    lower precision arithmetic in commercially available CPUs and GPUs will continue
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '7.3.2 Leveraging Spatial and Temporal Data Access Pattern with Lower Precision
    Arithmetic on FPGAs and ASIC:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In 2014, [[30](#bib.bib30)] stressed the limitation of commercially available
    GPUs and CPUs for DNN applications: “While a cache is an excellent storage structure
    for a general-purpose processor, it is a sub-optimal way to exploit reuse because
    of the cache access overhead (tag check, associativity, line size, speculative
    read, etc.) and cache conflicts.” To overcome this limitation, [[30](#bib.bib30)]
    proposed a SIMD style hardware accelerator, DianNao, that employs three separate
    local on-chip memories (SRAMs) to maximize the performance by fully leveraging
    the data reuse property. [[33](#bib.bib33)] pointed out that DianNao was still
    limited in the memory bandwidth to access massive weights in covolutional layers
    and proposed DaDianNao architecture employing large eDRAMs with four banks to
    store and share the weights in the eDRAMs efficiently. In 2016, [[31](#bib.bib31)]
    pointed out that the data movement cost is still dominant, compared to the computation
    cost for DNN applications on the SIMD/SIMT architectures of [[33](#bib.bib33),
    [30](#bib.bib30)] and proposed a dataflow architecture to minimize energy consumption
    caused by data movement in DNN applications. Since 2016, most implementation-level
    techniques leveraged the sparsity of weights and activations in DNNs to minimize
    the number of arithmetic operations during runtime [[10](#bib.bib10), [103](#bib.bib103),
    [87](#bib.bib87), [8](#bib.bib8), [41](#bib.bib41)] and the data transfer cost
    required to store and transfer the sparse weights and activations [[64](#bib.bib64),
    [7](#bib.bib7), [119](#bib.bib119), [125](#bib.bib125), [126](#bib.bib126)]. The
    research efforts to customize DNN dataflow by leveraging spatial and temporal
    data access patterns with lower precision arithmetic are expected to continue
    in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: '7.3.3 Resource-Efficient Implementation on Distributed AI Compute Platforms:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Resource-efficient techniques on distributed AI such as split federated learning
    [[62](#bib.bib62), [152](#bib.bib152)] and early exiting [[150](#bib.bib150),
    [151](#bib.bib151)] have recently attracted a great deal of attention thanks to
    fast wireless network technology development. The main open research issues include
    data communication overhead between an edge device and the cloud and energy consumption,
    required to run DNNs on a lower power (or battery) edge device. For example, many
    research attempts leveraged the bottleneck structure of a DNN to save the data
    communication bandwidth, but such attempts could degrade the accuracy significantly
    in the DNNs employing compact convolutions [[111](#bib.bib111)]. Thus, adapting
    such model compression techniques to distributed AI environments can be paid attention
    in the future in order to save energy consumption on edge devices and the bandwidth
    required for communication between an edge device and a cloud. For example, encoding
    and decoding offloading data from the cloud to edge devices or vice versa can
    minimize the data communication overhead [[173](#bib.bib173)]. Such resource-efficient
    encoding/decoding schemes for split learning (or inference) tasks can draw attention
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '7.3.4 Neuromorphic Computing for Deep Learning:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neuromorphic computing can lead to dramatic changes in energy efficiency for
    deep learning [[135](#bib.bib135)]. This expectation is based on the fact that
    neuromorphic computing is not an incremental improvement of existing von Neumann
    architectures requiring considerable energy due to substantial instruction fetch/decode
    operations, but an fully optimized dataflow optimization customized to the activity
    of a neural network. Therefore, neuromorphic computing research can be paid attention
    in the future to maximize the accuracy per Joule.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our survey is the first to provide a comprehensive survey coverage of the recent
    resource-efficient deep learning techniques based on the three-level hierarchy
    including model-, arithmetic-, and implementation-level techniques. Our survey
    also utilizes multiple resource efficiency metrics to clarify which resource efficiency
    metrics each technique can improve. For example, most model-level resource-efficient
    techniques contribute to improving abstract resource efficiency, while the arithmetic-
    and the implementation-level techniques directly contribute to improving physical
    resource efficiency by employing reduced precision arithmetic and/or optimizing
    the dataflow of DNN architectures. Therefore, the efficient implementation of
    the model-level techniques on given compute platforms is essential to improve
    physical resource efficiency [[145](#bib.bib145)].
  prefs: []
  type: TYPE_NORMAL
- en: In the future, we expect that the three-level resource-efficient deep learning
    techniques can be adapted to distributed AI applications, along with fast wireless
    communication technology development. Since edge or mobile devices are subjected
    to physical resource constraints such as power, memory, and inference speed, the
    implementation should consider such constraints for the distributed AI applications.
    The state-of-the-art works include the NAS variants of [[147](#bib.bib147), [133](#bib.bib133),
    [115](#bib.bib115)] that seek the optimal performing DNN models fitted to the
    resource-constrained edge-devices. Improving such NAS variants by combining them
    with various model-, arithmetic-, and implementation-level resource-efficient
    techniques can be paid attention in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, our survey suggests that the bias-variance trade-off [[54](#bib.bib54)]
    is behind the model-level resource-efficient techniques. According to the trade-off,
    a DNN having fewer parameters increases the regularization effect on the accuracy,
    minimizing overfitting issues. Therefore, there exists the least sufficient model
    size that produces the best accuracy on test dataset according to the problem
    complexity and the training data quantity and quality. Similarly, [[50](#bib.bib50)]
    claimed the lottery ticket hypothesis in that better (or equivalent) performing
    sub-DNNs using fewer weights exist inside a dense feed-forward DNN. Einstein quoted
    “Everything should be made as simple as possible, but not simpler.” We hope that
    our survey will contribute to machine learning, arithmetic, and system community
    by providing them with a comprehensive survey for various resource-efficient deep
    learning techniques as guidelines to seek DNN structures using least sufficient
    parameters and least sufficient precision arithmetic on particular compute platforms,
    customized to the problem complexity and the training data quantity and quality.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs: []
  type: TYPE_NORMAL
- en: This project has received funding by the Engineering and Physical Sciences Research
    Council under the grant agreement No. EP/T022345/1 and by CHIST-ERA under the
    grant agreement No. CHIST-ERA-18-SDCDN-002 (DiPET). This research was also partially
    supported by National R&D Program through the National Research Foundation of
    Korea (NRF) funded by Ministry of Science and ICT (2021M3H2A1038042)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Imagenet benchmark (image classification on imagenet). [https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Nvidia ampere architecture white paper. [https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Quantifying the performance of the TPU, our first machine learning chip.
    [https://cloud.google.com/blog/products/gcp/quantifying-the-performance-of-the-tpu-our-first-machine-learning-chip](https://cloud.google.com/blog/products/gcp/quantifying-the-performance-of-the-tpu-our-first-machine-learning-chip).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Ieee standard for floating-point arithmetic. IEEE Std 754-2019 (Revision
    of IEEE 754-2008), pages 1–84, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning
    From Data. AMLBook, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Agrawal, S. M. Mueller, B. M. Fleischer, X. Sun, N. Wang, J. Choi, and
    K. Gopalakrishnan. Dlfloat: A 16-b floating point format designed for deep learning
    training and inference. In 2019 IEEE 26th Symposium on Computer Arithmetic (ARITH),
    pages 92–95, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Alessandro Aimar, Hesham Mostafa, Enrico Calabrese, Antonio Rios-Navarro,
    Ricardo Tapiador-Morales, Iulia-Alexandra Lungu, Moritz B. Milde, Federico Corradi,
    Alejandro Linares-Barranco, Shih-Chii Liu, and Tobi Delbruck. Nullhop: A flexible
    convolutional neural network accelerator based on sparse representations of feature
    maps. IEEE Transactions on Neural Networks and Learning Systems, 30(3):644–656,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] V. Akhlaghi, A. Yazdanbakhsh, K. Samadi, R. K. Gupta, and H. Esmaeilzadeh.
    Snapea: Predictive early activation for reducing computation in deep convolutional
    neural networks. In 2018 ACM/IEEE 45th Annual International Symposium on Computer
    Architecture (ISCA), pages 662–673, Los Alamitos, CA, USA, jun 2018\. IEEE Computer
    Society.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Filipp Akopyan, Jun Sawada, Andrew Cassidy, Rodrigo Alvarez-Icaza, John
    Arthur, Paul Merolla, Nabil Imam, Yutaka Nakamura, Pallab Datta, Gi-Joon Nam,
    Brian Taba, Michael Beakes, Bernard Brezzo, Jente B. Kuang, Rajit Manohar, William P.
    Risk, Bryan Jackson, and Dharmendra S. Modha. Truenorth: Design and tool flow
    of a 65 mw 1 million neuron programmable neurosynaptic chip. IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems, 34(10):1537–1557,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright
    Jerger, and Andreas Moshovos. Cnvlutin: Ineffectual-neuron-free deep neural network
    computing. In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture
    (ISCA), pages 1–13, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Mustafa F. Ali, Robert Andrawis, and Kaushik Roy. Dynamic read current
    sensing with amplified bit-line voltage for stt-mrams. IEEE Transactions on Circuits
    and Systems II: Express Briefs, 67(3):551–555, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Mustafa F. Ali, Akhilesh Jaiswal, and Kaushik Roy. In-memory low-cost
    bit-serial addition using commodity dram technology. IEEE Transactions on Circuits
    and Systems I: Regular Papers, 67(1):155–165, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Andrew Anderson, Jing Su, Rozenn Dahyot, and David Gregg. Performance-oriented
    neural architecture search, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances
    in Neural Information Processing Systems (NeurIPS), 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Arindam Basu. Small-signal neural models and their applications. IEEE
    Transactions on Biomedical Circuits and Systems, 6(1):64–75, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Arindam Basu, Csaba Petre, and Paul Hasler. Bifurcations in a silicon
    neuron. In 2008 IEEE International Symposium on Circuits and Systems, pages 428–431,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Yoshua Bengio. Deep learning of representations for unsupervised and transfer
    learning. In Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and
    Daniel Silver, editors, Proceedings of ICML Workshop on Unsupervised and Transfer
    Learning, volume 27 of Proceedings of Machine Learning Research, pages 17–36,
    Bellevue, Washington, USA, 02 Jul 2012\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Ben Varkey Benjamin, Peiran Gao, Emmett McQuinn, Swadesh Choudhary, Anand R.
    Chandrasekaran, Jean-Marie Bussat, Rodrigo Alvarez-Icaza, John V. Arthur, Paul A.
    Merolla, and Kwabena Boahen. Neurogrid: A mixed-analog-digital multichip system
    for large-scale neural simulations. Proceedings of the IEEE, 102(5):699–716, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Pierre Blanchard, Nicholas J Higham, Florent Lopez, Theo Mary, and Srikara
    Pranesh. Mixed precision block fused multiply-add: Error analysis and application
    to gpu tensor cores. SIAM Journal on Scientific Computing, 42(3):C124–C141, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal
    speed and accuracy of object detection, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] R. Bordawekar, B. Abali, and M.H. Chen. Efloat: Entropy-coded floating
    point format for deep learning. arXiv:2102.02705, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Léon Bottou and Yann Le Cun. Large scale online learning. In Advances
    in Neural Information Processing Systems 16. MIT Press, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Cristian Buciluă, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression.
    In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining, KDD ’06, page 535–541, New York, NY, USA, 2006\. Association
    for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] N. Burgess, J. Milanovic, N. Stephens, K. Monachopoulos, and D. Mansell.
    Bfloat16 processing for neural networks. In 2019 IEEE 26th Symposium on Computer
    Arithmetic (ARITH), pages 88–91, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all:
    Train one network and specialize it for efficient deployment. In ICLR ’20: International
    Conference on Learning Representations, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Z. Carmichael, H. F. Langroudi, C. Khazanov, J. Lillie, J. L. Gustafson,
    and D. Kudithipudi. Deep positron: A deep neural network using the posit number
    system. In 2019 Design, Automation Test in Europe Conference Exhibition (DATE),
    pages 1421–1426, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Zachariah Carmichael, Hamed F. Langroudi, Char Khazanov, Jeffrey Lillie,
    John L. Gustafson, and Dhireesha Kudithipudi. Performance-efficiency trade-off
    of low-precision numerical formats in deep neural networks. In Proceedings of
    the Conference for Next Generation Arithmetic 2019, CoNGA’19, New York, NY, USA,
    2019\. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Chip-Hong Chang, Amir Sabbagh Molahosseini, Azadeh Alsadat Emrani Zarandi,
    and Tian Fatt Tay. Residue number systems: A new paradigm to datapath optimization
    for low-power and high-performance digital signal processing applications. IEEE
    Circuits and Systems Magazine, 15(4):26–44, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker.
    Learning efficient object detection models with knowledge distillation. In I. Guyon,
    U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
    editors, Advances in Neural Information Processing Systems 30, pages 742–751\.
    Curran Associates, Inc., 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,
    and Olivier Temam. Diannao: A small-footprint high-throughput accelerator for
    ubiquitous machine-learning. SIGPLAN Not., 49(4):269–284, February 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Chen, J. Emer, and V. Sze. Eyeriss: A spatial architecture for energy-efficient
    dataflow for convolutional neural networks. In 2016 ACM/IEEE 43rd Annual International
    Symposium on Computer Architecture (ISCA), pages 367–379, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Yu-Hsin Chen, Tushar Krishna, Joel S. Emer, and Vivienne Sze. Eyeriss:
    An energy-efficient reconfigurable accelerator for deep convolutional neural networks.
    IEEE Journal of Solid-State Circuits, 52(1):127–138, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling
    Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, and Olivier Temam. Dadiannao: A machine-learning
    supercomputer. In Proceedings of the 47th Annual IEEE/ACM International Symposium
    on Microarchitecture, MICRO-47, pages 609–622, Washington, DC, USA, 2014\. IEEE
    Computer Society.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus
    Rohrbach, Shuicheng Yan, and Jiashi Feng. Drop an octave: Reducing spatial redundancy
    in convolutional neural networks with octave convolution. In Proceedings of the
    IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Cheng, D. Wang, P. Zhou, and T. Zhang. Model compression and acceleration
    for deep neural networks: The principles, progress, and challenges. IEEE Signal
    Processing Magazine, 35(1):126–136, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Francois Chollet. Xception: Deep learning with depthwise separable convolutions.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] NVIDIA Corporation. NVIDIA Tesla V100 GPU architecture, 8 2017. WP-08608-001v1.1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect:
    Training deep neural networks with binary weights during propagations. In C. Cortes,
    N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural
    Information Processing Systems 28, pages 3123–3131\. Curran Associates, Inc.,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
    Bengio. Binarized neural networks: Training deep neural networks with weights
    and activations constrained to +1 or -1, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang
    Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain,
    Yuyun Liao, Chit-Kwan Lin, Andrew Lines, Ruokun Liu, Deepak Mathaikutty, Steven
    McCoy, Arnab Paul, Jonathan Tse, Guruguhanathan Venkataramanan, Yi-Hsin Weng,
    Andreas Wild, Yoonseok Yang, and Hong Wang. Loihi: A neuromorphic manycore processor
    with on-chip learning. IEEE Micro, 38(1):82–99, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Chunhua Deng, Yang Sui, Siyu Liao, Xuehai Qian, and Bo Yuan. Gospa: An
    energy-efficient high-performance globally optimized sparse convolutional neural
    network accelerator. In 2021 ACM/IEEE 48th Annual International Symposium on Computer
    Architecture (ISCA), pages 1110–1123, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression
    and hardware acceleration for neural networks: A comprehensive survey. Proceedings
    of the IEEE, 108(4):485–532, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Quan Deng, Lei Jiang, Youtao Zhang, Minxuan Zhang, and Jun Yang. Dracc:
    a dram based accelerator for accurate cnn inference. In 2018 55th ACM/ESDA/IEEE
    Design Automation Conference (DAC), pages 1–6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
    Exploiting linear structure within convolutional networks for efficient evaluation.
    In Proceedings of the 27th International Conference on Neural Information Processing
    Systems - Volume 1, NeurIPS’14, page 1269–1277, Cambridge, MA, USA, 2014\. MIT
    Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Mario Drumond, Tao Lin, Martin Jaggi, and Babak Falsafi. Training dnns
    with hybrid block floating point. In 32nd Conference on Neural Information Processing
    Systems, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] D. Dupeyron, S. Le Masson, Y. Deval, G. Le Masson, and J.-P. Dom. A bicmos
    implementation of the hodgkin-huxley formalism. In Proceedings of Fifth International
    Conference on Microelectronics for Neural Networks, pages 311–316, 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Biyi Fang, Xiao Zeng, and Mi Zhang. Nestdnn: Resource-aware multi-tenant
    on-device deep learning for continuous mobile vision. MobiCom ’18, New York, NY,
    USA, 2018\. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. Fasi, N.J. Higham, M. Mikaitis, and S. Pranesh. Numerical behavior
    of nvidia tensor cores. PeerJ Computer Science, 7(e330):1–19, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Sean Fox, Seyedramin Rasoulinezhad, Julian Faraone, David Boland, and
    Philip Leong. A block minifloat representation for training deep neural networks.
    In ICLR ’21: International Conference on Learning Representations, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding
    sparse, trainable neural networks. In ICLR ’19: International Conference on Learning
    Representations, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Adi Fuchs and David Wentzlaff. The accelerator wall: Limits of chip specialization.
    In 2019 IEEE International Symposium on High Performance Computer Architecture
    (HPCA), pages 1–14, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Mingyu Gao, Jing Pu, Xuan Yang, Mark Horowitz, and Christos Kozyrakis.
    Tetris: Scalable and efficient neural network acceleration with 3d memory. 45(1):751–764,
    April 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Xitong Gao, Yiren Zhao, Łukasz Dudziak, Robert Mullins, and Cheng zhong
    Xu. Dynamic channel pruning: Feature boosting and suppression. In ICLR ’19: International
    Conference on Learning Representations, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Stuart Geman, Elie Bienenstock, and René Doursat. Neural networks and
    the bias/variance dilemma. Neural Computation, 4(1):1–58, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter
    Jin, Sicheng Zhao, and Kurt Keutzer. Squeezenext: Hardware-aware neural network
    design. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR) Workshops, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Meisam Gholami and Saeed Saeedi. Digital cellular implementation of morris-lecar
    neuron model. In 2015 23rd Iranian Conference on Electrical Engineering, pages
    1235–1239, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and
    Edward Choi. Morphnet: Fast amp; simple resource-constrained structure learning
    of deep networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 1586–1595, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient
    dnns. In Proceedings of the 30th International Conference on Neural Information
    Processing Systems, NIPS’16, page 1387–1395, Red Hook, NY, USA, 2016\. Curran
    Associates Inc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Rishi Raj Gupta and Virender Ranga. Comparative study of different reduced
    precision techniques in deep neural network. In Shailesh Tiwari, Erma Suryani,
    Andrew Keong Ng, K. K. Mishra, and Nitin Singh, editors, Proceedings of International
    Conference on Big Data, Machine Learning and their Applications, pages 123–136,
    Singapore, 2021\. Springer Singapore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
    Deep learning with limited numerical precision. In Proceedings of the 32nd International
    Conference on International Conference on Machine Learning - Volume 37, ICML’15,
    page 1737–1746\. JMLR.org, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Gustafson and Yonemoto. Beating floating point at its own game: Posit
    arithmetic. Supercomput. Front. Innov.: Int. J., 4(2):71–86, June 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Yoo Jeong Ha, Minjae Yoo, Gusang Lee, Soyi Jung, Sae Won Choi, Joongheon
    Kim, and Seehwan Yoo. Spatio-temporal split learning for privacy-preserving medical
    platforms: Case studies with covid-19 ct, x-ray, and cholesterol data. IEEE Access,
    9:121046–121059, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Nastaran Hajinazar, Geraldo F. Oliveira, Sven Gregorio, João Dinis Ferreira,
    Nika Mansouri Ghiasi, Minesh Patel, Mohammed Alser, Saugata Ghose, Juan Gómez-Luna,
    and Onur Mutlu. Simdram: A framework for bit-serial simd processing using dram.
    In Proceedings of the 26th ACM International Conference on Architectural Support
    for Programming Languages and Operating Systems, ASPLOS 2021, page 329–345, New
    York, NY, USA, 2021\. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz,
    and William J. Dally. Eie: Efficient inference engine on compressed deep neural
    network. In Proceedings of the 43rd International Symposium on Computer Architecture,
    ISCA ’16, page 243–254\. IEEE Press, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. In
    ICLR ’16: International Conference on Learning Representations, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights
    and connections for efficient neural network. In C. Cortes, N. D. Lawrence, D. D.
    Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing
    Systems 28, pages 1135–1143\. Curran Associates, Inc., 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), pages 770–778, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok Jeong, Seho Kim, Il Park,
    Mithuna Thottethodi, and T. N. Vijaykumar. Newton: A dram-maker’s accelerator-in-memory
    (aim) architecture for machine learning. In 2020 53rd Annual IEEE/ACM International
    Symposium on Microarchitecture (MICRO), pages 372–385, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep
    neural networks. In 2017 IEEE International Conference on Computer Vision (ICCV),
    pages 1398–1406, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc:
    Automl for model compression and acceleration on mobile devices. In Proceedings
    of the European Conference on Computer Vision (ECCV), September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Kartik Hegde, Jiyong Yu, Rohit Agrawal, Mengjia Yan, Michael Pellauer,
    and Christopher W. Fletcher. Ucnn: Exploiting computational reuse in deep neural
    networks via weight repetition. In Proceedings of the 45th Annual International
    Symposium on Computer Architecture, ISCA ’18, page 674–687\. IEEE Press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge
    in a neural network, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Nhut-Minh Ho and Weng-Fai Wong. Exploiting half precision arithmetic in
    nvidia gpus. In 2017 IEEE High Performance Extreme Computing Conference (HPEC),
    pages 1–7, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    Comput., 9(8):1735–1780, nov 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane
    current and its application to conduction and excitation in nerve. The Journal
    of Physiology, 117(4):500–544, 1952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional
    neural networks for mobile vision applications, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming:
    A data-driven neuron pruning approach towards efficient deep architectures, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Huiyi Hu, Ang Li, Daniele Calandriello, , and Dilan Gorur. One pass imagenet.
    In NeurIPS 2021 Workshop on Imagenet: past, present and future, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected
    convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), pages 2261–2269, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q. Weinberger.
    Condensenet: An efficient densenet using learned group convolutions. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
    Bengio. Quantized neural networks: Training neural networks with low precision
    weights and activations. J. Mach. Learn. Res., 18(1):6869–6898, January 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J.
    Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters
    and <0.5mb model size, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In Francis Bach and
    David Blei, editors, Proceedings of the 32nd International Conference on Machine
    Learning, volume 37 of Proceedings of Machine Learning Research, pages 448–456,
    Lille, France, 07–09 Jul 2015\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] E.M. Izhikevich. Which model to use for cortical spiking neurons? IEEE
    Transactions on Neural Networks, 15(5):1063–1070, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. Dissecting
    the graphcore ipu architecture via microbenchmarking. arXiv preprint arXiv:1912.03413,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das,
    Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka,
    Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training.
    arXiv preprint arXiv:1905.12322, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Dongyoung Kim, Junwhan Ahn, and Sungjoo Yoo. Zena: Zero-aware neural network
    accelerator. IEEE Design Test, 35(1):39–46, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Duckhwan Kim, Jaeha Kung, Sek Chai, Sudhakar Yalamanchili, and Saibal
    Mukhopadhyay. Neurocube: A programmable digital neuromorphic architecture with
    high-density 3d memory. In 2016 ACM/IEEE 43rd Annual International Symposium on
    Computer Architecture (ISCA), pages 380–392, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun
    Shin. Compression of deep convolutional neural networks for fast and low power
    mobile applications. In ICLR ’16: International Conference on Learning Representations,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Urs Köster, Tristan J. Webb, Xin Wang, Marcel Nassar, Arjun K. Bansal,
    William H. Constable, Oğuz H. Elibol, Scott Gray, Stewart Hall, Luke Hornof, Amir
    Khosrowshahi, Carey Kloss, Ruby J. Pai, and Naveen Rao. Flexpoint: An adaptive
    numerical format for efficient training of deep neural networks. In Proceedings
    of the 31st International Conference on Neural Information Processing Systems,
    NeurIPS’17, page 1740–1750, Red Hook, NY, USA, 2017\. Curran Associates Inc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification
    with deep convolutional neural networks. Commun. ACM, 60(6):84–90, May 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] S. Le Masson, A. Laflaquiere, T. Bal, and G. Le Masson. Analog circuits
    for modeling biological neural networks: design and applications. IEEE Transactions
    on Biomedical Engineering, 46(6):638–645, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel. Backpropagation applied to handwritten zip code recognition.
    Neural Computation, 1(4):541–551, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky,
    editor, Advances in Neural Information Processing Systems 2, volume 2\. Morgan-Kaufmann,
    1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] JunKyu Lee, Gregory D. Peterson, Dimitrios S. Nikolopoulos, and Hans Vandierendonck.
    Air: Iterative refinement acceleration using arbitrary dynamic precision. Parallel
    Computing, 97:102663, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] JunKyu Lee, Blesson Varghese, Roger Woods, and Hans Vandierendonck. Tod:
    Transprecise object detection to maximise real-time accuracy on the edge. In IEEE
    International Conference on Fog and Edge Computing, pages 53–60, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Carl Lemaire, Andrew Achkar, and Pierre-Marc Jodoin. Structured pruning
    of neural networks with budget-aware regularization. In 2019 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 9100–9108, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. In Workshop
    on Efficient Methods for Deep Neural Networks in the 30th International Conference
    on Neural Information Processing Systems, NeurIPS’16, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf.
    Pruning filters for efficient convnets. In ICLR ’17: International Conference
    on Learning Representations, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Jiajun Li, Shuhao Jiang, Shijun Gong, Jingya Wu, Junchao Yan, Guihai
    Yan, and Xiaowei Li. Squeezeflow: A sparse cnn accelerator exploiting concise
    convolution rules. IEEE Transactions on Computers, 68(11):1663–1677, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Shuangchen Li, Dimin Niu, Krishna T. Malladi, Hongzhong Zheng, Bob Brennan,
    and Yuan Xie. Drisa: A dram-based reconfigurable in-situ accelerator. In 2017
    50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages
    288–301, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning.
    In Proceedings of the 31st International Conference on Neural Information Processing
    Systems, NeurIPS’17, page 2178–2188, Red Hook, NY, USA, 2017\. Curran Associates
    Inc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Liu, Z. Du, J. Tao, D. Han, T. Luo, Y. Xie, Y. Chen, and T. Chen.
    Cambricon: An instruction set architecture for neural networks. In 2016 ACM/IEEE
    43rd Annual International Symposium on Computer Architecture (ISCA), pages 393–405,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang. Learning efficient
    convolutional networks through network slimming. In 2017 IEEE International Conference
    on Computer Vision (ICCV), pages 2755–2763, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Zhi-Gang Liu and Matthew Mattina. Efficient residue number system based
    winograd convolution. In European Conference on Computer Vision, pages 53–68.
    Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Wei Lou, Lei Xun, Amin Sabet, Jia Bi, Jonathon Hare, and Geoff V. Merrett.
    Dynamic-ofa: Runtime dnn architecture switching for performance scaling on heterogeneous
    embedded platforms. In 2021 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Workshops (CVPRW), pages 3104–3112, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Luo, J. Wu, and W. Lin. Thinet: A filter level pruning method for
    deep neural network compression. In 2017 IEEE International Conference on Computer
    Vision (ICCV), pages 5068–5076, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2:
    Practical guidelines for efficient cnn architecture design. In Proceedings of
    the European Conference on Computer Vision (ECCV), September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Qingyun Ma, Mohammad Rafiqul Haider, Vinaya Lal Shrestha, and Yehia Massoud.
    Bursting hodgkin—huxley model-based ultra-low-power neuromimetic silicon neuron.
    Analog Integr. Circuits Signal Process., 73(1):329–337, October 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Zelda Mariet and Suvrit Sra. Diversity networks: Neural network compression
    using determinantal point processes. In ICLR ’16: International Conference on
    Learning Representations, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Yoshitomo Matsubara, Marco Levorato, and Francesco Restuccia. Split computing
    and early exiting for deep learning applications: Survey and research challenges,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Warren Mcculloch and Walter Pitts. A logical calculus of ideas immanent
    in nervous activity. Bulletin of Mathematical Biophysics, 5:127–147, 1943.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Sally A. McKee. Reflections on the memory wall. In Proceedings of the
    1st Conference on Computing Frontiers, CF ’04, page 162, New York, NY, USA, 2004\.
    Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich K
    Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
    Venkatesh, and Hao Wu. Mixed precision training. In ICLR ’18: International Conference
    on Learning Representations, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Umar Ibrahim Minhas, Lev Mukhanov, Georgios Karakonstantis, Hans Vandierendonck,
    and Roger Woods. Leveraging transprecision computing for machine vision applications
    at the edge. In 2021 IEEE Workshop on Signal Processing Systems (SiPS), pages
    205–210, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Michael A. Nielsen. Neural networks and deep learning, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Eustace Painkras, Luis A. Plana, Jim Garside, Steve Temple, Francesco
    Galluppi, Cameron Patterson, David R. Lester, Andrew D. Brown, and Steve B. Furber.
    Spinnaker: A 1-w 18-core system-on-chip for massively-parallel neural network
    simulation. IEEE Journal of Solid-State Circuits, 48(8):1943–1953, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional
    deep learning for energy-efficient and enhanced pattern recognition. In 2016 Design,
    Automation Test in Europe Conference Exhibition (DATE), pages 475–480, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan
    Venkatesan, Brucek Khailany, Joel Emer, Stephen W. Keckler, and William J. Dally.
    Scnn: An accelerator for compressed-sparse convolutional neural networks. In Proceedings
    of the 44th Annual International Symposium on Computer Architecture, ISCA ’17,
    page 27–40, New York, NY, USA, 2017. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Behrooz Parhami. Computer Arithmetic: Algorithms and Hardware Designs.
    Oxford University Press, New York, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Jongkil Park, Sohmyung Ha, Theodore Yu, Emre Neftci, and Gert Cauwenberghs.
    A 65k-neuron 73-mevents/s 22-pj/event asynchronous micro-pipelined integrate-and-fire
    array transceiver. In 2014 IEEE Biomedical Circuits and Systems Conference (BioCAS)
    Proceedings, pages 675–678, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Z. Qin, Z. Zhang, X. Chen, C. Wang, and Y. Peng. Fd-mobilenet: Improved
    mobilenet with a fast downsampling strategy. In 2018 25th IEEE International Conference
    on Image Processing (ICIP), pages 1363–1367, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
    Xnor-net: Imagenet classification using binary convolutional neural networks.
    In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision
    – ECCV 2016, pages 525–542, Cham, 2016\. Springer International Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Arthur J Redfern, Lijun Zhu, and Molly K Newquist. Bcnn: A binary cnn
    with all matrix ops quantized to 1 bit precision. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 4604–4612, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M. Rhu, M. O’Connor, N. Chatterjee, J. Pool, Y. Kwon, and S. W. Keckler.
    Compressing dma engine: Leveraging activation sparsity for training deep neural
    networks. In 2018 IEEE International Symposium on High Performance Computer Architecture
    (HPCA), pages 78–91, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W.
    Keckler. Vdnn: Virtualized deep neural networks for scalable, memory-efficient
    neural network design. In The 49th Annual IEEE/ACM International Symposium on
    Microarchitecture, MICRO-49\. IEEE Press, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang,
    Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR ’15:
    International Conference on Learning Representations, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] F. Rosenblatt. The perceptron: A probabilistic model for information
    storage and organization in the brain. Psychological Review, 65(6):386–408, 1958.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Sylvain Saighi, Laure Buhry, Yannick Bornat, Gilles N’Kaoua, Jean Tomas,
    and Sylvie Renaud. Adjusting the neurons models in neuromimetic ics using the
    voltage-clamp technique. In 2008 IEEE International Symposium on Circuits and
    Systems, pages 1564–1567, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S. Salamat, M. Imani, S. Gupta, and T. Rosing. Rnsnet: In-memory neural
    network acceleration using residue number system. In 2018 IEEE International Conference
    on Rebooting Computing (ICRC), pages 1–12, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] N. Samimi, M. Kamal, A. Afzali-Kusha, and M. Pedram. Res-dnn: A residue
    number system-based dnn accelerator unit. IEEE Transactions on Circuits and Systems
    I: Regular Papers, 67(2):658–671, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
    Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Florian Scheidegger, Luca Benini, Costas Bekas, and A. Cristiano I. Malossi.
    Constrained deep neural network architecture search for iot devices accounting
    for hardware calibration. In Advances in Neural Information Processing Systems
    32, pages 6056–6066, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Johannes Schemmel, Andreas Grübl, Stephan Hartmann, Alexander Kononov,
    Christian Mayr, Karlheinz Meier, Sebastian Millner, Johannes Partzsch, Stefan
    Schiefer, Stefan Scholze, Rene Schüffny, and Marc-Olivier Schwartz. Live demonstration:
    A scaled-down version of the brainscales wafer-scale neuromorphic system. In 2012
    IEEE International Symposium on Circuits and Systems (ISCAS), pages 702–702, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Catherine D Schuman, Thomas E Potok, Robert M Patton, J Douglas Birdwell,
    Mark E Dean, Garrett S Rose, and James S Plank. A survey of neuromorphic computing
    and neural networks in hardware. arXiv preprint arXiv:1705.06963, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. Shafiee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. P. Strachan,
    M. Hu, R. S. Williams, and V. Srikumar. Isaac: A convolutional neural network
    accelerator with in-situ analog arithmetic in crossbars. In 2016 ACM/IEEE 43rd
    Annual International Symposium on Computer Architecture (ISCA), pages 14–26, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Li Shang, Alireza S. Kaviani, and Kusuma Bathala. Dynamic power consumption
    in virtex™-ii fpga family. In Proceedings of the 2002 ACM/SIGDA Tenth International
    Symposium on Field-programmable Gate Arrays, FPGA ’02, pages 157–164, New York,
    NY, USA, 2002\. ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau,
    Vikas Chandra, and Hadi Esmaeilzadeh. Bit fusion: Bit-level dynamically composable
    architecture for accelerating deep neural networks. In Proceedings of the 45th
    Annual International Symposium on Computer Architecture, ISCA ’18, page 764–775\.
    IEEE Press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Jonghan Shin and C. Koch. Dynamic range and sensitivity adaptation in
    a silicon spiking neuron. IEEE Transactions on Neural Networks, 10(5):1232–1238,
    1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Mario F. Simoni, Gennady S. Cymbalyuk, Michael Elliott Sorensen, Ronald L.
    Calabrese, and Stephen P. DeWeerth. Development of hybrid systems: Interfacing
    a silicon neuron to a leech heart interneuron. In Todd K. Leen, Thomas G. Dietterich,
    and Volker Tresp, editors, Advances in Neural Information Processing Systems 13,
    Papers from Neural Information Processing Systems (NeurIPS) 2000, Denver, CO,
    USA, pages 173–179\. MIT Press, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] M.F. Simoni, G.S. Cymbalyuk, M.E. Sorensen, R.L. Calabrese, and S.P.
    DeWeerth. A multiconductance silicon neuron with biologically matched dynamics.
    IEEE Transactions on Biomedical Engineering, 51(2):342–354, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] M.F. Simoni and S.P. DeWeerth. Adaptation in a vlsi model of a neuron.
    IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing,
    46(7):967–970, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Suraj Srinivas and R. Venkatesh Babu. Data-free parameter pruning for
    deep neural networks. In Proceedings of the British Machine Vision Conference
    (BMVC), pages 31.1–31.12\. BMVA Press, September 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani,
    Vijayalakshmi (Viji) Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan.
    Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks.
    In Advances in Neural Information Processing Systems 32, pages 4900–4909\. Curran
    Associates, Inc., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] V. Sze, Y. Chen, T. Yang, and J. S. Emer. Efficient processing of deep
    neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295–2329,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Thierry Tambe, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi,
    Alexander Rush, David Brooks, and Gu-Yeon Wei. Algorithm-hardware co-design of
    adaptive floating-point encodings for resilient deep learning inference. In 2020
    57th ACM/IEEE Design Automation Conference (DAC), pages 1–6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
    Howard, and Quoc V. Le. Mnasnet: Platform-aware neural architecture search for
    mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), June 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for
    convolutional neural networks. volume 97 of Proceedings of Machine Learning Research,
    pages 6105–6114, Long Beach, California, USA, 09–15 Jun 2019\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and
    efficient object detection. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), June 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Surat Teerapittayanon, Bradley McDanel, and H.T. Kung. Branchynet: Fast
    inference via early exiting from deep neural networks. In 2016 23rd International
    Conference on Pattern Recognition (ICPR), pages 2464–2469, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Surat Teerapittayanon, Bradley McDanel, and H.T. Kung. Distributed deep
    neural networks over the cloud, the edge and end devices. In 2017 IEEE 37th International
    Conference on Distributed Computing Systems (ICDCS), pages 328–339, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Chandra Thapa, M. A. P. Chamikara, Seyit Camtepe, and Lichao Sun. Splitfed:
    When federated learning meets split learning, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Stephen Tridgell, Martin Kumm, Martin Hardieck, David Boland, Duncan
    Moss, Peter Zipf, and Philip H. W. Leong. Unrolling ternary neural networks. ACM
    Trans. Reconfigurable Technol. Syst., 12(4), October 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Robert van de Geijn and Kazushige Goto. BLAS (Basic Linear Algebra Subprograms),
    pages 157–164. Springer US, Boston, MA, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Vincent Vanhoucke, Andrew Senior, and Mark Z. Mao. Improving the speed
    of neural networks on cpus. In Deep Learning and Unsupervised Feature Learning
    Workshop, NeurIPS 2011, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] S. Vogel, M. Liang, A. Guntoro, W. Stechele, and G. Ascheid. Efficient
    hardware acceleration of cnns using logarithmic data representation with arbitrary
    log-base. In 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),
    pages 1–8, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Erwei Wang, James J. Davis, Ruizhe Zhao, Ho-Cheung Ng, Xinyu Niu, Wayne
    Luk, Peter Y. K. Cheung, and George A. Constantinides. Deep neural network approximation
    for custom hardware: Where we’ve been, where we’re going. ACM Comput. Surv., 52(2),
    May 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash
    Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers.
    In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
    editors, Advances in Neural Information Processing Systems 31, pages 7675–7684\.
    Curran Associates, Inc., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Peisong Wang, Xiangyu He, Qiang Chen, Anda Cheng, Qingshan Liu, and Jian
    Cheng. Unsupervised network quantization via fixed-point factorization. IEEE Transactions
    on Neural Networks and Learning Systems, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] P. J. Werbos. Backpropagation through time: what it does and how to do
    it. Proceedings of the IEEE, 78(10):1550–1560, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] J. H. Wilkinson, editor. The Algebraic Eigenvalue Problem. Oxford University
    Press, Inc., New York, NY, USA, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: An insightful
    visual performance model for multicore architectures. Commun. ACM, 52(4):65–76,
    April 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] H. R. Wilson and Jack D. Cowan. Excitatory and inhibitory interactions
    in localized populations of model neurons. Biophysical Journal, 12:1–24, 1972.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming
    Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware
    efficient convnet design via differentiable neural architecture search. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
    June 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius.
    Integer quantization for deep learning inference: Principles and empirical evaluation,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference
    with integers in deep neural networks. arXiv preprint arXiv:1802.04680, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.
    Aggregated residual transformations for deep neural networks. In 2017 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 5987–5995, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Xin Xin, Youtao Zhang, and Jun Yang. Elp2im: Efficient and low power
    bitwise operation processing in dram. In 2020 IEEE International Symposium on
    High Performance Computer Architecture (HPCA), pages 303–314, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang
    Huang, and Xian-sheng Hua. Quantization networks. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 7308–7316, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient
    convolutional neural networks using energy-aware pruning. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler,
    Vivienne Sze, and Hartwig Adam. Netadapt: Platform-aware neural network adaptation
    for mobile applications. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu,
    and Yair Weiss, editors, Computer Vision – ECCV 2018, pages 289–304, Cham, 2018\.
    Springer International Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Yukuan Yang, Lei Deng, Shuang Wu, Tianyi Yan, Yuan Xie, and Guoqi Li.
    Training high-performance and large-scale deep neural networks with full 8-bit
    integers. Neural Networks, 125:70 – 82, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Shuochao Yao, Jinyang Li, Dongxin Liu, Tianshi Wang, Shengzhong Liu,
    Huajie Shao, and Tarek Abdelzaher. Deep compressive offloading: Speeding up neural
    network inference by trading edge computation for network latency. In Proceedings
    of the 18th Conference on Embedded Networked Sensor Systems, SenSys ’20, page
    476–488, New York, NY, USA, 2020. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Reza Yazdani, Marc Riera, Jose-Maria Arnau, and Antonio González. The
    dark side of dnn pruning. In Proceedings of the 45th Annual International Symposium
    on Computer Architecture, ISCA ’18, page 790–801\. IEEE Press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] S. Yin, Z. Jiang, J. Seo, and M. Seok. Xnor-sram: In-memory computing
    sram macro for binary/ternary deep neural networks. IEEE Journal of Solid-State
    Circuits, 55(6):1733–1743, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong Cheng. Image
    classification at supercomputer scale. arXiv preprint arXiv:1811.06992, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable
    neural networks. In ICLR ’19: International Conference on Learning Representations,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] R. Yu, A. Li, C. Chen, J. Lai, V. I. Morariu, X. Han, M. Gao, C. Lin,
    and L. S. Davis. Nisp: Pruning networks using neuron importance score propagation.
    In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
    9194–9203, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional
    networks. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors,
    Computer Vision – ECCV 2014, pages 818–833, Cham, 2014. Springer International
    Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] C. Zhang, P. Patras, and H. Haddadi. Deep learning in mobile and wireless
    networking: A survey. IEEE Communications Surveys Tutorials, 21(3):2224–2287,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufflenet: An extremely efficient
    convolutional neural network for mobile devices. In 2018 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pages 6848–6856, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou,
    Jiaming Guo, Qi Guo, Zidong Du, Tian Zhi, and Yunji Chen. Fixed-point back-propagation
    training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), June 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Yongwei Zhao, Chang Liu, Zidong Du, Qi Guo, Xing Hu, Yimin Zhuang, Zhenxing
    Zhang, Xinkai Song, Wei Li, Xishan Zhang, Ling Li, Zhiwei Xu, and Tianshi Chen.
    Cambricon-q: A hybrid architecture for efficient training. In 2021 ACM/IEEE 48th
    Annual International Symposium on Computer Architecture (ISCA), pages 706–719,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou.
    Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth
    gradients, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary
    quantization. In ICLR ’17: International Conference on Learning Representations,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong
    Li, Xiuqi Yang, and Junjie Yan. Towards unified int8 training for convolutional
    neural network. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), pages 1966–1976, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Barret Zoph and Quoc Le. Neural architecture search with reinforcement
    learning. In ICLR ’17: International Conference on Learning Representations, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
