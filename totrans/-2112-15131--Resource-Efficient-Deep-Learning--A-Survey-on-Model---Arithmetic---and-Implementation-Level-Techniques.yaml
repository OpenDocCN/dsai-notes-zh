- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:48:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:48:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2112.15131] Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2112.15131] 资源高效的深度学习：模型、算术和实现层面的技术综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2112.15131](https://ar5iv.labs.arxiv.org/html/2112.15131)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2112.15131](https://ar5iv.labs.arxiv.org/html/2112.15131)
- en: 'Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源高效的深度学习：模型、算术和实现层面的技术综述
- en: JunKyu Lee
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 郑奎宇
- en: Queen’s University Belfast
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特女王大学
- en: Belfast, Northern Ireland, UK
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特，北爱尔兰，英国
- en: junkyu.lee@qub.ac.uk
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: junkyu.lee@qub.ac.uk
- en: '&Lev Mukhanov'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '&列夫·穆哈诺夫'
- en: Queen’s University Belfast
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特女王大学
- en: Belfast, Northern Ireland, UK
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特，北爱尔兰，英国
- en: l.mukhanov@qub.ac.uk
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: l.mukhanov@qub.ac.uk
- en: '&Amir Sabbagh Molahosseini'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '&阿米尔·萨巴赫·莫拉霍塞尼'
- en: Queen’s University Belfast
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特女王大学
- en: Belfast, Northern Ireland, UK
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特，北爱尔兰，英国
- en: a.sabbaghmolahosseini@qub.ac.uk
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: a.sabbaghmolahosseini@qub.ac.uk
- en: '&Umar Minhas'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '&乌玛尔·敏哈斯'
- en: Queen’s University Belfast
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特女王大学
- en: Belfast, Northern Ireland, UK
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特，北爱尔兰，英国
- en: u.minhas@qub.ac.uk
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: u.minhas@qub.ac.uk
- en: '&Yang Hua'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '&杨华'
- en: Queen’s University Belfast
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特女王大学
- en: Belfast, Northern Ireland, UK
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特，北爱尔兰，英国
- en: y.hua@qub.ac.uk
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: y.hua@qub.ac.uk
- en: '&Jesus Martinez del Rincon'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '&耶稣·马丁内斯·德尔·林孔'
- en: Queen’s University Belfast
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特女王大学
- en: Belfast, Northern Ireland, UK
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特，北爱尔兰，英国
- en: j.martinez-del-rincon@qub.ac.uk
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: j.martinez-del-rincon@qub.ac.uk
- en: '&Kiril Dichev'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '&基里尔·迪切夫'
- en: University of Cambridge
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 剑桥大学
- en: Cambridge, England, UK
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 剑桥，英格兰，英国
- en: kiril.dichev@gmail.com &Cheol-Ho Hong
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: kiril.dichev@gmail.com &洪哲浩
- en: Chung-Ang University
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 中央大学
- en: Seoul, South Korea
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首尔，韩国
- en: cheolhohong@cau.ac.kr
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: cheolhohong@cau.ac.kr
- en: '&Hans Vandierendonck'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '&汉斯·范迪伦登克'
- en: Queen’s University Belfast
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特女王大学
- en: Belfast, Northern Ireland, UK
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔法斯特，北爱尔兰，英国
- en: h.vandierendonck@qub.ac.uk Corresponding Author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: h.vandierendonck@qub.ac.uk 通讯作者
- en: Abstract
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning is pervasive in our daily life, including self-driving cars, virtual
    assistants, social network services, healthcare services, face recognition, etc.
    However, deep neural networks demand substantial compute resources during training
    and inference. The machine learning community has mainly focused on model-level
    optimizations such as architectural compression of deep learning models, while
    the system community has focused on implementation-level optimization. In between,
    various arithmetic-level optimization techniques have been proposed in the arithmetic
    community. This article provides a survey on resource-efficient deep learning
    techniques in terms of model-, arithmetic-, and implementation-level techniques
    and identifies the research gaps for resource-efficient deep learning techniques
    across the three different level techniques. Our survey clarifies the influence
    from higher to lower-level techniques based on our resource-efficiency metric
    definition and discusses the future trend for resource-efficient deep learning
    research.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在我们的日常生活中无处不在，包括自动驾驶汽车、虚拟助手、社交网络服务、医疗服务、面部识别等。然而，深度神经网络在训练和推理过程中需要大量的计算资源。机器学习界主要关注模型层面的优化，例如深度学习模型的架构压缩，而系统界则侧重于实现层面的优化。在这之间，算术社区提出了各种算术层面的优化技术。本文提供了关于资源高效的深度学习技术的综述，涉及模型层面、算术层面和实现层面的技术，并识别了这三种不同层次技术中资源高效深度学习技术的研究空白。我们的综述根据资源效率指标定义阐明了从高层到低层技术的影响，并讨论了资源高效深度学习研究的未来趋势。
- en: '*Keywords* deep learning, neural networks, resource efficiency, arithmetic
    utilization'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 深度学习，神经网络，资源效率，算术利用'
- en: 1 Introduction
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent improvements in network and storage devices have provided the machine
    learning community with the opportunity to utilize immense data sources, leading
    to the golden age of AI and deep learning [[22](#bib.bib22)]. Since modern deep
    neural networks (DNNs) require considerable computing resources and are deployed
    in a variety of compute devices, ranging from high-end servers to mobile devices
    with limited computational resources, there is a strong need to realize economical
    DNNs that fit within the resource constraints [[118](#bib.bib118), [150](#bib.bib150),
    [151](#bib.bib151)]. Resource-efficient deep learning research has vividly been
    carried out independently in various research communities including the machine
    learning, computer arithmetic, and computing system communities. Recently, DeepMind
    proposed the resource-efficient deep learning benchmark metric which is the accuracy
    along with the required memory footprint and number of operations [[78](#bib.bib78)].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 网络和存储设备的最新改进为机器学习社区提供了利用大量数据源的机会，从而迎来了人工智能和深度学习的黄金时代[[22](#bib.bib22)]。由于现代深度神经网络（DNNs）需要大量计算资源，并且部署在各种计算设备上，从高端服务器到具有有限计算资源的移动设备，因此迫切需要实现符合资源限制的经济型DNN[[118](#bib.bib118),
    [150](#bib.bib150), [151](#bib.bib151)]。资源高效深度学习研究在包括机器学习、计算机算术和计算系统等多个研究社区中得到了生动开展。最近，DeepMind提出了资源高效深度学习基准指标，即准确率以及所需的内存占用和操作数量[[78](#bib.bib78)]。
- en: 'With this regard, this article surveys resource-efficient techniques for deep
    learning based on the three-level categorization: the model-, arithmetic-, and
    implementation-level techniques along with various resource efficiency metrics
    as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").
    Our resource efficiency metrics include the accuracy per parameter, operation,
    memory footprint, core utilization, memory access, and Joule. For the resource-efficiency
    comparison between the baseline DNN and a DNN utilizing resource-efficient techniques,
    the accuracy should be equivalent between the two DNNs. In other words, it is
    not fair to compare the resource efficiency between a DNN producing a high accuracy
    and a DNN producing a low accuracy since the resource efficiency is significantly
    higher in a low performing DNN based on our resource metrics. We categorize the
    resource-efficient techniques into the model-level resource-efficient techniques
    if they compress the DNN model sizes; the arithmetic-level resource-efficient
    techniques if they utilize reduced precision arithmetic and/or customized arithmetic
    rules; the implementation-level resource-efficient techniques if they apply hardware
    optimization techniques to the DNNs (e.g., locating local memory near to processing
    elements) to improve physical resource efficiency such as the accuracy per compute
    resource and per Joule.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '在这方面，本文调查了基于三层分类的资源高效深度学习技术：模型层、算术层和实现层技术，以及各种资源效率指标，如图[1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques")所示。我们的资源效率指标包括每个参数的准确率、操作、内存占用、核心利用率、内存访问和焦耳。对于基准DNN和使用资源高效技术的DNN之间的资源效率比较，两个DNN的准确率应当相当。换句话说，将一个高准确率的DNN与一个低准确率的DNN进行资源效率比较是不公平的，因为根据我们的资源指标，低性能DNN的资源效率明显更高。我们将资源高效技术分类为模型层资源高效技术（如果它们压缩DNN模型大小）；算术层资源高效技术（如果它们利用了减少精度的算术和/或自定义算术规则）；实现层资源高效技术（如果它们对DNN应用了硬件优化技术，例如将局部内存靠近处理元素）以提高物理资源效率，如每个计算资源和每个焦耳的准确率。'
- en: 'In Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient Deep Learning:
    A Survey on Model-, Arithmetic-, and Implementation-Level Techniques"), Convolutional
    Neural Networks (CNNs) can be considered as a resource-efficient technique since
    they improve the accuracy per parameter, per operation, and per memory footprint,
    compared to fully connected neural networks. The resource-efficiency from CNNs
    can be further improved by applying the model-, arithmetic-, and implementation-level
    techniques. The model- and arithmetic-level techniques can affect the accuracy
    since they affect either the DNN model structure or the arithmetic rule, while
    the implementation-level techniques generally do not affect the accuracy. The
    model-level techniques mostly contribute to improving abstract resource efficiency,
    while the implementation-level techniques contribute to improve physical resource
    efficiency. Without careful consideration at the intersection between the model-
    and the implementation-level techniques, a DNN model compressed by the model-level
    techniques might require significant runtime compute resources, incurring longer
    training time and inference latency than the original model [[108](#bib.bib108),
    [31](#bib.bib31)]. Thus, to optimize the performance and energy efficiency on
    a particular hardware, it is essential to consider the joint effect of the model,
    arithmetic and implementation-level optimizations. Our survey focuses on the three
    different level resource-efficient techniques for CNN architectures, since CNN
    is one of the most widely used deep learning architectures [[91](#bib.bib91)].'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '在图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient Deep Learning:
    A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")中，卷积神经网络（CNNs）可以被视为一种资源高效的技术，因为与全连接神经网络相比，它们在每个参数、每个操作和每个内存占用上都提高了准确性。通过应用模型级、算术级和实现级技术，可以进一步提高CNNs的资源效率。模型级和算术级技术可能会影响准确性，因为它们影响DNN模型结构或算术规则，而实现级技术一般不会影响准确性。模型级技术主要有助于提高抽象资源效率，而实现级技术有助于提高物理资源效率。如果不仔细考虑模型级和实现级技术的交汇点，通过模型级技术压缩的DNN模型可能需要大量的运行时计算资源，导致比原始模型更长的训练时间和推理延迟[[108](#bib.bib108),
    [31](#bib.bib31)]。因此，为了优化特定硬件上的性能和能效，必须考虑模型、算术和实现级优化的联合作用。我们的调查重点关注CNN架构中三种不同级别的资源高效技术，因为CNN是最广泛使用的深度学习架构之一[[91](#bib.bib91)]。'
- en: '![Refer to caption](img/f4dbfa19be843d95900334910a2bf262.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f4dbfa19be843d95900334910a2bf262.png)'
- en: 'Figure 1: Survey on resource-efficient deep learning techniques based on resource
    efficiency metrics.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于资源效率指标的资源高效深度学习技术调查。
- en: Related survey works are as follows. Sze et al. [[145](#bib.bib145)] provided
    a comprehensive tutorial and survey towards efficient processing of DNNs, discussing
    DNN architectures, software frameworks (e.g., PyTorch, TensorFlow, Keras, etc.),
    and the implementation methods optimizing Multiply-and-Accumulate Computations
    (MACs) of DNNs on given compute platforms. Cheng et al. [[35](#bib.bib35)] conducted
    a survey on the model compression techniques including pruning, low-rank factorization,
    compact convolution, and knowledge distillation. Deng et al. [[42](#bib.bib42)]
    discussed joint model-compression methods which combined multiple model-level
    compression techniques, and their efficient implementation on particular computing
    platforms. Wang et al. [[157](#bib.bib157)] provided a survey on custom hardware
    implementations of DNNs and evaluated their performances using the Roofline model
    of [[162](#bib.bib162)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相关调查工作如下。Sze等人[[145](#bib.bib145)]提供了一个关于高效处理DNN的全面教程和调查，讨论了DNN架构、软件框架（例如，PyTorch、TensorFlow、Keras等）以及优化DNN的乘加计算（MACs）的实现方法。Cheng等人[[35](#bib.bib35)]对模型压缩技术进行了调查，包括剪枝、低秩分解、紧凑卷积和知识蒸馏。Deng等人[[42](#bib.bib42)]讨论了结合多种模型级压缩技术的联合模型压缩方法及其在特定计算平台上的高效实现。Wang等人[[157](#bib.bib157)]提供了关于DNN的定制硬件实现的调查，并使用[[162](#bib.bib162)]的Roofline模型评估了它们的性能。
- en: 'Unlike the previous survey works, we conduct a comprehensive survey on resource-efficient
    deep learning techniques in terms of the model-, arithmetic-, and implementation-level
    techniques by clarifying which resource-efficiency can be improved with particular
    techniques according to our resource-efficiency metrics as defined in Section [2.2](#S2.SS2
    "2.2 Resource Efficiency Metrics for Deep Learning ‣ 2 Background on Deep Learning
    and Resource-Efficiency ‣ Resource-Efficient Deep Learning: A Survey on Model-,
    Arithmetic-, and Implementation-Level Techniques"). Such clarification would provide
    machine learning engineers, computer arithmetic designers, software developers,
    and hardware manufacturers with useful information to improve particular resource
    efficiency for their DNN applications. Besides, since we notice that fast wireless
    communication and edge computing development affects deep learning applications
    [[180](#bib.bib180)], our survey also includes cutting-edge resource-efficient
    techniques for distributed AI such as early exiting techniques [[150](#bib.bib150),
    [151](#bib.bib151)]. The holistic and multi-facet view for resource-efficient
    techniques for deep learning from our survey would allow for a better understanding
    of the available techniques and, as consequence, a better global optimization,
    compared to previous survey works. The main contributions of our paper include:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '与以往的综述工作不同，我们通过明确哪些资源效率可以通过特定技术得到改善，基于我们在第[2.2节](#S2.SS2 "2.2 Resource Efficiency
    Metrics for Deep Learning ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques")中定义的资源效率指标，对模型层面、算术层面和实现层面的资源高效深度学习技术进行了全面综述。这种澄清将为机器学习工程师、计算机算术设计师、软件开发人员和硬件制造商提供有用的信息，以改善其DNN应用中的特定资源效率。此外，鉴于我们注意到快速无线通信和边缘计算的发展影响了深度学习应用[[180](#bib.bib180)]，我们的综述还包括了用于分布式AI的前沿资源高效技术，如早期退出技术[[150](#bib.bib150),
    [151](#bib.bib151)]。与以往的综述工作相比，我们的综述提供了对深度学习资源高效技术的整体和多方面的视角，使得对现有技术有更好的理解，从而实现更好的全球优化。本文的主要贡献包括：'
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This paper first provides a comprehensive survey coverage of the recent resource-efficient
    techniques for DNNs in terms of the model-, arithmetic-, and implementation-level
    techniques.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文首先提供了对深度神经网络（DNN）在模型层面、算术层面和实现层面的近期资源高效技术的全面综述。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, our paper is the first to provide comprehensive
    survey on arithmetic-level utilization techniques for deep learning.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，本文是首个对深度学习的算术级利用技术进行全面综述的论文。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This paper utilizes multiple resource efficiency metrics to clarify which resource
    efficiency metrics each technique improves.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文利用多个资源效率指标来阐明每种技术提高了哪些资源效率指标。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'This paper provides the influence of resource-efficient deep learning techniques
    from higher to lower level techniques (refer to Fig. [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques")).'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '本文展示了从高层到低层技术的资源高效深度学习技术的影响（参见图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")）。'
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss the future trend for the resource-efficient deep learning techniques.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了资源高效深度学习技术的未来趋势。
- en: 'We discuss our resource efficiency metrics for deep learning in Section [2](#S2
    "2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques"),
    the model-level resource-efficient techniques in Section [3](#S3 "3 Model-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques"), the arithmetic-level
    techniques in Section [4](#S4 "4 Arithmetic-Level resource-efficient Techniques
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques"), the implementation-level techniques in Section [5](#S5 "5 Implementation-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques"), the influences between
    different-level techniques in Section [6](#S6 "6 Interrelated Influences ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques"),
    the future trend in Section [7](#S7 "7 Future Trend for Resource-Efficient Deep
    Learning ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques"), and conclusion in Section [8](#S8 "8 Conclusion
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques"). Our paper excludes higher-level training procedure manipulation
    techniques such as one-pass ImageNet [[78](#bib.bib78)], bag of freebies [[20](#bib.bib20)],
    data augmentation, etc.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将在第[2](#S2 "2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")节讨论深度学习的资源效率指标，在第[3](#S3
    "3 Model-Level Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning:
    A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")节讨论模型级的资源效率技术，在第[4](#S4
    "4 Arithmetic-Level resource-efficient Techniques ‣ Resource-Efficient Deep Learning:
    A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")节讨论算术级的技术，在第[5](#S5
    "5 Implementation-Level Resource-Efficient Techniques ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")节讨论实施级的技术，在第[6](#S6
    "6 Interrelated Influences ‣ Resource-Efficient Deep Learning: A Survey on Model-,
    Arithmetic-, and Implementation-Level Techniques")节讨论不同级别技术之间的相互影响，在第[7](#S7 "7
    Future Trend for Resource-Efficient Deep Learning ‣ Resource-Efficient Deep Learning:
    A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")节讨论未来趋势，第[8](#S8
    "8 Conclusion ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques")节为结论。我们的论文不包括更高级的训练过程操作技术，如一次性ImageNet [[78](#bib.bib78)]、免费算法包
    [[20](#bib.bib20)]、数据增强等。'
- en: 2 Background on Deep Learning and Resource-Efficiency
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度学习背景与资源效率
- en: This section describes deep learning overview and resource efficiency metric,
    as preparatory to the description of resource-efficient techniques via the three
    different levels.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了深度学习概述和资源效率指标，为通过三种不同级别描述资源效率技术做准备。
- en: 2.1 Deep Learning Overview
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 深度学习概述
- en: Deep learning is defined as “learning multiple levels of representation” [[17](#bib.bib17)]
    and often utilizes DNNs to learn the multiple levels of representation. DNNs are
    trained using the training data set, and their prediction accuracy is evaluated
    using the test dataset [[5](#bib.bib5)]. In this section, we describe the perceptron
    model (i.e., artificial neuron) first and then DNNs later.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习被定义为“学习多层次的表示”[[17](#bib.bib17)]，并且通常利用DNNs来学习这些多层次的表示。DNNs通过训练数据集进行训练，并使用测试数据集来评估它们的预测准确性[[5](#bib.bib5)]。在这一节中，我们首先描述感知机模型（即人工神经元），然后再讨论DNNs。
- en: '2.1.1 Perceptron Model:'
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 感知机模型：
- en: 'The McCulloch and Pitts’s neuron (a.k.a. M-P neuron) [[112](#bib.bib112)],
    proposed in 1943, was a system mimicking the neuron in the nervous system, receiving
    multiple binary inputs and producing one binary output based on a threshold. Inspired
    by the work of [[112](#bib.bib112)], Rosenblatt [[128](#bib.bib128)] proposed
    the “perceptron” model consisting of multiple weights, a summation, and an activation
    function as shown in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1.1 Perceptron Model: ‣ 2.1
    Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques").(a).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'McCulloch和Pitts的神经元（又名M-P神经元）[[112](#bib.bib112)]，在1943年提出，是一个模仿神经系统中神经元的系统，它接收多个二进制输入，并基于阈值产生一个二进制输出。受到[[112](#bib.bib112)]工作的启发，Rosenblatt
    [[128](#bib.bib128)] 提出了“感知机”模型，该模型由多个权重、一个加和和一个激活函数组成，如图[2](#S2.F2 "Figure 2
    ‣ 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep
    Learning and Resource-Efficiency ‣ Resource-Efficient Deep Learning: A Survey
    on Model-, Arithmetic-, and Implementation-Level Techniques") (a)所示。'
- en: '![Refer to caption](img/8b5d86588b45eebc84a58efedce56962.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8b5d86588b45eebc84a58efedce56962.png)'
- en: 'Figure 2: Perceptron and neural network model.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：感知机和神经网络模型。
- en: 'Eq. ([1](#S2.E1 "In 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview ‣
    2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques"))
    describes a perceptron’s firing activity $y_{out}$ using the inputs $x_{i}$ associated
    with their weights $w_{i}$, where the $i$ represents an index to indicate one
    of multiple inputs.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq. ([1](#S2.E1 "在 2.1.1 感知机模型: ‣ 2.1 深度学习概述 ‣ 2 深度学习背景及资源效率 ‣ 资源高效深度学习：关于模型、算术和实现层级技术的调查"))
    描述了感知机的发射活动 $y_{out}$，使用与其权重 $w_{i}$ 相关联的输入 $x_{i}$，其中 $i$ 表示一个索引，指示多个输入中的一个。'
- en: '|  | $y_{out}=\begin{cases}1,&amp;\text{if }(\Sigma_{i=1}^{n_{in}}w_{i}\times
    x_{i}>threshold)\text{ or }(\Sigma_{i=1}^{n_{in}}w_{i}\times x_{i}+bias>0)\\ 0,&amp;\text{if
    }(\Sigma_{i=1}^{n_{in}}w_{i}\times x_{i}\leq threshold)\text{ or }(\Sigma_{i=1}^{n_{in}}w_{i}\times
    x_{i}+bias\leq 0),\end{cases}$ |  | (1) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{out}=\begin{cases}1,&amp;\text{如果 }(\Sigma_{i=1}^{n_{in}}w_{i}\times
    x_{i}>threshold)\text{ 或 }(\Sigma_{i=1}^{n_{in}}w_{i}\times x_{i}+bias>0)\\ 0,&amp;\text{如果
    }(\Sigma_{i=1}^{n_{in}}w_{i}\times x_{i}\leq threshold)\text{ 或 }(\Sigma_{i=1}^{n_{in}}w_{i}\times
    x_{i}+bias\leq 0),\end{cases}$ |  | (1) |'
- en: 'where $n_{in}$ is the number of the inputs. The function that determines the
    firing activity is referred to as the activation function, and the bias is in
    proportion to the probability of the firing activation [[116](#bib.bib116)]. Since
    single perceptron model is suitable only for linearly separable problems, a Multi-Layer
    Perceptron (MLP) model can be used for non-linearly separable problems as shown
    in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview
    ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").(b),
    where $w_{j,k,(i)}$ represents a weight linking the $j$^(th) neuron in the $(i-1)$^(th)
    layer to the $k$^(th) neuron in the $i$^(th) layer. The signal $s_{j,(i)}$ in
    Fig. [2](#S2.F2 "Figure 2 ‣ 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview
    ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")
    follows Eq. ([2](#S2.E2 "In 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview
    ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $n_{in}$ 是输入的数量。决定发射活动的函数被称为激活函数，偏差与发射激活的概率成比例 [[116](#bib.bib116)]。由于单层感知机模型仅适用于线性可分问题，对于非线性可分问题，可以使用多层感知机
    (MLP) 模型，如图 [2](#S2.F2 "图 2 ‣ 2.1.1 感知机模型: ‣ 2.1 深度学习概述 ‣ 2 深度学习背景及资源效率 ‣ 资源高效深度学习：关于模型、算术和实现层级技术的调查")
    所示。(b)，其中 $w_{j,k,(i)}$ 表示将 $(i-1)$ 层的第 $j$ 个神经元与第 $i$ 层的第 $k$ 个神经元连接的权重。图 [2](#S2.F2
    "图 2 ‣ 2.1.1 感知机模型: ‣ 2.1 深度学习概述 ‣ 2 深度学习背景及资源效率 ‣ 资源高效深度学习：关于模型、算术和实现层级技术的调查")
    中的信号 $s_{j,(i)}$ 遵循 Eq. ([2](#S2.E2 "在 2.1.1 感知机模型: ‣ 2.1 深度学习概述 ‣ 2 深度学习背景及资源效率
    ‣ 资源高效深度学习：关于模型、算术和实现层级技术的调查"))：'
- en: '|  | $s_{j,(l)}=\Sigma_{i=1}^{n_{in}^{(l-1)}}(w_{i,j,(l)}\times x_{i,(l-1)})=(\mathbf{W}_{(l)}^{T}\mathbf{x}_{(l-1)})_{j},$
    |  | (2) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{j,(l)}=\Sigma_{i=1}^{n_{in}^{(l-1)}}(w_{i,j,(l)}\times x_{i,(l-1)})=(\mathbf{W}_{(l)}^{T}\mathbf{x}_{(l-1)})_{j},$
    |  | (2) |'
- en: 'and $x_{j,(l)}=\theta_{P}(s_{j,(l)})$, where $\theta_{P}(s)$ is a perceptron’s
    activation function that follows Eq. ([1](#S2.E1 "In 2.1.1 Perceptron Model: ‣
    2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques")) (i.e., step function), and $\mathbf{W}_{(l)}$ consists of the matrix
    elements, $w_{i,j,(l)}$s, for the $i^{th}$ row and the $j^{th}$ column.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '并且 $x_{j,(l)}=\theta_{P}(s_{j,(l)})$，其中 $\theta_{P}(s)$ 是遵循 Eq. ([1](#S2.E1
    "在 2.1.1 感知机模型: ‣ 2.1 深度学习概述 ‣ 2 深度学习背景及资源效率 ‣ 资源高效深度学习：关于模型、算术和实现层级技术的调查")) (即，阶跃函数)
    的感知机激活函数，$\mathbf{W}_{(l)}$ 包含矩阵元素 $w_{i,j,(l)}$，用于第 $i^{th}$ 行和第 $j^{th}$ 列。'
- en: '2.1.2 Deep Neural Network:'
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 深度神经网络：
- en: 'Since it requires tremendous efforts for human to optimize MLPs manually, neural
    networks that adopts a soft threshold activation function $\theta_{N}$ (e.g.,
    $sigmoid$, $ReLU$, etc.) were proposed to train the weights according to the training
    data [[163](#bib.bib163), [160](#bib.bib160)]. [[116](#bib.bib116)] notice that
    neural network is sometimes interchangeably used with MLP. For clarity, we name
    an algorithm as an MLP if it utilizes a step function for its activation functions
    and as a neural network if it utilizes a soft threshold function. In Fig. [2](#S2.F2
    "Figure 2 ‣ 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview ‣ 2 Background
    on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep Learning: A
    Survey on Model-, Arithmetic-, and Implementation-Level Techniques").(b), the
    output from the $i$^(th) neuron at the $l$^(th) layer in a neural network employing
    a soft threshold activation function, $\theta_{N}(\cdot)$, can be represented
    as Eq. ([3](#S2.E3 "In 2.1.2 Deep Neural Network: ‣ 2.1 Deep Learning Overview
    ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '由于人工优化多层感知机 (MLP) 需要巨大的努力，因此提出了采用软阈值激活函数 $\theta_{N}$（例如，$sigmoid$、$ReLU$ 等）的神经网络来根据训练数据训练权重[[163](#bib.bib163),
    [160](#bib.bib160)]。[[116](#bib.bib116)] 注意到神经网络有时与 MLP 互换使用。为清晰起见，我们将利用阶跃函数作为激活函数的算法命名为
    MLP，将利用软阈值函数的算法命名为神经网络。在图 [2](#S2.F2 "Figure 2 ‣ 2.1.1 Perceptron Model: ‣ 2.1
    Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques").(b) 中，使用软阈值激活函数 $\theta_{N}(\cdot)$ 的神经网络中第 $i$^(th) 个神经元在第 $l$^(th)
    层的输出可以表示为公式 ([3](#S2.E3 "In 2.1.2 Deep Neural Network: ‣ 2.1 Deep Learning Overview
    ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")):'
- en: '|  | $x_{i,(l)}=\theta_{N}(s_{i,(l)}).$ |  | (3) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{i,(l)}=\theta_{N}(s_{i,(l)}).$ |  | (3) |'
- en: 'A neural network allows the weights and the biases to be trained using the
    backpropagation [[5](#bib.bib5)]. A neural network model is often referred to
    as a feed-forward model in that the weights always link the neurons in the current
    layer to the neurons in the very next layer. In a neural network, the middle layers
    located between the input and output layer, are often referred to as hidden layers
    (e.g., two hidden layers in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1.1 Perceptron Model:
    ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques").(b)). A neural network with multiple hidden layers is referred to
    as a DNN [[145](#bib.bib145)].'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '神经网络允许使用反向传播[[5](#bib.bib5)]对权重和偏置进行训练。神经网络模型通常被称为前馈模型，因为权重总是将当前层的神经元与下一层的神经元连接。在神经网络中，位于输入层和输出层之间的中间层，通常被称为隐藏层（例如，图 [2](#S2.F2
    "Figure 2 ‣ 2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview ‣ 2 Background
    on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep Learning: A
    Survey on Model-, Arithmetic-, and Implementation-Level Techniques").(b)）. 具有多个隐藏层的神经网络被称为深度神经网络
    (DNN) [[145](#bib.bib145)]。'
- en: '2.1.3 Training - Backpropagation:'
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 训练 - 反向传播：
- en: 'In the forward pass, the neuron outputs are propagated in forward direction
    based on the matrix-vector multiplications as shown in Eq. ([2](#S2.E2 "In 2.1.1
    Perceptron Model: ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning
    and Resource-Efficiency ‣ Resource-Efficient Deep Learning: A Survey on Model-,
    Arithmetic-, and Implementation-Level Techniques")). Likewise, the weights and
    the biases can be trained in backward direction using matrix-vector multiplications.
    This method is called as the backpropagation. The backpropagation method consists
    of the three steps, allowing a gradient descent algorithm to be implemented efficiently
    on computers. It finds the activation gradients, $\delta_{j,(l)}$s (i.e., the
    gradients with respect to all the signals, $s_{j,(l)}$s, in Eq ([2](#S2.E2 "In
    2.1.1 Perceptron Model: ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning
    and Resource-Efficiency ‣ Resource-Efficient Deep Learning: A Survey on Model-,
    Arithmetic-, and Implementation-Level Techniques"))), in step 1, finds the weight
    gradients (i.e., the gradients with respect to all the weights) using the activation
    gradients in step 2, and finally updates the weights using the weight gradients
    in step 3\. All $\delta_{j,(l-1)}$s are found in backward direction using the
    matrix-vector multiplications by replacing $\mathbf{W}_{(l)}^{T}$ to $\mathbf{W}_{(l)}$
    and $x_{j,(l-1)}$ to $\delta_{j,(l)}$ in Eq. ([3](#S2.E3 "In 2.1.2 Deep Neural
    Network: ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques")). After all activation gradients have been found, the weight gradients
    can be found. Finally, the weights are updated using the weight gradients. The
    backpropagation requires additional storage to store all the weights and activation
    values. Once a DNN is trained, the DNN is used for the inference task using the
    trained weights. Please refer to [[5](#bib.bib5)] for the further details for
    the backprogation method. After a DNN being trained, the DNN’s accuracy is evaluated
    using the validation dataset which is unseen from the training.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传递中，神经元输出基于矩阵-向量乘法向前传播，如公式 ([2](#S2.E2 "在2.1.1感知机模型：‣ 2.1 深度学习概述 ‣ 2 背景与资源效率
    ‣ 资源高效的深度学习：模型、算术和实现级别技术的调查")) 所示。类似地，权重和偏置可以通过矩阵-向量乘法在反向传播方向上进行训练。这种方法称为反向传播。反向传播方法包括三个步骤，使得梯度下降算法可以在计算机上高效实现。它在步骤1中找到激活梯度，$\delta_{j,(l)}$（即，相对于公式
    ([2](#S2.E2 "在2.1.1感知机模型：‣ 2.1 深度学习概述 ‣ 2 背景与资源效率 ‣ 资源高效的深度学习：模型、算术和实现级别技术的调查"))
    中的所有信号 $s_{j,(l)}$ 的梯度），在步骤2中利用激活梯度找到权重梯度（即，相对于所有权重的梯度），最后在步骤3中使用权重梯度更新权重。所有 $\delta_{j,(l-1)}$
    都通过矩阵-向量乘法在反向传播方向上找到，将 $\mathbf{W}_{(l)}^{T}$ 替换为 $\mathbf{W}_{(l)}$ 和 $x_{j,(l-1)}$
    替换为 $\delta_{j,(l)}$ 在公式 ([3](#S2.E3 "在2.1.2深度神经网络：‣ 2.1 深度学习概述 ‣ 2 背景与资源效率 ‣
    资源高效的深度学习：模型、算术和实现级别技术的调查")) 中。找到所有激活梯度后，便可以找到权重梯度。最后，使用权重梯度更新权重。反向传播需要额外的存储来存放所有权重和激活值。一旦深度神经网络（DNN）训练完成，就会使用训练好的权重进行推理任务。有关反向传播方法的更多详细信息，请参考
    [[5](#bib.bib5)]。在DNN训练完成后，使用未见过的验证数据集来评估DNN的准确性。
- en: '2.1.4 Convolutional Neural Network:'
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 卷积神经网络：
- en: 'Since CNN is one of the most successful and widely used deep learning architectures
    [[91](#bib.bib91)], we exemplify CNN as a representative deep learning architecture.
    CNN employs multiple convolutional layers, and each convolutional layer utilizes
    multiple filters to perform convolutions independently with respect to each filter
    as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional Neural Network: ‣
    2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques"), where a filter at a convolutional layer consists of as many kernels
    as the number of the channels at the input layer (e.g., 3 kernels per filter in
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional Neural Network: ‣ 2.1 Deep Learning
    Overview ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")).
    For example, each $3\times 3$ filter has 9 weight parameters and slides from the
    top-left to the bottom-right position, generating $4$ output values with respect
    to each position (e.g., top-left, top-right, bottom-left, and bottom-right position)
    in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional Neural Network: ‣ 2.1 Deep
    Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").
    The outputs from the convolutions are often called feature maps and are fed into
    activation functions. Modern CNNs such as ResNet [[67](#bib.bib67)] often employ
    a batch normalization layer [[83](#bib.bib83)] between the convolutional layer
    and the ReLU layer to improve the accuracy.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 CNN 是最成功和广泛使用的深度学习架构之一 [[91](#bib.bib91)]，我们以 CNN 为代表性深度学习架构进行示例。CNN 采用多个卷积层，每个卷积层使用多个滤波器独立地进行卷积，如图
    [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional Neural Network: ‣ 2.1 Deep Learning
    Overview ‣ 2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")
    所示，其中卷积层的一个滤波器包含与输入层通道数量相同的多个卷积核（例如，图 [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional
    Neural Network: ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and
    Resource-Efficiency ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques") 中每个滤波器有 3 个卷积核）。例如，每个 $3\times 3$ 滤波器有 9
    个权重参数，并从左上角滑动到右下角位置，在图 [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional Neural Network:
    ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques") 中，生成 4 个输出值与每个位置相关（例如，左上角、右上角、左下角和右下角位置）。卷积的输出通常称为特征图，并输入到激活函数中。现代
    CNN，例如 ResNet [[67](#bib.bib67)]，通常在卷积层和 ReLU 层之间采用批量归一化层 [[83](#bib.bib83)] 以提高准确性。'
- en: '![Refer to caption](img/89f5521a9fb6c0991e23bdbc1a342752.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/89f5521a9fb6c0991e23bdbc1a342752.png)'
- en: 'Figure 3: Convolution operations in a CNN.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：CNN 中的卷积操作。
- en: The CNN is a resource-efficient deep learning architecture in terms of the accuracy
    per parameter and per operation by leveraging the two properties, local receptive
    field and shared weights [[93](#bib.bib93)]. For example, performing convolutions
    using multiple small kernels extracts multiple local receptive features from the
    input image during training, and each kernel contains some meaningful pattern
    from the input image after being trained [[179](#bib.bib179)]. Thus, CNN utilizes
    much fewer weights than fully connected DNN, since the kernel’s height and weight
    are generally much smaller than the height and the width at the input layer, leading
    to the improved resource efficiency. Notice that a convolutional layer becomes
    a fully connected layer if the height and the width at the input layer are matched
    with each kernel’s height and width. The number of total weights in a layer in
    a CNN is much less than used in a fully connected neural network, since the local
    receptive weights are shared over the entire feature on a layer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是一种在每个参数和每次操作的准确性方面资源高效的深度学习架构，通过利用局部感受野和共享权重这两个特性 [[93](#bib.bib93)]。例如，使用多个小卷积核进行卷积可以在训练过程中从输入图像中提取多个局部感受特征，每个卷积核在训练后包含输入图像中的一些有意义的模式
    [[179](#bib.bib179)]。因此，CNN 使用的权重比全连接 DNN 少得多，因为卷积核的高度和宽度通常远小于输入层的高度和宽度，从而提高了资源效率。请注意，当输入层的高度和宽度与每个卷积核的高度和宽度匹配时，卷积层会变成全连接层。CNN
    中一层的总权重数量远少于全连接神经网络中使用的数量，因为局部感受权重在整个特征层上是共享的。
- en: Training CNN also utilizes the backprogation using the transpose of kernel matrices
    in a filter to update the weights in the filter. The mini-batch gradient descent
    algorithm is widely used to train CNNs, which utilizes part of training data to
    update the weights per iteration. The number of data used per iteration is often
    referred to as the batch size $B$ (e.g., $B=64$ or $128$). Each $epoch$ consumes
    the entire training data, consisting of $N/B$ iterations, where $N$ is the number
    of the entire training data. The mini-batch gradient descent method is a resource-efficient
    training algorithm in terms of the accuracy per operation, compared to the batch
    gradient descent method that utilizes entire training dataset per iteration (i.e.,
    the batch gradient descent method updates the weights per epoch). For parallel
    backpropagation implementation with respect to $B$ data samples in one mini-batch,
    all the weights and all the activation values using $B$ training samples should
    be stored to update the weights per the mini-batch iteration, requiring $B\times$
    additional storage, compared to the backpropagation using a stochastic gradient
    descent algorithm which updates the weights per training sample. Our paper refers
    to the term, DNN, as any neural network with several hidden layers, including
    CNN.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 CNN 还利用反向传播使用滤波器中内核矩阵的转置来更新滤波器中的权重。小批量梯度下降算法被广泛用于训练 CNN，它利用部分训练数据来每次迭代更新权重。每次迭代使用的数据量通常称为批量大小
    $B$（例如，$B=64$ 或 $128$）。每个 $epoch$ 消耗整个训练数据，由 $N/B$ 次迭代组成，其中 $N$ 是整个训练数据的数量。与每次迭代利用整个训练数据集的批量梯度下降方法（即，批量梯度下降方法在每个
    epoch 更新权重）相比，小批量梯度下降方法在每次操作的精度方面是一种资源高效的训练算法。对于相对于一个小批量中的 $B$ 数据样本的并行反向传播实现，所有权重和所有激活值使用
    $B$ 个训练样本应该被存储以更新小批量迭代中的权重，相比之下，使用随机梯度下降算法的反向传播方法每个训练样本更新权重。我们的论文将 DNN 一词指代任何具有多个隐藏层的神经网络，包括
    CNN。
- en: 2.2 Resource Efficiency Metrics for Deep Learning
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 深度学习的资源效率指标
- en: 'Recently, researchers from DeepMind [[78](#bib.bib78)] proposed the metrics
    for resource-efficient deep learning benchmarks, including the top-1 accuracy,
    the required memory footprint for training, and the required number of floating
    operations for training, and evaluated the resource-efficiency for deep learning
    applications with jointly considering the three metrics. The Roofline model [[162](#bib.bib162)]
    discussed attainable performance in terms of the operational intensity defined
    as the number of floating point operations per DRAM access. Motivated by [[78](#bib.bib78),
    [162](#bib.bib162)], our resource efficiency metrics include the accuracy per
    parameter, per operation, per memory footprint, per core utilization, per memory
    access, and per Joule as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques").'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，DeepMind 的研究人员[[78](#bib.bib78)] 提出了资源效率深度学习基准的度量标准，包括 top-1 精度、训练所需的内存占用和训练所需的浮点运算数量，并通过联合考虑这三项指标评估了深度学习应用的资源效率。Roofline
    模型[[162](#bib.bib162)] 讨论了在操作强度方面可以达到的性能，操作强度定义为每次 DRAM 访问的浮点运算次数。受[[78](#bib.bib78),
    [162](#bib.bib162)] 的启发，我们的资源效率指标包括每个参数、每次操作、每个内存占用、每个核心利用率、每次内存访问和每焦耳的精度，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient Deep Learning: A Survey on Model-,
    Arithmetic-, and Implementation-Level Techniques")所示。'
- en: '2.2.1 Accuracy per Parameter:'
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 每个参数的精度：
- en: We consider the accuracy per parameter (i.e., weight) for a resource-efficiency
    metric. The accuracy per parameter is an abstract resource efficiency metric since
    higher accuracy per parameter does not always imply higher physical resource efficiency
    after its implementation [[78](#bib.bib78), [1](#bib.bib1)].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个参数（即权重）的精度视为一种资源效率指标。每个参数的精度是一个抽象的资源效率指标，因为更高的每个参数的精度并不总是意味着其实现后的物理资源效率更高[[78](#bib.bib78),
    [1](#bib.bib1)]。
- en: '2.2.2 Accuracy per Operation:'
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 每次操作的精度：
- en: We consider the accuracy per arithmetic operation for a resource-efficiency
    metric. This is also an abstract metric, since it can be evaluated prior to the
    implementation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每次算术操作的精度视为一种资源效率指标。这也是一个抽象的指标，因为它可以在实现之前进行评估。
- en: '2.2.3 Accuracy per Compute Resource:'
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 每个计算资源的精度：
- en: The instruction-driven architecture such as CPU or GPU requires substantial
    memory accesses due to instruction fetch and decode operations, while the data-driven
    architecture such as ASIC or FPGA can minimize the number of memory accesses,
    resulting in energy efficiency. We further categorize such compute resource into
    core utilization, memory footprint, and memory access, required to operate a DNN
    on given computing platforms. For example, the memory access can be interpreted
    as GPU DRAM access (or off-chip memory) for a GPU and as FPGA on-chip memory access
    (or off-chip memory) for a FPGA.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 像CPU或GPU这样的指令驱动架构由于指令提取和解码操作，需要大量的内存访问，而像ASIC或FPGA这样的数据驱动架构可以最小化内存访问的数量，从而提高能效。我们进一步将这些计算资源分类为核心利用率、内存足迹和内存访问，这些都是在给定计算平台上操作DNN所需的。例如，内存访问可以解释为GPU的DRAM访问（或离芯存储器），以及FPGA的片上内存访问（或离芯存储器）。
- en: 'a. Accuracy per Core Utilization: The core utilization in this paper represents
    the utilization percentage of the processing cores or processing elements.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: a. 每核心利用率的准确度：本文中的核心利用率表示处理核心或处理单元的利用百分比。
- en: 'b. Accuracy per Memory Footprint: The accuracy per memory footprint is related
    to both physical and abstract resource efficiency as shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient Deep Learning: A Survey on Model-,
    Arithmetic-, and Implementation-Level Techniques"). The memory footprint is in
    proportion to the number of the parameters, but it can be varied according to
    a precision-level applied for arithmetic. For example, if a half precision arithmetic
    is applied for a deep learning, the memory footprint can be saved by $2\times$,
    compared to a single precision arithmetic deep learning.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 'b. 每内存足迹的准确度：每内存足迹的准确度与物理和抽象资源效率有关，如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques")所示。内存足迹与参数数量成正比，但根据所应用的运算精度级别，内存足迹可能会有所不同。例如，如果对深度学习应用了半精度运算，与单精度运算深度学习相比，内存足迹可以节省
    $2\times$。'
- en: 'c. Accuracy per Memory Access: A computing kernel having a low operational
    intensity cannot approach a peak performance defined by hardware specification
    since the data supply rate from DRAM to CPU cannot catch up with the data consumption
    rate by arithmetic operations. Such kernels are called “memory bound kernels”
    in [[162](#bib.bib162)]. Other type kernels are named “compute bound kernels”
    that can approach a peak performance defined by hardware specification. Utilizing
    reduced precision arithmetic can improve the performance for both memory bound
    kernels by improving the data supply rate from DRAM to CPU and compute bound kernels
    by increasing word-level parallelism on SIMD architectures [[95](#bib.bib95)].'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: c. 每内存访问的准确度：具有低操作强度的计算内核无法达到硬件规格定义的峰值性能，因为从DRAM到CPU的数据供应速率赶不上算术操作的数据消费速率。这类内核被称为“内存绑定内核”，在[[162](#bib.bib162)]中有所描述。另一类内核被称为“计算绑定内核”，可以达到硬件规格定义的峰值性能。利用降低精度的运算可以提高内存绑定内核的性能，改善从DRAM到CPU的数据供应速率，同时也能提高计算绑定内核的性能，通过在SIMD架构上增加字级并行性[[95](#bib.bib95)]。
- en: '2.2.4 Accuracy per Joule:'
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 每焦耳的准确度：
- en: 'The dynamic power consumption is the main factor to determine energy consumption
    required for computationally intensive tasks (e.g., DNN training/inference tasks).
    The dynamic power consumption, $P_{D}$, follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 动态功耗是决定计算密集型任务（如DNN训练/推理任务）所需能量消耗的主要因素。动态功耗 $P_{D}$ 的计算公式如下：
- en: '|  | $P_{D}=\#_{TTR}\times C_{CP}\times V^{2}_{CP}\times f_{CP},$ |  | (4)
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{D}=\#_{TTR}\times C_{CP}\times V^{2}_{CP}\times f_{CP},$ |  | (4)
    |'
- en: 'where $\#_{TTR}$ is the number of toggled transistors, $C_{CP}$ is an effective
    capacitance, $V_{CP}$ is an operational voltage, and $f_{CP}$ is an operational
    frequency for a given computing platform $CP$. Generally, the required minimum
    operational voltage is in proportion to the operational frequency. Therefore,
    adapting the frequency to the voltage scaling can save power cubically (a.k.a.
    Dynamic Voltage Frequency Scaling [[137](#bib.bib137)]). For example, minimizing
    the operations required to operate DNN during runtime contributes to minimizing
    $\#_{TTR}$, resulting in power reduction and energy saving; we discuss further
    the resource-efficient techniques leveraging this in Section [5.2.1](#S5.SS2.SSS1
    "5.2.1 Skipping Operations during Runtime: ‣ 5.2 Leveraging Sparsity of Weights
    and Activations ‣ 5 Implementation-Level Resource-Efficient Techniques ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\#_{TTR}$ 是切换晶体管的数量，$C_{CP}$ 是有效电容，$V_{CP}$ 是操作电压，$f_{CP}$ 是给定计算平台 $CP$
    的操作频率。一般来说，所需的最小操作电压与操作频率成正比。因此，通过将频率适应电压缩放，可以立方节省功耗（也称为动态电压频率缩放 [[137](#bib.bib137)]）。例如，减少在运行时操作
    DNN 所需的操作有助于减少 $\#_{TTR}$，从而实现功耗降低和能源节省；我们在第 [5.2.1](#S5.SS2.SSS1 "5.2.1 跳过运行时操作：
    ‣ 5.2 利用权重和激活的稀疏性 ‣ 5 实现级资源高效技术 ‣ 资源高效深度学习：模型级、算术级和实现级技术的调查") 节中进一步讨论利用这一点的资源高效技术。
- en: 3 Model-Level Resource-Efficient Techniques
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 模型级资源高效技术
- en: 'The model-level resource-efficient techniques, mostly developed from machine
    learning community, aim at reducing the DNN model size to fit the models to resource-constrained
    systems such as mobile devices, IoTs, etc. We categorize the model-level resource-efficient
    techniques as shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3 Model-Level Resource-Efficient
    Techniques ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques").'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 模型级资源高效技术主要由机器学习社区开发，旨在减少 DNN 模型的大小，以适应资源受限的系统，如移动设备、物联网等。我们将模型级资源高效技术分类，如图
    [4](#S3.F4 "图 4 ‣ 3 模型级资源高效技术 ‣ 资源高效深度学习：模型级、算术级和实现级技术的调查") 所示。
- en: '![Refer to caption](img/77bbc80aa3290d39beb1cd36421182e2.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/77bbc80aa3290d39beb1cd36421182e2.png)'
- en: 'Figure 4: Categorization for model-level resource-efficient techniques.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：模型级资源高效技术的分类。
- en: 3.1 Weight Quantization
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 权重量化
- en: The weight quantization techniques quantize the weights with a smaller number
    of bits, improving the accuracy per memory footprint. The training procedure should
    be amended according to the weight quantization schemes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 权重量化技术将权重量化为更少的比特数，从而提高了每个内存占用的准确性。训练过程应根据权重量化方案进行修改。
- en: '3.1.1 Binary Weight Quantization:'
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 二值权重量化：
- en: BinaryConnect training scheme [[38](#bib.bib38)] allowed a DNN to represent
    the weights using one bit. In step 1, the weights are encoded to $\{-1,1\}$ using
    a stochastic clipping function. In step 2, the forward pass is performed using
    the encoded binary weights. In step 3, backpropagation seeks all activation gradients
    using full precision. In step 4, the weights are updated using full precision,
    and the training procedure goes back to step 1 for the training using the next
    mini-batch. This method required only one bit to represent the weights, thus improving
    the accuracy per memory footprint. In addition, the binary weight quantization
    also removed the need for multiplication arithmetic operations for MAC operations,
    improving the accuracy per operation. Moreover, if the activations are also quantized
    to the binary value, all MAC operations in DNN can be implemented only with XNOR
    gates and a counter [[39](#bib.bib39), [124](#bib.bib124)].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: BinaryConnect 训练方案 [[38](#bib.bib38)] 允许 DNN 使用一个比特表示权重。在第 1 步中，使用随机剪切函数将权重编码为
    $\{-1,1\}$。在第 2 步中，使用编码后的二值权重进行前向传播。在第 3 步中，反向传播使用全精度求取所有激活梯度。在第 4 步中，使用全精度更新权重，训练过程回到第
    1 步，使用下一个小批量进行训练。该方法仅需一个比特来表示权重，从而提高了每个内存占用的准确性。此外，二值权重量化还消除了 MAC 操作中的乘法算术运算，提高了每个操作的准确性。此外，如果激活也量化为二值，则
    DNN 中的所有 MAC 操作仅能使用 XNOR 门和计数器 [[39](#bib.bib39), [124](#bib.bib124)]。
- en: '3.1.2 Ternary Weight Quantization:'
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 三值权重量化：
- en: Li et al. [[98](#bib.bib98)] proposed ternary weight networks that utilized
    ternary weights, improving accuracy, compared to the binary weight networks. All
    the weights on each layer were quantized into three values, requiring only two
    bits to represent the quantized weights. Overall training procedure was similar
    to [[38](#bib.bib38)], but with ternary valued weights instead of the binary weights.
    The ternary weight network showed equivalent accuracy to various single precision
    networks with MNIST, CIFAR-10 and ImageNet, while the binary weight quantization
    [[38](#bib.bib38)] showed minor accuracy loss. Zhu et al. [[185](#bib.bib185)]
    scaled the ternary weights independently for each layer with a layer-wise scaling
    approach, improving the accuracy further, compared to [[98](#bib.bib98)].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人[[98](#bib.bib98)] 提出了三值权重网络，该网络利用三值权重，相较于二值权重网络，提高了准确性。每一层的所有权重都被量化为三种值，只需两位即可表示量化后的权重。总体训练过程与[[38](#bib.bib38)]类似，但权重为三值而非二值。三值权重网络在
    MNIST、CIFAR-10 和 ImageNet 数据集上的准确性与各种单精度网络相当，而二值权重量化[[38](#bib.bib38)]则显示出轻微的准确性损失。Zhu
    等人[[185](#bib.bib185)] 通过逐层缩放方法对三值权重进行独立缩放，相较于[[98](#bib.bib98)]进一步提高了准确性。
- en: '3.1.3 Mixed Quantization:'
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 混合量化：
- en: '[[81](#bib.bib81)] proposed “Quantized Neural Network” that quantizes the activations
    and the weights to arbitrary lower precision format. For example, quantizing the
    weights to 1 bit and the activations to 2 bits improved the accuracy, compared
    to the binarized DNN of [[39](#bib.bib39)].'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[[81](#bib.bib81)] 提出了“量化神经网络”，该方法将激活值和权重量化为任意较低精度的格式。例如，将权重量化为 1 位，将激活值量化为
    2 位，相较于[[39](#bib.bib39)]的二值化深度神经网络，这提高了准确性。'
- en: 3.2 Pruning
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 剪枝
- en: Pruning unimportant neurons, filters, and channels can save computational resources
    for deep learning applications without sacrificing accuracy, improving the accuracy
    per parameter and per operation. Coarse-grained pruning methods such as pruning
    filters or channels are not flexible to achieve a prescribed accuracy, but can
    be implemented efficiently on hardware [[104](#bib.bib104)], implying higher physical
    resource efficiency than fine-grained pruning such as pruning weights. Notice
    that such pruning methods can degrade confidence scores without careful re-training,
    even though they did not affect top-1 accuracy [[174](#bib.bib174)].
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝不重要的神经元、滤波器和通道可以节省深度学习应用的计算资源，而不牺牲准确性，从而提高每个参数和每个操作的准确性。粗粒度剪枝方法，如剪枝滤波器或通道，灵活性不足以达到预定的准确性，但在硬件上实现效率较高[[104](#bib.bib104)]，这意味着其物理资源效率高于细粒度剪枝方法，例如剪枝权重。请注意，此类剪枝方法在未经仔细重新训练的情况下可能会降低置信度评分，尽管它们不会影响
    top-1 准确率[[174](#bib.bib174)]。
- en: '3.2.1 Pruning Weights:'
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 剪枝权重：
- en: In 1990, LeCun et al. proposed a weight pruning method to generate sparse DNNs
    with fewer weights without losing accuracy [[94](#bib.bib94)]. In 2015, the weight
    pruning approach was revisited [[66](#bib.bib66)], and the weights were pruned
    based on their magnitudes after training – the pruned DNNs were retrained to regain
    the lost accuracy. The pruning and re-training procedures could be performed iteratively
    to prune the weights further. This method reduced the number of weights of AlexNet
    by $9\times$ without losing accuracy. In 2016, Guo et al. [[58](#bib.bib58)] noticed
    that pruning wrong weights could not be revived, and proposed to prune and splice
    the weights per mini-batch training to minimize the risk from pruning wrong weights
    from previous mini-batch training. For example, the pruned weights were also participated
    in the weight update procedure during the backpropagation and were restored when
    they were re-considered as the important weights. In 2017, Yang et. al [[170](#bib.bib170)]
    proposed an energy-aware weight pruning method in which the energy consumption
    of a CNN was directly measured to guide the pruning process. In 2019, Frankly
    et al. [[50](#bib.bib50)] demonstrated that some of pruned models outperformed
    the original model by retraining the pruned models with replacing the survived
    weights with the initial random weights used for training the original model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在1990年，LeCun 等人提出了一种权重修剪方法，以生成具有更少权重的稀疏深度神经网络，而不损失准确性[[94](#bib.bib94)]。在2015年，权重修剪方法被重新审视[[66](#bib.bib66)]，并且根据训练后的权重幅度进行了修剪——修剪后的深度神经网络经过再训练以恢复丧失的准确性。修剪和再训练程序可以迭代进行，以进一步修剪权重。这种方法将
    AlexNet 的权重数量减少了 $9\times$，而没有损失准确性。在2016年，Guo 等人[[58](#bib.bib58)] 注意到，修剪错误的权重无法恢复，并提出了在每个小批量训练中修剪和拼接权重，以最小化从先前小批量训练中修剪错误权重的风险。例如，修剪的权重也参与了反向传播过程中的权重更新，并在重新考虑为重要权重时恢复。在2017年，Yang
    等人[[170](#bib.bib170)] 提出了一个能量感知的权重修剪方法，其中直接测量卷积神经网络的能量消耗以指导修剪过程。在2019年，Frankly
    等人[[50](#bib.bib50)] 证明，通过用用于训练原始模型的初始随机权重替换存活权重来重新训练修剪模型，有些修剪后的模型的表现优于原始模型。
- en: '3.2.2 Pruning Neurons:'
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 修剪神经元：
- en: Instead of pruning individual weights, pruning a neuron can remove a group of
    the weights belonging to the neuron [[143](#bib.bib143), [110](#bib.bib110), [77](#bib.bib77),
    [178](#bib.bib178)]. In 2015, [[143](#bib.bib143)] pruned the redundant neurons
    having similar weight values in a trained DNN model. For example, the weights
    in a baseline neuron were compared to the weights in other neurons at the same
    layer, and the neurons having similar weights to the baseline neuron were fused
    to the baseline neuron based on a Euclidean distance metric in the weight values
    between the two neurons. In 2016, [[110](#bib.bib110)] pruned the redundant neurons
    based on the “determinantal point process” metric. Hu et. al [[77](#bib.bib77)]
    measured the average percentage of zero activations per neuron and pruned the
    neurons having a high percentage of zero activations according to a given compression
    rate. Yu et al. [[178](#bib.bib178)] pruned unimportant neurons based on the effect
    of the pruning error propagation on the final response layer (e.g., the neurons
    were pruned backward from the final layer to the first layer). The methods of
    pruning neurons improved the resource efficiency such as the accuracy per parameter
    and per operation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与其修剪单独的权重，不如修剪一个神经元，这样可以移除属于该神经元的一组权重[[143](#bib.bib143), [110](#bib.bib110),
    [77](#bib.bib77), [178](#bib.bib178)]。在2015年，[[143](#bib.bib143)] 修剪了在训练后的深度神经网络模型中具有相似权重值的冗余神经元。例如，将基准神经元中的权重与同一层中其他神经元的权重进行比较，具有类似权重的神经元根据两个神经元权重值之间的欧几里得距离度量融合到基准神经元中。在2016年，[[110](#bib.bib110)]
    基于“行列式点过程”度量修剪了冗余神经元。Hu 等人[[77](#bib.bib77)] 测量了每个神经元零激活的平均百分比，并根据给定的压缩率修剪了零激活百分比高的神经元。Yu
    等人[[178](#bib.bib178)] 基于修剪误差传播对最终响应层的影响修剪了不重要的神经元（例如，神经元从最终层向第一层进行反向修剪）。这些修剪神经元的方法提高了资源效率，例如每个参数和每次操作的准确性。
- en: '3.2.3 Pruning Filters:'
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 修剪滤波器：
- en: Pruning insignificant filters after training can improve the accuracy per parameter
    and per operation. The feature maps associated with the pruned filters and the
    next kernels associated with the pruned feature maps should be also pruned. Pruning
    filters can maintain the dense structure of DNN unlike pruning weights, implying
    that it is highly probable to improve physical resource efficiency further, compared
    to pruning weights. Li et al. [[99](#bib.bib99)] pruned unimportant filters based
    on the summation of absolute weight values in the filter. The pruned DNNs were
    retrained with the survived filters to regain the lost accuracy. Yang et al. [[171](#bib.bib171)]
    pruned filters based on a platform-aware magnitude-based metric depending on the
    resource-constrained devices. ThiNet [[107](#bib.bib107)] calculated the significance
    of the filters using the outputs of the next layer and pruned the insignificant
    filters based on this significance measurement.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练后剪枝不重要的滤波器可以提高每个参数和每次操作的准确性。与被剪枝滤波器相关的特征图和与被剪枝特征图相关的下一层卷积核也应被剪枝。剪枝滤波器可以保持DNN的稠密结构，不像剪枝权重，这意味着与剪枝权重相比，进一步提高物理资源效率的可能性较高。Li
    等人 [[99](#bib.bib99)] 基于滤波器中绝对权重值的总和剪枝不重要的滤波器。剪枝后的DNN与幸存的滤波器一起重新训练，以恢复丢失的准确性。Yang
    等人 [[171](#bib.bib171)] 基于资源受限设备的以平台为意识的幅度度量剪枝滤波器。ThiNet [[107](#bib.bib107)]
    使用下一层的输出计算滤波器的重要性，并根据这种重要性测量剪枝不重要的滤波器。
- en: '3.2.4 Pruning Channels:'
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 剪枝通道：
- en: Unlike pruning filters, pruning channels removes the filters at the current
    layer and the kernels at the next layer associated with the pruned channels. The
    network slimming approach [[104](#bib.bib104)] pruned insignificant channels,
    producing compact models while keeping equivalent accuracy, compared to the models
    prior to pruning. For example, insignificant channels were identified based on
    scaling factors generated from the batch normalization of [[83](#bib.bib83)],
    and the channels associated with lower scaling factors were pruned. After the
    initial training, the channels associated with relatively low scaling factors
    were first pruned, and retraining was then performed to refine the network. He
    et al. [[69](#bib.bib69)] identified unimportant channels using LASSO regression
    from a pre-trained CNN model and pruned them. The channel pruning brought 5 $\times$
    speed-up on VGG-16 with minor accuracy loss. Lin et al. [[102](#bib.bib102)] pruned
    unimportant channels during runtime based on a decision maker trained by reinforcement
    learning. Gao et al. [[53](#bib.bib53)] proposed another dynamic channel pruning
    method that dynamically skipped the convolution operations associated with unimportant
    channels.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与剪枝滤波器不同，剪枝通道会移除当前层的滤波器以及与剪枝通道相关的下一层的卷积核。网络瘦身方法 [[104](#bib.bib104)] 剪枝不重要的通道，生成紧凑的模型，同时保持与剪枝前模型相当的准确性。例如，不重要的通道是基于
    [[83](#bib.bib83)] 的批量归一化生成的缩放因子识别的，缩放因子较低的通道被剪枝。在初始训练后，首先剪枝与相对较低缩放因子相关的通道，然后进行重新训练以优化网络。He
    等人 [[69](#bib.bib69)] 使用LASSO回归从预训练的CNN模型中识别不重要的通道并进行剪枝。通道剪枝使VGG-16的速度提高了5倍，准确性损失较小。Lin
    等人 [[102](#bib.bib102)] 在运行时基于由强化学习训练的决策者剪枝不重要的通道。Gao 等人 [[53](#bib.bib53)] 提出了另一种动态通道剪枝方法，该方法动态跳过与不重要通道相关的卷积操作。
- en: 3.3 Compact Convolution
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 紧凑卷积
- en: To improve resource-efficiency such as the accuracy per operation and per parameter
    from computationally intensive convolution operations, many compact convolution
    methods were proposed.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高资源效率，如每次操作和每个参数的准确性，提出了许多紧凑卷积方法。
- en: '3.3.1 Squeezing Channel:'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 压缩通道：
- en: In 2016, Iandola et al. [[82](#bib.bib82)] proposed SqueezeNet in which each
    network block utilized the number of 1$\times$1 filters less than the number of
    the input channels to reduce the network width in the squeezing stage and then
    utilized multiple 1$\times$1 and 3$\times$3 kernels in the expansion stage. The
    computational complexity was significantly reduced by squeezing the width, while
    compensating the accuracy in the expansion stage. SqueezeNet reduced the number
    of parameters by $50\times$, compared to AlexNet on ImageNet without losing accuracy,
    improving accuracy per parameter. Gholami et al. [[55](#bib.bib55)] proposed SqueezeNext
    that utilized separable convolutions in the expansion stage; a $k\times k$ filter
    was divided into a $k\times 1$ and a $1\times k$ filter. Such separable convolutions
    reduced the number of parameters further, compared to SqueezeNet while maintaining
    AlexNet’s accuracy on ImageNet, improving accuracy per parameter further, compared
    to SqueezeNet.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年，Iandola等人[[82](#bib.bib82)]提出了SqueezeNet，其中每个网络块在压缩阶段使用的$1\times1$滤波器数量少于输入通道的数量，以减少网络宽度，然后在扩展阶段使用多个$1\times1$和$3\times3$卷积核。通过压缩宽度显著减少了计算复杂度，同时在扩展阶段补偿了准确性。SqueezeNet在ImageNet上将参数数量减少了$50\times$，而不损失准确性，提高了每个参数的准确性。Gholami等人[[55](#bib.bib55)]提出了SqueezeNext，该方法在扩展阶段使用了可分离卷积；$k\times
    k$滤波器被分成了$k\times 1$和$1\times k$滤波器。这种可分离卷积进一步减少了参数数量，相比于SqueezeNet在保持AlexNet在ImageNet上的准确性的同时，进一步提高了每个参数的准确性。
- en: '3.3.2 Depth-Wise Separable Convolution:'
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 深度可分离卷积：
- en: 'Xception [[36](#bib.bib36)] utilized the depth-wise separable convolutions,
    that replace 3D convolutions with 2D separable convolutions followed by 1D convolutions
    (i.e., point-wise convolutions) as shown in Fig. [5](#S3.F5 "Figure 5 ‣ 3.3.2
    Depth-Wise Separable Convolution: ‣ 3.3 Compact Convolution ‣ 3 Model-Level Resource-Efficient
    Techniques ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques"), to reduce computational complexity. The
    2D separable convolutions are performed separately with respect to different channels.
    Howard et al. [[76](#bib.bib76)] proposed MobileNet v1 that utilizes the depth-wise
    separable convolutions with the two hyperparameters, “width multiplier and resolution
    multiplier", to fit DNNs to resource-constrained devices by fully leveraging the
    accuracy and resource trade-off in the DNNs. MobileNet v1 showed equivalent accuracy
    to GoogleNet and VGG16 on ImageNet dataset with less computational complexity,
    improving the accuracy per parameter and per operation.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 'Xception [[36](#bib.bib36)]利用了深度可分离卷积，它用2D可分离卷积代替了3D卷积，然后是1D卷积（即点卷积），如图[5](#S3.F5
    "图5 ‣ 3.3.2 深度可分离卷积: ‣ 3.3 紧凑卷积 ‣ 3 模型级资源高效技术 ‣ 资源高效深度学习：模型、算术和实现级技术调查")所示，以减少计算复杂度。2D可分离卷积在不同通道上分别执行。Howard等人[[76](#bib.bib76)]提出了MobileNet
    v1，该方法利用了深度可分离卷积，并使用了两个超参数，“宽度倍增器和分辨率倍增器”，通过充分利用DNN的准确性和资源权衡，将DNN适应于资源受限的设备。MobileNet
    v1在ImageNet数据集上显示出与GoogleNet和VGG16相当的准确性，但计算复杂度更低，提高了每个参数和每次操作的准确性。'
- en: '![Refer to caption](img/4e8b524db07de6f9ad5dc76e880381df.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4e8b524db07de6f9ad5dc76e880381df.png)'
- en: 'Figure 5: Depth-wise convolution used in [[76](#bib.bib76)].'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '图5: [[76](#bib.bib76)]中使用的深度卷积。'
- en: '3.3.3 Linear Bottleneck Layer:'
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 线性瓶颈层：
- en: 'In general, the manifold of interest (i.e., the subspace formed by the set
    of activations at each layer) could be embedded in low-dimensional subspaces in
    deep learning. Inspired by this, Sandler et al. [[132](#bib.bib132)] proposed
    MobileNet v2 consisting of a series of bottleneck layer blocks. Each bottleneck
    layer block as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3.3 Linear Bottleneck Layer:
    ‣ 3.3 Compact Convolution ‣ 3 Model-Level Resource-Efficient Techniques ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")
    received lower dimensional input, expanded the input to high dimensional intermediate
    feature maps, and projected the high dimensional intermediate features onto low
    dimensional features. Keeping linearity for the output feature maps was crucial
    to avoid destroying information from non-linear activations, so linear activation
    functions were used at the end of each bottleneck block.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，感兴趣的流形（即每层激活集形成的子空间）可以嵌入到深度学习中的低维子空间中。受此启发，Sandler 等人 [[132](#bib.bib132)]
    提出了由一系列瓶颈层块组成的 MobileNet v2。每个瓶颈层块如图[6](#S3.F6 "Figure 6 ‣ 3.3.3 Linear Bottleneck
    Layer: ‣ 3.3 Compact Convolution ‣ 3 Model-Level Resource-Efficient Techniques
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques") 所示，接收低维输入，将输入扩展到高维中间特征图，然后将高维中间特征投影到低维特征上。保持输出特征图的线性对于避免破坏非线性激活中的信息至关重要，因此在每个瓶颈块的末尾使用了线性激活函数。'
- en: '![Refer to caption](img/b901f0aa4a691d1efbaf78ea5cf00ed4.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b901f0aa4a691d1efbaf78ea5cf00ed4.png)'
- en: 'Figure 6: Bottleneck layer block used in [[132](#bib.bib132)].'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '图6: 在 [[132](#bib.bib132)] 中使用的瓶颈层块。'
- en: '3.3.4 Group Convolution:'
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 组卷积：
- en: In a group convolution method, the input channels are divided into several groups,
    and the channels in each group are separately participated in convolution with
    other groups. For example, the input channels with three groups required three
    separate convolutions. Since group convolution does not communicate with the channels
    in other groups, communication between different groups is performed after the
    separate convolutions. Group convolution methods of [[181](#bib.bib181), [108](#bib.bib108),
    [80](#bib.bib80), [79](#bib.bib79)] reduced the number of MAC operations, improving
    the accuracy per operation, compared to DNNs using regular convolution. In 2012,
    AlexNet utilized group convolution to train the DNNs effectively using the two
    NVIDIA GTX580 GPUs [[91](#bib.bib91)]. Surprisingly, an AlexNet using the group
    convolution showed superior accuracy to an AlexNet using regular convolution,
    improving the accuracy per operation. In 2017, ResNext [[167](#bib.bib167)] utilized
    group convolution based on ResNet [[67](#bib.bib67)] using a cardinality parameter
    (i.e., the number of groups). In 2018, Zhang et al. [[181](#bib.bib181)] noticed
    that the point-wise convolutions were computationally intensive in practice in
    the depth-wise convolutions and proposed ShuffleNet that applied group convolution
    to every point-wise convolution to reduce compute complexity further, compared
    to MobileNet v1\. ShuffleNet shuffled the output channels from the grouped point-wise
    convolution to communicate with different grouped convolutions, demonstrating
    superior accuracy to MobileNet v1 on ImageNet and COCO datasets, given the same
    arithmetic operation cost budget. Ma et al. [[108](#bib.bib108)] proposed ShuffleNet
    v2 that improved physical resource efficiency further, compared to ShuffleNet
    [[181](#bib.bib181)] by employing equal channel width for input and output channels
    where applicable and minimizing the number of operations required for $1\times
    1$ convolutions. Rather than choosing each group randomly and shuffling them,
    Huang et al. [[80](#bib.bib80)] proposed to learn each group for a group convolution
    during training. The “learned group convolution” was applied in Densenet [[79](#bib.bib79)],
    and Densenet improved the accuracy per parameter and per operation, compared to
    ShuffleNet, given a prescribed accuracy.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在组卷积方法中，输入通道被划分为几个组，每个组的通道与其他组单独参与卷积。例如，具有三个组的输入通道需要三个单独的卷积。由于组卷积不会与其他组的通道通信，不同组之间的通信是在单独的卷积之后进行的。与使用常规卷积的DNN相比，[[181](#bib.bib181),
    [108](#bib.bib108), [80](#bib.bib80), [79](#bib.bib79)]的组卷积方法减少了MAC操作的数量，提高了每次操作的准确性。2012年，AlexNet利用组卷积有效地训练了DNN，使用了两块NVIDIA
    GTX580 GPU [[91](#bib.bib91)]。令人惊讶的是，使用组卷积的AlexNet在每次操作的准确性上优于使用常规卷积的AlexNet。2017年，ResNext
    [[167](#bib.bib167)]基于ResNet [[67](#bib.bib67)]，利用了一个基数参数（即组的数量）的组卷积。2018年，Zhang等人[[181](#bib.bib181)]注意到在深度卷积中，点卷积在实践中计算量很大，并提出了ShuffleNet，该方法将组卷积应用于每个点卷积，以进一步降低计算复杂性，相较于MobileNet
    v1。ShuffleNet将来自分组点卷积的输出通道进行混洗，以与不同的分组卷积进行通信，在给定相同算术操作成本预算的情况下，表现出比MobileNet v1更高的准确性。Ma等人[[108](#bib.bib108)]提出了ShuffleNet
    v2，相比ShuffleNet [[181](#bib.bib181)]进一步提高了物理资源效率，通过在适用的情况下对输入和输出通道采用相等的通道宽度，并最小化$1\times
    1$卷积所需的操作数量。Huang等人[[80](#bib.bib80)]提出在训练过程中对每个组进行学习以实现组卷积，而不是随机选择每个组并进行混洗。所谓的“学习组卷积”被应用于Densenet
    [[79](#bib.bib79)]，Densenet在给定预定准确度的情况下，改善了每个参数和每个操作的准确性，相比ShuffleNet。
- en: '3.3.5 Octave Convolution:'
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5 八度卷积：
- en: Chen et al. [[34](#bib.bib34)] decomposed feature maps into a higher and a lower
    frequency part to save the feature maps’ memory footprint and reduce the computational
    cost. The decomposed feature maps were used by specific convolution called “Octave
    Convolution” that performs a convolution between the higher and lower frequency
    part. The application of the octave convolution to ResNet-152 architecture achieved
    higher accuracy using ImageNet dataset than the regular convolution, improving
    the accuracy per operation and per memory footprint.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Chen等人[[34](#bib.bib34)]将特征图分解为高频部分和低频部分，以节省特征图的内存占用并减少计算成本。分解后的特征图通过一种称为“八度卷积”的特定卷积进行卷积，该卷积在高频和低频部分之间进行。将八度卷积应用于ResNet-152架构，使用ImageNet数据集取得了比常规卷积更高的准确性，提高了每次操作和每单位内存的准确性。
- en: '3.3.6 Downsampling:'
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.6 下采样：
- en: Qin et al. [[122](#bib.bib122)] applied a downsampling approach (e.g., a larger
    stride size for a convolution) to MobileNet v1, improving the top-1 accuracy by
    5.5% over MobileNet v1 on the ILSVRC 2012 dataset, given a 12M arithmetic operations
    budget.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Qin 等人 [[122](#bib.bib122)] 采用了降采样方法（例如，对卷积操作使用较大的步幅）来改进 MobileNet v1，在 ILSVRC
    2012 数据集上将顶级准确率提高了 5.5%，在 12M 算术运算预算下进行。
- en: '3.3.7 Low Rank Approximation:'
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.7 低秩近似：
- en: Denton et al. [[44](#bib.bib44)] proposed a low rank approximation that compresses
    the kernel tensors in the convolutional layers and the weight matrices in the
    fully connected layers by using singular value decomposition. Another low rank
    approximation [[89](#bib.bib89)] used Tucker decomposition to compress the feature
    maps, resulting in significant reductions in the model size, the inference latency,
    and the energy consumption. Such low rank approximation methods improve the accuracy
    per parameter, per operation, and per memory footprint.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Denton 等人 [[44](#bib.bib44)] 提出了低秩近似，通过使用奇异值分解压缩卷积层中的内核张量和全连接层中的权重矩阵。另一种低秩近似
    [[89](#bib.bib89)] 使用 Tucker 分解来压缩特征图，显著减少了模型大小、推理延迟和能量消耗。这些低秩近似方法提高了每个参数、每个操作和每个内存占用的准确性。
- en: 3.4 Knowledge Distillation
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 知识蒸馏
- en: The knowledge from a large-scale high performing model (teacher network) could
    be transferred to a compact neural network (student network) to improve resource
    efficiency such as accuracy per parameter and per operation for inference tasks
    [[23](#bib.bib23), [127](#bib.bib127), [29](#bib.bib29)]. Buciluă et al. [[23](#bib.bib23)]
    utilized data with the labels generated from the teacher model (i.e., a large
    scale ensemble model) to train a compact neural network. The compact model was
    trained with the pseudo training data generated from the teacher model, demonstrating
    equivalent accuracy to the teacher model. Ba and Caruana [[14](#bib.bib14)] noticed
    that the softmax outputs often resulted in the student network ignoring the information
    of the other categorizations than the one with the highest probability, and utilized
    the values prior to the softmax layer, from the teacher network, for the training
    labels to allow the student network to learn the teacher network more efficiently.
    Hinton et al. [[72](#bib.bib72)] added a “temperature” term for the labels to
    enrich the information from the teacher network and train the student network
    more efficiently, compared to [[14](#bib.bib14)]. Romeo et. al [[127](#bib.bib127)]
    utilized both labels and intermediate representations from a wider teacher network
    to compress it to a thinner and deeper student network. The “hint layer” was chosen
    from the teacher network and the “guided layer” was chosen from the student network.
    The student network was then trained so that the intermediate representation deviation
    between the outputs from the hint layers and guided layers could be minimized.
    A thinner student network employed $10.4\times$ less weight parameters, compared
    to a wider teacher network, while improving accuracy. This technique is also known
    as “hint learning”. The hint learning was applied to both the region proposal
    and classification components for object detection applications [[29](#bib.bib29)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 从大规模高性能模型（教师网络）中获取的知识可以转移到紧凑的神经网络（学生网络），以提高资源效率，例如推理任务中的每参数和每操作的准确性 [[23](#bib.bib23),
    [127](#bib.bib127), [29](#bib.bib29)]。Buciluă 等人 [[23](#bib.bib23)] 利用从教师模型（即大规模集成模型）生成的标签数据来训练紧凑型神经网络。紧凑模型使用从教师模型生成的伪训练数据进行训练，展示了与教师模型相当的准确性。Ba
    和 Caruana [[14](#bib.bib14)] 注意到 softmax 输出通常导致学生网络忽略其他分类信息，仅关注概率最高的分类，因此利用教师网络中
    softmax 层之前的值作为训练标签，以便学生网络能更高效地学习教师网络。Hinton 等人 [[72](#bib.bib72)] 为标签添加了“温度”项，以丰富教师网络的信息，并更高效地训练学生网络，相比于
    [[14](#bib.bib14)]。Romeo 等人 [[127](#bib.bib127)] 利用来自更宽教师网络的标签和中间表示，将其压缩为更薄且更深的学生网络。选择了教师网络中的“提示层”和学生网络中的“指导层”。然后训练学生网络，以最小化提示层和指导层之间输出的中间表示偏差。与较宽的教师网络相比，较薄的学生网络使用了
    $10.4\times$ 更少的权重参数，同时提高了准确性。这种技术也被称为“提示学习”。提示学习应用于物体检测应用的区域提议和分类组件 [[29](#bib.bib29)]。
- en: 3.5 Neural Architecture Search for Compressed Models
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 压缩模型的神经架构搜索
- en: Zoph et al. [[187](#bib.bib187)] proposed Neural Architectural Search (NAS)
    technique to seek optimal DNN models in the space of hyperparameters of network
    width, depth, and resolution. In case that compute resource budget was limited
    (e.g., mobile devices), many NAS variants exploited the trade-off between accuracy
    and latency to maximize resource efficiency given compute resource budget [[70](#bib.bib70),
    [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149), [164](#bib.bib164),
    [13](#bib.bib13)]. He et al. [[70](#bib.bib70)] proposed a NAS employing reinforcement
    learning, AutoML, that sampled the least sufficient candidate design space to
    compress the DNN models. MnasNet [[147](#bib.bib147)] utilized reinforcement learning
    with a balanced reward function between the accuracy and the latency to seek a
    compact neural network model. Wu et. al [[164](#bib.bib164)] proposed a gradient-based
    NAS that produced a DNN model with $2.4\times$ model size reduction, compared
    to a MobileNet v2 without losing accuracy on ImageNet dataset. Florian et. al
    [[133](#bib.bib133)] proposed a narrow-space NAS to generate low-resource DNNs
    satisfying strict memory budget and inference time requirement for IoT applications.
    [[13](#bib.bib13)] noticed that conventional NAS might improve abstract resource
    efficiency rather than physical resource-efficiency, and utilized the hardware
    information including the inference latency for a NAS to ensure that the candidate
    models could improve the physical resource-efficiency in practice. Efficientnet
    [[148](#bib.bib148)] utilized a NAS with compound scaling of depth, width, and
    resolution to seek optimal DNN models given fixed compute resource budgets. Another
    NAS utilizing compound scaling, Efficientdet, was proposed for object detection
    applications [[149](#bib.bib149)]. Efficientdet improved the accuracy using COCO
    dataset with $4-9\times$ model size reduction, compared to state-of-the-art object
    detectors, improving the accuracy per parameters. Recently, [[25](#bib.bib25)]
    proposed a feed-forward NAS approach that produced a customized DNN, given compute
    resource and latency constraint.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Zoph 等人 [[187](#bib.bib187)] 提出了神经网络结构搜索（NAS）技术，用于在网络宽度、深度和分辨率的超参数空间中寻找最优的深度神经网络（DNN）模型。如果计算资源预算有限（例如，移动设备），许多
    NAS 变体利用了准确性和延迟之间的权衡，以在给定的计算资源预算下最大化资源效率 [[70](#bib.bib70), [147](#bib.bib147),
    [148](#bib.bib148), [149](#bib.bib149), [164](#bib.bib164), [13](#bib.bib13)]。He
    等人 [[70](#bib.bib70)] 提出了一个采用强化学习的 NAS，即 AutoML，该方法对候选设计空间进行采样，以压缩 DNN 模型。MnasNet
    [[147](#bib.bib147)] 利用具有准确性和延迟之间平衡奖励函数的强化学习来寻求紧凑的神经网络模型。Wu 等人 [[164](#bib.bib164)]
    提出了一个基于梯度的 NAS，生成了一个 DNN 模型，模型尺寸减少了 $2.4\times$，与 MobileNet v2 相比，在 ImageNet 数据集上没有损失准确性。Florian
    等人 [[133](#bib.bib133)] 提出了一个窄空间 NAS，用于生成符合严格内存预算和推理时间要求的低资源 DNN，适用于物联网应用。[[13](#bib.bib13)]
    注意到传统的 NAS 可能提高了抽象的资源效率而不是物理资源效率，并利用包括推理延迟在内的硬件信息来进行 NAS，以确保候选模型能够在实践中提高物理资源效率。Efficientnet
    [[148](#bib.bib148)] 利用复合缩放的 NAS 来在固定的计算资源预算下寻求最优的 DNN 模型。另一个利用复合缩放的 NAS，Efficientdet，被提出用于目标检测应用
    [[149](#bib.bib149)]。Efficientdet 使用 COCO 数据集提高了准确性，同时模型尺寸减少了 $4-9\times$，与最先进的目标检测器相比，提高了每参数的准确性。最近，[[25](#bib.bib25)]
    提出了一个前馈 NAS 方法，该方法在给定计算资源和延迟约束的情况下生成了定制的 DNN。
- en: 4 Arithmetic-Level resource-efficient Techniques
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 种算术级别的资源高效技术
- en: 'Utilizing lower precision arithmetic reduces the memory footprint and the time
    spent transferring data across buses and interconnections [[49](#bib.bib49), [114](#bib.bib114),
    [186](#bib.bib186), [172](#bib.bib172)]. Employing least sufficient arithmetic
    precision for DNN applications can improve the accuracy per memory footprint and
    the accuracy per memory access. We categorize the arithmetic-level resource-efficient
    techniques into the two categories as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4 Arithmetic-Level
    resource-efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques"), Arithmetic-Level Techniques
    for Inference and Arithmetic-Level Techniques for Training. We discuss different
    number formats first and the deployment of such number formats on DNNs later.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f645d4e9b0cfbe8d677086ae5fe9270f.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Categorization of arithmetic-level resource-efficient techniques.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Number Formats for Deep Learning
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection describes various number formats for deep learning applications,
    as preparatory to explaining the arithmetic-level resource-efficient techniques.
    Fixed-Point (FiP) number format utilizes a binary fixed point between fraction
    and integer part. For example, an 8-bit FiP format, “01.100000”, represents 1.5
    (i.e., $...0\times 2^{1}+1\times 2^{0}+1\times 2^{-1}+0\times 2^{-2}...$) for
    the decimal representation, and the point between integer part and fraction part
    is fixed for arithmetic operations. Therefore, it could be implemented with simple
    circuits, but the available data range is very limited [[120](#bib.bib120)].
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'We exemplify the IEEE-754 general-purpose floating-point standard [[4](#bib.bib4)]
    to explain Floating-Point (FP) format and its arithmetic, since this standard
    is used for most commercially available CPUs and GPUs. The IEEE 754 Floating-Point
    (IFP) data format [[4](#bib.bib4)] consists of sign, exponent, and significand
    as shown in Eq. ([5](#S4.E5 "In 4.1 Number Formats for Deep Learning ‣ 4 Arithmetic-Level
    resource-efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques")). For example, a floating
    point number has a $(p+1)$-bit significand (including the hidden one), an $e$-bit
    exponent, and an $1$ sign bit. The machine epsilon $\epsilon_{mach}$ is defined
    as $2^{-(p+1)}$. The value represented by FP is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{out}=\begin{cases}\text{normal mode:}&amp;(-1)^{sign}\times(1\times
    2^{0}+d_{1}\times 2^{-1}+...+d_{p}\times 2^{-p})\times 2^{exponent-bias}\\ \text{subnormal
    mode:}&amp;(-1)^{sign}\times(d_{1}\times 2^{-1}+...+d_{p}\times 2^{-p})\times
    2^{1-bias},\end{cases}$ |  | (5) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: 'where $d_{1}$, … , $d_{p}$ represent binary digits, the ‘$1$’ associated with
    the coefficient $2^{0}$ is referred to as the hidden ‘$1$’, the $exponent$ is
    stored in offset notation, and the $bias$ is a positive constant. If the absolute
    value of exponent is zero, the floating-point value is represented by the subnormal
    mode. IEEE 754 standard requires exact rounding for addition, subtraction, multiplication,
    and division; the floating point arithmetic result should be identical to the
    one obtained from the final rounding after exact calculation. For example, based
    on the IEEE 754 rounding to nearest mode standard, floating point arithmetic should
    follow Eq. ([6](#S4.E6 "In 4.1 Number Formats for Deep Learning ‣ 4 Arithmetic-Level
    resource-efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques")):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$d_{1}$, … , $d_{p}$表示二进制数字，系数$2^{0}$关联的‘$1$’被称为隐藏的‘$1$’，$exponent$以偏移量表示，而$bias$是一个正的常数。如果指数的绝对值为零，则浮点值以非规范模式表示。IEEE
    754标准要求加法、减法、乘法和除法的精确舍入；浮点运算结果应与经过精确计算后的最终舍入结果相同。例如，基于IEEE 754舍入到最近模式标准，浮点运算应遵循公式([6](#S4.E6
    "In 4.1 Number Formats for Deep Learning ‣ 4 Arithmetic-Level resource-efficient
    Techniques ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques"))：'
- en: '|  | $fl(x_{1}\odot x_{2})=(x_{1}\odot x_{2})(1+\epsilon_{r}),$ |  | (6) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $fl(x_{1}\odot x_{2})=(x_{1}\odot x_{2})(1+\epsilon_{r}),$ |  | (6) |'
- en: 'where $|\epsilon_{r}|\leq\epsilon_{mach}$, $\odot$ is one of the four arithmetic
    operations, and $fl(\cdot)$ represents the result from the floating point arithmetic.
    Notice that quantization quantizes data to lower precision, while arithmetic is
    a rule applied to arithmetic operations between the two operands. For example,
    the quantization affects the values for the two operands, $x_{1}$ and $x_{2}$
    in Eq. ([6](#S4.E6 "In 4.1 Number Formats for Deep Learning ‣ 4 Arithmetic-Level
    resource-efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques")), while arithmetic
    affects the rounding error, $\epsilon_{r}$.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$|\epsilon_{r}|\leq\epsilon_{mach}$，$\odot$是四种算术操作之一，$fl(\cdot)$表示浮点运算的结果。注意，量化将数据量化为较低精度，而算术是应用于两个操作数之间的运算规则。例如，量化影响公式([6](#S4.E6
    "In 4.1 Number Formats for Deep Learning ‣ 4 Arithmetic-Level resource-efficient
    Techniques ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques"))中两个操作数$x_{1}$和$x_{2}$的值，而算术影响舍入误差$\epsilon_{r}$。'
- en: '4.1.1 Half, Single, and Double Precision:'
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 半精度、单精度和双精度：
- en: The IEEE Floating-Point 32- (IFP32 or single precision) and 64-bit (IFP64 or
    double precision) versions are available on most of off-the-shelf conventional
    processors. Besides, IEEE-754 standard includes a 16-bit FP format (IFP16 or half
    precision) [[4](#bib.bib4)]. $p=52$, $e=11$, and $bias=1023$ for IFP64, $p=23$,
    $e=8$, and $bias=127$ for IFP32, and $p=10$, $e=5$, and $bias=15$ for IFP16\.
    IFP16 is currently supported in hardware on some of modern GPUs to accelerate
    DNN applications [[73](#bib.bib73), [37](#bib.bib37)].
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE浮点32位（IFP32或单精度）和64位（IFP64或双精度）版本在大多数现成的常规处理器上可用。此外，IEEE-754标准包括一种16位FP格式（IFP16或半精度）[[4](#bib.bib4)]。IFP64的$p=52$，$e=11$，$bias=1023$；IFP32的$p=23$，$e=8$，$bias=127$；IFP16的$p=10$，$e=5$，$bias=15$。IFP16目前在一些现代GPU中得到硬件支持，以加速DNN应用[[73](#bib.bib73),
    [37](#bib.bib37)]。
- en: '4.1.2 Brain Float-Point Format using 16 Bits (BFloat16):'
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 使用16位的Brain浮点格式（BFloat16）：
- en: In 2018, a 16-bit Brain Floating-Point format [[176](#bib.bib176), [24](#bib.bib24)]
    was proposed that was tailored to deep learning applications. The BFloat16 consists
    of an 8-bit exponent and a 7-bit significand, supporting a wider dynamic data
    range than IFP16. BFloat16 is currently supported in hardware in the Intel Cooper
    Lake Xeon processors, the NVIDIA A100 GPUs, and the Google TPUs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年，提出了一种针对深度学习应用的16位Brain浮点格式[[176](#bib.bib176), [24](#bib.bib24)]。BFloat16包含8位指数和7位尾数，比IFP16支持更宽的动态数据范围。BFloat16目前在Intel
    Cooper Lake Xeon处理器、NVIDIA A100 GPU和Google TPU中得到了硬件支持。
- en: '4.1.3 DLFloat:'
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 DLFloat：
- en: In the race of designing specific FP formats for DNNs, [[6](#bib.bib6), [158](#bib.bib158)]
    proposed another 16-bit precision format, DLFloat, consisting of 6-bit exponent
    and 9-bit significand to provide better balance between dynamic data range and
    precision than IFP16 and BFloat16 formats for some of deep learning applications.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '4.1.4 TensorFloat32 (TF32):'
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NVIDIA proposed a 19-bit data format, TF32, consisting of 1-bit sign, 8-bit
    exponent and 10-bit significand to accelerate deep learning applications on A100
    GPUs with the same dynamic range support as IFP32 [[48](#bib.bib48)]. TF32 Tensor
    cores in an A100 truncates IFP32 operands to 19-bit TF32 format but accumulates
    them using IFP32 arithmetic for MAC operations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Arithmetic-Level Techniques for Inference
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection discusses various resource-efficient arithmetic-level techniques
    based on the pre-trained DNNs for the inference tasks.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.1 Lower Precision Arithmetic:'
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lower precision FiP arithmetic has been widely used to deploy DNNs on edge devices
    [[169](#bib.bib169)]. [[165](#bib.bib165), [159](#bib.bib159)] analyzed the effect
    of deploying various lower precision arithmetic on the DNN inference tasks in
    terms of accuracy and latency. The BitFusion method accelerated DNN inference
    tasks by employing variable bit-width FiP formats dynamically depending on the
    different layers [[138](#bib.bib138)]. Similarly, Tambe et al. [[146](#bib.bib146)]
    proposed AdaptiveFloat that adjusted dynamic ranges of FP numbers depending on
    the different layers, resulting in higher energy efficiency than FiP-based methods,
    given the same accuracy requirement.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.2 Encoding Weights and Using Lookup Table:'
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[[21](#bib.bib21)] leveraged the fact that the exponent values of most of the
    weights were located within a narrow range and encoded the frequent exponent values
    of the weights with fewer bits using Huffman coding scheme, improving the accuracy
    per memory footprint for natural language processing applications. A lookup table,
    located between the memory and FP arithmetic units, is used to convert the encoded
    exponent values into FP exponent values.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.3 Applying Various Number Format Quantizations to DNNs:'
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Residue Number System (RNS) is a parallel and carry-limited number system
    that transforms a big natural number to several smaller residues. Therefore, RNS
    was often used to perform parallel and independent calculations on residues without
    carry-propagation. [[28](#bib.bib28)] exploited such parallelism to accelerate
    DNN computation. In a RNS-based DNN, the weights of a pre-trained model were transformed
    to RNS presentation. Recently, RNS was used to replace costly multiplication operations
    with simple logical operations such as multiplexing and shifting, accelerating
    DNN applications [[131](#bib.bib131), [105](#bib.bib105), [130](#bib.bib130)].
    The Logarithmic Number System (LNS) applies the logarithm to the absolute values
    of the real numbers [[120](#bib.bib120)]. The main advantage of LNS is in the
    capability of transforming multiplications into additions and divisions into subtractions.
    In 2018, [[156](#bib.bib156)] utilized a 5-bit logarithmic format using arbitrary
    log bases to improve the resource efficiency such as accuracy per memory footprint
    and per operations by replacing costly multiplication arithmetic operations to
    simple bit-shift operations [[156](#bib.bib156)]. The Posit number format [[61](#bib.bib61)]
    employs multiple separate exponent fields to represent dynamic range effectively.
    Recently, DNNs utilizing Posit showed higher accuracy than various FP8 formats
    using Mushroom and Iris datasets [[26](#bib.bib26), [27](#bib.bib27)].
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余数系统（RNS）是一种并行且限制进位的数字系统，它将一个大的自然数转换为几个较小的剩余数。因此，RNS 常用于对剩余数进行并行和独立的计算，而无需进位传播。[[28](#bib.bib28)]
    利用这种并行性来加速 DNN 计算。在基于 RNS 的 DNN 中，预训练模型的权重被转换为 RNS 表示。最近，RNS 被用来用简单的逻辑操作如多路复用和移位来替代成本高昂的乘法操作，从而加速
    DNN 应用 [[131](#bib.bib131), [105](#bib.bib105), [130](#bib.bib130)]。对数数系统（LNS）将对数应用于实数的绝对值
    [[120](#bib.bib120)]。LNS 的主要优势在于将乘法转化为加法，将除法转化为减法。2018年，[[156](#bib.bib156)] 利用5位对数格式和任意对数底数来提高资源效率，例如每单位内存和每操作的准确度，通过将高昂的乘法算术操作替换为简单的位移操作
    [[156](#bib.bib156)]。Posit 数字格式 [[61](#bib.bib61)] 使用多个独立的指数字段来有效地表示动态范围。最近，使用
    Posit 的 DNN 在 Mushroom 和 Iris 数据集 [[26](#bib.bib26), [27](#bib.bib27)] 上显示出比各种
    FP8 格式更高的准确度。
- en: 4.3 Arithmetic-Level Techniques for Training
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 算术级训练技术
- en: This subsection discusses arithmetic-level resource-efficient techniques used
    for DNN training tasks. Training DNNs generally requires a higher precision arithmetic
    due to extremely small weight gradient values [[38](#bib.bib38), [185](#bib.bib185)].
    Adjusting arithmetic precision according to different training procedures such
    as forward propagation, activation gradient updates, and weight updates can accelerate
    DNN training [[59](#bib.bib59)]. Training quantized DNNs often required stochastic
    rounding schemes [[60](#bib.bib60), [166](#bib.bib166), [172](#bib.bib172), [184](#bib.bib184)].
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节讨论了用于 DNN 训练任务的算术级资源高效技术。由于权重梯度值极小，训练 DNN 通常需要更高精度的算术 [[38](#bib.bib38),
    [185](#bib.bib185)]。根据不同的训练过程如前向传播、激活梯度更新和权重更新来调整算术精度，可以加速 DNN 训练 [[59](#bib.bib59)]。训练量化
    DNN 通常需要随机舍入方案 [[60](#bib.bib60), [166](#bib.bib166), [172](#bib.bib172), [184](#bib.bib184)]。
- en: '4.3.1 Mixed-Precision Training:'
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 混合精度训练：
- en: A conventional mixed precision training applied lower precision arithmetic to
    the multiplications in MACs, including both forward and backward path, and higher
    precision arithmetic to the accumulations in the MACs using the lower precision
    quantized operands [[86](#bib.bib86), [114](#bib.bib114)]. The higher precision
    outcomes from MACs were quantized to a lower precision format to be used for consequent
    operations. In the following (X + Y) formats, X represents the data format used
    for MAC operations, and Y represents the arithmetic applied for the accumulations
    in MAC operations (refer to [[59](#bib.bib59)] for the details for the lower and
    higher precision arithmetic usage.).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的混合精度训练将较低精度的算术应用于 MACs 中的乘法，包括前向和反向路径，并将较高精度的算术应用于使用较低精度量化操作数的 MACs 中的累加。来自
    MACs 的较高精度结果被量化为较低精度格式，以用于后续操作。在以下（X + Y）格式中，X 代表用于 MAC 操作的数据格式，Y 代表用于 MAC 操作中累加的算术（有关低精度和高精度算术使用的详细信息，请参阅
    [[59](#bib.bib59)]）。
- en: 'a. IFP16 + IFP32: In 2018, [[114](#bib.bib114)] noticed that the weights were
    updated using very small weight gradient values, and applied a lower precision
    arithmetic IFP16 to the multiplications and a higher precision IFP32 to the accumulations
    for the weight updates. For example, in the mixed-precision training approach
    in [[114](#bib.bib114)], IFP16 was used to store weights, activations, activation
    gradients and weight gradients, while IFP32 was used to keep the weight copies
    for their updates. Along with accumulating IFP16 operands using IFP32 arithmetic,
    the use of loss scaling allowed the mixed precision training to achieve equivalent
    accuracy to the IFP32 training while reducing the memory footprint.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: a. IFP16 + IFP32：2018年，[[114](#bib.bib114)] 发现权重在更新时使用了非常小的权重梯度值，并应用了较低精度的IFP16进行乘法运算，而较高精度的IFP32用于权重更新的累加。例如，在[[114](#bib.bib114)]中的混合精度训练方法中，IFP16用于存储权重、激活值、激活梯度和权重梯度，而IFP32用于保留权重副本以进行更新。通过使用IFP32算术来累加IFP16操作数，以及使用损失缩放，混合精度训练能够实现与IFP32训练相当的准确性，同时减少内存占用。
- en: 'b. BFloat16 + IFP32: In 2018, the mixed-precision DNN training using (BFloat16
    + IFP32) was explored in [[176](#bib.bib176)]. In 2019, [[86](#bib.bib86)] studied
    the BFloat16’s feasibility for mixed precision training for various DNNs including
    AlexNet, ResNet, GAN, etc., and concluded that (BFloat16 + IFP32) scheme outperformed
    the (IFP16 + IFP32) scheme since BFloat16 could represent the same dynamic range
    of data as IFP32 while using fewer bits.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: b. BFloat16 + IFP32：2018年，[[176](#bib.bib176)] 探索了使用（BFloat16 + IFP32）的混合精度DNN训练。2019年，[[86](#bib.bib86)]
    研究了BFloat16在包括AlexNet、ResNet、GAN等各种DNN中进行混合精度训练的可行性，并得出结论（BFloat16 + IFP32）方案优于（IFP16
    + IFP32）方案，因为BFloat16能够使用更少的位数表示与IFP32相同的数据动态范围。
- en: e. FP8 + DLFloat In 2018, [[158](#bib.bib158)] proposed a mixed-precision training
    method that applies the 5eFP8 format (1 sign-bit, 5-bit exponent, and 2-bit for
    significand) to the multiplications and DLFloat to the accumulations in MAC operations.
    The mixed precision method improved resource efficiency such as accuracy per memory
    footprint and accuracy per memory access, compared to various (FP16 + IFP32) schemes
    with respect to different FP16 formats. Compared to the previous (FP16 + IFP32)
    methods, the chunk-accumulation and stochastic rounding schemes were additionally
    used to minimize the accuracy loss in [[158](#bib.bib158)], The chunk-based accumulation
    utilized 64 data per chunk instead of one long sequential accumulation to reduce
    rounding errors. Utilizing stochastic rounding scheme with limited precision format
    for deep learning was proposed earlier in [[60](#bib.bib60)]. [[144](#bib.bib144)]
    noted that (5eFP8 + DLFloat) training degraded accuracy for DNNs utilizing depth-wise
    convolutions such as MobileNets. To overcome this issue, [[144](#bib.bib144)]
    proposed to employ two different 8 bit floating-point formats each for forward
    and backward propagation to minimize the accuracy degradation for compressed DNNs.
    The mixed-precision training utilized 5eFP8 for backpropagation and another 8-bit
    floating-point format with (Sign, Exponent, significand) = (1, 4, 3), 4eFP8, for
    forward propagation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: e. FP8 + DLFloat：2018年，[[158](#bib.bib158)] 提出了一个混合精度训练方法，该方法在MAC操作中将5eFP8格式（1个符号位、5位指数和2位尾数）应用于乘法运算，将DLFloat应用于累加。与不同FP16格式的各种（FP16
    + IFP32）方案相比，该混合精度方法提高了资源效率，如每单位内存的准确性和每次内存访问的准确性。与之前的（FP16 + IFP32）方法相比，[[158](#bib.bib158)]
    还使用了块累加和随机舍入方案来最小化准确性损失。块累加方法每块使用64个数据，而不是一个长的顺序累加，以减少舍入误差。早期的[[60](#bib.bib60)]
    提出了使用有限精度格式的随机舍入方案进行深度学习。[[144](#bib.bib144)] 指出（5eFP8 + DLFloat）训练在使用深度卷积如MobileNets的DNN中降低了准确性。为了解决这个问题，[[144](#bib.bib144)]
    提议采用两种不同的8位浮点格式分别用于前向和反向传播，以最小化压缩DNN的准确性退化。混合精度训练使用5eFP8进行反向传播，另一种8位浮点格式（符号、指数、尾数）=（1、4、3），4eFP8，用于前向传播。
- en: 'c. DLFloat only: In 2019, [[6](#bib.bib6)] employed the DLFloat for entire
    training procedure, removing the necessity of data conversions between the multiplications
    and the accumulations and found that DLFloat could provide better balance between
    dynamic range and precision than IFP16 and BFloat16 for LSTM networks [[74](#bib.bib74)]
    using Penn Treebank dataset. The DLFloat arithmetic units removed subnormal mode
    and supported the round-to-nearest up mode to minimize computational complexity.
    In [[6](#bib.bib6)], the DLFloat arithmetic showed equivalent performance to IFP32
    on ResNet-32 using CIFAR10 and ResNet-50 using ImageNet, while using a half of
    IFP32 bit width.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: DLFloat 仅：在 2019 年，[[6](#bib.bib6)] 采用 DLFloat 进行整个训练过程，消除了乘法和累加之间的数据转换的必要性，并发现
    DLFloat 可以提供比 IFP16 和 BFloat16 更好的动态范围和精度平衡，尤其是在使用 Penn Treebank 数据集的 LSTM 网络
    [[74](#bib.bib74)] 中。DLFloat 算法单元去除了次正常模式，支持最近舍入模式以最小化计算复杂度。在 [[6](#bib.bib6)]
    中，DLFloat 算法在使用 CIFAR10 的 ResNet-32 和使用 ImageNet 的 ResNet-50 上显示出与 IFP32 相当的性能，同时使用了一半的
    IFP32 位宽。
- en: 'f. INT8-based: Yang et al. [[172](#bib.bib172)] noticed that previously proposed
    mixed-precision training schemes did not quantize the data in the batch normalization
    layer, requiring high floating-point arithmetic in some parts of the data paths.
    To overcome this issue, [[172](#bib.bib172)] proposed a unified INT8-based quantization
    framework that quantizes all data paths in DNN including weights, activation,
    gradient, batch normalization, weight update, etc. into INT8-based data. However,
    this training method degraded the accuracy to some extent. In 2020, Zhu et al. [[186](#bib.bib186)]
    improved the accuracy, compared to the work of [[172](#bib.bib172)] while keeping
    unified INT8-based quantization framework. [[186](#bib.bib186)] minimized the
    deviation of the activation gradient direction between before and after quantization
    by measuring the distance during runtime based on the inner product between the
    two normalized gradient vectors generated before and after quantization.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: INT8 基于：杨等人 [[172](#bib.bib172)] 注意到以前提出的混合精度训练方案没有对批归一化层中的数据进行量化，这在数据路径的某些部分需要高精度浮点运算。为了解决这个问题，[[172](#bib.bib172)]
    提出了一个统一的 INT8 基于量化框架，将 DNN 中的所有数据路径，包括权重、激活、梯度、批归一化、权重更新等，都量化为 INT8 数据。然而，这种训练方法在某种程度上降低了准确性。在
    2020 年，朱等人 [[186](#bib.bib186)] 在保持统一的 INT8 基于量化框架的同时，改善了准确性。[[186](#bib.bib186)]
    通过基于量化前后生成的两个归一化梯度向量之间的内积，在运行时测量距离，最小化激活梯度方向的偏差。
- en: 'g. Layer-Wise Adaptive Fixed-Point Training: In 2020, Zhang et al. [[182](#bib.bib182)]
    proposed a layer-wise adaptive quantization scheme. For example, activation gradient
    distributions at fully connected layers followed a narrower distribution, requiring
    more bit-width for the quantizations. [[182](#bib.bib182)] quantized AlexNet using
    INT8 for all the weights and activations and both INT8 (22%) and INT16 (78%) for
    the activation gradients. The quantized AlexNet achieved equivalent accuracy to
    the one using IFP32 for entire training on ImageNet dataset.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 分层自适应定点训练：在 2020 年，张等人 [[182](#bib.bib182)] 提出了分层自适应量化方案。例如，完全连接层中的激活梯度分布遵循更窄的分布，要求更多的比特宽度进行量化。[[182](#bib.bib182)]
    使用 INT8 对 AlexNet 的所有权重和激活进行量化，并对激活梯度同时使用 INT8（22%）和 INT16（78%）。量化后的 AlexNet 在
    ImageNet 数据集上达到了与使用 IFP32 进行全训练的 AlexNet 相当的准确性。
- en: 4.3.2 Block Floating-Point Training
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 块浮点训练
- en: Block Floating-Point (BFP) format utilizes a shared exponent for a series of
    numbers in a data block in order to reduce data-size [[161](#bib.bib161)]. Applying
    BFP to DNNs can improve the resource efficiency in terms of accuracy per memory
    footprint and per memory access. In addition, BFP utilizes less transistors for
    simpler adders and multipliers than FP adders and multipliers, resulting in improving
    accuracy per Joule. Various versions of DNN training methods using BFP were proposed
    to improve resource efficiency.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 块浮点（BFP）格式利用数据块中一系列数字的共享指数，以减少数据大小 [[161](#bib.bib161)]。将 BFP 应用于 DNN 可以提高在每个内存占用和每个内存访问的准确性方面的资源效率。此外，与
    FP 加法器和乘法器相比，BFP 使用的晶体管更少，适用于更简单的加法器和乘法器，从而提高了每焦耳的准确性。各种使用 BFP 的 DNN 训练方法版本被提出，以提高资源效率。
- en: 'a. Flexpoint: A DNN-optimized BFP format, Flexpoint [[90](#bib.bib90)], was
    proposed by Intel, and it was used with the Nervana neural processors. The BFP
    format used 5 bits for a shared exponent and 16 bits for the significand for the
    data in a data block. Flexpoint utilized the format of (Flex N)+M, where Flex
    N represents variable number of bits for the shared exponent according to the
    different epochs, and M represents the number of bits for the separated significand.
    For example, the number of exponent bits is adapted based on the dynamic range
    of the weight values depending on the number of iterations; the dynamic range
    of the weight values at the current iteration was predicted at the previous iteration.
    The (Flex N + 16) format produced equivalent accuracy to IFP32 in AlexNet using
    ImageNet dataset and a ResNet using CIFAR-10 dataset, resulting in significant
    resource efficiency improvement in terms of accuracy per memory footprint and
    accuracy per memory access.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 'a. Flexpoint: 一种针对 DNN 优化的 BFP 格式，Flexpoint [[90](#bib.bib90)]，由 Intel 提出，并与
    Nervana 神经处理器一起使用。该 BFP 格式使用 5 位用于共享指数，16 位用于数据块中的有效数字。Flexpoint 采用 (Flex N)+M
    格式，其中 Flex N 表示根据不同的训练阶段共享指数的位数，M 表示分离的有效数字的位数。例如，指数位数根据权重值的动态范围进行调整，权重值的动态范围在当前迭代中是基于上一迭代的预测。
    (Flex N + 16) 格式在使用 ImageNet 数据集的 AlexNet 和使用 CIFAR-10 数据集的 ResNet 中产生了与 IFP32
    相当的精度，从而在每次内存占用的精度和每次内存访问的精度方面显著提高了资源效率。'
- en: 'b. BFP + FP training: Drumond et al. [[45](#bib.bib45)] proposed a hybrid use
    of BFP and FP for DNN training that uses BFP only for MAC operations and FP for
    the other operations. Such hybrid training method brought 8.5 $\times$ potential
    throughput improvement with minor accuracy loss in WideResNet28-10 using CIFAR-100
    dataset on a Stratix V FPGA.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 'b. BFP + FP 训练: Drumond 等人 [[45](#bib.bib45)] 提出了 BFP 和 FP 混合使用的 DNN 训练方法，其中
    BFP 仅用于 MAC 操作，FP 用于其他操作。这种混合训练方法在使用 CIFAR-100 数据集的 WideResNet28-10 中带来了 8.5 倍的潜在吞吐量提升，同时精度损失很小。'
- en: 'c. Block MiniFloat: [[49](#bib.bib49)] noticed that ordinary BFP formats were
    limited in minimizing original data loss with fewer bits and improving arithmetic
    density per memory access for deep learning applications. To address the two issues,
    Fox et al. [[49](#bib.bib49)] proposed the Block Minifloat (BM) along with customized
    hardware circuit design. The BM<e,m> format follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 'c. Block MiniFloat: [[49](#bib.bib49)] 发现普通的 BFP 格式在用更少的位数最小化原始数据丢失和提高深度学习应用中的每次内存访问的算术密度方面有限。为了解决这两个问题，Fox
    等人 [[49](#bib.bib49)] 提出了 Block Minifloat (BM) 以及定制的硬件电路设计。BM<e,m> 格式如下：'
- en: '|  | $y_{out}=\begin{cases}\text{normal mode:}&amp;(-1)^{sign}\times(1\times
    2^{0}+d_{1}\times 2^{-1}+...+d_{m}\times 2^{-m})\times 2^{exponent-bias-BIAS_{SE}}\\
    \text{subnormal mode:}&amp;(-1)^{sign}\times(d_{1}\times 2^{-1}+...+d_{m}\times
    2^{-m})\times 2^{1-bias-BIAS_{SE}},\end{cases}$ |  | (7) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{out}=\begin{cases}\text{normal mode:}&amp;(-1)^{sign}\times(1\times
    2^{0}+d_{1}\times 2^{-1}+...+d_{m}\times 2^{-m})\times 2^{exponent-bias-BIAS_{SE}}\\
    \text{subnormal mode:}&amp;(-1)^{sign}\times(d_{1}\times 2^{-1}+...+d_{m}\times
    2^{-m})\times 2^{1-bias-BIAS_{SE}},\end{cases}$ |  | (7) |'
- en: where $bias=2^{e-1}-1$ and $BIAS_{SE}$ is a shared exponent value. $BIAS_{SE}$
    is scaled according to the maximum value of the data for dot-product operations.
    For example, BM<2,3> represents a 6-bit data format having 1 sign bit, 2-bit exponent,
    and 3-bit significand. Such BM variant formats were applied for training. Utilizing
    these 6-bit BM formats produced equivalent accuracy to IFP32 formats but with
    fewer bits using CIFAR 10 and 100 dataset with ResNet-18, resulting in reduced
    memory-traffic and low energy consumption. Therefore, BM improved the resource
    efficiency in term of accuracy per memory access.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $bias=2^{e-1}-1$，$BIAS_{SE}$ 是共享指数值。$BIAS_{SE}$ 根据数据的最大值进行缩放以进行点积操作。例如，BM<2,3>
    表示一个 6 位的数据格式，具有 1 位符号位、2 位指数和 3 位有效数字。这种 BM 变体格式在训练中应用。利用这些 6 位 BM 格式产生了与 IFP32
    格式相当的精度，但使用了更少的位数，并使用 CIFAR 10 和 100 数据集与 ResNet-18 进行了训练，从而减少了内存流量和低能耗。因此，BM
    在每次内存访问的精度方面提高了资源效率。
- en: 5 Implementation-Level Resource-Efficient Techniques
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实施层面的资源效率技术
- en: 'Fig. [8](#S5.F8 "Figure 8 ‣ 5 Implementation-Level Resource-Efficient Techniques
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques") classifies the implementation-level resource-efficient techniques.
    Most implementation-level techniques have focused on improving energy efficiency
    and computational speed for MAC operations, since MACs generally occupy more than
     90% of computational workload for both training and inference tasks in DNNs [[145](#bib.bib145)].
    The implementation-level resource-efficient techniques exploited the characteristics
    of MACs in DNN including data reuse, sparsity of weights and activations, and
    weight repetition from quantized DNNs.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8](#S5.F8 "图 8 ‣ 5 实现层面资源效率技术 ‣ 资源高效深度学习：关于模型、算术和实现层面技术的调查") 分类了实现层面的资源效率技术。大多数实现层面技术专注于提升
    MAC 操作的能效和计算速度，因为 MAC 操作通常占 DNN 训练和推理任务计算负荷的 90% 以上 [[145](#bib.bib145)]。实现层面的资源效率技术利用了
    DNN 中 MAC 操作的特点，包括数据重用、权重和激活的稀疏性以及量化 DNN 的权重重复。
- en: '![Refer to caption](img/67533633a9419a48b6bc90999bef3957.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/67533633a9419a48b6bc90999bef3957.png)'
- en: 'Figure 8: Implementation-level resource-efficient techniques.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：实现层面的资源效率技术。
- en: 5.1 Leveraging Data Reuse from Convolution
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 利用卷积中的数据重用
- en: 'The weights and the activations are heavily reused in convolution operations.
    For example, the weights of a filter are reused $((H-k_{H}+1)\times(W-k_{W}+1))/stride$
    times, where $H=W=4$ (height and width at input channel) and $k_{H}=k_{W}=3$ (height
    and width for a kernel) in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.4 Convolutional Neural
    Network: ‣ 2.1 Deep Learning Overview ‣ 2 Background on Deep Learning and Resource-Efficiency
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques"). Generally, $H$ and $W$ are three orders of magnitude (e.g., 128,
    256, etc), $k_{H}$ and $k_{W}$ are one order of magnitude (e.g., 3, 5, etc), and
    $stride$ is either $1$ or $2$. For example, if $H=W=128$, $k_{H}=k_{W}=3$, and
    $stride=1$, each filter is reused $16129$ times for convolutional operations.
    Each input element at a covolutional layer is also reused approximately $M\times
    k_{H}\times k_{W}$ times, where $M$ is the number of the total kernels used in
    the layer. Fig. [9](#S5.F9 "Figure 9 ‣ 5.1 Leveraging Data Reuse from Convolution
    ‣ 5 Implementation-Level Resource-Efficient Techniques ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")
    describes the data access patterns for MAC operations used for convolutional layers.
    In each MAC computation in Fig. [9](#S5.F9 "Figure 9 ‣ 5.1 Leveraging Data Reuse
    from Convolution ‣ 5 Implementation-Level Resource-Efficient Techniques ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").(a),
    the data, a, b, and c, are read from the memory for multiply and add computation,
    and the result d is written back to the memory, where c contains a partial sum
    for the MAC. To save energy consumption, highly reused data for MAC computations
    can be stored in small local memory as shown in Fig. [9](#S5.F9 "Figure 9 ‣ 5.1
    Leveraging Data Reuse from Convolution ‣ 5 Implementation-Level Resource-Efficient
    Techniques ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques").(b). For example, the power consumption
    required to access data depends on where the data are located – accessing data
    from off-chip memory, DRAM, generally requires two orders of magnitude more than
    from on-chip memory [[31](#bib.bib31)]. For commercially available CPUs or GPUs,
    transforming convolutional operations into matrix multiplications can leverage
    such data reuse properties to accelerate the convolution operations by utilizing
    highly optimized BLAS libraries [[155](#bib.bib155)]. Many research works presented
    how to leverage such data reuse properties to improve the resource efficiency.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '在卷积操作中，权重和激活值被大量重复使用。例如，一个滤波器的权重会被重复使用$((H-k_{H}+1)\times(W-k_{W}+1))/stride$次，其中$H=W=4$（输入通道的高度和宽度），$k_{H}=k_{W}=3$（卷积核的高度和宽度），如图[3](#S2.F3
    "Figure 3 ‣ 2.1.4 Convolutional Neural Network: ‣ 2.1 Deep Learning Overview ‣
    2 Background on Deep Learning and Resource-Efficiency ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")所示。通常，$H$和$W$为三个数量级（例如，128、256等），$k_{H}$和$k_{W}$为一个数量级（例如，3、5等），$stride$为$1$或$2$。例如，如果$H=W=128$，$k_{H}=k_{W}=3$，且$stride=1$，每个滤波器在卷积操作中会被重复使用$16129$次。每个卷积层的输入元素也会被重复使用约$M\times
    k_{H}\times k_{W}$次，其中$M$是该层使用的卷积核总数。如图[9](#S5.F9 "Figure 9 ‣ 5.1 Leveraging Data
    Reuse from Convolution ‣ 5 Implementation-Level Resource-Efficient Techniques
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques")所示，描述了用于卷积层的MAC操作的数据访问模式。在图[9](#S5.F9 "Figure 9 ‣ 5.1 Leveraging Data
    Reuse from Convolution ‣ 5 Implementation-Level Resource-Efficient Techniques
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques").(a)中的每个MAC计算中，数据a、b和c从内存中读取用于乘加计算，结果d写回内存，其中c包含MAC的部分和。为了节省能量消耗，高度重复使用的数据可以存储在小型局部内存中，如图[9](#S5.F9
    "Figure 9 ‣ 5.1 Leveraging Data Reuse from Convolution ‣ 5 Implementation-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques").(b)所示。例如，访问数据所需的功耗取决于数据的位置——从片外内存（DRAM）访问数据通常需要比从片内存访问数据多两个数量级[[31](#bib.bib31)]。对于商业可用的CPU或GPU，通过将卷积操作转化为矩阵乘法，可以利用这种数据重用特性，通过使用高度优化的BLAS库[[155](#bib.bib155)]来加速卷积操作。许多研究工作展示了如何利用这些数据重用特性来提高资源效率。'
- en: '![Refer to caption](img/2f7ab022d096db7755ca2882841d5bcd.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f7ab022d096db7755ca2882841d5bcd.png)'
- en: 'Figure 9: Multiply-and-accumulate dataflow.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：乘加数据流。
- en: '5.1.1 Employing SRAM Local Memory near to Processing Elements:'
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 在处理单元附近使用SRAM局部内存：
- en: 'The use of SRAM buffers reduces the energy consumed by DNNs by up to two orders
    of magnitude, compared to DRAM. Similar to Fig. [9](#S5.F9 "Figure 9 ‣ 5.1 Leveraging
    Data Reuse from Convolution ‣ 5 Implementation-Level Resource-Efficient Techniques
    ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level
    Techniques").(b), Dianao architecture [[30](#bib.bib30)] employed one Neural Functional
    Unit (NFU) integrated with three separated local buffers, each for holding $16$
    input neurons, $16\times 16$ weights, and $16$ output neurons, in order to optimize
    circuitry for MAC operations. The weights and the activations stored to the local
    memories were reused efficiently by additionally using internal registers to store
    the partial sums and the circular buffer. The NFU is a three-stage pipelined architecture
    consisting of the multiplication, the adder-tree, and the activation stage. In
    the multiplication stage, $256$ multipliers support the multiplications based
    on the weight connections between $16$ input and $16$ output neurons. In the adder-tree
    stage, $16$ feature maps are generated from the multiplications based on adder-tree
    structure. In the activation stage, the $16$ feature maps are approximated for
    the $16$ activations by using piece-wise linear function approximation. DianNao
    with $65nm$ ASIC layout brought up to $118\times$ speedup and reduced the energy
    consumption by $21\times$, compared to a 128-bit 2GHz SIMD processor over the
    benchmarks used in [[30](#bib.bib30)]. One of the following studies adapted DianNao
    to deploy it on a supercomputer and named it as DaDianNao [[33](#bib.bib33)].
    Since the number of weights is generally larger than the number of input activations
    for convolution operations, DaDianNao stored a big chunk of weights and shared
    them to multiple NFUs by using a central embedded DRAM to reduce the data movement
    cost in delivering the weights associated to each NFU.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: SRAM 缓冲区的使用使得 DNN 的能耗相比于 DRAM 降低了两个数量级。类似于图 [9](#S5.F9 "图 9 ‣ 5.1 利用卷积中的数据重用
    ‣ 5 实现级资源高效技术 ‣ 资源高效深度学习：模型、算术和实现级技术的调查")。 (b)，Dianao 架构 [[30](#bib.bib30)] 采用了一个神经功能单元（NFU），它集成了三个独立的本地缓冲区，每个缓冲区用于保存
    $16$ 个输入神经元、$16\times 16$ 权重和 $16$ 个输出神经元，以优化 MAC 操作的电路设计。通过额外使用内部寄存器来存储部分和和循环缓冲区，局部内存中存储的权重和激活被高效重用。NFU
    是一个三阶段的流水线架构，包括乘法阶段、加法树阶段和激活阶段。在乘法阶段，$256$ 个乘法器支持基于 $16$ 个输入和 $16$ 个输出神经元之间的权重连接的乘法操作。在加法树阶段，从乘法操作中生成
    $16$ 个特征图，基于加法树结构。在激活阶段，通过使用分段线性函数逼近来对 $16$ 个特征图进行 $16$ 次激活的近似。与 [[30](#bib.bib30)]
    中的 128 位 2GHz SIMD 处理器相比，$65nm$ ASIC 布局的 DianNao 提供了高达 $118\times$ 的加速，并将能耗降低了
    $21\times$。以下研究之一将 DianNao 适配到超级计算机上，并将其命名为 DaDianNao [[33](#bib.bib33)]。由于卷积操作中的权重数量通常大于输入激活的数量，DaDianNao
    存储了大量权重，并通过使用中央嵌入式 DRAM 共享这些权重，以减少将权重传递到每个 NFU 的数据移动成本。
- en: '5.1.2 Leveraging Spatial Architecture:'
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 利用空间架构：
- en: 'Designing PEs and their local memory according to data reuse properties of
    MAC operations improved energy efficiency on FPGAs and ASIC [[31](#bib.bib31),
    [32](#bib.bib32)]. For example, Google TPU employs a systolic array architecture
    to send the data directly to an adjacent Processing Element (PE) as shown in Fig. [9](#S5.F9
    "Figure 9 ‣ 5.1 Leveraging Data Reuse from Convolution ‣ 5 Implementation-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques").(c) [[3](#bib.bib3)].
    Chen et al. [[31](#bib.bib31)] noticed that the computational throughput and energy
    consumption of CNNs mainly depended on data movement rather than computation and
    proposed a “row-stationary” spatial architecture (a variant of Fig. [9](#S5.F9
    "Figure 9 ‣ 5.1 Leveraging Data Reuse from Convolution ‣ 5 Implementation-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques").(c)), Eyeriss, to support
    parallel processing with minimal data movement energy cost by fully exploiting
    the data reuse property. For example, the three PEs in the first column in Fig [9](#S5.F9
    "Figure 9 ‣ 5.1 Leveraging Data Reuse from Convolution ‣ 5 Implementation-Level
    Resource-Efficient Techniques ‣ Resource-Efficient Deep Learning: A Survey on
    Model-, Arithmetic-, and Implementation-Level Techniques").(c) can be assigned
    to compute the first row of the convolution output using a $3\times 3$ filter
    - the three elements on each row of the kernel are stored to the local memory
    on each PE (i.e., “row-stationary” structure in [[145](#bib.bib145)]), and all
    the elements in the kernel are reused during convolution, generating the first
    row of the output. In this case, the partial summation values are stored back
    to the local memory on each PE.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '5.1.3 Circuit Optimization:'
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exploring binary weights [[38](#bib.bib38)] with binary inputs offered the opportunity
    to explore XNOR gates for the efficient implementation of CNN [[123](#bib.bib123)],
    improving the accuracy per memory foot print and per Joule. In 2021, [[183](#bib.bib183)]
    proposed hardware-friendly statistic-based quantization units and near data processing
    engines to accelerate mixed precision training schemes by minimizing the number
    of accesses to higher precision data.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Leveraging Sparsity of Weights and Activations
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the forward pass, negative feature map values are converted to zeros after
    ReLU activation functions, making the activation data structure sparse. In addition,
    the trained weight values follow a sharp Gaussian distribution centered at zero,
    locating most of the weights near to zero. Quantizing such weights makes the weight
    data structure sparse, so the sparse weights can be fully exploited on the quantized
    networks such as binarized DNNs [[39](#bib.bib39), [38](#bib.bib38)] and ternary
    weight DNNs [[98](#bib.bib98), [185](#bib.bib185)].
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '5.2.1 Skipping Operations during Runtime:'
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 运行时跳过操作：
- en: In 2016, several methods to conditionally skip MAC operations were proposed
    simultaneously [[31](#bib.bib31), [10](#bib.bib10), [103](#bib.bib103)]. Eyeriss [[31](#bib.bib31)]
    employed clock gating to block the convolution operations during runtime when
    either the weight or the activation was detected as zero in order to save computational
    power. Cnvlutin [[10](#bib.bib10)] skipped MAC operations associated with zero
    activations by employing separated “neuron lanes” according to different channels.
    Similarly, [[103](#bib.bib103)] proposed Cambricon-X that fetches the activations
    associated with any non-zero weights for convolutions by using the “step indexing”
    in order to skip the MAC operations associated with the zero weights. Cambricon-X
    brought 6 $\times$ resource-efficiency improvement in terms of accuracy per Joule,
    compared to the original DianNao architecture. In 2017, Kim et al. [[87](#bib.bib87)]
    proposed ZeNa that performs MAC operations only if both the weights and the activations
    are non-zero values. In 2018, Akhlaghi et al. [[8](#bib.bib8)] proposed a runtime
    technique, SnaPEA, that performs MAC operations associated with positive weights
    first and then negative weights later while monitoring the sign of the partial
    sum value. Since the activation values from ReLU are always greater or equal to
    zero, the convolution operation can be terminated once the partial sum value becomes
    negative. Notice that such decision should be performed during runtime, since
    the zero valued activation patterns depend on the test images. In 2021, another
    method skipping zero operations, GoSPA [[41](#bib.bib41)], was proposed, which
    is similar to ZeNa in that MAC operations were performed only when both input
    activations and weights were non-zero values. [[41](#bib.bib41)] constructed “Static
    Sparsity Filter" module by leveraging the property that the weight values are
    static while the activation values are dynamic to filter out zero activations
    associated with non-zero weights on the fly before MAC operations. Such skipping
    operation optimization techniques improved the accuracy per Joule, since the transistors
    associated with skipped operations were not toggled during runtime, saving dynamic
    power consumption.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年，几种有条件跳过MAC操作的方法被同时提出[[31](#bib.bib31), [10](#bib.bib10), [103](#bib.bib103)]。Eyeriss [[31](#bib.bib31)]
    采用时钟门控技术来阻止卷积操作，当权重或激活值被检测为零时，从而节省计算能力。Cnvlutin [[10](#bib.bib10)] 通过根据不同通道采用分开的“神经元通道”来跳过与零激活相关的MAC操作。同样，[[103](#bib.bib103)]
    提出了Cambricon-X，该方法利用“步进索引”来获取与任何非零权重相关的激活值，从而跳过与零权重相关的MAC操作。与原始DianNao架构相比，Cambricon-X在每焦耳准确度方面带来了6倍的资源效率提升。2017年，Kim等人[[87](#bib.bib87)]
    提出了ZeNa，该方法仅在权重和激活值都为非零值时才执行MAC操作。2018年，Akhlaghi等人[[8](#bib.bib8)] 提出了一个运行时技术SnaPEA，该技术首先执行与正权重相关的MAC操作，然后再执行与负权重相关的操作，同时监控部分和值的符号。由于ReLU的激活值始终大于或等于零，一旦部分和值变为负值，卷积操作可以终止。请注意，这种决策应在运行时进行，因为零值激活模式依赖于测试图像。2021年，另一种跳过零操作的方法GoSPA
    [[41](#bib.bib41)] 被提出，其与ZeNa类似，即仅在输入激活值和权重都为非零值时才执行MAC操作。[[41](#bib.bib41)] 通过利用权重值静态而激活值动态的特性，构建了“静态稀疏性滤波器”模块，以在MAC操作之前动态过滤掉与非零权重相关的零激活值。这些跳过操作的优化技术提高了每焦耳的准确度，因为与跳过的操作相关的晶体管在运行时未被切换，从而节省了动态功耗。
- en: '5.2.2 Encoding Sparse Weights/Activations/Feature Maps:'
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 编码稀疏权重/激活值/特征图：
- en: Since memory access operations dominate the power consumption in deep learning
    applications, fetching the weights less frequently from memory by encoding and
    compressing the weights and the activations can improve the resource efficiency
    such as the accuracy per memory footprint, per memory access, and per Joule. Han
    et al. [[65](#bib.bib65), [64](#bib.bib64)] utilized the Huffman encoding scheme
    to compress the weights. The quantized DNN reduced the memory footprint of AlexNet
    on ImageNet dataset by $35$ times without losing accuracy. In [[65](#bib.bib65),
    [64](#bib.bib64)], a three stage pipelined operation was performed in order to
    reduce the memory footprint of DNNs as follows. The pruned sparse quantized weights
    were stored with Compressed Sparse Row (CSR) format and then divided into several
    groups. The weights in the same group were shared with the average value over
    the weights in the group, and they were re-trained thereafter. Huffman coding
    was used to compress the weights further. Parashar et al. [[119](#bib.bib119)]
    employed an encoding scheme to compress sparse weights and activations and designed
    an associated dataflow, "Compressed-Sparse Convolutional Neural Networks (SCNN)",
    to minimise data transfer and reduce memory footprint. Aimar et al. [[7](#bib.bib7)]
    proposed NullHop that encodes the sparse feature maps by using two sequentially
    ordered (i.e., internally indexed) additional storage, one for a 3D mask to indicate
    the positions of non-zero values and the other for storing the non-zero data sequentially
    in the feature map. For example, ‘0’s are marked at the position of zero values
    in the 3D mask, otherwise ‘1’s are marked. Decoding refers to both the 3D mask
    and the non-zero value list. Rhu et al. [[125](#bib.bib125)] presented HashedNet
    that utilizes a low cost hash function to compress sparse activations. The virtualized
    DNN (vDNN) [[126](#bib.bib126)] compressed sparse activation units using the “zero-value
    compression” technique to minimize the data transfer cost between GPU and CPU.
    The vDNN allowed users to utilize both GPU and CPU memory for DNN training.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存访问操作在深度学习应用中占据了主要的功耗，通过对权重和激活进行编码和压缩，减少从内存中提取权重的频率，可以提高资源效率，如每单位内存占用的准确性、每次内存访问的准确性以及每焦耳的准确性。Han等人[[65](#bib.bib65),
    [64](#bib.bib64)] 利用Huffman编码方案来压缩权重。量化的DNN将AlexNet在ImageNet数据集上的内存占用减少了$35$倍而不失去准确性。在[[65](#bib.bib65),
    [64](#bib.bib64)]中，执行了一个三阶段的流水线操作，以减少DNN的内存占用。剪枝后的稀疏量化权重以压缩稀疏行（CSR）格式存储，然后被分成几个组。相同组中的权重通过组内权重的平均值进行共享，然后重新训练。Huffman编码用于进一步压缩权重。Parashar等人[[119](#bib.bib119)]采用了一种编码方案来压缩稀疏权重和激活，并设计了一个相关的数据流，“压缩稀疏卷积神经网络（SCNN）”，以最小化数据传输并减少内存占用。Aimar等人[[7](#bib.bib7)]
    提出了NullHop，该方法通过使用两个顺序排列的额外存储来编码稀疏特征图，一个用于3D掩码以指示非零值的位置，另一个用于在特征图中顺序存储非零数据。例如，‘0’标记在3D掩码中零值的位置，否则标记为‘1’。解码涉及到3D掩码和非零值列表。Rhu等人[[125](#bib.bib125)]
    提出了HashedNet，该网络利用低成本的哈希函数来压缩稀疏激活。虚拟DNN (vDNN) [[126](#bib.bib126)] 使用“零值压缩”技术来压缩稀疏激活单元，以最小化GPU与CPU之间的数据传输成本。vDNN允许用户同时利用GPU和CPU内存进行DNN训练。
- en: '5.2.3 Decomposing Kernel Matrix:'
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 分解核矩阵
- en: Li et al. [[100](#bib.bib100)] proposed SqueezeFlow that reduces the number
    of operations for convolutions by decomposing the kernel matrix into non-zero
    valued sub-matrices and zero-valued sub-matrices. This method can improve the
    accuracy per Joule.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Li等人[[100](#bib.bib100)] 提出了SqueezeFlow，该方法通过将卷积核矩阵分解为非零值子矩阵和零值子矩阵来减少卷积操作的数量。该方法可以提高每焦耳的准确性。
- en: 5.3 Leveraging Weight Repetition in Quantized DNNs
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 利用量化DNN中的权重重复
- en: 'Hedge et al. [[71](#bib.bib71)] noticed that the identical weight values were
    often repeated in quantized DNNs such as binary weight DNNs [[39](#bib.bib39),
    [38](#bib.bib38)] and ternary weight DNNs [[98](#bib.bib98), [185](#bib.bib185)]
    and proposed the Unique Weight CNN Accelerator (UCNN) that reduces the number
    of memory accesses and the number of operations by leveraging the repeated weight
    values in the quantized DNNs. For example, if a $2\times 2$ kernel consisting
    of $\{k_{1,1},k_{1,2},k_{2,1},k_{2,2}\}$ performs a convolution with the activation
    maps, $\{a_{1,1},a_{1,2},a_{2,1},a_{2,2}\}$. The conventional covolutional operation,
    $\Sigma_{i=1,j=1}^{i=2,j=2}k_{i,j}\times a_{i,j}$, requires $8$ read memory accesses,
    $4$ multiplications, and $3$ additions. If two of the weights in the kernel are
    identical (e.g., $k_{1,1}=k_{2,2}$ and $k_{1,2}=k_{2,1}$), the covolutional operation
    can be performed using: $k_{1,1}(a_{1,1}+a_{2,2})+k_{1,2}(a_{1,2}+a_{2,1})$, requiring
    $6$ read memory accesses, $2$ multiplications, and $3$ additions. The UCNN improved
    the accuracy per Joule by $1.2-4\times$ in AlexNet and LeNet on Eyeriss architecture
    using ImageNet dataset.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Hedge 等人 [[71](#bib.bib71)] 注意到在量化的深度神经网络（DNNs）中，例如二进制权重 DNNs [[39](#bib.bib39),
    [38](#bib.bib38)] 和三元权重 DNNs [[98](#bib.bib98), [185](#bib.bib185)]，相同的权重值经常重复出现，并提出了独特的权重卷积神经网络加速器（UCNN），通过利用量化
    DNNs 中的重复权重值，减少内存访问次数和运算次数。例如，如果一个 $2\times 2$ 的卷积核包含 $\{k_{1,1},k_{1,2},k_{2,1},k_{2,2}\}$，与激活图
    $\{a_{1,1},a_{1,2},a_{2,1},a_{2,2}\}$ 进行卷积操作。传统的卷积操作 $\Sigma_{i=1,j=1}^{i=2,j=2}k_{i,j}\times
    a_{i,j}$ 需要 $8$ 次读内存访问、$4$ 次乘法和 $3$ 次加法。如果卷积核中的两个权重相同（例如 $k_{1,1}=k_{2,2}$ 和 $k_{1,2}=k_{2,1}$），卷积操作可以通过以下方式进行：$k_{1,1}(a_{1,1}+a_{2,2})+k_{1,2}(a_{1,2}+a_{2,1})$，只需
    $6$ 次读内存访问、$2$ 次乘法和 $3$ 次加法。UCNN 在 Eyeriss 架构上使用 ImageNet 数据集，提升了 AlexNet 和 LeNet
    的每焦耳准确度 $1.2-4\times$。
- en: 5.4 Leveraging Innovative Technology
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 利用创新技术
- en: Many research attempts have leveraged innovative computing architecture technologies
    such as neuromorphic computing and in-memory processing as follows.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究尝试利用了创新的计算架构技术，如神经形态计算和内存处理，具体如下。
- en: '5.4.1 Neuromorphic Computing:'
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 神经形态计算：
- en: Neuromorphic computing mimics the brain, including brain components such as
    neurons and synapses; furthermore, biological neural systems include axons, dendrites,
    glial cells, and spiking signal transferring mechanisms [[84](#bib.bib84)]. The
    memristor, ‘memory resistor’, is one of the most widely used devices in neuromorphic
    computing. ISAAC [[136](#bib.bib136)] replaced MAC operation units with the memristor
    crossbars based on DaDianNao architecture. In the crossbars, every wire in horizontal
    wire array was connected to every wire in vertical wire array with a resistor.
    Different level voltages, $V=[v_{1},v_{2},...,v_{m}]$, were applied to the horizontal
    wires connected to a vertical wire by the different resistors, $R=[1/g_{1},1/g_{2},...,1/g_{m}]$.
    With mapping $v_{i}$ to input elements and $g_{i}$ to weights, where $i=1,...,m$,
    the output current, $I$, from the vertical wire can be represented as MAC operations
    in a layer, $I=\Sigma_{i}^{m}(v_{i}\times g_{i})$, based on the Kirchhoff’s law.
    Multiple MAC operations can be performed by collecting the currents from multiple
    vertical wires. ISAAC employed digital-to-analog converters to receive the input
    elements and covert them into the appropriate voltages and analog-to-digital converters
    to convert the current values into digitized feature map values. Due to lack of
    re-programability of resistors in the crossbars, ISAAC architecture was only available
    for the inference tasks. ISAAC improved $5.5\times$ accuracy per Joule, compared
    to full-fledged DaDianNao. As another neuromorphic computing approaches, many
    research attempts implemented Hodgkin-Huxley and Morris Lecar models [[75](#bib.bib75)]
    that describes the activity of neurons using nonlinear differential equations
    in hardware simulators [[109](#bib.bib109), [92](#bib.bib92), [129](#bib.bib129),
    [139](#bib.bib139), [142](#bib.bib142), [140](#bib.bib140), [141](#bib.bib141),
    [56](#bib.bib56), [16](#bib.bib16), [15](#bib.bib15), [46](#bib.bib46)]. Several
    studies implemented neuromorphic architectures in ASIC, including TrueNorth [[9](#bib.bib9)],
    SpiNNaker [[117](#bib.bib117)], Neurogrid [[18](#bib.bib18)], BrainScaleS [[134](#bib.bib134)],
    and IFAT [[121](#bib.bib121)]. Please refer to [[135](#bib.bib135)] for a comprehensive
    survey of neuromorphic computing.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '5.4.2 In-Memory Processing:'
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Scaling down the size of transistors enables energy efficiency and high performance
    for Von-Neumann computing systems. However, it became very challenging in the
    era of sub-10nm technologies due to physical limitations [[51](#bib.bib51), [113](#bib.bib113),
    [11](#bib.bib11)]. To address this challenge, researchers proposed a paradigm
    of in-memory processing to improve performance and energy efficiency by integrating
    computations units into memory devices [[63](#bib.bib63), [168](#bib.bib168),
    [101](#bib.bib101)]. Several studies proposed to enable in-memory processing to
    accelerate DNNs  [[12](#bib.bib12), [43](#bib.bib43), [68](#bib.bib68), [88](#bib.bib88),
    [52](#bib.bib52), [175](#bib.bib175)]. For example, XNOR-SRAM [[175](#bib.bib175)]
    integrated the XNOR gates and accumulation logic into SRAM to fetch data from
    SRAM and perform MAC operation in one cycle. Notice that this approach was applicable
    for binarized DNNs such as [[39](#bib.bib39)].
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Adaptive Compute Resource Assignment
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection comprises the methods assigning runtime compute resources adaptively
    to the DNN inference workload to improve resource efficiency. The implementation
    of the DNNs can be adapted to the accuracy requirements of the applications by
    using various runtime implementation techniques as follows.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '5.5.1 Early Exiting:'
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The required depth of DNN depends on the problem complexity. The “early exiting”
    technique allows a DNN to classify an object as early as possible by having multiple
    exit classifier points in a single DNN [[118](#bib.bib118), [150](#bib.bib150),
    [151](#bib.bib151)]. The early exiting technique was applied to distributed computing
    systems, addressing concern about privacy, response time, and higher quality of
    experience [[151](#bib.bib151)]. Such early exiting methods minimized the compute
    resources and the inference latency, improving the accuracy per Joule, per operation,
    and per core utilization. Please refer to [[111](#bib.bib111)] for the details
    on the early exiting techniques.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '5.5.2 Runtime Channel Width Adaptation:'
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The runtime channel width adaptation pruned unimportant filters during runtime.
    In 2018, Fang et. al [[47](#bib.bib47)] presented a single DNN model, NestDNN,
    being able to switch between multiple capacities of the DNN during runtime according
    to the accuracy and inference latency requirement. During training, unimportant
    filters from the original model were pruned to generate the smallest possible
    model, “seed model”. Each re-training, some of pruned filters were added to the
    seed model while fixing the filter parameters from the previous training. Since
    the seed model was descended from the original model, the accuracy for each capacity
    in NestDNN was higher than the model having the identical capacity trained from
    the scratch. Similarly, Yu et. al [[177](#bib.bib177)] proposed another runtime
    switchable DNN model, Slimmable Neural Network, in which a larger capacity model
    shared the filter parameters from a smaller capacity model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '5.5.3 Runtime Model Switching:'
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lee et al. [[96](#bib.bib96)] selected the best performing object detectors
    between multiple DNN detectors during runtime according to dynamic video content
    to improve the accuracy per core utilization and per Joule. Lou et al. [[106](#bib.bib106)]
    switched between multiple DNNs, generated from the Once-For-All NAS of [[25](#bib.bib25)],
    during runtime according to dynamic workload. For example, when the inference
    latency of a DNN was increased due to a newly assigned workload, a runtime decision
    maker downgraded the current DNN during runtime to meet a latency constraint.
    Such runtime model switching approaches were appropriate when memory resources
    were sufficient, since the multiple DNNs should be pre-loaded in DRAM.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 6 Interrelated Influences
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section discusses the influence from higher- to lower-level techniques
    as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Resource-Efficient Deep
    Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Influences of Model-Level Techniques on Arithmetic-Level Techniques
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Weight quantization [[38](#bib.bib38), [98](#bib.bib98), [185](#bib.bib185)]
    in model-level techniques influenced arithmetic-level techniques as shown in Fig. [7](#S4.F7
    "Figure 7 ‣ 4 Arithmetic-Level resource-efficient Techniques ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques").
    The multiplications using the quantized binary weights can be replaced with multiplexers,
    removing multiplication arithmetic operations. The resource efficiency from the
    model-level techniques can be further improved by utilizing the arithmetic-level
    techniques. For example, quantized DNNs such as ternary weight and binarized DNNs
    allowed INT8 arithmetic to be used in training [[166](#bib.bib166), [172](#bib.bib172)].
    When reduced precision DNNs suffered from zero gradients, the reduced precision
    arithmetic was replaced with a hybrid version arithmetic using both BFP and FP
    [[45](#bib.bib45)] or the Block MiniFloat format [[49](#bib.bib49)].'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '权重量化[[38](#bib.bib38), [98](#bib.bib98), [185](#bib.bib185)]在模型级技术中的影响如图[7](#S4.F7
    "Figure 7 ‣ 4 Arithmetic-Level resource-efficient Techniques ‣ Resource-Efficient
    Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques")所示，影响了算术级技术。使用量化的二进制权重的乘法可以用多路复用器替代，从而去除乘法算术运算。通过利用算术级技术，可以进一步提高模型级技术的资源效率。例如，量化的DNN，如三值权重和二值化DNN，使得训练中可以使用INT8算术[[166](#bib.bib166),
    [172](#bib.bib172)]。当降低精度的DNN出现零梯度时，降低精度的算术被替换为使用BFP和FP的混合版本算术[[45](#bib.bib45)]或Block
    MiniFloat格式[[49](#bib.bib49)]。'
- en: 6.2 Influences of Model-Level Techniques on Implementation-Level Techniques
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 模型级技术对实现级技术的影响
- en: 'Weight quantization in model-level techniques influenced the implementation-level
    techniques as shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5 Implementation-Level Resource-Efficient
    Techniques ‣ Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-,
    and Implementation-Level Techniques"). Pruning weights can bring sparsity in the
    hardware architecture while pruning filters [[107](#bib.bib107), [99](#bib.bib99)]
    maintains dense structure. Weight quantization in the model-level techniques allows
    a DNN to utilize fewer bits for weights in order to save memory resource usage,
    requiring customized hardware. For example, EIE [[64](#bib.bib64)] is an inference
    accelerator with weights quantized by 4 bits. To implement the weight quantization
    method effectively, EIE utilized weight sharing to reduce the model size further
    and fit the compressed DNN into the on-chip SRAM. Exploring binary weights [[39](#bib.bib39)]
    with binary inputs offered the opportunity to explore XNOR gates for the efficient
    implementation of CNNs [[123](#bib.bib123)], improving the accuracy per memory
    footprint and per Joule. In [[153](#bib.bib153)], ternary neural networks [[98](#bib.bib98),
    [185](#bib.bib185)] were implemented on FPGAs by unrolling convolution operations.
    Since quantized DNNs [[98](#bib.bib98), [185](#bib.bib185)] increased the number
    of repeated weights in DNNs, UCNN [[71](#bib.bib71)] leveraged the property of
    the repeated weight values in quantized DNNs to improve resource efficiency such
    as accuracy per memory access and per operation. As main limitation, the weight
    quantization methods such as [[38](#bib.bib38), [185](#bib.bib185), [98](#bib.bib98)]
    were not suitable for commercially available CPUs and GPUs, since such computing
    platforms do not support binary and ternary weights in hardware. Therefore, the
    implementation of weight quantization methods on CPUs or GPUs might not improve
    accuracy per Joule as higher precision arithmetic still was required in part of
    data path in training and inference. The bottleneck structures generated by compact
    convolutions in [[132](#bib.bib132), [82](#bib.bib82)] can be used to reduce the
    data size transferred between a local device and an edge server for the efficient
    implementation of edge-based AI [[111](#bib.bib111)].'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Influences of Arithmetic-Level Techniques on Implementation-Level Techniques
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The arithmetic-level techniques influenced the implementation-level techniques
    as follows.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: First, the research in arithmetic utilization acted as a catalyst for commodity
    CPUs and GPUs. For example, the mixed precision research [[114](#bib.bib114)]
    laid a foundation for tensor cores in latest NVIDIA GPUs, which can accelerate
    the performance of deep learning workloads by supporting a fused multiply–add
    operation and the mixed precision training capability in hardware [[19](#bib.bib19)].
    The BFloat16 format [[24](#bib.bib24)] designed by Google overcomes the limited
    accuracy issue of the IFP16 format by providing the same dynamic range as IFP32,
    and it is supported in hardware in Intel Cooper Lake Xeon processors, NVIDIA A100
    GPUs, and Google TPUs. In 2016, NVIDIA Pascal GPUs supported IFP16 arithmetic
    in hardware to accelerate DNN applications. In 2017, NVIDIA Volta GPUs supported
    IFP16 tensor cores. In 2020, the NVIDIA Ampere architecture supported tensor cores,
    TF32, BFloat16, and sparsity acceleration in hardware to accelerate MACs [[2](#bib.bib2)].
    The Graphcore company developed the Intelligent Processing Unit (IPU), which employs
    local memory assigned to each processing unit with support for a large number
    of independently operating hardware threads [[85](#bib.bib85)]. The IPU is an
    efficient computing architecture customized to “fine-grained, irregular computation
    that exhibits irregular data access”.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the arithmetic-level techniques led to specialized custom accelerators
    for deep learning. There is ample evidence in the arithmetic-level literature
    such as [[49](#bib.bib49), [45](#bib.bib45), [21](#bib.bib21), [158](#bib.bib158),
    [170](#bib.bib170)] that even smaller operators (e.g., 16 bits or even less) have
    almost no impact on the accuracy of DNNs. For example, DianNao [[30](#bib.bib30)]
    and DaDianNao [[33](#bib.bib33)] were customized to 16-bit fixed-point arithmetic
    operators instead of word-size (e.g., 32-bit) floating-point operators. ISAAC [[136](#bib.bib136)]
    is a fully-fledged crossbar-based CNN accelerator architecture, which implemented
    a memristor-based logic based on resistive memory, suitable for 16-bit arithmetic
    for DNN workloads. Wang et al. [[158](#bib.bib158)] designed their customized
    8-bit floating point arithmetic multiplications with 16-bit accumulations on an
    ASIC-based hardware platform with a $14nm$ silicon technology to support energy-efficient
    deep learning training. The Eyeriss [[31](#bib.bib31)] and SnaPEA [[8](#bib.bib8)]
    accelerators were customized to 16-bit arithmetic. UCNN [[71](#bib.bib71)] utilized
    8-bit and 16-bit fixed point configurations. SCNN [[119](#bib.bib119)] utilized
    16-bit multiplication and 24-bit accumulation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the mixed precision training schemes were accelerated in hardware by
    minimizing the data conversion overhead between lower and higher precision formats
    in updating weights and activations [[183](#bib.bib183)]. Also, the stochastic
    rounding scheme was supported in hardware in Intel Loihi processor [[40](#bib.bib40)]
    and Graphcore IPU [[85](#bib.bib85)], since it was often required for quantizing
    weights and activations during training [[166](#bib.bib166), [60](#bib.bib60),
    [172](#bib.bib172)].
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 7 Future Trend for Resource-Efficient Deep Learning
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open research issues in resource-efficient deep learning emerge in an attempt
    to improve the resource efficiency further, compared to the state-of-the-art resource-efficient
    techniques discussed in this paper.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Future Trend for Model-Level Resource-Efficient Techniques
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, edge-based computing has become pervasive, and fitting DNN models
    into such resource-constrained devices for inference tasks has become extremely
    challenging.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '7.1.1 Improving Physical Resource Efficiency under Very Low Compute Resource
    Budget:'
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many researchers considered keeping dense network structures after pruning parameters,
    including pruning channels [[104](#bib.bib104), [53](#bib.bib53)], filters [[99](#bib.bib99),
    [107](#bib.bib107)], etc., to implement the pruned networks efficiently on commercially
    available CPUs and GPUs. Since then, various budget-aware network pruning methods
    were proposed, given a resource budget such as the number of floating point operations
    [[57](#bib.bib57)] and the number of neurons [[97](#bib.bib97)] for the inference
    task. NetAdapt [[171](#bib.bib171)] pruned the filters as it measured physical
    resources such as latency, energy, memory footprint, etc. to improve the physical
    resource efficiency directly rather than abstract resource efficiency. Along with
    the fast technology development in computer networks and wireless communications,
    research attempts to improve physical resource efficiency are expected to continue
    to deploy appropriate DNN models on extremely low resource devices such as mobile,
    IoT, and edge devices.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '7.1.2 Neural Network Search Methods Combined with Domain Specific Knowledge:'
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2016 and 2017, handcrafted compressed DNNs were presented such as SqueezeNet
    [[82](#bib.bib82)], MobileNet [[76](#bib.bib76)], ShuffleNet [[181](#bib.bib181)],
    and DenseNet [[79](#bib.bib79)], and they improved both abstract and physical
    resource efficiency. Various NAS methods [[148](#bib.bib148), [149](#bib.bib149),
    [147](#bib.bib147), [70](#bib.bib70)] assisted to seek the optimized DNN models
    (e.g., least sufficient models) by searching candidate spaces according to the
    training dataset, and the compressed models found by the NAS methods generally
    showed superior physical resource efficiency to the handcrafted compressed DNNs.
    As mobile and edge devices become prevalent, we expect that automatic search methods
    integrating with domain specific model compression methods are expected to be
    paid attention in the future. For example, performance-aware NAS methods for resource-constrained
    devices have been vividly paid attention [[13](#bib.bib13), [147](#bib.bib147),
    [149](#bib.bib149), [148](#bib.bib148), [171](#bib.bib171)] since 2019\. Such
    performance-aware NAS methods can be enhanced by adopting recent domain specific
    model-level resource-efficient techniques such as [[65](#bib.bib65), [82](#bib.bib82),
    [76](#bib.bib76), [99](#bib.bib99), [50](#bib.bib50)].
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '7.1.3 Theoretical Studies Behind Model-Level Resource-Efficient Techniques:'
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The bias-variance trade-off [[54](#bib.bib54)] is behind the model-level resource-efficient
    techniques. For example, compressed models having fewer parameters increase the
    regularization effect on the accuracy, minimizing overfitting issues [[65](#bib.bib65),
    [54](#bib.bib54)]. For example, [[50](#bib.bib50)] proposed the lottery ticket
    hypothesis in that better (or equivalent) performing sub-DNNs using fewer weights
    exist inside a dense, randomly-initialized, feed-forward DNN. In order to seek
    the better performing sparse sub-DNNs, “winning tickets”, the survived weights
    from weight pruning were re-trained by replacing the survived weights with the
    random weights initially used to train the original dense DNN. The lottery ticket
    hypothesis implies that such sparse DNNs could be found in even compressed dense
    DNNs. We expect that such theoretical studies supporting model-level resource-efficient
    techniques can be paid attention in the future.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Future Trend for Arithmetic-Level Resource-Efficient Techniques
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As edge- and mobile-based devices becomes pervasive for AI applications, open
    research issues emerge in the attempts to improve further physical resource efficiency
    on such resource-constrained devices, compared to state-of-the-art arithmetic-level
    techniques.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Adapting Arithmetic Precision Level to Numerical Properties of DNNs
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2011, Vanhoucke et al. [[155](#bib.bib155)] demonstrated the feasibility
    of INT8 arithmetic for inference tasks using a shallow depth neural network on
    an Intel x86 architecture. In 2015, Gupta et al. [[60](#bib.bib60)] demonstrated
    that employing FiP16 with a stochastic rounding scheme for a shallow depth neural
    network produced equivalent accuracy using MNIST and CIFAR10 to that using IFP32\.
    In 2018, [[114](#bib.bib114)] presented the guidelines for mixed precision training.
    The guidelines contained the information on how to deploy different-level arithmetic
    precision on different computing components in MAC operations. The guidelines
    led to further research attempts in hardware optimization for mixed precision
    training [[183](#bib.bib183)]. We expect that the research attempts in adapting
    an arithmetic precision to DNN computing components according to their numerical
    stability characteristics will continue in the future.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '7.2.2 Adapting Arithmetic Format to Problem Complexity:'
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since floating point arithmetic is computationally intensive, several studies
    have removed floating point arithmetic in training tasks. For example, Wu et al. [[166](#bib.bib166)]
    demonstrated that quantized networks such as binary or ternary weight networks
    can be trained using INT8 arithmetic along with a scaling and a stochastic rounding
    scheme. In 2020, Yang et al. [[172](#bib.bib172)] demonstrated that quantizing
    weights and activations with INT8 format while applying INT24 arithmetic to the
    weight updates could accelerate training and inference tasks for various ResNet
    models using ImageNet with minor accuracy loss, compared to those using IFP32\.
    Recently, RNS-based quantization was applied to various DNNs [[131](#bib.bib131),
    [130](#bib.bib130)]. It will continue in the future to explore how to adapt a
    number format to given DNN structures for inference and training tasks in order
    to improve resource efficiency further on resource-constrained devices.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Future Trend for Implementation-Level Resource-Efficient Techniques
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, there are two ways to accelerate DNN computations. One is to optimize
    DNN computations on given compute architecture such as CPUs and GPUs. The other
    is to customize dataflow on FPGAs and ASIC.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '7.3.1 Leveraging Spatial and Temporal Data Access Pattern with Lower Precision
    Arithmetic on CPUs and GPUs:'
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A decade ago, [[154](#bib.bib154)] accelerated DNN computations on a SIMD CPU
    by leveraging the data reuse property from MAC operations using SSE instructions
    and fast fixed point arithmetic. NVIDIA Tensor cores and Google TPUs support a
    customized arithmetic precision format such as IFP16, BFloat16, etc. and customized
    datapath in hardware for deep learning applications [[2](#bib.bib2), [3](#bib.bib3)].
    The research attempts to leverage spatial and temporal data access patterns with
    lower precision arithmetic in commercially available CPUs and GPUs will continue
    in the future.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '7.3.2 Leveraging Spatial and Temporal Data Access Pattern with Lower Precision
    Arithmetic on FPGAs and ASIC:'
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In 2014, [[30](#bib.bib30)] stressed the limitation of commercially available
    GPUs and CPUs for DNN applications: “While a cache is an excellent storage structure
    for a general-purpose processor, it is a sub-optimal way to exploit reuse because
    of the cache access overhead (tag check, associativity, line size, speculative
    read, etc.) and cache conflicts.” To overcome this limitation, [[30](#bib.bib30)]
    proposed a SIMD style hardware accelerator, DianNao, that employs three separate
    local on-chip memories (SRAMs) to maximize the performance by fully leveraging
    the data reuse property. [[33](#bib.bib33)] pointed out that DianNao was still
    limited in the memory bandwidth to access massive weights in covolutional layers
    and proposed DaDianNao architecture employing large eDRAMs with four banks to
    store and share the weights in the eDRAMs efficiently. In 2016, [[31](#bib.bib31)]
    pointed out that the data movement cost is still dominant, compared to the computation
    cost for DNN applications on the SIMD/SIMT architectures of [[33](#bib.bib33),
    [30](#bib.bib30)] and proposed a dataflow architecture to minimize energy consumption
    caused by data movement in DNN applications. Since 2016, most implementation-level
    techniques leveraged the sparsity of weights and activations in DNNs to minimize
    the number of arithmetic operations during runtime [[10](#bib.bib10), [103](#bib.bib103),
    [87](#bib.bib87), [8](#bib.bib8), [41](#bib.bib41)] and the data transfer cost
    required to store and transfer the sparse weights and activations [[64](#bib.bib64),
    [7](#bib.bib7), [119](#bib.bib119), [125](#bib.bib125), [126](#bib.bib126)]. The
    research efforts to customize DNN dataflow by leveraging spatial and temporal
    data access patterns with lower precision arithmetic are expected to continue
    in the future.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '7.3.3 Resource-Efficient Implementation on Distributed AI Compute Platforms:'
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Resource-efficient techniques on distributed AI such as split federated learning
    [[62](#bib.bib62), [152](#bib.bib152)] and early exiting [[150](#bib.bib150),
    [151](#bib.bib151)] have recently attracted a great deal of attention thanks to
    fast wireless network technology development. The main open research issues include
    data communication overhead between an edge device and the cloud and energy consumption,
    required to run DNNs on a lower power (or battery) edge device. For example, many
    research attempts leveraged the bottleneck structure of a DNN to save the data
    communication bandwidth, but such attempts could degrade the accuracy significantly
    in the DNNs employing compact convolutions [[111](#bib.bib111)]. Thus, adapting
    such model compression techniques to distributed AI environments can be paid attention
    in the future in order to save energy consumption on edge devices and the bandwidth
    required for communication between an edge device and a cloud. For example, encoding
    and decoding offloading data from the cloud to edge devices or vice versa can
    minimize the data communication overhead [[173](#bib.bib173)]. Such resource-efficient
    encoding/decoding schemes for split learning (or inference) tasks can draw attention
    in the future.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '7.3.4 Neuromorphic Computing for Deep Learning:'
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neuromorphic computing can lead to dramatic changes in energy efficiency for
    deep learning [[135](#bib.bib135)]. This expectation is based on the fact that
    neuromorphic computing is not an incremental improvement of existing von Neumann
    architectures requiring considerable energy due to substantial instruction fetch/decode
    operations, but an fully optimized dataflow optimization customized to the activity
    of a neural network. Therefore, neuromorphic computing research can be paid attention
    in the future to maximize the accuracy per Joule.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our survey is the first to provide a comprehensive survey coverage of the recent
    resource-efficient deep learning techniques based on the three-level hierarchy
    including model-, arithmetic-, and implementation-level techniques. Our survey
    also utilizes multiple resource efficiency metrics to clarify which resource efficiency
    metrics each technique can improve. For example, most model-level resource-efficient
    techniques contribute to improving abstract resource efficiency, while the arithmetic-
    and the implementation-level techniques directly contribute to improving physical
    resource efficiency by employing reduced precision arithmetic and/or optimizing
    the dataflow of DNN architectures. Therefore, the efficient implementation of
    the model-level techniques on given compute platforms is essential to improve
    physical resource efficiency [[145](#bib.bib145)].
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: In the future, we expect that the three-level resource-efficient deep learning
    techniques can be adapted to distributed AI applications, along with fast wireless
    communication technology development. Since edge or mobile devices are subjected
    to physical resource constraints such as power, memory, and inference speed, the
    implementation should consider such constraints for the distributed AI applications.
    The state-of-the-art works include the NAS variants of [[147](#bib.bib147), [133](#bib.bib133),
    [115](#bib.bib115)] that seek the optimal performing DNN models fitted to the
    resource-constrained edge-devices. Improving such NAS variants by combining them
    with various model-, arithmetic-, and implementation-level resource-efficient
    techniques can be paid attention in the future.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Finally, our survey suggests that the bias-variance trade-off [[54](#bib.bib54)]
    is behind the model-level resource-efficient techniques. According to the trade-off,
    a DNN having fewer parameters increases the regularization effect on the accuracy,
    minimizing overfitting issues. Therefore, there exists the least sufficient model
    size that produces the best accuracy on test dataset according to the problem
    complexity and the training data quantity and quality. Similarly, [[50](#bib.bib50)]
    claimed the lottery ticket hypothesis in that better (or equivalent) performing
    sub-DNNs using fewer weights exist inside a dense feed-forward DNN. Einstein quoted
    “Everything should be made as simple as possible, but not simpler.” We hope that
    our survey will contribute to machine learning, arithmetic, and system community
    by providing them with a comprehensive survey for various resource-efficient deep
    learning techniques as guidelines to seek DNN structures using least sufficient
    parameters and least sufficient precision arithmetic on particular compute platforms,
    customized to the problem complexity and the training data quantity and quality.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: This project has received funding by the Engineering and Physical Sciences Research
    Council under the grant agreement No. EP/T022345/1 and by CHIST-ERA under the
    grant agreement No. CHIST-ERA-18-SDCDN-002 (DiPET). This research was also partially
    supported by National R&D Program through the National Research Foundation of
    Korea (NRF) funded by Ministry of Science and ICT (2021M3H2A1038042)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Imagenet benchmark (image classification on imagenet). [https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Nvidia ampere architecture white paper. [https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf).'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Quantifying the performance of the TPU, our first machine learning chip.
    [https://cloud.google.com/blog/products/gcp/quantifying-the-performance-of-the-tpu-our-first-machine-learning-chip](https://cloud.google.com/blog/products/gcp/quantifying-the-performance-of-the-tpu-our-first-machine-learning-chip).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Ieee standard for floating-point arithmetic. IEEE Std 754-2019 (Revision
    of IEEE 754-2008), pages 1–84, 2019.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning
    From Data. AMLBook, 2012.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Agrawal, S. M. Mueller, B. M. Fleischer, X. Sun, N. Wang, J. Choi, and
    K. Gopalakrishnan. Dlfloat: A 16-b floating point format designed for deep learning
    training and inference. In 2019 IEEE 26th Symposium on Computer Arithmetic (ARITH),
    pages 92–95, 2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Alessandro Aimar, Hesham Mostafa, Enrico Calabrese, Antonio Rios-Navarro,
    Ricardo Tapiador-Morales, Iulia-Alexandra Lungu, Moritz B. Milde, Federico Corradi,
    Alejandro Linares-Barranco, Shih-Chii Liu, and Tobi Delbruck. Nullhop: A flexible
    convolutional neural network accelerator based on sparse representations of feature
    maps. IEEE Transactions on Neural Networks and Learning Systems, 30(3):644–656,
    2019.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] V. Akhlaghi, A. Yazdanbakhsh, K. Samadi, R. K. Gupta, and H. Esmaeilzadeh.
    Snapea: Predictive early activation for reducing computation in deep convolutional
    neural networks. In 2018 ACM/IEEE 45th Annual International Symposium on Computer
    Architecture (ISCA), pages 662–673, Los Alamitos, CA, USA, jun 2018\. IEEE Computer
    Society.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Filipp Akopyan, Jun Sawada, Andrew Cassidy, Rodrigo Alvarez-Icaza, John
    Arthur, Paul Merolla, Nabil Imam, Yutaka Nakamura, Pallab Datta, Gi-Joon Nam,
    Brian Taba, Michael Beakes, Bernard Brezzo, Jente B. Kuang, Rajit Manohar, William P.
    Risk, Bryan Jackson, and Dharmendra S. Modha. Truenorth: Design and tool flow
    of a 65 mw 1 million neuron programmable neurosynaptic chip. IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems, 34(10):1537–1557,
    2015.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright
    Jerger, and Andreas Moshovos. Cnvlutin: Ineffectual-neuron-free deep neural network
    computing. In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture
    (ISCA), pages 1–13, 2016.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Mustafa F. Ali, Robert Andrawis, and Kaushik Roy. Dynamic read current
    sensing with amplified bit-line voltage for stt-mrams. IEEE Transactions on Circuits
    and Systems II: Express Briefs, 67(3):551–555, 2020.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Mustafa F. Ali, Akhilesh Jaiswal, and Kaushik Roy. In-memory low-cost
    bit-serial addition using commodity dram technology. IEEE Transactions on Circuits
    and Systems I: Regular Papers, 67(1):155–165, 2020.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Andrew Anderson, Jing Su, Rozenn Dahyot, and David Gregg. Performance-oriented
    neural architecture search, 2020.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances
    in Neural Information Processing Systems (NeurIPS), 2014.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Arindam Basu. Small-signal neural models and their applications. IEEE
    Transactions on Biomedical Circuits and Systems, 6(1):64–75, 2012.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Arindam Basu, Csaba Petre, and Paul Hasler. Bifurcations in a silicon
    neuron. In 2008 IEEE International Symposium on Circuits and Systems, pages 428–431,
    2008.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Yoshua Bengio. Deep learning of representations for unsupervised and transfer
    learning. In Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and
    Daniel Silver, editors, Proceedings of ICML Workshop on Unsupervised and Transfer
    Learning, volume 27 of Proceedings of Machine Learning Research, pages 17–36,
    Bellevue, Washington, USA, 02 Jul 2012\. PMLR.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Ben Varkey Benjamin, Peiran Gao, Emmett McQuinn, Swadesh Choudhary, Anand R.
    Chandrasekaran, Jean-Marie Bussat, Rodrigo Alvarez-Icaza, John V. Arthur, Paul A.
    Merolla, and Kwabena Boahen. Neurogrid: A mixed-analog-digital multichip system
    for large-scale neural simulations. Proceedings of the IEEE, 102(5):699–716, 2014.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Pierre Blanchard, Nicholas J Higham, Florent Lopez, Theo Mary, and Srikara
    Pranesh. Mixed precision block fused multiply-add: Error analysis and application
    to gpu tensor cores. SIAM Journal on Scientific Computing, 42(3):C124–C141, 2020.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal
    speed and accuracy of object detection, 2020.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] R. Bordawekar, B. Abali, and M.H. Chen. Efloat: Entropy-coded floating
    point format for deep learning. arXiv:2102.02705, 2021.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Léon Bottou and Yann Le Cun. Large scale online learning. In Advances
    in Neural Information Processing Systems 16. MIT Press, 2004.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Cristian Buciluă, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression.
    In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining, KDD ’06, page 535–541, New York, NY, USA, 2006\. Association
    for Computing Machinery.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] N. Burgess, J. Milanovic, N. Stephens, K. Monachopoulos, and D. Mansell.
    Bfloat16 processing for neural networks. In 2019 IEEE 26th Symposium on Computer
    Arithmetic (ARITH), pages 88–91, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all:
    Train one network and specialize it for efficient deployment. In ICLR ’20: International
    Conference on Learning Representations, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Z. Carmichael, H. F. Langroudi, C. Khazanov, J. Lillie, J. L. Gustafson,
    and D. Kudithipudi. Deep positron: A deep neural network using the posit number
    system. In 2019 Design, Automation Test in Europe Conference Exhibition (DATE),
    pages 1421–1426, 2019.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Zachariah Carmichael, Hamed F. Langroudi, Char Khazanov, Jeffrey Lillie,
    John L. Gustafson, and Dhireesha Kudithipudi. Performance-efficiency trade-off
    of low-precision numerical formats in deep neural networks. In Proceedings of
    the Conference for Next Generation Arithmetic 2019, CoNGA’19, New York, NY, USA,
    2019\. Association for Computing Machinery.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Chip-Hong Chang, Amir Sabbagh Molahosseini, Azadeh Alsadat Emrani Zarandi,
    and Tian Fatt Tay. Residue number systems: A new paradigm to datapath optimization
    for low-power and high-performance digital signal processing applications. IEEE
    Circuits and Systems Magazine, 15(4):26–44, 2015.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker.
    Learning efficient object detection models with knowledge distillation. In I. Guyon,
    U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
    editors, Advances in Neural Information Processing Systems 30, pages 742–751\.
    Curran Associates, Inc., 2017.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,
    and Olivier Temam. Diannao: A small-footprint high-throughput accelerator for
    ubiquitous machine-learning. SIGPLAN Not., 49(4):269–284, February 2014.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Chen, J. Emer, and V. Sze. Eyeriss: A spatial architecture for energy-efficient
    dataflow for convolutional neural networks. In 2016 ACM/IEEE 43rd Annual International
    Symposium on Computer Architecture (ISCA), pages 367–379, 2016.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Yu-Hsin Chen, Tushar Krishna, Joel S. Emer, and Vivienne Sze. Eyeriss:
    An energy-efficient reconfigurable accelerator for deep convolutional neural networks.
    IEEE Journal of Solid-State Circuits, 52(1):127–138, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling
    Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, and Olivier Temam. Dadiannao: A machine-learning
    supercomputer. In Proceedings of the 47th Annual IEEE/ACM International Symposium
    on Microarchitecture, MICRO-47, pages 609–622, Washington, DC, USA, 2014\. IEEE
    Computer Society.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus
    Rohrbach, Shuicheng Yan, and Jiashi Feng. Drop an octave: Reducing spatial redundancy
    in convolutional neural networks with octave convolution. In Proceedings of the
    IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Cheng, D. Wang, P. Zhou, and T. Zhang. Model compression and acceleration
    for deep neural networks: The principles, progress, and challenges. IEEE Signal
    Processing Magazine, 35(1):126–136, 2018.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Francois Chollet. Xception: Deep learning with depthwise separable convolutions.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), July 2017.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] NVIDIA Corporation. NVIDIA Tesla V100 GPU architecture, 8 2017. WP-08608-001v1.1.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect:
    Training deep neural networks with binary weights during propagations. In C. Cortes,
    N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural
    Information Processing Systems 28, pages 3123–3131\. Curran Associates, Inc.,
    2015.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
    Bengio. Binarized neural networks: Training deep neural networks with weights
    and activations constrained to +1 or -1, 2016.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang
    Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain,
    Yuyun Liao, Chit-Kwan Lin, Andrew Lines, Ruokun Liu, Deepak Mathaikutty, Steven
    McCoy, Arnab Paul, Jonathan Tse, Guruguhanathan Venkataramanan, Yi-Hsin Weng,
    Andreas Wild, Yoonseok Yang, and Hong Wang. Loihi: A neuromorphic manycore processor
    with on-chip learning. IEEE Micro, 38(1):82–99, 2018.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Chunhua Deng, Yang Sui, Siyu Liao, Xuehai Qian, and Bo Yuan. Gospa: An
    energy-efficient high-performance globally optimized sparse convolutional neural
    network accelerator. In 2021 ACM/IEEE 48th Annual International Symposium on Computer
    Architecture (ISCA), pages 1110–1123, 2021.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression
    and hardware acceleration for neural networks: A comprehensive survey. Proceedings
    of the IEEE, 108(4):485–532, 2020.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Quan Deng, Lei Jiang, Youtao Zhang, Minxuan Zhang, and Jun Yang. Dracc:
    a dram based accelerator for accurate cnn inference. In 2018 55th ACM/ESDA/IEEE
    Design Automation Conference (DAC), pages 1–6, 2018.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
    Exploiting linear structure within convolutional networks for efficient evaluation.
    In Proceedings of the 27th International Conference on Neural Information Processing
    Systems - Volume 1, NeurIPS’14, page 1269–1277, Cambridge, MA, USA, 2014\. MIT
    Press.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Mario Drumond, Tao Lin, Martin Jaggi, and Babak Falsafi. Training dnns
    with hybrid block floating point. In 32nd Conference on Neural Information Processing
    Systems, 2018.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] D. Dupeyron, S. Le Masson, Y. Deval, G. Le Masson, and J.-P. Dom. A bicmos
    implementation of the hodgkin-huxley formalism. In Proceedings of Fifth International
    Conference on Microelectronics for Neural Networks, pages 311–316, 1996.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Biyi Fang, Xiao Zeng, and Mi Zhang. Nestdnn: Resource-aware multi-tenant
    on-device deep learning for continuous mobile vision. MobiCom ’18, New York, NY,
    USA, 2018\. Association for Computing Machinery.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. Fasi, N.J. Higham, M. Mikaitis, and S. Pranesh. Numerical behavior
    of nvidia tensor cores. PeerJ Computer Science, 7(e330):1–19, 2021.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Sean Fox, Seyedramin Rasoulinezhad, Julian Faraone, David Boland, and
    Philip Leong. A block minifloat representation for training deep neural networks.
    In ICLR ’21: International Conference on Learning Representations, 2021.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding
    sparse, trainable neural networks. In ICLR ’19: International Conference on Learning
    Representations, 2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Adi Fuchs and David Wentzlaff. The accelerator wall: Limits of chip specialization.
    In 2019 IEEE International Symposium on High Performance Computer Architecture
    (HPCA), pages 1–14, 2019.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Mingyu Gao, Jing Pu, Xuan Yang, Mark Horowitz, and Christos Kozyrakis.
    Tetris: Scalable and efficient neural network acceleration with 3d memory. 45(1):751–764,
    April 2017.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Xitong Gao, Yiren Zhao, Łukasz Dudziak, Robert Mullins, and Cheng zhong
    Xu. Dynamic channel pruning: Feature boosting and suppression. In ICLR ’19: International
    Conference on Learning Representations, 2019.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Stuart Geman, Elie Bienenstock, and René Doursat. Neural networks and
    the bias/variance dilemma. Neural Computation, 4(1):1–58, 1992.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter
    Jin, Sicheng Zhao, and Kurt Keutzer. Squeezenext: Hardware-aware neural network
    design. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR) Workshops, June 2018.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Meisam Gholami and Saeed Saeedi. Digital cellular implementation of morris-lecar
    neuron model. In 2015 23rd Iranian Conference on Electrical Engineering, pages
    1235–1239, 2015.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and
    Edward Choi. Morphnet: Fast amp; simple resource-constrained structure learning
    of deep networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 1586–1595, 2018.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient
    dnns. In Proceedings of the 30th International Conference on Neural Information
    Processing Systems, NIPS’16, page 1387–1395, Red Hook, NY, USA, 2016\. Curran
    Associates Inc.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Rishi Raj Gupta and Virender Ranga. Comparative study of different reduced
    precision techniques in deep neural network. In Shailesh Tiwari, Erma Suryani,
    Andrew Keong Ng, K. K. Mishra, and Nitin Singh, editors, Proceedings of International
    Conference on Big Data, Machine Learning and their Applications, pages 123–136,
    Singapore, 2021\. Springer Singapore.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
    Deep learning with limited numerical precision. In Proceedings of the 32nd International
    Conference on International Conference on Machine Learning - Volume 37, ICML’15,
    page 1737–1746\. JMLR.org, 2015.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Gustafson and Yonemoto. Beating floating point at its own game: Posit
    arithmetic. Supercomput. Front. Innov.: Int. J., 4(2):71–86, June 2017.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Yoo Jeong Ha, Minjae Yoo, Gusang Lee, Soyi Jung, Sae Won Choi, Joongheon
    Kim, and Seehwan Yoo. Spatio-temporal split learning for privacy-preserving medical
    platforms: Case studies with covid-19 ct, x-ray, and cholesterol data. IEEE Access,
    9:121046–121059, 2021.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Nastaran Hajinazar, Geraldo F. Oliveira, Sven Gregorio, João Dinis Ferreira,
    Nika Mansouri Ghiasi, Minesh Patel, Mohammed Alser, Saugata Ghose, Juan Gómez-Luna,
    and Onur Mutlu. Simdram: A framework for bit-serial simd processing using dram.
    In Proceedings of the 26th ACM International Conference on Architectural Support
    for Programming Languages and Operating Systems, ASPLOS 2021, page 329–345, New
    York, NY, USA, 2021\. Association for Computing Machinery.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz,
    and William J. Dally. Eie: Efficient inference engine on compressed deep neural
    network. In Proceedings of the 43rd International Symposium on Computer Architecture,
    ISCA ’16, page 243–254\. IEEE Press, 2016.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. In
    ICLR ’16: International Conference on Learning Representations, 2016.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights
    and connections for efficient neural network. In C. Cortes, N. D. Lawrence, D. D.
    Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing
    Systems 28, pages 1135–1143\. Curran Associates, Inc., 2015.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), pages 770–778, 2016.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok Jeong, Seho Kim, Il Park,
    Mithuna Thottethodi, and T. N. Vijaykumar. Newton: A dram-maker’s accelerator-in-memory
    (aim) architecture for machine learning. In 2020 53rd Annual IEEE/ACM International
    Symposium on Microarchitecture (MICRO), pages 372–385, 2020.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep
    neural networks. In 2017 IEEE International Conference on Computer Vision (ICCV),
    pages 1398–1406, 2017.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc:
    Automl for model compression and acceleration on mobile devices. In Proceedings
    of the European Conference on Computer Vision (ECCV), September 2018.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Kartik Hegde, Jiyong Yu, Rohit Agrawal, Mengjia Yan, Michael Pellauer,
    and Christopher W. Fletcher. Ucnn: Exploiting computational reuse in deep neural
    networks via weight repetition. In Proceedings of the 45th Annual International
    Symposium on Computer Architecture, ISCA ’18, page 674–687\. IEEE Press, 2018.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge
    in a neural network, 2015.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Nhut-Minh Ho and Weng-Fai Wong. Exploiting half precision arithmetic in
    nvidia gpus. In 2017 IEEE High Performance Extreme Computing Conference (HPEC),
    pages 1–7, 2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    Comput., 9(8):1735–1780, nov 1997.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane
    current and its application to conduction and excitation in nerve. The Journal
    of Physiology, 117(4):500–544, 1952.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional
    neural networks for mobile vision applications, 2017.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming:
    A data-driven neuron pruning approach towards efficient deep architectures, 2016.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Huiyi Hu, Ang Li, Daniele Calandriello, , and Dilan Gorur. One pass imagenet.
    In NeurIPS 2021 Workshop on Imagenet: past, present and future, 2021.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected
    convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), pages 2261–2269, 2017.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q. Weinberger.
    Condensenet: An efficient densenet using learned group convolutions. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
    2018.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
    Bengio. Quantized neural networks: Training neural networks with low precision
    weights and activations. J. Mach. Learn. Res., 18(1):6869–6898, January 2017.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J.
    Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters
    and <0.5mb model size, 2016.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In Francis Bach and
    David Blei, editors, Proceedings of the 32nd International Conference on Machine
    Learning, volume 37 of Proceedings of Machine Learning Research, pages 448–456,
    Lille, France, 07–09 Jul 2015\. PMLR.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] E.M. Izhikevich. Which model to use for cortical spiking neurons? IEEE
    Transactions on Neural Networks, 15(5):1063–1070, 2004.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. Dissecting
    the graphcore ipu architecture via microbenchmarking. arXiv preprint arXiv:1912.03413,
    2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das,
    Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka,
    Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training.
    arXiv preprint arXiv:1905.12322, 2019.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Dongyoung Kim, Junwhan Ahn, and Sungjoo Yoo. Zena: Zero-aware neural network
    accelerator. IEEE Design Test, 35(1):39–46, 2018.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Duckhwan Kim, Jaeha Kung, Sek Chai, Sudhakar Yalamanchili, and Saibal
    Mukhopadhyay. Neurocube: A programmable digital neuromorphic architecture with
    high-density 3d memory. In 2016 ACM/IEEE 43rd Annual International Symposium on
    Computer Architecture (ISCA), pages 380–392, 2016.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun
    Shin. Compression of deep convolutional neural networks for fast and low power
    mobile applications. In ICLR ’16: International Conference on Learning Representations,
    2016.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Urs Köster, Tristan J. Webb, Xin Wang, Marcel Nassar, Arjun K. Bansal,
    William H. Constable, Oğuz H. Elibol, Scott Gray, Stewart Hall, Luke Hornof, Amir
    Khosrowshahi, Carey Kloss, Ruby J. Pai, and Naveen Rao. Flexpoint: An adaptive
    numerical format for efficient training of deep neural networks. In Proceedings
    of the 31st International Conference on Neural Information Processing Systems,
    NeurIPS’17, page 1740–1750, Red Hook, NY, USA, 2017\. Curran Associates Inc.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification
    with deep convolutional neural networks. Commun. ACM, 60(6):84–90, May 2017.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] S. Le Masson, A. Laflaquiere, T. Bal, and G. Le Masson. Analog circuits
    for modeling biological neural networks: design and applications. IEEE Transactions
    on Biomedical Engineering, 46(6):638–645, 1999.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel. Backpropagation applied to handwritten zip code recognition.
    Neural Computation, 1(4):541–551, 1989.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky,
    editor, Advances in Neural Information Processing Systems 2, volume 2\. Morgan-Kaufmann,
    1990.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] JunKyu Lee, Gregory D. Peterson, Dimitrios S. Nikolopoulos, and Hans Vandierendonck.
    Air: Iterative refinement acceleration using arbitrary dynamic precision. Parallel
    Computing, 97:102663, 2020.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] JunKyu Lee, Blesson Varghese, Roger Woods, and Hans Vandierendonck. Tod:
    Transprecise object detection to maximise real-time accuracy on the edge. In IEEE
    International Conference on Fog and Edge Computing, pages 53–60, 2021.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Carl Lemaire, Andrew Achkar, and Pierre-Marc Jodoin. Structured pruning
    of neural networks with budget-aware regularization. In 2019 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 9100–9108, 2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. In Workshop
    on Efficient Methods for Deep Neural Networks in the 30th International Conference
    on Neural Information Processing Systems, NeurIPS’16, 2016.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf.
    Pruning filters for efficient convnets. In ICLR ’17: International Conference
    on Learning Representations, 2017.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Jiajun Li, Shuhao Jiang, Shijun Gong, Jingya Wu, Junchao Yan, Guihai
    Yan, and Xiaowei Li. Squeezeflow: A sparse cnn accelerator exploiting concise
    convolution rules. IEEE Transactions on Computers, 68(11):1663–1677, 2019.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Shuangchen Li, Dimin Niu, Krishna T. Malladi, Hongzhong Zheng, Bob Brennan,
    and Yuan Xie. Drisa: A dram-based reconfigurable in-situ accelerator. In 2017
    50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages
    288–301, 2017.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning.
    In Proceedings of the 31st International Conference on Neural Information Processing
    Systems, NeurIPS’17, page 2178–2188, Red Hook, NY, USA, 2017\. Curran Associates
    Inc.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Liu, Z. Du, J. Tao, D. Han, T. Luo, Y. Xie, Y. Chen, and T. Chen.
    Cambricon: An instruction set architecture for neural networks. In 2016 ACM/IEEE
    43rd Annual International Symposium on Computer Architecture (ISCA), pages 393–405,
    2016.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang. Learning efficient
    convolutional networks through network slimming. In 2017 IEEE International Conference
    on Computer Vision (ICCV), pages 2755–2763, 2017.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Zhi-Gang Liu and Matthew Mattina. Efficient residue number system based
    winograd convolution. In European Conference on Computer Vision, pages 53–68.
    Springer, 2020.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Wei Lou, Lei Xun, Amin Sabet, Jia Bi, Jonathon Hare, and Geoff V. Merrett.
    Dynamic-ofa: Runtime dnn architecture switching for performance scaling on heterogeneous
    embedded platforms. In 2021 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Workshops (CVPRW), pages 3104–3112, 2021.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Luo, J. Wu, and W. Lin. Thinet: A filter level pruning method for
    deep neural network compression. In 2017 IEEE International Conference on Computer
    Vision (ICCV), pages 5068–5076, 2017.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2:
    Practical guidelines for efficient cnn architecture design. In Proceedings of
    the European Conference on Computer Vision (ECCV), September 2018.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Qingyun Ma, Mohammad Rafiqul Haider, Vinaya Lal Shrestha, and Yehia Massoud.
    Bursting hodgkin—huxley model-based ultra-low-power neuromimetic silicon neuron.
    Analog Integr. Circuits Signal Process., 73(1):329–337, October 2012.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Zelda Mariet and Suvrit Sra. Diversity networks: Neural network compression
    using determinantal point processes. In ICLR ’16: International Conference on
    Learning Representations, 2016.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Yoshitomo Matsubara, Marco Levorato, and Francesco Restuccia. Split computing
    and early exiting for deep learning applications: Survey and research challenges,
    2021.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Warren Mcculloch and Walter Pitts. A logical calculus of ideas immanent
    in nervous activity. Bulletin of Mathematical Biophysics, 5:127–147, 1943.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Sally A. McKee. Reflections on the memory wall. In Proceedings of the
    1st Conference on Computing Frontiers, CF ’04, page 162, New York, NY, USA, 2004\.
    Association for Computing Machinery.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich K
    Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
    Venkatesh, and Hao Wu. Mixed precision training. In ICLR ’18: International Conference
    on Learning Representations, 2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Umar Ibrahim Minhas, Lev Mukhanov, Georgios Karakonstantis, Hans Vandierendonck,
    and Roger Woods. Leveraging transprecision computing for machine vision applications
    at the edge. In 2021 IEEE Workshop on Signal Processing Systems (SiPS), pages
    205–210, 2021.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Michael A. Nielsen. Neural networks and deep learning, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Eustace Painkras, Luis A. Plana, Jim Garside, Steve Temple, Francesco
    Galluppi, Cameron Patterson, David R. Lester, Andrew D. Brown, and Steve B. Furber.
    Spinnaker: A 1-w 18-core system-on-chip for massively-parallel neural network
    simulation. IEEE Journal of Solid-State Circuits, 48(8):1943–1953, 2013.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional
    deep learning for energy-efficient and enhanced pattern recognition. In 2016 Design,
    Automation Test in Europe Conference Exhibition (DATE), pages 475–480, 2016.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan
    Venkatesan, Brucek Khailany, Joel Emer, Stephen W. Keckler, and William J. Dally.
    Scnn: An accelerator for compressed-sparse convolutional neural networks. In Proceedings
    of the 44th Annual International Symposium on Computer Architecture, ISCA ’17,
    page 27–40, New York, NY, USA, 2017. Association for Computing Machinery.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Behrooz Parhami. Computer Arithmetic: Algorithms and Hardware Designs.
    Oxford University Press, New York, 2010.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Jongkil Park, Sohmyung Ha, Theodore Yu, Emre Neftci, and Gert Cauwenberghs.
    A 65k-neuron 73-mevents/s 22-pj/event asynchronous micro-pipelined integrate-and-fire
    array transceiver. In 2014 IEEE Biomedical Circuits and Systems Conference (BioCAS)
    Proceedings, pages 675–678, 2014.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Z. Qin, Z. Zhang, X. Chen, C. Wang, and Y. Peng. Fd-mobilenet: Improved
    mobilenet with a fast downsampling strategy. In 2018 25th IEEE International Conference
    on Image Processing (ICIP), pages 1363–1367, 2018.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
    Xnor-net: Imagenet classification using binary convolutional neural networks.
    In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision
    – ECCV 2016, pages 525–542, Cham, 2016\. Springer International Publishing.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Arthur J Redfern, Lijun Zhu, and Molly K Newquist. Bcnn: A binary cnn
    with all matrix ops quantized to 1 bit precision. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 4604–4612, 2021.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M. Rhu, M. O’Connor, N. Chatterjee, J. Pool, Y. Kwon, and S. W. Keckler.
    Compressing dma engine: Leveraging activation sparsity for training deep neural
    networks. In 2018 IEEE International Symposium on High Performance Computer Architecture
    (HPCA), pages 78–91, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W.
    Keckler. Vdnn: Virtualized deep neural networks for scalable, memory-efficient
    neural network design. In The 49th Annual IEEE/ACM International Symposium on
    Microarchitecture, MICRO-49\. IEEE Press, 2016.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang,
    Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR ’15:
    International Conference on Learning Representations, 2015.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] F. Rosenblatt. The perceptron: A probabilistic model for information
    storage and organization in the brain. Psychological Review, 65(6):386–408, 1958.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Sylvain Saighi, Laure Buhry, Yannick Bornat, Gilles N’Kaoua, Jean Tomas,
    and Sylvie Renaud. Adjusting the neurons models in neuromimetic ics using the
    voltage-clamp technique. In 2008 IEEE International Symposium on Circuits and
    Systems, pages 1564–1567, 2008.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S. Salamat, M. Imani, S. Gupta, and T. Rosing. Rnsnet: In-memory neural
    network acceleration using residue number system. In 2018 IEEE International Conference
    on Rebooting Computing (ICRC), pages 1–12, 2018.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] N. Samimi, M. Kamal, A. Afzali-Kusha, and M. Pedram. Res-dnn: A residue
    number system-based dnn accelerator unit. IEEE Transactions on Circuits and Systems
    I: Regular Papers, 67(2):658–671, 2020.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
    Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Florian Scheidegger, Luca Benini, Costas Bekas, and A. Cristiano I. Malossi.
    Constrained deep neural network architecture search for iot devices accounting
    for hardware calibration. In Advances in Neural Information Processing Systems
    32, pages 6056–6066, 2019.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Johannes Schemmel, Andreas Grübl, Stephan Hartmann, Alexander Kononov,
    Christian Mayr, Karlheinz Meier, Sebastian Millner, Johannes Partzsch, Stefan
    Schiefer, Stefan Scholze, Rene Schüffny, and Marc-Olivier Schwartz. Live demonstration:
    A scaled-down version of the brainscales wafer-scale neuromorphic system. In 2012
    IEEE International Symposium on Circuits and Systems (ISCAS), pages 702–702, 2012.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Catherine D Schuman, Thomas E Potok, Robert M Patton, J Douglas Birdwell,
    Mark E Dean, Garrett S Rose, and James S Plank. A survey of neuromorphic computing
    and neural networks in hardware. arXiv preprint arXiv:1705.06963, 2017.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. Shafiee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. P. Strachan,
    M. Hu, R. S. Williams, and V. Srikumar. Isaac: A convolutional neural network
    accelerator with in-situ analog arithmetic in crossbars. In 2016 ACM/IEEE 43rd
    Annual International Symposium on Computer Architecture (ISCA), pages 14–26, 2016.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Li Shang, Alireza S. Kaviani, and Kusuma Bathala. Dynamic power consumption
    in virtex™-ii fpga family. In Proceedings of the 2002 ACM/SIGDA Tenth International
    Symposium on Field-programmable Gate Arrays, FPGA ’02, pages 157–164, New York,
    NY, USA, 2002\. ACM.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau,
    Vikas Chandra, and Hadi Esmaeilzadeh. Bit fusion: Bit-level dynamically composable
    architecture for accelerating deep neural networks. In Proceedings of the 45th
    Annual International Symposium on Computer Architecture, ISCA ’18, page 764–775\.
    IEEE Press, 2018.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Jonghan Shin and C. Koch. Dynamic range and sensitivity adaptation in
    a silicon spiking neuron. IEEE Transactions on Neural Networks, 10(5):1232–1238,
    1999.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Mario F. Simoni, Gennady S. Cymbalyuk, Michael Elliott Sorensen, Ronald L.
    Calabrese, and Stephen P. DeWeerth. Development of hybrid systems: Interfacing
    a silicon neuron to a leech heart interneuron. In Todd K. Leen, Thomas G. Dietterich,
    and Volker Tresp, editors, Advances in Neural Information Processing Systems 13,
    Papers from Neural Information Processing Systems (NeurIPS) 2000, Denver, CO,
    USA, pages 173–179\. MIT Press, 2000.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] M.F. Simoni, G.S. Cymbalyuk, M.E. Sorensen, R.L. Calabrese, and S.P.
    DeWeerth. A multiconductance silicon neuron with biologically matched dynamics.
    IEEE Transactions on Biomedical Engineering, 51(2):342–354, 2004.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] M.F. Simoni and S.P. DeWeerth. Adaptation in a vlsi model of a neuron.
    IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing,
    46(7):967–970, 1999.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Suraj Srinivas and R. Venkatesh Babu. Data-free parameter pruning for
    deep neural networks. In Proceedings of the British Machine Vision Conference
    (BMVC), pages 31.1–31.12\. BMVA Press, September 2015.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani,
    Vijayalakshmi (Viji) Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan.
    Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks.
    In Advances in Neural Information Processing Systems 32, pages 4900–4909\. Curran
    Associates, Inc., 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] V. Sze, Y. Chen, T. Yang, and J. S. Emer. Efficient processing of deep
    neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295–2329,
    2017.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Thierry Tambe, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi,
    Alexander Rush, David Brooks, and Gu-Yeon Wei. Algorithm-hardware co-design of
    adaptive floating-point encodings for resilient deep learning inference. In 2020
    57th ACM/IEEE Design Automation Conference (DAC), pages 1–6, 2020.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
    Howard, and Quoc V. Le. Mnasnet: Platform-aware neural architecture search for
    mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), June 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for
    convolutional neural networks. volume 97 of Proceedings of Machine Learning Research,
    pages 6105–6114, Long Beach, California, USA, 09–15 Jun 2019\. PMLR.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and
    efficient object detection. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), June 2020.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Surat Teerapittayanon, Bradley McDanel, and H.T. Kung. Branchynet: Fast
    inference via early exiting from deep neural networks. In 2016 23rd International
    Conference on Pattern Recognition (ICPR), pages 2464–2469, 2016.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Surat Teerapittayanon, Bradley McDanel, and H.T. Kung. Distributed deep
    neural networks over the cloud, the edge and end devices. In 2017 IEEE 37th International
    Conference on Distributed Computing Systems (ICDCS), pages 328–339, 2017.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Chandra Thapa, M. A. P. Chamikara, Seyit Camtepe, and Lichao Sun. Splitfed:
    When federated learning meets split learning, 2021.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Stephen Tridgell, Martin Kumm, Martin Hardieck, David Boland, Duncan
    Moss, Peter Zipf, and Philip H. W. Leong. Unrolling ternary neural networks. ACM
    Trans. Reconfigurable Technol. Syst., 12(4), October 2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Robert van de Geijn and Kazushige Goto. BLAS (Basic Linear Algebra Subprograms),
    pages 157–164. Springer US, Boston, MA, 2011.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Vincent Vanhoucke, Andrew Senior, and Mark Z. Mao. Improving the speed
    of neural networks on cpus. In Deep Learning and Unsupervised Feature Learning
    Workshop, NeurIPS 2011, 2011.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] S. Vogel, M. Liang, A. Guntoro, W. Stechele, and G. Ascheid. Efficient
    hardware acceleration of cnns using logarithmic data representation with arbitrary
    log-base. In 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),
    pages 1–8, 2018.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Erwei Wang, James J. Davis, Ruizhe Zhao, Ho-Cheung Ng, Xinyu Niu, Wayne
    Luk, Peter Y. K. Cheung, and George A. Constantinides. Deep neural network approximation
    for custom hardware: Where we’ve been, where we’re going. ACM Comput. Surv., 52(2),
    May 2019.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash
    Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers.
    In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
    editors, Advances in Neural Information Processing Systems 31, pages 7675–7684\.
    Curran Associates, Inc., 2018.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Peisong Wang, Xiangyu He, Qiang Chen, Anda Cheng, Qingshan Liu, and Jian
    Cheng. Unsupervised network quantization via fixed-point factorization. IEEE Transactions
    on Neural Networks and Learning Systems, 2020.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] P. J. Werbos. Backpropagation through time: what it does and how to do
    it. Proceedings of the IEEE, 78(10):1550–1560, 1990.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] J. H. Wilkinson, editor. The Algebraic Eigenvalue Problem. Oxford University
    Press, Inc., New York, NY, USA, 1988.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: An insightful
    visual performance model for multicore architectures. Commun. ACM, 52(4):65–76,
    April 2009.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] H. R. Wilson and Jack D. Cowan. Excitatory and inhibitory interactions
    in localized populations of model neurons. Biophysical Journal, 12:1–24, 1972.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming
    Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware
    efficient convnet design via differentiable neural architecture search. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
    June 2019.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius.
    Integer quantization for deep learning inference: Principles and empirical evaluation,
    2020.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference
    with integers in deep neural networks. arXiv preprint arXiv:1802.04680, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.
    Aggregated residual transformations for deep neural networks. In 2017 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 5987–5995, 2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Xin Xin, Youtao Zhang, and Jun Yang. Elp2im: Efficient and low power
    bitwise operation processing in dram. In 2020 IEEE International Symposium on
    High Performance Computer Architecture (HPCA), pages 303–314, 2020.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang
    Huang, and Xian-sheng Hua. Quantization networks. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 7308–7316, 2019.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient
    convolutional neural networks using energy-aware pruning. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler,
    Vivienne Sze, and Hartwig Adam. Netadapt: Platform-aware neural network adaptation
    for mobile applications. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu,
    and Yair Weiss, editors, Computer Vision – ECCV 2018, pages 289–304, Cham, 2018\.
    Springer International Publishing.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Yukuan Yang, Lei Deng, Shuang Wu, Tianyi Yan, Yuan Xie, and Guoqi Li.
    Training high-performance and large-scale deep neural networks with full 8-bit
    integers. Neural Networks, 125:70 – 82, 2020.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Shuochao Yao, Jinyang Li, Dongxin Liu, Tianshi Wang, Shengzhong Liu,
    Huajie Shao, and Tarek Abdelzaher. Deep compressive offloading: Speeding up neural
    network inference by trading edge computation for network latency. In Proceedings
    of the 18th Conference on Embedded Networked Sensor Systems, SenSys ’20, page
    476–488, New York, NY, USA, 2020. Association for Computing Machinery.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Reza Yazdani, Marc Riera, Jose-Maria Arnau, and Antonio González. The
    dark side of dnn pruning. In Proceedings of the 45th Annual International Symposium
    on Computer Architecture, ISCA ’18, page 790–801\. IEEE Press, 2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] S. Yin, Z. Jiang, J. Seo, and M. Seok. Xnor-sram: In-memory computing
    sram macro for binary/ternary deep neural networks. IEEE Journal of Solid-State
    Circuits, 55(6):1733–1743, 2020.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong Cheng. Image
    classification at supercomputer scale. arXiv preprint arXiv:1811.06992, 2018.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable
    neural networks. In ICLR ’19: International Conference on Learning Representations,
    2019.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] R. Yu, A. Li, C. Chen, J. Lai, V. I. Morariu, X. Han, M. Gao, C. Lin,
    and L. S. Davis. Nisp: Pruning networks using neuron importance score propagation.
    In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
    9194–9203, 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional
    networks. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors,
    Computer Vision – ECCV 2014, pages 818–833, Cham, 2014. Springer International
    Publishing.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] C. Zhang, P. Patras, and H. Haddadi. Deep learning in mobile and wireless
    networking: A survey. IEEE Communications Surveys Tutorials, 21(3):2224–2287,
    2019.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufflenet: An extremely efficient
    convolutional neural network for mobile devices. In 2018 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pages 6848–6856, 2018.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou,
    Jiaming Guo, Qi Guo, Zidong Du, Tian Zhi, and Yunji Chen. Fixed-point back-propagation
    training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), June 2020.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Yongwei Zhao, Chang Liu, Zidong Du, Qi Guo, Xing Hu, Yimin Zhuang, Zhenxing
    Zhang, Xinkai Song, Wei Li, Xishan Zhang, Ling Li, Zhiwei Xu, and Tianshi Chen.
    Cambricon-q: A hybrid architecture for efficient training. In 2021 ACM/IEEE 48th
    Annual International Symposium on Computer Architecture (ISCA), pages 706–719,
    2021.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou.
    Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth
    gradients, 2018.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary
    quantization. In ICLR ’17: International Conference on Learning Representations,
    2017.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong
    Li, Xiuqi Yang, and Junjie Yan. Towards unified int8 training for convolutional
    neural network. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), pages 1966–1976, 2020.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Barret Zoph and Quoc Le. Neural architecture search with reinforcement
    learning. In ICLR ’17: International Conference on Learning Representations, 2017.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
