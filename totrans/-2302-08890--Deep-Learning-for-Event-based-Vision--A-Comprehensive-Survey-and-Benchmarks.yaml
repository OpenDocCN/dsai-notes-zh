- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:41:50'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2302.08890] Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.08890](https://ar5iv.labs.arxiv.org/html/2302.08890)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Event-based Vision:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Comprehensive Survey and Benchmarks
  prefs: []
  type: TYPE_NORMAL
- en: Xu Zheng^∗, Yexin Liu^∗, Yunfan Lu, Tongyan Hua, Tianbo Pan, Weiming Zhang,
    Dacheng Tao, Lin Wang^†
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Event cameras are bio-inspired sensors that capture the per-pixel intensity
    changes asynchronously and produce event streams encoding the time, pixel position,
    and polarity (sign) of the intensity changes. Event cameras possess a myriad of
    advantages over canonical frame-based cameras, such as high temporal resolution,
    high dynamic range, low latency, etc. Being capable of capturing information in
    challenging visual conditions, event cameras have the potential to overcome the
    limitations of frame-based cameras in the computer vision and robotics community.
    In very recent years, deep learning (DL) has been brought to this emerging field
    and inspired active research endeavors in mining its potential. However, there
    is still a lack of taxonomies in DL techniques for event-based vision. We first
    scrutinize the typical event representations with quality enhancement methods
    as they play a pivotal role as inputs to the DL models. We then provide a comprehensive
    taxonomy for existing DL-based methods by structurally grouping them into two
    major categories: 1) image reconstruction and restoration; 2) event-based scene
    understanding and 3D vision. Importantly, we conduct benchmark experiments for
    the existing methods in some representative research directions (e.g., object
    recognition) to identify some critical insights and problems. Finally, we make
    important discussions regarding the challenges and provide new perspectives for
    inspiring more research studies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Event Cameras, Deep Learning, Computer Vision and Robotics, Taxonomy, Survey.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The breakthrough in neuromorphic engineering has recently extended the realms
    of sensory perception and initiated a novel paradigm that mimics biological vision.
    The bio-inspired sensors are also called event cameras [[1](#bib.bib1)]. The difference
    is that each pixel in the camera operates independently, triggering a response
    (i.e., event) only when there’s a brightness change, probably caused by motion
    or other visual changes [[2](#bib.bib2)]. In practice, the event streams captured
    by an event camera are sparse and are more concentrated along object boundaries,
    but also present in areas of continuous texture gradients, enabling the camera
    with a merit of low latency. By contrast, canonical frame-based cameras record
    a complete image or video of the scene with a fixed frame rate. Event cameras
    offer some other benefits, such as high temporal resolution and high dynamic range [[3](#bib.bib3)].
    This means that an event camera can capture high-quality data either in extreme
    lighting or high-speed motion conditions [[4](#bib.bib4)]. Therefore, event cameras
    have the potential to overcome the limitations of frame-based cameras in the computer
    vision and robotic fields and have helped the community in solving various downstream
    tasks, e.g., corner tracking[[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)], Simultaneous Localization And Mapping (SLAM)[[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)],
    image and video restoration[[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19)], object detection and segmentation[[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a95743e77b5213160eaf7061d57d402e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The structural and hierarchical taxonomy of event-based vision with
    deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivation: The first application of an event camera is in a hardware-based
    Deep Neural Network (DNN) system called CAVIAR [[26](#bib.bib26)], predating the
    first software-based DNN system from the computer vision community by approximately
    a decade. This early development also includes significant contributions from
    solid-state circuit papers that detailed the design and initial applications of
    camera chips [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)]. Recently,
    deep learning (DL) has received great attention in this emerging area with amounts
    of techniques developed based on various purposes. For example, DL-based feature
    trackers [[30](#bib.bib30)] achieve better accuracy than the non-DL-based methods
    (See Tab. [IX](#S4.T9 "TABLE IX ‣ 4.1.2 Feature Tracking ‣ 4.1 Scene Understanding
    ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks")). We explore some fundamental questions
    that have been driving this research area when examining the representative methods.
    For instance, how to feed event data to DNNs as they are designed for image- or
    tensor-like inputs? What makes DL more advantageous than optimization-based methods
    for learning events? Do we really need very deep models to learn visual features
    from events? How can we balance the distinct property of event cameras, e.g.,
    low latency, when applying DL models? Do we really need convolution operations
    to filter events as done for image data?'
  prefs: []
  type: TYPE_NORMAL
- en: With these questions being discussed, this paper provides an in-depth survey
    of current trends in DL for event-based vision. We focus on analyzing, categorizing,
    and benchmarking the DL-based methods with a highlight on recent progress. In
    particular, we review and scrutinize this promising area by systematically discussing
    the technical details, challenges, and potential directions. Previously, Gallego
    et al. [[1](#bib.bib1)] provided the first overview of the event-based vision
    with a particular focus on the principles and conventional algorithms. However,
    DL has invigorated almost every field of event-based vision recently, and remarkable
    advancements in methodologies and techniques have been achieved. Therefore, a
    more up-to-date yet insightful survey is urgently needed for capturing the research
    trends while clarifying the challenges and potential directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We survey the DL-based methods by focusing on three important aspects: 1) How
    to learn events with DNNs—event representation and quality enhancement (Sec. [2](#S2
    "2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks")); 2) Current research highlights by typically analyzing
    two hot fields, image restoration and enhancement (Sec.[3](#S3 "3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")), scene understanding and 3D vision (Sec. [4](#S4 "4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks")); 3) Potential future directions, such as event-based neural radiance
    for 3D reconstruction, cross-modal learning, and event-based model pretraining.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Contributions: In summary, the main contributions of this paper are five-fold:
    (I) We provide a comprehensive overview of the existing event representations
    as well as the quality enhancement methods for events. (II) We provide a summary
    of how existing DL-based approaches for event-based vision address challenges
    and offer insights for the computer vision community, including image restoration
    and enhancement and high-level scene understanding tasks. (III) We discuss some
    open problems and challenges on DL with events and identify future research directions,
    providing guidance for future developments in this field. (IV) We ask and discuss
    some broadly focused questions as well as some potential problems to answer the
    concern and dive deeply into the event-based vision. Furthermore, we create an
    open-source repository that provides a taxonomy of all mentioned papers and code
    links. Our open-source repository will be updated regularly with the latest research
    progress, and we hope this work can bring sparks to the research of this field.
    The repository link is [https://github.com/vlislab2022/Event-Deep-Learning-Survey](https://github.com/vlislab2022/Event-Deep-Learning-Survey).
    Meanwhile, we benchmark and highlight some representative event-based and event-guided
    vision tasks, e.g., in Tab. [VI](#S3.T6 "TABLE VI ‣ 3.4 Event-guided Image/Video
    Deblurring ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks") and [X](#S4.T10 "TABLE X ‣ 4.1.3
    Object Detection and Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks"), to identify the critical insights and problems for future studies.
    Due to the lack of space, some experimental results can be found in the supplementary
    material.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Outlines: In the following sections, we discuss and analyze the recent advances
    in DL methods for event-based vision. The structural and hierarchical taxonomy
    of this paper is depicted in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"). In Sec. [2](#S2
    "2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks"), we systematically summarize existing event representation
    methods for DL and compare the advantages and disadvantages of different representations
    on different tasks. In this section, the quality enhancement methods are also
    summarized. In Sec. [3](#S3 "3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks"), we review and
    analyze the image restoration and enhancement methods, including image and video
    reconstruction, super-resolution, video frame interpolation, image and video deblurring,
    and high dynamic range (HDR) image and video reconstruction. In Sec. [4](#S4 "4
    Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks"), we summarise existing event-based DL approaches in scene
    understanding tasks, including object classification, object detection and tracking,
    semantic segmentation, feature tracking, optical flow, and depth estimation. In
    this section, we mainly discuss the deep learning pipelines for these computer
    vision tasks with event cameras. In Sec. [5.1](#S5.SS1 "5.1 Discussions ‣ 5 Research
    Trend and Discussions ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks"), we ask and discuss some widely focused questions as well
    as some potential problems to answer the existing concern and dive deeply into
    the event-based vision. And we discuss some open problems and new applications
    on DL with events and identify future research directions in Sec. [5.2](#S5.SS2
    "5.2 New Directions ‣ 5 Research Trend and Discussions ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"), providing guidance for future
    developments in this field.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Event Processing for DNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Event cameras generate events when individual pixels detect the relative logarithm
    intensity change. Consequently, sparse and asynchronous event streams are generated.
    This inherent sparsity offers immediate advantages, such as low latency and low
    computational requirements for postprocessing systems [[31](#bib.bib31)]. Event
    cameras pose a distinctive shift in the imaging paradigm regarding how visual
    information is captured, making it impossible to directly apply the DNN models
    taking the image- or tensor-like inputs. Therefore, we first analyze the event
    representations, which are used as inputs to DNNs (Sec. [2.1](#S2.SS1 "2.1 Event
    Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks")). Moreover, as event data are often hampered
    by noises (especially in low-light conditions) and limited spatial resolution
    (in particular for DAVIS cameras), we analyze the DL methods that super-resolve
    and denoise the event streams for improving the learning performance (Sec. [2.2](#S2.SS2
    "2.2 Quality Enhancement for Events ‣ 2 Event Processing for DNNs ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Event Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We first review how an event camera responds asynchronously to each independent
    pixel and generates a stream of events. An event is interpreted as a tuple $(\textbf{u},t,p)$,
    which is triggered whenever a change in the logarithmic intensity $L$ surpasses
    a constant value (threshold) $C$, formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E1.m1.11" class="ltx_math_unparsed" alttext="p=\left\{\begin{aligned}
    +1,&amp;L(\textbf{u},t)-L(\textbf{u},t-\Delta t)\geq C\\ -1,&amp;L(\textbf{u},t)-L(\textbf{u},t-\Delta
    t)\leq-C\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0,&amp;other\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{aligned}\right." display="block"><semantics id="S2.E1.m1.11a"><mrow id="S2.E1.m1.11b"><mi
    id="S2.E1.m1.11.12">p</mi><mo id="S2.E1.m1.11.13">=</mo><mrow id="S2.E1.m1.11.14"><mo
    id="S2.E1.m1.11.14.1">{</mo><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"
    id="S2.E1.m1.11.11"><mtr id="S2.E1.m1.11.11a"><mtd class="ltx_align_right" columnalign="right"
    id="S2.E1.m1.11.11b"><mrow id="S2.E1.m1.1.1.1.1.1.1"><mrow id="S2.E1.m1.1.1.1.1.1.1.1"><mo
    id="S2.E1.m1.1.1.1.1.1.1.1a">+</mo><mn id="S2.E1.m1.1.1.1.1.1.1.1.2">1</mn></mrow><mo
    id="S2.E1.m1.1.1.1.1.1.1.2">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"
    id="S2.E1.m1.11.11c"><mrow id="S2.E1.m1.5.5.5.5.4"><mrow id="S2.E1.m1.5.5.5.5.4.4"><mrow
    id="S2.E1.m1.5.5.5.5.4.4.3"><mi id="S2.E1.m1.5.5.5.5.4.4.3.2">L</mi><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.5.5.5.5.4.4.3.1">​</mo><mrow id="S2.E1.m1.5.5.5.5.4.4.3.3.2"><mo
    stretchy="false" id="S2.E1.m1.5.5.5.5.4.4.3.3.2.1">(</mo><mtext class="ltx_mathvariant_bold"
    id="S2.E1.m1.2.2.2.2.1.1">u</mtext><mo id="S2.E1.m1.5.5.5.5.4.4.3.3.2.2">,</mo><mi
    id="S2.E1.m1.3.3.3.3.2.2">t</mi><mo stretchy="false" id="S2.E1.m1.5.5.5.5.4.4.3.3.2.3">)</mo></mrow></mrow><mo
    id="S2.E1.m1.5.5.5.5.4.4.2">−</mo><mrow id="S2.E1.m1.5.5.5.5.4.4.1"><mi id="S2.E1.m1.5.5.5.5.4.4.1.3">L</mi><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.5.5.5.5.4.4.1.2">​</mo><mrow id="S2.E1.m1.5.5.5.5.4.4.1.1.1"><mo
    stretchy="false" id="S2.E1.m1.5.5.5.5.4.4.1.1.1.2">(</mo><mtext class="ltx_mathvariant_bold"
    id="S2.E1.m1.4.4.4.4.3.3">u</mtext><mo id="S2.E1.m1.5.5.5.5.4.4.1.1.1.3">,</mo><mrow
    id="S2.E1.m1.5.5.5.5.4.4.1.1.1.1"><mi id="S2.E1.m1.5.5.5.5.4.4.1.1.1.1.2">t</mi><mo
    id="S2.E1.m1.5.5.5.5.4.4.1.1.1.1.1">−</mo><mrow id="S2.E1.m1.5.5.5.5.4.4.1.1.1.1.3"><mi
    mathvariant="normal" id="S2.E1.m1.5.5.5.5.4.4.1.1.1.1.3.2">Δ</mi><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.5.5.5.5.4.4.1.1.1.1.3.1">​</mo><mi id="S2.E1.m1.5.5.5.5.4.4.1.1.1.1.3.3">t</mi></mrow></mrow><mo
    stretchy="false" id="S2.E1.m1.5.5.5.5.4.4.1.1.1.4">)</mo></mrow></mrow></mrow><mo
    id="S2.E1.m1.5.5.5.5.4.5">≥</mo><mi id="S2.E1.m1.5.5.5.5.4.6">C</mi></mrow></mtd></mtr><mtr
    id="S2.E1.m1.11.11d"><mtd class="ltx_align_right" columnalign="right" id="S2.E1.m1.11.11e"><mrow
    id="S2.E1.m1.6.6.6.1.1.1"><mrow id="S2.E1.m1.6.6.6.1.1.1.1"><mo id="S2.E1.m1.6.6.6.1.1.1.1a">−</mo><mn
    id="S2.E1.m1.6.6.6.1.1.1.1.2">1</mn></mrow><mo id="S2.E1.m1.6.6.6.1.1.1.2">,</mo></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S2.E1.m1.11.11f"><mrow id="S2.E1.m1.10.10.10.5.4"><mrow
    id="S2.E1.m1.10.10.10.5.4.4"><mrow id="S2.E1.m1.10.10.10.5.4.4.3"><mi id="S2.E1.m1.10.10.10.5.4.4.3.2">L</mi><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.10.10.10.5.4.4.3.1">​</mo><mrow id="S2.E1.m1.10.10.10.5.4.4.3.3.2"><mo
    stretchy="false" id="S2.E1.m1.10.10.10.5.4.4.3.3.2.1">(</mo><mtext class="ltx_mathvariant_bold"
    id="S2.E1.m1.7.7.7.2.1.1">u</mtext><mo id="S2.E1.m1.10.10.10.5.4.4.3.3.2.2">,</mo><mi
    id="S2.E1.m1.8.8.8.3.2.2">t</mi><mo stretchy="false" id="S2.E1.m1.10.10.10.5.4.4.3.3.2.3">)</mo></mrow></mrow><mo
    id="S2.E1.m1.10.10.10.5.4.4.2">−</mo><mrow id="S2.E1.m1.10.10.10.5.4.4.1"><mi
    id="S2.E1.m1.10.10.10.5.4.4.1.3">L</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.10.10.10.5.4.4.1.2">​</mo><mrow
    id="S2.E1.m1.10.10.10.5.4.4.1.1.1"><mo stretchy="false" id="S2.E1.m1.10.10.10.5.4.4.1.1.1.2">(</mo><mtext
    class="ltx_mathvariant_bold" id="S2.E1.m1.9.9.9.4.3.3">u</mtext><mo id="S2.E1.m1.10.10.10.5.4.4.1.1.1.3">,</mo><mrow
    id="S2.E1.m1.10.10.10.5.4.4.1.1.1.1"><mi id="S2.E1.m1.10.10.10.5.4.4.1.1.1.1.2">t</mi><mo
    id="S2.E1.m1.10.10.10.5.4.4.1.1.1.1.1">−</mo><mrow id="S2.E1.m1.10.10.10.5.4.4.1.1.1.1.3"><mi
    mathvariant="normal" id="S2.E1.m1.10.10.10.5.4.4.1.1.1.1.3.2">Δ</mi><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.10.10.10.5.4.4.1.1.1.1.3.1">​</mo><mi id="S2.E1.m1.10.10.10.5.4.4.1.1.1.1.3.3">t</mi></mrow></mrow><mo
    stretchy="false" id="S2.E1.m1.10.10.10.5.4.4.1.1.1.4">)</mo></mrow></mrow></mrow><mo
    id="S2.E1.m1.10.10.10.5.4.5">≤</mo><mrow id="S2.E1.m1.10.10.10.5.4.6"><mo id="S2.E1.m1.10.10.10.5.4.6a">−</mo><mi
    id="S2.E1.m1.10.10.10.5.4.6.2">C</mi></mrow></mrow></mtd></mtr><mtr id="S2.E1.m1.11.11g"><mtd
    class="ltx_align_right" columnalign="right" id="S2.E1.m1.11.11h"><mrow id="S2.E1.m1.11.11.11.1.1.3"><mn
    id="S2.E1.m1.11.11.11.1.1.1">0</mn><mo id="S2.E1.m1.11.11.11.1.1.3.1">,</mo></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S2.E1.m1.11.11i"><mrow id="S2.E1.m1.11.11.11.2.1"><mi
    id="S2.E1.m1.11.11.11.2.1.2">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.11.11.11.2.1.1">​</mo><mi
    id="S2.E1.m1.11.11.11.2.1.3">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.11.11.11.2.1.1a">​</mo><mi
    id="S2.E1.m1.11.11.11.2.1.4">h</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.11.11.11.2.1.1b">​</mo><mi
    id="S2.E1.m1.11.11.11.2.1.5">e</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.11.11.11.2.1.1c">​</mo><mi
    id="S2.E1.m1.11.11.11.2.1.6">r</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" id="S2.E1.m1.11c">p=\left\{\begin{aligned} +1,&L(\textbf{u},t)-L(\textbf{u},t-\Delta
    t)\geq C\\ -1,&L(\textbf{u},t)-L(\textbf{u},t-\Delta t)\leq-C\\ 0,&other\\ \end{aligned}\right.</annotation></semantics></math>
    |  | (1) |
  prefs: []
  type: TYPE_NORMAL
- en: 'where $\textbf{u}=(x,y)$ is the pixel location, $t$ is the timestamp and $p\in\{-1,1\}$
    is the polarity, indicating the sign of brightness changes (1 and -1 represent
    positive and negative events, respectively), and $p=0$ means that there are no
    events. $\Delta t$ is a time interval since the last event at pixel $\textbf{u}=(x,y)$.
    A number (or stream) of events are triggered which can be denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{E}=\{e_{i}\}_{i=1}^{N}=\{\textbf{u}_{i},t_{i},p_{i}\},i\in N,$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'For more details of the event generation model, we refer readers to [[1](#bib.bib1)].
    In a nutshell, this particular type of data makes it difficult to apply the DNN
    models predominantly designed for frame-based cameras. Therefore, it is pivotal
    to exploit the effective alternative representations of event data for mining
    their visual information and power [[32](#bib.bib32)]. In the following, we review
    the representative event representation methods, which can be divided into six
    categories: image-based, surface-based, learning-based, voxel-based, graph-based,
    and spike-based representations, as shown in Tab. [I](#S2.T1 "TABLE I ‣ 2.1 Event
    Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Representative event representations, served as inputs to event-based
    DNN models. (SP: Steering Prediction; OF: Optical Flow Estimation; Cls: Classification;
    CD: Corner Detection; GR: Gesture recognition; Recon: Reconstruction; DE: Depth
    Estimation; N/A: Not Available.)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Event Representation | Tasks | Dimensions | Description | Characteristics
    |'
  prefs: []
  type: TYPE_TB
- en: '| Image | Maqueda et al.[[33](#bib.bib33)] | SP | (2,H,W) | Channels for positive
    and negative events | Characteristics |'
  prefs: []
  type: TYPE_TB
- en: '| EV-FlowNet [[34](#bib.bib34)] | OF | (4,H,W) | Image of event counts | Discards
    temporal information |'
  prefs: []
  type: TYPE_TB
- en: '| Event Image [[35](#bib.bib35)] | Cls | (4,H,W) | Image of event counts |
    Four channel event images |'
  prefs: []
  type: TYPE_TB
- en: '| AMAE[[36](#bib.bib36)] | Cls | (2,H,W) | Two-channel image used timestamps
    | Indistinguishable semantic information |'
  prefs: []
  type: TYPE_TB
- en: '| Bai et al.[[37](#bib.bib37)] | Cls | (3,H,W) | Three-channel event representation
    | Event count channel based on number |'
  prefs: []
  type: TYPE_TB
- en: '|  | MVF-Net [[38](#bib.bib38)] | Cls | N/A | Multi-view 2D maps | spatial-temporal
    complements |'
  prefs: []
  type: TYPE_TB
- en: '| Learning | EST [[39](#bib.bib39)] | Cls&OF | (2,B,H,W) | 4 grid of convolutions
    | Temporally quantized information into B bins |'
  prefs: []
  type: TYPE_TB
- en: '| Matrix-LSTM [[40](#bib.bib40)] | Cls&OF | (B,H,W) | End-to-end event surfaces
    | Temporally quantized information into B bins |'
  prefs: []
  type: TYPE_TB
- en: '| Surface | Timestamps Image [[41](#bib.bib41)] | Cls | N/A | Spatial and temporal
    downsampling | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| SAE [[42](#bib.bib42)] | Cls | (2,H,W) | Image of most recent timestamps
    | Discards all prior timestamps |'
  prefs: []
  type: TYPE_TB
- en: '| Time Surface [[43](#bib.bib43)] | Cls | (2,H,W) | Exponential of newest timestamps
    | Discards all prior timestamps |'
  prefs: []
  type: TYPE_TB
- en: '| Sorted Time Surface [[7](#bib.bib7)] | Cls | (2,H,W) | Sorted newest timestamps
    | Retains temporal relationship |'
  prefs: []
  type: TYPE_TB
- en: '| Event Histogram [[33](#bib.bib33)] | Cls | (2,H,W) | Image of event counts
    | Discard temporal information |'
  prefs: []
  type: TYPE_TB
- en: '| HATS [[44](#bib.bib44)] | Cls | (2,H,W) | Aggregated newest timestamps |
    Discards temporal information |'
  prefs: []
  type: TYPE_TB
- en: '| IETS [[45](#bib.bib45)] | Cls | (3,H,W) | Image of filtered timestamps &
    event count | Discard temporal information |'
  prefs: []
  type: TYPE_TB
- en: '| SITS [[46](#bib.bib46)] | CD | (2,H,W) | Speed invariant time surface | Discards
    absolute timestamps |'
  prefs: []
  type: TYPE_TB
- en: '| Chain SAE [[32](#bib.bib32)] | Cls | (2,H,W) | Chain Updating Strategy of
    time surface | Retains temporal relationship |'
  prefs: []
  type: TYPE_TB
- en: '| DS [[47](#bib.bib47)] | OP | (H,W) | Image of spatial distance to active
    pixel | Discards temporal/polarity information |'
  prefs: []
  type: TYPE_TB
- en: '| DiST [[48](#bib.bib48)] | Cls | (2,H,W) | Sorted discounted timestamps |
    Retains temporal relationship |'
  prefs: []
  type: TYPE_TB
- en: '| TORE [[3](#bib.bib3)] | Cls | (2,K,H,W) | 4D grid of last K timestamps |
    Retains all information for last K events |'
  prefs: []
  type: TYPE_TB
- en: '| Voxel | Zhu et al.[[49](#bib.bib49)] | OF | (B,H,W) | Generated by discretizing
    the time domain | Temporally quantized information into B bins |'
  prefs: []
  type: TYPE_TB
- en: '| Rebecq et al.[[50](#bib.bib50)] | Recon | (B,H,W) | Generated by discretizing
    the time domain | Temporally quantized information into B bins |'
  prefs: []
  type: TYPE_TB
- en: '| Ye et al.[[51](#bib.bib51)] | DE&OF | (B,H,W) | Generated by discretizing
    the time domain | Temporally quantized information into B bins |'
  prefs: []
  type: TYPE_TB
- en: '| TORE [[3](#bib.bib3)] | Recon, etc | (2,K,H,W) | 4D grid of last K timestamps
    | Retains all information for last K events |'
  prefs: []
  type: TYPE_TB
- en: '| Graph | RG-CNN [[52](#bib.bib52)] | Cls | N/A | End-to-end learning with
    graph convolution neural networks | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| EV-VGCNN [[53](#bib.bib53)] | Cls | (H,W,A) | Use voxel-wise vertices rather
    than point-wise inputs | Normalize the time dimension with a compensation coefficient
    A |'
  prefs: []
  type: TYPE_TB
- en: '| Spike | Lee et al.[[54](#bib.bib54)] | Cls | N/A | A novel supervised learning
    method for SNNs | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Tactilesgnet [[50](#bib.bib50)] | Cls | N/A | Design a spiking graph convolutional
    network | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Botzheim et al.[[55](#bib.bib55)] | GR | N/A | Use spiking neural network
    and classification learning | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Amiret al.[[56](#bib.bib56)] | GR | N/A | Fully Event-Based Gesture Recognition
    System | N/A |'
  prefs: []
  type: TYPE_TB
- en: 2.1.1 Image-based Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The straightforward solution to adopt events to the existing DL methods is to
    stack (or convert) events to synchronous 2D image representations (similar to
    frame-based cameras) as the inputs to DNNs. For example, Moeys et al. [[57](#bib.bib57)]
    proposed the first CNN driven by DVS frames to address the blurring issue in a
    predator-prey robot scenario. This study also marks the initial utilization of
    event count DVS images to guide a DNN using DVS data. The channels of image-based
    representation are often set to preserve polarities, timestamps, and event counts [[33](#bib.bib33),
    [34](#bib.bib34), [39](#bib.bib39), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38),
    [57](#bib.bib57)]. Based on how images are formed, we divide the prevailing methods
    into four types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stack based on polarity: Maqueda et al.[[33](#bib.bib33)] set up two separate
    channels to evaluate the histograms for positive and negative events to obtain
    two-channel event images, which are finally merged together into synchronous event
    frames.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stack based on timestamps: To consider the importance of event counts and timestamps
    for holistic information, [[36](#bib.bib36), [37](#bib.bib37), [17](#bib.bib17)]
    take the timestamps of events into consideration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stack based on the number of events: Due to the uneven triggers of events within
    fixed time intervals, another stacking strategy is proposed to sample and stack
    events in a fixed constant number [[58](#bib.bib58), [59](#bib.bib59), [17](#bib.bib17)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stack based on timestamps and polarity: In Ev-gait  [[35](#bib.bib35)], event
    streams are converted to frame-like representation with four channels, containing
    the positive or negative polarities in two channels and the temporal characteristics
    in another two channels. Also, some research has focused on exposure time control
    in event-based systems. Liu et al. [[60](#bib.bib60), [61](#bib.bib61)] have explored
    dynamic control of exposure time and inter-slice time interval to optimize the
    quality of slice features. This adaptive control helps to ensure the robustness
    of the model in dynamic scenes with varying motion speeds and scene structures.
    Detailed mathematical formulations can be found in Sec.1.1 of the supplementary
    material.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Surface-based Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first surface-based representation, i.e., Surface of Active Events (SAE) [[42](#bib.bib42)],
    it maps the event streams to a time-dependent surface and tracks the activity
    around the spatial location of the latest event $e_{i}$. Different from the basic
    image-based representation which utilizes intensity images to provide context
    content, the SAE achieves this through a totally different perspective, i.e.,
    the temporal-spatial perspective. Specifically, the time surface of the $i$ th
    event $e_{i}$ can be formulated as a spatial operator acting on the neighboring
    region of $e_{i}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\tau_{i}([x_{n},y_{n}]^{T},p)=\underset{j\leq i}{max}\{t_{j}&#124;[x_{i}+x_{n},y_{i}+y_{n}],p_{j}=p\}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $x_{n}\in\{-r,r\}$ is the horizontal coordinate of $e_{i}$, $y_{n}\in\{-r,r\}$
    is the vertical coordinate of $e_{i}$, $p_{j}\in\{-1,1\}$ is the polarity of the
    $j$ th event $e_{j}$, $t_{j}$ is the timestamp of $e_{j}$ and $r$ is the radius
    of the neighboring region used to obtain the time surface. As shown in Eq. [3](#S2.E3
    "In 2.1.2 Surface-based Representation ‣ 2.1 Event Representation ‣ 2 Event Processing
    for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    the time surface $\tau_{i}([x_{n},y_{n}]^{T},p)$ encodes the time context in the
    $(2r+1)\times(2r+1)$ neighborhood region of $e_{i}$, hence maintaining both temporal
    and spatial information for downstream tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the timestamps of events monotonically increase, which causes the temporal
    values in the surface from 0 to infinity [[32](#bib.bib32)]. Therefore, appropriate
    normalization approaches are required to preserve the temporal-invariant data
    representation from raw SAE by mapping the timestamps to $[0,1]$. Basic normalization
    methods are directly applied to time surfaces [[7](#bib.bib7), [43](#bib.bib43),
    [62](#bib.bib62), [63](#bib.bib63)], such as the min-max [[7](#bib.bib7)], time
    window [[63](#bib.bib63)], etc.
  prefs: []
  type: TYPE_NORMAL
- en: All these above normalization methods rely on empirical parameter tuning, leading
    to additional computational costs. To avoid this problem, sort normalization is
    employed by Alzugaray et al. [[7](#bib.bib7)] to sort all the timestamps within
    an SAE at each pixel. However, though this method alleviates the dependence on
    parameter tuning, the by-product of time complexity impedes the whole procedure’s
    efficiency. To build an efficient SAE and achieve robust speed-invariant characteristics,
    Manderscheid et al. [[46](#bib.bib46)] introduced a normalization scheme to obtain
    the Speed Invariant Time Surface (SITS). The SITS updates the time surface of
    each incoming event according to its neighborhood with the radius $r$. Overall,
    when large $r$ is adopted, the SITS updates the time surface when a new event
    is triggered, thus leading to inefficiency in the on-demand tasks. Lin et al. [[32](#bib.bib32)]
    suggested solving and alleviating this imbalance between normalization and the
    number of events by using a chain update strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Voxel-based Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The voxel-based representations map the raw events into the nearest temporal
    grid within temporal bins. The first spatial-temporal voxel grid is proposed in
     [[49](#bib.bib49)], inserting events into volumes using a linearly weighted accumulation
    to improve the resolution along the temporal domain. This spatial-temporal voxel
    grid is also used in some following works [[64](#bib.bib64), [51](#bib.bib51)].
    More recently, a time-ordered recent event (TORE) volume is proposed in  [[3](#bib.bib3)],
    aiming at compactly maintaining raw spike temporal information with minimal information
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4 Graph-based Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Aiming at preserving the sparsity of events, graph-based approaches transform
    the raw event streams within a time window into a set of connected nodes. Bi et
    al. first proposed a residual graph CNN architecture to obtain a compact graph
    representation for object classification [[65](#bib.bib65), [52](#bib.bib52)].
    The graph CNN preserves the spatial-temporal coherence of input events while avoiding
    large computational costs. More recently, Deng et al. proposed a voxel graph CNN
    which aims at exploiting the sparsity of event data [[53](#bib.bib53)]. The proposed
    EV-VGCNN [[53](#bib.bib53)] is a lightweight voxel graph CNN while achieving the
    SOTA classification accuracy with very low model complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.5 Spike-based Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Due to the sparsity and asynchronous nature of event streams, most of the above
    representations consider the timestamps. Different from the standard DNN models,
    spiking neural networks (SNNs)[[66](#bib.bib66), [67](#bib.bib67), [56](#bib.bib56),
    [50](#bib.bib50), [54](#bib.bib54), [55](#bib.bib55)] are advantageous in that
    they incorporate the concept of time into operational models. Therefore, SNNs
    better fit the biological neuronal mechanism by using signals in the form of pulses
    (discontinuous values) to convey visual information. SNNs are applied to extracting
    features from events asynchronously to solve diverse tasks, such as object classification[[54](#bib.bib54),
    [50](#bib.bib50)] and gesture recognition[[56](#bib.bib56), [55](#bib.bib55)].
    However, due to the complex dynamics and non-differentiable nature of the spikes,
    two challenges exist: 1) Well-established back-propagation methods cannot be applied
    to the training process, leading to a long training time and high costs; 2) Specialized
    and effective hardware and algorithms are lacking. Consequently, their accuracy
    cannot exceed SOTA methods. Future research could explore more in this direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.6 Learning-based Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The aforementioned event representations are mainly designed for a specific
    task and cannot be generally and flexibly applied to other tasks. To this end,
    Gehrig et al. [[39](#bib.bib39)] proposed the first learning-based approach to
    convert asynchronous raw events into tensor-like inputs, which can be flexibly
    applied to diverse downstream tasks. In particular, a multi-layer perceptron (MLP)
    is adopted to learn the coordinates and timestamps of events to obtain grid-like
    representations. Moreover, some methods [[68](#bib.bib68), [40](#bib.bib40)] extract
    features from events using Long Short-Term Memory (LSTM). A representative approach,
    Matrix-LSTM [[40](#bib.bib40)], utilizes a grid of LSTM cells to integrate information
    in the temporal axis. This approach follows a fully differentiable procedure that
    extracts the most relevant event representations for downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.7 Remarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [I](#S2.T1 "TABLE I ‣ 2.1 Event Representation ‣ 2 Event Processing for
    DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    outlines six types of mainstream representation methods for DNNs. Different event
    representations offer unique advantages and considerations when applied to various
    tasks. Image-based representation enables seamless integration with traditional
    deep learning algorithms, allowing for applications in object detection, segmentation,
    and feature extraction. Surface-based representation provides spatial-temporal
    context and preserves temporal information to a certain extent. Voxel-based representation
    enhances resolution and preserves raw spike temporal information by mapping raw
    events into temporal grids. Graph-based representation maintains sparsity and
    coherence while minimizing computational costs, achieving high classification
    accuracy. Spike-based representation, buttressed by SNNs, offers advantages in
    asynchronous processing, efficiency, noise robustness, and compatibility with
    neuromorphic hardware. Learning-based representation aims to discover optimal
    event representations, adapting to task-specific requirements. However, practical
    factors such as computational complexity and data availability should be considered.
    Overall, the selection of event representation should be considered based on the
    requirements of the task, and the trade-offs between complexity, computational
    efficiency, and interpretability. Further research and studies are urgently needed
    to explore more generic event representations for a wider range of tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Quality Enhancement for Events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Event cameras, e.g., DAVIS346 [[69](#bib.bib69)], are with relatively low resolution—346$\times$240,
    while some event cameras, e.g., Prophesse ¹¹1https://www.prophesee.ai/, show higher
    spatial resolution up to, e.g., 640$\times$480\. These cameras often suffer from
    unexpected noise in the captured event data, especially in challenging visual
    conditions and event representation processes. Also, the spatial resolution is
    still lower than that of the frame-based cameras. These problems often hamper
    applying deep learning to event-based vision. Therefore, research has been recently
    conducted to improve the spatial resolution of events and denoise the events to
    achieve higher quality.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Event Super-Resolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different from image super-solution (SR), event SR requires distribution estimation
    in both spatial and temporal dimensions. Li et al. [[70](#bib.bib70)] first proposed
    to solve the spatial-temporal SR problem of the LR event image. Later on, Wang
    et al. [[71](#bib.bib71)] proposed to bridge intensity images and events from
    the Dynamic Vision Sensor (DVS) via joint image filtering, so as to obtain motion-compensated
    event frames with high-resolution (HR) and less noise. The first DL-based approach
    is proposed by Duan et al. [[72](#bib.bib72)], which addresses the joint denoising
    and SR by using a multi-resolution event recording system and a 3D U-Net-based
    framework, called EventZoom. In particular, it incorporates event-to-image reconstruction
    to achieve resolution enhancement. Furthermore, Li et al.[[73](#bib.bib73)] proposed
    an SNN framework with a constraint learning mechanism to simultaneously learn
    the spatial and temporal distributions of event streams. Recently, Weng et al.[[74](#bib.bib74)]
    introduced a Recurrent Neural Network (RNN) that employs temporal propagation
    and spatial-temporal fusion net to ensure the restoration abilities of fine-grained
    event details without any auxiliary high-quality and HR frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Though these methods achieve plausible SR results, the spatial-temporal
    distribution estimation leads to high latency for large factor SR, e.g., $\times
    16$. Future research could focus on reducing inference latency and lightweight
    network design.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Event Denoising
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The presence of random noise, such as thermal noise and junction leakage currents,
    leads to background activity (BA) where an event is generated without any log-intensity
    change. To address this issue, various approaches have been developed, including
    filter-based methods, DL-based methods, and simulator-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: Filter-based methods utilize various types of filters to eliminate background
    activity. These methods include bio-inspired filters[[75](#bib.bib75)], hardware-based
    filters[[76](#bib.bib76), [77](#bib.bib77)], spatial filters[[42](#bib.bib42),
    [78](#bib.bib78), [79](#bib.bib79)], and temporal filters[[45](#bib.bib45), [45](#bib.bib45),
    [71](#bib.bib71)]. However, in complex environments with multiple noise sources,
    these methods often fail to achieve satisfactory results. To mitigate these problems,
    Guo et al.[[80](#bib.bib80)] introduced a novel framework that quantifies denoising
    algorithms more effectively by measuring receiver operating characteristics using
    known mixtures of signal and noise DVS events. For experiment results of different
    filter-based methods, refer to Fig. 6 in the supplementary material.
  prefs: []
  type: TYPE_NORMAL
- en: DL-based methods have also been introduced in  [[81](#bib.bib81), [82](#bib.bib82)].
    One representative framework is EDnCNN proposed by Baldwin et al.[[81](#bib.bib81)].
    It transforms neighboring events into voxels and differentiates noise using an
    Event Probability Mask (EPM). However, the artificial regularization operation,
    such as voxel transformation, can compromise the inherent properties of event
    data. To address this issue, AEDNet[[82](#bib.bib82)] decomposes DVS signals into
    temporal correlation and spatial affinity, leveraging the properties of temporal
    continuation and spatial discreteness. These signals are then separately processed
    using a unique feature extraction module. For detailed qualitative and quantitative
    experiment results, refer to Fig. 5 and Tab. 4 in the supplementary material.
    Another type of approach, a.w.a., simulator-based methods, focuses on incorporating
    noise effects into event simulators. The knowledge learned from simulated event
    data is then transferred to real data to reduce the influence of noise [[83](#bib.bib83),
    [84](#bib.bib84), [25](#bib.bib25), [85](#bib.bib85)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Some event samples have abnormal pixel values in the event-count channels.
    Different abnormal pixels in different feature areas of the same object lead to
    a decline in feature learning performance, affecting subsequent tasks. Thus, denoising
    methods are one of the bases of event-based vision. In future research, a more
    general DL-based denoising pipeline, which can be applied to various event-based
    vision tasks, is worth exploring.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Image Restoration and Enhancement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Event cameras hold immense potential for leveraging event cameras in the reconstruction
    and restoration of HDR images and high frame-rate videos. However, their unique
    imaging paradigm presents a challenge when applying vision algorithms designed
    for frame-based cameras. To address this challenge and bridge the gap between
    event-based and standard computer vision, many methods have been proposed to reconstruct
    intensity video frames or images from events.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we group the prevailing methods into two major types: event-based
    image (or video) reconstruction (with only events as inputs) and event-guided
    image restoration (hybrid inputs of events and frames). For the former, the main
    problem is how to fully explore the visual information, e.g., edge, from events
    with DNNs to reconstruct high-quality intensity images or video frames; while
    the latter explores how to fuse frames and events while leveraging the advantages
    of events, e.g., HDR, to benefit the image restoration process. We now review
    the state-of-the-art (SOTA) techniques in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Event-based Image/Video Reconstruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Insight: This task learns a mapping from a stream of events to a single intensity
    image or sequence of images (i.e., video). The mapped results allows for applying
    the off-the-shelf DL algorithms—developed for frame-based cameras—to learning
    downstream tasks. From our review, intensive research has been devoted to achieving
    this task, as summarized in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1 Event-based Image/Video
    Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"), Fig. [2](#S3.F2 "Figure 2 ‣ 3.1
    Event-based Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣
    Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    and Tab. [II](#S3.T2 "TABLE II ‣ 3.1 Event-based Image/Video Reconstruction ‣
    3 Image Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A
    Comprehensive Survey and Benchmarks").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Early methods rely on the assumption about the scene structure (or motion dynamics) [[86](#bib.bib86),
    [87](#bib.bib87)] or event integration with regularization terms [[88](#bib.bib88)]
    to reconstruct intensity images. However, these methods suffer from artifacts
    due to the direct event integration, and the reconstructed intensity images are
    not photo-realistic enough. DL-based methods, by contrast, bring significant accuracy
    gains. In this paper, we analyze the SOTA deep learning methods based on the challenges:
    1) A lack of large-scale datasets for training deep networks; 2) High computational
    complexity and low latency; 3) The low-quality of reconstructed images or videos,
    e.g., relatively low resolution and blurred images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Qualitative comparison results of some image reconstruction methods [[89](#bib.bib89)]
    on event dataset [[12](#bib.bib12)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Type | MSE $\downarrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ |
    Time |'
  prefs: []
  type: TYPE_TB
- en: '| E2VID  [[90](#bib.bib90)] | DL-based | 0.069 | 0.395 | 0.438 | 0.2448 s |'
  prefs: []
  type: TYPE_TB
- en: '| ECNN [[91](#bib.bib91)] | D-based | 0.056 | 0.416 | 0.442 | 0.2839 s |'
  prefs: []
  type: TYPE_TB
- en: '| BTEB [[92](#bib.bib92)] | DL-based | 0.090 | 0.357 | 0.520 | 0.4059 s |'
  prefs: []
  type: TYPE_TB
- en: '| Tikhonov  [[89](#bib.bib89)] | Model-based | 0.121 | 0.356 | 0.485 | 0.4401
    s |'
  prefs: []
  type: TYPE_TB
- en: '| TV [[89](#bib.bib89)] | Mode-based | 0.113 | 0.386 | 0.502 | 4.0443 s |'
  prefs: []
  type: TYPE_TB
- en: '| CNN [[89](#bib.bib89)] | DL-based | 0.080 | 0.437 | 0.485 | 28.3904 s | ![Refer
    to caption](img/7de71ad3bf8a765ce2bdf974e5df4ebd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Methods for event-based image/video reconstruction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Comparison of the representative event-guided video frame interpolation
    (VFI) methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Publication | Methods | Highlight | Event Representation | Optical Flow |
    Deblurring | Supervised | Backbone | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2021 | TimeLens[[93](#bib.bib93)] | synthesis-based and ﬂow-based |
    voxel grid[[49](#bib.bib49)] | ✓ | ✗ | ✓ | CNN | HQF,Vimeo90k,GoPro,Middlebury,HS-ERGB
    |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2021 | EFI-Net[[94](#bib.bib94)] | different spatial resolutions | voxel
    grids[[49](#bib.bib49)] | ✗ | ✗ | ✓ | CNN | Samsung GE3 DVS |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2021 | Yu et al.[[95](#bib.bib95)] | weakly supervised | Image-based
    | ✓ | ✗ | ✗ | ViT+CNN | GoPro, SloMo-DVS |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | Time Replayer[[96](#bib.bib96)] | unsupervised cycle-consistent
    style | 4-channel frames[[41](#bib.bib41)] | ✓ | ✗ | ✗ | CNN | GoPro, Adobe240,
    Vimeo90k |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | TimeLens++[[97](#bib.bib97)] | multi-scale feature-level fusion
    | voxel grid[[49](#bib.bib49)] | ✓ | ✗ | ✓ | CNN | BS-ERGB, HS-ERGB |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2022 | $A^{2}OF$[[98](#bib.bib98)] | optical flows adjustment | four-channel
    frame[[41](#bib.bib41)] | ✓ | ✗ | ✓ | CNN | Adobe240, GoPro, Middlebury, HS-ERGB,
    HQF |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV2020 | Lin et al.[[99](#bib.bib99)] | physical model inspired | stream
    and frame-based | ✗ | ✓ | ✓ | CNN | GoPro,Blur-DVS |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | E-CIR[[100](#bib.bib100)] | parametric intensity function | polynomial[[100](#bib.bib100)]
    | ✗ | ✓ | ✓ | CNN | REDS |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | Zhang et al.[[101](#bib.bib101)] | deblurring and frame interpolation
    | event streams | ✗ | ✓ | ✗ | CNN | GoPro, HQF, RBE | ![Refer to caption](img/d7d58c05f9f2268e94f2df983c90f6b9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Visual examples of some SOTA methods for video reconstruction (E2VID
     [[90](#bib.bib90)], EF [[91](#bib.bib91)], RCNN [[102](#bib.bib102)]).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a7aacc204721616ed7e24c81a0b7b2df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Representative VFI methods, including, e.g., (a) TimeLens [[93](#bib.bib93)],
    the first event-guided VFI method (b) TimeLens++[[97](#bib.bib97)], the SOTA event-based
    VFI method (c) TimeReplayer[[96](#bib.bib96)], the first unsupervised event-guided
    VFI method.'
  prefs: []
  type: TYPE_NORMAL
- en: For the first challenge,  [[103](#bib.bib103)] and  [[104](#bib.bib104)] are
    representative works leveraging generative adversarial networks (GANs) to bridge
    knowledge transfer between events and RGB images to alleviate the scarce labeled
    data problem. wMoreover, Stoffregen et al. [[91](#bib.bib91)] found that the contrast
    threshold is a key factor in synthesizing data to match the real event data well.
    Further, Vallés et al. [[92](#bib.bib92)] explored the theoretical basis of event
    cameras and proposes self-supervised learning to reduce the dependence on the
    ground truth video (including synthetic data) based on the photometric constancy
    of events.
  prefs: []
  type: TYPE_NORMAL
- en: For addressing the second challenge, Scheerlinck et al. [[19](#bib.bib19)] employed
    recurrent connections to build a state over time, allowing a much smaller recurrent
    neural network that reuses previously reconstructed results. Interestingly, Duwek
    et al. [[105](#bib.bib105)] combined CNNs with SNNs based on Laplacian prediction
    and Poisson integration to achieve video reconstruction with fewer parameters.
    To solve the third challenge, GANs, the double integral model [[106](#bib.bib106)],
    and RNNs are applied to avoid generating blurred results and obtain high-speed
    and HDR videos from events in [[17](#bib.bib17), [102](#bib.bib102), [90](#bib.bib90),
    [89](#bib.bib89)]. As for generating super-resolution (SR) images/videos from
    events, we divide the prevailing works into three categories, including optimization-based[[70](#bib.bib70)],
    supervised [[72](#bib.bib72), [107](#bib.bib107), [108](#bib.bib108), [71](#bib.bib71)],
    and adversarial learning methods [[109](#bib.bib109), [110](#bib.bib110)]. Optimization-based
    methods, e.g., [[70](#bib.bib70)], adopts a two-stage framework to solve the SR
    image reconstruction problem based on the non-homogeneous Poisson point process.
    Supervised methods either utilize residual connections to prevent the network
    models from the problem of gradients vanishing when generating SR images or estimate
    optical flow and temporal constraints to learn the motion cues, so as to reconstruct
    SR videos. For adversarial learning methods, Wang et al.[[109](#bib.bib109)] propose
    a representative end-to-end SR image reconstruction framework without access to
    the ground truth (GT), i.e., HR images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: In this section, we discuss various techniques for event-based image/video
    reconstruction. However, we acknowledge the need for a brief comparison to determine
    which technique is more suitable for different scenarios. While DL-based methods
    have shown significant accuracy gains in terms of accuracy and photorealism compared
    to early approaches relying on assumptions and regularization, they also come
    with increased computational complexity. On the other hand, traditional methods
    based on direct event integration may suffer from artifacts and produce less photo-realistic
    results. Furthermore, the choice of event representation remains an open question,
    and existing learned models often exhibit limited generalization capability. Noise
    in event data also poses a significant challenge, and the reconstruction of color
    images/videos from events is a particularly difficult problem. Future research
    efforts could focus on addressing these aspects to improve the quality and fidelity
    of reconstructed results.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Event-guided Image/Video Super-Resolution (SR)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Insight: The goal is to explore the visual information, e.g., edge, and high
    temporal resolution of events, which are fused with the low-resolution (LR) image/video
    to recover the high-resolution (HR) image/video, as shown in Tab. [IV](#S3.T4
    "TABLE IV ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image SR: eSL-Net [[111](#bib.bib111)] is the first work that introduces events
    for guiding image SR. It proposes a unified event-guided sparse learning framework
    that simultaneously denoises, deblurs, and super-resolves the low-quality active
    pixel sensor (APS) ²²2A type of frame-based sensor, embedded in DAVIS event cameras.
    images to recover high-quality images in an end-to-end learning manner. However,
    due to the limitations of sparse coding, this method performs poorly on more complex
    datasets [[111](#bib.bib111), [112](#bib.bib112)]. EvIntSR [[112](#bib.bib112)]
    achieves the goal of image SR in two steps: 1) Synthesizing a sequence of latent
    frames by combing events and blurry LR frames; 2) Merging latent frames to obtain
    a sharp HR frame. In general, EvIntSR explores the distinctive properties of events
    more directly than eSL and achieves better SR results on the simulation dataset.
    However, this method has two drawbacks: 1) Errors are accumulated in the two-stage
    training procedure; 2) The visual information of events is less explored in the
    second stage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Video SR: Compare with image SR, video SR pays more attention to the relationship
    between multiple frames. E-VSR [[108](#bib.bib108)] is the first VSR framework
    with events. Similar to EvIntSR, it also consists of two sub-tasks: video frame
    interpolation and video SR and is limited by the accumulated errors. Recently,
    EG-VSR [[113](#bib.bib113)] employs implicit functions for learning the continuous
    representation of videos. This method enables end-to-end upsampling at arbitrary
    scales, offering advantages in the video SR task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Comparison of the representative event-guided image/video SR methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Publication | Methods | Highlight | Backbone |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2020 | eSL-Net [[111](#bib.bib111)] | sparse learning | CNN |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2021 | EvIntSR [[112](#bib.bib112)] | two-step methods | CNN |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2021 | E-VSR [[108](#bib.bib108)] | two-step methods | CNN |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2023 | EG-VSR [[113](#bib.bib113)] | SR with arbitrary scales | ViT+CNN
    |'
  prefs: []
  type: TYPE_TB
- en: 'Remarks: While significant progress has been made for this task, including
    the ability to perform upsampling at arbitrary scales, there are still areas that
    need further investigation. For example, the research ignores the distinct modality
    differences between events and RGB frames. Therefore, the directly fusing features
    of two modalities might degrade the performance of SR as events are often disturbed
    by unexpected noises, e.g., in low-light scenes. Future research could explore
    more to tackle these problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2554f33da133e0a55171402a123ca86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Visual results of VFI by three different methods. (TimeLens[[93](#bib.bib93)],
    TimeReplayer[[96](#bib.bib96)], $A^{2}OF$[[98](#bib.bib98)])'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e16f657c729713bdfd1dc7d80a39e743.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Representative deblurring methods, including, e.g., (a) Interaction-based
    methods, (b) Event fusion-based methods, e.g., and (c) Event selection-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Event-guided Video Frame Interpolation (VFI)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Insight : This task leverages the high temporal resolution of events and aims
    to estimate the non-linear motion information between frames, so as to insert
    latent frames between two consecutive frames. Based on how the VFI frameworks
    are learned, we categorize them into three types: supervised, weakly-supervised,
    and unsupervised methods, as shown in Tab. [III](#S3.T3 "TABLE III ‣ 3.1 Event-based
    Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks"). The VFI results
    of some representative methods are visualized in Fig. [5](#S3.F5 "Figure 5 ‣ 3.2
    Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and Enhancement
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised methods: TimeLens [[93](#bib.bib93)] is the first and representative
    work, which employs four modules to fuse features to achieve warping-based and
    synthesis-based interpolation (See Fig. [4](#S3.F4 "Figure 4 ‣ 3.1 Event-based
    Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")(a)). A dataset
    with spatially aligned events and high-speed videos was also released. However,
    it has limitations in that 1) the optical flow estimated from events limits the
    warped frames; 2) the noisy events restrict the quality of optical flow; and 3)
    it is learned sequentially (i.e., not in an end-to-end manner). Therefore, training
    TimeLens is difficult, and errors are accumulated, degrading the performance.
    These problems are better addressed later on by Timelens++[[97](#bib.bib97)],
    $A^{2}OF$ [[98](#bib.bib98)], and EFI-Net[[94](#bib.bib94)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, Timelens++[[97](#bib.bib97)] proposes a framework, comprised
    of four modules including motion estimation, warping encoder, synthesis encoder,
    and fusion module, as depicted in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1 Event-based
    Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")(b). This method
    introduces multi-scale feature-level fusion and computes one-shot non-linear inter-frame
    motion, which could effectively be sampled for image warping based on events and
    frames. $A^{2}OF$ [[98](#bib.bib98)] focuses on generating the anisotropic optical
    flow from events. However, such an approach cannot model the complicated motion
    in real-world scenes; therefore, $A^{2}OF$ employs the distribution masks for
    optical flow from events to achieve the intricate intermediate motion interpolation.'
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the events used by the aforementioned methods have the
    same spatial resolution as RGB frames. Unfortunately, it is quite expensive to
    match a RGB sensor’s resolution with an event sensor in real scenarios. Therefore,
    EFI-Net[[94](#bib.bib94)] proposes a multi-phase CNN-based framework, which can
    fuse the frames and events with various spatial resolutions. In summary, supervised
    methods rely on paired data with high-frame-rate videos and events. HS-ERGB [[93](#bib.bib93)]
    and BS-ERGB[[97](#bib.bib97)] are representative datasets. However, these datasets
    suffer from strict pixel alignments between events and frames and are expensive
    to collect. Therefore, some weakly-supervised and unsupervised methods have been
    proposed recently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weakly-supervised methods: Yu et al.[[95](#bib.bib95)] proposed the first weakly-supervised
    event-based VFI method. In practice, it extracts complementary information from
    events to correct image appearance and employs an attention mechanism to support
    correspondence searching on the low-resolution feature maps. Meanwhile, a real-world
    dataset, namely SloMo-DVS, is also released.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised methods: TimeReplayer [[96](#bib.bib96)] is the first unsupervised
    method, trained in a cycle-consistent manner, as shown in Fig. [4](#S3.F4 "Figure
    4 ‣ 3.1 Event-based Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    (c). It directly estimates the optical flow between the key-frame and the input
    frame, instead of computing intermediate frames as a proportion of the computed
    optical flow between input frames. In this way, the complex motion can be estimated.
    Then, input frames can be reconstructed by key-frame and inverse optical flow.
    Totally, this cycle consistency method not only models complex nonlinear motion
    but also avoids the need for a large amount of paired high-speed frames and events.'
  prefs: []
  type: TYPE_NORMAL
- en: All the above-mentioned methods have the strong assumption that the exposure
    time of RGB frames is very short, and there are no blur artifacts in frames. However,
    this assumption is overly harsh because the practical exposure time can be long
    and results in blur artifacts in frames, particularly in complex lighting scenes.
    When the exposure time is longer, the issue of interpolation needs to be re-examined.
    For this reason, [[99](#bib.bib99), [101](#bib.bib101), [100](#bib.bib100)] jointly
    address the interpolation problem and deblurring. For example, E-CIR [[100](#bib.bib100)]
    transforms a blurry image into a sharp video that is represented as a time-to-intensity
    parametric function with events. Similarly, Zhang et al. [[101](#bib.bib101)]
    employed a learnable double integral network to map blurry frames to sharp latent
    images with event guidance. Lin et al. [[99](#bib.bib99)] emphasized that the
    residuals between a blurry image and a sharp image are event integrals. Based
    on this perspective, they proposed a network that uses events to estimate residuals
    for sharp frame restoration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: From our review, the majority of the methods are based on supervised
    learning, and weakly supervised or unsupervised methods still have a lot of room
    for further research. For example, mutual supervision could be performed through
    the imaging relationship between events and interpolated frames to relieve the
    need for ground truth, i.e., high frame rate video for training. Also, dense optical
    flow estimated by events[[114](#bib.bib114)], could be used as a constraint between
    interpolation results to improve VFI accuracy in an unsupervised manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Event-guided Image/Video Deblurring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Insight: This task gets inspired by the no-motion-blur property of events and
    aims to restore a sharp image/video from a blurry image/video sequence under the
    guidance of events. Because supervised methods tend to achieve higher PSNR and
    SSIM [[96](#bib.bib96), [93](#bib.bib93)]. Traditional deblurring methods rely
    on the physical event generation model [[71](#bib.bib71)]. In particular, Pan
    et al. [[106](#bib.bib106)] proposed an event-based double integral model for
    recovering latent intensity images. Based on this model, sharp images and videos
    could be generated by solving the non-convex optimization problems under adverse
    visual conditions. However, it suffers from the problem of accumulated error caused
    by noise in the sampling process. By contrast, learning-based methods directly
    explore the relationship between blurry and sharp images with the help of events
    and show more plausible deblurring result. In this paper, we divide the learning-based
    methods into three categories: 1) interaction-based methods; 2) fusion-based methods;
    and 3) selection-based methods (See Fig. [6](#S3.F6 "Figure 6 ‣ 3.2 Event-guided
    Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and Enhancement ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")). The
    deblur results of some representative methods are shown in Tab. [V](#S3.T5 "TABLE
    V ‣ 3.4 Event-guided Image/Video Deblurring ‣ 3 Image Restoration and Enhancement
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Qualitative comparison of deblurring methods on GoPro and HQF dataset
    from  [[101](#bib.bib101)]. ‘N/A’ means no results are available.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | GoPro | HQF | Param. |'
  prefs: []
  type: TYPE_TB
- en: '| PSNR$\uparrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ | PSNR $\uparrow$ |
    SSIM $\uparrow$ | LPIPS$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| LEVS  [[115](#bib.bib115)] | 20.84 | 0.5473 | 0.1111 | 20.08 | 0.5629 | 0.0998
    | 18.21M |'
  prefs: []
  type: TYPE_TB
- en: '| EDI  [[106](#bib.bib106)] | 21.29 | 0.6402 | 0.1104 | 19.65 | 0.5909 | 0.1173
    | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| eSL-Net  [[111](#bib.bib111)] | 17.80 | 0.5655 | 0.1141 | 21.36 | 0.6659
    | 0.0644 | 0.188M |'
  prefs: []
  type: TYPE_TB
- en: '| LEDVDI  [[99](#bib.bib99)] | 25.38 | 0.8567 | 0.0280 | 22.58 | 0.7472 | 0.0578
    | 4.996M |'
  prefs: []
  type: TYPE_TB
- en: '| RED [[116](#bib.bib116)] | 25.14 | 0.8587 | 0.0425 | 24.48 | 0.7572 | 0.0475
    | 9.762M |'
  prefs: []
  type: TYPE_TB
- en: '| EVDI [[101](#bib.bib101)] | 30.40 | 0.9058 | 0.0144 | 24.77 | 0.7664 | 0.0423
    | 0.393M |'
  prefs: []
  type: TYPE_TB
- en: 'Interaction-based methods usually input the blurry image and events into two
    different networks and then carry out information interaction after encoding the
    features in each branch to improve the deblurring effect (See Fig. [6](#S3.F6
    "Figure 6 ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks") (a)). For example, in [[116](#bib.bib116)], a self-supervised
    framework was proposed to reduce the domain gap between simulated and real-world
    data. Specifically, they first estimated the optical flow and exploited the blurry
    and photometric consistency to enable self-supervision on the deblurring network.
    Lin et al.[[99](#bib.bib99)] introduced a CNN framework to predict the residual
    between sharp and blurry images for deblurring, and the residual between sharp
    frames for interpolation. Jiang et al. [[117](#bib.bib117)] explored long-term,
    local appearance/motion cues and novel event boundary priors to solve motion deblurring.
    Zhang et al. [[101](#bib.bib101)] utilized low latency of events to alleviate
    motion blur and facilitate the prediction of intermediate frames.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fusion-based methods aims to design a principled framework for video deblurring
    and event-guided deblurring [[111](#bib.bib111)], as shown in Fig. [6](#S3.F6
    "Figure 6 ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")(b). For instance, Shang et al. [[118](#bib.bib118)] proposed
    a two-stream framework to explore the non-consecutively blurry frames and bridge
    the gap between event-guided and video deblurring.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Selection-based methods, e.g., [[119](#bib.bib119)], formulate the event-guided
    motion deblurring by considering the unknown exposure and readout time in the
    video frame acquisition process. The main challenge is how to selectively use
    event features by estimating the cross-modal correlation between the blurry frame
    features and the events. Therefore, the proposed event selection module subtly
    selects useful events, and the fusion module fuses the selected event features
    and blur frames effectively, as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.2 Event-guided
    Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and Enhancement ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks") (c).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Most of the aforementioned deblurring methods are limited to some
    specific scenes. In some scenes with large or fast motions, the model’s accuracy
    may deteriorate dramatically.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Experiments of representative methods on event object classification.
    N/A means no results available.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Publication | Methods | Dataset |  | Param. |'
  prefs: []
  type: TYPE_TB
- en: '| N-MINIST [[120](#bib.bib120)] | MINIST-DVS [[121](#bib.bib121)] | N-Caltech101 [[120](#bib.bib120)]
    | CIFAR10-DVS [[122](#bib.bib122)] | N-Cars [[44](#bib.bib44)] | ASL-DVS [[65](#bib.bib65)]
    | N-ImageNet [[48](#bib.bib48)] |'
  prefs: []
  type: TYPE_TB
- en: '| TPAMI 2015 | HFirst [[66](#bib.bib66)] | 0.712 | N/A | 0.054 | N/A | 0.561
    | N/A | N/A | 21.79M |'
  prefs: []
  type: TYPE_TB
- en: '| TPAMI 2016 | HOTS [[43](#bib.bib43)] | 0.808 | 0.803 | 0.210 | 0.271 | 0.624
    | N/A | N/A | 21.79M |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2018 | HATS [[44](#bib.bib44)] | 0.991 | 0.984 | 0.642 | 0.524 | 0.902
    | N/A | 0.471 | 21.79M |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2019 | EST [[39](#bib.bib39)] | N/A | N/A | 0.817 | N/A | 0.925 | N/A
    | 0.489 | 21.79M |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2019 | RG-CNNs [[52](#bib.bib52)] | 0.990 | 0.986 | 0.657 | 0.540 |
    0.914 | 0.901 | N/A | 19.46M |'
  prefs: []
  type: TYPE_TB
- en: '| TPAMI 2019 | DART [[123](#bib.bib123)] | 0.979 | 0.985 | 0.664 | 0.658 |
    N/A | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2020 | Matrix-LSTM [[40](#bib.bib40)] | 0.989 | N/A | 0.843 | N/A |
    0.943 | 0.997 | 0.322 | 25.56M |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2020 | ASCN [[59](#bib.bib59)] | N/A | N/A | 0.745 | N/A | 0.944 | N/A
    | N/A | 9.47M |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2021 | EvS  [[124](#bib.bib124)] | N/A | 0.991 | 0.761 | 0.680 | 0.931
    | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2021 | DiST [[48](#bib.bib48)] | N/A | N/A | N/A | N/A | N/A | N/A |
    0.484 | 21.79M |'
  prefs: []
  type: TYPE_TB
- en: '| TCSVT 2021 | MVF-Net [[38](#bib.bib38)] | 0.993 | N/A | 0.871 | 0.663 | 0.968
    | 0.996 | N/A | 21.79M |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | AEGNNs [[125](#bib.bib125)] | N/A | N/A | 0.668 | N/A | 0.945
    | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | EV-VGCNN [[53](#bib.bib53)] | 0.994 | N/A | 0.748 | N/A | 0.953
    | 0.983 | N/A | 21.79M |'
  prefs: []
  type: TYPE_TB
- en: '| TPAMI 2022 | TORE [[3](#bib.bib3)] | 0.994 | N/A | 0.798 | N/A | 0.977 |
    0.996 | N/A | 5.94M |'
  prefs: []
  type: TYPE_TB
- en: 3.5 Event-based Deep Image/Video HDR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Insight: The HDR of events makes it naturally more advantageous to reconstruct
    an HDR image/video. The predominant methods can be divided into two main categories:
    event-based HDR image/video HDR methods [[90](#bib.bib90), [102](#bib.bib102),
    [17](#bib.bib17)] and event-guided image/video HDR methods (a hybrid of event
    and frame data) [[126](#bib.bib126), [127](#bib.bib127), [18](#bib.bib18)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Event-based image/video HDR typically employs the idea of event-to-image translation—reconstructing
    HDR images from events, as mentioned in Sec. [3.1](#S3.SS1 "3.1 Event-based Image/Video
    Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"). Representative works are based
    on the recurrent neural networks (RNNs) [[90](#bib.bib90), [102](#bib.bib102)]
    (See Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 Event-based Deep Image/Video HDR ‣ 3 Image
    Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks") (a)) or generative adversarial networks (GANs) [[17](#bib.bib17)]
    (See Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 Event-based Deep Image/Video HDR ‣ 3 Image
    Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks") (b)). However, the reconstructed HDR results intrinsically
    lack textural details, especially in the static scene, as events are sparse and
    motion-dependent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/43598221bba1b31f9aa472c57b8d8bdc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Representative DL-based HDR imaging methods. (a) RNN-based methods [[90](#bib.bib90),
    [102](#bib.bib102)] and (b) GAN-based method [[17](#bib.bib17)] that use events
    only, and (c) event-frame fusion [[126](#bib.bib126), [127](#bib.bib127), [18](#bib.bib18)].
    (latent embedding denotes information learned by feature extractor, filter,etc.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Event-guided image/video HDR: HDR imaging methods are categorized into two
    types: single-exposure HDR and multi-exposure HDR (See [[128](#bib.bib128)] for
    details). Event-guided image/video also follows these two paradigms.  [[126](#bib.bib126),
    [127](#bib.bib127), [18](#bib.bib18)] explore the potential of merging both events
    and frames for this task, as shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 Event-based
    Deep Image/Video HDR ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for
    Event-based Vision: A Comprehensive Survey and Benchmarks") (c). In particular,
    Han et al. [[126](#bib.bib126)] proposed the first single-exposure HDR imaging
    framework to recover an HR HDR image by adding the LR intensity map generated
    from events. This framework addresses the gaps in spatial resolution, dynamic
    range, and color representation of the hybrid sensor system to pursue a better
    fusion. By contrast, EHDR  [[18](#bib.bib18)] is the first multi-exposure HDR
    imaging framework that combines bracketed LDR images and synchronized events to
    recover an HDR image. To alleviate the impact of scene motion between exposures,
    EHDR employs events to learn a deformable convolution kernel, which can align
    feature maps from images with different exposure times. By contrast, HDRev-Net [[129](#bib.bib129)]
    implicitly mitigates the misalignment of multi-modal representations by aligning
    them in the shared latent space and fusing them with a confidence-guided fusion
    module.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Based on the review, only two research works have been proposed for
    deep HDR imaging. The most possible reason is that it is practically difficult
    to collect paired datasets for training, especially for multi-exposure HDR imaging.
    Future directions could consider directly fusing LDR images and events and learning
    a unified HDR imaging framework without relying on image/video reconstruction.
    Also, it is promising to explore how to leverage events to guide color image HDR
    imaging.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Scene Understanding and 3D Vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Scene Understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 Object Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Insight: Event-based object classification aims to identify and classify objects
    from an event stream based on their visual characteristics. This allows for real-time
    object classification with high temporal resolution and low latency, making it
    suitable for applications in robotics, autonomous vehicles, and other mobile systems.
    Intuitively, we divide the event-based classification methods into three categories
    according to the input event representations and DNN types: 1) learning-based;
    2) graph-based; and 3) asynchronous model-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning-based methods Gehrig et al. [[39](#bib.bib39)] proposed the first end-to-end
    framework to learn event representation for object classification. In particular,
    it converts event streams into grid-like tensors, i.e., Event Spike Tensor (EST),
    through a sequence of differentiable operations. Though EST achieves high accuracy,
    it also brings redundant computation costs and high latency. To tackle this problem,
    Cannici et al. [[40](#bib.bib40)] proposed Matrix-LSTM to adaptively integrate
    and utilize information of events by the memory mechanism of LSTM. This makes
    it possible to efficiently aggregate the temporal information of event data.
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based methods Some works also utilize graphs for representing events for
    the computational efficiency of the Graph CNNs. Yin et al. [[65](#bib.bib65)]
    proposed a representative approach that represents event data as a graph and introduced
    residual graph CNNs (RG-CNNs).
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous model-based methods Though the learning-based methods obtain plausible
    classification results, they fail to fully explore the inherent asynchronicity
    and sparsity of event data. Consequently, Nico et al. [[59](#bib.bib59)] converted
    the classification models trained on the synchronous frame-like event representations
    into models taking asynchronous events as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Comparison of existing representative event classification benchmarks.
    MR denotes Monitor Recording. MR is the process of capturing the visual output
    displayed on a computer monitor or screen.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | # of Samples | # of Classes | Sources | Paired RGB Data |'
  prefs: []
  type: TYPE_TB
- en: '| N-Cars [[44](#bib.bib44)] | 24029 | 2 | Real | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| N-Caltech101 [[120](#bib.bib120)] | 8709 | 101 | MR | Caltech101 |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR10-DVS [[122](#bib.bib122)] | 10000 | 10 | MR | CIFAR10 |'
  prefs: []
  type: TYPE_TB
- en: '| ASL-DVS [[65](#bib.bib65)] | 100800 | 24 | Real | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| N-MNIST [[120](#bib.bib120)] | 70000 | 10 | MR | MNIST |'
  prefs: []
  type: TYPE_TB
- en: '| MNIST-DVS [[121](#bib.bib121)] | 30000 | 10 | MR | MNIST |'
  prefs: []
  type: TYPE_TB
- en: '| N-ImageNet [[48](#bib.bib48)] | 1781167 | 1000 | MR | ImageNet |'
  prefs: []
  type: TYPE_TB
- en: Besides, to fit with the sparse event data, VMV-GCN[[130](#bib.bib130)] first
    considers the relationships between vertices of the graph and then groups the
    vertices according to the proximity both in the original input and feature space.
    Furthermore, for computational efficiency, AEGNN [[125](#bib.bib125)] proposes
    to process events sparsely and asynchronously as temporally evolving graphs. Meanwhile,
    EV-VGCNN[[53](#bib.bib53)] utilizes voxel-wise vertices rather than point-wise
    inputs to explicitly exploit the regional 2D semantics of event streams while
    maintaining the trade-off between accuracy and model complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: Comparison of existing representative event object detection methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Publications | Method | Representations | Highlight | Backbone | Frame Images
    | Multi-modal |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | YOLE [[131](#bib.bib131)] | Surface-based | Event-based neural
    network components | CNN | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| WACV 2022 | PointConv [[132](#bib.bib132)] | Image-based | pint-cloud feature
    extractor | CNN | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| MFI 2022 | GFA-Net [[133](#bib.bib133)] | Image-based | Edge information
    & Temporal information across event frames | CNN & ViT | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2020 | NGA [[23](#bib.bib23)] | Image-based | Grafting pre-trained deep
    network for novel sensors | CNN | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| NeurIPS 2020 | RED [[23](#bib.bib23)] | Image-based | Recurrent architecture
    and temporal consistency | RNN | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| TIP 2022 | ASTMNet [[134](#bib.bib134)] | Image-based | Continuous event
    stream with lightweight RNN | RNN | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ICRA 2019 | Mixed-Yolo [[135](#bib.bib135)] | Image-based | Mixed APS frame
    and DVS frame | CNN | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ICME 2019 | JDF [[136](#bib.bib136)] | Spike-based | Joint detection with
    event streams and frames | CNN & SNN | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ICRA 2022 | FPN-fusion events [[137](#bib.bib137)] | Image-based | Robust
    detection with RGB and event-based sensors | CNN | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'Benchmark datasets are vital foundations for the development of event-based
    vision, given that sufficient event data is barely available due to the novelty
    of event sensors. The existing event datasets can be briefly divided into two
    categories according to the captured scenes, i.e., the real and the simulated
    ones. Gehrig et al. [[25](#bib.bib25)] proposed to convert video datasets into
    event datasets by adaptive upsampling and using an event camera simulator (ESIM) [[85](#bib.bib85)].
    Models trained on the simulated dataset generalize well on the real data. More
    recently, N-ImageNet [[48](#bib.bib48)] serves as the first real large-scale fine-grained
    benchmark, which provides various validation sets to test the robustness of event-based
    object recognition approaches amidst changes in motion or illumination. We summarize
    the existing datasets for event recognition in Tab. [VII](#S4.T7 "TABLE VII ‣
    4.1.1 Object Classification ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") and conduct a benchmark evaluation for the representative event-based
    classification methods in Tab. [VI](#S3.T6 "TABLE VI ‣ 3.4 Event-guided Image/Video
    Deblurring ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: The accuracy of event-based object classification is always hindered
    by insufficient annotated datasets. Therefore, endeavors have been made to simulate
    event data using ESIM or generate event data from the monitor-displayed image
    observation. It also deserves to adapt the model trained on synthetic data to
    real-world event data [[138](#bib.bib138), [139](#bib.bib139)]. Another research
    direction could focus on leveraging large amounts of unlabeled data or active
    learning, where the classifier can request additional labeled data as needed in
    order to improve its accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Feature Tracking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Insight: In recent years, researchers have focused on event-based feature tracking
    for its robustness in fast motion capture and extreme lighting conditions  [[30](#bib.bib30),
    [5](#bib.bib5)]. Early event-based feature trackers treat events as point sets
    and use Iterative Closest Point (ICP) [[140](#bib.bib140)] to track features [[141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144)], and there are also
    works use B-splines [[145](#bib.bib145)] and some other techniques to obtain the
    feature trajectories [[5](#bib.bib5), [146](#bib.bib146), [7](#bib.bib7), [147](#bib.bib147),
    [148](#bib.bib148)].'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, deep learning-based method, a.k.a., data-driven methods for event-based
    feature tracking method has been proposed. The most representative one is  [[30](#bib.bib30)]
    which serves as the first work of introducing a data-driven feature tracker for
    event cameras, leveraging low-latency events to track features detected in a grayscale
    frame. The data-driven tracker outperforms the existing non-DL-based methods in
    relative feature age by up to 120% while keeping the lowest latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: From our review, we find that deep learning is just introduced to
    event-based feature tracking recently and it is worth exploring this direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: The quantitative results of the evaluated trackers on the EDS and
    EC dataset are reported in terms of ”Feature Aeg (FA)” of the stable tracks and
    the ”Expected FA”, which is the multiplication of the feature age by the ratio
    of the number of table tracks over the number of initial features. This table
    is from  [[30](#bib.bib30)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | EDS | EC |'
  prefs: []
  type: TYPE_TB
- en: '| Feature Age (FA) | Expected FA | Feature Age (FA) | Expected FA |'
  prefs: []
  type: TYPE_TB
- en: '| ICP [[140](#bib.bib140)] | 0.060 | 0.040 | 0.256 | 0.245 |'
  prefs: []
  type: TYPE_TB
- en: '| EM-ICP [[143](#bib.bib143)] | 0.161 | 0.120 | 0.337 | 0.334 |'
  prefs: []
  type: TYPE_TB
- en: '| HASTE [[147](#bib.bib147)] | 0.096 | 0.063 | 0.442 | 0.427 |'
  prefs: []
  type: TYPE_TB
- en: '| EKLT [[5](#bib.bib5)] | 0.325 | 0.205 | 0.811 | 0.775 |'
  prefs: []
  type: TYPE_TB
- en: '| DDFT [[30](#bib.bib30)] (zero-shot) | 0.549 | 0.451 | 0.811 | 0.787 |'
  prefs: []
  type: TYPE_TB
- en: '| DDFT [[30](#bib.bib30)] (fine-tuned) | 0.576 | 0.472 | 0.825 | 0.818 |'
  prefs: []
  type: TYPE_TB
- en: 4.1.3 Object Detection and Tracking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Object Detection: Event cameras bring a new perspective in dealing with the
    challenges in object detection (e.g., motion blur, occlusions, and extreme lighting
    conditions). In reality, the RGB-based detection fails to enable robust perception
    under image corruptions or extreme weather conditions. Meanwhile, auxiliary sensors,
    such as LiDARs, are extremely bulky and costly [[137](#bib.bib137)]. Therefore,
    event-based detectors are introduced to overcome the dilemma, especially in challenging
    visual conditions [[133](#bib.bib133)]. In this work, we divide the event-based
    object detection methods into three categories according to the input data formats
    and data representations, as summarized in Tab. [VIII](#S4.T8 "TABLE VIII ‣ 4.1.1
    Object Classification ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D
    Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks").'
  prefs: []
  type: TYPE_NORMAL
- en: The first category simply converts the raw event data into frame-based images [[22](#bib.bib22),
    [149](#bib.bib149), [131](#bib.bib131), [135](#bib.bib135), [136](#bib.bib136),
    [133](#bib.bib133), [132](#bib.bib132), [150](#bib.bib150)], e.g., the recurrent
    vision transformer (RVT) [[150](#bib.bib150)] which takes 2-channel frames within
    a time duration. However, this kind of method loses the raw spatial-temporal information
    in the event stream. For this reason, event volume and some other formats tailored
    for object detectors are used in the methods of the second category. Some works [[58](#bib.bib58),
    [23](#bib.bib23)] obtain event volumes by taking the linear or convolve kernels
    to integrate the asynchronous events into multiple slices within the equal temporal
    volume.
  prefs: []
  type: TYPE_NORMAL
- en: However, the event volume still follows a frame-like 2D representation, and
    critical temporal information is lost. Recently, ASTMNet  [[134](#bib.bib134)]
    exploits the spatial-temporal information by directly processing the asynchronous
    events instead of the 2D frame-like representations. Furthermore, it also serves
    as the first end-to-end pipeline for continuous object detection. Notably, a branch
    of research has introduced temporal hints by integrating recurrent neural network
    layers, resulting in significant enhancements in detection accuracy  [[150](#bib.bib150),
    [23](#bib.bib23), [134](#bib.bib134)]. The third category of attempts combines
    the advantages of event images and RGB images [[135](#bib.bib135), [136](#bib.bib136),
    [151](#bib.bib151), [152](#bib.bib152)]. Tomy et al. [[137](#bib.bib137)] proposed
    a representative framework that fuses the information from the event- and frame-based
    cameras for better detection accuracy in normal conditions and robust performance
    in the presence of extreme scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Object Tracking: Tracking dynamic objects is an essential task in mobile robots,
    which requires the basic functionality of obstacle avoidance. RGB camera-based
    trackers perform poorly for high-speed and dynamic objects because of motion blur
    and time-delayed transmission. In such cases, introducing event cameras to address
    this problem is of great value.'
  prefs: []
  type: TYPE_NORMAL
- en: For mobile robots, earlier methods proposed to track moving objects on the conditions
    of geometric priors [[153](#bib.bib153)], known shape [[154](#bib.bib154)], [[154](#bib.bib154)]
    and motion-compensation model [[21](#bib.bib21)], [[155](#bib.bib155)]. More recently,
    many DL-based methods designed for canonical image data have undergone a paradigm
    shift and have been applied successfully to event data. For instance, the widely
    adopted object detector—Yolo and object tracker—Kalman filter, have been applied
    to event data, showing pleasant outcomes [[156](#bib.bib156)]. Many more endeavors
    have been made to enable the onboard inference ability of the deep learning models.
    For instance, EVDodge [[157](#bib.bib157)] specifically decouples the network
    to predict obstacle motion and ego-m otion separately by introducing two event
    cameras facing the ground and front, demonstrating that it outperforms their monocular
    counterparts which use a single event camera. Furthermore, EVReflex  [[158](#bib.bib158)]
    suggests an additional LiDARs sensor instead of an additional event camera exhibiting
    higher accuracy than EVDodge. EV-Catcher [[159](#bib.bib159)] trains a small CNN
    to process single-channel event images, achieving an inference speed of 2ms, which
    is faster than its predecessor by a large margin [[160](#bib.bib160)]. Because
    EV-Catcher only regresses the real-time target position and its uncertainty at
    x-coordinate from the DNNs, further estimation of hitting position and timing
    is based on the linear motion assumptions. Others tend to proceed with this problem
    in an end-to-end manner, showing marginal benefits.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE X: Experiments of representative methods on event-based optical flow
    estimation from  [[161](#bib.bib161)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'UL: unsupervised learning. SL: supervised learning. MB: model-based methods.
    ($\cdot$): evaluation on both outdoor_day1 and outdoor_day2 sequences. [$\cdot$]:
    evaluation on outdoor_day2 sequences. N/A means no results are available. The
    results without any brackets mean that they are not trained on any sequence of
    MVSEC.)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Method | Metric | indoor_flying1 | indoor_flying2 | indoor_flying3
    | outdoor_day1 | Param. |'
  prefs: []
  type: TYPE_TB
- en: '| EPE | %Out | EPE | %Out | EPE | %Out | EPE | %Out |'
  prefs: []
  type: TYPE_TB
- en: '| UL | Ev-FlowNet [[34](#bib.bib34)] | sparse | (1.03) | (2.2) | (1.72) | (15.1)
    | (1.53) | (11.9) | [0.49] | [0.2] | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Zhu et al. [[162](#bib.bib162)] | sparse | (0.58) | (0.0) | (1.02) | (4.0)
    | (0.87) | (3.0) | [0.32] | [0.0] | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Matrix-LSTM [[40](#bib.bib40)] | sparse | (0.82) | (0.53) | (1.19) | (5.59)
    | (1.08) | (4.81) | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Spike-FLowNet [[163](#bib.bib163)] | sparse | [0.84] | N/A | [1.28] | N/A
    | [1.11] | N/A | [0.49] | N/A | 13.039M |'
  prefs: []
  type: TYPE_TB
- en: '| Paredes et al. [[92](#bib.bib92)] | sparse | (0.79) | (1.2) | (1.40) | (10.9)
    | (1.18) | (7.4) | [0.92] | [5.4] | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| LIF-EV-FlowNet [[164](#bib.bib164)] | sparse | 0.71 | 1.41 | 1.44 | 12.75
    | 1.16 | 9.11 | 0.53 | 0.33 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Deng et al. [[165](#bib.bib165)] | sparse | (0.89) | (0.66) | (1.31) | (6.44)
    | (1.13) | (3.53) | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[166](#bib.bib166)] | sparse | (0.59) | (0.83) | (0.64) | (2.26)
    | N/A | N/A | [0.31] | [0.03] | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| STE-FlowNet [[167](#bib.bib167)] | sparse | [0.57] | [0.1] | [0.79] | [1.6]
    | [0.72] | [1.3] | [0.42] | [0.0] | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| SL | Stoffregen et al. [[91](#bib.bib91)] | dense | 0.56 | 1.00 | 0.66 |
    1.00 | 0.59 | 1.00 | 0.68 | 0.99 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| EST [[39](#bib.bib39)] | sparse | (0.97) | (0.91) | (1.38) | (8.20) | (1.43)
    | (6.47) | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| DCEIFlow [[161](#bib.bib161)] | dense | 0.56 | 0.28 | 0.64 | 0.16 | 0.57
    | 0.12 | 0.91 | 0.71 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| DCEIFlow [[161](#bib.bib161)] | sparse | 0.57 | 0.30 | 0.70 | 0.30 | 0.58
    | 0.15 | 0.74 | 0.29 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| MB | Pan et al. [[168](#bib.bib168)] | sparse | 0.93 | 0.48 | 0.93 | 0.48
    | 0.93 | 0.48 | 0.93 | 0.48 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Shiba [[169](#bib.bib169)] | sparse | 0.42 | 0.10 | 0.60 | 0.59 | 0.50 |
    0.28 | 0.30 | 0.10 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| $\text{Fusion-FlowNet}_{Early}$ [[170](#bib.bib170)] | dense | (0.56) | N/A
    | (0.95) | N/A | (0.76) | N/A | [0.59] | N/A | 12.269M |'
  prefs: []
  type: TYPE_TB
- en: '| $\text{Fusion-FlowNet}_{Late}$ [[170](#bib.bib170)] | sparse | (0.57) | N/A
    | (0.99) | N/A | (0.79) | N/A | [0.55] | N/A | 7.549M | ![Refer to caption](img/221954002d9b13c0853cdc1b71b8b042.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Visualization results of semantic segmentation with events, (a) Events,
    (b) Pseudo Label, (c) Ev-Transfer [[171](#bib.bib171)], (d) Image, (e) ESS [[172](#bib.bib172)],
    (f) E2VID [[90](#bib.bib90)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Overall, it is worth exploring how to build effective object detectors
    that leverage event cameras to overcome the deficiency of frame-based detectors
    in extreme visual conditions, e.g., high-speed motion or dark night. Meanwhile,
    it is promising to apply the abundant off-the-shelf DL-based object detectors,
    e.g., recurrent vision transformer [[150](#bib.bib150)], to event cameras. Moreover,
    though event cameras show distinct advantages, RGB-based cameras are still occupied
    by the mainstream object detection tasks. Intuitively, fusing the event and frame
    data could help improve detection accuracy, especially in scenarios where the
    events are temporarily absent, e.g., the static motion scenarios. In object detection
    and tracking, more and more attention is paid to multi-modal sensor fusion. Moreover,
    pure event-based solutions could be explored to exonerate the additional expensive
    sensors, like LiDARs, while achieving comparable or better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Semantic Segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Image segmentation [[173](#bib.bib173)] is a fundamental vision task with many
    pivotal applications [[174](#bib.bib174), [175](#bib.bib175), [176](#bib.bib176)],
    including robotic perception, scene understanding, augmented reality, etc. In
    these practical scenarios, the segmentation models always fail in the non-ideal
    weather and lighting conditions [[4](#bib.bib4)], leading to poor scene perception
    of intelligent systems. Event-based semantic segmentation, which is first proposed
    in Ev-SegNet [[24](#bib.bib24)], achieves a significant improvement by utilizing
    the asynchronous event data. Ev-SegNet also introduces a dataset extended from
    the DDD17 dataset [[177](#bib.bib177)]. However, the resolution and image quality
    is less satisfactory for the semantic segmentation task.
  prefs: []
  type: TYPE_NORMAL
- en: To address this problem, Gehrig et al. [[25](#bib.bib25)] proposed to convert
    video data to synthetic events. This work unlocks the usage of a large number
    of existing video datasets for event-based semantic segmentation. Inspired by
    this synthetic data source, Wang et al. [[178](#bib.bib178), [179](#bib.bib179)]
    suggested combining the labeled RGB data and unlabeled event data in a cross-modal
    knowledge distillation setting, so as to alleviate the shortage of labeled real
    event data. For higher segmentation results, Zhang et al. [[4](#bib.bib4)] constructed
    a multi-modal segmentation benchmark model by using the complementary information
    in both event and RGB branches. More recently, ESS [[172](#bib.bib172)] proposes
    an unsupervised domain adaptation (UDA) framework that leverages the still images
    without paired events and frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Due to the lack of precisely annotated large-scale real-world event
    datasets, existing works mostly focus on generating pseudo labels. However, the
    labels are not precise enough, rendering the learned segmentation models less
    robust, as demonstrated by the visual results in Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.3
    Object Detection and Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks"). Future work could further explore the multi-modal domain adaption
    from RGB data to event data for semantic segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 Optical Flow Estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Insight: Optical flow estimation is the process of estimating the motion field
    within an image sequence. Conventional RGB-based methods remain unsatisfying in
    extreme lighting conditions, e.g., at night and in high-speed motion. To overcome
    these limitations, event cameras have been introduced. The SOTA event-based methods
    for optical flow estimation can be classified into two categories: traditional
    methods and DL-based methods. DL-based methods further encompass supervised and
    unsupervised approaches. Table. [X](#S4.T10 "TABLE X ‣ 4.1.3 Object Detection
    and Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D Vision ‣
    Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    presents the results achieved by several representative methods in the field.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/93a78497cba654a26a4ab97183c52846.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Representative optical flow estimation methods, including supervised
    methods [[39](#bib.bib39), [44](#bib.bib44), [33](#bib.bib33)], (a) Correlation-based
    methods, (b) Multi-task learning methods, and unsupervised learning methods [[162](#bib.bib162),
    [51](#bib.bib51)] (c) Multi-task learning methods, (d) self-supervised learning
    methods, and (e) SNN-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional methods: Recent research has delved into understanding the principles
    and characteristics of event data that facilitate the estimation process. These
    studies have particularly focused on leveraging contrast maximization methods
    to estimate optical flow accurately [[180](#bib.bib180), [181](#bib.bib181), [169](#bib.bib169)].
    Furthermore, there is ongoing research aimed at designing innovative event camera
    platforms specifically tailored for the hardware implementation of adaptive block-matching
    optical flow. These platforms serve as practical demonstrations of the effectiveness
    of this approach [[61](#bib.bib61)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised methods: In  [[39](#bib.bib39), [44](#bib.bib44), [33](#bib.bib33)],
    event streams are first converted into image-based or surface-based representations
    and then trained via standard convolutional neural networks (CNNs). Kepple et
    al.[[182](#bib.bib182)] proposed to simultaneously generate the region’s local
    flow and the reliability of the prediction. Gehrig et al.[[114](#bib.bib114)]
    proposed an RNN-based framework that utilizes the cost volumes and learns the
    feature correlation of the volumetric voxel grid of events, so as to estimate
    optical ﬂow.'
  prefs: []
  type: TYPE_NORMAL
- en: Some research employs SNNs for optical flow estimation [[183](#bib.bib183),
    [184](#bib.bib184), [163](#bib.bib163)]. For example, Haessig et al. [[183](#bib.bib183)]
    introduced an SNN variant of the Barlow and Levick model utilizing IBM’s TrueNorth
    Neurosynaptic System. However, deep SNNs suffer from spike vanishing problems.
    To this end, [[163](#bib.bib163)] combined SNNs and CNNs in an end-to-end manner
    to estimate optical ﬂow, while [[184](#bib.bib184)] proposed a hierarchical SNN
    architecture for feature extraction and local and global motion perception. Future
    research could explore combining SNNs and transformer [[152](#bib.bib152)] to
    learn the global and local visual information from events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised methods: Recent research is focused on unsupervised learning for
    solving the data scarcity problem. Zhu et al. [[162](#bib.bib162)] introduced
    a novel event representation containing two channels for encoding the number of
    positive and negative events and two for the timestamp of the most recent positive
    and negative events. They utilized the grayscale, i.e., APS, images of the event
    camera as the self-supervision signals to train the network. Ye et al. [[51](#bib.bib51)]
    simultaneously predicted the dense depth and optical flow with two corresponding
    neural networks. With the guidance of depth maps, the optical flow is calculated
    based on the poses of neighboring frames and the depth of the middle frame. However,
    these methods are still based on the photo consistency principle, while this assumption
    may not be valid in some adverse visual conditions (e.g., high-speed motion).
    To this end, Zhu et al.[[162](#bib.bib162)] proposed a discretized volumetric
    event representation to maintain the events’ temporal distribution, and the input
    processed event data is used to predict the motions and remove the motion blur.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: While optical flow may not have standalone usefulness, it serves as
    a valuable tool in driving other computations or closed-loop control in various
    computer vision tasks. Its ability to estimate motion patterns provides crucial
    information for tasks like object tracking, visual odometry, video stabilization,
    action recognition, and motion-based segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.6 Depth estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bcbc59735cbdb2cc7de3fbd38e920980.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Event-based depth estimation methods, including (a) Monocular event
    only method [[185](#bib.bib185)], (b) Monocular event-frame based method  [[186](#bib.bib186)],
    (c) Stereo event only method [[187](#bib.bib187), [188](#bib.bib188)], (d) Stereo
    event-frame based method  [[189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Insight : Events streams reflect abundant edge information, HDR, and high temporal
    resolution, benefiting depth estimation tasks, especially in extreme conditions.
    Depth can be learned from either the monocular (single) input or stereo (multi-view
    of a scene) inputs. Under this outline, we categorize the depth estimation methods
    based on how events are used and learned.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Monocular depth estimation: Based on whether events are used alone or combined
    with the intensity frames, we divide the monocular depth estimation methods into
    two types. 1) Events-only approaches: [[185](#bib.bib185)] is a representative
    approach that adopts a recurrent network [[64](#bib.bib64)] to learn the temporal
    information from grid-like event inputs, depicted in Fig. [10](#S4.F10 "Figure
    10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") (a). However, as monocular depth estimation from events is an ill-posed
    problem, rendering such a learning framework difficult to achieve highly precise
    depth. Moreover, this learning paradigm may fail to predict depth in the static
    scene as events are only triggered by motion. 2) event-plus-frame approaches:
    RAM [[186](#bib.bib186)] employs the same RNN as [[185](#bib.bib185)] but combines
    events and frames (i.e., as complementary to each other) to learn to predict depth
    from the multi-modal inputs asynchronously, as shown in Fig. [10](#S4.F10 "Figure
    10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") (b). Nonetheless, the recurrent network is inevitably accompanied
    by long-term memory costs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stereo depth estimation: As the visual cues of the left and right event cameras
    are used in the stereo setting, the model complexity and memory cost of learning
    pipelines become more prohibitive. [[187](#bib.bib187), [188](#bib.bib188)] are
    two pioneering works in stereo depth estimation, as shown in Fig. [10](#S4.F10
    "Figure 10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") (c). In particular, DDES [[187](#bib.bib187)] is the first learning-based
    stereo-matching method, and in  [[188](#bib.bib188)], the first unsupervised learning
    framework is proposed. Both methods store events at each position as a First-in
    First-out queue, enabling concurrent time and polarity reservation. To adaptively
    extract features from sparse data, Zhang et al. [[192](#bib.bib192)] proposed
    continuous time convolution and discrete time convolution to encode high dimensional
    spatial-temporal event data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast, some research explores multi-modality fusion under different settings
    which serves as a remedy to utilize the benefits of each modality. HDES [[193](#bib.bib193)]
    mitigates the modal-gap between the data from different viewpoints by introducing
    a hybrid pyramid attention module for multi-modal data fusion. EIS [[189](#bib.bib189)]
    is a representative work to combine events and frames with a recycling network,
    as depicted in Fig. [10](#S4.F10 "Figure 10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene
    Understanding ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks") (d). However, since events are
    sparse, event stacking is an important factor that affects the quality of fusion
    and depth prediction because stacking inappropriate amounts of events can lead
    to information overriding or missing problems. To this end,  [[190](#bib.bib190),
    [191](#bib.bib191)] propose a selection module to filter more useful events. Specifically,
    Nam et al. [[190](#bib.bib190)] concatenated the event stacks with different densities
    and then adaptively learn these stacks to highlight the contribution of the well-stacked
    events. Moreover, considering the constant motion of the cameras, SCSNet [[191](#bib.bib191)]
    introduces a differentiable event selection network to extract more reliable events
    and correlate the feature from a neighbor region of events and images, diminishing
    the disruption of bad alignment intuitively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: From our review, inter-camera spatial correlation is the key to content
    matching between events and frames. Although [[189](#bib.bib189), [190](#bib.bib190),
    [191](#bib.bib191)] combine events and frames, it still deserves exploring which
    part events contribute most to the multi-modal feature fusion and alignment. Also,
    it is possible to use an event camera and a frame-based camera for stereo depth
    estimation. Future research could consider exploring these directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 3D Vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1 Visual SLAM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Insight: It is an essential module for various applications, e.g., robotic
    navigation and virtual reality. Visual SLAM receives the signals, e.g., 2D images,
    as the source for ego-motion estimation and builds 3D maps, which can generally
    be defined as the tracking thread and the mapping thread. Event-based visual SLAM
    shares a similar spirit and benefits from the robustness of event cameras to light-changing
    and fast-moving conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4cc32294b8a129e05a3b2f11a3c472af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: (Left) Illustration of the general framework for event-based SLAM
    via deep learning, key elements are retrieved from [[194](#bib.bib194), [195](#bib.bib195),
    [51](#bib.bib51), [162](#bib.bib162)]. The colored point cloud is a map reconstructed
    from event data of different sensor views (green trajectory) grounded by robot
    location (red directional trajectory). (Right) Visualization of 3D reconstruction
    from (a) blurred RGB images and (b) events from  [[196](#bib.bib196)].'
  prefs: []
  type: TYPE_NORMAL
- en: In traditional SLAM, depth and ego-motion could be easily estimated through
    triangulation and the subsequent local pose estimation by, e.g., Perspective-N-Points.
    Recently, the learning-based approaches are also been applied in SLAM, such as,
    [[51](#bib.bib51), [162](#bib.bib162)] propose united frameworks—with an encoder-decoder
    structure—for optical flow, depth, and ego-motion estimation.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, in [[51](#bib.bib51)], a united framework is proposed to estimate
    sparse optical flow, depths, and ego-motion, in which an encoder-decoder structure
    was adopted for sparse depth estimation, The key idea behind this framework is
    that the maximization of event frame contrast through optical flow warping provides
    natural, high-quality edge maps and enables applying a multi-view stereo loss
    to learn metric poses and depth. Two sub-networks under this framework are trained
    for the prediction of optical flow and depths, respectively.By contrast, Zhu et
    al.[[162](#bib.bib162)] proposed to directly learn 6-DOF poses from multi-view
    intensity frames, other than deriving from optical flow and depth, like [[51](#bib.bib51)].
    The recent EAGAN [[197](#bib.bib197)] adopts the vision transformer  [[198](#bib.bib198)]
    to boost the accuracy of optical flow estimation, yet no optimization is done
    to depth estimation. However, EAGAN shows an increase in the number of learnable
    parameters compared to the methods [[162](#bib.bib162), [51](#bib.bib51)], and
    the ego-motion estimation is not considered.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, a branch of research casts event-based SLAM as a re-localization problem.
    This paradigm proposes to directly learn the camera poses from extracted deep
    features in an end-to-end trainable manner. For example, Nguyen et al.[[199](#bib.bib199)]
    proposed a framework, with CNNs and four LSTM blocks after fully-connected layers
    of PoseNet [[200](#bib.bib200)], to regress the 6-DOF poses from event images
    directly. This, for the first time, reveals the potential of event data to address
    the large-scale re-localization problem. Later on, additional denoising modules
    are introduced in [[201](#bib.bib201)] to further increase the pose estimation
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Currently, the event-based SLAM systems employing deep learning are
    generally decoupled as separate modules rather than the cross-event frame and
    pose-map joint estimation as traditionally processed in visual SLAM. Some attempts
    [[162](#bib.bib162), [51](#bib.bib51)] generate frame association (as a form of
    optical flow), pose estimation, and depth map with a unified network architecture.
    Still, global consistency remains an unsolved problem. For the front end, applying
    DNNs for the intra-frame association purpose is a challenging problem. This leaves
    a vacancy for exploration since many frame-based SLAM systems have proved that
    the pose derived by cross-frame feature association is highly precise. Another
    interesting direction to investigate is the dense map or mesh reconstruction from
    event data that served as mapping for SLAM. we have seen mesh reconstruction with
    very limited precision and designed for small-size objects in Sec.[XI](#S4.T11
    "TABLE XI ‣ 4.2.2 3D Reconstruction ‣ 4.2 3D Vision ‣ 4 Scene Understanding and
    3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    leaving learning-based scene reconstruction an open problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 3D Reconstruction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'TABLE XI: Comparison of existing representative event-based 3D reconstruction
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Publications | Methods | Task | Representations | Frame Input | Real Time
    | Multi Cameras |'
  prefs: []
  type: TYPE_TB
- en: '| Arxiv 2020 | E3D [[196](#bib.bib196)] | 3D Reconstruction | Image-based |
    ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2020 | Stereo-event PTV [[202](#bib.bib202)] | 3D Fluid Flow Reconstruction
    | Stream-based | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2021 | EventHands [[203](#bib.bib203)] | 3D Hand Pose Estimation | Surface-based
    | ✗ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2022 | EvAC3D [[204](#bib.bib204)] | 3D Reconstruction | Surface-based
    | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: 'Insight: Event cameras capture dominant scene features, e.g., edges and silhouettes,
    making them more suitable for some 3D reconstruction methods than the frame-based
    data (See Fig. [11](#S4.F11 "Figure 11 ‣ 4.2.1 Visual SLAM ‣ 4.2 3D Vision ‣ 4
    Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks")).'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the traditional frame-based RGB and depth cameras widely explored in
    3D reconstruction [[205](#bib.bib205)], event cameras enjoy inherent benefits,
    such as low latency and HDR [[206](#bib.bib206), [207](#bib.bib207)]. Intuitively,
    approaches designed for prior RGB and depth data can not be directly applied to
    event data due to the distinct data format. Thus, endeavors have been made in
    converting events to sparse, semi-dense point clouds and full-frame depth maps
    for existing RGB-based pipelines [[202](#bib.bib202), [208](#bib.bib208), [209](#bib.bib209),
    [206](#bib.bib206), [186](#bib.bib186)]. These methods are committed to take advantages
    of the off-the-shelf 3D reconstruction pipelines built for RGB-based inputs while
    ignoring the unique strengths of event cameras. In some following research [[14](#bib.bib14),
    [196](#bib.bib196)], event cameras are combined with RGB cameras for the advantages
    of both sensors. For instance, Vidal et al. [[14](#bib.bib14)] proposed to simultaneously
    incorporate event data, intensity images, and inertial measurement unit (IMU)
    data in the SLAM pipeline for achieving superior accuracy than the purely event-based
    methods. Besides, some approaches combine event cameras with other types of sensors,
    such as ELS [[210](#bib.bib210)] that uses a laser point-projector and an event
    camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'To directly take advantage of the event data, Wang et al. [[211](#bib.bib211)]
    proposed to generate dimensional flow from the events instead of using the image-based
    reconstruction method [[212](#bib.bib212)] for 3D fluid flow reconstruction. More
    recently, EvAC3D [[204](#bib.bib204)] explores the direct reconstruction of mesh
    from a continuous stream of events while defining the boundaries of the objects
    as apparent contour events and continuously carving out high-fidelity meshes.
    The comparison of existing representative event-based 3D reconstruction methods
    is shown in Tab. [XI](#S4.T11 "TABLE XI ‣ 4.2.2 3D Reconstruction ‣ 4.2 3D Vision
    ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Recent research provides important insights regarding how events can
    be utilized to understand the 3D world. However, a general and unified pipeline
    is expected, which is left for future research. The fusion of the RGB and event
    cameras is also valuable for achieving better reconstruction results.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 3D Human Pose and Shape Estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Insight: The ability to capture the dynamic motion makes event cameras superior
    for estimating 3D moving objects, especially for 3D human pose and shape estimation
    (3D HPE) (see Fig. [12](#S4.F12 "Figure 12 ‣ 4.2.3 3D Human Pose and Shape Estimation
    ‣ 4.2 3D Vision ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks")).'
  prefs: []
  type: TYPE_NORMAL
- en: In the past few years, 3D HPE has been extensively explored with RGB images
    and videos in the deep learning era [[213](#bib.bib213), [214](#bib.bib214)].
    The most challenging scenario in 3D HPE is always related to the high-speed motion [[215](#bib.bib215)],
    which is essential in many practical applications, such as sports performance
    evaluation. However, the RGB cameras suffer inevitable fundamental problems [[216](#bib.bib216)],
    including unsatisfactory frame rates and data redundancy. By contrast, event cameras
    are more advisable for fast-motion scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: DHP19 [[217](#bib.bib217)] is the first DL-based pipeline and provides the first
    dataset for event-based 3D HPE. To utilize the frame-based DL algorithms, the
    event streams are transferred to DVS frames by accumulating a fixed number of
    events in  [[217](#bib.bib217)]. Meanwhile, the DHP19 takes events captured by
    multiple calibrated cameras. More recently, EventCap [[216](#bib.bib216)] is the
    first work to capture high-speed human motions from a single event camera with
    the guidance of gray-scale images. To alleviate the reliance on frame inputs,
    the following research EventHPE [[218](#bib.bib218)], proposes to infer 3D HPE
    from the sole source of event input, given the beginning shape from the first
    frame of the intensity image stream.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/09a7f8c0526c36770d00e5cf7c6e7635.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Qualitative comparison between 3D HPE methods from  [[218](#bib.bib218)].
    (a) VIBE [[215](#bib.bib215)]; (b) EventCap [[216](#bib.bib216)], (c) EventHPE [[218](#bib.bib218)]'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: A limitation of these approaches is that they need gray-scale images
    for initialization. For future research, it is worth investigating how to infer
    3D HPE purely from event signals without additional priors. Meanwhile, it also
    promises to combine the advantages of both RGB and event sensors and design multi-modal
    learning frameworks for more robust 3D HPE.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Research Trend and Discussions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Discussions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Do we really need deep networks for learning events? Motivated by the widespread
    applications and success of DL methods in computer vision using frame-based imagery,
    there is a growing research focus on applying DL to event data. DL-based approaches
    offer significant accuracy improvements compared to conventional event-based vision
    algorithms. For instance, a recent DL-based feature tracker [[30](#bib.bib30)]
    outperforms non-DL methods. Additionally, DL-based methods exhibit superior results
    in 3D HPE compared to multi-view models, showcasing the effectiveness of DL techniques [[217](#bib.bib217)].
    However, the adoption of DL techniques introduces certain challenges. For example,
    SNNs [[43](#bib.bib43), [33](#bib.bib33), [68](#bib.bib68)] naturally accommodate
    event streams and enable asynchronous inference at low computational cost. Nevertheless,
    training SNNs is difficult, and the supporting hardware infrastructure is not
    well-established, limiting their applications in the computer vision community.
    Recently, DNNs have been introduced to address tasks in event-based vision [[39](#bib.bib39)].
    However, existing methods overlook the unique characteristics of event data and
    primarily aim to bridge the domain gaps between RGB and event-based vision. Further
    exploration is needed in the conversion between raw event data and grid-based
    inputs (e.g., various event representations). Moreover, an urgent challenge is
    to determine the suitable learning pipeline for utilizing raw events with DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Do we really need convolution operations to filter events as done for image
    data? As canonical convolutions only operate through the spatial perspective,
    simply applying the 2D convolutional modules to event streams neglects the temporal
    correlation of events, leading to a sub-optimal network design. Intuitively, we
    suggest 1) using a graph to describe event streams [[53](#bib.bib53)]; 2) introducing
    the self-attention-based transformer for temporal information [[219](#bib.bib219),
    [220](#bib.bib220)]; 3) applying recurrent network [[221](#bib.bib221)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Low latency of event cameras vs. High computation of DNNs: One of the notable
    superiorities of event cameras is the low latency which enables real-time applications
    of event cameras. However, the computational complexity of the neural network
    is typically enormous, potentially negating the benefits of lower event latency.
    An important research question is how to accelerate the neural network in the
    field of events. We suggest that three angles be taken when conducting this issue.
    1) Make use of a light-weight network architecture, like MobileNet[[222](#bib.bib222),
    [223](#bib.bib223)] Shuffle Net[[224](#bib.bib224), [225](#bib.bib225)]. 2) Make
    use of network quantization and compression techniques[[226](#bib.bib226)]. 3)
    Construct a network based on SNN[[227](#bib.bib227)], which has low delay and
    sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to better deal with noisy events with DNNs? Random noise can be introduced
    throughout the trigger process of events due to many reasons. Thus the denoising
    procedure is necessary for accurate information capture. According to the existing
    denoising methods in Sec. [2.2.2](#S2.SS2.SSS2 "2.2.2 Event Denoising ‣ 2.2 Quality
    Enhancement for Events ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"), we suggest dealing with the noisy
    events with DNNs in three steps: 1) Formulate the spatial and temporal distributions
    of raw events; 2) Separately denoising from both spatial and temporal perspectives,
    e.g., operations in spatial neighbourhoods and current event surfaces; 3) Further
    consider the correlation between spatial and temporal distributions, aiming at
    maintaining spatial-temporal correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Can the high temporal resolution of events be fully reflected by DNNs? No matter
    what kind of event representation, summarized in Sec. [2.1](#S2.SS1 "2.1 Event
    Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks"), is used as DNN’s input, the grid-like
    tensors always lose some partial information of raw events, such as temporal information
    in the image-based representations [[33](#bib.bib33), [34](#bib.bib34), [39](#bib.bib39),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]. Obviously, the high temporal
    resolution of events cannot be fully reflected by the existing DNNs which are
    proposed to solve frame-based vision. The micro-second level temporal resolutions
    are predominantly fused to frame-like representations for the downstream tasks.
    SNNs can solve this problem according to their architectures, however, there are
    still technical difficulties in bringing SNNs into practical applications. Thus
    specific neural networks directly designed for event data are in the future outlook.'
  prefs: []
  type: TYPE_NORMAL
- en: Do events contain sufficient visual information for learning robust DNN models?
  prefs: []
  type: TYPE_NORMAL
- en: Less plausible accuracy of event-based vision models in the dark. Low light
    conditions bring noise mainly due to the sufficient intensity changes that do
    not generate an event. The noises can be briefly called ”holes” or false negatives.
    This leads to unreliable scene understanding of event cameras. Employing DL for
    noise cancellation of event data is a promising approach [[71](#bib.bib71)]. In
    addition, multi-sensor fusion is a valuable direction. For example, thermal sensors
    have been widely used in the dark [[228](#bib.bib228)]. Therefore, developing
    DL-based methods that take thermal and event data as inputs is worth exploring.
  prefs: []
  type: TYPE_NORMAL
- en: Are DL-based methods more advantageous than optimized-based ones? Optimization-based
    methods are more applicable to edge computing  [[229](#bib.bib229)] but are still
    unlikely to reach global optima—more likely to be trapped in saddle points  [[230](#bib.bib230)]
    ). By contrast, DNNs are superior in that they can flexibly learn the multi-dimension
    data and extract better feature representations. Intuitively, DNNs have the potential
    to learn spatial-temporal information from events. Superior results has been demonstrated
    in exploring the temporal correlation of events with various DNNs in the literature [[219](#bib.bib219),
    [220](#bib.bib220), [221](#bib.bib221)]. Also, owning to the numerical stability
    and efficiency, DNNs allow hidden state encoding for effective prediction in various
    tasks. For example, in the context of event-based SLAM, implicit encoding of event
    streams exhibits higher spatial-temporal consistency than naively tracking fired
    pixels, enabling estimation of ego-motion and 3D scene reconstruction jointly,
    whereas the optimization-based method shows significantly lower accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Is focal alignment necessary between RGB and event pixels in event cameras?
    Event cameras have emerged as an efficient alternative for capturing motion information.
    Recent studies highlight the potential of DNNs in leveraging both RGB and event
    data to enhance images, enabling crucial tasks like deblurring and video frame
    interpolation. This capability holds significant practical applications in areas
    such as augmented reality and virtual reality. However, achieving the desired
    image quality mandates RGB characteristics comparable to advanced mobile RGB sensors,
    along with precise focal alignment between RGB and event pixels on the sensor.
    To address these challenges, one promising approach involves the development of
    hybrid-type sensors that seamlessly integrate high-frame-rate event pixels with
    advanced mobile RGB pixels [[231](#bib.bib231), [232](#bib.bib232), [233](#bib.bib233)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 New Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NeRF for Event-based Neural Rendering: NeRF  [[234](#bib.bib234)] is a representative
    neural implicit 3D representation method that synthesizes 3D objects with volume
    rendering techniques. Most existing research in NeRF is investigated based on
    RGB cameras, which suffer from inevitable shortcomings, e.g., low dynamic range
    and motion blur in unfavorable lighting conditions. Thus, recent attention has
    been paid to the usage of event cameras for NeRF [[235](#bib.bib235), [236](#bib.bib236),
    [237](#bib.bib237)]. The first work is EventNeRF [[235](#bib.bib235)], which is
    trained with pure event-based supervision. It demonstrates that the NeRF estimation
    from a single fast-moving event camera in unfavourable scenarios (e.g., fast-moving
    objects, motion blur, or insufficient lighting) is feasible while frame-based
    approaches fail. Moreover, E-NeRF [[236](#bib.bib236)] takes the strengths of
    RGB and event cameras by combining color frames and events to achieve sharp and
    colorize reconstruction results.'
  prefs: []
  type: TYPE_NORMAL
- en: From the review of recent progress, event-based NeRF methods show superior abilities
    than the frame-based NeRF methods. Since the event-based NeRF is an emerging direction,
    future research could focus on improving the technique pipelines and more lightweight
    DNNs for mobile applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi- and Cross-modal Learning for Event-based Vision: In practical scenarios,
    event cameras always play an auxiliary role to provide multi-modal guidance in
    many aforementioned computer vision tasks, e.g., image and video SR [[112](#bib.bib112),
    [111](#bib.bib111), [108](#bib.bib108)]. With the development of event cameras,
    event-based vision will occupy an increasingly dominant position in both research
    and industry. Especially in some specific domains, there are already attempts
    to leverage pure event data to facilitate task accuracy, such as reconstructing
    RGB images and videos from pure event data [[17](#bib.bib17), [64](#bib.bib64),
    [102](#bib.bib102), [90](#bib.bib90)], etc. Consequently, how to utilize the domain
    knowledge from the frame-based vision to the emerging event-based vision deserves
    more intensive research. Recently, CTN [[238](#bib.bib238)] is proposed as an
    early attempt of transferring knowledge from frame-based vision to event-based
    vision.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Event-based model Pretaining: Pre-trained neural networks are the foundations
    of almost all downstream task models in the deep learning era. For the frame-based
    vision, the pre-trained weights on ImageNet are widely utilized for models’ outstanding
    accuracy gains. With the growing interest in event-based vision, a wide range
    of datasets are collected in many downstream tasks. Thus, unified pre-trained
    weights with large-scale data are required for better accuracy. Since the event
    data is a totally distinct format containing spatial-temporal information, a reliable
    and efficient pre-trained method is required for various applications. Recently,
    in  [[239](#bib.bib239)], Yang et al. proposed the first pre-training pipeline
    for dealing with event camera data, including various event data augmentations,
    a masking sample strategy, and a contrastive learning approach. Additionally,
    Hu et al. [[58](#bib.bib58)]  proposed the Network Grafting Algorithm, which replaces
    the front-end network of a pre-trained DNN with a new network that processes unconventional
    visual inputs. Through self-supervised training using synchronously-recorded intensity
    frames and novel sensor data, the algorithm maximizes feature similarity between
    the pretrained network and the grafted network. This enhances the pretrained network’s
    ability to process and extract meaningful features from diverse input sources.
    Future research can focus on how to fully utilize all kinds of event representations,
    summarized in Sec. [2.1](#S2.SS1 "2.1 Event Representation ‣ 2 Event Processing
    for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    to realize better event-based pre-training. More details of new directions can
    be found in the supplementary material.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] G. Gallego, T. Delbrück, G. Orchard, C. Bartolozzi, B. Taba, A. Censi,
    S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis *et al.*, “Event-based
    vision: A survey,” *TPAMI*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] M. Davies, A. Wild, G. Orchard, Y. Sandamirskaya, G. A. F. Guerra, P. Joshi,
    P. Plank, and S. R. Risbud, “Advancing neuromorphic computing with loihi: A survey
    of results and outlook,” *Proceedings of the IEEE*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] R. Baldwin, R. Liu, M. M. Almatrafi, V. K. Asari, and K. Hirakawa, “Time-ordered
    recent event volumes for event cameras,” *TPAMI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Zhang, K. Yang, and R. Stiefelhagen, “ISSAFE: Improving semantic segmentation
    in accidents by fusing event-based data.”   IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, “EKLT: Asynchronous
    photometric feature tracking using events and frames,” *IJCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] I. Alzugaray and M. Chli, “Asynchronous corner detection and tracking for
    event cameras in real time,” *RAL*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Alzugaray, Ignacio and Chli, Margarita, “ACE: An efficient asynchronous
    corner tracker for event cameras,” in *3DV*.   IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] R. Li, D. Shi, Y. Zhang, K. Li, and R. Li, “FA-Harris: A Fast and Asynchronous
    Corner Detector for Event Cameras,” in *IROS*.   IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Glover, A. Dinale, L. D. S. Rosa, S. Bamford, and C. Bartolozzi, “LuvHarris:
    A Practical Corner Detector for Event-cameras,” *TPAMI*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Henri, Rebecq, Timo, Horstschaefer, Guillermo, Gallego, Davide, and Scaramuzza,
    “EVO: A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping
    in Real Time,” *RAL*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Hidalgo-Carrió, G. Gallego, and D. Scaramuzza, “Event-aided direct
    sparse odometry,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] E. Mueggler, H. Rebecq, G. Gallego, T. Delbruck, and D. Scaramuzza, “The
    event-camera dataset and simulator: Event-based data for pose estimation, visual
    odometry, and SLAM,” *Int J Rob Res.*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Milford, H. Kim, S. Leutenegger, and A. Davison, “Towards visual slam
    with event-based cameras,” in *The problem of mobile sensors workshop in conjunction
    with RSS*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza, “Ultimate
    SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High
    Speed Scenarios,” *RAL*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] J. Jiao, H. Huang, L. Li, Z. He, Y. Zhu, and M. Liu, “Comparing Representations
    in Tracking for Event Camera-based SLAM,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Z. Chen, Q. Zheng, P. Niu, H. Tang, and G. Pan, “Indoor lighting estimation
    using an event camera,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. M. M. Isfahani, L. Wang, Y.-S. Ho, and K. jin Yoon, “Event-Based High
    Dynamic Range Image and Very High Frame Rate Video Generation Using Conditional
    Generative Adversarial Networks,” *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] N. Messikommer, S. Georgoulis, D. Gehrig, S. Tulyakov, J. Erbach, A. Bochicchio,
    Y. Li, and D. Scaramuzza, “Multi-Bracket High Dynamic Range Imaging with Event
    Cameras,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] C. Scheerlinck, H. Rebecq, D. Gehrig, N. Barnes, R. Mahony, and D. Scaramuzza,
    “Fast image reconstruction with an event camera,” in *WACV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. Mondal, J. H. Giraldo, T. Bouwmans, A. S. Chowdhury *et al.*, “Moving
    Object Detection for Event-based Vision using Graph Spectral Clustering,” in *ICCV*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] A. Mitrokhin, C. Fermüller, C. Parameshwara, and Y. Aloimonos, “Event-Based
    Moving Object Detection and Tracking,” in *IROS*.   IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] M. Iacono, S. Weber, A. Glover, and C. Bartolozzi, “Towards event-driven
    object detection with off-the-shelf deep learning,” in *IROS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] E. Perot, P. de Tournemire, D. Nitti, J. Masci, and A. Sironi, “Learning
    to detect objects with a 1 megapixel event camera,” *Adv Neural Inf Process Syst.*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] I. Alonso and A. C. Murillo, “EV-SegNet: Semantic segmentation for event-based
    cameras,” in *CVPRW*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] D. Gehrig, M. Gehrig, J. Hidalgo-Carrió, and D. Scaramuzza, “Video to
    events: Recycling video datasets for event cameras,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] R. Serrano-Gotarredona, M. Oster, P. Lichtsteiner, A. Linares-Barranco,
    R. Paz-Vicente, F. Gómez-Rodríguez, L. Camuñas-Mesa, R. Berner, M. Rivas-Pérez,
    T. Delbruck *et al.*, “CAVIAR: A 45k neuron, 5M synapse, 12G connects/s AER hardware
    sensory–processing–learning–actuating system for high-speed visual object recognition
    and tracking,” *IEEE Transactions on Neural networks*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] T. Delbrück, B. Linares-Barranco, E. Culurciello, and C. Posch, “Activity-driven,
    event-based vision sensors,” in *Proceedings of 2010 IEEE International Symposium
    on Circuits and Systems*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] P. Lichtsteiner, C. Posch, and T. Delbruck, “A 128× 128 120 dB 15 us Latency
    Asynchronous Temporal Contrast Vision Sensor,” *IEEE Journal of Solid-State Circuits*,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] P. U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, and M. Pfeiffer, “Fast-classifying,
    high-accuracy spiking deep networks through weight and threshold balancing,” in
    *International joint conference on neural networks*.   IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] N. Messikommer, C. Fang, M. Gehrig, and D. Scaramuzza, “Data-driven feature
    tracking for event cameras,” in *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S.-C. Liu, B. Rueckauer, E. Ceolini, A. Huber, and T. Delbruck, “Event-driven
    sensing for efficient perception: Vision and audition algorithms,” *IEEE Signal
    Processing Magazine*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. Lin, F. Xu, X. Wang, W. Yang, and L. Yu, “Efficient Spatial-Temporal
    Normalization of SAE Representation for Event Camera,” *RAL*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. I. Maqueda, A. Loquercio, G. Gallego, N. García, and D. Scaramuzza,
    “Event-based vision meets deep learning on steering prediction for self-driving
    cars,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, “EV-FlowNet: Self-supervised
    optical flow estimation for event-based cameras,” in *Robotics: Science and Systems*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Wang, B. Du, Y. Shen, K. Wu, G. Zhao, J. Sun, and H. Wen, “EV-Gait:
    Event-based robust gait recognition using dynamic vision sensors,” in *CVPR*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Y. Deng, Y. Li, and H. Chen, “AMAE: Adaptive Motion-Agnostic Encoder for
    Event-Based Object Classification,” *RAL*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] W. Bai, Y. Chen, R. Feng, and Y. Zheng, “Accurate and Efficient Frame-based
    Event Representation for AER Object Recognition,” in *IJCNN*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Deng, H. Chen, and Y. Li, “MVF-Net: A Multi-View Fusion Network for
    Event-Based Object Classification,” *TCSVT*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] D. Gehrig, A. Loquercio, K. G. Derpanis, and D. Scaramuzza, “End-to-end
    learning of representations for asynchronous event-based data,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, “A differentiable
    recurrent surface for asynchronous event-based data,” in *ECCV*.   Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] P. K. Park, B. H. Cho, J. M. Park, K. Lee, H. Y. Kim, H. A. Kang, H. G.
    Lee, J. Woo, Y. Roh, W. J. Lee *et al.*, “Performance improvement of deep learning
    based gesture recognition using spatiotemporal demosaicing technique,” in *ICIP*.   IEEE,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] R. Benosman, C. Clercq, X. Lagorce, S.-H. Ieng, and C. Bartolozzi, “Event-based
    visual flow,” *TNNLS*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] X. Lagorce, F. Orchard, Garrick andc Galluppi, B. E. Shi, and R. B. Benosman,
    “Hots: a hierarchy of event-based time-surfaces for pattern recognition,” *TPAMI*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Sironi, M. Brambilla, N. Bourdis, X. Lagorce, and R. Benosman, “HATS:
    Histograms of averaged time surfaces for robust event-based object classification,”
    in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] R. W. Baldwin, M. Almatrafi, J. R. Kaufman, V. Asari, and K. Hirakawa,
    “Inceptive event time-surfaces for object classification using neuromorphic cameras,”
    in *ICIAR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Manderscheid, A. Sironi, N. Bourdis, D. Migliore, and V. Lepetit, “Speed
    Invariant Time Surface for Learning to Detect Corner Points With Event-Based Cameras,”
    in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Almatrafi, R. Baldwin, K. Aizawa, and K. Hirakawa, “Distance Surface
    for Event-Based Optical Flow,” *TPAMI*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. Kim, J. Bae, G. Park, D. Zhang, and Y. M. Kim, “N-imagenet: Towards
    robust, fine-grained object recognition with event cameras,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] A. Zihao Zhu, L. Yuan, K. Chaney, and K. Daniilidis, “Unsupervised event-based
    optical flow using motion compensation,” in *ECCVW*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] F. Gu, W. Sng, T. Taunyazov, and H. Soh, “Tactilesgnet: A spiking graph
    neural network for event-based tactile object recognition,” in *IROS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C. Ye, A. Mitrokhin, C. Fermüller, J. A. Yorke, and Y. Aloimonos, “Unsupervised
    Learning of Dense Optical Flow, Depth and Egomotion with Event-Based Sensors,”
    in *IROS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-based
    spatio-temporal feature learning for neuromorphic vision sensing,” *TIP*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Deng, H. Chen, H. Liu, and Y. Li, “A Voxel Graph CNN for Object Classification
    With Event Cameras,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] J. H. Lee, T. Delbruck, and M. Pfeiffer, “Training deep spiking neural
    networks using backpropagation,” *FRONT NEUROSCI-SWITZ*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J. Botzheim, T. Obo, and N. Kubota, “Human gesture recognition for robot
    partners by spiking neural network and classification learning,” in *SCIS*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Amir, B. Taba, D. Berg, T. Melano, J. McKinstry, C. Di Nolfo, T. Nayak,
    A. Andreopoulos, G. Garreau, M. Mendoza *et al.*, “A low power, fully event-based
    gesture recognition system,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] D. P. Moeys, F. Corradi, E. Kerr, P. Vance, G. Das, D. Neil, D. Kerr,
    and T. Delbrück, “Steering a predator robot using a mixed frame/event-driven convolutional
    neural network,” in *2016 Second international conference on event-based control,
    communication, and signal processing (EBCCSP)*.   IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Hu, T. Delbruck, and S.-C. Liu, “Learning to exploit multiple vision
    modalities by using grafted networks,” in *ECCV*.   Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] N. Messikommer, D. Gehrig, A. Loquercio, and D. Scaramuzza, “Event-based
    asynchronous sparse convolutional networks,” in *ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] M. Liu and T. Delbruck, “Adaptive time-slice block-matching optical flow
    algorithm for dynamic vision sensors.”   BMVC, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Liu, Min and Delbruck, Tobi, “EDFLOW: Event driven optical flow camera
    with keypoint detection and adaptive block matching,” *IEEE Transactions on Circuits
    and Systems for Video Technology*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] B. R. Pradhan, Y. Bethi, S. Narayanan, A. Chakraborty, and C. S. Thakur,
    “N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory
    Surfaces,” in *ISCAS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] S. Afshar, T. J. Hamilton, J. Tapson, A. Van Schaik, and G. Cohen, “Investigation
    of event-based surfaces for high-speed detection, unsupervised feature extraction,
    and object recognition,” *Frontiers in Neuroscience*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, “Events-to-video:
    Bringing modern computer vision to event cameras,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-based
    object classification for neuromorphic vision sensing,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] G. Orchard, C. Meyer, R. Etienne-Cummings, C. Posch, N. Thakor, and R. Benosman,
    “HFirst: A temporal approach to object recognition,” *IEEE transactions on pattern
    analysis and machine intelligence*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] B. Zhao, R. Ding, S. Chen, B. Linares-Barranco, and H. Tang, “Feedforward
    categorization on aer motion events using cortex-like features in a spiking neural
    network,” *TNNLS*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] D. Neil, M. Pfeiffer, and S.-C. Liu, “Phased LSTM: Accelerating recurrent
    network training for long or event-based sequences,” *NeurIPS*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] C. Brandli, L. Muller, and T. Delbruck, “Real-time, high-speed video decompression
    using a frame-and event-based davis sensor,” in *Proc. IEEE Int. Symp. Circuits
    Syst.*   IEEE, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] H. Li, G. Li, and L. Shi, “Super-resolution of spatiotemporal event-stream
    image,” *Neurocomputing*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Z. W. Wang, P. Duan, O. Cossairt, A. Katsaggelos, T. Huang, and B. Shi,
    “Joint filtering of intensity images and neuromorphic events for high-resolution
    noise-robust imaging,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] P. Duan, Z. W. Wang, X. Zhou, Y. Ma, and B. Shi, “Eventzoom: Learning
    to denoise and super resolve neuromorphic events,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S. Li, Y. Feng, Y. Li, Y. Jiang, C. Zou, and Y. Gao, “Event stream super-resolution
    via spatiotemporal constraint learning,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] W. Weng, Y. Zhang, and Z. Xiong, “Boosting event stream super-resolution
    with a recurrent neural network,” in *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] J. Barrios-Avilés, A. Rosado-Muñoz, L. D. Medus, M. Bataller-Mompeán,
    and J. F. Guerrero-Martínez, “Less data same information for event-based sensors:
    A bioinspired filtering and data reduction algorithm,” *Sensors*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] A. Khodamoradi and R. Kastner, “$o(n)$-space spatiotemporal filter for
    reducing noise in neuromorphic vision sensors,” *TETC*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] H. Liu, C. Brandli, C. Li, S.-C. Liu, and T. Delbruck, “Design of a spatiotemporal
    correlation filter for event-based sensors,” in *ISCAS*.   IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] D. Czech and G. Orchard, “Evaluating noise filtering for event-based asynchronous
    change detection image sensors,” in *BioRob*.   IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] T. Delbruck, “Frame-free dynamic digital vision,” in *Proceedings of Intl.
    Symp. on Secure-Life Electronics, Advanced Electronics for Quality Life and Society*,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] S. Guo and T. Delbruck, “Low cost and latency event camera background
    activity denoising,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] R. Baldwin, M. Almatrafi, V. Asari, and K. Hirakawa, “Event probability
    mask (epm) and event denoising convolutional neural network (edncnn) for neuromorphic
    cameras,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] H. Fang, J. Wu, L. Li, J. Hou, W. Dong, and G. Shi, “Aednet: Asynchronous
    event denoising with spatial-temporal correlation among irregular data,” in *ACM
    MM*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Lin, Y. Ma, Z. Guo, and B. Wen, “Dvs-voltmeter: Stochastic process-based
    event simulator for dynamic vision sensors,” in *ECCV*.   Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. Hu, S.-C. Liu, and T. Delbruck, “v2e: From video frames to realistic
    dvs events,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] H. Rebecq, D. Gehrig, and D. Scaramuzza, “Esim: an open event camera simulator,”
    in *CoRL*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] H. Kim, A. Handa, R. Benosman, S.-H. Ieng, and A. J. Davison, “Simultaneous
    mosaicing and tracking with an event camera,” *J. Solid State Circ*, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] M. Cook, L. Gugelmann, F. Jug, C. Krautz, and A. Steger, “Interacting
    maps for fast visual interpretation,” in *IJCNN*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] G. Munda, C. Reinbacher, and T. Pock, “Real-time intensity-image reconstruction
    for event cameras using manifold regularisation,” *IJCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Z. Zhang, A. Yezzi, and G. Gallego, “Formulating event-based image reconstruction
    as a linear inverse problem with deep regularization using optical flow,” *TPAMI*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Rebecq, Henri and Ranftl, René and Koltun, Vladlen and Scaramuzza, Davide,
    “High speed and high dynamic range video with an event camera,” *TPAMI*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] T. Stoffregen, C. Scheerlinck, D. Scaramuzza, T. Drummond, N. Barnes,
    L. Kleeman, and R. Mahony, “Reducing the sim-to-real gap for event cameras,” in
    *ECCV*.   Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] F. Paredes-Vallés and G. C. de Croon, “Back to event basics: Self-supervised
    learning of image reconstruction for event cameras via photometric constancy,”
    in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] S. Tulyakov, D. Gehrig, S. Georgoulis, J. Erbach, M. Gehrig, Y. Li, and
    D. Scaramuzza, “Time Lens: Event-based video frame interpolation,” in *CVPR*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] G. Paikin, Y. Ater, R. Shaul, and E. Soloveichik, “EFI-Net: Video Frame
    Interpolation from Fusion of Events and Frames,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Z. Yu, Y. Zhang, D. Liu, D. Zou, X. Chen, Y. Liu, and J. S. Ren, “Training
    weakly supervised video frame interpolation with events,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] W. He, K. You, Z. Qiao, X. Jia, Z. Zhang, W. Wang, H. Lu, Y. Wang, and
    J. Liao, “Timereplayer: Unlocking the potential of event cameras for video interpolation,”
    in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. Tulyakov, A. Bochicchio, D. Gehrig, S. Georgoulis, Y. Li, and D. Scaramuzza,
    “Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow
    and Multi-scale Fusion,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] S. Wu, K. You, W. He, C. Yang, Y. Tian, Y. Wang, Z. Zhang, and J. Liao,
    “Video interpolation by event-driven anisotropic adjustment of optical flow,”
    in *ECCV*.   Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Lin, J. Zhang, J. Pan, Z. Jiang, D. Zou, Y. Wang, J. Chen, and J. Ren,
    “Learning event-driven video deblurring and interpolation,” in *ECCV*.   Springer,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] C. Song, Q. Huang, and C. Bajaj, “E-cir: Event-enhanced continuous intensity
    recovery,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X. Zhang and L. Yu, “Unifying motion deblurring and frame interpolation
    with events,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. Zou, Y. Zheng, T. Takatani, and Y. Fu, “Learning to reconstruct high
    speed and high dynamic range videos from events,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] L. Yu, W. Yang *et al.*, “Event-based high frame-rate video reconstruction
    with a novel cycle-event network,” in *ICIP*.   IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] A. Z. Zhu, Z. Wang, K. Khant, and K. Daniilidis, “Eventgan: Leveraging
    large scale image datasets for event cameras,” in *ICCP*.   IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] H. C. Duwek, A. Shalumov, and E. E. Tsur, “Image reconstruction from
    neuromorphic event cameras using laplacian-prediction and poisson integration
    with spiking and artificial neural networks,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] L. Pan, C. Scheerlinck, X. Yu, R. Hartley, M. Liu, and Y. Dai, “Bringing
    a blurry frame alive at high frame-rate with an event camera,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Choi, K.-J. Yoon *et al.*, “Learning to super resolve intensity images
    from events,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Jing, Y. Yang, X. Wang, M. Song, and D. Tao, “Turning frequency to
    resolution: Video super-resolution via event cameras,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] L. Wang, T.-K. Kim, and K.-J. Yoon, “EventSR: From Asynchronous Events
    to Image Reconstruction, Restoration, and Super-Resolution via End-to-End Adversarial
    Learning,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] M. Mostafavi, L. Wang, and K.-J. Yoon, “Learning to reconstruct hdr images
    from events, with applications to depth and flow prediction,” *IJCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] B. Wang, J. He, L. Yu, G.-S. Xia, and W. Yang, “Event enhanced high-quality
    image recovery,” in *ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] J. Han, Y. Yang, C. Zhou, C. Xu, and B. Shi, “Evintsr-net: Event guided
    multiple latent frames reconstruction and super-resolution,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Y. Lu, Z. Wang, M. Liu, H. Wang, and L. Wang, “Learning Spatial-Temporal
    Implicit Neural Representations for Event-Guided Video Super-Resolution,” in *CVPR*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] M. Gehrig, M. Millhäusler, D. Gehrig, and D. Scaramuzza, “E-raft: Dense
    optical flow from event cameras,” in *3DV*.   IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] M. Jin, G. Meishvili, and P. Favaro, “Learning to extract a video sequence
    from a single motion-blurred image,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] F. Xu, L. Yu, B. Wang, W. Yang, G.-S. Xia, X. Jia, Z. Qiao, and J. Liu,
    “Motion deblurring with real events,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Z. Jiang, Y. Zhang, D. Zou, J. Ren, J. Lv, and Y. Liu, “Learning event-based
    motion deblurring,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] W. Shang, D. Ren, D. Zou, J. S. Ren, P. Luo, and W. Zuo, “Bringing events
    into video deblurring with non-consecutively blurry frames,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] T. Kim, J. Lee, L. Wang, and K.-J. Yoon, “Event-guided Deblurring of
    Unknown Exposure Time Videos,” in *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] G. Orchard, A. Jayawant, G. K. Cohen, and N. Thakor, “Converting static
    image datasets to spiking neuromorphic datasets using saccades,” *Frontiers in
    Neuroscience*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] T. Serrano-Gotarredona and B. Linares-Barranco, “Poker-DVS and MNIST-DVS.
    Their history, how they were made, and other details,” *Frontiers in Neuroscience*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] H. Li, H. Liu, X. Ji, G. Li, and L. Shi, “Cifar10-dvs: an event-stream
    dataset for object classification,” *Frontiers in Neuroscience*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] B. Ramesh, H. Yang, G. Orchard, N. A. Le Thi, S. Zhang, and C. Xiang,
    “Dart: distribution aware retinal transform for event-based cameras,” *TPAMI*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Y. Li, H. Zhou, B. Yang, Y. Zhang, Z. Cui, H. Bao, and G. Zhang, “Graph-based
    asynchronous event processing for rapid object recognition,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] S. Schaefer, D. Gehrig, and D. Scaramuzza, “AEGNN: Asynchronous Event-based
    Graph Neural Networks,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Han, C. Zhou, P. Duan, Y. Tang, C. Xu, C. Xu, T. Huang, and B. Shi,
    “Neuromorphic Camera Guided High Dynamic Range Imaging,” *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Z. Wang, Y. Ng, C. Scheerlinck, and R. E. Mahony, “An Asynchronous Kalman
    Filter for Hybrid Event Cameras,” *ICCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] L. Wang and K.-J. Yoon, “Deep Learning for HDR Imaging: State-of-the-Art
    and Future Trends,” *TPAMI*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Y. Yang, J. Han, J. Liang, I. Sato, and B. Shi, “Learning event guided
    high dynamic range video reconstruction,” in *CVPR*, 2023, pp. 13 924–13 934.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] B. Xie, Y. Deng, Z. Shao, H. Liu, and Y. Li, “VMV-GCN: Volumetric Multi-View
    Based Graph CNN for Event Stream Classification,” *RAL*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, “Asynchronous
    convolutional networks for object detection in neuromorphic cameras,” in *CVPRW*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] N. Salvatore and J. Fletcher, “Learned Event-based Visual Perception
    for Improved Space Object Detection,” in *WACV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Z. Liang, H. Cao, C. Yang, Z. Zhang, and G. Chen, “Global-local Feature
    Aggregation for Event-based Object Detection on EventKITTI,” in *MFI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] J. Li, J. Li, L. Zhu, X. Xiang, T. Huang, and Y. Tian, “Asynchronous
    Spatio-Temporal Memory Network for Continuous Event-Based Object Detection,” *TIP*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Z. Jiang, P. Xia, K. Huang, W. Stechele, G. Chen, Z. Bing, and A. Knoll,
    “Mixed frame-/event-driven fast pedestrian detection,” in *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] J. Li, S. Dong, Z. Yu, Y. Tian, and T. Huang, “Event-based vision enhanced:
    A joint detection framework in autonomous driving,” in *ICME*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] A. Tomy, A. Paigwar, K. S. Mann, A. Renzaglia, and C. Laugier, “Fusing
    Event-based and RGB camera for Robust Object Detection in Adverse Conditions,”
    in *ICRA*.   IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] J. Kim, I. Hwang, and Y. M. Kim, “Ev-TTA: Test-Time Adaptation for Event-Based
    Object Recognition,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] M. Planamente, C. Plizzari, M. Cannici, M. Ciccone, F. Strada, A. Bottino,
    M. Matteucci, and B. Caputo, “Da4event: towards bridging the sim-to-real gap for
    event cameras using domain adaptation,” *RAL*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] P. J. Besl and N. D. McKay, “Method for registration of 3-d shapes,”
    in *Sens. Fusion*.   Spie, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] B. Kueng, E. Mueggler, G. Gallego, and D. Scaramuzza, “Low-latency visual
    odometry using event-based feature tracks,” in *IROS*.   IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Z. Ni, A. Bolopion, J. Agnus, R. Benosman, and S. Régnier, “Asynchronous
    event-based visual shape tracking for stable haptic feedback in microrobotics,”
    *IEEE Trans. Robot.*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A. Z. Zhu, N. Atanasov, and K. Daniilidis, “Event-based feature tracking
    with probabilistic data association,” in *IEEE ICRA*.   IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Y. Dong and T. Zhang, “Standard and Event Cameras Fusion for Feature
    Tracking,” in *Int. Conf. Mach. Vis. Appl.*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] J. Chui, S. Klenk, and D. Cremers, “Event-Based Feature Tracking in Continuous
    Time with Sliding Window Optimization.” [Online]. Available: [http://arxiv.org/abs/2107.04536.](http://arxiv.org/abs/2107.04536.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] H. Seok and J. Lim, “Robust feature tracking in dvs event stream using
    bézier mapping,” in *WACV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] I. Alzugaray and M. Chli, “Haste: multi-hypothesis asynchronous speeded-up
    tracking of events,” in *BMVC*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] S. Hu, Y. Kim, H. Lim, A. J. Lee, and H. Myung, “eCDT: Event Clustering
    for Simultaneous Feature Detection and Tracking,” in *IROS*.   IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] “Pseudo-labels for supervised learning on dynamic vision sensor data,
    applied to object detection under ego-motion, author=Chen, Nicholas FY,” in *CVPRW*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] M. Gehrig and D. Scaramuzza, “Recurrent vision transformers for object
    detection with event cameras,” in *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] H. Cao, G. Chen, J. Xia, G. Zhuang, and A. Knoll, “Fusion-based feature
    attention gate component for vehicle detection based on event camera,” *IEEE Sensors
    Journal*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] M. Liu, N. Qi, Y. Shi, and B. Yin, “An attention fusion network for event-based
    vehicle object detection,” in *ICIP*.   IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] “Event-driven ball detection and gaze fixation in clutter, author=Glover,
    Arren and Bartolozzi, Chiara,” in *IROS*.   IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] V. Vasco, A. Glover, E. Mueggler, D. Scaramuzza, L. Natale, and C. Bartolozzi,
    “Independent motion detection with event-driven cameras,” in *ICAR*.   IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] T. Stoffregen, G. Gallego, T. Drummond, L. Kleeman, and D. Scaramuzza,
    “Event-Based Motion Segmentation by Motion Compensation,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] R. Jiang, X. Mou, S. Shi, Y. Zhou, Q. Wang, M. Dong, and S. Chen, “Object
    tracking on event cameras with offline–online learning,” *CAAI Transactions on
    Intelligence Technology*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] N. J. Sanket, C. M. Parameshwara, C. D. Singh, A. V. Kuruttukulam, C. Fermüller,
    D. Scaramuzza, and Y. Aloimonos, “EVDodgeNet: Deep Dynamic Obstacle Dodging with
    Event Cameras,” in *ICRA*.   IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] C. Walters and S. Hadfield, “Evreflex: Dense time-to-impact prediction
    for event-based obstacle avoidance,” in *IROS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Z. Wang, F. C. Ojeda, A. Bisulco, D. Lee, C. J. Taylor, K. Daniilidis,
    M. A. Hsieh, D. D. Lee, and V. Isler, “EV-Catcher: High-Speed Object Catching
    Using Low-Latency Event-Based Neural Networks,” *RAL*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Bisulco, F. C. Ojeda, V. Isler, and D. D. Lee, “Fast motion understanding
    with spatiotemporal neural networks and dynamic vision sensors,” in *ICRA*.   IEEE,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Z. Wan, Y. Dai, and Y. Mao, “Learning Dense and Continuous Optical Flow
    From an Event Camera,” *TIP*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, “Unsupervised event-based
    learning of optical flow, depth, and egomotion,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] C. Lee, A. K. Kosta, A. Z. Zhu, K. Chaney, K. Daniilidis, and K. Roy,
    “Spike-flownet: event-based optical flow estimation with energy-efficient hybrid
    neural networks,” in *ECCV*.   Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] J. Hagenaars, F. Paredes-Vallés, and G. De Croon, “Self-supervised learning
    of event-based optical flow with spiking neural networks,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Y. Deng, H. Chen, H. Chen, and Y. Li, “Learning from images: A distillation
    learning framework for event cameras,” *TIP*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Z. Li, J. Shen, and R. Liu, “A lightweight network to learn optical flow
    from event data,” in *ICPR*.   IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Z. Ding, R. Zhao, J. Zhang, T. Gao, R. Xiong, Z. Yu, and T. Huang, “Spatio-temporal
    recurrent networks for event-based optical flow estimation,” in *AAAI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] L. Pan, M. Liu, and R. Hartley, “Single image optical flow estimation
    with an event camera,” in *CVPR*.   IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] S. Shiba, Y. Aoki, and G. Gallego, “Secrets of event-based optical flow,”
    in *ECCV*.   Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] C. Lee, A. K. Kosta, and K. Roy, “Fusion-FlowNet: Energy-efficient optical
    flow estimation using sensor fusion and deep fused spiking-analog network architectures,”
    in *ICRA*.   IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] N. Messikommer, D. Gehrig, M. Gehrig, and D. Scaramuzza, “Bridging the
    gap between events and frames through unsupervised domain adaptation,” *RAL*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Z. Sun, N. Messikommer, D. Gehrig, and D. Scaramuzza, “ESS: Learning
    Event-Based Semantic Segmentation from Still Images,” in *ECCV*.   Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos,
    “Image segmentation using deep learning: A survey,” *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] S. Jia, “Event Camera Survey and Extension Application to Semantic Segmentation,”
    in *IPMV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] A. Deliege, A. Cioppa, S. Giancola, M. J. Seikavandi, J. V. Dueholm,
    K. Nasrollahi, B. Ghanem, T. B. Moeslund, and M. Van Droogenbroeck, “Soccernet-v2:
    A dataset and benchmarks for holistic understanding of broadcast soccer videos,”
    in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] J. Zhang, K. Yang, and R. Stiefelhagen, “Exploring event-driven dynamic
    context for accident scene segmentation,” *TITS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] J. Binas, D. Neil, S.-C. Liu, and T. Delbruck, “DDD17: End-to-end DAVIS
    driving dataset,” *arXiv:1711.01458*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] L. Wang, Y. Chae, S.-H. Yoon, T.-K. Kim, and K.-J. Yoon, “Evdistill:
    Asynchronous events to end-task learning via bidirectional reconstruction-guided
    cross-modal knowledge distillation,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] L. Wang, D. Li, Y. Zhu, L. Tian, and Y. Shan, “Dual super-resolution
    learning for semantic segmentation,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] G. Gallego, H. Rebecq, and D. Scaramuzza, “A unifying contrast maximization
    framework for event cameras, with applications to motion, depth, and optical flow
    estimation,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] T. Stoffregen and L. Kleeman, “Event cameras, contrast maximization and
    reward functions: An analysis,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] D. R. Kepple, D. Lee, C. Prepsius, V. Isler, I. M. Park, and D. D. Lee,
    “Jointly learning visual motion and confidence from local patches in event cameras,”
    in *ECCV*.   Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] G. Haessig, A. Cassidy, R. Alvarez, R. Benosman, and G. Orchard, “Spiking
    optical flow for event-based sensors using IBM’s TrueNorth neurosynaptic system,”
    *T BIOMED CIRC S*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] F. Paredes-Vallés, K. Y. Scheper, and G. C. De Croon, “Unsupervised learning
    of a hierarchical spiking neural network for optical flow estimation: From events
    to global motion perception,” *TPAMI*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] J. Hidalgo-Carrió, D. Gehrig, and D. Scaramuzza, “Learning Monocular
    Dense Depth from Events,” in *3DV*.   IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] D. Gehrig, M. Rüegg, M. Gehrig, J. Hidalgo-Carrió, and D. Scaramuzza,
    “Combining Events and Frames Using Recurrent Asynchronous Multimodal Networks
    for Monocular Depth Prediction,” *RAL*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] S. Tulyakov, F. Fleuret, M. Kiefel, P. Gehler, and M. Hirsch, “Learning
    an Event Sequence Embedding for Dense Event-Based Deep Stereo,” *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] S. M. N. Uddin, S. H. Ahmed, and Y. J. Jung, “Unsupervised Deep Event
    Stereo for Depth Estimation,” *TCSVT*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] S. M. M. Isfahani, K.-J. Yoon, and J. Choi, “Event-Intensity Stereo:
    Estimating Depth by the Best of Both Worlds,” *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Y. Nam, S. M. M. Isfahani, K.-J. Yoon, and J. Choi, “Stereo Depth from
    Events Cameras: Concentrate and Focus on the Future,” *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] H. Cho and K.-J. Yoon, “Selection and Cross Similarity for Event-Image
    Deep Stereo,” in *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] K. Zhang, K. Che, J. Zhang, J. Cheng, Z. Zhang, Q. Guo, and L. Leng,
    “Discrete time convolution for fast event-based stereo,” *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Y. Zuo, L. Cui, X.-Z. Peng, Y. Xu, S. Gao, X. Wang, and L. Kneip, “Accurate
    depth estimation from a hybrid event-rgb stereo setup,” *IROS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] B. Alsadik and S. Karam, “The simultaneous localization and mapping (SLAM)-An
    overview,” *Surv. Geospat. Eng. J*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Y. Z., G. G., H. R., L. K., H. L., and D. S., “Semi-dense 3D reconstruction
    with a stereo event camera,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] A. Baudron, Z. W. Wang, O. Cossairt, and A. K. Katsaggelos, “E3D: Event-Based
    3D Shape Reconstruction,” *arXiv*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] X. Lin, C. Yang, X. Bian, W. Liu, and C. Wang, “EAGAN: Event‐based attention
    generative adversarial networks for optical flow and depth estimation,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] A. Nguyen, T. T. Do, D. G. Caldwell, and N. G. Tsagarakis, “Real-Time
    6DOF Pose Relocalization for Event Cameras with Stacked Spatial LSTM Networks,”
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] A. Kendall and R. Cipolla, “Geometric Loss Functions for Camera Pose
    Regression with Deep Learning,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Y. Jin, L. Yu, G. Li, and S. Fei, “A 6-DOFs event-based camera relocalization
    system by CNN-LSTM and image denoising,” *EXPERT SYST APPL*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] L. Steffen, S. Ulbrich, A. Roennau, and R. Dillmann, “Multi-view 3D reconstruction
    with self-organizing maps on event-based data,” in *ICAR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] V. Rudnev, V. Golyanik, J. Wang, H. Seidel, F. Mueller, M. Elgharib,
    and C. Theobalt, “EventHands: Real-Time Neural 3D Hand Pose Estimation from an
    Event Stream,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Z. Wang, K. Chaney, and K. Daniilidis, “EvAC3D: From Event-Based Apparent
    Contours to 3D Models via Continuous Visual Hulls,” in *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] X.-F. Han, H. Laga, and M. Bennamoun, “Image-based 3D object reconstruction:
    State-of-the-art and trends in the deep learning era,” *TPAMI*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] H. Rebecq, G. Gallego, and D. Scaramuzza, “EMVS: Event-based multi-view
    stereo,” 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] H. Kim, S. Leutenegger, and A. J. Davison, “Real-time 3D reconstruction
    and 6-DoF tracking with an event camera,” in *ECCV*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] K. Chaney, A. Zihao Zhu, and K. Daniilidis, “Learning event-based height
    from plane and parallax,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Y. Xue, H. Li, S. Leutenegger, and J. Stückler, “Event-based Non-Rigid
    Reconstruction from Contours,” *reconstruction*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] M. Muglikar, G. Gallego, and D. Scaramuzza, “ESL: Event-based Structured
    Light,” in *3DV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Y. Wang, R. Idoughi, and W. Heidrich, “Stereo event-based particle tracking
    velocimetry for 3d fluid flow reconstruction,” in *ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] I. Ihrke and M. Magnor, “Image-Based Tomographic Reconstruction of Flames,”
    *ACM*, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] J. Wang, S. Tan, X. Zhen, S. Xu, F. Zheng, Z. He, and L. Shao, “Deep
    3D human pose estimation: A review,” *Comput Vis Image Underst.*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Y. Desmarais, D. Mottet, P. Slangen, and P. Montesinos, “A review of
    3D human pose estimation algorithms for markerless motion capture,” *CVIU*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] M. Kocabas, N. Athanasiou, and M. J. Black, “Vibe: Video inference for
    human body pose and shape estimation,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] L. Xu, W. Xu, V. Golyanik, M. Habermann, L. Fang, and C. Theobalt, “Eventcap:
    Monocular 3d capture of high-speed human motions using an event camera,” in *CVPR*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] E. Calabrese, G. Taverni, C. Awai Easthope, S. Skriabine, F. Corradi,
    L. Longinotti, K. Eng, and T. Delbruck, “DHP19: Dynamic Vision Sensor 3D Human
    Pose Dataset,” in *CVPRW*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] S. Zou, C. Guo, X. Zuo, S. Wang, P. Wang, X. Hu, S. Chen, M. Gong, and
    L. Cheng, “EventHPE: Event-based 3D Human Pose and Shape Estimation,” in *ICCV*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] J. Zhang, B. Dong, H. Zhang, J. Ding, F. Heide, B. Yin, and X. Yang,
    “Spiking Transformers for Event-based Single Object Tracking,” in *CVPR*.   ,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] W. Weng, Y. Zhang, and Z. Xiong, “Event-based Video Reconstruction Using
    Transformer,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] X. Liu, J. Li, X. Fan, and Y. Tian, “Event-based Monocular Dense Depth
    Estimation with Recurrent Transformers,” *CoRR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,
    Y. Zhu, R. Pang, V. Vasudevan *et al.*, “Searching for mobilenetv3,” in *CVPR*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely efficient
    convolutional neural network for mobile devices,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufflenet v2: Practical guidelines
    for efficient cnn architecture design,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] R. Krishnamoorthi, “Quantizing deep convolutional networks for efficient
    inference: A whitepaper,” *arXiv*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and A. Maida,
    “Deep learning in spiking neural networks,” *NN*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] J. Zhang, X. Yang, Y. Fu, X. Wei, B. Yin, and B. Dong, “Object tracking
    by jointly exploiting frame and event domain,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] R. Battiti, “First-and second-order methods for learning: between steepest
    descent and Newton’s method,” *Neural Comput.*, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*.   MIT Press,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] A. Niwa, F. Mochizuki, R. Berner, T. Maruyarma, T. Terano, K. Takamiya,
    Y. Kimura, K. Mizoguchi, T. Miyazaki, S. Kaizu *et al.*, “A 2.97 $\mu$m-pitch
    event-based vision sensor with shared pixel front-end circuitry and low-noise
    intensity readout mode,” in *2023 IEEE International Solid-State Circuits Conference
    (ISSCC)*.   IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] M. Guo, S. Chen, Z. Gao, W. Yang, P. Bartkovjak, Q. Qin, X. Hu, D. Zhou,
    M. Uchiyama, S. Fukuoka *et al.*, “A 3-Wafer-Stacked hybrid 15MPixel CIS+ 1 MPixel
    EVS with 4.6 GEvent/s readout, In-Pixel TDC and On-Chip ISP and ESP function,”
    in *2023 IEEE International Solid-State Circuits Conference (ISSCC)*.   IEEE,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] K. Kodama, Y. Sato, Y. Yorikado, R. Berner, K. Mizoguchi, T. Miyazaki,
    M. Tsukamoto, Y. Matoba, H. Shinozaki, A. Niwa *et al.*, “1.22 $\mu$m 35.6 Mpixel
    RGB hybrid event-based vision sensor with 4.88 $\mu$m-pitch event pixels and up
    to 10K event frame rate by adaptive control on event sparsity,” in *2023 IEEE
    International Solid-State Circuits Conference (ISSCC)*.   IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    *Communications of the ACM*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] V. Rudnev, M. Elgharib, C. Theobalt, and V. Golyanik, “EventNeRF: Neural
    Radiance Fields from a Single Colour Event Camera,” *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] S. Klenk, L. Koestler, D. Scaramuzza, and D. Cremers, “E-NeRF: Neural
    Radiance Fields from a Moving Event Camera,” *RAL*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] I. Hwang, J. Kim, and Y. M. Kim, “Ev-NeRF: Event Based Neural Radiance
    Field,” *WACV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] J. Zhao, S. Zhang, and T. Huang, “Transformer-Based Domain Adaptation
    for Event Data Classification,” in *ICASSP*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Y. Yang, L. Pan, and L. Liu, “Event Camera Data Pre-training.” [Online].
    Available: [http://arxiv.org/abs/2301.01928.](http://arxiv.org/abs/2301.01928.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/83ab03d1b95826d31d0140ecafc11779.png) | Xu Zheng
    is a Ph.D. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include event-based vision, 3D vision, etc.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2a6e293cb71c8f46fda139b658a01ca0.png) | Yexin
    Liu is a Mphil. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include infrared- and event-based vision, and
    unsupervised domain adaptation. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/97cba95341b311a800aad8e65cdf35dd.png) | Yunfan
    Lu is a Ph.D. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include low-level vision (event camera, deblurring,
    SR), pattern recognition (image classification, object detection), and DL (transfer
    learning, unsupervised learning). |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/7b5e9f866be8d4a883613cc4adb7fbd7.png) | Tongyan
    Hua is a research assistant in the Visual Learning and Intelligent Systems Lab,
    Artificial Intelligence Thrust, The Hong Kong University of Science and Technology,
    Guangzhou (HKUST-GZ). Her research interests include robotics vision, Simultaneous
    localization and mapping (SLAM), Deep Learning, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/8f87c6ef36e63b2ec2311c28282503ab.png) | Tianbo
    Pan is a Mphil student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include event-based vision, Simultaneous localization
    and mapping (SLAM), 3D vision, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/d7595ede50696b0b7f080915ab66b9fa.png) | Weiming
    Zhang is a research assistant in the Visual Learning and Intelligent Systems Lab,
    Artificial Intelligence Thrust, The Hong Kong University of Science and Technology,
    Guangzhou (HKUST-GZ). His research interests include event-based vision, Deep
    Learning, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/7d35cabf58fc086d1889176113ac995b.png) | Dacheng
    Tao (Fellow IEEE) the Inaugural Director of the JD Explore Academy and a Senior
    Vice President of JD.com. He is also an advisor and chief scientist of the digital
    sciences initiative at the University of Sydney. He mainly applies statistics
    and mathematics to AI and data science, and his research is detailed in one monograph
    and over 200 publications in prestigious journals and proceedings at leading conferences.
    He received the 2015 Australian Scopus-Eureka Prize, the 2018 IEEE ICDM Research
    Contributions Award, and the 2021 IEEE Computer Society McCluskey Technical Achievement
    Award. He is a fellow of the Australian Academy of Science, AAAS, ACM and IEEE.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2057381ce4adb7b6d18493949a93f485.png) | Lin Wang
    (IEEE Member) is an assistant professor in the AI Thrust, HKUST-GZ, HKUST FYTRI,
    and an affiliate assistant professor in the Dept. of CSE, HKUST. He did his Postdoc
    at the Korea Advanced Institute of Science and Technology (KAIST). He got his
    Ph.D. (with honors) and M.S. from KAIST, Korea. He had rich cross-disciplinary
    research experience, covering mechanical, industrial, and computer engineering.
    His research interests lie in computer and robotic vision, machine learning, intelligent
    systems (XR, vision for HCI), etc. |'
  prefs: []
  type: TYPE_TB
