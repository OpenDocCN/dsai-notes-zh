- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:41:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:41:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2302.08890] Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2302.08890] 基于事件的视觉深度学习：综合调查与基准测试'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.08890](https://ar5iv.labs.arxiv.org/html/2302.08890)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2302.08890](https://ar5iv.labs.arxiv.org/html/2302.08890)
- en: 'Deep Learning for Event-based Vision:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于事件的视觉深度学习：
- en: A Comprehensive Survey and Benchmarks
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 综合调查与基准测试
- en: Xu Zheng^∗, Yexin Liu^∗, Yunfan Lu, Tongyan Hua, Tianbo Pan, Weiming Zhang,
    Dacheng Tao, Lin Wang^†
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 徐征^∗, 刘叶欣^∗, 吕云凡, 华同言, 潘天博, 张伟明, 陶大程, 王林^†
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Event cameras are bio-inspired sensors that capture the per-pixel intensity
    changes asynchronously and produce event streams encoding the time, pixel position,
    and polarity (sign) of the intensity changes. Event cameras possess a myriad of
    advantages over canonical frame-based cameras, such as high temporal resolution,
    high dynamic range, low latency, etc. Being capable of capturing information in
    challenging visual conditions, event cameras have the potential to overcome the
    limitations of frame-based cameras in the computer vision and robotics community.
    In very recent years, deep learning (DL) has been brought to this emerging field
    and inspired active research endeavors in mining its potential. However, there
    is still a lack of taxonomies in DL techniques for event-based vision. We first
    scrutinize the typical event representations with quality enhancement methods
    as they play a pivotal role as inputs to the DL models. We then provide a comprehensive
    taxonomy for existing DL-based methods by structurally grouping them into two
    major categories: 1) image reconstruction and restoration; 2) event-based scene
    understanding and 3D vision. Importantly, we conduct benchmark experiments for
    the existing methods in some representative research directions (e.g., object
    recognition) to identify some critical insights and problems. Finally, we make
    important discussions regarding the challenges and provide new perspectives for
    inspiring more research studies.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 事件摄像头是生物启发的传感器，它们异步捕捉每个像素的强度变化，并生成事件流，编码强度变化的时间、像素位置和极性（符号）。事件摄像头相较于传统的帧摄像头具有众多优势，如高时间分辨率、高动态范围、低延迟等。由于能够在复杂的视觉条件下捕捉信息，事件摄像头有可能克服计算机视觉和机器人学领域中帧摄像头的局限性。近年来，深度学习（DL）被引入到这一新兴领域，激发了对其潜力的积极研究。然而，针对基于事件的视觉的DL技术仍缺乏分类法。我们首先审视了典型事件表示及其质量增强方法，因为它们在DL模型中作为输入发挥了关键作用。接着，我们为现有的DL方法提供了一个全面的分类法，将其结构性地分为两个主要类别：1）图像重建与恢复；2）基于事件的场景理解和3D视觉。重要的是，我们对一些代表性研究方向（如物体识别）的现有方法进行了基准实验，以识别一些关键见解和问题。最后，我们对挑战进行了重要讨论，并提供了新的视角，以激发更多的研究工作。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Event Cameras, Deep Learning, Computer Vision and Robotics, Taxonomy, Survey.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 事件摄像头，深度学习，计算机视觉与机器人学，分类，调查。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The breakthrough in neuromorphic engineering has recently extended the realms
    of sensory perception and initiated a novel paradigm that mimics biological vision.
    The bio-inspired sensors are also called event cameras [[1](#bib.bib1)]. The difference
    is that each pixel in the camera operates independently, triggering a response
    (i.e., event) only when there’s a brightness change, probably caused by motion
    or other visual changes [[2](#bib.bib2)]. In practice, the event streams captured
    by an event camera are sparse and are more concentrated along object boundaries,
    but also present in areas of continuous texture gradients, enabling the camera
    with a merit of low latency. By contrast, canonical frame-based cameras record
    a complete image or video of the scene with a fixed frame rate. Event cameras
    offer some other benefits, such as high temporal resolution and high dynamic range [[3](#bib.bib3)].
    This means that an event camera can capture high-quality data either in extreme
    lighting or high-speed motion conditions [[4](#bib.bib4)]. Therefore, event cameras
    have the potential to overcome the limitations of frame-based cameras in the computer
    vision and robotic fields and have helped the community in solving various downstream
    tasks, e.g., corner tracking[[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)], Simultaneous Localization And Mapping (SLAM)[[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)],
    image and video restoration[[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19)], object detection and segmentation[[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [4](#bib.bib4)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态工程的突破最近拓展了感知领域，并引入了一种模仿生物视觉的新范式。生物启发的传感器也被称为事件摄像头[[1](#bib.bib1)]。不同之处在于，摄像头中的每个像素独立操作，仅在亮度发生变化（即事件）时触发响应，这种变化可能是由运动或其他视觉变化引起的[[2](#bib.bib2)]。实际上，事件摄像头捕捉到的事件流是稀疏的，主要集中在物体边界上，但也存在于连续纹理梯度区域，使得摄像头具有低延迟的优点。相比之下，经典的基于帧的摄像头以固定帧率记录完整的图像或视频。事件摄像头还具有其他一些优点，如高时间分辨率和高动态范围[[3](#bib.bib3)]。这意味着事件摄像头可以在极端光照或高速运动条件下捕捉高质量数据[[4](#bib.bib4)]。因此，事件摄像头有潜力克服计算机视觉和机器人领域中基于帧的摄像头的局限性，并帮助社区解决各种下游任务，例如角点跟踪[[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]，同时定位与地图构建（SLAM）[[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)]，图像和视频修复[[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]，以及物体检测和分割[[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [4](#bib.bib4)]。
- en: '![Refer to caption](img/a95743e77b5213160eaf7061d57d402e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a95743e77b5213160eaf7061d57d402e.png)'
- en: 'Figure 1: The structural and hierarchical taxonomy of event-based vision with
    deep learning.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于事件的视觉与深度学习的结构和层次分类。
- en: 'Motivation: The first application of an event camera is in a hardware-based
    Deep Neural Network (DNN) system called CAVIAR [[26](#bib.bib26)], predating the
    first software-based DNN system from the computer vision community by approximately
    a decade. This early development also includes significant contributions from
    solid-state circuit papers that detailed the design and initial applications of
    camera chips [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)]. Recently,
    deep learning (DL) has received great attention in this emerging area with amounts
    of techniques developed based on various purposes. For example, DL-based feature
    trackers [[30](#bib.bib30)] achieve better accuracy than the non-DL-based methods
    (See Tab. [IX](#S4.T9 "TABLE IX ‣ 4.1.2 Feature Tracking ‣ 4.1 Scene Understanding
    ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks")). We explore some fundamental questions
    that have been driving this research area when examining the representative methods.
    For instance, how to feed event data to DNNs as they are designed for image- or
    tensor-like inputs? What makes DL more advantageous than optimization-based methods
    for learning events? Do we really need very deep models to learn visual features
    from events? How can we balance the distinct property of event cameras, e.g.,
    low latency, when applying DL models? Do we really need convolution operations
    to filter events as done for image data?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '动机：事件相机的首个应用是在一个名为 CAVIAR 的硬件基础深度神经网络（DNN）系统中 [[26](#bib.bib26)]，这一系统的出现大约比计算机视觉社区的第一个软件基础
    DNN 系统早十年。这个早期发展还包括来自固态电路论文的重要贡献，这些论文详细描述了相机芯片的设计和初步应用 [[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29)]。最近，深度学习（DL）在这一新兴领域受到了广泛关注，各种基于不同目的开发的技术也随之出现。例如，基于 DL 的特征跟踪器
    [[30](#bib.bib30)] 相比于非 DL 方法具有更好的准确性（参见表 [IX](#S4.T9 "TABLE IX ‣ 4.1.2 Feature
    Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D Vision ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")）。我们在考察代表性方法时，探讨了一些驱动这一研究领域的基本问题。例如，如何将事件数据输入到
    DNN 中，因为它们是为图像或张量类输入设计的？为什么 DL 相比于基于优化的方法在学习事件时更具优势？我们是否真的需要非常深的模型来从事件中学习视觉特征？在应用
    DL 模型时，如何平衡事件相机的不同特性，例如低延迟？我们是否真的需要卷积操作来过滤事件，就像处理图像数据时那样？'
- en: With these questions being discussed, this paper provides an in-depth survey
    of current trends in DL for event-based vision. We focus on analyzing, categorizing,
    and benchmarking the DL-based methods with a highlight on recent progress. In
    particular, we review and scrutinize this promising area by systematically discussing
    the technical details, challenges, and potential directions. Previously, Gallego
    et al. [[1](#bib.bib1)] provided the first overview of the event-based vision
    with a particular focus on the principles and conventional algorithms. However,
    DL has invigorated almost every field of event-based vision recently, and remarkable
    advancements in methodologies and techniques have been achieved. Therefore, a
    more up-to-date yet insightful survey is urgently needed for capturing the research
    trends while clarifying the challenges and potential directions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些问题，本论文对基于事件的视觉领域的当前趋势进行了深入调查。我们专注于分析、分类和基准测试基于深度学习（DL）的方法，并重点关注最近的进展。特别地，我们通过系统地讨论技术细节、挑战和潜在方向来审视和深入剖析这一有前景的领域。之前，Gallego
    等人 [[1](#bib.bib1)] 提供了基于事件的视觉的首次概述，特别关注原理和传统算法。然而，近年来深度学习几乎重振了每个基于事件的视觉领域，并取得了显著的进展。因此，迫切需要一份更为最新且富有洞见的调查报告，以捕捉研究趋势，同时澄清挑战和潜在方向。
- en: 'We survey the DL-based methods by focusing on three important aspects: 1) How
    to learn events with DNNs—event representation and quality enhancement (Sec. [2](#S2
    "2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks")); 2) Current research highlights by typically analyzing
    two hot fields, image restoration and enhancement (Sec.[3](#S3 "3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")), scene understanding and 3D vision (Sec. [4](#S4 "4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks")); 3) Potential future directions, such as event-based neural radiance
    for 3D reconstruction, cross-modal learning, and event-based model pretraining.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过关注三个重要方面来调查基于深度学习的方法：1）如何利用深度神经网络学习事件——事件表示和质量提升（Sec. [2](#S2 "2 Event
    Processing for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")）；2）当前研究亮点，通常分析两个热门领域，图像修复和增强（Sec.[3](#S3 "3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")），场景理解和3D视觉（Sec. [4](#S4 "4 Scene Understanding and 3D Vision
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")）；3）潜在的未来方向，例如用于3D重建的事件基础神经辐射、跨模态学习和基于事件的模型预训练。'
- en: 'Contributions: In summary, the main contributions of this paper are five-fold:
    (I) We provide a comprehensive overview of the existing event representations
    as well as the quality enhancement methods for events. (II) We provide a summary
    of how existing DL-based approaches for event-based vision address challenges
    and offer insights for the computer vision community, including image restoration
    and enhancement and high-level scene understanding tasks. (III) We discuss some
    open problems and challenges on DL with events and identify future research directions,
    providing guidance for future developments in this field. (IV) We ask and discuss
    some broadly focused questions as well as some potential problems to answer the
    concern and dive deeply into the event-based vision. Furthermore, we create an
    open-source repository that provides a taxonomy of all mentioned papers and code
    links. Our open-source repository will be updated regularly with the latest research
    progress, and we hope this work can bring sparks to the research of this field.
    The repository link is [https://github.com/vlislab2022/Event-Deep-Learning-Survey](https://github.com/vlislab2022/Event-Deep-Learning-Survey).
    Meanwhile, we benchmark and highlight some representative event-based and event-guided
    vision tasks, e.g., in Tab. [VI](#S3.T6 "TABLE VI ‣ 3.4 Event-guided Image/Video
    Deblurring ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks") and [X](#S4.T10 "TABLE X ‣ 4.1.3
    Object Detection and Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks"), to identify the critical insights and problems for future studies.
    Due to the lack of space, some experimental results can be found in the supplementary
    material.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '贡献：总而言之，本文的主要贡献有五点：（I）我们提供了现有事件表示以及事件质量提升方法的全面概述。（II）我们总结了现有基于深度学习的事件视觉方法如何应对挑战，并为计算机视觉社区提供了见解，包括图像修复和增强以及高级场景理解任务。（III）我们讨论了一些基于事件的深度学习的开放问题和挑战，并确定了未来的研究方向，为该领域的未来发展提供指导。（IV）我们提出并讨论了一些广泛关注的问题以及一些潜在问题，以回应关切并深入探讨基于事件的视觉。此外，我们创建了一个开源仓库，提供了所有提及论文的分类和代码链接。我们的开源仓库将定期更新最新的研究进展，希望这项工作能为该领域的研究带来灵感。仓库链接是
    [https://github.com/vlislab2022/Event-Deep-Learning-Survey](https://github.com/vlislab2022/Event-Deep-Learning-Survey)。同时，我们对一些代表性的基于事件和事件引导的视觉任务进行了基准测试和突出显示，例如在表格
    [VI](#S3.T6 "TABLE VI ‣ 3.4 Event-guided Image/Video Deblurring ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks") 和 [X](#S4.T10 "TABLE X ‣ 4.1.3 Object Detection and Tracking
    ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks") 中，以识别未来研究的关键见解和问题。由于篇幅限制，一些实验结果可以在补充材料中找到。'
- en: 'Outlines: In the following sections, we discuss and analyze the recent advances
    in DL methods for event-based vision. The structural and hierarchical taxonomy
    of this paper is depicted in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"). In Sec. [2](#S2
    "2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks"), we systematically summarize existing event representation
    methods for DL and compare the advantages and disadvantages of different representations
    on different tasks. In this section, the quality enhancement methods are also
    summarized. In Sec. [3](#S3 "3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks"), we review and
    analyze the image restoration and enhancement methods, including image and video
    reconstruction, super-resolution, video frame interpolation, image and video deblurring,
    and high dynamic range (HDR) image and video reconstruction. In Sec. [4](#S4 "4
    Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks"), we summarise existing event-based DL approaches in scene
    understanding tasks, including object classification, object detection and tracking,
    semantic segmentation, feature tracking, optical flow, and depth estimation. In
    this section, we mainly discuss the deep learning pipelines for these computer
    vision tasks with event cameras. In Sec. [5.1](#S5.SS1 "5.1 Discussions ‣ 5 Research
    Trend and Discussions ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks"), we ask and discuss some widely focused questions as well
    as some potential problems to answer the existing concern and dive deeply into
    the event-based vision. And we discuss some open problems and new applications
    on DL with events and identify future research directions in Sec. [5.2](#S5.SS2
    "5.2 New Directions ‣ 5 Research Trend and Discussions ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"), providing guidance for future
    developments in this field.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大纲：在接下来的章节中，我们讨论和分析了事件驱动视觉中深度学习（DL）方法的最新进展。本文的结构和层次分类如图[1](#S1.F1 "图 1 ‣ 1 引言
    ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")所示。在第[2](#S2 "2 DNNs 的事件处理 ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")节中，我们系统地总结了现有的事件表示方法，并比较了不同任务上各种表示的优缺点。本节还总结了质量增强方法。在第[3](#S3
    "3 图像恢复与增强 ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")节中，我们回顾并分析了图像恢复和增强方法，包括图像和视频重建、超分辨率、视频帧插值、图像和视频去模糊以及高动态范围（HDR）图像和视频重建。在第[4](#S4
    "4 场景理解与 3D 视觉 ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")节中，我们总结了现有的基于事件的深度学习方法在场景理解任务中的应用，包括物体分类、物体检测与跟踪、语义分割、特征跟踪、光流估计和深度估计。本节主要讨论了使用事件相机进行这些计算机视觉任务的深度学习流程。在第[5.1](#S5.SS1
    "5.1 讨论 ‣ 5 研究趋势与讨论 ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")节中，我们提出并讨论了一些广泛关注的问题以及一些潜在的问题，以回答现有的关注点，并深入探讨基于事件的视觉技术。我们还在第[5.2](#S5.SS2
    "5.2 新方向 ‣ 5 研究趋势与讨论 ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")节中讨论了一些开放问题和新的应用方向，确定了未来的研究方向，为该领域的未来发展提供指导。
- en: 2 Event Processing for DNNs
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 DNNs 的事件处理
- en: 'Event cameras generate events when individual pixels detect the relative logarithm
    intensity change. Consequently, sparse and asynchronous event streams are generated.
    This inherent sparsity offers immediate advantages, such as low latency and low
    computational requirements for postprocessing systems [[31](#bib.bib31)]. Event
    cameras pose a distinctive shift in the imaging paradigm regarding how visual
    information is captured, making it impossible to directly apply the DNN models
    taking the image- or tensor-like inputs. Therefore, we first analyze the event
    representations, which are used as inputs to DNNs (Sec. [2.1](#S2.SS1 "2.1 Event
    Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks")). Moreover, as event data are often hampered
    by noises (especially in low-light conditions) and limited spatial resolution
    (in particular for DAVIS cameras), we analyze the DL methods that super-resolve
    and denoise the event streams for improving the learning performance (Sec. [2.2](#S2.SS2
    "2.2 Quality Enhancement for Events ‣ 2 Event Processing for DNNs ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '事件相机在单个像素检测到相对对数强度变化时生成事件。因此，会生成稀疏和异步的事件流。这种固有的稀疏性带来了即时的优势，例如低延迟和对后处理系统的低计算要求
    [[31](#bib.bib31)]。事件相机在成像范式中引入了显著的变化，即视觉信息的捕获方式，使得直接应用处理图像或张量输入的DNN模型变得不可能。因此，我们首先分析用于DNN输入的事件表示（第
    [2.1](#S2.SS1 "2.1 Event Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks") 节）。此外，由于事件数据通常受到噪声（特别是在低光照条件下）和有限空间分辨率（特别是对DAVIS相机）的影响，我们分析了可以超分辨率和去噪事件流的深度学习方法，以改善学习性能（第
    [2.2](#S2.SS2 "2.2 Quality Enhancement for Events ‣ 2 Event Processing for DNNs
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    节）。'
- en: 2.1 Event Representation
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 事件表示
- en: 'We first review how an event camera responds asynchronously to each independent
    pixel and generates a stream of events. An event is interpreted as a tuple $(\textbf{u},t,p)$,
    which is triggered whenever a change in the logarithmic intensity $L$ surpasses
    a constant value (threshold) $C$, formulated as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先回顾了事件相机如何异步响应每个独立像素并生成事件流。事件被解释为一个元组 $(\textbf{u},t,p)$，当对数强度 $L$ 的变化超过常数值（阈值）
    $C$ 时触发，如下所示：
- en: '|  | <math   alttext="p=\left\{\begin{aligned} +1,&amp;L(\textbf{u},t)-L(\textbf{u},t-\Delta
    t)\geq C\\ -1,&amp;L(\textbf{u},t)-L(\textbf{u},t-\Delta t)\leq-C\\'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="p=\left\{\begin{aligned} +1,&amp;L(\textbf{u},t)-L(\textbf{u},t-\Delta
    t)\geq C\\ -1,&amp;L(\textbf{u},t)-L(\textbf{u},t-\Delta t)\leq-C\\'
- en: 0,&amp;other\\
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 0,&amp;other\\
- en: \end{aligned}\right." display="block"><semantics ><mrow ><mi >p</mi><mo >=</mo><mrow
    ><mo >{</mo><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mo >+</mo><mn >1</mn></mrow><mo >,</mo></mrow></mtd><mtd  columnalign="left"
    ><mrow ><mrow ><mrow ><mi >L</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mi >t</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >L</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mrow ><mi
    >t</mi><mo >−</mo><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >t</mi></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >≥</mo><mi >C</mi></mrow></mtd></mtr><mtr ><mtd  columnalign="right" ><mrow ><mrow
    ><mo >−</mo><mn >1</mn></mrow><mo >,</mo></mrow></mtd><mtd columnalign="left"
    ><mrow ><mrow ><mrow ><mi >L</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mi >t</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >L</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mrow ><mi
    >t</mi><mo >−</mo><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >t</mi></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >≤</mo><mrow ><mo >−</mo><mi >C</mi></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"
    ><mrow ><mn >0</mn><mo >,</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mi
    >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >h</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >r</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >p=\left\{\begin{aligned} +1,&L(\textbf{u},t)-L(\textbf{u},t-\Delta
    t)\geq C\\ -1,&L(\textbf{u},t)-L(\textbf{u},t-\Delta t)\leq-C\\ 0,&other\\ \end{aligned}\right.</annotation></semantics></math>
    |  | (1) |
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'where $\textbf{u}=(x,y)$ is the pixel location, $t$ is the timestamp and $p\in\{-1,1\}$
    is the polarity, indicating the sign of brightness changes (1 and -1 represent
    positive and negative events, respectively), and $p=0$ means that there are no
    events. $\Delta t$ is a time interval since the last event at pixel $\textbf{u}=(x,y)$.
    A number (or stream) of events are triggered which can be denoted as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{E}=\{e_{i}\}_{i=1}^{N}=\{\textbf{u}_{i},t_{i},p_{i}\},i\in N,$
    |  | (2) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: 'For more details of the event generation model, we refer readers to [[1](#bib.bib1)].
    In a nutshell, this particular type of data makes it difficult to apply the DNN
    models predominantly designed for frame-based cameras. Therefore, it is pivotal
    to exploit the effective alternative representations of event data for mining
    their visual information and power [[32](#bib.bib32)]. In the following, we review
    the representative event representation methods, which can be divided into six
    categories: image-based, surface-based, learning-based, voxel-based, graph-based,
    and spike-based representations, as shown in Tab. [I](#S2.T1 "TABLE I ‣ 2.1 Event
    Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Representative event representations, served as inputs to event-based
    DNN models. (SP: Steering Prediction; OF: Optical Flow Estimation; Cls: Classification;
    CD: Corner Detection; GR: Gesture recognition; Recon: Reconstruction; DE: Depth
    Estimation; N/A: Not Available.)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Event Representation | Tasks | Dimensions | Description | Characteristics
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| Image | Maqueda et al.[[33](#bib.bib33)] | SP | (2,H,W) | Channels for positive
    and negative events | Characteristics |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| EV-FlowNet [[34](#bib.bib34)] | OF | (4,H,W) | Image of event counts | Discards
    temporal information |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| Event Image [[35](#bib.bib35)] | Cls | (4,H,W) | Image of event counts |
    Four channel event images |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| AMAE[[36](#bib.bib36)] | Cls | (2,H,W) | Two-channel image used timestamps
    | Indistinguishable semantic information |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| Bai et al.[[37](#bib.bib37)] | Cls | (3,H,W) | Three-channel event representation
    | Event count channel based on number |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '|  | MVF-Net [[38](#bib.bib38)] | Cls | N/A | Multi-view 2D maps | spatial-temporal
    complements |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| Learning | EST [[39](#bib.bib39)] | Cls&OF | (2,B,H,W) | 4 grid of convolutions
    | Temporally quantized information into B bins |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| Matrix-LSTM [[40](#bib.bib40)] | Cls&OF | (B,H,W) | End-to-end event surfaces
    | Temporally quantized information into B bins |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| Surface | Timestamps Image [[41](#bib.bib41)] | Cls | N/A | Spatial and temporal
    downsampling | N/A |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| SAE [[42](#bib.bib42)] | Cls | (2,H,W) | Image of most recent timestamps
    | Discards all prior timestamps |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| Time Surface [[43](#bib.bib43)] | Cls | (2,H,W) | Exponential of newest timestamps
    | Discards all prior timestamps |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| Sorted Time Surface [[7](#bib.bib7)] | Cls | (2,H,W) | Sorted newest timestamps
    | Retains temporal relationship |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| Event Histogram [[33](#bib.bib33)] | Cls | (2,H,W) | Image of event counts
    | Discard temporal information |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| HATS [[44](#bib.bib44)] | Cls | (2,H,W) | Aggregated newest timestamps |
    Discards temporal information |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| IETS [[45](#bib.bib45)] | Cls | (3,H,W) | Image of filtered timestamps &
    event count | Discard temporal information |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| SITS [[46](#bib.bib46)] | CD | (2,H,W) | Speed invariant time surface | Discards
    absolute timestamps |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| Chain SAE [[32](#bib.bib32)] | Cls | (2,H,W) | Chain Updating Strategy of
    time surface | Retains temporal relationship |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| DS [[47](#bib.bib47)] | OP | (H,W) | Image of spatial distance to active
    pixel | Discards temporal/polarity information |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| DiST [[48](#bib.bib48)] | Cls | (2,H,W) | Sorted discounted timestamps |
    Retains temporal relationship |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| TORE [[3](#bib.bib3)] | Cls | (2,K,H,W) | 4D grid of last K timestamps |
    Retains all information for last K events |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| Voxel | Zhu et al.[[49](#bib.bib49)] | OF | (B,H,W) | Generated by discretizing
    the time domain | Temporally quantized information into B bins |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| Rebecq et al.[[50](#bib.bib50)] | Recon | (B,H,W) | Generated by discretizing
    the time domain | Temporally quantized information into B bins |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| Ye et al.[[51](#bib.bib51)] | DE&OF | (B,H,W) | Generated by discretizing
    the time domain | Temporally quantized information into B bins |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| TORE [[3](#bib.bib3)] | Recon, etc | (2,K,H,W) | 4D grid of last K timestamps
    | Retains all information for last K events |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| Graph | RG-CNN [[52](#bib.bib52)] | Cls | N/A | End-to-end learning with
    graph convolution neural networks | N/A |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| EV-VGCNN [[53](#bib.bib53)] | Cls | (H,W,A) | Use voxel-wise vertices rather
    than point-wise inputs | Normalize the time dimension with a compensation coefficient
    A |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| Spike | Lee et al.[[54](#bib.bib54)] | Cls | N/A | A novel supervised learning
    method for SNNs | N/A |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: '| Tactilesgnet [[50](#bib.bib50)] | Cls | N/A | Design a spiking graph convolutional
    network | N/A |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '| Botzheim et al.[[55](#bib.bib55)] | GR | N/A | Use spiking neural network
    and classification learning | N/A |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| Amiret al.[[56](#bib.bib56)] | GR | N/A | Fully Event-Based Gesture Recognition
    System | N/A |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: 2.1.1 Image-based Representation
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The straightforward solution to adopt events to the existing DL methods is to
    stack (or convert) events to synchronous 2D image representations (similar to
    frame-based cameras) as the inputs to DNNs. For example, Moeys et al. [[57](#bib.bib57)]
    proposed the first CNN driven by DVS frames to address the blurring issue in a
    predator-prey robot scenario. This study also marks the initial utilization of
    event count DVS images to guide a DNN using DVS data. The channels of image-based
    representation are often set to preserve polarities, timestamps, and event counts [[33](#bib.bib33),
    [34](#bib.bib34), [39](#bib.bib39), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38),
    [57](#bib.bib57)]. Based on how images are formed, we divide the prevailing methods
    into four types.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Stack based on polarity: Maqueda et al.[[33](#bib.bib33)] set up two separate
    channels to evaluate the histograms for positive and negative events to obtain
    two-channel event images, which are finally merged together into synchronous event
    frames.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Stack based on timestamps: To consider the importance of event counts and timestamps
    for holistic information, [[36](#bib.bib36), [37](#bib.bib37), [17](#bib.bib17)]
    take the timestamps of events into consideration.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时间戳的堆叠：为了考虑事件计数和时间戳对整体信息的重要性，[[36](#bib.bib36), [37](#bib.bib37), [17](#bib.bib17)]
    考虑了事件的时间戳。
- en: 'Stack based on the number of events: Due to the uneven triggers of events within
    fixed time intervals, another stacking strategy is proposed to sample and stack
    events in a fixed constant number [[58](#bib.bib58), [59](#bib.bib59), [17](#bib.bib17)].'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于事件数量的堆叠：由于在固定时间间隔内事件触发的不均匀性，提出了一种新的堆叠策略来在固定常数数量的事件中进行采样和堆叠[[58](#bib.bib58),
    [59](#bib.bib59), [17](#bib.bib17)]。
- en: 'Stack based on timestamps and polarity: In Ev-gait  [[35](#bib.bib35)], event
    streams are converted to frame-like representation with four channels, containing
    the positive or negative polarities in two channels and the temporal characteristics
    in another two channels. Also, some research has focused on exposure time control
    in event-based systems. Liu et al. [[60](#bib.bib60), [61](#bib.bib61)] have explored
    dynamic control of exposure time and inter-slice time interval to optimize the
    quality of slice features. This adaptive control helps to ensure the robustness
    of the model in dynamic scenes with varying motion speeds and scene structures.
    Detailed mathematical formulations can be found in Sec.1.1 of the supplementary
    material.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时间戳和极性的堆叠：在 Ev-gait [[35](#bib.bib35)]中，事件流被转换为类似帧的表示，包含四个通道，其中两个通道包含正或负极性，另外两个通道包含时间特性。此外，一些研究集中于事件驱动系统中的曝光时间控制。刘等人[[60](#bib.bib60),
    [61](#bib.bib61)]探讨了曝光时间和切片间时间间隔的动态控制，以优化切片特征的质量。这种自适应控制有助于确保模型在动态场景中具有鲁棒性，适应不同的运动速度和场景结构。详细的数学公式可见于补充材料的第1.1节。
- en: 2.1.2 Surface-based Representation
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 基于表面的表示
- en: 'The first surface-based representation, i.e., Surface of Active Events (SAE) [[42](#bib.bib42)],
    it maps the event streams to a time-dependent surface and tracks the activity
    around the spatial location of the latest event $e_{i}$. Different from the basic
    image-based representation which utilizes intensity images to provide context
    content, the SAE achieves this through a totally different perspective, i.e.,
    the temporal-spatial perspective. Specifically, the time surface of the $i$ th
    event $e_{i}$ can be formulated as a spatial operator acting on the neighboring
    region of $e_{i}$:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个基于表面的表示，即主动事件表面（SAE）[[42](#bib.bib42)]，它将事件流映射到时间依赖的表面，并跟踪最新事件 $e_{i}$ 的空间位置周围的活动。与利用强度图像提供上下文内容的基本图像表示不同，SAE
    从完全不同的角度，即时间-空间角度来实现这一点。具体而言，第 $i$ 个事件 $e_{i}$ 的时间表面可以被表述为作用于 $e_{i}$ 邻域的空间算子：
- en: '|  | $\tau_{i}([x_{n},y_{n}]^{T},p)=\underset{j\leq i}{max}\{t_{j}&#124;[x_{i}+x_{n},y_{i}+y_{n}],p_{j}=p\}$
    |  | (3) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tau_{i}([x_{n},y_{n}]^{T},p)=\underset{j\leq i}{max}\{t_{j}&#124;[x_{i}+x_{n},y_{i}+y_{n}],p_{j}=p\}$
    |  | (3) |'
- en: 'where $x_{n}\in\{-r,r\}$ is the horizontal coordinate of $e_{i}$, $y_{n}\in\{-r,r\}$
    is the vertical coordinate of $e_{i}$, $p_{j}\in\{-1,1\}$ is the polarity of the
    $j$ th event $e_{j}$, $t_{j}$ is the timestamp of $e_{j}$ and $r$ is the radius
    of the neighboring region used to obtain the time surface. As shown in Eq. [3](#S2.E3
    "In 2.1.2 Surface-based Representation ‣ 2.1 Event Representation ‣ 2 Event Processing
    for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    the time surface $\tau_{i}([x_{n},y_{n}]^{T},p)$ encodes the time context in the
    $(2r+1)\times(2r+1)$ neighborhood region of $e_{i}$, hence maintaining both temporal
    and spatial information for downstream tasks.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{n}\in\{-r,r\}$ 是 $e_{i}$ 的水平坐标，$y_{n}\in\{-r,r\}$ 是 $e_{i}$ 的垂直坐标，$p_{j}\in\{-1,1\}$
    是第 $j$ 个事件 $e_{j}$ 的极性，$t_{j}$ 是 $e_{j}$ 的时间戳，$r$ 是用于获取时间表面的邻域半径。如公式 [3](#S2.E3
    "在 2.1.2 基于表面表示 ‣ 2.1 事件表示 ‣ 2 事件处理用于 DNNs ‣ 深度学习用于事件驱动视觉：全面调查与基准") 所示，时间表面 $\tau_{i}([x_{n},y_{n}]^{T},p)$
    编码了 $e_{i}$ 的 $(2r+1)\times(2r+1)$ 邻域区域中的时间上下文，从而保持了下游任务所需的时间和空间信息。
- en: However, the timestamps of events monotonically increase, which causes the temporal
    values in the surface from 0 to infinity [[32](#bib.bib32)]. Therefore, appropriate
    normalization approaches are required to preserve the temporal-invariant data
    representation from raw SAE by mapping the timestamps to $[0,1]$. Basic normalization
    methods are directly applied to time surfaces [[7](#bib.bib7), [43](#bib.bib43),
    [62](#bib.bib62), [63](#bib.bib63)], such as the min-max [[7](#bib.bib7)], time
    window [[63](#bib.bib63)], etc.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，事件的时间戳单调增加，这导致表面上的时间值从 0 到无穷大 [[32](#bib.bib32)]。因此，需要适当的归一化方法通过将时间戳映射到 $[0,1]$
    来保持原始 SAE 的时间不变数据表示。基本的归一化方法直接应用于时间表面 [[7](#bib.bib7), [43](#bib.bib43), [62](#bib.bib62),
    [63](#bib.bib63)]，例如 min-max [[7](#bib.bib7)]、时间窗口 [[63](#bib.bib63)] 等。
- en: All these above normalization methods rely on empirical parameter tuning, leading
    to additional computational costs. To avoid this problem, sort normalization is
    employed by Alzugaray et al. [[7](#bib.bib7)] to sort all the timestamps within
    an SAE at each pixel. However, though this method alleviates the dependence on
    parameter tuning, the by-product of time complexity impedes the whole procedure’s
    efficiency. To build an efficient SAE and achieve robust speed-invariant characteristics,
    Manderscheid et al. [[46](#bib.bib46)] introduced a normalization scheme to obtain
    the Speed Invariant Time Surface (SITS). The SITS updates the time surface of
    each incoming event according to its neighborhood with the radius $r$. Overall,
    when large $r$ is adopted, the SITS updates the time surface when a new event
    is triggered, thus leading to inefficiency in the on-demand tasks. Lin et al. [[32](#bib.bib32)]
    suggested solving and alleviating this imbalance between normalization and the
    number of events by using a chain update strategy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些归一化方法依赖于经验参数调整，从而导致额外的计算成本。为避免这个问题，Alzugaray 等人 [[7](#bib.bib7)] 采用了排序归一化方法，在每个像素处对
    SAE 中的所有时间戳进行排序。然而，尽管这种方法缓解了对参数调整的依赖，但时间复杂度的副作用妨碍了整个过程的效率。为了构建高效的 SAE 并实现稳健的速度不变特性，Manderscheid
    等人 [[46](#bib.bib46)] 引入了一种归一化方案，以获得速度不变时间表面 (SITS)。SITS 根据其半径 $r$ 的邻域更新每个到达事件的时间表面。总体而言，当采用较大的
    $r$ 时，SITS 在新事件触发时更新时间表面，从而导致按需任务的低效。Lin 等人 [[32](#bib.bib32)] 建议通过使用链式更新策略来解决和缓解归一化与事件数量之间的不平衡。
- en: 2.1.3 Voxel-based Representation
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 基于体素的表示
- en: The voxel-based representations map the raw events into the nearest temporal
    grid within temporal bins. The first spatial-temporal voxel grid is proposed in
     [[49](#bib.bib49)], inserting events into volumes using a linearly weighted accumulation
    to improve the resolution along the temporal domain. This spatial-temporal voxel
    grid is also used in some following works [[64](#bib.bib64), [51](#bib.bib51)].
    More recently, a time-ordered recent event (TORE) volume is proposed in  [[3](#bib.bib3)],
    aiming at compactly maintaining raw spike temporal information with minimal information
    loss.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基于体素的表示将原始事件映射到时间桶内的最近时间网格。第一个时空体素网格在 [[49](#bib.bib49)] 中提出，通过线性加权积累将事件插入体积，以改善时间域的分辨率。这个时空体素网格在一些后续工作中也被使用 [[64](#bib.bib64),
    [51](#bib.bib51)]。最近，[[3](#bib.bib3)] 中提出了一种时间排序最近事件 (TORE) 体积，旨在以最小的信息损失紧凑地保持原始尖峰时间信息。
- en: 2.1.4 Graph-based Representation
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 基于图的表示
- en: Aiming at preserving the sparsity of events, graph-based approaches transform
    the raw event streams within a time window into a set of connected nodes. Bi et
    al. first proposed a residual graph CNN architecture to obtain a compact graph
    representation for object classification [[65](#bib.bib65), [52](#bib.bib52)].
    The graph CNN preserves the spatial-temporal coherence of input events while avoiding
    large computational costs. More recently, Deng et al. proposed a voxel graph CNN
    which aims at exploiting the sparsity of event data [[53](#bib.bib53)]. The proposed
    EV-VGCNN [[53](#bib.bib53)] is a lightweight voxel graph CNN while achieving the
    SOTA classification accuracy with very low model complexity.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 针对保持事件稀疏性的目标，基于图的方法将时间窗口内的原始事件流转换为一组连接的节点。Bi 等人首次提出了一种残差图 CNN 架构，以获得用于对象分类的紧凑图表示 [[65](#bib.bib65),
    [52](#bib.bib52)]。图 CNN 保留了输入事件的时空一致性，同时避免了较大的计算成本。最近，Deng 等人提出了一种体素图 CNN，旨在利用事件数据的稀疏性 [[53](#bib.bib53)]。提出的
    EV-VGCNN [[53](#bib.bib53)] 是一种轻量级体素图 CNN，同时以非常低的模型复杂度实现了 SOTA 分类准确率。
- en: 2.1.5 Spike-based Representation
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5 基于尖峰的表示
- en: 'Due to the sparsity and asynchronous nature of event streams, most of the above
    representations consider the timestamps. Different from the standard DNN models,
    spiking neural networks (SNNs)[[66](#bib.bib66), [67](#bib.bib67), [56](#bib.bib56),
    [50](#bib.bib50), [54](#bib.bib54), [55](#bib.bib55)] are advantageous in that
    they incorporate the concept of time into operational models. Therefore, SNNs
    better fit the biological neuronal mechanism by using signals in the form of pulses
    (discontinuous values) to convey visual information. SNNs are applied to extracting
    features from events asynchronously to solve diverse tasks, such as object classification[[54](#bib.bib54),
    [50](#bib.bib50)] and gesture recognition[[56](#bib.bib56), [55](#bib.bib55)].
    However, due to the complex dynamics and non-differentiable nature of the spikes,
    two challenges exist: 1) Well-established back-propagation methods cannot be applied
    to the training process, leading to a long training time and high costs; 2) Specialized
    and effective hardware and algorithms are lacking. Consequently, their accuracy
    cannot exceed SOTA methods. Future research could explore more in this direction.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.6 Learning-based Representation
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The aforementioned event representations are mainly designed for a specific
    task and cannot be generally and flexibly applied to other tasks. To this end,
    Gehrig et al. [[39](#bib.bib39)] proposed the first learning-based approach to
    convert asynchronous raw events into tensor-like inputs, which can be flexibly
    applied to diverse downstream tasks. In particular, a multi-layer perceptron (MLP)
    is adopted to learn the coordinates and timestamps of events to obtain grid-like
    representations. Moreover, some methods [[68](#bib.bib68), [40](#bib.bib40)] extract
    features from events using Long Short-Term Memory (LSTM). A representative approach,
    Matrix-LSTM [[40](#bib.bib40)], utilizes a grid of LSTM cells to integrate information
    in the temporal axis. This approach follows a fully differentiable procedure that
    extracts the most relevant event representations for downstream tasks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.7 Remarks
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [I](#S2.T1 "TABLE I ‣ 2.1 Event Representation ‣ 2 Event Processing for
    DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    outlines six types of mainstream representation methods for DNNs. Different event
    representations offer unique advantages and considerations when applied to various
    tasks. Image-based representation enables seamless integration with traditional
    deep learning algorithms, allowing for applications in object detection, segmentation,
    and feature extraction. Surface-based representation provides spatial-temporal
    context and preserves temporal information to a certain extent. Voxel-based representation
    enhances resolution and preserves raw spike temporal information by mapping raw
    events into temporal grids. Graph-based representation maintains sparsity and
    coherence while minimizing computational costs, achieving high classification
    accuracy. Spike-based representation, buttressed by SNNs, offers advantages in
    asynchronous processing, efficiency, noise robustness, and compatibility with
    neuromorphic hardware. Learning-based representation aims to discover optimal
    event representations, adapting to task-specific requirements. However, practical
    factors such as computational complexity and data availability should be considered.
    Overall, the selection of event representation should be considered based on the
    requirements of the task, and the trade-offs between complexity, computational
    efficiency, and interpretability. Further research and studies are urgently needed
    to explore more generic event representations for a wider range of tasks.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [I](#S2.T1 "TABLE I ‣ 2.1 Event Representation ‣ 2 Event Processing for
    DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    概述了 DNNs 的六种主流表示方法。不同的事件表示在应用于各种任务时提供了独特的优势和考虑因素。基于图像的表示方法能够与传统深度学习算法无缝集成，应用于目标检测、分割和特征提取。基于表面的表示方法提供了时空背景，并在一定程度上保留了时间信息。基于体素的表示方法通过将原始事件映射到时间网格中来增强分辨率并保留原始脉冲时间信息。基于图的表示方法在保持稀疏性和连贯性的同时，最小化计算成本，实现了高分类准确度。基于脉冲的表示方法在
    SNNs 的支持下，在异步处理、效率、噪声鲁棒性以及与神经形态硬件的兼容性方面具有优势。基于学习的表示方法旨在发现最佳事件表示，适应任务特定要求。然而，实践中的计算复杂性和数据可用性等因素应予以考虑。总体而言，事件表示的选择应根据任务需求进行考虑，并权衡复杂性、计算效率和可解释性之间的利弊。进一步的研究和探索亟需以发现适用于更广泛任务的通用事件表示方法。'
- en: 2.2 Quality Enhancement for Events
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 事件质量提升
- en: Event cameras, e.g., DAVIS346 [[69](#bib.bib69)], are with relatively low resolution—346$\times$240,
    while some event cameras, e.g., Prophesse ¹¹1https://www.prophesee.ai/, show higher
    spatial resolution up to, e.g., 640$\times$480\. These cameras often suffer from
    unexpected noise in the captured event data, especially in challenging visual
    conditions and event representation processes. Also, the spatial resolution is
    still lower than that of the frame-based cameras. These problems often hamper
    applying deep learning to event-based vision. Therefore, research has been recently
    conducted to improve the spatial resolution of events and denoise the events to
    achieve higher quality.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 事件相机，例如 DAVIS346 [[69](#bib.bib69)]，具有相对较低的分辨率—346$\times$240，而一些事件相机，例如 Prophesse
    ¹¹1https://www.prophesee.ai/，显示出更高的空间分辨率，例如 640$\times$480。这些相机通常在捕捉的事件数据中遭遇意外噪声，尤其在具有挑战性的视觉条件和事件表示过程中。此外，其空间分辨率仍低于基于帧的相机。这些问题通常妨碍了深度学习在基于事件的视觉中的应用。因此，近期研究已致力于提高事件的空间分辨率并对事件进行去噪，以实现更高质量的结果。
- en: 2.2.1 Event Super-Resolution
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 事件超分辨率
- en: Different from image super-solution (SR), event SR requires distribution estimation
    in both spatial and temporal dimensions. Li et al. [[70](#bib.bib70)] first proposed
    to solve the spatial-temporal SR problem of the LR event image. Later on, Wang
    et al. [[71](#bib.bib71)] proposed to bridge intensity images and events from
    the Dynamic Vision Sensor (DVS) via joint image filtering, so as to obtain motion-compensated
    event frames with high-resolution (HR) and less noise. The first DL-based approach
    is proposed by Duan et al. [[72](#bib.bib72)], which addresses the joint denoising
    and SR by using a multi-resolution event recording system and a 3D U-Net-based
    framework, called EventZoom. In particular, it incorporates event-to-image reconstruction
    to achieve resolution enhancement. Furthermore, Li et al.[[73](#bib.bib73)] proposed
    an SNN framework with a constraint learning mechanism to simultaneously learn
    the spatial and temporal distributions of event streams. Recently, Weng et al.[[74](#bib.bib74)]
    introduced a Recurrent Neural Network (RNN) that employs temporal propagation
    and spatial-temporal fusion net to ensure the restoration abilities of fine-grained
    event details without any auxiliary high-quality and HR frames.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Though these methods achieve plausible SR results, the spatial-temporal
    distribution estimation leads to high latency for large factor SR, e.g., $\times
    16$. Future research could focus on reducing inference latency and lightweight
    network design.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Event Denoising
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The presence of random noise, such as thermal noise and junction leakage currents,
    leads to background activity (BA) where an event is generated without any log-intensity
    change. To address this issue, various approaches have been developed, including
    filter-based methods, DL-based methods, and simulator-based methods.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Filter-based methods utilize various types of filters to eliminate background
    activity. These methods include bio-inspired filters[[75](#bib.bib75)], hardware-based
    filters[[76](#bib.bib76), [77](#bib.bib77)], spatial filters[[42](#bib.bib42),
    [78](#bib.bib78), [79](#bib.bib79)], and temporal filters[[45](#bib.bib45), [45](#bib.bib45),
    [71](#bib.bib71)]. However, in complex environments with multiple noise sources,
    these methods often fail to achieve satisfactory results. To mitigate these problems,
    Guo et al.[[80](#bib.bib80)] introduced a novel framework that quantifies denoising
    algorithms more effectively by measuring receiver operating characteristics using
    known mixtures of signal and noise DVS events. For experiment results of different
    filter-based methods, refer to Fig. 6 in the supplementary material.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: DL-based methods have also been introduced in  [[81](#bib.bib81), [82](#bib.bib82)].
    One representative framework is EDnCNN proposed by Baldwin et al.[[81](#bib.bib81)].
    It transforms neighboring events into voxels and differentiates noise using an
    Event Probability Mask (EPM). However, the artificial regularization operation,
    such as voxel transformation, can compromise the inherent properties of event
    data. To address this issue, AEDNet[[82](#bib.bib82)] decomposes DVS signals into
    temporal correlation and spatial affinity, leveraging the properties of temporal
    continuation and spatial discreteness. These signals are then separately processed
    using a unique feature extraction module. For detailed qualitative and quantitative
    experiment results, refer to Fig. 5 and Tab. 4 in the supplementary material.
    Another type of approach, a.w.a., simulator-based methods, focuses on incorporating
    noise effects into event simulators. The knowledge learned from simulated event
    data is then transferred to real data to reduce the influence of noise [[83](#bib.bib83),
    [84](#bib.bib84), [25](#bib.bib25), [85](#bib.bib85)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Some event samples have abnormal pixel values in the event-count channels.
    Different abnormal pixels in different feature areas of the same object lead to
    a decline in feature learning performance, affecting subsequent tasks. Thus, denoising
    methods are one of the bases of event-based vision. In future research, a more
    general DL-based denoising pipeline, which can be applied to various event-based
    vision tasks, is worth exploring.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 3 Image Restoration and Enhancement
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Event cameras hold immense potential for leveraging event cameras in the reconstruction
    and restoration of HDR images and high frame-rate videos. However, their unique
    imaging paradigm presents a challenge when applying vision algorithms designed
    for frame-based cameras. To address this challenge and bridge the gap between
    event-based and standard computer vision, many methods have been proposed to reconstruct
    intensity video frames or images from events.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we group the prevailing methods into two major types: event-based
    image (or video) reconstruction (with only events as inputs) and event-guided
    image restoration (hybrid inputs of events and frames). For the former, the main
    problem is how to fully explore the visual information, e.g., edge, from events
    with DNNs to reconstruct high-quality intensity images or video frames; while
    the latter explores how to fuse frames and events while leveraging the advantages
    of events, e.g., HDR, to benefit the image restoration process. We now review
    the state-of-the-art (SOTA) techniques in the following sections.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Event-based Image/Video Reconstruction
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Insight: This task learns a mapping from a stream of events to a single intensity
    image or sequence of images (i.e., video). The mapped results allows for applying
    the off-the-shelf DL algorithms—developed for frame-based cameras—to learning
    downstream tasks. From our review, intensive research has been devoted to achieving
    this task, as summarized in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1 Event-based Image/Video
    Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"), Fig. [2](#S3.F2 "Figure 2 ‣ 3.1
    Event-based Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣
    Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    and Tab. [II](#S3.T2 "TABLE II ‣ 3.1 Event-based Image/Video Reconstruction ‣
    3 Image Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A
    Comprehensive Survey and Benchmarks").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Early methods rely on the assumption about the scene structure (or motion dynamics) [[86](#bib.bib86),
    [87](#bib.bib87)] or event integration with regularization terms [[88](#bib.bib88)]
    to reconstruct intensity images. However, these methods suffer from artifacts
    due to the direct event integration, and the reconstructed intensity images are
    not photo-realistic enough. DL-based methods, by contrast, bring significant accuracy
    gains. In this paper, we analyze the SOTA deep learning methods based on the challenges:
    1) A lack of large-scale datasets for training deep networks; 2) High computational
    complexity and low latency; 3) The low-quality of reconstructed images or videos,
    e.g., relatively low resolution and blurred images.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Qualitative comparison results of some image reconstruction methods [[89](#bib.bib89)]
    on event dataset [[12](#bib.bib12)].'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Type | MSE $\downarrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ |
    Time |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| E2VID  [[90](#bib.bib90)] | DL-based | 0.069 | 0.395 | 0.438 | 0.2448 s |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| ECNN [[91](#bib.bib91)] | D-based | 0.056 | 0.416 | 0.442 | 0.2839 s |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| BTEB [[92](#bib.bib92)] | DL-based | 0.090 | 0.357 | 0.520 | 0.4059 s |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| Tikhonov  [[89](#bib.bib89)] | Model-based | 0.121 | 0.356 | 0.485 | 0.4401
    s |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: '| TV [[89](#bib.bib89)] | Mode-based | 0.113 | 0.386 | 0.502 | 4.0443 s |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| CNN [[89](#bib.bib89)] | DL-based | 0.080 | 0.437 | 0.485 | 28.3904 s | ![Refer
    to caption](img/7de71ad3bf8a765ce2bdf974e5df4ebd.png)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Methods for event-based image/video reconstruction.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Comparison of the representative event-guided video frame interpolation
    (VFI) methods.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '| Publication | Methods | Highlight | Event Representation | Optical Flow |
    Deblurring | Supervised | Backbone | Dataset |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2021 | TimeLens[[93](#bib.bib93)] | synthesis-based and ﬂow-based |
    voxel grid[[49](#bib.bib49)] | ✓ | ✗ | ✓ | CNN | HQF,Vimeo90k,GoPro,Middlebury,HS-ERGB
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2021 | EFI-Net[[94](#bib.bib94)] | different spatial resolutions | voxel
    grids[[49](#bib.bib49)] | ✗ | ✗ | ✓ | CNN | Samsung GE3 DVS |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2021 | Yu et al.[[95](#bib.bib95)] | weakly supervised | Image-based
    | ✓ | ✗ | ✗ | ViT+CNN | GoPro, SloMo-DVS |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | Time Replayer[[96](#bib.bib96)] | unsupervised cycle-consistent
    style | 4-channel frames[[41](#bib.bib41)] | ✓ | ✗ | ✗ | CNN | GoPro, Adobe240,
    Vimeo90k |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | TimeLens++[[97](#bib.bib97)] | multi-scale feature-level fusion
    | voxel grid[[49](#bib.bib49)] | ✓ | ✗ | ✓ | CNN | BS-ERGB, HS-ERGB |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2022 | $A^{2}OF$[[98](#bib.bib98)] | optical flows adjustment | four-channel
    frame[[41](#bib.bib41)] | ✓ | ✗ | ✓ | CNN | Adobe240, GoPro, Middlebury, HS-ERGB,
    HQF |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| ECCV2020 | Lin et al.[[99](#bib.bib99)] | physical model inspired | stream
    and frame-based | ✗ | ✓ | ✓ | CNN | GoPro,Blur-DVS |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | E-CIR[[100](#bib.bib100)] | parametric intensity function | polynomial[[100](#bib.bib100)]
    | ✗ | ✓ | ✓ | CNN | REDS |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | Zhang et al.[[101](#bib.bib101)] | deblurring and frame interpolation
    | event streams | ✗ | ✓ | ✗ | CNN | GoPro, HQF, RBE | ![Refer to caption](img/d7d58c05f9f2268e94f2df983c90f6b9.png)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Visual examples of some SOTA methods for video reconstruction (E2VID
     [[90](#bib.bib90)], EF [[91](#bib.bib91)], RCNN [[102](#bib.bib102)]).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a7aacc204721616ed7e24c81a0b7b2df.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Representative VFI methods, including, e.g., (a) TimeLens [[93](#bib.bib93)],
    the first event-guided VFI method (b) TimeLens++[[97](#bib.bib97)], the SOTA event-based
    VFI method (c) TimeReplayer[[96](#bib.bib96)], the first unsupervised event-guided
    VFI method.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: For the first challenge,  [[103](#bib.bib103)] and  [[104](#bib.bib104)] are
    representative works leveraging generative adversarial networks (GANs) to bridge
    knowledge transfer between events and RGB images to alleviate the scarce labeled
    data problem. wMoreover, Stoffregen et al. [[91](#bib.bib91)] found that the contrast
    threshold is a key factor in synthesizing data to match the real event data well.
    Further, Vallés et al. [[92](#bib.bib92)] explored the theoretical basis of event
    cameras and proposes self-supervised learning to reduce the dependence on the
    ground truth video (including synthetic data) based on the photometric constancy
    of events.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: For addressing the second challenge, Scheerlinck et al. [[19](#bib.bib19)] employed
    recurrent connections to build a state over time, allowing a much smaller recurrent
    neural network that reuses previously reconstructed results. Interestingly, Duwek
    et al. [[105](#bib.bib105)] combined CNNs with SNNs based on Laplacian prediction
    and Poisson integration to achieve video reconstruction with fewer parameters.
    To solve the third challenge, GANs, the double integral model [[106](#bib.bib106)],
    and RNNs are applied to avoid generating blurred results and obtain high-speed
    and HDR videos from events in [[17](#bib.bib17), [102](#bib.bib102), [90](#bib.bib90),
    [89](#bib.bib89)]. As for generating super-resolution (SR) images/videos from
    events, we divide the prevailing works into three categories, including optimization-based[[70](#bib.bib70)],
    supervised [[72](#bib.bib72), [107](#bib.bib107), [108](#bib.bib108), [71](#bib.bib71)],
    and adversarial learning methods [[109](#bib.bib109), [110](#bib.bib110)]. Optimization-based
    methods, e.g., [[70](#bib.bib70)], adopts a two-stage framework to solve the SR
    image reconstruction problem based on the non-homogeneous Poisson point process.
    Supervised methods either utilize residual connections to prevent the network
    models from the problem of gradients vanishing when generating SR images or estimate
    optical flow and temporal constraints to learn the motion cues, so as to reconstruct
    SR videos. For adversarial learning methods, Wang et al.[[109](#bib.bib109)] propose
    a representative end-to-end SR image reconstruction framework without access to
    the ground truth (GT), i.e., HR images.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: In this section, we discuss various techniques for event-based image/video
    reconstruction. However, we acknowledge the need for a brief comparison to determine
    which technique is more suitable for different scenarios. While DL-based methods
    have shown significant accuracy gains in terms of accuracy and photorealism compared
    to early approaches relying on assumptions and regularization, they also come
    with increased computational complexity. On the other hand, traditional methods
    based on direct event integration may suffer from artifacts and produce less photo-realistic
    results. Furthermore, the choice of event representation remains an open question,
    and existing learned models often exhibit limited generalization capability. Noise
    in event data also poses a significant challenge, and the reconstruction of color
    images/videos from events is a particularly difficult problem. Future research
    efforts could focus on addressing these aspects to improve the quality and fidelity
    of reconstructed results.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Event-guided Image/Video Super-Resolution (SR)
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Insight: The goal is to explore the visual information, e.g., edge, and high
    temporal resolution of events, which are fused with the low-resolution (LR) image/video
    to recover the high-resolution (HR) image/video, as shown in Tab. [IV](#S3.T4
    "TABLE IV ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks").'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Image SR: eSL-Net [[111](#bib.bib111)] is the first work that introduces events
    for guiding image SR. It proposes a unified event-guided sparse learning framework
    that simultaneously denoises, deblurs, and super-resolves the low-quality active
    pixel sensor (APS) ²²2A type of frame-based sensor, embedded in DAVIS event cameras.
    images to recover high-quality images in an end-to-end learning manner. However,
    due to the limitations of sparse coding, this method performs poorly on more complex
    datasets [[111](#bib.bib111), [112](#bib.bib112)]. EvIntSR [[112](#bib.bib112)]
    achieves the goal of image SR in two steps: 1) Synthesizing a sequence of latent
    frames by combing events and blurry LR frames; 2) Merging latent frames to obtain
    a sharp HR frame. In general, EvIntSR explores the distinctive properties of events
    more directly than eSL and achieves better SR results on the simulation dataset.
    However, this method has two drawbacks: 1) Errors are accumulated in the two-stage
    training procedure; 2) The visual information of events is less explored in the
    second stage.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Video SR: Compare with image SR, video SR pays more attention to the relationship
    between multiple frames. E-VSR [[108](#bib.bib108)] is the first VSR framework
    with events. Similar to EvIntSR, it also consists of two sub-tasks: video frame
    interpolation and video SR and is limited by the accumulated errors. Recently,
    EG-VSR [[113](#bib.bib113)] employs implicit functions for learning the continuous
    representation of videos. This method enables end-to-end upsampling at arbitrary
    scales, offering advantages in the video SR task.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Comparison of the representative event-guided image/video SR methods.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '| Publication | Methods | Highlight | Backbone |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2020 | eSL-Net [[111](#bib.bib111)] | sparse learning | CNN |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2021 | EvIntSR [[112](#bib.bib112)] | two-step methods | CNN |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2021 | E-VSR [[108](#bib.bib108)] | two-step methods | CNN |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2023 | EG-VSR [[113](#bib.bib113)] | SR with arbitrary scales | ViT+CNN
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: 'Remarks: While significant progress has been made for this task, including
    the ability to perform upsampling at arbitrary scales, there are still areas that
    need further investigation. For example, the research ignores the distinct modality
    differences between events and RGB frames. Therefore, the directly fusing features
    of two modalities might degrade the performance of SR as events are often disturbed
    by unexpected noises, e.g., in low-light scenes. Future research could explore
    more to tackle these problems.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2554f33da133e0a55171402a123ca86.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Visual results of VFI by three different methods. (TimeLens[[93](#bib.bib93)],
    TimeReplayer[[96](#bib.bib96)], $A^{2}OF$[[98](#bib.bib98)])'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e16f657c729713bdfd1dc7d80a39e743.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Representative deblurring methods, including, e.g., (a) Interaction-based
    methods, (b) Event fusion-based methods, e.g., and (c) Event selection-based methods.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Event-guided Video Frame Interpolation (VFI)
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Insight : This task leverages the high temporal resolution of events and aims
    to estimate the non-linear motion information between frames, so as to insert
    latent frames between two consecutive frames. Based on how the VFI frameworks
    are learned, we categorize them into three types: supervised, weakly-supervised,
    and unsupervised methods, as shown in Tab. [III](#S3.T3 "TABLE III ‣ 3.1 Event-based
    Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks"). The VFI results
    of some representative methods are visualized in Fig. [5](#S3.F5 "Figure 5 ‣ 3.2
    Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and Enhancement
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks").'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised methods: TimeLens [[93](#bib.bib93)] is the first and representative
    work, which employs four modules to fuse features to achieve warping-based and
    synthesis-based interpolation (See Fig. [4](#S3.F4 "Figure 4 ‣ 3.1 Event-based
    Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")(a)). A dataset
    with spatially aligned events and high-speed videos was also released. However,
    it has limitations in that 1) the optical flow estimated from events limits the
    warped frames; 2) the noisy events restrict the quality of optical flow; and 3)
    it is learned sequentially (i.e., not in an end-to-end manner). Therefore, training
    TimeLens is difficult, and errors are accumulated, degrading the performance.
    These problems are better addressed later on by Timelens++[[97](#bib.bib97)],
    $A^{2}OF$ [[98](#bib.bib98)], and EFI-Net[[94](#bib.bib94)].'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, Timelens++[[97](#bib.bib97)] proposes a framework, comprised
    of four modules including motion estimation, warping encoder, synthesis encoder,
    and fusion module, as depicted in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1 Event-based
    Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")(b). This method
    introduces multi-scale feature-level fusion and computes one-shot non-linear inter-frame
    motion, which could effectively be sampled for image warping based on events and
    frames. $A^{2}OF$ [[98](#bib.bib98)] focuses on generating the anisotropic optical
    flow from events. However, such an approach cannot model the complicated motion
    in real-world scenes; therefore, $A^{2}OF$ employs the distribution masks for
    optical flow from events to achieve the intricate intermediate motion interpolation.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the events used by the aforementioned methods have the
    same spatial resolution as RGB frames. Unfortunately, it is quite expensive to
    match a RGB sensor’s resolution with an event sensor in real scenarios. Therefore,
    EFI-Net[[94](#bib.bib94)] proposes a multi-phase CNN-based framework, which can
    fuse the frames and events with various spatial resolutions. In summary, supervised
    methods rely on paired data with high-frame-rate videos and events. HS-ERGB [[93](#bib.bib93)]
    and BS-ERGB[[97](#bib.bib97)] are representative datasets. However, these datasets
    suffer from strict pixel alignments between events and frames and are expensive
    to collect. Therefore, some weakly-supervised and unsupervised methods have been
    proposed recently.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'Weakly-supervised methods: Yu et al.[[95](#bib.bib95)] proposed the first weakly-supervised
    event-based VFI method. In practice, it extracts complementary information from
    events to correct image appearance and employs an attention mechanism to support
    correspondence searching on the low-resolution feature maps. Meanwhile, a real-world
    dataset, namely SloMo-DVS, is also released.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised methods: TimeReplayer [[96](#bib.bib96)] is the first unsupervised
    method, trained in a cycle-consistent manner, as shown in Fig. [4](#S3.F4 "Figure
    4 ‣ 3.1 Event-based Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    (c). It directly estimates the optical flow between the key-frame and the input
    frame, instead of computing intermediate frames as a proportion of the computed
    optical flow between input frames. In this way, the complex motion can be estimated.
    Then, input frames can be reconstructed by key-frame and inverse optical flow.
    Totally, this cycle consistency method not only models complex nonlinear motion
    but also avoids the need for a large amount of paired high-speed frames and events.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: All the above-mentioned methods have the strong assumption that the exposure
    time of RGB frames is very short, and there are no blur artifacts in frames. However,
    this assumption is overly harsh because the practical exposure time can be long
    and results in blur artifacts in frames, particularly in complex lighting scenes.
    When the exposure time is longer, the issue of interpolation needs to be re-examined.
    For this reason, [[99](#bib.bib99), [101](#bib.bib101), [100](#bib.bib100)] jointly
    address the interpolation problem and deblurring. For example, E-CIR [[100](#bib.bib100)]
    transforms a blurry image into a sharp video that is represented as a time-to-intensity
    parametric function with events. Similarly, Zhang et al. [[101](#bib.bib101)]
    employed a learnable double integral network to map blurry frames to sharp latent
    images with event guidance. Lin et al. [[99](#bib.bib99)] emphasized that the
    residuals between a blurry image and a sharp image are event integrals. Based
    on this perspective, they proposed a network that uses events to estimate residuals
    for sharp frame restoration.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: From our review, the majority of the methods are based on supervised
    learning, and weakly supervised or unsupervised methods still have a lot of room
    for further research. For example, mutual supervision could be performed through
    the imaging relationship between events and interpolated frames to relieve the
    need for ground truth, i.e., high frame rate video for training. Also, dense optical
    flow estimated by events[[114](#bib.bib114)], could be used as a constraint between
    interpolation results to improve VFI accuracy in an unsupervised manner.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Event-guided Image/Video Deblurring
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Insight: This task gets inspired by the no-motion-blur property of events and
    aims to restore a sharp image/video from a blurry image/video sequence under the
    guidance of events. Because supervised methods tend to achieve higher PSNR and
    SSIM [[96](#bib.bib96), [93](#bib.bib93)]. Traditional deblurring methods rely
    on the physical event generation model [[71](#bib.bib71)]. In particular, Pan
    et al. [[106](#bib.bib106)] proposed an event-based double integral model for
    recovering latent intensity images. Based on this model, sharp images and videos
    could be generated by solving the non-convex optimization problems under adverse
    visual conditions. However, it suffers from the problem of accumulated error caused
    by noise in the sampling process. By contrast, learning-based methods directly
    explore the relationship between blurry and sharp images with the help of events
    and show more plausible deblurring result. In this paper, we divide the learning-based
    methods into three categories: 1) interaction-based methods; 2) fusion-based methods;
    and 3) selection-based methods (See Fig. [6](#S3.F6 "Figure 6 ‣ 3.2 Event-guided
    Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and Enhancement ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")). The
    deblur results of some representative methods are shown in Tab. [V](#S3.T5 "TABLE
    V ‣ 3.4 Event-guided Image/Video Deblurring ‣ 3 Image Restoration and Enhancement
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks").'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Qualitative comparison of deblurring methods on GoPro and HQF dataset
    from  [[101](#bib.bib101)]. ‘N/A’ means no results are available.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | GoPro | HQF | Param. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| PSNR$\uparrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ | PSNR $\uparrow$ |
    SSIM $\uparrow$ | LPIPS$\downarrow$ |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| LEVS  [[115](#bib.bib115)] | 20.84 | 0.5473 | 0.1111 | 20.08 | 0.5629 | 0.0998
    | 18.21M |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| EDI  [[106](#bib.bib106)] | 21.29 | 0.6402 | 0.1104 | 19.65 | 0.5909 | 0.1173
    | N/A |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| eSL-Net  [[111](#bib.bib111)] | 17.80 | 0.5655 | 0.1141 | 21.36 | 0.6659
    | 0.0644 | 0.188M |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| LEDVDI  [[99](#bib.bib99)] | 25.38 | 0.8567 | 0.0280 | 22.58 | 0.7472 | 0.0578
    | 4.996M |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| RED [[116](#bib.bib116)] | 25.14 | 0.8587 | 0.0425 | 24.48 | 0.7572 | 0.0475
    | 9.762M |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| EVDI [[101](#bib.bib101)] | 30.40 | 0.9058 | 0.0144 | 24.77 | 0.7664 | 0.0423
    | 0.393M |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: 'Interaction-based methods usually input the blurry image and events into two
    different networks and then carry out information interaction after encoding the
    features in each branch to improve the deblurring effect (See Fig. [6](#S3.F6
    "Figure 6 ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks") (a)). For example, in [[116](#bib.bib116)], a self-supervised
    framework was proposed to reduce the domain gap between simulated and real-world
    data. Specifically, they first estimated the optical flow and exploited the blurry
    and photometric consistency to enable self-supervision on the deblurring network.
    Lin et al.[[99](#bib.bib99)] introduced a CNN framework to predict the residual
    between sharp and blurry images for deblurring, and the residual between sharp
    frames for interpolation. Jiang et al. [[117](#bib.bib117)] explored long-term,
    local appearance/motion cues and novel event boundary priors to solve motion deblurring.
    Zhang et al. [[101](#bib.bib101)] utilized low latency of events to alleviate
    motion blur and facilitate the prediction of intermediate frames.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Fusion-based methods aims to design a principled framework for video deblurring
    and event-guided deblurring [[111](#bib.bib111)], as shown in Fig. [6](#S3.F6
    "Figure 6 ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")(b). For instance, Shang et al. [[118](#bib.bib118)] proposed
    a two-stream framework to explore the non-consecutively blurry frames and bridge
    the gap between event-guided and video deblurring.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Selection-based methods, e.g., [[119](#bib.bib119)], formulate the event-guided
    motion deblurring by considering the unknown exposure and readout time in the
    video frame acquisition process. The main challenge is how to selectively use
    event features by estimating the cross-modal correlation between the blurry frame
    features and the events. Therefore, the proposed event selection module subtly
    selects useful events, and the fusion module fuses the selected event features
    and blur frames effectively, as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.2 Event-guided
    Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and Enhancement ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks") (c).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Most of the aforementioned deblurring methods are limited to some
    specific scenes. In some scenes with large or fast motions, the model’s accuracy
    may deteriorate dramatically.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Experiments of representative methods on event object classification.
    N/A means no results available.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '| Publication | Methods | Dataset |  | Param. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| N-MINIST [[120](#bib.bib120)] | MINIST-DVS [[121](#bib.bib121)] | N-Caltech101 [[120](#bib.bib120)]
    | CIFAR10-DVS [[122](#bib.bib122)] | N-Cars [[44](#bib.bib44)] | ASL-DVS [[65](#bib.bib65)]
    | N-ImageNet [[48](#bib.bib48)] |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| TPAMI 2015 | HFirst [[66](#bib.bib66)] | 0.712 | N/A | 0.054 | N/A | 0.561
    | N/A | N/A | 21.79M |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| TPAMI 2016 | HOTS [[43](#bib.bib43)] | 0.808 | 0.803 | 0.210 | 0.271 | 0.624
    | N/A | N/A | 21.79M |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2018 | HATS [[44](#bib.bib44)] | 0.991 | 0.984 | 0.642 | 0.524 | 0.902
    | N/A | 0.471 | 21.79M |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2019 | EST [[39](#bib.bib39)] | N/A | N/A | 0.817 | N/A | 0.925 | N/A
    | 0.489 | 21.79M |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2019 | RG-CNNs [[52](#bib.bib52)] | 0.990 | 0.986 | 0.657 | 0.540 |
    0.914 | 0.901 | N/A | 19.46M |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| TPAMI 2019 | DART [[123](#bib.bib123)] | 0.979 | 0.985 | 0.664 | 0.658 |
    N/A | N/A | N/A | N/A |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2020 | Matrix-LSTM [[40](#bib.bib40)] | 0.989 | N/A | 0.843 | N/A |
    0.943 | 0.997 | 0.322 | 25.56M |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2020 | ASCN [[59](#bib.bib59)] | N/A | N/A | 0.745 | N/A | 0.944 | N/A
    | N/A | 9.47M |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2021 | EvS  [[124](#bib.bib124)] | N/A | 0.991 | 0.761 | 0.680 | 0.931
    | N/A | N/A | N/A |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2021 | DiST [[48](#bib.bib48)] | N/A | N/A | N/A | N/A | N/A | N/A |
    0.484 | 21.79M |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| TCSVT 2021 | MVF-Net [[38](#bib.bib38)] | 0.993 | N/A | 0.871 | 0.663 | 0.968
    | 0.996 | N/A | 21.79M |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | AEGNNs [[125](#bib.bib125)] | N/A | N/A | 0.668 | N/A | 0.945
    | N/A | N/A | N/A |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2022 | EV-VGCNN [[53](#bib.bib53)] | 0.994 | N/A | 0.748 | N/A | 0.953
    | 0.983 | N/A | 21.79M |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| TPAMI 2022 | TORE [[3](#bib.bib3)] | 0.994 | N/A | 0.798 | N/A | 0.977 |
    0.996 | N/A | 5.94M |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: 3.5 Event-based Deep Image/Video HDR
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Insight: The HDR of events makes it naturally more advantageous to reconstruct
    an HDR image/video. The predominant methods can be divided into two main categories:
    event-based HDR image/video HDR methods [[90](#bib.bib90), [102](#bib.bib102),
    [17](#bib.bib17)] and event-guided image/video HDR methods (a hybrid of event
    and frame data) [[126](#bib.bib126), [127](#bib.bib127), [18](#bib.bib18)].'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Event-based image/video HDR typically employs the idea of event-to-image translation—reconstructing
    HDR images from events, as mentioned in Sec. [3.1](#S3.SS1 "3.1 Event-based Image/Video
    Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"). Representative works are based
    on the recurrent neural networks (RNNs) [[90](#bib.bib90), [102](#bib.bib102)]
    (See Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 Event-based Deep Image/Video HDR ‣ 3 Image
    Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks") (a)) or generative adversarial networks (GANs) [[17](#bib.bib17)]
    (See Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 Event-based Deep Image/Video HDR ‣ 3 Image
    Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks") (b)). However, the reconstructed HDR results intrinsically
    lack textural details, especially in the static scene, as events are sparse and
    motion-dependent.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/43598221bba1b31f9aa472c57b8d8bdc.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Representative DL-based HDR imaging methods. (a) RNN-based methods [[90](#bib.bib90),
    [102](#bib.bib102)] and (b) GAN-based method [[17](#bib.bib17)] that use events
    only, and (c) event-frame fusion [[126](#bib.bib126), [127](#bib.bib127), [18](#bib.bib18)].
    (latent embedding denotes information learned by feature extractor, filter,etc.)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Event-guided image/video HDR: HDR imaging methods are categorized into two
    types: single-exposure HDR and multi-exposure HDR (See [[128](#bib.bib128)] for
    details). Event-guided image/video also follows these two paradigms.  [[126](#bib.bib126),
    [127](#bib.bib127), [18](#bib.bib18)] explore the potential of merging both events
    and frames for this task, as shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 Event-based
    Deep Image/Video HDR ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for
    Event-based Vision: A Comprehensive Survey and Benchmarks") (c). In particular,
    Han et al. [[126](#bib.bib126)] proposed the first single-exposure HDR imaging
    framework to recover an HR HDR image by adding the LR intensity map generated
    from events. This framework addresses the gaps in spatial resolution, dynamic
    range, and color representation of the hybrid sensor system to pursue a better
    fusion. By contrast, EHDR  [[18](#bib.bib18)] is the first multi-exposure HDR
    imaging framework that combines bracketed LDR images and synchronized events to
    recover an HDR image. To alleviate the impact of scene motion between exposures,
    EHDR employs events to learn a deformable convolution kernel, which can align
    feature maps from images with different exposure times. By contrast, HDRev-Net [[129](#bib.bib129)]
    implicitly mitigates the misalignment of multi-modal representations by aligning
    them in the shared latent space and fusing them with a confidence-guided fusion
    module.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Based on the review, only two research works have been proposed for
    deep HDR imaging. The most possible reason is that it is practically difficult
    to collect paired datasets for training, especially for multi-exposure HDR imaging.
    Future directions could consider directly fusing LDR images and events and learning
    a unified HDR imaging framework without relying on image/video reconstruction.
    Also, it is promising to explore how to leverage events to guide color image HDR
    imaging.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 4 Scene Understanding and 3D Vision
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Scene Understanding
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 Object Classification
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Insight: Event-based object classification aims to identify and classify objects
    from an event stream based on their visual characteristics. This allows for real-time
    object classification with high temporal resolution and low latency, making it
    suitable for applications in robotics, autonomous vehicles, and other mobile systems.
    Intuitively, we divide the event-based classification methods into three categories
    according to the input event representations and DNN types: 1) learning-based;
    2) graph-based; and 3) asynchronous model-based methods.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Learning-based methods Gehrig et al. [[39](#bib.bib39)] proposed the first end-to-end
    framework to learn event representation for object classification. In particular,
    it converts event streams into grid-like tensors, i.e., Event Spike Tensor (EST),
    through a sequence of differentiable operations. Though EST achieves high accuracy,
    it also brings redundant computation costs and high latency. To tackle this problem,
    Cannici et al. [[40](#bib.bib40)] proposed Matrix-LSTM to adaptively integrate
    and utilize information of events by the memory mechanism of LSTM. This makes
    it possible to efficiently aggregate the temporal information of event data.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based methods Some works also utilize graphs for representing events for
    the computational efficiency of the Graph CNNs. Yin et al. [[65](#bib.bib65)]
    proposed a representative approach that represents event data as a graph and introduced
    residual graph CNNs (RG-CNNs).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous model-based methods Though the learning-based methods obtain plausible
    classification results, they fail to fully explore the inherent asynchronicity
    and sparsity of event data. Consequently, Nico et al. [[59](#bib.bib59)] converted
    the classification models trained on the synchronous frame-like event representations
    into models taking asynchronous events as inputs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Comparison of existing representative event classification benchmarks.
    MR denotes Monitor Recording. MR is the process of capturing the visual output
    displayed on a computer monitor or screen.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | # of Samples | # of Classes | Sources | Paired RGB Data |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| N-Cars [[44](#bib.bib44)] | 24029 | 2 | Real | N/A |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| N-Caltech101 [[120](#bib.bib120)] | 8709 | 101 | MR | Caltech101 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| CIFAR10-DVS [[122](#bib.bib122)] | 10000 | 10 | MR | CIFAR10 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| ASL-DVS [[65](#bib.bib65)] | 100800 | 24 | Real | N/A |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| N-MNIST [[120](#bib.bib120)] | 70000 | 10 | MR | MNIST |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| MNIST-DVS [[121](#bib.bib121)] | 30000 | 10 | MR | MNIST |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| N-ImageNet [[48](#bib.bib48)] | 1781167 | 1000 | MR | ImageNet |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: Besides, to fit with the sparse event data, VMV-GCN[[130](#bib.bib130)] first
    considers the relationships between vertices of the graph and then groups the
    vertices according to the proximity both in the original input and feature space.
    Furthermore, for computational efficiency, AEGNN [[125](#bib.bib125)] proposes
    to process events sparsely and asynchronously as temporally evolving graphs. Meanwhile,
    EV-VGCNN[[53](#bib.bib53)] utilizes voxel-wise vertices rather than point-wise
    inputs to explicitly exploit the regional 2D semantics of event streams while
    maintaining the trade-off between accuracy and model complexity.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: Comparison of existing representative event object detection methods.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '| Publications | Method | Representations | Highlight | Backbone | Frame Images
    | Multi-modal |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | YOLE [[131](#bib.bib131)] | Surface-based | Event-based neural
    network components | CNN | ✗ | ✗ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| WACV 2022 | PointConv [[132](#bib.bib132)] | Image-based | pint-cloud feature
    extractor | CNN | ✗ | ✗ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| MFI 2022 | GFA-Net [[133](#bib.bib133)] | Image-based | Edge information
    & Temporal information across event frames | CNN & ViT | ✗ | ✗ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2020 | NGA [[23](#bib.bib23)] | Image-based | Grafting pre-trained deep
    network for novel sensors | CNN | ✗ | ✓ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| NeurIPS 2020 | RED [[23](#bib.bib23)] | Image-based | Recurrent architecture
    and temporal consistency | RNN | ✗ | ✗ |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| TIP 2022 | ASTMNet [[134](#bib.bib134)] | Image-based | Continuous event
    stream with lightweight RNN | RNN | ✗ | ✗ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| ICRA 2019 | Mixed-Yolo [[135](#bib.bib135)] | Image-based | Mixed APS frame
    and DVS frame | CNN | ✓ | ✗ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| ICME 2019 | JDF [[136](#bib.bib136)] | Spike-based | Joint detection with
    event streams and frames | CNN & SNN | ✓ | ✗ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| ICRA 2022 | FPN-fusion events [[137](#bib.bib137)] | Image-based | Robust
    detection with RGB and event-based sensors | CNN | ✗ | ✓ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: 'Benchmark datasets are vital foundations for the development of event-based
    vision, given that sufficient event data is barely available due to the novelty
    of event sensors. The existing event datasets can be briefly divided into two
    categories according to the captured scenes, i.e., the real and the simulated
    ones. Gehrig et al. [[25](#bib.bib25)] proposed to convert video datasets into
    event datasets by adaptive upsampling and using an event camera simulator (ESIM) [[85](#bib.bib85)].
    Models trained on the simulated dataset generalize well on the real data. More
    recently, N-ImageNet [[48](#bib.bib48)] serves as the first real large-scale fine-grained
    benchmark, which provides various validation sets to test the robustness of event-based
    object recognition approaches amidst changes in motion or illumination. We summarize
    the existing datasets for event recognition in Tab. [VII](#S4.T7 "TABLE VII ‣
    4.1.1 Object Classification ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") and conduct a benchmark evaluation for the representative event-based
    classification methods in Tab. [VI](#S3.T6 "TABLE VI ‣ 3.4 Event-guided Image/Video
    Deblurring ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks").'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: The accuracy of event-based object classification is always hindered
    by insufficient annotated datasets. Therefore, endeavors have been made to simulate
    event data using ESIM or generate event data from the monitor-displayed image
    observation. It also deserves to adapt the model trained on synthetic data to
    real-world event data [[138](#bib.bib138), [139](#bib.bib139)]. Another research
    direction could focus on leveraging large amounts of unlabeled data or active
    learning, where the classifier can request additional labeled data as needed in
    order to improve its accuracy.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Feature Tracking
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Insight: In recent years, researchers have focused on event-based feature tracking
    for its robustness in fast motion capture and extreme lighting conditions  [[30](#bib.bib30),
    [5](#bib.bib5)]. Early event-based feature trackers treat events as point sets
    and use Iterative Closest Point (ICP) [[140](#bib.bib140)] to track features [[141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144)], and there are also
    works use B-splines [[145](#bib.bib145)] and some other techniques to obtain the
    feature trajectories [[5](#bib.bib5), [146](#bib.bib146), [7](#bib.bib7), [147](#bib.bib147),
    [148](#bib.bib148)].'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Recently, deep learning-based method, a.k.a., data-driven methods for event-based
    feature tracking method has been proposed. The most representative one is  [[30](#bib.bib30)]
    which serves as the first work of introducing a data-driven feature tracker for
    event cameras, leveraging low-latency events to track features detected in a grayscale
    frame. The data-driven tracker outperforms the existing non-DL-based methods in
    relative feature age by up to 120% while keeping the lowest latency.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: From our review, we find that deep learning is just introduced to
    event-based feature tracking recently and it is worth exploring this direction.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: The quantitative results of the evaluated trackers on the EDS and
    EC dataset are reported in terms of ”Feature Aeg (FA)” of the stable tracks and
    the ”Expected FA”, which is the multiplication of the feature age by the ratio
    of the number of table tracks over the number of initial features. This table
    is from  [[30](#bib.bib30)].'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | EDS | EC |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| Feature Age (FA) | Expected FA | Feature Age (FA) | Expected FA |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| ICP [[140](#bib.bib140)] | 0.060 | 0.040 | 0.256 | 0.245 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| EM-ICP [[143](#bib.bib143)] | 0.161 | 0.120 | 0.337 | 0.334 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| HASTE [[147](#bib.bib147)] | 0.096 | 0.063 | 0.442 | 0.427 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| EKLT [[5](#bib.bib5)] | 0.325 | 0.205 | 0.811 | 0.775 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| DDFT [[30](#bib.bib30)] (zero-shot) | 0.549 | 0.451 | 0.811 | 0.787 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| DDFT [[30](#bib.bib30)] (fine-tuned) | 0.576 | 0.472 | 0.825 | 0.818 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: 4.1.3 Object Detection and Tracking
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Object Detection: Event cameras bring a new perspective in dealing with the
    challenges in object detection (e.g., motion blur, occlusions, and extreme lighting
    conditions). In reality, the RGB-based detection fails to enable robust perception
    under image corruptions or extreme weather conditions. Meanwhile, auxiliary sensors,
    such as LiDARs, are extremely bulky and costly [[137](#bib.bib137)]. Therefore,
    event-based detectors are introduced to overcome the dilemma, especially in challenging
    visual conditions [[133](#bib.bib133)]. In this work, we divide the event-based
    object detection methods into three categories according to the input data formats
    and data representations, as summarized in Tab. [VIII](#S4.T8 "TABLE VIII ‣ 4.1.1
    Object Classification ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D
    Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks").'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: The first category simply converts the raw event data into frame-based images [[22](#bib.bib22),
    [149](#bib.bib149), [131](#bib.bib131), [135](#bib.bib135), [136](#bib.bib136),
    [133](#bib.bib133), [132](#bib.bib132), [150](#bib.bib150)], e.g., the recurrent
    vision transformer (RVT) [[150](#bib.bib150)] which takes 2-channel frames within
    a time duration. However, this kind of method loses the raw spatial-temporal information
    in the event stream. For this reason, event volume and some other formats tailored
    for object detectors are used in the methods of the second category. Some works [[58](#bib.bib58),
    [23](#bib.bib23)] obtain event volumes by taking the linear or convolve kernels
    to integrate the asynchronous events into multiple slices within the equal temporal
    volume.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: However, the event volume still follows a frame-like 2D representation, and
    critical temporal information is lost. Recently, ASTMNet  [[134](#bib.bib134)]
    exploits the spatial-temporal information by directly processing the asynchronous
    events instead of the 2D frame-like representations. Furthermore, it also serves
    as the first end-to-end pipeline for continuous object detection. Notably, a branch
    of research has introduced temporal hints by integrating recurrent neural network
    layers, resulting in significant enhancements in detection accuracy  [[150](#bib.bib150),
    [23](#bib.bib23), [134](#bib.bib134)]. The third category of attempts combines
    the advantages of event images and RGB images [[135](#bib.bib135), [136](#bib.bib136),
    [151](#bib.bib151), [152](#bib.bib152)]. Tomy et al. [[137](#bib.bib137)] proposed
    a representative framework that fuses the information from the event- and frame-based
    cameras for better detection accuracy in normal conditions and robust performance
    in the presence of extreme scenarios.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Object Tracking: Tracking dynamic objects is an essential task in mobile robots,
    which requires the basic functionality of obstacle avoidance. RGB camera-based
    trackers perform poorly for high-speed and dynamic objects because of motion blur
    and time-delayed transmission. In such cases, introducing event cameras to address
    this problem is of great value.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: For mobile robots, earlier methods proposed to track moving objects on the conditions
    of geometric priors [[153](#bib.bib153)], known shape [[154](#bib.bib154)], [[154](#bib.bib154)]
    and motion-compensation model [[21](#bib.bib21)], [[155](#bib.bib155)]. More recently,
    many DL-based methods designed for canonical image data have undergone a paradigm
    shift and have been applied successfully to event data. For instance, the widely
    adopted object detector—Yolo and object tracker—Kalman filter, have been applied
    to event data, showing pleasant outcomes [[156](#bib.bib156)]. Many more endeavors
    have been made to enable the onboard inference ability of the deep learning models.
    For instance, EVDodge [[157](#bib.bib157)] specifically decouples the network
    to predict obstacle motion and ego-m otion separately by introducing two event
    cameras facing the ground and front, demonstrating that it outperforms their monocular
    counterparts which use a single event camera. Furthermore, EVReflex  [[158](#bib.bib158)]
    suggests an additional LiDARs sensor instead of an additional event camera exhibiting
    higher accuracy than EVDodge. EV-Catcher [[159](#bib.bib159)] trains a small CNN
    to process single-channel event images, achieving an inference speed of 2ms, which
    is faster than its predecessor by a large margin [[160](#bib.bib160)]. Because
    EV-Catcher only regresses the real-time target position and its uncertainty at
    x-coordinate from the DNNs, further estimation of hitting position and timing
    is based on the linear motion assumptions. Others tend to proceed with this problem
    in an end-to-end manner, showing marginal benefits.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE X: Experiments of representative methods on event-based optical flow
    estimation from  [[161](#bib.bib161)].'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'UL: unsupervised learning. SL: supervised learning. MB: model-based methods.
    ($\cdot$): evaluation on both outdoor_day1 and outdoor_day2 sequences. [$\cdot$]:
    evaluation on outdoor_day2 sequences. N/A means no results are available. The
    results without any brackets mean that they are not trained on any sequence of
    MVSEC.)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Method | Metric | indoor_flying1 | indoor_flying2 | indoor_flying3
    | outdoor_day1 | Param. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| EPE | %Out | EPE | %Out | EPE | %Out | EPE | %Out |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| UL | Ev-FlowNet [[34](#bib.bib34)] | sparse | (1.03) | (2.2) | (1.72) | (15.1)
    | (1.53) | (11.9) | [0.49] | [0.2] | N/A |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| Zhu et al. [[162](#bib.bib162)] | sparse | (0.58) | (0.0) | (1.02) | (4.0)
    | (0.87) | (3.0) | [0.32] | [0.0] | N/A |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| Matrix-LSTM [[40](#bib.bib40)] | sparse | (0.82) | (0.53) | (1.19) | (5.59)
    | (1.08) | (4.81) | N/A | N/A | N/A |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| Spike-FLowNet [[163](#bib.bib163)] | sparse | [0.84] | N/A | [1.28] | N/A
    | [1.11] | N/A | [0.49] | N/A | 13.039M |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| Paredes et al. [[92](#bib.bib92)] | sparse | (0.79) | (1.2) | (1.40) | (10.9)
    | (1.18) | (7.4) | [0.92] | [5.4] | N/A |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| LIF-EV-FlowNet [[164](#bib.bib164)] | sparse | 0.71 | 1.41 | 1.44 | 12.75
    | 1.16 | 9.11 | 0.53 | 0.33 | N/A |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| Deng et al. [[165](#bib.bib165)] | sparse | (0.89) | (0.66) | (1.31) | (6.44)
    | (1.13) | (3.53) | N/A | N/A | N/A |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[166](#bib.bib166)] | sparse | (0.59) | (0.83) | (0.64) | (2.26)
    | N/A | N/A | [0.31] | [0.03] | N/A |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| STE-FlowNet [[167](#bib.bib167)] | sparse | [0.57] | [0.1] | [0.79] | [1.6]
    | [0.72] | [1.3] | [0.42] | [0.0] | N/A |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| SL | Stoffregen et al. [[91](#bib.bib91)] | dense | 0.56 | 1.00 | 0.66 |
    1.00 | 0.59 | 1.00 | 0.68 | 0.99 | N/A |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| EST [[39](#bib.bib39)] | sparse | (0.97) | (0.91) | (1.38) | (8.20) | (1.43)
    | (6.47) | N/A | N/A | N/A |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| DCEIFlow [[161](#bib.bib161)] | dense | 0.56 | 0.28 | 0.64 | 0.16 | 0.57
    | 0.12 | 0.91 | 0.71 | N/A |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| DCEIFlow [[161](#bib.bib161)] | sparse | 0.57 | 0.30 | 0.70 | 0.30 | 0.58
    | 0.15 | 0.74 | 0.29 | N/A |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| MB | Pan et al. [[168](#bib.bib168)] | sparse | 0.93 | 0.48 | 0.93 | 0.48
    | 0.93 | 0.48 | 0.93 | 0.48 | N/A |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| Shiba [[169](#bib.bib169)] | sparse | 0.42 | 0.10 | 0.60 | 0.59 | 0.50 |
    0.28 | 0.30 | 0.10 | N/A |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| $\text{Fusion-FlowNet}_{Early}$ [[170](#bib.bib170)] | dense | (0.56) | N/A
    | (0.95) | N/A | (0.76) | N/A | [0.59] | N/A | 12.269M |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| $\text{Fusion-FlowNet}_{Late}$ [[170](#bib.bib170)] | sparse | (0.57) | N/A
    | (0.99) | N/A | (0.79) | N/A | [0.55] | N/A | 7.549M | ![Refer to caption](img/221954002d9b13c0853cdc1b71b8b042.png)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Visualization results of semantic segmentation with events, (a) Events,
    (b) Pseudo Label, (c) Ev-Transfer [[171](#bib.bib171)], (d) Image, (e) ESS [[172](#bib.bib172)],
    (f) E2VID [[90](#bib.bib90)].'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Overall, it is worth exploring how to build effective object detectors
    that leverage event cameras to overcome the deficiency of frame-based detectors
    in extreme visual conditions, e.g., high-speed motion or dark night. Meanwhile,
    it is promising to apply the abundant off-the-shelf DL-based object detectors,
    e.g., recurrent vision transformer [[150](#bib.bib150)], to event cameras. Moreover,
    though event cameras show distinct advantages, RGB-based cameras are still occupied
    by the mainstream object detection tasks. Intuitively, fusing the event and frame
    data could help improve detection accuracy, especially in scenarios where the
    events are temporarily absent, e.g., the static motion scenarios. In object detection
    and tracking, more and more attention is paid to multi-modal sensor fusion. Moreover,
    pure event-based solutions could be explored to exonerate the additional expensive
    sensors, like LiDARs, while achieving comparable or better performance.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Semantic Segmentation
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Image segmentation [[173](#bib.bib173)] is a fundamental vision task with many
    pivotal applications [[174](#bib.bib174), [175](#bib.bib175), [176](#bib.bib176)],
    including robotic perception, scene understanding, augmented reality, etc. In
    these practical scenarios, the segmentation models always fail in the non-ideal
    weather and lighting conditions [[4](#bib.bib4)], leading to poor scene perception
    of intelligent systems. Event-based semantic segmentation, which is first proposed
    in Ev-SegNet [[24](#bib.bib24)], achieves a significant improvement by utilizing
    the asynchronous event data. Ev-SegNet also introduces a dataset extended from
    the DDD17 dataset [[177](#bib.bib177)]. However, the resolution and image quality
    is less satisfactory for the semantic segmentation task.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: To address this problem, Gehrig et al. [[25](#bib.bib25)] proposed to convert
    video data to synthetic events. This work unlocks the usage of a large number
    of existing video datasets for event-based semantic segmentation. Inspired by
    this synthetic data source, Wang et al. [[178](#bib.bib178), [179](#bib.bib179)]
    suggested combining the labeled RGB data and unlabeled event data in a cross-modal
    knowledge distillation setting, so as to alleviate the shortage of labeled real
    event data. For higher segmentation results, Zhang et al. [[4](#bib.bib4)] constructed
    a multi-modal segmentation benchmark model by using the complementary information
    in both event and RGB branches. More recently, ESS [[172](#bib.bib172)] proposes
    an unsupervised domain adaptation (UDA) framework that leverages the still images
    without paired events and frames.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Due to the lack of precisely annotated large-scale real-world event
    datasets, existing works mostly focus on generating pseudo labels. However, the
    labels are not precise enough, rendering the learned segmentation models less
    robust, as demonstrated by the visual results in Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.3
    Object Detection and Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks"). Future work could further explore the multi-modal domain adaption
    from RGB data to event data for semantic segmentation.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 Optical Flow Estimation
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Insight: Optical flow estimation is the process of estimating the motion field
    within an image sequence. Conventional RGB-based methods remain unsatisfying in
    extreme lighting conditions, e.g., at night and in high-speed motion. To overcome
    these limitations, event cameras have been introduced. The SOTA event-based methods
    for optical flow estimation can be classified into two categories: traditional
    methods and DL-based methods. DL-based methods further encompass supervised and
    unsupervised approaches. Table. [X](#S4.T10 "TABLE X ‣ 4.1.3 Object Detection
    and Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D Vision ‣
    Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    presents the results achieved by several representative methods in the field.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/93a78497cba654a26a4ab97183c52846.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Representative optical flow estimation methods, including supervised
    methods [[39](#bib.bib39), [44](#bib.bib44), [33](#bib.bib33)], (a) Correlation-based
    methods, (b) Multi-task learning methods, and unsupervised learning methods [[162](#bib.bib162),
    [51](#bib.bib51)] (c) Multi-task learning methods, (d) self-supervised learning
    methods, and (e) SNN-based methods.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional methods: Recent research has delved into understanding the principles
    and characteristics of event data that facilitate the estimation process. These
    studies have particularly focused on leveraging contrast maximization methods
    to estimate optical flow accurately [[180](#bib.bib180), [181](#bib.bib181), [169](#bib.bib169)].
    Furthermore, there is ongoing research aimed at designing innovative event camera
    platforms specifically tailored for the hardware implementation of adaptive block-matching
    optical flow. These platforms serve as practical demonstrations of the effectiveness
    of this approach [[61](#bib.bib61)].'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised methods: In  [[39](#bib.bib39), [44](#bib.bib44), [33](#bib.bib33)],
    event streams are first converted into image-based or surface-based representations
    and then trained via standard convolutional neural networks (CNNs). Kepple et
    al.[[182](#bib.bib182)] proposed to simultaneously generate the region’s local
    flow and the reliability of the prediction. Gehrig et al.[[114](#bib.bib114)]
    proposed an RNN-based framework that utilizes the cost volumes and learns the
    feature correlation of the volumetric voxel grid of events, so as to estimate
    optical ﬂow.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Some research employs SNNs for optical flow estimation [[183](#bib.bib183),
    [184](#bib.bib184), [163](#bib.bib163)]. For example, Haessig et al. [[183](#bib.bib183)]
    introduced an SNN variant of the Barlow and Levick model utilizing IBM’s TrueNorth
    Neurosynaptic System. However, deep SNNs suffer from spike vanishing problems.
    To this end, [[163](#bib.bib163)] combined SNNs and CNNs in an end-to-end manner
    to estimate optical ﬂow, while [[184](#bib.bib184)] proposed a hierarchical SNN
    architecture for feature extraction and local and global motion perception. Future
    research could explore combining SNNs and transformer [[152](#bib.bib152)] to
    learn the global and local visual information from events.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised methods: Recent research is focused on unsupervised learning for
    solving the data scarcity problem. Zhu et al. [[162](#bib.bib162)] introduced
    a novel event representation containing two channels for encoding the number of
    positive and negative events and two for the timestamp of the most recent positive
    and negative events. They utilized the grayscale, i.e., APS, images of the event
    camera as the self-supervision signals to train the network. Ye et al. [[51](#bib.bib51)]
    simultaneously predicted the dense depth and optical flow with two corresponding
    neural networks. With the guidance of depth maps, the optical flow is calculated
    based on the poses of neighboring frames and the depth of the middle frame. However,
    these methods are still based on the photo consistency principle, while this assumption
    may not be valid in some adverse visual conditions (e.g., high-speed motion).
    To this end, Zhu et al.[[162](#bib.bib162)] proposed a discretized volumetric
    event representation to maintain the events’ temporal distribution, and the input
    processed event data is used to predict the motions and remove the motion blur.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: While optical flow may not have standalone usefulness, it serves as
    a valuable tool in driving other computations or closed-loop control in various
    computer vision tasks. Its ability to estimate motion patterns provides crucial
    information for tasks like object tracking, visual odometry, video stabilization,
    action recognition, and motion-based segmentation.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.6 Depth estimation
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bcbc59735cbdb2cc7de3fbd38e920980.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Event-based depth estimation methods, including (a) Monocular event
    only method [[185](#bib.bib185)], (b) Monocular event-frame based method  [[186](#bib.bib186)],
    (c) Stereo event only method [[187](#bib.bib187), [188](#bib.bib188)], (d) Stereo
    event-frame based method  [[189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)].'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Insight : Events streams reflect abundant edge information, HDR, and high temporal
    resolution, benefiting depth estimation tasks, especially in extreme conditions.
    Depth can be learned from either the monocular (single) input or stereo (multi-view
    of a scene) inputs. Under this outline, we categorize the depth estimation methods
    based on how events are used and learned.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Monocular depth estimation: Based on whether events are used alone or combined
    with the intensity frames, we divide the monocular depth estimation methods into
    two types. 1) Events-only approaches: [[185](#bib.bib185)] is a representative
    approach that adopts a recurrent network [[64](#bib.bib64)] to learn the temporal
    information from grid-like event inputs, depicted in Fig. [10](#S4.F10 "Figure
    10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") (a). However, as monocular depth estimation from events is an ill-posed
    problem, rendering such a learning framework difficult to achieve highly precise
    depth. Moreover, this learning paradigm may fail to predict depth in the static
    scene as events are only triggered by motion. 2) event-plus-frame approaches:
    RAM [[186](#bib.bib186)] employs the same RNN as [[185](#bib.bib185)] but combines
    events and frames (i.e., as complementary to each other) to learn to predict depth
    from the multi-modal inputs asynchronously, as shown in Fig. [10](#S4.F10 "Figure
    10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") (b). Nonetheless, the recurrent network is inevitably accompanied
    by long-term memory costs.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Stereo depth estimation: As the visual cues of the left and right event cameras
    are used in the stereo setting, the model complexity and memory cost of learning
    pipelines become more prohibitive. [[187](#bib.bib187), [188](#bib.bib188)] are
    two pioneering works in stereo depth estimation, as shown in Fig. [10](#S4.F10
    "Figure 10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") (c). In particular, DDES [[187](#bib.bib187)] is the first learning-based
    stereo-matching method, and in  [[188](#bib.bib188)], the first unsupervised learning
    framework is proposed. Both methods store events at each position as a First-in
    First-out queue, enabling concurrent time and polarity reservation. To adaptively
    extract features from sparse data, Zhang et al. [[192](#bib.bib192)] proposed
    continuous time convolution and discrete time convolution to encode high dimensional
    spatial-temporal event data.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast, some research explores multi-modality fusion under different settings
    which serves as a remedy to utilize the benefits of each modality. HDES [[193](#bib.bib193)]
    mitigates the modal-gap between the data from different viewpoints by introducing
    a hybrid pyramid attention module for multi-modal data fusion. EIS [[189](#bib.bib189)]
    is a representative work to combine events and frames with a recycling network,
    as depicted in Fig. [10](#S4.F10 "Figure 10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene
    Understanding ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks") (d). However, since events are
    sparse, event stacking is an important factor that affects the quality of fusion
    and depth prediction because stacking inappropriate amounts of events can lead
    to information overriding or missing problems. To this end,  [[190](#bib.bib190),
    [191](#bib.bib191)] propose a selection module to filter more useful events. Specifically,
    Nam et al. [[190](#bib.bib190)] concatenated the event stacks with different densities
    and then adaptively learn these stacks to highlight the contribution of the well-stacked
    events. Moreover, considering the constant motion of the cameras, SCSNet [[191](#bib.bib191)]
    introduces a differentiable event selection network to extract more reliable events
    and correlate the feature from a neighbor region of events and images, diminishing
    the disruption of bad alignment intuitively.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: From our review, inter-camera spatial correlation is the key to content
    matching between events and frames. Although [[189](#bib.bib189), [190](#bib.bib190),
    [191](#bib.bib191)] combine events and frames, it still deserves exploring which
    part events contribute most to the multi-modal feature fusion and alignment. Also,
    it is possible to use an event camera and a frame-based camera for stereo depth
    estimation. Future research could consider exploring these directions.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 3D Vision
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1 Visual SLAM
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Insight: It is an essential module for various applications, e.g., robotic
    navigation and virtual reality. Visual SLAM receives the signals, e.g., 2D images,
    as the source for ego-motion estimation and builds 3D maps, which can generally
    be defined as the tracking thread and the mapping thread. Event-based visual SLAM
    shares a similar spirit and benefits from the robustness of event cameras to light-changing
    and fast-moving conditions.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4cc32294b8a129e05a3b2f11a3c472af.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: (Left) Illustration of the general framework for event-based SLAM
    via deep learning, key elements are retrieved from [[194](#bib.bib194), [195](#bib.bib195),
    [51](#bib.bib51), [162](#bib.bib162)]. The colored point cloud is a map reconstructed
    from event data of different sensor views (green trajectory) grounded by robot
    location (red directional trajectory). (Right) Visualization of 3D reconstruction
    from (a) blurred RGB images and (b) events from  [[196](#bib.bib196)].'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: In traditional SLAM, depth and ego-motion could be easily estimated through
    triangulation and the subsequent local pose estimation by, e.g., Perspective-N-Points.
    Recently, the learning-based approaches are also been applied in SLAM, such as,
    [[51](#bib.bib51), [162](#bib.bib162)] propose united frameworks—with an encoder-decoder
    structure—for optical flow, depth, and ego-motion estimation.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: In particular, in [[51](#bib.bib51)], a united framework is proposed to estimate
    sparse optical flow, depths, and ego-motion, in which an encoder-decoder structure
    was adopted for sparse depth estimation, The key idea behind this framework is
    that the maximization of event frame contrast through optical flow warping provides
    natural, high-quality edge maps and enables applying a multi-view stereo loss
    to learn metric poses and depth. Two sub-networks under this framework are trained
    for the prediction of optical flow and depths, respectively.By contrast, Zhu et
    al.[[162](#bib.bib162)] proposed to directly learn 6-DOF poses from multi-view
    intensity frames, other than deriving from optical flow and depth, like [[51](#bib.bib51)].
    The recent EAGAN [[197](#bib.bib197)] adopts the vision transformer  [[198](#bib.bib198)]
    to boost the accuracy of optical flow estimation, yet no optimization is done
    to depth estimation. However, EAGAN shows an increase in the number of learnable
    parameters compared to the methods [[162](#bib.bib162), [51](#bib.bib51)], and
    the ego-motion estimation is not considered.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, a branch of research casts event-based SLAM as a re-localization problem.
    This paradigm proposes to directly learn the camera poses from extracted deep
    features in an end-to-end trainable manner. For example, Nguyen et al.[[199](#bib.bib199)]
    proposed a framework, with CNNs and four LSTM blocks after fully-connected layers
    of PoseNet [[200](#bib.bib200)], to regress the 6-DOF poses from event images
    directly. This, for the first time, reveals the potential of event data to address
    the large-scale re-localization problem. Later on, additional denoising modules
    are introduced in [[201](#bib.bib201)] to further increase the pose estimation
    accuracy.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Currently, the event-based SLAM systems employing deep learning are
    generally decoupled as separate modules rather than the cross-event frame and
    pose-map joint estimation as traditionally processed in visual SLAM. Some attempts
    [[162](#bib.bib162), [51](#bib.bib51)] generate frame association (as a form of
    optical flow), pose estimation, and depth map with a unified network architecture.
    Still, global consistency remains an unsolved problem. For the front end, applying
    DNNs for the intra-frame association purpose is a challenging problem. This leaves
    a vacancy for exploration since many frame-based SLAM systems have proved that
    the pose derived by cross-frame feature association is highly precise. Another
    interesting direction to investigate is the dense map or mesh reconstruction from
    event data that served as mapping for SLAM. we have seen mesh reconstruction with
    very limited precision and designed for small-size objects in Sec.[XI](#S4.T11
    "TABLE XI ‣ 4.2.2 3D Reconstruction ‣ 4.2 3D Vision ‣ 4 Scene Understanding and
    3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    leaving learning-based scene reconstruction an open problem.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 3D Reconstruction
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'TABLE XI: Comparison of existing representative event-based 3D reconstruction
    methods.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '| Publications | Methods | Task | Representations | Frame Input | Real Time
    | Multi Cameras |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| Arxiv 2020 | E3D [[196](#bib.bib196)] | 3D Reconstruction | Image-based |
    ✓ | ✗ | ✗ |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2020 | Stereo-event PTV [[202](#bib.bib202)] | 3D Fluid Flow Reconstruction
    | Stream-based | ✗ | ✗ | ✓ |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2021 | EventHands [[203](#bib.bib203)] | 3D Hand Pose Estimation | Surface-based
    | ✗ | ✓ | ✗ |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2022 | EvAC3D [[204](#bib.bib204)] | 3D Reconstruction | Surface-based
    | ✗ | ✗ | ✗ |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: 'Insight: Event cameras capture dominant scene features, e.g., edges and silhouettes,
    making them more suitable for some 3D reconstruction methods than the frame-based
    data (See Fig. [11](#S4.F11 "Figure 11 ‣ 4.2.1 Visual SLAM ‣ 4.2 3D Vision ‣ 4
    Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks")).'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the traditional frame-based RGB and depth cameras widely explored in
    3D reconstruction [[205](#bib.bib205)], event cameras enjoy inherent benefits,
    such as low latency and HDR [[206](#bib.bib206), [207](#bib.bib207)]. Intuitively,
    approaches designed for prior RGB and depth data can not be directly applied to
    event data due to the distinct data format. Thus, endeavors have been made in
    converting events to sparse, semi-dense point clouds and full-frame depth maps
    for existing RGB-based pipelines [[202](#bib.bib202), [208](#bib.bib208), [209](#bib.bib209),
    [206](#bib.bib206), [186](#bib.bib186)]. These methods are committed to take advantages
    of the off-the-shelf 3D reconstruction pipelines built for RGB-based inputs while
    ignoring the unique strengths of event cameras. In some following research [[14](#bib.bib14),
    [196](#bib.bib196)], event cameras are combined with RGB cameras for the advantages
    of both sensors. For instance, Vidal et al. [[14](#bib.bib14)] proposed to simultaneously
    incorporate event data, intensity images, and inertial measurement unit (IMU)
    data in the SLAM pipeline for achieving superior accuracy than the purely event-based
    methods. Besides, some approaches combine event cameras with other types of sensors,
    such as ELS [[210](#bib.bib210)] that uses a laser point-projector and an event
    camera.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'To directly take advantage of the event data, Wang et al. [[211](#bib.bib211)]
    proposed to generate dimensional flow from the events instead of using the image-based
    reconstruction method [[212](#bib.bib212)] for 3D fluid flow reconstruction. More
    recently, EvAC3D [[204](#bib.bib204)] explores the direct reconstruction of mesh
    from a continuous stream of events while defining the boundaries of the objects
    as apparent contour events and continuously carving out high-fidelity meshes.
    The comparison of existing representative event-based 3D reconstruction methods
    is shown in Tab. [XI](#S4.T11 "TABLE XI ‣ 4.2.2 3D Reconstruction ‣ 4.2 3D Vision
    ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks").'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: Recent research provides important insights regarding how events can
    be utilized to understand the 3D world. However, a general and unified pipeline
    is expected, which is left for future research. The fusion of the RGB and event
    cameras is also valuable for achieving better reconstruction results.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 3D Human Pose and Shape Estimation
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Insight: The ability to capture the dynamic motion makes event cameras superior
    for estimating 3D moving objects, especially for 3D human pose and shape estimation
    (3D HPE) (see Fig. [12](#S4.F12 "Figure 12 ‣ 4.2.3 3D Human Pose and Shape Estimation
    ‣ 4.2 3D Vision ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks")).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: In the past few years, 3D HPE has been extensively explored with RGB images
    and videos in the deep learning era [[213](#bib.bib213), [214](#bib.bib214)].
    The most challenging scenario in 3D HPE is always related to the high-speed motion [[215](#bib.bib215)],
    which is essential in many practical applications, such as sports performance
    evaluation. However, the RGB cameras suffer inevitable fundamental problems [[216](#bib.bib216)],
    including unsatisfactory frame rates and data redundancy. By contrast, event cameras
    are more advisable for fast-motion scenarios.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: DHP19 [[217](#bib.bib217)] is the first DL-based pipeline and provides the first
    dataset for event-based 3D HPE. To utilize the frame-based DL algorithms, the
    event streams are transferred to DVS frames by accumulating a fixed number of
    events in  [[217](#bib.bib217)]. Meanwhile, the DHP19 takes events captured by
    multiple calibrated cameras. More recently, EventCap [[216](#bib.bib216)] is the
    first work to capture high-speed human motions from a single event camera with
    the guidance of gray-scale images. To alleviate the reliance on frame inputs,
    the following research EventHPE [[218](#bib.bib218)], proposes to infer 3D HPE
    from the sole source of event input, given the beginning shape from the first
    frame of the intensity image stream.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/09a7f8c0526c36770d00e5cf7c6e7635.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Qualitative comparison between 3D HPE methods from  [[218](#bib.bib218)].
    (a) VIBE [[215](#bib.bib215)]; (b) EventCap [[216](#bib.bib216)], (c) EventHPE [[218](#bib.bib218)]'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks: A limitation of these approaches is that they need gray-scale images
    for initialization. For future research, it is worth investigating how to infer
    3D HPE purely from event signals without additional priors. Meanwhile, it also
    promises to combine the advantages of both RGB and event sensors and design multi-modal
    learning frameworks for more robust 3D HPE.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 5 Research Trend and Discussions
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Discussions
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Do we really need deep networks for learning events? Motivated by the widespread
    applications and success of DL methods in computer vision using frame-based imagery,
    there is a growing research focus on applying DL to event data. DL-based approaches
    offer significant accuracy improvements compared to conventional event-based vision
    algorithms. For instance, a recent DL-based feature tracker [[30](#bib.bib30)]
    outperforms non-DL methods. Additionally, DL-based methods exhibit superior results
    in 3D HPE compared to multi-view models, showcasing the effectiveness of DL techniques [[217](#bib.bib217)].
    However, the adoption of DL techniques introduces certain challenges. For example,
    SNNs [[43](#bib.bib43), [33](#bib.bib33), [68](#bib.bib68)] naturally accommodate
    event streams and enable asynchronous inference at low computational cost. Nevertheless,
    training SNNs is difficult, and the supporting hardware infrastructure is not
    well-established, limiting their applications in the computer vision community.
    Recently, DNNs have been introduced to address tasks in event-based vision [[39](#bib.bib39)].
    However, existing methods overlook the unique characteristics of event data and
    primarily aim to bridge the domain gaps between RGB and event-based vision. Further
    exploration is needed in the conversion between raw event data and grid-based
    inputs (e.g., various event representations). Moreover, an urgent challenge is
    to determine the suitable learning pipeline for utilizing raw events with DNNs.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Do we really need convolution operations to filter events as done for image
    data? As canonical convolutions only operate through the spatial perspective,
    simply applying the 2D convolutional modules to event streams neglects the temporal
    correlation of events, leading to a sub-optimal network design. Intuitively, we
    suggest 1) using a graph to describe event streams [[53](#bib.bib53)]; 2) introducing
    the self-attention-based transformer for temporal information [[219](#bib.bib219),
    [220](#bib.bib220)]; 3) applying recurrent network [[221](#bib.bib221)].
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Low latency of event cameras vs. High computation of DNNs: One of the notable
    superiorities of event cameras is the low latency which enables real-time applications
    of event cameras. However, the computational complexity of the neural network
    is typically enormous, potentially negating the benefits of lower event latency.
    An important research question is how to accelerate the neural network in the
    field of events. We suggest that three angles be taken when conducting this issue.
    1) Make use of a light-weight network architecture, like MobileNet[[222](#bib.bib222),
    [223](#bib.bib223)] Shuffle Net[[224](#bib.bib224), [225](#bib.bib225)]. 2) Make
    use of network quantization and compression techniques[[226](#bib.bib226)]. 3)
    Construct a network based on SNN[[227](#bib.bib227)], which has low delay and
    sparsity.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'How to better deal with noisy events with DNNs? Random noise can be introduced
    throughout the trigger process of events due to many reasons. Thus the denoising
    procedure is necessary for accurate information capture. According to the existing
    denoising methods in Sec. [2.2.2](#S2.SS2.SSS2 "2.2.2 Event Denoising ‣ 2.2 Quality
    Enhancement for Events ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"), we suggest dealing with the noisy
    events with DNNs in three steps: 1) Formulate the spatial and temporal distributions
    of raw events; 2) Separately denoising from both spatial and temporal perspectives,
    e.g., operations in spatial neighbourhoods and current event surfaces; 3) Further
    consider the correlation between spatial and temporal distributions, aiming at
    maintaining spatial-temporal correlation.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Can the high temporal resolution of events be fully reflected by DNNs? No matter
    what kind of event representation, summarized in Sec. [2.1](#S2.SS1 "2.1 Event
    Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks"), is used as DNN’s input, the grid-like
    tensors always lose some partial information of raw events, such as temporal information
    in the image-based representations [[33](#bib.bib33), [34](#bib.bib34), [39](#bib.bib39),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]. Obviously, the high temporal
    resolution of events cannot be fully reflected by the existing DNNs which are
    proposed to solve frame-based vision. The micro-second level temporal resolutions
    are predominantly fused to frame-like representations for the downstream tasks.
    SNNs can solve this problem according to their architectures, however, there are
    still technical difficulties in bringing SNNs into practical applications. Thus
    specific neural networks directly designed for event data are in the future outlook.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Do events contain sufficient visual information for learning robust DNN models?
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Less plausible accuracy of event-based vision models in the dark. Low light
    conditions bring noise mainly due to the sufficient intensity changes that do
    not generate an event. The noises can be briefly called ”holes” or false negatives.
    This leads to unreliable scene understanding of event cameras. Employing DL for
    noise cancellation of event data is a promising approach [[71](#bib.bib71)]. In
    addition, multi-sensor fusion is a valuable direction. For example, thermal sensors
    have been widely used in the dark [[228](#bib.bib228)]. Therefore, developing
    DL-based methods that take thermal and event data as inputs is worth exploring.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Are DL-based methods more advantageous than optimized-based ones? Optimization-based
    methods are more applicable to edge computing  [[229](#bib.bib229)] but are still
    unlikely to reach global optima—more likely to be trapped in saddle points  [[230](#bib.bib230)]
    ). By contrast, DNNs are superior in that they can flexibly learn the multi-dimension
    data and extract better feature representations. Intuitively, DNNs have the potential
    to learn spatial-temporal information from events. Superior results has been demonstrated
    in exploring the temporal correlation of events with various DNNs in the literature [[219](#bib.bib219),
    [220](#bib.bib220), [221](#bib.bib221)]. Also, owning to the numerical stability
    and efficiency, DNNs allow hidden state encoding for effective prediction in various
    tasks. For example, in the context of event-based SLAM, implicit encoding of event
    streams exhibits higher spatial-temporal consistency than naively tracking fired
    pixels, enabling estimation of ego-motion and 3D scene reconstruction jointly,
    whereas the optimization-based method shows significantly lower accuracy.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Is focal alignment necessary between RGB and event pixels in event cameras?
    Event cameras have emerged as an efficient alternative for capturing motion information.
    Recent studies highlight the potential of DNNs in leveraging both RGB and event
    data to enhance images, enabling crucial tasks like deblurring and video frame
    interpolation. This capability holds significant practical applications in areas
    such as augmented reality and virtual reality. However, achieving the desired
    image quality mandates RGB characteristics comparable to advanced mobile RGB sensors,
    along with precise focal alignment between RGB and event pixels on the sensor.
    To address these challenges, one promising approach involves the development of
    hybrid-type sensors that seamlessly integrate high-frame-rate event pixels with
    advanced mobile RGB pixels [[231](#bib.bib231), [232](#bib.bib232), [233](#bib.bib233)].
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 New Directions
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NeRF for Event-based Neural Rendering: NeRF  [[234](#bib.bib234)] is a representative
    neural implicit 3D representation method that synthesizes 3D objects with volume
    rendering techniques. Most existing research in NeRF is investigated based on
    RGB cameras, which suffer from inevitable shortcomings, e.g., low dynamic range
    and motion blur in unfavorable lighting conditions. Thus, recent attention has
    been paid to the usage of event cameras for NeRF [[235](#bib.bib235), [236](#bib.bib236),
    [237](#bib.bib237)]. The first work is EventNeRF [[235](#bib.bib235)], which is
    trained with pure event-based supervision. It demonstrates that the NeRF estimation
    from a single fast-moving event camera in unfavourable scenarios (e.g., fast-moving
    objects, motion blur, or insufficient lighting) is feasible while frame-based
    approaches fail. Moreover, E-NeRF [[236](#bib.bib236)] takes the strengths of
    RGB and event cameras by combining color frames and events to achieve sharp and
    colorize reconstruction results.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: From the review of recent progress, event-based NeRF methods show superior abilities
    than the frame-based NeRF methods. Since the event-based NeRF is an emerging direction,
    future research could focus on improving the technique pipelines and more lightweight
    DNNs for mobile applications.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi- and Cross-modal Learning for Event-based Vision: In practical scenarios,
    event cameras always play an auxiliary role to provide multi-modal guidance in
    many aforementioned computer vision tasks, e.g., image and video SR [[112](#bib.bib112),
    [111](#bib.bib111), [108](#bib.bib108)]. With the development of event cameras,
    event-based vision will occupy an increasingly dominant position in both research
    and industry. Especially in some specific domains, there are already attempts
    to leverage pure event data to facilitate task accuracy, such as reconstructing
    RGB images and videos from pure event data [[17](#bib.bib17), [64](#bib.bib64),
    [102](#bib.bib102), [90](#bib.bib90)], etc. Consequently, how to utilize the domain
    knowledge from the frame-based vision to the emerging event-based vision deserves
    more intensive research. Recently, CTN [[238](#bib.bib238)] is proposed as an
    early attempt of transferring knowledge from frame-based vision to event-based
    vision.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Event-based model Pretaining: Pre-trained neural networks are the foundations
    of almost all downstream task models in the deep learning era. For the frame-based
    vision, the pre-trained weights on ImageNet are widely utilized for models’ outstanding
    accuracy gains. With the growing interest in event-based vision, a wide range
    of datasets are collected in many downstream tasks. Thus, unified pre-trained
    weights with large-scale data are required for better accuracy. Since the event
    data is a totally distinct format containing spatial-temporal information, a reliable
    and efficient pre-trained method is required for various applications. Recently,
    in  [[239](#bib.bib239)], Yang et al. proposed the first pre-training pipeline
    for dealing with event camera data, including various event data augmentations,
    a masking sample strategy, and a contrastive learning approach. Additionally,
    Hu et al. [[58](#bib.bib58)]  proposed the Network Grafting Algorithm, which replaces
    the front-end network of a pre-trained DNN with a new network that processes unconventional
    visual inputs. Through self-supervised training using synchronously-recorded intensity
    frames and novel sensor data, the algorithm maximizes feature similarity between
    the pretrained network and the grafted network. This enhances the pretrained network’s
    ability to process and extract meaningful features from diverse input sources.
    Future research can focus on how to fully utilize all kinds of event representations,
    summarized in Sec. [2.1](#S2.SS1 "2.1 Event Representation ‣ 2 Event Processing
    for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    to realize better event-based pre-training. More details of new directions can
    be found in the supplementary material.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] G. Gallego, T. Delbrück, G. Orchard, C. Bartolozzi, B. Taba, A. Censi,
    S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis *et al.*, “Event-based
    vision: A survey,” *TPAMI*, 2020.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] M. Davies, A. Wild, G. Orchard, Y. Sandamirskaya, G. A. F. Guerra, P. Joshi,
    P. Plank, and S. R. Risbud, “Advancing neuromorphic computing with loihi: A survey
    of results and outlook,” *Proceedings of the IEEE*, 2021.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] R. Baldwin, R. Liu, M. M. Almatrafi, V. K. Asari, and K. Hirakawa, “Time-ordered
    recent event volumes for event cameras,” *TPAMI*, 2022.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Zhang, K. Yang, and R. Stiefelhagen, “ISSAFE: Improving semantic segmentation
    in accidents by fusing event-based data.”   IEEE, 2021.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, “EKLT: Asynchronous
    photometric feature tracking using events and frames,” *IJCV*, 2020.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] I. Alzugaray and M. Chli, “Asynchronous corner detection and tracking for
    event cameras in real time,” *RAL*, 2018.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Alzugaray, Ignacio and Chli, Margarita, “ACE: An efficient asynchronous
    corner tracker for event cameras,” in *3DV*.   IEEE, 2018.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] R. Li, D. Shi, Y. Zhang, K. Li, and R. Li, “FA-Harris: A Fast and Asynchronous
    Corner Detector for Event Cameras,” in *IROS*.   IEEE, 2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Glover, A. Dinale, L. D. S. Rosa, S. Bamford, and C. Bartolozzi, “LuvHarris:
    A Practical Corner Detector for Event-cameras,” *TPAMI*, 2021.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Henri, Rebecq, Timo, Horstschaefer, Guillermo, Gallego, Davide, and Scaramuzza,
    “EVO: A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping
    in Real Time,” *RAL*, 2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Hidalgo-Carrió, G. Gallego, and D. Scaramuzza, “Event-aided direct
    sparse odometry,” in *CVPR*, 2022.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] E. Mueggler, H. Rebecq, G. Gallego, T. Delbruck, and D. Scaramuzza, “The
    event-camera dataset and simulator: Event-based data for pose estimation, visual
    odometry, and SLAM,” *Int J Rob Res.*, 2017.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Milford, H. Kim, S. Leutenegger, and A. Davison, “Towards visual slam
    with event-based cameras,” in *The problem of mobile sensors workshop in conjunction
    with RSS*, 2015.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza, “Ultimate
    SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High
    Speed Scenarios,” *RAL*, 2018.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] J. Jiao, H. Huang, L. Li, Z. He, Y. Zhu, and M. Liu, “Comparing Representations
    in Tracking for Event Camera-based SLAM,” 2021.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Z. Chen, Q. Zheng, P. Niu, H. Tang, and G. Pan, “Indoor lighting estimation
    using an event camera,” in *CVPR*, 2021.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. M. M. Isfahani, L. Wang, Y.-S. Ho, and K. jin Yoon, “Event-Based High
    Dynamic Range Image and Very High Frame Rate Video Generation Using Conditional
    Generative Adversarial Networks,” *CVPR*, 2018.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] N. Messikommer, S. Georgoulis, D. Gehrig, S. Tulyakov, J. Erbach, A. Bochicchio,
    Y. Li, and D. Scaramuzza, “Multi-Bracket High Dynamic Range Imaging with Event
    Cameras,” in *CVPR*, 2022.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] C. Scheerlinck, H. Rebecq, D. Gehrig, N. Barnes, R. Mahony, and D. Scaramuzza,
    “Fast image reconstruction with an event camera,” in *WACV*, 2020.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. Mondal, J. H. Giraldo, T. Bouwmans, A. S. Chowdhury *et al.*, “Moving
    Object Detection for Event-based Vision using Graph Spectral Clustering,” in *ICCV*,
    2021.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] A. Mitrokhin, C. Fermüller, C. Parameshwara, and Y. Aloimonos, “Event-Based
    Moving Object Detection and Tracking,” in *IROS*.   IEEE, 2018.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] M. Iacono, S. Weber, A. Glover, and C. Bartolozzi, “Towards event-driven
    object detection with off-the-shelf deep learning,” in *IROS*, 2018.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] E. Perot, P. de Tournemire, D. Nitti, J. Masci, and A. Sironi, “Learning
    to detect objects with a 1 megapixel event camera,” *Adv Neural Inf Process Syst.*,
    2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] I. Alonso and A. C. Murillo, “EV-SegNet: Semantic segmentation for event-based
    cameras,” in *CVPRW*, 2019.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] D. Gehrig, M. Gehrig, J. Hidalgo-Carrió, and D. Scaramuzza, “Video to
    events: Recycling video datasets for event cameras,” in *CVPR*, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] R. Serrano-Gotarredona, M. Oster, P. Lichtsteiner, A. Linares-Barranco,
    R. Paz-Vicente, F. Gómez-Rodríguez, L. Camuñas-Mesa, R. Berner, M. Rivas-Pérez,
    T. Delbruck *et al.*, “CAVIAR: A 45k neuron, 5M synapse, 12G connects/s AER hardware
    sensory–processing–learning–actuating system for high-speed visual object recognition
    and tracking,” *IEEE Transactions on Neural networks*, 2009.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] T. Delbrück, B. Linares-Barranco, E. Culurciello, and C. Posch, “Activity-driven,
    event-based vision sensors,” in *Proceedings of 2010 IEEE International Symposium
    on Circuits and Systems*, 2010.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] P. Lichtsteiner, C. Posch, and T. Delbruck, “A 128× 128 120 dB 15 us Latency
    Asynchronous Temporal Contrast Vision Sensor,” *IEEE Journal of Solid-State Circuits*,
    2008.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] P. U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, and M. Pfeiffer, “Fast-classifying,
    high-accuracy spiking deep networks through weight and threshold balancing,” in
    *International joint conference on neural networks*.   IEEE, 2015.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] N. Messikommer, C. Fang, M. Gehrig, and D. Scaramuzza, “Data-driven feature
    tracking for event cameras,” in *CVPR*, 2023.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S.-C. Liu, B. Rueckauer, E. Ceolini, A. Huber, and T. Delbruck, “Event-driven
    sensing for efficient perception: Vision and audition algorithms,” *IEEE Signal
    Processing Magazine*, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. Lin, F. Xu, X. Wang, W. Yang, and L. Yu, “Efficient Spatial-Temporal
    Normalization of SAE Representation for Event Camera,” *RAL*, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. I. Maqueda, A. Loquercio, G. Gallego, N. García, and D. Scaramuzza,
    “Event-based vision meets deep learning on steering prediction for self-driving
    cars,” in *CVPR*, 2018.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, “EV-FlowNet: Self-supervised
    optical flow estimation for event-based cameras,” in *Robotics: Science and Systems*,
    2018.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Wang, B. Du, Y. Shen, K. Wu, G. Zhao, J. Sun, and H. Wen, “EV-Gait:
    Event-based robust gait recognition using dynamic vision sensors,” in *CVPR*,
    2019.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Y. Deng, Y. Li, and H. Chen, “AMAE: Adaptive Motion-Agnostic Encoder for
    Event-Based Object Classification,” *RAL*, 2020.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] W. Bai, Y. Chen, R. Feng, and Y. Zheng, “Accurate and Efficient Frame-based
    Event Representation for AER Object Recognition,” in *IJCNN*, 2022.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Deng, H. Chen, and Y. Li, “MVF-Net: A Multi-View Fusion Network for
    Event-Based Object Classification,” *TCSVT*, 2021.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] D. Gehrig, A. Loquercio, K. G. Derpanis, and D. Scaramuzza, “End-to-end
    learning of representations for asynchronous event-based data,” in *ICCV*, 2019.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, “A differentiable
    recurrent surface for asynchronous event-based data,” in *ECCV*.   Springer, 2020.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] P. K. Park, B. H. Cho, J. M. Park, K. Lee, H. Y. Kim, H. A. Kang, H. G.
    Lee, J. Woo, Y. Roh, W. J. Lee *et al.*, “Performance improvement of deep learning
    based gesture recognition using spatiotemporal demosaicing technique,” in *ICIP*.   IEEE,
    2016.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] R. Benosman, C. Clercq, X. Lagorce, S.-H. Ieng, and C. Bartolozzi, “Event-based
    visual flow,” *TNNLS*, 2013.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] X. Lagorce, F. Orchard, Garrick andc Galluppi, B. E. Shi, and R. B. Benosman,
    “Hots: a hierarchy of event-based time-surfaces for pattern recognition,” *TPAMI*,
    2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Sironi, M. Brambilla, N. Bourdis, X. Lagorce, and R. Benosman, “HATS:
    Histograms of averaged time surfaces for robust event-based object classification,”
    in *CVPR*, 2018.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] R. W. Baldwin, M. Almatrafi, J. R. Kaufman, V. Asari, and K. Hirakawa,
    “Inceptive event time-surfaces for object classification using neuromorphic cameras,”
    in *ICIAR*, 2019.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Manderscheid, A. Sironi, N. Bourdis, D. Migliore, and V. Lepetit, “Speed
    Invariant Time Surface for Learning to Detect Corner Points With Event-Based Cameras,”
    in *CVPR*, 2019.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Almatrafi, R. Baldwin, K. Aizawa, and K. Hirakawa, “Distance Surface
    for Event-Based Optical Flow,” *TPAMI*, 2020.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. Kim, J. Bae, G. Park, D. Zhang, and Y. M. Kim, “N-imagenet: Towards
    robust, fine-grained object recognition with event cameras,” in *ICCV*, 2021.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] A. Zihao Zhu, L. Yuan, K. Chaney, and K. Daniilidis, “Unsupervised event-based
    optical flow using motion compensation,” in *ECCVW*, 2018.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] F. Gu, W. Sng, T. Taunyazov, and H. Soh, “Tactilesgnet: A spiking graph
    neural network for event-based tactile object recognition,” in *IROS*, 2020.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C. Ye, A. Mitrokhin, C. Fermüller, J. A. Yorke, and Y. Aloimonos, “Unsupervised
    Learning of Dense Optical Flow, Depth and Egomotion with Event-Based Sensors,”
    in *IROS*, 2020.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-based
    spatio-temporal feature learning for neuromorphic vision sensing,” *TIP*, 2020.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Deng, H. Chen, H. Liu, and Y. Li, “A Voxel Graph CNN for Object Classification
    With Event Cameras,” in *CVPR*, 2022.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] J. H. Lee, T. Delbruck, and M. Pfeiffer, “Training deep spiking neural
    networks using backpropagation,” *FRONT NEUROSCI-SWITZ*, 2016.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J. Botzheim, T. Obo, and N. Kubota, “Human gesture recognition for robot
    partners by spiking neural network and classification learning,” in *SCIS*, 2012.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Amir, B. Taba, D. Berg, T. Melano, J. McKinstry, C. Di Nolfo, T. Nayak,
    A. Andreopoulos, G. Garreau, M. Mendoza *et al.*, “A low power, fully event-based
    gesture recognition system,” in *CVPR*, 2017.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] D. P. Moeys, F. Corradi, E. Kerr, P. Vance, G. Das, D. Neil, D. Kerr,
    and T. Delbrück, “Steering a predator robot using a mixed frame/event-driven convolutional
    neural network,” in *2016 Second international conference on event-based control,
    communication, and signal processing (EBCCSP)*.   IEEE, 2016.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Hu, T. Delbruck, and S.-C. Liu, “Learning to exploit multiple vision
    modalities by using grafted networks,” in *ECCV*.   Springer, 2020.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] N. Messikommer, D. Gehrig, A. Loquercio, and D. Scaramuzza, “Event-based
    asynchronous sparse convolutional networks,” in *ECCV*, 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] M. Liu and T. Delbruck, “Adaptive time-slice block-matching optical flow
    algorithm for dynamic vision sensors.”   BMVC, 2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Liu, Min and Delbruck, Tobi, “EDFLOW: Event driven optical flow camera
    with keypoint detection and adaptive block matching,” *IEEE Transactions on Circuits
    and Systems for Video Technology*, 2022.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] B. R. Pradhan, Y. Bethi, S. Narayanan, A. Chakraborty, and C. S. Thakur,
    “N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory
    Surfaces,” in *ISCAS*, 2019.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] S. Afshar, T. J. Hamilton, J. Tapson, A. Van Schaik, and G. Cohen, “Investigation
    of event-based surfaces for high-speed detection, unsupervised feature extraction,
    and object recognition,” *Frontiers in Neuroscience*, 2019.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, “Events-to-video:
    Bringing modern computer vision to event cameras,” in *CVPR*, 2019.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-based
    object classification for neuromorphic vision sensing,” in *ICCV*, 2019.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] G. Orchard, C. Meyer, R. Etienne-Cummings, C. Posch, N. Thakor, and R. Benosman,
    “HFirst: A temporal approach to object recognition,” *IEEE transactions on pattern
    analysis and machine intelligence*, 2015.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] B. Zhao, R. Ding, S. Chen, B. Linares-Barranco, and H. Tang, “Feedforward
    categorization on aer motion events using cortex-like features in a spiking neural
    network,” *TNNLS*, 2014.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] D. Neil, M. Pfeiffer, and S.-C. Liu, “Phased LSTM: Accelerating recurrent
    network training for long or event-based sequences,” *NeurIPS*, 2016.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] C. Brandli, L. Muller, and T. Delbruck, “Real-time, high-speed video decompression
    using a frame-and event-based davis sensor,” in *Proc. IEEE Int. Symp. Circuits
    Syst.*   IEEE, 2014.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] H. Li, G. Li, and L. Shi, “Super-resolution of spatiotemporal event-stream
    image,” *Neurocomputing*, 2019.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Z. W. Wang, P. Duan, O. Cossairt, A. Katsaggelos, T. Huang, and B. Shi,
    “Joint filtering of intensity images and neuromorphic events for high-resolution
    noise-robust imaging,” in *CVPR*, 2020.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] P. Duan, Z. W. Wang, X. Zhou, Y. Ma, and B. Shi, “Eventzoom: Learning
    to denoise and super resolve neuromorphic events,” in *CVPR*, 2021.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S. Li, Y. Feng, Y. Li, Y. Jiang, C. Zou, and Y. Gao, “Event stream super-resolution
    via spatiotemporal constraint learning,” in *ICCV*, 2021.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] W. Weng, Y. Zhang, and Z. Xiong, “Boosting event stream super-resolution
    with a recurrent neural network,” in *ECCV*, 2022.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] J. Barrios-Avilés, A. Rosado-Muñoz, L. D. Medus, M. Bataller-Mompeán,
    and J. F. Guerrero-Martínez, “Less data same information for event-based sensors:
    A bioinspired filtering and data reduction algorithm,” *Sensors*, 2018.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] A. Khodamoradi and R. Kastner, “$o(n)$-space spatiotemporal filter for
    reducing noise in neuromorphic vision sensors,” *TETC*, 2018.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] H. Liu, C. Brandli, C. Li, S.-C. Liu, and T. Delbruck, “Design of a spatiotemporal
    correlation filter for event-based sensors,” in *ISCAS*.   IEEE, 2015.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] D. Czech and G. Orchard, “Evaluating noise filtering for event-based asynchronous
    change detection image sensors,” in *BioRob*.   IEEE, 2016.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] T. Delbruck, “Frame-free dynamic digital vision,” in *Proceedings of Intl.
    Symp. on Secure-Life Electronics, Advanced Electronics for Quality Life and Society*,
    2008.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] S. Guo and T. Delbruck, “Low cost and latency event camera background
    activity denoising,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2022.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] R. Baldwin, M. Almatrafi, V. Asari, and K. Hirakawa, “Event probability
    mask (epm) and event denoising convolutional neural network (edncnn) for neuromorphic
    cameras,” in *CVPR*, 2020.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] H. Fang, J. Wu, L. Li, J. Hou, W. Dong, and G. Shi, “Aednet: Asynchronous
    event denoising with spatial-temporal correlation among irregular data,” in *ACM
    MM*, 2022.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Lin, Y. Ma, Z. Guo, and B. Wen, “Dvs-voltmeter: Stochastic process-based
    event simulator for dynamic vision sensors,” in *ECCV*.   Springer, 2022.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. Hu, S.-C. Liu, and T. Delbruck, “v2e: From video frames to realistic
    dvs events,” in *CVPR*, 2021.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] H. Rebecq, D. Gehrig, and D. Scaramuzza, “Esim: an open event camera simulator,”
    in *CoRL*, 2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] H. Kim, A. Handa, R. Benosman, S.-H. Ieng, and A. J. Davison, “Simultaneous
    mosaicing and tracking with an event camera,” *J. Solid State Circ*, 2008.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] M. Cook, L. Gugelmann, F. Jug, C. Krautz, and A. Steger, “Interacting
    maps for fast visual interpretation,” in *IJCNN*, 2011.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] G. Munda, C. Reinbacher, and T. Pock, “Real-time intensity-image reconstruction
    for event cameras using manifold regularisation,” *IJCV*, 2018.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Z. Zhang, A. Yezzi, and G. Gallego, “Formulating event-based image reconstruction
    as a linear inverse problem with deep regularization using optical flow,” *TPAMI*,
    2022.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Rebecq, Henri and Ranftl, René and Koltun, Vladlen and Scaramuzza, Davide,
    “High speed and high dynamic range video with an event camera,” *TPAMI*, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] T. Stoffregen, C. Scheerlinck, D. Scaramuzza, T. Drummond, N. Barnes,
    L. Kleeman, and R. Mahony, “Reducing the sim-to-real gap for event cameras,” in
    *ECCV*.   Springer, 2020.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] F. Paredes-Vallés and G. C. de Croon, “Back to event basics: Self-supervised
    learning of image reconstruction for event cameras via photometric constancy,”
    in *CVPR*, 2021.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] S. Tulyakov, D. Gehrig, S. Georgoulis, J. Erbach, M. Gehrig, Y. Li, and
    D. Scaramuzza, “Time Lens: Event-based video frame interpolation,” in *CVPR*,
    2021.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] G. Paikin, Y. Ater, R. Shaul, and E. Soloveichik, “EFI-Net: Video Frame
    Interpolation from Fusion of Events and Frames,” in *CVPR*, 2021.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Z. Yu, Y. Zhang, D. Liu, D. Zou, X. Chen, Y. Liu, and J. S. Ren, “Training
    weakly supervised video frame interpolation with events,” in *ICCV*, 2021.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] W. He, K. You, Z. Qiao, X. Jia, Z. Zhang, W. Wang, H. Lu, Y. Wang, and
    J. Liao, “Timereplayer: Unlocking the potential of event cameras for video interpolation,”
    in *CVPR*, 2022.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. Tulyakov, A. Bochicchio, D. Gehrig, S. Georgoulis, Y. Li, and D. Scaramuzza,
    “Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow
    and Multi-scale Fusion,” in *CVPR*, 2022.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] S. Wu, K. You, W. He, C. Yang, Y. Tian, Y. Wang, Z. Zhang, and J. Liao,
    “Video interpolation by event-driven anisotropic adjustment of optical flow,”
    in *ECCV*.   Springer, 2022.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Lin, J. Zhang, J. Pan, Z. Jiang, D. Zou, Y. Wang, J. Chen, and J. Ren,
    “Learning event-driven video deblurring and interpolation,” in *ECCV*.   Springer,
    2020.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] C. Song, Q. Huang, and C. Bajaj, “E-cir: Event-enhanced continuous intensity
    recovery,” in *CVPR*, 2022.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X. Zhang and L. Yu, “Unifying motion deblurring and frame interpolation
    with events,” in *CVPR*, 2022.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. Zou, Y. Zheng, T. Takatani, and Y. Fu, “Learning to reconstruct high
    speed and high dynamic range videos from events,” in *CVPR*, 2021.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] L. Yu, W. Yang *et al.*, “Event-based high frame-rate video reconstruction
    with a novel cycle-event network,” in *ICIP*.   IEEE, 2020.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] A. Z. Zhu, Z. Wang, K. Khant, and K. Daniilidis, “Eventgan: Leveraging
    large scale image datasets for event cameras,” in *ICCP*.   IEEE, 2021.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] H. C. Duwek, A. Shalumov, and E. E. Tsur, “Image reconstruction from
    neuromorphic event cameras using laplacian-prediction and poisson integration
    with spiking and artificial neural networks,” in *CVPR*, 2021.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] L. Pan, C. Scheerlinck, X. Yu, R. Hartley, M. Liu, and Y. Dai, “Bringing
    a blurry frame alive at high frame-rate with an event camera,” in *CVPR*, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Choi, K.-J. Yoon *et al.*, “Learning to super resolve intensity images
    from events,” in *CVPR*, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Jing, Y. Yang, X. Wang, M. Song, and D. Tao, “Turning frequency to
    resolution: Video super-resolution via event cameras,” in *CVPR*, 2021.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] L. Wang, T.-K. Kim, and K.-J. Yoon, “EventSR: From Asynchronous Events
    to Image Reconstruction, Restoration, and Super-Resolution via End-to-End Adversarial
    Learning,” in *CVPR*, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] M. Mostafavi, L. Wang, and K.-J. Yoon, “Learning to reconstruct hdr images
    from events, with applications to depth and flow prediction,” *IJCV*, 2021.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] B. Wang, J. He, L. Yu, G.-S. Xia, and W. Yang, “Event enhanced high-quality
    image recovery,” in *ECCV*, 2020.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] J. Han, Y. Yang, C. Zhou, C. Xu, and B. Shi, “Evintsr-net: Event guided
    multiple latent frames reconstruction and super-resolution,” in *ICCV*, 2021.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Y. Lu, Z. Wang, M. Liu, H. Wang, and L. Wang, “Learning Spatial-Temporal
    Implicit Neural Representations for Event-Guided Video Super-Resolution,” in *CVPR*,
    2023.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] M. Gehrig, M. Millhäusler, D. Gehrig, and D. Scaramuzza, “E-raft: Dense
    optical flow from event cameras,” in *3DV*.   IEEE, 2021.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] M. Jin, G. Meishvili, and P. Favaro, “Learning to extract a video sequence
    from a single motion-blurred image,” in *CVPR*, 2018.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] F. Xu, L. Yu, B. Wang, W. Yang, G.-S. Xia, X. Jia, Z. Qiao, and J. Liu,
    “Motion deblurring with real events,” in *ICCV*, 2021.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Z. Jiang, Y. Zhang, D. Zou, J. Ren, J. Lv, and Y. Liu, “Learning event-based
    motion deblurring,” in *CVPR*, 2020.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] W. Shang, D. Ren, D. Zou, J. S. Ren, P. Luo, and W. Zuo, “Bringing events
    into video deblurring with non-consecutively blurry frames,” in *ICCV*, 2021.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] T. Kim, J. Lee, L. Wang, and K.-J. Yoon, “Event-guided Deblurring of
    Unknown Exposure Time Videos,” in *ECCV*, 2022.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] G. Orchard, A. Jayawant, G. K. Cohen, and N. Thakor, “Converting static
    image datasets to spiking neuromorphic datasets using saccades,” *Frontiers in
    Neuroscience*, 2015.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] T. Serrano-Gotarredona and B. Linares-Barranco, “Poker-DVS and MNIST-DVS.
    Their history, how they were made, and other details,” *Frontiers in Neuroscience*,
    2015.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] H. Li, H. Liu, X. Ji, G. Li, and L. Shi, “Cifar10-dvs: an event-stream
    dataset for object classification,” *Frontiers in Neuroscience*, 2017.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] B. Ramesh, H. Yang, G. Orchard, N. A. Le Thi, S. Zhang, and C. Xiang,
    “Dart: distribution aware retinal transform for event-based cameras,” *TPAMI*,
    2019.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Y. Li, H. Zhou, B. Yang, Y. Zhang, Z. Cui, H. Bao, and G. Zhang, “Graph-based
    asynchronous event processing for rapid object recognition,” in *ICCV*, 2021.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] S. Schaefer, D. Gehrig, and D. Scaramuzza, “AEGNN: Asynchronous Event-based
    Graph Neural Networks,” in *CVPR*, 2022.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Han, C. Zhou, P. Duan, Y. Tang, C. Xu, C. Xu, T. Huang, and B. Shi,
    “Neuromorphic Camera Guided High Dynamic Range Imaging,” *CVPR*, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Z. Wang, Y. Ng, C. Scheerlinck, and R. E. Mahony, “An Asynchronous Kalman
    Filter for Hybrid Event Cameras,” *ICCV*, 2020.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] L. Wang and K.-J. Yoon, “Deep Learning for HDR Imaging: State-of-the-Art
    and Future Trends,” *TPAMI*, 2021.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Y. Yang, J. Han, J. Liang, I. Sato, and B. Shi, “Learning event guided
    high dynamic range video reconstruction,” in *CVPR*, 2023, pp. 13 924–13 934.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] B. Xie, Y. Deng, Z. Shao, H. Liu, and Y. Li, “VMV-GCN: Volumetric Multi-View
    Based Graph CNN for Event Stream Classification,” *RAL*, 2022.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, “Asynchronous
    convolutional networks for object detection in neuromorphic cameras,” in *CVPRW*,
    2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] N. Salvatore and J. Fletcher, “Learned Event-based Visual Perception
    for Improved Space Object Detection,” in *WACV*, 2022.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Z. Liang, H. Cao, C. Yang, Z. Zhang, and G. Chen, “Global-local Feature
    Aggregation for Event-based Object Detection on EventKITTI,” in *MFI*, 2022.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] J. Li, J. Li, L. Zhu, X. Xiang, T. Huang, and Y. Tian, “Asynchronous
    Spatio-Temporal Memory Network for Continuous Event-Based Object Detection,” *TIP*,
    2022.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Z. Jiang, P. Xia, K. Huang, W. Stechele, G. Chen, Z. Bing, and A. Knoll,
    “Mixed frame-/event-driven fast pedestrian detection,” in *ICRA*, 2019.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] J. Li, S. Dong, Z. Yu, Y. Tian, and T. Huang, “Event-based vision enhanced:
    A joint detection framework in autonomous driving,” in *ICME*, 2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] A. Tomy, A. Paigwar, K. S. Mann, A. Renzaglia, and C. Laugier, “Fusing
    Event-based and RGB camera for Robust Object Detection in Adverse Conditions,”
    in *ICRA*.   IEEE, 2022.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] J. Kim, I. Hwang, and Y. M. Kim, “Ev-TTA: Test-Time Adaptation for Event-Based
    Object Recognition,” in *CVPR*, 2022.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] M. Planamente, C. Plizzari, M. Cannici, M. Ciccone, F. Strada, A. Bottino,
    M. Matteucci, and B. Caputo, “Da4event: towards bridging the sim-to-real gap for
    event cameras using domain adaptation,” *RAL*, 2021.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] P. J. Besl and N. D. McKay, “Method for registration of 3-d shapes,”
    in *Sens. Fusion*.   Spie, 1992.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] B. Kueng, E. Mueggler, G. Gallego, and D. Scaramuzza, “Low-latency visual
    odometry using event-based feature tracks,” in *IROS*.   IEEE, 2016.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Z. Ni, A. Bolopion, J. Agnus, R. Benosman, and S. Régnier, “Asynchronous
    event-based visual shape tracking for stable haptic feedback in microrobotics,”
    *IEEE Trans. Robot.*, 2012.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A. Z. Zhu, N. Atanasov, and K. Daniilidis, “Event-based feature tracking
    with probabilistic data association,” in *IEEE ICRA*.   IEEE, 2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Y. Dong and T. Zhang, “Standard and Event Cameras Fusion for Feature
    Tracking,” in *Int. Conf. Mach. Vis. Appl.*, 2021.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] J. Chui, S. Klenk, and D. Cremers, “Event-Based Feature Tracking in Continuous
    Time with Sliding Window Optimization.” [Online]. Available: [http://arxiv.org/abs/2107.04536.](http://arxiv.org/abs/2107.04536.)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] H. Seok and J. Lim, “Robust feature tracking in dvs event stream using
    bézier mapping,” in *WACV*, 2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] I. Alzugaray and M. Chli, “Haste: multi-hypothesis asynchronous speeded-up
    tracking of events,” in *BMVC*, 2020.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] S. Hu, Y. Kim, H. Lim, A. J. Lee, and H. Myung, “eCDT: Event Clustering
    for Simultaneous Feature Detection and Tracking,” in *IROS*.   IEEE, 2022.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] “Pseudo-labels for supervised learning on dynamic vision sensor data,
    applied to object detection under ego-motion, author=Chen, Nicholas FY,” in *CVPRW*,
    2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] M. Gehrig and D. Scaramuzza, “Recurrent vision transformers for object
    detection with event cameras,” in *CVPR*, 2023.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] H. Cao, G. Chen, J. Xia, G. Zhuang, and A. Knoll, “Fusion-based feature
    attention gate component for vehicle detection based on event camera,” *IEEE Sensors
    Journal*, 2021.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] M. Liu, N. Qi, Y. Shi, and B. Yin, “An attention fusion network for event-based
    vehicle object detection,” in *ICIP*.   IEEE, 2021.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] “Event-driven ball detection and gaze fixation in clutter, author=Glover,
    Arren and Bartolozzi, Chiara,” in *IROS*.   IEEE, 2016.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] V. Vasco, A. Glover, E. Mueggler, D. Scaramuzza, L. Natale, and C. Bartolozzi,
    “Independent motion detection with event-driven cameras,” in *ICAR*.   IEEE, 2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] T. Stoffregen, G. Gallego, T. Drummond, L. Kleeman, and D. Scaramuzza,
    “Event-Based Motion Segmentation by Motion Compensation,” in *ICCV*, 2019.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] R. Jiang, X. Mou, S. Shi, Y. Zhou, Q. Wang, M. Dong, and S. Chen, “Object
    tracking on event cameras with offline–online learning,” *CAAI Transactions on
    Intelligence Technology*, 2020.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] N. J. Sanket, C. M. Parameshwara, C. D. Singh, A. V. Kuruttukulam, C. Fermüller,
    D. Scaramuzza, and Y. Aloimonos, “EVDodgeNet: Deep Dynamic Obstacle Dodging with
    Event Cameras,” in *ICRA*.   IEEE, 2020.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] C. Walters and S. Hadfield, “Evreflex: Dense time-to-impact prediction
    for event-based obstacle avoidance,” in *IROS*, 2021.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Z. Wang, F. C. Ojeda, A. Bisulco, D. Lee, C. J. Taylor, K. Daniilidis,
    M. A. Hsieh, D. D. Lee, and V. Isler, “EV-Catcher: High-Speed Object Catching
    Using Low-Latency Event-Based Neural Networks,” *RAL*, 2022.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Bisulco, F. C. Ojeda, V. Isler, and D. D. Lee, “Fast motion understanding
    with spatiotemporal neural networks and dynamic vision sensors,” in *ICRA*.   IEEE,
    2021.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Z. Wan, Y. Dai, and Y. Mao, “Learning Dense and Continuous Optical Flow
    From an Event Camera,” *TIP*, 2022.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, “Unsupervised event-based
    learning of optical flow, depth, and egomotion,” in *CVPR*, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] C. Lee, A. K. Kosta, A. Z. Zhu, K. Chaney, K. Daniilidis, and K. Roy,
    “Spike-flownet: event-based optical flow estimation with energy-efficient hybrid
    neural networks,” in *ECCV*.   Springer, 2020.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] J. Hagenaars, F. Paredes-Vallés, and G. De Croon, “Self-supervised learning
    of event-based optical flow with spiking neural networks,” 2021.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Y. Deng, H. Chen, H. Chen, and Y. Li, “Learning from images: A distillation
    learning framework for event cameras,” *TIP*, 2021.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Z. Li, J. Shen, and R. Liu, “A lightweight network to learn optical flow
    from event data,” in *ICPR*.   IEEE, 2021.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Z. Ding, R. Zhao, J. Zhang, T. Gao, R. Xiong, Z. Yu, and T. Huang, “Spatio-temporal
    recurrent networks for event-based optical flow estimation,” in *AAAI*, 2022.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] L. Pan, M. Liu, and R. Hartley, “Single image optical flow estimation
    with an event camera,” in *CVPR*.   IEEE, 2020.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] S. Shiba, Y. Aoki, and G. Gallego, “Secrets of event-based optical flow,”
    in *ECCV*.   Springer, 2022.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] C. Lee, A. K. Kosta, and K. Roy, “Fusion-FlowNet: Energy-efficient optical
    flow estimation using sensor fusion and deep fused spiking-analog network architectures,”
    in *ICRA*.   IEEE, 2022.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] N. Messikommer, D. Gehrig, M. Gehrig, and D. Scaramuzza, “Bridging the
    gap between events and frames through unsupervised domain adaptation,” *RAL*,
    2022.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Z. Sun, N. Messikommer, D. Gehrig, and D. Scaramuzza, “ESS: Learning
    Event-Based Semantic Segmentation from Still Images,” in *ECCV*.   Springer, 2022.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos,
    “Image segmentation using deep learning: A survey,” *CVPR*, 2021.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] S. Jia, “Event Camera Survey and Extension Application to Semantic Segmentation,”
    in *IPMV*, 2022.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] A. Deliege, A. Cioppa, S. Giancola, M. J. Seikavandi, J. V. Dueholm,
    K. Nasrollahi, B. Ghanem, T. B. Moeslund, and M. Van Droogenbroeck, “Soccernet-v2:
    A dataset and benchmarks for holistic understanding of broadcast soccer videos,”
    in *CVPR*, 2021.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] J. Zhang, K. Yang, and R. Stiefelhagen, “Exploring event-driven dynamic
    context for accident scene segmentation,” *TITS*, 2021.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] J. Binas, D. Neil, S.-C. Liu, and T. Delbruck, “DDD17: End-to-end DAVIS
    driving dataset,” *arXiv:1711.01458*, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] L. Wang, Y. Chae, S.-H. Yoon, T.-K. Kim, and K.-J. Yoon, “Evdistill:
    Asynchronous events to end-task learning via bidirectional reconstruction-guided
    cross-modal knowledge distillation,” in *CVPR*, 2021.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] L. Wang, D. Li, Y. Zhu, L. Tian, and Y. Shan, “Dual super-resolution
    learning for semantic segmentation,” in *CVPR*, 2020.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] G. Gallego, H. Rebecq, and D. Scaramuzza, “A unifying contrast maximization
    framework for event cameras, with applications to motion, depth, and optical flow
    estimation,” in *CVPR*, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] T. Stoffregen and L. Kleeman, “Event cameras, contrast maximization and
    reward functions: An analysis,” in *CVPR*, 2019.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] D. R. Kepple, D. Lee, C. Prepsius, V. Isler, I. M. Park, and D. D. Lee,
    “Jointly learning visual motion and confidence from local patches in event cameras,”
    in *ECCV*.   Springer, 2020.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] G. Haessig, A. Cassidy, R. Alvarez, R. Benosman, and G. Orchard, “Spiking
    optical flow for event-based sensors using IBM’s TrueNorth neurosynaptic system,”
    *T BIOMED CIRC S*, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] F. Paredes-Vallés, K. Y. Scheper, and G. C. De Croon, “Unsupervised learning
    of a hierarchical spiking neural network for optical flow estimation: From events
    to global motion perception,” *TPAMI*, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] J. Hidalgo-Carrió, D. Gehrig, and D. Scaramuzza, “Learning Monocular
    Dense Depth from Events,” in *3DV*.   IEEE, 2020.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] D. Gehrig, M. Rüegg, M. Gehrig, J. Hidalgo-Carrió, and D. Scaramuzza,
    “Combining Events and Frames Using Recurrent Asynchronous Multimodal Networks
    for Monocular Depth Prediction,” *RAL*, 2021.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] S. Tulyakov, F. Fleuret, M. Kiefel, P. Gehler, and M. Hirsch, “Learning
    an Event Sequence Embedding for Dense Event-Based Deep Stereo,” *ICCV*, 2019.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] S. M. N. Uddin, S. H. Ahmed, and Y. J. Jung, “Unsupervised Deep Event
    Stereo for Depth Estimation,” *TCSVT*, 2022.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] S. M. M. Isfahani, K.-J. Yoon, and J. Choi, “Event-Intensity Stereo:
    Estimating Depth by the Best of Both Worlds,” *ICCV*, 2021.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Y. Nam, S. M. M. Isfahani, K.-J. Yoon, and J. Choi, “Stereo Depth from
    Events Cameras: Concentrate and Focus on the Future,” *CVPR*, 2022.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] H. Cho and K.-J. Yoon, “Selection and Cross Similarity for Event-Image
    Deep Stereo,” in *ECCV*, 2022.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] K. Zhang, K. Che, J. Zhang, J. Cheng, Z. Zhang, Q. Guo, and L. Leng,
    “Discrete time convolution for fast event-based stereo,” *CVPR*, 2022.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Y. Zuo, L. Cui, X.-Z. Peng, Y. Xu, S. Gao, X. Wang, and L. Kneip, “Accurate
    depth estimation from a hybrid event-rgb stereo setup,” *IROS*, 2021.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] B. Alsadik and S. Karam, “The simultaneous localization and mapping (SLAM)-An
    overview,” *Surv. Geospat. Eng. J*, 2021.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Y. Z., G. G., H. R., L. K., H. L., and D. S., “Semi-dense 3D reconstruction
    with a stereo event camera,” in *ECCV*, 2018.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] A. Baudron, Z. W. Wang, O. Cossairt, and A. K. Katsaggelos, “E3D: Event-Based
    3D Shape Reconstruction,” *arXiv*, 2020.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] X. Lin, C. Yang, X. Bian, W. Liu, and C. Wang, “EAGAN: Event‐based attention
    generative adversarial networks for optical flow and depth estimation,” 2022.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *NeurIPS*, 2017.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] A. Nguyen, T. T. Do, D. G. Caldwell, and N. G. Tsagarakis, “Real-Time
    6DOF Pose Relocalization for Event Cameras with Stacked Spatial LSTM Networks,”
    2017.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] A. Kendall and R. Cipolla, “Geometric Loss Functions for Camera Pose
    Regression with Deep Learning,” in *CVPR*, 2017.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Y. Jin, L. Yu, G. Li, and S. Fei, “A 6-DOFs event-based camera relocalization
    system by CNN-LSTM and image denoising,” *EXPERT SYST APPL*, 2020.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] L. Steffen, S. Ulbrich, A. Roennau, and R. Dillmann, “Multi-view 3D reconstruction
    with self-organizing maps on event-based data,” in *ICAR*, 2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] V. Rudnev, V. Golyanik, J. Wang, H. Seidel, F. Mueller, M. Elgharib,
    and C. Theobalt, “EventHands: Real-Time Neural 3D Hand Pose Estimation from an
    Event Stream,” in *ICCV*, 2021.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Z. Wang, K. Chaney, and K. Daniilidis, “EvAC3D: From Event-Based Apparent
    Contours to 3D Models via Continuous Visual Hulls,” in *ECCV*, 2022.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] X.-F. Han, H. Laga, and M. Bennamoun, “Image-based 3D object reconstruction:
    State-of-the-art and trends in the deep learning era,” *TPAMI*, 2019.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] H. Rebecq, G. Gallego, and D. Scaramuzza, “EMVS: Event-based multi-view
    stereo,” 2016.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] H. Kim, S. Leutenegger, and A. J. Davison, “Real-time 3D reconstruction
    and 6-DoF tracking with an event camera,” in *ECCV*, 2016.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] K. Chaney, A. Zihao Zhu, and K. Daniilidis, “Learning event-based height
    from plane and parallax,” in *CVPR*, 2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Y. Xue, H. Li, S. Leutenegger, and J. Stückler, “Event-based Non-Rigid
    Reconstruction from Contours,” *reconstruction*, 2022.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] M. Muglikar, G. Gallego, and D. Scaramuzza, “ESL: Event-based Structured
    Light,” in *3DV*, 2021.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Y. Wang, R. Idoughi, and W. Heidrich, “Stereo event-based particle tracking
    velocimetry for 3d fluid flow reconstruction,” in *ECCV*, 2020.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] I. Ihrke and M. Magnor, “Image-Based Tomographic Reconstruction of Flames,”
    *ACM*, 2004.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] J. Wang, S. Tan, X. Zhen, S. Xu, F. Zheng, Z. He, and L. Shao, “Deep
    3D human pose estimation: A review,” *Comput Vis Image Underst.*, 2021.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Y. Desmarais, D. Mottet, P. Slangen, and P. Montesinos, “A review of
    3D human pose estimation algorithms for markerless motion capture,” *CVIU*, 2021.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] M. Kocabas, N. Athanasiou, and M. J. Black, “Vibe: Video inference for
    human body pose and shape estimation,” in *CVPR*, 2020.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] L. Xu, W. Xu, V. Golyanik, M. Habermann, L. Fang, and C. Theobalt, “Eventcap:
    Monocular 3d capture of high-speed human motions using an event camera,” in *CVPR*,
    2020.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] E. Calabrese, G. Taverni, C. Awai Easthope, S. Skriabine, F. Corradi,
    L. Longinotti, K. Eng, and T. Delbruck, “DHP19: Dynamic Vision Sensor 3D Human
    Pose Dataset,” in *CVPRW*, 2019.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] S. Zou, C. Guo, X. Zuo, S. Wang, P. Wang, X. Hu, S. Chen, M. Gong, and
    L. Cheng, “EventHPE: Event-based 3D Human Pose and Shape Estimation,” in *ICCV*,
    2021.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] J. Zhang, B. Dong, H. Zhang, J. Ding, F. Heide, B. Yin, and X. Yang,
    “Spiking Transformers for Event-based Single Object Tracking,” in *CVPR*.   ,
    2022.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] W. Weng, Y. Zhang, and Z. Xiong, “Event-based Video Reconstruction Using
    Transformer,” in *ICCV*, 2021.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] X. Liu, J. Li, X. Fan, and Y. Tian, “Event-based Monocular Dense Depth
    Estimation with Recurrent Transformers,” *CoRR*, 2022.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in *CVPR*, 2018.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,
    Y. Zhu, R. Pang, V. Vasudevan *et al.*, “Searching for mobilenetv3,” in *CVPR*,
    2019.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely efficient
    convolutional neural network for mobile devices,” in *CVPR*, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufflenet v2: Practical guidelines
    for efficient cnn architecture design,” in *ECCV*, 2018.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] R. Krishnamoorthi, “Quantizing deep convolutional networks for efficient
    inference: A whitepaper,” *arXiv*, 2018.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and A. Maida,
    “Deep learning in spiking neural networks,” *NN*, 2019.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] J. Zhang, X. Yang, Y. Fu, X. Wei, B. Yin, and B. Dong, “Object tracking
    by jointly exploiting frame and event domain,” in *ICCV*, 2021.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] R. Battiti, “First-and second-order methods for learning: between steepest
    descent and Newton’s method,” *Neural Comput.*, 1992.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*.   MIT Press,
    2016.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] A. Niwa, F. Mochizuki, R. Berner, T. Maruyarma, T. Terano, K. Takamiya,
    Y. Kimura, K. Mizoguchi, T. Miyazaki, S. Kaizu *et al.*, “A 2.97 $\mu$m-pitch
    event-based vision sensor with shared pixel front-end circuitry and low-noise
    intensity readout mode,” in *2023 IEEE International Solid-State Circuits Conference
    (ISSCC)*.   IEEE, 2023.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] M. Guo, S. Chen, Z. Gao, W. Yang, P. Bartkovjak, Q. Qin, X. Hu, D. Zhou,
    M. Uchiyama, S. Fukuoka *et al.*, “A 3-Wafer-Stacked hybrid 15MPixel CIS+ 1 MPixel
    EVS with 4.6 GEvent/s readout, In-Pixel TDC and On-Chip ISP and ESP function,”
    in *2023 IEEE International Solid-State Circuits Conference (ISSCC)*.   IEEE,
    2023.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] K. Kodama, Y. Sato, Y. Yorikado, R. Berner, K. Mizoguchi, T. Miyazaki,
    M. Tsukamoto, Y. Matoba, H. Shinozaki, A. Niwa *et al.*, “1.22 $\mu$m 35.6 Mpixel
    RGB hybrid event-based vision sensor with 4.88 $\mu$m-pitch event pixels and up
    to 10K event frame rate by adaptive control on event sparsity,” in *2023 IEEE
    International Solid-State Circuits Conference (ISSCC)*.   IEEE, 2023.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    *Communications of the ACM*, 2021.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] V. Rudnev, M. Elgharib, C. Theobalt, and V. Golyanik, “EventNeRF: Neural
    Radiance Fields from a Single Colour Event Camera,” *CVPR*, 2022.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] S. Klenk, L. Koestler, D. Scaramuzza, and D. Cremers, “E-NeRF: Neural
    Radiance Fields from a Moving Event Camera,” *RAL*, 2022.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] I. Hwang, J. Kim, and Y. M. Kim, “Ev-NeRF: Event Based Neural Radiance
    Field,” *WACV*, 2022.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] J. Zhao, S. Zhang, and T. Huang, “Transformer-Based Domain Adaptation
    for Event Data Classification,” in *ICASSP*, 2022.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Y. Yang, L. Pan, and L. Liu, “Event Camera Data Pre-training.” [Online].
    Available: [http://arxiv.org/abs/2301.01928.](http://arxiv.org/abs/2301.01928.)'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/83ab03d1b95826d31d0140ecafc11779.png) | Xu Zheng
    is a Ph.D. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include event-based vision, 3D vision, etc.
    |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2a6e293cb71c8f46fda139b658a01ca0.png) | Yexin
    Liu is a Mphil. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include infrared- and event-based vision, and
    unsupervised domain adaptation. |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/97cba95341b311a800aad8e65cdf35dd.png) | Yunfan
    Lu is a Ph.D. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include low-level vision (event camera, deblurring,
    SR), pattern recognition (image classification, object detection), and DL (transfer
    learning, unsupervised learning). |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/7b5e9f866be8d4a883613cc4adb7fbd7.png) | Tongyan
    Hua is a research assistant in the Visual Learning and Intelligent Systems Lab,
    Artificial Intelligence Thrust, The Hong Kong University of Science and Technology,
    Guangzhou (HKUST-GZ). Her research interests include robotics vision, Simultaneous
    localization and mapping (SLAM), Deep Learning, etc. |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/8f87c6ef36e63b2ec2311c28282503ab.png) | Tianbo
    Pan is a Mphil student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include event-based vision, Simultaneous localization
    and mapping (SLAM), 3D vision, etc. |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/d7595ede50696b0b7f080915ab66b9fa.png) | Weiming
    Zhang is a research assistant in the Visual Learning and Intelligent Systems Lab,
    Artificial Intelligence Thrust, The Hong Kong University of Science and Technology,
    Guangzhou (HKUST-GZ). His research interests include event-based vision, Deep
    Learning, etc. |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/7d35cabf58fc086d1889176113ac995b.png) | Dacheng
    Tao (Fellow IEEE) the Inaugural Director of the JD Explore Academy and a Senior
    Vice President of JD.com. He is also an advisor and chief scientist of the digital
    sciences initiative at the University of Sydney. He mainly applies statistics
    and mathematics to AI and data science, and his research is detailed in one monograph
    and over 200 publications in prestigious journals and proceedings at leading conferences.
    He received the 2015 Australian Scopus-Eureka Prize, the 2018 IEEE ICDM Research
    Contributions Award, and the 2021 IEEE Computer Society McCluskey Technical Achievement
    Award. He is a fellow of the Australian Academy of Science, AAAS, ACM and IEEE.
    |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2057381ce4adb7b6d18493949a93f485.png) | Lin Wang
    (IEEE Member) is an assistant professor in the AI Thrust, HKUST-GZ, HKUST FYTRI,
    and an affiliate assistant professor in the Dept. of CSE, HKUST. He did his Postdoc
    at the Korea Advanced Institute of Science and Technology (KAIST). He got his
    Ph.D. (with honors) and M.S. from KAIST, Korea. He had rich cross-disciplinary
    research experience, covering mechanical, industrial, and computer engineering.
    His research interests lie in computer and robotic vision, machine learning, intelligent
    systems (XR, vision for HCI), etc. |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
