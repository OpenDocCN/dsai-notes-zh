- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:41:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:41:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2302.11075] Deep Active Learning in the Presence of Label Noise: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2302.11075] 标签噪声存在下的深度主动学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.11075](https://ar5iv.labs.arxiv.org/html/2302.11075)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2302.11075](https://ar5iv.labs.arxiv.org/html/2302.11075)
- en: 'Deep Active Learning in the Presence of Label Noise: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签噪声存在下的深度主动学习：综述
- en: '[![[Uncaptioned image]](img/8bd0f2a4e588027ad9558380fab4a30d.png) Moseli Mots’oehli](https://orcid.org/0000-0002-9191-0565)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[![[未标注图像]](img/8bd0f2a4e588027ad9558380fab4a30d.png) 莫塞利·莫特索赫利](https://orcid.org/0000-0002-9191-0565)'
- en: Department of Information and Computer Science
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 信息与计算机科学系
- en: University of Hawai’i at Manoa
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 夏威夷大学马诺阿分校
- en: Honolulu, HI 96822
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 夏威夷檀香山 96822
- en: moselim@hawaii.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: moselim@hawaii.edu
- en: \And[![[Uncaptioned image]](img/8bd0f2a4e588027ad9558380fab4a30d.png) Kyungim
    Baek](https://orcid.org/0000-0000-0000-0000)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \And[![[未标注图像]](img/8bd0f2a4e588027ad9558380fab4a30d.png) 京任·贝克](https://orcid.org/0000-0000-0000-0000)
- en: Department of Information and Computer Science
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 信息与计算机科学系
- en: University of Hawai’i at Manoa
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 夏威夷大学马诺阿分校
- en: Honolulu, HI 96822
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 夏威夷檀香山 96822
- en: kyungim@hawaii.edu
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: kyungim@hawaii.edu
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep active learning has emerged as a powerful tool for training deep learning
    models within a predefined labeling budget. These models have achieved performances
    comparable to those trained in an offline setting. However, deep active learning
    faces substantial issues when dealing with classification datasets containing
    noisy labels. In this literature review, we discuss the current state of deep
    active learning in the presence of label noise, highlighting unique approaches,
    their strengths, and weaknesses. With the recent success of vision transformers
    in image classification tasks, we provide a brief overview and consider how the
    transformer layers and attention mechanisms can be used to enhance diversity,
    importance, and uncertainty-based selection in queries sent to an oracle for labeling.
    We further propose exploring contrastive learning methods to derive good image
    representations that can aid in selecting high-value samples for labeling in an
    active learning setting. We also highlight the need for creating unified benchmarks
    and standardized datasets for deep active learning in the presence of label noise
    for image classification to promote the reproducibility of research. The review
    concludes by suggesting avenues for future research in this area.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度主动学习作为一种在预定义标签预算内训练深度学习模型的强大工具逐渐兴起。这些模型在离线设置下的表现已相当可比。然而，深度主动学习在处理含有噪声标签的分类数据集时面临重大挑战。在这篇文献综述中，我们讨论了在标签噪声存在下深度主动学习的现状，强调了独特的方法及其优缺点。鉴于视觉变换器在图像分类任务中的近期成功，我们提供了一个简要概述，并考虑了如何利用变换器层和注意力机制来增强查询到预言机的多样性、重要性和不确定性基础的选择。我们进一步建议探索对比学习方法，以获取良好的图像表示，这可以帮助在主动学习设置中选择高价值样本进行标注。我们还强调了需要为图像分类中标签噪声存在下的深度主动学习创建统一的基准和标准化数据集，以促进研究的可重复性。综述最后建议了该领域未来研究的方向。
- en: '*Keywords* Active Learning  $\cdot$ Label Noise  $\cdot$ Image Classification
     $\cdot$ Vision Transformer'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 主动学习  $\cdot$ 标签噪声  $\cdot$ 图像分类  $\cdot$ 视觉变换器'
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Machine learning algorithms are a sub-class of artificial intelligence that
    learns from data to perform a pre-defined task such as classification, regression,
    or clustering. Of the numerous algorithms for machine learning, artificial neural
    networks, deep neural networks in particular have done exceptionally well in tasks
    involving complex data representations such as images, text, and sound. The main
    reason for this is that if you have a large enough dataset, you can build more
    extensive and complex models with little to no risk of over-fitting. While this
    works in theory, the practical applications have major drawbacks such as the need
    for labeled training examples that come at a high cost due to the time needed
    to label the data, the high cost of labor in very specialized fields, or the cost
    of running simulations that would produce the ground truth dataset. The solution
    comes in the form of deep active learning (DAL) algorithms, which strive to let
    the learning algorithm iteratively pick data examples to be labeled from a larger
    unlabelled dataset, in such a manner that results in: (1) A smaller labeled training
    set, (2) A dataset that is representative of the underlying data distribution
    leading to a near-optimal learner, (3) A data labeling skim that does not exceed
    the labeling budget.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法是人工智能的一个子类，通过从数据中学习来执行预定义的任务，如分类、回归或聚类。在众多机器学习算法中，人工神经网络，特别是深度神经网络，在涉及复杂数据表示（如图像、文本和声音）的任务中表现尤为出色。这主要是因为如果你拥有足够大的数据集，你可以构建更广泛、更复杂的模型，而几乎没有过拟合的风险。虽然在理论上是这样，但实际应用中存在主要缺陷，如需要标注的训练示例，其成本因标注数据所需的时间、非常专业领域的高劳动成本或运行生成真实数据集的模拟的成本而非常高。解决方案是深度主动学习（DAL）算法，它们致力于让学习算法以迭代方式从较大的未标注数据集中挑选需要标注的数据示例，以这种方式实现：(1)
    较小的标注训练集，(2) 代表底层数据分布的数据集，从而得到近似最优的学习者，(3) 不超过标注预算的数据标注方案。
- en: While this works well for most use cases, real-world dataset labeling has inherent
    label noise due to a variety of factors such as redundant observations being labeled
    differently, the best human expert classification performance being low, or the
    use of auto-labeling software such as Mechanical Turk. This has adverse effects
    on these DAL algorithms’ performance, and most existing DAL literature focuses
    on noise-free settings. We explore existing literature around the problem of using
    DAL algorithms in the existence of label noise. We are particularly interested
    in the image classification domain using different deep representation learning
    frameworks such as convolutional neural networks (CNNs) and vision transformer
    networks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这对于大多数使用场景效果良好，但实际世界中的数据集标注存在固有的标签噪声，这可能是由于各种因素，例如冗余观察被标注为不同的标签、最佳人工专家分类性能较低，或使用像
    Mechanical Turk 这样的自动标注软件。这对这些深度主动学习（DAL）算法的性能产生了不利影响，而现有的 DAL 文献大多数集中在无噪声的设置中。我们探讨了在标签噪声存在的情况下使用
    DAL 算法的问题。我们特别感兴趣的是在图像分类领域，使用不同的深度表示学习框架，如卷积神经网络（CNNs）和视觉变换器网络。
- en: 'In Section [2](#S2 "2 Deep Learning ‣ Deep Active Learning in the Presence
    of Label Noise: A Survey"), we briefly discuss deep learning and the architectures
    used in image classification. Section [3](#S3 "3 Active learning ‣ Deep Active
    Learning in the Presence of Label Noise: A Survey"), presents the main ideas behind
    active learning as well as the issues that arise when datasets contain noisy labels.
    In Section [4](#S4 "4 Evaluation Datasets and Metrics ‣ Deep Active Learning in
    the Presence of Label Noise: A Survey"), we detail commonly used datasets for
    active learning on image classification tasks as well as the evaluation metrics.
    Section [5](#S5 "5 Deep Active Learning Algorithms for Noisy Labels ‣ Deep Active
    Learning in the Presence of Label Noise: A Survey") is a detailed analysis of
    the literature on active learning with label noise in image classification tasks.
    We conclude by exploring possible directions for future research in DAL on vision
    tasks under label noise.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '在[2](#S2 "2 Deep Learning ‣ Deep Active Learning in the Presence of Label Noise:
    A Survey")节中，我们简要讨论了深度学习及其在图像分类中使用的架构。[3](#S3 "3 Active learning ‣ Deep Active
    Learning in the Presence of Label Noise: A Survey")节介绍了主动学习的主要思想以及数据集中存在噪声标签时出现的问题。[4](#S4
    "4 Evaluation Datasets and Metrics ‣ Deep Active Learning in the Presence of Label
    Noise: A Survey")节详细介绍了用于图像分类任务的主动学习常用数据集以及评估指标。[5](#S5 "5 Deep Active Learning
    Algorithms for Noisy Labels ‣ Deep Active Learning in the Presence of Label Noise:
    A Survey")节则是对具有标签噪声的图像分类任务中主动学习文献的详细分析。最后，我们探讨了在标签噪声下视觉任务中的DAL未来研究的可能方向。'
- en: 2 Deep Learning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度学习
- en: Deep Learning (DL) refers to the use of artificial neural networks (ANNs) with
    multiple hidden layers Ivakhnenko and Lapa ([1965](#bib.bib1)), to approximate
    known or unknown functions. The multi-layered neural network was built on top
    of the perceptron Rosenblatt ([1958](#bib.bib2)) introduced in 1958\. Over the
    years, different domain-specific DL architectures have been developed to enhance
    the quality of the learned representations from the different data modalities.
    Early research focused on improving optimization, custom layers and connections,
    activation functions, loss functions, and hyper-parameter tuning techniques for
    the multi-layer perceptron as a way to improve performance on different data modalities.
    For tabular data, tree-based ensemble learning algorithms such as Random forest
    Breiman ([2001](#bib.bib3)), XGBoost Chen and Guestrin ([2016](#bib.bib4)), and
    CatBoost Prokhorenkova et al. ([2018](#bib.bib5)) are preferred over DL for their
    superior performance and resource efficiency. A non-exhaustive selection of interesting
    neural network adaptations to tabular data includes Schäfl et al. ([2022](#bib.bib6));
    Roman et al. ([2022](#bib.bib7)); Popov et al. ([2020](#bib.bib8)); Arik and Pfister
    ([2021](#bib.bib9)); Baohua et al. ([2019](#bib.bib10)). In the natural language
    processing domain, earlier work involved learning word and sentence representation
    using shallow neural networks in an unsupervised setting Pennington et al. ([2014](#bib.bib11));
    Novák et al. ([2020](#bib.bib12)); Grave et al. ([2018](#bib.bib13)). Until the
    wide adoption of attention-based transformer language models Vaswani et al. ([2017](#bib.bib14));
    See et al. ([2017](#bib.bib15)), word and sentence level embeddings are fed to
    a DL model with recurrent connections such as a Long-Short-Term-Memory(LSTM) network
    Hochreiter and Schmidhuber ([1997](#bib.bib16)) to achieve state-of-the-art results
    on down-stream text classification, sentence completion, named entity recognition
    or summarization tasks. For non-temporal visual tasks such as image classification,
    object detection, segmentation, and pose estimation Artacho and Savakis ([2020](#bib.bib17)),
    CNN-based architectures with specialized output layers and a lot of training data
    are still the most widely adopted approach. With each of these complex tasks,
    there are different challenges in the data annotation process that introduce varying
    levels of label noise.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是指使用具有多个隐藏层的人工神经网络（ANNs）（Ivakhnenko 和 Lapa ([1965](#bib.bib1))），以近似已知或未知的函数。多层神经网络建立在1958年罗森布拉特（Rosenblatt
    ([1958](#bib.bib2))）介绍的感知器基础上。多年来，开发了不同领域特定的DL架构，以增强从不同数据模态中学习到的表示的质量。早期的研究集中在改进优化、自定义层和连接、激活函数、损失函数以及多层感知器的超参数调整技术，以提高在不同数据模态上的性能。对于表格数据，基于树的集成学习算法如随机森林（Breiman
    ([2001](#bib.bib3))）、XGBoost（Chen 和 Guestrin ([2016](#bib.bib4))）和CatBoost（Prokhorenkova
    等 ([2018](#bib.bib5))）由于其优越的性能和资源效率而优于DL。对表格数据有趣的神经网络适配器的非详尽选择包括Schäfl 等 ([2022](#bib.bib6))；Roman
    等 ([2022](#bib.bib7))；Popov 等 ([2020](#bib.bib8))；Arik 和 Pfister ([2021](#bib.bib9))；Baohua
    等 ([2019](#bib.bib10))。在自然语言处理领域，早期的工作涉及在无监督环境下使用浅层神经网络学习词和句子的表示（Pennington 等
    ([2014](#bib.bib11))；Novák 等 ([2020](#bib.bib12))；Grave 等 ([2018](#bib.bib13))）。直到广泛采用基于注意力的转换器语言模型（Vaswani
    等 ([2017](#bib.bib14))；See 等 ([2017](#bib.bib15))），词和句子级别的嵌入被输入到具有递归连接的DL模型中，如长短期记忆（LSTM）网络（Hochreiter
    和 Schmidhuber ([1997](#bib.bib16))），以在下游文本分类、句子完成、命名实体识别或摘要任务中取得最先进的结果。对于非时间序列视觉任务，如图像分类、对象检测、分割和姿势估计（Artacho
    和 Savakis ([2020](#bib.bib17))），基于CNN的架构具有专门的输出层和大量的训练数据仍然是最广泛采用的方法。在这些复杂任务中，数据注释过程面临不同的挑战，这些挑战引入了不同程度的标签噪声。
- en: While the DL methods discussed in this section have been applied to other supervised
    learning vision tasks such as detection and segmentation, we focus on approaches
    for image classification in this section. We give a brief overview of CNNs that
    are responsible for a large share of progress in vision-based tasks. We then highlight
    the use of more complex CNNs for image classification and finally explore the
    literature on state-of-the-art spatial attention-based models (Vision Transformers)
    in the context of image classification Kolesnikov et al. ([2021](#bib.bib18)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本节讨论的深度学习方法已经应用于其他监督学习视觉任务，如检测和分割，但我们在本节中重点关注图像分类的方法。我们简要概述了在视觉任务中取得巨大进展的**卷积神经网络（CNN）**。然后，我们突出了更复杂的CNN在图像分类中的应用，最后探索了有关最先进的基于空间注意力的模型（视觉变换器）的文献，涉及图像分类（Kolesnikov
    et al. ([2021](#bib.bib18))）。
- en: 2.1 Convolutional Neural Networks
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 卷积神经网络
- en: Convolutional neural networks were introduced by Yann Lecun and Yashua Bengio
    as an improvement to human-based feature extraction in training multi-layer neural
    networks on spatial data Lecun et al. ([1998](#bib.bib19)). The key deficiencies
    with training fully connected feed-forward neural networks (FFNN) using back-propagation
    for computer vision tasks are efficiency and transformation (rotation, translation)
    invariance. Handling high-dimensional image data with standard input neurons is
    non-trivial and inefficient. Given that low-resolution image datasets are normally
    $28\times 28$, the initial input layer using an FFNN would contain $784$ neurons.
    If the subsequent hidden layer had as little as $100$ neurons, the 2-layer fully
    connected network immediately has more than $78400$ weights and bias terms connecting
    the two layers. The weights of the network are stored in high-dimensional matrices,
    and the flow of information in the forward and backward pass is performed using
    matrix operations. The number of input neurons and hidden layer depth required
    for accurate approximation of complex image-to-class mappings on high-resolution
    images using FFNNs is large.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络由**Yann Lecun**和**Yashua Bengio**提出，是对训练多层神经网络在空间数据上的人工特征提取的改进（Lecun et
    al. ([1998](#bib.bib19))）。使用反向传播训练完全连接的前馈神经网络（FFNN）用于计算机视觉任务的关键缺陷在于效率和变换（旋转、平移）不变性。使用标准输入神经元处理高维图像数据既复杂又低效。考虑到低分辨率图像数据集通常为
    $28\times 28$，初始输入层使用FFNN将包含 $784$ 个神经元。如果随后的隐藏层只有 $100$ 个神经元，则2层完全连接的网络立即有超过
    $78400$ 的权重和偏置项连接这两层。网络的权重存储在高维矩阵中，前向和反向传递中的信息流是通过矩阵运算完成的。使用FFNN对高分辨率图像进行复杂的图像到类别映射的准确逼近所需的输入神经元数量和隐藏层深度非常大。
- en: '![Refer to caption](img/3b53f016af984acdceeda7a8b7cb2d45.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3b53f016af984acdceeda7a8b7cb2d45.png)'
- en: 'Figure 1: [Source: [Standard CNN](https://towardsdatascience.com/covolutional-neural-network-cb0883dd6529)].
    CNN for classifying an image into one of the categories: bird, sunset, dog, cat,
    and more common objects.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：[来源：[标准CNN](https://towardsdatascience.com/covolutional-neural-network-cb0883dd6529)]。CNN用于将图像分类到鸟类、日落、狗、猫以及更多常见物体中的一个类别。
- en: 'CNNs are especially good at handling image data for three main reasons; firstly
    the convolution operation uses a sliding filter to identify and highlight the
    presence of local relations between pixels that represent important features.
    By so doing, we capture features expressing lines, edges, and corners implicitly.
    Secondly, in CNNs, the input image is not flattened into an array as is the case
    in using FFNN. This means the relative positions of pixels in a grid format are
    preserved and so we do not lose information through rearranging the pixels. Finally,
    in CNNs, filters have their own weights, but the same filter is used to slide
    over the image, and this means the features learned are invariant to the positions
    of patches on the image as the convolution operation learns local relationships
    between pixels. In addition, this way CNNs are able to use fewer weights than
    would be the case if we were to consider the absolute positions of pixels on the
    image and capture positional encoding. Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Convolutional
    Neural Networks ‣ 2 Deep Learning ‣ Deep Active Learning in the Presence of Label
    Noise: A Survey") depicts a simple CNN with convolution, pooling, and fully connected
    layers with non-linearity activation functions for image classification.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'CNN 特别擅长处理图像数据，主要有三个原因；首先，卷积操作使用滑动滤波器来识别和突出像素之间的局部关系，这些关系代表了重要特征。通过这种方式，我们隐式捕捉表达线条、边缘和角点的特征。其次，在
    CNN 中，输入图像不会像使用 FFNN 时那样被展平为一个数组。这意味着像素在网格格式中的相对位置被保留，因此我们不会因为像素的重新排列而丢失信息。最后，在
    CNN 中，滤波器具有自己的权重，但相同的滤波器被用来滑过图像，这意味着学到的特征对图像上补丁的位置是不变的，因为卷积操作学习的是像素之间的局部关系。此外，这样
    CNN 能够使用比考虑像素在图像上的绝对位置和捕获位置编码时更少的权重。图 [1](#S2.F1 "Figure 1 ‣ 2.1 Convolutional
    Neural Networks ‣ 2 Deep Learning ‣ Deep Active Learning in the Presence of Label
    Noise: A Survey") 描述了一个简单的 CNN，包括卷积、池化和全连接层，并具有用于图像分类的非线性激活函数。'
- en: 'More advanced CNN architectures have been introduced, mostly similar in that
    they have multiple convolution and pooling blocks (earlier layers capture low-level
    features, and deeper layers capture higher-level features). The most notable of
    these are GoogLeNet Szegedy et al. ([2015](#bib.bib20)), VGG Simonyan and Zisserman
    ([2015](#bib.bib21)), ResNet He et al. ([2016](#bib.bib22)), DenseNet Huang et al.
    ([2017](#bib.bib23)), and EfficientNet M. Tan ([2019](#bib.bib24)). For example,
    ResNet, as shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Convolutional Neural Networks
    ‣ 2 Deep Learning ‣ Deep Active Learning in the Presence of Label Noise: A Survey"),
    demonstrated the idea of residual connections (also called skip connections).
    The residual connections pass one layer’s inputs directly to the next convolution
    block to provide lower-level context to the subsequent layer hence combating vanishing
    gradients in very deep networks. DenseNet on the other hand has a dense building
    block in that all the layers in a block have direct connections with each other,
    allowing for a more effective reuse of features in the network. Also, by having
    all layers connected, a regularization effect is created so that the network does
    not learn redundant representations, hence combating over-fitting.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '更高级的 CNN 架构已被引入，这些架构大多相似，具有多个卷积和池化块（早期层捕捉低级特征，更深层捕捉高级特征）。其中最著名的有 GoogLeNet
    Szegedy 等人 ([2015](#bib.bib20))，VGG Simonyan 和 Zisserman ([2015](#bib.bib21))，ResNet
    He 等人 ([2016](#bib.bib22))，DenseNet Huang 等人 ([2017](#bib.bib23)) 和 EfficientNet
    M. Tan ([2019](#bib.bib24))。例如，ResNet 如图 [2](#S2.F2 "Figure 2 ‣ 2.1 Convolutional
    Neural Networks ‣ 2 Deep Learning ‣ Deep Active Learning in the Presence of Label
    Noise: A Survey") 所示，展示了残差连接（也称为跳跃连接）的概念。残差连接将一层的输入直接传递到下一个卷积块，以便为后续层提供较低级的上下文，从而对抗非常深的网络中的梯度消失问题。另一方面，DenseNet
    具有一个密集的构建块，其中块中的所有层彼此直接连接，允许在网络中更有效地重用特征。此外，通过所有层的连接，产生了正则化效果，使网络不会学习冗余的表示，从而对抗过拟合。'
- en: '![Refer to caption](img/0993bab684bc55c76b67ce2b7fa24c7d.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0993bab684bc55c76b67ce2b7fa24c7d.png)'
- en: 'Figure 2: Resnet50 architecture with convolutional blocks of different filter
    sizes and max pooling. Residual connections acting as memory cells arch above
    the blocks passing initial information all the way to the final layer He et al.
    ([2016](#bib.bib22)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Resnet50 架构具有不同滤波器大小的卷积块和最大池化。残差连接作为记忆单元，弓形架设在块上方，将初始信息传递到最终层 He 等人 ([2016](#bib.bib22))。
- en: The different layers are connected by non-linear activation functions such as
    the popular ReLU and Elu Nair and Hinton ([2010](#bib.bib25)); Clevert et al.
    ([2016](#bib.bib26)). CNNs have been the dominant approach to computer vision
    benchmarks for a large part of the last decade mainly due to their ability to
    extract meaningful spatial features from images. The main catalysts in ascending
    order of importance for this were the availability of large labeled training datasets,
    advances in computing hardware, and a reduction in the computational cost of training
    such Deep Neural Networks (DNNs). Post ImageNet Jia et al. ([2009](#bib.bib27)),
    CNN-based models trained on very large labeled datasets have been used in the
    feature extraction and pre-training step of most fine-tuned state-of-the-art approaches
    in different vision tasks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的层通过非线性激活函数连接，如流行的 ReLU 和 Elu Nair 和 Hinton ([2010](#bib.bib25)); Clevert
    等人 ([2016](#bib.bib26))。CNN 已经是过去十年大部分时间内计算机视觉基准的主导方法，主要由于其从图像中提取有意义的空间特征的能力。推动这一发展的主要因素按重要性排序是：大规模标注训练数据集的可用性、计算硬件的进步，以及训练深度神经网络（DNN）时计算成本的降低。在
    ImageNet 之后 Jia 等人 ([2009](#bib.bib27))，基于 CNN 的模型在非常大的标注数据集上进行训练，并被用于大多数不同视觉任务的最先进方法中的特征提取和预训练步骤。
- en: 2.2 Vision Transformers
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 视觉变换器
- en: Before full transformer models in the language domain, the best LSTM models
    use a low dimensional vector representation to pass information from an encoder
    network to a decoder network, while using an attention mechanism. Attention in
    this setting is used to learn what parts of an input sequence are most important
    in predicting different parts of the output. In the original paper "Attention
    is all you need" Vaswani et al. ([2017](#bib.bib14)), Vaswani et al. demonstrate
    that long temporal dependencies can be learned without the need for recurrence.
    The three fundamental components in a transformer network are a positional encoding
    of words, attention, and self-attention mechanisms. Positional encoding of both
    input and output tokens is achieved by assigning integer values to tokens/words
    based on their relative position in the input and output sequences. Unlike LSTMs,
    the work of learning word progression and relationships between input and output
    words is done implicitly by the network instead of designing networks with explicit
    bias in the form of recurrent cells and sequential processing. Self-attention
    makes it possible to learn good representations for most languages given a sufficiently
    large collection of text in a semi-supervised manner by masking tokens and letting
    the network learn what the missing word is in any given input sequence. The learned
    representations are then used on a downstream task with fewer labeled data. Because
    transformers do not process input tokens in sequence, they are perfect for parallel
    GPU training.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言领域的全变换器模型之前，最好的 LSTM 模型使用低维向量表示来将信息从编码器网络传递到解码器网络，同时使用注意力机制。在这种情况下，注意力用于学习输入序列中哪些部分在预测输出的不同部分时最为重要。在原始论文“注意力是你需要的一切”
    Vaswani 等人 ([2017](#bib.bib14)) 中，Vaswani 等人展示了长时间依赖关系可以在无需递归的情况下进行学习。变换器网络中的三个基本组件是词的位置信息编码、注意力和自注意力机制。输入和输出令牌的位置信息编码通过根据令牌/词在输入和输出序列中的相对位置分配整数值来实现。与
    LSTM 不同，学习词的进展和输入输出词之间关系的工作是由网络隐式完成的，而不是通过设计具有递归单元和顺序处理的网络。自注意力使得在足够大的文本集合下，通过掩蔽令牌并让网络学习任何给定输入序列中缺失的词，从而能够学习大多数语言的良好表示。这些学习到的表示然后用于下游任务，尽管标记数据较少。由于变换器不按顺序处理输入令牌，因此它们非常适合并行
    GPU 训练。
- en: '![Refer to caption](img/750218be98ffd286a3c16acf23097374.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/750218be98ffd286a3c16acf23097374.png)'
- en: 'Figure 3: Vision transformer architecture showing an input image split into
    14 by 14 patches, and linearly projected to the standard transformer input space.
    The far right side of the image shows the components of the standard transformer
    block with multi-head attention Kolesnikov et al. ([2021](#bib.bib18)).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：视觉变换器架构显示了一个输入图像被分割成 14 x 14 的块，并线性投影到标准变换器输入空间。图像最右侧显示了标准变换器块的组件，包括多头注意力机制
    Kolesnikov 等人 ([2021](#bib.bib18))。
- en: 'Like most great innovations, the fundamental ideas of the transformer have
    been incorporated into CNNs Wortsman et al. ([2022](#bib.bib28)); Zihang et al.
    ([2021](#bib.bib29)); Srinivas1 et al. ([2021](#bib.bib30)), and in some cases
    completely replacing CNNs Touvron et al. ([2020](#bib.bib31)); Liu et al. ([2021](#bib.bib32));
    Chen et al. ([2022](#bib.bib33)) to produce state-of-the-art results in various
    computer vision benchmarks. In Kolesnikov et al. ([2021](#bib.bib18)), Kolesnikov
    et al. present the earliest vision transformer(ViT) to surpass state-of-the-art
    CNNs on most image classification benchmarks. They show that in the large dataset
    regime, ViTs achieve higher classification accuracy, are more computationally
    efficient, and show no signs of saturation compared to CNNs such as ResNet and
    EfficientNet on increasingly larger datasets. The main difference between the
    natural language processing (NLP) transformers and the vision transformers is
    in how the input is encoded. With vision transformers, they take 14 by 14 patches
    from an image, flatten them, and apply a linear projection onto a higher dimensional
    space equal to that of the original input space of the NLP transformer. The spatial
    proximity relations of patches are implicitly left to the transformer to learn
    in the following way: They add a trainable 1D positional encoding vector to each
    patch’s linear projection. The positional representations are organized in the
    order of the patches starting from the top left corner to the bottom right corner
    of the image as depicted in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Vision Transformers
    ‣ 2 Deep Learning ‣ Deep Active Learning in the Presence of Label Noise: A Survey").
    Beyond input encoding, the rest of the ViT architecture is similar to that of
    the language transformer for classification tasks.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 像大多数伟大的创新一样，变压器的基本思想已经被纳入到CNN中，如Wortsman等人（[2022](#bib.bib28)）；Zihang等人（[2021](#bib.bib29)）；Srinivas1等人（[2021](#bib.bib30)），在某些情况下，完全取代了CNN，例如Touvron等人（[2020](#bib.bib31)）；Liu等人（[2021](#bib.bib32)）；Chen等人（[2022](#bib.bib33)），在各种计算机视觉基准测试中取得了最先进的结果。在Kolesnikov等人（[2021](#bib.bib18)）中，Kolesnikov等人展示了最早超越最先进CNN的视觉变压器（ViT），在大多数图像分类基准测试中表现优异。他们展示了在大数据集的情况下，ViT实现了更高的分类准确率，更具计算效率，并且与ResNet和EfficientNet等CNN相比，没有饱和迹象。自然语言处理（NLP）变压器和视觉变压器之间的主要区别在于输入的编码方式。对于视觉变压器，它们从图像中提取14乘14的图块，将其展平，并应用线性投影到一个高维空间，这个空间与NLP变压器的原始输入空间相等。图块的空间邻近关系被隐式地留给变压器通过以下方式学习：它们为每个图块的线性投影添加一个可训练的1D位置编码向量。位置表示按图块的顺序组织，从图像的左上角到右下角，如图[3](#S2.F3
    "图 3 ‣ 2.2 视觉变压器 ‣ 2 深度学习 ‣ 在标签噪声存在下的深度主动学习：综述")所示。除了输入编码，ViT的其余架构类似于用于分类任务的语言变压器。
- en: 'The paper shows interestingly that, through the attention mechanism the transformer
    layers are able to learn the same low-to-high level features with increasing depth
    as is the case with deep CNNs. Other notable implementations of ViTs for image
    classification without label noise include Yu et al. ([2022](#bib.bib34)); Chen
    et al. ([2022](#bib.bib33)); Liu et al. ([2021](#bib.bib32)). ViTs are included
    in this review and further discussed in Section [6](#S6 "6 Conclusion and Future
    Research Directions ‣ Deep Active Learning in the Presence of Label Noise: A Survey")
    as we perceive them to be a very important area for future research. This is because
    they are progressively becoming the dominant multi-modal approach and yet very
    little work has been done in applying them to DAL and learning with label noise
    for image classification. These models are designed in a modular fashion to easily
    be able to learn both language and image representations for image captioning,
    classification, scene-text understanding, and visual question answering Yu et al.
    ([2022](#bib.bib34)). It is particularly interesting since the authors present
    a joint contrastive loss (image-to-text and text-to-image), image classification
    loss, and image-to-language captioning loss, allowing for efficient training of
    a single network for multiple tasks, and the ability to transfer the learned representations
    to a different downstream task and dataset.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '论文有趣地表明，通过注意力机制，变换器层能够像深度卷积神经网络（CNNs）一样，随着深度的增加学习相同的低到高层特征。其他在没有标签噪声的情况下用于图像分类的ViTs显著实现包括Yu等人（[2022](#bib.bib34)）；Chen等人（[2022](#bib.bib33)）；Liu等人（[2021](#bib.bib32)）。ViTs被纳入此综述，并在第[6](#S6
    "6 Conclusion and Future Research Directions ‣ Deep Active Learning in the Presence
    of Label Noise: A Survey")节中进一步讨论，因为我们认为它们是未来研究的一个非常重要的领域。这是因为它们正逐渐成为主导的多模态方法，但在将其应用于DAL（深度主动学习）和带标签噪声的图像分类方面，相关工作非常有限。这些模型以模块化的方式设计，可以轻松学习语言和图像表示，适用于图像描述、分类、场景文本理解和视觉问答Yu等人（[2022](#bib.bib34)）。这尤其有趣，因为作者提出了联合对比损失（图像到文本和文本到图像）、图像分类损失和图像到语言描述损失，允许高效训练一个单一网络来完成多个任务，并将学习到的表示转移到不同的下游任务和数据集。'
- en: In the next section the active learning (AL) framework for machine learning
    is described, including key approaches for training deep learning models on a
    labeling budget in the case of clean labels, and finally, the scene is set for
    label noise and the literature addressing DL on noisy labels.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，将描述机器学习的主动学习（AL）框架，包括在干净标签情况下，针对标注预算训练深度学习模型的关键方法，最后，为标签噪声和处理噪声标签的文献奠定基础。
- en: 3 Active learning
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 主动学习
- en: 'In most supervised machine learning use cases, there is an initial data collection
    and labeling cost, in both money and time. In some domains and tasks, datasets
    are inherently difficult to label for a variety of reasons, meaning more time
    is needed even by an expert human annotator to assign a label to each sample.
    In other cases the cost of hiring expert annotators is high, such as is the case
    in medical imaging Górriz et al. ([2017](#bib.bib35)); Konyushkova et al. ([2017](#bib.bib36)),
    or the cost of producing the samples is high, such as is the case in experimental
    physics where observations come from very expensive telescopes or particle accelerators.
    This presents a challenge to the real-world use of machine learning systems, especially
    as unlabeled dataset sizes increase. Active learning is a machine learning paradigm,
    as depicted in Figure [4](#S3.F4 "Figure 4 ‣ 3 Active learning ‣ Deep Active Learning
    in the Presence of Label Noise: A Survey"), that seeks to address this problem
    by letting learning algorithms iteratively select a subset $L^{m}$ of size $m$,
    from a larger unlabelled dataset $U^{n}$ of size $n:m\leq n$, to be labeled by
    an oracle $O$ for training. The active learning mantra can be stated as follows:
    Train a machine learning model on a significantly smaller labeled dataset, with
    little to no drop in test performance, all the while staying within a pre-determined
    labeling budget $B$.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '在大多数监督式机器学习使用案例中，初始数据收集和标注的成本，包括金钱和时间，都是不可避免的。在某些领域和任务中，由于各种原因，数据集本质上很难进行标注，这意味着即使是专家级的人类标注员也需要更多的时间来为每个样本分配标签。在其他情况下，雇佣专家标注员的成本很高，例如在医疗成像中
    Górriz et al. ([2017](#bib.bib35)); Konyushkova et al. ([2017](#bib.bib36))，或者生产样本的成本很高，例如在实验物理学中，观察结果来自非常昂贵的望远镜或粒子加速器。这给机器学习系统在实际应用中带来了挑战，特别是随着未标记数据集规模的增加。主动学习是一种机器学习范式，如图[4](#S3.F4
    "Figure 4 ‣ 3 Active learning ‣ Deep Active Learning in the Presence of Label
    Noise: A Survey")所示，它试图通过让学习算法迭代地从更大的未标记数据集$U^{n}$（大小为$n:m\leq n$）中选择一个子集$L^{m}$（大小为$m$），由oracle
    $O$进行标注以供训练，来解决这个问题。主动学习的口号可以表述为：在显著较小的标注数据集上训练机器学习模型，测试性能几乎没有下降，同时保持在预定的标注预算$B$之内。'
- en: '![Refer to caption](img/8421820e02ec763a1ca909e76b234b78.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/8421820e02ec763a1ca909e76b234b78.png)'
- en: 'Figure 4: The five main components to the standard Active Learning Framework.
    Each of these components may vary depending on the complexity of the data to be
    learned and the available resources. Most work in active learning has focused
    on the development of query selection algorithms that lead to highly informative
    and diverse data samples for labeling by the oracle.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：标准主动学习框架的五个主要组成部分。这些组成部分可能会根据要学习的数据复杂性和可用资源的不同而有所变化。大多数主动学习的研究工作集中在查询选择算法的开发上，这些算法能够生成高度信息化且多样的数据样本，供oracle进行标注。
- en: Deep active learning algorithms (DAL), while overlapping, can broadly be grouped
    into pool-based methods, density-based methods, and data expansion methods. Pool-based
    methods select samples for labeling from an unlabeled pool, based on either the
    uncertainty of the currently trained model on samples $U^{n}$, the diversity of
    samples in the labeled set $L^{m}$ used to train the current model or a combination
    of both Lewis and Gale ([1994](#bib.bib37)); McCallum and Nigam ([1998](#bib.bib38));
    C.Shui et al. ([2020](#bib.bib39)). Pool-based methods are simple in their formation
    and implementation but can be computationally expensive for large datasets of
    high dimensional data such as images. Since pool-based methods largely rely on
    metrics evaluated on the entire unlabeled dataset to select new candidates, this
    is not ideal for applications that require low latency. Density-based methods
    seek to capture key characteristics of the underlying data distribution. This
    is done by selecting a core-set of samples for labeling that are sufficiently
    representative of the entire dataset, and leads to good generalization Sener and
    Savarese ([2018](#bib.bib40)); Phillips and Tai ([2018](#bib.bib41)); Phillips
    ([2016](#bib.bib42)). More recent literature blends pool and density-based methods
    to take advantage of each approach’s benefits. These methods thus lead to efficient
    and robust models trained on core-sets containing diverse samples that maximize
    the margins between object classes Har-Peled et al. ([2007](#bib.bib43)); Geifman
    and El-Yaniv ([2017](#bib.bib44)). Some methods in this approach use the hidden
    layer representations from training a self-supervised task on the image data,
    instead of the raw pixels. These include pre-training on image orientation, random
    ($90,180,270,360)\degree$ rotation classification, or self-supervised contrastive
    learning, where the target is an arbitrary patch of adjacent pixels in the image
    Chen et al. ([2020](#bib.bib45)); Du et al. ([2021](#bib.bib46)); Wang et al.
    ([2021a](#bib.bib47)). Data expansion methods seek to expand the training dataset,
    by generating reasonably realistic synthetic data samples for each target class
    to enhance the learning algorithm’s performance on the real test dataset Chen
    et al. ([2020](#bib.bib45)). Since their introduction, Generative Adversarial
    Networks (GANs) and their variations Goodfellow et al. ([2014](#bib.bib48)); Gonog
    and Zhou ([2019](#bib.bib49)); Sinha et al. ([2019a](#bib.bib50)) were the go-to
    method for generating synthetic data. However, the training of GANs is unstable,
    the samples tend to be unrealistic, and it is hard to evaluate these samples for
    quality Barnett ([2018](#bib.bib51)); Mescheder et al. ([2018](#bib.bib52)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 深度主动学习算法（DAL），虽然有重叠，但可以大致分为基于池的方法、基于密度的方法和数据扩展方法。基于池的方法从未标记的池中选择样本进行标记，基于当前训练模型在样本$U^{n}$上的不确定性、用于训练当前模型的标记集$L^{m}$中样本的多样性或两者的结合
    Lewis 和 Gale ([1994](#bib.bib37))；McCallum 和 Nigam ([1998](#bib.bib38))；C.Shui
    等 ([2020](#bib.bib39))。基于池的方法在形成和实现上比较简单，但对于大规模的高维数据集（如图像）可能计算成本高昂。由于基于池的方法主要依赖于对整个未标记数据集进行的度量来选择新的候选样本，这对需要低延迟的应用并不理想。基于密度的方法旨在捕捉基础数据分布的关键特征。这是通过选择一个足够代表整个数据集的核心样本集进行标记来完成的，从而实现良好的泛化
    Sener 和 Savarese ([2018](#bib.bib40))；Phillips 和 Tai ([2018](#bib.bib41))；Phillips
    ([2016](#bib.bib42))。更近期的文献将池和密度基方法结合起来，以利用每种方法的优点。这些方法因此能够训练出高效且稳健的模型，这些模型在包含多样化样本的核心集合上进行训练，从而最大化对象类别之间的间隔
    Har-Peled 等 ([2007](#bib.bib43))；Geifman 和 El-Yaniv ([2017](#bib.bib44))。这种方法中的一些使用了在图像数据上训练自监督任务得到的隐藏层表示，而不是原始像素。这些方法包括图像方向的预训练、随机($90,180,270,360)\degree$
    旋转分类或自监督对比学习，其目标是图像中相邻像素的任意补丁 Chen 等 ([2020](#bib.bib45))；Du 等 ([2021](#bib.bib46))；Wang
    等 ([2021a](#bib.bib47))。数据扩展方法旨在通过生成合理现实的合成数据样本来扩展训练数据集，以增强学习算法在真实测试数据集上的表现 Chen
    等 ([2020](#bib.bib45))。自其引入以来，生成对抗网络（GANs）及其变体 Goodfellow 等 ([2014](#bib.bib48))；Gonog
    和 Zhou ([2019](#bib.bib49))；Sinha 等 ([2019a](#bib.bib50)) 一直是生成合成数据的首选方法。然而，GANs
    的训练不稳定，样本往往不现实，而且很难评估这些样本的质量 Barnett ([2018](#bib.bib51))；Mescheder 等 ([2018](#bib.bib52))。
- en: 3.1 Label Noise
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 标签噪声
- en: label noise refers to the scenario in which data labels are corrupted, with
    or without intention, so that we do not have $100\%$ confidence in their correctness.
    Label noise is different from feature noise which is normally used to refer to
    adding gaussian noise to feature values. Label noise impacts learning algorithms
    more adversely than feature noise does, and is harder to deal with Chicheng and
    Kamalika ([2015](#bib.bib53)); Wei et al. ([2022](#bib.bib54)); Algan and Ulusoy
    ([2021](#bib.bib55)); Cordeiro and Carneiro ([2020](#bib.bib56)). Label noise
    is inherent in the data collection and processing life-cycle. Most real-world
    datasets are subjected to a number of label noise sources based on how the data
    is collected, curated, and stored. Label noise in practice broadly stems from
    (1) incorrect crowd-sourced labels where the annotators are non-experts such as
    is the case with Worker ([2022](#bib.bib57)), and Amazon ([2022](#bib.bib58)),
    (2) incorrect expert annotations due to the complexity of the data, as is common
    in medical fields Górriz et al. ([2017](#bib.bib35)), (3) labeling errors introduced
    by automatic labeling by web crawling software and other AI labeling systems such
    as Scale.ai ([2022](#bib.bib59)), (4) noise introduced by multiple experts or
    non-experts labeling the same sample differently.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 标签噪声指的是数据标签被破坏的情况，无论是有意还是无意，因此我们无法对其正确性保持$100\%$的信心。标签噪声与特征噪声不同，后者通常指的是向特征值中添加高斯噪声。标签噪声对学习算法的负面影响比特征噪声更大，而且更难处理，参考
    Chicheng 和 Kamalika ([2015](#bib.bib53))；Wei 等人 ([2022](#bib.bib54))；Algan 和 Ulusoy
    ([2021](#bib.bib55))；Cordeiro 和 Carneiro ([2020](#bib.bib56))。标签噪声是数据收集和处理生命周期中的固有问题。大多数现实世界的数据集都受到标签噪声源的影响，这取决于数据如何收集、整理和存储。实践中的标签噪声主要来源于
    (1) 不正确的众包标签，其中注释者是非专家，如 Worker ([2022](#bib.bib57)) 和 Amazon ([2022](#bib.bib58))
    的情况，(2) 由于数据复杂性导致的不正确专家注释，如在医学领域中常见 Górriz 等人 ([2017](#bib.bib35))，(3) 自动标签系统（如
    Scale.ai ([2022](#bib.bib59))）和网页爬虫软件引入的标签错误，(4) 多个专家或非专家对同一样本进行不同标注所引入的噪声。
- en: 'Learning noisy labels is especially hard due to the fact that cost functions
    are generally significantly less complex than feature extraction layers. Label
    noise can be grouped, and is mostly treated based on what is known about the noise-generating
    distribution Nagarajan et al. ([2013](#bib.bib60)). Some datasets contain label
    noise from a known and quantifiable generative distribution, while in other cases,
    too little or nothing is known about the noise transition matrix to model. Label
    noise can be class-independent or class-dependent. Class-independent label noise
    is the easiest to generate. The generative process can be summarized in this manner:
    for each sample, the class label is replaced with a random class label, with a
    fixed probability $1/N$ where $N$ is the number of classes Patrinin et al. ([2017](#bib.bib61)).
    Class-dependent label noise is normally a result of expert human annotation. It
    results from pairs of very closely related or indistinguishable classes being
    occasionally mislabeled Han et al. ([2018a](#bib.bib62)). For example, the true
    large-sized cat is occasionally labeled as a small dog, and visa versa. Common
    methods for training DL models include first filtering out samples with a high
    probability of being noisy and iteratively training on a dataset with trusted
    labels until a threshold is reached. The filtering process in most literature
    involves training two different neural networks with a custom loss, and monitoring
    samples on which they disagree on predictions. This method works well since it
    has been shown that the networks train on stronger signals first, which is the
    case in a dataset with predominantly clean labels. Representative methods in this
    approach, trained in a non-active learning manner include Decoupling Malach and
    Shalev-Shwartz ([2017](#bib.bib63)) and Co-teaching Han et al. ([2018b](#bib.bib64)).
    The main implementation difference between the two approaches is in how the two
    networks’ weights are updated. Decoupling updates each network’s weights based
    on its prediction error when the networks have a prediction disagreement. Co-teaching
    on the other hand, cross-updates the weights with the error signal from the other
    network. Unlike Decoupling, Co-teaching addresses noisy labels explicitly by enabling
    the networks to peek into each other’s hidden state, simultaneously reducing the
    risk of each network over-fitting the noisy input.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 学习噪声标签尤其困难，因为代价函数通常比特征提取层复杂得多。标签噪声可以被分组，通常基于对噪声生成分布的了解来处理（Nagarajan 等人 ([2013](#bib.bib60))）。一些数据集包含来自已知且可量化生成分布的标签噪声，而在其他情况下，对于噪声转换矩阵的了解过少或完全不了解。标签噪声可以是类无关的或类相关的。类无关标签噪声是最容易生成的。生成过程可以总结为：对于每个样本，类标签以固定概率
    $1/N$ 被替换为随机类标签，其中 $N$ 是类别的数量（Patrinin 等人 ([2017](#bib.bib61))）。类相关标签噪声通常是专家人工标注的结果。它是由于非常相似或不可区分的类偶尔被错误标记（Han
    等人 ([2018a](#bib.bib62))）。例如，真正的大型猫偶尔被标记为小狗，反之亦然。训练深度学习模型的常用方法包括首先过滤掉具有高概率为噪声的样本，然后在一个带有可信标签的数据集上进行迭代训练，直到达到阈值。大多数文献中的过滤过程涉及训练两个不同的神经网络，使用自定义损失，并监控它们在预测上意见不一致的样本。这种方法效果良好，因为已证明这些网络首先训练在较强的信号上，这在一个主要由干净标签组成的数据集中是常见的。这种方法的代表性方法包括非主动学习方式训练的Decoupling（Malach
    和 Shalev-Shwartz ([2017](#bib.bib63))）和Co-teaching（Han 等人 ([2018b](#bib.bib64))）。这两种方法的主要实现差异在于两个网络权重的更新方式。Decoupling
    基于网络预测错误更新每个网络的权重，而 Co-teaching 则交叉更新权重，并使用另一个网络的错误信号。与 Decoupling 不同，Co-teaching
    通过使网络能够查看彼此的隐藏状态，明确解决了噪声标签问题，同时减少了每个网络过拟合噪声输入的风险。
- en: 'We have introduced deep learning, active learning, and learning with label
    noise. For further reading, we suggest the survey papers Algan and Ulusoy ([2021](#bib.bib55));
    Ren et al. ([2020](#bib.bib65)) on image classification with noisy labels, and
    DAL on clean labels respectively. DAL methods on noisy labels are presented in
    Section [5](#S5 "5 Deep Active Learning Algorithms for Noisy Labels ‣ Deep Active
    Learning in the Presence of Label Noise: A Survey").'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '我们介绍了深度学习、主动学习和带有标签噪声的学习。有关进一步阅读，我们建议参考 Algan 和 Ulusoy ([2021](#bib.bib55))
    的综述论文；Ren 等人 ([2020](#bib.bib65)) 关于带有噪声标签的图像分类，以及 DAL 关于干净标签的论文。带有噪声标签的 DAL 方法在第
    [5](#S5 "5 Deep Active Learning Algorithms for Noisy Labels ‣ Deep Active Learning
    in the Presence of Label Noise: A Survey") 节中介绍。'
- en: 4 Evaluation Datasets and Metrics
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估数据集和指标
- en: In this section, we introduce datasets and evaluation metrics commonly used
    for active learning and learning with label noise. The State-of-the-art DAL methods
    on zero-label noise datasets are Górriz et al. ([2017](#bib.bib35)); Konyushkova
    et al. ([2017](#bib.bib36)); Sener and Savarese ([2018](#bib.bib40)); Phillips
    and Tai ([2018](#bib.bib41)); Phillips ([2016](#bib.bib42)). Datasets and their
    meta-data are provided that pertain to AL and DL on label noise. We conclude the
    section by exploring the evaluation metrics for DAL for noisy data on common bench-marking
    datasets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了在主动学习和带标签噪声学习中常用的数据集和评估指标。关于零标签噪声数据集的最先进的DAL方法包括Górriz等（[2017](#bib.bib35)）；Konyushkova等（[2017](#bib.bib36)）；Sener和Savarese（[2018](#bib.bib40)）；Phillips和Tai（[2018](#bib.bib41)）；Phillips（[2016](#bib.bib42)）。提供了与AL和DL在标签噪声中的相关数据集及其元数据。我们通过探讨常见基准数据集上DAL对噪声数据的评估指标来结束本节。
- en: 4.1 Datasets
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: As stated previously, the meteoric rise of DL algorithms was largely due to
    the availability of large labeled training datasets. In image classification,
    the most notable datasets include MNIST LeCun et al. ([2010](#bib.bib66)), ImageNet
    Jia et al. ([2009](#bib.bib27)), CIFAR-100 Krizhevsky et al. ([2009](#bib.bib67)),
    CALTECH-101 Fei-Fei et al. ([2004](#bib.bib68)), SVHN Netzer et al. ([2011](#bib.bib69)),
    and MS COCO Lin et al. ([2014](#bib.bib70)). Public evaluation datasets facilitate
    a centralized evaluation of algorithms on a pre-defined task. These datasets can
    be downloaded from their websites or the different DL frameworks such as PyTorch,
    TensorFlow, Jax, and Theano. The best-performing models and their results on these
    datasets are normally hosted on a public leaderboard for the dataset. When evaluating
    datasets for DAL on image classification tasks, the standard practice is to use
    the same datasets as in full dataset image classification, but we monitor performance
    gain after a pre-defined number of labeled examples. While in practice this may
    not be the case that all labels are available upfront as is the case in using
    fully labeled datasets, the training cycle of DAL algorithms applied on these
    complete datasets substantially mimics the process of obtaining labels from an
    oracle for a live stream of unlabeled data within a budget.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，DL算法的迅猛发展在很大程度上归因于大规模标记训练数据集的可用性。在图像分类中，最著名的数据集包括MNIST LeCun等（[2010](#bib.bib66)）、ImageNet
    Jia等（[2009](#bib.bib27)）、CIFAR-100 Krizhevsky等（[2009](#bib.bib67)）、CALTECH-101
    Fei-Fei等（[2004](#bib.bib68)）、SVHN Netzer等（[2011](#bib.bib69)）以及MS COCO Lin等（[2014](#bib.bib70)）。公共评估数据集促进了对预定义任务的算法集中评估。这些数据集可以从其网站或不同的DL框架（如PyTorch、TensorFlow、Jax和Theano）中下载。最佳性能的模型及其在这些数据集上的结果通常会在公共排行榜上展示。在对图像分类任务的DAL数据集进行评估时，标准做法是使用与全数据集图像分类相同的数据集，但我们会监控在预定义数量的标记示例后的性能提升。虽然在实践中，所有标签可能不会像使用完全标记的数据集那样一次性可用，但在这些完整数据集上应用DAL算法的训练周期实质上模拟了从一个预设预算的实时未标记数据流中获取标签的过程。
- en: The datasets vary widely in size, the number of classes, and the complexity
    inherent in telling the classes apart. Of all the commonly used AL datasets, MNIST
    is the least complex, with only 10 classes of hand-written digits in single channel
    $28\times 28$ images. CALTECH-101, ImageNet, and CIFAR-100 are higher-resolution
    image datasets. These datasets contain more classes than MNIST, some of which
    are harder to tell apart. In passive learning, the model sees all available training
    samples per class, but in the DAL setting, depending on the query algorithm and
    scarcity of a class, the algorithms may never see more than a third of the samples
    of certain under-sampled classes. This leads to poor validation performance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在大小、类别数量以及区分类别的复杂性方面差异很大。在所有常用的AL数据集中，MNIST最简单，只有10个类别的手写数字，单通道$28\times
    28$图像。CALTECH-101、ImageNet和CIFAR-100是更高分辨率的图像数据集。这些数据集包含比MNIST更多的类别，其中一些类别较难区分。在被动学习中，模型会看到每个类别的所有可用训练样本，但在DAL设置中，根据查询算法和类别的稀缺程度，算法可能永远无法看到某些欠采样类别的三分之一以上的样本。这导致验证性能差。
- en: 'Large-sized datasets with high-resolution images also pose a computational
    problem in DAL algorithms that select diverse samples based on a distance measure
    to all other unlabeled images. This can be extremely costly to compute in both
    time and hardware resource requirements. For these reasons, the reported performance
    of DAL algorithms on these datasets is lower than that of non-active learning
    algorithms since researchers have a low incentive to test complex DAL algorithms
    on large datasets. Table [1](#S4.T1 "Table 1 ‣ 4.1 Datasets ‣ 4 Evaluation Datasets
    and Metrics ‣ Deep Active Learning in the Presence of Label Noise: A Survey")
    contains a non-exhaustive list of commonly used image classification datasets
    for active learning and learning with label noise.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '大规模高分辨率图像数据集在 DAL 算法中也会引发计算问题，这些算法根据与所有其他未标记图像的距离度量来选择多样化的样本。这在时间和硬件资源需求上都是极其昂贵的。因此，这些数据集上报告的
    DAL 算法性能低于非主动学习算法，因为研究人员对在大数据集上测试复杂的 DAL 算法的动力较低。表格 [1](#S4.T1 "Table 1 ‣ 4.1
    Datasets ‣ 4 Evaluation Datasets and Metrics ‣ Deep Active Learning in the Presence
    of Label Noise: A Survey") 包含了一个常用的图像分类数据集的非详尽列表，用于主动学习和带有标签噪声的学习。'
- en: '| Dataset | Year | # Samples | Classes | DAL Papers | label noise Papers |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 样本数量 | 类别 | DAL 论文 | 标签噪声论文 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| ImageNet | 2012 | 1,431,167 | 1000 | Yi et al. ([2022](#bib.bib71)) | Hataya
    and Nakayama ([2018](#bib.bib72)) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | 2012 | 1,431,167 | 1000 | Yi 等 ([2022](#bib.bib71)) | Hataya 和
    Nakayama ([2018](#bib.bib72)) |'
- en: '| SVHN | 2011 | 660,000 | 10 | Gupta et al. ([2020](#bib.bib73)); Wang et al.
    ([2021b](#bib.bib74)); Sener and Savarese ([2018](#bib.bib40)) | Gupta et al.
    ([2020](#bib.bib73)); Xia et al. ([2020](#bib.bib75)) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| SVHN | 2011 | 660,000 | 10 | Gupta 等 ([2020](#bib.bib73)); Wang 等 ([2021b](#bib.bib74));
    Sener 和 Savarese ([2018](#bib.bib40)) | Gupta 等 ([2020](#bib.bib73)); Xia 等 ([2020](#bib.bib75))
    |'
- en: '| MNIST | 2010 | 70,000 | 10 | Li et al. ([2022](#bib.bib76)); Haußmann et al.
    ([2019](#bib.bib77)); Gupta et al. ([2020](#bib.bib73)) | Younesian et al. ([2020](#bib.bib78));
    Gupta et al. ([2020](#bib.bib73)) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| MNIST | 2010 | 70,000 | 10 | Li 等 ([2022](#bib.bib76)); Haußmann 等 ([2019](#bib.bib77));
    Gupta 等 ([2020](#bib.bib73)) | Younesian 等 ([2020](#bib.bib78)); Gupta 等 ([2020](#bib.bib73))
    |'
- en: '| CIFAR(10,100) | 2009 | 60,000 | (10,100) | Du et al. ([2021](#bib.bib46));
    Younesian et al. ([2020](#bib.bib78)); Wei et al. ([2022](#bib.bib54)); Li et al.
    ([2022](#bib.bib76)); Shui et al. ([2020](#bib.bib79)); Haußmann et al. ([2019](#bib.bib77));
    Gupta et al. ([2020](#bib.bib73)); Younesian et al. ([2021](#bib.bib80)) | Younesian
    et al. ([2020](#bib.bib78)); Han et al. ([2018a](#bib.bib62)); Hataya and Nakayama
    ([2018](#bib.bib72)); Gupta et al. ([2020](#bib.bib73)); Younesian et al. ([2021](#bib.bib80))
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR(10,100) | 2009 | 60,000 | (10,100) | Du 等 ([2021](#bib.bib46)); Younesian
    等 ([2020](#bib.bib78)); Wei 等 ([2022](#bib.bib54)); Li 等 ([2022](#bib.bib76));
    Shui 等 ([2020](#bib.bib79)); Haußmann 等 ([2019](#bib.bib77)); Gupta 等 ([2020](#bib.bib73));
    Younesian 等 ([2021](#bib.bib80)) | Younesian 等 ([2020](#bib.bib78)); Han 等 ([2018a](#bib.bib62));
    Hataya 和 Nakayama ([2018](#bib.bib72)); Gupta 等 ([2020](#bib.bib73)); Younesian
    等 ([2021](#bib.bib80)) |'
- en: '| Caltech101 | 2004 | 9,000 | 101 | Li et al. ([2022](#bib.bib76)); Yi et al.
    ([2022](#bib.bib71)) | - |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Caltech101 | 2004 | 9,000 | 101 | Li 等 ([2022](#bib.bib76)); Yi 等 ([2022](#bib.bib71))
    | - |'
- en: 'Table 1: Image classification datasets commonly used for deep active learning
    and the training of DL with noisy labels'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：常用于深度主动学习和带有噪声标签的 DL 训练的图像分类数据集
- en: ImageNet and SVHN, being the larger of these datasets, are not well suited for
    DAL because training a single DL model on a large dataset is computationally expensive,
    and takes a lot of time. The computation complexity is worse in the case of DAL
    algorithms due to the iterative nature of the process. Retraining a large model
    over and over on the ImageNet or SVHN datasets is time-consuming. This is reflected
    in the literature by the reluctance of authors to use these large datasets for
    DAL classification, in favor of relatively small datasets such as CALTECH-101
    and CIFAR-100.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet 和 SVHN 作为这些数据集中较大的数据集，并不适合 DAL，因为在大数据集上训练单个 DL 模型计算开销大，耗时也很长。由于 DAL
    算法的迭代特性，计算复杂度更高。在 ImageNet 或 SVHN 数据集上反复训练大型模型非常耗时。这在文献中反映为作者不愿意使用这些大型数据集进行 DAL
    分类，而倾向于使用相对较小的数据集，如 CALTECH-101 和 CIFAR-100。
- en: 'Developing and training algorithms for handling noisy labels follows one of
    two paths: using datasets with noisy labels introduced by one or more of the noise
    sources listed in Section [3.1](#S3.SS1 "3.1 Label Noise ‣ 3 Active learning ‣
    Deep Active Learning in the Presence of Label Noise: A Survey"), or noise-free
    datasets to which measurable label noise is injected by perturbing existing trusted
    labels. In the existing literature, the same datasets (MNIST, CIFAR-100, CALTECH-100)
    commonly used for image classification are used for noisy label classification,
    with a pre-determined probability of swapping each label. This probability is
    also called the noise rate, and the higher it is, the more corrupted the dataset
    becomes after noise injection. In literature, it is common to inject $30\%-60\%$
    random symmetric label noise before training, while keeping a test set that is
    free of label noise.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '处理噪声标签的算法开发和训练遵循两种路径之一：使用在第[3.1](#S3.SS1 "3.1 Label Noise ‣ 3 Active learning
    ‣ Deep Active Learning in the Presence of Label Noise: A Survey")节中列出的一个或多个噪声源引入噪声标签的数据集，或者使用通过扰动现有可信标签而注入可测量标签噪声的无噪声数据集。在现有文献中，常用于图像分类的相同数据集（MNIST、CIFAR-100、CALTECH-100）也用于带噪声标签的分类，标签交换的概率是预先确定的。这个概率也叫做噪声率，噪声率越高，数据集在噪声注入后变得越腐蚀。在文献中，通常在训练前注入$30\%-60\%$的随机对称标签噪声，同时保持一个不含标签噪声的测试集。'
- en: Datasets such as ImageNet with a large number of classes (1000) tend to also
    not be favored for the purpose of evaluating DAL methods that address learning
    under label noise. The reason here is that, with more class labels, the likelihood
    of class-dependent labeling errors at the time the dataset was created is higher.
    The kind of deliberate noise injected into datasets for noisy label learning is
    class-independent and is measurable as opposed to class-dependent noise that may
    be inherent in the data collection and annotation process. Class-dependent label
    noise makes training DL models harder, and there is lower confidence in the correctness
    of the test set labels used for evaluation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 像ImageNet这样具有大量类别（1000）的数据集通常也不适合用于评估解决标签噪声下学习的DAL方法。原因在于，类别标签越多，数据集创建时发生类别相关标记错误的可能性越高。注入到数据集中的有意噪声是类别无关的，并且是可测量的，而类别相关噪声可能在数据收集和标注过程中固有。类别相关标签噪声使得训练深度学习模型变得更加困难，并且对用于评估的测试集标签的正确性信心较低。
- en: 4.2 Evaluation Metrics
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估指标
- en: The general evaluation methodologies for DAL algorithms on noisy labels are
    the same as those used for standard datasets for image classification. Top-1 accuracy
    is the most commonly used metric. Not much consideration is given to the underlying
    class distribution in most existing work and so it would be of interest to explore
    how class imbalances affect DAL algorithms in the presence of label noise. Since
    DAL is also concerned about performance under a budget, it would make sense to
    measure budget efficiency, a measure commonly not well documented in the literature.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 针对带有噪声标签的DAL算法的通用评估方法与用于图像分类的标准数据集相同。Top-1准确率是最常用的度量指标。在现有的大多数工作中，对底层类别分布的考虑并不多，因此探索类别不平衡如何在标签噪声存在的情况下影响DAL算法将是有趣的。由于DAL还关注预算下的性能，因此测量预算效率是有意义的，这是一种在文献中常常未被充分记录的度量。
- en: Given that active learning involves training a model a couple of times for every
    batch of labels received, it becomes obvious that the computational cost of DAL
    algorithms should be a big consideration. In Yoo et al. ([2017](#bib.bib81)),
    Yoo et al. propose the use of a small network for performing query selection so
    that the retraining and labeling cycles run faster. Once the labeling budget is
    exhausted, a larger and more powerful network is then trained using the obtained
    labels. While this approach can be very useful in scenarios where time is of the
    essence, it has a big drawback. The weakness is that using a weaker learner for
    sample selection could lead to lower sample diversity since a weaker learner does
    not perform a very good job of understanding the boundaries between classes in
    the feature space.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于主动学习涉及对每批接收的标签训练模型几次，显然 DAL 算法的计算成本应该是一个重要考虑因素。在 Yoo 等人（[2017](#bib.bib81)）的研究中，Yoo
    等人提议使用小型网络进行查询选择，以便重新训练和标记周期更快。一旦标签预算耗尽，就使用获得的标签训练一个更大、更强大的网络。尽管这种方法在时间至关重要的情况下非常有用，但它有一个重大缺点。弱点在于，使用较弱的学习者进行样本选择可能会导致样本多样性降低，因为较弱的学习者在理解特征空间中类别之间的边界时表现不佳。
- en: In Sinha et al. ([2019b](#bib.bib82)), Signha et al. demonstrate the use of
    transfer learning for fast extraction of useful representations in a DAL setting.
    They show that using large pre-trained models and only fine-tuning the DAL task
    achieves good results with considerably fewer labeled examples. This means that,
    given the same budget, their approach has a higher label efficiency than a model
    trained from scratch. This also means for a given target performance, they require
    less computational resources and time to fit the target, by leveraging good pre-trained
    model weights. The work of Settles Settles ([2009](#bib.bib83)) contains a comprehensive
    survey of the computational cost of active learning algorithms. The main findings
    in this work are that the cost is influenced largely by the dataset size in terms
    of both the number of samples and the size of each sample. Settles also states
    the complexity of the query selection algorithm as well as the number of samples
    per batch are big factors in the total computational cost of DAL. In the case
    of DAL with label noise, it is critical that we have a good understanding of the
    underlying label noise so that the test set remains clean. While this works in
    developing DAL algorithms on well-known datasets, it remains unclear how the test
    set integrity is guaranteed in practice. If the correctness of the test labels
    cannot be guaranteed, evaluation methods such as top-1 accuracy, precision, and
    recall do not offer any reliable measure for the network’s generalization performance.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Sinha 等人（[2019b](#bib.bib82)）的研究中，Signha 等人展示了在 DAL 设置中使用迁移学习以快速提取有用表示的方法。他们表明，使用大型预训练模型并仅对
    DAL 任务进行微调可以在标记样本显著减少的情况下取得良好结果。这意味着，在相同预算下，他们的方法比从头开始训练的模型具有更高的标签效率。这也意味着，对于给定的目标性能，他们需要较少的计算资源和时间来适应目标，通过利用良好的预训练模型权重。Settles
    的工作（[2009](#bib.bib83)）包含了对主动学习算法计算成本的全面调查。这项工作的主要发现是，成本主要受数据集规模的影响，包括样本数量和每个样本的大小。Settles
    还指出，查询选择算法的复杂性以及每批样本的数量是 DAL 总体计算成本中的重要因素。在带有标签噪声的 DAL 情况下，我们对潜在标签噪声的良好理解是至关重要的，以确保测试集保持干净。虽然这在开发
    DAL 算法时在知名数据集上效果良好，但在实践中如何保证测试集的完整性仍然不清楚。如果无法保证测试标签的正确性，评估方法如 top-1 精度、精确度和召回率无法提供对网络泛化性能的可靠衡量。
- en: In this section, we explored datasets and evaluation metrics commonly used in
    comparing DAL algorithms, in particular under the setting of label noise. The
    next section is the main focus of this work. We explore methods leveraging the
    versatility of deep neural networks in the active learning framework where labeling
    budget is an important metric, and we are faced with a noisy label challenge.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了在比较 DAL 算法时常用的数据集和评估指标，特别是在标签噪声的情况下。下一节是本工作的主要重点。我们探讨了在主动学习框架中利用深度神经网络的多样性的方法，其中标签预算是一个重要指标，同时我们面临着标签噪声的挑战。
- en: 5 Deep Active Learning Algorithms for Noisy Labels
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 种用于噪声标签的深度主动学习算法
- en: 'In this section, we focus on the main contribution of this manuscript: exploring
    literature on DAL algorithms used for image classification in the presence of
    label noise. It is worth stating that while literature is rich in theoretical
    approaches for handling label noise in the offline setting, very little has been
    done for active learning algorithms. Methods that address label noise by modeling
    the underlying generative distribution and filtering noisy label examples from
    the training set to achieve better performance are few and in between. We foresee
    a lot of work going into this work and look forward to understanding how iterative
    processes best approximate a noisy label distribution. We are also interested
    in understanding how low sample numbers affect label noise distributions. These
    ideas remain unexplored in literature.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们关注本文的主要贡献：探讨在标签噪声存在的情况下用于图像分类的DAL算法文献。值得指出的是，尽管在离线设置中处理标签噪声的理论方法丰富，但针对主动学习算法的研究非常有限。通过建模潜在生成分布和过滤训练集中噪声标签样本来实现更好性能的方法寥寥无几。我们预见到在这一领域将会有大量的工作投入，并期待了解迭代过程如何最佳地近似噪声标签分布。我们还对了解低样本数量如何影响标签噪声分布感兴趣。这些想法在文献中尚未被探讨。
- en: The methods in this section are predominantly independent of the noise distribution
    and seek noise-robust active training by using customized model architectures,
    loss functions, or training procedures. In Gupta et al. ([2020](#bib.bib73)),
    Gupta et al. propose the use of standard sample diversity and importance query
    policies, supplemented by the model’s confidence scores on samples. They argue
    that DNNs are normally uncertain about the decision boundaries between classes
    very early in training. Training with label noise exacerbates this problem since
    temporary and imaginary boundaries could form based on mislabeled samples, and
    through diversity sampling, get propagated into important query batches that influence
    model uncertainty and sample importance. The authors use the BALD score, introduced
    in Gal et al. ([2017](#bib.bib84)) (not to be confused with the paper Cao and
    Tsang ([2021](#bib.bib85))) as an importance score to ensure the information content
    of samples for labeling per batch is optimal and a good representation of the
    entire dataset. The inclusion of model uncertainty in the sample selection query
    ensures labeled batches will include samples the current model is very uncertain
    about, and these, assuming satisfactory oracle label accuracy, improve the entire
    DAL cycle under label noise.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的方法主要独立于噪声分布，通过使用定制的模型架构、损失函数或训练程序来寻求噪声鲁棒的主动训练。在Gupta等人([2020](#bib.bib73))的研究中，Gupta等人建议使用标准样本多样性和重要性查询策略，并辅以模型对样本的置信度分数。他们认为，DNN通常在训练的早期对类别之间的决策边界不确定。带有标签噪声的训练会加剧这个问题，因为基于错误标记的样本可能会形成临时和虚构的边界，通过多样性采样，这些边界可能会传播到影响模型不确定性和样本重要性的关键查询批次中。作者使用了Gal等人([2017](#bib.bib84))提出的BALD分数（不要与Cao和Tsang([2021](#bib.bib85))的论文混淆）作为重要性分数，以确保每批次标记样本的信息内容是最佳的，并且能够很好地代表整个数据集。样本选择查询中模型不确定性的纳入，确保标记批次将包括当前模型非常不确定的样本，并且这些样本在假设标签准确度令人满意的情况下，能够改善在标签噪声下的整个DAL周期。
- en: 'The inclusion of highly uncertain samples is only one-half of the novelty of
    their approach to robustify learning under a noisy oracle. They include a denoising
    layer to their network. The denoising layer is explained in the following manner:
    The softmax output of the original classifier is fed to the denoising layer, and
    the model is trained on the denoising layer, which represents a non-zero probability
    of predicting a particular label given the true label. This final denoising layer’s
    weights, unlike normal final softmax outputs, are constrained to ensure noisy
    labels have little impact during training. During testing, the penultimate layer’s
    output is used instead of the denoising layer for prediction. In this way, the
    denoising layer serves as a rigorous teacher to the student, becoming a noise-robust
    model used in testing and deployment.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 包含高度不确定的样本只是他们在噪声 oracle 下稳健学习方法新颖性的半部分。他们在网络中加入了一个去噪层。去噪层的解释如下：将原始分类器的 softmax
    输出输入到去噪层，模型在去噪层上进行训练，去噪层表示在给定真实标签的情况下预测特定标签的非零概率。这个最终的去噪层的权重，与正常的最终 softmax 输出不同，被限制以确保在训练期间噪声标签的影响很小。在测试时，使用倒数第二层的输出而不是去噪层进行预测。通过这种方式，去噪层作为一个严格的老师，为学生提供了一个噪声鲁棒的模型，用于测试和部署。
- en: 'Gupta et al. show that their method, in the noise-free setting achieves similar
    performance to the common baseline DAL methods, such as the original BALD Cao
    and Tsang ([2021](#bib.bib85)), core-set, entropy-based selection, and random
    sample selection on the MNIST, CIFAR10, and SVHN datasets. We attribute these
    results purely to the addition of model uncertainty to diversity and information
    gain in selecting samples. We argue the denoising layer as described in the paper
    would serve no purpose if the oracle provides only clean labels and so the results
    in the noise-free setting would be better stated as: “no performance loss or gain"
    from adding the denoising layer in the noise-free setting. At $10\%$ and $30\%$
    label noise, their method outperforms the reported standard benchmarks on all
    of the three datasets, speaking to the effectiveness of their denoising mechanism.
    While these are good results, they fail to demonstrate how the approach compares
    to similar state-of-the-art DAL methods tailored for noisy labels, and how adding
    the same denoising layer to DAL ResNets trained under entropy only or random sample
    selection compares to their approach. The paper also lacks details on the training
    setup that is important for reproducibility, such as the hardware used, the exact
    deep learning architecture, whether pre-trained weights are used or not, and the
    number of training epochs.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Gupta 等人展示了他们的方法在无噪声设置下，表现与常见的基线 DAL 方法类似，例如原始的 BALD Cao 和 Tsang ([2021](#bib.bib85))、核心集、基于熵的选择和随机样本选择，在
    MNIST、CIFAR10 和 SVHN 数据集上。我们将这些结果完全归因于在选择样本时加入模型不确定性以增强多样性和信息增益。我们认为，文中描述的去噪层如果
    oracle 仅提供干净标签则没有意义，因此无噪声设置下的结果更准确地表述为：“在无噪声设置中，添加去噪层没有性能损失或提升”。在 $10\%$ 和 $30\%$
    标签噪声下，他们的方法在所有三个数据集上都优于报告的标准基准，证明了他们去噪机制的有效性。虽然这些结果很好，但它们未能展示该方法如何与为噪声标签量身定制的类似最先进的
    DAL 方法进行比较，以及将相同的去噪层添加到仅基于熵或随机样本选择的 DAL ResNets 上与他们的方法进行比较。该论文还缺乏对训练设置的重要细节，如使用的硬件、确切的深度学习架构、是否使用了预训练权重以及训练周期的数量，这些都是重现性的关键。
- en: Similar to Gupta et al. ([2020](#bib.bib73)), in Younesian et al. ([2020](#bib.bib78)),
    Younesian et al. introduce a DAL framework (DuoLab) for training on noisy labels
    using weak and strong oracles. CIFAR10 and CIFAR100 are used in training and testing
    a CNN, with $30\%$ and $60\%$ label noise. They adopt similar criteria to Gupta
    et al. ([2020](#bib.bib73)) for query selection, namely using information gain
    and uncertainty. The weak and strong oracles refer to the innate differences in
    human labelers’ generalization abilities and labeling quality. It is assumed that
    weak oracles are cheaper and more likely to produce incorrect labels than strong
    oracles, and so this approach’s novelty is particularly more interesting in the
    real-world setting where there is always a need to reduce labeling fees paid to
    the oracles. However, the use of two oracles seems to not affect the overall performance
    of the DAL classifier in their work, but rather gives budget subsidies.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Gupta 等人 ([2020](#bib.bib73)) 类似，Younesian 等人 ([2020](#bib.bib78)) 引入了一个用于训练噪声标签的
    DAL 框架（DuoLab），利用弱和强神谕。CIFAR10 和 CIFAR100 被用于训练和测试 CNN，标签噪声分别为 $30\%$ 和 $60\%$。他们采用了类似于
    Gupta 等人 ([2020](#bib.bib73)) 的查询选择标准，即使用信息增益和不确定性。弱神谕和强神谕指的是人类标注者在泛化能力和标注质量上的固有差异。假设弱神谕更便宜，更容易产生错误标签，而强神谕则相对较少产生错误标签，因此该方法的创新性在于现实世界中总是需要减少支付给神谕的标注费用。然而，在他们的工作中，使用两个神谕似乎并未影响
    DAL 分类器的整体性能，而是提供了预算补贴。
- en: When it comes to dealing with noisy labels, instead of robustifying their model,
    Younesian et al. approach the problem by filtering out samples suspected to have
    noisy labels. Their DAL approach starts with an initially labeled dataset used
    to train the classifier, but they deviate from the conventional use of a random
    batch of samples to perform this initial training. Their overall approach hinges
    on a key and possibly flawed assumption that there exists a small and clean batch
    of training examples that can be used to initially train the model. While in practice
    we can optimistically assume it is possible to push physical data and labeling
    boundaries so this initial clean set is available, the paper lacks the minimum
    theoretical guarantees in the case we are unable to say with “100%" certainty
    that specific labels from the oracle are correct. If we were to initially assume
    some labels are “100%" correct, the paper does not make it clear as to how the
    correctness of such labels is verified given that the oracles have a noise rate
    of up to $60\%$. Once the model is trained on the initial clean samples, the model
    predicts the classes of all samples in the unlabeled pool, and the model’s confidence
    score on the top 2 classes is used in deciding whether a sample is noisy or not.
    They measure the difference in top-2 class probabilities for each sample, and
    declare the top-k samples with the lowest margin as potentially noisy and so not
    fit for training the network.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 处理噪声标签时，与其增强模型的鲁棒性，Younesian 等人选择通过过滤掉怀疑存在噪声标签的样本来解决问题。他们的 DAL 方法从一个初始标记的数据集开始训练分类器，但他们偏离了传统的使用随机样本批次进行初始训练的方法。他们的整体方法依赖于一个关键且可能存在缺陷的假设，即存在一个小而干净的训练样本批次，可以用来最初训练模型。虽然在实践中我们可以乐观地假设有可能推动物理数据和标签的边界，从而使初始的干净样本集可用，但该论文在无法以“100%”的确定性说出来自神谕的具体标签是否正确的情况下，缺乏最低限度的理论保证。如果我们最初假设某些标签是“100%”正确的，论文没有明确说明在神谕的噪声率高达
    $60\%$ 的情况下，如何验证这些标签的正确性。一旦模型在初始干净样本上训练完成，模型将预测未标记池中所有样本的类别，并使用模型在前 2 个类别上的置信度分数来决定样本是否为噪声。他们测量每个样本的前
    2 个类别概率的差异，并将前 k 个 margin 最低的样本声明为潜在噪声，因此不适合用来训练网络。
- en: It becomes obvious that this approach will lead to a number of false positives
    and false negatives, and are likely to affect the overall performance of the model
    in classes that are hard to tell apart. Another issue is that a model trained
    on a very small subset of a large dataset can display a high top-1 class prediction
    confidence while being totally wrong if it has seen very little to no examples
    of a class in training. They combat this by reusing the noisy labels once all
    the clean examples are exhausted. The noisy samples are clustered based on the
    trained model’s penultimate representation of each noisy sample. The samples within
    each cluster are then ranked based on their informativeness, and the top $K$ samples
    per cluster are picked and used to further train the model. The training batches
    on these samples are not random. They are ordered so that the most informative
    samples are in the first training batch and the least informative are in the last
    batch. It is unclear how this process circumvents the need for actual ground truth
    labels, and how this further training on noisy labels does not negatively affect
    the performance of a model first trained on clean labels. Reported test accuracy
    results show that DuoLab outperforms noise-resilient baselines on both CIFAR 10
    and 100 with $30\%$ label noise.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这种方法会导致一定数量的假阳性和假阴性，并可能影响模型在难以区分的类别上的整体性能。另一个问题是，训练于大数据集的非常小子集上的模型可能在顶级类别预测的置信度很高，但如果训练时几乎没有看到该类别的样本，它的预测可能完全错误。他们通过在所有干净样本用尽后重新使用噪声标签来应对这一问题。噪声样本根据训练模型对每个噪声样本的倒数第二层表示进行聚类。然后，按信息量对每个聚类中的样本进行排名，每个聚类中的前$K$个样本被选中用于进一步训练模型。这些样本的训练批次并非随机的，而是有序的，使得最具信息量的样本在首批训练中，信息量最少的样本在最后一批中。尚不清楚这一过程如何绕过对实际真实标签的需求，以及如何使得在噪声标签上进行的进一步训练不会对最初在干净标签上训练的模型性能产生负面影响。报告的测试准确率结果表明，DuoLab在CIFAR
    10和100上的表现优于噪声鲁棒基线，标签噪声为$30\%$。
- en: 'In Younesian et al. ([2021](#bib.bib80)), Younesian et al. propose QActor,
    an approach that follows the same idea as their earlier work in Younesian et al.
    ([2020](#bib.bib78)): Identifying and reusing possibly noisy labels. Over and
    above this, the paper introduces a noise-aware informative measure, and they demonstrate
    for the first time how dynamic allocation of the labeling budget per query leads
    to better performance as compared to the convention of equal distribution of the
    budget across the number of query cycles. In this paper, it is assumed that a
    small set of clean initial training data, as well as a clean test data, are readily
    available. The DAL algorithm decides which samples are to be sent to the oracle
    for labeling. They also leave it to the DAL algorithms to decide how many of the
    samples fit the selection criteria and are labeled per iteration. On each iteration,
    a batch of highly informative samples as measured by entropy is sent to the oracle
    for labeling, and then the labels are compared to the current model’s predictions.
    Samples, where there is a disagreement, are sent to a suspicious data collection.
    These samples are later ranked on informativeness, and resent to the oracle for
    relabeling, with the previously assigned label kept in mind. The authors argue
    that relabeling samples that the model is uncertain about and are potentially
    incorrectly labeled by the oracle leads to the highest gain in model performance
    since these are likely samples from classes that are easy to mix up.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在Younesian等人（[2021](#bib.bib80)）的研究中，Younesian等人提出了QActor，这种方法沿用了他们在Younesian等人（[2020](#bib.bib78)）的早期工作中的相同思路：识别和重用可能噪声标签。除此之外，本文介绍了一种噪声感知的信息量度，并首次展示了如何动态分配每个查询的标记预算，以相比于将预算均等分配到查询周期中的常规方法，能够带来更好的性能。本文假设有一小部分干净的初始训练数据和干净的测试数据是
    readily available 的。DAL算法决定哪些样本应发送给oracle进行标记。他们还将决定每次迭代中符合选择标准的样本数量以及标记的任务交给DAL算法。在每次迭代中，一批根据熵测量为高度信息量的样本被送到oracle进行标记，然后将标签与当前模型的预测进行比较。那些存在分歧的样本被送到可疑数据集合中。这些样本随后根据信息量进行排名，并重新发送给oracle进行重新标记，同时保留之前分配的标签。作者认为，对模型不确定且可能被oracle错误标记的样本进行重新标记能带来模型性能的最大提升，因为这些样本可能是易于混淆的类别样本。
- en: 'In Huang et al. ([2016](#bib.bib86)), Huang et al. approach DAL under oracle
    noise in a way that is different from most of the work discussed thus far. The
    complexity analysis performed in their work proves DAL is possible and viable
    with oracle epiphany. They explain that empirical evidence has shown that oracles
    are likely to delay providing labels on samples they are unsure about until more
    related examples are presented to the oracle, at which point they have an epiphany,
    and are able to provide a more confident label to such examples. Having had the
    experience of hand-labeling 1000 images from a large fisheries dataset, we agree
    with the authors that oracle abstention and epiphany are realistic considerations.
    An interesting idea mentioned in the paper is adding one more possible class label
    for a classification problem with $N$ classes so that the oracle has $N+1$ possible
    classes to choose from. This class label can be one of: "I don’t know" or "unsure".
    Adding this choice relieves the oracle of the urge to guess between two or three
    most likely label assignments for a hard sample image, hence increasing the overall
    confidence in the correctness of the class labels that are actually provided.
    This is true since in a perfect world the oracle has to be the source of absolute
    truth, and so we are not interested in cases where the oracle guesses correctly.
    An oracle guessing would be useful if they are allowed to provide a measure of
    how confident they are of their guess.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在黄等人（[2016](#bib.bib86)）的研究中，黄等人以一种不同于此前大多数讨论的工作的方法处理了具有神谕噪声的DAL。他们的复杂性分析证明了在神谕启示下，DAL是可能和可行的。他们解释说，实证证据表明，神谕通常会延迟对不确定样本提供标签，直到更多相关样本呈现给神谕，此时它们会有启示，并能为这些样本提供更有信心的标签。在从一个大型渔业数据集中手动标记1000张图像的经验基础上，我们同意作者的观点，即神谕的回避和启示是现实的考虑因素。论文中提到的一个有趣的想法是，为具有$N$类的分类问题增加一个可能的类别标签，使得神谕有$N+1$个可能的类别选择。这个类别标签可以是："我不知道"
    或 "不确定"。添加这一选择可以减轻神谕在面对难以标记的样本图像时在两个或三个最可能的标签之间猜测的冲动，从而提高实际提供的类别标签的整体正确性。在一个理想的世界中，神谕必须是绝对真理的源泉，因此我们不关心神谕是否能正确猜测的情况。如果允许神谕提供对其猜测的信心度量，那么神谕的猜测将会更有用。
- en: 'In their approach, they use Markov chains to model when epiphany occurs for
    a certain class, that was previously hard to label to the oracle. The two main
    assumptions made in this work are: 1) The oracle is honest, and 100% accurate
    on the samples he/she decides to label anything other than “unsure". 2) Given
    the oracle is honest, all samples avoided will have correct labels once epiphany
    occurs. For both assumptions, the authors further assume once an epiphany occurs,
    no drastic changes in the oracle’s assignment of labels will occur. While these
    assumptions sound reasonable, it is worth stating that in the real world, a time-constrained
    oracle optimizing for earnings is unlikely to avoid assigning labels to samples
    they are unsure about. This is especially true if they learn earlier on that samples
    they label "unsure" will always return, requiring more of their time. The assumption
    that the oracle is 100% accurate on examples they label is also very unrealistic
    and does not factor in the wide-ranging spectrum of human capabilities in labeling.
    It would be more useful if the authors stated how 100% oracle label accuracy would
    be guaranteed or tested post-epiphany. Comparing this approach to the works Younesian
    et al. ([2021](#bib.bib80), [2020](#bib.bib78)) of Younesian et al. a spectrum
    of computer-human involvement in label noise filtration can be drawn. At one end
    Huang et al. use the oracle as a sole decider of label uncertainty, and in a more
    hybrid setting, Younesian et al. use the currently trained classifier’s confidence
    score together with the oracle’s label as a filter for potentially noisy labels.
    The model-only approach to noise filtration is using the confidence score margin
    of the top-2 predicted classes as used in Gupta et al. ([2020](#bib.bib73)). In
    both Younesian et al. and Huang et al.’s approaches, the “uncertain" samples are
    sent back for relabeling.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的方法中，他们使用马尔可夫链来建模某个类别的顿悟何时发生，这在以前很难标记给oracle。该工作的两个主要假设是：1）oracle是诚实的，并且对他/她决定标记为“尚不确定”以外的样本100%准确。2）鉴于oracle是诚实的，一旦发生顿悟，所有被避免的样本将会有正确的标签。对于这两个假设，作者进一步假设一旦发生顿悟，oracle的标签分配将不会发生剧烈变化。虽然这些假设听起来合理，但值得指出的是，在现实世界中，一个时间受限的oracle优化其收益不太可能避免为他们不确定的样本分配标签。特别是如果他们早期了解到他们标记为“尚不确定”的样本总是会返回，要求更多的时间。假设oracle对他们标记的样本100%准确也是非常不现实的，没有考虑到人类在标记方面的广泛能力。作者如果说明如何保证或测试顿悟后的100%
    oracle标签准确性会更有用。将这种方法与Younesian等人（[2021](#bib.bib80), [2020](#bib.bib78)）的工作进行比较，可以绘制出计算机与人类在标签噪声过滤中的参与谱。在一个极端，Huang等人将oracle作为标签不确定性的唯一决策者，而在更混合的设置中，Younesian等人结合当前训练的分类器的置信度评分与oracle的标签作为潜在噪声标签的过滤器。仅模型方法的噪声过滤使用了Gupta等人（[2020](#bib.bib73)）使用的前两类预测的置信度评分边际。在Younesian等人和Huang等人方法中，“不确定”的样本会被送回重新标记。
- en: In Amin et al. ([2021](#bib.bib87)), Amin et al. present the dual-purpose learning
    framework. While they do not explicitly focus on nor address label noise, their
    combined DAL, and abstention learning approaches provide valuable insight into
    the intricacies of DAL and abstention, which are important components that are
    not discussed as rigorously in Huang et al. ([2016](#bib.bib86)). The two bodies
    of work also differ in that Amin et al. investigate the generalization bounds
    of the DAL and abstention setting and find interestingly that even with an unlimited
    labeling budget and no labeling noise, the upper bound on the observable generalization
    loss that exists in the passive learning case can not be guaranteed while oracle
    abstention is at play. The authors however do not provide empirical comparisons
    of their method to state-of-the-art DAL algorithms, nor do they detail how their
    unique dual method would impact performance when applied to such highly-performant
    algorithms. The work of Yan et al. Yan et al. ([2016](#bib.bib88)) is a more general
    approach to the work of Amin et al. since they consider DAL under imperfect labelers,
    and allow for abstention. Yan et al. go on to show that under strict assumptions
    on the dimensionality of the decision boundary, abstention, and noise rates close
    to the decision boundaries, their method generalizes the lower bounds on algorithms
    such as Amin et al. ([2021](#bib.bib87)). A significant contribution of their
    work is in demonstrating that their algorithm need not be aware of either the
    label noise rate or abstention rate. With restrictions on how label noise and
    the rate of abstention are distributed around the decision boundaries, their algorithm
    performs significantly better than prior methods.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amin等人（[2021](#bib.bib87)）的研究中，Amin等人提出了双重目的学习框架。尽管他们并未明确关注或解决标签噪声问题，但他们结合的DAL和弃权学习方法提供了对DAL和弃权复杂性的宝贵见解，这些是Huang等人（[2016](#bib.bib86)）未严格讨论的重要组成部分。这两个研究也有不同之处，Amin等人调查了DAL和弃权设置的一般化界限，并有趣地发现，即使在无限标签预算和没有标签噪声的情况下，处于被动学习情况下的可观测一般化损失的上界也无法得到保证，尤其是当使用oracle弃权时。然而，作者没有提供其方法与最先进DAL算法的经验比较，也没有详细说明其独特的双重方法在应用于这些高性能算法时会如何影响性能。Yan等人（[2016](#bib.bib88)）的工作是对Amin等人工作的更一般性方法，因为他们考虑了在不完美标注者下的DAL，并允许弃权。Yan等人进一步展示了在对决策边界的维度、弃权和接近决策边界的噪声率的严格假设下，他们的方法能够推广到类似Amin等人（[2021](#bib.bib87)）的算法的下界。他们工作的一个重要贡献是证明了他们的算法无需了解标签噪声率或弃权率。在标签噪声和弃权率在决策边界周围分布的限制下，他们的算法表现显著优于以前的方法。
- en: 6 Conclusion and Future Research Directions
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来研究方向
- en: 'To summarize the literature: (1) A lot of work has been done on training DL
    models on noisy labels in an offline setting. (2) The literature on DAL methods
    is also very rich in the case of no-label noise. (3) While this is an area of
    research that has very practical applications and financial impact, there is very
    little work done at the intersection of DAL and label noise. It is worth noting
    that the power of ViTs has not been explored in DAL as much as it has been in
    fully labeled image classification datasets, and so we see huge potential in improving
    DAL by leveraging unique characteristics of ViTs in image classification tasks.
    The exploration of the transformer layers and how attention can be used in understanding
    diversity, importance, and uncertainty is especially intriguing to us. In Section
    [3](#S3 "3 Active learning ‣ Deep Active Learning in the Presence of Label Noise:
    A Survey"), works using self-supervised pre-training are presented that attain
    good lower-dimensional representations of the images. These have been shown to
    lead to a good core set of samples for labeling in an active learning framework.
    It is of high importance that contrastive learning methods are explored further
    as methods for deriving good image representations that can then be used in improving
    the diversity, importance, and uncertainty-based selection in queries sent to
    an oracle for labeling.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '总结文献： (1) 许多工作已经在离线环境中处理噪声标签的深度学习模型训练上完成。 (2) 在无标签噪声的情况下，关于深度主动学习（DAL）方法的文献也非常丰富。
    (3) 尽管这是一个具有实际应用和财务影响的研究领域，但在深度主动学习和标签噪声交集处的工作非常有限。值得注意的是，视觉 Transformer（ViTs）的能力在深度主动学习中的应用还没有像在完全标注的图像分类数据集那样被充分探索，因此我们看到通过利用
    ViTs 在图像分类任务中的独特特性来提升深度主动学习的巨大潜力。对 Transformer 层的探索以及如何利用注意力机制理解多样性、重要性和不确定性对我们特别吸引。在第
    [3](#S3 "3 Active learning ‣ Deep Active Learning in the Presence of Label Noise:
    A Survey") 节中，使用自监督预训练的工作展示了图像的良好低维表示。这些表示被证明能在主动学习框架中提供良好的核心样本集。进一步探索对比学习方法作为获得良好图像表示的手段非常重要，这些表示可以用于提高多样性、重要性和基于不确定性的查询选择，以便发送到标注的
    oracle。'
- en: 'In terms of DAL for noisy labels, the works of Younesian et al, Huang et al.
    and Gupta et al. described in Section [5](#S5 "5 Deep Active Learning Algorithms
    for Noisy Labels ‣ Deep Active Learning in the Presence of Label Noise: A Survey")
    represent methods and ideas with substantial impact in DAL. In all these approaches,
    filtration of noise is well addressed, but with a lot of questionable assumptions.
    Establishing unified DAL and label noise benchmarks and datasets would add a lot
    of value and ensure future methods conform to realistic assumptions about the
    training process and the oracle. In all methods addressing this problem, only
    CNNs are used and the performance is only compared to baseline DAL methods as
    opposed to a more convincing comparison to state-of-the-art DAL methods. We would
    like to be able to perform the same analysis on DAL methods for label noise using
    ViTs against the best DAL methods. This is especially important since ViTs have
    been the most dominant architectural choice for image classification in recent
    times. It is also critical that substantial effort is put into understanding how
    key assumptions made in DAL methods for noisy labels affect results, and establish
    convergence and complexity guarantees mathematically. This means that it is not
    adequate to only understand DAL on noisy labels through experimentation, but also
    some effort needs to be put into establishing convergence guarantees at a theoretical
    level.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '在处理噪声标签的深度主动学习（DAL）方面，第 [5](#S5 "5 Deep Active Learning Algorithms for Noisy
    Labels ‣ Deep Active Learning in the Presence of Label Noise: A Survey") 节中描述的
    Younesian 等、Huang 等和 Gupta 等的工作代表了在深度主动学习中具有重要影响的方法和思想。在所有这些方法中，噪声的过滤得到了很好的解决，但有很多值得怀疑的假设。建立统一的深度主动学习和标签噪声基准及数据集将增加很多价值，并确保未来的方法符合对训练过程和
    oracle 的现实假设。在所有解决此问题的方法中，仅使用了 CNNs，性能也只是与基线深度主动学习方法进行比较，而不是与最先进的深度主动学习方法进行更有说服力的比较。我们希望能够对使用
    ViTs 处理标签噪声的深度主动学习方法进行相同的分析，并与最好的深度主动学习方法进行对比。这一点尤其重要，因为 ViTs 最近在图像分类中是最主流的架构选择。了解深度主动学习方法对噪声标签的关键假设如何影响结果，并在理论上建立收敛性和复杂性保证也至关重要。这意味着，仅通过实验来理解处理噪声标签的深度主动学习是不够的，还需要在理论层面上做一些工作来建立收敛性保证。'
- en: In all existing domain literature, to the best of our knowledge, the oracle
    gets no feedback. It would be very interesting to explore how a cyclical feedback
    loop between the oracle and the model improves both of them. Intuitively we hypothesize
    it would help to have tips generated by the model in an unsupervised manner accessible
    to the oracle in cases of difficult samples. A step closer to the ideal interaction
    is allowing for abstention and epiphany. One way to do this is through contrastive
    learning. We hypothesize that, since contrastive learning models, through rotation
    or patch prediction for example, can be trained with no actual problem domain
    labels and thus noise-free, the representations learned from this step can be
    used in proposing similar samples to an oracle before they can abstain from labeling
    a sample. In this manner, the oracle may reach epiphany much earlier than they
    would in the frameworks listed in the literature. We would also consider answering
    the question of whether a real human annotator with a known or unknown noise rate
    can improve, and by how much in this setting.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有现有领域文献中，据我们所知，oracle 没有收到反馈。探索 oracle 与模型之间的循环反馈环如何改善两者将非常有趣。我们直觉上假设，通过旋转或补丁预测等方式训练的对比学习模型，可以在没有实际问题领域标签的情况下训练，从而没有噪声，这样从这一步学到的表示可以用于在
    oracle 可以弃权之前向其提出类似样本。通过这种方式，oracle 可能比在文献中列出的框架中更早地获得顿悟。我们还会考虑回答一个问题，即一个已知或未知噪声率的真实人类标注者是否能在这种设置下改善，并且改善的程度如何。
- en: Since reproducibility and robustness are key factors in the ongoing development
    of DL and AL, we are interested in performing extensive training of out-of-the-box
    DL models in the active learning framework in the presence of label noise. This
    will not only ensure researchers know what to expect out of different models without
    any hyperparameter tuning but also set up benchmarks that are specific to DAL
    with label noise. We are interested in covering as many image classification datasets
    as possible, using multiple CNN and ViT-based models, different AL algorithms,
    different noise handling algorithms, as well as different loss functions. This
    is vital to the field as it encourages clear and concise statements about the
    mode and training conditions, and ensures future methods are to be compared on
    a level playing field.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可重复性和鲁棒性是深度学习（DL）和主动学习（AL）持续发展的关键因素，我们有兴趣在存在标签噪声的情况下，对开箱即用的DL模型进行广泛的训练。这不仅确保研究人员了解不同模型在没有任何超参数调整的情况下的预期效果，还建立了特定于有标签噪声的DAL的基准。我们希望涵盖尽可能多的图像分类数据集，使用多种CNN和ViT基础模型、不同的AL算法、不同的噪声处理算法以及不同的损失函数。这对该领域至关重要，因为它鼓励对模式和训练条件作出清晰而简洁的陈述，并确保未来的方法在一个公平的环境中进行比较。
- en: Lastly, a key consideration is in computation. Most work in the area does not
    explicitly state and discuss the computational complexity of the methods. In a
    world ever so gravitating towards lowering carbon footprint, it is important we
    are able to assess DAL methods on noisy labels not only on their accuracy but
    also on their computational requirement. This is an interesting avenue of research
    if one considers how recent work on training large language models has shown there
    are considerable trade-offs and gains to be made in the computational operations
    required, model performance, architectural choices as well as representation size.
    Some of these have gone in the face of established results Xiaoqi et al. ([2020](#bib.bib89));
    Clark et al. ([2020](#bib.bib90)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个关键的考虑因素是计算。在这一领域的大多数工作并未明确说明和讨论方法的计算复杂性。在一个日益关注降低碳足迹的世界中，我们需要能够在准确性之外评估噪声标签上的DAL方法的计算需求。如果考虑到近期关于训练大型语言模型的工作表明，在计算操作需求、模型性能、架构选择以及表示大小方面存在显著的权衡和收益，这将是一个有趣的研究方向。其中一些结果挑战了已建立的结论
    Xiaoqi 等 ([2020](#bib.bib89)); Clark 等 ([2020](#bib.bib90))。
- en: References
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ivakhnenko and Lapa [1965] G.V. Ivakhnenko and V.M Lapa. The group method of
    data handling. *Automation and remote control*, 26(6):895–902, 1965.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivakhnenko 和 Lapa [1965] G.V. Ivakhnenko 和 V.M. Lapa。《数据处理的组方法》。*自动化与遥控*，26(6):895–902，1965。
- en: 'Rosenblatt [1958] F. Rosenblatt. The perceptron: A probabilistic model for
    information storage and organization in the brain. In *Psychological Review.*,
    volume 65, pages 386–408, 1958.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rosenblatt [1958] F. Rosenblatt. 感知机: 一种用于信息存储和大脑组织的概率模型。发表于 *心理学评论*，第65卷，页码386–408，1958年。'
- en: Breiman [2001] L. Breiman. Random forests. *Machine Learning Journal*, 45:5–32,
    2001. doi:[10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Breiman [2001] L. Breiman. 随机森林。*机器学习期刊*，45:5–32，2001。doi:[10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324)。
- en: 'Chen and Guestrin [2016] T. Chen and C. Guestrin. Xgboost: A scalable tree
    boosting system. In *Proceedings of the 22nd ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining*, pages 785–794, 08 2016. doi:[10.1145/2939672.2939785](https://doi.org/10.1145/2939672.2939785).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen and Guestrin [2016] T. Chen 和 C. Guestrin. Xgboost: 一种可扩展的树提升系统。发表于 *第22届ACM
    SIGKDD国际知识发现与数据挖掘大会*，页码785–794，2016年8月。doi:[10.1145/2939672.2939785](https://doi.org/10.1145/2939672.2939785)。'
- en: 'Prokhorenkova et al. [2018] L. Prokhorenkova, G. Gleb, A. Vorobev, A.V. Dorogush,
    and A. Gulin. Catboost: unbiased boosting with categorical features. In S. Bengio,
    H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors,
    *Proceedings of the 32nd International Conference on Neural Information Processing
    Systems*, volume 31, page 6639–6649\. Curran Associates, Inc., 2018.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Prokhorenkova et al. [2018] L. Prokhorenkova, G. Gleb, A. Vorobev, A.V. Dorogush
    和 A. Gulin. Catboost: 使用类别特征的无偏提升。发表于 S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
    N. Cesa-Bianchi 和 R. Garnett 编辑的 *第32届国际神经信息处理系统大会论文集*，第31卷，页码6639–6649。Curran
    Associates, Inc.，2018。'
- en: 'Schäfl et al. [2022] B. Schäfl, L. Gruber, A. Bitto-Nemling, and S. Hochreiter.
    Hopular: Modern hopfield networks for tabular data. *ArXiv*, abs/2206.00664, 2022.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schäfl et al. [2022] B. Schäfl, L. Gruber, A. Bitto-Nemling 和 S. Hochreiter.
    Hopular: 现代Hopfield网络用于表格数据。*ArXiv*，abs/2206.00664，2022。'
- en: Roman et al. [2022] L. Roman, C. Valeriia, S. Avi, B. Arpit, C. Bruss, G. Tom,
    W. Andrew, and G. Micah. Transfer learning with deep tabular models. 06 2022.
    doi:[10.48550/arXiv.2206.15306](https://doi.org/10.48550/arXiv.2206.15306).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roman et al. [2022] L. Roman, C. Valeriia, S. Avi, B. Arpit, C. Bruss, G. Tom,
    W. Andrew 和 G. Micah. 深度表格模型的迁移学习。2022年6月。doi:[10.48550/arXiv.2206.15306](https://doi.org/10.48550/arXiv.2206.15306)。
- en: Popov et al. [2020] S. Popov, S. Morozov, and A. Babenko. Neural oblivious decision
    ensembles for deep learning on tabular data. *ArXiv*, abs/1909.06312, 2020.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Popov et al. [2020] S. Popov, S. Morozov 和 A. Babenko. 用于表格数据深度学习的神经遗忘决策集。*ArXiv*，abs/1909.06312，2020。
- en: 'Arik and Pfister [2021] S.O. Arik and T. Pfister. Tabnet: Attentive interpretable
    tabular learning. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 35, pages 6679–6687, 2021. doi:[10.1609/aaai.v35i8.16826](https://doi.org/10.1609/aaai.v35i8.16826).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arik and Pfister [2021] S.O. Arik 和 T. Pfister. Tabnet: 具有注意力机制的可解释表格学习。发表于
    *AAAI人工智能会议论文集*，第35卷，页码6679–6687，2021。doi:[10.1609/aaai.v35i8.16826](https://doi.org/10.1609/aaai.v35i8.16826)。'
- en: 'Baohua et al. [2019] S. Baohua, Y. Lin, Z. Wenhan, L. Michael, D. Patrick,
    Y. Charles, and D. Jason. Supertml: Two-dimensional word embedding for the precognition
    on structured tabular data. In *2019 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition Workshops*, pages 2973–2981, 06 2019. doi:[10.1109/CVPRW.2019.00360](https://doi.org/10.1109/CVPRW.2019.00360).'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baohua et al. [2019] S. Baohua, Y. Lin, Z. Wenhan, L. Michael, D. Patrick,
    Y. Charles 和 D. Jason. Supertml: 用于结构化表格数据的二维词嵌入预测。发表于 *2019 IEEE/CVF计算机视觉与模式识别会议工作坊*，页码2973–2981，2019年6月。doi:[10.1109/CVPRW.2019.00360](https://doi.org/10.1109/CVPRW.2019.00360)。'
- en: 'Pennington et al. [2014] J. Pennington, R. Socher, and C. Manning. Glove: Global
    vectors for word representation. In *Proceedings of the 2014 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)*, pages 1532–1543, 10 2014. doi:[10.3115/v1/D14-1162](https://doi.org/10.3115/v1/D14-1162).
    URL [http://www.aclweb.org/anthology/D14-1162](http://www.aclweb.org/anthology/D14-1162).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pennington et al. [2014] J. Pennington, R. Socher 和 C. Manning. Glove: 全球词向量表示。发表于
    *2014年自然语言处理经验方法会议（EMNLP）*，页码1532–1543，2014年10月。doi:[10.3115/v1/D14-1162](https://doi.org/10.3115/v1/D14-1162)。网址
    [http://www.aclweb.org/anthology/D14-1162](http://www.aclweb.org/anthology/D14-1162)。'
- en: 'Novák et al. [2020] A. Novák, L. Laki, and B. Novák. CBOW-tag: a modified CBOW
    algorithm for generating embedding models from annotated corpora. In *Proceedings
    of the Twelfth Language Resources and Evaluation Conference*, pages 4798–4801\.
    European Language Resources Association, 09 2020. ISBN 979-10-95546-34-4. URL
    [https://aclanthology.org/2020.lrec-1.590](https://aclanthology.org/2020.lrec-1.590).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Novák 等 [2020] A. Novák, L. Laki, 和 B. Novák. CBOW-tag: 一种用于从注释语料库生成嵌入模型的修改版
    CBOW 算法. 载于 *第十二届语言资源与评估会议论文集*，第 4798–4801 页。欧洲语言资源协会，2020 年 9 月。ISBN 979-10-95546-34-4。网址
    [https://aclanthology.org/2020.lrec-1.590](https://aclanthology.org/2020.lrec-1.590)。'
- en: Grave et al. [2018] E. Grave, P. Bojanowski, P. Gupta, A. Joulin, and T. Mikolov.
    Learning word vectors for 157 languages. In *Proceedings of the International
    Conference on Language Resources and Evaluation (LREC 2018)*, 2018.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grave 等 [2018] E. Grave, P. Bojanowski, P. Gupta, A. Joulin, 和 T. Mikolov. 为
    157 种语言学习词向量. 载于 *Proceedings of the International Conference on Language Resources
    and Evaluation (LREC 2018)*，2018 年。
- en: Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In *Advances
    in Neural Information Processing Systems*, volume 30, 2017. URL [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. Gomez, L. Kaiser, 和 I. Polosukhin. Attention is all you need. 载于 *Advances
    in Neural Information Processing Systems*，第 30 卷，2017 年。网址 [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)。
- en: 'See et al. [2017] A. See, P. Liu, and C. Manning. Get to the point: Summarization
    with pointer-generator networks. In *Proceedings of the 55th Annual Meeting of
    the Association for Computational Linguistics*, volume 1, pages 1073–1083, 2017.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: See 等 [2017] A. See, P. Liu, 和 C. Manning. 直击要点：使用指针生成网络进行摘要. 载于 *第 55 届计算语言学协会年会会议录*，第
    1 卷，第 1073–1083 页，2017 年。
- en: Hochreiter and Schmidhuber [1997] S. Hochreiter and J. Schmidhuber. Long short-term
    memory. In *Neural Computation*, volume 9, pages 1735–1780, 1997.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber [1997] S. Hochreiter 和 J. Schmidhuber. 长短期记忆. 载于 *Neural
    Computation*，第 9 卷，第 1735–1780 页，1997 年。
- en: 'Artacho and Savakis [2020] B. Artacho and A.E. Savakis. Unipose: Unified human
    pose estimation in single images and videos. In *2020 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, pages 7033–7042, 2020.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Artacho 和 Savakis [2020] B. Artacho 和 A.E. Savakis. Unipose: 统一的人体姿态估计在单张图像和视频中.
    载于 *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*，第
    7033–7042 页，2020 年。'
- en: 'Kolesnikov et al. [2021] A. Kolesnikov, A. Dosovitskiy, D. Weissenborn, G. Heigold,
    J. Uszkoreit, L. Beyer, M. Minderer, M. Dehghani, N. Houlsby, S. Gelly, T. Unterthiner,
    and X. Zhai. An image is worth 16x16 words: Transformers for image recognition
    at scale. In *9th International Conference on Learning Representations, ICLR 2021*,
    2021. URL [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolesnikov 等 [2021] A. Kolesnikov, A. Dosovitskiy, D. Weissenborn, G. Heigold,
    J. Uszkoreit, L. Beyer, M. Minderer, M. Dehghani, N. Houlsby, S. Gelly, T. Unterthiner,
    和 X. Zhai. 一张图像的价值相当于 16x16 个词：用于大规模图像识别的变换器. 载于 *第九届国际学习表征会议，ICLR 2021*，2021
    年。网址 [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy)。
- en: Lecun et al. [1998] Y. Lecun, L. Bottou, Y. Bengio., and P. Haffner. Gradient-based
    learning applied to document recognition. *Proceedings of the IEEE*, 86(11):2278–2324,
    09 1998. ISSN 0018-9219. doi:[10.1109/5.726791](https://doi.org/10.1109/5.726791).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lecun 等 [1998] Y. Lecun, L. Bottou, Y. Bengio., 和 P. Haffner. 基于梯度的学习应用于文档识别.
    *Proceedings of the IEEE*，86(11):2278–2324，1998 年 9 月。ISSN 0018-9219。doi:[10.1109/5.726791](https://doi.org/10.1109/5.726791)。
- en: Szegedy et al. [2015] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2015.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等 [2015] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke, 和 A. Rabinovich. 深度卷积网络. 载于 *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*，2015 年 6 月。
- en: Simonyan and Zisserman [2015] K. Simonyan and A. Zisserman. Very deep convolutional
    networks for large-scale image recognition. *International Conference on Learning
    Representations*, abs/1409.1556, 2015.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman [2015] K. Simonyan 和 A. Zisserman. 用于大规模图像识别的非常深的卷积网络.
    *International Conference on Learning Representations*，abs/1409.1556，2015 年。
- en: He et al. [2016] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
    for image recognition. In *IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, pages 770–778, 2016.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 [2016] K. He, X. Zhang, S. Ren, 和 J. Sun. 用于图像识别的深度残差学习. 载于 *IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*，第 770–778 页，2016 年。
- en: Huang et al. [2017] G. Huang, Z. Liu, L. Van Der Maaten, and K. Weinberger.
    Densely connected convolutional networks. In *2017 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*, pages 2261–2269, 2017. doi:[10.1109/CVPR.2017.243](https://doi.org/10.1109/CVPR.2017.243).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 [2017] G. Huang, Z. Liu, L. Van Der Maaten, 和 K. Weinberger. 密集连接卷积网络。在
    *2017 IEEE 计算机视觉与模式识别会议 (CVPR)*，页 2261–2269，2017。doi:[10.1109/CVPR.2017.243](https://doi.org/10.1109/CVPR.2017.243)。
- en: 'M. Tan [2019] Q.V. Le M. Tan. Efficientnet: Rethinking model scaling for convolutional
    neural networks. In *International conference on machine learning*, pages 6105–6114\.
    PMLR, 2019.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'M. Tan [2019] Q.V. Le M. Tan. Efficientnet: 重新思考卷积神经网络的模型缩放。在 *国际机器学习会议*，页
    6105–6114\. PMLR，2019。'
- en: Nair and Hinton [2010] V. Nair and G.E. Hinton. Rectified linear units improve
    restricted boltzmann machines. In *Proceedings of the 27th International Conference
    on International Conference on Machine Learning*, ICML’10, page 807–814, Madison,
    WI, USA, 2010\. Omnipress. ISBN 9781605589077.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 和 Hinton [2010] V. Nair 和 G.E. Hinton. 线性整流单元提升限制玻尔兹曼机的性能。在 *第27届国际机器学习会议*，ICML’10，页
    807–814，威斯康星州麦迪逊，美国，2010\. Omnipress。ISBN 9781605589077。
- en: Clevert et al. [2016] D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and
    accurate deep network learning by exponential linear units(elus). In Y. Bengio
    and Y. LeCun, editors, *4th International Conference on Learning Representations,
    ICLR 2016, Conference Track Proceedings*, 2016. URL [http://arxiv.org/abs/1511.07289](http://arxiv.org/abs/1511.07289).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clevert 等 [2016] D. Clevert, T. Unterthiner, 和 S. Hochreiter. 通过指数线性单元（elus）实现快速而准确的深度网络学习。在
    Y. Bengio 和 Y. LeCun 编辑的 *第4届国际学习表示会议，ICLR 2016，会议记录*，2016。网址 [http://arxiv.org/abs/1511.07289](http://arxiv.org/abs/1511.07289)。
- en: 'Jia et al. [2009] D. Jia, D. Wei, S. Richard, L. Li-Jia, L. Kai, and L. Fei-Fei.
    Imagenet: a large-scale hierarchical image database. In *IEEE Conference on Computer
    Vision and Pattern Recognition*, pages 248–255, 06 2009. doi:[10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jia 等 [2009] D. Jia, D. Wei, S. Richard, L. Li-Jia, L. Kai, 和 L. Fei-Fei. Imagenet:
    一个大规模分层图像数据库。在 *IEEE 计算机视觉与模式识别会议*，页 248–255，2009年6月。doi:[10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848)。'
- en: 'Wortsman et al. [2022] M. Wortsman, G. Ilharco, S.Y. Gadre, R. Roelofs, R. Gontijo-Lopes,
    A.S Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, and L. Schmidt.
    Model soups: averaging weights of multiple fine-tuned models improves accuracy
    without increasing inference time. In *International Conference on Machine Learning*,
    pages 23965–23998\. PMLR, 2022.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wortsman 等 [2022] M. Wortsman, G. Ilharco, S.Y. Gadre, R. Roelofs, R. Gontijo-Lopes,
    A.S Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, 和 L. Schmidt. 模型汤：通过平均多个微调模型的权重提高准确性，而不会增加推理时间。在
    *国际机器学习会议*，页 23965–23998\. PMLR，2022。
- en: 'Zihang et al. [2021] D. Zihang, L. Hanxiao, L. Quoc, and T. Mingxing. Coatnet:
    Marrying convolution and attention for all data sizes. In *35th Conference on
    Neural Information Processing Systems*, 06 2021.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zihang 等 [2021] D. Zihang, L. Hanxiao, L. Quoc, 和 T. Mingxing. Coatnet: 结合卷积和注意力以适应所有数据大小。在
    *第35届神经信息处理系统会议*，2021年6月。'
- en: Srinivas1 et al. [2021] A. Srinivas1, T. Lin, N. Parmar, J. Shlens, P. Abbeel,
    and A. Vaswani. Bottleneck transformers for visual recognition. In *2021 Conference
    on Computer Vision and Pattern Recognition*, 2021.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srinivas1 等 [2021] A. Srinivas1, T. Lin, N. Parmar, J. Shlens, P. Abbeel, 和
    A. Vaswani. 用于视觉识别的瓶颈变换器。在 *2021 计算机视觉与模式识别会议*，2021。
- en: 'Touvron et al. [2020] Herve Touvron, Joao Caballero, Matthieu Guillaumin, and
    Hervé Jégou. Going deeper with transformers: A study of deep multi-head attention
    models for image classification. In *International Conference on Computer Vision
    (ICCV)*, 2020.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 [2020] Herve Touvron, Joao Caballero, Matthieu Guillaumin, 和 Hervé
    Jégou. 使用变换器深入研究：一种用于图像分类的深度多头注意力模型研究。在 *国际计算机视觉会议 (ICCV)*，2020。
- en: 'Liu et al. [2021] H. Liu, G. Cheng, W. Lin, J. Yang, J. Yang, H. Zhang, Z. Zhang,
    and W. Wu. Swin transformer: Hierarchical vision transformer using shifted windows.
    In *International Conference on Computer Vision (ICCV)*, 2021.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 [2021] H. Liu, G. Cheng, W. Lin, J. Yang, J. Yang, H. Zhang, Z. Zhang,
    和 W. Wu. Swin transformer: 使用移位窗口的分层视觉变换器。在 *国际计算机视觉会议 (ICCV)*，2021。'
- en: 'Chen et al. [2022] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski,
    D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver,
    N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. Thapliyal, J. Bradbury, W. Kuo,
    M. Seyedhosseini, C. Jia, B. Ayan, C. Riquelme, A. Steiner, A. Angelova, X. Zhai,
    N. Houlsby, and R. Soricut. Pali: A jointly-scaled multilingual language-image
    model. In *Arxiv*, 2022.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2022] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski,
    D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver,
    N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. Thapliyal, J. Bradbury, W.
    Kuo, M. Seyedhosseini, C. Jia, B. Ayan, C. Riquelme, A. Steiner, A. Angelova,
    X. Zhai, N. Houlsby, 和 R. Soricut. Pali: 一个联合尺度的多语言图像-语言模型。发表于 *Arxiv*，2022年。'
- en: 'Yu et al. [2022] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini,
    and Y. Wu. Coca: Contrastive captioners are image-text foundation models. *Transactions
    on Machine Learning Research*, abs/2205.01917, 2022.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人 [2022] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, 和 Y.
    Wu. Coca: 对比性描述生成模型是图像-文本基础模型。 *机器学习研究论文集*，abs/2205.01917，2022年。'
- en: Górriz et al. [2017] M. Górriz, A. Carlier, E. Faure, and X. Giró i Nieto. Cost-effective
    active learning for melanoma segmentation. *ArXiv*, abs/1711.09168, 2017.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Górriz 等人 [2017] M. Górriz, A. Carlier, E. Faure, 和 X. Giró i Nieto. 成本效益高的主动学习用于黑色素瘤分割。
    *ArXiv*，abs/1711.09168，2017年。
- en: Konyushkova et al. [2017] K. Konyushkova, R. Sznitman, and P. Fua. Learning
    active learning from data. In *NIPS*, 2017.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konyushkova 等人 [2017] K. Konyushkova, R. Sznitman, 和 P. Fua. 从数据中学习主动学习。发表于
    *NIPS*，2017年。
- en: Lewis and Gale [1994] D. Lewis and W. Gale. A sequential algorithm for training
    text classifiers. In *SIGIR ’94*, pages 3–12, London, 1994\. Springer London.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 和 Gale [1994] D. Lewis 和 W. Gale. 用于训练文本分类器的序列算法。发表于 *SIGIR ’94*，第3-12页，伦敦，1994年。Springer
    London。
- en: McCallum and Nigam [1998] A. McCallum and K. Nigam. Employing em and pool-based
    active learning for text classification. In *International Conference of Machine
    Learning*, 1998.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCallum 和 Nigam [1998] A. McCallum 和 K. Nigam. 采用 EM 和基于池的主动学习进行文本分类。发表于 *国际机器学习会议*，1998年。
- en: 'C.Shui et al. [2020] C.Shui, F.Zhou, C.Gagn’e, and B.Wang. Deep active learning:
    Unified and principled method for query and training. In *International Conference
    on Artificial Intelligence and Statistics*, 2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C.Shui 等人 [2020] C.Shui, F.Zhou, C.Gagn’e, 和 B.Wang. 深度主动学习：统一而有原则的查询与训练方法。发表于
    *国际人工智能与统计会议*，2020年。
- en: 'Sener and Savarese [2018] O. Sener and S. Savarese. Active learning for convolutional
    neural networks: A core-set approach. *International Conference on Learning Representations
    (Poster)*, 2018. URL [http://dblp.uni-trier.de/db/conf/iclr/iclr2018.html#SenerS18](http://dblp.uni-trier.de/db/conf/iclr/iclr2018.html#SenerS18).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sener 和 Savarese [2018] O. Sener 和 S. Savarese. 卷积神经网络的主动学习：核心集方法。 *学习表征国际会议（海报）*，2018年。网址
    [http://dblp.uni-trier.de/db/conf/iclr/iclr2018.html#SenerS18](http://dblp.uni-trier.de/db/conf/iclr/iclr2018.html#SenerS18)。
- en: Phillips and Tai [2018] J. Phillips and W.M. Tai. Near-optimal coresets of kernel
    density estimates. In *34th International Symposium on Computational Geometry,
    SoCG 2018, June 11-14, 2018, Budapest, Hungary*, volume 99 of *LIPIcs*, pages
    66:1–66:13\. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2018. doi:[10.4230/LIPIcs.SoCG.2018.66](https://doi.org/10.4230/LIPIcs.SoCG.2018.66).
    URL [https://doi.org/10.4230/LIPIcs.SoCG.2018.66](https://doi.org/10.4230/LIPIcs.SoCG.2018.66).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phillips 和 Tai [2018] J. Phillips 和 W.M. Tai. 近似最优的核密度估计核心集。发表于 *第34届计算几何国际研讨会，SoCG
    2018，2018年6月11-14日，匈牙利布达佩斯*，*LIPIcs* 第99卷，第66:1–66:13页。Schloss Dagstuhl - Leibniz-Zentrum
    für Informatik，2018年。doi：[10.4230/LIPIcs.SoCG.2018.66](https://doi.org/10.4230/LIPIcs.SoCG.2018.66)。网址
    [https://doi.org/10.4230/LIPIcs.SoCG.2018.66](https://doi.org/10.4230/LIPIcs.SoCG.2018.66)。
- en: Phillips [2016] J. Phillips. Coresets and sketches. *CoRR*, abs/1601.00617,
    2016. URL [http://arxiv.org/abs/1601.00617](http://arxiv.org/abs/1601.00617).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phillips [2016] J. Phillips. 核心集与草图。 *CoRR*，abs/1601.00617，2016年。网址 [http://arxiv.org/abs/1601.00617](http://arxiv.org/abs/1601.00617)。
- en: Har-Peled et al. [2007] S. Har-Peled, D. Roth, and D. Zimak. Maximum margin
    coresets for active and noise tolerant learning. In *International Joint Conferences
    on Artificial Intelligence*, 2007.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Har-Peled 等人 [2007] S. Har-Peled, D. Roth, 和 D. Zimak. 最大边际核心集用于主动和噪声容忍学习。发表于
    *国际人工智能联合会议*，2007年。
- en: Geifman and El-Yaniv [2017] Y. Geifman and R. El-Yaniv. Deep active learning
    over the long tail. *ArXiv*, abs/1711.00941, 2017.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geifman 和 El-Yaniv [2017] Y. Geifman 和 R. El-Yaniv. 深度主动学习与长尾问题。 *ArXiv*，abs/1711.00941，2017年。
- en: Chen et al. [2020] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple
    framework for contrastive learning of visual representations. In *Proceedings
    of the 37th International Conference on Machine Learning*, ICML’20, pages 1597–1607\.
    JMLR.org, 2020.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2020] T. Chen、S. Kornblith、M. Norouzi 和 G. Hinton. 一种简单的视觉表示对比学习框架。发表于
    *第37届国际机器学习会议论文集*，ICML’20，第1597–1607页。JMLR.org，2020年。
- en: Du et al. [2021] P. Du, S. Zhao, H. Chen, S. Chai, H. Chen, and C. Li. Contrastive
    coding for active learning under class distribution mismatch. In *IEEE/CVF International
    Conference on Computer Vision (ICCV)*, pages 8907–8916, 2021. doi:[10.1109/ICCV48922.2021.00880](https://doi.org/10.1109/ICCV48922.2021.00880).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 [2021] P. Du、S. Zhao、H. Chen、S. Chai、H. Chen 和 C. Li. 在类别分布不匹配下的对比编码用于主动学习。发表于
    *IEEE/CVF国际计算机视觉会议（ICCV）*，第8907–8916页，2021年。doi：[10.1109/ICCV48922.2021.00880](https://doi.org/10.1109/ICCV48922.2021.00880)。
- en: Wang et al. [2021a] C. Wang, A. Singla, and Y. Chen. Teaching an active learner
    with contrastive examples. In *Advances in Neural Information Processing Systems*,
    2021a.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2021a] C. Wang、A. Singla 和 Y. Chen. 用对比示例教导主动学习者。发表于 *神经信息处理系统的进展*，2021年。
- en: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. In Z. Ghahramani,
    M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, *Advances in
    Neural Information Processing Systems*, volume 27\. Curran Associates, Inc., 2014.
    URL [https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 [2014] I. Goodfellow、J. Pouget-Abadie、M. Mirza、B. Xu、D. Warde-Farley、S. Ozair、A. Courville
    和 Y. Bengio. 生成对抗网络。由 Z. Ghahramani、M. Welling、C. Cortes、N. Lawrence 和 K.Q. Weinberger
    编辑，*神经信息处理系统的进展*，第27卷。Curran Associates, Inc.，2014年。URL [https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)。
- en: 'Gonog and Zhou [2019] L. Gonog and Y. Zhou. A review: Generative adversarial
    networks. In *The 14th IEEE Conference on Industrial Electronics and Applications
    (ICIEA)*, pages 505–510, 2019. doi:[10.1109/ICIEA.2019.8833686](https://doi.org/10.1109/ICIEA.2019.8833686).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gonog 和 Zhou [2019] L. Gonog 和 Y. Zhou. 综述：生成对抗网络。发表于 *第14届IEEE工业电子与应用会议（ICIEA）*，第505–510页，2019年。doi：[10.1109/ICIEA.2019.8833686](https://doi.org/10.1109/ICIEA.2019.8833686)。
- en: Sinha et al. [2019a] S. Sinha, S. Ebrahimi, and T. Darrell. Variational adversarial
    active learning. In *IEEE/CVF International Conference on Computer Vision (ICCV)*,
    pages 5971–5980, Los Alamitos, CA, USA, 03 2019a. IEEE Computer Society. doi:[10.1109/ICCV.2019.00607](https://doi.org/10.1109/ICCV.2019.00607).
    URL [https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00607](https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00607).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha 等 [2019a] S. Sinha、S. Ebrahimi 和 T. Darrell. 变分对抗主动学习。发表于 *IEEE/CVF国际计算机视觉会议（ICCV）*，第5971–5980页，美国加利福尼亚州洛斯阿拉米托斯，2019年3月。IEEE计算机学会。doi：[10.1109/ICCV.2019.00607](https://doi.org/10.1109/ICCV.2019.00607)。URL
    [https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00607](https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00607)。
- en: Barnett [2018] S. Barnett. Convergence problems with generative adversarial
    networks (gans). *ArXiv*, abs/1806.11382, 2018.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barnett [2018] S. Barnett. 生成对抗网络（GANs）的收敛问题。*ArXiv*，abs/1806.11382，2018年。
- en: Mescheder et al. [2018] L. Mescheder, A. Geiger, and S. Nowozin. Which training
    methods for gans do actually converge? In *The 35th International Conference on
    Machine Learning*, 2018.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mescheder 等 [2018] L. Mescheder、A. Geiger 和 S. Nowozin. 哪些训练方法对于GANs实际上会收敛？发表于
    *第35届国际机器学习会议*，2018年。
- en: Chicheng and Kamalika [2015] Z. Chicheng and C. Kamalika. Active learning from
    weak and strong labelers. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
    R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 28.
    Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper/2015/file/eba0dc302bcd9a273f8bbb72be3a687b-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/eba0dc302bcd9a273f8bbb72be3a687b-Paper.pdf).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chicheng 和 Kamalika [2015] Z. Chicheng 和 C. Kamalika. 从弱标注者和强标注者中进行主动学习。由 C. Cortes、N. Lawrence、D. Lee、M. Sugiyama
    和 R. Garnett 编辑，*神经信息处理系统的进展*，第28卷。Curran Associates, Inc.，2015年。URL [https://proceedings.neurips.cc/paper/2015/file/eba0dc302bcd9a273f8bbb72be3a687b-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/eba0dc302bcd9a273f8bbb72be3a687b-Paper.pdf)。
- en: 'Wei et al. [2022] J. Wei, Z. Zhu, H. Cheng, T. Liu, G. Niu, and Y. Liu. Learning
    with noisy labels revisited: A study using real-world human annotations. *10th
    International Conference on Learning Representations*, 2022.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 [2022] J. Wei、Z. Zhu、H. Cheng、T. Liu、G. Niu 和 Y. Liu. 重新审视带噪声标签的学习：使用真实世界人类注释的研究。*第10届国际学习表示会议*，2022年。
- en: 'Algan and Ulusoy [2021] G. Algan and I. Ulusoy. Image classification with deep
    learning in the presence of noisy labels: A survey. *ArXiv*, abs/1912.05170, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Algan 和 Ulusoy [2021] G. Algan 和 I. Ulusoy. 在标签噪声存在下的深度学习图像分类：一项调查。*ArXiv*，abs/1912.05170，2021年。
- en: 'Cordeiro and Carneiro [2020] F. Cordeiro and G. Carneiro. A survey on deep
    learning with noisy labels: How to train your model when you cannot trust on the
    annotations? In *The 33rd SIBGRAPI Conference on Graphics, Patterns and Images*,
    pages 9–16, 11 2020. doi:[10.1109/SIBGRAPI51738.2020.00010](https://doi.org/10.1109/SIBGRAPI51738.2020.00010).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cordeiro 和 Carneiro [2020] F. Cordeiro 和 G. Carneiro. 噪声标签下的深度学习调查：当你无法信任标注时如何训练模型？见于
    *第33届SIBGRAPI图形、模式与图像会议*，第9–16页，2020年11月。doi:[10.1109/SIBGRAPI51738.2020.00010](https://doi.org/10.1109/SIBGRAPI51738.2020.00010)。
- en: Worker [2022] Click Worker. Clickworker, 2022. URL [https://www.clickworker.com](https://www.clickworker.com).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Worker [2022] Click Worker. Clickworker，2022年。网址 [https://www.clickworker.com](https://www.clickworker.com)。
- en: Amazon [2022] Amazon. Amazon mechanical turk, 2022. URL [https://www.mturk.com/](https://www.mturk.com/).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon [2022] Amazon. Amazon机械土耳其，2022年。网址 [https://www.mturk.com/](https://www.mturk.com/)。
- en: Scale.ai [2022] Scale.ai. Scale ai, 2022. URL [https://scale.com/](https://scale.com/).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scale.ai [2022] Scale.ai. Scale ai，2022年。网址 [https://scale.com/](https://scale.com/)。
- en: Nagarajan et al. [2013] N. Nagarajan, D. Inderjit, R. Pradeep, and T. Ambuj.
    Learning with noisy labels. In *Advances in Neural Information Processing Systems*,
    volume 26\. Curran Associates, Inc., 2013. URL [https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf](https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagarajan 等 [2013] N. Nagarajan, D. Inderjit, R. Pradeep, 和 T. Ambuj. 学习带噪声标签的数据。见于
    *神经信息处理系统进展*，第26卷。Curran Associates, Inc.，2013年。网址 [https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf](https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf)。
- en: 'Patrinin et al. [2017] G. Patrinin, A. Rozza, A. Menon, R. Nock, and L. Qu.
    Making deep neural networks robust to label noise: A loss correction approach.
    In *2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages
    2233–2241, 07 2017. doi:[10.1109/CVPR.2017.240](https://doi.org/10.1109/CVPR.2017.240).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patrinin 等 [2017] G. Patrinin, A. Rozza, A. Menon, R. Nock, 和 L. Qu. 使深度神经网络对标签噪声具有鲁棒性：一种损失修正方法。见于
    *2017 IEEE计算机视觉与模式识别会议 (CVPR)*，第2233–2241页，2017年7月。doi:[10.1109/CVPR.2017.240](https://doi.org/10.1109/CVPR.2017.240)。
- en: 'Han et al. [2018a] B. Han, J. Yao, G. Niu, M. Zhou, I. Tsang, Y. Zhang, and
    M. Sugiyama. Masking: A new perspective of noisy supervision. In *Advances in
    Neural Information Processing Systems*, 05 2018a.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han 等 [2018a] B. Han, J. Yao, G. Niu, M. Zhou, I. Tsang, Y. Zhang, 和 M. Sugiyama.
    Masking: 噪声监督的新视角。见于 *神经信息处理系统进展*，2018年5月。'
- en: Malach and Shalev-Shwartz [2017] E. Malach and S. Shalev-Shwartz. Decoupling
    "when to update" from "how to update". In *Proceedings of the 31st International
    Conference on Neural Information Processing Systems*, NIPS’17, page 961–971, Red
    Hook, NY, USA, 2017\. Curran Associates Inc. ISBN 9781510860964.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malach 和 Shalev-Shwartz [2017] E. Malach 和 S. Shalev-Shwartz. 将“何时更新”与“如何更新”解耦。见于
    *第31届国际神经信息处理系统会议论文集*，NIPS’17，第961–971页，纽约红钩，美国，2017年。Curran Associates Inc. ISBN
    9781510860964。
- en: 'Han et al. [2018b] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and
    M. Sugiyama. Co-teaching: Robust training of deep neural networks with extremely
    noisy labels. In *Advances in Neural Information Processing Systems*, 2018b.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han 等 [2018b] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, 和 M. Sugiyama.
    Co-teaching: 用极度噪声标签进行深度神经网络的鲁棒训练。见于 *神经信息处理系统进展*，2018年。'
- en: Ren et al. [2020] P. Ren, Y. Xiao, X. Chang, P. Huang, Z. Li, X. Chen, and X. Wang.
    A survey of deep active learning. *ACM Computing Surveys (CSUR)*, 54:1 – 40, 2020.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等 [2020] P. Ren, Y. Xiao, X. Chang, P. Huang, Z. Li, X. Chen, 和 X. Wang.
    深度主动学习调查。*ACM计算调查 (CSUR)*，54:1 – 40，2020年。
- en: 'LeCun et al. [2010] Y. LeCun, C. Cortes, and C.J. Burges. Mnist handwritten
    digit database. *ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist*,
    2, 2010.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 [2010] Y. LeCun, C. Cortes, 和 C.J. Burges. Mnist手写数字数据库。*ATT Labs [在线]。可用：
    http://yann.lecun.com/exdb/mnist*，第2卷，2010年。
- en: Krizhevsky et al. [2009] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-100 (canadian
    institute for advanced research). 2009. URL [http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 [2009] A. Krizhevsky, V. Nair, 和 G. Hinton. Cifar-100（加拿大高级研究所）。2009年。网址
    [http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)。
- en: 'Fei-Fei et al. [2004] L. Fei-Fei, M. Andreetto, M. Ranzato, and P. Perona.
    Learning generative visual models from few training examples: An incremental bayesian
    approach tested on 101 object categories, 2004.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei-Fei等人 [2004] L. Fei-Fei, M. Andreetto, M. Ranzato, 和 P. Perona. 从少量训练示例中学习生成视觉模型：一种增量贝叶斯方法，测试于101个物体类别，2004。
- en: Netzer et al. [2011] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and
    A. Ng. Reading digits in natural images with unsupervised feature learning. In
    *NIPS Workshop on Deep Learning and Unsupervised Feature Learning*, 2011.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Netzer等人 [2011] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, 和 A. Ng.
    使用无监督特征学习读取自然图像中的数字。发表于*NIPS深度学习和无监督特征学习研讨会*，2011。
- en: 'Lin et al. [2014] T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays,
    P. Perona, D. Ramanan, P. Doll’a r, and C.L. Zitnick. Microsoft COCO: common objects
    in context. *CoRR*, abs/1405.0312, 2014. URL [http://arxiv.org/abs/1405.0312](http://arxiv.org/abs/1405.0312).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等人 [2014] T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays,
    P. Perona, D. Ramanan, P. Doll’ar, 和 C.L. Zitnick. Microsoft COCO：背景中的常见对象。*CoRR*，abs/1405.0312，2014。URL
    [http://arxiv.org/abs/1405.0312](http://arxiv.org/abs/1405.0312)。
- en: Yi et al. [2022] J.S Yi, M. Seo, J. Park, and D. Choi. Using self-supervised
    pretext tasks for active learning. In *European Conference on Computer Vision*,
    2022.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi等人 [2022] J.S Yi, M. Seo, J. Park, 和 D. Choi. 使用自监督预训练任务进行主动学习。发表于*欧洲计算机视觉会议*，2022。
- en: Hataya and Nakayama [2018] R. Hataya and H. Nakayama. Investigating cnns’ learning
    representation under label noise. In *International Conference on Learning Representations*,
    2018.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hataya和Nakayama [2018] R. Hataya 和 H. Nakayama. 研究标签噪声下CNN的学习表示。发表于*国际学习表征会议*，2018。
- en: 'Gupta et al. [2020] G. Gupta, A.K. Sahu, and W. Lin. Noisy batch active learning
    with deterministic annealing. In *arXiv: Learning*, 2020.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gupta等人 [2020] G. Gupta, A.K. Sahu, 和 W. Lin. 带有确定性退火的噪声批量主动学习。发表于*arXiv: Learning*，2020。'
- en: Wang et al. [2021b] T. Wang, X. Li, P. Yang, G. Hu, X. Zeng, S. Huang, C. Xu,
    and M. Xu. Boosting active learning via improving test performance. In *AAAI Conference
    on Artificial Intelligence*, 2021b.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 [2021b] T. Wang, X. Li, P. Yang, G. Hu, X. Zeng, S. Huang, C. Xu, 和 M.
    Xu. 通过提高测试性能来增强主动学习。发表于*AAAI人工智能会议*，2021b。
- en: 'Xia et al. [2020] X. Xia, T. Liu, B. Han, N. Wang, M. Gong, H. Liu, G. Niu,
    D. Tao, and M. Sugiyama. Part-dependent label noise: Towards instance-dependent
    label noise. In *Proceedings of the 34th International Conference on Neural Information
    Processing Systems*, NIPS’20, Red Hook, NY, USA, 2020\. Curran Associates Inc.
    ISBN 9781713829546.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia等人 [2020] X. Xia, T. Liu, B. Han, N. Wang, M. Gong, H. Liu, G. Niu, D. Tao,
    和 M. Sugiyama. 部分依赖标签噪声：朝着实例依赖标签噪声迈进。发表于*第34届国际神经信息处理系统会议*，NIPS’20，美国纽约Red Hook，2020。Curran
    Associates Inc. ISBN 9781713829546。
- en: Li et al. [2022] X. Li, P. Yang, T. Wang, X. Zhan, M. Xu, D. Dou, and C. Xu.
    Deep active learning with noise stability. In *International Conference on Learning
    Representations*, 05 2022. doi:[10.48550/arXiv.2205.13340](https://doi.org/10.48550/arXiv.2205.13340).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人 [2022] X. Li, P. Yang, T. Wang, X. Zhan, M. Xu, D. Dou, 和 C. Xu. 具有噪声稳定性的深度主动学习。发表于*国际学习表征会议*，2022年5月。doi:[10.48550/arXiv.2205.13340](https://doi.org/10.48550/arXiv.2205.13340)。
- en: Haußmann et al. [2019] M. Haußmann, F. Hamprecht, and M. Kandemir. Deep active
    learning with adaptive acquisition. In *Proceedings of the 28th International
    Joint Conference on Artificial Intelligence*, IJCAI’19, page 2470–2476\. AAAI
    Press, 2019. ISBN 9780999241141.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haußmann等人 [2019] M. Haußmann, F. Hamprecht, 和 M. Kandemir. 带有自适应获取的深度主动学习。发表于*第28届国际人工智能联合会议*，IJCAI’19，第2470–2476页。AAAI
    Press，2019。ISBN 9780999241141。
- en: Younesian et al. [2020] T. Younesian, D.H. Epema, and L. Chen. Active learning
    for noisy data streams using weak and strong labelers. *ArXiv*, abs/2010.14149,
    2020.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Younesian等人 [2020] T. Younesian, D.H. Epema, 和 L. Chen. 使用弱标注者和强标注者对噪声数据流进行主动学习。*ArXiv*，abs/2010.14149，2020。
- en: 'Shui et al. [2020] C. Shui, F. Zhou, C. Gagn’e, and B. Wang. Deep active learning:
    Unified and principled method for query and training. In *AISTATS*, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shui等人 [2020] C. Shui, F. Zhou, C. Gagn’e, 和 B. Wang. 深度主动学习：统一和原则性的方法用于查询和训练。发表于*AISTATS*，2020。
- en: 'Younesian et al. [2021] T. Younesian, Z. Zhao, A. Ghiassi, R. Birke, and L. Chen.
    Qactor: Active learning on noisy labels. In Vineeth N. Balasubramanian and Ivor
    Tsang, editors, *Proceedings of The 13th Asian Conference on Machine Learning*,
    volume 157 of *Proceedings of Machine Learning Research*, pages 548–563\. PMLR,
    17–19 Nov 2021. URL [https://proceedings.mlr.press/v157/younesian21a.html](https://proceedings.mlr.press/v157/younesian21a.html).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Younesian等人 [2021] T. Younesian, Z. Zhao, A. Ghiassi, R. Birke, 和 L. Chen.
    Qactor: 噪声标签上的主动学习。在Vineeth N. Balasubramanian和Ivor Tsang编辑的*第13届亚洲机器学习会议论文集*中，*机器学习研究论文集*第157卷，页码548–563。PMLR，2021年11月17–19日。网址
    [https://proceedings.mlr.press/v157/younesian21a.html](https://proceedings.mlr.press/v157/younesian21a.html)。'
- en: Yoo et al. [2017] S. Yoo, H. Kim, I. Kim, M. Kim, and S. Hwang. Active learning
    for deep neural networks. In *Proceedings of the 34th International Conference
    on Machine Learning-Volume 70*, pages 2070–2079\. JMLR. org, 2017.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoo等人 [2017] S. Yoo, H. Kim, I. Kim, M. Kim, 和 S. Hwang. 深度神经网络的主动学习。在*第34届国际机器学习会议-第70卷*，页码2070–2079。JMLR.org，2017年。
- en: Sinha et al. [2019b] K. Sinha, Y. Zhang, P. Doshi, and I. Dhillon. Efficient
    active learning for deep neural networks. In *Proceedings of the 36th International
    Conference on Machine Learning*, pages 3560–3568, 2019b.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha等人 [2019b] K. Sinha, Y. Zhang, P. Doshi, 和 I. Dhillon. 深度神经网络的高效主动学习。在*第36届国际机器学习会议论文集*，页码3560–3568，2019b年。
- en: Settles [2009] B. Settles. Active learning literature survey. In *ECML PKDD
    Workshop on Active Learning and Experimental Design*, pages 11–46, 2009.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Settles [2009] B. Settles. 主动学习文献综述。在*ECML PKDD主动学习与实验设计研讨会*，页码11–46，2009年。
- en: Gal et al. [2017] Y. Gal, R. Islam, and Z. Ghahramani. Deep bayesian active
    learning with image data. In *International Conference of Machine Learning*, volume
    abs/1703.02910, pages 1183–1192, 2017.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal等人 [2017] Y. Gal, R. Islam, 和 Z. Ghahramani. 基于图像数据的深度贝叶斯主动学习。在*国际机器学习会议*上，第abs/1703.02910卷，页码1183–1192，2017年。
- en: 'Cao and Tsang [2021] X. Cao and I. Tsang. Bayesian active learning by disagreements:
    A geometric perspective. *ArXiv*, abs/2105.02543, 2021.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao和Tsang [2021] X. Cao 和 I. Tsang. 通过争议进行贝叶斯主动学习: 几何视角。*ArXiv*，abs/2105.02543，2021年。'
- en: Huang et al. [2016] T. Huang, L. Lihong, A. Vartanian, S. Amershi, and X.J.
    Zhu. Active learning with oracle epiphany. In D. Lee, M. Sugiyama, U. Luxburg,
    I. Guyon, and R. Garnett, editors, *Advances in Neural Information Processing
    Systems*, volume 29. Curran Associates, Inc., 2016. URL [https://proceedings.neurips.cc/paper/2016/file/299fb2142d7de959380f91c01c3a293c-Paper.pdf](https://proceedings.neurips.cc/paper/2016/file/299fb2142d7de959380f91c01c3a293c-Paper.pdf).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人 [2016] T. Huang, L. Lihong, A. Vartanian, S. Amershi, 和 X.J. Zhu. 带有oracle顿悟的主动学习。在D.
    Lee, M. Sugiyama, U. Luxburg, I. Guyon, 和 R. Garnett编辑的*神经信息处理系统进展*中，第29卷。Curran
    Associates, Inc.，2016年。网址 [https://proceedings.neurips.cc/paper/2016/file/299fb2142d7de959380f91c01c3a293c-Paper.pdf](https://proceedings.neurips.cc/paper/2016/file/299fb2142d7de959380f91c01c3a293c-Paper.pdf)。
- en: Amin et al. [2021] K. Amin, G. DeSalvo, and A. Rostamizadeh. Learning with labeling
    induced abstentions. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and
    J. Wortman Vaughan, editors, *Advances in Neural Information Processing Systems*,
    volume 34, pages 12576–12586\. Curran Associates, Inc., 2021. URL [https://proceedings.neurips.cc/paper/2021/file/689041c2baed0f6d91050495d632d6e0-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/689041c2baed0f6d91050495d632d6e0-Paper.pdf).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amin等人 [2021] K. Amin, G. DeSalvo, 和 A. Rostamizadeh. 通过标注诱导的弃权进行学习。在M. Ranzato,
    A. Beygelzimer, Y. Dauphin, P.S. Liang, 和 J. Wortman Vaughan编辑的*神经信息处理系统进展*中，第34卷，页码12576–12586。Curran
    Associates, Inc.，2021年。网址 [https://proceedings.neurips.cc/paper/2021/file/689041c2baed0f6d91050495d632d6e0-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/689041c2baed0f6d91050495d632d6e0-Paper.pdf)。
- en: Yan et al. [2016] S. Yan, K. Chaudhuri, and T. Javidi. Active learning from
    imperfect labelers. In *Proceedings of the 30th International Conference on Neural
    Information Processing Systems*, NIPS’16, page 2136–2144, Red Hook, NY, USA, 2016\.
    Curran Associates Inc. ISBN 9781510838819.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan等人 [2016] S. Yan, K. Chaudhuri, 和 T. Javidi. 从不完美标注者中进行主动学习。在*第30届国际神经信息处理系统会议论文集*，NIPS’16，第2136–2144页，美国纽约州雷德胡克，2016年。Curran
    Associates Inc. ISBN 9781510838819。
- en: 'Xiaoqi et al. [2020] J. Xiaoqi, Y. Yichun, S. Lifeng, J. Xin, C. Xiao, L. Linlin,
    W. Fang, and L. Qun. TinyBERT: Distilling BERT for natural language understanding.
    In *Findings of the Association for Computational Linguistics: EMNLP 2020"*, pages
    4163–4174, Online, 2020\. Association for Computational Linguistics. URL [https://aclanthology.org/2020.findings-emnlp.372](https://aclanthology.org/2020.findings-emnlp.372).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiaoqi 等人 [2020] J. Xiaoqi, Y. Yichun, S. Lifeng, J. Xin, C. Xiao, L. Linlin,
    W. Fang, 和 L. Qun。TinyBERT：为自然语言理解蒸馏 BERT。在 *计算语言学协会会议记录：EMNLP 2020*，第 4163-4174
    页，在线，2020年。计算语言学协会。网址 [https://aclanthology.org/2020.findings-emnlp.372](https://aclanthology.org/2020.findings-emnlp.372)。
- en: 'Clark et al. [2020] K. Clark, M. Luong, Q.V. Le, and C.D. Manning. Electra:
    Pre-training text encoders as discriminators rather than generators. In *8th International
    Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
    26-30, 2020*. OpenReview.net, 2020. URL [https://openreview.net/forum?id=r1xMH1BtvB](https://openreview.net/forum?id=r1xMH1BtvB).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人 [2020] K. Clark, M. Luong, Q.V. Le, 和 C.D. Manning。Electra：将文本编码器作为判别器而非生成器进行预训练。在
    *第八届国际学习表示会议，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日*。OpenReview.net，2020。网址 [https://openreview.net/forum?id=r1xMH1BtvB](https://openreview.net/forum?id=r1xMH1BtvB)。
- en: Eitan et al. [2019] A. Eitan, E. Smolyansky, I. Harpaz, and S. Perets. Connected
    papers, 2019. URL [https://www.connectedpapers.com/](https://www.connectedpapers.com/).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eitan 等人 [2019] A. Eitan, E. Smolyansky, I. Harpaz, 和 S. Perets。Connected papers，2019。网址
    [https://www.connectedpapers.com/](https://www.connectedpapers.com/)。
- en: Appendix
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: .1 Literature Networks
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: .1 文献网络
- en: Below we present a visual depiction of the literature most related to this review
    focusing on deep active learning on noisy labels for image classification. We
    present the views as images of network graphs where the nodes represent papers
    and the edges represent the similarity between articles. The larger the node,
    the more influential the article is to related articles, and the thicker the connection
    between any two papers, the more closely related the articles are. The Connected
    Papers visualization tool Eitan et al. [[2019](#bib.bib91)] was used to create
    these graphs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们展示了与本综述最相关的文献的视觉图示，重点关注用于图像分类的噪声标签的深度主动学习。我们以网络图的形式展示这些视图，其中节点代表论文，边表示文章之间的相似性。节点越大，文章对相关文献的影响越大；任意两篇论文之间的连接越粗，说明这些文章的相关性越高。这些图是使用
    Connected Papers 视觉化工具 Eitan 等人 [[2019](#bib.bib91)] 创建的。
- en: '![Refer to caption](img/4199cc7c5faa688462c6335c82c52798.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/4199cc7c5faa688462c6335c82c52798.png)'
- en: 'Figure 5: Deep active learning with noisy labels literature closely related
    to Younesian et al. Younesian et al. [[2021](#bib.bib80)]'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 与 Younesian 等人工作的深度主动学习噪声标签文献密切相关。Younesian 等人 [[2021](#bib.bib80)]'
- en: '![Refer to caption](img/0ad25f7740ff663bfffeeafa08515067.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/0ad25f7740ff663bfffeeafa08515067.png)'
- en: 'Figure 6: Deep active learning with noisy labels papers related to the work
    of Huang et al. Huang et al. [[2016](#bib.bib86)]'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 与 Huang 等人工作的深度主动学习噪声标签相关的论文。Huang 等人 [[2016](#bib.bib86)]'
- en: '![Refer to caption](img/563b412de28ffd5cbc0b76ae4639f782.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/563b412de28ffd5cbc0b76ae4639f782.png)'
- en: 'Figure 7: Deep active learning with noisy labels papers related to the work
    of Yan et al. Yan et al. [[2016](#bib.bib88)]'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 与 Yan 等人工作的深度主动学习噪声标签相关的论文。Yan 等人 [[2016](#bib.bib88)]'
- en: '![Refer to caption](img/28f19ace31e9879698c5ef43419aa99c.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/28f19ace31e9879698c5ef43419aa99c.png)'
- en: 'Figure 8: Active learning work closely related to the survey manuscript by
    Settles Settles [[2009](#bib.bib83)]'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 与 Settles 综述手稿密切相关的主动学习工作。Settles [[2009](#bib.bib83)]'
