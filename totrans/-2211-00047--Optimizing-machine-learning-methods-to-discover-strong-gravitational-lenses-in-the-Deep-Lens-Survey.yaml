- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:43:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:43:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2211.00047] Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2211.00047] 优化机器学习方法以在深度透镜调查中发现强引力透镜'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2211.00047](https://ar5iv.labs.arxiv.org/html/2211.00047)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2211.00047](https://ar5iv.labs.arxiv.org/html/2211.00047)
- en: Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化机器学习方法以在深度透镜调查中发现强引力透镜
- en: Keerthi Vasan G.C.,¹ Stephen Sheng,² Tucker Jones,¹ Chi Po Choi,³ and James
    Sharpnack^(2,3)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Keerthi Vasan G.C.,¹ Stephen Sheng,² Tucker Jones,¹ Chi Po Choi,³ 和 James Sharpnack^(2,3)
- en: ¹Department of Physics and Astronomy, University of California, Davis, 1 Shields
    Avenue, Davis, CA 95616, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹加州大学戴维斯分校物理与天文学系，美国加州戴维斯1 Shields Avenue，邮政编码95616
- en: ²Amazon AWS AI
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²亚马逊AWS AI
- en: '³Department of Statistics, University of California, Davis, 1 Shields Avenue,
    Davis, CA 95616, USA E-mail: kvch@ucdavis.eduWork done prior to joining Amazon(Last
    updated ; in original form )'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³加州大学戴维斯分校统计系，美国加州戴维斯1 Shields Avenue，邮政编码95616 电子邮件：kvch@ucdavis.edu在加入亚马逊之前完成的工作（最后更新；原始形式）
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Machine learning models can greatly improve the search for strong gravitational
    lenses in imaging surveys by reducing the amount of human inspection required.
    In this work, we test the performance of supervised, semi-supervised, and unsupervised
    learning algorithms trained with the ResNetV2 neural network architecture on their
    ability to efficiently find strong gravitational lenses in the Deep Lens Survey
    (DLS). We use galaxy images from the survey, combined with simulated lensed sources,
    as labeled data in our training datasets. We find that models using semi-supervised
    learning along with data augmentations (transformations applied to an image during
    training, e.g., rotation) and Generative Adversarial Network (GAN) generated images
    yield the best performance. They offer 5–10 times better precision across all
    recall values compared to supervised algorithms. Applying the best performing
    models to the full 20 deg² DLS survey, we find 3 Grade-A lens candidates within
    the top 17 image predictions from the model. This increases to 9 Grade-A and 13
    Grade-B candidates when $1$% ($\sim 2500$ images) of the model predictions are
    visually inspected. This is $\gtrsim 10\times$ the sky density of lens candidates
    compared to current shallower wide-area surveys (such as the Dark Energy Survey),
    indicating a trove of lenses awaiting discovery in upcoming deeper all-sky surveys.
    These results suggest that pipelines tasked with finding strong lens systems can
    be highly efficient, minimizing human effort. We additionally report spectroscopic
    confirmation of the lensing nature of two Grade-A candidates identified by our
    model, further validating our methods.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型可以通过减少所需的人力检查量，显著提高在成像调查中寻找强引力透镜的效果。在这项工作中，我们测试了使用ResNetV2神经网络架构训练的监督、半监督和无监督学习算法在深度透镜调查（DLS）中高效寻找强引力透镜的能力。我们使用来自调查的星系图像，结合模拟的透镜源，作为我们训练数据集中的标记数据。我们发现，使用半监督学习并结合数据增强（在训练期间对图像应用的转换，如旋转）和生成对抗网络（GAN）生成的图像的模型，表现最佳。与监督算法相比，它们在所有召回值上提供了5–10倍更好的精确度。将表现最好的模型应用于完整的20平方度DLS调查中，我们在模型的前17张图像预测中发现了3个A级透镜候选者。当视觉检查模型预测的1%（约2500张图像）时，这一数字增加到9个A级和13个B级候选者。这是当前浅层广域调查（如暗能量调查）的透镜候选者的$\gtrsim
    10\times$天区密度，表明即将进行的更深全天调查中潜藏着大量的透镜。这些结果表明，专门寻找强透镜系统的管道可以非常高效，最小化人力工作。我们还报告了对我们模型识别的两个A级候选者的光谱确认，进一步验证了我们的方法。
- en: 'keywords:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'gravitational lensing: strong – methods: statistical^†^†pubyear: 2022^†^†pagerange:
    Optimizing machine learning methods to discover strong gravitational lenses in
    the Deep Lens Survey–[15](#A1.F15 "Figure 15 ‣ Appendix A Model performance and
    final lens sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 引力透镜：强 – 方法：统计^†^†出版年：2022^†^†页码范围：优化机器学习方法以在深度透镜调查中发现强引力透镜–[15](#A1.F15 "图
    15 ‣ 附录 A 模型性能和最终透镜样本 ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Under rare alignment configurations, the gravitational potential of a massive
    galaxy can cause light from a distant galaxy located behind it to take multiple
    paths around it. This results in the formation of several distinct images of the
    distant galaxy around the massive galaxy, a phenomenon known as strong gravitational
    lensing (e.g., Treu, [2010](#bib.bib76)). These multiple images are magnified
    by factors that can reach $>$10 times, making them appear brighter and more spatially
    extended. Such magnification makes these systems ideal for studying the formation
    and evolution of galaxies across cosmic time (e.g., Wuyts et al., [2014](#bib.bib81);
    Pettini et al., [2002](#bib.bib59); Swinbank et al., [2009](#bib.bib73); Koopmans
    et al., [2006](#bib.bib34); Leethochawalit et al., [2016](#bib.bib45)), while
    analysis of the lensing mass distribution enables insight into the nature of dark
    matter (e.g., Chiba, [2002](#bib.bib13); Bradač et al., [2002](#bib.bib10); Miranda
    & Macciò, [2007](#bib.bib51); Gilman et al., [2019](#bib.bib21); Shajib et al.,
    [2022](#bib.bib65)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在稀有的对准配置下，大质量星系的引力势可以使位于其后方的遥远星系的光线绕过它，形成多个路径。这导致在大质量星系周围形成几个不同的遥远星系图像，这种现象被称为强引力透镜效应（例如，Treu，[2010](#bib.bib76)）。这些多个图像的放大倍数可以达到$>$10倍，使它们显得更加明亮和空间扩展。这样的放大使这些系统成为研究星系在宇宙时间中形成和演化的理想对象（例如，Wuyts等，[2014](#bib.bib81)；Pettini等，[2002](#bib.bib59)；Swinbank等，[2009](#bib.bib73)；Koopmans等，[2006](#bib.bib34)；Leethochawalit等，[2016](#bib.bib45)），同时对透镜质量分布的分析能够提供有关暗物质性质的见解（例如，Chiba，[2002](#bib.bib13)；Bradač等，[2002](#bib.bib10)；Miranda
    & Macciò，[2007](#bib.bib51)；Gilman等，[2019](#bib.bib21)；Shajib等，[2022](#bib.bib65)）。
- en: The main current challenge in working with strong lens systems is their scarcity
    on the sky. Therefore, methods which are able to efficiently identify lensed galaxies
    from wide-area sky surveys are extremely beneficial. Automated methods will be
    especially valuable for lens searches in upcoming wide-area sky surveys to be
    carried out by the Vera Rubin Observatory, Euclid, and Roman (e.g., LSST Science
    Collaboration et al., [2009](#bib.bib40); Laureijs et al., [2011](#bib.bib42);
    Spergel et al., [2015](#bib.bib70)), whose improvements in sensitivity, angular
    resolution and sky coverage will enable detection of far more lens samples than
    are currently known.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 目前处理强透镜系统的主要挑战是它们在天空中的稀缺性。因此，能够从广域天空调查中高效识别透镜星系的方法非常有益。自动化方法将特别有价值，用于即将由维拉·鲁宾天文台、欧几里得和罗曼进行的广域天空调查的透镜搜索（例如，LSST科学协作组等，[2009](#bib.bib40)；Laureijs等，[2011](#bib.bib42)；Spergel等，[2015](#bib.bib70)），这些调查的灵敏度、角分辨率和天空覆盖范围的提升将使得能够检测到比目前已知的更多的透镜样本。
- en: Early approaches to finding strong lens systems included various algorithms
    searching for multiple lensed images or arc shapes, manual searches around massive
    galaxies, and citizen science projects (e.g., Moustakas et al., [2007](#bib.bib54);
    Paraficz et al., [2016](#bib.bib58); Seidel & Bartelmann, [2007](#bib.bib63);
    Gavazzi et al., [2014](#bib.bib20); Alard, [2006](#bib.bib2); Fassnacht et al.,
    [2004](#bib.bib17); More et al., [2016](#bib.bib53); Belokurov et al., [2009](#bib.bib5);
    Diehl et al., [2009](#bib.bib15); Garvin et al., [2022](#bib.bib19)). While successful,
    these methods are time-consuming and difficult to incorporate into an automated
    framework. Convolutional Neural Networks (CNNs; LeCun et al., [1989](#bib.bib43);
    Krizhevsky et al., [2012](#bib.bib38)), which have been successfully developed
    into a standard tool in the field of computer vision in the past decade, are a
    promising approach to solving image recognition problems. Depending on the problem,
    there are various neural network architectures that can be optimized for the desired
    objectives. CNNs and machine learning techniques in general have indeed been used
    with success in the past few years to uncover gravitationally lensed candidates
    in wide-area imaging surveys (e.g., Jacobs et al., [2017](#bib.bib30); Jacobs
    et al., [2019](#bib.bib31); Sonnenfeld et al., [2018](#bib.bib69); Pourrahmani
    et al., [2018](#bib.bib60); Huang et al., [2020](#bib.bib27); Li et al., [2020](#bib.bib46);
    Cañameras et al., [2020](#bib.bib11)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 早期寻找强引力透镜系统的方法包括各种算法，这些算法搜索多重透镜图像或弧形，围绕大质量星系进行手动搜索，以及公民科学项目（例如，Moustakas 等，[2007](#bib.bib54)；Paraficz
    等，[2016](#bib.bib58)；Seidel & Bartelmann，[2007](#bib.bib63)；Gavazzi 等，[2014](#bib.bib20)；Alard，[2006](#bib.bib2)；Fassnacht
    等，[2004](#bib.bib17)；More 等，[2016](#bib.bib53)；Belokurov 等，[2009](#bib.bib5)；Diehl
    等，[2009](#bib.bib15)；Garvin 等，[2022](#bib.bib19)）。虽然这些方法成功了，但它们耗时且难以纳入自动化框架。卷积神经网络（CNN；LeCun
    等，[1989](#bib.bib43)；Krizhevsky 等，[2012](#bib.bib38)），在过去十年中已成功发展为计算机视觉领域的标准工具，是解决图像识别问题的有前途的方法。根据问题的不同，有各种神经网络架构可以优化以实现预期目标。CNN
    和机器学习技术通常已在过去几年中成功用于发现广域成像调查中的引力透镜候选者（例如，Jacobs 等，[2017](#bib.bib30)；Jacobs 等，[2019](#bib.bib31)；Sonnenfeld
    等，[2018](#bib.bib69)；Pourrahmani 等，[2018](#bib.bib60)；Huang 等，[2020](#bib.bib27)；Li
    等，[2020](#bib.bib46)；Cañameras 等，[2020](#bib.bib11)）。
- en: Most machine learning searches for lenses have relied primarily on supervised
    learning methods (i.e., using a data set consisting of labeled lensed and non-lensed
    galaxies to train a model). However, while non-lensed galaxies are plentiful,
    current surveys have very few known lenses to be used as positive labels. Instead,
    machine learning models are trained on simulated lenses, which can be generated
    in abundance (e.g., Jacobs et al., [2017](#bib.bib30)). However, this presents
    a new problem, that the training data distribution (i.e., the simulated lenses)
    differs from the test data distribution (i.e., the real lenses) – a problem called
    distribution shift (Quinonero-Candela et al., [2008](#bib.bib61)). To overcome
    distribution shift, machine learning researchers have repurposed semi-supervised
    learning methods, which use unlabeled data and data augmentation to adapt the
    trained model to the test data (Berthelot et al., [2021](#bib.bib7)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习搜索透镜主要依赖于监督学习方法（即，使用包含标记的透镜和非透镜星系的数据集来训练模型）。然而，尽管非透镜星系很多，但当前调查中已知的透镜非常少，无法用作正样本标签。相反，机器学习模型是在模拟透镜上训练的，这些透镜可以大量生成（例如，Jacobs
    等，[2017](#bib.bib30)）。然而，这带来了一个新问题，即训练数据分布（即模拟透镜）与测试数据分布（即真实透镜）不同——这个问题被称为分布转移（Quinonero-Candela
    等，[2008](#bib.bib61)）。为了克服分布转移，机器学习研究人员重新利用了半监督学习方法，这些方法利用未标记数据和数据增强来使训练模型适应测试数据（Berthelot
    等，[2021](#bib.bib7)）。
- en: An advantage to the semi-supervised learning approach is that it can learn from
    the abundance of unlabeled images from the survey, which allows models to generalize
    better to unseen images. This is particularly useful to improve performance given
    millions of galaxy images that are detected in sky surveys but not included in
    the training data. The model performance is further improved through augmentations
    applied to images during training (e.g., translation and rotation). In addition
    to conventional transformations, a rich source of data augmentation can be derived
    by making use of unsupervised learning algorithms (e.g., Goodfellow et al., [2014](#bib.bib22);
    Kingma & Welling, [2014](#bib.bib32); Erhan et al., [2010](#bib.bib16)). Given
    the range of methodologies available, we now seek to address the question of which
    combination of machine learning methods (supervised and semi-supervised) and augmentations
    are best suited for finding strong gravitational lenses.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习方法的一个优势在于它可以从调查中大量未标记的图像中学习，这使得模型能够更好地对未见过的图像进行泛化。这对于提高性能尤其有用，因为在天空调查中检测到的数百万张银河图像并未包含在训练数据中。通过在训练期间对图像应用增强（例如，平移和旋转），模型性能进一步得到提升。除了传统的变换外，还可以通过利用无监督学习算法（例如，Goodfellow等，[2014](#bib.bib22)；Kingma
    & Welling，[2014](#bib.bib32)；Erhan等，[2010](#bib.bib16)）获得丰富的数据增强来源。考虑到可用的方法范围，我们现在寻求解决哪种机器学习方法（监督和半监督）以及增强组合最适合寻找强引力透镜的问题。
- en: We seek efficient models which minimize human effort by reducing the number
    of images that must be visually inspected to recover a given sample of lenses.
    In this work we apply CNN models to the Deep Lens Survey (DLS; Wittman et al.
    [2002](#bib.bib78)), which has relatively good image quality and also remains
    relatively unexplored in terms of machine learning searches, thus serving as a
    good testbed for this study. Also, because of the small size of known lenses from
    the DLS survey, we reserve those for use only in our test dataset. Training and
    validation datasets will only contain simulated lenses. In our previous methodology
    paper (Sheng et al., [2022](#bib.bib66), hereafter [S22](#bib.bib66)), we discussed
    the CNN models and lens detection techniques used in this work. Herein, we describe
    our training data in detail and focus on evaluating the performance of the different
    models on the DLS dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求有效的模型，通过减少必须视觉检查的图像数量来最小化人力劳动，以恢复给定的透镜样本。在这项工作中，我们将CNN模型应用于深度透镜调查（DLS；Wittman等，[2002](#bib.bib78)），该调查具有相对良好的图像质量，并且在机器学习搜索方面仍然相对未被探索，因此作为这项研究的良好测试平台。此外，由于DLS调查中已知透镜的数量较少，我们将这些透镜仅保留用于我们的测试数据集。训练和验证数据集将只包含模拟透镜。在我们之前的方法论文（Sheng等，[2022](#bib.bib66)，以下简称[S22](#bib.bib66)）中，我们讨论了在这项工作中使用的CNN模型和透镜检测技术。在这里，我们详细描述了我们的训练数据，并专注于评估不同模型在DLS数据集上的表现。
- en: This paper is organized as follows. In Section [2](#S2 "2 Deep Lens Survey Data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") we give an overview of the Deep Lens Survey and our
    source selection used for this work. We summarize our machine learning architecture
    and learning methods in Section [3](#S3 "3 Deep Learning Architecture and learning
    methods used ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey"). Section [4](#S4 "4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") describes the method used to generate training, validation,
    and testing data from DLS images. Section [5](#S5 "5 Metric to evaluate model
    performance ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") discusses our metric to evaluate the performance
    of the different CNN models. We discuss the results from our experiments in Section [6](#S6
    "6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey"), including the sample of new lens
    candidates from DLS and spectroscopic confirmation of two systems. Finally, we
    summarize the main conclusions in Section [7](#S7 "7 Conclusions ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey"). Throughout this paper we use the AB magnitude system and a $\Lambda$CDM
    cosmology with $\Omega_{M}=0.3$, $\Omega_{\Lambda}=0.7$ and $\mathrm{H}_{0}=70$ km s^(-1) Mpc^(-1).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文组织结构如下。在第[2节](#S2 "2 Deep Lens Survey Data ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")中，我们概述了深场透镜调查及用于本工作的源选择。在第[3节](#S3
    "3 Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")中，我们总结了我们的机器学习架构和学习方法。第[4节](#S4
    "4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey")描述了从DLS图像中生成训练、验证和测试数据的方法。第[5节](#S5
    "5 Metric to evaluate model performance ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")讨论了我们用来评估不同CNN模型性能的指标。我们在第[6节](#S6
    "6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")中讨论了实验结果，包括来自DLS的新透镜候选样本及两个系统的光谱确认。最后，我们在第[7节](#S7
    "7 Conclusions ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")总结了主要结论。本文中我们使用AB星等系统和一个$\Lambda$CDM宇宙学模型，$\Omega_{M}=0.3$，$\Omega_{\Lambda}=0.7$，以及$\mathrm{H}_{0}=70$ km s^(-1) Mpc^(-1)。
- en: 2 Deep Lens Survey Data
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深场透镜调查数据
- en: Here we give a brief overview of imaging data from the Deep Lens Survey (DLS)
    which we use to test and optimize strong lens detection methods. The DLS consists
    of relatively deep imaging over 20 square degrees in five independent $2^{\circ}\times
    2^{\circ}$ fields which are widely separated in the sky (Wittman et al., [2002](#bib.bib78)).
    Each field was imaged in BVRz photometric filters (Schmidt & Thorman, [2013](#bib.bib62))
    using the 4-meter Mayall telescope at Kitt Peak National Observatory or Blanco
    telescope at Cerro Tololo Inter-American Observatory, depending on declination.
    The survey was carried out over $\sim$120 nights. The survey was designed for
    weak gravitational lensing measurements, with stringent requirements on image
    quality and limiting magnitude, such that the data are naturally well suited for
    identifying strong lens systems. Typical $5\sigma$ point-source detection limits
    are 25.8, 26.3, and 26.9 AB magnitude in the $B$, $V$, and $R$ filters respectively
    (Schmidt & Thorman, [2013](#bib.bib62)). The $R$ band limit is only $\sim$0.6
    magnitudes shallower than the expected depth to be reached by Rubin observatory’s
    10-year survey (Ivezić et al., [2019](#bib.bib29)). The seeing is by design best
    in the R band (FWHM$\lesssim$0$\aas@@fstack{\prime\prime}$9) and is typically
    $\gtrsim$0$\aas@@fstack{\prime\prime}$9 in the B, V, and z bands (Wittman et al.,
    [2002](#bib.bib78)). Images in the $z$ band are shallowest and typically subject
    to worse seeing conditions. In this paper, we use only the $BVR$ data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简要概述了来自深度透镜调查（DLS）的成像数据，这些数据用于测试和优化强透镜检测方法。DLS由五个独立的$2^{\circ}\times 2^{\circ}$区域组成，覆盖20平方度的相对深层成像，这些区域在天空中相距较远（Wittman等人，[2002](#bib.bib78)）。每个区域都使用4米Mayall望远镜（位于Kitt
    Peak国家天文台）或Blanco望远镜（位于Cerro Tololo美洲天文台）在BVRz光度滤镜下进行成像，具体取决于赤纬。该调查持续了约120个夜晚。调查旨在进行弱引力透镜测量，对图像质量和极限星等有严格要求，因此数据自然适合识别强透镜系统。典型的$5\sigma$点源检测极限在$B$、$V$和$R$滤镜中分别为25.8、26.3和26.9
    AB星等（Schmidt & Thorman，[2013](#bib.bib62)）。$R$波段的极限仅比Rubin天文台10年调查的预期深度浅约0.6个星等（Ivezić等人，[2019](#bib.bib29)）。R波段的观测条件设计上最佳（FWHM$\lesssim$0$\aas@@fstack{\prime\prime}$9），而在B、V和z波段通常为$\gtrsim$0$\aas@@fstack{\prime\prime}$9（Wittman等人，[2002](#bib.bib78)）。$z$波段的图像最浅，通常受更差的观测条件影响。在本文中，我们仅使用$BVR$数据。
- en: 2.1 Source selection and regions of interest
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 来源选择和感兴趣区域
- en: The DLS catalog includes $\sim$5 million detected galaxies across 20 square
    degrees. However, only those of moderate redshift and relatively high mass will
    act as detectable strong lenses (i.e., with Einstein radii $\Theta_{E}\gtrsim
    1$ arcsecond). We applied a magnitude cut of $17.5<R<22$ (similar to that used
    by Jacobs et al. [2017](#bib.bib30)) in order to remove objects which are unlikely
    to produce a detectable lensing effect. Additionally, we use SExtractor (Bertin
    & Arnouts, [1996](#bib.bib8)) flags to eliminate saturated low-redshift galaxies,
    and exclusion masks to remove galaxies around bright stars or at the edge of the
    field. This results in 281,425 objects (hereafter referred to as the SurveyCatalog).
    We find that SExtractor flags and exclusion masks remove $\sim 5\%$ of the galaxies
    from the survey which reduces the effective sky area probed by our SurveyCatalog
    to $\sim 19$ square degrees. We set aside 2277 ($\sim$0.8%) randomly sampled object
    images from this catalog to experiment and tune the HumVI scaling parameters (discussed
    in Section [4.1](#S4.SS1 "4.1 Generating the NonLenses dataset ‣ 4 Training and
    Validation data ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). All model training analysis in this paper pertains
    to the remaining set of 279,149 objects (hereafter referred to as the TrainCatalog).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: DLS目录包括在20平方度范围内检测到的约500万只星系。然而，只有那些具有中等红移和相对较高质量的星系才会成为可检测的强引力透镜（即，具有爱因斯坦半径$\Theta_{E}\gtrsim
    1$角秒）。我们应用了$17.5<R<22$的星等切割（类似于Jacobs等人[2017](#bib.bib30)使用的），以去除那些不太可能产生可检测透镜效应的天体。此外，我们使用SExtractor（Bertin
    & Arnouts，[1996](#bib.bib8)）标记来排除饱和的低红移星系，并使用排除掩模来去除位于亮星周围或视场边缘的星系。这导致281,425个天体（以下简称为SurveyCatalog）。我们发现，SExtractor标记和排除掩模去除了约5%的星系，使我们的SurveyCatalog有效的观测天空面积减少到约19平方度。我们从该目录中抽取了2277个（约0.8%）随机采样的天体图像，用于实验和调整HumVI缩放参数（详见第[4.1](#S4.SS1
    "4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")节）。本文中的所有模型训练分析涉及剩余的279,149个天体（以下简称为TrainCatalog）。
- en: For our analysis we extract image cutouts spanning 25$\aas@@fstack{\prime\prime}$7
    $\times$ 25$\aas@@fstack{\prime\prime}$7 (100 $\times$ 100 pixels) centered on
    each object. This size is sufficient for galaxy- and group-scale lenses ($\Theta_{E}\lesssim
    12$”); we do not focus on the most massive cluster lenses which are already well
    cataloged (Ascaso et al., [2014](#bib.bib4)) and simpler to identify. We create
    color-composite images from the source $BVR$ FITS files for all targets in the
    SurveyCatalog (Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Generating the NonLenses dataset
    ‣ 4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"); discussed in detail in
    Section [4.1](#S4.SS1 "4.1 Generating the NonLenses dataset ‣ 4 Training and Validation
    data ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")). These color composite images have smaller file sizes
    compared to original data, enabling us to keep the rest of the analysis computationally
    efficient. These images are still able to capture the detected low-suface brightness
    features, while not saturating the brightest objects of interest for this work.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们提取了每个目标的图像剪切区域，大小为25$\aas@@fstack{\prime\prime}$7 $\times$ 25$\aas@@fstack{\prime\prime}$7（100
    $\times$ 100 像素），并以每个物体为中心。这一尺寸足以适应银河系和星系群尺度的透镜（$\Theta_{E}\lesssim 12$”）；我们不关注已经被很好
    catalog 的最大质量的星系团透镜（Ascaso 等，[2014](#bib.bib4)），这些透镜已经很容易识别。我们从 SurveyCatalog
    中所有目标的源 $BVR$ FITS 文件中创建彩色合成图像（图 [4](#S4.F4 "图 4 ‣ 4.1 生成非透镜数据集 ‣ 4 训练和验证数据 ‣
    优化机器学习方法以发现深度透镜调查中的强引力透镜")；详细讨论见第 [4.1](#S4.SS1 "4.1 生成非透镜数据集 ‣ 4 训练和验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")
    节）。这些彩色合成图像的文件大小比原始数据小，使我们能够保持其余分析的计算效率。这些图像仍能捕捉到检测到的低表面亮度特征，同时不会使本研究关注的最亮物体饱和。
- en: Additionally, they are better suited for the machine learning architecture and
    methods used in this work (discussed in Section [3](#S3 "3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它们更适合于本研究中使用的机器学习架构和方法（详见第 [3](#S3 "3 深度学习架构和使用的学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")
    节").
- en: 3 Deep Learning Architecture and learning methods used
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习架构和使用的学习方法
- en: The task at hand is to establish a machine learning (ML) algorithm that efficiently
    classifies the 281,425 color-composite images from the survey into lensed and
    non-lensed galaxies. Furthermore, by ranking the images from highest predicted
    probability of being a lens to lowest, we can order the images for human inspection.
    This requires the selection of an architecture (i.e., a function that takes images
    as input and gives prediction probabilities as output) and learning methods (i.e.,
    a way for our function to learn from the data). The key components of our ML training
    pipeline are a supervised convolutional neural net (CNN), domain adaptation with
    semi-supervised learning, and augmenting training samples with generative adversarial
    nets (GAN). A more detailed account of our ML method can be found in [S22](#bib.bib66).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当前任务是建立一个有效分类281,425张调查中的彩色合成图像的机器学习（ML）算法，将其分为透镜和非透镜星系。此外，通过按图像被预测为透镜的概率从高到低排序，我们可以将图像按顺序供人工检查。这需要选择一个架构（即一个以图像为输入、以预测概率为输出的函数）和学习方法（即让我们的函数从数据中学习的方法）。我们机器学习训练管道的关键组件是监督卷积神经网络（CNN）、通过半监督学习进行的领域适应，以及通过生成对抗网络（GAN）增加训练样本。有关我们机器学习方法的更详细说明，请参见
    [S22](#bib.bib66)。
- en: 3.1 Convolutional neural network architecture
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 卷积神经网络架构
- en: '![Refer to caption](img/b1586cf65e58aa051a84202fc5c4d064.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b1586cf65e58aa051a84202fc5c4d064.png)'
- en: 'Figure 1: Schematic depiction of the ResNetV2 deep learning architecture used
    in this work. The input to the network is an RGB color-composite image generated
    from the BVR fits files (Section. [4.1](#S4.SS1 "4.1 Generating the NonLenses
    dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")), and the output
    is a value between 0 and 1 indicating the probability of the input image being
    a lens. The general network consists of three stacks, each containing $3n$ residual
    units. In this work, we use a stack size of $n=1$ resulting in a total of three
    residual units. Each residual unit consists of two sets of Batch Normalization
    (BN), Rectified Linear Unit activation function (ReLU), and Conv units, where
    Conv denotes a convolutional layer with kernel size $3\times 3$ and appropriate
    stride size. The network ends with global average pooling and a softmax layer.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：本研究中使用的 ResNetV2 深度学习架构的示意图。网络的输入是从 BVR fits 文件生成的 RGB 彩色合成图像（第 [4.1](#S4.SS1
    "4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") 节），输出是一个介于 0 和 1 之间的值，表示输入图像为透镜的概率。一般网络由三个堆叠组成，每个堆叠包含 $3n$ 个残差单元。在本工作中，我们使用堆叠大小为
    $n=1$，总共三个残差单元。每个残差单元包括两组批量归一化（BN）、修正线性单元激活函数（ReLU）和卷积单元，其中卷积单元表示具有 $3\times 3$
    核心大小和适当步幅大小的卷积层。网络以全局平均池化和 softmax 层结束。
- en: CNNs have previously been used for classifying and identifying lens candidates
    (e.g., Jacobs et al., [2017](#bib.bib30)). They are a specific form of neural
    network that learns translation invariant representations via trainable convolution
    kernels. This is particularly well suited to astronomical images where patterns
    are repeated throughout the sky. Deep CNNs are models where these learned non-linear
    representations of the image (called layers) are stacked on top of one another.
    Deep CNNs are trained using variations of stochastic gradient descent, where an
    objective function is evaluated on small subsets of the data, called mini-batches,
    and the parameters are updated by subtracting some fraction of the objective’s
    gradient.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 曾被用于分类和识别透镜候选（例如，Jacobs 等，[2017](#bib.bib30)）。它们是一种特定形式的神经网络，通过可训练的卷积核学习平移不变的表示。这尤其适合于天文图像，因为模式在整个天空中重复出现。深度
    CNNs 是那些将图像的非线性表示（称为层）堆叠在一起的模型。深度 CNNs 通过随机梯度下降的变体进行训练，其中在数据的小子集上评估目标函数，这些小子集称为
    mini-batches，参数通过减去目标梯度的一部分来更新。
- en: There are many choices of how precisely these layers are constructed and combined,
    such as selection of the convolutional kernel size, number of output channels
    for each convolution layer, the non-linear activation function, and the incorporation
    of other layers that improve performance such as Batch Normalization (Ioffe &
    Szegedy, [2015](#bib.bib28)). All of these details together are called the model
    architecture.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层的构建和组合方式有许多选择，例如卷积核大小的选择、每个卷积层的输出通道数、非线性激活函数的选择，以及其他能提升性能的层的引入，如批量归一化（Ioffe
    & Szegedy, [2015](#bib.bib28)）。所有这些细节共同构成了模型架构。
- en: 'We make use of the ResNet version-2 architecture (ResNetV2; He et al., [2016a](#bib.bib25),
    [b](#bib.bib26)) designed for the CIFAR10 dataset (Krizhevsky, [2009a](#bib.bib36)),
    shown schematically in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Convolutional neural
    network architecture ‣ 3 Deep Learning Architecture and learning methods used
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). It is one of the widely used industry standard networks
    for image classification problems (e.g., Litjens et al., [2017](#bib.bib47); Gu
    et al., [2018](#bib.bib23); Madireddy et al., [2019](#bib.bib49)). The ResNetV2
    used in this work consists of three stacks (see Figure [1](#S3.F1 "Figure 1 ‣
    3.1 Convolutional neural network architecture ‣ 3 Deep Learning Architecture and
    learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey"); green blocks) and each stack consists
    of $n$ residual unit blocks, where $n$ is a parameter to be chosen that controls
    the depth of the neural network. A deeper neural network has more learning capacity
    but requires more computational power and training samples. Each residual unit
    block consists of three convolution layers of kernel size $3\times 3$ and one
    skip connection. To match the feature map dimensions (width, height) and the number
    of channels between stacks, a few extra convolution layers are included at the
    input and the beginning block of each stack. Therefore, $9n+4$ convolution layers
    are present in the network in total. For all the models used in this work, we
    adopt $n=1$. With strided convolutions, the feature map dimensions to each stack
    decrease by a factor of $1/2$. The number of input and output channels to each
    stack are: $(16\rightarrow 64),~{}(64\rightarrow 128),~{}(128\rightarrow 256)$.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了为CIFAR10数据集（Krizhevsky, [2009a](#bib.bib36)）设计的ResNet版本2架构（ResNetV2; He
    et al., [2016a](#bib.bib25), [b](#bib.bib26)），如图[1](#S3.F1 "图 1 ‣ 3.1 卷积神经网络架构
    ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")所示。它是用于图像分类问题的广泛使用的行业标准网络之一（例如，Litjens
    et al., [2017](#bib.bib47); Gu et al., [2018](#bib.bib23); Madireddy et al., [2019](#bib.bib49)）。在这项工作中使用的ResNetV2包含三个堆叠（见图[1](#S3.F1
    "图 1 ‣ 3.1 卷积神经网络架构 ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")；绿色块），每个堆叠由$n$个残差单元块组成，其中$n$是一个可选参数，用于控制神经网络的深度。更深的神经网络具有更强的学习能力，但需要更多的计算能力和训练样本。每个残差单元块由三个核大小为$3\times
    3$的卷积层和一个跳跃连接组成。为了匹配堆叠之间的特征图维度（宽度，高度）和通道数量，在每个堆叠的输入和起始块处包括了一些额外的卷积层。因此，网络中总共有$9n+4$个卷积层。对于本工作中使用的所有模型，我们采用$n=1$。通过步幅卷积，每个堆叠的特征图维度减少了$1/2$倍。每个堆叠的输入和输出通道数分别为：（$16\rightarrow
    64$），（$64\rightarrow 128$），（$128\rightarrow 256$）。
- en: The network ends with global average pooling, a fully-connected layer and softmax.
    The global average pooling constrains the output to be rotationally invariant.
    The softmax transforms the output to be a value between 0 and 1 which can be interpreted
    as a probability. Throughout this work, a value of 1 is designated for lensed
    candidates (referred to herein as Lenses) and 0 for nonlensed candidates (referred
    to as NonLenses).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 网络以全局平均池化、一个全连接层和softmax结束。全局平均池化将输出限制为旋转不变的。Softmax将输出转换为0和1之间的值，可以解释为概率。在本工作中，值1被指定为透镜候选（以下简称为Lenses），值0则为非透镜候选（以下简称为NonLenses）。
- en: 3.2 Domain adaptation with semi-supervised learning
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 半监督学习的领域适应
- en: In supervised learning, our algorithm is trained via mini-batches of images
    $X$ and corresponding labels $y$ ($1$ for Lenses and $0$ for NonLenses). The algorithm
    then tries to learn the neural network parameters, collectively referred to as
    $\Theta$. The output of the neural network after the softmax activation produces
    a prediction $p_{\Theta}(X)$, which is our predicted probability of $X$ being
    a lens. Our supervised learning objective function is the cross-entropy loss function,
    denoted $\ell_{S}$, which is a measure of the quality of our predictions, $p_{\Theta}(X)$,
    when compared to the true labels, $y$. Merely using supervised learning does not
    perform well in the face of distributional shift, and we turn to semi-supervised
    learning (SSL) methods which make use of the unlabeled test data to adapt to this
    domain.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们的算法通过图像 $X$ 和相应的标签 $y$（$1$ 代表镜头，$0$ 代表非镜头）的迷你批次进行训练。算法然后尝试学习神经网络参数，统称为
    $\Theta$。神经网络经过 softmax 激活后的输出产生一个预测 $p_{\Theta}(X)$，这是我们预测的 $X$ 为镜头的概率。我们的监督学习目标函数是交叉熵损失函数，表示为
    $\ell_{S}$，这是我们预测质量的度量，$p_{\Theta}(X)$ 与真实标签 $y$ 相比。仅使用监督学习在面对分布转移时表现不佳，因此我们转向半监督学习（SSL）方法，这些方法利用无标签的测试数据来适应这个领域。
- en: '| RGB-shuffle | Randomly perturb the order of the channels in the images |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| RGB-shuffle | 随机扰动图像中通道的顺序 |'
- en: '| --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JPEG-quality | 50-100% |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| JPEG-quality | 50-100% |'
- en: '| Rot90 | Randomly rotate the images by a multiple of 90 degrees |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Rot90 | 随机将图像旋转90度的倍数 |'
- en: '| Translations | Randomly translate the images by at most 20 pixels in the
    up, down, left and right directions |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Translations | 随机将图像在上下左右方向上平移最多 20 像素 |'
- en: '| Horizontal flips | Randomly flips the images across the x-axis |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Horizontal flips | 随机在 x 轴上翻转图像 |'
- en: '| Color augmentation | Randomly perturb the brightness(-0.1-0.1), saturation(0.9-1.3),
    hue(0.96-1.00), and gamma(1.23-1.25) of the images |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Color augmentation | 随机扰动图像的亮度（-0.1-0.1）、饱和度（0.9-1.3）、色调（0.96-1.00）和伽玛（1.23-1.25）
    |'
- en: 'Table 1: Data augmentations used on images in the semi-supervised training
    pipeline.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 在半监督训练流程中使用的数据增强方法。'
- en: There are many semi-supervised approaches to deep learning. The methods we explore
    are FixMatch¹¹1FixMatch was not part of the original lens search study since this
    technique had not been published at the time. We are including it in our results
    here to be thorough. (Sohn et al., [2020](#bib.bib67)), MixMatch (Berthelot et al.,
    [2019](#bib.bib6)), Virtual Adversarial Training (Miyato et al., [2019](#bib.bib52)),
    Mean Teacher (Tarvainen & Valpola, [2017](#bib.bib74)), $\Pi$-Model (Laine & Aila,
    [2017](#bib.bib41)), and Pseudo-Labeling (Lee, [2013](#bib.bib44)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多半监督学习方法可以用于深度学习。我们探索的方法包括 FixMatch¹¹1FixMatch 不是原始镜头搜索研究的一部分，因为当时该技术尚未发布。我们在这里将其包含在结果中，以确保全面。（Sohn
    等，[2020](#bib.bib67)）、MixMatch（Berthelot 等，[2019](#bib.bib6)）、虚拟对抗训练（Miyato 等，[2019](#bib.bib52)）、平均教师（Tarvainen
    & Valpola，[2017](#bib.bib74)）、$\Pi$-模型（Laine & Aila，[2017](#bib.bib41)）和伪标签（Lee，[2013](#bib.bib44)）。
- en: Most SSL algorithms follow the same template. We minimize an objective function
    consisting of a supervised component (i.e. $\ell_{S}$ losses), where the label
    is provided, plus an unsupervised component (i.e. $\ell_{U}$ losses). Both are
    optimized together over mini-batches, now consisting of labeled and unlabeled
    data, but without significant modification to the stochastic gradient descent
    algorithm. The main feature that distinguishes our setting from typical SSL is
    that our training NonLenses and test set come from the same pool of data, while
    the simulated Lenses do not exist in the test data. This is in contrast to Jacobs
    et al. ([2017](#bib.bib30)) for example, in which they produce simulated NonLenses
    as well, but do not attempt domain adaptation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 SSL 算法遵循相同的模板。我们最小化一个包含监督部分（即 $\ell_{S}$ 损失）的目标函数，其中提供了标签，以及一个无监督部分（即 $\ell_{U}$
    损失）。两者在迷你批次中一起优化，现在包括标记数据和无标签数据，但对随机梯度下降算法没有显著修改。区别于典型 SSL 的主要特征是我们的训练非镜头和测试集来自相同的数据池，而模拟镜头在测试数据中不存在。这与
    Jacobs 等人（[2017](#bib.bib30)）不同，他们也生成模拟非镜头，但未尝试领域适应。
- en: In the Pseudo-Label algorithm (Lee, [2013](#bib.bib44)), we assign pseudo-labels
    to unlabeled data by taking the model’s predicted class as the label. We can then
    use the same loss as in the supervised task (i.e., $\ell_{S}=\ell_{U}$). The motivation
    is that we are implicitly enforcing entropy minimization by forcing the model
    to be confident on unlabeled samples. An alternative approach to SSL is consistency
    regularization, where two independently augmented samples of the same test image
    are encouraged to produce similar predictions. The $\Pi$-model algorithm (Laine
    & Aila, [2017](#bib.bib41)) directly uses consistency regularization. The idea
    is to take two random augmentations of the same sample data point, $X$, and compute
    the squared difference of the model outputs for the augmented copies. We use $\text{aug},\widetilde{\text{aug}}$
    to denote two independent augmentations, which can be produced by selecting different
    randomization seeds. The unsupervised loss is then
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在伪标签算法 (Lee, [2013](#bib.bib44)) 中，我们通过将模型预测的类别作为标签来为未标记的数据分配伪标签。然后，我们可以使用与监督任务中相同的损失函数
    (即，$\ell_{S}=\ell_{U}$)。其动机是通过迫使模型对未标记样本有信心，从而隐含地强制执行熵最小化。SSL 的一种替代方法是一致性正则化，其中鼓励同一测试图像的两个独立增强样本产生类似的预测。$\Pi$-模型算法
    (Laine & Aila, [2017](#bib.bib41)) 直接使用一致性正则化。其思想是对相同样本数据点 $X$ 进行两个随机增强，并计算增强副本模型输出的平方差异。我们使用
    $\text{aug},\widetilde{\text{aug}}$ 来表示两个独立的增强，这些增强可以通过选择不同的随机种子来产生。无监督损失为
- en: '|  | $\ell_{U}(X)=\left\&#124;p_{\Theta}(\text{aug}(X))-p_{\Theta}(\widetilde{\text{aug}}(X))\right\&#124;^{2}.$
    |  | (1) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\ell_{U}(X)=\left\|p_{\Theta}(\text{aug}(X))-p_{\Theta}(\widetilde{\text{aug}}(X))\right\|^{2}.$
    |  | (1) |'
- en: The choice of stochastic augmentation function is up to the modeler and will
    often be domain specific.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 随机增强函数的选择由建模者决定，并且通常是领域特定的。
- en: The Mean Teacher algorithm (Tarvainen & Valpola, [2017](#bib.bib74)) also uses
    consistency regularization, but replaces one of the augmentations in Equation [1](#S3.E1
    "In 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") with the output of the model using
    an exponential moving average (the teacher model) of model parameters, $\Theta$.
    FixMatch (Sohn et al., [2020](#bib.bib67)) and MixMatch (Berthelot et al., [2019](#bib.bib6))
    employ both consistency regularization and entropy minimization. MixMatch was
    originally proposed as a heuristic approach, and FixMatch was later derived as
    a more principled simplification of MixMatch and other related SSL methods. Virtual
    adversarial training (VAT; Miyato et al., [2019](#bib.bib52)) uses an adversarial,
    worst-case, augmentation. This adversarial augmentation pushes the image in the
    direction which will cause the greatest increase in loss. One downside to VAT
    is that the adversarial augmentations are not able to encode the domain specific
    prior information that random augmentations can provide (see Table [1](#S3.T1
    "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey")).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Mean Teacher 算法 (Tarvainen & Valpola, [2017](#bib.bib74)) 也使用一致性正则化，但将方程 [1](#S3.E1
    "In 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") 中的一个增强替换为使用模型参数的指数移动平均（教师模型）得到的输出
    $\Theta$。FixMatch (Sohn et al., [2020](#bib.bib67)) 和 MixMatch (Berthelot et al.,
    [2019](#bib.bib6)) 同时采用一致性正则化和熵最小化。MixMatch 最初被提出作为一种启发式方法，FixMatch 则被后来作为 MixMatch
    和其他相关 SSL 方法的更具原则性的简化方法推导出来。虚拟对抗训练 (VAT; Miyato et al., [2019](#bib.bib52)) 使用对抗性最坏情况的增强。这种对抗性增强推动图像向最大增加损失的方向移动。VAT
    的一个缺点是，对抗性增强无法编码随机增强可以提供的领域特定先验信息 (见表 [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation
    with semi-supervised learning ‣ 3 Deep Learning Architecture and learning methods
    used ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"))。
- en: 3.3 Data augmentation and GANs
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据增强与 GANs
- en: Data augmentation serves as a crucial regularizer in semi-supervised learning
    (SSL) algorithms. Several SSL algorithms, including those mentioned in this paper
    such as pi-model (Laine & Aila, [2017](#bib.bib41)), MixMatch (Berthelot et al.,
    [2019](#bib.bib6)), and fixMatch (Sohn et al., [2020](#bib.bib67)), utilize data
    augmentation techniques. The data augmentation techniques we employed in our study
    are provided in Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised
    learning ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey"), and are particularly well-suited for DLS images.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强在半监督学习（SSL）算法中起着至关重要的正则化作用。包括本文提到的算法如 pi-model（Laine & Aila，[2017](#bib.bib41)）、MixMatch（Berthelot
    等，[2019](#bib.bib6)）和 fixMatch（Sohn 等，[2020](#bib.bib67)）在内的几种 SSL 算法都利用了数据增强技术。我们在研究中使用的数据增强技术列于表
    [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning ‣ 3
    Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") 中，并特别适合
    DLS 图像。
- en: RGB-shuffle randomizes the order of channels and Color augmentation perturbs
    the colors in the images. These have the effect of accounting for systematic bias
    in channel and color information introduced by the simulation pipeline. JPEG-quality
    augmentation accounts for varying levels of noise and image quality, and applies
    to any color composite image irrespective of the format that the image is saved
    in (e.g., in this case we use png format instead of jpeg). Rot90, Translations,
    and Horizontal flips induce translational and rotational invariance in the predictions.
    Examples of these augmentations are shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.3
    Data augmentation and GANs ‣ 3 Deep Learning Architecture and learning methods
    used ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). We note that even though some augmentations (e.g.,
    RGB-shuffle) result in unrealistic images, our empirical tests described in Section [6.1.1](#S6.SS1.SSS1
    "6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with
    GANs and Augmentations have superior performance ‣ 6 Results and Discussion ‣
    Optimizing machine learning methods to discover strong gravitational lenses in
    the Deep Lens Survey") indicate that these augmentations yield improved model
    performance. Domain adaptation problems employing semi-supervised algorithms (SSLs)
    have been shown to benefit greatly from data augmentations in general (e.g., Sohn
    et al., [2020](#bib.bib67)), suggesting that this effect is not specific to our
    lens search.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: RGB-shuffle 随机化通道的顺序，Color augmentation 则扰动图像中的颜色。这些操作用于修正模拟管线中引入的通道和颜色信息的系统偏差。JPEG-quality
    augmentation 处理噪声和图像质量的不同水平，并适用于任何颜色合成图像，无论图像保存的格式如何（例如，我们在此使用 png 格式而非 jpeg）。Rot90、Translations
    和 Horizontal flips 在预测中引入平移和旋转不变性。这些增强的示例如图 [2](#S3.F2 "Figure 2 ‣ 3.3 Data augmentation
    and GANs ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") 所示。我们注意到，尽管一些增强（例如 RGB-shuffle）会导致不真实的图像，但我们在第 [6.1.1](#S6.SS1.SSS1 "6.1.1
    Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs
    and Augmentations have superior performance ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") 节中描述的实证测试表明，这些增强会提高模型性能。一般而言，采用半监督算法（SSL）的领域适应问题已经证明从数据增强中受益良多（例如，Sohn
    等，[2020](#bib.bib67)），这表明这一效果并非特定于我们的镜头搜索。
- en: A second tool that we use to augment our data is to generate new images that
    mimic the simulated lenses. In deep learning, the state-of-the-art method to produce
    generative models is by using Generative Adversarial Networks (GANs; Goodfellow
    et al., [2014](#bib.bib22); Arjovsky et al., [2017](#bib.bib3)). GANs generate
    unseen samples that are distinct from the original images, but are distributionally
    quite similar. These generative models are trained along with an adversarial discriminator
    that is attempting to distinguish between the fake and real images.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来增强数据的第二种工具是生成模拟镜头的新图像。在深度学习中，生成模型的最先进方法是使用生成对抗网络（GANs; Goodfellow 等，[2014](#bib.bib22);
    Arjovsky 等，[2017](#bib.bib3)）。GANs 生成的样本与原始图像不同，但在分布上相似。这些生成模型与一个对抗性判别器一起训练，判别器试图区分假图像和真实图像。
- en: We trained a WGAN-GP (Wasserstein GAN + Gradient Penalty; Gulrajani et al.,
    [2017](#bib.bib24)) on simulated lenses and add the generated images (see examples
    in Figures [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey") and [4](#S4.F4
    "Figure 4 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")) to our training set as another form of data augmentation.
    The motivation is that GANs can provide a rich source of more exotic data augmentations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在模拟透镜上训练了一个WGAN-GP（Wasserstein GAN + 梯度惩罚；Gulrajani等，[2017](#bib.bib24)），并将生成的图像（见图 [3](#S3.F3
    "图 3 ‣ 3.3 数据增强和GAN ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")和[4](#S4.F4 "图
    4 ‣ 4.1 生成NonLenses数据集 ‣ 4 训练和验证数据 ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")）添加到我们的训练集中，作为另一种数据增强形式。其动机是GAN可以提供更多异域数据增强的丰富来源。
- en: Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey") gives a brief summary
    of the steps discussed thus far. The training, testing, and validation data along
    with the model checkpoints used in this paper are made available on our GitHub
    repository ²²2[https://github.com/sxsheng/SHLDN](https://github.com/sxsheng/SHLDN).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S3.F3 "图 3 ‣ 3.3 数据增强和GAN ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")简要总结了迄今为止讨论的步骤。本文中使用的训练、测试和验证数据以及模型检查点已在我们的GitHub仓库中提供[https://github.com/sxsheng/SHLDN](https://github.com/sxsheng/SHLDN)。
- en: '![Refer to caption](img/f510c0503d08844f1df839c1640e3068.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f510c0503d08844f1df839c1640e3068.png)'
- en: '![Refer to caption](img/a4c0b4e8dd2cae2185c4cfcf19f4b7f9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a4c0b4e8dd2cae2185c4cfcf19f4b7f9.png)'
- en: 'Figure 2: Example of augmentations used during training. From left to right,
    the original RGB color composite image undergoes the series of augmentations described
    in Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning
    ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey"):
    RGB-shuffle, JPEG quality, Rot90, Translation, Flip, Color adjustment. The final
    image is then passed as input to the model.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：训练过程中使用的增强示例。从左到右，原始RGB彩色复合图像经过表 [1](#S3.T1 "表 1 ‣ 3.2 带半监督学习的领域适应 ‣ 3 深度学习架构和学习方法
    ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")中描述的一系列增强：RGB-洗牌、JPEG质量、Rot90、平移、翻转、颜色调整。最终图像作为输入传递给模型。
- en: '![Refer to caption](img/e4ea8ff5ca0bd5d87cf19e8d4b45f0e0.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e4ea8ff5ca0bd5d87cf19e8d4b45f0e0.png)'
- en: 'Figure 3: Schematic of the pipeline used in this work to test the performance
    of different learning methods described in Sections [3.2](#S3.SS2 "3.2 Domain
    adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture and learning
    methods used ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") and [3.3](#S3.SS3 "3.3 Data augmentation and
    GANs ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")
    (see text for details). The GAN generated lenses are only included in the training
    data for unsupervised learning methods (e.g., GAN+MixMatch).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：本研究中用于测试不同学习方法性能的流程图，详细信息见第 [3.2](#S3.SS2 "3.2 带半监督学习的领域适应 ‣ 3 深度学习架构和学习方法
    ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")和[3.3](#S3.SS3 "3.3 数据增强和GAN ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")节（具体信息请参见正文）。GAN生成的透镜仅包括在无监督学习方法（例如，GAN+MixMatch）的训练数据中。
- en: 4 Training and Validation data
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 训练和验证数据
- en: 'One of the challenges that we face in gravitational lens searches is trying
    to generate a training and testing dataset when having limited knowledge of the
    type of strong lenses that we might find in a survey. Prior to this work, Kubo
    & Dell’Antonio ([2008](#bib.bib39)) used a semi-automated method to search for
    lensed candidates in one of the DLS fields (F2) and uncovered two lens candidates.
    But in order to train a machine learning model to recognize lenses, we require
    Lens and NonLens image samples on the order of a few thousand. This is not a problem
    for NonLens galaxies, as they are abundant. But this is challenging for Lenses,
    as the known samples are extremely small compared to training requirements. We
    note that although the DLS area overlaps with other surveys used for strong lens
    searches (e.g., SDSS), no lens candidates have been published from these other
    surveys within the DLS footprint. This is likely due to the shallower depth of
    other surveys (see Section [6.4](#S6.SS4 "6.4 Implications for future large-area
    sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")). We must therefore generate an artificial lens training set. We describe
    our process of generating the training and testing datasets in this section.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在引力透镜搜索中面临的挑战之一是当对可能在调查中发现的强透镜类型了解有限时，生成训练和测试数据集。在此之前，Kubo 和 Dell’Antonio ([2008](#bib.bib39))
    使用了半自动化的方法在 DLS 区域（F2）搜索透镜候选者，并发现了两个透镜候选者。然而，为了训练机器学习模型以识别透镜，我们需要数量达到几千的透镜和非透镜图像样本。对于非透镜星系来说这不是问题，因为它们非常丰富。但对于透镜来说，这就很有挑战性，因为已知样本与训练需求相比极为稀少。我们注意到，尽管
    DLS 区域与用于强引力透镜搜索的其他调查（例如 SDSS）重叠，但在 DLS 区域内没有发布来自这些其他调查的透镜候选者。这可能是由于其他调查的深度较浅（参见第
    [6.4](#S6.SS4 "6.4 未来大范围天空调查的影响：灵敏度和角分辨率 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")
    节）。因此，我们必须生成一个人工透镜训练集。本节描述了生成训练和测试数据集的过程。
- en: 4.1 Generating the NonLenses dataset
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 生成 NonLenses 数据集
- en: Color png images centered on each object in the SurveyCatalog are constructed
    from $BVR$ fits files using HumVI (Marshall et al., [2015](#bib.bib50)). HumVI
    is based on the color composition algorithm described in Lupton et al. ([2004](#bib.bib48))
    and offers several tunable parameters to control the output image (e.g., contrast).
    We randomly sample objects from the SurveyCatalog and visually inspect the effect
    of changing the HumVI parameters $s$ and $p$ which control the contrast and color
    balance respectively. Although there is a degeneracy in the choice of these values,
    we pick ones that reasonably represent both the bright and dim features in the
    data (i.e., spanning the range of detectable surface brightness). Table [2](#S4.T2
    "Table 2 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") lists our chosen HumVI parameters and Figure [4](#S4.F4
    "Figure 4 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") (top panel) shows 4 randomly selected color-composite
    survey images generated using these values. The chosen HumVI parameters are kept
    constant and applied to all images in the survey. It is beyond the scope of this
    work to explore the effect of choosing different HumVI parameters on the performance
    of the models, but we note that color augmentations applied during training (Table [1](#S3.T1
    "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey"); Section [6.1.1](#S6.SS1.SSS1
    "6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with
    GANs and Augmentations have superior performance ‣ 6 Results and Discussion ‣
    Optimizing machine learning methods to discover strong gravitational lenses in
    the Deep Lens Survey")) have the effect of making our models invariant to small
    perturbations in color.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在SurveyCatalog中，每个对象的彩色png图像是通过使用HumVI（Marshall等，[2015](#bib.bib50)）从$BVR$拟合文件构建的。HumVI基于Lupton等（[2004](#bib.bib48)）描述的颜色合成算法，并提供了多个可调参数来控制输出图像（例如，对比度）。我们随机从SurveyCatalog中抽取对象，直观检查改变HumVI参数$s$和$p$对对比度和颜色平衡的效果。尽管这些值的选择存在退化现象，我们选择了能够合理代表数据中明亮和暗淡特征的值（即，涵盖可检测的表面亮度范围）。表[2](#S4.T2
    "Table 2 ‣ 4.1 生成NonLenses数据集 ‣ 4 训练和验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")列出了我们选择的HumVI参数，图[4](#S4.F4
    "Figure 4 ‣ 4.1 生成NonLenses数据集 ‣ 4 训练和验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")（顶部面板）显示了使用这些值生成的4张随机选择的彩色合成调查图像。选择的HumVI参数保持不变，并应用于调查中的所有图像。探索选择不同HumVI参数对模型性能的影响超出了本工作的范围，但我们注意到，训练过程中应用的颜色增强（表[1](#S3.T1
    "Table 1 ‣ 3.2 领域适应与半监督学习 ‣ 3 深度学习架构及学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜"); 第[6.1.1](#S6.SS1.SSS1
    "6.1.1 数据增强的消融研究 ‣ 6.1 半监督算法与GANs和增强具有优越性能 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")节）使我们的模型对颜色的小扰动具有不变性。
- en: '![Refer to caption](img/76f1bc3fcff40ad920ebab66d215aaa6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/76f1bc3fcff40ad920ebab66d215aaa6.png)'
- en: 'Figure 4: *Top row:* Four randomly selected color composite survey images generated
    by running HumVI on their respective BVR FITS files. These images are examples
    of NonLenses used for training the network. Each image spans 25$\aas@@fstack{\prime\prime}$7
    $\times$ 25$\aas@@fstack{\prime\prime}$7 on the sky. Table [2](#S4.T2 "Table 2
    ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") lists the HumVI parameters used to generate these images. *Middle row:*
    The same set of survey images as in the top row, but superimposed with simulated
    lens configurations generated with glafic. Section [4.2](#S4.SS2 "4.2 Generating
    the simulated Lenses dataset ‣ 4 Training and Validation data ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")
    discusses the steps involved in detail. These images are examples of Lenses used
    during training. *Bottom row:* GAN generated simulated lenses. These are added
    to the training data as Lenses for our unsupervised models (e.g., GAN+MixMatch;
    Section. [3.3](#S3.SS3 "3.3 Data augmentation and GANs ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：*顶部行：* 四张随机选择的彩色合成调查图像，由运行HumVI生成。这些图像是用于训练网络的NonLenses示例。每张图像在天空上覆盖25$\aas@@fstack{\prime\prime}$7
    $\times$ 25$\aas@@fstack{\prime\prime}$7。表[2](#S4.T2 "表 2 ‣ 4.1 生成NonLenses数据集
    ‣ 4 训练与验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")列出了用于生成这些图像的HumVI参数。*中间行：* 与顶部行相同的调查图像集合，但叠加了使用glafic生成的模拟透镜配置。第[4.2节](#S4.SS2
    "4.2 生成模拟Lenses数据集 ‣ 4 训练与验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")详细讨论了相关步骤。这些图像是训练过程中使用的Lenses示例。*底部行：*
    GAN生成的模拟透镜。这些透镜作为Lenses添加到训练数据中，用于我们的无监督模型（例如，GAN+MixMatch；第[3.3节](#S3.SS3 "3.3
    数据增强与GANs ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")）。
- en: '|  | Parameter | Value |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | 参数 | 值 |'
- en: '|  | glafic |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | glafic |  |'
- en: '| Position | $x_{\text{def}},y_{\text{def}},x_{\text{src}},y_{\text{src}}$
    | U(-0.5,0.5) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | $x_{\text{def}},y_{\text{def}},x_{\text{src}},y_{\text{src}}$ | U(-0.5,0.5)
    |'
- en: '| (arcseconds) |  |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| (角秒) |  |  |'
- en: '| PA | $\theta_{\text{def}},\theta_{\text{src}}$ | U(0,180) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| PA | $\theta_{\text{def}},\theta_{\text{src}}$ | U(0,180) |'
- en: '| (degrees) |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| (度) |  |  |'
- en: '| Ellipticity | $e_{\text{def}},e_{\text{src}}$ | U(0.3,0.7) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 椭圆率 | $e_{\text{def}},e_{\text{src}}$ | U(0.3,0.7) |'
- en: '| Dispersion | $\sigma_{\text{def}}$ | U(250,450) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 色散 | $\sigma_{\text{def}}$ | U(250,450) |'
- en: '| (km s^(-1)) |  |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| (km s^(-1)) |  |  |'
- en: '|  | $r_{\text{core, def}}$ | U(0,0.5) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{\text{core, def}}$ | U(0,0.5) |'
- en: '| Brightness |  | U(200,600) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 亮度 |  | U(200,600) |'
- en: '| (counts/$\text{pix}^{2}$) |  |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| (counts/$\text{pix}^{2}$) |  |  |'
- en: '| Redshift | $z_{\text{def}}$ | U(0.3,0.7) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 红移 | $z_{\text{def}}$ | U(0.3,0.7) |'
- en: '| Redshift | $z_{\text{src}}$ | U($z_{\text{def}}$ + 0.5, $z_{\text{def}}$
    + 2.5) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 红移 | $z_{\text{src}}$ | U($z_{\text{def}}$ + 0.5, $z_{\text{def}}$ + 2.5)
    |'
- en: '|  | HUMVI |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | HUMVI |  |'
- en: '|  | -s | 0.2,0.7,1.3 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | -s | 0.2,0.7,1.3 |'
- en: '|  | -p | 2.5, 0.01 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | -p | 2.5, 0.01 |'
- en: '|  | -m | 0.1 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | -m | 0.1 |'
- en: 'Table 2: Values for the glafic and HumVI parameters used to generate the simulated
    arcs and png color-composite images respectively. $U(x_{min},x_{max})$ indicates
    that the value was sampled from a uniform distribution with $x_{min}$ and $x_{max}$
    being the minimum and maximum values.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：用于生成模拟弧线和png彩色合成图像的glafic和HumVI参数的值。$U(x_{min},x_{max})$表示该值是从一个均匀分布中抽样的，其中$x_{min}$和$x_{max}$分别为最小值和最大值。
- en: 4.2 Generating the simulated Lenses dataset
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 生成模拟Lenses数据集
- en: As described above, the scarcity of known lensed galaxies requires us to generate
    simulated lens samples for training ML models. Our approach is to add simulated
    lensed galaxies onto survey images, as has been used successfully in prior work
    (e.g., Jacobs et al., [2017](#bib.bib30); Jacobs et al., [2019](#bib.bib31)).
    For this work, we adopt an agnostic procedure for simulating lensed arcs which
    does not rely on photometric measurements of the deflector galaxy. We consider
    all galaxies which satisfy the magnitude cut criteria described in Section [2.1](#S2.SS1
    "2.1 Source selection and regions of interest ‣ 2 Deep Lens Survey Data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") (regardless of their color) for simulating the lensed arcs. We note that
    $\sim 50\%$ of the galaxies in our SurveyCatalog have a BPZ best fit photometric
    template from Schmidt & Thorman ([2013](#bib.bib62)) indicating that they are
    massive early-type galaxies at intermediate redshifts, and are indeed likely to
    act as strong lenses. We discuss the actual color distribution for lens candidates
    in Section [6.2.3](#S6.SS2.SSS3 "6.2.3 Distribution of lensed candidates in color-color
    space ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey").
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，已知的透镜星系稀缺性要求我们生成模拟的透镜样本来训练机器学习模型。我们的方法是将模拟的透镜星系添加到调查图像中，这在以前的工作中已成功使用（例如，Jacobs
    等，[2017](#bib.bib30)；Jacobs 等，[2019](#bib.bib31)）。在这项工作中，我们采用了一种无关的模拟透镜弧的程序，这种程序不依赖于透镜星系的光度测量。我们考虑所有满足第[2.1节](#S2.SS1
    "2.1 Source selection and regions of interest ‣ 2 Deep Lens Survey Data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")（无论其颜色如何）的星系来模拟透镜弧。我们注意到，约$\sim 50\%$的星系在我们的SurveyCatalog中具有来自Schmidt
    & Thorman（[2013](#bib.bib62)）的BPZ最佳拟合光度模板，这表明它们是中等红移的大型早期类型星系，确实很可能作为强透镜存在。我们在第[6.2.3节](#S6.SS2.SSS3
    "6.2.3 Distribution of lensed candidates in color-color space ‣ 6.2 Catalog of
    Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")中讨论了透镜候选星系的实际颜色分布。
- en: Given any object from the training dataset, we assume that the central galaxy
    (“deflector”) is at a redshift $z_{\text{def}}\in[0.3,0.7]$ and is characterized
    by a Singular Isothermal Ellipsoid (SIE) mass density (Kormann et al., [1994](#bib.bib35)).
    The mass profile is dependent on the galaxy’s position ($x_{\text{def}},y_{\text{def}}$),
    ellipticity ($e_{\text{def}}$), position angle ($\theta_{\text{def}}$), velocity
    dispersion ($\sigma_{\text{def}}$), and choice of $r_{\text{core,def}}$. The values
    for these parameters are sampled from a uniform distribution spanning the ranges
    listed in Table [2](#S4.T2 "Table 2 ‣ 4.1 Generating the NonLenses dataset ‣ 4
    Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"). These values ensure that
    the resulting mass profile of the deflector is sufficient to produce a detectable
    lensing effect (i.e., $\Theta_{E}\gtrsim 1$ arcsecond). A background galaxy (“source”)
    is assumed to lie at a redshift $z_{\text{src}}$ with morphology given by a Sérsic
    profile parameterized by its position $(x_{src},y_{src})$, central brightness
    (in units of counts/$\text{pix}^{2}$), ellipticity ($e_{\text{src}}$), position
    angle ($\theta_{\text{src}}$), and a Sérsic index of 1\. The value for $z_{\text{src}}$
    is randomly chosen from a uniform distribution between $z_{\text{def}}+0.5$ and
    $z_{\text{def}}+2.5$. These values for the deflector and source redshifts are
    typical of spectroscopically measured values from previous strong lens surveys
    (e.g., Sonnenfeld et al., [2013](#bib.bib68); Bolton et al., [2008](#bib.bib9);
    Tran et al., [2022](#bib.bib75)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 给定训练数据集中任意一个对象，我们假设中央星系（“引光体”）的红移为 $z_{\text{def}}\in[0.3,0.7]$，并且其质量密度由单向异形椭球（SIE）特征描述（Kormann
    等， [1994](#bib.bib35)）。质量分布依赖于星系的位置（$x_{\text{def}},y_{\text{def}}$）、椭圆率（$e_{\text{def}}$）、位置角（$\theta_{\text{def}}$）、速度色散（$\sigma_{\text{def}}$）和
    $r_{\text{core,def}}$ 的选择。这些参数的值从表 [2](#S4.T2 "Table 2 ‣ 4.1 Generating the NonLenses
    dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey") 中列出的范围内均匀分布中采样。这些值确保了引光体的质量分布足以产生可检测的透镜效应（即，$\Theta_{E}\gtrsim
    1$ 弧秒）。假设背景星系（“源”）位于红移 $z_{\text{src}}$ 处，其形态由 Sérsic 分布描述，其参数包括位置（$x_{src},y_{src}$）、中心亮度（以
    counts/$\text{pix}^{2}$ 为单位）、椭圆率（$e_{\text{src}}$）、位置角（$\theta_{\text{src}}$）和
    Sérsic 指数为 1。$z_{\text{src}}$ 的值从 $z_{\text{def}}+0.5$ 和 $z_{\text{def}}+2.5$
    之间的均匀分布中随机选择。这些引光体和源星系红移的值是先前强透镜调查中光谱测量的典型值（例如，Sonnenfeld 等，[2013](#bib.bib68)；Bolton
    等，[2008](#bib.bib9)；Tran 等，[2022](#bib.bib75)）。
- en: The light from the background galaxy is traced using glafic (Oguri, [2010](#bib.bib56))
    to produce a simulated lensed arc in the image plane. The simulated lensed arcs
    are convolved with the point spread function (PSF) of the survey, scaled by a
    factor of (1,1.5,3) for the BVR filters, and then added to the $BVR$ fits images
    of the galaxy. We model the PSF of the survey in all the three filters as a 2D
    Gaussian kernel with a FWHM of $\sim$1 arcsecond corresponding to the approximate
    average seeing conditions. In addition to smoothing, we add Poisson noise in order
    to produce more realistic simulated arc images. The fits images are converted
    to a color png image using HumVI (as described in Section [4.1](#S4.SS1 "4.1 Generating
    the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). For
    this paper, we focus on generating moderately bright blue lensed arcs, and the
    parameter ranges that produce these configurations are listed in Table [2](#S4.T2
    "Table 2 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Generating the NonLenses
    dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey") illustrates
    common configurations of the arcs produced using this method. However, we note
    that the RGB-shuffle augmentation which is applied during training produces arcs
    of different colors (e.g., Figure [2](#S3.F2 "Figure 2 ‣ 3.3 Data augmentation
    and GANs ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")). We find that such an approach, where the simulated arcs are not dependent
    on the photometric properties of the central deflector galaxy, likely serves as
    an additional form of augmentation. This approach prevents over-fitting of our
    deep learning models while allowing for rapid prototyping and testing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 背景银河的光通过glafic (Oguri, [2010](#bib.bib56)) 被追踪，以在图像平面中生成模拟的透镜弧。模拟的透镜弧与调查的点扩散函数（PSF）进行卷积，BVR滤镜的缩放因子为（1,1.5,3），然后添加到银河的$BVR$
    fits图像中。我们将调查的PSF在所有三个滤镜中建模为一个2D高斯核，FWHM约为1角秒，对应于近似的平均观测条件。除了平滑处理，我们还添加了泊松噪声，以生成更逼真的模拟弧图像。fits图像通过HumVI转换为彩色png图像（如第[4.1](#S4.SS1
    "4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")节所述）。对于本文，我们专注于生成中等亮度的蓝色透镜弧，产生这些配置的参数范围列在表[2](#S4.T2 "Table 2 ‣ 4.1 Generating
    the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")中。图[4](#S4.F4
    "Figure 4 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")展示了使用这种方法生成的弧的常见配置。然而，我们注意到训练过程中应用的RGB-shuffle增强会产生不同颜色的弧（例如，图[2](#S3.F2
    "Figure 2 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning Architecture and
    learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")）。我们发现，这种方法（即模拟弧不依赖于中央折射星系的光度特性）可能作为一种额外的增强形式。该方法防止了我们深度学习模型的过拟合，同时允许快速原型设计和测试。
- en: '4.3 Generating the training datasets: TrainingV1 and TrainingV2'
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 生成训练数据集：TrainingV1 和 TrainingV2
- en: 'Using the Lenses and NonLenses datasets, we construct two training sets: TrainingV1
    and TrainingV2\. The main difference between the two training sets is the number
    of labeled images used as Lenses and NonLenses. Prior work using CNNs (e.g., Jacobs
    et al., [2019](#bib.bib31)) have favored large training datasets (i.e., $\gtrsim$150,000
    galaxies). Therefore, for TrainingV1 we use 266,301 images for non-lenses and
    257,874 corresponding simulations as lenses (described in Section [4.2](#S4.SS2
    "4.2 Generating the simulated Lenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")). Since semi-supervised training requires both labeled
    and unlabeled data, TrainingV1 cannot be used to test semi-supervised learning
    methods.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用透镜和非透镜数据集，我们构建了两个训练集：TrainingV1和TrainingV2。两个训练集之间的主要区别在于用作透镜和非透镜的标记图像数量。以前使用CNN的工作（例如，Jacobs等，[2019](#bib.bib31)）倾向于使用大型训练数据集（即，$\gtrsim$150,000个星系）。因此，对于TrainingV1，我们使用266,301张非透镜图像和257,874个对应的模拟作为透镜（如第[4.2](#S4.SS2
    "4.2 生成模拟透镜数据集 ‣ 4 训练和验证数据 ‣ 优化机器学习方法以发现深层透镜调查中的强引力透镜")节所述）。由于半监督训练需要标记和未标记的数据，因此TrainingV1不能用于测试半监督学习方法。
- en: For TrainingV2, we choose the number of images for each class to be similar
    to those used in standard computer vision datasets such as Canadian Institute
    for Advanced Research-10 (CIFAR-10; Krizhevsky, [2009b](#bib.bib37)) and Street
    View House Numbers (SVHN; Netzer et al., [2011](#bib.bib55)) dataset. We use a
    set of 7,074 human-labeled objects as NonLenses and 6,929 corresponding simulations
    as Lenses. The human labeling was carried out on randomly chosen images from Field-1
    (F1) of the DLS. We note that the choice of labeling the data only from F1 does
    not affect the results presented in the rest of the paper (see Appendix [A](#A1
    "Appendix A Model performance and final lens sample ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). The
    259,248 NonLens images which are not part of TrainingV2 serve as unlabeled data
    for our semi-supervised learning methods (e.g., MixMatch; Section [3.2](#S3.SS2
    "3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于TrainingV2，我们选择每个类别的图像数量与标准计算机视觉数据集（如加拿大高级研究院-10（CIFAR-10; Krizhevsky, [2009b](#bib.bib37)）和街景房屋号码（SVHN;
    Netzer等，[2011](#bib.bib55)）数据集）类似。我们使用了7,074个人工标记的对象作为非透镜和6,929个对应的模拟作为透镜。人工标记是在DLS的Field-1（F1）中随机选择的图像上进行的。我们注意到，仅从F1标记数据的选择不会影响本文其余部分所呈现的结果（见附录[A](#A1
    "附录 A 模型性能和最终透镜样本 ‣ 优化机器学习方法以发现深层透镜调查中的强引力透镜")）。259,248个不属于TrainingV2的非透镜图像作为我们的半监督学习方法（例如，MixMatch;
    第[3.2](#S3.SS2 "3.2 领域适应与半监督学习 ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深层透镜调查中的强引力透镜")节）中的未标记数据使用。
- en: Counter-intuitively, we find that too much training data from simulated lenses
    and randomly selected NonLenses can hurt the performance of our algorithms. We
    refer readers to Section [6.1.2](#S6.SS1.SSS2 "6.1.2 Larger non-lens training
    samples can degrade the classifier’s performance ‣ 6.1 Semi-supervised algorithms
    with GANs and Augmentations have superior performance ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") and [S22](#bib.bib66) for further discussion of sample
    size effects, which can also contribute to differences in performance between
    the training sets. We note that the TrainingV2 labeled datasets are comparable
    to the size where we find peak performance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与直觉相反，我们发现来自模拟透镜和随机选择的非透镜的过多训练数据会损害我们算法的性能。我们建议读者查阅第[6.1.2](#S6.SS1.SSS2 "6.1.2
    较大的非透镜训练样本可能会降低分类器性能 ‣ 6.1 基于GAN和数据增强的半监督算法具有更优的性能 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深层透镜调查中的强引力透镜")节和[S22](#bib.bib66)，以进一步讨论样本大小效应，这也可能导致训练集之间性能的差异。我们注意到，TrainingV2标记的数据集大小与我们发现最佳性能的大小相当。
- en: We performed a 90-10 split for both TrainingV1 and TrainingV2, where 90% of
    the data was allocated for training the ResNetV2 model and 10% was kept aside
    for validation. We chose the maximum number of epochs (passes through the training
    dataset) for each training combination as 100, since this was sufficient to observe
    a plateau in the validation metrics. For each of the training combination described
    in Section [3](#S3 "3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey"), we conducted four independent trials and selected the checkpoint with
    the best validation metrics for testing it on the survey data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对TrainingV1和TrainingV2进行了90-10的分割，其中90%的数据用于训练ResNetV2模型，10%用于验证。我们选择每个训练组合的最大训练周期数（遍历训练数据集的次数）为100，因为这足以观察到验证指标的平稳期。对于第[3](#S3
    "3 Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")节中描述的每个训练组合，我们进行了四次独立试验，并选择了具有最佳验证指标的检查点进行调查数据测试。
- en: 5 Metric to evaluate model performance
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估模型性能的指标
- en: 'We have described several models which are each tuned to optimize validation
    accuracy, which is measured on the validation dataset (Section [4.3](#S4.SS3 "4.3
    Generating the training datasets: TrainingV1 and TrainingV2 ‣ 4 Training and Validation
    data ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")) consisting of simulated Lenses and survey NonLenses.
    In order to gauge the performance of the models on their ability to find real
    lenses from the survey, we require a testing dataset consisting of lenses from
    the survey, as well as a metric to evaluate them on.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '我们描述了几个模型，每个模型都经过调整以优化验证准确性，验证准确性是通过在包含模拟透镜和调查非透镜的验证数据集上测量的（第[4.3](#S4.SS3
    "4.3 Generating the training datasets: TrainingV1 and TrainingV2 ‣ 4 Training
    and Validation data ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")节）。为了评估模型在从调查中找到真实透镜的能力，我们需要一个包含调查透镜的测试数据集，以及用于评估它们的指标。'
- en: 5.1 Generating the Testing dataset
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 生成测试数据集
- en: Curating testing data in our case is a challenging task. As discussed earlier,
    only two strong lenses in the entire survey were known prior to this work, which
    is insufficient for meaningful evaluation. Therefore, we use an ensemble of 5
    ResNet models trained on simulated lenses but using polar transformed images as
    input to the network. The exclusive task of this model is to find real lens candidates
    to add to our test dataset. We emphasize that this model is independent of the
    rest of the models discussed so far in this paper, and does not influence their
    performance in any way. Details of its implementation are discussed in [S22](#bib.bib66).
    It is beyond the scope of this paper to quantify the performance of ensemble models
    or the effect of polar transformation during training, but it is an interesting
    avenue for future work.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，策划测试数据是一项具有挑战性的任务。如前所述，在此工作之前，整个调查中仅知晓两个强透镜，这不足以进行有意义的评估。因此，我们使用了一个由5个ResNet模型组成的集合，这些模型在模拟透镜上训练，但使用极坐标变换图像作为输入。该模型的唯一任务是寻找真实的透镜候选对象，并将其添加到我们的测试数据集中。我们强调，这个模型与本文中迄今讨论的其他模型是独立的，并且不会以任何方式影响它们的性能。其实现细节在[S22](#bib.bib66)中讨论。量化集合模型的性能或极坐标变换在训练期间的效果超出了本文的范围，但这是一个有趣的未来研究方向。
- en: 'We find 52 likely lens candidates from this model, of which 27 are deemed to
    be good candidates upon visual inspection. Therefore, we create two testing datasets:
    TestV1 and TestV2\. TestV1 contains all the 52 lens candidates found using our
    ensemble model approach, while TestV2 contains the 27 best visual candidates.
    NonLenses for both TestV1 and TestV2 were formed by randomly selecting 874 of
    our 8734 human-labeled non-lenses (Section [4](#S4 "4 Training and Validation
    data ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从这个模型中发现了52个可能的透镜候选者，其中27个在视觉检查后被认为是良好的候选者。因此，我们创建了两个测试数据集：TestV1和TestV2。TestV1包含使用我们集合模型方法找到的所有52个透镜候选者，而TestV2包含27个最佳视觉候选者。TestV1和TestV2的NonLenses是通过随机选择我们8734个人工标注的非透镜中的874个形成的（第[4](#S4
    "4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey")节）。
- en: 5.2 Precision and Recall
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 精确度和召回率
- en: 'A standard metric widely used in machine learning to evaluate the performance
    of test data on a trained model is the Precision-Recall curve (PR curve), where
    precision and recall are defined as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，一个广泛使用的标准指标是精度-召回曲线（PR曲线），其中精度和召回定义如下：
- en: '|  | $\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}},\quad\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}.$
    |  | (2) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}},\quad\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}.$
    |  | (2) |'
- en: Here TP, FP, and FN are the number of True Positive, False Positive, and False
    Negative images respectively. These values are computed by passing a labeled test
    dataset (TestV1 and TestV2 in this case) through a trained model (e.g., GAN+Mixmatch)
    and setting different prediction thresholds.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，TP、FP和FN分别表示真正例、假正例和假负例的图像数量。这些值是通过将标记的测试数据集（在本例中为TestV1和TestV2）传递给训练好的模型（例如，GAN+Mixmatch）并设置不同的预测阈值来计算的。
- en: Since the primary goal of this work is to find models which minimize the number
    of nonlensed images that an investigator encounters while maximizing the number
    of lensed images found (i.e., less FP and FN values), we seek models which have
    high precision at high recall. We present the results from our PR curve analysis
    in the next section.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这项工作的主要目标是找到能够在最大化找到透镜图像的数量（即，减少FP和FN值）的同时，最小化调查员遇到的非透镜图像的数量的模型，我们寻求那些在高召回率下具有高精度的模型。我们将在下一节中展示我们PR曲线分析的结果。
- en: 6 Results and Discussion
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结果与讨论
- en: 6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 结合GAN和增强的半监督算法表现优越
- en: 'We consider 17 variations on the learning approaches described in Section [3](#S3
    "3 Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey"): 4 supervised,
    6 semi-supervised, and 7 semi-supervised with GANs. SupervisedV1 and SupervisedV2
    are our baseline models. They were trained using a supervised learning approach
    with no data augmentation on TrainingV1 ($\sim$250,000 Lenses and NonLenses) and
    TrainingV2 ($\sim$7000 Lenses and NonLenses) respectively. On the other hand,
    SupervisedV1+DA and SupervisedV2+DA were trained using supervised learning with
    data augmentation (DA). The rest of the models were trained on TrainingV2 using
    semi-supervised learning methods with DA or with DA + GANs. In this subsection,
    we summarize the performance of these different models. We primarily use the PR
    curve (Section [5.2](#S5.SS2 "5.2 Precision and Recall ‣ 5 Metric to evaluate
    model performance ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")) evaluated on our TestV1 and TestV2 sets to gauge
    which models perform best. We note that our methodology paper [S22](#bib.bib66)
    includes an additional discussion of these results.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了第[3](#S3 "3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")节中描述的17种学习方法变体：4种监督学习，6种半监督学习，和7种结合GAN的半监督学习。SupervisedV1和SupervisedV2是我们的基准模型。它们分别使用了没有数据增强的监督学习方法在TrainingV1（$\sim$250,000
    Lenses和NonLenses）和TrainingV2（$\sim$7000 Lenses和NonLenses）上进行训练。另一方面，SupervisedV1+DA和SupervisedV2+DA使用了带数据增强（DA）的监督学习方法进行训练。其余模型在TrainingV2上使用带DA或DA
    + GANs的半监督学习方法进行训练。在本小节中，我们总结了这些不同模型的表现。我们主要使用PR曲线（第[5.2](#S5.SS2 "5.2 精度和召回 ‣
    5 评估模型性能的指标 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")节）在我们的TestV1和TestV2数据集上评估，以衡量哪些模型表现最佳。我们注意到我们的研究方法论文[S22](#bib.bib66)包含了这些结果的额外讨论。
- en: We plot the PR curve obtained for our best-performing baseline models (SupervisedV1,
    SupervisedV2) along with a subset of semi-supervised and GAN+semi-supervised models
    in Figure [5](#S6.F5 "Figure 5 ‣ 6.1.1 Ablation study on data augmentations ‣
    6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") (see Tables 3 and 4 of [S22](#bib.bib66)
    for additional model results). Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation study
    on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") lists
    the precision value obtained for a subset of models at 100% recall. We find that
    our models tend to generalize poorly when trained without any augmentations. Our
    baseline models, trained without any data augmentation, performed worst out of
    all models at every recall level. For example, at 100% recall, the baseline SupervisedV1
    and SupervisedV2 have a precision of $\sim 3\%$ on our TestV2 set, whereas the
    GAN+$\Pi$-model has a precision of $\sim 22\%$. The poor precision values of our
    supervised models may reflect challenges in simulating the characteristics of
    lenses from a survey given limited priors. Fortunately, we find that data augmentation
    methods are able to address this problem. We find a factor $\sim$5-10$\times$
    improved precision across almost all recall levels when applying the full set
    of augmentations (Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised
    learning ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")) to our supervised models.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 [5](#S6.F5 "Figure 5 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") 中绘制了我们最佳基线模型（SupervisedV1、SupervisedV2）以及一部分半监督和GAN+半监督模型的PR曲线（有关更多模型结果，请参见[S22](#bib.bib66)的表格 3
    和 4）。表格 [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") 列出了在100%召回率下获得的模型精度值。我们发现，当模型在没有任何增强的情况下训练时，表现通常较差。我们的基线模型在没有数据增强的情况下训练，在每个召回水平上表现最差。例如，在100%召回率下，基线模型SupervisedV1和SupervisedV2在我们的TestV2数据集上的精度约为$\sim
    3\%$，而GAN+$\Pi$-model的精度约为$\sim 22\%$。我们监督模型的较差精度值可能反映了在有限的先验条件下，模拟透镜特征的挑战。幸运的是，我们发现数据增强方法能够解决这一问题。应用完整的数据增强集（表格 [1](#S3.T1
    "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey")）后，我们几乎在所有召回水平上发现了约5-10$\times$的精度提升。
- en: The improvement of semi-supervised over supervised algorithms suggests that
    valuable features can in fact be extracted from the mostly unlabeled NonLenses,
    providing benefits in the classification of real lenses. Adding GAN images to
    our training pipelines had a seemingly profound impact at all recall levels, especially
    at higher recalls where more difficult-to-classify images come into play. This
    suggests that GAN-generated images contain subtle variations which, while not
    necessarily significant to the naked eye, do in fact produce a strong regularizing
    effect when used in training.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督算法优于监督算法的改进表明，实际上可以从大部分未标记的NonLenses中提取有价值的特征，从而在真实透镜的分类中提供好处。将GAN图像添加到我们的训练管道中在所有召回水平上产生了显著的影响，尤其是在更高的召回水平上，难以分类的图像更多。这表明，GAN生成的图像包含细微的变化，这些变化虽然肉眼不易察觉，但确实在训练中产生了强大的正则化效果。
- en: 6.1.1 Ablation study on data augmentations
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 数据增强的消融研究
- en: We investigated the impact of each of the data augmentations we used by doing
    an ablation study using TrainingV2\. The results from this study are tabulated
    in Table [7](#A1.T7 "Table 7 ‣ Appendix A Model performance and final lens sample
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). We find that removing GAN images from the training
    sets causes a noticeable decrease in model performance at all recall levels, which
    agrees with our earlier conclusion. It also appears that color augmentations and
    JPEG quality play a very significant role in model performance. Including these
    three augmentations in our training pipelines is apparently what allows our model
    to generalize so well, despite relying on simulated lenses for training. A curious
    result from this ablation study is that multiples of 90-degree rotations actually
    had a negative effect on model performance. The difference in performance is relatively
    small compared to that seen for other augmentations (e.g., GANs), but persists
    at all recall rates. A possible reason for this could be our small validation
    and test sets. Because the validation set is small, model selection may be biased
    towards certain orientations of the image. Likewise, an equally small test set
    may have preferred orientations that the model does not generalize to, resulting
    in degraded performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用 TrainingV2 进行消融研究，调查了我们使用的每种数据增强的影响。该研究的结果汇总在表 [7](#A1.T7 "表 7 ‣ 附录 A
    模型性能与最终镜片样本 ‣ 优化机器学习方法以发现深层镜头调查中的强引力透镜")。我们发现，从训练集中移除 GAN 图像会导致模型在所有召回水平上的性能明显下降，这与我们之前的结论一致。还发现，颜色增强和
    JPEG 质量在模型性能中起着非常重要的作用。显然，将这三种增强包含在我们的训练流程中，使我们的模型尽管依赖于模拟透镜进行训练，依然能够很好地泛化。一个有趣的结果是，90度旋转的倍数实际上对模型性能产生了负面影响。与其他增强（例如，GANs）相比，性能差异相对较小，但在所有召回率下都存在。造成这种情况的一个可能原因是我们的小型验证集和测试集。由于验证集很小，模型选择可能会偏向于图像的某些方向。同样，小型测试集可能会有模型不能泛化到的优选方向，从而导致性能下降。
- en: '| ![Refer to caption](img/4f5845efbceae76c9bfe98ec0c78b76e.png) | ![Refer to
    caption](img/b563c47b7a3161465dce364eeaadd948.png) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/4f5845efbceae76c9bfe98ec0c78b76e.png) | ![参见说明](img/b563c47b7a3161465dce364eeaadd948.png)
    |'
- en: 'Figure 5: Precision-Recall curves (PR curves) for a subset of the models described
    in Section [3](#S3 "3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") obtained using TestV1 (*Left*) and TestV2 (*Right*). TestV1 contains
    52 lens candidates found using our ensemble model approach, while TestV2 contains
    the 27 best visual candidates (Section [5.1](#S5.SS1 "5.1 Generating the Testing
    dataset ‣ 5 Metric to evaluate model performance ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). SupervisedV1
    and SupervisedV2 are our baseline models. They were trained using a supervised
    learning approach with no data augmentation on TrainingV1 ($\sim$250,000 Lenses
    and NonLenses) and TrainingV2 ($\sim$7000 Lenses and NonLenses) respectively.
    The rest of the models were trained on TrainingV2 with augmentations. MixMatch
    and $\Pi$-Model are semi-supervised learning approaches, whereas GAN+MixMatch
    and GAN+$\Pi$-Model use GAN generated images along with semi-supervised learning
    (see Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey") and Section [3](#S3
    "3 Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") for
    details). GAN+SupervisedV2 uses supervised learning with GAN generated images.
    Models which use semi-supervised learning along with GANs clearly outperform our
    baseline supervised learning models at all recall values, with GAN+$\Pi$-model
    having the highest precision at 100% recall (see results in Table [3](#S6.T3 "Table
    3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms
    with GANs and Augmentations have superior performance ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"); we note that Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation
    study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") reports
    the average of our four runs while this figure shows the runs with the best precision).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在使用 TestV1（*左侧*）和 TestV2（*右侧*）获得的一些模型的精度-召回曲线（PR 曲线）。TestV1 包含了使用我们的集成模型方法找到的
    52 个透镜候选者，而 TestV2 包含了 27 个最佳视觉候选者（第 [5.1](#S5.SS1 "5.1 生成测试数据集 ‣ 5 衡量模型性能的指标
    ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜") 节）。SupervisedV1 和 SupervisedV2 是我们的基线模型。它们分别使用监督学习方法在
    TrainingV1（$\sim$250,000 透镜和非透镜）和 TrainingV2（$\sim$7000 透镜和非透镜）上进行训练，没有数据增强。其余模型在带增强的
    TrainingV2 上进行训练。MixMatch 和 $\Pi$-Model 是半监督学习方法，而 GAN+MixMatch 和 GAN+$\Pi$-Model
    使用 GAN 生成的图像与半监督学习结合使用（详细信息请参见图 [3](#S3.F3 "图 3 ‣ 3.3 数据增强和 GANs ‣ 3 深度学习架构和学习方法
    ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜") 和第 [3](#S3 "3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")
    节）。GAN+SupervisedV2 使用带有 GAN 生成图像的监督学习。使用半监督学习和 GAN 的模型在所有召回值下明显优于我们的基线监督学习模型，其中
    GAN+$\Pi$-Model 在 100% 召回率下具有最高的精度（参见表 [3](#S6.T3 "表 3 ‣ 6.1.1 数据增强的消融研究 ‣ 6.1
    使用 GAN 和增强的半监督算法具有优越的性能 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")；我们注意到表 [3](#S6.T3
    "表 3 ‣ 6.1.1 数据增强的消融研究 ‣ 6.1 使用 GAN 和增强的半监督算法具有优越的性能 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")
    报告了我们四次运行的平均值，而此图显示了精度最佳的运行结果）。
- en: '| Model | Training data used | TestV1 Precision(%) | TestV2 Precision(%) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 使用的训练数据 | TestV1 精度 (%) | TestV2 精度 (%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SupervisedV1 | TrainingV1 w/ no augmentation | 5.62$\pm$0.01 | 3.01$\pm$0.02
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| SupervisedV1 | 不带增强的 TrainingV1 | 5.62$\pm$0.01 | 3.01$\pm$0.02 |'
- en: '| SupervisedV2 | TrainingV2 w/ no augmentation | 5.65$\pm$0.02 | 3.06$\pm$0.04
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| SupervisedV2 | 不带增强的 TrainingV2 | 5.65$\pm$0.02 | 3.06$\pm$0.04 |'
- en: '| MixMatch | TrainingV2 w/ augmentation | 12.28$\pm$5.09 | 6.84$\pm$3.00 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| MixMatch | 带增强的 TrainingV2 | 12.28$\pm$5.09 | 6.84$\pm$3.00 |'
- en: '| $\Pi$-Model | TrainingV2 w/ augmentation | 13.41$\pm$2.33 | 8.68$\pm$1.49
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| $\Pi$-Model | 带增强的 TrainingV2 | 13.41$\pm$2.33 | 8.68$\pm$1.49 |'
- en: '| GAN + Supervised | TrainingV2 w/ augmentation | 8.25$\pm$2.85 | 6.05$\pm$2.69
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| GAN + 有监督 | 经过增强的 TrainingV2 | 8.25$\pm$2.85 | 6.05$\pm$2.69 |'
- en: '| GAN + MixMatch | TrainingV2 w/ augmentation | 14.13$\pm$6.53 | 7.97$\pm$3.93
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| GAN + MixMatch | 经过增强的 TrainingV2 | 14.13$\pm$6.53 | 7.97$\pm$3.93 |'
- en: '| GAN + $\Pi$-Model | TrainingV2 w/ augmentation | 15.2$\pm$6.21 | 22.27$\pm$7.71
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| GAN + $\Pi$-模型 | 经过增强的 TrainingV2 | 15.2$\pm$6.21 | 22.27$\pm$7.71 |'
- en: 'Table 3: Average precision values were obtained for a subset of the models
    tested at $100\%$ recall. We note that a table with the performance of all the
    models at various recall values is presented in [S22](#bib.bib66). Here the average
    is computed from the performance of four independent runs on the test sets. The
    uncertainties are $1\sigma$ standard deviations from the mean.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 $100\%$ 召回率下对测试的模型子集获得的平均精度值。我们注意到，表格中提供了所有模型在不同召回率下的性能数据，详见 [S22](#bib.bib66)。这里的平均值是从对测试集的四次独立运行的性能中计算得出的。不确定性为均值的
    $1\sigma$ 标准偏差。
- en: '![Refer to caption](img/f9feb74cd6d3ef4bb4a3b2d24875a6c5.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f9feb74cd6d3ef4bb4a3b2d24875a6c5.png)'
- en: 'Figure 6: Grade-A lenses found in the DLS along with the rank (Section [6.2](#S6.SS2
    "6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")) assigned to them by GAN+MixMatch(MM) and GAN+$\Pi$-model(PI) models.
    All Grade-A lenses have a clear arc morphology and are located near a moderately
    massive galaxy or group, making them convincing lens candidates. Among these candidates,
    212072337 and 432021600 have been spectroscopically confirmed to be true strong
    lens systems (Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Spectroscopic confirmation of
    two Grade-A lenses ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在 DLS 中发现的 Grade-A 透镜以及 GAN+MixMatch(MM) 和 GAN+$\Pi$-模型(PI) 模型分配的等级（第 [6.2](#S6.SS2
    "6.2 Lens 候选目录 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜") 节）。所有 Grade-A 透镜都有明显的弧形形态，并位于一个中等大质量的星系或星系组附近，使它们成为令人信服的透镜候选者。在这些候选者中，212072337
    和 432021600 已通过光谱学确认是实际的强透镜系统（第 [6.2.2](#S6.SS2.SSS2 "6.2.2 对两个 Grade-A 透镜的光谱学确认
    ‣ 6.2 Lens 候选目录 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜") 节）。
- en: '![Refer to caption](img/af68b7ece7173fd5a27bd31f5fa26f59.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/af68b7ece7173fd5a27bd31f5fa26f59.png)'
- en: 'Figure 7: Grade-B lenses found along with the rank (Section [6.2](#S6.SS2 "6.2
    Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey"))
    assigned to them by GAN+MixMatch(MM) and GAN+PiModel(PI) models. Targets in this
    category have either a tentative nebulous arc-like feature surrounding a massive
    galaxy, or have approximately linear extended morphology near an apparent galaxy
    group or cluster. It is hard to discern if these features correspond to lensed
    arcs or are caused by blending of multiple sources, hence the uncertain Grade-B
    classification.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：与 GAN+MixMatch(MM) 和 GAN+PiModel(PI) 模型分配的等级（第 [6.2](#S6.SS2 "6.2 Lens 候选目录
    ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜") 节）一同发现的 Grade-B 透镜。此类别中的目标要么有一个围绕大质量星系的暂定模糊弧形特征，要么在明显的星系组或星系团附近具有大致线性的扩展形态。很难判断这些特征是否对应于透镜弧，或是否由于多个源的混合造成，因此被不确定地归为
    Grade-B。
- en: 6.1.2 Larger non-lens training samples can degrade the classifier’s performance
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 更大的非透镜训练样本可能会降低分类器的性能
- en: To understand why our larger training set (TrainingV1) led to poorer generalization,
    we also performed a test where we fixed the number of simulated Lenses and varied
    the number of NonLenses in the dataset (see [S22](#bib.bib66), Table 5). As we
    gradually increased the number of NonLenses in the training data from 0 to 256,000,
    we saw that precision gradually increased and peaked at around 8000-16000 NonLenses,
    then started to significantly decrease to around $\sim$6% precision for nearly
    all recall levels. One possible explanation for this effect is that as we increase
    the number of NonLenses in training, we also increase the number of NonLens false
    positives which appear similar to real lenses in the survey data (and perhaps
    even more similar to real lenses than the simulations we use). As a result, the
    decision boundary for non-lenses overlaps more with the regions occupied by real
    lenses, leading to higher levels of misclassification. Therefore, care must be
    taken in constructing training data based on simulations. Arbitrarily increasing
    the size of the training data can evidently lead to significantly worse performance
    than using a smaller well-curated training set.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么我们较大的训练集（TrainingV1）导致了更差的泛化能力，我们还进行了一项测试，在该测试中，我们固定了模拟镜头的数量，并在数据集中变化了非镜头的数量（见
    [S22](#bib.bib66)，表5）。当我们逐渐将训练数据中的非镜头数量从0增加到256,000时，我们发现精度逐渐增加，在约8000-16000个非镜头时达到峰值，然后开始显著下降，到几乎所有召回水平下的精度约为$\sim$6%。对此现象的一个可能解释是，随着我们在训练中增加非镜头的数量，我们也增加了与真实镜头在调查数据中相似的非镜头假阳性（可能比我们使用的模拟数据更相似）。因此，非镜头的决策边界与真实镜头占据的区域重叠更多，导致更高的误分类水平。因此，在基于模拟构建训练数据时必须谨慎。任意增加训练数据的大小显然会导致比使用较小的精心策划的训练集更差的性能。
- en: To summarize, we find that models trained with a semi-supervised learning approach
    using TrainingV2 and GAN-generated images along with all of our proposed list
    of data augmentations have high precision values at all recall values. In particular,
    among the models tested, the top two performing models are GAN+MixMatch and GAN+$\Pi$-model.
    In the following subsection, we turn to apply these models to the full set of
    DLS survey images (i.e., SurveyCatalog in Section [2.1](#S2.SS1 "2.1 Source selection
    and regions of interest ‣ 2 Deep Lens Survey Data ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey"))
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们发现使用半监督学习方法、TrainingV2和GAN生成的图像，以及我们提出的所有数据增强方法训练的模型，在所有召回值下都有高精度值。特别是，在测试的模型中，表现最好的前两名模型是GAN+MixMatch和GAN+$\Pi$-model。在接下来的子章节中，我们将这些模型应用于DLS调查图像的完整数据集（即，第二节
    [2.1](#S2.SS1 "2.1 Source selection and regions of interest ‣ 2 Deep Lens Survey
    Data ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")）。
- en: 6.2 Catalog of Lens candidates found
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 镜头候选目录
- en: Having established which of our trained models perform best on our test set
    in terms of PR curves, we now turn to the key question of how many lenses are
    identified in the DLS and importantly, how much human inspection effort is required
    to find them.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了我们训练的模型在PR曲线上的最佳表现之后，我们现在转向关键问题：在DLS中识别出多少镜头，以及需要多少人工检查工作来找到它们。
- en: '| Rank | Number of unique | Number of | Number of | Total lenses | Number of
    | Number of | Number of |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 排名 | 唯一数量 | 数量 | 数量 | 总镜头数 | 数量 | 数量 | 数量 |'
- en: '| threshold | lenses investigated | Grade-A lenses | Grade-A lenses | Grade-A
    | Grade-A lenses | Grade-A lenses | Grade-A lenses |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 阈值 | 调查的镜头 | A级镜头 | A级镜头 | A级 | A级镜头 | A级镜头 | A级镜头 |'
- en: '|  | (G+MM,G+PI) | (G+MM) | (G+PI) | (both models) | (SupervisedV2) | (SupervisedV2+DA)
    | (SupervisedV2+DA+GAN) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | (G+MM,G+PI) | (G+MM) | (G+PI) | (两种模型) | (SupervisedV2) | (SupervisedV2+DA)
    | (SupervisedV2+DA+GAN) |'
- en: '| 12 | 9, 9 | 1 | 1 | 1 | 0 | 0 | 0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 9, 9 | 1 | 1 | 1 | 0 | 0 | 0 |'
- en: '| 25 | 19, 16 | 1 | 3 | 3 | 0 | 0 | 0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 19, 16 | 1 | 3 | 3 | 0 | 0 | 0 |'
- en: '| 100 | 67, 56 | 2 | 3 | 3 | 0 | 1 | 2 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 67, 56 | 2 | 3 | 3 | 0 | 1 | 2 |'
- en: '| 800 | 513, 430 | 4 | 3 | 4 | 1 | 2 | 3 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 800 | 513, 430 | 4 | 3 | 4 | 1 | 2 | 3 |'
- en: '| 2800 | 1735, 1459 | 6 | 5 | 8 | - | - | - |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 2800 | 1735, 1459 | 6 | 5 | 8 | - | - | - |'
- en: '| 4000 | 2459, 2076 | 7 | 5 | 9 | - | - | - |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 4000 | 2459, 2076 | 7 | 5 | 9 | - | - | - |'
- en: 'Table 4: Comparison of the number of Grade-A lenses found by different models
    tested. The predictions from the models are ranked such that the most likely predicted
    lens has rank=1\. The rank threshold value sets the number of lenses that an investigator
    has to visually inspect. The left two columns show the chosen rank threshold and
    the number of unique lenses that it corresponds to (removing duplicates as described
    in Section [6.2](#S6.SS2 "6.2 Catalog of Lens candidates found ‣ 6 Results and
    Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). Our best performing models GAN+MixMatch (G+MM)
    and GAN+PiModel (G+PI) find 4 and 3 lensed candidates each among the top $\sim$500
    unique images (top 800 ranks), and 7 lensed candidates each when the top $\sim
    2300$ images are investigated. Combining the results from both the models, we
    find 9 Grade-A candidates (shown in Figure [6](#S6.F6 "Figure 6 ‣ 6.1.1 Ablation
    study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). The
    right three columns show the number of lenses found from the SupervisedV2, SupervisedV2+Data
    Augmentation(DA) and SupervisedV2+DA+GAN. Although they find fewer ($\lesssim
    50\%$) lens candidates than our best performing models, we can see that DA and
    GANs are able to boost the number of lenses found from 1 to 3 at a rank threshold
    of 800.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：比较不同模型测试发现的A级镜头数量。这些模型的预测结果被排名，其中最可能的预测镜头排名为1。排名阈值设置了调查员需要目视检查的镜头数量。左侧的两列显示了选择的排名阈值及其对应的唯一镜头数量（去除重复项，如第[6.2节](#S6.SS2
    "6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")中所述）。我们表现最好的模型 GAN+MixMatch (G+MM) 和 GAN+PiModel (G+PI) 在前 $\sim$500 个唯一图像（前800名）中各找到4个和3个镜头候选者，而在前
    $\sim 2300$ 张图像中各找到7个镜头候选者。将两个模型的结果结合，我们发现了9个A级候选者（如图[6](#S6.F6 "Figure 6 ‣ 6.1.1
    Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs
    and Augmentations have superior performance ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")所示）。右侧的三列显示了从 SupervisedV2、SupervisedV2+数据增强(DA) 和 SupervisedV2+DA+GAN
    发现的镜头数量。尽管它们发现的镜头候选者数量少于我们表现最好的模型（$\lesssim 50\%$），我们可以看到，DA 和 GAN 能够将排名阈值为800时发现的镜头数量从1提升到3。
- en: '| Rank | Number of unique | Number of | Number of | Total lenses | Total lenses
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 排名 | 唯一数量 | 数量 | 数量 | 总镜头 | 总镜头 |'
- en: '| threshold | lenses investigated | Grade-B lenses | Grade-B lenses | Grade-B
    lenses | Grade-A+B lenses |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 阈值 | 检查的镜头 | B级镜头 | B级镜头 | B级镜头 | A+B级镜头 |'
- en: '|  | (G+MM,G+PI) | (G+MM) | (G+PI) | (both models) | (both models) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | (G+MM,G+PI) | (G+MM) | (G+PI) | (两个模型) | (两个模型) |'
- en: '| 12 | 9, 9 | 0 | 0 | 0 | 1 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 9, 9 | 0 | 0 | 0 | 1 |'
- en: '| 25 | 19, 16 | 0 | 0 | 0 | 3 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 19, 16 | 0 | 0 | 0 | 3 |'
- en: '| 100 | 67, 56 | 0 | 2 | 2 | 5 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 67, 56 | 0 | 2 | 2 | 5 |'
- en: '| 800 | 513, 430 | 2 | 5 | 5 | 9 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 800 | 513, 430 | 2 | 5 | 5 | 9 |'
- en: '| 2800 | 1735, 1459 | 6 | 11 | 12 | 20 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 2800 | 1735, 1459 | 6 | 11 | 12 | 20 |'
- en: '| 4000 | 2459, 2076 | 9 | 11 | 13 | 22 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 4000 | 2459, 2076 | 9 | 11 | 13 | 22 |'
- en: 'Table 5: Similar to Table [5](#S6.T5 "Table 5 ‣ 6.2 Catalog of Lens candidates
    found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey") but for Grade-B lenses.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：与表[5](#S6.T5 "Table 5 ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")类似，但针对B级镜头。
- en: We obtain a $\sim$97% and $\sim$86% precision at 50% recall (i.e., to find 50%
    lenses from our test set) for the GAN + MixMatch and GAN + $\Pi$-model respectively.
    On the other hand, if we needed to reach $100\%$ recall (i.e., find all the lenses
    from our test set), the precision drops to $\sim 8\%$ and $\sim 22\%$ respectively
    (Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). Based on the results from Pourrahmani
    et al. ([2018](#bib.bib60)) who searched for gravitational lenses in the COSMOS
    field with excellent image quality from the Hubble Space Telescope, we expect
    a maximum of $\sim$7 Grade-A lenses per square degree. This gives an upper bound
    of $\lesssim$140 lens candidates in the 20 degree² of the DLS, where the number
    of detectable lenses will be smaller since many of the COSMOS lenses have Einstein
    radii which are too small to resolve in ground-based DLS data, and because the
    COSMOS data are more sensitive. We estimate that half of the COSMOS lenses are
    unresolved in DLS based on the distribution of Einstein radii of the sample, with
    median $\simeq$1$\aas@@fstack{\prime\prime}$2 reported by Pourrahmani et al. ([2018](#bib.bib60)),
    such that we would expect $\sim$70 detectable lenses in the DLS survey area. At
    100% recall, 8% precision, and a TP$\approx$70, the number of false positive (FP)
    images that an investigator has to look at to find 70 lenses is $\sim 850$. If
    the number of detectable lenses in DLS is much lower, as suggested by samples
    reported from large ground-based campaigns such as the Dark Energy Survey (DES),
    then the total number of images and false positives which must be searched is
    correspondingly smaller. We also note that these estimates are based on the assumption
    that the precision values obtained from our test set also apply to the survey
    data. A decrease in this precision value would increase the number of FPs. Therefore,
    considering these uncertainties, for this work we visually examine the top 12,
    25, 100, 800, 2800, and 4000 predictions from the GAN+Mixmatch and GAN+$\Pi$-model,
    and investigate the number of lenses found. Throughout this paper, we focus only
    on using relative ranks (i.e., top $n$ prediction) to assess model performance
    since the distribution of absolute prediction threshold values (such as those
    employed in Jacobs et al. [2019](#bib.bib31)) can vary significantly between different
    models (Appendix [12](#A1.F12 "Figure 12 ‣ Appendix A Model performance and final
    lens sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). The absolute prediction values obtained from
    different models can be calibrated, for example by scaling the obtained model
    weights to the softmax layer, but this is beyond the scope of our study. We note
    that the relative ranks which we use in this study will be unaffected under such
    scaling transformations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GAN + MixMatch和GAN + $\Pi$-model，我们在50%的召回率下（即，从我们的测试集中找到50%的透镜）分别获得了$\sim$97%和$\sim$86%的精度。另一方面，如果我们需要达到$100\%$的召回率（即，从我们的测试集中找到所有透镜），精度则分别下降到$\sim
    8\%$和$\sim 22\%$（见表[3](#S6.T3 "表3 ‣ 6.1.1 数据增强的消融研究 ‣ 6.1 使用GANs和数据增强的半监督算法具有优越的性能
    ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")）。根据Pourrahmani等人（[2018](#bib.bib60)）在COSMOS领域中利用哈勃空间望远镜拍摄的高质量图像搜索引力透镜的结果，我们预计每平方度最多有$\sim$7个A级透镜。这为DLS的20度²区域提供了一个上限，即$\lesssim$140个透镜候选者，其中可检测到的透镜数量会更少，因为许多COSMOS透镜的爱因斯坦半径在地面DLS数据中太小而无法分辨，而且COSMOS数据更为敏感。根据Pourrahmani等人（[2018](#bib.bib60)）报告的样本的爱因斯坦半径分布（中位数$\simeq$1$\aas@@fstack{\prime\prime}$2），我们估计DLS中有一半的COSMOS透镜未被分辨出来，因此我们预计DLS调查区域中会有$\sim$70个可检测到的透镜。在100%召回率、8%精度和TP$\approx$70的情况下，研究人员需要查看的假阳性（FP）图像数量约为$\sim
    850$，以找到70个透镜。如果DLS中可检测到的透镜数量远低于此值，如大型地面观测项目（例如暗能量调查（DES））报告的样本所示，那么需要搜索的图像和假阳性总数相应地会更少。我们还注意到，这些估计是基于从测试集中获得的精度值也适用于调查数据的假设。如果这一精度值下降，假阳性数量将会增加。因此，考虑到这些不确定性，对于本工作，我们视觉检查了来自GAN+Mixmatch和GAN+$\Pi$-model的前12、25、100、800、2800和4000个预测，并调查了发现的透镜数量。整个论文中，我们仅关注使用相对排名（即，前$n$个预测）来评估模型性能，因为绝对预测阈值（如Jacobs等人[2019](#bib.bib31)中使用的值）的分布在不同模型之间可能会有显著差异（见附录[12](#A1.F12
    "图12 ‣ 附录A 模型性能和最终透镜样本 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")）。不同模型获得的绝对预测值可以进行标定，例如通过将获得的模型权重缩放到softmax层，但这超出了我们研究的范围。我们注意到，我们在本研究中使用的相对排名在这种缩放转换下将不会受到影响。
- en: One substantial caveat when looking at the top $n$ predictions is that, due
    to the density of galaxies in the sky and our image selection method, the top
    predictions are not necessarily unique. For example, the top 25 predictions from
    the GAN+$\Pi$-Model contain 17 unique sources and 8 duplicates centered on different
    nearby objects (shown in Figure [13](#A1.F13 "Figure 13 ‣ Appendix A Model performance
    and final lens sample ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") in the Appendix). For the top 2800
    predictions, the number of unique candidates is $\sim 1600$ on average (i.e.,
    $\sim 40\%$ are repeated). Since this is a significant portion of the number of
    images and would increase human effort during labeling, we remove such repetitions
    based on their sky coordinates. Given our image size, we remove duplicates within
    a radius of 26 arcseconds of each object in the top $n$ predictions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看前 $n$ 个预测时，一个重大警告是，由于天空中星系的密度和我们的图像选择方法，前面的预测不一定是唯一的。例如，从GAN+$\Pi$-Model获得的前25个预测包含17个独特源和8个重复源，后者集中在不同的附近物体上（见附录中的图[13](#A1.F13
    "Figure 13 ‣ Appendix A Model performance and final lens sample ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey)）。对于前2800个预测，独特候选者的数量平均为
    $\sim 1600$（即 $\sim 40\%$ 是重复的）。由于这是图像数量中的一个重要部分，并且会增加标记时的人工工作量，我们根据它们的天空坐标去除这些重复项。鉴于我们的图像大小，我们在前
    $n$ 个预测中，对于每个物体，在半径26角秒内去除重复项。
- en: 'The remaining images are then replaced with a larger field of view, ensuring
    that a given region of the sky needs to be visually inspected only once. We note
    that removing duplicates is strictly a post-processing step. Two of us (KVGC and
    TJ) visually inspected the lens candidates and classified them into confidence
    categories: Grade-A, Grade-B, Grade-C, and non-lenses. Grade-A indicates a high
    likelihood of being a strong lens system, on the basis of a clear arc morphology
    and/or coincidence with a moderately massive group of galaxies. Grade-B lenses
    generally have a nebulous arc-like feature surrounding a massive galaxy and/or
    have approximated linear extended arc morphology near a group or cluster of galaxies.
    It is uncertain if these features are from the lens or the effect of blending
    multiple sources. Grade-C lenses (not discussed in this paper) are the lowest-confidence
    candidates which typically show blended arc-like features likely arising from
    spiral arms, tidal features, or asymmetric diffuse light from the onset of mergers.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的图像随后被替换为更大的视野，以确保给定区域的天空仅需视觉检查一次。我们指出，去除重复项严格来说是后处理步骤。我们中的两人（KVGC和TJ）对透镜候选对象进行了视觉检查，并将其分类为不同的置信度等级：A级、B级、C级和非透镜。A级表示基于清晰的弧形形态和/或与中等质量的星系群重合的基础上，有很高的可能性是强透镜系统。B级透镜通常具有围绕巨大星系的模糊弧形特征和/或在星系群或星系团附近具有近似线性的扩展弧形形态。这些特征是否来自透镜还是由于多重源的混合效果尚不确定。C级透镜（本文未讨论）是置信度最低的候选者，通常显示出可能来源于螺旋臂、潮汐特征或合并开始时的不对称散射光的混合弧形特征。
- en: Figures [6](#S6.F6 "Figure 6 ‣ 6.1.1 Ablation study on data augmentations ‣
    6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") and [7](#S6.F7 "Figure 7 ‣ 6.1.1
    Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs
    and Augmentations have superior performance ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") show the color composite images for the 9 Grade-A and 13 Grade-B lenses
    found from the survey upon visually inspecting $\sim$ 2500 unique candidates (the
    top 4000 by rank). Their sky coordinates are listed in Table [8](#A1.T8 "Table
    8 ‣ Appendix A Model performance and final lens sample ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") in the
    Appendix. Several of the Grade-A lenses appear to be compound lenses or part of
    a moderately massive group or cluster of galaxies. This is interesting since our
    training data consists of only galaxy-galaxy lenses. This is likely due to the
    addition of GAN-generated images to our training data, as the GAN-generated images
    (Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey")) include irregularly
    shaped arcs.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6](#S6.F6 "图 6 ‣ 6.1.1 数据增强的消融研究 ‣ 6.1 GAN和增强的半监督算法性能优越 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")和[7](#S6.F7
    "图 7 ‣ 6.1.1 数据增强的消融研究 ‣ 6.1 GAN和增强的半监督算法性能优越 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")展示了从调查中通过视觉检查$\sim$
    2500个独特候选者（按排名前4000名）找到的9个A级和13个B级透镜的颜色复合图像。它们的天球坐标列在附录中的表[8](#A1.T8 "表 8 ‣ 附录A
    模型性能和最终透镜样本 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")中。几个A级透镜似乎是复合透镜或属于中等质量的星系团或群。这很有趣，因为我们的训练数据仅包含星系-星系透镜。这很可能是由于将GAN生成的图像添加到我们的训练数据中，因为GAN生成的图像（图[3](#S3.F3
    "图 3 ‣ 3.3 数据增强和GAN ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")）包括不规则形状的弧形。
- en: DLS212072337 ($z=1.81$)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: DLS212072337 ($z=1.81$)
- en: '![Refer to caption](img/46a6ed83bb9c00a87a84c03fc89bf8ce.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/46a6ed83bb9c00a87a84c03fc89bf8ce.png)'
- en: DLS432021848 ($z=1.94$; tentative)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: DLS432021848 ($z=1.94$; 暂定)
- en: '![Refer to caption](img/096f6830d97d090595d5865401bc071a.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/096f6830d97d090595d5865401bc071a.png)'
- en: 'Figure 8: *(*Top): NIRES spectra of Grade-A lens DLS212072337 at a redshift
    of $z=1.81$ with prominent [O iii] emission lines marked in blue. *(*Bottom):
    NIRES spectra of DLS432021848 showing the single emission line detected at $1.93\mu
    m$ which we tentatively identify as H$\alpha$ at $z=1.94$. In both panels the
    scaled sky spectrum is shown in orange (offset by -100), with gray shading denoting
    regions affected by strong sky lines.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '图8: *（*上图）：红移为$z=1.81$的A级透镜DLS212072337的NIRES光谱，显著的[O iii]发射线用蓝色标出。*（*下图）：DLS432021848的NIRES光谱显示在$1.93\mu
    m$处检测到的单一发射线，我们暂时将其识别为$z=1.94$的H$\alpha$。在两个面板中，缩放后的天球光谱以橙色显示（偏移-100），灰色阴影表示受强天线影响的区域。'
- en: '![Refer to caption](img/e578a5a04701de6f764c635142856f80.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e578a5a04701de6f764c635142856f80.png)'
- en: 'Figure 9: Distribution of the top 4000 lenses found by GAN+$\Pi$-model in color-color
    space. The left panel shows $B-R$ vs $B-V$ and the right panel shows $B-R$ vs
    $R-z$. The images above show examples of galaxies found in the two regions of
    the left panel separated by the purple line. Low-z galaxy candidates are clustered
    in the region above the trend line whereas all of the Grade-A lens candidates
    are below it. The right panel additionally shows that lens candidates are typically
    redder in $R-z$ colors ($\gtrsim 0.5$). A color selection based on the purple
    lines in each panel would yield higher precision in our lens candidate samples
    while retaining nearly all of the most probable lenses.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '图9: GAN+$\Pi$-模型在颜色-颜色空间中找到的前4000个透镜的分布。左面板显示$B-R$与$B-V$，右面板显示$B-R$与$R-z$。上面的图像展示了在左面板紫色线分隔的两个区域中找到的星系示例。低红移星系候选者聚集在趋势线以上的区域，而所有A级透镜候选者都在其下方。右面板还显示透镜候选者在$R-z$颜色中通常更红（$\gtrsim
    0.5$）。基于每个面板中的紫色线的颜色选择将提高我们透镜候选样本的精度，同时保留几乎所有最可能的透镜。'
- en: 6.2.1 Human inspection effort
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 人工检查工作
- en: 'We now examine how much human effort is required to find the 22 Grade-A and
    B lens candidates. To quantify the effort we consider the number of lenses found
    at different ranks, listed in Table [5](#S6.T5 "Table 5 ‣ 6.2 Catalog of Lens
    candidates found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey"). The rank threshold
    determines the number of unique images which must be visually inspected. Looking
    at the top 800 predictions from the GAN+MixMatch and GAN+$\Pi$-model (corresponding
    to 513 and 430 unique lens candidates respectively), we find 4 and 3 Grade-A lenses,
    and 2 and 5 Grade-B lenses respectively. This is several times ($\gtrsim$3$\times$)
    higher sky density than has been found from the shallower ground-based DES survey,
    and smaller than the density found in COSMOS with HST, as expected. The number
    of lens candidates found increases to 9 Grade-A and 13 Grade-B candidates when
    the top 4000 candidates ($\sim$2500 unique images) are considered. This corresponds
    to $\sim$1 lens per deg² searched, which is $\gtrsim$10$\times$ higher sky density
    of lenses compared to previous shallower ground-based surveys (as we discuss in
    Section [6.4](#S6.SS4 "6.4 Implications for future large-area sky surveys: sensitivity
    and angular resolution ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在来探讨寻找22个A级和B级透镜候选体所需的人力。为了量化这些努力，我们考虑了在不同等级下发现的透镜数量，详见表[5](#S6.T5 "Table
    5 ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")。排名阈值决定了必须进行视觉检查的独特图像数量。查看来自GAN+MixMatch和GAN+$\Pi$-model的前800个预测（分别对应513和430个独特的透镜候选体），我们发现4个和3个A级透镜，以及2个和5个B级透镜。这比从较浅的地面DES调查中发现的密度高出几倍（$\gtrsim$3$\times$），且比HST在COSMOS中发现的密度小，符合预期。当考虑前4000个候选体（$\sim$2500个独特图像）时，发现的透镜候选体数量增加到9个A级和13个B级候选体。这对应于每平方度搜索$\sim$1个透镜，比之前较浅的地面调查的透镜密度高出$\gtrsim$10$\times$（如我们在第[6.4](#S6.SS4
    "6.4 Implications for future large-area sky surveys: sensitivity and angular resolution
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")节中讨论的）。'
- en: In comparison, our supervised models (e.g., SupervisedV1, SupervisedV2) find
    $\lesssim 50\%$ of these top lens candidates. They also have lower precision values
    (Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")), with no compelling lenses found
    within the top $17$ candidates inspected (whereas G+PI finds 3 within this threshold
    range). This again highlights the value of adding data augmentation and GAN images.
    The SupervisedV2+DA+GAN model finds 3 times more lenses than SupervisedV2 within
    the same threshold range. These results demonstrate the efficiency with which
    the models explored in this work can find strong lenses.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们的监督模型（例如SupervisedV1，SupervisedV2）发现的前透镜候选体数量少于$\lesssim 50\%$。它们的精确度值也较低（表[3](#S6.T3
    "Table 3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms
    with GANs and Augmentations have superior performance ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")），在前$17$个检查的候选体中没有发现有吸引力的透镜（而G+PI在这个阈值范围内发现了3个）。这再次突显了数据增强和GAN图像的价值。SupervisedV2+DA+GAN模型在相同阈值范围内发现的透镜数量是SupervisedV2的3倍。这些结果展示了本研究中探讨的模型在发现强透镜方面的效率。
- en: 6.2.2 Spectroscopic confirmation of two Grade-A lenses
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 两个A级透镜的光谱确认
- en: 'While image morphology can provide compelling evidence for strong gravitational
    lensing, spectroscopic redshifts are the standard to unambiguously establish the
    lensing nature of a system. We have obtained spectroscopy with Keck Observatory
    to confirm the lensing nature of two Grade-A systems presented herein: DLS212072337
    and DLS432021848 (Figure [8](#S6.F8 "Figure 8 ‣ 6.2 Catalog of Lens candidates
    found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey")). Observations of the arcs
    were conducted with NIRES (Wilson et al., [2004](#bib.bib77)) on the Keck II telescope.
    Full details of the observations and data reduction are described in Tran et al.
    ([2022](#bib.bib75)), along with spectroscopic redshifts for DLS212072337 (reported
    as AGEL091935+303156). We find a secure redshift of $z_{\text{arc}}=1.81$ for
    DLS212072337 from detection of H$\alpha$ $\lambda$6564 and [O III] $\lambda\lambda$4960,5008
    emission lines. The deflector galaxy is at a redshift of $z_{\text{def}}=0.43$,
    based on stellar absorption features from optical SDSS/BOSS spectra.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图像形态可以提供有力的强引力透镜证据，但光谱红移是明确确定系统引力透镜特性的标准。我们已在Keck天文台获得了光谱数据，以确认本文中呈现的两个A等级系统的透镜特性：DLS212072337和DLS432021848（图[8](#S6.F8
    "Figure 8 ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣
    Optimizing machine learning methods to discover strong gravitational lenses in
    the Deep Lens Survey")）。对弧形图像的观测是在Keck II望远镜上使用NIRES（Wilson等，[2004](#bib.bib77)）进行的。观测和数据处理的详细信息已在Tran等（[2022](#bib.bib75)）中描述，以及DLS212072337的光谱红移（报告为AGEL091935+303156）。我们通过检测H$\alpha$
    $\lambda$6564和[O III] $\lambda\lambda$4960,5008发射线，确定了DLS212072337的红移为$z_{\text{arc}}=1.81$。基于光学SDSS/BOSS光谱中的恒星吸收特征，该引力透镜星系的红移为$z_{\text{def}}=0.43$。
- en: 'We observed DLS432021848 with NIRES on 12 January 2022 using the same methodology.
    We obtained 6 exposures of 300 seconds each. We detect a single emission line
    at $~{}\lambda=1.93\mu\text{m}$ which we tentatively identify as either H$\alpha$
    at $z_{\text{arc}}=1.94$ or [O III] $\lambda$5008 at $z_{\text{arc}}=2.85$. However,
    we are unable to confirm the redshift with other strong lines, which fall in regions
    of poor atmospheric transmission at both potential redshifts. We find further
    support for the lensing nature of DLS432021848 from its morphology in follow-up
    HST imaging (discussed in Section [6.4](#S6.SS4 "6.4 Implications for future large-area
    sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")), which shows clear kurtosis and evidence of multiple lensed images.
    Thus we are reasonably confident that this is indeed a strong lensing system on
    the basis of high-resolution imaging, despite the limited spectroscopic information.
    Together with DLS212072337, these results give additional confidence in the sample
    of lens candidates presented in this paper and demonstrate that our methods are
    successful.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用相同的方法在2022年1月12日用NIRES观测了DLS432021848。我们获得了6个每个300秒的曝光。我们检测到一个发射线在$~{}\lambda=1.93\mu\text{m}$，我们初步将其识别为H$\alpha$（$z_{\text{arc}}=1.94$）或[O
    III] $\lambda$5008（$z_{\text{arc}}=2.85$）。然而，我们无法用其他强线确认红移，因为这些线落在两个潜在红移的气候传输差的区域。我们通过后续HST成像的形态学进一步支持DLS432021848的引力透镜特性（在第[6.4节](#S6.SS4
    "6.4 Implications for future large-area sky surveys: sensitivity and angular resolution
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")讨论），该成像显示出明显的峰度和多重透镜图像的证据。因此，尽管光谱信息有限，我们基于高分辨率成像合理地认为这是一个强引力透镜系统。结合DLS212072337，这些结果进一步增强了对本文所提出的透镜候选样本的信心，并证明了我们方法的成功。'
- en: We note that redshifts are known for two additional Grade-A candidate deflectors
    (DLS212148326, DLS421095124) from archival data. DLS212148326 is at $z_{\text{def}}=0.424$
    from SDSS/BOSS spectra, while DLS421095124 is part of a massive galaxy cluster
    spectroscopically confirmed at $z_{\text{def}}=0.680$ (Wittman et al., [2003](#bib.bib79);
    Wittman et al., [2006](#bib.bib80), reported as DLSCL J1055.2-0503). These redshifts
    are promising, as the distances and approximate masses are consistent with the
    deflection angles implied by the strong lensing interpretation of these images.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，通过档案数据已知有两个额外的A等级候选引力透镜（DLS212148326，DLS421095124）的红移。DLS212148326的红移为$z_{\text{def}}=0.424$，来源于SDSS/BOSS光谱，而DLS421095124是一个通过光谱确认的巨大星系团，红移为$z_{\text{def}}=0.680$（Wittman等，[2003](#bib.bib79)；Wittman等，[2006](#bib.bib80)，报告为DLSCL
    J1055.2-0503）。这些红移非常有前景，因为其距离和近似质量与这些图像的强引力透镜解释所暗示的弯曲角度一致。
- en: 6.2.3 Distribution of lensed candidates in color-color space
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 透镜候选者在颜色-颜色空间中的分布
- en: The analysis and model performance described thus far in the paper is based
    on a source selection using an intentionally simple $R$ band magnitude cut and
    SExtractor flags (Section [2.1](#S2.SS1 "2.1 Source selection and regions of interest
    ‣ 2 Deep Lens Survey Data ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). We have demonstrated in the above
    sections that such cuts are sufficient to search for lensed candidates in the
    DLS. However, more sophisticated selections can increase the efficiency of lens
    searches. Here we briefly consider how color selection can provide higher-purity
    samples.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本文到目前为止描述的分析和模型性能基于使用有意简单的$R$带幅度切割和SExtractor标志的源选择（第[2.1节](#S2.SS1 "2.1 Source
    selection and regions of interest ‣ 2 Deep Lens Survey Data ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")）。我们在上述部分中展示了这些切割足以在DLS中搜索透镜候选者。然而，更复杂的选择可以提高镜头搜索的效率。在这里，我们简要考虑如何通过颜色选择提供更高纯度的样本。
- en: 'In Figure [9](#S6.F9 "Figure 9 ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") we show the distribution of Grade-A and B lenses
    from Section [6.2](#S6.SS2 "6.2 Catalog of Lens candidates found ‣ 6 Results and
    Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") in various color-color spaces, along with the
    top 4000 ranked images from the GAN-$\Pi$-model as an example. These colors generally
    correspond to the central (candidate deflector) galaxy. The top lens candidates
    are not distributed uniformly, and we demonstrate two color-color selections where
    the top candidates are clustered: $(B-V)<0.56(B-R)-0.02$ (purple line in left
    panel), and $R-z\gtrsim 0.4$ (right panel). Such simple color cuts can retain
    all Grade-A lenses while removing the majority of false positives, thereby reducing
    the required human inspection effort. Physically, these colors are indicative
    of 4000 Å breaks at redshifts $z\gtrsim 0.25$ (i.e. in the $V$ or $R$ band) whereas
    lower-$z$ galaxies are less likely to act as strong lenses.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[9](#S6.F9 "Figure 9 ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and
    Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")中，我们展示了第[6.2节](#S6.SS2 "6.2 Catalog of Lens candidates
    found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey")中的A等级和B等级镜头在各种颜色-颜色空间中的分布，以及来自GAN-$\Pi$-model的排名前4000张图像作为示例。这些颜色通常对应于中心（候选偏折体）银河系。最佳镜头候选者的分布并不均匀，我们展示了两个颜色-颜色选择，其中顶级候选者被聚集：$(B-V)<0.56(B-R)-0.02$（左侧面板中的紫色线），以及$R-z\gtrsim
    0.4$（右侧面板）。这种简单的颜色切割可以保留所有A等级镜头，同时去除大多数假阳性，从而减少所需的人为检查工作。物理上，这些颜色指示了红移$z\gtrsim
    0.25$（即在$V$或$R$带）处的4000 Å 断裂，而低红移的银河系不太可能作为强镜头。
- en: The distribution of lens candidates in color space suggests that the precision
    of our models can be further improved by adopting color criteria as a pre- or
    post-processing step, with minimal loss of the best candidates. Using photometric
    redshift and mass estimates is a similar and potentially even more promising method
    (Schmidt & Thorman, [2013](#bib.bib62)) although it is beyond the scope of this
    paper. Alternatively, a state-of-the-art automated means to address this would
    be by using self-similarity based approaches (e.g., Stein et al., [2021](#bib.bib72)),
    wherein a CNN further classifies the lens probabilities based on their similarity
    with each other.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在色彩空间中镜头候选者的分布表明，通过将颜色标准作为预处理或后处理步骤，我们的模型精度可以进一步提高，几乎不会丢失最佳候选者。使用光度红移和质量估计是一种类似且可能更有前景的方法（Schmidt
    & Thorman, [2013](#bib.bib62)），尽管这超出了本文的范围。另一种解决此问题的最先进的自动化方法是使用基于自相似性的 approaches（例如，Stein
    et al., [2021](#bib.bib72)），其中CNN根据镜头的相似性进一步分类镜头概率。
- en: '|  | Simulated Lens |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | 模拟镜头 |  |'
- en: '| ![Refer to caption](img/fd799fc2a314bccb6e59b862a2f3f466.png) | ![Refer to
    caption](img/fe9f67eca2d0530e8540eb15d54696bc.png) | ![Refer to caption](img/0dfcf8170d042162540146bb64a0ba07.png)
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/fd799fc2a314bccb6e59b862a2f3f466.png) | ![参见说明](img/fe9f67eca2d0530e8540eb15d54696bc.png)
    | ![参见说明](img/0dfcf8170d042162540146bb64a0ba07.png) |'
- en: '|  | Lenses found in DLS |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | DLS中发现的镜头 |  |'
- en: '| ![Refer to caption](img/4acf4a18c43c2ec72f0054cfc7f39d80.png) | ![Refer to
    caption](img/f7a9f921b1ac930c89460311201f9fdb.png) | ![Refer to caption](img/c2ac9a854f0ef76f9df6d7488a15d049.png)
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/4acf4a18c43c2ec72f0054cfc7f39d80.png) | ![参见说明](img/f7a9f921b1ac930c89460311201f9fdb.png)
    | ![参见说明](img/c2ac9a854f0ef76f9df6d7488a15d049.png) |'
- en: '| ![Refer to caption](img/96eb6a632a1d7da39266a3c7e3e4eb54.png) | ![Refer to
    caption](img/3c0405d32bf175fa35f6a135feae4a72.png) | ![Refer to caption](img/41fe0a3e6c3c951f843d02b47c2f7ac3.png)
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/96eb6a632a1d7da39266a3c7e3e4eb54.png) | ![参见说明](img/3c0405d32bf175fa35f6a135feae4a72.png)
    | ![参见说明](img/41fe0a3e6c3c951f843d02b47c2f7ac3.png) |'
- en: '|  | False Positive |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | 假阳性 |  |'
- en: '| ![Refer to caption](img/4a0909392ae6cd5ed6e8b05174970daf.png) | ![Refer to
    caption](img/624900a89ff1d2211285c8e9b2ebccc3.png) | ![Refer to caption](img/2035371952e5a738f999d52fab84bddd.png)
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/4a0909392ae6cd5ed6e8b05174970daf.png) | ![参见说明](img/624900a89ff1d2211285c8e9b2ebccc3.png)
    | ![参见说明](img/2035371952e5a738f999d52fab84bddd.png) |'
- en: '| ![Refer to caption](img/1ebe6fac7c53d3c2ce74cabbecccedcd.png) | ![Refer to
    caption](img/f3a9decd588bd5f2c1225dfb68120db6.png) | ![Refer to caption](img/2ab90d661252d8607fe4da82af929c98.png)
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/1ebe6fac7c53d3c2ce74cabbecccedcd.png) | ![参见说明](img/f3a9decd588bd5f2c1225dfb68120db6.png)
    | ![参见说明](img/2ab90d661252d8607fe4da82af929c98.png) |'
- en: '|  | NonLens |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | 非透镜 |  |'
- en: '| ![Refer to caption](img/b86684d65f2f4f8fd1b65fd68d0a53bd.png) | ![Refer to
    caption](img/975e912c0c2dd45583a862792a0ea095.png) | ![Refer to caption](img/0b876ca983ef7a599fb443effe499c4b.png)
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/b86684d65f2f4f8fd1b65fd68d0a53bd.png) | ![参见说明](img/975e912c0c2dd45583a862792a0ea095.png)
    | ![参见说明](img/0b876ca983ef7a599fb443effe499c4b.png) |'
- en: 'Figure 10: Grad-CAM++ heatmaps for an example simulated lens, two Grade-A lenses,
    two false positive lenses, and a NonLens. The left column shows the color composite
    image obtained from HumVI and passed to the model. The right column shows the
    Gradcam++ heatmaps. The red and green shading indicates regions of high and moderate
    importance to the model, respectively, whereas blue represents low importance.
    The middle column shows the heatmaps superimposed on input images for visualization
    purposes. For the simulated lens, we can clearly see that the entire lensed arc
    region is taken into consideration. For the Grade-A lens candidates found in DLS,
    we also find that the lensed arc features are considered important by the model,
    despite a range of lensing morphologies and colors. This suggests that models
    have indeed successfully generalized to the survey data. Notably, the massive
    deflector (i.e., the luminous red galaxy) causing the lensing effect is not highlighted
    in the simulated or candidate lens systems. Additional objects in the field are
    also highlighted in heatmaps for the Grade-A lenses, which is also apparent in
    the False Positive and NonLens examples. In the case of the False Positives, the
    highlighted object distributions resemble an “Einstein cross” lens configuration.
    Heatmaps for all the Grade-A lenses are provided in Figure [15](#A1.F15 "Figure
    15 ‣ Appendix A Model performance and final lens sample ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") in the
    appendix.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：示例模拟透镜、两个A级透镜、两个假阳性透镜和一个非透镜的 Grad-CAM++ 热力图。左列显示了从 HumVI 获得并传递给模型的彩色合成图像。右列显示了
    Grad-CAM++ 热力图。红色和绿色阴影分别表示模型的高重要性和中等重要性区域，而蓝色表示低重要性。中间列显示了叠加在输入图像上的热力图，便于可视化。对于模拟透镜，我们可以清楚地看到整个透镜弧区域被考虑在内。对于
    DLS 中找到的A级透镜候选者，我们也发现透镜弧特征被模型认为重要，尽管透镜的形态和颜色各异。这表明模型确实成功地泛化到了调查数据。值得注意的是，导致透镜效应的巨大偏折体（即发光的红色星系）在模拟或候选透镜系统中没有被突出显示。领域中的其他物体在A级透镜的热力图中也被突出显示，这在假阳性和非透镜示例中也很明显。在假阳性的情况下，突出显示的物体分布类似于“爱因斯坦十字”透镜配置。所有A级透镜的热力图见附录中的图
    [15](#A1.F15 "图 15 ‣ 附录 A 模型性能和最终透镜样本 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")。
- en: 6.3 Lensing signatures identified by the models
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 模型识别的透镜特征
- en: We now examine which features of the lens candidate images are most relevant
    for the model predictions. Deep neural networks (such as ResNetV2 used in this
    work) are often considered as “black boxes” with all input information collapsed
    to a simple prediction for the user to interpret. Having only a single output,
    it is impossible to discern which distinguishing features of a gravitational lens
    are actually being identified and considered by the models. Fortunately, in the
    past few years, there have been a variety of methods proposed to alleviate this
    such as occlusion methods, Guided Backprop (Springenberg et al., [2015](#bib.bib71)),
    CAM (Zhou et al., [2016](#bib.bib82)), Grad-CAM (Selvaraju et al., [2017](#bib.bib64)),
    Grad-CAM++ (Chattopadhay et al., [2018](#bib.bib12)), and DeepSHAP (Fernando et al.,
    [2019](#bib.bib18)).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在检查透镜候选图像的哪些特征对模型预测最为相关。深度神经网络（例如本文中使用的ResNetV2）通常被认为是“黑箱”，所有输入信息被压缩为用户解释的简单预测。由于只有一个输出，无法辨别模型实际识别和考虑的是引力透镜的哪些区分特征。幸运的是，在过去几年中，已经提出了各种方法来缓解这个问题，如遮挡方法、Guided
    Backprop（Springenberg等，[2015](#bib.bib71)）、CAM（Zhou等，[2016](#bib.bib82)）、Grad-CAM（Selvaraju等，[2017](#bib.bib64)）、Grad-CAM++（Chattopadhay等，[2018](#bib.bib12)）和DeepSHAP（Fernando等，[2019](#bib.bib18)）。
- en: Gradient-based interpretation methods (e.g., Grad-CAM++) effectively compute
    gradients on intermediate feature maps of the network to determine the importance
    of a feature. These gradient maps can then be overlaid on top of the original
    input image, in order to assess which image regions are contributing most to the
    predicted output from the classifier. These methods are not without drawbacks
    (e.g., Adebayo et al., [2018](#bib.bib1)) but can provide valuable insight. Here
    we use Grad-CAM++ to analyze some of our trained models.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的解释方法（例如Grad-CAM++）有效地计算网络中间特征图上的梯度，以确定特征的重要性。这些梯度图可以叠加在原始输入图像上，以评估哪些图像区域对分类器的预测输出贡献最大。这些方法也并非没有缺点（例如Adebayo等，[2018](#bib.bib1)），但可以提供有价值的见解。在这里，我们使用Grad-CAM++分析我们的一些训练模型。
- en: Figure [10](#S6.F10 "Figure 10 ‣ 6.2.3 Distribution of lensed candidates in
    color-color space ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") shows Grad-CAM++ heatmaps obtained for a few illustrative
    examples. We consider a simulated lens from the training data, real Grade-A lenses
    from the survey, false positive images (i.e., images which are classified as lenses
    but show no visual evidence of lensing), and a non-lens. In the case of the simulated
    lens, it is clear that the model is indeed making its prediction based on the
    lensed arc features. For the Grade-A lenses, the model does indeed discern the
    lensed arcs, but there are additional unrelated regions within the images that
    also influence its decision. Curiously, the central massive deflector galaxy is
    not highlighted in these cases. In the case of the false positives, the model
    encouragingly is not misled by the extended central galaxies, but rather the heatmap
    highlights multiple sources of similar color which surround the central galaxy.
    For example in the spiral galaxy false-positive image, it is clear that the model
    picks up on the three nearby red objects. The location and color of these nearby
    objects is indeed similar to plausible multiple-image lensing configurations.
    It thus appears that the model has successfully learned to identify the astrophysical
    signatures of strong lensing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](#S6.F10 "图 10 ‣ 6.2.3 色彩空间中的透镜候选分布 ‣ 6.2 发现的透镜候选目录 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")展示了为一些示例获得的Grad-CAM++热图。我们考虑了来自训练数据的模拟透镜、调查中的真实A等级透镜、假阳性图像（即被分类为透镜但没有显示透镜视觉证据的图像）以及非透镜。在模拟透镜的情况下，模型显然是基于透镜弧线特征做出预测。对于A等级透镜，模型确实能够识别透镜弧线，但图像中还有额外的不相关区域也会影响模型的决策。有趣的是，中央的大质量透镜星系在这些情况下没有被突出显示。在假阳性的情况下，模型令人鼓舞地没有被扩展的中央星系误导，而是热图突出了围绕中央星系的多个相似颜色的来源。例如，在螺旋星系假阳性图像中，模型明显关注到三个附近的红色物体。这些附近物体的位置和颜色确实与可能的多图像透镜配置类似。因此，模型似乎成功地学会了识别强透镜的天体物理特征。
- en: 6.3.1 Finding red arcs
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1 查找红色弧线
- en: As discussed in Section [4.2](#S4.SS2 "4.2 Generating the simulated Lenses dataset
    ‣ 4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), our Lens dataset used for
    training only consists of lensed arcs with blue optical colors. However, it is
    encouraging that the models have also identified red arcs such as the system DLS212148326
    (Figure [6](#S6.F6 "Figure 6 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). The network may be learning to
    identify red arcs through color augmentations (Figure [2](#S3.F2 "Figure 2 ‣ 3.3
    Data augmentation and GANs ‣ 3 Deep Learning Architecture and learning methods
    used ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")). Although red-lensed arcs are known to exist, presumably
    a training dataset consisting of only blue arcs is not ideal to robustly search
    for and quantify them. It could be the case that adding more augmentations or
    fine-tuning existing ones might suffice to search for arcs of various colors.
    Alternatively, a broader range of arc colors could be used in the simulated training
    set, or a separate classifier could be constructed from a training set of red
    arcs. Given our adopted training set, we consider the number of red-lensed arcs
    found from this work to be a lower limit (relative to the blue arcs). Additionally,
    there are likely many fainter blue or red arcs which our training set does not
    represent, although the detection of fainter objects is naturally more challenging.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[4.2节](#S4.SS2 "4.2 生成模拟透镜数据集 ‣ 4 训练和验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")中讨论的，我们用于训练的透镜数据集仅包含具有蓝色光学颜色的透镜弧。然而，令人鼓舞的是，这些模型也识别了红色弧，例如系统DLS212148326（图[6](#S6.F6
    "图6 ‣ 6.1.1 数据增强的消融研究 ‣ 6.1 使用GANs和数据增强的半监督算法表现优越 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")）。网络可能通过颜色增强学习识别红色弧（图[2](#S3.F2
    "图2 ‣ 3.3 数据增强和GANs ‣ 3 深度学习架构及学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")）。尽管已知存在红色透镜弧，但仅包含蓝色弧的训练数据集可能并不理想，因为它无法稳健地搜索和量化这些弧。可能需要通过增加更多的增强或微调现有的增强来搜索各种颜色的弧。或者，可以在模拟训练集中使用更广泛的弧色范围，或从红色弧的训练集中构建一个单独的分类器。鉴于我们采用的训练集，我们认为这项工作中发现的红色透镜弧的数量是一个下限（相对于蓝色弧）。此外，可能还有许多较暗的蓝色或红色弧，而我们的训练集没有代表，尽管探测较暗的对象自然更具挑战性。
- en: '6.4 Implications for future large-area sky surveys: sensitivity and angular
    resolution'
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 对未来大面积天区调查的影响：灵敏度和角分辨率
- en: 'The next generation of wide-area sky surveys is expected to uncover $\gtrsim
    10^{5}$ strong lens systems (e.g., Oguri & Marshall, [2010](#bib.bib57); Collett,
    [2015](#bib.bib14)). Here we consider the gain in lens detection with survey depth
    and angular resolution based on our DLS sample from Section [6.2](#S6.SS2 "6.2
    Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey").
    We compare the sky density of detected lens candidates with two other illustrative
    examples of CNN-based searches in Table [6](#S6.T6 "Table 6 ‣ 6.4 Implications
    for future large-area sky surveys: sensitivity and angular resolution ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey"). In our DLS search, we find $\sim$0.5 Grade-A
    lenses per square degree (or $\sim$1 Grade-A+B lenses per square degree). This
    is considerably larger than found in shallower surveys such as SDSS and DES, which
    have uncovered $\sim$0.1 lenses per square degree (in regions far from the galactic
    plane). While these surveys have a comparable seeing-limited resolution, sharper
    image quality enables more lenses to be found. An example is the search of COSMOS
    HST imaging by Pourrahmani et al. ([2018](#bib.bib60)) using a CNN approach, which
    found 13 Grade-A candidates and 70 Grade-A+B candidates in the 2 square degree
    field (i.e., $\sim$35 per square degree). Therefore, we see that the sky density
    of detectable strong lens systems increases by $\sim$10 times when going from
    shallower ground-based surveys (e.g., SDSS) to the DLS, and by another factor
    of $\gtrsim$10 when the angular resolution is improved by an order of magnitude
    with space-based HST imaging at modest depth. These results generally support
    the predictions of large lens samples which will become detectable with near-future
    surveys planned with the Rubin (LSST Science Collaboration et al., [2009](#bib.bib40)),
    Roman (Spergel et al., [2015](#bib.bib70)), and Euclid (Laureijs et al., [2011](#bib.bib42))
    observatories.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '下一代广域天空调查预计将揭示$\gtrsim 10^{5}$个强引力透镜系统（例如，Oguri & Marshall，[2010](#bib.bib57)；Collett，[2015](#bib.bib14)）。在这里，我们考虑基于第[6.2节](#S6.SS2
    "6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")的DLS样本，透镜检测在调查深度和角分辨率方面的增益。我们在表[6](#S6.T6 "Table 6 ‣ 6.4 Implications
    for future large-area sky surveys: sensitivity and angular resolution ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")中将检测到的透镜候选者的天空密度与其他两个基于CNN的搜索示例进行比较。在我们的DLS搜索中，我们发现每平方度$\sim$0.5个A级透镜（或每平方度$\sim$1个A级+B级透镜）。这比在较浅的调查中发现的要大得多，例如SDSS和DES，这些调查揭示了每平方度$\sim$0.1个透镜（在远离银河平面的区域）。虽然这些调查具有可比的视场限制分辨率，但更清晰的图像质量使得发现更多的透镜成为可能。例如，Pourrahmani等人（[2018](#bib.bib60)）使用CNN方法对COSMOS
    HST成像的搜索，在2平方度的区域内发现了13个A级候选者和70个A级+B级候选者（即每平方度$\sim$35个）。因此，我们看到，从较浅的地面调查（例如，SDSS）到DLS时，可检测的强引力透镜系统的天空密度增加了$\sim$10倍，当通过空间HST成像在适中深度下提高角分辨率一个数量级时，又增加了$\gtrsim$10倍。这些结果通常支持对将来计划的大型透镜样本的预测，这些样本将在即将到来的调查中变得可检测，例如与Rubin（LSST科学合作组等，[2009](#bib.bib40)）、Roman（Spergel等，[2015](#bib.bib70)）和Euclid（Laureijs等，[2011](#bib.bib42)）天文台相关的调查。'
- en: 'To visually illustrate the detection of lenses at different depths and angular
    resolutions, Figure [11](#S6.F11 "Figure 11 ‣ 6.4 Implications for future large-area
    sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") compares DECaLS, DLS, and HST imaging³³3The HST image was secured as
    part of program HST-GO-16773 targeting lens candidates identified primarily in
    DES and DECaLS imaging (Tran et al., [2022](#bib.bib75)). In brief, the HST image
    in Figure [11](#S6.F11 "Figure 11 ‣ 6.4 Implications for future large-area sky
    surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") was taken with WFC3-IR in the F140W filter with $\sim$30 minutes of exposure
    time ($<$1 orbit), and reduced using standard procedures. Details of the HST program
    will be described in a forthcoming paper. for the Grade-A lens candidate DLS432021848
    found in this work. A blue arc is clearly visible in the DLS image and shows typical
    lensing morphology in the high-resolution HST image. However, the arc is only
    marginally visible in shallower DECaLS imaging. Indeed, most (if not all) of the
    Grade-A lens candidates found from this work would be difficult to detect in shallower
    imaging surveys (e.g., DECaLS; hence for example they are not included in the
    catalog of Huang et al. [2020](#bib.bib27)).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '为了直观地展示在不同深度和角分辨率下透镜的检测，图 [11](#S6.F11 "Figure 11 ‣ 6.4 Implications for future
    large-area sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") 比较了 DECaLS、DLS 和 HST 图像³³3 HST 图像作为 HST-GO-16773 项目的组成部分拍摄，主要针对在
    DES 和 DECaLS 图像中识别的透镜候选体（Tran et al., [2022](#bib.bib75)）。简而言之，图 [11](#S6.F11
    "Figure 11 ‣ 6.4 Implications for future large-area sky surveys: sensitivity and
    angular resolution ‣ 6 Results and Discussion ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey") 中的 HST 图像使用了
    F140W 滤镜的 WFC3-IR 拍摄，曝光时间约为 $ \sim $30 分钟（$<1$ 轨道），并使用标准程序进行处理。HST 项目的详细信息将在即将发表的论文中描述。对于本文中发现的
    Grade-A 透镜候选体 DLS432021848，DLS 图像中清晰可见一个蓝色弧形，并且在高分辨率的 HST 图像中显示出典型的透镜形态。然而，在较浅的
    DECaLS 图像中弧形仅能勉强看到。实际上，本文中发现的大部分（如果不是全部）Grade-A 透镜候选体在较浅的成像调查中难以检测（例如 DECaLS；例如，它们未被
    Huang et al. [2020](#bib.bib27) 的目录包含）。'
- en: '![Refer to caption](img/f067c4629041fd17b96a99dc88b1ec9b.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/f067c4629041fd17b96a99dc88b1ec9b.png)'
- en: 'Figure 11: Comparison of the image quality from different observations of the
    lens system DLS432021848, which shows a prominent blue arc in DLS imaging (below
    center of images; all panels show the same field of view). *Left*: The arc is
    apparent but not well detected in DECaLS imaging, which has modest sensitivity.
    This image would likely be flagged in a low-confidence category and indeed was
    not identified in previous lens searches (e.g., Huang et al., [2020](#bib.bib27)).
    *Middle*: DLS image of the target showing a prominent blue arc-like feature below
    the red deflector galaxy, characteristic of a gravitational lens system. The increased
    sensitivity of DLS compared to DECaLS imaging (Table [6](#S6.T6 "Table 6 ‣ 6.4
    Implications for future large-area sky surveys: sensitivity and angular resolution
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")) enables clear arc detection. *Right*:
    Near-infrared image of the same target observed with HST, with a diffraction-limited
    angular resolution approximately 6 times sharper than DLS or DECaLS images. The
    HST image reveals the lensed arc morphology at a high signal-to-noise ratio. This
    demonstrates the capabilities of a ground-based telescope at good depth (e.g.,
    DLS), and a diffraction-limited space-based telescope with moderate exposure time
    (e.g., HST).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11：不同观测中透镜系统 DLS432021848 的图像质量比较，DLS 图像中显示了明显的蓝色弧形（图像下方中心；所有面板显示相同的视场）。*左*:
    在 DECaLS 图像中弧形明显但未被很好检测，该图像的灵敏度适中。这张图像可能会被标记为低置信度类别，实际上在之前的透镜搜索中未被识别（例如，Huang
    et al., [2020](#bib.bib27)）。*中间*: 目标的 DLS 图像显示出红色偏转星系下方的明显蓝色弧形特征，这是引力透镜系统的特征。与
    DECaLS 图像相比，DLS 的灵敏度提高（表 [6](#S6.T6 "Table 6 ‣ 6.4 Implications for future large-area
    sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")）使得弧形检测更清晰。*右*: 使用 HST 观察的同一目标的近红外图像，其衍射极限的角分辨率大约比 DLS 或 DECaLS 图像锐利 6
    倍。HST 图像在高信噪比下揭示了透镜弧形的形态。这展示了地面望远镜在良好深度（例如 DLS）和衍射极限空间望远镜在适中曝光时间下（例如 HST）的能力。'
- en: '| Survey | Lenses found | 5$\sigma$ point | FWHM | References |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 调查 | 发现的透镜 | $5\sigma$点 | FWHM | 参考文献 |'
- en: '|  | per sq.deg | source detection |  |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | 每平方度 | 来源检测 |  |  |'
- en: '|  |  | (r/R/F814W-band |  |  |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (r/R/F814W波段 |  |  |'
- en: '|  |  | magnitude) |  |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 亮度) |  |  |'
- en: '| DES/DECaLS | $\sim 0.1$ | 23.6 (r) | 0$\aas@@fstack{\prime\prime}$98 | J19
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| DES/DECaLS | $\sim 0.1$ | 23.6 (r) | 0$\aas@@fstack{\prime\prime}$98 | J19
    |'
- en: '| DLS | 1 | 26.7 (R) | 0$\aas@@fstack{\prime\prime}$9 | This work |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| DLS | 1 | 26.7 (R) | 0$\aas@@fstack{\prime\prime}$9 | 本研究 |'
- en: '| COSMOS | $\sim 35$ | 27.2 (F814W) | 0$\aas@@fstack{\prime\prime}$07 | P18,K07
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| COSMOS | $\sim 35$ | 27.2 (F814W) | 0$\aas@@fstack{\prime\prime}$07 | P18,K07
    |'
- en: 'Table 6: Number of lenses found using machine learning methods per square degree
    of sky in different surveys, along with the $5\sigma$ point source detection depth
    and median angular resolution (given as the FWHM: full-width at half maximum).
    We note that CNN and grading methods employed to find lenses in each survey are
    different; the density of lenses should thus be treated as an approximate comparison.
    References are as follows. J19: Jacobs et al. ([2019](#bib.bib31)), P18: Pourrahmani
    et al. ([2018](#bib.bib60)), K07: Koekemoer et al. ([2007](#bib.bib33)).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：在不同调查中每平方度天空中使用机器学习方法发现的透镜数量，以及$5\sigma$点源检测深度和中值角分辨率（以FWHM：半最大宽度给出）。我们注意到用于发现透镜的CNN和分级方法在每项调查中有所不同，因此透镜的密度应视为近似比较。参考文献如下。J19：Jacobs等人（[2019](#bib.bib31)），P18：Pourrahmani等人（[2018](#bib.bib60)），K07：Koekemoer等人（[2007](#bib.bib33)）。
- en: Given the detectability of many lens systems with upcoming surveys, it is clear
    that machine learning approaches (such as those we have explored here) will be
    vitally important for the efficient selection of large samples. We have also demonstrated
    the feasibility of spectroscopically following up on these moderately faint arc
    systems (Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Spectroscopic confirmation of two
    Grade-A lenses ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")), which will be vital for confirmation and subsequent
    analyses.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到即将进行的调查中许多透镜系统的可检测性，显然机器学习方法（例如我们在这里探索的那些）对于高效选择大样本至关重要。我们还展示了光谱跟踪这些中等暗弧系统的可行性（第[6.2.2节](#S6.SS2.SSS2
    "6.2.2 确认两个A级透镜 ‣ 6.2 透镜候选目录 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")），这对确认和后续分析至关重要。
- en: 7 Conclusions
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we have evaluated the performance of different CNN learning approaches
    and data augmentations on their ability to efficiently find gravitational lens
    candidates in the Deep Lens Survey. We make use of the deep learning architecture
    ResNet for our experiments, along with a training dataset consisting of simulated
    Lenses and survey image NonLenses. We demonstrate that by using these state-of-the-art
    semi-supervised learning approaches, we can greatly reduce the human effort required
    to find lensed candidates from a survey. We summarize our key results below.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们评估了不同CNN学习方法和数据增强在有效寻找深度透镜调查中的引力透镜候选者方面的表现。我们在实验中使用了深度学习架构ResNet，并使用了一个由模拟透镜和调查图像非透镜组成的训练数据集。我们展示了通过使用这些最先进的半监督学习方法，我们可以大大减少从调查中寻找透镜候选者所需的人工努力。我们总结了我们的关键结果如下。
- en: '1.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Among 17 variants of learning approaches tested in this work, we find that our
    best performing models (i.e., those which have high precision and minimize false
    positives during human inspection) are GAN+MixMatch and GAN+$\Pi-$model. They
    have a precision of $\sim 86\%$ and $\sim 97\%$ at 50% recall and, $\sim 22\%$
    and $\sim 8\%$ at 100% recall respectively. In comparison, our supervised models
    have a precision of $\sim 3\%$ at 100% recall. This increase in the performance
    of the best models can be attributed largely to three factors. (1) They leverage
    data augmentation (Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised
    learning ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")) during training, which helps them to generalize better. (2) The datasets
    used to train these models to contain simulated Lenses as well as GAN-generated
    images (Section [3](#S3 "3 Deep Learning Architecture and learning methods used
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")), which serves as an additional form of data augmentation.
    (3) Both of these top models employ a semi-supervised learning approach (MixMatch,
    $\Pi$-model) which enables our methods to adapt to distributional shift (Section [3.2](#S3.SS2
    "3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). These results indicate that data
    augmentation, GANs, and semi-supervised learning are highly effective approaches
    for building an efficient lens classifier.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本工作测试的17种学习方法变体中，我们发现我们表现最佳的模型（即在人工检查中具有高精度和最小化假阳性的模型）是 GAN+MixMatch 和 GAN+$\Pi-$model。它们在50%召回率下的精度分别为
    $\sim 86\%$ 和 $\sim 97\%$，在100%召回率下的精度分别为 $\sim 22\%$ 和 $\sim 8\%$。相比之下，我们的监督模型在100%召回率下的精度为
    $\sim 3\%$。最佳模型性能的提高主要归因于三个因素。（1）它们在训练过程中利用了数据增强（第[1表](#S3.T1 "表1 ‣ 3.2 半监督学习中的领域适应
    ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")），这有助于它们更好地进行泛化。（2）用于训练这些模型的数据集包含了模拟透镜以及
    GAN 生成的图像（第[3节](#S3 "3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")），这作为一种额外的数据增强形式。（3）这两个顶级模型都采用了半监督学习方法（MixMatch，$\Pi$-model），这使得我们的方法能够适应分布变化（第[3.2节](#S3.SS2
    "3.2 半监督学习中的领域适应 ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")）。这些结果表明，数据增强、GANs
    和半监督学习是构建高效透镜分类器的有效方法。
- en: '2.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We investigated the Grad-CAM++ feature maps (Section [6.3](#S6.SS3 "6.3 Lensing
    signatures identified by the models ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey"))
    used by our best performing models to make their predictions, finding that they
    indeed are influenced mostly by lensed arc regions and are generally not misled
    by other galaxies/artifacts (e.g., diffraction spikes) in the images. This supplements
    our results presented above that salient information regarding the arcs needed
    for classification has been successfully learned by the models through our methods.
    This is encouraging for future lens searches, since simulated Lenses used in this
    work are generated without relying on photometric data of the deflector galaxy
    (Section [4](#S4 "4 Training and Validation data ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")), making
    it simpler to automate the task of generating a training dataset.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们研究了我们表现最佳的模型在预测时使用的 Grad-CAM++ 特征图（第[6.3节](#S6.SS3 "6.3 通过模型识别的透镜特征 ‣ 6 结果与讨论
    ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")），发现这些特征图确实主要受透镜弧区域的影响，并且通常不会被图像中的其他星系/伪影（如衍射尖峰）误导。这补充了我们上面展示的结果，表明通过我们的方法，模型成功地学习了进行分类所需的关于弧的显著信息。这对未来的透镜搜索非常鼓舞人心，因为这项工作中使用的模拟透镜是在不依赖于透镜星系的光度数据的情况下生成的（第[4节](#S4
    "4 训练和验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")），这使得生成训练数据集的任务更简单。
- en: '3.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Applying the GAN+MixMatch and GAN+$\Pi$-model to the entire DLS survey, and
    visually inspecting the top $\sim 2500$ lens candidates, we find 9 Grade-A and
    13 Grade-B lensed candidates (22 in total). 3 out of the 9 Grade-A candidates
    are found within the top 17 ranked images. The number of lenses found in the DLS
    corresponds to $\sim 10\times$ higher sky density of lenses per deg² compared
    to the shallower DES/DECaLS survey imaging and supports predictions that vast
    numbers of lens systems ($\gtrsim 10^{5}$) will be detectable in the upcoming
    generation of sky surveys. We further confirmed the lensed nature of 2 Grade-A
    candidates with spectroscopy and high-resolution imaging, demonstrating that our
    methods are successful.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将GAN+MixMatch和GAN+$\Pi$-model应用于整个DLS调查，并对排名前$\sim 2500$的透镜候选者进行视觉检查，我们发现了9个A级和13个B级透镜候选者（共22个）。在这9个A级候选者中，有3个被发现位于前17名的图像中。在DLS中发现的透镜数量对应于每平方度的透镜天空密度约高出$\sim
    10\times$，与较浅的DES/DECaLS调查成像相比，这支持了预测，即在即将到来的天空调查中将能够探测到大量的透镜系统（$\gtrsim 10^{5}$）。我们进一步通过光谱学和高分辨率成像确认了2个A级候选者的透镜特性，证明了我们的方法是成功的。
- en: We have generally explored methods intended to find as many lenses as possible
    while minimizing human inspection effort. While there are likely additional detectable
    lenses beyond those we have identified, it is encouraging that our models have
    been able to identify lenses that are not represented in the training set. In
    particular, our training set focused on blue lensed arcs, while our models also
    find red arc candidates such as DLS212072337 (Section [6.3.1](#S6.SS3.SSS1 "6.3.1
    Finding red arcs ‣ 6.3 Lensing signatures identified by the models ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")), although at a lower rank compared to the bluer
    lenses. Additional augmentation methods and/or training datasets may be able to
    provide further improvement for diverse lens system properties. Another straightforward
    improvement to our lens search efficiency is to include simple cuts in color-color
    space as demonstrated in Section [6.2.3](#S6.SS2.SSS3 "6.2.3 Distribution of lensed
    candidates in color-color space ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey"). Such cuts can help increase the model precision
    by excluding sources that are not likely to act as strong lenses based on their
    color and magnitude (which is physically related to their mass and distance).
    Since our sample is agnostic to color information, our results are well-suited
    for assessing the color space distribution of the best lens candidates.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总体上探索了旨在尽可能多地找到透镜的方法，同时尽量减少人工检查工作。尽管可能还有其他可检测的透镜未被我们识别，但我们的模型能够识别训练集中未包含的透镜，这令人鼓舞。特别是，我们的训练集集中在蓝色透镜弧上，而我们的模型也找到了红色弧候选者，如DLS212072337（第[6.3.1节](#S6.SS3.SSS1
    "6.3.1 Finding red arcs ‣ 6.3 Lensing signatures identified by the models ‣ 6
    Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")），尽管排名低于蓝色透镜。额外的增强方法和/或训练数据集可能会进一步改善对不同透镜系统特性的识别。提高我们透镜搜索效率的另一个直接方法是包括简单的颜色-颜色空间切割，如第[6.2.3节](#S6.SS2.SSS3
    "6.2.3 Distribution of lensed candidates in color-color space ‣ 6.2 Catalog of
    Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")中所示。这些切割可以通过排除那些根据其颜色和亮度（与其质量和距离在物理上相关）不太可能作为强透镜的源，来帮助提高模型的精度。由于我们的样本对颜色信息无偏，因此我们的结果非常适合评估最佳透镜候选者的颜色空间分布。
- en: 'The scope of our models is currently limited to the DLS. However, our methodology
    can be adapted for other data sets, and we note that the DLS fields overlap with
    wide-area surveys such as DECaLS and SDSS. Exploring ways to translate these models
    across surveys would be greatly beneficial. Finally, confirming the lensing nature
    of new candidates either through spectroscopy (Section [6.2.2](#S6.SS2.SSS2 "6.2.2
    Spectroscopic confirmation of two Grade-A lenses ‣ 6.2 Catalog of Lens candidates
    found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey")) or via arc morphology (Section [6.4](#S6.SS4
    "6.4 Implications for future large-area sky surveys: sensitivity and angular resolution
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")) is essential for a variety of
    investigations, including probes of galaxy evolution and cosmology. We have demonstrated
    the feasibility of confirming moderately faint arcs in our sample. Accomplishing
    confirmation for the thousands of lenses that will be discovered in forthcoming
    surveys (such as with Rubin/LSST, Roman, and Euclid) will aid in our understanding
    of the formation and evolution of galaxies and the contents of the Universe.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的模型目前仅限于DLS。然而，我们的方法可以适用于其他数据集，我们注意到DLS领域与广域调查（如DECaLS和SDSS）存在重叠。探索如何将这些模型转化到不同的调查中将极具价值。最后，通过光谱学（第[6.2.2节](#S6.SS2.SSS2
    "6.2.2 Spectroscopic confirmation of two Grade-A lenses ‣ 6.2 Catalog of Lens
    candidates found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")）或弧形态学（第[6.4节](#S6.SS4
    "6.4 Implications for future large-area sky surveys: sensitivity and angular resolution
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")）确认新候选者的透镜性质对于包括探测星系演化和宇宙学在内的各种研究是至关重要的。我们已经展示了确认我们样本中适度微弱弧的可行性。完成对即将发现的数千个透镜（如Rubin/LSST、Roman和Euclid）的确认，将有助于我们理解星系的形成与演化及宇宙的内容。'
- en: Acknowledgements
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Imran Hasan, Sam Schmidt, David Wittman, Tony Tyson and Anupreeta More
    for their helpful discussions which greatly improved this work. We are immensely
    grateful to Brian Lemaux and Debora Pelliccia for manually labeling a subset of
    the survey data. We thank the referee for many helpful comments which improved
    the content and clarity of this manuscript. TJ and KVGC gratefully acknowledge
    financial support from NASA through grant HST-GO-16773, the Gordon and Betty Moore
    Foundation through Grant GBMF8549, the National Science Foundation through grant
    AST-2108515, and from a Dean’s Faculty Fellowship. Some of the data presented
    herein were obtained at the W. M. Keck Observatory, which is operated as a scientific
    partnership among the California Institute of Technology, the University of California
    and the National Aeronautics and Space Administration. The Observatory was made
    possible by the generous financial support of the W. M. Keck Foundation. The authors
    wish to recognize and acknowledge the very significant cultural role and reverence
    that the summit of Maunakea has always had within the indigenous Hawaiian community.
    We are most fortunate to have the opportunity to conduct observations from this
    mountain. Some of the results herein are based on observations with the NASA/ESA
    Hubble Space Telescope obtained from the Mikulski Archive for Space Telescopes
    at the Space Telescope Science Institute, which is operated by the Association
    of Universities for Research in Astronomy, Incorporated, under NASA contract NAS
    5-26555\. Support for program number HST-GO-16773 was provided through a grant
    from the STScI under NASA contract NAS5-26555.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Imran Hasan、Sam Schmidt、David Wittman、Tony Tyson 和 Anupreeta More 进行的有益讨论，这些讨论极大地改进了这项工作。我们非常感激
    Brian Lemaux 和 Debora Pelliccia 对部分调查数据进行的人工标记。我们感谢评审人提出的许多有益意见，这些意见提升了稿件的内容和清晰度。TJ
    和 KVGC 感谢 NASA 通过 HST-GO-16773 号资助、Gordon and Betty Moore Foundation 通过 GBMF8549
    号资助、国家科学基金会通过 AST-2108515 号资助，以及 Dean’s Faculty Fellowship 的财务支持。部分数据是在 W. M.
    Keck 天文台获得的，该天文台由加州理工学院、加州大学和国家航空航天局合作运营。天文台的建立得到了 W. M. Keck 基金会的慷慨财务支持。作者希望认识并感谢
    Maunakea 顶峰在夏威夷原住民社区中始终扮演的重要文化角色和尊崇。我们非常幸运能够在这座山上进行观测。部分结果基于 NASA/ESA 哈勃太空望远镜的观测数据，这些数据来自空间望远镜科学研究所的
    Mikulski 太空望远镜档案馆，该机构由大学天文学研究协会运营，受 NASA 合同 NAS 5-26555 监管。HST-GO-16773 号计划的支持由
    STScI 通过 NASA 合同 NAS5-26555 提供。
- en: Data availability
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据可用性
- en: The data underlying this article are available on our GitHub repository ([https://github.com/sxsheng/SHLDN](https://github.com/sxsheng/SHLDN)).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的数据可以在我们的 GitHub 仓库中找到（[https://github.com/sxsheng/SHLDN](https://github.com/sxsheng/SHLDN)）。
- en: References
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Adebayo et al. (2018) Adebayo J., Gilmer J., Muelly M., Goodfellow I., Hardt
    M., Kim B., 2018, in Proceedings of the 32nd International Conference on Neural
    Information Processing Systems. NIPS’18. Curran Associates Inc., Red Hook, NY,
    USA, p. 9525–9536
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adebayo 等（2018）Adebayo J., Gilmer J., Muelly M., Goodfellow I., Hardt M., Kim
    B., 2018年，《第32届国际神经信息处理系统会议论文集》。NIPS’18。Curran Associates Inc.，纽约红钩，USA，第9525–9536页
- en: Alard (2006) Alard C., 2006, arXiv e-prints, [pp astro–ph/0606757](https://ui.adsabs.harvard.edu/abs/2006astro.ph..6757A)
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alard（2006）Alard C., 2006年，arXiv e-prints，[pp astro–ph/0606757](https://ui.adsabs.harvard.edu/abs/2006astro.ph..6757A)
- en: Arjovsky et al. (2017) Arjovsky M., Chintala S., Bottou L., 2017, in Precup
    D., Teh Y. W., eds, Proceedings of Machine Learning Research Vol. 70, Proceedings
    of the 34th International Conference on Machine Learning. PMLR, International
    Convention Centre, Sydney, Australia, pp 214–223, [http://proceedings.mlr.press/v70/arjovsky17a.html](http://proceedings.mlr.press/v70/arjovsky17a.html)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky 等（2017）Arjovsky M., Chintala S., Bottou L., 2017年，在 Precup D., Teh
    Y. W.（编辑），《机器学习研究论文集》第70卷，第34届国际机器学习会议论文集。PMLR，澳大利亚悉尼国际会议中心，第214–223页，[http://proceedings.mlr.press/v70/arjovsky17a.html](http://proceedings.mlr.press/v70/arjovsky17a.html)
- en: Ascaso et al. (2014) Ascaso B., Wittman D., Dawson W., 2014, [MNRAS](http://dx.doi.org/10.1093/mnras/stu074),
    [439, 1980](https://ui.adsabs.harvard.edu/abs/2014MNRAS.439.1980A)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ascaso 等（2014）Ascaso B., Wittman D., Dawson W., 2014年，[MNRAS](http://dx.doi.org/10.1093/mnras/stu074)，[439,
    1980](https://ui.adsabs.harvard.edu/abs/2014MNRAS.439.1980A)
- en: Belokurov et al. (2009) Belokurov V., Evans N. W., Hewett P. C., Moiseev A.,
    McMahon R. G., Sanchez S. F., King L. J., 2009, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2008.14075.x),
    [392, 104](https://ui.adsabs.harvard.edu/abs/2009MNRAS.392..104B)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belokurov et al. (2009) Belokurov V., Evans N. W., Hewett P. C., Moiseev A.,
    McMahon R. G., Sanchez S. F., King L. J., 2009, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2008.14075.x),
    [392, 104](https://ui.adsabs.harvard.edu/abs/2009MNRAS.392..104B)
- en: Berthelot et al. (2019) Berthelot D., Carlini N., Goodfellow I., Papernot N.,
    Oliver A., Raffel C. A., 2019, in Wallach H., Larochelle H., Beygelzimer A., d'Alché-Buc
    F., Fox E., Garnett R., eds, , Advances in Neural Information Processing Systems
    32. Curran Associates, Inc., pp 5049–5059
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berthelot et al. (2019) Berthelot D., Carlini N., Goodfellow I., Papernot N.,
    Oliver A., Raffel C. A., 2019, 在 Wallach H., Larochelle H., Beygelzimer A., d'Alché-Buc
    F., Fox E., Garnett R., 主编的《神经信息处理系统进展 32》中，Curran Associates, Inc., pp 5049–5059
- en: Berthelot et al. (2021) Berthelot D., Roelofs R., Sohn K., Carlini N., Kurakin
    A., 2021, arXiv preprint arXiv:2106.04732
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berthelot et al. (2021) Berthelot D., Roelofs R., Sohn K., Carlini N., Kurakin
    A., 2021, arXiv 预印本 arXiv:2106.04732
- en: Bertin & Arnouts (1996) Bertin E., Arnouts S., 1996, [A&AS](http://dx.doi.org/10.1051/aas:1996164),
    [117, 393](https://ui.adsabs.harvard.edu/abs/1996A&AS..117..393B)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertin & Arnouts (1996) Bertin E., Arnouts S., 1996, [A&AS](http://dx.doi.org/10.1051/aas:1996164),
    [117, 393](https://ui.adsabs.harvard.edu/abs/1996A&AS..117..393B)
- en: Bolton et al. (2008) Bolton A. S., Burles S., Koopmans L. V. E., Treu T., Gavazzi
    R., Moustakas L. A., Wayth R., Schlegel D. J., 2008, [ApJ](http://dx.doi.org/10.1086/589327),
    [682, 964](https://ui.adsabs.harvard.edu/abs/2008ApJ...682..964B)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolton et al. (2008) Bolton A. S., Burles S., Koopmans L. V. E., Treu T., Gavazzi
    R., Moustakas L. A., Wayth R., Schlegel D. J., 2008, [ApJ](http://dx.doi.org/10.1086/589327),
    [682, 964](https://ui.adsabs.harvard.edu/abs/2008ApJ...682..964B)
- en: Bradač et al. (2002) Bradač M., Schneider P., Steinmetz M., Lombardi M., King
    L. J., Porcas R., 2002, [A&A](http://dx.doi.org/10.1051/0004-6361:20020559), [388,
    373](https://ui.adsabs.harvard.edu/abs/2002A&A...388..373B)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradač et al. (2002) Bradač M., Schneider P., Steinmetz M., Lombardi M., King
    L. J., Porcas R., 2002, [A&A](http://dx.doi.org/10.1051/0004-6361:20020559), [388,
    373](https://ui.adsabs.harvard.edu/abs/2002A&A...388..373B)
- en: Cañameras et al. (2020) Cañameras R., et al., 2020, [A&A](http://dx.doi.org/10.1051/0004-6361/202038219),
    [644, A163](https://ui.adsabs.harvard.edu/abs/2020A&A...644A.163C)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cañameras et al. (2020) Cañameras R., 等，2020, [A&A](http://dx.doi.org/10.1051/0004-6361/202038219),
    [644, A163](https://ui.adsabs.harvard.edu/abs/2020A&A...644A.163C)
- en: Chattopadhay et al. (2018) Chattopadhay A., Sarkar A., Howlader P., Balasubramanian
    V. N., 2018, in 2018 IEEE Winter Conference on Applications of Computer Vision
    (WACV). pp 839–847, [doi:10.1109/WACV.2018.00097](http://dx.doi.org/10.1109/WACV.2018.00097)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chattopadhay et al. (2018) Chattopadhay A., Sarkar A., Howlader P., Balasubramanian
    V. N., 2018, 在 2018 IEEE Winter Conference on Applications of Computer Vision
    (WACV) 中，pp 839–847, [doi:10.1109/WACV.2018.00097](http://dx.doi.org/10.1109/WACV.2018.00097)
- en: Chiba (2002) Chiba M., 2002, [ApJ](http://dx.doi.org/10.1086/324493), [565,
    17](https://ui.adsabs.harvard.edu/abs/2002ApJ...565...17C)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiba (2002) Chiba M., 2002, [ApJ](http://dx.doi.org/10.1086/324493), [565,
    17](https://ui.adsabs.harvard.edu/abs/2002ApJ...565...17C)
- en: Collett (2015) Collett T. E., 2015, [ApJ](http://dx.doi.org/10.1088/0004-637X/811/1/20),
    [811, 20](https://ui.adsabs.harvard.edu/abs/2015ApJ...811...20C)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collett (2015) Collett T. E., 2015, [ApJ](http://dx.doi.org/10.1088/0004-637X/811/1/20),
    [811, 20](https://ui.adsabs.harvard.edu/abs/2015ApJ...811...20C)
- en: Diehl et al. (2009) Diehl H. T., et al., 2009, [ApJ](http://dx.doi.org/10.1088/0004-637X/707/1/686),
    [707, 686](https://ui.adsabs.harvard.edu/abs/2009ApJ...707..686D)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diehl et al. (2009) Diehl H. T., 等，2009, [ApJ](http://dx.doi.org/10.1088/0004-637X/707/1/686),
    [707, 686](https://ui.adsabs.harvard.edu/abs/2009ApJ...707..686D)
- en: Erhan et al. (2010) Erhan D., Courville A., Bengio Y., Vincent P., 2010, in
    Teh Y. W., Titterington M., eds, Proceedings of Machine Learning Research Vol.
    9, Proceedings of the Thirteenth International Conference on Artificial Intelligence
    and Statistics. PMLR, Chia Laguna Resort, Sardinia, Italy, pp 201–208, [http://proceedings.mlr.press/v9/erhan10a.html](http://proceedings.mlr.press/v9/erhan10a.html)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erhan et al. (2010) Erhan D., Courville A., Bengio Y., Vincent P., 2010, 在 Teh
    Y. W., Titterington M., 主编的《机器学习研究会议论文集 第 9 卷》中，第十三届人工智能与统计国际会议的论文。PMLR, Chia
    Laguna Resort, Sardinia, Italy, pp 201–208, [http://proceedings.mlr.press/v9/erhan10a.html](http://proceedings.mlr.press/v9/erhan10a.html)
- en: Fassnacht et al. (2004) Fassnacht C. D., Moustakas L. A., Casertano S., Ferguson
    H. C., Lucas R. A., Park Y., 2004, [ApJ](http://dx.doi.org/10.1086/379004), [600,
    L155](https://ui.adsabs.harvard.edu/abs/2004ApJ...600L.155F)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fassnacht et al. (2004) Fassnacht C. D., Moustakas L. A., Casertano S., Ferguson
    H. C., Lucas R. A., Park Y., 2004, [ApJ](http://dx.doi.org/10.1086/379004), [600,
    L155](https://ui.adsabs.harvard.edu/abs/2004ApJ...600L.155F)
- en: Fernando et al. (2019) Fernando Z. T., Singh J., Anand A., 2019, in Proceedings
    of the 42nd International ACM SIGIR Conference on Research and Development in
    Information Retrieval. ACM, [doi:10.1145/3331184.3331312](http://dx.doi.org/10.1145/3331184.3331312),
    [https://doi.org/10.1145%2F3331184.3331312](https://doi.org/10.1145%2F3331184.3331312)
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernando et al. (2019) Fernando Z. T., Singh J., Anand A., 2019, 在第42届国际 ACM
    SIGIR 信息检索研究与发展会议论文集. ACM, [doi:10.1145/3331184.3331312](http://dx.doi.org/10.1145/3331184.3331312),
    [https://doi.org/10.1145%2F3331184.3331312](https://doi.org/10.1145%2F3331184.3331312)
- en: Garvin et al. (2022) Garvin E. O., Kruk S., Cornen C., Bhatawdekar R., Cañameras
    R., Merín B., 2022, arXiv e-prints, [p. arXiv:2207.06997](https://ui.adsabs.harvard.edu/abs/2022arXiv220706997G)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garvin et al. (2022) Garvin E. O., Kruk S., Cornen C., Bhatawdekar R., Cañameras
    R., Merín B., 2022, arXiv 预印本, [p. arXiv:2207.06997](https://ui.adsabs.harvard.edu/abs/2022arXiv220706997G)
- en: Gavazzi et al. (2014) Gavazzi R., Marshall P. J., Treu T., Sonnenfeld A., 2014,
    [ApJ](http://dx.doi.org/10.1088/0004-637X/785/2/144), [785, 144](https://ui.adsabs.harvard.edu/abs/2014ApJ...785..144G)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gavazzi et al. (2014) Gavazzi R., Marshall P. J., Treu T., Sonnenfeld A., 2014,
    [ApJ](http://dx.doi.org/10.1088/0004-637X/785/2/144), [785, 144](https://ui.adsabs.harvard.edu/abs/2014ApJ...785..144G)
- en: Gilman et al. (2019) Gilman D., Birrer S., Treu T., Nierenberg A., Benson A.,
    2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz1593), [487, 5721](https://ui.adsabs.harvard.edu/abs/2019MNRAS.487.5721G)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gilman et al. (2019) Gilman D., Birrer S., Treu T., Nierenberg A., Benson A.,
    2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz1593), [487, 5721](https://ui.adsabs.harvard.edu/abs/2019MNRAS.487.5721G)
- en: Goodfellow et al. (2014) Goodfellow I. J., Pouget-Abadie J., Mirza M., Xu B.,
    Warde-Farley D., Ozair S., Courville A., Bengio Y., 2014, in Proceedings of the
    27th International Conference on Neural Information Processing Systems - Volume
    2. NIPS’14. MIT Press, Cambridge, MA, USA, p. 2672–2680
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014) Goodfellow I. J., Pouget-Abadie J., Mirza M., Xu B.,
    Warde-Farley D., Ozair S., Courville A., Bengio Y., 2014, 在第27届神经信息处理系统国际会议论文集
    - 第二卷. NIPS’14. MIT Press, Cambridge, MA, USA, p. 2672–2680
- en: Gu et al. (2018) Gu J., et al., 2018, [Pattern Recognition](http://dx.doi.org/https://doi.org/10.1016/j.patcog.2017.10.013),
    77, 354
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2018) Gu J., 等, 2018, [模式识别](http://dx.doi.org/https://doi.org/10.1016/j.patcog.2017.10.013),
    77, 354
- en: Gulrajani et al. (2017) Gulrajani I., Ahmed F., Arjovsky M., Dumoulin V., Courville
    A. C., 2017, in Guyon I., Luxburg U. V., Bengio S., Wallach H., Fergus R., Vishwanathan
    S., Garnett R., eds,   Vol. 30, Advances in Neural Information Processing Systems.
    Curran Associates, Inc., [https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf)
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulrajani et al. (2017) Gulrajani I., Ahmed F., Arjovsky M., Dumoulin V., Courville
    A. C., 2017, 在 Guyon I., Luxburg U. V., Bengio S., Wallach H., Fergus R., Vishwanathan
    S., Garnett R., 编,   第30卷, 神经信息处理系统进展. Curran Associates, Inc., [https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf)
- en: He et al. (2016a) He K., Zhang X., Ren S., Sun J., 2016a, in European conference
    on computer vision. pp 630–645
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016a) He K., Zhang X., Ren S., Sun J., 2016a, 在欧洲计算机视觉会议论文集. pp
    630–645
- en: He et al. (2016b) He K., Zhang X., Ren S., Sun J., 2016b, in Proceedings of
    the IEEE conference on computer vision and pattern recognition. pp 770–778
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016b) He K., Zhang X., Ren S., Sun J., 2016b, 在 IEEE 计算机视觉与模式识别会议论文集.
    pp 770–778
- en: Huang et al. (2020) Huang X., et al., 2020, [The Astrophysical Journal](http://dx.doi.org/10.3847/1538-4357/ab7ffb),
    894, 78
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2020) Huang X., 等, 2020, [天体物理学杂志](http://dx.doi.org/10.3847/1538-4357/ab7ffb),
    894, 78
- en: Ioffe & Szegedy (2015) Ioffe S., Szegedy C., 2015, in International conference
    on machine learning. pp 448–456
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe & Szegedy (2015) Ioffe S., Szegedy C., 2015, 在国际机器学习会议论文集. pp 448–456
- en: Ivezić et al. (2019) Ivezić Ž., et al., 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivezić et al. (2019) Ivezić Ž., 等, 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
- en: Jacobs et al. (2017) Jacobs C., Glazebrook K., Collett T., More A., McCarthy
    C., 2017, [MNRAS](http://dx.doi.org/10.1093/mnras/stx1492), [471, 167](https://ui.adsabs.harvard.edu/abs/2017MNRAS.471..167J)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacobs et al. (2017) Jacobs C., Glazebrook K., Collett T., More A., McCarthy
    C., 2017, [MNRAS](http://dx.doi.org/10.1093/mnras/stx1492), [471, 167](https://ui.adsabs.harvard.edu/abs/2017MNRAS.471..167J)
- en: Jacobs et al. (2019) Jacobs C., et al., 2019, [ApJS](http://dx.doi.org/10.3847/1538-4365/ab26b6),
    [243, 17](https://ui.adsabs.harvard.edu/abs/2019ApJS..243...17J)
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacobs et al. (2019) Jacobs C., 等, 2019, [ApJS](http://dx.doi.org/10.3847/1538-4365/ab26b6),
    [243, 17](https://ui.adsabs.harvard.edu/abs/2019ApJS..243...17J)
- en: Kingma & Welling (2014) Kingma D. P., Welling M., 2014, Auto-Encoding Variational
    Bayes ([arXiv:1312.6114](http://arxiv.org/abs/1312.6114))
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Welling (2014) Kingma D. P., Welling M., 2014, 自编码变分贝叶斯 ([arXiv:1312.6114](http://arxiv.org/abs/1312.6114))
- en: Koekemoer et al. (2007) Koekemoer A. M., et al., 2007, [ApJS](http://dx.doi.org/10.1086/520086),
    [172, 196](https://ui.adsabs.harvard.edu/abs/2007ApJS..172..196K)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koekemoer 等（2007）Koekemoer A. M., 等，2007, [ApJS](http://dx.doi.org/10.1086/520086),
    [172, 196](https://ui.adsabs.harvard.edu/abs/2007ApJS..172..196K)
- en: Koopmans et al. (2006) Koopmans L. V. E., Treu T., Bolton A. S., Burles S.,
    Moustakas L. A., 2006, [ApJ](http://dx.doi.org/10.1086/505696), [649, 599](https://ui.adsabs.harvard.edu/abs/2006ApJ...649..599K)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koopmans 等（2006）Koopmans L. V. E., Treu T., Bolton A. S., Burles S., Moustakas
    L. A., 2006, [ApJ](http://dx.doi.org/10.1086/505696), [649, 599](https://ui.adsabs.harvard.edu/abs/2006ApJ...649..599K)
- en: Kormann et al. (1994) Kormann R., Schneider P., Bartelmann M., 1994, A&A, [284,
    285](https://ui.adsabs.harvard.edu/abs/1994A&A...284..285K)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kormann 等（1994）Kormann R., Schneider P., Bartelmann M., 1994, A&A, [284, 285](https://ui.adsabs.harvard.edu/abs/1994A&A...284..285K)
- en: Krizhevsky (2009a) Krizhevsky A., 2009a, Technical report, Learning multiple
    layers of features from tiny images
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky（2009a）Krizhevsky A., 2009a, 技术报告，《从微小图像中学习多个特征层》
- en: Krizhevsky (2009b) Krizhevsky A., 2009b.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky（2009b）Krizhevsky A., 2009b.
- en: Krizhevsky et al. (2012) Krizhevsky A., Sutskever I., Hinton G. E., 2012, in
    Pereira F., Burges C. J. C., Bottou L., Weinberger K. Q., eds,   Vol. 25, Advances
    in Neural Information Processing Systems. Curran Associates, Inc., [https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等（2012）Krizhevsky A., Sutskever I., Hinton G. E., 2012, 在 Pereira
    F., Burges C. J. C., Bottou L., Weinberger K. Q., 编著，Vol. 25, 《神经信息处理系统进展》。Curran
    Associates, Inc., [https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- en: Kubo & Dell’Antonio (2008) Kubo J. M., Dell’Antonio I. P., 2008, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2008.12880.x),
    [385, 918](https://ui.adsabs.harvard.edu/abs/2008MNRAS.385..918K)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubo & Dell’Antonio（2008）Kubo J. M., Dell’Antonio I. P., 2008, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2008.12880.x),
    [385, 918](https://ui.adsabs.harvard.edu/abs/2008MNRAS.385..918K)
- en: LSST Science Collaboration et al. (2009) LSST Science Collaboration et al.,
    2009, arXiv e-prints, [p. arXiv:0912.0201](https://ui.adsabs.harvard.edu/abs/2009arXiv0912.0201L)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSST 科学合作组 等（2009）LSST 科学合作组 等，2009, arXiv 电子预印本, [p. arXiv:0912.0201](https://ui.adsabs.harvard.edu/abs/2009arXiv0912.0201L)
- en: Laine & Aila (2017) Laine S., Aila T., 2017, ArXiv, abs/1610.02242
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laine & Aila（2017）Laine S., Aila T., 2017, ArXiv, abs/1610.02242
- en: Laureijs et al. (2011) Laureijs R., et al., 2011, arXiv e-prints, [p. arXiv:1110.3193](https://ui.adsabs.harvard.edu/abs/2011arXiv1110.3193L)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laureijs 等（2011）Laureijs R., 等，2011, arXiv 电子预印本, [p. arXiv:1110.3193](https://ui.adsabs.harvard.edu/abs/2011arXiv1110.3193L)
- en: LeCun et al. (1989) LeCun Y., Boser B., Denker J. S., Henderson D., Howard R. E.,
    Hubbard W., Jackel L. D., 1989, [Neural Computation](http://dx.doi.org/10.1162/neco.1989.1.4.541),
    1, 541
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等（1989）LeCun Y., Boser B., Denker J. S., Henderson D., Howard R. E., Hubbard
    W., Jackel L. D., 1989, [Neural Computation](http://dx.doi.org/10.1162/neco.1989.1.4.541),
    1, 541
- en: 'Lee (2013) Lee D.-H., 2013, ICML 2013 Workshop : Challenges in Representation
    Learning (WREPL)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee（2013）Lee D.-H., 2013, ICML 2013 工作坊：表示学习中的挑战（WREPL）
- en: Leethochawalit et al. (2016) Leethochawalit N., Jones T. A., Ellis R. S., Stark
    D. P., Richard J., Zitrin A., Auger M., 2016, [ApJ](http://dx.doi.org/10.3847/0004-637X/820/2/84),
    [820, 84](https://ui.adsabs.harvard.edu/abs/2016ApJ...820...84L)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leethochawalit 等（2016）Leethochawalit N., Jones T. A., Ellis R. S., Stark D.
    P., Richard J., Zitrin A., Auger M., 2016, [ApJ](http://dx.doi.org/10.3847/0004-637X/820/2/84),
    [820, 84](https://ui.adsabs.harvard.edu/abs/2016ApJ...820...84L)
- en: Li et al. (2020) Li R., et al., 2020, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab9dfa),
    [899, 30](https://ui.adsabs.harvard.edu/abs/2020ApJ...899...30L)
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）Li R., 等，2020, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab9dfa),
    [899, 30](https://ui.adsabs.harvard.edu/abs/2020ApJ...899...30L)
- en: Litjens et al. (2017) Litjens G., et al., 2017, [Medical Image Analysis](http://dx.doi.org/https://doi.org/10.1016/j.media.2017.07.005),
    42, 60
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Litjens 等（2017）Litjens G., 等，2017, [Medical Image Analysis](http://dx.doi.org/https://doi.org/10.1016/j.media.2017.07.005),
    42, 60
- en: Lupton et al. (2004) Lupton R., Blanton M. R., Fekete G., Hogg D. W., O’Mullane
    W., Szalay A., Wherry N., 2004, [PASP](http://dx.doi.org/10.1086/382245), [116,
    133](https://ui.adsabs.harvard.edu/abs/2004PASP..116..133L)
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lupton 等（2004）Lupton R., Blanton M. R., Fekete G., Hogg D. W., O’Mullane W.,
    Szalay A., Wherry N., 2004, [PASP](http://dx.doi.org/10.1086/382245), [116, 133](https://ui.adsabs.harvard.edu/abs/2004PASP..116..133L)
- en: Madireddy et al. (2019) Madireddy S., Li N., Ramachandra N., Butler J., Balaprakash
    P., Habib S., Heitmann K., 2019, arXiv e-prints, [p. arXiv:1911.03867](https://ui.adsabs.harvard.edu/abs/2019arXiv191103867M)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madireddy 等（2019）Madireddy S., Li N., Ramachandra N., Butler J., Balaprakash
    P., Habib S., Heitmann K., 2019, arXiv 电子预印本, [p. arXiv:1911.03867](https://ui.adsabs.harvard.edu/abs/2019arXiv191103867M)
- en: 'Marshall et al. (2015) Marshall P., Sandford C., More A., Buddelmeijerr H.,
    2015, HumVI: Human Viewable Image creation (ascl:1511.014)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marshall 等人（2015）Marshall P., Sandford C., More A., Buddelmeijerr H., 2015,
    HumVI: Human Viewable Image creation (ascl:1511.014)'
- en: Miranda & Macciò (2007) Miranda M., Macciò A. V., 2007, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2007.12440.x),
    [382, 1225](https://ui.adsabs.harvard.edu/abs/2007MNRAS.382.1225M)
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miranda & Macciò（2007）Miranda M., Macciò A. V., 2007, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2007.12440.x),
    [382, 1225](https://ui.adsabs.harvard.edu/abs/2007MNRAS.382.1225M)
- en: Miyato et al. (2019) Miyato T., Maeda S., Koyama M., Ishii S., 2019, IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 41, 1979
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miyato 等人（2019）Miyato T., Maeda S., Koyama M., Ishii S., 2019, IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 41, 1979
- en: More et al. (2016) More A., et al., 2016, [MNRAS](http://dx.doi.org/10.1093/mnras/stv1966),
    [455, 1191](https://ui.adsabs.harvard.edu/abs/2016MNRAS.455.1191M)
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: More 等人（2016）More A., 等人, 2016, [MNRAS](http://dx.doi.org/10.1093/mnras/stv1966),
    [455, 1191](https://ui.adsabs.harvard.edu/abs/2016MNRAS.455.1191M)
- en: Moustakas et al. (2007) Moustakas L. A., et al., 2007, [ApJ](http://dx.doi.org/10.1086/517930),
    [660, L31](https://ui.adsabs.harvard.edu/abs/2007ApJ...660L..31M)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moustakas 等人（2007）Moustakas L. A., 等人, 2007, [ApJ](http://dx.doi.org/10.1086/517930),
    [660, L31](https://ui.adsabs.harvard.edu/abs/2007ApJ...660L..31M)
- en: Netzer et al. (2011) Netzer Y., Wang T., Coates A., Bissacco A., Wu B., Ng A. Y.,
    2011, in NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011\.
    [http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf](http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Netzer 等人（2011）Netzer Y., Wang T., Coates A., Bissacco A., Wu B., Ng A. Y.,
    2011, 见于NIPS深度学习与无监督特征学习研讨会 2011\. [http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf](http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf)
- en: Oguri (2010) Oguri M., 2010, [PASJ](http://dx.doi.org/10.1093/pasj/62.4.1017),
    [62, 1017](https://ui.adsabs.harvard.edu/abs/2010PASJ...62.1017O)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oguri（2010）Oguri M., 2010, [PASJ](http://dx.doi.org/10.1093/pasj/62.4.1017),
    [62, 1017](https://ui.adsabs.harvard.edu/abs/2010PASJ...62.1017O)
- en: Oguri & Marshall (2010) Oguri M., Marshall P. J., 2010, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2010.16639.x),
    [405, 2579](https://ui.adsabs.harvard.edu/abs/2010MNRAS.405.2579O)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oguri & Marshall（2010）Oguri M., Marshall P. J., 2010, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2010.16639.x),
    [405, 2579](https://ui.adsabs.harvard.edu/abs/2010MNRAS.405.2579O)
- en: Paraficz et al. (2016) Paraficz D., et al., 2016, [A&A](http://dx.doi.org/10.1051/0004-6361/201527971),
    [592, A75](https://ui.adsabs.harvard.edu/abs/2016A&A...592A..75P)
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paraficz 等人（2016）Paraficz D., 等人, 2016, [A&A](http://dx.doi.org/10.1051/0004-6361/201527971),
    [592, A75](https://ui.adsabs.harvard.edu/abs/2016A&A...592A..75P)
- en: Pettini et al. (2002) Pettini M., Rix S. A., Steidel C. C., Adelberger K. L.,
    Hunt M. P., Shapley A. E., 2002, [ApJ](http://dx.doi.org/10.1086/339355), [569,
    742](https://ui.adsabs.harvard.edu/abs/2002ApJ...569..742P)
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pettini 等人（2002）Pettini M., Rix S. A., Steidel C. C., Adelberger K. L., Hunt
    M. P., Shapley A. E., 2002, [ApJ](http://dx.doi.org/10.1086/339355), [569, 742](https://ui.adsabs.harvard.edu/abs/2002ApJ...569..742P)
- en: Pourrahmani et al. (2018) Pourrahmani M., Nayyeri H., Cooray A., 2018, [ApJ](http://dx.doi.org/10.3847/1538-4357/aaae6a),
    [856, 68](https://ui.adsabs.harvard.edu/abs/2018ApJ...856...68P)
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pourrahmani 等人（2018）Pourrahmani M., Nayyeri H., Cooray A., 2018, [ApJ](http://dx.doi.org/10.3847/1538-4357/aaae6a),
    [856, 68](https://ui.adsabs.harvard.edu/abs/2018ApJ...856...68P)
- en: Quinonero-Candela et al. (2008) Quinonero-Candela J., Sugiyama M., Schwaighofer
    A., Lawrence N. D., 2008, Dataset shift in machine learning. Mit Press
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quinonero-Candela 等人（2008）Quinonero-Candela J., Sugiyama M., Schwaighofer A.,
    Lawrence N. D., 2008, 机器学习中的数据集偏移。MIT出版社
- en: Schmidt & Thorman (2013) Schmidt S. J., Thorman P., 2013, [MNRAS](http://dx.doi.org/10.1093/mnras/stt373),
    [431, 2766](https://ui.adsabs.harvard.edu/abs/2013MNRAS.431.2766S)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidt & Thorman（2013）Schmidt S. J., Thorman P., 2013, [MNRAS](http://dx.doi.org/10.1093/mnras/stt373),
    [431, 2766](https://ui.adsabs.harvard.edu/abs/2013MNRAS.431.2766S)
- en: Seidel & Bartelmann (2007) Seidel G., Bartelmann M., 2007, [A&A](http://dx.doi.org/10.1051/0004-6361:20066097),
    [472, 341](https://ui.adsabs.harvard.edu/abs/2007A&A...472..341S)
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seidel & Bartelmann（2007）Seidel G., Bartelmann M., 2007, [A&A](http://dx.doi.org/10.1051/0004-6361:20066097),
    [472, 341](https://ui.adsabs.harvard.edu/abs/2007A&A...472..341S)
- en: Selvaraju et al. (2017) Selvaraju R. R., Cogswell M., Das A., Vedantam R., Parikh
    D., Batra D., 2017, in Proceedings of the IEEE International Conference on Computer
    Vision (ICCV).
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Selvaraju 等人（2017）Selvaraju R. R., Cogswell M., Das A., Vedantam R., Parikh
    D., Batra D., 2017, 见于《IEEE计算机视觉国际会议论文集》（ICCV）。
- en: Shajib et al. (2022) Shajib A. J., et al., 2022, arXiv e-prints, [p. arXiv:2210.10790](https://ui.adsabs.harvard.edu/abs/2022arXiv221010790S)
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shajib 等人（2022）Shajib A. J., 等人, 2022, arXiv 预印本, [p. arXiv:2210.10790](https://ui.adsabs.harvard.edu/abs/2022arXiv221010790S)
- en: Sheng et al. (2022) Sheng S., C K. V. G., Choi C. P., Sharpnack J., Jones T.,
    2022, [arXiv e-prints](http://dx.doi.org/10.48550/arXiv.2210.11681), [p. arXiv:2210.11681](https://ui.adsabs.harvard.edu/abs/2022arXiv221011681S)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng 等人（2022）Sheng S., C K. V. G., Choi C. P., Sharpnack J., Jones T., 2022,
    [arXiv e-prints](http://dx.doi.org/10.48550/arXiv.2210.11681), [p. arXiv:2210.11681](https://ui.adsabs.harvard.edu/abs/2022arXiv221011681S)
- en: Sohn et al. (2020) Sohn K., et al., 2020, in Larochelle H., Ranzato M., Hadsell
    R., Balcan M., Lin H., eds,   Vol. 33, Advances in Neural Information Processing
    Systems. Curran Associates, Inc., pp 596–608, [https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf)
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohn 等人（2020）Sohn K., 等人，2020年，在 Larochelle H., Ranzato M., Hadsell R., Balcan
    M., Lin H., 编，卷 33，《神经信息处理系统进展》。Curran Associates, Inc., 页 596–608, [https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf)
- en: Sonnenfeld et al. (2013) Sonnenfeld A., Treu T., Gavazzi R., Suyu S. H., Marshall
    P. J., Auger M. W., Nipoti C., 2013, [ApJ](http://dx.doi.org/10.1088/0004-637X/777/2/98),
    [777, 98](https://ui.adsabs.harvard.edu/abs/2013ApJ...777...98S)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sonnenfeld 等人（2013）Sonnenfeld A., Treu T., Gavazzi R., Suyu S. H., Marshall
    P. J., Auger M. W., Nipoti C., 2013年，[ApJ](http://dx.doi.org/10.1088/0004-637X/777/2/98),
    [777, 98](https://ui.adsabs.harvard.edu/abs/2013ApJ...777...98S)
- en: Sonnenfeld et al. (2018) Sonnenfeld A., et al., 2018, [PASJ](http://dx.doi.org/10.1093/pasj/psx062),
    [70, S29](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..29S)
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sonnenfeld 等人（2018）Sonnenfeld A., 等人，2018年，[PASJ](http://dx.doi.org/10.1093/pasj/psx062),
    [70, S29](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..29S)
- en: Spergel et al. (2015) Spergel D., et al., 2015, arXiv e-prints, [p. arXiv:1503.03757](https://ui.adsabs.harvard.edu/abs/2015arXiv150303757S)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spergel 等人（2015）Spergel D., 等人，2015年，arXiv e-prints, [p. arXiv:1503.03757](https://ui.adsabs.harvard.edu/abs/2015arXiv150303757S)
- en: Springenberg et al. (2015) Springenberg J. T., Dosovitskiy A., Brox T., Riedmiller
    M. A., 2015, CoRR, abs/1412.6806
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Springenberg 等人（2015）Springenberg J. T., Dosovitskiy A., Brox T., Riedmiller
    M. A., 2015年，CoRR, abs/1412.6806
- en: Stein et al. (2021) Stein G., Harrington P., Blaum J., Medan T., Lukic Z., 2021,
    arXiv e-prints, [p. arXiv:2110.13151](https://ui.adsabs.harvard.edu/abs/2021arXiv211013151S)
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stein 等人（2021）Stein G., Harrington P., Blaum J., Medan T., Lukic Z., 2021年，arXiv
    e-prints, [p. arXiv:2110.13151](https://ui.adsabs.harvard.edu/abs/2021arXiv211013151S)
- en: Swinbank et al. (2009) Swinbank A. M., et al., 2009, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2009.15617.x),
    [400, 1121](https://ui.adsabs.harvard.edu/abs/2009MNRAS.400.1121S)
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swinbank 等人（2009）Swinbank A. M., 等人，2009年，[MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2009.15617.x),
    [400, 1121](https://ui.adsabs.harvard.edu/abs/2009MNRAS.400.1121S)
- en: Tarvainen & Valpola (2017) Tarvainen A., Valpola H., 2017, in Guyon I., Luxburg
    U. V., Bengio S., Wallach H., Fergus R., Vishwanathan S., Garnett R., eds, , Advances
    in Neural Information Processing Systems 30. Curran Associates, Inc., pp 1195–1204
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarvainen & Valpola（2017）Tarvainen A., Valpola H., 2017年，在 Guyon I., Luxburg
    U. V., Bengio S., Wallach H., Fergus R., Vishwanathan S., Garnett R., 编，《神经信息处理系统进展》第30卷。Curran
    Associates, Inc., 页 1195–1204
- en: Tran et al. (2022) Tran K.-V. H., et al., 2022, arXiv e-prints, [p. arXiv:2205.05307](https://ui.adsabs.harvard.edu/abs/2022arXiv220505307T)
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等人（2022）Tran K.-V. H., 等人，2022年，arXiv e-prints, [p. arXiv:2205.05307](https://ui.adsabs.harvard.edu/abs/2022arXiv220505307T)
- en: Treu (2010) Treu T., 2010, [ARA&A](http://dx.doi.org/10.1146/annurev-astro-081309-130924),
    [48, 87](https://ui.adsabs.harvard.edu/abs/2010ARA&A..48...87T)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Treu（2010）Treu T., 2010年，[ARA&A](http://dx.doi.org/10.1146/annurev-astro-081309-130924),
    [48, 87](https://ui.adsabs.harvard.edu/abs/2010ARA&A..48...87T)
- en: Wilson et al. (2004) Wilson J. C., et al., 2004, in Moorwood A. F. M., Iye M.,
    eds, Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series
    Vol. 5492, Ground-based Instrumentation for Astronomy. pp 1295–1305, [doi:10.1117/12.550925](http://dx.doi.org/10.1117/12.550925)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilson 等人（2004）Wilson J. C., 等人，2004年，在 Moorwood A. F. M., Iye M., 编，《光学工程学会（SPIE）会议系列》第5492卷，地面观测天文学仪器。页
    1295–1305, [doi:10.1117/12.550925](http://dx.doi.org/10.1117/12.550925)
- en: Wittman et al. (2002) Wittman D. M., et al., 2002, in Tyson J. A., Wolff S.,
    eds, Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series
    Vol. 4836, Survey and Other Telescope Technologies and Discoveries. pp 73–82 ([arXiv:astro-ph/0210118](http://arxiv.org/abs/astro-ph/0210118)),
    [doi:10.1117/12.457348](http://dx.doi.org/10.1117/12.457348)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wittman 等人（2002）Wittman D. M., 等人，2002年，在 Tyson J. A., Wolff S., 编，《光学工程学会（SPIE）会议系列》第4836卷，调查及其他望远镜技术与发现。页
    73–82 ([arXiv:astro-ph/0210118](http://arxiv.org/abs/astro-ph/0210118)), [doi:10.1117/12.457348](http://dx.doi.org/10.1117/12.457348)
- en: Wittman et al. (2003) Wittman D., Margoniner V. E., Tyson J. A., Cohen J. G.,
    Becker A. C., Dell’Antonio I. P., 2003, [ApJ](http://dx.doi.org/10.1086/378344),
    [597, 218](https://ui.adsabs.harvard.edu/abs/2003ApJ...597..218W)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wittman 等人（2003）Wittman D., Margoniner V. E., Tyson J. A., Cohen J. G., Becker
    A. C., Dell’Antonio I. P., 2003, [ApJ](http://dx.doi.org/10.1086/378344), [597,
    218](https://ui.adsabs.harvard.edu/abs/2003ApJ...597..218W)
- en: Wittman et al. (2006) Wittman D., Dell’Antonio I. P., Hughes J. P., Margoniner
    V. E., Tyson J. A., Cohen J. G., Norman D., 2006, [ApJ](http://dx.doi.org/10.1086/502621),
    [643, 128](https://ui.adsabs.harvard.edu/abs/2006ApJ...643..128W)
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wittman 等人（2006）Wittman D., Dell’Antonio I. P., Hughes J. P., Margoniner V.
    E., Tyson J. A., Cohen J. G., Norman D., 2006, [ApJ](http://dx.doi.org/10.1086/502621),
    [643, 128](https://ui.adsabs.harvard.edu/abs/2006ApJ...643..128W)
- en: Wuyts et al. (2014) Wuyts E., Rigby J. R., Gladders M. D., Sharon K., 2014,
    [ApJ](http://dx.doi.org/10.1088/0004-637X/781/2/61), [781, 61](https://ui.adsabs.harvard.edu/abs/2014ApJ...781...61W)
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wuyts 等人（2014）Wuyts E., Rigby J. R., Gladders M. D., Sharon K., 2014, [ApJ](http://dx.doi.org/10.1088/0004-637X/781/2/61),
    [781, 61](https://ui.adsabs.harvard.edu/abs/2014ApJ...781...61W)
- en: Zhou et al. (2016) Zhou B., Khosla A., A. L., Oliva A., Torralba A., 2016, CVPR
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2016）Zhou B., Khosla A., A. L., Oliva A., Torralba A., 2016, CVPR
- en: Appendix A Model performance and final lens sample
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 模型性能和最终透镜样本
- en: In this appendix, we provide some additional details of the model performance
    and the top lens candidates identified in this work.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们提供了模型性能的一些额外细节以及在此工作中识别出的顶级透镜候选者。
- en: Figure [12](#A1.F12 "Figure 12 ‣ Appendix A Model performance and final lens
    sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") shows the distribution of model scores across
    the different DLS fields (F1 to F5), demonstrating similar performance in each
    field. This is generally expected given the similar image quality across the DLS
    survey. Importantly it shows that our use of labeled training data from only F1
    does not substantially affect the model performance in the other fields.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图[12](#A1.F12 "图 12 ‣ 附录 A 模型性能和最终透镜样本 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜") 显示了模型分数在不同
    DLS 区域（F1 到 F5）中的分布，展示了每个区域中类似的表现。这是预期中的情况，因为 DLS 调查中的图像质量相似。重要的是，它显示了我们仅使用 F1
    的标记训练数据并没有显著影响模型在其他区域的表现。
- en: Table [7](#A1.T7 "Table 7 ‣ Appendix A Model performance and final lens sample
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") lists the results from our ablation study discussed
    in Section [6.1.1](#S6.SS1.SSS1 "6.1.1 Ablation study on data augmentations ‣
    6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey"). We show the precision across recall
    rates from 50-100%. The performance differences are generally similar across all
    recall rates. Color augmentation, JPEG quality, and GAN images appear to most
    prominently improving the model performance (i.e., the models perform significantly
    worse when these augmentations are removed).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 表[7](#A1.T7 "表 7 ‣ 附录 A 模型性能和最终透镜样本 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜") 列出了我们在第[6.1.1节](#S6.SS1.SSS1
    "6.1.1 数据增强的消融研究 ‣ 6.1 使用 GAN 和数据增强的半监督算法具有更好的性能 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")中讨论的消融研究结果。我们展示了从
    50-100% 的召回率下的精确度。性能差异在所有召回率下通常相似。颜色增强、JPEG 质量和 GAN 图像似乎最显著地改善了模型性能（即，当去除这些增强时，模型表现显著下降）。
- en: 'We show the top 25 predicted lens candidates from the GAN+$\Pi$-model and GAN+MixMatch
    models in Figures [13](#A1.F13 "Figure 13 ‣ Appendix A Model performance and final
    lens sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") and [14](#A1.F14 "Figure 14 ‣ Appendix A Model
    performance and final lens sample ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), respectively. These include
    several of our top lens candidates based on human inspection (see Figures [6](#S6.F6
    "Figure 6 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms
    with GANs and Augmentations have superior performance ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")) and [7](#S6.F7 "Figure 7 ‣ 6.1.1 Ablation study on
    data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")), but
    many do not show obvious signs of strong lensing. There are several duplicate
    images at slightly different sky positions as discussed in the main text. In Figure [15](#A1.F15
    "Figure 15 ‣ Appendix A Model performance and final lens sample ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")
    we include the GradCAM++ heatmaps obtained for all the Grade-A candidates (analogous
    to the example subsets shown in Figure [10](#S6.F10 "Figure 10 ‣ 6.2.3 Distribution
    of lensed candidates in color-color space ‣ 6.2 Catalog of Lens candidates found
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). These heatmaps were generated
    using our best performing models: GAN+MixMatch or GAN+$\Pi$-model (discussed in
    Section [6.3](#S6.SS3 "6.3 Lensing signatures identified by the models ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). Finally, we list the sky coordinates of all
    Grade-A and Grade-B lenses in Table [8](#A1.T8 "Table 8 ‣ Appendix A Model performance
    and final lens sample ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey").'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[13](#A1.F13 "图 13 ‣ 附录 A 模型性能和最终透镜样本 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")和图[14](#A1.F14
    "图 14 ‣ 附录 A 模型性能和最终透镜样本 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")中展示了来自GAN+$\Pi$-模型和GAN+MixMatch模型的前25个预测透镜候选者。这些包括我们根据人工检查的几个顶级透镜候选者（见图[6](#S6.F6
    "图 6 ‣ 6.1.1 数据增强的消融研究 ‣ 6.1 GAN和数据增强的半监督算法具有更好的性能 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")和图[7](#S6.F7
    "图 7 ‣ 6.1.1 数据增强的消融研究 ‣ 6.1 GAN和数据增强的半监督算法具有更好的性能 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")），但许多没有显示出明显的强透镜迹象。正如主文中讨论的那样，有几个略有不同天空位置的重复图像。在图[15](#A1.F15
    "图 15 ‣ 附录 A 模型性能和最终透镜样本 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")中，我们包括了所有A级候选者获得的GradCAM++热图（类似于图[10](#S6.F10
    "图 10 ‣ 6.2.3 透镜候选者在颜色-颜色空间中的分布 ‣ 6.2 发现的透镜候选者目录 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")中展示的示例子集）。这些热图是使用我们最佳性能模型生成的：GAN+MixMatch或GAN+$\Pi$-模型（讨论见第[6.3节](#S6.SS3
    "6.3 模型识别的透镜特征 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")）。最后，我们在表[8](#A1.T8 "表 8
    ‣ 附录 A 模型性能和最终透镜样本 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")中列出了所有A级和B级透镜的天空坐标。
- en: '| ![Refer to caption](img/d844e2b4ff9902248ebfe8d3e4a197b7.png) | ![Refer to
    caption](img/791cee5cba4f86510e6cb6085473d51b.png) | ![Refer to caption](img/13f743324dd7558609ee3a99a6312d28.png)
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/d844e2b4ff9902248ebfe8d3e4a197b7.png) | ![参见说明](img/791cee5cba4f86510e6cb6085473d51b.png)
    | ![参见说明](img/13f743324dd7558609ee3a99a6312d28.png) |'
- en: '| ![Refer to caption](img/81841bf07799d297df79e08d750c4144.png) | ![Refer to
    caption](img/a623961ba45e97cb6506825169ddbae8.png) |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/81841bf07799d297df79e08d750c4144.png) | ![参见说明](img/a623961ba45e97cb6506825169ddbae8.png)
    |  |'
- en: 'Figure 12: Histogram of the scores obtained by the GAN+MixMatch and GAN+$\Pi$-model
    in the five independent DLS Fields F1 through F5\. As discussed in Section [4](#S4
    "4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), the training set used to
    train our models (TrainingV2) contains human labeled NonLenses which were randomly
    sampled only from Field F1\. But as we clearly see, the distribution of scores
    (and performance of the models as a result) is independent of the field chosen.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：GAN+MixMatch 和 GAN+$\Pi$-模型在五个独立 DLS 领域 F1 到 F5 中获得的分数直方图。如在第[4节](#S4 "4
    训练和验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")中讨论的，训练我们的模型所用的训练集（TrainingV2）包含了从 F1 领域随机采样的人工标注的
    NonLenses。但是，正如我们清楚看到的，分数的分布（以及模型的性能结果）与选择的领域无关。
- en: '| Augmentation removed | TestV1 Precision(%) | TestV2 Precision(%) | TestV1
    baseline difference(%) | TestV2 baseline difference(%) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 增强移除 | TestV1 精度（%） | TestV2 精度（%） | TestV1 基准差异（%） | TestV2 基准差异（%） |'
- en: '|  |  | Performance at 50% recall rate |  |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 50% 召回率的性能 |  |  |'
- en: '| None | $84.95\pm 8.70$ | $80.19\pm 17.08$ | - | - |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 无 | $84.95\pm 8.70$ | $80.19\pm 17.08$ | - | - |'
- en: '| GAN | $65.46\pm 15.00$ | $55.77\pm 17.43$ | -19.49 | -24.42 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| GAN | $65.46\pm 15.00$ | $55.77\pm 17.43$ | -19.49 | -24.42 |'
- en: '| RGB shuffle | $44.8\pm 24.32$ | $35.45\pm 21.75$ | -40.15 | -44.74 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| RGB 洗牌 | $44.8\pm 24.32$ | $35.45\pm 21.75$ | -40.15 | -44.74 |'
- en: '| JPEG quality | $65.30\pm 13.84$ | $56.84\pm 14.06$ | -19.65 | -23.35 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| JPEG 质量 | $65.30\pm 13.84$ | $56.84\pm 14.06$ | -19.65 | -23.35 |'
- en: '| Rot90 | $91.81\pm 4.41$ | $89.96\pm 6.33$ | +6.86 | +9.77 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| Rot90 | $91.81\pm 4.41$ | $89.96\pm 6.33$ | +6.86 | +9.77 |'
- en: '| Translations | $89.47\pm 10.34$ | $84.59\pm 13.04$ | +4.52 | +4.4 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 平移 | $89.47\pm 10.34$ | $84.59\pm 13.04$ | +4.52 | +4.4 |'
- en: '| Horizontal flips | $84.63\pm 10.85$ | $75.74\pm 13.96$ | -0.32 | -4.45 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 水平翻转 | $84.63\pm 10.85$ | $75.74\pm 13.96$ | -0.32 | -4.45 |'
- en: '| Color augmentation | $71.88\pm 17.35$ | $68.23\pm 15.2$ | -13.07 | -11.96
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 颜色增强 | $71.88\pm 17.35$ | $68.23\pm 15.2$ | -13.07 | -11.96 |'
- en: '|  |  | Performance at 60% recall rate |  |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 60% 召回率的性能 |  |  |'
- en: '| None | $79.88\pm 6.30$ | $78.03\pm 12.45$ | - | - |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 无 | $79.88\pm 6.30$ | $78.03\pm 12.45$ | - | - |'
- en: '| GAN | $55.54\pm 13.13$ | $44.09\pm 12.94$ | - 24.34 | -33.94 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| GAN | $55.54\pm 13.13$ | $44.09\pm 12.94$ | -24.34 | -33.94 |'
- en: '| RGB shuffle | $34.25\pm 24.49$ | $26.14\pm 15.9$ | - 45.63 | -51.89 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| RGB 洗牌 | $34.25\pm 24.49$ | $26.14\pm 15.9$ | -45.63 | -51.89 |'
- en: '| JPEG quality | $52.75\pm 24.68$ | $51.69\pm 17.91$ | - 27.13 | -26.34 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| JPEG 质量 | $52.75\pm 24.68$ | $51.69\pm 17.91$ | -27.13 | -26.34 |'
- en: '| Rot90 | $84.94\pm 7.04$ | $78.99\pm 5.23$ | +5.06 | + 0.96 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Rot90 | $84.94\pm 7.04$ | $78.99\pm 5.23$ | +5.06 | +0.96 |'
- en: '| Translations | $74.95\pm 16.11$ | $72.26\pm 24.65$ | -4.93 | -5.77 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 平移 | $74.95\pm 16.11$ | $72.26\pm 24.65$ | -4.93 | -5.77 |'
- en: '| Horizontal flips | $71.15\pm 11.46$ | $60.64\pm 12.27$ | -8.73 | -17.39 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 水平翻转 | $71.15\pm 11.46$ | $60.64\pm 12.27$ | -8.73 | -17.39 |'
- en: '| Color augmentation | $63.24\pm 15.67$ | $50.91\pm 15.53$ | -16.64 | -27.12
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 颜色增强 | $63.24\pm 15.67$ | $50.91\pm 15.53$ | -16.64 | -27.12 |'
- en: '|  |  | Performance at 70% recall rate |  |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 70% 召回率的性能 |  |  |'
- en: '| None | $69.42\pm 4.60$ | $68.05\pm 12.96$ | - | - |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 无 | $69.42\pm 4.60$ | $68.05\pm 12.96$ | - | - |'
- en: '| GAN | $46.45\pm 14.49$ | $37.24\pm 12.85$ | -22.97 | -30.81 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| GAN | $46.45\pm 14.49$ | $37.24\pm 12.85$ | -22.97 | -30.81 |'
- en: '| RGB shuffle | $26.68\pm 17.28$ | $18.75\pm 11.78$ | -42.74 | -49.3 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| RGB 洗牌 | $26.68\pm 17.28$ | $18.75\pm 11.78$ | -42.74 | -49.3 |'
- en: '| JPEG quality | $40.38\pm 18.64$ | $40.52\pm 21.74$ | - 29.04 | -27.53 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| JPEG 质量 | $40.38\pm 18.64$ | $40.52\pm 21.74$ | -29.04 | -27.53 |'
- en: '| Rot90 | $79.97\pm 12.15$ | $73.69\pm 7.11$ | +10.55 | +5.64 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Rot90 | $79.97\pm 12.15$ | $73.69\pm 7.11$ | +10.55 | +5.64 |'
- en: '| Translations | $67.69\pm 12.67$ | $59.31\pm 22.34$ | -1.73 | -8.74 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 平移 | $67.69\pm 12.67$ | $59.31\pm 22.34$ | -1.73 | -8.74 |'
- en: '| Horizontal flips | $67.36\pm 11.95$ | $55.94\pm 14.71$ | -2.06 | -12.11 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 水平翻转 | $67.36\pm 11.95$ | $55.94\pm 14.71$ | -2.06 | -12.11 |'
- en: '| Color augmentation | $52.79\pm 14.92$ | $45.93\pm 13.97$ | -16.63 | -22.12
    |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 颜色增强 | $52.79\pm 14.92$ | $45.93\pm 13.97$ | -16.63 | -22.12 |'
- en: '|  |  | Performance at 80% recall rate |  |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 80% 召回率的性能 |  |  |'
- en: '| None | $54.35\pm 4.57$ | $40.98\pm 17.12$ | - | - |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 无 | $54.35\pm 4.57$ | $40.98\pm 17.12$ | - | - |'
- en: '| GAN | $33.02\pm 10.26$ | $24.37\pm 10.50$ | -21.33 | -16.61 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| GAN | $33.02\pm 10.26$ | $24.37\pm 10.50$ | -21.33 | -16.61 |'
- en: '| RGB shuffle | $15.34\pm 7.59$ | $11.75\pm 4.90$ | -39.01 | -29.23 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| RGB 洗牌 | $15.34\pm 7.59$ | $11.75\pm 4.90$ | -39.01 | -29.23 |'
- en: '| JPEG quality | $21.33\pm 8.29$ | $15.25\pm 3.41$ | -33.02 | -25.73 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| JPEG 质量 | $21.33\pm 8.29$ | $15.25\pm 3.41$ | -33.02 | -25.73 |'
- en: '| Rot90 | $62.63\pm 12.43$ | $52.2\pm 13.65$ | +8.28 | +11.22 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Rot90 | $62.63\pm 12.43$ | $52.2\pm 13.65$ | +8.28 | +11.22 |'
- en: '| Translations | $52.17\pm 12.92$ | $35.09\pm 16.64$ | -2.18 | -5.89 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 平移 | $52.17\pm 12.92$ | $35.09\pm 16.64$ | -2.18 | -5.89 |'
- en: '| Horizontal flips | $60.54\pm 9.78$ | $36.46\pm 9.03$ | +6.19 | -4.52 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 水平翻转 | $60.54\pm 9.78$ | $36.46\pm 9.03$ | +6.19 | -4.52 |'
- en: '| Color augmentation | $39.47\pm 13.24$ | $33.84\pm 10.92$ | -14.88 | -7.14
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 颜色增强 | $39.47\pm 13.24$ | $33.84\pm 10.92$ | -14.88 | -7.14 |'
- en: '|  |  | Performance at 90% recall rate |  |  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 90%召回率的性能 |  |  |'
- en: '| None | $34.12\pm 7.47$ | $16.33\pm 8.661$ | - | - |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 无 | $34.12\pm 7.47$ | $16.33\pm 8.661$ | - | - |'
- en: '| GAN | $22.60\pm 4.78$ | $10.99\pm 4.25$ | -11.52 | -5.34 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| GAN | $22.60\pm 4.78$ | $10.99\pm 4.25$ | -11.52 | -5.34 |'
- en: '| RGB shuffle | $10.37\pm 4.68$ | $5.88\pm 2.61$ | -23.75 | -10.45 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| RGB shuffle | $10.37\pm 4.68$ | $5.88\pm 2.61$ | -23.75 | -10.45 |'
- en: '| JPEG quality | $15.15\pm 6.37$ | $6.18\pm 2.41$ | -18.97 | -10.15 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| JPEG质量 | $15.15\pm 6.37$ | $6.18\pm 2.41$ | -18.97 | -10.15 |'
- en: '| Rot90 | $40.72\pm 12.74$ | $21.79\pm 3.08$ | +6.6 | +5.46 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| Rot90 | $40.72\pm 12.74$ | $21.79\pm 3.08$ | +6.6 | +5.46 |'
- en: '| Translations | $32.88\pm 3.35$ | $16.14\pm 6.72$ | -1.24 | -0.19 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 平移 | $32.88\pm 3.35$ | $16.14\pm 6.72$ | -1.24 | -0.19 |'
- en: '| Horizontal flips | $30.81\pm 20.44$ | $14.72\pm 6.59$ | -3.31 | -1.61 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 水平翻转 | $30.81\pm 20.44$ | $14.72\pm 6.59$ | -3.31 | -1.61 |'
- en: '| Color augmentation | $23.86\pm 8.13$ | $14.25\pm 8.45$ | -10.26 | -2.08 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 颜色增强 | $23.86\pm 8.13$ | $14.25\pm 8.45$ | -10.26 | -2.08 |'
- en: '|  |  | Performance at 100% recall rate |  |  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 100%召回率的性能 |  |  |'
- en: '| None | $8.25\pm 2.85$ | $6.05\pm 2.69$ | - | - |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 无 | $8.25\pm 2.85$ | $6.05\pm 2.69$ | - | - |'
- en: '| GAN | $8.13\pm 2.49$ | $5.79\pm 3.93$ | -0.12 | -0.26 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| GAN | $8.13\pm 2.49$ | $5.79\pm 3.93$ | -0.12 | -0.26 |'
- en: '| RGB shuffle | $6.43\pm 0.97$ | $3.84\pm 1.02$ | -1.82 | -2.21 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| RGB shuffle | $6.43\pm 0.97$ | $3.84\pm 1.02$ | -1.82 | -2.21 |'
- en: '| JPEG quality | $5.88\pm 0.21$ | $4.14\pm 2.09$ | -2.37 | -1.91 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| JPEG质量 | $5.88\pm 0.21$ | $4.14\pm 2.09$ | -2.37 | -1.91 |'
- en: '| Rot90 | $14.76\pm 8.42$ | $13.00\pm 5.16$ | +6.51 | +6.95 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| Rot90 | $14.76\pm 8.42$ | $13.00\pm 5.16$ | +6.51 | +6.95 |'
- en: '| Translations | $10.83\pm 2.46$ | $7.37\pm 3.66$ | +2.58 | +1.32 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 平移 | $10.83\pm 2.46$ | $7.37\pm 3.66$ | +2.58 | +1.32 |'
- en: '| Horizontal flips | $15.74\pm 3.06$ | $8.86\pm 1.84$ | +7.49 | +2.81 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 水平翻转 | $15.74\pm 3.06$ | $8.86\pm 1.84$ | +7.49 | +2.81 |'
- en: '| Color augmentation | $11.42\pm 7.25$ | $6.84\pm 4.12$ | +3.17 | +0.79 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 颜色增强 | $11.42\pm 7.25$ | $6.84\pm 4.12$ | +3.17 | +0.79 |'
- en: 'Table 7: Ablation performance for 50–100% recall rates (in steps of 10%) for
    the GAN+Supervised model using TrainingV2\. The first row of each recall rate
    shows the baseline precision value obtained from the model on the test sets (TestV1,
    TestV2) when none of the augmentations are removed. In the subsequent rows, we
    report the precision obtained when the model was trained without the specified
    augmentation. For example, the baseline model at 50% recall has a precision of
    80.19% for TestV2 and decreases to 55.77% when GAN images are removed during training.
    The difference in the obtained precision values are quoted in the last two columns.
    Augmentations which improve model performance (i.e., improve precision when included
    and decrease decrease precision when removed) are shown in red, while those which
    decrease model performance are shown in blue. Overall, the models perform worse
    when color augmentations, JPEG quality and GANs are not included, indicating that
    these augmentations are important for optimal performance. The errrors quoted
    here are 1$\sigma$.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：GAN+Supervised模型在50–100%召回率（每10%一步）的消融性能，使用TrainingV2。每个召回率的第一行显示了从模型在测试集（TestV1、TestV2）上获得的基准精度值，当未移除任何增强时。在随后的行中，我们报告了在没有指定增强时训练模型获得的精度。例如，50%召回率的基准模型在TestV2上的精度为80.19%，在训练时移除GAN图像后降至55.77%。获得的精度值差异在最后两列中列出。增强模型性能（即，包含时提高精度、移除时降低精度）的用红色显示，而降低模型性能的用蓝色显示。总体来说，当不包含颜色增强、JPEG质量和GAN时，模型表现较差，表明这些增强对最佳性能至关重要。这里引用的误差为1$\sigma$。
- en: '![Refer to caption](img/397c7f01d5a4158f1c06a41192cef2c1.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/397c7f01d5a4158f1c06a41192cef2c1.png)'
- en: 'Figure 13: DLS images of the top 25 predictions from GAN+$\Pi$-model. Several
    show clear evidence of strong lensing, while other images appear to be false positives.
    We note that many images are duplicates (at overlapping regions of the sky), which
    we remove before visual inspection.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：GAN+$\Pi$-model的前25个预测的DLS图像。几张显示了明显的强引力透镜现象，而其他图像似乎是假阳性。我们注意到许多图像是重复的（位于重叠的天空区域），我们在视觉检查前将其删除。
- en: '![Refer to caption](img/418c06a7b48a4dc0eb406031822094d8.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/418c06a7b48a4dc0eb406031822094d8.png)'
- en: 'Figure 14: Equivalent to Figure [13](#A1.F13 "Figure 13 ‣ Appendix A Model
    performance and final lens sample ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), showing the top 25 predictions
    from GAN+Mimatch.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：相当于图 [13](#A1.F13 "图 13 ‣ 附录 A 模型性能和最终镜头样本 ‣ 优化机器学习方法以发现深度镜头调查中的强引力透镜")，显示了来自
    GAN+MixMatch 的前 25 个预测。
- en: '| object ID | RA | DEC | Field | Rank (GAN+MixMatch) | Rank (GAN+$\Pi$-model)
    |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 对象 ID | RA | DEC | 领域 | 排名 (GAN+MixMatch) | 排名 (GAN+$\Pi$-model) |'
- en: '|  |  |  | Grade-A candidates |  |  |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | A等级候选者 |  |  |'
- en: '| 421095124 | 163.792076 | -5.070373 | F4 | 2 | 12 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 421095124 | 163.792076 | -5.070373 | F4 | 2 | 12 |'
- en: '| 513097468 | 209.340092 | -10.244328 | F5 | 38 | 13 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 513097468 | 209.340092 | -10.244328 | F5 | 38 | 13 |'
- en: '| 212072337 | 139.896040 | 30.532355 | F2 | 181 | 21 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 212072337 | 139.896040 | 30.532355 | F2 | 181 | 21 |'
- en: '| 322054393 | 79.839914 | -48.949647 | F3 | 733 | 8326 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 322054393 | 79.839914 | -48.949647 | F3 | 733 | 8326 |'
- en: '| 432021600 | 162.750073 | -5.941902 | F4 | 1262 | 12424 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 432021600 | 162.750073 | -5.941902 | F4 | 1262 | 12424 |'
- en: '| 431010921 | 163.364259 | -5.789092 | F4 | 1279 | 19826 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 431010921 | 163.364259 | -5.789092 | F4 | 1279 | 19826 |'
- en: '| 512037933 | 209.677055 | -10.687652 | F5 | 7461 | 2068 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 512037933 | 209.677055 | -10.687652 | F5 | 7461 | 2068 |'
- en: '| 421117552 | 163.897903 | -5.054885 | F4 | 4799 | 2768 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 421117552 | 163.897903 | -5.054885 | F4 | 4799 | 2768 |'
- en: '| 212148326 | 139.512033 | 30.953524 | F2 | 3579 | 23223 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 212148326 | 139.512033 | 30.953524 | F2 | 3579 | 23223 |'
- en: '|  |  |  | Grade-B candidates |  |  |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | B等级候选者 |  |  |'
- en: '| 313032462 | 78.742878 | -48.149829 | F3 | 365 | 59 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 313032462 | 78.742878 | -48.149829 | F3 | 365 | 59 |'
- en: '| 331108599 | 81.300608 | -49.432676 | F3 | 1974 | 98 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 331108599 | 81.300608 | -49.432676 | F3 | 1974 | 98 |'
- en: '| 132023380 | 13.551513 | 11.794606 | F1 | 3400 | 462 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 132023380 | 13.551513 | 11.794606 | F1 | 3400 | 462 |'
- en: '| 533097114 | 209.328083 | -11.993324 | F5 | 518 | 676 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 533097114 | 209.328083 | -11.993324 | F5 | 518 | 676 |'
- en: '| 433116975 | 162.551767 | -5.697394 | F4 | 13673 | 720 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 433116975 | 162.551767 | -5.697394 | F4 | 13673 | 720 |'
- en: '| 233074254 | 139.046712 | 29.298535 | F2 | 870 | 2320 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 233074254 | 139.046712 | 29.298535 | F2 | 870 | 2320 |'
- en: '| 413115231 | 162.585545 | -4.498548 | F4 | 8839 | 884 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 413115231 | 162.585545 | -4.498548 | F4 | 8839 | 884 |'
- en: '| 211134050 | 140.304878 | 30.471131 | F2 | 12662 | 979 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 211134050 | 140.304878 | 30.471131 | F2 | 12662 | 979 |'
- en: '| 122079323 | 13.182525 | 12.323637 | F1 | 8567 | 1145 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 122079323 | 13.182525 | 12.323637 | F1 | 8567 | 1145 |'
- en: '| 312158847 | 80.455801 | -48.489660 | F3 | 3896 | 1209 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 312158847 | 80.455801 | -48.489660 | F3 | 3896 | 1209 |'
- en: '| 322092794 | 80.115321 | -49.246309 | F3 | 1234 | 24945 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 322092794 | 80.115321 | -49.246309 | F3 | 1234 | 24945 |'
- en: '| 421019105 | 163.411890 | -4.870280 | F4 | 1996 | 2702 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 421019105 | 163.411890 | -4.870280 | F4 | 1996 | 2702 |'
- en: '| 221061603 | 140.662872 | 29.846367 | F2 | 3990 | 8584 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 221061603 | 140.662872 | 29.846367 | F2 | 3990 | 8584 |'
- en: 'Table 8: Grade-A and Grade-B Lens candidates found from this work with their
    object ID, RA and DEC coordinates, DLS field (F1 through F5), and their corresponding
    ranks from GAN+MixMatch and GAN+$\Pi$-models. The rank is obtained by passing
    all the survey images (281,425 objects in total; Section [2](#S2 "2 Deep Lens
    Survey Data ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")) through the models and sorting them based on
    their prediction scores. High-confidence Lens candidates have lower ranks and
    high prediction scores. For example, the Grade-A lens candidate DLS212072337 whose
    lensing nature has been spectroscopically confirmed (Section [6.2.2](#S6.SS2.SSS2
    "6.2.2 Spectroscopic confirmation of two Grade-A lenses ‣ 6.2 Catalog of Lens
    candidates found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")) has a rank
    of 21 from the GAN+$\Pi$-model and a prediction score of $\simeq 1$. The ranks
    quoted here represent an upper bound on the number of images an investigator has
    to look at to find the lens candidate, as they do not account for duplicated sky
    regions which we remove before visual inspection (as discussed in Section [6.2](#S6.SS2
    "6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")), reducing the number of unique lens candidates investigated.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：从这项工作中发现的Grade-A和Grade-B透镜候选者，包括其对象ID、RA和DEC坐标、DLS场（F1至F5），以及GAN+MixMatch和GAN+$\Pi$-模型的相应排名。排名是通过将所有调查图像（总共281,425个对象；第[2](#S2
    "2 Deep Lens Survey Data ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")节）传入模型并根据其预测分数进行排序获得的。高置信度的透镜候选者具有较低的排名和高预测分数。例如，Grade-A透镜候选者DLS212072337，其透镜性质已通过光谱确认（第[6.2.2](#S6.SS2.SSS2
    "6.2.2 Spectroscopic confirmation of two Grade-A lenses ‣ 6.2 Catalog of Lens
    candidates found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")节），其在GAN+$\Pi$-模型中的排名为21，预测分数为$\simeq
    1$。这里引用的排名表示调查人员必须查看的图像数量的上限，因为这些排名未考虑我们在视觉检查之前去除的重复天空区域（如第[6.2](#S6.SS2 "6.2
    Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")节讨论的），从而减少了调查的独特透镜候选者数量。
- en: '![Refer to caption](img/59c45db608f465dc3e009ae386f6c820.png)![Refer to caption](img/886bb3561092d12764f5ad5df8cfd783.png)![Refer
    to caption](img/c4be8039222e6f305850a4914fd293ec.png)![Refer to caption](img/675dbb1a76fa13445a76e754decd851c.png)![Refer
    to caption](img/ac5a5f2c6ffd15598e9118d0cf81a7b5.png)![Refer to caption](img/3f163efa26e3db96f3cabe4d0e08e935.png)![Refer
    to caption](img/bcbc9751b997d7959d39e45f83dc61f3.png)![Refer to caption](img/04ccfbb3a5d31706551cf3b507bd9f88.png)![Refer
    to caption](img/c5831d832ffc3d639861c190339a8dcb.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/59c45db608f465dc3e009ae386f6c820.png)![参见说明](img/886bb3561092d12764f5ad5df8cfd783.png)![参见说明](img/c4be8039222e6f305850a4914fd293ec.png)![参见说明](img/675dbb1a76fa13445a76e754decd851c.png)![参见说明](img/ac5a5f2c6ffd15598e9118d0cf81a7b5.png)![参见说明](img/3f163efa26e3db96f3cabe4d0e08e935.png)![参见说明](img/bcbc9751b997d7959d39e45f83dc61f3.png)![参见说明](img/04ccfbb3a5d31706551cf3b507bd9f88.png)![参见说明](img/c5831d832ffc3d639861c190339a8dcb.png)'
- en: 'Figure 15: GradCAM++ heatmaps for all Grade-A lenses, equivalent to Figure [10](#S6.F10
    "Figure 10 ‣ 6.2.3 Distribution of lensed candidates in color-color space ‣ 6.2
    Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey").
    Each image is labeled with its object ID, and the model corresponding to the heatmaps
    (MM = GAN+MixMatch, PI = GAN+$\Pi$-model).'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：所有Grade-A透镜的GradCAM++热图，相当于图[10](#S6.F10 "Figure 10 ‣ 6.2.3 Distribution
    of lensed candidates in color-color space ‣ 6.2 Catalog of Lens candidates found
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")。每个图像都标有其对象ID，以及与热图对应的模型（MM = GAN+MixMatch，PI
    = GAN+$\Pi$-model）。
