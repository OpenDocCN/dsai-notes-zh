- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:43:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2211.00047] Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2211.00047](https://ar5iv.labs.arxiv.org/html/2211.00047)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keerthi Vasan G.C.,¹ Stephen Sheng,² Tucker Jones,¹ Chi Po Choi,³ and James
    Sharpnack^(2,3)
  prefs: []
  type: TYPE_NORMAL
- en: ¹Department of Physics and Astronomy, University of California, Davis, 1 Shields
    Avenue, Davis, CA 95616, USA
  prefs: []
  type: TYPE_NORMAL
- en: ²Amazon AWS AI
  prefs: []
  type: TYPE_NORMAL
- en: '³Department of Statistics, University of California, Davis, 1 Shields Avenue,
    Davis, CA 95616, USA E-mail: kvch@ucdavis.eduWork done prior to joining Amazon(Last
    updated ; in original form )'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Machine learning models can greatly improve the search for strong gravitational
    lenses in imaging surveys by reducing the amount of human inspection required.
    In this work, we test the performance of supervised, semi-supervised, and unsupervised
    learning algorithms trained with the ResNetV2 neural network architecture on their
    ability to efficiently find strong gravitational lenses in the Deep Lens Survey
    (DLS). We use galaxy images from the survey, combined with simulated lensed sources,
    as labeled data in our training datasets. We find that models using semi-supervised
    learning along with data augmentations (transformations applied to an image during
    training, e.g., rotation) and Generative Adversarial Network (GAN) generated images
    yield the best performance. They offer 5–10 times better precision across all
    recall values compared to supervised algorithms. Applying the best performing
    models to the full 20 deg² DLS survey, we find 3 Grade-A lens candidates within
    the top 17 image predictions from the model. This increases to 9 Grade-A and 13
    Grade-B candidates when $1$% ($\sim 2500$ images) of the model predictions are
    visually inspected. This is $\gtrsim 10\times$ the sky density of lens candidates
    compared to current shallower wide-area surveys (such as the Dark Energy Survey),
    indicating a trove of lenses awaiting discovery in upcoming deeper all-sky surveys.
    These results suggest that pipelines tasked with finding strong lens systems can
    be highly efficient, minimizing human effort. We additionally report spectroscopic
    confirmation of the lensing nature of two Grade-A candidates identified by our
    model, further validating our methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'gravitational lensing: strong – methods: statistical^†^†pubyear: 2022^†^†pagerange:
    Optimizing machine learning methods to discover strong gravitational lenses in
    the Deep Lens Survey–[15](#A1.F15 "Figure 15 ‣ Appendix A Model performance and
    final lens sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Under rare alignment configurations, the gravitational potential of a massive
    galaxy can cause light from a distant galaxy located behind it to take multiple
    paths around it. This results in the formation of several distinct images of the
    distant galaxy around the massive galaxy, a phenomenon known as strong gravitational
    lensing (e.g., Treu, [2010](#bib.bib76)). These multiple images are magnified
    by factors that can reach $>$10 times, making them appear brighter and more spatially
    extended. Such magnification makes these systems ideal for studying the formation
    and evolution of galaxies across cosmic time (e.g., Wuyts et al., [2014](#bib.bib81);
    Pettini et al., [2002](#bib.bib59); Swinbank et al., [2009](#bib.bib73); Koopmans
    et al., [2006](#bib.bib34); Leethochawalit et al., [2016](#bib.bib45)), while
    analysis of the lensing mass distribution enables insight into the nature of dark
    matter (e.g., Chiba, [2002](#bib.bib13); Bradač et al., [2002](#bib.bib10); Miranda
    & Macciò, [2007](#bib.bib51); Gilman et al., [2019](#bib.bib21); Shajib et al.,
    [2022](#bib.bib65)).
  prefs: []
  type: TYPE_NORMAL
- en: The main current challenge in working with strong lens systems is their scarcity
    on the sky. Therefore, methods which are able to efficiently identify lensed galaxies
    from wide-area sky surveys are extremely beneficial. Automated methods will be
    especially valuable for lens searches in upcoming wide-area sky surveys to be
    carried out by the Vera Rubin Observatory, Euclid, and Roman (e.g., LSST Science
    Collaboration et al., [2009](#bib.bib40); Laureijs et al., [2011](#bib.bib42);
    Spergel et al., [2015](#bib.bib70)), whose improvements in sensitivity, angular
    resolution and sky coverage will enable detection of far more lens samples than
    are currently known.
  prefs: []
  type: TYPE_NORMAL
- en: Early approaches to finding strong lens systems included various algorithms
    searching for multiple lensed images or arc shapes, manual searches around massive
    galaxies, and citizen science projects (e.g., Moustakas et al., [2007](#bib.bib54);
    Paraficz et al., [2016](#bib.bib58); Seidel & Bartelmann, [2007](#bib.bib63);
    Gavazzi et al., [2014](#bib.bib20); Alard, [2006](#bib.bib2); Fassnacht et al.,
    [2004](#bib.bib17); More et al., [2016](#bib.bib53); Belokurov et al., [2009](#bib.bib5);
    Diehl et al., [2009](#bib.bib15); Garvin et al., [2022](#bib.bib19)). While successful,
    these methods are time-consuming and difficult to incorporate into an automated
    framework. Convolutional Neural Networks (CNNs; LeCun et al., [1989](#bib.bib43);
    Krizhevsky et al., [2012](#bib.bib38)), which have been successfully developed
    into a standard tool in the field of computer vision in the past decade, are a
    promising approach to solving image recognition problems. Depending on the problem,
    there are various neural network architectures that can be optimized for the desired
    objectives. CNNs and machine learning techniques in general have indeed been used
    with success in the past few years to uncover gravitationally lensed candidates
    in wide-area imaging surveys (e.g., Jacobs et al., [2017](#bib.bib30); Jacobs
    et al., [2019](#bib.bib31); Sonnenfeld et al., [2018](#bib.bib69); Pourrahmani
    et al., [2018](#bib.bib60); Huang et al., [2020](#bib.bib27); Li et al., [2020](#bib.bib46);
    Cañameras et al., [2020](#bib.bib11)).
  prefs: []
  type: TYPE_NORMAL
- en: Most machine learning searches for lenses have relied primarily on supervised
    learning methods (i.e., using a data set consisting of labeled lensed and non-lensed
    galaxies to train a model). However, while non-lensed galaxies are plentiful,
    current surveys have very few known lenses to be used as positive labels. Instead,
    machine learning models are trained on simulated lenses, which can be generated
    in abundance (e.g., Jacobs et al., [2017](#bib.bib30)). However, this presents
    a new problem, that the training data distribution (i.e., the simulated lenses)
    differs from the test data distribution (i.e., the real lenses) – a problem called
    distribution shift (Quinonero-Candela et al., [2008](#bib.bib61)). To overcome
    distribution shift, machine learning researchers have repurposed semi-supervised
    learning methods, which use unlabeled data and data augmentation to adapt the
    trained model to the test data (Berthelot et al., [2021](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: An advantage to the semi-supervised learning approach is that it can learn from
    the abundance of unlabeled images from the survey, which allows models to generalize
    better to unseen images. This is particularly useful to improve performance given
    millions of galaxy images that are detected in sky surveys but not included in
    the training data. The model performance is further improved through augmentations
    applied to images during training (e.g., translation and rotation). In addition
    to conventional transformations, a rich source of data augmentation can be derived
    by making use of unsupervised learning algorithms (e.g., Goodfellow et al., [2014](#bib.bib22);
    Kingma & Welling, [2014](#bib.bib32); Erhan et al., [2010](#bib.bib16)). Given
    the range of methodologies available, we now seek to address the question of which
    combination of machine learning methods (supervised and semi-supervised) and augmentations
    are best suited for finding strong gravitational lenses.
  prefs: []
  type: TYPE_NORMAL
- en: We seek efficient models which minimize human effort by reducing the number
    of images that must be visually inspected to recover a given sample of lenses.
    In this work we apply CNN models to the Deep Lens Survey (DLS; Wittman et al.
    [2002](#bib.bib78)), which has relatively good image quality and also remains
    relatively unexplored in terms of machine learning searches, thus serving as a
    good testbed for this study. Also, because of the small size of known lenses from
    the DLS survey, we reserve those for use only in our test dataset. Training and
    validation datasets will only contain simulated lenses. In our previous methodology
    paper (Sheng et al., [2022](#bib.bib66), hereafter [S22](#bib.bib66)), we discussed
    the CNN models and lens detection techniques used in this work. Herein, we describe
    our training data in detail and focus on evaluating the performance of the different
    models on the DLS dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This paper is organized as follows. In Section [2](#S2 "2 Deep Lens Survey Data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") we give an overview of the Deep Lens Survey and our
    source selection used for this work. We summarize our machine learning architecture
    and learning methods in Section [3](#S3 "3 Deep Learning Architecture and learning
    methods used ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey"). Section [4](#S4 "4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") describes the method used to generate training, validation,
    and testing data from DLS images. Section [5](#S5 "5 Metric to evaluate model
    performance ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") discusses our metric to evaluate the performance
    of the different CNN models. We discuss the results from our experiments in Section [6](#S6
    "6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey"), including the sample of new lens
    candidates from DLS and spectroscopic confirmation of two systems. Finally, we
    summarize the main conclusions in Section [7](#S7 "7 Conclusions ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey"). Throughout this paper we use the AB magnitude system and a $\Lambda$CDM
    cosmology with $\Omega_{M}=0.3$, $\Omega_{\Lambda}=0.7$ and $\mathrm{H}_{0}=70$ km s^(-1) Mpc^(-1).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep Lens Survey Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we give a brief overview of imaging data from the Deep Lens Survey (DLS)
    which we use to test and optimize strong lens detection methods. The DLS consists
    of relatively deep imaging over 20 square degrees in five independent $2^{\circ}\times
    2^{\circ}$ fields which are widely separated in the sky (Wittman et al., [2002](#bib.bib78)).
    Each field was imaged in BVRz photometric filters (Schmidt & Thorman, [2013](#bib.bib62))
    using the 4-meter Mayall telescope at Kitt Peak National Observatory or Blanco
    telescope at Cerro Tololo Inter-American Observatory, depending on declination.
    The survey was carried out over $\sim$120 nights. The survey was designed for
    weak gravitational lensing measurements, with stringent requirements on image
    quality and limiting magnitude, such that the data are naturally well suited for
    identifying strong lens systems. Typical $5\sigma$ point-source detection limits
    are 25.8, 26.3, and 26.9 AB magnitude in the $B$, $V$, and $R$ filters respectively
    (Schmidt & Thorman, [2013](#bib.bib62)). The $R$ band limit is only $\sim$0.6
    magnitudes shallower than the expected depth to be reached by Rubin observatory’s
    10-year survey (Ivezić et al., [2019](#bib.bib29)). The seeing is by design best
    in the R band (FWHM$\lesssim$0$\aas@@fstack{\prime\prime}$9) and is typically
    $\gtrsim$0$\aas@@fstack{\prime\prime}$9 in the B, V, and z bands (Wittman et al.,
    [2002](#bib.bib78)). Images in the $z$ band are shallowest and typically subject
    to worse seeing conditions. In this paper, we use only the $BVR$ data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Source selection and regions of interest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The DLS catalog includes $\sim$5 million detected galaxies across 20 square
    degrees. However, only those of moderate redshift and relatively high mass will
    act as detectable strong lenses (i.e., with Einstein radii $\Theta_{E}\gtrsim
    1$ arcsecond). We applied a magnitude cut of $17.5<R<22$ (similar to that used
    by Jacobs et al. [2017](#bib.bib30)) in order to remove objects which are unlikely
    to produce a detectable lensing effect. Additionally, we use SExtractor (Bertin
    & Arnouts, [1996](#bib.bib8)) flags to eliminate saturated low-redshift galaxies,
    and exclusion masks to remove galaxies around bright stars or at the edge of the
    field. This results in 281,425 objects (hereafter referred to as the SurveyCatalog).
    We find that SExtractor flags and exclusion masks remove $\sim 5\%$ of the galaxies
    from the survey which reduces the effective sky area probed by our SurveyCatalog
    to $\sim 19$ square degrees. We set aside 2277 ($\sim$0.8%) randomly sampled object
    images from this catalog to experiment and tune the HumVI scaling parameters (discussed
    in Section [4.1](#S4.SS1 "4.1 Generating the NonLenses dataset ‣ 4 Training and
    Validation data ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). All model training analysis in this paper pertains
    to the remaining set of 279,149 objects (hereafter referred to as the TrainCatalog).
  prefs: []
  type: TYPE_NORMAL
- en: For our analysis we extract image cutouts spanning 25$\aas@@fstack{\prime\prime}$7
    $\times$ 25$\aas@@fstack{\prime\prime}$7 (100 $\times$ 100 pixels) centered on
    each object. This size is sufficient for galaxy- and group-scale lenses ($\Theta_{E}\lesssim
    12$”); we do not focus on the most massive cluster lenses which are already well
    cataloged (Ascaso et al., [2014](#bib.bib4)) and simpler to identify. We create
    color-composite images from the source $BVR$ FITS files for all targets in the
    SurveyCatalog (Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Generating the NonLenses dataset
    ‣ 4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"); discussed in detail in
    Section [4.1](#S4.SS1 "4.1 Generating the NonLenses dataset ‣ 4 Training and Validation
    data ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")). These color composite images have smaller file sizes
    compared to original data, enabling us to keep the rest of the analysis computationally
    efficient. These images are still able to capture the detected low-suface brightness
    features, while not saturating the brightest objects of interest for this work.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, they are better suited for the machine learning architecture and
    methods used in this work (discussed in Section [3](#S3 "3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning Architecture and learning methods used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The task at hand is to establish a machine learning (ML) algorithm that efficiently
    classifies the 281,425 color-composite images from the survey into lensed and
    non-lensed galaxies. Furthermore, by ranking the images from highest predicted
    probability of being a lens to lowest, we can order the images for human inspection.
    This requires the selection of an architecture (i.e., a function that takes images
    as input and gives prediction probabilities as output) and learning methods (i.e.,
    a way for our function to learn from the data). The key components of our ML training
    pipeline are a supervised convolutional neural net (CNN), domain adaptation with
    semi-supervised learning, and augmenting training samples with generative adversarial
    nets (GAN). A more detailed account of our ML method can be found in [S22](#bib.bib66).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Convolutional neural network architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b1586cf65e58aa051a84202fc5c4d064.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Schematic depiction of the ResNetV2 deep learning architecture used
    in this work. The input to the network is an RGB color-composite image generated
    from the BVR fits files (Section. [4.1](#S4.SS1 "4.1 Generating the NonLenses
    dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")), and the output
    is a value between 0 and 1 indicating the probability of the input image being
    a lens. The general network consists of three stacks, each containing $3n$ residual
    units. In this work, we use a stack size of $n=1$ resulting in a total of three
    residual units. Each residual unit consists of two sets of Batch Normalization
    (BN), Rectified Linear Unit activation function (ReLU), and Conv units, where
    Conv denotes a convolutional layer with kernel size $3\times 3$ and appropriate
    stride size. The network ends with global average pooling and a softmax layer.'
  prefs: []
  type: TYPE_NORMAL
- en: CNNs have previously been used for classifying and identifying lens candidates
    (e.g., Jacobs et al., [2017](#bib.bib30)). They are a specific form of neural
    network that learns translation invariant representations via trainable convolution
    kernels. This is particularly well suited to astronomical images where patterns
    are repeated throughout the sky. Deep CNNs are models where these learned non-linear
    representations of the image (called layers) are stacked on top of one another.
    Deep CNNs are trained using variations of stochastic gradient descent, where an
    objective function is evaluated on small subsets of the data, called mini-batches,
    and the parameters are updated by subtracting some fraction of the objective’s
    gradient.
  prefs: []
  type: TYPE_NORMAL
- en: There are many choices of how precisely these layers are constructed and combined,
    such as selection of the convolutional kernel size, number of output channels
    for each convolution layer, the non-linear activation function, and the incorporation
    of other layers that improve performance such as Batch Normalization (Ioffe &
    Szegedy, [2015](#bib.bib28)). All of these details together are called the model
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'We make use of the ResNet version-2 architecture (ResNetV2; He et al., [2016a](#bib.bib25),
    [b](#bib.bib26)) designed for the CIFAR10 dataset (Krizhevsky, [2009a](#bib.bib36)),
    shown schematically in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Convolutional neural
    network architecture ‣ 3 Deep Learning Architecture and learning methods used
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). It is one of the widely used industry standard networks
    for image classification problems (e.g., Litjens et al., [2017](#bib.bib47); Gu
    et al., [2018](#bib.bib23); Madireddy et al., [2019](#bib.bib49)). The ResNetV2
    used in this work consists of three stacks (see Figure [1](#S3.F1 "Figure 1 ‣
    3.1 Convolutional neural network architecture ‣ 3 Deep Learning Architecture and
    learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey"); green blocks) and each stack consists
    of $n$ residual unit blocks, where $n$ is a parameter to be chosen that controls
    the depth of the neural network. A deeper neural network has more learning capacity
    but requires more computational power and training samples. Each residual unit
    block consists of three convolution layers of kernel size $3\times 3$ and one
    skip connection. To match the feature map dimensions (width, height) and the number
    of channels between stacks, a few extra convolution layers are included at the
    input and the beginning block of each stack. Therefore, $9n+4$ convolution layers
    are present in the network in total. For all the models used in this work, we
    adopt $n=1$. With strided convolutions, the feature map dimensions to each stack
    decrease by a factor of $1/2$. The number of input and output channels to each
    stack are: $(16\rightarrow 64),~{}(64\rightarrow 128),~{}(128\rightarrow 256)$.'
  prefs: []
  type: TYPE_NORMAL
- en: The network ends with global average pooling, a fully-connected layer and softmax.
    The global average pooling constrains the output to be rotationally invariant.
    The softmax transforms the output to be a value between 0 and 1 which can be interpreted
    as a probability. Throughout this work, a value of 1 is designated for lensed
    candidates (referred to herein as Lenses) and 0 for nonlensed candidates (referred
    to as NonLenses).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Domain adaptation with semi-supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In supervised learning, our algorithm is trained via mini-batches of images
    $X$ and corresponding labels $y$ ($1$ for Lenses and $0$ for NonLenses). The algorithm
    then tries to learn the neural network parameters, collectively referred to as
    $\Theta$. The output of the neural network after the softmax activation produces
    a prediction $p_{\Theta}(X)$, which is our predicted probability of $X$ being
    a lens. Our supervised learning objective function is the cross-entropy loss function,
    denoted $\ell_{S}$, which is a measure of the quality of our predictions, $p_{\Theta}(X)$,
    when compared to the true labels, $y$. Merely using supervised learning does not
    perform well in the face of distributional shift, and we turn to semi-supervised
    learning (SSL) methods which make use of the unlabeled test data to adapt to this
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB-shuffle | Randomly perturb the order of the channels in the images |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| JPEG-quality | 50-100% |'
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | Randomly rotate the images by a multiple of 90 degrees |'
  prefs: []
  type: TYPE_TB
- en: '| Translations | Randomly translate the images by at most 20 pixels in the
    up, down, left and right directions |'
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | Randomly flips the images across the x-axis |'
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | Randomly perturb the brightness(-0.1-0.1), saturation(0.9-1.3),
    hue(0.96-1.00), and gamma(1.23-1.25) of the images |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Data augmentations used on images in the semi-supervised training
    pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many semi-supervised approaches to deep learning. The methods we explore
    are FixMatch¹¹1FixMatch was not part of the original lens search study since this
    technique had not been published at the time. We are including it in our results
    here to be thorough. (Sohn et al., [2020](#bib.bib67)), MixMatch (Berthelot et al.,
    [2019](#bib.bib6)), Virtual Adversarial Training (Miyato et al., [2019](#bib.bib52)),
    Mean Teacher (Tarvainen & Valpola, [2017](#bib.bib74)), $\Pi$-Model (Laine & Aila,
    [2017](#bib.bib41)), and Pseudo-Labeling (Lee, [2013](#bib.bib44)).
  prefs: []
  type: TYPE_NORMAL
- en: Most SSL algorithms follow the same template. We minimize an objective function
    consisting of a supervised component (i.e. $\ell_{S}$ losses), where the label
    is provided, plus an unsupervised component (i.e. $\ell_{U}$ losses). Both are
    optimized together over mini-batches, now consisting of labeled and unlabeled
    data, but without significant modification to the stochastic gradient descent
    algorithm. The main feature that distinguishes our setting from typical SSL is
    that our training NonLenses and test set come from the same pool of data, while
    the simulated Lenses do not exist in the test data. This is in contrast to Jacobs
    et al. ([2017](#bib.bib30)) for example, in which they produce simulated NonLenses
    as well, but do not attempt domain adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: In the Pseudo-Label algorithm (Lee, [2013](#bib.bib44)), we assign pseudo-labels
    to unlabeled data by taking the model’s predicted class as the label. We can then
    use the same loss as in the supervised task (i.e., $\ell_{S}=\ell_{U}$). The motivation
    is that we are implicitly enforcing entropy minimization by forcing the model
    to be confident on unlabeled samples. An alternative approach to SSL is consistency
    regularization, where two independently augmented samples of the same test image
    are encouraged to produce similar predictions. The $\Pi$-model algorithm (Laine
    & Aila, [2017](#bib.bib41)) directly uses consistency regularization. The idea
    is to take two random augmentations of the same sample data point, $X$, and compute
    the squared difference of the model outputs for the augmented copies. We use $\text{aug},\widetilde{\text{aug}}$
    to denote two independent augmentations, which can be produced by selecting different
    randomization seeds. The unsupervised loss is then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\ell_{U}(X)=\left\&#124;p_{\Theta}(\text{aug}(X))-p_{\Theta}(\widetilde{\text{aug}}(X))\right\&#124;^{2}.$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: The choice of stochastic augmentation function is up to the modeler and will
    often be domain specific.
  prefs: []
  type: TYPE_NORMAL
- en: The Mean Teacher algorithm (Tarvainen & Valpola, [2017](#bib.bib74)) also uses
    consistency regularization, but replaces one of the augmentations in Equation [1](#S3.E1
    "In 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") with the output of the model using
    an exponential moving average (the teacher model) of model parameters, $\Theta$.
    FixMatch (Sohn et al., [2020](#bib.bib67)) and MixMatch (Berthelot et al., [2019](#bib.bib6))
    employ both consistency regularization and entropy minimization. MixMatch was
    originally proposed as a heuristic approach, and FixMatch was later derived as
    a more principled simplification of MixMatch and other related SSL methods. Virtual
    adversarial training (VAT; Miyato et al., [2019](#bib.bib52)) uses an adversarial,
    worst-case, augmentation. This adversarial augmentation pushes the image in the
    direction which will cause the greatest increase in loss. One downside to VAT
    is that the adversarial augmentations are not able to encode the domain specific
    prior information that random augmentations can provide (see Table [1](#S3.T1
    "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey")).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Data augmentation and GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data augmentation serves as a crucial regularizer in semi-supervised learning
    (SSL) algorithms. Several SSL algorithms, including those mentioned in this paper
    such as pi-model (Laine & Aila, [2017](#bib.bib41)), MixMatch (Berthelot et al.,
    [2019](#bib.bib6)), and fixMatch (Sohn et al., [2020](#bib.bib67)), utilize data
    augmentation techniques. The data augmentation techniques we employed in our study
    are provided in Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised
    learning ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey"), and are particularly well-suited for DLS images.
  prefs: []
  type: TYPE_NORMAL
- en: RGB-shuffle randomizes the order of channels and Color augmentation perturbs
    the colors in the images. These have the effect of accounting for systematic bias
    in channel and color information introduced by the simulation pipeline. JPEG-quality
    augmentation accounts for varying levels of noise and image quality, and applies
    to any color composite image irrespective of the format that the image is saved
    in (e.g., in this case we use png format instead of jpeg). Rot90, Translations,
    and Horizontal flips induce translational and rotational invariance in the predictions.
    Examples of these augmentations are shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.3
    Data augmentation and GANs ‣ 3 Deep Learning Architecture and learning methods
    used ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). We note that even though some augmentations (e.g.,
    RGB-shuffle) result in unrealistic images, our empirical tests described in Section [6.1.1](#S6.SS1.SSS1
    "6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with
    GANs and Augmentations have superior performance ‣ 6 Results and Discussion ‣
    Optimizing machine learning methods to discover strong gravitational lenses in
    the Deep Lens Survey") indicate that these augmentations yield improved model
    performance. Domain adaptation problems employing semi-supervised algorithms (SSLs)
    have been shown to benefit greatly from data augmentations in general (e.g., Sohn
    et al., [2020](#bib.bib67)), suggesting that this effect is not specific to our
    lens search.
  prefs: []
  type: TYPE_NORMAL
- en: A second tool that we use to augment our data is to generate new images that
    mimic the simulated lenses. In deep learning, the state-of-the-art method to produce
    generative models is by using Generative Adversarial Networks (GANs; Goodfellow
    et al., [2014](#bib.bib22); Arjovsky et al., [2017](#bib.bib3)). GANs generate
    unseen samples that are distinct from the original images, but are distributionally
    quite similar. These generative models are trained along with an adversarial discriminator
    that is attempting to distinguish between the fake and real images.
  prefs: []
  type: TYPE_NORMAL
- en: We trained a WGAN-GP (Wasserstein GAN + Gradient Penalty; Gulrajani et al.,
    [2017](#bib.bib24)) on simulated lenses and add the generated images (see examples
    in Figures [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey") and [4](#S4.F4
    "Figure 4 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")) to our training set as another form of data augmentation.
    The motivation is that GANs can provide a rich source of more exotic data augmentations.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey") gives a brief summary
    of the steps discussed thus far. The training, testing, and validation data along
    with the model checkpoints used in this paper are made available on our GitHub
    repository ²²2[https://github.com/sxsheng/SHLDN](https://github.com/sxsheng/SHLDN).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f510c0503d08844f1df839c1640e3068.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Refer to caption](img/a4c0b4e8dd2cae2185c4cfcf19f4b7f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Example of augmentations used during training. From left to right,
    the original RGB color composite image undergoes the series of augmentations described
    in Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning
    ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey"):
    RGB-shuffle, JPEG quality, Rot90, Translation, Flip, Color adjustment. The final
    image is then passed as input to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4ea8ff5ca0bd5d87cf19e8d4b45f0e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Schematic of the pipeline used in this work to test the performance
    of different learning methods described in Sections [3.2](#S3.SS2 "3.2 Domain
    adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture and learning
    methods used ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") and [3.3](#S3.SS3 "3.3 Data augmentation and
    GANs ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")
    (see text for details). The GAN generated lenses are only included in the training
    data for unsupervised learning methods (e.g., GAN+MixMatch).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Training and Validation data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the challenges that we face in gravitational lens searches is trying
    to generate a training and testing dataset when having limited knowledge of the
    type of strong lenses that we might find in a survey. Prior to this work, Kubo
    & Dell’Antonio ([2008](#bib.bib39)) used a semi-automated method to search for
    lensed candidates in one of the DLS fields (F2) and uncovered two lens candidates.
    But in order to train a machine learning model to recognize lenses, we require
    Lens and NonLens image samples on the order of a few thousand. This is not a problem
    for NonLens galaxies, as they are abundant. But this is challenging for Lenses,
    as the known samples are extremely small compared to training requirements. We
    note that although the DLS area overlaps with other surveys used for strong lens
    searches (e.g., SDSS), no lens candidates have been published from these other
    surveys within the DLS footprint. This is likely due to the shallower depth of
    other surveys (see Section [6.4](#S6.SS4 "6.4 Implications for future large-area
    sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")). We must therefore generate an artificial lens training set. We describe
    our process of generating the training and testing datasets in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Generating the NonLenses dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Color png images centered on each object in the SurveyCatalog are constructed
    from $BVR$ fits files using HumVI (Marshall et al., [2015](#bib.bib50)). HumVI
    is based on the color composition algorithm described in Lupton et al. ([2004](#bib.bib48))
    and offers several tunable parameters to control the output image (e.g., contrast).
    We randomly sample objects from the SurveyCatalog and visually inspect the effect
    of changing the HumVI parameters $s$ and $p$ which control the contrast and color
    balance respectively. Although there is a degeneracy in the choice of these values,
    we pick ones that reasonably represent both the bright and dim features in the
    data (i.e., spanning the range of detectable surface brightness). Table [2](#S4.T2
    "Table 2 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") lists our chosen HumVI parameters and Figure [4](#S4.F4
    "Figure 4 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") (top panel) shows 4 randomly selected color-composite
    survey images generated using these values. The chosen HumVI parameters are kept
    constant and applied to all images in the survey. It is beyond the scope of this
    work to explore the effect of choosing different HumVI parameters on the performance
    of the models, but we note that color augmentations applied during training (Table [1](#S3.T1
    "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey"); Section [6.1.1](#S6.SS1.SSS1
    "6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with
    GANs and Augmentations have superior performance ‣ 6 Results and Discussion ‣
    Optimizing machine learning methods to discover strong gravitational lenses in
    the Deep Lens Survey")) have the effect of making our models invariant to small
    perturbations in color.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/76f1bc3fcff40ad920ebab66d215aaa6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: *Top row:* Four randomly selected color composite survey images generated
    by running HumVI on their respective BVR FITS files. These images are examples
    of NonLenses used for training the network. Each image spans 25$\aas@@fstack{\prime\prime}$7
    $\times$ 25$\aas@@fstack{\prime\prime}$7 on the sky. Table [2](#S4.T2 "Table 2
    ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") lists the HumVI parameters used to generate these images. *Middle row:*
    The same set of survey images as in the top row, but superimposed with simulated
    lens configurations generated with glafic. Section [4.2](#S4.SS2 "4.2 Generating
    the simulated Lenses dataset ‣ 4 Training and Validation data ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")
    discusses the steps involved in detail. These images are examples of Lenses used
    during training. *Bottom row:* GAN generated simulated lenses. These are added
    to the training data as Lenses for our unsupervised models (e.g., GAN+MixMatch;
    Section. [3.3](#S3.SS3 "3.3 Data augmentation and GANs ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Parameter | Value |'
  prefs: []
  type: TYPE_TB
- en: '|  | glafic |  |'
  prefs: []
  type: TYPE_TB
- en: '| Position | $x_{\text{def}},y_{\text{def}},x_{\text{src}},y_{\text{src}}$
    | U(-0.5,0.5) |'
  prefs: []
  type: TYPE_TB
- en: '| (arcseconds) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| PA | $\theta_{\text{def}},\theta_{\text{src}}$ | U(0,180) |'
  prefs: []
  type: TYPE_TB
- en: '| (degrees) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ellipticity | $e_{\text{def}},e_{\text{src}}$ | U(0.3,0.7) |'
  prefs: []
  type: TYPE_TB
- en: '| Dispersion | $\sigma_{\text{def}}$ | U(250,450) |'
  prefs: []
  type: TYPE_TB
- en: '| (km s^(-1)) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $r_{\text{core, def}}$ | U(0,0.5) |'
  prefs: []
  type: TYPE_TB
- en: '| Brightness |  | U(200,600) |'
  prefs: []
  type: TYPE_TB
- en: '| (counts/$\text{pix}^{2}$) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Redshift | $z_{\text{def}}$ | U(0.3,0.7) |'
  prefs: []
  type: TYPE_TB
- en: '| Redshift | $z_{\text{src}}$ | U($z_{\text{def}}$ + 0.5, $z_{\text{def}}$
    + 2.5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | HUMVI |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | -s | 0.2,0.7,1.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | -p | 2.5, 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '|  | -m | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Values for the glafic and HumVI parameters used to generate the simulated
    arcs and png color-composite images respectively. $U(x_{min},x_{max})$ indicates
    that the value was sampled from a uniform distribution with $x_{min}$ and $x_{max}$
    being the minimum and maximum values.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Generating the simulated Lenses dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As described above, the scarcity of known lensed galaxies requires us to generate
    simulated lens samples for training ML models. Our approach is to add simulated
    lensed galaxies onto survey images, as has been used successfully in prior work
    (e.g., Jacobs et al., [2017](#bib.bib30); Jacobs et al., [2019](#bib.bib31)).
    For this work, we adopt an agnostic procedure for simulating lensed arcs which
    does not rely on photometric measurements of the deflector galaxy. We consider
    all galaxies which satisfy the magnitude cut criteria described in Section [2.1](#S2.SS1
    "2.1 Source selection and regions of interest ‣ 2 Deep Lens Survey Data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") (regardless of their color) for simulating the lensed arcs. We note that
    $\sim 50\%$ of the galaxies in our SurveyCatalog have a BPZ best fit photometric
    template from Schmidt & Thorman ([2013](#bib.bib62)) indicating that they are
    massive early-type galaxies at intermediate redshifts, and are indeed likely to
    act as strong lenses. We discuss the actual color distribution for lens candidates
    in Section [6.2.3](#S6.SS2.SSS3 "6.2.3 Distribution of lensed candidates in color-color
    space ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey").
  prefs: []
  type: TYPE_NORMAL
- en: Given any object from the training dataset, we assume that the central galaxy
    (“deflector”) is at a redshift $z_{\text{def}}\in[0.3,0.7]$ and is characterized
    by a Singular Isothermal Ellipsoid (SIE) mass density (Kormann et al., [1994](#bib.bib35)).
    The mass profile is dependent on the galaxy’s position ($x_{\text{def}},y_{\text{def}}$),
    ellipticity ($e_{\text{def}}$), position angle ($\theta_{\text{def}}$), velocity
    dispersion ($\sigma_{\text{def}}$), and choice of $r_{\text{core,def}}$. The values
    for these parameters are sampled from a uniform distribution spanning the ranges
    listed in Table [2](#S4.T2 "Table 2 ‣ 4.1 Generating the NonLenses dataset ‣ 4
    Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"). These values ensure that
    the resulting mass profile of the deflector is sufficient to produce a detectable
    lensing effect (i.e., $\Theta_{E}\gtrsim 1$ arcsecond). A background galaxy (“source”)
    is assumed to lie at a redshift $z_{\text{src}}$ with morphology given by a Sérsic
    profile parameterized by its position $(x_{src},y_{src})$, central brightness
    (in units of counts/$\text{pix}^{2}$), ellipticity ($e_{\text{src}}$), position
    angle ($\theta_{\text{src}}$), and a Sérsic index of 1\. The value for $z_{\text{src}}$
    is randomly chosen from a uniform distribution between $z_{\text{def}}+0.5$ and
    $z_{\text{def}}+2.5$. These values for the deflector and source redshifts are
    typical of spectroscopically measured values from previous strong lens surveys
    (e.g., Sonnenfeld et al., [2013](#bib.bib68); Bolton et al., [2008](#bib.bib9);
    Tran et al., [2022](#bib.bib75)).
  prefs: []
  type: TYPE_NORMAL
- en: The light from the background galaxy is traced using glafic (Oguri, [2010](#bib.bib56))
    to produce a simulated lensed arc in the image plane. The simulated lensed arcs
    are convolved with the point spread function (PSF) of the survey, scaled by a
    factor of (1,1.5,3) for the BVR filters, and then added to the $BVR$ fits images
    of the galaxy. We model the PSF of the survey in all the three filters as a 2D
    Gaussian kernel with a FWHM of $\sim$1 arcsecond corresponding to the approximate
    average seeing conditions. In addition to smoothing, we add Poisson noise in order
    to produce more realistic simulated arc images. The fits images are converted
    to a color png image using HumVI (as described in Section [4.1](#S4.SS1 "4.1 Generating
    the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). For
    this paper, we focus on generating moderately bright blue lensed arcs, and the
    parameter ranges that produce these configurations are listed in Table [2](#S4.T2
    "Table 2 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Generating the NonLenses
    dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey") illustrates
    common configurations of the arcs produced using this method. However, we note
    that the RGB-shuffle augmentation which is applied during training produces arcs
    of different colors (e.g., Figure [2](#S3.F2 "Figure 2 ‣ 3.3 Data augmentation
    and GANs ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")). We find that such an approach, where the simulated arcs are not dependent
    on the photometric properties of the central deflector galaxy, likely serves as
    an additional form of augmentation. This approach prevents over-fitting of our
    deep learning models while allowing for rapid prototyping and testing.
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 Generating the training datasets: TrainingV1 and TrainingV2'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the Lenses and NonLenses datasets, we construct two training sets: TrainingV1
    and TrainingV2\. The main difference between the two training sets is the number
    of labeled images used as Lenses and NonLenses. Prior work using CNNs (e.g., Jacobs
    et al., [2019](#bib.bib31)) have favored large training datasets (i.e., $\gtrsim$150,000
    galaxies). Therefore, for TrainingV1 we use 266,301 images for non-lenses and
    257,874 corresponding simulations as lenses (described in Section [4.2](#S4.SS2
    "4.2 Generating the simulated Lenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")). Since semi-supervised training requires both labeled
    and unlabeled data, TrainingV1 cannot be used to test semi-supervised learning
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: For TrainingV2, we choose the number of images for each class to be similar
    to those used in standard computer vision datasets such as Canadian Institute
    for Advanced Research-10 (CIFAR-10; Krizhevsky, [2009b](#bib.bib37)) and Street
    View House Numbers (SVHN; Netzer et al., [2011](#bib.bib55)) dataset. We use a
    set of 7,074 human-labeled objects as NonLenses and 6,929 corresponding simulations
    as Lenses. The human labeling was carried out on randomly chosen images from Field-1
    (F1) of the DLS. We note that the choice of labeling the data only from F1 does
    not affect the results presented in the rest of the paper (see Appendix [A](#A1
    "Appendix A Model performance and final lens sample ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). The
    259,248 NonLens images which are not part of TrainingV2 serve as unlabeled data
    for our semi-supervised learning methods (e.g., MixMatch; Section [3.2](#S3.SS2
    "3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")).
  prefs: []
  type: TYPE_NORMAL
- en: Counter-intuitively, we find that too much training data from simulated lenses
    and randomly selected NonLenses can hurt the performance of our algorithms. We
    refer readers to Section [6.1.2](#S6.SS1.SSS2 "6.1.2 Larger non-lens training
    samples can degrade the classifier’s performance ‣ 6.1 Semi-supervised algorithms
    with GANs and Augmentations have superior performance ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") and [S22](#bib.bib66) for further discussion of sample
    size effects, which can also contribute to differences in performance between
    the training sets. We note that the TrainingV2 labeled datasets are comparable
    to the size where we find peak performance.
  prefs: []
  type: TYPE_NORMAL
- en: We performed a 90-10 split for both TrainingV1 and TrainingV2, where 90% of
    the data was allocated for training the ResNetV2 model and 10% was kept aside
    for validation. We chose the maximum number of epochs (passes through the training
    dataset) for each training combination as 100, since this was sufficient to observe
    a plateau in the validation metrics. For each of the training combination described
    in Section [3](#S3 "3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey"), we conducted four independent trials and selected the checkpoint with
    the best validation metrics for testing it on the survey data.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Metric to evaluate model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have described several models which are each tuned to optimize validation
    accuracy, which is measured on the validation dataset (Section [4.3](#S4.SS3 "4.3
    Generating the training datasets: TrainingV1 and TrainingV2 ‣ 4 Training and Validation
    data ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")) consisting of simulated Lenses and survey NonLenses.
    In order to gauge the performance of the models on their ability to find real
    lenses from the survey, we require a testing dataset consisting of lenses from
    the survey, as well as a metric to evaluate them on.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Generating the Testing dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Curating testing data in our case is a challenging task. As discussed earlier,
    only two strong lenses in the entire survey were known prior to this work, which
    is insufficient for meaningful evaluation. Therefore, we use an ensemble of 5
    ResNet models trained on simulated lenses but using polar transformed images as
    input to the network. The exclusive task of this model is to find real lens candidates
    to add to our test dataset. We emphasize that this model is independent of the
    rest of the models discussed so far in this paper, and does not influence their
    performance in any way. Details of its implementation are discussed in [S22](#bib.bib66).
    It is beyond the scope of this paper to quantify the performance of ensemble models
    or the effect of polar transformation during training, but it is an interesting
    avenue for future work.
  prefs: []
  type: TYPE_NORMAL
- en: 'We find 52 likely lens candidates from this model, of which 27 are deemed to
    be good candidates upon visual inspection. Therefore, we create two testing datasets:
    TestV1 and TestV2\. TestV1 contains all the 52 lens candidates found using our
    ensemble model approach, while TestV2 contains the 27 best visual candidates.
    NonLenses for both TestV1 and TestV2 were formed by randomly selecting 874 of
    our 8734 human-labeled non-lenses (Section [4](#S4 "4 Training and Validation
    data ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Precision and Recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A standard metric widely used in machine learning to evaluate the performance
    of test data on a trained model is the Precision-Recall curve (PR curve), where
    precision and recall are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}},\quad\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}.$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Here TP, FP, and FN are the number of True Positive, False Positive, and False
    Negative images respectively. These values are computed by passing a labeled test
    dataset (TestV1 and TestV2 in this case) through a trained model (e.g., GAN+Mixmatch)
    and setting different prediction thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Since the primary goal of this work is to find models which minimize the number
    of nonlensed images that an investigator encounters while maximizing the number
    of lensed images found (i.e., less FP and FN values), we seek models which have
    high precision at high recall. We present the results from our PR curve analysis
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Results and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We consider 17 variations on the learning approaches described in Section [3](#S3
    "3 Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey"): 4 supervised,
    6 semi-supervised, and 7 semi-supervised with GANs. SupervisedV1 and SupervisedV2
    are our baseline models. They were trained using a supervised learning approach
    with no data augmentation on TrainingV1 ($\sim$250,000 Lenses and NonLenses) and
    TrainingV2 ($\sim$7000 Lenses and NonLenses) respectively. On the other hand,
    SupervisedV1+DA and SupervisedV2+DA were trained using supervised learning with
    data augmentation (DA). The rest of the models were trained on TrainingV2 using
    semi-supervised learning methods with DA or with DA + GANs. In this subsection,
    we summarize the performance of these different models. We primarily use the PR
    curve (Section [5.2](#S5.SS2 "5.2 Precision and Recall ‣ 5 Metric to evaluate
    model performance ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")) evaluated on our TestV1 and TestV2 sets to gauge
    which models perform best. We note that our methodology paper [S22](#bib.bib66)
    includes an additional discussion of these results.'
  prefs: []
  type: TYPE_NORMAL
- en: We plot the PR curve obtained for our best-performing baseline models (SupervisedV1,
    SupervisedV2) along with a subset of semi-supervised and GAN+semi-supervised models
    in Figure [5](#S6.F5 "Figure 5 ‣ 6.1.1 Ablation study on data augmentations ‣
    6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") (see Tables 3 and 4 of [S22](#bib.bib66)
    for additional model results). Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation study
    on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") lists
    the precision value obtained for a subset of models at 100% recall. We find that
    our models tend to generalize poorly when trained without any augmentations. Our
    baseline models, trained without any data augmentation, performed worst out of
    all models at every recall level. For example, at 100% recall, the baseline SupervisedV1
    and SupervisedV2 have a precision of $\sim 3\%$ on our TestV2 set, whereas the
    GAN+$\Pi$-model has a precision of $\sim 22\%$. The poor precision values of our
    supervised models may reflect challenges in simulating the characteristics of
    lenses from a survey given limited priors. Fortunately, we find that data augmentation
    methods are able to address this problem. We find a factor $\sim$5-10$\times$
    improved precision across almost all recall levels when applying the full set
    of augmentations (Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised
    learning ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")) to our supervised models.
  prefs: []
  type: TYPE_NORMAL
- en: The improvement of semi-supervised over supervised algorithms suggests that
    valuable features can in fact be extracted from the mostly unlabeled NonLenses,
    providing benefits in the classification of real lenses. Adding GAN images to
    our training pipelines had a seemingly profound impact at all recall levels, especially
    at higher recalls where more difficult-to-classify images come into play. This
    suggests that GAN-generated images contain subtle variations which, while not
    necessarily significant to the naked eye, do in fact produce a strong regularizing
    effect when used in training.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Ablation study on data augmentations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We investigated the impact of each of the data augmentations we used by doing
    an ablation study using TrainingV2\. The results from this study are tabulated
    in Table [7](#A1.T7 "Table 7 ‣ Appendix A Model performance and final lens sample
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). We find that removing GAN images from the training
    sets causes a noticeable decrease in model performance at all recall levels, which
    agrees with our earlier conclusion. It also appears that color augmentations and
    JPEG quality play a very significant role in model performance. Including these
    three augmentations in our training pipelines is apparently what allows our model
    to generalize so well, despite relying on simulated lenses for training. A curious
    result from this ablation study is that multiples of 90-degree rotations actually
    had a negative effect on model performance. The difference in performance is relatively
    small compared to that seen for other augmentations (e.g., GANs), but persists
    at all recall rates. A possible reason for this could be our small validation
    and test sets. Because the validation set is small, model selection may be biased
    towards certain orientations of the image. Likewise, an equally small test set
    may have preferred orientations that the model does not generalize to, resulting
    in degraded performance.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/4f5845efbceae76c9bfe98ec0c78b76e.png) | ![Refer to
    caption](img/b563c47b7a3161465dce364eeaadd948.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5: Precision-Recall curves (PR curves) for a subset of the models described
    in Section [3](#S3 "3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") obtained using TestV1 (*Left*) and TestV2 (*Right*). TestV1 contains
    52 lens candidates found using our ensemble model approach, while TestV2 contains
    the 27 best visual candidates (Section [5.1](#S5.SS1 "5.1 Generating the Testing
    dataset ‣ 5 Metric to evaluate model performance ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). SupervisedV1
    and SupervisedV2 are our baseline models. They were trained using a supervised
    learning approach with no data augmentation on TrainingV1 ($\sim$250,000 Lenses
    and NonLenses) and TrainingV2 ($\sim$7000 Lenses and NonLenses) respectively.
    The rest of the models were trained on TrainingV2 with augmentations. MixMatch
    and $\Pi$-Model are semi-supervised learning approaches, whereas GAN+MixMatch
    and GAN+$\Pi$-Model use GAN generated images along with semi-supervised learning
    (see Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey") and Section [3](#S3
    "3 Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") for
    details). GAN+SupervisedV2 uses supervised learning with GAN generated images.
    Models which use semi-supervised learning along with GANs clearly outperform our
    baseline supervised learning models at all recall values, with GAN+$\Pi$-model
    having the highest precision at 100% recall (see results in Table [3](#S6.T3 "Table
    3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms
    with GANs and Augmentations have superior performance ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"); we note that Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation
    study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") reports
    the average of our four runs while this figure shows the runs with the best precision).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Training data used | TestV1 Precision(%) | TestV2 Precision(%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SupervisedV1 | TrainingV1 w/ no augmentation | 5.62$\pm$0.01 | 3.01$\pm$0.02
    |'
  prefs: []
  type: TYPE_TB
- en: '| SupervisedV2 | TrainingV2 w/ no augmentation | 5.65$\pm$0.02 | 3.06$\pm$0.04
    |'
  prefs: []
  type: TYPE_TB
- en: '| MixMatch | TrainingV2 w/ augmentation | 12.28$\pm$5.09 | 6.84$\pm$3.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $\Pi$-Model | TrainingV2 w/ augmentation | 13.41$\pm$2.33 | 8.68$\pm$1.49
    |'
  prefs: []
  type: TYPE_TB
- en: '| GAN + Supervised | TrainingV2 w/ augmentation | 8.25$\pm$2.85 | 6.05$\pm$2.69
    |'
  prefs: []
  type: TYPE_TB
- en: '| GAN + MixMatch | TrainingV2 w/ augmentation | 14.13$\pm$6.53 | 7.97$\pm$3.93
    |'
  prefs: []
  type: TYPE_TB
- en: '| GAN + $\Pi$-Model | TrainingV2 w/ augmentation | 15.2$\pm$6.21 | 22.27$\pm$7.71
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Average precision values were obtained for a subset of the models
    tested at $100\%$ recall. We note that a table with the performance of all the
    models at various recall values is presented in [S22](#bib.bib66). Here the average
    is computed from the performance of four independent runs on the test sets. The
    uncertainties are $1\sigma$ standard deviations from the mean.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f9feb74cd6d3ef4bb4a3b2d24875a6c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Grade-A lenses found in the DLS along with the rank (Section [6.2](#S6.SS2
    "6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")) assigned to them by GAN+MixMatch(MM) and GAN+$\Pi$-model(PI) models.
    All Grade-A lenses have a clear arc morphology and are located near a moderately
    massive galaxy or group, making them convincing lens candidates. Among these candidates,
    212072337 and 432021600 have been spectroscopically confirmed to be true strong
    lens systems (Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Spectroscopic confirmation of
    two Grade-A lenses ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/af68b7ece7173fd5a27bd31f5fa26f59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Grade-B lenses found along with the rank (Section [6.2](#S6.SS2 "6.2
    Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey"))
    assigned to them by GAN+MixMatch(MM) and GAN+PiModel(PI) models. Targets in this
    category have either a tentative nebulous arc-like feature surrounding a massive
    galaxy, or have approximately linear extended morphology near an apparent galaxy
    group or cluster. It is hard to discern if these features correspond to lensed
    arcs or are caused by blending of multiple sources, hence the uncertain Grade-B
    classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Larger non-lens training samples can degrade the classifier’s performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To understand why our larger training set (TrainingV1) led to poorer generalization,
    we also performed a test where we fixed the number of simulated Lenses and varied
    the number of NonLenses in the dataset (see [S22](#bib.bib66), Table 5). As we
    gradually increased the number of NonLenses in the training data from 0 to 256,000,
    we saw that precision gradually increased and peaked at around 8000-16000 NonLenses,
    then started to significantly decrease to around $\sim$6% precision for nearly
    all recall levels. One possible explanation for this effect is that as we increase
    the number of NonLenses in training, we also increase the number of NonLens false
    positives which appear similar to real lenses in the survey data (and perhaps
    even more similar to real lenses than the simulations we use). As a result, the
    decision boundary for non-lenses overlaps more with the regions occupied by real
    lenses, leading to higher levels of misclassification. Therefore, care must be
    taken in constructing training data based on simulations. Arbitrarily increasing
    the size of the training data can evidently lead to significantly worse performance
    than using a smaller well-curated training set.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we find that models trained with a semi-supervised learning approach
    using TrainingV2 and GAN-generated images along with all of our proposed list
    of data augmentations have high precision values at all recall values. In particular,
    among the models tested, the top two performing models are GAN+MixMatch and GAN+$\Pi$-model.
    In the following subsection, we turn to apply these models to the full set of
    DLS survey images (i.e., SurveyCatalog in Section [2.1](#S2.SS1 "2.1 Source selection
    and regions of interest ‣ 2 Deep Lens Survey Data ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey"))
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Catalog of Lens candidates found
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having established which of our trained models perform best on our test set
    in terms of PR curves, we now turn to the key question of how many lenses are
    identified in the DLS and importantly, how much human inspection effort is required
    to find them.
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank | Number of unique | Number of | Number of | Total lenses | Number of
    | Number of | Number of |'
  prefs: []
  type: TYPE_TB
- en: '| threshold | lenses investigated | Grade-A lenses | Grade-A lenses | Grade-A
    | Grade-A lenses | Grade-A lenses | Grade-A lenses |'
  prefs: []
  type: TYPE_TB
- en: '|  | (G+MM,G+PI) | (G+MM) | (G+PI) | (both models) | (SupervisedV2) | (SupervisedV2+DA)
    | (SupervisedV2+DA+GAN) |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 9, 9 | 1 | 1 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 19, 16 | 1 | 3 | 3 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 67, 56 | 2 | 3 | 3 | 0 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 800 | 513, 430 | 4 | 3 | 4 | 1 | 2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2800 | 1735, 1459 | 6 | 5 | 8 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 4000 | 2459, 2076 | 7 | 5 | 9 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Comparison of the number of Grade-A lenses found by different models
    tested. The predictions from the models are ranked such that the most likely predicted
    lens has rank=1\. The rank threshold value sets the number of lenses that an investigator
    has to visually inspect. The left two columns show the chosen rank threshold and
    the number of unique lenses that it corresponds to (removing duplicates as described
    in Section [6.2](#S6.SS2 "6.2 Catalog of Lens candidates found ‣ 6 Results and
    Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). Our best performing models GAN+MixMatch (G+MM)
    and GAN+PiModel (G+PI) find 4 and 3 lensed candidates each among the top $\sim$500
    unique images (top 800 ranks), and 7 lensed candidates each when the top $\sim
    2300$ images are investigated. Combining the results from both the models, we
    find 9 Grade-A candidates (shown in Figure [6](#S6.F6 "Figure 6 ‣ 6.1.1 Ablation
    study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). The
    right three columns show the number of lenses found from the SupervisedV2, SupervisedV2+Data
    Augmentation(DA) and SupervisedV2+DA+GAN. Although they find fewer ($\lesssim
    50\%$) lens candidates than our best performing models, we can see that DA and
    GANs are able to boost the number of lenses found from 1 to 3 at a rank threshold
    of 800.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank | Number of unique | Number of | Number of | Total lenses | Total lenses
    |'
  prefs: []
  type: TYPE_TB
- en: '| threshold | lenses investigated | Grade-B lenses | Grade-B lenses | Grade-B
    lenses | Grade-A+B lenses |'
  prefs: []
  type: TYPE_TB
- en: '|  | (G+MM,G+PI) | (G+MM) | (G+PI) | (both models) | (both models) |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 9, 9 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 19, 16 | 0 | 0 | 0 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 67, 56 | 0 | 2 | 2 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 800 | 513, 430 | 2 | 5 | 5 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2800 | 1735, 1459 | 6 | 11 | 12 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| 4000 | 2459, 2076 | 9 | 11 | 13 | 22 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Similar to Table [5](#S6.T5 "Table 5 ‣ 6.2 Catalog of Lens candidates
    found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey") but for Grade-B lenses.'
  prefs: []
  type: TYPE_NORMAL
- en: We obtain a $\sim$97% and $\sim$86% precision at 50% recall (i.e., to find 50%
    lenses from our test set) for the GAN + MixMatch and GAN + $\Pi$-model respectively.
    On the other hand, if we needed to reach $100\%$ recall (i.e., find all the lenses
    from our test set), the precision drops to $\sim 8\%$ and $\sim 22\%$ respectively
    (Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). Based on the results from Pourrahmani
    et al. ([2018](#bib.bib60)) who searched for gravitational lenses in the COSMOS
    field with excellent image quality from the Hubble Space Telescope, we expect
    a maximum of $\sim$7 Grade-A lenses per square degree. This gives an upper bound
    of $\lesssim$140 lens candidates in the 20 degree² of the DLS, where the number
    of detectable lenses will be smaller since many of the COSMOS lenses have Einstein
    radii which are too small to resolve in ground-based DLS data, and because the
    COSMOS data are more sensitive. We estimate that half of the COSMOS lenses are
    unresolved in DLS based on the distribution of Einstein radii of the sample, with
    median $\simeq$1$\aas@@fstack{\prime\prime}$2 reported by Pourrahmani et al. ([2018](#bib.bib60)),
    such that we would expect $\sim$70 detectable lenses in the DLS survey area. At
    100% recall, 8% precision, and a TP$\approx$70, the number of false positive (FP)
    images that an investigator has to look at to find 70 lenses is $\sim 850$. If
    the number of detectable lenses in DLS is much lower, as suggested by samples
    reported from large ground-based campaigns such as the Dark Energy Survey (DES),
    then the total number of images and false positives which must be searched is
    correspondingly smaller. We also note that these estimates are based on the assumption
    that the precision values obtained from our test set also apply to the survey
    data. A decrease in this precision value would increase the number of FPs. Therefore,
    considering these uncertainties, for this work we visually examine the top 12,
    25, 100, 800, 2800, and 4000 predictions from the GAN+Mixmatch and GAN+$\Pi$-model,
    and investigate the number of lenses found. Throughout this paper, we focus only
    on using relative ranks (i.e., top $n$ prediction) to assess model performance
    since the distribution of absolute prediction threshold values (such as those
    employed in Jacobs et al. [2019](#bib.bib31)) can vary significantly between different
    models (Appendix [12](#A1.F12 "Figure 12 ‣ Appendix A Model performance and final
    lens sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). The absolute prediction values obtained from
    different models can be calibrated, for example by scaling the obtained model
    weights to the softmax layer, but this is beyond the scope of our study. We note
    that the relative ranks which we use in this study will be unaffected under such
    scaling transformations.
  prefs: []
  type: TYPE_NORMAL
- en: One substantial caveat when looking at the top $n$ predictions is that, due
    to the density of galaxies in the sky and our image selection method, the top
    predictions are not necessarily unique. For example, the top 25 predictions from
    the GAN+$\Pi$-Model contain 17 unique sources and 8 duplicates centered on different
    nearby objects (shown in Figure [13](#A1.F13 "Figure 13 ‣ Appendix A Model performance
    and final lens sample ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") in the Appendix). For the top 2800
    predictions, the number of unique candidates is $\sim 1600$ on average (i.e.,
    $\sim 40\%$ are repeated). Since this is a significant portion of the number of
    images and would increase human effort during labeling, we remove such repetitions
    based on their sky coordinates. Given our image size, we remove duplicates within
    a radius of 26 arcseconds of each object in the top $n$ predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining images are then replaced with a larger field of view, ensuring
    that a given region of the sky needs to be visually inspected only once. We note
    that removing duplicates is strictly a post-processing step. Two of us (KVGC and
    TJ) visually inspected the lens candidates and classified them into confidence
    categories: Grade-A, Grade-B, Grade-C, and non-lenses. Grade-A indicates a high
    likelihood of being a strong lens system, on the basis of a clear arc morphology
    and/or coincidence with a moderately massive group of galaxies. Grade-B lenses
    generally have a nebulous arc-like feature surrounding a massive galaxy and/or
    have approximated linear extended arc morphology near a group or cluster of galaxies.
    It is uncertain if these features are from the lens or the effect of blending
    multiple sources. Grade-C lenses (not discussed in this paper) are the lowest-confidence
    candidates which typically show blended arc-like features likely arising from
    spiral arms, tidal features, or asymmetric diffuse light from the onset of mergers.'
  prefs: []
  type: TYPE_NORMAL
- en: Figures [6](#S6.F6 "Figure 6 ‣ 6.1.1 Ablation study on data augmentations ‣
    6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") and [7](#S6.F7 "Figure 7 ‣ 6.1.1
    Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs
    and Augmentations have superior performance ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") show the color composite images for the 9 Grade-A and 13 Grade-B lenses
    found from the survey upon visually inspecting $\sim$ 2500 unique candidates (the
    top 4000 by rank). Their sky coordinates are listed in Table [8](#A1.T8 "Table
    8 ‣ Appendix A Model performance and final lens sample ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") in the
    Appendix. Several of the Grade-A lenses appear to be compound lenses or part of
    a moderately massive group or cluster of galaxies. This is interesting since our
    training data consists of only galaxy-galaxy lenses. This is likely due to the
    addition of GAN-generated images to our training data, as the GAN-generated images
    (Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey")) include irregularly
    shaped arcs.
  prefs: []
  type: TYPE_NORMAL
- en: DLS212072337 ($z=1.81$)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46a6ed83bb9c00a87a84c03fc89bf8ce.png)'
  prefs: []
  type: TYPE_IMG
- en: DLS432021848 ($z=1.94$; tentative)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/096f6830d97d090595d5865401bc071a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: *(*Top): NIRES spectra of Grade-A lens DLS212072337 at a redshift
    of $z=1.81$ with prominent [O iii] emission lines marked in blue. *(*Bottom):
    NIRES spectra of DLS432021848 showing the single emission line detected at $1.93\mu
    m$ which we tentatively identify as H$\alpha$ at $z=1.94$. In both panels the
    scaled sky spectrum is shown in orange (offset by -100), with gray shading denoting
    regions affected by strong sky lines.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e578a5a04701de6f764c635142856f80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Distribution of the top 4000 lenses found by GAN+$\Pi$-model in color-color
    space. The left panel shows $B-R$ vs $B-V$ and the right panel shows $B-R$ vs
    $R-z$. The images above show examples of galaxies found in the two regions of
    the left panel separated by the purple line. Low-z galaxy candidates are clustered
    in the region above the trend line whereas all of the Grade-A lens candidates
    are below it. The right panel additionally shows that lens candidates are typically
    redder in $R-z$ colors ($\gtrsim 0.5$). A color selection based on the purple
    lines in each panel would yield higher precision in our lens candidate samples
    while retaining nearly all of the most probable lenses.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Human inspection effort
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We now examine how much human effort is required to find the 22 Grade-A and
    B lens candidates. To quantify the effort we consider the number of lenses found
    at different ranks, listed in Table [5](#S6.T5 "Table 5 ‣ 6.2 Catalog of Lens
    candidates found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey"). The rank threshold
    determines the number of unique images which must be visually inspected. Looking
    at the top 800 predictions from the GAN+MixMatch and GAN+$\Pi$-model (corresponding
    to 513 and 430 unique lens candidates respectively), we find 4 and 3 Grade-A lenses,
    and 2 and 5 Grade-B lenses respectively. This is several times ($\gtrsim$3$\times$)
    higher sky density than has been found from the shallower ground-based DES survey,
    and smaller than the density found in COSMOS with HST, as expected. The number
    of lens candidates found increases to 9 Grade-A and 13 Grade-B candidates when
    the top 4000 candidates ($\sim$2500 unique images) are considered. This corresponds
    to $\sim$1 lens per deg² searched, which is $\gtrsim$10$\times$ higher sky density
    of lenses compared to previous shallower ground-based surveys (as we discuss in
    Section [6.4](#S6.SS4 "6.4 Implications for future large-area sky surveys: sensitivity
    and angular resolution ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, our supervised models (e.g., SupervisedV1, SupervisedV2) find
    $\lesssim 50\%$ of these top lens candidates. They also have lower precision values
    (Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")), with no compelling lenses found
    within the top $17$ candidates inspected (whereas G+PI finds 3 within this threshold
    range). This again highlights the value of adding data augmentation and GAN images.
    The SupervisedV2+DA+GAN model finds 3 times more lenses than SupervisedV2 within
    the same threshold range. These results demonstrate the efficiency with which
    the models explored in this work can find strong lenses.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Spectroscopic confirmation of two Grade-A lenses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While image morphology can provide compelling evidence for strong gravitational
    lensing, spectroscopic redshifts are the standard to unambiguously establish the
    lensing nature of a system. We have obtained spectroscopy with Keck Observatory
    to confirm the lensing nature of two Grade-A systems presented herein: DLS212072337
    and DLS432021848 (Figure [8](#S6.F8 "Figure 8 ‣ 6.2 Catalog of Lens candidates
    found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey")). Observations of the arcs
    were conducted with NIRES (Wilson et al., [2004](#bib.bib77)) on the Keck II telescope.
    Full details of the observations and data reduction are described in Tran et al.
    ([2022](#bib.bib75)), along with spectroscopic redshifts for DLS212072337 (reported
    as AGEL091935+303156). We find a secure redshift of $z_{\text{arc}}=1.81$ for
    DLS212072337 from detection of H$\alpha$ $\lambda$6564 and [O III] $\lambda\lambda$4960,5008
    emission lines. The deflector galaxy is at a redshift of $z_{\text{def}}=0.43$,
    based on stellar absorption features from optical SDSS/BOSS spectra.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We observed DLS432021848 with NIRES on 12 January 2022 using the same methodology.
    We obtained 6 exposures of 300 seconds each. We detect a single emission line
    at $~{}\lambda=1.93\mu\text{m}$ which we tentatively identify as either H$\alpha$
    at $z_{\text{arc}}=1.94$ or [O III] $\lambda$5008 at $z_{\text{arc}}=2.85$. However,
    we are unable to confirm the redshift with other strong lines, which fall in regions
    of poor atmospheric transmission at both potential redshifts. We find further
    support for the lensing nature of DLS432021848 from its morphology in follow-up
    HST imaging (discussed in Section [6.4](#S6.SS4 "6.4 Implications for future large-area
    sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")), which shows clear kurtosis and evidence of multiple lensed images.
    Thus we are reasonably confident that this is indeed a strong lensing system on
    the basis of high-resolution imaging, despite the limited spectroscopic information.
    Together with DLS212072337, these results give additional confidence in the sample
    of lens candidates presented in this paper and demonstrate that our methods are
    successful.'
  prefs: []
  type: TYPE_NORMAL
- en: We note that redshifts are known for two additional Grade-A candidate deflectors
    (DLS212148326, DLS421095124) from archival data. DLS212148326 is at $z_{\text{def}}=0.424$
    from SDSS/BOSS spectra, while DLS421095124 is part of a massive galaxy cluster
    spectroscopically confirmed at $z_{\text{def}}=0.680$ (Wittman et al., [2003](#bib.bib79);
    Wittman et al., [2006](#bib.bib80), reported as DLSCL J1055.2-0503). These redshifts
    are promising, as the distances and approximate masses are consistent with the
    deflection angles implied by the strong lensing interpretation of these images.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Distribution of lensed candidates in color-color space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The analysis and model performance described thus far in the paper is based
    on a source selection using an intentionally simple $R$ band magnitude cut and
    SExtractor flags (Section [2.1](#S2.SS1 "2.1 Source selection and regions of interest
    ‣ 2 Deep Lens Survey Data ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). We have demonstrated in the above
    sections that such cuts are sufficient to search for lensed candidates in the
    DLS. However, more sophisticated selections can increase the efficiency of lens
    searches. Here we briefly consider how color selection can provide higher-purity
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [9](#S6.F9 "Figure 9 ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") we show the distribution of Grade-A and B lenses
    from Section [6.2](#S6.SS2 "6.2 Catalog of Lens candidates found ‣ 6 Results and
    Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") in various color-color spaces, along with the
    top 4000 ranked images from the GAN-$\Pi$-model as an example. These colors generally
    correspond to the central (candidate deflector) galaxy. The top lens candidates
    are not distributed uniformly, and we demonstrate two color-color selections where
    the top candidates are clustered: $(B-V)<0.56(B-R)-0.02$ (purple line in left
    panel), and $R-z\gtrsim 0.4$ (right panel). Such simple color cuts can retain
    all Grade-A lenses while removing the majority of false positives, thereby reducing
    the required human inspection effort. Physically, these colors are indicative
    of 4000 Å breaks at redshifts $z\gtrsim 0.25$ (i.e. in the $V$ or $R$ band) whereas
    lower-$z$ galaxies are less likely to act as strong lenses.'
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of lens candidates in color space suggests that the precision
    of our models can be further improved by adopting color criteria as a pre- or
    post-processing step, with minimal loss of the best candidates. Using photometric
    redshift and mass estimates is a similar and potentially even more promising method
    (Schmidt & Thorman, [2013](#bib.bib62)) although it is beyond the scope of this
    paper. Alternatively, a state-of-the-art automated means to address this would
    be by using self-similarity based approaches (e.g., Stein et al., [2021](#bib.bib72)),
    wherein a CNN further classifies the lens probabilities based on their similarity
    with each other.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Simulated Lens |  |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/fd799fc2a314bccb6e59b862a2f3f466.png) | ![Refer to
    caption](img/fe9f67eca2d0530e8540eb15d54696bc.png) | ![Refer to caption](img/0dfcf8170d042162540146bb64a0ba07.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Lenses found in DLS |  |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/4acf4a18c43c2ec72f0054cfc7f39d80.png) | ![Refer to
    caption](img/f7a9f921b1ac930c89460311201f9fdb.png) | ![Refer to caption](img/c2ac9a854f0ef76f9df6d7488a15d049.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/96eb6a632a1d7da39266a3c7e3e4eb54.png) | ![Refer to
    caption](img/3c0405d32bf175fa35f6a135feae4a72.png) | ![Refer to caption](img/41fe0a3e6c3c951f843d02b47c2f7ac3.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | False Positive |  |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/4a0909392ae6cd5ed6e8b05174970daf.png) | ![Refer to
    caption](img/624900a89ff1d2211285c8e9b2ebccc3.png) | ![Refer to caption](img/2035371952e5a738f999d52fab84bddd.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/1ebe6fac7c53d3c2ce74cabbecccedcd.png) | ![Refer to
    caption](img/f3a9decd588bd5f2c1225dfb68120db6.png) | ![Refer to caption](img/2ab90d661252d8607fe4da82af929c98.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | NonLens |  |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/b86684d65f2f4f8fd1b65fd68d0a53bd.png) | ![Refer to
    caption](img/975e912c0c2dd45583a862792a0ea095.png) | ![Refer to caption](img/0b876ca983ef7a599fb443effe499c4b.png)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 10: Grad-CAM++ heatmaps for an example simulated lens, two Grade-A lenses,
    two false positive lenses, and a NonLens. The left column shows the color composite
    image obtained from HumVI and passed to the model. The right column shows the
    Gradcam++ heatmaps. The red and green shading indicates regions of high and moderate
    importance to the model, respectively, whereas blue represents low importance.
    The middle column shows the heatmaps superimposed on input images for visualization
    purposes. For the simulated lens, we can clearly see that the entire lensed arc
    region is taken into consideration. For the Grade-A lens candidates found in DLS,
    we also find that the lensed arc features are considered important by the model,
    despite a range of lensing morphologies and colors. This suggests that models
    have indeed successfully generalized to the survey data. Notably, the massive
    deflector (i.e., the luminous red galaxy) causing the lensing effect is not highlighted
    in the simulated or candidate lens systems. Additional objects in the field are
    also highlighted in heatmaps for the Grade-A lenses, which is also apparent in
    the False Positive and NonLens examples. In the case of the False Positives, the
    highlighted object distributions resemble an “Einstein cross” lens configuration.
    Heatmaps for all the Grade-A lenses are provided in Figure [15](#A1.F15 "Figure
    15 ‣ Appendix A Model performance and final lens sample ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") in the
    appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Lensing signatures identified by the models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now examine which features of the lens candidate images are most relevant
    for the model predictions. Deep neural networks (such as ResNetV2 used in this
    work) are often considered as “black boxes” with all input information collapsed
    to a simple prediction for the user to interpret. Having only a single output,
    it is impossible to discern which distinguishing features of a gravitational lens
    are actually being identified and considered by the models. Fortunately, in the
    past few years, there have been a variety of methods proposed to alleviate this
    such as occlusion methods, Guided Backprop (Springenberg et al., [2015](#bib.bib71)),
    CAM (Zhou et al., [2016](#bib.bib82)), Grad-CAM (Selvaraju et al., [2017](#bib.bib64)),
    Grad-CAM++ (Chattopadhay et al., [2018](#bib.bib12)), and DeepSHAP (Fernando et al.,
    [2019](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-based interpretation methods (e.g., Grad-CAM++) effectively compute
    gradients on intermediate feature maps of the network to determine the importance
    of a feature. These gradient maps can then be overlaid on top of the original
    input image, in order to assess which image regions are contributing most to the
    predicted output from the classifier. These methods are not without drawbacks
    (e.g., Adebayo et al., [2018](#bib.bib1)) but can provide valuable insight. Here
    we use Grad-CAM++ to analyze some of our trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [10](#S6.F10 "Figure 10 ‣ 6.2.3 Distribution of lensed candidates in
    color-color space ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") shows Grad-CAM++ heatmaps obtained for a few illustrative
    examples. We consider a simulated lens from the training data, real Grade-A lenses
    from the survey, false positive images (i.e., images which are classified as lenses
    but show no visual evidence of lensing), and a non-lens. In the case of the simulated
    lens, it is clear that the model is indeed making its prediction based on the
    lensed arc features. For the Grade-A lenses, the model does indeed discern the
    lensed arcs, but there are additional unrelated regions within the images that
    also influence its decision. Curiously, the central massive deflector galaxy is
    not highlighted in these cases. In the case of the false positives, the model
    encouragingly is not misled by the extended central galaxies, but rather the heatmap
    highlights multiple sources of similar color which surround the central galaxy.
    For example in the spiral galaxy false-positive image, it is clear that the model
    picks up on the three nearby red objects. The location and color of these nearby
    objects is indeed similar to plausible multiple-image lensing configurations.
    It thus appears that the model has successfully learned to identify the astrophysical
    signatures of strong lensing.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Finding red arcs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As discussed in Section [4.2](#S4.SS2 "4.2 Generating the simulated Lenses dataset
    ‣ 4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), our Lens dataset used for
    training only consists of lensed arcs with blue optical colors. However, it is
    encouraging that the models have also identified red arcs such as the system DLS212148326
    (Figure [6](#S6.F6 "Figure 6 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). The network may be learning to
    identify red arcs through color augmentations (Figure [2](#S3.F2 "Figure 2 ‣ 3.3
    Data augmentation and GANs ‣ 3 Deep Learning Architecture and learning methods
    used ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")). Although red-lensed arcs are known to exist, presumably
    a training dataset consisting of only blue arcs is not ideal to robustly search
    for and quantify them. It could be the case that adding more augmentations or
    fine-tuning existing ones might suffice to search for arcs of various colors.
    Alternatively, a broader range of arc colors could be used in the simulated training
    set, or a separate classifier could be constructed from a training set of red
    arcs. Given our adopted training set, we consider the number of red-lensed arcs
    found from this work to be a lower limit (relative to the blue arcs). Additionally,
    there are likely many fainter blue or red arcs which our training set does not
    represent, although the detection of fainter objects is naturally more challenging.
  prefs: []
  type: TYPE_NORMAL
- en: '6.4 Implications for future large-area sky surveys: sensitivity and angular
    resolution'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next generation of wide-area sky surveys is expected to uncover $\gtrsim
    10^{5}$ strong lens systems (e.g., Oguri & Marshall, [2010](#bib.bib57); Collett,
    [2015](#bib.bib14)). Here we consider the gain in lens detection with survey depth
    and angular resolution based on our DLS sample from Section [6.2](#S6.SS2 "6.2
    Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey").
    We compare the sky density of detected lens candidates with two other illustrative
    examples of CNN-based searches in Table [6](#S6.T6 "Table 6 ‣ 6.4 Implications
    for future large-area sky surveys: sensitivity and angular resolution ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey"). In our DLS search, we find $\sim$0.5 Grade-A
    lenses per square degree (or $\sim$1 Grade-A+B lenses per square degree). This
    is considerably larger than found in shallower surveys such as SDSS and DES, which
    have uncovered $\sim$0.1 lenses per square degree (in regions far from the galactic
    plane). While these surveys have a comparable seeing-limited resolution, sharper
    image quality enables more lenses to be found. An example is the search of COSMOS
    HST imaging by Pourrahmani et al. ([2018](#bib.bib60)) using a CNN approach, which
    found 13 Grade-A candidates and 70 Grade-A+B candidates in the 2 square degree
    field (i.e., $\sim$35 per square degree). Therefore, we see that the sky density
    of detectable strong lens systems increases by $\sim$10 times when going from
    shallower ground-based surveys (e.g., SDSS) to the DLS, and by another factor
    of $\gtrsim$10 when the angular resolution is improved by an order of magnitude
    with space-based HST imaging at modest depth. These results generally support
    the predictions of large lens samples which will become detectable with near-future
    surveys planned with the Rubin (LSST Science Collaboration et al., [2009](#bib.bib40)),
    Roman (Spergel et al., [2015](#bib.bib70)), and Euclid (Laureijs et al., [2011](#bib.bib42))
    observatories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To visually illustrate the detection of lenses at different depths and angular
    resolutions, Figure [11](#S6.F11 "Figure 11 ‣ 6.4 Implications for future large-area
    sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") compares DECaLS, DLS, and HST imaging³³3The HST image was secured as
    part of program HST-GO-16773 targeting lens candidates identified primarily in
    DES and DECaLS imaging (Tran et al., [2022](#bib.bib75)). In brief, the HST image
    in Figure [11](#S6.F11 "Figure 11 ‣ 6.4 Implications for future large-area sky
    surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") was taken with WFC3-IR in the F140W filter with $\sim$30 minutes of exposure
    time ($<$1 orbit), and reduced using standard procedures. Details of the HST program
    will be described in a forthcoming paper. for the Grade-A lens candidate DLS432021848
    found in this work. A blue arc is clearly visible in the DLS image and shows typical
    lensing morphology in the high-resolution HST image. However, the arc is only
    marginally visible in shallower DECaLS imaging. Indeed, most (if not all) of the
    Grade-A lens candidates found from this work would be difficult to detect in shallower
    imaging surveys (e.g., DECaLS; hence for example they are not included in the
    catalog of Huang et al. [2020](#bib.bib27)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f067c4629041fd17b96a99dc88b1ec9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Comparison of the image quality from different observations of the
    lens system DLS432021848, which shows a prominent blue arc in DLS imaging (below
    center of images; all panels show the same field of view). *Left*: The arc is
    apparent but not well detected in DECaLS imaging, which has modest sensitivity.
    This image would likely be flagged in a low-confidence category and indeed was
    not identified in previous lens searches (e.g., Huang et al., [2020](#bib.bib27)).
    *Middle*: DLS image of the target showing a prominent blue arc-like feature below
    the red deflector galaxy, characteristic of a gravitational lens system. The increased
    sensitivity of DLS compared to DECaLS imaging (Table [6](#S6.T6 "Table 6 ‣ 6.4
    Implications for future large-area sky surveys: sensitivity and angular resolution
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")) enables clear arc detection. *Right*:
    Near-infrared image of the same target observed with HST, with a diffraction-limited
    angular resolution approximately 6 times sharper than DLS or DECaLS images. The
    HST image reveals the lensed arc morphology at a high signal-to-noise ratio. This
    demonstrates the capabilities of a ground-based telescope at good depth (e.g.,
    DLS), and a diffraction-limited space-based telescope with moderate exposure time
    (e.g., HST).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Survey | Lenses found | 5$\sigma$ point | FWHM | References |'
  prefs: []
  type: TYPE_TB
- en: '|  | per sq.deg | source detection |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | (r/R/F814W-band |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | magnitude) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| DES/DECaLS | $\sim 0.1$ | 23.6 (r) | 0$\aas@@fstack{\prime\prime}$98 | J19
    |'
  prefs: []
  type: TYPE_TB
- en: '| DLS | 1 | 26.7 (R) | 0$\aas@@fstack{\prime\prime}$9 | This work |'
  prefs: []
  type: TYPE_TB
- en: '| COSMOS | $\sim 35$ | 27.2 (F814W) | 0$\aas@@fstack{\prime\prime}$07 | P18,K07
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Number of lenses found using machine learning methods per square degree
    of sky in different surveys, along with the $5\sigma$ point source detection depth
    and median angular resolution (given as the FWHM: full-width at half maximum).
    We note that CNN and grading methods employed to find lenses in each survey are
    different; the density of lenses should thus be treated as an approximate comparison.
    References are as follows. J19: Jacobs et al. ([2019](#bib.bib31)), P18: Pourrahmani
    et al. ([2018](#bib.bib60)), K07: Koekemoer et al. ([2007](#bib.bib33)).'
  prefs: []
  type: TYPE_NORMAL
- en: Given the detectability of many lens systems with upcoming surveys, it is clear
    that machine learning approaches (such as those we have explored here) will be
    vitally important for the efficient selection of large samples. We have also demonstrated
    the feasibility of spectroscopically following up on these moderately faint arc
    systems (Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Spectroscopic confirmation of two
    Grade-A lenses ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")), which will be vital for confirmation and subsequent
    analyses.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we have evaluated the performance of different CNN learning approaches
    and data augmentations on their ability to efficiently find gravitational lens
    candidates in the Deep Lens Survey. We make use of the deep learning architecture
    ResNet for our experiments, along with a training dataset consisting of simulated
    Lenses and survey image NonLenses. We demonstrate that by using these state-of-the-art
    semi-supervised learning approaches, we can greatly reduce the human effort required
    to find lensed candidates from a survey. We summarize our key results below.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Among 17 variants of learning approaches tested in this work, we find that our
    best performing models (i.e., those which have high precision and minimize false
    positives during human inspection) are GAN+MixMatch and GAN+$\Pi-$model. They
    have a precision of $\sim 86\%$ and $\sim 97\%$ at 50% recall and, $\sim 22\%$
    and $\sim 8\%$ at 100% recall respectively. In comparison, our supervised models
    have a precision of $\sim 3\%$ at 100% recall. This increase in the performance
    of the best models can be attributed largely to three factors. (1) They leverage
    data augmentation (Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised
    learning ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")) during training, which helps them to generalize better. (2) The datasets
    used to train these models to contain simulated Lenses as well as GAN-generated
    images (Section [3](#S3 "3 Deep Learning Architecture and learning methods used
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")), which serves as an additional form of data augmentation.
    (3) Both of these top models employ a semi-supervised learning approach (MixMatch,
    $\Pi$-model) which enables our methods to adapt to distributional shift (Section [3.2](#S3.SS2
    "3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). These results indicate that data
    augmentation, GANs, and semi-supervised learning are highly effective approaches
    for building an efficient lens classifier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We investigated the Grad-CAM++ feature maps (Section [6.3](#S6.SS3 "6.3 Lensing
    signatures identified by the models ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey"))
    used by our best performing models to make their predictions, finding that they
    indeed are influenced mostly by lensed arc regions and are generally not misled
    by other galaxies/artifacts (e.g., diffraction spikes) in the images. This supplements
    our results presented above that salient information regarding the arcs needed
    for classification has been successfully learned by the models through our methods.
    This is encouraging for future lens searches, since simulated Lenses used in this
    work are generated without relying on photometric data of the deflector galaxy
    (Section [4](#S4 "4 Training and Validation data ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")), making
    it simpler to automate the task of generating a training dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying the GAN+MixMatch and GAN+$\Pi$-model to the entire DLS survey, and
    visually inspecting the top $\sim 2500$ lens candidates, we find 9 Grade-A and
    13 Grade-B lensed candidates (22 in total). 3 out of the 9 Grade-A candidates
    are found within the top 17 ranked images. The number of lenses found in the DLS
    corresponds to $\sim 10\times$ higher sky density of lenses per deg² compared
    to the shallower DES/DECaLS survey imaging and supports predictions that vast
    numbers of lens systems ($\gtrsim 10^{5}$) will be detectable in the upcoming
    generation of sky surveys. We further confirmed the lensed nature of 2 Grade-A
    candidates with spectroscopy and high-resolution imaging, demonstrating that our
    methods are successful.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have generally explored methods intended to find as many lenses as possible
    while minimizing human inspection effort. While there are likely additional detectable
    lenses beyond those we have identified, it is encouraging that our models have
    been able to identify lenses that are not represented in the training set. In
    particular, our training set focused on blue lensed arcs, while our models also
    find red arc candidates such as DLS212072337 (Section [6.3.1](#S6.SS3.SSS1 "6.3.1
    Finding red arcs ‣ 6.3 Lensing signatures identified by the models ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")), although at a lower rank compared to the bluer
    lenses. Additional augmentation methods and/or training datasets may be able to
    provide further improvement for diverse lens system properties. Another straightforward
    improvement to our lens search efficiency is to include simple cuts in color-color
    space as demonstrated in Section [6.2.3](#S6.SS2.SSS3 "6.2.3 Distribution of lensed
    candidates in color-color space ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey"). Such cuts can help increase the model precision
    by excluding sources that are not likely to act as strong lenses based on their
    color and magnitude (which is physically related to their mass and distance).
    Since our sample is agnostic to color information, our results are well-suited
    for assessing the color space distribution of the best lens candidates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scope of our models is currently limited to the DLS. However, our methodology
    can be adapted for other data sets, and we note that the DLS fields overlap with
    wide-area surveys such as DECaLS and SDSS. Exploring ways to translate these models
    across surveys would be greatly beneficial. Finally, confirming the lensing nature
    of new candidates either through spectroscopy (Section [6.2.2](#S6.SS2.SSS2 "6.2.2
    Spectroscopic confirmation of two Grade-A lenses ‣ 6.2 Catalog of Lens candidates
    found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey")) or via arc morphology (Section [6.4](#S6.SS4
    "6.4 Implications for future large-area sky surveys: sensitivity and angular resolution
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")) is essential for a variety of
    investigations, including probes of galaxy evolution and cosmology. We have demonstrated
    the feasibility of confirming moderately faint arcs in our sample. Accomplishing
    confirmation for the thousands of lenses that will be discovered in forthcoming
    surveys (such as with Rubin/LSST, Roman, and Euclid) will aid in our understanding
    of the formation and evolution of galaxies and the contents of the Universe.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Imran Hasan, Sam Schmidt, David Wittman, Tony Tyson and Anupreeta More
    for their helpful discussions which greatly improved this work. We are immensely
    grateful to Brian Lemaux and Debora Pelliccia for manually labeling a subset of
    the survey data. We thank the referee for many helpful comments which improved
    the content and clarity of this manuscript. TJ and KVGC gratefully acknowledge
    financial support from NASA through grant HST-GO-16773, the Gordon and Betty Moore
    Foundation through Grant GBMF8549, the National Science Foundation through grant
    AST-2108515, and from a Dean’s Faculty Fellowship. Some of the data presented
    herein were obtained at the W. M. Keck Observatory, which is operated as a scientific
    partnership among the California Institute of Technology, the University of California
    and the National Aeronautics and Space Administration. The Observatory was made
    possible by the generous financial support of the W. M. Keck Foundation. The authors
    wish to recognize and acknowledge the very significant cultural role and reverence
    that the summit of Maunakea has always had within the indigenous Hawaiian community.
    We are most fortunate to have the opportunity to conduct observations from this
    mountain. Some of the results herein are based on observations with the NASA/ESA
    Hubble Space Telescope obtained from the Mikulski Archive for Space Telescopes
    at the Space Telescope Science Institute, which is operated by the Association
    of Universities for Research in Astronomy, Incorporated, under NASA contract NAS
    5-26555\. Support for program number HST-GO-16773 was provided through a grant
    from the STScI under NASA contract NAS5-26555.
  prefs: []
  type: TYPE_NORMAL
- en: Data availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data underlying this article are available on our GitHub repository ([https://github.com/sxsheng/SHLDN](https://github.com/sxsheng/SHLDN)).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adebayo et al. (2018) Adebayo J., Gilmer J., Muelly M., Goodfellow I., Hardt
    M., Kim B., 2018, in Proceedings of the 32nd International Conference on Neural
    Information Processing Systems. NIPS’18. Curran Associates Inc., Red Hook, NY,
    USA, p. 9525–9536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alard (2006) Alard C., 2006, arXiv e-prints, [pp astro–ph/0606757](https://ui.adsabs.harvard.edu/abs/2006astro.ph..6757A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky et al. (2017) Arjovsky M., Chintala S., Bottou L., 2017, in Precup
    D., Teh Y. W., eds, Proceedings of Machine Learning Research Vol. 70, Proceedings
    of the 34th International Conference on Machine Learning. PMLR, International
    Convention Centre, Sydney, Australia, pp 214–223, [http://proceedings.mlr.press/v70/arjovsky17a.html](http://proceedings.mlr.press/v70/arjovsky17a.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ascaso et al. (2014) Ascaso B., Wittman D., Dawson W., 2014, [MNRAS](http://dx.doi.org/10.1093/mnras/stu074),
    [439, 1980](https://ui.adsabs.harvard.edu/abs/2014MNRAS.439.1980A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belokurov et al. (2009) Belokurov V., Evans N. W., Hewett P. C., Moiseev A.,
    McMahon R. G., Sanchez S. F., King L. J., 2009, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2008.14075.x),
    [392, 104](https://ui.adsabs.harvard.edu/abs/2009MNRAS.392..104B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berthelot et al. (2019) Berthelot D., Carlini N., Goodfellow I., Papernot N.,
    Oliver A., Raffel C. A., 2019, in Wallach H., Larochelle H., Beygelzimer A., d'Alché-Buc
    F., Fox E., Garnett R., eds, , Advances in Neural Information Processing Systems
    32. Curran Associates, Inc., pp 5049–5059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berthelot et al. (2021) Berthelot D., Roelofs R., Sohn K., Carlini N., Kurakin
    A., 2021, arXiv preprint arXiv:2106.04732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertin & Arnouts (1996) Bertin E., Arnouts S., 1996, [A&AS](http://dx.doi.org/10.1051/aas:1996164),
    [117, 393](https://ui.adsabs.harvard.edu/abs/1996A&AS..117..393B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bolton et al. (2008) Bolton A. S., Burles S., Koopmans L. V. E., Treu T., Gavazzi
    R., Moustakas L. A., Wayth R., Schlegel D. J., 2008, [ApJ](http://dx.doi.org/10.1086/589327),
    [682, 964](https://ui.adsabs.harvard.edu/abs/2008ApJ...682..964B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bradač et al. (2002) Bradač M., Schneider P., Steinmetz M., Lombardi M., King
    L. J., Porcas R., 2002, [A&A](http://dx.doi.org/10.1051/0004-6361:20020559), [388,
    373](https://ui.adsabs.harvard.edu/abs/2002A&A...388..373B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cañameras et al. (2020) Cañameras R., et al., 2020, [A&A](http://dx.doi.org/10.1051/0004-6361/202038219),
    [644, A163](https://ui.adsabs.harvard.edu/abs/2020A&A...644A.163C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chattopadhay et al. (2018) Chattopadhay A., Sarkar A., Howlader P., Balasubramanian
    V. N., 2018, in 2018 IEEE Winter Conference on Applications of Computer Vision
    (WACV). pp 839–847, [doi:10.1109/WACV.2018.00097](http://dx.doi.org/10.1109/WACV.2018.00097)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chiba (2002) Chiba M., 2002, [ApJ](http://dx.doi.org/10.1086/324493), [565,
    17](https://ui.adsabs.harvard.edu/abs/2002ApJ...565...17C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collett (2015) Collett T. E., 2015, [ApJ](http://dx.doi.org/10.1088/0004-637X/811/1/20),
    [811, 20](https://ui.adsabs.harvard.edu/abs/2015ApJ...811...20C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diehl et al. (2009) Diehl H. T., et al., 2009, [ApJ](http://dx.doi.org/10.1088/0004-637X/707/1/686),
    [707, 686](https://ui.adsabs.harvard.edu/abs/2009ApJ...707..686D)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erhan et al. (2010) Erhan D., Courville A., Bengio Y., Vincent P., 2010, in
    Teh Y. W., Titterington M., eds, Proceedings of Machine Learning Research Vol.
    9, Proceedings of the Thirteenth International Conference on Artificial Intelligence
    and Statistics. PMLR, Chia Laguna Resort, Sardinia, Italy, pp 201–208, [http://proceedings.mlr.press/v9/erhan10a.html](http://proceedings.mlr.press/v9/erhan10a.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fassnacht et al. (2004) Fassnacht C. D., Moustakas L. A., Casertano S., Ferguson
    H. C., Lucas R. A., Park Y., 2004, [ApJ](http://dx.doi.org/10.1086/379004), [600,
    L155](https://ui.adsabs.harvard.edu/abs/2004ApJ...600L.155F)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fernando et al. (2019) Fernando Z. T., Singh J., Anand A., 2019, in Proceedings
    of the 42nd International ACM SIGIR Conference on Research and Development in
    Information Retrieval. ACM, [doi:10.1145/3331184.3331312](http://dx.doi.org/10.1145/3331184.3331312),
    [https://doi.org/10.1145%2F3331184.3331312](https://doi.org/10.1145%2F3331184.3331312)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garvin et al. (2022) Garvin E. O., Kruk S., Cornen C., Bhatawdekar R., Cañameras
    R., Merín B., 2022, arXiv e-prints, [p. arXiv:2207.06997](https://ui.adsabs.harvard.edu/abs/2022arXiv220706997G)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gavazzi et al. (2014) Gavazzi R., Marshall P. J., Treu T., Sonnenfeld A., 2014,
    [ApJ](http://dx.doi.org/10.1088/0004-637X/785/2/144), [785, 144](https://ui.adsabs.harvard.edu/abs/2014ApJ...785..144G)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gilman et al. (2019) Gilman D., Birrer S., Treu T., Nierenberg A., Benson A.,
    2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz1593), [487, 5721](https://ui.adsabs.harvard.edu/abs/2019MNRAS.487.5721G)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Goodfellow I. J., Pouget-Abadie J., Mirza M., Xu B.,
    Warde-Farley D., Ozair S., Courville A., Bengio Y., 2014, in Proceedings of the
    27th International Conference on Neural Information Processing Systems - Volume
    2. NIPS’14. MIT Press, Cambridge, MA, USA, p. 2672–2680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2018) Gu J., et al., 2018, [Pattern Recognition](http://dx.doi.org/https://doi.org/10.1016/j.patcog.2017.10.013),
    77, 354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulrajani et al. (2017) Gulrajani I., Ahmed F., Arjovsky M., Dumoulin V., Courville
    A. C., 2017, in Guyon I., Luxburg U. V., Bengio S., Wallach H., Fergus R., Vishwanathan
    S., Garnett R., eds,   Vol. 30, Advances in Neural Information Processing Systems.
    Curran Associates, Inc., [https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016a) He K., Zhang X., Ren S., Sun J., 2016a, in European conference
    on computer vision. pp 630–645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016b) He K., Zhang X., Ren S., Sun J., 2016b, in Proceedings of
    the IEEE conference on computer vision and pattern recognition. pp 770–778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2020) Huang X., et al., 2020, [The Astrophysical Journal](http://dx.doi.org/10.3847/1538-4357/ab7ffb),
    894, 78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ioffe & Szegedy (2015) Ioffe S., Szegedy C., 2015, in International conference
    on machine learning. pp 448–456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivezić et al. (2019) Ivezić Ž., et al., 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacobs et al. (2017) Jacobs C., Glazebrook K., Collett T., More A., McCarthy
    C., 2017, [MNRAS](http://dx.doi.org/10.1093/mnras/stx1492), [471, 167](https://ui.adsabs.harvard.edu/abs/2017MNRAS.471..167J)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacobs et al. (2019) Jacobs C., et al., 2019, [ApJS](http://dx.doi.org/10.3847/1538-4365/ab26b6),
    [243, 17](https://ui.adsabs.harvard.edu/abs/2019ApJS..243...17J)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Welling (2014) Kingma D. P., Welling M., 2014, Auto-Encoding Variational
    Bayes ([arXiv:1312.6114](http://arxiv.org/abs/1312.6114))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koekemoer et al. (2007) Koekemoer A. M., et al., 2007, [ApJS](http://dx.doi.org/10.1086/520086),
    [172, 196](https://ui.adsabs.harvard.edu/abs/2007ApJS..172..196K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koopmans et al. (2006) Koopmans L. V. E., Treu T., Bolton A. S., Burles S.,
    Moustakas L. A., 2006, [ApJ](http://dx.doi.org/10.1086/505696), [649, 599](https://ui.adsabs.harvard.edu/abs/2006ApJ...649..599K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kormann et al. (1994) Kormann R., Schneider P., Bartelmann M., 1994, A&A, [284,
    285](https://ui.adsabs.harvard.edu/abs/1994A&A...284..285K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky (2009a) Krizhevsky A., 2009a, Technical report, Learning multiple
    layers of features from tiny images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky (2009b) Krizhevsky A., 2009b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Krizhevsky A., Sutskever I., Hinton G. E., 2012, in
    Pereira F., Burges C. J. C., Bottou L., Weinberger K. Q., eds,   Vol. 25, Advances
    in Neural Information Processing Systems. Curran Associates, Inc., [https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubo & Dell’Antonio (2008) Kubo J. M., Dell’Antonio I. P., 2008, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2008.12880.x),
    [385, 918](https://ui.adsabs.harvard.edu/abs/2008MNRAS.385..918K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSST Science Collaboration et al. (2009) LSST Science Collaboration et al.,
    2009, arXiv e-prints, [p. arXiv:0912.0201](https://ui.adsabs.harvard.edu/abs/2009arXiv0912.0201L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laine & Aila (2017) Laine S., Aila T., 2017, ArXiv, abs/1610.02242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laureijs et al. (2011) Laureijs R., et al., 2011, arXiv e-prints, [p. arXiv:1110.3193](https://ui.adsabs.harvard.edu/abs/2011arXiv1110.3193L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) LeCun Y., Boser B., Denker J. S., Henderson D., Howard R. E.,
    Hubbard W., Jackel L. D., 1989, [Neural Computation](http://dx.doi.org/10.1162/neco.1989.1.4.541),
    1, 541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee (2013) Lee D.-H., 2013, ICML 2013 Workshop : Challenges in Representation
    Learning (WREPL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leethochawalit et al. (2016) Leethochawalit N., Jones T. A., Ellis R. S., Stark
    D. P., Richard J., Zitrin A., Auger M., 2016, [ApJ](http://dx.doi.org/10.3847/0004-637X/820/2/84),
    [820, 84](https://ui.adsabs.harvard.edu/abs/2016ApJ...820...84L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Li R., et al., 2020, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab9dfa),
    [899, 30](https://ui.adsabs.harvard.edu/abs/2020ApJ...899...30L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Litjens et al. (2017) Litjens G., et al., 2017, [Medical Image Analysis](http://dx.doi.org/https://doi.org/10.1016/j.media.2017.07.005),
    42, 60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lupton et al. (2004) Lupton R., Blanton M. R., Fekete G., Hogg D. W., O’Mullane
    W., Szalay A., Wherry N., 2004, [PASP](http://dx.doi.org/10.1086/382245), [116,
    133](https://ui.adsabs.harvard.edu/abs/2004PASP..116..133L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madireddy et al. (2019) Madireddy S., Li N., Ramachandra N., Butler J., Balaprakash
    P., Habib S., Heitmann K., 2019, arXiv e-prints, [p. arXiv:1911.03867](https://ui.adsabs.harvard.edu/abs/2019arXiv191103867M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marshall et al. (2015) Marshall P., Sandford C., More A., Buddelmeijerr H.,
    2015, HumVI: Human Viewable Image creation (ascl:1511.014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miranda & Macciò (2007) Miranda M., Macciò A. V., 2007, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2007.12440.x),
    [382, 1225](https://ui.adsabs.harvard.edu/abs/2007MNRAS.382.1225M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miyato et al. (2019) Miyato T., Maeda S., Koyama M., Ishii S., 2019, IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 41, 1979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More et al. (2016) More A., et al., 2016, [MNRAS](http://dx.doi.org/10.1093/mnras/stv1966),
    [455, 1191](https://ui.adsabs.harvard.edu/abs/2016MNRAS.455.1191M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moustakas et al. (2007) Moustakas L. A., et al., 2007, [ApJ](http://dx.doi.org/10.1086/517930),
    [660, L31](https://ui.adsabs.harvard.edu/abs/2007ApJ...660L..31M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Netzer et al. (2011) Netzer Y., Wang T., Coates A., Bissacco A., Wu B., Ng A. Y.,
    2011, in NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011\.
    [http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf](http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oguri (2010) Oguri M., 2010, [PASJ](http://dx.doi.org/10.1093/pasj/62.4.1017),
    [62, 1017](https://ui.adsabs.harvard.edu/abs/2010PASJ...62.1017O)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oguri & Marshall (2010) Oguri M., Marshall P. J., 2010, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2010.16639.x),
    [405, 2579](https://ui.adsabs.harvard.edu/abs/2010MNRAS.405.2579O)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paraficz et al. (2016) Paraficz D., et al., 2016, [A&A](http://dx.doi.org/10.1051/0004-6361/201527971),
    [592, A75](https://ui.adsabs.harvard.edu/abs/2016A&A...592A..75P)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pettini et al. (2002) Pettini M., Rix S. A., Steidel C. C., Adelberger K. L.,
    Hunt M. P., Shapley A. E., 2002, [ApJ](http://dx.doi.org/10.1086/339355), [569,
    742](https://ui.adsabs.harvard.edu/abs/2002ApJ...569..742P)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pourrahmani et al. (2018) Pourrahmani M., Nayyeri H., Cooray A., 2018, [ApJ](http://dx.doi.org/10.3847/1538-4357/aaae6a),
    [856, 68](https://ui.adsabs.harvard.edu/abs/2018ApJ...856...68P)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quinonero-Candela et al. (2008) Quinonero-Candela J., Sugiyama M., Schwaighofer
    A., Lawrence N. D., 2008, Dataset shift in machine learning. Mit Press
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidt & Thorman (2013) Schmidt S. J., Thorman P., 2013, [MNRAS](http://dx.doi.org/10.1093/mnras/stt373),
    [431, 2766](https://ui.adsabs.harvard.edu/abs/2013MNRAS.431.2766S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seidel & Bartelmann (2007) Seidel G., Bartelmann M., 2007, [A&A](http://dx.doi.org/10.1051/0004-6361:20066097),
    [472, 341](https://ui.adsabs.harvard.edu/abs/2007A&A...472..341S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selvaraju et al. (2017) Selvaraju R. R., Cogswell M., Das A., Vedantam R., Parikh
    D., Batra D., 2017, in Proceedings of the IEEE International Conference on Computer
    Vision (ICCV).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shajib et al. (2022) Shajib A. J., et al., 2022, arXiv e-prints, [p. arXiv:2210.10790](https://ui.adsabs.harvard.edu/abs/2022arXiv221010790S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheng et al. (2022) Sheng S., C K. V. G., Choi C. P., Sharpnack J., Jones T.,
    2022, [arXiv e-prints](http://dx.doi.org/10.48550/arXiv.2210.11681), [p. arXiv:2210.11681](https://ui.adsabs.harvard.edu/abs/2022arXiv221011681S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn et al. (2020) Sohn K., et al., 2020, in Larochelle H., Ranzato M., Hadsell
    R., Balcan M., Lin H., eds,   Vol. 33, Advances in Neural Information Processing
    Systems. Curran Associates, Inc., pp 596–608, [https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sonnenfeld et al. (2013) Sonnenfeld A., Treu T., Gavazzi R., Suyu S. H., Marshall
    P. J., Auger M. W., Nipoti C., 2013, [ApJ](http://dx.doi.org/10.1088/0004-637X/777/2/98),
    [777, 98](https://ui.adsabs.harvard.edu/abs/2013ApJ...777...98S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sonnenfeld et al. (2018) Sonnenfeld A., et al., 2018, [PASJ](http://dx.doi.org/10.1093/pasj/psx062),
    [70, S29](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..29S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spergel et al. (2015) Spergel D., et al., 2015, arXiv e-prints, [p. arXiv:1503.03757](https://ui.adsabs.harvard.edu/abs/2015arXiv150303757S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Springenberg et al. (2015) Springenberg J. T., Dosovitskiy A., Brox T., Riedmiller
    M. A., 2015, CoRR, abs/1412.6806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stein et al. (2021) Stein G., Harrington P., Blaum J., Medan T., Lukic Z., 2021,
    arXiv e-prints, [p. arXiv:2110.13151](https://ui.adsabs.harvard.edu/abs/2021arXiv211013151S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swinbank et al. (2009) Swinbank A. M., et al., 2009, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2009.15617.x),
    [400, 1121](https://ui.adsabs.harvard.edu/abs/2009MNRAS.400.1121S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tarvainen & Valpola (2017) Tarvainen A., Valpola H., 2017, in Guyon I., Luxburg
    U. V., Bengio S., Wallach H., Fergus R., Vishwanathan S., Garnett R., eds, , Advances
    in Neural Information Processing Systems 30. Curran Associates, Inc., pp 1195–1204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. (2022) Tran K.-V. H., et al., 2022, arXiv e-prints, [p. arXiv:2205.05307](https://ui.adsabs.harvard.edu/abs/2022arXiv220505307T)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Treu (2010) Treu T., 2010, [ARA&A](http://dx.doi.org/10.1146/annurev-astro-081309-130924),
    [48, 87](https://ui.adsabs.harvard.edu/abs/2010ARA&A..48...87T)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wilson et al. (2004) Wilson J. C., et al., 2004, in Moorwood A. F. M., Iye M.,
    eds, Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series
    Vol. 5492, Ground-based Instrumentation for Astronomy. pp 1295–1305, [doi:10.1117/12.550925](http://dx.doi.org/10.1117/12.550925)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wittman et al. (2002) Wittman D. M., et al., 2002, in Tyson J. A., Wolff S.,
    eds, Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series
    Vol. 4836, Survey and Other Telescope Technologies and Discoveries. pp 73–82 ([arXiv:astro-ph/0210118](http://arxiv.org/abs/astro-ph/0210118)),
    [doi:10.1117/12.457348](http://dx.doi.org/10.1117/12.457348)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wittman et al. (2003) Wittman D., Margoniner V. E., Tyson J. A., Cohen J. G.,
    Becker A. C., Dell’Antonio I. P., 2003, [ApJ](http://dx.doi.org/10.1086/378344),
    [597, 218](https://ui.adsabs.harvard.edu/abs/2003ApJ...597..218W)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wittman et al. (2006) Wittman D., Dell’Antonio I. P., Hughes J. P., Margoniner
    V. E., Tyson J. A., Cohen J. G., Norman D., 2006, [ApJ](http://dx.doi.org/10.1086/502621),
    [643, 128](https://ui.adsabs.harvard.edu/abs/2006ApJ...643..128W)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wuyts et al. (2014) Wuyts E., Rigby J. R., Gladders M. D., Sharon K., 2014,
    [ApJ](http://dx.doi.org/10.1088/0004-637X/781/2/61), [781, 61](https://ui.adsabs.harvard.edu/abs/2014ApJ...781...61W)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2016) Zhou B., Khosla A., A. L., Oliva A., Torralba A., 2016, CVPR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Model performance and final lens sample
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this appendix, we provide some additional details of the model performance
    and the top lens candidates identified in this work.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [12](#A1.F12 "Figure 12 ‣ Appendix A Model performance and final lens
    sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") shows the distribution of model scores across
    the different DLS fields (F1 to F5), demonstrating similar performance in each
    field. This is generally expected given the similar image quality across the DLS
    survey. Importantly it shows that our use of labeled training data from only F1
    does not substantially affect the model performance in the other fields.
  prefs: []
  type: TYPE_NORMAL
- en: Table [7](#A1.T7 "Table 7 ‣ Appendix A Model performance and final lens sample
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") lists the results from our ablation study discussed
    in Section [6.1.1](#S6.SS1.SSS1 "6.1.1 Ablation study on data augmentations ‣
    6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey"). We show the precision across recall
    rates from 50-100%. The performance differences are generally similar across all
    recall rates. Color augmentation, JPEG quality, and GAN images appear to most
    prominently improving the model performance (i.e., the models perform significantly
    worse when these augmentations are removed).
  prefs: []
  type: TYPE_NORMAL
- en: 'We show the top 25 predicted lens candidates from the GAN+$\Pi$-model and GAN+MixMatch
    models in Figures [13](#A1.F13 "Figure 13 ‣ Appendix A Model performance and final
    lens sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") and [14](#A1.F14 "Figure 14 ‣ Appendix A Model
    performance and final lens sample ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), respectively. These include
    several of our top lens candidates based on human inspection (see Figures [6](#S6.F6
    "Figure 6 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms
    with GANs and Augmentations have superior performance ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")) and [7](#S6.F7 "Figure 7 ‣ 6.1.1 Ablation study on
    data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")), but
    many do not show obvious signs of strong lensing. There are several duplicate
    images at slightly different sky positions as discussed in the main text. In Figure [15](#A1.F15
    "Figure 15 ‣ Appendix A Model performance and final lens sample ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")
    we include the GradCAM++ heatmaps obtained for all the Grade-A candidates (analogous
    to the example subsets shown in Figure [10](#S6.F10 "Figure 10 ‣ 6.2.3 Distribution
    of lensed candidates in color-color space ‣ 6.2 Catalog of Lens candidates found
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). These heatmaps were generated
    using our best performing models: GAN+MixMatch or GAN+$\Pi$-model (discussed in
    Section [6.3](#S6.SS3 "6.3 Lensing signatures identified by the models ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). Finally, we list the sky coordinates of all
    Grade-A and Grade-B lenses in Table [8](#A1.T8 "Table 8 ‣ Appendix A Model performance
    and final lens sample ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/d844e2b4ff9902248ebfe8d3e4a197b7.png) | ![Refer to
    caption](img/791cee5cba4f86510e6cb6085473d51b.png) | ![Refer to caption](img/13f743324dd7558609ee3a99a6312d28.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/81841bf07799d297df79e08d750c4144.png) | ![Refer to
    caption](img/a623961ba45e97cb6506825169ddbae8.png) |  |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 12: Histogram of the scores obtained by the GAN+MixMatch and GAN+$\Pi$-model
    in the five independent DLS Fields F1 through F5\. As discussed in Section [4](#S4
    "4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), the training set used to
    train our models (TrainingV2) contains human labeled NonLenses which were randomly
    sampled only from Field F1\. But as we clearly see, the distribution of scores
    (and performance of the models as a result) is independent of the field chosen.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Augmentation removed | TestV1 Precision(%) | TestV2 Precision(%) | TestV1
    baseline difference(%) | TestV2 baseline difference(%) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 50% recall rate |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| None | $84.95\pm 8.70$ | $80.19\pm 17.08$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | $65.46\pm 15.00$ | $55.77\pm 17.43$ | -19.49 | -24.42 |'
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $44.8\pm 24.32$ | $35.45\pm 21.75$ | -40.15 | -44.74 |'
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $65.30\pm 13.84$ | $56.84\pm 14.06$ | -19.65 | -23.35 |'
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $91.81\pm 4.41$ | $89.96\pm 6.33$ | +6.86 | +9.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Translations | $89.47\pm 10.34$ | $84.59\pm 13.04$ | +4.52 | +4.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $84.63\pm 10.85$ | $75.74\pm 13.96$ | -0.32 | -4.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $71.88\pm 17.35$ | $68.23\pm 15.2$ | -13.07 | -11.96
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 60% recall rate |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| None | $79.88\pm 6.30$ | $78.03\pm 12.45$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | $55.54\pm 13.13$ | $44.09\pm 12.94$ | - 24.34 | -33.94 |'
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $34.25\pm 24.49$ | $26.14\pm 15.9$ | - 45.63 | -51.89 |'
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $52.75\pm 24.68$ | $51.69\pm 17.91$ | - 27.13 | -26.34 |'
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $84.94\pm 7.04$ | $78.99\pm 5.23$ | +5.06 | + 0.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Translations | $74.95\pm 16.11$ | $72.26\pm 24.65$ | -4.93 | -5.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $71.15\pm 11.46$ | $60.64\pm 12.27$ | -8.73 | -17.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $63.24\pm 15.67$ | $50.91\pm 15.53$ | -16.64 | -27.12
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 70% recall rate |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| None | $69.42\pm 4.60$ | $68.05\pm 12.96$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | $46.45\pm 14.49$ | $37.24\pm 12.85$ | -22.97 | -30.81 |'
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $26.68\pm 17.28$ | $18.75\pm 11.78$ | -42.74 | -49.3 |'
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $40.38\pm 18.64$ | $40.52\pm 21.74$ | - 29.04 | -27.53 |'
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $79.97\pm 12.15$ | $73.69\pm 7.11$ | +10.55 | +5.64 |'
  prefs: []
  type: TYPE_TB
- en: '| Translations | $67.69\pm 12.67$ | $59.31\pm 22.34$ | -1.73 | -8.74 |'
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $67.36\pm 11.95$ | $55.94\pm 14.71$ | -2.06 | -12.11 |'
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $52.79\pm 14.92$ | $45.93\pm 13.97$ | -16.63 | -22.12
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 80% recall rate |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| None | $54.35\pm 4.57$ | $40.98\pm 17.12$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | $33.02\pm 10.26$ | $24.37\pm 10.50$ | -21.33 | -16.61 |'
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $15.34\pm 7.59$ | $11.75\pm 4.90$ | -39.01 | -29.23 |'
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $21.33\pm 8.29$ | $15.25\pm 3.41$ | -33.02 | -25.73 |'
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $62.63\pm 12.43$ | $52.2\pm 13.65$ | +8.28 | +11.22 |'
  prefs: []
  type: TYPE_TB
- en: '| Translations | $52.17\pm 12.92$ | $35.09\pm 16.64$ | -2.18 | -5.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $60.54\pm 9.78$ | $36.46\pm 9.03$ | +6.19 | -4.52 |'
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $39.47\pm 13.24$ | $33.84\pm 10.92$ | -14.88 | -7.14
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 90% recall rate |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| None | $34.12\pm 7.47$ | $16.33\pm 8.661$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | $22.60\pm 4.78$ | $10.99\pm 4.25$ | -11.52 | -5.34 |'
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $10.37\pm 4.68$ | $5.88\pm 2.61$ | -23.75 | -10.45 |'
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $15.15\pm 6.37$ | $6.18\pm 2.41$ | -18.97 | -10.15 |'
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $40.72\pm 12.74$ | $21.79\pm 3.08$ | +6.6 | +5.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Translations | $32.88\pm 3.35$ | $16.14\pm 6.72$ | -1.24 | -0.19 |'
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $30.81\pm 20.44$ | $14.72\pm 6.59$ | -3.31 | -1.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $23.86\pm 8.13$ | $14.25\pm 8.45$ | -10.26 | -2.08 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 100% recall rate |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| None | $8.25\pm 2.85$ | $6.05\pm 2.69$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | $8.13\pm 2.49$ | $5.79\pm 3.93$ | -0.12 | -0.26 |'
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $6.43\pm 0.97$ | $3.84\pm 1.02$ | -1.82 | -2.21 |'
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $5.88\pm 0.21$ | $4.14\pm 2.09$ | -2.37 | -1.91 |'
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $14.76\pm 8.42$ | $13.00\pm 5.16$ | +6.51 | +6.95 |'
  prefs: []
  type: TYPE_TB
- en: '| Translations | $10.83\pm 2.46$ | $7.37\pm 3.66$ | +2.58 | +1.32 |'
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $15.74\pm 3.06$ | $8.86\pm 1.84$ | +7.49 | +2.81 |'
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $11.42\pm 7.25$ | $6.84\pm 4.12$ | +3.17 | +0.79 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Ablation performance for 50–100% recall rates (in steps of 10%) for
    the GAN+Supervised model using TrainingV2\. The first row of each recall rate
    shows the baseline precision value obtained from the model on the test sets (TestV1,
    TestV2) when none of the augmentations are removed. In the subsequent rows, we
    report the precision obtained when the model was trained without the specified
    augmentation. For example, the baseline model at 50% recall has a precision of
    80.19% for TestV2 and decreases to 55.77% when GAN images are removed during training.
    The difference in the obtained precision values are quoted in the last two columns.
    Augmentations which improve model performance (i.e., improve precision when included
    and decrease decrease precision when removed) are shown in red, while those which
    decrease model performance are shown in blue. Overall, the models perform worse
    when color augmentations, JPEG quality and GANs are not included, indicating that
    these augmentations are important for optimal performance. The errrors quoted
    here are 1$\sigma$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/397c7f01d5a4158f1c06a41192cef2c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: DLS images of the top 25 predictions from GAN+$\Pi$-model. Several
    show clear evidence of strong lensing, while other images appear to be false positives.
    We note that many images are duplicates (at overlapping regions of the sky), which
    we remove before visual inspection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/418c06a7b48a4dc0eb406031822094d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Equivalent to Figure [13](#A1.F13 "Figure 13 ‣ Appendix A Model
    performance and final lens sample ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), showing the top 25 predictions
    from GAN+Mimatch.'
  prefs: []
  type: TYPE_NORMAL
- en: '| object ID | RA | DEC | Field | Rank (GAN+MixMatch) | Rank (GAN+$\Pi$-model)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Grade-A candidates |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 421095124 | 163.792076 | -5.070373 | F4 | 2 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| 513097468 | 209.340092 | -10.244328 | F5 | 38 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| 212072337 | 139.896040 | 30.532355 | F2 | 181 | 21 |'
  prefs: []
  type: TYPE_TB
- en: '| 322054393 | 79.839914 | -48.949647 | F3 | 733 | 8326 |'
  prefs: []
  type: TYPE_TB
- en: '| 432021600 | 162.750073 | -5.941902 | F4 | 1262 | 12424 |'
  prefs: []
  type: TYPE_TB
- en: '| 431010921 | 163.364259 | -5.789092 | F4 | 1279 | 19826 |'
  prefs: []
  type: TYPE_TB
- en: '| 512037933 | 209.677055 | -10.687652 | F5 | 7461 | 2068 |'
  prefs: []
  type: TYPE_TB
- en: '| 421117552 | 163.897903 | -5.054885 | F4 | 4799 | 2768 |'
  prefs: []
  type: TYPE_TB
- en: '| 212148326 | 139.512033 | 30.953524 | F2 | 3579 | 23223 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Grade-B candidates |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 313032462 | 78.742878 | -48.149829 | F3 | 365 | 59 |'
  prefs: []
  type: TYPE_TB
- en: '| 331108599 | 81.300608 | -49.432676 | F3 | 1974 | 98 |'
  prefs: []
  type: TYPE_TB
- en: '| 132023380 | 13.551513 | 11.794606 | F1 | 3400 | 462 |'
  prefs: []
  type: TYPE_TB
- en: '| 533097114 | 209.328083 | -11.993324 | F5 | 518 | 676 |'
  prefs: []
  type: TYPE_TB
- en: '| 433116975 | 162.551767 | -5.697394 | F4 | 13673 | 720 |'
  prefs: []
  type: TYPE_TB
- en: '| 233074254 | 139.046712 | 29.298535 | F2 | 870 | 2320 |'
  prefs: []
  type: TYPE_TB
- en: '| 413115231 | 162.585545 | -4.498548 | F4 | 8839 | 884 |'
  prefs: []
  type: TYPE_TB
- en: '| 211134050 | 140.304878 | 30.471131 | F2 | 12662 | 979 |'
  prefs: []
  type: TYPE_TB
- en: '| 122079323 | 13.182525 | 12.323637 | F1 | 8567 | 1145 |'
  prefs: []
  type: TYPE_TB
- en: '| 312158847 | 80.455801 | -48.489660 | F3 | 3896 | 1209 |'
  prefs: []
  type: TYPE_TB
- en: '| 322092794 | 80.115321 | -49.246309 | F3 | 1234 | 24945 |'
  prefs: []
  type: TYPE_TB
- en: '| 421019105 | 163.411890 | -4.870280 | F4 | 1996 | 2702 |'
  prefs: []
  type: TYPE_TB
- en: '| 221061603 | 140.662872 | 29.846367 | F2 | 3990 | 8584 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Grade-A and Grade-B Lens candidates found from this work with their
    object ID, RA and DEC coordinates, DLS field (F1 through F5), and their corresponding
    ranks from GAN+MixMatch and GAN+$\Pi$-models. The rank is obtained by passing
    all the survey images (281,425 objects in total; Section [2](#S2 "2 Deep Lens
    Survey Data ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")) through the models and sorting them based on
    their prediction scores. High-confidence Lens candidates have lower ranks and
    high prediction scores. For example, the Grade-A lens candidate DLS212072337 whose
    lensing nature has been spectroscopically confirmed (Section [6.2.2](#S6.SS2.SSS2
    "6.2.2 Spectroscopic confirmation of two Grade-A lenses ‣ 6.2 Catalog of Lens
    candidates found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")) has a rank
    of 21 from the GAN+$\Pi$-model and a prediction score of $\simeq 1$. The ranks
    quoted here represent an upper bound on the number of images an investigator has
    to look at to find the lens candidate, as they do not account for duplicated sky
    regions which we remove before visual inspection (as discussed in Section [6.2](#S6.SS2
    "6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")), reducing the number of unique lens candidates investigated.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59c45db608f465dc3e009ae386f6c820.png)![Refer to caption](img/886bb3561092d12764f5ad5df8cfd783.png)![Refer
    to caption](img/c4be8039222e6f305850a4914fd293ec.png)![Refer to caption](img/675dbb1a76fa13445a76e754decd851c.png)![Refer
    to caption](img/ac5a5f2c6ffd15598e9118d0cf81a7b5.png)![Refer to caption](img/3f163efa26e3db96f3cabe4d0e08e935.png)![Refer
    to caption](img/bcbc9751b997d7959d39e45f83dc61f3.png)![Refer to caption](img/04ccfbb3a5d31706551cf3b507bd9f88.png)![Refer
    to caption](img/c5831d832ffc3d639861c190339a8dcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: GradCAM++ heatmaps for all Grade-A lenses, equivalent to Figure [10](#S6.F10
    "Figure 10 ‣ 6.2.3 Distribution of lensed candidates in color-color space ‣ 6.2
    Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey").
    Each image is labeled with its object ID, and the model corresponding to the heatmaps
    (MM = GAN+MixMatch, PI = GAN+$\Pi$-model).'
  prefs: []
  type: TYPE_NORMAL
