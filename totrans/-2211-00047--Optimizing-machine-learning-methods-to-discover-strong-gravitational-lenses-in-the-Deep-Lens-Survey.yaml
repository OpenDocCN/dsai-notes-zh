- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:43:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[2211.00047] Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2211.00047](https://ar5iv.labs.arxiv.org/html/2211.00047)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keerthi Vasan G.C.,¹ Stephen Sheng,² Tucker Jones,¹ Chi Po Choi,³ and James
    Sharpnack^(2,3)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: ¹Department of Physics and Astronomy, University of California, Davis, 1 Shields
    Avenue, Davis, CA 95616, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: ²Amazon AWS AI
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '³Department of Statistics, University of California, Davis, 1 Shields Avenue,
    Davis, CA 95616, USA E-mail: kvch@ucdavis.eduWork done prior to joining Amazon(Last
    updated ; in original form )'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Machine learning models can greatly improve the search for strong gravitational
    lenses in imaging surveys by reducing the amount of human inspection required.
    In this work, we test the performance of supervised, semi-supervised, and unsupervised
    learning algorithms trained with the ResNetV2 neural network architecture on their
    ability to efficiently find strong gravitational lenses in the Deep Lens Survey
    (DLS). We use galaxy images from the survey, combined with simulated lensed sources,
    as labeled data in our training datasets. We find that models using semi-supervised
    learning along with data augmentations (transformations applied to an image during
    training, e.g., rotation) and Generative Adversarial Network (GAN) generated images
    yield the best performance. They offer 5–10 times better precision across all
    recall values compared to supervised algorithms. Applying the best performing
    models to the full 20 deg² DLS survey, we find 3 Grade-A lens candidates within
    the top 17 image predictions from the model. This increases to 9 Grade-A and 13
    Grade-B candidates when $1$% ($\sim 2500$ images) of the model predictions are
    visually inspected. This is $\gtrsim 10\times$ the sky density of lens candidates
    compared to current shallower wide-area surveys (such as the Dark Energy Survey),
    indicating a trove of lenses awaiting discovery in upcoming deeper all-sky surveys.
    These results suggest that pipelines tasked with finding strong lens systems can
    be highly efficient, minimizing human effort. We additionally report spectroscopic
    confirmation of the lensing nature of two Grade-A candidates identified by our
    model, further validating our methods.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'gravitational lensing: strong – methods: statistical^†^†pubyear: 2022^†^†pagerange:
    Optimizing machine learning methods to discover strong gravitational lenses in
    the Deep Lens Survey–[15](#A1.F15 "Figure 15 ‣ Appendix A Model performance and
    final lens sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Under rare alignment configurations, the gravitational potential of a massive
    galaxy can cause light from a distant galaxy located behind it to take multiple
    paths around it. This results in the formation of several distinct images of the
    distant galaxy around the massive galaxy, a phenomenon known as strong gravitational
    lensing (e.g., Treu, [2010](#bib.bib76)). These multiple images are magnified
    by factors that can reach $>$10 times, making them appear brighter and more spatially
    extended. Such magnification makes these systems ideal for studying the formation
    and evolution of galaxies across cosmic time (e.g., Wuyts et al., [2014](#bib.bib81);
    Pettini et al., [2002](#bib.bib59); Swinbank et al., [2009](#bib.bib73); Koopmans
    et al., [2006](#bib.bib34); Leethochawalit et al., [2016](#bib.bib45)), while
    analysis of the lensing mass distribution enables insight into the nature of dark
    matter (e.g., Chiba, [2002](#bib.bib13); Bradač et al., [2002](#bib.bib10); Miranda
    & Macciò, [2007](#bib.bib51); Gilman et al., [2019](#bib.bib21); Shajib et al.,
    [2022](#bib.bib65)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The main current challenge in working with strong lens systems is their scarcity
    on the sky. Therefore, methods which are able to efficiently identify lensed galaxies
    from wide-area sky surveys are extremely beneficial. Automated methods will be
    especially valuable for lens searches in upcoming wide-area sky surveys to be
    carried out by the Vera Rubin Observatory, Euclid, and Roman (e.g., LSST Science
    Collaboration et al., [2009](#bib.bib40); Laureijs et al., [2011](#bib.bib42);
    Spergel et al., [2015](#bib.bib70)), whose improvements in sensitivity, angular
    resolution and sky coverage will enable detection of far more lens samples than
    are currently known.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Early approaches to finding strong lens systems included various algorithms
    searching for multiple lensed images or arc shapes, manual searches around massive
    galaxies, and citizen science projects (e.g., Moustakas et al., [2007](#bib.bib54);
    Paraficz et al., [2016](#bib.bib58); Seidel & Bartelmann, [2007](#bib.bib63);
    Gavazzi et al., [2014](#bib.bib20); Alard, [2006](#bib.bib2); Fassnacht et al.,
    [2004](#bib.bib17); More et al., [2016](#bib.bib53); Belokurov et al., [2009](#bib.bib5);
    Diehl et al., [2009](#bib.bib15); Garvin et al., [2022](#bib.bib19)). While successful,
    these methods are time-consuming and difficult to incorporate into an automated
    framework. Convolutional Neural Networks (CNNs; LeCun et al., [1989](#bib.bib43);
    Krizhevsky et al., [2012](#bib.bib38)), which have been successfully developed
    into a standard tool in the field of computer vision in the past decade, are a
    promising approach to solving image recognition problems. Depending on the problem,
    there are various neural network architectures that can be optimized for the desired
    objectives. CNNs and machine learning techniques in general have indeed been used
    with success in the past few years to uncover gravitationally lensed candidates
    in wide-area imaging surveys (e.g., Jacobs et al., [2017](#bib.bib30); Jacobs
    et al., [2019](#bib.bib31); Sonnenfeld et al., [2018](#bib.bib69); Pourrahmani
    et al., [2018](#bib.bib60); Huang et al., [2020](#bib.bib27); Li et al., [2020](#bib.bib46);
    Cañameras et al., [2020](#bib.bib11)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 早期寻找强引力透镜系统的方法包括各种算法，这些算法搜索多重透镜图像或弧形，围绕大质量星系进行手动搜索，以及公民科学项目（例如，Moustakas 等，[2007](#bib.bib54)；Paraficz
    等，[2016](#bib.bib58)；Seidel & Bartelmann，[2007](#bib.bib63)；Gavazzi 等，[2014](#bib.bib20)；Alard，[2006](#bib.bib2)；Fassnacht
    等，[2004](#bib.bib17)；More 等，[2016](#bib.bib53)；Belokurov 等，[2009](#bib.bib5)；Diehl
    等，[2009](#bib.bib15)；Garvin 等，[2022](#bib.bib19)）。虽然这些方法成功了，但它们耗时且难以纳入自动化框架。卷积神经网络（CNN；LeCun
    等，[1989](#bib.bib43)；Krizhevsky 等，[2012](#bib.bib38)），在过去十年中已成功发展为计算机视觉领域的标准工具，是解决图像识别问题的有前途的方法。根据问题的不同，有各种神经网络架构可以优化以实现预期目标。CNN
    和机器学习技术通常已在过去几年中成功用于发现广域成像调查中的引力透镜候选者（例如，Jacobs 等，[2017](#bib.bib30)；Jacobs 等，[2019](#bib.bib31)；Sonnenfeld
    等，[2018](#bib.bib69)；Pourrahmani 等，[2018](#bib.bib60)；Huang 等，[2020](#bib.bib27)；Li
    等，[2020](#bib.bib46)；Cañameras 等，[2020](#bib.bib11)）。
- en: Most machine learning searches for lenses have relied primarily on supervised
    learning methods (i.e., using a data set consisting of labeled lensed and non-lensed
    galaxies to train a model). However, while non-lensed galaxies are plentiful,
    current surveys have very few known lenses to be used as positive labels. Instead,
    machine learning models are trained on simulated lenses, which can be generated
    in abundance (e.g., Jacobs et al., [2017](#bib.bib30)). However, this presents
    a new problem, that the training data distribution (i.e., the simulated lenses)
    differs from the test data distribution (i.e., the real lenses) – a problem called
    distribution shift (Quinonero-Candela et al., [2008](#bib.bib61)). To overcome
    distribution shift, machine learning researchers have repurposed semi-supervised
    learning methods, which use unlabeled data and data augmentation to adapt the
    trained model to the test data (Berthelot et al., [2021](#bib.bib7)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习搜索透镜主要依赖于监督学习方法（即，使用包含标记的透镜和非透镜星系的数据集来训练模型）。然而，尽管非透镜星系很多，但当前调查中已知的透镜非常少，无法用作正样本标签。相反，机器学习模型是在模拟透镜上训练的，这些透镜可以大量生成（例如，Jacobs
    等，[2017](#bib.bib30)）。然而，这带来了一个新问题，即训练数据分布（即模拟透镜）与测试数据分布（即真实透镜）不同——这个问题被称为分布转移（Quinonero-Candela
    等，[2008](#bib.bib61)）。为了克服分布转移，机器学习研究人员重新利用了半监督学习方法，这些方法利用未标记数据和数据增强来使训练模型适应测试数据（Berthelot
    等，[2021](#bib.bib7)）。
- en: An advantage to the semi-supervised learning approach is that it can learn from
    the abundance of unlabeled images from the survey, which allows models to generalize
    better to unseen images. This is particularly useful to improve performance given
    millions of galaxy images that are detected in sky surveys but not included in
    the training data. The model performance is further improved through augmentations
    applied to images during training (e.g., translation and rotation). In addition
    to conventional transformations, a rich source of data augmentation can be derived
    by making use of unsupervised learning algorithms (e.g., Goodfellow et al., [2014](#bib.bib22);
    Kingma & Welling, [2014](#bib.bib32); Erhan et al., [2010](#bib.bib16)). Given
    the range of methodologies available, we now seek to address the question of which
    combination of machine learning methods (supervised and semi-supervised) and augmentations
    are best suited for finding strong gravitational lenses.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习方法的一个优势在于它可以从调查中大量未标记的图像中学习，这使得模型能够更好地对未见过的图像进行泛化。这对于提高性能尤其有用，因为在天空调查中检测到的数百万张银河图像并未包含在训练数据中。通过在训练期间对图像应用增强（例如，平移和旋转），模型性能进一步得到提升。除了传统的变换外，还可以通过利用无监督学习算法（例如，Goodfellow等，[2014](#bib.bib22)；Kingma
    & Welling，[2014](#bib.bib32)；Erhan等，[2010](#bib.bib16)）获得丰富的数据增强来源。考虑到可用的方法范围，我们现在寻求解决哪种机器学习方法（监督和半监督）以及增强组合最适合寻找强引力透镜的问题。
- en: We seek efficient models which minimize human effort by reducing the number
    of images that must be visually inspected to recover a given sample of lenses.
    In this work we apply CNN models to the Deep Lens Survey (DLS; Wittman et al.
    [2002](#bib.bib78)), which has relatively good image quality and also remains
    relatively unexplored in terms of machine learning searches, thus serving as a
    good testbed for this study. Also, because of the small size of known lenses from
    the DLS survey, we reserve those for use only in our test dataset. Training and
    validation datasets will only contain simulated lenses. In our previous methodology
    paper (Sheng et al., [2022](#bib.bib66), hereafter [S22](#bib.bib66)), we discussed
    the CNN models and lens detection techniques used in this work. Herein, we describe
    our training data in detail and focus on evaluating the performance of the different
    models on the DLS dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求有效的模型，通过减少必须视觉检查的图像数量来最小化人力劳动，以恢复给定的透镜样本。在这项工作中，我们将CNN模型应用于深度透镜调查（DLS；Wittman等，[2002](#bib.bib78)），该调查具有相对良好的图像质量，并且在机器学习搜索方面仍然相对未被探索，因此作为这项研究的良好测试平台。此外，由于DLS调查中已知透镜的数量较少，我们将这些透镜仅保留用于我们的测试数据集。训练和验证数据集将只包含模拟透镜。在我们之前的方法论文（Sheng等，[2022](#bib.bib66)，以下简称[S22](#bib.bib66)）中，我们讨论了在这项工作中使用的CNN模型和透镜检测技术。在这里，我们详细描述了我们的训练数据，并专注于评估不同模型在DLS数据集上的表现。
- en: This paper is organized as follows. In Section [2](#S2 "2 Deep Lens Survey Data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") we give an overview of the Deep Lens Survey and our
    source selection used for this work. We summarize our machine learning architecture
    and learning methods in Section [3](#S3 "3 Deep Learning Architecture and learning
    methods used ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey"). Section [4](#S4 "4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") describes the method used to generate training, validation,
    and testing data from DLS images. Section [5](#S5 "5 Metric to evaluate model
    performance ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") discusses our metric to evaluate the performance
    of the different CNN models. We discuss the results from our experiments in Section [6](#S6
    "6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey"), including the sample of new lens
    candidates from DLS and spectroscopic confirmation of two systems. Finally, we
    summarize the main conclusions in Section [7](#S7 "7 Conclusions ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey"). Throughout this paper we use the AB magnitude system and a $\Lambda$CDM
    cosmology with $\Omega_{M}=0.3$, $\Omega_{\Lambda}=0.7$ and $\mathrm{H}_{0}=70$ km s^(-1) Mpc^(-1).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文组织结构如下。在第[2节](#S2 "2 Deep Lens Survey Data ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")中，我们概述了深场透镜调查及用于本工作的源选择。在第[3节](#S3
    "3 Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")中，我们总结了我们的机器学习架构和学习方法。第[4节](#S4
    "4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey")描述了从DLS图像中生成训练、验证和测试数据的方法。第[5节](#S5
    "5 Metric to evaluate model performance ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")讨论了我们用来评估不同CNN模型性能的指标。我们在第[6节](#S6
    "6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")中讨论了实验结果，包括来自DLS的新透镜候选样本及两个系统的光谱确认。最后，我们在第[7节](#S7
    "7 Conclusions ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")总结了主要结论。本文中我们使用AB星等系统和一个$\Lambda$CDM宇宙学模型，$\Omega_{M}=0.3$，$\Omega_{\Lambda}=0.7$，以及$\mathrm{H}_{0}=70$ km s^(-1) Mpc^(-1)。
- en: 2 Deep Lens Survey Data
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深场透镜调查数据
- en: Here we give a brief overview of imaging data from the Deep Lens Survey (DLS)
    which we use to test and optimize strong lens detection methods. The DLS consists
    of relatively deep imaging over 20 square degrees in five independent $2^{\circ}\times
    2^{\circ}$ fields which are widely separated in the sky (Wittman et al., [2002](#bib.bib78)).
    Each field was imaged in BVRz photometric filters (Schmidt & Thorman, [2013](#bib.bib62))
    using the 4-meter Mayall telescope at Kitt Peak National Observatory or Blanco
    telescope at Cerro Tololo Inter-American Observatory, depending on declination.
    The survey was carried out over $\sim$120 nights. The survey was designed for
    weak gravitational lensing measurements, with stringent requirements on image
    quality and limiting magnitude, such that the data are naturally well suited for
    identifying strong lens systems. Typical $5\sigma$ point-source detection limits
    are 25.8, 26.3, and 26.9 AB magnitude in the $B$, $V$, and $R$ filters respectively
    (Schmidt & Thorman, [2013](#bib.bib62)). The $R$ band limit is only $\sim$0.6
    magnitudes shallower than the expected depth to be reached by Rubin observatory’s
    10-year survey (Ivezić et al., [2019](#bib.bib29)). The seeing is by design best
    in the R band (FWHM$\lesssim$0$\aas@@fstack{\prime\prime}$9) and is typically
    $\gtrsim$0$\aas@@fstack{\prime\prime}$9 in the B, V, and z bands (Wittman et al.,
    [2002](#bib.bib78)). Images in the $z$ band are shallowest and typically subject
    to worse seeing conditions. In this paper, we use only the $BVR$ data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简要概述了来自深度透镜调查（DLS）的成像数据，这些数据用于测试和优化强透镜检测方法。DLS由五个独立的$2^{\circ}\times 2^{\circ}$区域组成，覆盖20平方度的相对深层成像，这些区域在天空中相距较远（Wittman等人，[2002](#bib.bib78)）。每个区域都使用4米Mayall望远镜（位于Kitt
    Peak国家天文台）或Blanco望远镜（位于Cerro Tololo美洲天文台）在BVRz光度滤镜下进行成像，具体取决于赤纬。该调查持续了约120个夜晚。调查旨在进行弱引力透镜测量，对图像质量和极限星等有严格要求，因此数据自然适合识别强透镜系统。典型的$5\sigma$点源检测极限在$B$、$V$和$R$滤镜中分别为25.8、26.3和26.9
    AB星等（Schmidt & Thorman，[2013](#bib.bib62)）。$R$波段的极限仅比Rubin天文台10年调查的预期深度浅约0.6个星等（Ivezić等人，[2019](#bib.bib29)）。R波段的观测条件设计上最佳（FWHM$\lesssim$0$\aas@@fstack{\prime\prime}$9），而在B、V和z波段通常为$\gtrsim$0$\aas@@fstack{\prime\prime}$9（Wittman等人，[2002](#bib.bib78)）。$z$波段的图像最浅，通常受更差的观测条件影响。在本文中，我们仅使用$BVR$数据。
- en: 2.1 Source selection and regions of interest
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 来源选择和感兴趣区域
- en: The DLS catalog includes $\sim$5 million detected galaxies across 20 square
    degrees. However, only those of moderate redshift and relatively high mass will
    act as detectable strong lenses (i.e., with Einstein radii $\Theta_{E}\gtrsim
    1$ arcsecond). We applied a magnitude cut of $17.5<R<22$ (similar to that used
    by Jacobs et al. [2017](#bib.bib30)) in order to remove objects which are unlikely
    to produce a detectable lensing effect. Additionally, we use SExtractor (Bertin
    & Arnouts, [1996](#bib.bib8)) flags to eliminate saturated low-redshift galaxies,
    and exclusion masks to remove galaxies around bright stars or at the edge of the
    field. This results in 281,425 objects (hereafter referred to as the SurveyCatalog).
    We find that SExtractor flags and exclusion masks remove $\sim 5\%$ of the galaxies
    from the survey which reduces the effective sky area probed by our SurveyCatalog
    to $\sim 19$ square degrees. We set aside 2277 ($\sim$0.8%) randomly sampled object
    images from this catalog to experiment and tune the HumVI scaling parameters (discussed
    in Section [4.1](#S4.SS1 "4.1 Generating the NonLenses dataset ‣ 4 Training and
    Validation data ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). All model training analysis in this paper pertains
    to the remaining set of 279,149 objects (hereafter referred to as the TrainCatalog).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: DLS目录包括在20平方度范围内检测到的约500万只星系。然而，只有那些具有中等红移和相对较高质量的星系才会成为可检测的强引力透镜（即，具有爱因斯坦半径$\Theta_{E}\gtrsim
    1$角秒）。我们应用了$17.5<R<22$的星等切割（类似于Jacobs等人[2017](#bib.bib30)使用的），以去除那些不太可能产生可检测透镜效应的天体。此外，我们使用SExtractor（Bertin
    & Arnouts，[1996](#bib.bib8)）标记来排除饱和的低红移星系，并使用排除掩模来去除位于亮星周围或视场边缘的星系。这导致281,425个天体（以下简称为SurveyCatalog）。我们发现，SExtractor标记和排除掩模去除了约5%的星系，使我们的SurveyCatalog有效的观测天空面积减少到约19平方度。我们从该目录中抽取了2277个（约0.8%）随机采样的天体图像，用于实验和调整HumVI缩放参数（详见第[4.1](#S4.SS1
    "4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")节）。本文中的所有模型训练分析涉及剩余的279,149个天体（以下简称为TrainCatalog）。
- en: For our analysis we extract image cutouts spanning 25$\aas@@fstack{\prime\prime}$7
    $\times$ 25$\aas@@fstack{\prime\prime}$7 (100 $\times$ 100 pixels) centered on
    each object. This size is sufficient for galaxy- and group-scale lenses ($\Theta_{E}\lesssim
    12$”); we do not focus on the most massive cluster lenses which are already well
    cataloged (Ascaso et al., [2014](#bib.bib4)) and simpler to identify. We create
    color-composite images from the source $BVR$ FITS files for all targets in the
    SurveyCatalog (Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Generating the NonLenses dataset
    ‣ 4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"); discussed in detail in
    Section [4.1](#S4.SS1 "4.1 Generating the NonLenses dataset ‣ 4 Training and Validation
    data ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")). These color composite images have smaller file sizes
    compared to original data, enabling us to keep the rest of the analysis computationally
    efficient. These images are still able to capture the detected low-suface brightness
    features, while not saturating the brightest objects of interest for this work.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们提取了每个目标的图像剪切区域，大小为25$\aas@@fstack{\prime\prime}$7 $\times$ 25$\aas@@fstack{\prime\prime}$7（100
    $\times$ 100 像素），并以每个物体为中心。这一尺寸足以适应银河系和星系群尺度的透镜（$\Theta_{E}\lesssim 12$”）；我们不关注已经被很好
    catalog 的最大质量的星系团透镜（Ascaso 等，[2014](#bib.bib4)），这些透镜已经很容易识别。我们从 SurveyCatalog
    中所有目标的源 $BVR$ FITS 文件中创建彩色合成图像（图 [4](#S4.F4 "图 4 ‣ 4.1 生成非透镜数据集 ‣ 4 训练和验证数据 ‣
    优化机器学习方法以发现深度透镜调查中的强引力透镜")；详细讨论见第 [4.1](#S4.SS1 "4.1 生成非透镜数据集 ‣ 4 训练和验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")
    节）。这些彩色合成图像的文件大小比原始数据小，使我们能够保持其余分析的计算效率。这些图像仍能捕捉到检测到的低表面亮度特征，同时不会使本研究关注的最亮物体饱和。
- en: Additionally, they are better suited for the machine learning architecture and
    methods used in this work (discussed in Section [3](#S3 "3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它们更适合于本研究中使用的机器学习架构和方法（详见第 [3](#S3 "3 深度学习架构和使用的学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")
    节").
- en: 3 Deep Learning Architecture and learning methods used
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习架构和使用的学习方法
- en: The task at hand is to establish a machine learning (ML) algorithm that efficiently
    classifies the 281,425 color-composite images from the survey into lensed and
    non-lensed galaxies. Furthermore, by ranking the images from highest predicted
    probability of being a lens to lowest, we can order the images for human inspection.
    This requires the selection of an architecture (i.e., a function that takes images
    as input and gives prediction probabilities as output) and learning methods (i.e.,
    a way for our function to learn from the data). The key components of our ML training
    pipeline are a supervised convolutional neural net (CNN), domain adaptation with
    semi-supervised learning, and augmenting training samples with generative adversarial
    nets (GAN). A more detailed account of our ML method can be found in [S22](#bib.bib66).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当前任务是建立一个有效分类281,425张调查中的彩色合成图像的机器学习（ML）算法，将其分为透镜和非透镜星系。此外，通过按图像被预测为透镜的概率从高到低排序，我们可以将图像按顺序供人工检查。这需要选择一个架构（即一个以图像为输入、以预测概率为输出的函数）和学习方法（即让我们的函数从数据中学习的方法）。我们机器学习训练管道的关键组件是监督卷积神经网络（CNN）、通过半监督学习进行的领域适应，以及通过生成对抗网络（GAN）增加训练样本。有关我们机器学习方法的更详细说明，请参见
    [S22](#bib.bib66)。
- en: 3.1 Convolutional neural network architecture
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 卷积神经网络架构
- en: '![Refer to caption](img/b1586cf65e58aa051a84202fc5c4d064.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b1586cf65e58aa051a84202fc5c4d064.png)'
- en: 'Figure 1: Schematic depiction of the ResNetV2 deep learning architecture used
    in this work. The input to the network is an RGB color-composite image generated
    from the BVR fits files (Section. [4.1](#S4.SS1 "4.1 Generating the NonLenses
    dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")), and the output
    is a value between 0 and 1 indicating the probability of the input image being
    a lens. The general network consists of three stacks, each containing $3n$ residual
    units. In this work, we use a stack size of $n=1$ resulting in a total of three
    residual units. Each residual unit consists of two sets of Batch Normalization
    (BN), Rectified Linear Unit activation function (ReLU), and Conv units, where
    Conv denotes a convolutional layer with kernel size $3\times 3$ and appropriate
    stride size. The network ends with global average pooling and a softmax layer.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：本研究中使用的 ResNetV2 深度学习架构的示意图。网络的输入是从 BVR fits 文件生成的 RGB 彩色合成图像（第 [4.1](#S4.SS1
    "4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") 节），输出是一个介于 0 和 1 之间的值，表示输入图像为透镜的概率。一般网络由三个堆叠组成，每个堆叠包含 $3n$ 个残差单元。在本工作中，我们使用堆叠大小为
    $n=1$，总共三个残差单元。每个残差单元包括两组批量归一化（BN）、修正线性单元激活函数（ReLU）和卷积单元，其中卷积单元表示具有 $3\times 3$
    核心大小和适当步幅大小的卷积层。网络以全局平均池化和 softmax 层结束。
- en: CNNs have previously been used for classifying and identifying lens candidates
    (e.g., Jacobs et al., [2017](#bib.bib30)). They are a specific form of neural
    network that learns translation invariant representations via trainable convolution
    kernels. This is particularly well suited to astronomical images where patterns
    are repeated throughout the sky. Deep CNNs are models where these learned non-linear
    representations of the image (called layers) are stacked on top of one another.
    Deep CNNs are trained using variations of stochastic gradient descent, where an
    objective function is evaluated on small subsets of the data, called mini-batches,
    and the parameters are updated by subtracting some fraction of the objective’s
    gradient.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 曾被用于分类和识别透镜候选（例如，Jacobs 等，[2017](#bib.bib30)）。它们是一种特定形式的神经网络，通过可训练的卷积核学习平移不变的表示。这尤其适合于天文图像，因为模式在整个天空中重复出现。深度
    CNNs 是那些将图像的非线性表示（称为层）堆叠在一起的模型。深度 CNNs 通过随机梯度下降的变体进行训练，其中在数据的小子集上评估目标函数，这些小子集称为
    mini-batches，参数通过减去目标梯度的一部分来更新。
- en: There are many choices of how precisely these layers are constructed and combined,
    such as selection of the convolutional kernel size, number of output channels
    for each convolution layer, the non-linear activation function, and the incorporation
    of other layers that improve performance such as Batch Normalization (Ioffe &
    Szegedy, [2015](#bib.bib28)). All of these details together are called the model
    architecture.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层的构建和组合方式有许多选择，例如卷积核大小的选择、每个卷积层的输出通道数、非线性激活函数的选择，以及其他能提升性能的层的引入，如批量归一化（Ioffe
    & Szegedy, [2015](#bib.bib28)）。所有这些细节共同构成了模型架构。
- en: 'We make use of the ResNet version-2 architecture (ResNetV2; He et al., [2016a](#bib.bib25),
    [b](#bib.bib26)) designed for the CIFAR10 dataset (Krizhevsky, [2009a](#bib.bib36)),
    shown schematically in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Convolutional neural
    network architecture ‣ 3 Deep Learning Architecture and learning methods used
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). It is one of the widely used industry standard networks
    for image classification problems (e.g., Litjens et al., [2017](#bib.bib47); Gu
    et al., [2018](#bib.bib23); Madireddy et al., [2019](#bib.bib49)). The ResNetV2
    used in this work consists of three stacks (see Figure [1](#S3.F1 "Figure 1 ‣
    3.1 Convolutional neural network architecture ‣ 3 Deep Learning Architecture and
    learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey"); green blocks) and each stack consists
    of $n$ residual unit blocks, where $n$ is a parameter to be chosen that controls
    the depth of the neural network. A deeper neural network has more learning capacity
    but requires more computational power and training samples. Each residual unit
    block consists of three convolution layers of kernel size $3\times 3$ and one
    skip connection. To match the feature map dimensions (width, height) and the number
    of channels between stacks, a few extra convolution layers are included at the
    input and the beginning block of each stack. Therefore, $9n+4$ convolution layers
    are present in the network in total. For all the models used in this work, we
    adopt $n=1$. With strided convolutions, the feature map dimensions to each stack
    decrease by a factor of $1/2$. The number of input and output channels to each
    stack are: $(16\rightarrow 64),~{}(64\rightarrow 128),~{}(128\rightarrow 256)$.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了为CIFAR10数据集（Krizhevsky, [2009a](#bib.bib36)）设计的ResNet版本2架构（ResNetV2; He
    et al., [2016a](#bib.bib25), [b](#bib.bib26)），如图[1](#S3.F1 "图 1 ‣ 3.1 卷积神经网络架构
    ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")所示。它是用于图像分类问题的广泛使用的行业标准网络之一（例如，Litjens
    et al., [2017](#bib.bib47); Gu et al., [2018](#bib.bib23); Madireddy et al., [2019](#bib.bib49)）。在这项工作中使用的ResNetV2包含三个堆叠（见图[1](#S3.F1
    "图 1 ‣ 3.1 卷积神经网络架构 ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")；绿色块），每个堆叠由$n$个残差单元块组成，其中$n$是一个可选参数，用于控制神经网络的深度。更深的神经网络具有更强的学习能力，但需要更多的计算能力和训练样本。每个残差单元块由三个核大小为$3\times
    3$的卷积层和一个跳跃连接组成。为了匹配堆叠之间的特征图维度（宽度，高度）和通道数量，在每个堆叠的输入和起始块处包括了一些额外的卷积层。因此，网络中总共有$9n+4$个卷积层。对于本工作中使用的所有模型，我们采用$n=1$。通过步幅卷积，每个堆叠的特征图维度减少了$1/2$倍。每个堆叠的输入和输出通道数分别为：（$16\rightarrow
    64$），（$64\rightarrow 128$），（$128\rightarrow 256$）。
- en: The network ends with global average pooling, a fully-connected layer and softmax.
    The global average pooling constrains the output to be rotationally invariant.
    The softmax transforms the output to be a value between 0 and 1 which can be interpreted
    as a probability. Throughout this work, a value of 1 is designated for lensed
    candidates (referred to herein as Lenses) and 0 for nonlensed candidates (referred
    to as NonLenses).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 网络以全局平均池化、一个全连接层和softmax结束。全局平均池化将输出限制为旋转不变的。Softmax将输出转换为0和1之间的值，可以解释为概率。在本工作中，值1被指定为透镜候选（以下简称为Lenses），值0则为非透镜候选（以下简称为NonLenses）。
- en: 3.2 Domain adaptation with semi-supervised learning
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 半监督学习的领域适应
- en: In supervised learning, our algorithm is trained via mini-batches of images
    $X$ and corresponding labels $y$ ($1$ for Lenses and $0$ for NonLenses). The algorithm
    then tries to learn the neural network parameters, collectively referred to as
    $\Theta$. The output of the neural network after the softmax activation produces
    a prediction $p_{\Theta}(X)$, which is our predicted probability of $X$ being
    a lens. Our supervised learning objective function is the cross-entropy loss function,
    denoted $\ell_{S}$, which is a measure of the quality of our predictions, $p_{\Theta}(X)$,
    when compared to the true labels, $y$. Merely using supervised learning does not
    perform well in the face of distributional shift, and we turn to semi-supervised
    learning (SSL) methods which make use of the unlabeled test data to adapt to this
    domain.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB-shuffle | Randomly perturb the order of the channels in the images |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| JPEG-quality | 50-100% |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | Randomly rotate the images by a multiple of 90 degrees |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| Translations | Randomly translate the images by at most 20 pixels in the
    up, down, left and right directions |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | Randomly flips the images across the x-axis |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | Randomly perturb the brightness(-0.1-0.1), saturation(0.9-1.3),
    hue(0.96-1.00), and gamma(1.23-1.25) of the images |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Data augmentations used on images in the semi-supervised training
    pipeline.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: There are many semi-supervised approaches to deep learning. The methods we explore
    are FixMatch¹¹1FixMatch was not part of the original lens search study since this
    technique had not been published at the time. We are including it in our results
    here to be thorough. (Sohn et al., [2020](#bib.bib67)), MixMatch (Berthelot et al.,
    [2019](#bib.bib6)), Virtual Adversarial Training (Miyato et al., [2019](#bib.bib52)),
    Mean Teacher (Tarvainen & Valpola, [2017](#bib.bib74)), $\Pi$-Model (Laine & Aila,
    [2017](#bib.bib41)), and Pseudo-Labeling (Lee, [2013](#bib.bib44)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Most SSL algorithms follow the same template. We minimize an objective function
    consisting of a supervised component (i.e. $\ell_{S}$ losses), where the label
    is provided, plus an unsupervised component (i.e. $\ell_{U}$ losses). Both are
    optimized together over mini-batches, now consisting of labeled and unlabeled
    data, but without significant modification to the stochastic gradient descent
    algorithm. The main feature that distinguishes our setting from typical SSL is
    that our training NonLenses and test set come from the same pool of data, while
    the simulated Lenses do not exist in the test data. This is in contrast to Jacobs
    et al. ([2017](#bib.bib30)) for example, in which they produce simulated NonLenses
    as well, but do not attempt domain adaptation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: In the Pseudo-Label algorithm (Lee, [2013](#bib.bib44)), we assign pseudo-labels
    to unlabeled data by taking the model’s predicted class as the label. We can then
    use the same loss as in the supervised task (i.e., $\ell_{S}=\ell_{U}$). The motivation
    is that we are implicitly enforcing entropy minimization by forcing the model
    to be confident on unlabeled samples. An alternative approach to SSL is consistency
    regularization, where two independently augmented samples of the same test image
    are encouraged to produce similar predictions. The $\Pi$-model algorithm (Laine
    & Aila, [2017](#bib.bib41)) directly uses consistency regularization. The idea
    is to take two random augmentations of the same sample data point, $X$, and compute
    the squared difference of the model outputs for the augmented copies. We use $\text{aug},\widetilde{\text{aug}}$
    to denote two independent augmentations, which can be produced by selecting different
    randomization seeds. The unsupervised loss is then
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在伪标签算法 (Lee, [2013](#bib.bib44)) 中，我们通过将模型预测的类别作为标签来为未标记的数据分配伪标签。然后，我们可以使用与监督任务中相同的损失函数
    (即，$\ell_{S}=\ell_{U}$)。其动机是通过迫使模型对未标记样本有信心，从而隐含地强制执行熵最小化。SSL 的一种替代方法是一致性正则化，其中鼓励同一测试图像的两个独立增强样本产生类似的预测。$\Pi$-模型算法
    (Laine & Aila, [2017](#bib.bib41)) 直接使用一致性正则化。其思想是对相同样本数据点 $X$ 进行两个随机增强，并计算增强副本模型输出的平方差异。我们使用
    $\text{aug},\widetilde{\text{aug}}$ 来表示两个独立的增强，这些增强可以通过选择不同的随机种子来产生。无监督损失为
- en: '|  | $\ell_{U}(X)=\left\&#124;p_{\Theta}(\text{aug}(X))-p_{\Theta}(\widetilde{\text{aug}}(X))\right\&#124;^{2}.$
    |  | (1) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\ell_{U}(X)=\left\|p_{\Theta}(\text{aug}(X))-p_{\Theta}(\widetilde{\text{aug}}(X))\right\|^{2}.$
    |  | (1) |'
- en: The choice of stochastic augmentation function is up to the modeler and will
    often be domain specific.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 随机增强函数的选择由建模者决定，并且通常是领域特定的。
- en: The Mean Teacher algorithm (Tarvainen & Valpola, [2017](#bib.bib74)) also uses
    consistency regularization, but replaces one of the augmentations in Equation [1](#S3.E1
    "In 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") with the output of the model using
    an exponential moving average (the teacher model) of model parameters, $\Theta$.
    FixMatch (Sohn et al., [2020](#bib.bib67)) and MixMatch (Berthelot et al., [2019](#bib.bib6))
    employ both consistency regularization and entropy minimization. MixMatch was
    originally proposed as a heuristic approach, and FixMatch was later derived as
    a more principled simplification of MixMatch and other related SSL methods. Virtual
    adversarial training (VAT; Miyato et al., [2019](#bib.bib52)) uses an adversarial,
    worst-case, augmentation. This adversarial augmentation pushes the image in the
    direction which will cause the greatest increase in loss. One downside to VAT
    is that the adversarial augmentations are not able to encode the domain specific
    prior information that random augmentations can provide (see Table [1](#S3.T1
    "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey")).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Mean Teacher 算法 (Tarvainen & Valpola, [2017](#bib.bib74)) 也使用一致性正则化，但将方程 [1](#S3.E1
    "In 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") 中的一个增强替换为使用模型参数的指数移动平均（教师模型）得到的输出
    $\Theta$。FixMatch (Sohn et al., [2020](#bib.bib67)) 和 MixMatch (Berthelot et al.,
    [2019](#bib.bib6)) 同时采用一致性正则化和熵最小化。MixMatch 最初被提出作为一种启发式方法，FixMatch 则被后来作为 MixMatch
    和其他相关 SSL 方法的更具原则性的简化方法推导出来。虚拟对抗训练 (VAT; Miyato et al., [2019](#bib.bib52)) 使用对抗性最坏情况的增强。这种对抗性增强推动图像向最大增加损失的方向移动。VAT
    的一个缺点是，对抗性增强无法编码随机增强可以提供的领域特定先验信息 (见表 [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation
    with semi-supervised learning ‣ 3 Deep Learning Architecture and learning methods
    used ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"))。
- en: 3.3 Data augmentation and GANs
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据增强与 GANs
- en: Data augmentation serves as a crucial regularizer in semi-supervised learning
    (SSL) algorithms. Several SSL algorithms, including those mentioned in this paper
    such as pi-model (Laine & Aila, [2017](#bib.bib41)), MixMatch (Berthelot et al.,
    [2019](#bib.bib6)), and fixMatch (Sohn et al., [2020](#bib.bib67)), utilize data
    augmentation techniques. The data augmentation techniques we employed in our study
    are provided in Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised
    learning ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey"), and are particularly well-suited for DLS images.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强在半监督学习（SSL）算法中起着至关重要的正则化作用。包括本文提到的算法如 pi-model（Laine & Aila，[2017](#bib.bib41)）、MixMatch（Berthelot
    等，[2019](#bib.bib6)）和 fixMatch（Sohn 等，[2020](#bib.bib67)）在内的几种 SSL 算法都利用了数据增强技术。我们在研究中使用的数据增强技术列于表
    [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning ‣ 3
    Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") 中，并特别适合
    DLS 图像。
- en: RGB-shuffle randomizes the order of channels and Color augmentation perturbs
    the colors in the images. These have the effect of accounting for systematic bias
    in channel and color information introduced by the simulation pipeline. JPEG-quality
    augmentation accounts for varying levels of noise and image quality, and applies
    to any color composite image irrespective of the format that the image is saved
    in (e.g., in this case we use png format instead of jpeg). Rot90, Translations,
    and Horizontal flips induce translational and rotational invariance in the predictions.
    Examples of these augmentations are shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.3
    Data augmentation and GANs ‣ 3 Deep Learning Architecture and learning methods
    used ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). We note that even though some augmentations (e.g.,
    RGB-shuffle) result in unrealistic images, our empirical tests described in Section [6.1.1](#S6.SS1.SSS1
    "6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with
    GANs and Augmentations have superior performance ‣ 6 Results and Discussion ‣
    Optimizing machine learning methods to discover strong gravitational lenses in
    the Deep Lens Survey") indicate that these augmentations yield improved model
    performance. Domain adaptation problems employing semi-supervised algorithms (SSLs)
    have been shown to benefit greatly from data augmentations in general (e.g., Sohn
    et al., [2020](#bib.bib67)), suggesting that this effect is not specific to our
    lens search.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: RGB-shuffle 随机化通道的顺序，Color augmentation 则扰动图像中的颜色。这些操作用于修正模拟管线中引入的通道和颜色信息的系统偏差。JPEG-quality
    augmentation 处理噪声和图像质量的不同水平，并适用于任何颜色合成图像，无论图像保存的格式如何（例如，我们在此使用 png 格式而非 jpeg）。Rot90、Translations
    和 Horizontal flips 在预测中引入平移和旋转不变性。这些增强的示例如图 [2](#S3.F2 "Figure 2 ‣ 3.3 Data augmentation
    and GANs ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") 所示。我们注意到，尽管一些增强（例如 RGB-shuffle）会导致不真实的图像，但我们在第 [6.1.1](#S6.SS1.SSS1 "6.1.1
    Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs
    and Augmentations have superior performance ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") 节中描述的实证测试表明，这些增强会提高模型性能。一般而言，采用半监督算法（SSL）的领域适应问题已经证明从数据增强中受益良多（例如，Sohn
    等，[2020](#bib.bib67)），这表明这一效果并非特定于我们的镜头搜索。
- en: A second tool that we use to augment our data is to generate new images that
    mimic the simulated lenses. In deep learning, the state-of-the-art method to produce
    generative models is by using Generative Adversarial Networks (GANs; Goodfellow
    et al., [2014](#bib.bib22); Arjovsky et al., [2017](#bib.bib3)). GANs generate
    unseen samples that are distinct from the original images, but are distributionally
    quite similar. These generative models are trained along with an adversarial discriminator
    that is attempting to distinguish between the fake and real images.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来增强数据的第二种工具是生成模拟镜头的新图像。在深度学习中，生成模型的最先进方法是使用生成对抗网络（GANs; Goodfellow 等，[2014](#bib.bib22);
    Arjovsky 等，[2017](#bib.bib3)）。GANs 生成的样本与原始图像不同，但在分布上相似。这些生成模型与一个对抗性判别器一起训练，判别器试图区分假图像和真实图像。
- en: We trained a WGAN-GP (Wasserstein GAN + Gradient Penalty; Gulrajani et al.,
    [2017](#bib.bib24)) on simulated lenses and add the generated images (see examples
    in Figures [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey") and [4](#S4.F4
    "Figure 4 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")) to our training set as another form of data augmentation.
    The motivation is that GANs can provide a rich source of more exotic data augmentations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在模拟透镜上训练了一个WGAN-GP（Wasserstein GAN + 梯度惩罚；Gulrajani等，[2017](#bib.bib24)），并将生成的图像（见图 [3](#S3.F3
    "图 3 ‣ 3.3 数据增强和GAN ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")和[4](#S4.F4 "图
    4 ‣ 4.1 生成NonLenses数据集 ‣ 4 训练和验证数据 ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")）添加到我们的训练集中，作为另一种数据增强形式。其动机是GAN可以提供更多异域数据增强的丰富来源。
- en: Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey") gives a brief summary
    of the steps discussed thus far. The training, testing, and validation data along
    with the model checkpoints used in this paper are made available on our GitHub
    repository ²²2[https://github.com/sxsheng/SHLDN](https://github.com/sxsheng/SHLDN).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S3.F3 "图 3 ‣ 3.3 数据增强和GAN ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")简要总结了迄今为止讨论的步骤。本文中使用的训练、测试和验证数据以及模型检查点已在我们的GitHub仓库中提供[https://github.com/sxsheng/SHLDN](https://github.com/sxsheng/SHLDN)。
- en: '![Refer to caption](img/f510c0503d08844f1df839c1640e3068.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f510c0503d08844f1df839c1640e3068.png)'
- en: '![Refer to caption](img/a4c0b4e8dd2cae2185c4cfcf19f4b7f9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a4c0b4e8dd2cae2185c4cfcf19f4b7f9.png)'
- en: 'Figure 2: Example of augmentations used during training. From left to right,
    the original RGB color composite image undergoes the series of augmentations described
    in Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning
    ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey"):
    RGB-shuffle, JPEG quality, Rot90, Translation, Flip, Color adjustment. The final
    image is then passed as input to the model.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：训练过程中使用的增强示例。从左到右，原始RGB彩色复合图像经过表 [1](#S3.T1 "表 1 ‣ 3.2 带半监督学习的领域适应 ‣ 3 深度学习架构和学习方法
    ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")中描述的一系列增强：RGB-洗牌、JPEG质量、Rot90、平移、翻转、颜色调整。最终图像作为输入传递给模型。
- en: '![Refer to caption](img/e4ea8ff5ca0bd5d87cf19e8d4b45f0e0.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e4ea8ff5ca0bd5d87cf19e8d4b45f0e0.png)'
- en: 'Figure 3: Schematic of the pipeline used in this work to test the performance
    of different learning methods described in Sections [3.2](#S3.SS2 "3.2 Domain
    adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture and learning
    methods used ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") and [3.3](#S3.SS3 "3.3 Data augmentation and
    GANs ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")
    (see text for details). The GAN generated lenses are only included in the training
    data for unsupervised learning methods (e.g., GAN+MixMatch).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：本研究中用于测试不同学习方法性能的流程图，详细信息见第 [3.2](#S3.SS2 "3.2 带半监督学习的领域适应 ‣ 3 深度学习架构和学习方法
    ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")和[3.3](#S3.SS3 "3.3 数据增强和GAN ‣ 3 深度学习架构和学习方法 ‣ 优化机器学习方法以在深度透镜调查中发现强引力透镜")节（具体信息请参见正文）。GAN生成的透镜仅包括在无监督学习方法（例如，GAN+MixMatch）的训练数据中。
- en: 4 Training and Validation data
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 训练和验证数据
- en: 'One of the challenges that we face in gravitational lens searches is trying
    to generate a training and testing dataset when having limited knowledge of the
    type of strong lenses that we might find in a survey. Prior to this work, Kubo
    & Dell’Antonio ([2008](#bib.bib39)) used a semi-automated method to search for
    lensed candidates in one of the DLS fields (F2) and uncovered two lens candidates.
    But in order to train a machine learning model to recognize lenses, we require
    Lens and NonLens image samples on the order of a few thousand. This is not a problem
    for NonLens galaxies, as they are abundant. But this is challenging for Lenses,
    as the known samples are extremely small compared to training requirements. We
    note that although the DLS area overlaps with other surveys used for strong lens
    searches (e.g., SDSS), no lens candidates have been published from these other
    surveys within the DLS footprint. This is likely due to the shallower depth of
    other surveys (see Section [6.4](#S6.SS4 "6.4 Implications for future large-area
    sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")). We must therefore generate an artificial lens training set. We describe
    our process of generating the training and testing datasets in this section.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在引力透镜搜索中面临的挑战之一是当对可能在调查中发现的强透镜类型了解有限时，生成训练和测试数据集。在此之前，Kubo 和 Dell’Antonio ([2008](#bib.bib39))
    使用了半自动化的方法在 DLS 区域（F2）搜索透镜候选者，并发现了两个透镜候选者。然而，为了训练机器学习模型以识别透镜，我们需要数量达到几千的透镜和非透镜图像样本。对于非透镜星系来说这不是问题，因为它们非常丰富。但对于透镜来说，这就很有挑战性，因为已知样本与训练需求相比极为稀少。我们注意到，尽管
    DLS 区域与用于强引力透镜搜索的其他调查（例如 SDSS）重叠，但在 DLS 区域内没有发布来自这些其他调查的透镜候选者。这可能是由于其他调查的深度较浅（参见第
    [6.4](#S6.SS4 "6.4 未来大范围天空调查的影响：灵敏度和角分辨率 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")
    节）。因此，我们必须生成一个人工透镜训练集。本节描述了生成训练和测试数据集的过程。
- en: 4.1 Generating the NonLenses dataset
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 生成 NonLenses 数据集
- en: Color png images centered on each object in the SurveyCatalog are constructed
    from $BVR$ fits files using HumVI (Marshall et al., [2015](#bib.bib50)). HumVI
    is based on the color composition algorithm described in Lupton et al. ([2004](#bib.bib48))
    and offers several tunable parameters to control the output image (e.g., contrast).
    We randomly sample objects from the SurveyCatalog and visually inspect the effect
    of changing the HumVI parameters $s$ and $p$ which control the contrast and color
    balance respectively. Although there is a degeneracy in the choice of these values,
    we pick ones that reasonably represent both the bright and dim features in the
    data (i.e., spanning the range of detectable surface brightness). Table [2](#S4.T2
    "Table 2 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") lists our chosen HumVI parameters and Figure [4](#S4.F4
    "Figure 4 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") (top panel) shows 4 randomly selected color-composite
    survey images generated using these values. The chosen HumVI parameters are kept
    constant and applied to all images in the survey. It is beyond the scope of this
    work to explore the effect of choosing different HumVI parameters on the performance
    of the models, but we note that color augmentations applied during training (Table [1](#S3.T1
    "Table 1 ‣ 3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey"); Section [6.1.1](#S6.SS1.SSS1
    "6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with
    GANs and Augmentations have superior performance ‣ 6 Results and Discussion ‣
    Optimizing machine learning methods to discover strong gravitational lenses in
    the Deep Lens Survey")) have the effect of making our models invariant to small
    perturbations in color.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在SurveyCatalog中，每个对象的彩色png图像是通过使用HumVI（Marshall等，[2015](#bib.bib50)）从$BVR$拟合文件构建的。HumVI基于Lupton等（[2004](#bib.bib48)）描述的颜色合成算法，并提供了多个可调参数来控制输出图像（例如，对比度）。我们随机从SurveyCatalog中抽取对象，直观检查改变HumVI参数$s$和$p$对对比度和颜色平衡的效果。尽管这些值的选择存在退化现象，我们选择了能够合理代表数据中明亮和暗淡特征的值（即，涵盖可检测的表面亮度范围）。表[2](#S4.T2
    "Table 2 ‣ 4.1 生成NonLenses数据集 ‣ 4 训练和验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")列出了我们选择的HumVI参数，图[4](#S4.F4
    "Figure 4 ‣ 4.1 生成NonLenses数据集 ‣ 4 训练和验证数据 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")（顶部面板）显示了使用这些值生成的4张随机选择的彩色合成调查图像。选择的HumVI参数保持不变，并应用于调查中的所有图像。探索选择不同HumVI参数对模型性能的影响超出了本工作的范围，但我们注意到，训练过程中应用的颜色增强（表[1](#S3.T1
    "Table 1 ‣ 3.2 领域适应与半监督学习 ‣ 3 深度学习架构及学习方法 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜"); 第[6.1.1](#S6.SS1.SSS1
    "6.1.1 数据增强的消融研究 ‣ 6.1 半监督算法与GANs和增强具有优越性能 ‣ 6 结果与讨论 ‣ 优化机器学习方法以发现深度透镜调查中的强引力透镜")节）使我们的模型对颜色的小扰动具有不变性。
- en: '![Refer to caption](img/76f1bc3fcff40ad920ebab66d215aaa6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/76f1bc3fcff40ad920ebab66d215aaa6.png)'
- en: 'Figure 4: *Top row:* Four randomly selected color composite survey images generated
    by running HumVI on their respective BVR FITS files. These images are examples
    of NonLenses used for training the network. Each image spans 25$\aas@@fstack{\prime\prime}$7
    $\times$ 25$\aas@@fstack{\prime\prime}$7 on the sky. Table [2](#S4.T2 "Table 2
    ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") lists the HumVI parameters used to generate these images. *Middle row:*
    The same set of survey images as in the top row, but superimposed with simulated
    lens configurations generated with glafic. Section [4.2](#S4.SS2 "4.2 Generating
    the simulated Lenses dataset ‣ 4 Training and Validation data ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")
    discusses the steps involved in detail. These images are examples of Lenses used
    during training. *Bottom row:* GAN generated simulated lenses. These are added
    to the training data as Lenses for our unsupervised models (e.g., GAN+MixMatch;
    Section. [3.3](#S3.SS3 "3.3 Data augmentation and GANs ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Parameter | Value |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '|  | glafic |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| Position | $x_{\text{def}},y_{\text{def}},x_{\text{src}},y_{\text{src}}$
    | U(-0.5,0.5) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| (arcseconds) |  |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| PA | $\theta_{\text{def}},\theta_{\text{src}}$ | U(0,180) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| (degrees) |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| Ellipticity | $e_{\text{def}},e_{\text{src}}$ | U(0.3,0.7) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| Dispersion | $\sigma_{\text{def}}$ | U(250,450) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| (km s^(-1)) |  |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '|  | $r_{\text{core, def}}$ | U(0,0.5) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| Brightness |  | U(200,600) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| (counts/$\text{pix}^{2}$) |  |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| Redshift | $z_{\text{def}}$ | U(0.3,0.7) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| Redshift | $z_{\text{src}}$ | U($z_{\text{def}}$ + 0.5, $z_{\text{def}}$
    + 2.5) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '|  | HUMVI |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '|  | -s | 0.2,0.7,1.3 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '|  | -p | 2.5, 0.01 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '|  | -m | 0.1 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Values for the glafic and HumVI parameters used to generate the simulated
    arcs and png color-composite images respectively. $U(x_{min},x_{max})$ indicates
    that the value was sampled from a uniform distribution with $x_{min}$ and $x_{max}$
    being the minimum and maximum values.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Generating the simulated Lenses dataset
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As described above, the scarcity of known lensed galaxies requires us to generate
    simulated lens samples for training ML models. Our approach is to add simulated
    lensed galaxies onto survey images, as has been used successfully in prior work
    (e.g., Jacobs et al., [2017](#bib.bib30); Jacobs et al., [2019](#bib.bib31)).
    For this work, we adopt an agnostic procedure for simulating lensed arcs which
    does not rely on photometric measurements of the deflector galaxy. We consider
    all galaxies which satisfy the magnitude cut criteria described in Section [2.1](#S2.SS1
    "2.1 Source selection and regions of interest ‣ 2 Deep Lens Survey Data ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") (regardless of their color) for simulating the lensed arcs. We note that
    $\sim 50\%$ of the galaxies in our SurveyCatalog have a BPZ best fit photometric
    template from Schmidt & Thorman ([2013](#bib.bib62)) indicating that they are
    massive early-type galaxies at intermediate redshifts, and are indeed likely to
    act as strong lenses. We discuss the actual color distribution for lens candidates
    in Section [6.2.3](#S6.SS2.SSS3 "6.2.3 Distribution of lensed candidates in color-color
    space ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey").
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Given any object from the training dataset, we assume that the central galaxy
    (“deflector”) is at a redshift $z_{\text{def}}\in[0.3,0.7]$ and is characterized
    by a Singular Isothermal Ellipsoid (SIE) mass density (Kormann et al., [1994](#bib.bib35)).
    The mass profile is dependent on the galaxy’s position ($x_{\text{def}},y_{\text{def}}$),
    ellipticity ($e_{\text{def}}$), position angle ($\theta_{\text{def}}$), velocity
    dispersion ($\sigma_{\text{def}}$), and choice of $r_{\text{core,def}}$. The values
    for these parameters are sampled from a uniform distribution spanning the ranges
    listed in Table [2](#S4.T2 "Table 2 ‣ 4.1 Generating the NonLenses dataset ‣ 4
    Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"). These values ensure that
    the resulting mass profile of the deflector is sufficient to produce a detectable
    lensing effect (i.e., $\Theta_{E}\gtrsim 1$ arcsecond). A background galaxy (“source”)
    is assumed to lie at a redshift $z_{\text{src}}$ with morphology given by a Sérsic
    profile parameterized by its position $(x_{src},y_{src})$, central brightness
    (in units of counts/$\text{pix}^{2}$), ellipticity ($e_{\text{src}}$), position
    angle ($\theta_{\text{src}}$), and a Sérsic index of 1\. The value for $z_{\text{src}}$
    is randomly chosen from a uniform distribution between $z_{\text{def}}+0.5$ and
    $z_{\text{def}}+2.5$. These values for the deflector and source redshifts are
    typical of spectroscopically measured values from previous strong lens surveys
    (e.g., Sonnenfeld et al., [2013](#bib.bib68); Bolton et al., [2008](#bib.bib9);
    Tran et al., [2022](#bib.bib75)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The light from the background galaxy is traced using glafic (Oguri, [2010](#bib.bib56))
    to produce a simulated lensed arc in the image plane. The simulated lensed arcs
    are convolved with the point spread function (PSF) of the survey, scaled by a
    factor of (1,1.5,3) for the BVR filters, and then added to the $BVR$ fits images
    of the galaxy. We model the PSF of the survey in all the three filters as a 2D
    Gaussian kernel with a FWHM of $\sim$1 arcsecond corresponding to the approximate
    average seeing conditions. In addition to smoothing, we add Poisson noise in order
    to produce more realistic simulated arc images. The fits images are converted
    to a color png image using HumVI (as described in Section [4.1](#S4.SS1 "4.1 Generating
    the NonLenses dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). For
    this paper, we focus on generating moderately bright blue lensed arcs, and the
    parameter ranges that produce these configurations are listed in Table [2](#S4.T2
    "Table 2 ‣ 4.1 Generating the NonLenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Generating the NonLenses
    dataset ‣ 4 Training and Validation data ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey") illustrates
    common configurations of the arcs produced using this method. However, we note
    that the RGB-shuffle augmentation which is applied during training produces arcs
    of different colors (e.g., Figure [2](#S3.F2 "Figure 2 ‣ 3.3 Data augmentation
    and GANs ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")). We find that such an approach, where the simulated arcs are not dependent
    on the photometric properties of the central deflector galaxy, likely serves as
    an additional form of augmentation. This approach prevents over-fitting of our
    deep learning models while allowing for rapid prototyping and testing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 Generating the training datasets: TrainingV1 and TrainingV2'
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the Lenses and NonLenses datasets, we construct two training sets: TrainingV1
    and TrainingV2\. The main difference between the two training sets is the number
    of labeled images used as Lenses and NonLenses. Prior work using CNNs (e.g., Jacobs
    et al., [2019](#bib.bib31)) have favored large training datasets (i.e., $\gtrsim$150,000
    galaxies). Therefore, for TrainingV1 we use 266,301 images for non-lenses and
    257,874 corresponding simulations as lenses (described in Section [4.2](#S4.SS2
    "4.2 Generating the simulated Lenses dataset ‣ 4 Training and Validation data
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")). Since semi-supervised training requires both labeled
    and unlabeled data, TrainingV1 cannot be used to test semi-supervised learning
    methods.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: For TrainingV2, we choose the number of images for each class to be similar
    to those used in standard computer vision datasets such as Canadian Institute
    for Advanced Research-10 (CIFAR-10; Krizhevsky, [2009b](#bib.bib37)) and Street
    View House Numbers (SVHN; Netzer et al., [2011](#bib.bib55)) dataset. We use a
    set of 7,074 human-labeled objects as NonLenses and 6,929 corresponding simulations
    as Lenses. The human labeling was carried out on randomly chosen images from Field-1
    (F1) of the DLS. We note that the choice of labeling the data only from F1 does
    not affect the results presented in the rest of the paper (see Appendix [A](#A1
    "Appendix A Model performance and final lens sample ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). The
    259,248 NonLens images which are not part of TrainingV2 serve as unlabeled data
    for our semi-supervised learning methods (e.g., MixMatch; Section [3.2](#S3.SS2
    "3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Counter-intuitively, we find that too much training data from simulated lenses
    and randomly selected NonLenses can hurt the performance of our algorithms. We
    refer readers to Section [6.1.2](#S6.SS1.SSS2 "6.1.2 Larger non-lens training
    samples can degrade the classifier’s performance ‣ 6.1 Semi-supervised algorithms
    with GANs and Augmentations have superior performance ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") and [S22](#bib.bib66) for further discussion of sample
    size effects, which can also contribute to differences in performance between
    the training sets. We note that the TrainingV2 labeled datasets are comparable
    to the size where we find peak performance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: We performed a 90-10 split for both TrainingV1 and TrainingV2, where 90% of
    the data was allocated for training the ResNetV2 model and 10% was kept aside
    for validation. We chose the maximum number of epochs (passes through the training
    dataset) for each training combination as 100, since this was sufficient to observe
    a plateau in the validation metrics. For each of the training combination described
    in Section [3](#S3 "3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey"), we conducted four independent trials and selected the checkpoint with
    the best validation metrics for testing it on the survey data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 5 Metric to evaluate model performance
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have described several models which are each tuned to optimize validation
    accuracy, which is measured on the validation dataset (Section [4.3](#S4.SS3 "4.3
    Generating the training datasets: TrainingV1 and TrainingV2 ‣ 4 Training and Validation
    data ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")) consisting of simulated Lenses and survey NonLenses.
    In order to gauge the performance of the models on their ability to find real
    lenses from the survey, we require a testing dataset consisting of lenses from
    the survey, as well as a metric to evaluate them on.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Generating the Testing dataset
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Curating testing data in our case is a challenging task. As discussed earlier,
    only two strong lenses in the entire survey were known prior to this work, which
    is insufficient for meaningful evaluation. Therefore, we use an ensemble of 5
    ResNet models trained on simulated lenses but using polar transformed images as
    input to the network. The exclusive task of this model is to find real lens candidates
    to add to our test dataset. We emphasize that this model is independent of the
    rest of the models discussed so far in this paper, and does not influence their
    performance in any way. Details of its implementation are discussed in [S22](#bib.bib66).
    It is beyond the scope of this paper to quantify the performance of ensemble models
    or the effect of polar transformation during training, but it is an interesting
    avenue for future work.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'We find 52 likely lens candidates from this model, of which 27 are deemed to
    be good candidates upon visual inspection. Therefore, we create two testing datasets:
    TestV1 and TestV2\. TestV1 contains all the 52 lens candidates found using our
    ensemble model approach, while TestV2 contains the 27 best visual candidates.
    NonLenses for both TestV1 and TestV2 were formed by randomly selecting 874 of
    our 8734 human-labeled non-lenses (Section [4](#S4 "4 Training and Validation
    data ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Precision and Recall
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A standard metric widely used in machine learning to evaluate the performance
    of test data on a trained model is the Precision-Recall curve (PR curve), where
    precision and recall are defined as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}},\quad\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}.$
    |  | (2) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: Here TP, FP, and FN are the number of True Positive, False Positive, and False
    Negative images respectively. These values are computed by passing a labeled test
    dataset (TestV1 and TestV2 in this case) through a trained model (e.g., GAN+Mixmatch)
    and setting different prediction thresholds.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Since the primary goal of this work is to find models which minimize the number
    of nonlensed images that an investigator encounters while maximizing the number
    of lensed images found (i.e., less FP and FN values), we seek models which have
    high precision at high recall. We present the results from our PR curve analysis
    in the next section.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 6 Results and Discussion
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We consider 17 variations on the learning approaches described in Section [3](#S3
    "3 Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey"): 4 supervised,
    6 semi-supervised, and 7 semi-supervised with GANs. SupervisedV1 and SupervisedV2
    are our baseline models. They were trained using a supervised learning approach
    with no data augmentation on TrainingV1 ($\sim$250,000 Lenses and NonLenses) and
    TrainingV2 ($\sim$7000 Lenses and NonLenses) respectively. On the other hand,
    SupervisedV1+DA and SupervisedV2+DA were trained using supervised learning with
    data augmentation (DA). The rest of the models were trained on TrainingV2 using
    semi-supervised learning methods with DA or with DA + GANs. In this subsection,
    we summarize the performance of these different models. We primarily use the PR
    curve (Section [5.2](#S5.SS2 "5.2 Precision and Recall ‣ 5 Metric to evaluate
    model performance ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")) evaluated on our TestV1 and TestV2 sets to gauge
    which models perform best. We note that our methodology paper [S22](#bib.bib66)
    includes an additional discussion of these results.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: We plot the PR curve obtained for our best-performing baseline models (SupervisedV1,
    SupervisedV2) along with a subset of semi-supervised and GAN+semi-supervised models
    in Figure [5](#S6.F5 "Figure 5 ‣ 6.1.1 Ablation study on data augmentations ‣
    6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") (see Tables 3 and 4 of [S22](#bib.bib66)
    for additional model results). Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation study
    on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") lists
    the precision value obtained for a subset of models at 100% recall. We find that
    our models tend to generalize poorly when trained without any augmentations. Our
    baseline models, trained without any data augmentation, performed worst out of
    all models at every recall level. For example, at 100% recall, the baseline SupervisedV1
    and SupervisedV2 have a precision of $\sim 3\%$ on our TestV2 set, whereas the
    GAN+$\Pi$-model has a precision of $\sim 22\%$. The poor precision values of our
    supervised models may reflect challenges in simulating the characteristics of
    lenses from a survey given limited priors. Fortunately, we find that data augmentation
    methods are able to address this problem. We find a factor $\sim$5-10$\times$
    improved precision across almost all recall levels when applying the full set
    of augmentations (Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised
    learning ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")) to our supervised models.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The improvement of semi-supervised over supervised algorithms suggests that
    valuable features can in fact be extracted from the mostly unlabeled NonLenses,
    providing benefits in the classification of real lenses. Adding GAN images to
    our training pipelines had a seemingly profound impact at all recall levels, especially
    at higher recalls where more difficult-to-classify images come into play. This
    suggests that GAN-generated images contain subtle variations which, while not
    necessarily significant to the naked eye, do in fact produce a strong regularizing
    effect when used in training.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Ablation study on data augmentations
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We investigated the impact of each of the data augmentations we used by doing
    an ablation study using TrainingV2\. The results from this study are tabulated
    in Table [7](#A1.T7 "Table 7 ‣ Appendix A Model performance and final lens sample
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"). We find that removing GAN images from the training
    sets causes a noticeable decrease in model performance at all recall levels, which
    agrees with our earlier conclusion. It also appears that color augmentations and
    JPEG quality play a very significant role in model performance. Including these
    three augmentations in our training pipelines is apparently what allows our model
    to generalize so well, despite relying on simulated lenses for training. A curious
    result from this ablation study is that multiples of 90-degree rotations actually
    had a negative effect on model performance. The difference in performance is relatively
    small compared to that seen for other augmentations (e.g., GANs), but persists
    at all recall rates. A possible reason for this could be our small validation
    and test sets. Because the validation set is small, model selection may be biased
    towards certain orientations of the image. Likewise, an equally small test set
    may have preferred orientations that the model does not generalize to, resulting
    in degraded performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/4f5845efbceae76c9bfe98ec0c78b76e.png) | ![Refer to
    caption](img/b563c47b7a3161465dce364eeaadd948.png) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: 'Figure 5: Precision-Recall curves (PR curves) for a subset of the models described
    in Section [3](#S3 "3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") obtained using TestV1 (*Left*) and TestV2 (*Right*). TestV1 contains
    52 lens candidates found using our ensemble model approach, while TestV2 contains
    the 27 best visual candidates (Section [5.1](#S5.SS1 "5.1 Generating the Testing
    dataset ‣ 5 Metric to evaluate model performance ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). SupervisedV1
    and SupervisedV2 are our baseline models. They were trained using a supervised
    learning approach with no data augmentation on TrainingV1 ($\sim$250,000 Lenses
    and NonLenses) and TrainingV2 ($\sim$7000 Lenses and NonLenses) respectively.
    The rest of the models were trained on TrainingV2 with augmentations. MixMatch
    and $\Pi$-Model are semi-supervised learning approaches, whereas GAN+MixMatch
    and GAN+$\Pi$-Model use GAN generated images along with semi-supervised learning
    (see Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey") and Section [3](#S3
    "3 Deep Learning Architecture and learning methods used ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") for
    details). GAN+SupervisedV2 uses supervised learning with GAN generated images.
    Models which use semi-supervised learning along with GANs clearly outperform our
    baseline supervised learning models at all recall values, with GAN+$\Pi$-model
    having the highest precision at 100% recall (see results in Table [3](#S6.T3 "Table
    3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms
    with GANs and Augmentations have superior performance ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey"); we note that Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation
    study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") reports
    the average of our four runs while this figure shows the runs with the best precision).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Training data used | TestV1 Precision(%) | TestV2 Precision(%) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| SupervisedV1 | TrainingV1 w/ no augmentation | 5.62$\pm$0.01 | 3.01$\pm$0.02
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| SupervisedV2 | TrainingV2 w/ no augmentation | 5.65$\pm$0.02 | 3.06$\pm$0.04
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| MixMatch | TrainingV2 w/ augmentation | 12.28$\pm$5.09 | 6.84$\pm$3.00 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| $\Pi$-Model | TrainingV2 w/ augmentation | 13.41$\pm$2.33 | 8.68$\pm$1.49
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| GAN + Supervised | TrainingV2 w/ augmentation | 8.25$\pm$2.85 | 6.05$\pm$2.69
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| GAN + MixMatch | TrainingV2 w/ augmentation | 14.13$\pm$6.53 | 7.97$\pm$3.93
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| GAN + $\Pi$-Model | TrainingV2 w/ augmentation | 15.2$\pm$6.21 | 22.27$\pm$7.71
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Average precision values were obtained for a subset of the models
    tested at $100\%$ recall. We note that a table with the performance of all the
    models at various recall values is presented in [S22](#bib.bib66). Here the average
    is computed from the performance of four independent runs on the test sets. The
    uncertainties are $1\sigma$ standard deviations from the mean.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f9feb74cd6d3ef4bb4a3b2d24875a6c5.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Grade-A lenses found in the DLS along with the rank (Section [6.2](#S6.SS2
    "6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")) assigned to them by GAN+MixMatch(MM) and GAN+$\Pi$-model(PI) models.
    All Grade-A lenses have a clear arc morphology and are located near a moderately
    massive galaxy or group, making them convincing lens candidates. Among these candidates,
    212072337 and 432021600 have been spectroscopically confirmed to be true strong
    lens systems (Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Spectroscopic confirmation of
    two Grade-A lenses ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/af68b7ece7173fd5a27bd31f5fa26f59.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Grade-B lenses found along with the rank (Section [6.2](#S6.SS2 "6.2
    Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey"))
    assigned to them by GAN+MixMatch(MM) and GAN+PiModel(PI) models. Targets in this
    category have either a tentative nebulous arc-like feature surrounding a massive
    galaxy, or have approximately linear extended morphology near an apparent galaxy
    group or cluster. It is hard to discern if these features correspond to lensed
    arcs or are caused by blending of multiple sources, hence the uncertain Grade-B
    classification.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Larger non-lens training samples can degrade the classifier’s performance
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To understand why our larger training set (TrainingV1) led to poorer generalization,
    we also performed a test where we fixed the number of simulated Lenses and varied
    the number of NonLenses in the dataset (see [S22](#bib.bib66), Table 5). As we
    gradually increased the number of NonLenses in the training data from 0 to 256,000,
    we saw that precision gradually increased and peaked at around 8000-16000 NonLenses,
    then started to significantly decrease to around $\sim$6% precision for nearly
    all recall levels. One possible explanation for this effect is that as we increase
    the number of NonLenses in training, we also increase the number of NonLens false
    positives which appear similar to real lenses in the survey data (and perhaps
    even more similar to real lenses than the simulations we use). As a result, the
    decision boundary for non-lenses overlaps more with the regions occupied by real
    lenses, leading to higher levels of misclassification. Therefore, care must be
    taken in constructing training data based on simulations. Arbitrarily increasing
    the size of the training data can evidently lead to significantly worse performance
    than using a smaller well-curated training set.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we find that models trained with a semi-supervised learning approach
    using TrainingV2 and GAN-generated images along with all of our proposed list
    of data augmentations have high precision values at all recall values. In particular,
    among the models tested, the top two performing models are GAN+MixMatch and GAN+$\Pi$-model.
    In the following subsection, we turn to apply these models to the full set of
    DLS survey images (i.e., SurveyCatalog in Section [2.1](#S2.SS1 "2.1 Source selection
    and regions of interest ‣ 2 Deep Lens Survey Data ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey"))
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Catalog of Lens candidates found
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having established which of our trained models perform best on our test set
    in terms of PR curves, we now turn to the key question of how many lenses are
    identified in the DLS and importantly, how much human inspection effort is required
    to find them.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank | Number of unique | Number of | Number of | Total lenses | Number of
    | Number of | Number of |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| threshold | lenses investigated | Grade-A lenses | Grade-A lenses | Grade-A
    | Grade-A lenses | Grade-A lenses | Grade-A lenses |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '|  | (G+MM,G+PI) | (G+MM) | (G+PI) | (both models) | (SupervisedV2) | (SupervisedV2+DA)
    | (SupervisedV2+DA+GAN) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| 12 | 9, 9 | 1 | 1 | 1 | 0 | 0 | 0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| 25 | 19, 16 | 1 | 3 | 3 | 0 | 0 | 0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| 100 | 67, 56 | 2 | 3 | 3 | 0 | 1 | 2 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| 800 | 513, 430 | 4 | 3 | 4 | 1 | 2 | 3 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| 2800 | 1735, 1459 | 6 | 5 | 8 | - | - | - |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| 4000 | 2459, 2076 | 7 | 5 | 9 | - | - | - |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Comparison of the number of Grade-A lenses found by different models
    tested. The predictions from the models are ranked such that the most likely predicted
    lens has rank=1\. The rank threshold value sets the number of lenses that an investigator
    has to visually inspect. The left two columns show the chosen rank threshold and
    the number of unique lenses that it corresponds to (removing duplicates as described
    in Section [6.2](#S6.SS2 "6.2 Catalog of Lens candidates found ‣ 6 Results and
    Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). Our best performing models GAN+MixMatch (G+MM)
    and GAN+PiModel (G+PI) find 4 and 3 lensed candidates each among the top $\sim$500
    unique images (top 800 ranks), and 7 lensed candidates each when the top $\sim
    2300$ images are investigated. Combining the results from both the models, we
    find 9 Grade-A candidates (shown in Figure [6](#S6.F6 "Figure 6 ‣ 6.1.1 Ablation
    study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")). The
    right three columns show the number of lenses found from the SupervisedV2, SupervisedV2+Data
    Augmentation(DA) and SupervisedV2+DA+GAN. Although they find fewer ($\lesssim
    50\%$) lens candidates than our best performing models, we can see that DA and
    GANs are able to boost the number of lenses found from 1 to 3 at a rank threshold
    of 800.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank | Number of unique | Number of | Number of | Total lenses | Total lenses
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| threshold | lenses investigated | Grade-B lenses | Grade-B lenses | Grade-B
    lenses | Grade-A+B lenses |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '|  | (G+MM,G+PI) | (G+MM) | (G+PI) | (both models) | (both models) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| 12 | 9, 9 | 0 | 0 | 0 | 1 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| 25 | 19, 16 | 0 | 0 | 0 | 3 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| 100 | 67, 56 | 0 | 2 | 2 | 5 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| 800 | 513, 430 | 2 | 5 | 5 | 9 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| 2800 | 1735, 1459 | 6 | 11 | 12 | 20 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| 4000 | 2459, 2076 | 9 | 11 | 13 | 22 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Similar to Table [5](#S6.T5 "Table 5 ‣ 6.2 Catalog of Lens candidates
    found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey") but for Grade-B lenses.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: We obtain a $\sim$97% and $\sim$86% precision at 50% recall (i.e., to find 50%
    lenses from our test set) for the GAN + MixMatch and GAN + $\Pi$-model respectively.
    On the other hand, if we needed to reach $100\%$ recall (i.e., find all the lenses
    from our test set), the precision drops to $\sim 8\%$ and $\sim 22\%$ respectively
    (Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). Based on the results from Pourrahmani
    et al. ([2018](#bib.bib60)) who searched for gravitational lenses in the COSMOS
    field with excellent image quality from the Hubble Space Telescope, we expect
    a maximum of $\sim$7 Grade-A lenses per square degree. This gives an upper bound
    of $\lesssim$140 lens candidates in the 20 degree² of the DLS, where the number
    of detectable lenses will be smaller since many of the COSMOS lenses have Einstein
    radii which are too small to resolve in ground-based DLS data, and because the
    COSMOS data are more sensitive. We estimate that half of the COSMOS lenses are
    unresolved in DLS based on the distribution of Einstein radii of the sample, with
    median $\simeq$1$\aas@@fstack{\prime\prime}$2 reported by Pourrahmani et al. ([2018](#bib.bib60)),
    such that we would expect $\sim$70 detectable lenses in the DLS survey area. At
    100% recall, 8% precision, and a TP$\approx$70, the number of false positive (FP)
    images that an investigator has to look at to find 70 lenses is $\sim 850$. If
    the number of detectable lenses in DLS is much lower, as suggested by samples
    reported from large ground-based campaigns such as the Dark Energy Survey (DES),
    then the total number of images and false positives which must be searched is
    correspondingly smaller. We also note that these estimates are based on the assumption
    that the precision values obtained from our test set also apply to the survey
    data. A decrease in this precision value would increase the number of FPs. Therefore,
    considering these uncertainties, for this work we visually examine the top 12,
    25, 100, 800, 2800, and 4000 predictions from the GAN+Mixmatch and GAN+$\Pi$-model,
    and investigate the number of lenses found. Throughout this paper, we focus only
    on using relative ranks (i.e., top $n$ prediction) to assess model performance
    since the distribution of absolute prediction threshold values (such as those
    employed in Jacobs et al. [2019](#bib.bib31)) can vary significantly between different
    models (Appendix [12](#A1.F12 "Figure 12 ‣ Appendix A Model performance and final
    lens sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). The absolute prediction values obtained from
    different models can be calibrated, for example by scaling the obtained model
    weights to the softmax layer, but this is beyond the scope of our study. We note
    that the relative ranks which we use in this study will be unaffected under such
    scaling transformations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: One substantial caveat when looking at the top $n$ predictions is that, due
    to the density of galaxies in the sky and our image selection method, the top
    predictions are not necessarily unique. For example, the top 25 predictions from
    the GAN+$\Pi$-Model contain 17 unique sources and 8 duplicates centered on different
    nearby objects (shown in Figure [13](#A1.F13 "Figure 13 ‣ Appendix A Model performance
    and final lens sample ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") in the Appendix). For the top 2800
    predictions, the number of unique candidates is $\sim 1600$ on average (i.e.,
    $\sim 40\%$ are repeated). Since this is a significant portion of the number of
    images and would increase human effort during labeling, we remove such repetitions
    based on their sky coordinates. Given our image size, we remove duplicates within
    a radius of 26 arcseconds of each object in the top $n$ predictions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining images are then replaced with a larger field of view, ensuring
    that a given region of the sky needs to be visually inspected only once. We note
    that removing duplicates is strictly a post-processing step. Two of us (KVGC and
    TJ) visually inspected the lens candidates and classified them into confidence
    categories: Grade-A, Grade-B, Grade-C, and non-lenses. Grade-A indicates a high
    likelihood of being a strong lens system, on the basis of a clear arc morphology
    and/or coincidence with a moderately massive group of galaxies. Grade-B lenses
    generally have a nebulous arc-like feature surrounding a massive galaxy and/or
    have approximated linear extended arc morphology near a group or cluster of galaxies.
    It is uncertain if these features are from the lens or the effect of blending
    multiple sources. Grade-C lenses (not discussed in this paper) are the lowest-confidence
    candidates which typically show blended arc-like features likely arising from
    spiral arms, tidal features, or asymmetric diffuse light from the onset of mergers.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Figures [6](#S6.F6 "Figure 6 ‣ 6.1.1 Ablation study on data augmentations ‣
    6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey") and [7](#S6.F7 "Figure 7 ‣ 6.1.1
    Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms with GANs
    and Augmentations have superior performance ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") show the color composite images for the 9 Grade-A and 13 Grade-B lenses
    found from the survey upon visually inspecting $\sim$ 2500 unique candidates (the
    top 4000 by rank). Their sky coordinates are listed in Table [8](#A1.T8 "Table
    8 ‣ Appendix A Model performance and final lens sample ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") in the
    Appendix. Several of the Grade-A lenses appear to be compound lenses or part of
    a moderately massive group or cluster of galaxies. This is interesting since our
    training data consists of only galaxy-galaxy lenses. This is likely due to the
    addition of GAN-generated images to our training data, as the GAN-generated images
    (Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Data augmentation and GANs ‣ 3 Deep Learning
    Architecture and learning methods used ‣ Optimizing machine learning methods to
    discover strong gravitational lenses in the Deep Lens Survey")) include irregularly
    shaped arcs.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: DLS212072337 ($z=1.81$)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46a6ed83bb9c00a87a84c03fc89bf8ce.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: DLS432021848 ($z=1.94$; tentative)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/096f6830d97d090595d5865401bc071a.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: *(*Top): NIRES spectra of Grade-A lens DLS212072337 at a redshift
    of $z=1.81$ with prominent [O iii] emission lines marked in blue. *(*Bottom):
    NIRES spectra of DLS432021848 showing the single emission line detected at $1.93\mu
    m$ which we tentatively identify as H$\alpha$ at $z=1.94$. In both panels the
    scaled sky spectrum is shown in orange (offset by -100), with gray shading denoting
    regions affected by strong sky lines.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e578a5a04701de6f764c635142856f80.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Distribution of the top 4000 lenses found by GAN+$\Pi$-model in color-color
    space. The left panel shows $B-R$ vs $B-V$ and the right panel shows $B-R$ vs
    $R-z$. The images above show examples of galaxies found in the two regions of
    the left panel separated by the purple line. Low-z galaxy candidates are clustered
    in the region above the trend line whereas all of the Grade-A lens candidates
    are below it. The right panel additionally shows that lens candidates are typically
    redder in $R-z$ colors ($\gtrsim 0.5$). A color selection based on the purple
    lines in each panel would yield higher precision in our lens candidate samples
    while retaining nearly all of the most probable lenses.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Human inspection effort
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We now examine how much human effort is required to find the 22 Grade-A and
    B lens candidates. To quantify the effort we consider the number of lenses found
    at different ranks, listed in Table [5](#S6.T5 "Table 5 ‣ 6.2 Catalog of Lens
    candidates found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey"). The rank threshold
    determines the number of unique images which must be visually inspected. Looking
    at the top 800 predictions from the GAN+MixMatch and GAN+$\Pi$-model (corresponding
    to 513 and 430 unique lens candidates respectively), we find 4 and 3 Grade-A lenses,
    and 2 and 5 Grade-B lenses respectively. This is several times ($\gtrsim$3$\times$)
    higher sky density than has been found from the shallower ground-based DES survey,
    and smaller than the density found in COSMOS with HST, as expected. The number
    of lens candidates found increases to 9 Grade-A and 13 Grade-B candidates when
    the top 4000 candidates ($\sim$2500 unique images) are considered. This corresponds
    to $\sim$1 lens per deg² searched, which is $\gtrsim$10$\times$ higher sky density
    of lenses compared to previous shallower ground-based surveys (as we discuss in
    Section [6.4](#S6.SS4 "6.4 Implications for future large-area sky surveys: sensitivity
    and angular resolution ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, our supervised models (e.g., SupervisedV1, SupervisedV2) find
    $\lesssim 50\%$ of these top lens candidates. They also have lower precision values
    (Table [3](#S6.T3 "Table 3 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")), with no compelling lenses found
    within the top $17$ candidates inspected (whereas G+PI finds 3 within this threshold
    range). This again highlights the value of adding data augmentation and GAN images.
    The SupervisedV2+DA+GAN model finds 3 times more lenses than SupervisedV2 within
    the same threshold range. These results demonstrate the efficiency with which
    the models explored in this work can find strong lenses.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Spectroscopic confirmation of two Grade-A lenses
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While image morphology can provide compelling evidence for strong gravitational
    lensing, spectroscopic redshifts are the standard to unambiguously establish the
    lensing nature of a system. We have obtained spectroscopy with Keck Observatory
    to confirm the lensing nature of two Grade-A systems presented herein: DLS212072337
    and DLS432021848 (Figure [8](#S6.F8 "Figure 8 ‣ 6.2 Catalog of Lens candidates
    found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey")). Observations of the arcs
    were conducted with NIRES (Wilson et al., [2004](#bib.bib77)) on the Keck II telescope.
    Full details of the observations and data reduction are described in Tran et al.
    ([2022](#bib.bib75)), along with spectroscopic redshifts for DLS212072337 (reported
    as AGEL091935+303156). We find a secure redshift of $z_{\text{arc}}=1.81$ for
    DLS212072337 from detection of H$\alpha$ $\lambda$6564 and [O III] $\lambda\lambda$4960,5008
    emission lines. The deflector galaxy is at a redshift of $z_{\text{def}}=0.43$,
    based on stellar absorption features from optical SDSS/BOSS spectra.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'We observed DLS432021848 with NIRES on 12 January 2022 using the same methodology.
    We obtained 6 exposures of 300 seconds each. We detect a single emission line
    at $~{}\lambda=1.93\mu\text{m}$ which we tentatively identify as either H$\alpha$
    at $z_{\text{arc}}=1.94$ or [O III] $\lambda$5008 at $z_{\text{arc}}=2.85$. However,
    we are unable to confirm the redshift with other strong lines, which fall in regions
    of poor atmospheric transmission at both potential redshifts. We find further
    support for the lensing nature of DLS432021848 from its morphology in follow-up
    HST imaging (discussed in Section [6.4](#S6.SS4 "6.4 Implications for future large-area
    sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")), which shows clear kurtosis and evidence of multiple lensed images.
    Thus we are reasonably confident that this is indeed a strong lensing system on
    the basis of high-resolution imaging, despite the limited spectroscopic information.
    Together with DLS212072337, these results give additional confidence in the sample
    of lens candidates presented in this paper and demonstrate that our methods are
    successful.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: We note that redshifts are known for two additional Grade-A candidate deflectors
    (DLS212148326, DLS421095124) from archival data. DLS212148326 is at $z_{\text{def}}=0.424$
    from SDSS/BOSS spectra, while DLS421095124 is part of a massive galaxy cluster
    spectroscopically confirmed at $z_{\text{def}}=0.680$ (Wittman et al., [2003](#bib.bib79);
    Wittman et al., [2006](#bib.bib80), reported as DLSCL J1055.2-0503). These redshifts
    are promising, as the distances and approximate masses are consistent with the
    deflection angles implied by the strong lensing interpretation of these images.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Distribution of lensed candidates in color-color space
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The analysis and model performance described thus far in the paper is based
    on a source selection using an intentionally simple $R$ band magnitude cut and
    SExtractor flags (Section [2.1](#S2.SS1 "2.1 Source selection and regions of interest
    ‣ 2 Deep Lens Survey Data ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). We have demonstrated in the above
    sections that such cuts are sufficient to search for lensed candidates in the
    DLS. However, more sophisticated selections can increase the efficiency of lens
    searches. Here we briefly consider how color selection can provide higher-purity
    samples.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [9](#S6.F9 "Figure 9 ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") we show the distribution of Grade-A and B lenses
    from Section [6.2](#S6.SS2 "6.2 Catalog of Lens candidates found ‣ 6 Results and
    Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") in various color-color spaces, along with the
    top 4000 ranked images from the GAN-$\Pi$-model as an example. These colors generally
    correspond to the central (candidate deflector) galaxy. The top lens candidates
    are not distributed uniformly, and we demonstrate two color-color selections where
    the top candidates are clustered: $(B-V)<0.56(B-R)-0.02$ (purple line in left
    panel), and $R-z\gtrsim 0.4$ (right panel). Such simple color cuts can retain
    all Grade-A lenses while removing the majority of false positives, thereby reducing
    the required human inspection effort. Physically, these colors are indicative
    of 4000 Å breaks at redshifts $z\gtrsim 0.25$ (i.e. in the $V$ or $R$ band) whereas
    lower-$z$ galaxies are less likely to act as strong lenses.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of lens candidates in color space suggests that the precision
    of our models can be further improved by adopting color criteria as a pre- or
    post-processing step, with minimal loss of the best candidates. Using photometric
    redshift and mass estimates is a similar and potentially even more promising method
    (Schmidt & Thorman, [2013](#bib.bib62)) although it is beyond the scope of this
    paper. Alternatively, a state-of-the-art automated means to address this would
    be by using self-similarity based approaches (e.g., Stein et al., [2021](#bib.bib72)),
    wherein a CNN further classifies the lens probabilities based on their similarity
    with each other.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Simulated Lens |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/fd799fc2a314bccb6e59b862a2f3f466.png) | ![Refer to
    caption](img/fe9f67eca2d0530e8540eb15d54696bc.png) | ![Refer to caption](img/0dfcf8170d042162540146bb64a0ba07.png)
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '|  | Lenses found in DLS |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/4acf4a18c43c2ec72f0054cfc7f39d80.png) | ![Refer to
    caption](img/f7a9f921b1ac930c89460311201f9fdb.png) | ![Refer to caption](img/c2ac9a854f0ef76f9df6d7488a15d049.png)
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/96eb6a632a1d7da39266a3c7e3e4eb54.png) | ![Refer to
    caption](img/3c0405d32bf175fa35f6a135feae4a72.png) | ![Refer to caption](img/41fe0a3e6c3c951f843d02b47c2f7ac3.png)
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '|  | False Positive |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/4a0909392ae6cd5ed6e8b05174970daf.png) | ![Refer to
    caption](img/624900a89ff1d2211285c8e9b2ebccc3.png) | ![Refer to caption](img/2035371952e5a738f999d52fab84bddd.png)
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/1ebe6fac7c53d3c2ce74cabbecccedcd.png) | ![Refer to
    caption](img/f3a9decd588bd5f2c1225dfb68120db6.png) | ![Refer to caption](img/2ab90d661252d8607fe4da82af929c98.png)
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '|  | NonLens |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/b86684d65f2f4f8fd1b65fd68d0a53bd.png) | ![Refer to
    caption](img/975e912c0c2dd45583a862792a0ea095.png) | ![Refer to caption](img/0b876ca983ef7a599fb443effe499c4b.png)
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: 'Figure 10: Grad-CAM++ heatmaps for an example simulated lens, two Grade-A lenses,
    two false positive lenses, and a NonLens. The left column shows the color composite
    image obtained from HumVI and passed to the model. The right column shows the
    Gradcam++ heatmaps. The red and green shading indicates regions of high and moderate
    importance to the model, respectively, whereas blue represents low importance.
    The middle column shows the heatmaps superimposed on input images for visualization
    purposes. For the simulated lens, we can clearly see that the entire lensed arc
    region is taken into consideration. For the Grade-A lens candidates found in DLS,
    we also find that the lensed arc features are considered important by the model,
    despite a range of lensing morphologies and colors. This suggests that models
    have indeed successfully generalized to the survey data. Notably, the massive
    deflector (i.e., the luminous red galaxy) causing the lensing effect is not highlighted
    in the simulated or candidate lens systems. Additional objects in the field are
    also highlighted in heatmaps for the Grade-A lenses, which is also apparent in
    the False Positive and NonLens examples. In the case of the False Positives, the
    highlighted object distributions resemble an “Einstein cross” lens configuration.
    Heatmaps for all the Grade-A lenses are provided in Figure [15](#A1.F15 "Figure
    15 ‣ Appendix A Model performance and final lens sample ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey") in the
    appendix.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Lensing signatures identified by the models
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now examine which features of the lens candidate images are most relevant
    for the model predictions. Deep neural networks (such as ResNetV2 used in this
    work) are often considered as “black boxes” with all input information collapsed
    to a simple prediction for the user to interpret. Having only a single output,
    it is impossible to discern which distinguishing features of a gravitational lens
    are actually being identified and considered by the models. Fortunately, in the
    past few years, there have been a variety of methods proposed to alleviate this
    such as occlusion methods, Guided Backprop (Springenberg et al., [2015](#bib.bib71)),
    CAM (Zhou et al., [2016](#bib.bib82)), Grad-CAM (Selvaraju et al., [2017](#bib.bib64)),
    Grad-CAM++ (Chattopadhay et al., [2018](#bib.bib12)), and DeepSHAP (Fernando et al.,
    [2019](#bib.bib18)).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-based interpretation methods (e.g., Grad-CAM++) effectively compute
    gradients on intermediate feature maps of the network to determine the importance
    of a feature. These gradient maps can then be overlaid on top of the original
    input image, in order to assess which image regions are contributing most to the
    predicted output from the classifier. These methods are not without drawbacks
    (e.g., Adebayo et al., [2018](#bib.bib1)) but can provide valuable insight. Here
    we use Grad-CAM++ to analyze some of our trained models.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Figure [10](#S6.F10 "Figure 10 ‣ 6.2.3 Distribution of lensed candidates in
    color-color space ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") shows Grad-CAM++ heatmaps obtained for a few illustrative
    examples. We consider a simulated lens from the training data, real Grade-A lenses
    from the survey, false positive images (i.e., images which are classified as lenses
    but show no visual evidence of lensing), and a non-lens. In the case of the simulated
    lens, it is clear that the model is indeed making its prediction based on the
    lensed arc features. For the Grade-A lenses, the model does indeed discern the
    lensed arcs, but there are additional unrelated regions within the images that
    also influence its decision. Curiously, the central massive deflector galaxy is
    not highlighted in these cases. In the case of the false positives, the model
    encouragingly is not misled by the extended central galaxies, but rather the heatmap
    highlights multiple sources of similar color which surround the central galaxy.
    For example in the spiral galaxy false-positive image, it is clear that the model
    picks up on the three nearby red objects. The location and color of these nearby
    objects is indeed similar to plausible multiple-image lensing configurations.
    It thus appears that the model has successfully learned to identify the astrophysical
    signatures of strong lensing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Finding red arcs
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As discussed in Section [4.2](#S4.SS2 "4.2 Generating the simulated Lenses dataset
    ‣ 4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), our Lens dataset used for
    training only consists of lensed arcs with blue optical colors. However, it is
    encouraging that the models have also identified red arcs such as the system DLS212148326
    (Figure [6](#S6.F6 "Figure 6 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1
    Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). The network may be learning to
    identify red arcs through color augmentations (Figure [2](#S3.F2 "Figure 2 ‣ 3.3
    Data augmentation and GANs ‣ 3 Deep Learning Architecture and learning methods
    used ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")). Although red-lensed arcs are known to exist, presumably
    a training dataset consisting of only blue arcs is not ideal to robustly search
    for and quantify them. It could be the case that adding more augmentations or
    fine-tuning existing ones might suffice to search for arcs of various colors.
    Alternatively, a broader range of arc colors could be used in the simulated training
    set, or a separate classifier could be constructed from a training set of red
    arcs. Given our adopted training set, we consider the number of red-lensed arcs
    found from this work to be a lower limit (relative to the blue arcs). Additionally,
    there are likely many fainter blue or red arcs which our training set does not
    represent, although the detection of fainter objects is naturally more challenging.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '6.4 Implications for future large-area sky surveys: sensitivity and angular
    resolution'
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next generation of wide-area sky surveys is expected to uncover $\gtrsim
    10^{5}$ strong lens systems (e.g., Oguri & Marshall, [2010](#bib.bib57); Collett,
    [2015](#bib.bib14)). Here we consider the gain in lens detection with survey depth
    and angular resolution based on our DLS sample from Section [6.2](#S6.SS2 "6.2
    Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey").
    We compare the sky density of detected lens candidates with two other illustrative
    examples of CNN-based searches in Table [6](#S6.T6 "Table 6 ‣ 6.4 Implications
    for future large-area sky surveys: sensitivity and angular resolution ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey"). In our DLS search, we find $\sim$0.5 Grade-A
    lenses per square degree (or $\sim$1 Grade-A+B lenses per square degree). This
    is considerably larger than found in shallower surveys such as SDSS and DES, which
    have uncovered $\sim$0.1 lenses per square degree (in regions far from the galactic
    plane). While these surveys have a comparable seeing-limited resolution, sharper
    image quality enables more lenses to be found. An example is the search of COSMOS
    HST imaging by Pourrahmani et al. ([2018](#bib.bib60)) using a CNN approach, which
    found 13 Grade-A candidates and 70 Grade-A+B candidates in the 2 square degree
    field (i.e., $\sim$35 per square degree). Therefore, we see that the sky density
    of detectable strong lens systems increases by $\sim$10 times when going from
    shallower ground-based surveys (e.g., SDSS) to the DLS, and by another factor
    of $\gtrsim$10 when the angular resolution is improved by an order of magnitude
    with space-based HST imaging at modest depth. These results generally support
    the predictions of large lens samples which will become detectable with near-future
    surveys planned with the Rubin (LSST Science Collaboration et al., [2009](#bib.bib40)),
    Roman (Spergel et al., [2015](#bib.bib70)), and Euclid (Laureijs et al., [2011](#bib.bib42))
    observatories.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'To visually illustrate the detection of lenses at different depths and angular
    resolutions, Figure [11](#S6.F11 "Figure 11 ‣ 6.4 Implications for future large-area
    sky surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") compares DECaLS, DLS, and HST imaging³³3The HST image was secured as
    part of program HST-GO-16773 targeting lens candidates identified primarily in
    DES and DECaLS imaging (Tran et al., [2022](#bib.bib75)). In brief, the HST image
    in Figure [11](#S6.F11 "Figure 11 ‣ 6.4 Implications for future large-area sky
    surveys: sensitivity and angular resolution ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey") was taken with WFC3-IR in the F140W filter with $\sim$30 minutes of exposure
    time ($<$1 orbit), and reduced using standard procedures. Details of the HST program
    will be described in a forthcoming paper. for the Grade-A lens candidate DLS432021848
    found in this work. A blue arc is clearly visible in the DLS image and shows typical
    lensing morphology in the high-resolution HST image. However, the arc is only
    marginally visible in shallower DECaLS imaging. Indeed, most (if not all) of the
    Grade-A lens candidates found from this work would be difficult to detect in shallower
    imaging surveys (e.g., DECaLS; hence for example they are not included in the
    catalog of Huang et al. [2020](#bib.bib27)).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f067c4629041fd17b96a99dc88b1ec9b.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Comparison of the image quality from different observations of the
    lens system DLS432021848, which shows a prominent blue arc in DLS imaging (below
    center of images; all panels show the same field of view). *Left*: The arc is
    apparent but not well detected in DECaLS imaging, which has modest sensitivity.
    This image would likely be flagged in a low-confidence category and indeed was
    not identified in previous lens searches (e.g., Huang et al., [2020](#bib.bib27)).
    *Middle*: DLS image of the target showing a prominent blue arc-like feature below
    the red deflector galaxy, characteristic of a gravitational lens system. The increased
    sensitivity of DLS compared to DECaLS imaging (Table [6](#S6.T6 "Table 6 ‣ 6.4
    Implications for future large-area sky surveys: sensitivity and angular resolution
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")) enables clear arc detection. *Right*:
    Near-infrared image of the same target observed with HST, with a diffraction-limited
    angular resolution approximately 6 times sharper than DLS or DECaLS images. The
    HST image reveals the lensed arc morphology at a high signal-to-noise ratio. This
    demonstrates the capabilities of a ground-based telescope at good depth (e.g.,
    DLS), and a diffraction-limited space-based telescope with moderate exposure time
    (e.g., HST).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '| Survey | Lenses found | 5$\sigma$ point | FWHM | References |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '|  | per sq.deg | source detection |  |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '|  |  | (r/R/F814W-band |  |  |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '|  |  | magnitude) |  |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| DES/DECaLS | $\sim 0.1$ | 23.6 (r) | 0$\aas@@fstack{\prime\prime}$98 | J19
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| DLS | 1 | 26.7 (R) | 0$\aas@@fstack{\prime\prime}$9 | This work |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| COSMOS | $\sim 35$ | 27.2 (F814W) | 0$\aas@@fstack{\prime\prime}$07 | P18,K07
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Number of lenses found using machine learning methods per square degree
    of sky in different surveys, along with the $5\sigma$ point source detection depth
    and median angular resolution (given as the FWHM: full-width at half maximum).
    We note that CNN and grading methods employed to find lenses in each survey are
    different; the density of lenses should thus be treated as an approximate comparison.
    References are as follows. J19: Jacobs et al. ([2019](#bib.bib31)), P18: Pourrahmani
    et al. ([2018](#bib.bib60)), K07: Koekemoer et al. ([2007](#bib.bib33)).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Given the detectability of many lens systems with upcoming surveys, it is clear
    that machine learning approaches (such as those we have explored here) will be
    vitally important for the efficient selection of large samples. We have also demonstrated
    the feasibility of spectroscopically following up on these moderately faint arc
    systems (Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Spectroscopic confirmation of two
    Grade-A lenses ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")), which will be vital for confirmation and subsequent
    analyses.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we have evaluated the performance of different CNN learning approaches
    and data augmentations on their ability to efficiently find gravitational lens
    candidates in the Deep Lens Survey. We make use of the deep learning architecture
    ResNet for our experiments, along with a training dataset consisting of simulated
    Lenses and survey image NonLenses. We demonstrate that by using these state-of-the-art
    semi-supervised learning approaches, we can greatly reduce the human effort required
    to find lensed candidates from a survey. We summarize our key results below.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Among 17 variants of learning approaches tested in this work, we find that our
    best performing models (i.e., those which have high precision and minimize false
    positives during human inspection) are GAN+MixMatch and GAN+$\Pi-$model. They
    have a precision of $\sim 86\%$ and $\sim 97\%$ at 50% recall and, $\sim 22\%$
    and $\sim 8\%$ at 100% recall respectively. In comparison, our supervised models
    have a precision of $\sim 3\%$ at 100% recall. This increase in the performance
    of the best models can be attributed largely to three factors. (1) They leverage
    data augmentation (Table [1](#S3.T1 "Table 1 ‣ 3.2 Domain adaptation with semi-supervised
    learning ‣ 3 Deep Learning Architecture and learning methods used ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")) during training, which helps them to generalize better. (2) The datasets
    used to train these models to contain simulated Lenses as well as GAN-generated
    images (Section [3](#S3 "3 Deep Learning Architecture and learning methods used
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")), which serves as an additional form of data augmentation.
    (3) Both of these top models employ a semi-supervised learning approach (MixMatch,
    $\Pi$-model) which enables our methods to adapt to distributional shift (Section [3.2](#S3.SS2
    "3.2 Domain adaptation with semi-supervised learning ‣ 3 Deep Learning Architecture
    and learning methods used ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). These results indicate that data
    augmentation, GANs, and semi-supervised learning are highly effective approaches
    for building an efficient lens classifier.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We investigated the Grad-CAM++ feature maps (Section [6.3](#S6.SS3 "6.3 Lensing
    signatures identified by the models ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey"))
    used by our best performing models to make their predictions, finding that they
    indeed are influenced mostly by lensed arc regions and are generally not misled
    by other galaxies/artifacts (e.g., diffraction spikes) in the images. This supplements
    our results presented above that salient information regarding the arcs needed
    for classification has been successfully learned by the models through our methods.
    This is encouraging for future lens searches, since simulated Lenses used in this
    work are generated without relying on photometric data of the deflector galaxy
    (Section [4](#S4 "4 Training and Validation data ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")), making
    it simpler to automate the task of generating a training dataset.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying the GAN+MixMatch and GAN+$\Pi$-model to the entire DLS survey, and
    visually inspecting the top $\sim 2500$ lens candidates, we find 9 Grade-A and
    13 Grade-B lensed candidates (22 in total). 3 out of the 9 Grade-A candidates
    are found within the top 17 ranked images. The number of lenses found in the DLS
    corresponds to $\sim 10\times$ higher sky density of lenses per deg² compared
    to the shallower DES/DECaLS survey imaging and supports predictions that vast
    numbers of lens systems ($\gtrsim 10^{5}$) will be detectable in the upcoming
    generation of sky surveys. We further confirmed the lensed nature of 2 Grade-A
    candidates with spectroscopy and high-resolution imaging, demonstrating that our
    methods are successful.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have generally explored methods intended to find as many lenses as possible
    while minimizing human inspection effort. While there are likely additional detectable
    lenses beyond those we have identified, it is encouraging that our models have
    been able to identify lenses that are not represented in the training set. In
    particular, our training set focused on blue lensed arcs, while our models also
    find red arc candidates such as DLS212072337 (Section [6.3.1](#S6.SS3.SSS1 "6.3.1
    Finding red arcs ‣ 6.3 Lensing signatures identified by the models ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")), although at a lower rank compared to the bluer
    lenses. Additional augmentation methods and/or training datasets may be able to
    provide further improvement for diverse lens system properties. Another straightforward
    improvement to our lens search efficiency is to include simple cuts in color-color
    space as demonstrated in Section [6.2.3](#S6.SS2.SSS3 "6.2.3 Distribution of lensed
    candidates in color-color space ‣ 6.2 Catalog of Lens candidates found ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey"). Such cuts can help increase the model precision
    by excluding sources that are not likely to act as strong lenses based on their
    color and magnitude (which is physically related to their mass and distance).
    Since our sample is agnostic to color information, our results are well-suited
    for assessing the color space distribution of the best lens candidates.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'The scope of our models is currently limited to the DLS. However, our methodology
    can be adapted for other data sets, and we note that the DLS fields overlap with
    wide-area surveys such as DECaLS and SDSS. Exploring ways to translate these models
    across surveys would be greatly beneficial. Finally, confirming the lensing nature
    of new candidates either through spectroscopy (Section [6.2.2](#S6.SS2.SSS2 "6.2.2
    Spectroscopic confirmation of two Grade-A lenses ‣ 6.2 Catalog of Lens candidates
    found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey")) or via arc morphology (Section [6.4](#S6.SS4
    "6.4 Implications for future large-area sky surveys: sensitivity and angular resolution
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")) is essential for a variety of
    investigations, including probes of galaxy evolution and cosmology. We have demonstrated
    the feasibility of confirming moderately faint arcs in our sample. Accomplishing
    confirmation for the thousands of lenses that will be discovered in forthcoming
    surveys (such as with Rubin/LSST, Roman, and Euclid) will aid in our understanding
    of the formation and evolution of galaxies and the contents of the Universe.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Imran Hasan, Sam Schmidt, David Wittman, Tony Tyson and Anupreeta More
    for their helpful discussions which greatly improved this work. We are immensely
    grateful to Brian Lemaux and Debora Pelliccia for manually labeling a subset of
    the survey data. We thank the referee for many helpful comments which improved
    the content and clarity of this manuscript. TJ and KVGC gratefully acknowledge
    financial support from NASA through grant HST-GO-16773, the Gordon and Betty Moore
    Foundation through Grant GBMF8549, the National Science Foundation through grant
    AST-2108515, and from a Dean’s Faculty Fellowship. Some of the data presented
    herein were obtained at the W. M. Keck Observatory, which is operated as a scientific
    partnership among the California Institute of Technology, the University of California
    and the National Aeronautics and Space Administration. The Observatory was made
    possible by the generous financial support of the W. M. Keck Foundation. The authors
    wish to recognize and acknowledge the very significant cultural role and reverence
    that the summit of Maunakea has always had within the indigenous Hawaiian community.
    We are most fortunate to have the opportunity to conduct observations from this
    mountain. Some of the results herein are based on observations with the NASA/ESA
    Hubble Space Telescope obtained from the Mikulski Archive for Space Telescopes
    at the Space Telescope Science Institute, which is operated by the Association
    of Universities for Research in Astronomy, Incorporated, under NASA contract NAS
    5-26555\. Support for program number HST-GO-16773 was provided through a grant
    from the STScI under NASA contract NAS5-26555.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Data availability
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data underlying this article are available on our GitHub repository ([https://github.com/sxsheng/SHLDN](https://github.com/sxsheng/SHLDN)).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adebayo et al. (2018) Adebayo J., Gilmer J., Muelly M., Goodfellow I., Hardt
    M., Kim B., 2018, in Proceedings of the 32nd International Conference on Neural
    Information Processing Systems. NIPS’18. Curran Associates Inc., Red Hook, NY,
    USA, p. 9525–9536
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alard (2006) Alard C., 2006, arXiv e-prints, [pp astro–ph/0606757](https://ui.adsabs.harvard.edu/abs/2006astro.ph..6757A)
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky et al. (2017) Arjovsky M., Chintala S., Bottou L., 2017, in Precup
    D., Teh Y. W., eds, Proceedings of Machine Learning Research Vol. 70, Proceedings
    of the 34th International Conference on Machine Learning. PMLR, International
    Convention Centre, Sydney, Australia, pp 214–223, [http://proceedings.mlr.press/v70/arjovsky17a.html](http://proceedings.mlr.press/v70/arjovsky17a.html)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ascaso et al. (2014) Ascaso B., Wittman D., Dawson W., 2014, [MNRAS](http://dx.doi.org/10.1093/mnras/stu074),
    [439, 1980](https://ui.adsabs.harvard.edu/abs/2014MNRAS.439.1980A)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belokurov et al. (2009) Belokurov V., Evans N. W., Hewett P. C., Moiseev A.,
    McMahon R. G., Sanchez S. F., King L. J., 2009, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2008.14075.x),
    [392, 104](https://ui.adsabs.harvard.edu/abs/2009MNRAS.392..104B)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berthelot et al. (2019) Berthelot D., Carlini N., Goodfellow I., Papernot N.,
    Oliver A., Raffel C. A., 2019, in Wallach H., Larochelle H., Beygelzimer A., d'Alché-Buc
    F., Fox E., Garnett R., eds, , Advances in Neural Information Processing Systems
    32. Curran Associates, Inc., pp 5049–5059
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berthelot et al. (2021) Berthelot D., Roelofs R., Sohn K., Carlini N., Kurakin
    A., 2021, arXiv preprint arXiv:2106.04732
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertin & Arnouts (1996) Bertin E., Arnouts S., 1996, [A&AS](http://dx.doi.org/10.1051/aas:1996164),
    [117, 393](https://ui.adsabs.harvard.edu/abs/1996A&AS..117..393B)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bolton et al. (2008) Bolton A. S., Burles S., Koopmans L. V. E., Treu T., Gavazzi
    R., Moustakas L. A., Wayth R., Schlegel D. J., 2008, [ApJ](http://dx.doi.org/10.1086/589327),
    [682, 964](https://ui.adsabs.harvard.edu/abs/2008ApJ...682..964B)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bradač et al. (2002) Bradač M., Schneider P., Steinmetz M., Lombardi M., King
    L. J., Porcas R., 2002, [A&A](http://dx.doi.org/10.1051/0004-6361:20020559), [388,
    373](https://ui.adsabs.harvard.edu/abs/2002A&A...388..373B)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cañameras et al. (2020) Cañameras R., et al., 2020, [A&A](http://dx.doi.org/10.1051/0004-6361/202038219),
    [644, A163](https://ui.adsabs.harvard.edu/abs/2020A&A...644A.163C)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chattopadhay et al. (2018) Chattopadhay A., Sarkar A., Howlader P., Balasubramanian
    V. N., 2018, in 2018 IEEE Winter Conference on Applications of Computer Vision
    (WACV). pp 839–847, [doi:10.1109/WACV.2018.00097](http://dx.doi.org/10.1109/WACV.2018.00097)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chiba (2002) Chiba M., 2002, [ApJ](http://dx.doi.org/10.1086/324493), [565,
    17](https://ui.adsabs.harvard.edu/abs/2002ApJ...565...17C)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collett (2015) Collett T. E., 2015, [ApJ](http://dx.doi.org/10.1088/0004-637X/811/1/20),
    [811, 20](https://ui.adsabs.harvard.edu/abs/2015ApJ...811...20C)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diehl et al. (2009) Diehl H. T., et al., 2009, [ApJ](http://dx.doi.org/10.1088/0004-637X/707/1/686),
    [707, 686](https://ui.adsabs.harvard.edu/abs/2009ApJ...707..686D)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erhan et al. (2010) Erhan D., Courville A., Bengio Y., Vincent P., 2010, in
    Teh Y. W., Titterington M., eds, Proceedings of Machine Learning Research Vol.
    9, Proceedings of the Thirteenth International Conference on Artificial Intelligence
    and Statistics. PMLR, Chia Laguna Resort, Sardinia, Italy, pp 201–208, [http://proceedings.mlr.press/v9/erhan10a.html](http://proceedings.mlr.press/v9/erhan10a.html)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fassnacht et al. (2004) Fassnacht C. D., Moustakas L. A., Casertano S., Ferguson
    H. C., Lucas R. A., Park Y., 2004, [ApJ](http://dx.doi.org/10.1086/379004), [600,
    L155](https://ui.adsabs.harvard.edu/abs/2004ApJ...600L.155F)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fernando et al. (2019) Fernando Z. T., Singh J., Anand A., 2019, in Proceedings
    of the 42nd International ACM SIGIR Conference on Research and Development in
    Information Retrieval. ACM, [doi:10.1145/3331184.3331312](http://dx.doi.org/10.1145/3331184.3331312),
    [https://doi.org/10.1145%2F3331184.3331312](https://doi.org/10.1145%2F3331184.3331312)
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garvin et al. (2022) Garvin E. O., Kruk S., Cornen C., Bhatawdekar R., Cañameras
    R., Merín B., 2022, arXiv e-prints, [p. arXiv:2207.06997](https://ui.adsabs.harvard.edu/abs/2022arXiv220706997G)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gavazzi et al. (2014) Gavazzi R., Marshall P. J., Treu T., Sonnenfeld A., 2014,
    [ApJ](http://dx.doi.org/10.1088/0004-637X/785/2/144), [785, 144](https://ui.adsabs.harvard.edu/abs/2014ApJ...785..144G)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gilman et al. (2019) Gilman D., Birrer S., Treu T., Nierenberg A., Benson A.,
    2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz1593), [487, 5721](https://ui.adsabs.harvard.edu/abs/2019MNRAS.487.5721G)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Goodfellow I. J., Pouget-Abadie J., Mirza M., Xu B.,
    Warde-Farley D., Ozair S., Courville A., Bengio Y., 2014, in Proceedings of the
    27th International Conference on Neural Information Processing Systems - Volume
    2. NIPS’14. MIT Press, Cambridge, MA, USA, p. 2672–2680
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2018) Gu J., et al., 2018, [Pattern Recognition](http://dx.doi.org/https://doi.org/10.1016/j.patcog.2017.10.013),
    77, 354
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulrajani et al. (2017) Gulrajani I., Ahmed F., Arjovsky M., Dumoulin V., Courville
    A. C., 2017, in Guyon I., Luxburg U. V., Bengio S., Wallach H., Fergus R., Vishwanathan
    S., Garnett R., eds,   Vol. 30, Advances in Neural Information Processing Systems.
    Curran Associates, Inc., [https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf)
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016a) He K., Zhang X., Ren S., Sun J., 2016a, in European conference
    on computer vision. pp 630–645
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016b) He K., Zhang X., Ren S., Sun J., 2016b, in Proceedings of
    the IEEE conference on computer vision and pattern recognition. pp 770–778
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2020) Huang X., et al., 2020, [The Astrophysical Journal](http://dx.doi.org/10.3847/1538-4357/ab7ffb),
    894, 78
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ioffe & Szegedy (2015) Ioffe S., Szegedy C., 2015, in International conference
    on machine learning. pp 448–456
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivezić et al. (2019) Ivezić Ž., et al., 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacobs et al. (2017) Jacobs C., Glazebrook K., Collett T., More A., McCarthy
    C., 2017, [MNRAS](http://dx.doi.org/10.1093/mnras/stx1492), [471, 167](https://ui.adsabs.harvard.edu/abs/2017MNRAS.471..167J)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacobs et al. (2019) Jacobs C., et al., 2019, [ApJS](http://dx.doi.org/10.3847/1538-4365/ab26b6),
    [243, 17](https://ui.adsabs.harvard.edu/abs/2019ApJS..243...17J)
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Welling (2014) Kingma D. P., Welling M., 2014, Auto-Encoding Variational
    Bayes ([arXiv:1312.6114](http://arxiv.org/abs/1312.6114))
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koekemoer et al. (2007) Koekemoer A. M., et al., 2007, [ApJS](http://dx.doi.org/10.1086/520086),
    [172, 196](https://ui.adsabs.harvard.edu/abs/2007ApJS..172..196K)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koopmans et al. (2006) Koopmans L. V. E., Treu T., Bolton A. S., Burles S.,
    Moustakas L. A., 2006, [ApJ](http://dx.doi.org/10.1086/505696), [649, 599](https://ui.adsabs.harvard.edu/abs/2006ApJ...649..599K)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kormann et al. (1994) Kormann R., Schneider P., Bartelmann M., 1994, A&A, [284,
    285](https://ui.adsabs.harvard.edu/abs/1994A&A...284..285K)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky (2009a) Krizhevsky A., 2009a, Technical report, Learning multiple
    layers of features from tiny images
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky (2009b) Krizhevsky A., 2009b.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Krizhevsky A., Sutskever I., Hinton G. E., 2012, in
    Pereira F., Burges C. J. C., Bottou L., Weinberger K. Q., eds,   Vol. 25, Advances
    in Neural Information Processing Systems. Curran Associates, Inc., [https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubo & Dell’Antonio (2008) Kubo J. M., Dell’Antonio I. P., 2008, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2008.12880.x),
    [385, 918](https://ui.adsabs.harvard.edu/abs/2008MNRAS.385..918K)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSST Science Collaboration et al. (2009) LSST Science Collaboration et al.,
    2009, arXiv e-prints, [p. arXiv:0912.0201](https://ui.adsabs.harvard.edu/abs/2009arXiv0912.0201L)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laine & Aila (2017) Laine S., Aila T., 2017, ArXiv, abs/1610.02242
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laureijs et al. (2011) Laureijs R., et al., 2011, arXiv e-prints, [p. arXiv:1110.3193](https://ui.adsabs.harvard.edu/abs/2011arXiv1110.3193L)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) LeCun Y., Boser B., Denker J. S., Henderson D., Howard R. E.,
    Hubbard W., Jackel L. D., 1989, [Neural Computation](http://dx.doi.org/10.1162/neco.1989.1.4.541),
    1, 541
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee (2013) Lee D.-H., 2013, ICML 2013 Workshop : Challenges in Representation
    Learning (WREPL)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leethochawalit et al. (2016) Leethochawalit N., Jones T. A., Ellis R. S., Stark
    D. P., Richard J., Zitrin A., Auger M., 2016, [ApJ](http://dx.doi.org/10.3847/0004-637X/820/2/84),
    [820, 84](https://ui.adsabs.harvard.edu/abs/2016ApJ...820...84L)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Li R., et al., 2020, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab9dfa),
    [899, 30](https://ui.adsabs.harvard.edu/abs/2020ApJ...899...30L)
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Litjens et al. (2017) Litjens G., et al., 2017, [Medical Image Analysis](http://dx.doi.org/https://doi.org/10.1016/j.media.2017.07.005),
    42, 60
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lupton et al. (2004) Lupton R., Blanton M. R., Fekete G., Hogg D. W., O’Mullane
    W., Szalay A., Wherry N., 2004, [PASP](http://dx.doi.org/10.1086/382245), [116,
    133](https://ui.adsabs.harvard.edu/abs/2004PASP..116..133L)
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madireddy et al. (2019) Madireddy S., Li N., Ramachandra N., Butler J., Balaprakash
    P., Habib S., Heitmann K., 2019, arXiv e-prints, [p. arXiv:1911.03867](https://ui.adsabs.harvard.edu/abs/2019arXiv191103867M)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marshall et al. (2015) Marshall P., Sandford C., More A., Buddelmeijerr H.,
    2015, HumVI: Human Viewable Image creation (ascl:1511.014)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miranda & Macciò (2007) Miranda M., Macciò A. V., 2007, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2007.12440.x),
    [382, 1225](https://ui.adsabs.harvard.edu/abs/2007MNRAS.382.1225M)
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miyato et al. (2019) Miyato T., Maeda S., Koyama M., Ishii S., 2019, IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 41, 1979
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More et al. (2016) More A., et al., 2016, [MNRAS](http://dx.doi.org/10.1093/mnras/stv1966),
    [455, 1191](https://ui.adsabs.harvard.edu/abs/2016MNRAS.455.1191M)
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moustakas et al. (2007) Moustakas L. A., et al., 2007, [ApJ](http://dx.doi.org/10.1086/517930),
    [660, L31](https://ui.adsabs.harvard.edu/abs/2007ApJ...660L..31M)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Netzer et al. (2011) Netzer Y., Wang T., Coates A., Bissacco A., Wu B., Ng A. Y.,
    2011, in NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011\.
    [http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf](http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oguri (2010) Oguri M., 2010, [PASJ](http://dx.doi.org/10.1093/pasj/62.4.1017),
    [62, 1017](https://ui.adsabs.harvard.edu/abs/2010PASJ...62.1017O)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oguri & Marshall (2010) Oguri M., Marshall P. J., 2010, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2010.16639.x),
    [405, 2579](https://ui.adsabs.harvard.edu/abs/2010MNRAS.405.2579O)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paraficz et al. (2016) Paraficz D., et al., 2016, [A&A](http://dx.doi.org/10.1051/0004-6361/201527971),
    [592, A75](https://ui.adsabs.harvard.edu/abs/2016A&A...592A..75P)
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pettini et al. (2002) Pettini M., Rix S. A., Steidel C. C., Adelberger K. L.,
    Hunt M. P., Shapley A. E., 2002, [ApJ](http://dx.doi.org/10.1086/339355), [569,
    742](https://ui.adsabs.harvard.edu/abs/2002ApJ...569..742P)
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pourrahmani et al. (2018) Pourrahmani M., Nayyeri H., Cooray A., 2018, [ApJ](http://dx.doi.org/10.3847/1538-4357/aaae6a),
    [856, 68](https://ui.adsabs.harvard.edu/abs/2018ApJ...856...68P)
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quinonero-Candela et al. (2008) Quinonero-Candela J., Sugiyama M., Schwaighofer
    A., Lawrence N. D., 2008, Dataset shift in machine learning. Mit Press
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidt & Thorman (2013) Schmidt S. J., Thorman P., 2013, [MNRAS](http://dx.doi.org/10.1093/mnras/stt373),
    [431, 2766](https://ui.adsabs.harvard.edu/abs/2013MNRAS.431.2766S)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seidel & Bartelmann (2007) Seidel G., Bartelmann M., 2007, [A&A](http://dx.doi.org/10.1051/0004-6361:20066097),
    [472, 341](https://ui.adsabs.harvard.edu/abs/2007A&A...472..341S)
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selvaraju et al. (2017) Selvaraju R. R., Cogswell M., Das A., Vedantam R., Parikh
    D., Batra D., 2017, in Proceedings of the IEEE International Conference on Computer
    Vision (ICCV).
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shajib et al. (2022) Shajib A. J., et al., 2022, arXiv e-prints, [p. arXiv:2210.10790](https://ui.adsabs.harvard.edu/abs/2022arXiv221010790S)
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheng et al. (2022) Sheng S., C K. V. G., Choi C. P., Sharpnack J., Jones T.,
    2022, [arXiv e-prints](http://dx.doi.org/10.48550/arXiv.2210.11681), [p. arXiv:2210.11681](https://ui.adsabs.harvard.edu/abs/2022arXiv221011681S)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn et al. (2020) Sohn K., et al., 2020, in Larochelle H., Ranzato M., Hadsell
    R., Balcan M., Lin H., eds,   Vol. 33, Advances in Neural Information Processing
    Systems. Curran Associates, Inc., pp 596–608, [https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf)
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sonnenfeld et al. (2013) Sonnenfeld A., Treu T., Gavazzi R., Suyu S. H., Marshall
    P. J., Auger M. W., Nipoti C., 2013, [ApJ](http://dx.doi.org/10.1088/0004-637X/777/2/98),
    [777, 98](https://ui.adsabs.harvard.edu/abs/2013ApJ...777...98S)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sonnenfeld et al. (2018) Sonnenfeld A., et al., 2018, [PASJ](http://dx.doi.org/10.1093/pasj/psx062),
    [70, S29](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..29S)
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spergel et al. (2015) Spergel D., et al., 2015, arXiv e-prints, [p. arXiv:1503.03757](https://ui.adsabs.harvard.edu/abs/2015arXiv150303757S)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Springenberg et al. (2015) Springenberg J. T., Dosovitskiy A., Brox T., Riedmiller
    M. A., 2015, CoRR, abs/1412.6806
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stein et al. (2021) Stein G., Harrington P., Blaum J., Medan T., Lukic Z., 2021,
    arXiv e-prints, [p. arXiv:2110.13151](https://ui.adsabs.harvard.edu/abs/2021arXiv211013151S)
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swinbank et al. (2009) Swinbank A. M., et al., 2009, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2009.15617.x),
    [400, 1121](https://ui.adsabs.harvard.edu/abs/2009MNRAS.400.1121S)
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tarvainen & Valpola (2017) Tarvainen A., Valpola H., 2017, in Guyon I., Luxburg
    U. V., Bengio S., Wallach H., Fergus R., Vishwanathan S., Garnett R., eds, , Advances
    in Neural Information Processing Systems 30. Curran Associates, Inc., pp 1195–1204
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. (2022) Tran K.-V. H., et al., 2022, arXiv e-prints, [p. arXiv:2205.05307](https://ui.adsabs.harvard.edu/abs/2022arXiv220505307T)
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Treu (2010) Treu T., 2010, [ARA&A](http://dx.doi.org/10.1146/annurev-astro-081309-130924),
    [48, 87](https://ui.adsabs.harvard.edu/abs/2010ARA&A..48...87T)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wilson et al. (2004) Wilson J. C., et al., 2004, in Moorwood A. F. M., Iye M.,
    eds, Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series
    Vol. 5492, Ground-based Instrumentation for Astronomy. pp 1295–1305, [doi:10.1117/12.550925](http://dx.doi.org/10.1117/12.550925)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wittman et al. (2002) Wittman D. M., et al., 2002, in Tyson J. A., Wolff S.,
    eds, Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series
    Vol. 4836, Survey and Other Telescope Technologies and Discoveries. pp 73–82 ([arXiv:astro-ph/0210118](http://arxiv.org/abs/astro-ph/0210118)),
    [doi:10.1117/12.457348](http://dx.doi.org/10.1117/12.457348)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wittman et al. (2003) Wittman D., Margoniner V. E., Tyson J. A., Cohen J. G.,
    Becker A. C., Dell’Antonio I. P., 2003, [ApJ](http://dx.doi.org/10.1086/378344),
    [597, 218](https://ui.adsabs.harvard.edu/abs/2003ApJ...597..218W)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wittman et al. (2006) Wittman D., Dell’Antonio I. P., Hughes J. P., Margoniner
    V. E., Tyson J. A., Cohen J. G., Norman D., 2006, [ApJ](http://dx.doi.org/10.1086/502621),
    [643, 128](https://ui.adsabs.harvard.edu/abs/2006ApJ...643..128W)
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wuyts et al. (2014) Wuyts E., Rigby J. R., Gladders M. D., Sharon K., 2014,
    [ApJ](http://dx.doi.org/10.1088/0004-637X/781/2/61), [781, 61](https://ui.adsabs.harvard.edu/abs/2014ApJ...781...61W)
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2016) Zhou B., Khosla A., A. L., Oliva A., Torralba A., 2016, CVPR
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Model performance and final lens sample
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this appendix, we provide some additional details of the model performance
    and the top lens candidates identified in this work.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Figure [12](#A1.F12 "Figure 12 ‣ Appendix A Model performance and final lens
    sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") shows the distribution of model scores across
    the different DLS fields (F1 to F5), demonstrating similar performance in each
    field. This is generally expected given the similar image quality across the DLS
    survey. Importantly it shows that our use of labeled training data from only F1
    does not substantially affect the model performance in the other fields.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Table [7](#A1.T7 "Table 7 ‣ Appendix A Model performance and final lens sample
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey") lists the results from our ablation study discussed
    in Section [6.1.1](#S6.SS1.SSS1 "6.1.1 Ablation study on data augmentations ‣
    6.1 Semi-supervised algorithms with GANs and Augmentations have superior performance
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey"). We show the precision across recall
    rates from 50-100%. The performance differences are generally similar across all
    recall rates. Color augmentation, JPEG quality, and GAN images appear to most
    prominently improving the model performance (i.e., the models perform significantly
    worse when these augmentations are removed).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'We show the top 25 predicted lens candidates from the GAN+$\Pi$-model and GAN+MixMatch
    models in Figures [13](#A1.F13 "Figure 13 ‣ Appendix A Model performance and final
    lens sample ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey") and [14](#A1.F14 "Figure 14 ‣ Appendix A Model
    performance and final lens sample ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), respectively. These include
    several of our top lens candidates based on human inspection (see Figures [6](#S6.F6
    "Figure 6 ‣ 6.1.1 Ablation study on data augmentations ‣ 6.1 Semi-supervised algorithms
    with GANs and Augmentations have superior performance ‣ 6 Results and Discussion
    ‣ Optimizing machine learning methods to discover strong gravitational lenses
    in the Deep Lens Survey")) and [7](#S6.F7 "Figure 7 ‣ 6.1.1 Ablation study on
    data augmentations ‣ 6.1 Semi-supervised algorithms with GANs and Augmentations
    have superior performance ‣ 6 Results and Discussion ‣ Optimizing machine learning
    methods to discover strong gravitational lenses in the Deep Lens Survey")), but
    many do not show obvious signs of strong lensing. There are several duplicate
    images at slightly different sky positions as discussed in the main text. In Figure [15](#A1.F15
    "Figure 15 ‣ Appendix A Model performance and final lens sample ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey")
    we include the GradCAM++ heatmaps obtained for all the Grade-A candidates (analogous
    to the example subsets shown in Figure [10](#S6.F10 "Figure 10 ‣ 6.2.3 Distribution
    of lensed candidates in color-color space ‣ 6.2 Catalog of Lens candidates found
    ‣ 6 Results and Discussion ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey")). These heatmaps were generated
    using our best performing models: GAN+MixMatch or GAN+$\Pi$-model (discussed in
    Section [6.3](#S6.SS3 "6.3 Lensing signatures identified by the models ‣ 6 Results
    and Discussion ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")). Finally, we list the sky coordinates of all
    Grade-A and Grade-B lenses in Table [8](#A1.T8 "Table 8 ‣ Appendix A Model performance
    and final lens sample ‣ Optimizing machine learning methods to discover strong
    gravitational lenses in the Deep Lens Survey").'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/d844e2b4ff9902248ebfe8d3e4a197b7.png) | ![Refer to
    caption](img/791cee5cba4f86510e6cb6085473d51b.png) | ![Refer to caption](img/13f743324dd7558609ee3a99a6312d28.png)
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/81841bf07799d297df79e08d750c4144.png) | ![Refer to
    caption](img/a623961ba45e97cb6506825169ddbae8.png) |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: 'Figure 12: Histogram of the scores obtained by the GAN+MixMatch and GAN+$\Pi$-model
    in the five independent DLS Fields F1 through F5\. As discussed in Section [4](#S4
    "4 Training and Validation data ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), the training set used to
    train our models (TrainingV2) contains human labeled NonLenses which were randomly
    sampled only from Field F1\. But as we clearly see, the distribution of scores
    (and performance of the models as a result) is independent of the field chosen.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '| Augmentation removed | TestV1 Precision(%) | TestV2 Precision(%) | TestV1
    baseline difference(%) | TestV2 baseline difference(%) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 50% recall rate |  |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| None | $84.95\pm 8.70$ | $80.19\pm 17.08$ | - | - |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| GAN | $65.46\pm 15.00$ | $55.77\pm 17.43$ | -19.49 | -24.42 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $44.8\pm 24.32$ | $35.45\pm 21.75$ | -40.15 | -44.74 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $65.30\pm 13.84$ | $56.84\pm 14.06$ | -19.65 | -23.35 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $91.81\pm 4.41$ | $89.96\pm 6.33$ | +6.86 | +9.77 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| Translations | $89.47\pm 10.34$ | $84.59\pm 13.04$ | +4.52 | +4.4 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $84.63\pm 10.85$ | $75.74\pm 13.96$ | -0.32 | -4.45 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $71.88\pm 17.35$ | $68.23\pm 15.2$ | -13.07 | -11.96
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 60% recall rate |  |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| None | $79.88\pm 6.30$ | $78.03\pm 12.45$ | - | - |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| GAN | $55.54\pm 13.13$ | $44.09\pm 12.94$ | - 24.34 | -33.94 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $34.25\pm 24.49$ | $26.14\pm 15.9$ | - 45.63 | -51.89 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $52.75\pm 24.68$ | $51.69\pm 17.91$ | - 27.13 | -26.34 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $84.94\pm 7.04$ | $78.99\pm 5.23$ | +5.06 | + 0.96 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| Translations | $74.95\pm 16.11$ | $72.26\pm 24.65$ | -4.93 | -5.77 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $71.15\pm 11.46$ | $60.64\pm 12.27$ | -8.73 | -17.39 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $63.24\pm 15.67$ | $50.91\pm 15.53$ | -16.64 | -27.12
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 70% recall rate |  |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '| None | $69.42\pm 4.60$ | $68.05\pm 12.96$ | - | - |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| GAN | $46.45\pm 14.49$ | $37.24\pm 12.85$ | -22.97 | -30.81 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $26.68\pm 17.28$ | $18.75\pm 11.78$ | -42.74 | -49.3 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $40.38\pm 18.64$ | $40.52\pm 21.74$ | - 29.04 | -27.53 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $79.97\pm 12.15$ | $73.69\pm 7.11$ | +10.55 | +5.64 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| Translations | $67.69\pm 12.67$ | $59.31\pm 22.34$ | -1.73 | -8.74 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $67.36\pm 11.95$ | $55.94\pm 14.71$ | -2.06 | -12.11 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $52.79\pm 14.92$ | $45.93\pm 13.97$ | -16.63 | -22.12
    |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 80% recall rate |  |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| None | $54.35\pm 4.57$ | $40.98\pm 17.12$ | - | - |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '| GAN | $33.02\pm 10.26$ | $24.37\pm 10.50$ | -21.33 | -16.61 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $15.34\pm 7.59$ | $11.75\pm 4.90$ | -39.01 | -29.23 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $21.33\pm 8.29$ | $15.25\pm 3.41$ | -33.02 | -25.73 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $62.63\pm 12.43$ | $52.2\pm 13.65$ | +8.28 | +11.22 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| Translations | $52.17\pm 12.92$ | $35.09\pm 16.64$ | -2.18 | -5.89 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $60.54\pm 9.78$ | $36.46\pm 9.03$ | +6.19 | -4.52 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $39.47\pm 13.24$ | $33.84\pm 10.92$ | -14.88 | -7.14
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 90% recall rate |  |  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| None | $34.12\pm 7.47$ | $16.33\pm 8.661$ | - | - |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| GAN | $22.60\pm 4.78$ | $10.99\pm 4.25$ | -11.52 | -5.34 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $10.37\pm 4.68$ | $5.88\pm 2.61$ | -23.75 | -10.45 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $15.15\pm 6.37$ | $6.18\pm 2.41$ | -18.97 | -10.15 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $40.72\pm 12.74$ | $21.79\pm 3.08$ | +6.6 | +5.46 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Translations | $32.88\pm 3.35$ | $16.14\pm 6.72$ | -1.24 | -0.19 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $30.81\pm 20.44$ | $14.72\pm 6.59$ | -3.31 | -1.61 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $23.86\pm 8.13$ | $14.25\pm 8.45$ | -10.26 | -2.08 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '|  |  | Performance at 100% recall rate |  |  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| None | $8.25\pm 2.85$ | $6.05\pm 2.69$ | - | - |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| GAN | $8.13\pm 2.49$ | $5.79\pm 3.93$ | -0.12 | -0.26 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| RGB shuffle | $6.43\pm 0.97$ | $3.84\pm 1.02$ | -1.82 | -2.21 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| JPEG quality | $5.88\pm 0.21$ | $4.14\pm 2.09$ | -2.37 | -1.91 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| Rot90 | $14.76\pm 8.42$ | $13.00\pm 5.16$ | +6.51 | +6.95 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| Translations | $10.83\pm 2.46$ | $7.37\pm 3.66$ | +2.58 | +1.32 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flips | $15.74\pm 3.06$ | $8.86\pm 1.84$ | +7.49 | +2.81 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| Color augmentation | $11.42\pm 7.25$ | $6.84\pm 4.12$ | +3.17 | +0.79 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Ablation performance for 50–100% recall rates (in steps of 10%) for
    the GAN+Supervised model using TrainingV2\. The first row of each recall rate
    shows the baseline precision value obtained from the model on the test sets (TestV1,
    TestV2) when none of the augmentations are removed. In the subsequent rows, we
    report the precision obtained when the model was trained without the specified
    augmentation. For example, the baseline model at 50% recall has a precision of
    80.19% for TestV2 and decreases to 55.77% when GAN images are removed during training.
    The difference in the obtained precision values are quoted in the last two columns.
    Augmentations which improve model performance (i.e., improve precision when included
    and decrease decrease precision when removed) are shown in red, while those which
    decrease model performance are shown in blue. Overall, the models perform worse
    when color augmentations, JPEG quality and GANs are not included, indicating that
    these augmentations are important for optimal performance. The errrors quoted
    here are 1$\sigma$.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/397c7f01d5a4158f1c06a41192cef2c1.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: DLS images of the top 25 predictions from GAN+$\Pi$-model. Several
    show clear evidence of strong lensing, while other images appear to be false positives.
    We note that many images are duplicates (at overlapping regions of the sky), which
    we remove before visual inspection.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/418c06a7b48a4dc0eb406031822094d8.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Equivalent to Figure [13](#A1.F13 "Figure 13 ‣ Appendix A Model
    performance and final lens sample ‣ Optimizing machine learning methods to discover
    strong gravitational lenses in the Deep Lens Survey"), showing the top 25 predictions
    from GAN+Mimatch.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '| object ID | RA | DEC | Field | Rank (GAN+MixMatch) | Rank (GAN+$\Pi$-model)
    |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Grade-A candidates |  |  |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| 421095124 | 163.792076 | -5.070373 | F4 | 2 | 12 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| 513097468 | 209.340092 | -10.244328 | F5 | 38 | 13 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| 212072337 | 139.896040 | 30.532355 | F2 | 181 | 21 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| 322054393 | 79.839914 | -48.949647 | F3 | 733 | 8326 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| 432021600 | 162.750073 | -5.941902 | F4 | 1262 | 12424 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| 431010921 | 163.364259 | -5.789092 | F4 | 1279 | 19826 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| 512037933 | 209.677055 | -10.687652 | F5 | 7461 | 2068 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| 421117552 | 163.897903 | -5.054885 | F4 | 4799 | 2768 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| 212148326 | 139.512033 | 30.953524 | F2 | 3579 | 23223 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Grade-B candidates |  |  |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| 313032462 | 78.742878 | -48.149829 | F3 | 365 | 59 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| 331108599 | 81.300608 | -49.432676 | F3 | 1974 | 98 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| 132023380 | 13.551513 | 11.794606 | F1 | 3400 | 462 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| 533097114 | 209.328083 | -11.993324 | F5 | 518 | 676 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| 433116975 | 162.551767 | -5.697394 | F4 | 13673 | 720 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| 233074254 | 139.046712 | 29.298535 | F2 | 870 | 2320 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| 413115231 | 162.585545 | -4.498548 | F4 | 8839 | 884 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| 211134050 | 140.304878 | 30.471131 | F2 | 12662 | 979 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| 122079323 | 13.182525 | 12.323637 | F1 | 8567 | 1145 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| 312158847 | 80.455801 | -48.489660 | F3 | 3896 | 1209 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| 322092794 | 80.115321 | -49.246309 | F3 | 1234 | 24945 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| 421019105 | 163.411890 | -4.870280 | F4 | 1996 | 2702 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| 221061603 | 140.662872 | 29.846367 | F2 | 3990 | 8584 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Grade-A and Grade-B Lens candidates found from this work with their
    object ID, RA and DEC coordinates, DLS field (F1 through F5), and their corresponding
    ranks from GAN+MixMatch and GAN+$\Pi$-models. The rank is obtained by passing
    all the survey images (281,425 objects in total; Section [2](#S2 "2 Deep Lens
    Survey Data ‣ Optimizing machine learning methods to discover strong gravitational
    lenses in the Deep Lens Survey")) through the models and sorting them based on
    their prediction scores. High-confidence Lens candidates have lower ranks and
    high prediction scores. For example, the Grade-A lens candidate DLS212072337 whose
    lensing nature has been spectroscopically confirmed (Section [6.2.2](#S6.SS2.SSS2
    "6.2.2 Spectroscopic confirmation of two Grade-A lenses ‣ 6.2 Catalog of Lens
    candidates found ‣ 6 Results and Discussion ‣ Optimizing machine learning methods
    to discover strong gravitational lenses in the Deep Lens Survey")) has a rank
    of 21 from the GAN+$\Pi$-model and a prediction score of $\simeq 1$. The ranks
    quoted here represent an upper bound on the number of images an investigator has
    to look at to find the lens candidate, as they do not account for duplicated sky
    regions which we remove before visual inspection (as discussed in Section [6.2](#S6.SS2
    "6.2 Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing
    machine learning methods to discover strong gravitational lenses in the Deep Lens
    Survey")), reducing the number of unique lens candidates investigated.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59c45db608f465dc3e009ae386f6c820.png)![Refer to caption](img/886bb3561092d12764f5ad5df8cfd783.png)![Refer
    to caption](img/c4be8039222e6f305850a4914fd293ec.png)![Refer to caption](img/675dbb1a76fa13445a76e754decd851c.png)![Refer
    to caption](img/ac5a5f2c6ffd15598e9118d0cf81a7b5.png)![Refer to caption](img/3f163efa26e3db96f3cabe4d0e08e935.png)![Refer
    to caption](img/bcbc9751b997d7959d39e45f83dc61f3.png)![Refer to caption](img/04ccfbb3a5d31706551cf3b507bd9f88.png)![Refer
    to caption](img/c5831d832ffc3d639861c190339a8dcb.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: GradCAM++ heatmaps for all Grade-A lenses, equivalent to Figure [10](#S6.F10
    "Figure 10 ‣ 6.2.3 Distribution of lensed candidates in color-color space ‣ 6.2
    Catalog of Lens candidates found ‣ 6 Results and Discussion ‣ Optimizing machine
    learning methods to discover strong gravitational lenses in the Deep Lens Survey").
    Each image is labeled with its object ID, and the model corresponding to the heatmaps
    (MM = GAN+MixMatch, PI = GAN+$\Pi$-model).'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
