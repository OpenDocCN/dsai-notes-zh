- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:04:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:04:04
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1911.12562] Towards Security Threats of Deep Learning Systems: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1911.12562] 深度学习系统的安全威胁：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1911.12562](https://ar5iv.labs.arxiv.org/html/1911.12562)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1911.12562](https://ar5iv.labs.arxiv.org/html/1911.12562)
- en: 'Towards Security Threats of Deep Learning Systems: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习系统的安全威胁：综述
- en: Yingzhe He^(1,2), Guozhu Meng^(1,2), Kai Chen^(1,2), Xingbo Hu^(1,2), Jinwen He^(1,2)
    ¹Institute of Information Engineering, Chinese Academy of Sciences, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yingzhe He^(1,2)，Guozhu Meng^(1,2)，Kai Chen^(1,2)，Xingbo Hu^(1,2)，Jinwen He^(1,2)
    ¹中国科学院信息工程研究所，中国
- en: ²School of Cybersecurity, University of Chinese Academy of Sciences
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²中国科学院网络安全学院
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep learning has gained tremendous success and great popularity in the past
    few years. However, deep learning systems are suffering several inherent weaknesses,
    which can threaten the security of learning models. Deep learning’s wide use further
    magnifies the impact and consequences. To this end, lots of research has been
    conducted with the purpose of exhaustively identifying intrinsic weaknesses and
    subsequently proposing feasible mitigation. Yet few are clear about how these
    weaknesses are incurred and how effective these attack approaches are in assaulting
    deep learning. In order to unveil the security weaknesses and aid in the development
    of a robust deep learning system, we undertake an investigation on attacks towards
    deep learning, and analyze these attacks to conclude some findings in multiple
    views. In particular, we focus on four types of attacks associated with security
    threats of deep learning: model extraction attack, model inversion attack, poisoning
    attack and adversarial attack. For each type of attack, we construct its essential
    workflow as well as adversary capabilities and attack goals. Pivot metrics are
    devised for comparing the attack approaches, by which we perform quantitative
    and qualitative analyses. From the analysis, we have identified significant and
    indispensable factors in an attack vector, e.g., how to reduce queries to target
    models, what distance should be used for measuring perturbation. We shed light
    on 18 findings covering these approaches’ merits and demerits, success probability,
    deployment complexity and prospects. Moreover, we discuss other potential security
    weaknesses and possible mitigation which can inspire relevant research in this
    area.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在过去几年取得了巨大的成功和广泛的普及。然而，深度学习系统存在一些固有的弱点，这些弱点可能威胁到学习模型的安全性。深度学习的广泛应用进一步放大了这些影响和后果。为此，已经进行大量研究，旨在全面识别内在弱点并提出可行的缓解措施。然而，对于这些弱点如何产生以及这些攻击方法在深度学习中有效性如何，仍然不够清晰。为了揭示安全弱点并帮助开发一个强健的深度学习系统，我们对深度学习的攻击进行了调查，并从多个角度分析了这些攻击，以得出一些结论。特别地，我们关注四种与深度学习安全威胁相关的攻击类型：模型提取攻击、模型反演攻击、投毒攻击和对抗攻击。对于每种攻击类型，我们构建了其基本工作流程以及对手能力和攻击目标。设计了关键指标用于比较攻击方法，通过这些指标我们进行了定量和定性分析。从分析中，我们识别了攻击向量中的重要且不可或缺的因素，例如，如何减少对目标模型的查询，应该使用什么距离来测量扰动。我们揭示了18个发现，涵盖了这些方法的优缺点、成功概率、部署复杂性和前景。此外，我们讨论了其他潜在的安全弱点和可能的缓解措施，这些可以激发相关领域的研究。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引术语：
- en: deep learning, poisoning attack, adversarial attack, model extraction attack,
    model inversion attack
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、投毒攻击、对抗攻击、模型提取攻击、模型反演攻击
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Deep learning has gained tremendous success and is the most significant driving
    force for artificial intelligence (AI). It fuels multiple areas including image
    classification, speech recognition, natural language processing, and malware detection.
    Due to the great advances in computing power and the dramatic increase in data
    volume, deep learning has exhibited superior potential in these scenarios, compared
    to traditional techniques. Deep learning excels in feature learning, deepening
    the understanding of one object, and unparalleled prediction ability. In image
    recognition, convolutional neural networks (CNNs) can classify different unknown
    images for us, and some even perform better than humans. In natural language processing,
    recurrent neural networks (RNNs) or long-short-term memory networks (LSTMs) can
    help us translate and summarize text information. Other fields including autonomous
    driving, speech recognition, and malware detection all have widespread application
    of deep learning. The Internet of things (IoT) and intelligent home systems have
    also arisen in recent years. As such, we are stepping into the era of intelligence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习取得了巨大的成功，是人工智能（AI）最重要的推动力。它推动了多个领域的发展，包括图像分类、语音识别、自然语言处理和恶意软件检测。由于计算能力的巨大进步和数据量的急剧增加，深度学习在这些场景中表现出了优于传统技术的潜力。深度学习在特征学习、加深对单一对象的理解以及无与伦比的预测能力方面表现突出。在图像识别中，卷积神经网络（CNNs）可以为我们分类不同的未知图像，有些甚至表现得比人类更好。在自然语言处理领域，递归神经网络（RNNs）或长短期记忆网络（LSTMs）可以帮助我们翻译和总结文本信息。其他领域，包括自动驾驶、语音识别和恶意软件检测，都广泛应用了深度学习。物联网（IoT）和智能家居系统也在近年来出现。因此，我们正在迈入智能时代。
- en: However, deep learning-based intelligent systems around us are suffering from
    a number of security problems. Machine learning models could be stolen through
    APIs [[220](#bib.bib220)]. Intelligent voice systems may execute unexpected commands [[262](#bib.bib262)].
    3D-printing objects could fool real-world image classifiers [[20](#bib.bib20)].
    Moreover, to ensure safety, technologies such as autonomous driving need lots
    of security testing before it can be widely used [[217](#bib.bib217)][[271](#bib.bib271)].
    In the past few years, the security of deep learning has drawn the attention of
    many relevant researchers and practitioners. They are exploring and studying the
    potential attacks as well as corresponding defense techniques against deep learning
    systems (DLS). Szegedy et al. [[213](#bib.bib213)] pioneered exploring the stability
    of neural networks, and uncovered their fragile properties in front of *imperceptible
    perturbations*. Since then, adversarial attacks have swiftly grown into a buzzing
    term in both artificial intelligence and security. Many efforts have been dedicated
    to disclosing the vulnerabilities in varying deep learning models (e.g., CNN [[175](#bib.bib175)][[156](#bib.bib156)][[155](#bib.bib155)],
    LSTM [[67](#bib.bib67)][[44](#bib.bib44)][[177](#bib.bib177)], reinforcement learning
    (RL) [[94](#bib.bib94)], generative adversarial network (GAN) [[114](#bib.bib114)][[190](#bib.bib190)]),
    and meanwhile testing the safety and robustness for DLS [[113](#bib.bib113)][[146](#bib.bib146)][[170](#bib.bib170)][[211](#bib.bib211)][[79](#bib.bib79)][[250](#bib.bib250)].
    On the other hand, the wide commercial deployment of DLS raises interest in proprietary
    asset protection such as the training data [[162](#bib.bib162)][[181](#bib.bib181)][[272](#bib.bib272)][[10](#bib.bib10)]
    and model parameters [[107](#bib.bib107)][[122](#bib.bib122)][[92](#bib.bib92)][[111](#bib.bib111)].
    It has started a war where privacy hunters exert corporate espionage to collect
    privacy from their rivals and the corresponding defenders conduct extensive measures
    to counteract the attacks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们周围基于深度学习的智能系统正遭受许多安全问题的困扰。机器学习模型可能通过API被窃取 [[220](#bib.bib220)]。智能语音系统可能执行意外的命令 [[262](#bib.bib262)]。3D打印物体可能欺骗现实世界的图像分类器 [[20](#bib.bib20)]。此外，为了确保安全，诸如自动驾驶这样的技术在广泛使用之前需要大量的安全测试 [[217](#bib.bib217)][[271](#bib.bib271)]。近年来，深度学习的安全性引起了许多相关研究人员和从业者的关注。他们正在探索和研究潜在的攻击以及针对深度学习系统（DLS）的相应防御技术。Szegedy等人 [[213](#bib.bib213)]率先探讨了神经网络的稳定性，揭示了它们在*不可察觉的扰动*面前的脆弱性。从那时起，对抗性攻击迅速成为人工智能和安全领域的热门术语。许多努力致力于揭示各种深度学习模型（例如，CNN [[175](#bib.bib175)][[156](#bib.bib156)][[155](#bib.bib155)]、LSTM [[67](#bib.bib67)][[44](#bib.bib44)][[177](#bib.bib177)]、强化学习（RL） [[94](#bib.bib94)]、生成对抗网络（GAN） [[114](#bib.bib114)][[190](#bib.bib190)]）中的漏洞，同时测试DLS的安全性和鲁棒性 [[113](#bib.bib113)][[146](#bib.bib146)][[170](#bib.bib170)][[211](#bib.bib211)][[79](#bib.bib79)][[250](#bib.bib250)]。另一方面，DLS的广泛商业部署引发了对专有资产保护的兴趣，例如训练数据 [[162](#bib.bib162)][[181](#bib.bib181)][[272](#bib.bib272)][[10](#bib.bib10)]和模型参数 [[107](#bib.bib107)][[122](#bib.bib122)][[92](#bib.bib92)][[111](#bib.bib111)]。这场战争已经开始，隐私猎手通过企业间谍活动从对手那里收集隐私，而相应的防御者则采取广泛的措施来对抗这些攻击。
- en: '![Refer to caption](img/cf7d9594b3bcdb5a39cffad841e92c82.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf7d9594b3bcdb5a39cffad841e92c82.png)'
- en: 'Figure 1: Publications we surveyed of four attacks and corresponding defenses
    in deep learning. The X-axis represents the year, and the Y-axis represents the
    corresponding number of publications for every year.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们调查了深度学习中四种攻击及其相应防御的出版物。X轴表示年份，Y轴表示每年的出版物数量。
- en: 'Prior works have been conducted to survey security and privacy issues in machine
    learning and deep learning [[13](#bib.bib13)][[26](#bib.bib26)][[175](#bib.bib175)][[22](#bib.bib22)].
    They enumerate and analyze attacks as well as defenses that are relevant to both
    the training phase and prediction phase. However, these works mainly evaluate
    the attacks either in limited domains (e.g., computer vision) or perspectives
    (e.g., adversarial attack). Few studies can provide a systematical evaluation
    of these attacks in their entire life cycles, which include the general workflow,
    adversary model, and comprehensive comparisons between different approaches. This
    knowledge can help demystify how these attacks happen, what capabilities the attackers
    possess, and both salient and tiny differences in attack effects. This motivates
    us to explore a variety of characteristics for the attacks against deep learning.
    In particular, we aim to dissect attacks in a stepwise manner (i.e., how the attacks
    are carried on progressively), identify the diverse capabilities of attackers,
    evaluate these attacks in terms of deliberate metrics, and distill insights for
    future research. This study is deemed to benefit the community threefold: 1) it
    presents a fine-grained description of attack vectors for defenders from which
    they can undertake cost-effective measures to enhance the security of the target
    model. 2) the evaluation on these attacks can unveil some significant properties
    such as success rate, capabilities. 3) the insights concluded from the survey
    can inspire researchers to explore new solutions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究已经对机器学习和深度学习中的安全性和隐私问题进行了调查[[13](#bib.bib13)][[26](#bib.bib26)][[175](#bib.bib175)][[22](#bib.bib22)]。这些研究列举并分析了与训练阶段和预测阶段相关的攻击和防御。然而，这些研究主要在有限的领域（例如计算机视觉）或视角（例如对抗攻击）中评估攻击。很少有研究能对这些攻击进行系统性的全生命周期评估，包括一般工作流程、对抗模型以及不同方法之间的全面比较。这些知识有助于揭示这些攻击的发生机制、攻击者的能力以及攻击效果中的显著和微小差异。这激励我们探索对深度学习攻击的各种特征。特别地，我们旨在逐步剖析攻击（即攻击如何逐步进行），识别攻击者的多样能力，从精确的指标评估这些攻击，并为未来的研究提炼见解。这项研究被认为对社区有三方面的好处：1）它为防御者提供了攻击向量的细致描述，防御者可以基于此采取具有成本效益的措施来增强目标模型的安全性。2）对这些攻击的评估可以揭示一些重要的属性，如成功率、能力。3）从调查中得出的见解可以激励研究人员探索新的解决方案。
- en: 'Our Approach. To gain a comprehensive understanding of privacy and security
    issues in deep learning, we conduct extensive investigations on the relevant literature
    and systems. In total, 245 publications have been studied which are mainly spanning
    across four prevailing areas–image classification, speech recognition, natural
    language processing and malware detection. Overall, we summarize these attacks
    into four classes: *model extraction attack*, *model inversion attack*, *data
    poisoning attack*, and *adversarial attack*. In particular, model extraction and
    inversion attacks are targeting privacy (cf. Section [4](#S4 "4 Model Extraction
    Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey"),[5](#S5
    "5 Model Inversion Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey")), and data poisoning and adversarial attacks can influence prediction
    results by either downgrading the formation of deep learning models or creating
    imperceptible perturbations that can deceive the model (cf. Section [6](#S6 "6
    Poisoning Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey"),[7](#S7
    "7 Adversarial Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey")).
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards Security Threats of Deep
    Learning Systems: A Survey") shows the publications we surveyed on these attacks
    in the past years. We collect papers from authoritative international venues,
    including artificial intelligence community, such as ICML, CVPR, NIPS, ICCV, ICLR,
    AAAI, IJCAI, ACL, and security community, such as IEEE S&P, CCS, USENIX Security,
    NDSS, TIFS, TDSC, Euro S&P, Asia CCS, RAID, and software engineering community,
    such as TSE, ASE, FSE, ICSE, ISSTA. We choose some keywords in the search process,
    including “security”, “attack”, “defense”, “privacy”, “adversarial”, “poison”,
    “inversion”, “inference”, “membership”, “backdoor”, “extract”, “steal”, “protect”,
    “detect”, and their variants. We also pay attention to the topics related to machine
    learning security in these venues. Furthermore, we also survey papers which cite
    or are cited by the foregoing papers, and include them if they have high citations.
    The number of related publications is experiencing a drastic increase in the past
    years. In our research, it gains 94% increase in 2017, 66% increase in 2018, and
    61% increase in 2019. Adversarial attack is obviously the most intriguing research
    and occupies around 47% of researchers’ attention based on the papers we collected.
    It is also worth mentioning that there is an ever-increasing interest in model
    inversion attack recently, which is largely credited to the laborious processing
    of training data (More discussions can be found in Section [8](#S8 "8 Discussion
    ‣ Towards Security Threats of Deep Learning Systems: A Survey")).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的方法。为了全面了解深度学习中的隐私和安全问题，我们对相关文献和系统进行了广泛的调查。我们共研究了245篇文献，这些文献主要涵盖了四个主要领域——图像分类、语音识别、自然语言处理和恶意软件检测。总体来说，我们将这些攻击总结为四类：*模型提取攻击*、*模型反演攻击*、*数据中毒攻击*和*对抗攻击*。特别地，模型提取和反演攻击主要针对隐私（参见第[4](#S4
    "4 Model Extraction Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey")、[5](#S5 "5 Model Inversion Attack ‣ Towards Security Threats of Deep
    Learning Systems: A Survey")节），数据中毒和对抗攻击则通过降低深度学习模型的构建质量或制造不可察觉的扰动来影响预测结果，从而欺骗模型（参见第[6](#S6
    "6 Poisoning Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey")、[7](#S7
    "7 Adversarial Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey")节）。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Towards Security Threats of Deep Learning Systems:
    A Survey")展示了我们对这些攻击进行调查的文献。我们从权威的国际会议中收集论文，包括人工智能领域的会议，如ICML、CVPR、NIPS、ICCV、ICLR、AAAI、IJCAI、ACL，以及安全领域的会议，如IEEE
    S&P、CCS、USENIX Security、NDSS、TIFS、TDSC、Euro S&P、Asia CCS、RAID，以及软件工程领域的会议，如TSE、ASE、FSE、ICSE、ISSTA。我们在搜索过程中选择了一些关键词，包括“security”、“attack”、“defense”、“privacy”、“adversarial”、“poison”、“inversion”、“inference”、“membership”、“backdoor”、“extract”、“steal”、“protect”、“detect”及其变体。我们还关注了这些会议上与机器学习安全相关的主题。此外，我们还调查了引用或被上述论文引用的论文，并在其引用次数较高时将其纳入研究。相关出版物的数量在过去几年中经历了急剧增加。在我们的研究中，2017年增长了94%，2018年增长了66%，2019年增长了61%。对抗攻击显然是最引人注目的研究，占据了我们收集的论文中约47%的研究者关注度。值得一提的是，最近对模型反演攻击的兴趣不断增加，这在很大程度上归功于对训练数据的艰苦处理（更多讨论可以在第[8](#S8
    "8 Discussion ‣ Towards Security Threats of Deep Learning Systems: A Survey")节中找到）。'
- en: In this study, we first introduce the background of deep learning, and summarize
    relevant risks and commercial DLS deployed in the cloud for public. For each type
    of attacks, we systematically study its capabilities, workflow and attack targets.
    More specifically, if one attacker is confronting a commercial deep learning system,
    what action it can perform in order to achieve the target, how the system is subverted
    step by step in the investigated approaches, and what influences the attack will
    make to both users and the system owner. In addition, we develop a number of metrics
    to evaluate these approaches such as *reducing query* strategies, *precision*
    of recovered training data, and *distance* with perturbed images. Based on a quantitative
    or qualitative analysis, we conclude many insights covering the popularity of
    specific attack techniques, merits and demerits of these approaches, future trends
    and so forth.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们首先介绍了深度学习的背景，并总结了相关风险以及在云端为公众部署的商业深度学习系统（DLS）。对于每种攻击类型，我们系统地研究了其能力、工作流程和攻击目标。更具体地说，当攻击者面对商业深度学习系统时，为了达到目标，它可以采取什么行动，系统如何在调查的方法中逐步被颠覆，以及攻击对用户和系统所有者的影响。此外，我们开发了一些指标来评估这些方法，例如*减少查询*策略、恢复训练数据的*精确度*以及与扰动图像的*距离*。基于定量或定性分析，我们总结了许多见解，涵盖了特定攻击技术的流行程度、这些方法的优缺点、未来趋势等。
- en: 'Takeaways. According to our investigation, we have drawn a number of insightful
    findings for future research. In black-box settings, attackers usually interact
    by querying certain inputs from the target DLS. How to reduce the number of queries
    for avoiding the security detection is a significant consideration for attackers
    (cf. Section [4](#S4 "4 Model Extraction Attack ‣ Towards Security Threats of
    Deep Learning Systems: A Survey")). The substitute model can be a prerequisite
    for attacks, because of its similar behavior and transferability. Model extraction,
    model inversion and adversarial attacks can all benefit from it (cf. Section [4](#S4
    "4 Model Extraction Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey")). Data synthesis is a common practice to represent similar training
    data. Either generated by the distribution or GAN, synthesized data can provide
    sufficient samples for training a substitute model (cf. Section [5](#S5 "5 Model
    Inversion Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey")).
    A more advanced way for poisoning purposes is to implant a backdoor in data and
    then attackers can manipulate the prediction results with crafted input (cf. Section [6](#S6
    "6 Poisoning Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey")).
    Most adversarial attacks have focused their main efforts on maximizing prediction
    errors but minimizing “distance”. However, “distance” can be measured in varying
    fashions and still need to be improved for better estimations and new applications
    (cf. Section [7](#S7 "7 Adversarial Attack ‣ Towards Security Threats of Deep
    Learning Systems: A Survey")). Moreover, we have discussed more security issues
    for modern DLS in Section [8](#S8 "8 Discussion ‣ Towards Security Threats of
    Deep Learning Systems: A Survey"), such as ethical considerations, system security,
    physical attacks and interpretability. We have investigated some works on deep
    learning defenses and summarized them in terms of attacks (cf. Section [8.6](#S8.SS6
    "8.6 Corresponding defense methods ‣ 8 Discussion ‣ Towards Security Threats of
    Deep Learning Systems: A Survey")).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '收获。根据我们的调查，我们对未来研究得出了若干有洞察力的发现。在黑箱设置中，攻击者通常通过查询目标深度学习系统的某些输入进行交互。如何减少查询次数以避免安全检测是攻击者的重要考虑因素（参见第[4节](#S4
    "4 Model Extraction Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey")）。替代模型可能是攻击的前提，因为它具有类似的行为和可迁移性。模型提取、模型反演和对抗攻击都可以从中受益（参见第[4节](#S4 "4
    Model Extraction Attack ‣ Towards Security Threats of Deep Learning Systems: A
    Survey")）。数据合成是一种常见的实践，用于表示类似的训练数据。无论是由分布还是生成对抗网络（GAN）生成，合成数据都可以为训练替代模型提供足够的样本（参见第[5节](#S5
    "5 Model Inversion Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey")）。一种更先进的毒化方式是将后门植入数据中，然后攻击者可以通过精心设计的输入操控预测结果（参见第[6节](#S6 "6 Poisoning
    Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey")）。大多数对抗攻击主要集中在最大化预测错误而最小化“距离”上。然而，“距离”可以以不同的方式度量，仍需改进以获得更好的估计和新应用（参见第[7节](#S7
    "7 Adversarial Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey")）。此外，我们在第[8节](#S8
    "8 Discussion ‣ Towards Security Threats of Deep Learning Systems: A Survey")讨论了现代深度学习系统的更多安全问题，如伦理考虑、系统安全、物理攻击和可解释性。我们调查了一些关于深度学习防御的工作，并根据攻击进行了总结（参见第[8.6节](#S8.SS6
    "8.6 Corresponding defense methods ‣ 8 Discussion ‣ Towards Security Threats of
    Deep Learning Systems: A Survey")）。'
- en: '![Refer to caption](img/f85b7f15a108d2281ba2c3afff8d9774.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f85b7f15a108d2281ba2c3afff8d9774.png)'
- en: 'Figure 2: Deep learning systems and the encountered attacks'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：深度学习系统和遇到的攻击
- en: Contributions. We make the following contributions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献。我们做出了以下贡献。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Systematic security analysis of deep learning. We summarize 4 types of attacks.
    For each attack, we construct their attack vectors and pivot properties, i.e.,
    workflow, adversary model (it contains attacker’s capabilities and limitations),
    and attack goal. This could ease the understanding of how these attacks are executed
    and facilitate the development of counter measures.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度学习的系统安全分析。我们总结了4种攻击类型。对于每种攻击，我们构建了其攻击向量和关键属性，即工作流程、对手模型（它包含攻击者的能力和限制）以及攻击目标。这有助于理解这些攻击的执行方式，并促进防御措施的发展。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Quantitative and qualitative analysis. We develop a number of metrics that are
    pertinent to each type of attacks, for a better assessment of different approaches.
    These metrics also serve as highlights in the development of attack approaches
    that facilitate more robust attacks.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定量和定性分析。我们开发了多个与每种攻击类型相关的指标，以更好地评估不同的方法。这些指标还作为攻击方法发展的亮点，促进了更强大的攻击。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'New findings. Based on the analysis, we have concluded 18 findings that span
    the four attacks, and uncover implicit properties for these attack methods. Our
    findings summarize some results and analyze phenomena based on existing surveyed
    work, and predict the possible future direction of the field based on the summary
    results. All findings include quantitative or qualitative analysis. Beyond these
    attacks, we have discussed other related security problems in Section [8](#S8
    "8 Discussion ‣ Towards Security Threats of Deep Learning Systems: A Survey")
    such as secure implementation, interpretability, discrimination and defense techniques.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '新发现。基于分析，我们得出了跨越四种攻击的18个发现，并揭示了这些攻击方法的隐含特性。我们的发现总结了一些结果并分析了现有调查工作的现象，并根据总结结果预测了该领域可能的未来方向。所有发现均包括定量或定性分析。除了这些攻击之外，我们还在第[8](#S8
    "8 Discussion ‣ Towards Security Threats of Deep Learning Systems: A Survey")节讨论了其他相关的安全问题，如安全实现、可解释性、歧视和防御技术。'
- en: 2 Related Work
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: There is a line of works that survey and evaluate attacks toward machine learning
    or deep learning.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有一系列的工作调查和评估了针对机器学习或深度学习的攻击。
- en: Barreno et al. conduct a survey of machine learning security and present a taxonomy
    of attacks against machine learning systems [[26](#bib.bib26)]. They experiment
    on a popular statistical spam filter to illustrate their effectiveness. Attacks
    are dissected in terms of three dimensions, including workable manners, influence
    to input and generality. Amodei et al. [[17](#bib.bib17)] introduce five possible
    research problems related to accident risk and discuss probable approaches, with
    an example of how a cleaning robot works. Papernot et al. [[176](#bib.bib176)]
    study the security and privacy of machine learning. They summarize some attack
    and defense methods, and propose a threat model for machine learning. It introduces
    attack methods in training and inferring process, black-box and white-box model.
    However, they do not include much information about defenses or the most widely
    used deep learning models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Barreno 等人对机器学习安全进行了一项调查，并提出了针对机器学习系统攻击的分类法 [[26](#bib.bib26)]。他们在一个流行的统计垃圾邮件过滤器上进行了实验，以说明其有效性。攻击从三个维度进行剖析，包括可行方式、对输入的影响和普遍性。Amodei
    等人 [[17](#bib.bib17)] 提出了五个与事故风险相关的可能研究问题，并讨论了可能的方法，以清洁机器人如何工作的例子进行说明。Papernot
    等人 [[176](#bib.bib176)] 研究了机器学习的安全性和隐私。他们总结了一些攻击和防御方法，并提出了一个机器学习威胁模型。它介绍了训练和推断过程中的攻击方法、黑箱和白箱模型。然而，他们没有包括关于防御或最广泛使用的深度学习模型的详细信息。
- en: Bae et al. [[22](#bib.bib22)] review the attack and defense methods under security
    and privacy AI concept. They inspect evasion and poisoning attacks, in black-box
    and white-box. In addition, their study focuses on privacy with no mention of
    other attack types.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Bae 等人 [[22](#bib.bib22)] 回顾了在安全和隐私 AI 概念下的攻击和防御方法。他们检查了黑箱和白箱中的规避和中毒攻击。此外，他们的研究专注于隐私，未提及其他攻击类型。
- en: Liu et al. [[138](#bib.bib138)] aim to provide a literature review in two phases
    of machine learning, i.e., the training phase and the testing/inferring phase.
    As for the corresponding defenses, they sum up with four categories. In addition,
    this survey focuses more on data distribution drifting caused by adversarial samples
    and sensitive information violation problems in statistical machine learning algorithms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Liu 等人 [[138](#bib.bib138)] 旨在提供机器学习两个阶段的文献综述，即训练阶段和测试/推断阶段。至于相应的防御，他们总结了四个类别。此外，这项调查更多地关注由对抗样本引起的数据分布漂移和统计机器学习算法中的敏感信息泄露问题。
- en: Akhtar et al. [[13](#bib.bib13)] conduct a study on adversarial attacks of deep
    learning in computer vision. They summarize 12 attack methods for classification,
    and study attacks on models or algorithms such as autoencoders, generative models,
    RNNs and so on. They also study attacks in the real world and summarize defenses.
    However, they only research the computer vision part of adversarial attack.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Akhtar 等人 [[13](#bib.bib13)] 进行了一项关于计算机视觉中深度学习对抗攻击的研究。他们总结了12种分类攻击方法，并研究了针对自动编码器、生成模型、RNN等模型或算法的攻击。他们还研究了现实世界中的攻击并总结了防御措施。然而，他们仅研究了对抗攻击的计算机视觉部分。
- en: Huang et al. [[96](#bib.bib96)] research the safety and trustworthiness on the
    deployment of DNNs. They address the trustworthiness within a certification process
    and an explanation process. In certification, they study DNN verification and
    testing techniques, and in explanation, they consider DNN interpretability problems.
    Adversarial attack and defense techniques go through the whole procedure. Different
    from us, their security considerations pay more attention to ensure trustworthiness
    during the DNN deployment process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 黄等人 [[96](#bib.bib96)] 研究了 DNN 部署的安全性和可靠性。他们在认证过程中和解释过程中处理信任问题。在认证中，他们研究 DNN
    验证和测试技术，而在解释中，他们考虑 DNN 可解释性问题。对抗攻击和防御技术贯穿整个过程。与我们不同，他们的安全考虑更加关注确保在 DNN 部署过程中的可信度。
- en: Zhang et al. [[270](#bib.bib270)] summarize and analyze machine learning testing
    techniques. Testing can expose problems and improve the trustworthiness of machine
    learning systems. Their survey covers testing properties (such as correctness,
    robustness, fairness), testing components (such as data, learning program, framework),
    testing workflow (such as test generation, test evaluation), and application scenarios
    (such as autonomous driving, machine translation). Unlike us, their focus on safety
    is from a testing perspective.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人 [[270](#bib.bib270)] 总结和分析了机器学习测试技术。测试可以暴露问题并提高机器学习系统的可靠性。他们的调查涵盖了测试属性（如正确性、稳健性、公平性）、测试组件（如数据、学习程序、框架）、测试工作流（如测试生成、测试评估）和应用场景（如自动驾驶、机器翻译）。与我们不同，他们从测试角度关注安全性。
- en: 3 Overview
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 概述
- en: 'TABLE I: Notations used in this paper'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：本文中使用的符号
- en: '| Notation | Explanation |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 说明 |'
- en: '| --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $D$ | dataset |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| $D$ | 数据集 |'
- en: '| $x=\{x^{1},\ldots,x^{n}\}$ | inputs in $D$ |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| $x=\{x^{1},\ldots,x^{n}\}$ | $D$ 中的输入 |'
- en: '| $y=\{y^{1},\ldots,y^{n}\}$ | predicted labels of $x$ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| $y=\{y^{1},\ldots,y^{n}\}$ | $x$ 的预测标签 |'
- en: '| $y_{t}=\{y^{1}_{t},\ldots,y^{n}_{t}\}$ | true labels of $x$ |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| $y_{t}=\{y^{1}_{t},\ldots,y^{n}_{t}\}$ | $x$ 的真实标签 |'
- en: '| $&#124;&#124;x-y&#124;&#124;^{2}$ | the $Euclidean$ distance for $x$ and
    $y$ |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| $&#124;&#124;x-y&#124;&#124;^{2}$ | $x$ 和 $y$ 之间的 $Euclidean$ 距离 |'
- en: '| $F$ | model function |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| $F$ | 模型函数 |'
- en: '| $Z$ | output of second-to-last layer |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| $Z$ | 倒数第二层的输出 |'
- en: '| $\mathcal{L}$ | loss function |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}$ | 损失函数 |'
- en: '| $w$ | weights of parameters |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| $w$ | 参数的权重 |'
- en: '| $b$ | bias of parameters |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| $b$ | 参数的偏差 |'
- en: '| $\lambda$ | hyperparameters |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda$ | 超参数 |'
- en: '| $L_{p}$ | distance measurement |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| $L_{p}$ | 距离测量 |'
- en: '| $\delta$ | perturbation to input $x$ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| $\delta$ | 输入 $x$ 的扰动 |'
- en: 3.1 Deep Learning System
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 深度学习系统
- en: 'Deep learning is inspired by biological nervous systems and is composed of
    thousands of neurons to transfer information. Figure [2](#S1.F2 "Figure 2 ‣ 1
    Introduction ‣ Towards Security Threats of Deep Learning Systems: A Survey") demonstrates
    a classic deep learning process. Typically, it exhibits to the public an overall
    process including: 1) *Model Training*, where it converts a large volume of data
    into a trained model, and 2) *Model Prediction*, where the model can be used for
    prediction as per input data. Prediction tasks are widely used in different fields.
    For instance, image classification, speech recognition, natural language processing
    and malware detection are all pertinent applications for deep learning.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习受到生物神经系统的启发，由成千上万的神经元组成，用于传递信息。图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Towards Security Threats of Deep Learning Systems: A Survey") 展示了一个经典的深度学习过程。通常，它向公众展示了一个包括以下内容的整体过程：1)
    *模型训练*，它将大量数据转换为训练模型，以及 2) *模型预测*，模型可以根据输入数据进行预测。预测任务在不同领域广泛应用。例如，图像分类、语音识别、自然语言处理和恶意软件检测都是深度学习相关的应用。'
- en: 'TABLE II: Commercial MLaaS systems and the provided functionalities, output
    for clients and charges per 1M queries'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：商业 MLaaS 系统及其提供的功能、客户输出和每百万次查询的费用
- en: '| System | Functionality | Output | Cost/M-times |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 功能 | 输出 | 每百万次查询费用 |'
- en: '| Alibaba Image Recognition | Image marking | label, confidence | 2500 CNY
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 阿里巴巴图像识别 | 图像标记 | 标签，自信度 | 2500 元人民币 |'
- en: '| scene recognition | label, confidence | 1500 CNY |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 场景识别 | 标签，自信度 | 1500 元人民币 |'
- en: '| porn identification | label, suggestion | 1620 CNY |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 色情识别 | 标签，建议 | 1620 元人民币 |'
- en: '| Amazon Image Recognition | Object & Scene Recognition | label, boundingbox,
    confidence | 1300 USD |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 亚马逊图像识别 | 对象与场景识别 | 标签，边界框，自信度 | 1300 美元 |'
- en: '| face recognition | AgeRange, boundingbox, emotions, eyeglasses, gender, pose,
    etc | 1300 USD |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 人脸识别 | 年龄范围、边界框、情感、眼镜、性别、姿势等 | 1300 美元 |'
- en: '| Google Vision API | label description | description, score | 1500 USD |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Google Vision API | 标签描述 | 描述，分数 | 1500 美元 |'
- en: 'To formalize the process of deep learning systems, we present some notations
    in Table [I](#S3.T1 "TABLE I ‣ 3 Overview ‣ Towards Security Threats of Deep Learning
    Systems: A Survey"). Given a learning task, the training data can be represented
    as $(x,y_{t})\in D$. Let $F$ be the deep learning model and it computes the corresponding
    outcomes $y$ based on the given input $x$, i.e., $y=F(x)$. $y_{t}$ is the true
    label of input $x$. Within the course of model training, there is a loss function
    $\mathcal{L}$ to measure the prediction error between predicted result and true
    label, and the training process intends to gain a minimal error value via fine-tuning
    parameters. There exist many loss functions to measure the differences. One commonly
    used loss function can be computed as $\mathcal{L}\,=\,\Sigma_{1\leqslant i\leqslant
    n}||y_{t}^{i}-y^{i}||^{2}$. So the process of model training can be formalized
    as [[183](#bib.bib183)]:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '为了规范化深度学习系统的过程，我们在表 [I](#S3.T1 "TABLE I ‣ 3 Overview ‣ Towards Security Threats
    of Deep Learning Systems: A Survey")中介绍了一些符号。给定一个学习任务，训练数据可以表示为 $(x,y_{t})\in
    D$。令 $F$ 为深度学习模型，它根据给定输入 $x$ 计算相应的结果 $y$，即 $y=F(x)$。$y_{t}$ 是输入 $x$ 的真实标签。在模型训练过程中，有一个损失函数
    $\mathcal{L}$ 用于衡量预测结果与真实标签之间的预测误差，训练过程旨在通过调整参数来获得最小误差值。存在许多损失函数用于衡量差异。一个常用的损失函数可以计算为
    $\mathcal{L}\,=\,\Sigma_{1\leqslant i\leqslant n}||y_{t}^{i}-y^{i}||^{2}$。因此，模型训练的过程可以形式化为 [[183](#bib.bib183)]：'
- en: '|  | $\begin{split}\operatorname*{arg\,min}_{F}\sum_{1\leqslant i\leqslant
    n}&#124;&#124;y_{t}^{i}-y^{i}&#124;&#124;^{2}\end{split}$ |  | (1) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\operatorname*{arg\,min}_{F}\sum_{1\leqslant i\leqslant
    n}&#124;&#124;y_{t}^{i}-y^{i}&#124;&#124;^{2}\end{split}$ |  | (1) |'
- en: 3.2 Risks in Deep Learning
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 深度学习中的风险
- en: 'One deep learning system involves several pivotal assets that are confidential
    and significant for the owner. As per the phases in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Towards Security Threats of Deep Learning Systems: A Survey"),
    risks stem from three types of concerned assets in deep learning systems: 1) training
    dataset. 2) trained model including model structures, and model parameters. 3)
    inputs and results of predictions.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '一个深度学习系统涉及几个关键资产，这些资产对所有者来说是机密且重要的。根据图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Towards Security Threats of Deep Learning Systems: A Survey")中的各个阶段，风险源于深度学习系统中三类相关资产：1)
    训练数据集。2) 训练模型，包括模型结构和模型参数。3) 输入和预测结果。'
- en: <svg   height="15.26" overflow="visible" version="1.1" width="15.26"><g transform="translate(0,15.26)
    matrix(1 0 0 -1 0 0) translate(7.63,0) translate(0,7.63)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -2.77 -3.57)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="15.26" overflow="visible" version="1.1" width="15.26"><g transform="translate(0,15.26)
    matrix(1 0 0 -1 0 0) translate(7.63,0) translate(0,7.63)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -2.77 -3.57)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
- en: 'Training dataset. High-quality training data is significant and vital for a
    better performance of the deep learning model. As a deep learning system has to
    absorb plenty of data to form a qualified model, mislabelled or inferior data
    can hinder this formation and affect the model’s quality. These kinds of data
    can be intentionally appended to the benign data by attackers, which is referred
    to as *poisoning attack* (cf. Section [6](#S6 "6 Poisoning Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey")). On the other hand, the collection
    of training data takes lots of human resources and time costs. Industry giants
    such as Google have far more data than other companies. They are more inclined
    to share their state-of-the-art algorithms [[106](#bib.bib106)][[55](#bib.bib55)],
    but they barely share data. Therefore, training data is crucial and considerably
    valuable for a company, and its leakage means big loss of assets. However, recent
    research found there is an inverse flow from prediction results to training data [[221](#bib.bib221)].
    It leads that one attacker can infer out the confidential information in training
    data, merely relying on authorized access to the victim system. It is literally
    noted as *model inversion attack* whose goal is to uncover the composition of
    the training data or its specific properties (cf. Section [5](#S5 "5 Model Inversion
    Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey")).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '训练数据集。高质量的训练数据对于深度学习模型的更好表现至关重要。由于深度学习系统需要吸收大量数据以形成合格的模型，错误标注或劣质数据会阻碍这一过程，影响模型的质量。这类数据可能被攻击者故意添加到良性数据中，这被称为*中毒攻击*（参见第[6](#S6
    "6 Poisoning Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey)节）。另一方面，收集训练数据需要大量的人力和时间成本。像Google这样的行业巨头拥有的数据远多于其他公司，他们更倾向于分享他们的最先进算法[[106](#bib.bib106)][[55](#bib.bib55)]，但几乎不分享数据。因此，训练数据对公司至关重要且极具价值，其泄露意味着资产的巨大损失。然而，最近的研究发现，预测结果到训练数据的反向流动[[221](#bib.bib221)]，这意味着攻击者可以仅凭授权访问受害者系统，推断出训练数据中的机密信息。这被称为*模型反演攻击*，其目标是揭示训练数据的组成或其具体属性（参见第[5](#S5
    "5 Model Inversion Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey)节）。'
- en: <svg   height="15.26" overflow="visible" version="1.1" width="15.26"><g transform="translate(0,15.26)
    matrix(1 0 0 -1 0 0) translate(7.63,0) translate(0,7.63)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -2.77 -3.57)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="15.26" overflow="visible" version="1.1" width="15.26"><g transform="translate(0,15.26)
    matrix(1 0 0 -1 0 0) translate(7.63,0) translate(0,7.63)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -2.77 -3.57)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
- en: 'Trained model. The trained model is an abstract representation of its training
    data. Modern deep learning systems have to cope with a large volume of data in
    the training phase, which has a rigorous demand for high performance computing
    and mass storage. Therefore, the trained model is regarded as the core competitiveness
    for a deep learning system, endowed with commercial value and creative achievements.
    Once it is cloned, leaked or extracted, the interests of model owners will be
    seriously damaged. More specifically, attackers have started to steal model parameters [[220](#bib.bib220)],
    functionality [[167](#bib.bib167)] or decision boundaries [[174](#bib.bib174)],
    which are collectively known as *model extraction attack* (cf. Section [4](#S4
    "4 Model Extraction Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey")).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '训练模型。训练模型是其训练数据的抽象表示。现代深度学习系统在训练阶段必须处理大量数据，这对高性能计算和大规模存储有严格要求。因此，训练模型被视为深度学习系统的核心竞争力，具有商业价值和创造性成果。一旦它被克隆、泄露或提取，模型所有者的利益将受到严重损害。更具体地说，攻击者已开始窃取模型参数[[220](#bib.bib220)]、功能[[167](#bib.bib167)]或决策边界[[174](#bib.bib174)]，这些被统称为*模型提取攻击*（参见第[4](#S4
    "4 Model Extraction Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey)节）。'
- en: <svg   height="15.26" overflow="visible" version="1.1" width="15.26"><g transform="translate(0,15.26)
    matrix(1 0 0 -1 0 0) translate(7.63,0) translate(0,7.63)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -2.77 -3.57)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="15.26" overflow="visible" version="1.1" width="15.26"><g transform="translate(0,15.26)
    matrix(1 0 0 -1 0 0) translate(7.63,0) translate(0,7.63)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -2.77 -3.57)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
- en: 'Inputs and results of predictions. As for prediction data and results, curious
    service providers may retain user’s prediction data and results to extract sensitive
    information. These data may also be attacked by miscreants who intend to utilize
    these data to make their own profits. On the other hand, attackers may submit
    carefully modified input to fool models, which is dubbed *adversarial example* [[213](#bib.bib213)].
    An adversarial example is crafted by inserting slight perturbations into the original
    normal sample which are not easy to perceive. This is recognized as *adversarial
    attack* or *evasion attack* (cf. Section [7](#S7 "7 Adversarial Attack ‣ Towards
    Security Threats of Deep Learning Systems: A Survey")).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '输入和预测结果。对于预测数据和结果，某些服务提供商可能会保留用户的预测数据和结果，以提取敏感信息。这些数据也可能被恶意分子攻击，意图利用这些数据谋取自身利益。另一方面，攻击者可能会提交经过精心修改的输入以欺骗模型，这被称为*对抗样本*
    [[213](#bib.bib213)]。对抗样本是通过在原始正常样本中插入轻微的扰动来制作的，这些扰动不容易察觉。这被称为*对抗攻击*或*规避攻击*（参见第[7](#S7
    "7 Adversarial Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey)节"）。'
- en: 3.3 Commercial Off-The-Shelf
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 商业现货产品
- en: 'Machine learning as a Service (MLaaS) has gained momentum in recent years [[130](#bib.bib130)],
    and lets its clients benefit from machine learning without establishing their
    own predictive models. To ease the usage, the MLaaS suppliers make a number of
    APIs for clients to accomplish machine learning tasks, e.g., classifying an image,
    recognizing a slice of audio or identifying the intent of a passage. Certainly,
    these services are the core competence which also charge clients for their queries.
    Table [II](#S3.T2 "TABLE II ‣ 3.1 Deep Learning System ‣ 3 Overview ‣ Towards
    Security Threats of Deep Learning Systems: A Survey") shows representative COTS
    as well as their functionalities, outputs to the clients, and usage charges. Taking
    Amazon Image Recognition for example, it can recognize the person in a profile
    photo and tell his/her gender, age range, emotions. Amazon charges this service
    at 1,300 USD per one million queries.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，机器学习即服务（MLaaS）获得了广泛关注[[130](#bib.bib130)]，使客户能够在不建立自身预测模型的情况下受益于机器学习。为了简化使用，MLaaS
    供应商提供了多种 API，供客户完成机器学习任务，例如：图像分类、音频片段识别或文本意图识别。当然，这些服务是核心竞争力，并且会对客户的查询收费。表[II](#S3.T2
    "TABLE II ‣ 3.1 Deep Learning System ‣ 3 Overview ‣ Towards Security Threats of
    Deep Learning Systems: A Survey") 显示了代表性的现货产品及其功能、提供给客户的输出和使用费用。以 Amazon 图像识别为例，它可以识别头像中的人并告诉其性别、年龄范围和情绪。Amazon
    对每百万次查询收取 1,300 美元的费用。'
- en: 4 Model Extraction Attack
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 模型提取攻击
- en: 4.1 Introduction of Model Extraction Attack
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模型提取攻击介绍
- en: Model extraction attack attempts to duplicate a machine learning model through
    the provided APIs, without prior knowledge of training data and algorithms [[220](#bib.bib220)].
    To formalize, given a specifically selected input $x$, one attacker queries the
    target model $\mathcal{F}$ and obtains the corresponding prediction results $y$.
    Then the attacker can infer or even extract the entire in-use model $\mathcal{F}$.
    With regard to an artificial neural network $y=wx+b$, model extraction attack
    can somehow approximate the values of $w$ and $b$. Model extraction attacks cannot
    only destroy the confidentiality of a model, and damage the interests of its owners,
    but also construct a near-equivalent white-box model for further attacks such
    as adversarial attack [[174](#bib.bib174)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 模型提取攻击试图通过提供的 API 复制机器学习模型，而不需要了解训练数据和算法[[220](#bib.bib220)]。具体来说，给定一个特定选择的输入
    $x$，攻击者查询目标模型 $\mathcal{F}$ 并获得相应的预测结果 $y$。然后，攻击者可以推断甚至提取整个正在使用的模型 $\mathcal{F}$。关于人工神经网络
    $y=wx+b$，模型提取攻击可以在某种程度上近似 $w$ 和 $b$ 的值。模型提取攻击不仅会破坏模型的机密性，损害其所有者的利益，还可能构建一个近似的白盒模型以便进一步攻击，例如对抗攻击
    [[174](#bib.bib174)]。
- en: 'Adversary Model. This attack is mostly carried out under a black-box setting
    and attackers only have access to prediction APIs. The attacker can use an input
    sample to query the target model, and obtain the output including both predicted
    label and class probability vector. Their capabilities are limited in three ways:
    model knowledge, dataset access, and query frequency. Attackers have no idea about
    model architectures, hyperparameters, training process of the victim’s model.
    They cannot obtain natural data with the same distribution of the target model
    training data. In addition, attackers may be blocked by API if submitting queries
    too frequently.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗模型。该攻击大多数在黑箱设置下进行，攻击者只能访问预测API。攻击者可以使用输入样本查询目标模型，并获取包括预测标签和类别概率向量在内的输出。他们的能力有三方面的限制：模型知识、数据集访问和查询频率。攻击者不了解受害者模型的模型架构、超参数和训练过程。他们无法获取与目标模型训练数据相同分布的自然数据。此外，如果查询过于频繁，攻击者可能会被API阻止。
- en: '![Refer to caption](img/5e0449c7022958db84ac7795a33ea3fd.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5e0449c7022958db84ac7795a33ea3fd.png)'
- en: 'Figure 3: Workflow of model extraction attack'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：模型提取攻击的工作流程
- en: 'Workflow. Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Introduction of Model Extraction
    Attack ‣ 4 Model Extraction Attack ‣ Towards Security Threats of Deep Learning
    Systems: A Survey") shows a typical workflow of this attack. First, attackers
    submit inputs to the target model and get prediction values. Then they use input-output
    pairs and different approaches to extract the confidential data. More specifically,
    confidential data includes parameters [[220](#bib.bib220)], hyperparameters [[226](#bib.bib226)],
    architectures [[164](#bib.bib164)], decision boundaries [[174](#bib.bib174)][[107](#bib.bib107)],
    and functionality [[167](#bib.bib167)][[54](#bib.bib54)].'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '工作流程。图[3](#S4.F3 "Figure 3 ‣ 4.1 Introduction of Model Extraction Attack ‣
    4 Model Extraction Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey") 展示了这种攻击的典型工作流程。首先，攻击者向目标模型提交输入并获取预测值。然后，他们使用输入-输出对和不同的方法来提取机密数据。更具体地说，机密数据包括参数[[220](#bib.bib220)]、超参数[[226](#bib.bib226)]、架构[[164](#bib.bib164)]、决策边界[[174](#bib.bib174)][[107](#bib.bib107)]
    和功能[[167](#bib.bib167)][[54](#bib.bib54)]。'
- en: 4.2 Approaches for Extracting Models
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 模型提取的方法
- en: 'There are basically three types of approaches to extract models:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上有三种类型的模型提取方法：
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Equation Solving (ES). For a classification model computing class probabilities
    as a continuous function, it can be denoted as $F(x)=\sigma(w\cdot x+b$) [[220](#bib.bib220)].
    Hence, given sufficient samples ($x$, $F(x)$), attackers can recover the parameters
    (e.g., $w$, $b$) by solving the equation $w\cdot x+b=\sigma^{-1}(F(x))$.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方程求解（ES）。对于将类别概率计算为连续函数的分类模型，它可以表示为 $F(x)=\sigma(w\cdot x+b$) [[220](#bib.bib220)]。因此，给定足够的样本（$x$，$F(x)$），攻击者可以通过求解方程
    $w\cdot x+b=\sigma^{-1}(F(x))$ 来恢复参数（例如 $w$，$b$）。
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Training Metamodel (MM). Metamodel is a classifier for classification models[[164](#bib.bib164)].
    By querying a classification model on the outputs $y$ for certain inputs $x$,
    attackers train a meta-model $F^{m}$, mapping $y$ to $x$, i.e., $x~{}=~{}F^{m}(y)$.
    The trained model can further predict model attributes from the query outputs
    $y$.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练元模型（MM）。元模型是用于分类模型的分类器[[164](#bib.bib164)]。通过对某些输入 $x$ 查询分类模型的输出 $y$，攻击者训练一个元模型
    $F^{m}$，将 $y$ 映射到 $x$，即 $x~{}=~{}F^{m}(y)$。训练后的模型可以进一步从查询输出 $y$ 中预测模型属性。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Training Substitute Model (SM). Substitute model is a simulative model mimicking
    behaviors of the original model. With sufficient querying inputs $x$ and corresponding
    outputs $y$, attackers train the model $F^{s}$ where $y~{}=~{}F^{s}(x)$. As a
    result, the attributes of the substitute model can be near-equivalent to those
    of the original.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练替代模型（SM）。替代模型是模仿原始模型行为的模拟模型。通过足够的查询输入 $x$ 和对应的输出 $y$，攻击者训练模型 $F^{s}$，其中 $y~{}=~{}F^{s}(x)$。结果，替代模型的属性可以与原始模型的属性接近。
- en: Stealing different information corresponds to different methods. In terms of
    time, equation solving is earlier than training meta- and substitute models. It
    can restore precise parameters but is only suitable for small scale models. Due
    to the increase of model size, it is common to train a substitute model to simulate
    the original model’s decision boundaries or classification functionalities. However,
    precise parameters seem less important. Metamodel [[164](#bib.bib164)] is an inverse
    training with substitute model, as it takes the query outputs as input and predicts
    the query inputs as well as model attributes. Besides, it can be also used to
    explore more informative inputs that help infer more internal information of the
    model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 窃取不同的信息对应不同的方法。在时间方面，方程求解比训练元模型和替代模型更早。它可以恢复精确的参数，但只适用于小规模模型。由于模型规模的增加，通常会训练一个替代模型来模拟原始模型的决策边界或分类功能。然而，精确的参数似乎不那么重要。元模型[[164](#bib.bib164)]是与替代模型的逆向训练，因为它将查询输出作为输入，预测查询输入及模型属性。此外，它还可以用来探索更多有用的输入，从而帮助推断模型的更多内部信息。
- en: 'TABLE III: Evaluation on model extraction attacks as per stolen information.
    We sort them by the stolen “Information”, corresponding to Section [4.3](#S4.SS3
    "4.3 Different Extracted Information ‣ 4 Model Extraction Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey"). “Approach” is the attack method,
    corresponding to Section [4.2](#S4.SS2 "4.2 Approaches for Extracting Models ‣
    4 Model Extraction Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey"). “Reducing Query” is the technique for reducing query number in this
    attack. “Recovery Rate” is the accuracy of extracted information. “SVM” is support
    vector machine. “DT” is decision tree. “LR” is logistic regression. “kNN” is K-nearest
    neighbor. “Queries” is the number of required queries for an attack.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：基于窃取的信息对模型提取攻击的评估。我们按窃取的“信息”进行排序，相关内容见第[4.3节](#S4.SS3 "4.3 不同提取信息 ‣ 4 模型提取攻击
    ‣ 深度学习系统安全威胁的调查")。“方法”是攻击方法，相关内容见第[4.2节](#S4.SS2 "4.2 提取模型的方法 ‣ 4 模型提取攻击 ‣ 深度学习系统安全威胁的调查")。“减少查询”是减少此攻击中查询数量的技术。“恢复率”是提取信息的准确性。“SVM”是支持向量机。“DT”是决策树。“LR”是逻辑回归。“kNN”是K最近邻。“查询”是攻击所需的查询数量。
- en: '| Information | Paper | Approach | Reducing Query | Recovery Rate (%) for Models
    | Queries |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 信息 | 论文 | 方法 | 减少查询 | 模型恢复率 (%) | 查询数量 |'
- en: '| SVM | DT | LR | kNN | CNN | DNN |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| SVM | DT | LR | kNN | CNN | DNN |'
- en: '| Parameter | Tramer et al. [[220](#bib.bib220)] | ES | - | 99 | 99 | 99 |
    - | - | 90 | 108,200 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | Tramer et al. [[220](#bib.bib220)] | ES | - | 99 | 99 | 99 | - | - |
    90 | 108,200 |'
- en: '| Hyperparameter | Wang et al. [[226](#bib.bib226)] | ES | - | 99 | - | 99
    | - | - | - | 200 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | Wang et al. [[226](#bib.bib226)] | ES | - | 99 | - | 99 | - | - | -
    | 200 |'
- en: '| Architecture | Joon et al. [[164](#bib.bib164)] | MM | KENNEN-IO | - | -
    | - | - | - | 88 | 500 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | Joon et al. [[164](#bib.bib164)] | MM | KENNEN-IO | - | - | - | - |
    - | 88 | 500 |'
- en: '| Decision Boundary | Papernot et al. [[174](#bib.bib174)] | SM | reservoir
    sampling [[223](#bib.bib223)] | - | - | - | - | - | 84 | 800 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 决策边界 | Papernot et al. [[174](#bib.bib174)] | SM | reservoir sampling [[223](#bib.bib223)]
    | - | - | - | - | - | 84 | 800 |'
- en: '| Papernot et al. [[173](#bib.bib173)] | SM | reservoir sampling [[223](#bib.bib223)]
    | 83 | 61 | 89 | 85 | - | 89 | 800 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Papernot et al. [[173](#bib.bib173)] | SM |  reservoir sampling [[223](#bib.bib223)]
    | 83 | 61 | 89 | 85 | - | 89 | 800 |'
- en: '| PRADA [[107](#bib.bib107)] | SM | - | - | - | - | - | - | 91 | 300 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| PRADA [[107](#bib.bib107)] | SM | - | - | - | - | - | - | 91 | 300 |'
- en: '| Functionality | Silva et al. [[54](#bib.bib54)] | SM | - | - | - | - | -
    | 98 | - | - |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 功能 | Silva et al. [[54](#bib.bib54)] | SM | - | - | - | - | - | 98 | - |
    - |'
- en: '| Orekondy et al. [[167](#bib.bib167)] | SM | random, adaptive sampling | -
    | - | - | - | 98 | - | 60,000 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Orekondy et al. [[167](#bib.bib167)] | SM | 随机，自适应采样 | - | - | - | - | 98
    | - | 60,000 |'
- en: 4.3 Different Extracted Information
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 不同提取信息
- en: 4.3.1 Model Parameters & Hyperparameters
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 模型参数与超参数
- en: Parameters are variables that the model can learn automatically from the data,
    such as weights and bias. Hyperparameters are specific parameters whose values
    are set before the training process, including dropout rate, learning rate, mini-batch
    size, parameters in objective functions to balance loss function and regularization
    terms, and so on. In the early work, Tramèr et al. [[220](#bib.bib220)] tried
    equation solving to recover parameters in machine learning models, such as logistic
    regression, SVM, and MLP. They built equations about the model by querying APIs,
    and obtained parameters by solving equations. However, it needs plenty of queries
    and is not applicable to DNN. Wang et al. [[226](#bib.bib226)] tried to steal
    hyperparameter-$\lambda$ on the premise of known model algorithm and training
    data. $\lambda$ is used to balance loss functions and regularization terms. They
    assumed that the gradient of the objective function is $\vec{0}$ and thus got
    many linear equations through many queries. They estimated the hyperparameters
    through linear least square method.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数是模型可以从数据中自动学习的变量，如权重和偏差。超参数是训练过程开始前设置的特定参数，包括 dropout 率、学习率、小批量大小、平衡损失函数和正则化项的目标函数中的参数等等。在早期的研究中，Tramèr
    等人[[220](#bib.bib220)] 尝试通过方程求解来恢复机器学习模型中的参数，如逻辑回归、SVM 和 MLP。他们通过查询 API 建立关于模型的方程，并通过求解方程获得参数。然而，这需要大量的查询，并且不适用于
    DNN。Wang 等人[[226](#bib.bib226)] 尝试在已知模型算法和训练数据的前提下窃取超参数-$\lambda$。$\lambda$ 用于平衡损失函数和正则化项。他们假设目标函数的梯度为
    $\vec{0}$，因此通过许多查询得到了许多线性方程。他们通过线性最小二乘法估计超参数。
- en: 4.3.2 Model Architectures
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 模型架构
- en: Architectural details include the number of layers in the model, the number
    of neurons in each layer, how are they connected, what activation functions are
    used, and so on. Recent papers usually train classifiers to predict attributes.
    Joon et al. [[164](#bib.bib164)] trained metamodel, a supervised classifier of
    classifiers, to steal model attributes (architecture, operation time, and training
    data size). They submitted query inputs via APIs, and took corresponding outputs
    as inputs of metamodel, then trained metamodel to predict model attributes as
    outputs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 架构细节包括模型中的层数、每层中的神经元数量、它们是如何连接的、使用了哪些激活函数等等。近期的论文通常训练分类器以预测属性。Joon 等人[[164](#bib.bib164)]
    训练了元模型，一个监督学习的分类器，用于窃取模型属性（架构、操作时间和训练数据大小）。他们通过 API 提交查询输入，将相应的输出作为元模型的输入，然后训练元模型以预测模型属性作为输出。
- en: 4.3.3 Model Decision Boundaries
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 模型决策边界
- en: Decision boundaries are the classification boundary between different classes.
    They are important for generating adversarial examples. In [[174](#bib.bib174)][[107](#bib.bib107)][[173](#bib.bib173)],
    they steal decision boundaries and generate transferable adversarial samples to
    attack a black box model. Papernot et al. [[174](#bib.bib174)] used Jacobian-based
    Dataset Augmentation (JbDA) to produce synthetic samples, which moved to the nearest
    boundary between the current class and all other classes. This technology aims
    not to maximize the accuracy of substitute models, but ensures that samples arrive
    at decision boundaries with small queries. Juuti et al. [[107](#bib.bib107)] extended
    JbDA to Jb-topk, where samples move to the nearest $k$ boundaries between current
    class and any other class. They produced transferable targeted adversarial samples
    rather than untargeted [[174](#bib.bib174)]. In terms of model knowledge, Papernot
    et al. [[173](#bib.bib173)] found that model architecture knowledge was unnecessary
    because a simple model could be extracted by a more complex model, such as a DNN.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 决策边界是不同类别之间的分类边界。它们对于生成对抗样本至关重要。在 [[174](#bib.bib174)][[107](#bib.bib107)][[173](#bib.bib173)]
    中，他们窃取决策边界并生成可转移的对抗样本以攻击黑箱模型。Papernot 等人[[174](#bib.bib174)] 使用基于雅可比矩阵的数据增强（JbDA）来生成合成样本，将样本移动到当前类别与所有其他类别之间的最近边界。该技术旨在不最大化替代模型的准确性，而是确保样本通过少量查询到达决策边界。Juuti
    等人[[107](#bib.bib107)] 将 JbDA 扩展到 Jb-topk，其中样本移动到当前类别与任何其他类别之间的最近 $k$ 个边界。他们生成了可转移的针对性对抗样本，而不是非针对性样本[[174](#bib.bib174)]。在模型知识方面，Papernot
    等人[[173](#bib.bib173)] 发现模型架构知识是不必要的，因为一个简单的模型可以通过更复杂的模型提取出来，例如 DNN。
- en: 4.3.4 Model Functionalities
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 模型功能
- en: Similar functionalities refer to replicating the original model as much as possible
    on prediction results. The primary goal is to construct a predictive model that
    has closest input-output pairs with the original. In [[167](#bib.bib167)][[54](#bib.bib54)],
    they try to improve the classification accuracy of a substitute model. Silva et
    al. [[54](#bib.bib54)] used a problem domain dataset, non-problem domain dataset,
    and their mixture to train a model respectively. They found the model trained
    with a non-problem domain dataset also did well in accuracy. Besides, Orekondy
    et al. [[167](#bib.bib167)] assumed attackers had no semantic knowledge over model
    outputs. They chose very large datasets and selected suitable samples one by one
    to query the black-box model. A reinforcement learning approach was introduced
    to improve query efficiency and reduce query counts.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 相似功能是指在预测结果上尽可能地复制原始模型。主要目标是构建一个与原始模型的输入输出对最接近的预测模型。在[[167](#bib.bib167)][[54](#bib.bib54)]中，他们尝试提高替代模型的分类准确性。Silva等人[[54](#bib.bib54)]分别使用问题领域数据集、非问题领域数据集及其混合数据集来训练模型。他们发现，用非问题领域数据集训练的模型在准确性上也表现良好。此外，Orekondy等人[[167](#bib.bib167)]假设攻击者对模型输出没有语义知识。他们选择了非常大的数据集，并逐个选择合适的样本来查询黑箱模型。引入了一种强化学习方法来提高查询效率和减少查询次数。
- en: 4.4 Analysis of Model Extraction Attack
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 模型提取攻击分析
- en: 'Model extraction attack is an emerging field of attack. In this study, we survey
    8 related papers and classify them by extracted information as shown in Table [III](#S4.T3
    "TABLE III ‣ 4.2 Approaches for Extracting Models ‣ 4 Model Extraction Attack
    ‣ Towards Security Threats of Deep Learning Systems: A Survey"). Based on the
    statistics, we draw the following conclusions.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '模型提取攻击是一个新兴的攻击领域。在本研究中，我们调查了8篇相关论文，并按照提取的信息进行分类，如表[III](#S4.T3 "TABLE III ‣
    4.2 Approaches for Extracting Models ‣ 4 Model Extraction Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey")所示。根据统计数据，我们得出以下结论。'
- en: Finding 1. Training the substitute model (SM) is the dominant method in model
    extraction attacks with manifold advantages.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 发现1. 训练替代模型（SM）是模型提取攻击中主要的方法，具有多种优势。
- en: The ES approach needs more than 100 thousand queries to attack a DNN model,
    while the SM method only needs hundreds of queries, and it can attack a more complex
    CNN network. Equation solving is deemed as an efficient way to recover parameters [[220](#bib.bib220)]
    or hyperparameters [[226](#bib.bib226)] in linear algorithms, since it has an
    upper bound for sufficient queries. However, the ES approach is hardly applicable
    to the non-linear deep learning models. Attacking DNN requires a huge amount of
    queries (108,200 in [[220](#bib.bib220)]). So researchers turn to the compelling
    training-based approach. For instance, [[164](#bib.bib164)] trains a classifier
    based on a target model, dubbed as metamodel, to predict structure information.
    This approach cannot cope with complex model attributes such as decision boundary
    and functionality. That drives the prevalence of substitute models (SM) which
    serve as an incarnation of the target model which behaves quite similarly. As
    such, the substitute model has approximated attributes and prediction results.
    The SM approach only needs 300 queries to attack DNN in [[107](#bib.bib107)].
    For a more complex CNN, SM needs 60,000 queries in [[167](#bib.bib167)]. This
    shows that attacking more complex models requires more queries. Besides, it can
    be further used to steal model’s training data [[107](#bib.bib107)] and generating
    adversarial examples [[173](#bib.bib173)].
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ES方法需要超过10万次查询才能攻击一个DNN模型，而SM方法只需要数百次查询，并且可以攻击更复杂的CNN网络。方程求解被认为是在线性算法中恢复参数[[220](#bib.bib220)]或超参数[[226](#bib.bib226)]的有效方法，因为它对足够的查询有一个上限。然而，ES方法几乎无法应用于非线性深度学习模型。攻击DNN需要大量的查询（在[[220](#bib.bib220)]中为108,200次）。因此，研究人员转向了引人注目的基于训练的方法。例如，[[164](#bib.bib164)]基于目标模型训练了一个分类器，称为元模型，以预测结构信息。这种方法无法应对复杂的模型属性，如决策边界和功能性。这推动了替代模型（SM）的普及，SM作为目标模型的一个化身，表现得非常相似。因此，替代模型具有近似的属性和预测结果。SM方法只需300次查询即可攻击DNN（在[[107](#bib.bib107)]中）。对于更复杂的CNN，SM需要60,000次查询（在[[167](#bib.bib167)]中）。这表明攻击更复杂的模型需要更多的查询。此外，它还可以进一步用于窃取模型的训练数据[[107](#bib.bib107)]和生成对抗样本[[173](#bib.bib173)]。
- en: Finding 2. Reducing queries, which can save monetary costs for a pay-per-query
    MLaaS commercial system and also be resistant to attack detection, has become
    an intriguing research direction in recent years.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 2. 减少查询可以节省按查询付费的 MLaaS 商业系统的货币成本，同时也能够抵抗攻击检测，这在近年来已成为一个引人入胜的研究方向。
- en: The requirement of query reduction arises due to the high expense of queries
    and query amount limitation. In our investigated papers, [[164](#bib.bib164)]
    trains a metamodel–KENNEN-IO for optimizing the query inputs. [[174](#bib.bib174)],
    leverage *reservoir sampling* to select representative samples for querying, and
    [[167](#bib.bib167)] proposes two sampling strategies, i.e., *random* and *adaptive*
    to reduce queries. Moreover, active learning [[126](#bib.bib126)], natural evolutionary
    strategies [[99](#bib.bib99)], optimization-based approaches [[50](#bib.bib50)][[193](#bib.bib193)]
    have been adopted for query reduction.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 查询减少的需求产生是由于查询的高费用和查询量限制。在我们调查的论文中，[[164](#bib.bib164)] 训练了一个元模型–KENNEN-IO 来优化查询输入。[[174](#bib.bib174)]
    利用*水库抽样*来选择代表性样本进行查询，[[167](#bib.bib167)] 提出了两种采样策略，即*随机*和*自适应*来减少查询。此外，主动学习[[126](#bib.bib126)]、自然进化策略[[99](#bib.bib99)]、基于优化的方法[[50](#bib.bib50)][[193](#bib.bib193)]
    已被用于查询减少。
- en: Finding 3. Model extraction attack is evolving from a puzzle solving game to
    a simulation game with cost-profit tradeoffs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 3. 模型提取攻击正从一个难题解决游戏演变为一个具有成本效益权衡的模拟游戏。
- en: MLaaS magnates like Amazon and Google have a tremendous scale of networks running
    behind services. It costs much to infer how many layers or neurons are in the
    neural networks. Therefore, it makes a remarkable dent in attackers’ interest
    of solving model attributes. On the other hand, inferring decision boundary and
    model functionality emerge as new circumvention. Treating the target model as
    a black box, attackers observe the response by feeding it with crafted inputs,
    and finally construct a close approximation. Although the substitute model is
    likely simpler and underperforms in some cases, its prediction capabilities still
    make profits for attackers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Amazon 和 Google 这样的 MLaaS 巨头在服务背后运行着大规模的网络。推断神经网络中有多少层或神经元成本高昂。因此，这对攻击者解决模型属性的兴趣产生了显著影响。另一方面，推断决策边界和模型功能成为新的规避方式。将目标模型视为黑箱，攻击者通过向其输入精心设计的数据来观察响应，最终构建出一个近似模型。尽管替代模型在某些情况下可能更简单且表现不佳，但其预测能力仍然能为攻击者带来利润。
- en: 5 Model Inversion Attack
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 模型反演攻击
- en: '![Refer to caption](img/af6f28de7db971bcc45de4ba6513c69f.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/af6f28de7db971bcc45de4ba6513c69f.png)'
- en: 'Figure 4: Workflow of model inversion attack'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：模型反演攻击的工作流程
- en: 5.1 Introduction of Model Inversion Attack
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 模型反演攻击简介
- en: In a typical model training process, lots of information is extracted and abstracted
    from the training data to the product model. However, there also exists one inverse
    information flow which allows attackers to infer the training data from the model
    since neural networks may remember too much information of the training data [[205](#bib.bib205)].
    Model inversion attack leverages this information flow and restores data memberships
    or data properties, such as faces in face recognition systems through model prediction
    or its confidence coefficient. Model inversion can also be used to form physical
    watermarking to detect a replay attack [[192](#bib.bib192)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的模型训练过程中，大量的信息从训练数据中提取并抽象到产品模型中。然而，也存在一种逆向信息流，这使得攻击者能够从模型中推断出训练数据，因为神经网络可能记住了过多的训练数据的信息[[205](#bib.bib205)]。模型反演攻击利用这种信息流，通过模型预测或其置信系数恢复数据成员或数据属性，例如在面部识别系统中的面孔。模型反演也可以用于形成物理水印，以检测重放攻击[[192](#bib.bib192)]。
- en: Additionally, model inversion attack can be further refined into *membership
    inference attack (MIA)* and *property inference attack (PIA)*. We distinguish
    them based on whether the attacker obtains individual information (MIA) or statistical
    information (PIA). In MIA, the attacker can determine whether a specific record
    is included or not in the training data. In PIA, the attacker can speculate whether
    there is a certain statistical property in the training dataset.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模型反演攻击还可以进一步细化为*成员推断攻击 (MIA)* 和 *属性推断攻击 (PIA)*。我们根据攻击者是否获得个人信息（MIA）或统计信息（PIA）来区分它们。在
    MIA 中，攻击者可以确定特定记录是否包含在训练数据中。在 PIA 中，攻击者可以推测训练数据集中是否存在某些统计属性。
- en: Adversary Model. Model inversion attack can be executed in both black-box or
    white-box settings. In a white-box attack, the parameters and architecture of
    the target model are known by attackers. Hence, they can easily obtain a substitute
    model that behaves similarly, even without querying the model. In a black-box
    attack, attacker’s capabilities are limited in model architectures, statistics
    and distribution of training data and so on. Attackers cannot obtain complete
    training set information. However, in either setting, attackers can make queries
    with specific inputs and get corresponding outputs as well as confidence values.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗模型。模型反演攻击可以在黑盒或白盒设置中执行。在白盒攻击中，攻击者已知目标模型的参数和架构。因此，即使不查询模型，他们也能轻松获得一个表现相似的替代模型。在黑盒攻击中，攻击者的能力受到模型架构、统计数据和训练数据分布等方面的限制。攻击者无法获得完整的训练集信息。然而，在任何设置下，攻击者都可以使用特定的输入进行查询，并获取相应的输出以及置信度值。
- en: 'Workflow. Figure [4](#S5.F4 "Figure 4 ‣ 5 Model Inversion Attack ‣ Towards
    Security Threats of Deep Learning Systems: A Survey") shows a workflow of model
    inversion attack which is suitable for both MIA and PIA. Here we take MIA as an
    example. MIA can be accomplished in varying ways: by querying the target model
    to get input-output pairs, attackers can merely exercise Step 4 with heuristic
    methods to determine the membership of a record [[198](#bib.bib198)][[144](#bib.bib144)][[89](#bib.bib89)][[137](#bib.bib137)]
    (Approach 1); Alternatively, attackers can train an attack model for determination,
    which necessitates an attack model training process (Step 3). Attack model’s training
    data is obtained by query inputs and response [[184](#bib.bib184)][[19](#bib.bib19)]
    (Approach 2); Due to the limitation of queries and model attributes, some studies
    introduce shadow models (see Section [5.2.2](#S5.SS2.SSS2 "5.2.2 Step 2: Shadow
    Model Training ‣ 5.2 Membership Inference Attack ‣ 5 Model Inversion Attack ‣
    Towards Security Threats of Deep Learning Systems: A Survey") in detail) to provide
    training data for the attack model [[203](#bib.bib203), [198](#bib.bib198)], which
    necessitates shadow model training (Step 2). Moreover, data synthesis (Step 1)
    is proposed to provide more training data for a sufficient training (Approach
    3).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '工作流程。图 [4](#S5.F4 "Figure 4 ‣ 5 Model Inversion Attack ‣ Towards Security Threats
    of Deep Learning Systems: A Survey") 显示了适用于 MIA 和 PIA 的模型反演攻击工作流程。这里我们以 MIA 为例。MIA
    可以通过不同的方式完成：通过查询目标模型以获取输入-输出对，攻击者可以仅使用启发式方法进行步骤 4 来确定记录的成员身份 [[198](#bib.bib198)][[144](#bib.bib144)][[89](#bib.bib89)][[137](#bib.bib137)]（方法
    1）；或者，攻击者可以训练一个攻击模型来进行确定，这需要一个攻击模型训练过程（步骤 3）。攻击模型的训练数据通过查询输入和响应 [[184](#bib.bib184)][[19](#bib.bib19)]
    获得（方法 2）；由于查询和模型属性的限制，一些研究引入了影子模型（见第 [5.2.2](#S5.SS2.SSS2 "5.2.2 Step 2: Shadow
    Model Training ‣ 5.2 Membership Inference Attack ‣ 5 Model Inversion Attack ‣
    Towards Security Threats of Deep Learning Systems: A Survey") 节详细介绍）来为攻击模型提供训练数据
    [[203](#bib.bib203), [198](#bib.bib198)]，这需要影子模型训练（步骤 2）。此外，数据合成（步骤 1）被提出以提供更多的训练数据以确保足够的训练（方法
    3）。'
- en: 5.2 Membership Inference Attack
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 成员推断攻击
- en: Truex et al. [[221](#bib.bib221)] presented a generally systematic formulation
    of MIA. Given an instance $x$ and black-box access to the classification model
    $F_{t}$ trained on the dataset $D$, can an adversary infer whether the instance
    $x$ is included in $D$ when training $F_{t}$ with a high degree of confidence?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Truex 等人 [[221](#bib.bib221)] 提出了 MIA 的一般性系统化表述。给定一个实例 $x$ 和对在数据集 $D$ 上训练的分类模型
    $F_{t}$ 的黑盒访问，攻击者能否以较高的置信度推断出实例 $x$ 是否包含在训练 $F_{t}$ 的数据集 $D$ 中？
- en: 'Most of MIAs proceed in accordance with the workflow in Figure [4](#S5.F4 "Figure
    4 ‣ 5 Model Inversion Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey"). More specifically, to infer whether one data item or property exists
    in the training set, the attacker may prepare the initial data and make transformations
    to the data. Subsequently, it devises a number of principles for determining the
    correction of its guessing. This attack destroys information privacy. The privacy
    protection terms used in related articles are explained in detail in Section [8.6](#S8.SS6
    "8.6 Corresponding defense methods ‣ 8 Discussion ‣ Towards Security Threats of
    Deep Learning Systems: A Survey").'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数MIAs按照图[4](#S5.F4 "Figure 4 ‣ 5 Model Inversion Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey")中的工作流程进行。更具体地说，为了推测某个数据项或属性是否存在于训练集中，攻击者可能会准备初始数据并对数据进行转换。随后，它制定了一些原则来判断其猜测的准确性。这种攻击破坏了信息隐私。相关论文中使用的隐私保护术语在第[8.6](#S8.SS6
    "8.6 Corresponding defense methods ‣ 8 Discussion ‣ Towards Security Threats of
    Deep Learning Systems: A Survey")节中详细解释。'
- en: '5.2.1 Step 1: Data Synthesis'
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '5.2.1 步骤 1: 数据合成'
- en: 'Initial data has to be collected as prerequisites for determining the membership.
    According to our investigation, an approximated set of training data is desired
    to imply membership. This set can be obtained either by:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 初始数据必须收集作为确定成员资格的前提条件。根据我们的调查，需要一个近似的训练数据集来暗示成员资格。这个数据集可以通过以下方式获得：
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Generating samples manually.* This method needs some prior knowledge to generate
    data. For instance, Shokri [[203](#bib.bib203)] produced datasets similar to the
    target training dataset and used the same MLaaS to train several shadow models.
    These datasets were produced by model-based synthesis, statistics-based synthesis,
    noisy real data and other methods. If the attacker has access to part of the dataset,
    then he can generate noisy real data by flipping a few randomly selected features
    on real data. These data make up the noisy dataset. If the attacker has some statistical
    information about the dataset, such as marginal distributions of different features,
    then he can generate statistics-based synthesis using this knowledge. If the attacker
    has no knowledge above, he can also generate model-based synthesis by searching
    for possible data records. The records the search algorithm needs to find are
    correctly classified by the target model with high confidence.'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*手动生成样本。* 这种方法需要一些先验知识来生成数据。例如，Shokri [[203](#bib.bib203)] 产生了类似于目标训练数据集的数据集，并使用相同的MLaaS训练了多个影子模型。这些数据集是通过模型基础合成、统计基础合成、噪声真实数据及其他方法产生的。如果攻击者可以访问部分数据集，他可以通过翻转真实数据上随机选择的几个特征来生成噪声真实数据。这些数据组成了噪声数据集。如果攻击者有关于数据集的一些统计信息，比如不同特征的边际分布，他可以利用这些知识生成统计基础合成。如果攻击者没有上述知识，他也可以通过搜索可能的数据记录来生成模型基础合成。搜索算法需要找到的记录在目标模型中被高置信度地正确分类。'
- en: In [[198](#bib.bib198)], they proposed a data transferring attack without any
    query to the target model. They chose different datasets to train the shadow model.
    The shadow model was used to capture membership status of data points in datasets.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 [[198](#bib.bib198)] 中，他们提出了一种不对目标模型进行任何查询的数据传输攻击。他们选择了不同的数据集来训练影子模型。影子模型被用来捕捉数据点在数据集中的成员状态。
- en: •
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Generating samples by model.* This method aims to produce training records
    by training generated models such as GAN. Generated samples are similar to that
    from the target training dataset. Improving the similarity ratio will make this
    method more useful.'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*通过模型生成样本。* 这种方法旨在通过训练生成的模型（如GAN）生成训练记录。生成的样本类似于目标训练数据集中的样本。提高相似度比例将使这种方法更有用。'
- en: Both [[137](#bib.bib137)] and [[86](#bib.bib86)] attacked generated models.
    Liu et al. [[137](#bib.bib137)] presented a new white-box method for single membership
    attacks and co-membership attacks. The basic idea was to train a generated model
    with the target model, which took the output of the target model as input, and
    took the similar input of the target model as output. After training, the attack
    model could generate data that is similar to the target training dataset. Considering
    about the difficult implementation of CNN in [[203](#bib.bib203)], Hitaj et al.
    [[89](#bib.bib89)] proposed a more general MIA method. They performed a white-box
    attack in the scenario of collaborative deep learning models. They constructed
    a generator for the target classification model, and used it to form a GAN. After
    training, the GAN could generate data similar to the target training set. However,
    this method was limited in that all samples belonging to the same classification
    need to be visually similar, and it could not generate an actual target training
    pattern or distinguish them under the same class. Through analyzing a black-box
    model before and after being updated, Salem et al. [[197](#bib.bib197)] proposed
    a hybrid generative model to steal information of the updated dataset.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[[137](#bib.bib137)] 和 [[86](#bib.bib86)] 都攻击了生成的模型。Liu 等人 [[137](#bib.bib137)]
    提出了一个新的白盒方法，用于单一成员攻击和共同成员攻击。基本思路是用目标模型训练一个生成模型，该生成模型以目标模型的输出作为输入，并将目标模型的类似输入作为输出。经过训练，攻击模型能够生成与目标训练数据集相似的数据。考虑到
    [[203](#bib.bib203)] 中 CNN 实现的困难，Hitaj 等人 [[89](#bib.bib89)] 提出了一个更通用的 MIA 方法。他们在协作深度学习模型的场景下进行了白盒攻击。他们为目标分类模型构建了一个生成器，并用它形成了一个
    GAN。经过训练，GAN 能够生成类似于目标训练集的数据。然而，这种方法的局限在于所有属于相同分类的样本需要在视觉上相似，并且无法生成实际的目标训练模式或在同一类别下区分它们。通过分析在更新前后的黑盒模型，Salem
    等人 [[197](#bib.bib197)] 提出了一个混合生成模型来窃取更新数据集的信息。'
- en: '5.2.2 Step 2: Shadow Model Training'
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 第2步：影子模型训练
- en: Attackers have sometimes to transform the initial data for further determination.
    In particular, *shadow model* is proposed to imitate target model’s behavior by
    training on a similar dataset [[203](#bib.bib203)]. The dataset takes records
    by data synthesis as inputs, and their labels as outputs. Shadow model is trained
    on such a dataset. It can provide class probability vector and classification
    result of a record. Shokri et al. [[203](#bib.bib203)] implement the first MIA
    attack method for a black-box model by API calls in machine learning. They produced
    datasets similar to the target training dataset and used the same MLaaS to train
    several shadow models. These datasets were produced by model-based synthesis,
    statistics-based synthesis, noisy real data and other methods. Shadow models were
    used to provide training set (class labels, prediction probabilities and whether
    data record belongs to shadow training set) for the attack model. Salem et al. [[198](#bib.bib198)]
    relax the constraints in [[203](#bib.bib203)] (need to train shadow models on
    the same MLaaS, and the same distribution between datasets of shadow models and
    target model), and use only one shadow model without the knowledge of target model
    structure and training dataset distribution. Here, the shadow model just captures
    the membership status of records in a different dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者有时需要转换初始数据以进行进一步的确定。特别是，*影子模型* 被提出以通过在相似数据集上训练来模仿目标模型的行为 [[203](#bib.bib203)]。该数据集以数据合成为输入记录，并以其标签作为输出。影子模型在这样的数据集上进行训练。它可以提供记录的类别概率向量和分类结果。Shokri
    等人 [[203](#bib.bib203)] 实现了第一个针对黑盒模型的 MIA 攻击方法，通过机器学习中的 API 调用。他们生成了与目标训练数据集相似的数据集，并使用相同的
    MLaaS 训练了多个影子模型。这些数据集是通过基于模型的合成、基于统计的合成、噪声真实数据以及其他方法生成的。影子模型用于为攻击模型提供训练集（类别标签、预测概率以及数据记录是否属于影子训练集）。Salem
    等人 [[198](#bib.bib198)] 放宽了 [[203](#bib.bib203)] 中的约束（需要在相同的 MLaaS 上训练影子模型，以及影子模型和目标模型的数据集之间的分布相同），并且只使用了一个影子模型，而无需了解目标模型的结构和训练数据集分布。在这里，影子模型只是捕获了记录在不同数据集中的成员状态。
- en: '5.2.3 Step 3: Attack Model Training'
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 第3步：攻击模型训练
- en: The attack model is a binary classifier. Its input is the class probabilities
    and label of the record to be judged, and output is yes (means the record belongs
    to the dataset of target model) or no. Training dataset is usually required to
    train the attack model. The problem is that the output label of whether a record
    belongs to the dataset of target model cannot be obtained. So here attackers often
    generate substituted dataset by data synthesis. The input of this training is
    generated either by the shadow model (Approach 3) [[203](#bib.bib203)][[198](#bib.bib198)]
    or the target model (Approach 2) [[184](#bib.bib184)][[153](#bib.bib153)]. The
    attack model training process first selects some records from both inside and
    outside the substituted dataset, and then obtains the class probability vector
    through target model or shadow model. The vector and the label of record are taken
    as input, and whether this record belongs to substituted dataset is taken as output.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击模型是一个二分类器。其输入是待判断记录的类别概率和标签，输出是“是”（表示记录属于目标模型的数据集）或“否”。通常需要训练数据集来训练攻击模型。问题在于，无法获得记录是否属于目标模型数据集的输出标签。因此，攻击者通常通过数据合成生成替代数据集。此训练的输入由影子模型（方法
    3）[[203](#bib.bib203)][[198](#bib.bib198)] 或目标模型（方法 2）[[184](#bib.bib184)][[153](#bib.bib153)]
    生成。攻击模型训练过程首先从替代数据集中选取一些记录，然后通过目标模型或影子模型获取类别概率向量。将该向量和记录的标签作为输入，记录是否属于替代数据集作为输出。
- en: For a model $F$ and its training dataset $D$, training attack model needs information
    of label $x$, $F(x)$, and whether $x\in D$. If using a shadow model, shadow model
    $F$ and its dataset $D$ are known. All information is from shadow model and corresponding
    dataset. If using the target model, $F$ is the target model and $D$ is the training
    dataset. However, attackers do not know $D$. So information whether $x\in D$ need
    to be replaced by whether $x\in D^{\prime}$, where $D^{\prime}$ is similar to
    $D$.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型 $F$ 及其训练数据集 $D$，训练攻击模型需要标签 $x$、$F(x)$ 以及是否 $x\in D$ 的信息。如果使用影子模型，则影子模型
    $F$ 及其数据集 $D$ 已知。所有信息来自影子模型和相应数据集。如果使用目标模型，$F$ 是目标模型，而 $D$ 是训练数据集。然而，攻击者不知道 $D$。因此，需要用是否
    $x\in D^{\prime}$ 来替换信息，其中 $D^{\prime}$ 与 $D$ 类似。
- en: '5.2.4 Step 4: Membership Determination'
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 步骤 4：成员资格确定
- en: 'Given one input, this component is responsible for determining whether the
    query input is a member of the training set of the target system. To accomplish
    the goal, the contemporary approaches can be categorized into two classes:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入，该组件负责确定查询输入是否为目标系统训练集的成员。为了实现这一目标，当代方法可以分为两类：
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Attack model-based Method.* In inference phase, attackers first put the record
    to be judged into the target model, and get its class probability vector, then
    put the vector and label of record into the attack model, and get the membership
    of this record. Pyrgelis et al. [[184](#bib.bib184)] implemented MIA for aggregating
    location data. The main idea was to use priori position information and attack
    through a distinguishability game process with a distinguishing function. They
    trained a classifier (attack model) as distinguishing function to determine whether
    data is in target dataset. Yang et al. [[256](#bib.bib256)] leverage the background
    knowledge to form an auxiliary set to train the attack model, without access to
    the original training data. Nasr et al. [[161](#bib.bib161)] implement a white-box
    MIA on both centralized and federated learning. They take all gradients and outputs
    of each layer as the attack features. All these features are used to train the
    attack model.'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*基于攻击模型的方法*。在推断阶段，攻击者首先将待判断记录输入目标模型，获取其类别概率向量，然后将该向量和记录的标签输入攻击模型，获得该记录的成员身份。Pyrgelis
    等人 [[184](#bib.bib184)] 实现了用于聚合位置数据的 MIA。其主要思想是利用先验位置信息，通过区分性游戏过程和区分函数进行攻击。他们训练了一个分类器（攻击模型）作为区分函数，以确定数据是否在目标数据集中。Yang
    等人 [[256](#bib.bib256)] 利用背景知识形成辅助集来训练攻击模型，而不访问原始训练数据。Nasr 等人 [[161](#bib.bib161)]
    实现了针对集中式和联邦学习的白盒 MIA。他们将每层的所有梯度和输出作为攻击特征，所有这些特征用于训练攻击模型。'
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Heuristic Method.* This method uses prediction probability, instead of an
    attack model, to determine the membership. Intuitively, the maximum value in class
    probabilities of a record in the target dataset is usually greater than the record
    not in it. But they require some preconditions and auxiliary information to obtain
    reliable probability vectors or binary results, which is a limitation to apply
    to more general scenarios. How to lower attack cost and reduce auxiliary information
    can be considered in the future study. Fredrikson et al. [[65](#bib.bib65)] construct
    the probability of whether a certain data appears in the target training dataset.
    Then they searched for input data with maximum probability, which is similar to
    the target training set. The third attack method in Salem et al. [[198](#bib.bib198)]
    only required the probability vector of outputs from the target model, and used
    statistical measurement method to compare whether the maximum classification probability
    exceeds a certain value.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*启发式方法。* 该方法使用预测概率而非攻击模型来确定成员资格。直观地，目标数据集中记录的类别概率的最大值通常大于不在其中的记录。但是，它们需要一些前提条件和辅助信息来获得可靠的概率向量或二元结果，这限制了其应用于更一般的场景。未来的研究可以考虑如何降低攻击成本和减少辅助信息。Fredrikson
    等人[[65](#bib.bib65)] 构建了某个数据是否出现在目标训练数据集中的概率。然后，他们搜索最大概率的输入数据，这些数据类似于目标训练集。Salem
    等人[[198](#bib.bib198)] 提出的第三种攻击方法仅需目标模型输出的概率向量，并使用统计测量方法比较最大分类概率是否超过某个值。'
- en: Long et al. [[144](#bib.bib144)] put forward a generalized MIA method, which
    was easier to attack non-overfitted data, different from [[203](#bib.bib203)].
    They trained a number of reference models similar to the target model, and chose
    vulnerable data according to the output of reference models before Softmax, then
    compared outputs between the target model and reference models to calculate the
    probability of data belonging to the target training dataset. Reference models
    in this paper were used to mimic the target model, like shadow models. But they
    did not need an attack model. Hayes et al. [[86](#bib.bib86)] proposed a method
    of attacking generated models. The idea was that attackers determined which dataset
    from attackers belonged to the target training set, according to the probability
    vector output by classifier. Higher probability was more likely from the target
    training set (they selected the upper $n$ sizes). In white-box, the classifier
    was constructed by that of target model. In black-box, they used obtained data
    by querying target model to reproduce classifier with GAN.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Long 等人[[144](#bib.bib144)] 提出了一个广义的 MIA 方法，这种方法比[[203](#bib.bib203)]更容易攻击非过拟合数据。他们训练了多个与目标模型相似的参考模型，并根据参考模型在
    Softmax 之前的输出选择脆弱数据，然后比较目标模型和参考模型之间的输出以计算数据属于目标训练数据集的概率。本文中的参考模型被用来模拟目标模型，就像阴影模型一样。但他们不需要攻击模型。Hayes
    等人[[86](#bib.bib86)] 提出了攻击生成模型的方法。其思路是攻击者通过分类器输出的概率向量确定攻击者的数据集是否属于目标训练集。概率越高越可能来自目标训练集（他们选择了上层
    $n$ 大小）。在白盒中，分类器由目标模型构建。在黑盒中，他们使用通过查询目标模型获得的数据来通过 GAN 复现分类器。
- en: Hagestedt et al. [[81](#bib.bib81)] propose an MIA tailored to DNA methylation
    data, which may cause severe consequences. This attack relies on the likelihood
    ratio test and probability estimation to judge membership. Sablayrolles et al. [[196](#bib.bib196)]
    assume attackers know the loss incurred by the correct label in black-box settings.
    They use a probabilistic framework including Bayesian learning and noisy training
    to analyze membership. They find the optimal inference only depends on the loss
    function, not on the parameters. He et al. [[88](#bib.bib88)] extend model inversion
    attack into collaborative inference system. They find that one intermediate participant
    can recover an arbitrary input sample. They recover inference data by adopting
    regularized maximum likelihood estimation technique under white-box setting, inverse-network
    technique under black-box setting.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hagestedt 等人[[81](#bib.bib81)] 提出了一个针对 DNA 甲基化数据量身定制的 MIA，这可能会导致严重的后果。该攻击依赖于似然比检验和概率估计来判断成员资格。Sablayrolles
    等人[[196](#bib.bib196)] 假设攻击者知道在黑盒设置中正确标签所造成的损失。他们使用包括贝叶斯学习和噪声训练在内的概率框架来分析成员资格。他们发现，最佳推断只依赖于损失函数，而不依赖于参数。He
    等人[[88](#bib.bib88)] 将模型反演攻击扩展到协作推断系统中。他们发现一个中间参与者可以恢复任意输入样本。他们通过在白盒设置下采用正则化最大似然估计技术，在黑盒设置下采用逆网络技术来恢复推断数据。
- en: 'TABLE IV: Evaluation on model inversion attack. It presents how the “Step”
    in “Workflow” proceeds for each work in Figure [4](#S5.F4 "Figure 4 ‣ 5 Model
    Inversion Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey"),
    and its “Goal”, either MIA or PIA. We select one experimental “Dataset” in the
    works and the corresponding “Precision” achieved as well as the target “Model”.
    “Precision” is the accuracy of judgement. “Knowledge” denotes the acquisitions
    of attackers to the model, and “Application” is the applicable domain of the target
    model. “structured data” refers to any data in a fixed field within a record or
    file [[28](#bib.bib28)].'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 模型反演攻击的评估。它展示了图 [4](#S5.F4 "Figure 4 ‣ 5 Model Inversion Attack ‣ Towards
    Security Threats of Deep Learning Systems: A Survey") 中每项工作的“步骤”如何在“工作流程”中进行，以及其“目标”，即
    MIA 或 PIA。我们选择了这些工作中的一个实验“数据集”和相应的“精确度”以及目标“模型”。“精确度”是判断的准确性。“知识”表示攻击者对模型的获取，而“应用”是目标模型的适用领域。“结构化数据”指的是记录或文件中固定字段的数据 [[28](#bib.bib28)]。'
- en: '| Paper | Workflow | Goal | Precision | Dataset | Model | Knowledge | Application
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 工作流程 | 目标 | 精确度 | 数据集 | 模型 | 知识 | 应用 |'
- en: '| Step 1 | Step 2 | Step 3 | Step 4 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 1 | 步骤 2 | 步骤 3 | 步骤 4 |'
- en: '| Truex et al. [[221](#bib.bib221)] |  |  | $\checkmark$ | $\checkmark$ | MIA
    | 61.75% | MNIST [[120](#bib.bib120)] | DT | Black | image |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Truex 等人 [[221](#bib.bib221)] |  |  | $\checkmark$ | $\checkmark$ | MIA |
    61.75% | MNIST [[120](#bib.bib120)] | DT | 黑色 | 图像 |'
- en: '| Pyrgelis et al. [[184](#bib.bib184)] |  |  | $\checkmark$ | $\checkmark$
    | MIA | - | TFL | MLP | Black | structured data |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Pyrgelis 等人 [[184](#bib.bib184)] |  |  | $\checkmark$ | $\checkmark$ | MIA
    | - | TFL | MLP | 黑色 | 结构化数据 |'
- en: '| Shokri et al. [[203](#bib.bib203)] | $\checkmark$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | MIA | 51.7% | MNIST | DNN | Black | image |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Shokri 等人 [[203](#bib.bib203)] | $\checkmark$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | MIA | 51.7% | MNIST | DNN | 黑色 | 图像 |'
- en: '| Hayes et al. [[86](#bib.bib86)] | $\checkmark$ |  |  | $\checkmark$ | MIA
    | 58% | CIFAR-10 [[117](#bib.bib117)] | GAN | Black | image |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Hayes 等人 [[86](#bib.bib86)] | $\checkmark$ |  |  | $\checkmark$ | MIA | 58%
    | CIFAR-10 [[117](#bib.bib117)] | GAN | 黑色 | 图像 |'
- en: '| Long et al. [[144](#bib.bib144)] |  |  |  | $\checkmark$ | MIA | 93.36% |
    MNIST | NN | Black | image |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Long 等人 [[144](#bib.bib144)] |  |  |  | $\checkmark$ | MIA | 93.36% | MNIST
    | NN | 黑色 | 图像 |'
- en: '| Melis et al. [[153](#bib.bib153)] |  |  | $\checkmark$ | $\checkmark$ | MIA/PIA
    | - | FaceScrub | DNN | White | image |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Melis 等人 [[153](#bib.bib153)] |  |  | $\checkmark$ | $\checkmark$ | MIA/PIA
    | - | FaceScrub | DNN | 白色 | 图像 |'
- en: '| Liu et al. [[137](#bib.bib137)] | $\checkmark$ |  |  | $\checkmark$ | MIA
    | - | MNIST | GAN | White | image |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 [[137](#bib.bib137)] | $\checkmark$ |  |  | $\checkmark$ | MIA | -
    | MNIST | GAN | 白色 | 图像 |'
- en: '| Salem et al. [[198](#bib.bib198)] | $\checkmark$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | MIA | 75% | MNIST | CNN | Black | image |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Salem 等人 [[198](#bib.bib198)] | $\checkmark$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | MIA | 75% | MNIST | CNN | 黑色 | 图像 |'
- en: '| Ateniese et al. [[19](#bib.bib19)] | $\checkmark$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | PIA | 95% | - | SVM | White | speech |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Ateniese 等人 [[19](#bib.bib19)] | $\checkmark$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | PIA | 95% | - | SVM | 白色 | 语音 |'
- en: '| Buolamwini et al. [[38](#bib.bib38)] |  |  |  | $\checkmark$ | PIA | 79.6%
    | IJB-A [[6](#bib.bib6)] | DNN | Black | image |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Buolamwini 等人 [[38](#bib.bib38)] |  |  |  | $\checkmark$ | PIA | 79.6% |
    IJB-A [[6](#bib.bib6)] | DNN | 黑色 | 图像 |'
- en: '| Ganju et al. [[66](#bib.bib66)] | $\checkmark$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | PIA | 85% | MNIST | NN | White | image |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Ganju 等人 [[66](#bib.bib66)] | $\checkmark$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | PIA | 85% | MNIST | NN | 白色 | 图像 |'
- en: '| Hitaj et al. [[89](#bib.bib89)] | $\checkmark$ |  |  | $\checkmark$ | MIA
    | - | - | CNN | White | image |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Hitaj 等人 [[89](#bib.bib89)] | $\checkmark$ |  |  | $\checkmark$ | MIA | -
    | - | CNN | 白色 | 图像 |'
- en: '| Yang et al. [[256](#bib.bib256)] | ✓ |  | ✓ | ✓ | MIA | 78.3% | FaceScrub
    | CNN | Black | image |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等人 [[256](#bib.bib256)] | ✓ |  | ✓ | ✓ | MIA | 78.3% | FaceScrub | CNN
    | 黑色 | 图像 |'
- en: '| Nasr et al. [[161](#bib.bib161)] |  |  | ✓ | ✓ | MIA | 74.3% | CIFAR-100
    | DenseNet | White | image |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Nasr 等人 [[161](#bib.bib161)] |  |  | ✓ | ✓ | MIA | 74.3% | CIFAR-100 | DenseNet
    | 白色 | 图像 |'
- en: '| Sablayrolles et al. [[196](#bib.bib196)] |  |  |  | ✓ | MIA | 57.0% | CIFAR-100
    | ResNet | Black | image |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Sablayrolles 等人 [[196](#bib.bib196)] |  |  |  | ✓ | MIA | 57.0% | CIFAR-100
    | ResNet | 黑色 | 图像 |'
- en: 5.3 Property Inference Attack
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 属性推断攻击
- en: Property inference attack (PIA) mainly deduces properties in the training dataset.
    For instance, how many people have long hair or wear dresses in a generic gender
    classifier. Are there enough women or minorities in the dataset of common classifiers.
    The approach is largely the same for a membership inference attack. In this section,
    we only remark main differences between model inversion attacks.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 属性推断攻击（PIA）主要推断训练数据集中的属性。例如，在一个通用的性别分类器中，多少人有长发或穿裙子。数据集中是否有足够的女性或少数群体。对于成员推断攻击，方法大致相同。在本节中，我们只指出模型反演攻击之间的主要区别。
- en: Data Synthesis. In PIA, training datasets are classified by including or not
    including a specific attribute [[19](#bib.bib19)].
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 数据合成。在PIA中，训练数据集通过包含或不包含特定属性来分类[[19](#bib.bib19)]。
- en: Shadow Model Training. In PIA, shadow models are trained by training sets with
    or without a certain property. In [[19](#bib.bib19)][[66](#bib.bib66)], they used
    several training datasets with or without a certain property, then built corresponding
    shadow models to provide training data for a meta-classifier.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 影子模型训练。在PIA中，通过使用具有或不具有特定属性的训练集来训练影子模型。在[[19](#bib.bib19)][[66](#bib.bib66)]中，他们使用了几个具有或不具有特定属性的训练数据集，然后建立了相应的影子模型，为元分类器提供训练数据。
- en: Attack Model Training. Here, attack model is usually also a binary classifier.
    Ateniese et al. [[19](#bib.bib19)] proposed a white-box PIA method by training
    a meta-classifier. It took model features as input, and output whether the corresponding
    dataset contained a certain property. However, this approach did not work well
    on DNNs. To address this, Ganju et al. [[66](#bib.bib66)] mainly studied how to
    extract feature values of DNNs. The part of meta-classifier was similar to [[19](#bib.bib19)].
    Melis et al. [[153](#bib.bib153)] trained a binary classifier to judge dataset
    properties in collaborative learning, which took updated gradient values as input.
    Here the model is continuously updated, so attacker could analyze updated information
    at each stage to infer properties.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击模型训练。在这里，攻击模型通常也是一个二分类器。Ateniese等人[[19](#bib.bib19)]提出了一种通过训练元分类器的白盒PIA方法。它以模型特征作为输入，并输出相应的数据集是否包含某一特定属性。然而，这种方法在DNN上效果不佳。为了解决这个问题，Ganju等人[[66](#bib.bib66)]主要研究了如何提取DNN的特征值。元分类器的部分类似于[[19](#bib.bib19)]。Melis等人[[153](#bib.bib153)]在协作学习中训练了一个二分类器来判断数据集属性，该分类器以更新的梯度值作为输入。在这里，模型是持续更新的，因此攻击者可以在每个阶段分析更新的信息以推断属性。
- en: 5.4 Analysis of Model Inversion Attack
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 模型反演攻击分析
- en: 'We have surveyed 21 model inversion attack papers, and display 15 related papers
    in Table [IV](#S5.T4 "TABLE IV ‣ 5.2.4 Step 4: Membership Determination ‣ 5.2
    Membership Inference Attack ‣ 5 Model Inversion Attack ‣ Towards Security Threats
    of Deep Learning Systems: A Survey").'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '我们调查了21篇模型反演攻击论文，并在表[IV](#S5.T4 "TABLE IV ‣ 5.2.4 Step 4: Membership Determination
    ‣ 5.2 Membership Inference Attack ‣ 5 Model Inversion Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey")中展示了15篇相关论文。'
- en: Finding 4. There are not many papers (4/15) using shadow models to train the
    attack model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 发现4。使用影子模型来训练攻击模型的论文不多（4/15）。
- en: 'In our surveyed papers, shadow models (4/15) are used in both MIA (2/15) [[203](#bib.bib203)][[198](#bib.bib198)]
    and PIA (2/15) [[19](#bib.bib19)][[66](#bib.bib66)]. Although Shokri et al. [[203](#bib.bib203)]
    proposed the method of training shadow models to provide training data for attack
    model in a model inversion attack, few recent papers still train shadow models
    for attack. This is mainly because training shadow models requires much extra
    overhead, and the effect of directly training attack model is getting better.
    However, shadow models still have some advantages: 1) requiring no additional
    auxiliary information [[65](#bib.bib65)], such as assuming that higher confidence
    means higher probability from dataset. 2) providing true information as training
    data for attack model.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们调查的论文中，影子模型（4/15）被用于MIA（2/15）[[203](#bib.bib203)][[198](#bib.bib198)]和PIA（2/15）[[19](#bib.bib19)][[66](#bib.bib66)]。尽管Shokri等人[[203](#bib.bib203)]提出了通过训练影子模型为攻击模型提供训练数据的模型反演攻击方法，但最近的论文中很少有继续训练影子模型的。这主要是因为训练影子模型需要额外的开销，而直接训练攻击模型的效果越来越好。然而，影子模型仍有一些优势：1）不需要额外的辅助信息[[65](#bib.bib65)]，例如假设更高的信心意味着来自数据集的更高概率。2）为攻击模型提供真实的信息作为训练数据。
- en: Finding 5. Data synthesis is a commonly-used solution (8/15) in a model inversion
    attack, if there is a lack of valid data, and attackers want to save query costs.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 发现5。如果缺乏有效数据，攻击者希望节省查询成本，数据合成是一种常用的解决方案（8/15）。
- en: Data synthesis could generate data similar to the target dataset conveniently [[203](#bib.bib203)][[65](#bib.bib65)][[89](#bib.bib89)][[137](#bib.bib137)],
    without querying too many times. The synthesized data could be generated either
    by the statistical distribution of known training data, or a generative adversarial
    network. These data can effectively imitate the original data. It avoids too many
    queries to the target model and thereby lowers the perception by security mechanisms.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 数据合成可以方便地生成类似于目标数据集的数据[[203](#bib.bib203)][[65](#bib.bib65)][[89](#bib.bib89)][[137](#bib.bib137)]，无需过多查询。合成的数据可以通过已知训练数据的统计分布或生成对抗网络生成。这些数据可以有效地模拟原始数据。它避免了对目标模型的过多查询，从而降低了安全机制的感知。
- en: Finding 6. MIA is essentially a process that expresses the logical relations
    and data information contained in the trained model. It exposes many areas to
    the risk of information leakage.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 发现6. MIA本质上是一个表达训练模型中逻辑关系和数据信息的过程。它暴露了许多信息泄露的风险区域。
- en: In addition to centralized learning, attackers also implement model inversion
    attacks in federated learning [[161](#bib.bib161)][[153](#bib.bib153)]. Although
    the majority papers of information inference occur in image filed (13/15), some
    researchers also perform inference attack against DNA methylation data [[81](#bib.bib81)].
    This medical application could cause more serious damage to personal privacy.
    The technology of model inversion attack can also be used to recover an input
    sample [[65](#bib.bib65)][[88](#bib.bib88)], and detect a replay attack [[192](#bib.bib192)].
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 除了集中式学习，攻击者还在联邦学习中实施模型反演攻击[[161](#bib.bib161)][[153](#bib.bib153)]。尽管信息推断的绝大多数论文发生在图像领域（13/15），一些研究人员也对DNA甲基化数据进行推断攻击[[81](#bib.bib81)]。这种医学应用可能对个人隐私造成更严重的损害。模型反演攻击的技术还可以用于恢复输入样本[[65](#bib.bib65)][[88](#bib.bib88)]，并检测重放攻击[[192](#bib.bib192)]。
- en: Finding 7. Researchers pay more attention to individual membership information
    (12/15) than statistical property information (4/15).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 发现7. 研究人员对个体会员信息（12/15）的关注程度高于统计属性信息（4/15）。
- en: This is because membership inference now has a more general adaptation scenario,
    and it emerges earlier. The leakage of individual information is more serious
    than that of statistical information. Furthermore, MIA can get more information
    than PIA in one-time attack (just like training an attack model). A trained attack
    model can be applied to many records in MIA, but only a few properties in PIA.
    In [[19](#bib.bib19)], attackers want to know if their speech classifier was trained
    only with voices from people who speak Indian English. In [[66](#bib.bib66)],
    they try to find if some classifiers have enough women or minorities in training
    dataset. In [[38](#bib.bib38)], they are interested in the global distribution
    of skin color. In [[153](#bib.bib153)], they want to know the proportion between
    black and asian people.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为会员推断现在有更通用的适应场景，并且出现得更早。个体信息泄露比统计信息泄露更严重。此外，MIA在一次攻击中可以获得比PIA更多的信息（就像训练一个攻击模型一样）。一个训练好的攻击模型可以应用于MIA中的许多记录，但仅适用于PIA中的少数属性。在[[19](#bib.bib19)]中，攻击者想知道他们的语音分类器是否仅用说印度英语的人声进行训练。在[[66](#bib.bib66)]中，他们尝试找出某些分类器在训练数据集中是否包含足够的女性或少数群体。在[[38](#bib.bib38)]中，他们对肤色的全球分布感兴趣。在[[153](#bib.bib153)]中，他们想了解黑人员和亚洲人之间的比例。
- en: Finding 8. Heuristic methods (6/15) are simple, but effects are not very good.
    More studies still adopts the attack model (9/15).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 发现8. 启发式方法（6/15）简单，但效果不是很好。更多的研究仍采用攻击模型（9/15）。
- en: In heuristic methods, naively using probabilities is easy to implement, but
    barely works (0.5 precision and 0.54 recall) on MNIST dataset [[198](#bib.bib198)].
    Obtaining similar datasets usually needs to train a generative model [[86](#bib.bib86)][[137](#bib.bib137)][[89](#bib.bib89)].
    In attack model methods, attackers need to train an attack model [[184](#bib.bib184)][[19](#bib.bib19)].
    Shadow models [[203](#bib.bib203)][[198](#bib.bib198)][[19](#bib.bib19)] are proposed
    to provide datasets for the attack model, but increase training costs.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在启发式方法中，天真地使用概率容易实现，但在MNIST数据集上几乎无效（0.5的精度和0.54的召回率）[[198](#bib.bib198)]。获得类似的数据集通常需要训练生成模型[[86](#bib.bib86)][[137](#bib.bib137)][[89](#bib.bib89)]。在攻击模型方法中，攻击者需要训练攻击模型[[184](#bib.bib184)][[19](#bib.bib19)]。提出了影子模型[[203](#bib.bib203)][[198](#bib.bib198)][[19](#bib.bib19)]，以为攻击模型提供数据集，但增加了训练成本。
- en: 6 Poisoning Attack
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. 中毒攻击
- en: Poisoning attack seeks to downgrade deep learning systems’ prediction accuracy
    by polluting training data. Since it happens before the training phase, the caused
    contamination is usually inextricable by tuning the involved parameters or adopting
    alternative models.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 投毒攻击旨在通过污染训练数据来降低深度学习系统的预测准确性。由于它发生在训练阶段之前，因此造成的污染通常无法通过调整相关参数或采用替代模型来纠正。
- en: '![Refer to caption](img/b2025e17f2cda59f2567a442d229f81e.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b2025e17f2cda59f2567a442d229f81e.png)'
- en: 'Figure 5: Workflow of poisoning attack'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：投毒攻击的工作流程
- en: 6.1 Introduction of Poisoning Attack
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 投毒攻击介绍
- en: In the early age of machine learning, poisoning attack had been proposed as
    a non-trivial threat to the mainstream algorithms. It was originally proposed
    to decrease machine learning model accuracy. For instance, Bayes classifiers [[163](#bib.bib163)],
    Support Vector Machine (SVM) [[31](#bib.bib31)][[35](#bib.bib35)][[244](#bib.bib244)][[243](#bib.bib243)][[39](#bib.bib39)],
    Hierarchical Clustering [[32](#bib.bib32)], Logistic Regression [[152](#bib.bib152)]
    are all suffering degradation from data poisoning. Along with the broad use of
    deep learning, attackers have moved their attention to deep learning instead [[100](#bib.bib100)][[200](#bib.bib200)][[210](#bib.bib210)].
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的早期，投毒攻击已经被提出作为对主流算法的非平凡威胁。它最初是为了降低机器学习模型的准确性。例如，贝叶斯分类器 [[163](#bib.bib163)]、支持向量机
    (SVM) [[31](#bib.bib31)][[35](#bib.bib35)][[244](#bib.bib244)][[243](#bib.bib243)][[39](#bib.bib39)]、层次聚类
    [[32](#bib.bib32)]、逻辑回归 [[152](#bib.bib152)] 都受到数据投毒的影响。随着深度学习的广泛应用，攻击者将注意力转向了深度学习
    [[100](#bib.bib100)][[200](#bib.bib200)][[210](#bib.bib210)]。
- en: Adversary Model. Attackers can implement this attack with full knowledge (white-box)
    and limited knowledge (black-box). Usually, black-box attackers have no knowledge
    of the training dataset and the trained parameters, but they can know the feature
    set, the learning algorithm, and obtain a substitute dataset. Knowledge mainly
    means the understanding of training process, including training algorithms, model
    architectures, and so on. Capabilities of attackers refer to controlling over
    the training dataset. In particular, it discriminates how much new poisoned data
    attackers can insert, and whether they can alter labels in the original dataset
    and so on.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗模型。攻击者可以通过完全知识（白盒）和有限知识（黑盒）来实施此攻击。通常，黑盒攻击者对训练数据集和训练参数没有了解，但他们可以知道特征集、学习算法，并获得替代数据集。知识主要指对训练过程的理解，包括训练算法、模型架构等。攻击者的能力指的是对训练数据集的控制程度。特别地，它区分了攻击者可以插入多少新的毒化数据，以及是否可以修改原始数据集中的标签等。
- en: 'Attack Goal. There are two main purposes for poisoning the data. The original
    and intuitive purpose is to destroy the model’s availability by deviating its
    decision boundary. As a result, the poisoned model could not well represent the
    correct data and is prone to making wrong predictions. This is likely caused by
    *mislabeled data* (cf. Section [6.2.1](#S6.SS2.SSS1 "6.2.1 Manipulating Mislabeled
    Data ‣ 6.2 Poisoning Attack Approach ‣ 6 Poisoning Attack ‣ Towards Security Threats
    of Deep Learning Systems: A Survey")), whose labels are intentionally tampered
    by attackers, e.g., one photo with a cat in it is marked as dog. Recently, many
    researchers utilize poisoning attack to create a backdoor in the target model
    by inserting *confused data* (cf. Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Injecting
    Confused Data ‣ 6.2 Poisoning Attack Approach ‣ 6 Poisoning Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey")). The model may behave normally most
    of the time, but arouse wrong predictions with crafted data. With the pre-implanted
    backdoor and trigger data, one attacker can manipulate prediction results and
    launch further attacks.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '攻击目标。投毒数据主要有两个目的。原始且直观的目的是通过偏离模型的决策边界来破坏模型的可用性。结果是，毒化的模型无法很好地表示正确的数据，容易做出错误预测。这通常是由于*mislabeled
    data*（参见第[6.2.1](#S6.SS2.SSS1 "6.2.1 Manipulating Mislabeled Data ‣ 6.2 Poisoning
    Attack Approach ‣ 6 Poisoning Attack ‣ Towards Security Threats of Deep Learning
    Systems: A Survey")节），其标签被攻击者故意篡改，例如，一张包含猫的照片被标记为狗。最近，许多研究人员利用投毒攻击通过插入*confused
    data*（参见第[6.2.2](#S6.SS2.SSS2 "6.2.2 Injecting Confused Data ‣ 6.2 Poisoning Attack
    Approach ‣ 6 Poisoning Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey")节）在目标模型中创建一个后门。模型可能大部分时间正常工作，但在遇到精心制作的数据时会产生错误预测。通过预先植入的后门和触发数据，攻击者可以操控预测结果并发起进一步攻击。'
- en: 'TABLE V: Evaluation on poisoning attack. The data denotes an attacker needs
    to contaminate how many percent of training data “Poison Percent” and achieves
    how many “Success Rate” under specific “Dataset”. “Model” indicates the attacked
    model. “Timeliness” denotes whether the poison attack is in an online or offline
    setting. “Damage” means how many predictions can be impacted. Attackers may possess
    two different “Knowledge”, either black-box or white-box, and make poisoned model
    predict as expected, i.e., “Targeted”, or not. “structured data” is the same as
    Table [IV](#S5.T4 "TABLE IV ‣ 5.2.4 Step 4: Membership Determination ‣ 5.2 Membership
    Inference Attack ‣ 5 Model Inversion Attack ‣ Towards Security Threats of Deep
    Learning Systems: A Survey"). “LR” is linear regression. “OLR” is online logistic
    regression. “SLHC” is single-linkage hierarchical clustering.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V：污染攻击评估。数据表示攻击者需要污染多少百分比的训练数据“污染百分比”并在特定“数据集”下达到多少“成功率”。“模型”表示被攻击的模型。“时效性”表示污染攻击是在在线还是离线环境中进行。“损害”表示有多少预测受到影响。攻击者可能拥有两种不同的“知识”，即黑盒或白盒，并使污染模型按预期进行预测，即“定向”，或不进行预测。“结构化数据”与表
    [IV](#S5.T4 "TABLE IV ‣ 5.2.4 Step 4: Membership Determination ‣ 5.2 Membership
    Inference Attack ‣ 5 Model Inversion Attack ‣ Towards Security Threats of Deep
    Learning Systems: A Survey") 相同。“LR” 是线性回归。“OLR” 是在线逻辑回归。“SLHC” 是单链接层次聚类。'
- en: '| Paper | Success Rate | Dataset | Poison Percent | Model | Timeliness | Damage
    | Knowledge | Targeted | Application |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 成功率 | 数据集 | 污染百分比 | 模型 | 时效性 | 损害 | 知识 | 定向 | 应用 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Xiao et al. [[242](#bib.bib242)] | 20% | 11944 files | 5% | LASSO | offline
    | - | Black | No | malware |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Xiao 等人 [[242](#bib.bib242)] | 20% | 11944 文件 | 5% | LASSO | 离线 | - | 黑色
    | 否 | 恶意软件 |'
- en: '| Muñoz-González et al. [[158](#bib.bib158)] | 25% | MNIST | 15% | CNN | offline
    | 30% error | Black | No | image, malware |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Muñoz-González 等人 [[158](#bib.bib158)] | 25% | MNIST | 15% | CNN | 离线 | 30%
    错误 | 黑色 | 否 | 图像，恶意软件 |'
- en: '| Jagielski et al. [[100](#bib.bib100)] | 75% | Health care dataset | 20% |
    LASSO | offline | 75% error | Black | No | structured data |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Jagielski 等人 [[100](#bib.bib100)] | 75% | 健康护理数据集 | 20% | LASSO | 离线 | 75%
    错误 | 黑色 | 否 | 结构化数据 |'
- en: '| Alfeld et al. [[16](#bib.bib16)] | - | - | - | LR | offline | - | White |
    Yes | - |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Alfeld 等人 [[16](#bib.bib16)] | - | - | - | LR | 离线 | - | 白色 | 是 | - |'
- en: '| Shafahi et al. [[200](#bib.bib200)] | 60% | CIFAR-10 | 5% | DNN | offline
    | 20% error | White | Yes | image |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Shafahi 等人 [[200](#bib.bib200)] | 60% | CIFAR-10 | 5% | DNN | 离线 | 20% 错误
    | 白色 | 是 | 图像 |'
- en: '| Wang et al. [[235](#bib.bib235)] | 90% | MNIST | 100% | OLR | online | -
    | White | Both | image |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 [[235](#bib.bib235)] | 90% | MNIST | 100% | OLR | 在线 | - | 白色 | 两者
    | 图像 |'
- en: '| Biggio et al. [[32](#bib.bib32)] | - | MNIST | 1% | SLHC | offline | - |
    White | Yes | image, malware |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Biggio 等人 [[32](#bib.bib32)] | - | MNIST | 1% | SLHC | 离线 | - | 白色 | 是 |
    图像，恶意软件 |'
- en: '| BadNets [[76](#bib.bib76)] | 99% | MNIST | - | CNN | offline | - | White
    | Both | image |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| BadNets [[76](#bib.bib76)] | 99% | MNIST | - | CNN | 离线 | - | 白色 | 两者 | 图像
    |'
- en: '| Yao et al. [[258](#bib.bib258)] | 96% | MNIST | 0.15% | CNN | offline | -
    | White | Yes | image |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Yao 等人 [[258](#bib.bib258)] | 96% | MNIST | 0.15% | CNN | 离线 | - | 白色 | 是
    | 图像 |'
- en: '| Liu et al. [[139](#bib.bib139)] | - | MNIST | 4% | GNN | offline | 50% error
    | White | Yes | image |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 [[139](#bib.bib139)] | - | MNIST | 4% | GNN | 离线 | 50% 错误 | 白色 | 是
    | 图像 |'
- en: 'Workflow. Figure [5](#S6.F5 "Figure 5 ‣ 6 Poisoning Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey") shows a common workflow of poisoning
    attack. Basically, this attack is accomplished by two methods: mislabel original
    data, and craft confused data. The poisoned data then enters into the original
    data and subverts the training process, leading to greatly degraded prediction
    capability or a backdoor implanted into the model. More specifically, mislabeled
    data is yielded by selecting certain records of interest and flipping their labels.
    Confused data is crafted by embedding special features that can be learnt by the
    model which are actually not the essence of target objects. These special features
    can serve as a trigger, incurring a wrong classification.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '工作流程。图 [5](#S6.F5 "Figure 5 ‣ 6 Poisoning Attack ‣ Towards Security Threats
    of Deep Learning Systems: A Survey") 展示了污染攻击的常见工作流程。基本上，这种攻击通过两种方法实现：伪造原始数据标签和制作混淆数据。污染数据进入原始数据中，颠覆了训练过程，导致预测能力显著下降或在模型中植入后门。更具体地说，伪造标签的数据通过选择特定感兴趣的记录并翻转它们的标签来产生。混淆数据则通过嵌入模型可以学习的特殊特征来制作，这些特征实际上不是目标对象的本质。这些特殊特征可以作为触发器，引起错误分类。'
- en: 6.2 Poisoning Attack Approach
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 污染攻击方法
- en: 6.2.1 Manipulating Mislabeled Data
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 操作误标数据
- en: Learning model usually experiences training under labeled data in advance. Attackers
    may get access to a dataset, and change a correct label to wrong. Mislabeled data
    could push the decision boundary of the classifier significantly to incorrect
    zones, thus reducing its classification accuracy. Muñoz-González et al. [[158](#bib.bib158)]
    undertook a poisoning attack towards multi-class problem based on back-gradient
    optimization. It calculated gradient by automatic differentiation and reversed
    the learning process to reduce attack complexity. This attack is resultful for
    spam filtering, malware detection and handwirtten digit recognition.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 学习模型通常在标记数据上进行训练。攻击者可能会获取数据集，并将正确的标签更改为错误的标签。被标记错误的数据可能会显著地将分类器的决策边界推向不正确的区域，从而降低其分类准确性。Muñoz-González
    等人 [[158](#bib.bib158)] 对多类问题进行了基于反向梯度优化的毒化攻击。他们通过自动微分计算梯度，并反转学习过程以降低攻击复杂度。这种攻击在垃圾邮件过滤、恶意软件检测和手写数字识别方面效果显著。
- en: Xiao et al. [[244](#bib.bib244)] adjusted a training dataset to attack SVM by
    flipping labels of records. They proposed an optimized framework for finding the
    label flips which maximizes classification errors, and thus reducing the accuracy
    of classifier successfully. Biggio et al. [[32](#bib.bib32)] used obfuscation
    attack to maximally worsen clustering results, where they relied on heuristic
    algorithms to find the optimal attack strategy. Alfeld et al. [[16](#bib.bib16)]
    added optimal special records into the training dataset to drive predictions in
    a certain direction. They presented a framework to encode an attacker’s desires
    and constraints under linear autoregressive models. Jagielski et al. [[100](#bib.bib100)]
    could manipulate datasets and algorithms to influence linear regression models.
    They also introduced a fast statistical attack which only required limited knowledge
    of training process.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao 等人 [[244](#bib.bib244)] 通过翻转记录的标签调整训练数据集，以攻击支持向量机（SVM）。他们提出了一种优化框架，用于找到最大化分类错误的标签翻转，从而成功地降低分类器的准确性。Biggio
    等人 [[32](#bib.bib32)] 使用模糊攻击来最大化恶化聚类结果，他们依赖启发式算法来寻找最佳攻击策略。Alfeld 等人 [[16](#bib.bib16)]
    向训练数据集中添加了最佳特殊记录，以驱动预测朝向特定方向。他们提出了一个框架，用于在线性自回归模型下编码攻击者的愿望和约束。Jagielski 等人 [[100](#bib.bib100)]
    能够操控数据集和算法以影响线性回归模型。他们还介绍了一种快速统计攻击，只需对训练过程有有限的了解。
- en: Liu et al. [[134](#bib.bib134)] poison stochastic multi-armed bandit algorithms
    through convex optimization based attacks. They can force the bandit algorithm
    to pull the target arm with a high probability through a slight operation on the
    reward in the data. Zhang et al. [[267](#bib.bib267)] propose a data poisoning
    strategy against knowledge graph embedding technique. Attackers can effectively
    manipulate the plausibility of targeted facts in the knowledge graph by adding
    or deleting facts on the knowledge graph. Zügner et al. [[277](#bib.bib277)] research
    the poisoning attack on graph neural network (GNN). They generate poisoned data
    targeting the node’s features and the graph structure. They use incremental calculation
    to solve the potential discrete domain problem. Liu et al. [[139](#bib.bib139)]
    propose a data poisoning attack framework on graph-based semi-supervised learning.
    They adopt a gradient-based algorithm and a probabilistic solver to settle two
    constraints in poisoning tasks.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Liu 等人 [[134](#bib.bib134)] 通过凸优化基础的攻击对随机多臂赌博机算法进行毒化。他们可以通过对数据中的奖励进行轻微操作，迫使赌博机算法以较高的概率选择目标臂。Zhang
    等人 [[267](#bib.bib267)] 提出了针对知识图谱嵌入技术的数据毒化策略。攻击者可以通过在知识图谱中添加或删除事实，来有效地操控目标事实的可信度。Zügner
    等人 [[277](#bib.bib277)] 研究了对图神经网络（GNN）的毒化攻击。他们生成了针对节点特征和图结构的毒化数据，并使用增量计算解决潜在的离散域问题。Liu
    等人 [[139](#bib.bib139)] 提出了基于图的半监督学习的数据毒化攻击框架。他们采用了基于梯度的算法和概率求解器来解决毒化任务中的两个约束。
- en: The major research focuses on an offline environment where the classifier is
    trained on fixed inputs. However, training also happens as data arrives sequentially
    in a stream, i.e., in an online setting. Wang et al. [[235](#bib.bib235)] conducted
    poisoning attacks for online learning. They formalized the problem into semi-online
    and fully-online, with three attack algorithms of incremental, interval and teach-and-reinforce.
    Except for one-party poisoning, Mahloujifar et al. [[150](#bib.bib150)] study
    a online $(k,p)$-poisoning attack, which applies to multi-party learning processes.
    The adversary controls $k$ parties, and the poisoned data is still $(1-p)$-close
    to the correct data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 主要研究集中在离线环境中，其中分类器在固定输入上进行训练。然而，训练也发生在数据以流的形式顺序到达的在线设置中。Wang 等人 [[235](#bib.bib235)]
    进行了在线学习的毒化攻击。他们将问题形式化为半在线和完全在线，并提出了增量、间隔和教学强化三种攻击算法。除了单方毒化之外，Mahloujifar 等人 [[150](#bib.bib150)]
    研究了一种适用于多方学习过程的在线 $(k,p)$-毒化攻击。对手控制 $k$ 个参与方，而毒化数据仍然与正确数据 $(1-p)$-接近。
- en: 6.2.2 Injecting Confused Data
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 注入混淆数据
- en: Learning algorithms elicit representative features from a large amount of information
    for learning and training. However, if attackers submit crafted data with special
    features, the classifier may learn fooled features. For example, marking figures
    with number “6” as a turn left sign and putting them into the dataset, then images
    with a bomb may be identified as a turn-left sign, even if it is in fact a STOP
    sign.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 学习算法从大量信息中提取代表性特征用于学习和训练。然而，如果攻击者提交带有特殊特征的伪造数据，分类器可能会学习到虚假的特征。例如，将数字“6”标记为左转标志并将其放入数据集中，那么带有炸弹的图像可能会被识别为左转标志，即使它实际上是STOP标志。
- en: Xiao et al. [[242](#bib.bib242)] directly investigate the robustness of popular
    feature selection algorithms under poisoning attack. They reduced LASSO to almost
    random choices of feature sets by inserting less than 5% poisoned training samples.
    Shafahi et al. [[200](#bib.bib200)] find a specific test instance to control the
    behavior of classifier with backdoor, without any access to data collection or
    labeling process. They proposed a watermarking strategy and trained a classifier
    with multiple poisoned instances. Low-opacity watermark of the target instance
    is added to poisoned instances to allow overlap of some indivisible features.
    Liu et al. [[141](#bib.bib141)] propose a trojaning attack. Attackers first download
    a public model, then generate a trojan trigger by inversing the neural network
    and next retrain the model to inject malicious behaviors. Then they republish
    the mutated neural network with a trojan trigger. This attack is effective on
    face, speech, age, sentence attitude recognition. Xi et al. [[240](#bib.bib240)]
    propose graph-oriented GNN poisoning attack. The triggers are specific sub-graphs,
    including both topological structures and descriptive features.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao 等人 [[242](#bib.bib242)] 直接研究了在毒化攻击下流行特征选择算法的鲁棒性。他们通过插入不到 5% 的毒化训练样本，将 LASSO
    减少到几乎是随机的特征集选择。Shafahi 等人 [[200](#bib.bib200)] 找到一个特定的测试实例来控制带有后门的分类器行为，而无需访问数据收集或标注过程。他们提出了一种水印策略，并用多个毒化实例训练分类器。目标实例的低透明度水印被添加到毒化实例中，以允许一些不可分割特征的重叠。Liu
    等人 [[141](#bib.bib141)] 提出了一个特洛伊攻击。攻击者首先下载一个公开模型，然后通过逆向神经网络生成特洛伊触发器，接着重新训练模型以注入恶意行为。然后，他们重新发布带有特洛伊触发器的变异神经网络。该攻击在面部、语音、年龄、句子态度识别上效果显著。Xi
    等人 [[240](#bib.bib240)] 提出了图形导向的 GNN 毒化攻击。触发器是特定的子图，包括拓扑结构和描述特征。
- en: 6.2.3 Attacks in Transfer Learning.
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 转移学习中的攻击。
- en: Gu et al. [[76](#bib.bib76)] introduce the threat of poisoning attack in an
    outsourced training setting. The user model will also be poisoned if he performs
    transfer learning on the backdoor model (BadNet) provided by the adversary. Yao
    et al. [[258](#bib.bib258)] propose latent backdoors to insert a backdoor trigger
    into a teacher model in transfer learning. At the teacher side, attackers inject
    backdoor data related to a target class $y$. When the student side downloads the
    infected teacher model, the transfer learning can silently activate the latent
    backdoor into a live backdoor, and form an infected student model. Kurita et al. [[119](#bib.bib119)]
    find downloading untrusted pre-trained weights poses a security threat. Attackers
    construct a weight poisoning attack, and the user model will also carry a backdoor
    after fine-tuning the pre-trained injected weights. This allows the attacker to
    manipulate model prediction.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Gu 等人 [[76](#bib.bib76)] 介绍了在外包训练环境中的中毒攻击威胁。如果用户在对抗者提供的后门模型（BadNet）上进行迁移学习，那么用户模型也会被中毒。Yao
    等人 [[258](#bib.bib258)] 提出了潜在后门技术，在迁移学习中将后门触发器插入教师模型。在教师端，攻击者注入与目标类别 $y$ 相关的后门数据。当学生端下载了被感染的教师模型时，迁移学习可能会悄悄地将潜在后门激活为实际后门，并形成一个被感染的学生模型。Kurita
    等人 [[119](#bib.bib119)] 发现下载不可信的预训练权重会带来安全威胁。攻击者构建了权重中毒攻击，经过微调的预训练注入权重后，用户模型也会携带后门。这允许攻击者操控模型预测。
- en: 6.2.4 Attacks in Federated Learning.
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.4 联邦学习中的攻击。
- en: Some recent articles have begun to study how to conduct backdoor attacks in
    federated learning [[23](#bib.bib23)][[212](#bib.bib212)]. In federated learning,
    there may be one or several attackers who can participate in model training. Their
    goal is to implant a specific backdoor in the final trained model. In [[23](#bib.bib23)][[212](#bib.bib212)],
    attackers try to strictly limit the loss items to avoid anomaly detection, and
    boost maliciously updated values to reserve backdoor. Bhagoji et al. [[29](#bib.bib29)]
    introduce the technology of alternating minimization with distance constraints
    to avoid the updated value statistics anomaly detection. Xie et al. [[246](#bib.bib246)]
    propose distributed backdoor attacks. They decompose a trigger into several small
    patterns. Each attacker implants a small pattern into the final model. Then the
    complete trigger can also attack successfully in the final model. Fang et al. [[63](#bib.bib63)]
    assume the attacker manipulates local model parameters on compromised client devices,
    resulting in a large testing error in the global model.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最近的文章已经开始研究如何在联邦学习中进行后门攻击 [[23](#bib.bib23)][[212](#bib.bib212)]。在联邦学习中，可能有一个或多个攻击者参与模型训练。他们的目标是将特定的后门植入最终训练好的模型中。在
    [[23](#bib.bib23)][[212](#bib.bib212)] 中，攻击者试图严格限制损失项以避免异常检测，并恶意提升更新值以保留后门。Bhagoji
    等人 [[29](#bib.bib29)] 引入了带有距离约束的交替最小化技术，以避免更新值统计的异常检测。Xie 等人 [[246](#bib.bib246)]
    提出了分布式后门攻击。他们将触发器分解为几个小模式。每个攻击者将一个小模式植入最终模型中。然后，完整的触发器也能在最终模型中成功攻击。Fang 等人 [[63](#bib.bib63)]
    假设攻击者在被攻陷的客户端设备上操控本地模型参数，从而导致全球模型的测试误差较大。
- en: 6.3 Analysis of Poisoning Attack
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 中毒攻击的分析
- en: 'In this Section, we investigate 20 representative poisoning attack papers in
    detail, and compare 10 of them in Table [V](#S6.T5 "TABLE V ‣ 6.1 Introduction
    of Poisoning Attack ‣ 6 Poisoning Attack ‣ Towards Security Threats of Deep Learning
    Systems: A Survey").'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们详细调查了20篇代表性的中毒攻击论文，并在表 [V](#S6.T5 "TABLE V ‣ 6.1 Introduction of Poisoning
    Attack ‣ 6 Poisoning Attack ‣ Towards Security Threats of Deep Learning Systems:
    A Survey") 中对其中10篇进行了比较。'
- en: Finding 9. Poisoning attacks have been researched in extensive fields. In our
    surveyed papers, 3(/20) papers studied how to insert backdoors in transfer learning
    settings. 4(/20) papers researched implanting backdoors in federated learning
    settings. 2(/20) papers studied online poisoning attacks.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 9. 中毒攻击已在广泛领域中进行研究。在我们调查的论文中，3(/20) 篇研究了如何在迁移学习设置中插入后门。4(/20) 篇论文研究了在联邦学习设置中植入后门。2(/20)
    篇论文研究了在线中毒攻击。
- en: With the wide application of deep learning in multiple fields, poisoning attacks
    have also been studied in different fields. Liu et al. [[134](#bib.bib134)] apply
    poisoning attacks to multi-armed bandit algorithms. Zhang et al. [[267](#bib.bib267)]
    attack knowledge graph embedding technique. Zügner et al. [[277](#bib.bib277)]
    and Xi et al. [[240](#bib.bib240)] poison graph neural networks. Liu et al. [[139](#bib.bib139)]
    attack graph-based semi-supervised learning. Poisoning attacks for online learning
    have been studied in [[235](#bib.bib235)][[150](#bib.bib150)]. In online setting,
    attackers feed poisonous data into the models gradually. This makes attackers
    consider more factors such as the order of fed data, the evasiveness of poisonous
    data. Some attacks [[76](#bib.bib76)][[258](#bib.bib258)][[119](#bib.bib119)]
    inject backdoors into pre-trained model or teacher model. When users perform transfer
    learning through a poisonous model, the backdoors will be embedded into their
    models accordingly. Poisoning attacks also exist in federated learning [[23](#bib.bib23)][[212](#bib.bib212)][[29](#bib.bib29)][[246](#bib.bib246)].
    Attackers need to upload malicious updated values, bypass the anomaly detection,
    and inject the backdoor into the final model. These studies also mean that many
    current learning algorithms are not robust and vulnerable to poisoning attack.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习在多个领域的广泛应用，污染攻击也在不同领域得到了研究。刘等人[[134](#bib.bib134)]将污染攻击应用于多臂赌博机算法。张等人[[267](#bib.bib267)]攻击知识图谱嵌入技术。Zügner等人[[277](#bib.bib277)]和Xi等人[[240](#bib.bib240)]对图神经网络进行污染。刘等人[[139](#bib.bib139)]攻击基于图的半监督学习。在线学习中的污染攻击已在[[235](#bib.bib235)][[150](#bib.bib150)]中得到研究。在在线设置中，攻击者逐渐将有毒数据输入模型。这使得攻击者需要考虑更多因素，如数据输入的顺序和有毒数据的隐蔽性。一些攻击[[76](#bib.bib76)][[258](#bib.bib258)][[119](#bib.bib119)]向预训练模型或教师模型注入后门。当用户通过有毒模型进行迁移学习时，后门将相应地嵌入到他们的模型中。联邦学习中也存在污染攻击[[23](#bib.bib23)][[212](#bib.bib212)][[29](#bib.bib29)][[246](#bib.bib246)]。攻击者需要上传恶意的更新值，绕过异常检测，并将后门注入最终模型。这些研究还表明，许多当前的学习算法并不健壮，容易受到污染攻击。
- en: Finding 10. There are more papers using confused data to inject backdoors into
    the model. Totally, 10(/20) papers use confused data to implant a backdoor. In
    2019, the ratio increases up to 8/13.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 10。越来越多的论文使用混淆数据向模型注入后门。总的来说，10(/20)篇论文使用混淆数据植入后门。在2019年，这一比例上升至8/13。
- en: Making mistakes imperceptible is more difficult and harmful than making misclassification
    for a model. A backdoor is such an imperceptible mistake. The model performs well
    under normal functions, while it opens the door for attackers when they need it.
    In recent years, with the development of technology, more research has focused
    on backdoor poisoning attacks [[258](#bib.bib258)][[76](#bib.bib76)][[141](#bib.bib141)][[23](#bib.bib23)][[212](#bib.bib212)].
    Backdoor attacks are more difficult to detect, and the manipulation to the model
    is also stronger.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使错误不可察觉比让模型出现误分类更困难也更具危害。后门就是一种不可察觉的错误。模型在正常功能下表现良好，但在攻击者需要时，它会为攻击者打开大门。近年来，随着技术的发展，更多的研究集中在后门污染攻击上[[258](#bib.bib258)][[76](#bib.bib76)][[141](#bib.bib141)][[23](#bib.bib23)][[212](#bib.bib212)]。后门攻击更难以检测，且对模型的操控也更强。
- en: Finding 11. Pre-trained models from unknown sources still suffer from poisoning
    attacks.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 11。来自未知来源的预训练模型仍然会遭受污染攻击。
- en: The performance of learning model is largely dependent on the quality of training
    data. High quality data is commonly acknowledged as being comprehensive, unbiased,
    and representative. In [[258](#bib.bib258)][[119](#bib.bib119)][[76](#bib.bib76)],
    researchers find that the pre-trained model can transmit its triggers to uses’
    training model. Even if the user has a high-quality dataset, as long as the pre-trained
    model is trained on a low-quality dataset or injected with a backdoor, the final
    model is still at risk of being poisoned.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 学习模型的性能在很大程度上依赖于训练数据的质量。高质量的数据通常被认为是全面的、公正的和有代表性的。在[[258](#bib.bib258)][[119](#bib.bib119)][[76](#bib.bib76)]中，研究人员发现预训练模型可以将其触发器传递给用户的训练模型。即使用户拥有高质量的数据集，只要预训练模型是在低质量数据集上训练的或注入了后门，最终模型仍然有被污染的风险。
- en: 7 Adversarial Attack
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 对抗攻击
- en: '![Refer to caption](img/33917291415bb0f89918dc3ea934fa3f.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/33917291415bb0f89918dc3ea934fa3f.png)'
- en: 'Figure 6: Workflow of adversarial attack'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：对抗攻击的工作流程
- en: Similar to poisoning attack, adversarial attack also makes a model classify
    a malicious sample wrongly. Their difference is that poisoning attack inserts
    malicious samples into the training data, directly contaminating the model, while
    adversarial attack leverages adversarial examples to exploit the weaknesses of
    the model and gets a wrong prediction result.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于中毒攻击，对抗攻击也会使模型错误地分类恶意样本。他们的区别在于，中毒攻击将恶意样本插入到训练数据中，直接污染模型，而对抗攻击利用对抗样本来利用模型的弱点，从而得到错误的预测结果。
- en: 7.1 Introduction of Adversarial Attack
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 对抗攻击的介绍
- en: Adversarial attack adds unperceived perturbations to normal samples during the
    prediction process, and then produces adversarial examples (AEs). This is an exploratory
    attack and violates the availability of a model. It can be used in many fields,
    e.g., image classification [[213](#bib.bib213)][[74](#bib.bib74)][[43](#bib.bib43)],
    speech recognition [[73](#bib.bib73)][[262](#bib.bib262)], text processing [[67](#bib.bib67)][[268](#bib.bib268)][[199](#bib.bib199)][[123](#bib.bib123)],
    and malware detection [[95](#bib.bib95)][[177](#bib.bib177)][[179](#bib.bib179)][[116](#bib.bib116)],
    particularly widespread in image classification. They can deceive the trained
    model but look nothing unusual to humans. That is to say, AEs need to both fool
    the classifier and be imperceptible to humans. For an image, the added perturbation
    is usually tuned by minimizing the distance between the original and adversarial
    examples. For a piece of speech or text, the perturbation should not change the
    original meaning or context. In the field of malware detection, AEs need to avoid
    being detected by models. Adversarial attack can be classified into the targeted
    attack and untargeted attack. The former requires adversarial examples to be misclassified
    as a specific label, while the latter desires a wrong prediction, no matter what
    it will be recognized as.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击在预测过程中向正常样本添加不可感知的扰动，然后生成对抗样本（AEs）。这是一种探索性攻击，违反了模型的可用性。它可以用于许多领域，例如，图像分类
    [[213](#bib.bib213)][[74](#bib.bib74)][[43](#bib.bib43)]、语音识别 [[73](#bib.bib73)][[262](#bib.bib262)]、文本处理
    [[67](#bib.bib67)][[268](#bib.bib268)][[199](#bib.bib199)][[123](#bib.bib123)]
    和恶意软件检测 [[95](#bib.bib95)][[177](#bib.bib177)][[179](#bib.bib179)][[116](#bib.bib116)]，特别是在图像分类领域非常普遍。它们可以欺骗训练好的模型，但对人类来说没有任何异常。也就是说，对抗样本需要既能欺骗分类器，又对人类不可察觉。对于图像来说，添加的扰动通常通过最小化原始样本和对抗样本之间的距离来调整。对于一段语音或文本，扰动不应改变原始的含义或上下文。在恶意软件检测领域，对抗样本需要避免被模型检测到。对抗攻击可以分为有针对性的攻击和无针对性的攻击。前者要求对抗样本被错误分类为特定标签，而后者则期望预测错误，不管它将被识别为什么。
- en: Adversary Model. In adversarial attack, black-box setting means the attacker
    cannot directly calculate the required gradients (such as FGSM [[74](#bib.bib74)])
    or solve optimization functions (such as C&W [[43](#bib.bib43)]) from the target
    model. but attackers in white-box setting can do these. Black-box attackers can
    know the model architecture and hyperparameters to train a substitute model. They
    can also query the target black-box model and obtain outputs with predicted label
    and confidence scores to estimate gradients.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗模型。在对抗攻击中，黑箱设置意味着攻击者不能直接计算所需的梯度（如 FGSM [[74](#bib.bib74)]）或解决优化函数（如 C&W [[43](#bib.bib43)]）从目标模型中。但是白箱设置中的攻击者可以做到这些。黑箱攻击者可以了解模型架构和超参数，以训练替代模型。他们还可以查询目标黑箱模型，获得带有预测标签和置信度分数的输出，以估计梯度。
- en: 'Workflow. Figure [6](#S7.F6 "Figure 6 ‣ 7 Adversarial Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey") depicts the general workflow for
    an adversarial attack. In white-box setting, attackers could directly calculate
    gradients [[74](#bib.bib74)][[15](#bib.bib15)][[57](#bib.bib57)] or solve optimization
    functions [[43](#bib.bib43)][[48](#bib.bib48)][[87](#bib.bib87)] to find perturbations
    on original samples (Step 3). In black-box setting, attackers obtain information
    by querying the target model many times (Step 1). Then they could train a substitute
    model to perform a white-box attack [[173](#bib.bib173)][[174](#bib.bib174)] (Step
    2.1), or estimate gradients to search for AEs [[98](#bib.bib98)] (Step 2.2).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '工作流程。图 [6](#S7.F6 "Figure 6 ‣ 7 Adversarial Attack ‣ Towards Security Threats
    of Deep Learning Systems: A Survey") 描述了对抗攻击的一般工作流程。在白盒设置中，攻击者可以直接计算梯度 [[74](#bib.bib74)][[15](#bib.bib15)][[57](#bib.bib57)]
    或解决优化函数 [[43](#bib.bib43)][[48](#bib.bib48)][[87](#bib.bib87)] 来寻找原始样本上的扰动（步骤
    3）。在黑盒设置中，攻击者通过多次查询目标模型来获取信息（步骤 1）。然后，他们可以训练一个替代模型来进行白盒攻击 [[173](#bib.bib173)][[174](#bib.bib174)]（步骤
    2.1），或估计梯度以搜索对抗样本 [[98](#bib.bib98)]（步骤 2.2）。'
- en: In addition to deceiving the classification model, AEs should carry minimal
    perturbations that evade the awareness of human. Generally, the distance between
    normal and adversarial sample can be measured by $L_{p}$ Distance (or Minkowski
    Distance), e.g., $L_{0}$, $L_{1}$, $L_{2}$ and $L_{\infty}$.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 除了欺骗分类模型外，对抗样本还应尽可能减少扰动，以避免被人类察觉。通常，正常样本与对抗样本之间的距离可以通过 $L_{p}$ 距离（或 Minkowski
    距离）来度量，例如 $L_{0}$、$L_{1}$、$L_{2}$ 和 $L_{\infty}$。
- en: '|  | $\begin{split}L_{p}(x,y)&amp;=(\sum_{i=1}^{n}&#124;x^{i}-y^{i}&#124;^{p})^{\frac{1}{p}}\\
    x=\{x^{1},x^{2},...,&amp;x^{n}\},\;y=\{y^{1},y^{2},...,y^{n}\}\end{split}$ |  |
    (2) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}L_{p}(x,y)&=(\sum_{i=1}^{n}&#124;x^{i}-y^{i}&#124;^{p})^{\frac{1}{p}}\\
    x=\{x^{1},x^{2},...,&amp;x^{n}\},\;y=\{y^{1},y^{2},...,y^{n}\}\end{split}$ |  |
    (2) |'
- en: 7.2 Adversarial Attack Approach
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 对抗攻击方法
- en: Since the main development of adversarial attack is in the field of image classification [[213](#bib.bib213)][[74](#bib.bib74)][[43](#bib.bib43)],
    we will introduce more related work on image using CNN, and supplement research
    on other fields or other models at the end of this section.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对抗攻击的主要发展集中在图像分类领域[[213](#bib.bib213)][[74](#bib.bib74)][[43](#bib.bib43)]，我们将介绍更多使用
    CNN 的相关图像工作，并在本节末补充其他领域或模型的研究。
- en: 7.2.1 White-box attacks in the image classification field
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1 图像分类领域的白盒攻击
- en: The white-box in image field is the main setting of early adversarial attack
    research. We introduce and compare formulas used to generate adversarial examples.
    First, we define that $F:\mathbb{R}^{n}\longrightarrow\{1\dots k\}$ is the classifier
    of a model to map image value vectors to a class label. $Z(\cdot)$ is the output
    of second-to-last layer, usually indicates class probability. $Z(\cdot)_{t}$ is
    the probability of $t$-th class. $Loss$ function describes the loss of input and
    output under classifier $F$, and we set $Loss(x,F(x))=0$. $\delta$ is the perturbation.
    $\left\|\delta\right\|_{p}$ is the $L_{p}$-norm of $\delta$. $x=\{x^{1},x^{2},...,x^{n}\}$
    is the original sample, $x^{i}$ is the pixel or element in sample where $x^{i}\in
    x,1\leqslant i\leqslant n$. $x_{i}$ is sample of the $i$-th iteration, usually
    $x_{0}=x$. $x+\delta$ is the adversarial sample. Here, $x\in[0,1]^{n},x+\delta\in[0,1]^{n}$.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图像领域中的白盒是早期对抗攻击研究的主要设置。我们介绍并比较生成对抗样本的公式。首先，我们定义 $F:\mathbb{R}^{n}\longrightarrow\{1\dots
    k\}$ 为将图像值向量映射到类别标签的模型分类器。$Z(\cdot)$ 是倒数第二层的输出，通常表示类别概率。$Z(\cdot)_{t}$ 是 $t$-th
    类的概率。$Loss$ 函数描述了在分类器 $F$ 下输入和输出的损失，我们设定 $Loss(x,F(x))=0$。$\delta$ 是扰动。$\left\|\delta\right\|_{p}$
    是 $\delta$ 的 $L_{p}$-范数。$x=\{x^{1},x^{2},...,x^{n}\}$ 是原始样本，$x^{i}$ 是样本中像素或元素，$x^{i}\in
    x,1\leqslant i\leqslant n$。$x_{i}$ 是第 $i$ 次迭代的样本，通常 $x_{0}=x$。$x+\delta$ 是对抗样本。这里，$x\in[0,1]^{n},x+\delta\in[0,1]^{n}$。
- en: 'The process of finding perturbations essentially needs to solve the following
    optimization problems (the first equation is non-targeted attack, the second equation
    is targeted attack, $T$ is targeted class label):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找扰动的过程本质上需要解决以下优化问题（第一个方程是非目标攻击，第二个方程是目标攻击，$T$ 是目标类别标签）：
- en: '|  | $\begin{split}&amp;\arg\min_{\delta}\ \left\&#124;\delta\right\&#124;_{p},s.t.\
    F(x+\delta)\neq F(x)\\ &amp;\arg\min_{\delta}\ \left\&#124;\delta\right\&#124;_{p},s.t.\
    F(x+\delta)=T\end{split}$ |  | (3) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&\arg\min_{\delta}\ \left\&#124;\delta\right\&#124;_{p},s.t.\
    F(x+\delta)\neq F(x)\\ &\arg\min_{\delta}\ \left\&#124;\delta\right\&#124;_{p},s.t.\
    F(x+\delta)=T\end{split}$ |  | (3) |'
- en: Methods of finding perturbations can be roughly divided into calculating gradients
    and solving optimization function. Szegedy et al. [[213](#bib.bib213)] first proposed
    an optimization function to find AEs and solved it with L-BFGS. FGSM [[74](#bib.bib74)],
    BIM [[15](#bib.bib15)], MI-FGSM [[57](#bib.bib57)] are a series of methods for
    finding perturbations by directly calculating gradients. Deepfool [[156](#bib.bib156)]
    and NewtonFool [[102](#bib.bib102)] approximate the nearest classification boundary
    by Taylor expansion. Instead of perturbing a whole image, JSMA [[175](#bib.bib175)]
    finds a few pixels to perturb through calculating partial derivative. C&W [[43](#bib.bib43)],
    EAD [[48](#bib.bib48)], OptMargin [[87](#bib.bib87)] are a series of methods to
    find perturbations by optimizing the objective function.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找扰动的方法大致可以分为计算梯度和求解优化函数。Szegedy 等人 [[213](#bib.bib213)] 首次提出了一种优化函数来寻找对抗样本，并使用
    L-BFGS 进行求解。FGSM [[74](#bib.bib74)]、BIM [[15](#bib.bib15)]、MI-FGSM [[57](#bib.bib57)]
    是一系列通过直接计算梯度来寻找扰动的方法。Deepfool [[156](#bib.bib156)] 和 NewtonFool [[102](#bib.bib102)]
    通过泰勒展开来近似最近的分类边界。JSMA [[175](#bib.bib175)] 通过计算部分导数找到少量像素进行扰动，而不是扰动整张图像。C&W [[43](#bib.bib43)]、EAD
    [[48](#bib.bib48)]、OptMargin [[87](#bib.bib87)] 是一系列通过优化目标函数来寻找扰动的方法。
- en: 'L-BFGS attack. Szegedy et al. [[213](#bib.bib213)] try to find small $\delta$
    that satisfies $F(x+\delta)=l$. So they construct a function with $\delta$ and
    $Loss$ function, and use box-constrained L-BFGS to minimize this optimization
    problem. In Equation [4](#S7.E4 "In 7.2.1 White-box attacks in the image classification
    field ‣ 7.2 Adversarial Attack Approach ‣ 7 Adversarial Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey"), $c\ (>0)$ is a hyperparameter to
    balance them.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 'L-BFGS 攻击。Szegedy 等人 [[213](#bib.bib213)] 尝试找到满足 $F(x+\delta)=l$ 的小 $\delta$。因此，他们构造了一个包含
    $\delta$ 和 $Loss$ 函数的函数，并使用带有盒约束的 L-BFGS 来最小化这个优化问题。在方程 [4](#S7.E4 "In 7.2.1 White-box
    attacks in the image classification field ‣ 7.2 Adversarial Attack Approach ‣
    7 Adversarial Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey")
    中，$c\ (>0)$ 是一个超参数，用于平衡它们。'
- en: '|  | $\begin{split}\min_{\delta}&amp;\ c\left\&#124;\delta\right\&#124;_{2}+Loss(x+\delta,l)\\
    \end{split}$ |  | (4) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\min_{\delta}&\ c\left\|\delta\right\|_{2}+Loss(x+\delta,l)\\
    \end{split}$ |  | (4) |'
- en: FGSM attack. Goodfellow et al. [[74](#bib.bib74)] find perturbations based on
    the gradient of input. $l_{x}$ is the true label of $x$. The direction of perturbation
    is determined by the computed gradient using back-propagation. $\varepsilon$ is
    self-defined, and each pixel goes $\varepsilon$ size in gradient direction.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: FGSM 攻击。Goodfellow 等人 [[74](#bib.bib74)] 根据输入的梯度来寻找扰动。$l_{x}$ 是 $x$ 的真实标签。扰动的方向由使用反向传播计算的梯度决定。$\varepsilon$
    是自定义的，每个像素在梯度方向上移动 $\varepsilon$ 大小。
- en: '|  | $\begin{split}\delta=\varepsilon\cdot sign(\nabla_{x}Loss(x,l_{x}))\end{split}$
    |  | (5) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\delta=\varepsilon\cdot sign(\nabla_{x}Loss(x,l_{x}))\end{split}$
    |  | (5) |'
- en: 'BIM attack. BIM (or I-FGSM) [[15](#bib.bib15)] iteratively solves $\delta$
    and updates new adversarial samples based on FGSM [[74](#bib.bib74)] in Equation [6](#S7.E6
    "In 7.2.1 White-box attacks in the image classification field ‣ 7.2 Adversarial
    Attack Approach ‣ 7 Adversarial Attack ‣ Towards Security Threats of Deep Learning
    Systems: A Survey"). $l_{x}$ is the true label of $x$. $Clip\{x\}$ is a clipping
    function on image per pixel.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 'BIM 攻击。BIM（或 I-FGSM）[[15](#bib.bib15)] 迭代地解决 $\delta$ 并基于 FGSM [[74](#bib.bib74)]
    更新新的对抗样本，见方程 [6](#S7.E6 "In 7.2.1 White-box attacks in the image classification
    field ‣ 7.2 Adversarial Attack Approach ‣ 7 Adversarial Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey")。$l_{x}$ 是 $x$ 的真实标签。$Clip\{x\}$ 是图像每个像素的裁剪函数。'
- en: '|  | $\begin{split}x_{i+1}&amp;=Clip\{x_{i}+\varepsilon\cdot sign(\nabla_{x}Loss(x_{i},l_{x}))\}\end{split}$
    |  | (6) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}x_{i+1}&=Clip\{x_{i}+\varepsilon\cdot sign(\nabla_{x}Loss(x_{i},l_{x}))\}\end{split}$
    |  | (6) |'
- en: 'MI-FGSM attack. MI-FGSM [[57](#bib.bib57)] adds momentum based on I-FGSM [[15](#bib.bib15)].
    Momentum is used to escape from poor local maximum and iterations are used to
    stabilize optimization. In Equation [7](#S7.E7 "In 7.2.1 White-box attacks in
    the image classification field ‣ 7.2 Adversarial Attack Approach ‣ 7 Adversarial
    Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey"), $g_{i}$
    represents the gradient like Equation [6](#S7.E6 "In 7.2.1 White-box attacks in
    the image classification field ‣ 7.2 Adversarial Attack Approach ‣ 7 Adversarial
    Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey"), it has
    both the current step gradient and previous step gradient. $y$ is the target wrong
    label.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: MI-FGSM 攻击。MI-FGSM [[57](#bib.bib57)] 在 I-FGSM [[15](#bib.bib15)] 的基础上增加了动量。动量用于摆脱较差的局部极大值，并且迭代被用来稳定优化。在方程
    [7](#S7.E7 "在 7.2.1 白盒攻击在图像分类领域 ‣ 7.2 对抗攻击方法 ‣ 7 对抗攻击 ‣ 针对深度学习系统的安全威胁：一项调查") 中，$g_{i}$
    代表梯度，如方程 [6](#S7.E6 "在 7.2.1 白盒攻击在图像分类领域 ‣ 7.2 对抗攻击方法 ‣ 7 对抗攻击 ‣ 针对深度学习系统的安全威胁：一项调查")
    所示，它包含了当前步骤的梯度和之前步骤的梯度。$y$ 是目标错误标签。
- en: '|  | $\begin{split}x_{i+1}&amp;=Clip\{x_{i}+\varepsilon\cdot\frac{g_{i+1}}{\left\&#124;g_{i+1}\right\&#124;_{2}}\}\\
    g_{i+1}&amp;=\mu\cdot g_{i}+\frac{\nabla_{x}Loss(x_{i},y)}{\left\&#124;\nabla_{x}Loss(x_{i},y)\right\&#124;_{1}}\end{split}$
    |  | (7) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}x_{i+1}&amp;=Clip\{x_{i}+\varepsilon\cdot\frac{g_{i+1}}{\left\&#124;g_{i+1}\right\&#124;_{2}}\}\\
    g_{i+1}&amp;=\mu\cdot g_{i}+\frac{\nabla_{x}Loss(x_{i},y)}{\left\&#124;\nabla_{x}Loss(x_{i},y)\right\&#124;_{1}}\end{split}$
    |  | (7) |'
- en: 'JSMA attack. JSMA [[175](#bib.bib175)] only modifies a few pixels at every
    iteration. In each iteration, shown in Equation [8](#S7.E8 "In 7.2.1 White-box
    attacks in the image classification field ‣ 7.2 Adversarial Attack Approach ‣
    7 Adversarial Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey"),
    $\alpha_{pq}$ represents the impact on target classification of pixels $p,q$,
    and $\beta_{pq}$ represents the impact on all other outputs. In the last formula,
    larger value means greater possibility to fool the network. They pick $(p^{*},q^{*})$
    pixels to perturb.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: JSMA 攻击。JSMA [[175](#bib.bib175)] 每次迭代只修改少量像素。在每次迭代中，如方程 [8](#S7.E8 "在 7.2.1
    白盒攻击在图像分类领域 ‣ 7.2 对抗攻击方法 ‣ 7 对抗攻击 ‣ 针对深度学习系统的安全威胁：一项调查") 所示，$\alpha_{pq}$ 代表像素
    $p,q$ 对目标分类的影响，$\beta_{pq}$ 代表对所有其他输出的影响。在最后一个公式中，值越大意味着欺骗网络的可能性越大。他们选择 $(p^{*},q^{*})$
    像素进行扰动。
- en: '|  | <math   alttext="\begin{split}\alpha_{pq}&amp;\ =\sum_{i\in\{p,q\}}\frac{\partial
    Z(x)_{t}}{\partial x^{i}}\\ \beta_{pq}&amp;\ =(\sum_{i\in\{p,q\}}\sum_{j}\frac{\partial
    Z(x)_{j}}{\partial x^{i}})-\alpha_{pq}\\'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\alpha_{pq}&amp;\ =\sum_{i\in\{p,q\}}\frac{\partial
    Z(x)_{t}}{\partial x^{i}}\\ \beta_{pq}&amp;\ =(\sum_{i\in\{p,q\}}\sum_{j}\frac{\partial
    Z(x)_{j}}{\partial x^{i}})-\alpha_{pq}\\'
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><msub ><mi >α</mi><mrow  ><mi
    >p</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >q</mi></mrow></msub></mtd><mtd
    columnalign="left"  ><mrow ><mo lspace="0.778em" rspace="0.111em" >=</mo><mrow
    ><munder  ><mo movablelimits="false"  >∑</mo><mrow ><mi >i</mi><mo  >∈</mo><mrow
    ><mo stretchy="false" >{</mo><mi >p</mi><mo >,</mo><mi >q</mi><mo stretchy="false"
    >}</mo></mrow></mrow></munder><mfrac ><mrow  ><mo rspace="0em"  >∂</mo><mrow ><mi
    >Z</mi><mo lspace="0em" rspace="0em" >​</mo><msub ><mrow ><mo stretchy="false"
    >(</mo><mi >x</mi><mo stretchy="false" >)</mo></mrow><mi >t</mi></msub></mrow></mrow><mrow
    ><mo rspace="0em" >∂</mo><msup ><mi  >x</mi><mi >i</mi></msup></mrow></mfrac></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><msub ><mi >β</mi><mrow  ><mi >p</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >q</mi></mrow></msub></mtd><mtd columnalign="left"  ><mrow
    ><mo lspace="0.778em" >=</mo><mrow ><mrow  ><mo stretchy="false"  >(</mo><mrow
    ><munder ><mo lspace="0em" movablelimits="false" rspace="0em" >∑</mo><mrow ><mi  >i</mi><mo
    >∈</mo><mrow ><mo stretchy="false"  >{</mo><mi >p</mi><mo >,</mo><mi >q</mi><mo
    stretchy="false"  >}</mo></mrow></mrow></munder><mrow ><munder ><mo movablelimits="false"
    >∑</mo><mi >j</mi></munder><mfrac ><mrow  ><mo rspace="0em"  >∂</mo><mrow ><mi
    >Z</mi><mo lspace="0em" rspace="0em" >​</mo><msub ><mrow ><mo stretchy="false"  >(</mo><mi
    >x</mi><mo stretchy="false"  >)</mo></mrow><mi >j</mi></msub></mrow></mrow><mrow
    ><mo rspace="0em" >∂</mo><msup ><mi >x</mi><mi >i</mi></msup></mrow></mfrac></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow><mo >−</mo><msub ><mi >α</mi><mrow ><mi >p</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >q</mi></mrow></msub></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝛼</ci><apply  ><ci >𝑝</ci><ci >𝑞</ci></apply></apply><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑖</ci><set ><ci  >𝑝</ci><ci >𝑞</ci></set></apply></apply><apply
    ><apply ><apply  ><apply ><ci >𝑍</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><ci >𝑡</ci></apply></apply></apply><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑥</ci><ci >𝑖</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝛽</ci><apply ><ci  >𝑝</ci><ci >𝑞</ci></apply></apply></apply></apply></apply><apply
    ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><ci
    >𝑖</ci><set ><ci >𝑝</ci><ci >𝑞</ci></set></apply></apply><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑗</ci></apply><apply ><apply ><apply
    ><ci >𝑍</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><ci
    >𝑗</ci></apply></apply></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑥</ci><ci >𝑖</ci></apply></apply></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝛼</ci><apply ><ci >𝑝</ci><ci >𝑞</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\alpha_{pq}&\ =\sum_{i\in\{p,q\}}\frac{\partial
    Z(x)_{t}}{\partial x^{i}}\\ \beta_{pq}&\ =(\sum_{i\in\{p,q\}}\sum_{j}\frac{\partial
    Z(x)_{j}}{\partial x^{i}})-\alpha_{pq}\\ \end{split}</annotation></semantics></math>
    |  | (8) |
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><msub ><mi >α</mi><mrow  ><mi
    >p</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >q</mi></mrow></msub></mtd><mtd
    columnalign="left"  ><mrow ><mo lspace="0.778em" rspace="0.111em" >=</mo><mrow
    ><munder  ><mo movablelimits="false"  >∑</mo><mrow ><mi >i</mi><mo  >∈</mo><mrow
    ><mo stretchy="false" >{</mo><mi >p</mi><mo >,</mo><mi >q</mi><mo stretchy="false"
    >}</mo></mrow></mrow></munder><mfrac ><mrow  ><mo rspace="0em"  >∂</mo><mrow ><mi
    >Z</mi><mo lspace="0em" rspace="0em" >​</mo><msub ><mrow ><mo stretchy="false"
    >(</mo><mi >x</mi><mo stretchy="false" >)</mo></mrow><mi >t</mi></msub></mrow></mrow><mrow
    ><mo rspace="0em" >∂</mo><msup ><mi  >x</mi><mi >i</mi></msup></mrow></mfrac></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><msub ><mi >β</mi><mrow  ><mi >p</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >q</mi></mrow></msub></mtd><mtd columnalign="left"  ><mrow
    ><mo lspace="0.778em" >=</mo><mrow ><mrow  ><mo stretchy="false"  >(</mo><mrow
    ><munder ><mo lspace="0em" movablelimits="false" rspace="0em" >∑</mo><mrow ><mi  >i</mi><mo
    >∈</mo><mrow ><mo stretchy="false"  >{</mo><mi >p</mi><mo >,</mo><mi >q</mi><mo
    stretchy="false"  >}</mo></mrow></mrow></munder><mrow ><munder ><mo movablelimits="false"
    >∑</mo><mi >j</mi></munder><mfrac ><mrow  ><mo rspace="0em"  >∂</mo><mrow ><mi
    >Z</mi><mo lspace="0em" rspace="0em" >​</mo><msub ><mrow ><mo stretchy="false"  >(</mo><mi
    >x</mi><mo stretchy="false"  >)</mo></mrow><mi >j</mi></msub></mrow></mrow><mrow
    ><mo rspace="0em" >∂</mo><msup ><mi >x</mi><mi >i</mi></msup></mrow></mfrac></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow><mo >−</mo><msub ><mi >α</mi><mrow ><mi >p</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >q</mi></mrow></msub></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝛼</ci><apply  ><ci >𝑝</ci><ci >𝑞</ci></apply></apply><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑖</ci><set ><ci  >𝑝</ci><ci >𝑞</ci></set></apply></apply><apply
    ><apply ><apply  ><apply ><ci >𝑍</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><ci >𝑡</ci></apply></apply></apply><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑥</ci><ci >𝑖</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝛽</ci><apply ><ci  >𝑝</ci><ci >𝑞</ci></apply></apply></apply></apply></apply><apply
    ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><ci
    >𝑖</ci><set ><ci >𝑝</ci><ci >𝑞</ci></set></apply></apply><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑗</ci></apply><apply ><apply ><apply
    ><ci >𝑍</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><ci
    >𝑗</ci></apply></apply></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑥</ci><ci >𝑖</ci></apply></apply></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝛼</ci><apply ><ci >𝑝</ci><ci >𝑞</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\alpha_{pq}&\ =\sum_{i\in\{p,q\}}\frac{\partial
    Z(x)_{t}}{\partial x^{i}}\\ \beta_{pq}&\ =(\sum_{i\in\{p,q\}}\sum_{j}\frac{\partial
    Z(x)_{j}}{\partial x^{i}})-\alpha_{pq}\\ \end{split}</annotation></semantics></math>
    |  | (8) |
- en: '|  | $\begin{split}(p^{*},q^{*})=\arg\max_{(p,q)}&amp;(-\alpha_{pq}\cdot\beta_{pq})\cdot(\alpha_{pq}>0)\cdot(\beta_{pq}<0)\end{split}$
    |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}(p^{*},q^{*})=\arg\max_{(p,q)}&\;(-\alpha_{pq}\cdot\beta_{pq})\cdot(\alpha_{pq}>0)\cdot(\beta_{pq}<0)\end{split}$
    |  |'
- en: 'NewtonFool attack. NewtonFool [[102](#bib.bib102)] uses softmax output $Z(x)$.
    In Equation [9](#S7.E9 "In 7.2.1 White-box attacks in the image classification
    field ‣ 7.2 Adversarial Attack Approach ‣ 7 Adversarial Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey"), $x_{0}$ is the original sample and
    $l=F(x_{0})$. $\delta_{i}=x_{i+1}-x_{i}$ is the perturbation at iteration $i$.
    They tried to find small $\delta$ so that $Z(x_{0}+\delta)_{l}\approx 0$. Starting
    with $x_{0}$, they approximated $Z(x_{i})_{l}$ using a linear function step by
    step.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 'NewtonFool 攻击。NewtonFool [[102](#bib.bib102)] 使用 softmax 输出 $Z(x)$。在方程 [9](#S7.E9
    "In 7.2.1 White-box attacks in the image classification field ‣ 7.2 Adversarial
    Attack Approach ‣ 7 Adversarial Attack ‣ Towards Security Threats of Deep Learning
    Systems: A Survey") 中，$x_{0}$ 是原始样本，$l=F(x_{0})$。$\delta_{i}=x_{i+1}-x_{i}$ 是第
    $i$ 次迭代的扰动。他们尝试找到小的 $\delta$ 使得 $Z(x_{0}+\delta)_{l}\approx 0$。从 $x_{0}$ 开始，他们逐步使用线性函数逼近
    $Z(x_{i})_{l}$。'
- en: '|  | $\begin{split}Z(x_{i+1})_{l}\approx Z(x_{i})_{l}+\nabla Z(x_{i})_{l}\cdot(x_{i+1}-x_{i})\end{split}$
    |  | (9) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}Z(x_{i+1})_{l}\approx Z(x_{i})_{l}+\nabla Z(x_{i})_{l}\cdot(x_{i+1}-x_{i})\end{split}$
    |  | (9) |'
- en: C&W attack. C&W [[43](#bib.bib43)] tries to find small $\delta$ in $L_{0}$,
    $L_{2}$, and $L_{\infty}$ norms. They change the $Loss$ function part in L-BFGS [[213](#bib.bib213)]
    to an optimization function $f(\cdot)$.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: C&W 攻击。C&W [[43](#bib.bib43)] 试图在 $L_{0}$、$L_{2}$ 和 $L_{\infty}$ 范数中找到小的 $\delta$。他们将
    L-BFGS [[213](#bib.bib213)] 中的 $Loss$ 函数部分更改为优化函数 $f(\cdot)$。
- en: '|  | $\begin{split}\min_{\delta}&amp;\ \left\&#124;\delta\right\&#124;_{p}+c\cdot
    f(x+\delta)\\ \end{split}$ |  | (10) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\min_{\delta}&\ \left\|\delta\right\|_{p}+c\cdot f(x+\delta)\\
    \end{split}$ |  | (10) |'
- en: '|  | $f(x+\delta)=\max(\max\{Z(x+\delta)_{i}:i\neq t\}-Z(x+\delta)_{t},-\mathcal{K})$
    |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(x+\delta)=\max(\max\{Z(x+\delta)_{i}:i\neq t\}-Z(x+\delta)_{t},-\mathcal{K})$
    |  |'
- en: $c$ is a hyperparameter and $f(\cdot)$ is an artificially defined function,
    the above is just one case. Here, $f(\cdot)\leqslant 0$ if and only if classification
    result is adversarial targeted label $t$. $\mathcal{K}$ guarantees $x+\delta$
    will be classified as $t$ with high confidence.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: $c$ 是一个超参数，而 $f(\cdot)$ 是一个人工定义的函数，上述只是一个情况。这里，当且仅当分类结果为对抗目标标签 $t$ 时，$f(\cdot)\leqslant
    0$。$\mathcal{K}$ 保证 $x+\delta$ 将以高置信度被分类为 $t$。
- en: 'EAD attack. EAD [[48](#bib.bib48)] combines $L_{1}$ and $L_{2}$ penalty functions
    based on C&W [[43](#bib.bib43)]. In Equation [11](#S7.E11 "In 7.2.1 White-box
    attacks in the image classification field ‣ 7.2 Adversarial Attack Approach ‣
    7 Adversarial Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey"),
    $f(x+\delta)$ is the same as C&W and $\beta$ is another hyperparameter. C&W attack
    becomes a special EAD case when $\beta=0$.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 'EAD 攻击。EAD [[48](#bib.bib48)] 基于 C&W [[43](#bib.bib43)] 结合了 $L_{1}$ 和 $L_{2}$
    惩罚函数。在方程 [11](#S7.E11 "In 7.2.1 White-box attacks in the image classification
    field ‣ 7.2 Adversarial Attack Approach ‣ 7 Adversarial Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey") 中，$f(x+\delta)$ 与 C&W 相同，$\beta$
    是另一个超参数。当 $\beta=0$ 时，C&W 攻击变成一个特殊的 EAD 情况。'
- en: '|  | $\begin{split}\min_{\delta}&amp;\ c\cdot f(x+\delta)+\beta\left\&#124;\delta\right\&#124;_{1}+\left\&#124;\delta\right\&#124;_{2}^{2}\\
    \end{split}$ |  | (11) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\min_{\delta}&\;c\cdot f(x+\delta)+\beta\left\|\delta\right\|_{1}+\left\|\delta\right\|_{2}^{2}\\
    \end{split}$ |  | (11) |'
- en: 'OptMargin attack. OptMargin [[87](#bib.bib87)] is an extension of C&W [[43](#bib.bib43)]
    attack by adding many objective functions around $x$. In Equation [12](#S7.E12
    "In 7.2.1 White-box attacks in the image classification field ‣ 7.2 Adversarial
    Attack Approach ‣ 7 Adversarial Attack ‣ Towards Security Threats of Deep Learning
    Systems: A Survey"), $x_{0}$ is the original example. $x=x_{0}+\delta$ is adversarial.
    $y$ is the true label of $x_{0}$. $v_{i}$ are many perturbations applied to $x$.
    OptMargin guarantees not only $x$ fools network, but also its neighbors $x+v_{i}$.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 'OptMargin 攻击。OptMargin [[87](#bib.bib87)] 通过在 $x$ 周围添加许多目标函数扩展了 C&W [[43](#bib.bib43)]
    攻击。在方程 [12](#S7.E12 "In 7.2.1 White-box attacks in the image classification field
    ‣ 7.2 Adversarial Attack Approach ‣ 7 Adversarial Attack ‣ Towards Security Threats
    of Deep Learning Systems: A Survey") 中，$x_{0}$ 是原始样本。$x=x_{0}+\delta$ 是对抗样本。$y$
    是 $x_{0}$ 的真实标签。$v_{i}$ 是施加在 $x$ 上的许多扰动。OptMargin 不仅保证 $x$ 会欺骗网络，而且它的邻居 $x+v_{i}$
    也会。'
- en: '|  | $\begin{split}&amp;\min_{\delta}\;\left\&#124;\delta\right\&#124;_{2}^{2}+c\cdot(f_{1}(x)+\cdots+f_{m}(x))\\
    \end{split}$ |  | (12) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&\;\min_{\delta}\;\left\|\delta\right\|_{2}^{2}+c\cdot(f_{1}(x)+\cdots+f_{m}(x))\\
    \end{split}$ |  | (12) |'
- en: '|  | $f_{i}(x)=\max(Z(x+v_{i})_{y}-\max\{Z(x+v_{i})_{j}:j\neq y\},-\mathcal{K})$
    |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{i}(x)=\max(Z(x+v_{i})_{y}-\max\{Z(x+v_{i})_{j}:j\neq y\},-\mathcal{K})$
    |  |'
- en: UAP attack. Universal adversarial perturbations (UAPs) [[155](#bib.bib155)]
    can suit almost all samples of a certain dataset. The purpose is to seek a universal
    perturbation $\delta$ which fools $F(\cdot)$ on almost any sample from the dataset.
    Liu et al. [[135](#bib.bib135)] extend UAPs to unsupervised learning. Co et al. [[53](#bib.bib53)]
    try to generate UAPs with procedural noise functions.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: UAP攻击。通用对抗扰动（UAPs）[[155](#bib.bib155)]可以适用于某个数据集的几乎所有样本。其目的是寻找一个通用扰动$\delta$，它可以欺骗几乎所有来自数据集的样本的$F(\cdot)$。Liu等人[[135](#bib.bib135)]将UAPs扩展到无监督学习中。Co等人[[53](#bib.bib53)]尝试使用程序噪声函数生成UAPs。
- en: 7.2.2 Black-box attacks in the image classification field
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2 图像分类领域的黑盒攻击
- en: Finding small perturbations often requires white-box models to calculate gradients.
    However, it does not work in a black-box setting. Attackers are limited only to
    query access to the model. Therefore, researchers propose several methods to overcome
    constraints on query budget.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找小扰动通常需要白盒模型来计算梯度。然而，在黑盒设置中无法实现。攻击者只能查询模型。因此，研究人员提出了几种方法来克服查询预算的限制。
- en: 'Step 2.1\. Training substitute model. As mentioned in Section [4](#S4 "4 Model
    Extraction Attack ‣ Towards Security Threats of Deep Learning Systems: A Survey"),
    stealing decision boundaries in model extraction attack and training substitute
    model can facilitate black-box adversarial attacks [[174](#bib.bib174)][[173](#bib.bib173)][[107](#bib.bib107)].
    Papernot et al. [[174](#bib.bib174)] propose a method based on an alternative
    training algorithm using synthetic data generation in black-box settings.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤2.1。训练替代模型。如第[4](#S4 "4 Model Extraction Attack ‣ Towards Security Threats
    of Deep Learning Systems: A Survey")节所述，在模型提取攻击中窃取决策边界和训练替代模型可以促进黑盒对抗攻击[[174](#bib.bib174)][[173](#bib.bib173)][[107](#bib.bib107)]。Papernot等人[[174](#bib.bib174)]提出了一种基于合成数据生成的替代训练算法的方法，用于黑盒设置。'
- en: This step needs that AEs have high transferability from the substitute model
    to the target model [[248](#bib.bib248), [58](#bib.bib58)]. Gradient aligned adversarial
    subspace [[219](#bib.bib219)] estimate unknown dimensions of the input space.
    They find that a large part of the subspace is shared for two different models,
    thus achieving transferability. Further, they determine sufficient conditions
    for the transferability of model-agnostic perturbations. Naseer et al. [[160](#bib.bib160)]
    propose a framework to launch highly transferable attacks. It can create adversarial
    patterns to mislead networks trained in completely different domains.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤需要对抗样本在替代模型到目标模型之间具有较高的可转移性[[248](#bib.bib248), [58](#bib.bib58)]。梯度对齐对抗子空间[[219](#bib.bib219)]估计输入空间的未知维度。他们发现两个不同模型的大部分子空间是共享的，从而实现了可转移性。此外，他们确定了模型无关扰动的可转移性的充分条件。Naseer等人[[160](#bib.bib160)]提出了一个框架来发起高度可转移的攻击。它可以创建对抗模式以误导在完全不同领域中训练的网络。
- en: Step 2.2\. Estimating gradients. This method needs many queries to estimate
    gradients and then search for AEs. Narodytska et al. [[159](#bib.bib159)] use
    a technique based on local search to construct the numerical approximation of
    network gradients, and then constructed perturbations in an image. Moreover, Ilyas
    et al. [[98](#bib.bib98)] introduce a more rigorous and practical black-box threat
    model. They applied a natural evolution strategy to estimate gradients and perform
    black-box attacks, using 2$\sim$3 orders of magnitude less queries. Guo et al. [[80](#bib.bib80)]
    utilize the gradients of some reference models to reduce queries. These reference
    models can span some promising search subspaces. Liu et al. [[142](#bib.bib142)]
    propose a decision-based attack method by constraining perturbations in low-frequency
    subspace with small queries. Cheng et al. [[51](#bib.bib51)] present a prior-guided
    random gradient-free method, which takes advantage of a transfer-based prior and
    query information simultaneously.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2.2。估计梯度。这种方法需要大量查询来估计梯度，然后寻找对抗样本。Narodytska等人[[159](#bib.bib159)]使用基于局部搜索的技术构建网络梯度的数值近似，然后在图像中构建扰动。此外，Ilyas等人[[98](#bib.bib98)]引入了更严格且实用的黑盒威胁模型。他们应用自然进化策略来估计梯度并执行黑盒攻击，使用了2$\sim$3个数量级更少的查询。Guo等人[[80](#bib.bib80)]利用一些参考模型的梯度来减少查询。这些参考模型可以覆盖一些有前景的搜索子空间。Liu等人[[142](#bib.bib142)]提出了一种基于决策的攻击方法，通过在低频子空间中限制扰动来减少查询。Cheng等人[[51](#bib.bib51)]提出了一种先验引导的随机无梯度方法，同时利用了基于迁移的先验和查询信息。
- en: 7.2.3 Attacks in the speech recognition field
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.3 语音识别领域的攻击
- en: The difficulties of attacking speech recognition model are that, humans can
    identify adversarial perturbations, and audio AEs may be ineffective during over-the-air
    playback. Yuan et al. [[262](#bib.bib262)] embed voice commands into songs, and
    thereby attack speech recognition systems, not being detected by humans. DeepSearch [[44](#bib.bib44)]
    could convert any given waveform into any desired target phrase through adding
    small perturbations on speech-to-text neural networks. Qin et al. [[186](#bib.bib186)]
    leverage the psychoacoustic principle of auditory masking to generate effectively
    imperceptible audio AEs. Yakura et al. [[254](#bib.bib254)] simulate the transformations
    caused by playback or recording in the physical world, and incorporates these
    transformations into the generation process to obtain robust AEs.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击语音识别模型的难点在于，人类可以识别对抗扰动，并且在无线播放过程中，音频对抗样本可能效果不佳。Yuan 等人 [[262](#bib.bib262)]
    将语音命令嵌入歌曲中，从而攻击语音识别系统，且不会被人类检测到。DeepSearch [[44](#bib.bib44)] 通过在语音到文本神经网络上添加小的扰动，将任意给定的波形转换为所需的目标短语。Qin
    等人 [[186](#bib.bib186)] 利用听觉掩蔽的心理声学原理生成有效的不可察觉的音频对抗样本。Yakura 等人 [[254](#bib.bib254)]
    模拟物理世界中播放或录制造成的变换，并将这些变换融入生成过程，以获得鲁棒的对抗样本。
- en: 7.2.4 Attacks in the text processing field
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.4 文本处理领域的攻击
- en: Constructing adversarial examples for natural language processing (NLP) is a
    large challenge. The word and sentence spaces are discrete. It is difficult to
    produce small perturbations along the gradient direction, and hard to guarantee
    its fluency [[268](#bib.bib268)]. DeepWordBug [[67](#bib.bib67)] generate adversarial
    text sequences in black-box settings. They adopt different score functions to
    better mutate words and minimize edit distance between the original and modified
    texts. TextBugger [[123](#bib.bib123)] also generated adversarial texts. In black-box
    setting, its process is finding important sentences and words, and bugs generation.
    The computational complexity is sub-linear to the text length. It has higher success
    rate and less perturbed words than DeepWordBug on IMDB dataset.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为自然语言处理（NLP）构建对抗样本是一个巨大的挑战。词汇和句子空间是离散的。沿梯度方向产生小的扰动很困难，而且难以保证其流畅性 [[268](#bib.bib268)]。DeepWordBug
    [[67](#bib.bib67)] 在黑箱设置下生成对抗文本序列。他们采用不同的评分函数来更好地变异单词，并最小化原始文本和修改文本之间的编辑距离。TextBugger
    [[123](#bib.bib123)] 也生成了对抗文本。在黑箱设置下，其过程是寻找重要句子和单词，并生成漏洞。其计算复杂度与文本长度呈子线性关系。在IMDB数据集上，其成功率更高，扰动单词更少，相较于DeepWordBug表现更佳。
- en: However, the above work is achieved by similar-looking character substitution
    (‘o’ and ‘0’), adding space and so on, which destroy lexical correctness. In [[189](#bib.bib189)][[263](#bib.bib263)],
    they study word-level substitution attack to guarantee lexical correctness, grammatical
    correctness and semantic similarity. Ren et al. [[189](#bib.bib189)] propose a
    word replacement order determined by word saliency and classification probability
    based on synonyms replacement strategy. Zang et al. [[263](#bib.bib263)] present
    a word replacement method based on sememe, and a search algorithm based on particle
    swarm optimization.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述工作是通过类似字符替换（‘o’和‘0’）、添加空格等方式实现的，这破坏了词汇的正确性。在 [[189](#bib.bib189)][[263](#bib.bib263)]
    中，他们研究了词级替换攻击，以保证词汇正确性、语法正确性和语义相似性。Ren 等人 [[189](#bib.bib189)] 提出了基于同义词替换策略的词替换顺序，该顺序由词的显著性和分类概率决定。Zang
    等人 [[263](#bib.bib263)] 提出了基于义素的词替换方法和基于粒子群优化的搜索算法。
- en: Neural machine translation (NMT) models in NLP also suffer from the vulnerability
    to adversarial perturbations [[52](#bib.bib52)]. Zou et al. [[276](#bib.bib276)]
    generate adversarial translation examples based on a new paradigm of reinforcement
    learning, instead of limited manual analyzed error features. Experiments show
    that the replacement of synonyms in Chinese will cause obvious errors in English
    translation results. Sato et al. [[199](#bib.bib199)] reveal that adversarial
    regularization technology can also improve the NMT models.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）中的神经机器翻译（NMT）模型也受到对抗扰动的影响 [[52](#bib.bib52)]。Zou 等人 [[276](#bib.bib276)]
    基于强化学习的新范式生成对抗翻译示例，而不是依赖有限的手动分析的错误特征。实验表明，中文中同义词的替换会导致英文翻译结果的明显错误。Sato 等人 [[199](#bib.bib199)]
    揭示了对抗正则化技术也可以提高NMT模型的性能。
- en: 7.2.5 Attacks in the malware detection field
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.5 恶意软件检测领域的攻击
- en: In the malware field, Rigaki et al. [[190](#bib.bib190)] used GANs to avoid
    malware detection by modifying network behavior to imitate traffic of legitimate
    applications. They can adjust command and control channels to simulate Facebook
    chat network traffic by modifying the source code of malware. Hu et al. [[90](#bib.bib90)][[91](#bib.bib91)]
    and Rosenberg et al. [[194](#bib.bib194)] proposed methods to generate adversarial
    malware examples in black-box to attack detection models. Dujaili et al. [[14](#bib.bib14)]
    proposed SLEIPNIR for adversarial attack on binary encoded malware detection.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在恶意软件领域，Rigaki 等人[[190](#bib.bib190)] 使用生成对抗网络（GANs）通过修改网络行为以模拟合法应用的流量，从而避免恶意软件检测。他们可以通过修改恶意软件的源代码调整命令和控制通道，以模拟
    Facebook 聊天网络流量。Hu 等人[[90](#bib.bib90)][[91](#bib.bib91)] 和 Rosenberg 等人[[194](#bib.bib194)]
    提出了在黑箱中生成对抗恶意软件示例以攻击检测模型的方法。Dujaili 等人[[14](#bib.bib14)] 提出了 SLEIPNIR 用于二进制编码恶意软件检测的对抗攻击。
- en: 7.2.6 Attacks in the object detection field
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.6 物体检测领域中的攻击
- en: Zhao et al. [[274](#bib.bib274)] propose hiding attack and appearing attack
    to produce practical AEs. Their attacks can attack real-world object detectors
    in both long and short distance. Wei et al. [[236](#bib.bib236)] manipulate the
    feature maps extracted by the feature network, and enhance the transferability
    of AEs when attacking image object detection models.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人[[274](#bib.bib274)] 提出了隐藏攻击和出现攻击来产生实际的对抗样本。他们的攻击可以在远距离和近距离攻击现实世界的物体检测器。Wei
    等人[[236](#bib.bib236)] 操控特征网络提取的特征图，增强对抗样本在攻击图像物体检测模型时的可迁移性。
- en: 7.2.7 Attacks in the physical world.
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.7 物理世界中的攻击
- en: In this setting, attackers need to consider more environmental factors. Zeng
    et al. [[265](#bib.bib265)] pay special attention to AEs corresponding to meaningful
    changes in 3D physical properties, such as rotation, translation, lighting conditions,
    etc. Li et al. [[124](#bib.bib124)] implement physical attacks by placing a mainly-translucent
    sticker over the lens of a camera. The perturbations are imperceptible, but can
    make models misclassify objects taken by this camera.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种环境下，攻击者需要考虑更多的环境因素。Zeng 等人[[265](#bib.bib265)] 特别关注对应于 3D 物理属性有意义变化的对抗样本，如旋转、平移、光照条件等。Li
    等人[[124](#bib.bib124)] 通过在摄像头镜头上放置主要为半透明的贴纸来实施物理攻击。这些扰动难以察觉，但能使模型错误分类由该摄像头拍摄的物体。
- en: 7.2.8 Attacks in real-time stream input tasks
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.8 实时流输入任务中的攻击
- en: In this situation, attackers cannot observe the entire original sample and then
    add a perturbation at any point as in static input. Gong et al. [[72](#bib.bib72)]
    propose a real-time adversarial attack approach, in which attackers can only observe
    past data points and add perturbations to the remaining data points of the input.
    Li et al. [[127](#bib.bib127)] generate 3D adversarial perturbed fragments to
    attack real-time video classification models. They find AEs need to consider the
    uncertainty in the clip boundaries input to the video classifier. Ranjan et al. [[187](#bib.bib187)]
    find that destroying small patches (¡1%) of the image size will significantly
    affect optical flow estimation in self-driving cars.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，攻击者无法像在静态输入中那样观察整个原始样本然后在任何点添加扰动。Gong 等人[[72](#bib.bib72)] 提出了一个实时对抗攻击方法，其中攻击者只能观察过去的数据点，并对输入的剩余数据点添加扰动。Li
    等人[[127](#bib.bib127)] 生成了 3D 对抗扰动片段来攻击实时视频分类模型。他们发现对抗样本需要考虑视频分类器输入的剪辑边界的不确定性。Ranjan
    等人[[187](#bib.bib187)] 发现摧毁图像尺寸的少量区域（¡1%）会显著影响自动驾驶汽车中的光流估计。
- en: 7.2.9 Attacks against graph neural networks
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.9 针对图神经网络的攻击
- en: Graph neural networks (GNNs) are also vulnerable to adversarial attacks [[277](#bib.bib277)].
    However, the discrete edges and features of the graph data also bring new challenges
    for attacks. Wu et al. [[239](#bib.bib239)] use integrated gradient technology
    to deal with discrete graph connections and discrete features. It can accurately
    determine the effect of changing selected features or edges. For handling discrete
    graph data, Xu et al. [[251](#bib.bib251)] study a technology of generating topology
    attacks via convex relaxation to apply gradient-based adversarial attacks to GNNs.
    Bojchevski et al. [[33](#bib.bib33)] provide adversarial vulnerability analysis
    on widely used methods based on random walks. Wang et al. [[225](#bib.bib225)]
    try to evade detection through manipulating the graph structure and formulate
    this attack as a graph-based optimization problem.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）也容易受到对抗攻击[[277](#bib.bib277)]。然而，图数据的离散边缘和特征也为攻击带来了新的挑战。吴等人[[239](#bib.bib239)]使用集成梯度技术处理离散图连接和离散特征，能够准确确定改变选定特征或边缘的效果。对于处理离散图数据，徐等人[[251](#bib.bib251)]研究了一种通过凸松弛生成拓扑攻击的技术，以便将基于梯度的对抗攻击应用于GNNs。博伊切夫斯基等人[[33](#bib.bib33)]对基于随机游走的广泛使用的方法提供了对抗脆弱性分析。王等人[[225](#bib.bib225)]尝试通过操控图结构来规避检测，并将这种攻击形式化为一个基于图的优化问题。
- en: 7.2.10 Attacks against other models
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.10 针对其他模型的攻击
- en: There is furthermore research besides CNNs, RNNs, GNNs, such as generative model,
    reinforcement learning and some machine learning algorithms. Mei et al. [[152](#bib.bib152)]
    identified the optimal training set attack for SVM, logistic regression, and linear
    regression. They proved the optimal attack can be described as a bilevel optimization
    problem, which can be solved by gradient methods. Chen et al. [[47](#bib.bib47)]
    prove that tree-based models are also vulnerable to AEs. Huang et al. [[94](#bib.bib94)]
    and Gleave et al. [[71](#bib.bib71)] demonstrate that adversarial attack policies
    are also effective in reinforcement learning. Kos et al. [[114](#bib.bib114)]
    attempted to produce AEs using deep generative models such as variational autoencoder.
    Their methods include a classifier-based attack, and an attack on latent space.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 除了CNNs、RNNs和GNNs，研究还包括生成模型、强化学习和一些机器学习算法。梅等人[[152](#bib.bib152)]确定了SVM、逻辑回归和线性回归的最佳训练集攻击。他们证明了最佳攻击可以描述为一个双层优化问题，可以通过梯度方法解决。陈等人[[47](#bib.bib47)]证明了基于树的模型也容易受到对抗样本的攻击。黄等人[[94](#bib.bib94)]和格里夫等人[[71](#bib.bib71)]展示了对抗攻击策略在强化学习中的有效性。科斯等人[[114](#bib.bib114)]尝试使用深度生成模型如变分自编码器产生对抗样本。他们的方法包括基于分类器的攻击和对潜在空间的攻击。
- en: 'TABLE VI: Evaluation on adversarial attacks. This table presents “Success Rate”
    of these attacks in specific “Dataset” with varying target “System” and “Model”.
    “Distance” implies how these works measure the distance between samples. “Real-world”
    is used to distinguish the works that are also suitable for physical adversarial
    attacks. “Knowledge” is valued either black-box or white-box. “Iterative” illustrates
    whether the optimization steps are iterative. “Targeted” differs whether an attack
    is a targeted attack or not. “Application” covers the practical areas.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 对抗攻击评估。该表呈现了这些攻击在特定“数据集”中的“成功率”，针对不同的“系统”和“模型”。“距离”表示这些工作如何衡量样本之间的距离。“现实世界”用于区分那些也适用于物理对抗攻击的工作。“知识”分为黑盒或白盒。“迭代”说明优化步骤是否是迭代的。“有针对性”区分攻击是否有明确目标。“应用”涵盖了实际应用领域。'
- en: '| Paper | Success Rate | Dataset | System | Distance | Model | Real-world |
    Knowledge | Iterative | Targeted | Application |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 成功率 | 数据集 | 系统 | 距离 | 模型 | 现实世界 | 知识 | 迭代 | 有针对性 | 应用 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| L-BFGS[[213](#bib.bib213)] | 20.3% | MNIST | FC100-100-10 | $L_{2}$ | DNN
    | No | White | Yes | Yes | image |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| L-BFGS[[213](#bib.bib213)] | 20.3% | MNIST | FC100-100-10 | $L_{2}$ | DNN
    | 否 | 白盒 | 是 | 是 | 图像 |'
- en: '| FGSM[[74](#bib.bib74)] | 55.4% | MNIST | a shallow RBF network | $L_{\infty}$
    | DNN | No | White | No | No | image |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| FGSM[[74](#bib.bib74)] | 55.4% | MNIST | 一个浅层RBF网络 | $L_{\infty}$ | DNN |
    否 | 白盒 | 否 | 否 | 图像 |'
- en: '| BIM[[15](#bib.bib15)] | 24% | ImageNet [[2](#bib.bib2)] | Inception v3 |
    $L_{\infty}$ | CNN | Yes | White | Yes | No | image |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| BIM[[15](#bib.bib15)] | 24% | ImageNet [[2](#bib.bib2)] | Inception v3 |
    $L_{\infty}$ | CNN | 是 | 白盒 | 是 | 否 | 图像'
- en: '| MI-FGSM [[57](#bib.bib57)] | 37.6% | ImageNet | Inception v3 | $L_{\infty}$
    | CNN | No | White | Yes | Both | image |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| MI-FGSM [[57](#bib.bib57)] | 37.6% | ImageNet | Inception v3 | $L_{\infty}$
    | CNN | 否 | 白盒 | 是 | 两者 | 图像 |'
- en: '| JSMA[[175](#bib.bib175)] | 97.05% | MNIST | LeNet | $L_{0}$ | CNN | No |
    White | Yes | Yes | image |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| JSMA [[175](#bib.bib175)] | 97.05% | MNIST | LeNet | $L_{0}$ | CNN | 否 |
    白色 | 是 | 是 | 图像 |'
- en: '| C&W [[43](#bib.bib43)] | 100% | ImageNet | Inception v3 | $L_{0},L_{2},L_{\infty}$
    | CNN | No | White | Yes | Yes | image |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| C&W [[43](#bib.bib43)] | 100% | ImageNet | Inception v3 | $L_{0},L_{2},L_{\infty}$
    | CNN | 否 | 白色 | 是 | 是 | 图像 |'
- en: '| EAD[[48](#bib.bib48)] | 100% | ImageNet | Inception v3 | $L_{1},L_{2},L_{\infty}$
    | CNN | No | White | Yes | Yes | image |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| EAD [[48](#bib.bib48)] | 100% | ImageNet | Inception v3 | $L_{1},L_{2},L_{\infty}$
    | CNN | 否 | 白色 | 是 | 是 | 图像 |'
- en: '| OptMargin[[87](#bib.bib87)] | 100% | CIFAR-10 | ResNet | $L_{0},L_{2},L_{\infty}$
    | CNN | No | White | Yes | No | image |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| OptMargin [[87](#bib.bib87)] | 100% | CIFAR-10 | ResNet | $L_{0},L_{2},L_{\infty}$
    | CNN | 否 | 白色 | 是 | 否 | 图像 |'
- en: '| Guo et al. [[77](#bib.bib77)] | 95.5% | ImageNet | ResNet-50 | $L_{2}$ |
    CNN | No | Both | Yes | No | image |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Guo et al. [[77](#bib.bib77)] | 95.5% | ImageNet | ResNet-50 | $L_{2}$ |
    CNN | 否 | 两者 | 是 | 否 | 图像 |'
- en: '| Deepfool [[156](#bib.bib156)] | 68.7% | ILSVRC2012 | GoogLeNet | $L_{2}$
    | CNN | No | White | Yes | No | image |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Deepfool [[156](#bib.bib156)] | 68.7% | ILSVRC2012 | GoogLeNet | $L_{2}$
    | CNN | 否 | 白色 | 是 | 否 | 图像 |'
- en: '| NewtonFool[[102](#bib.bib102)] | 81.63% | GTSRB [[4](#bib.bib4)] | CNN(3Conv+1FC)
    | $L_{2}$ | CNN | No | White | Yes | No | image |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| NewtonFool [[102](#bib.bib102)] | 81.63% | GTSRB [[4](#bib.bib4)] | CNN(3Conv+1FC)
    | $L_{2}$ | CNN | 否 | 白色 | 是 | 否 | 图像 |'
- en: '| UAP[[155](#bib.bib155)] | 90.7% | ILSVRC2012 | VGG-16 | $L_{2},L_{\infty}$
    | CNN | No | White | Yes | No | image |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| UAP [[155](#bib.bib155)] | 90.7% | ILSVRC2012 | VGG-16 | $L_{2},L_{\infty}$
    | CNN | 否 | 白色 | 是 | 否 | 图像 |'
- en: '| UAN[[85](#bib.bib85)] | 91.8% | ImageNet | ResNet-152 | $L_{2},L_{\infty}$
    | CNN | No | White | Yes | Yes | image |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| UAN [[85](#bib.bib85)] | 91.8% | ImageNet | ResNet-152 | $L_{2},L_{\infty}$
    | CNN | 否 | 白色 | 是 | 是 | 图像 |'
- en: '| ATN[[24](#bib.bib24)] | 89.2% | MNIST | CNN(3Conv+1FC) | $L_{2}$ | CNN |
    No | White | Yes | Yes | image |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| ATN [[24](#bib.bib24)] | 89.2% | MNIST | CNN(3Conv+1FC) | $L_{2}$ | CNN |
    否 | 白色 | 是 | 是 | 图像 |'
- en: '| Athalye et al. [[20](#bib.bib20)] | 83.4% | 3D-printed turtle | Inception-v3
    | $L_{2}$ | CNN | Yes | White | No | Yes | image |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Athalye et al. [[20](#bib.bib20)] | 83.4% | 3D-printed turtle | Inception-v3
    | $L_{2}$ | CNN | 是 | 白色 | 否 | 是 | 图像 |'
- en: '| Ilyas et al. [[98](#bib.bib98)] | 99.2% | ImageNet | Inception-v3 | - | CNN
    | No | Black | No | Both | image |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Ilyas et al. [[98](#bib.bib98)] | 99.2% | ImageNet | Inception-v3 | - | CNN
    | 否 | 黑色 | 否 | 两者 | 图像 |'
- en: '| Narodytska et al. [[159](#bib.bib159)] | 97.51% | CIFAR-10 | VGG | $L_{0}$
    | CNN | No | Black | No | No | image |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Narodytska et al. [[159](#bib.bib159)] | 97.51% | CIFAR-10 | VGG | $L_{0}$
    | CNN | 否 | 黑色 | 否 | 否 | 图像 |'
- en: '| Kos et al. [[114](#bib.bib114)] | 76% | MNIST | VAE-GAN | $L_{2}$ | GAN |
    No | White | No | Yes | image |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Kos et al. [[114](#bib.bib114)] | 76% | MNIST | VAE-GAN | $L_{2}$ | GAN |
    否 | 白色 | 否 | 是 | 图像 |'
- en: '| Mei et al. [[152](#bib.bib152)] | - | - | - | $L_{2}$ | SVM | No | Black
    | Yes | No | image |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Mei et al. [[152](#bib.bib152)] | - | - | - | $L_{2}$ | SVM | No | 黑色 | 是
    | 否 | 图像 |'
- en: '| Huang et al. [[94](#bib.bib94)] | - | - | A3C,TRPO,DQN | $L_{1},L_{2},L_{\infty}$
    | RL | No | Both | No | No | image |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al. [[94](#bib.bib94)] | - | - | A3C,TRPO,DQN | $L_{1},L_{2},L_{\infty}$
    | RL | 否 | 两者 | 否 | 否 | 图像 |'
- en: '| Papernot et al. [[177](#bib.bib177)] | 100% | Reviews | LSTM | $L_{2}$ |
    RNN | Yes | White | No | No | text |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Papernot et al. [[177](#bib.bib177)] | 100% | Reviews | LSTM | $L_{2}$ |
    RNN | 是 | 白色 | 否 | 否 | 文本'
- en: '| DeepWordBug [[67](#bib.bib67)] | 51.80% | IMDB Review [[7](#bib.bib7)] |
    LSTM | $L_{0}$ | RNN | Yes | Black | Yes | Yes | text |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| DeepWordBug [[67](#bib.bib67)] | 51.80% | IMDB Review [[7](#bib.bib7)] |
    LSTM | $L_{0}$ | RNN | 是 | 黑色 | 是 | 是 | 文本 |'
- en: '| DeepSpeech [[44](#bib.bib44)] | 100% | Mozilla Common Voice [[9](#bib.bib9)]
    | LSTM | $L_{\infty}$ | RNN | No | White | No | Yes | speech |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| DeepSpeech [[44](#bib.bib44)] | 100% | Mozilla Common Voice [[9](#bib.bib9)]
    | LSTM | $L_{\infty}$ | RNN | 否 | 白色 | 否 | 是 | 语音 |'
- en: '| Gong et al. [[73](#bib.bib73)] | 72% | IEMOCAP | LSTM | $L_{2}$ | RNN | Yes
    | White | No | No | speech |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Gong et al. [[73](#bib.bib73)] | 72% | IEMOCAP | LSTM | $L_{2}$ | RNN | 是
    | 白色 | 否 | 否 | 语音 |'
- en: '| CommanderSong[[262](#bib.bib262)] | 96% | Fisher | ASplRE Chain Model | $L_{1}$
    | RNN | Yes | White | No | Yes | speech |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| CommanderSong [[262](#bib.bib262)] | 96% | Fisher | ASplRE Chain Model |
    $L_{1}$ | RNN | 是 | 白色 | 否 | 是 | 语音 |'
- en: '| Rosenberg et al. [[194](#bib.bib194)] | 99.99% | 500000 files | LSTM | $L_{2}$
    | RNN | Yes | Black | Yes | No | malware |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Rosenberg et al. [[194](#bib.bib194)] | 99.99% | 500000 files | LSTM | $L_{2}$
    | RNN | 是 | 黑色 | 是 | 否 | 恶意软件 |'
- en: '| MtNet[[95](#bib.bib95)] | 97% | 4500000 files | DNN(4 Hidden layers) | $L_{2}$
    | DNN | Yes | Black | No | No | malware |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| MtNet [[95](#bib.bib95)] | 97% | 4500000 files | DNN(4 Hidden layers) | $L_{2}$
    | DNN | 是 | 黑色 | 否 | 否 | 恶意软件 |'
- en: '| SLEIPNIR [[14](#bib.bib14)] | 99.7% | 55000 PEs | DNN | $L_{2},L_{\infty}$
    | DNN | Yes | Black | No | No | malware |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| SLEIPNIR [[14](#bib.bib14)] | 99.7% | 55000 PEs | DNN | $L_{2},L_{\infty}$
    | DNN | 是 | 黑色 | 否 | 否 | 恶意软件 |'
- en: '| Rigaki et al. [[190](#bib.bib190)] | 63% | - | GAN | $L_{0}$ | GAN | Yes
    | Black | No | No | malware |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| Rigaki et al. [[190](#bib.bib190)] | 63% | - | GAN | $L_{0}$ | GAN | 是 |
    黑色 | 否 | 否 | 恶意软件 |'
- en: '| Pascanu et al. [[179](#bib.bib179)] | 69% | DREBIN [[1](#bib.bib1)] | DNN
    | $L_{1}$ | DNN | Yes | Black | No | No | malware |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Pascanu et al. [[179](#bib.bib179)] | 69% | DREBIN [[1](#bib.bib1)] | DNN
    | $L_{1}$ | DNN | 是 | 黑色 | 否 | 否 | 恶意软件 |'
- en: '| Kreuk et al. [[116](#bib.bib116)] | 88% | Microsoft Malware [[8](#bib.bib8)]
    | CNN | $L_{2},L_{\infty}$ | CNN | Yes | White | No | Yes | malware |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Kreuk et al. [[116](#bib.bib116)] | 88% | Microsoft Malware [[8](#bib.bib8)]
    | CNN | $L_{2},L_{\infty}$ | CNN | 是 | 白色 | 否 | 是 | 恶意软件 |'
- en: '| Hu et al. [[90](#bib.bib90)] | 90.05% | 180 programs | BiLTSM | $L_{1}$ |
    RNN | Yes | Black | Yes | No | malware |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Hu et al. [[90](#bib.bib90)] | 90.05% | 180 程序 | BiLTSM | $L_{1}$ | RNN |
    是 | 黑色 | 是 | 否 | 恶意软件 |'
- en: '| Hu et al. [[91](#bib.bib91)] | 99.80% | 180000 programs | MalGAN | $L_{1}$
    | GAN | Yes | Black | No | No | malware |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Hu et al. [[91](#bib.bib91)] | 99.80% | 180000 程序 | MalGAN | $L_{1}$ | GAN
    | 是 | 黑色 | 否 | 否 | 恶意软件 |'
- en: 7.3 Analysis of Adversarial Attack
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 对抗性攻击分析
- en: 'In conclusion, we have surveyed 66 adversarial attack papers, and measured
    33 related papers in Table [VI](#S7.T6 "TABLE VI ‣ 7.2.10 Attacks against other
    models ‣ 7.2 Adversarial Attack Approach ‣ 7 Adversarial Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey"), and identified following observations.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，我们调查了66篇对抗性攻击的论文，并在表 [VI](#S7.T6 "TABLE VI ‣ 7.2.10 Attacks against other
    models ‣ 7.2 Adversarial Attack Approach ‣ 7 Adversarial Attack ‣ Towards Security
    Threats of Deep Learning Systems: A Survey")中测量了33篇相关论文，并得出了以下观察结果。'
- en: Finding 12. AEs may be inevitable in high-dimensional classifiers under the
    computational limitations.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 12. 在计算限制下，高维分类器中对抗性样本可能是不可避免的。
- en: Many classifiers are found to be vulnerable to adversarial attacks. Besides
    the most commonly attacked CNNs in image classification, RNNs are also vulnerable
    in such text processing and malware detection fields. With the development of
    GNNs, they also suffer from adversarial attacks (5/66). SVM [[152](#bib.bib152)],
    reinforcement learning [[94](#bib.bib94)][[71](#bib.bib71)], generative models [[114](#bib.bib114)]
    are all proved to be attacked. The reason why high-dimensional classifiers suffer
    from AEs may be that computational constraints and input data limitations make
    it difficult to restore the decision boundaries. AEs may be an inevitable byproduct
    of the computational constraints of learning algorithms [[36](#bib.bib36)]. Dohmatob
    et al. [[56](#bib.bib56)] give a theoretical proof that once the perturbations
    are slightly larger than the natural noise level, any classifier can be adversarially
    deceived with high probability.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分类器被发现对对抗性攻击有脆弱性。除了在图像分类中最常受到攻击的CNN外，RNN在文本处理和恶意软件检测领域也很脆弱。随着GNN的出现，它们也遭受对抗性攻击（5/66）。SVM [[152](#bib.bib152)]、强化学习 [[94](#bib.bib94)][[71](#bib.bib71)]、生成模型 [[114](#bib.bib114)]都被证明会受到攻击。高维分类器遭受对抗样本的原因可能是计算约束和输入数据的限制使得恢复决策边界变得困难。对抗样本可能是学习算法计算约束的一个不可避免的副产品 [[36](#bib.bib36)]。Dohmatob
    et al. [[56](#bib.bib56)]提供了一个理论证明，一旦扰动稍大于自然噪声水平，任何分类器都有很高的概率被对抗性欺骗。
- en: Finding 13. Adversarial samples widely exist in the samples space of various
    fields.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 13. 对抗样本在各个领域的样本空间中广泛存在。
- en: Adversarial attacks have penetrated into many fields. In our 66 surveyed papers,
    28 papers focus on image classification, 6 papers focus on speech recognition,
    9 papers attack text processing, 8 papers attack malware detection. Whether the
    sample space is a discrete domain (text or malware) or a continuous domain (speech),
    whether the input is a definite size (image) or an indefinite size (text or speech),
    adversarial examples are all widespread. In the entire sample space, adversarial
    samples and normal samples are likely to be in a symbiotic relationship.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击已经渗透到许多领域。在我们调查的66篇论文中，28篇专注于图像分类，6篇专注于语音识别，9篇攻击文本处理，8篇攻击恶意软件检测。不论样本空间是离散领域（文本或恶意软件）还是连续领域（语音），无论输入是确定大小（图像）还是不确定大小（文本或语音），对抗样本都普遍存在。在整个样本空间中，对抗样本和正常样本可能处于一种共生关系。
- en: Finding 14. Physical attacks bring the harm of adversarial samples to a new
    level.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 14. 物理攻击将对抗样本的危害提升到一个新水平。
- en: AEs in the digital space may fail to fool classifiers in the physical space
    because physical attacks need to consider more environmental factors. Recently,
    in the image field, real-world attack studies become more according to our research
    (6/15 in 2019 and only 2/20 in previous years). Physical attack needs to consider
    photographing viewpoints, environmental lighting, camera noise and so on. This
    causes many previous studies only worked at the digital space. As the technology
    matures, more physical attacks are being studied. Physical attacks are more harmful
    to us, such as traffic signs that truly fool object detectors [[274](#bib.bib274)],
    and voices that actually fool smart speakers [[262](#bib.bib262)]. Physical attacks
    still need more in-depth research, which will also lead to security research in
    real AI systems. Besides, physical problem does not exist in text or malware field,
    so we give them all “Yes” in “Real-world”.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 数字空间中的对抗样本可能无法欺骗物理空间中的分类器，因为物理攻击需要考虑更多环境因素。最近，在图像领域，真实世界的攻击研究逐渐增多，根据我们的研究（2019年为6/15，之前几年仅为2/20）。物理攻击需要考虑拍摄视角、环境光照、相机噪声等。这导致许多以前的研究仅在数字空间中有效。随着技术的成熟，更多的物理攻击正在被研究。物理攻击对我们更具危害性，例如，交通标志真正欺骗了物体检测器[[274](#bib.bib274)]，以及声音真正欺骗了智能扬声器[[262](#bib.bib262)]。物理攻击仍需深入研究，这也将引发对真实AI系统的安全研究。此外，文本或恶意软件领域不存在物理问题，因此我们在“真实世界”中对它们全都标记为“是”。
- en: Finding 15. Untargeted adversarial attacks (57.6%) are easier to achieve but
    less severe than targeted adversarial attacks.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 发现15. 未针对性对抗攻击（57.6%）更容易实现，但比针对性对抗攻击的严重性要低。
- en: Untargeted attacks aim at inducing wrong predictions, and thus more flexible
    in finding perturbations which only need smaller modifications. Therefore, it
    can achieve success more easily. Targeted attacks have to make the model predict
    what as expected. Therefore, much more perturbations need to be created for accomplishing
    the target. However, they are usually more harmful and practical in reality. For
    example, attackers may disguise themselves as authenticated users in a face recognition
    system, in order to gain the access to privileged resources.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 未针对性攻击旨在诱导错误预测，因此在寻找扰动时更具灵活性，只需较小的修改即可。因此，它们更容易取得成功。针对性攻击则必须使模型按预期进行预测。因此，需要创建更多的扰动以实现目标。然而，它们通常在现实中更具危害性和实用性。例如，攻击者可能伪装成经过认证的用户，以获得对特权资源的访问权限。
- en: Finding 16. Almost all attacks adopt $L_{p}$-distance, including $L_{0},L_{1},L_{2},L_{\infty}$,
    while $L_{2}$ distance is the most widely used.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 发现16. 几乎所有攻击都采用$L_{p}$-距离，包括$L_{0},L_{1},L_{2},L_{\infty}$，其中$L_{2}$距离使用最为广泛。
- en: 'Distance metrics is an important factor to find minimum perturbations, which
    mostly use $L_{p}$-distance currently. In “Distance” column of Table [VI](#S7.T6
    "TABLE VI ‣ 7.2.10 Attacks against other models ‣ 7.2 Adversarial Attack Approach
    ‣ 7 Adversarial Attack ‣ Towards Security Threats of Deep Learning Systems: A
    Survey"), 60.1% attacks use $L_{2}$ distance, 36.4% use $L_{\infty}$ distance,
    18.2% use $L_{1}$ distance and 18.2% use $L_{0}$ distance. Considering image classification
    only, 70% attacks use $L_{2}$ distance, 45% use $L_{\infty}$ distance, 10% use
    $L_{1}$ distance and 20% use $L_{0}$ distance.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '距离度量是寻找最小扰动的重要因素，目前大多数使用$L_{p}$-距离。在表[VI](#S7.T6 "TABLE VI ‣ 7.2.10 Attacks
    against other models ‣ 7.2 Adversarial Attack Approach ‣ 7 Adversarial Attack
    ‣ Towards Security Threats of Deep Learning Systems: A Survey")的“距离”列中，60.1%的攻击使用$L_{2}$距离，36.4%使用$L_{\infty}$距离，18.2%使用$L_{1}$距离和18.2%使用$L_{0}$距离。仅考虑图像分类，70%的攻击使用$L_{2}$距离，45%使用$L_{\infty}$距离，10%使用$L_{1}$距离和20%使用$L_{0}$距离。'
- en: $L_{0}$ distance reflects the number of changed elements, but it is unable to
    limit the variation of each element. It suits the scenes that only care about
    the number of perturbation pixels, but not variation size. $L_{1}$ distance is
    the absolute values summation of every element in perturbations, equivalent to
    Manhattan distance in 2D space. It limits the sum of all variations, but does
    not limit large perturbation of individual elements. $L_{\infty}$ distance does
    not care about how many elements have been changed, but only cares about the maximum
    of perturbations, equivalent to Chebyshev distance in 2D space. $L_{2}$ distance
    is an Euclidean distance that considers all pixel perturbation, which is a more
    balanced and the most widespread metric. It takes into account both the largest
    perturbation and the number of changed elements.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: $L_{0}$ 距离反映了更改元素的数量，但无法限制每个元素的变化。它适用于只关心扰动像素数量而不考虑变化大小的场景。$L_{1}$ 距离是扰动中每个元素绝对值的总和，相当于二维空间中的曼哈顿距离。它限制了所有变化的总和，但不限制个别元素的大幅扰动。$L_{\infty}$
    距离不关心改变了多少个元素，只关心扰动的最大值，相当于二维空间中的切比雪夫距离。$L_{2}$ 距离是考虑所有像素扰动的欧几里得距离，是一种更平衡且最广泛使用的度量。它考虑了最大扰动和更改的元素数量。
- en: Finding 17. Different positions should have different weights for perturbations.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 发现17. 不同的位置应对扰动有不同的权重。
- en: In the current measurement methods, the perturbations of different elements
    are considered to have the same weight. However, in face images, the same perturbations
    applied on the important part of face such as nose, eyes and mouth, will be easier
    to identify than that applied on the background. Similarly, in audio analysis,
    perturbations are difficult to be noticed in a chaotic scene, but are easily perceived
    in a quiet scene. According to the above analysis, we can consider to adopt different
    weights on different elements when measuring distance. The important part has
    a larger weight, so it can only make smaller perturbations, while the unimportant
    part has a smaller weight, which can introduce larger perturbations.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的测量方法中，不同元素的扰动被认为具有相同的权重。然而，在人脸图像中，对鼻子、眼睛和嘴巴等重要部分应用的相同扰动会比对背景应用的扰动更容易被识别。类似地，在音频分析中，扰动在混乱的场景中难以察觉，但在安静的场景中容易被感知。根据上述分析，我们可以考虑在测量距离时对不同元素采用不同的权重。重要部分权重大，因此只能引入较小的扰动，而不重要的部分权重小，因此可以引入较大的扰动。
- en: Finding 18. More advanced measurements for the human perception are desired.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 发现18. 需要更先进的人类感知测量方法。
- en: The original goal of AEs is to make the model classify samples wrongly while
    keeping humans unaware of the differences. However, it is difficult to measure
    humans’ perception of these perturbations. Intuitively, small $L_{p}$ distance
    implies a low probability of being detected by humans. While recent work found
    that $L_{p}$ distance is neither necessary nor sufficient for perceptual similarity [[201](#bib.bib201)].
    That is, perturbations with large $L_{p}$ values may also look similar to humans,
    such as translations and rotations of images, and small $L_{p}$ perturbations
    do not mean imperceptible. Research [[62](#bib.bib62)] also proves that neural
    network-based classifiers are vulnerable to rotations and translations. In a recent
    paper, Bhattad et al. [[30](#bib.bib30)] introduce unrestricted perturbations
    to generate effective and realistic AEs. Therefore, we should break the constraint
    of $L_{p}$ distance. How to search for AEs systematically without $L_{p}$ limitation,
    and how to propose new measurements that could be necessary or sufficient for
    perceptual similarity, will be a trend of adversarial attack in the near future.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本（AEs）的最初目标是让模型错误分类样本，同时保持人类对差异的无感知。然而，测量人类对这些扰动的感知是困难的。直观上，较小的 $L_{p}$ 距离意味着被人类检测到的概率较低。然而，最近的研究发现
    $L_{p}$ 距离既不是感知相似性的必要条件，也不是充分条件[[201](#bib.bib201)]。也就是说，具有大 $L_{p}$ 值的扰动也可能对人类看起来相似，例如图像的平移和旋转，而小
    $L_{p}$ 扰动并不意味着不可察觉。研究[[62](#bib.bib62)] 还证明了基于神经网络的分类器对旋转和平移很脆弱。在最近的一篇论文中，Bhattad
    等人[[30](#bib.bib30)] 引入了无限制扰动来生成有效且真实的对抗样本。因此，我们应该打破 $L_{p}$ 距离的限制。如何在没有 $L_{p}$
    限制的情况下系统地寻找对抗样本，以及如何提出可能是感知相似性所必需或足够的新测量，将成为未来对抗攻击的趋势。
- en: 8 Discussion
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论
- en: In this section, we summarize 7 observations according to the survey as follows.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了根据调查得到的7个观察结果，如下所示。
- en: 8.1 Regulations on privacy protection
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 隐私保护的法规
- en: 'As shown in Section [4](#S4 "4 Model Extraction Attack ‣ Towards Security Threats
    of Deep Learning Systems: A Survey") and [5](#S5 "5 Model Inversion Attack ‣ Towards
    Security Threats of Deep Learning Systems: A Survey"), both the enterprises and
    users are suffering from the risk of privacy. In addition to removing privacy
    in the data, governments and related organizations can issue laws and regulations
    against privacy violations in the course of data use and transmission. In particular,
    it is recommended that: 1) introducing regulatory authorities to monitor these
    deep learning systems and strictly supervise the use of data. The involved systems
    are only allowed to extract features and predict results within the permitted
    range. The private information is forbidden for being extracted and inferred without
    authorization. 2) establishing and improving relevant laws and regulations (e.g.,
    GDPR [[3](#bib.bib3)]), for supervising the process of data collection, use, storage
    and deletion. 3) adding digital watermarks into the data for leak source tracking [[21](#bib.bib21)].
    The watermarks help to fast find out the rule breakers that are liable for exposing
    privacy.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[4](#S4 "4 模型提取攻击 ‣ 深度学习系统的安全威胁：调查")和第[5](#S5 "5 模型反演攻击 ‣ 深度学习系统的安全威胁：调查")节所示，企业和用户都面临隐私风险。除了在数据中去除隐私外，政府及相关组织可以制定法律法规以防止在数据使用和传输过程中侵犯隐私。特别是，建议：1)
    引入监管机构来监控这些深度学习系统，并严格监督数据的使用。这些系统只允许在许可范围内提取特征和预测结果。未经授权，禁止提取和推断私人信息。2) 制定和完善相关法律法规（如GDPR
    [[3](#bib.bib3)]），以监督数据收集、使用、存储和删除的过程。3) 在数据中添加数字水印以跟踪泄露源[[21](#bib.bib21)]。水印有助于快速找到违反规则并导致隐私泄露的责任方。
- en: 8.2 Secure implementation of deep learning systems
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 深度学习系统的安全实现
- en: Most of the research on deep learning security is concentrating on the leak
    of private data and the correctness of classification. As a software system, deep
    learning can be easily built on mature frameworks such as TensorFlow, Torch or
    Caffe. The vulnerabilities residing in these frameworks can make the constructed
    deep learning systems vulnerable to other types of attacks. The work [[245](#bib.bib245)]
    enumerates the security issues such as *heap overflow*, *integer overflow* and
    *use after free* in these widespread frameworks. These vulnerabilities can result
    in denial of service, control-flow hijacking or system compromise. Moreover, deep
    learning systems often depend on third-party libraries to provide auxiliary functions.
    For instance, OpenCV is commonly used to process images, and Sound eXchange (SoX)
    is oftentimes used for audios. Once the vulnerabilities are exploited, the attacker
    can cause more severe losses to deep learning systems. Therefore, the security
    auditing of deep learning implementation deserves more research attention and
    efforts in the further work.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习安全研究集中在私人数据泄露和分类正确性上。作为一种软件系统，深度学习可以轻松地构建在成熟的框架上，如TensorFlow、Torch或Caffe。这些框架中的漏洞可能使构建的深度学习系统容易受到其他类型攻击。相关工作[[245](#bib.bib245)]列举了这些广泛使用框架中的安全问题，如*堆溢出*、*整数溢出*和*使用后释放*。这些漏洞可能导致服务拒绝、控制流劫持或系统妥协。此外，深度学习系统通常依赖第三方库提供辅助功能。例如，OpenCV常用于处理图像，Sound
    eXchange (SoX)常用于处理音频。一旦这些漏洞被利用，攻击者可能会对深度学习系统造成更严重的损失。因此，深度学习实现的安全审计值得进一步的研究关注和努力。
- en: On the other hand, there are emerging a large number of research works that
    leverage deep learning to detect and exploit software vulnerabilities automatically [[260](#bib.bib260)][[252](#bib.bib252)][[101](#bib.bib101)][[209](#bib.bib209)].
    It is believed that these techniques are also applicable in deep learning systems.
    Even more, deep learning might help uncover the interpretation and fix the classification
    vulnerabilities in future.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，越来越多的研究工作利用深度学习自动检测和利用软件漏洞[[260](#bib.bib260)][[252](#bib.bib252)][[101](#bib.bib101)][[209](#bib.bib209)]。这些技术也被认为适用于深度学习系统。更重要的是，深度学习可能有助于揭示解释和修复未来的分类漏洞。
- en: 8.3 How far away from a complete black-box attack?
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 离完全的黑箱攻击还有多远？
- en: Black-box attacks are relatively more destructive as they do not require much
    information about the target which lowers the cost of attack. Many works are claiming
    they are performing black-box attacks towards deep learning systems [[203](#bib.bib203)][[198](#bib.bib198)][[100](#bib.bib100)].
    But it is not clear that whether they are feasible on a large number of models
    and systems, and what is the gap between these works with the real world attack.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 黑箱攻击相对更具破坏性，因为它们不需要太多关于目标的信息，这降低了攻击成本。许多研究声称它们在对深度学习系统执行黑箱攻击[[203](#bib.bib203)][[198](#bib.bib198)][[100](#bib.bib100)]。但目前尚不清楚这些攻击是否对大量模型和系统可行，以及这些研究与实际世界攻击之间的差距。
- en: According to the surveyed results, we find that many black-box attacks still
    assume that some information is accessible. For example, [[220](#bib.bib220)]
    has to know what exact model is running as well as its model structure before
    successfully stealing out the model parameters. [[203](#bib.bib203)] conducts
    a membership inference attack built on the fact that the statistics of training
    data is publicly known and similar data with the same distribution can be easily
    synthesized. However, these conditions may be difficult to satisfy the real world,
    and a complete black-box attack is rarely seen in the recent research.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 根据调查结果，我们发现许多黑箱攻击仍然假设可以访问某些信息。例如，[[220](#bib.bib220)]必须知道正在运行的确切模型以及其模型结构，才能成功窃取模型参数。[[203](#bib.bib203)]进行的成员推断攻击建立在训练数据统计信息公开且可以轻易合成相同分布的数据的事实基础上。然而，这些条件在现实世界中可能难以满足，最近的研究中很少见到完全的黑箱攻击。
- en: 'Another difficulty of a complete black-box attack stems from the protection
    measures performed by deep learning systems: 1) *query limit*. Commercial deep
    learning systems usually set a limit for service requests that prevents substitute
    model training. In [[107](#bib.bib107)], PRADA can detect model extraction attacks
    based on characteristic distribution of queries. 2) *uncharted defense deployment*.
    Besides not fully tangible models, a black-box attacker also cannot infer how
    the defense is deployed and configured at the backend. These defenses may block
    a malicious request [[154](#bib.bib154)][[148](#bib.bib148)], create misleading
    results [[107](#bib.bib107)] and dynamically change or enhance their abilities [[226](#bib.bib226)][[220](#bib.bib220)].
    Due to the extreme imbalance of knowledge between attackers and defenders, all
    of the above measures can avoid black-box attacks efficiently and effectively.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 完全黑箱攻击的另一个难点来自于深度学习系统实施的保护措施：1) *查询限制*。商业深度学习系统通常会对服务请求设置限制，以防止替代模型训练。在[[107](#bib.bib107)]中，PRADA可以基于查询的特征分布检测模型提取攻击。2)
    *未被详细描述的防御部署*。除了模型不完全可触及外，黑箱攻击者还无法推测防御措施如何在后台部署和配置。这些防御可能阻挡恶意请求[[154](#bib.bib154)][[148](#bib.bib148)]，产生误导性结果[[107](#bib.bib107)]，并动态地改变或增强其能力[[226](#bib.bib226)][[220](#bib.bib220)]。由于攻击者和防御者之间知识的极端不平衡，以上所有措施可以有效地避免黑箱攻击。
- en: 8.4 Relationship between interpretability and security
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 解释性与安全性之间的关系
- en: The development of interpretability can help us better understand the underlying
    principles of all these attacks. Since the neural network was born, it has the
    problem of low interpretability. A small change of model parameters may affect
    the prediction results drastically. People also cannot directly understand how
    neural network operates. Recently, interpretability has become an urgent field
    in deep learning. In May of 2018, GDPR is announced to protect the privacy of
    personal data and it requires interpretability when using AI algorithms [[3](#bib.bib3)].
    How to deeply understand the neural network itself, and explain how the output
    is affected by the input are all problems that need to be solved urgently.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性的进展可以帮助我们更好地理解所有这些攻击的潜在原理。自神经网络诞生以来，它就存在解释性差的问题。模型参数的微小变化可能会极大地影响预测结果。人们也无法直接理解神经网络是如何操作的。最近，解释性已成为深度学习领域的一个紧迫问题。2018年5月，GDPR宣布保护个人数据隐私，并要求在使用AI算法时具备解释性[[3](#bib.bib3)]。如何深入理解神经网络本身，并解释输出如何受到输入的影响，都是需要紧急解决的问题。
- en: Interpretability mainly refers to the ability to explain the logic behind every
    decision/judgment made by AI and how to trust these decisions [[222](#bib.bib222)].
    It mainly includes rationality, traceability, and understandability [[109](#bib.bib109)].
    Rationality means being able to understand the reasoning behind each prediction.
    Traceability refers to the ability to track predictive processes, which can be
    derived from the logic of mathematical algorithms [[110](#bib.bib110)][[233](#bib.bib233)].
    Understandability refers to a complete understanding of the model on which decisions
    are based.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性主要指的是解释 AI 做出每个决策/判断背后的逻辑能力以及如何信任这些决策 [[222](#bib.bib222)]。它主要包括理性、可追溯性和可理解性 [[109](#bib.bib109)]。理性意味着能够理解每个预测背后的推理。可追溯性指的是能够跟踪预测过程，这可以从数学算法的逻辑中得出 [[110](#bib.bib110)][[233](#bib.bib233)]。可理解性指的是对决策所依据模型的全面理解。
- en: At present, some work is being conducted on security and robustness proofs,
    usually against adversarial attack [[233](#bib.bib233)]. Deeper work requires
    to explain the reasons for prediction results, making training and prediction
    processes are no longer in black-box.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，一些工作正在进行关于安全性和鲁棒性证明的研究，通常针对对抗性攻击 [[233](#bib.bib233)]。更深入的工作需要解释预测结果的原因，使训练和预测过程不再是黑箱。
- en: Kantchelian et al. [[109](#bib.bib109)] suggested that system designers need
    to broaden the classification goal into an explanatory goal and deepen interaction
    with human operators to address the challenge of adversarial drift. Reluplex [[110](#bib.bib110)]
    can prove in which situations, small perturbations to inputs cannot cause misclassification.
    The main idea is the lazy handling of ReLU constraints. It temporarily ignores
    ReLU constraints and tries to solve the linear part of problems. As a development,
    Wang et al. [[233](#bib.bib233)] presented ReluVal to do formal security analysis
    of neural networks using symbolic intervals. They proposed a new direction for
    formally checking security properties without Satisfiability Modulo Theory. They
    leveraged symbolic interval algorithm to compute rigorous bounds on DNN outputs
    through minimizing over-estimations. $AI^{2}$ [[69](#bib.bib69)] attempts to do
    abstract interpretation in AI systems, and tries to prove the security and robustness
    of neural networks. They constructed almost all perturbations, made them propagate
    automatically, and captured the behavior of convolutional layers, max pooling
    layers and fully connected layers. They also solved the state space explosion
    problem. DeepStellar [[59](#bib.bib59)] characterizes RNN internal behaviors by
    modeling a RNN as an abstract state transition system. They design two trace similarity
    metrics to analyze RNNs quantitatively and also detect AEs with very small perturbations.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: Kantchelian 等人 [[109](#bib.bib109)] 提出系统设计师需要将分类目标拓展为解释目标，并加深与人类操作员的互动，以应对对抗性漂移的挑战。Reluplex [[110](#bib.bib110)]
    可以证明在何种情况下，对输入的小扰动不会导致误分类。其主要思想是懒惰处理 ReLU 约束。它暂时忽略 ReLU 约束，并尝试解决问题的线性部分。作为发展，Wang
    等人 [[233](#bib.bib233)] 提出了 ReluVal，用于通过符号区间进行神经网络的形式安全分析。他们提出了一种新的方向，用于在没有可满足性模理论的情况下正式检查安全属性。他们利用符号区间算法，通过最小化过高估计来计算
    DNN 输出的严格界限。$AI^{2}$ [[69](#bib.bib69)] 尝试在 AI 系统中进行抽象解释，并尝试证明神经网络的安全性和鲁棒性。他们构建了几乎所有的扰动，使其自动传播，并捕捉卷积层、最大池化层和全连接层的行为。他们还解决了状态空间爆炸问题。DeepStellar [[59](#bib.bib59)]
    通过将 RNN 建模为抽象状态转换系统来表征 RNN 内部行为。他们设计了两个跟踪相似性度量来定量分析 RNN，并且还检测非常小扰动的对抗样本。
- en: The interpretability cannot only bring security, but also uncover the mystery
    of neural network and make us understand its working mechanism easily. However,
    this is also beneficial to attackers. They can exclude the range of input proved
    secure, thus reducing the retrieval space and finding AEs more efficiently. They
    can also construct targeted attacks through an in-depth understanding on models.
    In spite of this, this field should not be stagnant. Because a black-box model
    does not guarantee security [[205](#bib.bib205)]. Therefore, with the improvement
    of interpretability, deep learning security may rise in a zigzag way.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性不仅能带来安全性，还能揭示神经网络的奥秘，使我们更容易理解其工作机制。然而，这对攻击者也是有利的。他们可以排除被证明安全的输入范围，从而减少检索空间，更有效地找到对抗样本。他们还可以通过对模型的深入理解来构建针对性的攻击。尽管如此，这一领域不应停滞不前。因为黑箱模型不能保证安全性 [[205](#bib.bib205)]。因此，随着可解释性的提高，深度学习的安全性可能会以曲折的方式上升。
- en: The development of interpretability is also conductive to solving the hysteresis
    of defensive methods. Since we have not yet achieved a deep understanding of DNN
    (it is not clear why a record is predicted to the result, and how different data
    affect model parameters), finding vulnerabilities for attack is easier than preventing
    in advance. So there is a certain lag in deep learning security. If we can understand
    models thoroughly, it is believed that defense will precede or synchronize with
    attack [[110](#bib.bib110)][[233](#bib.bib233)][[69](#bib.bib69)].
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性的发展也有助于解决防御方法的滞后问题。由于我们尚未对深度神经网络（DNN）实现深入理解（尚不清楚为什么某个记录会被预测到结果，以及不同数据如何影响模型参数），因此发现攻击的漏洞比提前防范更容易。这使得深度学习安全存在一定的滞后。如果我们能彻底理解模型，防御预计将会领先或同步于攻击 [[110](#bib.bib110)][[233](#bib.bib233)][[69](#bib.bib69)]。
- en: 8.5 Discrimination in AI
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5 AI中的歧视
- en: 'AI system may seem rational, neutral and unbiased, but actually, AI and algorithmic
    decisions can lead to unfair and discrimination [[34](#bib.bib34)]. For example,
    amazon’s AI hiring tool taught itself that male candidates were preferable [[82](#bib.bib82)].
    There are also discrimination in crime prevention, online shops [[34](#bib.bib34)],
    bank loan [[5](#bib.bib5)], and so on. There are two main reasons causing AI discrimination [[5](#bib.bib5)]:
    1) Imbalanced training data; 2)Training data reflects past discrimination.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: AI系统可能看起来是理性的、中立的且不带偏见的，但实际上，AI和算法决策可能会导致不公平和歧视 [[34](#bib.bib34)]。例如，亚马逊的AI招聘工具自我学习得出男性候选人更受青睐 [[82](#bib.bib82)]。在犯罪预防、在线商店 [[34](#bib.bib34)]、银行贷款 [[5](#bib.bib5)]等方面也存在歧视。导致AI歧视的主要原因有两个 [[5](#bib.bib5)]：1）不平衡的训练数据；2）训练数据反映了过去的歧视。
- en: 'In order to solve this problem and make AI system better benefit humans, what
    we need to do is: 1) balancing dataset, by adding/removing data about under/over
    represented subsets. 2) modifying data or trained model where training data reflects
    past discrimination [[5](#bib.bib5)]; 3) importing testing techniques to test
    the fairness of models, such as symbolic execution and local interpretability [[11](#bib.bib11)];
    4) enacting non-discrimination law, and data protection law, such as GDPR [[3](#bib.bib3)].'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题并使AI系统更好地造福人类，我们需要做的是：1）平衡数据集，通过添加/删除关于不足/过多表示子集的数据；2）修改数据或训练模型，使训练数据反映过去的歧视 [[5](#bib.bib5)]；3）引入测试技术来测试模型的公平性，例如符号执行和局部解释性 [[11](#bib.bib11)]；4）制定反歧视法和数据保护法，例如GDPR [[3](#bib.bib3)]。
- en: 8.6 Corresponding defense methods
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6 相应的防御方法
- en: There is a line of approaches for preventing the aforementioned attacks.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 有一系列方法用于防止上述攻击。
- en: Model extraction defense. Blurring the prediction results is an effective way
    to prevent model stealing, for instance, rounding parameters [[226](#bib.bib226)][[220](#bib.bib220)],
    adding noise into class probabilities [[122](#bib.bib122)][[107](#bib.bib107)].
    On the other hand, detecting and prevent abnormal queries can also resolve this
    attack. Kesarwani et al. [[111](#bib.bib111)] recorded all requests made by clients
    and calculated the explored feature space to detect attack. PRADA [[107](#bib.bib107)]
    detected attack based on sudden changes in the distribution of samples submitted
    by a given customer. Orekondy et al. [[168](#bib.bib168)] proposed an active defense
    which perturbs predictions targeted at attacking the training objective.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 模型提取防御。模糊预测结果是防止模型盗窃的有效方法，例如，四舍五入参数 [[226](#bib.bib226)][[220](#bib.bib220)]，向类别概率中添加噪声 [[122](#bib.bib122)][[107](#bib.bib107)]。另一方面，检测和防止异常查询也可以解决这种攻击。Kesarwani
    等人 [[111](#bib.bib111)] 记录了客户发出的所有请求，并计算了探索的特征空间以检测攻击。PRADA [[107](#bib.bib107)]
    基于某客户提交的样本分布的突然变化来检测攻击。Orekondy 等人 [[168](#bib.bib168)] 提出了一个主动防御方法，通过扰动预测来针对训练目标进行攻击。
- en: 'Model inversion defense. To defend with model inversion attacks, researchers
    propose the following approaches:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 模型反演防御。为了防御模型反演攻击，研究人员提出了以下方法：
- en: •
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Differential privacy (DP)*, which is a cryptographic scheme designed to maximize
    the accuracy of data queries while minimizing the opportunity to identify their
    records when querying from a statistical database [[61](#bib.bib61)]. Individual
    features are removed to preserve user privacy. It is first proposed in [[60](#bib.bib60)]
    and proved to be effective in privacy preservation in database. In model privacy
    preserving, DP strategy can be applied to model parameters [[180](#bib.bib180)],
    prediction outputs [[46](#bib.bib46)][[83](#bib.bib83)][[229](#bib.bib229)][[269](#bib.bib269)][[97](#bib.bib97)],
    loss function [[112](#bib.bib112)][[214](#bib.bib214)], and gradients [[207](#bib.bib207)][[27](#bib.bib27)][[214](#bib.bib214)][[10](#bib.bib10)][[269](#bib.bib269)][[273](#bib.bib273)].
    Yu et al. [[261](#bib.bib261)] propose concentrated DP to analyze and optimize
    privacy loss.'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*差分隐私（DP）* 是一种加密方案，旨在最大化数据查询的准确性，同时最小化在从统计数据库查询时识别其记录的机会 [[61](#bib.bib61)]。通过移除个人特征来保护用户隐私。它首次在
    [[60](#bib.bib60)] 中提出，并被证明在数据库隐私保护中有效。在模型隐私保护中，DP 策略可以应用于模型参数 [[180](#bib.bib180)]、预测输出
    [[46](#bib.bib46)][[83](#bib.bib83)][[229](#bib.bib229)][[269](#bib.bib269)][[97](#bib.bib97)]、损失函数
    [[112](#bib.bib112)][[214](#bib.bib214)] 和梯度 [[207](#bib.bib207)][[27](#bib.bib27)][[214](#bib.bib214)][[10](#bib.bib10)][[269](#bib.bib269)][[273](#bib.bib273)]。Yu
    等人 [[261](#bib.bib261)] 提出了集中差分隐私以分析和优化隐私损失。'
- en: •
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Homomorphic encryption (HE)*, which is an encryption function and enables
    the following two operations are value-equivalent [[191](#bib.bib191)]: exercising
    arithmetic operations $\oplus$ on the ring of plain text and encrypting the result,
    encrypting operators first and then carry on the same arithmetic operations, i.e.,
    $En(x)\oplus En(y)=En(x+y)$. In this way, clients can encrypt their data and then
    send it to MLaaS. The server returns encrypted predictions without learning anything
    about the plain data. In the meantime, the clients have no idea about the model
    attributes [[70](#bib.bib70)][[136](#bib.bib136)][[108](#bib.bib108)][[105](#bib.bib105)].
    BAYHENN [[249](#bib.bib249)] uses HE to protect the client data, and uses Bayesian
    neural network to protect DNN weights, realizing secure DNN inference.'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*同态加密（HE）* 是一种加密函数，它使以下两种操作在值上等效 [[191](#bib.bib191)]：在明文环上进行算术操作 $\oplus$
    并对结果进行加密，先对操作数进行加密然后进行相同的算术操作，即 $En(x)\oplus En(y)=En(x+y)$。通过这种方式，客户端可以加密其数据并将其发送到
    MLaaS。服务器返回加密的预测结果，而不会了解任何有关明文数据的信息。同时，客户端对模型属性一无所知 [[70](#bib.bib70)][[136](#bib.bib136)][[108](#bib.bib108)][[105](#bib.bib105)]。BAYHENN
    [[249](#bib.bib249)] 使用 HE 保护客户端数据，并使用贝叶斯神经网络保护 DNN 权重，实现安全的 DNN 推理。'
- en: •
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Secure multi-party computation (SMC)*, stemming from Yao’s Millionaires’ problem [[257](#bib.bib257)]
    and enabling a safe calculation of contract functions without trusted third parties.
    In the context of deep learning, it extends to that multiple parties collectively
    train a model and preserve their own data [[224](#bib.bib224)][[202](#bib.bib202)][[181](#bib.bib181)][[182](#bib.bib182)][[188](#bib.bib188)].
    As such, the training data cannot be easily inferred by attackers residing at
    either computing servers or the client side. Helen [[275](#bib.bib275)] is a cooperative
    learning system that allows multiple parties to train a linear model without revealing
    data. DCOP [[215](#bib.bib215)] can protect privacy under the assumption of an
    honest majority and is not affected by collusion.'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*安全多方计算（SMC）* 源自姚期智的百万富翁问题 [[257](#bib.bib257)]，能够在没有可信第三方的情况下安全地计算合同函数。在深度学习的背景下，它扩展到多个参与方共同训练一个模型，并保留各自的数据
    [[224](#bib.bib224)][[202](#bib.bib202)][[181](#bib.bib181)][[182](#bib.bib182)][[188](#bib.bib188)]。因此，训练数据不能被驻留在计算服务器或客户端的攻击者轻易推测。Helen
    [[275](#bib.bib275)] 是一个合作学习系统，允许多个方在不揭示数据的情况下训练线性模型。DCOP [[215](#bib.bib215)]
    在诚实多数的假设下可以保护隐私，并不受合谋的影响。'
- en: •
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Training reconstitution*. Cao et al. [[42](#bib.bib42)] put forward machine
    unlearning, which makes ML models completely forget a piece of training data and
    recover the effects to models and features. Ohrimenko et al. [[165](#bib.bib165)]
    proposed a data-oblivious machine learning algorithm. Osia et al. [[169](#bib.bib169)]
    broke down large, complex deep models to enable scalable and privacy-preserving
    analytics by removing sensitive information with a feature extractor. MemGuard [[104](#bib.bib104)]
    adds noise to each confidence score vector predicted by the target classifier.
    Song et al. [[206](#bib.bib206)] find adversarial defense methods even increase
    the risk of target model against membership inference attack.'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*训练重建。* Cao 等人 [[42](#bib.bib42)] 提出了机器遗忘，这使得机器学习模型完全忘记一部分训练数据，并恢复模型和特征的效果。Ohrimenko
    等人 [[165](#bib.bib165)] 提出了数据无关的机器学习算法。Osia 等人 [[169](#bib.bib169)] 通过使用特征提取器删除敏感信息，分解大型复杂的深度模型，以实现可扩展且保护隐私的分析。MemGuard
    [[104](#bib.bib104)] 在目标分类器预测的每个置信度得分向量中添加噪声。Song 等人 [[206](#bib.bib206)] 发现对抗防御方法甚至增加了目标模型面临的成员身份推断攻击的风险。'
- en: 'Poisoning defense. Poisoning attack can be mitigated through two aspects:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 毒化防御。毒化攻击可以通过两个方面来缓解：
- en: •
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Protecting data.* This method includes avoiding data tampering, denial and
    falsification, and detecting poisonous data [[234](#bib.bib234)][[145](#bib.bib145)][[84](#bib.bib84)].
    Through perturbing inputs, Gao et al. [[68](#bib.bib68)] observed the randomness
    of their predicted classes from a given model. The low entropy in predicted classes
    violates the input dependency property of a benign model and implies the existence
    of a trojan input. Olufowobi et al. [[166](#bib.bib166)] described the context
    of creation or modification of data points to enhance trustworthiness and dependability
    of the data. Chakarov et al. [[45](#bib.bib45)] evaluated the effect of individual
    data points on the performance of trained model. Baracaldo et al. [[25](#bib.bib25)]
    used source information of training data points and the transformation context
    to identify poisonous data.'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*保护数据。* 这种方法包括避免数据篡改、否认和伪造，并检测有毒数据 [[234](#bib.bib234)][[145](#bib.bib145)][[84](#bib.bib84)]。通过扰动输入，Gao
    等人 [[68](#bib.bib68)] 观察到从给定模型预测的类别的随机性。预测类别中的低熵违反了良性模型的输入依赖性特性，暗示存在特洛伊木马输入。Olufowobi
    等人 [[166](#bib.bib166)] 描述了数据点的创建或修改背景，以增强数据的可信度和可靠性。Chakarov 等人 [[45](#bib.bib45)]
    评估了单个数据点对训练模型性能的影响。Baracaldo 等人 [[25](#bib.bib25)] 使用训练数据点的源信息和变换上下文来识别有毒数据。'
- en: •
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Protecting algorithm.* This method adjusts training algorithms, e.g., robust
    PCA [[40](#bib.bib40)], robust linear regression [[49](#bib.bib49)][[132](#bib.bib132)],
    and robust logistic regression [[64](#bib.bib64)]. Wang et al. [[227](#bib.bib227)]
    detect poisoning techniques via input filters, neuron pruning and machine unlearning.
    ABS [[140](#bib.bib140)] analyze the changes inside the neurons to detect trojan
    triggers, when introducing different levels of stimuli to neurons. FABA algorithm [[241](#bib.bib241)]
    can eliminate outliers in the uploaded gradient and obtain a gradient close to
    the true gradient in distributed learning. Qiao et al. [[185](#bib.bib185)] explore
    all possible backdoor triggers space formed by the pixel values and remove the
    triggers from a backdoored model.'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*保护算法。* 这种方法调整训练算法，例如，鲁棒 PCA [[40](#bib.bib40)]、鲁棒线性回归 [[49](#bib.bib49)][[132](#bib.bib132)]
    和鲁棒逻辑回归 [[64](#bib.bib64)]。Wang 等人 [[227](#bib.bib227)] 通过输入过滤器、神经元修剪和机器遗忘检测毒化技术。ABS
    [[140](#bib.bib140)] 分析神经元内部的变化，以检测特洛伊木马触发器，当向神经元引入不同水平的刺激时。FABA 算法 [[241](#bib.bib241)]
    可以消除上传梯度中的异常值，并在分布式学习中获得接近真实梯度的梯度。Qiao 等人 [[185](#bib.bib185)] 探索由像素值形成的所有可能的后门触发器空间，并从后门模型中移除这些触发器。'
- en: 'Adversarial defense. As adversarial attack draws the major attention, defensive
    work is more comprehensive and ample accordingly. The mainstream defense approaches
    are as follows:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗防御。由于对抗攻击引起了主要关注，相应的防御工作也更加全面和充分。主流的防御方法如下：
- en: •
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Model robustness.* Model robustness means that small perturbations to the
    input will not cause the network to misclassify. Certified robustness is an effective
    method to defend against adversarial attack. In order to verify the model robustness,
    Anderson et al. [[18](#bib.bib18)] combine the gradient-based optimization method
    of AEs search with the abstract-based proof search. PixelDP [[121](#bib.bib121)]
    is a certified defense that scales to large networks and datasets. It is based
    on a connection between robustness against AEs and differential privacy. Ma et
    al. [[147](#bib.bib147)] analyze the internal structures of DNN under various
    attacks, and propose a method of extracting DNN invariants to detect AEs at runtime.
    Liu et al. [[133](#bib.bib133)] aim to seek certified adversary-free regions around
    data points as large as possible. Research [[204](#bib.bib204)] proves that adversarial
    vulnerability of networks increases as gradients, and gradients grow as the input
    image dimension. PROVEN [[237](#bib.bib237)] provides probability certificates
    of the neural network robustness when the input perturbation obeys the distribution
    characteristics. For improving the provable error bound, Robustra [[125](#bib.bib125)]
    utilizes the adversarial space to solve the min-max game between attackers and
    defenders.'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*模型鲁棒性*。模型鲁棒性意味着对输入的小扰动不会导致网络错误分类。经过认证的鲁棒性是防御对抗攻击的有效方法。为了验证模型鲁棒性，安德森等[[18](#bib.bib18)]将对抗样本搜索的基于梯度的优化方法与基于抽象的证明搜索结合。PixelDP[[121](#bib.bib121)]是一种可以扩展到大网络和数据集的认证防御方法。它基于对抗样本鲁棒性与差分隐私之间的联系。马等[[147](#bib.bib147)]分析了在各种攻击下深度神经网络（DNN）的内部结构，并提出了一种提取DNN不变性以在运行时检测对抗样本的方法。刘等[[133](#bib.bib133)]旨在寻找尽可能大的认证对抗自由区域。研究[[204](#bib.bib204)]证明，网络的对抗脆弱性随着梯度的增加而增加，梯度随着输入图像维度的增加而增长。PROVEN[[237](#bib.bib237)]提供了神经网络鲁棒性的概率认证，当输入扰动服从分布特征时。为了提高可证明的误差界限，Robustra[[125](#bib.bib125)]利用对抗空间解决攻击者和防御者之间的最小-最大博弈。'
- en: •
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Adversarial training*. This method selects AEs as part of the training dataset
    to make trained model learn characteristics of AEs [[93](#bib.bib93)][[118](#bib.bib118)][[103](#bib.bib103)][[157](#bib.bib157)].
    Furthermore, Ensemble Adversarial Training [[218](#bib.bib218)] contained each
    turbine input transferred from other pre-trained models. Wang et al. [[228](#bib.bib228)]
    introduce adversarial noise to the output embedding layer while training neural
    language models. Ye et al. [[259](#bib.bib259)] propose a framework for simultaneous
    adversarial training and weights pruning, which can compress the model while maintaining
    robustness. Wang et al. [[232](#bib.bib232)] propose bilateral adversarial training,
    which both perturbs both the image and the label. Zhang et al. [[266](#bib.bib266)]
    generate adversarial images for training by feature scattering in the latent space.
    Wong et al. [[238](#bib.bib238)] successfully trained robust models using a weaker
    and cheaper adversary, which saves much time. Li et al. [[128](#bib.bib128)] proved
    that adversarial training indeed promotes robustness through theoretical insights.'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*对抗训练*。这种方法将对抗样本（AEs）作为训练数据集的一部分，以使训练模型学习对抗样本的特征[[93](#bib.bib93)][[118](#bib.bib118)][[103](#bib.bib103)][[157](#bib.bib157)]。此外，集成对抗训练[[218](#bib.bib218)]包含从其他预训练模型传输的每个涡轮输入。王等[[228](#bib.bib228)]在训练神经语言模型时向输出嵌入层引入对抗噪声。叶等[[259](#bib.bib259)]提出了一种用于同时进行对抗训练和权重剪枝的框架，该框架能够在保持鲁棒性的同时压缩模型。王等[[232](#bib.bib232)]提出了双向对抗训练，该方法对图像和标签进行扰动。张等[[266](#bib.bib266)]通过在潜在空间中进行特征散射来生成用于训练的对抗图像。翁等[[238](#bib.bib238)]成功地使用较弱且便宜的对手训练了鲁棒模型，从而节省了大量时间。李等[[128](#bib.bib128)]通过理论见解证明了对抗训练确实促进了鲁棒性。'
- en: •
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Region-based method*. Understanding properties of adversarial regions and
    using more robust region-based classification could also defend adversarial attack.
    Cao et al. [[41](#bib.bib41)] develop DNNs using region-based classification instead
    of point-based. They predicted labels through randomly selecting several points
    from the hypercube centered at the testing sample. In [[171](#bib.bib171)], the
    classifier mapped normal samples to the neighborhood of low-dimensional manifolds
    in the final-layer hidden space. Local Intrinsic Dimensionality [[148](#bib.bib148)]
    characterized dimensional properties of adversarial regions and evaluated the
    spatial fill capability. Background Class [[151](#bib.bib151)] added a large and
    diverse class of background images into datasets.'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*基于区域的方法*。了解对抗区域的特性并使用更鲁棒的基于区域的分类也可以防御对抗攻击。Cao 等人 [[41](#bib.bib41)] 开发了使用基于区域的分类而不是基于点的
    DNN。他们通过从以测试样本为中心的超立方体中随机选择几个点来预测标签。在 [[171](#bib.bib171)] 中，分类器将正常样本映射到最终层隐藏空间的低维流形邻域。局部内在维度
    [[148](#bib.bib148)] 描述了对抗区域的维度特性，并评估了空间填充能力。背景类 [[151](#bib.bib151)] 将大量多样的背景图像类添加到数据集中。'
- en: •
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Transformation*. Transforming inputs can defend adversarial attack to a large
    extent. Song et al. [[208](#bib.bib208)] found that AEs mainly lay in the low
    probability regions of the training regions. So they purified an AE by moving
    it back towards the distribution adaptively. Guo et al. [[78](#bib.bib78)] explored
    model-agnostic defenses on image-classification systems by image transformations.
    Xie et al. [[247](#bib.bib247)] used randomization at inference time, including
    random resizing and padding. Tian et al. [[216](#bib.bib216)] considered that
    AEs are more sensitive to certain image transformation operations, such as rotation
    and shifting, than normal images. Wang et al. [[231](#bib.bib231)][[230](#bib.bib230)]
    thought AEs are more sensitive to random perturbations than normal. Buckman et
    al. [[37](#bib.bib37)] used thermometer code and one-hot code discretization to
    increase the robustness of network to AEs. Kou et al. [[115](#bib.bib115)] trained
    a separate lightweight distribution classifier to recognize different features
    of transformed images.'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*变换*。转换输入可以在很大程度上防御对抗攻击。Song 等人 [[208](#bib.bib208)] 发现对抗样本主要位于训练区域的低概率区域。因此，他们通过自适应地将对抗样本移回分布来进行净化。Guo
    等人 [[78](#bib.bib78)] 通过图像变换探索了与模型无关的防御。Xie 等人 [[247](#bib.bib247)] 在推断时使用了随机化，包括随机缩放和填充。Tian
    等人 [[216](#bib.bib216)] 认为对抗样本对某些图像变换操作（如旋转和位移）比正常图像更为敏感。Wang 等人 [[231](#bib.bib231)][[230](#bib.bib230)]
    认为对抗样本对随机扰动比正常样本更为敏感。Buckman 等人 [[37](#bib.bib37)] 使用温度计编码和独热编码离散化来提高网络对对抗样本的鲁棒性。Kou
    等人 [[115](#bib.bib115)] 训练了一个单独的轻量级分布分类器，以识别转化图像的不同特征。'
- en: •
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Gradient regularization/masking*. This method hides gradients or reduces the
    sensitivity of models. Madry et al. [[149](#bib.bib149)] realized it by optimizing
    a saddle point formulation, which included solving an inner maximization solved
    and an outer minimization. Ross et al. [[195](#bib.bib195)] trained differentiable
    models that penalized the degree to infinitesimal changes in inputs.'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*梯度正则化/掩蔽*。这种方法隐藏梯度或减少模型的敏感性。Madry 等人 [[149](#bib.bib149)] 通过优化鞍点公式实现了这一点，其中包括解决内层最大化和外层最小化问题。Ross
    等人 [[195](#bib.bib195)] 训练了惩罚输入微小变化程度的可微分模型。'
- en: •
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Distillation*. Papernot et al. [[172](#bib.bib172)] proposed Defensive Distillation,
    which could successfully mitigate AEs constructed by FGSM and JSMA. Papernot et
    al. [[178](#bib.bib178)] also used the knowledge extracted in distillation to
    reduce the magnitude of network gradient. Liu et al. [[143](#bib.bib143)] propose
    feature distillation, a JPEG-based defensive compression framework to rectify
    AEs.'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*蒸馏*。Papernot 等人 [[172](#bib.bib172)] 提出了防御蒸馏，这可以成功缓解由 FGSM 和 JSMA 构造的对抗样本。Papernot
    等人 [[178](#bib.bib178)] 还利用蒸馏中提取的知识来减少网络梯度的幅度。Liu 等人 [[143](#bib.bib143)] 提出了特征蒸馏，一种基于
    JPEG 的防御压缩框架，用于纠正对抗样本。'
- en: •
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Data preprocessing*. Liang et al. [[129](#bib.bib129)] introduced scalar quantization
    and smooth spatial filtering to reduce the effect of perturbations. Zantedeschi
    et al. [[264](#bib.bib264)] used bounded ReLU activation function for hedging
    forward propagation of adversarial perturbation. Xu et al. [[253](#bib.bib253)]
    proposed feature squeezing methods, including reducing the depth of color bit
    on each pixel and spatial smoothing. Yang et al. [[255](#bib.bib255)] preprocess
    images by randomly removing pixels from the image, and using matrix estimation
    to reconstruct it.'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*数据预处理*。Liang等人[[129](#bib.bib129)]引入了标量量化和平滑空间滤波以减少扰动的影响。Zantedeschi等人[[264](#bib.bib264)]使用了有界ReLU激活函数以对抗对抗扰动的前向传播。Xu等人[[253](#bib.bib253)]提出了特征压缩方法，包括减少每个像素的颜色位深和空间平滑。Yang等人[[255](#bib.bib255)]通过随机移除图像中的像素，并使用矩阵估计来重建图像。'
- en: •
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Defense network*. Some studies use networks to automatically fight against
    AEs. Gu et al. [[75](#bib.bib75)] used deep contractive network with contractive
    autoencoders and denoising autoencoders, which can remove amounts of adversarial
    noise. Akhtar et al. [[12](#bib.bib12)] proposed a perturbation rectifying network
    as pre-input layers to defend against UAPs. MagNet [[154](#bib.bib154)] used detector
    networks to detect AEs which are far from the boundary of manifold, and used a
    reformer to reform AEs which are close to the boundary. Liu et al. [[131](#bib.bib131)]
    propose a defense model which uses feature prioritization of the nonlinear attention
    module and the $L_{2}$ feature regularization.'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*防御网络*。一些研究使用网络自动抵御对抗样本。Gu等人[[75](#bib.bib75)]使用了深度收缩网络，结合了收缩自编码器和去噪自编码器，能够去除大量的对抗噪声。Akhtar等人[[12](#bib.bib12)]提出了一种扰动修正网络作为前置输入层，以防御UAPs。MagNet[[154](#bib.bib154)]使用检测网络来检测远离流形边界的对抗样本，并使用重构器重构接近边界的对抗样本。Liu等人[[131](#bib.bib131)]提出了一种防御模型，利用非线性注意力模块的特征优先级和$L_{2}$特征正则化。'
- en: 8.7 Future direction of attack and defense
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7 攻击与防御的未来方向
- en: It is an endless war between attackers and defenders, and neither of them can
    win an absolute victory. But both sides can research new techniques and applications
    to gain advantages. From the attacker’s point of view, one effective way is to
    explore new attack surfaces, find out new attack scenarios, seek for new attack
    purposes and broaden the scope of attack effects. In particular, main attack surfaces
    on deep learning systems include malformed operational input, malformed training
    data and malformed models [[245](#bib.bib245)].
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这是攻击者和防御者之间一场无尽的战争，双方都无法获得绝对的胜利。但双方都可以研究新技术和应用以获得优势。从攻击者的角度来看，一种有效的方法是探索新的攻击面，发现新的攻击场景，寻找新的攻击目的，并扩大攻击效果的范围。特别是，深度学习系统的主要攻击面包括格式不正确的操作输入、格式不正确的训练数据和格式不正确的模型[[245](#bib.bib245)]。
- en: In adversary attack, $L_{p}$-distance is not an ideal measurement. Some images
    with big perturbations are still indistinguishable for humans. However, unlike
    $L_{p}$-distance, there is no standard measure for large $L_{p}$ perturbations.
    This will be a hot point for adversarial learning in future. In model extraction
    attack, stealing functionality of complex models needs massive queries. How to
    come up with a better method to reduce the number of queries in order of magnitude
    will be the focus of this field.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗攻击中，$L_{p}$-距离不是一个理想的度量。一些有大幅扰动的图像对于人类仍然无法区分。然而，与$L_{p}$-距离不同，大$L_{p}$扰动没有标准的度量。这将成为未来对抗学习的一个热点。在模型提取攻击中，窃取复杂模型的功能需要大量的查询。如何提出更好的方法来减少数量级的查询将是这个领域的重点。
- en: The balance of attack cost and benefit is also an important factor. Some attacks,
    even can achieve fruitful targets, have to perform costly computation or resources [[220](#bib.bib220)].
    For example, in [[203](#bib.bib203)], the attacker has to train a number of shadow
    models that simulate the target model, and then undertake membership inference.
    They need 156 queries to produce a data point on average.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击成本与收益的平衡也是一个重要因素。一些攻击即使能够达到有利的目标，也必须进行昂贵的计算或资源消耗[[220](#bib.bib220)]。例如，在[[203](#bib.bib203)]中，攻击者必须训练多个模拟目标模型的影子模型，然后进行成员身份推断。他们需要156次查询才能平均生成一个数据点。
- en: Attack cost and attack benefit are a trade-off process [[152](#bib.bib152)].
    Generally, the cost of attack contains time, computation resources, acquired knowledge,
    and monetary expense. The benefit from an attack include economic payback, rivals’
    failure and so forth. In this study, we will not give a uniform formula to quantify
    the cost and benefit as the importance of each element is varying in different
    scenarios. Nevertheless, it is usually modeled as an optimization problem where
    the cost is minimized while the benefit is maximized, like a min-max game [[162](#bib.bib162)].
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击成本和攻击收益是一个权衡过程 [[152](#bib.bib152)]。通常，攻击成本包括时间、计算资源、获得的知识和货币开支。攻击带来的收益包括经济回报、竞争对手的失败等。在本研究中，我们不会给出统一的公式来量化成本和收益，因为不同场景下每个元素的重要性有所不同。然而，通常将其建模为一个优化问题，即在最小化成本的同时最大化收益，如最小-最大博弈 [[162](#bib.bib162)]。
- en: As for defenders, a combination of multiple defense techniques is a good choice
    to reduce the risk of being attacked. But the combination may incur additional
    overhead on the system that should be solved in design. For example, in [[136](#bib.bib136)][[108](#bib.bib108)],
    they adopted a mixed protocol combining HE and MPC, which improved performance
    but with high bandwidth.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 对于防御者来说，结合多种防御技术是降低被攻击风险的好选择。但这种组合可能会给系统带来额外的开销，这应该在设计时解决。例如，在 [[136](#bib.bib136)][[108](#bib.bib108)]中，他们采用了结合同态加密（HE）和安全多方计算（MPC）的混合协议，这提高了性能，但带来了高带宽消耗。
- en: 9 Conclusion
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: In this paper, we conduct a comprehensive and extensive investigation on attacks
    towards deep learning systems. Different from other surveys, we dissect an attack
    in a systematical way, where interested readers can clearly understand how these
    attacks happen step by step. We have compared the investigated works on their
    attack vectors and proposed a number of metrics to compare their performance.
    Based on the comparison, we then proceed to distill a number of insights, disclosing
    advantages and disadvantages of attack methods, limitations and trends. The discussion
    covering the difficulties of these attacks in the physical world, security concerns
    in other aspects and potential mitigation for these attacks provide a platform
    on which future research can be based.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对针对深度学习系统的攻击进行了全面而深入的调查。与其他调查不同，我们以系统化的方式剖析了攻击过程，让感兴趣的读者能够清楚地理解这些攻击是如何一步步发生的。我们对调查的工作进行了攻击向量的比较，并提出了多个指标来比较它们的性能。基于这些比较，我们进一步提炼出一些见解，揭示了攻击方法的优缺点、局限性和趋势。讨论涵盖了这些攻击在实际世界中的困难、其他方面的安全问题以及这些攻击的潜在缓解措施，为未来的研究提供了一个基础平台。
- en: References
  id: totrans-412
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Drebin dataset. https://www.sec.tu-bs.de/~danarp/drebin/, 2016.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Drebin 数据集。 https://www.sec.tu-bs.de/~danarp/drebin/, 2016。'
- en: '[2] Imagenet dataset. http://www.image-net.org, 2017.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Imagenet 数据集。 http://www.image-net.org, 2017。'
- en: '[3] General data protection regulation. https://gdpr-info.eu, May 2018.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 通用数据保护条例。 https://gdpr-info.eu, 2018年5月。'
- en: '[4] Gtsrb dataset. http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset,
    2019.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Gtsrb 数据集。 http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset,
    2019。'
- en: '[5] Human bias and discrimination in ai systems. https://ai-auditingframework.blogspot.com/2019/06/human-bias-and-discrimination-in-ai.html,
    2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 人工智能系统中的人类偏见和歧视。 https://ai-auditingframework.blogspot.com/2019/06/human-bias-and-discrimination-in-ai.html,
    2019。'
- en: '[6] Ijb-a dataset. https://www.nist.gov/itl/iad/image-group/ijb-dataset-request-form,
    2019.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Ijb-a 数据集。 https://www.nist.gov/itl/iad/image-group/ijb-dataset-request-form,
    2019。'
- en: '[7] Imdb review dataset. https://www.kaggle.com/utathya/imdb-review-dataset,
    2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Imdb 评测数据集。 https://www.kaggle.com/utathya/imdb-review-dataset, 2019。'
- en: '[8] Microsoft kaggle dataset. https://www.kaggle.com/c/microsoft-malware-prediction,
    2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Microsoft kaggle 数据集。 https://www.kaggle.com/c/microsoft-malware-prediction,
    2019。'
- en: '[9] Mozilla common voice. https://voice.mozilla.org/en, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Mozilla 通用语音。 https://voice.mozilla.org/en, 2019。'
- en: '[10] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar,
    and L. Zhang. Deep learning with differential privacy. In Proceedings of the ACM
    Conference on Computer and Communications Security (CCS), Vienna, Austria, 2016,
    pages 308–318, October 24-28, 2016.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar,
    和 L. Zhang. 使用差分隐私的深度学习。在ACM计算机与通信安全会议（CCS）上，奥地利维也纳，2016年10月24-28日，页面308–318，2016。'
- en: '[11] A. Aggarwal, P. Lohia, S. Nagar, K. Dey, and D. Saha. Black box fairness
    testing of machine learning models. In Proceedings of the ACM Joint Meeting on
    European Software Engineering Conference and Symposium on the Foundations of Software
    Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-30, 2019., pages
    625–635.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Aggarwal, P. Lohia, S. Nagar, K. Dey, 和 D. Saha. 机器学习模型的黑箱公平性测试。在ACM欧洲软件工程会议和软件工程基础研讨会联合会议论文集，ESEC/SIGSOFT
    FSE 2019，塔林，爱沙尼亚，2019年8月26-30日，页625–635。'
- en: '[12] N. Akhtar, J. Liu, and A. S. Mian. Defense against universal adversarial
    perturbations. CoRR, abs/1711.05929, 2017.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] N. Akhtar, J. Liu, 和 A. S. Mian. 针对普遍对抗扰动的防御。CoRR, abs/1711.05929, 2017。'
- en: '[13] N. Akhtar and A. S. Mian. Threat of adversarial attacks on deep learning
    in computer vision: A survey. IEEE Access, 6:14410–14430, 2018.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] N. Akhtar 和 A. S. Mian. 对计算机视觉中深度学习的对抗攻击威胁: 一项调查。IEEE Access, 6:14410–14430,
    2018。'
- en: '[14] A. Al-Dujaili, A. Huang, E. Hemberg, and U. O’Reilly. Adversarial deep
    learning for robust detection of binary encoded malware. In IEEE Security and
    Privacy Workshops, San Francisco, CA, USA, pages 76–82, May 24, 2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. Al-Dujaili, A. Huang, E. Hemberg, 和 U. O’Reilly. 对抗性深度学习用于二进制编码恶意软件的鲁棒检测。在
    IEEE 安全与隐私研讨会，旧金山，加州，美国，页76–82，2018年5月24日。'
- en: '[15] Alexey, I. J. Goodfellow, and S. Bengio. Adversarial examples in the physical
    world. CoRR, abs/1607.02533, 2016.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Alexey, I. J. Goodfellow, 和 S. Bengio. 物理世界中的对抗性示例。CoRR, abs/1607.02533,
    2016。'
- en: '[16] S. Alfeld, X. Zhu, and P. Barford. Data poisoning attacks against autoregressive
    models. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,
    Phoenix, Arizona, USA., pages 1452–1458, February 12-17, 2016.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Alfeld, X. Zhu, 和 P. Barford. 针对自回归模型的数据投毒攻击。在第30届AAAI人工智能大会论文集，凤凰城，亚利桑那州，美国，页1452–1458，2016年2月12-17日。'
- en: '[17] D. Amodei, C. Olah, J. Steinhardt, P. F. Christiano, J. Schulman, and
    D. Mané. Concrete problems in AI safety. CoRR, abs/1606.06565, 2016.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] D. Amodei, C. Olah, J. Steinhardt, P. F. Christiano, J. Schulman, 和 D.
    Mané. 人工智能安全中的具体问题。CoRR, abs/1606.06565, 2016。'
- en: '[18] G. Anderson, S. Pailoor, I. Dillig, and S. Chaudhuri. Optimization and
    abstraction: a synergistic approach for analyzing neural network robustness. In
    Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design
    and Implementation, PLDI 2019, Phoenix, AZ, USA, June 22-26, 2019, pages 731–744.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] G. Anderson, S. Pailoor, I. Dillig, 和 S. Chaudhuri. 优化与抽象: 一种分析神经网络鲁棒性的协同方法。在第40届ACM
    SIGPLAN编程语言设计与实现会议，PLDI 2019，凤凰城，亚利桑那州，美国，2019年6月22-26日，页731–744。'
- en: '[19] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and G. Felici.
    Hacking smart machines with smarter ones: How to extract meaningful data from
    machine learning classifiers. IJSN, 10(3):137–150, 2015.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, 和 G.
    Felici. 用更聪明的机器攻击智能机器: 如何从机器学习分类器中提取有意义的数据。IJSN, 10(3):137–150, 2015。'
- en: '[20] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Synthesizing robust adversarial
    examples. In Proceedings of the 35th International Conference on Machine Learning
    (ICML), Stockholmsmässan, Stockholm, Sweden, pages 284–293, July 10-15, 2018.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Athalye, L. Engstrom, A. Ilyas, 和 K. Kwok. 合成鲁棒的对抗性示例。在第35届国际机器学习会议（ICML），斯德哥尔摩会展中心，斯德哥尔摩，瑞典，页284–293，2018年7月10-15日。'
- en: '[21] A. Awad, J. Traub, and S. Sakr. Adaptive watermarks: A concept drift-based
    approach for predicting event-time progress in data streams. In 22nd International
    Conference on Extending Database Technology (EDBT), Lisbon, Portugal, pages 622–625,
    March 26-29, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Awad, J. Traub, 和 S. Sakr. 自适应水印: 一种基于概念漂移的数据流事件时间进展预测方法。在第22届国际扩展数据库技术会议（EDBT），里斯本，葡萄牙，页622–625，2019年3月26-29日。'
- en: '[22] H. Bae, J. Jang, D. Jung, H. Jang, H. Ha, and S. Yoon. Security and privacy
    issues in deep learning. CoRR, abs/1807.11655, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Bae, J. Jang, D. Jung, H. Jang, H. Ha, 和 S. Yoon. 深度学习中的安全和隐私问题。CoRR,
    abs/1807.11655, 2018。'
- en: '[23] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov. How to backdoor
    federated learning. In The 23rd International Conference on Artificial Intelligence
    and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy],
    pages 2938–2948.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, 和 V. Shmatikov. 如何在联邦学习中植入后门。在第23届国际人工智能与统计会议，AISTATS
    2020，2020年8月26-28日，在线[巴勒莫，西西里岛，意大利]，页2938–2948。'
- en: '[24] S. Baluja and I. Fischer. Learning to attack: Adversarial transformation
    networks. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,
    New Orleans, Louisiana, USA, February 2-7, 2018.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Baluja 和 I. Fischer. 学会攻击: 对抗性变换网络。在第32届AAAI人工智能大会论文集，新奥尔良，路易斯安那州，美国，2018年2月2-7日。'
- en: '[25] N. Baracaldo, B. Chen, H. Ludwig, and J. A. Safavi. Mitigating poisoning
    attacks on machine learning models: A data provenance based approach. In Proceedings
    of the 10th ACM Workshop on Artificial Intelligence and Security, AISec@CCS, Dallas,
    TX, USA, pages 103–110, November 3, 2017.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] N. Baracaldo, B. Chen, H. Ludwig 和 J. A. Safavi. 缓解对机器学习模型的中毒攻击：一种基于数据来源的方法。发表于第10届ACM人工智能与安全研讨会，AISec@CCS，美国德克萨斯州达拉斯，第103-110页，2017年11月3日。'
- en: '[26] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar. The security of
    machine learning. Machine Learning, 81(2):121–148, Nov 2010.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Barreno, B. Nelson, A. D. Joseph 和 J. D. Tygar. 机器学习的安全性。机器学习，81(2):121-148，2010年11月。'
- en: '[27] R. Bassily, A. D. Smith, and A. Thakurta. Private empirical risk minimization:
    Efficient algorithms and tight error bounds. In 55th IEEE Annual Symposium on
    Foundations of Computer Science, FOCS, Philadelphia, PA, USA, pages 464–473, October
    18-21, 2014.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] R. Bassily, A. D. Smith 和 A. Thakurta. 私有经验风险最小化：高效算法与紧致错误界限。发表于第55届IEEE计算机科学基础年会，FOCS，美国宾夕法尼亚州费城，第464-473页，2014年10月18-21日。'
- en: '[28] V. Beal. What is structured data? webopedia definition. https://www.webopedia.com/TERM/S/structured_data.html,
    Aug. 2018.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] V. Beal. 什么是结构化数据？Webopedia定义。https://www.webopedia.com/TERM/S/structured_data.html，2018年8月。'
- en: '[29] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. B. Calo. Analyzing federated
    learning through an adversarial lens. In Proceedings of the 36th International
    Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
    USA, pages 634–643.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] A. N. Bhagoji, S. Chakraborty, P. Mittal 和 S. B. Calo. 从对抗视角分析联邦学习。发表于第36届国际机器学习大会，ICML
    2019，2019年6月9-15日，加利福尼亚州长滩，美国，第634-643页。'
- en: '[30] A. Bhattad, M. J. Chong, K. Liang, B. Li, and D. A. Forsyth. Unrestricted
    adversarial examples via semantic manipulation. In International Conference on
    Learning Representations, 2020.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] A. Bhattad, M. J. Chong, K. Liang, B. Li 和 D. A. Forsyth. 通过语义操作生成无限制对抗样本。发表于国际学习表征会议，2020年。'
- en: '[31] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support
    vector machines. In Proceedings of the 29th International Conference on Machine
    Learning, ICML, Edinburgh, Scotland, UK, June 26 - July 1, 2012.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] B. Biggio, B. Nelson 和 P. Laskov. 对支持向量机的中毒攻击。发表于第29届国际机器学习大会，ICML，苏格兰爱丁堡，英国，2012年6月26日-7月1日。'
- en: '[32] B. Biggio, I. Pillai, S. R. Bulò, D. Ariu, M. Pelillo, and F. Roli. Is
    data clustering in adversarial settings secure? In AISec’13, Proceedings of the
    ACM Workshop on Artificial Intelligence and Security, Co-located with CCS, Berlin,
    Germany, pages 87–98, November 4, 2013.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] B. Biggio, I. Pillai, S. R. Bulò, D. Ariu, M. Pelillo 和 F. Roli. 对抗环境中的数据聚类是否安全？发表于AISec’13，ACM人工智能与安全研讨会论文集，与CCS会议同期，德国柏林，第87-98页，2013年11月4日。'
- en: '[33] A. Bojchevski and S. Günnemann. Adversarial attacks on node embeddings
    via graph poisoning. In Proceedings of the 36th International Conference on Machine
    Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 695–704.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. Bojchevski 和 S. Günnemann. 通过图形中毒进行节点嵌入的对抗攻击。发表于第36届国际机器学习大会，ICML 2019，2019年6月9-15日，加利福尼亚州长滩，美国，第695-704页。'
- en: '[34] F. Z. Borgesius. Discrimination, artificial intelligence, and algorithmic
    decision-making. https://rm.coe.int/discrimination-artificial-intelligence-and-algorithmic-decision-making/1680925d73,
    2018.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] F. Z. Borgesius. 歧视、人工智能与算法决策。https://rm.coe.int/discrimination-artificial-intelligence-and-algorithmic-decision-making/1680925d73，2018年。'
- en: '[35] M. Brückner and T. Scheffer. Nash equilibria of static prediction games.
    In 23rd Annual Conference on Neural Information Processing Systems, Vancouver,
    British Columbia, Canada, pages 171–179, December 7-10, 2009.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] M. Brückner 和 T. Scheffer. 静态预测博弈的纳什均衡。发表于第23届神经信息处理系统年会，加拿大不列颠哥伦比亚省温哥华，第171-179页，2009年12月7-10日。'
- en: '[36] S. Bubeck, Y. T. Lee, E. Price, and I. P. Razenshteyn. Adversarial examples
    from computational constraints. In Proceedings of the 36th International Conference
    on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages
    831–840.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Bubeck, Y. T. Lee, E. Price 和 I. P. Razenshteyn. 由于计算约束产生的对抗样本。发表于第36届国际机器学习大会，ICML
    2019，2019年6月9-15日，加利福尼亚州长滩，美国，第831-840页。'
- en: '[37] J. Buckman, A. Roy, C. Raffel, and I. Goodfellow. Thermometer encoding:
    One hot way to resist adversarial examples. In International Conference on Learning
    Representations, 2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Buckman, A. Roy, C. Raffel 和 I. Goodfellow. 温度编码：抵抗对抗样本的一种热编码方法。发表于国际学习表征会议，2018年。'
- en: '[38] J. Buolamwini and T. Gebru. Gender shades: Intersectional accuracy disparities
    in commercial gender classification. In Conference on Fairness, Accountability
    and Transparency, FAT, New York, NY, USA, pages 77–91, February 23-24, 2018.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. Buolamwini 和 T. Gebru. 性别阴影：商业性别分类中的交叉精度差异。发表于公平性、问责制与透明度会议，FAT，纽约，NY，美国，第77–91页，2018年2月23-24日。'
- en: '[39] C. Burkard and B. Lagesse. Analysis of causative attacks against svms
    learning from data streams. In Proceedings of the 3rd ACM on International Workshop
    on Security And Privacy Analytics, IWSPA@CODASPY, Scottsdale, Arizona, USA, pages
    31–36, March 24, 2017.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C. Burkard 和 B. Lagesse. 分析针对svms数据流学习的因果攻击。发表于第3届ACM国际安全与隐私分析研讨会，IWSPA@CODASPY，斯科茨代尔，亚利桑那州，美国，第31–36页，2017年3月24日。'
- en: '[40] E. J. Candès, X. Li, Y. Ma, and J. Wright. Robust principal component
    analysis? J. ACM, 58(3):11:1–11:37, 2011.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] E. J. Candès, X. Li, Y. Ma, 和 J. Wright. 鲁棒的主成分分析？J. ACM，58(3):11:1–11:37，2011年。'
- en: '[41] X. Cao and N. Z. Gong. Mitigating evasion attacks to deep neural networks
    via region-based classification. In Proceedings of the 33rd Annual Computer Security
    Applications Conference, Orlando, FL, USA, pages 278–287, December 4-8, 2017.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] X. Cao 和 N. Z. Gong. 通过基于区域的分类缓解对深度神经网络的规避攻击。发表于第33届年度计算机安全应用会议，奥兰多，FL，美国，第278–287页，2017年12月4-8日。'
- en: '[42] Y. Cao and J. Yang. Towards making systems forget with machine unlearning.
    In IEEE Symposium on Security and Privacy, SP, San Jose, CA, USA, pages 463–480,
    May 17-21, 2015.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Cao 和 J. Yang. 通过机器遗忘让系统“忘记”。发表于IEEE安全与隐私研讨会，SP，加州圣荷西，美国，第463–480页，2015年5月17-21日。'
- en: '[43] N. Carlini and D. A. Wagner. Towards evaluating the robustness of neural
    networks. In IEEE Symposium on Security and Privacy (SP), pages 39–57, 2017.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] N. Carlini 和 D. A. Wagner. 评估神经网络鲁棒性的进展。发表于IEEE安全与隐私研讨会（SP），第39–57页，2017年。'
- en: '[44] N. Carlini and D. A. Wagner. Audio adversarial examples: Targeted attacks
    on speech-to-text. In IEEE Security and Privacy Workshops, SP Workshops, San Francisco,
    CA, USA, pages 1–7, May 24, 2018.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] N. Carlini 和 D. A. Wagner. 音频对抗样本：针对语音转文本的定向攻击。发表于IEEE安全与隐私研讨会，SP研讨会，加州旧金山，美国，第1–7页，2018年5月24日。'
- en: '[45] A. Chakarov, A. V. Nori, S. K. Rajamani, S. Sen, and D. Vijaykeerthy.
    Debugging machine learning tasks. CoRR, abs/1603.07292, 2016.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Chakarov, A. V. Nori, S. K. Rajamani, S. Sen, 和 D. Vijaykeerthy. 调试机器学习任务。CoRR，abs/1603.07292，2016年。'
- en: '[46] K. Chaudhuri and C. Monteleoni. Privacy-preserving logistic regression.
    In Proceedings of the Twenty-Second Annual Conference on Neural Information Processing
    Systems, Vancouver, British Columbia, Canada, pages 289–296, December 8-11, 2008.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] K. Chaudhuri 和 C. Monteleoni. 隐私保护的逻辑回归。发表于第22届年度神经信息处理系统会议，加拿大温哥华，第289–296页，2008年12月8-11日。'
- en: '[47] H. Chen, H. Zhang, D. S. Boning, and C. Hsieh. Robust decision trees against
    adversarial examples. In Proceedings of the 36th International Conference on Machine
    Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 1122–1131.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] H. Chen, H. Zhang, D. S. Boning, 和 C. Hsieh. 针对对抗样本的鲁棒决策树。发表于第36届国际机器学习会议，ICML
    2019，2019年6月9-15日，加利福尼亚州长滩，美国，第1122–1131页。'
- en: '[48] P. Chen, Y. Sharma, H. Zhang, J. Yi, and C. Hsieh. EAD: elastic-net attacks
    to deep neural networks via adversarial examples. In Proceedings of the Thirty-Second
    AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, pages
    10–17, February 2-7, 2018.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] P. Chen, Y. Sharma, H. Zhang, J. Yi, 和 C. Hsieh. EAD：通过对抗样本对深度神经网络进行弹性网攻击。发表于第32届AAAI人工智能大会，新奥尔良，路易斯安那州，美国，第10–17页，2018年2月2-7日。'
- en: '[49] Y. Chen, C. Caramanis, and S. Mannor. Robust high dimensional sparse regression
    and matching pursuit. CoRR, abs/1301.2725, 2013.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Y. Chen, C. Caramanis, 和 S. Mannor. 鲁棒的高维稀疏回归和匹配追踪。CoRR，abs/1301.2725，2013年。'
- en: '[50] M. Cheng, T. Le, P. Chen, H. Zhang, J. Yi, and C. Hsieh. Query-efficient
    hard-label black-box attack: An optimization-based approach. In 7th International
    Conference on Learning Representations, ICLR, New Orleans, LA, USA, May 6-9, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] M. Cheng, T. Le, P. Chen, H. Zhang, J. Yi, 和 C. Hsieh. 查询高效硬标签黑箱攻击：一种基于优化的方法。发表于第7届国际学习表征会议，ICLR，新奥尔良，LA，美国，2019年5月6-9日。'
- en: '[51] S. Cheng, Y. Dong, T. Pang, H. Su, and J. Zhu. Improving black-box adversarial
    attacks with a transfer-based prior. In Advances in Neural Information Processing
    Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS
    2019, 8-14 December 2019, Vancouver, BC, Canada, pages 10932–10942.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] S. Cheng, Y. Dong, T. Pang, H. Su, 和 J. Zhu. 通过基于转移的先验改进黑箱对抗攻击。发表于神经信息处理系统进展32：神经信息处理系统年会2019，NeurIPS
    2019，2019年12月8-14日，加拿大温哥华，第10932–10942页。'
- en: '[52] Y. Cheng, L. Jiang, and W. Macherey. Robust neural machine translation
    with doubly adversarial inputs. In Proceedings of the 57th Conference of the Association
    for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,
    pages 4324–4333.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Cheng, L. Jiang, 和 W. Macherey. 使用双重对抗输入的鲁棒神经机器翻译。发表于第57届计算语言学协会会议，ACL
    2019，意大利佛罗伦萨，2019年7月28日至8月2日，页面4324–4333。'
- en: '[53] K. T. Co, L. Muñoz-González, S. de Maupeou, and E. C. Lupu. Procedural
    noise adversarial examples for black-box attacks on deep convolutional networks.
    In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications
    Security, CCS 2019, London, UK, November 11-15, 2019, pages 275–289.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] K. T. Co, L. Muñoz-González, S. de Maupeou, 和 E. C. Lupu. 黑箱攻击深度卷积网络的程序化噪声对抗样本。发表于2019年ACM
    SIGSAC计算机与通信安全会议，CCS 2019，伦敦，英国，2019年11月11-15日，页面275–289。'
- en: '[54] J. R. C. da Silva, R. F. Berriel, C. Badue, A. F. de Souza, and T. Oliveira-Santos.
    Copycat CNN: stealing knowledge by persuading confession with random non-labeled
    data. In International Joint Conference on Neural Networks, IJCNN, Rio de Janeiro,
    Brazil, pages 1–8, July 8-13, 2018.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] J. R. C. da Silva, R. F. Berriel, C. Badue, A. F. de Souza, 和 T. Oliveira-Santos.
    Copycat CNN: 通过随机未标记数据说服 confession 以窃取知识。发表于国际神经网络联合会议，IJCNN，里约热内卢，巴西，页面1–8，2018年7月8-13日。'
- en: '[55] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep
    bidirectional transformers for language understanding. In Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (NAACL-HLT), pages 4171–4186, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. Devlin, M. Chang, K. Lee, 和 K. Toutanova. BERT: 用于语言理解的深度双向转换器预训练。发表于2019年北美计算语言学协会：人类语言技术会议（NAACL-HLT），页面4171–4186，2019年。'
- en: '[56] E. Dohmatob. Generalized no free lunch theorem for adversarial robustness.
    In Proceedings of the 36th International Conference on Machine Learning, ICML
    2019, 9-15 June 2019, Long Beach, California, USA, pages 1646–1654.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] E. Dohmatob. 针对对抗性鲁棒性的广义无免费午餐定理。发表于第36届国际机器学习会议，ICML 2019，2019年6月9-15日，加利福尼亚州洛杉矶，美国，页面1646–1654。'
- en: '[57] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li. Boosting adversarial
    attacks with momentum. In IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR, Salt Lake City, UT, USA, pages 9185–9193, June 18-22, 2018.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, 和 J. Li. 使用动量提升对抗攻击。发表于IEEE计算机视觉与模式识别会议，CVPR，盐湖城，犹他州，美国，页面9185–9193，2018年6月18-22日。'
- en: '[58] Y. Dong, T. Pang, H. Su, and J. Zhu. Evading defenses to transferable
    adversarial examples by translation-invariant attacks. In IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019,
    pages 4312–4321.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. Dong, T. Pang, H. Su, 和 J. Zhu. 通过平移不变攻击规避对可转移对抗样本的防御。发表于IEEE计算机视觉与模式识别会议，CVPR
    2019，长滩，加利福尼亚州，美国，2019年6月16-20日，页面4312–4321。'
- en: '[59] X. Du, X. Xie, Y. Li, L. Ma, Y. Liu, and J. Zhao. Deepstellar: model-based
    quantitative analysis of stateful deep learning systems. In Proceedings of the
    ACM Joint Meeting on European Software Engineering Conference and Symposium on
    the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia,
    August 26-30, 2019., pages 477–487.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] X. Du, X. Xie, Y. Li, L. Ma, Y. Liu, 和 J. Zhao. Deepstellar: 基于模型的有状态深度学习系统的定量分析。发表于ACM联合会议欧洲软件工程会议与软件工程基础研讨会，ESEC/SIGSOFT
    FSE 2019，爱沙尼亚塔林，2019年8月26-30日，页面477–487。'
- en: '[60] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor. Our data,
    ourselves: Privacy via distributed noise generation. In 25th Annual International
    Conference on the Theory and Applications of Cryptographic Techniques, St. Petersburg,
    Russia, pages 486–503, May 28-June 1, 2006.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, 和 M. Naor. 我们的数据，我们自己：通过分布式噪声生成保护隐私。发表于第25届国际密码学理论与应用大会，圣彼得堡，俄罗斯，页面486–503，2006年5月28日至6月1日。'
- en: '[61] C. Dwork, F. McSherry, K. Nissim, and A. D. Smith. Calibrating noise to
    sensitivity in private data analysis. In Theory of Cryptography, Third Theory
    of Cryptography Conference, TCC, New York, NY, USA, pages 265–284, March 4-7,
    2006.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] C. Dwork, F. McSherry, K. Nissim, 和 A. D. Smith. 校准噪声以适应隐私数据分析中的敏感度。发表于密码学理论，第三届密码学理论会议，TCC，纽约，美国，页面265–284，2006年3月4-7日。'
- en: '[62] L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry. Exploring
    the landscape of spatial robustness. In Proceedings of the 36th International
    Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
    USA, pages 1802–1811.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] L. Engstrom, B. Tran, D. Tsipras, L. Schmidt 和 A. Madry. 探索空间鲁棒性的景观。载于第36届国际机器学习会议论文集，ICML
    2019，2019年6月9日至15日，加利福尼亚州长滩，美国，第1802–1811页。'
- en: '[63] M. Fang, X. Cao, J. Jia, and N. Gong. Local model poisoning attacks to
    byzantine-robust federated learning. In 29th USENIX Security Symposium (USENIX
    Security 20), pages 1605–1622\. USENIX Association, Aug. 2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] M. Fang, X. Cao, J. Jia 和 N. Gong. 针对拜占庭鲁棒联邦学习的局部模型中毒攻击。载于第29届USENIX安全研讨会（USENIX
    Security 20），第1605–1622页。USENIX协会，2020年8月。'
- en: '[64] J. Feng, H. Xu, S. Mannor, and S. Yan. Robust logistic regression and
    classification. In Annual Conference on Neural Information Processing Systems,
    Montreal, Quebec, Canada, pages 253–261, December 8-13, 2014.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. Feng, H. Xu, S. Mannor 和 S. Yan. 鲁棒逻辑回归与分类。载于神经信息处理系统年度会议论文集，蒙特利尔，魁北克，加拿大，第253–261页，2014年12月8日至13日。'
- en: '[65] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that
    exploit confidence information and basic countermeasures. In Proceedings of the
    22nd ACM SIGSAC Conference on Computer and Communications Security, Denver, CO,
    USA, pages 1322–1333, October 12-16, 2015.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] M. Fredrikson, S. Jha 和 T. Ristenpart. 利用置信信息的模型反演攻击及基本对策。载于第22届ACM SIGSAC计算机与通信安全会议论文集，丹佛，CO，美国，第1322–1333页，2015年10月12日至16日。'
- en: '[66] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov. Property inference
    attacks on fully connected neural networks using permutation invariant representations.
    In Proceedings of the ACM SIGSAC Conference on Computer and Communications Security,
    CCS, Toronto, ON, Canada, pages 619–633, October 15-19, 2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] K. Ganju, Q. Wang, W. Yang, C. A. Gunter 和 N. Borisov. 使用排列不变表示对全连接神经网络进行属性推断攻击。载于ACM
    SIGSAC计算机与通信安全会议论文集，CCS，多伦多，ON，加拿大，第619–633页，2018年10月15日至19日。'
- en: '[67] J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi. Black-box generation of
    adversarial text sequences to evade deep learning classifiers. In IEEE Security
    and Privacy Workshops, SP Workshops, San Francisco, CA, USA, pages 50–56, May
    24, 2018.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Gao, J. Lanchantin, M. L. Soffa 和 Y. Qi. 生成对抗文本序列以规避深度学习分类器。载于IEEE安全与隐私研讨会，SP研讨会，旧金山，加州，美国，第50–56页，2018年5月24日。'
- en: '[68] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal. STRIP:
    a defence against trojan attacks on deep neural networks. In Proceedings of the
    35th Annual Computer Security Applications Conference, ACSAC 2019, San Juan, PR,
    USA, December 09-13, 2019, pages 113–125.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe 和 S. Nepal. STRIP：防御深度神经网络中的木马攻击。载于第35届年度计算机安全应用会议论文集，ACSAC
    2019，2019年12月9日至13日，圣胡安，PR，美国，第113–125页。'
- en: '[69] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and
    M. T. Vechev. AI2: safety and robustness certification of neural networks with
    abstract interpretation. In IEEE Symposium on Security and Privacy, SP, San Francisco,
    CA, USA, pages 3–18, May 21-23, 2018.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri 和 M.
    T. Vechev. AI2：通过抽象解释对神经网络进行安全性和鲁棒性认证。载于IEEE安全与隐私研讨会，SP，旧金山，加州，美国，第3–18页，2018年5月21日至23日。'
- en: '[70] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. E. Lauter, M. Naehrig, and
    J. Wernsing. Cryptonets: Applying neural networks to encrypted data with high
    throughput and accuracy. In Proceedings of the 33nd International Conference on
    Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 201–210,
    2016.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. E. Lauter, M. Naehrig 和 J.
    Wernsing. Cryptonets：将神经网络应用于加密数据，以高吞吐量和准确性进行处理。载于第33届国际机器学习会议论文集，ICML 2016，2016年6月19日至24日，纽约市，NY，美国，第201–210页，2016年。'
- en: '[71] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell. Adversarial
    policies: Attacking deep reinforcement learning. In International Conference on
    Learning Representations, 2020.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine 和 S. Russell. 对抗策略：攻击深度强化学习。载于2020年国际学习表征会议。'
- en: '[72] Y. Gong, B. Li, C. Poellabauer, and Y. Shi. Real-time adversarial attacks.
    In Proceedings of the Twenty-Eighth International Joint Conference on Artificial
    Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 4672–4680.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Gong, B. Li, C. Poellabauer 和 Y. Shi. 实时对抗攻击。载于第二十八届国际人工智能联合会议论文集，IJCAI
    2019，2019年8月10日至16日，澳门，中国，第4672–4680页。'
- en: '[73] Y. Gong and C. Poellabauer. Crafting adversarial examples for speech paralinguistics
    applications. CoRR, abs/1711.03280, 2017.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. Gong 和 C. Poellabauer. 为语音副语言应用制作对抗样本。CoRR，abs/1711.03280，2017年。'
- en: '[74] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing
    adversarial examples. CoRR, abs/1412.6572, 2014.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] I. J. Goodfellow, J. Shlens, 和 C. Szegedy. 解释和利用对抗性示例. CoRR, abs/1412.6572,
    2014.'
- en: '[75] S. Gu and L. Rigazio. Towards deep neural network architectures robust
    to adversarial examples. CoRR, abs/1412.5068, 2014.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] S. Gu 和 L. Rigazio. 朝着对抗性示例鲁棒的深度神经网络架构. CoRR, abs/1412.5068, 2014.'
- en: '[76] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg. Badnets: Evaluating backdooring
    attacks on deep neural networks. IEEE Access, 7:47230–47244, 2019.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] T. Gu, K. Liu, B. Dolan-Gavitt, 和 S. Garg. Badnets: 评估深度神经网络中的后门攻击. IEEE
    Access, 7:47230–47244, 2019.'
- en: '[77] C. Guo, J. S. Frank, and K. Q. Weinberger. Low frequency adversarial perturbation.
    CoRR, abs/1809.08758, 2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] C. Guo, J. S. Frank, 和 K. Q. Weinberger. 低频对抗扰动. CoRR, abs/1809.08758,
    2018.'
- en: '[78] C. Guo, M. Rana, M. Cissé, and L. van der Maaten. Countering adversarial
    images using input transformations. CoRR, abs/1711.00117, 2017.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] C. Guo, M. Rana, M. Cissé, 和 L. van der Maaten. 使用输入变换对抗对抗性图像. CoRR, abs/1711.00117,
    2017.'
- en: '[79] J. Guo, Y. Jiang, Y. Zhao, Q. Chen, and J. Sun. Dlfuzz: differential fuzzing
    testing of deep learning systems. In Proceedings of the 2018 ACM Joint Meeting
    on European Software Engineering Conference and Symposium on the Foundations of
    Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November
    04-09, 2018, pages 739–743.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] J. Guo, Y. Jiang, Y. Zhao, Q. Chen, 和 J. Sun. Dlfuzz: 深度学习系统的差分模糊测试. 在2018年ACM联合欧洲软件工程会议与软件工程基础研讨会,
    ESEC/SIGSOFT FSE 2018, 美国佛罗里达州湖布埃纳维斯塔, 2018年11月4-9日, 页739–743.'
- en: '[80] Y. Guo, Z. Yan, and C. Zhang. Subspace attack: Exploiting promising subspaces
    for query-efficient black-box attacks. In Advances in Neural Information Processing
    Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS
    2019, 8-14 December 2019, Vancouver, BC, Canada, pages 3820–3829.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Guo, Z. Yan, 和 C. Zhang. 子空间攻击: 利用有前景的子空间进行查询高效的黑箱攻击. 在神经信息处理系统进展32:
    神经信息处理系统年会2019, NeurIPS 2019, 2019年12月8-14日, 加拿大温哥华, 页3820–3829.'
- en: '[81] I. Hagestedt, Y. Zhang, M. Humbert, P. Berrang, H. Tang, X. Wang, and
    M. Backes. Mbeacon: Privacy-preserving beacons for DNA methylation data. In 26th
    Annual Network and Distributed System Security Symposium, NDSS 2019, San Diego,
    California, USA, February 24-27, 2019.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] I. Hagestedt, Y. Zhang, M. Humbert, P. Berrang, H. Tang, X. Wang, 和 M.
    Backes. Mbeacon: 用于DNA甲基化数据的隐私保护信标. 在第26届年度网络和分布式系统安全研讨会, NDSS 2019, 美国加州圣地亚哥,
    2019年2月24-27日.'
- en: '[82] I. A. Hamilton. Amazon built an ai tool to hire people but had to shut
    it down because it was discriminating against women. https://www.businessinsider.com/amazon-built-ai-to-hire-people-discriminated-against-women-2018-10,
    Oct. 2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] I. A. Hamilton. 亚马逊开发了一种AI工具来招聘人员，但因为对女性存在歧视而不得不关闭. https://www.businessinsider.com/amazon-built-ai-to-hire-people-discriminated-against-women-2018-10,
    2018年10月.'
- en: '[83] J. Hamm, Y. Cao, and M. Belkin. Learning privately from multiparty data.
    In Proceedings of the 33nd International Conference on Machine Learning, ICML,
    New York City, NY, USA, pages 555–563, June 19-24, 2016.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. Hamm, Y. Cao, 和 M. Belkin. 从多方数据中私密学习. 在第33届国际机器学习大会论文集, ICML, 美国纽约市,
    页555–563, 2016年6月19-24日.'
- en: '[84] R. Hasan, R. Sion, and M. Winslett. The case of the fake picasso: Preventing
    history forgery with secure provenance. In 7th USENIX Conference on File and Storage
    Technologies, February 24-27, 2009, San Francisco, CA, USA. Proceedings, pages
    1–14, 2009.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] R. Hasan, R. Sion, 和 M. Winslett. 伪毕加索的案例: 通过安全来源防止历史伪造. 在第7届USENIX文件和存储技术会议,
    2009年2月24-27日, 美国加州旧金山. 会议记录, 页1–14, 2009.'
- en: '[85] J. Hayes and G. Danezis. Learning universal adversarial perturbations
    with generative models. In IEEE Security and Privacy Workshops, SP Workshops,
    San Francisco, CA, USA, pages 43–49, May 24, 2018.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Hayes 和 G. Danezis. 使用生成模型学习通用对抗扰动. 在IEEE安全与隐私研讨会, SP研讨会, 美国加州旧金山,
    页43–49, 2018年5月24日.'
- en: '[86] J. Hayes, L. Melis, G. Danezis, and E. D. Cristofaro. LOGAN: evaluating
    privacy leakage of generative models using generative adversarial networks. CoRR,
    abs/1705.07663, 2017.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Hayes, L. Melis, G. Danezis, 和 E. D. Cristofaro. LOGAN: 使用生成对抗网络评估生成模型的隐私泄露.
    CoRR, abs/1705.07663, 2017.'
- en: '[87] W. He, B. Li, and D. Song. Decision boundary analysis of adversarial examples.
    In International Conference on Learning Representations, 2018.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] W. He, B. Li, 和 D. Song. 对抗性示例的决策边界分析. 在国际学习表征会议, 2018.'
- en: '[88] Z. He, T. Zhang, and R. B. Lee. Model inversion attacks against collaborative
    inference. In Proceedings of the 35th Annual Computer Security Applications Conference,
    ACSAC 2019, San Juan, PR, USA, December 09-13, 2019.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Z. He, T. Zhang, 和 R. B. Lee. 针对协作推理的模型反演攻击。发表于第35届年度计算机安全应用会议（ACSAC 2019），波多黎各圣胡安，美国，2019年12月09日至13日。'
- en: '[89] B. Hitaj, G. Ateniese, and F. Pérez-Cruz. Deep models under the GAN: information
    leakage from collaborative deep learning. In Proceedings of the ACM SIGSAC Conference
    on Computer and Communications Security, CCS, Dallas, TX, USA, pages 603–618,
    October 30-November 03, 2017.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] B. Hitaj, G. Ateniese, 和 F. Pérez-Cruz. GAN下的深度模型：协作深度学习中的信息泄露。发表于ACM
    SIGSAC计算机与通信安全会议（CCS），德克萨斯州达拉斯，美国，页码603–618，2017年10月30日至11月03日。'
- en: '[90] W. Hu and Y. Tan. Black-box attacks against RNN based malware detection
    algorithms. In The Workshops of the The Thirty-Second AAAI Conference on Artificial
    Intelligence, New Orleans, Louisiana, USA, pages 245–251, February 2-7,.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] W. Hu 和 Y. Tan. 针对基于RNN的恶意软件检测算法的黑箱攻击。在第32届AAAI人工智能会议的研讨会，路易斯安那州新奥尔良，美国，页码245–251，2016年2月2日至7日。'
- en: '[91] W. Hu and Y. Tan. Generating adversarial malware examples for black-box
    attacks based on GAN. CoRR, abs/1702.05983, 2017.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] W. Hu 和 Y. Tan. 基于GAN的黑箱攻击生成对抗恶意软件样本。CoRR，abs/1702.05983，2017年。'
- en: '[92] W. Hua, Z. Zhang, and G. E. Suh. Reverse engineering convolutional neural
    networks through side-channel information leaks. In Proceedings of the 55th Annual
    Design Automation Conference, DAC, San Francisco, CA, USA, pages 4:1–4:6, June
    24-29, 2018.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] W. Hua, Z. Zhang, 和 G. E. Suh. 通过侧信道信息泄露反向工程卷积神经网络。发表于第55届年度设计自动化会议（DAC），加利福尼亚州旧金山，美国，页码4:1–4:6，2018年6月24日至29日。'
- en: '[93] R. Huang, B. Xu, D. Schuurmans, and C. Szepesvári. Learning with a strong
    adversary. CoRR, abs/1511.03034, 2015.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] R. Huang, B. Xu, D. Schuurmans, 和 C. Szepesvári. 与强敌对者的学习。CoRR，abs/1511.03034，2015年。'
- en: '[94] S. H. Huang, N. Papernot, I. J. Goodfellow, Y. Duan, and P. Abbeel. Adversarial
    attacks on neural network policies. CoRR, abs/1702.02284, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] S. H. Huang, N. Papernot, I. J. Goodfellow, Y. Duan, 和 P. Abbeel. 对神经网络策略的对抗攻击。CoRR，abs/1702.02284，2017年。'
- en: '[95] W. Huang and J. W. Stokes. Mtnet: A multi-task neural network for dynamic
    malware classification. In 13th International Conference, Detection of Intrusions
    and Malware, and Vulnerability Assessment, DIMVA, San Sebastián, Spain, pages
    399–418, July 7-8, 2016.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] W. Huang 和 J. W. Stokes. Mtnet：一种用于动态恶意软件分类的多任务神经网络。在第13届国际会议上，入侵检测与恶意软件检测与漏洞评估（DIMVA），西班牙圣塞巴斯蒂安，页码399–418，2016年7月7日至8日。'
- en: '[96] X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo, M. Wu, and
    X. Yi. A survey of safety and trustworthiness of deep neural networks: Verification,
    testing, adversarial attack and defence, and interpretability. Comput. Sci. Rev.,
    37:100270, 2020.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo, M. Wu, 和 X.
    Yi. 深度神经网络的安全性与可信度调查：验证、测试、对抗攻击与防御、以及可解释性。计算机科学评论，37:100270，2020年。'
- en: '[97] N. Hynes, R. Cheng, and D. Song. Efficient deep learning on multi-source
    private data. CoRR, abs/1807.06689, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] N. Hynes, R. Cheng, 和 D. Song. 在多源私有数据上的高效深度学习。CoRR，abs/1807.06689，2018年。'
- en: '[98] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Query-efficient black-box
    adversarial examples. CoRR, abs/1712.07113, 2017.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] A. Ilyas, L. Engstrom, A. Athalye, 和 J. Lin. 查询高效的黑箱对抗样本。CoRR，abs/1712.07113，2017年。'
- en: '[99] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Black-box adversarial attacks
    with limited queries and information. In Proceedings of the 35th International
    Conference on Machine Learning, ICML, Stockholmsmässan, Stockholm, Sweden, pages
    2142–2151, 2018.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Ilyas, L. Engstrom, A. Athalye, 和 J. Lin. 限查询和信息下的黑箱对抗攻击。发表于第35届国际机器学习会议（ICML），瑞典斯德哥尔摩，页码2142–2151，2018年。'
- en: '[100] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li.
    Manipulating machine learning: Poisoning attacks and countermeasures for regression
    learning. In IEEE Symposium on Security and Privacy, SP, San Francisco, California,
    USA, pages 19–35, May 21-23, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, 和 B. Li. 操控机器学习：回归学习的投毒攻击及对策。发表于IEEE安全与隐私研讨会（SP），加利福尼亚州旧金山，美国，页码19–35，2018年5月21日至23日。'
- en: '[101] S. Jan, A. Panichella, A. Arcuri, and L. C. Briand. Automatic generation
    of tests to exploit XML injection vulnerabilities in web applications. IEEE Trans.
    Software Eng., 45(4):335–362, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. Jan, A. Panichella, A. Arcuri, 和 L. C. Briand. 自动生成测试以利用Web应用程序中的XML注入漏洞。IEEE软件工程学报，45(4):335–362，2019年。'
- en: '[102] U. Jang, X. Wu, and S. Jha. Objective metrics and gradient descent algorithms
    for adversarial examples in machine learning. In Proceedings of the 33rd Annual
    Computer Security Applications Conference, Orlando, FL, USA, December 4-8, 2017,
    pages 262–277.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] U. Jang, X. Wu, 和 S. Jha. 机器学习中对抗样本的客观度量和梯度下降算法。发表于第33届年度计算机安全应用会议，奥兰多，FL，美国，2017年12月4日至8日，页面262–277。'
- en: '[103] Y. Jang, T. Zhao, S. Hong, and H. Lee. Adversarial defense via learning
    to generate diverse attacks. In 2019 IEEE/CVF International Conference on Computer
    Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages
    2740–2749.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Jang, T. Zhao, S. Hong, 和 H. Lee. 通过学习生成多样化攻击进行对抗防御。发表于2019 IEEE/CVF计算机视觉国际会议（ICCV
    2019），首尔，韩国，2019年10月27日至11月2日，页面2740–2749。'
- en: '[104] J. Jia, A. Salem, M. Backes, Y. Zhang, and N. Z. Gong. Memguard: Defending
    against black-box membership inference attacks via adversarial examples. In Proceedings
    of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS
    2019, London, UK, November 11-15, 2019, pages 259–274.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. Jia, A. Salem, M. Backes, Y. Zhang, 和 N. Z. Gong. Memguard: 通过对抗样本防御黑盒成员推断攻击。发表于2019年ACM
    SIGSAC计算机与通信安全会议（CCS 2019），伦敦，英国，2019年11月11日至15日，页面259–274。'
- en: '[105] X. Jiang, M. Kim, K. E. Lauter, and Y. Song. Secure outsourced matrix
    computation and application to neural networks. In Proceedings of the 2018 ACM
    SIGSAC Conference on Computer and Communications Security, CCS 2018, Toronto,
    ON, Canada, October 15-19, 2018, pages 1209–1222, 2018.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] X. Jiang, M. Kim, K. E. Lauter, 和 Y. Song. 安全的外包矩阵计算及其在神经网络中的应用。发表于2018年ACM
    SIGSAC计算机与通信安全会议（CCS 2018），多伦多，ON，加拿大，2018年10月15日至19日，页面1209–1222。'
- en: '[106] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov. Bag of tricks for
    efficient text classification. In Proceedings of the 15th Conference of the European
    Chapter of the Association for Computational Linguistics: Volume 2, Short Papers,
    pages 427–431\. Association for Computational Linguistics, April 2017.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Joulin, E. Grave, P. Bojanowski, 和 T. Mikolov. 高效文本分类的技巧合集。发表于第15届欧洲计算语言学协会会议（Volume
    2, Short Papers），页面427–431，计算语言学协会，2017年4月。'
- en: '[107] M. Juuti, S. Szyller, A. Dmitrenko, S. Marchal, and N. Asokan. PRADA:
    protecting against DNN model stealing attacks. CoRR, abs/1805.02628, 2018.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] M. Juuti, S. Szyller, A. Dmitrenko, S. Marchal, 和 N. Asokan. PRADA：防御DNN模型盗窃攻击。CoRR,
    abs/1805.02628, 2018年。'
- en: '[108] C. Juvekar, V. Vaikuntanathan, and A. Chandrakasan. GAZELLE: A low latency
    framework for secure neural network inference. In 27th USENIX Security Symposium,
    USENIX Security, Baltimore, MD, USA, pages 1651–1669, August 15-17, 2018.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] C. Juvekar, V. Vaikuntanathan, 和 A. Chandrakasan. GAZELLE：一个低延迟的安全神经网络推理框架。发表于第27届USENIX安全研讨会（USENIX
    Security），巴尔的摩，MD，美国，页面1651–1669，2018年8月15日至17日。'
- en: '[109] A. Kantchelian, S. Afroz, L. Huang, A. C. Islam, B. Miller, M. C. Tschantz,
    R. Greenstadt, A. D. Joseph, and J. D. Tygar. Approaches to adversarial drift.
    In Proceedings of the ACM Workshop on Artificial Intelligence and Security, AISec,
    Berlin, Germany, pages 99–110, November 4, 2013.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] A. Kantchelian, S. Afroz, L. Huang, A. C. Islam, B. Miller, M. C. Tschantz,
    R. Greenstadt, A. D. Joseph, 和 J. D. Tygar. 对抗漂移的方法。发表于ACM人工智能与安全研讨会（AISec），柏林，德国，页面99–110，2013年11月4日。'
- en: '[110] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Towards
    proving the adversarial robustness of deep neural networks. In Proceedings First
    Workshop on Formal Verification of Autonomous Vehicles, FVAV@iFM, Turin, Italy,
    19th September., pages 19–26, 2017.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] G. Katz, C. Barrett, D. L. Dill, K. Julian, 和 M. J. Kochenderfer. 朝着证明深度神经网络的对抗鲁棒性迈进。发表于第一次自动驾驶汽车正式验证研讨会（FVAV@iFM），都灵，意大利，2017年9月19日，页面19–26。'
- en: '[111] M. Kesarwani, B. Mukhoty, V. Arya, and S. Mehta. Model extraction warning
    in mlaas paradigm. CoRR, abs/1711.07221, 2017.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] M. Kesarwani, B. Mukhoty, V. Arya, 和 S. Mehta. 在MLaaS范式中的模型提取警告。CoRR,
    abs/1711.07221, 2017年。'
- en: '[112] D. Kifer, A. D. Smith, and A. Thakurta. Private convex optimization for
    empirical risk minimization with applications to high-dimensional regression.
    In The 25th Annual Conference on Learning Theory, COLT, Edinburgh, Scotland, pages
    25.1–25.40, June 25-27, 2012.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] D. Kifer, A. D. Smith, 和 A. Thakurta. 具有高维回归应用的经验风险最小化的私人凸优化。发表于第25届年度学习理论会议（COLT），爱丁堡，苏格兰，页面25.1–25.40，2012年6月25日至27日。'
- en: '[113] J. Kim, R. Feldt, and S. Yoo. Guiding deep learning system testing using
    surprise adequacy. In Proceedings of the 41st International Conference on Software
    Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019, pages 1039–1049.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] J. Kim, R. Feldt, 和 S. Yoo. 使用惊讶性充分性指导深度学习系统测试。在第41届国际软件工程会议论文集中，ICSE
    2019，加拿大蒙特利尔，2019年5月25日至31日，页面1039–1049。'
- en: '[114] J. Kos, I. Fischer, and D. Song. Adversarial examples for generative
    models. In IEEE Security and Privacy Workshops, SP Workshops, San Francisco, CA,
    USA, May 24, 2018, pages 36–42.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] J. Kos, I. Fischer, 和 D. Song. 生成模型的对抗性示例。在IEEE安全与隐私研讨会，SP研讨会，加州旧金山，美国，2018年5月24日，页面36–42。'
- en: '[115] C. Kou, H. K. Lee, E.-C. Chang, and T. K. Ng. Enhancing transformation-based
    defenses against adversarial attacks with a distribution classifier. In International
    Conference on Learning Representations, 2020.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] C. Kou, H. K. Lee, E.-C. Chang, 和 T. K. Ng. 使用分布分类器增强基于变换的防御对抗性攻击。在国际学习表征会议，2020年。'
- en: '[116] F. Kreuk, A. Barak, S. Aviv-Reuven, M. Baruch, B. Pinkas, and J. Keshet.
    Deceiving end-to-end deep learning malware detectors using adversarial examples.
    2018.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] F. Kreuk, A. Barak, S. Aviv-Reuven, M. Baruch, B. Pinkas, 和 J. Keshet.
    使用对抗性示例欺骗端到端深度学习恶意软件检测器。2018年。'
- en: '[117] A. Krizhevsky, V. Nair, and G. Hinton. CIFAR dataset. https://www.cs.toronto.edu/~kriz/cifar.html,
    2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] A. Krizhevsky, V. Nair, 和 G. Hinton. CIFAR数据集。https://www.cs.toronto.edu/~kriz/cifar.html，2019年。'
- en: '[118] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial machine learning
    at scale. CoRR, abs/1611.01236, 2016.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] A. Kurakin, I. J. Goodfellow, 和 S. Bengio. 大规模对抗性机器学习。CoRR，abs/1611.01236，2016年。'
- en: '[119] K. Kurita, P. Michel, and G. Neubig. Weight poisoning attacks on pretrained
    models. In Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, ACL 2020, Online, July 5-10, 2020, pages 2793–2806.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] K. Kurita, P. Michel, 和 G. Neubig. 对预训练模型的权重中毒攻击。在第58届计算语言学协会年会论文集中，ACL
    2020，线上，2020年7月5日至10日，页面2793–2806。'
- en: '[120] Y. LeCun, C. Cortes, and C. Burges. Mnist dataset. http://yann.lecun.com/exdb/mnist/,
    2017.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Y. LeCun, C. Cortes, 和 C. Burges. Mnist数据集。http://yann.lecun.com/exdb/mnist/，2017年。'
- en: '[121] M. Lécuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana. Certified
    robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium
    on Security and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pages
    656–672.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Lécuyer, V. Atlidakis, R. Geambasu, D. Hsu, 和 S. Jana. 具有差分隐私的对抗性示例认证鲁棒性。在2019年IEEE安全与隐私研讨会，SP
    2019，加州旧金山，美国，2019年5月19日至23日，页面656–672。'
- en: '[122] T. Lee, B. Edwards, I. Molloy, and D. Su. Defending against model stealing
    attacks using deceptive perturbations. CoRR, abs/1806.00054, 2018.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] T. Lee, B. Edwards, I. Molloy, 和 D. Su. 使用欺骗性扰动防御模型盗窃攻击。CoRR，abs/1806.00054，2018年。'
- en: '[123] J. Li, S. Ji, T. Du, B. Li, and T. Wang. Textbugger: Generating adversarial
    text against real-world applications. In 26th Annual Network and Distributed System
    Security Symposium, NDSS 2019, San Diego, California, USA, February 24-27, 2019.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] J. Li, S. Ji, T. Du, B. Li, 和 T. Wang. Textbugger：针对现实世界应用生成对抗性文本。在第26届年度网络与分布式系统安全研讨会，NDSS
    2019，加州圣地亚哥，美国，2019年2月24日至27日。'
- en: '[124] J. Li, F. R. Schmidt, and J. Z. Kolter. Adversarial camera stickers:
    A physical camera-based attack on deep learning systems. In Proceedings of the
    36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,
    Long Beach, California, USA, pages 3896–3904.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. Li, F. R. Schmidt, 和 J. Z. Kolter. 对抗性摄像头贴纸：一种基于物理摄像头的深度学习系统攻击。在第36届国际机器学习会议论文集中，ICML
    2019，2019年6月9日至15日，加利福尼亚州长滩，美国，页面3896–3904。'
- en: '[125] L. Li, Z. Zhong, B. Li, and T. Xie. Robustra: Training provable robust
    neural networks over reference adversarial space. In Proceedings of the Twenty-Eighth
    International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,
    China, August 10-16, 2019, pages 4711–4717.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] L. Li, Z. Zhong, B. Li, 和 T. Xie. Robustra：在参考对抗空间上训练可证明鲁棒的神经网络。在第二十八届国际联合人工智能会议论文集中，IJCAI
    2019，中国澳门，2019年8月10日至16日，页面4711–4717。'
- en: '[126] P. Li, J. Yi, and L. Zhang. Query-efficient black-box attack by active
    learning. In IEEE International Conference on Data Mining, ICDM , Singapore, November
    17-20, 2018, pages 1200–1205.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] P. Li, J. Yi, 和 L. Zhang. 通过主动学习进行查询高效的黑盒攻击。在IEEE国际数据挖掘会议，ICDM，新加坡，2018年11月17日至20日，页面1200–1205。'
- en: '[127] S. Li, A. Neupane, S. Paul, C. Song, S. V. Krishnamurthy, A. K. Roy-Chowdhury,
    and A. Swami. Stealthy adversarial perturbations against real-time video classification
    systems. In 26th Annual Network and Distributed System Security Symposium, NDSS
    2019, San Diego, California, USA, February 24-27, 2019.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] S. Li, A. Neupane, S. Paul, C. Song, S. V. Krishnamurthy, A. K. Roy-Chowdhury,
    和 A. Swami. 针对实时视频分类系统的隐蔽对抗扰动。在第26届网络和分布式系统安全研讨会（NDSS 2019），2019年2月24-27日，圣地亚哥，加利福尼亚州，美国。'
- en: '[128] Y. Li, E. X.Fang, H. Xu, and T. Zhao. Implicit bias of gradient descent
    based adversarial training on separable data. In International Conference on Learning
    Representations, 2020.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Y. Li, E. X. Fang, H. Xu, 和 T. Zhao. 基于梯度下降的对抗训练在可分数据上的隐性偏差。发表于《国际学习表征会议》，2020年。'
- en: '[129] B. Liang, H. Li, M. Su, X. Li, W. Shi, and X. Wang. Detecting adversarial
    image examples in deep networks with adaptive noise reduction. 2017.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] B. Liang, H. Li, M. Su, X. Li, W. Shi, 和 X. Wang. 通过自适应噪声减少检测深度网络中的对抗图像示例。2017年。'
- en: '[130] R. Light. Ai trends: Machine learning as a service (mlaas). https://learn.g2.com/trends/machine-learning-service-mlaas,
    Jan. 2018.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] R. Light. 人工智能趋势：机器学习即服务（mlaas）。https://learn.g2.com/trends/machine-learning-service-mlaas，2018年1月。'
- en: '[131] C. Liu and J. JáJá. Feature prioritization and regularization improve
    standard accuracy and adversarial robustness. In Proceedings of the Twenty-Eighth
    International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,
    China, August 10-16, 2019, pages 2994–3000.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] C. Liu 和 J. JáJá. 特征优先级和正则化提高了标准准确性和对抗鲁棒性。在第28届国际联合人工智能会议（IJCAI 2019），2019年8月10-16日，澳门，中国，第2994–3000页。'
- en: '[132] C. Liu, B. Li, Y. Vorobeychik, and A. Oprea. Robust linear regression
    against training data poisoning. In Proceedings of the 10th ACM Workshop on Artificial
    Intelligence and Security, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017,
    pages 91–102, 2017.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] C. Liu, B. Li, Y. Vorobeychik, 和 A. Oprea. 针对训练数据投毒的鲁棒线性回归。在第10届ACM人工智能与安全工作坊（AISec@CCS
    2017），2017年11月3日，达拉斯，德克萨斯州，美国，第91–102页，2017年。'
- en: '[133] C. Liu, R. Tomioka, and V. Cevher. On certifying non-uniform bounds against
    adversarial attacks. In Proceedings of the 36th International Conference on Machine
    Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 4072–4081.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] C. Liu, R. Tomioka, 和 V. Cevher. 认证对抗攻击的非均匀界限。在第36届国际机器学习会议（ICML 2019），2019年6月9-15日，长滩，加利福尼亚州，美国，第4072–4081页。'
- en: '[134] F. Liu and N. B. Shroff. Data poisoning attacks on stochastic bandits.
    In Proceedings of the 36th International Conference on Machine Learning, ICML
    2019, 9-15 June 2019, Long Beach, California, USA, pages 4042–4050.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] F. Liu 和 N. B. Shroff. 对随机赌博机的数据投毒攻击。在第36届国际机器学习会议（ICML 2019），2019年6月9-15日，长滩，加利福尼亚州，美国，第4042–4050页。'
- en: '[135] H. Liu, R. Ji, J. Li, B. Zhang, Y. Gao, Y. Wu, and F. Huang. Universal
    adversarial perturbation via prior driven uncertainty approximation. In 2019 IEEE/CVF
    International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South),
    October 27 - November 2, 2019, pages 2941–2949.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] H. Liu, R. Ji, J. Li, B. Zhang, Y. Gao, Y. Wu, 和 F. Huang. 通过先验驱动的不确定性逼近实现通用对抗扰动。在2019
    IEEE/CVF国际计算机视觉会议（ICCV 2019），2019年10月27日 - 11月2日，首尔，韩国，第2941–2949页。'
- en: '[136] J. Liu, M. Juuti, Y. Lu, and N. Asokan. Oblivious neural network predictions
    via minionn transformations. In Proceedings of the ACM SIGSAC Conference on Computer
    and Communications Security, CCS, Dallas, TX, USA, October 30 - November 03, 2017,
    pages 619–631.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] J. Liu, M. Juuti, Y. Lu, 和 N. Asokan. 通过minionn变换实现隐式神经网络预测。在ACM SIGSAC计算机与通信安全会议（CCS），2017年10月30日
    - 11月3日，达拉斯，德克萨斯州，美国，第619–631页。'
- en: '[137] K. S. Liu, B. Li, and J. Gao. Generative model: Membership attack, generalization
    and diversity. CoRR, abs/1805.09898, 2018.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] K. S. Liu, B. Li, 和 J. Gao. 生成模型：会员攻击、泛化和多样性。CoRR, abs/1805.09898, 2018年。'
- en: '[138] Q. Liu, P. Li, W. Zhao, W. Cai, S. Yu, and V. C. M. Leung. A survey on
    security threats and defensive techniques of machine learning: A data driven view.
    IEEE Access, 6:12103–12117, 2018.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Q. Liu, P. Li, W. Zhao, W. Cai, S. Yu, 和 V. C. M. Leung. 关于机器学习的安全威胁和防御技术的调查：基于数据的视角。IEEE
    Access, 6:12103–12117, 2018年。'
- en: '[139] X. Liu, S. Si, J. Zhu, Y. Li, and C. Hsieh. A unified framework for data
    poisoning attack to graph-based semi-supervised learning. In Advances in Neural
    Information Processing Systems 32: Annual Conference on Neural Information Processing
    Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 9777–9787.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] X. Liu, S. Si, J. Zhu, Y. Li, 和 C. Hsieh. 用于图基半监督学习的数据中毒攻击统一框架。在《神经信息处理系统进展
    32：2019年神经信息处理系统年会，NeurIPS 2019》，2019年12月8-14日，加拿大不列颠哥伦比亚省温哥华，页码 9777–9787。'
- en: '[140] Y. Liu, W. Lee, G. Tao, S. Ma, Y. Aafer, and X. Zhang. ABS: scanning
    neural networks for back-doors by artificial brain stimulation. In Proceedings
    of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS
    2019, London, UK, November 11-15, 2019, pages 1265–1282.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Y. Liu, W. Lee, G. Tao, S. Ma, Y. Aafer, 和 X. Zhang. ABS：通过人工大脑刺激扫描神经网络的后门。在
    2019 年 ACM SIGSAC 计算机与通信安全会议，CCS 2019，英国伦敦，2019年11月11-15日，页码 1265–1282。'
- en: '[141] Y. Liu, S. Ma, Y. Aafer, W. Lee, J. Zhai, W. Wang, and X. Zhang. Trojaning
    attack on neural networks. In 25th Annual Network and Distributed System Security
    Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Y. Liu, S. Ma, Y. Aafer, W. Lee, J. Zhai, W. Wang, 和 X. Zhang. 对神经网络的木马攻击。在第25届网络和分布式系统安全研讨会，NDSS
    2018，美国加州圣地亚哥，2018年2月18-21日，2018年。'
- en: '[142] Y. Liu, S. Moosavi-Dezfooli, and P. Frossard. A geometry-inspired decision-based
    attack. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019,
    Seoul, Korea (South), October 27 - November 2, 2019, pages 4889–4897.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Y. Liu, S. Moosavi-Dezfooli, 和 P. Frossard. 基于几何的决策攻击。在 2019 年 IEEE/CVF
    国际计算机视觉会议，ICCV 2019，韩国首尔，2019年10月27日 - 11月2日，页码 4889–4897。'
- en: '[143] Z. Liu, Q. Liu, T. Liu, N. Xu, X. Lin, Y. Wang, and W. Wen. Feature distillation:
    Dnn-oriented JPEG compression against adversarial examples. In IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June
    16-20, 2019, pages 860–868.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Z. Liu, Q. Liu, T. Liu, N. Xu, X. Lin, Y. Wang, 和 W. Wen. 特征蒸馏：针对对抗样本的DNN导向JPEG压缩。在
    IEEE 计算机视觉与模式识别会议，CVPR 2019，美国加州长滩，2019年6月16-20日，页码 860–868。'
- en: '[144] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A. Gunter,
    and K. Chen. Understanding membership inferences on well-generalized learning
    models. CoRR, abs/1802.04889, 2018.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A. Gunter,
    和 K. Chen. 理解对良好泛化学习模型的成员身份推断。CoRR, abs/1802.04889, 2018。'
- en: '[145] J. Lyle and A. P. Martin. Trusted computing and provenance: Better together.
    In 2nd Workshop on the Theory and Practice of Provenance, TaPP’10, San Jose, CA,
    USA, February 22, 2010, 2010.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] J. Lyle 和 A. P. Martin. 可信计算和溯源：更好的结合。在第二届溯源理论与实践研讨会，TaPP’10，美国加州圣荷西，2010年2月22日，2010年。'
- en: '[146] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su,
    L. Li, Y. Liu, J. Zhao, and Y. Wang. Deepgauge: multi-granularity testing criteria
    for deep learning systems. In Proceedings of the 33rd ACM/IEEE International Conference
    on Automated Software Engineering, ASE 2018, Montpellier, France, September 3-7,
    2018, pages 120–131.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su,
    L. Li, Y. Liu, J. Zhao, 和 Y. Wang. Deepgauge：深度学习系统的多粒度测试标准。在第33届ACM/IEEE国际自动化软件工程会议，ASE
    2018，法国蒙彼利埃，2018年9月3-7日，页码 120–131。'
- en: '[147] S. Ma, Y. Liu, G. Tao, W. Lee, and X. Zhang. NIC: detecting adversarial
    samples with neural network invariant checking. In 26th Annual Network and Distributed
    System Security Symposium, NDSS 2019, San Diego, California, USA, February 24-27,
    2019.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] S. Ma, Y. Liu, G. Tao, W. Lee, 和 X. Zhang. NIC：使用神经网络不变性检测对抗样本。在第26届网络和分布式系统安全研讨会，NDSS
    2019，美国加州圣地亚哥，2019年2月24-27日。'
- en: '[148] X. Ma, B. Li, Y. Wang, S. M. Erfani, S. N. R. Wijewickrema, M. E. Houle,
    G. Schoenebeck, D. Song, and J. Bailey. Characterizing adversarial subspaces using
    local intrinsic dimensionality. CoRR, abs/1801.02613, 2018.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] X. Ma, B. Li, Y. Wang, S. M. Erfani, S. N. R. Wijewickrema, M. E. Houle,
    G. Schoenebeck, D. Song, 和 J. Bailey. 使用局部内在维度来表征对抗子空间。CoRR, abs/1801.02613, 2018。'
- en: '[149] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep
    learning models resistant to adversarial attacks. CoRR, abs/1706.06083, 2017.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, 和 A. Vladu. 朝着抗对抗攻击的深度学习模型迈进。CoRR,
    abs/1706.06083, 2017。'
- en: '[150] S. Mahloujifar, M. Mahmoody, and A. Mohammed. Data poisoning attacks
    in multi-party learning. In Proceedings of the 36th International Conference on
    Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages
    4274–4283.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. Mahloujifar, M. Mahmoody, 和 A. Mohammed. 多方学习中的数据中毒攻击。在第36届国际机器学习会议，ICML
    2019，2019年6月9-15日，美国加州长滩，页码 4274–4283。'
- en: '[151] M. McCoyd and D. A. Wagner. Background class defense against adversarial
    examples. In 2018 IEEE Security and Privacy Workshops, SP Workshops 2018, San
    Francisco, CA, USA, May 24, 2018, pages 96–102, 2018.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] M. McCoyd 和 D. A. Wagner. 背景类防御对抗样本。在 2018 年 IEEE 安全与隐私研讨会，SP Workshops
    2018，旧金山，加利福尼亚州，美国，2018 年 5 月 24 日，页码 96–102，2018 年。'
- en: '[152] S. Mei and X. Zhu. Using machine teaching to identify optimal training-set
    attacks on machine learners. In Proceedings of the Twenty-Ninth AAAI Conference
    on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pages 2871–2877,
    2015.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] S. Mei 和 X. Zhu. 使用机器教学识别对机器学习者的最佳训练集攻击。发表于第二十九届 AAAI 人工智能会议，2015 年 1
    月 25-30 日，奥斯丁，德克萨斯州，美国，页码 2871–2877，2015 年。'
- en: '[153] L. Melis, C. Song, E. D. Cristofaro, and V. Shmatikov. Exploiting unintended
    feature leakage in collaborative learning. In 2019 IEEE Symposium on Security
    and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pages 691–706.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] L. Melis, C. Song, E. D. Cristofaro, 和 V. Shmatikov. 利用协作学习中的意外特征泄露。在
    2019 年 IEEE 安全与隐私研讨会，SP 2019，旧金山，加利福尼亚州，美国，2019 年 5 月 19-23 日，页码 691–706。'
- en: '[154] D. Meng and H. Chen. Magnet: A two-pronged defense against adversarial
    examples. In Proceedings of the ACM SIGSAC Conference on Computer and Communications
    Security, CCS, Dallas, TX, USA, October 30 - November 03, 2017, pages 135–147.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] D. Meng 和 H. Chen. Magnet：一种对抗样本的双管齐下防御。发表于 ACM SIGSAC 计算机与通信安全会议，CCS，达拉斯，德克萨斯州，美国，2017
    年 10 月 30 日 - 11 月 3 日，页码 135–147。'
- en: '[155] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial
    perturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 86–94, 2017.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, 和 P. Frossard. 通用对抗扰动。发表于 2017
    年 IEEE 计算机视觉与模式识别大会，CVPR 2017，檀香山，夏威夷州，美国，2017 年 7 月 21-26 日，页码 86–94，2017 年。'
- en: '[156] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: A simple and
    accurate method to fool deep neural networks. In IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR, Las Vegas, NV, USA, June 27-30, 2016, pages 2574–2582.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] S. Moosavi-Dezfooli, A. Fawzi, 和 P. Frossard. Deepfool：一种简单而准确的欺骗深度神经网络的方法。发表于
    IEEE 计算机视觉与模式识别大会，CVPR，拉斯维加斯，内华达州，美国，2016 年 6 月 27-30 日，页码 2574–2582。'
- en: '[157] C. K. Mummadi, T. Brox, and J. H. Metzen. Defending against universal
    perturbations with shared adversarial training. In 2019 IEEE/CVF International
    Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November
    2, 2019, pages 4927–4936.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] C. K. Mummadi, T. Brox, 和 J. H. Metzen. 使用共享对抗训练防御通用扰动。在 2019 年 IEEE/CVF
    国际计算机视觉大会，ICCV 2019，首尔，韩国，2019 年 10 月 27 日 - 11 月 2 日，页码 4927–4936。'
- en: '[158] L. Muñoz-González, B. Biggio, A. Demontis, A. Paudice, V. Wongrassamee,
    E. C. Lupu, and F. Roli. Towards poisoning of deep learning algorithms with back-gradient
    optimization. In Proceedings of the 10th ACM Workshop on Artificial Intelligence
    and Security, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017, pages 27–38,
    2017.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] L. Muñoz-González, B. Biggio, A. Demontis, A. Paudice, V. Wongrassamee,
    E. C. Lupu, 和 F. Roli. 朝向使用反向梯度优化的深度学习算法中毒。发表于第 10 届 ACM 人工智能与安全研讨会，AISec@CCS
    2017，达拉斯，德克萨斯州，美国，2017 年 11 月 3 日，页码 27–38，2017 年。'
- en: '[159] N. Narodytska and S. P. Kasiviswanathan. Simple black-box adversarial
    attacks on deep neural networks. In IEEE Conference on Computer Vision and Pattern
    Recognition Workshops, CVPR Workshops, Honolulu, HI, USA, July 21-26, 2017, pages
    1310–1318.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] N. Narodytska 和 S. P. Kasiviswanathan. 对深度神经网络的简单黑盒对抗攻击。发表于 IEEE 计算机视觉与模式识别研讨会，CVPR
    Workshops，檀香山，夏威夷州，美国，2017 年 7 月 21-26 日，页码 1310–1318。'
- en: '[160] M. Naseer, S. H. Khan, M. H. Khan, F. S. Khan, and F. Porikli. Cross-domain
    transferability of adversarial perturbations. In Advances in Neural Information
    Processing Systems 32: Annual Conference on Neural Information Processing Systems
    2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 12885–12895.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] M. Naseer, S. H. Khan, M. H. Khan, F. S. Khan, 和 F. Porikli. 对抗扰动的跨领域可转移性。发表于神经信息处理系统
    32：2019 年神经信息处理系统年度会议，NeurIPS 2019，2019 年 12 月 8-14 日，温哥华，不列颠哥伦比亚省，加拿大，页码 12885–12895。'
- en: '[161] M. Nasr, R. Shokri, and A. Houmansadr. Comprehensive privacy analysis
    of deep learning: Passive and active white-box inference attacks against centralized
    and federated learning. In 2019 IEEE Symposium on Security and Privacy, SP 2019,
    San Francisco, CA, USA, May 19-23, 2019, pages 739–753.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] M. Nasr, R. Shokri, 和 A. Houmansadr. 深度学习的全面隐私分析：对集中式和联邦学习的被动和主动白盒推断攻击。发表于
    2019 年 IEEE 安全与隐私研讨会，SP 2019，旧金山，加利福尼亚州，美国，2019 年 5 月 19-23 日，页码 739–753。'
- en: '[162] M. Nasr, R. Shokri, and A. Houmansadr. Machine learning with membership
    privacy using adversarial regularization. In Proceedings of the 2018 ACM SIGSAC
    Conference on Computer and Communications Security (CCS), pages 634–646, 2018.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] M. Nasr, R. Shokri, 和 A. Houmansadr. 使用对抗正则化的机器学习中的会员隐私。在2018年 ACM SIGSAC
    计算机与通信安全会议（CCS）的论文集中，页码 634–646，2018年。'
- en: '[163] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein,
    U. Saini, C. A. Sutton, J. D. Tygar, and K. Xia. Exploiting machine learning to
    subvert your spam filter. In First USENIX Workshop on Large-Scale Exploits and
    Emergent Threats, LEET, 2008.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein,
    U. Saini, C. A. Sutton, J. D. Tygar, 和 K. Xia. 利用机器学习来破坏你的垃圾邮件过滤器。发表于第一次 USENIX
    大规模漏洞与新兴威胁研讨会，LEET，2008年。'
- en: '[164] S. J. Oh, M. Augustin, M. Fritz, and B. Schiele. Towards reverse-engineering
    black-box neural networks. In International Conference on Learning Representations,
    2018.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] S. J. Oh, M. Augustin, M. Fritz, 和 B. Schiele. 反向工程黑盒神经网络的研究。发表于国际学习表征会议，2018年。'
- en: '[165] O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin, K. Vaswani,
    and M. Costa. Oblivious multi-party machine learning on trusted processors. In
    25th USENIX Security Symposium, USENIX Security 16, Austin, TX, USA, August 10-12,
    2016., pages 619–636, 2016.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin, K. Vaswani,
    和 M. Costa. 在受信任的处理器上进行无知多方机器学习。发表于第25届 USENIX 安全研讨会，USENIX Security 16，德克萨斯州奥斯汀，2016年8月10-12日，页码
    619–636，2016年。'
- en: '[166] H. Olufowobi, R. Engel, N. Baracaldo, L. A. D. Bathen, S. Tata, and H. Ludwig.
    Data provenance model for internet of things (iot) systems. In Service-Oriented
    Computing, ICSOC 2016 Workshops, Banff, AB, Canada, October 10-13, 2016., pages
    85–91.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] H. Olufowobi, R. Engel, N. Baracaldo, L. A. D. Bathen, S. Tata, 和 H.
    Ludwig. 物联网（IoT）系统的数据溯源模型。在服务导向计算，ICSOC 2016 工作坊，加拿大班夫，2016年10月10-13日，页码 85–91。'
- en: '[167] T. Orekondy, B. Schiele, and M. Fritz. Knockoff nets: Stealing functionality
    of black-box models. June 2019.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] T. Orekondy, B. Schiele, 和 M. Fritz. Knockoff nets: 盗取黑盒模型的功能。2019年6月。'
- en: '[168] T. Orekondy, B. Schiele, and M. Fritz. Prediction poisoning: Towards
    defenses against dnn model stealing attacks. In International Conference on Learning
    Representations, 2020.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] T. Orekondy, B. Schiele, 和 M. Fritz. 预测毒化：针对深度神经网络模型偷窃攻击的防御。发表于国际学习表征会议，2020年。'
- en: '[169] S. A. Ossia, A. S. Shamsabadi, A. Taheri, H. R. Rabiee, N. D. Lane, and
    H. Haddadi. A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile
    Analytics. CoRR, abs/1703.02952, 2017.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] S. A. Ossia, A. S. Shamsabadi, A. Taheri, H. R. Rabiee, N. D. Lane, 和
    H. Haddadi. 用于隐私保护移动分析的混合深度学习架构。CoRR, abs/1703.02952，2017年。'
- en: '[170] R. Pan. Static deep neural network analysis for robustness. In Proceedings
    of the ACM Joint Meeting on European Software Engineering Conference and Symposium
    on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia,
    August 26-30, 2019., pages 1238–1240.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] R. Pan. 静态深度神经网络的鲁棒性分析。发表于 ACM 欧洲软件工程会议及软件工程基础研讨会的联合会议，ESEC/SIGSOFT FSE
    2019，爱沙尼亚塔林，2019年8月26-30日，页码 1238–1240。'
- en: '[171] T. Pang, C. Du, and J. Zhu. Robust deep learning via reverse cross-entropy
    training and thresholding test. CoRR, abs/1706.00633, 2017.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] T. Pang, C. Du, 和 J. Zhu. 通过逆向交叉熵训练和阈值测试实现鲁棒的深度学习。CoRR, abs/1706.00633，2017年。'
- en: '[172] N. Papernot and P. D. McDaniel. On the effectiveness of defensive distillation.
    CoRR, abs/1607.05113, 2016.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] N. Papernot 和 P. D. McDaniel. 防御性蒸馏的有效性研究。CoRR, abs/1607.05113，2016年。'
- en: '[173] N. Papernot, P. D. McDaniel, and I. J. Goodfellow. Transferability in
    machine learning: from phenomena to black-box attacks using adversarial samples.
    CoRR, abs/1605.07277, 2016.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] N. Papernot, P. D. McDaniel, 和 I. J. Goodfellow. 机器学习中的迁移性：从现象到使用对抗样本的黑盒攻击。CoRR,
    abs/1605.07277，2016年。'
- en: '[174] N. Papernot, P. D. McDaniel, I. J. Goodfellow, S. Jha, Z. B. Celik, and
    A. Swami. Practical black-box attacks against machine learning. In Proceedings
    of the 2017 ACM on Asia Conference on Computer and Communications Security, AsiaCCS,
    Abu Dhabi, United Arab Emirates, April 2-6, 2017, pages 506–519.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] N. Papernot, P. D. McDaniel, I. J. Goodfellow, S. Jha, Z. B. Celik, 和
    A. Swami. 对机器学习的实用黑盒攻击。在2017年 ACM 亚洲计算机与通信安全会议 AsiaCCS，阿布扎比，阿拉伯联合酋长国，2017年4月2-6日，页码
    506–519。'
- en: '[175] N. Papernot, P. D. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
    A. Swami. The limitations of deep learning in adversarial settings. In IEEE European
    Symposium on Security and Privacy, EuroS&P 2016, Saarbrücken, Germany, March 21-24,
    2016, pages 372–387, 2016.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] N. Papernot, P. D. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, 和 A.
    Swami. 深度学习在对抗设置中的局限性。发表于 IEEE 欧洲安全与隐私研讨会，EuroS&P 2016，德国萨尔布吕肯，2016年3月21-24日，页码
    372–387，2016年。'
- en: '[176] N. Papernot, P. D. McDaniel, A. Sinha, and M. P. Wellman. Sok: Security
    and privacy in machine learning. In 2018 IEEE European Symposium on Security and
    Privacy, EuroS&P 2018, London, United Kingdom, April 24-26, 2018, pages 399–414,
    2018.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] N. Papernot, P. D. McDaniel, A. Sinha, 和 M. P. Wellman. Sok: 机器学习中的安全与隐私。见于2018年IEEE欧洲安全与隐私研讨会，EuroS&P
    2018，伦敦，英国，2018年4月24-26日，第399–414页，2018年。'
- en: '[177] N. Papernot, P. D. McDaniel, A. Swami, and R. E. Harang. Crafting adversarial
    input sequences for recurrent neural networks. In 2016 IEEE Military Communications
    Conference, MILCOM 2016, Baltimore, MD, USA, November 1-3, 2016, pages 49–54,
    2016.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] N. Papernot, P. D. McDaniel, A. Swami, 和 R. E. Harang. 为递归神经网络构建对抗输入序列。见于2016年IEEE军事通信会议，MILCOM
    2016，马里兰州巴尔的摩，美国，2016年11月1-3日，第49–54页，2016年。'
- en: '[178] N. Papernot, P. D. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation
    as a defense to adversarial perturbations against deep neural networks. In IEEE
    Symposium on Security and Privacy, SP 2016, San Jose, CA, USA, May 22-26, 2016,
    pages 582–597, 2016.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] N. Papernot, P. D. McDaniel, X. Wu, S. Jha, 和 A. Swami. 作为对抗扰动的防御的蒸馏。见于IEEE安全与隐私研讨会，SP
    2016，加利福尼亚州圣荷西，美国，2016年5月22-26日，第582–597页，2016年。'
- en: '[179] R. Pascanu, J. W. Stokes, H. Sanossian, M. Marinescu, and A. Thomas.
    Malware classification with recurrent networks. In 2015 IEEE International Conference
    on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland,
    Australia, April 19-24, 2015, pages 1916–1920, 2015.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] R. Pascanu, J. W. Stokes, H. Sanossian, M. Marinescu, 和 A. Thomas. 使用递归网络进行恶意软件分类。见于2015年IEEE国际声学、语音与信号处理会议，ICASSP
    2015，南布里斯班，昆士兰州，澳大利亚，2015年4月19-24日，第1916–1920页，2015年。'
- en: '[180] N. Phan, M. N. Vu, Y. Liu, R. Jin, D. Dou, X. Wu, and M. T. Thai. Heterogeneous
    gaussian mechanism: Preserving differential privacy in deep learning with provable
    robustness. In Proceedings of the Twenty-Eighth International Joint Conference
    on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages
    4753–4759.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] N. Phan, M. N. Vu, Y. Liu, R. Jin, D. Dou, X. Wu, 和 M. T. Thai. 异质高斯机制：在深度学习中保持差分隐私并具有可证明的鲁棒性。见于第二十八届国际联合人工智能会议论文集，IJCAI
    2019，澳门，中国，2019年8月10-16日，第4753–4759页。'
- en: '[181] L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai. Privacy-preserving
    deep learning via additively homomorphic encryption. IEEE Trans. Information Forensics
    and Security, 13(5):1333–1345, 2018.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] L. T. Phong, Y. Aono, T. Hayashi, L. Wang, 和 S. Moriai. 通过加性同态加密实现隐私保护的深度学习。IEEE
    信息取证与安全汇刊，13(5):1333–1345，2018年。'
- en: '[182] L. T. Phong and T. T. Phuong. Privacy-preserving deep learning for any
    activation function. CoRR, abs/1809.03272, 2018.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] L. T. Phong 和 T. T. Phuong. 适用于任何激活函数的隐私保护深度学习。CoRR, abs/1809.03272，2018年。'
- en: '[183] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu,
    S.-C. Chen, and S. S. Iyengar. A survey on deep learning: Algorithms, techniques,
    and applications. ACM Computing Surveys, 51(5):92:1–92:36, Sept. 2018.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu,
    S.-C. Chen, 和 S. S. Iyengar. 关于深度学习的调查：算法、技术和应用。ACM Computing Surveys, 51(5):92:1–92:36,
    2018年9月。'
- en: '[184] A. Pyrgelis, C. Troncoso, and E. D. Cristofaro. Knock knock, who’s there?
    membership inference on aggregate location data. 2017.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] A. Pyrgelis, C. Troncoso, 和 E. D. Cristofaro. 敲门声，谁在那儿？对汇总位置数据的成员推断。2017年。'
- en: '[185] X. Qiao, Y. Yang, and H. Li. Defending neural backdoors via generative
    distribution modeling. In Advances in Neural Information Processing Systems 32:
    Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
    8-14 December 2019, Vancouver, BC, Canada, pages 14004–14013.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] X. Qiao, Y. Yang, 和 H. Li. 通过生成分布建模防御神经后门。见于神经信息处理系统进展32：2019年神经信息处理系统年会，NeurIPS
    2019，2019年12月8-14日，加拿大温哥华，第14004–14013页。'
- en: '[186] Y. Qin, N. Carlini, G. W. Cottrell, I. J. Goodfellow, and C. Raffel.
    Imperceptible, robust, and targeted adversarial examples for automatic speech
    recognition. In Proceedings of the 36th International Conference on Machine Learning,
    ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 5231–5240.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Y. Qin, N. Carlini, G. W. Cottrell, I. J. Goodfellow, 和 C. Raffel. 对自动语音识别的不可察觉、鲁棒且有针对性的对抗样本。见于第36届国际机器学习会议论文集，ICML
    2019，2019年6月9-15日，加利福尼亚州长滩，美国，第5231–5240页。'
- en: '[187] A. Ranjan, J. Janai, A. Geiger, and M. J. Black. Attacking optical flow.
    In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,
    Korea (South), October 27 - November 2, 2019, pages 2404–2413.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] A. Ranjan, J. Janai, A. Geiger, 和 M. J. Black. 攻击光流。见于2019年IEEE/CVF国际计算机视觉会议，ICCV
    2019，韩国首尔，2019年10月27 - 11月2日，第2404–2413页。'
- en: '[188] D. Reich, A. Todoki, R. Dowsley, M. D. Cock, and A. C. A. Nascimento.
    Privacy-preserving classification of personal text messages with secure multi-party
    computation. In Advances in Neural Information Processing Systems 32: Annual Conference
    on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
    Vancouver, BC, Canada, pages 3752–3764.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] D. Reich, A. Todoki, R. Dowsley, M. D. Cock, 和 A. C. A. Nascimento. 使用安全多方计算进行个人文本消息的隐私保护分类。发表于神经信息处理系统会议32（NeurIPS
    2019），2019年12月8日至14日，加拿大温哥华，页面3752–3764。'
- en: '[189] S. Ren, Y. Deng, K. He, and W. Che. Generating natural language adversarial
    examples through probability weighted word saliency. In Proceedings of the 57th
    Conference of the Association for Computational Linguistics, ACL 2019, Florence,
    Italy, July 28- August 2, 2019, pages 1085–1097.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] S. Ren, Y. Deng, K. He, 和 W. Che. 通过概率加权词语显著性生成自然语言对抗样本。发表于第57届计算语言学协会会议（ACL
    2019），2019年7月28日至8月2日，意大利佛罗伦萨，页面1085–1097。'
- en: '[190] M. Rigaki and S. Garcia. Bringing a GAN to a knife-fight: Adapting malware
    communication to avoid detection. In 2018 IEEE Security and Privacy Workshops,
    SP Workshops 2018, San Francisco, CA, USA, May 24, 2018, pages 70–75, 2018.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] M. Rigaki 和 S. Garcia. 将GAN带入刀剑战斗：调整恶意软件通信以避免检测。发表于2018年IEEE安全与隐私研讨会（SP
    Workshops 2018），2018年5月24日，加利福尼亚州旧金山，页面70–75，2018年。'
- en: '[191] R. L. Rivest, L. Adleman, M. L. Dertouzos, et al. On data banks and privacy
    homomorphisms. Foundations of secure computation, 4(11):169–180, 1978.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] R. L. Rivest, L. Adleman, M. L. Dertouzos, 等. 关于数据银行和隐私同态。安全计算基础，4(11):169–180,
    1978年。'
- en: '[192] R. Romagnoli, S. Weerakkody, and B. Sinopoli. A model inversion based
    watermark for replay attack detection with output tracking. In 2019 American Control
    Conference, ACC 2019, Philadelphia, PA, USA, July 10-12, 2019, pages 384–390.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] R. Romagnoli, S. Weerakkody, 和 B. Sinopoli. 基于模型反演的水印用于回放攻击检测及输出追踪。发表于2019年美国控制会议（ACC
    2019），2019年7月10日至12日，宾夕法尼亚州费城，页面384–390。'
- en: '[193] I. Rosenberg, A. Shabtai, Y. Elovici, and L. Rokach. Low resource black-box
    end-to-end attack against state of the art API call based malware classifiers.
    CoRR, abs/1804.08778, 2018.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] I. Rosenberg, A. Shabtai, Y. Elovici, 和 L. Rokach. 针对先进的基于API调用的恶意软件分类器的低资源黑盒端到端攻击。CoRR,
    abs/1804.08778, 2018年。'
- en: '[194] I. Rosenberg, A. Shabtai, L. Rokach, and Y. Elovici. Generic black-box
    end-to-end attack against state of the art API call based malware classifiers.
    In 21st International Symposium, Research in Attacks, Intrusions, and Defenses,
    RAID 2018, Heraklion, Crete, Greece, September 10-12, 2018, Proceedings, pages
    490–510.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] I. Rosenberg, A. Shabtai, L. Rokach, 和 Y. Elovici. 通用黑盒端到端攻击针对先进的基于API调用的恶意软件分类器。发表于第21届国际研讨会，攻击、入侵和防御研究（RAID
    2018），2018年9月10日至12日，希腊克里特岛伊拉克利翁，会议记录，页面490–510。'
- en: '[195] A. S. Ross and F. Doshi-Velez. Improving the adversarial robustness and
    interpretability of deep neural networks by regularizing their input gradients.
    In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence
    (AAAI), pages 1660–1669, 2018.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] A. S. Ross 和 F. Doshi-Velez. 通过对输入梯度进行正则化来提高深度神经网络的对抗鲁棒性和可解释性。发表于第三十二届AAAI人工智能会议（AAAI），页面1660–1669，2018年。'
- en: '[196] A. Sablayrolles, M. Douze, C. Schmid, Y. Ollivier, and H. Jégou. White-box
    vs black-box: Bayes optimal strategies for membership inference. In Proceedings
    of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
    2019, Long Beach, California, USA, pages 5558–5567.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] A. Sablayrolles, M. Douze, C. Schmid, Y. Ollivier, 和 H. Jégou. 白盒与黑盒：用于成员推断的贝叶斯最优策略。发表于第36届国际机器学习大会（ICML
    2019），2019年6月9日至15日，加利福尼亚州长滩，页面5558–5567。'
- en: '[197] A. Salem, A. Bhattacharya, M. Backes, M. Fritz, and Y. Zhang. Updates-leak:
    Data set inference and reconstruction attacks in online learning. In 29th USENIX
    Security Symposium (USENIX Security 20), pages 1291–1308\. USENIX Association,
    Aug. 2020.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] A. Salem, A. Bhattacharya, M. Backes, M. Fritz, 和 Y. Zhang. Updates-leak:
    在线学习中的数据集推断和重建攻击。发表于第29届USENIX安全研讨会（USENIX Security 20），页面1291–1308。USENIX协会，2020年8月。'
- en: '[198] A. Salem, Y. Zhang, M. Humbert, M. Fritz, and M. Backes. Ml-leaks: Model
    and data independent membership inference attacks and defenses on machine learning
    models. CoRR, abs/1806.01246, 2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] A. Salem, Y. Zhang, M. Humbert, M. Fritz, 和 M. Backes. Ml-leaks: 独立于模型和数据的成员推断攻击及防御。CoRR,
    abs/1806.01246, 2018年。'
- en: '[199] M. Sato, J. Suzuki, and S. Kiyono. Effective adversarial regularization
    for neural machine translation. In Proceedings of the 57th Conference of the Association
    for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,
    pages 204–210.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] M. Sato, J. Suzuki, 和 S. Kiyono. 神经机器翻译的有效对抗正则化。在第57届计算语言学协会会议论文集中，ACL
    2019，佛罗伦萨，意大利，2019年7月28日-8月2日，页面204–210。'
- en: '[200] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras,
    and T. Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural
    networks. CoRR, abs/1804.00792, 2018.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras,
    和 T. Goldstein. 毒蛙！针对神经网络的目标清洁标签中毒攻击。CoRR, abs/1804.00792, 2018。'
- en: '[201] M. Sharif, L. Bauer, and M. K. Reiter. On the suitability of lp-norms
    for creating and preventing adversarial examples. In 2018 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR) Workshops, Salt Lake City, UT, USA, June
    18-22, 2018, pages 1605–1613.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] M. Sharif, L. Bauer, 和 M. K. Reiter. lp-范数在创建和防止对抗样本中的适用性。在2018年IEEE计算机视觉与模式识别会议（CVPR）研讨会，盐湖城，犹他州，美国，2018年6月18-22日，页面1605–1613。'
- en: '[202] R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In Proceedings
    of the 22nd ACM SIGSAC Conference on Computer and Communications Security, Denver,
    CO, USA, October 12-16, 2015, pages 1310–1321, 2015.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] R. Shokri 和 V. Shmatikov. 隐私保护深度学习。在第22届ACM SIGSAC计算机与通信安全会议论文集中，丹佛，科罗拉多州，美国，2015年10月12-16日，页面1310–1321，2015。'
- en: '[203] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference
    attacks against machine learning models. In 2017 IEEE Symposium on Security and
    Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pages 3–18, 2017.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] R. Shokri, M. Stronati, C. Song, 和 V. Shmatikov. 对机器学习模型的成员身份推断攻击。在2017年IEEE安全与隐私研讨会，SP
    2017，圣荷西，加利福尼亚州，美国，2017年5月22-26日，页面3–18，2017。'
- en: '[204] C. Simon-Gabriel, Y. Ollivier, L. Bottou, B. Schölkopf, and D. Lopez-Paz.
    First-order adversarial vulnerability of neural networks and input dimension.
    In Proceedings of the 36th International Conference on Machine Learning, ICML
    2019, 9-15 June 2019, Long Beach, California, USA, pages 5809–5817.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] C. Simon-Gabriel, Y. Ollivier, L. Bottou, B. Schölkopf, 和 D. Lopez-Paz.
    神经网络的一级对抗脆弱性和输入维度。在第36届国际机器学习会议论文集中，ICML 2019，2019年6月9-15日，长滩，加利福尼亚州，美国，页面5809–5817。'
- en: '[205] C. Song, T. Ristenpart, and V. Shmatikov. Machine learning models that
    remember too much. In Proceedings of ACM SIGSAC Conference on Computer and Communications
    Security, CCS 2017, Dallas, TX, USA, October 30 - November 03, 2017, pages 587–601.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] C. Song, T. Ristenpart, 和 V. Shmatikov. 记忆过多的机器学习模型。在ACM SIGSAC计算机与通信安全会议论文集中，CCS
    2017，达拉斯，德克萨斯州，美国，2017年10月30日 - 11月3日，页面587–601。'
- en: '[206] L. Song, R. Shokri, and P. Mittal. Privacy risks of securing machine
    learning models against adversarial examples. In Proceedings of the 2019 ACM SIGSAC
    Conference on Computer and Communications Security, CCS 2019, London, UK, November
    11-15, 2019, pages 241–257.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] L. Song, R. Shokri, 和 P. Mittal. 保护机器学习模型免受对抗样本攻击的隐私风险。在2019年ACM SIGSAC计算机与通信安全会议论文集中，CCS
    2019，伦敦，英国，2019年11月11-15日，页面241–257。'
- en: '[207] S. Song, K. Chaudhuri, and A. D. Sarwate. Stochastic gradient descent
    with differentially private updates. In IEEE Global Conference on Signal and Information
    Processing, GlobalSIP 2013, Austin, TX, USA, December 3-5, 2013, pages 245–248,
    2013.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] S. Song, K. Chaudhuri, 和 A. D. Sarwate. 带有差分隐私更新的随机梯度下降。在IEEE全球信号与信息处理会议，GlobalSIP
    2013，奥斯汀，德克萨斯州，美国，2013年12月3-5日，页面245–248，2013。'
- en: '[208] Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman. Pixeldefend: Leveraging
    generative models to understand and defend against adversarial examples. CoRR,
    abs/1710.10766, 2017.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Y. Song, T. Kim, S. Nowozin, S. Ermon, 和 N. Kushman. Pixeldefend: 利用生成模型理解和防御对抗样本。CoRR,
    abs/1710.10766, 2017。'
- en: '[209] A. Stasinopoulos, C. Ntantogian, and C. Xenakis. Commix: automating evaluation
    and exploitation of command injection vulnerabilities in web applications. Int.
    J. Inf. Sec., 18(1):49–72, 2019.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] A. Stasinopoulos, C. Ntantogian, 和 C. Xenakis. Commix: 自动化评估和利用Web应用中的命令注入漏洞。国际信息安全杂志，18(1):49–72，2019。'
- en: '[210] J. Steinhardt, P. W. Koh, and P. S. Liang. Certified defenses for data
    poisoning attacks. In Annual Conference on Neural Information Processing Systems,
    4-9 December 2017, Long Beach, CA, USA, pages 3520–3532, 2017.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] J. Steinhardt, P. W. Koh, 和 P. S. Liang. 数据中毒攻击的认证防御。在神经信息处理系统年会，2017年12月4-9日，长滩，加利福尼亚州，美国，页面3520–3532，2017。'
- en: '[211] Y. Sun, M. Wu, W. Ruan, X. Huang, M. Kwiatkowska, and D. Kroening. Concolic
    testing for deep neural networks. In Proceedings of the 33rd ACM/IEEE International
    Conference on Automated Software Engineering, ASE 2018, Montpellier, France, September
    3-7, 2018, pages 109–119.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Y. Sun, M. Wu, W. Ruan, X. Huang, M. Kwiatkowska, 和 D. Kroening. 深度神经网络的合符测试。在第33届ACM/IEEE国际自动化软件工程会议论文集中，ASE
    2018，蒙彼利埃，法国，2018年9月3-7日，第109–119页。'
- en: '[212] Z. Sun, P. Kairouz, A. T. Suresh, and H. B. McMahan. Can you really backdoor
    federated learning? CoRR, abs/1911.07963, 2019.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] Z. Sun, P. Kairouz, A. T. Suresh, 和 H. B. McMahan. 你真的可以后门攻击联邦学习吗？CoRR,
    abs/1911.07963, 2019。'
- en: '[213] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow,
    and R. Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199,
    2013.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow,
    和 R. Fergus. 神经网络的有趣特性。CoRR, abs/1312.6199, 2013。'
- en: '[214] K. Talwar, A. Thakurta, and L. Zhang. Private empirical risk minimization
    beyond the worst case: The effect of the constraint set geometry. CoRR, abs/1411.5417,
    2014.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] K. Talwar, A. Thakurta, 和 L. Zhang. 超越最坏情况的私有经验风险最小化：约束集几何的影响。CoRR, abs/1411.5417,
    2014。'
- en: '[215] T. Tassa, T. Grinshpoun, and A. Yanai. A privacy preserving collusion
    secure DCOP algorithm. In Proceedings of the Twenty-Eighth International Joint
    Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16,
    2019, pages 4774–4780.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] T. Tassa, T. Grinshpoun, 和 A. Yanai. 一种隐私保护的碰撞安全DCOP算法。在第二十八届国际人工智能联合会议论文集中，IJCAI
    2019，澳门，中国，2019年8月10-16日，第4774–4780页。'
- en: '[216] S. Tian, G. Yang, and Y. Cai. Detecting adversarial examples through
    image transformation. In Proceedings of the Thirty-Second AAAI Conference on Artificial
    Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018, pages 4139–4146,
    2018.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] S. Tian, G. Yang, 和 Y. Cai. 通过图像变换检测对抗样本。在第三十二届 AAAI 人工智能会议论文集中，新奥尔良，路易斯安那州，美国，2018年2月2-7日，第4139–4146页，2018年。'
- en: '[217] Y. Tian, K. Pei, S. Jana, and B. Ray. Deeptest: automated testing of
    deep-neural-network-driven autonomous cars. In Proceedings of the 40th International
    Conference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June
    03, 2018, pages 303–314.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Y. Tian, K. Pei, S. Jana, 和 B. Ray. Deeptest：自动化测试深度神经网络驱动的自动驾驶汽车。在第40届国际软件工程会议论文集中，ICSE
    2018，哥德堡，瑞典，2018年5月27日至6月3日，第303–314页。'
- en: '[218] F. Tramèr, A. Kurakin, N. Papernot, D. Boneh, and P. D. McDaniel. Ensemble
    adversarial training: Attacks and defenses. CoRR, abs/1705.07204, 2017.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] F. Tramèr, A. Kurakin, N. Papernot, D. Boneh, 和 P. D. McDaniel. 集成对抗训练：攻击与防御。CoRR,
    abs/1705.07204, 2017。'
- en: '[219] F. Tramèr, N. Papernot, I. J. Goodfellow, D. Boneh, and P. D. McDaniel.
    The space of transferable adversarial examples. CoRR, abs/1704.03453, 2017.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] F. Tramèr, N. Papernot, I. J. Goodfellow, D. Boneh, 和 P. D. McDaniel.
    可迁移对抗样本的空间。CoRR, abs/1704.03453, 2017。'
- en: '[220] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. Stealing
    machine learning models via prediction apis. In 25th USENIX Security Symposium,
    USENIX Security 16, Austin, TX, USA, August 10-12, 2016., pages 601–618, 2016.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, 和 T. Ristenpart. 通过预测API窃取机器学习模型。在第25届
    USENIX 安全研讨会，USENIX Security 16，奥斯汀，德克萨斯州，美国，2016年8月10-12日，第601–618页，2016年。'
- en: '[221] S. Truex, L. Liu, M. E. Gursoy, L. Yu, and W. Wei. Towards demystifying
    membership inference attacks. CoRR, abs/1807.09173, 2018.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] S. Truex, L. Liu, M. E. Gursoy, L. Yu, 和 W. Wei. 旨在揭示成员推断攻击。CoRR, abs/1807.09173,
    2018。'
- en: '[222] M. Veale, R. Binns, and L. Edwards. Algorithms that remember: Model inversion
    attacks and data protection law. CoRR, abs/1807.04644, 2018.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] M. Veale, R. Binns, 和 L. Edwards. 记忆算法：模型反演攻击与数据保护法。CoRR, abs/1807.04644,
    2018。'
- en: '[223] J. S. Vitter. Random sampling with a reservoir. ACM Trans. Math. Softw.,
    11(1):37–57, 1985.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] J. S. Vitter. 带储备的随机抽样。ACM Trans. Math. Softw., 11(1):37–57, 1985。'
- en: '[224] S. Wagh, D. Gupta, and N. Chandran. Securenn: Efficient and private neural
    network training. IACR Cryptology ePrint Archive, 2018:442, 2018.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] S. Wagh, D. Gupta, 和 N. Chandran. Securenn：高效且私密的神经网络训练。IACR Cryptology
    ePrint Archive, 2018:442, 2018。'
- en: '[225] B. Wang and N. Z. Gong. Attacking graph-based classification via manipulating
    the graph structure. In Proceedings of the 2019 ACM SIGSAC Conference on Computer
    and Communications Security, CCS 2019, London, UK, November 11-15, 2019, pages
    2023–2040.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] B. Wang 和 N. Z. Gong. 通过操控图结构攻击基于图的分类。在2019年ACM SIGSAC计算机与通信安全会议论文集中，CCS
    2019，伦敦，英国，2019年11月11-15日，第2023–2040页。'
- en: '[226] B. Wang and N. Z. Gong. Stealing hyperparameters in machine learning.
    In IEEE Symposium on Security and Privacy (SP), San Francisco, California, USA,
    21-23 May 2018, pages 36–52, 2018.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] B. Wang 和 N. Z. Gong. 在机器学习中窃取超参数。在IEEE安全与隐私研讨会（SP），2018年5月21-23日，美国加利福尼亚州旧金山，页面36–52，2018年。'
- en: '[227] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao.
    Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.
    In 2019 IEEE Symposium on Security and Privacy, SP 2019, San Francisco, CA, USA,
    May 19-23, 2019, pages 707–723.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng 和 B. Y. Zhao.
    神经清洗：识别和缓解神经网络中的后门攻击。在2019年IEEE安全与隐私研讨会（SP 2019），2019年5月19-23日，美国加利福尼亚州旧金山，页面707–723。'
- en: '[228] D. Wang, C. Gong, and Q. Liu. Improving neural language modeling via
    adversarial training. In Proceedings of the 36th International Conference on Machine
    Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 6555–6565.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] D. Wang, C. Gong 和 Q. Liu. 通过对抗训练改进神经语言建模。在第36届国际机器学习会议（ICML 2019），2019年6月9-15日，美国加利福尼亚州长滩，页面6555–6565。'
- en: '[229] D. Wang, M. Ye, and J. Xu. Differentially private empirical risk minimization
    revisited: Faster and more general. In Annual Conference on Neural Information
    Processing Systems, Long Beach, CA, USA, 4-9 December 2017, pages 2719–2728, 2017.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] D. Wang, M. Ye 和 J. Xu. 差分隐私经验风险最小化再探：更快且更通用。在年度神经信息处理系统会议（NeurIPS），2017年12月4-9日，美国加利福尼亚州长滩，页面2719–2728，2017年。'
- en: '[230] J. Wang, G. Dong, J. Sun, X. Wang, and P. Zhang. Adversarial sample detection
    for deep neural network through model mutation testing. In Proceedings of the
    41st International Conference on Software Engineering, ICSE 2019, Montreal, QC,
    Canada, May 25-31, 2019, pages 1245–1256.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] J. Wang, G. Dong, J. Sun, X. Wang 和 P. Zhang. 通过模型突变测试检测深度神经网络的对抗样本。在第41届国际软件工程会议（ICSE
    2019），2019年5月25-31日，加拿大蒙特利尔，页面1245–1256。'
- en: '[231] J. Wang, J. Sun, P. Zhang, and X. Wang. Detecting adversarial samples
    for deep neural networks through mutation testing. CoRR, abs/1805.05010, 2018.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] J. Wang, J. Sun, P. Zhang 和 X. Wang. 通过突变测试检测深度神经网络的对抗样本。CoRR，abs/1805.05010，2018年。'
- en: '[232] J. Wang and H. Zhang. Bilateral adversarial training: Towards fast training
    of more robust models against adversarial attacks. In 2019 IEEE/CVF International
    Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November
    2, 2019, pages 6628–6637.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] J. Wang 和 H. Zhang. 双边对抗训练：向更快速的鲁棒模型训练迈进，以应对对抗攻击。在2019年IEEE/CVF国际计算机视觉大会（ICCV
    2019），2019年10月27日 - 11月2日，韩国首尔，页面6628–6637。'
- en: '[233] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Formal security
    analysis of neural networks using symbolic intervals. In 27th USENIX Security
    Symposium, USENIX Security 2018, Baltimore, MD, USA, August 15-17, 2018., pages
    1599–1614, 2018.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] S. Wang, K. Pei, J. Whitehouse, J. Yang 和 S. Jana. 使用符号区间的神经网络形式安全分析。在第27届USENIX安全研讨会（USENIX
    Security 2018），2018年8月15-17日，美国马里兰州巴尔的摩，页面1599–1614，2018年。'
- en: '[234] X. O. Wang, K. Zeng, K. Govindan, and P. Mohapatra. Chaining for securing
    data provenance in distributed information networks. In 31st IEEE Military Communications
    Conference, MILCOM, Orlando, FL, USA, October 29 - November 1, 2012, pages 1–6.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] X. O. Wang, K. Zeng, K. Govindan 和 P. Mohapatra. 链接技术用于在分布式信息网络中保护数据来源。在第31届IEEE军事通信会议（MILCOM），2012年10月29日
    - 11月1日，美国佛罗里达州奥兰多，页面1–6。'
- en: '[235] Y. Wang and K. Chaudhuri. Data poisoning attacks against online learning.
    CoRR, abs/1808.08994, 2018.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] Y. Wang 和 K. Chaudhuri. 针对在线学习的数据中毒攻击。CoRR，abs/1808.08994，2018年。'
- en: '[236] X. Wei, S. Liang, N. Chen, and X. Cao. Transferable adversarial attacks
    for image and video object detection. In Proceedings of the Twenty-Eighth International
    Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August
    10-16, 2019, pages 954–960.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] X. Wei, S. Liang, N. Chen 和 X. Cao. 用于图像和视频物体检测的可转移对抗攻击。在第28届国际人工智能联合会议（IJCAI
    2019），2019年8月10-16日，中国澳门，页面954–960。'
- en: '[237] L. Weng, P. Chen, L. M. Nguyen, M. S. Squillante, A. Boopathy, I. V.
    Oseledets, and L. Daniel. PROVEN: verifying robustness of neural networks with
    a probabilistic approach. In Proceedings of the 36th International Conference
    on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages
    6727–6736.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] L. Weng, P. Chen, L. M. Nguyen, M. S. Squillante, A. Boopathy, I. V.
    Oseledets 和 L. Daniel. PROVEN：通过概率方法验证神经网络的鲁棒性。在第36届国际机器学习会议（ICML 2019），2019年6月9-15日，美国加利福尼亚州长滩，页面6727–6736。'
- en: '[238] E. Wong, L. Rice, and J. Z. Kolter. Fast is better than free: Revisiting
    adversarial training. In International Conference on Learning Representations,
    2020.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] E. 黄, L. Rice, and J. Z. Kolter. 快速比免费好: 重新审视敌对训练. In 国际学习代表大会, 2020.'
- en: '[239] H. Wu, C. Wang, Y. Tyshetskiy, A. Docherty, K. Lu, and L. Zhu. Adversarial
    examples for graph data: Deep insights into attack and defense. In Proceedings
    of the Twenty-Eighth International Joint Conference on Artificial Intelligence,
    IJCAI 2019, Macao, China, August 10-16, 2019, pages 4816–4823.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] H. 吴, C. 王, Y. Tyshetskiy, A. Docherty, K. 卢, and L. 朱. 针对图数据的敌对样本: 攻击与防御的深入见解。In
    第二十八届国际人工智能联合会议, IJCAI 2019, 中国澳门, 2019年8月10-16, 页数4816–4823。'
- en: '[240] Z. Xi, R. Pang, S. Ji, and T. Wang. Graph backdoor. CoRR, abs/2006.11890,
    2020.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Z. 西, R. 庞, S. 季, and T. 王. 图形后门. CoRR, abs/2006.11890, 2020.'
- en: '[241] Q. Xia, Z. Tao, Z. Hao, and Q. Li. FABA: an algorithm for fast aggregation
    against byzantine attacks in distributed neural networks. In Proceedings of the
    Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI
    2019, Macao, China, August 10-16, 2019, pages 4824–4830.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] Q. 夏, Z. 陶, Z. 郝, and Q. 李. FABA: 分布式神经网络中快速聚合对拜占庭攻击的算法。In 第二十八届国际人工智能联合会议,
    IJCAI 2019, 中国澳门, 2019年8月10-16, 页数4824–4830.'
- en: '[242] H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli. Is feature
    selection secure against training data poisoning? In Proceedings of the 32nd International
    Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages
    1689–1698, 2015.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] H. 肖, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli. 特征选择对训练数据污染是否安全?
    In 机器学习的第三十二届国际会议, ICML 2015, 法国 里尔, 2015年7月6-11, 页数1689–1698, 2015.'
- en: '[243] H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli. Support
    vector machines under adversarial label contamination. Neurocomputing, 160:53–62,
    2015.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] H. 肖, B. Biggio, B. Nelson, H. 肖, C. Eckert, and F. Roli. 支持向量机在敌对标签污染下的表现.
    Neurocomputing, 160:53–62, 2015.'
- en: '[244] H. Xiao, H. Xiao, and C. Eckert. Adversarial label flips attack on support
    vector machines. In 20th European Conference on Artificial Intelligence (ECAI),
    Montpellier, France, August 27-31, 2012, pages 870–875, 2012.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] H. 肖, H. 肖, and C. Eckert. 支持向量机上的敌对标签翻转攻击。在第二十届人工智能欧洲大会(ECAI), 法国 蒙彼利埃,
    2012年8月27-31, 页数870–875, 2012.'
- en: '[245] Q. Xiao, K. Li, D. Zhang, and W. Xu. Security risks in deep learning
    implementations. In 2018 IEEE Security and Privacy (SP) Workshops, San Francisco,
    CA, USA, May 24, 2018, pages 123–128.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] Q. 肖, K. 李, D. 张, and W. 徐. 深度学习实现中的安全风险。In 2018年IEEE安全与隐私(SP)研讨会, 美国
    加利福尼亚州 旧金山, 2018年5月24, 页数123–128.'
- en: '[246] C. Xie, K. Huang, P. Chen, and B. Li. DBA: distributed backdoor attacks
    against federated learning. In 8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] C. 谢, K. 黄, P. 陈, and B. 李. DBA: 分布式背门攻击对联邦学习的影响. In 第八届国际学习代表大会, ICLR
    2020, 埃塞俄比亚, 阿迪斯阿贝巴, 2020年4月26-30.'
- en: '[247] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. L. Yuille. Mitigating adversarial
    effects through randomization. CoRR, abs/1711.01991, 2017.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] C. 谢, J. 王, Z. 张, Z. 任, and A. L. Yuille. 通过随机化减轻敌对效应。CoRR, abs/1711.01991,
    2017.'
- en: '[248] C. Xie, Z. Zhang, Y. Zhou, S. Bai, J. Wang, Z. Ren, and A. L. Yuille.
    Improving transferability of adversarial examples with input diversity. In IEEE
    Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach,
    CA, USA, June 16-20, 2019, pages 2730–2739.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] C. 谢, Z. 张, Y. 周，S. 白, J. 王, Z. 任, and A. L. Yuille. 通过输入多样性改善敌对样本的可转移性。In
    IEEE 电脑视觉与模式识别会议, CVPR 2019, 美国 加利福尼亚州 长滩, 2019年6月16-20, 页数2730–2739。'
- en: '[249] P. Xie, B. Wu, and G. Sun. BAYHENN: combining bayesian deep learning
    and homomorphic encryption for secure DNN inference. In Proceedings of the Twenty-Eighth
    International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,
    China, August 10-16, 2019, pages 4831–4837.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] P. 谢, B. 吴, and G. 孙. BAYHENN: 结合贝叶斯深度学习和同态加密进行安全DNN推断. In 第二十八届国际人工智能联合会议论文集,
    IJCAI 2019, 中国澳门, 2019年8月10-16, 页数4831–4837.'
- en: '[250] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y. Liu, J. Zhao, B. Li,
    J. Yin, and S. See. Deephunter: a coverage-guided fuzz testing framework for deep
    neural networks. In Proceedings of the 28th ACM SIGSOFT International Symposium
    on Software Testing and Analysis, ISSTA 2019, Beijing, China, July 15-19, 2019.,
    pages 146–157.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] X. 谢, L. 马, F. Juefei-Xu, M. 薛, H. 陈, Y. 刘, J. 赵, B. 李, J. 尹, and S. See.
    Deephunter: 一个用于深度神经网络的覆盖式模糊测试框架. In 第二十八届ACM SIGSOFT国际软件测试与分析研讨会, ISSTA 2019,
    中国北京, 2019年7月15-19, 页数146–157.'
- en: '[251] K. Xu, H. Chen, S. Liu, P. Chen, T. Weng, M. Hong, and X. Lin. Topology
    attack and defense for graph neural networks: An optimization perspective. In
    Proceedings of the Twenty-Eighth International Joint Conference on Artificial
    Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 3961–3967.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] K. Xu, H. Chen, S. Liu, P. Chen, T. Weng, M. Hong 和 X. Lin. 图神经网络的拓扑攻击与防御：一种优化视角。发表于第28届国际人工智能联合会议，IJCAI
    2019，中国澳门，2019年8月10-16日，页码3961–3967。'
- en: '[252] L. Xu, W. Jia, W. Dong, and Y. Li. Automatic exploit generation for buffer
    overflow vulnerabilities. In 2018 IEEE International Conference on Software Quality,
    Reliability and Security (QRS) Companion, Lisbon, Portugal, July 16-20, 2018,
    pages 463–468.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] L. Xu, W. Jia, W. Dong 和 Y. Li. 自动利用缓冲区溢出漏洞生成攻击代码。发表于2018年IEEE国际软件质量、可靠性与安全（QRS）大会，葡萄牙里斯本，2018年7月16-20日，页码463–468。'
- en: '[253] W. Xu, D. Evans, and Y. Qi. Feature squeezing: Detecting adversarial
    examples in deep neural networks. In 25th Annual Network and Distributed System
    Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018,
    2018.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] W. Xu, D. Evans 和 Y. Qi. 特征挤压：在深度神经网络中检测对抗性样本。发表于第25届网络与分布式系统安全研讨会，NDSS
    2018，美国加利福尼亚州圣地亚哥，2018年2月18-21日，2018年。'
- en: '[254] H. Yakura and J. Sakuma. Robust audio adversarial example for a physical
    attack. In Proceedings of the Twenty-Eighth International Joint Conference on
    Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 5334–5341.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] H. Yakura 和 J. Sakuma. 物理攻击的鲁棒音频对抗样本。发表于第28届国际人工智能联合会议，IJCAI 2019，中国澳门，2019年8月10-16日，页码5334–5341。'
- en: '[255] Y. Yang, G. Zhang, Z. Xu, and D. Katabi. Me-net: Towards effective adversarial
    robustness with matrix estimation. In Proceedings of the 36th International Conference
    on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages
    7025–7034.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] Y. Yang, G. Zhang, Z. Xu 和 D. Katabi. Me-net：通过矩阵估计实现有效的对抗性鲁棒性。发表于第36届国际机器学习会议，ICML
    2019，美国加利福尼亚州长滩，2019年6月9-15日，页码7025–7034。'
- en: '[256] Z. Yang, J. Zhang, E. Chang, and Z. Liang. Neural network inversion in
    adversarial setting via background knowledge alignment. In Proceedings of the
    2019 ACM SIGSAC Conference on Computer and Communications Security, CCS 2019,
    London, UK, November 11-15, 2019, pages 225–240.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] Z. Yang, J. Zhang, E. Chang 和 Z. Liang. 在对抗性环境中通过背景知识对齐进行神经网络反演。发表于2019年ACM
    SIGSAC计算机与通信安全会议，CCS 2019，英国伦敦，2019年11月11-15日，页码225–240。'
- en: '[257] A. C. Yao. Protocols for secure computations (extended abstract). In
    23rd Annual Symposium on Foundations of Computer Science, Chicago, Illinois, USA,
    3-5 November 1982, pages 160–164, 1982.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] A. C. Yao. 安全计算协议（扩展摘要）。发表于第23届计算机科学基础年会，芝加哥，美国，1982年11月3-5日，页码160–164，1982年。'
- en: '[258] Y. Yao, H. Li, H. Zheng, and B. Y. Zhao. Latent backdoor attacks on deep
    neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer
    and Communications Security, CCS 2019, London, UK, November 11-15, 2019, pages
    2041–2055.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] Y. Yao, H. Li, H. Zheng 和 B. Y. Zhao. 对深度神经网络的潜在后门攻击。发表于2019年ACM SIGSAC计算机与通信安全会议，CCS
    2019，英国伦敦，2019年11月11-15日，页码2041–2055。'
- en: '[259] S. Ye, X. Lin, K. Xu, S. Liu, H. Cheng, J. Lambrechts, H. Zhang, A. Zhou,
    K. Ma, and Y. Wang. Adversarial robustness vs. model compression, or both? In
    2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea
    (South), October 27 - November 2, 2019, pages 111–120.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] S. Ye, X. Lin, K. Xu, S. Liu, H. Cheng, J. Lambrechts, H. Zhang, A. Zhou,
    K. Ma 和 Y. Wang. 对抗性鲁棒性与模型压缩，还是两者兼顾？发表于2019年IEEE/CVF国际计算机视觉大会，ICCV 2019，韩国首尔，2019年10月27日-11月2日，页码111–120。'
- en: '[260] W. You, P. Zong, K. Chen, X. Wang, X. Liao, P. Bian, and B. Liang. Semfuzz:
    Semantics-based automatic generation of proof-of-concept exploits. In Proceedings
    of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS),
    Dallas, TX, USA, October 30 - November 03, 2017, pages 2139–2154.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] W. You, P. Zong, K. Chen, X. Wang, X. Liao, P. Bian 和 B. Liang. Semfuzz：基于语义的自动化概念验证攻击生成。发表于2017年ACM
    SIGSAC计算机与通信安全会议（CCS），美国德克萨斯州达拉斯，2017年10月30日-11月3日，页码2139–2154。'
- en: '[261] L. Yu, L. Liu, C. Pu, M. E. Gursoy, and S. Truex. Differentially private
    model publishing for deep learning. In 2019 IEEE Symposium on Security and Privacy,
    SP 2019, San Francisco, CA, USA, May 19-23, 2019, pages 332–349, 2019.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] L. Yu, L. Liu, C. Pu, M. E. Gursoy 和 S. Truex. 面向深度学习的差分隐私模型发布。发表于2019年IEEE安全与隐私研讨会，SP
    2019，美国加州旧金山，2019年5月19-23日，页码332–349，2019年。'
- en: '[262] X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen, S. Zhang, H. Huang,
    X. Wang, and C. A. Gunter. Commandersong: A systematic approach for practical
    adversarial voice recognition. In 27th USENIX Security Symposium, USENIX Security
    2018, Baltimore, MD, USA, August 15-17, 2018., pages 49–64, 2018.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] X. 袁，Y. 陈，Y. 赵，Y. 龙，X. 刘，K. 陈，S. 张，H. 黄，X. 王，和 C. A. 冈特。Commandersong：一种实际对抗性语音识别的系统方法。在第27届USENIX安全研讨会（USENIX
    Security 2018），巴尔的摩，MD，美国，2018年8月15-17日，页49–64，2018。'
- en: '[263] Y. Zang, F. Qi, C. Yang, Z. Liu, M. Zhang, Q. Liu, and M. Sun. Word-level
    textual adversarial attacking as combinatorial optimization. In Proceedings of
    the 58th Annual Meeting of the Association for Computational Linguistics, ACL
    2020, Online, July 5-10, 2020, pages 6066–6080.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] Y. 曾，F. 齐，C. 杨，Z. 刘，M. 张，Q. 刘，和 M. 孙。基于组合优化的词级文本对抗攻击。在第58届计算语言学协会年会（ACL
    2020）论文集，在线，2020年7月5-10日，页6066–6080。'
- en: '[264] V. Zantedeschi, M. Nicolae, and A. Rawat. Efficient defenses against
    adversarial attacks. In Proceedings of the 10th ACM Workshop on Artificial Intelligence
    and Security, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017, pages 39–49,
    2017.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] V. 贾恩特斯基，M. 尼古拉，和 A. 拉瓦特。针对对抗攻击的高效防御。在第10届ACM人工智能与安全研讨会（AISec@CCS 2017）论文集，达拉斯，TX，美国，2017年11月3日，页39–49，2017。'
- en: '[265] X. Zeng, C. Liu, Y. Wang, W. Qiu, L. Xie, Y. Tai, C. Tang, and A. L.
    Yuille. Adversarial attacks beyond the image space. In IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019,
    pages 4302–4311.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] X. 曾，C. 刘，Y. 王，W. 丘，L. 谢，Y. 台，C. 唐，和 A. L. 尤伊。超越图像空间的对抗攻击。在IEEE计算机视觉与模式识别会议（CVPR
    2019）论文集，长滩，CA，美国，2019年6月16-20日，页4302–4311。'
- en: '[266] H. Zhang and J. Wang. Defense against adversarial attacks using feature
    scattering-based adversarial training. In Advances in Neural Information Processing
    Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS
    2019, 8-14 December 2019, Vancouver, BC, Canada, pages 1829–1839.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] H. 张和 J. 王。基于特征散布的对抗训练防御对抗攻击。在《神经信息处理系统进展 32：神经信息处理系统年会 2019（NeurIPS
    2019）》论文集，2019年12月8-14日，温哥华，BC，加拿大，页1829–1839。'
- en: '[267] H. Zhang, T. Zheng, J. Gao, C. Miao, L. Su, Y. Li, and K. Ren. Data poisoning
    attack against knowledge graph embedding. In Proceedings of the Twenty-Eighth
    International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,
    China, August 10-16, 2019, pages 4853–4859.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] H. 张，T. 郑，J. 高，C. 苗，L. 苏，Y. 李，和 K. 任。针对知识图谱嵌入的数据中毒攻击。在第二十八届国际人工智能联合会议（IJCAI
    2019）论文集，澳门，中国，2019年8月10-16日，页4853–4859。'
- en: '[268] H. Zhang, H. Zhou, N. Miao, and L. Li. Generating fluent adversarial
    examples for natural languages. In Proceedings of the 57th Conference of the Association
    for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,
    pages 5564–5569.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] H. 张，H. 周，N. 苗，和 L. 李。生成流畅的自然语言对抗示例。在第57届计算语言学协会会议（ACL 2019）论文集，佛罗伦萨，意大利，2019年7月28日-8月2日，页5564–5569。'
- en: '[269] J. Zhang, K. Zheng, W. Mou, and L. Wang. Efficient private ERM for smooth
    objectives. In Proceedings of the Twenty-Sixth International Joint Conference
    on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017,
    pages 3922–3928, 2017.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] J. 张，K. 郑，W. 谷，和 L. 王。针对平滑目标的高效私有ERM。在第二十六届国际人工智能联合会议（IJCAI 2017）论文集，墨尔本，澳大利亚，2017年8月19-25日，页3922–3928，2017。'
- en: '[270] J. M. Zhang, M. Harman, L. Ma, and Y. Liu. Machine learning testing:
    Survey, landscapes and horizons. IEEE Transactions on Software Engineering, PP(99):1–1,
    2020.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] J. M. 张，M. 哈曼，L. 马，和 Y. 刘。机器学习测试：调查、现状与前景。IEEE 软件工程学报，PP(99):1–1, 2020。'
- en: '[271] M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid. Deeproad: Gan-based
    metamorphic testing and input validation framework for autonomous driving systems.
    In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software
    Engineering, ASE 2018, Montpellier, France, September 3-7, 2018, pages 132–142.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] M. 张，Y. 张，L. 张，C. 刘，和 S. 库尔希德。Deeproad：基于GAN的变形测试和自主驾驶系统的输入验证框架。在第33届ACM/IEEE国际自动化软件工程会议（ASE
    2018）论文集，蒙彼利埃，法国，2018年9月3-7日，页132–142。'
- en: '[272] T. Zhang, Z. He, and R. B. Lee. Privacy-preserving machine learning through
    data obfuscation. CoRR, abs/1807.01860, 2018.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] T. 张，Z. 赫，和 R. B. 李。通过数据混淆进行隐私保护的机器学习。CoRR, abs/1807.01860, 2018。'
- en: '[273] T. Zhang and Q. Zhu. A dual perturbation approach for differential private
    admm-based distributed empirical risk minimization. In Proceedings of ACM Workshop
    on Artificial Intelligence and Security (AISec), Vienna, Austria, October 28,
    2016, pages 129–137.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] T. Zhang 和 Q. Zhu. 基于差分隐私的ADMM分布式经验风险最小化的双重扰动方法。载于ACM人工智能与安全研讨会（AISec）论文集，维也纳，奥地利，2016年10月28日，页面129–137。'
- en: '[274] Y. Zhao, H. Zhu, R. Liang, Q. Shen, S. Zhang, and K. Chen. Seeing isn’t
    believing: Towards more robust adversarial attack against real world object detectors.
    In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications
    Security, CCS 2019, London, UK, November 11-15, 2019, pages 1989–2004.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] Y. Zhao、H. Zhu、R. Liang、Q. Shen、S. Zhang 和 K. Chen. 眼见未必为实：针对现实世界物体检测器的更强鲁棒对抗攻击。载于2019年ACM
    SIGSAC计算机与通信安全会议论文集，CCS 2019，伦敦，英国，2019年11月11-15日，页面1989–2004。'
- en: '[275] W. Zheng, R. A. Popa, J. E. Gonzalez, and I. Stoica. Helen: Maliciously
    secure coopetitive learning for linear models. In 2019 IEEE Symposium on Security
    and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pages 724–738.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] W. Zheng、R. A. Popa、J. E. Gonzalez 和 I. Stoica. Helen：针对线性模型的恶意安全合作学习。载于2019年IEEE安全与隐私研讨会，SP
    2019，旧金山，加州，美国，2019年5月19-23日，页面724–738。'
- en: '[276] W. Zou, S. Huang, J. Xie, X. Dai, and J. Chen. A reinforced generation
    of adversarial examples for neural machine translation. In Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,
    Online, July 5-10, 2020, pages 3486–3497.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] W. Zou、S. Huang、J. Xie、X. Dai 和 J. Chen. 针对神经机器翻译的对抗样本生成增强。载于第58届计算语言学协会年会论文集，ACL
    2020，在线，2020年7月5-10日，页面3486–3497。'
- en: '[277] D. Zügner, A. Akbarnejad, and S. Günnemann. Adversarial attacks on neural
    networks for graph data. In Proceedings of the Twenty-Eighth International Joint
    Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16,
    2019, pages 6246–6250.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] D. Zügner、A. Akbarnejad 和 S. Günnemann. 图数据神经网络的对抗攻击。载于第二十八届国际人工智能联合会议论文集，IJCAI
    2019，澳门，中国，2019年8月10-16日，页面6246–6250。'
