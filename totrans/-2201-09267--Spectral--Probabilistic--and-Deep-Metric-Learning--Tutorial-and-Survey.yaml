- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:48:17'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2201.09267] Spectral, Probabilistic, and Deep Metric Learning: Tutorial and
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2201.09267](https://ar5iv.labs.arxiv.org/html/2201.09267)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Benyamin Ghojogh Department of Electrical and Computer Engineering,
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada   
    Ali Ghodsi Department of Statistics and Actuarial Science & David R. Cheriton
    School of Computer Science,
  prefs: []
  type: TYPE_NORMAL
- en: Data Analytics Laboratory, University of Waterloo, Waterloo, ON, Canada    Fakhri
    Karray Department of Electrical and Computer Engineering,
  prefs: []
  type: TYPE_NORMAL
- en: Centre for Pattern Analysis and Machine Intelligence, University of Waterloo,
    Waterloo, ON, Canada    Mark Crowley Department of Electrical and Computer Engineering,
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a tutorial and survey paper on metric learning. Algorithms are divided
    into spectral, probabilistic, and deep metric learning. We first start with the
    definition of distance metric, Mahalanobis distance, and generalized Mahalanobis
    distance. In spectral methods, we start with methods using scatters of data, including
    the first spectral metric learning, relevant methods to Fisher discriminant analysis,
    Relevant Component Analysis (RCA), Discriminant Component Analysis (DCA), and
    the Fisher-HSIC method. Then, large-margin metric learning, imbalanced metric
    learning, locally linear metric adaptation, and adversarial metric learning are
    covered. We also explain several kernel spectral methods for metric learning in
    the feature space. We also introduce geometric metric learning methods on the
    Riemannian manifolds. In probabilistic methods, we start with collapsing classes
    in both input and feature spaces and then explain the neighborhood component analysis
    methods, Bayesian metric learning, information theoretic methods, and empirical
    risk minimization in metric learning. In deep learning methods, we first introduce
    reconstruction autoencoders and supervised loss functions for metric learning.
    Then, Siamese networks and its various loss functions, triplet mining, and triplet
    sampling are explained. Deep discriminant analysis methods, based on Fisher discriminant
    analysis, are also reviewed. Finally, we introduce multi-modal deep metric learning,
    geometric metric learning by neural networks, and few-shot metric learning.
  prefs: []
  type: TYPE_NORMAL
- en: Tutorial\AddToShipoutPictureBG
  prefs: []
  type: TYPE_NORMAL
- en: '*\AtPageUpperLeft                                                                                             
    <svg version="1.1" width="595.68" height="12.3" overflow="visible"><g transform="translate(0,12.3)
    scale(1,-1)"><g class="makebox" transform="translate(-297.84,-43.65)"><text x="0"
    y="0" transform="scale(1, -1)" fill="black">To appear as a part of an upcoming
    textbook on dimensionality reduction and manifold learning.</text></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dimensionality reduction and manifold learning are used for feature extraction
    from raw data. A family of dimensionality reduction methods is metric learning
    which learns a distance metric or an embedding space for separation of dissimilar
    points and closeness of similar points. In supervised metric learning, we aim
    to discriminate classes by learning an appropriate metric. Dimensionality reduction
    methods can be divided into spectral, probabilistic, and deep methods (Ghojogh,
    [2021](#bib.bib36)). Spectral methods have a geometrical approach and usually
    are reduced to generalized eigenvalue problems (Ghojogh et al., [2019a](#bib.bib38)).
    Probabilistic methods are based on probability distributions. Deep methods use
    neural network for learning. In each of these categories, there exist several
    metric learning methods. In this paper, we review and introduce the most important
    metric learning algorithms in these categories. Note that there exist some other
    surveys on metric learning such as (Yang & Jin, [2006](#bib.bib132); Yang, [2007](#bib.bib131);
    Kulis, [2013](#bib.bib76); Bellet et al., [2013](#bib.bib10); Wang & Sun, [2015](#bib.bib116);
    Suárez et al., [2021](#bib.bib107)). A survey specific to deep metric learning
    is (Kaya & Bilge, [2019](#bib.bib72)). A book on metric learning is (Bellet et al.,
    [2015](#bib.bib11)). Finally, some Python toolboxes for metric learning are (Suárez
    et al., [2020](#bib.bib106); De Vazelhes et al., [2020](#bib.bib24); Musgrave
    et al., [2020](#bib.bib90)). The remainder of this paper is organized as follows.
    Section [2](#S2 "2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey") defines distance metric and the
    generalized Mahalanobis distance. Sections [3](#S3 "3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"), [4](#S4
    "4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"), and [5](#S5 "5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey") introduce and discuss spectral,
    probabilistic, and deep metric learning methods, respectively. Finally, section
    [6](#S6 "6 Conclusion ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey") concludes the paper. The table of contents can be found at the end
    of paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Required Background for the Reader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper assumes that the reader has general knowledge of calculus, probability,
    linear algebra, and basics of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Generalized Mahalanobis Distance Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Distance Metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Definition 1  (Distance metric).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider a metric space $\mathcal{X}$. A distance metric is a mapping $d:\mathcal{X}\times\mathcal{X}\rightarrow[0,\infty)$
    which satisfies the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'non-negativity: $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\geq 0$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'identity: $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=0\iff\boldsymbol{x}_{i}=\boldsymbol{x}_{j}$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'symmetry: $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=d(\boldsymbol{x}_{j},\boldsymbol{x}_{i})$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'triangle inequality: $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\leq d(\boldsymbol{x}_{i},\boldsymbol{x}_{k})+d(\boldsymbol{x}_{k},\boldsymbol{x}_{j})$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where $\boldsymbol{x}_{i},\boldsymbol{x}_{j},\boldsymbol{x}_{k}\in\mathcal{X}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of distance metric is the Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{2}:=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})}.$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 2.2 Mahalanobis Distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Mahalanobis distance is another distance metric which was originally proposed
    in (Mahalanobis, [1930](#bib.bib82)).
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2  (Mahalanobis distance (Mahalanobis, [1930](#bib.bib82))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider a $d$-dimensional metric space $\mathcal{X}$. Let two clouds or sets
    of points $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$ be in the data, i.e., $\mathcal{X}_{1},\mathcal{X}_{2}\in\mathcal{X}$.
    A point is considered in each set, i.e., $\boldsymbol{x}_{i}\in\mathcal{X}_{1}$
    and $\boldsymbol{x}_{j}\in\mathcal{X}_{2}$. The Mahalanobis distance between the
    two points is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{\Sigma}}:=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})},$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{\Sigma}\in\mathbb{R}^{d\times d}$ is the covariance matrix
    of data in the two sets $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the points $\boldsymbol{x}_{i}$ and $\boldsymbol{x}_{j}$ are the means of
    the sets $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$, respectively, as the representatives
    of the sets, this Mahalanobis distance is a good measure of distance of the sets
    (McLachlan, [1999](#bib.bib84)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2}\&#124;_{\boldsymbol{\Sigma}}:=\sqrt{(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2})},$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{\mu}_{1}$ and $\boldsymbol{\mu}_{2}$ are the means of the
    sets $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $\mathcal{X}_{1}:=\{\boldsymbol{x}_{1,i}\}_{i=1}^{n_{1}}$ and $\mathcal{X}_{2}:=\{\boldsymbol{x}_{2,i}\}_{i=1}^{n_{2}}$.
    The unbiased sample covariance matrices of these two sets are:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{1}:=\frac{1}{n_{1}-1}\sum_{i=1}^{n_{1}}(\boldsymbol{x}_{1,i}-\boldsymbol{\mu}_{1})(\boldsymbol{x}_{1,i}-\boldsymbol{\mu}_{1})^{\top},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'and $\boldsymbol{\Sigma}_{2}$ similarly. The covariance matrix $\boldsymbol{\Sigma}$
    can be an unbiased sample covariance matrix (McLachlan, [1999](#bib.bib84)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Sigma}:=\frac{1}{n_{1}+n_{2}-2}\Big{(}(n_{1}-1)\boldsymbol{\Sigma}_{1}+(n_{2}-1)\boldsymbol{\Sigma}_{2}\Big{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The Mahalanobis distance can also be defined between a point $\boldsymbol{x}$
    and a cloud or set of points $\mathcal{X}$ (De Maesschalck et al., [2000](#bib.bib23)).
    Let $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ be the mean and the (sample)
    covariance matrix of the set $\mathcal{X}$. The Mahalanobis distance of $\boldsymbol{x}$
    and $\mathcal{X}$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}-\boldsymbol{\mu}\&#124;_{\boldsymbol{\Sigma}}:=\sqrt{(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}.$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/250b5f41ffd51a70b54877640e6751d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An example for comparison of the Euclidean and Mahalanobis distances.'
  prefs: []
  type: TYPE_NORMAL
- en: Remark 1  (Justification of the Mahalanobis distance (De Maesschalck et al.,
    [2000](#bib.bib23))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider two clouds of data, $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$, depicted
    in Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"). We want to compute the distance of a point $\boldsymbol{x}$ from
    these two data clouds to see which cloud this point is closer to. The Euclidean
    distance ignores the scatter/variance of clouds and only measures the distances
    of the point from the means of clouds. Hence, in this example, it says that $\boldsymbol{x}$
    belongs to $\mathcal{X}_{1}$ because it is closer to the mean of $\mathcal{X}_{1}$
    compared to $\mathcal{X}_{2}$. However, the Mahalanobis distance takes the variance
    of clouds into account and says that $\boldsymbol{x}$ belongs to $\mathcal{X}_{2}$
    because it is closer to its scatter compared to $\mathcal{X}_{1}$. Visually, human
    also says $\boldsymbol{x}$ belongs to $\mathcal{X}_{2}$; hence, the Mahalanobis
    distance has performed better than the Euclidean distance by considering the variances
    of data.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Generalized Mahalanobis Distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Definition 3  (Generalized Mahalanobis distance).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In Mahalanobis distance, i.e. Eq. ([2](#S2.E2 "In Definition 2 (Mahalanobis
    distance (Mahalanobis, 1930)). ‣ 2.2 Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), the covariance matrix $\boldsymbol{\Sigma}$ and its inverse $\boldsymbol{\Sigma}^{-1}$
    are positive semi-definite. We can replace $\boldsymbol{\Sigma}^{-1}$ with a positive
    semi-definite weight matrix $\boldsymbol{W}\succeq\boldsymbol{0}$ in the squared
    Mahalanobis distance. We name this distance a generalized Mahalanobis distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}:=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})}.$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\therefore\,\,\,\,\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}:=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'We define the generalized Mahalanobis norm as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}\&#124;_{\boldsymbol{W}}:=\sqrt{\boldsymbol{x}^{\top}\boldsymbol{W}\boldsymbol{x}}.$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Lemma 1  (Triangle inequality of norm).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $\|.\|$ be a norm. Using the Cauchy-Schwarz inequality, it satisfies the
    triangle inequality:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}+\boldsymbol{x}_{j}\&#124;\leq\&#124;\boldsymbol{x}_{i}\&#124;+\&#124;\boldsymbol{x}_{j}\&#124;.$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}+\boldsymbol{x}_{j}\&#124;^{2}$
    | $\displaystyle=(\boldsymbol{x}_{i}+\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{i}+\boldsymbol{x}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\&#124;\boldsymbol{x}_{i}\&#124;^{2}+\&#124;\boldsymbol{x}_{j}\&#124;^{2}+2\boldsymbol{x}_{i}^{\top}\boldsymbol{x}_{j}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(a)}{\leq}\&#124;\boldsymbol{x}_{i}\&#124;^{2}+\&#124;\boldsymbol{x}_{j}\&#124;^{2}+2\&#124;\boldsymbol{x}_{i}\&#124;\&#124;\boldsymbol{x}_{j}\&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=(\&#124;\boldsymbol{x}_{i}\&#124;+\&#124;\boldsymbol{x}_{j}\&#124;)^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because of the Cauchy-Schwarz inequality, i.e., $\boldsymbol{x}_{i}^{\top}\boldsymbol{x}_{j}\leq\|\boldsymbol{x}_{i}\|\|\boldsymbol{x}_{j}\|$.
    Taking second root from the sides gives Eq. ([7](#S2.E7 "In Lemma 1 (Triangle
    inequality of norm). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). Q.E.D. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The generalized Mahalanobis distance is a valid distance metric.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We show that the characteristics in Definition [1](#Thmdefinition1 "Definition
    1 (Distance metric). ‣ 2.1 Distance Metric ‣ 2 Generalized Mahalanobis Distance
    Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")
    are satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As $\boldsymbol{W}\succeq\boldsymbol{0}$, Eq. ([5](#S2.E5 "In Definition 3
    (Generalized Mahalanobis distance). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2
    Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) is non-negative.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'identity: if $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=0$,
    according to Eq. ([5](#S2.E5 "In Definition 3 (Generalized Mahalanobis distance).
    ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we
    have $\boldsymbol{x}_{i}-\boldsymbol{x}_{j}=0\implies\boldsymbol{x}_{i}=\boldsymbol{x}_{j}$.
    If $\boldsymbol{x}_{i}=\boldsymbol{x}_{j}$, we have $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=0$
    according to Eq. ([5](#S2.E5 "In Definition 3 (Generalized Mahalanobis distance).
    ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'symmetry:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})}=\sqrt{(\boldsymbol{x}_{j}-\boldsymbol{x}_{i})^{\top}\boldsymbol{W}(\boldsymbol{x}_{j}-\boldsymbol{x}_{i})}=\|\boldsymbol{x}_{j}-\boldsymbol{x}_{i}\|_{\boldsymbol{W}}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'triangle inequality: $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}+\boldsymbol{x}_{k}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}\overset{(\ref{equation_xi_xj_triangle_inequality})}{\leq}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\|_{\boldsymbol{W}}+\|\boldsymbol{x}_{k}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Remark 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is noteworthy that $\boldsymbol{W}\succeq\boldsymbol{0}$ is required so that
    the generalized Mahalanobis distance is convex and satisfies the triangle inequality.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The weight matrix $\boldsymbol{W}$ in Eq. ([5](#S2.E5 "In Definition 3 (Generalized
    Mahalanobis distance). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) weights the dimensions and determines some correlation
    between dimensions of data points. In other words, it changes the space in a way
    that the scatters of clouds are considered.'
  prefs: []
  type: TYPE_NORMAL
- en: Remark 4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Euclidean distance is a special case of the Mahalanobis distance where
    the weight matrix is the identity matrix, i.e., $\boldsymbol{W}=\boldsymbol{I}$
    (cf. Eqs. ([1](#S2.E1 "In 2.1 Distance Metric ‣ 2 Generalized Mahalanobis Distance
    Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    and ([5](#S2.E5 "In Definition 3 (Generalized Mahalanobis distance). ‣ 2.3 Generalized
    Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))). In other words, the Euclidean
    distance does not change the space for computing the distance.'
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 2  (Projection in metric learning).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider the eigenvalue decomposition of the weight matrix $\boldsymbol{W}$
    in the generalized Mahalanobis distance with $\boldsymbol{V}$ and $\boldsymbol{\Lambda}$
    as the matrix of eigenvectors and the diagonal matrix of eigenvalues of the weight,
    respectively. Let $\boldsymbol{U}:=\boldsymbol{V}\boldsymbol{\Lambda}^{(1/2)}$.
    The generalized Mahalanobis distance can be seen as the Euclidean distance after
    applying a linear projection onto the column space of $\boldsymbol{U}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    | $\displaystyle=(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})^{\top}(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\&#124;\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j}\&#124;_{2}^{2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: If $\boldsymbol{U}\in\mathbb{R}^{d\times p}$ with $p\leq d$, the column space
    of the projection matrix $\boldsymbol{U}$ is a $p$-dimensional subspace.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By the eigenvalue decomposition of $\boldsymbol{W}$, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{W}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}\overset{(a)}{=}\boldsymbol{V}\boldsymbol{\Lambda}^{(1/2)}\boldsymbol{\Lambda}^{(1/2)}\boldsymbol{V}^{\top}\overset{(b)}{=}\boldsymbol{U}\boldsymbol{U}^{\top},$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because $\boldsymbol{W}$ is positive semi-definite so all its
    eigenvalues are non-negative and can be written as multiplication of its second
    roots. Also, $(b)$ is because we define $\boldsymbol{U}:=\boldsymbol{V}\boldsymbol{\Lambda}^{(1/2)}$.
    Substituting Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) in Eq. ([5](#S2.E5 "In Definition 3 (Generalized
    Mahalanobis distance). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    | $\displaystyle=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})^{\top}(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\&#124;\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j}\&#124;_{2}^{2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Q.E.D. It is noteworthy that Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis
    Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) can also be obtained using singular
    value decomposition rather than eigenvalue decomposition. In that case, the matrices
    of right and left singular vectors are equal because of symmetry of $\boldsymbol{W}$.
    ∎'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 The Main Idea of Metric Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider a $d$-dimensional dataset $\{\boldsymbol{x}_{i}\}_{i=1}^{n}\subset\mathbb{R}^{d}$
    of size $n$. Assume some data points are similar in some sense. For example, they
    have similar pattern or the same characteristics. Hence, we have a set of similar
    pair points, denotes by $\mathcal{S}$. In contrast, we can have dissimilar points
    which are different in pattern or characteristics. Let the set of dissimilar pair
    points be denoted by $\mathcal{D}$. In summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\text{
    if }\boldsymbol{x}_{i}\text{ and }\boldsymbol{x}_{j}\text{ are similar},$ |  |
    (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}\text{
    if }\boldsymbol{x}_{i}\text{ and }\boldsymbol{x}_{j}\text{ are dissimilar}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The measure of similarity and dissimilarity can be belonging to the same or
    different classes, if class labels are available for dataset. In this case, we
    have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\text{
    if }\boldsymbol{x}_{i}\text{ and }\boldsymbol{x}_{j}\text{ are in the same class},$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}\text{
    if }\boldsymbol{x}_{i}\text{ and }\boldsymbol{x}_{j}\text{ are in different classes}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'In metric learning, we learn the weight matrix so that the distances of similar
    points become smaller and the distances of dissimilar points become larger. In
    this way, the variance of similar and dissimilar points get smaller and larger,
    respectively. A 2D visualization of metric learning is depicted in Fig. [2](#S2.F2
    "Figure 2 ‣ 2.4 The Main Idea of Metric Learning ‣ 2 Generalized Mahalanobis Distance
    Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey").
    If the class labels are available, metric learning tries to make the intra-class
    and inter-class variances smaller and larger, respectively. This is the same idea
    as the idea of Fisher Discriminant Analysis (FDA) (Fisher, [1936](#bib.bib33);
    Ghojogh et al., [2019b](#bib.bib39)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/508b5d11ef0b47e4fe0b78b0344c5a49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Visualizing metric learning in 2D: (a) the contour of Euclidean distance
    which does not properly discriminate classes, and (b) the contour of Euclidean
    distance which is better in discrimination of classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Spectral Metric Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Spectral Methods Using Scatters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1 The First Spectral Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first metric learning method was proposed in (Xing et al., [2002](#bib.bib127)).
    In this method, we minimize the distances of the similar points by the weight
    matrix $\boldsymbol{W}$ where this matrix is positive semi-definite:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'However, the solution of this optimization problem is trivial, i.e., $\boldsymbol{W}=\boldsymbol{0}$.
    Hence, we add a constraint on the dissimilar points to have distances larger than
    some positive amount:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}\geq\alpha,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha>0$ is some positive number such as $\alpha=1$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 2  ((Xing et al., [2002](#bib.bib127))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If the constraint in Eq. ([12](#S3.E12 "In 3.1.1 The First Spectral Method
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is squared, i.e.,
    $\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}\geq\alpha$,
    the solution of optimization will have rank $1$. Hence, we are using a non-squared
    constraint in the optimization problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If the constraint in Eq. ([12](#S3.E12 "In 3.1.1 The First Spectral Method
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is squared, the
    problem is equivalent to (see (Ghojogh et al., [2019b](#bib.bib39), Appendix B)
    for proof):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\frac{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}}{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'which is a Rayleigh-Ritz quotient (Ghojogh et al., [2019a](#bib.bib38)). We
    can restate $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}),$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{D}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\textbf{tr}(.)$ denotes the trace of matrix and:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{S}}:=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top},$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{D}}:=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Hence, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\frac{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}}{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}}=\frac{\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{D}})}{\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})}\overset{(\ref{equation_W_U_UT})}{=}\frac{\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}})}{\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(a)}{=}\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{U})}=\frac{\sum_{i=1}^{d}\boldsymbol{u}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{u}}{\sum_{i=1}^{d}\boldsymbol{u}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{u}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because of the cyclic property of trace and $(b)$ is because
    $\boldsymbol{U}=[\boldsymbol{u}_{1},\dots,\boldsymbol{u}_{d}]$. Maximizing this
    Rayleigh-Ritz quotient results in the following generalized eigenvalue problem
    (Ghojogh et al., [2019a](#bib.bib38)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{u}_{1}=\lambda\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{u}_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{u}_{1}$ is the eigenvector with largest eigenvalue and the
    other eigenvectors $\boldsymbol{u}_{2},\dots,\boldsymbol{u}_{d}$ are zero vectors.
    Q.E.D. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'The Eq. ([12](#S3.E12 "In 3.1.1 The First Spectral Method ‣ 3.1 Spectral Methods
    Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) can be restated as a maximization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq\alpha,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'We can solve this problem using projected gradient method (Ghojogh et al.,
    [2021c](#bib.bib48)) where a step of gradient ascent is followed by projection
    onto the two constraint sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{W}+\eta\frac{\partial}{\partial\boldsymbol{W}}\Big{(}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{W}:=\arg\min_{\boldsymbol{Q}}\Big{(}\&#124;\boldsymbol{Q}-\boldsymbol{W}\&#124;_{F}^{2}\,\text{
    s.t.}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{Q}}^{2}\leq\alpha\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{V}\,\textbf{diag}(\max(\lambda_{1},0),\dots,\max(\lambda_{d},0))\,\boldsymbol{V}^{\top},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\eta>0$ is the learning rate and $\boldsymbol{V}$ and $\boldsymbol{\Lambda}=\textbf{diag}(\lambda_{1},\dots,\lambda_{d})$
    are the eigenvectors and eigenvalues of $\boldsymbol{W}$, respectively (see Eq.
    ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Formulating as Semidefinite Programming
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another metric learning method is (Ghodsi et al., [2007](#bib.bib35)) which
    minimizes the distances of similar points and maximizes the distances of dissimilar
    points. For this, we minimize the distances of similar points and the negation
    of distances of dissimilar points. The weight matrix should be positive semi-definite
    to satisfy the triangle inequality and convexity. The trace of weight matrix is
    also set to a constant to eliminate the trivial solution $\boldsymbol{W}=\boldsymbol{0}$.
    The optimization problem is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\textbf{tr}(\boldsymbol{W})=1,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $|.|$ denotes the cardinality of set.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 3  ((Ghodsi et al., [2007](#bib.bib35))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The objective function can be simplified as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}=\textbf{vec}(\boldsymbol{W})^{\top}\Big{(}\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\textbf{vec}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\textbf{vec}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\big{)}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\textbf{vec}(.)$ vectorizes the matrix to a vector (Ghojogh et al., [2021c](#bib.bib48)).
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: See (Ghodsi et al., [2007](#bib.bib35), Section 2.1) for proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Lemma [3](#Thmlemma3 "Lemma 3 ((Ghodsi et al., 2007)). ‣ 3.1.2
    Formulating as Semidefinite Programming ‣ 3.1 Spectral Methods Using Scatters
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"), Eq. ([16](#S3.E16 "In 3.1.2 Formulating as Semidefinite
    Programming ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is
    a Semidefinite Programming (SDP) problem. It can be solved iteratively using the
    interior-point method (Ghojogh et al., [2021c](#bib.bib48)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Relevant to Fisher Discriminant Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another metric learning method is (Alipanahi et al., [2008](#bib.bib2)) which
    has two approaches, introduced in the following. The relation of metric learning
    with Fisher discriminant analysis (Fisher, [1936](#bib.bib33); Ghojogh et al.,
    [2019b](#bib.bib39)) was discussed in this paper (Alipanahi et al., [2008](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: '– Approach 1: As $\boldsymbol{W}\succeq\boldsymbol{0}$, the weight matrix can
    be decomposed as in Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), i.e., $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$.
    Hence, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    | $\displaystyle\overset{(\ref{equation_generalized_Mahalanobis_distance})}{=}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(a)}{=}\textbf{tr}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(\ref{equation_W_U_UT})}{=}\textbf{tr}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(b)}{=}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\big{)},$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because a scalar is equal to its trace and $(b)$ is because
    of the cyclic property of trace. We can substitute Eq. ([18](#S3.E18 "In 3.1.3
    Relevant to Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) in Eq. ([16](#S3.E16 "In 3.1.2 Formulating as Semidefinite
    Programming ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) to
    obtain an optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}$ |  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\big{)}$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\!\!\!\!\!\!\!-\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top})=1,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'whose objective variable is $\boldsymbol{U}$. Note that the constraint $\boldsymbol{W}\succeq\boldsymbol{0}$
    is implicitly satisfied because of the decomposition $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$.
    We define:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\overset{(\ref{equation_spectral_ML_first_method_trace_W_Sigma_S})}{=}\frac{1}{&#124;\mathcal{S}&#124;}\boldsymbol{\Sigma}_{\mathcal{S}},$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\overset{(\ref{equation_spectral_ML_first_method_trace_W_Sigma_S})}{=}\frac{1}{&#124;\mathcal{D}&#124;}\boldsymbol{\Sigma}_{\mathcal{D}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Hence, Eq. ([19](#S3.E19 "In 3.1.3 Relevant to Fisher Discriminant Analysis
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) can be restated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top})=1,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'whose Lagrangian is (Ghojogh et al., [2021c](#bib.bib48)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}=\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})-\lambda(\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top})-1).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Taking derivative of the Lagrangian and setting it to zero gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\boldsymbol{U}}=2(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}-2\lambda\boldsymbol{U}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}=\lambda\boldsymbol{U},$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: 'which is the eigenvalue problem for $(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    (Ghojogh et al., [2019a](#bib.bib38)). Hence, $\boldsymbol{U}$ is the eigenvector
    of $(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    with the smallest eigenvalue because Eq. ([19](#S3.E19 "In 3.1.3 Relevant to Fisher
    Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    is a minimization problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '– Approach 2: We can change the constraint in Eq. ([21](#S3.E21 "In 3.1.3 Relevant
    to Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) to have orthogonal projection matrix, i.e., $\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}$.
    Rather, we can make the rotation of the projection matrix by the matrix $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$
    be orthogonal, i.e., $\boldsymbol{U}^{\top}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\boldsymbol{U}=\boldsymbol{I}$.
    Hence, the optimization problem becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}=\boldsymbol{I},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'whose Lagrangian is (Ghojogh et al., [2021c](#bib.bib48)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}=\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})-\textbf{tr}(\boldsymbol{\Lambda}^{\top}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}-\boldsymbol{I})).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\boldsymbol{U}}=2(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}-2\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}\boldsymbol{\Lambda}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}=\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}\boldsymbol{\Lambda},$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: which is the generalized eigenvalue problem for $(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}},\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})$
    (Ghojogh et al., [2019a](#bib.bib38)). Hence, $\boldsymbol{U}$ is a matrix whose
    columns are the eigenvectors sorted from the smallest to largest eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: The optimization problem is similar to the optimization of Fisher discriminant
    analysis (FDA) (Fisher, [1936](#bib.bib33); Ghojogh et al., [2019b](#bib.bib39))
    where $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$ and $\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}$
    are replaced with the intra-class and inter-class covariance matrices of data,
    respectively. This shows the relation of this method with FDA. It makes sense
    because both metric learning and FDA have the same goal and that is decreasing
    and increasing the variances of similar and dissimilar points, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Relevant Component Analysis (RCA)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose the $n$ data points can be divided into $c$ clusters, or so-called
    chunklets. If class labels are available, classes are the chunklets. If $\mathcal{X}_{l}$
    denotes the data of the $l$-th cluster and $\boldsymbol{\mu}_{l}$ is the mean
    of $\mathcal{X}_{l}$, the summation of intra-cluster scatters is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{S}_{w}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{\boldsymbol{x}_{i}\in\mathcal{X}_{l}}(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{l})(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{l})^{\top}.$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: 'Relevant Component Analysis (RCA) (Shental et al., [2002](#bib.bib101)) is
    a metric learning method. In this method, we first apply Principal Component Analysis
    (PCA) (Ghojogh & Crowley, [2019](#bib.bib37)) on data using the total scatter
    of data. Let the projection matrix of PCA be denoted by $\boldsymbol{U}$. After
    projection onto the PCA subspace, the summation of intra-cluster scatters is $\widehat{\boldsymbol{S}}_{w}:=\boldsymbol{U}^{\top}\boldsymbol{S}_{w}\boldsymbol{U}$
    because of the quadratic characteristic of covariance. RCA uses $\widehat{\boldsymbol{S}}_{w}$
    as the covariance matrix in the Mahalanobis distance, i.e., Eq. ([2](#S2.E2 "In
    Definition 2 (Mahalanobis distance (Mahalanobis, 1930)). ‣ 2.2 Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")). According to Eq. ([8](#S2.E8 "In Proposition
    2 (Projection in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2
    Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")), the subspace of RDA is obtained by the eigenvalue
    (or singular value) decomposition of $\widehat{\boldsymbol{S}}_{w}^{-1}$ (see
    Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Discriminative Component Analysis (DCA)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Discriminative Component Analysis (DCA) (Hoi et al., [2006](#bib.bib69)) is
    another spectral metric learning method based on scatters of clusters/classes.
    Consider the $c$ clusters, chunklets, or classes of data. The intra-class scatter
    is as in Eq. ([25](#S3.E25 "In 3.1.4 Relevant Component Analysis (RCA) ‣ 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). The inter-class scatter is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{S}_{b}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{j=1}^{c}(\boldsymbol{\mu}_{l}-\boldsymbol{\mu}_{j})(\boldsymbol{\mu}_{l}-\boldsymbol{\mu}_{j})^{\top},\text{
    or }$ |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{S}_{b}:=\frac{1}{n}\sum_{l=1}^{c}(\boldsymbol{\mu}_{l}-\boldsymbol{\mu})(\boldsymbol{\mu}_{l}-\boldsymbol{\mu})^{\top},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{\mu}_{l}$ is the mean of the $l$-th class and $\boldsymbol{\mu}$
    is the total mean of data. According to Proposition [2](#Thmproposition2 "Proposition
    2 (Projection in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2
    Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey"), metric learning can be seen as Euclidean distance
    after projection onto the column space of a projection matrix $\boldsymbol{U}$
    where $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$. Similar to Fisher
    discriminant analysis (Fisher, [1936](#bib.bib33); Ghojogh et al., [2019b](#bib.bib39)),
    DCA maximizes the inter-class variance and minimizes the intra-class variance
    after projection. Hence, its optimization is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{w}\boldsymbol{U})},$
    |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: 'which is a generalized Rayleigh-Ritz quotient. The solution $\boldsymbol{U}$
    to this optimization problem is the generalized eigenvalue problem $(\boldsymbol{S}_{b},\boldsymbol{S}_{w})$
    (Ghojogh et al., [2019a](#bib.bib38)). According to Eq. ([9](#S2.E9 "In Proof.
    ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we
    can set the weight matrix of the generalized Mahalanobis distance as $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$
    where $\boldsymbol{U}$ is the matrix of eigenvectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6 High Dimensional Discriminative Component Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another spectral method for metric learning is (Xiang et al., [2008](#bib.bib126))
    which minimizes and maximizes the intra-class and inter-class variances, respectively,
    by the the same optimization problem as Eq. ([27](#S3.E27 "In 3.1.5 Discriminative
    Component Analysis (DCA) ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    with an additional constraint on the orthogonality of the projection matrix, i.e.,
    $\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}$. This problem can be restated
    by posing penalty on the denominator:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{S}_{b}-\lambda\boldsymbol{S}_{w})\boldsymbol{U})$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda>0$ is the regularization parameter. The solution to this problem
    is the eigenvalue problem for $\boldsymbol{S}_{b}-\lambda\boldsymbol{S}_{w}$.
    The eigenvectors are the columns of $\boldsymbol{U}$ and the weight matrix of
    the generalized Mahalanobis is obtained using Eq. ([9](#S2.E9 "In Proof. ‣ 2.3
    Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the dimensionality of data is large, computing the eigenvectors of $(\boldsymbol{S}_{b}-\lambda\boldsymbol{S}_{w})\in\mathbb{R}^{d\times
    d}$ is very time-consuming. According to (Xiang et al., [2008](#bib.bib126), Theorem
    3), the optimization problem ([28](#S3.E28 "In 3.1.6 High Dimensional Discriminative
    Component Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) can
    be solved in the orthogonal complement space of the null space of $\boldsymbol{S}_{b}+\boldsymbol{S}_{w}$
    without loss of any information (see (Xiang et al., [2008](#bib.bib126), Appendix
    A) for proof). Hence, if $d\gg 1$, we find $\boldsymbol{U}$ as follows. Let $\boldsymbol{X}:=[\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}]\in\mathbb{R}^{d\times
    n}$ be the matrix of data. Let $\boldsymbol{A}_{w}$ and $\boldsymbol{A}_{b}$ be
    the adjacency matrices for the sets $\mathcal{S}$ and $\mathcal{D}$, respectively.
    For example, if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$, then
    $\boldsymbol{A}_{w}(i,j)=1$; otherwise, $\boldsymbol{A}_{w}(i,j)=0$. If $\boldsymbol{L}_{w}$
    and $\boldsymbol{L}_{b}$ are the Laplacian matrices of $\boldsymbol{A}_{w}$ and
    $\boldsymbol{A}_{b}$, respectively, we have $\boldsymbol{S}_{w}=0.5\boldsymbol{X}\boldsymbol{L}_{w}\boldsymbol{X}^{\top}$
    and $\boldsymbol{S}_{b}=0.5\boldsymbol{X}\boldsymbol{L}_{b}\boldsymbol{X}^{\top}$
    (see (Belkin & Niyogi, [2002](#bib.bib9); Ghojogh et al., [2021d](#bib.bib49))
    for proof). We have $\textbf{tr}(\boldsymbol{S}_{w}+\boldsymbol{S}_{b})=\textbf{tr}(\boldsymbol{X}(0.5\boldsymbol{L}_{w}+0.5\boldsymbol{L}_{b})\boldsymbol{X}^{\top})=\textbf{tr}(\boldsymbol{X}^{\top}\boldsymbol{X}(0.5\boldsymbol{L}_{w}+0.5\boldsymbol{L}_{b}))$
    because of the cyclic property of trace. If the rank of $\boldsymbol{L}:=\boldsymbol{X}^{\top}\boldsymbol{X}(0.5\boldsymbol{L}_{w}+0.5\boldsymbol{L}_{b})\in\mathbb{R}^{n\times
    n}$ is $r\leq n$, it has $r$ non-zero eigenvalues which we compute its corresponding
    eigenvectors. We stack these eigenvectors to have $\boldsymbol{V}\in\mathbb{R}^{d\times
    r}$. The projected intra-class and inter-class variances after projection onto
    the column space of $\boldsymbol{V}$ are $\boldsymbol{S}^{\prime}_{w}:=\boldsymbol{V}^{\top}\boldsymbol{S}_{w}\boldsymbol{V}$
    and $\boldsymbol{S}^{\prime}_{b}:=\boldsymbol{V}^{\top}\boldsymbol{S}_{b}\boldsymbol{V}$,
    respectively. Then, we use $\boldsymbol{S}^{\prime}_{w}$ and $\boldsymbol{S}^{\prime}_{b}$
    in Eq. ([28](#S3.E28 "In 3.1.6 High Dimensional Discriminative Component Analysis
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) and the weight
    matrix of the generalized Mahalanobis is obtained using Eq. ([9](#S2.E9 "In Proof.
    ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.7 Regularization by Locally Linear Embedding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The spectral metric learning methods using scatters can be modeled as maximization
    of the following Rayleigh–Ritz quotient (Baghshah & Shouraki, [2009](#bib.bib4)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}}{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}+\lambda\Omega(\boldsymbol{U})},$
    |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$ (see Eq. ([9](#S2.E9
    "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"))), $\lambda>0$ is the regularization parameter, and $\Omega(\boldsymbol{U})$
    is a penalty or regularization term on the projection matrix $\boldsymbol{U}$.
    This optimization maximizes and minimizes the distances of the similar and dissimilar
    points, respectively. According to Section [3.1.3](#S3.SS1.SSS3 "3.1.3 Relevant
    to Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"), Eq. ([29](#S3.E29 "In 3.1.7 Regularization by Locally Linear Embedding
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) can be restated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{w}\boldsymbol{U})+\lambda\Omega(\boldsymbol{U})},$
    |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'As was discussed in Proposition [2](#Thmproposition2 "Proposition 2 (Projection
    in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"), metric learning can be seen as projection onto a subspace. The regularization
    term can be linear reconstruction of every projected point by its $k$ Nearest
    Neighbors ($k$NN) using the same reconstruction weights as before projection (Baghshah
    & Shouraki, [2009](#bib.bib4)). The weights for linear reconstruction in the input
    space can be found as in locally linear embedding (Roweis & Saul, [2000](#bib.bib97);
    Ghojogh et al., [2020a](#bib.bib40)). If $s_{ij}$ denotes the weight of $\boldsymbol{x}_{j}$
    in reconstruction of $\boldsymbol{x}_{i}$ and $\mathcal{N}(\boldsymbol{x}_{i})$
    is the set of $k$NN for $\boldsymbol{x}_{i}$, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{s_{ij}}{\text{minimize}}$ | $\displaystyle\sum_{i=1}^{n}\Big{\&#124;}\boldsymbol{x}_{i}-\sum_{\boldsymbol{x}_{j}\in\mathcal{N}(\boldsymbol{x}_{i})}s_{ij}\boldsymbol{x}_{j}\Big{\&#124;}_{2}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | subject to | $\displaystyle\sum_{\boldsymbol{x}_{j}\in\mathcal{N}(\boldsymbol{x}_{i})}s_{ij}=1.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The solution of this optimization is (Ghojogh et al., [2020a](#bib.bib40)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle s_{ij}^{*}=\frac{\boldsymbol{G}_{i}^{-1}\boldsymbol{1}}{\boldsymbol{1}^{\top}\boldsymbol{G}_{i}^{-1}\boldsymbol{1}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{G}_{i}:=(\boldsymbol{x}_{i}\boldsymbol{1}^{\top}-\boldsymbol{X}_{i})^{\top}(\boldsymbol{x}_{i}\boldsymbol{1}^{\top}-\boldsymbol{X}_{i})$
    in which $\boldsymbol{X}_{i}\in\mathbb{R}^{d\times k}$ denotes the stack of $k$NN
    for $\boldsymbol{x}_{i}$. We define $\boldsymbol{S}^{*}:=[s^{*}_{ij}]\in\mathbb{R}^{n\times
    n}$. The regularization term can be reconstruction in the subspace using the same
    reconstruction weights as in the input space (Baghshah & Shouraki, [2009](#bib.bib4)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Omega(\boldsymbol{U})$ | $\displaystyle:=\sum_{i=1}^{n}\Big{\&#124;}\boldsymbol{U}^{\top}\boldsymbol{x}-\sum_{\boldsymbol{x}_{j}\in\mathcal{N}(\boldsymbol{x}_{i})}s^{*}_{ij}\boldsymbol{U}^{\top}\boldsymbol{x}_{j}\Big{\&#124;}_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top}\boldsymbol{U}),$
    |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{X}=[\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}]\in\mathbb{R}^{d\times
    n}$ and $\mathbb{R}^{n\times n}\ni\boldsymbol{E}:=(\boldsymbol{I}-\boldsymbol{S}^{*})^{\top}(\boldsymbol{I}-\boldsymbol{S}^{*})$.
    Putting Eq. ([31](#S3.E31 "In 3.1.7 Regularization by Locally Linear Embedding
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) in Eq. ([30](#S3.E30
    "In 3.1.7 Regularization by Locally Linear Embedding ‣ 3.1 Spectral Methods Using
    Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}\boldsymbol{U})}{\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{w}+\lambda\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top})\boldsymbol{U}\big{)}},$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The solution to this optimization problem is the generalized eigenvalue problem
    $(\boldsymbol{S}_{b},\boldsymbol{S}_{w}+\lambda\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top})$
    where $\boldsymbol{U}$ has the eigenvectors as its columns (Ghojogh et al., [2019a](#bib.bib38)).
    According to Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), the weight matrix of metric is $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.8 Fisher-HSIC Multi-view Metric Learning (FISH-MML)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Fisher-HSIC Multi-view Metric Learning (FISH-MML) (Zhang et al., [2018](#bib.bib142))
    is a metric learning method for multi-view data. In multi-view data, we have different
    types of features for every data point. For example, an image dataset, which has
    a descriptive caption for every image, is multi-view. Let $\boldsymbol{X}^{(r)}:=\{\boldsymbol{x}_{i}^{(r)}\}_{i=1}^{n}$
    be the features of data points in the $r$-th view, $c$ be the number of classes/clusters,
    and $v$ be the number of views. According to Proposition [2](#Thmproposition2
    "Proposition 2 (Projection in metric learning). ‣ 2.3 Generalized Mahalanobis
    Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"), metric learning is the Euclidean
    distance after projection with $\boldsymbol{U}$. The inter-class scatter of data,
    in the $r$-th view, is denoted by $\boldsymbol{S}_{b}^{(r)}$ and calculated using
    Eqs. ([26](#S3.E26 "In 3.1.5 Discriminative Component Analysis (DCA) ‣ 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). The total scatter of data, in
    the $r$-th view, is denoted by $\boldsymbol{S}_{t}^{(r)}$ and is the covariance
    of data in that view.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by Fisher discriminant analysis (Fisher, [1936](#bib.bib33); Ghojogh
    et al., [2019b](#bib.bib39)), we maximize the inter-class variances of projected
    data, $\sum_{r=1}^{v}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}^{(r)}\boldsymbol{U})$,
    to discriminate the classes after projection. Also, inspired by principal component
    analysis (Ghojogh & Crowley, [2019](#bib.bib37)), we maximize the total scatter
    of projected data, $\sum_{r=1}^{v}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{t}^{(r)}\boldsymbol{U})$,
    for expressiveness. Moreover, we maximize the dependence of the projected data
    in all views because various views of a point should be related. A measure of
    dependence between two random variables $X$ and $Y$ is the Hilbert-Schmidt Independence
    Criterion (HSIC) (Gretton et al., [2005](#bib.bib56)) whose empirical estimation
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{HSIC}(X,Y)=\frac{1}{(n-1)^{2}}\textbf{tr}(\boldsymbol{K}_{x}\boldsymbol{H}\boldsymbol{K}_{y}\boldsymbol{H}),$
    |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{K}_{x}$ and $\boldsymbol{K}_{y}$ are kernel matrices over
    $X$ and $Y$ variables, respectively, and $\boldsymbol{H}:=\boldsymbol{I}-(1/n)\boldsymbol{1}\boldsymbol{1}^{\top}$
    is the centering matrix. The HSIC between projection of two views $\boldsymbol{X}^{(r)}$
    and $\boldsymbol{X}^{(w)}$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{HSIC}(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)},\boldsymbol{U}^{\top}\boldsymbol{X}^{(w)})\overset{(\ref{equation_HSIC})}{\propto}\textbf{tr}(\boldsymbol{K}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}(\boldsymbol{X}^{(r)\top}\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(b)}{=}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top}\boldsymbol{U})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $(a)$ is because we use the linear kernel for $\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}$,
    i.e., $\boldsymbol{K}^{(r)}:=(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)})^{\top}\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}$
    and $(b)$ is because of the cyclic property of trace.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we maximize the summation of inter-class scatter, total scatter,
    and the dependence of views, which is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sum_{r=1}^{v}\big{(}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}^{(r)}\boldsymbol{U})+\lambda_{1}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{t}^{(r)}\boldsymbol{U})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{2}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top}\boldsymbol{U})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\sum_{r=1}^{v}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{b}^{(r)}+\lambda_{1}\boldsymbol{S}_{t}^{(r)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{2}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top})\boldsymbol{U}\big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda_{1},\lambda_{2}>0$ are the regularization parameters. The optimization
    problem is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{r=1}^{v}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{b}^{(r)}+\lambda_{1}\boldsymbol{S}_{t}^{(r)}$
    |  | (34) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}+\lambda_{2}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top})\boldsymbol{U}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: whose solution is the eigenvalue problem for $\boldsymbol{S}_{b}^{(r)}+\lambda_{1}\boldsymbol{S}_{t}^{(r)}+\lambda_{2}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top}$
    where $\boldsymbol{U}$ has the eigenvectors as its columns (Ghojogh et al., [2019a](#bib.bib38)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Spectral Methods Using Hinge Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.2.1 Large-Margin Metric Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '$k$-Nearest Neighbors ($k$NN) classification is highly impacted by the metric
    used for measuring distances between points. Hence, we can use metric learning
    for improving the performance of $k$NN classification (Weinberger et al., [2006](#bib.bib124);
    Weinberger & Saul, [2009](#bib.bib123)). Let $y_{ij}=1$ if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$
    and $y_{ij}=0$ if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}$. Moreover,
    we consider $k$NN for similar points where we find the nearest neighbors of every
    point among the similar points to that point. Let $\eta_{ij}=1$ if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$
    and $\boldsymbol{x}_{j}$ is among $k$NN of $\boldsymbol{x}_{i}$. Otherwise, $\eta_{ij}=0$.
    The optimization problem for finding the best weigh matrix in the metric can be
    (Weinberger et al., [2006](#bib.bib124); Weinberger & Saul, [2009](#bib.bib123)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}\eta_{ij}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\lambda\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{l=1}^{n}\eta_{ij}(1-y_{il})\Big{[}1$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}\Big{]}_{+},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda>0$ is the regularization parameter, and $[.]_{+}:=\max(.,0)$
    is the standard Hinge loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first term in Eq. ([35](#S3.E35 "In 3.2.1 Large-Margin Metric Learning
    ‣ 3.2 Spectral Methods Using Hinge Loss ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) pushes the similar
    neighbors close to each other. The second term in this equation is the triplet
    loss (Schroff et al., [2015](#bib.bib100)) which pushes the similar neighbors
    to each other and pulls the dissimilar points away from one another. This is because
    minimizing $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$ for
    $\eta_{ij}=1$ decreases the distances of similar neighbors. Moreover, minimizing
    $-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}$ for $1-y_{il}=1$
    (i.e., $y_{il}=0$) is equivalent to maximizing $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}$
    which maximizes the distances of dissimilar points. Minimizing the whole second
    term forces the distances of dissimilar points to be at least greater that the
    distances of similar points up to a threshold (or margin) of one. We can change
    the margin by changing $1$ in this term with some other positive number. In this
    sense, this loss is closely related to the triplet loss for neural networks (Schroff
    et al., [2015](#bib.bib100)) (see Section [5.3.5](#S5.SS3.SSS5 "5.3.5 Triplet
    Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Eq. ([35](#S3.E35 "In 3.2.1 Large-Margin Metric Learning ‣ 3.2 Spectral Methods
    Using Hinge Loss ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) can be restated using slack variables
    $\xi_{ijl},\forall i,j,l\in\{1,\dots,n\}$. The Hinge loss in term $[1+\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}]_{+}$
    requires to have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 1+\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}\geq
    0$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    1.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'If $\xi_{ijl}\geq 0$, we can have sandwich the term $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$
    in order to minimize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 1-\xi_{ijl}\leq\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    1.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Hence, we can replace the term of Hinge loss with the slack variable. Therefore,
    Eq. ([35](#S3.E35 "In 3.2.1 Large-Margin Metric Learning ‣ 3.2 Spectral Methods
    Using Hinge Loss ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) can be restated as (Weinberger et al.,
    [2006](#bib.bib124); Weinberger & Saul, [2009](#bib.bib123)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W},\,\{\xi_{ijl}\}}{\text{minimize}}$
    |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}\eta_{ij}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\lambda\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{l=1}^{n}\eta_{ij}(1-y_{il})\,\xi_{ijl}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\geq
    1-\xi_{ijl},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},\eta_{ij}=1,(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\xi_{ijl}\geq 0,$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This optimization problem is a semidefinite programming which can be solved
    iteratively using interior-point method (Ghojogh et al., [2021c](#bib.bib48)).
  prefs: []
  type: TYPE_NORMAL
- en: This problem uses triplets of similar and dissimilar points, i.e., $\{\boldsymbol{x}_{i},\boldsymbol{x}_{j},\boldsymbol{x}_{l}\}$
    where $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$, $\eta_{ij}=1$,
    $(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D}$. Hence, triplets should
    be extracted randomly from the dataset for this metric learning. Solving semidefinite
    programming is usually slow and time-consuming especially for large datasets.
    Triplet minimizing can be used for finding the best triplets for learning (Poorheravi
    et al., [2020](#bib.bib92)). For example, the similar and dissimilar points with
    smallest and/or largest distances can be used to limit the number of triplets
    (Sikaroudi et al., [2020a](#bib.bib102)). The reader can also refer to for Lipschitz
    analysis in large margin metric learning (Dong, [2019](#bib.bib29)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Imbalanced Metric Learning (IML)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imbalanced Metric Learning (IML) (Gautheron et al., [2019](#bib.bib34)) is
    a spectral metric learning method which handles imbalanced classes by further
    decomposition of the similar set $\mathcal{S}$ and dissimilar set $\mathcal{D}$.
    Suppose the dataset is composed of two classes $c_{0}$ and $c_{1}$. Let $\mathcal{S}_{0}$
    and $\mathcal{S}_{1}$ denote the similarity sets for classes $c_{0}$ and $c_{1}$,
    respectively. We define pairs of points taken randomly from these sets to have
    similarity and dissimilarity sets (Gautheron et al., [2019](#bib.bib34)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Sim}_{0}\subseteq\mathcal{S}_{0}\times\mathcal{S}_{0},\quad\text{Sim}_{1}\subseteq\mathcal{S}_{1}\times\mathcal{S}_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{Dis}_{0}\subseteq\mathcal{S}_{0}\times\mathcal{S}_{1},\quad\text{Dis}_{1}\subseteq\mathcal{S}_{1}\times\mathcal{S}_{0}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The optimization problem of IML is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}~{}~{}~{}~{}~{}~{}\frac{\lambda}{4&#124;\text{Sim}_{0}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Sim}_{0}}\big{[}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-1\big{]}_{+}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\frac{\lambda}{4&#124;\text{Sim}_{1}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Sim}_{1}}\big{[}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-1\big{]}_{+}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\frac{1-\lambda}{4&#124;\text{Dis}_{0}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Dis}_{0}}\big{[}\!-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}+1+m\big{]}_{+}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\frac{1-\lambda}{4&#124;\text{Dis}_{1}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Dis}_{1}}\big{[}\!-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}+1+m\big{]}_{+}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\gamma\&#124;\boldsymbol{W}-\boldsymbol{I}\&#124;_{F}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{subject to}~{}~{}~{}~{}\boldsymbol{W}\succeq\boldsymbol{0},$
    |  | (37) |'
  prefs: []
  type: TYPE_TB
- en: where $|.|$ denotes the cardinality of set, $[.]_{+}:=\max(.,0)$ is the standard
    Hinge loss, $m>0$ is the desired margin between classes, and $\lambda\in[0,1]$
    and $\gamma>0$ are the regularization parameters. This optimization pulls the
    similar points to have distance less than $1$ and pushes the dissimilar points
    away to have distance more than $m+1$. Also, the regularization term $\|\boldsymbol{W}-\boldsymbol{I}\|_{F}^{2}$
    tries to make the weight matrix is the generalized Mahalanobis distance close
    to identity for simplicity of metric. In this way, the metric becomes close to
    the Euclidean distance, preventing overfitting, while satisfying the desired margins
    in distances.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Locally Linear Metric Adaptation (LLMA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another method for metric learning is Locally Linear Metric Adaptation (LLMA)
    (Chang & Yeung, [2004](#bib.bib16)). LLMA performs nonlinear and linear transformations
    globally and locally, respectively. For every point $\boldsymbol{x}_{l}$, we consider
    its $k$ nearest (similar) neighbors. The local linear transformation for every
    point $\boldsymbol{x}_{l}$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{R}^{d}\ni\boldsymbol{y}_{l}:=\boldsymbol{x}_{l}+\boldsymbol{B}\boldsymbol{\pi}_{i},$
    |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{B}\in\mathbb{R}^{d\times k}$ is the matrix of biases, $\mathbb{R}^{k}\ni\boldsymbol{\pi}_{i}=[\pi_{i1},\dots,\pi_{ik}]^{\top}$,
    and $\pi_{ij}:=\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{2}^{2}/2w^{2})$
    is a Gaussian measure of similarity between $\boldsymbol{x}_{i}$ and $\boldsymbol{x}_{j}$.
    The variables $\boldsymbol{B}$ and $w$ are found by optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this method, we minimize the distances between the linearly transformed
    similar points while the distances of similar points are tried to be preserved
    after the transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\{\boldsymbol{y}_{i}\}_{i=1}^{n},\boldsymbol{B},w,\sigma}{\text{minimize}}$
    |  | $\displaystyle\sum_{(\boldsymbol{y}_{i},\boldsymbol{y}_{j})\in\mathcal{S}}\&#124;\boldsymbol{y}_{i}-\boldsymbol{y}_{j}\&#124;_{2}^{2}$
    |  | (39) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\lambda\,\sum_{i=1}^{n}\sum_{j=1}^{n}(q_{ij}-d_{ij})^{2}\exp(\frac{-d_{ij}^{2}}{\sigma^{2}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda>0$ is the regularization parameter, $\sigma_{2}^{2}$ is the variance
    to be optimized, and $d_{ij}:=\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{2}$ and
    $q_{ij}:=\|\boldsymbol{y}_{i}-\boldsymbol{y}_{j}\|_{2}$. This objective function
    is optimized iteratively until convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Relevant to Support Vector Machine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inspired by $\nu$-Support Vector Machine ($\nu$-SVM) (Schölkopf et al., [2000](#bib.bib99)),
    the weight matrix in the generalized Mahalanobis distance can be obtained as (Tsang
    et al., [2003](#bib.bib112)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W},\gamma,\{\xi_{il}\}}{\text{minimize}}$
    |  | $\displaystyle\frac{1}{2}\&#124;\boldsymbol{W}\&#124;_{2}^{2}+\frac{\lambda_{1}}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (40) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\lambda_{2}\Big{(}\nu\gamma+\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D}}\xi_{il}\Big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\gamma\geq 0,$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}\geq\gamma-\xi_{il},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\xi_{il}\geq 0,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda_{1},\lambda_{2}>0$ are regularization parameters. Using KKT
    conditions and Lagrange multipliers (Ghojogh et al., [2021c](#bib.bib48)), the
    dual optimization problem is (see (Tsang et al., [2003](#bib.bib112)) for derivation):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\{\alpha_{ij}\}}{\text{maximize}}~{}~{}~{}~{}~{}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  | (41) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{D}}\alpha_{ij}\alpha_{kl}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{k}-\boldsymbol{x}_{l}))^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\frac{\lambda_{1}}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{S}}\alpha_{ij}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{k}-\boldsymbol{x}_{l}))^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\text{subject to}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\frac{1}{\lambda_{2}}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}\geq\nu,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\alpha_{ij}\in[0,\frac{\lambda_{2}}{&#124;\mathcal{D}&#124;}],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\{\alpha_{ij}\}$ are the dual variables. This problem is a quadratic
    programming problem and can be solved using optimization solvers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Relevant to Multidimensional Scaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multidimensional Scaling (MDS) tries to preserve the distance after projection
    onto its subspace (Cox & Cox, [2008](#bib.bib21); Ghojogh et al., [2020b](#bib.bib41)).
    We saw in Proposition [2](#Thmproposition2 "Proposition 2 (Projection in metric
    learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey") that metric learning can be seen as projection onto the column space
    of $\boldsymbol{U}$ where $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$.
    Inspired by MDS, we can learn a metric which preserves the distances between points
    after projection onto the subspace of metric (Zhang et al., [2003](#bib.bib144)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{2}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})^{2}$
    |  | (42) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: It can be solved using any optimization method (Ghojogh et al., [2021c](#bib.bib48)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Kernel Spectral Metric Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let $k(\boldsymbol{x}_{i},\boldsymbol{x}_{j}):=\boldsymbol{\phi}(\boldsymbol{x}_{i})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})$
    be the kernel function over data points $\boldsymbol{x}_{i}$ and $\boldsymbol{x}_{j}$,
    where $\boldsymbol{\phi}(.)$ is the pulling function to the Reproducing Kernel
    Hilbert Space (RKHS) (Ghojogh et al., [2021e](#bib.bib50)). Let $\mathbb{R}^{n\times
    n}\ni\boldsymbol{K}:=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})$
    be the kernel matrix of data. In the following, we introduce some of the kernel
    spectral metric learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.1 Using Eigenvalue Decomposition of Kernel
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the kernel methods for spectral metric learning is (Yeung & Chang, [2007](#bib.bib140)).
    It has two approaches; we explain one of its approaches here. The eigenvalue decomposition
    of the kernel matrix is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{K}=\sum_{r=1}^{p}\beta_{r}^{2}\boldsymbol{\alpha}_{r}\boldsymbol{\alpha}_{r}^{\top}\overset{(a)}{=}\sum_{r=1}^{p}\beta_{r}^{2}\boldsymbol{K}_{r}$
    |  | (43) |'
  prefs: []
  type: TYPE_TB
- en: 'where $p$ is the rank of kernel matrix, $\beta_{r}^{2}$ is the non-negative
    $r$-th eigenvalue (because $\boldsymbol{K}\succeq\boldsymbol{0}$), $\boldsymbol{\alpha}_{r}\in\mathbb{R}^{n}$
    is the $r$-th eigenvector, and $(a)$ is because we define $\boldsymbol{K}_{r}:=\boldsymbol{\alpha}_{r}\boldsymbol{\alpha}_{r}^{\top}$.
    We can consider $\{\beta_{r}^{2}\}_{r=1}^{p}$ as learnable parameters and not
    the eigenvalues. Hence, we learn $\{\beta_{r}^{2}\}_{r=1}^{p}$ for the sake of
    metric learning. The distance metric of pulled data points to RKHS is (Schölkopf,
    [2001](#bib.bib98); Ghojogh et al., [2021e](#bib.bib50)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-$ | $\displaystyle\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  | (44) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'In metric learning, we want to make the distances of similar points small;
    hence the objective to be minimized is: Hence, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(\ref{equation_kernel_eigenvalue_decomposition})}{=}\sum_{r=1}^{p}\beta_{r}^{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}k_{r}(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k_{r}(\boldsymbol{x}_{j},\boldsymbol{x}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-2k_{r}(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(a)}{=}\sum_{r=1}^{p}\beta_{r}^{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{r}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(b)}{=}\sum_{r=1}^{p}\beta_{r}^{2}f_{r}\overset{(c)}{=}\boldsymbol{\beta}^{\top}\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because $\boldsymbol{e}_{i}$ is the vector whose $i$-th element
    is one and other elements are zero, $(b)$ is because we define $f_{r}:=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{r}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})$,
    and $(c)$ is because we define $\boldsymbol{D}_{\mathcal{S}}:=\textbf{diag}([f_{1},\dots,f_{p}]^{\top})$
    and $\boldsymbol{\beta}:=[\beta_{1},\dots,\beta_{p}]^{\top}$. By adding a constraint
    on the summation of $\{\beta_{r}^{2}\}_{r=1}^{p}$, the optimization problem for
    metric learning is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{\beta}}{\text{minimize}}$ |  |
    $\displaystyle\boldsymbol{\beta}^{\top}\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta}$
    |  | (45) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{1}^{\top}\boldsymbol{\beta}=1.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'This optimization is similar to the form of one of the optimization problems
    in locally linear embedding (Roweis & Saul, [2000](#bib.bib97); Ghojogh et al.,
    [2020a](#bib.bib40)). The Lagrangian for this problem is (Ghojogh et al., [2021c](#bib.bib48)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}=\boldsymbol{\beta}^{\top}\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta}-\lambda(\boldsymbol{1}^{\top}\boldsymbol{\beta}-1),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda$ is the dual variable. Taking derivative of the Lagrangian w.r.t.
    the variables and setting to zero gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\boldsymbol{\beta}}=2\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta}-\lambda\boldsymbol{1}\overset{\text{set}}{=}0\implies\boldsymbol{\beta}=\frac{\lambda}{2}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\lambda}=\boldsymbol{1}^{\top}\boldsymbol{\beta}-1\overset{\text{set}}{=}0\implies\boldsymbol{1}^{\top}\boldsymbol{\beta}=1,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies\frac{\lambda}{2}\boldsymbol{1}^{\top}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}=1\implies\lambda=\frac{2}{\boldsymbol{1}^{\top}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies\boldsymbol{\beta}=\frac{\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}}{\boldsymbol{1}^{\top}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Hence, the optimal $\boldsymbol{\beta}$ is obtained for metric learning in the
    RKHS where the distances of similar points is smaller than in the input Euclidean
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.2 Regularization by Locally Linear Embedding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The method (Baghshah & Shouraki, [2009](#bib.bib4)), which was introduced in
    Section [3.1.7](#S3.SS1.SSS7 "3.1.7 Regularization by Locally Linear Embedding
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey"), can be kernelized.
    Recall that this method used locally linear embedding for regularization. According
    to the representation theory (Ghojogh et al., [2021e](#bib.bib50)), the solution
    in the RKHS can be represented as a linear combination of all pulled data points
    to RKHS:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})=\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T},$
    |  | (46) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{X}=[\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}]$ and $\boldsymbol{T}\in\mathbb{R}^{n\times
    p}$ ($p$ is the dimensionality of subspace) is the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the similarity and dissimilarity adjacency matrices as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\boldsymbol{A}_{S}(i,j):=\left\{\begin{array}[]{ll}1&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},\\ 0&amp;\mbox{otherwise.}\end{array}\right.$
    |  | (47) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\boldsymbol{A}_{D}(i,j):=\left\{\begin{array}[]{ll}1&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},\\ 0&amp;\mbox{otherwise.}\end{array}\right.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Let $\boldsymbol{L}_{w}$ and $\boldsymbol{L}_{b}$ denote the Laplacian matrices
    (Ghojogh et al., [2021d](#bib.bib49)) of these adjacency matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{L}_{w}:=\boldsymbol{D}_{S}-\boldsymbol{A}_{S}(i,j),\quad\boldsymbol{L}_{b}:=\boldsymbol{D}_{D}-\boldsymbol{A}_{D}(i,j),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{D}_{S}(i,i):=\sum_{j=1}^{n}\boldsymbol{A}_{S}(i,j)$ and
    $\boldsymbol{D}_{D}(i,i):=\sum_{j=1}^{n}\boldsymbol{A}_{D}(i,j)$ are diagonal
    matrices. The terms in the objective of Eq. ([32](#S3.E32 "In 3.1.7 Regularization
    by Locally Linear Embedding ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) can be restated using Laplacian of adjacency matrices rather than
    the scatters:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{L}_{b}\boldsymbol{U})}{\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{L}_{w}+\lambda\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top})\boldsymbol{U}\big{)}},$
    |  | (48) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'According to the representation theory, the pulled Laplacian matrices to RKHS
    are $\boldsymbol{\Phi}(\boldsymbol{L}_{b})=\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{b}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}$
    and $\boldsymbol{\Phi}(\boldsymbol{L}_{w})=\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{w}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}$.
    Hence, the numerator of Eq. ([32](#S3.E32 "In 3.1.7 Regularization by Locally
    Linear Embedding ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) in
    RKHS becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textbf{tr}(\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{b}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{U}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{b}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{L}_{b}\boldsymbol{K}_{x}\boldsymbol{T}\big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $(a)$ is because of the kernel trick (Ghojogh et al., [2021e](#bib.bib50)),
    i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{K}_{x}:=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X}).$
    |  | (49) |'
  prefs: []
  type: TYPE_TB
- en: 'similarly, the denominator of Eq. ([32](#S3.E32 "In 3.1.7 Regularization by
    Locally Linear Embedding ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    in RKHS becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textbf{tr}\big{(}\boldsymbol{\Phi}(\boldsymbol{U})^{\top}(\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{w}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}+\lambda\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{E}\boldsymbol{\Phi}(\boldsymbol{X})^{\top})\boldsymbol{\Phi}(\boldsymbol{U})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(\ref{equation_kernelization_representation_theory})}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}(\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{w}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{E}\boldsymbol{\Phi}(\boldsymbol{X})^{\top})\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}(\boldsymbol{L}_{w}+\lambda\boldsymbol{E})\boldsymbol{K}_{x}\boldsymbol{T}\big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because of the kernel trick (Ghojogh et al., [2021e](#bib.bib50)).
    The constrain in RKHS becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{U})\overset{(\ref{equation_kernelization_representation_theory})}{=}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}\overset{(a)}{=}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because of the kernel trick (Ghojogh et al., [2021e](#bib.bib50)).
    The Eq. ([32](#S3.E32 "In 3.1.7 Regularization by Locally Linear Embedding ‣ 3.1
    Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) in RKHS is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{T}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{L}_{b}\boldsymbol{K}_{x}\boldsymbol{T}\big{)}}{\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}(\boldsymbol{L}_{w}+\lambda\boldsymbol{E})\boldsymbol{K}_{x}\boldsymbol{T}\big{)}},$
    |  | (50) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T}=\boldsymbol{I}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'It can be solved using projected gradient method (Ghojogh et al., [2021c](#bib.bib48))
    to find the optimal $\boldsymbol{T}$. Then, the projected data onto the subspace
    of metric is found as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\overset{(\ref{equation_kernelization_representation_theory})}{=}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\overset{(a)}{=}\boldsymbol{T}^{\top}\boldsymbol{K}_{x},$
    |  | (51) |'
  prefs: []
  type: TYPE_TB
- en: where $(a)$ is because of the kernel trick (Ghojogh et al., [2021e](#bib.bib50)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.3 Regularization by Laplacian
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another kernel spectral metric learning method is (Baghshah & Shouraki, [2010](#bib.bib5))
    whose optimization is in the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{\Phi}(\boldsymbol{X})}{\text{minimize}}$
    |  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}+\lambda\Omega(\boldsymbol{\Phi}(\boldsymbol{X})),$
    |  | (52) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}\geq
    c,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $c>0$ is a hyperparameter and $\lambda>0$ is the regularization parameter.
    Consider the $k$NN graph of data with an adjacency matrix $\boldsymbol{A}\in\mathbb{R}^{n\times
    n}$ whose $(i,j)$-th element is one if $\boldsymbol{x}_{i}$ and $\boldsymbol{x}_{j}$
    are neighbors and is zero otherwise. Let the Laplacian matrix of this adjacency
    matrix be denoted by $\boldsymbol{L}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this method, the regularization term $\Omega(\boldsymbol{\Phi}(\boldsymbol{X}))$
    can be the objective of Laplacian eigenmap (Ghojogh et al., [2021d](#bib.bib49)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Omega(\boldsymbol{\Phi}(\boldsymbol{X})):=$ | $\displaystyle\frac{1}{2n}\sum_{i=1}^{n}\sum_{j=1}^{n}\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}\boldsymbol{A}(i,j)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(a)}{=}\textbf{tr}(\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}\boldsymbol{\Phi}(\boldsymbol{X})^{\top})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(b)}{=}\textbf{tr}(\boldsymbol{L}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X}))\overset{(c)}{=}\textbf{tr}(\boldsymbol{L}\boldsymbol{K}_{x}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is according to (Belkin & Niyogi, [2001](#bib.bib8)) (see (Ghojogh
    et al., [2021d](#bib.bib49)) for proof), $(b)$ is because of the cyclic property
    of trace, and $(c)$ is because of the kernel trick (Ghojogh et al., [2021e](#bib.bib50)).
    Moreover, according to Eq. ([44](#S3.E44 "In 3.6.1 Using Eigenvalue Decomposition
    of Kernel ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), the
    distance in RKHS is $\|\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\|_{2}^{2}=k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$.
    We can simplify the term in Eq. ([52](#S3.E52 "In 3.6.3 Regularization by Laplacian
    ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(\ref{equation_distance_in_RKHS})}{=}\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})\overset{(a)}{=}\textbf{tr}(\boldsymbol{E}_{\mathcal{S}}\boldsymbol{K}_{x}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $(a)$ is because the scalar is equal to its trace and we use the cyclic
    property of trace, i.e., $(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})=\textbf{tr}((\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j}))=\textbf{tr}((\boldsymbol{e}_{i}-\boldsymbol{e}_{j})(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x})$,
    and then we define $\boldsymbol{E}_{\mathcal{S}}:=(1/|\mathcal{S}|)\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, Eq. ([52](#S3.E52 "In 3.6.3 Regularization by Laplacian ‣ 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) can be restated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{K}_{x}}{\text{minimize}}$ |  |
    $\displaystyle\textbf{tr}(\boldsymbol{E}_{\mathcal{S}}\boldsymbol{K}_{x})+\lambda\,\textbf{tr}(\boldsymbol{L}\boldsymbol{K}_{x}),$
    |  | (53) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\geq
    c,$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\boldsymbol{K}_{x}\succeq\boldsymbol{0},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'noticing that the kernel matrix is positive semidefinite. This problem is a
    Semidefinite Programming (SDP) problem and can be solved using the interior point
    method (Ghojogh et al., [2021c](#bib.bib48)). The optimal kernel matrix can be
    decomposed using eigenvalue decomposition to find the embedding of data in RKHS,
    i.e., $\boldsymbol{\Phi}(\boldsymbol{X})$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{K}_{x}=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}\boldsymbol{V}=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}^{(1/2}\boldsymbol{\Sigma}^{(1/2)}\boldsymbol{V}\overset{(\ref{equation_Kernel_X})}{=}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{V}$ and $\boldsymbol{\Sigma}$ are the eigenvectors and eigenvalues,
    $(a)$ is because $\boldsymbol{K}_{x}\succeq\boldsymbol{0}$ so its eigenvalues
    are non-negative can be taken second root of, and $(b)$ is because we get $\boldsymbol{\Phi}(\boldsymbol{X}):=\boldsymbol{\Sigma}^{(1/2)}\boldsymbol{V}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.4 Kernel Discriminative Component Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here, we explain the kernel version of DCA (Hoi et al., [2006](#bib.bib69))
    which was introduced in Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Discriminative Component
    Analysis (DCA) ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The generalized Mahalanobis distance metric in RKHS, with the pulled weight
    matrix to RKHS denoted by $\boldsymbol{\Phi}(\boldsymbol{W})$, can be seen as
    measuring the Euclidean distance in RKHS after projection onto the column subspace
    of $\boldsymbol{T}$ where $\boldsymbol{T}$ is the coefficient matrix in Eq. ([46](#S3.E46
    "In 3.6.2 Regularization by Locally Linear Embedding ‣ 3.6 Kernel Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")). In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-$ | $\displaystyle\,\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{\boldsymbol{\Phi}(\boldsymbol{W})}^{2}=\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\&#124;_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$
    |  | (54) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{k}_{i}:=\boldsymbol{k}(\boldsymbol{X},\boldsymbol{x}_{i})=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})=[k(\boldsymbol{x}_{1},\boldsymbol{x}_{i}),\dots,k(\boldsymbol{x}_{n},\boldsymbol{x}_{i})]^{\top}\in\mathbb{R}^{n}$
    is the kernel vector between $\boldsymbol{X}$ and $\boldsymbol{x}_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can have the decomposition of the weight matrix, i.e. Eq. ([9](#S2.E9 "In
    Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance
    Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")),
    in RKHS which is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{W})=\boldsymbol{\Phi}(\boldsymbol{U})\boldsymbol{\Phi}(\boldsymbol{U})^{\top}.$
    |  | (55) |'
  prefs: []
  type: TYPE_TB
- en: 'The generalized Mahalanobis distance metric in RKHS is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{\boldsymbol{\Phi}(\boldsymbol{W})}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(\ref{equation_W_U_UT})}{=}(\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j}))^{\top}\boldsymbol{\Phi}(\boldsymbol{U})\boldsymbol{\Phi}(\boldsymbol{U})^{\top}(\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\big{(}\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}^{\top}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\big{(}\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(\ref{equation_kernelization_representation_theory})}{=}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}^{\top}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(a)}{=}\big{(}\boldsymbol{T}^{\top}\boldsymbol{k}_{i}-\boldsymbol{T}^{\top}\boldsymbol{k}_{j}\big{)}^{\top}\big{(}\boldsymbol{T}^{\top}\boldsymbol{k}_{i}-\boldsymbol{T}^{\top}\boldsymbol{k}_{j}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\big{(}\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\big{)}^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}\big{(}\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\big{)}=\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\&#124;_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $(a)$ is because of the kernel trick, i.e., $\boldsymbol{k}(\boldsymbol{X},\boldsymbol{x}_{i})=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})$.
    Q.E.D. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $\boldsymbol{\nu}_{l}:=[\frac{1}{n_{l}}\sum_{i=1}^{n_{l}}\boldsymbol{k}(\boldsymbol{x}_{1},\boldsymbol{x}_{i}),\dots,\frac{1}{n_{l}}\sum_{i=1}^{n_{l}}\boldsymbol{k}(\boldsymbol{x}_{n},\boldsymbol{x}_{i})]^{\top}\in\mathbb{R}^{n}$
    where $n_{l}$ denotes the cardinality of the $l$-th class. Let $\boldsymbol{K}_{w}$
    and $\boldsymbol{K}_{b}$ be the kernelized versions of $\boldsymbol{S}_{w}$ and
    $\boldsymbol{S}_{b}$, respectively (see Eqs. ([25](#S3.E25 "In 3.1.4 Relevant
    Component Analysis (RCA) ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    and ([26](#S3.E26 "In 3.1.5 Discriminative Component Analysis (DCA) ‣ 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))). If $\mathcal{X}_{l}$ denotes
    the $l$-th class, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{R}^{n\times n}\ni\boldsymbol{K}_{w}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{\boldsymbol{x}_{i}\in\mathcal{X}_{l}}(\boldsymbol{k}_{i}-\boldsymbol{\nu}_{l})(\boldsymbol{k}_{i}-\boldsymbol{\nu}_{l})^{\top}$
    |  | (56) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbb{R}^{n\times n}\ni\boldsymbol{K}_{b}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{j=1}^{c}(\boldsymbol{\nu}_{l}-\boldsymbol{\nu}_{j})(\boldsymbol{\nu}_{l}-\boldsymbol{\nu}_{j})^{\top}.$
    |  | (57) |'
  prefs: []
  type: TYPE_TB
- en: 'We saw the metric in RKHS can be seen as projection onto a subspace with the
    projection matrix $\boldsymbol{T}$. Therefore, Eq. ([27](#S3.E27 "In 3.1.5 Discriminative
    Component Analysis (DCA) ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    in RKHS becomes (Hoi et al., [2006](#bib.bib69)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{T}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{K}_{b}\boldsymbol{T})}{\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{K}_{w}\boldsymbol{T})},$
    |  | (58) |'
  prefs: []
  type: TYPE_TB
- en: 'which is a generalized Rayleigh-Ritz quotient. The solution $\boldsymbol{T}$
    to this optimization problem is the generalized eigenvalue problem $(\boldsymbol{K}_{b},\boldsymbol{K}_{w})$
    (Ghojogh et al., [2019a](#bib.bib38)). The weight matrix of the generalized Mahalanobis
    distance is obtained by Eqs. ([46](#S3.E46 "In 3.6.2 Regularization by Locally
    Linear Embedding ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) and
    ([55](#S3.E55 "In Proof. ‣ 3.6.4 Kernel Discriminative Component Analysis ‣ 3.6
    Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.5 Relevant to Kernel Fisher Discriminant Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here, we explain the kernel version of the metric learning method (Alipanahi
    et al., [2008](#bib.bib2)) which was introduced in Section [3.1.3](#S3.SS1.SSS3
    "3.1.3 Relevant to Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Eq. ([54](#S3.E54 "In Lemma 4\. ‣ 3.6.4 Kernel Discriminative
    Component Analysis ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we
    have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{\boldsymbol{\Phi}(\boldsymbol{W})}^{2}=(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}\textbf{tr}\big{(}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(b)}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because a scalar it equal to its trace and $(b)$ is because
    of the cyclic property of trace. Hence, Eq. ([20](#S3.E20 "In 3.1.3 Relevant to
    Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) in RKHS becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\textbf{tr}\Big{(}\boldsymbol{T}^{\top}\big{(}\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)}\Big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}\boldsymbol{T}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'and likewise:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}}\boldsymbol{T}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}:=\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}}:=\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Hence, in RKHS, the objective of the optimization problem ([23](#S3.E23 "In
    3.1.3 Relevant to Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) becomes $\textbf{tr}(\boldsymbol{T}^{\top}(\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}})\boldsymbol{T}^{\top})$.
    We change the constraint in Eq. ([23](#S3.E23 "In 3.1.3 Relevant to Fisher Discriminant
    Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) to $\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}$.
    In RKHS, this constraint becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{U})$
    | $\displaystyle\overset{(\ref{equation_kernelization_representation_theory})}{=}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(\ref{equation_Kernel_X})}{=}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T}\overset{\text{set}}{=}\boldsymbol{I},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, ([23](#S3.E23 "In 3.1.3 Relevant to Fisher Discriminant Analysis ‣
    3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) in RKHS becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{T}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{T}^{\top}(\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}})\boldsymbol{T})$
    |  | (59) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T}=\boldsymbol{I},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'whose solution is a generalized eigenvalue problem $(\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}},\boldsymbol{K}_{x})$
    where $\boldsymbol{T}$ is the matrix of eigenvectors. The weight matrix of the
    generalized Mahalanobis distance is obtained by Eqs. ([46](#S3.E46 "In 3.6.2 Regularization
    by Locally Linear Embedding ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) and ([55](#S3.E55 "In Proof. ‣ 3.6.4 Kernel Discriminative Component
    Analysis ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")). This
    is relevant to kernel Fisher discriminant analysis (Mika et al., [1999](#bib.bib86);
    Ghojogh et al., [2019b](#bib.bib39)) which minimizes and maximizes the intra-class
    and inter-class variances in RKHS.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.6 Relevant to Kernel Support Vector Machine
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here, we explain the kernel version of the metric learning method (Tsang et al.,
    [2003](#bib.bib112)) which was introduced in Section [3.4](#S3.SS4 "3.4 Relevant
    to Support Vector Machine ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"). It is relevant to kernel SVM.
    Using kernel trick (Ghojogh et al., [2021e](#bib.bib50)) and Eq. ([54](#S3.E54
    "In Lemma 4\. ‣ 3.6.4 Kernel Discriminative Component Analysis ‣ 3.6 Kernel Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), the Eq. ([41](#S3.E41 "In 3.4 Relevant
    to Support Vector Machine ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) can be kernelized as (Tsang et al.,
    [2003](#bib.bib112)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\{\alpha_{ij}\}}{\text{maximize}}~{}~{}~{}~{}~{}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}\boldsymbol{T}^{\top}(k_{ii}+k_{jj}-2k_{ij})$
    |  | (60) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{D}}\alpha_{ij}\alpha_{kl}(k_{ik}-k_{il}-k_{jk}+k_{jl})^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\frac{\lambda_{1}}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{S}}\alpha_{ij}(k_{ik}-k_{il}-k_{jk}+k_{jl})^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\text{subject to}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\frac{1}{\lambda_{2}}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}\geq\nu,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\alpha_{ij}\in[0,\frac{\lambda_{2}}{&#124;\mathcal{D}&#124;}],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which is a quadratic programming problem and can be solved by optimization solvers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Geometric Spectral Metric Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some spectral metric learning methods are geometric methods which use Riemannian
    manifolds. In the following, we introduce the mist well-known geometric methods.
    There are some other geometric methods, such as (Hauberg et al., [2012](#bib.bib62)),
    which are not covered for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.1 Geometric Mean Metric Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the geometric spectral metric learning is Geometric Mean Metric Learning
    (GMML) (Zadeh et al., [2016](#bib.bib141)). Let $\boldsymbol{W}$ be the weight
    matrix in the generalized Mahalanobis distance for similar points.
  prefs: []
  type: TYPE_NORMAL
- en: '– Regular GMML: In GMML, we use the inverse of weight matrix, i.e. $\boldsymbol{W}^{-1}$
    , for the dissimilar points. The optimization problem of GMML is (Zadeh et al.,
    [2016](#bib.bib141)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (61) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}^{-1}}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'According to Eq. ([13](#S3.E13 "In Proof. ‣ 3.1.1 The First Spectral Method
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), this problem
    can be restated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (62) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{\Sigma}_{\mathcal{S}}$ and $\boldsymbol{\Sigma}_{\mathcal{D}}$
    are defined in Eq. ([14](#S3.E14 "In Proof. ‣ 3.1.1 The First Spectral Method
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")). Taking derivative
    of the objective function w.r.t. $\boldsymbol{W}$ and setting it to zero gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial\boldsymbol{W}}\big{(}\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\boldsymbol{\Sigma}_{\mathcal{S}}-\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{W}^{-1}\overset{\text{set}}{=}\boldsymbol{0}\implies\boldsymbol{\Sigma}_{\mathcal{D}}=\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}.$
    |  | (63) |'
  prefs: []
  type: TYPE_TB
- en: This equation is the Riccati equation (Riccati, [1724](#bib.bib94)) and its
    solution is the midpoint of the geodesic connecting $\boldsymbol{\Sigma}_{\mathcal{S}}^{-1}$
    and $\boldsymbol{\Sigma}_{\mathcal{D}}$ (Bhatia, [2007](#bib.bib12), Section 1.2.13).
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 5  ((Bhatia, [2007](#bib.bib12), Chapter 6)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The geodesic curve connecting two points $\boldsymbol{\Sigma}_{1}$ and $\boldsymbol{\Sigma}_{2}$
    on the Symmetric Positive Definite (SPD) Riemannian manifold is denoted by $\boldsymbol{\Sigma}_{1}\sharp_{t}\boldsymbol{\Sigma}_{2}$
    and is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{1}\sharp_{t}\boldsymbol{\Sigma}_{2}:=\boldsymbol{\Sigma}_{1}^{(1/2)}\big{(}\boldsymbol{\Sigma}_{1}^{(-1/2)}\boldsymbol{\Sigma}_{2}\boldsymbol{\Sigma}_{1}^{(-1/2)}\big{)}^{t}\boldsymbol{\Sigma}_{1}^{(1/2)},$
    |  | (64) |'
  prefs: []
  type: TYPE_TB
- en: where $t\in[0,1]$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, the solution of Eq. ([63](#S3.E63 "In 3.7.1 Geometric Mean Metric Learning
    ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{W}$ | $\displaystyle=\boldsymbol{\Sigma}_{\mathcal{S}}^{-1}\sharp_{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(\ref{equation_SPD_geodesic})}{=}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}\big{(}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\big{)}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}.$
    |  | (65) |'
  prefs: []
  type: TYPE_TB
- en: 'The proof of Eq. ([65](#S3.E65 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7
    Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) is as follows (Hajiabadi et al.,
    [2019](#bib.bib59)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{D}}\overset{(\ref{equation_GMML_solution_1})}{=}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}((\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}))^{(1/2)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}=\boldsymbol{W},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $(a)$ is because $\boldsymbol{\Sigma}_{\mathcal{S}}\succeq\boldsymbol{0}$
    so its eigenvalues are non-negative and the matrix of eigenvalues can be decomposed
    by the second root in its eigenvalue decomposition to have $\boldsymbol{\Sigma}_{\mathcal{S}}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}$.
  prefs: []
  type: TYPE_NORMAL
- en: '– Regularized GMML: The matrix $\boldsymbol{\Sigma}_{\mathcal{S}}$ might be
    singular or near singular and hence non-invertible. Therefore, we regularize Eq.
    ([62](#S3.E62 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) to make the weight matrix close to a prior
    known positive definite matrix $\boldsymbol{W}_{0}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (66) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\lambda\big{(}\textbf{tr}(\boldsymbol{W}\boldsymbol{W}_{0}^{-1})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{W}_{0})-2d\big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda>0$ is the regularization parameter. The regularization term
    is the symmetrized log-determinant divergence between $\boldsymbol{W}$ and $\boldsymbol{W}_{0}$.
    Taking derivative of the objective function w.r.t. $\boldsymbol{W}$ and setting
    it to zero gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial\boldsymbol{W}}\big{(}\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})+\lambda\textbf{tr}(\boldsymbol{W}\boldsymbol{W}_{0}^{-1})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{W}_{0})-2\lambda
    d\big{)}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\boldsymbol{\Sigma}_{\mathcal{S}}-\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{W}^{-1}+\lambda\boldsymbol{W}_{0}^{-1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\boldsymbol{W}^{-1}\boldsymbol{W}_{0}\boldsymbol{W}^{-1}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0}=\boldsymbol{W}(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})\boldsymbol{W},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'which is again a Riccati equation (Riccati, [1724](#bib.bib94)) whose solution
    is the midpoint of the geodesic connecting $(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})^{-1}$
    and $(\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0})$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{W}$ | $\displaystyle=(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})^{-1}\sharp_{(1/2)}(\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0}).$
    |  | (67) |'
  prefs: []
  type: TYPE_TB
- en: '– Weighted GMML: Eq. ([62](#S3.E62 "In 3.7.1 Geometric Mean Metric Learning
    ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) can be restated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{S}}^{-1})+\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (68) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\delta(.,.)$ is the Riemannian distance (or Fréchet mean) on the SPD
    manifold (Arsigny et al., [2007](#bib.bib3), Eq 1.1):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\delta(\boldsymbol{\Sigma}_{1},\boldsymbol{\Sigma}_{2}):=\&#124;\log(\boldsymbol{\Sigma}_{2}^{(-1/2)}\boldsymbol{\Sigma}_{1}\boldsymbol{\Sigma}_{2}^{(-1/2)})\&#124;_{F},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\|.\|_{F}$ is the Frobenius norm. We can weight the objective in Eq.
    ([68](#S3.E68 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle(1-t)\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{S}}^{-1})+t\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (69) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $t\in[0,1]$ is a hyperparameter. The solution of this problem is the
    weighted version of Eq. ([67](#S3.E67 "In 3.7.1 Geometric Mean Metric Learning
    ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{W}$ | $\displaystyle=(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})^{-1}\sharp_{t}(\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0}).$
    |  | (70) |'
  prefs: []
  type: TYPE_TB
- en: 3.7.2 Low-rank Geometric Mean Metric Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can learn a low-rank weight matrix in GMML (Bhutani et al., [2018](#bib.bib13)),
    where the rank of wight matrix is set to be $p\ll d$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (71) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\textbf{rank}(\boldsymbol{W})=p.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'We can decompose it using eigenvalue decomposition as done in Eq. ([9](#S2.E9
    "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), i.e., $\boldsymbol{W}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}=\boldsymbol{U}\boldsymbol{U}^{\top}$,
    where we only have $p$ eigenvectors and $p$ eigenvalues. Therefore, the sizes
    of matrices are $\boldsymbol{V}\in\mathbb{R}^{d\times p}$, $\boldsymbol{\Lambda}\in\mathbb{R}^{p\times
    p}$, and $\boldsymbol{U}\in\mathbb{R}^{d\times p}$. By this decomposition, the
    objective function in Eq. ([71](#S3.E71 "In 3.7.2 Low-rank Geometric Mean Metric
    Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey") can
    be restated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textbf{tr}(\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{V}\boldsymbol{\Lambda}^{-1}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}(\boldsymbol{\Lambda}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V})+\textbf{tr}(\boldsymbol{\Lambda}^{-1}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(b)}{=}\textbf{tr}(\boldsymbol{\Lambda}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{\Lambda}^{-1}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(\boldsymbol{V}^{\top})^{-1}=\boldsymbol{V}$ because it is orthogonal,
    $(a)$ is because of the cyclic property of trace, and $(b)$ is because we define
    $\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}}:=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V}$
    and $\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}}:=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V}$.
    Noticing that the matrix of eigenvectors $\boldsymbol{V}$ is orthogonal, the Eq.
    ([71](#S3.E71 "In 3.7.2 Low-rank Geometric Mean Metric Learning ‣ 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) is restated to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{\Lambda},\boldsymbol{V}}{\text{minimize}}$
    |  | $\displaystyle\textbf{tr}(\boldsymbol{\Lambda}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{\Lambda}^{-1}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}})$
    |  | (72) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{\Lambda}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\textbf{rank}(\boldsymbol{W})=p$ is automatically satisfied by taking
    $\boldsymbol{V}\in\mathbb{R}^{d\times p}$ and $\boldsymbol{\Lambda}\in\mathbb{R}^{p\times
    p}$ in the decomposition. This problem can be solved by the alternative optimization
    (Ghojogh et al., [2021c](#bib.bib48)). If the variable $\boldsymbol{V}$ is fixed,
    minimization w.r.t. $\boldsymbol{\Lambda}$ is similar to the problem ([62](#S3.E62
    "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")); hence, its solution is similar to Eq. ([65](#S3.E65 "In
    3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")), i.e., $\boldsymbol{\Lambda}={\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}}}^{-1}\sharp_{(1/2)}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}}$
    (see Eq. ([64](#S3.E64 "In Lemma 5 ((Bhatia, 2007, Chapter 6)). ‣ 3.7.1 Geometric
    Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    for the definition of $\sharp_{t}$). If $\boldsymbol{\Lambda}$ is fixed, the orthogonality
    constraint $\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I}$ can be modeled
    by $\boldsymbol{V}$ belonging to the Grassmannian manifold $G(p,d)$ which is the
    set of $p$-dimensional subspaces of $\mathbb{R}^{d}$. To sum up, the alternative
    optimization is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Lambda}^{(\tau+1)}=(\boldsymbol{V}^{(\tau)\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V}^{(\tau)})^{-1}\sharp_{(1/2)}(\boldsymbol{V}^{(\tau)\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V}^{(\tau)}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{V}^{(\tau+1)}:=\arg\min_{\boldsymbol{V}\in G(p,d)}\Big{(}\textbf{tr}(\boldsymbol{\Lambda}^{(\tau+1)}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\textbf{tr}((\boldsymbol{\Lambda}^{(\tau+1)})^{-1}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V})\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tau$ is the iteration index. Optimization of $\boldsymbol{V}$ can be
    solved by Riemannian optimization (Absil et al., [2009](#bib.bib1)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.3 Geometric Mean Metric Learning for Partial Labels
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Partial label learning (Cour et al., [2011](#bib.bib20)) refers to when a set
    of candidate labels is available for every data point. GMML can be modified to
    be used for partial label learning (Zhou & Gu, [2018](#bib.bib145)). Let $\mathcal{Y}_{i}$
    denote the set of candidate labels for $\boldsymbol{x}_{i}$. If there are $q$
    candidate labels in total, we denote $\boldsymbol{y}_{i}=[y_{i1},\dots,y_{iq}]^{\top}\in\{0,1\}^{q}$
    where $y_{ij}$ is one if the $j$-th label is a candidate label for $\boldsymbol{x}_{i}$
    and is zero otherwise. We define $\boldsymbol{X}_{i}^{+}:=\{\boldsymbol{x}_{j}|j=1,\dots,n,j\neq
    i,\mathcal{Y}_{i}\cap\mathcal{Y}_{j}\neq\varnothing\}$ and $\boldsymbol{X}_{i}^{-}:=\{\boldsymbol{x}_{j}|j=1,\dots,n,\mathcal{Y}_{i}\cap\mathcal{Y}_{j}=\varnothing\}$.
    In other words, $\boldsymbol{X}_{i}^{+}$ and $\boldsymbol{X}_{i}^{-}$ are the
    data points which share and do not share some candidate labels with $\boldsymbol{x}_{i}$,
    respectively. Let $\mathcal{N}_{i}^{+}$ be the indices of the $k$ nearest neighbors
    of $\boldsymbol{x}_{i}$ among $\boldsymbol{X}_{i}^{+}$. Also, let $\mathcal{N}_{i}^{-}$
    be the indices of points in $\boldsymbol{X}_{i}^{-}$ whose distance from $\boldsymbol{x}_{i}$
    are smaller than the distance of the furthest point in $\mathcal{N}_{i}^{+}$ from
    $\boldsymbol{x}_{i}$. In other words, $\mathcal{N}_{i}^{-}:=\{j|j=1,\dots,n,\boldsymbol{x}_{j}\in\boldsymbol{X}_{i}^{-},\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{2}\leq\max_{t\in\mathcal{N}_{i}^{+}}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{t}\|_{2}\}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $\boldsymbol{w}_{i}^{(1)}=[w_{i,t}^{(1)},\forall t\in\mathcal{N}_{i}^{+}]^{\top}\in\mathbb{R}^{k}$
    contain the probabilities that each of the $k$ neighbors of $\boldsymbol{x}_{i}$
    share the same label with $\boldsymbol{x}_{i}$. It can be estimated by linear
    reconstruction of $\boldsymbol{y}_{i}$ by the neighbor $\boldsymbol{y}_{t}$’s:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{w}_{i}^{(1)}}{\text{minimize}}$
    |  | $\displaystyle\frac{1}{q}\big{\&#124;}\boldsymbol{y}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(1)}\boldsymbol{y}_{t}\big{\&#124;}_{2}^{2}+\frac{\lambda_{1}}{k}\sum_{t\in\mathcal{N}_{i}^{+}}(w_{i,t}^{(1)})^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle w_{i,t}^{(1)}\geq 0,\quad t\in\mathcal{N}_{i}^{+},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda_{1}>0$ is the regularization parameter. Let $\boldsymbol{w}_{i}^{(2)}=[w_{i,t}^{(2)},\forall
    t\in\mathcal{N}_{i}^{+}]^{\top}\in\mathbb{R}^{k}$ denote the coefficients for
    linear reconstruction of $\boldsymbol{x}_{i}$ by its $k$ nearest neighbors. It
    is obtained as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{w}_{i}^{(2)}}{\text{minimize}}$
    |  | $\displaystyle\big{\&#124;}\boldsymbol{x}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(2)}\boldsymbol{x}_{t}\big{\&#124;}_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle w_{i,t}^{(2)}\geq 0,\quad t\in\mathcal{N}_{i}^{+}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: These two optimization problems are quadratic programming and can be solved
    using the interior point method (Ghojogh et al., [2021c](#bib.bib48)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The main optimization problem of GMML for partial labels is (Zhou & Gu, [2018](#bib.bib145)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    |  | (73) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\sum_{i=1}^{n}\Bigg{(}\frac{\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(1)}(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})^{\top}}{\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(1)}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\Big{(}\boldsymbol{x}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(2)}\boldsymbol{x}_{t}\Big{)}\Big{(}\boldsymbol{x}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(2)}\boldsymbol{x}_{t}\Big{)}^{\top}\Bigg{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\sum_{i=1}^{n}\sum_{t\in\mathcal{N}_{i}^{-}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})^{\top}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Minimizing the first term of $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$ in
    $\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})$ decreases
    the distances of similar points which share some candidate labels. Minimizing
    the second term of $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$ in $\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})$
    tries to preserve linear reconstruction of $\boldsymbol{x}_{i}$ by its neighbors
    after projection onto the subspace of metric. Minimizing $\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    increases the the distances of dissimilar points which do not share any candidate
    labels. The problem ([73](#S3.E73 "In 3.7.3 Geometric Mean Metric Learning for
    Partial Labels ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is
    similar to the problem ([62](#S3.E62 "In 3.7.1 Geometric Mean Metric Learning
    ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")); hence, its solution
    is similar to Eq. ([65](#S3.E65 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7
    Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), i.e., $\boldsymbol{W}={\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}}^{-1}\sharp_{(1/2)}\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}$
    (see Eq. ([64](#S3.E64 "In Lemma 5 ((Bhatia, 2007, Chapter 6)). ‣ 3.7.1 Geometric
    Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    for the definition of $\sharp_{t}$).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.4 Geometric Mean Metric Learning on SPD and Grassmannian Manifolds
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The GMML method (Zadeh et al., [2016](#bib.bib141)), introduced in Section
    [3.7.1](#S3.SS7.SSS1 "3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey"), can be implemented on Symmetric Positive
    Definite (SPD) and Grassmannian manifolds (Zhu et al., [2018](#bib.bib146)). If
    $\boldsymbol{X}_{i},\boldsymbol{X}_{j}\in\mathcal{S}_{++}^{d}$ is a point on the
    SPD manifold, the distance metric on this manifold is (Zhu et al., [2018](#bib.bib146)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle d_{\boldsymbol{W}}(\boldsymbol{T}_{i},\boldsymbol{T}_{j}):=\textbf{tr}\big{(}\boldsymbol{W}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})\big{)},$
    |  | (74) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{W}\in\mathbb{R}^{d\times d}$ is the weight matrix of metric
    and $\boldsymbol{T}_{i}:=\log(\boldsymbol{X}_{i})$ is the logarithm operation
    on the SPD manifold. The Grassmannian manifold $Gr(k,d)$ is the $k$-dimensional
    subspaces of the $d$-dimensional vector space. A point in $Gr(k,d)$ is a linear
    subspace spanned by a full-rank $\boldsymbol{X}_{i}\in\mathbb{R}^{d\times k}$
    which is orthogonal, i.e., $\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}=\boldsymbol{I}$.
    If $\boldsymbol{M}\in\mathbb{R}^{d\times r}$ is any matrix, We define $\boldsymbol{X}^{\prime}_{i}$
    in a way that $\boldsymbol{M}^{\top}\boldsymbol{X}^{\prime}_{i}$ is the orthogonal
    components of $\boldsymbol{M}^{\top}\boldsymbol{X}_{i}$. If $\mathbb{R}^{d\times
    d}\ni\boldsymbol{T}_{ij}:=\boldsymbol{X}^{\prime}_{i}\boldsymbol{X}^{{}^{\prime}\top}_{i}-\boldsymbol{X}^{\prime}_{j}\boldsymbol{X}^{{}^{\prime}\top}_{j}$,
    the distance on the Grassmannian manifold is (Zhu et al., [2018](#bib.bib146)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle d_{\boldsymbol{W}}(\boldsymbol{T}_{ij}):=\textbf{tr}\big{(}\boldsymbol{W}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}\big{)},$
    |  | (75) |'
  prefs: []
  type: TYPE_TB
- en: $\boldsymbol{W}\in\mathbb{R}^{d\times d}$ is the weight matrix of metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the optimization problem of GMML, i.e. Eq. ([61](#S3.E61 "In 3.7.1
    Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), we solve the following problem for the SPD manifold:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{W}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})\big{)}$
    |  | (76) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{W}^{-1}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Likewise, for the Grassmannian manifold, the optimization problem is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{W}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}\big{)}$
    |  | (77) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{W}^{-1}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Suppose, for the SPD manifold, we define:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'and, for the Grassmannian manifold, we define:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Hence, for either SPD or Grassmannian manifold, the optimization problem becomes
    Eq. ([62](#S3.E62 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) in which $\boldsymbol{\Sigma}_{\mathcal{S}}$
    and $\boldsymbol{\Sigma}_{\mathcal{D}}$ are replaced with $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$
    and $\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}$, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.5 Metric Learning on Stiefel and SPD Manifolds
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'According to Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), the weight matrix in the metric can be
    decomposed as $\boldsymbol{W}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}$.
    If we do not restrict $\boldsymbol{V}$ and $\boldsymbol{\Lambda}$ to be the matrices
    of eigenvectors and eigenvalues as in Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized
    Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), we can learn both $\boldsymbol{V}\in\mathbb{R}^{d\times
    p}$ and $\boldsymbol{\Lambda}\in\mathbb{R}^{p\times p}$ by optimization (Harandi
    et al., [2017](#bib.bib60)). The optimization problem in this method is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{V},\boldsymbol{\Lambda}}{\text{minimize}}$
    |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\log(1+q_{ij})$
    |  | (78) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\log(1+q_{ij}^{-1})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\lambda\Big{(}\textbf{tr}(\boldsymbol{\Lambda}\boldsymbol{\Lambda}_{0}^{-1})-\log\big{(}\textbf{det}(\boldsymbol{\Lambda}\boldsymbol{\Lambda}_{0}^{-1})\big{)}-p\Big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\boldsymbol{\Lambda}\succeq\boldsymbol{0},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda>0$ is the regularization parameter, $\textbf{det}(.)$ denotes
    the determinant of matrix, and $q_{ij}$ models Gaussian distribution with the
    generalized Mahalanobis distance metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q_{ij}:=\exp(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The constraint $\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I}$ means that
    the matrix $\boldsymbol{V}$ belongs to the Stiefel manifold $\text{St}(p,d):=\{\boldsymbol{V}\in\mathbb{R}^{d\times
    p}|\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I}\}$ and the constraint $\boldsymbol{\Lambda}\succeq\boldsymbol{0}$
    means $\boldsymbol{\Lambda}$ belongs to the SPD manifold $\mathcal{S}^{p}_{++}$.
    Hence, these two variables belong to the product manifold $\text{St}(p,d)\times\mathcal{S}^{p}_{++}$.
    Hence, we can solve this optimization problem using Riemannian optimization methods
    (Absil et al., [2009](#bib.bib1)). This method can also be kernelized; the reader
    can refer to (Harandi et al., [2017](#bib.bib60), Section 4) for its kernel version.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.6 Curvilinear Distance Metric Learning (CDML)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lemma 6  ((Chen et al., [2019](#bib.bib18))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The generalized Mahalanobis distance can be restated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\sum_{l=1}^{p}\&#124;\boldsymbol{u}_{l}\&#124;_{2}^{2}\Big{(}\int_{T_{l}(\boldsymbol{x}_{i})}^{T_{l}(\boldsymbol{x}_{j})}\&#124;\boldsymbol{u}_{l}\&#124;_{2}\,dt\Big{)}^{2},$
    |  | (79) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{u}_{l}\in\mathbb{R}^{d}$ is the $l$-th column of $\boldsymbol{U}$
    in Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")), $t\in\mathbb{R}$, and $T_{l}(\boldsymbol{x})\in\mathbb{R}$
    is the projection of $\boldsymbol{x}$ satisfying $(\boldsymbol{u}_{l}T_{l}(\boldsymbol{x})-\boldsymbol{x})^{\top}\boldsymbol{u}_{l}=0$.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(\ref{equation_W_U_UT})}{=}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})=\&#124;\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\&#124;[\boldsymbol{u}_{1}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}),\dots,\boldsymbol{u}_{p}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})]^{\top}\&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\sum_{l=1}^{p}\big{(}\boldsymbol{u}_{l}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(a)}{=}\sum_{l=1}^{p}\&#124;\boldsymbol{u}_{l}\&#124;_{2}^{2}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{2}^{2}\cos^{2}(\boldsymbol{u}_{l},\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(b)}{=}\sum_{l=1}^{p}\&#124;\boldsymbol{u}_{l}\&#124;_{2}^{2}\&#124;\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{i})-\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{j})\&#124;_{2}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because of the law of cosines and $(b)$ is because of $(\boldsymbol{u}_{l}T_{l}(\boldsymbol{x})-\boldsymbol{x})^{\top}\boldsymbol{u}_{l}=0$.
    The distance $\|\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{i})-\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{j})\|_{2}$
    can be replaced by the length of the arc between $T_{l}(\boldsymbol{x}_{i})$ and
    $T_{l}(\boldsymbol{x}_{j})$ on the straight line $\boldsymbol{u}_{l}t$ for $t\in\mathbb{R}$.
    This gives the Eq. ([79](#S3.E79 "In Lemma 6 ((Chen et al., 2019)). ‣ 3.7.6 Curvilinear
    Distance Metric Learning (CDML) ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). Q.E.D. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: 'The condition $(\boldsymbol{u}_{l}T_{l}(\boldsymbol{x})-\boldsymbol{x})^{\top}\boldsymbol{u}_{l}=0$
    is equivalent to finding the nearest neighbor to the line $\boldsymbol{u}_{l}t,\forall
    t\in\mathbb{R}$, i.e., $T_{l}(\boldsymbol{x}):=\arg\min_{t\in\mathbb{R}}\|\boldsymbol{u}_{l}t-\boldsymbol{x}\|_{2}^{2}$
    (Chen et al., [2019](#bib.bib18)). This equation can be generalized to find the
    nearest neighbor to the geodesic curve $\boldsymbol{\theta}_{l}(t)$ rather than
    the line $\boldsymbol{u}_{l}t$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle T_{\boldsymbol{\theta}_{l}}(\boldsymbol{x}):=\arg\min_{t\in\mathbb{R}}\&#124;\boldsymbol{\theta}_{l}(t)-\boldsymbol{x}\&#124;_{2}^{2}.$
    |  | (80) |'
  prefs: []
  type: TYPE_TB
- en: 'Hence, we can replace the arc length of the straight line in Eq. ([79](#S3.E79
    "In Lemma 6 ((Chen et al., 2019)). ‣ 3.7.6 Curvilinear Distance Metric Learning
    (CDML) ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) with
    the arc length of the curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\sum_{l=1}^{p}\alpha_{l}\Big{(}\int_{T_{\boldsymbol{\theta}_{l}}(\boldsymbol{x}_{i})}^{T_{\boldsymbol{\theta}_{l}}(\boldsymbol{x}_{j})}\&#124;\boldsymbol{\theta}^{\prime}_{l}(t)\&#124;_{2}\,dt\Big{)}^{2},$
    |  | (81) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{\theta}^{\prime}_{l}(t)$ is derivative of $\boldsymbol{\theta}_{l}(t)$
    w.r.t. $t$ and $\alpha_{l}:=(\int_{0}^{1}\|\boldsymbol{\theta}^{\prime}_{l}(t)\|_{2}\,dt)^{2}$
    is the scale factor. The Curvilinear Distance Metric Learning (CDML) (Chen et al.,
    [2019](#bib.bib18)) uses this approximation of distance metric by the above curvy
    geodesic on manifold, i.e., Eq. ([81](#S3.E81 "In 3.7.6 Curvilinear Distance Metric
    Learning (CDML) ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")). The
    optimization problem in CDML is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{\Theta}}{\text{minimize}}$ |  |
    $\displaystyle\frac{1}{n}\sum_{i=1}^{n}\mathcal{L}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2};y_{ij})+\lambda\Omega(\boldsymbol{\Theta}),$
    |  | (82) |'
  prefs: []
  type: TYPE_TB
- en: 'where $n$ is the number of points, $\boldsymbol{\Theta}:=[\boldsymbol{\theta}_{1},\dots,\boldsymbol{\theta}_{p}]$,
    $y_{ij}=1$ if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$ and $y_{ij}=0$
    if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}$, $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$
    is defined in Eq. ([81](#S3.E81 "In 3.7.6 Curvilinear Distance Metric Learning
    (CDML) ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), $\lambda>0$
    is the regularization parameter, $\mathcal{L}(.)$ is some loss function, and $\Omega(\boldsymbol{\Theta})$
    is some penalty term. The optimal $\boldsymbol{\Theta}$, obtained from Eq. ([82](#S3.E82
    "In 3.7.6 Curvilinear Distance Metric Learning (CDML) ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), can be used in Eq. ([81](#S3.E81 "In
    3.7.6 Curvilinear Distance Metric Learning (CDML) ‣ 3.7 Geometric Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) to have the optimal distance metric. A recent
    follow-up of CDML is (Zhang et al., [2021](#bib.bib143)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.8 Adversarial Metric Learning (AML)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial Metric Learning (AML) (Chen et al., [2018](#bib.bib17)) uses adversarial
    learning (Goodfellow et al., [2014](#bib.bib54); Ghojogh et al., [2021b](#bib.bib47))
    for metric learning. On one hand, we have a distinguishment stage which tries
    to discriminate the dissimilar points and push similar points close to one another.
    On the other hand, we have an confusion or adversarial stage which tries to fool
    the metric learning method by pulling the dissimilar points close to each other
    and pushing the similar points away. The distinguishment and confusion stages
    are trained simultaneously and they make each other stronger gradually.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the dataset, we form random pairs $\mathcal{X}:=\{(\boldsymbol{x}_{i},\boldsymbol{x}^{\prime}_{i})\}_{i=1}^{n/2}$.
    If $\boldsymbol{x}_{i}$ and $\boldsymbol{x}^{\prime}_{i}$ are similar points,
    we set $y_{i}=1$ and if they are dissimilar, we have $y_{i}=-1$. We also generate
    some random new points in pairs $\mathcal{X}^{g}:=\{(\boldsymbol{x}_{i}^{g},\boldsymbol{x}^{g^{\prime}}_{i})\}_{i=1}^{n/2}$.
    The generated points are updated iteratively by optimization of the confusion
    stage to fool the metric. The loss functions for both stages are Eq. ([61](#S3.E61
    "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) used in geometric mean metric learning (see Section [3.7.1](#S3.SS7.SSS1
    "3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The alternative optimization (Ghojogh et al., [2021c](#bib.bib48)) used in
    AML is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\boldsymbol{W}^{(t+1)}:=\arg\min_{\boldsymbol{W}}\Big{(}\sum_{y_{i}=1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{y_{i}=-1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{\boldsymbol{W}^{-1}}^{2}+\lambda_{1}\big{(}\sum_{y_{i}=1}\&#124;\boldsymbol{x}^{g(t)}_{i}-\boldsymbol{x}^{g^{\prime}(t)}_{i}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{y_{i}=-1}\&#124;\boldsymbol{x}^{g(t)}_{i}-\boldsymbol{x}^{g^{\prime}(t)}_{i}\&#124;_{\boldsymbol{W}^{-1}}^{2}\big{)}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\mathcal{X}}^{g(t+1)}:=\arg\min_{\mathcal{X}^{\prime}}\Big{(}\sum_{y_{i}=-1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{\boldsymbol{W}^{(t+1)}}^{2}$
    |  | (83) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{y_{i}=1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{(\boldsymbol{W}^{(t+1)})^{-1}}^{2}+\lambda_{2}\big{(}\sum_{i=1}^{n/2}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{g}_{i}\&#124;_{\boldsymbol{W}^{(t+1)}}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{i=1}^{n/2}\&#124;\boldsymbol{x}^{\prime}_{i}-\boldsymbol{x}^{g^{\prime}}_{i}\&#124;_{\boldsymbol{W}^{(t+1)}}^{2}\big{)}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: until convergence, where $\lambda_{1},\lambda_{2}>0$ are the regularization
    parameters. Updating $\boldsymbol{W}$ and $\mathcal{X}^{g}$ are the distinguishment
    and confusion stages, respectively. In the distinguishment stage, we find a weight
    matrix $\boldsymbol{W}$ to minimize the distances of similar points in both $\mathcal{X}$
    and $\mathcal{X}^{g}$ and maximize the distances of dissimilar points in both
    $\mathcal{X}$ and $\mathcal{X}^{g}$. In the confusion stage, we generate new points
    $\mathcal{X}^{g}$ to adversarially maximize the distances of similar points in
    $\mathcal{X}$ and adversarially minimize the distances of dissimilar points in
    $\mathcal{X}$. In this stage, we also make the points $\boldsymbol{x}_{i}^{g}$
    and $\boldsymbol{x}_{i}^{g^{\prime}}$ similar to their corresponding points $\boldsymbol{x}_{i}$
    and $\boldsymbol{x}^{\prime}_{i}$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Probabilistic Metric Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Probabilistic methods for metric learning learn the weight matrix in the generalized
    Mahalanobis distance using probability distributions. They define some probability
    distribution for each point accepting other points as its neighbors. Of course,
    the closer points have higher probability for being neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Collapsing Classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One probabilistic method for metric learning is collapsing similar points to
    the same class while pushing the dissimilar points away from one another (Globerson
    & Roweis, [2005](#bib.bib52)). The probability distribution between points for
    being neighbors can be a Gaussian distribution which uses the generalized Mahalanobis
    distance as its metric. The distribution for $\boldsymbol{x}_{i}$ to take $\boldsymbol{x}_{j}$
    as its neighbor is (Goldberger et al., [2005](#bib.bib53)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p^{W}_{ij}:=\frac{\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})}{\sum_{k\neq
    i}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\&#124;_{\boldsymbol{W}}^{2})},\quad
    j\neq i,$ |  | (84) |'
  prefs: []
  type: TYPE_TB
- en: 'where we define the normalization factor, also called the partition function,
    as $Z_{i}:=\sum_{k\neq i}\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\|_{\boldsymbol{W}}^{2})$.
    This factor makes the summation of distribution one. Eq. ([84](#S4.E84 "In 4.1
    Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) is a Gaussian distribution whose
    covariance matrix is $\boldsymbol{W}^{-1}$ because it is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p^{W}_{ij}:=\frac{1}{Z_{i}}\exp\big{(}-(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'We want the similar points to collapse to the same point after projection onto
    the subspace of metric (see Proposition [2](#Thmproposition2 "Proposition 2 (Projection
    in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). Hence, we define the desired neighborhood distribution to be a
    bi-level distribution (Globerson & Roweis, [2005](#bib.bib52)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p^{0}_{ij}:=\left\{\begin{array}[]{ll}1&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\
    0&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$
    |  | (87) |'
  prefs: []
  type: TYPE_TB
- en: This makes all similar points of a group/class a same point after projection.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Collapsing Classes in the Input Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For making $p^{W}_{ij}$ close to the desired distribution $p^{0}_{ij}$, we
    minimize the KL-divergence between them (Globerson & Roweis, [2005](#bib.bib52)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1,j\neq
    i}^{n}\text{KL}(p^{0}_{ij}\,\&#124;\,p^{W}_{ij})$ |  | (88) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Lemma 7  ((Globerson & Roweis, [2005](#bib.bib52))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let the the objective function in Eq. ([88](#S4.E88 "In 4.1.1 Collapsing Classes
    in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) be
    denoted by $c:=\sum_{i=1}^{n}\sum_{j=1,j\neq i}^{n}\text{KL}(p^{0}_{ij}\,\|\,p^{W}_{ij})$.
    The gradient of this function w.r.t. $\boldsymbol{W}$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{W}}=\sum_{i=1}^{n}\sum_{j=1,j\neq
    i}^{n}(p^{0}_{ij}-p^{W}_{ij})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}.$
    |  | (89) |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The derivation is similar to the derivation of gradient in Stochastic Neighbor
    Embedding (SNE) and t-SNE (Hinton & Roweis, [2003](#bib.bib66); van der Maaten
    & Hinton, [2008](#bib.bib113); Ghojogh et al., [2020c](#bib.bib42)). Let:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{R}\ni r_{ij}:=d_{ij}^{2}=&#124;&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}&#124;&#124;_{\boldsymbol{W}}^{2}.$
    |  | (90) |'
  prefs: []
  type: TYPE_TB
- en: 'By changing $\boldsymbol{x}_{i}$, we only have change impact in $d_{ij}$ and
    $d_{ji}$ (or $r_{ij}$ and $r_{ji}$) for all $j$’s. According to chain rule, we
    have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{W}}=\sum_{i,j}\big{(}\frac{\partial
    c}{\partial r_{ij}}\frac{\partial r_{ij}}{\partial\boldsymbol{W}}+\frac{\partial
    c}{\partial r_{ji}}\frac{\partial r_{ji}}{\partial\boldsymbol{W}}\big{)}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'According to Eq. ([90](#S4.E90 "In Proof. ‣ 4.1.1 Collapsing Classes in the
    Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle r_{ij}=&#124;&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}&#124;&#124;_{\boldsymbol{W}}^{2}=\textbf{tr}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}\textbf{tr}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies\frac{\partial r_{ij}}{\partial\boldsymbol{W}}=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle r_{ji}=&#124;&#124;\boldsymbol{x}_{j}-\boldsymbol{x}_{i}&#124;&#124;_{\boldsymbol{W}}^{2}=&#124;&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}&#124;&#124;_{\boldsymbol{W}}^{2}=r_{ij}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies\frac{\partial r_{ji}}{\partial\boldsymbol{W}}=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because of the cyclic property of trace. Therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\therefore~{}~{}~{}~{}\frac{\partial c}{\partial\boldsymbol{W}}=2\sum_{i,j}\big{(}\frac{\partial
    c}{\partial r_{ij}}\big{)}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}.$
    |  | (91) |'
  prefs: []
  type: TYPE_TB
- en: 'The dummy variables in cost function can be re-written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle c$ | $\displaystyle=\sum_{k}\sum_{l\neq k}p_{0}(l&#124;k)\log(\frac{p_{0}(l&#124;k)}{p_{W}(l&#124;k)})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{k\neq l}p_{0}(l&#124;k)\log(\frac{p_{0}(l&#124;k)}{p_{W}(l&#124;k)})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{k\neq l}\big{(}p_{0}(l&#124;k)\log(p_{0}(l&#124;k))-p_{0}(l&#124;k)\log(p_{W}(l&#124;k))\big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'whose first term is a constant with respect to $p_{W}(l|k)$ and thus to $\boldsymbol{W}$.
    We have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{R}\ni\frac{\partial c}{\partial r_{ij}}=-\sum_{k\neq
    l}p_{0}(l&#124;k)\frac{\partial(\log(p_{W}(l&#124;k)))}{\partial r_{ij}}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'According to Eqs. ([84](#S4.E84 "In 4.1 Collapsing Classes ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) and ([90](#S4.E90 "In Proof. ‣ 4.1.1 Collapsing Classes in the Input
    Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), the $p_{W}(l|k)$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{W}(l&#124;k):=\frac{\exp(-d_{kl}^{2})}{\sum_{k\neq f}\exp(-d_{kf}^{2})}=\frac{\exp(-r_{kl})}{\sum_{k\neq
    f}\exp(-r_{kf})}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'We take the denominator of $p_{W}(l|k)$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\beta:=\sum_{k\neq f}\exp(-d_{kf}^{2})=\sum_{k\neq f}\exp(-r_{kf}).$
    |  | (92) |'
  prefs: []
  type: TYPE_TB
- en: 'We have $\log(p_{W}(l|k))=\log(p_{W}(l|k))+\log\beta-\log\beta=\log(p_{W}(l|k)\,\beta)-\log\beta$.
    Therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\therefore~{}~{}~{}\frac{\partial c}{\partial r_{ij}}=-\sum_{k\neq
    l}p_{0}(l&#124;k)\frac{\partial\big{(}\log(p_{W}(l&#124;k)\beta)-\log\beta\big{)}}{\partial
    r_{ij}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=-\sum_{k\neq l}p_{0}(l&#124;k)\bigg{[}\frac{\partial\big{(}\log(p_{W}(l&#124;k)\beta)\big{)}}{\partial
    r_{ij}}-\frac{\partial\big{(}\log\beta\big{)}}{\partial r_{ij}}\bigg{]}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=-\sum_{k\neq l}p_{0}(l&#124;k)\bigg{[}\frac{1}{p_{W}(l&#124;k)\beta}\frac{\partial\big{(}p_{W}(l&#124;k)\beta\big{)}}{\partial
    r_{ij}}-\frac{1}{\beta}\frac{\partial\beta}{\partial r_{ij}}\bigg{]}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The $p_{W}(l|k)\beta$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{W}(l&#124;k)\beta$ | $\displaystyle=\frac{\exp(-r_{kl})}{\sum_{f\neq
    k}\exp(-r_{kf})}\times\sum_{k\neq f}\exp(-r_{kf})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\exp(-r_{kl}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Therefore, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\therefore~{}~{}~{}\frac{\partial c}{\partial r_{ij}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle-\sum_{k\neq l}p_{0}(l&#124;k)\bigg{[}\frac{1}{p_{W}(l&#124;k)\beta}\frac{\partial\big{(}\exp(-r_{kl})\big{)}}{\partial
    r_{ij}}-\frac{1}{\beta}\frac{\partial\beta}{\partial r_{ij}}\bigg{]}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The $\partial\big{(}\exp(-r_{kl})\big{)}/\partial r_{ij}$ is non-zero for only
    $k=i$ and $l=j$; therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial\big{(}\exp(-r_{ij})\big{)}}{\partial r_{ij}}$
    | $\displaystyle=-\exp(-r_{ij}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\frac{\partial\beta}{\partial r_{ij}}$ | $\displaystyle=\frac{\partial\sum_{k\neq
    f}\exp(-r_{kf})}{\partial r_{ij}}=\frac{\partial\exp(-r_{ij})}{\partial r_{ij}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\exp(-r_{ij}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\therefore~{}~{}~{}\frac{\partial c}{\partial r_{ij}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle-\bigg{(}p^{0}_{ij}\Big{[}\frac{-1}{p^{W}_{ij}\beta}\exp(-r_{ij})\Big{]}+0+\dots+0\bigg{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle-\sum_{k\neq l}p_{0}(l&#124;k)\Big{[}\frac{1}{\beta}\exp(-r_{ij})\Big{]}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'We have $\sum_{k\neq l}p_{0}(l|k)=1$ because summation of all possible probabilities
    is one. Thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial c}{\partial r_{ij}}$ | $\displaystyle=-p^{0}_{ij}\Big{[}\frac{-1}{p^{W}_{ij}\beta}\exp(-r_{ij})\Big{]}-\Big{[}\frac{1}{\beta}\exp(-r_{ij})\Big{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\underbrace{\frac{\exp(-r_{ij})}{\beta}}_{=p^{W}_{ij}}\Big{[}\frac{p^{0}_{ij}}{p^{W}_{ij}}-1\Big{]}=p^{0}_{ij}-p^{W}_{ij}.$
    |  | (93) |'
  prefs: []
  type: TYPE_TB
- en: 'Substituting the obtained derivative in Eq. ([91](#S4.E91 "In Proof. ‣ 4.1.1
    Collapsing Classes in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) gives Eq. ([89](#S4.E89 "In Lemma 7 ((Globerson & Roweis, 2005)).
    ‣ 4.1.1 Collapsing Classes in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). Q.E.D. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization problem ([88](#S4.E88 "In 4.1.1 Collapsing Classes in the
    Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is convex; hence,
    it has a unique solution. We can solve it using any optimization method such as
    the projected gradient method, where after every gradient descent step, we project
    the solution onto the positive semi-definite cone (Ghojogh et al., [2021c](#bib.bib48)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{W}-\eta\frac{\partial c}{\partial\boldsymbol{W}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{V}\,\textbf{diag}(\max(\lambda_{1},0),\dots,\max(\lambda_{d},0))\,\boldsymbol{V}^{\top},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\eta>0$ is the learning rate and $\boldsymbol{V}$ and $\boldsymbol{\Lambda}=\textbf{diag}(\lambda_{1},\dots,\lambda_{d})$
    are the eigenvectors and eigenvalues of $\boldsymbol{W}$, respectively (see Eq.
    ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Collapsing Classes in the Feature Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'According to Eq. ([54](#S3.E54 "In Lemma 4\. ‣ 3.6.4 Kernel Discriminative
    Component Analysis ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), the
    distance in the feature space can be stated using kernels as $\|\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\|_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$
    where $\boldsymbol{k}_{i}\in\mathbb{R}^{n}$ is the kernel vector between dataset
    $\boldsymbol{X}$ and the point $\boldsymbol{x}_{i}$. We define $\boldsymbol{R}:=\boldsymbol{T}\boldsymbol{T}^{\top}\in\mathbb{R}^{n\times
    n}$. Hence, in the feature space, Eq. ([84](#S4.E84 "In 4.1 Collapsing Classes
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p^{R}_{ij}:=\frac{\exp(-\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\&#124;_{\boldsymbol{R}}^{2})}{\sum_{k\neq
    i}\exp(-\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{k}\&#124;_{\boldsymbol{R}}^{2})},\quad
    j\neq i.$ |  | (94) |'
  prefs: []
  type: TYPE_TB
- en: 'The gradient in Eq. ([89](#S4.E89 "In Lemma 7 ((Globerson & Roweis, 2005)).
    ‣ 4.1.1 Collapsing Classes in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{R}}=\sum_{i=1}^{n}\sum_{j=1,j\neq
    i}^{n}(p^{0}_{ij}-p^{R}_{ij})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}.$
    |  | (95) |'
  prefs: []
  type: TYPE_TB
- en: Again, we can find the optimal $\boldsymbol{R}$ using projected gradient method.
    This gives us the optimal metric for collapsing classes in the feature space (Globerson
    & Roweis, [2005](#bib.bib52)). Note that we can also regularize the objective
    function, using the trace operator or Frobenius norm, for avoiding overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Neighborhood Component Analysis Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neighborhood Component Analysis (NCA) is one of the most well-known probabilistic
    metric learning methods. In the following, we introduce different variants of
    NCA.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Neighborhood Component Analysis (NCA)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the original NCA (Goldberger et al., [2005](#bib.bib53)), the probability
    that $\boldsymbol{x}_{j}$ takes $\boldsymbol{x}_{i}$ as its neighbor is as in
    Eq. ([84](#S4.E84 "In 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), where
    we assume $p^{W}_{ii}=0$ by convention:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p^{W}_{ij}:=\left\{\begin{array}[]{ll}\frac{\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})}{\sum_{k\neq
    i}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\&#124;_{\boldsymbol{W}}^{2})}&amp;\mbox{if
    }j\neq i\\ 0&amp;\mbox{if }j=i.\end{array}\right.$ |  | (98) |'
  prefs: []
  type: TYPE_TB
- en: 'Consider the decomposition of the weight matrix of metric as in Eq. ([9](#S2.E9
    "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), i.e., $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$. Let
    $\mathcal{S}_{i}$ denote the set of similar points to $\boldsymbol{x}_{i}$ where
    $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$. The optimization problem
    of NCA is to find a $\boldsymbol{U}$ to maximize this probability distribution
    for similar points (Goldberger et al., [2005](#bib.bib53)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}=\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}=\sum_{i=1}^{n}p^{W}_{i},$
    |  | (99) |'
  prefs: []
  type: TYPE_TB
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p^{W}_{i}:=\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}.$
    |  | (100) |'
  prefs: []
  type: TYPE_TB
- en: 'Note that the required constraint $\boldsymbol{W}\succeq\boldsymbol{0}$ is
    already satisfied because of the decomposition in Eq. ([84](#S4.E84 "In 4.1 Collapsing
    Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8  ((Goldberger et al., [2005](#bib.bib53))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Suppose the objective function of Eq. ([99](#S4.E99 "In 4.2.1 Neighborhood
    Component Analysis (NCA) ‣ 4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) is denoted by $c$. The gradient of this cost function w.r.t. $\boldsymbol{U}$
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}=$ | $\displaystyle\,2\sum_{i=1}^{n}\Big{(}p^{W}_{i}\sum_{k=1}^{n}p^{W}_{ik}(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})^{\top}$
    |  | (101) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\Big{)}\boldsymbol{U}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The derivation of this gradient is similar to the approach in the proof of
    Lemma [7](#Thmlemma7 "Lemma 7 ((Globerson & Roweis, 2005)). ‣ 4.1.1 Collapsing
    Classes in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"). We
    can use gradient ascent for solving the optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach is to maximize the log-likelihood of neighborhood probability
    (Goldberger et al., [2005](#bib.bib53)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}\Big{)},$
    |  | (102) |'
  prefs: []
  type: TYPE_TB
- en: 'whose gradient is (Goldberger et al., [2005](#bib.bib53)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}=$ | $\displaystyle\,2\sum_{i=1}^{n}\Big{(}\sum_{k=1}^{n}p^{W}_{ik}(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})^{\top}$
    |  | (103) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\frac{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}}{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}}\Big{)}\boldsymbol{U}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Again, gradient ascent can give us the optimal $\boldsymbol{U}$. As explained
    in Proposition [2](#Thmproposition2 "Proposition 2 (Projection in metric learning).
    ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"), the
    subspace is metric is the column space of $\boldsymbol{U}$ and projection of points
    onto this subspace reduces the dimensionality of data.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Regularized Neighborhood Component Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is shown by some experiments that NCA can overfit to training data for high-dimensional
    data (Yang & Laaksonen, [2007](#bib.bib138)). Hence, we can regularize it to avoid
    overfitting. In regularized NCA (Yang & Laaksonen, [2007](#bib.bib138)), we use
    the log-posterior of the matrix $\boldsymbol{U}$ which is equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}(\boldsymbol{U}&#124;\boldsymbol{x}_{i},\mathcal{S}_{i})=\frac{\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i}&#124;\boldsymbol{U})\,\mathbb{P}(\boldsymbol{U})}{\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i})},$
    |  | (104) |'
  prefs: []
  type: TYPE_TB
- en: 'according to the Bayes’ rule. We can use Gaussian distribution for the prior:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}(\boldsymbol{U})=\prod_{k=1}^{d}\prod_{l=1}^{d}c\,\exp(-\lambda(\boldsymbol{U}(k,l))^{2}),$
    |  | (105) |'
  prefs: []
  type: TYPE_TB
- en: 'where $c>0$ is a constant factor including the normalization factor, $\lambda>0$
    is the inverse of variance, and $\boldsymbol{U}(k,l)$ is the $(k,l)$-th element
    of $\boldsymbol{U}\in\mathbb{R}^{d\times d}$. Note that we can have $\boldsymbol{U}\in\mathbb{R}^{d\times
    p}$ if we truncate it to have $p$ leading eigenvectors of $\boldsymbol{W}$ (see
    Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))). The likelihood'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i}&#124;\boldsymbol{U})\propto\exp\Big{(}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}\Big{)}.$
    |  | (106) |'
  prefs: []
  type: TYPE_TB
- en: 'The regularized NCA maximizes the log-posterior (Yang & Laaksonen, [2007](#bib.bib138)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\log\mathbb{P}(\boldsymbol{U}&#124;\boldsymbol{x}_{i},\mathcal{S}_{i})\overset{(\ref{equation_regularized_NCA_posterior})}{=}\log\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i}&#124;\boldsymbol{U})+\log\mathbb{P}(\boldsymbol{U})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\underbrace{\log\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i})}_{\text{constant
    w.r.t. }\boldsymbol{U}}\overset{(a)}{=}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}-\lambda\&#124;\boldsymbol{U}\&#124;_{F}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because of Eqs. ([105](#S4.E105 "In 4.2.2 Regularized Neighborhood
    Component Analysis ‣ 4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) and ([106](#S4.E106 "In 4.2.2 Regularized Neighborhood Component
    Analysis ‣ 4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    and $\|.\|_{F}$ denotes the Frobenius norm. Hence, the optimization problem of
    regularized NCA is (Yang & Laaksonen, [2007](#bib.bib138)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}-\lambda\&#124;\boldsymbol{U}\&#124;_{F}^{2},$
    |  | (107) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda>0$ can be seen as the regularization parameter. The gradient
    is similar to Eq. ([101](#S4.E101 "In Lemma 8 ((Goldberger et al., 2005)). ‣ 4.2.1
    Neighborhood Component Analysis (NCA) ‣ 4.2 Neighborhood Component Analysis Methods
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) but plus the derivative of the regularization term which
    is $-2\lambda\boldsymbol{U}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Fast Neighborhood Component Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '– Fast NCA: The fast NCA (Yang et al., [2012](#bib.bib136)) accelerates NCA
    by using $k$-Nearest Neighbors ($k$NN) rather than using all points for computing
    the neighborhood distribution of every point. Let $\mathcal{N}_{i}$ and $\mathcal{M}_{i}$
    denote the $k$NN of $\boldsymbol{x}_{i}$ among the similar points to $\boldsymbol{x}_{i}$
    (denoted by $\mathcal{S}_{i}$) and dissimilar points (denoted by $\mathcal{D}_{i}$),
    respectively. Fast NCA uses following probability distribution for $\boldsymbol{x}_{i}$
    to take $\boldsymbol{x}_{i}$ as its neighbor (Yang et al., [2012](#bib.bib136)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle p^{W}_{ij}:=\left\{\begin{array}[]{ll}\frac{\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}})}{\sum_{\boldsymbol{x}_{k}\in\mathcal{N}_{i}\cup\mathcal{M}_{i}}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\&#124;_{\boldsymbol{W}})}&amp;\mbox{if
    }\boldsymbol{x}_{k}\in\mathcal{N}_{i}\cup\mathcal{M}_{i}\\ 0&amp;\mbox{otherwise.}\end{array}\right.$
    |  | (110) |'
  prefs: []
  type: TYPE_TB
- en: 'The optimization problem of fast NCA is similar to Eq. ([107](#S4.E107 "In
    4.2.2 Regularized Neighborhood Component Analysis ‣ 4.2 Neighborhood Component
    Analysis Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{M}_{i}}p^{W}_{ij}-\lambda\&#124;\boldsymbol{U}\&#124;_{F}^{2},$
    |  | (111) |'
  prefs: []
  type: TYPE_TB
- en: 'where $p^{W}_{ij}$ is Eq. ([110](#S4.E110 "In 4.2.3 Fast Neighborhood Component
    Analysis ‣ 4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    and $\boldsymbol{U}$ is the matrix in the decomposition of $\boldsymbol{W}$ (see
    Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))).'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 9  ((Yang et al., [2012](#bib.bib136))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Suppose the objective function of Eq. ([111](#S4.E111 "In 4.2.3 Fast Neighborhood
    Component Analysis ‣ 4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) is denoted by $c$. The gradient of this cost function w.r.t. $\boldsymbol{U}$
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}=\sum_{i=1}^{n}\Big{(}p^{W}_{i}\sum_{\boldsymbol{x}_{k}\in\mathcal{N}_{i}}p^{W}_{ik}(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})^{\top}$
    |  | (112) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+(p^{W}_{i}-1)\sum_{\boldsymbol{x}_{j}\in\mathcal{M}_{i}}p^{W}_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\Big{)}\boldsymbol{U}-2\lambda\boldsymbol{U}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'This is similar to Eq. ([101](#S4.E101 "In Lemma 8 ((Goldberger et al., 2005)).
    ‣ 4.2.1 Neighborhood Component Analysis (NCA) ‣ 4.2 Neighborhood Component Analysis
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")). See (Yang et al., [2012](#bib.bib136))
    for the derivation. We can use gradient ascent for solving the optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: '– Kernel Fast NCA: According to Eq. ([54](#S3.E54 "In Lemma 4\. ‣ 3.6.4 Kernel
    Discriminative Component Analysis ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), the distance in the feature space is $\|\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\|_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$
    where $\boldsymbol{k}_{i}\in\mathbb{R}^{n}$ is the kernel vector between dataset
    $\boldsymbol{X}$ and the point $\boldsymbol{x}_{i}$. We can use this distance
    metric in Eq. ([110](#S4.E110 "In 4.2.3 Fast Neighborhood Component Analysis ‣
    4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) to
    have kernel fast NCA (Yang et al., [2012](#bib.bib136)). Hence, the gradient of
    kernel fast NCA is similar to Eq. ([112](#S4.E112 "In Lemma 9 ((Yang et al., 2012)).
    ‣ 4.2.3 Fast Neighborhood Component Analysis ‣ 4.2 Neighborhood Component Analysis
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{T}}=\sum_{i=1}^{n}\Big{(}p^{W}_{i}\sum_{\boldsymbol{x}_{k}\in\mathcal{N}_{i}}p^{W}_{ik}(\boldsymbol{k}_{i}-\boldsymbol{k}_{k})(\boldsymbol{k}_{i}-\boldsymbol{k}_{k})^{\top}$
    |  | (113) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+(p^{W}_{i}-1)\sum_{\boldsymbol{x}_{j}\in\mathcal{M}_{i}}p^{W}_{ij}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\Big{)}\boldsymbol{T}-2\lambda\boldsymbol{T}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Again, we can find the optimal $\boldsymbol{T}$ using gradient ascent. Note
    that the same technique can be used to kernelize the original NCA.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Bayesian Metric Learning Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce the Bayesian metric learning methods which use
    variational inference (Ghojogh et al., [2021a](#bib.bib46)) for metric learning.
    In Bayesian metric learning, we learn a distribution for the distance metric between
    every two points; we sample the pairwise distances from these learned distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we provide some definition required in these methods. According to Eq.
    ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")), we can decompose the weight matrix in the metric using
    the eigenvalue decomposition. Accordingly, we can approximate this matrix by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{W}\approx\boldsymbol{V}_{x}\boldsymbol{\Lambda}\boldsymbol{V}_{x}^{\top},$
    |  | (114) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{V}_{x}$ contains the eigenvectors of $\boldsymbol{X}\boldsymbol{X}^{\top}$
    and $\boldsymbol{\Lambda}=\textbf{diag}([\lambda_{1},\dots,\lambda_{d}]^{\top})$
    is the diagonal matrix of eigenvalues which we learn in Bayesian metric learning.
    Let $X$ and $Y$ denote the random variables for data and labels, respectively,
    and let $\boldsymbol{\lambda}=[\lambda_{1},\dots,\lambda_{d}]^{\top}\in\mathbb{R}^{d}$
    denote the learnable eigenvalues. Let $\boldsymbol{v}_{x}^{l}\in\mathbb{R}^{d}$
    denote the $l$-th column of $\boldsymbol{V}_{x}$. We define $\boldsymbol{w}_{ij}=[w_{ij}^{1},\dots,w_{ij}^{d}]^{\top}:=[((\boldsymbol{v}_{x}^{1})^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}))^{2},\dots,((\boldsymbol{v}_{x}^{d})^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}))^{2}]^{\top}\in\mathbb{R}^{d}$.
    The reader should not confuse $\boldsymbol{w}_{ij}$ with $\boldsymbol{W}$ which
    is the weight matrix of metric in out notations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Bayesian Metric Learning Using Sigmoid Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the Bayesian metric learning methods is (Yang et al., [2007](#bib.bib134)).
    We define:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle y_{ij}:=\left\{\begin{array}[]{ll}1&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\
    -1&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$
    |  | (117) |'
  prefs: []
  type: TYPE_TB
- en: 'We can consider a sigmoid function for the likelihood (Yang et al., [2007](#bib.bib134)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}(Y&#124;X,\boldsymbol{\Lambda})=\frac{1}{1+\exp(y_{ij}(\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}-\mu))},$
    |  | (118) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mu>0$ is a threshold. We can also derive an evidence lower bound for
    $\mathbb{P}(\mathcal{S},\mathcal{D})$; we do not provide the derivation for brevity
    (see (Yang et al., [2007](#bib.bib134)) for derivation of the lower bound). As
    in the variational inference, we maximize this lower bound for likelihood maximization
    (Ghojogh et al., [2021a](#bib.bib46)). We assume a Gaussian distribution with
    mean $\boldsymbol{m}_{\lambda}\in\mathbb{R}^{d}$ and covariance $\boldsymbol{V}_{\lambda}\in\mathbb{R}^{d\times
    d}$ for the distribution $\mathbb{P}(\boldsymbol{\lambda})$. By maximizing the
    lower bound, we can estimate these parameters as (Yang et al., [2007](#bib.bib134)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{V}_{T}:=\Big{(}\delta\boldsymbol{I}+2\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\frac{\tanh(\xi_{ij}^{s})}{4\xi_{ij}^{s}}\boldsymbol{w}_{ij}\boldsymbol{w}_{ij}^{\top}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+2\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\frac{\tanh(\xi_{ij}^{d})}{4\xi_{ij}^{d}}\boldsymbol{w}_{ij}\boldsymbol{w}_{ij}^{\top}\Big{)}^{-1},$
    |  | (119) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{m}_{T}:=\boldsymbol{V}_{T}\Big{(}\delta\boldsymbol{\gamma}_{0}-\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\boldsymbol{w}_{ij}+\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\boldsymbol{w}_{ij}\Big{)},$
    |  | (120) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\delta>0$ and $\boldsymbol{\gamma}_{0}$ are hyper-parameters related
    to the priors on the weight matrix of metric and the threshold. We define the
    following variational parameter (Yang et al., [2007](#bib.bib134)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\xi_{ij}^{s}:=\sqrt{(\boldsymbol{m}_{T}^{\top}\boldsymbol{w}_{ij})^{2}+\boldsymbol{w}_{ij}^{\top}\boldsymbol{V}_{T}\boldsymbol{w}_{ij}},$
    |  | (121) |'
  prefs: []
  type: TYPE_TB
- en: 'for $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$. We similarly define
    the variational parameter $\xi_{ij}^{d}$ for $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}$.
    The variables $\boldsymbol{V}_{T}$, $\boldsymbol{m}_{T}$, $\xi_{ij}^{s}$, and
    $\xi_{ij}^{d}$ are updated iteratively by Eqs. ([123](#S4.E123 "In 4.3.2 Bayesian
    Neighborhood Component Analysis ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), ([124](#S4.E124 "In 4.3.2 Bayesian Neighborhood Component Analysis
    ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), and ([121](#S4.E121
    "In 4.3.1 Bayesian Metric Learning Using Sigmoid Function ‣ 4.3 Bayesian Metric
    Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), respectively, until convergence.
    After these parameters are learned, we can sample the eigenvalues from the posterior,
    $\boldsymbol{\lambda}\sim\mathcal{N}(\boldsymbol{m}_{T},\boldsymbol{V}_{T})$.
    These eigenvalues can be used in Eq. ([114](#S4.E114 "In 4.3 Bayesian Metric Learning
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) to obtain the weight matrix in the metric.
    Note that Bayesian metric learning can also be used for active learning (see (Yang
    et al., [2007](#bib.bib134)) for details).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Bayesian Neighborhood Component Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Bayesian NCA (Wang & Tan, [2017](#bib.bib115)) using variational inference
    (Ghojogh et al., [2021a](#bib.bib46)) in the NCA formulation. If $\mathcal{N}_{im}$
    denotes the dataset index of the $m$-th nearest neighbor of $\boldsymbol{x}_{i}$,
    we define $\boldsymbol{W}_{i}^{j}:=[w_{ij}-w_{i\mathcal{N}_{i1}},\dots,w_{ij}-w_{i\mathcal{N}_{ik}}]\in\mathbb{R}^{d\times
    k}$. As in the variational inference (Ghojogh et al., [2021a](#bib.bib46)), we
    consider an evidence lower-bound on the log-likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\log(\mathbb{P}(Y&#124;X,\boldsymbol{\Lambda}))>\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{N}_{i}}\Big{(}$
    | $\displaystyle-\frac{1}{2}\boldsymbol{\lambda}^{\top}\boldsymbol{W}_{i}^{j}\boldsymbol{H}(\boldsymbol{W}_{i}^{j})^{\top}\boldsymbol{\lambda}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\boldsymbol{b}_{ij}^{\top}(\boldsymbol{W}_{i}^{j})^{\top}\boldsymbol{\lambda}-c_{ij}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{N}_{i}$ was defined before in Section [4.2.3](#S4.SS2.SSS3
    "4.2.3 Fast Neighborhood Component Analysis ‣ 4.2 Neighborhood Component Analysis
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey"), $\boldsymbol{H}:=\frac{1}{2}(\boldsymbol{I}-\frac{1}{k+1}\boldsymbol{1}\boldsymbol{1}^{\top})\in\mathbb{R}^{k\times
    k}$ is the centering matrix, and:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{R}^{k}\ni\boldsymbol{b}_{ij}:=\boldsymbol{H}\boldsymbol{\psi}_{ij}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle-\exp\Bigg{(}\boldsymbol{\psi}_{ij}-\log\Big{(}1+\sum_{\boldsymbol{x}_{t}\in\mathcal{N}_{i}}\exp\big{(}(\boldsymbol{w}_{ij}-\boldsymbol{w}_{it})^{\top}\boldsymbol{\lambda}\big{)}\Big{)}\Bigg{)},$
    |  | (122) |'
  prefs: []
  type: TYPE_TB
- en: 'in which $\boldsymbol{\psi}_{ij}\in\mathbb{R}^{k}$ is the learnable variational
    parameter. See (Wang & Tan, [2017](#bib.bib115)) for the derivation of this lower-bound.
    The sketch of this derivation is using Eq. ([84](#S4.E84 "In 4.1 Collapsing Classes
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) but for the $k$NN among the similar points, i.e., $\mathcal{N}_{i}$.
    Then, the lower-bound is obtained by a logarithm inequality as well as the Bohning’s
    quadratic bound (Murphy, [2012](#bib.bib89)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We assume a Gaussian distribution for the prior of $\boldsymbol{\lambda}$ with
    mean $\boldsymbol{m}_{0}\in\mathbb{R}^{d}$ and covariance $\boldsymbol{V}_{0}\in\mathbb{R}^{d\times
    d}$. This prior is assumed to be known. Likewise, we assume a Gaussian distribution
    with mean $\boldsymbol{m}_{T}\in\mathbb{R}^{d}$ and covariance $\boldsymbol{V}_{T}\in\mathbb{R}^{d\times
    d}$ for the posterior $\mathbb{P}(X,\boldsymbol{\Lambda}|Y)$. Using Bayes’ rule
    and the above lower-bound on the likelihood, we can estimate these parameters
    as (Wang & Tan, [2017](#bib.bib115)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{V}_{T}:=\Big{(}\boldsymbol{V}_{0}^{-1}+\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{N}_{i}}\boldsymbol{W}_{i}^{j}\boldsymbol{H}(\boldsymbol{W}_{i}^{j})^{\top}\Big{)}^{-1},$
    |  | (123) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{m}_{T}:=\boldsymbol{V}_{T}\Big{(}\boldsymbol{V}_{0}^{-1}\boldsymbol{m}_{0}+\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{N}_{i}}\boldsymbol{W}_{i}^{j}\boldsymbol{b}_{ij}\Big{)}.$
    |  | (124) |'
  prefs: []
  type: TYPE_TB
- en: 'The variational parameter can also be obtained by (Wang & Tan, [2017](#bib.bib115)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\psi}_{ij}:=(\boldsymbol{W}_{i}^{j})^{\top}\boldsymbol{m}_{T}.$
    |  | (125) |'
  prefs: []
  type: TYPE_TB
- en: 'The variables $\boldsymbol{b}_{ij}$, $\boldsymbol{V}_{T}$, $\boldsymbol{m}_{T}$,
    and $\boldsymbol{\psi}_{ij}$ are updated iteratively by Eqs. ([122](#S4.E122 "In
    4.3.2 Bayesian Neighborhood Component Analysis ‣ 4.3 Bayesian Metric Learning
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), ([123](#S4.E123 "In 4.3.2 Bayesian Neighborhood
    Component Analysis ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")),
    ([124](#S4.E124 "In 4.3.2 Bayesian Neighborhood Component Analysis ‣ 4.3 Bayesian
    Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), and ([125](#S4.E125 "In 4.3.2
    Bayesian Neighborhood Component Analysis ‣ 4.3 Bayesian Metric Learning Methods
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")), respectively, until convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After these parameters are learned, we can sample the eigenvalues from the
    posterior, $\boldsymbol{\lambda}\sim\mathcal{N}(\boldsymbol{m}_{T},\boldsymbol{V}_{T})$.
    These eigenvalues can be used in Eq. ([114](#S4.E114 "In 4.3 Bayesian Metric Learning
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) to obtain the weight matrix in the metric.
    Alternatively, we can directly sample the distance metric from the following distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\sim\mathcal{N}(\boldsymbol{w}_{ij}^{\top}\boldsymbol{m}_{T},\boldsymbol{w}_{ij}^{\top}\boldsymbol{V}_{T}\boldsymbol{w}_{ij}).$
    |  | (126) |'
  prefs: []
  type: TYPE_TB
- en: 4.3.3 Local Distance Metric (LDM)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let the set of similar and dissimilar points for the point $\boldsymbol{x}_{i}$
    be denoted by $\mathcal{S}_{i}$ and $\mathcal{D}_{i}$, respectively. In Local
    Distance Metric (LDM) (Yang et al., [2006](#bib.bib133)), we consider the following
    for the likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}(y_{i}&#124;\boldsymbol{x}_{i})=$ | $\displaystyle\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\times\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{j}\in\mathcal{D}_{i}}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})\Big{)}^{-1}.$
    |  | (127) |'
  prefs: []
  type: TYPE_TB
- en: 'If we consider Eq. ([114](#S4.E114 "In 4.3 Bayesian Metric Learning Methods
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) for decomposition of the weight matrix, the log-likelihood
    becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sum_{i=1}^{n}\log(\mathbb{P}(y_{i}&#124;\boldsymbol{x}_{i},\boldsymbol{\Lambda}))=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}\Big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{j}\in\mathcal{D}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}\Big{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'We want to maximize this log-likelihood for learning the variables $\{\lambda_{1},\dots,\lambda_{d}\}$.
    An evidence lower bound on this log-likelihood can be (Yang et al., [2006](#bib.bib133)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sum_{i=1}^{n}\log$ | $\displaystyle(\mathbb{P}(y_{i}&#124;\boldsymbol{x}_{i},\boldsymbol{\Lambda}))\geq$
    |  | (128) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\phi_{ij}\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{j}\in\mathcal{D}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\phi_{ij}$ is the variational parameter which is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\phi_{ij}:=\frac{\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}\times$
    |  | (129) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\Big{(}1+\frac{\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}\Big{)}^{-1}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'See (Yang et al., [2006](#bib.bib133)) for derivation of the lower bound. Iteratively,
    we maximize the lower bound, i.e. Eq. ([128](#S4.E128 "In 4.3.3 Local Distance
    Metric (LDM) ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), and
    update $\phi_{ij}$ by Eq. ([129](#S4.E129 "In 4.3.3 Local Distance Metric (LDM)
    ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")). The learned parameters
    $\{\lambda_{1},\dots,\lambda_{d}\}$ can be used in Eq. ([114](#S4.E114 "In 4.3
    Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) to obtain the
    weight matrix in the metric.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Information Theoretic Metric Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There exist information theoretic approaches for metric learning where KL-divergence
    (relative entropy) or mutual information is used.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Information Theoretic Metric Learning with a Prior Weight Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the information theoretic methods for metric learning is using a prior
    weight matrix (Davis et al., [2007](#bib.bib22)) where we consider a known weight
    matrix $\boldsymbol{W}_{0}$ as the regularizer and try to minimize the KL-divergence
    between the distributions with $\boldsymbol{W}$ and $\boldsymbol{W}_{0}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{KL}(p_{ij}^{W_{0}}\&#124;p_{ij}^{W}):=\sum_{i=1}^{n}\sum_{j=1}^{n}p_{ij}^{W_{0}}\log\Big{(}\frac{p_{ij}^{W_{0}}}{p_{ij}^{W}}\Big{)}.$
    |  | (130) |'
  prefs: []
  type: TYPE_TB
- en: There are both offline and online approaches for metric learning using batch
    and streaming data, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '– Offline Information Theoretic Metric Learning: We consider a Gaussian distribution,
    i.e. Eq. ([84](#S4.E84 "In 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), for
    the probability of $\boldsymbol{x}_{i}$ taking $\boldsymbol{x}_{j}$ as its neighbor,
    i.e. $p_{ij}^{W}$. While we make the weight matrix similar to the prior weight
    matrix through KL-divergence, we find a weight matrix which makes all the distances
    of similar points less than an upper bound $u>0$ and all the distances of dissimilar
    points larger than a lower bound $l$ (where $l>u$). Note that, for Gaussian distributions,
    the KL divergence is related to the LogDet $D_{ld}(.,.)$ between covariance matrices
    (Dhillon, [2007](#bib.bib25)); hence, we can say:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{KL}(p_{ij}^{W_{0}}\&#124;p_{ij}^{W})=\frac{1}{2}D_{ld}(\boldsymbol{W}_{0}^{-1},\boldsymbol{W}^{-1})=\frac{1}{2}D_{ld}(\boldsymbol{W},\boldsymbol{W}_{0})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}\textbf{tr}(\boldsymbol{W}\boldsymbol{W}_{0}^{-1})-\log(\det(\boldsymbol{W}\boldsymbol{W}_{0}^{-1}))-n,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $(a)$ is because of the definition of LogDet. Hence, the optimization
    problem can be (Davis et al., [2007](#bib.bib22)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle
    D_{ld}(\boldsymbol{W},\boldsymbol{W}_{0})$ |  | (131) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    u,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\geq
    l,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: '– Online Information Theoretic Metric Learning: The online information theoretic
    metric learning (Davis et al., [2007](#bib.bib22)) is suitable for streaming data.
    For this, we use the offline approach where the known weight matrix $\boldsymbol{W}_{0}$
    is learned weight matrix by the data which have been received so far. Consider
    the time slot $t$ where we have been accumulated some data until then and some
    new data points are received at this time. The optimization problem is Eq. ([131](#S4.E131
    "In 4.4.1 Information Theoretic Metric Learning with a Prior Weight Matrix ‣ 4.4
    Information Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) where $\boldsymbol{W}_{0}=\boldsymbol{W}_{t}$
    which is the learned weight matrix so far at time $t$. Note that if there is some
    label information available, we can incorporate it in the optimization problem
    as a regularizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Information Theoretic Metric Learning for Imbalanced Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Distance Metric by Balancing KL-divergence (DMBK) (Feng et al., [2018](#bib.bib31))
    can be used for imbalanced data where the cardinality of classes are different.
    Assume the classes have Gaussian distributions where $\boldsymbol{\mu}_{i}\in\mathbb{R}^{d}$
    and $\boldsymbol{\Sigma}_{i}\in\mathbb{R}^{d\times d}$ denote the mean and covariance
    of the $i$-th class. Recall the projection matrix $\boldsymbol{U}$ in Eq. ([9](#S2.E9
    "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) and Proposition [2](#Thmproposition2 "Proposition 2 (Projection
    in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"). The KL-divergence between the probabilities of the $i$-th and $j$-th
    classes after projection onto the subspace of metric is (Feng et al., [2018](#bib.bib31)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{KL}(p_{i}\&#124;p_{j})=$ | $\displaystyle\,\frac{1}{2}\Big{(}\log\big{(}\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{j}\boldsymbol{U})\big{)}$
    |  | (132) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\log\big{(}\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{i}\boldsymbol{U})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\textbf{tr}\big{(}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{j}\boldsymbol{U})^{-1}\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}_{i}+\boldsymbol{D}_{ij})\boldsymbol{U}\big{)}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{D}_{ij}:=(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{j})(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{j})^{\top}$.
    To cancel the effect of cardinality of classes in imbalanced data, we use the
    normalized divergence of classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle e_{ij}:=\frac{n_{i}n_{j}\text{KL}(p_{i}\&#124;p_{j})}{\sum_{1\leq
    k<l\leq c}n_{k}n_{l}\text{KL}(p_{k}\&#124;p_{l})},$ |  | (133) |'
  prefs: []
  type: TYPE_TB
- en: 'where $n_{i}$ and $c$ denote the number of the $i$-th class and the number
    of classes, respectively. We maximize the geometric mean of this divergence between
    pairs of classes to separate classes after projection onto the subspace of metric.
    A regularization term is used to increase the distances of dissimilar points and
    a constraint is used to decrease the similar points (Feng et al., [2018](#bib.bib31)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\log\Big{(}\Big{(}\prod_{1\leq
    i<j\leq c}e_{ij}\Big{)}^{\frac{1}{c(c-1)}}\Big{)}$ |  | (134) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    1,$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda>0$ is the regularization parameter. This problem can be solved
    using projected gradient method (Ghojogh et al., [2021c](#bib.bib48)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Probabilistic Relevant Component Analysis Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recall the Relevant Component Analysis (RCA method) (Shental et al., [2002](#bib.bib101))
    which was introduced in Section [3.1.4](#S3.SS1.SSS4 "3.1.4 Relevant Component
    Analysis (RCA) ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"). Here,
    we introduce probabilistic RCA (Bar-Hillel et al., [2003](#bib.bib6), [2005](#bib.bib7))
    which uses information theory. Suppose the $n$ data points can be divided into
    $c$ clusters, or so-called chunklets. Let $\mathcal{X}_{l}$ denote the data of
    the $l$-th chunklet and $\boldsymbol{\mu}_{l}$ be the mean of $\mathcal{X}_{l}$.
    Consider Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2
    Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) for decomposition of the weight matrix in the
    metric where the column-space of $\boldsymbol{U}$ is the subspace of metric. Let
    projection of data onto this subspace be denoted by $\boldsymbol{Y}=\boldsymbol{U}^{\top}\boldsymbol{X}$,
    the projected data in the $l$-th chunklet be $\mathcal{Y}_{l}$, and $\boldsymbol{\mu}^{y}_{l}$
    be the mean of $\mathcal{Y}_{l}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In probabilistic RCA, we maximize the mutual information between data and the
    projected data while we want the summation of distances of points in a chunklet
    from the mean of chunklet is less than a threshold or margin $m>0$. The mutual
    information is related to the entropy as $I(X,Y):=H(Y)-H(Y|X)$; hence, we can
    maximize the entropy of projected data $H(Y)$ rather than the mutual information.
    Because $\boldsymbol{Y}=\boldsymbol{U}^{\top}\boldsymbol{X}$, we have $H(Y)\propto\det(\boldsymbol{U})$.
    According to Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), we have $\det(\boldsymbol{U})\propto\det(\boldsymbol{W})$.
    Hence, the optimization problem can be (Bar-Hillel et al., [2003](#bib.bib6),
    [2005](#bib.bib7)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\det(\boldsymbol{W})$
    |  | (135) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\sum_{l=1}^{c}\sum_{\boldsymbol{y}_{i}\in\mathcal{Y}_{l}}\&#124;\boldsymbol{y}_{i}-\boldsymbol{\mu}^{y}_{l}\&#124;_{\boldsymbol{W}}^{2}\leq
    m,$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This preserves the information of data after projection while the inter-chunklet
    variances are upper-bounded by a margin.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we assume Gaussian distribution for each chunklet with the covariance matrix
    $\boldsymbol{\Sigma}_{l}$ for the $l$-th chunklet, we have $\det(\boldsymbol{W})\propto\log(\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{l}\boldsymbol{U}))$
    because of the quadratic characteristic of covariance. In this case, the optimization
    problem becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{l=1}^{c}\log(\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{l}\boldsymbol{U}))$
    |  | (136) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\sum_{l=1}^{c}\sum_{\boldsymbol{y}_{i}\in\mathcal{Y}_{l}}\&#124;\boldsymbol{y}_{i}-\boldsymbol{\mu}^{y}_{l}\&#124;_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}\leq
    m,$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{W}\succeq\boldsymbol{0}$ is already satisfied because of
    Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 Metric Learning by Information Geometry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another information theoretic methods for metric learning is using information
    geometry in which kernels on data and labels are used (Wang & Jin, [2009](#bib.bib118)).
    Let $\boldsymbol{L}\in\mathbb{R}^{c\times n}$ denote the one-hot encoded labels
    of $n$ data points with $c$ classes and let $\boldsymbol{X}\in\mathbb{R}^{d\times
    n}$ be the data points. The kernel matrix on the labels is $\boldsymbol{K}_{L}=\boldsymbol{Y}^{\top}\boldsymbol{Y}+\lambda\boldsymbol{I}$
    whose main diagonal is strengthened by a small positive number $\lambda$ to have
    a full rank. Recall Proposition [2](#Thmproposition2 "Proposition 2 (Projection
    in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey") and Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) where $\boldsymbol{U}$ is the projection
    matrix onto the subspace of metric. The kernel matrix over the projected data,
    $\boldsymbol{Y}=\boldsymbol{U}^{\top}\boldsymbol{X}$, is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{K}_{Y}=$ | $\displaystyle\boldsymbol{Y}^{\top}\boldsymbol{Y}=(\boldsymbol{U}^{\top}\boldsymbol{X})^{\top}(\boldsymbol{U}^{\top}\boldsymbol{X})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\boldsymbol{X}^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{X}\overset{(\ref{equation_W_U_UT})}{=}\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X}.$
    |  | (137) |'
  prefs: []
  type: TYPE_TB
- en: 'We can minimize the KL-divergence between the distributions of kernels $\boldsymbol{K}_{Y}$
    and $\boldsymbol{K}_{L}$ (Wang & Jin, [2009](#bib.bib118)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\text{KL}(\boldsymbol{K}_{Y}\&#124;\boldsymbol{K}_{L})$
    |  | (138) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'For simplicity, we assume Gaussian distributions for the kernels. The KL divergence
    between the distributions of two matrices, $\boldsymbol{K}_{Y}\in\mathbb{R}^{n\times
    n}$ and $\boldsymbol{K}_{L}\in\mathbb{R}^{n\times n}$, with Gaussian distributions
    is simplified to (Wang & Jin, [2009](#bib.bib118), Theorem 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{KL}(\boldsymbol{K}_{Y}\&#124;\boldsymbol{K}_{L})=\frac{1}{2}\Big{(}\textbf{tr}(\boldsymbol{K}_{L}^{-1}\boldsymbol{K}_{Y})+\log(\det(\boldsymbol{K}_{L}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\log(\det(\boldsymbol{K}_{Y}))-n\Big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(\ref{equation_ML_information_geometry_K_Y})}{\propto}\frac{1}{2}\Big{(}\textbf{tr}(\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X})+\log(\det(\boldsymbol{K}_{L}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\log(\det(\boldsymbol{W}))-n\Big{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'After ignoring the constant terms w.r.t. $\boldsymbol{W}$, we can restate Eq.
    ([138](#S4.E138 "In 4.4.4 Metric Learning by Information Geometry ‣ 4.4 Information
    Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X})-\log(\det(\boldsymbol{W}))$
    |  | (139) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'If we take the derivative of the objective function in Eq. ([139](#S4.E139
    "In 4.4.4 Metric Learning by Information Geometry ‣ 4.4 Information Theoretic
    Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and
    Deep Metric Learning: Tutorial and Survey")) and set it to zero, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{W}}=\boldsymbol{X}\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top}-\boldsymbol{W}^{-1}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies\boldsymbol{W}=(\boldsymbol{X}\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top})^{-1}.$
    |  | (140) |'
  prefs: []
  type: TYPE_TB
- en: 'Note that the constraint $\boldsymbol{W}\succeq\boldsymbol{0}$ is already satisfied
    by the solution, i.e., Eq. ([140](#S4.E140 "In 4.4.4 Metric Learning by Information
    Geometry ‣ 4.4 Information Theoretic Metric Learning ‣ 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this method has used kernels, it can be kernelized further. We can
    also have a kernel version of this method by using Eq. ([54](#S3.E54 "In Lemma
    4\. ‣ 3.6.4 Kernel Discriminative Component Analysis ‣ 3.6 Kernel Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) as the generalized Mahalanobis distance in the
    feature space, where $\boldsymbol{T}$ (defined in Eq. ([46](#S3.E46 "In 3.6.2
    Regularization by Locally Linear Embedding ‣ 3.6 Kernel Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))) is the projection matrix for the metric. Using this in
    Eqs. ([139](#S4.E139 "In 4.4.4 Metric Learning by Information Geometry ‣ 4.4 Information
    Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) and ([140](#S4.E140 "In 4.4.4
    Metric Learning by Information Geometry ‣ 4.4 Information Theoretic Metric Learning
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) can give us the kernel version of this method. See (Wang
    & Jin, [2009](#bib.bib118)) for more information about it.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Empirical Risk Minimization in Metric Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can learn the metric by minimizing some empirical risk. In the following,
    some metric learning metric learning methods by risk minimization are introduced.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1 Metric Learning Using the Sigmoid Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the metric learning methods by risk minimization is (Guillaumin et al.,
    [2009](#bib.bib57)). The distribution for $\boldsymbol{x}_{i}$ to take $\boldsymbol{x}_{j}$
    as its neighbor can be stated using a sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p^{W}_{ij}$ | $\displaystyle:=\frac{1}{1+\exp(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-b)},$
    |  | (141) |'
  prefs: []
  type: TYPE_TB
- en: 'where $b>0$ is a bias, because close-by points should have larger probability.
    We can maximize and minimize this probability for similar and dissimilar points,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}y_{ij}\log(p^{W}_{ij})+(1-y_{ij})\log(1-p^{W}_{ij})$
    |  | (142) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $y_{ij}$ is defined in Eq. ([117](#S4.E117 "In 4.3.1 Bayesian Metric
    Learning Using Sigmoid Function ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). This can be solved using projected gradient method (Ghojogh et al.,
    [2021c](#bib.bib48)). This optimization can be seen as minimization of the empirical
    risk where close-by points are pushed toward each other and dissimilar points
    are pushed away to have less error.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Pairwise Constrained Component Analysis (PCCA)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Pairwise Constrained Component Analysis (PCCA) (Mignon & Jurie, [2012](#bib.bib85))
    minimizes the following empirical risk to minimize and maximize the distances
    of similar points and dissimilar points, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}$ |  | (143)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{i=1}^{n}\sum_{j=1}^{n}\log\Big{(}1+\exp\big{(}y_{ij}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}-b)\big{)}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $y_{ij}$ is defined in Eq. ([117](#S4.E117 "In 4.3.1 Bayesian Metric
    Learning Using Sigmoid Function ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), $b>0$ is a bias, $\boldsymbol{W}\succeq\boldsymbol{0}$ is already
    satisfied because of Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis
    Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). This can be solved using projected
    gradient method (Ghojogh et al., [2021c](#bib.bib48)) with the gradient (Mignon
    & Jurie, [2012](#bib.bib85)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}$ | $\displaystyle=2\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{y_{ij}}{1+\exp\big{(}y_{ij}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}-b)\big{)}}$
    |  | (144) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\times(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note that we can have kernel PCCA by using Eq. ([54](#S3.E54 "In Lemma 4\.
    ‣ 3.6.4 Kernel Discriminative Component Analysis ‣ 3.6 Kernel Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")). In other words, we can replace $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}$
    and $(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}$
    with $\|\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\|_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$
    and $(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}$,
    respectively, to have PCCA in the feature space.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.3 Metric Learning for Privileged Information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In some applications, we have a dataset with privileged information where for
    every point, we have two feature vector; one for the main feature (denoted by
    $\{\boldsymbol{x}_{i}\}_{i=1}^{n}$) and one for the privileged information (denoted
    by $\{\boldsymbol{z}_{i}\}_{i=1}^{n}$). A metric learning method for using privileged
    information is (Yang et al., [2016](#bib.bib137)) where we minimize and maximize
    the distances of similar and dissimilar points, respectively, for the main features.
    Simultaneously, we make the distances of privileged features close to the distances
    of main features. Having these two simultaneous goals, we minimize the following
    empirical risk (Yang et al., [2016](#bib.bib137)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}_{1},\boldsymbol{W}_{2}}{\text{minimize}}$
    |  | $\displaystyle\sum_{i=1}^{n}\log\Big{(}1+$ |  | (145) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\exp\big{(}y_{ij}\,(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}_{1}}^{2}-\&#124;\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\&#124;_{\boldsymbol{W}_{2}}^{2})\big{)}\Big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}_{1}\succeq\boldsymbol{0},\quad\boldsymbol{W}_{2}\succeq\boldsymbol{0}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 5 Deep Metric Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We saw in Sections [3](#S3 "3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey") and [4](#S4 "4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")
    that both spectral and probabilistic metric learning methods use the generalized
    Mahalanobis distance, i.e. Eq. ([5](#S2.E5 "In Definition 3 (Generalized Mahalanobis
    distance). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), and learn the weight matrix in the metric. Deep metric learning,
    however, has a different approach. The methods in deep metric learning usually
    do not use a generalized Mahalanobis distance but they earn an embedding space
    using a neural network. The network learns a $p$-dimensional embedding space for
    discriminating classes or the dissimilar points and making the similar points
    close to each other. The network embeds data in the embedding space (or subspace)
    of metric. Then, any distance metric $d(.,.):\mathbb{R}^{p}\times\mathbb{R}^{p}\rightarrow\mathbb{R}$
    can be used in this embedding space. In the loss functions of network, we can
    use the distance function $d(.,.)$ in the embedding space. For example, an option
    for the distance function is the squared $\ell_{2}$ norm or squared Euclidean
    distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}:=\&#124;\textbf{f}(\boldsymbol{x}_{i}^{1})-\textbf{f}(\boldsymbol{x}_{i}^{2})\&#124;_{2}^{2},$
    |  | (146) |'
  prefs: []
  type: TYPE_TB
- en: where $\textbf{f}(\boldsymbol{x}_{i})\in\mathbb{R}^{p}$ denotes the output of
    network for the input $\boldsymbol{x}_{i}$ as its $p$-dimensional embedding. We
    train the network using mini-batch methods such as the mini-batch stochastic gradient
    descent and denote the mini-batch size by $b$. The shared weights of sub-networks
    are denoted by the learnable parameter $\theta$.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Reconstruction Autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.1.1 Types of Autoencoders
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An autoencoder is a model consisting of an encoder $E(.)$ and a decoder $D(.)$.
    There are several types of autoencoders. All types of autoencoders learn a code
    layer in the middle of encoder and decoder. Inferential autoencoders learn a stochastic
    latent space in the code layer between the encoder and decoder. Variational autoencoder
    (Ghojogh et al., [2021a](#bib.bib46)) and adversarial autoencoder (Ghojogh et al.,
    [2021b](#bib.bib47)) are two important types of inferential autoencoders. Another
    type of autoencoder is the reconstruction autoencoder consisting of an encoder,
    transforming data to a code, and a decoder, transforming the code back to the
    data. Hence, the decoder reconstructs the input data to the encoder. The code
    is a representation for data. Each of the encoder and decoder can be multiple
    layers of neural network with activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Reconstruction Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We denote the input data point to the encoder by $\boldsymbol{x}\in\mathbb{R}^{d}$
    where $d$ is the dimensionality of data. The reconstructed data point is the output
    of decoder and is denoted by $\widehat{\boldsymbol{x}}\in\mathbb{R}^{d}$. The
    representation code, which is the output of encoder and the input of decoder,
    is denoted by $\textbf{f}(\boldsymbol{x}):=E(\boldsymbol{x})\in\mathbb{R}^{p}$.
    We have $\widehat{\boldsymbol{x}}=D(E(\boldsymbol{x}))=D(\textbf{f}(\boldsymbol{x}))$.
    If the dimensionality of code is greater than the dimensionality of input data,
    i.e. $p>d$, the autoencoder is called an over-complete autoencoder (Goodfellow
    et al., [2016](#bib.bib55)). Otherwise, if $p<d$, the autoencoder is an under-complete
    autoencoder (Goodfellow et al., [2016](#bib.bib55)). The loss function of reconstruction
    autoencoder tries to make the reconstructed data close to the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}\Big{(}d\big{(}\boldsymbol{x}_{i},\widehat{\boldsymbol{x}}_{i}\big{)}+\lambda\Omega(\theta)\Big{)},$
    |  | (147) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda\geq 0$ is the regularization parameter and $\Omega(\theta)$ is
    some penalty or regularization on the weights. Here, the distance function $d(.,.)$
    is defined on $\mathbb{R}^{d}\times\mathbb{R}^{d}$. Note that the penalty term
    can be regularization on the code $\textbf{f}(\boldsymbol{x}_{i})$. If the used
    distance metric is the squared Euclidean distance, this loss is named the regularized
    Mean Squared Error (MSE) loss.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Denoising Autoencoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A problem with over-complete autoencoder is that its training only copies each
    feature of data input to one of the neurons in the code layer and then copies
    it back to the corresponding feature of output layer. This is because the number
    of neurons in the code layer is greater than the number of neurons in the input
    and output layers. In other words, the networks just memorizes or gets overfit.
    This coping happens by making some of the weights equal to one (or a scale of
    one depending on the activation functions) and the rest of weights equal to zero.
    To avoid this problem in over-complete autoencoders, one can add some noise to
    the input data and try to reconstruct the data without noise. For this, Eq. ([147](#S5.E147
    "In 5.1.2 Reconstruction Loss ‣ 5.1 Reconstruction Autoencoders ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    is used while the input to the network is the mini-batch plus some noise. This
    forces the over-complete autoencoder to not just copy data to the code layer.
    This autoencoder can be used for denoising as it reconstructs the data without
    noise for a noisy input. This network is called the Denoising Autoencoder (DAE)
    (Goodfellow et al., [2016](#bib.bib55)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.4 Metric Learning by Reconstruction Autoencoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The under-complete reconstruction autoencoder can be used for metric learning
    and dimensionality reduction, especially when $p\ll d$. The loss function for
    learning a low-dimensional representation code and reconstructing data by the
    autoencoder is Eq. ([147](#S5.E147 "In 5.1.2 Reconstruction Loss ‣ 5.1 Reconstruction
    Autoencoders ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")). The code layer between the encoder and decoder
    is the embedding space of metric.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that if the activation functions of all layers are linear, the under-complete
    autoencoder is reduced to Principal Component Analysis (Ghojogh & Crowley, [2019](#bib.bib37)).
    Let $\boldsymbol{U}_{l}$ denote the weight matrix of the $l$-th layer of network,
    $\ell_{e}$ be the number of layers of encoder, and $\ell_{d}$ be the number of
    layers of decoder. With linear activation function, the encoder and decoder are:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{encoder: }\quad\mathbb{R}^{p}\ni\textbf{f}(\boldsymbol{x}_{i})=\underbrace{\boldsymbol{U}_{\ell_{e}}^{\top}\boldsymbol{U}_{\ell_{e}-1}^{\top}\dots\boldsymbol{U}_{1}^{\top}}_{\boldsymbol{U}_{e}^{\top}}\boldsymbol{x}_{i},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{decoder: }\quad\mathbb{R}^{d}\ni\widehat{\boldsymbol{x}}_{i}=\underbrace{\boldsymbol{U}_{1}\dots\boldsymbol{U}_{\ell_{d}-1}\boldsymbol{U}_{\ell_{d}}}_{\boldsymbol{U}_{d}}\textbf{f}(\boldsymbol{x}_{i}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where linear projection by $\ell$ projection matrices can be replaced by linear
    projection with one projection matrices $\boldsymbol{U}_{e}$ and $\boldsymbol{U}_{d}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'For learning complicated data patterns, we can use nonlinear activation functions
    between layers of the encoder and decoder to have nonlinear metric learning and
    dimensionality reduction. It is noteworthy that nonlinear neural network can be
    seen as an ensemble or concatenation of dimensionality reduction (or feature extraction)
    and kernel methods. The justification of this claim is as follows. Let the dimensionality
    for a layer of network be $\boldsymbol{U}\in\mathbb{R}^{d_{1}\times d_{2}}$ so
    it connects $d_{1}$ neurons to $d_{2}$ neurons. Two cases can happen:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If $d_{1}\geq d_{2}$, this layer acts as dimensionality reduction or feature
    extraction because it has reduced the dimensionality of its input data. If this
    layer has a nonlinear activation function, the dimensionality reduction is nonlinear;
    otherwise, it is linear.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If $d_{1}<d_{2}$, this layer acts as a kernel method which maps its input data
    to the high-dimensional feature space in some Reproducing Kernel Hilbert Space
    (RKHS). This kernelization can help nonlinear separation of some classes which
    are not separable linearly (Ghojogh et al., [2021e](#bib.bib50)). An example use
    of kernelization in machine learning is kernel support vector machine (Vapnik,
    [1995](#bib.bib114)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Therefore, a neural network is a complicated feature extraction method as a
    concatenation of dimensionality reduction and kernel methods. Each layer of network
    learns its own features from data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Supervised Metric Learning by Supervised Loss Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Various loss functions exist for supervised metric learning by neural networks.
    Supervised loss functions can teach the network to separate classes in the embedding
    space (Sikaroudi et al., [2020b](#bib.bib103)). For this, we use a network whose
    last layer is for classification of data points. The features of the one-to-last
    layer can be used for feature embedding. The last layer after the embedding features
    is named the classification layer. The structure of this network is shown in Fig.
    [3](#S5.F3 "Figure 3 ‣ 5.2 Supervised Metric Learning by Supervised Loss Functions
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"). Let the $i$-th point in the mini-batch be denoted by $\boldsymbol{x}_{i}\in\mathbb{R}^{d}$
    and its label be denoted by $y_{i}\in\mathbb{R}$. Suppose the network has one
    output neuron and its output for the input $\boldsymbol{x}_{i}$ is denoted by
    $\textbf{f}_{o}(\boldsymbol{x}_{i})\in\mathbb{R}$. This output is the estimated
    class label by the network. We denote output of the the one-to-last layer by $\textbf{f}(\boldsymbol{x}_{i})\in\mathbb{R}^{p}$
    where $p$ is the number of neurons in that layer which is equivalent to the dimensionality
    of the embedding space. The last layer of network, connecting the $p$ neurons
    to the output neuron is a fully-connected layer. The network until the one-to-last
    layer can be any feed-forward or convolutional network depending on the type of
    data. If the network is convolutional, it should be flattened at the one-to-last
    layer. The network learns to classify the classes, by the supervised loss functions,
    so the features of the one-to-last layers will be discriminating features and
    suitable for embedding.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ea9454e1a404f5c59dd66e70a29e326d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The structure of network for metric learning with supervised loss
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Mean Squared Error and Mean Absolute Value Losses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the supervised losses is the Mean Squared Error (MSE) which makes the
    estimated labels close to the true labels using squared $\ell_{2}$ norm:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}(\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i})^{2}.$
    |  | (148) |'
  prefs: []
  type: TYPE_TB
- en: 'One problem with this loss function is exaggerating outliers because of the
    square but its advantage is its differentiability. Another loss function is the
    Mean Absolute Error (MAE) which makes the estimated labels close to the true labels
    using $\ell_{1}$ norm or the absolute value:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}&#124;\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i}&#124;.$
    |  | (149) |'
  prefs: []
  type: TYPE_TB
- en: The distance used in this loss is also named the Manhattan distance. This loss
    function does not have the problem of MSE and it can be used for imposing sparsity
    in the embedding. It is not differentiable at the point $\textbf{f}(\boldsymbol{x}_{i})=y_{i}$
    but as the derivatives are calculated numerically by the neural network, this
    is not a big issue nowadays.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Huber and KL-Divergence Losss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another loss function is the Huber loss which is a combination of the MSE and
    MAE to have the advantages of both of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}$ |  | (150)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}\sum_{i=1}^{b}\left\{\begin{array}[]{ll}0.5(\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i})^{2}&amp;\mbox{if
    }&#124;\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i}&#124;\leq\delta\\ \delta(&#124;\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i}&#124;-0.5\delta)&amp;\mbox{otherwise}.\end{array}\right.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'KL-divergence loss function makes the distribution of the estimated labels
    close to the distribution of the true labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\text{KL}(\mathbb{P}(\textbf{f}(\boldsymbol{x}))\&#124;\mathbb{P}(y))=\sum_{i=1}^{b}\textbf{f}(\boldsymbol{x}_{i})\log(\frac{\textbf{f}(\boldsymbol{x}_{i})}{y_{i}}).$
    |  | (151) |'
  prefs: []
  type: TYPE_TB
- en: 5.2.3 Hinge Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If there are two classes, i.e. $c=2$, we can have true labels as $y_{i}\in\{-1,1\}$.
    In this case, a possible loss function is the Hinge loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}\big{[}m-y_{i}\,\textbf{f}_{o}(\boldsymbol{x}_{i})\big{]}_{+},$
    |  | (152) |'
  prefs: []
  type: TYPE_TB
- en: where $[\cdot]_{+}:=\max(\cdot,0)$ and $m>0$ is the margin. If the signs of
    the estimated and true labels are different, the loss is positive which should
    be minimized. If the signs are the same and $|\textbf{f}_{o}(\boldsymbol{x}_{i})|\geq
    m$, then the loss function is zero. If the signs are the same but $|\textbf{f}_{o}(\boldsymbol{x}_{i})|<m$,
    the loss is positive and should be minimized because the estimation is correct
    but not with enough margin from the incorrect estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 Cross-entropy Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For any number of classes, denoted by $c$, we can have a cross-entropy loss.
    For this loss, we have $c$ neurons, rather than one neuron, at the last layer.
    In contrast to the MSE, MAE, Huber, and KL-divergence losses which use linear
    activation function at the last layer, cross-entropy requires softmax or sigmoid
    activation function at the last layer so the output values are between zero and
    one. For this loss, we have $c$ outputs, i.e. $\textbf{f}_{o}(\boldsymbol{x}_{i})\in\mathbb{R}^{c}$
    (continuous values between zero and one), and the true labels are one-hot encoded,
    i.e., $\boldsymbol{y}_{i}\in\{0,1\}^{c}$. This loss is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}-\sum_{i=1}^{b}\sum_{l=1}^{c}(\boldsymbol{y}_{i})_{l}\log\big{(}\textbf{f}_{o}(\boldsymbol{x}_{i})_{l}\big{)},$
    |  | (153) |'
  prefs: []
  type: TYPE_TB
- en: where $(\boldsymbol{y}_{i})_{l}$ and $\textbf{f}_{o}(\boldsymbol{x}_{i})_{l}$
    denote the $l$-th element of $\boldsymbol{y}_{i}$ and $\textbf{f}_{o}(\boldsymbol{x}_{i})$,
    respectively. Minimizing this loss separates classes for classification; this
    separation of classes also gives us discriminating embedding in the one-to-last
    layer (Sikaroudi et al., [2020b](#bib.bib103); Boudiaf et al., [2020](#bib.bib14)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason for why cross-entropy can be suitable for metric learning is theoretically
    justified in (Boudiaf et al., [2020](#bib.bib14)), explained in the following.
    Consider the mutual information between the true labels $Y$ and the estimated
    labels $\textbf{f}_{o}(X)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle I(\textbf{f}_{o}(X);Y)$ | $\displaystyle=H(\textbf{f}_{o}(X))-H(\textbf{f}_{o}(X)&#124;Y)$
    |  | (154) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=H(Y)-H(Y&#124;\textbf{f}_{o}(X)),$ |  | (155) |'
  prefs: []
  type: TYPE_TB
- en: 'where $H(.)$ denotes entropy. On the one hand, Eq. ([154](#S5.E154 "In 5.2.4
    Cross-entropy Loss ‣ 5.2 Supervised Metric Learning by Supervised Loss Functions
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) has a generative view which exists in the metric learning
    loss functions generating embedding features. Eq. ([155](#S5.E155 "In 5.2.4 Cross-entropy
    Loss ‣ 5.2 Supervised Metric Learning by Supervised Loss Functions ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")),
    one the other hand, has a discriminative view used in the cross-entropy loss function.
    Therefore, the metric learning losses and the cross-entropy loss are related.
    It is shown in (Boudiaf et al., [2020](#bib.bib14), Proposition 1) that the cross-entropy
    is an upper-bound on the metric learning losses so its minimization for classification
    also provides embedding features.'
  prefs: []
  type: TYPE_NORMAL
- en: It is noteworthy that another supervised loss function is triplet loss, introduced
    in the next section. Triplet loss can be used for both hard labels (for classification)
    and soft labels (for similarity and dissimilarity of points). The triplet loss
    also does not need a last classification layer; therefore, the embedding layer
    can be the last layer for this loss.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Metric Learning by Siamese Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7eb80bae9536c4a86a7cca93f33527db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The structure of Siamese network with (a) two and (b) three sub-networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Siamese and Triplet Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the important deep metric learning methods is Siamese network which is
    widely used for feature extraction. Siamese network, originally proposed in (Bromley
    et al., [1993](#bib.bib15)), is a network consisting of several equivalent sub-networks
    sharing their weights. The number of sub-networks in a Siamese network can be
    any number but it usually is two or three. A Siamese network with three sub-networks
    is also called a triplet network (Hoffer & Ailon, [2015](#bib.bib68)). The weights
    of sub-networks in a Siamese network are trained in a way that the intra- and
    inter-class variances are decreased and increased, respectively. In other words,
    the similar points are pushed toward each other while the dissimilar points are
    pulled away from one another. Siamese networks have been used in various applications
    such as computer vision (Schroff et al., [2015](#bib.bib100)) and natural language
    processing (Yang et al., [2020](#bib.bib135)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Pairs and Triplets of Data Points
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Depending on the number of sub-networks in the Siamese network, we have loss
    functions for training. The loss functions of Siamese networks usually require
    pairs or triplets of data points. Siamese networks do not use the data points
    one by one but we need to make pairs or triplets of points out of dataset for
    training a Siamese network. For making the pairs or triplets, we consider every
    data point as the anchor point, denoted by $\boldsymbol{x}_{i}^{a}$. Then, we
    take one of the similar points to the anchor point as the positive (or neighbor)
    point, denoted by $\boldsymbol{x}_{i}^{p}$. We also take one of the dissimilar
    points to the anchor point as the negative (or distant) point, denoted by $\boldsymbol{x}_{i}^{n}$.
    If class labels are available, we can use them to find the positive point as one
    of the points in the same class as the anchor point, and to find the the negative
    point as one of the points in a different class from the anchor point’s class.
    Another approach is to augment the anchor point, using one of the augmentation
    methods, to obtain a positive points for the anchor point (Khodadadeh et al.,
    [2019](#bib.bib73); Chen et al., [2020](#bib.bib19)).
  prefs: []
  type: TYPE_NORMAL
- en: For Siamese networks with two sub-networks, we make pairs of anchor-positive
    points $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p})\}_{i=1}^{n_{t}}$ and
    anchor-negative points $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{n})\}_{i=1}^{n_{t}}$,
    where $n_{t}$ is the number of pairs. For Siamese networks with three sub-networks,
    we make triplets of anchor-positive-negative points $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})\}_{i=1}^{n_{t}}$,
    where $n_{t}$ is the number of triplets. If we consider every point of dataset
    as an anchor, the number of pairs/triplets is the same as the number of data points,
    i.e., $n_{t}=n$.
  prefs: []
  type: TYPE_NORMAL
- en: Various loss functions of Siamese networks use pairs or triplets of data points
    to push the positive point towards the anchor point and pull the negative point
    away from it. Doing this iteratively for all pairs or triplets will make the intra-class
    variances smaller and the inter-class variances larger for better discrimination
    of classes or clusters. Later in the following, we introduce some of the loss
    functions for training a Siamese network.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Implementation of Siamese Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Siamese network with two and three sub-networks is depicted in Fig. [4](#S5.F4
    "Figure 4 ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"). We denote
    the output of Siamese network for input $\boldsymbol{x}\in\mathbb{R}^{d}$ by $\textbf{f}(\boldsymbol{x})\in\mathbb{R}^{p}$
    where $p$ is the dimensionality of embedding (or the number of neurons at the
    last layer of the network) which is usually much less than the dimensionality
    of data, i.e., $p\ll d$. Note that the sub-networks of a Siamese network can be
    any fully-connected or convolutional network depending on the type of data. The
    used network structure for the sub-networks is usually called the backbone network.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/42359ac264e77a7e100ae4ec5ffd0ced.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Visualization of what contrastive and triplet losses do: (a) a triplet
    of anchor (green circle), positive (blue circle), and negative (red diamond) points,
    (b) the effect of contrastive loss making a margin between the anchor and negative
    point, and (c) the effect of triplet loss making a margin between the positive
    and negative points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights of sub-networks are shared in the sense that the values of their
    weights are equal. Implementation of a Siamese network can be done in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can implement several sub-networks in the memory. In the training phase,
    we feed every data point in the pairs or triplets to one of the sub-networks and
    take the outputs of sub-networks to have $\textbf{f}(\boldsymbol{x}_{i}^{a})$,
    $\textbf{f}(\boldsymbol{x}_{i}^{p})$, and $\textbf{f}(\boldsymbol{x}_{i}^{n})$.
    We use these in the loss function and update the weights of only one of the sub-networks
    by backpropagation (Ghojogh et al., [2021c](#bib.bib48)). Then, we copy the updated
    weights to the other sub-networks. We repeat this for all mini-batches and epochs
    until convergence. In the test phase, we feed the test point $\boldsymbol{x}$
    to only one of the sub-networks and get the output $\textbf{f}(\boldsymbol{x})$
    as its embedding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can implement only one sub-network in the memory. In the training phase,
    we feed the data points in the pairs or triplets to the sub-network ont by one
    and take the outputs of sub-network to have $\textbf{f}(\boldsymbol{x}_{i}^{a})$,
    $\textbf{f}(\boldsymbol{x}_{i}^{p})$, and $\textbf{f}(\boldsymbol{x}_{i}^{n})$.
    We use these in the loss function and update the weights of the sub-network by
    backpropagation (Ghojogh et al., [2021c](#bib.bib48)). We repeat this for all
    mini-batches and epochs until convergence. In the test phase, we feed the test
    point $\boldsymbol{x}$ to the sub-network and get the output $\textbf{f}(\boldsymbol{x})$
    as its embedding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The advantage of the first approach is to have all the sub-networks ready and
    we do not need to feed the points of pairs or triplets one by one. Its disadvantage
    is using more memory. As the number of points in the pairs or triplets is small
    (i.e., only two or three), the second approach is more recommended as it is memory-efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4 Contrastive Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One loss function for Siamese networks is the contrastive loss which uses the
    anchor-positive and anchor-negative pairs of points. Suppose, in each mini-batch,
    we have $b$ pairs of points $\{(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\}_{i=1}^{b}$
    some of which are anchor-positive and some are anchor-negative pairs. The points
    in an anchor-positive pair are similar, i.e. $(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{S}$,
    and the points in an anchor-negative pair are dissimilar, i.e. $(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{D}$,
    where $\mathcal{S}$ and $\mathcal{D}$ denote the similar and dissimilar sets.
  prefs: []
  type: TYPE_NORMAL
- en: '– Contrastive Loss: We define:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle y_{i}:=\left\{\begin{array}[]{ll}0&amp;\mbox{if }(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{S}\\
    1&amp;\mbox{if }(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{D}.\end{array}\right.\quad\forall
    i\in\{1,\dots,n_{t}\}.$ |  | (158) |'
  prefs: []
  type: TYPE_TB
- en: 'The main contrastive loss was proposed in (Hadsell et al., [2006](#bib.bib58))
    and is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}$
    | $\displaystyle\Big{(}(1-y_{i})d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}$
    |  | (159) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+y_{i}\big{[}\!-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}+m\big{]}_{+}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $m>0$ is the margin and $[.]_{+}:=\max(.,0)$ is the standard Hinge loss.
    The first term of loss minimizes the embedding distances of similar points and
    the second term maximizes the embedding distances of dissimilar points. As shown
    in Fig. [5](#S5.F5 "Figure 5 ‣ 5.3.3 Implementation of Siamese Networks ‣ 5.3
    Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")-b, it tries to make the distances
    of similar points as small as possible and the distances of dissimilar points
    at least greater than a margin $m$ (because the term inside the Hinge loss should
    become close to zero).'
  prefs: []
  type: TYPE_NORMAL
- en: '– Generalized Contrastive Loss: The $y_{i}$, defined in Eq. ([158](#S5.E158
    "In 5.3.4 Contrastive Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), is used in the contrastive loss, i.e., Eq. ([159](#S5.E159 "In
    5.3.4 Contrastive Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")).
    This variable is binary and a hard measure of similarity and dissimilarity. Rather
    than this hard measure, we can have a soft measure of similarity and dissimilarity,
    denoted by $\psi_{i}$, which states how similar $\boldsymbol{x}_{i}^{1}$ and $\boldsymbol{x}_{i}^{2}$
    are. This measure is between zero (completely similar) and one (completely dissimilar).
    It can be either given by the dataset as a hand-set measure or can be computed
    using any similarity measure such as the cosine function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle[0,1]\ni\psi_{i}:=\frac{1}{2}\big{(}-\cos(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})+1\big{)}.$
    |  | (160) |'
  prefs: []
  type: TYPE_TB
- en: 'In this case, the pairs $\{(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\}_{i=1}^{b}$
    need not be completely similar or dissimilar points but they can be any two random
    points from the dataset with some level of similarity/dissimilarity. The generalized
    contrastive loss generalizes the contrastive loss using this soft measure of similarity
    (Leyva-Vallina et al., [2021](#bib.bib78)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}$
    | $\displaystyle\Big{(}(1-\psi_{i})d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}$
    |  | (161) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\psi_{i}\big{[}\!-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}+m\big{]}_{+}\Big{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 5.3.5 Triplet Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the losses for Siamese networks with three sub-networks is the triplet
    loss (Schroff et al., [2015](#bib.bib100)) which uses the triplets in mini-batches,
    denoted by $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})\}_{i=1}^{b}$.
    It is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}\,\sum_{i=1}^{b}$ | $\displaystyle\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)}+m\Big{]}_{+},$
    |  | (162) |'
  prefs: []
  type: TYPE_TB
- en: 'where $m>0$ is the margin and $[.]_{+}:=\max(.,0)$ is the standard Hinge loss.
    As shown in Fig. [5](#S5.F5 "Figure 5 ‣ 5.3.3 Implementation of Siamese Networks
    ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")-c, because of the
    used Hinge loss, this loss makes the distances of dissimilar points greater than
    the distances of similar points by at least a margin $m$; in other words, there
    will be a distance of at least margin $m$ between the positive and negative points.
    This loss desires to eventually have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}+m\leq
    d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)},$
    |  | (163) |'
  prefs: []
  type: TYPE_TB
- en: 'for all triplets. The triplet loss is closely related to the cost function
    for spectral large margin metric learning (Weinberger et al., [2006](#bib.bib124);
    Weinberger & Saul, [2009](#bib.bib123)) (see Section [3.2.1](#S3.SS2.SSS1 "3.2.1
    Large-Margin Metric Learning ‣ 3.2 Spectral Methods Using Hinge Loss ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). It is also noteworthy that using the triplet loss as regularization
    for cross-entropy loss has been shown to increase robustness of network to some
    adversarial attacks (Mao et al., [2019](#bib.bib83)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.6 Tuplet Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In triplet loss, i.e. Eq. ([162](#S5.E162 "In 5.3.5 Triplet Loss ‣ 5.3 Metric
    Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), we use one positive and one
    negative point per anchor point. The tuplet loss (Sohn, [2016](#bib.bib105)) uses
    several negative points per anchor point. If $k$ denotes the number of negative
    points per anchor point and $\boldsymbol{x}_{i}^{n,j}$ denotes the $j$-th negative
    point for $\boldsymbol{x}_{i}$, the tuplet loss is (Sohn, [2016](#bib.bib105)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}\,\sum_{i=1}^{b}\sum_{j=1}^{k}$
    | $\displaystyle\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}$
    |  | (164) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n,j})\big{)}+m\Big{]}_{+}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This loss function pushes multiple negative points away from the anchor point
    simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.7 Neighborhood Component Analysis Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Neighborhood Component Analysis (NCA) (Goldberger et al., [2005](#bib.bib53))
    was originally proposed as a spectral metric learning method (see Section [4.2.1](#S4.SS2.SSS1
    "4.2.1 Neighborhood Component Analysis (NCA) ‣ 4.2 Neighborhood Component Analysis
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")). After the success of deep learning, it
    was used as the loss function of Siamese networks where we minimize the negative
    log-likelihood using Gaussian distribution or the softmax form within the mini-batch.
    Assume we have $c$ classes in every mini-batch. We denote the class index of $\boldsymbol{x}_{i}$
    by $c(\boldsymbol{x}_{i})$ and the data points of the $j$-th class in the mini-batch
    by $\mathcal{X}_{j}$. The NCA loss is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}\big{)}$
    |  | (165) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\times\Big{[}\sum_{j=1,j\neq c(\boldsymbol{x}_{i})}^{c}\sum_{\boldsymbol{x}_{j}^{n}\in\mathcal{X}_{j}}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})-\textbf{f}(\boldsymbol{x}_{j}^{n})\big{)}\big{)}\Big{]}^{-1}\Big{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The numerator minimizes the distances of similar points and the denominator
    maximizes the distances of dissimilar points.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.8 Proxy Neighborhood Component Analysis Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Computation of terms, especially the normalization factor in the denominator,
    is time- and memory-consuming in the NCA loss function (see Eq. ([165](#S5.E165
    "In 5.3.7 Neighborhood Component Analysis Loss ‣ 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))). Proxy-NCA loss functions define some proxy points in
    the embedding space of network and use them in the NCA loss to accelerate computation
    and make it memory-efficient (Movshovitz-Attias et al., [2017](#bib.bib87)). The
    proxies are representatives of classes in the embedding space and they can be
    defined in various ways. The simplest way is to define the proxy of every class
    as the mean of embedded points of that class. Of course, new mini-batches come
    during training. We can accumulate the embedded points of mini-batches and update
    the proxies after training the network by every mini-batch. Another approach for
    defining proxies is to cluster the embedded points into $c$ clusters (e.g., by
    K-means) and use the centroid of clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the set of proxies be denotes by $\mathcal{P}$ whose cardinality is the
    number of classes, i.e., $c$. Every embedded point is assigned to one of the proxies
    by (Movshovitz-Attias et al., [2017](#bib.bib87)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Pi(\textbf{f}(\boldsymbol{x}_{i})):=\arg\min_{\boldsymbol{\pi}\in\mathcal{P}}\&#124;\textbf{f}(\boldsymbol{x}_{i})-\boldsymbol{\pi}\&#124;_{2}^{2},$
    |  | (166) |'
  prefs: []
  type: TYPE_TB
- en: 'or we can assign every point to the proxy of its own class. Let $\boldsymbol{pi}_{j}$
    denote the proxy associated with the $j$-th class. The Proxy-NCA loss is the NCA
    loss, i.e. Eq. ([165](#S5.E165 "In 5.3.7 Neighborhood Component Analysis Loss
    ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), but using proxies
    (Movshovitz-Attias et al., [2017](#bib.bib87)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\Pi(\textbf{f}(\boldsymbol{x}_{i}^{p}))\big{)}\big{)}$
    |  | (167) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\times\Big{[}\sum_{j=1,j\neq c(\boldsymbol{x}_{i})}^{c}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})-\boldsymbol{\pi}_{j}\big{)}\big{)}\Big{]}^{-1}\Big{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'It is shown in (Movshovitz-Attias et al., [2017](#bib.bib87)) that the Proxy-NCA
    loss, i.e. Eq. ([167](#S5.E167 "In 5.3.8 Proxy Neighborhood Component Analysis
    Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), is an upper-bound
    on the NCA loss, i.e. Eq. ([165](#S5.E165 "In 5.3.7 Neighborhood Component Analysis
    Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")); hence, its minimization
    also achieves the goal of NCA. Comparing Eqs. ([165](#S5.E165 "In 5.3.7 Neighborhood
    Component Analysis Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    and ([167](#S5.E167 "In 5.3.8 Proxy Neighborhood Component Analysis Loss ‣ 5.3
    Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) shows that Proxy-NCA is faster
    and more efficient than NCA because it uses only proxies of negative classes rather
    than using all negative points in the mini-batch. Proxy-NCA has also been used
    in feature extraction from medical images (Teh & Taylor, [2020](#bib.bib108)).
    It is noteworthy that we can incorporate temperature scaling (Hinton et al., [2014](#bib.bib65))
    in the Proxy-NCA loss. The obtained loss is named Proxy-NCA++ (Teh et al., [2020](#bib.bib109))
    and is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\Pi(\textbf{f}(\boldsymbol{x}_{i}^{p}))\big{)}\times\frac{1}{\tau}\big{)}$
    |  | (168) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\times\Big{[}\sum_{j=1,j\neq c(\boldsymbol{x}_{i})}^{c}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})-\boldsymbol{\pi}_{j}\big{)}\times\frac{1}{\tau}\big{)}\Big{]}^{-1}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tau>0$ is the temperature which is a hyper-parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.9 Softmax Triplet Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a mini-batch containing points from $c$ classes where $c(\boldsymbol{x}_{i})$
    is the class index of $\boldsymbol{x}_{i}$ and $\mathcal{X}_{j}$ denotes the points
    of the $j$-th class in the mini-batch. We can use the softmax function or the
    Gaussian distribution for the probability that the point $\boldsymbol{x}_{i}$
    takes $\boldsymbol{x}_{j}$ as its neighbor. Similar to Eq. ([84](#S4.E84 "In 4.1
    Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) or Eq. ([165](#S5.E165 "In 5.3.7
    Neighborhood Component Analysis Loss ‣ 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")), we can have the softmax function used in NCA (Goldberger
    et al., [2005](#bib.bib53)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{ij}:=\frac{\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}\big{)}}{\sum_{k\neq
    i,k=1}^{b}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}\big{)}},\quad
    j\neq i.$ |  | (169) |'
  prefs: []
  type: TYPE_TB
- en: 'Another approach for the softmax form is to use inner product in the exponent
    (Ye et al., [2019](#bib.bib139)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{ij}:=\frac{\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})\big{)}}{\sum_{k=1,k\neq
    i}^{b}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{k})\big{)}},\quad
    j\neq i.$ |  | (170) |'
  prefs: []
  type: TYPE_TB
- en: 'The loss function for training the network can be the negative log-likelihood
    which can be called the softmax triplet loss (Ye et al., [2019](#bib.bib139)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}-\sum_{i=1}^{b}\Big{(}$
    | $\displaystyle\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\log(p_{ij})$
    |  | (171) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\sum_{\boldsymbol{x}_{j}\not\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\log(1-p_{ij})\Big{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This decreases and increases the distances of similar points and dissimilar
    points, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.10 Triplet Global Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The triplet global loss (Kumar BG et al., [2016](#bib.bib77)) uses the mean
    and variance of the anchor-positive pairs and anchor-negative pairs. It is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}(\sigma_{p}^{2}+\sigma_{n}^{2})+\lambda\,[\mu_{p}-\mu_{n}+m]_{+},$
    |  | (172) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda>0$ is the regularization parameter, $m>0$ is the margin, the
    means of pairs are:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mu_{p}:=\frac{1}{b}\sum_{i=1}^{b}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mu_{n}:=\frac{1}{b}\sum_{i=1}^{b}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'and the variances of pairs are:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sigma_{p}^{2}:=\frac{1}{b}\sum_{i=1}^{b}\Big{(}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}-\mu_{p}\Big{)}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\sigma_{n}^{2}:=\frac{1}{b}\sum_{i=1}^{b}\Big{(}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)}-\mu_{n}\Big{)}^{2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The first term of this loss minimizes the variances of anchor-positive and anchor-negative
    pairs. The second term, however, discriminates the anchor-positive pairs from
    the anchor-negative pairs. Hence, the negative points are separated from the positive
    points.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.11 Angular Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a triplet $(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})$,
    consider a triangle whose vertices are the anchor, positive, and negative points.
    To satisfy Eq. ([163](#S5.E163 "In 5.3.5 Triplet Loss ‣ 5.3 Metric Learning by
    Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) in the triplet loss, the angle at the
    vertex $\boldsymbol{x}_{i}^{n}$ should be small so the edge $d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)}$
    becomes larger than the edge $d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}$.
    Hence, we need to have and upper bound $\alpha>0$ on the angle at the vertex $\boldsymbol{x}_{i}^{n}$.
    If $\boldsymbol{x}_{i}^{c}:=(\boldsymbol{x}_{i}^{a}+\boldsymbol{x}_{i}^{p})/2$,
    the angular loss is defined to be (Wang et al., [2017](#bib.bib117)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,$ |  | (173) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\sum_{i=1}^{b}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}-4\tan^{2}\!\big{(}\alpha\,d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{c})\big{)}\big{)}\Big{]}_{+}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This loss reduces the distance of the anchor and positive points and increases
    the distance of anchor and $\boldsymbol{x}_{i}^{c}$ and the upper bound $\alpha$.
    This increases the distance of the anchor and negative points for discrimination
    of dissimilar points.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.12 SoftTriple Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If we normalize the points to have unit length, Eq. ([163](#S5.E163 "In 5.3.5
    Triplet Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) can
    be restated by using inner products:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}_{i}^{p})+m\leq\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}_{i}^{n}),$
    |  | (174) |'
  prefs: []
  type: TYPE_TB
- en: 'whose margin is not exactly equal to the margin in Eq. ([163](#S5.E163 "In
    5.3.5 Triplet Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")). Consider
    a Siamese network whose last layer’s weights are $\{\boldsymbol{w}_{l}\in\mathbb{R}^{p}\}_{l=1}^{c}$
    where $p$ is the dimensionality of the one-to-last layer and $c$ is the number
    of classes and the number of output neurons. We consider $k$ centers for the embedding
    of every class; hence, we define $\boldsymbol{w}_{l}^{j}\in\mathbb{R}^{p}$ as
    $\boldsymbol{w}_{l}$ for its $j$-th center. It is shown in (Qian et al., [2019](#bib.bib93))
    that softmax loss results in Eq. ([174](#S5.E174 "In 5.3.12 SoftTriple Loss ‣
    5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). Therefore, we can use the SoftTriple
    loss for training a Siamese network (Qian et al., [2019](#bib.bib93)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp(\lambda(s_{i,y_{i}}-\delta))$
    |  | (175) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\times\big{(}\exp(\lambda(s_{i,y_{i}}-\delta))+\sum_{l=1,l\neq
    y_{i}}^{c}\exp(\lambda s_{i,l})\big{)}^{-1}\Big{)},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda,\delta>0$ are hyper-parameters, $y_{i}$ is the label of $\boldsymbol{x}_{i}$,
    and:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle s_{i,l}:=\sum_{j=1}^{k}\frac{\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\boldsymbol{w}_{l}^{j}\big{)}}{\sum_{t=1}^{k}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\boldsymbol{w}_{l}^{t}\big{)}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\boldsymbol{w}_{l}^{k}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This loss increases and decreases the intra-class and inter-class distances,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.13 Fisher Siamese Losses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fisher Discriminant Analysis (FDA) (Fisher, [1936](#bib.bib33); Ghojogh et al.,
    [2019b](#bib.bib39)) decreases the intra-class variance and increases the inter-class
    variance by maximizing the Fisher criterion. This idea is very similar to the
    idea of loss functions for Siamese networks. Hence, we can combine the methods
    of FDA and Siamese loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a Siamese network whose last layer is denoted by the projection matrix
    $\boldsymbol{U}$. We consider the features of the one-to-last layer in the mini-batch.
    The covariance matrices of similar points and dissimilar points (one-to-last layer
    features) in the mini-batch are denoted by $\boldsymbol{S}_{W}$ and $\boldsymbol{S}_{B}$.
    These covariances become $\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U}$
    and $\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U}$, respectively, after
    the later layer’s projection because of the quadratic characteristic of covariance.
    As in FDA, we can maximize the Fisher criterion or equivalently minimize the negative
    Fisher criterion:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}~{}~{}~{}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'This problem is ill-posed because it increases the total covariance of embedded
    data to increase the term $\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})$.
    Hence, we add minimization of the total covariance as the regularization term:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}{}{}{}$ | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\epsilon\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{T}\boldsymbol{U}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\epsilon\in(0,1)$ is the regularization parameter and $\boldsymbol{S}_{T}$
    is the covariance of all points of the mini-batch in the one-to-last layer. The
    total scatter can be written as the summation of $\boldsymbol{S}_{W}$ and $\boldsymbol{S}_{B}$;
    hence:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})+\epsilon\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{T}\boldsymbol{U})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{W}-\boldsymbol{S}_{W}+\epsilon\boldsymbol{S}_{W}+\epsilon\boldsymbol{S}_{B})\boldsymbol{U}\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=(2-\lambda)\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\lambda\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda:=1-\epsilon$. Inspired by Eq. ([162](#S5.E162 "In 5.3.5 Triplet
    Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we can have the
    following loss, named the Fisher discriminant triplet loss (Ghojogh et al., [2020f](#bib.bib45)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}{}{}{}{}$ | $\displaystyle\Big{[}(2-\lambda)\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})$
    |  | (176) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}-\lambda\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})+m\Big{]}_{+},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $m>0$ is the margin. Backpropagating the error of this loss can update
    both $\boldsymbol{U}$ and other layers of network. Note that the summation over
    the mini-batch is integrated in the computation of covariance matrices $\boldsymbol{S}_{W}$
    and $\boldsymbol{S}_{B}$. Inspired by Eq. ([159](#S5.E159 "In 5.3.4 Contrastive
    Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we can also have
    the Fisher discriminant contrastive loss (Ghojogh et al., [2020f](#bib.bib45)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}{}{}{}$ | $\displaystyle(2-\lambda)\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})$
    |  | (177) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\big{[}\!-\lambda\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})+m\big{]}_{+}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note that the variable $y_{i}$ used in the contrastive loss (see Eq. ([158](#S5.E158
    "In 5.3.4 Contrastive Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"))) is already used in computation of the covariances $\boldsymbol{S}_{W}$
    and $\boldsymbol{S}_{B}$. There exist some other loss functions inspired by Fisher
    discriminant analysis but they are not used for Siamese networks. Those methods
    will be introduced in Section [5.4](#S5.SS4 "5.4 Deep Discriminant Analysis Metric
    Learning ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.14 Deep Adversarial Metric Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In deep adversarial metric learning (Duan et al., [2018](#bib.bib30)), negative
    points are generated in an adversarial learning (Goodfellow et al., [2014](#bib.bib54);
    Ghojogh et al., [2021b](#bib.bib47)). In this method, we have a generator $G(.)$
    which tries to generate negative points fooling the metric learning. Using triplet
    inputs $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})\}_{i=1}^{b}$,
    the loss function of generator is (Duan et al., [2018](#bib.bib30)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathcal{L}_{G}:=\sum_{i=1}^{b}\Big{(}\&#124;G(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})-\boldsymbol{x}_{i}^{a}\&#124;_{2}^{2}$
    |  | (178) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{1}\&#124;G(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})-\boldsymbol{x}_{i}^{n}\&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{2}\big{[}d(\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(G(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d(\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p}))+m\big{]}_{+}\Big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{1},\lambda_{2}>0$ are the regularization parameters. This loss
    makes the generated negative point close to the real negative point (to be negative)
    and the anchor point (for fooling metric learning adversarially). The Hinge loss
    makes the generated negative point different from the anchor and positive points
    so it also acts like a real negative. If $\mathcal{L}_{M}$ denotes any loss function
    for Siamese network, such as the triplet loss, the total loss function in deep
    adversarial metric learning is minimizing $\mathcal{L}_{G}+\lambda_{3}\mathcal{L}_{M}$
    where $\lambda_{3}>0$ is the regularization parameter (Duan et al., [2018](#bib.bib30)).
    It is noteworthy that there exists another adversarial metric learning which is
    not for Siamese networks but for cross-modal data (Xu et al., [2019a](#bib.bib128)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.15 Triplet Mining
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In every mini-batch containing data points from $c$ classes, we can select
    and use triplets of data points in different ways. For example, we can use all
    similar and dissimilar points for every anchor point as positive and negative
    points, respectively. Another approach is to only use some of the similar and
    dissimilar points within the mini-batch. These approaches for selecting and using
    triplets are called triplet mining (Sikaroudi et al., [2020a](#bib.bib102)). In
    the following, we review some of the most important triplet mining methods. We
    use triplet mining methods for the triplet loss, i.e., Eq. ([162](#S5.E162 "In
    5.3.5 Triplet Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")). Suppose
    $b$ is the mini-batch size, $c(\boldsymbol{x}_{i})$ is the class index of $\boldsymbol{x}_{i}$,
    $\mathcal{X}_{j}$ denotes the points of the $j$-th class in the mini-batch, and
    $\mathcal{X}$ denotes the data points in the mini-batch.'
  prefs: []
  type: TYPE_NORMAL
- en: '– Batch-all: Batch-all triplet mining (Ding et al., [2015](#bib.bib28)) considers
    every point in the mini-batch as an anchor point. All points in the mini-batch
    which are in the same class the anchor point are used as positive points. All
    points in the mini-batch which are in a different class from the class of anchor
    point are used as negative points:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (179) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Batch-all mining makes use of all data points in the mini-batch to utilize all
    available information.
  prefs: []
  type: TYPE_NORMAL
- en: '– Batch-hard: Batch-hard triplet mining (Hermans et al., [2017](#bib.bib64))
    considers every point in the mini-batch as an anchor point. The hardest positive,
    which is the farthest point from the anchor point in the same class, is used as
    the positive point. The hardest negative, which is the closest point to the anchor
    point from another class, is used as the negative point:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\Big{[}\max_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (180) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\min_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Bath-hard mining uses hardest points so that the network learns the hardest
    cases. By learning the hardest cases, other cases are expected to be learned properly.
    Learning the hardest cases can also be justified by the opposition-based learning
    (Tizhoosh, [2005](#bib.bib110)). Batch-hard mining has been used in many applications
    such as person re-identification (Wang et al., [2019](#bib.bib120)).
  prefs: []
  type: TYPE_NORMAL
- en: '– Batch-semi-hard: Batch-semi-hard triplet mining (Schroff et al., [2015](#bib.bib100))
    considers every point in the mini-batch as an anchor point. All points in the
    mini-batch which are in the same class the anchor point are used as positive points.
    The hardest negative (closest to the anchor point from another class), which is
    farther than the positive point, is used as the negative point:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (181) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\min_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{\{}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}\,&#124;\,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}>d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}\big{\}}+m\Big{]}_{+}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '– Easy-positive: Easy-positive triplet mining (Xuan et al., [2020](#bib.bib130))
    considers every point in the mini-batch as an anchor point. The easiest positive
    (closest to the anchor point from the same class) is used as the positive point.
    All points in the mini-batch which are in a different class from the class of
    anchor point are used as negative points:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}\min_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (182) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'We can use this triplet mining approach in NCA loss function such as in Eq.
    ([170](#S5.E170 "In 5.3.9 Softmax Triplet Loss ‣ 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")). For example, we can have (Xuan et al., [2020](#bib.bib130)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\bigg{(}\min_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (183) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}\times\Big{(}\min_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{k})\big{)}\Big{)}^{-1}\bigg{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the embeddings for all points of the mini-batch are normalized to have
    length one.
  prefs: []
  type: TYPE_NORMAL
- en: '– Lifted embedding loss: The lifted embedding loss (Oh Song et al., [2016](#bib.bib91))
    is related to the anchor-positive distance and the smallest (hardest) anchor-negative
    distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}{}{}{}$ | $\displaystyle\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{(}\Big{[}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))$
    |  | (184) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\max\Big{(}\max_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{\{}m-d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k}))\big{\}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\max_{\boldsymbol{x}_{l}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{j})}}\big{\{}m-d(\textbf{f}(\boldsymbol{x}_{j}),\textbf{f}(\boldsymbol{x}_{l}))\big{\}}\Big{)}\Big{]}_{+}\Big{)}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'This loss is using triplet mining because of using extreme distances. Alternatively,
    another version of this loss function uses logarithm and exponential operators
    (Oh Song et al., [2016](#bib.bib91)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}$ | $\displaystyle\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{(}\Big{[}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))$
    |  | (185) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\log\Big{(}\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}m-d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k}))\big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\sum_{\boldsymbol{x}_{l}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{j})}}\exp\big{(}m-d(\textbf{f}(\boldsymbol{x}_{j}),\textbf{f}(\boldsymbol{x}_{l}))\big{)}\Big{)}\Big{]}_{+}\Big{)}^{2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '– Hard mining center-triplet loss: Let the mini-batch contain data points from
    $c$ classes. Hard mining center–triplet loss (Lv et al., [2019](#bib.bib81)) considers
    the mean of every class as an anchor point. The hardest (farthest) positive point
    and the hardest (closest) negative point are used in this loss as (Lv et al.,
    [2019](#bib.bib81)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{l=1}^{c}\Big{[}\max_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\bar{\boldsymbol{x}}^{l})}}d\big{(}\textbf{f}(\bar{\boldsymbol{x}}^{l}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (186) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\min_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\bar{\boldsymbol{x}}^{l})}}d\big{(}\textbf{f}(\bar{\boldsymbol{x}}^{l}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\bar{\boldsymbol{x}}^{l}$ denotes the mean of the $l$-th class.
  prefs: []
  type: TYPE_NORMAL
- en: '– Triplet loss with cross-batch memory: A version of triplet loss can be (Wang
    et al., [2020a](#bib.bib121)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\bigg{(}-\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})$
    |  | (187) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{k})\bigg{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'This triplet loss can use a cross-batch memory where we accumulate a few latest
    mini-batches. Every coming mini-batch updates the memory. Let the capacity of
    the memory be $w$ points and the mini-batch size be $b$. Let $\widetilde{\boldsymbol{x}}_{i}$
    denote the $i$-th data point in the memory. The triplet loss with cross-batch
    memory is defined as (Wang et al., [2020a](#bib.bib121)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\bigg{(}-\sum_{\widetilde{\boldsymbol{x}}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\widetilde{\boldsymbol{x}}_{j})$
    |  | (188) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\widetilde{\boldsymbol{x}}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\widetilde{\boldsymbol{x}}_{k})\bigg{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which takes the positive and negative points from the memory rather than from
    the coming mini-batch.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.16 Triplet Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rather than using the extreme (hardest or easiest) positive and negative points
    (Sikaroudi et al., [2020a](#bib.bib102)), we can sample positive and negative
    points from the points in the mini-batch or from some distributions. There are
    several approaches for the positive and negative points to be sampled (Ghojogh,
    [2021](#bib.bib36)):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampled by extreme distances of points,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampled randomly from classes,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampled by distribution but from existing points,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampled stochastically from distributions of classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These approaches are used for triplet sampling. The first approach was introduced
    in Section [5.3.15](#S5.SS3.SSS15 "5.3.15 Triplet Mining ‣ 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey"). The first, second, and third approaches
    sample the positive and negative points from the set of points in the mini-batch.
    This type of sampling is called survey sampling (Ghojogh et al., [2020e](#bib.bib44)).
    The third and fourth approaches sample points from distributions stochastically.
    In the following, we introduce some of the triplet sampling methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '– Distance weighted sampling: Distance weighted sampling (Wu et al., [2017](#bib.bib125))
    is a method in the third approach, i.e., sampling by distribution but from existing
    points. The distribution of the pairwise distances is proportional to (Wu et al.,
    [2017](#bib.bib125)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}$
    | $\displaystyle\sim\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}^{p-2}\times$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\Big{(}1-0.25\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}^{2}\Big{)}^{(b-3)/2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $b$ is the number of points in the mini-batch and $p$ is the dimensionality
    of embedding space (i.e., the number of neurons in the last layer of the Siamese
    network). In every mini-batch, we consider every point once as an anchor point.
    For an anchor point, we consider all points of the mini-batch which are in a different
    class as candidates for the negative point. We sample a negative point, denoted
    by $\boldsymbol{x}_{*}^{n}$ from these candidates (Wu et al., [2017](#bib.bib125)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{x}_{*}^{n}\sim\min\Big{(}\lambda,\mathbb{P}^{-1}\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}\Big{)},\quad\forall
    j\neq i,$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda>0$ is a hyperparameter to ensure that all candidates have a
    chance to be chosen. This sampling is performed for every mini-batch. The loss
    function in distance weighted sampling is (Wu et al., [2017](#bib.bib125)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (189) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{*}^{n})\big{)}+m\Big{]}_{+}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '– Sampling by Bayesian updating theorem: We can sample triplets from distributions
    of classes which is the forth approach of sampling, mentioned above. One method
    for this sampling is using the Bayesian updating theorem (Sikaroudi et al., [2021](#bib.bib104))
    which is updating the posterior by the Bayes’ rule from some new data. In this
    method, we assume $p$-dimensional Gaussian distribution for every class in the
    embedding space where $p$ is the dimensionality of embedding space. We accumulate
    the embedded points for every class when the new mini-batches are introduced to
    the network. The distributions of classes are updated based on both the existing
    points available so far and the new-coming data points. It can be shown that the
    posterior of mean and covariance of a Gaussian distribution is a normal inverse
    Wishart distribution (Murphy, [2007](#bib.bib88)). The mean and covariance of
    a Gaussian distribution have a generalized Student-t distribution and inverse
    Wishart distribution, respectively (Murphy, [2007](#bib.bib88)). Let the so-far
    available data have sample size $n_{0}$, mean $\boldsymbol{\mu}^{0}$, and covariance
    $\boldsymbol{\Sigma}^{0}$. Also, let the newly coming data have sample size $n^{\prime}$,
    mean $\boldsymbol{\mu}^{\prime}$, and covariance $\boldsymbol{\Sigma}^{\prime}$.
    We update the mean and covariance by expectation of these distributions (Sikaroudi
    et al., [2021](#bib.bib104)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\mu}^{0}\leftarrow\mathbb{E}(\boldsymbol{\mu}\,&#124;\,\boldsymbol{x}^{0})=\frac{n^{\prime}\boldsymbol{\mu}^{\prime}+n_{0}\boldsymbol{\mu}^{0}}{n^{\prime}+n_{0}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{0}\leftarrow\mathbb{E}(\boldsymbol{\Sigma}\,&#124;\,\boldsymbol{x}^{0})=\frac{\boldsymbol{\Upsilon}^{-1}}{n^{\prime}\!+\!n_{0}\!-\!p\!-\!1},~{}~{}~{}\forall\,n^{\prime}\!+\!n_{0}\!>\!p\!+\!1,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{\Upsilon}:=$ | $\displaystyle\,n^{\prime}\boldsymbol{\Sigma}^{\prime}+n_{0}\boldsymbol{\Sigma}^{0}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\frac{n^{\prime}_{1}n_{0}}{n^{\prime}_{1}+n_{0}}(\boldsymbol{\mu}^{0}-\boldsymbol{\mu}^{\prime})(\boldsymbol{\mu}^{0}-\boldsymbol{\mu}^{\prime})^{\top}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The updated mean and covariance are used for Gaussian distributions of the classes.
    Then, we sample triplets from the distributions of classes rather than from the
    points of mini-batch. We consider every point of the new mini-batch as an anchor
    point and sample a positive point from the distribution of the same class. We
    sample $c-1$ negative points from the distributions of $c-1$ other classes. If
    this triplet sampling procedure is used with triplet and contrastive loss functions,
    the approach is named Bayesian Updating with Triplet loss (BUT) and Bayesian Updating
    with NCA loss (BUNCA) (Sikaroudi et al., [2021](#bib.bib104)).
  prefs: []
  type: TYPE_NORMAL
- en: '– Hard negative sampling: Let the anchor, positive, and negative points be
    denoted by $\boldsymbol{x}^{a}$, $\boldsymbol{x}^{p}$, and $\boldsymbol{x}^{n}$,
    respectively. Consider the following distributions for the negative and positive
    points (Robinson et al., [2021](#bib.bib95)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}(\boldsymbol{x}^{n})\propto\alpha\mathbb{P}_{n}(\boldsymbol{x}^{n})+(1-\alpha)\mathbb{P}_{p}(\boldsymbol{x}^{n}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbb{P}_{n}(\boldsymbol{x})\propto\exp\big{(}\beta\textbf{f}(\boldsymbol{x}^{a})^{\top}\textbf{f}(\boldsymbol{x})\big{)}\,\mathbb{P}(\boldsymbol{x}&#124;c(\boldsymbol{x})\neq
    c(\boldsymbol{x}^{a})),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbb{P}_{p}(\boldsymbol{x})\propto\exp\big{(}\beta\textbf{f}(\boldsymbol{x}^{a})^{\top}\textbf{f}(\boldsymbol{x})\big{)}\,\mathbb{P}(\boldsymbol{x}&#124;c(\boldsymbol{x})=c(\boldsymbol{x}^{a})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\alpha\in(0,1)$ is a hyper-parameter. The loss function with hard negative
    sampling is (Robinson et al., [2021](#bib.bib95)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\mathbb{E}_{\boldsymbol{x}^{p}\sim\mathbb{P}_{p}(\boldsymbol{x})}\log\bigg{(}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}^{p})\big{)}$
    |  | (190) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\Big{(}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}^{p})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\mathbb{E}_{\boldsymbol{x}^{n}\sim\mathbb{P}(\boldsymbol{x}^{n})}\big{[}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}^{n})\big{)}\big{]}\Big{)}^{-1}\bigg{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where positive and negative points are sampled from positive and negative distributions
    defined above. The expectations can be estimated using the Monte Carlo approximation
    (Ghojogh et al., [2020e](#bib.bib44)). This time of triplet sampling is a method
    in the fourth type of triplet sampling, i.e., sampling stochastically from distributions
    of classes.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Deep Discriminant Analysis Metric Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep discriminant analysis metric learning methods use the idea of Fisher discriminant
    analysis (Fisher, [1936](#bib.bib33); Ghojogh et al., [2019b](#bib.bib39)) in
    deep learning, for learning an embedding space which separates classes. Some of
    these methods are deep probabilistic discriminant analysis (Li et al., [2019](#bib.bib79)),
    discriminant analysis with virtual samples (Kim & Song, [2021](#bib.bib74)), Fisher
    Siamese losses (Ghojogh et al., [2020f](#bib.bib45)), and deep Fisher discriminant
    analysis (Díaz-Vico et al., [2017](#bib.bib27); Díaz-Vico & Dorronsoro, [2019](#bib.bib26)).
    The Fisher Siamese losses were already introduced in Section [5.3.13](#S5.SS3.SSS13
    "5.3.13 Fisher Siamese Losses ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Deep Probabilistic Discriminant Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep probabilistic discriminant analysis (Li et al., [2019](#bib.bib79)) minimizes
    the inverse Fisher criterion:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\frac{\mathbb{E}[\textbf{tr}(\text{cov}(\textbf{f}(\boldsymbol{x})&#124;y))]}{\textbf{tr}(\text{cov}(\mathbb{E}[\textbf{f}(\boldsymbol{x})&#124;y]))}=\frac{\sum_{i=1}^{b}\mathbb{E}[\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i})]}{\sum_{i=1}^{b}\text{var}(\mathbb{E}[\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i}])}$
    |  | (191) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(a)}{=}\frac{\sum_{i=1}^{b}\mathbb{E}[\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i})]}{\sum_{i=1}^{b}\big{(}\text{var}(\textbf{f}(\boldsymbol{x}_{i}))-\mathbb{E}[\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i})]\big{)}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(b)}{=}\frac{\sum_{i=1}^{b}\sum_{l=1}^{c}\mathbb{P}(y=l)\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i}=l)}{\sum_{i=1}^{b}\big{(}\text{var}(\textbf{f}(\boldsymbol{x}_{i}))-\sum_{l=1}^{c}\mathbb{P}(y=l)\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i}=l)\big{)}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $b$ is the mini-batch size, $c$ is the number of classes, $y_{i}$ is the
    class label of $\boldsymbol{x}_{i}$, $\text{cov}(.)$ denotes covariance, $\text{var}(.)$
    denotes variance, $\mathbb{P}(y=l)$ is the prior of the $l$-th class (estimated
    by the ratio of class population to the total number of points in the mini-batch),
    $(a)$ is because of the law of total variance, and $(b)$ is because of the definition
    of expectation. The numerator and denominator represent the intra-class and inter-class
    variances, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Discriminant Analysis with Virtual Samples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In discriminant analysis metric learning with virtual samples (Kim & Song, [2021](#bib.bib74)),
    we consider any backbone network until the one-to-last layer of neural network
    and a last layer with linear activation function. Let the outputs of the one-to-last
    layer be denoted by $\{\textbf{f}^{\prime}(\boldsymbol{x}_{i})\}_{i=1}^{b}$ and
    the weights of the last layer be $\boldsymbol{U}$. We compute the intra-class
    scatter $\boldsymbol{S}_{W}$ and inter-class scatter $\boldsymbol{S}_{B}$ for
    the one-to-last layer’s features $\{\textbf{f}^{\prime}(\boldsymbol{x}_{i})\}_{i=1}^{b}$.
    If we see the last layer as a Fisher discriminant analysis model with projection
    matrix $\boldsymbol{U}$, the solution is the eigenvalue problem (Ghojogh et al.,
    [2019a](#bib.bib38)) for $\boldsymbol{S}_{W}^{-1}\boldsymbol{S}_{B}$. Let $\lambda_{j}$
    denote the $j$-th eigenvalue of this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume $\mathcal{S}_{b}$ and $\mathcal{D}_{b}$ denote the similar and dissimilar
    points in the mini-batch where $|\mathcal{S}_{b}|=|\mathcal{D}_{b}|=q$. We define
    (Kim & Song, [2021](#bib.bib74)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{g}_{p}:=[\exp(-\textbf{f}^{\prime}(\boldsymbol{x}_{i})^{\top}\textbf{f}^{\prime}(\boldsymbol{x}_{j}))\,&#124;\,(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}_{b}]^{\top}\in\mathbb{R}^{q},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{g}_{n}:=[\exp(-\textbf{f}^{\prime}(\boldsymbol{x}_{i})^{\top}\textbf{f}^{\prime}(\boldsymbol{x}_{j}))\,&#124;\,(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}_{b}]^{\top}\in\mathbb{R}^{q},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle s_{ctr}:=\frac{1}{2q}\sum_{i=1}^{q}\big{(}\boldsymbol{g}_{p}(i)+\boldsymbol{g}_{n}(i)\big{)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{g}(i)$ is the $i$-th element of $\boldsymbol{g}$. We sample
    $q$ numbers, namely virtual samples, from the uniform distribution $U(s_{ctr}-\epsilon\bar{\lambda},s_{ctr}+\epsilon\bar{\lambda})$
    where $\epsilon$ is a small positive number and $\bar{\lambda}$ is the mean of
    eigenvalues $\lambda_{j}$’s. The $q$ virtual samples are put in a vector $\boldsymbol{r}\in\mathbb{R}^{q}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function for discriminant analysis with virtual samples is (Kim &
    Song, [2021](#bib.bib74)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta,\boldsymbol{U}}{\text{minimize}}~{}~{}~{}\frac{1}{q}\sum_{i=1}^{q}\Big{[}\frac{1}{q}\,\boldsymbol{g}_{p}(i)\,\&#124;\boldsymbol{r}\&#124;_{1}-\frac{1}{q}\,\boldsymbol{g}_{n}(i)\,\&#124;\boldsymbol{r}\&#124;_{1}+m\Big{]}_{+}$
    |  | (192) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-10^{-5}\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\|.\|_{1}$ is the $\ell_{1}$ norm, $[.]_{+}:=\max(.,0)$, $m>0$ is the
    margin, and the second term is maximization of the Fisher criterion.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 Deep Fisher Discriminant Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is shown in (Hart et al., [2000](#bib.bib61)) that the solution to the following
    least squares problem is equivalent to the solution of Fisher discriminant analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\boldsymbol{w}_{0}\in\mathbb{R}^{c},\boldsymbol{W}\in\mathbb{R}^{d\times
    c}}{\text{minimize}}~{}~{}~{}\frac{1}{2}\&#124;\boldsymbol{Y}-\boldsymbol{1}_{n\times
    1}\boldsymbol{w}_{0}^{\top}-\boldsymbol{X}\boldsymbol{W}\&#124;_{F}^{2},$ |  |
    (193) |'
  prefs: []
  type: TYPE_TB
- en: where $\|.\|_{F}$ is the Frobenius norm, $\boldsymbol{X}\in\mathbb{R}^{n\times
    d}$ is the row-wise stack of data points, $\boldsymbol{Y}:=\boldsymbol{H}\boldsymbol{E}\boldsymbol{\Pi}^{-(1/2)}\in\mathbb{R}^{n\times
    c}$ where $\boldsymbol{H}:=\boldsymbol{I}-(1/n)\boldsymbol{1}\boldsymbol{1}^{\top}\in\mathbb{R}^{n\times
    n}$ is the centering matrix, $\boldsymbol{E}\in\{0,1\}^{n\times c}$ is the one-hot-encoded
    labels stacked row-wise, $\boldsymbol{\Pi}\in\mathbb{R}^{c\times c}$ is the diagonal
    matrix whose $(l,l)$-th element is the cardinality of the $l$-th class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Fisher discriminant analysis (Díaz-Vico et al., [2017](#bib.bib27); Díaz-Vico
    & Dorronsoro, [2019](#bib.bib26)) implements Eq. ([193](#S5.E193 "In 5.4.3 Deep
    Fisher Discriminant Analysis ‣ 5.4 Deep Discriminant Analysis Metric Learning
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) by a nonlinear neural network with loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\frac{1}{2}\&#124;\boldsymbol{Y}-\textbf{f}(\boldsymbol{X};\theta)\&#124;_{F}^{2},$
    |  | (194) |'
  prefs: []
  type: TYPE_TB
- en: where $\theta$ is the weights of network, $\boldsymbol{X}\in\mathbb{R}^{n\times
    d}$ denotes the row-wise stack of points in the mini-batch of size $b$, $\boldsymbol{Y}:=\boldsymbol{H}\boldsymbol{E}\boldsymbol{\Pi}^{-(1/2)}\in\mathbb{R}^{b\times
    c}$ is computed in every mini-batch, and $\textbf{f}(.)\in\mathbb{R}^{b\times
    c}$ is the row-wise stack of output embeddings of the network. After training,
    the output $\textbf{f}(\boldsymbol{x})$ is the embedding for the input point $\boldsymbol{x}$.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Multi-Modal Deep Metric Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data has several modals where a separate set of features is available for every
    modality of data. In other words, we can have several features for every data
    point. Note that the dimensionality of features may differ. Multi-modal deep metric
    learning (Roostaiyan et al., [2017](#bib.bib96)) addresses this problem in metric
    learning. Let $m$ denote the number of modalities. Consider $m$ stacked autoencoders
    each of which is for one of the modalities. The $l$-th autoencoder gets the $l$-th
    modality of the $i$-th data point, denoted by $\boldsymbol{x}_{i}^{l}$, and reconstructs
    it as output, denoted by $\widehat{\boldsymbol{x}}_{i}^{l}$. The embedding layer,
    or the layer between encoder and decoder, is shared between all $m$ autoencoders.
    We denote the output of this shared embedding layer by $\textbf{f}(\boldsymbol{x}_{i})$.
    The loss function for training the $m$ stacked autoencoders with the shared embedding
    layer can be (Roostaiyan et al., [2017](#bib.bib96)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}\sum_{l=1}^{m}\&#124;\boldsymbol{x}_{i}^{l}-\widehat{\boldsymbol{x}}_{i}^{l}\&#124;_{2}^{2}$
    |  | (195) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\lambda_{1}\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{[}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))-m_{1}\big{]}_{+}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\lambda_{2}\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{[}-d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))+m_{2}\big{]}_{+},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{1},\lambda_{2}>0$ are the regularization parameters and $m_{1},m_{2}>0$
    are the margins. The first term is the reconstruction loss and the second and
    third terms are for metric learning which collapses each class to a margin $m_{1}$
    and discriminates classes by a margin $m_{2}$. This loss function is optimized
    in a stacked autoencoder setup (Hinton & Salakhutdinov, [2006](#bib.bib67); Wang
    et al., [2014](#bib.bib119)). Then, it is fine-tuned by backpropagation (Ghojogh
    et al., [2021f](#bib.bib51)). After training, the embedding layer can be used
    for embedding data points. Note that another there exists another multi-modal
    deep metric learning, which is (Xu et al., [2019a](#bib.bib128)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Geometric Metric Learning by Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There exist some works, such as (Huang & Van Gool, [2017](#bib.bib70)), (Hauser,
    [2017](#bib.bib63)), and (Hajiabadi et al., [2019](#bib.bib59)), which have implemented
    neural networks on the Riemannian manifolds. Layered geometric learning (Hajiabadi
    et al., [2019](#bib.bib59)) implements Geometric Mean Metric Learning (GMML) (Zadeh
    et al., [2016](#bib.bib141)) (recall Section [3.7.1](#S3.SS7.SSS1 "3.7.1 Geometric
    Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    in a neural network framework. In this method, every layer of network is a metric
    layer which projects the output of its previous layer onto the subspace of its
    own metric (see Proposition [2](#Thmproposition2 "Proposition 2 (Projection in
    metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey") and Proposition [8](#S2.E8 "In Proposition 2 (Projection in metric
    learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: For the $l$-th layer of network, we denote the weight matrix (i.e., the projection
    matrix of metric) and the output of layer for the $i$-th data point by $\boldsymbol{U}_{l}$
    and $\boldsymbol{x}_{i,l}$, respectively. Hence, the metric in the $l$-th layer
    models $\|\boldsymbol{x}_{i,l}-\boldsymbol{x}_{j,l}\|_{\boldsymbol{U}_{l}\boldsymbol{U}_{l}^{\top}}$.
    Consider the dataset of $n$ points $\boldsymbol{X}\in\mathbb{R}^{d\times n}$.
    We denote the output of the $l$-th layer by $\boldsymbol{X}_{l}\in\mathbb{R}^{d\times
    n}$. The projection of a layer onto its metric subspace is $\boldsymbol{X}_{l}=\boldsymbol{U}_{l}^{\top}\boldsymbol{X}_{l-1}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every layer solves the optimization problem of GMML (Zadeh et al., [2016](#bib.bib141)),
    i.e., Eq. ([61](#S3.E61 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). For this, we start from the
    first layer and proceed to the last layer by feed-propagation. The $l$-th layer
    computes $\boldsymbol{\Sigma}_{\mathcal{S}}$ and $\boldsymbol{\Sigma}_{\mathcal{D}}$
    for $\boldsymbol{X}_{l-1}$ by Eq. ([14](#S3.E14 "In Proof. ‣ 3.1.1 The First Spectral
    Method ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")). Then, the solution
    of optimization ([61](#S3.E61 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) is computed which is the Eq.
    ([65](#S3.E65 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), i.e., $\boldsymbol{W}_{l}=\boldsymbol{\Sigma}_{\mathcal{S}}^{-1}\sharp_{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}\big{(}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\big{)}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}$.
    Then, using Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), we decompose the obtained $\boldsymbol{W}_{l}$
    to find $\boldsymbol{U}_{l}$. Then, data points are projected onto the metric
    subspace as $\boldsymbol{X}_{l}=\boldsymbol{U}_{l}^{\top}\boldsymbol{X}_{l-1}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want the output of layers lie on the positive semi-definite manifold,
    the activation function of every layer can be projection onto the positive semi-definite
    cone (Ghojogh et al., [2021c](#bib.bib48)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{X}_{l}:=\boldsymbol{V}\,\textbf{diag}(\max(\lambda_{1},0),\dots,\max(\lambda_{d},0))\,\boldsymbol{V}^{\top},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{V}$ and $\{\lambda_{1},\dots,\lambda_{d}\}$ are the eigenvectors
    and eigenvalues of $\boldsymbol{X}_{l}$, respectively. This activation function
    is called the eigenvalue rectification layer in (Huang & Van Gool, [2017](#bib.bib70)).
    Finally, it is noteworthy that there is another work, named backprojection (Ghojogh
    et al., [2020d](#bib.bib43)), which has similar idea but in the Euclidean and
    Hilbert spaces and not in the Riemannian space.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Few-shot Metric Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Few-shot learning refers to learning from a few data points rather than from
    a large enough dataset. Few-shot learning is used for domain generalization to
    be able to use for unseen data in the test phase (Wang et al., [2020b](#bib.bib122)).
    The training phase of few-shot learning is episodic where in every iteration or
    so-called episode of training, we have a support set and a query set. In other
    words, the training dataset is divided into mini-batches where every mini-batch
    contains a support set and a query set (Triantafillou et al., [2020](#bib.bib111)).
    Consider a training dataset with $c_{\text{tr}}$ classes and a test dataset with
    $c_{\text{te}}$ classes. As mentioned before, test and training datasets are usually
    disjoint in few-shot learning so it is useful for domain generalization. In every
    episode, also called the task or the mini-batch, we train using some (and not
    all) training classes by randomly sampling from classes.
  prefs: []
  type: TYPE_NORMAL
- en: The support set is $\mathcal{S}_{s}:=\{(\boldsymbol{x}_{s,i},y_{s,i})\}_{i=1}^{|\mathcal{S}_{s}|}$
    where $\boldsymbol{x}$ and $y$ denote the data point and its label, respectively.
    The query set is $\mathcal{S}_{q}:=\{(\boldsymbol{x}_{q,i},y_{q,i})\}_{i=1}^{|\mathcal{S}_{q}|}$.
    The training data of every episode (mini-batch) is the union of the support and
    query sets. At every episode, we randomly sample $c_{s}$ classes out of the total
    $c_{\text{tr}}$ classes of training dataset, where we usually have $c_{s}\ll c_{\text{tr}}$.
    Then, we sample $k_{s}$ training data points from these $c_{s}$ selected classes.
    These $c_{s}\times k_{s}=|\mathcal{S}_{s}|$ form the support set. This few-shot
    setup is called $c_{s}$-way, $k_{s}$-shot in which the support set contains $c_{s}$
    classes and $k_{s}$ points in every class. The number of classes and every class’s
    points in the query set of every episode may or may not be the same as in the
    support set.
  prefs: []
  type: TYPE_NORMAL
- en: In every episode of the training phase of few-shot learning, we update the network
    weights by back-propagating error using the support set. These updated weights
    are not finalized yet. We feed the query set to the network with the updated weights
    and back-propagate error using the query set. This second back-propagation with
    the query set updates the weights of network finally at the end of episode. In
    other words, the query set is used to evaluate how good the update by support
    set are. This learning procedure for few-shot learning is called meta-learning
    (Finn et al., [2017](#bib.bib32)).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several family of methods for few-shot learning, one of which is
    some deep metric learning methods. Various metric learning methods have been proposed
    for learning from few-shot data. For example, Siamese network, introduced in Section
    [5.3](#S5.SS3 "5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"), has
    been used for few-shot learning (Koch et al., [2015](#bib.bib75); Li et al., [2020](#bib.bib80)).
    In the following, we introduce two metric learning methods for few-shot learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.7.1 Multi-scale Metric Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multi-scale metric learning (Jiang et al., [2020](#bib.bib71)) learns the embedding
    space by learning multiple scales of middle features in the training process.
    It has several steps. In the first step, we use a pre-trained network with multiple
    output layers which produce several different scales of features for both the
    support and query sets. In the second step, within every scale of support set,
    we take average of the $k_{s}$ features in every class. This gives us $c_{s}$
    features for every scale in the support set. This and the features of the query
    set are fed to the third step. In the third step, we feed every scale to a sub-network
    where larger scales are fed to sub-networks with more number of layers as they
    contain more information to process. These sub-networks are concatenated to give
    a scalar output for every data point with multiple scales of features. Hence,
    we obtain a scalar score for every data point in the support and query sets. Finally,
    a combination of a classification loss function, such as the cross-entropy loss
    (see Eq. ([153](#S5.E153 "In 5.2.4 Cross-entropy Loss ‣ 5.2 Supervised Metric
    Learning by Supervised Loss Functions ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))), and triplet loss (see Eq. [162](#S5.E162
    "In 5.3.5 Triplet Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    is used in the support-query setup explained before.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.7.2 Metric Learning with Continuous Similarity Scores
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another few-shot metric learning is (Xu et al., [2019b](#bib.bib129)) which
    takes pairs of data points as the input support and query sets. For the pair $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$,
    consider binary similarity score, $y_{ij}$, defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle y_{ij}:=\left\{\begin{array}[]{ll}1&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\
    0&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$
    |  | (198) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{S}$ and $\mathcal{D}$ denote the sets of similar and dissimilar
    points, respectively. We can define continuous similarity score, $y^{\prime}_{ij}$,
    as (Xu et al., [2019b](#bib.bib129)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle y^{\prime}_{ij}:=\left\{\begin{array}[]{ll}(\beta-1)d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})+1&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\ -\alpha d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})+\alpha&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},\end{array}\right.$ |  |
    (201) |'
  prefs: []
  type: TYPE_TB
- en: 'where $0<\alpha<\beta<1$ and $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$ is
    the normalized squared Euclidean distance (we normalize distances within every
    mini-batch). The ranges of these continuous similarities are:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle y^{\prime}_{ij}\in\left\{\begin{array}[]{ll}\,[\beta,1]&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\ \,[0,\alpha]&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'In every episode (mini-batch), the pairs are fed to a network with several
    feature vector outputs. For every pair $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$,
    these feature vectors are fed to another network which outputs a scalar similarity
    score $s_{ij}$. The loss function of metric learning in this method is (Xu et al.,
    [2019b](#bib.bib129)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\theta}{\text{maximize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{X}}(1+\lambda)(s_{ij}-y^{\prime}_{ij})^{2},$
    |  | (202) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | subject to |  | $\displaystyle\beta\leq s_{ij},y^{\prime}_{ij}\leq
    1\quad\text{ if }\quad y_{ij}=1,$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle 0\leq s_{ij},y^{\prime}_{ij}\leq\alpha\quad\text{ if
    }\quad y_{ij}=0,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda>0$ is the regularization parameter and $\mathcal{X}$ is the mini-batch
    of the support or query set depending on whether it is the phase of support or
    query.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a tutorial and survey on spectral, probabilistic, and deep metric learning.
    We started with defining distance metric. In spectral methods, we covered methods
    using scatters of data, methods using Hinge loss, locally linear metric adaptation,
    kernel methods, geometric methods, and adversarial metric learning. In probabilistic
    category, we covered collapsing classes, neighborhood component analysis, Bayesian
    metric learning, information theoretic methods, and empirical risk minimization
    approaches. In deep learning methods, we explain reconstruction autoencoders,
    supervised loss functions, Siamese networks, deep discriminant analysis methods,
    multi-modal learning, geometric deep metric learning, and few-shot metric learning.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Absil et al. (2009) Absil, P-A, Mahony, Robert, and Sepulchre, Rodolphe. *Optimization
    algorithms on matrix manifolds*. Princeton University Press, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alipanahi et al. (2008) Alipanahi, Babak, Biggs, Michael, and Ghodsi, Ali. Distance
    metric learning vs. Fisher discriminant analysis. In *Proceedings of the 23rd
    national conference on Artificial intelligence*, volume 2, pp.  598–603, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arsigny et al. (2007) Arsigny, Vincent, Fillard, Pierre, Pennec, Xavier, and
    Ayache, Nicholas. Geometric means in a novel vector space structure on symmetric
    positive-definite matrices. *SIAM journal on matrix analysis and applications*,
    29(1):328–347, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baghshah & Shouraki (2009) Baghshah, Mahdieh Soleymani and Shouraki, Saeed Bagheri.
    Semi-supervised metric learning using pairwise constraints. In *Twenty-First International
    Joint Conference on Artificial Intelligence*, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baghshah & Shouraki (2010) Baghshah, Mahdieh Soleymani and Shouraki, Saeed Bagheri.
    Kernel-based metric learning for semi-supervised clustering. *Neurocomputing*,
    73(7-9):1352–1361, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bar-Hillel et al. (2003) Bar-Hillel, Aharon, Hertz, Tomer, Shental, Noam, and
    Weinshall, Daphna. Learning distance functions using equivalence relations. In
    *Proceedings of the 20th international conference on machine learning (ICML-03)*,
    pp.  11–18, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bar-Hillel et al. (2005) Bar-Hillel, Aharon, Hertz, Tomer, Shental, Noam, Weinshall,
    Daphna, and Ridgeway, Greg. Learning a mahalanobis metric from equivalence constraints.
    *Journal of machine learning research*, 6(6), 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belkin & Niyogi (2001) Belkin, Mikhail and Niyogi, Partha. Laplacian eigenmaps
    and spectral techniques for embedding and clustering. In *Advances in neural information
    processing systems*, volume 14, pp.  585–591, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belkin & Niyogi (2002) Belkin, Mikhail and Niyogi, Partha. Laplacian eigenmaps
    and spectral techniques for embedding and clustering. In *Advances in neural information
    processing systems*, pp. 585–591, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellet et al. (2013) Bellet, Aurélien, Habrard, Amaury, and Sebban, Marc. A
    survey on metric learning for feature vectors and structured data. *arXiv preprint
    arXiv:1306.6709*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellet et al. (2015) Bellet, Aurélien, Habrard, Amaury, and Sebban, Marc. Metric
    learning. *Synthesis Lectures on Artificial Intelligence and Machine Learning*,
    9(1):1–151, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhatia (2007) Bhatia, Rajendra. *Positive definite matrices*. Princeton university
    press, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhutani et al. (2018) Bhutani, Mukul, Jawanpuria, Pratik, Kasai, Hiroyuki, and
    Mishra, Bamdev. Low-rank geometric mean metric learning. *arXiv preprint arXiv:1806.05454*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boudiaf et al. (2020) Boudiaf, Malik, Rony, Jérôme, Ziko, Imtiaz Masud, Granger,
    Eric, Pedersoli, Marco, Piantanida, Pablo, and Ayed, Ismail Ben. A unifying mutual
    information view of metric learning: cross-entropy vs. pairwise losses. In *European
    Conference on Computer Vision*, pp.  548–564. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bromley et al. (1993) Bromley, Jane, Bentz, James W, Bottou, Léon, Guyon, Isabelle,
    LeCun, Yann, Moore, Cliff, Säckinger, Eduard, and Shah, Roopak. Signature verification
    using a “Siamese” time delay neural network. *International Journal of Pattern
    Recognition and Artificial Intelligence*, 7(04):669–688, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang & Yeung (2004) Chang, Hong and Yeung, Dit-Yan. Locally linear metric adaptation
    for semi-supervised clustering. In *Proceedings of the twenty-first international
    conference on Machine learning*, pp.  20, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018) Chen, Shuo, Gong, Chen, Yang, Jian, Li, Xiang, Wei, Yang,
    and Li, Jun. Adversarial metric learning. *arXiv preprint arXiv:1802.03170*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019) Chen, Shuo, Luo, Lei, Yang, Jian, Gong, Chen, Li, Jun, and
    Huang, Heng. Curvilinear distance metric learning. *Advances in Neural Information
    Processing Systems*, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Chen, Ting, Kornblith, Simon, Norouzi, Mohammad, and Hinton,
    Geoffrey. A simple framework for contrastive learning of visual representations.
    In *International conference on machine learning*, pp. 1597–1607, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cour et al. (2011) Cour, Timothee, Sapp, Ben, and Taskar, Ben. Learning from
    partial labels. *The Journal of Machine Learning Research*, 12:1501–1536, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cox & Cox (2008) Cox, Michael AA and Cox, Trevor F. Multidimensional scaling.
    In *Handbook of data visualization*, pp.  315–347\. Springer, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Davis et al. (2007) Davis, Jason V, Kulis, Brian, Jain, Prateek, Sra, Suvrit,
    and Dhillon, Inderjit S. Information-theoretic metric learning. In *Proceedings
    of the 24th international conference on Machine learning*, pp.  209–216, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Maesschalck et al. (2000) De Maesschalck, Roy, Jouan-Rimbaud, Delphine, and
    Massart, Désiré L. The mahalanobis distance. *Chemometrics and intelligent laboratory
    systems*, 50(1):1–18, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De Vazelhes et al. (2020) De Vazelhes, William, Carey, CJ, Tang, Yuan, Vauquier,
    Nathalie, and Bellet, Aurélien. metric-learn: Metric learning algorithms in Python.
    *Journal of Machine Learning Research*, 21:138–1, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhillon (2007) Dhillon, JVDI. Differential entropic clustering of multivariate
    Gaussians. *Advances in Neural Information Processing Systems*, 19:337, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Díaz-Vico & Dorronsoro (2019) Díaz-Vico, David and Dorronsoro, José R. Deep
    least squares Fisher discriminant analysis. *IEEE transactions on neural networks
    and learning systems*, 31(8):2752–2763, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Díaz-Vico et al. (2017) Díaz-Vico, David, Omari, Adil, Torres-Barrán, Alberto,
    and Dorronsoro, José Ramón. Deep Fisher discriminant analysis. In *International
    Work-Conference on Artificial Neural Networks*, pp.  501–512\. Springer, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2015) Ding, Shengyong, Lin, Liang, Wang, Guangrun, and Chao, Hongyang.
    Deep feature learning with relative distance comparison for person re-identification.
    *Pattern Recognition*, 48(10):2993–3003, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong (2019) Dong, Minghzi. *Metric learning with Lipschitz continuous functions*.
    PhD thesis, UCL (University College London), 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duan et al. (2018) Duan, Yueqi, Zheng, Wenzhao, Lin, Xudong, Lu, Jiwen, and
    Zhou, Jie. Deep adversarial metric learning. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, pp.  2780–2789, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2018) Feng, Lin, Wang, Huibing, Jin, Bo, Li, Haohao, Xue, Mingliang,
    and Wang, Le. Learning a distance metric by balancing KL-divergence for imbalanced
    datasets. *IEEE Transactions on Systems, Man, and Cybernetics: Systems*, 49(12):2384–2395,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finn et al. (2017) Finn, Chelsea, Abbeel, Pieter, and Levine, Sergey. Model-agnostic
    meta-learning for fast adaptation of deep networks. In *International Conference
    on Machine Learning*, pp. 1126–1135, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fisher (1936) Fisher, Ronald A. The use of multiple measurements in taxonomic
    problems. *Annals of eugenics*, 7(2):179–188, 1936.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gautheron et al. (2019) Gautheron, Léo, Habrard, Amaury, Morvant, Emilie, and
    Sebban, Marc. Metric learning from imbalanced data. In *2019 IEEE 31st International
    Conference on Tools with Artificial Intelligence (ICTAI)*, pp.  923–930\. IEEE,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghodsi et al. (2007) Ghodsi, Ali, Wilkinson, Dana F, and Southey, Finnegan.
    Improving embeddings by flexible exploitation of side information. In *IJCAI*,
    pp.  810–816, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghojogh (2021) Ghojogh, Benyamin. *Data Reduction Algorithms in Machine Learning
    and Data Science*. PhD thesis, University of Waterloo, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh & Crowley (2019) Ghojogh, Benyamin and Crowley, Mark. Unsupervised
    and supervised principal component analysis: Tutorial. *arXiv preprint arXiv:1906.03148*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2019a) Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
    Eigenvalue and generalized eigenvalue problems: Tutorial. *arXiv preprint arXiv:1903.11240*,
    2019a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2019b) Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
    Fisher and kernel Fisher discriminant analysis: Tutorial. *arXiv preprint arXiv:1906.09436*,
    2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2020a) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Locally linear embedding and its variants: Tutorial and survey.
    *arXiv preprint arXiv:2011.10925*, 2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2020b) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Multidimensional scaling, Sammon mapping, and Isomap: Tutorial
    and survey. *arXiv preprint arXiv:2009.08136*, 2020b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2020c) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Stochastic neighbor embedding with Gaussian and Student-t distributions:
    Tutorial and survey. *arXiv preprint arXiv:2009.10301*, 2020c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghojogh et al. (2020d) Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
    Backprojection for training feedforward neural networks in the input and feature
    spaces. In *International Conference on Image Analysis and Recognition*, pp. 
    16–24\. Springer, 2020d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2020e) Ghojogh, Benyamin, Nekoei, Hadi, Ghojogh, Aydin, Karray,
    Fakhri, and Crowley, Mark. Sampling algorithms, from survey sampling to Monte
    Carlo methods: Tutorial and literature review. *arXiv preprint arXiv:2011.00901*,
    2020e.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghojogh et al. (2020f) Ghojogh, Benyamin, Sikaroudi, Milad, Shafiei, Sobhan,
    Tizhoosh, Hamid R, Karray, Fakhri, and Crowley, Mark. Fisher discriminant triplet
    and contrastive losses for training Siamese networks. In *2020 international joint
    conference on neural networks (IJCNN)*, pp.  1–7\. IEEE, 2020f.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2021a) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Factor analysis, probabilistic principal component analysis, variational
    inference, and variational autoencoder: Tutorial and survey. *arXiv preprint arXiv:2101.00734*,
    2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2021b) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Generative adversarial networks and adversarial autoencoders: Tutorial
    and survey. *arXiv preprint arXiv:2111.13282*, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2021c) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. KKT conditions, first-order and second-order optimization, and
    distributed optimization: Tutorial and survey. *arXiv preprint arXiv:2110.01858*,
    2021c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2021d) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Laplacian-based dimensionality reduction including spectral clustering,
    Laplacian eigenmap, locality preserving projection, graph embedding, and diffusion
    map: Tutorial and survey. *arXiv preprint arXiv:2106.02154*, 2021d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2021e) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Reproducing kernel Hilbert space, Mercer’s theorem, eigenfunctions,
    Nyström method, and use of kernels in machine learning: Tutorial and survey. *arXiv
    preprint arXiv:2106.08443*, 2021e.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghojogh et al. (2021f) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Restricted Boltzmann machine and deep belief network: Tutorial
    and survey. *arXiv preprint arXiv:2107.12521*, 2021f.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Globerson & Roweis (2005) Globerson, Amir and Roweis, Sam. Metric learning by
    collapsing classes. *Advances in neural information processing systems*, 18:451–458,
    2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goldberger et al. (2005) Goldberger, Jacob, Hinton, Geoffrey E, Roweis, Sam T,
    and Salakhutdinov, Ruslan R. Neighbourhood components analysis. In *Advances in
    neural information processing systems*, pp. 513–520, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi,
    Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua.
    Generative adversarial nets. *Advances in neural information processing systems*,
    27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2016) Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron.
    *Deep learning*. MIT press, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gretton et al. (2005) Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and Schölkopf,
    Bernhard. Measuring statistical dependence with Hilbert-Schmidt norms. In *International
    conference on algorithmic learning theory*, pp.  63–77\. Springer, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guillaumin et al. (2009) Guillaumin, Matthieu, Verbeek, Jakob, and Schmid, Cordelia.
    Is that you? metric learning approaches for face identification. In *2009 IEEE
    12th international conference on computer vision*, pp.  498–505\. IEEE, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadsell et al. (2006) Hadsell, Raia, Chopra, Sumit, and LeCun, Yann. Dimensionality
    reduction by learning an invariant mapping. In *2006 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition (CVPR’06)*, volume 2, pp.  1735–1742\.
    IEEE, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hajiabadi et al. (2019) Hajiabadi, Hamideh, Godaz, Reza, Ghasemi, Morteza, and
    Monsefi, Reza. Layered geometric learning. In *International Conference on Artificial
    Intelligence and Soft Computing*, pp.  571–582\. Springer, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Harandi et al. (2017) Harandi, Mehrtash, Salzmann, Mathieu, and Hartley, Richard.
    Joint dimensionality reduction and metric learning: A geometric take. In *International
    Conference on Machine Learning*, pp. 1404–1413\. PMLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hart et al. (2000) Hart, Peter E, Stork, David G, and Duda, Richard O. *Pattern
    classification*. Wiley Hoboken, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hauberg et al. (2012) Hauberg, Søren, Freifeld, Oren, and Black, Michael J.
    A geometric take on metric learning. In *Advances in neural information processing
    systems*, volume 25, pp.  2033–2041, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hauser (2017) Hauser, Michael B. Principles of Riemannian geometry in neural
    networks. In *Advances in neural information processing systems*, pp. 2807––2816,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hermans et al. (2017) Hermans, Alexander, Beyer, Lucas, and Leibe, Bastian.
    In defense of the triplet loss for person re-identification. *arXiv preprint arXiv:1703.07737*,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2014) Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distilling
    the knowledge in a neural network. In *NIPS 2014 Deep Learning Workshop*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton & Roweis (2003) Hinton, Geoffrey E and Roweis, Sam T. Stochastic neighbor
    embedding. In *Advances in neural information processing systems*, pp. 857–864,
    2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton & Salakhutdinov (2006) Hinton, Geoffrey E and Salakhutdinov, Ruslan R.
    Reducing the dimensionality of data with neural networks. *Science*, 313(5786):504–507,
    2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoffer & Ailon (2015) Hoffer, Elad and Ailon, Nir. Deep metric learning using
    triplet network. In *International workshop on similarity-based pattern recognition*,
    pp.  84–92\. Springer, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoi et al. (2006) Hoi, Steven CH, Liu, Wei, Lyu, Michael R, and Ma, Wei-Ying.
    Learning distance metrics with contextual constraints for image retrieval. In
    *2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    (CVPR’06)*, volume 2, pp.  2072–2078\. IEEE, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang & Van Gool (2017) Huang, Zhiwu and Van Gool, Luc. A Riemannian network
    for SPD matrix learning. In *Thirty-First AAAI Conference on Artificial Intelligence*,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2020) Jiang, Wen, Huang, Kai, Geng, Jie, and Deng, Xinyang. Multi-scale
    metric learning for few-shot learning. *IEEE Transactions on Circuits and Systems
    for Video Technology*, 31(3):1091–1102, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaya & Bilge (2019) Kaya, Mahmut and Bilge, Hasan Şakir. Deep metric learning:
    A survey. *Symmetry*, 11(9):1066, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khodadadeh et al. (2019) Khodadadeh, Siavash, Bölöni, Ladislau, and Shah, Mubarak.
    Unsupervised meta-learning for few-shot image classification. In *Advances in
    neural information processing systems*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim & Song (2021) Kim, Dae Ha and Song, Byung Cheol. Virtual sample-based deep
    metric learning using discriminant analysis. *Pattern Recognition*, 110:107643,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koch et al. (2015) Koch, Gregory, Zemel, Richard, Salakhutdinov, Ruslan, et al.
    Siamese neural networks for one-shot image recognition. In *ICML deep learning
    workshop*, volume 2\. Lille, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kulis (2013) Kulis, Brian. Metric learning: A survey. *Foundations and Trends®
    in Machine Learning*, 5(4):287–364, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar BG et al. (2016) Kumar BG, Vijay, Carneiro, Gustavo, and Reid, Ian. Learning
    local image descriptors with deep Siamese and triplet convolutional networks by
    minimising global loss functions. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, pp.  5385–5394, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leyva-Vallina et al. (2021) Leyva-Vallina, María, Strisciuglio, Nicola, and
    Petkov, Nicolai. Generalized contrastive optimization of Siamese networks for
    place recognition. *arXiv preprint arXiv:2103.06638*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Li, Li, Doroslovački, Miloš, and Loew, Murray H. Discriminant
    analysis deep neural networks. In *2019 53rd annual conference on information
    sciences and systems (CISS)*, pp.  1–6\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Li, Xiaomeng, Yu, Lequan, Fu, Chi-Wing, Fang, Meng, and Heng,
    Pheng-Ann. Revisiting metric learning for few-shot image classification. *Neurocomputing*,
    406:49–58, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lv et al. (2019) Lv, Xinbi, Zhao, Cairong, and Chen, Wei. A novel hard mining
    center-triplet loss for person re-identification. In *Chinese Conference on Pattern
    Recognition and Computer Vision (PRCV)*, pp.  199–210\. Springer, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahalanobis (1930) Mahalanobis, Prasanta Chandra. On tests and measures of group
    divergence. *Journal of the Asiatic Society of Bengal*, 26:541–588, 1930.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao et al. (2019) Mao, Chengzhi, Zhong, Ziyuan, Yang, Junfeng, Vondrick, Carl,
    and Ray, Baishakhi. Metric learning for adversarial robustness. *Advances in neural
    information processing systems*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McLachlan (1999) McLachlan, Goeffrey J. Mahalanobis distance. *Resonance*, 4(6):20–26,
    1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mignon & Jurie (2012) Mignon, Alexis and Jurie, Frédéric. PCCA: A new approach
    for distance learning from sparse pairwise constraints. In *2012 IEEE conference
    on computer vision and pattern recognition*, pp.  2666–2672\. IEEE, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mika et al. (1999) Mika, Sebastian, Ratsch, Gunnar, Weston, Jason, Scholkopf,
    Bernhard, and Mullers, Klaus-Robert. Fisher discriminant analysis with kernels.
    In *Neural networks for signal processing IX: Proceedings of the 1999 IEEE signal
    processing society workshop (cat. no. 98th8468)*, pp. 41–48\. Ieee, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Movshovitz-Attias et al. (2017) Movshovitz-Attias, Yair, Toshev, Alexander,
    Leung, Thomas K, Ioffe, Sergey, and Singh, Saurabh. No fuss distance metric learning
    using proxies. In *Proceedings of the IEEE International Conference on Computer
    Vision*, pp.  360–368, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Murphy (2007) Murphy, Kevin P. Conjugate Bayesian analysis of the Gaussian distribution.
    Technical report, University of British Colombia, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Murphy (2012) Murphy, Kevin P. *Machine learning: a probabilistic perspective*.
    MIT press, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Musgrave et al. (2020) Musgrave, Kevin, Belongie, Serge, and Lim, Ser-Nam. Pytorch
    metric learning. *arXiv preprint arXiv:2008.09164*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oh Song et al. (2016) Oh Song, Hyun, Xiang, Yu, Jegelka, Stefanie, and Savarese,
    Silvio. Deep metric learning via lifted structured feature embedding. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pp.  4004–4012,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poorheravi et al. (2020) Poorheravi, Parisa Abdolrahim, Ghojogh, Benyamin, Gaudet,
    Vincent, Karray, Fakhri, and Crowley, Mark. Acceleration of large margin metric
    learning for nearest neighbor classification using triplet mining and stratified
    sampling. *Journal of Computational Vision and Imaging Systems*, 6(1), 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qian et al. (2019) Qian, Qi, Shang, Lei, Sun, Baigui, Hu, Juhua, Li, Hao, and
    Jin, Rong. SoftTriple loss: Deep metric learning without triplet sampling. In
    *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 
    6450–6458, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riccati (1724) Riccati, Jacobo. Animadversiones in aequationes differentiales
    secundi gradus. *Actorum Eruditorum Supplementa*, 8(1724):66–73, 1724.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robinson et al. (2021) Robinson, Joshua, Chuang, Ching-Yao, Sra, Suvrit, and
    Jegelka, Stefanie. Contrastive learning with hard negative samples. In *International
    Conference on Learning Representations*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roostaiyan et al. (2017) Roostaiyan, Seyed Mahdi, Imani, Ehsan, and Baghshah,
    Mahdieh Soleymani. Multi-modal deep distance metric learning. *Intelligent Data
    Analysis*, 21(6):1351–1369, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roweis & Saul (2000) Roweis, Sam T and Saul, Lawrence K. Nonlinear dimensionality
    reduction by locally linear embedding. *Science*, 290(5500):2323–2326, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schölkopf (2001) Schölkopf, Bernhard. The kernel trick for distances. *Advances
    in neural information processing systems*, pp. 301–307, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schölkopf et al. (2000) Schölkopf, Bernhard, Smola, Alex J, Williamson, Robert C,
    and Bartlett, Peter L. New support vector algorithms. *Neural computation*, 12(5):1207–1245,
    2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schroff et al. (2015) Schroff, Florian, Kalenichenko, Dmitry, and Philbin,
    James. FaceNet: A unified embedding for face recognition and clustering. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pp.  815–823,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shental et al. (2002) Shental, Noam, Hertz, Tomer, Weinshall, Daphna, and Pavel,
    Misha. Adjustment learning and relevant component analysis. In *European conference
    on computer vision*, pp.  776–790. Springer, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sikaroudi et al. (2020a) Sikaroudi, Milad, Ghojogh, Benyamin, Safarpoor, Amir,
    Karray, Fakhri, Crowley, Mark, and Tizhoosh, Hamid R. Offline versus online triplet
    mining based on extreme distances of histopathology patches. In *International
    Symposium on Visual Computing*, pp. 333–345\. Springer, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sikaroudi et al. (2020b) Sikaroudi, Milad, Safarpoor, Amir, Ghojogh, Benyamin,
    Shafiei, Sobhan, Crowley, Mark, and Tizhoosh, Hamid R. Supervision and source
    domain impact on representation learning: A histopathology case study. In *2020
    42nd Annual International Conference of the IEEE Engineering in Medicine & Biology
    Society (EMBC)*, pp.  1400–1403\. IEEE, 2020b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sikaroudi et al. (2021) Sikaroudi, Milad, Ghojogh, Benyamin, Karray, Fakhri,
    Crowley, Mark, and Tizhoosh, Hamid R. Batch-incremental triplet sampling for training
    triplet networks using Bayesian updating theorem. In *2020 25th International
    Conference on Pattern Recognition (ICPR)*, pp.  7080–7086\. IEEE, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn (2016) Sohn, Kihyuk. Improved deep metric learning with multi-class n-pair
    loss objective. In *Advances in neural information processing systems*, pp. 1857–1865,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suárez et al. (2020) Suárez, Juan-Luis, García, Salvador, and Herrera, Francisco.
    pyDML: A Python library for distance metric learning. *Journal of Machine Learning
    Research*, 21:96–1, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suárez et al. (2021) Suárez, Juan Luis, García, Salvador, and Herrera, Francisco.
    A tutorial on distance metric learning: Mathematical foundations, algorithms,
    experimental analysis, prospects and challenges. *Neurocomputing*, 425:300–322,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teh & Taylor (2020) Teh, Eu Wern and Taylor, Graham W. Learning with less data
    via weakly labeled patch classification in digital pathology. In *2020 IEEE 17th
    International Symposium on Biomedical Imaging (ISBI)*, pp.  471–475\. IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teh et al. (2020) Teh, Eu Wern, DeVries, Terrance, and Taylor, Graham W. ProxyNCA++:
    Revisiting and revitalizing proxy neighborhood component analysis. In *European
    Conference on Computer Vision (ECCV)*, pp. 448–464\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tizhoosh (2005) Tizhoosh, Hamid R. Opposition-based learning: a new scheme
    for machine intelligence. In *International conference on computational intelligence
    for modelling, control and automation and international conference on intelligent
    agents, web technologies and internet commerce (CIMCA-IAWTIC’06)*, volume 1, pp. 
    695–701\. IEEE, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Triantafillou et al. (2020) Triantafillou, Eleni, Zhu, Tyler, Dumoulin, Vincent,
    Lamblin, Pascal, Evci, Utku, Xu, Kelvin, Goroshin, Ross, Gelada, Carles, Swersky,
    Kevin, Manzagol, Pierre-Antoine, et al. Meta-dataset: A dataset of datasets for
    learning to learn from few examples. In *International Conference on Learning
    Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsang et al. (2003) Tsang, Ivor W, Kwok, James T, Bay, C, and Kong, H. Distance
    metric learning with kernels. In *Proceedings of the International Conference
    on Artificial Neural Networks*, pp.  126–129, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van der Maaten & Hinton (2008) van der Maaten, Laurens and Hinton, Geoffrey.
    Visualizing data using t-SNE. *Journal of machine learning research*, 9(Nov):2579–2605,
    2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vapnik (1995) Vapnik, Vladimir. *The nature of statistical learning theory*.
    Springer science & business media, 1995.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang & Tan (2017) Wang, Dong and Tan, Xiaoyang. Bayesian neighborhood component
    analysis. *IEEE transactions on neural networks and learning systems*, 29(7):3140–3151,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang & Sun (2015) Wang, Fei and Sun, Jimeng. Survey on distance metric learning
    and dimensionality reduction in data mining. *Data mining and knowledge discovery*,
    29(2):534–564, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Wang, Jian, Zhou, Feng, Wen, Shilei, Liu, Xiao, and Lin,
    Yuanqing. Deep metric learning with angular loss. In *Proceedings of the IEEE
    International Conference on Computer Vision*, pp.  2593–2601, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang & Jin (2009) Wang, Shijun and Jin, Rong. An information geometry approach
    for distance metric learning. In *Artificial intelligence and statistics*, pp. 
    591–598. PMLR, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2014) Wang, Wei, Ooi, Beng Chin, Yang, Xiaoyan, Zhang, Dongxiang,
    and Zhuang, Yueting. Effective multi-modal retrieval based on stacked auto-encoders.
    *Proceedings of the VLDB Endowment*, 7(8):649–660, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Wang, Xiao, Chen, Ziliang, Yang, Rui, Luo, Bin, and Tang,
    Jin. Improved hard example mining by discovering attribute-based hard person identity.
    *arXiv preprint arXiv:1905.02102*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Wang, Xun, Zhang, Haozhi, Huang, Weilin, and Scott, Matthew R.
    Cross-batch memory for embedding learning. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pp.  6388–6397, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Wang, Yaqing, Yao, Quanming, Kwok, James T, and Ni, Lionel M.
    Generalizing from a few examples: A survey on few-shot learning. *ACM Computing
    Surveys (CSUR)*, 53(3):1–34, 2020b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weinberger & Saul (2009) Weinberger, Kilian Q and Saul, Lawrence K. Distance
    metric learning for large margin nearest neighbor classification. *Journal of
    machine learning research*, 10(2), 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weinberger et al. (2006) Weinberger, Kilian Q, Blitzer, John, and Saul, Lawrence K.
    Distance metric learning for large margin nearest neighbor classification. In
    *Advances in neural information processing systems*, pp. 1473–1480, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2017) Wu, Chao-Yuan, Manmatha, R, Smola, Alexander J, and Krahenbuhl,
    Philipp. Sampling matters in deep embedding learning. In *Proceedings of the IEEE
    International Conference on Computer Vision*, pp.  2840–2848, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiang et al. (2008) Xiang, Shiming, Nie, Feiping, and Zhang, Changshui. Learning
    a Mahalanobis distance metric for data clustering and classification. *Pattern
    recognition*, 41(12):3600–3612, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xing et al. (2002) Xing, Eric, Jordan, Michael, Russell, Stuart J, and Ng, Andrew.
    Distance metric learning with application to clustering with side-information.
    *Advances in neural information processing systems*, 15:521–528, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019a) Xu, Xing, He, Li, Lu, Huimin, Gao, Lianli, and Ji, Yanli.
    Deep adversarial metric learning for cross-modal retrieval. *World Wide Web*,
    22(2):657–672, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019b) Xu, Xinyi, Cao, Huanhuan, Yang, Yanhua, Yang, Erkun, and Deng,
    Cheng. Zero-shot metric learning. In *International Joint Conference on Artificial
    Intelligence*, pp.  3996–4002, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xuan et al. (2020) Xuan, Hong, Stylianou, Abby, and Pless, Robert. Improved
    embeddings with easy positive triplet mining. In *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, pp.  2474–2482, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang (2007) Yang, Liu. An overview of distance metric learning. In *Proceedings
    of the computer vision and pattern recognition conference*, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang & Jin (2006) Yang, Liu and Jin, Rong. Distance metric learning: A comprehensive
    survey. *Michigan State Universiy*, 2(2):4, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2006) Yang, Liu, Jin, Rong, Sukthankar, Rahul, and Liu, Yi. An
    efficient algorithm for local distance metric learning. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, volume 2, pp.  543–548, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2007) Yang, Liu, Jin, Rong, and Sukthankar, Rahul. Bayesian active
    distance metric learning. In *Conference on Uncertainty in Artificial Intelligence
    (UAI)*, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020) Yang, Liu, Zhang, Mingyang, Li, Cheng, Bendersky, Michael,
    and Najork, Marc. Beyond 512 tokens: Siamese multi-depth transformer-based hierarchical
    encoder for long-form document matching. In *Proceedings of the 29th ACM International
    Conference on Information & Knowledge Management*, pp.  1725–1734, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2012) Yang, Wei, Wang, Kuanquan, and Zuo, Wangmeng. Fast neighborhood
    component analysis. *Neurocomputing*, 83:31–37, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2016) Yang, Xun, Wang, Meng, Zhang, Luming, and Tao, Dacheng. Empirical
    risk minimization for metric learning using privileged information. In *IJCAI
    International Joint Conference on Artificial Intelligence*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang & Laaksonen (2007) Yang, Zhirong and Laaksonen, Jorma. Regularized neighborhood
    component analysis. In *Scandinavian Conference on Image Analysis*, pp.  253–262.
    Springer, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2019) Ye, Mang, Zhang, Xu, Yuen, Pong C, and Chang, Shih-Fu. Unsupervised
    embedding learning via invariant and spreading instance feature. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  6210–6219,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yeung & Chang (2007) Yeung, Dit-Yan and Chang, Hong. A kernel approach for semisupervised
    metric learning. *IEEE Transactions on Neural Networks*, 18(1):141–149, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zadeh et al. (2016) Zadeh, Pourya, Hosseini, Reshad, and Sra, Suvrit. Geometric
    mean metric learning. In *International conference on machine learning*, pp. 2464–2471,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Zhang, Changqing, Liu, Yeqing, Liu, Yue, Hu, Qinghua, Liu,
    Xinwang, and Zhu, Pengfei. FISH-MML: Fisher-HSIC multi-view metric learning. In
    *IJCAI*, pp.  3054–3060, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) Zhang, Hangbin, Wong, Raymond K, and Chu, Victor W. Curvilinear
    collaborative metric learning with macro-micro attentions. In *2021 International
    Joint Conference on Neural Networks (IJCNN)*, pp.  1–8\. IEEE, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2003) Zhang, Zhihua, Kwok, James T, and Yeung, Dit-Yan. Parametric
    distance metric learning with label information. In *IJCAI*, volume 1450, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou & Gu (2018) Zhou, Yu and Gu, Hong. Geometric mean metric learning for partial
    label data. *Neurocomputing*, 275:394–402, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2018) Zhu, Pengfei, Cheng, Hao, Hu, Qinghua, Wang, Qilong, and Zhang,
    Changqing. Towards generalized and efficient metric learning on riemannian manifold.
    In *IJCAI*, pp.  3235–3241, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[1 Introduction](#S1 "In Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2 Generalized Mahalanobis Distance Metric](#S2 "In Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.1 Distance Metric](#S2.SS1 "In 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.2 Mahalanobis Distance](#S2.SS2 "In 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.3 Generalized Mahalanobis Distance](#S2.SS3 "In 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.4 The Main Idea of Metric Learning](#S2.SS4 "In 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3 Spectral Metric Learning](#S3 "In Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1 Spectral Methods Using Scatters](#S3.SS1 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.1 The First Spectral Method](#S3.SS1.SSS1 "In 3.1 Spectral Methods Using
    Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.2 Formulating as Semidefinite Programming](#S3.SS1.SSS2 "In 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.3 Relevant to Fisher Discriminant Analysis](#S3.SS1.SSS3 "In 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.4 Relevant Component Analysis (RCA)](#S3.SS1.SSS4 "In 3.1 Spectral Methods
    Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.5 Discriminative Component Analysis (DCA)](#S3.SS1.SSS5 "In 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.6 High Dimensional Discriminative Component Analysis](#S3.SS1.SSS6 "In
    3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.7 Regularization by Locally Linear Embedding](#S3.SS1.SSS7 "In 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.8 Fisher-HSIC Multi-view Metric Learning (FISH-MML)](#S3.SS1.SSS8 "In
    3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2 Spectral Methods Using Hinge Loss](#S3.SS2 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2.1 Large-Margin Metric Learning](#S3.SS2.SSS1 "In 3.2 Spectral Methods
    Using Hinge Loss ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2.2 Imbalanced Metric Learning (IML)](#S3.SS2.SSS2 "In 3.2 Spectral Methods
    Using Hinge Loss ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.3 Locally Linear Metric Adaptation (LLMA)](#S3.SS3 "In 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.4 Relevant to Support Vector Machine](#S3.SS4 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.5 Relevant to Multidimensional Scaling](#S3.SS5 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.6 Kernel Spectral Metric Learning](#S3.SS6 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.6.1 Using Eigenvalue Decomposition of Kernel](#S3.SS6.SSS1 "In 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.6.2 Regularization by Locally Linear Embedding](#S3.SS6.SSS2 "In 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.6.3 Regularization by Laplacian](#S3.SS6.SSS3 "In 3.6 Kernel Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.6.4 Kernel Discriminative Component Analysis](#S3.SS6.SSS4 "In 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.6.5 Relevant to Kernel Fisher Discriminant Analysis](#S3.SS6.SSS5 "In 3.6
    Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.6.6 Relevant to Kernel Support Vector Machine](#S3.SS6.SSS6 "In 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.7 Geometric Spectral Metric Learning](#S3.SS7 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.7.1 Geometric Mean Metric Learning](#S3.SS7.SSS1 "In 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.7.2 Low-rank Geometric Mean Metric Learning](#S3.SS7.SSS2 "In 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.7.3 Geometric Mean Metric Learning for Partial Labels](#S3.SS7.SSS3 "In
    3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.7.4 Geometric Mean Metric Learning on SPD and Grassmannian Manifolds](#S3.SS7.SSS4
    "In 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.7.5 Metric Learning on Stiefel and SPD Manifolds](#S3.SS7.SSS5 "In 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.7.6 Curvilinear Distance Metric Learning (CDML)](#S3.SS7.SSS6 "In 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.8 Adversarial Metric Learning (AML)](#S3.SS8 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4 Probabilistic Metric Learning](#S4 "In Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1 Collapsing Classes](#S4.SS1 "In 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1.1 Collapsing Classes in the Input Space](#S4.SS1.SSS1 "In 4.1 Collapsing
    Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1.2 Collapsing Classes in the Feature Space](#S4.SS1.SSS2 "In 4.1 Collapsing
    Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2 Neighborhood Component Analysis Methods](#S4.SS2 "In 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2.1 Neighborhood Component Analysis (NCA)](#S4.SS2.SSS1 "In 4.2 Neighborhood
    Component Analysis Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2.2 Regularized Neighborhood Component Analysis](#S4.SS2.SSS2 "In 4.2 Neighborhood
    Component Analysis Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2.3 Fast Neighborhood Component Analysis](#S4.SS2.SSS3 "In 4.2 Neighborhood
    Component Analysis Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.3 Bayesian Metric Learning Methods](#S4.SS3 "In 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.3.1 Bayesian Metric Learning Using Sigmoid Function](#S4.SS3.SSS1 "In 4.3
    Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.3.2 Bayesian Neighborhood Component Analysis](#S4.SS3.SSS2 "In 4.3 Bayesian
    Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.3.3 Local Distance Metric (LDM)](#S4.SS3.SSS3 "In 4.3 Bayesian Metric Learning
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.4 Information Theoretic Metric Learning](#S4.SS4 "In 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.4.1 Information Theoretic Metric Learning with a Prior Weight Matrix](#S4.SS4.SSS1
    "In 4.4 Information Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.4.2 Information Theoretic Metric Learning for Imbalanced Data](#S4.SS4.SSS2
    "In 4.4 Information Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.4.3 Probabilistic Relevant Component Analysis Methods](#S4.SS4.SSS3 "In
    4.4 Information Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.4.4 Metric Learning by Information Geometry](#S4.SS4.SSS4 "In 4.4 Information
    Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.5 Empirical Risk Minimization in Metric Learning](#S4.SS5 "In 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.5.1 Metric Learning Using the Sigmoid Function](#S4.SS5.SSS1 "In 4.5 Empirical
    Risk Minimization in Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.5.2 Pairwise Constrained Component Analysis (PCCA)](#S4.SS5.SSS2 "In 4.5
    Empirical Risk Minimization in Metric Learning ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.5.3 Metric Learning for Privileged Information](#S4.SS5.SSS3 "In 4.5 Empirical
    Risk Minimization in Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5 Deep Metric Learning](#S5 "In Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.1 Reconstruction Autoencoders](#S5.SS1 "In 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.1.1 Types of Autoencoders](#S5.SS1.SSS1 "In 5.1 Reconstruction Autoencoders
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.1.2 Reconstruction Loss](#S5.SS1.SSS2 "In 5.1 Reconstruction Autoencoders
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.1.3 Denoising Autoencoder](#S5.SS1.SSS3 "In 5.1 Reconstruction Autoencoders
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.1.4 Metric Learning by Reconstruction Autoencoder](#S5.SS1.SSS4 "In 5.1
    Reconstruction Autoencoders ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2 Supervised Metric Learning by Supervised Loss Functions](#S5.SS2 "In 5
    Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2.1 Mean Squared Error and Mean Absolute Value Losses](#S5.SS2.SSS1 "In
    5.2 Supervised Metric Learning by Supervised Loss Functions ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2.2 Huber and KL-Divergence Losss](#S5.SS2.SSS2 "In 5.2 Supervised Metric
    Learning by Supervised Loss Functions ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2.3 Hinge Loss](#S5.SS2.SSS3 "In 5.2 Supervised Metric Learning by Supervised
    Loss Functions ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2.4 Cross-entropy Loss](#S5.SS2.SSS4 "In 5.2 Supervised Metric Learning
    by Supervised Loss Functions ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3 Metric Learning by Siamese Networks](#S5.SS3 "In 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.1 Siamese and Triplet Networks](#S5.SS3.SSS1 "In 5.3 Metric Learning by
    Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.2 Pairs and Triplets of Data Points](#S5.SS3.SSS2 "In 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.3 Implementation of Siamese Networks](#S5.SS3.SSS3 "In 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.4 Contrastive Loss](#S5.SS3.SSS4 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.5 Triplet Loss](#S5.SS3.SSS5 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.6 Tuplet Loss](#S5.SS3.SSS6 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.7 Neighborhood Component Analysis Loss](#S5.SS3.SSS7 "In 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.8 Proxy Neighborhood Component Analysis Loss](#S5.SS3.SSS8 "In 5.3 Metric
    Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.9 Softmax Triplet Loss](#S5.SS3.SSS9 "In 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.10 Triplet Global Loss](#S5.SS3.SSS10 "In 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.11 Angular Loss](#S5.SS3.SSS11 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.12 SoftTriple Loss](#S5.SS3.SSS12 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.13 Fisher Siamese Losses](#S5.SS3.SSS13 "In 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.14 Deep Adversarial Metric Learning](#S5.SS3.SSS14 "In 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.15 Triplet Mining](#S5.SS3.SSS15 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3.16 Triplet Sampling](#S5.SS3.SSS16 "In 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.4 Deep Discriminant Analysis Metric Learning](#S5.SS4 "In 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.4.1 Deep Probabilistic Discriminant Analysis](#S5.SS4.SSS1 "In 5.4 Deep
    Discriminant Analysis Metric Learning ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.4.2 Discriminant Analysis with Virtual Samples](#S5.SS4.SSS2 "In 5.4 Deep
    Discriminant Analysis Metric Learning ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.4.3 Deep Fisher Discriminant Analysis](#S5.SS4.SSS3 "In 5.4 Deep Discriminant
    Analysis Metric Learning ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and
    Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.5 Multi-Modal Deep Metric Learning](#S5.SS5 "In 5 Deep Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.6 Geometric Metric Learning by Neural Network](#S5.SS6 "In 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.7 Few-shot Metric Learning](#S5.SS7 "In 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.7.1 Multi-scale Metric Learning](#S5.SS7.SSS1 "In 5.7 Few-shot Metric Learning
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.7.2 Metric Learning with Continuous Similarity Scores](#S5.SS7.SSS2 "In
    5.7 Few-shot Metric Learning ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6 Conclusion](#S6 "In Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
