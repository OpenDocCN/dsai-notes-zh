- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:46:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:46:13'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2206.00356] A Survey on Deep Learning for Skin Lesion Segmentation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2206.00356] 皮肤病变分割的深度学习调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2206.00356](https://ar5iv.labs.arxiv.org/html/2206.00356)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2206.00356](https://ar5iv.labs.arxiv.org/html/2206.00356)
- en: A Survey on Deep Learning for Skin Lesion Segmentation
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 皮肤病变分割的深度学习调查
- en: Zahra Mirikharaji¹¹1Joint first authors Kumar Abhishek²²2Joint first authors
    Alceu Bissoto Catarina Barata Sandra Avila Eduardo Valle M. Emre Celebi³³3Joint
    senior authors Ghassan Hamarneh⁴⁴4Joint senior authors Medical Image Analysis
    Lab, School of Computing Science, Simon Fraser University, Burnaby V5A 1S6, Canada
    Institute for Systems and Robotics, Instituto Superior Técnico, Avenida Rovisco
    Pais, Lisbon 1049-001, Portugal RECOD.ai Lab, Institute of Computing, University
    of Campinas, Av. Albert Einstein 1251, Campinas 13083-852, Brazil RECOD.ai Lab,
    School of Electrical and Computing Engineering, University of Campinas, Av. Albert
    Einstein 400, Campinas 13083-952, Brazil Department of Computer Science and Engineering,
    University of Central Arkansas, 201 Donaghey Ave., Conway, AR 72035, USA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zahra Mirikharaji¹¹1联合第一作者 Kumar Abhishek²²2联合第一作者 Alceu Bissoto Catarina Barata
    Sandra Avila Eduardo Valle M. Emre Celebi³³3联合资深作者 Ghassan Hamarneh⁴⁴4联合资深作者 医学图像分析实验室，计算机科学学院，西蒙弗雷泽大学，加拿大伯纳比
    V5A 1S6 系统与机器人研究所，里斯本技术大学，Avenida Rovisco Pais，里斯本 1049-001，葡萄牙 RECOD.ai 实验室，坎皮纳斯大学计算机学院，Av.
    Albert Einstein 1251，坎皮纳斯 13083-852，巴西 RECOD.ai 实验室，坎皮纳斯大学电气与计算机工程学院，Av. Albert
    Einstein 400，坎皮纳斯 13083-952，巴西 中央阿肯色大学计算机科学与工程系，201 Donaghey Ave.，康威，AR 72035，美国
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Skin cancer is a major public health problem that could benefit from computer-aided
    diagnosis to reduce the burden of this common disease. Skin lesion segmentation
    from images is an important step toward achieving this goal. However, the presence
    of natural and artificial artifacts (e.g., hair and air bubbles), intrinsic factors
    (e.g., lesion shape and contrast), and variations in image acquisition conditions
    make skin lesion segmentation a challenging task. Recently, various researchers
    have explored the applicability of deep learning models to skin lesion segmentation.
    In this survey, we cross-examine $177$ research papers that deal with deep learning-based
    segmentation of skin lesions. We analyze these works along several dimensions,
    including input data (datasets, preprocessing, and synthetic data generation),
    model design (architecture, modules, and losses), and evaluation aspects (data
    annotation requirements and segmentation performance). We discuss these dimensions
    both from the viewpoint of select seminal works, and from a systematic viewpoint,
    examining how those choices have influenced current trends, and how their limitations
    should be addressed. To facilitate comparisons, we summarize all examined works
    in a comprehensive table as well as an interactive table available online⁵⁵5 [https://github.com/sfu-mial/skin-lesion-segmentation-survey](https://github.com/sfu-mial/skin-lesion-segmentation-survey)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤癌是一个重大公共健康问题，可以通过计算机辅助诊断来减少这种常见疾病的负担。皮肤病变从图像中分割是实现这一目标的重要步骤。然而，自然和人工伪影（例如头发和气泡）、内在因素（例如病变的形状和对比度）以及图像采集条件的变化使得皮肤病变分割成为一项具有挑战性的任务。最近，许多研究者探讨了深度学习模型在皮肤病变分割中的应用。在这项调查中，我们交叉审查了$177$篇涉及基于深度学习的皮肤病变分割的研究论文。我们从多个维度分析了这些研究，包括输入数据（数据集、预处理和合成数据生成）、模型设计（架构、模块和损失函数）以及评估方面（数据注释要求和分割性能）。我们从一些重要工作的视角和系统视角讨论了这些维度，考察了这些选择如何影响当前趋势，以及如何解决它们的局限性。为了便于比较，我们在一个全面的表格中总结了所有审查的工作，并在在线互动表格中提供了详细信息⁵⁵5
    [https://github.com/sfu-mial/skin-lesion-segmentation-survey](https://github.com/sfu-mial/skin-lesion-segmentation-survey)
- en: '^†^†journal: Medical Image Analysis'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '^†^†期刊: 医学图像分析'
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Segmentation is a challenging and critical operation in the automated skin
    lesion analysis workflow. Rule-based skin lesion diagnostic systems, popular in
    the clinical setting, rely on an accurate lesion segmentation for the estimation
    of diagnostic criteria such as asymmetry, border irregularity, and lesion size,
    as needed for implementing the ABCD algorithm (Asymmetry, Border, Color, Diameter
    of lesions) (Friedman et al., [1985](#bib.bib138); Nachbar et al., [1994](#bib.bib284))
    and its derivatives: ABCDE (ABCD plus Evolution of lesions) (Abbasi et al., [2004](#bib.bib2))
    and ABCDEF (ABCDE plus the “ugly duckling” sign) (Jensen and Elewski, [2015](#bib.bib200)).
    By contrast, in machine learning-based diagnostic systems, restricting the areas
    within an image, thereby focusing the model on the interior of the lesion, can
    improve the robustness of the classification. For example, recent studies have
    shown the utility of segmentation in improving the deep learning (DL)-based classification
    performance for certain diagnostic categories by regularizing attention maps (Yan
    et al., [2019](#bib.bib424)), allowing the cropping of lesion images (Yu et al.,
    [2017a](#bib.bib431); Mahbod et al., [2020](#bib.bib268); Liu et al., [2020](#bib.bib259);
    Singh et al., [2023](#bib.bib354)), tracking the evolution of lesions (Navarro
    et al., [2018](#bib.bib287)) and the removal of imaging artifacts (Maron et al.,
    [2021a](#bib.bib271); Bissoto et al., [2022](#bib.bib58)). In a DL-based skin
    lesion classification framework, presenting the delineated skin lesion to the
    user can also help with interpreting the DL black box (Jaworek-Korjakowska et al.,
    [2021](#bib.bib198)), and thus may either instill trust, or raise suspicion, in
    computer-aided diagnosis (CAD) systems for skin cancer.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 分割是在自动化皮肤病变分析工作流程中一项具有挑战性和关键性的操作。基于规则的皮肤病变诊断系统，在临床环境中较为流行，依赖于准确的病变分割来评估诊断标准，如对称性、边界不规则性和病变大小，这些都是实施ABCD算法（病变的对称性、边界、颜色、直径）所需的 (Friedman
    et al., [1985](#bib.bib138); Nachbar et al., [1994](#bib.bib284))及其衍生算法：ABCDE（ABCD加上病变的演变） (Abbasi
    et al., [2004](#bib.bib2))和ABCDEF（ABCDE加上“丑小鸭”标志） (Jensen and Elewski, [2015](#bib.bib200))。相较而言，在基于机器学习的诊断系统中，通过限制图像中的区域，从而将模型的关注点集中在病变的内部，可以提高分类的稳健性。例如，最近的研究表明，分割在通过规范化注意力图 (Yan
    et al., [2019](#bib.bib424))、允许裁剪病变图像 (Yu et al., [2017a](#bib.bib431); Mahbod
    et al., [2020](#bib.bib268); Liu et al., [2020](#bib.bib259); Singh et al., [2023](#bib.bib354))、跟踪病变的演变 (Navarro
    et al., [2018](#bib.bib287))以及去除成像伪影 (Maron et al., [2021a](#bib.bib271); Bissoto
    et al., [2022](#bib.bib58))来提高基于深度学习（DL）的分类性能方面表现出了实用性。在基于DL的皮肤病变分类框架中，将勾画出的皮肤病变呈现给用户也有助于解释DL的黑箱 (Jaworek-Korjakowska
    et al., [2021](#bib.bib198))，因此可能会对计算机辅助诊断（CAD）系统在皮肤癌诊断中的信任感产生影响，或者引发怀疑。
- en: Lesion detection and segmentation are also useful as preprocessing steps when
    analyzing wide-field images with multiple lesions (Birkenfeld et al., [2020](#bib.bib57)).
    Additionally, radiation therapy and image-guided human or robotic surgical lesion
    excision require localization and delineation of lesions (American Cancer Society,
    [2023](#bib.bib23)). Ensuring fair diagnosis that is unbiased to minority groups,
    a pressing issue with the deployment of these models and the trust therein, requires
    the estimation of lesion-free skin tone, which in turn also relies upon the delineation
    of skin lesions (Kinyanjui et al., [2020](#bib.bib228)). However, despite the
    importance of lesion segmentation, manual delineation of skin lesions remains
    a laborious task that suffers from significant inter- and intra-observer variability
    and consequently, a fast, reliable, and automated segmentation algorithm is needed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 病变检测和分割在分析多病变的广域图像时也作为预处理步骤非常有用 (Birkenfeld et al., [2020](#bib.bib57))。此外，放射治疗和影像引导的人体或机器人手术病变切除需要对病变进行定位和勾画 (American
    Cancer Society, [2023](#bib.bib23))。确保对少数群体公平的诊断，这是这些模型及其信任部署中的一个紧迫问题，需要估算无病变的肤色，这也依赖于对皮肤病变的勾画 (Kinyanjui
    et al., [2020](#bib.bib228))。然而，尽管病变分割非常重要，手动勾画皮肤病变仍是一项繁重的任务，存在显著的观察者间和观察者内变异，因此，需要一种快速、可靠且自动化的分割算法。
- en: 'Skin cancer and its associated expenses, $\$8.1$ billion annually in U.S. (Guy Jr
    et al., [2015](#bib.bib168)), have grown into a major public health issue in the
    past decades. In the USA alone, $97,610$ new cases of melanoma are expected in
    2023 (Siegel et al., [2023](#bib.bib351)). Broadly speaking, there are two types
    of skin cancer: melanomas and non-melanomas, the former making up just $1\%$ of
    the cases, but the majority of the deaths due to its aggressiveness. Early diagnosis
    is critical for a good prognosis: melanoma can be cured with a simple outpatient
    surgery if detected early, but its five-year survival rate drops from over $99\%$
    to $32\%$ if it is diagnosed at an advanced stage (American Cancer Society, [2023](#bib.bib23)).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤癌及其相关费用，每年在美国高达$8.1$亿（Guy Jr et al., [2015](#bib.bib168)），在过去几十年中已成为一个主要的公共健康问题。在美国，仅在2023年，就预计会出现$97,610$例新的黑色素瘤病例（Siegel
    et al., [2023](#bib.bib351)）。一般来说，皮肤癌分为两种类型：黑色素瘤和非黑色素瘤，前者仅占病例的$1\%$，但由于其侵袭性导致了大多数的死亡。早期诊断对于良好的预后至关重要：如果早期发现，黑色素瘤可以通过简单的门诊手术治愈，但如果在晚期诊断，其五年生存率将从超过$99\%$降至$32\%$（American
    Cancer Society, [2023](#bib.bib23)）。
- en: 'Two imaging modalities are commonly employed in automated skin lesion analysis (Daneshjou
    et al., [2022](#bib.bib109)): dermoscopic (microscopic) images and clinical (macroscopic)
    images. While dermoscopic images allow the inspection of lesion properties that
    are invisible to the naked eye, they are not always accessible even to dermatologists (Engasser
    and Warshaw, [2010](#bib.bib133)). On the other hand, clinical images acquired
    using conventional cameras are easily accessible but suffer from lower quality.
    Dermoscopy is a non-invasive skin imaging technique that aids in the diagnosis
    of skin lesions by allowing dermatologists to visualize sub-surface structures (Kittler
    et al., [2002](#bib.bib229)). However, even with dermoscopy, diagnostic accuracy
    can vary widely, ranging from $24\%$ to $77\%$, depending on the clinician’s level
    of expertise (Tran et al., [2005](#bib.bib378)). Moreover, dermoscopy may actually
    lower the diagnostic accuracy in the hands of inexperienced dermatologists (Binder
    et al., [1995](#bib.bib55)). Therefore, to minimize the diagnostic errors that
    result from the difficulty and the subjectivity of visual interpretation and to
    reduce the burden of skin diseases and limited access to dermatologists, the development
    of CAD systems is crucial.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动化皮肤病变分析中，通常使用两种影像模式（Daneshjou et al., [2022](#bib.bib109)）：皮肤镜（显微）图像和临床（宏观）图像。皮肤镜图像能够检查肉眼不可见的病变特征，但即使对皮肤科医生而言也并非总是可用（Engasser
    and Warshaw, [2010](#bib.bib133)）。另一方面，使用常规相机获取的临床图像易于获得，但质量较低。皮肤镜检查是一种非侵入性的皮肤成像技术，通过让皮肤科医生可视化皮下结构来帮助诊断皮肤病变（Kittler
    et al., [2002](#bib.bib229)）。然而，即使使用皮肤镜，诊断准确性也可能有很大差异，范围从$24\%$到$77\%$，取决于临床医生的专业水平（Tran
    et al., [2005](#bib.bib378)）。此外，皮肤镜在经验不足的皮肤科医生手中实际上可能会降低诊断准确性（Binder et al., [1995](#bib.bib55)）。因此，为了减少由于视觉解释的困难和主观性所导致的诊断错误，并减轻皮肤病的负担以及皮肤科医生的有限可及性，开发计算机辅助诊断系统是至关重要的。
- en: Segmentation is the partitioning of an image into meaningful regions. Semantic
    segmentation, in particular, assigns appropriate class labels to each region.
    For skin lesions, the task is almost always binary, separating the lesion from
    the surrounding skin. Automated skin lesion segmentation is hindered by illumination
    and contrast issues, intrinsic inter-class similarities and intra-class variability,
    occlusions, artifacts, and the diversity of imaging tools used. The lack of large
    datasets with ground-truth segmentation masks generated by experts compounds the
    problem, impeding both the training of models and their reliable evaluation. Skin
    lesion images are occluded by natural artifacts such as hair (Fig. [1(a)](#S1.F1.sf1
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation")),
    blood vessels (Fig. [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), and artificial ones such as
    surgical marker annotations (Fig. [1(c)](#S1.F1.sf3 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")), lens artifacts (dark
    corners) (Fig. [1(d)](#S1.F1.sf4 "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation")), and air bubbles (Fig. [1(e)](#S1.F1.sf5
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).
    Intrinsic factors such as lesion size and shape variation (Fig. [1(f)](#S1.F1.sf6
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation")
    and [1(g)](#S1.F1.sf7 "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation")), different skin colors (Fig. [1(h)](#S1.F1.sf8
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation")),
    low contrast (Fig. [1(i)](#S1.F1.sf9 "In Figure 1 ‣ 1 Introduction ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), and ambiguous boundaries (Fig. [1(h)](#S1.F1.sf8
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation"))
    complicate the automated segmentation of skin lesions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分割是将图像划分为有意义区域的过程。特别是语义分割，为每个区域分配适当的类别标签。对于皮肤病变，任务几乎总是二分类的，将病变与周围皮肤分开。自动化皮肤病变分割受限于照明和对比度问题、固有的类别间相似性和类别内变异、遮挡、伪影以及使用的成像工具的多样性。缺乏由专家生成的真实分割掩膜的大型数据集进一步加剧了问题，阻碍了模型的训练和可靠评估。皮肤病变图像被自然伪影如毛发（图[1(a)](#S1.F1.sf1
    "在图1 ‣ 1 引言 ‣ 关于皮肤病变分割的深度学习综述")）、血管（图[1(b)](#S1.F1.sf2 "在图1 ‣ 1 引言 ‣ 关于皮肤病变分割的深度学习综述")）以及人工伪影如外科标记注释（图[1(c)](#S1.F1.sf3
    "在图1 ‣ 1 引言 ‣ 关于皮肤病变分割的深度学习综述")）、镜头伪影（暗角）（图[1(d)](#S1.F1.sf4 "在图1 ‣ 1 引言 ‣ 关于皮肤病变分割的深度学习综述")）和气泡（图[1(e)](#S1.F1.sf5
    "在图1 ‣ 1 引言 ‣ 关于皮肤病变分割的深度学习综述")）遮挡。固有因素如病变的大小和形状变化（图[1(f)](#S1.F1.sf6 "在图1 ‣ 1
    引言 ‣ 关于皮肤病变分割的深度学习综述") 和[1(g)](#S1.F1.sf7 "在图1 ‣ 1 引言 ‣ 关于皮肤病变分割的深度学习综述")）、不同的肤色（图[1(h)](#S1.F1.sf8
    "在图1 ‣ 1 引言 ‣ 关于皮肤病变分割的深度学习综述")）、低对比度（图[1(i)](#S1.F1.sf9 "在图1 ‣ 1 引言 ‣ 关于皮肤病变分割的深度学习综述")）和模糊边界（图[1(h)](#S1.F1.sf8
    "在图1 ‣ 1 引言 ‣ 关于皮肤病变分割的深度学习综述")）使得皮肤病变的自动分割更加复杂。
- en: '![Refer to caption](img/4ce85f98d07e7289005fc65bb3645cd2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4ce85f98d07e7289005fc65bb3645cd2.png)'
- en: (a) Hairs
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 毛发
- en: '![Refer to caption](img/2e88b01dcc49f95d30b1f512f7f24272.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2e88b01dcc49f95d30b1f512f7f24272.png)'
- en: (b) Blood vessels
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 血管
- en: '![Refer to caption](img/c1de0f3633244e028d0617a52d8677a1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c1de0f3633244e028d0617a52d8677a1.png)'
- en: (c) Surgical marking
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 外科标记
- en: '![Refer to caption](img/723d4b16631977bfe6ad4cafd16ec889.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/723d4b16631977bfe6ad4cafd16ec889.png)'
- en: (d) Irregular border and black frame
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 不规则边界和黑色边框
- en: '![Refer to caption](img/d8834cede825ff2bba8ed22db26dffc6.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d8834cede825ff2bba8ed22db26dffc6.png)'
- en: (e) Bubbles
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 气泡
- en: '![Refer to caption](img/c2ae928f72d89c60cbe48bea953d5b9c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c2ae928f72d89c60cbe48bea953d5b9c.png)'
- en: (f) Very small lesion
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 非常小的病变
- en: '![Refer to caption](img/9ba57e8cced90f761b121a3f8a43f438.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9ba57e8cced90f761b121a3f8a43f438.png)'
- en: (g) Very large lesion
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 非常大的病变
- en: '![Refer to caption](img/b1ccb1ba0dee81e1a0d639338744f0eb.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b1ccb1ba0dee81e1a0d639338744f0eb.png)'
- en: (h) Fuzzy border and variegated coloring
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (h) 模糊边界和花纹色彩
- en: '![Refer to caption](img/437b2937add4639427f7ead48a0bfbff.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/437b2937add4639427f7ead48a0bfbff.png)'
- en: (i) Low contrast and color calibration chart
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 低对比度和颜色校准图
- en: 'Figure 1: Factors that complicate dermoscopy image segmentation (image source:
    ISIC 2016 dataset (Gutman et al., [2016](#bib.bib167))).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使皮肤镜图像分割变得复杂的因素（图片来源：ISIC 2016 数据集（Gutman 等，[2016](#bib.bib167)））。
- en: <svg   height="576.17" overflow="visible" version="1.1" width="580.52"><g transform="translate(0,576.17)
    matrix(1 0 0 -1 0 0) translate(283.59,0) translate(0,292.58)" fill="#000000" stroke="#000000"
    stroke-width="1.2pt"><g transform="matrix(1.0 0.0 0.0 1.0 -68.9 5.9)" fill="#000000"
    stroke="#000000"><foreignobject width="137.8" height="31.18" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">DL-based Skin Lesion Segmentation <g fill="#99FF99"
    stroke="#99FF99"><path d="M -94.9 -139.19 C -94.9 -114.73 -114.73 -94.9 -139.19
    -94.9 C -163.66 -94.9 -183.49 -114.73 -183.49 -139.19 C -183.49 -163.66 -163.66
    -183.49 -139.19 -183.49 C -114.73 -183.49 -94.9 -163.66 -94.9 -139.19 Z M -139.19
    -139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -178.56 -134.35)" fill="#000000"
    stroke="#000000"><foreignobject width="78.74" height="29.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Future Research §[5](#S5 "5 Discussion and
    Future Research ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M -55.68 -55.68 M -33.28 -71.36 C -50.06 -63.54 -63.54 -50.06
    -71.36 -33.28 C -63.04 -51.12 -72.34 -66.86 -86.26 -80.78 L -80.78 -86.26 C -66.86
    -72.34 -51.12 -63.04 -33.28 -71.36 Z M -89.47 -94.95 L -80.78 -86.26 L -86.26
    -80.78 L -94.95 -89.47 Z M -86.26 -80.78 M -99.05 -120.47 C -103.45 -111.03 -111.03
    -103.45 -120.47 -99.05 C -110.44 -103.73 -102.78 -97.3 -94.95 -89.47 L -89.47
    -94.95 C -97.3 -102.78 -103.73 -110.44 -99.05 -120.47 Z"></path></clippath><g
    clip-path="url(#pgfcp9)"><g transform="matrix(1.0 0.0 0.0 1.0 -55.68 -55.68)"><g
    fill="#FFCC99"><path d="M 54.71 -56.66 L 111.36 -0.01 L -0.01 111.36 L -56.66
    54.71 Z M -0.01 111.36" style="stroke:none"></path></g><g fill="#99FF99"><path
    d="M 4.46 -106.9 L -27.83 -139.2 L -139.2 -27.83 L -106.9 4.46 Z M -139.2 -27.83"
    style="stroke:none"><g transform="matrix(-0.37585 -0.37585 0.80185 -0.80185 -26.1
    -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#99CCCC" stroke="#99CCCC"><path d="M 183.49 -139.19 C 183.49 -114.73 163.66
    -94.9 139.19 -94.9 C 114.73 -94.9 94.9 -114.73 94.9 -139.19 C 94.9 -163.66 114.73
    -183.49 139.19 -183.49 C 163.66 -183.49 183.49 -163.66 183.49 -139.19 Z M 139.19
    -139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 99.82 -142.65)" fill="#000000"
    stroke="#000000"><foreignobject width="78.74" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Evaluation §[4](#S4 "4 Evaluation ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")</foreignobject></g> <clippath
    ><path d="M 55.68 -55.68 M 71.36 -33.28 C 63.54 -50.06 50.06 -63.54 33.28 -71.36
    C 51.12 -63.04 66.86 -72.34 80.78 -86.26 L 86.26 -80.78 C 72.34 -66.86 63.04 -51.12
    71.36 -33.28 Z M 94.95 -89.47 L 86.26 -80.78 L 80.78 -86.26 L 89.47 -94.95 Z M
    80.78 -86.26 M 120.47 -99.05 C 111.03 -103.45 103.45 -111.03 99.05 -120.47 C 103.73
    -110.44 97.3 -102.78 89.47 -94.95 L 94.95 -89.47 C 102.78 -97.3 110.44 -103.73
    120.47 -99.05 Z"></path></clippath><g clip-path="url(#pgfcp11)"><g transform="matrix(1.0
    0.0 0.0 1.0 55.68 -55.68)"><g fill="#FFCC99"><path d="M 56.66 54.71 L 0.01 111.36
    L -111.36 -0.01 L -54.71 -56.66 Z M -111.36 -0.01" style="stroke:none"></path></g><g
    fill="#99CCCC"><path d="M 106.9 4.46 L 139.2 -27.83 L 27.83 -139.2 L -4.46 -106.9
    Z M 27.83 -139.2" style="stroke:none"><g transform="matrix(0.37585 -0.37585 0.80185
    0.80185 26.1 -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#99CCCC" stroke="#99CCCC"><path d="M 173.64 -257.3 C 173.64 -238.28 158.22
    -222.86 139.19 -222.86 C 120.17 -222.86 104.75 -238.28 104.75 -257.3 C 104.75
    -276.33 120.17 -291.75 139.19 -291.75 C 158.22 -291.75 173.64 -276.33 173.64 -257.3
    Z M 139.19 -257.3"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 109.67 -261.46)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Metrics §[4.3](#S4.SS3 "4.3 Evaluation Metrics
    ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#99CCCC"><path d="M 139.19 -183.49 M 154.34 -180.81 C 144.56 -184.38
    133.83 -184.38 124.05 -180.81 C 134.45 -184.6 136.18 -194.56 136.18 -205.63 L
    142.21 -205.63 C 142.21 -194.56 143.94 -184.6 154.34 -180.81 Z M 142.21 -205.63
    L 142.21 -205.63 L 136.18 -205.63 L 136.18 -205.63 Z M 136.18 -205.63 M 150.98
    -224.93 C 143.37 -222.16 135.02 -222.16 127.41 -224.93 C 135.5 -221.99 136.18
    -214.24 136.18 -205.63 L 142.21 -205.63 C 142.21 -214.24 142.88 -221.99 150.98
    -224.93 Z" style="stroke:none"></path></g><g fill="#99CCCC" stroke="#99CCCC"><path
    d="M 261.5 -222.71 C 261.5 -201.29 244.13 -183.92 222.71 -183.92 C 201.29 -183.92
    183.92 -201.29 183.92 -222.71 C 183.92 -244.13 201.29 -261.5 222.71 -261.5 C 244.13
    -261.5 261.5 -244.13 261.5 -222.71 Z M 222.71 -222.71"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 193.18 -210.26)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="44.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Inter- Annotator
    Agreement §[4.2](#S4.SS2 "4.2 Inter-Annotator Agreement ‣ 4 Evaluation ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")</foreignobject></g> <g fill="#99CCCC"><path
    d="M 170.51 -170.51 M 179.34 -157.91 C 174.94 -167.35 167.35 -174.94 157.91 -179.34
    C 167.95 -174.66 175.94 -180.74 183.77 -188.57 L 188.57 -183.77 C 180.74 -175.94
    174.66 -167.95 179.34 -157.91 Z M 183.97 -179.17 L 188.57 -183.77 L 183.77 -188.57
    L 179.17 -183.97 Z M 183.77 -188.57 M 206.32 -187.55 C 198.05 -191.41 191.41 -198.05
    187.55 -206.32 C 191.65 -197.53 186.02 -190.82 179.17 -183.97 L 183.97 -179.17
    C 190.82 -186.02 197.53 -191.65 206.32 -187.55 Z" style="stroke:none"></path></g><g
    fill="#99CCCC" stroke="#99CCCC"><path d="M 296.1 -139.19 C 296.1 -117.77 278.73
    -100.4 257.3 -100.4 C 235.88 -100.4 218.51 -117.77 218.51 -139.19 C 218.51 -160.62
    235.88 -177.98 257.3 -177.98 C 278.73 -177.98 296.1 -160.62 296.1 -139.19 Z M
    257.3 -139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 227.78 -126.74)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="44.28" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Segmentation Annotation §[4.1](#S4.SS1 "4.1
    Segmentation Annotation ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")</foreignobject></g> <g fill="#99CCCC"><path d="M 183.49 -139.19
    M 180.81 -124.05 C 184.38 -133.83 184.38 -144.56 180.81 -154.34 C 184.6 -143.94
    194.56 -142.59 205.63 -142.59 L 205.63 -135.8 C 194.56 -135.8 184.6 -134.45 180.81
    -124.05 Z M 205.63 -142.59 h -6.51 v 6.79 h 6.51 Z M 220.85 -125.93 C 217.73 -134.5
    217.73 -143.89 220.85 -152.46 C 217.54 -143.35 208.82 -142.59 199.12 -142.59 L
    199.12 -135.8 C 208.82 -135.8 217.54 -135.04 220.85 -125.93 Z" style="stroke:none"></path></g><g
    fill="#FFB3B3" stroke="#FFB3B3"><path d="M 183.49 139.19 C 183.49 163.66 163.66
    183.49 139.19 183.49 C 114.73 183.49 94.9 163.66 94.9 139.19 C 94.9 114.73 114.73
    94.9 139.19 94.9 C 163.66 94.9 183.49 114.73 183.49 139.19 Z M 139.19 139.19"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 99.82 144.04)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Model
    Design & Training §[3](#S3 "3 Model Design and Training ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation")</foreignobject></g> <clippath ><path d="M 55.68
    55.68 M 33.28 71.36 C 50.06 63.54 63.54 50.06 71.36 33.28 C 63.04 51.12 72.34
    66.86 86.26 80.78 L 80.78 86.26 C 66.86 72.34 51.12 63.04 33.28 71.36 Z M 89.47
    94.95 L 80.78 86.26 L 86.26 80.78 L 94.95 89.47 Z M 86.26 80.78 M 99.05 120.47
    C 103.45 111.03 111.03 103.45 120.47 99.05 C 110.44 103.73 102.78 97.3 94.95 89.47
    L 89.47 94.95 C 97.3 102.78 103.73 110.44 99.05 120.47 Z"></path></clippath><g
    clip-path="url(#pgfcp13)"><g transform="matrix(1.0 0.0 0.0 1.0 55.68 55.68)"><g
    fill="#FFCC99"><path d="M -54.71 56.66 L -111.36 0.01 L 0.01 -111.36 L 56.66 -54.71
    Z M 0.01 -111.36" style="stroke:none"></path></g><g fill="#FFB3B3"><path d="M
    -4.46 106.9 L 27.83 139.2 L 139.2 27.83 L 106.9 -4.46 Z M 139.2 27.83" style="stroke:none"><g
    transform="matrix(0.37585 0.37585 -0.80185 0.80185 26.1 26.1)"><defs><lineargradient
    ></lineargradient></defs></g></path></g></g></g><g fill="#FFB3B3" stroke="#FFB3B3"><path
    d="M 282.76 184.39 C 282.76 203.42 267.34 218.84 248.31 218.84 C 229.29 218.84
    213.86 203.42 213.86 184.39 C 213.86 165.37 229.29 149.95 248.31 149.95 C 267.34
    149.95 282.76 165.37 282.76 184.39 Z M 248.31 184.39"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 218.78 188.55)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Loss Functions
    §[3.2](#S3.SS2 "3.2 Loss Functions ‣ 3 Model Design and Training ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")</foreignobject></g> <g fill="#FFB3B3"><path
    d="M 180.11 156.14 M 171.85 169.12 C 178.88 161.44 182.99 151.53 183.44 141.13
    C 182.96 152.19 191.5 157.6 201.73 161.83 L 199.42 167.4 C 189.19 163.17 179.33
    160.95 171.85 169.12 Z M 199.42 167.4 L 199.42 167.4 L 201.73 161.83 L 201.72
    161.83 Z M 201.73 161.83 M 213.89 182.89 C 214.25 174.8 217.44 167.09 222.91 161.12
    C 217.09 167.47 209.68 165.13 201.72 161.83 L 199.42 167.4 C 207.37 170.7 214.27
    174.29 213.89 182.89 Z" style="stroke:none"></path></g><g fill="#FFB3B3" stroke="#FFB3B3"><path
    d="M 218.84 248.31 C 218.84 267.34 203.42 282.76 184.39 282.76 C 165.37 282.76
    149.95 267.34 149.95 248.31 C 149.95 229.29 165.37 213.86 184.39 213.86 C 203.42
    213.86 218.84 229.29 218.84 248.31 Z M 184.39 248.31"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 154.87 252.46)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Model Architecture
    §[3.1](#S3.SS1 "3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation")</foreignobject></g> <g fill="#FFB3B3"><path
    d="M 156.14 180.11 M 141.13 183.44 C 151.53 182.99 161.44 178.88 169.12 171.85
    C 160.95 179.33 163.17 189.19 167.4 199.42 L 161.83 201.73 C 157.6 191.5 152.19
    182.96 141.13 183.44 Z M 161.83 201.72 L 161.83 201.73 L 167.4 199.42 L 167.4
    199.42 Z M 167.4 199.42 M 161.12 222.91 C 167.09 217.44 174.8 214.25 182.89 213.89
    C 174.29 214.27 170.7 207.37 167.4 199.42 L 161.83 201.72 C 165.13 209.68 167.47
    217.09 161.12 222.91 Z" style="stroke:none"></path></g><g fill="#B3B3FF" stroke="#B3B3FF"><path
    d="M -94.9 139.19 C -94.9 163.66 -114.73 183.49 -139.19 183.49 C -163.66 183.49
    -183.49 163.66 -183.49 139.19 C -183.49 114.73 -163.66 94.9 -139.19 94.9 C -114.73
    94.9 -94.9 114.73 -94.9 139.19 Z M -139.19 139.19"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -178.56 135.74)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Input Data
    §[2](#S2 "2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M -55.68 55.68 M -71.36 33.28 C -63.54 50.06 -50.06 63.54
    -33.28 71.36 C -51.12 63.04 -66.86 72.34 -80.78 86.26 L -86.26 80.78 C -72.34
    66.86 -63.04 51.12 -71.36 33.28 Z M -94.95 89.47 L -86.26 80.78 L -80.78 86.26
    L -89.47 94.95 Z M -80.78 86.26 M -120.47 99.05 C -111.03 103.45 -103.45 111.03
    -99.05 120.47 C -103.73 110.44 -97.3 102.78 -89.47 94.95 L -94.95 89.47 C -102.78
    97.3 -110.44 103.73 -120.47 99.05 Z"></path></clippath><g clip-path="url(#pgfcp15)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -55.68 55.68)"><g fill="#FFCC99"><path d="M
    -56.66 -54.71 L -0.01 -111.36 L 111.36 0.01 L 54.71 56.66 Z M 111.36 0.01" style="stroke:none"></path></g><g
    fill="#B3B3FF"><path d="M -106.9 -4.46 L -139.2 27.83 L -27.83 139.2 L 4.46 106.9
    Z M -27.83 139.2" style="stroke:none"><g transform="matrix(-0.37585 0.37585 -0.80185
    -0.80185 -26.1 26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#B3B3FF" stroke="#B3B3FF"><path d="M -59.55 248.31 C -59.55 267.34 -74.97
    282.76 -93.99 282.76 C -113.02 282.76 -128.44 267.34 -128.44 248.31 C -128.44
    229.29 -113.02 213.86 -93.99 213.86 C -74.97 213.86 -59.55 229.29 -59.55 248.31
    Z M -93.99 248.31"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -123.52 252.46)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Image Preprocessing §[2.4](#S2.SS4 "2.4 Image
    Preprocessing ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#B3B3FF"><path d="M -122.24 180.11 M -137.26 183.44 C -126.86 182.99
    -116.95 178.88 -109.27 171.85 C -117.43 179.33 -115.22 189.19 -110.98 199.42 L
    -116.55 201.73 C -120.79 191.5 -126.2 182.96 -137.26 183.44 Z M -116.56 201.72
    L -116.55 201.73 L -110.98 199.42 L -110.99 199.42 Z M -110.98 199.42 M -117.27
    222.91 C -111.3 217.44 -103.59 214.25 -95.5 213.89 C -104.1 214.27 -107.69 207.37
    -110.99 199.42 L -116.56 201.72 C -113.26 209.68 -110.92 217.09 -117.27 222.91
    Z" style="stroke:none"></path></g><g fill="#B3B3FF" stroke="#B3B3FF"><path d="M
    -149.95 248.31 C -149.95 267.34 -165.37 282.76 -184.39 282.76 C -203.42 282.76
    -218.84 267.34 -218.84 248.31 C -218.84 229.29 -203.42 213.86 -184.39 213.86 C
    -165.37 213.86 -149.95 229.29 -149.95 248.31 Z M -184.39 248.31"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -213.92 252.46)" fill="#000000" stroke="#000000"><foreignobject
    width="59.06" height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Supervision
    §[2.3](#S2.SS3 "2.3 Supervised, Semi-supervised, Weakly supervised, Self-supervised
    learning ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#B3B3FF"><path d="M -156.14 180.11 M -169.12 171.85 C -161.44 178.88
    -151.53 182.99 -141.13 183.44 C -152.19 182.96 -157.6 191.5 -161.83 201.73 L -167.4
    199.42 C -163.17 189.19 -160.95 179.33 -169.12 171.85 Z M -167.4 199.42 L -167.4
    199.42 L -161.83 201.73 L -161.83 201.72 Z M -161.83 201.73 M -182.89 213.89 C
    -174.8 214.25 -167.09 217.44 -161.12 222.91 C -167.47 217.09 -165.13 209.68 -161.83
    201.72 L -167.4 199.42 C -170.7 207.37 -174.29 214.27 -182.89 213.89 Z" style="stroke:none"></path></g><g
    fill="#B3B3FF" stroke="#B3B3FF"><path d="M -213.86 184.39 C -213.86 203.42 -229.29
    218.84 -248.31 218.84 C -267.34 218.84 -282.76 203.42 -282.76 184.39 C -282.76
    165.37 -267.34 149.95 -248.31 149.95 C -229.29 149.95 -213.86 165.37 -213.86 184.39
    Z M -248.31 184.39"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -277.84 188.55)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Synthetic Data §[2.2](#S2.SS2 "2.2 Synthetic
    Data Generation ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#B3B3FF"><path d="M -180.11 156.14 M -183.44 141.13 C -182.99 151.53
    -178.88 161.44 -171.85 169.12 C -179.33 160.95 -189.19 163.17 -199.42 167.4 L
    -201.73 161.83 C -191.5 157.6 -182.96 152.19 -183.44 141.13 Z M -201.72 161.83
    L -201.73 161.83 L -199.42 167.4 L -199.42 167.4 Z M -199.42 167.4 M -222.91 161.12
    C -217.44 167.09 -214.25 174.8 -213.89 182.89 C -214.27 174.29 -207.37 170.7 -199.42
    167.4 L -201.72 161.83 C -209.68 165.13 -217.09 167.47 -222.91 161.12 Z" style="stroke:none"></path></g><g
    fill="#B3B3FF" stroke="#B3B3FF"><path d="M -213.86 93.99 C -213.86 113.02 -229.29
    128.44 -248.31 128.44 C -267.34 128.44 -282.76 113.02 -282.76 93.99 C -282.76
    74.97 -267.34 59.55 -248.31 59.55 C -229.29 59.55 -213.86 74.97 -213.86 93.99
    Z M -248.31 93.99"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -277.84 89.84)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Datasets §[2.1](#S2.SS1 "2.1 Datasets ‣ 2 Input
    Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g><g
    fill="#B3B3FF"><path d="M -180.11 122.24 M -171.85 109.27 C -178.88 116.95 -182.99
    126.86 -183.44 137.26 C -182.96 126.2 -191.5 120.79 -201.73 116.55 L -199.42 110.98
    C -189.19 115.22 -179.33 117.43 -171.85 109.27 Z M -199.42 110.99 L -199.42 110.98
    L -201.73 116.55 L -201.72 116.56 Z M -201.73 116.55 M -213.89 95.5 C -214.25
    103.59 -217.44 111.3 -222.91 117.27 C -217.09 110.92 -209.68 113.26 -201.72 116.56
    L -199.42 110.99 C -207.37 107.69 -214.27 104.1 -213.89 95.5 Z" style="stroke:none"></path></g>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`<svg height="576.17" overflow="visible" version="1.1" width="580.52"><g transform="translate(0,576.17)
    matrix(1 0 0 -1 0 0) translate(283.59,0) translate(0,292.58)" fill="#000000" stroke="#000000"
    stroke-width="1.2pt"><g transform="matrix(1.0 0.0 0.0 1.0 -68.9 5.9)" fill="#000000"
    stroke="#000000"><foreignobject width="137.8" height="31.18" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">基于 DL 的皮肤病变分割 <g fill="#99FF99" stroke="#99FF99"><path
    d="M -94.9 -139.19 C -94.9 -114.73 -114.73 -94.9 -139.19 -94.9 C -163.66 -94.9
    -183.49 -114.73 -183.49 -139.19 C -183.49 -163.66 -163.66 -183.49 -139.19 -183.49
    C -114.73 -183.49 -94.9 -163.66 -94.9 -139.19 Z M -139.19 -139.19"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -178.56 -134.35)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">未来研究
    §[5](#S5 "5 讨论与未来研究 ‣ 深度学习在皮肤病变分割中的应用综述")</foreignobject></g> <clippath ><path
    d="M -55.68 -55.68 M -33.28 -71.36 C -50.06 -63.54 -63.54 -50.06 -71.36 -33.28
    C -63.04 -51.12 -72.34 -66.86 -86.26 -80.78 L -80.78 -86.26 C -66.86 -72.34 -51.12
    -63.04 -33.28 -71.36 Z M -89.47 -94.95 L -80.78 -86.26 L -86.26 -80.78 L -94.95
    -89.47 Z M -86.26 -80.78 M -99.05 -120.47 C -103.45 -111.03 -111.03 -103.45 -120.47
    -99.05 C -110.44 -103.73 -102.78 -97.3 -94.95 -89.47 L -89.47 -94.95 C -97.3 -102.78
    -103.73 -110.44 -99.05 -120.47 Z"></path></clippath><g clip-path="url(#pgfcp9)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -55.68 -55.68)"><g fill="#FFCC99"><path d="M
    54.71 -56.66 L 111.36 -0.01 L -0.01 111.36 L -56.66 54.71 Z M -0.01 111.36" style="stroke:none"></path></g><g
    fill="#99FF99"><path d="M 4.46 -106.9 L -27.83 -139.2 L -139.2 -27.83 L -106.9
    4.46 Z M -139.2 -27.83" style="stroke:none"><g transform="matrix(-0.37585 -0.37585
    0.80185 -0.80185 -26.1 -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#99CCCC" stroke="#99CCCC"><path d="M 183.49 -139.19 C 183.49 -114.73 163.66
    -94.9 139.19 -94.9 C 114.73 -94.9 94.9 -114.73 94.9 -139.19 C 94.9 -163.66 114.73
    -183.49 139.19 -183.49 C 163.66 -183.49 183.49 -163.66 183.49 -139.19 Z M 139.19
    -139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 99.82 -142.65)" fill="#000000"
    stroke="#000000"><foreignobject width="78.74" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">评估 §[4](#S4 "4 评估 ‣ 深度学习在皮肤病变分割中的应用综述")</foreignobject></g>
    <clippath ><path d="M 55.68 -55.68 M 71.36 -33.28 C 63.54 -50.06 50.06 -63.54
    33.28 -71.36 C 51.12 -63.04 66.86 -72.34 80.78 -86.26 L 86.26 -80.78 C 72.34 -66.86
    63.04 -51.12 71.36 -33.28 Z M 94.95 -89.47 L 86.26 -80.78 L 80.78 -86.26 L 89.47
    -94.95 Z M 80.78 -86.26 M 120.47 -99.05 C 111.03 -103.45 103.45 -111.03 99.05
    -120.47 C 103.73 -110.44 97.3 -102.78 89.47 -94.95 L 94.95 -89.47 C 102.78 -97.3
    110.44 -103.73 120.47 -99.05 Z"></path></clippath><g clip-path="url(#pgfcp11)"><g
    transform="matrix(1.0 0.0 0.0 1.0 55.68 -55.68)"><g fill="#FFCC99"><path d="M
    56.66 54.71 L 0.01 111.36 L -111.36 -0.01 L -54.71 -56.66 Z M -111.36 -0.01" style="stroke:none"></path></g><g
    fill="#99CCCC"><path d="M 106.9 4.46 L 139.2 -27.83 L 27.83 -139.2 L -4.46 -106.9
    Z M 27.83 -139.2" style="stroke:none"><g transform="matrix(0.37585 -0.37585 0.80185
    0.80185 26.1 -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#99CCCC" stroke="#99CCCC"><path d="M 173.64 -257.3 C 173.64 -238.28 158.22
    -222.86 139.19 -222.86 C 120.17 -222.86 104.75 -238.28 104.75 -257.3 C 104.75
    -276.33 120.17 -291.75 139.19 -291.75 C 158.22 -291.75 173.64 -276.33 173.64 -257.3
    Z M 139.19 -257.3"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 109.67 -261.46)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">指标 §[4.3](#S4.SS3 "4.3 评估指标 ‣ 4 评估 ‣ 深度学习在皮肤病变分割中的应用综述")</foreignobject></g>
    <g fill="#99CCCC"><path d="M 139.19 -183.49 M 154.34 -180.81 C 144.56 -184.38
    133.83 -184.38 124.05 -180.81 C 134.45 -184.6 136.18 -194.56 136.18 -205.63 L
    142.21 -205.63 C 142.21 -194.56 143.94 -184.6 154.34 -180.81 Z M 142.21 -205.63
    L 142.21 -205.63 L 136.18 -205.63 L 136.18'
- en: 'Figure 2: An overview of the various components of this review. We structure
    the review based on the different elements of a DL-based segmentation pipeline
    and conclude it with discussions on future potential research directions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：本综述各个组成部分的概述。我们根据 DL 基于分割管道的不同元素来构建综述，并以对未来潜在研究方向的讨论作为结论。
- en: Before the deep learning (DL) revolution, segmentation was based on classical
    image processing and machine learning techniques such as adaptive thresholding (Green
    et al., [1994](#bib.bib157); Celebi et al., [2013](#bib.bib81)), active contours (Erkol
    et al., [2005](#bib.bib134)), region growing (Iyatomi et al., [2006](#bib.bib189);
    Celebi et al., [2007a](#bib.bib73)), unsupervised clustering (Gómez et al., [2007](#bib.bib150)),
    and support vector machines (Zortea et al., [2011](#bib.bib458)). These approaches
    depend on hand-crafted features, which are difficult to engineer and often limit
    invariance and discriminative power from the outset. As a result, such conventional
    segmentation algorithms do not always perform well on larger and more complex
    datasets. In contrast, DL integrates feature extraction and task-specific decision
    seamlessly, and does not just cope with, but actually requires larger datasets.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习（DL）革命之前，分割基于经典的图像处理和机器学习技术，例如自适应阈值（Green et al., [1994](#bib.bib157);
    Celebi et al., [2013](#bib.bib81)），主动轮廓（Erkol et al., [2005](#bib.bib134)），区域生长（Iyatomi
    et al., [2006](#bib.bib189); Celebi et al., [2007a](#bib.bib73)），无监督聚类（Gómez et
    al., [2007](#bib.bib150)），以及支持向量机（Zortea et al., [2011](#bib.bib458)）。这些方法依赖于手工制作的特征，这些特征难以工程化，并且通常从一开始就限制了不变性和辨别能力。因此，这些传统的分割算法在更大且更复杂的数据集上并不总是表现良好。相比之下，DL
    无缝集成了特征提取和任务特定决策，并且不仅应对大数据集，实际上还需要更大的数据集。
- en: '*Survey of surveys.* Celebi et al. ([2009b](#bib.bib76)) reviewed 18 skin lesion
    segmentation algorithms for dermoscopic images, published between 1998 and 2008,
    with their required preprocessing and postprocessing steps. Celebi et al. ([2015b](#bib.bib82))
    later extended their work with 32 additional algorithms published between 2009
    and 2014, discussing performance evaluation and computational requirements of
    each approach, and suggesting guidelines for future works. Both surveys appeared
    before DL was widely adopted for skin lesion segmentation, but cover all the important
    works based on classical image processing and machine learning. Adegun and Viriri
    ([2020a](#bib.bib11)) reviewed the literature on DL-based skin image analysis,
    with an emphasis on the best-performing algorithms in the ISIC (International
    Skin Imaging Collaboration) Skin Image Analysis Challenges 2018 (Codella et al.,
    [2019](#bib.bib96)) and 2019 (Tschandl et al., [2018](#bib.bib381); Codella et al.,
    [2018](#bib.bib98); Combalia et al., [2019](#bib.bib101)). However, since their
    review focused on the ISIC Challenges 2018 and 2019, it is more general as it
    covers both lesion classification and segmentation. Consequently, the number of
    papers surveyed for skin lesion segmentation by Adegun and Viriri ([2020a](#bib.bib11))
    is almost an order of magnitude smaller than that in this review.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*文献综述。* Celebi et al. ([2009b](#bib.bib76)) 综述了 18 种用于皮肤病变 dermoscopic 图像的分割算法，这些算法发布于
    1998 到 2008 年间，并介绍了它们所需的预处理和后处理步骤。Celebi et al. ([2015b](#bib.bib82)) 随后扩展了他们的工作，增加了
    32 种在 2009 到 2014 年间发布的算法，讨论了每种方法的性能评估和计算需求，并提出了未来工作的指导方针。这些综述都出现在 DL 被广泛应用于皮肤病变分割之前，但涵盖了所有基于经典图像处理和机器学习的重要工作。Adegun
    和 Viriri ([2020a](#bib.bib11)) 综述了基于 DL 的皮肤图像分析文献，重点关注 ISIC（国际皮肤成像合作）皮肤图像分析挑战
    2018（Codella et al., [2019](#bib.bib96)）和 2019（Tschandl et al., [2018](#bib.bib381);
    Codella et al., [2018](#bib.bib98); Combalia et al., [2019](#bib.bib101)）中的最佳表现算法。然而，由于他们的综述关注于
    ISIC 挑战 2018 和 2019，因此它更为通用，涵盖了病变分类和分割。因此，Adegun 和 Viriri ([2020a](#bib.bib11))
    对皮肤病变分割的文献综述的论文数量比本综述要少一个数量级。'
- en: '*Main contributions.* No existing survey approaches the present work in breadth
    or depth, as we cross-examine $177$ research papers that deal with the automated
    segmentation of skin lesions in clinical and dermoscopic images. We analyze the
    works along several dimensions, including input data (datasets, preprocessing,
    and synthetic data generation), model design (architecture, modules, and losses),
    and evaluation (data annotation and evaluation metrics). We discuss these dimensions
    both from the viewpoint of select seminal works, and from a systematic viewpoint,
    examining how those choices have influenced current trends, and how their limitations
    should be addressed. We summarize all examined works in a comprehensive table
    to facilitate comparisons.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*主要贡献。* 现有的调查没有在广度或深度上接近本工作，因为我们交叉审查了 $177$ 篇涉及临床和皮肤镜图像中皮肤病变自动分割的研究论文。我们从多个维度分析了这些工作，包括输入数据（数据集、预处理和合成数据生成）、模型设计（架构、模块和损失）以及评估（数据注释和评估指标）。我们从一些重要工作的视角以及系统视角讨论了这些维度，审视这些选择如何影响当前趋势以及如何解决其局限性。我们在一张综合表格中总结了所有审查过的工作，以便于比较。'
- en: '*Search strategy.* We searched DBLP and Arxiv Sanity Preserver for all scholarly
    publications: peer-reviewed journal papers, papers published in the proceedings
    of conferences or workshops, and non-peer-reviewed preprints from 2014 to 2022\.
    The DBLP search query was (conv* | trans* | deep | neural | learn*) (skin | derm*)
    (segment* | delineat* | extract* | localiz*), thus restricting our search to DL-based
    works involving skin and segmentation. We use DBLP for our literature search because
    (a) it allows for customized search queries and lists, and (b) we did not find
    any relevant publications on other platforms (Google Scholar and PubMed) that
    were not indexed by DBLP. For unpublished preprints, we also searched on Arxiv
    Sanity Preserver using a similar query⁶⁶6Arxiv Sanity Preserver: [https://www.arxiv-sanity-lite.com/search?q=segmentation+skin+melanoma+deep+learning+convolution+transformer](https://www.arxiv-sanity-lite.com/search?q=segmentation+skin+melanoma+deep+learning+convolution+transformer).
    We filtered our search results to remove false positives ($31$ papers) and included
    only papers related to skin lesion segmentation. We excluded papers that focused
    on general skin segmentation and general skin conditions (e.g., psoriasis, acne,
    or certain sub-types of skin lesions). We also included unpublished preprints
    from arXiv, which (a) passed minimum quality checks levels and (b) had at least
    10 citations, and excluded those that were clearly of low quality. In particular,
    papers that had one or more of the following were excluded from this survey: (a)
    missing quantitative results, (b) missing important sections such as Abstract
    or Methods, (c) conspicuously poor writing quality, and (d) no methodological
    contribution. This led to the filtering out of papers of visibly low quality ((a-c)
    criteria above; $18$ papers) and those with no methodological contribution ($20$ papers).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*搜索策略。* 我们在 DBLP 和 Arxiv Sanity Preserver 上搜索了所有学术出版物：同行评审的期刊论文、会议或研讨会论文以及
    2014 年至 2022 年间的非同行评审预印本。DBLP 的搜索查询为 (conv* | trans* | deep | neural | learn*)
    (skin | derm*) (segment* | delineat* | extract* | localiz*)，从而将我们的搜索限制在涉及皮肤和分割的基于深度学习的工作上。我们使用
    DBLP 进行文献搜索的原因是 (a) 它允许定制搜索查询和列表，以及 (b) 我们在其他平台（Google Scholar 和 PubMed）上没有找到
    DBLP 未收录的相关出版物。对于未发表的预印本，我们还在 Arxiv Sanity Preserver 上使用类似的查询进行了搜索⁶⁶6Arxiv Sanity
    Preserver: [https://www.arxiv-sanity-lite.com/search?q=segmentation+skin+melanoma+deep+learning+convolution+transformer](https://www.arxiv-sanity-lite.com/search?q=segmentation+skin+melanoma+deep+learning+convolution+transformer)。我们过滤了搜索结果以去除假阳性（$31$
    篇论文），仅包括与皮肤病变分割相关的论文。我们排除了关注一般皮肤分割和一般皮肤状况（例如银屑病、痤疮或某些皮肤病变亚型）的论文。我们还包括了来自 arXiv
    的未发表预印本，这些预印本 (a) 通过了最低质量检查级别，并且 (b) 至少有 10 次引用，排除了那些明显低质量的论文。特别地，以下有一个或多个特征的论文被排除在本次调查之外：(a)
    缺少定量结果，(b) 缺少重要部分，如摘要或方法，(c) 写作质量显著差，(d) 没有方法学贡献。这导致了过滤掉明显低质量的论文（上述 (a-c) 标准；$18$
    篇论文）和没有方法学贡献的论文（$20$ 篇论文）。'
- en: 'The remaining text is organized as follows: in Section [2](#S2 "2 Input Data
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation"), we introduce the publicly
    available datasets and discuss preprocessing and synthetic data generation; in
    Section [3](#S3 "3 Model Design and Training ‣ A Survey on Deep Learning for Skin
    Lesion Segmentation"), we review the various network architectures used in deep
    segmentation models and discuss how deep models benefit from these networks. We
    also describe various loss functions designed either for general use or specifically
    for skin lesion segmentation. In Section [4](#S4 "4 Evaluation ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation"), we detail segmentation evaluation techniques
    and measures. Finally, in Section [5](#S5 "5 Discussion and Future Research ‣
    A Survey on Deep Learning for Skin Lesion Segmentation"), we discuss the open
    challenges in DL-based skin lesion segmentation and conclude our survey. A visual
    overview of the structure of this survey is presented in Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的文本组织如下：在第[2](#S2 "2 Input Data ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")节中，我们介绍了公开可用的数据集，并讨论了预处理和合成数据生成；在第[3](#S3 "3 Model Design and Training
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")节中，我们回顾了用于深度分割模型的各种网络架构，并讨论了深度模型如何从这些网络中获益。我们还描述了为一般用途或专门用于皮肤病变分割而设计的各种损失函数。在第[4](#S4
    "4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation")节中，我们详细说明了分割评估技术和措施。最后，在第[5](#S5
    "5 Discussion and Future Research ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")节中，我们讨论了基于深度学习的皮肤病变分割中的开放挑战，并总结了我们的调查。图[2](#S1.F2 "Figure 2 ‣ 1
    Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation")提供了本调查的结构的视觉概述。
- en: 2 Input Data
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 输入数据
- en: Obtaining data in sufficient quantity and quality is often a significant obstacle
    to developing effective segmentation models. State-of-the-art segmentation models
    have a huge number of adjustable parameters that allow them to generalize well,
    provided they are trained on massive labeled datasets (Sun et al., [2017](#bib.bib363);
    Buslaev et al., [2020](#bib.bib68)). Unfortunately, skin lesion datasets—like
    most medical image datasets (Asgari Taghanaki et al., [2021](#bib.bib27))—tend
    to be small (Curiel-Lewandrowski et al., [2019](#bib.bib106)) due to issues such
    as copyright, patient privacy, acquisition and annotation cost, standardization,
    and scarcity of many pathologies of interest. The two most common modalities used
    in the training of skin lesion segmentation models are clinical images, which
    are close-ups of the lesions acquired using conventional cameras, and dermoscopic
    images, which are acquired using dermoscopy, a non-invasive skin imaging through
    optical magnification, and either liquid immersion and low angle-of-incidence
    lighting, or cross-polarized lighting. Dermoscopy eliminates skin surface reflections (Kittler
    et al., [2002](#bib.bib229)), reveals subsurface skin structures, and allows the
    identification of dozens of morphological features such as atypical pigment networks,
    dots/globules, streaks, blue-white areas, and blotches (Menzies et al., [2003](#bib.bib276)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 获得足够数量和质量的数据通常是开发有效分割模型的重大障碍。最先进的分割模型具有大量可调参数，这些参数可以使它们在训练于大量标注数据集的情况下实现良好的泛化（Sun
    et al., [2017](#bib.bib363); Buslaev et al., [2020](#bib.bib68)）。不幸的是，皮肤病变数据集——像大多数医学图像数据集（Asgari
    Taghanaki et al., [2021](#bib.bib27)）——由于版权、患者隐私、采集和注释成本、标准化以及许多感兴趣病理的稀缺等问题，往往较小（Curiel-Lewandrowski
    et al., [2019](#bib.bib106)）。在皮肤病变分割模型的训练中，最常用的两种模式是临床图像，这些图像是使用传统相机拍摄的病变特写，以及皮肤镜图像，这些图像通过光学放大、液体浸泡和低入射角照明或交叉偏振照明的皮肤镜技术获得。皮肤镜可以消除皮肤表面反射（Kittler
    et al., [2002](#bib.bib229)），揭示皮肤下的结构，并允许识别几十种形态特征，如非典型色素网络、点/小球、条纹、蓝白色区域和斑点（Menzies
    et al., [2003](#bib.bib276)）。
- en: Annotation is often the greatest barrier for increasing the amount of data.
    Objective evaluation of segmentation often requires laborious *region-based annotation*,
    in which an expert manually outlines the region where the lesion (or a clinical
    feature) appears in the image. By contrast, *textual annotation* may involve diagnosis
    (e.g., melanoma, carcinoma, benign nevi), presence/absence/score of dermoscopic
    features (e.g., pigment networks, blue-white areas, streaks, globules), diagnostic
    strategy (e.g., pattern analysis, ABCD rule, 7-point checklist, 3-point checklist),
    clinical metadata (e.g., sex, age, anatomic site, familial history), and other
    details (e.g., timestamp, camera model) (Caffery et al., [2018](#bib.bib69)).
    We extensively discuss the image annotation issue in Section [4.1](#S4.SS1 "4.1
    Segmentation Annotation ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation").
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注释通常是增加数据量的最大障碍。客观评价分割常常需要繁琐的*区域基础注释*，专家手动描绘图像中病变（或临床特征）出现的区域。相比之下，*文本注释*可能涉及诊断（例如，黑色素瘤、癌症、良性痣）、皮肤镜特征的存在/缺失/评分（例如，色素网络、蓝白区、条纹、球形体）、诊断策略（例如，模式分析、ABCD规则、7点检查表、3点检查表）、临床元数据（例如，性别、年龄、解剖部位、家族历史）以及其他细节（例如，时间戳、相机型号）（Caffery等，
    [2018](#bib.bib69)）。我们在第[4.1节](#S4.SS1 "4.1 Segmentation Annotation ‣ 4 Evaluation
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")中深入讨论了图像注释问题。
- en: 2.1 Datasets
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 数据集
- en: The availability of larger, more diverse, and better-annotated datasets is one
    of the main driving factors for the advances in skin image analysis in the past
    decade (Marchetti et al., [2018](#bib.bib270); Celebi et al., [2019](#bib.bib74)).
    Works in skin image analysis date back to the 1980s (Vanker and Van Stoecker,
    [1984](#bib.bib391); Dhawan et al., [1984](#bib.bib122)), but until the mid-2000s,
    these works used small, private datasets, containing a few hundred images.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 较大、更多样化和更好注释的数据集的可用性是过去十年皮肤图像分析进展的主要推动因素之一（Marchetti等，[2018](#bib.bib270)；Celebi等，[2019](#bib.bib74)）。皮肤图像分析的研究可以追溯到1980年代（Vanker和Van
    Stoecker，[1984](#bib.bib391)；Dhawan等，[1984](#bib.bib122)），但直到2000年代中期，这些研究才使用包含几百张图像的小型私有数据集。
- en: 'The *Interactive Atlas of Dermoscopy* (sometimes called the *Edra Atlas*, in
    reference to the publisher) by Argenziano et al. ([2000](#bib.bib25)) included
    a CD-ROM with $1,039$ dermoscopy images ($26\%$ melanomas, $4\%$ carcinomas, $70\%$
    nevi) of $1,024\times 683$ pixels, acquired by three European university hospitals
    (University of Graz, Austria, University of Naples, Italy, and University of Florence,
    Italy). The works of Celebi et al. ([2007b](#bib.bib78), [2008](#bib.bib77)) popularized
    the dataset in the dermoscopy image analysis community, where it became a de facto
    evaluation standard for almost a decade, until the much larger ISIC Archive datasets
    (see below) became available. Recently, Kawahara et al. ([2019](#bib.bib219))
    placed this valuable dataset, along with additional textual annotations based
    on the 7-point checklist, in public domain under the name *derm7pt*. Shortly after
    the publication of the Interactive Atlas of Dermoscopy, Menzies et al. ([2003](#bib.bib276))
    published *An Atlas of Surface Microscopy of Pigmented Skin Lesions: Dermoscopy*,
    with a CD-ROM containing $217$ dermoscopic images ($39\%$ melanomas, $7\%$ carcinomas,
    $54\%$ nevi) of $712\times 454$ pixels, acquired at the Sydney Melanoma Unit,
    Australia.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*皮肤镜互动图谱*（有时称为*Edra图谱*，以出版商命名）由Argenziano等（[2000](#bib.bib25)）出版，包含一个CD-ROM，内有$1,039$张皮肤镜图像（$26\%$黑色素瘤，$4\%$癌症，$70\%$痣），分辨率为$1,024\times
    683$像素，由三个欧洲大学医院（奥地利格拉茨大学、意大利那不勒斯大学和意大利佛罗伦萨大学）采集。Celebi等（[2007b](#bib.bib78)，[2008](#bib.bib77)）的研究在皮肤镜图像分析社区推广了该数据集，成为近十年来的事实标准，直到更大的ISIC档案数据集（见下文）可用。最近，Kawahara等（[2019](#bib.bib219)）将这一宝贵数据集以及基于7点检查表的附加文本注释，公开发布为*derm7pt*。在皮肤镜互动图谱出版后不久，Menzies等（[2003](#bib.bib276)）出版了*色素皮肤病变表面显微镜图谱：皮肤镜*，其中的CD-ROM包含$217$张皮肤镜图像（$39\%$黑色素瘤，$7\%$癌症，$54\%$痣），分辨率为$712\times
    454$像素，由澳大利亚悉尼黑色素瘤单位采集。'
- en: The $\mathsf{PH}^{2}$ dataset, released by Mendonca et al. ([2013](#bib.bib274))
    and detailed by Mendonca et al. ([2015](#bib.bib275)), was the first public dataset
    to provide region-based annotations with segmentation masks, and masks for the
    clinically significant colors (white, red, light brown, dark brown, blue-gray,
    and black) present in the images. The dataset contains $200$ dermoscopic images
    ($20\%$ melanomas, $40\%$ atypical nevi, and $40\%$ common nevi) of $768\times
    560$ pixels, acquired at the Hospital Pedro Hispano, Portugal. The Edinburgh DermoFit
    Image Library (Ballerini et al., [2013](#bib.bib37)) also provides region-based
    annotations for $1,300$ clinical images (10 diagnostic categories including melanomas,
    seborrhoeic keratosis, and basal cell carcinoma) of sizes ranging from $177\times
    189$ to $2,176\times 2,549$ pixels. The images were acquired with a Canon EOS
    350D SLR camera, in controlled lighting and at a consistent distance from the
    lesions, resulting in a level of quality atypical for clinical images.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathsf{PH}^{2}$ 数据集由 Mendonca 等人发布（[2013](#bib.bib274)），并由 Mendonca 等人详细描述（[2015](#bib.bib275)）。这是第一个提供基于区域的注释以及图像中临床显著颜色（白色、红色、浅棕色、深棕色、蓝灰色和黑色）分割掩模的公开数据集。该数据集包含
    $200$ 张皮肤镜图像（$20\%$ 为黑色素瘤，$40\%$ 为非典型痣，$40\%$ 为普通痣），分辨率为 $768\times 560$ 像素，图像在葡萄牙
    Pedro Hispano 医院采集。爱丁堡 DermoFit 图像库（Ballerini 等人，[2013](#bib.bib37)）还提供了 $1,300$
    张临床图像的基于区域的注释（包括黑色素瘤、脂溢性角化病和基底细胞癌在内的 $10$ 种诊断类别），图像尺寸从 $177\times 189$ 到 $2,176\times
    2,549$ 像素不等。图像使用 Canon EOS 350D 单反相机在受控光照条件下并与病变保持一致的距离进行采集，因此图像质量在临床图像中较为罕见。
- en: The ISIC Archive contains the world’s largest curated repository of dermoscopic
    images. ISIC, an international academia-industry partnership sponsored by ISDIS
    (International Society for Digital Imaging of the Skin), aims to “facilitate the
    application of digital skin imaging to help reduce melanoma mortality” (ISIC,
    [2023](#bib.bib186)). At the time of writing, the archive contains more than $240,000$
    images, of which more than $71,000$ are publicly available. These images were
    acquired in leading worldwide clinical centers, using a variety of devices.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ISIC 存档包含世界上最大规模的皮肤镜图像策展库。ISIC 是一个由 ISDIS（国际皮肤数字成像学会）赞助的国际学术界和工业界合作伙伴关系，旨在“促进数字皮肤成像的应用，以帮助减少黑色素瘤的死亡率”（ISIC，[2023](#bib.bib186)）。截至撰写时，档案馆包含超过
    $240,000$ 张图像，其中超过 $71,000$ 张是公开可用的。这些图像是在全球领先的临床中心采集的，使用了各种设备。
- en: In addition to curating the datasets that collectively form the “ISIC Archive”,
    ISIC has released standard archive subsets as part of its *Skin Lesion Analysis
    Towards Melanoma Detection* Challenge, organized annually since 2016\. The 2016,
    2017, and 2018 challenges comprised segmentation, feature extraction, and classification
    tasks, while the 2019 and 2020 challenges featured only classification. Each subset
    is associated with a challenge (year), one or more tasks, and has two (training/test)
    or three (training/validation/test) splits. The ISIC Challenge 2016 (Gutman et al.,
    [2016](#bib.bib167)) (ISIC 2016, for brevity) contains $1,279$ images split into
    $900$ for training ($19\%$ melanomas, $81\%$ nevi), and $379$ for testing ($20\%$
    melanomas, $80\%$ nevi). There is a large variation in image size, ranging from
    $0.5$ to $12$ megapixels. All tasks used the same images. The ISIC 2017 (Codella
    et al., [2018](#bib.bib98)) dataset more than doubled, with $2,750$ images split
    into $2,000$ for training ($18.7\%$ melanomas, $12.7\%$ seborrheic keratoses,
    $68.6\%$ nevi), $150$ for validation ($20\%$ melanomas, $28\%$ seborrheic keratoses,
    $52\%$ nevi), and $600$ for testing ($19.5\%$ melanomas, $15\%$ seborrheic keratoses,
    $65.5\%$ nevi). Again, image size varied markedly, ranging from $0.5$ to $29$
    megapixels, and all tasks used the same images.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了策划共同形成“ISIC 存档”的数据集外，ISIC 还作为其 *皮肤病变分析以期实现黑色素瘤检测* 挑战的一部分发布了标准档案子集，该挑战自 2016
    年起每年组织一次。2016 年、2017 年和 2018 年的挑战包括分割、特征提取和分类任务，而 2019 年和 2020 年的挑战仅包含分类任务。每个子集与一个挑战（年份）、一个或多个任务相关，并有两个（训练/测试）或三个（训练/验证/测试）拆分。ISIC
    Challenge 2016（Gutman 等人，[2016](#bib.bib167)）（简称 ISIC 2016）包含 $1,279$ 张图像，分为 $900$
    张训练图像（$19\%$ 为黑色素瘤，$81\%$ 为痣）和 $379$ 张测试图像（$20\%$ 为黑色素瘤，$80\%$ 为痣）。图像尺寸差异很大，从
    $0.5$ 到 $12$ 兆像素不等。所有任务使用了相同的图像。ISIC 2017（Codella 等人，[2018](#bib.bib98)）数据集的规模增加了两倍多，包含
    $2,750$ 张图像，分为 $2,000$ 张训练图像（$18.7\%$ 为黑色素瘤，$12.7\%$ 为脂溢性角化病，$68.6\%$ 为痣）、$150$
    张验证图像（$20\%$ 为黑色素瘤，$28\%$ 为脂溢性角化病，$52\%$ 为痣）和 $600$ 张测试图像（$19.5\%$ 为黑色素瘤，$15\%$
    为脂溢性角化病，$65.5\%$ 为痣）。同样，图像尺寸差异显著，从 $0.5$ 到 $29$ 兆像素不等，所有任务使用了相同的图像。
- en: 'ISIC 2018 provided, for the first time, separate datasets for the tasks, with
    $2,594$ training ($20$% melanomas, $72$% nevi, and $8$% seborrheic keratoses)
    and $100$/$1,000$ for validation/test images ranging from $0.5$ to $29$ megapixels,
    for the tasks of segmentation and feature extraction (Codella et al., [2019](#bib.bib96)),
    and $10,015$/$1,512$ training/test images for the classification task, all with
    $600\times 450$ pixels. The training dataset for classification was the HAM10000
    dataset (Tschandl et al., [2018](#bib.bib381)), acquired over a period of $20$
    years at the Medical University of Vienna, Austria and the private practice of
    Dr. Cliff Rosendahl, Australia. It allowed a five-fold increase in training images
    in comparison to 2017 and comprised seven diagnostic categories: melanoma ($11.1\%$),
    nevus ($66.9\%$), basal cell carcinoma ($5.1\%$), actinic keratosis or Bowen’s
    disease ($3.3\%$), benign keratosis (solar lentigo, seborrheic keratosis, or lichen
    planus-like keratosis, $11\%$), dermatofibroma ($1.1\%$), and vascular lesion
    ($1.4\%$). As a part of a 2020 study of human-computer collaboration for skin
    lesion diagnosis involving dermatologists and general practitioners (Tschandl
    et al., [2020](#bib.bib380)), the lesions in the HAM10000 dataset were segmented
    by a single dermatologist and consequently released publicly (ViDIR Dataverse,
    [2020](#bib.bib396)), making this the single largest publicly available skin lesion
    segmentation dataset (Table [1](#S2.T1 "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ISIC 2018首次提供了任务的单独数据集，包括$2,594$张训练图像（$20$%为黑色素瘤，$72$%为痣，$8$%为脂溢性角化病），以及$100$/$1,000$张用于验证/测试的图像，图像尺寸从$0.5$到$29$兆像素，适用于分割和特征提取任务（Codella等，*[2019](#bib.bib96)*），以及$10,015$/$1,512$张训练/测试图像用于分类任务，所有图像分辨率为$600\times
    450$像素。分类任务的训练数据集为HAM10000数据集（Tschandl等，*[2018](#bib.bib381)*），该数据集在奥地利维也纳医科大学和澳大利亚Cliff
    Rosendahl博士的私人诊所中收集，历时$20$年。这使得训练图像数量相比2017年增长了五倍，包含七个诊断类别：黑色素瘤（$11.1\%$），痣（$66.9\%$），基底细胞癌（$5.1\%$），光化性角化病或Bowen病（$3.3\%$），良性角化病（太阳雀斑、脂溢性角化病或类扁平苔藓角化病，$11\%$），皮肤纤维瘤（$1.1\%$），血管病变（$1.4\%$）。作为2020年一项关于皮肤病变诊断的人机协作研究的一部分，该研究涉及皮肤科医生和全科医生（Tschandl等，*[2020](#bib.bib380)*），HAM10000数据集中的病变由一名皮肤科医生进行分割，随后公开发布（ViDIR
    Dataverse，*[2020](#bib.bib396)*），使其成为最大型的公开皮肤病变分割数据集（表*[1](#S2.T1 "Table 1 ‣ 2.1
    Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")*）。
- en: 'Table 1: Public skin lesion datasets with lesion segmentation annotations.
    All the datasets contain RGB images of skin lesions.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：具有病变分割注释的公共皮肤病变数据集。所有数据集包含皮肤病变的RGB图像。
- en: '| dataset | year | modality | size | training/validation/test | class distribution
    | additional info |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 模式 | 大小 | 训练/验证/测试 | 类别分布 | 额外信息 |'
- en: '| DermQuest⁷⁷7DermQuest was deactivated on December 31, 2019\. However, 137
    of its images are publicly available (Glaister, [2013](#bib.bib148)). (DermQuest,
    [2012](#bib.bib120)) | 2012 | clinical | $137$ | – | 61 non-melanomas 76 melanomas
    | acquired with different cameras under various lighting conditions |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| DermQuest⁷⁷7DermQuest于2019年12月31日停用。然而，137张图像仍可公开获取（Glaister，*[2013](#bib.bib148)*）。
    （DermQuest，*[2012](#bib.bib120)*） | 2012 | 临床 | $137$ | – | 61个非黑色素瘤，76个黑色素瘤 |
    通过不同相机在各种光照条件下获取 |'
- en: '| DermoFit (Ballerini et al., [2013](#bib.bib37)) | 2013 | clinical | $1,300$
    | – | $1,224$ non-melanomas 76 melanomas | sizes ranging from $177\times 189$
    to $2,176\times 2,549$ pixels |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| DermoFit（Ballerini等，*[2013](#bib.bib37)*） | 2013 | 临床 | $1,300$ | – | $1,224$个非黑色素瘤，76个黑色素瘤
    | 尺寸范围从$177\times 189$到$2,176\times 2,549$像素 |'
- en: '| Pedro Hispano Hospital (PH²)  (Mendonca et al., [2013](#bib.bib274)) | 2013
    | dermoscopy | $200$ | – | 160 benign nevi 40 melanomas | sizes ranging from $553\times
    763$ to $577\times 769$ pixels acquired at $20\times$ magnification |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Pedro Hispano医院（PH²）（Mendonca等，*[2013](#bib.bib274)*） | 2013 | 镜下检查 | $200$
    | – | 160个良性痣，40个黑色素瘤 | 尺寸范围从$553\times 763$到$577\times 769$像素，在$20\times$放大倍数下获取
    |'
- en: '| ISIC2016 (Gutman et al., [2016](#bib.bib167)) | 2016 | dermoscopy | $1,279$
    | $900$/–/$379$ | Training: 727 non-melanomas 173 melanomas Test: 304 non-melanomas
    75 melanomas | sizes ranging from $566\times 679$ to $2,848\times 4,288$ pixels
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ISIC2016（Gutman等，*[2016](#bib.bib167)*） | 2016 | 镜下检查 | $1,279$ | $900$/–/$379$
    | 训练集：727个非黑色素瘤，173个黑色素瘤 测试集：304个非黑色素瘤，75个黑色素瘤 | 尺寸范围从$566\times 679$到$2,848\times
    4,288$像素 |'
- en: '| ISIC2017 (Codella et al., [2018](#bib.bib98)) | 2017 | dermoscopy | $2,750$
    | $2,000$/$150$/$600$ | Training: $1,626$ non-melanomas 374 melanomas Test: 483
    non-melanomas 117 melanomas | sizes ranging from $540\times 722$ to $4,499\times
    6,748$ pixels |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ISIC2017 (Codella et al., [2018](#bib.bib98)) | 2017 | 皮肤镜检查 | $2,750$ |
    $2,000$/$150$/$600$ | 训练集：$1,626$ 个非黑色素瘤，374 个黑色素瘤 测试集：483 个非黑色素瘤，117 个黑色素瘤 |
    图片尺寸从 $540\times 722$ 到 $4,499\times 6,748$ 像素 |'
- en: '| ISIC2018 (Codella et al., [2019](#bib.bib96)) | 2018 | dermoscopy | $3,694$
    | $2,594$/$100$/$1,000$ | – | sizes ranging from $540\times 576$ to $4,499\times
    6,748$ pixels |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ISIC2018 (Codella et al., [2019](#bib.bib96)) | 2018 | 皮肤镜检查 | $3,694$ |
    $2,594$/$100$/$1,000$ | – | 图片尺寸从 $540\times 576$ 到 $4,499\times 6,748$ 像素 |'
- en: '| HAM10000 (Tschandl et al., [2018](#bib.bib381)) (Tschandl et al., [2020](#bib.bib380))
    (ViDIR Dataverse, [2020](#bib.bib396)) | 2020 | dermoscopy | $10,015$ | – | $1,113$
    non-melanomas $8,902$ melanomas | all images of $600\times 450$ pixels |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| HAM10000 (Tschandl et al., [2018](#bib.bib381)) (Tschandl et al., [2020](#bib.bib380))
    (ViDIR Dataverse, [2020](#bib.bib396)) | 2020 | 皮肤镜检查 | $10,015$ | – | $1,113$
    个非黑色素瘤，$8,902$ 个黑色素瘤 | 所有图像尺寸为 $600\times 450$ 像素 |'
- en: ISIC 2019 (Codella et al., [2018](#bib.bib98); Tschandl et al., [2018](#bib.bib381);
    Combalia et al., [2019](#bib.bib101)) contains $25,331$ training images ($18\%$
    melanomas, $51\%$ nevi, $13\%$ basal cell carcinomas, $3.5\%$ actinic keratoses,
    $10\%$ benign keratoses, $1\%$ dermatofibromas, $1\%$ vascular lesions, and $2.5\%$
    squamous cell carcinomas) and $8,238$ test images (diagnostic distribution unknown).
    The images range from $600\times 450$ to $1,024\times 1,024$ pixels.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ISIC 2019 (Codella et al., [2018](#bib.bib98); Tschandl et al., [2018](#bib.bib381);
    Combalia et al., [2019](#bib.bib101)) 包含 $25,331$ 张训练图像（$18\%$ 黑色素瘤，$51\%$ 痣，$13\%$
    基底细胞癌，$3.5\%$ 日光角化病，$10\%$ 良性角化病，$1\%$ 真皮纤维瘤，$1\%$ 血管病变，以及 $2.5\%$ 鳞状细胞癌）和 $8,238$
    张测试图像（诊断分布未知）。图像尺寸范围从 $600\times 450$ 到 $1,024\times 1,024$ 像素。
- en: ISIC 2020 (Rotemberg et al., [2021](#bib.bib330)) contains $33,126$ training
    images ($1.8\%$ melanomas, $97.6\%$ nevi, $0.4\%$ seborrheic keratoses, $0.1\%$
    lentigines simplex, $0.1\%$ lichenoid keratoses, $0.02\%$ solar lentigines, $0.003\%$
    cafe-au-lait macules, $0.003\%$ atypical melanocytic proliferations) and $10,982$
    test images (diagnostic distribution unknown), ranging from 0.5 to 24 megapixels.
    Multiple centers, distributed worldwide, contributed to the dataset, including
    the Memorial Sloan Kettering Cancer Center (USA), the Melanoma Institute, the
    Sydney Melanoma Diagnostic Centre, and the University of Queensland (Australia),
    the Medical University of Vienna (Austria), the University of Athens (Greece),
    and the Hospital Clinic Barcelona (Spain). An important novelty in this dataset
    is the presence of multiple lesions per patient, with the express motivation of
    exploiting intra- and inter-patient lesion patterns, e.g., the so-called “ugly-ducklings”,
    lesions whose appearances are atypical for a given patient, and which present
    an increased risk of malignancy (Gachon et al., [2005](#bib.bib140)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ISIC 2020 (Rotemberg et al., [2021](#bib.bib330)) 包含 $33,126$ 张训练图像（$1.8\%$
    黑色素瘤，$97.6\%$ 痣，$0.4\%$ 脂溢性角化病，$0.1\%$ 单纯性雀斑，$0.1\%$ 类似角化病，$0.02\%$ 日光雀斑，$0.003\%$
    咖啡奶块，$0.003\%$ 异常黑色素细胞增生）和 $10,982$ 张测试图像（诊断分布未知），图像尺寸范围从 0.5 到 24 兆像素。多个全球中心贡献了这个数据集，包括美国纪念斯隆-凯特琳癌症中心、黑色素瘤研究所、悉尼黑色素瘤诊断中心、昆士兰大学（澳大利亚）、维也纳医科大学（奥地利）、雅典大学（希腊）和巴塞罗那诊所医院（西班牙）。这个数据集的一个重要新特点是每位患者有多个病变，明确目的是利用患者间和患者内的病变模式，例如所谓的“丑小鸭”病变，这些病变在特定患者中外观异常，并且有较高的恶性风险 (Gachon
    et al., [2005](#bib.bib140))。
- en: There is, however, an overlap among these ISIC Challenge datasets. Abhishek
    ([2020](#bib.bib4)) analyzed all the lesion segmentation datasets from the ISIC
    Challenges (2016-2018) and found considerable overlap between these 3 datasets,
    with as many as $1,940$ images shared between at least 2 datasets and $706$ images
    shared between all 3 datasets. In a more recent analysis of the ISIC Challenge
    datasets for the lesion diagnosis task from 2016 through 2020, Cassidy et al.
    ([2022](#bib.bib72)) found overlap between the datasets as well as the presence
    of duplicates within the datasets. Using a duplicate removal strategy, they curated
    a new set of $45,590$ training images ($8.61\%$ melanomas, $91.39\%$ others) and
    $11,397$ validation images ($8.61\%$ melanomas, $91.39\%$ others), leading to
    a total of $56,987$ images. Additionally, since the resulting dataset is highly
    imbalanced (melanomas versus others in a ratio of $1:10.62$), the authors also
    curated a balanced dataset with $7,848$ training images (50% melanoma, 50% others)
    and $1,962$ validation images (50% melanoma, 50% others).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些ISIC挑战数据集之间存在重叠。Abhishek（[2020](#bib.bib4)）分析了所有来自ISIC挑战（2016-2018）的病变分割数据集，发现这3个数据集之间有相当大的重叠，至少$1,940$张图片在2个数据集之间共享，$706$张图片在所有3个数据集之间共享。在对2016年至2020年的ISIC挑战数据集进行的最近分析中，Cassidy等人（[2022](#bib.bib72)）发现了数据集之间的重叠以及数据集内的重复项。通过使用重复项移除策略，他们整理出一个新的数据集，其中包括$45,590$张训练图片（$8.61\%$黑色素瘤，$91.39\%$其他）和$11,397$张验证图片（$8.61\%$黑色素瘤，$91.39\%$其他），总共$56,987$张图片。此外，由于结果数据集极度不平衡（黑色素瘤与其他的比例为$1:10.62$），作者还整理了一个平衡数据集，其中包括$7,848$张训练图片（50%黑色素瘤，50%其他）和$1,962$张验证图片（50%黑色素瘤，50%其他）。
- en: '![Refer to caption](img/1c13819ddd5fa85a9764e0abcd8f4541.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1c13819ddd5fa85a9764e0abcd8f4541.png)'
- en: 'Figure 3: The frequency of utilization of different skin lesion segmentation
    datasets in the surveyed studies. We found that $82$ papers evaluated on more
    than $1$ dataset, with $36$ papers opting for cross-dataset evaluation (CDE in
    Table LABEL:tab:main). ISIC datasets (ISIC 2016, ISIC 2017, ISIC 2018, and ISIC
    Archive) are used in the majority of papers, with $168$ of $177$ papers using
    at least one ISIC dataset and the ISIC 2017 dataset being the most popular ($117$
    papers). The PH² dataset is the second most widely used ($56$ papers) following
    ISIC datasets.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：调查研究中不同皮肤病变分割数据集的使用频率。我们发现有$82$篇论文在超过$1$个数据集上进行评估，其中$36$篇论文选择了跨数据集评估（见表LABEL:tab:main）。ISIC数据集（ISIC
    2016、ISIC 2017、ISIC 2018和ISIC档案）在大多数论文中被使用，其中$177$篇论文中的$168$篇使用了至少一个ISIC数据集，而ISIC
    2017数据集是最受欢迎的（$117$篇论文）。PH²数据集是使用频率第二高的数据集（$56$篇论文），仅次于ISIC数据集。
- en: Table [1](#S2.T1 "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation") shows a list of publicly available skin lesion
    datasets with pixel-wise annotations, image modality, sample size, original split
    sizes, and diagnostic distribution. Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Datasets ‣
    2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation") shows
    how frequently these datasets appear in the literature. It is also worth noting
    that several other skin lesion image datasets have not been described in our survey
    as they do not provide the corresponding skin lesion segmentation annotations.
    However, these datasets, including SD-198 (Sun et al., [2016](#bib.bib364)), MED-NODE (Giotis
    et al., [2015](#bib.bib146)), derm7pt (Kawahara et al., [2019](#bib.bib219)),
    Interactive Dermatology Atlas (Usatine and Madden, [2013](#bib.bib386)), Dermatology
    Information System (DermIS, [2012](#bib.bib119)), DermWeb (Lui et al., [2009](#bib.bib265)),
    DermNet New Zealand (Oakley et al., [1995](#bib.bib291)), may still be relevant
    for skin lesion segmentation research (see Section [5](#S5 "5 Discussion and Future
    Research ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S2.T1 "表 1 ‣ 2.1 数据集 ‣ 2 输入数据 ‣ 深度学习在皮肤病变分割中的应用调查")显示了具有像素级注释的公开可用皮肤病变数据集的列表，包括图像模态、样本大小、原始分割大小和诊断分布。图[3](#S2.F3
    "图 3 ‣ 2.1 数据集 ‣ 2 输入数据 ‣ 深度学习在皮肤病变分割中的应用调查")显示了这些数据集在文献中的出现频率。值得注意的是，还有一些皮肤病变图像数据集没有在我们的调查中描述，因为它们没有提供相应的皮肤病变分割注释。然而，这些数据集，包括SD-198（Sun等，[2016](#bib.bib364)）、MED-NODE（Giotis等，[2015](#bib.bib146)）、derm7pt（Kawahara等，[2019](#bib.bib219)）、互动皮肤病学图谱（Usatine和Madden，[2013](#bib.bib386)）、皮肤病学信息系统（DermIS，[2012](#bib.bib119)）、DermWeb（Lui等，[2009](#bib.bib265)）、DermNet新西兰（Oakley等，[1995](#bib.bib291)），可能仍对皮肤病变分割研究具有参考价值（见第[5](#S5
    "5 讨论与未来研究 ‣ 深度学习在皮肤病变分割中的应用调查")节）。
- en: Biases in computer vision datasets are a constant source of issues (Torralba
    and Efros, [2011](#bib.bib377)), which is compounded in medical imaging due to
    the smaller number of samples, insufficient image resolution, lack of geographical
    or ethnic diversity, or statistics unrepresentative of clinical practice. All
    existing skin lesion datasets suffer to a certain extent from one or more of the
    aforementioned issues, to which we add the specific issue of the availability
    and reliability of annotations. For lesion classification, many samples lack the
    gold standard histopathological confirmation, and ground-truth segmentation, even
    when available, is inherently noisy (Section [4.2](#S4.SS2 "4.2 Inter-Annotator
    Agreement ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).
    The presence of artifacts (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")) may lead to spurious correlations,
    an issue that Bissoto et al. ([2019](#bib.bib59)) attempted to quantify for classification
    models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉数据集中的偏差是一个持续存在的问题 (Torralba and Efros, [2011](#bib.bib377))，在医学影像中尤为严重，原因包括样本数量较少、图像分辨率不足、缺乏地理或种族多样性，或统计数据与临床实践不符。所有现有的皮肤病变数据集在一定程度上都存在上述问题，我们还增加了注释的可用性和可靠性这一具体问题。对于病变分类，许多样本缺乏黄金标准的组织病理学确认，即使有的地面真实分割也本质上是嘈杂的
    (第 [4.2](#S4.SS2 "4.2 Inter-Annotator Agreement ‣ 4 Evaluation ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation") 节)。伪影的存在 (图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")) 可能导致虚假的相关性，这是 Bissoto
    等人 ([2019](#bib.bib59)) 尝试对分类模型进行量化的问题。
- en: 2.2 Synthetic Data Generation
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 合成数据生成
- en: Data augmentation—synthesizing new samples from existing ones—is commonly employed
    in the training of DL models. Augmented data serve as a regularizer, increase
    the amount and diversity of data (Shorten and Khoshgoftaar, [2019](#bib.bib350)),
    induce desirable invariances on the model, and alleviate class imbalance. Traditional
    data augmentation applies simple geometric, photometric, and colorimetric transformations
    on the samples, including mirroring, translation, scaling, rotation, cropping,
    random region erasing, affine or elastic deformation, modifications of hue, saturation,
    brightness, and contrast. Usually, several transformations are chosen at random
    and combined. Fig. [4](#S2.F4 "Figure 4 ‣ 2.2 Synthetic Data Generation ‣ 2 Input
    Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation") exemplifies the
    procedure, as applied to a dermoscopic image with Albumentations (Buslaev et al.,
    [2020](#bib.bib68)), a state-of-the-art open-source library for image augmentation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强——从现有样本中合成新样本——在深度学习模型训练中常被使用。增强数据作为正则化器，增加数据的数量和多样性 (Shorten and Khoshgoftaar,
    [2019](#bib.bib350))，诱导模型上的期望不变性，并缓解类别不平衡。传统的数据增强对样本应用简单的几何、光度和色彩变换，包括镜像、平移、缩放、旋转、裁剪、随机区域擦除、仿射或弹性变形、色调、饱和度、亮度和对比度的修改。通常，会随机选择几种变换并组合。图
    [4](#S2.F4 "Figure 4 ‣ 2.2 Synthetic Data Generation ‣ 2 Input Data ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation") 展示了这一过程，应用于使用 Albumentations (Buslaev
    et al., [2020](#bib.bib68)) 的皮肤镜图像，Albumentations 是一种先进的开源图像增强库。
- en: 'As mentioned earlier, augmented training data induce invariance on the models:
    random translations and croppings, for example, help induce a translation-invariant
    model. This has implications for skin lesion analysis, e.g., data augmentation
    for generic datasets (such as ImageNet (Deng et al., [2009](#bib.bib113))) forgo
    vertical mirroring and large-angle rotations, because natural scenes have a strong
    vertical anisotropy, while skin lesion images are isotropic. In addition, augmented
    test data (test-time augmentation) may also improve generalization by combining
    the predictions of several augmented samples through, for example, average pooling
    or majority voting (Shorten and Khoshgoftaar, [2019](#bib.bib350)). Perez et al.
    ([2018](#bib.bib305)) have systematically evaluated the effect of several data
    augmentation schemes for skin lesion classification, finding that the use of both
    training and test augmentation is critical for performance, surpassing, in some
    cases, increases of real data without augmentation. Valle et al. ([2020](#bib.bib388))
    found, in a very large-scale experiment, that test-time augmentation was the second
    most influential factor for classification performance, after training set size.
    No systematic study of this kind exists for skin lesion segmentation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，增强训练数据可以使模型具有不变性：例如，随机平移和裁剪有助于生成平移不变的模型。这对皮肤病变分析有重要影响，例如，通用数据集（如ImageNet (Deng
    et al., [2009](#bib.bib113)))的增强不会包括垂直镜像和大角度旋转，因为自然场景具有强垂直各向异性，而皮肤病变图像是各向同性的。此外，增强测试数据（测试时增强）也可以通过例如平均池化或多数投票 (Shorten
    and Khoshgoftaar, [2019](#bib.bib350))，提高泛化能力。Perez et al. ([2018](#bib.bib305))系统地评估了多种数据增强方案对皮肤病变分类的影响，发现训练和测试增强的结合对性能至关重要，有时甚至超越了没有增强的真实数据增加。Valle
    et al. ([2020](#bib.bib388))在一个大规模实验中发现，测试时增强是分类性能的第二大影响因素，仅次于训练集大小。对于皮肤病变分割尚无此类系统性研究。
- en: '![Refer to caption](img/ddbe6db194a16e3c6574ed08f9798f7c.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ddbe6db194a16e3c6574ed08f9798f7c.png)'
- en: (a) Original
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始
- en: '![Refer to caption](img/3468b4eb275c14f115276b88b469e344.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3468b4eb275c14f115276b88b469e344.png)'
- en: (b) Affine deformation
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 仿射变形
- en: '![Refer to caption](img/b05e7b6776ac119ad5f206985e4bbdc9.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b05e7b6776ac119ad5f206985e4bbdc9.png)'
- en: (c) Elastic deformation
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 弹性变形
- en: '![Refer to caption](img/fd4db66520337dbeb5a994ee0911a523.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fd4db66520337dbeb5a994ee0911a523.png)'
- en: (d) Histogram equalization
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 直方图均衡化
- en: '![Refer to caption](img/b7f3f7a06fdd99c67e25683612d9f4fd.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b7f3f7a06fdd99c67e25683612d9f4fd.png)'
- en: (e) HSV shift
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (e) HSV 变换
- en: '![Refer to caption](img/06b3a6c614d7cb82a690acd68847f080.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/06b3a6c614d7cb82a690acd68847f080.png)'
- en: (f) RGB shift
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: (f) RGB 变换
- en: 'Figure 4: Various data augmentation transformations applied to a dermoscopic
    image (image source: ISIC 2016 dataset (Gutman et al., [2016](#bib.bib167))) using
    the Albumentations library (Buslaev et al., [2020](#bib.bib68)).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：应用于皮肤镜图像的各种数据增强变换（图像来源：ISIC 2016 数据集 (Gutman et al., [2016](#bib.bib167)))，使用
    Albumentations 库 (Buslaev et al., [2020](#bib.bib68))。
- en: Although traditional data augmentation is crucial for training DL models, it
    falls short of providing samples that are both diverse and plausible from the
    same distribution as real data. Thus, modern data augmentation (Tajbakhsh et al.,
    [2020](#bib.bib367)) employs generative modeling, learning the probability distribution
    of the real data, and sampling from that distribution. Generative adversarial
    networks (GANs) (Goodfellow et al., [2020](#bib.bib152)) are the most promising
    approach in this direction (Shorten and Khoshgoftaar, [2019](#bib.bib350)), especially
    for medical image analysis (Yi et al., [2019](#bib.bib428); Kazeminia et al.,
    [2020](#bib.bib222); Shamsolmoali et al., [2021](#bib.bib347)). GANs employ an
    adversarial training between a generator, which attempts to generate realistic
    fake samples, and a discriminator, which attempts to differentiate real samples
    from fake ones. When the procedure converges, the generator output is surprisingly
    convincing, but GANs are computationally expensive and difficult to train (Creswell
    et al., [2018](#bib.bib103)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管传统的数据增强对于训练深度学习模型至关重要，但它不足以提供既多样化又可信的样本，这些样本来自于与真实数据相同的分布。因此，现代数据增强（Tajbakhsh
    等，[2020](#bib.bib367)）采用了生成建模，学习真实数据的概率分布，并从该分布中进行采样。生成对抗网络（GANs）（Goodfellow 等，[2020](#bib.bib152)）是这个方向上最有前景的方法（Shorten
    和 Khoshgoftaar，[2019](#bib.bib350)），尤其在医学图像分析中（Yi 等，[2019](#bib.bib428)；Kazeminia
    等，[2020](#bib.bib222)；Shamsolmoali 等，[2021](#bib.bib347)）。GANs 通过生成器和鉴别器之间的对抗训练来工作，生成器试图生成逼真的假样本，而鉴别器试图区分真实样本和假样本。当过程收敛时，生成器的输出令人惊讶地令人信服，但
    GANs 计算开销大且训练困难（Creswell 等，[2018](#bib.bib103)）。
- en: Synthetic generation of skin lesions has received some recent interest, especially
    in the context of improving classification. Works can be roughly divided into
    those that use GANs to create new images from a Gaussian latent variable (Baur
    et al., [2018](#bib.bib43); Pollastri et al., [2020](#bib.bib309); Abdelhalim
    et al., [2021](#bib.bib3)), and those that implement GANs based on image-to-image
    translation (Abhishek and Hamarneh, [2019](#bib.bib5); Bissoto et al., [2018](#bib.bib60);
    Ding et al., [2021](#bib.bib124)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 合成皮肤病变的生成最近受到了一些关注，特别是在提高分类准确性方面。相关工作大致可以分为使用 GANs 从高斯潜变量创建新图像的研究（Baur 等，[2018](#bib.bib43)；Pollastri
    等，[2020](#bib.bib309)；Abdelhalim 等，[2021](#bib.bib3)），以及基于图像到图像转换的 GANs 实现（Abhishek
    和 Hamarneh，[2019](#bib.bib5)；Bissoto 等，[2018](#bib.bib60)；Ding 等，[2021](#bib.bib124)）。
- en: Noise-based GANs, such as DCGAN (Yu et al., [2017b](#bib.bib432)), LAPGAN (Denton
    et al., [2015](#bib.bib116)), and PGAN (Karras et al., [2018](#bib.bib211)), learn
    to decode a Gaussian latent variable into an image that belongs to the training
    set distribution. The main advantage of these techniques is the ability to create
    more, and more diverse images, as, in principle, any sample from a multivariate
    Gaussian distribution may become a different image. The disadvantage is that the
    images tend to be of lower quality, and, in the case of segmentation, one needs
    to generate plausible pairs of images and segmentation masks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基于噪声的 GANs，如 DCGAN（Yu 等，[2017b](#bib.bib432)），LAPGAN（Denton 等，[2015](#bib.bib116)）和
    PGAN（Karras 等，[2018](#bib.bib211)），学习将高斯潜变量解码为属于训练集分布的图像。这些技术的主要优势是能够创建更多、更具多样性的图像，因为原则上，来自多变量高斯分布的任何样本都可能成为不同的图像。缺点是图像质量通常较低，在分割情况下，需要生成可信的图像和分割掩膜对。
- en: Image-to-image translation GANs, such as pix2pix (Isola et al., [2017](#bib.bib187))
    and pix2pixHD (Wang et al., [2018](#bib.bib403)), learn to create new samples
    from a semantic segmentation map. They have complementary advantages and disadvantages.
    Because the procedure is deterministic (one map creates one image), they have
    much less freedom in the number of samples available, but the images tend to be
    of higher quality (or more “plausible”). There is no need to generate separate
    segmentation maps because the generated image is intrinsically compatible with
    the input segmentation map.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到图像转换 GANs，如 pix2pix（Isola 等，[2017](#bib.bib187)）和 pix2pixHD（Wang 等，[2018](#bib.bib403)），学习从语义分割图中创建新样本。它们具有互补的优缺点。由于过程是确定性的（一个图创建一个图像），它们在样本数量上自由度较小，但图像质量通常较高（或更“可信”）。由于生成的图像与输入的分割图本质上兼容，因此无需生成单独的分割图。
- en: The two seminal papers on GANs for skin lesions (Baur et al., [2018](#bib.bib43);
    Bissoto et al., [2018](#bib.bib60)) evaluate several models. Baur et al. ([2018](#bib.bib43))
    compare the noise-based DCGAN, LAPGAN, and PGAN for the generation of $256\times
    256$-pixel images using both qualitative and quantitative criteria, finding that
    the PGAN gives considerably better results. They further examine the PGAN against
    a panel of human judges, composed by dermatologists and DL experts, in a “visual
    Turing test”, showing that both had difficulties in distinguishing the fake images
    from the true ones. Bissoto et al. ([2018](#bib.bib60)) adapt the PGAN to be class-conditioned
    on diagnostic category, and the image-to-image pix2pixHD to employ the semantic
    annotation provided by the feature extraction task of the ISIC 2018 dataset (Table [1](#S2.T1
    "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")), comparing those to an unmodified DCGAN on $256\times 256$-pixel
    images, and finding the modified pix2pixHD to be qualitatively better. They use
    the performance improvement on a separate classification network as a quantitative
    metric, finding that the use of samples from both PGAN and pix2pixHD leads to
    the best improvements. They also showcase images of size up to $1,024\times 1,024$
    pixels generated by the pix2pixHD-derived model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 关于皮肤病变的 GANs 的两篇开创性论文（Baur 等人，[2018](#bib.bib43)；Bissoto 等人，[2018](#bib.bib60)）评估了几种模型。Baur
    等人 ([2018](#bib.bib43)) 比较了基于噪声的 DCGAN、LAPGAN 和 PGAN 在生成 $256\times 256$ 像素图像方面的表现，使用了定性和定量标准，发现
    PGAN 的结果显著更好。他们进一步将 PGAN 与由皮肤科医生和深度学习专家组成的评审团进行“视觉图灵测试”，结果显示两者都在区分假图像和真实图像时遇到困难。Bissoto
    等人 ([2018](#bib.bib60)) 将 PGAN 适配为在诊断类别上进行条件化，并将图像到图像的 pix2pixHD 应用于 ISIC 2018
    数据集的特征提取任务提供的语义注释（表 [1](#S2.T1 "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation)），与未修改的 DCGAN 在 $256\times 256$
    像素图像上进行比较，发现修改后的 pix2pixHD 在定性上更优。他们使用在单独分类网络上的性能提升作为定量指标，发现使用 PGAN 和 pix2pixHD
    的样本能带来最佳改进。他们还展示了由 pix2pixHD 派生模型生成的高达 $1,024\times 1,024$ 像素的图像。
- en: Pollastri et al. ([2020](#bib.bib309)) extended DCGAN and LAPGAN architectures
    to generate the segmentation masks (in the pairwise scheme explained above), making
    their work the only noise-based GANs usable for segmentation to date. Bi et al.
    ([2019a](#bib.bib48)) introduced stacked adversarial learning to GANs to learn
    class-specific skin lesion image generators given the ground-truth segmentations.
    Abhishek and Hamarneh ([2019](#bib.bib5)) employ pix2pix to translate a binary
    segmentation mask into a dermoscopic image and use the generated image-mask pairs
    to augment skin lesion segmentation training datasets, improving segmentation
    performance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Pollastri 等人 ([2020](#bib.bib309)) 扩展了 DCGAN 和 LAPGAN 架构以生成分割掩码（如上文所述的配对方案），使他们的工作成为迄今为止唯一可用于分割的基于噪声的
    GANs。Bi 等人 ([2019a](#bib.bib48)) 引入了堆叠对抗学习到 GANs 中，以学习特定类别的皮肤病变图像生成器，基于真实的分割结果。Abhishek
    和 Hamarneh ([2019](#bib.bib5)) 使用 pix2pix 将二进制分割掩码转换为皮肤镜图像，并利用生成的图像-掩码对来增强皮肤病变分割训练数据集，从而提高分割性能。
- en: Ding et al. ([2021](#bib.bib124)) feed a segmentation mask and an instance mask
    to a conditional GAN generator, where the instance mask states the diagnostic
    category to be synthesized. In both cases, the discriminator receives different
    resolutions of the generated image and is required to make a decision for each
    of them. Abdelhalim et al. ([2021](#bib.bib3)) is a recent work that also conditions
    PGAN on the class label and uses the generated outputs to augment a melanoma diagnosis
    dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Ding 等人 ([2021](#bib.bib124)) 将一个分割掩码和一个实例掩码输入到条件 GAN 生成器中，其中实例掩码表示要合成的诊断类别。在这两种情况下，鉴别器接收生成图像的不同分辨率，并要求对每一个做出决策。Abdelhalim
    等人 ([2021](#bib.bib3)) 是一项近期的工作，也将 PGAN 条件化于类别标签，并使用生成的输出增强黑色素瘤诊断数据集。
- en: Recently, Bissoto et al. ([2021](#bib.bib61)) cast doubt on the power of GAN-synthesized
    data augmentation to reliably improve skin lesion classification. Their evaluation,
    which included four GAN models, four datasets, and several augmentation scenarios,
    showed improvement only in a severe cross-modality scenario (training on dermoscopic
    and testing on clinical images). To the best of our knowledge, no corresponding
    systematic evaluation exists for skin lesion segmentation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Bissoto 等人（[2021](#bib.bib61)）对GAN合成数据增强在可靠提高皮肤病变分类方面的能力提出了质疑。他们的评估包括四个GAN模型、四个数据集和几个增强场景，只在严重的跨模态场景（在皮肤镜图像上训练并在临床图像上测试）中显示出改进。据我们所知，皮肤病变分割没有对应的系统评估。
- en: 2.3 Supervised, Semi-supervised, Weakly supervised, Self-supervised learning
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 监督、半监督、弱监督、自监督学习
- en: Although supervised DL has achieved outstanding performance in various medical
    image analysis applications, its dependency on high-quality annotations limits
    its applicability, as well as its generalizability to unseen, out-of-distribution
    data. Semi-supervised techniques attempt to learn from both labeled and unlabeled
    samples. Weakly supervised techniques attempt to exploit partial annotations like
    image-level labels or bounding boxes, often in conjunction with a subset of pixel-level
    fully-annotated samples.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管监督深度学习在各种医学图像分析应用中取得了出色的表现，但其对高质量注释的依赖限制了其适用性，以及对未见过的、分布外的数据的泛化能力。半监督技术尝试从有标签和无标签样本中学习。弱监督技术尝试利用部分注释，如图像级标签或边界框，通常结合少量像素级完全注释的样本。
- en: '![Refer to caption](img/800c0cff0aaefcfed7a113bb1d37cdde.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/800c0cff0aaefcfed7a113bb1d37cdde.png)'
- en: 'Figure 5: A breakdown of different levels of supervision used in the $177$
    surveyed works. Fully supervised models continue to make up the majority of the
    literature ($163$ papers), with semi-supervised and weakly supervised methods
    appearing in only $9$ papers. Self-supervision in skin lesion segmentation is
    fairly new, with all the $5$ papers appearing from 2020 onwards.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：对$177$项调查研究中不同监督级别的分解。完全监督模型仍然占据文献的大多数（$163$篇论文），而半监督和弱监督方法仅出现在$9$篇论文中。皮肤病变分割中的自监督方法相当新颖，所有$5$篇论文都出现在2020年以后。
- en: Since pixel-level annotation of skin lesion images is costly, there is a trade-off
    between annotation precision and efficiency. In practice, the annotations are
    intrinsically noisy, which can be modeled explicitly to avoid over-fitting. (We
    discuss the issue of annotation variability in Section [4.2](#S4.SS2 "4.2 Inter-Annotator
    Agreement ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation").)
    To deal with label noise, Mirikharaji et al. ([2019](#bib.bib281)) learn a model
    robust to annotation noise, making use of a large set of unreliable annotations
    and a small set of perfect clean annotations. They propose to learn a spatially
    adaptive weight map corresponding to each training data, assigning different weights
    to noisy and clean pixel-level annotations while training the deep model. To remove
    the dependency on having a set of perfectly clean annotations, Redekop and Chernyavskiy
    ([2021](#bib.bib321)) propose to alter noisy ground-truth masks during training
    by considering the quantification of aleatoric uncertainty (Der Kiureghian and
    Ditlevsen, [2009](#bib.bib118); Gal, [2016](#bib.bib141); Depeweg et al., [2018](#bib.bib117);
    Kwon et al., [2020](#bib.bib235)) to obtain a map of regions of high and low uncertainty.
    Pixels of ground-truth masks in highly uncertain regions are flipped, progressively
    increasing the model’s robustness to label noise. Ribeiro et al. ([2020](#bib.bib326))
    deal with noise by discarding inconsistent samples and annotation detail during
    training time, showing that the model generalizes better even when detailed annotations
    are required in test time.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于皮肤病变图像的像素级注释成本高昂，因此在注释精度和效率之间存在权衡。在实践中，注释本质上是有噪声的，可以通过显式建模来避免过拟合。（我们在第[4.2节](#S4.SS2
    "4.2 Inter-Annotator Agreement ‣ 4 Evaluation ‣ A Survey on Deep Learning for
    Skin Lesion Segmentation")讨论了注释变异性的问题。）为应对标签噪声，Mirikharaji 等人（[2019](#bib.bib281)）学习了一个对注释噪声具有鲁棒性的模型，利用大量不可靠的注释和少量完美的干净注释。他们建议学习一个与每个训练数据对应的空间自适应权重图，在训练深度模型时对噪声和干净的像素级注释赋予不同的权重。为了去除对完美干净注释集的依赖，Redekop
    和 Chernyavskiy（[2021](#bib.bib321)）建议通过考虑随机不确定性的量化（Der Kiureghian 和 Ditlevsen，[2009](#bib.bib118)；Gal，[2016](#bib.bib141)；Depeweg
    等人，[2018](#bib.bib117)；Kwon 等人，[2020](#bib.bib235)）在训练过程中修改噪声的真实值掩码，从而获得高不确定性和低不确定性的区域图。在高度不确定区域中的真实值掩码像素会被翻转，逐渐提高模型对标签噪声的鲁棒性。Ribeiro
    等人（[2020](#bib.bib326)）通过在训练期间丢弃不一致的样本和注释细节来处理噪声，表明即使在测试时需要详细注释，模型的泛化能力也更强。
- en: When there is a labeled dataset, even if the number of labeled samples is far
    less than that of unlabeled samples, semi- and self-supervision techniques can
    be applied. Li et al. ([2021c](#bib.bib247)) propose a semi-supervised approach,
    using a transformation-consistent self-ensemble to leverage unlabeled data and
    to regularize the model. They minimize the difference between the network predictions
    of different transformations (random perturbations, flipping, and rotation) applied
    to the input image and the transformation of the model prediction for the input
    image. Self-supervision attempts to exploit intrinsic labels by solving proxy
    tasks, enabling the use of a large, unlabeled corpus of data to pretrain a model
    before fine-tuning it on the target task. An example is to artificially apply
    random rotations in the input images, and train the model to predict the exact
    degree of rotation (Gidaris et al., [2018](#bib.bib145)). Note that the degree
    of rotation of each image is known, since it was artificially applied, and thus,
    can be used as a label during training. Similarly, for skin lesion segmentation,
    Li et al. ([2020b](#bib.bib249)) propose to exploit the color distribution information,
    the proxy task being to predict values from blue and red color channels while
    having the green one as input. They also include a task to estimate the red and
    blue color distributions to improve the model’s ability to extract global features.
    After the pretraining, they use a smaller set of labeled data to fine-tune the
    model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在标记数据集时，即使标记样本的数量远少于未标记样本的数量，也可以应用半监督和自监督技术。Li 等（[2021c](#bib.bib247)）提出了一种半监督方法，使用变换一致的自集成来利用未标记数据并对模型进行正则化。他们最小化网络对不同变换（随机扰动、翻转和旋转）应用于输入图像的预测与模型预测的变换之间的差异。自监督试图通过解决代理任务来利用内在标签，使得可以使用大量未标记的数据对模型进行预训练，然后在目标任务上进行微调。例如，人工在输入图像中应用随机旋转，并训练模型预测确切的旋转角度（Gidaris
    et al., [2018](#bib.bib145)）。注意到每张图像的旋转角度是已知的，因为它是人工应用的，因此可以在训练过程中用作标签。类似地，对于皮肤病变分割，Li
    等（[2020b](#bib.bib249)）建议利用颜色分布信息，代理任务是预测来自蓝色和红色颜色通道的值，而绿色通道作为输入。他们还包括了一个任务来估计红色和蓝色颜色分布，以提高模型提取全局特征的能力。预训练后，他们使用较小的标记数据集来微调模型。
- en: 2.4 Image Preprocessing
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 图像预处理
- en: 'Preprocessing may facilitate the segmentation of skin lesion images. Typical
    preprocessing operations include:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理可能有助于皮肤病变图像的分割。典型的预处理操作包括：
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Downsampling: Dermoscopy is typically a high-resolution technique, resulting
    in large image sizes, while many convolutional neural network (CNN) architectures,
    e.g., LeNet, AlexNet, VGG, GoogLeNet, ResNet, etc., require fixed-size input images,
    usually $224\times 224$ or $299\times 299$ pixels, and even those CNNs that can
    handle arbitrary-sized images (e.g., fully-convolutional networks (FCNs)) may
    benefit from downsampling for computational reasons. Downsampling is common in
    the skin lesion segmentation literature (Codella et al., [2017](#bib.bib97); Yu
    et al., [2017a](#bib.bib431); Yuan et al., [2017](#bib.bib433); Al-Masni et al.,
    [2018](#bib.bib15); Zhang et al., [2019b](#bib.bib441); Pollastri et al., [2020](#bib.bib309)).'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下采样：皮肤镜检查通常是一种高分辨率技术，产生的图像尺寸较大，而许多卷积神经网络（CNN）架构，例如 LeNet、AlexNet、VGG、GoogLeNet、ResNet
    等，需要固定大小的输入图像，通常为 $224\times 224$ 或 $299\times 299$ 像素，即使那些能够处理任意大小图像的 CNN（例如，完全卷积网络（FCNs））也可能因计算原因受益于下采样。下采样在皮肤病变分割文献中很常见（Codella
    et al., [2017](#bib.bib97); Yu et al., [2017a](#bib.bib431); Yuan et al., [2017](#bib.bib433);
    Al-Masni et al., [2018](#bib.bib15); Zhang et al., [2019b](#bib.bib441); Pollastri
    et al., [2020](#bib.bib309)）。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Color space transformations: RGB images are expected by most models, but some
    works (Codella et al., [2017](#bib.bib97); Al-Masni et al., [2018](#bib.bib15);
    Yuan and Lo, [2019](#bib.bib434); Pollastri et al., [2020](#bib.bib309); Pour
    and Seker, [2020](#bib.bib311)) employ alternative color spaces (Busin et al.,
    [2008](#bib.bib67)), such as CIELAB, CIELUV, and HSV. Often, one or more channels
    of the transformed space are combined with the RGB channels for reasons including,
    but not limited to, increasing the class separability, decoupling luminance and
    chromaticity, ensuring (approximate) perceptual uniformity, achieving invariance
    to illumination or viewpoint, and eliminating highlights.'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 颜色空间转换：大多数模型期望使用RGB图像，但一些研究（Codella等，[2017](#bib.bib97)；Al-Masni等，[2018](#bib.bib15)；Yuan和Lo，[2019](#bib.bib434)；Pollastri等，[2020](#bib.bib309)；Pour和Seker，[2020](#bib.bib311)）采用了替代颜色空间（Busin等，[2008](#bib.bib67)），如CIELAB、CIELUV和HSV。通常，转换空间的一个或多个通道与RGB通道结合，原因包括但不限于提高类别可分性、解耦亮度和色度、确保（近似）感知均匀性、实现对光照或视角的不变性，以及消除高光。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Additional inputs: In addition to color space transformations, recent works
    incorporate more focused and domain-specific inputs to the segmentation models,
    such as Fourier domain representation using the discrete Fourier transform (Tang
    et al., [2021b](#bib.bib373)) and inputs based on the physics of skin illumination
    and imaging (Abhishek et al., [2020](#bib.bib7)).'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 额外输入：除了颜色空间转换，近期的研究将更多聚焦于领域特定的输入纳入分割模型中，例如使用离散傅里叶变换的傅里叶域表示（Tang等，[2021b](#bib.bib373)）和基于皮肤照明和成像物理的输入（Abhishek等，[2020](#bib.bib7)）。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Contrast enhancement: Insufficient contrast (Fig. [1(i)](#S1.F1.sf9 "In Figure
    1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation"))
    is a prime reason for segmentation failures (Bogo et al., [2015](#bib.bib62)),
    leading some works (Saba et al., [2019](#bib.bib334); Schaefer et al., [2011](#bib.bib344))
    to enhance the image contrast prior to segmentation.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对比度增强：对比度不足（图[1(i)](#S1.F1.sf9 "在图1 ‣ 1 引言 ‣ 深度学习在皮肤病变分割中的调查")）是分割失败的主要原因之一（Bogo等，[2015](#bib.bib62)），导致一些研究（Saba等，[2019](#bib.bib334)；Schaefer等，[2011](#bib.bib344)）在分割前增强图像对比度。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Color normalization: Varying illumination (Barata et al., [2015a](#bib.bib38),
    [b](#bib.bib39)) may lead to inconsistencies in skin lesion segmentation. This
    problem can be addressed by color normalization (Goyal et al., [2019b](#bib.bib154)).'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 颜色标准化：光照变化（Barata等，[2015a](#bib.bib38)，[b](#bib.bib39)）可能导致皮肤病变分割中的不一致性。这个问题可以通过颜色标准化来解决（Goyal等，[2019b](#bib.bib154)）。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Artifact removal: Dermoscopic images often present artifacts, among which hair
    (Fig. [1(g)](#S1.F1.sf7 "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation")) is the most distracting (Abbas et al., [2011](#bib.bib1)),
    leading some studies (Ünver and Ayan, [2019](#bib.bib385); Zafar et al., [2020](#bib.bib435);
    Li et al., [2021b](#bib.bib246)) to remove it prior to segmentation.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人工物去除：皮肤镜图像经常出现伪影，其中头发（图[1(g)](#S1.F1.sf7 "在图1 ‣ 1 引言 ‣ 深度学习在皮肤病变分割中的调查")）是最具干扰性的（Abbas等，[2011](#bib.bib1)），导致一些研究（Ünver和Ayan，[2019](#bib.bib385)；Zafar等，[2020](#bib.bib435)；Li等，[2021b](#bib.bib246)）在分割前去除它。
- en: Classical machine learning models (e.g., nearest neighbors, decision trees,
    support vector machines (Celebi et al., [2007b](#bib.bib78), [2008](#bib.bib77);
    Iyatomi et al., [2008](#bib.bib188); Barata et al., [2014](#bib.bib41); Shimizu
    et al., [2015](#bib.bib349))), which rely on hand-crafted features (Barata et al.,
    [2019](#bib.bib40)), tend to benefit more from preprocessing than DL models, which,
    when properly trained, can learn from the data how to bypass input issues (Celebi
    et al., [2015a](#bib.bib79); Valle et al., [2020](#bib.bib388)). However, preprocessing
    may still be helpful when dealing with small or noisy datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习模型（如最近邻、决策树、支持向量机（Celebi等，[2007b](#bib.bib78)，[2008](#bib.bib77)；Iyatomi等，[2008](#bib.bib188)；Barata等，[2014](#bib.bib41)；Shimizu等，[2015](#bib.bib349)）），依赖于手工特征（Barata等，[2019](#bib.bib40)），比深度学习模型更受益于预处理，而深度学习模型在正确训练时可以从数据中学习如何绕过输入问题（Celebi等，[2015a](#bib.bib79)；Valle等，[2020](#bib.bib388)）。然而，处理小型或噪声数据集时，预处理仍可能有帮助。
- en: 3 Model Design and Training
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 模型设计与训练
- en: Multi-layer perceptrons (MLPs) for pixel-level classification (Gish and Blanz,
    [1989](#bib.bib147); Katz and Merickel, [1989](#bib.bib214)) appeared soon after
    the publication of the seminal backpropagation paper (Rumelhart et al., [1986](#bib.bib333)),
    but these shallow feed-forward networks had many drawbacks (LeCun et al., [1998](#bib.bib239)),
    including an excessive number of parameters, lack of invariance, and disregard
    for the inherent structure present in images.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 用于像素级分类的多层感知机（MLPs）（Gish and Blanz, [1989](#bib.bib147); Katz and Merickel,
    [1989](#bib.bib214)）在开创性的反向传播论文（Rumelhart et al., [1986](#bib.bib333)）发表后不久出现，但这些浅层前馈网络存在许多缺陷（LeCun
    et al., [1998](#bib.bib239)），包括参数过多、缺乏不变性以及忽视图像中固有结构。
- en: 'CNNs are deep feedforward neural networks designed to extract progressively
    more abstract features from multidimensional signals ($1$-D signals, $2$-D images,
    $3$-D video, etc.) (LeCun et al., [2015](#bib.bib238)). Therefore, in addition
    to addressing the aforementioned problems of MLPs, CNNs automate *feature engineering* (Bengio
    et al., [2013](#bib.bib46)), that is, the design of algorithms that can transform
    raw signal values to discriminative features. Another advantage of CNNs over traditional
    machine learning classifiers is that they require minimal preprocessing of the
    input data. Due to their significant advantages, CNNs have become the method of
    choice in many medical image analysis applications over the past decade (Litjens
    et al., [2017](#bib.bib256)). The key enablers in this deep learning revolution
    were: (i) the availability of massive data sets; (ii) the availability of powerful
    and inexpensive graphics processing units; (iii) the development of better network
    architectures, learning algorithms, and regularization techniques; and (iv) the
    development of open-source deep learning frameworks.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是深度前馈神经网络，旨在从多维信号（$1$-D 信号、$2$-D 图像、$3$-D 视频等）中逐步提取更加抽象的特征（LeCun et al.,
    [2015](#bib.bib238)）。因此，除了处理 MLPs 上述问题外，CNN 还自动进行 *特征工程*（Bengio et al., [2013](#bib.bib46)），即设计能够将原始信号值转化为判别特征的算法。CNN
    相比于传统机器学习分类器的另一个优势是对输入数据的预处理要求较少。由于其显著的优势，CNN 在过去十年中已成为许多医学图像分析应用的首选方法（Litjens
    et al., [2017](#bib.bib256)）。深度学习革命中的关键推动因素包括：（i）海量数据集的可用性；（ii）强大且廉价的图形处理单元的可用性；（iii）更好的网络架构、学习算法和正则化技术的发展；（iv）开源深度学习框架的发展。
- en: Semantic segmentation may be understood as the attempt to answer the parallel
    and complementary questions “what” and “where” in a given image. The former is
    better answered by translation-invariant global features, while the latter requires
    well-localized features, posing a challenge to deep models. CNNs for pixel-level
    classification first appeared in the mid-2000s (Ning et al., [2005](#bib.bib288)),
    but their use accelerated after the seminal paper on FCNs by Long et al. ([2015](#bib.bib264)),
    which, along with U-Net (Ronneberger et al., [2015](#bib.bib328)), have become
    the basis for many state-of-the-art segmentation models. In contrast to classification
    CNNs (e.g., LeNet, AlexNet, VGG, GoogLeNet, ResNet), FCNs easily cope with arbitrary-sized
    input images.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割可以理解为试图回答给定图像中的“什么”和“哪里”这两个并行互补的问题。前者由平移不变的全局特征更好地回答，而后者则需要良好定位的特征，这对深度模型提出了挑战。用于像素级分类的
    CNN 最早出现在2000年代中期（Ning et al., [2005](#bib.bib288)），但在 Long et al. ([2015](#bib.bib264))
    关于 FCNs 的开创性论文发布后，其使用加速了，这篇论文与 U-Net（Ronneberger et al., [2015](#bib.bib328)）已成为许多最先进分割模型的基础。与分类
    CNN（如 LeNet、AlexNet、VGG、GoogLeNet、ResNet）相比，FCNs 可以轻松处理任意大小的输入图像。
- en: 3.1 Architecture
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 架构
- en: 'An ideal skin lesion segmentation algorithm is accurate, computationally inexpensive,
    invariant to noise and input transformations, requires little training data and
    is easy to implement and train. Unfortunately, no algorithm has, so far, been
    able to achieve these conflicting goals. DL-based segmentation tends towards accuracy
    and invariance at the cost of computation and training data. Ease of implementation
    is debatable: on the one hand, the algorithms often forgo cumbersome preprocessing,
    postprocessing, and feature engineering steps. On the other hand, tuning and optimizing
    them is often a painstaking task.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的皮肤病变分割算法应具备准确性、计算开销小、对噪声和输入变换不变、需要少量训练数据且易于实现和训练。不幸的是，到目前为止，没有算法能够实现这些相互矛盾的目标。基于深度学习的分割趋向于在计算和训练数据的代价下实现准确性和不变性。实现的难易程度存在争议：一方面，这些算法通常省去了繁琐的预处理、后处理和特征工程步骤。另一方面，调优和优化它们通常是一个费力的任务。
- en: As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Architecture ‣ 3 Model Design and
    Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation"), we have classified
    the existing literature into single-network models, multiple-network models, hybrid-feature
    models, and Transformer models. The first and second groups are somewhat self-descriptive,
    but notice that the latter is further divided into ensembles of models, multi-task
    methods (often performing simultaneous classification and segmentation), and GANs.
    Hybrid-feature models combine DL with hand-crafted features. Transformer models,
    as the name suggests, employ Transformers either with or without CNNs for segmentation,
    and have started being used for skin lesion segmentation only recently. We classified
    works according to their most relevant feature, but the architectural improvements
    discussed in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Single Network Models ‣ 3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")
    also appear in the models listed in the other sections. In Fig. [7](#S3.F7 "Figure
    7 ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture ‣ 3 Model Design and Training
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation"), we show how frequently
    different architectural modules appear in the $177$ surveyed works, grouped by
    our taxonomy of model architectures (Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[6](#S3.F6 "Figure 6 ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A
    Survey on Deep Learning for Skin Lesion Segmentation")所示，我们将现有文献分类为单网络模型、多网络模型、混合特征模型和Transformer模型。前两组模型较为自描述，但注意到后者进一步分为模型集成、多任务方法（通常同时进行分类和分割）和GANs。混合特征模型结合了深度学习与手工特征。Transformer模型，如其名所示，使用Transformer进行分割，无论是否结合CNN，并且最近才开始用于皮肤病变分割。我们根据其最相关的特征对工作进行了分类，但第[3.1.1](#S3.SS1.SSS1
    "3.1.1 Single Network Models ‣ 3.1 Architecture ‣ 3 Model Design and Training
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")节中讨论的架构改进也出现在其他部分列出的模型中。图[7](#S3.F7
    "Figure 7 ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture ‣ 3 Model Design and
    Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")显示了不同架构模块在$177$项调查工作中出现的频率，并按我们对模型架构的分类（图[6](#S3.F6
    "Figure 6 ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation")）进行分组。
- en: 'Table LABEL:tab:main summarizes all the $177$ surveyed works in this review,
    with the following attributes for each work: type of publication, datasets, architectural
    modules, loss functions, and augmentations used, reported Jaccard index, whether
    the paper performed cross-dataset evaluation (CDE) and postprocessing (PP), and
    whether the corresponding code was released publicly. For papers that reported
    segmentation results on more than 1 dataset, we list all of them and list the
    performance on only one dataset, formatting that particular dataset in bold. Since
    ISIC 2017 is the most popular dataset (Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Datasets
    ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")), wherever
    reported, we note the performance (Jaccard index) on ISIC 2017\. For papers that
    do not report the Jaccard index and instead report the Dice score, we compute
    the former based on the latter and report this computed score denoted by an asterisk.
    Cross-dataset evaluation (CDE) refers to when a paper trained model(s) on one
    dataset but evaluated on another.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 LABEL:tab:main 总结了本综述中调查的所有 $177$ 项工作，每项工作的属性包括：出版类型、数据集、架构模块、损失函数和使用的增强方法，报告的
    Jaccard 指数，论文是否进行了跨数据集评估 (CDE) 和后处理 (PP)，以及相关代码是否公开发布。对于在多个数据集上报告分割结果的论文，我们列出了所有数据集，并仅列出一个数据集的性能，将该数据集格式化为粗体。由于
    ISIC 2017 是最受欢迎的数据集（图 [3](#S2.F3 "Figure 3 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")），无论何时报告，我们都注明在 ISIC 2017 上的性能（Jaccard
    指数）。对于未报告 Jaccard 指数而报告 Dice 分数的论文，我们根据后者计算前者，并报告该计算得分，标记为星号。跨数据集评估 (CDE) 是指论文在一个数据集上训练模型但在另一个数据集上进行评估。
- en: <svg   height="476.9" overflow="visible" version="1.1" width="585.17"><g transform="translate(0,476.9)
    matrix(1 0 0 -1 0 0) translate(292.58,0) translate(0,184.32)" fill="#000000" stroke="#000000"
    stroke-width="1.2pt"><g transform="matrix(1.0 0.0 0.0 1.0 -68.9 6.92)" fill="#000000"
    stroke="#000000"><foreignobject width="137.8" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Segmentation Model Architectures §[3.1](#S3.SS1
    "3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for
    Skin Lesion Segmentation") <g fill="#FFCC99" stroke="#FFCC99"><path d="M -94.9
    -139.19 C -94.9 -114.73 -114.73 -94.9 -139.19 -94.9 C -163.66 -94.9 -183.49 -114.73
    -183.49 -139.19 C -183.49 -163.66 -163.66 -183.49 -139.19 -183.49 C -114.73 -183.49
    -94.9 -163.66 -94.9 -139.19 Z M -139.19 -139.19"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -178.56 -134.35)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transformer
    Models §[3.1.4](#S3.SS1.SSS4 "3.1.4 Transformer Models ‣ 3.1 Architecture ‣ 3
    Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M -55.68 -55.68 M -33.28 -71.36 C -50.06 -63.54 -63.54 -50.06
    -71.36 -33.28 C -63.04 -51.12 -72.34 -66.86 -86.26 -80.78 L -80.78 -86.26 C -66.86
    -72.34 -51.12 -63.04 -33.28 -71.36 Z M -89.47 -94.95 L -80.78 -86.26 L -86.26
    -80.78 L -94.95 -89.47 Z M -86.26 -80.78 M -99.05 -120.47 C -103.45 -111.03 -111.03
    -103.45 -120.47 -99.05 C -110.44 -103.73 -102.78 -97.3 -94.95 -89.47 L -89.47
    -94.95 C -97.3 -102.78 -103.73 -110.44 -99.05 -120.47 Z"></path></clippath><g
    clip-path="url(#pgfcp17)"><g transform="matrix(1.0 0.0 0.0 1.0 -55.68 -55.68)"><g
    fill="#FFB3B3"><path d="M 54.71 -56.66 L 111.36 -0.01 L -0.01 111.36 L -56.66
    54.71 Z M -0.01 111.36" style="stroke:none"></path></g><g fill="#FFCC99"><path
    d="M 4.46 -106.9 L -27.83 -139.2 L -139.2 -27.83 L -106.9 4.46 Z M -139.2 -27.83"
    style="stroke:none"><g transform="matrix(-0.37585 -0.37585 0.80185 -0.80185 -26.1
    -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#D9668C" stroke="#D9668C"><path d="M 183.49 -139.19 C 183.49 -114.73 163.66
    -94.9 139.19 -94.9 C 114.73 -94.9 94.9 -114.73 94.9 -139.19 C 94.9 -163.66 114.73
    -183.49 139.19 -183.49 C 163.66 -183.49 183.49 -163.66 183.49 -139.19 Z M 139.19
    -139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 99.82 -134.35)" fill="#000000"
    stroke="#000000"><foreignobject width="78.74" height="29.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Hybrid Feature Models §[3.1.3](#S3.SS1.SSS3
    "3.1.3 Hybrid Feature Models ‣ 3.1 Architecture ‣ 3 Model Design and Training
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M 55.68 -55.68 M 71.36 -33.28 C 63.54 -50.06 50.06 -63.54
    33.28 -71.36 C 51.12 -63.04 66.86 -72.34 80.78 -86.26 L 86.26 -80.78 C 72.34 -66.86
    63.04 -51.12 71.36 -33.28 Z M 94.95 -89.47 L 86.26 -80.78 L 80.78 -86.26 L 89.47
    -94.95 Z M 80.78 -86.26 M 120.47 -99.05 C 111.03 -103.45 103.45 -111.03 99.05
    -120.47 C 103.73 -110.44 97.3 -102.78 89.47 -94.95 L 94.95 -89.47 C 102.78 -97.3
    110.44 -103.73 120.47 -99.05 Z"></path></clippath><g clip-path="url(#pgfcp19)"><g
    transform="matrix(1.0 0.0 0.0 1.0 55.68 -55.68)"><g fill="#FFB3B3"><path d="M
    56.66 54.71 L 0.01 111.36 L -111.36 -0.01 L -54.71 -56.66 Z M -111.36 -0.01" style="stroke:none"></path></g><g
    fill="#D9668C"><path d="M 106.9 4.46 L 139.2 -27.83 L 27.83 -139.2 L -4.46 -106.9
    Z M 27.83 -139.2" style="stroke:none"><g transform="matrix(0.37585 -0.37585 0.80185
    0.80185 26.1 -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#CCCCCC" stroke="#CCCCCC"><path d="M 183.49 139.19 C 183.49 163.66 163.66
    183.49 139.19 183.49 C 114.73 183.49 94.9 163.66 94.9 139.19 C 94.9 114.73 114.73
    94.9 139.19 94.9 C 163.66 94.9 183.49 114.73 183.49 139.19 Z M 139.19 139.19"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 99.82 144.04)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multiple
    Network Models §[3.1.2](#S3.SS1.SSS2 "3.1.2 Multiple Network Models ‣ 3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M 55.68 55.68 M 33.28 71.36 C 50.06 63.54 63.54 50.06 71.36
    33.28 C 63.04 51.12 72.34 66.86 86.26 80.78 L 80.78 86.26 C 66.86 72.34 51.12
    63.04 33.28 71.36 Z M 89.47 94.95 L 80.78 86.26 L 86.26 80.78 L 94.95 89.47 Z
    M 86.26 80.78 M 99.05 120.47 C 103.45 111.03 111.03 103.45 120.47 99.05 C 110.44
    103.73 102.78 97.3 94.95 89.47 L 89.47 94.95 C 97.3 102.78 103.73 110.44 99.05
    120.47 Z"></path></clippath><g clip-path="url(#pgfcp21)"><g transform="matrix(1.0
    0.0 0.0 1.0 55.68 55.68)"><g fill="#FFB3B3"><path d="M -54.71 56.66 L -111.36
    0.01 L 0.01 -111.36 L 56.66 -54.71 Z M 0.01 -111.36" style="stroke:none"></path></g><g
    fill="#CCCCCC"><path d="M -4.46 106.9 L 27.83 139.2 L 139.2 27.83 L 106.9 -4.46
    Z M 139.2 27.83" style="stroke:none"><g transform="matrix(0.37585 0.37585 -0.80185
    0.80185 26.1 26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#CCCCCC" stroke="#CCCCCC"><path d="M 291.75 139.19 C 291.75 158.22 276.33
    173.64 257.3 173.64 C 238.28 173.64 222.86 158.22 222.86 139.19 C 222.86 120.17
    238.28 104.75 257.3 104.75 C 276.33 104.75 291.75 120.17 291.75 139.19 Z M 257.3
    139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 227.78 135.04)" fill="#000000"
    stroke="#000000"><foreignobject width="59.06" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">GANs §[3.1.2.3](#S3.SS1.SSS2.P3 "3.1.2.3 Generative
    Adversarial Models ‣ 3.1.2 Multiple Network Models ‣ 3.1 Architecture ‣ 3 Model
    Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#CCCCCC"><path d="M 183.49 139.19 M 180.81 154.34 C 184.38 144.56 184.38
    133.83 180.81 124.05 C 184.6 134.45 194.56 136.18 205.63 136.18 L 205.63 142.21
    C 194.56 142.21 184.6 143.94 180.81 154.34 Z M 205.63 136.18 h 0 v 6.03 h 0 Z
    M 224.93 150.98 C 222.16 143.37 222.16 135.02 224.93 127.41 C 221.99 135.5 214.24
    136.18 205.63 136.18 L 205.63 142.21 C 214.24 142.21 221.99 142.88 224.93 150.98
    Z" style="stroke:none"></path></g><g fill="#CCCCCC" stroke="#CCCCCC"><path d="M
    257.16 222.71 C 257.16 241.74 241.74 257.16 222.71 257.16 C 203.69 257.16 188.26
    241.74 188.26 222.71 C 188.26 203.69 203.69 188.26 222.71 188.26 C 241.74 188.26
    257.16 203.69 257.16 222.71 Z M 222.71 222.71"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 193.18 226.86)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multi-task
    Models §[3.1.2.2](#S3.SS1.SSS2.P2 "3.1.2.2 Multi-task Models ‣ 3.1.2 Multiple
    Network Models ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")</foreignobject></g> <g fill="#CCCCCC"><path
    d="M 170.51 170.51 M 157.91 179.34 C 167.35 174.94 174.94 167.35 179.34 157.91
    C 174.66 167.95 180.47 176.21 188.3 184.04 L 184.04 188.3 C 176.21 180.47 167.95
    174.66 157.91 179.34 Z M 184.04 188.3 L 184.04 188.3 L 188.3 184.04 L 188.3 184.04
    Z M 188.3 184.04 M 191.49 208.15 C 194.91 200.81 200.81 194.91 208.15 191.49 C
    200.35 195.13 194.39 190.13 188.3 184.04 L 184.04 188.3 C 190.13 194.39 195.13
    200.35 191.49 208.15 Z" style="stroke:none"></path></g><g fill="#CCCCCC" stroke="#CCCCCC"><path
    d="M 173.64 257.3 C 173.64 276.33 158.22 291.75 139.19 291.75 C 120.17 291.75
    104.75 276.33 104.75 257.3 C 104.75 238.28 120.17 222.86 139.19 222.86 C 158.22
    222.86 173.64 238.28 173.64 257.3 Z M 139.19 257.3"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 109.67 253.15)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Ensembles
    §[3.1.2.1](#S3.SS1.SSS2.P1 "3.1.2.1 Standard Ensembles ‣ 3.1.2 Multiple Network
    Models ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation")</foreignobject></g> <g fill="#CCCCCC"><path d="M
    139.19 183.49 M 124.05 180.81 C 133.83 184.38 144.56 184.38 154.34 180.81 C 143.94
    184.6 142.21 194.56 142.21 205.63 L 136.18 205.63 C 136.18 194.56 134.45 184.6
    124.05 180.81 Z M 136.18 205.63 L 136.18 205.63 L 142.21 205.63 L 142.21 205.63
    Z M 142.21 205.63 M 127.41 224.93 C 135.02 222.16 143.37 222.16 150.98 224.93
    C 142.88 221.99 142.21 214.24 142.21 205.63 L 136.18 205.63 C 136.18 214.24 135.5
    221.99 127.41 224.93 Z" style="stroke:none"></path></g><g fill="#FFF07E" stroke="#FFF07E"><path
    d="M -94.9 139.19 C -94.9 163.66 -114.73 183.49 -139.19 183.49 C -163.66 183.49
    -183.49 163.66 -183.49 139.19 C -183.49 114.73 -163.66 94.9 -139.19 94.9 C -114.73
    94.9 -94.9 114.73 -94.9 139.19 Z M -139.19 139.19"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -178.56 144.04)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Single Network
    Models §[3.1.1](#S3.SS1.SSS1 "3.1.1 Single Network Models ‣ 3.1 Architecture ‣
    3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M -55.68 55.68 M -71.36 33.28 C -63.54 50.06 -50.06 63.54
    -33.28 71.36 C -51.12 63.04 -66.86 72.34 -80.78 86.26 L -86.26 80.78 C -72.34
    66.86 -63.04 51.12 -71.36 33.28 Z M -94.95 89.47 L -86.26 80.78 L -80.78 86.26
    L -89.47 94.95 Z M -80.78 86.26 M -120.47 99.05 C -111.03 103.45 -103.45 111.03
    -99.05 120.47 C -103.73 110.44 -97.3 102.78 -89.47 94.95 L -94.95 89.47 C -102.78
    97.3 -110.44 103.73 -120.47 99.05 Z"></path></clippath><g clip-path="url(#pgfcp23)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -55.68 55.68)"><g fill="#FFB3B3"><path d="M
    -56.66 -54.71 L -0.01 -111.36 L 111.36 0.01 L 54.71 56.66 Z M 111.36 0.01" style="stroke:none"></path></g><g
    fill="#FFF07E"><path d="M -106.9 -4.46 L -139.2 27.83 L -27.83 139.2 L 4.46 106.9
    Z M -27.83 139.2" style="stroke:none"><g transform="matrix(-0.37585 0.37585 -0.80185
    -0.80185 -26.1 26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#FFF07E" stroke="#FFF07E"><path d="M -21.23 222.71 C -21.23 241.74 -36.65
    257.16 -55.68 257.16 C -74.7 257.16 -90.13 241.74 -90.13 222.71 C -90.13 203.69
    -74.7 188.26 -55.68 188.26 C -36.65 188.26 -21.23 203.69 -21.23 222.71 Z M -55.68
    222.71"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -85.21 226.86)" fill="#000000"
    stroke="#000000"><foreignobject width="59.06" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Recurrent CNNs S[3.1.1.5](#S3.SS1.SSS1.P5 "3.1.1.5
    Recurrent Convolutional Neural Networks ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#FFF07E"><path d="M -107.88 170.51 M -120.48 179.34 C -111.04 174.94
    -103.45 167.35 -99.05 157.91 C -103.73 167.95 -97.91 176.21 -90.08 184.04 L -94.35
    188.3 C -102.18 180.47 -110.44 174.66 -120.48 179.34 Z M -94.35 188.3 L -94.35
    188.3 L -90.08 184.04 L -90.09 184.04 Z M -90.08 184.04 M -86.9 208.15 C -83.48
    200.81 -77.58 194.91 -70.24 191.49 C -78.04 195.13 -84 190.13 -90.09 184.04 L
    -94.35 188.3 C -88.26 194.39 -83.26 200.35 -86.9 208.15 Z" style="stroke:none"></path></g><g
    fill="#FFF07E" stroke="#FFF07E"><path d="M -104.75 257.3 C -104.75 276.33 -120.17
    291.75 -139.19 291.75 C -158.22 291.75 -173.64 276.33 -173.64 257.3 C -173.64
    238.28 -158.22 222.86 -139.19 222.86 C -120.17 222.86 -104.75 238.28 -104.75 257.3
    Z M -139.19 257.3"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -168.72 261.46)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Attention Modules §[3.1.1.4](#S3.SS1.SSS1.P4
    "3.1.1.4 Attention Modules ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture ‣
    3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#FFF07E"><path d="M -139.19 183.49 M -154.34 180.81 C -144.56 184.38
    -133.83 184.38 -124.05 180.81 C -134.45 184.6 -136.18 194.56 -136.18 205.63 L
    -142.21 205.63 C -142.21 194.56 -143.94 184.6 -154.34 180.81 Z M -142.21 205.63
    L -142.21 205.63 L -136.18 205.63 L -136.18 205.63 Z M -136.18 205.63 M -150.98
    224.93 C -143.37 222.16 -135.02 222.16 -127.41 224.93 C -135.5 221.99 -136.18
    214.24 -136.18 205.63 L -142.21 205.63 C -142.21 214.24 -142.88 221.99 -150.98
    224.93 Z" style="stroke:none"></path></g><g fill="#FFF07E" stroke="#FFF07E"><path
    d="M -188.26 222.71 C -188.26 241.74 -203.69 257.16 -222.71 257.16 C -241.74 257.16
    -257.16 241.74 -257.16 222.71 C -257.16 203.69 -241.74 188.26 -222.71 188.26 C
    -203.69 188.26 -188.26 203.69 -188.26 222.71 Z M -222.71 222.71"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -252.24 226.86)" fill="#000000" stroke="#000000"><foreignobject
    width="59.06" height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multi-scale
    Modules §[3.1.1.3](#S3.SS1.SSS1.P3 "3.1.1.3 Multi-scale Modules ‣ 3.1.1 Single
    Network Models ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")</foreignobject></g> <g fill="#FFF07E"><path
    d="M -170.51 170.51 M -179.34 157.91 C -174.94 167.35 -167.35 174.94 -157.91 179.34
    C -167.95 174.66 -176.21 180.47 -184.04 188.3 L -188.3 184.04 C -180.47 176.21
    -174.66 167.95 -179.34 157.91 Z M -188.3 184.04 L -188.3 184.04 L -184.04 188.3
    L -184.04 188.3 Z M -184.04 188.3 M -208.15 191.49 C -200.81 194.91 -194.91 200.81
    -191.49 208.15 C -195.13 200.35 -190.13 194.39 -184.04 188.3 L -188.3 184.04 C
    -194.39 190.13 -200.35 195.13 -208.15 191.49 Z" style="stroke:none"></path></g><g
    fill="#FFF07E" stroke="#FFF07E"><path d="M -222.86 139.19 C -222.86 158.22 -238.28
    173.64 -257.3 173.64 C -276.33 173.64 -291.75 158.22 -291.75 139.19 C -291.75
    120.17 -276.33 104.75 -257.3 104.75 C -238.28 104.75 -222.86 120.17 -222.86 139.19
    Z M -257.3 139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -286.83 143.35)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Conv. Modules §[3.1.1.2](#S3.SS1.SSS1.P2 "3.1.1.2
    Convolutional Modules ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture ‣ 3 Model
    Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#FFF07E"><path d="M -183.49 139.19 M -180.81 124.05 C -184.38 133.83
    -184.38 144.56 -180.81 154.34 C -184.6 143.94 -194.56 142.21 -205.63 142.21 L
    -205.63 136.18 C -194.56 136.18 -184.6 134.45 -180.81 124.05 Z M -205.63 136.18
    L -205.63 136.18 L -205.63 142.21 L -205.63 142.21 Z M -205.63 142.21 M -224.93
    127.41 C -222.16 135.02 -222.16 143.37 -224.93 150.98 C -221.99 142.88 -214.24
    142.21 -205.63 142.21 L -205.63 136.18 C -214.24 136.18 -221.99 135.5 -224.93
    127.41 Z" style="stroke:none"></path></g><g fill="#FFF07E" stroke="#FFF07E"><path
    d="M -188.26 55.68 C -188.26 74.7 -203.69 90.13 -222.71 90.13 C -241.74 90.13
    -257.16 74.7 -257.16 55.68 C -257.16 36.65 -241.74 21.23 -222.71 21.23 C -203.69
    21.23 -188.26 36.65 -188.26 55.68 Z M -222.71 55.68"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -252.24 59.83)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Shortcut
    Connections §[3.1.1.1](#S3.SS1.SSS1.P1 "3.1.1.1 Shortcut Connections ‣ 3.1.1 Single
    Network Models ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")</foreignobject></g><g fill="#FFF07E"><path
    d="M -170.51 107.88 M -157.91 99.05 C -167.35 103.45 -174.94 111.04 -179.34 120.48
    C -174.66 110.44 -180.47 102.18 -188.3 94.35 L -184.04 90.08 C -176.21 97.91 -167.95
    103.73 -157.91 99.05 Z M -184.04 90.09 L -184.04 90.08 L -188.3 94.35 L -188.3
    94.35 Z M -188.3 94.35 M -191.49 70.24 C -194.91 77.58 -200.81 83.48 -208.15 86.9
    C -200.35 83.26 -194.39 88.26 -188.3 94.35 L -184.04 90.09 C -190.13 84 -195.13
    78.04 -191.49 70.24 Z" style="stroke:none"></path></g>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`<svg height="476.9" overflow="visible" version="1.1" width="585.17"><g transform="translate(0,476.9)
    matrix(1 0 0 -1 0 0) translate(292.58,0) translate(0,184.32)" fill="#000000" stroke="#000000"
    stroke-width="1.2pt"><g transform="matrix(1.0 0.0 0.0 1.0 -68.9 6.92)" fill="#000000"
    stroke="#000000"><foreignobject width="137.8" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">分割模型架构 §[3.1](#S3.SS1 "3.1 架构 ‣ 3 模型设计与训练 ‣
    深度学习在皮肤病变分割中的应用综述") <g fill="#FFCC99" stroke="#FFCC99"><path d="M -94.9 -139.19
    C -94.9 -114.73 -114.73 -94.9 -139.19 -94.9 C -163.66 -94.9 -183.49 -114.73 -183.49
    -139.19 C -183.49 -163.66 -163.66 -183.49 -139.19 -183.49 C -114.73 -183.49 -94.9
    -163.66 -94.9 -139.19 Z M -139.19 -139.19"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -178.56 -134.35)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">变压器模型 §[3.1.4](#S3.SS1.SSS4
    "3.1.4 变压器模型 ‣ 3.1 架构 ‣ 3 模型设计与训练 ‣ 深度学习在皮肤病变分割中的应用综述")</foreignobject></g> <clippath
    ><path d="M -55.68 -55.68 M -33.28 -71.36 C -50.06 -63.54 -63.54 -50.06 -71.36
    -33.28 C -63.04 -51.12 -72.34 -66.86 -86.26 -80.78 L -80.78 -86.26 C -66.86 -72.34
    -51.12 -63.04 -33.28 -71.36 Z M -89.47 -94.95 L -80.78 -86.26 L -86.26 -80.78
    L -94.95 -89.47 Z M -86.26 -80.78 M -99.05 -120.47 C -103.45 -111.03 -111.03 -103.45
    -120.47 -99.05 C -110.44 -103.73 -102.78 -97.3 -94.95 -89.47 L -89.47 -94.95 C
    -97.3 -102.78 -103.73 -110.44 -99.05 -120.47 Z"></path></clippath><g clip-path="url(#pgfcp17)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -55.68 -55.68)"><g fill="#FFB3B3"><path d="M
    54.71 -56.66 L 111.36 -0.01 L -0.01 111.36 L -56.66 54.71 Z M -0.01 111.36" style="stroke:none"></path></g><g
    fill="#FFCC99"><path d="M 4.46 -106.9 L -27.83 -139.2 L -139.2 -27.83 L -106.9
    4.46 Z M -139.2 -27.83" style="stroke:none"><g transform="matrix(-0.37585 -0.37585
    0.80185 -0.80185 -26.1 -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#D9668C" stroke="#D9668C"><path d="M 183.49 -139.19 C 183.49 -114.73 163.66
    -94.9 139.19 -94.9 C 114.73 -94.9 94.9 -114.73 94.9 -139.19 C 94.9 -163.66 114.73
    -183.49 139.19 -183.49 C 163.66 -183.49 183.49 -163.66 183.49 -139.19 Z M 139.19
    -139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 99.82 -134.35)" fill="#000000"
    stroke="#000000"><foreignobject width="78.74" height="29.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">混合特征模型 §[3.1.3](#S3.SS1.SSS3 "3.1.3 混合特征模型
    ‣ 3.1 架构 ‣ 3 模型设计与训练 ‣ 深度学习在皮肤病变分割中的应用综述")</foreignobject></g> <clippath ><path
    d="M 55.68 -55.68 M 71.36 -33.28 C 63.54 -50.06 50.06 -63.54 33.28 -71.36 C 51.12
    -63.04 66.86 -72.34 80.78 -86.26 L 86.26 -80.78 C 72.34 -66.86 63.04 -51.12 71.36
    -33.28 Z M 94.95 -89.47 L 86.26 -80.78 L 80.78 -86.26 L 89.47 -94.95 Z M 80.78
    -86.26 M 120.47 -99.05 C 111.03 -103.45 103.45 -111.03 99.05 -120.47 C 103.73
    -110.44 97.3 -102.78 89.47 -94.95 L 94.95 -89.47 C 102.78 -97.3 110.44 -103.73
    120.47 -99.05 Z"></path></clippath><g clip-path="url(#pgfcp19)"><g transform="matrix(1.0
    0.0 0.0 1.0 55.68 -55.68)"><g fill="#FFB3B3"><path d="M 56.66 54.71 L 0.01 111.36
    L -111.36 -0.01 L -54.71 -56.66 Z M -111.36 -0.01" style="stroke:none"></path></g><g
    fill="#D9668C"><path d="M 106.9 4.46 L 139.2 -27.83 L 27.83 -139.2 L -4.46 -106.9
    Z M 27.83 -139.2" style="stroke:none"><g transform="matrix(0.37585 -0.37585 0.80185
    0.80185 26.1 -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#CCCCCC" stroke="#CCCCCC"><path d="M 183.49 139.19 C 183.49 163.66 163.66
    183.49 139.19 183.49 C 114.73 183.49 94.9 163.66 94.9 139.19 C 94.9 114.73 114.73
    94.9 139.19 94.9 C 163.66 94.9 183.49 114.73 183.49 139.19 Z M 139.19 139.19"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 99.82 144.04)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">多网络模型
    §[3.1.2](#S3.SS1.SSS2 "3.1.2 多网络模型 ‣ 3.1 架构 ‣ 3 模型设计与训练 ‣ 深度学习在皮肤病变分割中的应用综述")</foreignobject></g>
    <clippath'
- en: 'Figure 6: Taxonomy of DL-based skin lesion segmentation model architectures.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：基于深度学习的皮肤病变分割模型架构分类。
- en: 3.1.1 Single Network Models
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 单网络模型
- en: The approaches in this section employ a single DL model, usually an FCN, following
    an encoder-decoder structure, where the encoder extracts increasingly abstract
    features, and the decoder outputs the segmentation mask. In this section, we discuss
    these architectural choices for designing deep models for skin lesion segmentation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的方法采用单个深度学习模型，通常是 FCN，遵循编码器-解码器结构，其中编码器提取越来越抽象的特征，解码器输出分割掩码。本节讨论了为皮肤病变分割设计深度模型的这些架构选择。
- en: Earlier DL-based skin lesion segmentation works adopted either FCN (Long et al.,
    [2015](#bib.bib264)) or U-Net (Ronneberger et al., [2015](#bib.bib328)). FCN originally
    comprised a backbone of VGG16 (Simonyan and Zisserman, [2014](#bib.bib353)) CNN
    layers in the encoder and a single deconvolution layer in the encoder. The original
    paper proposes three versions, two with skip connections (FCN-8 and FCN-16), and
    one without them (FCN-32). U-Net (Ronneberger et al., [2015](#bib.bib328)), originally
    proposed for segmenting electron microscopy images, was rapidly adopted in the
    medical image segmentation literature. As its name suggests, it is a U-shaped
    model, with an encoder stacking convolutional layers that double in size filterwise,
    intercalated by pooling layers, and a symmetric decoder with pooling layers replaced
    by up-convolutions. Skip connections between corresponding encoder-decoder blocks
    improve the flow of information between layers, preserving low-level features
    lost during pooling and producing detailed segmentation boundaries.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 早期基于深度学习的皮肤病变分割工作采用了 FCN（Long 等人，[2015](#bib.bib264)）或 U-Net（Ronneberger 等人，[2015](#bib.bib328)）。FCN
    最初由 VGG16（Simonyan 和 Zisserman，[2014](#bib.bib353)）卷积神经网络层组成的主干和一个解卷积层组成。原始论文提出了三个版本，其中两个带有跳跃连接（FCN-8
    和 FCN-16），一个没有（FCN-32）。U-Net（Ronneberger 等人，[2015](#bib.bib328)），最初提出用于分割电子显微镜图像，在医学图像分割文献中迅速被采纳。顾名思义，它是一个
    U 形模型，编码器堆叠的卷积层在滤波器大小上加倍，通过池化层间隔，解码器对称，池化层被上卷积替换。对应的编码器-解码器块之间的跳跃连接改善了层间信息流动，保留了在池化过程中丢失的低级特征，并产生详细的分割边界。
- en: U-Net frequently appears in the skin lesion segmentation literature both in
    its original form (Codella et al., [2017](#bib.bib97); Pollastri et al., [2020](#bib.bib309);
    Ramani and Ranjani, [2019](#bib.bib318)) and modified forms  (Tang et al., [2019a](#bib.bib371);
    Alom et al., [2019](#bib.bib22); Hasan et al., [2020](#bib.bib172)), discussed
    below. Some works introduce their own models (Yuan et al., [2017](#bib.bib433);
    Al-Masni et al., [2018](#bib.bib15)).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 在皮肤病变分割文献中经常出现，包括其原始形式（Codella 等人，[2017](#bib.bib97)；Pollastri 等人，[2020](#bib.bib309)；Ramani
    和 Ranjani，[2019](#bib.bib318)）和修改形式（Tang 等人，[2019a](#bib.bib371)；Alom 等人，[2019](#bib.bib22)；Hasan
    等人，[2020](#bib.bib172)），下文将讨论。一些工作介绍了他们自己的模型（Yuan 等人，[2017](#bib.bib433)；Al-Masni
    等人，[2018](#bib.bib15)）。
- en: '![Refer to caption](img/9a4cebf8afb4dde7928bbccb452b7605.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9a4cebf8afb4dde7928bbccb452b7605.png)'
- en: 'Figure 7: The frequency of utilization of different architectural modules in
    the surveyed studies. Shortcut connections, particularly, skip connections ($112$
    papers) and residual connections ($70$ papers) are the two most frequent components
    in DL-based skin lesion segmentation models. Attention mechanisms learn dependencies
    between elements in sequences, either spatially or channel-wise, and are therefore
    used by several encoder-decoder-style segmentation models ($41$ papers). Dilated
    convolutions help expand the receptive field of CNN-models without any additional
    parameters, which is why they are the most popular variant of convolution in the
    surveyed studies ($35$ papers). Finally, papers using Transformers ($12$ papers)
    started appearing from 2021 onwards and are on the rise.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：调查研究中不同建筑模块的使用频率。快捷连接，尤其是跳跃连接（$112$ 篇论文）和残差连接（$70$ 篇论文）是基于深度学习的皮肤病变分割模型中最常见的两个组件。注意力机制学习序列中元素之间的依赖关系，无论是空间上的还是通道上的，因此被多个编码器-解码器风格的分割模型使用（$41$
    篇论文）。扩张卷积有助于扩大卷积神经网络模型的感受野，而无需额外的参数，这也是它们在调查研究中成为最受欢迎的卷积变体的原因（$35$ 篇论文）。最后，从 2021
    年开始，使用 Transformers 的论文（$12$ 篇论文）开始出现并且呈上升趋势。
- en: 3.1.1.1 Shortcut Connections
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.1.1 快捷连接
- en: Connections between early and late layers in FCNs have been widely explored
    to improve both the forward and backward (gradient) information flow in the models,
    facilitating the training. The three most popular types of connections are described
    below.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在全卷积网络（FCNs）中，早期层和晚期层之间的连接已经被广泛探索，以改善模型中前向和后向（梯度）信息的流动，从而促进训练。下面描述了三种最常见的连接类型。
- en: 'Residual connections: Creating non-linear blocks that add their unmodified
    inputs to their outputs (He et al., [2016](#bib.bib174)) alleviates gradient degradation
    in very deep networks. It provides a direct path for the gradient to flow through
    to the early layers of the network, while still allowing for very deep models.
    The technique appears often in skin lesion segmentation, in the implementation
    of the encoder (Sarker et al., [2018](#bib.bib342); Baghersalimi et al., [2019](#bib.bib35);
    Yu et al., [2017a](#bib.bib431)) or both encoder and decoder (He et al., [2017](#bib.bib175);
    Venkatesh et al., [2018](#bib.bib393); Li et al., [2018a](#bib.bib243); Tu et al.,
    [2019](#bib.bib383); Zhang et al., [2019a](#bib.bib437); He et al., [2018](#bib.bib176);
    Xue et al., [2018](#bib.bib423)). Residual connections have also appeared in recurrent
    units (Alom et al., [2019](#bib.bib22), [2020](#bib.bib21)), dense blocks (Song
    et al., [2019](#bib.bib359)), chained pooling (He et al., [2017](#bib.bib175);
    Li et al., [2018a](#bib.bib243); He et al., [2018](#bib.bib176)), and 1-D factorized
    convolutions (Singh et al., [2019](#bib.bib355)).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接：创建非线性块，将其未修改的输入添加到其输出中（He et al., [2016](#bib.bib174)），减轻了非常深层网络中的梯度衰减。它为梯度流经网络的早期层提供了直接路径，同时仍允许非常深的模型。这一技术在皮肤病变分割中经常出现，在编码器的实现中（Sarker
    et al., [2018](#bib.bib342); Baghersalimi et al., [2019](#bib.bib35); Yu et al.,
    [2017a](#bib.bib431)）或在编码器和解码器中均有应用（He et al., [2017](#bib.bib175); Venkatesh
    et al., [2018](#bib.bib393); Li et al., [2018a](#bib.bib243); Tu et al., [2019](#bib.bib383);
    Zhang et al., [2019a](#bib.bib437); He et al., [2018](#bib.bib176); Xue et al.,
    [2018](#bib.bib423)）。残差连接也出现在递归单元中（Alom et al., [2019](#bib.bib22), [2020](#bib.bib21)），密集块中（Song
    et al., [2019](#bib.bib359)），链式池化中（He et al., [2017](#bib.bib175); Li et al.,
    [2018a](#bib.bib243); He et al., [2018](#bib.bib176)），以及 1-D 因式分解卷积中（Singh et
    al., [2019](#bib.bib355)）。
- en: Skip connections appear in encoder-decoder architectures, connecting high-resolution
    features from the encoder’s contracting path to the semantic features on the decoder’s
    expanding path (Ronneberger et al., [2015](#bib.bib328)). These connections help
    preserve localization, especially near region boundaries, and combine multi-scale
    features, resulting in sharper boundaries in the predicted segmentation. Skip
    connections are very popular in skin lesion segmentation because they are effective
    and easy to implement (Zhang et al., [2019a](#bib.bib437); Baghersalimi et al.,
    [2019](#bib.bib35); Song et al., [2019](#bib.bib359); Wei et al., [2019](#bib.bib412);
    Venkatesh et al., [2018](#bib.bib393); Azad et al., [2019](#bib.bib29); He et al.,
    [2017](#bib.bib175); Alom et al., [2019](#bib.bib22); Sarker et al., [2018](#bib.bib342);
    Zeng and Zheng, [2018](#bib.bib436); Li et al., [2018a](#bib.bib243); Tu et al.,
    [2019](#bib.bib383); Yu et al., [2017a](#bib.bib431); Singh et al., [2019](#bib.bib355);
    He et al., [2018](#bib.bib176); Xue et al., [2018](#bib.bib423); Alom et al.,
    [2020](#bib.bib21); Vesal et al., [2018b](#bib.bib395); Liu et al., [2019b](#bib.bib258)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃连接出现在编码器-解码器架构中，将编码器收缩路径中的高分辨率特征连接到解码器扩展路径上的语义特征（Ronneberger et al., [2015](#bib.bib328)）。这些连接有助于保持定位，特别是在区域边界附近，并结合多尺度特征，从而使预测的分割边界更为清晰。跳跃连接在皮肤病变分割中非常受欢迎，因为它们有效且易于实现（Zhang
    et al., [2019a](#bib.bib437); Baghersalimi et al., [2019](#bib.bib35); Song et
    al., [2019](#bib.bib359); Wei et al., [2019](#bib.bib412); Venkatesh et al., [2018](#bib.bib393);
    Azad et al., [2019](#bib.bib29); He et al., [2017](#bib.bib175); Alom et al.,
    [2019](#bib.bib22); Sarker et al., [2018](#bib.bib342); Zeng and Zheng, [2018](#bib.bib436);
    Li et al., [2018a](#bib.bib243); Tu et al., [2019](#bib.bib383); Yu et al., [2017a](#bib.bib431);
    Singh et al., [2019](#bib.bib355); He et al., [2018](#bib.bib176); Xue et al.,
    [2018](#bib.bib423); Alom et al., [2020](#bib.bib21); Vesal et al., [2018b](#bib.bib395);
    Liu et al., [2019b](#bib.bib258)）。
- en: Dense connections expand the convolutional layers by connecting each layer to
    all its subsequent layers, concatenating their features (Huang et al., [2017](#bib.bib182)).
    Iterative reuse of features in dense connections maximizes information flow forward
    and backward. Similar to deep supervision (Section [3.2.5](#S3.SS2.SSS5 "3.2.5
    Deep Supervision Loss ‣ 3.2 Loss Functions ‣ 3 Model Design and Training ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), the gradient is propagated backwards
    directly through all previous layers. Several works (Zeng and Zheng, [2018](#bib.bib436);
    Song et al., [2019](#bib.bib359); Li et al., [2021c](#bib.bib247); Tu et al.,
    [2019](#bib.bib383); Vesal et al., [2018b](#bib.bib395)) integrated dense blocks
    in both the encoder and the decoder. Baghersalimi et al. ([2019](#bib.bib35)),
    Hasan et al. ([2020](#bib.bib172)) and Wei et al. ([2019](#bib.bib412)) used multiple
    dense blocks iteratively in only the encoder, while Li et al. ([2018a](#bib.bib243))
    proposed dense deconvolutional blocks to reuse features from the previous layers.
    Azad et al. ([2019](#bib.bib29)) encoded densely connected convolutions into the
    bottleneck of their encoder-decoder to obtain better features.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密连接通过将每一层与其所有后续层连接，扩展了卷积层，串联了它们的特征（Huang 等人，[2017](#bib.bib182)）。在稠密连接中，特征的迭代重用最大化了信息的前向和反向流动。类似于深度监督（第[3.2.5](#S3.SS2.SSS5
    "3.2.5 Deep Supervision Loss ‣ 3.2 Loss Functions ‣ 3 Model Design and Training
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")节），梯度直接通过所有前面的层向后传播。几项工作（Zeng
    和 Zheng，[2018](#bib.bib436)；Song 等人，[2019](#bib.bib359)；Li 等人，[2021c](#bib.bib247)；Tu
    等人，[2019](#bib.bib383)；Vesal 等人，[2018b](#bib.bib395)）在编码器和解码器中集成了稠密块。Baghersalimi
    等人（[2019](#bib.bib35)），Hasan 等人（[2020](#bib.bib172)）和 Wei 等人（[2019](#bib.bib412)）仅在编码器中迭代使用了多个稠密块，而
    Li 等人（[2018a](#bib.bib243)）提出了稠密反卷积块以重用来自前面层的特征。Azad 等人（[2019](#bib.bib29)）将稠密连接的卷积编码到其编码器-解码器的瓶颈中，以获得更好的特征。
- en: 3.1.1.2 Convolutional Modules
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.1.2 卷积模块
- en: As mentioned earlier, convolution not only provides a structural advantage,
    respecting the local connectivity structure of images in the output futures, but
    also dramatically improves parameter sharing since the parameters of a relatively
    small convolutional kernel are shared by all patches of a large image. Convolution
    is a critical element of deep segmentation models. In this section, we discuss
    some new convolution variants, which have enhanced and diversified this operation,
    appearing in the skin lesion segmentation literature.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，卷积不仅提供了结构上的优势，尊重了图像输出特征的局部连通结构，而且显著提高了参数共享，因为相对较小的卷积核的参数被大图像的所有补丁共享。卷积是深度分割模型的关键元素。在本节中，我们讨论了一些新的卷积变体，它们增强和多样化了这一操作，这些变体在皮肤病变分割文献中出现。
- en: 'Dilated convolution: In contrast to requiring full-resolution outputs in dense
    prediction networks, pooling and striding operations have been adopted in deep
    convolutional neural networks (DCNNs) to increase the receptive field and diminish
    the spatial resolution of feature maps. Dilated or atrous convolutions are designed
    specifically for the semantic segmentation task to exponentially expand the receptive
    fields while keeping the number of parameters constant (Yu and Koltun, [2016](#bib.bib430)).
    Dilated convolutions are convolutional modules with upsampled filters containing
    zeros between consecutive filter values. Sarker et al. ([2018](#bib.bib342)) and
    Jiang et al. ([2019](#bib.bib203)) utilized dilated residual blocks in the encoder
    to control the image field-of-view explicitly and incorporated multi-scale contextual
    information into the segmentation network. SkinNet (Vesal et al., [2018b](#bib.bib395))
    used dilated convolutions at the lower level of the network to enlarge the field-of-view
    and capture non-local information. Liu et al. ([2019b](#bib.bib258)) introduced
    dilated convolutions to the U-Net architecture, significantly improving the segmentation
    performance. Furthermore, different versions of the DeepLab architecture (Chen
    et al., [2017a](#bib.bib86), [b](#bib.bib87), [2018a](#bib.bib88)), which replace
    standard convolutions with dilated ones, have been used in skin lesion segmentation (Goyal
    et al., [2019a](#bib.bib153), [b](#bib.bib154); Cui et al., [2019](#bib.bib105);
    Chen et al., [2018b](#bib.bib90); Canalini et al., [2019](#bib.bib70)).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积：与密集预测网络中需要全分辨率输出的要求相对，深度卷积神经网络（DCNNs）中采用了池化和步幅操作，以增加感受野并降低特征图的空间分辨率。膨胀或空洞卷积专门设计用于语义分割任务，以在保持参数数量不变的同时，指数级地扩展感受野（Yu
    and Koltun, [2016](#bib.bib430)）。膨胀卷积是卷积模块，其中的过滤器在连续的过滤值之间包含零。Sarker et al.（[2018](#bib.bib342)）和Jiang
    et al.（[2019](#bib.bib203)）在编码器中利用了膨胀残差块，以显式控制图像视野，并将多尺度上下文信息整合到分割网络中。SkinNet（Vesal
    et al., [2018b](#bib.bib395)）在网络的低层使用了膨胀卷积，以扩大视野并捕捉非局部信息。Liu et al.（[2019b](#bib.bib258)）将膨胀卷积引入U-Net架构，显著提高了分割性能。此外，DeepLab架构的不同版本（Chen
    et al., [2017a](#bib.bib86), [b](#bib.bib87), [2018a](#bib.bib88)），将标准卷积替换为膨胀卷积，已被用于皮肤病变分割（Goyal
    et al., [2019a](#bib.bib153), [b](#bib.bib154); Cui et al., [2019](#bib.bib105);
    Chen et al., [2018b](#bib.bib90); Canalini et al., [2019](#bib.bib70)）。
- en: 'Separable convolution: Separable convolution or depth-wise separable convolution (Chollet,
    [2017](#bib.bib93)) is a spatial convolution operation that convolves each input
    channel with its corresponding kernel. This is followed by a $1\times 1$ standard
    convolution to capture the channel-wise dependencies in the output of depth-wise
    convolution. Depth-wise convolutions are designed to reduce the number of parameters
    and the computation of standard convolutions while maintaining the accuracy. DSNet (Hasan
    et al., [2020](#bib.bib172)) and separable-Unet (Tang et al., [2019a](#bib.bib371))
    utilized depth-wise separable convolutions in the model to have a lightweight
    network with a reduced number of parameters. Adopted from the DeepLab architecture,
    Goyal et al. ([2019b](#bib.bib154)), Cui et al. ([2019](#bib.bib105)) and, Canalini
    et al. ([2019](#bib.bib70)) incorporated depth-wise separable convolutions in
    conjunction with dilated convolution to improve the speed and accuracy of dense
    predictions.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 可分离卷积：可分离卷积或深度可分离卷积（Chollet, [2017](#bib.bib93)）是一种空间卷积操作，它将每个输入通道与其对应的卷积核进行卷积。接下来使用$1\times
    1$标准卷积来捕捉深度卷积输出中的通道间依赖关系。深度卷积旨在减少标准卷积的参数数量和计算量，同时保持准确性。DSNet（Hasan et al., [2020](#bib.bib172)）和可分离-Unet（Tang
    et al., [2019a](#bib.bib371)）在模型中利用了深度可分离卷积，以实现参数减少的轻量化网络。Goyal et al.（[2019b](#bib.bib154)）、Cui
    et al.（[2019](#bib.bib105)）和Canalini et al.（[2019](#bib.bib70)）从DeepLab架构中采纳了深度可分离卷积，并与膨胀卷积结合，以提高密集预测的速度和准确性。
- en: 'Global convolution: State-of-the-art segmentation models remove densely connected
    and global pooling layers to preserve spatial information required for full-resolution
    output recovery. As a result, by keeping high-resolution feature maps, segmentation
    models become more suitable for localization and, in contrast, less suitable for
    per-pixel classification, which needs transformation invariant features. To increase
    the connectivity between feature maps and classifiers, large convolutional kernels
    should be adopted. However, such kernels have a large number of parameters, which
    renders them computationally expensive. To tackle this, global convolutional network
    (GCN) modules adopt a combination of symmetric parallel convolutions in the form
    of $1\times k+k\times 1$ and $k\times 1+1\times k$ to cover a $k\times k$ area
    of feature maps (Peng et al., [2017b](#bib.bib303)). SeGAN (Xue et al., [2018](#bib.bib423))
    employed GCN modules with large kernels in the generator’s decoder to reconstruct
    segmentation masks and in the discriminator architecture to optimally capture
    a larger receptive field.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 全局卷积：最先进的分割模型去除了密集连接和全局池化层，以保留恢复全分辨率输出所需的空间信息。因此，通过保持高分辨率特征图，分割模型变得更加适合定位，但相比之下，不适合需要变换不变特征的逐像素分类。为了增加特征图与分类器之间的连接性，应采用较大的卷积核。然而，这些卷积核具有大量参数，从而使其计算开销较大。为了解决这个问题，全局卷积网络（GCN）模块采用了对称并行卷积的组合形式，如$1\times
    k+k\times 1$和$k\times 1+1\times k$，以覆盖特征图的$k\times k$区域（Peng et al., [2017b](#bib.bib303)）。SeGAN（Xue
    et al., [2018](#bib.bib423)）在生成器的解码器中使用了带有大卷积核的GCN模块来重建分割掩膜，并在判别器架构中使用这些模块以最佳地捕获更大的感受野。
- en: 'Factorized convolution: Factorized convolutions (Wang et al., [2017](#bib.bib400))
    are designed to reduce the number of convolution filter parameters as well as
    the computation time through kernel decomposition when a high-dimensional kernel
    is substituted with a sequence of lower-dimensional convolutions. Additionally,
    by adding non-linearity between the composited kernels, the network’s capacity
    may improve. FCA-Net (Singh et al., [2019](#bib.bib355)) and MobileGAN (Sarker
    et al., [2019](#bib.bib341)) utilized residual 1-D factorized convolutions (a
    sequence of $k\times 1$ and $1\times k$ convolutions with ReLU non-linearity)
    in their segmentation architecture.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 分解卷积：分解卷积（Wang et al., [2017](#bib.bib400)）旨在通过用一系列低维卷积代替高维卷积核，从而减少卷积滤波器参数数量和计算时间。此外，通过在组合卷积核之间添加非线性，网络的容量可能会提高。FCA-Net（Singh
    et al., [2019](#bib.bib355)）和MobileGAN（Sarker et al., [2019](#bib.bib341)）在其分割架构中使用了残差1-D分解卷积（一个序列的$k\times
    1$和$1\times k$卷积，带有ReLU非线性）。
- en: 3.1.1.3 Multi-scale Modules
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.1.3 多尺度模块
- en: In FCNs, taking semantic context into account when assigning per-pixel labels
    leads to a more accurate prediction (Long et al., [2015](#bib.bib264)). Exploiting
    multi-scale contextual information, effectively combining them as well as encoding
    them in deep semantic segmentation have been widely explored.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在全卷积网络（FCNs）中，考虑语义上下文以进行逐像素标签分配可以得到更准确的预测（Long et al., [2015](#bib.bib264)）。利用多尺度上下文信息、有效地将它们组合起来以及在深度语义分割中编码这些信息已经被广泛探索。
- en: 'Image Pyramid: RefineNet (He et al., [2017](#bib.bib175)) and its extension (He
    et al., [2018](#bib.bib176)), MSFCDN (Zeng and Zheng, [2018](#bib.bib436)), FCA-Net (Singh
    et al., [2019](#bib.bib355)), and Abraham and Khan ([2019](#bib.bib9)) fed a pyramid
    of multi-resolution skin lesion images as input to their deep segmentation network
    to extract multi-scale discriminative features. RefineNet (He et al., [2017](#bib.bib175),
    [2018](#bib.bib176)), Factorized channel attention network (FCA-Net (Singh et al.,
    [2019](#bib.bib355))) and Abraham and Khan ([2019](#bib.bib9)) applied convolutional
    blocks to different image resolutions in parallel to generate features which are
    then up-sampled in order to fuse multi-scale feature maps. Multi-scale fully convolutional
    DenseNets (MSFCDN (Zeng and Zheng, [2018](#bib.bib436))) gradually integrated
    multi-scale features extracted from the image pyramid into the encoder’s down-sampling
    path. Also,  Jafari et al. ([2016](#bib.bib194), [2017](#bib.bib195)) extracted
    multi-scale patches from clinical images to predict semantic labels and refine
    lesion boundaries by deploying local and global information. While aggregating
    the feature maps computed at various image scales improves the segmentation performance,
    it also increases the computational cost of the network.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图像金字塔：RefineNet （He 等，[2017](#bib.bib175)）及其扩展（He 等，[2018](#bib.bib176)），MSFCDN （Zeng
    和 Zheng，[2018](#bib.bib436)），FCA-Net （Singh 等，[2019](#bib.bib355)），以及 Abraham
    和 Khan（[2019](#bib.bib9)）将多分辨率皮肤病变图像的金字塔作为输入提供给他们的深度分割网络，以提取多尺度的判别特征。RefineNet （He
    等，[2017](#bib.bib175)，[2018](#bib.bib176)），分解通道注意力网络（FCA-Net （Singh 等，[2019](#bib.bib355)））和
    Abraham 与 Khan（[2019](#bib.bib9)）将卷积块并行应用于不同图像分辨率，以生成特征，然后通过上采样将这些特征融合成多尺度特征图。多尺度全卷积DenseNets（MSFCDN （Zeng
    和 Zheng，[2018](#bib.bib436)））将从图像金字塔中提取的多尺度特征逐渐整合到编码器的下采样路径中。此外，Jafari 等（[2016](#bib.bib194)，[2017](#bib.bib195)）从临床图像中提取多尺度的补丁，以预测语义标签，并通过使用局部和全局信息来细化病变边界。虽然在各种图像尺度下聚合计算得到的特征图可以提高分割性能，但也增加了网络的计算成本。
- en: 'Parallel multi-scale convolutions: Alternatively, given a single image resolution,
    multiple convolutional filters with different kernel sizes (Zhang et al., [2019a](#bib.bib437);
    Wang et al., [2019a](#bib.bib397); Jahanifar et al., [2018](#bib.bib196)) or multiple
    dilated convolutions with different dilation rates (Goyal et al., [2019a](#bib.bib153),
    [b](#bib.bib154); Cui et al., [2019](#bib.bib105); Chen et al., [2018b](#bib.bib90);
    Canalini et al., [2019](#bib.bib70)) can be adopted in parallel paths to extract
    multi-scale contextual features from images. DSM (Zhang et al., [2019a](#bib.bib437))
    integrated multi-scale convolutional blocks into the skip connections of an encoder-decoder
    structure to handle different lesion sizes. Wang et al. ([2019a](#bib.bib397))
    utilized multi-scale convolutional branches in the bottleneck of an encoder-decoder
    architecture, followed by attention modules to selectively aggregate the extracted
    multi-scale features.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 并行多尺度卷积：另一种方法是，在给定单一图像分辨率的情况下，可以采用具有不同核大小的多个卷积滤波器（Zhang 等，[2019a](#bib.bib437)；Wang
    等，[2019a](#bib.bib397)；Jahanifar 等，[2018](#bib.bib196)），或具有不同膨胀率的多个膨胀卷积（Goyal
    等，[2019a](#bib.bib153)，[b](#bib.bib154)；Cui 等，[2019](#bib.bib105)；Chen 等，[2018b](#bib.bib90)；Canalini
    等，[2019](#bib.bib70)）在并行路径中提取图像的多尺度上下文特征。DSM （Zhang 等，[2019a](#bib.bib437)）将多尺度卷积块集成到编码器-解码器结构的跳跃连接中，以处理不同的病变大小。Wang
    等（[2019a](#bib.bib397)）在编码器-解码器架构的瓶颈中利用多尺度卷积分支，接着使用注意力模块选择性地聚合提取的多尺度特征。
- en: 'Pyramid pooling: Another way of incorporating multi-scale information into
    deep segmentation models is to integrate a pyramid pooling (PP) module in the
    network architecture (Zhao et al., [2017](#bib.bib450)). PP fuses a hierarchy
    of features extracted from different sub-regions by adopting parallel pooling
    kernels of various sizes, followed by up-sampling and concatenation to create
    the final feature maps. Sarker et al. ([2018](#bib.bib342)) and Jahanifar et al.
    ([2018](#bib.bib196)) utilized PP in the decoder to benefit from coarse-to-fine
    features extracted by different receptive fields from skin lesion images.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 金字塔池化：另一种将多尺度信息整合到深度分割模型中的方法是将金字塔池化（PP）模块集成到网络架构中（Zhao 等，[2017](#bib.bib450)）。PP通过采用不同大小的并行池化核来融合从不同子区域提取的特征层级，接着进行上采样和拼接，以创建最终的特征图。Sarker
    等（[2018](#bib.bib342)）和 Jahanifar 等（[2018](#bib.bib196)）在解码器中利用 PP 以从皮肤病变图像中提取的不同接收场的粗到细特征中获益。
- en: Dilated convolutions and skip connections are two other types of multi-scale
    information extraction techniques, which are explained in Sections [3.1.1.2](#S3.SS1.SSS1.P2
    "3.1.1.2 Convolutional Modules ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")
    and [3.1.1.1](#S3.SS1.SSS1.P1 "3.1.1.1 Shortcut Connections ‣ 3.1.1 Single Network
    Models ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation"), respectively.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积和跳跃连接是另外两种多尺度信息提取技术，这些在第[3.1.1.2](#S3.SS1.SSS1.P2 "3.1.1.2 卷积模块 ‣ 3.1.1
    单网络模型 ‣ 3.1 架构 ‣ 3 模型设计与训练 ‣ 深度学习在皮肤病变分割中的应用调研")和第[3.1.1.1](#S3.SS1.SSS1.P1 "3.1.1.1
    跳跃连接 ‣ 3.1.1 单网络模型 ‣ 3.1 架构 ‣ 3 模型设计与训练 ‣ 深度学习在皮肤病变分割中的应用调研")节中进行了说明。
- en: 3.1.1.4 Attention Modules
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.1.4 注意力模块
- en: An explicit way to exploit contextual dependencies in the pixel-wise labeling
    task is the self-attention mechanism (Hu et al., [2018](#bib.bib181); Fu et al.,
    [2019](#bib.bib139)). Two types of attention modules capture global dependencies
    in spatial and channel dimensions by integrating features among all positions
    and channels, respectively. Wang et al. ([2019a](#bib.bib397)) and Sarker et al.
    ([2019](#bib.bib341)) leveraged both spatial and channel attention modules to
    recalibrate the feature maps by examining the feature similarity between pairs
    of positions or channels and updating each feature value by a weighted sum of
    all other features. Singh et al. ([2019](#bib.bib355)) utilized a channel attention
    block in the proposed factorized channel attention (FCA) blocks, which was used
    to investigate the correlation of different channel maps for extraction of relevant
    patterns. Inspired by attention U-Net (Oktay et al., [2018](#bib.bib292)), multiple
    works (Abraham and Khan, [2019](#bib.bib9); Song et al., [2019](#bib.bib359);
    Wei et al., [2019](#bib.bib412)) integrated a spatial attention gate in an encoder-decoder
    architecture to combine coarse semantic feature maps and fine localization feature
    maps. Kaul et al. ([2019](#bib.bib215)) proposed FocusNet which utilizes squeeze-and-excitation
    blocks into a hybrid encoder-decoder architecture. Squeeze-and-excitation blocks
    model the channel-wise interdependencies to re-weight feature maps and improve
    their representation power. Experimental results demonstrate that attention modules
    help the network focus on the lesions and suppress irrelevant feature responses
    in the background.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在像素级标注任务中利用上下文依赖的明确方式是自注意力机制（Hu等，[2018](#bib.bib181)；Fu等，[2019](#bib.bib139)）。两种类型的注意力模块通过分别在所有位置和通道之间集成特征，捕捉空间和通道维度的全局依赖性。Wang等（[2019a](#bib.bib397)）和Sarker等（[2019](#bib.bib341)）利用了空间和通道注意力模块，通过检查位置或通道对之间的特征相似性，并通过对所有其他特征的加权和更新每个特征值，从而重新校准特征图。Singh等（[2019](#bib.bib355)）在提出的因式分解通道注意力（FCA）模块中利用了一个通道注意力块，该模块用于研究不同通道图的相关性以提取相关模式。受注意力U-Net（Oktay等，[2018](#bib.bib292)）的启发，多项工作（Abraham和Khan，[2019](#bib.bib9)；Song等，[2019](#bib.bib359)；Wei等，[2019](#bib.bib412)）将空间注意力门集成到编码器-解码器架构中，以结合粗糙的语义特征图和精细的定位特征图。Kaul等（[2019](#bib.bib215)）提出了FocusNet，该网络在混合编码器-解码器架构中利用了压缩-激励块。压缩-激励块对通道之间的相互依赖性进行建模，以重新加权特征图并提高其表示能力。实验结果表明，注意力模块帮助网络聚焦于病变区域并抑制背景中不相关的特征响应。
- en: 3.1.1.5 Recurrent Convolutional Neural Networks
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.1.5 循环卷积神经网络
- en: Recurrent convolutional neural networks (RCNN) integrate recurrent connections
    into convolutional layers by evolving the recurrent input over time (Pinheiro
    and Collobert, [2014](#bib.bib307)). Stacking recurrent convolutional layers (RCL)
    on top of the convolutional layer feature extractors ensures capturing spatial
    and contextual dependencies in images while limiting the network capacity by sharing
    the same set of parameters in RCL blocks. In the application of skin lesion segmentation,
    Attia et al. ([2017](#bib.bib28)) utilized recurrent layers in the decoder to
    capture spatial dependencies between deep-encoded features and recover segmentation
    maps at the original resolution. $\nabla^{N}$-Net (Alom et al., [2020](#bib.bib21)),
    RU-Net, and R2U-Net (Alom et al., [2019](#bib.bib22)) incorporated RCL blocks
    into the FCN architecture to accumulate features across time in a computationally
    efficient way and boosted the skin lesion boundary detection. Azad et al. ([2019](#bib.bib29))
    deployed a non-linear combination of the encoder feature and decoder feature maps
    by adding a bi-convolutional LSTM (BConvLSTM) in skip connections. BConvLSTM consists
    of two independent convolutional LSTM modules (ConvLSTMs) which process the feature
    maps into two directions of backward and forward paths and concatenate their outputs
    to obtain the final output. Modifications to the traditional pooling layers were
    also proposed, using a dense pooling strategy (Nasr-Esfahani et al., [2019](#bib.bib285)).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 循环卷积神经网络（RCNN）通过随时间演变循环输入将循环连接整合到卷积层中（Pinheiro和Collobert，[2014](#bib.bib307)）。将循环卷积层（RCL）叠加在卷积层特征提取器之上，可以在限制网络容量的情况下捕获图像中的空间和上下文依赖关系，并共享RCL块中的同一组参数。在皮肤损伤分割的应用中，Attia等人（[2017](#bib.bib28)）在解码器中使用循环层来捕获深度编码特征之间的空间依赖关系，并在原始分辨率下恢复分割图。$\nabla^{N}$-Net（Alom等人，[2020](#bib.bib21)），RU-Net和R2U-Net（Alom等人，[2019](#bib.bib22)）将RCL块纳入FCN架构中，以一种计算高效的方式累积时间上的特征，并提高了皮肤损伤边界检测。Azad等人（[2019](#bib.bib29)）在跳跃连接中添加了一个双卷积LSTM（BConvLSTM），通过对编码器特征和解码器特征图进行非线性组合。BConvLSTM由两个独立的卷积LSTM模块（ConvLSTM）组成，它们在正向和反向路径上处理特征映射，并将它们的输出连接起来获得最终输出。还提出了对传统池化层的修改，使用了一种稠密池化策略（Nasr-Esfahani等人，[2019](#bib.bib285)）。
- en: 3.1.2 Multiple Network Models
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 多个网络模型
- en: Motivations for models comprising more than one DL sub-model are diverse, ranging
    from alleviating training noise and exploiting a diversity of features learned
    by different models to exploring synergies between multi-task learners. After
    examining the literature (Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Architecture ‣ 3 Model
    Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")),
    we further classified the works in this section into standard ensembles and multi-task
    models. We also discuss generative adversarial models, which are intrinsically
    multi-network models, in a separate category.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 包含多个DL子模型的模型的动机多种多样，从减轻训练噪声和利用不同模型学到的特征的多样性到探索多任务学习器之间的协同效应。在查阅了文献之后（图[6](#S3.F6
    "Figure 6 ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation")），我们进一步将本节中的作品分为标准集成和多任务模型。我们还将内在上是多网络模型的生成对抗模型分为单独的类别。
- en: 3.1.2.1 Standard Ensembles
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.2.1 标准集成
- en: Ensemble models are widely used in machine learning, motivated by the hope that
    the complementarity of different models may lead to more stable combined predictions (Sagi
    and Rokach, [2018](#bib.bib336)). Ensemble performance is contingent on the quality
    and diversity of the component models, which can be combined at the feature level
    (early fusion) or the prediction level (late fusion). The former combines the
    features extracted by the components and learns a meta-model on them, while the
    latter pools or combines the models’ predictions with or without a meta-model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型在机器学习中被广泛应用，这是因为不同模型的互补性可能会导致更稳定的组合预测（Sagi和Rokach，[2018](#bib.bib336)）。集成性能取决于组成模型的质量和多样性，可以在特征级别（早期融合）或预测级别（晚期融合）进行组合。前者结合了组件提取的特征并在其上学习元模型，而后者使用或不使用元模型汇总或组合模型的预测。
- en: All methods discussed in this section employ late fusion, except for an approach
    loosely related to early fusion (Tang et al., [2019a](#bib.bib371)), which explores
    various learning-rate decay schemes, and builds a single model by averaging the
    weights learned at different epochs to bypass poor local minima during training.
    Since the weights correspond to features learned by the convolution filters, this
    approach can be interpreted as feature fusion.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的所有方法都采用了晚期融合，除了一个与早期融合松散相关的方法 (Tang et al., [2019a](#bib.bib371))，该方法探索了各种学习率衰减方案，并通过平均不同训练周期学到的权重来构建单一模型，从而绕过训练中的糟糕局部最小值。由于权重对应于卷积滤波器学习到的特征，因此这种方法可以解释为特征融合。
- en: 'Most works employ a single DL architecture with multiple training routines,
    varying configurations more or less during training (Canalini et al., [2019](#bib.bib70)).
    The changes between component models may involve network hyperparameters: number
    of filters per block and their size (Codella et al., [2017](#bib.bib97)); optimization
    and regularization hyperparameters: learning rate, weight decay (Tan et al., [2019b](#bib.bib370));
    the training set: multiple splits of a training set (Yuan et al., [2017](#bib.bib433);
    Yuan and Lo, [2019](#bib.bib434)), separate models per class (Bi et al., [2019b](#bib.bib52));
    preprocessing: different color spaces (Pollastri et al., [2020](#bib.bib309));
    different pretraining strategies to initialize feature extractors (Canalini et al.,
    [2019](#bib.bib70)); or different ways to initialize the network parameters (Cui
    et al., [2019](#bib.bib105)). Test-time augmentation may also be seen as a form
    of inference-time ensembling (Chen et al., [2018b](#bib.bib90); Liu et al., [2019b](#bib.bib258);
    Jahanifar et al., [2018](#bib.bib196)) that combines the outputs of multiple augmented
    images to generate a more reliable prediction.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数工作使用单一的深度学习架构配合多个训练流程，在训练过程中对配置进行不同程度的调整 (Canalini et al., [2019](#bib.bib70))。组件模型之间的变化可能涉及网络超参数：每个块的滤波器数量及其大小 (Codella
    et al., [2017](#bib.bib97))；优化和正则化超参数：学习率、权重衰减 (Tan et al., [2019b](#bib.bib370))；训练集：训练集的多次分割 (Yuan
    et al., [2017](#bib.bib433); Yuan and Lo, [2019](#bib.bib434))，每个类别的单独模型 (Bi et al.,
    [2019b](#bib.bib52))；预处理：不同的颜色空间 (Pollastri et al., [2020](#bib.bib309))；不同的预训练策略以初始化特征提取器 (Canalini
    et al., [2019](#bib.bib70))；或不同的网络参数初始化方法 (Cui et al., [2019](#bib.bib105))。测试时增强也可以被视为一种推理时集成方法 (Chen
    et al., [2018b](#bib.bib90); Liu et al., [2019b](#bib.bib258); Jahanifar et al.,
    [2018](#bib.bib196))，它结合了多个增强图像的输出以生成更可靠的预测。
- en: Bi et al. ([2019b](#bib.bib52)) trained a separate DL model for each class,
    as well as a separate classification model. For inference, the classification
    model output is used to weight the outputs of the category-specific segmentation
    networks. In contrast, Soudani and Barhoumi ([2019](#bib.bib361)) trained a meta
    “recommender” model to dynamically choose, for each input, a segmentation technique
    from the top five scorers in the ISIC 2017 challenge, although their proposition
    was validated on a very small test set ($10\%$ of ISIC 2017 test set).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Bi et al. ([2019b](#bib.bib52)) 为每个类别训练了一个单独的深度学习模型，以及一个单独的分类模型。对于推理，分类模型的输出用于加权类别特定分割网络的输出。相比之下，Soudani
    和 Barhoumi ([2019](#bib.bib361)) 训练了一个元“推荐”模型，动态选择每个输入的分割技术，从 ISIC 2017 挑战中前五名得分者中选择，尽管他们的提议仅在一个非常小的测试集上进行了验证（ISIC
    2017 测试集的 $10\%$）。
- en: Several works also ensemble different model architectures for skin lesion segmentation.
    Goyal et al. ([2019b](#bib.bib154)) investigate multiple fusion approaches to
    avoid severe errors from individual models, comparing the average-, maximum- and
    minimum-pooling of their outputs. A common assumption is that the component models
    of the ensemble are trained independently, but Bi et al. ([2017b](#bib.bib53))
    cascaded the component models, i.e., used the output of one model as the input
    of the next (in association with the actual image input). Thus, each model attempts
    to refine the segmentation obtained by the previous one. They consider not only
    the final model output, but all the outputs in the cascade, making the technique
    a legitimate ensemble.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究还通过集成不同的模型架构来进行皮肤病变分割。Goyal et al. ([2019b](#bib.bib154)) 探讨了多种融合方法以避免个别模型的严重错误，比较了其输出的平均池化、最大池化和最小池化。一个常见的假设是，集成的组件模型是独立训练的，但
    Bi et al. ([2017b](#bib.bib53)) 将组件模型级联，即使用一个模型的输出作为下一个模型的输入（结合实际图像输入）。因此，每个模型尝试细化前一个模型获得的分割结果。他们不仅考虑最终模型的输出，还考虑级联中的所有输出，使该技术成为合法的集成。
- en: 3.1.2.2 Multi-task Models
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.2.2 多任务模型
- en: Multi-task models jointly address more than one goal, in the hope that synergies
    among the tasks will improve overall performance (Zhang and Yang, [2022](#bib.bib446)).
    This can be particularly helpful in medical image analysis, wherein aggregating
    tasks may alleviate the issue of insufficient data or annotations. For skin lesions,
    a few multi-task models exploiting segmentation and classification have been proposed (Chen
    et al., [2018b](#bib.bib90); Li and Shen, [2018](#bib.bib251); Yang et al., [2018](#bib.bib426);
    Xie et al., [2020b](#bib.bib420); Jin et al., [2021](#bib.bib206)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务模型共同解决多个目标，希望任务之间的协同作用能够提高整体性能 (Zhang 和 Yang, [2022](#bib.bib446))。这在医学图像分析中尤其有帮助，其中任务的聚合可能会缓解数据或标注不足的问题。针对皮肤病变，已经提出了一些利用分割和分类的多任务模型
    (Chen 等, [2018b](#bib.bib90); Li 和 Shen, [2018](#bib.bib251); Yang 等, [2018](#bib.bib426);
    Xie 等, [2020b](#bib.bib420); Jin 等, [2021](#bib.bib206))。
- en: The synergy between tasks may appear when their models share common relevant
    features. Li and Shen ([2018](#bib.bib251)) assume that all features are shareable
    between the tasks, and train a single fully convolutional residual network to
    assign class probabilities at the pixel level. They aggregate the class probability
    maps to estimate both lesion region and class by weighted averaging of probabilities
    for different classes inside the lesion area. Yang et al. ([2018](#bib.bib426))
    learn an end-to-end model formed by a shared convolutional feature extractor followed
    by three task-specific branches (one to segment skin lesions, one to classify
    them as melanoma versus non-melanoma, and one to classify them as seborrheic keratosis
    versus non-seborrheic keratosis.) Similarly, Chen et al. ([2018b](#bib.bib90))
    add a common feature extractor and separate task heads, and introduce a learnable
    gate function that controls the flow of information between the tasks to model
    the latent relationship between two tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务的模型共享共同的相关特征时，任务之间的协同效应可能会出现。Li 和 Shen ([2018](#bib.bib251)) 认为所有特征都可以在任务之间共享，并训练一个单一的全卷积残差网络来在像素级别上分配类别概率。他们通过对病变区域内不同类别的概率进行加权平均，来估计病变区域和类别。Yang
    等 ([2018](#bib.bib426)) 学习了一个端到端的模型，该模型由一个共享的卷积特征提取器和三个特定任务的分支组成（一个用于分割皮肤病变，一个用于将其分类为黑色素瘤与非黑色素瘤，另一个用于将其分类为脂溢性角化症与非脂溢性角化症）。类似地，Chen
    等 ([2018b](#bib.bib90)) 添加了一个共同的特征提取器和分开的任务头，并引入了一个可学习的门控函数来控制任务之间信息流动，以建模两个任务之间的潜在关系。
- en: Instead of using a single architecture for classification and segmentation,
    Xie et al. ([2020b](#bib.bib420)) and Jin et al. ([2021](#bib.bib206)) use three
    CNNs in sequence to perform a coarse segmentation, followed by classification
    and, finally, a fine segmentation. Instead of shared features, these works exploit
    sequential guidance, in which the output of each task improves the learning of
    the next. While Xie et al. ([2020b](#bib.bib420)) feed the output of each network
    to the next, assuming that the classification network is a diagnostic category
    and a class activation map (Zhou et al., [2016](#bib.bib454)), Jin et al. ([2021](#bib.bib206))
    introduce feature entanglement modules, which aggregate features learned by different
    networks.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Xie 等 ([2020b](#bib.bib420)) 和 Jin 等 ([2021](#bib.bib206)) 并没有使用单一的架构来进行分类和分割，而是依次使用三个
    CNN 来进行粗略分割、分类，最后进行精细分割。这些工作并没有使用共享特征，而是利用了顺序指导，其中每个任务的输出改善了下一个任务的学习。虽然 Xie 等
    ([2020b](#bib.bib420)) 将每个网络的输出传递给下一个，假设分类网络是诊断类别和类别激活图 (Zhou 等, [2016](#bib.bib454))，但
    Jin 等 ([2021](#bib.bib206)) 引入了特征纠缠模块，这些模块汇总了不同网络学习到的特征。
- en: All multi-task models discussed so far have results suggesting complementarity
    between classification and segmentation, but there is no clear advantage among
    these models. The segmentation of dermoscopic features (e.g., networks, globules,
    regression areas) combined with the other tasks is a promising avenue of research,
    which could bridge classification and segmentation, by fostering the extraction
    of features that “see” the lesion as human specialists do.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的所有多任务模型的结果都表明分类和分割之间具有互补性，但这些模型之间没有明显的优势。将皮肤镜特征（例如网络、珠粒、回归区域）的分割与其他任务结合起来是一个有前景的研究方向，这可以通过促进提取能够“像人类专家一样”看待病变的特征来弥合分类和分割之间的差距。
- en: We do not consider in the hybrid group, two-stage models in which segmentation
    is used as ancillary preprocessing to classification (Yu et al., [2017a](#bib.bib431);
    Codella et al., [2017](#bib.bib97); Gonzalez-Diaz, [2018](#bib.bib151); Al-Masni
    et al., [2020](#bib.bib17)), since without mutual influence (sharing of losses
    or features) or feedback between the two tasks, there is no opportunity for synergy.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不考虑在混合组中，将分割作为辅助预处理来进行分类的两阶段模型（Yu 等人，[2017a](#bib.bib431)；Codella 等人，[2017](#bib.bib97)；Gonzalez-Diaz，[2018](#bib.bib151)；Al-Masni
    等人，[2020](#bib.bib17)），因为在没有两个任务之间的相互影响（损失或特征共享）或反馈的情况下，没有协同作用的机会。
- en: Vesal et al. ([2018a](#bib.bib394)) stressed the importance of object localization
    as an ancillary task for lesion delineation, in particular deploying Faster-RCNN (Ren
    et al., [2015](#bib.bib323)) to regress a bounding box to crop the lesions before
    training a SkinNet segmentation model. While this two-stage approach considerably
    improves the results, it is computationally expensive (a fast non-DL-based bounding
    box detection algorithm was proposed earlier by Celebi et al. ([2009a](#bib.bib75))).
    Goyal et al. ([2019a](#bib.bib153)) employed ROI detection with a deep extreme
    cut to extract the extreme points of lesions (leftmost, rightmost, topmost, bottommost
    pixels) and feed them, in a new auxiliary channel, to a segmentation model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Vesal 等人（[2018a](#bib.bib394)）强调了物体定位作为病变描绘辅助任务的重要性，特别是部署 Faster-RCNN（Ren 等人，[2015](#bib.bib323)）来回归一个边界框以裁剪病变，然后再训练
    SkinNet 分割模型。虽然这种两阶段的方法显著提高了结果，但其计算开销较大（Celebi 等人（[2009a](#bib.bib75)）早期提出了一种快速的非深度学习边界框检测算法）。Goyal
    等人（[2019a](#bib.bib153)）采用了 ROI 检测和深度极值切割来提取病变的极值点（最左、最右、最上、最下像素），并将这些点作为新的辅助通道输入到分割模型中。
- en: 3.1.2.3 Generative Adversarial Models
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.2.3 生成对抗模型
- en: We discussed GANs for synthesizing new samples, their main use in skin lesion
    analysis, in Section [2.2](#S2.SS2 "2.2 Synthetic Data Generation ‣ 2 Input Data
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation"). In this section, we
    are interested in GANs not for generating additional training samples, but for
    directly providing enhanced segmentation models. Adversarial training encourages
    high-order consistency in predicted segmentation by implicitly looking into the
    joint distribution of class labels and ground-truth segmentation masks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[2.2](#S2.SS2 "2.2 Synthetic Data Generation ‣ 2 Input Data ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")节讨论了用于合成新样本的 GANs 及其在皮肤病变分析中的主要用途。在本节中，我们关注的
    GANs 不是用于生成额外的训练样本，而是用于直接提供增强的分割模型。对抗训练通过隐式地观察类别标签和地面真实分割掩码的联合分布，鼓励预测分割的一致性。
- en: Peng et al. ([2019](#bib.bib304)), Tu et al. ([2019](#bib.bib383)), Lei et al.
    ([2020](#bib.bib241)), and Izadi et al. ([2018](#bib.bib190)) use a U-Net-like
    generator that takes a dermoscopic image as input, and outputs the corresponding
    segmentation, while the discriminator is a traditional CNN which attempts to discriminate
    pairs of image and generated segmentation from pairs of image and ground-truth.
    The generator has to learn to correctly segment the lesion in order to fool the
    discriminator. Jiang et al. ([2019](#bib.bib203)) use the same scheme, with a
    dual discriminator. Lei et al. ([2020](#bib.bib241)) also employ a second discriminator
    that takes as input only segmentations (unpaired from input images).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Peng 等人（[2019](#bib.bib304)），Tu 等人（[2019](#bib.bib383)），Lei 等人（[2020](#bib.bib241)）和
    Izadi 等人（[2018](#bib.bib190)）使用了类似 U-Net 的生成器，该生成器以皮肤镜图像作为输入，输出相应的分割结果，而判别器则是一个传统的
    CNN，它尝试区分图像与生成的分割对和图像与地面真实值对。生成器必须学会正确地分割病变，以欺骗判别器。Jiang 等人（[2019](#bib.bib203)）使用了相同的方案，但增加了一个双重判别器。Lei
    等人（[2020](#bib.bib241)）还使用了一个第二个判别器，该判别器仅以分割图像作为输入（与输入图像无配对关系）。
- en: Since the discriminator may trivially learn to recognize the generated masks
    due to the presence of continuous probabilities, instead of the sharp discrete
    boundaries of the ground-truths, Wei et al. ([2019](#bib.bib412)) and Tu et al.
    ([2019](#bib.bib383)) address this by pre-multiplying both the generated and real
    segmentations with the (normalized) input images before feeding them to the discriminator.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于判别器可能会因为生成掩码的连续概率而容易识别这些掩码，而不是地面真实值的明确离散边界，Wei 等人（[2019](#bib.bib412)）和 Tu
    等人（[2019](#bib.bib383)）通过在将生成的和真实的分割图像输入到判别器之前，先将它们与（归一化的）输入图像进行预乘来解决这个问题。
- en: We discuss adversarial loss functions further in Section [3.2.8](#S3.SS2.SSS8
    "3.2.8 Adversarial Loss ‣ 3.2 Loss Functions ‣ 3 Model Design and Training ‣ A
    Survey on Deep Learning for Skin Lesion Segmentation").
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第[3.2.8节](#S3.SS2.SSS8 "3.2.8 对抗损失 ‣ 3.2 损失函数 ‣ 3 模型设计与训练 ‣ 深度学习在皮肤病变分割中的综述")进一步讨论对抗损失函数。
- en: 3.1.3 Hybrid Feature Models
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 混合特征模型
- en: Although the major strength of CNNs is their ability to learn meaningful image
    features without human intervention, a few works tried to combine the best of
    both worlds, with strategies ranging from employing pre- or postprocessing to
    enforce prior knowledge to adding hand-crafted features. Providing the model with
    prior knowledge about the expected shape of skin lesions—which is missing from
    CNNs—may improve the performance. Mirikharaji and Hamarneh ([2018](#bib.bib279))
    encode shape information into an additional regularization loss, which penalizes
    segmentation maps that deviate from a star-shaped prior (Section [3.2.6](#S3.SS2.SSS6
    "3.2.6 Star-Shape Loss ‣ 3.2 Loss Functions ‣ 3 Model Design and Training ‣ A
    Survey on Deep Learning for Skin Lesion Segmentation")).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CNN的主要优势在于其能够在没有人工干预的情况下学习有意义的图像特征，但一些研究尝试结合两者的优点，策略包括使用前处理或后处理来施加先验知识，或者添加手工制作的特征。向模型提供关于皮肤病变预期形状的先验知识——这是CNN所缺乏的——可能会提高性能。Mirikharaji和Hamarneh（[2018](#bib.bib279)）将形状信息编码到额外的正则化损失中，惩罚偏离星形先验的分割图（第[3.2.6节](#S3.SS2.SSS6
    "3.2.6 星形损失 ‣ 3.2 损失函数 ‣ 3 模型设计与训练 ‣ 深度学习在皮肤病变分割中的综述")）。
- en: Conditional random fields (CRFs) use pixel-level color information models to
    refine the segmentation masks output by the CNN. While both Tschandl et al. ([2019](#bib.bib382))
    and Adegun and Viriri ([2020b](#bib.bib12)) consider a single CNN, Qiu et al.
    ([2020](#bib.bib312)) combine the outputs of multiple CNNs into a single mask,
    before feeding it together with the input image to the CRFs. Ünver and Ayan ([2019](#bib.bib385))
    use GrabCut (Rother et al., [2004](#bib.bib332)) to obtain the segmentation mask
    given the dermoscopy image and a region proposal obtained by the YOLO (Redmon
    et al., [2016](#bib.bib322)) network. These methods regularize the CNN segmentation,
    which is mainly based on textural patterns, with expected priors based on the
    color of the pixels.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 条件随机场（CRFs）使用像素级颜色信息模型来细化CNN输出的分割掩码。虽然Tschandl等（[2019](#bib.bib382)）和Adegun与Viriri（[2020b](#bib.bib12)）考虑了单个CNN，但Qiu等（[2020](#bib.bib312)）将多个CNN的输出组合成一个掩码，然后将其与输入图像一起输入到CRFs中。Ünver和Ayan（[2019](#bib.bib385)）使用GrabCut（Rother等，[2004](#bib.bib332)）来获取给定皮肤镜图像和通过YOLO（Redmon等，[2016](#bib.bib322)）网络获得的区域提案的分割掩码。这些方法通过基于像素颜色的期望先验来正则化主要基于纹理模式的CNN分割。
- en: Works that combine hand-crafted features with CNNs follow two distinct approaches.
    The first consists of pre-filtering the input images to increase the contrast
    between the lesion and the surrounding skin. Techniques explored include local
    binary patterns (LBPs) (Ross-Howe and Tizhoosh, [2018](#bib.bib329); Jayapriya
    and Jacob, [2020](#bib.bib199)), wavelets (Ross-Howe and Tizhoosh, [2018](#bib.bib329)),
    Laplacian pyramids (Pour and Seker, [2020](#bib.bib311)), and Laplacian filtering (Saba
    et al., [2019](#bib.bib334)). The second approach consists of predicting an additional
    segmentation mask to combine with the one generated by the CNN. Zhang et al. ([2019b](#bib.bib441)),
    for example, use LBPs to consider the textural patterns of skin lesions and guide
    the networks towards more refined segmentations. Bozorgtabar et al. ([2017b](#bib.bib66))
    also employ LBPs combined with pixel-level color information to divide the dermoscopic
    image into superpixels, which are then scored as part of the lesion or the background.
    The score mask is then combined with the CNN output mask to compute the final
    segmentation mask. Despite the limited number of works devoted to integrating
    deep features with hand-crafted ones, the results so far indicate that this may
    be a promising research direction.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 将手工特征与 CNN 结合的工作遵循两种不同的方法。第一种方法包括预过滤输入图像，以增强病变与周围皮肤之间的对比度。探索的技术包括局部二值模式 (LBPs)
    (Ross-Howe 和 Tizhoosh, [2018](#bib.bib329); Jayapriya 和 Jacob, [2020](#bib.bib199))，小波
    (Ross-Howe 和 Tizhoosh, [2018](#bib.bib329))，拉普拉斯金字塔 (Pour 和 Seker, [2020](#bib.bib311))，以及拉普拉斯滤波
    (Saba et al., [2019](#bib.bib334))。第二种方法包括预测一个额外的分割掩膜，以与 CNN 生成的掩膜结合。例如，Zhang
    et al. ([2019b](#bib.bib441)) 使用 LBPs 考虑皮肤病变的纹理模式，并引导网络进行更精细的分割。Bozorgtabar et
    al. ([2017b](#bib.bib66)) 还结合像素级颜色信息使用 LBPs 将皮肤镜图像划分为超像素，然后将其评分为病变的一部分或背景。得分掩膜随后与
    CNN 输出掩膜结合，以计算最终的分割掩膜。尽管致力于将深度特征与手工特征结合的工作数量有限，但迄今为止的结果表明，这可能是一个有前景的研究方向。
- en: 3.1.4 Transformer Models
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 Transformer 模型
- en: Initially proposed for natural language processing (Vaswani et al., [2017](#bib.bib392)),
    Transformers have proliferated in the last couple of years in other areas, including
    computer vision applications, especially with improvements made over the years
    for optimizing the computational cost of self-attention (Parmar et al., [2018](#bib.bib298);
    Hu et al., [2019](#bib.bib180); Ramachandran et al., [2019](#bib.bib316); Cordonnier
    et al., [2019](#bib.bib102); Zhao et al., [2020](#bib.bib449); Dosovitskiy et al.,
    [2020](#bib.bib127)), and have consequently also been adapted for semantic segmentation
    tasks (Ranftl et al., [2021](#bib.bib320); Strudel et al., [2021](#bib.bib362);
    Zheng et al., [2021](#bib.bib453)). For medical image segmentation, TransUNet (Chen
    et al., [2021](#bib.bib85)) was one of the first works to use Transformers along
    with CNNs in the encoder of a U-Net-like encoder-decoder architecture, and Gulzar
    and Khan ([2022](#bib.bib164)) showed that TransUNet outperforms several CNN-only
    models for skin lesion segmentation. To reduce the computational complexity involved
    with high-resolution medical images, Cao et al. ([2021](#bib.bib71)) proposed
    the Swin-Unet architecture that uses self-attention within shifted windows (Liu
    et al., [2021b](#bib.bib263)). For a comprehensive review of the literature of
    Transformers in general medical image analysis, we refer the interested readers
    to the surveys by He et al. ([2022](#bib.bib173)) and Shamshad et al. ([2022](#bib.bib346)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最初提出用于自然语言处理 (Vaswani et al., [2017](#bib.bib392))，近年来 Transformer 已在其他领域广泛应用，包括计算机视觉应用，特别是随着自注意力
    (Parmar et al., [2018](#bib.bib298); Hu et al., [2019](#bib.bib180); Ramachandran
    et al., [2019](#bib.bib316); Cordonnier et al., [2019](#bib.bib102); Zhao et al.,
    [2020](#bib.bib449); Dosovitskiy et al., [2020](#bib.bib127)) 计算成本优化的改进，Transformer
    也被适应用于语义分割任务 (Ranftl et al., [2021](#bib.bib320); Strudel et al., [2021](#bib.bib362);
    Zheng et al., [2021](#bib.bib453))。在医学图像分割方面，TransUNet (Chen et al., [2021](#bib.bib85))
    是首批在 U-Net 类编码器-解码器架构的编码器中使用 Transformer 和 CNN 的工作之一，Gulzar 和 Khan ([2022](#bib.bib164))
    显示 TransUNet 在皮肤病变分割中优于多个仅使用 CNN 的模型。为了降低高分辨率医学图像的计算复杂度，Cao et al. ([2021](#bib.bib71))
    提出了 Swin-Unet 架构，该架构在移动窗口内使用自注意力 (Liu et al., [2021b](#bib.bib263))。有关 Transformer
    在一般医学图像分析中的文献综述，我们建议感兴趣的读者参考 He et al. ([2022](#bib.bib173)) 和 Shamshad et al.
    ([2022](#bib.bib346)) 的综述。
- en: Zhang et al. ([2021b](#bib.bib445)) propose TransFuse which parallelly computes
    features from CNN and Transformer modules, with the former capturing low-level
    spatial information and the latter responsible for modeling global context, and
    these features are then combined using a self-attention-based fusion module. Evaluation
    on the ISIC 2017 dataset shows superior segmentation performance and faster convergence.
    The multi-compound Transformer (Ji et al., [2021](#bib.bib202)) leverages Transformer-based
    self-attention and cross-attention modules between the encoder and the decoder
    components of U-Net to learn rich features from multi-scale CNN features. Wang
    et al. ([2021a](#bib.bib399)) incorporate boundary-wise prior knowledge in segmentation
    models using a boundary-aware Transformer (BAT) to deal with the ambiguous boundaries
    in skin lesion images. More recently, Wu et al. ([2022a](#bib.bib415)) introduce
    a feature-adaptive Transformer network (FAT-Net) that comprised of a dual CNN-Transformer
    encoder, a light-weight trainable feature-adaptation module, and a memory-efficient
    decoder using a squeeze-and-excitation module. The resulting segmentation model
    is more accurate at segmenting skin lesions while also being faster (fewer parameters
    and computation) than several CNN-only models.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人 ([2021b](#bib.bib445)) 提出了 TransFuse，该方法并行地从 CNN 和 Transformer 模块中计算特征，其中前者捕捉低级空间信息，后者负责建模全局上下文，然后使用基于自注意力的融合模块将这些特征组合在一起。在
    ISIC 2017 数据集上的评估显示了优越的分割性能和更快的收敛速度。多复合 Transformer (Ji 等人, [2021](#bib.bib202))
    利用 Transformer 基于自注意力和跨注意力模块在 U-Net 的编码器和解码器组件之间，学习来自多尺度 CNN 特征的丰富特征。Wang 等人 ([2021a](#bib.bib399))
    结合边界感知 Transformer (BAT) 将边界知识融入分割模型中，以处理皮肤病变图像中的模糊边界。最近，Wu 等人 ([2022a](#bib.bib415))
    介绍了一种特征自适应 Transformer 网络 (FAT-Net)，它包括一个双 CNN-Transformer 编码器、一个轻量级可训练的特征自适应模块，以及一个使用挤压和激励模块的内存高效解码器。结果分割模型在分割皮肤病变时更为准确，同时比几个仅使用
    CNN 的模型更快（参数更少，计算更少）。
- en: 3.2 Loss Functions
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 损失函数
- en: 'A segmentation model $f$ may be formalized as a function $\hat{y}=f_{\theta}(x)$,
    which maps an input image $x$ to an estimated segmentation map $\hat{y}$ parameterized
    by a (large) set of parameters $\theta$. For skin lesions, $\hat{y}$ is a binary
    mask separating the lesion from the surrounding skin. Given a training set of
    images $x_{i}$ and their corresponding ground-truth masks $y_{i}$ $\{(x_{i},y_{i});i=1,...,N\}$,
    training a segmentation model consists of finding the model parameters $\theta$
    that maximize the likelihood of observing those data:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分割模型 $f$ 可以形式化为一个函数 $\hat{y}=f_{\theta}(x)$，它将输入图像 $x$ 映射到由（大量）参数 $\theta$
    参数化的估计分割图 $\hat{y}$。对于皮肤病变，$\hat{y}$ 是一个二值掩码，将病变与周围皮肤分开。给定一组图像 $x_{i}$ 及其对应的真实掩码
    $y_{i}$ $\{(x_{i},y_{i});i=1,...,N\}$，训练一个分割模型的过程包括寻找模型参数 $\theta$，以最大化观察这些数据的可能性：
- en: '|  | $\theta^{*}=\operatorname*{arg\,max}_{\theta}\sum_{i=1}^{N}\log\mathrm{P}(y_{i}&#124;x_{i};\theta),$
    |  | (1) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta^{*}=\operatorname*{arg\,max}_{\theta}\sum_{i=1}^{N}\log\mathrm{P}(y_{i}&#124;x_{i};\theta),$
    |  | (1) |'
- en: 'which is performed indirectly, via the minimization of a loss function between
    the estimated and the true segmentation masks:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是通过最小化估计分割掩码与真实分割掩码之间的损失函数间接进行的：
- en: '|  | $\theta^{*}=\operatorname*{arg\,min}_{\theta}\sum_{i=1}^{N}\mathcal{L}(\hat{y}_{i}&#124;y_{i})=\operatorname*{arg\,min}_{\theta}\sum_{i=1}^{N}\mathcal{L}(f_{\theta}(x_{i})&#124;y_{i}).$
    |  | (2) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta^{*}=\operatorname*{arg\,min}_{\theta}\sum_{i=1}^{N}\mathcal{L}(\hat{y}_{i}&#124;y_{i})=\operatorname*{arg\,min}_{\theta}\sum_{i=1}^{N}\mathcal{L}(f_{\theta}(x_{i})&#124;y_{i}).$
    |  | (2) |'
- en: The choice of the loss function is thus critical, as it encodes not only the
    main optimization objective, but also much of the prior information needed to
    guide the learning and constrain the search space. As can been in Table LABEL:tab:main,
    many skin lesion segmentation models employ a combination of losses to enhance
    generalization (see Fig. [8](#S3.F8 "Figure 8 ‣ 3.2 Loss Functions ‣ 3 Model Design
    and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的选择至关重要，因为它不仅编码了主要的优化目标，还包含了指导学习和限制搜索空间所需的先验信息。如表 LABEL:tab:main 所示，许多皮肤病变分割模型使用了损失的组合来增强泛化能力（见图
    [8](#S3.F8 "Figure 8 ‣ 3.2 Loss Functions ‣ 3 Model Design and Training ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")）。
- en: '![Refer to caption](img/54702960d2f783a7c3546355375b8b8e.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/54702960d2f783a7c3546355375b8b8e.png)'
- en: 'Figure 8: The distribution of loss functions used by the surveyed works in
    DL-based skin lesion segmentation. Cross-entropy loss is the most popular loss
    function ($96$ papers), followed by Dice ($53$ papers) and Jaccard ($19$ papers)
    losses. Of the $177$ surveyed papers, $65$ use a combination of losses, with CE
    + Dice ($27$ papers) and CE + Jaccard ($11$ papers) being the most popular combinations.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：调查的基于深度学习的皮肤病变分割工作的损失函数分布。交叉熵损失是最流行的损失函数（$96$篇论文），其次是Dice（$53$篇论文）和Jaccard（$19$篇论文）损失。在$177$篇调查的论文中，有$65$篇使用了损失组合，其中CE
    + Dice（$27$篇论文）和CE + Jaccard（$11$篇论文）是最受欢迎的组合。
- en: 3.2.1 Losses based on $p$-norms
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 基于$p$-范数的损失
- en: Losses based on $p$-norms are the simplest ones, and comprise the mean squared
    error (MSE) (for $p=2$) and the mean absolute error (MAE) (for $p=1$).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 基于$p$-范数的损失是最简单的，包括均方误差（MSE）（对于$p=2$）和平均绝对误差（MAE）（对于$p=1$）。
- en: '|  | $\mathsf{MSE}(X,Y;\theta)=-\sum_{i=1}^{N}\&#124;y_{i}-\hat{y}_{i}\&#124;_{2},$
    |  | (3) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{MSE}(X,Y;\theta)=-\sum_{i=1}^{N}\&#124;y_{i}-\hat{y}_{i}\&#124;_{2},$
    |  | (3) |'
- en: '|  | $\mathsf{MAE}(X,Y;\theta)=-\sum_{i=1}^{N}\&#124;y_{i}-\hat{y}_{i}\&#124;_{1}.$
    |  | (4) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{MAE}(X,Y;\theta)=-\sum_{i=1}^{N}\&#124;y_{i}-\hat{y}_{i}\&#124;_{1}.$
    |  | (4) |'
- en: In GANs, to regularize the segmentations produced by the generator, it is common
    to utilize hybrid losses containing MSE ($\ell_{2}$ loss) (Peng et al., [2019](#bib.bib304))
    or MAE ($\ell_{1}$ loss) (Peng et al., [2019](#bib.bib304); Tu et al., [2019](#bib.bib383);
    Lei et al., [2020](#bib.bib241)). The MSE has also been used as a regularizer
    to match attention and ground-truth maps (Xie et al., [2020a](#bib.bib419)).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成对抗网络（GANs）中，为了规范生成器产生的分割结果，通常会使用包含MSE（$\ell_{2}$损失）（Peng等，[2019](#bib.bib304)）或MAE（$\ell_{1}$损失）（Peng等，[2019](#bib.bib304)；Tu等，[2019](#bib.bib383)；Lei等，[2020](#bib.bib241)）的混合损失。MSE也被用作匹配注意力和真实标签图的正则化器（Xie等，[2020a](#bib.bib419)）。
- en: 3.2.2 Cross-entropy Loss
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 交叉熵损失
- en: 'Semantic segmentation may be viewed as classification at the pixel level, i.e.,
    as assigning a class label to each pixel. From this perspective, minimizing the
    negative log-likelihoods of pixel-wise predictions (i.e., maximizing their likelihood)
    may be achieved by minimizing a cross-entropy loss $\mathcal{L}_{ce}$:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割可以视为像素级的分类，即为每个像素分配一个类别标签。从这个角度来看，通过最小化像素级预测的负对数似然（即最大化其似然）可以通过最小化交叉熵损失$\mathcal{L}_{ce}$来实现：
- en: '|  | $\mathcal{L}_{ce}(X,Y;\theta)=-\sum_{i=1}^{N}\sum_{p\in\Omega_{i}}y_{ip}\log\hat{y}_{ip}+(1-y_{ip})\log(1-\hat{y}_{ip}),~{}~{}\hat{y}_{ip}=P(y_{ip}=1&#124;X(i);\theta),$
    |  | (5) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{ce}(X,Y;\theta)=-\sum_{i=1}^{N}\sum_{p\in\Omega_{i}}y_{ip}\log\hat{y}_{ip}+(1-y_{ip})\log(1-\hat{y}_{ip}),~{}~{}\hat{y}_{ip}=P(y_{ip}=1&#124;X(i);\theta),$
    |  | (5) |'
- en: where $\Omega_{i}$ is the set of all image $i$ pixels, $P$ is the probability,
    $x_{ip}$ is $p^{th}$ image pixel in $i^{th}$ image and, $y_{ip}\in\{0,1\}$ and
    $\hat{y}_{ip}\in[0,1]$ are respectively the true and the predicted labels of $x_{ip}$.
    The cross-entropy loss appears in the majority of deep skin lesion segmentation
    works, e.g., Song et al. ([2019](#bib.bib359)), Singh et al. ([2019](#bib.bib355)),
    and Zhang et al. ([2019a](#bib.bib437)).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Omega_{i}$是所有图像$i$像素的集合，$P$是概率，$x_{ip}$是第$i$张图像中的第$p$个像素，$y_{ip}\in\{0,1\}$和$\hat{y}_{ip}\in[0,1]$分别是真实和预测的$x_{ip}$标签。交叉熵损失出现在大多数深度皮肤病变分割工作中，例如Song等（[2019](#bib.bib359)），Singh等（[2019](#bib.bib355)），以及Zhang等（[2019a](#bib.bib437)）。
- en: Since the gradient of the cross-entropy loss function is inversely proportional
    to the predicted probabilities, hard-to-predict samples are weighted more in the
    parameter update equations, leading to faster convergence. A variant, the weighted
    cross-entropy loss, penalizes pixels and class labels differently. Nasr-Esfahani
    et al. ([2019](#bib.bib285)) used pixel weights inversely proportional to their
    distance to lesion boundaries to enforce sharper boundaries. Class weighting may
    also mitigate the class imbalance, which, left uncorrected, tends to bias models
    towards the background class, since lesions tend to occupy a relatively small
    portion of images. Chen et al. ([2018b](#bib.bib90)), Goyal et al. ([2019a](#bib.bib153)),
    and Wang et al. ([2019b](#bib.bib404)) apply such a correction, using class weights
    inversely proportional to the class pixel frequency. Mirikharaji et al. ([2019](#bib.bib281))
    weighted the pixels according to annotation noise estimated using a set of cleanly
    annotated data. All the aforementioned losses treat pixels independently without
    enforcing spatial coherence, which motivates their combination with other consistency-seeking
    losses.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 由于交叉熵损失函数的梯度与预测概率成反比，因此难以预测的样本在参数更新方程中会被赋予更高的权重，从而加快收敛速度。一种变体，即加权交叉熵损失，对像素和类别标签施加不同的惩罚。Nasr-Esfahani
    等人（[2019](#bib.bib285)）使用与病变边界距离成反比的像素权重，以加强边界的清晰度。类别加权还可以缓解类别不平衡的问题，如果不加以纠正，模型通常会倾向于背景类别，因为病变往往占据图像中相对较小的部分。Chen
    等人（[2018b](#bib.bib90)）、Goyal 等人（[2019a](#bib.bib153)）和 Wang 等人（[2019b](#bib.bib404)）应用了这种纠正方法，使用与类别像素频率成反比的类别权重。Mirikharaji
    等人（[2019](#bib.bib281)）根据使用一组干净标注数据估计的注释噪声对像素进行了加权。上述所有损失函数都独立处理像素而不强制空间一致性，这促使它们与其他一致性寻求损失函数结合使用。
- en: 3.2.3 Dice and Jaccard Loss
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 Dice 和 Jaccard 损失
- en: The Dice score and the Jaccard index are two popular metrics for segmentation
    evaluation (Section [4.3](#S4.SS3 "4.3 Evaluation Metrics ‣ 4 Evaluation ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), measuring the overlap between
    predicted segmentation and ground-truth. Models may employ differentiable approximations
    of these metrics, known as soft Dice (He et al., [2017](#bib.bib175); Kaul et al.,
    [2019](#bib.bib215); He et al., [2018](#bib.bib176); Wang et al., [2019a](#bib.bib397))
    and soft Jaccard (Venkatesh et al., [2018](#bib.bib393); Hasan et al., [2020](#bib.bib172);
    Sarker et al., [2019](#bib.bib341)) to optimize an objective directly related
    to the evaluation metric.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Dice 得分和 Jaccard 指数是两个用于分割评估的流行指标（见第 [4.3](#S4.SS3 "4.3 Evaluation Metrics ‣
    4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation)节），用于衡量预测分割与真实情况的重叠。模型可能会采用这些指标的可微分近似，称为软
    Dice（He 等人，[2017](#bib.bib175)；Kaul 等人，[2019](#bib.bib215)；He 等人，[2018](#bib.bib176)；Wang
    等人，[2019a](#bib.bib397)）和软 Jaccard（Venkatesh 等人，[2018](#bib.bib393)；Hasan 等人，[2020](#bib.bib172)；Sarker
    等人，[2019](#bib.bib341)）来优化与评估指标直接相关的目标。
- en: 'For two classes, these losses are defined as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个类别，这些损失函数定义如下：
- en: '|  | $\mathcal{L}_{dice}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{2\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}+\hat{y}_{ip}},$
    |  | (6) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{dice}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{2\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}+\hat{y}_{ip}},$
    |  | (6) |'
- en: '|  | $\mathcal{L}_{jacc}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}+\hat{y}_{ip}-y_{ip}\hat{y}_{ip}}.$
    |  | (7) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{jacc}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}+\hat{y}_{ip}-y_{ip}\hat{y}_{ip}}.$
    |  | (7) |'
- en: 'Different variations of overlap-based loss functions address the class imbalance
    problem in medical image segmentation tasks. The Tanimoto distance loss, $\mathcal{L}_{td}$
    is a modified Jaccard loss optimized in some models (Canalini et al., [2019](#bib.bib70);
    Baghersalimi et al., [2019](#bib.bib35); Yuan et al., [2017](#bib.bib433)):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 基于重叠的损失函数的不同变体解决了医学图像分割任务中的类别不平衡问题。Tanimoto 距离损失 $\mathcal{L}_{td}$ 是一种修改过的
    Jaccard 损失，在一些模型中进行了优化（Canalini 等人，[2019](#bib.bib70)；Baghersalimi 等人，[2019](#bib.bib35)；Yuan
    等人，[2017](#bib.bib433)）：
- en: '|  | $\mathcal{L}_{td}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}^{2}+\hat{y}_{ip}^{2}-y_{ip}\hat{y}_{ip}},$
    |  | (8) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{td}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}^{2}+\hat{y}_{ip}^{2}-y_{ip}\hat{y}_{ip}},$
    |  | (8) |'
- en: which is equivalent to the Jaccard loss when both ${y}_{ip}$ and $\hat{y}_{ip}$
    are binary.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 Jaccard 损失在 ${y}_{ip}$ 和 $\hat{y}_{ip}$ 都是二值时等效。
- en: 'The Tversky loss (Abraham and Khan, [2019](#bib.bib9)), inspired by the Tversky
    index, is another Jaccard variant that penalizes false positives and false negatives
    differently to address the class imbalance problem:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Tversky 损失 (Abraham and Khan, [2019](#bib.bib9)) 受 Tversky 指数的启发，是另一种 Jaccard
    变体，它以不同的方式惩罚假阳性和假阴性，以解决类别不平衡问题：
- en: '|  | $\mathcal{L}_{tv}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}+\alpha
    y_{ip}(1-\hat{y}_{ip})+\beta(1-y_{ip})\hat{y}_{ip}},$ |  | (9) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{tv}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}+\alpha
    y_{ip}(1-\hat{y}_{ip})+\beta(1-y_{ip})\hat{y}_{ip}},$ |  | (9) |'
- en: where $\alpha$ and $\beta$ tune the contributions of false negatives and false
    positives with $\alpha+\beta=1$.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 和 $\beta$ 调节假阴性和假阳性的贡献，且满足 $\alpha+\beta=1$。
- en: 'Abraham and Khan ([2019](#bib.bib9)) combined the Tvserky and focal losses (Lin
    et al., [2017](#bib.bib255)), the latter encouraging the algorithm to focus on
    the hard-to-predict pixels:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Abraham 和 Khan ([2019](#bib.bib9)) 将 Tversky 损失与焦点损失 (Lin et al., [2017](#bib.bib255))
    结合，后者鼓励算法关注难以预测的像素：
- en: '|  | $\mathcal{L}_{ftv}=\mathcal{L}_{tv}^{\frac{1}{\gamma}},$ |  | (10) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{ftv}=\mathcal{L}_{tv}^{\frac{1}{\gamma}},$ |  | (10) |'
- en: where $\gamma$ controls the relative importance of the hard-to-predict samples.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma$ 控制难以预测样本的相对重要性。
- en: 3.2.4 Matthews Correlation Coefficient Loss
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 Matthews 相关系数损失
- en: 'Matthews correlation coefficient (MCC) loss is a metric-based loss function
    based on the correlation between predicted and ground-truth labels (Abhishek and
    Hamarneh, [2021](#bib.bib6)). In contrast to the overlap-based losses discussed
    in Section [3.2.3](#S3.SS2.SSS3 "3.2.3 Dice and Jaccard Loss ‣ 3.2 Loss Functions
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation"),
    MCC considers misclassifying the background pixels by penalizing false negative
    labels, making it more effective in the presence of skewed class distributions.
    MCC loss is defined as:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Matthews 相关系数 (MCC) 损失是一种基于预测标签和真实标签之间相关性的度量型损失函数 (Abhishek and Hamarneh, [2021](#bib.bib6))。与第
    [3.2.3](#S3.SS2.SSS3 "3.2.3 Dice 和 Jaccard 损失 ‣ 3.2 损失函数 ‣ 3 模型设计与训练 ‣ 深度学习在皮肤病变分割中的应用")
    节中讨论的基于重叠的损失不同，MCC 通过惩罚假阴性标签来考虑背景像素的错误分类，使其在类别分布不均的情况下更有效。MCC 损失定义为：
- en: '|  | $\mathcal{L}_{MCC}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}\hat{y}_{ip}y_{ip}\frac{\sum_{p\in\Omega}\hat{y}_{ip}\sum_{p\in\Omega}y_{ip}}{M_{i}}}{f(\hat{y}_{i}y_{i})},$
    |  | (11) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{MCC}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}\hat{y}_{ip}y_{ip}\frac{\sum_{p\in\Omega}\hat{y}_{ip}\sum_{p\in\Omega}y_{ip}}{M_{i}}}{f(\hat{y}_{i}y_{i})},$
    |  | (11) |'
- en: '|  | $f(\hat{y}_{i},y_{i})=\sqrt{\sum_{p\in\Omega}\hat{y}_{ip}\sum_{p\in\Omega}{y}_{ip}-\frac{\sum_{p\in\Omega}\hat{y}_{ip}(\sum_{p\in\Omega}{y}_{ip})^{2}}{M_{i}}-\frac{(\sum_{p\in\Omega}\hat{y}_{ip})^{2}\sum_{p\in\Omega}{y}_{ip}}{M_{i}}+(\frac{\sum_{p\in\Omega}\hat{y}_{ip}\sum_{p\in\Omega}{y}_{ip}}{M_{i}})^{2}}~{},$
    |  | (12) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(\hat{y}_{i},y_{i})=\sqrt{\sum_{p\in\Omega}\hat{y}_{ip}\sum_{p\in\Omega}{y}_{ip}-\frac{\sum_{p\in\Omega}\hat{y}_{ip}(\sum_{p\in\Omega}{y}_{ip})^{2}}{M_{i}}-\frac{(\sum_{p\in\Omega}\hat{y}_{ip})^{2}\sum_{p\in\Omega}{y}_{ip}}{M_{i}}+(\frac{\sum_{p\in\Omega}\hat{y}_{ip}\sum_{p\in\Omega}{y}_{ip}}{M_{i}})^{2}}~{},$
    |  | (12) |'
- en: where $M_{i}$ is the total number of pixels in the image $i$.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M_{i}$ 是图像 $i$ 中的像素总数。
- en: 3.2.5 Deep Supervision Loss
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5 深度监督损失
- en: 'In DL models, the loss may apply not only to the final decision layer, but
    also to the intermediate hidden layers. The supervision of hidden layers, known
    as deep supervision, guides the learning of intermediate features. Deep supervision
    also addresses the vanishing gradient problem, leading to faster convergence and
    improves segmentation performance by constraining the feature space. Deep supervision
    loss appears in several skin lesion segmentation works (He et al., [2017](#bib.bib175);
    Zeng and Zheng, [2018](#bib.bib436); Li et al., [2018a](#bib.bib243), [b](#bib.bib248);
    He et al., [2018](#bib.bib176); Zhang et al., [2019a](#bib.bib437); Tang et al.,
    [2019b](#bib.bib374)), where it is computed in multiple layers, at different scales.
    The loss has the general form of a weighted summation of multi-scale segmentation
    losses:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型中，损失函数不仅可以应用于最终的决策层，还可以应用于中间的隐藏层。隐藏层的监督被称为深度监督，它引导中间特征的学习。深度监督还解决了梯度消失问题，加快了收敛速度，并通过约束特征空间提高了分割性能。深度监督损失出现在若干皮肤病变分割工作中
    (He et al., [2017](#bib.bib175); Zeng and Zheng, [2018](#bib.bib436); Li et al.,
    [2018a](#bib.bib243), [b](#bib.bib248); He et al., [2018](#bib.bib176); Zhang
    et al., [2019a](#bib.bib437); Tang et al., [2019b](#bib.bib374))，其中它在多个层次和不同的尺度上进行计算。损失函数一般为多尺度分割损失的加权和：
- en: '|  | $\mathcal{L}_{ds}(X,Y;\theta)=\sum_{l=1}^{m}\gamma_{l}\mathcal{L}_{l}(X,Y;\theta),$
    |  | (13) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{ds}(X,Y;\theta)=\sum_{l=1}^{m}\gamma_{l}\mathcal{L}_{l}(X,Y;\theta),$
    |  | (13) |'
- en: where $m$ is the number of scales, $\mathcal{L}_{l}$ is the loss at the $l^{th}$
    scale, and $\gamma_{l}$ adjusts the contribution of different losses.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 是尺度的数量，$\mathcal{L}_{l}$ 是第 $l^{th}$ 称尺度的损失，$\gamma_{l}$ 调整不同损失的贡献。
- en: 3.2.6 Star-Shape Loss
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.6 星形损失
- en: 'In contrast to pixel-wise losses which act on pixels independently and cannot
    enforce spatial constraints, the star-shape loss (Mirikharaji and Hamarneh, [2018](#bib.bib279))
    aims to capture class label dependencies and preserve the target object structure
    in the predicted segmentation masks. Based upon prior knowledge about the shape
    of skin lesions, the star-shape loss, $\mathcal{L}_{ssh}$ penalizes discontinuous
    decisions in the estimated output as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与像素级损失作用于独立像素并且无法强制空间约束不同，星形损失 (Mirikharaji 和 Hamarneh, [2018](#bib.bib279))
    旨在捕捉类别标签依赖性并在预测的分割掩码中保留目标物体结构。基于关于皮肤病灶形状的先验知识，星形损失 $\mathcal{L}_{ssh}$ 对估计输出中的不连续决策进行如下惩罚：
- en: '|  | $\mathcal{L}_{ssh}(X,Y;\theta)=\sum_{i=1}^{N}\sum_{p\in\Omega}\sum_{q\in\ell_{pc}}\mathbbm{1}_{y_{ip}=y_{iq}}\times&#124;y_{ip}-\hat{y}_{ip}&#124;\times&#124;\hat{y}_{ip}-\hat{y}_{iq}&#124;,$
    |  | (14) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{ssh}(X,Y;\theta)=\sum_{i=1}^{N}\sum_{p\in\Omega}\sum_{q\in\ell_{pc}}\mathbbm{1}_{y_{ip}=y_{iq}}\times\lvert
    y_{ip}-\hat{y}_{ip} \rvert \times \lvert \hat{y}_{ip}-\hat{y}_{iq} \rvert,$ |  |
    (14) |'
- en: where $c$ is the lesion center, $\ell_{pc}$ is the line segment connecting pixels
    $p$ and $c$ and, $q$ is any pixel lying on $\ell_{pc}$. This loss encourages all
    pixels lying between $p$ and $q$ on $\ell_{pc}$ to be assigned the same estimator
    whenever $p$ and $q$ have the same ground-truth label. The result is a radial
    spatial coherence from the lesion center.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c$ 是病灶中心，$\ell_{pc}$ 是连接像素 $p$ 和 $c$ 的线段，而 $q$ 是位于 $\ell_{pc}$ 上的任意像素。这个损失鼓励在
    $\ell_{pc}$ 上位于 $p$ 和 $q$ 之间的所有像素在 $p$ 和 $q$ 具有相同真实标签时被分配相同的估计值。结果是在病灶中心形成径向空间一致性。
- en: 3.2.7 End-Point Error Loss
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.7 端点误差损失
- en: 'Many authors consider the lesion boundary the most challenging region to segment.
    The end-point error loss (Sarker et al., [2018](#bib.bib342); Singh et al., [2019](#bib.bib355))
    underscores borders by using the first derivative of the segmentation masks instead
    of their raw values:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 许多作者认为病灶边界是分割中最具挑战性的区域。端点误差损失 (Sarker 等， [2018](#bib.bib342); Singh 等， [2019](#bib.bib355))
    通过使用分割掩码的一阶导数来突出边界，而不是使用其原始值：
- en: '|  | $\mathcal{L}_{epe}(X,Y;\theta)=\sum_{i=1}^{N}\sum_{p\in\Omega}\sqrt{(\hat{y}^{0}_{ip}-y^{0}_{ip})^{2}+(\hat{y}^{1}_{ip}-y^{1}_{ip})^{2}},$
    |  | (15) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{epe}(X,Y;\theta)=\sum_{i=1}^{N}\sum_{p\in\Omega}\sqrt{(\hat{y}^{0}_{ip}-y^{0}_{ip})^{2}+(\hat{y}^{1}_{ip}-y^{1}_{ip})^{2}},$
    |  | (15) |'
- en: where $\hat{y}^{0}_{ip}$ and $\hat{y}^{1}_{ip}$ are the directional first derivatives
    of the estimated segmentation map in the $x$ and $y$ spatial directions, respectively
    and, similarly, $y^{0}_{ip}$ and $y^{1}_{ip}$ for the ground-truth derivatives.
    Thus, this loss function encourages the magnitude and orientation of edges of
    estimation and ground-truth to match, thereby mitigating vague boundaries in skin
    lesion segmentation.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{y}^{0}_{ip}$ 和 $\hat{y}^{1}_{ip}$ 分别是估计分割图在 $x$ 和 $y$ 空间方向上的方向一阶导数，同样，$y^{0}_{ip}$
    和 $y^{1}_{ip}$ 是真实值的导数。因此，这个损失函数鼓励估计边缘和真实边缘的幅度与方向匹配，从而减轻皮肤病灶分割中的模糊边界。
- en: 3.2.8 Adversarial Loss
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.8 对抗损失
- en: 'Another way to add high-order class-label consistency is adversarial training.
    Adversarial training may be employed along with traditional supervised training
    to distinguish estimated segmentation from ground-truths using a discriminator.
    The optimization objective will weight a pixel-wise loss $\mathcal{L}_{s}$ matching
    prediction to ground-truth, and an adversarial loss, as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种增加高阶类别标签一致性的方法是对抗训练。对抗训练可以与传统的监督训练一起使用，通过判别器区分估计的分割与真实标签。优化目标将对比像素级损失 $\mathcal{L}_{s}$
    与真实值匹配，并且对抗损失如下：
- en: '|  | $\mathcal{L}_{adv}(X,Y;\theta,\theta_{a})=\mathcal{L}_{s}(X,Y;\theta)-\lambda[\mathcal{L}_{ce}(Y,1;\theta_{a})+\mathcal{L}_{ce}(\hat{Y},0;\theta,\theta_{a})],$
    |  | (16) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{adv}(X,Y;\theta,\theta_{a})=\mathcal{L}_{s}(X,Y;\theta)-\lambda[\mathcal{L}_{ce}(Y,1;\theta_{a})+\mathcal{L}_{ce}(\hat{Y},0;\theta,\theta_{a})],$
    |  | (16) |'
- en: where $\theta_{a}$ are the adversarial model parameters. The adversarial loss
    employs a binary cross-entropy loss to encourage the segmentation model to produce
    indistinguishable prediction maps from ground-truth maps. The adversarial objective
    (Eqn. ([16](#S3.E16 "In 3.2.8 Adversarial Loss ‣ 3.2 Loss Functions ‣ 3 Model
    Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")))
    is optimized in a mini-max game by simultaneously minimizing it with respect to
    $\theta$ and maximizing it with respect to $\theta_{a}$.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta_{a}$ 是对抗模型参数。对抗损失使用二元交叉熵损失来促使分割模型生成与真实标签图无区别的预测图。对抗目标（公式 ([16](#S3.E16
    "在 3.2.8 对抗损失 ‣ 3.2 损失函数 ‣ 3 模型设计与训练 ‣ 深度学习在皮肤病变分割中的应用综述"))) 在一个小型最大博弈中被优化，即同时对
    $\theta$ 进行最小化，对 $\theta_{a}$ 进行最大化。
- en: Pixel-wise losses, such as cross-entropy (Izadi et al., [2018](#bib.bib190);
    Singh et al., [2019](#bib.bib355); Jiang et al., [2019](#bib.bib203)), soft Jaccard (Sarker
    et al., [2019](#bib.bib341); Tu et al., [2019](#bib.bib383); Wei et al., [2019](#bib.bib412)),
    end-point error (Tu et al., [2019](#bib.bib383); Singh et al., [2019](#bib.bib355)),
    MSE (Peng et al., [2019](#bib.bib304)) and MAE  (Sarker et al., [2019](#bib.bib341);
    Singh et al., [2019](#bib.bib355); Jiang et al., [2019](#bib.bib203)) losses have
    all been incorporated in adversarial learning of skin lesion segmentation. In
    addition, Xue et al. ([2018](#bib.bib423)) and Tu et al. ([2019](#bib.bib383))
    presented a multi-scale adversarial term to match a hierarchy of local and global
    contextual features in the predicted maps and ground-truths. In particular, they
    minimize the MAE of multi-scale features extracted from different layers of the
    adversarial model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 像交叉熵（Izadi et al., [2018](#bib.bib190); Singh et al., [2019](#bib.bib355); Jiang
    et al., [2019](#bib.bib203)）、软Jaccard（Sarker et al., [2019](#bib.bib341); Tu et
    al., [2019](#bib.bib383); Wei et al., [2019](#bib.bib412)）、端点误差（Tu et al., [2019](#bib.bib383);
    Singh et al., [2019](#bib.bib355)）、均方误差（MSE）（Peng et al., [2019](#bib.bib304)）和平均绝对误差（MAE）（Sarker
    et al., [2019](#bib.bib341); Singh et al., [2019](#bib.bib355); Jiang et al.,
    [2019](#bib.bib203)）等像素级损失均已被纳入皮肤病变分割的对抗学习中。此外，Xue et al. ([2018](#bib.bib423))
    和 Tu et al. ([2019](#bib.bib383)) 提出了一个多尺度对抗项，用于匹配预测图和真实标签中的局部和全局上下文特征层次。特别是，他们最小化从对抗模型的不同层提取的多尺度特征的
    MAE。
- en: 3.2.9 Rank Loss
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.9 排序损失
- en: 'Assuming that hard-to-predict pixels lead to larger prediction errors while
    training the model, rank loss (Xie et al., [2020b](#bib.bib420)) is proposed to
    encourage learning more discriminative information for harder pixels. The image
    pixels are ranked based on their prediction errors, and the top $K$ pixels with
    the largest prediction errors from the lesion or background areas are selected.
    Let $\hat{y}_{ij}^{0}$ and $\hat{y}_{il}^{1}$ are respectively the selected $j^{th}$
    hard-to-predict pixel of background and $l^{th}$ hard-to-predict pixel of lesion
    in the image $i$, we have:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在训练模型时，难以预测的像素会导致较大的预测误差，排序损失（Xie et al., [2020b](#bib.bib420)）被提出以鼓励对更难的像素学习更多的判别信息。图像像素根据其预测误差进行排名，选择来自病变或背景区域的预测误差最大的前
    $K$ 个像素。设 $\hat{y}_{ij}^{0}$ 和 $\hat{y}_{il}^{1}$ 分别为图像 $i$ 中选定的第 $j$ 个难预测的背景像素和第
    $l$ 个难预测的病变像素，我们有：
- en: '|  | $\mathcal{L}_{rank}(X,Y;\theta)=\sum_{i=1}^{N}\sum_{j=1}^{K}\sum_{l=1}^{K}\max\{0,\hat{y}_{ij}^{0}-\hat{y}_{il}^{1}+margin\},$
    |  | (17) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{rank}(X,Y;\theta)=\sum_{i=1}^{N}\sum_{j=1}^{K}\sum_{l=1}^{K}\max\{0,\hat{y}_{ij}^{0}-\hat{y}_{il}^{1}+margin\},$
    |  | (17) |'
- en: which encourages $\hat{y}_{il}^{1}$ to be greater than $\hat{y}_{ij}^{0}$ plus
    margin.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 该损失鼓励 $\hat{y}_{il}^{1}$ 大于 $\hat{y}_{ij}^{0}$ 加上边际。
- en: Similar to rank loss, narrowband suppression loss (Deng et al., [2020](#bib.bib115))
    also adds a constraint between hard-to-predict pixels of background and lesion.
    Different from rank loss, narrowband suppression loss collects pixels in a narrowband
    along the ground-truth lesion boundary with radius $r$ instead of all image pixels
    and then selects the top $K$ pixels with the largest prediction errors.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于排序损失，窄带抑制损失（Deng et al., [2020](#bib.bib115)）也在背景和病变的难以预测的像素之间增加了约束。与排序损失不同，窄带抑制损失在真实标签病变边界上以半径
    $r$ 收集像素，而不是所有图像像素，然后选择预测误差最大的前 $K$ 个像素。
- en: 4 Evaluation
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估
- en: Evaluation is one of the main challenges for any image segmentation task, skin
    lesions included (Celebi et al., [2015b](#bib.bib82)). Segmentation evaluation
    may be subjective or objective (Zhang et al., [2008](#bib.bib438)), the former
    involving the visual assessment of the results by a panel of human experts, and
    the latter involving the comparison of the results with ground-truth segmentations
    using quantitative evaluation metrics.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 评估是任何图像分割任务中的主要挑战之一，包括皮肤病变（Celebi 等，[2015b](#bib.bib82)）。分割评估可以是主观的或客观的（Zhang
    等，[2008](#bib.bib438)），前者涉及由一组人类专家对结果的视觉评估，后者涉及使用定量评估指标将结果与真实分割进行比较。
- en: Subjective evaluation may provide a nuanced assessment of results, but because
    experts must grade each batch of results, it is usually too laborious to be applied,
    except in limited settings. In objective assessment, experts are consulted once,
    to provide the ground-truth segmentations, and that knowledge can then be reused
    indefinitely. However, due to intra- and inter-annotator variations, it raises
    the question of whether any individual ground-truth segmentation reflects the
    ideal “true” segmentation, an issue we address in Section [4.2](#S4.SS2 "4.2 Inter-Annotator
    Agreement ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation").
    It also raises the issue of choosing one or more evaluation metrics (Section [4.3](#S4.SS3
    "4.3 Evaluation Metrics ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 主观评估可能提供对结果的细致评估，但由于专家必须对每批结果进行评分，因此通常过于繁琐，除非在有限的情况下使用。在客观评估中，专家仅在提供真实分割时被咨询一次，然后这些知识可以无限次重用。然而，由于注释者之间和注释者内部的变化，这引出了一个问题，即任何单一的真实分割是否反映了理想的“真实”分割，这一问题我们在第[4.2](#S4.SS2
    "4.2 Inter-Annotator Agreement ‣ 4 Evaluation ‣ A Survey on Deep Learning for
    Skin Lesion Segmentation")节中讨论。它还引出了选择一个或多个评估指标的问题（第[4.3](#S4.SS3 "4.3 Evaluation
    Metrics ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation")节）。
- en: 4.1 Segmentation Annotation
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 分割注释
- en: Obtaining ground-truth segmentations is paramount for the objective evaluation
    of segmentation algorithms. For synthetically generated images (Section [2.2](#S2.SS2
    "2.2 Synthetic Data Generation ‣ 2 Input Data ‣ A Survey on Deep Learning for
    Skin Lesion Segmentation")), ground-truth segmentations may be known by construction,
    either by applying parallel transformations to the original ground-truth masks
    in the case of traditional data augmentation, or by training generative models
    to synthesize images paired with their segmentation masks.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 获取真实分割对分割算法的客观评估至关重要。对于合成生成的图像（第[2.2](#S2.SS2 "2.2 Synthetic Data Generation
    ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")节），真实分割可能通过构建得知，或通过对传统数据增强中的原始真实掩模应用并行变换，或通过训练生成模型来合成与其分割掩模配对的图像。
- en: For images obtained from real patients, however, human experts have to provide
    the ground-truth segmentations. Various workflows have been proposed to reconcile
    the conflicting goals of ease of learning, speed, accuracy, and flexibility of
    annotation. On one end of the spectrum, the expert traces the lesion by hand,
    on images of the skin lesion printed on photographic paper, which are then scanned (Bogo
    et al., [2015](#bib.bib62)). The technique is easy to learn and fast, but the
    printing and scanning procedure limits the accuracy, and the physical nature of
    the annotations makes corrections burdensome. On the other end of the spectrum,
    the annotation is performed on the computer, by a semi-automated procedure (Codella
    et al., [2019](#bib.bib96)), with an initial border generated by a segmentation
    algorithm, which is then refined by the expert using an annotation software, by
    adjusting the parameters of the segmentation algorithm manually. This method is
    fast and easy to correct, but there might be a learning curve, and its accuracy
    depends on which algorithm is employed and how much the experts understand it.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于从真实患者那里获得的图像，人类专家必须提供真实分割。已经提出了各种工作流程，以协调学习的简便性、速度、准确性和注释的灵活性。在一个极端，专家在打印在照片纸上的皮肤病变图像上手动描绘病变，然后扫描图像（Bogo
    等，[2015](#bib.bib62)）。这种技术易于学习且速度快，但打印和扫描程序限制了准确性，且注释的物理性质使得修正变得繁琐。在另一个极端，注释是在计算机上通过半自动化程序进行的（Codella
    等，[2019](#bib.bib96)），最初由分割算法生成边界，然后由专家使用注释软件通过手动调整分割算法的参数进行细化。这种方法快速且易于修正，但可能需要学习曲线，其准确性取决于所采用的算法以及专家对其的理解程度。
- en: By far, the commonest annotation method in the literature is somewhere in the
    middle, with fully manual annotations performed on a computer. The skin lesion
    image file may be opened either in a raster graphics editor (e.g., GNU Image Manipulation
    Program (GIMP) or Adobe Photoshop), or in a dedicated annotation software (Ferreira
    et al., [2012](#bib.bib135)), where the expert traces the borders of the lesion
    using a mouse or stylus, with continuous freehand drawing, or with discrete control
    points connecting line segments (resulting in a polygon (Codella et al., [2019](#bib.bib96)))
    or smooth curve segments (e.g., cubic B-splines (Celebi et al., [2007a](#bib.bib73))).
    This method provides a good compromise, being easy to implement, fast, and accurate
    to perform, after an acceptable learning period for the annotator.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，文献中最常见的标注方法是介于完全手动标注和计算机上进行的标注之间。皮肤病变图像文件可以在栅格图形编辑器（例如，GNU 图像处理程序 (GIMP)
    或 Adobe Photoshop）中打开，或者在专用标注软件中打开（Ferreira 等人，[2012](#bib.bib135)），在这些软件中，专家使用鼠标或触控笔跟踪病变的边界，可以是连续的手绘，也可以是通过连接线段的离散控制点（结果形成多边形（Codella
    等人，[2019](#bib.bib96)））或平滑曲线段（例如，立方 B 样条（Celebi 等人，[2007a](#bib.bib73)））。这种方法提供了一个良好的折中方案，易于实施，快速且准确，在标注者经过适当学习期后。
- en: 4.2 Inter-Annotator Agreement
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 标注者间一致性
- en: Formally, dataset ground-truths can be viewed as samples of an estimator of
    the true label, which can never be directly observed (Smyth et al., [1995](#bib.bib357)).
    This problem is often immaterial for classification, when annotation noise is
    small. However, in medical image segmentation, ground-truths suffer from both
    biases (systematic deviations from the “ideal”) and significant noise (Zijdenbos
    et al., [1994](#bib.bib457); Chalana and Kim, [1997](#bib.bib84); Guillod et al.,
    [2002](#bib.bib163); Grau et al., [2004](#bib.bib156); Bogo et al., [2015](#bib.bib62);
    Lampert et al., [2016](#bib.bib236)), the latter appearing as inter-annotator
    (different experts) and intra-annotator (same expert at different times) variability.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上看，数据集的真实值可以视为真实标签的估计器的样本，这些真实标签是无法直接观察的（Smyth 等人，[1995](#bib.bib357)）。对于分类任务，当标注噪声较小时，这个问题通常是微不足道的。然而，在医学图像分割中，真实值受到了偏差（系统性偏离“理想”）和显著噪声的影响（Zijdenbos
    等人，[1994](#bib.bib457)；Chalana 和 Kim，[1997](#bib.bib84)；Guillod 等人，[2002](#bib.bib163)；Grau
    等人，[2004](#bib.bib156)；Bogo 等人，[2015](#bib.bib62)；Lampert 等人，[2016](#bib.bib236)），后者表现为标注者间（不同专家）和标注者内（同一专家在不同时间）变异性。
- en: In the largest study of its kind to date, Fortina et al. ([2012](#bib.bib137))
    measured the inter-annotator variability among $12$ dermatologists with varying
    levels of experience on a set of $77$ dermoscopic images, showing that the average
    pairwise XOR dissimilarity (Section [4.3](#S4.SS3 "4.3 Evaluation Metrics ‣ 4
    Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation")) between
    annotators was $\approx 15\%$, and that in $10\%$ of cases, this value was $>28\%$.
    They found more agreement among more experienced dermatologists than less experienced
    ones. Also, more experienced dermatologists tend to outline tighter borders than
    less experienced ones. They suggest that the level of agreement among experienced
    dermatologists could serve as an upper bound for the accuracy achievable by a
    segmentation algorithm, i.e., if even highly experienced dermatologists disagree
    on how to classify $10\%$ of an image, it might be unreasonable to expect a segmentation
    algorithm to agree with more than $90\%$ of any given ground-truth on the same
    image (Fortina et al., [2012](#bib.bib137)).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在迄今为止最大的此类研究中，Fortina 等人 ([2012](#bib.bib137)) 测量了 $12$ 名具有不同经验水平的皮肤科医生在一组 $77$
    张皮肤镜图像上的标注者间变异性，结果显示，标注者间的平均成对 XOR 不相似度（第 [4.3](#S4.SS3 "4.3 Evaluation Metrics
    ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation)）约为 $15\%$，而在
    $10\%$ 的情况下，这一数值大于 $28\%$。他们发现经验更丰富的皮肤科医生之间的意见更趋一致，而经验较少的皮肤科医生则一致性较差。此外，经验丰富的皮肤科医生往往比经验较少的皮肤科医生勾勒出更紧密的边界。他们建议，经验丰富的皮肤科医生之间的一致性水平可以作为分割算法可达到的准确度的上限，即如果即使是高度经验丰富的皮肤科医生也对图像的
    $10\%$ 部分有分歧，那么期望分割算法能与同一图像上超过 $90\%$ 的真实值一致可能是不合理的（Fortina 等人，[2012](#bib.bib137)）。
- en: Due to the aforementioned variability issues, whenever possible, skin lesion
    segmentation should be evaluated against multiple expert ground-truths, a good
    algorithm being one that agrees with the ground-truths at least as well as the
    expert agree among themselves (Chalana and Kim, [1997](#bib.bib84)). Due to the
    cost of annotation, however, algorithms are often evaluated against a single ground-truth.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述变异性问题，尽可能地，皮肤病变分割应对多个专家真实值进行评估，一个好的算法是与真实值的符合程度至少与专家之间的符合程度一样好 (Chalana
    and Kim, [1997](#bib.bib84))。然而，由于注释成本，算法通常只对单一真实值进行评估。
- en: 'When multiple ground-truths are available, the critical issue is how to employ
    them. Several approaches have been proposed:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当有多个真实值可用时，关键问题是如何使用它们。已经提出了几种方法：
- en: •
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Preferring one of the annotations (e.g., the one by the most experienced expert)
    and ignoring the others (Celebi et al., [2007a](#bib.bib73)).
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 偏好其中一个注释（例如，最有经验的专家的注释）并忽略其他注释 (Celebi et al., [2007a](#bib.bib73))。
- en: •
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Measuring and reporting the results for each annotator separately (Celebi et al.,
    [2008](#bib.bib77)), which might require non-trivial multivariate analyses if
    the aim is to rank the algorithms.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分别测量并报告每个注释者的结果 (Celebi et al., [2008](#bib.bib77))，如果目的是对算法进行排名，这可能需要复杂的多变量分析。
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Measuring each automated segmentation against all corresponding ground-truths
    and reporting the average result (Schaefer et al., [2011](#bib.bib344)).
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将每个自动分割结果与所有相应的真实值进行测量并报告平均结果 (Schaefer et al., [2011](#bib.bib344))。
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Measuring each automated segmentation against an *ensemble ground-truth* formed
    by combining the corresponding ground-truths pixel-wise using a bitwise OR (Garnavi
    et al., [2011a](#bib.bib143); Garnavi and Aldeen, [2011](#bib.bib142)), bitwise
    AND (Garnavi et al., [2011b](#bib.bib144)), or a majority voting (Iyatomi et al.,
    [2006](#bib.bib189), [2008](#bib.bib188); Norton et al., [2012](#bib.bib289)).
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将每个自动分割结果与通过逐像素使用按位或 (Garnavi et al., [2011a](#bib.bib143); Garnavi and Aldeen,
    [2011](#bib.bib142))、按位与 (Garnavi et al., [2011b](#bib.bib144)) 或多数投票 (Iyatomi
    et al., [2006](#bib.bib189), [2008](#bib.bib188); Norton et al., [2012](#bib.bib289))
    形成的*集成真实值*进行测量。
- en: The ground-truth ensembling process can be generalized using a *thresholded
    probability map* (Biancardi et al., [2010](#bib.bib54)). First, all ground-truths
    for a sample are averaged pixel-wise into a *probability map*. Then the map is
    binarized, with the lesion corresponding to pixels greater than or equal to a
    chosen threshold. The operations of OR, AND, and majority voting, correspond,
    respectively to thresholds of $1/n$, $1$, and $(n-\varepsilon)/2n$, with $n$ being
    the number of ground-truths, and $\varepsilon$ being a small positive constant.
    AND and OR correspond, respectively, to the tightest and loosest possible contours,
    with other thresholds leading to intermediate results. While the optimal threshold
    value is data-dependent, large thresholds focus the evaluation on unambiguous
    regions, leading to overly optimistic evaluations of segmentation quality (Smyth
    et al., [1995](#bib.bib357); Lampert et al., [2016](#bib.bib236)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 真实值集成过程可以使用*阈值概率图*进行推广 (Biancardi et al., [2010](#bib.bib54))。首先，将一个样本的所有真实值逐像素平均成一个*概率图*。然后将该图二值化，病变对应于大于或等于选择阈值的像素。OR、AND
    和多数投票操作分别对应于阈值 $1/n$、$1$ 和 $(n-\varepsilon)/2n$，其中 $n$ 是真实值的数量，$\varepsilon$ 是一个小的正常数。AND
    和 OR 分别对应于最紧和最松的轮廓，其他阈值会导致中间结果。虽然最佳阈值依赖于数据，但较大的阈值将评估重点放在明确的区域，从而导致对分割质量的过于乐观的评估
    (Smyth et al., [1995](#bib.bib357); Lampert et al., [2016](#bib.bib236))。
- en: The abovementioned approaches fail to consider the differences of experience
    or performance of the annotators (Warfield and Wells, [2004](#bib.bib411)). More
    elaborate ground-truth fusion alternatives include shape averaging (Rohlfing and
    Maurer, [2006](#bib.bib327)), border averaging (Chen and Parent, [1989](#bib.bib91);
    Chalana and Kim, [1997](#bib.bib84)), binary label fusion algorithms such as STAPLE
    (Warfield and Wells, [2004](#bib.bib411)), TESD (Biancardi et al., [2010](#bib.bib54)),
    and SIMPLE (Langerak et al., [2010](#bib.bib237)), as well as other more recent
    algorithms (Peng and Li, [2013](#bib.bib300); Peng et al., [2016](#bib.bib301),
    [2017a](#bib.bib302)).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法未能考虑标注者的经验或表现差异（Warfield 和 Wells，[2004](#bib.bib411)）。更复杂的真实标注融合替代方案包括形状平均（Rohlfing
    和 Maurer，[2006](#bib.bib327)）、边界平均（Chen 和 Parent，[1989](#bib.bib91)；Chalana 和
    Kim，[1997](#bib.bib84)）、如 STAPLE（Warfield 和 Wells，[2004](#bib.bib411)）、TESD（Biancardi
    等人，[2010](#bib.bib54)）和 SIMPLE（Langerak 等人，[2010](#bib.bib237)）的二值标签融合算法，以及其他更近期的算法（Peng
    和 Li，[2013](#bib.bib300)；Peng 等人，[2016](#bib.bib301)，[2017a](#bib.bib302)）。
- en: STAPLE (Simultaneous Truth And Performance Level Estimation) has been very influential
    in medical image segmentation evaluation, inspiring many variants. For each image
    and its ground-truth segmentations, STAPLE estimates a probabilistic true segmentation
    through the optimal combination of individual ground-truths, weighting each one
    by the estimated sensitivity and specificity of its annotator. STAPLE may fail
    when there are only a few annotators or when their performances vary too much (Langerak
    et al., [2010](#bib.bib237); Lampert et al., [2016](#bib.bib236)), a situation
    addressed by SIMPLE (Selective and Iterative Method for Performance Level Estimation) (Langerak
    et al., [2010](#bib.bib237)) by iteratively discarding poor quality ground-truths.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: STAPLE（Simultaneous Truth And Performance Level Estimation）在医学图像分割评估中具有很大影响，启发了许多变体。对于每个图像及其真实标注，STAPLE
    通过对单个真实标注的最佳组合来估计概率真分割，按标注者的估计灵敏度和特异性加权每一个。STAPLE 可能在标注者较少或其表现差异过大时失效（Langerak
    等人，[2010](#bib.bib237)；Lampert 等人，[2016](#bib.bib236)），这种情况由 SIMPLE（Selective
    and Iterative Method for Performance Level Estimation）（Langerak 等人，[2010](#bib.bib237)）通过迭代丢弃低质量真实标注来解决。
- en: 'Instead of attempting to fuse multiple ground-truths into a single one before
    employing conventional evaluation metrics, the metrics themselves may be modified
    to take into account annotation variability. Celebi et al. ([2009c](#bib.bib80))
    proposed the *normalized probabilistic rand index* (NPRI) (Unnikrishnan et al.,
    [2007](#bib.bib384)), a generalization of the *rand index* (Rand, [1971](#bib.bib319)).
    It penalizes segmentation results more (less) in regions where the ground-truths
    agree (disagree). Fig. [9](#S4.F9 "Figure 9 ‣ 4.2 Inter-Annotator Agreement ‣
    4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation") illustrates
    the idea: ground-truths outlined by three experienced dermatologists appear in
    red, green, and blue, while the automated result appears in black. NPRI does *not*
    penalize the automated segmentation in the upper part of the image, where the
    blue border seriously disagrees with the other two (Celebi et al., [2009c](#bib.bib80)).
    Despite its many desirable qualities, NPRI has a subtle flaw: it is non-monotonic
    with the fraction of misclassified pixels (Peserico and Silletti, [2010](#bib.bib306)).
    Consequently, this index might be unsuitable for comparing poor segmentation algorithms.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用传统评估指标之前，不是尝试将多个真实标注融合为一个，而是可以修改这些指标以考虑标注的变异性。Celebi 等人（[2009c](#bib.bib80)）提出了*标准化概率兰德指数*（NPRI）（Unnikrishnan
    等人，[2007](#bib.bib384)），这是*兰德指数*（Rand，[1971](#bib.bib319)）的推广。它在真实标注一致（不一致）的区域对分割结果施加更多（更少）的惩罚。图
    [9](#S4.F9 "Figure 9 ‣ 4.2 Inter-Annotator Agreement ‣ 4 Evaluation ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation") 说明了这个概念：三个经验丰富的皮肤科医生标注的真实标注以红色、绿色和蓝色显示，而自动化结果以黑色显示。NPRI
    *不* 在图像上部对自动分割结果施加惩罚，因为蓝色边界严重不同于其他两个标注（Celebi 等人，[2009c](#bib.bib80)）。尽管NPRI具有许多优点，但它有一个微妙的缺陷：它在误分类像素的比例上并非单调（Peserico
    和 Silletti，[2010](#bib.bib306)）。因此，这个指标可能不适合用来比较效果较差的分割算法。
- en: '![Refer to caption](img/b60caf0a36afa3c44e323ca3ae1f5429.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b60caf0a36afa3c44e323ca3ae1f5429.png)'
- en: 'Figure 9: Sample segmentation results demonstrating inter-annotator disagreements.
    Note how annotator preferences can affect the manual segmentations, e.g., smooth
    lesion borders (green), jagged lesion borders (black), oversegmented lesion (blue),
    etc. Figure taken from Celebi et al. ([2009c](#bib.bib80)) with permission.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 样本分割结果展示了注释者之间的不一致。注意注释者偏好如何影响人工分割，例如，平滑病变边界（绿色）、锯齿状病变边界（黑色）、过度分割的病变（蓝色）等。图源自
    Celebi 等 ([2009c](#bib.bib80)) 的许可。'
- en: 4.3 Evaluation Metrics
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评价指标
- en: 'We can frame the skin lesion segmentation problem as a binary pixel-wise classification
    task, where the positive and negative classes correspond to the lesion and the
    background skin, respectively. Suppose that we have an input image and its corresponding
    segmentations: an *automated segmentation* (AS) produced by a segmentation algorithm
    and a *manual segmentation* (MS) outlined by a human expert. We can formulate
    a number of quantitative segmentation evaluation measures based on the concepts
    of *true positive*, *false negative*, *false positive*, and *true negative*, whose
    definitions are given in Table [2](#S4.T2 "Table 2 ‣ 4.3 Evaluation Metrics ‣
    4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation"). In this
    table, actual and detected pixels refer to any given pixel in the MS and the corresponding
    pixel in the AS, respectively.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将皮肤病变分割问题框架化为一个二元像素级分类任务，其中正类和负类分别对应病变和背景皮肤。假设我们有一张输入图像及其相应的分割：一个由分割算法生成的*自动分割*（AS）和一个由人工专家标出的*人工分割*（MS）。我们可以基于*真正例*、*假阴性*、*假阳性*和*真阴性*的概念制定一些定量分割评价指标，这些定义见表
    [2](#S4.T2 "表 2 ‣ 4.3 评价指标 ‣ 4 评价 ‣ 深度学习在皮肤病变分割中的应用调查")。在该表中，实际像素和检测像素分别指 MS 和
    AS 中的任何给定像素。
- en: 'Table 2: Definitions of true positive, false negative, false positive, and
    true negative pixels in the context of skin lesion segmentation.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 皮肤病变分割中真正例、假阴性、假阳性和真阴性像素的定义。'
- en: '|  | Detected Pixel |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | 检测像素 |'
- en: '|  |  | Lesion $(+)$ | Background $(-)$ |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 病变 $(+)$ | 背景 $(-)$ |'
- en: '| Actual | Lesion $(+)$ | True Positive | False Negative |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 实际 | 病变 $(+)$ | 真正例 | 假阴性 |'
- en: '| Pixel | Background $(-)$ | False Positive | True Negative |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 像素 | 背景 $(-)$ | 假阳性 | 真阴性 |'
- en: For a given pair of automated and manual segmentations, we can construct a $2\times
    2$ confusion matrix (aka a contingency table (Pearson, [1904](#bib.bib299); Miller
    and Nicely, [1955](#bib.bib277))) <math alttext="\mathsf{C}=\begin{pmatrix}\textsf{TP}&amp;\textsf{FN}\\
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的一对自动和人工分割，我们可以构建一个 $2\times 2$ 混淆矩阵（也称为列联表（Pearson, [1904](#bib.bib299);
    Miller 和 Nicely, [1955](#bib.bib277)）） <math alttext="\mathsf{C}=\begin{pmatrix}\textsf{TP}&amp;\textsf{FN}\\
- en: '\textsf{FP}&amp;\textsf{TN}\end{pmatrix}" display="inline"><semantics ><mrow
    ><mi >𝖢</mi><mo  >=</mo><mrow ><mo >(</mo><mtable columnspacing="5pt" rowspacing="0pt"
    ><mtr ><mtd  ><mtext mathsize="70%" >TP</mtext></mtd><mtd ><mtext  mathsize="70%"
    >FN</mtext></mtd></mtr><mtr ><mtd ><mtext  mathsize="70%" >FP</mtext></mtd><mtd
    ><mtext mathsize="70%"  >TN</mtext></mtd></mtr></mtable><mo >)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><ci  >𝖢</ci><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><ci  ><mtext mathsize="70%" >TP</mtext></ci><ci ><mtext mathsize="70%"  >FN</mtext></ci></matrixrow><matrixrow
    ><ci ><mtext mathsize="70%"  >FP</mtext></ci><ci ><mtext mathsize="70%" >TN</mtext></ci></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathsf{C}=\begin{pmatrix}\textsf{TP}&\textsf{FN}\\
    \textsf{FP}&\textsf{TN}\end{pmatrix}</annotation></semantics></math>, where TP,
    FN, FP, and TN denote the numbers of true positives, false negatives, false positives,
    and true negatives, respectively. Clearly, we have $N=\textsf{TP}+\textsf{FN}+\textsf{FP}+\textsf{TN}$,
    where $N$ is the number of pixels in either image. Based on these quantities,
    we can define a variety of scalar similarity measures to quantify the accuracy
    of segmentation (Baldi et al., [2000](#bib.bib36); Japkowicz and Shah, [2011](#bib.bib197);
    Taha and Hanbury, [2015](#bib.bib366)):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: $\textsf{C}=\begin{pmatrix}\textsf{TP}&\textsf{FN}\\ \textsf{FP}&\textsf{TN}\end{pmatrix}$，其中
    TP、FN、FP 和 TN 分别表示真正例、假负例、假正例和真负例的数量。显然，我们有 $N=\textsf{TP}+\textsf{FN}+\textsf{FP}+\textsf{TN}$，其中
    $N$ 是任一图像中的像素数量。基于这些量，我们可以定义各种标量相似性度量来量化分割的准确性（Baldi 等，[2000](#bib.bib36)；Japkowicz
    和 Shah，[2011](#bib.bib197)；Taha 和 Hanbury，[2015](#bib.bib366)）：
- en: •
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sensitivity (SE) and Specificity (SP) (Kahn, [1942](#bib.bib207); Yerushalmy,
    [1947](#bib.bib427); Binney et al., [2021](#bib.bib56)): SE = $\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FN}}$
    & SP = $\dfrac{\textsf{TN}}{\textsf{TN}+\textsf{FP}}$'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 敏感度（SE）和特异性（SP）（Kahn，[1942](#bib.bib207)；Yerushalmy，[1947](#bib.bib427)；Binney
    等，[2021](#bib.bib56)）：SE = $\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FN}}$ & SP
    = $\dfrac{\textsf{TN}}{\textsf{TN}+\textsf{FP}}$
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Precision (PR) and Recall (RE) (Kent et al., [1955](#bib.bib223)): PR = $\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FP}}$
    & RE = $\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FN}}$'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精确度（PR）和召回率（RE）（Kent 等，[1955](#bib.bib223)）：PR = $\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FP}}$
    & RE = $\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FN}}$
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\text{Accuracy ({AC})}=\dfrac{\textsf{TP}+\textsf{TN}}{\textsf{TP}+\textsf{FN}+\textsf{FP}+\textsf{TN}}$
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率（AC）= $\dfrac{\textsf{TP}+\textsf{TN}}{\textsf{TP}+\textsf{FN}+\textsf{FP}+\textsf{TN}}$
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: F-measure (F) (van Rijsbergen, [1979](#bib.bib389)) = $\dfrac{2|\textsf{AS}\cap\textsf{MS}|}{|\textsf{AS}|+|\textsf{MS}|}=\dfrac{2\cdot\textsf{PR}\cdot\textsf{RE}}{\textsf{PR}+\textsf{RE}}=\dfrac{2\textsf{TP}}{2\textsf{TP}+\textsf{FP}+\textsf{FN}}$
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: F-measure（F）（van Rijsbergen，[1979](#bib.bib389)）= $\dfrac{2|\textsf{AS}\cap\textsf{MS}|}{|\textsf{AS}|+|\textsf{MS}|}=\dfrac{2\cdot\textsf{PR}\cdot\textsf{RE}}{\textsf{PR}+\textsf{RE}}=\dfrac{2\textsf{TP}}{2\textsf{TP}+\textsf{FP}+\textsf{FN}}$
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: G-mean (GM) (Kubat et al., [1998](#bib.bib234)) = $\sqrt{\textsf{SE}\cdot\textsf{SP}}$
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: G-mean（GM）（Kubat 等，[1998](#bib.bib234)）= $\sqrt{\textsf{SE}\cdot\textsf{SP}}$
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Balanced Accuracy (BA)  (Chou and Fasman, [1978](#bib.bib94)) = $\dfrac{\textsf{SE}+\textsf{SP}}{2}$
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平衡准确率（BA）（Chou 和 Fasman，[1978](#bib.bib94)）= $\dfrac{\textsf{SE}+\textsf{SP}}{2}$
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Jaccard index (J) (Jaccard, [1901](#bib.bib191)) = $\dfrac{|\textsf{AS}\cap\textsf{MS}|}{|\textsf{AS}\cup\textsf{MS}|}=\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FN}+\textsf{FP}}$
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Jaccard 指数（J）（Jaccard，[1901](#bib.bib191)）= $\dfrac{|\textsf{AS}\cap\textsf{MS}|}{|\textsf{AS}\cup\textsf{MS}|}=\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FN}+\textsf{FP}}$
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Matthews Correlation Coefficient (MCC) (Matthews, [1975](#bib.bib273)) = $\dfrac{\textsf{TP}\cdot\textsf{TN}-\textsf{FP}\cdot\textsf{FN}}{\sqrt{(\textsf{TP}+\textsf{FP})(\textsf{TP}+\textsf{FN})(\textsf{TN}+\textsf{FP})(\textsf{TN}+\textsf{FN})}}$
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Matthews 相关系数（MCC）（Matthews，[1975](#bib.bib273)）= $\dfrac{\textsf{TP}\cdot\textsf{TN}-\textsf{FP}\cdot\textsf{FN}}{\sqrt{(\textsf{TP}+\textsf{FP})(\textsf{TP}+\textsf{FN})(\textsf{TN}+\textsf{FP})(\textsf{TN}+\textsf{FN})}}$
- en: 'For each similarity measure, the higher the value, the better the segmentation.
    Except for MCC, all of these measures have a unit range, that is, $[0,1]$. The
    $[-1,1]$ range of MCC can be mapped to $[0,1]$ by adding one to it and then dividing
    by two. Each of these unit-range similarity measures can then be converted to
    a unit-range dissimilarity measure by subtracting it from one. Note that there
    are also dissimilarity measures with no corresponding similarity formulation.
    A prime example is the well-known XOR measure (Hance et al., [1996](#bib.bib170))
    defined as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个相似性度量，值越高，分割效果越好。除了 MCC 之外，所有这些度量都有单位范围，即 $[0,1]$。MCC 的 $[-1,1]$ 范围可以通过加一并除以二映射到
    $[0,1]$。然后，将这些单位范围的相似性度量减去一，可以转换为单位范围的差异性度量。注意，也有一些差异性度量没有对应的相似性公式。一个典型的例子是著名的
    XOR 度量（Hance 等人，[1996](#bib.bib170)），其定义如下：
- en: '|  | $\text{{XOR}}=\dfrac{&#124;\textsf{AS}\oplus\textsf{MS}&#124;}{&#124;\textsf{MS}&#124;}=\dfrac{&#124;\left(\textsf{AS}\cup\textsf{MS}\right)-\left(\textsf{AS}\cap\textsf{MS}\right)&#124;}{&#124;\textsf{MS}&#124;}=\dfrac{\textsf{FP}+\textsf{FN}}{\textsf{TP}+\textsf{FN}}.$
    |  | (18) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{{XOR}}=\dfrac{|\textsf{AS}\oplus\textsf{MS}|}{|\textsf{MS}|}=\dfrac{|\left(\textsf{AS}\cup\textsf{MS}\right)-\left(\textsf{AS}\cap\textsf{MS}\right)|}{|\textsf{MS}|}=\dfrac{\textsf{FP}+\textsf{FN}}{\textsf{TP}+\textsf{FN}}.$
    |  | (18) |'
- en: It is essential to notice that different evaluation measures capture different
    aspects of a segmentation algorithm’s performance on a given dataset, and thus
    there is no universally applicable evaluation measure (Japkowicz and Shah, [2011](#bib.bib197)).
    This is why most studies employ multiple evaluation measures in an effort to perform
    a comprehensive performance evaluation. Such a strategy, however, complicates
    algorithm comparisons, unless one algorithm completely dominates the others with
    respect to all adopted evaluation measures.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，不同的评价指标捕捉到分割算法在给定数据集上的不同方面的表现，因此没有一种通用的评价指标（Japkowicz 和 Shah, [2011](#bib.bib197)）。这就是为什么大多数研究采用多种评价指标，以便进行全面的性能评估。然而，这种策略会使算法比较变得复杂，除非一种算法在所有采用的评价指标上都完全优于其他算法。
- en: Based on their observation that experts tend to avoid missing parts of the lesion
    in their manual borders, Garnavi et al. ([2011a](#bib.bib143)) argue that true
    positives have the highest importance in the segmentation of skin lesion images.
    The authors also assert that false positives (background pixels incorrectly identified
    as part of the lesion) are less important than false negatives (lesion pixels
    incorrectly identified as part of the background). Accordingly, they assign a
    weight of $1.5$ to TP to signify its overall importance. Furthermore, in measures
    that involve both FN and FP (e.g., AC, F, and XOR), they assign a weight of 0.5
    to FP to emphasize its importance over FN. Using these weights, they construct
    a *weighted performance index*, which is an arithmetic average of six commonly
    used measures, namely SE, SP, PR, AC, F, and (unit complement of) XOR. This scalar
    evaluation measure facilitates comparisons among algorithms.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 基于他们观察到专家在手动标记中倾向于避免遗漏病变部分，Garnavi 等人 ([2011a](#bib.bib143)) 认为真正的正例在皮肤病变图像的分割中具有最高的重要性。作者还断言，假阳性（错误标识为病变部分的背景像素）比假阴性（错误标识为背景部分的病变像素）重要性要低。因此，他们为
    TP 分配了 $1.5$ 的权重，以体现其整体重要性。此外，在涉及 FN 和 FP 的指标中（例如，AC、F 和 XOR），他们为 FP 分配了 0.5 的权重，以强调其相对于
    FN 的重要性。使用这些权重，他们构建了一个*加权性能指数*，这是六种常用指标的算术平均值，即 SE、SP、PR、AC、F 和（单位补数）XOR。这个标量评价指标有助于算法之间的比较。
- en: In a follow-up study, Garnavi and Aldeen ([2011](#bib.bib142)) parameterize
    the weights of TP, FN, FP, and TN in their weighted performance index and then
    use a constrained non-linear program to determine the optimal weights. They conduct
    experiments with five segmentation algorithms on $55$ dermoscopic images. They
    conclude that the optimized weights not only lead to automated algorithms that
    are more accurate against manual segmentations, but also diminish the differences
    among those algorithms.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在一项后续研究中，Garnavi 和 Aldeen ([2011](#bib.bib142)) 对其加权性能指数中的 TP、FN、FP 和 TN 的权重进行了参数化，然后使用约束非线性程序确定最佳权重。他们在
    $55$ 张皮肤镜图像上对五种分割算法进行了实验。他们得出结论，优化后的权重不仅使自动化算法在与人工分割的比较中更准确，而且还减少了这些算法之间的差异。
- en: 'We make the following key observations about the popular evaluation metrics
    and how they have been used in the skin lesion segmentation literature:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对流行的评价指标及其在皮肤病变分割文献中的应用做出以下关键观察：
- en: •
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Historically, AC has been the most popular evaluation measure owing to its simple
    and intuitive formulation. However, this measure tends to favor the majority class,
    leading to overly optimistic performance estimates in class-imbalanced domains.
    This drawback prompted the development of more elaborate performance evaluation
    measures, including GM, BA, and MCC.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 历史上，由于其简单直观的公式，AC 一直是最受欢迎的评价指标。然而，该指标倾向于偏向多数类，导致在类别不平衡的领域中表现过于乐观。这个缺陷促使了更复杂的性能评价指标的发展，包括
    GM、BA 和 MCC。
- en: •
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SE and SP are especially popular in medical domains, tracing their usage in
    serologic test reports in the early 1900s (Binney et al., [2021](#bib.bib56)).
    SE (aka *True Positive Rate*) quantifies the accuracy on the positive class, whereas
    SP (aka *True Negative Rate*) quantifies the accuracy on the negative class. These
    measures are generally used together because it is otherwise trivial to maximize
    one at the expense of the other (an automated border enclosing the corresponding
    manual border will attain a perfect SE, whereas in the opposite case, we will
    have a perfect SP). Unlike AC, they are suitable for class-imbalanced domains.
    BA and GM combine these measures into a single evaluation measure through arithmetic
    and geometric averaging, respectively. Unlike AC, these composite measures are
    suitable for class-imbalanced domains (Luque et al., [2020](#bib.bib266)).
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SE 和 SP 在医学领域尤其受欢迎，可以追溯到 1900 年代初的血清学测试报告（Binney 等, [2021](#bib.bib56)）。SE（也称为*真正正例率*）量化了正类的准确性，而
    SP（也称为*真正负例率*）量化了负类的准确性。这些指标通常一起使用，因为否则很容易在牺牲另一个指标的情况下最大化一个指标（自动边界包围相应的手动边界将达到完美的
    SE，而在相反的情况下，我们将得到完美的 SP）。与 AC 不同，它们适用于类别不平衡的领域。BA 和 GM 分别通过算术平均和几何平均将这些指标合并为一个单一的评价指标。与
    AC 不同，这些复合指标适用于类别不平衡的领域（Luque 等, [2020](#bib.bib266)）。
- en: •
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: PR is the proportion of examples assigned to the positive class that actually
    belongs to the positive class. RE is equivalent to SE. PR and RE are typically
    used in information retrieval applications, where the focus is solely on relevant
    documents (positive class). F combines these measures into a single evaluation
    measure through harmonic averaging. This composite measure, however, is unsuitable
    for class-imbalanced domains (Zou et al., [2004](#bib.bib459); Chicco and Jurman,
    [2020](#bib.bib92); Luque et al., [2020](#bib.bib266)).
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PR 是分配到正类的示例中实际属于正类的比例。RE 等同于 SE。PR 和 RE 通常用于信息检索应用，其中重点仅在于相关文档（正类）。F 通过调和平均将这些指标合并为一个单一的评价指标。然而，这种复合指标不适用于类别不平衡的领域（Zou
    等, [2004](#bib.bib459); Chicco 和 Jurman, [2020](#bib.bib92); Luque 等, [2020](#bib.bib266)）。
- en: •
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MCC is equivalent to the *phi coefficient*, which is simply the *Pearson correlation
    coefficient* applied to binary data (Chicco and Jurman, [2020](#bib.bib92)). MCC
    values fall within the range of $[-1,1]$ with $-1$ and $1$ indicating perfect
    misclassification and perfect classification, respectively, while $0$ indicating
    a classification no better than random (Matthews, [1975](#bib.bib273)). Although
    it is biased to a certain extent (Luque et al., [2020](#bib.bib266); Zhu, [2020](#bib.bib456)),
    this measure appears to be suitable for class-imbalanced domains (Boughorbel et al.,
    [2017](#bib.bib64); Chicco and Jurman, [2020](#bib.bib92); Luque et al., [2020](#bib.bib266)).
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MCC 等同于*phi系数*，这实际上是应用于二元数据的*Pearson相关系数*（Chicco 和 Jurman, [2020](#bib.bib92)）。MCC
    值的范围是 $[-1,1]$，其中 $-1$ 和 $1$ 分别表示完全误分类和完全分类，而 $0$ 表示分类效果与随机分类没有差别（Matthews, [1975](#bib.bib273)）。尽管在一定程度上存在偏差（Luque
    等, [2020](#bib.bib266); Zhu, [2020](#bib.bib456)），但该度量似乎适用于类别不平衡的领域（Boughorbel
    等, [2017](#bib.bib64); Chicco 和 Jurman, [2020](#bib.bib92); Luque 等, [2020](#bib.bib266)）。
- en: •
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'J (aka *Intersection over Union* (Jaccard, [1912](#bib.bib192))) and F (aka
    *Dice coefficient* aka *Sørensen-Dice coefficient* (Dice, [1945](#bib.bib123);
    Sørensen, [1948](#bib.bib360))) are highly popular in medical image segmentation
    (Crum et al., [2006](#bib.bib104)). These measures are monotonically related as
    follows: $J=F/(2-F)$ and $F=2J/(1+J)$. Thus, it makes little sense to use them
    together. There are two major differences between these measures: (i) $(1-J)$is
    a proper distance metric, whereas $(1-F)$ is *not* (it violates the triangle inequality).
    (ii) It can be shown (Zijdenbos et al., [1994](#bib.bib457)) that if TN is sufficiently
    large compared to TP, FN, and FP, which is common in skin lesion segmentation,
    $F$ becomes equivalent to *Cohen’s kappa* (Cohen, [1960](#bib.bib99)), which is
    a chance-corrected measure of inter-observer agreement.'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: J(也称为*交集联合* (Jaccard, [1912](#bib.bib192)))和F(也称为*Dice系数* 或 *Sørensen-Dice系数*
    (Dice, [1945](#bib.bib123); Sørensen, [1948](#bib.bib360)))在医学图像分割中非常流行（Crum等，[2006](#bib.bib104)）。这些指标之间是单调相关的，如下所示：$J=F/(2-F)$和$F=2J/(1+J)$。因此，将它们同时使用没有太多意义。这些指标之间有两个主要区别：(i)
    $(1-J)$是一个合适的距离度量，而$(1-F)$则*不是*（它违反了三角不等式）。(ii) 可以证明（Zijdenbos等，[1994](#bib.bib457)），如果TN相对于TP，FN和FP足够大，在皮肤病变分割中很常见，$F$将等效于*Cohen's
    kappa* (Cohen, [1960](#bib.bib99)), 这是一种被纠正了机会的测量方法。
- en: •
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '•   '
- en: Among the seven composite evaluation measures given above, AC, GM, BA, and MCC
    are symmetric, that is, they are invariant to class swapping, while F, J, and
    XOR are asymmetric.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面提到的七个复合评估指标中，AC，GM，BA和MCC是对称的，即它们不受交换类别的影响，而F，J和XOR是非对称的。
- en: •
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: XOR is similar to *False Negative Rate*, that is, the unit complement of SE,
    with the exception that XOR has an extra additive TN term in its numerator. While
    XOR values are guaranteed to be nonnegative, they do *not* have a fixed upper
    bound, which makes aggregations of this measure difficult. XOR is also biased
    against small lesions (Celebi et al., [2009c](#bib.bib80)). Nevertheless, owing
    to its intuitive formulation, XOR was popular in skin lesion segmentation until
    about 2015 (Celebi et al., [2015b](#bib.bib82)).
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: XOR类似于*假阴性率*，即SE的补集，只是XOR的分子中多了一个额外的TN项。虽然XOR的值保证是非负的，但它们没有固定上界，这使得这个指标的聚合变得困难。XOR也对小病变有偏见（Celebi等，[2009c](#bib.bib80)）。然而，由于其直观的形式，XOR在2015年左右在皮肤病变分割中很流行（Celebi等，[2015b](#bib.bib82)）。
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The 2016 and 2017 ISIC Challenges (Gutman et al., [2016](#bib.bib167); Codella
    et al., [2018](#bib.bib98)) adopted five measures: AC, SE, SP, F, and J, with
    the participants ranked based on the last measure. The 2018 ISIC Challenge (Codella
    et al., [2019](#bib.bib96)) featured a *thresholded Jaccard index*, which returns
    the same value as the original J if the value is greater than or equal to a predefined
    threshold and zero otherwise. Essentially, this modified index considers automated
    segmentations yielding J values below the threshold as complete failures. The
    challenge organizers set the threshold equal to $0.65$ based on an earlier study
    (Codella et al., [2017](#bib.bib97)) that determined the average pairwise J similarities
    among the manual segmentations outlined by three expert dermatologists. Since
    the majority of papers in this survey ($168$ out of $177$ papers) use the ISIC
    datasets (Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")), we list the J for all the papers
    in Table LABEL:tab:main wherever it has been reported in the corresponding papers.
    For papers that did not report J and instead reported F, we list the computed
    J based on F and denote it with an asterisk.'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2016年和2017年的ISIC挑战（Gutman等，[2016](#bib.bib167); Codella等，[2018](#bib.bib98)）采用了五个指标：AC，SE，SP，F和J，并根据最后一个指标对参与者进行排名。2018年的ISIC挑战（Codella等，[2019](#bib.bib96)）添加了一个*阈值Jaccard指数*，当值大于或等于预定义的阈值时，返回与原始J相同的值，否则返回零。实际上，这个修改后的指数将自动分割的J值低于阈值的情况视为完全失败。根据早期研究（Codella等，[2017](#bib.bib97)），挑战的组织者将阈值设为$0.65$，该研究确定了三名专家皮肤科医生绘制的手动分割之间的平均pairwise
    J相似性。由于这份调查的大多数论文（177篇中的168篇）使用ISIC数据集（图[3](#S2.F3 "Figure 3 ‣ 2.1 Datasets ‣
    2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")），我们在表LABEL:tab:main中列出了报告了J的所有论文的J值。
- en: •
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Some of the aforementioned measures (i.e., GM and BA) have *not* been used in
    a skin lesion segmentation study yet.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尚未在皮肤病变分割研究中应用一些上述指标（即GM和BA）。
- en: •
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The evaluation measures discussed above are all region-based and thus fairly
    insensitive to border irregularities (Lee et al., [2003](#bib.bib240)), i.e.,
    indentations, and protrusions along the border. Boundary-based evaluation measures (Taha
    and Hanbury, [2015](#bib.bib366)) have *not* been used in the skin lesion segmentation
    literature much except for the symmetric Hausdorff metric (Silveira et al., [2009](#bib.bib352)),
    which is known to be sensitive to noise (Huttenlocher et al., [1993](#bib.bib184))
    and biased in favor of small lesions (Bogo et al., [2015](#bib.bib62)).
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述讨论的评估度量都是基于区域的，因此对边界不规则性（Lee et al., [2003](#bib.bib240)），即边界上的凹陷和突起，比较不敏感。基于边界的评估度量（Taha
    and Hanbury, [2015](#bib.bib366)）在皮肤病变分割文献中*并未*得到广泛应用，除了对噪声敏感的对称Hausdorff度量（Silveira
    et al., [2009](#bib.bib352)）（Huttenlocher et al., [1993](#bib.bib184)）和偏向小病变的度量（Bogo
    et al., [2015](#bib.bib62)）。
- en: 5 Discussion and Future Research
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论与未来研究
- en: In this paper, we presented an overview of DL-based skin lesion segmentation
    algorithms. A lot of work has been done in this field since the first application
    of CNNs on these images in 2015 (Codella et al., [2015](#bib.bib95)). In fact,
    the number of skin lesion segmentation papers published over the past $8$ years
    (2015–2022) is more than thrice those published over the previous $17$ years (1998–2014) (Celebi
    et al., [2015b](#bib.bib82)).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们概述了基于深度学习的皮肤病变分割算法。自从2015年首次在这些图像上应用CNNs以来，该领域已经取得了大量进展（Codella et al.,
    [2015](#bib.bib95)）。事实上，过去$8$年（2015–2022）发表的皮肤病变分割论文数量是之前$17$年（1998–2014）发表数量的三倍多（Celebi
    et al., [2015b](#bib.bib82)）。
- en: 'However, despite the large body of work, skin lesion segmentation remains an
    open problem, as evidenced by the ISIC 2018 Skin Lesion Segmentation Live Leaderboard (ISIC,
    [2018](#bib.bib185)). The live leaderboard has been open and accepting submissions
    since 2018, and even after the permitted usage of external data, the best thresholded
    Jaccard index (the metric used to rank submissions) is $83.6\%$. Additionally,
    the release of the HAM10000 lesion segmentations (Tschandl et al., [2020](#bib.bib380);
    ViDIR Dataverse, [2020](#bib.bib396)) in 2020 shows that progressively larger
    skin lesion segmentation datasets continue to be released. We believe that the
    following aspects of skin lesion segmentation via deep learning are worthy of
    future work:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管已经有大量的研究工作，皮肤病变分割仍然是一个未解的问题，正如ISIC 2018皮肤病变分割实时排行榜（ISIC, [2018](#bib.bib185)）所示。该实时排行榜自2018年以来一直开放并接受提交，即使在允许使用外部数据的情况下，最佳的阈值Jaccard指数（用于排名的指标）仍为$83.6\%$。此外，2020年发布的HAM10000病变分割数据集（Tschandl
    et al., [2020](#bib.bib380); ViDIR Dataverse, [2020](#bib.bib396)）表明，越来越大的皮肤病变分割数据集不断被发布。我们认为，基于深度学习的皮肤病变分割的以下方面值得未来的研究：
- en: •
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Mobile dermoscopic image analysis: With the availability of various inexpensive
    dermoscopes designed for smartphones, mobile dermoscopic image analysis is of
    great interest worldwide, especially in regions where access to dermatologists
    is limited. Typical DL-based image segmentation algorithms have millions of weights.
    In addition, classical CNN architectures are known to exhibit difficulty dealing
    with certain image distortions such as noise and blur (Dodge and Karam, [2016](#bib.bib125)),
    and DL-based skin lesion diagnosis models have been demonstrated to be susceptible
    to similar artifacts: various kinds of noise and blur, brightness and contrast
    changes, dark corners (Maron et al., [2021b](#bib.bib272)), bubbles, rulers, ink
    markings, etc. (Katsch et al., [2022](#bib.bib213)). Therefore, the current dermoscopic
    image segmentation algorithms may not be ideal for execution on typically resource-constrained
    mobile and edge devices, needed for patient privacy so that uploading skin images
    to remote servers is avoided. Leaner DL architectures, e.g., MobileNet (Howard
    et al., [2019](#bib.bib179)), ShuffleNet (Zhang et al., [2018](#bib.bib443)),
    EfficientNet (Tan and Le, [2019](#bib.bib369)), MnasNet (Tan et al., [2019a](#bib.bib368)),
    and UNeXt (Valanarasu and Patel, [2022](#bib.bib387)), should be investigated
    in addition to the robustness of such architectures with respect to image noise
    and blur.'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 移动皮肤镜图像分析：随着各种针对智能手机设计的廉价皮肤镜的出现，移动皮肤镜图像分析在全球范围内引起了极大兴趣，尤其是在皮肤科医生有限的地区。典型的基于深度学习（DL）的图像分割算法有数百万个权重。此外，经典的卷积神经网络（CNN）架构已知在处理某些图像失真（如噪声和模糊）方面存在困难（Dodge
    and Karam，[2016](#bib.bib125)），并且基于DL的皮肤病变诊断模型已被证明容易受到类似的伪影影响：各种噪声和模糊，亮度和对比度变化，黑暗角落（Maron
    et al.，[2021b](#bib.bib272)），气泡，尺子，墨水标记等（Katsch et al.，[2022](#bib.bib213)）。因此，当前的皮肤镜图像分割算法可能不适合在资源受限的移动和边缘设备上执行，这些设备需要保护患者隐私，以避免将皮肤图像上传到远程服务器。应调查更精简的DL架构，例如MobileNet（Howard
    et al.，[2019](#bib.bib179)），ShuffleNet（Zhang et al.，[2018](#bib.bib443)），EfficientNet（Tan
    and Le，[2019](#bib.bib369)），MnasNet（Tan et al.，[2019a](#bib.bib368)）和UNeXt（Valanarasu
    and Patel，[2022](#bib.bib387)），以及这些架构在图像噪声和模糊方面的鲁棒性。
- en: •
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Datasets: To train more accurate and robust deep neural segmentation architectures,
    we need larger, more diverse, and more representative skin lesion datasets with
    multiple manual segmentations per image. Additionally, as mentioned in Section [2.1](#S2.SS1
    "2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation"),
    several skin lesion image classification datasets do not have the corresponding
    lesion mask annotations, and given their popularity in skin image analysis tasks,
    they may be good targets for manual delineations. For example, the PAD-UFES-20
    dataset (Pacheco et al., [2020](#bib.bib294)) consists of clinical images of skin
    lesions captured using smartphones, and obtaining ground-truth segmentations on
    this dataset would help advance skin image analysis on mobile devices. Additionally,
    a recent study conducted by Daneshjou et al. ([2021a](#bib.bib110)) found that
    as little as 10% of the AI-based studies for dermatological diagnosis included
    skin tone information for at least one dataset used, and that several studies
    included little to no images of darker skin tones, underlining the need to curate
    datasets with diverse skin tones.'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集：为了训练更准确和更强大的深度神经网络分割架构，我们需要更大、更具多样性和更具代表性的皮肤病变数据集，每张图像有多个手动分割。此外，如第[2.1节](#S2.SS1
    "2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")中提到的，一些皮肤病变图像分类数据集没有相应的病变掩膜注释，鉴于它们在皮肤图像分析任务中的受欢迎程度，它们可能是手动勾画的良好目标。例如，PAD-UFES-20数据集（Pacheco
    et al.，[2020](#bib.bib294)）包含使用智能手机拍摄的皮肤病变的临床图像，在该数据集上获取真实分割将有助于推进移动设备上的皮肤图像分析。此外，Daneshjou
    et al.（[2021a](#bib.bib110)）进行的最新研究发现，只有10%的皮肤病学诊断的AI研究包含了至少一个数据集的肤色信息，而且一些研究几乎没有包含深色肤色的图像，这突显了整理具有多样肤色数据集的必要性。
- en: •
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Collecting segmentation annotations: At the time of this writing, the ISIC
    Archive contains over $71,000$ publicly available images. Considering that the
    largest public dermoscopic image set contained a little over $1,000$ images about
    six years ago, we have come a long way. The more pressing problem now is the lack
    of manual segmentations for most of these images. Since manual segmentation by
    medical experts is laborious and costly, crowdsourcing techniques (Kovashka et al.,
    [2016](#bib.bib232)) could be explored to collect annotations from non-experts.
    Experts could then revise these initial annotations, or methods that tackle the
    problem of annotation noise (Mirikharaji et al., [2019](#bib.bib281); Karimi et al.,
    [2020](#bib.bib210); Li et al., [2021a](#bib.bib245)) could be explored. Note
    that the utility of crowdsourcing in medical image annotation has been demonstrated
    in multiple studies (Foncubierta-Rodriguez and Muller, [2012](#bib.bib136); Gurari
    et al., [2015](#bib.bib166); Sharma et al., [2017](#bib.bib348); Goel et al.,
    [2020](#bib.bib149)). Additionally, keeping in mind the time-consuming nature
    of manual supervised annotation, an alternative is to use weakly-supervised annotation,
    e.g., bounding-box annotations (Dai et al., [2015](#bib.bib108); Papandreou et al.,
    [2015](#bib.bib297)), which are much less time-consuming to collect. For example,
    for several large skin lesion image datasets that do not have any lesion mask
    annotations (see Section [2.1](#S2.SS1 "2.1 Datasets ‣ 2 Input Data ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), bounding-box lesion annotations
    can be obtained more easily than dense pixel-level segmentation annotations. In
    addition, weakly-supervised annotation (Bearman et al., [2016](#bib.bib44); Tajbakhsh
    et al., [2020](#bib.bib367); Roth et al., [2021](#bib.bib331); En and Guo, [2022](#bib.bib132))
    is more amenable to crowdsourcing (Maier-Hein et al., [2014](#bib.bib269); Rajchl
    et al., [2016](#bib.bib313); Papadopoulos et al., [2017](#bib.bib296); Lin et al.,
    [2019](#bib.bib254)), especially for non-experts.'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 收集分割注释：截至本文撰写时，ISIC档案馆包含超过$71,000$张公开可用的图像。考虑到六年前最大公开皮肤镜图像集仅包含略多于$1,000$张图像，我们已经取得了长足的进步。当前更紧迫的问题是大多数这些图像缺乏人工分割。由于医疗专家的人工分割既费时又昂贵，可以探索众包技术（Kovashka等，[2016](#bib.bib232)）来收集非专家的注释。然后，专家可以修订这些初步注释，或者可以探索解决注释噪声问题的方法（Mirikharaji等，[2019](#bib.bib281);
    Karimi等，[2020](#bib.bib210); Li等，[2021a](#bib.bib245)）。值得注意的是，众包在医学图像注释中的效用已经在多项研究中得到了证明（Foncubierta-Rodriguez和Muller，[2012](#bib.bib136);
    Gurari等，[2015](#bib.bib166); Sharma等，[2017](#bib.bib348); Goel等，[2020](#bib.bib149)）。此外，考虑到人工监督注释的时间消耗，另一种选择是使用弱监督注释，例如边界框注释（Dai等，[2015](#bib.bib108);
    Papandreou等，[2015](#bib.bib297)），这些注释的收集耗时要少得多。例如，对于一些没有病变掩码注释的大型皮肤病变图像数据集（见第[2.1节](#S2.SS1
    "2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")），边界框病变注释比密集像素级分割注释更容易获得。此外，弱监督注释（Bearman等，[2016](#bib.bib44);
    Tajbakhsh等，[2020](#bib.bib367); Roth等，[2021](#bib.bib331); En和Guo，[2022](#bib.bib132)）更适合众包（Maier-Hein等，[2014](#bib.bib269);
    Rajchl等，[2016](#bib.bib313); Papadopoulos等，[2017](#bib.bib296); Lin等，[2019](#bib.bib254)），尤其是对于非专家。
- en: •
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Handling multiple annotations per image: If the skin lesion image dataset at
    hand contains multiple manual segmentations per image, one should consider either
    using an algorithm such as STAPLE (Warfield and Wells, [2004](#bib.bib411)) for
    fusing the manual segmentations (see Section [4](#S4 "4 Evaluation ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), or relying on learning-based
    approaches, either through variants of STAPLE adapted for DL-based segmentation (Kats
    et al., [2019](#bib.bib212); Zhang et al., [2020b](#bib.bib440)), or other methods (Mirikharaji
    et al., [2021](#bib.bib278); Lemay et al., [2022](#bib.bib242)). Such a fusion
    algorithm can also be used to build an ensemble of multiple automated segmentations.'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '处理每张图像的多个注释：如果手头的皮肤病变图像数据集包含每张图像的多个人工分割，则应考虑使用如STAPLE（Warfield和Wells，[2004](#bib.bib411)）的算法来融合这些人工分割（见第[4节](#S4
    "4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation")），或者依赖基于学习的方法，无论是通过适用于DL基础分割的STAPLE变体（Kats等，[2019](#bib.bib212);
    Zhang等，[2020b](#bib.bib440)），还是其他方法（Mirikharaji等，[2021](#bib.bib278); Lemay等，[2022](#bib.bib242)）。这种融合算法也可以用来构建多个自动分割的集成。 '
- en: •
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Supervised segmentation evaluation measures: Supervised segmentation evaluation
    measures popular in the skin image analysis literature (see Section [4.3](#S4.SS3
    "4.3 Evaluation Metrics ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")) are often region-based, pair-counting measures. Other region-based
    measures, such as information-theoretic measures (e.g., mutual information, variation
    of information, etc.) as well as boundary-based measures e.g., Hausdorff distance
    (Taha and Hanbury, [2015](#bib.bib366)) should be explored as well.'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 监督分割评估措施：在皮肤图像分析文献（参见第[4.3](#S4.SS3 "4.3 Evaluation Metrics ‣ 4 Evaluation ‣
    A Survey on Deep Learning for Skin Lesion Segmentation")节）中受欢迎的监督分割评估措施通常是基于区域的，成对计数的措施。其他基于区域的措施，如信息论措施（例如，互信息，信息变化等），以及基于边界的措施，例如豪斯多夫距离（Taha
    and Hanbury，[2015](#bib.bib366)）也应该被探索。
- en: •
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unsupervised segmentation and unsupervised segmentation evaluation: Current
    DL-based skin lesion segmentation algorithms are mostly based on supervised learning,
    as shown in a supervision-level breakdown of the surveyed works (Fig. [5](#S2.F5
    "Figure 5 ‣ 2.3 Supervised, Semi-supervised, Weakly supervised, Self-supervised
    learning ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")),
    meaning that these algorithms require manual segmentations for training segmentation
    prediction models. Nearly all of these segmentation studies employ supervised
    segmentation evaluation, meaning that they also require manual segmentations for
    testing. Due to the scarcity of annotated skin lesion images, it may be beneficial
    to investigate unsupervised DL (Ji et al., [2019](#bib.bib201)) as well as unsupervised
    segmentation evaluation (Chabrier et al., [2006](#bib.bib83); Zhang et al., [2008](#bib.bib438)).'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无监督分割和无监督分割评估：当前基于DL的皮肤病变分割算法大多是基于监督学习的，如调查研究作品的监督级别分布所示（图[5](#S2.F5 "Figure
    5 ‣ 2.3 Supervised, Semi-supervised, Weakly supervised, Self-supervised learning
    ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")），这意味着这些算法需要手动进行分割以训练分割预测模型。几乎所有这些分割研究都采用了监督分割评估，这也意味着它们需要手动进行分割以进行测试。由于标注的皮肤病变图像的稀缺性，研究无监督DL（Ji
    et al.，[2019](#bib.bib201)）以及无监督分割评估（Chabrier et al.，[2006](#bib.bib83); Zhang
    et al.，[2008](#bib.bib438)）可能是有益的。
- en: •
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Systematic evaluations: Systematic evaluations that have been performed for
    skin lesion classification (Valle et al., [2020](#bib.bib388); Bissoto et al.,
    [2021](#bib.bib61); Perez et al., [2018](#bib.bib305)) are, so far, nonexistent
    in the skin lesion segmentation literature. For example, statistical significance
    analysis are conducted on the results of a few prior studies in skin lesion segmentation,
    e.g., Fortina et al. ([2012](#bib.bib137)).'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统评估：到目前为止，对皮肤病变分割文献中进行的系统评估几乎没有。例如，在一些先前的皮肤病变分割研究结果上进行了统计显著性分析，例如Fortina et
    al.（[2012](#bib.bib137)）。
- en: •
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fusion of hand-crafted and deep features: Can we integrate the deep features
    extracted by DL models and hand-crafted features synergistically? For example,
    exploration of shape and appearance priors of skin lesions that may be beneficial
    to incorporate, via loss terms (Nosrati and Hamarneh, [2016](#bib.bib290); El Jurdi
    et al., [2021](#bib.bib130); Ma et al., [2021](#bib.bib267)), in deep learning
    models for skin lesion segmentation, similar to star-shape (Mirikharaji and Hamarneh,
    [2018](#bib.bib279)) and boundary priors (Wang et al., [2021a](#bib.bib399)).'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手工制作和深度特征的融合：我们能否协同地整合DL模型提取的深度特征和手工制作的特征？例如，探索皮肤病变的形状和外观先验，这可能有益于通过损失项（Nosrati
    and Hamarneh，[2016](#bib.bib290); El Jurdi et al.，[2021](#bib.bib130); Ma et al.，[2021](#bib.bib267)）纳入到皮肤病变分割的深度学习模型中，类似于星形（Mirikharaji
    and Hamarneh，[2018](#bib.bib279)）和边界先验（Wang et al.，[2021a](#bib.bib399)）。
- en: •
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Loss of spatial resolution: The use of repeated subsampling in CNNs leads to
    coarse segmentations. Various approaches have been proposed to minimize the loss
    of spatial resolution, including fractionally-strided convolution (or deconvolution) (Long
    et al., [2015](#bib.bib264)), atrous (or dilated) convolution (Chen et al., [2017a](#bib.bib86)),
    and conditional random fields (Krahenbuhl and Koltun, [2011](#bib.bib233)). More
    research needs to be conducted to determine appropriate strategies for skin lesion
    segmentation that effectively minimize or avoid the loss of spatial resolution.'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空间分辨率的损失：CNN中使用重复的下采样会导致粗略的分割。为了最小化空间分辨率的损失，提出了各种方法，包括分数步幅卷积（或反卷积）（Long et al.,
    [2015](#bib.bib264)）、空洞卷积（Chen et al., [2017a](#bib.bib86)）和条件随机场（Krahenbuhl and
    Koltun, [2011](#bib.bib233)）。需要进一步研究以确定适用于皮肤病变分割的合适策略，从而有效地最小化或避免空间分辨率的损失。
- en: •
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hyperparameter tuning: Compared to traditional machine learning classifiers
    (e.g., nearest neighbors, decision trees, and support vector machines), deep neural
    networks have a large number of hyperparameters related to their architecture,
    optimization, and regularization. An average CNN classifier has about a dozen
    or more hyperparameters (Bengio, [2012](#bib.bib45)) and tuning these hyperparameters
    systematically is a laborious undertaking. *Neural architecture search* is an
    active area of research (Elsken et al., [2019](#bib.bib131)), and some of these
    model selection approaches have already been applied to semantic segmentation (Liu
    et al., [2019a](#bib.bib257)) and medical image segmentation (Weng et al., [2019](#bib.bib413)).'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数调整：与传统的机器学习分类器（如最近邻、决策树和支持向量机）相比，深度神经网络具有大量与其架构、优化和正则化相关的超参数。一个普通的CNN分类器有大约十几个或更多的超参数（Bengio,
    [2012](#bib.bib45)），系统地调整这些超参数是一项艰巨的任务。*神经架构搜索*是一个活跃的研究领域（Elsken et al., [2019](#bib.bib131)），其中一些模型选择方法已经应用于语义分割（Liu
    et al., [2019a](#bib.bib257)）和医学图像分割（Weng et al., [2019](#bib.bib413)）。
- en: •
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reproducibility of results: Kapoor and Narayanan ([2022](#bib.bib209)) define
    research in ML-based science to be reproducible if the associated datasets and
    the code are publicly available and if there are no problems with the data analysis,
    where problems include the lack of well-defined training and testing partitions
    of the dataset, leakage across dataset partitions, features selection using the
    entire dataset instead of only the training partition, etc. Since several skin
    lesion segmentation datasets come with standardized partitions (Table [1](#S2.T1
    "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")), sharing of the code can lead to more reproducible research (Colliot
    et al., [2022](#bib.bib100)), with the added benefit to researchers who release
    their code to be cited significantly more (Vandewalle, [2012](#bib.bib390)). In
    our analysis, we found that only $38$ of the $177$ surveyed papers ($21.47\%$)
    had publicly accessible code (Table LABEL:tab:main), a proportion similar to a
    smaller-scale analysis by Renard et al. ([2020](#bib.bib325)) for medical image
    segmentation. Another potential assessment of a method’s generalization performance
    is its evaluation on a common held-out test set, where the ground truth segmentation
    masks are private, and users submit their test predictions to receive a performance
    assessment. For example, the ISIC 2018 dataset’s test partition is available through
    a live leaderboard (ISIC, [2018](#bib.bib185)), but it is rarely used. We found
    that out of $71$ papers published in 2021 and 2022 included in this survey, $36$
    papers reported results on the ISIC 2018 dataset, but only 1 paper (Saini et al.,
    [2021](#bib.bib340)) used the online submission platform for evaluation.'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果的可重复性：Kapoor 和 Narayanan ([2022](#bib.bib209)) 定义 ML 基础科学的研究为可重复的条件是相关数据集和代码是公开的，并且数据分析没有问题，其中问题包括数据集的训练和测试分区不明确、数据集分区之间的泄漏、使用整个数据集而不是仅使用训练分区进行特征选择等。由于多个皮肤病变分割数据集附带标准化分区（表
    [1](#S2.T1 "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation")），代码的共享可以导致更具可重复性的研究（Colliot 等，[2022](#bib.bib100)），并且发布代码的研究人员会被引用更多（Vandewalle，[2012](#bib.bib390)）。在我们的分析中，我们发现仅有
    $38$ 篇被调查的 $177$ 篇论文（$21.47\%$）具有公开访问的代码（表 LABEL:tab:main），这一比例与 Renard 等人（[2020](#bib.bib325)）对医学图像分割的小规模分析相似。另一个评估方法泛化性能的潜在评估是其在常见的保留测试集上的评估，其中真实分割掩模是私有的，用户提交他们的测试预测以获得性能评估。例如，ISIC
    2018 数据集的测试分区通过实时排行榜提供（ISIC，[2018](#bib.bib185)），但很少使用。我们发现，在 2021 年和 2022 年中包含在这次调查中的
    $71$ 篇论文中，有 $36$ 篇报告了 ISIC 2018 数据集的结果，但只有 1 篇论文（Saini 等，[2021](#bib.bib340)）使用了在线提交平台进行评估。
- en: '![Refer to caption](img/f7d19e3b7a821257d875dd32c0ff5351.png)'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见标题](img/f7d19e3b7a821257d875dd32c0ff5351.png)'
- en: 'Figure 10: Number of skin lesion images with ground-truth segmentation maps
    per year categorized based on modality. It is evident that while the number of
    dermoscopic skin lesion images has been constantly on the rise, the number of
    clinical images has remained unchanged for the past several years.'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10：按类别基于模态的皮肤病变图像与真实分割图的数量。显然，尽管皮肤镜皮肤病变图像的数量持续增加，但临床图像的数量在过去几年里保持不变。
- en: •
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Research on clinical images: Another limitation is the limited number of benchmark
    datasets of clinical skin lesion images with expert pixel-level annotations. Fig. [10](#S5.F10.1
    "Figure 10 ‣ 11st item ‣ 5 Discussion and Future Research ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation") shows that while the number of dermoscopic image
    datasets with ground-truth segmentation masks has been increasing over the last
    few years, only a few datasets with clinical images are available. In contrast
    to dermoscopic images requiring a special tool that is not always utilized even
    by dermatologists (Engasser and Warshaw, [2010](#bib.bib133)), clinical images
    captured by digital cameras or smartphones have the advantage of easy accessibility,
    which can be utilized to evaluate the priority of patients by their lesion severity
    level, i.e., triage patients. As shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Datasets
    ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation") and
    Table LABEL:tab:main, most of the deep skin lesion segmentation models are trained
    and evaluated on dermoscopic images, primarily because of the lack of large-scale
    clinical skin lesion image segmentation datasets (Table [1](#S2.T1 "Table 1 ‣
    2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")),
    leaving the need to develop automated tools for non-specialists unmet.'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 临床图像研究：另一个限制是缺乏带有专家像素级注释的临床皮肤病变图像的基准数据集。如图[10](#S5.F10.1 "Figure 10 ‣ 11st item
    ‣ 5 Discussion and Future Research ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")所示，尽管近年来具有真实分割掩模的皮肤镜图像数据集数量有所增加，但可用的临床图像数据集仍然很少。与需要特殊工具的皮肤镜图像相比，即使是皮肤科医生也不总是使用这些工具（Engasser
    和 Warshaw，[2010](#bib.bib133)），由数字相机或智能手机拍摄的临床图像具有易于获取的优势，这可以用于根据病变严重程度评估患者的优先级，即对患者进行分诊。如图[3](#S2.F3
    "Figure 3 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")和表LABEL:tab:main所示，大多数深度皮肤病变分割模型是在皮肤镜图像上训练和评估的，主要是由于缺乏大规模临床皮肤病变图像分割数据集（表[1](#S2.T1
    "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")），这导致了对非专业人员自动化工具的需求未得到满足。
- en: •
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Research on total body images: While there has been some research towards detecting
    and tracking skin lesions over time in 2D wide-field images (Mirzaalian et al.,
    [2016](#bib.bib282); Li et al., [2017](#bib.bib250); Korotkov et al., [2019](#bib.bib230);
    Soenksen et al., [2021](#bib.bib358); Huang et al., [2022](#bib.bib183)) and in
    3D total body images (Bogo et al., [2014](#bib.bib63); Zhao et al., [2022a](#bib.bib451);
    Ahmedt-Aristizabal et al., [2023](#bib.bib13)), simultaneous segmentation of skin
    lesions from total body images (Sinha et al., [2023](#bib.bib356)) would help
    with early detection of melanoma (Halpern, [2003](#bib.bib169); Hornung et al.,
    [2021](#bib.bib178)), thus improving patient outcomes.'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全身图像研究：虽然已有一些研究致力于在2D宽视场图像（Mirzaalian et al., [2016](#bib.bib282)；Li et al.,
    [2017](#bib.bib250)；Korotkov et al., [2019](#bib.bib230)；Soenksen et al., [2021](#bib.bib358)；Huang
    et al., [2022](#bib.bib183)）和3D全身图像（Bogo et al., [2014](#bib.bib63)；Zhao et al.,
    [2022a](#bib.bib451)；Ahmedt-Aristizabal et al., [2023](#bib.bib13)）中检测和追踪皮肤病变的研究，皮肤病变从全身图像中同时分割（Sinha
    et al., [2023](#bib.bib356)）将有助于早期检测黑色素瘤（Halpern, [2003](#bib.bib169)；Hornung
    et al., [2021](#bib.bib178)），从而改善患者的预后。
- en: •
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Effect on downstream tasks: End-to-end systems have been proposed for skin
    images analysis tasks that directly learn the final tasks (e.g., predicting the
    diagnosis (Kawahara et al., [2019](#bib.bib219)) or the clinical management decisions (Abhishek
    et al., [2021](#bib.bib8)) of skin lesions), and these approaches present a number
    of advantages such as computational efficiency and ease of optimization. On the
    other hand, skin lesion diagnosis pipelines have been shown to benefit from the
    incorporation of prior knowledge, specifically lesion segmentation masks (Yan
    et al., [2019](#bib.bib424)). Therefore, it is worth investigating how lesion
    segmentation, often an intermediate step in the skin image analysis pipeline,
    affects the downstream dermatological tasks.'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下游任务的影响：已经提出了针对皮肤图像分析任务的端到端系统，这些系统直接学习最终任务（例如，预测诊断（Kawahara et al., [2019](#bib.bib219)）或皮肤病变的临床管理决策（Abhishek
    et al., [2021](#bib.bib8)），这些方法具有计算效率高和优化容易等优点。另一方面，皮肤病变诊断流程已被证明从前期知识的融入中受益，特别是病变分割掩模（Yan
    et al., [2019](#bib.bib424)）。因此，值得研究病变分割，作为皮肤图像分析流程中的一个中间步骤，如何影响下游的皮肤科任务。
- en: •
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'From binary to multi-class segmentation: While the existing work in skin lesion
    segmentation is mainly binary segmentation, future work may explore multi-class
    settings. For example, automated detection and delineation of clinical dermoscopic
    features (e.g., globules, streaks, pigment networks) within a skin lesion may
    lead to superior classification performance. Further, dermoscopic feature extraction,
    a task in the ISIC 2016 (Gutman et al., [2016](#bib.bib167)) and 2017 (Codella
    et al., [2018](#bib.bib98)) challenges, can be formulated as a multi-class segmentation
    problem (Kawahara and Hamarneh, [2018](#bib.bib220)). The multiclass formulation
    can then be addressed by DL models, and can be used either as an intermediate
    step for improving skin lesion diagnosis or used directly in diagnosis models
    for regularizing attention maps (Yan et al., [2019](#bib.bib424)). Similarly,
    multi-class segmentation scenarios may also include multiple skin pathologies
    on one subject, especially in images with large fields of view, or segmentation
    of the skin, the lesion(s), and the background, especially in in-the-wild images
    with diverse backgrounds, such as those in the Fitzpatrick17k dataset (Groh et al.,
    [2021](#bib.bib158)).'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从二分类到多分类分割：尽管现有的皮肤病变分割工作主要集中在二分类分割上，但未来的工作可能会探索多分类设置。例如，对皮肤病变中临床皮肤镜特征（如小球、条纹、色素网络）的自动检测和描绘可能会提高分类性能。此外，皮肤镜特征提取任务，如在
    ISIC 2016（Gutman et al., [2016](#bib.bib167)）和 2017（Codella et al., [2018](#bib.bib98)）挑战中的任务，可以被表述为一个多分类分割问题（Kawahara
    and Hamarneh, [2018](#bib.bib220)）。这种多分类表述可以通过深度学习（DL）模型来解决，并可以作为改进皮肤病变诊断的中间步骤，或直接用于诊断模型以规范化注意力图（Yan
    et al., [2019](#bib.bib424)）。类似地，多分类分割场景还可能包括一个对象上的多种皮肤病理，特别是在视野广阔的图像中，或者分割皮肤、病变和背景，特别是在背景多样的实景图像中，例如
    Fitzpatrick17k 数据集中的图像（Groh et al., [2021](#bib.bib158)）。
- en: •
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transferability of models: As the majority of skin lesion datasets are from
    fair-skinned patients, the generalizability of deep models to populations with
    diverse skin complexions is questionable. With the emergence of dermatological
    datasets with diverse skin tones (Groh et al., [2021](#bib.bib158); Daneshjou
    et al., [2021b](#bib.bib111)) and methods for diagnosing pathologies fairly (Bevan
    and Atapour-Abarghouei, [2022](#bib.bib47); Wu et al., [2022c](#bib.bib418); Pakzad
    et al., [2023](#bib.bib295); Du et al., [2023](#bib.bib128)), it is important
    to assess the transferability of DL-based skin lesion segmentation models to datasets
    with diverse skin tones.'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的迁移性：由于大多数皮肤病变数据集来自肤色较浅的患者，因此深度模型对肤色多样的群体的普适性存在疑问。随着包含多样肤色的皮肤病学数据集的出现（Groh
    et al., [2021](#bib.bib158)；Daneshjou et al., [2021b](#bib.bib111)）以及用于诊断病理的有效方法（Bevan
    and Atapour-Abarghouei, [2022](#bib.bib47)；Wu et al., [2022c](#bib.bib418)；Pakzad
    et al., [2023](#bib.bib295)；Du et al., [2023](#bib.bib128)），评估基于深度学习的皮肤病变分割模型在多样肤色数据集上的迁移性变得十分重要。
- en: 6 Acknowledgements
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 致谢
- en: The authors would like to acknowledge Ben Cardoen and Aditi Jain for help with
    proofreading the manuscript and with creating the interactive table, respectively.
    Z. Mirikharaji, K. Abhishek, and G. Hamarneh are partially funded by the BC Cancer
    Foundation - BrainCare BC Fund, the Natural Sciences and Engineering Research
    Council of Canada (NSERC RGPIN-06752), and the Canadian Institutes of Health Research
    (CIHR OQI-137993). A. Bissoto is partially funded by FAPESP 2019/19619-7\. E.
    Valle is partially funded by CNPq 315168/2020-0\. S. Avila is partially funded
    by CNPq PQ-2 315231/2020-3, and FAPESP 2013/08293-7\. A. Bissoto and S. Avila
    are also partially funded by Google LARA 2020\. The RECOD.ai lab is supported
    by projects from FAPESP, CNPq, and CAPES. C. Barata is funded by FCT project and
    multi-year funding [CEECIND/00326/2017] and LARSyS - FCT Plurianual funding 2020-2023\.
    M. E. Celebi was supported by the US National Science Foundation under Award No.
    OIA-1946391\. Any opinions, findings, and conclusions or recommendations expressed
    in this material are those of the authors and do not necessarily reflect the views
    of the National Science Foundation.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 作者要感谢 Ben Cardoen 和 Aditi Jain 对手稿进行校对以及创建互动表格的帮助。Z. Mirikharaji、K. Abhishek
    和 G. Hamarneh 部分由 BC Cancer Foundation - BrainCare BC Fund、加拿大自然科学与工程研究委员会（NSERC
    RGPIN-06752）和加拿大健康研究院（CIHR OQI-137993）资助。A. Bissoto 部分由 FAPESP 2019/19619-7 资助。E.
    Valle 部分由 CNPq 315168/2020-0 资助。S. Avila 部分由 CNPq PQ-2 315231/2020-3 和 FAPESP
    2013/08293-7 资助。A. Bissoto 和 S. Avila 还部分由 Google LARA 2020 资助。RECOD.ai 实验室得到
    FAPESP、CNPq 和 CAPES 项目的支持。C. Barata 由 FCT 项目和多年资助 [CEECIND/00326/2017] 以及 LARSyS
    - FCT Plurianual 2020-2023 资助。M. E. Celebi 获得了美国国家科学基金会的支持，奖励号为 OIA-1946391。任何意见、发现、结论或建议均为作者个人观点，不一定反映美国国家科学基金会的观点。
- en: 'Table 3: DL models for skin lesion segmentation. Performance measure reported
    is the Jaccard index computed on the dataset, shown in boldface. The score is
    asterisked if it is computed based on the reported Dice index. The following abbreviations
    are used: Ref.: reference, Arch.: architecture, Seg.: segmentation, J: Jaccard
    index, CDE : cross-data evaluation. the highlighted dataset and PP: postprocessing,
    con.: connection and conv.: convolution, CE: cross-entropy, WCE: weighted cross-entropy,
    DS: deep supervision, EPE: end point error, $\ell_{1}$: $\ell_{1}$ norm, $\ell_{2}$:
    $\ell_{2}$ norm and ADV: adversarial loss. Please see the corresponding sections
    for more details: Section [3.1](#S3.SS1 "3.1 Architecture ‣ 3 Model Design and
    Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation") for model
    architectures, Section [3.2](#S3.SS2 "3.2 Loss Functions ‣ 3 Model Design and
    Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation") for loss functions,
    and Section [4](#S4 "4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation") for model evaluation. An interactive version of this table is available
    online at [https://github.com/sfu-mial/skin-lesion-segmentation-survey](https://github.com/sfu-mial/skin-lesion-segmentation-survey).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：皮肤病变分割的深度学习模型。报告的性能度量是基于数据集计算的Jaccard指数，以粗体显示。如果计算基于报告的Dice指数，则分数以星号标记。以下缩写被使用：Ref.：参考文献，Arch.：架构，Seg.：分割，J：Jaccard指数，CDE：跨数据评估。突出显示的数据集和PP：后处理，con.：连接，conv.：卷积，CE：交叉熵，WCE：加权交叉熵，DS：深度监督，EPE：端点误差，$\ell_{1}$：$\ell_{1}$
    范数，$\ell_{2}$：$\ell_{2}$ 范数，ADV：对抗损失。有关更多详细信息，请参阅相应部分：第[3.1](#S3.SS1 "3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")节了解模型架构，第[3.2](#S3.SS2
    "3.2 Loss Functions ‣ 3 Model Design and Training ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation")节了解损失函数，第[4](#S4 "4 Evaluation ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation")节了解模型评估。该表格的互动版本可以在 [https://github.com/sfu-mial/skin-lesion-segmentation-survey](https://github.com/sfu-mial/skin-lesion-segmentation-survey)
    上在线查看。
- en: '| Ref. | Venue | Data | Arch. modules | Seg. loss | J | CDE | Augmentation
    | PP | code |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Ref. | 场地 | 数据 | 架构模块 | 分割损失 | J | CDE | 增强 | PP | 代码 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Jafari et al. ([2016](#bib.bib194)) | peer-reviewed conference | DermQuest
    | image pyramid | - | - | ✗ | - | ✓ | ✗ |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Jafari 等人 ([2016](#bib.bib194)) | 同行评审会议 | DermQuest | 图像金字塔 | - | - | ✗
    | - | ✓ | ✗ |'
- en: '| He et al. ([2017](#bib.bib175)) | peer-reviewed conference | ISIC2016 ISIC2017
    | residual con. skip con. image pyramid | Dice CE DS | 75.80% | ✗ | rotation |
    ✓ | ✗ |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| He 等人 ([2017](#bib.bib175)) | 同行评审会议 | ISIC2016 ISIC2017 | 残差连接跳跃连接图像金字塔
    | Dice CE DS | 75.80% | ✗ | 旋转 | ✓ | ✗ |'
- en: '| Bozorgtabar et al. ([2017b](#bib.bib66)) | peer-reviewed journal | ISIC2016
    | - | - | 80.60% | ✗ | rotation | ✗ | ✗ |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| Bozorgtabar 等（[2017b](#bib.bib66)） | 同行评审期刊 | ISIC2016 | - | - | 80.60% |
    ✗ | 旋转 | ✗ | ✗ |'
- en: '| Ramachandram and Taylor ([2017](#bib.bib315)) | peer-reviewed journal | ISIC2017
    | - | CE | 79.20% | ✗ | rotation, flipping color jittering | ✗ | ✗ |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Ramachandram 和 Taylor（[2017](#bib.bib315)） | 同行评审期刊 | ISIC2017 | - | CE |
    79.20% | ✗ | 旋转、翻转、颜色抖动 | ✗ | ✗ |'
- en: '| Yu et al. ([2017a](#bib.bib431)) | peer-reviewed journal | ISIC2016 | skip
    con. residual con. | - | 82.90% | ✗ | rotation,translation random noise cropping
    | ✗ | ✓ |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Yu 等（[2017a](#bib.bib431)） | 同行评审期刊 | ISIC2016 | 跳过卷积，残差卷积 | - | 82.90% |
    ✗ | 旋转、平移、随机噪声裁剪 | ✗ | ✓ |'
- en: '| Bi et al. ([2017b](#bib.bib53)) | peer-reviewed journal | ISIC2016 PH² |
    - | CE | 84.64% | ✓ | flipping,cropping | ✓ | ✗ |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| Bi 等（[2017b](#bib.bib53)） | 同行评审期刊 | ISIC2016 PH² | - | CE | 84.64% | ✓ |
    翻转、裁剪 | ✓ | ✗ |'
- en: '| Jafari et al. ([2017](#bib.bib195)) | peer-reviewed journal | DermQuest |
    image pyramid | - | - | ✗ | - | ✓ | ✗ |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| Jafari 等（[2017](#bib.bib195)） | 同行评审期刊 | DermQuest | 图像金字塔 | - | - | ✗ |
    - | ✓ | ✗ |'
- en: '| Yuan et al. ([2017](#bib.bib433)) | peer-reviewed journal | ISIC2016 PH²
    | - | Tanimoto | 84.7% | ✓ | flipping, rotation scaling,shifting contrast norm.
    | ✓ | ✗ |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Yuan 等（[2017](#bib.bib433)） | 同行评审期刊 | ISIC2016 PH² | - | Tanimoto | 84.7%
    | ✓ | 翻转、旋转、缩放、平移对比度归一化 | ✓ | ✗ |'
- en: '| Ramachandram and DeVries ([2017](#bib.bib314)) | non peer-reviewed technical
    report | ISIC2017 | dilated conv. | CE | 64.20% | ✗ | rotation flipping | ✓ |
    ✗ |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| Ramachandram 和 DeVries（[2017](#bib.bib314)） | 非同行评审技术报告 | ISIC2017 | 扩张卷积
    | CE | 64.20% | ✗ | 旋转、翻转 | ✓ | ✗ |'
- en: '| Bozorgtabar et al. ([2017a](#bib.bib65)) | peer-reviewed conference | ISIC2016
    | - | CE | 82.90% | ✗ | rotations | ✓ | ✗ |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| Bozorgtabar 等（[2017a](#bib.bib65)） | 同行评审会议 | ISIC2016 | - | CE | 82.90%
    | ✗ | 旋转 | ✓ | ✗ |'
- en: '| Bi et al. ([2017a](#bib.bib51)) | peer-reviewed conference | ISIC2016 | parallel
    m. s. | - | 86.36% | ✗ | crops,flipping | ✓ | ✗ |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| Bi 等（[2017a](#bib.bib51)） | 同行评审会议 | ISIC2016 | 平行多尺度卷积 | - | 86.36% | ✗
    | 裁剪、翻转 | ✓ | ✗ |'
- en: '| Attia et al. ([2017](#bib.bib28)) | peer-reviewed conference | ISIC2016 |
    recurrent net. | - | 93.00% | ✗ | - | ✗ | ✗ |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| Attia 等（[2017](#bib.bib28)） | 同行评审会议 | ISIC2016 | 循环网络 | - | 93.00% | ✗ |
    - | ✗ | ✗ |'
- en: '| Deng et al. ([2017](#bib.bib114)) | peer-reviewed conference | ISIC2016 |
    parallel m. s. | - | 84.1% | ✗ | - | ✗ | ✗ |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| Deng 等（[2017](#bib.bib114)） | 同行评审会议 | ISIC2016 | 平行多尺度卷积 | - | 84.1% | ✗
    | - | ✗ | ✗ |'
- en: '| Mishra and Daescu ([2017](#bib.bib283)) | peer-reviewed conference | ISIC2017
    | skip con. | Dice | 84.2% | ✗ | rotation flipping | ✓ | ✗ |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| Mishra 和 Daescu（[2017](#bib.bib283)） | 同行评审会议 | ISIC2017 | 跳过卷积 | Dice |
    84.2% | ✗ | 旋转、翻转 | ✓ | ✗ |'
- en: '| Goyal et al. ([2017](#bib.bib155)) | peer-reviewed conference | ISIC2017
    | - | CE Dice | - | ✗ | - | ✗ | ✗ |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Goyal 等（[2017](#bib.bib155)） | 同行评审会议 | ISIC2017 | - | CE Dice | - | ✗ |
    - | ✗ | ✗ |'
- en: '| Vesal et al. ([2018a](#bib.bib394)) | peer-reviewed conference | ISIC2017
    PH² | dilated conv. dense con. skip con. | Dice | 88.00% | ✓ | - | ✗ | ✗ |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| Vesal 等（[2018a](#bib.bib394)） | 同行评审会议 | ISIC2017 PH² | 扩张卷积，密集卷积，跳过卷积 |
    Dice | 88.00% | ✓ | - | ✗ | ✗ |'
- en: '| Venkatesh et al. ([2018](#bib.bib393)) | peer-reviewed conference | ISIC2017
    | residual con. skip con. | Jaccard | 76.40% | ✗ | rotation,flipping translation,
    scaling | ✓ | ✗ |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| Venkatesh 等（[2018](#bib.bib393)） | 同行评审会议 | ISIC2017 | 残差卷积，跳过卷积 | Jaccard
    | 76.40% | ✗ | 旋转、翻转、平移、缩放 | ✓ | ✗ |'
- en: '| Yang et al. ([2018](#bib.bib426)) | peer-reviewed conference | ISIC2017 |
    skip con. parallel m.s. conv. | - | 74.10% | ✗ | rotation,flipping | ✗ | ✗ |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等（[2018](#bib.bib426)） | 同行评审会议 | ISIC2017 | 跳过卷积，平行多尺度卷积 | - | 74.10%
    | ✗ | 旋转、翻转 | ✗ | ✗ |'
- en: '| Sarker et al. ([2018](#bib.bib342)) | peer-reviewed conference | ISIC2016
    ISIC2017 | skip con. residual con. dilated conv. pyramid pooling | CE EPE | 78.20%
    | ✗ | rotation,scaling | ✗ | ✓ |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| Sarker 等（[2018](#bib.bib342)） | 同行评审会议 | ISIC2016 ISIC2017 | 跳过卷积，残差卷积，扩张卷积，金字塔池化
    | CE EPE | 78.20% | ✗ | 旋转、缩放 | ✗ | ✓ |'
- en: '| Al-Masni et al. ([2018](#bib.bib15)) | peer-reviewed journal | ISIC2017 PH²
    | - | CE | 77.10% | ✓ | rotation | ✗ | ✗ |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Al-Masni 等（[2018](#bib.bib15)） | 同行评审期刊 | ISIC2017 PH² | - | CE | 77.10%
    | ✓ | 旋转 | ✗ | ✗ |'
- en: '| Li et al. ([2018b](#bib.bib248)) | peer-reviewed conference | ISIC2017 |
    skip con. residual con. | DS | 77.23% | ✗ | flipping, rotation | ✗ | ✓ |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Li 等（[2018b](#bib.bib248)） | 同行评审会议 | ISIC2017 | 跳过卷积，残差卷积 | DS | 77.23%
    | ✗ | 翻转、旋转 | ✗ | ✓ |'
- en: '| Zeng and Zheng ([2018](#bib.bib436)) | peer-reviewed conference | ISIC2017
    | dense con. skip con. image pyramid | CE $\ell_{2}$ DS | 78.50% | ✗ | flipping,
    rotation | ✓ | ✗ |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| Zeng 和 Zheng（[2018](#bib.bib436)） | 同行评审会议 | ISIC2017 | 密集卷积，跳过卷积，图像金字塔 |
    CE $\ell_{2}$ DS | 78.50% | ✗ | 翻转、旋转 | ✓ | ✗ |'
- en: '| DeVries and Taylor ([2018](#bib.bib121)) | non peer-reviewed technical report
    | ISIC2017 | skip con. | CE | 73.00% | ✗ | flipping, rotation | ✗ | ✗ |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| DeVries 和 Taylor ([2018](#bib.bib121)) | 非同行评审技术报告 | ISIC2017 | 跳过卷积 | CE
    | 73.00% | ✗ | 翻转，旋转 | ✗ | ✗ |'
- en: '| Izadi et al. ([2018](#bib.bib190)) | peer-reviewed conference | DermoFit
    | skip con. | CE ADV | 81.20% | ✗ | flipping, rotation elastic deformation | ✗
    | ✓ |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Izadi 等 ([2018](#bib.bib190)) | 同行评审会议 | DermoFit | 跳过卷积 | CE ADV | 81.20%
    | ✗ | 翻转，旋转弹性变形 | ✗ | ✓ |'
- en: '| Li et al. ([2018a](#bib.bib243)) | peer-reviewed journal | ISIC2016 ISIC2017
    | skip con. residual con. dense con. | Jaccard DS | 76.50% | ✗ | - | ✗ | ✗ |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| Li 等 ([2018a](#bib.bib243)) | 同行评审期刊 | ISIC2016 ISIC2017 | 跳过卷积，残差卷积，密集卷积
    | Jaccard DS | 76.50% | ✗ | - | ✗ | ✗ |'
- en: '| Mirikharaji and Hamarneh ([2018](#bib.bib279)) | peer-reviewed conference
    | ISIC2017 | residual con. | CE Star shape | 77.30% | ✗ | - | ✗ | ✗ |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| Mirikharaji 和 Hamarneh ([2018](#bib.bib279)) | 同行评审会议 | ISIC2017 | 残差卷积 |
    CE 星形 | 77.30% | ✗ | - | ✗ | ✗ |'
- en: '| Pollastri et al. ([2018](#bib.bib308)) | peer-reviewed conference | ISIC2017
    | - | Jaccard $\ell_{1}$ | 78.10% | ✗ | GAN | ✓ | ✗ |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Pollastri 等 ([2018](#bib.bib308)) | 同行评审会议 | ISIC2017 | - | Jaccard $\ell_{1}$
    | 78.10% | ✗ | GAN | ✓ | ✗ |'
- en: '| Vesal et al. ([2018b](#bib.bib395)) | abstract | ISIC2017 | dilated conv.
    dense con. skip con. | Dice | 76.67% | ✗ | rotation, flipping, translation, scaling,
    color shift | ✗ | ✗ |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| Vesal 等 ([2018b](#bib.bib395)) | 摘要 | ISIC2017 | 膨胀卷积，密集卷积，跳过卷积 | Dice |
    76.67% | ✗ | 旋转，翻转，平移，缩放，颜色偏移 | ✗ | ✗ |'
- en: '| Chen et al. ([2018b](#bib.bib90)) | peer-reviewed conference | ISIC2017 |
    residual con. dilated conv. parallel m.s. conv. | WCE | 78.70% | ✗ | rotation,
    flipping cropping, zooming Gaussian noise | ✓ | ✗ |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等 ([2018b](#bib.bib90)) | 同行评审会议 | ISIC2017 | 残差卷积，膨胀卷积，平行多尺度卷积 | WCE
    | 78.70% | ✗ | 旋转，翻转，裁剪，缩放，高斯噪声 | ✓ | ✗ |'
- en: '| Jahanifar et al. ([2018](#bib.bib196)) | non peer-reviewed technical report
    | ISIC2016 ISIC2017 ISIC2018 | skip con. pyramid pooling parallel m.s. conv. |
    Tanimoto | 80.60% | ✓ | flipping, rotation zooming,translation shearing,color
    shift intensity scaling adding noises contrast adjust. sharpness adjust. disturb
    illumination hair occlusion | ✓ | ✗ |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| Jahanifar 等 ([2018](#bib.bib196)) | 非同行评审技术报告 | ISIC2016 ISIC2017 ISIC2018
    | 跳过卷积，金字塔池化，平行多尺度卷积 | Tanimoto | 80.60% | ✓ | 翻转，旋转，缩放，平移，剪切，颜色偏移，强度缩放，添加噪声，对比度调整，清晰度调整，光照干扰，头发遮挡
    | ✓ | ✗ |'
- en: '| Mirikharaji et al. ([2018](#bib.bib280)) | peer-reviewed conference | ISIC2016
    | skip con. | CE | 83.30% | ✗ | flipping,rottaion | ✗ | ✗ |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| Mirikharaji 等 ([2018](#bib.bib280)) | 同行评审会议 | ISIC2016 | 跳过卷积 | CE | 83.30%
    | ✗ | 翻转，旋转 | ✗ | ✗ |'
- en: '| Bi et al. ([2018](#bib.bib49)) | non peer-reviewed technical report | ISIC2018
    | residual con. | CE | 83.12% | ✗ | GAN | ✗ | ✗ |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Bi 等 ([2018](#bib.bib49)) | 非同行评审技术报告 | ISIC2018 | 残差卷积 | CE | 83.12% | ✗
    | GAN | ✗ | ✗ |'
- en: '| He et al. ([2018](#bib.bib176)) | peer-reviewed journal | ISIC2016 ISIC2017
    | skip con. residual con. image pyramid | CE Dice DS | 76.10% | ✗ | rotation |
    ✓ | ✗ |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| He 等 ([2018](#bib.bib176)) | 同行评审期刊 | ISIC2016 ISIC2017 | 跳过卷积，残差卷积，图像金字塔
    | CE Dice DS | 76.10% | ✗ | 旋转 | ✓ | ✗ |'
- en: '| Xue et al. ([2018](#bib.bib423)) | peer-reviewed conference | ISIC2017 |
    skip con. residual con. global conv. GAN | $\ell_{1}$ DS ADV | 78.50% | ✗ | cropping
    color jittering | ✗ | ✗ |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| Xue 等 ([2018](#bib.bib423)) | 同行评审会议 | ISIC2017 | 跳过卷积，残差卷积，全球卷积，GAN | $\ell_{1}$
    DS ADV | 78.50% | ✗ | 裁剪，颜色抖动 | ✗ | ✗ |'
- en: '| Ebenezer and Rajapakse ([2018](#bib.bib129)) | non peer-reviewed technical
    report | ISIC 2018 | skip con. | Dice | 75.6% | ✗ | rotation flipping zooming
    | ✓ | ✓ |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| Ebenezer 和 Rajapakse ([2018](#bib.bib129)) | 非同行评审技术报告 | ISIC 2018 | 跳过卷积
    | Dice | 75.6% | ✗ | 旋转，翻转，缩放 | ✓ | ✓ |'
- en: '| Goyal et al. ([2019b](#bib.bib154)) | peer-reviewed journal | ISIC2017 PH²
    | dilated conv. parallel m.s. conv. separable conv. | - | 79.34% | ✓ | - | ✓ |
    ✗ |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| Goyal 等 ([2019b](#bib.bib154)) | 同行评审期刊 | ISIC2017 PH² | 膨胀卷积，平行多尺度卷积，可分离卷积
    | - | 79.34% | ✓ | - | ✓ | ✗ |'
- en: '| Azad et al. ([2019](#bib.bib29)) | peer-reviewed conference | ISIC2018 |
    skip con. dense con. recurrent CNN | CE | 74.00% | ✗ | - | ✗ | ✓ |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| Azad 等 ([2019](#bib.bib29)) | 同行评审会议 | ISIC2018 | 跳过卷积，密集卷积，递归 CNN | CE |
    74.00% | ✗ | - | ✗ | ✓ |'
- en: '| Alom et al. ([2019](#bib.bib22)) | peer-reviewed journal | ISIC2017 | skip
    con. residual con. recurrent CNN | CE | 75.68% | ✗ | - | ✗ | ✗ |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Alom 等 ([2019](#bib.bib22)) | 同行评审期刊 | ISIC2017 | 跳过卷积，残差卷积，递归 CNN | CE |
    75.68% | ✗ | - | ✗ | ✗ |'
- en: '| Yuan and Lo ([2019](#bib.bib434)) | peer-reviewed journal | ISIC2017 | -
    | Tanimoto | 76.50% | ✗ | rotation,flipping shifting, scaling random normaliz.
    | ✓ | ✗ |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| Yuan 和 Lo ([2019](#bib.bib434)) | 同行评审期刊 | ISIC2017 | - | Tanimoto | 76.50%
    | ✗ | 旋转，翻转，平移，缩放，随机归一化 | ✓ | ✗ |'
- en: '| Goyal et al. ([2019a](#bib.bib153)) | peer-reviewed conference | ISIC2017
    PH² | dilated conv. parallel m.s. conv. | WCE | 82.20% | ✓ | - | ✗ | ✗ |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| Goyal 等（[2019a](#bib.bib153)） | 同行评审会议 | ISIC2017 PH² | dilated conv. parallel
    m.s. conv. | WCE | 82.20% | ✓ | - | ✗ | ✗ |'
- en: '| Bi et al. ([2019b](#bib.bib52)) | peer-reviewed journal | ISIC2016 ISIC2017
    PH² | skip con. residual con. | CE | 77.73% | ✓ | flipping, cropping | ✓ | ✗ |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Bi 等（[2019b](#bib.bib52)） | 同行评审期刊 | ISIC2016 ISIC2017 PH² | skip con. residual
    con. | CE | 77.73% | ✓ | flipping, cropping | ✓ | ✗ |'
- en: '| Tschandl et al. ([2019](#bib.bib382)) | peer-reviewed journal | ISIC2017
    | skip con. | CE Jaccard | 76.80% | ✗ | flipping, rotation | ✓ | ✗ |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| Tschandl 等（[2019](#bib.bib382)） | 同行评审期刊 | ISIC2017 | skip con. | CE Jaccard
    | 76.80% | ✗ | flipping, rotation | ✓ | ✗ |'
- en: '| Li et al. ([2021c](#bib.bib247)) | peer-reviewed journal | ISIC2017 | skip
    con. dense con. semi-supervised ensemble | CE $\ell_{1}$ | 79.80% | ✗ | flipping,rotating
    scaling | ✓ | ✗ |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| Li 等（[2021c](#bib.bib247)） | 同行评审期刊 | ISIC2017 | skip con. dense con. semi-supervised
    ensemble | CE $\ell_{1}$ | 79.80% | ✗ | flipping,rotating scaling | ✓ | ✗ |'
- en: '| Zhang et al. ([2019b](#bib.bib441)) | peer-reviewed journal | ISIC2016 ISIC2017
    | skip con. | CE | 72.94% | ✗ | - | ✗ | ✗ |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等（[2019b](#bib.bib441)） | 同行评审期刊 | ISIC2016 ISIC2017 | skip con. |
    CE | 72.94% | ✗ | - | ✗ | ✗ |'
- en: '| Baghersalimi et al. ([2019](#bib.bib35)) | peer-reviewed journal | ISIC2016
    ISIC2017 PH² | skip con. residual con. dense con. | Tanimoto | 78.30% | ✓ | flipping,cropping
    | ✗ | ✗ |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| Baghersalimi 等（[2019](#bib.bib35)） | 同行评审期刊 | ISIC2016 ISIC2017 PH² | skip
    con. residual con. dense con. | Tanimoto | 78.30% | ✓ | flipping,cropping | ✗
    | ✗ |'
- en: '| Jiang et al. ([2019](#bib.bib203)) | peer-reviewed conference | ISIC2017
    | residual con. dilated conv. GAN | ADV $\ell_{2}$ | 76.90% | ✗ | rotation,flipping
    | ✗ | ✗ |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| Jiang 等（[2019](#bib.bib203)） | 同行评审会议 | ISIC2017 | residual con. dilated
    conv. GAN | ADV $\ell_{2}$ | 76.90% | ✗ | rotation,flipping | ✗ | ✗ |'
- en: '| Tang et al. ([2019b](#bib.bib374)) | peer-reviewed conference | ISIC2016
    | skip con. | Tanimoto DS | 85.34% | ✗ | rotation,flipping | ✗ | ✗ |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| Tang 等（[2019b](#bib.bib374)） | 同行评审会议 | ISIC2016 | skip con. | Tanimoto DS
    | 85.34% | ✗ | rotation,flipping | ✗ | ✗ |'
- en: '| Bi et al. ([2019a](#bib.bib48)) | peer-reviewed conference | ISIC2017 | residual
    con. | CE | 77.14% | ✗ | GAN | ✗ | ✗ |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| Bi 等（[2019a](#bib.bib48)） | 同行评审会议 | ISIC2017 | residual con. | CE | 77.14%
    | ✗ | GAN | ✗ | ✗ |'
- en: '| Abraham and Khan ([2019](#bib.bib9)) | peer-reviewed conference | ISIC2018
    | skip con. image pyramid attention | TV Focal | 74.80% | ✗ | - | ✗ | ✓ |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| Abraham 和 Khan（[2019](#bib.bib9)） | 同行评审会议 | ISIC2018 | skip con. image pyramid
    attention | TV Focal | 74.80% | ✗ | - | ✗ | ✓ |'
- en: '| Cui et al. ([2019](#bib.bib105)) | peer-reviewed conference | ISIC2018 |
    dilated conv. parallel m.s. conv. separable conv. | - | 83.00% | ✗ | - | ✗ | ✗
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| Cui 等（[2019](#bib.bib105)） | 同行评审会议 | ISIC2018 | dilated conv. parallel m.s.
    conv. separable conv. | - | 83.00% | ✗ | - | ✗ | ✗ |'
- en: '| Song et al. ([2019](#bib.bib359)) | peer-reviewed conference | ISIC2017 |
    skip con. residual con. dense con. attention mod. | CE Jaccard | 76.50% | ✗ |
    - | ✗ | ✗ |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| Song 等（[2019](#bib.bib359)） | 同行评审会议 | ISIC2017 | skip con. residual con.
    dense con. attention mod. | CE Jaccard | 76.50% | ✗ | - | ✗ | ✗ |'
- en: '| Singh et al. ([2019](#bib.bib355)) | peer-reviewed journal | ISIC2016 ISIC2017
    ISIC2018 | skip con. residual con. factorized conv. attention mod. GAN | CE $\ell_{1}$
    EPE | 78.65% | ✗ | - | ✗ | ✓ |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| Singh 等（[2019](#bib.bib355)） | 同行评审期刊 | ISIC2016 ISIC2017 ISIC2018 | skip
    con. residual con. factorized conv. attention mod. GAN | CE $\ell_{1}$ EPE | 78.65%
    | ✗ | - | ✗ | ✓ |'
- en: '| Tan et al. ([2019b](#bib.bib370)) | peer-reviewed journal | ISIC2017 DermoFit
    PH² | dilated conv. | Dice | 62.29%^∗ | ✓ | - | ✓ | ✗ |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| Tan 等（[2019b](#bib.bib370)） | 同行评审期刊 | ISIC2017 DermoFit PH² | dilated conv.
    | Dice | 62.29%^∗ | ✓ | - | ✓ | ✗ |'
- en: '| Kaul et al. ([2019](#bib.bib215)) | peer-reviewed conference | ISIC2017 |
    skip con. residual con. attention mod. | Dice | 75.60% | ✗ | channel shift | ✗
    | ✗ |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| Kaul 等（[2019](#bib.bib215)） | 同行评审会议 | ISIC2017 | skip con. residual con.
    attention mod. | Dice | 75.60% | ✗ | channel shift | ✗ | ✗ |'
- en: '| De Angelo et al. ([2019](#bib.bib112)) | peer-reviewed conference | ISIC2017
    Private | skip con. | CE Dice | 76.07% | ✗ | flipping, shifting rotation color
    jittering | ✓ | ✗ |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| De Angelo 等（[2019](#bib.bib112)） | 同行评审会议 | ISIC2017 Private | skip con.
    | CE Dice | 76.07% | ✗ | flipping, shifting rotation color jittering | ✓ | ✗ |'
- en: '| Zhang et al. ([2019a](#bib.bib437)) | peer-reviewed journal | ISIC2017 PH²
    | skip con. residual con. parallel m.s. conv. | CE Dice DS | 78.50% | ✓ | flipping,
    rotation whitening contrast enhance. | ✓ | ✗ |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等（[2019a](#bib.bib437)） | 同行评审期刊 | ISIC2017 PH² | skip con. residual
    con. parallel m.s. conv. | CE Dice DS | 78.50% | ✓ | flipping, rotation whitening
    contrast enhance. | ✓ | ✗ |'
- en: '| Soudani and Barhoumi ([2019](#bib.bib361)) | peer-reviewed journal | ISIC2017
    | residual con. | CE | 78.60% | ✗ | rotation, flipping | ✗ | ✗ |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| Soudani 和 Barhoumi（[2019](#bib.bib361)） | 同行评审期刊 | ISIC2017 | residual con.
    | CE | 78.60% | ✗ | rotation, flipping | ✗ | ✗ |'
- en: '| Mirikharaji et al. ([2019](#bib.bib281)) | peer-reviewed conference | ISIC2017
    | skip con. | WCE | 68.91%^∗ | ✗ | - | ✗ | ✗ |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| Mirikharaji 等 ([2019](#bib.bib281)) | 经过同行评审的会议 | ISIC2017 | 跳过卷积 | WCE |
    68.91%^∗ | ✗ | - | ✗ | ✗ |'
- en: '| Nasr-Esfahani et al. ([2019](#bib.bib285)) | peer-reviewed journal | DermQuest
    | dense con. | WCE | 85.20% | ✗ | rotation,flipping cropping | ✗ | ✗ |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| Nasr-Esfahani 等 ([2019](#bib.bib285)) | 经过同行评审的期刊 | DermQuest | 密集卷积 | WCE
    | 85.20% | ✗ | 旋转、翻转、裁剪 | ✗ | ✗ |'
- en: '| Wang et al. ([2019a](#bib.bib397)) | peer-reviewed conference | ISIC2017
    ISIC2018 | skip con. residual con. parallel m.s. conv. attention mod. | WDice
    | 77.60% | ✗ | copping, flipping | ✗ | ✗ |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 ([2019a](#bib.bib397)) | 经过同行评审的会议 | ISIC2017 ISIC2018 | 跳过卷积 残差卷积
    并行多尺度卷积 注意力模型 | WDice | 77.60% | ✗ | 剪裁、翻转 | ✗ | ✗ |'
- en: '| Sarker et al. ([2019](#bib.bib341)) | non peer-reviewed technical report
    | ISIC2017 ISIC2018 | factrized conv. attention mod. GAN | CE Jaccard $\ell_{1}$,ADV
    | 77.98% | ✗ | flipping gamma reconst. contrast adjust. | ✗ | ✗ |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| Sarker 等 ([2019](#bib.bib341)) | 非同行评审的技术报告 | ISIC2017 ISIC2018 | 分解卷积 注意力模型
    GAN | CE Jaccard $\ell_{1}$,ADV | 77.98% | ✗ | 翻转 gamma 重新构建 对比度调整 | ✗ | ✗ |'
- en: '| Tu et al. ([2019](#bib.bib383)) | peer-reviewed journal | ISIC2017 PH² |
    skip con. residual con. dense con. GAN | Jaccard EPE, $\ell_{1}$ DS, ADV | 76.80%
    | ✓ | flipping | ✗ | ✗ |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| Tu 等 ([2019](#bib.bib383)) | 经过同行评审的期刊 | ISIC2017 PH² | 跳过卷积 残差卷积 密集卷积 GAN
    | Jaccard EPE, $\ell_{1}$ DS, ADV | 76.80% | ✓ | 翻转 | ✗ | ✗ |'
- en: '| Wei et al. ([2019](#bib.bib412)) | peer-reviewed journal | ISIC2016 ISIC2017
    PH² | skip con. residual con. attention mod. GAN | Jaccard $\ell_{1}$ ADV | 80.45%
    | ✓ | rotation,flipping color jittering | ✗ | ✗ |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| Wei 等 ([2019](#bib.bib412)) | 经过同行评审的期刊 | ISIC2016 ISIC2017 PH² | 跳过卷积 残差卷积
    注意力模型 GAN | Jaccard $\ell_{1}$ ADV | 80.45% | ✓ | 旋转、翻转 颜色抖动 | ✗ | ✗ |'
- en: '| Ünver and Ayan ([2019](#bib.bib385)) | peer-reviewed journal | ISIC2017 PH²
    | - | $\ell_{2}$ | 74.81% | ✓ | - | ✓ | ✗ |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| Ünver 和 Ayan ([2019](#bib.bib385)) | 经过同行评审的期刊 | ISIC2017 PH² | - | $\ell_{2}$
    | 74.81% | ✓ | - | ✓ | ✗ |'
- en: '| Al-masni et al. ([2019](#bib.bib16)) | peer-reviewed conference | ISIC2017
    | - | - | 77.11% | ✗ | rotation,flipping | ✗ | ✗ |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| Al-masni 等 ([2019](#bib.bib16)) | 经过同行评审的会议 | ISIC2017 | - | - | 77.11% |
    ✗ | 旋转、翻转 | ✗ | ✗ |'
- en: '| Canalini et al. ([2019](#bib.bib70)) | peer-reviewed conference | ISIC2017
    | dilated conv. parallel m.s. conv. separable conv. | CE Tanimoto | 85.00% | ✗
    | rotating, flipping shifting, shearing scaling color jittering | ✓ | ✗ |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| Canalini 等 ([2019](#bib.bib70)) | 经过同行评审的会议 | ISIC2017 | 膨胀卷积 并行多尺度卷积 可分离卷积
    | CE Tanimoto | 85.00% | ✗ | 旋转、翻转、平移、剪切、缩放、颜色抖动 | ✓ | ✗ |'
- en: '| Wang et al. ([2019b](#bib.bib404)) | peer-reviewed conference | ISIC2017
    | residual con. | WCE | 78.10% | ✗ | flipping, scaling | ✗ | ✗ |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 ([2019b](#bib.bib404)) | 经过同行评审的会议 | ISIC2017 | 残差卷积 | WCE | 78.10%
    | ✗ | 翻转、缩放 | ✗ | ✗ |'
- en: '| Alom et al. ([2020](#bib.bib21)) | peer-reviewed conference | ISIC2018 |
    skip con. residual con. recurrent CNN | CE | 88.83% | ✗ | flipping | ✗ | ✗ |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| Alom 等 ([2020](#bib.bib21)) | 经过同行评审的会议 | ISIC2018 | 跳过卷积 残差卷积 循环 CNN | CE
    | 88.83% | ✗ | 翻转 | ✗ | ✗ |'
- en: '| Pollastri et al. ([2020](#bib.bib309)) | peer-reviewed journal | ISIC2017
    | - | Tanimoto | 78.90% | ✗ | GAN flipping,rotation shifting, scaling color jittering
    | ✗ | ✗ |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| Pollastri 等 ([2020](#bib.bib309)) | 经过同行评审的期刊 | ISIC2017 | - | Tanimoto |
    78.90% | ✗ | GAN 翻转、旋转 平移、缩放 颜色抖动 | ✗ | ✗ |'
- en: '| Liu et al. ([2019b](#bib.bib258)) | peer-reviewed conference | ISIC2017 |
    skip con. dilated conv. | CE | 75.20% | ✗ | scaling, cropping rotation, flipping
    image deformation | ✗ | ✗ |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等 ([2019b](#bib.bib258)) | 经过同行评审的会议 | ISIC2017 | 跳过卷积 膨胀卷积 | CE | 75.20%
    | ✗ | 缩放、裁剪、旋转、翻转图像变形 | ✗ | ✗ |'
- en: '| Abhishek and Hamarneh ([2019](#bib.bib5)) | peer-reviewed conference | ISIC2017
    PH² | skip con. | - | 68.69%^∗ | ✓ | rotation,flipping GAN | ✗ | ✓ |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| Abhishek 和 Hamarneh ([2019](#bib.bib5)) | 经过同行评审的会议 | ISIC2017 PH² | 跳过卷积
    | - | 68.69%^∗ | ✓ | 旋转、翻转 GAN | ✗ | ✓ |'
- en: '| Shahin et al. ([2019](#bib.bib345)) | peer-reviewed conference | ISIC2018
    | skip con. image pyramid | Generalized Dice | 73.8% | ✗ | rotation flipping zooming
    | ✗ | ✗ |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| Shahin 等 ([2019](#bib.bib345)) | 经过同行评审的会议 | ISIC2018 | 跳过卷积 图像金字塔 | 广义 Dice
    | 73.8% | ✗ | 旋转翻转缩放 | ✗ | ✗ |'
- en: '| Adegun and Viriri ([2019](#bib.bib10)) | peer-reviewed conference | ISIC2017
    | - | Dice | 83.0% | ✗ | elastic | ✗ | ✗ |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| Adegun 和 Viriri ([2019](#bib.bib10)) | 经过同行评审的会议 | ISIC2017 | - | Dice |
    83.0% | ✗ | 弹性 | ✗ | ✗ |'
- en: '| Taghanaki et al. ([2019](#bib.bib365)) | peer-reviewed conference | ISIC
    2017 | skip con. | Dice $\ell_{1}$ SSIM | 69.35%^∗ | ✗ | rotation flipping gradient-based
    perturbation | ✗ | ✗ |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| Taghanaki 等 ([2019](#bib.bib365)) | 经过同行评审的会议 | ISIC 2017 | 跳过卷积 | Dice $\ell_{1}$
    SSIM | 69.35%^∗ | ✗ | 旋转翻转基于梯度的扰动 | ✗ | ✗ |'
- en: '| Saini et al. ([2019](#bib.bib339)) | peer-reviewed conference | ISIC 2017
    ISIC 2018 PH2 | skip con. multi-task | Dice | 84.9% | ✗ | rotation, flipping shearing,
    stretch crop, contrast | ✗ | ✗ |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| Saini 等人 ([2019](#bib.bib339)) | 同行评审会议 | ISIC 2017 ISIC 2018 PH2 | 跳过连接
    多任务 | Dice | 84.9% | ✗ | 旋转、翻转剪切、拉伸裁剪、对比度 | ✗ | ✗ |'
- en: '| Wang et al. ([2019c](#bib.bib405)) | peer-reviewed journal | ISIC2016 ISIC2017
    | skip con. residual con. dilated conv. | WCE | 81.47% | ✗ | flipping,scaling
    | ✗ | ✗ |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([2019c](#bib.bib405)) | 同行评审期刊 | ISIC2016 ISIC2017 | 跳过连接 残差连接 膨胀卷积
    | WCE | 81.47% | ✗ | 翻转、缩放 | ✗ | ✗ |'
- en: '| Kamalakannan et al. ([2019](#bib.bib208)) | peer-reviewed journal | ISIC
    Archive | skip con. | CE | - | ✗ | - | ✗ | ✗ |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| Kamalakannan 等人 ([2019](#bib.bib208)) | 同行评审期刊 | ISIC Archive | 跳过连接 | CE
    | - | ✗ | - | ✗ | ✗ |'
- en: '| Hasan et al. ([2020](#bib.bib172)) | peer-reviewed journal | ISIC2017 PH²
    | skip con. dense con. separable conv. | CE Jaccard | 77.50% | ✓ | rotation, zooming
    shifting, flipping | ✗ | ✓ |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| Hasan 等人 ([2020](#bib.bib172)) | 同行评审期刊 | ISIC2017 PH² | 跳过连接 密集连接 可分离卷积
    | CE Jaccard | 77.50% | ✓ | 旋转、缩放 移动、翻转 | ✗ | ✓ |'
- en: '| Al Nazi and Abir ([2020](#bib.bib18)) | peer-reviewed conference | ISIC2018
    PH² | skip con. | Dice | 80.00% | ✓ | rotation, zooming flipping,elastic dist.
    Gaussian dist. histogram equal. color jittering | ✗ | ✓ |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| Al Nazi 和 Abir ([2020](#bib.bib18)) | 同行评审会议 | ISIC2018 PH² | 跳过连接 | Dice
    | 80.00% | ✓ | 旋转、缩放 翻转、弹性分布 高斯分布 直方图均衡 颜色抖动 | ✗ | ✓ |'
- en: '| Deng et al. ([2020](#bib.bib115)) | peer-reviewed conference | ISIC2017 PH²
    | dilated conv. parallel m.s. conv. separable conv. semi-supervised | Dice Narrowband
    suppression | 83.9% | ✓ | rotation | ✓ | ✗ |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| Deng 等人 ([2020](#bib.bib115)) | 同行评审会议 | ISIC2017 PH² | 膨胀卷积 并行 m.s. 卷积 可分离卷积
    半监督 | Dice 窄带抑制 | 83.9% | ✓ | 旋转 | ✓ | ✗ |'
- en: '| Xie et al. ([2020b](#bib.bib420)) | peer-reviewed journal | ISIC2017 PH²
    | dilated conv. parallel m.s. conv. separable conv. | Dice Rank | 80.4% | ✓ |
    cropping,scaling rotation, shearing shifting,zooming whitening, flipping | ✗ |
    ✓ |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| Xie 等人 ([2020b](#bib.bib420)) | 同行评审期刊 | ISIC2017 PH² | 膨胀卷积 并行 m.s. 卷积 可分离卷积
    | Dice Rank | 80.4% | ✓ | 裁剪、缩放 旋转、剪切 移动、缩放 去白化、翻转 | ✗ | ✓ |'
- en: '| Zhang et al. ([2020a](#bib.bib439)) | peer-reviewed conference | SCD ISIC2016
    ISIC2017 ISIC2018 | skip con. | Kappa Loss | 84.00%^∗ | ✗ | rotation,shifting
    shearing,zooming flipping | ✗ | ✓ |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等人 ([2020a](#bib.bib439)) | 同行评审会议 | SCD ISIC2016 ISIC2017 ISIC2018
    | 跳过连接 | Kappa Loss | 84.00%^∗ | ✗ | 旋转、移动剪切、缩放 翻转 | ✗ | ✓ |'
- en: '| Saha et al. ([2020](#bib.bib337)) | peer-reviewed conference | ISIC2017 ISIC2018
    | skip con. dense con. | CE | 81.9% | ✗ | color jittering rotation flipping translation
    | ✗ | ✗ |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| Saha 等人 ([2020](#bib.bib337)) | 同行评审会议 | ISIC2017 ISIC2018 | 跳过连接 密集连接 |
    CE | 81.9% | ✗ | 颜色抖动 旋转 翻转 翻译 | ✗ | ✗ |'
- en: '| Henry et al. ([2020](#bib.bib177)) | peer-reviewed conference | ISIC2018
    | skip con. parallel m. s. conv. attention mod. | - | 78.04% | ✗ | color jittering
    rotation,cropping flipping,shift | ✗ | ✓ |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| Henry 等人 ([2020](#bib.bib177)) | 同行评审会议 | ISIC2018 | 跳过连接 并行 m. s. 卷积 注意力模块
    | - | 78.04% | ✗ | 颜色抖动 旋转、裁剪 翻转、位移 | ✗ | ✓ |'
- en: '| Jafari et al. ([2020](#bib.bib193)) | peer-reviewed conference | ISIC2018
    | skip con. residual con. dense con. | CE | 75.5% | ✗ | - | ✗ | ✓ |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| Jafari 等人 ([2020](#bib.bib193)) | 同行评审会议 | ISIC2018 | 跳过连接 残差连接 密集连接 | CE
    | 75.5% | ✗ | - | ✗ | ✓ |'
- en: '| Li et al. ([2020a](#bib.bib244)) | peer-reviewed conference | ISIC2018 |
    skip con. residual con. ensemble semi-supervised | CE Dice | 75.5% | ✗ | - | ✗
    | ✗ |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人 ([2020a](#bib.bib244)) | 同行评审会议 | ISIC2018 | 跳过连接 残差连接 集成 半监督 | CE
    Dice | 75.5% | ✗ | - | ✗ | ✗ |'
- en: '| Guo et al. ([2020](#bib.bib165)) | peer-reviewed conference | ISIC2018 |
    skip con. dilated conv. parallel m. s. conv. | Focal Jaccard | 77.60% | ✗ | -
    | ✗ | ✓ |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| Guo 等人 ([2020](#bib.bib165)) | 同行评审会议 | ISIC2018 | 跳过连接 膨胀卷积 并行 m. s. 卷积
    | Focal Jaccard | 77.60% | ✗ | - | ✗ | ✓ |'
- en: '| Li et al. ([2020b](#bib.bib249)) | peer-reviewed conference | ISIC2018 |
    skip con. residual con. self-supervised | MSE KLD | 87.74%^∗ | ✗ | - | ✗ | ✗ |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人 ([2020b](#bib.bib249)) | 同行评审会议 | ISIC2018 | 跳过连接 残差连接 自监督 | MSE KLD
    | 87.74%^∗ | ✗ | - | ✗ | ✗ |'
- en: '| Jiang et al. ([2020](#bib.bib205)) | peer-reviewed journal | ISIC2017 PH²
    | skip con. residual con. attention mod. | CE | 73.35% | ✗ | flipping | ✗ | ✗
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| Jiang 等人 ([2020](#bib.bib205)) | 同行评审期刊 | ISIC2017 PH² | 跳过连接 残差连接 注意力模块
    | CE | 73.35% | ✗ | 翻转 | ✗ | ✗ |'
- en: '| Qiu et al. ([2020](#bib.bib312)) | peer-reviewed journal | ISIC2017 PH² |
    ensemble | - | 80.02% | ✗ | translation rotation shearing | ✓ | ✗ |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| Qiu 等人 ([2020](#bib.bib312)) | 同行评审期刊 | ISIC2017 PH² | 集成 | - | 80.02% |
    ✗ | 翻译 旋转 剪切 | ✓ | ✗ |'
- en: '| Xie et al. ([2020a](#bib.bib419)) | peer-reviewed journal | ISIC2016 ISIC2017
    PH² | attention mod. | CE | 78.3% | ✗ | rotation flipping | ✗ | ✗ |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| Xie 等人 ([2020a](#bib.bib419)) | 同行评审期刊 | ISIC2016 ISIC2017 PH² | 注意力模块 |
    CE | 78.3% | ✗ | 旋转 翻转 | ✗ | ✗ |'
- en: '| Zafar et al. ([2020](#bib.bib435)) | peer-reviewed journal | ISIC2017 PH²
    | skip con. residual con. | CE | 77.2% | ✗ | rotation | ✗ | ✗ |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| Zafar 等人 ([2020](#bib.bib435)) | 同行评审期刊 | ISIC2017 PH² | 跳过连接 残差连接 | CE |
    77.2% | ✗ | 旋转 | ✗ | ✗ |'
- en: '| Azad et al. ([2020](#bib.bib30)) | peer-reviewed conference | ISIC 2017 ISIC
    2018 PH2 | dilated conv. attention mod. | - | 96.98% | ✗ | - | ✗ | ✓ |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| Azad 等人 ([2020](#bib.bib30)) | 同行评审会议 | ISIC 2017 ISIC 2018 PH2 | 扩张卷积 注意力模块
    | - | 96.98% | ✗ | - | ✗ | ✓ |'
- en: '| Nathan and Kansal ([2020](#bib.bib286)) | non peer-reviewed technical report
    | ISIC 2016 ISIC 2017 ISIC 2018 PH2 | skip con. residual con. | CE Dice | 78.28%
    | ✗ | rotation, flipping shearing, zoom | ✗ | ✗ |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| Nathan 和 Kansal ([2020](#bib.bib286)) | 非同行评审技术报告 | ISIC 2016 ISIC 2017 ISIC
    2018 PH2 | 跳过连接 残差连接 | CE Dice | 78.28% | ✗ | 旋转，翻转 剪切，缩放 | ✗ | ✗ |'
- en: '| Mirikharaji et al. ([2021](#bib.bib278)) | peer-reviewed conference | ISIC
    Archive PH2 DermoFit | skip con. residual con. ensemble | CE | 72.11% | ✗ | -
    | ✗ | ✗ |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| Mirikharaji 等人 ([2021](#bib.bib278)) | 同行评审会议 | ISIC Archive PH2 DermoFit
    | 跳过连接 残差连接 集成 | CE | 72.11% | ✗ | - | ✗ | ✗ |'
- en: '| Öztürk and Özkaya ([2020](#bib.bib293)) | peer-reviewed journal | ISIC 2017
    PH2 | residual con. | - | 78.34% | ✓ | - | ✗ | ✗ |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| Öztürk 和 Özkaya ([2020](#bib.bib293)) | 同行评审期刊 | ISIC 2017 PH2 | 残差连接 | -
    | 78.34% | ✓ | - | ✗ | ✗ |'
- en: '| Abhishek et al. ([2020](#bib.bib7)) | peer-reviewed conference | ISIC 2017
    DermoFit PH2 | skip con. | Dice | 75.70% | ✓ | rotation flipping | ✗ | ✓ |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| Abhishek 等人 ([2020](#bib.bib7)) | 同行评审会议 | ISIC 2017 DermoFit PH2 | 跳过连接
    | Dice | 75.70% | ✓ | 旋转 翻转 | ✗ | ✓ |'
- en: '| Kaymak et al. ([2020](#bib.bib221)) | peer-reviewed journal | ISIC 2017 |
    - | - | 72.5% | ✗ | - | ✗ | ✗ |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| Kaymak 等人 ([2020](#bib.bib221)) | 同行评审期刊 | ISIC 2017 | - | - | 72.5% | ✗
    | - | ✗ | ✗ |'
- en: '| Bagheri et al. ([2020](#bib.bib32)) | peer-reviewed journal | ISIC2017 DermQuest
    | dilated conv. parallel m.s. conv. separable conv. | - | 79.05% | ✓ | rotation,flipping
    brightness change resizing | ✗ | ✗ |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| Bagheri 等人 ([2020](#bib.bib32)) | 同行评审期刊 | ISIC2017 DermQuest | 扩张卷积 并行多尺度卷积
    可分离卷积 | - | 79.05% | ✓ | 旋转，翻转 亮度变化 尺寸调整 | ✗ | ✗ |'
- en: '| Jayapriya and Jacob ([2020](#bib.bib199)) | peer-reviewed journal | ISIC2016
    | skip con. parallel m.s. conv. | - | 92.42% | ✗ | - | ✗ | ✗ |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| Jayapriya 和 Jacob ([2020](#bib.bib199)) | 同行评审期刊 | ISIC2016 | 跳过连接 并行多尺度卷积
    | - | 92.42% | ✗ | - | ✗ | ✗ |'
- en: '| Wang et al. ([2020a](#bib.bib401)) | non peer-reviewed technical report |
    ISIC2016 ISIC2017 PH² | residual con. dilated conv. attention mod. | CE Dice DS
    | 80.30% | ✓ | flipping, rotation cropping | ✗ | ✗ |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([2020a](#bib.bib401)) | 非同行评审技术报告 | ISIC2016 ISIC2017 PH² | 残差连接
    扩张卷积 注意力模块 | CE Dice DS | 80.30% | ✓ | 翻转，旋转 裁剪 | ✗ | ✗ |'
- en: '| Wang et al. ([2020b](#bib.bib408)) | non peer-reviewed technical report |
    ISIC2018 PH² | attention mod. skip con. parallel m.s. conv. recurrent CNN | Dice
    Focal Tversky | 80.6% | ✗ | rotation flipping cropping | ✗ | ✗ |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([2020b](#bib.bib408)) | 非同行评审技术报告 | ISIC2018 PH² | 注意力模块 跳过连接 并行多尺度卷积
    循环 CNN | Dice Focal Tversky | 80.6% | ✗ | 旋转 翻转 裁剪 | ✗ | ✗ |'
- en: '| Ribeiro et al. ([2020](#bib.bib326)) | peer-reviewed conference | ISIC Archive
    PH² DermoFit | skip con. residual con. dilated conv. | Soft Jaccard CE | - | ✓
    | Gaussian noise color jittering | ✓ | ✓ |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| Ribeiro 等人 ([2020](#bib.bib326)) | 同行评审会议 | ISIC Archive PH² DermoFit | 跳过连接
    残差连接 扩张卷积 | Soft Jaccard CE | - | ✓ | 高斯噪声颜色抖动 | ✓ | ✓ |'
- en: '| Zhu et al. ([2020](#bib.bib455)) | peer-reviewed conference | ISIC2018 |
    skip con. residual con. dilated conv. attention mod. | CE Dice | 82.15% | ✗ |
    flipping | ✗ | ✗ |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| Zhu 等人 ([2020](#bib.bib455)) | 同行评审会议 | ISIC2018 | 跳过连接 残差连接 扩张卷积 注意力模块 |
    CE Dice | 82.15% | ✗ | 翻转 | ✗ | ✗ |'
- en: '| Gu et al. ([2020](#bib.bib160)) | peer-reviewed journal | ISIC 2018 | residual
    con. skip con. attention mod. | Dice | 85.32%^∗ | ✗ | cropping, flipping rotation
    | ✗ | ✓ |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| Gu 等人 ([2020](#bib.bib160)) | 同行评审期刊 | ISIC 2018 | 残差连接 跳过连接 注意力模块 | Dice
    | 85.32%^∗ | ✗ | 裁剪，翻转 旋转 | ✗ | ✓ |'
- en: '| Lei et al. ([2020](#bib.bib241)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 | skip con. dense con. dilated conv. GAN | CE $\ell_{1}$ ADV | 77.1% | ✓
    | flipping, rotation | ✗ | ✗ |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| Lei 等人 ([2020](#bib.bib241)) | 同行评审期刊 | ISIC 2017 ISIC 2018 | 跳过连接 稠密连接 扩张卷积
    GAN | CE $\ell_{1}$ ADV | 77.1% | ✓ | 翻转，旋转 | ✗ | ✗ |'
- en: '| Andrade et al. ([2020](#bib.bib24)) | peer-reviewed journal | DermoFit SMARTSKINS
    | residual con. dilated conv. GAN | Dice | 81.03% | ✗ | flipping, brightness saturation,
    contrast, hue Gaussian hue | ✗ | ✗ |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| Andrade 等人 ([2020](#bib.bib24)) | 同行评审期刊 | DermoFit SMARTSKINS | 残差连接 扩张卷积
    GAN | Dice | 81.03% | ✗ | 翻转，亮度饱和，对比度，色调高斯色调 | ✗ | ✗ |'
- en: '| Wu et al. ([2020](#bib.bib416)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 | residual con. attention mod. multi-scale | CE Dice | 82.55% | ✗ | flipping,
    rotation scaling, cropping sharpening, color distribution adj., noise | ✗ | ✗
    |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等 ([2020](#bib.bib416)) | 同行评审期刊 | ISIC 2017 ISIC 2018 | 残差连接，注意力模块，多尺度
    | CE Dice | 82.55% | ✗ | 翻转、旋转、缩放、裁剪、锐化、颜色分布调整、噪声 | ✗ | ✗ |'
- en: '| Arora et al. ([2021](#bib.bib26)) | peer-reviewed journal | ISIC 2018 | skip
    con. attention mod. | Dice Tversky Focal Tversky | 83% | ✗ | flipping | ✓ | ✗
    |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| Arora 等 ([2021](#bib.bib26)) | 同行评审期刊 | ISIC 2018 | 跳跃连接、注意力模块 | Dice Tversky
    Focal Tversky | 83% | ✗ | 翻转 | ✓ | ✗ |'
- en: '| Jin et al. ([2021](#bib.bib206)) | peer-reviewed journal | ISIC2017 ISIC2018
    | skip con. residual con. attention mod. | Dice Focal | 80.00% | ✗ | flipping,
    rotation affine trans. scaling, cropping | ✗ | ✓ |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| Jin 等 ([2021](#bib.bib206)) | 同行评审期刊 | ISIC2017 ISIC2018 | 跳跃连接，残差连接，注意力模块
    | Dice Focal | 80.00% | ✗ | 翻转、旋转仿射变换、缩放、裁剪 | ✗ | ✓ |'
- en: '| Hasan et al. ([2021](#bib.bib171)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 | skip con. residual con. separable conv. | Dice CE | 66.66%^∗ | ✗ | flipping,
    rotation shifting, zooming intensity adjust. | ✗ | ✗ |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| Hasan 等 ([2021](#bib.bib171)) | 同行评审期刊 | ISIC 2016 ISIC 2017 | 跳跃连接、残差连接、可分离卷积
    | Dice CE | 66.66%^∗ | ✗ | 翻转、旋转、位移、缩放、强度调整 | ✗ | ✗ |'
- en: '| Kosgiker et al. ([2021](#bib.bib231)) | peer-reviewed journal | ISIC 2017
    PH² | - | MSE CE | 90.25% | ✗ | - | ✗ | ✗ |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| Kosgiker 等 ([2021](#bib.bib231)) | 同行评审期刊 | ISIC 2017 PH² | - | MSE CE |
    90.25% | ✗ | - | ✗ | ✗ |'
- en: '| Bagheri et al. ([2021a](#bib.bib33)) | peer-reviewed journal | ISIC2016 ISIC2017
    ISIC2018 PH² DermQuest | parallel m.s. conv. dilated conv. | Dice CE | 85.04%
    | ✓ | rotation flipping color jittering | ✗ | ✗ |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| Bagheri 等 ([2021a](#bib.bib33)) | 同行评审期刊 | ISIC2016 ISIC2017 ISIC2018 PH²
    DermQuest | 并行多尺度卷积、膨胀卷积 | Dice CE | 85.04% | ✓ | 旋转、翻转、颜色抖动 | ✗ | ✗ |'
- en: '| Saini et al. ([2021](#bib.bib340)) | peer-reviewed conference | ISIC2017
    ISIC2018 PH² | pyramid pooling residual con. skip con. dilated conv. attention
    mod. | Dice | 85.00% | ✓ | rotation,shearing color jittering | ✗ | ✗ |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| Saini 等 ([2021](#bib.bib340)) | 同行评审会议 | ISIC2017 ISIC2018 PH² | 金字塔池化、残差连接、跳跃连接、膨胀卷积、注意力模块
    | Dice | 85.00% | ✓ | 旋转、剪切、颜色抖动 | ✗ | ✗ |'
- en: '| Tong et al. ([2021](#bib.bib376)) | peer-reviewed journal | ISIC2016 ISIC2017
    PH² | skip con. attention mod. | CE | 84.2% | ✓ | flipping | ✗ | ✗ |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| Tong 等 ([2021](#bib.bib376)) | 同行评审期刊 | ISIC2016 ISIC2017 PH² | 跳跃连接、注意力模块
    | CE | 84.2% | ✓ | 翻转 | ✗ | ✗ |'
- en: '| Bagheri et al. ([2021b](#bib.bib34)) | peer-reviewed journal | DermQuest
    ISIC2017 PH² | ensemble | CE Focal | 86.53% | ✓ | rotation flipping color jittering
    | ✓ | ✗ |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| Bagheri 等 ([2021b](#bib.bib34)) | 同行评审期刊 | DermQuest ISIC2017 PH² | 集成 |
    CE Focal | 86.53% | ✓ | 旋转、翻转、颜色抖动 | ✓ | ✗ |'
- en: '| Ren et al. ([2021](#bib.bib324)) | peer-reviewed journal | ISIC2017 | dense
    con. dilated conv. separable conv. attention mod. | Dice CE | 76.92% | ✗ | flipping,
    rotation | ✗ | ✗ |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| Ren 等 ([2021](#bib.bib324)) | 同行评审期刊 | ISIC2017 | 密集连接、膨胀卷积、可分离卷积、注意力模块 |
    Dice CE | 76.92% | ✗ | 翻转、旋转 | ✗ | ✗ |'
- en: '| Liu et al. ([2021a](#bib.bib260)) | peer-reviewed journal | ISIC2017 | residual
    con. dilated conv. pyramid pooling | WCE | 79.46% | ✗ | flipping, cropping rotation
    image deformation | ✗ | ✗ |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等 ([2021a](#bib.bib260)) | 同行评审期刊 | ISIC2017 | 残差连接、膨胀卷积、金字塔池化 | WCE
    | 79.46% | ✗ | 翻转、裁剪、旋转、图像变形 | ✗ | ✗ |'
- en: '| Khan et al. ([2021](#bib.bib224)) | peer-reviewed journal | ISIC2018 | skip
    con. image pyramid | Dice | 85.10% | ✗ | - | ✗ | ✓ |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| Khan 等 ([2021](#bib.bib224)) | 同行评审期刊 | ISIC2018 | 跳跃连接、图像金字塔 | Dice | 85.10%
    | ✗ | - | ✗ | ✓ |'
- en: '| Redekop and Chernyavskiy ([2021](#bib.bib321)) | peer-reviewed conference
    | ISIC2017 | - | - | 68.77$\%^{*}$ | ✗ | - | ✗ | ✗ |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| Redekop 和 Chernyavskiy ([2021](#bib.bib321)) | 同行评审会议 | ISIC2017 | - | -
    | 68.77$\%^{*}$ | ✗ | - | ✗ | ✗ |'
- en: '| Kaul et al. ([2021](#bib.bib216)) | peer-reviewed conference | ISIC2018 |
    skip con. residual con. attention mod. | CE Tversky adaptive logarithmic | 82.71%
    | ✗ | - | ✗ | ✓ |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| Kaul 等 ([2021](#bib.bib216)) | 同行评审会议 | ISIC2018 | 跳跃连接、残差连接、注意力模块 | CE Tversky
    自适应对数 | 82.71% | ✗ | - | ✗ | ✓ |'
- en: '| Abhishek and Hamarneh ([2021](#bib.bib6)) | peer-reviewed conference | ISIC2017
    PH² DermoFit | skip con. | MCC | 75.18% | ✗ | flipping, rotation | ✗ | ✓ |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| Abhishek 和 Hamarneh ([2021](#bib.bib6)) | 同行评审会议 | ISIC2017 PH² DermoFit
    | 跳跃连接 | MCC | 75.18% | ✗ | 翻转、旋转 | ✗ | ✓ |'
- en: '| Tang et al. ([2021b](#bib.bib373)) | peer-reviewed journal | ISIC2018 | skip
    con. | CE | 78.25% | ✗ | - | ✗ | ✗ |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| Tang 等 ([2021b](#bib.bib373)) | 同行评审期刊 | ISIC2018 | 跳跃连接 | CE | 78.25% |
    ✗ | - | ✗ | ✗ |'
- en: '| Xie et al. ([2021](#bib.bib421)) | peer-reviewed conference | ISIC2018 |
    dilated conv. semi-supervised | CE KL div. | 82.37% | ✗ | scaling,rotation elastic
    transformation | ✗ | ✗ |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| Xie 等 ([2021](#bib.bib421)) | 同行评审会议 | ISIC2018 | 膨胀卷积、半监督 | CE KL 散度 | 82.37%
    | ✗ | 缩放、旋转、弹性变换 | ✗ | ✗ |'
- en: '| Poudel and Lee ([2021](#bib.bib310)) | peer-reviewed journal | ISIC2017 |
    skip con. attention mod. | CE | 87.44% | ✗ | scaling, flipping rotation Gaussian
    noise median blur | ✗ | ✗ |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| Poudel and Lee ([2021](#bib.bib310)) | 同行评审期刊 | ISIC2017 | 跳跃连接，注意力模块 | CE
    | 87.44% | ✗ | 缩放、翻转、旋转、高斯噪声、中值模糊 | ✗ | ✗ |'
- en: '| Şahin et al. ([2021](#bib.bib338)) | peer-reviewed journal | ISIC2016 ISIC
    2017 | skip con. Gaussian process | - | 74.51% | ✗ | resize rotation reflection
    | ✓ | ✗ |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| Şahin et al. ([2021](#bib.bib338)) | 同行评审期刊 | ISIC2016 ISIC 2017 | 跳跃连接，高斯过程
    | - | 74.51% | ✗ | 调整大小、旋转、反射 | ✓ | ✗ |'
- en: '| Sarker et al. ([2021](#bib.bib343)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 | parallel m.s. conv. attention mod. GAN | $\ell_{1}$ Jaccard | 81.98% |
    ✗ | flipping, contrast gamma reconstruction | ✗ | ✗ |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| Sarker et al. ([2021](#bib.bib343)) | 同行评审期刊 | ISIC 2017 ISIC 2018 | 平行多尺度卷积，注意力模块，GAN
    | $\ell_{1}$ Jaccard | 81.98% | ✗ | 翻转、对比度、伽马校正、重建 | ✗ | ✗ |'
- en: '| Wang et al. ([2021b](#bib.bib406)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 | residual con. skip con. lesion-based pooling feature fusion | CE | 82.4%
    | ✗ | flipping, scaling cropping | ✗ | ✗ |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. ([2021b](#bib.bib406)) | 同行评审期刊 | ISIC 2016 ISIC 2017 | 残差连接，跳跃连接，基于病变的池化，特征融合
    | CE | 82.4% | ✗ | 翻转、缩放、裁剪 | ✗ | ✗ |'
- en: '| Sachin et al. ([2021](#bib.bib335)) | book chapter | ISIC 2018 | residual
    con. skip con. | - | 75.96% | ✗ | flipping, scaling color jittering | ✗ | ✗ |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| Sachin et al. ([2021](#bib.bib335)) | 书籍章节 | ISIC 2018 | 残差连接，跳跃连接 | - |
    75.96% | ✗ | 翻转、缩放、颜色抖动 | ✗ | ✗ |'
- en: '| Wibowo et al. ([2021](#bib.bib414)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 PH2 | BConvLSTM separable conv. residual con. skip con. | Jaccard | 80.25%
    | ✗ | distortion, blur color jittering contrast gamma sharpen | ✓ | ✓ |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| Wibowo et al. ([2021](#bib.bib414)) | 同行评审期刊 | ISIC 2017 ISIC 2018 PH2 |
    BConvLSTM 可分离卷积，残差连接，跳跃连接 | Jaccard | 80.25% | ✗ | 失真、模糊、颜色抖动、对比度、伽马校正、锐化 | ✓
    | ✓ |'
- en: '| Gudhe et al. ([2021](#bib.bib162)) | peer-reviewed journal | ISIC 2018 |
    dilated conv. residual con. skip con. | CE | 91% | ✗ | flipping, scaling shearing,
    color jittering Gaussian blur Gaussian noise | ✗ | ✓ |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| Gudhe et al. ([2021](#bib.bib162)) | 同行评审期刊 | ISIC 2018 | 扩张卷积，残差连接，跳跃连接
    | CE | 91% | ✗ | 翻转、缩放、剪切、颜色抖动、高斯模糊、高斯噪声 | ✗ | ✓ |'
- en: '| Khouloud et al. ([2021](#bib.bib226)) | peer-reviewed journal | ISIC 2016
    ISIC 2017 ISIC 2018 PH2 | feature pyramid residual con. skip con. attention mod.
    | - | 86.92%^∗ | ✗ | - | ✗ | ✗ |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| Khouloud et al. ([2021](#bib.bib226)) | 同行评审期刊 | ISIC 2016 ISIC 2017 ISIC
    2018 PH2 | 特征金字塔，残差连接，跳跃连接，注意力模块 | - | 86.92%^∗ | ✗ | - | ✗ | ✗ |'
- en: '| Gu et al. ([2021](#bib.bib159)) | peer-reviewed conference | ISIC 2017 |
    asymmetric conv. skip con. | DS | 79.4% | ✗ | cropping, flipping rotation | ✗
    | ✗ |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| Gu et al. ([2021](#bib.bib159)) | 同行评审会议 | ISIC 2017 | 非对称卷积，跳跃连接 | DS |
    79.4% | ✗ | 裁剪、翻转、旋转 | ✗ | ✗ |'
- en: '| Zhao et al. ([2021](#bib.bib448)) | peer-reviewed journal | ISIC 2018 | pyramid
    pooling attention mod. residual con. skip con. | CE Dice | 86.84% | ✗ | cropping
    | ✗ | ✗ |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. ([2021](#bib.bib448)) | 同行评审期刊 | ISIC 2018 | 金字塔池化，注意力模块，残差连接，跳跃连接
    | CE Dice | 86.84% | ✗ | 裁剪 | ✗ | ✗ |'
- en: '| Tang et al. ([2021a](#bib.bib372)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 ISIC 2018 | attention mod. residual con. skip con. ensemble pyramid pooling
    | Focal | 80.7% | ✗ | copying | ✗ | ✗ |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| Tang et al. ([2021a](#bib.bib372)) | 同行评审期刊 | ISIC 2016 ISIC 2017 ISIC 2018
    | 注意力模块，残差连接，跳跃连接，集成，金字塔池化 | Focal | 80.7% | ✗ | 复制 | ✗ | ✗ |'
- en: '| Zunair and Hamza ([2021](#bib.bib460)) | peer-reviewed journal | ISIC 2018
    | sharpening kernel residual con. | CE | 79.78% | ✗ | - | ✗ | ✓ |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| Zunair and Hamza ([2021](#bib.bib460)) | 同行评审期刊 | ISIC 2018 | 锐化核，残差连接 |
    CE | 79.78% | ✗ | - | ✗ | ✓ |'
- en: '| Li et al. ([2021a](#bib.bib245)) | peer-reviewed conference | ISIC 2017 |
    skip con. | CE KL div. | 71.12%* | ✗ | - | ✗ | ✓ |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. ([2021a](#bib.bib245)) | 同行评审会议 | ISIC 2017 | 跳跃连接 | CE KL 散度 |
    71.12%* | ✗ | - | ✗ | ✓ |'
- en: '| Zhang et al. ([2021a](#bib.bib442)) | peer-reviewed conference | ISIC 2016
    | skip con. residual con. feature fusion semi-supervised self-supervised | CE
    Dice | 80.49% | ✗ | flipping, rotation zooming, cropping | ✗ | ✓ |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. ([2021a](#bib.bib442)) | 同行评审会议 | ISIC 2016 | 跳跃连接，残差连接，特征融合，半监督，自监督
    | CE Dice | 80.49% | ✗ | 翻转、旋转、缩放、裁剪 | ✗ | ✓ |'
- en: '| Xu et al. ([2021](#bib.bib422)) | peer-reviewed conference | ISIC 2018 |
    Transformer multi-scale | Dice | 89.6% | ✗ | flipping, rotation | ✗ | ✗ |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. ([2021](#bib.bib422)) | 同行评审会议 | ISIC 2018 | Transformer 多尺度 |
    Dice | 89.6% | ✗ | 翻转、旋转 | ✗ | ✗ |'
- en: '| Ahn et al. ([2021](#bib.bib14)) | peer-reviewed conference | PH² | self-supervised
    clustering | CE Spatial loss Consistency loss | 71.53%* | ✗ | - | ✗ | ✓ |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| Ahn et al. ([2021](#bib.bib14)) | 同行评审会议 | PH² | 自监督聚类 | CE 空间损失 一致性损失 |
    71.53%* | ✗ | - | ✗ | ✓ |'
- en: '| Zhang et al. ([2021b](#bib.bib445)) | peer-reviewed conference | ISIC 2017
    | skip con. feature fusion Transformer | CE Jaccard | 79.5% | ✗ | rotation, flipping
    color jittering | ✗ | ✓ |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等 ([2021b](#bib.bib445)) | 同行评审会议 | ISIC 2017 | skip con. 特征融合 Transformer
    | CE Jaccard | 79.5% | ✗ | 旋转，翻转，颜色抖动 | ✗ | ✓ |'
- en: '| Ji et al. ([2021](#bib.bib202)) | peer-reviewed conference | ISIC 2018 |
    skip con. multi-scale Transformer | CE Dice | 82.4%* | ✗ | flipping | ✗ | ✓ |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| Ji 等 ([2021](#bib.bib202)) | 同行评审会议 | ISIC 2018 | skip con. multi-scale Transformer
    | CE Dice | 82.4%* | ✗ | 翻转 | ✗ | ✓ |'
- en: '| Wang et al. ([2021a](#bib.bib399)) | peer-reviewed conference | ISIC 2016
    ISIC 2018 PH² | multi-scale Transformer | CE Dice | 84.3%* | ✓ | flipping, scaling
    | ✗ | ✓ |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 ([2021a](#bib.bib399)) | 同行评审会议 | ISIC 2016 ISIC 2018 PH² | multi-scale
    Transformer | CE Dice | 84.3%* | ✓ | 翻转，缩放 | ✗ | ✓ |'
- en: '| Yang et al. ([2021](#bib.bib425)) | peer-reviewed journal | ISIC 2018 PH²
    | skip con. multi-scale feature fusion | CE Dice | 94.0% | ✗ | rotation, flipping
    cropping, HSC manipulation, luminance and contrast shift | ✗ | ✗ |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等 ([2021](#bib.bib425)) | 同行评审期刊 | ISIC 2018 PH² | skip con. multi-scale
    特征融合 | CE Dice | 94.0% | ✗ | 旋转，翻转，裁剪，HSC 操作，亮度和对比度变化 | ✗ | ✗ |'
- en: '| Tao et al. ([2021](#bib.bib375)) | peer-reviewed journal | ISIC 2017 PH²
    | skip con. dense con. attention mod. multi-scale | - | 78.85% | ✗ | rotation
    | ✗ | ✗ |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| Tao 等 ([2021](#bib.bib375)) | 同行评审期刊 | ISIC 2017 PH² | skip con. dense con.
    attention mod. multi-scale | - | 78.85% | ✗ | 旋转 | ✗ | ✗ |'
- en: '| Kim and Lee ([2021](#bib.bib227)) | peer-reviewed journal | ISIC 2016 PH²
    | residual con. skip con. | boundary aware loss | 84.33%* | ✗ | - | ✗ | ✗ |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| Kim 和 Lee ([2021](#bib.bib227)) | 同行评审期刊 | ISIC 2016 PH² | residual con.
    skip con. | 边界感知损失 | 84.33%* | ✗ | - | ✗ | ✗ |'
- en: '| Dai et al. ([2022](#bib.bib107)) | peer-reviewed journal | ISIC2018 PH2 |
    residual con. skip con. dilated conv. image pyramid attention mod. | CE Dice SoftDice
    | 83.45% | ✓ | cropping, flipping rotation | ✗ | ✗ |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| Dai 等 ([2022](#bib.bib107)) | 同行评审期刊 | ISIC2018 PH2 | residual con. skip
    con. 膨胀卷积 图像金字塔 attention mod. | CE Dice SoftDice | 83.45% | ✓ | 裁剪，翻转，旋转 | ✗
    | ✗ |'
- en: '| Bi et al. ([2022](#bib.bib50)) | peer-reviewed journal | ISIC2016 ISIC2017
    PH2 | residual con. skip con. attention mod. feature fusion | CE | 83.70% | ✓
    | cropping, flipping | ✗ | ✗ |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| Bi 等 ([2022](#bib.bib50)) | 同行评审期刊 | ISIC2016 ISIC2017 PH2 | residual con.
    skip con. attention mod. 特征融合 | CE | 83.70% | ✓ | 裁剪，翻转 | ✗ | ✗ |'
- en: '| Lin et al. ([2022](#bib.bib253)) | peer-reviewed conference | ISIC 2017 ISIC
    2018 | attention mod. Transformer | CE Jaccard DS | 77.81%* | ✗ | flipping, rotation
    | ✗ | ✗ |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| Lin 等 ([2022](#bib.bib253)) | 同行评审会议 | ISIC 2017 ISIC 2018 | attention mod.
    Transformer | CE Jaccard DS | 77.81%* | ✗ | 翻转，旋转 | ✗ | ✗ |'
- en: '| Wu et al. ([2022b](#bib.bib417)) | peer-reviewed conference | PH² | skip
    con. Transformer multi-scale | CE | 70.0%* | ✗ | - | ✗ | ✗ |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等 ([2022b](#bib.bib417)) | 同行评审会议 | PH² | skip con. Transformer multi-scale
    | CE | 70.0%* | ✗ | - | ✗ | ✗ |'
- en: '| Valanarasu and Patel ([2022](#bib.bib387)) | peer-reviewed conference | ISIC
    2018 | skip con. | CE Dice | 81.7% | ✗ | - | ✗ | ✓ |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| Valanarasu 和 Patel ([2022](#bib.bib387)) | 同行评审会议 | ISIC 2018 | skip con.
    | CE Dice | 81.7% | ✗ | - | ✗ | ✓ |'
- en: '| Basak et al. ([2022](#bib.bib42)) | peer-reviewed journal | ISIC 2017 PH²
    HAM10000 | residual con. multi-scale attention mod. | CE Jaccard DS | 97.4% |
    ✗ | - | ✗ | ✓ |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| Basak 等 ([2022](#bib.bib42)) | 同行评审期刊 | ISIC 2017 PH² HAM10000 | residual
    con. multi-scale attention mod. | CE Jaccard DS | 97.4% | ✗ | - | ✗ | ✓ |'
- en: '| Wu et al. ([2022a](#bib.bib415)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 ISIC 2018 PH² | skip con. residual con. attention mod. Transformer | CE Dice
    | 76.53% | ✗ | flipping, rotation brightness change contrast change change in
    H,S,V | ✗ | ✓ |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等 ([2022a](#bib.bib415)) | 同行评审期刊 | ISIC 2016 ISIC 2017 ISIC 2018 PH²
    | skip con. residual con. attention mod. Transformer | CE Dice | 76.53% | ✗ |
    翻转，旋转，亮度变化，对比度变化，H,S,V 变化 | ✗ | ✓ |'
- en: '| Liu et al. ([2022a](#bib.bib261)) | peer-reviewed journal | ISIC 2017 | skip
    con. residual con. dilated conv. attention mod. | CE Dice | 78.62% | ✗ | flipping,
    rotation | ✗ | ✗ |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等 ([2022a](#bib.bib261)) | 同行评审期刊 | ISIC 2017 | skip con. residual con.
    膨胀卷积 attention mod. | CE Dice | 78.62% | ✗ | 翻转，旋转 | ✗ | ✗ |'
- en: '| Wang et al. ([2022b](#bib.bib402)) | peer-reviewed journal | ISIC 2017 |
    skip con. residual con. Transformer | - | 84.52% | ✗ | flipping, rotation | ✗
    | ✓ |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 ([2022b](#bib.bib402)) | 同行评审期刊 | ISIC 2017 | skip con. residual con.
    Transformer | - | 84.52% | ✗ | 翻转，旋转 | ✗ | ✓ |'
- en: '| Zhang et al. ([2022a](#bib.bib444)) | peer-reviewed conference | ISIC 2017
    | skip con. feature fusion | Dice Focal | 74.54% | ✗ | flipping | ✗ | ✗ |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等 ([2022a](#bib.bib444)) | 同行评审会议 | ISIC 2017 | skip con. 特征融合 | Dice
    Focal | 74.54% | ✗ | 翻转 | ✗ | ✗ |'
- en: '| Wang et al. ([2022d](#bib.bib410)) | peer-reviewed conference | ISIC 2017
    PH² | skip con. residual con. self-supervised | Dice | 76.5% | ✓ | rotation, flipping
    color jittering | ✗ | ✗ |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 ([2022d](#bib.bib410)) | 同行评审会议 | ISIC 2017 PH² | skip con. residual
    con. 自监督 | Dice | 76.5% | ✓ | 旋转，翻转，颜色抖动 | ✗ | ✗ |'
- en: '| Dong et al. ([2022](#bib.bib126)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 ISIC 2018 | residual con. skip con. Transformer feature fusion | CE Dice
    | 74.55% | ✗ | - | ✗ | ✗ |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| Dong 等人 ([2022](#bib.bib126)) | 同行评审期刊 | ISIC 2016 ISIC 2017 ISIC 2018 |
    残差连接跳跃连接 Transformer 特征融合 | CE Dice | 74.55% | ✗ | - | ✗ | ✗ |'
- en: '| Chen et al. ([2022](#bib.bib89)) | peer-reviewed journal | ISIC 2017 PH²
    | skip con. attention mod. recurrent net. | CE | 80.36% | ✓ | flipping, rotation
    affine trans. masking, mesh distortion | ✗ | ✗ |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 ([2022](#bib.bib89)) | 同行评审期刊 | ISIC 2017 PH² | 跳跃连接注意力模块递归网络 | CE
    | 80.36% | ✓ | 翻转、旋转仿射变换、掩码、网格扭曲 | ✗ | ✗ |'
- en: '| Kaur et al. ([2022b](#bib.bib218)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 ISIC 2018 PH² | dilated conv. | CE | 81.7% | ✓ | scaling, rotation translation
    | ✗ | ✗ |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| Kaur 等人 ([2022b](#bib.bib218)) | 同行评审期刊 | ISIC 2016 ISIC 2017 ISIC 2018 PH²
    | 膨胀卷积 | CE | 81.7% | ✓ | 缩放、旋转平移 | ✗ | ✗ |'
- en: '| Badshah and Ahmad ([2022](#bib.bib31)) | peer-reviewed journal | ISIC 2018
    | residual con. BConvLSTM | - | 94.5% | ✗ | - | ✗ | ✗ |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| Badshah 和 Ahmad ([2022](#bib.bib31)) | 同行评审期刊 | ISIC 2018 | 残差连接 BConvLSTM
    | - | 94.5% | ✗ | - | ✗ | ✗ |'
- en: '| Alam et al. ([2022](#bib.bib20)) | peer-reviewed journal | HAM10000 | residual
    con. separable conv. | Dice | 91.1% | ✗ | - | ✗ | ✓ |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| Alam 等人 ([2022](#bib.bib20)) | 同行评审期刊 | HAM10000 | 残差连接可分离卷积 | Dice | 91.1%
    | ✗ | - | ✗ | ✓ |'
- en: '| Yu et al. ([2022](#bib.bib429)) | peer-reviewed journal | ISIC 2018 | skip
    con. attention mod. multi-scale | - | 87.89% | ✗ | - | ✗ | ✗ |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| Yu 等人 ([2022](#bib.bib429)) | 同行评审期刊 | ISIC 2018 | 跳跃连接注意力模块多尺度 | - | 87.89%
    | ✗ | - | ✗ | ✗ |'
- en: '| Jiang et al. ([2022](#bib.bib204)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 | skip con. attention mod. ConvLSTM | CE Jaccard | 80.5% | ✗ | - | ✗ | ✗
    |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| Jiang 等人 ([2022](#bib.bib204)) | 同行评审期刊 | ISIC 2017 ISIC 2018 | 跳跃连接注意力模块
    ConvLSTM | CE Jaccard | 80.5% | ✗ | - | ✗ | ✗ |'
- en: '| Ramadan et al. ([2022](#bib.bib317)) | peer-reviewed journal | ISIC 2018
    | skip con. attention mod. | CE Dice sens.-spec. loss | 91.4% | ✗ | - | ✗ | ✗
    |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| Ramadan 等人 ([2022](#bib.bib317)) | 同行评审期刊 | ISIC 2018 | 跳跃连接注意力模块 | CE Dice
    灵敏度-特异度损失 | 91.4% | ✗ | - | ✗ | ✗ |'
- en: '| Zhang et al. ([2022b](#bib.bib447)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 | skip con. dense con. semi-supervised | CE contrastive loss | 73.89% | ✗
    | scaling, flipping color distortion | ✗ | ✗ |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等人 ([2022b](#bib.bib447)) | 同行评审期刊 | ISIC 2017 ISIC 2018 | 跳跃连接密集连接半监督
    | CE 对比损失 | 73.89% | ✗ | 缩放、翻转颜色失真 | ✗ | ✗ |'
- en: '| Tran and Pham ([2022](#bib.bib379)) | peer-reviewed journal | ISIC 2017 PH²
    | skip con. attention mod. | Focal Tversky fuzzy loss | 79.2% | ✗ | rotation,
    zooming flipping | ✗ | ✗ |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| Tran 和 Pham ([2022](#bib.bib379)) | 同行评审期刊 | ISIC 2017 PH² | 跳跃连接注意力模块 |
    Focal Tversky 模糊损失 | 79.2% | ✗ | 旋转、缩放翻转 | ✗ | ✗ |'
- en: '| Wang and Wang ([2022](#bib.bib407)) | peer-reviewed journal | ISIC 2017 |
    skip con. residual con. attention mod. | CE Jaccard | 78.28% | ✗ | rotation, zooming
    resizing, shifting | ✗ | ✗ |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| Wang 和 Wang ([2022](#bib.bib407)) | 同行评审期刊 | ISIC 2017 | 跳跃连接残差连接注意力模块 |
    CE Jaccard | 78.28% | ✗ | 旋转、缩放调整、位移 | ✗ | ✗ |'
- en: '| Zhao et al. ([2022b](#bib.bib452)) | peer-reviewed conference | ISIC 2017
    | skip con. self-supervised | CE Dice | 67.08%* | ✗ | - | ✗ | ✗ |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| Zhao 等人 ([2022b](#bib.bib452)) | 同行评审会议 | ISIC 2017 | 跳跃连接自监督 | CE Dice |
    67.08%* | ✗ | - | ✗ | ✗ |'
- en: '| Wang et al. ([2022c](#bib.bib409)) | peer-reviewed conference | PH² | few
    shot mask avg. pooling | Dice | 86.97%* | ✗ | - | ✗ | ✗ |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([2022c](#bib.bib409)) | 同行评审会议 | PH² | 少量样本掩膜平均池化 | Dice | 86.97%*
    | ✗ | - | ✗ | ✗ |'
- en: '| Wang et al. ([2022a](#bib.bib398)) | peer-reviewed conference | ISIC 2017
    ISIC 2018 | residual con. dilated conv. multi-scale feature fusion Transformer
    | CE Jaccard | 78.76% | ✗ | - | ✗ | ✗ |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([2022a](#bib.bib398)) | 同行评审会议 | ISIC 2017 ISIC 2018 | 残差连接膨胀卷积多尺度特征融合
    Transformer | CE Jaccard | 78.76% | ✗ | - | ✗ | ✗ |'
- en: '| Liu et al. ([2022b](#bib.bib262)) | peer-reviewed conference | ISIC 2017
    ISIC 2018 | skip con. dilated conv. multi-scale pyramid pooling Transformer |
    CE | 80.19% | ✗ | - | ✗ | ✗ |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 ([2022b](#bib.bib262)) | 同行评审会议 | ISIC 2017 ISIC 2018 | 跳跃连接膨胀卷积多尺度金字塔池化
    Transformer | CE | 80.19% | ✗ | - | ✗ | ✗ |'
- en: '| Gu et al. ([2022](#bib.bib161)) | peer-reviewed journal | ISIC 2017 | skip
    con. global adaptive pooling | CE $\ell_{2}$ | 80.53% | ✗ | scaling, rotation
    flipping | ✗ | ✗ |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| Gu 等人 ([2022](#bib.bib161)) | 同行评审期刊 | ISIC 2017 | 跳跃连接全局自适应池化 | CE $\ell_{2}$
    | 80.53% | ✗ | 缩放、旋转翻转 | ✗ | ✗ |'
- en: '| Khan et al. ([2022](#bib.bib225)) | peer-reviewed journal | ISIC 2017 PH²
    | residual con. attention mod. ensemble | CE | 79.2% | ✗ | - | ✗ | ✗ |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| Khan 等人 ([2022](#bib.bib225)) | 同行评审期刊 | ISIC 2017 PH² | 残差连接注意力模块集成 | CE
    | 79.2% | ✗ | - | ✗ | ✗ |'
- en: '| Alahmadi and Alghamdi ([2022](#bib.bib19)) | peer-reviewed journal | ISIC
    2017 ISIC 2018 PH² | skip con. feature fusion semi-supervised Transformer | CE
    Dice $\ell_{2}$ | 82.78%* | ✗ | - | ✗ | ✗ |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| Alahmadi和Alghamdi ([2022](#bib.bib19)) | 同行评审期刊 | ISIC 2017 ISIC 2018 PH²
    | 跳过 con. 特征融合半监督 Transformer | CE Dice $\ell_{2}$ | 82.78%* | ✗ | - | ✗ | ✗ |'
- en: '| Li et al. ([2022](#bib.bib252)) | peer-reviewed journal | ISIC 2018 | skip
    con. residual con. dilated conv. attention mod. pyramid pooling multi-scale |
    CE Dice | 88.92% | ✗ | flipping, rotation | ✗ | ✗ |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| Li等（2022）([2022](#bib.bib252)) | 同行评审期刊 | ISIC 2018 | 跳过 con. 残差 con. 膨胀卷积
    注意力模块 金字塔池化 多尺度 | CE Dice | 88.92% | ✗ | 翻转，旋转 | ✗ | ✗ |'
- en: '| Kaur et al. ([2022a](#bib.bib217)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 ISIC 2018 PH² | - | Tversky | 77.8% | ✓ | rotation, scaling | ✗ | ✗ |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| Kaur等（2022a）([2022a](#bib.bib217)) | 同行评审期刊 | ISIC 2016 ISIC 2017 ISIC 2018
    PH² | - | Tversky | 77.8% | ✓ | 旋转，缩放 | ✗ | ✗ |'
- en: References
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abbas et al. (2011) Abbas, Q., Celebi, M.E., Garcia, I.F., 2011. Hair Removal
    Methods: A Comparative Study for Dermoscopy Images. Biomedical Signal Processing
    and Control 6, 395–404.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbas等（2011）Abbas, Q., Celebi, M.E., Garcia, I.F., 2011. **毛发去除方法**：对皮肤镜图像的比较研究。生物医学信号处理与控制
    6, 395–404。
- en: 'Abbasi et al. (2004) Abbasi, N.R., Shaw, H.M., Rigel, D.S., Friedman, R.J.,
    McCarthy, W.H., Osman, I., Kopf, A.W., Polsky, D., 2004. Early diagnosis of cutaneous
    melanoma: Revisiting the ABCD criteria. Jama 292, 2771–2776.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbasi等（2004）Abbasi, N.R., Shaw, H.M., Rigel, D.S., Friedman, R.J., McCarthy,
    W.H., Osman, I., Kopf, A.W., Polsky, D., 2004. **早期诊断皮肤黑色素瘤**：重新审视ABCD标准。Jama
    292, 2771–2776。
- en: Abdelhalim et al. (2021) Abdelhalim, I.S.A., Mohamed, M.F., Mahdy, Y.B., 2021.
    Data Augmentation For Skin Lesion Using Self-Attention Based Progressive Generative
    Adversarial Network. Expert Systems with Applications 165, 113922.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelhalim等（2021）Abdelhalim, I.S.A., Mohamed, M.F., Mahdy, Y.B., 2021. **基于自注意力的渐进生成对抗网络**用于皮肤病变的数据增强。专家系统与应用
    165, 113922。
- en: 'Abhishek (2020) Abhishek, K., 2020. Input Space Augmentation for Skin Lesion
    Segmentation in Dermoscopic Images. Master’s thesis. Applied Sciences: School
    of Computing Science, Simon Fraser University. [https://summit.sfu.ca/item/20247](https://summit.sfu.ca/item/20247).'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abhishek（2020）Abhishek, K., 2020. **输入空间增强**用于皮肤病变分割。硕士论文。应用科学：计算机科学学院，西蒙弗雷泽大学。
    [https://summit.sfu.ca/item/20247](https://summit.sfu.ca/item/20247)。
- en: 'Abhishek and Hamarneh (2019) Abhishek, K., Hamarneh, G., 2019. Mask2Lesion:
    Mask-constrained adversarial skin lesion image synthesis, in: International Workshop
    on Simulation and Synthesis in Medical Imaging, Springer. pp. 71–80.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abhishek和Hamarneh（2019）Abhishek, K., Hamarneh, G., 2019. **Mask2Lesion**：掩膜约束的对抗性皮肤病变图像合成，载于：国际医学成像模拟与合成研讨会，Springer。页码71–80。
- en: 'Abhishek and Hamarneh (2021) Abhishek, K., Hamarneh, G., 2021. Matthews correlation
    coefficient loss for deep convolutional networks: Application to skin lesion segmentation,
    in: 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), IEEE.
    pp. 225–229.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abhishek和Hamarneh（2021）Abhishek, K., Hamarneh, G., 2021. **马修斯相关系数损失**用于深度卷积网络：应用于皮肤病变分割，载于：2021
    IEEE第18届国际生物医学成像研讨会（ISBI），IEEE。页码225–229。
- en: 'Abhishek et al. (2020) Abhishek, K., Hamarneh, G., Drew, M.S., 2020. Illumination-based
    transformations improve skin lesion segmentation in dermoscopic images, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,
    pp. 728–729.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abhishek等（2020）Abhishek, K., Hamarneh, G., Drew, M.S., 2020. 基于**光照**的变换改善皮肤病变分割，载于：IEEE/CVF计算机视觉与模式识别会议工作坊论文集，页码728–729。
- en: Abhishek et al. (2021) Abhishek, K., Kawahara, J., Hamarneh, G., 2021. Predicting
    the clinical management of skin lesions using deep learning. Scientific reports
    11, 1–14.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abhishek等（2021）Abhishek, K., Kawahara, J., Hamarneh, G., 2021. 利用**深度学习**预测皮肤病变的临床管理。科学报告
    11, 1–14。
- en: 'Abraham and Khan (2019) Abraham, N., Khan, N.M., 2019. A novel focal tversky
    loss function with improved attention U-Net for lesion segmentation, in: 2019
    IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), IEEE. pp.
    683–687.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abraham和Khan（2019）Abraham, N., Khan, N.M., 2019. 一种**新型焦点Tversky损失函数**结合改进的注意力U-Net用于病变分割，载于：2019
    IEEE第16届国际生物医学成像研讨会（ISBI 2019），IEEE。页码683–687。
- en: 'Adegun and Viriri (2019) Adegun, A., Viriri, S., 2019. An enhanced deep learning
    framework for skin lesions segmentation, in: International conference on computational
    collective intelligence, Springer. pp. 414–425.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adegun和Viriri（2019）Adegun, A., Viriri, S., 2019. 一种**增强的深度学习框架**用于皮肤病变分割，载于：计算集体智能国际会议，Springer。页码414–425。
- en: 'Adegun and Viriri (2020a) Adegun, A., Viriri, S., 2020a. Deep learning techniques
    for skin lesion analysis and melanoma cancer detection: a survey of state-of-the-art.
    Artificial Intelligence Review , 1–31URL: [https://doi.org/10.1007/s10462-020-09865-y](https://doi.org/10.1007/s10462-020-09865-y),
    doi:[10.1007/s10462-020-09865-y](http://dx.doi.org/10.1007/s10462-020-09865-y).'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Adegun 和 Viriri（2020a）Adegun, A., Viriri, S., 2020a。皮肤病变分析和黑色素瘤癌症检测的深度学习技术：最前沿的调查。《人工智能评论》，1–31URL:
    [https://doi.org/10.1007/s10462-020-09865-y](https://doi.org/10.1007/s10462-020-09865-y)，doi:[10.1007/s10462-020-09865-y](http://dx.doi.org/10.1007/s10462-020-09865-y)。'
- en: Adegun and Viriri (2020b) Adegun, A.A., Viriri, S., 2020b. Fcn-based densenet
    framework for automated detection and classification of skin lesions in dermoscopy
    images. IEEE Access 8, 150377–150396.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adegun 和 Viriri（2020b）Adegun, A.A., Viriri, S., 2020b。基于 FCN 的 DenseNet 框架用于皮肤镜图像中皮肤病变的自动检测和分类。《IEEE
    Access》8，150377–150396。
- en: Ahmedt-Aristizabal et al. (2023) Ahmedt-Aristizabal, D., Nguyen, C., Tychsen-Smith,
    L., Stacey, A., Li, S., Pathikulangara, J., Petersson, L., Wang, D., 2023. Monitoring
    of pigmented skin lesions using 3D whole body imaging. Computer Methods and Programs
    in Biomedicine 232, 107451.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmedt-Aristizabal 等（2023）Ahmedt-Aristizabal, D., Nguyen, C., Tychsen-Smith,
    L., Stacey, A., Li, S., Pathikulangara, J., Petersson, L., Wang, D., 2023。使用 3D
    全身成像监测色素性皮肤病变。《生物医学计算方法与程序》232，107451。
- en: 'Ahn et al. (2021) Ahn, E., Feng, D., Kim, J., 2021. A spatial guided self-supervised
    clustering network for medical image segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 379–388.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn 等（2021）Ahn, E., Feng, D., Kim, J., 2021。用于医学图像分割的空间引导自监督聚类网络，在：国际医学图像计算与计算机辅助手术会议，Springer，第379–388页。
- en: Al-Masni et al. (2018) Al-Masni, M.A., Al-antari, M.A., Choi, M.T., Han, S.M.,
    Kim, T.S., 2018. Skin lesion segmentation in dermoscopy images via deep full resolution
    convolutional networks. Computer methods and programs in biomedicine 162, 221–231.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Masni 等（2018）Al-Masni, M.A., Al-antari, M.A., Choi, M.T., Han, S.M., Kim,
    T.S., 2018。通过深度全分辨率卷积网络进行皮肤病变分割。《生物医学计算方法与程序》162，221–231。
- en: 'Al-masni et al. (2019) Al-masni, M.A., Al-antari, M.A., Park, H.M., Park, N.H.,
    Kim, T.S., 2019. A deep learning model integrating FrCN and residual convolutional
    networks for skin lesion segmentation and classification, in: 2019 IEEE Eurasia
    Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS),
    IEEE. pp. 95–98.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-masni 等（2019）Al-masni, M.A., Al-antari, M.A., Park, H.M., Park, N.H., Kim,
    T.S., 2019。整合 FrCN 和残差卷积网络的深度学习模型用于皮肤病变分割和分类，在：2019年 IEEE 欧亚生物医学工程、医疗保健与可持续发展会议（ECBIOS），IEEE，第95–98页。
- en: Al-Masni et al. (2020) Al-Masni, M.A., Kim, D.H., Kim, T.S., 2020. Multiple
    skin lesions diagnostics via integrated deep convolutional networks for segmentation
    and classification. Computer methods and programs in biomedicine 190, 105351.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Masni 等（2020）Al-Masni, M.A., Kim, D.H., Kim, T.S., 2020。通过集成深度卷积网络进行多重皮肤病变诊断。《生物医学计算方法与程序》190，105351。
- en: 'Al Nazi and Abir (2020) Al Nazi, Z., Abir, T.A., 2020. Automatic skin lesion
    segmentation and melanoma detection: Transfer learning approach with U-Net and
    DCNN-SVM, in: Proceedings of International Joint Conference on Computational Intelligence,
    Springer. pp. 371–381.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al Nazi 和 Abir（2020）Al Nazi, Z., Abir, T.A., 2020。自动皮肤病变分割和黑色素瘤检测：基于 U-Net 和
    DCNN-SVM 的迁移学习方法，在：国际计算智能联合会议论文集，Springer，第371–381页。
- en: Alahmadi and Alghamdi (2022) Alahmadi, M.D., Alghamdi, W., 2022. Semi-supervised
    skin lesion segmentation with coupling cnn and transformer features. IEEE Access
    10, 122560–122569.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alahmadi 和 Alghamdi（2022）Alahmadi, M.D., Alghamdi, W., 2022。基于 CNN 和 Transformer
    特征的半监督皮肤病变分割。《IEEE Access》10，122560–122569。
- en: 'Alam et al. (2022) Alam, M.J., Mohammad, M.S., Hossain, M.A.F., Showmik, I.A.,
    Raihan, M.S., Ahmed, S., Mahmud, T.I., 2022. S2C-DeLeNet: A parameter transfer
    based segmentation-classification integration for detecting skin cancer lesions
    from dermoscopic images. Computers in Biology and Medicine 150, 106148.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alam 等（2022）Alam, M.J., Mohammad, M.S., Hossain, M.A.F., Showmik, I.A., Raihan,
    M.S., Ahmed, S., Mahmud, T.I., 2022。S2C-DeLeNet：一种基于参数转移的分割-分类整合方法，用于从皮肤镜图像中检测皮肤癌病变。《生物医学计算机》150，106148。
- en: 'Alom et al. (2020) Alom, M.Z., Aspiras, T., Taha, T.M., Asari, V.K., 2020.
    Skin cancer segmentation and classification with improved deep convolutional neural
    network, in: Proceedings of SPIE Medical Imaging 2020: Imaging Informatics for
    Healthcare, Research, and Applications, p. 1131814.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alom 等（2020）Alom, M.Z., Aspiras, T., Taha, T.M., Asari, V.K., 2020。通过改进的深度卷积神经网络进行皮肤癌分割和分类，在：2020年SPIE医学成像：医疗保健、研究和应用信息学会议记录，第1131814页。
- en: Alom et al. (2019) Alom, M.Z., Yakopcic, C., Hasan, M., Taha, T.M., Asari, V.K.,
    2019. Recurrent residual u-net for medical image segmentation. Journal of Medical
    Imaging 6, 014006.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alom 等人 (2019) Alom, M.Z., Yakopcic, C., Hasan, M., Taha, T.M., Asari, V.K.,
    2019. 用于医学图像分割的循环残差 U-Net。医学成像杂志 6, 014006。
- en: American Cancer Society (2023) American Cancer Society, 2023. Cancer facts and
    figures 2023. [https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/annual-cancer-facts-and-figures/2023/2023-cancer-facts-and-figures.pdf](https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/annual-cancer-facts-and-figures/2023/2023-cancer-facts-and-figures.pdf).
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国癌症协会 (2023) American Cancer Society, 2023. 2023年癌症事实与数字。[https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/annual-cancer-facts-and-figures/2023/2023-cancer-facts-and-figures.pdf](https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/annual-cancer-facts-and-figures/2023/2023-cancer-facts-and-figures.pdf)。
- en: Andrade et al. (2020) Andrade, C., Teixeira, L.F., Vasconcelos, M.J.M., Rosado,
    L., 2020. Data augmentation using adversarial image-to-image translation for the
    segmentation of mobile-acquired dermatological images. Journal of Imaging 7, 2.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrade 等人 (2020) Andrade, C., Teixeira, L.F., Vasconcelos, M.J.M., Rosado,
    L., 2020. 利用敌对图像翻译的数据增强进行移动获取皮肤病理图像分割。成像杂志 7, 2。
- en: Argenziano et al. (2000) Argenziano, G., Soyer, H.P., De Giorgio, V., Piccolo,
    D., Carli, P., Delfino, M., Ferrari, A., Hofmann-Wellenhof, R., Massi, D., Mazzocchetti,
    G., Scalvenzi, M., Wolf, I.H., 2000. Interactive Atlas of Dermoscopy. Edra Medical
    Publishing and New Media.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argenziano 等人 (2000) Argenziano, G., Soyer, H.P., De Giorgio, V., Piccolo, D.,
    Carli, P., Delfino, M., Ferrari, A., Hofmann-Wellenhof, R., Massi, D., Mazzocchetti,
    G., Scalvenzi, M., Wolf, I.H., 2000. 皮肤镜互动图谱。Edra医学出版与新媒体。
- en: Arora et al. (2021) Arora, R., Raman, B., Nayyar, K., Awasthi, R., 2021. Automated
    skin lesion segmentation using attention-based deep convolutional neural network.
    Biomedical Signal Processing and Control 65, 102358.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora 等人 (2021) Arora, R., Raman, B., Nayyar, K., Awasthi, R., 2021. 基于注意力的深度卷积神经网络的自动皮肤病变分割。生物医学信号处理与控制
    65, 102358。
- en: 'Asgari Taghanaki et al. (2021) Asgari Taghanaki, S., Abhishek, K., Cohen, J.P.,
    Cohen-Adad, J., Hamarneh, G., 2021. Deep semantic segmentation of natural and
    medical images: a review. Artificial Intelligence Review 54, 137–178.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asgari Taghanaki 等人 (2021) Asgari Taghanaki, S., Abhishek, K., Cohen, J.P.,
    Cohen-Adad, J., Hamarneh, G., 2021. 自然和医学图像的深度语义分割：一项综述。人工智能评论 54, 137–178。
- en: 'Attia et al. (2017) Attia, M., Hossny, M., Nahavandi, S., Yazdabadi, A., 2017.
    Skin melanoma segmentation using recurrent and convolutional neural networks,
    in: 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),
    IEEE. pp. 292–296.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Attia 等人 (2017) Attia, M., Hossny, M., Nahavandi, S., Yazdabadi, A., 2017. 使用循环卷积神经网络进行皮肤黑色素瘤分割，见：2017年IEEE第十四届国际生物医学成像研讨会（ISBI
    2017），IEEE，pp. 292–296。
- en: 'Azad et al. (2019) Azad, R., Asadi-Aghbolaghi, M., Fathy, M., Escalera, S.,
    2019. Bi-directional ConvLSTM U-Net with densley connected convolutions, in: Proceedings
    of the IEEE International Conference on Computer Vision Workshops, pp. 0–0.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azad 等人 (2019) Azad, R., Asadi-Aghbolaghi, M., Fathy, M., Escalera, S., 2019.
    具有密集连接卷积的双向 ConvLSTM U-Net，在：IEEE国际计算机视觉会议研讨会论文集，pp. 0–0.
- en: 'Azad et al. (2020) Azad, R., Asadi-Aghbolaghi, M., Fathy, M., Escalera, S.,
    2020. Attention deeplabv3+: Multi-level context attention mechanism for skin lesion
    segmentation, in: European Conference on Computer Vision Workshops, Springer.
    pp. 251–266.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Azad 等人 (2020) Azad, R., Asadi-Aghbolaghi, M., Fathy, M., Escalera, S., 2020.
    Attention deeplabv3+: 皮肤病变分割的多层级上下文注意力机制，在：欧洲计算机视觉会议研讨会，Springer，pp. 251–266。'
- en: 'Badshah and Ahmad (2022) Badshah, N., Ahmad, A., 2022. ResBCU-Net: Deep learning
    approach for segmentation of skin images. Biomedical Signal Processing and Control
    71, 103137.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Badshah 和 Ahmad (2022) Badshah, N., Ahmad, A., 2022. ResBCU-Net：皮肤图像分割的深度学习方法。生物医学信号处理与控制
    71, 103137。
- en: Bagheri et al. (2020) Bagheri, F., Tarokh, M.J., Ziaratban, M., 2020. Two-stage
    skin lesion segmentation from dermoscopic images by using deep neural networks.
    Jorjani Biomedicine Journal 8, 58–72.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagheri 等人 (2020) Bagheri, F., Tarokh, M.J., Ziaratban, M., 2020. 利用深度神经网络从皮肤镜图像中进行两阶段皮肤病变分割。Jorjani生物医学杂志
    8, 58–72。
- en: Bagheri et al. (2021a) Bagheri, F., Tarokh, M.J., Ziaratban, M., 2021a. Skin
    lesion segmentation based on mask rcnn, multi atrous full-cnn, and a geodesic
    method. International Journal of Imaging Systems and Technology .
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagheri 等人 (2021a) Bagheri, F., Tarokh, M.J., Ziaratban, M., 2021a. 基于 Mask
    RCNN、multi atrous full-cnn 和测地线方法的皮肤病变分割。国际成像系统与技术杂志。
- en: Bagheri et al. (2021b) Bagheri, F., Tarokh, M.J., Ziaratban, M., 2021b. Skin
    lesion segmentation from dermoscopic images by using mask r-cnn, retina-deeplab,
    and graph-based methods. Biomedical Signal Processing and Control 67, 102533.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagheri et al. (2021b) Bagheri, F., Tarokh, M.J., Ziaratban, M., 2021b. Skin
    lesion segmentation from dermoscopic images by using mask r-cnn, retina-deeplab,
    and graph-based methods. Biomedical Signal Processing and Control 67, 102533.
- en: 'Baghersalimi et al. (2019) Baghersalimi, S., Bozorgtabar, B., Schmid-Saugeon,
    P., Ekenel, H.K., Thiran, J.P., 2019. DermoNet: densely linked convolutional neural
    network for efficient skin lesion segmentation. EURASIP Journal on Image and Video
    Processing 2019, 71.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baghersalimi et al. (2019) Baghersalimi, S., Bozorgtabar, B., Schmid-Saugeon,
    P., Ekenel, H.K., Thiran, J.P., 2019. DermoNet: densely linked convolutional neural
    network for efficient skin lesion segmentation. EURASIP Journal on Image and Video
    Processing 2019, 71.'
- en: 'Baldi et al. (2000) Baldi, P., Brunak, S., Chauvin, Y., Andersen, C.A., Nielsen,
    H., 2000. Assessing the Accuracy of Prediction Algorithms for Classification:
    An Overview. Bioinformatics 16, 412–424.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baldi et al. (2000) Baldi, P., Brunak, S., Chauvin, Y., Andersen, C.A., Nielsen,
    H., 2000. Assessing the Accuracy of Prediction Algorithms for Classification:
    An Overview. Bioinformatics 16, 412–424.'
- en: 'Ballerini et al. (2013) Ballerini, L., Fisher, R.B., Aldridge, B., Rees, J.,
    2013. A color and texture based hierarchical k-nn approach to the classification
    of non-melanoma skin lesions, in: Celebi, M.E., Schaefer, G. (Eds.), Color Medical
    Image Analysis. Springer, pp. 63–86.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ballerini et al. (2013) Ballerini, L., Fisher, R.B., Aldridge, B., Rees, J.,
    2013. A color and texture based hierarchical k-nn approach to the classification
    of non-melanoma skin lesions, in: Celebi, M.E., Schaefer, G. (Eds.), Color Medical
    Image Analysis. Springer, pp. 63–86.'
- en: Barata et al. (2015a) Barata, C., Celebi, M.E., Marques, J.S., 2015a. Improving
    Dermoscopy Image Classification Using Color Constancy. IEEE Journal of Biomedical
    and Health Informatics 19, 1146–1152.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barata et al. (2015a) Barata, C., Celebi, M.E., Marques, J.S., 2015a. Improving
    Dermoscopy Image Classification Using Color Constancy. IEEE Journal of Biomedical
    and Health Informatics 19, 1146–1152.
- en: 'Barata et al. (2015b) Barata, C., Celebi, M.E., Marques, J.S., 2015b. Toward
    a Robust Analysis of Dermoscopy Images Acquired Under Different Conditions, in:
    Celebi, M.E., Mendonca, T., Marques, J.S. (Eds.), Dermoscopy Image Analysis. CRC
    Press, pp. 1–22.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Barata et al. (2015b) Barata, C., Celebi, M.E., Marques, J.S., 2015b. Toward
    a Robust Analysis of Dermoscopy Images Acquired Under Different Conditions, in:
    Celebi, M.E., Mendonca, T., Marques, J.S. (Eds.), Dermoscopy Image Analysis. CRC
    Press, pp. 1–22.'
- en: Barata et al. (2019) Barata, C., Celebi, M.E., Marques, J.S., 2019. A Survey
    of Feature Extraction in Dermoscopy Image Analysis of Skin Cancer. IEEE Journal
    of Biomedical and Health Informatics 23, 1096–1109.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barata et al. (2019) Barata, C., Celebi, M.E., Marques, J.S., 2019. A Survey
    of Feature Extraction in Dermoscopy Image Analysis of Skin Cancer. IEEE Journal
    of Biomedical and Health Informatics 23, 1096–1109.
- en: Barata et al. (2014) Barata, C., Ruela, M., Francisco, M., Mendonca, T., Marques,
    J.S., 2014. Two Systems for the Detection of Melanomas In Dermoscopy Images Using
    Texture and Color Features. IEEE Systems Journal 8, 965–979.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barata et al. (2014) Barata, C., Ruela, M., Francisco, M., Mendonca, T., Marques,
    J.S., 2014. Two Systems for the Detection of Melanomas In Dermoscopy Images Using
    Texture and Color Features. IEEE Systems Journal 8, 965–979.
- en: 'Basak et al. (2022) Basak, H., Kundu, R., Sarkar, R., 2022. MFSNet: A multi
    focus segmentation network for skin lesion segmentation. Pattern Recognition 128,
    108673.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Basak et al. (2022) Basak, H., Kundu, R., Sarkar, R., 2022. MFSNet: A multi
    focus segmentation network for skin lesion segmentation. Pattern Recognition 128,
    108673.'
- en: 'Baur et al. (2018) Baur, C., Albarqouni, S., Navab, N., 2018. Generating Highly
    Realistic Images of Skin Lesions with GANs, in: Proceedings of the Third ISIC
    Workshop on Skin Image Analysis, pp. 260–267.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baur et al. (2018) Baur, C., Albarqouni, S., Navab, N., 2018. Generating Highly
    Realistic Images of Skin Lesions with GANs, in: Proceedings of the Third ISIC
    Workshop on Skin Image Analysis, pp. 260–267.'
- en: 'Bearman et al. (2016) Bearman, A., Russakovsky, O., Ferrari, V., Fei-Fei, L.,
    2016. What’s the point: Semantic segmentation with point supervision, in: European
    Conference on Computer Vision, Springer. pp. 549–565.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bearman et al. (2016) Bearman, A., Russakovsky, O., Ferrari, V., Fei-Fei, L.,
    2016. What’s the point: Semantic segmentation with point supervision, in: European
    Conference on Computer Vision, Springer. pp. 549–565.'
- en: 'Bengio (2012) Bengio, Y., 2012. Practical Recommendations for Gradient-Based
    Training of Deep Architectures, in: Montavon, G., Orr, G., Muller, K.R. (Eds.),
    Neural networks: Tricks of the Trade. Second ed.. Springer, pp. 437–478.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bengio (2012) Bengio, Y., 2012. Practical Recommendations for Gradient-Based
    Training of Deep Architectures, in: Montavon, G., Orr, G., Muller, K.R. (Eds.),
    Neural networks: Tricks of the Trade. Second ed.. Springer, pp. 437–478.'
- en: 'Bengio et al. (2013) Bengio, Y., Courville, A., Vincent, P., 2013. Representation
    Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis
    and Machine Intelligence 35, 1798–1828.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bengio et al. (2013) Bengio, Y., Courville, A., Vincent, P., 2013. Representation
    Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis
    and Machine Intelligence 35, 1798–1828.'
- en: 'Bevan and Atapour-Abarghouei (2022) Bevan, P.J., Atapour-Abarghouei, A., 2022.
    Detecting melanoma fairly: Skin tone detection and debiasing for skin lesion classification.
    arXiv preprint arXiv:2202.02832 .'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bevan 和 Atapour-Abarghouei（2022）Bevan, P.J., Atapour-Abarghouei, A., 2022. 公平地检测黑色素瘤：皮肤色调检测和去偏见处理用于皮肤病变分类。arXiv
    预印本 arXiv:2202.02832。
- en: 'Bi et al. (2019a) Bi, L., Feng, D., Fulham, M., Kim, J., 2019a. Improving skin
    lesion segmentation via stacked adversarial learning, in: 2019 IEEE 16th International
    Symposium on Biomedical Imaging (ISBI 2019), IEEE. pp. 1100–1103.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi 等人（2019a）Bi, L., Feng, D., Fulham, M., Kim, J., 2019a. 通过堆叠对抗学习改进皮肤病变分割，发表于：2019
    IEEE 第16届生物医学成像国际研讨会（ISBI 2019），IEEE. 第1100–1103页。
- en: Bi et al. (2018) Bi, L., Feng, D., Kim, J., 2018. Improving automatic skin lesion
    segmentation using adversarial learning based data augmentation. arXiv preprint
    arXiv:1807.08392 .
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi 等人（2018）Bi, L., Feng, D., Kim, J., 2018. 使用对抗学习基础的数据增强来改进自动皮肤病变分割。arXiv 预印本
    arXiv:1807.08392。
- en: Bi et al. (2022) Bi, L., Fulham, M., Kim, J., 2022. Hyper-fusion network for
    semi-automatic segmentation of skin lesions. Medical Image Analysis 76, 102334.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi 等人（2022）Bi, L., Fulham, M., Kim, J., 2022. 用于半自动皮肤病变分割的超融合网络。医学图像分析 76, 102334。
- en: 'Bi et al. (2017a) Bi, L., Kim, J., Ahn, E., Feng, D., Fulham, M., 2017a. Semi-automatic
    skin lesion segmentation via fully convolutional networks, in: 2017 IEEE 14th
    International Symposium on Biomedical Imaging (ISBI 2017), IEEE. pp. 561–564.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi 等人（2017a）Bi, L., Kim, J., Ahn, E., Feng, D., Fulham, M., 2017a. 通过全卷积网络实现半自动皮肤病变分割，发表于：2017
    IEEE 第14届生物医学成像国际研讨会（ISBI 2017），IEEE. 第561–564页。
- en: Bi et al. (2019b) Bi, L., Kim, J., Ahn, E., Kumar, A., Feng, D., Fulham, M.,
    2019b. Step-wise integration of deep class-specific learning for dermoscopic image
    segmentation. Pattern recognition 85, 78–89.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi 等人（2019b）Bi, L., Kim, J., Ahn, E., Kumar, A., Feng, D., Fulham, M., 2019b.
    深度类别特定学习的逐步集成用于皮肤镜图像分割。模式识别 85, 78–89。
- en: Bi et al. (2017b) Bi, L., Kim, J., Ahn, E., Kumar, A., Fulham, M., Feng, D.,
    2017b. Dermoscopic image segmentation via multistage fully convolutional networks.
    IEEE Transactions on Biomedical Engineering 64, 2065–2074.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi 等人（2017b）Bi, L., Kim, J., Ahn, E., Kumar, A., Fulham, M., Feng, D., 2017b.
    通过多阶段全卷积网络进行皮肤镜图像分割。IEEE 生物医学工程汇刊 64, 2065–2074。
- en: Biancardi et al. (2010) Biancardi, A.M., Jirapatnakul, A.C., Reeves, A.P., 2010.
    A Comparison of Ground Truth Estimation Methods. International Journal of Computer
    Assisted Radiology and Surgery 5, 295–305.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biancardi 等人（2010）Biancardi, A.M., Jirapatnakul, A.C., Reeves, A.P., 2010. 基准真值估计方法的比较。计算机辅助放射学与外科国际杂志
    5, 295–305。
- en: Binder et al. (1995) Binder, M., Schwarz, M., Winkler, A., Steiner, A., Kaider,
    A., Wolff, K., Pehamberger, H., 1995. Epiluminescence microscopy. a useful tool
    for the diagnosis of pigmented skin lesions for formally trained dermatologists.
    Archives of Dermatology 131, 286–291.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Binder 等人（1995）Binder, M., Schwarz, M., Winkler, A., Steiner, A., Kaider, A.,
    Wolff, K., Pehamberger, H., 1995. 表面光学显微镜。对皮肤色素病变的诊断具有重要价值，适用于经过正规培训的皮肤科医生。皮肤科档案
    131, 286–291。
- en: Binney et al. (2021) Binney, N., Hyde, C., Bossuyt, P.M., 2021. On the origin
    of sensitivity and specificity. Annals of Internal Medicine 174, 401–407.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Binney 等人（2021）Binney, N., Hyde, C., Bossuyt, P.M., 2021. 敏感性和特异性的起源。内科学年鉴 174,
    401–407。
- en: Birkenfeld et al. (2020) Birkenfeld, J.S., Tucker-Schwartz, J.M., Soenksen,
    L.R., Avilés-Izquierdo, J.A., Marti-Fuster, B., 2020. Computer-aided classification
    of suspicious pigmented lesions using wide-field images. Computer methods and
    programs in biomedicine 195, 105631.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Birkenfeld 等人（2020）Birkenfeld, J.S., Tucker-Schwartz, J.M., Soenksen, L.R.,
    Avilés-Izquierdo, J.A., Marti-Fuster, B., 2020. 使用宽视场图像对可疑色素病变进行计算机辅助分类。生物医学计算方法与程序
    195, 105631。
- en: Bissoto et al. (2022) Bissoto, A., Barata, C., Valle, E., Avila, S., 2022. Artifact-based
    domain generalization of skin lesion models. arXiv preprint arXiv:2208.09756 .
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bissoto 等人（2022）Bissoto, A., Barata, C., Valle, E., Avila, S., 2022. 基于伪影的皮肤病变模型领域泛化。arXiv
    预印本 arXiv:2208.09756。
- en: 'Bissoto et al. (2019) Bissoto, A., Fornaciali, M., Valle, E., Avila, S., 2019.
    (de)constructing bias on skin lesion datasets, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 0–0.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bissoto 等人（2019）Bissoto, A., Fornaciali, M., Valle, E., Avila, S., 2019. 在皮肤病变数据集上（去）构建偏见，发表于：IEEE/CVF
    计算机视觉与模式识别会议（CVPR）研讨会，第0–0页。
- en: 'Bissoto et al. (2018) Bissoto, A., Perez, F., Valle, E., Avila, S., 2018. Skin
    lesion synthesis with generative adversarial networks, in: OR 2.0 context-aware
    operating theaters, computer assisted robotic endoscopy, clinical image-based
    procedures, and skin image analysis, pp. 294–302.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bissoto 等（2018）Bissoto, A., Perez, F., Valle, E., Avila, S., 2018. 使用生成对抗网络合成皮肤病变，发表于：OR
    2.0 语境感知手术室、计算机辅助机器人内窥镜、临床图像基础程序和皮肤图像分析，第 294–302 页。
- en: 'Bissoto et al. (2021) Bissoto, A., Valle, E., Avila, S., 2021. Gan-based data
    augmentation and anonymization for skin-lesion analysis: A critical review, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) Workshops, pp. 1847–1856.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bissoto 等（2021）Bissoto, A., Valle, E., Avila, S., 2021. 基于生成对抗网络的数据增强与匿名化用于皮肤病变分析：一项关键评审，发表于：IEEE/CVF
    计算机视觉与模式识别会议（CVPR）研讨会论文集，第 1847–1856 页。
- en: 'Bogo et al. (2015) Bogo, F., Peruch, F., Fortina, A.B., Peserico, E., 2015.
    Where’s the Lesion? Variability in Human and Automated Segmentation of Dermoscopy
    Images of Melanocytic Skin Lesions, in: Celebi, M.E., Mendonca, T., Marques, J.S.
    (Eds.), Dermoscopy Image Analysis. CRC Press, pp. 67–95.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bogo 等（2015）Bogo, F., Peruch, F., Fortina, A.B., Peserico, E., 2015. 皮损在哪里？人类与自动分割在黑素细胞皮肤病变的皮肤镜图像中的变异性，发表于：Celebi,
    M.E., Mendonca, T., Marques, J.S.（编），皮肤镜图像分析。CRC出版社，第 67–95 页。
- en: 'Bogo et al. (2014) Bogo, F., Romero, J., Peserico, E., Black, M.J., 2014. Automated
    detection of new or evolving melanocytic lesions using a 3D body model, in: International
    Conference on Medical Image Computing and Computer Assisted Intervention, pp.
    593–600.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bogo 等（2014）Bogo, F., Romero, J., Peserico, E., Black, M.J., 2014. 使用 3D 身体模型自动检测新的或变化的黑素细胞病变，发表于：医学图像计算与计算机辅助干预国际会议，第
    593–600 页。
- en: Boughorbel et al. (2017) Boughorbel, S., Jarray, F., El-Anbari, M., 2017. Optimal
    Classifier for Imbalanced Data Using Matthews Correlation Coefficient Metric.
    PLOS One 12, e0177678.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boughorbel 等（2017）Boughorbel, S., Jarray, F., El-Anbari, M., 2017. 使用 Matthews
    相关系数度量的优化分类器用于不平衡数据。PLOS One 12, e0177678。
- en: 'Bozorgtabar et al. (2017a) Bozorgtabar, B., Ge, Z., Chakravorty, R., Abedini,
    M., Demyanov, S., Garnavi, R., 2017a. Investigating deep side layers for skin
    lesion segmentation, in: 2017 IEEE 14th International Symposium on Biomedical
    Imaging (ISBI 2017), IEEE. pp. 256–260.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bozorgtabar 等（2017a）Bozorgtabar, B., Ge, Z., Chakravorty, R., Abedini, M., Demyanov,
    S., Garnavi, R., 2017a. 调查用于皮肤病变分割的深度侧层，发表于：2017 IEEE 第14届生物医学成像国际研讨会（ISBI 2017），IEEE。第
    256–260 页。
- en: Bozorgtabar et al. (2017b) Bozorgtabar, B., Sedai, S., Roy, P.K., Garnavi, R.,
    2017b. Skin lesion segmentation using deep convolution networks guided by local
    unsupervised learning. IBM Journal of Research and Development 61, 6–1.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bozorgtabar 等（2017b）Bozorgtabar, B., Sedai, S., Roy, P.K., Garnavi, R., 2017b.
    使用深度卷积网络进行皮肤病变分割，辅以局部无监督学习。IBM 研究与开发期刊 61, 6–1。
- en: 'Busin et al. (2008) Busin, L., Vandenbroucke, N., Macaire, L., 2008. Color
    Spaces and Image Segmentation, in: Hawkes, P.W. (Ed.), Advances in Imaging and
    Electron Physics. Academic Press. volume 151, pp. 65–168.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Busin 等（2008）Busin, L., Vandenbroucke, N., Macaire, L., 2008. 颜色空间与图像分割，发表于：Hawkes,
    P.W.（编），成像与电子物理学进展。学术出版社，第 151 卷，第 65–168 页。
- en: 'Buslaev et al. (2020) Buslaev, A., Iglovikov, V.I., Khvedchenya, E., Parinov,
    A., Druzhinin, M., Kalinin, A.A., 2020. Albumentations: Fast and Flexible Image
    Augmentations. Information 11, 125.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buslaev 等（2020）Buslaev, A., Iglovikov, V.I., Khvedchenya, E., Parinov, A., Druzhinin,
    M., Kalinin, A.A., 2020. Albumentations：快速且灵活的图像增强。信息 11, 125。
- en: 'Caffery et al. (2018) Caffery, L.J., Clunie, D., Curiel-Lewandrowski, C., Malvehy,
    J., Soyer, H.P., Halpern, A.C., 2018. Transforming Dermatologic Imaging for the
    Digital Era: Metadata and Standards. Journal of Digital Imaging 31, pages568–577.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caffery 等（2018）Caffery, L.J., Clunie, D., Curiel-Lewandrowski, C., Malvehy,
    J., Soyer, H.P., Halpern, A.C., 2018. 将皮肤病学成像转变为数字时代：元数据和标准。数字成像期刊 31, 第 568–577
    页。
- en: 'Canalini et al. (2019) Canalini, L., Pollastri, F., Bolelli, F., Cancilla,
    M., Allegretti, S., Grana, C., 2019. Skin lesion segmentation ensemble with diverse
    training strategies, in: International Conference on Computer Analysis of Images
    and Patterns, Springer. pp. 89–101.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Canalini 等（2019）Canalini, L., Pollastri, F., Bolelli, F., Cancilla, M., Allegretti,
    S., Grana, C., 2019. 具有多样化训练策略的皮肤病变分割集成，发表于：国际图像和模式分析会议，Springer。第 89–101 页。
- en: 'Cao et al. (2021) Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian,
    Q., Wang, M., 2021. Swin-Unet: Unet-like pure transformer for medical image segmentation.
    arXiv preprint arXiv:2105.05537 .'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等（2021）Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang,
    M., 2021. Swin-Unet：类似 Unet 的纯变换器用于医学图像分割。arXiv 预印本 arXiv:2105.05537。
- en: 'Cassidy et al. (2022) Cassidy, B., Kendrick, C., Brodzicki, A., Jaworek-Korjakowska,
    J., Yap, M.H., 2022. Analysis of the ISIC image datasets: Usage, benchmarks and
    recommendations. Medical Image Analysis 75, 102305.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cassidy等（2022）Cassidy, B., Kendrick, C., Brodzicki, A., Jaworek-Korjakowska,
    J., Yap, M.H., 2022. ISIC图像数据集的分析：使用情况、基准和建议。Medical Image Analysis 75, 102305。
- en: Celebi et al. (2007a) Celebi, M.E., Aslandogan, A., Stoecker, W.V., 2007a. Unsupervised
    Border Detection in Dermoscopy Images. Skin Research and Technology 13, 454–462.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celebi等（2007a）Celebi, M.E., Aslandogan, A., Stoecker, W.V., 2007a. 皮肤镜图像中的无监督边界检测。Skin
    Research and Technology 13, 454–462。
- en: 'Celebi et al. (2019) Celebi, M.E., Codella, N., Halpern, A., 2019. Dermoscopy
    Image Analysis: Overview and Future Directions. IEEE Journal of Biomedical and
    Health Informatics 23, 474–478.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celebi等（2019）Celebi, M.E., Codella, N., Halpern, A., 2019. 皮肤镜图像分析：概述与未来方向。IEEE
    Journal of Biomedical and Health Informatics 23, 474–478。
- en: Celebi et al. (2009a) Celebi, M.E., Iyatomi, H., Schaefer, G., Stoecker, W.V.,
    2009a. Approximate Lesion Localization in Dermoscopy Images. Skin Research and
    Technology 15, 314–322.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celebi等（2009a）Celebi, M.E., Iyatomi, H., Schaefer, G., Stoecker, W.V., 2009a.
    皮肤镜图像中病灶的近似定位。Skin Research and Technology 15, 314–322。
- en: Celebi et al. (2009b) Celebi, M.E., Iyatomi, H., Schaefer, G., Stoecker, W.V.,
    2009b. Lesion Border Detection in Dermoscopy Images. Computerized Medical Imaging
    and Graphics 33, 148–153.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celebi等（2009b）Celebi, M.E., Iyatomi, H., Schaefer, G., Stoecker, W.V., 2009b.
    皮肤镜图像中的病灶边界检测。Computerized Medical Imaging and Graphics 33, 148–153。
- en: Celebi et al. (2008) Celebi, M.E., Iyatomi, H., Stoecker, W.V., Moss, R.H.,
    Rabinovitz, H.S., Argenziano, G., Soyer, H.P., 2008. Automatic Detection of Blue-White
    Veil and Related Structures in Dermoscopy Images. Computerized Medical Imaging
    and Graphics 32, 670–677.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celebi等（2008）Celebi, M.E., Iyatomi, H., Stoecker, W.V., Moss, R.H., Rabinovitz,
    H.S., Argenziano, G., Soyer, H.P., 2008. 皮肤镜图像中蓝白面纱及相关结构的自动检测。Computerized Medical
    Imaging and Graphics 32, 670–677。
- en: Celebi et al. (2007b) Celebi, M.E., Kingravi, H., Uddin, B., Iyatomi, H., Aslandogan,
    A., Stoecker, W.V., Moss, R.H., 2007b. A Methodological Approach to the Classification
    of Dermoscopy Images. Computerized Medical Imaging and Graphics 31, 362–373.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celebi等（2007b）Celebi, M.E., Kingravi, H., Uddin, B., Iyatomi, H., Aslandogan,
    A., Stoecker, W.V., Moss, R.H., 2007b. 皮肤镜图像分类的方法论方法。Computerized Medical Imaging
    and Graphics 31, 362–373。
- en: Celebi et al. (2015a) Celebi, M.E., Mendonca, T., Marques, J.S. (Eds.), 2015a.
    Dermoscopy Image Analysis. CRC Press.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celebi等（2015a）Celebi, M.E., Mendonca, T., Marques, J.S.（编辑），2015a. 皮肤镜图像分析。CRC
    Press。
- en: Celebi et al. (2009c) Celebi, M.E., Schaefer, G., Iyatomi, H., Stoecker, W.V.,
    Malters, J.M., Grichnik, J.M., 2009c. An Improved Objective Evaluation Measure
    for Border Detection in Dermoscopy Images. Skin Research and Technology 15, 444–450.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celebi等（2009c）Celebi, M.E., Schaefer, G., Iyatomi, H., Stoecker, W.V., Malters,
    J.M., Grichnik, J.M., 2009c. 皮肤镜图像中边界检测的改进客观评价指标。Skin Research and Technology
    15, 444–450。
- en: Celebi et al. (2013) Celebi, M.E., Wen, Q., Hwang, S., Iyatomi, H., Schaefer,
    G., 2013. Lesion Border Detection in Dermoscopy Images Using Ensembles of Thresholding
    Methods. Skin Research and Technology 19, e252–e258.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celebi等（2013）Celebi, M.E., Wen, Q., Hwang, S., Iyatomi, H., Schaefer, G., 2013.
    使用阈值方法集成的皮肤镜图像病灶边界检测。Skin Research and Technology 19, e252–e258。
- en: 'Celebi et al. (2015b) Celebi, M.E., Wen, Q., Iyatomi, H., Shimizu, K., Zhou,
    H., Schaefer, G., 2015b. A State-of-the-Art Survey on Lesion Border Detection
    in Dermoscopy Images, in: Celebi, M.E., Mendonca, T., Marques, J.S. (Eds.), Dermoscopy
    Image Analysis. CRC Press, pp. 97–129.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celebi等（2015b）Celebi, M.E., Wen, Q., Iyatomi, H., Shimizu, K., Zhou, H., Schaefer,
    G., 2015b. 皮肤镜图像病灶边界检测的最新调查，见：Celebi, M.E., Mendonca, T., Marques, J.S.（编辑），皮肤镜图像分析。CRC
    Press，第97–129页。
- en: Chabrier et al. (2006) Chabrier, S., Emile, B., Rosenberger, C., Laurent, H.,
    2006. Unsupervised Performance Evaluation of Image Segmentation. EURASIP Journal
    on Advances in Signal Processing 2006, 1–12.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chabrier等（2006）Chabrier, S., Emile, B., Rosenberger, C., Laurent, H., 2006.
    图像分割的无监督性能评估。EURASIP Journal on Advances in Signal Processing 2006, 1–12。
- en: Chalana and Kim (1997) Chalana, V., Kim, Y., 1997. A Methodology for Evaluation
    of Boundary Detection Algorithms on Medical Images. IEEE Transactions on Medical
    Imaging 16, 642–652.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chalana和Kim（1997）Chalana, V., Kim, Y., 1997. 医学图像边界检测算法评估方法。IEEE Transactions
    on Medical Imaging 16, 642–652。
- en: 'Chen et al. (2021) Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y.,
    Lu, L., Yuille, A.L., Zhou, Y., 2021. TransUNet: Transformers make strong encoders
    for medical image segmentation. arXiv preprint arXiv:2102.04306 .'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2021）Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille,
    A.L., Zhou, Y., 2021. TransUNet：变换器在医学图像分割中的强大编码能力。arXiv预印本 arXiv:2102.04306。
- en: 'Chen et al. (2017a) Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille,
    A.L., 2017a. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,
    Atrous Convolution, and Fully Connected CRFs. IEEE Transactions on Pattern Analysis
    and Machine Intelligence 40, 834–848.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2017a）Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.,
    2017a. DeepLab：利用深度卷积网络、空洞卷积和全连接条件随机场进行语义图像分割。IEEE模式分析与机器智能汇刊 40，834–848。
- en: Chen et al. (2017b) Chen, L.C., Papandreou, G., Schroff, F., Adam, H., 2017b.
    Rethinking atrous convolution for semantic image segmentation. arXiv preprint
    arXiv:1706.05587 .
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2017b）Chen, L.C., Papandreou, G., Schroff, F., Adam, H., 2017b. 重新思考空洞卷积在语义图像分割中的应用。arXiv预印本
    arXiv:1706.05587。
- en: 'Chen et al. (2018a) Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam,
    H., 2018a. Encoder-decoder with atrous separable convolution for semantic image
    segmentation, in: Proceedings of the European conference on computer vision (ECCV),
    pp. 801–818.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2018a）Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., 2018a.
    使用空洞可分离卷积的编码器-解码器进行语义图像分割，见于：欧洲计算机视觉会议（ECCV）论文集，页码801–818。
- en: Chen et al. (2022) Chen, P., Huang, S., Yue, Q., 2022. Skin lesion segmentation
    using recurrent attentional convolutional networks. IEEE Access 10, 94007–94018.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2022）Chen, P., Huang, S., Yue, Q., 2022. 使用递归注意力卷积网络进行皮肤病变分割。IEEE Access
    10，94007–94018。
- en: 'Chen et al. (2018b) Chen, S., Wang, Z., Shi, J., Liu, B., Yu, N., 2018b. A
    multi-task framework with feature passing module for skin lesion classification
    and segmentation, in: 2018 IEEE 15th international symposium on biomedical imaging
    (ISBI 2018), IEEE. pp. 1126–1129.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2018b）Chen, S., Wang, Z., Shi, J., Liu, B., Yu, N., 2018b. 具有特征传递模块的多任务框架用于皮肤病变分类和分割，见于：2018
    IEEE第15届国际生物医学成像研讨会（ISBI 2018），IEEE。页码1126–1129。
- en: Chen and Parent (1989) Chen, S.E., Parent, R.E., 1989. Shape Averaging and its
    Applications to Industrial Design. IEEE Computer Graphics and Applications 9,
    47–54.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen和Parent（1989）Chen, S.E., Parent, R.E., 1989. 形状平均及其在工业设计中的应用。IEEE计算机图形与应用
    9，47–54。
- en: Chicco and Jurman (2020) Chicco, D., Jurman, G., 2020. The Advantages of the
    Matthews Correlation Coefficient (MCC) over F1 Score and Accuracy in Binary Classification
    Evaluation. BMC Genomics 21.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chicco和Jurman（2020）Chicco, D., Jurman, G., 2020. Matthews相关系数（MCC）在二分类评价中相较于F1分数和准确度的优势。BMC基因组学
    21。
- en: 'Chollet (2017) Chollet, F., 2017. Xception: Deep learning with depthwise separable
    convolutions, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 1251–1258.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet（2017）Chollet, F., 2017. Xception：使用深度可分离卷积的深度学习，见于：IEEE计算机视觉与模式识别会议论文集，页码1251–1258。
- en: 'Chou and Fasman (1978) Chou, P.Y., Fasman, G.D., 1978. Prediction of the Secondary
    Structure of Proteins from Their Amino Acid Sequence, in: Meister, A. (Ed.), Advances
    in Enzymology and Related Areas of Molecular Biology. John Wiley & Sons. volume 47,
    pp. 45–148.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chou和Fasman（1978）Chou, P.Y., Fasman, G.D., 1978. 从氨基酸序列预测蛋白质的二级结构，见于：Meister,
    A.（编），酶学及相关分子生物学领域的进展。John Wiley & Sons，第47卷，页码45–148。
- en: 'Codella et al. (2015) Codella, N., Cai, J., Abedini, M., Garnavi, R., Halpern,
    A., Smith, J.R., 2015. Deep Learning, Sparse Coding, and SVM for Melanoma Recognition
    in Dermoscopy Images, in: Proceedings of the International Workshop on Machine
    Learning in Medical Imaging, pp. 118–126.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codella等（2015）Codella, N., Cai, J., Abedini, M., Garnavi, R., Halpern, A., Smith,
    J.R., 2015. 用于黑色素瘤识别的深度学习、稀疏编码和SVM，在：国际医学成像机器学习研讨会论文集中，页码118–126。
- en: 'Codella et al. (2019) Codella, N., Rotemberg, V., Tschandl, P., Celebi, M.E.,
    Dusza, S., Gutman, D., Helba, B., Kalloo, A., Liopyris, K., Marchetti, M., Kittler,
    H., Halpern, A., 2019. Skin Lesion Analysis Toward Melanoma Detection 2018: A
    Challenge Hosted by the International Skin Imaging Collaboration (ISIC). [https://arxiv.org/abs/1902.03368](https://arxiv.org/abs/1902.03368).'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codella等（2019）Codella, N., Rotemberg, V., Tschandl, P., Celebi, M.E., Dusza,
    S., Gutman, D., Helba, B., Kalloo, A., Liopyris, K., Marchetti, M., Kittler, H.,
    Halpern, A., 2019. 皮肤病变分析以期检测黑色素瘤2018：国际皮肤成像合作组织（ISIC）主办的挑战。[https://arxiv.org/abs/1902.03368](https://arxiv.org/abs/1902.03368)。
- en: Codella et al. (2017) Codella, N.C., Nguyen, Q.B., Pankanti, S., Gutman, D.A.,
    Helba, B., Halpern, A.C., Smith, J.R., 2017. Deep Learning Ensembles for Melanoma
    Recognition in Dermoscopy Images. IBM Journal of Research and Development 61,
    5:1–5:15.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codella等（2017）Codella, N.C., Nguyen, Q.B., Pankanti, S., Gutman, D.A., Helba,
    B., Halpern, A.C., Smith, J.R., 2017. 用于黑色素瘤识别的深度学习集成方法在皮肤镜图像中的应用。IBM研究与发展汇刊 61，5:1–5:15。
- en: 'Codella et al. (2018) Codella, N.C.F., Gutman, D., Celebi, M.E., Helba, B.,
    Marchetti, M.A., Dusza, S.W., Kalloo, A., Liopyris, K., Mishra, N., Kittler, H.,
    Halpern, A., 2018. Skin Lesion Analysis Toward Melanoma Detection: A Challenge
    at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the
    International Skin Imaging Collaboration (ISIC), in: Proceedings of the 2018 IEEE
    International Symposium on Biomedical Imaging (ISBI 2018), pp. 168–172.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codella 等（2018）Codella, N.C.F., Gutman, D., Celebi, M.E., Helba, B., Marchetti,
    M.A., Dusza, S.W., Kalloo, A., Liopyris, K., Mishra, N., Kittler, H., Halpern,
    A., 2018. 皮肤病变分析与黑色素瘤检测：2017年国际生物医学影像研讨会（ISBI）挑战，由国际皮肤影像协作组织（ISIC）主办，载于：2018年IEEE国际生物医学影像研讨会论文集（ISBI
    2018），第168–172页。
- en: Cohen (1960) Cohen, J., 1960. A Coefficient of Agreement for Nominal Scales.
    Educational and Psychological Measurement 20, 37–46.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen（1960）Cohen, J., 1960. 名义尺度的一致性系数。教育与心理测量，第20卷，第37–46页。
- en: Colliot et al. (2022) Colliot, O., Thibeau-Sutre, E., Burgos, N., 2022. Reproducibility
    in machine learning for medical imaging. arXiv preprint arXiv:2209.05097 .
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colliot 等（2022）Colliot, O., Thibeau-Sutre, E., Burgos, N., 2022. 医学影像中的机器学习重现性。arXiv
    预印本 arXiv:2209.05097。
- en: 'Combalia et al. (2019) Combalia, M., Codella, N.C., Rotemberg, V., Helba, B.,
    Vilaplana, V., Reiter, O., Carrera, C., Barreiro, A., Halpern, A.C., Puig, S.,
    Malvehy, J., 2019. BCN20000: Dermoscopic lesions in the wild. arXiv preprint arXiv:1908.02288
    .'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Combalia 等（2019）Combalia, M., Codella, N.C., Rotemberg, V., Helba, B., Vilaplana,
    V., Reiter, O., Carrera, C., Barreiro, A., Halpern, A.C., Puig, S., Malvehy, J.,
    2019. BCN20000: 现实中的皮肤镜病变。arXiv 预印本 arXiv:1908.02288。'
- en: Cordonnier et al. (2019) Cordonnier, J.B., Loukas, A., Jaggi, M., 2019. On the
    relationship between self-attention and convolutional layers. arXiv preprint arXiv:1911.03584
    .
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cordonnier 等（2019）Cordonnier, J.B., Loukas, A., Jaggi, M., 2019. 自注意力与卷积层之间的关系。arXiv
    预印本 arXiv:1911.03584。
- en: 'Creswell et al. (2018) Creswell, A., White, T., Dumoulin, V., Arulkumaran,
    K., Sengupta, B., Bharath, A.A., 2018. Generative Adversarial Networks: An Overview.
    IEEE Signal Processing Magazine 35, 53–65.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Creswell 等（2018）Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., Sengupta,
    B., Bharath, A.A., 2018. 生成对抗网络：概述。IEEE信号处理杂志，第35卷，第53–65页。
- en: Crum et al. (2006) Crum, W.R., Camara, O., Hill, D.L., 2006. Generalized Overlap
    Measures for Evaluation and Validation in Medical Image Analysis. IEEE Transactions
    on Medical Imaging 25, 1451–1461.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Crum 等（2006）Crum, W.R., Camara, O., Hill, D.L., 2006. 医学图像分析中的广义重叠度量用于评估与验证。IEEE医学成像学报，第25卷，第1451–1461页。
- en: 'Cui et al. (2019) Cui, Z., Wu, L., Wang, R., Zheng, W.S., 2019. Ensemble transductive
    learning for skin lesion segmentation, in: Chinese Conference on Pattern Recognition
    and Computer Vision (PRCV), Springer. pp. 572–581.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等（2019）Cui, Z., Wu, L., Wang, R., Zheng, W.S., 2019. 皮肤病变分割的集成转导学习，载于：模式识别与计算机视觉中国会议（PRCV），Springer出版社，第572–581页。
- en: 'Curiel-Lewandrowski et al. (2019) Curiel-Lewandrowski, C., Novoa, R.A., Berry,
    E., Celebi, M.E., Codella, N., Giuste, F., Gutman, D., Halpern, A., Leachman,
    S., Liu, Y., Liu, Y., Reiter, O., Tschandl, P., 2019. Artificial Intelligence
    Approach in Melanoma, in: Fisher, D.E., Bastian, B.C. (Eds.), Melanoma. Spriner,
    pp. 599–628.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Curiel-Lewandrowski 等（2019）Curiel-Lewandrowski, C., Novoa, R.A., Berry, E.,
    Celebi, M.E., Codella, N., Giuste, F., Gutman, D., Halpern, A., Leachman, S.,
    Liu, Y., Liu, Y., Reiter, O., Tschandl, P., 2019. 在黑色素瘤中的人工智能方法，载于：Fisher, D.E.,
    Bastian, B.C.（编辑），《黑色素瘤》。Springer出版社，第599–628页。
- en: 'Dai et al. (2022) Dai, D., Dong, C., Xu, S., Yan, Q., Li, Z., Zhang, C., Luo,
    N., 2022. Ms red: A novel multi-scale residual encoding and decoding network for
    skin lesion segmentation. Medical Image Analysis 75, 102293.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等（2022）Dai, D., Dong, C., Xu, S., Yan, Q., Li, Z., Zhang, C., Luo, N.,
    2022. Ms red: 一种新颖的多尺度残差编码解码网络用于皮肤病变分割。医学影像分析，第75卷，102293。'
- en: 'Dai et al. (2015) Dai, J., He, K., Sun, J., 2015. BoxSup: Exploiting Bounding
    Boxes to Supervise Convolutional Networks for Semantic Segmentation, in: Proceedings
    of the IEEE International Conference on Computer Vision, pp. 1635–1643.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等（2015）Dai, J., He, K., Sun, J., 2015. BoxSup: 利用边界框监督卷积网络进行语义分割，载于：IEEE国际计算机视觉会议论文集，第1635–1643页。'
- en: 'Daneshjou et al. (2022) Daneshjou, R., Barata, C., Betz-Stablein, B., Celebi,
    M.E., Codella, N., Combalia, M., Guitera, P., Gutman, D., Halpern, A., Helba,
    B., Kittler, H., Kose, K., Liopyris, K., Malvehy, J., Seog, H.S., Soyer, H.P.,
    Tkaczyk, E.R., Tschandl, P., Rotemberg, V., 2022. Evaluation of Image-Based AI
    Artificial Intelligence Reports in Dermatology: CLEAR Derm Consensus Guidelines
    from the International Skin Imaging Collaboration Artificial Intelligence Working
    Group. JAMA Dermatology 158, 90–96.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daneshjou 等 (2022) Daneshjou, R., Barata, C., Betz-Stablein, B., Celebi, M.E.,
    Codella, N., Combalia, M., Guitera, P., Gutman, D., Halpern, A., Helba, B., Kittler,
    H., Kose, K., Liopyris, K., Malvehy, J., Seog, H.S., Soyer, H.P., Tkaczyk, E.R.,
    Tschandl, P., Rotemberg, V., 2022. 皮肤科基于图像的人工智能报告评估：国际皮肤成像合作组织人工智能工作组的CLEAR Derm
    共识指南。JAMA 皮肤科 158, 90–96。
- en: 'Daneshjou et al. (2021a) Daneshjou, R., Smith, M.P., Sun, M.D., Rotemberg,
    V., Zou, J., 2021a. Lack of transparency and potential bias in artificial intelligence
    data sets and algorithms: A scoping review. JAMA Dermatology 157, 1362–1369.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daneshjou 等 (2021a) Daneshjou, R., Smith, M.P., Sun, M.D., Rotemberg, V., Zou,
    J., 2021a. 人工智能数据集和算法中的透明度缺乏和潜在偏见：范围评估。JAMA 皮肤科 157, 1362–1369。
- en: 'Daneshjou et al. (2021b) Daneshjou, R., Vodrahalli, K., Liang, W., Novoa, R.A.,
    Jenkins, M., Rotemberg, V., Ko, J., Swetter, S.M., Bailey, E.E., Gevaert, O.,
    Mukherjee, P., Phung, M., Yekrang, K., Fong, B., Sahasrabudhe, R., Zou, J., Chiou,
    A., 2021b. Disparities in dermatology AI: Assessments using diverse clinical images.
    arXiv preprint arXiv:2111.08006 .'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daneshjou 等 (2021b) Daneshjou, R., Vodrahalli, K., Liang, W., Novoa, R.A., Jenkins,
    M., Rotemberg, V., Ko, J., Swetter, S.M., Bailey, E.E., Gevaert, O., Mukherjee,
    P., Phung, M., Yekrang, K., Fong, B., Sahasrabudhe, R., Zou, J., Chiou, A., 2021b.
    皮肤科人工智能中的差异：使用多样临床图像的评估。arXiv 预印本 arXiv:2111.08006。
- en: 'De Angelo et al. (2019) De Angelo, G.G., Pacheco, A.G., Krohling, R.A., 2019.
    Skin lesion segmentation using deep learning for images acquired from smartphones,
    in: 2019 International Joint Conference on Neural Networks (IJCNN), IEEE. pp.
    1–8.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Angelo 等 (2019) De Angelo, G.G., Pacheco, A.G., Krohling, R.A., 2019. 使用深度学习进行智能手机拍摄的图像皮肤病变分割，见：2019
    国际联合神经网络会议 (IJCNN)，IEEE. 页码 1–8。
- en: 'Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei,
    L., 2009. ImageNet: A large-scale hierarchical image database, in: 2009 IEEE Conference
    on Computer Vision and Pattern Recognition, IEEE. pp. 248–255.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2009) Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.,
    2009. ImageNet：一个大规模分层图像数据库，见：2009 IEEE 计算机视觉与模式识别会议，IEEE. 页码 248–255。
- en: 'Deng et al. (2017) Deng, Z., Fan, H., Xie, F., Cui, Y., Liu, J., 2017. Segmentation
    of dermoscopy images based on fully convolutional neural network, in: 2017 IEEE
    International Conference on Image Processing (ICIP), IEEE. pp. 1732–1736.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2017) Deng, Z., Fan, H., Xie, F., Cui, Y., Liu, J., 2017. 基于完全卷积神经网络的皮肤镜图像分割，见：2017
    IEEE 国际图像处理会议 (ICIP)，IEEE. 页码 1732–1736。
- en: 'Deng et al. (2020) Deng, Z., Xin, Y., Qiu, X., Chen, Y., 2020. Weakly and semi-supervised
    deep level set network for automated skin lesion segmentation, in: Innovation
    in Medicine and Healthcare. Springer, pp. 145–155.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2020) Deng, Z., Xin, Y., Qiu, X., Chen, Y., 2020. 弱监督和半监督深度水平集网络用于自动化皮肤病变分割，见：医学与健康创新。Springer,
    页码 145–155。
- en: 'Denton et al. (2015) Denton, E., Chintala, S., Szlam, A., Fergus, R., 2015.
    Deep generative image models using a laplacian pyramid of adversarial networks,
    in: Proceedings of the 28th International Conference on Neural Information Processing
    Systems-Volume 1, pp. 1486–1494.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denton 等 (2015) Denton, E., Chintala, S., Szlam, A., Fergus, R., 2015. 使用对抗网络的拉普拉斯金字塔进行深度生成图像模型，见：第28届国际神经信息处理系统会议-卷1，页码
    1486–1494。
- en: 'Depeweg et al. (2018) Depeweg, S., Hernandez-Lobato, J.M., Doshi-Velez, F.,
    Udluft, S., 2018. Decomposition of uncertainty in bayesian deep learning for efficient
    and risk-sensitive learning, in: International Conference on Machine Learning,
    PMLR. pp. 1184–1193.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Depeweg 等 (2018) Depeweg, S., Hernandez-Lobato, J.M., Doshi-Velez, F., Udluft,
    S., 2018. 在贝叶斯深度学习中的不确定性分解，用于高效且风险敏感的学习，见：国际机器学习会议，PMLR. 页码 1184–1193。
- en: Der Kiureghian and Ditlevsen (2009) Der Kiureghian, A., Ditlevsen, O., 2009.
    Aleatory or epistemic? does it matter? Structural safety 31, 105–112.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Der Kiureghian 和 Ditlevsen (2009) Der Kiureghian, A., Ditlevsen, O., 2009. 随机还是认知？这重要吗？结构安全
    31, 105–112。
- en: DermIS (2012) DermIS, 2012. Dermatology Information System. [https://www.dermis.net/](https://www.dermis.net/).
    [Online. Accessed January 26, 2022].
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DermIS (2012) DermIS, 2012. 皮肤病信息系统。 [https://www.dermis.net/](https://www.dermis.net/).
    [在线。访问于2022年1月26日]。
- en: 'DermQuest (2012) DermQuest, 2012. Dermquest. [http://www.dermquest.com](http://www.dermquest.com).
    Cited: 2020-04-28.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DermQuest (2012) DermQuest, 2012. Dermquest。 [http://www.dermquest.com](http://www.dermquest.com).
    引用：2020-04-28。
- en: DeVries and Taylor (2018) DeVries, T., Taylor, G.W., 2018. Leveraging uncertainty
    estimates for predicting segmentation quality. arXiv preprint arXiv:1807.00502
    .
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeVries和Taylor（2018）DeVries, T., Taylor, G.W., 2018. 利用不确定性估计预测分割质量。arXiv预印本arXiv:1807.00502。
- en: 'Dhawan et al. (1984) Dhawan, A.P., Gordon, R., , Rangayyan, R.M., 1984. Nevoscopy:
    Three-dimensional computed tomography of nevi and melanomas in situ by transillumination.
    IEEE Transactions on Medical Imaging 3, 54–61.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhawan等（1984）Dhawan, A.P., Gordon, R., Rangayyan, R.M., 1984. 皮肤镜检查：通过透光成像对痣和原位黑色素瘤进行三维计算机断层扫描。《IEEE医学成像学报》3,
    54–61。
- en: Dice (1945) Dice, L.R., 1945. Measures of the Amount of Ecologic Association
    Between Species. Ecology 26, 297–302.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dice（1945）Dice, L.R., 1945. 物种间生态关联量度。 《生态学》26, 297–302。
- en: Ding et al. (2021) Ding, S., Zheng, J., Liu, Z., Zheng, Y., Chen, Y., Xu, X.,
    Lu, J., Xie, J., 2021. High-Resolution Dermoscopy Image Synthesis with Conditional
    Generative Adversarial Networks. Biomedical Signal Processing and Control 64,
    102224.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding等（2021）Ding, S., Zheng, J., Liu, Z., Zheng, Y., Chen, Y., Xu, X., Lu, J.,
    Xie, J., 2021. 基于条件生成对抗网络的高分辨率皮肤镜图像合成。《生物医学信号处理与控制》64, 102224。
- en: 'Dodge and Karam (2016) Dodge, S., Karam, L., 2016. Understanding How Image
    Quality Affects Deep Neural Networks, in: Proceedings of the 2016 International
    Conference on Quality of Multimedia Experience, pp. 1–6.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dodge和Karam（2016）Dodge, S., Karam, L., 2016. 理解图像质量如何影响深度神经网络，载于：2016年国际多媒体体验质量会议论文集，第1–6页。
- en: 'Dong et al. (2022) Dong, Y., Wang, L., Li, Y., 2022. TC-Net: Dual coding network
    of Transformer and CNN for skin lesion segmentation. Plos one 17, e0277578.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong等（2022）Dong, Y., Wang, L., Li, Y., 2022. TC-Net：用于皮肤病变分割的变换器和卷积神经网络双编码网络。《PLOS
    ONE》17, e0277578。
- en: 'Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al., 2020. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929 .'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy等（2020）Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D.,
    Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
    等, 2020. 一张图像胜过16x16个词：用于大规模图像识别的变换器。arXiv预印本arXiv:2010.11929。
- en: 'Du et al. (2023) Du, S., Hers, B., Bayasi, N., Hamarneh, G., Garbi, R., 2023.
    FairDisCo: Fairer AI in dermatology via disentanglement contrastive learning,
    in: Computer Vision–ECCV 2022 Workshops: Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part IV, Springer. pp. 185–202.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du等（2023）Du, S., Hers, B., Bayasi, N., Hamarneh, G., Garbi, R., 2023. FairDisCo：通过解缠结对比学习实现皮肤病学中的公平AI，载于：计算机视觉–ECCV
    2022研讨会：以色列特拉维夫，2022年10月23–27日，论文集，第四部分，Springer，第185–202页。
- en: Ebenezer and Rajapakse (2018) Ebenezer, J.P., Rajapakse, J.C., 2018. Automatic
    segmentation of skin lesions using deep learning. arXiv preprint arXiv:1807.04893
    .
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ebenezer和Rajapakse（2018）Ebenezer, J.P., Rajapakse, J.C., 2018. 使用深度学习进行皮肤病变的自动分割。arXiv预印本arXiv:1807.04893。
- en: 'El Jurdi et al. (2021) El Jurdi, R., Petitjean, C., Honeine, P., Cheplygina,
    V., Abdallah, F., 2021. High-level prior-based loss functions for medical image
    segmentation: A survey. Computer Vision and Image Understanding 210, 103248.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: El Jurdi等（2021）El Jurdi, R., Petitjean, C., Honeine, P., Cheplygina, V., Abdallah,
    F., 2021. 基于高级先验的医学图像分割损失函数：综述。《计算机视觉与图像理解》210, 103248。
- en: 'Elsken et al. (2019) Elsken, T., Metzen, J.H., Hutter, F., 2019. Neural Architecture
    Search: A Survey. Journal of Machine Learning Research 20, 1–21.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsken等（2019）Elsken, T., Metzen, J.H., Hutter, F., 2019. 神经架构搜索：综述。《机器学习研究杂志》20,
    1–21。
- en: 'En and Guo (2022) En, Q., Guo, Y., 2022. Annotation by clicks: A point-supervised
    contrastive variance method for medical semantic segmentation. arXiv preprint
    arXiv:2212.08774 .'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: En和Guo（2022）En, Q., Guo, Y., 2022. 点击标注：一种用于医学语义分割的点监督对比方差方法。arXiv预印本arXiv:2212.08774。
- en: 'Engasser and Warshaw (2010) Engasser, H.C., Warshaw, E.M., 2010. Dermatoscopy
    use by us dermatologists: a cross-sectional survey. Journal of the American Academy
    of Dermatology 63, 412–419.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engasser和Warshaw（2010）Engasser, H.C., Warshaw, E.M., 2010. 美国皮肤科医生使用皮肤镜的情况：一项横断面调查。《美国皮肤病学会杂志》63,
    412–419。
- en: Erkol et al. (2005) Erkol, B., Moss, R.H., Stanley, R.J., Stoecker, W.V., Hvatum,
    E., 2005. Automatic Lesion Boundary Detection in Dermoscopy Images Using Gradient
    Vector Flow Snakes. Skin Research and Technology 11, 17–26.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erkol等（2005）Erkol, B., Moss, R.H., Stanley, R.J., Stoecker, W.V., Hvatum, E.,
    2005. 使用梯度向量流蛇自动检测皮肤镜图像中的病变边界。《皮肤研究与技术》11, 17–26。
- en: 'Ferreira et al. (2012) Ferreira, P.M., Mendonca, T., Rozeira, J., Rocha, P.,
    2012. An Annotation Tool for Dermoscopic Image Segmentation, in: Proceedings of
    the 1st International Workshop on Visual Interfaces for Ground Truth Collection
    in Computer Vision Applications, pp. 1–6.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira 等 (2012) Ferreira, P.M., Mendonca, T., Rozeira, J., Rocha, P., 2012.
    一种用于皮肤镜图像分割的注释工具，见：第 1 届国际计算机视觉应用真实数据收集视觉接口研讨会论文集，第 1–6 页。
- en: 'Foncubierta-Rodriguez and Muller (2012) Foncubierta-Rodriguez, A., Muller,
    H., 2012. Ground Truth Generation in Medical Imaging: A Crowdsourcing-Based Iterative
    Approach, in: Proceedings of the ACM Multimedia 2012 Workshop on Crowdsourcing
    for Multimedia, pp. 9–14.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foncubierta-Rodriguez 和 Muller (2012) Foncubierta-Rodriguez, A., Muller, H.,
    2012. 医学成像中的真实数据生成：基于众包的迭代方法，见：ACM 多媒体 2012 会议众包多媒体研讨会论文集，第 9–14 页。
- en: Fortina et al. (2012) Fortina, A.B., Peserico, E., Silletti, A., Zattra, E.,
    2012. Where’s the Naevus? Inter-Operator Variability in the Localization of Melanocytic
    Lesion Border. Skin Research and Technology 18, 311–315.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fortina 等 (2012) Fortina, A.B., Peserico, E., Silletti, A., Zattra, E., 2012.
    哪儿是痣？黑色素瘤病变边界定位的操作员间变异性。皮肤研究与技术 18, 311–315。
- en: 'Friedman et al. (1985) Friedman, R.J., Rigel, D.S., Kopf, A.W., 1985. Early
    detection of malignant melanoma: The role of physician examination and self-examination
    of the skin. CA: A Cancer Journal for Clinicians 35, 130–151.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Friedman 等 (1985) Friedman, R.J., Rigel, D.S., Kopf, A.W., 1985. 恶性黑色素瘤的早期检测：医生检查和自我检查的作用。CA：临床医生癌症杂志
    35, 130–151。
- en: 'Fu et al. (2019) Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu,
    H., 2019. Dual attention network for scene segmentation, in: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 3146–3154.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2019) Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu, H., 2019.
    用于场景分割的双重注意力网络，见：IEEE 计算机视觉与模式识别会议论文集，第 3146–3154 页。
- en: Gachon et al. (2005) Gachon, J., Beaulieu, P., Sei, J.F., Gouvernet, J., Claudel,
    J.P., Lemaitre, M., Richard, M.A., Grob, J.J., 2005. First prospective study of
    the recognition process of melanoma in dermatological practice. Archives of dermatology
    141, 434–438.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gachon 等 (2005) Gachon, J., Beaulieu, P., Sei, J.F., Gouvernet, J., Claudel,
    J.P., Lemaitre, M., Richard, M.A., Grob, J.J., 2005. 皮肤科实践中黑色素瘤识别过程的首次前瞻性研究。皮肤科档案
    141, 434–438。
- en: Gal (2016) Gal, Y., 2016. Uncertainty in deep learning. Ph.D. thesis. Department
    of Engineering, University of Cambridge. [https://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf](https://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf).
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal (2016) Gal, Y., 2016. 深度学习中的不确定性。博士论文。剑桥大学工程系。[https://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf](https://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf)。
- en: Garnavi and Aldeen (2011) Garnavi, R., Aldeen, M., 2011. Optimized Weighted
    Performance Index for Objective Evaluation of Border-Detection Methods in Dermoscopy
    Images. IEEE Transactions on Information Technology in Biomedicine 15, 908–917.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garnavi 和 Aldeen (2011) Garnavi, R., Aldeen, M., 2011. 皮肤镜图像中边界检测方法的优化加权性能指数。IEEE
    生物医学信息技术学报 15, 908–917。
- en: Garnavi et al. (2011a) Garnavi, R., Aldeen, M., Celebi, M.E., 2011a. Weighted
    Performance Index for Objective Evaluation of BorderDetection Methods in Dermoscopy
    Images. Skin Research and Technology 17, 35–44.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garnavi 等 (2011a) Garnavi, R., Aldeen, M., Celebi, M.E., 2011a. 皮肤镜图像中边界检测方法的加权性能指数。皮肤研究与技术
    17, 35–44。
- en: Garnavi et al. (2011b) Garnavi, R., Aldeen, M., Celebi, M.E., Varigos, G., Finch,
    S., 2011b. Border Detection in Dermoscopy Images Using Hybrid Thresholding on
    Optimized Color Channels. Computerized Medical Imaging and Graphics 35, 105–115.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garnavi 等 (2011b) Garnavi, R., Aldeen, M., Celebi, M.E., Varigos, G., Finch,
    S., 2011b. 使用优化颜色通道上的混合阈值进行皮肤镜图像中的边界检测。计算机医学成像与图形 35, 105–115。
- en: 'Gidaris et al. (2018) Gidaris, S., Singh, P., Komodakis, N., 2018. Unsupervised
    representation learning by predicting image rotations, in: International Conference
    on Learning Representations (ICLR), pp. 1–16. URL: [https://openreview.net/forum?id=S1v4N2l0-](https://openreview.net/forum?id=S1v4N2l0-).'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gidaris 等 (2018) Gidaris, S., Singh, P., Komodakis, N., 2018. 通过预测图像旋转进行无监督表示学习，见：国际学习表示会议（ICLR），第
    1–16 页。网址：[https://openreview.net/forum?id=S1v4N2l0-](https://openreview.net/forum?id=S1v4N2l0-)。
- en: 'Giotis et al. (2015) Giotis, I., Molders, N., Land, S., Biehl, M., Jonkman,
    M.F., Petkov, N., 2015. MED-NODE: A computer-assisted melanoma diagnosis system
    using non-dermoscopic images. Expert Systems with Applications 42, 6578–6585.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giotis 等 (2015) Giotis, I., Molders, N., Land, S., Biehl, M., Jonkman, M.F.,
    Petkov, N., 2015. MED-NODE：一种使用非皮肤镜图像的计算机辅助黑色素瘤诊断系统。专家系统与应用 42, 6578–6585。
- en: 'Gish and Blanz (1989) Gish, S.L., Blanz, W.E., 1989. Comparing the Performance
    of Connectionist and Statistical Classifiers on an Image Segmentation Problem,
    in: Proceedings of the Second International Conference on Neural Information Processing
    Systems, pp. 614–621.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gish和Blanz（1989）Gish, S.L., Blanz, W.E., 1989。比较连接主义和统计分类器在图像分割问题上的表现，见：第二届国际神经信息处理系统会议论文集，614–621页。
- en: 'Glaister (2013) Glaister, J.L., 2013. Automatic segmentation of skin lesions
    from dermatological photographs. [https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection](https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection).
    Cited: 2022-1-31.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glaister（2013）Glaister, J.L., 2013。从皮肤科照片中自动分割皮肤病变。[https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection](https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection)。引用日期：2022-1-31。
- en: 'Goel et al. (2020) Goel, S., Sharma, Y., Jauer, M.L., Deserno, T.M., 2020.
    WeLineation: Crowdsourcing Delineations for Reliable Ground Truth Estimation,
    in: Proceedings of the Medical Imaging 2020: Imaging Informatics for Healthcare,
    Research, and Applications, pp. 113180C–1–113180C–8.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goel等（2020）Goel, S., Sharma, Y., Jauer, M.L., Deserno, T.M., 2020。WeLineation：用于可靠的真值估计的众包分界，见：2020年医学成像：医疗保健、研究和应用中的成像信息学论文集，113180C–1–113180C–8页。
- en: Gómez et al. (2007) Gómez, D.D., Butakoff, C., Ersboll, B.K., Stoecker, W.,
    2007. Independent histogram pursuit for segmentation of skin lesions. IEEE transactions
    on biomedical engineering 55, 157–161.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gómez等（2007）Gómez, D.D., Butakoff, C., Ersboll, B.K., Stoecker, W., 2007。独立直方图追踪用于皮肤病变的分割。《IEEE生物医学工程学报》55，157–161。
- en: 'Gonzalez-Diaz (2018) Gonzalez-Diaz, I., 2018. Dermaknet: Incorporating the
    knowledge of dermatologists to convolutional neural networks for skin lesion diagnosis.
    IEEE journal of biomedical and health informatics 23, 547–559.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gonzalez-Diaz（2018）Gonzalez-Diaz, I., 2018。Dermaknet：将皮肤科医生的知识融入卷积神经网络用于皮肤病变诊断。《IEEE生物医学与健康信息学杂志》23，547–559。
- en: Goodfellow et al. (2020) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2020. Generative Adversarial
    Networks. Communications of the ACM 63, 139–144.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow等（2020）Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,
    D., Ozair, S., Courville, A., Bengio, Y., 2020。生成对抗网络。《ACM通讯》63，139–144。
- en: 'Goyal et al. (2019a) Goyal, M., Ng, J., Oakley, A., Yap, M.H., 2019a. Skin
    lesion boundary segmentation with fully automated deep extreme cut methods, in:
    Medical Imaging 2019: Biomedical Applications in Molecular, Structural, and Functional
    Imaging, International Society for Optics and Photonics. p. 109530Q.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal等（2019a）Goyal, M., Ng, J., Oakley, A., Yap, M.H., 2019a。利用全自动深度极端切割方法进行皮肤病变边界分割，见：2019年医学成像：分子、结构和功能成像中的生物医学应用，国际光学与光子学学会。第109530Q页。
- en: Goyal et al. (2019b) Goyal, M., Oakley, A., Bansal, P., Dancey, D., Yap, M.H.,
    2019b. Skin lesion segmentation in dermoscopic images with ensemble deep learning
    methods. IEEE Access 8, 4171–4181.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal等（2019b）Goyal, M., Oakley, A., Bansal, P., Dancey, D., Yap, M.H., 2019b。使用集成深度学习方法进行皮肤病变的分割。《IEEE
    Access》8，4171–4181。
- en: 'Goyal et al. (2017) Goyal, M., Yap, M.H., Hassanpour, S., 2017. Multi-class
    semantic segmentation of skin lesions via fully convolutional networks, in: Proceedings
    of the 13th International Joint Conference on Biomedical Engineering Systems and
    Technologies, Comp2Clinic Workshop, pp. 290–295.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal等（2017）Goyal, M., Yap, M.H., Hassanpour, S., 2017。通过全卷积网络进行皮肤病变的多类语义分割，见：第13届国际生物医学工程系统与技术联合会议论文集，Comp2Clinic研讨会，290–295页。
- en: Grau et al. (2004) Grau, V., Mewes, A.U.J., Alcaniz, M., Kikinis, R., Warfield,
    S.K., 2004. Improved Watershed Transform for Medical Image Segmentation Using
    Prior Information. IEEE Transactions on Medical Imaging 23, 447–458.
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grau等（2004）Grau, V., Mewes, A.U.J., Alcaniz, M., Kikinis, R., Warfield, S.K.,
    2004。使用先验信息改进的分水岭变换用于医学图像分割。《IEEE医学成像学报》23，447–458。
- en: Green et al. (1994) Green, A., Martin, N., Pfitzner, J., O’Rourke, M., Knight,
    N., 1994. Computer image analysis in the diagnosis of melanoma. Journal of the
    American Academy of Dermatology 31, 958–964.
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Green等（1994）Green, A., Martin, N., Pfitzner, J., O’Rourke, M., Knight, N., 1994。计算机图像分析在黑色素瘤诊断中的应用。《美国皮肤病学会杂志》31，958–964。
- en: 'Groh et al. (2021) Groh, M., Harris, C., Soenksen, L., Lau, F., Han, R., Kim,
    A., Koochek, A., Badri, O., 2021. Evaluating deep neural networks trained on clinical
    images in dermatology with the Fitzpatrick 17k dataset, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1820–1828.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Groh 等（2021）Groh, M., Harris, C., Soenksen, L., Lau, F., Han, R., Kim, A., Koochek,
    A., Badri, O., 2021. 使用Fitzpatrick 17k数据集评估在皮肤病学中训练的深度神经网络，见：IEEE/CVF计算机视觉与模式识别会议论文集，pp.
    1820–1828。
- en: 'Gu et al. (2021) Gu, P., Zheng, H., Zhang, Y., Wang, C., Chen, D.Z., 2021.
    kCBAC-Net: Deeply supervised complete bipartite networks with asymmetric convolutions
    for medical image segmentation, in: International Conference on Medical Image
    Computing and Computer-Assisted Intervention, Springer. pp. 337–347.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2021）Gu, P., Zheng, H., Zhang, Y., Wang, C., Chen, D.Z., 2021. kCBAC-Net：深度监督的完全二分网络与非对称卷积用于医学图像分割，见：医学图像计算与计算机辅助干预国际会议，Springer。pp.
    337–347。
- en: 'Gu et al. (2020) Gu, R., Wang, G., Song, T., Huang, R., Aertsen, M., Deprest,
    J., Ourselin, S., Vercauteren, T., Zhang, S., 2020. CA-Net: Comprehensive attention
    convolutional neural networks for explainable medical image segmentation. IEEE
    transactions on medical imaging 40, 699–711.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2020）Gu, R., Wang, G., Song, T., Huang, R., Aertsen, M., Deprest, J., Ourselin,
    S., Vercauteren, T., Zhang, S., 2020. CA-Net：用于可解释医学图像分割的全面注意卷积神经网络。《IEEE医学成像学报》40,
    699–711。
- en: 'Gu et al. (2022) Gu, R., Wang, L., Zhang, L., 2022. DE-Net: A deep edge network
    with boundary information for automatic skin lesion segmentation. Neurocomputing
    468, 71–84.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2022）Gu, R., Wang, L., Zhang, L., 2022. DE-Net：一种具有边界信息的深度边缘网络用于自动皮肤病变分割。《神经计算》468,
    71–84。
- en: Gudhe et al. (2021) Gudhe, N.R., Behravan, H., Sudah, M., Okuma, H., Vanninen,
    R., Kosma, V.M., Mannermaa, A., 2021. Multi-level dilated residual network for
    biomedical image segmentation. Scientific Reports 11, 1–18.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gudhe 等（2021）Gudhe, N.R., Behravan, H., Sudah, M., Okuma, H., Vanninen, R.,
    Kosma, V.M., Mannermaa, A., 2021. 用于生物医学图像分割的多级膨胀残差网络。《科学报告》11, 1–18。
- en: Guillod et al. (2002) Guillod, J., Schmid-Saugeon, P., Guggisberg, D., Cerottini,
    J.P., Braun, R., Krischer, J., Saurat, J.H., Kunt, M., 2002. Validation of Segmentation
    Techniques for Digital Dermoscopy. Skin Research and Technology 8, 240–249.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guillod 等（2002）Guillod, J., Schmid-Saugeon, P., Guggisberg, D., Cerottini, J.P.,
    Braun, R., Krischer, J., Saurat, J.H., Kunt, M., 2002. 数字皮肤镜分割技术的验证。《皮肤研究与技术》8,
    240–249。
- en: Gulzar and Khan (2022) Gulzar, Y., Khan, S.A., 2022. Skin lesion segmentation
    based on vision transformers and convolutional neural networks—a comparative study.
    Applied Sciences 12, 5990.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulzar 和 Khan（2022）Gulzar, Y., Khan, S.A., 2022. 基于视觉变换器和卷积神经网络的皮肤病变分割——比较研究。《应用科学》12,
    5990。
- en: 'Guo et al. (2020) Guo, X., Chen, Z., Yuan, Y., 2020. Complementary network
    with adaptive receptive fields for melanoma segmentation, in: 2020 IEEE 17th International
    Symposium on Biomedical Imaging (ISBI), IEEE. pp. 2010–2013.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2020）Guo, X., Chen, Z., Yuan, Y., 2020. 带有自适应感受野的互补网络用于黑色素瘤分割，见：2020 IEEE第17届生物医学成像国际研讨会（ISBI），IEEE。pp.
    2010–2013。
- en: 'Gurari et al. (2015) Gurari, D., Theriault, D., Sameki, M., Isenberg, B., Pham,
    T.A., Purwada, A., Solski, P., Walker, M., Zhang, C., Wong, J.Y., Betke, M., 2015.
    How to Collect Segmentations for Biomedical Images? A Benchmark Evaluating the
    Performance of Experts, Crowdsourced Non-Experts, and Algorithms, in: 2015 IEEE
    Winter Conference on Applications of Computer Vision, pp. 1169–1176.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gurari 等（2015）Gurari, D., Theriault, D., Sameki, M., Isenberg, B., Pham, T.A.,
    Purwada, A., Solski, P., Walker, M., Zhang, C., Wong, J.Y., Betke, M., 2015. 如何收集生物医学图像的分割？专家、众包非专家和算法的性能评估基准，见：2015
    IEEE冬季计算机视觉应用会议，pp. 1169–1176。
- en: 'Gutman et al. (2016) Gutman, D., Codella, N.C.F., Celebi, M.E., Helba, B.,
    Marchetti, M., Mishra, N., Halpern, A., 2016. Skin Lesion Analysis Toward Melanoma
    Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI)
    2016, hosted by the International Skin Imaging Collaboration (ISIC). [http://arxiv.org/abs/1605.01397](http://arxiv.org/abs/1605.01397).'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gutman 等（2016）Gutman, D., Codella, N.C.F., Celebi, M.E., Helba, B., Marchetti,
    M., Mishra, N., Halpern, A., 2016. 皮肤病变分析以检测黑色素瘤：在2016年国际生物医学成像研讨会（ISBI）上的挑战，由国际皮肤成像合作组织（ISIC）主办。[http://arxiv.org/abs/1605.01397](http://arxiv.org/abs/1605.01397)。
- en: Guy Jr et al. (2015) Guy Jr, G.P., Machlin, S.R., Ekwueme, D.U., Yabroff, K.R.,
    2015. Prevalence and costs of skin cancer treatment in the us, 2002- 2006 and
    2007- 2011. American Journal of Preventive Medicine 48, 183–187.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guy Jr 等（2015）Guy Jr, G.P., Machlin, S.R., Ekwueme, D.U., Yabroff, K.R., 2015.
    美国皮肤癌治疗的流行率和费用，2002-2006年和2007-2011年。《美国预防医学杂志》48, 183–187。
- en: 'Halpern (2003) Halpern, A.C., 2003. Total body skin imaging as an aid to melanoma
    detection., in: Seminars in Cutaneous Medicine and Surgery, pp. 2–8.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Halpern（2003）Halpern, A.C., 2003. 全身皮肤成像作为黑色素瘤检测的辅助工具，见：皮肤医学与外科研讨会，页码 2–8。
- en: Hance et al. (1996) Hance, G.A., Umbaugh, S.E., Moss, R.H., Stoecker, W.V.,
    1996. Unsupervised Color Image Segmentation with Application to Skin Tumor Borders.
    IEEE Engineering in Medicine and Biology Magazine 15, 104–111.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hance 等人（1996）Hance, G.A., Umbaugh, S.E., Moss, R.H., Stoecker, W.V., 1996.
    无监督的彩色图像分割及其在皮肤肿瘤边界检测中的应用。IEEE 医学与生物学工程杂志 15, 104–111。
- en: 'Hasan et al. (2021) Hasan, M., Roy, S., Mondal, C., Alam, M., Elahi, M., Toufick,
    E., Dutta, A., Raju, S., Ahmad, M., et al., 2021. Dermo-doctor: A framework for
    concurrent skin lesion detection and recognition using a deep convolutional neural
    network with end-to-end dual encoders. Biomedical Signal Processing and Control
    68, 102661.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasan 等人（2021）Hasan, M., Roy, S., Mondal, C., Alam, M., Elahi, M., Toufick,
    E., Dutta, A., Raju, S., Ahmad, M., 等，2021. Dermo-doctor：一个用于皮肤病变检测和识别的框架，基于深度卷积神经网络和端到端双编码器。生物医学信号处理与控制
    68, 102661。
- en: 'Hasan et al. (2020) Hasan, M.K., Dahal, L., Samarakoon, P.N., Tushar, F.I.,
    Martí, R., 2020. DSNet: Automatic dermoscopic skin lesion segmentation. Computers
    in Biology and Medicine , 103738.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasan 等人（2020）Hasan, M.K., Dahal, L., Samarakoon, P.N., Tushar, F.I., Martí,
    R., 2020. DSNet：自动化皮肤镜皮肤病变分割。生物医学与医学计算机 103738。
- en: 'He et al. (2022) He, K., Gan, C., Li, Z., Rekik, I., Yin, Z., Ji, W., Gao,
    Y., Wang, Q., Zhang, J., Shen, D., 2022. Transformers in medical image analysis:
    A review. Intelligent Medicine URL: [https://www.sciencedirect.com/science/article/pii/S2667102622000717](https://www.sciencedirect.com/science/article/pii/S2667102622000717),
    doi:[https://doi.org/10.1016/j.imed.2022.07.002](http://dx.doi.org/https://doi.org/10.1016/j.imed.2022.07.002).'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2022）He, K., Gan, C., Li, Z., Rekik, I., Yin, Z., Ji, W., Gao, Y., Wang,
    Q., Zhang, J., Shen, D., 2022. 医学图像分析中的变换器：综述。Intelligent Medicine 网址：[https://www.sciencedirect.com/science/article/pii/S2667102622000717](https://www.sciencedirect.com/science/article/pii/S2667102622000717)，doi：[https://doi.org/10.1016/j.imed.2022.07.002](http://dx.doi.org/https://doi.org/10.1016/j.imed.2022.07.002)。
- en: 'He et al. (2016) He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
    for image recognition, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp. 770–778.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2016）He, K., Zhang, X., Ren, S., Sun, J., 2016. 图像识别的深度残差学习，见：IEEE 计算机视觉与模式识别会议论文集，页码
    770–778。
- en: 'He et al. (2017) He, X., Yu, Z., Wang, T., Lei, B., 2017. Skin lesion segmentation
    via deep RefineNet, in: Deep Learning in Medical Image Analysis and Multimodal
    Learning for Clinical Decision Support. Springer, pp. 303–311.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2017）He, X., Yu, Z., Wang, T., Lei, B., 2017. 通过深度 RefineNet 进行皮肤病变分割，见：医学图像分析中的深度学习与临床决策支持的多模态学习。Springer，页码
    303–311。
- en: 'He et al. (2018) He, X., Yu, Z., Wang, T., Lei, B., Shi, Y., 2018. Dense deconvolution
    net: Multi path fusion and dense deconvolution for high resolution skin lesion
    segmentation. Technology and Health Care 26, 307–316.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2018）He, X., Yu, Z., Wang, T., Lei, B., Shi, Y., 2018. 密集反卷积网络：多路径融合与密集反卷积用于高分辨率皮肤病变分割。技术与健康护理
    26, 307–316。
- en: 'Henry et al. (2020) Henry, H.Y., Feng, X., Wang, Z., Sun, H., 2020. Mixmodule:
    Mixed cnn kernel module for medical image segmentation, in: 2020 IEEE 17th International
    Symposium on Biomedical Imaging (ISBI), IEEE. pp. 1508–1512.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henry 等人（2020）Henry, H.Y., Feng, X., Wang, Z., Sun, H., 2020. Mixmodule：用于医学图像分割的混合
    CNN 核模块，见：2020 IEEE 第17届生物医学成像国际研讨会（ISBI），IEEE。页码 1508–1512。
- en: 'Hornung et al. (2021) Hornung, A., Steeb, T., Wessely, A., Brinker, T.J., Breakell,
    T., Erdmann, M., Berking, C., Heppt, M.V., 2021. The value of total body photography
    for the early detection of melanoma: A systematic review. International Journal
    of Environmental Research and Public Health 18, 1726.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hornung 等人（2021）Hornung, A., Steeb, T., Wessely, A., Brinker, T.J., Breakell,
    T., Erdmann, M., Berking, C., Heppt, M.V., 2021. 全身摄影在早期检测黑色素瘤中的价值：系统评价。国际环境研究与公共卫生杂志
    18, 1726。
- en: 'Howard et al. (2019) Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B.,
    Tan, M., W., W., Zhu, Y., Pang, R., Vasudevan, V., Le, Q.V., Adam, H., 2019. Searching
    for MobileNetV3, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp. 1314–1324.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard 等人（2019）Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan,
    M., W., W., Zhu, Y., Pang, R., Vasudevan, V., Le, Q.V., Adam, H., 2019. 寻找 MobileNetV3，见：IEEE/CVF
    国际计算机视觉会议论文集，页码 1314–1324。
- en: 'Hu et al. (2019) Hu, H., Zhang, Z., Xie, Z., Lin, S., 2019. Local relation
    networks for image recognition, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 3464–3473.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2019）Hu, H., Zhang, Z., Xie, Z., Lin, S., 2019. 用于图像识别的局部关系网络，见：IEEE/CVF
    国际计算机视觉会议论文集，页码 3464–3473。
- en: 'Hu et al. (2018) Hu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation networks,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 7132–7141.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2018) Hu, J., Shen, L., Sun, G., 2018. 压缩与激励网络，见：IEEE 计算机视觉与模式识别会议论文集，第7132–7141页。
- en: 'Huang et al. (2017) Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.,
    2017. Densely connected convolutional networks, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 4700–4708.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2017) Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.,
    2017. 密集连接卷积网络，见：IEEE 计算机视觉与模式识别会议论文集，第4700–4708页。
- en: 'Huang et al. (2022) Huang, W.L., Liu, S., Kang, J., Gandjbakhche, A., Armand,
    M., 2022. DICOM file for total body photography: a work item proposal, in: Photonics
    in Dermatology and Plastic Surgery 2022, SPIE. pp. 64–74.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2022) Huang, W.L., Liu, S., Kang, J., Gandjbakhche, A., Armand,
    M., 2022. 总体摄影的 DICOM 文件：工作项目提案，见：2022年光子学在皮肤科和整形外科中的应用，SPIE。第64–74页。
- en: Huttenlocher et al. (1993) Huttenlocher, D.P., Klanderman, G.A., Rucklidge,
    W.J., 1993. Comparing Images Using the Hausdorff Distance. IEEE Transactions on
    Pattern Analysis and Machine Intelligence 15, 850–863.
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huttenlocher et al. (1993) Huttenlocher, D.P., Klanderman, G.A., Rucklidge,
    W.J., 1993. 使用Hausdorff距离比较图像。IEEE 模式分析与机器智能学报 15, 850–863。
- en: 'ISIC (2018) ISIC, 2018. ISIC Live Leaderboards: 2018.1: Lesion Boundary Segmentation.
    [https://challenge.isic-archive.com/leaderboards/live/](https://challenge.isic-archive.com/leaderboards/live/).
    [Online. Accessed January 17, 2023].'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ISIC (2018) ISIC, 2018. ISIC 实时排行榜：2018.1：病变边界分割。 [https://challenge.isic-archive.com/leaderboards/live/](https://challenge.isic-archive.com/leaderboards/live/)。
    [在线。访问日期：2023年1月17日]。
- en: 'ISIC (2023) ISIC, 2023. International Skin Imaging Collaboration: Melanoma
    Project. [https://www.isic-archive.com/](https://www.isic-archive.com/). [Online.
    Accessed January 17, 2023].'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ISIC (2023) ISIC, 2023. 国际皮肤成像合作：黑色素瘤项目。 [https://www.isic-archive.com/](https://www.isic-archive.com/)。
    [在线。访问日期：2023年1月17日]。
- en: 'Isola et al. (2017) Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A., 2017. Image-to-image
    translation with conditional adversarial networks, in: Proceedings of the IEEE
    conference on computer vision and pattern recognition, pp. 1125–1134.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isola et al. (2017) Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A., 2017. 基于条件对抗网络的图像到图像翻译，见：IEEE
    计算机视觉与模式识别会议论文集，第1125–1134页。
- en: Iyatomi et al. (2008) Iyatomi, H., Oka, H., Celebi, M.E., Hashimoto, M., Hagiwara,
    M., Tanaka, M., Ogawa, K., 2008. An Improved Internet-Based Melanoma Screening
    System with Dermatologist-Like Tumor Area Extraction Algorithm. Computerized Medical
    Imaging and Graphics 32, 566–579.
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyatomi et al. (2008) Iyatomi, H., Oka, H., Celebi, M.E., Hashimoto, M., Hagiwara,
    M., Tanaka, M., Ogawa, K., 2008. 改进的基于互联网的黑色素瘤筛查系统，具有类似皮肤科医生的肿瘤区域提取算法。计算医学成像与图形学
    32, 566–579。
- en: Iyatomi et al. (2006) Iyatomi, H., Oka, H., Saito, M., Miyake, A., Kimoto, M.,
    Yamagami, J., Kobayashi, S., Tanikawa, A., Hagiwara, M., Ogawa, K., Argenziano,
    G., Soyer, H.P., Tanaka, M., 2006. Quantitative Assessment of Tumor Extraction
    from Dermoscopy Images and Evaluation of Computer-Based Extraction Methods for
    Automatic Melanoma Diagnostic System. Melanoma Research 16, 183–190.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyatomi et al. (2006) Iyatomi, H., Oka, H., Saito, M., Miyake, A., Kimoto, M.,
    Yamagami, J., Kobayashi, S., Tanikawa, A., Hagiwara, M., Ogawa, K., Argenziano,
    G., Soyer, H.P., Tanaka, M., 2006. 从皮肤镜图像中定量评估肿瘤提取及评估计算机化提取方法用于自动黑色素瘤诊断系统。黑色素瘤研究
    16, 183–190。
- en: 'Izadi et al. (2018) Izadi, S., Mirikharaji, Z., Kawahara, J., Hamarneh, G.,
    2018. Generative adversarial networks to segment skin lesions, in: 2018 IEEE 15th
    International Symposium on Biomedical Imaging (ISBI 2018), IEEE. pp. 881–884.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izadi et al. (2018) Izadi, S., Mirikharaji, Z., Kawahara, J., Hamarneh, G.,
    2018. 使用生成对抗网络进行皮肤病变分割，见：2018 IEEE 第十五届生物医学成像国际研讨会（ISBI 2018），IEEE。第881–884页。
- en: Jaccard (1901) Jaccard, P., 1901. Distribution de la Flore Alpine dans le Bassin
    des Dranses et dans Quelques Regions Voisines. Bulletin de la Societe Vaudoise
    des Sciences Naturelles 37, 241–272.
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaccard (1901) Jaccard, P., 1901. 分布在德朗斯盆地及邻近区域的高山植物群。沃州自然科学学会公报 37, 241–272。
- en: Jaccard (1912) Jaccard, P., 1912. The distribution of the flora in the alpine
    zone. New Phytologist 11, 37–50.
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaccard (1912) Jaccard, P., 1912. 高山区域植物群的分布。新植物学家 11, 37–50。
- en: 'Jafari et al. (2020) Jafari, M., Auer, D., Francis, S., Garibaldi, J., Chen,
    X., 2020. Dru-net: An efficient deep convolutional neural network for medical
    image segmentation, in: 2020 IEEE 17th International Symposium on Biomedical Imaging
    (ISBI), IEEE. pp. 1144–1148.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jafari et al. (2020) Jafari, M., Auer, D., Francis, S., Garibaldi, J., Chen,
    X., 2020. Dru-net：一种高效的深度卷积神经网络用于医学图像分割，见：2020 IEEE 第十七届生物医学成像国际研讨会（ISBI），IEEE。第1144–1148页。
- en: 'Jafari et al. (2016) Jafari, M.H., Karimi, N., Nasr-Esfahani, E., Samavi, S.,
    Soroushmehr, S.M.R., Ward, K., Najarian, K., 2016. Skin lesion segmentation in
    clinical images using deep learning, in: 2016 23rd International conference on
    pattern recognition (ICPR), IEEE. pp. 337–342.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jafari等人（2016）Jafari, M.H., Karimi, N., Nasr-Esfahani, E., Samavi, S., Soroushmehr,
    S.M.R., Ward, K., Najarian, K., 2016. 使用深度学习对临床图像中的皮肤病变进行分割，发表于：2016年第23届国际模式识别大会（ICPR），IEEE出版社，第337–342页。
- en: Jafari et al. (2017) Jafari, M.H., Nasr-Esfahani, E., Karimi, N., Soroushmehr,
    S.R., Samavi, S., Najarian, K., 2017. Extraction of skin lesions from non-dermoscopic
    images for surgical excision of melanoma. International journal of computer assisted
    radiology and surgery 12, 1021–1030.
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jafari等人（2017）Jafari, M.H., Nasr-Esfahani, E., Karimi, N., Soroushmehr, S.R.,
    Samavi, S., Najarian, K., 2017. 从非皮肤镜图像中提取皮肤病变以进行黑色素瘤的外科切除。国际计算机辅助放射学与外科杂志 12,
    1021–1030。
- en: Jahanifar et al. (2018) Jahanifar, M., Tajeddin, N.Z., Koohbanani, N.A., Gooya,
    A., Rajpoot, N., 2018. Segmentation of skin lesions and their attributes using
    multi-scale convolutional neural networks and domain specific augmentations. arXiv
    preprint arXiv:1809.10243 .
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jahanifar等人（2018）Jahanifar, M., Tajeddin, N.Z., Koohbanani, N.A., Gooya, A.,
    Rajpoot, N., 2018. 使用多尺度卷积神经网络和领域特定增强的皮肤病变及其属性分割。arXiv预印本 arXiv:1809.10243。
- en: 'Japkowicz and Shah (2011) Japkowicz, N., Shah, M., 2011. Evaluating Learning
    Algorithms: A Classification Perspective. Cambridge University Press.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Japkowicz和Shah（2011）Japkowicz, N., Shah, M., 2011. 评估学习算法：分类视角。剑桥大学出版社。
- en: Jaworek-Korjakowska et al. (2021) Jaworek-Korjakowska, J., Brodzicki, A., Cassidy,
    B., Kendrick, C., Yap, M.H., 2021. Interpretability of a deep learning based approach
    for the classification of skin lesions into main anatomic body sites. Cancers
    13, 6048.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaworek-Korjakowska等人（2021）Jaworek-Korjakowska, J., Brodzicki, A., Cassidy,
    B., Kendrick, C., Yap, M.H., 2021. 基于深度学习的方法对皮肤病变进行主要解剖部位分类的可解释性。癌症 13, 6048。
- en: Jayapriya and Jacob (2020) Jayapriya, K., Jacob, I.J., 2020. Hybrid fully convolutional
    networks-based skin lesion segmentation and melanoma detection using deep feature.
    International Journal of Imaging Systems and Technology 30, 348–357.
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jayapriya和Jacob（2020）Jayapriya, K., Jacob, I.J., 2020. 基于混合全卷积网络的皮肤病变分割和黑色素瘤检测，使用深度特征。国际成像系统与技术期刊
    30, 348–357。
- en: 'Jensen and Elewski (2015) Jensen, J.D., Elewski, B.E., 2015. The ABCDEF rule:
    combining the “ABCDE rule” and the “ugly duckling sign” in an effort to improve
    patient self-screening examinations. The Journal of Clinical and Aesthetic Dermatology
    8, 15.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jensen和Elewski（2015）Jensen, J.D., Elewski, B.E., 2015. ABCDEF规则：结合“ABCDE规则”和“丑小鸭标志”，以改进患者自我筛查检查。《临床与美学皮肤科杂志》8,
    15。
- en: 'Ji et al. (2019) Ji, X., Henriques, J.F., Vedaldi, A., 2019. Invariant Information
    Clustering for Unsupervised Image Classification and Segmentation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 9865–9874.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji等人（2019）Ji, X., Henriques, J.F., Vedaldi, A., 2019. 用于无监督图像分类和分割的不变信息聚类，发表于：IEEE/CVF国际计算机视觉会议论文集，第9865–9874页。
- en: 'Ji et al. (2021) Ji, Y., Zhang, R., Wang, H., Li, Z., Wu, L., Zhang, S., Luo,
    P., 2021. Multi-compound Transformer for accurate biomedical image segmentation,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer. pp. 326–336.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji等人（2021）Ji, Y., Zhang, R., Wang, H., Li, Z., Wu, L., Zhang, S., Luo, P., 2021.
    用于准确生物医学图像分割的多复合体Transformer，发表于：国际医学图像计算与计算机辅助干预会议，Springer出版社，第326–336页。
- en: 'Jiang et al. (2019) Jiang, F., Zhou, F., Qin, J., Wang, T., Lei, B., 2019.
    Decision-augmented generative adversarial network for skin lesion segmentation,
    in: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),
    IEEE. pp. 447–450.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人（2019）Jiang, F., Zhou, F., Qin, J., Wang, T., Lei, B., 2019. 用于皮肤病变分割的决策增强生成对抗网络，发表于：2019年IEEE第16届生物医学成像国际研讨会（ISBI
    2019），IEEE出版社，第447–450页。
- en: 'Jiang et al. (2022) Jiang, X., Jiang, J., Wang, B., Yu, J., Wang, J., 2022.
    SEACU-Net: Attentive ConvLSTM U-Net with squeeze-and-excitation layer for skin
    lesion segmentation. Computer Methods and Programs in Biomedicine 225, 107076.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人（2022）Jiang, X., Jiang, J., Wang, B., Yu, J., Wang, J., 2022. SEACU-Net：具有挤压和激励层的注意力ConvLSTM
    U-Net，用于皮肤病变分割。生物医学计算方法与程序 225, 107076。
- en: Jiang et al. (2020) Jiang, Y., Cao, S., Tao, S., Zhang, H., 2020. Skin lesion
    segmentation based on multi-scale attention convolutional neural network. IEEE
    Access 8, 122811–122825.
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人（2020）Jiang, Y., Cao, S., Tao, S., Zhang, H., 2020. 基于多尺度注意力卷积神经网络的皮肤病变分割。IEEE
    Access 8, 122811–122825。
- en: Jin et al. (2021) Jin, Q., Cui, H., Sun, C., Meng, Z., Su, R., 2021. Cascade
    knowledge diffusion network for skin lesion diagnosis and segmentation. Applied
    Soft Computing 99, 106881.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin等（2021）Jin, Q., Cui, H., Sun, C., Meng, Z., Su, R., 2021. 用于皮肤病变诊断和分割的级联知识扩散网络。《应用软计算》99,
    106881。
- en: 'Kahn (1942) Kahn, R.L., 1942. Serology in Syphilis Control: Principles of Sensitivity
    and Specificity with an Appendix for Health Officers and Industrial Physicians.
    American Journal of Clinical Pathology 12, 446–446. URL: [https://doi.org/10.1093/ajcp/12.8.446d](https://doi.org/10.1093/ajcp/12.8.446d),
    doi:[10.1093/ajcp/12.8.446d](http://dx.doi.org/10.1093/ajcp/12.8.446d), [arXiv:https://academic.oup.com/ajcp/article-pdf/12/8/446/24886161/ajcpath12-0446d.pdf](http://arxiv.org/abs/https://academic.oup.com/ajcp/article-pdf/12/8/446/24886161/ajcpath12-0446d.pdf).'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kahn（1942）Kahn, R.L., 1942. 梅毒控制中的血清学：灵敏度和特异性的原则，并附健康官员和工业医生的附录。《美国临床病理学杂志》12,
    446–446。网址：[https://doi.org/10.1093/ajcp/12.8.446d](https://doi.org/10.1093/ajcp/12.8.446d)，doi：[10.1093/ajcp/12.8.446d](http://dx.doi.org/10.1093/ajcp/12.8.446d)，[arXiv:https://academic.oup.com/ajcp/article-pdf/12/8/446/24886161/ajcpath12-0446d.pdf](http://arxiv.org/abs/https://academic.oup.com/ajcp/article-pdf/12/8/446/24886161/ajcpath12-0446d.pdf)。
- en: Kamalakannan et al. (2019) Kamalakannan, A., Ganesan, S.S., Rajamanickam, G.,
    2019. Self-learning ai framework for skin lesion image segmentation and classification.
    International Journal of Computer Science and Information Technology 11, 29--38.
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamalakannan等（2019）Kamalakannan, A., Ganesan, S.S., Rajamanickam, G., 2019.
    自学习AI框架用于皮肤病变图像分割和分类。《计算机科学与信息技术国际期刊》11, 29--38。
- en: Kapoor and Narayanan (2022) Kapoor, S., Narayanan, A., 2022. Leakage and the
    reproducibility crisis in ML-based science. arXiv preprint arXiv:2207.07048 .
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapoor和Narayanan（2022）Kapoor, S., Narayanan, A., 2022. ML基础科学中的泄漏与可重复性危机。arXiv预印本
    arXiv:2207.07048。
- en: 'Karimi et al. (2020) Karimi, D., Dou, H., Warfield, S.K., Gholipour, A., 2020.
    Deep learning with noisy labels: Exploring techniques and remedies in medical
    image analysis. Medical Image Analysis 65, 101759.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karimi等（2020）Karimi, D., Dou, H., Warfield, S.K., Gholipour, A., 2020. 带有噪声标签的深度学习：在医学图像分析中探索技术和解决方法。《医学图像分析》65,
    101759。
- en: 'Karras et al. (2018) Karras, T., Aila, T., Laine, S., Lehtinen, J., 2018. Progressive
    growing of GANs for improved quality, stability, and variation, in: International
    Conference on Learning Representations, pp. 1--26. URL: [https://openreview.net/forum?id=Hk99zCeAb](https://openreview.net/forum?id=Hk99zCeAb).'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras等（2018）Karras, T., Aila, T., Laine, S., Lehtinen, J., 2018. GAN的渐进式增长以改进质量、稳定性和变异性，见：国际学习表征会议，第1--26页。网址：[https://openreview.net/forum?id=Hk99zCeAb](https://openreview.net/forum?id=Hk99zCeAb)。
- en: 'Kats et al. (2019) Kats, E., Goldberger, J., Greenspan, H., 2019. A soft staple
    algorithm combined with anatomical knowledge, in: International Conference on
    Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 510--517.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kats等（2019）Kats, E., Goldberger, J., Greenspan, H., 2019. 结合解剖知识的软钉算法，见：国际医学图像计算与计算机辅助干预会议，Springer。第510--517页。
- en: Katsch et al. (2022) Katsch, F., Rinner, C., Tschandl, P., 2022. Comparison
    of convolutional neural network architectures for robustness against common artefacts
    in dermatoscopic images. Dermatology Practical & Conceptual , e2022126--e2022126.
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katsch等（2022）Katsch, F., Rinner, C., Tschandl, P., 2022. 卷积神经网络架构在皮肤镜图像中对常见伪影的鲁棒性比较。《皮肤病学实践与概念》，e2022126--e2022126。
- en: 'Katz and Merickel (1989) Katz, W.T., Merickel, M.B., 1989. Translation-Invariant
    Aorta Segmentation from Magnetic Resonance Images, in: Proceedings of the 1989
    International Joint Conference on Neural Networks, pp. 327--333.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katz和Merickel（1989）Katz, W.T., Merickel, M.B., 1989. 磁共振图像中的平移不变主动脉分割，见：1989年国际联合神经网络会议论文集，第327--333页。
- en: 'Kaul et al. (2019) Kaul, C., Manandhar, S., Pears, N., 2019. FocusNet: an attention-based
    fully convolutional network for medical image segmentation, in: 2019 IEEE 16th
    International Symposium on Biomedical Imaging (ISBI 2019), IEEE. pp. 455--458.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaul等（2019）Kaul, C., Manandhar, S., Pears, N., 2019. FocusNet：一种基于注意力的全卷积网络用于医学图像分割，见：2019
    IEEE第16届生物医学成像国际研讨会（ISBI 2019），IEEE。第455--458页。
- en: 'Kaul et al. (2021) Kaul, C., Pears, N., Dai, H., Murray-Smith, R., Manandhar,
    S., 2021. Focusnet++: Attentive aggregated transformations for efficient and accurate
    medical image segmentation, in: 2021 IEEE 18th International Symposium on Biomedical
    Imaging (ISBI), IEEE. pp. 1042--1046.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaul等（2021）Kaul, C., Pears, N., Dai, H., Murray-Smith, R., Manandhar, S., 2021.
    Focusnet++：用于高效且准确医学图像分割的注意力聚合变换，见：2021 IEEE第18届生物医学成像国际研讨会（ISBI），IEEE。第1042--1046页。
- en: Kaur et al. (2022a) Kaur, R., GholamHosseini, H., Sinha, R., 2022a. Skin lesion
    segmentation using an improved framework of encoder-decoder based convolutional
    neural network. International Journal of Imaging Systems and Technology .
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaur 等 (2022a) Kaur, R., GholamHosseini, H., Sinha, R., 2022a. 使用改进的编码器-解码器基于卷积神经网络的框架进行皮肤病变分割。国际成像系统与技术期刊。
- en: Kaur et al. (2022b) Kaur, R., GholamHosseini, H., Sinha, R., Lindén, M., 2022b.
    Automatic lesion segmentation using atrous convolutional deep neural networks
    in dermoscopic skin cancer images. BMC Medical Imaging 22, 1--13.
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaur 等 (2022b) Kaur, R., GholamHosseini, H., Sinha, R., Lindén, M., 2022b. 在皮肤镜皮肤癌图像中使用膨胀卷积深度神经网络进行自动病变分割。BMC
    医学影像 22, 1--13。
- en: Kawahara et al. (2019) Kawahara, J., Daneshvar, S., Argenziano, G., Hamarneh,
    G., 2019. Seven-Point Checklist and Skin Lesion Classification Using Multitask
    Multimodal Neural Nets. IEEE Journal of Biomedical and Health Informatics 23,
    538--546.
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kawahara 等 (2019) Kawahara, J., Daneshvar, S., Argenziano, G., Hamarneh, G.,
    2019. 七点检查表和皮肤病变分类使用多任务多模态神经网络。IEEE 生物医学与健康信息学期刊 23, 538--546。
- en: Kawahara and Hamarneh (2018) Kawahara, J., Hamarneh, G., 2018. Fully convolutional
    neural networks to detect clinical dermoscopic features. IEEE journal of biomedical
    and health informatics 23, 578--585.
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kawahara 和 Hamarneh (2018) Kawahara, J., Hamarneh, G., 2018. 全卷积神经网络用于检测临床皮肤镜特征。IEEE
    生物医学与健康信息学期刊 23, 578--585。
- en: 'Kaymak et al. (2020) Kaymak, R., Kaymak, C., Ucar, A., 2020. Skin lesion segmentation
    using fully convolutional networks: A comparative experimental study. Expert Systems
    with Applications 161, 113742.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaymak 等 (2020) Kaymak, R., Kaymak, C., Ucar, A., 2020. 使用全卷积网络的皮肤病变分割：一项比较实验研究。专家系统应用
    161, 113742。
- en: Kazeminia et al. (2020) Kazeminia, S., Baur, C., Kuijper, A., van Ginneken,
    B., Navab, N., Albarqouni, S., Mukhopadhyay, A., 2020. GANs for Medical Image
    Analysis. Artificial Intelligence in Medicine 109, 101938.
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kazeminia 等 (2020) Kazeminia, S., Baur, C., Kuijper, A., van Ginneken, B., Navab,
    N., Albarqouni, S., Mukhopadhyay, A., 2020. 用于医学图像分析的 GANs。医学中的人工智能 109, 101938。
- en: 'Kent et al. (1955) Kent, A., Berry, M.M., Luehrs Jr, F.U., Perry, J.W., 1955.
    Machine literature searching: VIII. Operational criteria for designing information
    retrieval systems. American Documentation (pre-1986) 6, 93--101.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kent 等 (1955) Kent, A., Berry, M.M., Luehrs Jr, F.U., Perry, J.W., 1955. 机器文献检索：VIII.
    设计信息检索系统的操作标准。美国文献 (1986年以前) 6, 93--101。
- en: 'Khan et al. (2021) Khan, A., Kim, H., Chua, L., 2021. Pmed-net: Pyramid based
    multi-scale encoder-decoder network for medical image segmentation. IEEE Access
    9, 55988--55998.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等 (2021) Khan, A., Kim, H., Chua, L., 2021. Pmed-net：基于金字塔的多尺度编码器-解码器网络用于医学图像分割。IEEE
    Access 9, 55988--55998。
- en: 'Khan et al. (2022) Khan, A.H., Awang Iskandar, D.N., Al-Asad, J.F., Mewada,
    H., Sherazi, M.A., 2022. Ensemble learning of deep learning and traditional machine
    learning approaches for skin lesion segmentation and classification. Concurrency
    and Computation: Practice and Experience 34, e6907.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等 (2022) Khan, A.H., Awang Iskandar, D.N., Al-Asad, J.F., Mewada, H., Sherazi,
    M.A., 2022. 深度学习与传统机器学习方法的集成学习用于皮肤病变分割和分类。并发与计算：实践与经验 34, e6907。
- en: Khouloud et al. (2021) Khouloud, S., Ahlem, M., Fadel, T., Amel, S., 2021. W-net
    and inception residual network for skin lesion segmentation and classification.
    Applied Intelligence , 1--19.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khouloud 等 (2021) Khouloud, S., Ahlem, M., Fadel, T., Amel, S., 2021. 用于皮肤病变分割和分类的
    W-net 和 inception 残差网络。应用智能，1--19。
- en: Kim and Lee (2021) Kim, M., Lee, B.D., 2021. A simple generic method for effective
    boundary extraction in medical image segmentation. IEEE Access 9, 103875--103884.
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Lee (2021) Kim, M., Lee, B.D., 2021. 一种简单通用的方法用于医学图像分割中的有效边界提取。IEEE Access
    9, 103875--103884。
- en: 'Kinyanjui et al. (2020) Kinyanjui, N.M., Odonga, T., Cintas, C., Codella, N.C.,
    Panda, R., Sattigeri, P., Varshney, K.R., 2020. Fairness of classifiers across
    skin tones in dermatology, in: Medical Image Computing and Computer Assisted Intervention--MICCAI
    2020: 23rd International Conference, Lima, Peru, October 4--8, 2020, Proceedings,
    Part VI, Springer. pp. 320--329.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinyanjui 等 (2020) Kinyanjui, N.M., Odonga, T., Cintas, C., Codella, N.C., Panda,
    R., Sattigeri, P., Varshney, K.R., 2020. 皮肤科分类器在不同肤色中的公平性，见：医学图像计算与计算机辅助手术--MICCAI
    2020：第23届国际会议，秘鲁利马，2020年10月4--8日，会议论文，第六部分，Springer。第320--329页。
- en: Kittler et al. (2002) Kittler, H., Pehamberger, H., Wolff, K., Binder, M., 2002.
    Diagnostic accuracy of dermoscopy. The lancet oncology 3, 159--165.
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kittler 等 (2002) Kittler, H., Pehamberger, H., Wolff, K., Binder, M., 2002.
    皮肤镜检查的诊断准确性。柳叶刀肿瘤学 3, 159--165。
- en: Korotkov et al. (2019) Korotkov, K., Quintana, J., Campos, R., Jesús-Silva,
    A., Iglesias, P., Puig, S., Malvehy, J., Garcia, R., 2019. An improved skin lesion
    matching scheme in total body photography. IEEE Journal of Biomedical and Health
    Informatics 23, 586--598.
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korotkov 等 (2019) Korotkov, K., Quintana, J., Campos, R., Jesús-Silva, A., Iglesias,
    P., Puig, S., Malvehy, J., Garcia, R., 2019. 在全身摄影中的改进皮肤病变匹配方案。IEEE 生物医学与健康信息学杂志
    23, 586--598。
- en: 'Kosgiker et al. (2021) Kosgiker, G.M., Deshpande, A., Kauser, A., 2021. Segcaps:
    An efficient segcaps network-based skin lesion segmentation in dermoscopic images.
    International Journal of Imaging Systems and Technology 31, 874--894.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kosgiker 等 (2021) Kosgiker, G.M., Deshpande, A., Kauser, A., 2021. Segcaps：一种基于
    Segcaps 网络的高效皮肤病变分割方法。国际成像系统与技术杂志 31, 874--894。
- en: Kovashka et al. (2016) Kovashka, A., Russakovsky, O., Fei-Fei, L., Grauman,
    K., 2016. Crowdsourcing in Computer Vision. Foundations and Trends in Computer
    Graphics and Vision 10, 177--243.
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kovashka 等 (2016) Kovashka, A., Russakovsky, O., Fei-Fei, L., Grauman, K., 2016.
    计算机视觉中的众包。计算机图形学与视觉基础与趋势 10, 177--243。
- en: 'Krahenbuhl and Koltun (2011) Krahenbuhl, P., Koltun, V., 2011. Efficient Inference
    in Fully Connected CRFs with Gaussian Edge Potentials, in: Proceedings of the
    24th International Conference on Neural Information Processing Systems, pp. 109--117.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krahenbuhl 和 Koltun (2011) Krahenbuhl, P., Koltun, V., 2011. 在具有高斯边缘势的完全连接 CRF
    中进行高效推断，见：第24届国际神经信息处理系统大会论文集，第 109--117 页。
- en: Kubat et al. (1998) Kubat, M., Holte, R.C., Matwin, S., 1998. Machine Learning
    for the Detection of Oil Spills in Satellite Radar Images. Machine Learning 30,
    195--215.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubat 等 (1998) Kubat, M., Holte, R.C., Matwin, S., 1998. 用于检测卫星雷达图像中油污的机器学习。机器学习
    30, 195--215。
- en: 'Kwon et al. (2020) Kwon, Y., Won, J.H., Kim, B.J., Paik, M.C., 2020. Uncertainty
    quantification using bayesian neural networks in classification: Application to
    biomedical image segmentation. Computational Statistics & Data Analysis 142, 106816.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等 (2020) Kwon, Y., Won, J.H., Kim, B.J., Paik, M.C., 2020. 使用贝叶斯神经网络进行分类的不确定性量化：应用于生物医学图像分割。计算统计与数据分析
    142, 106816。
- en: Lampert et al. (2016) Lampert, T.A., Stumpf, A., Gancarski, P., 2016. An Empirical
    Study into Annotator Agreement, Ground Truth Estimation, and Algorithm Evaluation.
    IEEE Transactions on Image Processing 25, 2557--2572.
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lampert 等 (2016) Lampert, T.A., Stumpf, A., Gancarski, P., 2016. 对标注者一致性、地面真值估计和算法评估的实证研究。IEEE
    图像处理汇刊 25, 2557--2572。
- en: Langerak et al. (2010) Langerak, T.R., van der Heide, U.A., Kotte, A.N.T.J.,
    Viergever, M.A., Van Vulpen, M., Pluim, J.P.W., 2010. Label Fusion in Atlas-Based
    Segmentation Using a Selective and Iterative Method for Performance Level Estimation
    (SIMPLE). IEEE Transactions on Medical Imaging 29, 2000--2008.
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Langerak 等 (2010) Langerak, T.R., van der Heide, U.A., Kotte, A.N.T.J., Viergever,
    M.A., Van Vulpen, M., Pluim, J.P.W., 2010. 使用选择性和迭代方法进行性能水平估计 (SIMPLE) 的基于图谱的分割中的标签融合。IEEE
    医学成像学报 29, 2000--2008。
- en: LeCun et al. (2015) LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep Learning.
    Nature 521, 436--444.
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 (2015) LeCun, Y., Bengio, Y., Hinton, G., 2015. 深度学习。自然 521, 436--444。
- en: LeCun et al. (1998) LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., 1998. Gradient-Based
    Learning Applied to Document Recognition. Proceedings of the IEEE 86, 2278--2324.
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 (1998) LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., 1998. 应用于文档识别的基于梯度的学习。IEEE
    汇刊 86, 2278--2324。
- en: 'Lee et al. (2003) Lee, T.K., McLean, D.I., Atkins, M.S., 2003. Irregularity
    Index: A New Border Irregularity Measure for Cutaneous Melanocytic Lesions. Medical
    Image Analysis 7, 47--64.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2003) Lee, T.K., McLean, D.I., Atkins, M.S., 2003. 不规则指数：一种用于皮肤黑色素瘤病变的新边界不规则性测量方法。医学图像分析
    7, 47--64。
- en: Lei et al. (2020) Lei, B., Xia, Z., Jiang, F., Jiang, X., Ge, Z., Xu, Y., Qin,
    J., Chen, S., Wang, T., Wang, S., 2020. Skin lesion segmentation via generative
    adversarial networks with dual discriminators. Medical Image Analysis 64, 101716.
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等 (2020) Lei, B., Xia, Z., Jiang, F., Jiang, X., Ge, Z., Xu, Y., Qin, J.,
    Chen, S., Wang, T., Wang, S., 2020. 通过具有双重鉴别器的生成对抗网络进行皮肤病变分割。医学图像分析 64, 101716。
- en: 'Lemay et al. (2022) Lemay, A., Gros, C., Naga Karthik, E., Cohen-Adad, J.,
    2022. Label fusion and training methods for reliable representation of inter-rater
    uncertainty. Machine Learning for Biomedical Imaging 1, 1--27. URL: [https://melba-journal.org/2022:031](https://melba-journal.org/2022:031).'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lemay 等 (2022) Lemay, A., Gros, C., Naga Karthik, E., Cohen-Adad, J., 2022.
    标签融合与训练方法用于可靠表示评估者间不确定性。生物医学成像机器学习 1, 1--27。网址：[https://melba-journal.org/2022:031](https://melba-journal.org/2022:031)。
- en: Li et al. (2018a) Li, H., He, X., Zhou, F., Yu, Z., Ni, D., Chen, S., Wang,
    T., Lei, B., 2018a. Dense deconvolutional network for skin lesion segmentation.
    IEEE journal of biomedical and health informatics 23, 527--537.
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2018a）Li, H., He, X., Zhou, F., Yu, Z., Ni, D., Chen, S., Wang, T., Lei,
    B., 2018a. 用于皮肤病变分割的密集反卷积网络。IEEE生物医学与健康信息学杂志 23, 527--537。
- en: 'Li et al. (2020a) Li, R., Wagner, C., Chen, X., Auer, D., 2020a. A generic
    ensemble based deep convolutional neural network for semi-supervised medical image
    segmentation, in: 2020 IEEE 17th International Symposium on Biomedical Imaging
    (ISBI), IEEE. pp. 1168--1172.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2020a）Li, R., Wagner, C., Chen, X., Auer, D., 2020a. 基于通用集成的深度卷积神经网络用于半监督医学图像分割，见：2020
    IEEE第17届生物医学成像国际研讨会（ISBI），IEEE出版社。第1168--1172页。
- en: 'Li et al. (2021a) Li, S., Gao, Z., He, X., 2021a. Superpixel-guided iterative
    learning from noisy labels for medical image segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 525--535.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2021a）Li, S., Gao, Z., He, X., 2021a. 基于超像素引导的噪声标签迭代学习用于医学图像分割，见：国际医学图像计算与计算机辅助干预会议，Springer出版社。第525--535页。
- en: Li et al. (2021b) Li, W., Raj, A.N.J., Tjahjadi, T., Zhuang, Z., 2021b. Digital
    hair removal by deep learning for skin lesion segmentation. Pattern Recognition
    117, 107994.
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2021b）Li, W., Raj, A.N.J., Tjahjadi, T., Zhuang, Z., 2021b. 通过深度学习进行数字化脱毛用于皮肤病变分割。模式识别
    117, 107994。
- en: Li et al. (2021c) Li, X., Yu, L., Chen, H., Fu, C.W., Xing, L., Heng, P.A.,
    2021c. Transformation-consistent self-ensembling model for semi-supervised medical
    image segmentation. IEEE Transactions on Neural Networks and Learning Systems
    32, 523--534.
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2021c）Li, X., Yu, L., Chen, H., Fu, C.W., Xing, L., Heng, P.A., 2021c. 一致变换自集成模型用于半监督医学图像分割。IEEE神经网络与学习系统汇刊
    32, 523--534。
- en: 'Li et al. (2018b) Li, X., Yu, L., Fu, C.W., Heng, P.A., 2018b. Deeply supervised
    rotation equivariant network for lesion segmentation in dermoscopy images, in:
    OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy,
    Clinical Image-Based Procedures, and Skin Image Analysis. Springer, pp. 235--243.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2018b）Li, X., Yu, L., Fu, C.W., Heng, P.A., 2018b. 用于皮肤镜图像病变分割的深度监督旋转等变网络，见：OR
    2.0上下文感知手术室、计算机辅助机器人内窥镜、临床图像基础程序和皮肤图像分析。Springer出版社，第235--243页。
- en: 'Li et al. (2020b) Li, Y., Chen, J., Zheng, Y., 2020b. A multi-task self-supervised
    learning framework for scopy images, in: 2020 IEEE 17th International Symposium
    on Biomedical Imaging (ISBI), IEEE. pp. 2005--2009.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2020b）Li, Y., Chen, J., Zheng, Y., 2020b. 用于内窥镜图像的多任务自监督学习框架，见：2020 IEEE第17届生物医学成像国际研讨会（ISBI），IEEE出版社。第2005--2009页。
- en: 'Li et al. (2017) Li, Y., Esteva, A., Kuprel, B., Novoa, R., Ko, J., Thrun,
    S., 2017. Skin cancer detection and tracking using data synthesis and deep learning,
    in: AAAI Conference on Artificial Intelligence Joint Workshop on Health Intelligence,
    pp. 1--4.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2017）Li, Y., Esteva, A., Kuprel, B., Novoa, R., Ko, J., Thrun, S., 2017.
    使用数据合成和深度学习进行皮肤癌检测与追踪，见：AAAI人工智能会议健康智能联合研讨会，第1--4页。
- en: Li and Shen (2018) Li, Y., Shen, L., 2018. Skin lesion analysis towards melanoma
    detection using deep learning network. Sensors 18, 556.
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li和Shen（2018）Li, Y., Shen, L., 2018. 基于深度学习网络的皮肤病变分析以检测黑色素瘤。传感器 18, 556。
- en: 'Li et al. (2022) Li, Y., Xu, C., Han, J., An, Z., Wang, D., Ma, H., Liu, C.,
    2022. MHAU-Net: Skin lesion segmentation based on multi-scale hybrid residual
    attention network. Sensors 22, 8701.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2022）Li, Y., Xu, C., Han, J., An, Z., Wang, D., Ma, H., Liu, C., 2022. MHAU-Net：基于多尺度混合残差注意力网络的皮肤病变分割。传感器
    22, 8701。
- en: 'Lin et al. (2022) Lin, A., Xu, J., Li, J., Lu, G., 2022. ConTrans: Improving
    Transformer with convolutional attention for medical image segmentation, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 297--307.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2022）Lin, A., Xu, J., Li, J., Lu, G., 2022. ConTrans：通过卷积注意力改进Transformer用于医学图像分割，见：国际医学图像计算与计算机辅助干预会议，Springer出版社。第297--307页。
- en: 'Lin et al. (2019) Lin, H., Upchurch, P., Bala, K., 2019. Block annotation:
    Better image annotation with sub-image decomposition, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp. 5290--5300.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2019）Lin, H., Upchurch, P., Bala, K., 2019. 块注释：通过子图像分解改进图像注释，见：IEEE/CVF国际计算机视觉会议论文集，第5290--5300页。
- en: 'Lin et al. (2017) Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P., 2017.
    Focal loss for dense object detection, in: Proceedings of the IEEE international
    conference on computer vision, pp. 2980--2988.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2017）Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P., 2017. 用于密集物体检测的焦点损失，见：IEEE国际计算机视觉会议论文集，第2980--2988页。
- en: Litjens et al. (2017) Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A.,
    Ciompi, F., Ghafoorian, M., van der Laak, J.A.W.M., van Ginneken, B., Sanchez,
    C.I., 2017. A Survey on Deep Learning in Medical Image Analysis. Medical Image
    Analysis 42, 60--88.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Litjens 等 (2017) Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi,
    F., Ghafoorian, M., van der Laak, J.A.W.M., van Ginneken, B., Sanchez, C.I., 2017.
    医学图像分析中的深度学习综述。医学图像分析 42, 60--88。
- en: 'Liu et al. (2019a) Liu, C., Chen, L.C., Schroff, F., Adam, H., Hua, W., Yuille,
    A.L., Fei-Fei, L., 2019a. Auto-DeepLab: Hierarchical Neural Architecture Search
    for Semantic Image Segmentation, in: Proceedings of the 2019 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 82--92.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2019a) Liu, C., Chen, L.C., Schroff, F., Adam, H., Hua, W., Yuille,
    A.L., Fei-Fei, L., 2019a. Auto-DeepLab: 用于语义图像分割的层次神经架构搜索，载于：2019 IEEE/CVF计算机视觉与模式识别会议论文集，pp.
    82--92。'
- en: 'Liu et al. (2019b) Liu, L., Mou, L., Zhu, X.X., Mandal, M., 2019b. Skin lesion
    segmentation based on improved U-Net, in: 2019 IEEE Canadian Conference of Electrical
    and Computer Engineering (CCECE), IEEE. pp. 1--4.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2019b) Liu, L., Mou, L., Zhu, X.X., Mandal, M., 2019b. 基于改进的U-Net的皮肤病变分割，载于：2019
    IEEE加拿大电气与计算机工程会议（CCECE），IEEE。pp. 1--4。
- en: Liu et al. (2020) Liu, L., Mou, L., Zhu, X.X., Mandal, M., 2020. Automatic skin
    lesion classification based on mid-level feature learning. Computerized Medical
    Imaging and Graphics 84, 101765.
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2020) Liu, L., Mou, L., Zhu, X.X., Mandal, M., 2020. 基于中级特征学习的自动皮肤病变分类。计算机化医学成像与图形
    84, 101765。
- en: Liu et al. (2021a) Liu, L., Tsui, Y.Y., Mandal, M., 2021a. Skin lesion segmentation
    using deep learning with auxiliary task. Journal of Imaging 7, 67.
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2021a) Liu, L., Tsui, Y.Y., Mandal, M., 2021a. 使用辅助任务的深度学习皮肤病变分割。成像期刊
    7, 67。
- en: 'Liu et al. (2022a) Liu, Q., Wang, J., Zuo, M., Cao, W., Zheng, J., Zhao, H.,
    Xie, J., 2022a. NCRNet: Neighborhood context refinement network for skin lesion
    segmentation. Computers in Biology and Medicine 146, 105545.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2022a) Liu, Q., Wang, J., Zuo, M., Cao, W., Zheng, J., Zhao, H., Xie,
    J., 2022a. NCRNet: 用于皮肤病变分割的邻域上下文细化网络。生物医学与计算机 146, 105545。'
- en: 'Liu et al. (2022b) Liu, X., Fan, W., Zhou, D., 2022b. Skin lesion segmentation
    via intensive atrous spatial Transformer, in: International Conference on Wireless
    Algorithms, Systems, and Applications, Springer. pp. 15--26.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2022b) Liu, X., Fan, W., Zhou, D., 2022b. 通过密集空洞空间变换器进行皮肤病变分割，载于：无线算法、系统与应用国际会议，Springer。pp.
    15--26。
- en: 'Liu et al. (2021b) Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
    S., Guo, B., 2021b. Swin transformer: Hierarchical vision transformer using shifted
    windows, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp. 10012--10022.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2021b) Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S.,
    Guo, B., 2021b. Swin Transformer: 使用移动窗口的层次视觉变换器，载于：IEEE/CVF国际计算机视觉会议论文集，pp. 10012--10022。'
- en: 'Long et al. (2015) Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional
    networks for semantic segmentation, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 3431--3440.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等 (2015) Long, J., Shelhamer, E., Darrell, T., 2015. 用于语义分割的全卷积网络，载于：IEEE计算机视觉与模式识别会议论文集，pp.
    3431--3440。
- en: Lui et al. (2009) Lui, H., et al., 2009. DermWeb, Department of Dermatology
    and Skin Science, the University of British Columbia. [http://www.dermweb.com/](http://www.dermweb.com/).
    [Online. Accessed January 26, 2022].
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lui 等 (2009) Lui, H., 等, 2009. DermWeb, 不列颠哥伦比亚大学皮肤科与皮肤科学系。 [http://www.dermweb.com/](http://www.dermweb.com/)。
    [在线。访问日期：2022年1月26日]。
- en: Luque et al. (2020) Luque, A., Carrasco, A., Martin, A., de las Heras, A., 2020.
    The Impact of Class Imbalance in Classification Performance Metrics Based on the
    Binary Confusion Matrix. Pattern Recognition 91, 216--231.
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luque 等 (2020) Luque, A., Carrasco, A., Martin, A., de las Heras, A., 2020.
    基于二元混淆矩阵的分类性能指标中的类别不平衡影响。模式识别 91, 216--231。
- en: Ma et al. (2021) Ma, J., Chen, J., Ng, M., Huang, R., Li, Y., Li, C., Yang,
    X., Martel, A.L., 2021. Loss odyssey in medical image segmentation. Medical Image
    Analysis 71, 102035.
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等 (2021) Ma, J., Chen, J., Ng, M., Huang, R., Li, Y., Li, C., Yang, X., Martel,
    A.L., 2021. 医学图像分割中的损失奥德赛。医学图像分析 71, 102035。
- en: Mahbod et al. (2020) Mahbod, A., Tschandl, P., Langs, G., Ecker, R., Ellinger,
    I., 2020. The effects of skin lesion segmentation on the performance of dermatoscopic
    image classification. Computer Methods and Programs in Biomedicine 197, 105725.
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahbod 等 (2020) Mahbod, A., Tschandl, P., Langs, G., Ecker, R., Ellinger, I.,
    2020. 皮肤病变分割对皮肤镜图像分类性能的影响。生物医学计算方法与程序 197, 105725。
- en: 'Maier-Hein et al. (2014) Maier-Hein, L., Mersmann, S., Kondermann, D., Bodenstedt,
    S., Sanchez, A., Stock, C., Kenngott, H.G., Eisenmann, M., Speidel, S., 2014.
    Can masses of non-experts train highly accurate image classifiers?, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 438--445.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maier-Hein 等人 (2014) Maier-Hein, L., Mersmann, S., Kondermann, D., Bodenstedt,
    S., Sanchez, A., Stock, C., Kenngott, H.G., Eisenmann, M., Speidel, S., 2014.
    非专家能否训练出高度准确的图像分类器？，见：国际医学图像计算与计算机辅助干预会议，Springer。第 438--445 页。
- en: 'Marchetti et al. (2018) Marchetti, M.A., Codella, N.C.F., Dusza, S.W., Gutman,
    D.A., Helba, B., Kalloo, A., Mishra, N., Carrera, C., Celebi, M.E., DeFazio, J.L.,
    Jaimes, N., Marghoob, A.A., Quigley, E., Scope, A., Yelamos, O., Halpern, A.C.,
    2018. Results of the 2016 International Skin Imaging Collaboration International
    Symposium on Biomedical Imaging Challenge: Comparison of the Accuracy of Computer
    Algorithms to Dermatologists for the Diagnosis of Melanoma from Dermoscopic Images.
    Journal of the American Academy of Dermatology 78, 270--277.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marchetti 等人 (2018) Marchetti, M.A., Codella, N.C.F., Dusza, S.W., Gutman, D.A.,
    Helba, B., Kalloo, A., Mishra, N., Carrera, C., Celebi, M.E., DeFazio, J.L., Jaimes,
    N., Marghoob, A.A., Quigley, E., Scope, A., Yelamos, O., Halpern, A.C., 2018.
    2016年国际皮肤影像合作国际研讨会生物医学影像挑战赛结果：计算机算法与皮肤科医生在从皮肤镜图像中诊断黑色素瘤的准确性比较。《美国皮肤科医师学会杂志》78,
    270--277。
- en: 'Maron et al. (2021a) Maron, R.C., Hekler, A., Krieghoff-Henning, E., Schmitt,
    M., Schlager, J.G., Utikal, J.S., Brinker, T.J., 2021a. Reducing the impact of
    confounding factors on skin cancer classification via image segmentation: Technical
    model study. Journal of Medical Internet Research 23, e21695.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maron 等人 (2021a) Maron, R.C., Hekler, A., Krieghoff-Henning, E., Schmitt, M.,
    Schlager, J.G., Utikal, J.S., Brinker, T.J., 2021a. 通过图像分割减少混杂因素对皮肤癌分类的影响：技术模型研究。《医学互联网研究杂志》23,
    e21695。
- en: Maron et al. (2021b) Maron, R.C., Schlager, J.G., Haggenmüller, S., von Kalle,
    C., Utikal, J.S., Meier, F., Gellrich, F.F., Hobelsberger, S., Hauschild, A.,
    French, L., Heinzerling, L., Schlaak, M., Ghoreschi, K., Hilke, F.J., Poch, G.,
    Heppt, M.V., Berking, C., Haferkamp, S., Sondermann, W., Schadendorf, D., Schilling,
    B., Goebeler, M., Krieghoff-Henning, E., Hekler, A., Fröhling, S., Lipka, D.B.,
    Kather, J.N., Brinker, T.J., 2021b. A benchmark for neural network robustness
    in skin cancer classification. European Journal of Cancer 155, 191--199.
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maron 等人 (2021b) Maron, R.C., Schlager, J.G., Haggenmüller, S., von Kalle, C.,
    Utikal, J.S., Meier, F., Gellrich, F.F., Hobelsberger, S., Hauschild, A., French,
    L., Heinzerling, L., Schlaak, M., Ghoreschi, K., Hilke, F.J., Poch, G., Heppt,
    M.V., Berking, C., Haferkamp, S., Sondermann, W., Schadendorf, D., Schilling,
    B., Goebeler, M., Krieghoff-Henning, E., Hekler, A., Fröhling, S., Lipka, D.B.,
    Kather, J.N., Brinker, T.J., 2021b. 皮肤癌分类中神经网络鲁棒性的基准测试。《欧洲癌症杂志》155, 191--199。
- en: Matthews (1975) Matthews, B.W., 1975. Comparison of the Predicted and Observed
    Secondary Structure of T4 Phage Lysozyme. Biochimica et Biophysica Acta 405, 442--451.
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matthews (1975) Matthews, B.W., 1975. T4 噬菌体溶菌酶的预测与观察的二级结构比较。《生物化学与生物物理学报》405,
    442--451。
- en: 'Mendonca et al. (2013) Mendonca, T., Ferreira, P.M., Marques, J.S., Marcal,
    A.R.S., Rozeira, J., 2013. PH²---A Dermoscopic Image Database for Research and
    Benchmarking, in: Proceedings of the 35th Annual International Conference of the
    IEEE Engineering in Medicine and Biology Society, pp. 5437--5440.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mendonca 等人 (2013) Mendonca, T., Ferreira, P.M., Marques, J.S., Marcal, A.R.S.,
    Rozeira, J., 2013. PH²---用于研究和基准测试的皮肤镜图像数据库，见：第 35 届国际医学与生物工程学会年会论文集，第 5437--5440
    页。
- en: 'Mendonca et al. (2015) Mendonca, T.F., Ferreira, P.M., Marcal, A.R.S., Barata,
    C., Marques, J.S., Rocha, J., Rozeira, J., 2015. PH²---A Dermoscopic Image Database
    for Research and Benchmarking, in: Celebi, M.E., Mendonca, T., Marques, J.S. (Eds.),
    Dermoscopy Image Analysis. CRC Press, pp. 419--439.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mendonca 等人 (2015) Mendonca, T.F., Ferreira, P.M., Marcal, A.R.S., Barata, C.,
    Marques, J.S., Rocha, J., Rozeira, J., 2015. PH²---用于研究和基准测试的皮肤镜图像数据库，见：Celebi,
    M.E., Mendonca, T., Marques, J.S. (编)，皮肤镜图像分析。CRC Press，第 419--439 页。
- en: 'Menzies et al. (2003) Menzies, S.W., Crotty, K.A., Ingwar, C., McCarthy, W.H.,
    2003. An Atlas of Surface Microscopy of Pigmented Skin Lesions: Dermoscopy. Second
    ed., McGraw-Hill.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menzies 等人 (2003) Menzies, S.W., Crotty, K.A., Ingwar, C., McCarthy, W.H., 2003.
    色素性皮肤病变的表面显微镜图谱：皮肤镜检查。第二版，McGraw-Hill。
- en: Miller and Nicely (1955) Miller, G.A., Nicely, P.E., 1955. An analysis of perceptual
    confusions among some english consonants. The Journal of the Acoustical Society
    of America 27, 338--352.
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller 和 Nicely (1955) Miller, G.A., Nicely, P.E., 1955. 对一些英语辅音的感知混淆分析。《声学学会杂志》27,
    338--352。
- en: 'Mirikharaji et al. (2021) Mirikharaji, Z., Abhishek, K., Izadi, S., Hamarneh,
    G., 2021. D-LEMA: Deep learning ensembles from multiple annotations-application
    to skin lesion segmentation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 1837--1846.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirikharaji等 (2021) Mirikharaji, Z., Abhishek, K., Izadi, S., Hamarneh, G.,
    2021. D-LEMA：来自多个注释的深度学习集成——应用于皮肤病变分割，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，第1837--1846页。
- en: 'Mirikharaji and Hamarneh (2018) Mirikharaji, Z., Hamarneh, G., 2018. Star shape
    prior in fully convolutional networks for skin lesion segmentation, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 737--745.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirikharaji和Hamarneh (2018) Mirikharaji, Z., Hamarneh, G., 2018. 用于皮肤病变分割的全卷积网络中的星形先验，发表于：医学图像计算与计算机辅助手术国际会议，Springer。第737--745页。
- en: 'Mirikharaji et al. (2018) Mirikharaji, Z., Izadi, S., Kawahara, J., Hamarneh,
    G., 2018. Deep auto-context fully convolutional neural network for skin lesion
    segmentation, in: 2018 IEEE 15th International Symposium on Biomedical Imaging
    (ISBI 2018), IEEE. pp. 877--880.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirikharaji等 (2018) Mirikharaji, Z., Izadi, S., Kawahara, J., Hamarneh, G.,
    2018. 用于皮肤病变分割的深度自我上下文全卷积神经网络，发表于：2018 IEEE第15届生物医学成像国际研讨会（ISBI 2018），IEEE。第877--880页。
- en: 'Mirikharaji et al. (2019) Mirikharaji, Z., Yan, Y., Hamarneh, G., 2019. Learning
    to segment skin lesions from noisy annotations, in: Domain Adaptation and Representation
    Transfer and Medical Image Learning with Less Labels and Imperfect Data. Springer,
    pp. 207--215.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirikharaji等 (2019) Mirikharaji, Z., Yan, Y., Hamarneh, G., 2019. 从噪声注释中学习分割皮肤病变，发表于：领域适应与表示迁移及少标签和不完美数据的医学图像学习。Springer，第207--215页。
- en: Mirzaalian et al. (2016) Mirzaalian, H., Lee, T.K., Hamarneh, G., 2016. Skin
    lesion tracking using structured graphical models. Medical Image Analysis 27,
    84--92.
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirzaalian等 (2016) Mirzaalian, H., Lee, T.K., Hamarneh, G., 2016. 使用结构化图模型跟踪皮肤病变。医学图像分析
    27, 84--92。
- en: 'Mishra and Daescu (2017) Mishra, R., Daescu, O., 2017. Deep learning for skin
    lesion segmentation, in: 2017 IEEE International Conference on Bioinformatics
    and Biomedicine (BIBM), IEEE. pp. 1189--1194.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra和Daescu (2017) Mishra, R., Daescu, O., 2017. 深度学习用于皮肤病变分割，发表于：2017 IEEE生物信息学与生物医学国际会议（BIBM），IEEE。第1189--1194页。
- en: 'Nachbar et al. (1994) Nachbar, F., Stolz, W., Merkle, T., Cognetta, A.B., Vogt,
    T., Landthaler, M., Bilek, P., Braun-Falco, O., Plewig, G., 1994. The ABCD rule
    of dermatoscopy: High prospective value in the diagnosis of doubtful melanocytic
    skin lesions. Journal of the American Academy of Dermatology 30, 551--559.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nachbar等 (1994) Nachbar, F., Stolz, W., Merkle, T., Cognetta, A.B., Vogt, T.,
    Landthaler, M., Bilek, P., Braun-Falco, O., Plewig, G., 1994. 皮肤镜检查的ABCD规则：在诊断可疑的黑色素皮肤病变中的高前瞻性价值。美国皮肤病学会杂志
    30, 551--559。
- en: Nasr-Esfahani et al. (2019) Nasr-Esfahani, E., Rafiei, S., Jafari, M.H., Karimi,
    N., Wrobel, J.S., Samavi, S., Soroushmehr, S.R., 2019. Dense pooling layers in
    fully convolutional network for skin lesion segmentation. Computerized Medical
    Imaging and Graphics 78, 101658.
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nasr-Esfahani等 (2019) Nasr-Esfahani, E., Rafiei, S., Jafari, M.H., Karimi, N.,
    Wrobel, J.S., Samavi, S., Soroushmehr, S.R., 2019. 用于皮肤病变分割的全卷积网络中的密集池化层。计算机化医学成像与图形
    78, 101658。
- en: Nathan and Kansal (2020) Nathan, S., Kansal, P., 2020. Lesion net--skin lesion
    segmentation using coordinate convolution and deep residual units. arXiv preprint
    arXiv:2012.14249 .
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nathan和Kansal (2020) Nathan, S., Kansal, P., 2020. 病变网——使用坐标卷积和深度残差单元的皮肤病变分割。arXiv预印本
    arXiv:2012.14249。
- en: Navarro et al. (2018) Navarro, F., Escudero-Vinolo, M., Bescós, J., 2018. Accurate
    segmentation and registration of skin lesion images to evaluate lesion change.
    IEEE Journal of Biomedical and Health Informatics 23, 501--508.
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Navarro等 (2018) Navarro, F., Escudero-Vinolo, M., Bescós, J., 2018. 精确的皮肤病变图像分割与配准以评估病变变化。IEEE生物医学与健康信息学期刊
    23, 501--508。
- en: Ning et al. (2005) Ning, F., Delhomme, D., LeCun, Y., Piano, F., Bottou, L.,
    Barbano, P.E., 2005. Toward Automatic Phenotyping of Developing Embryos from Videos.
    IEEE Transactions on Image Processing 14, 1360--1371.
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ning等 (2005) Ning, F., Delhomme, D., LeCun, Y., Piano, F., Bottou, L., Barbano,
    P.E., 2005. 朝向从视频中自动表型的胚胎。IEEE图像处理汇刊 14, 1360--1371。
- en: Norton et al. (2012) Norton, K.A., Iyatomi, H., Celebi, M.E., Ishizaki, S.,
    Sawada, M., Suzaki, R., Kobayashi, K., Tanaka, M., Ogawa, K., 2012. Three-Phase
    General Border Detection Method for Dermoscopy Images Using Non-Uniform Illumination
    Correction. Skin Research and Technology 18, 290--300.
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Norton等 (2012) Norton, K.A., Iyatomi, H., Celebi, M.E., Ishizaki, S., Sawada,
    M., Suzaki, R., Kobayashi, K., Tanaka, M., Ogawa, K., 2012. 使用非均匀照明校正的三阶段通用边界检测方法应用于皮肤镜图像。皮肤研究与技术
    18, 290--300。
- en: 'Nosrati and Hamarneh (2016) Nosrati, M.S., Hamarneh, G., 2016. Incorporating
    prior knowledge in medical image segmentation: a survey. arXiv preprint arXiv:1607.01092
    .'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nosrati 和 Hamarneh (2016) Nosrati, M.S., Hamarneh, G., 2016. 在医学图像分割中融入先验知识：综述。arXiv
    预印本 arXiv:1607.01092。
- en: Oakley et al. (1995) Oakley, A., et al., 1995. DermNet New Zealand Trust. [https://dermnetnz.org/](https://dermnetnz.org/).
    [Online. Accessed January 26, 2022].
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oakley 等人 (1995) Oakley, A., 等人，1995. DermNet 新西兰信托。 [https://dermnetnz.org/](https://dermnetnz.org/)。
    [在线访问。访问日期：2022年1月26日]。
- en: 'Oktay et al. (2018) Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich,
    M., Misawa, K., Mori, K., McDonagh, S., Hammerla, N.Y., Kainz, B., et al., 2018.
    Attention U-Net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999
    .'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oktay 等人 (2018) Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M.,
    Misawa, K., Mori, K., McDonagh, S., Hammerla, N.Y., Kainz, B., 等人，2018. 注意力 U-Net：学习如何寻找胰腺。arXiv
    预印本 arXiv:1804.03999。
- en: Öztürk and Özkaya (2020) Öztürk, Ş., Özkaya, U., 2020. Skin lesion segmentation
    with improved convolutional neural network. Journal of digital imaging 33, 958--970.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Öztürk 和 Özkaya (2020) Öztürk, Ş., Özkaya, U., 2020. 改进的卷积神经网络用于皮肤病变分割。数字成像杂志
    33, 958--970。
- en: 'Pacheco et al. (2020) Pacheco, A.G., Lima, G.R., Salomão, A.S., Krohling, B.,
    Biral, I.P., de Angelo, G.G., Alves Jr, F.C., Esgario, J.G., Simora, A.C., Castro,
    P.B., et al., 2020. PAD-UFES-20: A skin lesion dataset composed of patient data
    and clinical images collected from smartphones. Data in Brief 32, 106221.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pacheco 等人 (2020) Pacheco, A.G., Lima, G.R., Salomão, A.S., Krohling, B., Biral,
    I.P., de Angelo, G.G., Alves Jr, F.C., Esgario, J.G., Simora, A.C., Castro, P.B.,
    等人，2020. PAD-UFES-20：一个由患者数据和从智能手机收集的临床图像组成的皮肤病变数据集。Data in Brief 32, 106221。
- en: 'Pakzad et al. (2023) Pakzad, A., Abhishek, K., Hamarneh, G., 2023. CIRCLe:
    Color invariant representation learning for unbiased classification of skin lesions,
    in: Computer Vision--ECCV 2022 Workshops: Tel Aviv, Israel, October 23--27, 2022,
    Proceedings, Part IV, Springer. pp. 203--219.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pakzad 等人 (2023) Pakzad, A., Abhishek, K., Hamarneh, G., 2023. CIRCLe: 颜色不变表征学习用于无偏分类皮肤病变，见：计算机视觉--ECCV
    2022 研讨会：以色列特拉维夫，2022年10月23--27日，会议论文集，第IV部分，Springer。第203--219页。'
- en: 'Papadopoulos et al. (2017) Papadopoulos, D.P., Uijlings, J.R., Keller, F.,
    Ferrari, V., 2017. Extreme clicking for efficient object annotation, in: Proceedings
    of the IEEE International Conference on Computer Vision, pp. 4930--4939.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papadopoulos 等人 (2017) Papadopoulos, D.P., Uijlings, J.R., Keller, F., Ferrari,
    V., 2017. 极端点击以实现高效对象标注，见：IEEE 国际计算机视觉会议论文集，第4930--4939页。
- en: 'Papandreou et al. (2015) Papandreou, G., Chen, L.C., Murphy, K.P., Yuille,
    A.L., 2015. Weakly- and Semi-Supervised Learning of a Deep Convolutional Network
    for Semantic Image Segmentation, in: Proceedings of the IEEE International Conference
    on Computer Vision, pp. 1742--1750.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papandreou 等人 (2015) Papandreou, G., Chen, L.C., Murphy, K.P., Yuille, A.L.,
    2015. 对深度卷积网络进行弱监督和半监督学习以进行语义图像分割，见：IEEE 国际计算机视觉会议论文集，第1742--1750页。
- en: 'Parmar et al. (2018) Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer,
    N., Ku, A., Tran, D., 2018. Image transformer, in: International Conference on
    Machine Learning, PMLR. pp. 4055--4064.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parmar 等人 (2018) Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer,
    N., Ku, A., Tran, D., 2018. 图像变换器，见：国际机器学习会议，PMLR。第4055--4064页。
- en: Pearson (1904) Pearson, K., 1904. On the theory of contingency and its relation
    to association and normal correlation. volume 1. Dulau and Company London, UK.
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearson (1904) Pearson, K., 1904. 关于偶然性理论及其与关联和正常相关性的关系。第1卷。Dulau and Company
    伦敦，英国。
- en: Peng and Li (2013) Peng, B., Li, T., 2013. A Probabilistic Measure for Quantitative
    Evaluation of Image Segmentation. IEEE Signal Processing Letters 20, 689--692.
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 和 Li (2013) Peng, B., Li, T., 2013. 一种用于图像分割定量评估的概率测量。IEEE 信号处理通讯 20, 689--692。
- en: Peng et al. (2016) Peng, B., Wang, X., Yang, Y., 2016. Region Based Exemplar
    References for Image Segmentation Evaluation. IEEE Signal Processing Letters 23,
    459--462.
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人 (2016) Peng, B., Wang, X., Yang, Y., 2016. 基于区域的示例参考用于图像分割评估。IEEE 信号处理通讯
    23, 459--462。
- en: Peng et al. (2017a) Peng, B., Zhang, L., Mou, X., Yang, M.H., 2017a. Evaluation
    of Segmentation Quality via Adaptive Composition of Reference Segmentations. IEEE
    Transactions on Pattern Analysis and Machine Intelligence 39, 1929--1941.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人 (2017a) Peng, B., Zhang, L., Mou, X., Yang, M.H., 2017a. 通过参考分割的自适应组合评估分割质量。IEEE
    模式分析与机器智能汇刊 39, 1929--1941。
- en: 'Peng et al. (2017b) Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J., 2017b. Large
    kernel matters--improve semantic segmentation by global convolutional network,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 4353--4361.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等 (2017b) Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J., 2017b. 大核的重要性--通过全局卷积网络改善语义分割，见：IEEE计算机视觉与模式识别会议论文集，pp.
    4353--4361。
- en: Peng et al. (2019) Peng, Y., Wang, N., Wang, Y., Wang, M., 2019. Segmentation
    of dermoscopy image using adversarial networks. Multimedia Tools and Applications
    78, 10965--10981.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等 (2019) Peng, Y., Wang, N., Wang, Y., Wang, M., 2019. 使用对抗网络的皮肤镜图像分割。多媒体工具与应用
    78, 10965--10981。
- en: 'Perez et al. (2018) Perez, F., Vasconcelos, C., Avila, S., Valle, E., 2018.
    Data Augmentation for Skin Lesion Analysis, in: Proceedings of the Third ISIC
    Workshop on Skin Image Analysis, pp. 303--311.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 等 (2018) Perez, F., Vasconcelos, C., Avila, S., Valle, E., 2018. 皮肤病变分析的数据增强，见：第三届ISIC皮肤图像分析研讨会论文集，pp.
    303--311。
- en: Peserico and Silletti (2010) Peserico, E., Silletti, A., 2010. Is (N)PRI Suitable
    for Evaluating Automated Segmentation of Cutaneous Lesions? Pattern Recognition
    Letters 31, 2464--2467.
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peserico 和 Silletti (2010) Peserico, E., Silletti, A., 2010. (N)PRI 是否适用于评估皮肤病变自动分割？模式识别信函
    31, 2464--2467。
- en: 'Pinheiro and Collobert (2014) Pinheiro, P.H., Collobert, R., 2014. Recurrent
    convolutional neural networks for scene labeling, in: 31st International Conference
    on Machine Learning (ICML), PMLR. pp. 82--90.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinheiro 和 Collobert (2014) Pinheiro, P.H., Collobert, R., 2014. 用于场景标注的递归卷积神经网络，见：第31届国际机器学习会议
    (ICML)，PMLR。pp. 82--90。
- en: 'Pollastri et al. (2018) Pollastri, F., Bolelli, F., Palacios, R.P., Grana,
    C., 2018. Improving skin lesion segmentation with generative adversarial networks,
    in: 2018 IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS),
    IEEE. pp. 442--443.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pollastri 等 (2018) Pollastri, F., Bolelli, F., Palacios, R.P., Grana, C., 2018.
    使用生成对抗网络改善皮肤病变分割，见：2018年IEEE第31届计算机医学系统国际研讨会 (CBMS)，IEEE。pp. 442--443。
- en: Pollastri et al. (2020) Pollastri, F., Bolelli, F., Paredes, R., Grana, C.,
    2020. Augmenting Data with GANs to Segment Melanoma Skin Lesions. Multimedia Tools
    and Applications 79, 15575--15592.
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pollastri 等 (2020) Pollastri, F., Bolelli, F., Paredes, R., Grana, C., 2020.
    使用GANs增强数据以分割黑色素瘤皮损。多媒体工具与应用 79, 15575--15592。
- en: Poudel and Lee (2021) Poudel, S., Lee, S.W., 2021. Deep multi-scale attentional
    features for medical image segmentation. Applied Soft Computing 109, 107445.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poudel 和 Lee (2021) Poudel, S., Lee, S.W., 2021. 用于医学图像分割的深度多尺度注意力特征。应用软计算 109,
    107445。
- en: Pour and Seker (2020) Pour, M.P., Seker, H., 2020. Transform Domain Representation-Driven
    Convolutional Neural Networks for Skin Lesion Segmentation. Expert Systems with
    Applications 144, 113129.
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pour 和 Seker (2020) Pour, M.P., Seker, H., 2020. 基于变换域表示的卷积神经网络用于皮肤病变分割。专家系统与应用
    144, 113129。
- en: Qiu et al. (2020) Qiu, Y., Cai, J., Qin, X., Zhang, J., 2020. Inferring skin
    lesion deep convolutional neural networks. IEEE Access 8, 144246--144258.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等 (2020) Qiu, Y., Cai, J., Qin, X., Zhang, J., 2020. 推断皮肤病变的深度卷积神经网络。IEEE
    Access 8, 144246--144258。
- en: Rajchl et al. (2016) Rajchl, M., Lee, M.C., Schrans, F., Davidson, A., Passerat-Palmbach,
    J., Tarroni, G., Alansary, A., Oktay, O., Kainz, B., Rueckert, D., 2016. Learning
    under distributed weak supervision. arXiv preprint arXiv:1606.01100 .
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajchl 等 (2016) Rajchl, M., Lee, M.C., Schrans, F., Davidson, A., Passerat-Palmbach,
    J., Tarroni, G., Alansary, A., Oktay, O., Kainz, B., Rueckert, D., 2016. 在分布式弱监督下学习。arXiv预印本
    arXiv:1606.01100。
- en: 'Ramachandram and DeVries (2017) Ramachandram, D., DeVries, T., 2017. Lesionseg:
    semantic segmentation of skin lesions using deep convolutional neural network.
    arXiv preprint arXiv:1703.03372 .'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramachandram 和 DeVries (2017) Ramachandram, D., DeVries, T., 2017. Lesionseg:
    使用深度卷积神经网络进行皮损的语义分割。arXiv预印本 arXiv:1703.03372。'
- en: Ramachandram and Taylor (2017) Ramachandram, D., Taylor, G.W., 2017. Skin lesion
    segmentation using deep hypercolumn descriptors. Journal of Computational Vision
    and Imaging Systems 3.
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandram 和 Taylor (2017) Ramachandram, D., Taylor, G.W., 2017. 使用深度超列描述符进行皮肤病变分割。计算机视觉与成像系统杂志
    3。
- en: Ramachandran et al. (2019) Ramachandran, P., Parmar, N., Vaswani, A., Bello,
    I., Levskaya, A., Shlens, J., 2019. Stand-alone self-attention in vision models.
    Advances in Neural Information Processing Systems 32.
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandran 等 (2019) Ramachandran, P., Parmar, N., Vaswani, A., Bello, I.,
    Levskaya, A., Shlens, J., 2019. 视觉模型中的独立自注意力。神经信息处理系统进展 32。
- en: Ramadan et al. (2022) Ramadan, R., Aly, S., Abdel-Atty, M., 2022. Color-invariant
    skin lesion semantic segmentation based on modified U-Net deep convolutional neural
    network. Health Information Science and Systems 10, 1--12.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramadan 等人 (2022) Ramadan, R., Aly, S., Abdel-Atty, M., 2022. 基于修改版 U-Net 深度卷积神经网络的颜色不变皮肤病变语义分割。健康信息科学与系统
    10, 1--12。
- en: 'Ramani and Ranjani (2019) Ramani, D.R., Ranjani, S.S., 2019. U-net based segmentation
    and multiple feature extraction of dermascopic images for efficient diagnosis
    of melanoma, in: Computer Aided Intervention and Diagnostics in Clinical and Medical
    Images, pp. 81--101.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramani 和 Ranjani (2019) Ramani, D.R., Ranjani, S.S., 2019. 基于 U-net 的皮肤镜图像分割及多特征提取，用于高效的黑色素瘤诊断，发表于：计算机辅助干预与临床医学图像诊断，页码
    81--101。
- en: Rand (1971) Rand, W.M., 1971. Objective Criteria for the Evaluation of Clustering
    Methods. Journal of the American Statistical Association 66, 846--850.
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rand (1971) Rand, W.M., 1971. 聚类方法评价的客观标准。美国统计协会杂志 66, 846--850。
- en: 'Ranftl et al. (2021) Ranftl, R., Bochkovskiy, A., Koltun, V., 2021. Vision
    transformers for dense prediction, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 12179--12188.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ranftl 等人 (2021) Ranftl, R., Bochkovskiy, A., Koltun, V., 2021. 用于密集预测的视觉变换器，发表于：IEEE/CVF
    国际计算机视觉会议论文集，页码 12179--12188。
- en: 'Redekop and Chernyavskiy (2021) Redekop, E., Chernyavskiy, A., 2021. Uncertainty-based
    method for improving poorly labeled segmentation datasets, in: 2021 IEEE 18th
    International Symposium on Biomedical Imaging (ISBI), IEEE. pp. 1831--1835.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redekop 和 Chernyavskiy (2021) Redekop, E., Chernyavskiy, A., 2021. 基于不确定性的方法来改善标签不佳的分割数据集，发表于：2021
    IEEE 第18届生物医学成像国际研讨会 (ISBI)，IEEE，页码 1831--1835。
- en: 'Redmon et al. (2016) Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016.
    You only look once: Unified, real-time object detection, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, pp. 779--788.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redmon 等人 (2016) Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. 你只看一次：统一的实时目标检测，发表于：IEEE
    计算机视觉与模式识别会议论文集，页码 779--788。
- en: 'Ren et al. (2015) Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster R-CNN:
    Towards real-time object detection with region proposal networks. Advances in
    Neural Information Processing Systems 28.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人 (2015) Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster R-CNN：基于区域提议网络的实时目标检测。神经信息处理系统进展
    28。
- en: Ren et al. (2021) Ren, Y., Yu, L., Tian, S., Cheng, J., Guo, Z., Zhang, Y.,
    2021. Serial attention network for skin lesion segmentation. Journal of Ambient
    Intelligence and Humanized Computing , 1--12.
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人 (2021) Ren, Y., Yu, L., Tian, S., Cheng, J., Guo, Z., Zhang, Y., 2021.
    用于皮肤病变分割的序列注意力网络。环境智能与人性化计算杂志，1--12。
- en: Renard et al. (2020) Renard, F., Guedria, S., Palma, N.D., Vuillerme, N., 2020.
    Variability and reproducibility in deep learning for medical image segmentation.
    Scientific Reports 10, 1--16.
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Renard 等人 (2020) Renard, F., Guedria, S., Palma, N.D., Vuillerme, N., 2020.
    深度学习在医学图像分割中的变异性和可重复性。科学报告 10, 1--16。
- en: 'Ribeiro et al. (2020) Ribeiro, V., Avila, S., Valle, E., 2020. Less is more:
    Sample selection and label conditioning improve skin lesion segmentation, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops, pp. 738--739.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro 等人 (2020) Ribeiro, V., Avila, S., Valle, E., 2020. 少即是多：样本选择和标签条件改善皮肤病变分割，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，页码 738--739。
- en: Rohlfing and Maurer (2006) Rohlfing, T., Maurer, C.R., 2006. Shape-Based Averaging.
    IEEE Transactions on Image Processing 16, 153--161.
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rohlfing 和 Maurer (2006) Rohlfing, T., Maurer, C.R., 2006. 基于形状的平均。IEEE 图像处理汇刊
    16, 153--161。
- en: 'Ronneberger et al. (2015) Ronneberger, O., Fischer, P., Brox, T., 2015. U-Net:
    Convolutional networks for biomedical image segmentation, in: International Conference
    on Medical image computing and computer-assisted intervention, pp. 234--241.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ronneberger 等人 (2015) Ronneberger, O., Fischer, P., Brox, T., 2015. U-Net：用于生物医学图像分割的卷积网络，发表于：医学图像计算与计算机辅助手术国际会议，页码
    234--241。
- en: 'Ross-Howe and Tizhoosh (2018) Ross-Howe, S., Tizhoosh, H.R., 2018. The effects
    of image pre-and post-processing, wavelet decomposition, and local binary patterns
    on u-nets for skin lesion segmentation, in: 2018 International Joint Conference
    on Neural Networks (IJCNN), pp. 1--8.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross-Howe 和 Tizhoosh (2018) Ross-Howe, S., Tizhoosh, H.R., 2018. 图像预处理和后处理、波形分解以及局部二值模式对
    U-net 皮肤病变分割的影响，发表于：2018 国际神经网络联合会议 (IJCNN)，页码 1--8。
- en: Rotemberg et al. (2021) Rotemberg, V., Kurtansky, N., Betz-Stablein, B., Caffery,
    L., Chousakos, E., Codella, N., Combalia, M., Dusza, S., Guitera, P., Gutman,
    D., Halpern, A., Helba, B., Kittler, H., Kose, K., Langer, S., Lioprys, K., Malvehy,
    J., Musthaq, S., Nanda, J., Reiter, O., Shih, G., Stratigos, A., Tschandl, P.,
    Weber, J., Soyer, H.P., 2021. A Patient-Centric Dataset of Images and Metadata
    for Identifying Melanomas Using Clinical Context. Scientific Data 8, 34.
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rotemberg等（2021）Rotemberg, V., Kurtansky, N., Betz-Stablein, B., Caffery, L.,
    Chousakos, E., Codella, N., Combalia, M., Dusza, S., Guitera, P., Gutman, D.,
    Halpern, A., Helba, B., Kittler, H., Kose, K., Langer, S., Lioprys, K., Malvehy,
    J., Musthaq, S., Nanda, J., Reiter, O., Shih, G., Stratigos, A., Tschandl, P.,
    Weber, J., Soyer, H.P., 2021. 一个以患者为中心的图像和元数据数据集，用于通过临床背景识别黑色素瘤。科学数据 8, 34。
- en: 'Roth et al. (2021) Roth, H.R., Yang, D., Xu, Z., Wang, X., Xu, D., 2021. Going
    to extremes: Weakly supervised medical image segmentation. Machine Learning and
    Knowledge Extraction 3, 507--524.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roth等（2021）Roth, H.R., Yang, D., Xu, Z., Wang, X., Xu, D., 2021. 极端情况：弱监督医学图像分割。机器学习与知识提取
    3, 507--524。
- en: Rother et al. (2004) Rother, C., Kolmogorov, V., Blake, A., 2004. "GrabCut"
    interactive foreground extraction using iterated graph cuts. ACM Transactions
    on Graphics (TOG) 23, 309--314.
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rother等（2004）Rother, C., Kolmogorov, V., Blake, A., 2004. "GrabCut" 交互式前景提取使用迭代图割方法。ACM图形学会会刊（TOG）23,
    309--314。
- en: Rumelhart et al. (1986) Rumelhart, D.E., Hinton, G.E., Williams, R.J., 1986.
    Learning Representations by Back-Propagating Errors. Nature 323, 533--536.
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart等（1986）Rumelhart, D.E., Hinton, G.E., Williams, R.J., 1986. 通过反向传播误差学习表征。自然
    323, 533--536。
- en: 'Saba et al. (2019) Saba, T., Khan, M.A., Rehman, A., Marie-Sainte, S.L., 2019.
    Region Extraction and Classification of Skin Cancer: A Heterogeneous Framework
    of Deep CNN Features Fusion and Reduction. Journal of Medical Systems 43, 289.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saba等（2019）Saba, T., Khan, M.A., Rehman, A., Marie-Sainte, S.L., 2019. 皮肤癌区域提取与分类：深度CNN特征融合与降维的异质框架。医学系统杂志
    43, 289。
- en: 'Sachin et al. (2021) Sachin, T.S., Sowmya, V., Soman, K., 2021. Performance
    analysis of deep learning models for biomedical image segmentation, in: Deep Learning
    for Biomedical Applications. CRC Press, pp. 83--100.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sachin等（2021）Sachin, T.S., Sowmya, V., Soman, K., 2021. 深度学习模型在生物医学图像分割中的性能分析，见：生物医学应用中的深度学习。CRC出版社，第83--100页。
- en: 'Sagi and Rokach (2018) Sagi, O., Rokach, L., 2018. Ensemble learning: A survey.
    Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8, e1249.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sagi和Rokach（2018）Sagi, O., Rokach, L., 2018. 集成学习：一项调查。Wiley跨学科评论：数据挖掘与知识发现
    8, e1249。
- en: 'Saha et al. (2020) Saha, A., Prasad, P., Thabit, A., 2020. Leveraging adaptive
    color augmentation in convolutional neural networks for deep skin lesion segmentation,
    in: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), IEEE.
    pp. 2014--2017.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saha等（2020）Saha, A., Prasad, P., Thabit, A., 2020. 利用卷积神经网络中的自适应颜色增强进行深度皮肤病变分割，见：2020年IEEE第17届生物医学成像国际研讨会（ISBI），IEEE，第2014--2017页。
- en: Şahin et al. (2021) Şahin, N., Alpaslan, N., Hanbay, D., 2021. Robust optimization
    of SegNet hyperparameters for skin lesion segmentation. Multimedia Tools and Applications
    , 1--21.
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Şahin等（2021）Şahin, N., Alpaslan, N., Hanbay, D., 2021. SegNet超参数的鲁棒优化用于皮肤病变分割。多媒体工具与应用，1--21。
- en: 'Saini et al. (2019) Saini, S., Gupta, D., Tiwari, A.K., 2019. Detector-segmentor
    network for skin lesion localization and segmentation, in: National Conference
    on Computer Vision, Pattern Recognition, Image Processing, and Graphics, Springer.
    pp. 589--599.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saini等（2019）Saini, S., Gupta, D., Tiwari, A.K., 2019. 检测器-分割器网络用于皮肤病变定位与分割，见：计算机视觉、模式识别、图像处理与图形国家会议，Springer，第589--599页。
- en: 'Saini et al. (2021) Saini, S., Jeon, Y.S., Feng, M., 2021. B-segnet: branched-segmentor
    network for skin lesion segmentation, in: Proceedings of the Conference on Health,
    Inference, and Learning, pp. 214--221.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saini等（2021）Saini, S., Jeon, Y.S., Feng, M., 2021. B-segnet：用于皮肤病变分割的分支分割网络，见：健康、推理和学习会议论文集，第214--221页。
- en: 'Sarker et al. (2019) Sarker, M., Kamal, M., Rashwan, H.A., Abdel-Nasser, M.,
    Singh, V.K., Banu, S.F., Akram, F., Chowdhury, F.U., Choudhury, K.A., Chambon,
    S., et al., 2019. MobileGAN: Skin lesion segmentation using a lightweight generative
    adversarial network. arXiv preprint arXiv:1907.00856 .'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarker等（2019）Sarker, M., Kamal, M., Rashwan, H.A., Abdel-Nasser, M., Singh,
    V.K., Banu, S.F., Akram, F., Chowdhury, F.U., Choudhury, K.A., Chambon, S., 等，2019.
    MobileGAN：使用轻量级生成对抗网络进行皮肤病变分割。arXiv预印本 arXiv:1907.00856。
- en: 'Sarker et al. (2018) Sarker, M.M.K., Rashwan, H.A., Akram, F., Banu, S.F.,
    Saleh, A., Singh, V.K., Chowdhury, F.U., Abdulwahab, S., Romani, S., Radeva, P.,
    et al., 2018. SLSDeep: Skin lesion segmentation based on dilated residual and
    pyramid pooling networks, in: International Conference on Medical Image Computing
    and Computer-Assisted Intervention, Springer. pp. 21--29.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarker 等人（2018）Sarker, M.M.K., Rashwan, H.A., Akram, F., Banu, S.F., Saleh,
    A., Singh, V.K., Chowdhury, F.U., Abdulwahab, S., Romani, S., Radeva, P., 等人，2018。SLSDeep：基于膨胀残差和金字塔池化网络的皮肤病变分割，收录于：医学图像计算与计算机辅助手术国际会议，Springer。pp.
    21--29。
- en: 'Sarker et al. (2021) Sarker, M.M.K., Rashwan, H.A., Akram, F., Singh, V.K.,
    Banu, S.F., Chowdhury, F.U., Choudhury, K.A., Chambon, S., Radeva, P., Puig, D.,
    et al., 2021. SLSNet: Skin lesion segmentation using a lightweight generative
    adversarial network. Expert Systems with Applications 183, 115433.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarker 等人（2021）Sarker, M.M.K., Rashwan, H.A., Akram, F., Singh, V.K., Banu,
    S.F., Chowdhury, F.U., Choudhury, K.A., Chambon, S., Radeva, P., Puig, D., 等人，2021。SLSNet：一种使用轻量级生成对抗网络的皮肤病变分割。专家系统与应用
    183, 115433。
- en: Schaefer et al. (2011) Schaefer, G., Rajab, M.I., Celebi, M.E., Iyatomi, H.,
    2011. Colour and Contrast Enhancement for Improved Skin Lesion Segmentation. Computerized
    Medical Imaging and Graphics 35, 99--104.
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaefer 等人（2011）Schaefer, G., Rajab, M.I., Celebi, M.E., Iyatomi, H., 2011.
    改进皮肤病变分割的颜色与对比度增强。计算机化医学成像与图形 35, 99--104。
- en: 'Shahin et al. (2019) Shahin, A.H., Amer, K., Elattar, M.A., 2019. Deep convolutional
    encoder-decoders with aggregated multi-resolution skip connections for skin lesion
    segmentation, in: 2019 IEEE 16th International Symposium on Biomedical Imaging
    (ISBI 2019), IEEE. pp. 451--454.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shahin 等人（2019）Shahin, A.H., Amer, K., Elattar, M.A., 2019. 具有聚合多分辨率跳跃连接的深度卷积编码解码器用于皮肤病变分割，收录于：2019
    IEEE第16届国际生物医学成像研讨会（ISBI 2019），IEEE。pp. 451--454。
- en: 'Shamshad et al. (2022) Shamshad, F., Khan, S., Zamir, S.W., Khan, M.H., Hayat,
    M., Khan, F.S., Fu, H., 2022. Transformers in medical imaging: A survey. arXiv
    preprint arXiv:2201.09873 .'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shamshad 等人（2022）Shamshad, F., Khan, S., Zamir, S.W., Khan, M.H., Hayat, M.,
    Khan, F.S., Fu, H., 2022. 医学成像中的变压器：一项调查。arXiv 预印本 arXiv:2201.09873。
- en: 'Shamsolmoali et al. (2021) Shamsolmoali, P., Zareapoor, M., Granger, E., Zhou,
    H., Wang, R., Celebi, M.E., Yang, J., 2021. Image Synthesis with Adversarial Networks:
    A Comprehensive Survey and Case Studies. Information Fusion 72, 126--146.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shamsolmoali 等人（2021）Shamsolmoali, P., Zareapoor, M., Granger, E., Zhou, H.,
    Wang, R., Celebi, M.E., Yang, J., 2021. 图像合成与对抗网络：综合调查与案例研究。信息融合 72, 126--146。
- en: 'Sharma et al. (2017) Sharma, M., Saha, O., Sriraman, A., Hebbalaguppe, R.,
    Vig, L., Karande, S., 2017. Crowdsourcing for Chromosome Segmentation and Deep
    Classification, in: Proceedings of the IEEE Conference on Computer vision and
    Pattern Recognition Workshops, pp. 786--793.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人（2017）Sharma, M., Saha, O., Sriraman, A., Hebbalaguppe, R., Vig, L.,
    Karande, S., 2017. 基于众包的染色体分割与深度分类，收录于：IEEE计算机视觉与模式识别会议论文集，pp. 786--793。
- en: Shimizu et al. (2015) Shimizu, K., Iyatomi, H., Celebi, M.E., Norton, K.A.,
    Tanaka, M., 2015. Four-Class Classification of Skin Lesions with Task Decomposition
    Strategy. IEEE Transactions on Biomedical Engineering 62, 274--283.
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shimizu 等人（2015）Shimizu, K., Iyatomi, H., Celebi, M.E., Norton, K.A., Tanaka,
    M., 2015. 皮肤病变的四类分类与任务分解策略。IEEE生物医学工程学报 62, 274--283。
- en: Shorten and Khoshgoftaar (2019) Shorten, C., Khoshgoftaar, T.M., 2019. A Survey
    on Image Data Augmentation for Deep Learning. Journal of Big Data 6, 60.
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shorten 和 Khoshgoftaar（2019）Shorten, C., Khoshgoftaar, T.M., 2019. 关于深度学习图像数据增强的调查。大数据期刊
    6, 60。
- en: 'Siegel et al. (2023) Siegel, R.L., Miller, K.D., Wagle, N.S., Jemal, A., 2023.
    Cancer statistics, 2023. CA: A Cancer Journal for Clinicians 73, 17--48. URL:
    [https://doi.org/10.3322/caac.21763](https://doi.org/10.3322/caac.21763), doi:[10.3322/caac.21763](http://dx.doi.org/10.3322/caac.21763).'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Siegel 等人（2023）Siegel, R.L., Miller, K.D., Wagle, N.S., Jemal, A., 2023. 癌症统计数据，2023。CA:
    临床医生癌症期刊 73, 17--48。网址：[https://doi.org/10.3322/caac.21763](https://doi.org/10.3322/caac.21763)，doi:[10.3322/caac.21763](http://dx.doi.org/10.3322/caac.21763)。'
- en: Silveira et al. (2009) Silveira, M., Nascimento, J.C., Marques, J.S., Marcal,
    A.R.S., Mendonca, T., Yamauchi, S., Maeda, J., Rozeira, J., 2009. Comparison of
    Segmentation Methods for Melanoma Diagnosis in Dermoscopy Images. IEEE Journal
    of Selected Topics in Signal Processing 3, 35--45.
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silveira 等人（2009）Silveira, M., Nascimento, J.C., Marques, J.S., Marcal, A.R.S.,
    Mendonca, T., Yamauchi, S., Maeda, J., Rozeira, J., 2009. 针对皮肤黑色素瘤诊断的分割方法比较。IEEE信号处理学会精选主题期刊
    3, 35--45。
- en: Simonyan and Zisserman (2014) Simonyan, K., Zisserman, A., 2014. Very deep convolutional
    networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 .
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman（2014）Simonyan, K., Zisserman, A., 2014. 大规模图像识别的非常深卷积网络。arXiv
    预印本 arXiv:1409.1556。
- en: Singh et al. (2023) Singh, L., Janghel, R.R., Sahu, S.P., 2023. An empirical
    review on evaluating the impact of image segmentation on the classification performance
    for skin lesion detection. IETE Technical Review 40, 190--201.
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人（2023）Singh, L., Janghel, R.R., Sahu, S.P., 2023. 评估图像分割对皮肤病变检测分类性能影响的实证回顾。IETE技术评论
    40, 190--201。
- en: 'Singh et al. (2019) Singh, V.K., Abdel-Nasser, M., Rashwan, H.A., Akram, F.,
    Pandey, N., Lalande, A., Presles, B., Romani, S., Puig, D., 2019. FCA-Net: Adversarial
    learning for skin lesion segmentation based on multi-scale features and factorized
    channel attention. IEEE Access 7, 130552--130565.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人（2019）Singh, V.K., Abdel-Nasser, M., Rashwan, H.A., Akram, F., Pandey,
    N., Lalande, A., Presles, B., Romani, S., Puig, D., 2019. FCA-Net：基于多尺度特征和因子化通道注意力的皮肤病变分割对抗学习。IEEE
    Access 7, 130552--130565。
- en: 'Sinha et al. (2023) Sinha, A., Kawahara, J., Pakzad, A., Abhishek, K., Ruthven,
    M., Ghorbel, E., Kacem, A., Aouada, D., Hamarneh, G., 2023. DermSynth3D: Synthesis
    of in-the-wild annotated dermatology images. arXiv preprint arXiv:2305.12621 .'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha 等人（2023）Sinha, A., Kawahara, J., Pakzad, A., Abhishek, K., Ruthven, M.,
    Ghorbel, E., Kacem, A., Aouada, D., Hamarneh, G., 2023. DermSynth3D：野外注释皮肤病图像的合成。arXiv预印本
    arXiv:2305.12621。
- en: 'Smyth et al. (1995) Smyth, P., Fayyad, U.M., Burl, M.C., Perona, P., Baldi,
    P., 1995. Inferring Ground Truth from Subjective Labelling of Venus Images, in:
    Advances in Neural Information Processing Systems, pp. 1085--1092.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smyth 等人（1995）Smyth, P., Fayyad, U.M., Burl, M.C., Perona, P., Baldi, P., 1995.
    从主观标记的金星图像推断真实情况，发表于：神经信息处理系统进展，第1085--1092页。
- en: Soenksen et al. (2021) Soenksen, L.R., Kassis, T., Conover, S.T., Marti-Fuster,
    B., Birkenfeld, J.S., Tucker-Schwartz, J., Naseem, A., Stavert, R.R., Kim, C.C.,
    Senna, M.M., Avilés-Izquierdo, J., Collins, J.J., Barzilay, R., Gray, M.L., 2021.
    Using deep learning for dermatologist-level detection of suspicious pigmented
    skin lesions from wide-field images. Science Translational Medicine 13, eabb3652.
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soenksen 等人（2021）Soenksen, L.R., Kassis, T., Conover, S.T., Marti-Fuster, B.,
    Birkenfeld, J.S., Tucker-Schwartz, J., Naseem, A., Stavert, R.R., Kim, C.C., Senna,
    M.M., Avilés-Izquierdo, J., Collins, J.J., Barzilay, R., Gray, M.L., 2021. 使用深度学习从宽视场图像中检测可疑色素皮肤病变，以达到皮肤科医生级别的检测。科学转化医学
    13, eabb3652。
- en: 'Song et al. (2019) Song, L., Lin, J., Wang, Z.J., Wang, H., 2019. Dense-residual
    attention network for skin lesion segmentation, in: International Workshop on
    Machine Learning in Medical Imaging, Springer. pp. 319--327.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人（2019）Song, L., Lin, J., Wang, Z.J., Wang, H., 2019. 用于皮肤病变分割的密集残差注意力网络，发表于：医学成像中的机器学习国际研讨会，Springer出版社。第319--327页。
- en: Sørensen (1948) Sørensen, T.A., 1948. A method of establishing groups of equal
    amplitude in plant sociology based on similarity of species content and its application
    to analyses of the vegetation on Danish commons. Biol. Skar. 5, 1--34.
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sørensen（1948）Sørensen, T.A., 1948. 一种在植物社会学中建立相等振幅组的方法，基于物种内容的相似性及其在丹麦公共草地植被分析中的应用。生物学杂志
    5, 1--34。
- en: Soudani and Barhoumi (2019) Soudani, A., Barhoumi, W., 2019. An image-based
    segmentation recommender using crowdsourcing and transfer learning for skin lesion
    extraction. Expert Systems with Applications 118, 400--410.
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soudani 和 Barhoumi（2019）Soudani, A., Barhoumi, W., 2019. 基于图像的分割推荐系统，利用众包和迁移学习进行皮肤病变提取。专家系统与应用
    118, 400--410。
- en: 'Strudel et al. (2021) Strudel, R., Garcia, R., Laptev, I., Schmid, C., 2021.
    Segmenter: Transformer for semantic segmentation, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp. 7262--7272.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strudel 等人（2021）Strudel, R., Garcia, R., Laptev, I., Schmid, C., 2021. Segmenter：用于语义分割的变换器，发表于：IEEE/CVF国际计算机视觉会议论文集，第7262--7272页。
- en: 'Sun et al. (2017) Sun, C., Shrivastava, A., Singh, S., Gupta, A., 2017. Revisiting
    unreasonable effectiveness of data in deep learning era, in: Proceedings of the
    IEEE International Conference on Computer Vision, pp. 843--852.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2017）Sun, C., Shrivastava, A., Singh, S., Gupta, A., 2017. 重新审视数据在深度学习时代的不合理有效性，发表于：IEEE国际计算机视觉会议论文集，第843--852页。
- en: 'Sun et al. (2016) Sun, X., Yang, J., Sun, M., Wang, K., 2016. A benchmark for
    automatic visual classification of clinical skin disease images, in: European
    Conference on Computer Vision, Springer. pp. 206--222.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2016）Sun, X., Yang, J., Sun, M., Wang, K., 2016. 临床皮肤疾病图像自动视觉分类的基准，发表于：欧洲计算机视觉会议，Springer出版社。第206--222页。
- en: 'Taghanaki et al. (2019) Taghanaki, S.A., Abhishek, K., Hamarneh, G., 2019.
    Improved inference via deep input transfer, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer. pp. 819--827.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taghanaki 等人（2019）Taghanaki, S.A., Abhishek, K., Hamarneh, G., 2019. 通过深度输入转移改进推断，发表于：国际医学图像计算与计算机辅助干预会议，Springer出版社。第819--827页。
- en: 'Taha and Hanbury (2015) Taha, A.A., Hanbury, A., 2015. Metrics for Evaluating
    3D Medical Image Segmentation: Analysis, Selection, and Tool. BMC Medical Imaging
    15, 29.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taha和Hanbury（2015）Taha, A.A., Hanbury, A., 2015. 评估3D医学图像分割的度量：分析、选择和工具。BMC医学成像
    15, 29。
- en: 'Tajbakhsh et al. (2020) Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J.N.,
    Wu, Z., Ding, X., 2020. Embracing imperfect datasets: A review of deep learning
    solutions for medical image segmentation. Medical Image Analysis 63, 101693.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tajbakhsh等人（2020）Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J.N., Wu, Z.,
    Ding, X., 2020. 接受不完美的数据集：医学图像分割深度学习解决方案的综述。医学图像分析 63, 101693。
- en: 'Tan et al. (2019a) Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M.,
    Howard, A., Le, Q.V., 2019a. MnasNet: Platform-Aware Neural Architecture Search
    for Mobile, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 2820--2828.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan等人（2019a）Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard,
    A., Le, Q.V., 2019a. MnasNet: 面向平台的神经架构搜索用于移动设备，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，页码：2820--2828。'
- en: 'Tan and Le (2019) Tan, M., Le, Q., 2019. EfficientNet: Rethinking Model Scaling
    for Convolutional neural Networks, in: Proceedings of the International Conference
    on Machine Learning, pp. 6105--6114.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan和Le（2019）Tan, M., Le, Q., 2019. EfficientNet: 重新思考卷积神经网络的模型缩放，发表于：国际机器学习会议论文集，页码：6105--6114。'
- en: Tan et al. (2019b) Tan, T.Y., Zhang, L., Lim, C.P., Fielding, B., Yu, Y., Anderson,
    E., 2019b. Evolving ensemble models for image segmentation using enhanced particle
    swarm optimization. IEEE access 7, 34004--34019.
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan等人（2019b）Tan, T.Y., Zhang, L., Lim, C.P., Fielding, B., Yu, Y., Anderson,
    E., 2019b. 使用增强粒子群优化的图像分割进化集成模型。IEEE Access 7, 34004--34019。
- en: Tang et al. (2019a) Tang, P., Liang, Q., Yan, X., Xiang, S., Sun, W., Zhang,
    D., Coppola, G., 2019a. Efficient skin lesion segmentation using separable-unet
    with stochastic weight averaging. Computer methods and programs in biomedicine
    178, 289--301.
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人（2019a）Tang, P., Liang, Q., Yan, X., Xiang, S., Sun, W., Zhang, D., Coppola,
    G., 2019a. 使用可分离的unet和随机权重平均的高效皮肤病变分割。生物医学计算方法与程序 178, 289--301。
- en: 'Tang et al. (2021a) Tang, P., Yan, X., Liang, Q., Zhang, D., 2021a. AFLN-DGCL:
    Adaptive feature learning network with difficulty-guided curriculum learning for
    skin lesion segmentation. Applied Soft Computing 110, 107656.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang等人（2021a）Tang, P., Yan, X., Liang, Q., Zhang, D., 2021a. AFLN-DGCL: 具有难度引导课程学习的自适应特征学习网络用于皮肤病变分割。应用软计算
    110, 107656。'
- en: Tang et al. (2021b) Tang, X., Peng, J., Zhong, B., Li, J., Yan, Z., 2021b. Introducing
    frequency representation into convolution neural networks for medical image segmentation
    via twin-kernel fourier convolution. Computer Methods and Programs in Biomedicine
    205, 106110.
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人（2021b）Tang, X., Peng, J., Zhong, B., Li, J., Yan, Z., 2021b. 将频率表示引入卷积神经网络以进行医学图像分割，通过双核傅里叶卷积。生物医学计算方法与程序
    205, 106110。
- en: 'Tang et al. (2019b) Tang, Y., Yang, F., Yuan, S., et al., 2019b. A multi-stage
    framework with context information fusion structure for skin lesion segmentation,
    in: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),
    IEEE. pp. 1407--1410.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人（2019b）Tang, Y., Yang, F., Yuan, S., 等, 2019b. 具有上下文信息融合结构的多阶段框架用于皮肤病变分割，发表于：2019
    IEEE第16届生物医学成像国际研讨会（ISBI 2019），IEEE. 页码：1407--1410。
- en: Tao et al. (2021) Tao, S., Jiang, Y., Cao, S., Wu, C., Ma, Z., 2021. Attention-guided
    network with densely connected convolution for skin lesion segmentation. Sensors
    21, 3462.
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao等人（2021）Tao, S., Jiang, Y., Cao, S., Wu, C., Ma, Z., 2021. 具有密集连接卷积的注意力引导网络用于皮肤病变分割。传感器
    21, 3462。
- en: 'Tong et al. (2021) Tong, X., Wei, J., Sun, B., Su, S., Zuo, Z., Wu, P., 2021.
    Ascu-net: Attention gate, spatial and channel attention u-net for skin lesion
    segmentation. Diagnostics 11, 501.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tong等人（2021）Tong, X., Wei, J., Sun, B., Su, S., Zuo, Z., Wu, P., 2021. Ascu-net:
    具有注意力门控、空间和通道注意力的u-net用于皮肤病变分割。诊断学 11, 501。'
- en: 'Torralba and Efros (2011) Torralba, A., Efros, A.A., 2011. Unbiased look at
    dataset bias, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 1521--1528.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torralba和Efros（2011）Torralba, A., Efros, A.A., 2011. 对数据集偏差的无偏见审视，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，页码：1521--1528。
- en: 'Tran et al. (2005) Tran, H., Chen, K., Lim, A.C., Jabbour, J., Shumack, S.,
    2005. Assessing diagnostic skill in dermatology: a comparison between general
    practitioners and dermatologists. Australasian journal of dermatology 46, 230--234.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran等人（2005）Tran, H., Chen, K., Lim, A.C., Jabbour, J., Shumack, S., 2005. 评估皮肤科诊断技能：全科医生与皮肤科医生的比较。澳大利亚皮肤病学杂志
    46, 230--234。
- en: Tran and Pham (2022) Tran, T.T., Pham, V.T., 2022. Fully convolutional neural
    network with attention gate and fuzzy active contour model for skin lesion segmentation.
    Multimedia Tools and Applications 81, 13979--13999.
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran和Pham（2022）Tran, T.T., Pham, V.T., 2022. 带有注意力门和模糊主动轮廓模型的全卷积神经网络用于皮肤病变分割。《多媒体工具与应用》81,
    13979--13999。
- en: 'Tschandl et al. (2020) Tschandl, P., Rinner, C., Apalla, Z., Argenziano, G.,
    Codella, N., Halpern, A., Janda, M., Lallas, A., Longo, C., Malvehy, J., Paoli,
    J., Puig, S., Rosendahl, C., Soyer, H.P., Zalaudek, I., Kittler, H., 2020. Human--computer
    collaboration for skin cancer recognition. Nature Medicine 26, 1229--1234. URL:
    [https://doi.org/10.1038/s41591-020-0942-0](https://doi.org/10.1038/s41591-020-0942-0),
    doi:[10.1038/s41591-020-0942-0](http://dx.doi.org/10.1038/s41591-020-0942-0).'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tschandl等（2020）Tschandl, P., Rinner, C., Apalla, Z., Argenziano, G., Codella,
    N., Halpern, A., Janda, M., Lallas, A., Longo, C., Malvehy, J., Paoli, J., Puig,
    S., Rosendahl, C., Soyer, H.P., Zalaudek, I., Kittler, H., 2020. 人机协作用于皮肤癌识别。《自然医学》26,
    1229--1234。网址：[https://doi.org/10.1038/s41591-020-0942-0](https://doi.org/10.1038/s41591-020-0942-0)，doi:[10.1038/s41591-020-0942-0](http://dx.doi.org/10.1038/s41591-020-0942-0)。
- en: Tschandl et al. (2018) Tschandl, P., Rosendahl, C., Kittler, H., 2018. The HAM10000
    Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented
    Skin Lesions. Scientific Data , 180161.
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tschandl等（2018）Tschandl, P., Rosendahl, C., Kittler, H., 2018. HAM10000数据集，一大批来自多个来源的常见色素性皮肤病变的皮肤镜图像。《科学数据》，180161。
- en: Tschandl et al. (2019) Tschandl, P., Sinz, C., Kittler, H., 2019. Domain-specific
    classification-pretrained fully convolutional network encoders for skin lesion
    segmentation. Computers in Biology and Medicine 104, 111--116.
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tschandl等（2019）Tschandl, P., Sinz, C., Kittler, H., 2019. 针对皮肤病变分割的领域特定分类-预训练的全卷积网络编码器。《生物医学计算机》104,
    111--116。
- en: Tu et al. (2019) Tu, W., Liu, X., Hu, W., Pan, Z., 2019. Dense-residual network
    with adversarial learning for skin lesion segmentation. IEEE Access 7, 77037--77051.
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu等（2019）Tu, W., Liu, X., Hu, W., Pan, Z., 2019. 带有对抗学习的密集残差网络用于皮肤病变分割。《IEEE
    Access》7, 77037--77051。
- en: Unnikrishnan et al. (2007) Unnikrishnan, R., Pantofaru, C., Hebert, M., 2007.
    Toward Objective Evaluation of Image Segmentation Algorithms. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 29, 929--944.
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unnikrishnan等（2007）Unnikrishnan, R., Pantofaru, C., Hebert, M., 2007. 朝向客观评价图像分割算法。《IEEE模式分析与机器智能学报》29,
    929--944。
- en: Ünver and Ayan (2019) Ünver, H.M., Ayan, E., 2019. Skin lesion segmentation
    in dermoscopic images with combination of YOLO and GrabCut algorithm. Diagnostics
    9, 72.
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ünver和Ayan（2019）Ünver, H.M., Ayan, E., 2019. 结合YOLO和GrabCut算法的皮肤病变分割。《诊断学》9,
    72。
- en: Usatine and Madden (2013) Usatine, R.P., Madden, B.D., 2013. Interactive dermatology
    atlas. Department of Dermatology and Cutaneous Surgery, University of Texas [https://www.dermatlas.net/](https://www.dermatlas.net/)
    [Accessed January 26, 2022].
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Usatine和Madden（2013）Usatine, R.P., Madden, B.D., 2013. 互动皮肤科图谱。德克萨斯大学皮肤科与皮肤外科系
    [https://www.dermatlas.net/](https://www.dermatlas.net/) [访问日期：2022年1月26日]。
- en: 'Valanarasu and Patel (2022) Valanarasu, J.M.J., Patel, V.M., 2022. UNeXt: MLP-based
    rapid medical image segmentation network, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer. pp. 23--33.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valanarasu和Patel（2022）Valanarasu, J.M.J., Patel, V.M., 2022. UNeXt：基于MLP的快速医学图像分割网络，发表于：医学图像计算与计算机辅助干预国际会议，Springer.
    页码23--33。
- en: 'Valle et al. (2020) Valle, E., Fornaciali, M., Menegola, A., Tavares, J., Bittencourt,
    F.V., Li, L.T., Avila, S., 2020. Data, depth, and design: Learning reliable models
    for skin lesion analysis. Neurocomputing 383, 303--313.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valle等（2020）Valle, E., Fornaciali, M., Menegola, A., Tavares, J., Bittencourt,
    F.V., Li, L.T., Avila, S., 2020. 数据、深度与设计：学习可靠的皮肤病变分析模型。《神经计算》383, 303--313。
- en: van Rijsbergen (1979) van Rijsbergen, C.J., 1979. Information Retrieval. Second
    ed., Butterworth--Heinemann.
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Rijsbergen（1979）van Rijsbergen, C.J., 1979. 信息检索。第二版，Butterworth--Heinemann。
- en: Vandewalle (2012) Vandewalle, P., 2012. Code sharing is associated with research
    impact in image processing. Computing in Science & Engineering 14, 42--47.
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vandewalle（2012）Vandewalle, P., 2012. 代码共享与图像处理研究影响的关系。《计算科学与工程》14, 42--47。
- en: Vanker and Van Stoecker (1984) Vanker, A.D., Van Stoecker, W., 1984. An expert
    diagnostic program for dermatology. Computers and Biomedical Research 17, 241--247.
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vanker和Van Stoecker（1984）Vanker, A.D., Van Stoecker, W., 1984. 一种用于皮肤科的专家诊断程序。《计算机与生物医学研究》17,
    241--247。
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need.
    Advances in Neural Information Processing Systems 30.
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017. 注意力机制才是你所需要的一切。神经信息处理系统进展 30。
- en: 'Venkatesh et al. (2018) Venkatesh, G., Naresh, Y., Little, S., O’Connor, N.E.,
    2018. A deep residual architecture for skin lesion segmentation, in: OR 2.0 Context-Aware
    Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical Image-Based
    Procedures, and Skin Image Analysis. Springer, pp. 277--284.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venkatesh 等 (2018) Venkatesh, G., Naresh, Y., Little, S., O’Connor, N.E., 2018.
    一种用于皮肤病变分割的深度残差架构，载于：OR 2.0 上下文感知手术室、计算机辅助手术内窥镜、临床图像处理程序和皮肤图像分析。Springer，第 277--284
    页。
- en: 'Vesal et al. (2018a) Vesal, S., Patil, S.M., Ravikumar, N., Maier, A.K., 2018a.
    A multi-task framework for skin lesion detection and segmentation, in: OR 2.0
    Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical
    Image-Based Procedures, and Skin Image Analysis. Springer, pp. 285--293.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vesal 等 (2018a) Vesal, S., Patil, S.M., Ravikumar, N., Maier, A.K., 2018a. 一个多任务框架用于皮肤病变检测与分割，载于：OR
    2.0 上下文感知手术室、计算机辅助手术内窥镜、临床图像处理程序和皮肤图像分析。Springer，第 285--293 页。
- en: 'Vesal et al. (2018b) Vesal, S., Ravikumar, N., Maier, A., 2018b. SkinNet: A
    deep learning framework for skin lesion segmentation, in: 2018 IEEE Nuclear Science
    Symposium and Medical Imaging Conference Proceedings (NSS/MIC), IEEE. pp. 1--3.'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vesal 等 (2018b) Vesal, S., Ravikumar, N., Maier, A., 2018b. SkinNet: 一种用于皮肤病变分割的深度学习框架，载于：2018
    IEEE 核科学研讨会及医学成像会议论文集 (NSS/MIC)，IEEE. 第 1--3 页。'
- en: ViDIR Dataverse (2020) ViDIR Dataverse, 2020. HAM10000 Binary Lesion Segmentations.
    [https://doi.org/10.7910/DVN/DBW86T](https://doi.org/10.7910/DVN/DBW86T). [Online.
    Accessed January 9, 2023].
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViDIR 数据库 (2020) ViDIR 数据库, 2020. HAM10000 二分类病变分割。 [https://doi.org/10.7910/DVN/DBW86T](https://doi.org/10.7910/DVN/DBW86T)。
    [在线访问。访问日期：2023年1月9日]。
- en: 'Wang et al. (2019a) Wang, H., Wang, G., Sheng, Z., Zhang, S., 2019a. Automated
    segmentation of skin lesion based on pyramid attention network, in: International
    Workshop on Machine Learning in Medical Imaging, Springer. pp. 435--443.'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2019a) Wang, H., Wang, G., Sheng, Z., Zhang, S., 2019a. 基于金字塔注意力网络的皮肤病变自动分割，载于：医学成像中的机器学习国际研讨会，Springer.
    第 435--443 页。
- en: 'Wang et al. (2022a) Wang, J., Li, B., Guo, X., Huang, J., Song, M., Wei, M.,
    2022a. CTCNet: A bi-directional cascaded segmentation network combining Transformers
    with CNNs for skin lesions, in: Chinese Conference on Pattern Recognition and
    Computer Vision (PRCV), Springer. pp. 215--226.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2022a) Wang, J., Li, B., Guo, X., Huang, J., Song, M., Wei, M., 2022a.
    CTCNet: 一种结合变换器与卷积神经网络的双向级联分割网络用于皮肤病变，载于：中华模式识别与计算机视觉会议 (PRCV)，Springer. 第 215--226
    页。'
- en: 'Wang et al. (2021a) Wang, J., Wei, L., Wang, L., Zhou, Q., Zhu, L., Qin, J.,
    2021a. Boundary-aware Transformers for skin lesion segmentation, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 206--216.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2021a) Wang, J., Wei, L., Wang, L., Zhou, Q., Zhu, L., Qin, J., 2021a.
    边界感知变换器用于皮肤病变分割，载于：医学图像计算与计算机辅助干预国际会议，Springer. 第 206--216 页。
- en: 'Wang et al. (2017) Wang, M., Liu, B., Foroosh, H., 2017. Factorized convolutional
    neural networks, in: Proceedings of the IEEE International Conference on Computer
    Vision Workshops, pp. 545--553.'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2017) Wang, M., Liu, B., Foroosh, H., 2017. 分解卷积神经网络，载于：IEEE 国际计算机视觉会议研讨会论文集，第
    545--553 页。
- en: Wang et al. (2020a) Wang, R., Chen, S., Fan, J., Li, Y., 2020a. Cascaded context
    enhancement for automated skin lesion segmentation. arXiv preprint arXiv:2004.08107
    .
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2020a) Wang, R., Chen, S., Fan, J., Li, Y., 2020a. 级联上下文增强用于自动化皮肤病变分割。arXiv
    预印本 arXiv:2004.08107。
- en: 'Wang et al. (2022b) Wang, T., Lan, J., Han, Z., Hu, Z., Huang, Y., Deng, Y.,
    Zhang, H., Wang, J., Chen, M., Jiang, H., et al., 2022b. O-Net: a novel framework
    with deep fusion of CNN and Transformer for simultaneous segmentation and classification.
    Frontiers in Neuroscience 16.'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2022b) Wang, T., Lan, J., Han, Z., Hu, Z., Huang, Y., Deng, Y., Zhang,
    H., Wang, J., Chen, M., Jiang, H., 等, 2022b. O-Net: 一种具有深度融合 CNN 和 Transformer
    的新框架，用于同时分割和分类。神经科学前沿 16。'
- en: 'Wang et al. (2018) Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro,
    B., 2018. High-resolution image synthesis and semantic manipulation with conditional
    gans, in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 8798--8807.'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2018) Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro,
    B., 2018. 使用条件生成对抗网络进行高分辨率图像合成和语义操控，载于：IEEE 计算机视觉与模式识别大会论文集，第 8798--8807 页。
- en: 'Wang et al. (2019b) Wang, X., Ding, H., Jiang, X., 2019b. Dermoscopic image
    segmentation through the enhanced high-level parsing and class weighted loss,
    in: 2019 IEEE International Conference on Image Processing (ICIP), IEEE. pp. 245--249.'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2019b) Wang, X., Ding, H., Jiang, X., 2019b. 通过增强的高级解析和类别加权损失进行皮肤镜图像分割，见：2019
    IEEE 国际图像处理会议 (ICIP)，IEEE。第245--249页。
- en: Wang et al. (2019c) Wang, X., Jiang, X., Ding, H., Liu, J., 2019c. Bi-directional
    dermoscopic feature learning and multi-scale consistent decision fusion for skin
    lesion segmentation. IEEE transactions on image processing 29, 3039--3051.
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2019c) Wang, X., Jiang, X., Ding, H., Liu, J., 2019c. 双向皮肤镜特征学习和多尺度一致性决策融合用于皮肤病变分割。IEEE
    transactions on image processing 29, 3039--3051。
- en: Wang et al. (2021b) Wang, X., Jiang, X., Ding, H., Zhao, Y., Liu, J., 2021b.
    Knowledge-aware deep framework for collaborative skin lesion segmentation and
    melanoma recognition. Pattern Recognition 120, 108075.
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2021b) Wang, X., Jiang, X., Ding, H., Zhao, Y., Liu, J., 2021b. 知识感知深度框架用于协作皮肤病变分割和黑色素瘤识别。Pattern
    Recognition 120, 108075。
- en: Wang and Wang (2022) Wang, Y., Wang, S., 2022. Skin lesion segmentation with
    attention-based SC-Conv U-Net and feature map distortion. Signal, Image and Video
    Processing , 1--9.
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Wang (2022) Wang, Y., Wang, S., 2022. 基于注意力的 SC-Conv U-Net 和特征图扭曲的皮肤病变分割。Signal,
    Image and Video Processing , 1--9。
- en: 'Wang et al. (2020b) Wang, Y., Wei, Y., Qian, X., Zhu, L., Yang, Y., 2020b.
    DONet: Dual objective networks for skin lesion segmentation. arXiv preprint arXiv:2008.08278
    .'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2020b) Wang, Y., Wei, Y., Qian, X., Zhu, L., Yang, Y., 2020b. DONet:
    双目标网络用于皮肤病变分割。arXiv 预印本 arXiv:2008.08278。'
- en: 'Wang et al. (2022c) Wang, Y., Xu, Z., Tian, J., Luo, J., Shi, Z., Zhang, Y.,
    Fan, J., He, Z., 2022c. Cross-domain few-shot learning for rare-disease skin lesion
    segmentation, in: ICASSP 2022-2022 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), IEEE. pp. 1086--1090.'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2022c) Wang, Y., Xu, Z., Tian, J., Luo, J., Shi, Z., Zhang, Y., Fan,
    J., He, Z., 2022c. 跨域少样本学习用于罕见疾病皮肤病变分割，见：ICASSP 2022-2022 IEEE 国际声学、语音与信号处理会议
    (ICASSP)，IEEE。第1086--1090页。
- en: 'Wang et al. (2022d) Wang, Z., Lyu, J., Luo, W., Tang, X., 2022d. Superpixel
    inpainting for self-supervised skin lesion segmentation from dermoscopic images,
    in: 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), IEEE.
    pp. 1--4.'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2022d) Wang, Z., Lyu, J., Luo, W., Tang, X., 2022d. 用于自监督皮肤病变分割的超像素修复，见：2022
    IEEE 第19届国际生物医学成像研讨会 (ISBI)，IEEE。第1--4页。
- en: 'Warfield and Wells (2004) Warfield, S. K. anbd Zou, K.H., Wells, W.M., 2004.
    Simultaneous Truth and Performance Level Estimation (STAPLE): An Algorithm for
    the Validation of Image Segmentation. IEEE Transactions on Medical Imaging 23,
    903--921.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Warfield 和 Wells (2004) Warfield, S. K. 和 Zou, K.H., Wells, W.M., 2004. 同时真实与性能水平估计
    (STAPLE)：一种用于图像分割验证的算法。IEEE Transactions on Medical Imaging 23, 903--921。
- en: Wei et al. (2019) Wei, Z., Song, H., Chen, L., Li, Q., Han, G., 2019. Attention-based
    DenseUnet network with adversarial training for skin lesion segmentation. IEEE
    Access 7, 136616--136629.
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 (2019) Wei, Z., Song, H., Chen, L., Li, Q., Han, G., 2019. 基于注意力的 DenseUnet
    网络与对抗训练用于皮肤病变分割。IEEE Access 7, 136616--136629。
- en: 'Weng et al. (2019) Weng, Y., Zhou, T., Li, Y., Qiu, X., 2019. NAS-Unet: Neural
    architecture search for medical image segmentation. IEEE Access 7, 44247--44257.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Weng 等 (2019) Weng, Y., Zhou, T., Li, Y., Qiu, X., 2019. NAS-Unet: 医学图像分割的神经架构搜索。IEEE
    Access 7, 44247--44257。'
- en: Wibowo et al. (2021) Wibowo, A., Purnama, S.R., Wirawan, P.W., Rasyidi, H.,
    2021. Lightweight encoder-decoder model for automatic skin lesion segmentation.
    Informatics in Medicine Unlocked , 100640.
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wibowo 等 (2021) Wibowo, A., Purnama, S.R., Wirawan, P.W., Rasyidi, H., 2021.
    轻量级编码-解码模型用于自动皮肤病变分割。Informatics in Medicine Unlocked , 100640。
- en: 'Wu et al. (2022a) Wu, H., Chen, S., Chen, G., Wang, W., Lei, B., Wen, Z., 2022a.
    FAT-Net: Feature adaptive Transformers for automated skin lesion segmentation.
    Medical Image Analysis 76, 102327.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等 (2022a) Wu, H., Chen, S., Chen, G., Wang, W., Lei, B., Wen, Z., 2022a.
    FAT-Net: 用于自动皮肤病变分割的特征自适应变换器。Medical Image Analysis 76, 102327。'
- en: Wu et al. (2020) Wu, H., Pan, J., Li, Z., Wen, Z., Qin, J., 2020. Automated
    skin lesion segmentation via an adaptive dual attention module. IEEE Transactions
    on Medical Imaging 40, 357--370.
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2020) Wu, H., Pan, J., Li, Z., Wen, Z., Qin, J., 2020. 通过自适应双注意力模块实现自动皮肤病变分割。IEEE
    Transactions on Medical Imaging 40, 357--370。
- en: 'Wu et al. (2022b) Wu, J., Fang, H., Shang, F., Yang, D., Wang, Z., Gao, J.,
    Yang, Y., Xu, Y., 2022b. SeATrans: Learning segmentation-assisted diagnosis model
    via Transformer, in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer. pp. 677--687.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu等（2022b）Wu, J., Fang, H., Shang, F., Yang, D., Wang, Z., Gao, J., Yang, Y.,
    Xu, Y., 2022b. SeATrans: 通过Transformer学习分割辅助诊断模型，载于：国际医学图像计算与计算机辅助干预会议，Springer.
    第677--687页。'
- en: 'Wu et al. (2022c) Wu, Y., Zeng, D., Xu, X., Shi, Y., Hu, J., 2022c. FairPrune:
    Achieving fairness through pruning for dermatological disease diagnosis, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 743--753.'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu等（2022c）Wu, Y., Zeng, D., Xu, X., Shi, Y., Hu, J., 2022c. FairPrune: 通过剪枝实现皮肤病诊断的公平性，载于：国际医学图像计算与计算机辅助干预会议，Springer.
    第743--753页。'
- en: Xie et al. (2020a) Xie, F., Yang, J., Liu, J., Jiang, Z., Zheng, Y., Wang, Y.,
    2020a. Skin lesion segmentation using high-resolution convolutional neural network.
    Computer methods and programs in biomedicine 186, 105241.
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等（2020a）Xie, F., Yang, J., Liu, J., Jiang, Z., Zheng, Y., Wang, Y., 2020a.
    使用高分辨率卷积神经网络进行皮损分割。生物医学计算方法与程序 186, 105241。
- en: Xie et al. (2020b) Xie, Y., Zhang, J., Xia, Y., Shen, C., 2020b. A mutual bootstrapping
    model for automated skin lesion segmentation and classification. IEEE Transactions
    on Medical Imaging 39, 2482--2493.
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等（2020b）Xie, Y., Zhang, J., Xia, Y., Shen, C., 2020b. 一种用于自动化皮损分割和分类的互 bootstrapping
    模型。IEEE医学影像学报 39, 2482--2493。
- en: 'Xie et al. (2021) Xie, Z., Tu, E., Zheng, H., Gu, Y., Yang, J., 2021. Semi-supervised
    skin lesion segmentation with learning model confidence, in: ICASSP 2021-2021
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE. pp. 1135--1139.'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等（2021）Xie, Z., Tu, E., Zheng, H., Gu, Y., Yang, J., 2021. 带有学习模型信心的半监督皮损分割，载于：ICASSP
    2021-2021 IEEE国际声学、语音与信号处理会议（ICASSP），IEEE. 第1135--1139页。
- en: 'Xu et al. (2021) Xu, R., Wang, C., Xu, S., Meng, W., Zhang, X., 2021. DC-Net:
    Dual context network for 2D medical image segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 503--513.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu等（2021）Xu, R., Wang, C., Xu, S., Meng, W., Zhang, X., 2021. DC-Net: 双重上下文网络用于2D医学图像分割，载于：国际医学图像计算与计算机辅助干预会议，Springer.
    第503--513页。'
- en: 'Xue et al. (2018) Xue, Y., Xu, T., Huang, X., 2018. Adversarial learning with
    multi-scale loss for skin lesion segmentation, in: 2018 IEEE 15th International
    Symposium on Biomedical Imaging (ISBI 2018), IEEE. pp. 859--863.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue等（2018）Xue, Y., Xu, T., Huang, X., 2018. 使用多尺度损失进行对抗性学习以进行皮损分割，载于：2018 IEEE第15届生物医学影像国际研讨会（ISBI
    2018），IEEE. 第859--863页。
- en: 'Yan et al. (2019) Yan, Y., Kawahara, J., Hamarneh, G., 2019. Melanoma recognition
    via visual attention, in: International Conference on Information Processing in
    Medical Imaging, Springer. pp. 793--804.'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan等（2019）Yan, Y., Kawahara, J., Hamarneh, G., 2019. 通过视觉注意力识别黑色素瘤，载于：医学影像信息处理国际会议，Springer.
    第793--804页。
- en: Yang et al. (2021) Yang, C.H., Ren, J.H., Huang, H.C., Chuang, L.Y., Chang,
    P.Y., 2021. Deep hybrid convolutional neural network for segmentation of melanoma
    skin lesion. Computational Intelligence and Neuroscience 2021.
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2021）Yang, C.H., Ren, J.H., Huang, H.C., Chuang, L.Y., Chang, P.Y., 2021.
    深度混合卷积神经网络用于黑色素瘤皮损分割。计算智能与神经科学 2021。
- en: 'Yang et al. (2018) Yang, X., Li, H., Wang, L., Yeo, S.Y., Su, Y., Zeng, Z.,
    2018. Skin lesion analysis by multi-target deep neural networks, in: 2018 40th
    Annual International Conference of the IEEE Engineering in Medicine and Biology
    Society (EMBC), IEEE. pp. 1263--1266.'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2018）Yang, X., Li, H., Wang, L., Yeo, S.Y., Su, Y., Zeng, Z., 2018. 多目标深度神经网络的皮损分析，载于：2018年IEEE医学与生物工程年会（EMBC），IEEE.
    第1263--1266页。
- en: Yerushalmy (1947) Yerushalmy, J., 1947. Statistical problems in assessing methods
    of medical diagnosis, with special reference to X-ray techniques. Public Health
    Reports (1896-1970) 62, 1432--1449.
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yerushalmy（1947）Yerushalmy, J., 1947. 评估医学诊断方法中的统计问题，特别是X射线技术。公共卫生报告（1896-1970）62,
    1432--1449。
- en: 'Yi et al. (2019) Yi, X., Walia, E., Babyn, P., 2019. Generative Adversarial
    Network in Medical Imaging: A Review. Medical Image Analysis 58, 101552.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi等（2019）Yi, X., Walia, E., Babyn, P., 2019. 医学影像中的生成对抗网络：综述。医学图像分析 58, 101552。
- en: 'Yu et al. (2022) Yu, B., Yu, L., Tian, S., Wu, W., Zhang, D., Kang, X., 2022.
    mCA-Net: modified comprehensive attention convolutional neural network for skin
    lesion segmentation. Computer Methods in Biomechanics and Biomedical Engineering:
    Imaging & Visualization 10, 85--95.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 (2022) Yu, B., Yu, L., Tian, S., Wu, W., Zhang, D., Kang, X., 2022. mCA-Net：用于皮肤病变分割的改进综合注意卷积神经网络。计算方法在生物力学与生物医学工程：成像与可视化
    10, 85--95。
- en: Yu and Koltun (2016) Yu, F., Koltun, V., 2016. Multi-scale context aggregation
    by dilated convolutions. international conference on learning representations
    .
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 和 Koltun (2016) Yu, F., Koltun, V., 2016. 通过扩张卷积进行多尺度上下文聚合。国际学习表征会议。
- en: Yu et al. (2017a) Yu, L., Chen, H., Dou, Q., Qin, J., Heng, P.A., 2017a. Automated
    melanoma recognition in dermoscopy images via very deep residual networks. IEEE
    transactions on medical imaging 36, 994--1004.
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 (2017a) Yu, L., Chen, H., Dou, Q., Qin, J., Heng, P.A., 2017a. 通过非常深的残差网络在皮肤镜图像中自动识别黑色素瘤。IEEE
    医学成像交易 36, 994--1004。
- en: 'Yu et al. (2017b) Yu, Y., Gong, Z., Zhong, P., Shan, J., 2017b. Unsupervised
    representation learning with deep convolutional neural network for remote sensing
    images, in: International Conference on Image and Graphics, pp. 97--108.'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 (2017b) Yu, Y., Gong, Z., Zhong, P., Shan, J., 2017b. 使用深度卷积神经网络进行遥感图像的无监督表示学习，见：国际图像与图形会议，第97--108页。
- en: Yuan et al. (2017) Yuan, Y., Chao, M., Lo, Y.C., 2017. Automatic skin lesion
    segmentation using deep fully convolutional networks with jaccard distance. IEEE
    transactions on medical imaging 36, 1876--1886.
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等 (2017) Yuan, Y., Chao, M., Lo, Y.C., 2017. 使用深度全卷积网络和 Jaccard 距离进行自动皮肤病变分割。IEEE
    医学成像交易 36, 1876--1886。
- en: Yuan and Lo (2019) Yuan, Y., Lo, Y.C., 2019. Improving Dermoscopic Image Segmentation
    with Enhanced Convolutional-Deconvolutional Networks. IEEE Journal of Biomedical
    and Health Informatics 23, 519--526.
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 和 Lo (2019) Yuan, Y., Lo, Y.C., 2019. 使用增强的卷积-反卷积网络改进皮肤镜图像分割。IEEE 生物医学与健康信息学杂志
    23, 519--526。
- en: Zafar et al. (2020) Zafar, K., Gilani, S.O., Waris, A., Ahmed, A., Jamil, M.,
    Khan, M.N., Sohail Kashif, A., 2020. Skin lesion segmentation from dermoscopic
    images using convolutional neural network. Sensors 20, 1601.
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zafar 等 (2020) Zafar, K., Gilani, S.O., Waris, A., Ahmed, A., Jamil, M., Khan,
    M.N., Sohail Kashif, A., 2020. 使用卷积神经网络从皮肤镜图像中进行皮肤病变分割。传感器 20, 1601。
- en: 'Zeng and Zheng (2018) Zeng, G., Zheng, G., 2018. Multi-scale fully convolutional
    densenets for automated skin lesion segmentation in dermoscopy images, in: International
    Conference Image Analysis and Recognition, Springer. pp. 513--521.'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 和 Zheng (2018) Zeng, G., Zheng, G., 2018. 多尺度全卷积密集网络用于皮肤镜图像中的自动化皮肤病变分割，见：国际图像分析与识别会议，Springer。第513--521页。
- en: 'Zhang et al. (2019a) Zhang, G., Shen, X., Chen, S., Liang, L., Luo, Y., Yu,
    J., Lu, J., 2019a. DSM: A deep supervised multi-scale network learning for skin
    cancer segmentation. IEEE Access 7, 140936--140945.'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019a) Zhang, G., Shen, X., Chen, S., Liang, L., Luo, Y., Yu, J., Lu,
    J., 2019a. DSM：用于皮肤癌分割的深度监督多尺度网络学习。IEEE Access 7, 140936--140945。
- en: 'Zhang et al. (2008) Zhang, H., Fritts, J.E., Goldman, S.A., 2008. Image Segmentation
    Evaluation: A Survey of Unsupervised Methods. Computer Vision and Image Understanding
    110, 260--280.'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2008) Zhang, H., Fritts, J.E., Goldman, S.A., 2008. 图像分割评估：无监督方法的调查。计算机视觉与图像理解
    110, 260--280。
- en: 'Zhang et al. (2020a) Zhang, J., Petitjean, C., Ainouz, S., 2020a. Kappa loss
    for skin lesion segmentation in fully convolutional network, in: 2020 IEEE 17th
    International Symposium on Biomedical Imaging (ISBI), IEEE. pp. 2001--2004.'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020a) Zhang, J., Petitjean, C., Ainouz, S., 2020a. 在全卷积网络中用于皮肤病变分割的
    Kappa 损失，见：2020 IEEE 第17届国际生物医学成像研讨会（ISBI），IEEE。第2001--2004页。
- en: 'Zhang et al. (2020b) Zhang, L., Tanno, R., Bronik, K., Jin, C., Nachev, P.,
    Barkhof, F., Ciccarelli, O., Alexander, D.C., 2020b. Learning to segment when
    experts disagree, in: International Conference on Medical Image Computing and
    Computer-Assisted Intervention, Springer. pp. 179--190.'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020b) Zhang, L., Tanno, R., Bronik, K., Jin, C., Nachev, P., Barkhof,
    F., Ciccarelli, O., Alexander, D.C., 2020b. 学习在专家意见不一致时进行分割，见：国际医学图像计算与计算机辅助干预会议，Springer。第179--190页。
- en: Zhang et al. (2019b) Zhang, L., Yang, G., Ye, X., 2019b. Automatic skin lesion
    segmentation by coupling deep fully convolutional networks and shallow network
    with textons. Journal of Medical Imaging 6, 024001.
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019b) Zhang, L., Yang, G., Ye, X., 2019b. 通过结合深度全卷积网络和浅层网络与纹理进行自动皮肤病变分割。医学成像杂志
    6, 024001。
- en: 'Zhang et al. (2021a) Zhang, R., Liu, S., Yu, Y., Li, G., 2021a. Self-supervised
    correction learning for semi-supervised biomedical image segmentation, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 134--144.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Zhang, X., Zhou, X., Lin, M., Sun, J., 2018. ShuffleNet:
    An Extremely Efficient Convolutional Neural Network for Mobile Devices, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6848--6856.'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022a) Zhang, Y., Chen, Z., Yu, H., Yao, X., Li, H., 2022a. Feature
    fusion for segmentation and classification of skin lesions, in: 2022 IEEE 19th
    International Symposium on Biomedical Imaging (ISBI), IEEE. pp. 1--5.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021b) Zhang, Y., Liu, H., Hu, Q., 2021b. TransFuse: Fusing Transformers
    and CNNs for medical image segmentation, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer. pp. 14--24.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Yang (2022) Zhang, Y., Yang, Q., 2022. A survey on multi-task learning.
    IEEE Transactions on Knowledge and Data Engineering .
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Zhang, Z., Tian, C., Gao, X., Wang, C., Feng, X., Bai,
    H.X., Jiao, Z., 2022b. Dynamic prototypical feature representation learning framework
    for semi-supervised skin lesion segmentation. Neurocomputing 507, 369--382.
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2021) Zhao, C., Shuai, R., Ma, L., Liu, W., Wu, M., 2021. Segmentation
    of dermoscopy images based on deformable 3D convolution and ResU-NeXt++. Medical
    & Biological Engineering & Computing 59, 1815--1832.
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020) Zhao, H., Jia, J., Koltun, V., 2020. Exploring self-attention
    for image recognition, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 10076--10085.'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2017) Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., 2017. Pyramid
    Scene Parsing Network, in: Proceedings of the 2017 IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 2881--2890.'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2022a) Zhao, M., Kawahara, J., Abhishek, K., Shamanian, S., Hamarneh,
    G., 2022a. Skin3d: Detection and longitudinal tracking of pigmented skin lesions
    in 3D total-body textured meshes. Medical Image Analysis 77, 102329.'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2022b) Zhao, Z., Lu, W., Zeng, Z., Xu, K., Veeravalli, B., Guan,
    C., 2022b. Self-supervised assisted active learning for skin lesion segmentation,
    in: 2022 44th Annual International Conference of the IEEE Engineering in Medicine
    & Biology Society (EMBC), IEEE. pp. 5043--5046. URL: [https://doi.org/10.1109/embc48229.2022.9871734](https://doi.org/10.1109/embc48229.2022.9871734),
    doi:[10.1109/embc48229.2022.9871734](http://dx.doi.org/10.1109/embc48229.2022.9871734).'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2021) Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y.,
    Fu, Y., Feng, J., Xiang, T., Torr, P.H., et al., 2021. Rethinking semantic segmentation
    from a sequence-to-sequence perspective with transformers, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6881--6890.'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2021）Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y.,
    Feng, J., Xiang, T., Torr, P.H., 等, 2021. 从序列到序列的角度重新思考语义分割，见于：IEEE/CVF 计算机视觉与模式识别会议论文集，第6881--6890页。
- en: 'Zhou et al. (2016) Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba,
    A., 2016. Learning deep features for discriminative localization, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 2921--2929.'
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2016）Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A., 2016.
    学习用于辨别定位的深度特征，见于：IEEE 计算机视觉与模式识别会议论文集，第2921--2929页。
- en: 'Zhu et al. (2020) Zhu, L., Feng, S., Zhu, W., Chen, X., 2020. ASNet: An adaptive
    scale network for skin lesion segmentation in dermoscopy images, in: Medical Imaging
    2020: Biomedical Applications in Molecular, Structural, and Functional Imaging,
    International Society for Optics and Photonics. SPIE. pp. 226--231.'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2020）Zhu, L., Feng, S., Zhu, W., Chen, X., 2020. ASNet：用于皮肤病变分割的自适应尺度网络，见于：医学成像
    2020：分子、结构与功能成像中的生物医学应用，国际光学与光子学学会。SPIE. 第226--231页。
- en: Zhu (2020) Zhu, Q., 2020. On the Performance of Matthews Correlation Coefficient
    (MCC) for Imbalanced Dataset. Pattern Recognition Letters 136, 71--80.
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu（2020）Zhu, Q., 2020. 对不平衡数据集的 Matthews 相关系数（MCC）性能的研究。模式识别快报 136, 71--80。
- en: 'Zijdenbos et al. (1994) Zijdenbos, A.P., Dawant, B.M., Margolin, R.A., Palmer,
    A.C., 1994. Morphometric Analysis of White Matter Lesions in MR Images: Method
    and Validation. IEEE Transactions on Medical Imaging 13, 716--724.'
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zijdenbos 等（1994）Zijdenbos, A.P., Dawant, B.M., Margolin, R.A., Palmer, A.C.,
    1994. 磁共振图像中白质病灶的形态学分析：方法与验证。IEEE 医学成像交易 13, 716--724。
- en: Zortea et al. (2011) Zortea, M., Skrøvseth, S.O., Schopf, T.R., Kirchesch, H.M.,
    Godtliebsen, F., 2011. Automatic segmentation of dermoscopic images by iterative
    classification. International journal of biomedical imaging 2011.
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zortea 等（2011）Zortea, M., Skrøvseth, S.O., Schopf, T.R., Kirchesch, H.M., Godtliebsen,
    F., 2011. 通过迭代分类自动分割皮肤镜图像。国际生物医学成像杂志 2011。
- en: Zou et al. (2004) Zou, K.H., Warfield, S.K., Bharatha, A., Tempany, C.M., Kaus,
    M.R., Haker, S.J., Wells III, W.M., Jolesz, F.A., Kikinis, R., 2004. Statistical
    Validation of Image Segmentation Quality Based on a Spatial Overlap Index. Academic
    Radiology 11, 178--189.
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等（2004）Zou, K.H., Warfield, S.K., Bharatha, A., Tempany, C.M., Kaus, M.R.,
    Haker, S.J., Wells III, W.M., Jolesz, F.A., Kikinis, R., 2004. 基于空间重叠指数的图像分割质量的统计验证。学术放射学
    11, 178--189。
- en: 'Zunair and Hamza (2021) Zunair, H., Hamza, A.B., 2021. Sharp U-Net: Depthwise
    convolutional network for biomedical image segmentation. Computers in Biology
    and Medicine 136, 104699.'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zunair 和 Hamza（2021）Zunair, H., Hamza, A.B., 2021. Sharp U-Net：用于生物医学图像分割的深度卷积网络。生物医学与医学计算机
    136, 104699。
