- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:46:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:46:13'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2206.00356] A Survey on Deep Learning for Skin Lesion Segmentation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2206.00356] 皮肤病变分割的深度学习调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2206.00356](https://ar5iv.labs.arxiv.org/html/2206.00356)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2206.00356](https://ar5iv.labs.arxiv.org/html/2206.00356)
- en: A Survey on Deep Learning for Skin Lesion Segmentation
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 皮肤病变分割的深度学习调查
- en: Zahra Mirikharaji¹¹1Joint first authors Kumar Abhishek²²2Joint first authors
    Alceu Bissoto Catarina Barata Sandra Avila Eduardo Valle M. Emre Celebi³³3Joint
    senior authors Ghassan Hamarneh⁴⁴4Joint senior authors Medical Image Analysis
    Lab, School of Computing Science, Simon Fraser University, Burnaby V5A 1S6, Canada
    Institute for Systems and Robotics, Instituto Superior Técnico, Avenida Rovisco
    Pais, Lisbon 1049-001, Portugal RECOD.ai Lab, Institute of Computing, University
    of Campinas, Av. Albert Einstein 1251, Campinas 13083-852, Brazil RECOD.ai Lab,
    School of Electrical and Computing Engineering, University of Campinas, Av. Albert
    Einstein 400, Campinas 13083-952, Brazil Department of Computer Science and Engineering,
    University of Central Arkansas, 201 Donaghey Ave., Conway, AR 72035, USA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zahra Mirikharaji¹¹1联合第一作者 Kumar Abhishek²²2联合第一作者 Alceu Bissoto Catarina Barata
    Sandra Avila Eduardo Valle M. Emre Celebi³³3联合资深作者 Ghassan Hamarneh⁴⁴4联合资深作者 医学图像分析实验室，计算机科学学院，西蒙弗雷泽大学，加拿大伯纳比
    V5A 1S6 系统与机器人研究所，里斯本技术大学，Avenida Rovisco Pais，里斯本 1049-001，葡萄牙 RECOD.ai 实验室，坎皮纳斯大学计算机学院，Av.
    Albert Einstein 1251，坎皮纳斯 13083-852，巴西 RECOD.ai 实验室，坎皮纳斯大学电气与计算机工程学院，Av. Albert
    Einstein 400，坎皮纳斯 13083-952，巴西 中央阿肯色大学计算机科学与工程系，201 Donaghey Ave.，康威，AR 72035，美国
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Skin cancer is a major public health problem that could benefit from computer-aided
    diagnosis to reduce the burden of this common disease. Skin lesion segmentation
    from images is an important step toward achieving this goal. However, the presence
    of natural and artificial artifacts (e.g., hair and air bubbles), intrinsic factors
    (e.g., lesion shape and contrast), and variations in image acquisition conditions
    make skin lesion segmentation a challenging task. Recently, various researchers
    have explored the applicability of deep learning models to skin lesion segmentation.
    In this survey, we cross-examine $177$ research papers that deal with deep learning-based
    segmentation of skin lesions. We analyze these works along several dimensions,
    including input data (datasets, preprocessing, and synthetic data generation),
    model design (architecture, modules, and losses), and evaluation aspects (data
    annotation requirements and segmentation performance). We discuss these dimensions
    both from the viewpoint of select seminal works, and from a systematic viewpoint,
    examining how those choices have influenced current trends, and how their limitations
    should be addressed. To facilitate comparisons, we summarize all examined works
    in a comprehensive table as well as an interactive table available online⁵⁵5 [https://github.com/sfu-mial/skin-lesion-segmentation-survey](https://github.com/sfu-mial/skin-lesion-segmentation-survey)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤癌是一个重大公共健康问题，可以通过计算机辅助诊断来减少这种常见疾病的负担。皮肤病变从图像中分割是实现这一目标的重要步骤。然而，自然和人工伪影（例如头发和气泡）、内在因素（例如病变的形状和对比度）以及图像采集条件的变化使得皮肤病变分割成为一项具有挑战性的任务。最近，许多研究者探讨了深度学习模型在皮肤病变分割中的应用。在这项调查中，我们交叉审查了$177$篇涉及基于深度学习的皮肤病变分割的研究论文。我们从多个维度分析了这些研究，包括输入数据（数据集、预处理和合成数据生成）、模型设计（架构、模块和损失函数）以及评估方面（数据注释要求和分割性能）。我们从一些重要工作的视角和系统视角讨论了这些维度，考察了这些选择如何影响当前趋势，以及如何解决它们的局限性。为了便于比较，我们在一个全面的表格中总结了所有审查的工作，并在在线互动表格中提供了详细信息⁵⁵5
    [https://github.com/sfu-mial/skin-lesion-segmentation-survey](https://github.com/sfu-mial/skin-lesion-segmentation-survey)
- en: '^†^†journal: Medical Image Analysis'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '^†^†期刊: 医学图像分析'
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Segmentation is a challenging and critical operation in the automated skin
    lesion analysis workflow. Rule-based skin lesion diagnostic systems, popular in
    the clinical setting, rely on an accurate lesion segmentation for the estimation
    of diagnostic criteria such as asymmetry, border irregularity, and lesion size,
    as needed for implementing the ABCD algorithm (Asymmetry, Border, Color, Diameter
    of lesions) (Friedman et al., [1985](#bib.bib138); Nachbar et al., [1994](#bib.bib284))
    and its derivatives: ABCDE (ABCD plus Evolution of lesions) (Abbasi et al., [2004](#bib.bib2))
    and ABCDEF (ABCDE plus the “ugly duckling” sign) (Jensen and Elewski, [2015](#bib.bib200)).
    By contrast, in machine learning-based diagnostic systems, restricting the areas
    within an image, thereby focusing the model on the interior of the lesion, can
    improve the robustness of the classification. For example, recent studies have
    shown the utility of segmentation in improving the deep learning (DL)-based classification
    performance for certain diagnostic categories by regularizing attention maps (Yan
    et al., [2019](#bib.bib424)), allowing the cropping of lesion images (Yu et al.,
    [2017a](#bib.bib431); Mahbod et al., [2020](#bib.bib268); Liu et al., [2020](#bib.bib259);
    Singh et al., [2023](#bib.bib354)), tracking the evolution of lesions (Navarro
    et al., [2018](#bib.bib287)) and the removal of imaging artifacts (Maron et al.,
    [2021a](#bib.bib271); Bissoto et al., [2022](#bib.bib58)). In a DL-based skin
    lesion classification framework, presenting the delineated skin lesion to the
    user can also help with interpreting the DL black box (Jaworek-Korjakowska et al.,
    [2021](#bib.bib198)), and thus may either instill trust, or raise suspicion, in
    computer-aided diagnosis (CAD) systems for skin cancer.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 分割是在自动化皮肤病变分析工作流程中一项具有挑战性和关键性的操作。基于规则的皮肤病变诊断系统，在临床环境中较为流行，依赖于准确的病变分割来评估诊断标准，如对称性、边界不规则性和病变大小，这些都是实施ABCD算法（病变的对称性、边界、颜色、直径）所需的 (Friedman
    et al., [1985](#bib.bib138); Nachbar et al., [1994](#bib.bib284))及其衍生算法：ABCDE（ABCD加上病变的演变） (Abbasi
    et al., [2004](#bib.bib2))和ABCDEF（ABCDE加上“丑小鸭”标志） (Jensen and Elewski, [2015](#bib.bib200))。相较而言，在基于机器学习的诊断系统中，通过限制图像中的区域，从而将模型的关注点集中在病变的内部，可以提高分类的稳健性。例如，最近的研究表明，分割在通过规范化注意力图 (Yan
    et al., [2019](#bib.bib424))、允许裁剪病变图像 (Yu et al., [2017a](#bib.bib431); Mahbod
    et al., [2020](#bib.bib268); Liu et al., [2020](#bib.bib259); Singh et al., [2023](#bib.bib354))、跟踪病变的演变 (Navarro
    et al., [2018](#bib.bib287))以及去除成像伪影 (Maron et al., [2021a](#bib.bib271); Bissoto
    et al., [2022](#bib.bib58))来提高基于深度学习（DL）的分类性能方面表现出了实用性。在基于DL的皮肤病变分类框架中，将勾画出的皮肤病变呈现给用户也有助于解释DL的黑箱 (Jaworek-Korjakowska
    et al., [2021](#bib.bib198))，因此可能会对计算机辅助诊断（CAD）系统在皮肤癌诊断中的信任感产生影响，或者引发怀疑。
- en: Lesion detection and segmentation are also useful as preprocessing steps when
    analyzing wide-field images with multiple lesions (Birkenfeld et al., [2020](#bib.bib57)).
    Additionally, radiation therapy and image-guided human or robotic surgical lesion
    excision require localization and delineation of lesions (American Cancer Society,
    [2023](#bib.bib23)). Ensuring fair diagnosis that is unbiased to minority groups,
    a pressing issue with the deployment of these models and the trust therein, requires
    the estimation of lesion-free skin tone, which in turn also relies upon the delineation
    of skin lesions (Kinyanjui et al., [2020](#bib.bib228)). However, despite the
    importance of lesion segmentation, manual delineation of skin lesions remains
    a laborious task that suffers from significant inter- and intra-observer variability
    and consequently, a fast, reliable, and automated segmentation algorithm is needed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 病变检测和分割在分析多病变的广域图像时也作为预处理步骤非常有用 (Birkenfeld et al., [2020](#bib.bib57))。此外，放射治疗和影像引导的人体或机器人手术病变切除需要对病变进行定位和勾画 (American
    Cancer Society, [2023](#bib.bib23))。确保对少数群体公平的诊断，这是这些模型及其信任部署中的一个紧迫问题，需要估算无病变的肤色，这也依赖于对皮肤病变的勾画 (Kinyanjui
    et al., [2020](#bib.bib228))。然而，尽管病变分割非常重要，手动勾画皮肤病变仍是一项繁重的任务，存在显著的观察者间和观察者内变异，因此，需要一种快速、可靠且自动化的分割算法。
- en: 'Skin cancer and its associated expenses, $\$8.1$ billion annually in U.S. (Guy Jr
    et al., [2015](#bib.bib168)), have grown into a major public health issue in the
    past decades. In the USA alone, $97,610$ new cases of melanoma are expected in
    2023 (Siegel et al., [2023](#bib.bib351)). Broadly speaking, there are two types
    of skin cancer: melanomas and non-melanomas, the former making up just $1\%$ of
    the cases, but the majority of the deaths due to its aggressiveness. Early diagnosis
    is critical for a good prognosis: melanoma can be cured with a simple outpatient
    surgery if detected early, but its five-year survival rate drops from over $99\%$
    to $32\%$ if it is diagnosed at an advanced stage (American Cancer Society, [2023](#bib.bib23)).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤癌及其相关费用，每年在美国高达$8.1$亿（Guy Jr et al., [2015](#bib.bib168)），在过去几十年中已成为一个主要的公共健康问题。在美国，仅在2023年，就预计会出现$97,610$例新的黑色素瘤病例（Siegel
    et al., [2023](#bib.bib351)）。一般来说，皮肤癌分为两种类型：黑色素瘤和非黑色素瘤，前者仅占病例的$1\%$，但由于其侵袭性导致了大多数的死亡。早期诊断对于良好的预后至关重要：如果早期发现，黑色素瘤可以通过简单的门诊手术治愈，但如果在晚期诊断，其五年生存率将从超过$99\%$降至$32\%$（American
    Cancer Society, [2023](#bib.bib23)）。
- en: 'Two imaging modalities are commonly employed in automated skin lesion analysis (Daneshjou
    et al., [2022](#bib.bib109)): dermoscopic (microscopic) images and clinical (macroscopic)
    images. While dermoscopic images allow the inspection of lesion properties that
    are invisible to the naked eye, they are not always accessible even to dermatologists (Engasser
    and Warshaw, [2010](#bib.bib133)). On the other hand, clinical images acquired
    using conventional cameras are easily accessible but suffer from lower quality.
    Dermoscopy is a non-invasive skin imaging technique that aids in the diagnosis
    of skin lesions by allowing dermatologists to visualize sub-surface structures (Kittler
    et al., [2002](#bib.bib229)). However, even with dermoscopy, diagnostic accuracy
    can vary widely, ranging from $24\%$ to $77\%$, depending on the clinician’s level
    of expertise (Tran et al., [2005](#bib.bib378)). Moreover, dermoscopy may actually
    lower the diagnostic accuracy in the hands of inexperienced dermatologists (Binder
    et al., [1995](#bib.bib55)). Therefore, to minimize the diagnostic errors that
    result from the difficulty and the subjectivity of visual interpretation and to
    reduce the burden of skin diseases and limited access to dermatologists, the development
    of CAD systems is crucial.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动化皮肤病变分析中，通常使用两种影像模式（Daneshjou et al., [2022](#bib.bib109)）：皮肤镜（显微）图像和临床（宏观）图像。皮肤镜图像能够检查肉眼不可见的病变特征，但即使对皮肤科医生而言也并非总是可用（Engasser
    and Warshaw, [2010](#bib.bib133)）。另一方面，使用常规相机获取的临床图像易于获得，但质量较低。皮肤镜检查是一种非侵入性的皮肤成像技术，通过让皮肤科医生可视化皮下结构来帮助诊断皮肤病变（Kittler
    et al., [2002](#bib.bib229)）。然而，即使使用皮肤镜，诊断准确性也可能有很大差异，范围从$24\%$到$77\%$，取决于临床医生的专业水平（Tran
    et al., [2005](#bib.bib378)）。此外，皮肤镜在经验不足的皮肤科医生手中实际上可能会降低诊断准确性（Binder et al., [1995](#bib.bib55)）。因此，为了减少由于视觉解释的困难和主观性所导致的诊断错误，并减轻皮肤病的负担以及皮肤科医生的有限可及性，开发计算机辅助诊断系统是至关重要的。
- en: Segmentation is the partitioning of an image into meaningful regions. Semantic
    segmentation, in particular, assigns appropriate class labels to each region.
    For skin lesions, the task is almost always binary, separating the lesion from
    the surrounding skin. Automated skin lesion segmentation is hindered by illumination
    and contrast issues, intrinsic inter-class similarities and intra-class variability,
    occlusions, artifacts, and the diversity of imaging tools used. The lack of large
    datasets with ground-truth segmentation masks generated by experts compounds the
    problem, impeding both the training of models and their reliable evaluation. Skin
    lesion images are occluded by natural artifacts such as hair (Fig. [1(a)](#S1.F1.sf1
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation")),
    blood vessels (Fig. [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), and artificial ones such as
    surgical marker annotations (Fig. [1(c)](#S1.F1.sf3 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")), lens artifacts (dark
    corners) (Fig. [1(d)](#S1.F1.sf4 "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation")), and air bubbles (Fig. [1(e)](#S1.F1.sf5
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).
    Intrinsic factors such as lesion size and shape variation (Fig. [1(f)](#S1.F1.sf6
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation")
    and [1(g)](#S1.F1.sf7 "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation")), different skin colors (Fig. [1(h)](#S1.F1.sf8
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation")),
    low contrast (Fig. [1(i)](#S1.F1.sf9 "In Figure 1 ‣ 1 Introduction ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), and ambiguous boundaries (Fig. [1(h)](#S1.F1.sf8
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation"))
    complicate the automated segmentation of skin lesions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4ce85f98d07e7289005fc65bb3645cd2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: (a) Hairs
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2e88b01dcc49f95d30b1f512f7f24272.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: (b) Blood vessels
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c1de0f3633244e028d0617a52d8677a1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: (c) Surgical marking
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/723d4b16631977bfe6ad4cafd16ec889.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: (d) Irregular border and black frame
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d8834cede825ff2bba8ed22db26dffc6.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: (e) Bubbles
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c2ae928f72d89c60cbe48bea953d5b9c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: (f) Very small lesion
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ba57e8cced90f761b121a3f8a43f438.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: (g) Very large lesion
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b1ccb1ba0dee81e1a0d639338744f0eb.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: (h) Fuzzy border and variegated coloring
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/437b2937add4639427f7ead48a0bfbff.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: (i) Low contrast and color calibration chart
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Factors that complicate dermoscopy image segmentation (image source:
    ISIC 2016 dataset (Gutman et al., [2016](#bib.bib167))).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: <svg   height="576.17" overflow="visible" version="1.1" width="580.52"><g transform="translate(0,576.17)
    matrix(1 0 0 -1 0 0) translate(283.59,0) translate(0,292.58)" fill="#000000" stroke="#000000"
    stroke-width="1.2pt"><g transform="matrix(1.0 0.0 0.0 1.0 -68.9 5.9)" fill="#000000"
    stroke="#000000"><foreignobject width="137.8" height="31.18" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">DL-based Skin Lesion Segmentation <g fill="#99FF99"
    stroke="#99FF99"><path d="M -94.9 -139.19 C -94.9 -114.73 -114.73 -94.9 -139.19
    -94.9 C -163.66 -94.9 -183.49 -114.73 -183.49 -139.19 C -183.49 -163.66 -163.66
    -183.49 -139.19 -183.49 C -114.73 -183.49 -94.9 -163.66 -94.9 -139.19 Z M -139.19
    -139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -178.56 -134.35)" fill="#000000"
    stroke="#000000"><foreignobject width="78.74" height="29.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Future Research §[5](#S5 "5 Discussion and
    Future Research ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M -55.68 -55.68 M -33.28 -71.36 C -50.06 -63.54 -63.54 -50.06
    -71.36 -33.28 C -63.04 -51.12 -72.34 -66.86 -86.26 -80.78 L -80.78 -86.26 C -66.86
    -72.34 -51.12 -63.04 -33.28 -71.36 Z M -89.47 -94.95 L -80.78 -86.26 L -86.26
    -80.78 L -94.95 -89.47 Z M -86.26 -80.78 M -99.05 -120.47 C -103.45 -111.03 -111.03
    -103.45 -120.47 -99.05 C -110.44 -103.73 -102.78 -97.3 -94.95 -89.47 L -89.47
    -94.95 C -97.3 -102.78 -103.73 -110.44 -99.05 -120.47 Z"></path></clippath><g
    clip-path="url(#pgfcp9)"><g transform="matrix(1.0 0.0 0.0 1.0 -55.68 -55.68)"><g
    fill="#FFCC99"><path d="M 54.71 -56.66 L 111.36 -0.01 L -0.01 111.36 L -56.66
    54.71 Z M -0.01 111.36" style="stroke:none"></path></g><g fill="#99FF99"><path
    d="M 4.46 -106.9 L -27.83 -139.2 L -139.2 -27.83 L -106.9 4.46 Z M -139.2 -27.83"
    style="stroke:none"><g transform="matrix(-0.37585 -0.37585 0.80185 -0.80185 -26.1
    -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#99CCCC" stroke="#99CCCC"><path d="M 183.49 -139.19 C 183.49 -114.73 163.66
    -94.9 139.19 -94.9 C 114.73 -94.9 94.9 -114.73 94.9 -139.19 C 94.9 -163.66 114.73
    -183.49 139.19 -183.49 C 163.66 -183.49 183.49 -163.66 183.49 -139.19 Z M 139.19
    -139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 99.82 -142.65)" fill="#000000"
    stroke="#000000"><foreignobject width="78.74" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Evaluation §[4](#S4 "4 Evaluation ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")</foreignobject></g> <clippath
    ><path d="M 55.68 -55.68 M 71.36 -33.28 C 63.54 -50.06 50.06 -63.54 33.28 -71.36
    C 51.12 -63.04 66.86 -72.34 80.78 -86.26 L 86.26 -80.78 C 72.34 -66.86 63.04 -51.12
    71.36 -33.28 Z M 94.95 -89.47 L 86.26 -80.78 L 80.78 -86.26 L 89.47 -94.95 Z M
    80.78 -86.26 M 120.47 -99.05 C 111.03 -103.45 103.45 -111.03 99.05 -120.47 C 103.73
    -110.44 97.3 -102.78 89.47 -94.95 L 94.95 -89.47 C 102.78 -97.3 110.44 -103.73
    120.47 -99.05 Z"></path></clippath><g clip-path="url(#pgfcp11)"><g transform="matrix(1.0
    0.0 0.0 1.0 55.68 -55.68)"><g fill="#FFCC99"><path d="M 56.66 54.71 L 0.01 111.36
    L -111.36 -0.01 L -54.71 -56.66 Z M -111.36 -0.01" style="stroke:none"></path></g><g
    fill="#99CCCC"><path d="M 106.9 4.46 L 139.2 -27.83 L 27.83 -139.2 L -4.46 -106.9
    Z M 27.83 -139.2" style="stroke:none"><g transform="matrix(0.37585 -0.37585 0.80185
    0.80185 26.1 -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#99CCCC" stroke="#99CCCC"><path d="M 173.64 -257.3 C 173.64 -238.28 158.22
    -222.86 139.19 -222.86 C 120.17 -222.86 104.75 -238.28 104.75 -257.3 C 104.75
    -276.33 120.17 -291.75 139.19 -291.75 C 158.22 -291.75 173.64 -276.33 173.64 -257.3
    Z M 139.19 -257.3"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 109.67 -261.46)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Metrics §[4.3](#S4.SS3 "4.3 Evaluation Metrics
    ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#99CCCC"><path d="M 139.19 -183.49 M 154.34 -180.81 C 144.56 -184.38
    133.83 -184.38 124.05 -180.81 C 134.45 -184.6 136.18 -194.56 136.18 -205.63 L
    142.21 -205.63 C 142.21 -194.56 143.94 -184.6 154.34 -180.81 Z M 142.21 -205.63
    L 142.21 -205.63 L 136.18 -205.63 L 136.18 -205.63 Z M 136.18 -205.63 M 150.98
    -224.93 C 143.37 -222.16 135.02 -222.16 127.41 -224.93 C 135.5 -221.99 136.18
    -214.24 136.18 -205.63 L 142.21 -205.63 C 142.21 -214.24 142.88 -221.99 150.98
    -224.93 Z" style="stroke:none"></path></g><g fill="#99CCCC" stroke="#99CCCC"><path
    d="M 261.5 -222.71 C 261.5 -201.29 244.13 -183.92 222.71 -183.92 C 201.29 -183.92
    183.92 -201.29 183.92 -222.71 C 183.92 -244.13 201.29 -261.5 222.71 -261.5 C 244.13
    -261.5 261.5 -244.13 261.5 -222.71 Z M 222.71 -222.71"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 193.18 -210.26)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="44.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Inter- Annotator
    Agreement §[4.2](#S4.SS2 "4.2 Inter-Annotator Agreement ‣ 4 Evaluation ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")</foreignobject></g> <g fill="#99CCCC"><path
    d="M 170.51 -170.51 M 179.34 -157.91 C 174.94 -167.35 167.35 -174.94 157.91 -179.34
    C 167.95 -174.66 175.94 -180.74 183.77 -188.57 L 188.57 -183.77 C 180.74 -175.94
    174.66 -167.95 179.34 -157.91 Z M 183.97 -179.17 L 188.57 -183.77 L 183.77 -188.57
    L 179.17 -183.97 Z M 183.77 -188.57 M 206.32 -187.55 C 198.05 -191.41 191.41 -198.05
    187.55 -206.32 C 191.65 -197.53 186.02 -190.82 179.17 -183.97 L 183.97 -179.17
    C 190.82 -186.02 197.53 -191.65 206.32 -187.55 Z" style="stroke:none"></path></g><g
    fill="#99CCCC" stroke="#99CCCC"><path d="M 296.1 -139.19 C 296.1 -117.77 278.73
    -100.4 257.3 -100.4 C 235.88 -100.4 218.51 -117.77 218.51 -139.19 C 218.51 -160.62
    235.88 -177.98 257.3 -177.98 C 278.73 -177.98 296.1 -160.62 296.1 -139.19 Z M
    257.3 -139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 227.78 -126.74)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="44.28" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Segmentation Annotation §[4.1](#S4.SS1 "4.1
    Segmentation Annotation ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")</foreignobject></g> <g fill="#99CCCC"><path d="M 183.49 -139.19
    M 180.81 -124.05 C 184.38 -133.83 184.38 -144.56 180.81 -154.34 C 184.6 -143.94
    194.56 -142.59 205.63 -142.59 L 205.63 -135.8 C 194.56 -135.8 184.6 -134.45 180.81
    -124.05 Z M 205.63 -142.59 h -6.51 v 6.79 h 6.51 Z M 220.85 -125.93 C 217.73 -134.5
    217.73 -143.89 220.85 -152.46 C 217.54 -143.35 208.82 -142.59 199.12 -142.59 L
    199.12 -135.8 C 208.82 -135.8 217.54 -135.04 220.85 -125.93 Z" style="stroke:none"></path></g><g
    fill="#FFB3B3" stroke="#FFB3B3"><path d="M 183.49 139.19 C 183.49 163.66 163.66
    183.49 139.19 183.49 C 114.73 183.49 94.9 163.66 94.9 139.19 C 94.9 114.73 114.73
    94.9 139.19 94.9 C 163.66 94.9 183.49 114.73 183.49 139.19 Z M 139.19 139.19"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 99.82 144.04)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Model
    Design & Training §[3](#S3 "3 Model Design and Training ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation")</foreignobject></g> <clippath ><path d="M 55.68
    55.68 M 33.28 71.36 C 50.06 63.54 63.54 50.06 71.36 33.28 C 63.04 51.12 72.34
    66.86 86.26 80.78 L 80.78 86.26 C 66.86 72.34 51.12 63.04 33.28 71.36 Z M 89.47
    94.95 L 80.78 86.26 L 86.26 80.78 L 94.95 89.47 Z M 86.26 80.78 M 99.05 120.47
    C 103.45 111.03 111.03 103.45 120.47 99.05 C 110.44 103.73 102.78 97.3 94.95 89.47
    L 89.47 94.95 C 97.3 102.78 103.73 110.44 99.05 120.47 Z"></path></clippath><g
    clip-path="url(#pgfcp13)"><g transform="matrix(1.0 0.0 0.0 1.0 55.68 55.68)"><g
    fill="#FFCC99"><path d="M -54.71 56.66 L -111.36 0.01 L 0.01 -111.36 L 56.66 -54.71
    Z M 0.01 -111.36" style="stroke:none"></path></g><g fill="#FFB3B3"><path d="M
    -4.46 106.9 L 27.83 139.2 L 139.2 27.83 L 106.9 -4.46 Z M 139.2 27.83" style="stroke:none"><g
    transform="matrix(0.37585 0.37585 -0.80185 0.80185 26.1 26.1)"><defs><lineargradient
    ></lineargradient></defs></g></path></g></g></g><g fill="#FFB3B3" stroke="#FFB3B3"><path
    d="M 282.76 184.39 C 282.76 203.42 267.34 218.84 248.31 218.84 C 229.29 218.84
    213.86 203.42 213.86 184.39 C 213.86 165.37 229.29 149.95 248.31 149.95 C 267.34
    149.95 282.76 165.37 282.76 184.39 Z M 248.31 184.39"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 218.78 188.55)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Loss Functions
    §[3.2](#S3.SS2 "3.2 Loss Functions ‣ 3 Model Design and Training ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")</foreignobject></g> <g fill="#FFB3B3"><path
    d="M 180.11 156.14 M 171.85 169.12 C 178.88 161.44 182.99 151.53 183.44 141.13
    C 182.96 152.19 191.5 157.6 201.73 161.83 L 199.42 167.4 C 189.19 163.17 179.33
    160.95 171.85 169.12 Z M 199.42 167.4 L 199.42 167.4 L 201.73 161.83 L 201.72
    161.83 Z M 201.73 161.83 M 213.89 182.89 C 214.25 174.8 217.44 167.09 222.91 161.12
    C 217.09 167.47 209.68 165.13 201.72 161.83 L 199.42 167.4 C 207.37 170.7 214.27
    174.29 213.89 182.89 Z" style="stroke:none"></path></g><g fill="#FFB3B3" stroke="#FFB3B3"><path
    d="M 218.84 248.31 C 218.84 267.34 203.42 282.76 184.39 282.76 C 165.37 282.76
    149.95 267.34 149.95 248.31 C 149.95 229.29 165.37 213.86 184.39 213.86 C 203.42
    213.86 218.84 229.29 218.84 248.31 Z M 184.39 248.31"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 154.87 252.46)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Model Architecture
    §[3.1](#S3.SS1 "3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation")</foreignobject></g> <g fill="#FFB3B3"><path
    d="M 156.14 180.11 M 141.13 183.44 C 151.53 182.99 161.44 178.88 169.12 171.85
    C 160.95 179.33 163.17 189.19 167.4 199.42 L 161.83 201.73 C 157.6 191.5 152.19
    182.96 141.13 183.44 Z M 161.83 201.72 L 161.83 201.73 L 167.4 199.42 L 167.4
    199.42 Z M 167.4 199.42 M 161.12 222.91 C 167.09 217.44 174.8 214.25 182.89 213.89
    C 174.29 214.27 170.7 207.37 167.4 199.42 L 161.83 201.72 C 165.13 209.68 167.47
    217.09 161.12 222.91 Z" style="stroke:none"></path></g><g fill="#B3B3FF" stroke="#B3B3FF"><path
    d="M -94.9 139.19 C -94.9 163.66 -114.73 183.49 -139.19 183.49 C -163.66 183.49
    -183.49 163.66 -183.49 139.19 C -183.49 114.73 -163.66 94.9 -139.19 94.9 C -114.73
    94.9 -94.9 114.73 -94.9 139.19 Z M -139.19 139.19"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -178.56 135.74)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Input Data
    §[2](#S2 "2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M -55.68 55.68 M -71.36 33.28 C -63.54 50.06 -50.06 63.54
    -33.28 71.36 C -51.12 63.04 -66.86 72.34 -80.78 86.26 L -86.26 80.78 C -72.34
    66.86 -63.04 51.12 -71.36 33.28 Z M -94.95 89.47 L -86.26 80.78 L -80.78 86.26
    L -89.47 94.95 Z M -80.78 86.26 M -120.47 99.05 C -111.03 103.45 -103.45 111.03
    -99.05 120.47 C -103.73 110.44 -97.3 102.78 -89.47 94.95 L -94.95 89.47 C -102.78
    97.3 -110.44 103.73 -120.47 99.05 Z"></path></clippath><g clip-path="url(#pgfcp15)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -55.68 55.68)"><g fill="#FFCC99"><path d="M
    -56.66 -54.71 L -0.01 -111.36 L 111.36 0.01 L 54.71 56.66 Z M 111.36 0.01" style="stroke:none"></path></g><g
    fill="#B3B3FF"><path d="M -106.9 -4.46 L -139.2 27.83 L -27.83 139.2 L 4.46 106.9
    Z M -27.83 139.2" style="stroke:none"><g transform="matrix(-0.37585 0.37585 -0.80185
    -0.80185 -26.1 26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#B3B3FF" stroke="#B3B3FF"><path d="M -59.55 248.31 C -59.55 267.34 -74.97
    282.76 -93.99 282.76 C -113.02 282.76 -128.44 267.34 -128.44 248.31 C -128.44
    229.29 -113.02 213.86 -93.99 213.86 C -74.97 213.86 -59.55 229.29 -59.55 248.31
    Z M -93.99 248.31"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -123.52 252.46)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Image Preprocessing §[2.4](#S2.SS4 "2.4 Image
    Preprocessing ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#B3B3FF"><path d="M -122.24 180.11 M -137.26 183.44 C -126.86 182.99
    -116.95 178.88 -109.27 171.85 C -117.43 179.33 -115.22 189.19 -110.98 199.42 L
    -116.55 201.73 C -120.79 191.5 -126.2 182.96 -137.26 183.44 Z M -116.56 201.72
    L -116.55 201.73 L -110.98 199.42 L -110.99 199.42 Z M -110.98 199.42 M -117.27
    222.91 C -111.3 217.44 -103.59 214.25 -95.5 213.89 C -104.1 214.27 -107.69 207.37
    -110.99 199.42 L -116.56 201.72 C -113.26 209.68 -110.92 217.09 -117.27 222.91
    Z" style="stroke:none"></path></g><g fill="#B3B3FF" stroke="#B3B3FF"><path d="M
    -149.95 248.31 C -149.95 267.34 -165.37 282.76 -184.39 282.76 C -203.42 282.76
    -218.84 267.34 -218.84 248.31 C -218.84 229.29 -203.42 213.86 -184.39 213.86 C
    -165.37 213.86 -149.95 229.29 -149.95 248.31 Z M -184.39 248.31"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -213.92 252.46)" fill="#000000" stroke="#000000"><foreignobject
    width="59.06" height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Supervision
    §[2.3](#S2.SS3 "2.3 Supervised, Semi-supervised, Weakly supervised, Self-supervised
    learning ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#B3B3FF"><path d="M -156.14 180.11 M -169.12 171.85 C -161.44 178.88
    -151.53 182.99 -141.13 183.44 C -152.19 182.96 -157.6 191.5 -161.83 201.73 L -167.4
    199.42 C -163.17 189.19 -160.95 179.33 -169.12 171.85 Z M -167.4 199.42 L -167.4
    199.42 L -161.83 201.73 L -161.83 201.72 Z M -161.83 201.73 M -182.89 213.89 C
    -174.8 214.25 -167.09 217.44 -161.12 222.91 C -167.47 217.09 -165.13 209.68 -161.83
    201.72 L -167.4 199.42 C -170.7 207.37 -174.29 214.27 -182.89 213.89 Z" style="stroke:none"></path></g><g
    fill="#B3B3FF" stroke="#B3B3FF"><path d="M -213.86 184.39 C -213.86 203.42 -229.29
    218.84 -248.31 218.84 C -267.34 218.84 -282.76 203.42 -282.76 184.39 C -282.76
    165.37 -267.34 149.95 -248.31 149.95 C -229.29 149.95 -213.86 165.37 -213.86 184.39
    Z M -248.31 184.39"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -277.84 188.55)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Synthetic Data §[2.2](#S2.SS2 "2.2 Synthetic
    Data Generation ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#B3B3FF"><path d="M -180.11 156.14 M -183.44 141.13 C -182.99 151.53
    -178.88 161.44 -171.85 169.12 C -179.33 160.95 -189.19 163.17 -199.42 167.4 L
    -201.73 161.83 C -191.5 157.6 -182.96 152.19 -183.44 141.13 Z M -201.72 161.83
    L -201.73 161.83 L -199.42 167.4 L -199.42 167.4 Z M -199.42 167.4 M -222.91 161.12
    C -217.44 167.09 -214.25 174.8 -213.89 182.89 C -214.27 174.29 -207.37 170.7 -199.42
    167.4 L -201.72 161.83 C -209.68 165.13 -217.09 167.47 -222.91 161.12 Z" style="stroke:none"></path></g><g
    fill="#B3B3FF" stroke="#B3B3FF"><path d="M -213.86 93.99 C -213.86 113.02 -229.29
    128.44 -248.31 128.44 C -267.34 128.44 -282.76 113.02 -282.76 93.99 C -282.76
    74.97 -267.34 59.55 -248.31 59.55 C -229.29 59.55 -213.86 74.97 -213.86 93.99
    Z M -248.31 93.99"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -277.84 89.84)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Datasets §[2.1](#S2.SS1 "2.1 Datasets ‣ 2 Input
    Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g><g
    fill="#B3B3FF"><path d="M -180.11 122.24 M -171.85 109.27 C -178.88 116.95 -182.99
    126.86 -183.44 137.26 C -182.96 126.2 -191.5 120.79 -201.73 116.55 L -199.42 110.98
    C -189.19 115.22 -179.33 117.43 -171.85 109.27 Z M -199.42 110.99 L -199.42 110.98
    L -201.73 116.55 L -201.72 116.56 Z M -201.73 116.55 M -213.89 95.5 C -214.25
    103.59 -217.44 111.3 -222.91 117.27 C -217.09 110.92 -209.68 113.26 -201.72 116.56
    L -199.42 110.99 C -207.37 107.69 -214.27 104.1 -213.89 95.5 Z" style="stroke:none"></path></g>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: An overview of the various components of this review. We structure
    the review based on the different elements of a DL-based segmentation pipeline
    and conclude it with discussions on future potential research directions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：本综述各个组成部分的概述。我们根据 DL 基于分割管道的不同元素来构建综述，并以对未来潜在研究方向的讨论作为结论。
- en: Before the deep learning (DL) revolution, segmentation was based on classical
    image processing and machine learning techniques such as adaptive thresholding (Green
    et al., [1994](#bib.bib157); Celebi et al., [2013](#bib.bib81)), active contours (Erkol
    et al., [2005](#bib.bib134)), region growing (Iyatomi et al., [2006](#bib.bib189);
    Celebi et al., [2007a](#bib.bib73)), unsupervised clustering (Gómez et al., [2007](#bib.bib150)),
    and support vector machines (Zortea et al., [2011](#bib.bib458)). These approaches
    depend on hand-crafted features, which are difficult to engineer and often limit
    invariance and discriminative power from the outset. As a result, such conventional
    segmentation algorithms do not always perform well on larger and more complex
    datasets. In contrast, DL integrates feature extraction and task-specific decision
    seamlessly, and does not just cope with, but actually requires larger datasets.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习（DL）革命之前，分割基于经典的图像处理和机器学习技术，例如自适应阈值（Green et al., [1994](#bib.bib157);
    Celebi et al., [2013](#bib.bib81)），主动轮廓（Erkol et al., [2005](#bib.bib134)），区域生长（Iyatomi
    et al., [2006](#bib.bib189); Celebi et al., [2007a](#bib.bib73)），无监督聚类（Gómez et
    al., [2007](#bib.bib150)），以及支持向量机（Zortea et al., [2011](#bib.bib458)）。这些方法依赖于手工制作的特征，这些特征难以工程化，并且通常从一开始就限制了不变性和辨别能力。因此，这些传统的分割算法在更大且更复杂的数据集上并不总是表现良好。相比之下，DL
    无缝集成了特征提取和任务特定决策，并且不仅应对大数据集，实际上还需要更大的数据集。
- en: '*Survey of surveys.* Celebi et al. ([2009b](#bib.bib76)) reviewed 18 skin lesion
    segmentation algorithms for dermoscopic images, published between 1998 and 2008,
    with their required preprocessing and postprocessing steps. Celebi et al. ([2015b](#bib.bib82))
    later extended their work with 32 additional algorithms published between 2009
    and 2014, discussing performance evaluation and computational requirements of
    each approach, and suggesting guidelines for future works. Both surveys appeared
    before DL was widely adopted for skin lesion segmentation, but cover all the important
    works based on classical image processing and machine learning. Adegun and Viriri
    ([2020a](#bib.bib11)) reviewed the literature on DL-based skin image analysis,
    with an emphasis on the best-performing algorithms in the ISIC (International
    Skin Imaging Collaboration) Skin Image Analysis Challenges 2018 (Codella et al.,
    [2019](#bib.bib96)) and 2019 (Tschandl et al., [2018](#bib.bib381); Codella et al.,
    [2018](#bib.bib98); Combalia et al., [2019](#bib.bib101)). However, since their
    review focused on the ISIC Challenges 2018 and 2019, it is more general as it
    covers both lesion classification and segmentation. Consequently, the number of
    papers surveyed for skin lesion segmentation by Adegun and Viriri ([2020a](#bib.bib11))
    is almost an order of magnitude smaller than that in this review.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*文献综述。* Celebi et al. ([2009b](#bib.bib76)) 综述了 18 种用于皮肤病变 dermoscopic 图像的分割算法，这些算法发布于
    1998 到 2008 年间，并介绍了它们所需的预处理和后处理步骤。Celebi et al. ([2015b](#bib.bib82)) 随后扩展了他们的工作，增加了
    32 种在 2009 到 2014 年间发布的算法，讨论了每种方法的性能评估和计算需求，并提出了未来工作的指导方针。这些综述都出现在 DL 被广泛应用于皮肤病变分割之前，但涵盖了所有基于经典图像处理和机器学习的重要工作。Adegun
    和 Viriri ([2020a](#bib.bib11)) 综述了基于 DL 的皮肤图像分析文献，重点关注 ISIC（国际皮肤成像合作）皮肤图像分析挑战
    2018（Codella et al., [2019](#bib.bib96)）和 2019（Tschandl et al., [2018](#bib.bib381);
    Codella et al., [2018](#bib.bib98); Combalia et al., [2019](#bib.bib101)）中的最佳表现算法。然而，由于他们的综述关注于
    ISIC 挑战 2018 和 2019，因此它更为通用，涵盖了病变分类和分割。因此，Adegun 和 Viriri ([2020a](#bib.bib11))
    对皮肤病变分割的文献综述的论文数量比本综述要少一个数量级。'
- en: '*Main contributions.* No existing survey approaches the present work in breadth
    or depth, as we cross-examine $177$ research papers that deal with the automated
    segmentation of skin lesions in clinical and dermoscopic images. We analyze the
    works along several dimensions, including input data (datasets, preprocessing,
    and synthetic data generation), model design (architecture, modules, and losses),
    and evaluation (data annotation and evaluation metrics). We discuss these dimensions
    both from the viewpoint of select seminal works, and from a systematic viewpoint,
    examining how those choices have influenced current trends, and how their limitations
    should be addressed. We summarize all examined works in a comprehensive table
    to facilitate comparisons.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*主要贡献。* 现有的调查没有在广度或深度上接近本工作，因为我们交叉审查了 $177$ 篇涉及临床和皮肤镜图像中皮肤病变自动分割的研究论文。我们从多个维度分析了这些工作，包括输入数据（数据集、预处理和合成数据生成）、模型设计（架构、模块和损失）以及评估（数据注释和评估指标）。我们从一些重要工作的视角以及系统视角讨论了这些维度，审视这些选择如何影响当前趋势以及如何解决其局限性。我们在一张综合表格中总结了所有审查过的工作，以便于比较。'
- en: '*Search strategy.* We searched DBLP and Arxiv Sanity Preserver for all scholarly
    publications: peer-reviewed journal papers, papers published in the proceedings
    of conferences or workshops, and non-peer-reviewed preprints from 2014 to 2022\.
    The DBLP search query was (conv* | trans* | deep | neural | learn*) (skin | derm*)
    (segment* | delineat* | extract* | localiz*), thus restricting our search to DL-based
    works involving skin and segmentation. We use DBLP for our literature search because
    (a) it allows for customized search queries and lists, and (b) we did not find
    any relevant publications on other platforms (Google Scholar and PubMed) that
    were not indexed by DBLP. For unpublished preprints, we also searched on Arxiv
    Sanity Preserver using a similar query⁶⁶6Arxiv Sanity Preserver: [https://www.arxiv-sanity-lite.com/search?q=segmentation+skin+melanoma+deep+learning+convolution+transformer](https://www.arxiv-sanity-lite.com/search?q=segmentation+skin+melanoma+deep+learning+convolution+transformer).
    We filtered our search results to remove false positives ($31$ papers) and included
    only papers related to skin lesion segmentation. We excluded papers that focused
    on general skin segmentation and general skin conditions (e.g., psoriasis, acne,
    or certain sub-types of skin lesions). We also included unpublished preprints
    from arXiv, which (a) passed minimum quality checks levels and (b) had at least
    10 citations, and excluded those that were clearly of low quality. In particular,
    papers that had one or more of the following were excluded from this survey: (a)
    missing quantitative results, (b) missing important sections such as Abstract
    or Methods, (c) conspicuously poor writing quality, and (d) no methodological
    contribution. This led to the filtering out of papers of visibly low quality ((a-c)
    criteria above; $18$ papers) and those with no methodological contribution ($20$ papers).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*搜索策略。* 我们在 DBLP 和 Arxiv Sanity Preserver 上搜索了所有学术出版物：同行评审的期刊论文、会议或研讨会论文以及
    2014 年至 2022 年间的非同行评审预印本。DBLP 的搜索查询为 (conv* | trans* | deep | neural | learn*)
    (skin | derm*) (segment* | delineat* | extract* | localiz*)，从而将我们的搜索限制在涉及皮肤和分割的基于深度学习的工作上。我们使用
    DBLP 进行文献搜索的原因是 (a) 它允许定制搜索查询和列表，以及 (b) 我们在其他平台（Google Scholar 和 PubMed）上没有找到
    DBLP 未收录的相关出版物。对于未发表的预印本，我们还在 Arxiv Sanity Preserver 上使用类似的查询进行了搜索⁶⁶6Arxiv Sanity
    Preserver: [https://www.arxiv-sanity-lite.com/search?q=segmentation+skin+melanoma+deep+learning+convolution+transformer](https://www.arxiv-sanity-lite.com/search?q=segmentation+skin+melanoma+deep+learning+convolution+transformer)。我们过滤了搜索结果以去除假阳性（$31$
    篇论文），仅包括与皮肤病变分割相关的论文。我们排除了关注一般皮肤分割和一般皮肤状况（例如银屑病、痤疮或某些皮肤病变亚型）的论文。我们还包括了来自 arXiv
    的未发表预印本，这些预印本 (a) 通过了最低质量检查级别，并且 (b) 至少有 10 次引用，排除了那些明显低质量的论文。特别地，以下有一个或多个特征的论文被排除在本次调查之外：(a)
    缺少定量结果，(b) 缺少重要部分，如摘要或方法，(c) 写作质量显著差，(d) 没有方法学贡献。这导致了过滤掉明显低质量的论文（上述 (a-c) 标准；$18$
    篇论文）和没有方法学贡献的论文（$20$ 篇论文）。'
- en: 'The remaining text is organized as follows: in Section [2](#S2 "2 Input Data
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation"), we introduce the publicly
    available datasets and discuss preprocessing and synthetic data generation; in
    Section [3](#S3 "3 Model Design and Training ‣ A Survey on Deep Learning for Skin
    Lesion Segmentation"), we review the various network architectures used in deep
    segmentation models and discuss how deep models benefit from these networks. We
    also describe various loss functions designed either for general use or specifically
    for skin lesion segmentation. In Section [4](#S4 "4 Evaluation ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation"), we detail segmentation evaluation techniques
    and measures. Finally, in Section [5](#S5 "5 Discussion and Future Research ‣
    A Survey on Deep Learning for Skin Lesion Segmentation"), we discuss the open
    challenges in DL-based skin lesion segmentation and conclude our survey. A visual
    overview of the structure of this survey is presented in Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的文本组织如下：在第[2](#S2 "2 Input Data ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")节中，我们介绍了公开可用的数据集，并讨论了预处理和合成数据生成；在第[3](#S3 "3 Model Design and Training
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")节中，我们回顾了用于深度分割模型的各种网络架构，并讨论了深度模型如何从这些网络中获益。我们还描述了为一般用途或专门用于皮肤病变分割而设计的各种损失函数。在第[4](#S4
    "4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation")节中，我们详细说明了分割评估技术和措施。最后，在第[5](#S5
    "5 Discussion and Future Research ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")节中，我们讨论了基于深度学习的皮肤病变分割中的开放挑战，并总结了我们的调查。图[2](#S1.F2 "Figure 2 ‣ 1
    Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation")提供了本调查的结构的视觉概述。
- en: 2 Input Data
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 输入数据
- en: Obtaining data in sufficient quantity and quality is often a significant obstacle
    to developing effective segmentation models. State-of-the-art segmentation models
    have a huge number of adjustable parameters that allow them to generalize well,
    provided they are trained on massive labeled datasets (Sun et al., [2017](#bib.bib363);
    Buslaev et al., [2020](#bib.bib68)). Unfortunately, skin lesion datasets—like
    most medical image datasets (Asgari Taghanaki et al., [2021](#bib.bib27))—tend
    to be small (Curiel-Lewandrowski et al., [2019](#bib.bib106)) due to issues such
    as copyright, patient privacy, acquisition and annotation cost, standardization,
    and scarcity of many pathologies of interest. The two most common modalities used
    in the training of skin lesion segmentation models are clinical images, which
    are close-ups of the lesions acquired using conventional cameras, and dermoscopic
    images, which are acquired using dermoscopy, a non-invasive skin imaging through
    optical magnification, and either liquid immersion and low angle-of-incidence
    lighting, or cross-polarized lighting. Dermoscopy eliminates skin surface reflections (Kittler
    et al., [2002](#bib.bib229)), reveals subsurface skin structures, and allows the
    identification of dozens of morphological features such as atypical pigment networks,
    dots/globules, streaks, blue-white areas, and blotches (Menzies et al., [2003](#bib.bib276)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 获得足够数量和质量的数据通常是开发有效分割模型的重大障碍。最先进的分割模型具有大量可调参数，这些参数可以使它们在训练于大量标注数据集的情况下实现良好的泛化（Sun
    et al., [2017](#bib.bib363); Buslaev et al., [2020](#bib.bib68)）。不幸的是，皮肤病变数据集——像大多数医学图像数据集（Asgari
    Taghanaki et al., [2021](#bib.bib27)）——由于版权、患者隐私、采集和注释成本、标准化以及许多感兴趣病理的稀缺等问题，往往较小（Curiel-Lewandrowski
    et al., [2019](#bib.bib106)）。在皮肤病变分割模型的训练中，最常用的两种模式是临床图像，这些图像是使用传统相机拍摄的病变特写，以及皮肤镜图像，这些图像通过光学放大、液体浸泡和低入射角照明或交叉偏振照明的皮肤镜技术获得。皮肤镜可以消除皮肤表面反射（Kittler
    et al., [2002](#bib.bib229)），揭示皮肤下的结构，并允许识别几十种形态特征，如非典型色素网络、点/小球、条纹、蓝白色区域和斑点（Menzies
    et al., [2003](#bib.bib276)）。
- en: Annotation is often the greatest barrier for increasing the amount of data.
    Objective evaluation of segmentation often requires laborious *region-based annotation*,
    in which an expert manually outlines the region where the lesion (or a clinical
    feature) appears in the image. By contrast, *textual annotation* may involve diagnosis
    (e.g., melanoma, carcinoma, benign nevi), presence/absence/score of dermoscopic
    features (e.g., pigment networks, blue-white areas, streaks, globules), diagnostic
    strategy (e.g., pattern analysis, ABCD rule, 7-point checklist, 3-point checklist),
    clinical metadata (e.g., sex, age, anatomic site, familial history), and other
    details (e.g., timestamp, camera model) (Caffery et al., [2018](#bib.bib69)).
    We extensively discuss the image annotation issue in Section [4.1](#S4.SS1 "4.1
    Segmentation Annotation ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation").
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注释通常是增加数据量的最大障碍。客观评价分割常常需要繁琐的*区域基础注释*，专家手动描绘图像中病变（或临床特征）出现的区域。相比之下，*文本注释*可能涉及诊断（例如，黑色素瘤、癌症、良性痣）、皮肤镜特征的存在/缺失/评分（例如，色素网络、蓝白区、条纹、球形体）、诊断策略（例如，模式分析、ABCD规则、7点检查表、3点检查表）、临床元数据（例如，性别、年龄、解剖部位、家族历史）以及其他细节（例如，时间戳、相机型号）（Caffery等，
    [2018](#bib.bib69)）。我们在第[4.1节](#S4.SS1 "4.1 Segmentation Annotation ‣ 4 Evaluation
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")中深入讨论了图像注释问题。
- en: 2.1 Datasets
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 数据集
- en: The availability of larger, more diverse, and better-annotated datasets is one
    of the main driving factors for the advances in skin image analysis in the past
    decade (Marchetti et al., [2018](#bib.bib270); Celebi et al., [2019](#bib.bib74)).
    Works in skin image analysis date back to the 1980s (Vanker and Van Stoecker,
    [1984](#bib.bib391); Dhawan et al., [1984](#bib.bib122)), but until the mid-2000s,
    these works used small, private datasets, containing a few hundred images.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 较大、更多样化和更好注释的数据集的可用性是过去十年皮肤图像分析进展的主要推动因素之一（Marchetti等，[2018](#bib.bib270)；Celebi等，[2019](#bib.bib74)）。皮肤图像分析的研究可以追溯到1980年代（Vanker和Van
    Stoecker，[1984](#bib.bib391)；Dhawan等，[1984](#bib.bib122)），但直到2000年代中期，这些研究才使用包含几百张图像的小型私有数据集。
- en: 'The *Interactive Atlas of Dermoscopy* (sometimes called the *Edra Atlas*, in
    reference to the publisher) by Argenziano et al. ([2000](#bib.bib25)) included
    a CD-ROM with $1,039$ dermoscopy images ($26\%$ melanomas, $4\%$ carcinomas, $70\%$
    nevi) of $1,024\times 683$ pixels, acquired by three European university hospitals
    (University of Graz, Austria, University of Naples, Italy, and University of Florence,
    Italy). The works of Celebi et al. ([2007b](#bib.bib78), [2008](#bib.bib77)) popularized
    the dataset in the dermoscopy image analysis community, where it became a de facto
    evaluation standard for almost a decade, until the much larger ISIC Archive datasets
    (see below) became available. Recently, Kawahara et al. ([2019](#bib.bib219))
    placed this valuable dataset, along with additional textual annotations based
    on the 7-point checklist, in public domain under the name *derm7pt*. Shortly after
    the publication of the Interactive Atlas of Dermoscopy, Menzies et al. ([2003](#bib.bib276))
    published *An Atlas of Surface Microscopy of Pigmented Skin Lesions: Dermoscopy*,
    with a CD-ROM containing $217$ dermoscopic images ($39\%$ melanomas, $7\%$ carcinomas,
    $54\%$ nevi) of $712\times 454$ pixels, acquired at the Sydney Melanoma Unit,
    Australia.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*皮肤镜互动图谱*（有时称为*Edra图谱*，以出版商命名）由Argenziano等（[2000](#bib.bib25)）出版，包含一个CD-ROM，内有$1,039$张皮肤镜图像（$26\%$黑色素瘤，$4\%$癌症，$70\%$痣），分辨率为$1,024\times
    683$像素，由三个欧洲大学医院（奥地利格拉茨大学、意大利那不勒斯大学和意大利佛罗伦萨大学）采集。Celebi等（[2007b](#bib.bib78)，[2008](#bib.bib77)）的研究在皮肤镜图像分析社区推广了该数据集，成为近十年来的事实标准，直到更大的ISIC档案数据集（见下文）可用。最近，Kawahara等（[2019](#bib.bib219)）将这一宝贵数据集以及基于7点检查表的附加文本注释，公开发布为*derm7pt*。在皮肤镜互动图谱出版后不久，Menzies等（[2003](#bib.bib276)）出版了*色素皮肤病变表面显微镜图谱：皮肤镜*，其中的CD-ROM包含$217$张皮肤镜图像（$39\%$黑色素瘤，$7\%$癌症，$54\%$痣），分辨率为$712\times
    454$像素，由澳大利亚悉尼黑色素瘤单位采集。'
- en: The $\mathsf{PH}^{2}$ dataset, released by Mendonca et al. ([2013](#bib.bib274))
    and detailed by Mendonca et al. ([2015](#bib.bib275)), was the first public dataset
    to provide region-based annotations with segmentation masks, and masks for the
    clinically significant colors (white, red, light brown, dark brown, blue-gray,
    and black) present in the images. The dataset contains $200$ dermoscopic images
    ($20\%$ melanomas, $40\%$ atypical nevi, and $40\%$ common nevi) of $768\times
    560$ pixels, acquired at the Hospital Pedro Hispano, Portugal. The Edinburgh DermoFit
    Image Library (Ballerini et al., [2013](#bib.bib37)) also provides region-based
    annotations for $1,300$ clinical images (10 diagnostic categories including melanomas,
    seborrhoeic keratosis, and basal cell carcinoma) of sizes ranging from $177\times
    189$ to $2,176\times 2,549$ pixels. The images were acquired with a Canon EOS
    350D SLR camera, in controlled lighting and at a consistent distance from the
    lesions, resulting in a level of quality atypical for clinical images.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathsf{PH}^{2}$ 数据集由 Mendonca 等人发布（[2013](#bib.bib274)），并由 Mendonca 等人详细描述（[2015](#bib.bib275)）。这是第一个提供基于区域的注释以及图像中临床显著颜色（白色、红色、浅棕色、深棕色、蓝灰色和黑色）分割掩模的公开数据集。该数据集包含
    $200$ 张皮肤镜图像（$20\%$ 为黑色素瘤，$40\%$ 为非典型痣，$40\%$ 为普通痣），分辨率为 $768\times 560$ 像素，图像在葡萄牙
    Pedro Hispano 医院采集。爱丁堡 DermoFit 图像库（Ballerini 等人，[2013](#bib.bib37)）还提供了 $1,300$
    张临床图像的基于区域的注释（包括黑色素瘤、脂溢性角化病和基底细胞癌在内的 $10$ 种诊断类别），图像尺寸从 $177\times 189$ 到 $2,176\times
    2,549$ 像素不等。图像使用 Canon EOS 350D 单反相机在受控光照条件下并与病变保持一致的距离进行采集，因此图像质量在临床图像中较为罕见。
- en: The ISIC Archive contains the world’s largest curated repository of dermoscopic
    images. ISIC, an international academia-industry partnership sponsored by ISDIS
    (International Society for Digital Imaging of the Skin), aims to “facilitate the
    application of digital skin imaging to help reduce melanoma mortality” (ISIC,
    [2023](#bib.bib186)). At the time of writing, the archive contains more than $240,000$
    images, of which more than $71,000$ are publicly available. These images were
    acquired in leading worldwide clinical centers, using a variety of devices.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ISIC 存档包含世界上最大规模的皮肤镜图像策展库。ISIC 是一个由 ISDIS（国际皮肤数字成像学会）赞助的国际学术界和工业界合作伙伴关系，旨在“促进数字皮肤成像的应用，以帮助减少黑色素瘤的死亡率”（ISIC，[2023](#bib.bib186)）。截至撰写时，档案馆包含超过
    $240,000$ 张图像，其中超过 $71,000$ 张是公开可用的。这些图像是在全球领先的临床中心采集的，使用了各种设备。
- en: In addition to curating the datasets that collectively form the “ISIC Archive”,
    ISIC has released standard archive subsets as part of its *Skin Lesion Analysis
    Towards Melanoma Detection* Challenge, organized annually since 2016\. The 2016,
    2017, and 2018 challenges comprised segmentation, feature extraction, and classification
    tasks, while the 2019 and 2020 challenges featured only classification. Each subset
    is associated with a challenge (year), one or more tasks, and has two (training/test)
    or three (training/validation/test) splits. The ISIC Challenge 2016 (Gutman et al.,
    [2016](#bib.bib167)) (ISIC 2016, for brevity) contains $1,279$ images split into
    $900$ for training ($19\%$ melanomas, $81\%$ nevi), and $379$ for testing ($20\%$
    melanomas, $80\%$ nevi). There is a large variation in image size, ranging from
    $0.5$ to $12$ megapixels. All tasks used the same images. The ISIC 2017 (Codella
    et al., [2018](#bib.bib98)) dataset more than doubled, with $2,750$ images split
    into $2,000$ for training ($18.7\%$ melanomas, $12.7\%$ seborrheic keratoses,
    $68.6\%$ nevi), $150$ for validation ($20\%$ melanomas, $28\%$ seborrheic keratoses,
    $52\%$ nevi), and $600$ for testing ($19.5\%$ melanomas, $15\%$ seborrheic keratoses,
    $65.5\%$ nevi). Again, image size varied markedly, ranging from $0.5$ to $29$
    megapixels, and all tasks used the same images.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了策划共同形成“ISIC 存档”的数据集外，ISIC 还作为其 *皮肤病变分析以期实现黑色素瘤检测* 挑战的一部分发布了标准档案子集，该挑战自 2016
    年起每年组织一次。2016 年、2017 年和 2018 年的挑战包括分割、特征提取和分类任务，而 2019 年和 2020 年的挑战仅包含分类任务。每个子集与一个挑战（年份）、一个或多个任务相关，并有两个（训练/测试）或三个（训练/验证/测试）拆分。ISIC
    Challenge 2016（Gutman 等人，[2016](#bib.bib167)）（简称 ISIC 2016）包含 $1,279$ 张图像，分为 $900$
    张训练图像（$19\%$ 为黑色素瘤，$81\%$ 为痣）和 $379$ 张测试图像（$20\%$ 为黑色素瘤，$80\%$ 为痣）。图像尺寸差异很大，从
    $0.5$ 到 $12$ 兆像素不等。所有任务使用了相同的图像。ISIC 2017（Codella 等人，[2018](#bib.bib98)）数据集的规模增加了两倍多，包含
    $2,750$ 张图像，分为 $2,000$ 张训练图像（$18.7\%$ 为黑色素瘤，$12.7\%$ 为脂溢性角化病，$68.6\%$ 为痣）、$150$
    张验证图像（$20\%$ 为黑色素瘤，$28\%$ 为脂溢性角化病，$52\%$ 为痣）和 $600$ 张测试图像（$19.5\%$ 为黑色素瘤，$15\%$
    为脂溢性角化病，$65.5\%$ 为痣）。同样，图像尺寸差异显著，从 $0.5$ 到 $29$ 兆像素不等，所有任务使用了相同的图像。
- en: 'ISIC 2018 provided, for the first time, separate datasets for the tasks, with
    $2,594$ training ($20$% melanomas, $72$% nevi, and $8$% seborrheic keratoses)
    and $100$/$1,000$ for validation/test images ranging from $0.5$ to $29$ megapixels,
    for the tasks of segmentation and feature extraction (Codella et al., [2019](#bib.bib96)),
    and $10,015$/$1,512$ training/test images for the classification task, all with
    $600\times 450$ pixels. The training dataset for classification was the HAM10000
    dataset (Tschandl et al., [2018](#bib.bib381)), acquired over a period of $20$
    years at the Medical University of Vienna, Austria and the private practice of
    Dr. Cliff Rosendahl, Australia. It allowed a five-fold increase in training images
    in comparison to 2017 and comprised seven diagnostic categories: melanoma ($11.1\%$),
    nevus ($66.9\%$), basal cell carcinoma ($5.1\%$), actinic keratosis or Bowen’s
    disease ($3.3\%$), benign keratosis (solar lentigo, seborrheic keratosis, or lichen
    planus-like keratosis, $11\%$), dermatofibroma ($1.1\%$), and vascular lesion
    ($1.4\%$). As a part of a 2020 study of human-computer collaboration for skin
    lesion diagnosis involving dermatologists and general practitioners (Tschandl
    et al., [2020](#bib.bib380)), the lesions in the HAM10000 dataset were segmented
    by a single dermatologist and consequently released publicly (ViDIR Dataverse,
    [2020](#bib.bib396)), making this the single largest publicly available skin lesion
    segmentation dataset (Table [1](#S2.T1 "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Public skin lesion datasets with lesion segmentation annotations.
    All the datasets contain RGB images of skin lesions.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '| dataset | year | modality | size | training/validation/test | class distribution
    | additional info |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| DermQuest⁷⁷7DermQuest was deactivated on December 31, 2019\. However, 137
    of its images are publicly available (Glaister, [2013](#bib.bib148)). (DermQuest,
    [2012](#bib.bib120)) | 2012 | clinical | $137$ | – | 61 non-melanomas 76 melanomas
    | acquired with different cameras under various lighting conditions |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| DermoFit (Ballerini et al., [2013](#bib.bib37)) | 2013 | clinical | $1,300$
    | – | $1,224$ non-melanomas 76 melanomas | sizes ranging from $177\times 189$
    to $2,176\times 2,549$ pixels |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| Pedro Hispano Hospital (PH²)  (Mendonca et al., [2013](#bib.bib274)) | 2013
    | dermoscopy | $200$ | – | 160 benign nevi 40 melanomas | sizes ranging from $553\times
    763$ to $577\times 769$ pixels acquired at $20\times$ magnification |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| ISIC2016 (Gutman et al., [2016](#bib.bib167)) | 2016 | dermoscopy | $1,279$
    | $900$/–/$379$ | Training: 727 non-melanomas 173 melanomas Test: 304 non-melanomas
    75 melanomas | sizes ranging from $566\times 679$ to $2,848\times 4,288$ pixels
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| ISIC2017 (Codella et al., [2018](#bib.bib98)) | 2017 | dermoscopy | $2,750$
    | $2,000$/$150$/$600$ | Training: $1,626$ non-melanomas 374 melanomas Test: 483
    non-melanomas 117 melanomas | sizes ranging from $540\times 722$ to $4,499\times
    6,748$ pixels |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ISIC2017 (Codella et al., [2018](#bib.bib98)) | 2017 | 皮肤镜检查 | $2,750$ |
    $2,000$/$150$/$600$ | 训练集：$1,626$ 个非黑色素瘤，374 个黑色素瘤 测试集：483 个非黑色素瘤，117 个黑色素瘤 |
    图片尺寸从 $540\times 722$ 到 $4,499\times 6,748$ 像素 |'
- en: '| ISIC2018 (Codella et al., [2019](#bib.bib96)) | 2018 | dermoscopy | $3,694$
    | $2,594$/$100$/$1,000$ | – | sizes ranging from $540\times 576$ to $4,499\times
    6,748$ pixels |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ISIC2018 (Codella et al., [2019](#bib.bib96)) | 2018 | 皮肤镜检查 | $3,694$ |
    $2,594$/$100$/$1,000$ | – | 图片尺寸从 $540\times 576$ 到 $4,499\times 6,748$ 像素 |'
- en: '| HAM10000 (Tschandl et al., [2018](#bib.bib381)) (Tschandl et al., [2020](#bib.bib380))
    (ViDIR Dataverse, [2020](#bib.bib396)) | 2020 | dermoscopy | $10,015$ | – | $1,113$
    non-melanomas $8,902$ melanomas | all images of $600\times 450$ pixels |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| HAM10000 (Tschandl et al., [2018](#bib.bib381)) (Tschandl et al., [2020](#bib.bib380))
    (ViDIR Dataverse, [2020](#bib.bib396)) | 2020 | 皮肤镜检查 | $10,015$ | – | $1,113$
    个非黑色素瘤，$8,902$ 个黑色素瘤 | 所有图像尺寸为 $600\times 450$ 像素 |'
- en: ISIC 2019 (Codella et al., [2018](#bib.bib98); Tschandl et al., [2018](#bib.bib381);
    Combalia et al., [2019](#bib.bib101)) contains $25,331$ training images ($18\%$
    melanomas, $51\%$ nevi, $13\%$ basal cell carcinomas, $3.5\%$ actinic keratoses,
    $10\%$ benign keratoses, $1\%$ dermatofibromas, $1\%$ vascular lesions, and $2.5\%$
    squamous cell carcinomas) and $8,238$ test images (diagnostic distribution unknown).
    The images range from $600\times 450$ to $1,024\times 1,024$ pixels.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ISIC 2019 (Codella et al., [2018](#bib.bib98); Tschandl et al., [2018](#bib.bib381);
    Combalia et al., [2019](#bib.bib101)) 包含 $25,331$ 张训练图像（$18\%$ 黑色素瘤，$51\%$ 痣，$13\%$
    基底细胞癌，$3.5\%$ 日光角化病，$10\%$ 良性角化病，$1\%$ 真皮纤维瘤，$1\%$ 血管病变，以及 $2.5\%$ 鳞状细胞癌）和 $8,238$
    张测试图像（诊断分布未知）。图像尺寸范围从 $600\times 450$ 到 $1,024\times 1,024$ 像素。
- en: ISIC 2020 (Rotemberg et al., [2021](#bib.bib330)) contains $33,126$ training
    images ($1.8\%$ melanomas, $97.6\%$ nevi, $0.4\%$ seborrheic keratoses, $0.1\%$
    lentigines simplex, $0.1\%$ lichenoid keratoses, $0.02\%$ solar lentigines, $0.003\%$
    cafe-au-lait macules, $0.003\%$ atypical melanocytic proliferations) and $10,982$
    test images (diagnostic distribution unknown), ranging from 0.5 to 24 megapixels.
    Multiple centers, distributed worldwide, contributed to the dataset, including
    the Memorial Sloan Kettering Cancer Center (USA), the Melanoma Institute, the
    Sydney Melanoma Diagnostic Centre, and the University of Queensland (Australia),
    the Medical University of Vienna (Austria), the University of Athens (Greece),
    and the Hospital Clinic Barcelona (Spain). An important novelty in this dataset
    is the presence of multiple lesions per patient, with the express motivation of
    exploiting intra- and inter-patient lesion patterns, e.g., the so-called “ugly-ducklings”,
    lesions whose appearances are atypical for a given patient, and which present
    an increased risk of malignancy (Gachon et al., [2005](#bib.bib140)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ISIC 2020 (Rotemberg et al., [2021](#bib.bib330)) 包含 $33,126$ 张训练图像（$1.8\%$
    黑色素瘤，$97.6\%$ 痣，$0.4\%$ 脂溢性角化病，$0.1\%$ 单纯性雀斑，$0.1\%$ 类似角化病，$0.02\%$ 日光雀斑，$0.003\%$
    咖啡奶块，$0.003\%$ 异常黑色素细胞增生）和 $10,982$ 张测试图像（诊断分布未知），图像尺寸范围从 0.5 到 24 兆像素。多个全球中心贡献了这个数据集，包括美国纪念斯隆-凯特琳癌症中心、黑色素瘤研究所、悉尼黑色素瘤诊断中心、昆士兰大学（澳大利亚）、维也纳医科大学（奥地利）、雅典大学（希腊）和巴塞罗那诊所医院（西班牙）。这个数据集的一个重要新特点是每位患者有多个病变，明确目的是利用患者间和患者内的病变模式，例如所谓的“丑小鸭”病变，这些病变在特定患者中外观异常，并且有较高的恶性风险 (Gachon
    et al., [2005](#bib.bib140))。
- en: There is, however, an overlap among these ISIC Challenge datasets. Abhishek
    ([2020](#bib.bib4)) analyzed all the lesion segmentation datasets from the ISIC
    Challenges (2016-2018) and found considerable overlap between these 3 datasets,
    with as many as $1,940$ images shared between at least 2 datasets and $706$ images
    shared between all 3 datasets. In a more recent analysis of the ISIC Challenge
    datasets for the lesion diagnosis task from 2016 through 2020, Cassidy et al.
    ([2022](#bib.bib72)) found overlap between the datasets as well as the presence
    of duplicates within the datasets. Using a duplicate removal strategy, they curated
    a new set of $45,590$ training images ($8.61\%$ melanomas, $91.39\%$ others) and
    $11,397$ validation images ($8.61\%$ melanomas, $91.39\%$ others), leading to
    a total of $56,987$ images. Additionally, since the resulting dataset is highly
    imbalanced (melanomas versus others in a ratio of $1:10.62$), the authors also
    curated a balanced dataset with $7,848$ training images (50% melanoma, 50% others)
    and $1,962$ validation images (50% melanoma, 50% others).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些ISIC挑战数据集之间存在重叠。Abhishek（[2020](#bib.bib4)）分析了所有来自ISIC挑战（2016-2018）的病变分割数据集，发现这3个数据集之间有相当大的重叠，至少$1,940$张图片在2个数据集之间共享，$706$张图片在所有3个数据集之间共享。在对2016年至2020年的ISIC挑战数据集进行的最近分析中，Cassidy等人（[2022](#bib.bib72)）发现了数据集之间的重叠以及数据集内的重复项。通过使用重复项移除策略，他们整理出一个新的数据集，其中包括$45,590$张训练图片（$8.61\%$黑色素瘤，$91.39\%$其他）和$11,397$张验证图片（$8.61\%$黑色素瘤，$91.39\%$其他），总共$56,987$张图片。此外，由于结果数据集极度不平衡（黑色素瘤与其他的比例为$1:10.62$），作者还整理了一个平衡数据集，其中包括$7,848$张训练图片（50%黑色素瘤，50%其他）和$1,962$张验证图片（50%黑色素瘤，50%其他）。
- en: '![Refer to caption](img/1c13819ddd5fa85a9764e0abcd8f4541.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1c13819ddd5fa85a9764e0abcd8f4541.png)'
- en: 'Figure 3: The frequency of utilization of different skin lesion segmentation
    datasets in the surveyed studies. We found that $82$ papers evaluated on more
    than $1$ dataset, with $36$ papers opting for cross-dataset evaluation (CDE in
    Table LABEL:tab:main). ISIC datasets (ISIC 2016, ISIC 2017, ISIC 2018, and ISIC
    Archive) are used in the majority of papers, with $168$ of $177$ papers using
    at least one ISIC dataset and the ISIC 2017 dataset being the most popular ($117$
    papers). The PH² dataset is the second most widely used ($56$ papers) following
    ISIC datasets.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：调查研究中不同皮肤病变分割数据集的使用频率。我们发现有$82$篇论文在超过$1$个数据集上进行评估，其中$36$篇论文选择了跨数据集评估（见表LABEL:tab:main）。ISIC数据集（ISIC
    2016、ISIC 2017、ISIC 2018和ISIC档案）在大多数论文中被使用，其中$177$篇论文中的$168$篇使用了至少一个ISIC数据集，而ISIC
    2017数据集是最受欢迎的（$117$篇论文）。PH²数据集是使用频率第二高的数据集（$56$篇论文），仅次于ISIC数据集。
- en: Table [1](#S2.T1 "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation") shows a list of publicly available skin lesion
    datasets with pixel-wise annotations, image modality, sample size, original split
    sizes, and diagnostic distribution. Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Datasets ‣
    2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation") shows
    how frequently these datasets appear in the literature. It is also worth noting
    that several other skin lesion image datasets have not been described in our survey
    as they do not provide the corresponding skin lesion segmentation annotations.
    However, these datasets, including SD-198 (Sun et al., [2016](#bib.bib364)), MED-NODE (Giotis
    et al., [2015](#bib.bib146)), derm7pt (Kawahara et al., [2019](#bib.bib219)),
    Interactive Dermatology Atlas (Usatine and Madden, [2013](#bib.bib386)), Dermatology
    Information System (DermIS, [2012](#bib.bib119)), DermWeb (Lui et al., [2009](#bib.bib265)),
    DermNet New Zealand (Oakley et al., [1995](#bib.bib291)), may still be relevant
    for skin lesion segmentation research (see Section [5](#S5 "5 Discussion and Future
    Research ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S2.T1 "表 1 ‣ 2.1 数据集 ‣ 2 输入数据 ‣ 深度学习在皮肤病变分割中的应用调查")显示了具有像素级注释的公开可用皮肤病变数据集的列表，包括图像模态、样本大小、原始分割大小和诊断分布。图[3](#S2.F3
    "图 3 ‣ 2.1 数据集 ‣ 2 输入数据 ‣ 深度学习在皮肤病变分割中的应用调查")显示了这些数据集在文献中的出现频率。值得注意的是，还有一些皮肤病变图像数据集没有在我们的调查中描述，因为它们没有提供相应的皮肤病变分割注释。然而，这些数据集，包括SD-198（Sun等，[2016](#bib.bib364)）、MED-NODE（Giotis等，[2015](#bib.bib146)）、derm7pt（Kawahara等，[2019](#bib.bib219)）、互动皮肤病学图谱（Usatine和Madden，[2013](#bib.bib386)）、皮肤病学信息系统（DermIS，[2012](#bib.bib119)）、DermWeb（Lui等，[2009](#bib.bib265)）、DermNet新西兰（Oakley等，[1995](#bib.bib291)），可能仍对皮肤病变分割研究具有参考价值（见第[5](#S5
    "5 讨论与未来研究 ‣ 深度学习在皮肤病变分割中的应用调查")节）。
- en: Biases in computer vision datasets are a constant source of issues (Torralba
    and Efros, [2011](#bib.bib377)), which is compounded in medical imaging due to
    the smaller number of samples, insufficient image resolution, lack of geographical
    or ethnic diversity, or statistics unrepresentative of clinical practice. All
    existing skin lesion datasets suffer to a certain extent from one or more of the
    aforementioned issues, to which we add the specific issue of the availability
    and reliability of annotations. For lesion classification, many samples lack the
    gold standard histopathological confirmation, and ground-truth segmentation, even
    when available, is inherently noisy (Section [4.2](#S4.SS2 "4.2 Inter-Annotator
    Agreement ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).
    The presence of artifacts (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")) may lead to spurious correlations,
    an issue that Bissoto et al. ([2019](#bib.bib59)) attempted to quantify for classification
    models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉数据集中的偏差是一个持续存在的问题 (Torralba and Efros, [2011](#bib.bib377))，在医学影像中尤为严重，原因包括样本数量较少、图像分辨率不足、缺乏地理或种族多样性，或统计数据与临床实践不符。所有现有的皮肤病变数据集在一定程度上都存在上述问题，我们还增加了注释的可用性和可靠性这一具体问题。对于病变分类，许多样本缺乏黄金标准的组织病理学确认，即使有的地面真实分割也本质上是嘈杂的
    (第 [4.2](#S4.SS2 "4.2 Inter-Annotator Agreement ‣ 4 Evaluation ‣ A Survey on Deep
    Learning for Skin Lesion Segmentation") 节)。伪影的存在 (图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")) 可能导致虚假的相关性，这是 Bissoto
    等人 ([2019](#bib.bib59)) 尝试对分类模型进行量化的问题。
- en: 2.2 Synthetic Data Generation
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 合成数据生成
- en: Data augmentation—synthesizing new samples from existing ones—is commonly employed
    in the training of DL models. Augmented data serve as a regularizer, increase
    the amount and diversity of data (Shorten and Khoshgoftaar, [2019](#bib.bib350)),
    induce desirable invariances on the model, and alleviate class imbalance. Traditional
    data augmentation applies simple geometric, photometric, and colorimetric transformations
    on the samples, including mirroring, translation, scaling, rotation, cropping,
    random region erasing, affine or elastic deformation, modifications of hue, saturation,
    brightness, and contrast. Usually, several transformations are chosen at random
    and combined. Fig. [4](#S2.F4 "Figure 4 ‣ 2.2 Synthetic Data Generation ‣ 2 Input
    Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation") exemplifies the
    procedure, as applied to a dermoscopic image with Albumentations (Buslaev et al.,
    [2020](#bib.bib68)), a state-of-the-art open-source library for image augmentation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强——从现有样本中合成新样本——在深度学习模型训练中常被使用。增强数据作为正则化器，增加数据的数量和多样性 (Shorten and Khoshgoftaar,
    [2019](#bib.bib350))，诱导模型上的期望不变性，并缓解类别不平衡。传统的数据增强对样本应用简单的几何、光度和色彩变换，包括镜像、平移、缩放、旋转、裁剪、随机区域擦除、仿射或弹性变形、色调、饱和度、亮度和对比度的修改。通常，会随机选择几种变换并组合。图
    [4](#S2.F4 "Figure 4 ‣ 2.2 Synthetic Data Generation ‣ 2 Input Data ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation") 展示了这一过程，应用于使用 Albumentations (Buslaev
    et al., [2020](#bib.bib68)) 的皮肤镜图像，Albumentations 是一种先进的开源图像增强库。
- en: 'As mentioned earlier, augmented training data induce invariance on the models:
    random translations and croppings, for example, help induce a translation-invariant
    model. This has implications for skin lesion analysis, e.g., data augmentation
    for generic datasets (such as ImageNet (Deng et al., [2009](#bib.bib113))) forgo
    vertical mirroring and large-angle rotations, because natural scenes have a strong
    vertical anisotropy, while skin lesion images are isotropic. In addition, augmented
    test data (test-time augmentation) may also improve generalization by combining
    the predictions of several augmented samples through, for example, average pooling
    or majority voting (Shorten and Khoshgoftaar, [2019](#bib.bib350)). Perez et al.
    ([2018](#bib.bib305)) have systematically evaluated the effect of several data
    augmentation schemes for skin lesion classification, finding that the use of both
    training and test augmentation is critical for performance, surpassing, in some
    cases, increases of real data without augmentation. Valle et al. ([2020](#bib.bib388))
    found, in a very large-scale experiment, that test-time augmentation was the second
    most influential factor for classification performance, after training set size.
    No systematic study of this kind exists for skin lesion segmentation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，增强训练数据可以使模型具有不变性：例如，随机平移和裁剪有助于生成平移不变的模型。这对皮肤病变分析有重要影响，例如，通用数据集（如ImageNet (Deng
    et al., [2009](#bib.bib113)))的增强不会包括垂直镜像和大角度旋转，因为自然场景具有强垂直各向异性，而皮肤病变图像是各向同性的。此外，增强测试数据（测试时增强）也可以通过例如平均池化或多数投票 (Shorten
    and Khoshgoftaar, [2019](#bib.bib350))，提高泛化能力。Perez et al. ([2018](#bib.bib305))系统地评估了多种数据增强方案对皮肤病变分类的影响，发现训练和测试增强的结合对性能至关重要，有时甚至超越了没有增强的真实数据增加。Valle
    et al. ([2020](#bib.bib388))在一个大规模实验中发现，测试时增强是分类性能的第二大影响因素，仅次于训练集大小。对于皮肤病变分割尚无此类系统性研究。
- en: '![Refer to caption](img/ddbe6db194a16e3c6574ed08f9798f7c.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ddbe6db194a16e3c6574ed08f9798f7c.png)'
- en: (a) Original
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始
- en: '![Refer to caption](img/3468b4eb275c14f115276b88b469e344.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3468b4eb275c14f115276b88b469e344.png)'
- en: (b) Affine deformation
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 仿射变形
- en: '![Refer to caption](img/b05e7b6776ac119ad5f206985e4bbdc9.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b05e7b6776ac119ad5f206985e4bbdc9.png)'
- en: (c) Elastic deformation
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 弹性变形
- en: '![Refer to caption](img/fd4db66520337dbeb5a994ee0911a523.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fd4db66520337dbeb5a994ee0911a523.png)'
- en: (d) Histogram equalization
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 直方图均衡化
- en: '![Refer to caption](img/b7f3f7a06fdd99c67e25683612d9f4fd.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b7f3f7a06fdd99c67e25683612d9f4fd.png)'
- en: (e) HSV shift
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (e) HSV 变换
- en: '![Refer to caption](img/06b3a6c614d7cb82a690acd68847f080.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/06b3a6c614d7cb82a690acd68847f080.png)'
- en: (f) RGB shift
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: (f) RGB 变换
- en: 'Figure 4: Various data augmentation transformations applied to a dermoscopic
    image (image source: ISIC 2016 dataset (Gutman et al., [2016](#bib.bib167))) using
    the Albumentations library (Buslaev et al., [2020](#bib.bib68)).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：应用于皮肤镜图像的各种数据增强变换（图像来源：ISIC 2016 数据集 (Gutman et al., [2016](#bib.bib167)))，使用
    Albumentations 库 (Buslaev et al., [2020](#bib.bib68))。
- en: Although traditional data augmentation is crucial for training DL models, it
    falls short of providing samples that are both diverse and plausible from the
    same distribution as real data. Thus, modern data augmentation (Tajbakhsh et al.,
    [2020](#bib.bib367)) employs generative modeling, learning the probability distribution
    of the real data, and sampling from that distribution. Generative adversarial
    networks (GANs) (Goodfellow et al., [2020](#bib.bib152)) are the most promising
    approach in this direction (Shorten and Khoshgoftaar, [2019](#bib.bib350)), especially
    for medical image analysis (Yi et al., [2019](#bib.bib428); Kazeminia et al.,
    [2020](#bib.bib222); Shamsolmoali et al., [2021](#bib.bib347)). GANs employ an
    adversarial training between a generator, which attempts to generate realistic
    fake samples, and a discriminator, which attempts to differentiate real samples
    from fake ones. When the procedure converges, the generator output is surprisingly
    convincing, but GANs are computationally expensive and difficult to train (Creswell
    et al., [2018](#bib.bib103)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic generation of skin lesions has received some recent interest, especially
    in the context of improving classification. Works can be roughly divided into
    those that use GANs to create new images from a Gaussian latent variable (Baur
    et al., [2018](#bib.bib43); Pollastri et al., [2020](#bib.bib309); Abdelhalim
    et al., [2021](#bib.bib3)), and those that implement GANs based on image-to-image
    translation (Abhishek and Hamarneh, [2019](#bib.bib5); Bissoto et al., [2018](#bib.bib60);
    Ding et al., [2021](#bib.bib124)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Noise-based GANs, such as DCGAN (Yu et al., [2017b](#bib.bib432)), LAPGAN (Denton
    et al., [2015](#bib.bib116)), and PGAN (Karras et al., [2018](#bib.bib211)), learn
    to decode a Gaussian latent variable into an image that belongs to the training
    set distribution. The main advantage of these techniques is the ability to create
    more, and more diverse images, as, in principle, any sample from a multivariate
    Gaussian distribution may become a different image. The disadvantage is that the
    images tend to be of lower quality, and, in the case of segmentation, one needs
    to generate plausible pairs of images and segmentation masks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-image translation GANs, such as pix2pix (Isola et al., [2017](#bib.bib187))
    and pix2pixHD (Wang et al., [2018](#bib.bib403)), learn to create new samples
    from a semantic segmentation map. They have complementary advantages and disadvantages.
    Because the procedure is deterministic (one map creates one image), they have
    much less freedom in the number of samples available, but the images tend to be
    of higher quality (or more “plausible”). There is no need to generate separate
    segmentation maps because the generated image is intrinsically compatible with
    the input segmentation map.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: The two seminal papers on GANs for skin lesions (Baur et al., [2018](#bib.bib43);
    Bissoto et al., [2018](#bib.bib60)) evaluate several models. Baur et al. ([2018](#bib.bib43))
    compare the noise-based DCGAN, LAPGAN, and PGAN for the generation of $256\times
    256$-pixel images using both qualitative and quantitative criteria, finding that
    the PGAN gives considerably better results. They further examine the PGAN against
    a panel of human judges, composed by dermatologists and DL experts, in a “visual
    Turing test”, showing that both had difficulties in distinguishing the fake images
    from the true ones. Bissoto et al. ([2018](#bib.bib60)) adapt the PGAN to be class-conditioned
    on diagnostic category, and the image-to-image pix2pixHD to employ the semantic
    annotation provided by the feature extraction task of the ISIC 2018 dataset (Table [1](#S2.T1
    "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")), comparing those to an unmodified DCGAN on $256\times 256$-pixel
    images, and finding the modified pix2pixHD to be qualitatively better. They use
    the performance improvement on a separate classification network as a quantitative
    metric, finding that the use of samples from both PGAN and pix2pixHD leads to
    the best improvements. They also showcase images of size up to $1,024\times 1,024$
    pixels generated by the pix2pixHD-derived model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Pollastri et al. ([2020](#bib.bib309)) extended DCGAN and LAPGAN architectures
    to generate the segmentation masks (in the pairwise scheme explained above), making
    their work the only noise-based GANs usable for segmentation to date. Bi et al.
    ([2019a](#bib.bib48)) introduced stacked adversarial learning to GANs to learn
    class-specific skin lesion image generators given the ground-truth segmentations.
    Abhishek and Hamarneh ([2019](#bib.bib5)) employ pix2pix to translate a binary
    segmentation mask into a dermoscopic image and use the generated image-mask pairs
    to augment skin lesion segmentation training datasets, improving segmentation
    performance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Ding et al. ([2021](#bib.bib124)) feed a segmentation mask and an instance mask
    to a conditional GAN generator, where the instance mask states the diagnostic
    category to be synthesized. In both cases, the discriminator receives different
    resolutions of the generated image and is required to make a decision for each
    of them. Abdelhalim et al. ([2021](#bib.bib3)) is a recent work that also conditions
    PGAN on the class label and uses the generated outputs to augment a melanoma diagnosis
    dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Bissoto et al. ([2021](#bib.bib61)) cast doubt on the power of GAN-synthesized
    data augmentation to reliably improve skin lesion classification. Their evaluation,
    which included four GAN models, four datasets, and several augmentation scenarios,
    showed improvement only in a severe cross-modality scenario (training on dermoscopic
    and testing on clinical images). To the best of our knowledge, no corresponding
    systematic evaluation exists for skin lesion segmentation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Supervised, Semi-supervised, Weakly supervised, Self-supervised learning
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although supervised DL has achieved outstanding performance in various medical
    image analysis applications, its dependency on high-quality annotations limits
    its applicability, as well as its generalizability to unseen, out-of-distribution
    data. Semi-supervised techniques attempt to learn from both labeled and unlabeled
    samples. Weakly supervised techniques attempt to exploit partial annotations like
    image-level labels or bounding boxes, often in conjunction with a subset of pixel-level
    fully-annotated samples.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/800c0cff0aaefcfed7a113bb1d37cdde.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A breakdown of different levels of supervision used in the $177$
    surveyed works. Fully supervised models continue to make up the majority of the
    literature ($163$ papers), with semi-supervised and weakly supervised methods
    appearing in only $9$ papers. Self-supervision in skin lesion segmentation is
    fairly new, with all the $5$ papers appearing from 2020 onwards.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Since pixel-level annotation of skin lesion images is costly, there is a trade-off
    between annotation precision and efficiency. In practice, the annotations are
    intrinsically noisy, which can be modeled explicitly to avoid over-fitting. (We
    discuss the issue of annotation variability in Section [4.2](#S4.SS2 "4.2 Inter-Annotator
    Agreement ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation").)
    To deal with label noise, Mirikharaji et al. ([2019](#bib.bib281)) learn a model
    robust to annotation noise, making use of a large set of unreliable annotations
    and a small set of perfect clean annotations. They propose to learn a spatially
    adaptive weight map corresponding to each training data, assigning different weights
    to noisy and clean pixel-level annotations while training the deep model. To remove
    the dependency on having a set of perfectly clean annotations, Redekop and Chernyavskiy
    ([2021](#bib.bib321)) propose to alter noisy ground-truth masks during training
    by considering the quantification of aleatoric uncertainty (Der Kiureghian and
    Ditlevsen, [2009](#bib.bib118); Gal, [2016](#bib.bib141); Depeweg et al., [2018](#bib.bib117);
    Kwon et al., [2020](#bib.bib235)) to obtain a map of regions of high and low uncertainty.
    Pixels of ground-truth masks in highly uncertain regions are flipped, progressively
    increasing the model’s robustness to label noise. Ribeiro et al. ([2020](#bib.bib326))
    deal with noise by discarding inconsistent samples and annotation detail during
    training time, showing that the model generalizes better even when detailed annotations
    are required in test time.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: When there is a labeled dataset, even if the number of labeled samples is far
    less than that of unlabeled samples, semi- and self-supervision techniques can
    be applied. Li et al. ([2021c](#bib.bib247)) propose a semi-supervised approach,
    using a transformation-consistent self-ensemble to leverage unlabeled data and
    to regularize the model. They minimize the difference between the network predictions
    of different transformations (random perturbations, flipping, and rotation) applied
    to the input image and the transformation of the model prediction for the input
    image. Self-supervision attempts to exploit intrinsic labels by solving proxy
    tasks, enabling the use of a large, unlabeled corpus of data to pretrain a model
    before fine-tuning it on the target task. An example is to artificially apply
    random rotations in the input images, and train the model to predict the exact
    degree of rotation (Gidaris et al., [2018](#bib.bib145)). Note that the degree
    of rotation of each image is known, since it was artificially applied, and thus,
    can be used as a label during training. Similarly, for skin lesion segmentation,
    Li et al. ([2020b](#bib.bib249)) propose to exploit the color distribution information,
    the proxy task being to predict values from blue and red color channels while
    having the green one as input. They also include a task to estimate the red and
    blue color distributions to improve the model’s ability to extract global features.
    After the pretraining, they use a smaller set of labeled data to fine-tune the
    model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Image Preprocessing
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Preprocessing may facilitate the segmentation of skin lesion images. Typical
    preprocessing operations include:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Downsampling: Dermoscopy is typically a high-resolution technique, resulting
    in large image sizes, while many convolutional neural network (CNN) architectures,
    e.g., LeNet, AlexNet, VGG, GoogLeNet, ResNet, etc., require fixed-size input images,
    usually $224\times 224$ or $299\times 299$ pixels, and even those CNNs that can
    handle arbitrary-sized images (e.g., fully-convolutional networks (FCNs)) may
    benefit from downsampling for computational reasons. Downsampling is common in
    the skin lesion segmentation literature (Codella et al., [2017](#bib.bib97); Yu
    et al., [2017a](#bib.bib431); Yuan et al., [2017](#bib.bib433); Al-Masni et al.,
    [2018](#bib.bib15); Zhang et al., [2019b](#bib.bib441); Pollastri et al., [2020](#bib.bib309)).'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Color space transformations: RGB images are expected by most models, but some
    works (Codella et al., [2017](#bib.bib97); Al-Masni et al., [2018](#bib.bib15);
    Yuan and Lo, [2019](#bib.bib434); Pollastri et al., [2020](#bib.bib309); Pour
    and Seker, [2020](#bib.bib311)) employ alternative color spaces (Busin et al.,
    [2008](#bib.bib67)), such as CIELAB, CIELUV, and HSV. Often, one or more channels
    of the transformed space are combined with the RGB channels for reasons including,
    but not limited to, increasing the class separability, decoupling luminance and
    chromaticity, ensuring (approximate) perceptual uniformity, achieving invariance
    to illumination or viewpoint, and eliminating highlights.'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additional inputs: In addition to color space transformations, recent works
    incorporate more focused and domain-specific inputs to the segmentation models,
    such as Fourier domain representation using the discrete Fourier transform (Tang
    et al., [2021b](#bib.bib373)) and inputs based on the physics of skin illumination
    and imaging (Abhishek et al., [2020](#bib.bib7)).'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contrast enhancement: Insufficient contrast (Fig. [1(i)](#S1.F1.sf9 "In Figure
    1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Skin Lesion Segmentation"))
    is a prime reason for segmentation failures (Bogo et al., [2015](#bib.bib62)),
    leading some works (Saba et al., [2019](#bib.bib334); Schaefer et al., [2011](#bib.bib344))
    to enhance the image contrast prior to segmentation.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Color normalization: Varying illumination (Barata et al., [2015a](#bib.bib38),
    [b](#bib.bib39)) may lead to inconsistencies in skin lesion segmentation. This
    problem can be addressed by color normalization (Goyal et al., [2019b](#bib.bib154)).'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Artifact removal: Dermoscopic images often present artifacts, among which hair
    (Fig. [1(g)](#S1.F1.sf7 "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation")) is the most distracting (Abbas et al., [2011](#bib.bib1)),
    leading some studies (Ünver and Ayan, [2019](#bib.bib385); Zafar et al., [2020](#bib.bib435);
    Li et al., [2021b](#bib.bib246)) to remove it prior to segmentation.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Classical machine learning models (e.g., nearest neighbors, decision trees,
    support vector machines (Celebi et al., [2007b](#bib.bib78), [2008](#bib.bib77);
    Iyatomi et al., [2008](#bib.bib188); Barata et al., [2014](#bib.bib41); Shimizu
    et al., [2015](#bib.bib349))), which rely on hand-crafted features (Barata et al.,
    [2019](#bib.bib40)), tend to benefit more from preprocessing than DL models, which,
    when properly trained, can learn from the data how to bypass input issues (Celebi
    et al., [2015a](#bib.bib79); Valle et al., [2020](#bib.bib388)). However, preprocessing
    may still be helpful when dealing with small or noisy datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 3 Model Design and Training
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-layer perceptrons (MLPs) for pixel-level classification (Gish and Blanz,
    [1989](#bib.bib147); Katz and Merickel, [1989](#bib.bib214)) appeared soon after
    the publication of the seminal backpropagation paper (Rumelhart et al., [1986](#bib.bib333)),
    but these shallow feed-forward networks had many drawbacks (LeCun et al., [1998](#bib.bib239)),
    including an excessive number of parameters, lack of invariance, and disregard
    for the inherent structure present in images.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs are deep feedforward neural networks designed to extract progressively
    more abstract features from multidimensional signals ($1$-D signals, $2$-D images,
    $3$-D video, etc.) (LeCun et al., [2015](#bib.bib238)). Therefore, in addition
    to addressing the aforementioned problems of MLPs, CNNs automate *feature engineering* (Bengio
    et al., [2013](#bib.bib46)), that is, the design of algorithms that can transform
    raw signal values to discriminative features. Another advantage of CNNs over traditional
    machine learning classifiers is that they require minimal preprocessing of the
    input data. Due to their significant advantages, CNNs have become the method of
    choice in many medical image analysis applications over the past decade (Litjens
    et al., [2017](#bib.bib256)). The key enablers in this deep learning revolution
    were: (i) the availability of massive data sets; (ii) the availability of powerful
    and inexpensive graphics processing units; (iii) the development of better network
    architectures, learning algorithms, and regularization techniques; and (iv) the
    development of open-source deep learning frameworks.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation may be understood as the attempt to answer the parallel
    and complementary questions “what” and “where” in a given image. The former is
    better answered by translation-invariant global features, while the latter requires
    well-localized features, posing a challenge to deep models. CNNs for pixel-level
    classification first appeared in the mid-2000s (Ning et al., [2005](#bib.bib288)),
    but their use accelerated after the seminal paper on FCNs by Long et al. ([2015](#bib.bib264)),
    which, along with U-Net (Ronneberger et al., [2015](#bib.bib328)), have become
    the basis for many state-of-the-art segmentation models. In contrast to classification
    CNNs (e.g., LeNet, AlexNet, VGG, GoogLeNet, ResNet), FCNs easily cope with arbitrary-sized
    input images.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Architecture
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An ideal skin lesion segmentation algorithm is accurate, computationally inexpensive,
    invariant to noise and input transformations, requires little training data and
    is easy to implement and train. Unfortunately, no algorithm has, so far, been
    able to achieve these conflicting goals. DL-based segmentation tends towards accuracy
    and invariance at the cost of computation and training data. Ease of implementation
    is debatable: on the one hand, the algorithms often forgo cumbersome preprocessing,
    postprocessing, and feature engineering steps. On the other hand, tuning and optimizing
    them is often a painstaking task.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Architecture ‣ 3 Model Design and
    Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation"), we have classified
    the existing literature into single-network models, multiple-network models, hybrid-feature
    models, and Transformer models. The first and second groups are somewhat self-descriptive,
    but notice that the latter is further divided into ensembles of models, multi-task
    methods (often performing simultaneous classification and segmentation), and GANs.
    Hybrid-feature models combine DL with hand-crafted features. Transformer models,
    as the name suggests, employ Transformers either with or without CNNs for segmentation,
    and have started being used for skin lesion segmentation only recently. We classified
    works according to their most relevant feature, but the architectural improvements
    discussed in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Single Network Models ‣ 3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")
    also appear in the models listed in the other sections. In Fig. [7](#S3.F7 "Figure
    7 ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture ‣ 3 Model Design and Training
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation"), we show how frequently
    different architectural modules appear in the $177$ surveyed works, grouped by
    our taxonomy of model architectures (Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Table LABEL:tab:main summarizes all the $177$ surveyed works in this review,
    with the following attributes for each work: type of publication, datasets, architectural
    modules, loss functions, and augmentations used, reported Jaccard index, whether
    the paper performed cross-dataset evaluation (CDE) and postprocessing (PP), and
    whether the corresponding code was released publicly. For papers that reported
    segmentation results on more than 1 dataset, we list all of them and list the
    performance on only one dataset, formatting that particular dataset in bold. Since
    ISIC 2017 is the most popular dataset (Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Datasets
    ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")), wherever
    reported, we note the performance (Jaccard index) on ISIC 2017\. For papers that
    do not report the Jaccard index and instead report the Dice score, we compute
    the former based on the latter and report this computed score denoted by an asterisk.
    Cross-dataset evaluation (CDE) refers to when a paper trained model(s) on one
    dataset but evaluated on another.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: <svg   height="476.9" overflow="visible" version="1.1" width="585.17"><g transform="translate(0,476.9)
    matrix(1 0 0 -1 0 0) translate(292.58,0) translate(0,184.32)" fill="#000000" stroke="#000000"
    stroke-width="1.2pt"><g transform="matrix(1.0 0.0 0.0 1.0 -68.9 6.92)" fill="#000000"
    stroke="#000000"><foreignobject width="137.8" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Segmentation Model Architectures §[3.1](#S3.SS1
    "3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for
    Skin Lesion Segmentation") <g fill="#FFCC99" stroke="#FFCC99"><path d="M -94.9
    -139.19 C -94.9 -114.73 -114.73 -94.9 -139.19 -94.9 C -163.66 -94.9 -183.49 -114.73
    -183.49 -139.19 C -183.49 -163.66 -163.66 -183.49 -139.19 -183.49 C -114.73 -183.49
    -94.9 -163.66 -94.9 -139.19 Z M -139.19 -139.19"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -178.56 -134.35)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transformer
    Models §[3.1.4](#S3.SS1.SSS4 "3.1.4 Transformer Models ‣ 3.1 Architecture ‣ 3
    Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M -55.68 -55.68 M -33.28 -71.36 C -50.06 -63.54 -63.54 -50.06
    -71.36 -33.28 C -63.04 -51.12 -72.34 -66.86 -86.26 -80.78 L -80.78 -86.26 C -66.86
    -72.34 -51.12 -63.04 -33.28 -71.36 Z M -89.47 -94.95 L -80.78 -86.26 L -86.26
    -80.78 L -94.95 -89.47 Z M -86.26 -80.78 M -99.05 -120.47 C -103.45 -111.03 -111.03
    -103.45 -120.47 -99.05 C -110.44 -103.73 -102.78 -97.3 -94.95 -89.47 L -89.47
    -94.95 C -97.3 -102.78 -103.73 -110.44 -99.05 -120.47 Z"></path></clippath><g
    clip-path="url(#pgfcp17)"><g transform="matrix(1.0 0.0 0.0 1.0 -55.68 -55.68)"><g
    fill="#FFB3B3"><path d="M 54.71 -56.66 L 111.36 -0.01 L -0.01 111.36 L -56.66
    54.71 Z M -0.01 111.36" style="stroke:none"></path></g><g fill="#FFCC99"><path
    d="M 4.46 -106.9 L -27.83 -139.2 L -139.2 -27.83 L -106.9 4.46 Z M -139.2 -27.83"
    style="stroke:none"><g transform="matrix(-0.37585 -0.37585 0.80185 -0.80185 -26.1
    -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#D9668C" stroke="#D9668C"><path d="M 183.49 -139.19 C 183.49 -114.73 163.66
    -94.9 139.19 -94.9 C 114.73 -94.9 94.9 -114.73 94.9 -139.19 C 94.9 -163.66 114.73
    -183.49 139.19 -183.49 C 163.66 -183.49 183.49 -163.66 183.49 -139.19 Z M 139.19
    -139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 99.82 -134.35)" fill="#000000"
    stroke="#000000"><foreignobject width="78.74" height="29.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Hybrid Feature Models §[3.1.3](#S3.SS1.SSS3
    "3.1.3 Hybrid Feature Models ‣ 3.1 Architecture ‣ 3 Model Design and Training
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M 55.68 -55.68 M 71.36 -33.28 C 63.54 -50.06 50.06 -63.54
    33.28 -71.36 C 51.12 -63.04 66.86 -72.34 80.78 -86.26 L 86.26 -80.78 C 72.34 -66.86
    63.04 -51.12 71.36 -33.28 Z M 94.95 -89.47 L 86.26 -80.78 L 80.78 -86.26 L 89.47
    -94.95 Z M 80.78 -86.26 M 120.47 -99.05 C 111.03 -103.45 103.45 -111.03 99.05
    -120.47 C 103.73 -110.44 97.3 -102.78 89.47 -94.95 L 94.95 -89.47 C 102.78 -97.3
    110.44 -103.73 120.47 -99.05 Z"></path></clippath><g clip-path="url(#pgfcp19)"><g
    transform="matrix(1.0 0.0 0.0 1.0 55.68 -55.68)"><g fill="#FFB3B3"><path d="M
    56.66 54.71 L 0.01 111.36 L -111.36 -0.01 L -54.71 -56.66 Z M -111.36 -0.01" style="stroke:none"></path></g><g
    fill="#D9668C"><path d="M 106.9 4.46 L 139.2 -27.83 L 27.83 -139.2 L -4.46 -106.9
    Z M 27.83 -139.2" style="stroke:none"><g transform="matrix(0.37585 -0.37585 0.80185
    0.80185 26.1 -26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#CCCCCC" stroke="#CCCCCC"><path d="M 183.49 139.19 C 183.49 163.66 163.66
    183.49 139.19 183.49 C 114.73 183.49 94.9 163.66 94.9 139.19 C 94.9 114.73 114.73
    94.9 139.19 94.9 C 163.66 94.9 183.49 114.73 183.49 139.19 Z M 139.19 139.19"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 99.82 144.04)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multiple
    Network Models §[3.1.2](#S3.SS1.SSS2 "3.1.2 Multiple Network Models ‣ 3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M 55.68 55.68 M 33.28 71.36 C 50.06 63.54 63.54 50.06 71.36
    33.28 C 63.04 51.12 72.34 66.86 86.26 80.78 L 80.78 86.26 C 66.86 72.34 51.12
    63.04 33.28 71.36 Z M 89.47 94.95 L 80.78 86.26 L 86.26 80.78 L 94.95 89.47 Z
    M 86.26 80.78 M 99.05 120.47 C 103.45 111.03 111.03 103.45 120.47 99.05 C 110.44
    103.73 102.78 97.3 94.95 89.47 L 89.47 94.95 C 97.3 102.78 103.73 110.44 99.05
    120.47 Z"></path></clippath><g clip-path="url(#pgfcp21)"><g transform="matrix(1.0
    0.0 0.0 1.0 55.68 55.68)"><g fill="#FFB3B3"><path d="M -54.71 56.66 L -111.36
    0.01 L 0.01 -111.36 L 56.66 -54.71 Z M 0.01 -111.36" style="stroke:none"></path></g><g
    fill="#CCCCCC"><path d="M -4.46 106.9 L 27.83 139.2 L 139.2 27.83 L 106.9 -4.46
    Z M 139.2 27.83" style="stroke:none"><g transform="matrix(0.37585 0.37585 -0.80185
    0.80185 26.1 26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#CCCCCC" stroke="#CCCCCC"><path d="M 291.75 139.19 C 291.75 158.22 276.33
    173.64 257.3 173.64 C 238.28 173.64 222.86 158.22 222.86 139.19 C 222.86 120.17
    238.28 104.75 257.3 104.75 C 276.33 104.75 291.75 120.17 291.75 139.19 Z M 257.3
    139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 227.78 135.04)" fill="#000000"
    stroke="#000000"><foreignobject width="59.06" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">GANs §[3.1.2.3](#S3.SS1.SSS2.P3 "3.1.2.3 Generative
    Adversarial Models ‣ 3.1.2 Multiple Network Models ‣ 3.1 Architecture ‣ 3 Model
    Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#CCCCCC"><path d="M 183.49 139.19 M 180.81 154.34 C 184.38 144.56 184.38
    133.83 180.81 124.05 C 184.6 134.45 194.56 136.18 205.63 136.18 L 205.63 142.21
    C 194.56 142.21 184.6 143.94 180.81 154.34 Z M 205.63 136.18 h 0 v 6.03 h 0 Z
    M 224.93 150.98 C 222.16 143.37 222.16 135.02 224.93 127.41 C 221.99 135.5 214.24
    136.18 205.63 136.18 L 205.63 142.21 C 214.24 142.21 221.99 142.88 224.93 150.98
    Z" style="stroke:none"></path></g><g fill="#CCCCCC" stroke="#CCCCCC"><path d="M
    257.16 222.71 C 257.16 241.74 241.74 257.16 222.71 257.16 C 203.69 257.16 188.26
    241.74 188.26 222.71 C 188.26 203.69 203.69 188.26 222.71 188.26 C 241.74 188.26
    257.16 203.69 257.16 222.71 Z M 222.71 222.71"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 193.18 226.86)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multi-task
    Models §[3.1.2.2](#S3.SS1.SSS2.P2 "3.1.2.2 Multi-task Models ‣ 3.1.2 Multiple
    Network Models ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")</foreignobject></g> <g fill="#CCCCCC"><path
    d="M 170.51 170.51 M 157.91 179.34 C 167.35 174.94 174.94 167.35 179.34 157.91
    C 174.66 167.95 180.47 176.21 188.3 184.04 L 184.04 188.3 C 176.21 180.47 167.95
    174.66 157.91 179.34 Z M 184.04 188.3 L 184.04 188.3 L 188.3 184.04 L 188.3 184.04
    Z M 188.3 184.04 M 191.49 208.15 C 194.91 200.81 200.81 194.91 208.15 191.49 C
    200.35 195.13 194.39 190.13 188.3 184.04 L 184.04 188.3 C 190.13 194.39 195.13
    200.35 191.49 208.15 Z" style="stroke:none"></path></g><g fill="#CCCCCC" stroke="#CCCCCC"><path
    d="M 173.64 257.3 C 173.64 276.33 158.22 291.75 139.19 291.75 C 120.17 291.75
    104.75 276.33 104.75 257.3 C 104.75 238.28 120.17 222.86 139.19 222.86 C 158.22
    222.86 173.64 238.28 173.64 257.3 Z M 139.19 257.3"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 109.67 253.15)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Ensembles
    §[3.1.2.1](#S3.SS1.SSS2.P1 "3.1.2.1 Standard Ensembles ‣ 3.1.2 Multiple Network
    Models ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation")</foreignobject></g> <g fill="#CCCCCC"><path d="M
    139.19 183.49 M 124.05 180.81 C 133.83 184.38 144.56 184.38 154.34 180.81 C 143.94
    184.6 142.21 194.56 142.21 205.63 L 136.18 205.63 C 136.18 194.56 134.45 184.6
    124.05 180.81 Z M 136.18 205.63 L 136.18 205.63 L 142.21 205.63 L 142.21 205.63
    Z M 142.21 205.63 M 127.41 224.93 C 135.02 222.16 143.37 222.16 150.98 224.93
    C 142.88 221.99 142.21 214.24 142.21 205.63 L 136.18 205.63 C 136.18 214.24 135.5
    221.99 127.41 224.93 Z" style="stroke:none"></path></g><g fill="#FFF07E" stroke="#FFF07E"><path
    d="M -94.9 139.19 C -94.9 163.66 -114.73 183.49 -139.19 183.49 C -163.66 183.49
    -183.49 163.66 -183.49 139.19 C -183.49 114.73 -163.66 94.9 -139.19 94.9 C -114.73
    94.9 -94.9 114.73 -94.9 139.19 Z M -139.19 139.19"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -178.56 144.04)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Single Network
    Models §[3.1.1](#S3.SS1.SSS1 "3.1.1 Single Network Models ‣ 3.1 Architecture ‣
    3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <clippath ><path d="M -55.68 55.68 M -71.36 33.28 C -63.54 50.06 -50.06 63.54
    -33.28 71.36 C -51.12 63.04 -66.86 72.34 -80.78 86.26 L -86.26 80.78 C -72.34
    66.86 -63.04 51.12 -71.36 33.28 Z M -94.95 89.47 L -86.26 80.78 L -80.78 86.26
    L -89.47 94.95 Z M -80.78 86.26 M -120.47 99.05 C -111.03 103.45 -103.45 111.03
    -99.05 120.47 C -103.73 110.44 -97.3 102.78 -89.47 94.95 L -94.95 89.47 C -102.78
    97.3 -110.44 103.73 -120.47 99.05 Z"></path></clippath><g clip-path="url(#pgfcp23)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -55.68 55.68)"><g fill="#FFB3B3"><path d="M
    -56.66 -54.71 L -0.01 -111.36 L 111.36 0.01 L 54.71 56.66 Z M 111.36 0.01" style="stroke:none"></path></g><g
    fill="#FFF07E"><path d="M -106.9 -4.46 L -139.2 27.83 L -27.83 139.2 L 4.46 106.9
    Z M -27.83 139.2" style="stroke:none"><g transform="matrix(-0.37585 0.37585 -0.80185
    -0.80185 -26.1 26.1)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    fill="#FFF07E" stroke="#FFF07E"><path d="M -21.23 222.71 C -21.23 241.74 -36.65
    257.16 -55.68 257.16 C -74.7 257.16 -90.13 241.74 -90.13 222.71 C -90.13 203.69
    -74.7 188.26 -55.68 188.26 C -36.65 188.26 -21.23 203.69 -21.23 222.71 Z M -55.68
    222.71"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -85.21 226.86)" fill="#000000"
    stroke="#000000"><foreignobject width="59.06" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Recurrent CNNs S[3.1.1.5](#S3.SS1.SSS1.P5 "3.1.1.5
    Recurrent Convolutional Neural Networks ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#FFF07E"><path d="M -107.88 170.51 M -120.48 179.34 C -111.04 174.94
    -103.45 167.35 -99.05 157.91 C -103.73 167.95 -97.91 176.21 -90.08 184.04 L -94.35
    188.3 C -102.18 180.47 -110.44 174.66 -120.48 179.34 Z M -94.35 188.3 L -94.35
    188.3 L -90.08 184.04 L -90.09 184.04 Z M -90.08 184.04 M -86.9 208.15 C -83.48
    200.81 -77.58 194.91 -70.24 191.49 C -78.04 195.13 -84 190.13 -90.09 184.04 L
    -94.35 188.3 C -88.26 194.39 -83.26 200.35 -86.9 208.15 Z" style="stroke:none"></path></g><g
    fill="#FFF07E" stroke="#FFF07E"><path d="M -104.75 257.3 C -104.75 276.33 -120.17
    291.75 -139.19 291.75 C -158.22 291.75 -173.64 276.33 -173.64 257.3 C -173.64
    238.28 -158.22 222.86 -139.19 222.86 C -120.17 222.86 -104.75 238.28 -104.75 257.3
    Z M -139.19 257.3"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -168.72 261.46)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Attention Modules §[3.1.1.4](#S3.SS1.SSS1.P4
    "3.1.1.4 Attention Modules ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture ‣
    3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#FFF07E"><path d="M -139.19 183.49 M -154.34 180.81 C -144.56 184.38
    -133.83 184.38 -124.05 180.81 C -134.45 184.6 -136.18 194.56 -136.18 205.63 L
    -142.21 205.63 C -142.21 194.56 -143.94 184.6 -154.34 180.81 Z M -142.21 205.63
    L -142.21 205.63 L -136.18 205.63 L -136.18 205.63 Z M -136.18 205.63 M -150.98
    224.93 C -143.37 222.16 -135.02 222.16 -127.41 224.93 C -135.5 221.99 -136.18
    214.24 -136.18 205.63 L -142.21 205.63 C -142.21 214.24 -142.88 221.99 -150.98
    224.93 Z" style="stroke:none"></path></g><g fill="#FFF07E" stroke="#FFF07E"><path
    d="M -188.26 222.71 C -188.26 241.74 -203.69 257.16 -222.71 257.16 C -241.74 257.16
    -257.16 241.74 -257.16 222.71 C -257.16 203.69 -241.74 188.26 -222.71 188.26 C
    -203.69 188.26 -188.26 203.69 -188.26 222.71 Z M -222.71 222.71"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -252.24 226.86)" fill="#000000" stroke="#000000"><foreignobject
    width="59.06" height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multi-scale
    Modules §[3.1.1.3](#S3.SS1.SSS1.P3 "3.1.1.3 Multi-scale Modules ‣ 3.1.1 Single
    Network Models ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")</foreignobject></g> <g fill="#FFF07E"><path
    d="M -170.51 170.51 M -179.34 157.91 C -174.94 167.35 -167.35 174.94 -157.91 179.34
    C -167.95 174.66 -176.21 180.47 -184.04 188.3 L -188.3 184.04 C -180.47 176.21
    -174.66 167.95 -179.34 157.91 Z M -188.3 184.04 L -188.3 184.04 L -184.04 188.3
    L -184.04 188.3 Z M -184.04 188.3 M -208.15 191.49 C -200.81 194.91 -194.91 200.81
    -191.49 208.15 C -195.13 200.35 -190.13 194.39 -184.04 188.3 L -188.3 184.04 C
    -194.39 190.13 -200.35 195.13 -208.15 191.49 Z" style="stroke:none"></path></g><g
    fill="#FFF07E" stroke="#FFF07E"><path d="M -222.86 139.19 C -222.86 158.22 -238.28
    173.64 -257.3 173.64 C -276.33 173.64 -291.75 158.22 -291.75 139.19 C -291.75
    120.17 -276.33 104.75 -257.3 104.75 C -238.28 104.75 -222.86 120.17 -222.86 139.19
    Z M -257.3 139.19"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -286.83 143.35)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Conv. Modules §[3.1.1.2](#S3.SS1.SSS1.P2 "3.1.1.2
    Convolutional Modules ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture ‣ 3 Model
    Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")</foreignobject></g>
    <g fill="#FFF07E"><path d="M -183.49 139.19 M -180.81 124.05 C -184.38 133.83
    -184.38 144.56 -180.81 154.34 C -184.6 143.94 -194.56 142.21 -205.63 142.21 L
    -205.63 136.18 C -194.56 136.18 -184.6 134.45 -180.81 124.05 Z M -205.63 136.18
    L -205.63 136.18 L -205.63 142.21 L -205.63 142.21 Z M -205.63 142.21 M -224.93
    127.41 C -222.16 135.02 -222.16 143.37 -224.93 150.98 C -221.99 142.88 -214.24
    142.21 -205.63 142.21 L -205.63 136.18 C -214.24 136.18 -221.99 135.5 -224.93
    127.41 Z" style="stroke:none"></path></g><g fill="#FFF07E" stroke="#FFF07E"><path
    d="M -188.26 55.68 C -188.26 74.7 -203.69 90.13 -222.71 90.13 C -241.74 90.13
    -257.16 74.7 -257.16 55.68 C -257.16 36.65 -241.74 21.23 -222.71 21.23 C -203.69
    21.23 -188.26 36.65 -188.26 55.68 Z M -222.71 55.68"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -252.24 59.83)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Shortcut
    Connections §[3.1.1.1](#S3.SS1.SSS1.P1 "3.1.1.1 Shortcut Connections ‣ 3.1.1 Single
    Network Models ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")</foreignobject></g><g fill="#FFF07E"><path
    d="M -170.51 107.88 M -157.91 99.05 C -167.35 103.45 -174.94 111.04 -179.34 120.48
    C -174.66 110.44 -180.47 102.18 -188.3 94.35 L -184.04 90.08 C -176.21 97.91 -167.95
    103.73 -157.91 99.05 Z M -184.04 90.09 L -184.04 90.08 L -188.3 94.35 L -188.3
    94.35 Z M -188.3 94.35 M -191.49 70.24 C -194.91 77.58 -200.81 83.48 -208.15 86.9
    C -200.35 83.26 -194.39 88.26 -188.3 94.35 L -184.04 90.09 C -190.13 84 -195.13
    78.04 -191.49 70.24 Z" style="stroke:none"></path></g>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Taxonomy of DL-based skin lesion segmentation model architectures.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Single Network Models
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The approaches in this section employ a single DL model, usually an FCN, following
    an encoder-decoder structure, where the encoder extracts increasingly abstract
    features, and the decoder outputs the segmentation mask. In this section, we discuss
    these architectural choices for designing deep models for skin lesion segmentation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Earlier DL-based skin lesion segmentation works adopted either FCN (Long et al.,
    [2015](#bib.bib264)) or U-Net (Ronneberger et al., [2015](#bib.bib328)). FCN originally
    comprised a backbone of VGG16 (Simonyan and Zisserman, [2014](#bib.bib353)) CNN
    layers in the encoder and a single deconvolution layer in the encoder. The original
    paper proposes three versions, two with skip connections (FCN-8 and FCN-16), and
    one without them (FCN-32). U-Net (Ronneberger et al., [2015](#bib.bib328)), originally
    proposed for segmenting electron microscopy images, was rapidly adopted in the
    medical image segmentation literature. As its name suggests, it is a U-shaped
    model, with an encoder stacking convolutional layers that double in size filterwise,
    intercalated by pooling layers, and a symmetric decoder with pooling layers replaced
    by up-convolutions. Skip connections between corresponding encoder-decoder blocks
    improve the flow of information between layers, preserving low-level features
    lost during pooling and producing detailed segmentation boundaries.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: U-Net frequently appears in the skin lesion segmentation literature both in
    its original form (Codella et al., [2017](#bib.bib97); Pollastri et al., [2020](#bib.bib309);
    Ramani and Ranjani, [2019](#bib.bib318)) and modified forms  (Tang et al., [2019a](#bib.bib371);
    Alom et al., [2019](#bib.bib22); Hasan et al., [2020](#bib.bib172)), discussed
    below. Some works introduce their own models (Yuan et al., [2017](#bib.bib433);
    Al-Masni et al., [2018](#bib.bib15)).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9a4cebf8afb4dde7928bbccb452b7605.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The frequency of utilization of different architectural modules in
    the surveyed studies. Shortcut connections, particularly, skip connections ($112$
    papers) and residual connections ($70$ papers) are the two most frequent components
    in DL-based skin lesion segmentation models. Attention mechanisms learn dependencies
    between elements in sequences, either spatially or channel-wise, and are therefore
    used by several encoder-decoder-style segmentation models ($41$ papers). Dilated
    convolutions help expand the receptive field of CNN-models without any additional
    parameters, which is why they are the most popular variant of convolution in the
    surveyed studies ($35$ papers). Finally, papers using Transformers ($12$ papers)
    started appearing from 2021 onwards and are on the rise.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1.1 Shortcut Connections
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Connections between early and late layers in FCNs have been widely explored
    to improve both the forward and backward (gradient) information flow in the models,
    facilitating the training. The three most popular types of connections are described
    below.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Residual connections: Creating non-linear blocks that add their unmodified
    inputs to their outputs (He et al., [2016](#bib.bib174)) alleviates gradient degradation
    in very deep networks. It provides a direct path for the gradient to flow through
    to the early layers of the network, while still allowing for very deep models.
    The technique appears often in skin lesion segmentation, in the implementation
    of the encoder (Sarker et al., [2018](#bib.bib342); Baghersalimi et al., [2019](#bib.bib35);
    Yu et al., [2017a](#bib.bib431)) or both encoder and decoder (He et al., [2017](#bib.bib175);
    Venkatesh et al., [2018](#bib.bib393); Li et al., [2018a](#bib.bib243); Tu et al.,
    [2019](#bib.bib383); Zhang et al., [2019a](#bib.bib437); He et al., [2018](#bib.bib176);
    Xue et al., [2018](#bib.bib423)). Residual connections have also appeared in recurrent
    units (Alom et al., [2019](#bib.bib22), [2020](#bib.bib21)), dense blocks (Song
    et al., [2019](#bib.bib359)), chained pooling (He et al., [2017](#bib.bib175);
    Li et al., [2018a](#bib.bib243); He et al., [2018](#bib.bib176)), and 1-D factorized
    convolutions (Singh et al., [2019](#bib.bib355)).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Skip connections appear in encoder-decoder architectures, connecting high-resolution
    features from the encoder’s contracting path to the semantic features on the decoder’s
    expanding path (Ronneberger et al., [2015](#bib.bib328)). These connections help
    preserve localization, especially near region boundaries, and combine multi-scale
    features, resulting in sharper boundaries in the predicted segmentation. Skip
    connections are very popular in skin lesion segmentation because they are effective
    and easy to implement (Zhang et al., [2019a](#bib.bib437); Baghersalimi et al.,
    [2019](#bib.bib35); Song et al., [2019](#bib.bib359); Wei et al., [2019](#bib.bib412);
    Venkatesh et al., [2018](#bib.bib393); Azad et al., [2019](#bib.bib29); He et al.,
    [2017](#bib.bib175); Alom et al., [2019](#bib.bib22); Sarker et al., [2018](#bib.bib342);
    Zeng and Zheng, [2018](#bib.bib436); Li et al., [2018a](#bib.bib243); Tu et al.,
    [2019](#bib.bib383); Yu et al., [2017a](#bib.bib431); Singh et al., [2019](#bib.bib355);
    He et al., [2018](#bib.bib176); Xue et al., [2018](#bib.bib423); Alom et al.,
    [2020](#bib.bib21); Vesal et al., [2018b](#bib.bib395); Liu et al., [2019b](#bib.bib258)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Dense connections expand the convolutional layers by connecting each layer to
    all its subsequent layers, concatenating their features (Huang et al., [2017](#bib.bib182)).
    Iterative reuse of features in dense connections maximizes information flow forward
    and backward. Similar to deep supervision (Section [3.2.5](#S3.SS2.SSS5 "3.2.5
    Deep Supervision Loss ‣ 3.2 Loss Functions ‣ 3 Model Design and Training ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), the gradient is propagated backwards
    directly through all previous layers. Several works (Zeng and Zheng, [2018](#bib.bib436);
    Song et al., [2019](#bib.bib359); Li et al., [2021c](#bib.bib247); Tu et al.,
    [2019](#bib.bib383); Vesal et al., [2018b](#bib.bib395)) integrated dense blocks
    in both the encoder and the decoder. Baghersalimi et al. ([2019](#bib.bib35)),
    Hasan et al. ([2020](#bib.bib172)) and Wei et al. ([2019](#bib.bib412)) used multiple
    dense blocks iteratively in only the encoder, while Li et al. ([2018a](#bib.bib243))
    proposed dense deconvolutional blocks to reuse features from the previous layers.
    Azad et al. ([2019](#bib.bib29)) encoded densely connected convolutions into the
    bottleneck of their encoder-decoder to obtain better features.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1.2 Convolutional Modules
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As mentioned earlier, convolution not only provides a structural advantage,
    respecting the local connectivity structure of images in the output futures, but
    also dramatically improves parameter sharing since the parameters of a relatively
    small convolutional kernel are shared by all patches of a large image. Convolution
    is a critical element of deep segmentation models. In this section, we discuss
    some new convolution variants, which have enhanced and diversified this operation,
    appearing in the skin lesion segmentation literature.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Dilated convolution: In contrast to requiring full-resolution outputs in dense
    prediction networks, pooling and striding operations have been adopted in deep
    convolutional neural networks (DCNNs) to increase the receptive field and diminish
    the spatial resolution of feature maps. Dilated or atrous convolutions are designed
    specifically for the semantic segmentation task to exponentially expand the receptive
    fields while keeping the number of parameters constant (Yu and Koltun, [2016](#bib.bib430)).
    Dilated convolutions are convolutional modules with upsampled filters containing
    zeros between consecutive filter values. Sarker et al. ([2018](#bib.bib342)) and
    Jiang et al. ([2019](#bib.bib203)) utilized dilated residual blocks in the encoder
    to control the image field-of-view explicitly and incorporated multi-scale contextual
    information into the segmentation network. SkinNet (Vesal et al., [2018b](#bib.bib395))
    used dilated convolutions at the lower level of the network to enlarge the field-of-view
    and capture non-local information. Liu et al. ([2019b](#bib.bib258)) introduced
    dilated convolutions to the U-Net architecture, significantly improving the segmentation
    performance. Furthermore, different versions of the DeepLab architecture (Chen
    et al., [2017a](#bib.bib86), [b](#bib.bib87), [2018a](#bib.bib88)), which replace
    standard convolutions with dilated ones, have been used in skin lesion segmentation (Goyal
    et al., [2019a](#bib.bib153), [b](#bib.bib154); Cui et al., [2019](#bib.bib105);
    Chen et al., [2018b](#bib.bib90); Canalini et al., [2019](#bib.bib70)).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Separable convolution: Separable convolution or depth-wise separable convolution (Chollet,
    [2017](#bib.bib93)) is a spatial convolution operation that convolves each input
    channel with its corresponding kernel. This is followed by a $1\times 1$ standard
    convolution to capture the channel-wise dependencies in the output of depth-wise
    convolution. Depth-wise convolutions are designed to reduce the number of parameters
    and the computation of standard convolutions while maintaining the accuracy. DSNet (Hasan
    et al., [2020](#bib.bib172)) and separable-Unet (Tang et al., [2019a](#bib.bib371))
    utilized depth-wise separable convolutions in the model to have a lightweight
    network with a reduced number of parameters. Adopted from the DeepLab architecture,
    Goyal et al. ([2019b](#bib.bib154)), Cui et al. ([2019](#bib.bib105)) and, Canalini
    et al. ([2019](#bib.bib70)) incorporated depth-wise separable convolutions in
    conjunction with dilated convolution to improve the speed and accuracy of dense
    predictions.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'Global convolution: State-of-the-art segmentation models remove densely connected
    and global pooling layers to preserve spatial information required for full-resolution
    output recovery. As a result, by keeping high-resolution feature maps, segmentation
    models become more suitable for localization and, in contrast, less suitable for
    per-pixel classification, which needs transformation invariant features. To increase
    the connectivity between feature maps and classifiers, large convolutional kernels
    should be adopted. However, such kernels have a large number of parameters, which
    renders them computationally expensive. To tackle this, global convolutional network
    (GCN) modules adopt a combination of symmetric parallel convolutions in the form
    of $1\times k+k\times 1$ and $k\times 1+1\times k$ to cover a $k\times k$ area
    of feature maps (Peng et al., [2017b](#bib.bib303)). SeGAN (Xue et al., [2018](#bib.bib423))
    employed GCN modules with large kernels in the generator’s decoder to reconstruct
    segmentation masks and in the discriminator architecture to optimally capture
    a larger receptive field.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Factorized convolution: Factorized convolutions (Wang et al., [2017](#bib.bib400))
    are designed to reduce the number of convolution filter parameters as well as
    the computation time through kernel decomposition when a high-dimensional kernel
    is substituted with a sequence of lower-dimensional convolutions. Additionally,
    by adding non-linearity between the composited kernels, the network’s capacity
    may improve. FCA-Net (Singh et al., [2019](#bib.bib355)) and MobileGAN (Sarker
    et al., [2019](#bib.bib341)) utilized residual 1-D factorized convolutions (a
    sequence of $k\times 1$ and $1\times k$ convolutions with ReLU non-linearity)
    in their segmentation architecture.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1.3 Multi-scale Modules
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In FCNs, taking semantic context into account when assigning per-pixel labels
    leads to a more accurate prediction (Long et al., [2015](#bib.bib264)). Exploiting
    multi-scale contextual information, effectively combining them as well as encoding
    them in deep semantic segmentation have been widely explored.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Pyramid: RefineNet (He et al., [2017](#bib.bib175)) and its extension (He
    et al., [2018](#bib.bib176)), MSFCDN (Zeng and Zheng, [2018](#bib.bib436)), FCA-Net (Singh
    et al., [2019](#bib.bib355)), and Abraham and Khan ([2019](#bib.bib9)) fed a pyramid
    of multi-resolution skin lesion images as input to their deep segmentation network
    to extract multi-scale discriminative features. RefineNet (He et al., [2017](#bib.bib175),
    [2018](#bib.bib176)), Factorized channel attention network (FCA-Net (Singh et al.,
    [2019](#bib.bib355))) and Abraham and Khan ([2019](#bib.bib9)) applied convolutional
    blocks to different image resolutions in parallel to generate features which are
    then up-sampled in order to fuse multi-scale feature maps. Multi-scale fully convolutional
    DenseNets (MSFCDN (Zeng and Zheng, [2018](#bib.bib436))) gradually integrated
    multi-scale features extracted from the image pyramid into the encoder’s down-sampling
    path. Also,  Jafari et al. ([2016](#bib.bib194), [2017](#bib.bib195)) extracted
    multi-scale patches from clinical images to predict semantic labels and refine
    lesion boundaries by deploying local and global information. While aggregating
    the feature maps computed at various image scales improves the segmentation performance,
    it also increases the computational cost of the network.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel multi-scale convolutions: Alternatively, given a single image resolution,
    multiple convolutional filters with different kernel sizes (Zhang et al., [2019a](#bib.bib437);
    Wang et al., [2019a](#bib.bib397); Jahanifar et al., [2018](#bib.bib196)) or multiple
    dilated convolutions with different dilation rates (Goyal et al., [2019a](#bib.bib153),
    [b](#bib.bib154); Cui et al., [2019](#bib.bib105); Chen et al., [2018b](#bib.bib90);
    Canalini et al., [2019](#bib.bib70)) can be adopted in parallel paths to extract
    multi-scale contextual features from images. DSM (Zhang et al., [2019a](#bib.bib437))
    integrated multi-scale convolutional blocks into the skip connections of an encoder-decoder
    structure to handle different lesion sizes. Wang et al. ([2019a](#bib.bib397))
    utilized multi-scale convolutional branches in the bottleneck of an encoder-decoder
    architecture, followed by attention modules to selectively aggregate the extracted
    multi-scale features.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyramid pooling: Another way of incorporating multi-scale information into
    deep segmentation models is to integrate a pyramid pooling (PP) module in the
    network architecture (Zhao et al., [2017](#bib.bib450)). PP fuses a hierarchy
    of features extracted from different sub-regions by adopting parallel pooling
    kernels of various sizes, followed by up-sampling and concatenation to create
    the final feature maps. Sarker et al. ([2018](#bib.bib342)) and Jahanifar et al.
    ([2018](#bib.bib196)) utilized PP in the decoder to benefit from coarse-to-fine
    features extracted by different receptive fields from skin lesion images.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Dilated convolutions and skip connections are two other types of multi-scale
    information extraction techniques, which are explained in Sections [3.1.1.2](#S3.SS1.SSS1.P2
    "3.1.1.2 Convolutional Modules ‣ 3.1.1 Single Network Models ‣ 3.1 Architecture
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")
    and [3.1.1.1](#S3.SS1.SSS1.P1 "3.1.1.1 Shortcut Connections ‣ 3.1.1 Single Network
    Models ‣ 3.1 Architecture ‣ 3 Model Design and Training ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation"), respectively.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1.4 Attention Modules
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An explicit way to exploit contextual dependencies in the pixel-wise labeling
    task is the self-attention mechanism (Hu et al., [2018](#bib.bib181); Fu et al.,
    [2019](#bib.bib139)). Two types of attention modules capture global dependencies
    in spatial and channel dimensions by integrating features among all positions
    and channels, respectively. Wang et al. ([2019a](#bib.bib397)) and Sarker et al.
    ([2019](#bib.bib341)) leveraged both spatial and channel attention modules to
    recalibrate the feature maps by examining the feature similarity between pairs
    of positions or channels and updating each feature value by a weighted sum of
    all other features. Singh et al. ([2019](#bib.bib355)) utilized a channel attention
    block in the proposed factorized channel attention (FCA) blocks, which was used
    to investigate the correlation of different channel maps for extraction of relevant
    patterns. Inspired by attention U-Net (Oktay et al., [2018](#bib.bib292)), multiple
    works (Abraham and Khan, [2019](#bib.bib9); Song et al., [2019](#bib.bib359);
    Wei et al., [2019](#bib.bib412)) integrated a spatial attention gate in an encoder-decoder
    architecture to combine coarse semantic feature maps and fine localization feature
    maps. Kaul et al. ([2019](#bib.bib215)) proposed FocusNet which utilizes squeeze-and-excitation
    blocks into a hybrid encoder-decoder architecture. Squeeze-and-excitation blocks
    model the channel-wise interdependencies to re-weight feature maps and improve
    their representation power. Experimental results demonstrate that attention modules
    help the network focus on the lesions and suppress irrelevant feature responses
    in the background.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1.5 Recurrent Convolutional Neural Networks
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recurrent convolutional neural networks (RCNN) integrate recurrent connections
    into convolutional layers by evolving the recurrent input over time (Pinheiro
    and Collobert, [2014](#bib.bib307)). Stacking recurrent convolutional layers (RCL)
    on top of the convolutional layer feature extractors ensures capturing spatial
    and contextual dependencies in images while limiting the network capacity by sharing
    the same set of parameters in RCL blocks. In the application of skin lesion segmentation,
    Attia et al. ([2017](#bib.bib28)) utilized recurrent layers in the decoder to
    capture spatial dependencies between deep-encoded features and recover segmentation
    maps at the original resolution. $\nabla^{N}$-Net (Alom et al., [2020](#bib.bib21)),
    RU-Net, and R2U-Net (Alom et al., [2019](#bib.bib22)) incorporated RCL blocks
    into the FCN architecture to accumulate features across time in a computationally
    efficient way and boosted the skin lesion boundary detection. Azad et al. ([2019](#bib.bib29))
    deployed a non-linear combination of the encoder feature and decoder feature maps
    by adding a bi-convolutional LSTM (BConvLSTM) in skip connections. BConvLSTM consists
    of two independent convolutional LSTM modules (ConvLSTMs) which process the feature
    maps into two directions of backward and forward paths and concatenate their outputs
    to obtain the final output. Modifications to the traditional pooling layers were
    also proposed, using a dense pooling strategy (Nasr-Esfahani et al., [2019](#bib.bib285)).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Multiple Network Models
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Motivations for models comprising more than one DL sub-model are diverse, ranging
    from alleviating training noise and exploiting a diversity of features learned
    by different models to exploring synergies between multi-task learners. After
    examining the literature (Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Architecture ‣ 3 Model
    Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")),
    we further classified the works in this section into standard ensembles and multi-task
    models. We also discuss generative adversarial models, which are intrinsically
    multi-network models, in a separate category.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2.1 Standard Ensembles
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Ensemble models are widely used in machine learning, motivated by the hope that
    the complementarity of different models may lead to more stable combined predictions (Sagi
    and Rokach, [2018](#bib.bib336)). Ensemble performance is contingent on the quality
    and diversity of the component models, which can be combined at the feature level
    (early fusion) or the prediction level (late fusion). The former combines the
    features extracted by the components and learns a meta-model on them, while the
    latter pools or combines the models’ predictions with or without a meta-model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: All methods discussed in this section employ late fusion, except for an approach
    loosely related to early fusion (Tang et al., [2019a](#bib.bib371)), which explores
    various learning-rate decay schemes, and builds a single model by averaging the
    weights learned at different epochs to bypass poor local minima during training.
    Since the weights correspond to features learned by the convolution filters, this
    approach can be interpreted as feature fusion.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Most works employ a single DL architecture with multiple training routines,
    varying configurations more or less during training (Canalini et al., [2019](#bib.bib70)).
    The changes between component models may involve network hyperparameters: number
    of filters per block and their size (Codella et al., [2017](#bib.bib97)); optimization
    and regularization hyperparameters: learning rate, weight decay (Tan et al., [2019b](#bib.bib370));
    the training set: multiple splits of a training set (Yuan et al., [2017](#bib.bib433);
    Yuan and Lo, [2019](#bib.bib434)), separate models per class (Bi et al., [2019b](#bib.bib52));
    preprocessing: different color spaces (Pollastri et al., [2020](#bib.bib309));
    different pretraining strategies to initialize feature extractors (Canalini et al.,
    [2019](#bib.bib70)); or different ways to initialize the network parameters (Cui
    et al., [2019](#bib.bib105)). Test-time augmentation may also be seen as a form
    of inference-time ensembling (Chen et al., [2018b](#bib.bib90); Liu et al., [2019b](#bib.bib258);
    Jahanifar et al., [2018](#bib.bib196)) that combines the outputs of multiple augmented
    images to generate a more reliable prediction.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Bi et al. ([2019b](#bib.bib52)) trained a separate DL model for each class,
    as well as a separate classification model. For inference, the classification
    model output is used to weight the outputs of the category-specific segmentation
    networks. In contrast, Soudani and Barhoumi ([2019](#bib.bib361)) trained a meta
    “recommender” model to dynamically choose, for each input, a segmentation technique
    from the top five scorers in the ISIC 2017 challenge, although their proposition
    was validated on a very small test set ($10\%$ of ISIC 2017 test set).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Several works also ensemble different model architectures for skin lesion segmentation.
    Goyal et al. ([2019b](#bib.bib154)) investigate multiple fusion approaches to
    avoid severe errors from individual models, comparing the average-, maximum- and
    minimum-pooling of their outputs. A common assumption is that the component models
    of the ensemble are trained independently, but Bi et al. ([2017b](#bib.bib53))
    cascaded the component models, i.e., used the output of one model as the input
    of the next (in association with the actual image input). Thus, each model attempts
    to refine the segmentation obtained by the previous one. They consider not only
    the final model output, but all the outputs in the cascade, making the technique
    a legitimate ensemble.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2.2 Multi-task Models
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Multi-task models jointly address more than one goal, in the hope that synergies
    among the tasks will improve overall performance (Zhang and Yang, [2022](#bib.bib446)).
    This can be particularly helpful in medical image analysis, wherein aggregating
    tasks may alleviate the issue of insufficient data or annotations. For skin lesions,
    a few multi-task models exploiting segmentation and classification have been proposed (Chen
    et al., [2018b](#bib.bib90); Li and Shen, [2018](#bib.bib251); Yang et al., [2018](#bib.bib426);
    Xie et al., [2020b](#bib.bib420); Jin et al., [2021](#bib.bib206)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The synergy between tasks may appear when their models share common relevant
    features. Li and Shen ([2018](#bib.bib251)) assume that all features are shareable
    between the tasks, and train a single fully convolutional residual network to
    assign class probabilities at the pixel level. They aggregate the class probability
    maps to estimate both lesion region and class by weighted averaging of probabilities
    for different classes inside the lesion area. Yang et al. ([2018](#bib.bib426))
    learn an end-to-end model formed by a shared convolutional feature extractor followed
    by three task-specific branches (one to segment skin lesions, one to classify
    them as melanoma versus non-melanoma, and one to classify them as seborrheic keratosis
    versus non-seborrheic keratosis.) Similarly, Chen et al. ([2018b](#bib.bib90))
    add a common feature extractor and separate task heads, and introduce a learnable
    gate function that controls the flow of information between the tasks to model
    the latent relationship between two tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a single architecture for classification and segmentation,
    Xie et al. ([2020b](#bib.bib420)) and Jin et al. ([2021](#bib.bib206)) use three
    CNNs in sequence to perform a coarse segmentation, followed by classification
    and, finally, a fine segmentation. Instead of shared features, these works exploit
    sequential guidance, in which the output of each task improves the learning of
    the next. While Xie et al. ([2020b](#bib.bib420)) feed the output of each network
    to the next, assuming that the classification network is a diagnostic category
    and a class activation map (Zhou et al., [2016](#bib.bib454)), Jin et al. ([2021](#bib.bib206))
    introduce feature entanglement modules, which aggregate features learned by different
    networks.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: All multi-task models discussed so far have results suggesting complementarity
    between classification and segmentation, but there is no clear advantage among
    these models. The segmentation of dermoscopic features (e.g., networks, globules,
    regression areas) combined with the other tasks is a promising avenue of research,
    which could bridge classification and segmentation, by fostering the extraction
    of features that “see” the lesion as human specialists do.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: We do not consider in the hybrid group, two-stage models in which segmentation
    is used as ancillary preprocessing to classification (Yu et al., [2017a](#bib.bib431);
    Codella et al., [2017](#bib.bib97); Gonzalez-Diaz, [2018](#bib.bib151); Al-Masni
    et al., [2020](#bib.bib17)), since without mutual influence (sharing of losses
    or features) or feedback between the two tasks, there is no opportunity for synergy.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Vesal et al. ([2018a](#bib.bib394)) stressed the importance of object localization
    as an ancillary task for lesion delineation, in particular deploying Faster-RCNN (Ren
    et al., [2015](#bib.bib323)) to regress a bounding box to crop the lesions before
    training a SkinNet segmentation model. While this two-stage approach considerably
    improves the results, it is computationally expensive (a fast non-DL-based bounding
    box detection algorithm was proposed earlier by Celebi et al. ([2009a](#bib.bib75))).
    Goyal et al. ([2019a](#bib.bib153)) employed ROI detection with a deep extreme
    cut to extract the extreme points of lesions (leftmost, rightmost, topmost, bottommost
    pixels) and feed them, in a new auxiliary channel, to a segmentation model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2.3 Generative Adversarial Models
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We discussed GANs for synthesizing new samples, their main use in skin lesion
    analysis, in Section [2.2](#S2.SS2 "2.2 Synthetic Data Generation ‣ 2 Input Data
    ‣ A Survey on Deep Learning for Skin Lesion Segmentation"). In this section, we
    are interested in GANs not for generating additional training samples, but for
    directly providing enhanced segmentation models. Adversarial training encourages
    high-order consistency in predicted segmentation by implicitly looking into the
    joint distribution of class labels and ground-truth segmentation masks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Peng et al. ([2019](#bib.bib304)), Tu et al. ([2019](#bib.bib383)), Lei et al.
    ([2020](#bib.bib241)), and Izadi et al. ([2018](#bib.bib190)) use a U-Net-like
    generator that takes a dermoscopic image as input, and outputs the corresponding
    segmentation, while the discriminator is a traditional CNN which attempts to discriminate
    pairs of image and generated segmentation from pairs of image and ground-truth.
    The generator has to learn to correctly segment the lesion in order to fool the
    discriminator. Jiang et al. ([2019](#bib.bib203)) use the same scheme, with a
    dual discriminator. Lei et al. ([2020](#bib.bib241)) also employ a second discriminator
    that takes as input only segmentations (unpaired from input images).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Since the discriminator may trivially learn to recognize the generated masks
    due to the presence of continuous probabilities, instead of the sharp discrete
    boundaries of the ground-truths, Wei et al. ([2019](#bib.bib412)) and Tu et al.
    ([2019](#bib.bib383)) address this by pre-multiplying both the generated and real
    segmentations with the (normalized) input images before feeding them to the discriminator.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: We discuss adversarial loss functions further in Section [3.2.8](#S3.SS2.SSS8
    "3.2.8 Adversarial Loss ‣ 3.2 Loss Functions ‣ 3 Model Design and Training ‣ A
    Survey on Deep Learning for Skin Lesion Segmentation").
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Hybrid Feature Models
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the major strength of CNNs is their ability to learn meaningful image
    features without human intervention, a few works tried to combine the best of
    both worlds, with strategies ranging from employing pre- or postprocessing to
    enforce prior knowledge to adding hand-crafted features. Providing the model with
    prior knowledge about the expected shape of skin lesions—which is missing from
    CNNs—may improve the performance. Mirikharaji and Hamarneh ([2018](#bib.bib279))
    encode shape information into an additional regularization loss, which penalizes
    segmentation maps that deviate from a star-shaped prior (Section [3.2.6](#S3.SS2.SSS6
    "3.2.6 Star-Shape Loss ‣ 3.2 Loss Functions ‣ 3 Model Design and Training ‣ A
    Survey on Deep Learning for Skin Lesion Segmentation")).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Conditional random fields (CRFs) use pixel-level color information models to
    refine the segmentation masks output by the CNN. While both Tschandl et al. ([2019](#bib.bib382))
    and Adegun and Viriri ([2020b](#bib.bib12)) consider a single CNN, Qiu et al.
    ([2020](#bib.bib312)) combine the outputs of multiple CNNs into a single mask,
    before feeding it together with the input image to the CRFs. Ünver and Ayan ([2019](#bib.bib385))
    use GrabCut (Rother et al., [2004](#bib.bib332)) to obtain the segmentation mask
    given the dermoscopy image and a region proposal obtained by the YOLO (Redmon
    et al., [2016](#bib.bib322)) network. These methods regularize the CNN segmentation,
    which is mainly based on textural patterns, with expected priors based on the
    color of the pixels.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Works that combine hand-crafted features with CNNs follow two distinct approaches.
    The first consists of pre-filtering the input images to increase the contrast
    between the lesion and the surrounding skin. Techniques explored include local
    binary patterns (LBPs) (Ross-Howe and Tizhoosh, [2018](#bib.bib329); Jayapriya
    and Jacob, [2020](#bib.bib199)), wavelets (Ross-Howe and Tizhoosh, [2018](#bib.bib329)),
    Laplacian pyramids (Pour and Seker, [2020](#bib.bib311)), and Laplacian filtering (Saba
    et al., [2019](#bib.bib334)). The second approach consists of predicting an additional
    segmentation mask to combine with the one generated by the CNN. Zhang et al. ([2019b](#bib.bib441)),
    for example, use LBPs to consider the textural patterns of skin lesions and guide
    the networks towards more refined segmentations. Bozorgtabar et al. ([2017b](#bib.bib66))
    also employ LBPs combined with pixel-level color information to divide the dermoscopic
    image into superpixels, which are then scored as part of the lesion or the background.
    The score mask is then combined with the CNN output mask to compute the final
    segmentation mask. Despite the limited number of works devoted to integrating
    deep features with hand-crafted ones, the results so far indicate that this may
    be a promising research direction.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Transformer Models
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Initially proposed for natural language processing (Vaswani et al., [2017](#bib.bib392)),
    Transformers have proliferated in the last couple of years in other areas, including
    computer vision applications, especially with improvements made over the years
    for optimizing the computational cost of self-attention (Parmar et al., [2018](#bib.bib298);
    Hu et al., [2019](#bib.bib180); Ramachandran et al., [2019](#bib.bib316); Cordonnier
    et al., [2019](#bib.bib102); Zhao et al., [2020](#bib.bib449); Dosovitskiy et al.,
    [2020](#bib.bib127)), and have consequently also been adapted for semantic segmentation
    tasks (Ranftl et al., [2021](#bib.bib320); Strudel et al., [2021](#bib.bib362);
    Zheng et al., [2021](#bib.bib453)). For medical image segmentation, TransUNet (Chen
    et al., [2021](#bib.bib85)) was one of the first works to use Transformers along
    with CNNs in the encoder of a U-Net-like encoder-decoder architecture, and Gulzar
    and Khan ([2022](#bib.bib164)) showed that TransUNet outperforms several CNN-only
    models for skin lesion segmentation. To reduce the computational complexity involved
    with high-resolution medical images, Cao et al. ([2021](#bib.bib71)) proposed
    the Swin-Unet architecture that uses self-attention within shifted windows (Liu
    et al., [2021b](#bib.bib263)). For a comprehensive review of the literature of
    Transformers in general medical image analysis, we refer the interested readers
    to the surveys by He et al. ([2022](#bib.bib173)) and Shamshad et al. ([2022](#bib.bib346)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. ([2021b](#bib.bib445)) propose TransFuse which parallelly computes
    features from CNN and Transformer modules, with the former capturing low-level
    spatial information and the latter responsible for modeling global context, and
    these features are then combined using a self-attention-based fusion module. Evaluation
    on the ISIC 2017 dataset shows superior segmentation performance and faster convergence.
    The multi-compound Transformer (Ji et al., [2021](#bib.bib202)) leverages Transformer-based
    self-attention and cross-attention modules between the encoder and the decoder
    components of U-Net to learn rich features from multi-scale CNN features. Wang
    et al. ([2021a](#bib.bib399)) incorporate boundary-wise prior knowledge in segmentation
    models using a boundary-aware Transformer (BAT) to deal with the ambiguous boundaries
    in skin lesion images. More recently, Wu et al. ([2022a](#bib.bib415)) introduce
    a feature-adaptive Transformer network (FAT-Net) that comprised of a dual CNN-Transformer
    encoder, a light-weight trainable feature-adaptation module, and a memory-efficient
    decoder using a squeeze-and-excitation module. The resulting segmentation model
    is more accurate at segmenting skin lesions while also being faster (fewer parameters
    and computation) than several CNN-only models.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Loss Functions
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A segmentation model $f$ may be formalized as a function $\hat{y}=f_{\theta}(x)$,
    which maps an input image $x$ to an estimated segmentation map $\hat{y}$ parameterized
    by a (large) set of parameters $\theta$. For skin lesions, $\hat{y}$ is a binary
    mask separating the lesion from the surrounding skin. Given a training set of
    images $x_{i}$ and their corresponding ground-truth masks $y_{i}$ $\{(x_{i},y_{i});i=1,...,N\}$,
    training a segmentation model consists of finding the model parameters $\theta$
    that maximize the likelihood of observing those data:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta^{*}=\operatorname*{arg\,max}_{\theta}\sum_{i=1}^{N}\log\mathrm{P}(y_{i}&#124;x_{i};\theta),$
    |  | (1) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: 'which is performed indirectly, via the minimization of a loss function between
    the estimated and the true segmentation masks:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta^{*}=\operatorname*{arg\,min}_{\theta}\sum_{i=1}^{N}\mathcal{L}(\hat{y}_{i}&#124;y_{i})=\operatorname*{arg\,min}_{\theta}\sum_{i=1}^{N}\mathcal{L}(f_{\theta}(x_{i})&#124;y_{i}).$
    |  | (2) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: The choice of the loss function is thus critical, as it encodes not only the
    main optimization objective, but also much of the prior information needed to
    guide the learning and constrain the search space. As can been in Table LABEL:tab:main,
    many skin lesion segmentation models employ a combination of losses to enhance
    generalization (see Fig. [8](#S3.F8 "Figure 8 ‣ 3.2 Loss Functions ‣ 3 Model Design
    and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/54702960d2f783a7c3546355375b8b8e.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The distribution of loss functions used by the surveyed works in
    DL-based skin lesion segmentation. Cross-entropy loss is the most popular loss
    function ($96$ papers), followed by Dice ($53$ papers) and Jaccard ($19$ papers)
    losses. Of the $177$ surveyed papers, $65$ use a combination of losses, with CE
    + Dice ($27$ papers) and CE + Jaccard ($11$ papers) being the most popular combinations.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Losses based on $p$-norms
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Losses based on $p$-norms are the simplest ones, and comprise the mean squared
    error (MSE) (for $p=2$) and the mean absolute error (MAE) (for $p=1$).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathsf{MSE}(X,Y;\theta)=-\sum_{i=1}^{N}\&#124;y_{i}-\hat{y}_{i}\&#124;_{2},$
    |  | (3) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathsf{MAE}(X,Y;\theta)=-\sum_{i=1}^{N}\&#124;y_{i}-\hat{y}_{i}\&#124;_{1}.$
    |  | (4) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: In GANs, to regularize the segmentations produced by the generator, it is common
    to utilize hybrid losses containing MSE ($\ell_{2}$ loss) (Peng et al., [2019](#bib.bib304))
    or MAE ($\ell_{1}$ loss) (Peng et al., [2019](#bib.bib304); Tu et al., [2019](#bib.bib383);
    Lei et al., [2020](#bib.bib241)). The MSE has also been used as a regularizer
    to match attention and ground-truth maps (Xie et al., [2020a](#bib.bib419)).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Cross-entropy Loss
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Semantic segmentation may be viewed as classification at the pixel level, i.e.,
    as assigning a class label to each pixel. From this perspective, minimizing the
    negative log-likelihoods of pixel-wise predictions (i.e., maximizing their likelihood)
    may be achieved by minimizing a cross-entropy loss $\mathcal{L}_{ce}$:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{ce}(X,Y;\theta)=-\sum_{i=1}^{N}\sum_{p\in\Omega_{i}}y_{ip}\log\hat{y}_{ip}+(1-y_{ip})\log(1-\hat{y}_{ip}),~{}~{}\hat{y}_{ip}=P(y_{ip}=1&#124;X(i);\theta),$
    |  | (5) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: where $\Omega_{i}$ is the set of all image $i$ pixels, $P$ is the probability,
    $x_{ip}$ is $p^{th}$ image pixel in $i^{th}$ image and, $y_{ip}\in\{0,1\}$ and
    $\hat{y}_{ip}\in[0,1]$ are respectively the true and the predicted labels of $x_{ip}$.
    The cross-entropy loss appears in the majority of deep skin lesion segmentation
    works, e.g., Song et al. ([2019](#bib.bib359)), Singh et al. ([2019](#bib.bib355)),
    and Zhang et al. ([2019a](#bib.bib437)).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Since the gradient of the cross-entropy loss function is inversely proportional
    to the predicted probabilities, hard-to-predict samples are weighted more in the
    parameter update equations, leading to faster convergence. A variant, the weighted
    cross-entropy loss, penalizes pixels and class labels differently. Nasr-Esfahani
    et al. ([2019](#bib.bib285)) used pixel weights inversely proportional to their
    distance to lesion boundaries to enforce sharper boundaries. Class weighting may
    also mitigate the class imbalance, which, left uncorrected, tends to bias models
    towards the background class, since lesions tend to occupy a relatively small
    portion of images. Chen et al. ([2018b](#bib.bib90)), Goyal et al. ([2019a](#bib.bib153)),
    and Wang et al. ([2019b](#bib.bib404)) apply such a correction, using class weights
    inversely proportional to the class pixel frequency. Mirikharaji et al. ([2019](#bib.bib281))
    weighted the pixels according to annotation noise estimated using a set of cleanly
    annotated data. All the aforementioned losses treat pixels independently without
    enforcing spatial coherence, which motivates their combination with other consistency-seeking
    losses.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Dice and Jaccard Loss
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Dice score and the Jaccard index are two popular metrics for segmentation
    evaluation (Section [4.3](#S4.SS3 "4.3 Evaluation Metrics ‣ 4 Evaluation ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), measuring the overlap between
    predicted segmentation and ground-truth. Models may employ differentiable approximations
    of these metrics, known as soft Dice (He et al., [2017](#bib.bib175); Kaul et al.,
    [2019](#bib.bib215); He et al., [2018](#bib.bib176); Wang et al., [2019a](#bib.bib397))
    and soft Jaccard (Venkatesh et al., [2018](#bib.bib393); Hasan et al., [2020](#bib.bib172);
    Sarker et al., [2019](#bib.bib341)) to optimize an objective directly related
    to the evaluation metric.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'For two classes, these losses are defined as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{dice}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{2\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}+\hat{y}_{ip}},$
    |  | (6) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathcal{L}_{jacc}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}+\hat{y}_{ip}-y_{ip}\hat{y}_{ip}}.$
    |  | (7) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: 'Different variations of overlap-based loss functions address the class imbalance
    problem in medical image segmentation tasks. The Tanimoto distance loss, $\mathcal{L}_{td}$
    is a modified Jaccard loss optimized in some models (Canalini et al., [2019](#bib.bib70);
    Baghersalimi et al., [2019](#bib.bib35); Yuan et al., [2017](#bib.bib433)):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{td}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}^{2}+\hat{y}_{ip}^{2}-y_{ip}\hat{y}_{ip}},$
    |  | (8) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: which is equivalent to the Jaccard loss when both ${y}_{ip}$ and $\hat{y}_{ip}$
    are binary.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'The Tversky loss (Abraham and Khan, [2019](#bib.bib9)), inspired by the Tversky
    index, is another Jaccard variant that penalizes false positives and false negatives
    differently to address the class imbalance problem:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{tv}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}}{\sum_{p\in\Omega}y_{ip}\hat{y}_{ip}+\alpha
    y_{ip}(1-\hat{y}_{ip})+\beta(1-y_{ip})\hat{y}_{ip}},$ |  | (9) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ and $\beta$ tune the contributions of false negatives and false
    positives with $\alpha+\beta=1$.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Abraham and Khan ([2019](#bib.bib9)) combined the Tvserky and focal losses (Lin
    et al., [2017](#bib.bib255)), the latter encouraging the algorithm to focus on
    the hard-to-predict pixels:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{ftv}=\mathcal{L}_{tv}^{\frac{1}{\gamma}},$ |  | (10) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: where $\gamma$ controls the relative importance of the hard-to-predict samples.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Matthews Correlation Coefficient Loss
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Matthews correlation coefficient (MCC) loss is a metric-based loss function
    based on the correlation between predicted and ground-truth labels (Abhishek and
    Hamarneh, [2021](#bib.bib6)). In contrast to the overlap-based losses discussed
    in Section [3.2.3](#S3.SS2.SSS3 "3.2.3 Dice and Jaccard Loss ‣ 3.2 Loss Functions
    ‣ 3 Model Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation"),
    MCC considers misclassifying the background pixels by penalizing false negative
    labels, making it more effective in the presence of skewed class distributions.
    MCC loss is defined as:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{MCC}(X,Y;\theta)=1-\frac{1}{N}\sum_{i=1}^{N}\frac{\sum_{p\in\Omega}\hat{y}_{ip}y_{ip}\frac{\sum_{p\in\Omega}\hat{y}_{ip}\sum_{p\in\Omega}y_{ip}}{M_{i}}}{f(\hat{y}_{i}y_{i})},$
    |  | (11) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '|  | $f(\hat{y}_{i},y_{i})=\sqrt{\sum_{p\in\Omega}\hat{y}_{ip}\sum_{p\in\Omega}{y}_{ip}-\frac{\sum_{p\in\Omega}\hat{y}_{ip}(\sum_{p\in\Omega}{y}_{ip})^{2}}{M_{i}}-\frac{(\sum_{p\in\Omega}\hat{y}_{ip})^{2}\sum_{p\in\Omega}{y}_{ip}}{M_{i}}+(\frac{\sum_{p\in\Omega}\hat{y}_{ip}\sum_{p\in\Omega}{y}_{ip}}{M_{i}})^{2}}~{},$
    |  | (12) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: where $M_{i}$ is the total number of pixels in the image $i$.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5 Deep Supervision Loss
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In DL models, the loss may apply not only to the final decision layer, but
    also to the intermediate hidden layers. The supervision of hidden layers, known
    as deep supervision, guides the learning of intermediate features. Deep supervision
    also addresses the vanishing gradient problem, leading to faster convergence and
    improves segmentation performance by constraining the feature space. Deep supervision
    loss appears in several skin lesion segmentation works (He et al., [2017](#bib.bib175);
    Zeng and Zheng, [2018](#bib.bib436); Li et al., [2018a](#bib.bib243), [b](#bib.bib248);
    He et al., [2018](#bib.bib176); Zhang et al., [2019a](#bib.bib437); Tang et al.,
    [2019b](#bib.bib374)), where it is computed in multiple layers, at different scales.
    The loss has the general form of a weighted summation of multi-scale segmentation
    losses:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{ds}(X,Y;\theta)=\sum_{l=1}^{m}\gamma_{l}\mathcal{L}_{l}(X,Y;\theta),$
    |  | (13) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: where $m$ is the number of scales, $\mathcal{L}_{l}$ is the loss at the $l^{th}$
    scale, and $\gamma_{l}$ adjusts the contribution of different losses.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.6 Star-Shape Loss
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In contrast to pixel-wise losses which act on pixels independently and cannot
    enforce spatial constraints, the star-shape loss (Mirikharaji and Hamarneh, [2018](#bib.bib279))
    aims to capture class label dependencies and preserve the target object structure
    in the predicted segmentation masks. Based upon prior knowledge about the shape
    of skin lesions, the star-shape loss, $\mathcal{L}_{ssh}$ penalizes discontinuous
    decisions in the estimated output as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{ssh}(X,Y;\theta)=\sum_{i=1}^{N}\sum_{p\in\Omega}\sum_{q\in\ell_{pc}}\mathbbm{1}_{y_{ip}=y_{iq}}\times&#124;y_{ip}-\hat{y}_{ip}&#124;\times&#124;\hat{y}_{ip}-\hat{y}_{iq}&#124;,$
    |  | (14) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: where $c$ is the lesion center, $\ell_{pc}$ is the line segment connecting pixels
    $p$ and $c$ and, $q$ is any pixel lying on $\ell_{pc}$. This loss encourages all
    pixels lying between $p$ and $q$ on $\ell_{pc}$ to be assigned the same estimator
    whenever $p$ and $q$ have the same ground-truth label. The result is a radial
    spatial coherence from the lesion center.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.7 End-Point Error Loss
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many authors consider the lesion boundary the most challenging region to segment.
    The end-point error loss (Sarker et al., [2018](#bib.bib342); Singh et al., [2019](#bib.bib355))
    underscores borders by using the first derivative of the segmentation masks instead
    of their raw values:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{epe}(X,Y;\theta)=\sum_{i=1}^{N}\sum_{p\in\Omega}\sqrt{(\hat{y}^{0}_{ip}-y^{0}_{ip})^{2}+(\hat{y}^{1}_{ip}-y^{1}_{ip})^{2}},$
    |  | (15) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: where $\hat{y}^{0}_{ip}$ and $\hat{y}^{1}_{ip}$ are the directional first derivatives
    of the estimated segmentation map in the $x$ and $y$ spatial directions, respectively
    and, similarly, $y^{0}_{ip}$ and $y^{1}_{ip}$ for the ground-truth derivatives.
    Thus, this loss function encourages the magnitude and orientation of edges of
    estimation and ground-truth to match, thereby mitigating vague boundaries in skin
    lesion segmentation.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.8 Adversarial Loss
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another way to add high-order class-label consistency is adversarial training.
    Adversarial training may be employed along with traditional supervised training
    to distinguish estimated segmentation from ground-truths using a discriminator.
    The optimization objective will weight a pixel-wise loss $\mathcal{L}_{s}$ matching
    prediction to ground-truth, and an adversarial loss, as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{adv}(X,Y;\theta,\theta_{a})=\mathcal{L}_{s}(X,Y;\theta)-\lambda[\mathcal{L}_{ce}(Y,1;\theta_{a})+\mathcal{L}_{ce}(\hat{Y},0;\theta,\theta_{a})],$
    |  | (16) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: where $\theta_{a}$ are the adversarial model parameters. The adversarial loss
    employs a binary cross-entropy loss to encourage the segmentation model to produce
    indistinguishable prediction maps from ground-truth maps. The adversarial objective
    (Eqn. ([16](#S3.E16 "In 3.2.8 Adversarial Loss ‣ 3.2 Loss Functions ‣ 3 Model
    Design and Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation")))
    is optimized in a mini-max game by simultaneously minimizing it with respect to
    $\theta$ and maximizing it with respect to $\theta_{a}$.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Pixel-wise losses, such as cross-entropy (Izadi et al., [2018](#bib.bib190);
    Singh et al., [2019](#bib.bib355); Jiang et al., [2019](#bib.bib203)), soft Jaccard (Sarker
    et al., [2019](#bib.bib341); Tu et al., [2019](#bib.bib383); Wei et al., [2019](#bib.bib412)),
    end-point error (Tu et al., [2019](#bib.bib383); Singh et al., [2019](#bib.bib355)),
    MSE (Peng et al., [2019](#bib.bib304)) and MAE  (Sarker et al., [2019](#bib.bib341);
    Singh et al., [2019](#bib.bib355); Jiang et al., [2019](#bib.bib203)) losses have
    all been incorporated in adversarial learning of skin lesion segmentation. In
    addition, Xue et al. ([2018](#bib.bib423)) and Tu et al. ([2019](#bib.bib383))
    presented a multi-scale adversarial term to match a hierarchy of local and global
    contextual features in the predicted maps and ground-truths. In particular, they
    minimize the MAE of multi-scale features extracted from different layers of the
    adversarial model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.9 Rank Loss
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Assuming that hard-to-predict pixels lead to larger prediction errors while
    training the model, rank loss (Xie et al., [2020b](#bib.bib420)) is proposed to
    encourage learning more discriminative information for harder pixels. The image
    pixels are ranked based on their prediction errors, and the top $K$ pixels with
    the largest prediction errors from the lesion or background areas are selected.
    Let $\hat{y}_{ij}^{0}$ and $\hat{y}_{il}^{1}$ are respectively the selected $j^{th}$
    hard-to-predict pixel of background and $l^{th}$ hard-to-predict pixel of lesion
    in the image $i$, we have:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{rank}(X,Y;\theta)=\sum_{i=1}^{N}\sum_{j=1}^{K}\sum_{l=1}^{K}\max\{0,\hat{y}_{ij}^{0}-\hat{y}_{il}^{1}+margin\},$
    |  | (17) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: which encourages $\hat{y}_{il}^{1}$ to be greater than $\hat{y}_{ij}^{0}$ plus
    margin.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Similar to rank loss, narrowband suppression loss (Deng et al., [2020](#bib.bib115))
    also adds a constraint between hard-to-predict pixels of background and lesion.
    Different from rank loss, narrowband suppression loss collects pixels in a narrowband
    along the ground-truth lesion boundary with radius $r$ instead of all image pixels
    and then selects the top $K$ pixels with the largest prediction errors.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluation is one of the main challenges for any image segmentation task, skin
    lesions included (Celebi et al., [2015b](#bib.bib82)). Segmentation evaluation
    may be subjective or objective (Zhang et al., [2008](#bib.bib438)), the former
    involving the visual assessment of the results by a panel of human experts, and
    the latter involving the comparison of the results with ground-truth segmentations
    using quantitative evaluation metrics.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Subjective evaluation may provide a nuanced assessment of results, but because
    experts must grade each batch of results, it is usually too laborious to be applied,
    except in limited settings. In objective assessment, experts are consulted once,
    to provide the ground-truth segmentations, and that knowledge can then be reused
    indefinitely. However, due to intra- and inter-annotator variations, it raises
    the question of whether any individual ground-truth segmentation reflects the
    ideal “true” segmentation, an issue we address in Section [4.2](#S4.SS2 "4.2 Inter-Annotator
    Agreement ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation").
    It also raises the issue of choosing one or more evaluation metrics (Section [4.3](#S4.SS3
    "4.3 Evaluation Metrics ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Segmentation Annotation
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Obtaining ground-truth segmentations is paramount for the objective evaluation
    of segmentation algorithms. For synthetically generated images (Section [2.2](#S2.SS2
    "2.2 Synthetic Data Generation ‣ 2 Input Data ‣ A Survey on Deep Learning for
    Skin Lesion Segmentation")), ground-truth segmentations may be known by construction,
    either by applying parallel transformations to the original ground-truth masks
    in the case of traditional data augmentation, or by training generative models
    to synthesize images paired with their segmentation masks.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: For images obtained from real patients, however, human experts have to provide
    the ground-truth segmentations. Various workflows have been proposed to reconcile
    the conflicting goals of ease of learning, speed, accuracy, and flexibility of
    annotation. On one end of the spectrum, the expert traces the lesion by hand,
    on images of the skin lesion printed on photographic paper, which are then scanned (Bogo
    et al., [2015](#bib.bib62)). The technique is easy to learn and fast, but the
    printing and scanning procedure limits the accuracy, and the physical nature of
    the annotations makes corrections burdensome. On the other end of the spectrum,
    the annotation is performed on the computer, by a semi-automated procedure (Codella
    et al., [2019](#bib.bib96)), with an initial border generated by a segmentation
    algorithm, which is then refined by the expert using an annotation software, by
    adjusting the parameters of the segmentation algorithm manually. This method is
    fast and easy to correct, but there might be a learning curve, and its accuracy
    depends on which algorithm is employed and how much the experts understand it.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: By far, the commonest annotation method in the literature is somewhere in the
    middle, with fully manual annotations performed on a computer. The skin lesion
    image file may be opened either in a raster graphics editor (e.g., GNU Image Manipulation
    Program (GIMP) or Adobe Photoshop), or in a dedicated annotation software (Ferreira
    et al., [2012](#bib.bib135)), where the expert traces the borders of the lesion
    using a mouse or stylus, with continuous freehand drawing, or with discrete control
    points connecting line segments (resulting in a polygon (Codella et al., [2019](#bib.bib96)))
    or smooth curve segments (e.g., cubic B-splines (Celebi et al., [2007a](#bib.bib73))).
    This method provides a good compromise, being easy to implement, fast, and accurate
    to perform, after an acceptable learning period for the annotator.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Inter-Annotator Agreement
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Formally, dataset ground-truths can be viewed as samples of an estimator of
    the true label, which can never be directly observed (Smyth et al., [1995](#bib.bib357)).
    This problem is often immaterial for classification, when annotation noise is
    small. However, in medical image segmentation, ground-truths suffer from both
    biases (systematic deviations from the “ideal”) and significant noise (Zijdenbos
    et al., [1994](#bib.bib457); Chalana and Kim, [1997](#bib.bib84); Guillod et al.,
    [2002](#bib.bib163); Grau et al., [2004](#bib.bib156); Bogo et al., [2015](#bib.bib62);
    Lampert et al., [2016](#bib.bib236)), the latter appearing as inter-annotator
    (different experts) and intra-annotator (same expert at different times) variability.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: In the largest study of its kind to date, Fortina et al. ([2012](#bib.bib137))
    measured the inter-annotator variability among $12$ dermatologists with varying
    levels of experience on a set of $77$ dermoscopic images, showing that the average
    pairwise XOR dissimilarity (Section [4.3](#S4.SS3 "4.3 Evaluation Metrics ‣ 4
    Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation")) between
    annotators was $\approx 15\%$, and that in $10\%$ of cases, this value was $>28\%$.
    They found more agreement among more experienced dermatologists than less experienced
    ones. Also, more experienced dermatologists tend to outline tighter borders than
    less experienced ones. They suggest that the level of agreement among experienced
    dermatologists could serve as an upper bound for the accuracy achievable by a
    segmentation algorithm, i.e., if even highly experienced dermatologists disagree
    on how to classify $10\%$ of an image, it might be unreasonable to expect a segmentation
    algorithm to agree with more than $90\%$ of any given ground-truth on the same
    image (Fortina et al., [2012](#bib.bib137)).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Due to the aforementioned variability issues, whenever possible, skin lesion
    segmentation should be evaluated against multiple expert ground-truths, a good
    algorithm being one that agrees with the ground-truths at least as well as the
    expert agree among themselves (Chalana and Kim, [1997](#bib.bib84)). Due to the
    cost of annotation, however, algorithms are often evaluated against a single ground-truth.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'When multiple ground-truths are available, the critical issue is how to employ
    them. Several approaches have been proposed:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preferring one of the annotations (e.g., the one by the most experienced expert)
    and ignoring the others (Celebi et al., [2007a](#bib.bib73)).
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring and reporting the results for each annotator separately (Celebi et al.,
    [2008](#bib.bib77)), which might require non-trivial multivariate analyses if
    the aim is to rank the algorithms.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring each automated segmentation against all corresponding ground-truths
    and reporting the average result (Schaefer et al., [2011](#bib.bib344)).
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring each automated segmentation against an *ensemble ground-truth* formed
    by combining the corresponding ground-truths pixel-wise using a bitwise OR (Garnavi
    et al., [2011a](#bib.bib143); Garnavi and Aldeen, [2011](#bib.bib142)), bitwise
    AND (Garnavi et al., [2011b](#bib.bib144)), or a majority voting (Iyatomi et al.,
    [2006](#bib.bib189), [2008](#bib.bib188); Norton et al., [2012](#bib.bib289)).
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The ground-truth ensembling process can be generalized using a *thresholded
    probability map* (Biancardi et al., [2010](#bib.bib54)). First, all ground-truths
    for a sample are averaged pixel-wise into a *probability map*. Then the map is
    binarized, with the lesion corresponding to pixels greater than or equal to a
    chosen threshold. The operations of OR, AND, and majority voting, correspond,
    respectively to thresholds of $1/n$, $1$, and $(n-\varepsilon)/2n$, with $n$ being
    the number of ground-truths, and $\varepsilon$ being a small positive constant.
    AND and OR correspond, respectively, to the tightest and loosest possible contours,
    with other thresholds leading to intermediate results. While the optimal threshold
    value is data-dependent, large thresholds focus the evaluation on unambiguous
    regions, leading to overly optimistic evaluations of segmentation quality (Smyth
    et al., [1995](#bib.bib357); Lampert et al., [2016](#bib.bib236)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: The abovementioned approaches fail to consider the differences of experience
    or performance of the annotators (Warfield and Wells, [2004](#bib.bib411)). More
    elaborate ground-truth fusion alternatives include shape averaging (Rohlfing and
    Maurer, [2006](#bib.bib327)), border averaging (Chen and Parent, [1989](#bib.bib91);
    Chalana and Kim, [1997](#bib.bib84)), binary label fusion algorithms such as STAPLE
    (Warfield and Wells, [2004](#bib.bib411)), TESD (Biancardi et al., [2010](#bib.bib54)),
    and SIMPLE (Langerak et al., [2010](#bib.bib237)), as well as other more recent
    algorithms (Peng and Li, [2013](#bib.bib300); Peng et al., [2016](#bib.bib301),
    [2017a](#bib.bib302)).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: STAPLE (Simultaneous Truth And Performance Level Estimation) has been very influential
    in medical image segmentation evaluation, inspiring many variants. For each image
    and its ground-truth segmentations, STAPLE estimates a probabilistic true segmentation
    through the optimal combination of individual ground-truths, weighting each one
    by the estimated sensitivity and specificity of its annotator. STAPLE may fail
    when there are only a few annotators or when their performances vary too much (Langerak
    et al., [2010](#bib.bib237); Lampert et al., [2016](#bib.bib236)), a situation
    addressed by SIMPLE (Selective and Iterative Method for Performance Level Estimation) (Langerak
    et al., [2010](#bib.bib237)) by iteratively discarding poor quality ground-truths.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of attempting to fuse multiple ground-truths into a single one before
    employing conventional evaluation metrics, the metrics themselves may be modified
    to take into account annotation variability. Celebi et al. ([2009c](#bib.bib80))
    proposed the *normalized probabilistic rand index* (NPRI) (Unnikrishnan et al.,
    [2007](#bib.bib384)), a generalization of the *rand index* (Rand, [1971](#bib.bib319)).
    It penalizes segmentation results more (less) in regions where the ground-truths
    agree (disagree). Fig. [9](#S4.F9 "Figure 9 ‣ 4.2 Inter-Annotator Agreement ‣
    4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation") illustrates
    the idea: ground-truths outlined by three experienced dermatologists appear in
    red, green, and blue, while the automated result appears in black. NPRI does *not*
    penalize the automated segmentation in the upper part of the image, where the
    blue border seriously disagrees with the other two (Celebi et al., [2009c](#bib.bib80)).
    Despite its many desirable qualities, NPRI has a subtle flaw: it is non-monotonic
    with the fraction of misclassified pixels (Peserico and Silletti, [2010](#bib.bib306)).
    Consequently, this index might be unsuitable for comparing poor segmentation algorithms.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b60caf0a36afa3c44e323ca3ae1f5429.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Sample segmentation results demonstrating inter-annotator disagreements.
    Note how annotator preferences can affect the manual segmentations, e.g., smooth
    lesion borders (green), jagged lesion borders (black), oversegmented lesion (blue),
    etc. Figure taken from Celebi et al. ([2009c](#bib.bib80)) with permission.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Evaluation Metrics
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can frame the skin lesion segmentation problem as a binary pixel-wise classification
    task, where the positive and negative classes correspond to the lesion and the
    background skin, respectively. Suppose that we have an input image and its corresponding
    segmentations: an *automated segmentation* (AS) produced by a segmentation algorithm
    and a *manual segmentation* (MS) outlined by a human expert. We can formulate
    a number of quantitative segmentation evaluation measures based on the concepts
    of *true positive*, *false negative*, *false positive*, and *true negative*, whose
    definitions are given in Table [2](#S4.T2 "Table 2 ‣ 4.3 Evaluation Metrics ‣
    4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion Segmentation"). In this
    table, actual and detected pixels refer to any given pixel in the MS and the corresponding
    pixel in the AS, respectively.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Definitions of true positive, false negative, false positive, and
    true negative pixels in the context of skin lesion segmentation.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Detected Pixel |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '|  |  | Lesion $(+)$ | Background $(-)$ |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| Actual | Lesion $(+)$ | True Positive | False Negative |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| Pixel | Background $(-)$ | False Positive | True Negative |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: For a given pair of automated and manual segmentations, we can construct a $2\times
    2$ confusion matrix (aka a contingency table (Pearson, [1904](#bib.bib299); Miller
    and Nicely, [1955](#bib.bib277))) <math alttext="\mathsf{C}=\begin{pmatrix}\textsf{TP}&amp;\textsf{FN}\\
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '\textsf{FP}&amp;\textsf{TN}\end{pmatrix}" display="inline"><semantics ><mrow
    ><mi >𝖢</mi><mo  >=</mo><mrow ><mo >(</mo><mtable columnspacing="5pt" rowspacing="0pt"
    ><mtr ><mtd  ><mtext mathsize="70%" >TP</mtext></mtd><mtd ><mtext  mathsize="70%"
    >FN</mtext></mtd></mtr><mtr ><mtd ><mtext  mathsize="70%" >FP</mtext></mtd><mtd
    ><mtext mathsize="70%"  >TN</mtext></mtd></mtr></mtable><mo >)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><ci  >𝖢</ci><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><ci  ><mtext mathsize="70%" >TP</mtext></ci><ci ><mtext mathsize="70%"  >FN</mtext></ci></matrixrow><matrixrow
    ><ci ><mtext mathsize="70%"  >FP</mtext></ci><ci ><mtext mathsize="70%" >TN</mtext></ci></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathsf{C}=\begin{pmatrix}\textsf{TP}&\textsf{FN}\\
    \textsf{FP}&\textsf{TN}\end{pmatrix}</annotation></semantics></math>, where TP,
    FN, FP, and TN denote the numbers of true positives, false negatives, false positives,
    and true negatives, respectively. Clearly, we have $N=\textsf{TP}+\textsf{FN}+\textsf{FP}+\textsf{TN}$,
    where $N$ is the number of pixels in either image. Based on these quantities,
    we can define a variety of scalar similarity measures to quantify the accuracy
    of segmentation (Baldi et al., [2000](#bib.bib36); Japkowicz and Shah, [2011](#bib.bib197);
    Taha and Hanbury, [2015](#bib.bib366)):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sensitivity (SE) and Specificity (SP) (Kahn, [1942](#bib.bib207); Yerushalmy,
    [1947](#bib.bib427); Binney et al., [2021](#bib.bib56)): SE = $\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FN}}$
    & SP = $\dfrac{\textsf{TN}}{\textsf{TN}+\textsf{FP}}$'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision (PR) and Recall (RE) (Kent et al., [1955](#bib.bib223)): PR = $\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FP}}$
    & RE = $\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FN}}$'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\text{Accuracy ({AC})}=\dfrac{\textsf{TP}+\textsf{TN}}{\textsf{TP}+\textsf{FN}+\textsf{FP}+\textsf{TN}}$
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F-measure (F) (van Rijsbergen, [1979](#bib.bib389)) = $\dfrac{2|\textsf{AS}\cap\textsf{MS}|}{|\textsf{AS}|+|\textsf{MS}|}=\dfrac{2\cdot\textsf{PR}\cdot\textsf{RE}}{\textsf{PR}+\textsf{RE}}=\dfrac{2\textsf{TP}}{2\textsf{TP}+\textsf{FP}+\textsf{FN}}$
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: G-mean (GM) (Kubat et al., [1998](#bib.bib234)) = $\sqrt{\textsf{SE}\cdot\textsf{SP}}$
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balanced Accuracy (BA)  (Chou and Fasman, [1978](#bib.bib94)) = $\dfrac{\textsf{SE}+\textsf{SP}}{2}$
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaccard index (J) (Jaccard, [1901](#bib.bib191)) = $\dfrac{|\textsf{AS}\cap\textsf{MS}|}{|\textsf{AS}\cup\textsf{MS}|}=\dfrac{\textsf{TP}}{\textsf{TP}+\textsf{FN}+\textsf{FP}}$
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matthews Correlation Coefficient (MCC) (Matthews, [1975](#bib.bib273)) = $\dfrac{\textsf{TP}\cdot\textsf{TN}-\textsf{FP}\cdot\textsf{FN}}{\sqrt{(\textsf{TP}+\textsf{FP})(\textsf{TP}+\textsf{FN})(\textsf{TN}+\textsf{FP})(\textsf{TN}+\textsf{FN})}}$
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For each similarity measure, the higher the value, the better the segmentation.
    Except for MCC, all of these measures have a unit range, that is, $[0,1]$. The
    $[-1,1]$ range of MCC can be mapped to $[0,1]$ by adding one to it and then dividing
    by two. Each of these unit-range similarity measures can then be converted to
    a unit-range dissimilarity measure by subtracting it from one. Note that there
    are also dissimilarity measures with no corresponding similarity formulation.
    A prime example is the well-known XOR measure (Hance et al., [1996](#bib.bib170))
    defined as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{{XOR}}=\dfrac{&#124;\textsf{AS}\oplus\textsf{MS}&#124;}{&#124;\textsf{MS}&#124;}=\dfrac{&#124;\left(\textsf{AS}\cup\textsf{MS}\right)-\left(\textsf{AS}\cap\textsf{MS}\right)&#124;}{&#124;\textsf{MS}&#124;}=\dfrac{\textsf{FP}+\textsf{FN}}{\textsf{TP}+\textsf{FN}}.$
    |  | (18) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: It is essential to notice that different evaluation measures capture different
    aspects of a segmentation algorithm’s performance on a given dataset, and thus
    there is no universally applicable evaluation measure (Japkowicz and Shah, [2011](#bib.bib197)).
    This is why most studies employ multiple evaluation measures in an effort to perform
    a comprehensive performance evaluation. Such a strategy, however, complicates
    algorithm comparisons, unless one algorithm completely dominates the others with
    respect to all adopted evaluation measures.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Based on their observation that experts tend to avoid missing parts of the lesion
    in their manual borders, Garnavi et al. ([2011a](#bib.bib143)) argue that true
    positives have the highest importance in the segmentation of skin lesion images.
    The authors also assert that false positives (background pixels incorrectly identified
    as part of the lesion) are less important than false negatives (lesion pixels
    incorrectly identified as part of the background). Accordingly, they assign a
    weight of $1.5$ to TP to signify its overall importance. Furthermore, in measures
    that involve both FN and FP (e.g., AC, F, and XOR), they assign a weight of 0.5
    to FP to emphasize its importance over FN. Using these weights, they construct
    a *weighted performance index*, which is an arithmetic average of six commonly
    used measures, namely SE, SP, PR, AC, F, and (unit complement of) XOR. This scalar
    evaluation measure facilitates comparisons among algorithms.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: In a follow-up study, Garnavi and Aldeen ([2011](#bib.bib142)) parameterize
    the weights of TP, FN, FP, and TN in their weighted performance index and then
    use a constrained non-linear program to determine the optimal weights. They conduct
    experiments with five segmentation algorithms on $55$ dermoscopic images. They
    conclude that the optimized weights not only lead to automated algorithms that
    are more accurate against manual segmentations, but also diminish the differences
    among those algorithms.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'We make the following key observations about the popular evaluation metrics
    and how they have been used in the skin lesion segmentation literature:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Historically, AC has been the most popular evaluation measure owing to its simple
    and intuitive formulation. However, this measure tends to favor the majority class,
    leading to overly optimistic performance estimates in class-imbalanced domains.
    This drawback prompted the development of more elaborate performance evaluation
    measures, including GM, BA, and MCC.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SE and SP are especially popular in medical domains, tracing their usage in
    serologic test reports in the early 1900s (Binney et al., [2021](#bib.bib56)).
    SE (aka *True Positive Rate*) quantifies the accuracy on the positive class, whereas
    SP (aka *True Negative Rate*) quantifies the accuracy on the negative class. These
    measures are generally used together because it is otherwise trivial to maximize
    one at the expense of the other (an automated border enclosing the corresponding
    manual border will attain a perfect SE, whereas in the opposite case, we will
    have a perfect SP). Unlike AC, they are suitable for class-imbalanced domains.
    BA and GM combine these measures into a single evaluation measure through arithmetic
    and geometric averaging, respectively. Unlike AC, these composite measures are
    suitable for class-imbalanced domains (Luque et al., [2020](#bib.bib266)).
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PR is the proportion of examples assigned to the positive class that actually
    belongs to the positive class. RE is equivalent to SE. PR and RE are typically
    used in information retrieval applications, where the focus is solely on relevant
    documents (positive class). F combines these measures into a single evaluation
    measure through harmonic averaging. This composite measure, however, is unsuitable
    for class-imbalanced domains (Zou et al., [2004](#bib.bib459); Chicco and Jurman,
    [2020](#bib.bib92); Luque et al., [2020](#bib.bib266)).
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MCC is equivalent to the *phi coefficient*, which is simply the *Pearson correlation
    coefficient* applied to binary data (Chicco and Jurman, [2020](#bib.bib92)). MCC
    values fall within the range of $[-1,1]$ with $-1$ and $1$ indicating perfect
    misclassification and perfect classification, respectively, while $0$ indicating
    a classification no better than random (Matthews, [1975](#bib.bib273)). Although
    it is biased to a certain extent (Luque et al., [2020](#bib.bib266); Zhu, [2020](#bib.bib456)),
    this measure appears to be suitable for class-imbalanced domains (Boughorbel et al.,
    [2017](#bib.bib64); Chicco and Jurman, [2020](#bib.bib92); Luque et al., [2020](#bib.bib266)).
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'J (aka *Intersection over Union* (Jaccard, [1912](#bib.bib192))) and F (aka
    *Dice coefficient* aka *Sørensen-Dice coefficient* (Dice, [1945](#bib.bib123);
    Sørensen, [1948](#bib.bib360))) are highly popular in medical image segmentation
    (Crum et al., [2006](#bib.bib104)). These measures are monotonically related as
    follows: $J=F/(2-F)$ and $F=2J/(1+J)$. Thus, it makes little sense to use them
    together. There are two major differences between these measures: (i) $(1-J)$is
    a proper distance metric, whereas $(1-F)$ is *not* (it violates the triangle inequality).
    (ii) It can be shown (Zijdenbos et al., [1994](#bib.bib457)) that if TN is sufficiently
    large compared to TP, FN, and FP, which is common in skin lesion segmentation,
    $F$ becomes equivalent to *Cohen’s kappa* (Cohen, [1960](#bib.bib99)), which is
    a chance-corrected measure of inter-observer agreement.'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among the seven composite evaluation measures given above, AC, GM, BA, and MCC
    are symmetric, that is, they are invariant to class swapping, while F, J, and
    XOR are asymmetric.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XOR is similar to *False Negative Rate*, that is, the unit complement of SE,
    with the exception that XOR has an extra additive TN term in its numerator. While
    XOR values are guaranteed to be nonnegative, they do *not* have a fixed upper
    bound, which makes aggregations of this measure difficult. XOR is also biased
    against small lesions (Celebi et al., [2009c](#bib.bib80)). Nevertheless, owing
    to its intuitive formulation, XOR was popular in skin lesion segmentation until
    about 2015 (Celebi et al., [2015b](#bib.bib82)).
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The 2016 and 2017 ISIC Challenges (Gutman et al., [2016](#bib.bib167); Codella
    et al., [2018](#bib.bib98)) adopted five measures: AC, SE, SP, F, and J, with
    the participants ranked based on the last measure. The 2018 ISIC Challenge (Codella
    et al., [2019](#bib.bib96)) featured a *thresholded Jaccard index*, which returns
    the same value as the original J if the value is greater than or equal to a predefined
    threshold and zero otherwise. Essentially, this modified index considers automated
    segmentations yielding J values below the threshold as complete failures. The
    challenge organizers set the threshold equal to $0.65$ based on an earlier study
    (Codella et al., [2017](#bib.bib97)) that determined the average pairwise J similarities
    among the manual segmentations outlined by three expert dermatologists. Since
    the majority of papers in this survey ($168$ out of $177$ papers) use the ISIC
    datasets (Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on
    Deep Learning for Skin Lesion Segmentation")), we list the J for all the papers
    in Table LABEL:tab:main wherever it has been reported in the corresponding papers.
    For papers that did not report J and instead reported F, we list the computed
    J based on F and denote it with an asterisk.'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the aforementioned measures (i.e., GM and BA) have *not* been used in
    a skin lesion segmentation study yet.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evaluation measures discussed above are all region-based and thus fairly
    insensitive to border irregularities (Lee et al., [2003](#bib.bib240)), i.e.,
    indentations, and protrusions along the border. Boundary-based evaluation measures (Taha
    and Hanbury, [2015](#bib.bib366)) have *not* been used in the skin lesion segmentation
    literature much except for the symmetric Hausdorff metric (Silveira et al., [2009](#bib.bib352)),
    which is known to be sensitive to noise (Huttenlocher et al., [1993](#bib.bib184))
    and biased in favor of small lesions (Bogo et al., [2015](#bib.bib62)).
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 Discussion and Future Research
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we presented an overview of DL-based skin lesion segmentation
    algorithms. A lot of work has been done in this field since the first application
    of CNNs on these images in 2015 (Codella et al., [2015](#bib.bib95)). In fact,
    the number of skin lesion segmentation papers published over the past $8$ years
    (2015–2022) is more than thrice those published over the previous $17$ years (1998–2014) (Celebi
    et al., [2015b](#bib.bib82)).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'However, despite the large body of work, skin lesion segmentation remains an
    open problem, as evidenced by the ISIC 2018 Skin Lesion Segmentation Live Leaderboard (ISIC,
    [2018](#bib.bib185)). The live leaderboard has been open and accepting submissions
    since 2018, and even after the permitted usage of external data, the best thresholded
    Jaccard index (the metric used to rank submissions) is $83.6\%$. Additionally,
    the release of the HAM10000 lesion segmentations (Tschandl et al., [2020](#bib.bib380);
    ViDIR Dataverse, [2020](#bib.bib396)) in 2020 shows that progressively larger
    skin lesion segmentation datasets continue to be released. We believe that the
    following aspects of skin lesion segmentation via deep learning are worthy of
    future work:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mobile dermoscopic image analysis: With the availability of various inexpensive
    dermoscopes designed for smartphones, mobile dermoscopic image analysis is of
    great interest worldwide, especially in regions where access to dermatologists
    is limited. Typical DL-based image segmentation algorithms have millions of weights.
    In addition, classical CNN architectures are known to exhibit difficulty dealing
    with certain image distortions such as noise and blur (Dodge and Karam, [2016](#bib.bib125)),
    and DL-based skin lesion diagnosis models have been demonstrated to be susceptible
    to similar artifacts: various kinds of noise and blur, brightness and contrast
    changes, dark corners (Maron et al., [2021b](#bib.bib272)), bubbles, rulers, ink
    markings, etc. (Katsch et al., [2022](#bib.bib213)). Therefore, the current dermoscopic
    image segmentation algorithms may not be ideal for execution on typically resource-constrained
    mobile and edge devices, needed for patient privacy so that uploading skin images
    to remote servers is avoided. Leaner DL architectures, e.g., MobileNet (Howard
    et al., [2019](#bib.bib179)), ShuffleNet (Zhang et al., [2018](#bib.bib443)),
    EfficientNet (Tan and Le, [2019](#bib.bib369)), MnasNet (Tan et al., [2019a](#bib.bib368)),
    and UNeXt (Valanarasu and Patel, [2022](#bib.bib387)), should be investigated
    in addition to the robustness of such architectures with respect to image noise
    and blur.'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Datasets: To train more accurate and robust deep neural segmentation architectures,
    we need larger, more diverse, and more representative skin lesion datasets with
    multiple manual segmentations per image. Additionally, as mentioned in Section [2.1](#S2.SS1
    "2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation"),
    several skin lesion image classification datasets do not have the corresponding
    lesion mask annotations, and given their popularity in skin image analysis tasks,
    they may be good targets for manual delineations. For example, the PAD-UFES-20
    dataset (Pacheco et al., [2020](#bib.bib294)) consists of clinical images of skin
    lesions captured using smartphones, and obtaining ground-truth segmentations on
    this dataset would help advance skin image analysis on mobile devices. Additionally,
    a recent study conducted by Daneshjou et al. ([2021a](#bib.bib110)) found that
    as little as 10% of the AI-based studies for dermatological diagnosis included
    skin tone information for at least one dataset used, and that several studies
    included little to no images of darker skin tones, underlining the need to curate
    datasets with diverse skin tones.'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Collecting segmentation annotations: At the time of this writing, the ISIC
    Archive contains over $71,000$ publicly available images. Considering that the
    largest public dermoscopic image set contained a little over $1,000$ images about
    six years ago, we have come a long way. The more pressing problem now is the lack
    of manual segmentations for most of these images. Since manual segmentation by
    medical experts is laborious and costly, crowdsourcing techniques (Kovashka et al.,
    [2016](#bib.bib232)) could be explored to collect annotations from non-experts.
    Experts could then revise these initial annotations, or methods that tackle the
    problem of annotation noise (Mirikharaji et al., [2019](#bib.bib281); Karimi et al.,
    [2020](#bib.bib210); Li et al., [2021a](#bib.bib245)) could be explored. Note
    that the utility of crowdsourcing in medical image annotation has been demonstrated
    in multiple studies (Foncubierta-Rodriguez and Muller, [2012](#bib.bib136); Gurari
    et al., [2015](#bib.bib166); Sharma et al., [2017](#bib.bib348); Goel et al.,
    [2020](#bib.bib149)). Additionally, keeping in mind the time-consuming nature
    of manual supervised annotation, an alternative is to use weakly-supervised annotation,
    e.g., bounding-box annotations (Dai et al., [2015](#bib.bib108); Papandreou et al.,
    [2015](#bib.bib297)), which are much less time-consuming to collect. For example,
    for several large skin lesion image datasets that do not have any lesion mask
    annotations (see Section [2.1](#S2.SS1 "2.1 Datasets ‣ 2 Input Data ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), bounding-box lesion annotations
    can be obtained more easily than dense pixel-level segmentation annotations. In
    addition, weakly-supervised annotation (Bearman et al., [2016](#bib.bib44); Tajbakhsh
    et al., [2020](#bib.bib367); Roth et al., [2021](#bib.bib331); En and Guo, [2022](#bib.bib132))
    is more amenable to crowdsourcing (Maier-Hein et al., [2014](#bib.bib269); Rajchl
    et al., [2016](#bib.bib313); Papadopoulos et al., [2017](#bib.bib296); Lin et al.,
    [2019](#bib.bib254)), especially for non-experts.'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Handling multiple annotations per image: If the skin lesion image dataset at
    hand contains multiple manual segmentations per image, one should consider either
    using an algorithm such as STAPLE (Warfield and Wells, [2004](#bib.bib411)) for
    fusing the manual segmentations (see Section [4](#S4 "4 Evaluation ‣ A Survey
    on Deep Learning for Skin Lesion Segmentation")), or relying on learning-based
    approaches, either through variants of STAPLE adapted for DL-based segmentation (Kats
    et al., [2019](#bib.bib212); Zhang et al., [2020b](#bib.bib440)), or other methods (Mirikharaji
    et al., [2021](#bib.bib278); Lemay et al., [2022](#bib.bib242)). Such a fusion
    algorithm can also be used to build an ensemble of multiple automated segmentations.'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supervised segmentation evaluation measures: Supervised segmentation evaluation
    measures popular in the skin image analysis literature (see Section [4.3](#S4.SS3
    "4.3 Evaluation Metrics ‣ 4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")) are often region-based, pair-counting measures. Other region-based
    measures, such as information-theoretic measures (e.g., mutual information, variation
    of information, etc.) as well as boundary-based measures e.g., Hausdorff distance
    (Taha and Hanbury, [2015](#bib.bib366)) should be explored as well.'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unsupervised segmentation and unsupervised segmentation evaluation: Current
    DL-based skin lesion segmentation algorithms are mostly based on supervised learning,
    as shown in a supervision-level breakdown of the surveyed works (Fig. [5](#S2.F5
    "Figure 5 ‣ 2.3 Supervised, Semi-supervised, Weakly supervised, Self-supervised
    learning ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")),
    meaning that these algorithms require manual segmentations for training segmentation
    prediction models. Nearly all of these segmentation studies employ supervised
    segmentation evaluation, meaning that they also require manual segmentations for
    testing. Due to the scarcity of annotated skin lesion images, it may be beneficial
    to investigate unsupervised DL (Ji et al., [2019](#bib.bib201)) as well as unsupervised
    segmentation evaluation (Chabrier et al., [2006](#bib.bib83); Zhang et al., [2008](#bib.bib438)).'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Systematic evaluations: Systematic evaluations that have been performed for
    skin lesion classification (Valle et al., [2020](#bib.bib388); Bissoto et al.,
    [2021](#bib.bib61); Perez et al., [2018](#bib.bib305)) are, so far, nonexistent
    in the skin lesion segmentation literature. For example, statistical significance
    analysis are conducted on the results of a few prior studies in skin lesion segmentation,
    e.g., Fortina et al. ([2012](#bib.bib137)).'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fusion of hand-crafted and deep features: Can we integrate the deep features
    extracted by DL models and hand-crafted features synergistically? For example,
    exploration of shape and appearance priors of skin lesions that may be beneficial
    to incorporate, via loss terms (Nosrati and Hamarneh, [2016](#bib.bib290); El Jurdi
    et al., [2021](#bib.bib130); Ma et al., [2021](#bib.bib267)), in deep learning
    models for skin lesion segmentation, similar to star-shape (Mirikharaji and Hamarneh,
    [2018](#bib.bib279)) and boundary priors (Wang et al., [2021a](#bib.bib399)).'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loss of spatial resolution: The use of repeated subsampling in CNNs leads to
    coarse segmentations. Various approaches have been proposed to minimize the loss
    of spatial resolution, including fractionally-strided convolution (or deconvolution) (Long
    et al., [2015](#bib.bib264)), atrous (or dilated) convolution (Chen et al., [2017a](#bib.bib86)),
    and conditional random fields (Krahenbuhl and Koltun, [2011](#bib.bib233)). More
    research needs to be conducted to determine appropriate strategies for skin lesion
    segmentation that effectively minimize or avoid the loss of spatial resolution.'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hyperparameter tuning: Compared to traditional machine learning classifiers
    (e.g., nearest neighbors, decision trees, and support vector machines), deep neural
    networks have a large number of hyperparameters related to their architecture,
    optimization, and regularization. An average CNN classifier has about a dozen
    or more hyperparameters (Bengio, [2012](#bib.bib45)) and tuning these hyperparameters
    systematically is a laborious undertaking. *Neural architecture search* is an
    active area of research (Elsken et al., [2019](#bib.bib131)), and some of these
    model selection approaches have already been applied to semantic segmentation (Liu
    et al., [2019a](#bib.bib257)) and medical image segmentation (Weng et al., [2019](#bib.bib413)).'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reproducibility of results: Kapoor and Narayanan ([2022](#bib.bib209)) define
    research in ML-based science to be reproducible if the associated datasets and
    the code are publicly available and if there are no problems with the data analysis,
    where problems include the lack of well-defined training and testing partitions
    of the dataset, leakage across dataset partitions, features selection using the
    entire dataset instead of only the training partition, etc. Since several skin
    lesion segmentation datasets come with standardized partitions (Table [1](#S2.T1
    "Table 1 ‣ 2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation")), sharing of the code can lead to more reproducible research (Colliot
    et al., [2022](#bib.bib100)), with the added benefit to researchers who release
    their code to be cited significantly more (Vandewalle, [2012](#bib.bib390)). In
    our analysis, we found that only $38$ of the $177$ surveyed papers ($21.47\%$)
    had publicly accessible code (Table LABEL:tab:main), a proportion similar to a
    smaller-scale analysis by Renard et al. ([2020](#bib.bib325)) for medical image
    segmentation. Another potential assessment of a method’s generalization performance
    is its evaluation on a common held-out test set, where the ground truth segmentation
    masks are private, and users submit their test predictions to receive a performance
    assessment. For example, the ISIC 2018 dataset’s test partition is available through
    a live leaderboard (ISIC, [2018](#bib.bib185)), but it is rarely used. We found
    that out of $71$ papers published in 2021 and 2022 included in this survey, $36$
    papers reported results on the ISIC 2018 dataset, but only 1 paper (Saini et al.,
    [2021](#bib.bib340)) used the online submission platform for evaluation.'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f7d19e3b7a821257d875dd32c0ff5351.png)'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 10: Number of skin lesion images with ground-truth segmentation maps
    per year categorized based on modality. It is evident that while the number of
    dermoscopic skin lesion images has been constantly on the rise, the number of
    clinical images has remained unchanged for the past several years.'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Research on clinical images: Another limitation is the limited number of benchmark
    datasets of clinical skin lesion images with expert pixel-level annotations. Fig. [10](#S5.F10.1
    "Figure 10 ‣ 11st item ‣ 5 Discussion and Future Research ‣ A Survey on Deep Learning
    for Skin Lesion Segmentation") shows that while the number of dermoscopic image
    datasets with ground-truth segmentation masks has been increasing over the last
    few years, only a few datasets with clinical images are available. In contrast
    to dermoscopic images requiring a special tool that is not always utilized even
    by dermatologists (Engasser and Warshaw, [2010](#bib.bib133)), clinical images
    captured by digital cameras or smartphones have the advantage of easy accessibility,
    which can be utilized to evaluate the priority of patients by their lesion severity
    level, i.e., triage patients. As shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Datasets
    ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation") and
    Table LABEL:tab:main, most of the deep skin lesion segmentation models are trained
    and evaluated on dermoscopic images, primarily because of the lack of large-scale
    clinical skin lesion image segmentation datasets (Table [1](#S2.T1 "Table 1 ‣
    2.1 Datasets ‣ 2 Input Data ‣ A Survey on Deep Learning for Skin Lesion Segmentation")),
    leaving the need to develop automated tools for non-specialists unmet.'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Research on total body images: While there has been some research towards detecting
    and tracking skin lesions over time in 2D wide-field images (Mirzaalian et al.,
    [2016](#bib.bib282); Li et al., [2017](#bib.bib250); Korotkov et al., [2019](#bib.bib230);
    Soenksen et al., [2021](#bib.bib358); Huang et al., [2022](#bib.bib183)) and in
    3D total body images (Bogo et al., [2014](#bib.bib63); Zhao et al., [2022a](#bib.bib451);
    Ahmedt-Aristizabal et al., [2023](#bib.bib13)), simultaneous segmentation of skin
    lesions from total body images (Sinha et al., [2023](#bib.bib356)) would help
    with early detection of melanoma (Halpern, [2003](#bib.bib169); Hornung et al.,
    [2021](#bib.bib178)), thus improving patient outcomes.'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Effect on downstream tasks: End-to-end systems have been proposed for skin
    images analysis tasks that directly learn the final tasks (e.g., predicting the
    diagnosis (Kawahara et al., [2019](#bib.bib219)) or the clinical management decisions (Abhishek
    et al., [2021](#bib.bib8)) of skin lesions), and these approaches present a number
    of advantages such as computational efficiency and ease of optimization. On the
    other hand, skin lesion diagnosis pipelines have been shown to benefit from the
    incorporation of prior knowledge, specifically lesion segmentation masks (Yan
    et al., [2019](#bib.bib424)). Therefore, it is worth investigating how lesion
    segmentation, often an intermediate step in the skin image analysis pipeline,
    affects the downstream dermatological tasks.'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From binary to multi-class segmentation: While the existing work in skin lesion
    segmentation is mainly binary segmentation, future work may explore multi-class
    settings. For example, automated detection and delineation of clinical dermoscopic
    features (e.g., globules, streaks, pigment networks) within a skin lesion may
    lead to superior classification performance. Further, dermoscopic feature extraction,
    a task in the ISIC 2016 (Gutman et al., [2016](#bib.bib167)) and 2017 (Codella
    et al., [2018](#bib.bib98)) challenges, can be formulated as a multi-class segmentation
    problem (Kawahara and Hamarneh, [2018](#bib.bib220)). The multiclass formulation
    can then be addressed by DL models, and can be used either as an intermediate
    step for improving skin lesion diagnosis or used directly in diagnosis models
    for regularizing attention maps (Yan et al., [2019](#bib.bib424)). Similarly,
    multi-class segmentation scenarios may also include multiple skin pathologies
    on one subject, especially in images with large fields of view, or segmentation
    of the skin, the lesion(s), and the background, especially in in-the-wild images
    with diverse backgrounds, such as those in the Fitzpatrick17k dataset (Groh et al.,
    [2021](#bib.bib158)).'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transferability of models: As the majority of skin lesion datasets are from
    fair-skinned patients, the generalizability of deep models to populations with
    diverse skin complexions is questionable. With the emergence of dermatological
    datasets with diverse skin tones (Groh et al., [2021](#bib.bib158); Daneshjou
    et al., [2021b](#bib.bib111)) and methods for diagnosing pathologies fairly (Bevan
    and Atapour-Abarghouei, [2022](#bib.bib47); Wu et al., [2022c](#bib.bib418); Pakzad
    et al., [2023](#bib.bib295); Du et al., [2023](#bib.bib128)), it is important
    to assess the transferability of DL-based skin lesion segmentation models to datasets
    with diverse skin tones.'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6 Acknowledgements
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to acknowledge Ben Cardoen and Aditi Jain for help with
    proofreading the manuscript and with creating the interactive table, respectively.
    Z. Mirikharaji, K. Abhishek, and G. Hamarneh are partially funded by the BC Cancer
    Foundation - BrainCare BC Fund, the Natural Sciences and Engineering Research
    Council of Canada (NSERC RGPIN-06752), and the Canadian Institutes of Health Research
    (CIHR OQI-137993). A. Bissoto is partially funded by FAPESP 2019/19619-7\. E.
    Valle is partially funded by CNPq 315168/2020-0\. S. Avila is partially funded
    by CNPq PQ-2 315231/2020-3, and FAPESP 2013/08293-7\. A. Bissoto and S. Avila
    are also partially funded by Google LARA 2020\. The RECOD.ai lab is supported
    by projects from FAPESP, CNPq, and CAPES. C. Barata is funded by FCT project and
    multi-year funding [CEECIND/00326/2017] and LARSyS - FCT Plurianual funding 2020-2023\.
    M. E. Celebi was supported by the US National Science Foundation under Award No.
    OIA-1946391\. Any opinions, findings, and conclusions or recommendations expressed
    in this material are those of the authors and do not necessarily reflect the views
    of the National Science Foundation.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: DL models for skin lesion segmentation. Performance measure reported
    is the Jaccard index computed on the dataset, shown in boldface. The score is
    asterisked if it is computed based on the reported Dice index. The following abbreviations
    are used: Ref.: reference, Arch.: architecture, Seg.: segmentation, J: Jaccard
    index, CDE : cross-data evaluation. the highlighted dataset and PP: postprocessing,
    con.: connection and conv.: convolution, CE: cross-entropy, WCE: weighted cross-entropy,
    DS: deep supervision, EPE: end point error, $\ell_{1}$: $\ell_{1}$ norm, $\ell_{2}$:
    $\ell_{2}$ norm and ADV: adversarial loss. Please see the corresponding sections
    for more details: Section [3.1](#S3.SS1 "3.1 Architecture ‣ 3 Model Design and
    Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation") for model
    architectures, Section [3.2](#S3.SS2 "3.2 Loss Functions ‣ 3 Model Design and
    Training ‣ A Survey on Deep Learning for Skin Lesion Segmentation") for loss functions,
    and Section [4](#S4 "4 Evaluation ‣ A Survey on Deep Learning for Skin Lesion
    Segmentation") for model evaluation. An interactive version of this table is available
    online at [https://github.com/sfu-mial/skin-lesion-segmentation-survey](https://github.com/sfu-mial/skin-lesion-segmentation-survey).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | Venue | Data | Arch. modules | Seg. loss | J | CDE | Augmentation
    | PP | code |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| Jafari et al. ([2016](#bib.bib194)) | peer-reviewed conference | DermQuest
    | image pyramid | - | - | ✗ | - | ✓ | ✗ |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| He et al. ([2017](#bib.bib175)) | peer-reviewed conference | ISIC2016 ISIC2017
    | residual con. skip con. image pyramid | Dice CE DS | 75.80% | ✗ | rotation |
    ✓ | ✗ |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Bozorgtabar et al. ([2017b](#bib.bib66)) | peer-reviewed journal | ISIC2016
    | - | - | 80.60% | ✗ | rotation | ✗ | ✗ |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| Ramachandram and Taylor ([2017](#bib.bib315)) | peer-reviewed journal | ISIC2017
    | - | CE | 79.20% | ✗ | rotation, flipping color jittering | ✗ | ✗ |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| Yu et al. ([2017a](#bib.bib431)) | peer-reviewed journal | ISIC2016 | skip
    con. residual con. | - | 82.90% | ✗ | rotation,translation random noise cropping
    | ✗ | ✓ |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| Bi et al. ([2017b](#bib.bib53)) | peer-reviewed journal | ISIC2016 PH² |
    - | CE | 84.64% | ✓ | flipping,cropping | ✓ | ✗ |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| Jafari et al. ([2017](#bib.bib195)) | peer-reviewed journal | DermQuest |
    image pyramid | - | - | ✗ | - | ✓ | ✗ |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| Yuan et al. ([2017](#bib.bib433)) | peer-reviewed journal | ISIC2016 PH²
    | - | Tanimoto | 84.7% | ✓ | flipping, rotation scaling,shifting contrast norm.
    | ✓ | ✗ |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| Ramachandram and DeVries ([2017](#bib.bib314)) | non peer-reviewed technical
    report | ISIC2017 | dilated conv. | CE | 64.20% | ✗ | rotation flipping | ✓ |
    ✗ |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| Bozorgtabar et al. ([2017a](#bib.bib65)) | peer-reviewed conference | ISIC2016
    | - | CE | 82.90% | ✗ | rotations | ✓ | ✗ |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| Bi et al. ([2017a](#bib.bib51)) | peer-reviewed conference | ISIC2016 | parallel
    m. s. | - | 86.36% | ✗ | crops,flipping | ✓ | ✗ |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| Attia et al. ([2017](#bib.bib28)) | peer-reviewed conference | ISIC2016 |
    recurrent net. | - | 93.00% | ✗ | - | ✗ | ✗ |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| Deng et al. ([2017](#bib.bib114)) | peer-reviewed conference | ISIC2016 |
    parallel m. s. | - | 84.1% | ✗ | - | ✗ | ✗ |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| Mishra and Daescu ([2017](#bib.bib283)) | peer-reviewed conference | ISIC2017
    | skip con. | Dice | 84.2% | ✗ | rotation flipping | ✓ | ✗ |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| Goyal et al. ([2017](#bib.bib155)) | peer-reviewed conference | ISIC2017
    | - | CE Dice | - | ✗ | - | ✗ | ✗ |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| Vesal et al. ([2018a](#bib.bib394)) | peer-reviewed conference | ISIC2017
    PH² | dilated conv. dense con. skip con. | Dice | 88.00% | ✓ | - | ✗ | ✗ |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| Venkatesh et al. ([2018](#bib.bib393)) | peer-reviewed conference | ISIC2017
    | residual con. skip con. | Jaccard | 76.40% | ✗ | rotation,flipping translation,
    scaling | ✓ | ✗ |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. ([2018](#bib.bib426)) | peer-reviewed conference | ISIC2017 |
    skip con. parallel m.s. conv. | - | 74.10% | ✗ | rotation,flipping | ✗ | ✗ |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| Sarker et al. ([2018](#bib.bib342)) | peer-reviewed conference | ISIC2016
    ISIC2017 | skip con. residual con. dilated conv. pyramid pooling | CE EPE | 78.20%
    | ✗ | rotation,scaling | ✗ | ✓ |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| Al-Masni et al. ([2018](#bib.bib15)) | peer-reviewed journal | ISIC2017 PH²
    | - | CE | 77.10% | ✓ | rotation | ✗ | ✗ |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2018b](#bib.bib248)) | peer-reviewed conference | ISIC2017 |
    skip con. residual con. | DS | 77.23% | ✗ | flipping, rotation | ✗ | ✓ |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| Zeng and Zheng ([2018](#bib.bib436)) | peer-reviewed conference | ISIC2017
    | dense con. skip con. image pyramid | CE $\ell_{2}$ DS | 78.50% | ✗ | flipping,
    rotation | ✓ | ✗ |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| DeVries and Taylor ([2018](#bib.bib121)) | non peer-reviewed technical report
    | ISIC2017 | skip con. | CE | 73.00% | ✗ | flipping, rotation | ✗ | ✗ |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| Izadi et al. ([2018](#bib.bib190)) | peer-reviewed conference | DermoFit
    | skip con. | CE ADV | 81.20% | ✗ | flipping, rotation elastic deformation | ✗
    | ✓ |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2018a](#bib.bib243)) | peer-reviewed journal | ISIC2016 ISIC2017
    | skip con. residual con. dense con. | Jaccard DS | 76.50% | ✗ | - | ✗ | ✗ |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| Mirikharaji and Hamarneh ([2018](#bib.bib279)) | peer-reviewed conference
    | ISIC2017 | residual con. | CE Star shape | 77.30% | ✗ | - | ✗ | ✗ |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| Pollastri et al. ([2018](#bib.bib308)) | peer-reviewed conference | ISIC2017
    | - | Jaccard $\ell_{1}$ | 78.10% | ✗ | GAN | ✓ | ✗ |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| Vesal et al. ([2018b](#bib.bib395)) | abstract | ISIC2017 | dilated conv.
    dense con. skip con. | Dice | 76.67% | ✗ | rotation, flipping, translation, scaling,
    color shift | ✗ | ✗ |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. ([2018b](#bib.bib90)) | peer-reviewed conference | ISIC2017 |
    residual con. dilated conv. parallel m.s. conv. | WCE | 78.70% | ✗ | rotation,
    flipping cropping, zooming Gaussian noise | ✓ | ✗ |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| Jahanifar et al. ([2018](#bib.bib196)) | non peer-reviewed technical report
    | ISIC2016 ISIC2017 ISIC2018 | skip con. pyramid pooling parallel m.s. conv. |
    Tanimoto | 80.60% | ✓ | flipping, rotation zooming,translation shearing,color
    shift intensity scaling adding noises contrast adjust. sharpness adjust. disturb
    illumination hair occlusion | ✓ | ✗ |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| Mirikharaji et al. ([2018](#bib.bib280)) | peer-reviewed conference | ISIC2016
    | skip con. | CE | 83.30% | ✗ | flipping,rottaion | ✗ | ✗ |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| Bi et al. ([2018](#bib.bib49)) | non peer-reviewed technical report | ISIC2018
    | residual con. | CE | 83.12% | ✗ | GAN | ✗ | ✗ |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| He et al. ([2018](#bib.bib176)) | peer-reviewed journal | ISIC2016 ISIC2017
    | skip con. residual con. image pyramid | CE Dice DS | 76.10% | ✗ | rotation |
    ✓ | ✗ |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| Xue et al. ([2018](#bib.bib423)) | peer-reviewed conference | ISIC2017 |
    skip con. residual con. global conv. GAN | $\ell_{1}$ DS ADV | 78.50% | ✗ | cropping
    color jittering | ✗ | ✗ |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| Ebenezer and Rajapakse ([2018](#bib.bib129)) | non peer-reviewed technical
    report | ISIC 2018 | skip con. | Dice | 75.6% | ✗ | rotation flipping zooming
    | ✓ | ✓ |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| Goyal et al. ([2019b](#bib.bib154)) | peer-reviewed journal | ISIC2017 PH²
    | dilated conv. parallel m.s. conv. separable conv. | - | 79.34% | ✓ | - | ✓ |
    ✗ |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| Azad et al. ([2019](#bib.bib29)) | peer-reviewed conference | ISIC2018 |
    skip con. dense con. recurrent CNN | CE | 74.00% | ✗ | - | ✗ | ✓ |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| Alom et al. ([2019](#bib.bib22)) | peer-reviewed journal | ISIC2017 | skip
    con. residual con. recurrent CNN | CE | 75.68% | ✗ | - | ✗ | ✗ |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| Yuan and Lo ([2019](#bib.bib434)) | peer-reviewed journal | ISIC2017 | -
    | Tanimoto | 76.50% | ✗ | rotation,flipping shifting, scaling random normaliz.
    | ✓ | ✗ |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| Goyal et al. ([2019a](#bib.bib153)) | peer-reviewed conference | ISIC2017
    PH² | dilated conv. parallel m.s. conv. | WCE | 82.20% | ✓ | - | ✗ | ✗ |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| Bi et al. ([2019b](#bib.bib52)) | peer-reviewed journal | ISIC2016 ISIC2017
    PH² | skip con. residual con. | CE | 77.73% | ✓ | flipping, cropping | ✓ | ✗ |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| Tschandl et al. ([2019](#bib.bib382)) | peer-reviewed journal | ISIC2017
    | skip con. | CE Jaccard | 76.80% | ✗ | flipping, rotation | ✓ | ✗ |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2021c](#bib.bib247)) | peer-reviewed journal | ISIC2017 | skip
    con. dense con. semi-supervised ensemble | CE $\ell_{1}$ | 79.80% | ✗ | flipping,rotating
    scaling | ✓ | ✗ |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([2019b](#bib.bib441)) | peer-reviewed journal | ISIC2016 ISIC2017
    | skip con. | CE | 72.94% | ✗ | - | ✗ | ✗ |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| Baghersalimi et al. ([2019](#bib.bib35)) | peer-reviewed journal | ISIC2016
    ISIC2017 PH² | skip con. residual con. dense con. | Tanimoto | 78.30% | ✓ | flipping,cropping
    | ✗ | ✗ |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. ([2019](#bib.bib203)) | peer-reviewed conference | ISIC2017
    | residual con. dilated conv. GAN | ADV $\ell_{2}$ | 76.90% | ✗ | rotation,flipping
    | ✗ | ✗ |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. ([2019b](#bib.bib374)) | peer-reviewed conference | ISIC2016
    | skip con. | Tanimoto DS | 85.34% | ✗ | rotation,flipping | ✗ | ✗ |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| Bi et al. ([2019a](#bib.bib48)) | peer-reviewed conference | ISIC2017 | residual
    con. | CE | 77.14% | ✗ | GAN | ✗ | ✗ |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| Abraham and Khan ([2019](#bib.bib9)) | peer-reviewed conference | ISIC2018
    | skip con. image pyramid attention | TV Focal | 74.80% | ✗ | - | ✗ | ✓ |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| Cui et al. ([2019](#bib.bib105)) | peer-reviewed conference | ISIC2018 |
    dilated conv. parallel m.s. conv. separable conv. | - | 83.00% | ✗ | - | ✗ | ✗
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| Song et al. ([2019](#bib.bib359)) | peer-reviewed conference | ISIC2017 |
    skip con. residual con. dense con. attention mod. | CE Jaccard | 76.50% | ✗ |
    - | ✗ | ✗ |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| Singh et al. ([2019](#bib.bib355)) | peer-reviewed journal | ISIC2016 ISIC2017
    ISIC2018 | skip con. residual con. factorized conv. attention mod. GAN | CE $\ell_{1}$
    EPE | 78.65% | ✗ | - | ✗ | ✓ |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| Tan et al. ([2019b](#bib.bib370)) | peer-reviewed journal | ISIC2017 DermoFit
    PH² | dilated conv. | Dice | 62.29%^∗ | ✓ | - | ✓ | ✗ |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| Kaul et al. ([2019](#bib.bib215)) | peer-reviewed conference | ISIC2017 |
    skip con. residual con. attention mod. | Dice | 75.60% | ✗ | channel shift | ✗
    | ✗ |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| De Angelo et al. ([2019](#bib.bib112)) | peer-reviewed conference | ISIC2017
    Private | skip con. | CE Dice | 76.07% | ✗ | flipping, shifting rotation color
    jittering | ✓ | ✗ |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([2019a](#bib.bib437)) | peer-reviewed journal | ISIC2017 PH²
    | skip con. residual con. parallel m.s. conv. | CE Dice DS | 78.50% | ✓ | flipping,
    rotation whitening contrast enhance. | ✓ | ✗ |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| Soudani and Barhoumi ([2019](#bib.bib361)) | peer-reviewed journal | ISIC2017
    | residual con. | CE | 78.60% | ✗ | rotation, flipping | ✗ | ✗ |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| Mirikharaji et al. ([2019](#bib.bib281)) | peer-reviewed conference | ISIC2017
    | skip con. | WCE | 68.91%^∗ | ✗ | - | ✗ | ✗ |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '| Nasr-Esfahani et al. ([2019](#bib.bib285)) | peer-reviewed journal | DermQuest
    | dense con. | WCE | 85.20% | ✗ | rotation,flipping cropping | ✗ | ✗ |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2019a](#bib.bib397)) | peer-reviewed conference | ISIC2017
    ISIC2018 | skip con. residual con. parallel m.s. conv. attention mod. | WDice
    | 77.60% | ✗ | copping, flipping | ✗ | ✗ |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '| Sarker et al. ([2019](#bib.bib341)) | non peer-reviewed technical report
    | ISIC2017 ISIC2018 | factrized conv. attention mod. GAN | CE Jaccard $\ell_{1}$,ADV
    | 77.98% | ✗ | flipping gamma reconst. contrast adjust. | ✗ | ✗ |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| Tu et al. ([2019](#bib.bib383)) | peer-reviewed journal | ISIC2017 PH² |
    skip con. residual con. dense con. GAN | Jaccard EPE, $\ell_{1}$ DS, ADV | 76.80%
    | ✓ | flipping | ✗ | ✗ |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: '| Wei et al. ([2019](#bib.bib412)) | peer-reviewed journal | ISIC2016 ISIC2017
    PH² | skip con. residual con. attention mod. GAN | Jaccard $\ell_{1}$ ADV | 80.45%
    | ✓ | rotation,flipping color jittering | ✗ | ✗ |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '| Ünver and Ayan ([2019](#bib.bib385)) | peer-reviewed journal | ISIC2017 PH²
    | - | $\ell_{2}$ | 74.81% | ✓ | - | ✓ | ✗ |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: '| Al-masni et al. ([2019](#bib.bib16)) | peer-reviewed conference | ISIC2017
    | - | - | 77.11% | ✗ | rotation,flipping | ✗ | ✗ |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
- en: '| Canalini et al. ([2019](#bib.bib70)) | peer-reviewed conference | ISIC2017
    | dilated conv. parallel m.s. conv. separable conv. | CE Tanimoto | 85.00% | ✗
    | rotating, flipping shifting, shearing scaling color jittering | ✓ | ✗ |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2019b](#bib.bib404)) | peer-reviewed conference | ISIC2017
    | residual con. | WCE | 78.10% | ✗ | flipping, scaling | ✗ | ✗ |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
- en: '| Alom et al. ([2020](#bib.bib21)) | peer-reviewed conference | ISIC2018 |
    skip con. residual con. recurrent CNN | CE | 88.83% | ✗ | flipping | ✗ | ✗ |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '| Pollastri et al. ([2020](#bib.bib309)) | peer-reviewed journal | ISIC2017
    | - | Tanimoto | 78.90% | ✗ | GAN flipping,rotation shifting, scaling color jittering
    | ✗ | ✗ |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2019b](#bib.bib258)) | peer-reviewed conference | ISIC2017 |
    skip con. dilated conv. | CE | 75.20% | ✗ | scaling, cropping rotation, flipping
    image deformation | ✗ | ✗ |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
- en: '| Abhishek and Hamarneh ([2019](#bib.bib5)) | peer-reviewed conference | ISIC2017
    PH² | skip con. | - | 68.69%^∗ | ✓ | rotation,flipping GAN | ✗ | ✓ |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| Shahin et al. ([2019](#bib.bib345)) | peer-reviewed conference | ISIC2018
    | skip con. image pyramid | Generalized Dice | 73.8% | ✗ | rotation flipping zooming
    | ✗ | ✗ |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| Adegun and Viriri ([2019](#bib.bib10)) | peer-reviewed conference | ISIC2017
    | - | Dice | 83.0% | ✗ | elastic | ✗ | ✗ |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| Taghanaki et al. ([2019](#bib.bib365)) | peer-reviewed conference | ISIC
    2017 | skip con. | Dice $\ell_{1}$ SSIM | 69.35%^∗ | ✗ | rotation flipping gradient-based
    perturbation | ✗ | ✗ |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '| Saini et al. ([2019](#bib.bib339)) | peer-reviewed conference | ISIC 2017
    ISIC 2018 PH2 | skip con. multi-task | Dice | 84.9% | ✗ | rotation, flipping shearing,
    stretch crop, contrast | ✗ | ✗ |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2019c](#bib.bib405)) | peer-reviewed journal | ISIC2016 ISIC2017
    | skip con. residual con. dilated conv. | WCE | 81.47% | ✗ | flipping,scaling
    | ✗ | ✗ |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: '| Kamalakannan et al. ([2019](#bib.bib208)) | peer-reviewed journal | ISIC
    Archive | skip con. | CE | - | ✗ | - | ✗ | ✗ |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
- en: '| Hasan et al. ([2020](#bib.bib172)) | peer-reviewed journal | ISIC2017 PH²
    | skip con. dense con. separable conv. | CE Jaccard | 77.50% | ✓ | rotation, zooming
    shifting, flipping | ✗ | ✓ |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
- en: '| Al Nazi and Abir ([2020](#bib.bib18)) | peer-reviewed conference | ISIC2018
    PH² | skip con. | Dice | 80.00% | ✓ | rotation, zooming flipping,elastic dist.
    Gaussian dist. histogram equal. color jittering | ✗ | ✓ |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| Deng et al. ([2020](#bib.bib115)) | peer-reviewed conference | ISIC2017 PH²
    | dilated conv. parallel m.s. conv. separable conv. semi-supervised | Dice Narrowband
    suppression | 83.9% | ✓ | rotation | ✓ | ✗ |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. ([2020b](#bib.bib420)) | peer-reviewed journal | ISIC2017 PH²
    | dilated conv. parallel m.s. conv. separable conv. | Dice Rank | 80.4% | ✓ |
    cropping,scaling rotation, shearing shifting,zooming whitening, flipping | ✗ |
    ✓ |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([2020a](#bib.bib439)) | peer-reviewed conference | SCD ISIC2016
    ISIC2017 ISIC2018 | skip con. | Kappa Loss | 84.00%^∗ | ✗ | rotation,shifting
    shearing,zooming flipping | ✗ | ✓ |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| Saha et al. ([2020](#bib.bib337)) | peer-reviewed conference | ISIC2017 ISIC2018
    | skip con. dense con. | CE | 81.9% | ✗ | color jittering rotation flipping translation
    | ✗ | ✗ |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| Henry et al. ([2020](#bib.bib177)) | peer-reviewed conference | ISIC2018
    | skip con. parallel m. s. conv. attention mod. | - | 78.04% | ✗ | color jittering
    rotation,cropping flipping,shift | ✗ | ✓ |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| Jafari et al. ([2020](#bib.bib193)) | peer-reviewed conference | ISIC2018
    | skip con. residual con. dense con. | CE | 75.5% | ✗ | - | ✗ | ✓ |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2020a](#bib.bib244)) | peer-reviewed conference | ISIC2018 |
    skip con. residual con. ensemble semi-supervised | CE Dice | 75.5% | ✗ | - | ✗
    | ✗ |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| Guo et al. ([2020](#bib.bib165)) | peer-reviewed conference | ISIC2018 |
    skip con. dilated conv. parallel m. s. conv. | Focal Jaccard | 77.60% | ✗ | -
    | ✗ | ✓ |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2020b](#bib.bib249)) | peer-reviewed conference | ISIC2018 |
    skip con. residual con. self-supervised | MSE KLD | 87.74%^∗ | ✗ | - | ✗ | ✗ |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. ([2020](#bib.bib205)) | peer-reviewed journal | ISIC2017 PH²
    | skip con. residual con. attention mod. | CE | 73.35% | ✗ | flipping | ✗ | ✗
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
- en: '| Qiu et al. ([2020](#bib.bib312)) | peer-reviewed journal | ISIC2017 PH² |
    ensemble | - | 80.02% | ✗ | translation rotation shearing | ✓ | ✗ |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. ([2020a](#bib.bib419)) | peer-reviewed journal | ISIC2016 ISIC2017
    PH² | attention mod. | CE | 78.3% | ✗ | rotation flipping | ✗ | ✗ |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '| Zafar et al. ([2020](#bib.bib435)) | peer-reviewed journal | ISIC2017 PH²
    | skip con. residual con. | CE | 77.2% | ✗ | rotation | ✗ | ✗ |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: '| Azad et al. ([2020](#bib.bib30)) | peer-reviewed conference | ISIC 2017 ISIC
    2018 PH2 | dilated conv. attention mod. | - | 96.98% | ✗ | - | ✗ | ✓ |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
- en: '| Nathan and Kansal ([2020](#bib.bib286)) | non peer-reviewed technical report
    | ISIC 2016 ISIC 2017 ISIC 2018 PH2 | skip con. residual con. | CE Dice | 78.28%
    | ✗ | rotation, flipping shearing, zoom | ✗ | ✗ |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
- en: '| Mirikharaji et al. ([2021](#bib.bib278)) | peer-reviewed conference | ISIC
    Archive PH2 DermoFit | skip con. residual con. ensemble | CE | 72.11% | ✗ | -
    | ✗ | ✗ |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
- en: '| Öztürk and Özkaya ([2020](#bib.bib293)) | peer-reviewed journal | ISIC 2017
    PH2 | residual con. | - | 78.34% | ✓ | - | ✗ | ✗ |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '| Abhishek et al. ([2020](#bib.bib7)) | peer-reviewed conference | ISIC 2017
    DermoFit PH2 | skip con. | Dice | 75.70% | ✓ | rotation flipping | ✗ | ✓ |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
- en: '| Kaymak et al. ([2020](#bib.bib221)) | peer-reviewed journal | ISIC 2017 |
    - | - | 72.5% | ✗ | - | ✗ | ✗ |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
- en: '| Bagheri et al. ([2020](#bib.bib32)) | peer-reviewed journal | ISIC2017 DermQuest
    | dilated conv. parallel m.s. conv. separable conv. | - | 79.05% | ✓ | rotation,flipping
    brightness change resizing | ✗ | ✗ |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
- en: '| Jayapriya and Jacob ([2020](#bib.bib199)) | peer-reviewed journal | ISIC2016
    | skip con. parallel m.s. conv. | - | 92.42% | ✗ | - | ✗ | ✗ |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2020a](#bib.bib401)) | non peer-reviewed technical report |
    ISIC2016 ISIC2017 PH² | residual con. dilated conv. attention mod. | CE Dice DS
    | 80.30% | ✓ | flipping, rotation cropping | ✗ | ✗ |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2020b](#bib.bib408)) | non peer-reviewed technical report |
    ISIC2018 PH² | attention mod. skip con. parallel m.s. conv. recurrent CNN | Dice
    Focal Tversky | 80.6% | ✗ | rotation flipping cropping | ✗ | ✗ |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
- en: '| Ribeiro et al. ([2020](#bib.bib326)) | peer-reviewed conference | ISIC Archive
    PH² DermoFit | skip con. residual con. dilated conv. | Soft Jaccard CE | - | ✓
    | Gaussian noise color jittering | ✓ | ✓ |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
- en: '| Zhu et al. ([2020](#bib.bib455)) | peer-reviewed conference | ISIC2018 |
    skip con. residual con. dilated conv. attention mod. | CE Dice | 82.15% | ✗ |
    flipping | ✗ | ✗ |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
- en: '| Gu et al. ([2020](#bib.bib160)) | peer-reviewed journal | ISIC 2018 | residual
    con. skip con. attention mod. | Dice | 85.32%^∗ | ✗ | cropping, flipping rotation
    | ✗ | ✓ |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. ([2020](#bib.bib241)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 | skip con. dense con. dilated conv. GAN | CE $\ell_{1}$ ADV | 77.1% | ✓
    | flipping, rotation | ✗ | ✗ |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
- en: '| Andrade et al. ([2020](#bib.bib24)) | peer-reviewed journal | DermoFit SMARTSKINS
    | residual con. dilated conv. GAN | Dice | 81.03% | ✗ | flipping, brightness saturation,
    contrast, hue Gaussian hue | ✗ | ✗ |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. ([2020](#bib.bib416)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 | residual con. attention mod. multi-scale | CE Dice | 82.55% | ✗ | flipping,
    rotation scaling, cropping sharpening, color distribution adj., noise | ✗ | ✗
    |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: '| Arora et al. ([2021](#bib.bib26)) | peer-reviewed journal | ISIC 2018 | skip
    con. attention mod. | Dice Tversky Focal Tversky | 83% | ✗ | flipping | ✓ | ✗
    |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. ([2021](#bib.bib206)) | peer-reviewed journal | ISIC2017 ISIC2018
    | skip con. residual con. attention mod. | Dice Focal | 80.00% | ✗ | flipping,
    rotation affine trans. scaling, cropping | ✗ | ✓ |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| Hasan et al. ([2021](#bib.bib171)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 | skip con. residual con. separable conv. | Dice CE | 66.66%^∗ | ✗ | flipping,
    rotation shifting, zooming intensity adjust. | ✗ | ✗ |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '| Kosgiker et al. ([2021](#bib.bib231)) | peer-reviewed journal | ISIC 2017
    PH² | - | MSE CE | 90.25% | ✗ | - | ✗ | ✗ |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
- en: '| Bagheri et al. ([2021a](#bib.bib33)) | peer-reviewed journal | ISIC2016 ISIC2017
    ISIC2018 PH² DermQuest | parallel m.s. conv. dilated conv. | Dice CE | 85.04%
    | ✓ | rotation flipping color jittering | ✗ | ✗ |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '| Saini et al. ([2021](#bib.bib340)) | peer-reviewed conference | ISIC2017
    ISIC2018 PH² | pyramid pooling residual con. skip con. dilated conv. attention
    mod. | Dice | 85.00% | ✓ | rotation,shearing color jittering | ✗ | ✗ |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: '| Tong et al. ([2021](#bib.bib376)) | peer-reviewed journal | ISIC2016 ISIC2017
    PH² | skip con. attention mod. | CE | 84.2% | ✓ | flipping | ✗ | ✗ |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '| Bagheri et al. ([2021b](#bib.bib34)) | peer-reviewed journal | DermQuest
    ISIC2017 PH² | ensemble | CE Focal | 86.53% | ✓ | rotation flipping color jittering
    | ✓ | ✗ |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
- en: '| Ren et al. ([2021](#bib.bib324)) | peer-reviewed journal | ISIC2017 | dense
    con. dilated conv. separable conv. attention mod. | Dice CE | 76.92% | ✗ | flipping,
    rotation | ✗ | ✗ |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2021a](#bib.bib260)) | peer-reviewed journal | ISIC2017 | residual
    con. dilated conv. pyramid pooling | WCE | 79.46% | ✗ | flipping, cropping rotation
    image deformation | ✗ | ✗ |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
- en: '| Khan et al. ([2021](#bib.bib224)) | peer-reviewed journal | ISIC2018 | skip
    con. image pyramid | Dice | 85.10% | ✗ | - | ✗ | ✓ |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
- en: '| Redekop and Chernyavskiy ([2021](#bib.bib321)) | peer-reviewed conference
    | ISIC2017 | - | - | 68.77$\%^{*}$ | ✗ | - | ✗ | ✗ |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
- en: '| Kaul et al. ([2021](#bib.bib216)) | peer-reviewed conference | ISIC2018 |
    skip con. residual con. attention mod. | CE Tversky adaptive logarithmic | 82.71%
    | ✗ | - | ✗ | ✓ |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
- en: '| Abhishek and Hamarneh ([2021](#bib.bib6)) | peer-reviewed conference | ISIC2017
    PH² DermoFit | skip con. | MCC | 75.18% | ✗ | flipping, rotation | ✗ | ✓ |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. ([2021b](#bib.bib373)) | peer-reviewed journal | ISIC2018 | skip
    con. | CE | 78.25% | ✗ | - | ✗ | ✗ |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. ([2021](#bib.bib421)) | peer-reviewed conference | ISIC2018 |
    dilated conv. semi-supervised | CE KL div. | 82.37% | ✗ | scaling,rotation elastic
    transformation | ✗ | ✗ |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
- en: '| Poudel and Lee ([2021](#bib.bib310)) | peer-reviewed journal | ISIC2017 |
    skip con. attention mod. | CE | 87.44% | ✗ | scaling, flipping rotation Gaussian
    noise median blur | ✗ | ✗ |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
- en: '| Şahin et al. ([2021](#bib.bib338)) | peer-reviewed journal | ISIC2016 ISIC
    2017 | skip con. Gaussian process | - | 74.51% | ✗ | resize rotation reflection
    | ✓ | ✗ |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
- en: '| Sarker et al. ([2021](#bib.bib343)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 | parallel m.s. conv. attention mod. GAN | $\ell_{1}$ Jaccard | 81.98% |
    ✗ | flipping, contrast gamma reconstruction | ✗ | ✗ |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2021b](#bib.bib406)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 | residual con. skip con. lesion-based pooling feature fusion | CE | 82.4%
    | ✗ | flipping, scaling cropping | ✗ | ✗ |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
- en: '| Sachin et al. ([2021](#bib.bib335)) | book chapter | ISIC 2018 | residual
    con. skip con. | - | 75.96% | ✗ | flipping, scaling color jittering | ✗ | ✗ |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
- en: '| Wibowo et al. ([2021](#bib.bib414)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 PH2 | BConvLSTM separable conv. residual con. skip con. | Jaccard | 80.25%
    | ✗ | distortion, blur color jittering contrast gamma sharpen | ✓ | ✓ |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
- en: '| Gudhe et al. ([2021](#bib.bib162)) | peer-reviewed journal | ISIC 2018 |
    dilated conv. residual con. skip con. | CE | 91% | ✗ | flipping, scaling shearing,
    color jittering Gaussian blur Gaussian noise | ✗ | ✓ |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
- en: '| Khouloud et al. ([2021](#bib.bib226)) | peer-reviewed journal | ISIC 2016
    ISIC 2017 ISIC 2018 PH2 | feature pyramid residual con. skip con. attention mod.
    | - | 86.92%^∗ | ✗ | - | ✗ | ✗ |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
- en: '| Gu et al. ([2021](#bib.bib159)) | peer-reviewed conference | ISIC 2017 |
    asymmetric conv. skip con. | DS | 79.4% | ✗ | cropping, flipping rotation | ✗
    | ✗ |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. ([2021](#bib.bib448)) | peer-reviewed journal | ISIC 2018 | pyramid
    pooling attention mod. residual con. skip con. | CE Dice | 86.84% | ✗ | cropping
    | ✗ | ✗ |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. ([2021a](#bib.bib372)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 ISIC 2018 | attention mod. residual con. skip con. ensemble pyramid pooling
    | Focal | 80.7% | ✗ | copying | ✗ | ✗ |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
- en: '| Zunair and Hamza ([2021](#bib.bib460)) | peer-reviewed journal | ISIC 2018
    | sharpening kernel residual con. | CE | 79.78% | ✗ | - | ✗ | ✓ |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2021a](#bib.bib245)) | peer-reviewed conference | ISIC 2017 |
    skip con. | CE KL div. | 71.12%* | ✗ | - | ✗ | ✓ |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([2021a](#bib.bib442)) | peer-reviewed conference | ISIC 2016
    | skip con. residual con. feature fusion semi-supervised self-supervised | CE
    Dice | 80.49% | ✗ | flipping, rotation zooming, cropping | ✗ | ✓ |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. ([2021](#bib.bib422)) | peer-reviewed conference | ISIC 2018 |
    Transformer multi-scale | Dice | 89.6% | ✗ | flipping, rotation | ✗ | ✗ |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| Ahn et al. ([2021](#bib.bib14)) | peer-reviewed conference | PH² | self-supervised
    clustering | CE Spatial loss Consistency loss | 71.53%* | ✗ | - | ✗ | ✓ |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([2021b](#bib.bib445)) | peer-reviewed conference | ISIC 2017
    | skip con. feature fusion Transformer | CE Jaccard | 79.5% | ✗ | rotation, flipping
    color jittering | ✗ | ✓ |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| Ji et al. ([2021](#bib.bib202)) | peer-reviewed conference | ISIC 2018 |
    skip con. multi-scale Transformer | CE Dice | 82.4%* | ✗ | flipping | ✗ | ✓ |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2021a](#bib.bib399)) | peer-reviewed conference | ISIC 2016
    ISIC 2018 PH² | multi-scale Transformer | CE Dice | 84.3%* | ✓ | flipping, scaling
    | ✗ | ✓ |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. ([2021](#bib.bib425)) | peer-reviewed journal | ISIC 2018 PH²
    | skip con. multi-scale feature fusion | CE Dice | 94.0% | ✗ | rotation, flipping
    cropping, HSC manipulation, luminance and contrast shift | ✗ | ✗ |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '| Tao et al. ([2021](#bib.bib375)) | peer-reviewed journal | ISIC 2017 PH²
    | skip con. dense con. attention mod. multi-scale | - | 78.85% | ✗ | rotation
    | ✗ | ✗ |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: '| Kim and Lee ([2021](#bib.bib227)) | peer-reviewed journal | ISIC 2016 PH²
    | residual con. skip con. | boundary aware loss | 84.33%* | ✗ | - | ✗ | ✗ |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. ([2022](#bib.bib107)) | peer-reviewed journal | ISIC2018 PH2 |
    residual con. skip con. dilated conv. image pyramid attention mod. | CE Dice SoftDice
    | 83.45% | ✓ | cropping, flipping rotation | ✗ | ✗ |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
- en: '| Bi et al. ([2022](#bib.bib50)) | peer-reviewed journal | ISIC2016 ISIC2017
    PH2 | residual con. skip con. attention mod. feature fusion | CE | 83.70% | ✓
    | cropping, flipping | ✗ | ✗ |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. ([2022](#bib.bib253)) | peer-reviewed conference | ISIC 2017 ISIC
    2018 | attention mod. Transformer | CE Jaccard DS | 77.81%* | ✗ | flipping, rotation
    | ✗ | ✗ |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. ([2022b](#bib.bib417)) | peer-reviewed conference | PH² | skip
    con. Transformer multi-scale | CE | 70.0%* | ✗ | - | ✗ | ✗ |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
- en: '| Valanarasu and Patel ([2022](#bib.bib387)) | peer-reviewed conference | ISIC
    2018 | skip con. | CE Dice | 81.7% | ✗ | - | ✗ | ✓ |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
- en: '| Basak et al. ([2022](#bib.bib42)) | peer-reviewed journal | ISIC 2017 PH²
    HAM10000 | residual con. multi-scale attention mod. | CE Jaccard DS | 97.4% |
    ✗ | - | ✗ | ✓ |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. ([2022a](#bib.bib415)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 ISIC 2018 PH² | skip con. residual con. attention mod. Transformer | CE Dice
    | 76.53% | ✗ | flipping, rotation brightness change contrast change change in
    H,S,V | ✗ | ✓ |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2022a](#bib.bib261)) | peer-reviewed journal | ISIC 2017 | skip
    con. residual con. dilated conv. attention mod. | CE Dice | 78.62% | ✗ | flipping,
    rotation | ✗ | ✗ |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2022b](#bib.bib402)) | peer-reviewed journal | ISIC 2017 |
    skip con. residual con. Transformer | - | 84.52% | ✗ | flipping, rotation | ✗
    | ✓ |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([2022a](#bib.bib444)) | peer-reviewed conference | ISIC 2017
    | skip con. feature fusion | Dice Focal | 74.54% | ✗ | flipping | ✗ | ✗ |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2022d](#bib.bib410)) | peer-reviewed conference | ISIC 2017
    PH² | skip con. residual con. self-supervised | Dice | 76.5% | ✓ | rotation, flipping
    color jittering | ✗ | ✗ |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
- en: '| Dong et al. ([2022](#bib.bib126)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 ISIC 2018 | residual con. skip con. Transformer feature fusion | CE Dice
    | 74.55% | ✗ | - | ✗ | ✗ |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. ([2022](#bib.bib89)) | peer-reviewed journal | ISIC 2017 PH²
    | skip con. attention mod. recurrent net. | CE | 80.36% | ✓ | flipping, rotation
    affine trans. masking, mesh distortion | ✗ | ✗ |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
- en: '| Kaur et al. ([2022b](#bib.bib218)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 ISIC 2018 PH² | dilated conv. | CE | 81.7% | ✓ | scaling, rotation translation
    | ✗ | ✗ |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
- en: '| Badshah and Ahmad ([2022](#bib.bib31)) | peer-reviewed journal | ISIC 2018
    | residual con. BConvLSTM | - | 94.5% | ✗ | - | ✗ | ✗ |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
- en: '| Alam et al. ([2022](#bib.bib20)) | peer-reviewed journal | HAM10000 | residual
    con. separable conv. | Dice | 91.1% | ✗ | - | ✗ | ✓ |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
- en: '| Yu et al. ([2022](#bib.bib429)) | peer-reviewed journal | ISIC 2018 | skip
    con. attention mod. multi-scale | - | 87.89% | ✗ | - | ✗ | ✗ |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. ([2022](#bib.bib204)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 | skip con. attention mod. ConvLSTM | CE Jaccard | 80.5% | ✗ | - | ✗ | ✗
    |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
- en: '| Ramadan et al. ([2022](#bib.bib317)) | peer-reviewed journal | ISIC 2018
    | skip con. attention mod. | CE Dice sens.-spec. loss | 91.4% | ✗ | - | ✗ | ✗
    |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([2022b](#bib.bib447)) | peer-reviewed journal | ISIC 2017 ISIC
    2018 | skip con. dense con. semi-supervised | CE contrastive loss | 73.89% | ✗
    | scaling, flipping color distortion | ✗ | ✗ |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
- en: '| Tran and Pham ([2022](#bib.bib379)) | peer-reviewed journal | ISIC 2017 PH²
    | skip con. attention mod. | Focal Tversky fuzzy loss | 79.2% | ✗ | rotation,
    zooming flipping | ✗ | ✗ |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
- en: '| Wang and Wang ([2022](#bib.bib407)) | peer-reviewed journal | ISIC 2017 |
    skip con. residual con. attention mod. | CE Jaccard | 78.28% | ✗ | rotation, zooming
    resizing, shifting | ✗ | ✗ |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. ([2022b](#bib.bib452)) | peer-reviewed conference | ISIC 2017
    | skip con. self-supervised | CE Dice | 67.08%* | ✗ | - | ✗ | ✗ |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2022c](#bib.bib409)) | peer-reviewed conference | PH² | few
    shot mask avg. pooling | Dice | 86.97%* | ✗ | - | ✗ | ✗ |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2022a](#bib.bib398)) | peer-reviewed conference | ISIC 2017
    ISIC 2018 | residual con. dilated conv. multi-scale feature fusion Transformer
    | CE Jaccard | 78.76% | ✗ | - | ✗ | ✗ |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2022b](#bib.bib262)) | peer-reviewed conference | ISIC 2017
    ISIC 2018 | skip con. dilated conv. multi-scale pyramid pooling Transformer |
    CE | 80.19% | ✗ | - | ✗ | ✗ |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
- en: '| Gu et al. ([2022](#bib.bib161)) | peer-reviewed journal | ISIC 2017 | skip
    con. global adaptive pooling | CE $\ell_{2}$ | 80.53% | ✗ | scaling, rotation
    flipping | ✗ | ✗ |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
- en: '| Khan et al. ([2022](#bib.bib225)) | peer-reviewed journal | ISIC 2017 PH²
    | residual con. attention mod. ensemble | CE | 79.2% | ✗ | - | ✗ | ✗ |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
- en: '| Alahmadi and Alghamdi ([2022](#bib.bib19)) | peer-reviewed journal | ISIC
    2017 ISIC 2018 PH² | skip con. feature fusion semi-supervised Transformer | CE
    Dice $\ell_{2}$ | 82.78%* | ✗ | - | ✗ | ✗ |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2022](#bib.bib252)) | peer-reviewed journal | ISIC 2018 | skip
    con. residual con. dilated conv. attention mod. pyramid pooling multi-scale |
    CE Dice | 88.92% | ✗ | flipping, rotation | ✗ | ✗ |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
- en: '| Kaur et al. ([2022a](#bib.bib217)) | peer-reviewed journal | ISIC 2016 ISIC
    2017 ISIC 2018 PH² | - | Tversky | 77.8% | ✓ | rotation, scaling | ✗ | ✗ |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
- en: References
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abbas et al. (2011) Abbas, Q., Celebi, M.E., Garcia, I.F., 2011. Hair Removal
    Methods: A Comparative Study for Dermoscopy Images. Biomedical Signal Processing
    and Control 6, 395–404.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abbasi et al. (2004) Abbasi, N.R., Shaw, H.M., Rigel, D.S., Friedman, R.J.,
    McCarthy, W.H., Osman, I., Kopf, A.W., Polsky, D., 2004. Early diagnosis of cutaneous
    melanoma: Revisiting the ABCD criteria. Jama 292, 2771–2776.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abdelhalim et al. (2021) Abdelhalim, I.S.A., Mohamed, M.F., Mahdy, Y.B., 2021.
    Data Augmentation For Skin Lesion Using Self-Attention Based Progressive Generative
    Adversarial Network. Expert Systems with Applications 165, 113922.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abhishek (2020) Abhishek, K., 2020. Input Space Augmentation for Skin Lesion
    Segmentation in Dermoscopic Images. Master’s thesis. Applied Sciences: School
    of Computing Science, Simon Fraser University. [https://summit.sfu.ca/item/20247](https://summit.sfu.ca/item/20247).'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abhishek and Hamarneh (2019) Abhishek, K., Hamarneh, G., 2019. Mask2Lesion:
    Mask-constrained adversarial skin lesion image synthesis, in: International Workshop
    on Simulation and Synthesis in Medical Imaging, Springer. pp. 71–80.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abhishek and Hamarneh (2021) Abhishek, K., Hamarneh, G., 2021. Matthews correlation
    coefficient loss for deep convolutional networks: Application to skin lesion segmentation,
    in: 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), IEEE.
    pp. 225–229.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abhishek et al. (2020) Abhishek, K., Hamarneh, G., Drew, M.S., 2020. Illumination-based
    transformations improve skin lesion segmentation in dermoscopic images, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,
    pp. 728–729.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abhishek et al. (2021) Abhishek, K., Kawahara, J., Hamarneh, G., 2021. Predicting
    the clinical management of skin lesions using deep learning. Scientific reports
    11, 1–14.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abraham and Khan (2019) Abraham, N., Khan, N.M., 2019. A novel focal tversky
    loss function with improved attention U-Net for lesion segmentation, in: 2019
    IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), IEEE. pp.
    683–687.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adegun and Viriri (2019) Adegun, A., Viriri, S., 2019. An enhanced deep learning
    framework for skin lesions segmentation, in: International conference on computational
    collective intelligence, Springer. pp. 414–425.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adegun and Viriri (2020a) Adegun, A., Viriri, S., 2020a. Deep learning techniques
    for skin lesion analysis and melanoma cancer detection: a survey of state-of-the-art.
    Artificial Intelligence Review , 1–31URL: [https://doi.org/10.1007/s10462-020-09865-y](https://doi.org/10.1007/s10462-020-09865-y),
    doi:[10.1007/s10462-020-09865-y](http://dx.doi.org/10.1007/s10462-020-09865-y).'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adegun and Viriri (2020b) Adegun, A.A., Viriri, S., 2020b. Fcn-based densenet
    framework for automated detection and classification of skin lesions in dermoscopy
    images. IEEE Access 8, 150377–150396.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmedt-Aristizabal et al. (2023) Ahmedt-Aristizabal, D., Nguyen, C., Tychsen-Smith,
    L., Stacey, A., Li, S., Pathikulangara, J., Petersson, L., Wang, D., 2023. Monitoring
    of pigmented skin lesions using 3D whole body imaging. Computer Methods and Programs
    in Biomedicine 232, 107451.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ahn et al. (2021) Ahn, E., Feng, D., Kim, J., 2021. A spatial guided self-supervised
    clustering network for medical image segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 379–388.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-Masni et al. (2018) Al-Masni, M.A., Al-antari, M.A., Choi, M.T., Han, S.M.,
    Kim, T.S., 2018. Skin lesion segmentation in dermoscopy images via deep full resolution
    convolutional networks. Computer methods and programs in biomedicine 162, 221–231.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Al-masni et al. (2019) Al-masni, M.A., Al-antari, M.A., Park, H.M., Park, N.H.,
    Kim, T.S., 2019. A deep learning model integrating FrCN and residual convolutional
    networks for skin lesion segmentation and classification, in: 2019 IEEE Eurasia
    Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS),
    IEEE. pp. 95–98.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-Masni et al. (2020) Al-Masni, M.A., Kim, D.H., Kim, T.S., 2020. Multiple
    skin lesions diagnostics via integrated deep convolutional networks for segmentation
    and classification. Computer methods and programs in biomedicine 190, 105351.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Al Nazi and Abir (2020) Al Nazi, Z., Abir, T.A., 2020. Automatic skin lesion
    segmentation and melanoma detection: Transfer learning approach with U-Net and
    DCNN-SVM, in: Proceedings of International Joint Conference on Computational Intelligence,
    Springer. pp. 371–381.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alahmadi and Alghamdi (2022) Alahmadi, M.D., Alghamdi, W., 2022. Semi-supervised
    skin lesion segmentation with coupling cnn and transformer features. IEEE Access
    10, 122560–122569.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alam et al. (2022) Alam, M.J., Mohammad, M.S., Hossain, M.A.F., Showmik, I.A.,
    Raihan, M.S., Ahmed, S., Mahmud, T.I., 2022. S2C-DeLeNet: A parameter transfer
    based segmentation-classification integration for detecting skin cancer lesions
    from dermoscopic images. Computers in Biology and Medicine 150, 106148.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alom et al. (2020) Alom, M.Z., Aspiras, T., Taha, T.M., Asari, V.K., 2020.
    Skin cancer segmentation and classification with improved deep convolutional neural
    network, in: Proceedings of SPIE Medical Imaging 2020: Imaging Informatics for
    Healthcare, Research, and Applications, p. 1131814.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alom et al. (2019) Alom, M.Z., Yakopcic, C., Hasan, M., Taha, T.M., Asari, V.K.,
    2019. Recurrent residual u-net for medical image segmentation. Journal of Medical
    Imaging 6, 014006.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: American Cancer Society (2023) American Cancer Society, 2023. Cancer facts and
    figures 2023. [https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/annual-cancer-facts-and-figures/2023/2023-cancer-facts-and-figures.pdf](https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/annual-cancer-facts-and-figures/2023/2023-cancer-facts-and-figures.pdf).
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andrade et al. (2020) Andrade, C., Teixeira, L.F., Vasconcelos, M.J.M., Rosado,
    L., 2020. Data augmentation using adversarial image-to-image translation for the
    segmentation of mobile-acquired dermatological images. Journal of Imaging 7, 2.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Argenziano et al. (2000) Argenziano, G., Soyer, H.P., De Giorgio, V., Piccolo,
    D., Carli, P., Delfino, M., Ferrari, A., Hofmann-Wellenhof, R., Massi, D., Mazzocchetti,
    G., Scalvenzi, M., Wolf, I.H., 2000. Interactive Atlas of Dermoscopy. Edra Medical
    Publishing and New Media.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arora et al. (2021) Arora, R., Raman, B., Nayyar, K., Awasthi, R., 2021. Automated
    skin lesion segmentation using attention-based deep convolutional neural network.
    Biomedical Signal Processing and Control 65, 102358.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Asgari Taghanaki et al. (2021) Asgari Taghanaki, S., Abhishek, K., Cohen, J.P.,
    Cohen-Adad, J., Hamarneh, G., 2021. Deep semantic segmentation of natural and
    medical images: a review. Artificial Intelligence Review 54, 137–178.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attia et al. (2017) Attia, M., Hossny, M., Nahavandi, S., Yazdabadi, A., 2017.
    Skin melanoma segmentation using recurrent and convolutional neural networks,
    in: 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),
    IEEE. pp. 292–296.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Azad et al. (2019) Azad, R., Asadi-Aghbolaghi, M., Fathy, M., Escalera, S.,
    2019. Bi-directional ConvLSTM U-Net with densley connected convolutions, in: Proceedings
    of the IEEE International Conference on Computer Vision Workshops, pp. 0–0.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Azad et al. (2020) Azad, R., Asadi-Aghbolaghi, M., Fathy, M., Escalera, S.,
    2020. Attention deeplabv3+: Multi-level context attention mechanism for skin lesion
    segmentation, in: European Conference on Computer Vision Workshops, Springer.
    pp. 251–266.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Badshah and Ahmad (2022) Badshah, N., Ahmad, A., 2022. ResBCU-Net: Deep learning
    approach for segmentation of skin images. Biomedical Signal Processing and Control
    71, 103137.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagheri et al. (2020) Bagheri, F., Tarokh, M.J., Ziaratban, M., 2020. Two-stage
    skin lesion segmentation from dermoscopic images by using deep neural networks.
    Jorjani Biomedicine Journal 8, 58–72.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagheri et al. (2021a) Bagheri, F., Tarokh, M.J., Ziaratban, M., 2021a. Skin
    lesion segmentation based on mask rcnn, multi atrous full-cnn, and a geodesic
    method. International Journal of Imaging Systems and Technology .
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagheri et al. (2021b) Bagheri, F., Tarokh, M.J., Ziaratban, M., 2021b. Skin
    lesion segmentation from dermoscopic images by using mask r-cnn, retina-deeplab,
    and graph-based methods. Biomedical Signal Processing and Control 67, 102533.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baghersalimi et al. (2019) Baghersalimi, S., Bozorgtabar, B., Schmid-Saugeon,
    P., Ekenel, H.K., Thiran, J.P., 2019. DermoNet: densely linked convolutional neural
    network for efficient skin lesion segmentation. EURASIP Journal on Image and Video
    Processing 2019, 71.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baldi et al. (2000) Baldi, P., Brunak, S., Chauvin, Y., Andersen, C.A., Nielsen,
    H., 2000. Assessing the Accuracy of Prediction Algorithms for Classification:
    An Overview. Bioinformatics 16, 412–424.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ballerini et al. (2013) Ballerini, L., Fisher, R.B., Aldridge, B., Rees, J.,
    2013. A color and texture based hierarchical k-nn approach to the classification
    of non-melanoma skin lesions, in: Celebi, M.E., Schaefer, G. (Eds.), Color Medical
    Image Analysis. Springer, pp. 63–86.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barata et al. (2015a) Barata, C., Celebi, M.E., Marques, J.S., 2015a. Improving
    Dermoscopy Image Classification Using Color Constancy. IEEE Journal of Biomedical
    and Health Informatics 19, 1146–1152.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barata et al. (2015b) Barata, C., Celebi, M.E., Marques, J.S., 2015b. Toward
    a Robust Analysis of Dermoscopy Images Acquired Under Different Conditions, in:
    Celebi, M.E., Mendonca, T., Marques, J.S. (Eds.), Dermoscopy Image Analysis. CRC
    Press, pp. 1–22.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barata et al. (2019) Barata, C., Celebi, M.E., Marques, J.S., 2019. A Survey
    of Feature Extraction in Dermoscopy Image Analysis of Skin Cancer. IEEE Journal
    of Biomedical and Health Informatics 23, 1096–1109.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barata et al. (2014) Barata, C., Ruela, M., Francisco, M., Mendonca, T., Marques,
    J.S., 2014. Two Systems for the Detection of Melanomas In Dermoscopy Images Using
    Texture and Color Features. IEEE Systems Journal 8, 965–979.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Basak et al. (2022) Basak, H., Kundu, R., Sarkar, R., 2022. MFSNet: A multi
    focus segmentation network for skin lesion segmentation. Pattern Recognition 128,
    108673.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baur et al. (2018) Baur, C., Albarqouni, S., Navab, N., 2018. Generating Highly
    Realistic Images of Skin Lesions with GANs, in: Proceedings of the Third ISIC
    Workshop on Skin Image Analysis, pp. 260–267.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bearman et al. (2016) Bearman, A., Russakovsky, O., Ferrari, V., Fei-Fei, L.,
    2016. What’s the point: Semantic segmentation with point supervision, in: European
    Conference on Computer Vision, Springer. pp. 549–565.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio (2012) Bengio, Y., 2012. Practical Recommendations for Gradient-Based
    Training of Deep Architectures, in: Montavon, G., Orr, G., Muller, K.R. (Eds.),
    Neural networks: Tricks of the Trade. Second ed.. Springer, pp. 437–478.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio et al. (2013) Bengio, Y., Courville, A., Vincent, P., 2013. Representation
    Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis
    and Machine Intelligence 35, 1798–1828.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bevan and Atapour-Abarghouei (2022) Bevan, P.J., Atapour-Abarghouei, A., 2022.
    Detecting melanoma fairly: Skin tone detection and debiasing for skin lesion classification.
    arXiv preprint arXiv:2202.02832 .'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bi et al. (2019a) Bi, L., Feng, D., Fulham, M., Kim, J., 2019a. Improving skin
    lesion segmentation via stacked adversarial learning, in: 2019 IEEE 16th International
    Symposium on Biomedical Imaging (ISBI 2019), IEEE. pp. 1100–1103.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi et al. (2018) Bi, L., Feng, D., Kim, J., 2018. Improving automatic skin lesion
    segmentation using adversarial learning based data augmentation. arXiv preprint
    arXiv:1807.08392 .
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi et al. (2022) Bi, L., Fulham, M., Kim, J., 2022. Hyper-fusion network for
    semi-automatic segmentation of skin lesions. Medical Image Analysis 76, 102334.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bi et al. (2017a) Bi, L., Kim, J., Ahn, E., Feng, D., Fulham, M., 2017a. Semi-automatic
    skin lesion segmentation via fully convolutional networks, in: 2017 IEEE 14th
    International Symposium on Biomedical Imaging (ISBI 2017), IEEE. pp. 561–564.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi et al. (2019b) Bi, L., Kim, J., Ahn, E., Kumar, A., Feng, D., Fulham, M.,
    2019b. Step-wise integration of deep class-specific learning for dermoscopic image
    segmentation. Pattern recognition 85, 78–89.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi et al. (2017b) Bi, L., Kim, J., Ahn, E., Kumar, A., Fulham, M., Feng, D.,
    2017b. Dermoscopic image segmentation via multistage fully convolutional networks.
    IEEE Transactions on Biomedical Engineering 64, 2065–2074.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biancardi et al. (2010) Biancardi, A.M., Jirapatnakul, A.C., Reeves, A.P., 2010.
    A Comparison of Ground Truth Estimation Methods. International Journal of Computer
    Assisted Radiology and Surgery 5, 295–305.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binder et al. (1995) Binder, M., Schwarz, M., Winkler, A., Steiner, A., Kaider,
    A., Wolff, K., Pehamberger, H., 1995. Epiluminescence microscopy. a useful tool
    for the diagnosis of pigmented skin lesions for formally trained dermatologists.
    Archives of Dermatology 131, 286–291.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binney et al. (2021) Binney, N., Hyde, C., Bossuyt, P.M., 2021. On the origin
    of sensitivity and specificity. Annals of Internal Medicine 174, 401–407.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Birkenfeld et al. (2020) Birkenfeld, J.S., Tucker-Schwartz, J.M., Soenksen,
    L.R., Avilés-Izquierdo, J.A., Marti-Fuster, B., 2020. Computer-aided classification
    of suspicious pigmented lesions using wide-field images. Computer methods and
    programs in biomedicine 195, 105631.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bissoto et al. (2022) Bissoto, A., Barata, C., Valle, E., Avila, S., 2022. Artifact-based
    domain generalization of skin lesion models. arXiv preprint arXiv:2208.09756 .
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bissoto et al. (2019) Bissoto, A., Fornaciali, M., Valle, E., Avila, S., 2019.
    (de)constructing bias on skin lesion datasets, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 0–0.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bissoto et al. (2018) Bissoto, A., Perez, F., Valle, E., Avila, S., 2018. Skin
    lesion synthesis with generative adversarial networks, in: OR 2.0 context-aware
    operating theaters, computer assisted robotic endoscopy, clinical image-based
    procedures, and skin image analysis, pp. 294–302.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bissoto et al. (2021) Bissoto, A., Valle, E., Avila, S., 2021. Gan-based data
    augmentation and anonymization for skin-lesion analysis: A critical review, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) Workshops, pp. 1847–1856.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bogo et al. (2015) Bogo, F., Peruch, F., Fortina, A.B., Peserico, E., 2015.
    Where’s the Lesion? Variability in Human and Automated Segmentation of Dermoscopy
    Images of Melanocytic Skin Lesions, in: Celebi, M.E., Mendonca, T., Marques, J.S.
    (Eds.), Dermoscopy Image Analysis. CRC Press, pp. 67–95.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bogo et al. (2014) Bogo, F., Romero, J., Peserico, E., Black, M.J., 2014. Automated
    detection of new or evolving melanocytic lesions using a 3D body model, in: International
    Conference on Medical Image Computing and Computer Assisted Intervention, pp.
    593–600.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boughorbel et al. (2017) Boughorbel, S., Jarray, F., El-Anbari, M., 2017. Optimal
    Classifier for Imbalanced Data Using Matthews Correlation Coefficient Metric.
    PLOS One 12, e0177678.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bozorgtabar et al. (2017a) Bozorgtabar, B., Ge, Z., Chakravorty, R., Abedini,
    M., Demyanov, S., Garnavi, R., 2017a. Investigating deep side layers for skin
    lesion segmentation, in: 2017 IEEE 14th International Symposium on Biomedical
    Imaging (ISBI 2017), IEEE. pp. 256–260.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bozorgtabar et al. (2017b) Bozorgtabar, B., Sedai, S., Roy, P.K., Garnavi, R.,
    2017b. Skin lesion segmentation using deep convolution networks guided by local
    unsupervised learning. IBM Journal of Research and Development 61, 6–1.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Busin et al. (2008) Busin, L., Vandenbroucke, N., Macaire, L., 2008. Color
    Spaces and Image Segmentation, in: Hawkes, P.W. (Ed.), Advances in Imaging and
    Electron Physics. Academic Press. volume 151, pp. 65–168.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Buslaev et al. (2020) Buslaev, A., Iglovikov, V.I., Khvedchenya, E., Parinov,
    A., Druzhinin, M., Kalinin, A.A., 2020. Albumentations: Fast and Flexible Image
    Augmentations. Information 11, 125.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caffery et al. (2018) Caffery, L.J., Clunie, D., Curiel-Lewandrowski, C., Malvehy,
    J., Soyer, H.P., Halpern, A.C., 2018. Transforming Dermatologic Imaging for the
    Digital Era: Metadata and Standards. Journal of Digital Imaging 31, pages568–577.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Canalini et al. (2019) Canalini, L., Pollastri, F., Bolelli, F., Cancilla,
    M., Allegretti, S., Grana, C., 2019. Skin lesion segmentation ensemble with diverse
    training strategies, in: International Conference on Computer Analysis of Images
    and Patterns, Springer. pp. 89–101.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2021) Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian,
    Q., Wang, M., 2021. Swin-Unet: Unet-like pure transformer for medical image segmentation.
    arXiv preprint arXiv:2105.05537 .'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cassidy et al. (2022) Cassidy, B., Kendrick, C., Brodzicki, A., Jaworek-Korjakowska,
    J., Yap, M.H., 2022. Analysis of the ISIC image datasets: Usage, benchmarks and
    recommendations. Medical Image Analysis 75, 102305.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celebi et al. (2007a) Celebi, M.E., Aslandogan, A., Stoecker, W.V., 2007a. Unsupervised
    Border Detection in Dermoscopy Images. Skin Research and Technology 13, 454–462.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Celebi et al. (2019) Celebi, M.E., Codella, N., Halpern, A., 2019. Dermoscopy
    Image Analysis: Overview and Future Directions. IEEE Journal of Biomedical and
    Health Informatics 23, 474–478.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celebi et al. (2009a) Celebi, M.E., Iyatomi, H., Schaefer, G., Stoecker, W.V.,
    2009a. Approximate Lesion Localization in Dermoscopy Images. Skin Research and
    Technology 15, 314–322.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celebi et al. (2009b) Celebi, M.E., Iyatomi, H., Schaefer, G., Stoecker, W.V.,
    2009b. Lesion Border Detection in Dermoscopy Images. Computerized Medical Imaging
    and Graphics 33, 148–153.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celebi et al. (2008) Celebi, M.E., Iyatomi, H., Stoecker, W.V., Moss, R.H.,
    Rabinovitz, H.S., Argenziano, G., Soyer, H.P., 2008. Automatic Detection of Blue-White
    Veil and Related Structures in Dermoscopy Images. Computerized Medical Imaging
    and Graphics 32, 670–677.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celebi et al. (2007b) Celebi, M.E., Kingravi, H., Uddin, B., Iyatomi, H., Aslandogan,
    A., Stoecker, W.V., Moss, R.H., 2007b. A Methodological Approach to the Classification
    of Dermoscopy Images. Computerized Medical Imaging and Graphics 31, 362–373.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celebi et al. (2015a) Celebi, M.E., Mendonca, T., Marques, J.S. (Eds.), 2015a.
    Dermoscopy Image Analysis. CRC Press.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celebi et al. (2009c) Celebi, M.E., Schaefer, G., Iyatomi, H., Stoecker, W.V.,
    Malters, J.M., Grichnik, J.M., 2009c. An Improved Objective Evaluation Measure
    for Border Detection in Dermoscopy Images. Skin Research and Technology 15, 444–450.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celebi et al. (2013) Celebi, M.E., Wen, Q., Hwang, S., Iyatomi, H., Schaefer,
    G., 2013. Lesion Border Detection in Dermoscopy Images Using Ensembles of Thresholding
    Methods. Skin Research and Technology 19, e252–e258.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Celebi et al. (2015b) Celebi, M.E., Wen, Q., Iyatomi, H., Shimizu, K., Zhou,
    H., Schaefer, G., 2015b. A State-of-the-Art Survey on Lesion Border Detection
    in Dermoscopy Images, in: Celebi, M.E., Mendonca, T., Marques, J.S. (Eds.), Dermoscopy
    Image Analysis. CRC Press, pp. 97–129.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chabrier et al. (2006) Chabrier, S., Emile, B., Rosenberger, C., Laurent, H.,
    2006. Unsupervised Performance Evaluation of Image Segmentation. EURASIP Journal
    on Advances in Signal Processing 2006, 1–12.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chalana and Kim (1997) Chalana, V., Kim, Y., 1997. A Methodology for Evaluation
    of Boundary Detection Algorithms on Medical Images. IEEE Transactions on Medical
    Imaging 16, 642–652.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y.,
    Lu, L., Yuille, A.L., Zhou, Y., 2021. TransUNet: Transformers make strong encoders
    for medical image segmentation. arXiv preprint arXiv:2102.04306 .'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2017a) Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille,
    A.L., 2017a. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,
    Atrous Convolution, and Fully Connected CRFs. IEEE Transactions on Pattern Analysis
    and Machine Intelligence 40, 834–848.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2017b) Chen, L.C., Papandreou, G., Schroff, F., Adam, H., 2017b.
    Rethinking atrous convolution for semantic image segmentation. arXiv preprint
    arXiv:1706.05587 .
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018a) Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam,
    H., 2018a. Encoder-decoder with atrous separable convolution for semantic image
    segmentation, in: Proceedings of the European conference on computer vision (ECCV),
    pp. 801–818.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022) Chen, P., Huang, S., Yue, Q., 2022. Skin lesion segmentation
    using recurrent attentional convolutional networks. IEEE Access 10, 94007–94018.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018b) Chen, S., Wang, Z., Shi, J., Liu, B., Yu, N., 2018b. A
    multi-task framework with feature passing module for skin lesion classification
    and segmentation, in: 2018 IEEE 15th international symposium on biomedical imaging
    (ISBI 2018), IEEE. pp. 1126–1129.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Parent (1989) Chen, S.E., Parent, R.E., 1989. Shape Averaging and its
    Applications to Industrial Design. IEEE Computer Graphics and Applications 9,
    47–54.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chicco and Jurman (2020) Chicco, D., Jurman, G., 2020. The Advantages of the
    Matthews Correlation Coefficient (MCC) over F1 Score and Accuracy in Binary Classification
    Evaluation. BMC Genomics 21.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet (2017) Chollet, F., 2017. Xception: Deep learning with depthwise separable
    convolutions, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 1251–1258.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chou and Fasman (1978) Chou, P.Y., Fasman, G.D., 1978. Prediction of the Secondary
    Structure of Proteins from Their Amino Acid Sequence, in: Meister, A. (Ed.), Advances
    in Enzymology and Related Areas of Molecular Biology. John Wiley & Sons. volume 47,
    pp. 45–148.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Codella et al. (2015) Codella, N., Cai, J., Abedini, M., Garnavi, R., Halpern,
    A., Smith, J.R., 2015. Deep Learning, Sparse Coding, and SVM for Melanoma Recognition
    in Dermoscopy Images, in: Proceedings of the International Workshop on Machine
    Learning in Medical Imaging, pp. 118–126.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Codella et al. (2019) Codella, N., Rotemberg, V., Tschandl, P., Celebi, M.E.,
    Dusza, S., Gutman, D., Helba, B., Kalloo, A., Liopyris, K., Marchetti, M., Kittler,
    H., Halpern, A., 2019. Skin Lesion Analysis Toward Melanoma Detection 2018: A
    Challenge Hosted by the International Skin Imaging Collaboration (ISIC). [https://arxiv.org/abs/1902.03368](https://arxiv.org/abs/1902.03368).'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Codella et al. (2017) Codella, N.C., Nguyen, Q.B., Pankanti, S., Gutman, D.A.,
    Helba, B., Halpern, A.C., Smith, J.R., 2017. Deep Learning Ensembles for Melanoma
    Recognition in Dermoscopy Images. IBM Journal of Research and Development 61,
    5:1–5:15.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Codella et al. (2018) Codella, N.C.F., Gutman, D., Celebi, M.E., Helba, B.,
    Marchetti, M.A., Dusza, S.W., Kalloo, A., Liopyris, K., Mishra, N., Kittler, H.,
    Halpern, A., 2018. Skin Lesion Analysis Toward Melanoma Detection: A Challenge
    at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the
    International Skin Imaging Collaboration (ISIC), in: Proceedings of the 2018 IEEE
    International Symposium on Biomedical Imaging (ISBI 2018), pp. 168–172.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen (1960) Cohen, J., 1960. A Coefficient of Agreement for Nominal Scales.
    Educational and Psychological Measurement 20, 37–46.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Colliot et al. (2022) Colliot, O., Thibeau-Sutre, E., Burgos, N., 2022. Reproducibility
    in machine learning for medical imaging. arXiv preprint arXiv:2209.05097 .
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combalia et al. (2019) Combalia, M., Codella, N.C., Rotemberg, V., Helba, B.,
    Vilaplana, V., Reiter, O., Carrera, C., Barreiro, A., Halpern, A.C., Puig, S.,
    Malvehy, J., 2019. BCN20000: Dermoscopic lesions in the wild. arXiv preprint arXiv:1908.02288
    .'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cordonnier et al. (2019) Cordonnier, J.B., Loukas, A., Jaggi, M., 2019. On the
    relationship between self-attention and convolutional layers. arXiv preprint arXiv:1911.03584
    .
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creswell et al. (2018) Creswell, A., White, T., Dumoulin, V., Arulkumaran,
    K., Sengupta, B., Bharath, A.A., 2018. Generative Adversarial Networks: An Overview.
    IEEE Signal Processing Magazine 35, 53–65.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crum et al. (2006) Crum, W.R., Camara, O., Hill, D.L., 2006. Generalized Overlap
    Measures for Evaluation and Validation in Medical Image Analysis. IEEE Transactions
    on Medical Imaging 25, 1451–1461.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cui et al. (2019) Cui, Z., Wu, L., Wang, R., Zheng, W.S., 2019. Ensemble transductive
    learning for skin lesion segmentation, in: Chinese Conference on Pattern Recognition
    and Computer Vision (PRCV), Springer. pp. 572–581.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Curiel-Lewandrowski et al. (2019) Curiel-Lewandrowski, C., Novoa, R.A., Berry,
    E., Celebi, M.E., Codella, N., Giuste, F., Gutman, D., Halpern, A., Leachman,
    S., Liu, Y., Liu, Y., Reiter, O., Tschandl, P., 2019. Artificial Intelligence
    Approach in Melanoma, in: Fisher, D.E., Bastian, B.C. (Eds.), Melanoma. Spriner,
    pp. 599–628.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2022) Dai, D., Dong, C., Xu, S., Yan, Q., Li, Z., Zhang, C., Luo,
    N., 2022. Ms red: A novel multi-scale residual encoding and decoding network for
    skin lesion segmentation. Medical Image Analysis 75, 102293.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2015) Dai, J., He, K., Sun, J., 2015. BoxSup: Exploiting Bounding
    Boxes to Supervise Convolutional Networks for Semantic Segmentation, in: Proceedings
    of the IEEE International Conference on Computer Vision, pp. 1635–1643.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daneshjou et al. (2022) Daneshjou, R., Barata, C., Betz-Stablein, B., Celebi,
    M.E., Codella, N., Combalia, M., Guitera, P., Gutman, D., Halpern, A., Helba,
    B., Kittler, H., Kose, K., Liopyris, K., Malvehy, J., Seog, H.S., Soyer, H.P.,
    Tkaczyk, E.R., Tschandl, P., Rotemberg, V., 2022. Evaluation of Image-Based AI
    Artificial Intelligence Reports in Dermatology: CLEAR Derm Consensus Guidelines
    from the International Skin Imaging Collaboration Artificial Intelligence Working
    Group. JAMA Dermatology 158, 90–96.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daneshjou et al. (2021a) Daneshjou, R., Smith, M.P., Sun, M.D., Rotemberg,
    V., Zou, J., 2021a. Lack of transparency and potential bias in artificial intelligence
    data sets and algorithms: A scoping review. JAMA Dermatology 157, 1362–1369.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daneshjou et al. (2021b) Daneshjou, R., Vodrahalli, K., Liang, W., Novoa, R.A.,
    Jenkins, M., Rotemberg, V., Ko, J., Swetter, S.M., Bailey, E.E., Gevaert, O.,
    Mukherjee, P., Phung, M., Yekrang, K., Fong, B., Sahasrabudhe, R., Zou, J., Chiou,
    A., 2021b. Disparities in dermatology AI: Assessments using diverse clinical images.
    arXiv preprint arXiv:2111.08006 .'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De Angelo et al. (2019) De Angelo, G.G., Pacheco, A.G., Krohling, R.A., 2019.
    Skin lesion segmentation using deep learning for images acquired from smartphones,
    in: 2019 International Joint Conference on Neural Networks (IJCNN), IEEE. pp.
    1–8.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei,
    L., 2009. ImageNet: A large-scale hierarchical image database, in: 2009 IEEE Conference
    on Computer Vision and Pattern Recognition, IEEE. pp. 248–255.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2017) Deng, Z., Fan, H., Xie, F., Cui, Y., Liu, J., 2017. Segmentation
    of dermoscopy images based on fully convolutional neural network, in: 2017 IEEE
    International Conference on Image Processing (ICIP), IEEE. pp. 1732–1736.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2020) Deng, Z., Xin, Y., Qiu, X., Chen, Y., 2020. Weakly and semi-supervised
    deep level set network for automated skin lesion segmentation, in: Innovation
    in Medicine and Healthcare. Springer, pp. 145–155.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Denton et al. (2015) Denton, E., Chintala, S., Szlam, A., Fergus, R., 2015.
    Deep generative image models using a laplacian pyramid of adversarial networks,
    in: Proceedings of the 28th International Conference on Neural Information Processing
    Systems-Volume 1, pp. 1486–1494.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depeweg et al. (2018) Depeweg, S., Hernandez-Lobato, J.M., Doshi-Velez, F.,
    Udluft, S., 2018. Decomposition of uncertainty in bayesian deep learning for efficient
    and risk-sensitive learning, in: International Conference on Machine Learning,
    PMLR. pp. 1184–1193.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Der Kiureghian and Ditlevsen (2009) Der Kiureghian, A., Ditlevsen, O., 2009.
    Aleatory or epistemic? does it matter? Structural safety 31, 105–112.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DermIS (2012) DermIS, 2012. Dermatology Information System. [https://www.dermis.net/](https://www.dermis.net/).
    [Online. Accessed January 26, 2022].
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DermQuest (2012) DermQuest, 2012. Dermquest. [http://www.dermquest.com](http://www.dermquest.com).
    Cited: 2020-04-28.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeVries and Taylor (2018) DeVries, T., Taylor, G.W., 2018. Leveraging uncertainty
    estimates for predicting segmentation quality. arXiv preprint arXiv:1807.00502
    .
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dhawan et al. (1984) Dhawan, A.P., Gordon, R., , Rangayyan, R.M., 1984. Nevoscopy:
    Three-dimensional computed tomography of nevi and melanomas in situ by transillumination.
    IEEE Transactions on Medical Imaging 3, 54–61.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dice (1945) Dice, L.R., 1945. Measures of the Amount of Ecologic Association
    Between Species. Ecology 26, 297–302.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2021) Ding, S., Zheng, J., Liu, Z., Zheng, Y., Chen, Y., Xu, X.,
    Lu, J., Xie, J., 2021. High-Resolution Dermoscopy Image Synthesis with Conditional
    Generative Adversarial Networks. Biomedical Signal Processing and Control 64,
    102224.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dodge and Karam (2016) Dodge, S., Karam, L., 2016. Understanding How Image
    Quality Affects Deep Neural Networks, in: Proceedings of the 2016 International
    Conference on Quality of Multimedia Experience, pp. 1–6.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2022) Dong, Y., Wang, L., Li, Y., 2022. TC-Net: Dual coding network
    of Transformer and CNN for skin lesion segmentation. Plos one 17, e0277578.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al., 2020. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929 .'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2023) Du, S., Hers, B., Bayasi, N., Hamarneh, G., Garbi, R., 2023.
    FairDisCo: Fairer AI in dermatology via disentanglement contrastive learning,
    in: Computer Vision–ECCV 2022 Workshops: Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part IV, Springer. pp. 185–202.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ebenezer and Rajapakse (2018) Ebenezer, J.P., Rajapakse, J.C., 2018. Automatic
    segmentation of skin lesions using deep learning. arXiv preprint arXiv:1807.04893
    .
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'El Jurdi et al. (2021) El Jurdi, R., Petitjean, C., Honeine, P., Cheplygina,
    V., Abdallah, F., 2021. High-level prior-based loss functions for medical image
    segmentation: A survey. Computer Vision and Image Understanding 210, 103248.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsken et al. (2019) Elsken, T., Metzen, J.H., Hutter, F., 2019. Neural Architecture
    Search: A Survey. Journal of Machine Learning Research 20, 1–21.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'En and Guo (2022) En, Q., Guo, Y., 2022. Annotation by clicks: A point-supervised
    contrastive variance method for medical semantic segmentation. arXiv preprint
    arXiv:2212.08774 .'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engasser and Warshaw (2010) Engasser, H.C., Warshaw, E.M., 2010. Dermatoscopy
    use by us dermatologists: a cross-sectional survey. Journal of the American Academy
    of Dermatology 63, 412–419.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erkol et al. (2005) Erkol, B., Moss, R.H., Stanley, R.J., Stoecker, W.V., Hvatum,
    E., 2005. Automatic Lesion Boundary Detection in Dermoscopy Images Using Gradient
    Vector Flow Snakes. Skin Research and Technology 11, 17–26.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ferreira et al. (2012) Ferreira, P.M., Mendonca, T., Rozeira, J., Rocha, P.,
    2012. An Annotation Tool for Dermoscopic Image Segmentation, in: Proceedings of
    the 1st International Workshop on Visual Interfaces for Ground Truth Collection
    in Computer Vision Applications, pp. 1–6.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Foncubierta-Rodriguez and Muller (2012) Foncubierta-Rodriguez, A., Muller,
    H., 2012. Ground Truth Generation in Medical Imaging: A Crowdsourcing-Based Iterative
    Approach, in: Proceedings of the ACM Multimedia 2012 Workshop on Crowdsourcing
    for Multimedia, pp. 9–14.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortina et al. (2012) Fortina, A.B., Peserico, E., Silletti, A., Zattra, E.,
    2012. Where’s the Naevus? Inter-Operator Variability in the Localization of Melanocytic
    Lesion Border. Skin Research and Technology 18, 311–315.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Friedman et al. (1985) Friedman, R.J., Rigel, D.S., Kopf, A.W., 1985. Early
    detection of malignant melanoma: The role of physician examination and self-examination
    of the skin. CA: A Cancer Journal for Clinicians 35, 130–151.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2019) Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu,
    H., 2019. Dual attention network for scene segmentation, in: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 3146–3154.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gachon et al. (2005) Gachon, J., Beaulieu, P., Sei, J.F., Gouvernet, J., Claudel,
    J.P., Lemaitre, M., Richard, M.A., Grob, J.J., 2005. First prospective study of
    the recognition process of melanoma in dermatological practice. Archives of dermatology
    141, 434–438.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gal (2016) Gal, Y., 2016. Uncertainty in deep learning. Ph.D. thesis. Department
    of Engineering, University of Cambridge. [https://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf](https://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf).
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garnavi and Aldeen (2011) Garnavi, R., Aldeen, M., 2011. Optimized Weighted
    Performance Index for Objective Evaluation of Border-Detection Methods in Dermoscopy
    Images. IEEE Transactions on Information Technology in Biomedicine 15, 908–917.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garnavi et al. (2011a) Garnavi, R., Aldeen, M., Celebi, M.E., 2011a. Weighted
    Performance Index for Objective Evaluation of BorderDetection Methods in Dermoscopy
    Images. Skin Research and Technology 17, 35–44.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garnavi et al. (2011b) Garnavi, R., Aldeen, M., Celebi, M.E., Varigos, G., Finch,
    S., 2011b. Border Detection in Dermoscopy Images Using Hybrid Thresholding on
    Optimized Color Channels. Computerized Medical Imaging and Graphics 35, 105–115.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gidaris et al. (2018) Gidaris, S., Singh, P., Komodakis, N., 2018. Unsupervised
    representation learning by predicting image rotations, in: International Conference
    on Learning Representations (ICLR), pp. 1–16. URL: [https://openreview.net/forum?id=S1v4N2l0-](https://openreview.net/forum?id=S1v4N2l0-).'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Giotis et al. (2015) Giotis, I., Molders, N., Land, S., Biehl, M., Jonkman,
    M.F., Petkov, N., 2015. MED-NODE: A computer-assisted melanoma diagnosis system
    using non-dermoscopic images. Expert Systems with Applications 42, 6578–6585.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gish and Blanz (1989) Gish, S.L., Blanz, W.E., 1989. Comparing the Performance
    of Connectionist and Statistical Classifiers on an Image Segmentation Problem,
    in: Proceedings of the Second International Conference on Neural Information Processing
    Systems, pp. 614–621.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Glaister (2013) Glaister, J.L., 2013. Automatic segmentation of skin lesions
    from dermatological photographs. [https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection](https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection).
    Cited: 2022-1-31.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goel et al. (2020) Goel, S., Sharma, Y., Jauer, M.L., Deserno, T.M., 2020.
    WeLineation: Crowdsourcing Delineations for Reliable Ground Truth Estimation,
    in: Proceedings of the Medical Imaging 2020: Imaging Informatics for Healthcare,
    Research, and Applications, pp. 113180C–1–113180C–8.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gómez et al. (2007) Gómez, D.D., Butakoff, C., Ersboll, B.K., Stoecker, W.,
    2007. Independent histogram pursuit for segmentation of skin lesions. IEEE transactions
    on biomedical engineering 55, 157–161.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gonzalez-Diaz (2018) Gonzalez-Diaz, I., 2018. Dermaknet: Incorporating the
    knowledge of dermatologists to convolutional neural networks for skin lesion diagnosis.
    IEEE journal of biomedical and health informatics 23, 547–559.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2020) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2020. Generative Adversarial
    Networks. Communications of the ACM 63, 139–144.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal et al. (2019a) Goyal, M., Ng, J., Oakley, A., Yap, M.H., 2019a. Skin
    lesion boundary segmentation with fully automated deep extreme cut methods, in:
    Medical Imaging 2019: Biomedical Applications in Molecular, Structural, and Functional
    Imaging, International Society for Optics and Photonics. p. 109530Q.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goyal et al. (2019b) Goyal, M., Oakley, A., Bansal, P., Dancey, D., Yap, M.H.,
    2019b. Skin lesion segmentation in dermoscopic images with ensemble deep learning
    methods. IEEE Access 8, 4171–4181.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal et al. (2017) Goyal, M., Yap, M.H., Hassanpour, S., 2017. Multi-class
    semantic segmentation of skin lesions via fully convolutional networks, in: Proceedings
    of the 13th International Joint Conference on Biomedical Engineering Systems and
    Technologies, Comp2Clinic Workshop, pp. 290–295.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grau et al. (2004) Grau, V., Mewes, A.U.J., Alcaniz, M., Kikinis, R., Warfield,
    S.K., 2004. Improved Watershed Transform for Medical Image Segmentation Using
    Prior Information. IEEE Transactions on Medical Imaging 23, 447–458.
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Green et al. (1994) Green, A., Martin, N., Pfitzner, J., O’Rourke, M., Knight,
    N., 1994. Computer image analysis in the diagnosis of melanoma. Journal of the
    American Academy of Dermatology 31, 958–964.
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Groh et al. (2021) Groh, M., Harris, C., Soenksen, L., Lau, F., Han, R., Kim,
    A., Koochek, A., Badri, O., 2021. Evaluating deep neural networks trained on clinical
    images in dermatology with the Fitzpatrick 17k dataset, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1820–1828.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2021) Gu, P., Zheng, H., Zhang, Y., Wang, C., Chen, D.Z., 2021.
    kCBAC-Net: Deeply supervised complete bipartite networks with asymmetric convolutions
    for medical image segmentation, in: International Conference on Medical Image
    Computing and Computer-Assisted Intervention, Springer. pp. 337–347.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2020) Gu, R., Wang, G., Song, T., Huang, R., Aertsen, M., Deprest,
    J., Ourselin, S., Vercauteren, T., Zhang, S., 2020. CA-Net: Comprehensive attention
    convolutional neural networks for explainable medical image segmentation. IEEE
    transactions on medical imaging 40, 699–711.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2022) Gu, R., Wang, L., Zhang, L., 2022. DE-Net: A deep edge network
    with boundary information for automatic skin lesion segmentation. Neurocomputing
    468, 71–84.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gudhe et al. (2021) Gudhe, N.R., Behravan, H., Sudah, M., Okuma, H., Vanninen,
    R., Kosma, V.M., Mannermaa, A., 2021. Multi-level dilated residual network for
    biomedical image segmentation. Scientific Reports 11, 1–18.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guillod et al. (2002) Guillod, J., Schmid-Saugeon, P., Guggisberg, D., Cerottini,
    J.P., Braun, R., Krischer, J., Saurat, J.H., Kunt, M., 2002. Validation of Segmentation
    Techniques for Digital Dermoscopy. Skin Research and Technology 8, 240–249.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulzar and Khan (2022) Gulzar, Y., Khan, S.A., 2022. Skin lesion segmentation
    based on vision transformers and convolutional neural networks—a comparative study.
    Applied Sciences 12, 5990.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Guo, X., Chen, Z., Yuan, Y., 2020. Complementary network
    with adaptive receptive fields for melanoma segmentation, in: 2020 IEEE 17th International
    Symposium on Biomedical Imaging (ISBI), IEEE. pp. 2010–2013.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gurari et al. (2015) Gurari, D., Theriault, D., Sameki, M., Isenberg, B., Pham,
    T.A., Purwada, A., Solski, P., Walker, M., Zhang, C., Wong, J.Y., Betke, M., 2015.
    How to Collect Segmentations for Biomedical Images? A Benchmark Evaluating the
    Performance of Experts, Crowdsourced Non-Experts, and Algorithms, in: 2015 IEEE
    Winter Conference on Applications of Computer Vision, pp. 1169–1176.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gutman et al. (2016) Gutman, D., Codella, N.C.F., Celebi, M.E., Helba, B.,
    Marchetti, M., Mishra, N., Halpern, A., 2016. Skin Lesion Analysis Toward Melanoma
    Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI)
    2016, hosted by the International Skin Imaging Collaboration (ISIC). [http://arxiv.org/abs/1605.01397](http://arxiv.org/abs/1605.01397).'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guy Jr et al. (2015) Guy Jr, G.P., Machlin, S.R., Ekwueme, D.U., Yabroff, K.R.,
    2015. Prevalence and costs of skin cancer treatment in the us, 2002- 2006 and
    2007- 2011. American Journal of Preventive Medicine 48, 183–187.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Halpern (2003) Halpern, A.C., 2003. Total body skin imaging as an aid to melanoma
    detection., in: Seminars in Cutaneous Medicine and Surgery, pp. 2–8.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hance et al. (1996) Hance, G.A., Umbaugh, S.E., Moss, R.H., Stoecker, W.V.,
    1996. Unsupervised Color Image Segmentation with Application to Skin Tumor Borders.
    IEEE Engineering in Medicine and Biology Magazine 15, 104–111.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hasan et al. (2021) Hasan, M., Roy, S., Mondal, C., Alam, M., Elahi, M., Toufick,
    E., Dutta, A., Raju, S., Ahmad, M., et al., 2021. Dermo-doctor: A framework for
    concurrent skin lesion detection and recognition using a deep convolutional neural
    network with end-to-end dual encoders. Biomedical Signal Processing and Control
    68, 102661.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hasan et al. (2020) Hasan, M.K., Dahal, L., Samarakoon, P.N., Tushar, F.I.,
    Martí, R., 2020. DSNet: Automatic dermoscopic skin lesion segmentation. Computers
    in Biology and Medicine , 103738.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2022) He, K., Gan, C., Li, Z., Rekik, I., Yin, Z., Ji, W., Gao,
    Y., Wang, Q., Zhang, J., Shen, D., 2022. Transformers in medical image analysis:
    A review. Intelligent Medicine URL: [https://www.sciencedirect.com/science/article/pii/S2667102622000717](https://www.sciencedirect.com/science/article/pii/S2667102622000717),
    doi:[https://doi.org/10.1016/j.imed.2022.07.002](http://dx.doi.org/https://doi.org/10.1016/j.imed.2022.07.002).'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2016) He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
    for image recognition, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp. 770–778.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2017) He, X., Yu, Z., Wang, T., Lei, B., 2017. Skin lesion segmentation
    via deep RefineNet, in: Deep Learning in Medical Image Analysis and Multimodal
    Learning for Clinical Decision Support. Springer, pp. 303–311.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2018) He, X., Yu, Z., Wang, T., Lei, B., Shi, Y., 2018. Dense deconvolution
    net: Multi path fusion and dense deconvolution for high resolution skin lesion
    segmentation. Technology and Health Care 26, 307–316.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Henry et al. (2020) Henry, H.Y., Feng, X., Wang, Z., Sun, H., 2020. Mixmodule:
    Mixed cnn kernel module for medical image segmentation, in: 2020 IEEE 17th International
    Symposium on Biomedical Imaging (ISBI), IEEE. pp. 1508–1512.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hornung et al. (2021) Hornung, A., Steeb, T., Wessely, A., Brinker, T.J., Breakell,
    T., Erdmann, M., Berking, C., Heppt, M.V., 2021. The value of total body photography
    for the early detection of melanoma: A systematic review. International Journal
    of Environmental Research and Public Health 18, 1726.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Howard et al. (2019) Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B.,
    Tan, M., W., W., Zhu, Y., Pang, R., Vasudevan, V., Le, Q.V., Adam, H., 2019. Searching
    for MobileNetV3, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp. 1314–1324.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2019) Hu, H., Zhang, Z., Xie, Z., Lin, S., 2019. Local relation
    networks for image recognition, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 3464–3473.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2018) Hu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation networks,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 7132–7141.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2017) Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.,
    2017. Densely connected convolutional networks, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 4700–4708.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2022) Huang, W.L., Liu, S., Kang, J., Gandjbakhche, A., Armand,
    M., 2022. DICOM file for total body photography: a work item proposal, in: Photonics
    in Dermatology and Plastic Surgery 2022, SPIE. pp. 64–74.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huttenlocher et al. (1993) Huttenlocher, D.P., Klanderman, G.A., Rucklidge,
    W.J., 1993. Comparing Images Using the Hausdorff Distance. IEEE Transactions on
    Pattern Analysis and Machine Intelligence 15, 850–863.
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ISIC (2018) ISIC, 2018. ISIC Live Leaderboards: 2018.1: Lesion Boundary Segmentation.
    [https://challenge.isic-archive.com/leaderboards/live/](https://challenge.isic-archive.com/leaderboards/live/).
    [Online. Accessed January 17, 2023].'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ISIC (2023) ISIC, 2023. International Skin Imaging Collaboration: Melanoma
    Project. [https://www.isic-archive.com/](https://www.isic-archive.com/). [Online.
    Accessed January 17, 2023].'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Isola et al. (2017) Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A., 2017. Image-to-image
    translation with conditional adversarial networks, in: Proceedings of the IEEE
    conference on computer vision and pattern recognition, pp. 1125–1134.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iyatomi et al. (2008) Iyatomi, H., Oka, H., Celebi, M.E., Hashimoto, M., Hagiwara,
    M., Tanaka, M., Ogawa, K., 2008. An Improved Internet-Based Melanoma Screening
    System with Dermatologist-Like Tumor Area Extraction Algorithm. Computerized Medical
    Imaging and Graphics 32, 566–579.
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iyatomi et al. (2006) Iyatomi, H., Oka, H., Saito, M., Miyake, A., Kimoto, M.,
    Yamagami, J., Kobayashi, S., Tanikawa, A., Hagiwara, M., Ogawa, K., Argenziano,
    G., Soyer, H.P., Tanaka, M., 2006. Quantitative Assessment of Tumor Extraction
    from Dermoscopy Images and Evaluation of Computer-Based Extraction Methods for
    Automatic Melanoma Diagnostic System. Melanoma Research 16, 183–190.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Izadi et al. (2018) Izadi, S., Mirikharaji, Z., Kawahara, J., Hamarneh, G.,
    2018. Generative adversarial networks to segment skin lesions, in: 2018 IEEE 15th
    International Symposium on Biomedical Imaging (ISBI 2018), IEEE. pp. 881–884.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaccard (1901) Jaccard, P., 1901. Distribution de la Flore Alpine dans le Bassin
    des Dranses et dans Quelques Regions Voisines. Bulletin de la Societe Vaudoise
    des Sciences Naturelles 37, 241–272.
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaccard (1912) Jaccard, P., 1912. The distribution of the flora in the alpine
    zone. New Phytologist 11, 37–50.
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jafari et al. (2020) Jafari, M., Auer, D., Francis, S., Garibaldi, J., Chen,
    X., 2020. Dru-net: An efficient deep convolutional neural network for medical
    image segmentation, in: 2020 IEEE 17th International Symposium on Biomedical Imaging
    (ISBI), IEEE. pp. 1144–1148.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jafari et al. (2016) Jafari, M.H., Karimi, N., Nasr-Esfahani, E., Samavi, S.,
    Soroushmehr, S.M.R., Ward, K., Najarian, K., 2016. Skin lesion segmentation in
    clinical images using deep learning, in: 2016 23rd International conference on
    pattern recognition (ICPR), IEEE. pp. 337–342.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jafari et al. (2017) Jafari, M.H., Nasr-Esfahani, E., Karimi, N., Soroushmehr,
    S.R., Samavi, S., Najarian, K., 2017. Extraction of skin lesions from non-dermoscopic
    images for surgical excision of melanoma. International journal of computer assisted
    radiology and surgery 12, 1021–1030.
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jahanifar et al. (2018) Jahanifar, M., Tajeddin, N.Z., Koohbanani, N.A., Gooya,
    A., Rajpoot, N., 2018. Segmentation of skin lesions and their attributes using
    multi-scale convolutional neural networks and domain specific augmentations. arXiv
    preprint arXiv:1809.10243 .
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Japkowicz and Shah (2011) Japkowicz, N., Shah, M., 2011. Evaluating Learning
    Algorithms: A Classification Perspective. Cambridge University Press.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaworek-Korjakowska et al. (2021) Jaworek-Korjakowska, J., Brodzicki, A., Cassidy,
    B., Kendrick, C., Yap, M.H., 2021. Interpretability of a deep learning based approach
    for the classification of skin lesions into main anatomic body sites. Cancers
    13, 6048.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jayapriya and Jacob (2020) Jayapriya, K., Jacob, I.J., 2020. Hybrid fully convolutional
    networks-based skin lesion segmentation and melanoma detection using deep feature.
    International Journal of Imaging Systems and Technology 30, 348–357.
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jensen and Elewski (2015) Jensen, J.D., Elewski, B.E., 2015. The ABCDEF rule:
    combining the “ABCDE rule” and the “ugly duckling sign” in an effort to improve
    patient self-screening examinations. The Journal of Clinical and Aesthetic Dermatology
    8, 15.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. (2019) Ji, X., Henriques, J.F., Vedaldi, A., 2019. Invariant Information
    Clustering for Unsupervised Image Classification and Segmentation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 9865–9874.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. (2021) Ji, Y., Zhang, R., Wang, H., Li, Z., Wu, L., Zhang, S., Luo,
    P., 2021. Multi-compound Transformer for accurate biomedical image segmentation,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer. pp. 326–336.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2019) Jiang, F., Zhou, F., Qin, J., Wang, T., Lei, B., 2019.
    Decision-augmented generative adversarial network for skin lesion segmentation,
    in: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),
    IEEE. pp. 447–450.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2022) Jiang, X., Jiang, J., Wang, B., Yu, J., Wang, J., 2022.
    SEACU-Net: Attentive ConvLSTM U-Net with squeeze-and-excitation layer for skin
    lesion segmentation. Computer Methods and Programs in Biomedicine 225, 107076.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2020) Jiang, Y., Cao, S., Tao, S., Zhang, H., 2020. Skin lesion
    segmentation based on multi-scale attention convolutional neural network. IEEE
    Access 8, 122811–122825.
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2021) Jin, Q., Cui, H., Sun, C., Meng, Z., Su, R., 2021. Cascade
    knowledge diffusion network for skin lesion diagnosis and segmentation. Applied
    Soft Computing 99, 106881.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kahn (1942) Kahn, R.L., 1942. Serology in Syphilis Control: Principles of Sensitivity
    and Specificity with an Appendix for Health Officers and Industrial Physicians.
    American Journal of Clinical Pathology 12, 446–446. URL: [https://doi.org/10.1093/ajcp/12.8.446d](https://doi.org/10.1093/ajcp/12.8.446d),
    doi:[10.1093/ajcp/12.8.446d](http://dx.doi.org/10.1093/ajcp/12.8.446d), [arXiv:https://academic.oup.com/ajcp/article-pdf/12/8/446/24886161/ajcpath12-0446d.pdf](http://arxiv.org/abs/https://academic.oup.com/ajcp/article-pdf/12/8/446/24886161/ajcpath12-0446d.pdf).'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kamalakannan et al. (2019) Kamalakannan, A., Ganesan, S.S., Rajamanickam, G.,
    2019. Self-learning ai framework for skin lesion image segmentation and classification.
    International Journal of Computer Science and Information Technology 11, 29--38.
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kapoor and Narayanan (2022) Kapoor, S., Narayanan, A., 2022. Leakage and the
    reproducibility crisis in ML-based science. arXiv preprint arXiv:2207.07048 .
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karimi et al. (2020) Karimi, D., Dou, H., Warfield, S.K., Gholipour, A., 2020.
    Deep learning with noisy labels: Exploring techniques and remedies in medical
    image analysis. Medical Image Analysis 65, 101759.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karras et al. (2018) Karras, T., Aila, T., Laine, S., Lehtinen, J., 2018. Progressive
    growing of GANs for improved quality, stability, and variation, in: International
    Conference on Learning Representations, pp. 1--26. URL: [https://openreview.net/forum?id=Hk99zCeAb](https://openreview.net/forum?id=Hk99zCeAb).'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kats et al. (2019) Kats, E., Goldberger, J., Greenspan, H., 2019. A soft staple
    algorithm combined with anatomical knowledge, in: International Conference on
    Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 510--517.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Katsch et al. (2022) Katsch, F., Rinner, C., Tschandl, P., 2022. Comparison
    of convolutional neural network architectures for robustness against common artefacts
    in dermatoscopic images. Dermatology Practical & Conceptual , e2022126--e2022126.
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Katz and Merickel (1989) Katz, W.T., Merickel, M.B., 1989. Translation-Invariant
    Aorta Segmentation from Magnetic Resonance Images, in: Proceedings of the 1989
    International Joint Conference on Neural Networks, pp. 327--333.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaul et al. (2019) Kaul, C., Manandhar, S., Pears, N., 2019. FocusNet: an attention-based
    fully convolutional network for medical image segmentation, in: 2019 IEEE 16th
    International Symposium on Biomedical Imaging (ISBI 2019), IEEE. pp. 455--458.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaul et al. (2021) Kaul, C., Pears, N., Dai, H., Murray-Smith, R., Manandhar,
    S., 2021. Focusnet++: Attentive aggregated transformations for efficient and accurate
    medical image segmentation, in: 2021 IEEE 18th International Symposium on Biomedical
    Imaging (ISBI), IEEE. pp. 1042--1046.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaur et al. (2022a) Kaur, R., GholamHosseini, H., Sinha, R., 2022a. Skin lesion
    segmentation using an improved framework of encoder-decoder based convolutional
    neural network. International Journal of Imaging Systems and Technology .
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaur et al. (2022b) Kaur, R., GholamHosseini, H., Sinha, R., Lindén, M., 2022b.
    Automatic lesion segmentation using atrous convolutional deep neural networks
    in dermoscopic skin cancer images. BMC Medical Imaging 22, 1--13.
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kawahara et al. (2019) Kawahara, J., Daneshvar, S., Argenziano, G., Hamarneh,
    G., 2019. Seven-Point Checklist and Skin Lesion Classification Using Multitask
    Multimodal Neural Nets. IEEE Journal of Biomedical and Health Informatics 23,
    538--546.
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kawahara and Hamarneh (2018) Kawahara, J., Hamarneh, G., 2018. Fully convolutional
    neural networks to detect clinical dermoscopic features. IEEE journal of biomedical
    and health informatics 23, 578--585.
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaymak et al. (2020) Kaymak, R., Kaymak, C., Ucar, A., 2020. Skin lesion segmentation
    using fully convolutional networks: A comparative experimental study. Expert Systems
    with Applications 161, 113742.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kazeminia et al. (2020) Kazeminia, S., Baur, C., Kuijper, A., van Ginneken,
    B., Navab, N., Albarqouni, S., Mukhopadhyay, A., 2020. GANs for Medical Image
    Analysis. Artificial Intelligence in Medicine 109, 101938.
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kent et al. (1955) Kent, A., Berry, M.M., Luehrs Jr, F.U., Perry, J.W., 1955.
    Machine literature searching: VIII. Operational criteria for designing information
    retrieval systems. American Documentation (pre-1986) 6, 93--101.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khan et al. (2021) Khan, A., Kim, H., Chua, L., 2021. Pmed-net: Pyramid based
    multi-scale encoder-decoder network for medical image segmentation. IEEE Access
    9, 55988--55998.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khan et al. (2022) Khan, A.H., Awang Iskandar, D.N., Al-Asad, J.F., Mewada,
    H., Sherazi, M.A., 2022. Ensemble learning of deep learning and traditional machine
    learning approaches for skin lesion segmentation and classification. Concurrency
    and Computation: Practice and Experience 34, e6907.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khouloud et al. (2021) Khouloud, S., Ahlem, M., Fadel, T., Amel, S., 2021. W-net
    and inception residual network for skin lesion segmentation and classification.
    Applied Intelligence , 1--19.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim and Lee (2021) Kim, M., Lee, B.D., 2021. A simple generic method for effective
    boundary extraction in medical image segmentation. IEEE Access 9, 103875--103884.
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kinyanjui et al. (2020) Kinyanjui, N.M., Odonga, T., Cintas, C., Codella, N.C.,
    Panda, R., Sattigeri, P., Varshney, K.R., 2020. Fairness of classifiers across
    skin tones in dermatology, in: Medical Image Computing and Computer Assisted Intervention--MICCAI
    2020: 23rd International Conference, Lima, Peru, October 4--8, 2020, Proceedings,
    Part VI, Springer. pp. 320--329.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kittler et al. (2002) Kittler, H., Pehamberger, H., Wolff, K., Binder, M., 2002.
    Diagnostic accuracy of dermoscopy. The lancet oncology 3, 159--165.
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Korotkov et al. (2019) Korotkov, K., Quintana, J., Campos, R., Jesús-Silva,
    A., Iglesias, P., Puig, S., Malvehy, J., Garcia, R., 2019. An improved skin lesion
    matching scheme in total body photography. IEEE Journal of Biomedical and Health
    Informatics 23, 586--598.
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kosgiker et al. (2021) Kosgiker, G.M., Deshpande, A., Kauser, A., 2021. Segcaps:
    An efficient segcaps network-based skin lesion segmentation in dermoscopic images.
    International Journal of Imaging Systems and Technology 31, 874--894.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kovashka et al. (2016) Kovashka, A., Russakovsky, O., Fei-Fei, L., Grauman,
    K., 2016. Crowdsourcing in Computer Vision. Foundations and Trends in Computer
    Graphics and Vision 10, 177--243.
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krahenbuhl and Koltun (2011) Krahenbuhl, P., Koltun, V., 2011. Efficient Inference
    in Fully Connected CRFs with Gaussian Edge Potentials, in: Proceedings of the
    24th International Conference on Neural Information Processing Systems, pp. 109--117.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubat et al. (1998) Kubat, M., Holte, R.C., Matwin, S., 1998. Machine Learning
    for the Detection of Oil Spills in Satellite Radar Images. Machine Learning 30,
    195--215.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kwon et al. (2020) Kwon, Y., Won, J.H., Kim, B.J., Paik, M.C., 2020. Uncertainty
    quantification using bayesian neural networks in classification: Application to
    biomedical image segmentation. Computational Statistics & Data Analysis 142, 106816.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lampert et al. (2016) Lampert, T.A., Stumpf, A., Gancarski, P., 2016. An Empirical
    Study into Annotator Agreement, Ground Truth Estimation, and Algorithm Evaluation.
    IEEE Transactions on Image Processing 25, 2557--2572.
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Langerak et al. (2010) Langerak, T.R., van der Heide, U.A., Kotte, A.N.T.J.,
    Viergever, M.A., Van Vulpen, M., Pluim, J.P.W., 2010. Label Fusion in Atlas-Based
    Segmentation Using a Selective and Iterative Method for Performance Level Estimation
    (SIMPLE). IEEE Transactions on Medical Imaging 29, 2000--2008.
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (2015) LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep Learning.
    Nature 521, 436--444.
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1998) LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., 1998. Gradient-Based
    Learning Applied to Document Recognition. Proceedings of the IEEE 86, 2278--2324.
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2003) Lee, T.K., McLean, D.I., Atkins, M.S., 2003. Irregularity
    Index: A New Border Irregularity Measure for Cutaneous Melanocytic Lesions. Medical
    Image Analysis 7, 47--64.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2020) Lei, B., Xia, Z., Jiang, F., Jiang, X., Ge, Z., Xu, Y., Qin,
    J., Chen, S., Wang, T., Wang, S., 2020. Skin lesion segmentation via generative
    adversarial networks with dual discriminators. Medical Image Analysis 64, 101716.
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lemay et al. (2022) Lemay, A., Gros, C., Naga Karthik, E., Cohen-Adad, J.,
    2022. Label fusion and training methods for reliable representation of inter-rater
    uncertainty. Machine Learning for Biomedical Imaging 1, 1--27. URL: [https://melba-journal.org/2022:031](https://melba-journal.org/2022:031).'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2018a) Li, H., He, X., Zhou, F., Yu, Z., Ni, D., Chen, S., Wang,
    T., Lei, B., 2018a. Dense deconvolutional network for skin lesion segmentation.
    IEEE journal of biomedical and health informatics 23, 527--537.
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020a) Li, R., Wagner, C., Chen, X., Auer, D., 2020a. A generic
    ensemble based deep convolutional neural network for semi-supervised medical image
    segmentation, in: 2020 IEEE 17th International Symposium on Biomedical Imaging
    (ISBI), IEEE. pp. 1168--1172.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021a) Li, S., Gao, Z., He, X., 2021a. Superpixel-guided iterative
    learning from noisy labels for medical image segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 525--535.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021b) Li, W., Raj, A.N.J., Tjahjadi, T., Zhuang, Z., 2021b. Digital
    hair removal by deep learning for skin lesion segmentation. Pattern Recognition
    117, 107994.
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021c) Li, X., Yu, L., Chen, H., Fu, C.W., Xing, L., Heng, P.A.,
    2021c. Transformation-consistent self-ensembling model for semi-supervised medical
    image segmentation. IEEE Transactions on Neural Networks and Learning Systems
    32, 523--534.
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018b) Li, X., Yu, L., Fu, C.W., Heng, P.A., 2018b. Deeply supervised
    rotation equivariant network for lesion segmentation in dermoscopy images, in:
    OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy,
    Clinical Image-Based Procedures, and Skin Image Analysis. Springer, pp. 235--243.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020b) Li, Y., Chen, J., Zheng, Y., 2020b. A multi-task self-supervised
    learning framework for scopy images, in: 2020 IEEE 17th International Symposium
    on Biomedical Imaging (ISBI), IEEE. pp. 2005--2009.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017) Li, Y., Esteva, A., Kuprel, B., Novoa, R., Ko, J., Thrun,
    S., 2017. Skin cancer detection and tracking using data synthesis and deep learning,
    in: AAAI Conference on Artificial Intelligence Joint Workshop on Health Intelligence,
    pp. 1--4.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Shen (2018) Li, Y., Shen, L., 2018. Skin lesion analysis towards melanoma
    detection using deep learning network. Sensors 18, 556.
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Li, Y., Xu, C., Han, J., An, Z., Wang, D., Ma, H., Liu, C.,
    2022. MHAU-Net: Skin lesion segmentation based on multi-scale hybrid residual
    attention network. Sensors 22, 8701.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2022) Lin, A., Xu, J., Li, J., Lu, G., 2022. ConTrans: Improving
    Transformer with convolutional attention for medical image segmentation, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 297--307.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2019) Lin, H., Upchurch, P., Bala, K., 2019. Block annotation:
    Better image annotation with sub-image decomposition, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp. 5290--5300.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2017) Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P., 2017.
    Focal loss for dense object detection, in: Proceedings of the IEEE international
    conference on computer vision, pp. 2980--2988.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Litjens et al. (2017) Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A.,
    Ciompi, F., Ghafoorian, M., van der Laak, J.A.W.M., van Ginneken, B., Sanchez,
    C.I., 2017. A Survey on Deep Learning in Medical Image Analysis. Medical Image
    Analysis 42, 60--88.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Liu, C., Chen, L.C., Schroff, F., Adam, H., Hua, W., Yuille,
    A.L., Fei-Fei, L., 2019a. Auto-DeepLab: Hierarchical Neural Architecture Search
    for Semantic Image Segmentation, in: Proceedings of the 2019 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 82--92.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Liu, L., Mou, L., Zhu, X.X., Mandal, M., 2019b. Skin lesion
    segmentation based on improved U-Net, in: 2019 IEEE Canadian Conference of Electrical
    and Computer Engineering (CCECE), IEEE. pp. 1--4.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Liu, L., Mou, L., Zhu, X.X., Mandal, M., 2020. Automatic skin
    lesion classification based on mid-level feature learning. Computerized Medical
    Imaging and Graphics 84, 101765.
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021a) Liu, L., Tsui, Y.Y., Mandal, M., 2021a. Skin lesion segmentation
    using deep learning with auxiliary task. Journal of Imaging 7, 67.
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022a) Liu, Q., Wang, J., Zuo, M., Cao, W., Zheng, J., Zhao, H.,
    Xie, J., 2022a. NCRNet: Neighborhood context refinement network for skin lesion
    segmentation. Computers in Biology and Medicine 146, 105545.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022b) Liu, X., Fan, W., Zhou, D., 2022b. Skin lesion segmentation
    via intensive atrous spatial Transformer, in: International Conference on Wireless
    Algorithms, Systems, and Applications, Springer. pp. 15--26.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021b) Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
    S., Guo, B., 2021b. Swin transformer: Hierarchical vision transformer using shifted
    windows, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp. 10012--10022.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Long et al. (2015) Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional
    networks for semantic segmentation, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 3431--3440.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lui et al. (2009) Lui, H., et al., 2009. DermWeb, Department of Dermatology
    and Skin Science, the University of British Columbia. [http://www.dermweb.com/](http://www.dermweb.com/).
    [Online. Accessed January 26, 2022].
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luque et al. (2020) Luque, A., Carrasco, A., Martin, A., de las Heras, A., 2020.
    The Impact of Class Imbalance in Classification Performance Metrics Based on the
    Binary Confusion Matrix. Pattern Recognition 91, 216--231.
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2021) Ma, J., Chen, J., Ng, M., Huang, R., Li, Y., Li, C., Yang,
    X., Martel, A.L., 2021. Loss odyssey in medical image segmentation. Medical Image
    Analysis 71, 102035.
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahbod et al. (2020) Mahbod, A., Tschandl, P., Langs, G., Ecker, R., Ellinger,
    I., 2020. The effects of skin lesion segmentation on the performance of dermatoscopic
    image classification. Computer Methods and Programs in Biomedicine 197, 105725.
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maier-Hein et al. (2014) Maier-Hein, L., Mersmann, S., Kondermann, D., Bodenstedt,
    S., Sanchez, A., Stock, C., Kenngott, H.G., Eisenmann, M., Speidel, S., 2014.
    Can masses of non-experts train highly accurate image classifiers?, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 438--445.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marchetti et al. (2018) Marchetti, M.A., Codella, N.C.F., Dusza, S.W., Gutman,
    D.A., Helba, B., Kalloo, A., Mishra, N., Carrera, C., Celebi, M.E., DeFazio, J.L.,
    Jaimes, N., Marghoob, A.A., Quigley, E., Scope, A., Yelamos, O., Halpern, A.C.,
    2018. Results of the 2016 International Skin Imaging Collaboration International
    Symposium on Biomedical Imaging Challenge: Comparison of the Accuracy of Computer
    Algorithms to Dermatologists for the Diagnosis of Melanoma from Dermoscopic Images.
    Journal of the American Academy of Dermatology 78, 270--277.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maron et al. (2021a) Maron, R.C., Hekler, A., Krieghoff-Henning, E., Schmitt,
    M., Schlager, J.G., Utikal, J.S., Brinker, T.J., 2021a. Reducing the impact of
    confounding factors on skin cancer classification via image segmentation: Technical
    model study. Journal of Medical Internet Research 23, e21695.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maron et al. (2021b) Maron, R.C., Schlager, J.G., Haggenmüller, S., von Kalle,
    C., Utikal, J.S., Meier, F., Gellrich, F.F., Hobelsberger, S., Hauschild, A.,
    French, L., Heinzerling, L., Schlaak, M., Ghoreschi, K., Hilke, F.J., Poch, G.,
    Heppt, M.V., Berking, C., Haferkamp, S., Sondermann, W., Schadendorf, D., Schilling,
    B., Goebeler, M., Krieghoff-Henning, E., Hekler, A., Fröhling, S., Lipka, D.B.,
    Kather, J.N., Brinker, T.J., 2021b. A benchmark for neural network robustness
    in skin cancer classification. European Journal of Cancer 155, 191--199.
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matthews (1975) Matthews, B.W., 1975. Comparison of the Predicted and Observed
    Secondary Structure of T4 Phage Lysozyme. Biochimica et Biophysica Acta 405, 442--451.
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mendonca et al. (2013) Mendonca, T., Ferreira, P.M., Marques, J.S., Marcal,
    A.R.S., Rozeira, J., 2013. PH²---A Dermoscopic Image Database for Research and
    Benchmarking, in: Proceedings of the 35th Annual International Conference of the
    IEEE Engineering in Medicine and Biology Society, pp. 5437--5440.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mendonca et al. (2015) Mendonca, T.F., Ferreira, P.M., Marcal, A.R.S., Barata,
    C., Marques, J.S., Rocha, J., Rozeira, J., 2015. PH²---A Dermoscopic Image Database
    for Research and Benchmarking, in: Celebi, M.E., Mendonca, T., Marques, J.S. (Eds.),
    Dermoscopy Image Analysis. CRC Press, pp. 419--439.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Menzies et al. (2003) Menzies, S.W., Crotty, K.A., Ingwar, C., McCarthy, W.H.,
    2003. An Atlas of Surface Microscopy of Pigmented Skin Lesions: Dermoscopy. Second
    ed., McGraw-Hill.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miller and Nicely (1955) Miller, G.A., Nicely, P.E., 1955. An analysis of perceptual
    confusions among some english consonants. The Journal of the Acoustical Society
    of America 27, 338--352.
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mirikharaji et al. (2021) Mirikharaji, Z., Abhishek, K., Izadi, S., Hamarneh,
    G., 2021. D-LEMA: Deep learning ensembles from multiple annotations-application
    to skin lesion segmentation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 1837--1846.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mirikharaji and Hamarneh (2018) Mirikharaji, Z., Hamarneh, G., 2018. Star shape
    prior in fully convolutional networks for skin lesion segmentation, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 737--745.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mirikharaji et al. (2018) Mirikharaji, Z., Izadi, S., Kawahara, J., Hamarneh,
    G., 2018. Deep auto-context fully convolutional neural network for skin lesion
    segmentation, in: 2018 IEEE 15th International Symposium on Biomedical Imaging
    (ISBI 2018), IEEE. pp. 877--880.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mirikharaji et al. (2019) Mirikharaji, Z., Yan, Y., Hamarneh, G., 2019. Learning
    to segment skin lesions from noisy annotations, in: Domain Adaptation and Representation
    Transfer and Medical Image Learning with Less Labels and Imperfect Data. Springer,
    pp. 207--215.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirzaalian et al. (2016) Mirzaalian, H., Lee, T.K., Hamarneh, G., 2016. Skin
    lesion tracking using structured graphical models. Medical Image Analysis 27,
    84--92.
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mishra and Daescu (2017) Mishra, R., Daescu, O., 2017. Deep learning for skin
    lesion segmentation, in: 2017 IEEE International Conference on Bioinformatics
    and Biomedicine (BIBM), IEEE. pp. 1189--1194.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nachbar et al. (1994) Nachbar, F., Stolz, W., Merkle, T., Cognetta, A.B., Vogt,
    T., Landthaler, M., Bilek, P., Braun-Falco, O., Plewig, G., 1994. The ABCD rule
    of dermatoscopy: High prospective value in the diagnosis of doubtful melanocytic
    skin lesions. Journal of the American Academy of Dermatology 30, 551--559.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nasr-Esfahani et al. (2019) Nasr-Esfahani, E., Rafiei, S., Jafari, M.H., Karimi,
    N., Wrobel, J.S., Samavi, S., Soroushmehr, S.R., 2019. Dense pooling layers in
    fully convolutional network for skin lesion segmentation. Computerized Medical
    Imaging and Graphics 78, 101658.
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nathan and Kansal (2020) Nathan, S., Kansal, P., 2020. Lesion net--skin lesion
    segmentation using coordinate convolution and deep residual units. arXiv preprint
    arXiv:2012.14249 .
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navarro et al. (2018) Navarro, F., Escudero-Vinolo, M., Bescós, J., 2018. Accurate
    segmentation and registration of skin lesion images to evaluate lesion change.
    IEEE Journal of Biomedical and Health Informatics 23, 501--508.
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ning et al. (2005) Ning, F., Delhomme, D., LeCun, Y., Piano, F., Bottou, L.,
    Barbano, P.E., 2005. Toward Automatic Phenotyping of Developing Embryos from Videos.
    IEEE Transactions on Image Processing 14, 1360--1371.
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Norton et al. (2012) Norton, K.A., Iyatomi, H., Celebi, M.E., Ishizaki, S.,
    Sawada, M., Suzaki, R., Kobayashi, K., Tanaka, M., Ogawa, K., 2012. Three-Phase
    General Border Detection Method for Dermoscopy Images Using Non-Uniform Illumination
    Correction. Skin Research and Technology 18, 290--300.
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nosrati and Hamarneh (2016) Nosrati, M.S., Hamarneh, G., 2016. Incorporating
    prior knowledge in medical image segmentation: a survey. arXiv preprint arXiv:1607.01092
    .'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oakley et al. (1995) Oakley, A., et al., 1995. DermNet New Zealand Trust. [https://dermnetnz.org/](https://dermnetnz.org/).
    [Online. Accessed January 26, 2022].
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oktay et al. (2018) Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich,
    M., Misawa, K., Mori, K., McDonagh, S., Hammerla, N.Y., Kainz, B., et al., 2018.
    Attention U-Net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999
    .'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Öztürk and Özkaya (2020) Öztürk, Ş., Özkaya, U., 2020. Skin lesion segmentation
    with improved convolutional neural network. Journal of digital imaging 33, 958--970.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pacheco et al. (2020) Pacheco, A.G., Lima, G.R., Salomão, A.S., Krohling, B.,
    Biral, I.P., de Angelo, G.G., Alves Jr, F.C., Esgario, J.G., Simora, A.C., Castro,
    P.B., et al., 2020. PAD-UFES-20: A skin lesion dataset composed of patient data
    and clinical images collected from smartphones. Data in Brief 32, 106221.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pakzad et al. (2023) Pakzad, A., Abhishek, K., Hamarneh, G., 2023. CIRCLe:
    Color invariant representation learning for unbiased classification of skin lesions,
    in: Computer Vision--ECCV 2022 Workshops: Tel Aviv, Israel, October 23--27, 2022,
    Proceedings, Part IV, Springer. pp. 203--219.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papadopoulos et al. (2017) Papadopoulos, D.P., Uijlings, J.R., Keller, F.,
    Ferrari, V., 2017. Extreme clicking for efficient object annotation, in: Proceedings
    of the IEEE International Conference on Computer Vision, pp. 4930--4939.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papandreou et al. (2015) Papandreou, G., Chen, L.C., Murphy, K.P., Yuille,
    A.L., 2015. Weakly- and Semi-Supervised Learning of a Deep Convolutional Network
    for Semantic Image Segmentation, in: Proceedings of the IEEE International Conference
    on Computer Vision, pp. 1742--1750.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parmar et al. (2018) Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer,
    N., Ku, A., Tran, D., 2018. Image transformer, in: International Conference on
    Machine Learning, PMLR. pp. 4055--4064.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearson (1904) Pearson, K., 1904. On the theory of contingency and its relation
    to association and normal correlation. volume 1. Dulau and Company London, UK.
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng and Li (2013) Peng, B., Li, T., 2013. A Probabilistic Measure for Quantitative
    Evaluation of Image Segmentation. IEEE Signal Processing Letters 20, 689--692.
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2016) Peng, B., Wang, X., Yang, Y., 2016. Region Based Exemplar
    References for Image Segmentation Evaluation. IEEE Signal Processing Letters 23,
    459--462.
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2017a) Peng, B., Zhang, L., Mou, X., Yang, M.H., 2017a. Evaluation
    of Segmentation Quality via Adaptive Composition of Reference Segmentations. IEEE
    Transactions on Pattern Analysis and Machine Intelligence 39, 1929--1941.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2017b) Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J., 2017b. Large
    kernel matters--improve semantic segmentation by global convolutional network,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 4353--4361.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2019) Peng, Y., Wang, N., Wang, Y., Wang, M., 2019. Segmentation
    of dermoscopy image using adversarial networks. Multimedia Tools and Applications
    78, 10965--10981.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perez et al. (2018) Perez, F., Vasconcelos, C., Avila, S., Valle, E., 2018.
    Data Augmentation for Skin Lesion Analysis, in: Proceedings of the Third ISIC
    Workshop on Skin Image Analysis, pp. 303--311.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peserico and Silletti (2010) Peserico, E., Silletti, A., 2010. Is (N)PRI Suitable
    for Evaluating Automated Segmentation of Cutaneous Lesions? Pattern Recognition
    Letters 31, 2464--2467.
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pinheiro and Collobert (2014) Pinheiro, P.H., Collobert, R., 2014. Recurrent
    convolutional neural networks for scene labeling, in: 31st International Conference
    on Machine Learning (ICML), PMLR. pp. 82--90.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pollastri et al. (2018) Pollastri, F., Bolelli, F., Palacios, R.P., Grana,
    C., 2018. Improving skin lesion segmentation with generative adversarial networks,
    in: 2018 IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS),
    IEEE. pp. 442--443.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pollastri et al. (2020) Pollastri, F., Bolelli, F., Paredes, R., Grana, C.,
    2020. Augmenting Data with GANs to Segment Melanoma Skin Lesions. Multimedia Tools
    and Applications 79, 15575--15592.
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poudel and Lee (2021) Poudel, S., Lee, S.W., 2021. Deep multi-scale attentional
    features for medical image segmentation. Applied Soft Computing 109, 107445.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pour and Seker (2020) Pour, M.P., Seker, H., 2020. Transform Domain Representation-Driven
    Convolutional Neural Networks for Skin Lesion Segmentation. Expert Systems with
    Applications 144, 113129.
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiu et al. (2020) Qiu, Y., Cai, J., Qin, X., Zhang, J., 2020. Inferring skin
    lesion deep convolutional neural networks. IEEE Access 8, 144246--144258.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rajchl et al. (2016) Rajchl, M., Lee, M.C., Schrans, F., Davidson, A., Passerat-Palmbach,
    J., Tarroni, G., Alansary, A., Oktay, O., Kainz, B., Rueckert, D., 2016. Learning
    under distributed weak supervision. arXiv preprint arXiv:1606.01100 .
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramachandram and DeVries (2017) Ramachandram, D., DeVries, T., 2017. Lesionseg:
    semantic segmentation of skin lesions using deep convolutional neural network.
    arXiv preprint arXiv:1703.03372 .'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramachandram and Taylor (2017) Ramachandram, D., Taylor, G.W., 2017. Skin lesion
    segmentation using deep hypercolumn descriptors. Journal of Computational Vision
    and Imaging Systems 3.
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramachandran et al. (2019) Ramachandran, P., Parmar, N., Vaswani, A., Bello,
    I., Levskaya, A., Shlens, J., 2019. Stand-alone self-attention in vision models.
    Advances in Neural Information Processing Systems 32.
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramadan et al. (2022) Ramadan, R., Aly, S., Abdel-Atty, M., 2022. Color-invariant
    skin lesion semantic segmentation based on modified U-Net deep convolutional neural
    network. Health Information Science and Systems 10, 1--12.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramani and Ranjani (2019) Ramani, D.R., Ranjani, S.S., 2019. U-net based segmentation
    and multiple feature extraction of dermascopic images for efficient diagnosis
    of melanoma, in: Computer Aided Intervention and Diagnostics in Clinical and Medical
    Images, pp. 81--101.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rand (1971) Rand, W.M., 1971. Objective Criteria for the Evaluation of Clustering
    Methods. Journal of the American Statistical Association 66, 846--850.
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ranftl et al. (2021) Ranftl, R., Bochkovskiy, A., Koltun, V., 2021. Vision
    transformers for dense prediction, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 12179--12188.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redekop and Chernyavskiy (2021) Redekop, E., Chernyavskiy, A., 2021. Uncertainty-based
    method for improving poorly labeled segmentation datasets, in: 2021 IEEE 18th
    International Symposium on Biomedical Imaging (ISBI), IEEE. pp. 1831--1835.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon et al. (2016) Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016.
    You only look once: Unified, real-time object detection, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, pp. 779--788.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster R-CNN:
    Towards real-time object detection with region proposal networks. Advances in
    Neural Information Processing Systems 28.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2021) Ren, Y., Yu, L., Tian, S., Cheng, J., Guo, Z., Zhang, Y.,
    2021. Serial attention network for skin lesion segmentation. Journal of Ambient
    Intelligence and Humanized Computing , 1--12.
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Renard et al. (2020) Renard, F., Guedria, S., Palma, N.D., Vuillerme, N., 2020.
    Variability and reproducibility in deep learning for medical image segmentation.
    Scientific Reports 10, 1--16.
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribeiro et al. (2020) Ribeiro, V., Avila, S., Valle, E., 2020. Less is more:
    Sample selection and label conditioning improve skin lesion segmentation, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops, pp. 738--739.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rohlfing and Maurer (2006) Rohlfing, T., Maurer, C.R., 2006. Shape-Based Averaging.
    IEEE Transactions on Image Processing 16, 153--161.
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Ronneberger, O., Fischer, P., Brox, T., 2015. U-Net:
    Convolutional networks for biomedical image segmentation, in: International Conference
    on Medical image computing and computer-assisted intervention, pp. 234--241.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ross-Howe and Tizhoosh (2018) Ross-Howe, S., Tizhoosh, H.R., 2018. The effects
    of image pre-and post-processing, wavelet decomposition, and local binary patterns
    on u-nets for skin lesion segmentation, in: 2018 International Joint Conference
    on Neural Networks (IJCNN), pp. 1--8.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotemberg et al. (2021) Rotemberg, V., Kurtansky, N., Betz-Stablein, B., Caffery,
    L., Chousakos, E., Codella, N., Combalia, M., Dusza, S., Guitera, P., Gutman,
    D., Halpern, A., Helba, B., Kittler, H., Kose, K., Langer, S., Lioprys, K., Malvehy,
    J., Musthaq, S., Nanda, J., Reiter, O., Shih, G., Stratigos, A., Tschandl, P.,
    Weber, J., Soyer, H.P., 2021. A Patient-Centric Dataset of Images and Metadata
    for Identifying Melanomas Using Clinical Context. Scientific Data 8, 34.
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roth et al. (2021) Roth, H.R., Yang, D., Xu, Z., Wang, X., Xu, D., 2021. Going
    to extremes: Weakly supervised medical image segmentation. Machine Learning and
    Knowledge Extraction 3, 507--524.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rother et al. (2004) Rother, C., Kolmogorov, V., Blake, A., 2004. "GrabCut"
    interactive foreground extraction using iterated graph cuts. ACM Transactions
    on Graphics (TOG) 23, 309--314.
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1986) Rumelhart, D.E., Hinton, G.E., Williams, R.J., 1986.
    Learning Representations by Back-Propagating Errors. Nature 323, 533--536.
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saba et al. (2019) Saba, T., Khan, M.A., Rehman, A., Marie-Sainte, S.L., 2019.
    Region Extraction and Classification of Skin Cancer: A Heterogeneous Framework
    of Deep CNN Features Fusion and Reduction. Journal of Medical Systems 43, 289.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sachin et al. (2021) Sachin, T.S., Sowmya, V., Soman, K., 2021. Performance
    analysis of deep learning models for biomedical image segmentation, in: Deep Learning
    for Biomedical Applications. CRC Press, pp. 83--100.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sagi and Rokach (2018) Sagi, O., Rokach, L., 2018. Ensemble learning: A survey.
    Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8, e1249.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saha et al. (2020) Saha, A., Prasad, P., Thabit, A., 2020. Leveraging adaptive
    color augmentation in convolutional neural networks for deep skin lesion segmentation,
    in: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), IEEE.
    pp. 2014--2017.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Şahin et al. (2021) Şahin, N., Alpaslan, N., Hanbay, D., 2021. Robust optimization
    of SegNet hyperparameters for skin lesion segmentation. Multimedia Tools and Applications
    , 1--21.
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saini et al. (2019) Saini, S., Gupta, D., Tiwari, A.K., 2019. Detector-segmentor
    network for skin lesion localization and segmentation, in: National Conference
    on Computer Vision, Pattern Recognition, Image Processing, and Graphics, Springer.
    pp. 589--599.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saini et al. (2021) Saini, S., Jeon, Y.S., Feng, M., 2021. B-segnet: branched-segmentor
    network for skin lesion segmentation, in: Proceedings of the Conference on Health,
    Inference, and Learning, pp. 214--221.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarker et al. (2019) Sarker, M., Kamal, M., Rashwan, H.A., Abdel-Nasser, M.,
    Singh, V.K., Banu, S.F., Akram, F., Chowdhury, F.U., Choudhury, K.A., Chambon,
    S., et al., 2019. MobileGAN: Skin lesion segmentation using a lightweight generative
    adversarial network. arXiv preprint arXiv:1907.00856 .'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarker et al. (2018) Sarker, M.M.K., Rashwan, H.A., Akram, F., Banu, S.F.,
    Saleh, A., Singh, V.K., Chowdhury, F.U., Abdulwahab, S., Romani, S., Radeva, P.,
    et al., 2018. SLSDeep: Skin lesion segmentation based on dilated residual and
    pyramid pooling networks, in: International Conference on Medical Image Computing
    and Computer-Assisted Intervention, Springer. pp. 21--29.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarker et al. (2021) Sarker, M.M.K., Rashwan, H.A., Akram, F., Singh, V.K.,
    Banu, S.F., Chowdhury, F.U., Choudhury, K.A., Chambon, S., Radeva, P., Puig, D.,
    et al., 2021. SLSNet: Skin lesion segmentation using a lightweight generative
    adversarial network. Expert Systems with Applications 183, 115433.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaefer et al. (2011) Schaefer, G., Rajab, M.I., Celebi, M.E., Iyatomi, H.,
    2011. Colour and Contrast Enhancement for Improved Skin Lesion Segmentation. Computerized
    Medical Imaging and Graphics 35, 99--104.
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shahin et al. (2019) Shahin, A.H., Amer, K., Elattar, M.A., 2019. Deep convolutional
    encoder-decoders with aggregated multi-resolution skip connections for skin lesion
    segmentation, in: 2019 IEEE 16th International Symposium on Biomedical Imaging
    (ISBI 2019), IEEE. pp. 451--454.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shamshad et al. (2022) Shamshad, F., Khan, S., Zamir, S.W., Khan, M.H., Hayat,
    M., Khan, F.S., Fu, H., 2022. Transformers in medical imaging: A survey. arXiv
    preprint arXiv:2201.09873 .'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shamsolmoali et al. (2021) Shamsolmoali, P., Zareapoor, M., Granger, E., Zhou,
    H., Wang, R., Celebi, M.E., Yang, J., 2021. Image Synthesis with Adversarial Networks:
    A Comprehensive Survey and Case Studies. Information Fusion 72, 126--146.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. (2017) Sharma, M., Saha, O., Sriraman, A., Hebbalaguppe, R.,
    Vig, L., Karande, S., 2017. Crowdsourcing for Chromosome Segmentation and Deep
    Classification, in: Proceedings of the IEEE Conference on Computer vision and
    Pattern Recognition Workshops, pp. 786--793.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shimizu et al. (2015) Shimizu, K., Iyatomi, H., Celebi, M.E., Norton, K.A.,
    Tanaka, M., 2015. Four-Class Classification of Skin Lesions with Task Decomposition
    Strategy. IEEE Transactions on Biomedical Engineering 62, 274--283.
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shorten and Khoshgoftaar (2019) Shorten, C., Khoshgoftaar, T.M., 2019. A Survey
    on Image Data Augmentation for Deep Learning. Journal of Big Data 6, 60.
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Siegel et al. (2023) Siegel, R.L., Miller, K.D., Wagle, N.S., Jemal, A., 2023.
    Cancer statistics, 2023. CA: A Cancer Journal for Clinicians 73, 17--48. URL:
    [https://doi.org/10.3322/caac.21763](https://doi.org/10.3322/caac.21763), doi:[10.3322/caac.21763](http://dx.doi.org/10.3322/caac.21763).'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silveira et al. (2009) Silveira, M., Nascimento, J.C., Marques, J.S., Marcal,
    A.R.S., Mendonca, T., Yamauchi, S., Maeda, J., Rozeira, J., 2009. Comparison of
    Segmentation Methods for Melanoma Diagnosis in Dermoscopy Images. IEEE Journal
    of Selected Topics in Signal Processing 3, 35--45.
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2014) Simonyan, K., Zisserman, A., 2014. Very deep convolutional
    networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 .
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2023) Singh, L., Janghel, R.R., Sahu, S.P., 2023. An empirical
    review on evaluating the impact of image segmentation on the classification performance
    for skin lesion detection. IETE Technical Review 40, 190--201.
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. (2019) Singh, V.K., Abdel-Nasser, M., Rashwan, H.A., Akram, F.,
    Pandey, N., Lalande, A., Presles, B., Romani, S., Puig, D., 2019. FCA-Net: Adversarial
    learning for skin lesion segmentation based on multi-scale features and factorized
    channel attention. IEEE Access 7, 130552--130565.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sinha et al. (2023) Sinha, A., Kawahara, J., Pakzad, A., Abhishek, K., Ruthven,
    M., Ghorbel, E., Kacem, A., Aouada, D., Hamarneh, G., 2023. DermSynth3D: Synthesis
    of in-the-wild annotated dermatology images. arXiv preprint arXiv:2305.12621 .'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smyth et al. (1995) Smyth, P., Fayyad, U.M., Burl, M.C., Perona, P., Baldi,
    P., 1995. Inferring Ground Truth from Subjective Labelling of Venus Images, in:
    Advances in Neural Information Processing Systems, pp. 1085--1092.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soenksen et al. (2021) Soenksen, L.R., Kassis, T., Conover, S.T., Marti-Fuster,
    B., Birkenfeld, J.S., Tucker-Schwartz, J., Naseem, A., Stavert, R.R., Kim, C.C.,
    Senna, M.M., Avilés-Izquierdo, J., Collins, J.J., Barzilay, R., Gray, M.L., 2021.
    Using deep learning for dermatologist-level detection of suspicious pigmented
    skin lesions from wide-field images. Science Translational Medicine 13, eabb3652.
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2019) Song, L., Lin, J., Wang, Z.J., Wang, H., 2019. Dense-residual
    attention network for skin lesion segmentation, in: International Workshop on
    Machine Learning in Medical Imaging, Springer. pp. 319--327.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sørensen (1948) Sørensen, T.A., 1948. A method of establishing groups of equal
    amplitude in plant sociology based on similarity of species content and its application
    to analyses of the vegetation on Danish commons. Biol. Skar. 5, 1--34.
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soudani and Barhoumi (2019) Soudani, A., Barhoumi, W., 2019. An image-based
    segmentation recommender using crowdsourcing and transfer learning for skin lesion
    extraction. Expert Systems with Applications 118, 400--410.
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strudel et al. (2021) Strudel, R., Garcia, R., Laptev, I., Schmid, C., 2021.
    Segmenter: Transformer for semantic segmentation, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp. 7262--7272.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2017) Sun, C., Shrivastava, A., Singh, S., Gupta, A., 2017. Revisiting
    unreasonable effectiveness of data in deep learning era, in: Proceedings of the
    IEEE International Conference on Computer Vision, pp. 843--852.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2016) Sun, X., Yang, J., Sun, M., Wang, K., 2016. A benchmark for
    automatic visual classification of clinical skin disease images, in: European
    Conference on Computer Vision, Springer. pp. 206--222.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taghanaki et al. (2019) Taghanaki, S.A., Abhishek, K., Hamarneh, G., 2019.
    Improved inference via deep input transfer, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer. pp. 819--827.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taha and Hanbury (2015) Taha, A.A., Hanbury, A., 2015. Metrics for Evaluating
    3D Medical Image Segmentation: Analysis, Selection, and Tool. BMC Medical Imaging
    15, 29.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tajbakhsh et al. (2020) Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J.N.,
    Wu, Z., Ding, X., 2020. Embracing imperfect datasets: A review of deep learning
    solutions for medical image segmentation. Medical Image Analysis 63, 101693.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2019a) Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M.,
    Howard, A., Le, Q.V., 2019a. MnasNet: Platform-Aware Neural Architecture Search
    for Mobile, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 2820--2828.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan and Le (2019) Tan, M., Le, Q., 2019. EfficientNet: Rethinking Model Scaling
    for Convolutional neural Networks, in: Proceedings of the International Conference
    on Machine Learning, pp. 6105--6114.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2019b) Tan, T.Y., Zhang, L., Lim, C.P., Fielding, B., Yu, Y., Anderson,
    E., 2019b. Evolving ensemble models for image segmentation using enhanced particle
    swarm optimization. IEEE access 7, 34004--34019.
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2019a) Tang, P., Liang, Q., Yan, X., Xiang, S., Sun, W., Zhang,
    D., Coppola, G., 2019a. Efficient skin lesion segmentation using separable-unet
    with stochastic weight averaging. Computer methods and programs in biomedicine
    178, 289--301.
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2021a) Tang, P., Yan, X., Liang, Q., Zhang, D., 2021a. AFLN-DGCL:
    Adaptive feature learning network with difficulty-guided curriculum learning for
    skin lesion segmentation. Applied Soft Computing 110, 107656.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2021b) Tang, X., Peng, J., Zhong, B., Li, J., Yan, Z., 2021b. Introducing
    frequency representation into convolution neural networks for medical image segmentation
    via twin-kernel fourier convolution. Computer Methods and Programs in Biomedicine
    205, 106110.
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2019b) Tang, Y., Yang, F., Yuan, S., et al., 2019b. A multi-stage
    framework with context information fusion structure for skin lesion segmentation,
    in: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),
    IEEE. pp. 1407--1410.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tao et al. (2021) Tao, S., Jiang, Y., Cao, S., Wu, C., Ma, Z., 2021. Attention-guided
    network with densely connected convolution for skin lesion segmentation. Sensors
    21, 3462.
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tong et al. (2021) Tong, X., Wei, J., Sun, B., Su, S., Zuo, Z., Wu, P., 2021.
    Ascu-net: Attention gate, spatial and channel attention u-net for skin lesion
    segmentation. Diagnostics 11, 501.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Torralba and Efros (2011) Torralba, A., Efros, A.A., 2011. Unbiased look at
    dataset bias, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 1521--1528.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. (2005) Tran, H., Chen, K., Lim, A.C., Jabbour, J., Shumack, S.,
    2005. Assessing diagnostic skill in dermatology: a comparison between general
    practitioners and dermatologists. Australasian journal of dermatology 46, 230--234.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran and Pham (2022) Tran, T.T., Pham, V.T., 2022. Fully convolutional neural
    network with attention gate and fuzzy active contour model for skin lesion segmentation.
    Multimedia Tools and Applications 81, 13979--13999.
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tschandl et al. (2020) Tschandl, P., Rinner, C., Apalla, Z., Argenziano, G.,
    Codella, N., Halpern, A., Janda, M., Lallas, A., Longo, C., Malvehy, J., Paoli,
    J., Puig, S., Rosendahl, C., Soyer, H.P., Zalaudek, I., Kittler, H., 2020. Human--computer
    collaboration for skin cancer recognition. Nature Medicine 26, 1229--1234. URL:
    [https://doi.org/10.1038/s41591-020-0942-0](https://doi.org/10.1038/s41591-020-0942-0),
    doi:[10.1038/s41591-020-0942-0](http://dx.doi.org/10.1038/s41591-020-0942-0).'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tschandl et al. (2018) Tschandl, P., Rosendahl, C., Kittler, H., 2018. The HAM10000
    Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented
    Skin Lesions. Scientific Data , 180161.
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tschandl et al. (2019) Tschandl, P., Sinz, C., Kittler, H., 2019. Domain-specific
    classification-pretrained fully convolutional network encoders for skin lesion
    segmentation. Computers in Biology and Medicine 104, 111--116.
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tu et al. (2019) Tu, W., Liu, X., Hu, W., Pan, Z., 2019. Dense-residual network
    with adversarial learning for skin lesion segmentation. IEEE Access 7, 77037--77051.
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unnikrishnan et al. (2007) Unnikrishnan, R., Pantofaru, C., Hebert, M., 2007.
    Toward Objective Evaluation of Image Segmentation Algorithms. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 29, 929--944.
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ünver and Ayan (2019) Ünver, H.M., Ayan, E., 2019. Skin lesion segmentation
    in dermoscopic images with combination of YOLO and GrabCut algorithm. Diagnostics
    9, 72.
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usatine and Madden (2013) Usatine, R.P., Madden, B.D., 2013. Interactive dermatology
    atlas. Department of Dermatology and Cutaneous Surgery, University of Texas [https://www.dermatlas.net/](https://www.dermatlas.net/)
    [Accessed January 26, 2022].
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valanarasu and Patel (2022) Valanarasu, J.M.J., Patel, V.M., 2022. UNeXt: MLP-based
    rapid medical image segmentation network, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer. pp. 23--33.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valle et al. (2020) Valle, E., Fornaciali, M., Menegola, A., Tavares, J., Bittencourt,
    F.V., Li, L.T., Avila, S., 2020. Data, depth, and design: Learning reliable models
    for skin lesion analysis. Neurocomputing 383, 303--313.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Rijsbergen (1979) van Rijsbergen, C.J., 1979. Information Retrieval. Second
    ed., Butterworth--Heinemann.
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vandewalle (2012) Vandewalle, P., 2012. Code sharing is associated with research
    impact in image processing. Computing in Science & Engineering 14, 42--47.
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanker and Van Stoecker (1984) Vanker, A.D., Van Stoecker, W., 1984. An expert
    diagnostic program for dermatology. Computers and Biomedical Research 17, 241--247.
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need.
    Advances in Neural Information Processing Systems 30.
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Venkatesh et al. (2018) Venkatesh, G., Naresh, Y., Little, S., O’Connor, N.E.,
    2018. A deep residual architecture for skin lesion segmentation, in: OR 2.0 Context-Aware
    Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical Image-Based
    Procedures, and Skin Image Analysis. Springer, pp. 277--284.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vesal et al. (2018a) Vesal, S., Patil, S.M., Ravikumar, N., Maier, A.K., 2018a.
    A multi-task framework for skin lesion detection and segmentation, in: OR 2.0
    Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical
    Image-Based Procedures, and Skin Image Analysis. Springer, pp. 285--293.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vesal et al. (2018b) Vesal, S., Ravikumar, N., Maier, A., 2018b. SkinNet: A
    deep learning framework for skin lesion segmentation, in: 2018 IEEE Nuclear Science
    Symposium and Medical Imaging Conference Proceedings (NSS/MIC), IEEE. pp. 1--3.'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ViDIR Dataverse (2020) ViDIR Dataverse, 2020. HAM10000 Binary Lesion Segmentations.
    [https://doi.org/10.7910/DVN/DBW86T](https://doi.org/10.7910/DVN/DBW86T). [Online.
    Accessed January 9, 2023].
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Wang, H., Wang, G., Sheng, Z., Zhang, S., 2019a. Automated
    segmentation of skin lesion based on pyramid attention network, in: International
    Workshop on Machine Learning in Medical Imaging, Springer. pp. 435--443.'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022a) Wang, J., Li, B., Guo, X., Huang, J., Song, M., Wei, M.,
    2022a. CTCNet: A bi-directional cascaded segmentation network combining Transformers
    with CNNs for skin lesions, in: Chinese Conference on Pattern Recognition and
    Computer Vision (PRCV), Springer. pp. 215--226.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021a) Wang, J., Wei, L., Wang, L., Zhou, Q., Zhu, L., Qin, J.,
    2021a. Boundary-aware Transformers for skin lesion segmentation, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 206--216.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2017) Wang, M., Liu, B., Foroosh, H., 2017. Factorized convolutional
    neural networks, in: Proceedings of the IEEE International Conference on Computer
    Vision Workshops, pp. 545--553.'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Wang, R., Chen, S., Fan, J., Li, Y., 2020a. Cascaded context
    enhancement for automated skin lesion segmentation. arXiv preprint arXiv:2004.08107
    .
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022b) Wang, T., Lan, J., Han, Z., Hu, Z., Huang, Y., Deng, Y.,
    Zhang, H., Wang, J., Chen, M., Jiang, H., et al., 2022b. O-Net: a novel framework
    with deep fusion of CNN and Transformer for simultaneous segmentation and classification.
    Frontiers in Neuroscience 16.'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro,
    B., 2018. High-resolution image synthesis and semantic manipulation with conditional
    gans, in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 8798--8807.'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019b) Wang, X., Ding, H., Jiang, X., 2019b. Dermoscopic image
    segmentation through the enhanced high-level parsing and class weighted loss,
    in: 2019 IEEE International Conference on Image Processing (ICIP), IEEE. pp. 245--249.'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019c) Wang, X., Jiang, X., Ding, H., Liu, J., 2019c. Bi-directional
    dermoscopic feature learning and multi-scale consistent decision fusion for skin
    lesion segmentation. IEEE transactions on image processing 29, 3039--3051.
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021b) Wang, X., Jiang, X., Ding, H., Zhao, Y., Liu, J., 2021b.
    Knowledge-aware deep framework for collaborative skin lesion segmentation and
    melanoma recognition. Pattern Recognition 120, 108075.
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Wang (2022) Wang, Y., Wang, S., 2022. Skin lesion segmentation with
    attention-based SC-Conv U-Net and feature map distortion. Signal, Image and Video
    Processing , 1--9.
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Wang, Y., Wei, Y., Qian, X., Zhu, L., Yang, Y., 2020b.
    DONet: Dual objective networks for skin lesion segmentation. arXiv preprint arXiv:2008.08278
    .'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022c) Wang, Y., Xu, Z., Tian, J., Luo, J., Shi, Z., Zhang, Y.,
    Fan, J., He, Z., 2022c. Cross-domain few-shot learning for rare-disease skin lesion
    segmentation, in: ICASSP 2022-2022 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), IEEE. pp. 1086--1090.'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022d) Wang, Z., Lyu, J., Luo, W., Tang, X., 2022d. Superpixel
    inpainting for self-supervised skin lesion segmentation from dermoscopic images,
    in: 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), IEEE.
    pp. 1--4.'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Warfield and Wells (2004) Warfield, S. K. anbd Zou, K.H., Wells, W.M., 2004.
    Simultaneous Truth and Performance Level Estimation (STAPLE): An Algorithm for
    the Validation of Image Segmentation. IEEE Transactions on Medical Imaging 23,
    903--921.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2019) Wei, Z., Song, H., Chen, L., Li, Q., Han, G., 2019. Attention-based
    DenseUnet network with adversarial training for skin lesion segmentation. IEEE
    Access 7, 136616--136629.
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng et al. (2019) Weng, Y., Zhou, T., Li, Y., Qiu, X., 2019. NAS-Unet: Neural
    architecture search for medical image segmentation. IEEE Access 7, 44247--44257.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wibowo et al. (2021) Wibowo, A., Purnama, S.R., Wirawan, P.W., Rasyidi, H.,
    2021. Lightweight encoder-decoder model for automatic skin lesion segmentation.
    Informatics in Medicine Unlocked , 100640.
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022a) Wu, H., Chen, S., Chen, G., Wang, W., Lei, B., Wen, Z., 2022a.
    FAT-Net: Feature adaptive Transformers for automated skin lesion segmentation.
    Medical Image Analysis 76, 102327.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Wu, H., Pan, J., Li, Z., Wen, Z., Qin, J., 2020. Automated
    skin lesion segmentation via an adaptive dual attention module. IEEE Transactions
    on Medical Imaging 40, 357--370.
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022b) Wu, J., Fang, H., Shang, F., Yang, D., Wang, Z., Gao, J.,
    Yang, Y., Xu, Y., 2022b. SeATrans: Learning segmentation-assisted diagnosis model
    via Transformer, in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer. pp. 677--687.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022c) Wu, Y., Zeng, D., Xu, X., Shi, Y., Hu, J., 2022c. FairPrune:
    Achieving fairness through pruning for dermatological disease diagnosis, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 743--753.'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020a) Xie, F., Yang, J., Liu, J., Jiang, Z., Zheng, Y., Wang, Y.,
    2020a. Skin lesion segmentation using high-resolution convolutional neural network.
    Computer methods and programs in biomedicine 186, 105241.
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020b) Xie, Y., Zhang, J., Xia, Y., Shen, C., 2020b. A mutual bootstrapping
    model for automated skin lesion segmentation and classification. IEEE Transactions
    on Medical Imaging 39, 2482--2493.
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2021) Xie, Z., Tu, E., Zheng, H., Gu, Y., Yang, J., 2021. Semi-supervised
    skin lesion segmentation with learning model confidence, in: ICASSP 2021-2021
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE. pp. 1135--1139.'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Xu, R., Wang, C., Xu, S., Meng, W., Zhang, X., 2021. DC-Net:
    Dual context network for 2D medical image segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 503--513.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2018) Xue, Y., Xu, T., Huang, X., 2018. Adversarial learning with
    multi-scale loss for skin lesion segmentation, in: 2018 IEEE 15th International
    Symposium on Biomedical Imaging (ISBI 2018), IEEE. pp. 859--863.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2019) Yan, Y., Kawahara, J., Hamarneh, G., 2019. Melanoma recognition
    via visual attention, in: International Conference on Information Processing in
    Medical Imaging, Springer. pp. 793--804.'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2021) Yang, C.H., Ren, J.H., Huang, H.C., Chuang, L.Y., Chang,
    P.Y., 2021. Deep hybrid convolutional neural network for segmentation of melanoma
    skin lesion. Computational Intelligence and Neuroscience 2021.
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2018) Yang, X., Li, H., Wang, L., Yeo, S.Y., Su, Y., Zeng, Z.,
    2018. Skin lesion analysis by multi-target deep neural networks, in: 2018 40th
    Annual International Conference of the IEEE Engineering in Medicine and Biology
    Society (EMBC), IEEE. pp. 1263--1266.'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yerushalmy (1947) Yerushalmy, J., 1947. Statistical problems in assessing methods
    of medical diagnosis, with special reference to X-ray techniques. Public Health
    Reports (1896-1970) 62, 1432--1449.
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. (2019) Yi, X., Walia, E., Babyn, P., 2019. Generative Adversarial
    Network in Medical Imaging: A Review. Medical Image Analysis 58, 101552.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2022) Yu, B., Yu, L., Tian, S., Wu, W., Zhang, D., Kang, X., 2022.
    mCA-Net: modified comprehensive attention convolutional neural network for skin
    lesion segmentation. Computer Methods in Biomechanics and Biomedical Engineering:
    Imaging & Visualization 10, 85--95.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu and Koltun (2016) Yu, F., Koltun, V., 2016. Multi-scale context aggregation
    by dilated convolutions. international conference on learning representations
    .
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2017a) Yu, L., Chen, H., Dou, Q., Qin, J., Heng, P.A., 2017a. Automated
    melanoma recognition in dermoscopy images via very deep residual networks. IEEE
    transactions on medical imaging 36, 994--1004.
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2017b) Yu, Y., Gong, Z., Zhong, P., Shan, J., 2017b. Unsupervised
    representation learning with deep convolutional neural network for remote sensing
    images, in: International Conference on Image and Graphics, pp. 97--108.'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2017) Yuan, Y., Chao, M., Lo, Y.C., 2017. Automatic skin lesion
    segmentation using deep fully convolutional networks with jaccard distance. IEEE
    transactions on medical imaging 36, 1876--1886.
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan and Lo (2019) Yuan, Y., Lo, Y.C., 2019. Improving Dermoscopic Image Segmentation
    with Enhanced Convolutional-Deconvolutional Networks. IEEE Journal of Biomedical
    and Health Informatics 23, 519--526.
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zafar et al. (2020) Zafar, K., Gilani, S.O., Waris, A., Ahmed, A., Jamil, M.,
    Khan, M.N., Sohail Kashif, A., 2020. Skin lesion segmentation from dermoscopic
    images using convolutional neural network. Sensors 20, 1601.
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng and Zheng (2018) Zeng, G., Zheng, G., 2018. Multi-scale fully convolutional
    densenets for automated skin lesion segmentation in dermoscopy images, in: International
    Conference Image Analysis and Recognition, Springer. pp. 513--521.'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019a) Zhang, G., Shen, X., Chen, S., Liang, L., Luo, Y., Yu,
    J., Lu, J., 2019a. DSM: A deep supervised multi-scale network learning for skin
    cancer segmentation. IEEE Access 7, 140936--140945.'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2008) Zhang, H., Fritts, J.E., Goldman, S.A., 2008. Image Segmentation
    Evaluation: A Survey of Unsupervised Methods. Computer Vision and Image Understanding
    110, 260--280.'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020a) Zhang, J., Petitjean, C., Ainouz, S., 2020a. Kappa loss
    for skin lesion segmentation in fully convolutional network, in: 2020 IEEE 17th
    International Symposium on Biomedical Imaging (ISBI), IEEE. pp. 2001--2004.'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Zhang, L., Tanno, R., Bronik, K., Jin, C., Nachev, P.,
    Barkhof, F., Ciccarelli, O., Alexander, D.C., 2020b. Learning to segment when
    experts disagree, in: International Conference on Medical Image Computing and
    Computer-Assisted Intervention, Springer. pp. 179--190.'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019b) Zhang, L., Yang, G., Ye, X., 2019b. Automatic skin lesion
    segmentation by coupling deep fully convolutional networks and shallow network
    with textons. Journal of Medical Imaging 6, 024001.
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021a) Zhang, R., Liu, S., Yu, Y., Li, G., 2021a. Self-supervised
    correction learning for semi-supervised biomedical image segmentation, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 134--144.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Zhang, X., Zhou, X., Lin, M., Sun, J., 2018. ShuffleNet:
    An Extremely Efficient Convolutional Neural Network for Mobile Devices, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6848--6856.'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022a) Zhang, Y., Chen, Z., Yu, H., Yao, X., Li, H., 2022a. Feature
    fusion for segmentation and classification of skin lesions, in: 2022 IEEE 19th
    International Symposium on Biomedical Imaging (ISBI), IEEE. pp. 1--5.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021b) Zhang, Y., Liu, H., Hu, Q., 2021b. TransFuse: Fusing Transformers
    and CNNs for medical image segmentation, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer. pp. 14--24.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Yang (2022) Zhang, Y., Yang, Q., 2022. A survey on multi-task learning.
    IEEE Transactions on Knowledge and Data Engineering .
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Zhang, Z., Tian, C., Gao, X., Wang, C., Feng, X., Bai,
    H.X., Jiao, Z., 2022b. Dynamic prototypical feature representation learning framework
    for semi-supervised skin lesion segmentation. Neurocomputing 507, 369--382.
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2021) Zhao, C., Shuai, R., Ma, L., Liu, W., Wu, M., 2021. Segmentation
    of dermoscopy images based on deformable 3D convolution and ResU-NeXt++. Medical
    & Biological Engineering & Computing 59, 1815--1832.
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020) Zhao, H., Jia, J., Koltun, V., 2020. Exploring self-attention
    for image recognition, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 10076--10085.'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2017) Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., 2017. Pyramid
    Scene Parsing Network, in: Proceedings of the 2017 IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 2881--2890.'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2022a) Zhao, M., Kawahara, J., Abhishek, K., Shamanian, S., Hamarneh,
    G., 2022a. Skin3d: Detection and longitudinal tracking of pigmented skin lesions
    in 3D total-body textured meshes. Medical Image Analysis 77, 102329.'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2022b) Zhao, Z., Lu, W., Zeng, Z., Xu, K., Veeravalli, B., Guan,
    C., 2022b. Self-supervised assisted active learning for skin lesion segmentation,
    in: 2022 44th Annual International Conference of the IEEE Engineering in Medicine
    & Biology Society (EMBC), IEEE. pp. 5043--5046. URL: [https://doi.org/10.1109/embc48229.2022.9871734](https://doi.org/10.1109/embc48229.2022.9871734),
    doi:[10.1109/embc48229.2022.9871734](http://dx.doi.org/10.1109/embc48229.2022.9871734).'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2021) Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y.,
    Fu, Y., Feng, J., Xiang, T., Torr, P.H., et al., 2021. Rethinking semantic segmentation
    from a sequence-to-sequence perspective with transformers, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6881--6890.'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2016) Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba,
    A., 2016. Learning deep features for discriminative localization, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 2921--2929.'
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020) Zhu, L., Feng, S., Zhu, W., Chen, X., 2020. ASNet: An adaptive
    scale network for skin lesion segmentation in dermoscopy images, in: Medical Imaging
    2020: Biomedical Applications in Molecular, Structural, and Functional Imaging,
    International Society for Optics and Photonics. SPIE. pp. 226--231.'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu (2020) Zhu, Q., 2020. On the Performance of Matthews Correlation Coefficient
    (MCC) for Imbalanced Dataset. Pattern Recognition Letters 136, 71--80.
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zijdenbos et al. (1994) Zijdenbos, A.P., Dawant, B.M., Margolin, R.A., Palmer,
    A.C., 1994. Morphometric Analysis of White Matter Lesions in MR Images: Method
    and Validation. IEEE Transactions on Medical Imaging 13, 716--724.'
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zortea et al. (2011) Zortea, M., Skrøvseth, S.O., Schopf, T.R., Kirchesch, H.M.,
    Godtliebsen, F., 2011. Automatic segmentation of dermoscopic images by iterative
    classification. International journal of biomedical imaging 2011.
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2004) Zou, K.H., Warfield, S.K., Bharatha, A., Tempany, C.M., Kaus,
    M.R., Haker, S.J., Wells III, W.M., Jolesz, F.A., Kikinis, R., 2004. Statistical
    Validation of Image Segmentation Quality Based on a Spatial Overlap Index. Academic
    Radiology 11, 178--189.
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zunair and Hamza (2021) Zunair, H., Hamza, A.B., 2021. Sharp U-Net: Depthwise
    convolutional network for biomedical image segmentation. Computers in Biology
    and Medicine 136, 104699.'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
