- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:06:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:57
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1812.04202] Deep Learning on Graphs: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1812.04202] 图上的深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1812.04202](https://ar5iv.labs.arxiv.org/html/1812.04202)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1812.04202](https://ar5iv.labs.arxiv.org/html/1812.04202)
- en: 'Deep Learning on Graphs: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图上的深度学习：综述
- en: Ziwei Zhang, Peng Cui and Wenwu Zhu Z. Zhang, P. Cui, and W. Zhu are with the
    Department of Computer Science and Technology, Tsinghua University, Beijing, China.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张子伟、崔鹏和朱文武 Z. Zhang、P. Cui 和 W. Zhu 现为清华大学计算机科学与技术系，北京，中国。
- en: 'E-mail: zw-zhang16@mails.tsinghua.edu.cn, cuip@tsinghua.edu.cn,'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：zw-zhang16@mails.tsinghua.edu.cn, cuip@tsinghua.edu.cn,
- en: wwzhu@tsinghua.edu.cn. P. Cui and W. Zhu are corresponding authors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: wwzhu@tsinghua.edu.cn。P. Cui 和 W. Zhu 是通讯作者。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep learning has been shown to be successful in a number of domains, ranging
    from acoustics, images, to natural language processing. However, applying deep
    learning to the ubiquitous graph data is non-trivial because of the unique characteristics
    of graphs. Recently, substantial research efforts have been devoted to applying
    deep learning methods to graphs, resulting in beneficial advances in graph analysis
    techniques. In this survey, we comprehensively review the different types of deep
    learning methods on graphs. We divide the existing methods into five categories
    based on their model architectures and training strategies: graph recurrent neural
    networks, graph convolutional networks, graph autoencoders, graph reinforcement
    learning, and graph adversarial methods. We then provide a comprehensive overview
    of these methods in a systematic manner mainly by following their development
    history. We also analyze the differences and compositions of different methods.
    Finally, we briefly outline the applications in which they have been used and
    discuss potential future research directions.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已在多个领域取得成功，包括声学、图像和自然语言处理。然而，由于图的独特特性，将深度学习应用于普遍存在的图数据并非易事。近年来，大量研究致力于将深度学习方法应用于图，从而在图分析技术上取得了有益的进展。在这项综述中，我们全面回顾了图上不同类型的深度学习方法。我们根据模型架构和训练策略将现有方法分为五类：图递归神经网络、图卷积网络、图自编码器、图强化学习和图对抗方法。然后，我们系统地概述了这些方法，主要按照其发展历程进行。我们还分析了不同方法的差异和组成。最后，我们简要概述了这些方法的应用，并讨论了潜在的未来研究方向。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Graph Data, Deep Learning, Graph Neural Network, Graph Convolutional Network,
    Graph Autoencoder.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据、深度学习、图神经网络、图卷积网络、图自编码器。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Over the past decade, deep learning has become the “crown jewel” of artificial
    intelligence and machine learning [[1](#bib.bib1)], showing superior performance
    in acoustics [[2](#bib.bib2)], images [[3](#bib.bib3)] and natural language processing [[4](#bib.bib4)],
    etc. The expressive power of deep learning to extract complex patterns from underlying
    data is well recognized. On the other hand, graphs¹¹1Graphs are also called *networks*
    such as in social networks. In this paper, we use two terms interchangeably. are
    ubiquitous in the real world, representing objects and their relationships in
    varied domains, including social networks, e-commerce networks, biology networks,
    traffic networks, and so on. Graphs are also known to have complicated structures
    that can contain rich underlying values [[5](#bib.bib5)]. As a result, how to
    utilize deep learning methods to analyze graph data has attracted considerable
    research attention over the past few years. This problem is non-trivial because
    several challenges exist in applying traditional deep learning architectures to
    graphs:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，深度学习已成为人工智能和机器学习的“皇冠上的明珠”[[1](#bib.bib1)]，在声学[[2](#bib.bib2)]、图像[[3](#bib.bib3)]和自然语言处理[[4](#bib.bib4)]等领域表现优越。深度学习提取潜在数据中的复杂模式的表达能力得到广泛认可。另一方面，图¹¹1图也被称为*网络*，例如社交网络。本文中，我们将这两个术语交替使用。在现实世界中，图是普遍存在的，表示不同领域中的对象及其关系，包括社交网络、电子商务网络、生物网络、交通网络等。图的结构复杂，包含丰富的潜在值[[5](#bib.bib5)]。因此，如何利用深度学习方法分析图数据在过去几年中引起了大量研究关注。这一问题并非易事，因为将传统深度学习架构应用于图存在多个挑战：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Irregular structures of graphs. Unlike images, audio, and text, which have a
    clear grid structure, graphs have irregular structures, making it hard to generalize
    some of the basic mathematical operations to graphs [[6](#bib.bib6)]. For example,
    defining convolution and pooling operations, which are the fundamental operations
    in convolutional neural networks (CNNs), for graph data is not straightforward.
    This problem is often referred to as the geometric deep learning problem [[7](#bib.bib7)].
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图的非规则结构。与图像、音频和文本具有清晰的网格结构不同，图具有非规则结构，这使得将一些基本数学操作推广到图上变得困难[[6](#bib.bib6)]。例如，为图数据定义卷积和池化操作，这些是卷积神经网络（CNNs）中的基本操作，并不是很简单。这个问题通常被称为几何深度学习问题[[7](#bib.bib7)]。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Heterogeneity and diversity of graphs. A graph itself can be complicated, containing
    diverse types and properties. For example, graphs can be heterogeneous or homogenous,
    weighted or unweighted, and signed or unsigned. In addition, the tasks of graphs
    also vary widely, ranging from node-focused problems such as node classification
    and link prediction to graph-focused problems such as graph classification and
    graph generation. These diverse types, properties, and tasks require different
    model architectures to tackle specific problems.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图的异质性和多样性。图本身可能非常复杂，包含多种类型和属性。例如，图可以是异质的或同质的，加权或未加权，以及有符号或无符号。此外，图的任务也广泛多样，从节点分类和链接预测等以节点为中心的问题，到图分类和图生成等以图为中心的问题。这些多样的类型、属性和任务需要不同的模型架构来解决特定问题。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Large-scale graphs. In the big-data era, real graphs can easily have millions
    or billions of nodes and edges; some well-known examples are social networks and
    e-commerce networks [[8](#bib.bib8)]. Therefore, how to design scalable models,
    preferably models that have a linear time complexity with respect to the graph
    size, is a key problem.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大规模图。在大数据时代，真实的图可以拥有数百万或数十亿的节点和边；一些著名的例子包括社交网络和电子商务网络[[8](#bib.bib8)]。因此，如何设计可扩展的模型，最好是与图大小成线性时间复杂度的模型，是一个关键问题。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Incorporating interdisciplinary knowledge. Graphs are often connected to other
    disciplines, such as biology, chemistry, and social sciences. This interdisciplinary
    nature provides both opportunities and challenges: domain knowledge can be leveraged
    to solve specific problems but integrating domain knowledge can complicate model
    designs. For example, when generating molecular graphs, the objective function
    and chemical constraints are often non-differentiable; therefore gradient-based
    training methods cannot easily be applied.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨学科知识的融合。图常常与其他学科相关联，如生物学、化学和社会科学。这种跨学科的性质带来了机会和挑战：可以利用领域知识来解决特定问题，但整合领域知识可能会使模型设计复杂化。例如，在生成分子图时，目标函数和化学约束通常是不可微分的；因此，基于梯度的训练方法难以应用。
- en: To tackle these challenges, tremendous efforts have been made in this area,
    resulting in a rich literature of related papers and methods. The adopted architectures
    and training strategies also vary greatly, ranging from supervised to unsupervised
    and from convolutional to recursive. However, to the best of our knowledge, little
    effort has been made to systematically summarize the differences and connections
    between these diverse methods.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，在这一领域已经付出了巨大的努力，产生了丰富的相关文献和方法。采用的架构和训练策略也大相径庭，从监督到无监督，从卷积到递归。然而，据我们所知，尚未有系统总结这些不同方法之间差异和联系的工作。
- en: '![Refer to caption](img/761eb80733568f5b76cfa0590e399272.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/761eb80733568f5b76cfa0590e399272.png)'
- en: 'Figure 1: A categorization of deep learning methods on graphs. We divide the
    existing methods into five categories: graph recurrent neural networks, graph
    convolutional networks, graph autoencoders, graph reinforcement learning, and
    graph adversarial methods.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：图上的深度学习方法的分类。我们将现有方法分为五类：图的递归神经网络、图卷积网络、图自编码器、图强化学习和图对抗方法。
- en: 'TABLE I: Main Distinctions among Deep Learning Methods on Graphs'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：图上的深度学习方法主要区别
- en: '| Category | Basic Assumptions/Aims | Main Functions |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 基本假设/目标 | 主要功能 |'
- en: '| --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Graph recurrent neural networks | Recursive and sequential patterns of graphs
    | Definitions of states for nodes or graphs |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 图的递归神经网络 | 图的递归和顺序模式 | 节点或图的状态定义 |'
- en: '| Graph convolutional networks | Common local and global structural patterns
    of graphs | Graph convolution and readout operations |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 图形卷积网络 | 图形的常见局部和全局结构模式 | 图形卷积和读取操作 |'
- en: '| Graph autoencoders | Low-rank structures of graphs | Unsupervised node representation
    learning |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 图形自编码器 | 图形的低秩结构 | 无监督的节点表示学习 |'
- en: '| Graph reinforcement learning | Feedbacks and constraints of graph tasks |
    Graph-based actions and rewards |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 图形强化学习 | 图形任务的反馈和约束 | 基于图形的动作和奖励 |'
- en: '| Graph adversarial methods | The generalization ability and robustness of
    graph-based models | Graph adversarial trainings and attacks |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 图形对抗方法 | 基于图形模型的泛化能力和鲁棒性 | 图形对抗训练和攻击 |'
- en: 'In this paper, we try to fill this knowledge gap by comprehensively reviewing
    deep learning methods on graphs. Specifically, as shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Deep Learning on Graphs: A Survey"), we divide the existing
    methods into five categories based on their model architectures and training strategies:
    graph recurrent neural networks (Graph RNNs), graph convolutional networks (GCNs),
    graph autoencoders (GAEs), graph reinforcement learning (Graph RL), and graph
    adversarial methods. We summarize some of the main characteristics of these categories
    in Table [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Deep Learning on Graphs: A Survey")
    based on the following high-level distinctions. Graph RNNs capture recursive and
    sequential patterns of graphs by modeling states at either the node-level or the
    graph-level. GCNs define convolution and readout operations on irregular graph
    structures to capture common local and global structural patterns. GAEs assume
    low-rank graph structures and adopt unsupervised methods for node representation
    learning. Graph RL defines graph-based actions and rewards to obtain feedbacks
    on graph tasks while following constraints. Graph adversarial methods adopt adversarial
    training techniques to enhance the generalization ability of graph-based models
    and test their robustness by adversarial attacks.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们试图通过全面回顾图形上的深度学习方法来填补这一知识空白。具体来说，如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep Learning on Graphs: A Survey")所示，我们根据模型架构和训练策略将现有的方法分为五类：图形递归神经网络（Graph
    RNNs）、图形卷积网络（GCNs）、图形自编码器（GAEs）、图形强化学习（Graph RL）和图形对抗方法。我们在表 [I](#S1.T1 "TABLE
    I ‣ 1 Introduction ‣ Deep Learning on Graphs: A Survey")中基于以下高层次的区别总结了这些类别的一些主要特点。图形递归神经网络通过在节点级或图级建模状态来捕捉图形的递归和序列模式。图形卷积网络在不规则的图形结构上定义卷积和读取操作，以捕捉常见的局部和全局结构模式。图形自编码器假设图形结构是低秩的，并采用无监督的方法进行节点表示学习。图形强化学习定义基于图形的动作和奖励，以在遵循约束的同时获得图形任务的反馈。图形对抗方法采用对抗训练技术来增强基于图形模型的泛化能力，并通过对抗攻击测试其鲁棒性。'
- en: In the following sections, we provide a comprehensive and detailed overview
    of these methods, mainly by following their development history and the various
    ways these methods solve the challenges posed by graphs. We also analyze the differences
    between these models and delve into how to composite different architectures.
    Finally, we briefly outline the applications of these models, introduce several
    open libraries, and discuss potential future research directions. In the appendix,
    we provide a source code repository, analyze the time complexity of various methods
    discussed in the paper, and summarize some common applications.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们提供了这些方法的全面而详细的概述，主要通过追溯它们的发展历史以及这些方法解决图形问题的各种方式来进行分析。我们还分析了这些模型之间的差异，并探讨了如何组合不同的架构。最后，我们简要概述了这些模型的应用，介绍了几个开放库，并讨论了潜在的未来研究方向。在附录中，我们提供了源代码库，分析了论文中讨论的各种方法的时间复杂度，并总结了一些常见的应用。
- en: Related works. Several previous surveys are related to our paper. Bronstein et al. [[7](#bib.bib7)]
    summarized some early GCN methods as well as CNNs on manifolds and studied them
    comprehensively through geometric deep learning. Battaglia et al. [[9](#bib.bib9)]
    summarized how to use GNNs and GCNs for relational reasoning using a unified framework
    called graph networks, Lee et al. [[10](#bib.bib10)] reviewed the attention models
    for graphs, Zhang et al. [[11](#bib.bib11)] summarized some GCNs, and Sun et al. [[12](#bib.bib12)]
    briefly surveyed adversarial attacks on graphs. Our work differs from these previous
    works in that we systematically and comprehensively review different deep learning
    architectures on graphs rather than focusing on one specific branch. Concurrent
    to our work, Zhou et al. [[13](#bib.bib13)] and Wu  et al. [[14](#bib.bib14)]
    surveyed this field from different viewpoints and categorizations. Specifically,
    neither of their works consider graph reinforcement learning or graph adversarial
    methods, which are covered in this paper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作。若干先前的调查与我们的论文相关。Bronstein 等人 [[7](#bib.bib7)] 总结了一些早期的GCN方法以及流形上的CNN，并通过几何深度学习进行了全面研究。Battaglia 等人 [[9](#bib.bib9)]
    总结了如何使用GNN和GCN进行关系推理，采用了名为图网络的统一框架，Lee 等人 [[10](#bib.bib10)] 回顾了图的注意力模型，Zhang 等人 [[11](#bib.bib11)]
    总结了一些GCN，而Sun 等人 [[12](#bib.bib12)] 简要调查了对图的对抗攻击。我们的工作与这些先前的工作不同之处在于，我们系统地、全面地回顾了图上的不同深度学习架构，而不是专注于某个特定的分支。与我们的工作同时进行的还有Zhou 等人 [[13](#bib.bib13)]
    和Wu 等人 [[14](#bib.bib14)] 从不同的观点和分类进行调查。具体而言，他们的工作都没有考虑图强化学习或图对抗方法，而这些在本文中都有涉及。
- en: Another closely related topic is network embedding, aiming to embed nodes into
    a low-dimensional vector space [[15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)].
    The main distinction between network embedding and our paper is that we focus
    on how different deep learning models are applied to graphs, and network embedding
    can be recognized as a concrete application example that uses some of these models
    (and it uses non-deep-learning methods as well).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个密切相关的主题是网络嵌入，旨在将节点嵌入到低维向量空间中 [[15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)]。网络嵌入与我们论文的主要区别在于，我们关注的是不同深度学习模型如何应用于图，而网络嵌入可以被看作是一个具体的应用示例，它使用了这些模型中的一些（同时也使用了非深度学习方法）。
- en: The rest of this paper is organized as follows. In Section 2, we introduce the
    notations used in this paper and provide preliminaries. Then, we review Graph
    RNNs, GCNs, GAEs, Graph RL, and graph adversarial methods in Section 3 to 7, respectively.
    We conclude with a discussion in Section 8.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：在第2节中，我们介绍了本文中使用的符号并提供了初步知识。然后，在第3至第7节中，我们分别回顾了图RNN、GCN、GAE、图RL和图对抗方法。最后，在第8节中，我们进行总结讨论。
- en: 2 Notations and Preliminaries
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 符号和初步知识
- en: Notations. In this paper, a graph²²2We consider only graphs without self-loops
    or multiple edges. is represented as $G=\left(V,E\right)$ where $V=\left\{v_{1},...,v_{N}\right\}$
    is a set of $N=\left|V\right|$ nodes and $E\subseteq V\times V$ is a set of $M=\left|E\right|$
    edges between nodes. We use $\mathbf{A}\in\mathbb{R}^{N\times N}$ to denote the
    adjacency matrix, whose $i^{th}$ row, $j^{th}$ column, and an element are denoted
    as $\mathbf{A}(i,:),\mathbf{A}(:,j),\mathbf{A}(i,j)$, respectively. The graph
    can be either directed or undirected and weighted or unweighted. In this paper,
    we mainly consider unsigned graphs; therefore, $\mathbf{A}(i,j)\geq 0$. Signed
    graphs will be discussed in future research directions. We use $\mathbf{F}^{V}$
    and $\mathbf{F}^{E}$ to denote features of nodes and edges, respectively. For
    other variables, we use bold uppercase characters to denote matrices and bold
    lowercase characters to denote vectors, e.g., a matrix $\mathbf{X}$ and a vector
    $\mathbf{x}$. The transpose of a matrix is denoted as $\mathbf{X}^{T}$ and the
    element-wise multiplication is denoted as $\mathbf{X}_{1}\odot\mathbf{X}_{2}$.
    Functions are marked with curlicues, e.g., $\mathcal{F}(\cdot)$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 符号说明。本文中，图²²2我们只考虑没有自环或多重边的图。表示为 $G=\left(V,E\right)$，其中 $V=\left\{v_{1},...,v_{N}\right\}$
    是 $N=\left|V\right|$ 个节点的集合，$E\subseteq V\times V$ 是 $M=\left|E\right|$ 个节点之间的边的集合。我们使用
    $\mathbf{A}\in\mathbb{R}^{N\times N}$ 来表示邻接矩阵，其第 $i$ 行、第 $j$ 列和一个元素分别表示为 $\mathbf{A}(i,:),\mathbf{A}(:,j),\mathbf{A}(i,j)$。图可以是有向的或无向的，可以是加权的或无权的。本文中，我们主要考虑无符号图；因此，$\mathbf{A}(i,j)\geq
    0$。带符号的图将在未来的研究方向中讨论。我们使用 $\mathbf{F}^{V}$ 和 $\mathbf{F}^{E}$ 来分别表示节点和边的特征。对于其他变量，我们使用粗体大写字母表示矩阵，使用粗体小写字母表示向量，例如，矩阵
    $\mathbf{X}$ 和向量 $\mathbf{x}$。矩阵的转置表示为 $\mathbf{X}^{T}$，逐元素乘法表示为 $\mathbf{X}_{1}\odot\mathbf{X}_{2}$。函数用花体字母表示，例如，$\mathcal{F}(\cdot)$。
- en: To better illustrate the notations, we take social networks as an example. Each
    node $v_{i}\in V$ corresponds to a user, and the edges $E$ correspond to relations
    between users. The profiles of users (e.g., age, gender, and location) can be
    represented as node features $\mathbf{F}^{V}$ and interaction data (e.g., sending
    messages and comments) can be represented as edge features $\mathbf{F}^{E}$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地说明这些符号，我们以社交网络为例。每个节点 $v_{i}\in V$ 对应于一个用户，而边 $E$ 对应于用户之间的关系。用户的个人资料（例如，年龄、性别和位置）可以表示为节点特征
    $\mathbf{F}^{V}$，而交互数据（例如，发送消息和评论）可以表示为边特征 $\mathbf{F}^{E}$。
- en: 'TABLE II: A Table for Commonly Used Notations'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 常用符号表'
- en: '| $G=(V,E)$ | A graph |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| $G=(V,E)$ | 图 |'
- en: '| $N,M$ | The number of nodes and edges |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| $N,M$ | 节点和边的数量 |'
- en: '| $V=\left\{v_{1},...,v_{N}\right\}$ | The set of nodes |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| $V=\left\{v_{1},...,v_{N}\right\}$ | 节点集合 |'
- en: '| $\mathbf{F}^{V},\mathbf{F}^{E}$ | The attributes/features of nodes and edges
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{F}^{V},\mathbf{F}^{E}$ | 节点和边的属性/特征 |'
- en: '| $\mathbf{A}$ | The adjacency matrix |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{A}$ | 邻接矩阵 |'
- en: '| $\mathbf{D}(i,i)=\sum_{j}\mathbf{A}(i,j)$ | The diagonal degree matrix |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{D}(i,i)=\sum_{j}\mathbf{A}(i,j)$ | 对角度矩阵 |'
- en: '| $\mathbf{L}=\mathbf{D}-\mathbf{A}$ | The Laplacian matrix |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{L}=\mathbf{D}-\mathbf{A}$ | 拉普拉斯矩阵 |'
- en: '| $\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{T}=\mathbf{L}$ | The eigendecomposition
    of $\mathbf{L}$ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{T}=\mathbf{L}$ | $\mathbf{L}$ 的特征分解
    |'
- en: '| $\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$ | The transition matrix |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$ | 转移矩阵 |'
- en: '| $\mathcal{N}_{k}(i),\mathcal{N}(i)$ | The k-step and 1-step neighbors of
    $v_{i}$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{N}_{k}(i),\mathcal{N}(i)$ | $v_{i}$ 的 k 步邻居和 1 步邻居 |'
- en: '| $\mathbf{H}^{l}$ | The hidden representation in the $l^{th}$ layer |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{H}^{l}$ | 第 $l$ 层的隐藏表示 |'
- en: '| $f_{l}$ | The dimensionality of $\mathbf{H}^{l}$ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| $f_{l}$ | $\mathbf{H}^{l}$ 的维度 |'
- en: '| $\rho(\cdot)$ | Some non-linear activation function |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| $\rho(\cdot)$ | 一些非线性激活函数 |'
- en: '| $\mathbf{X}_{1}\odot\mathbf{X}_{2}$ | The element-wise multiplication |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{X}_{1}\odot\mathbf{X}_{2}$ | 逐元素乘法 |'
- en: '| $\mathbf{\Theta}$ | Learnable parameters |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{\Theta}$ | 可学习参数 |'
- en: '| $s$ | The sample size |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| $s$ | 样本大小 |'
- en: Preliminaries. The Laplacian matrix of an undirected graph is defined as $\mathbf{L}=\mathbf{D}-\mathbf{A}$,
    where $\mathbf{D}\in\mathbb{R}^{N\times N}$ is a diagonal degree matrix with $\mathbf{D}(i,i)=\sum_{j}\mathbf{A}(i,j)$.
    Its eigendecomposition is denoted as $\mathbf{L}=\mathbf{Q\Lambda Q^{T}}$, where
    $\mathbf{\Lambda}\in\mathbb{R}^{N\times N}$ is a diagonal matrix of eigenvalues
    sorted in ascending order and $\mathbf{Q}\in\mathbb{R}^{N\times N}$ are the corresponding
    eigenvectors. The transition matrix is defined as $\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$,
    where $\mathbf{P}(i,j)$ represents the probability of a random walk starting from
    node $v_{i}$ landing at node $v_{j}$. The $k$-step neighbors of node $v_{i}$ are
    defined as $\mathcal{N}_{k}(i)=\left\{j|\mathcal{D}(i,j)\leq k\right\}$, where
    $\mathcal{D}(i,j)$ is the shortest distance from node $v_{i}$ to $v_{j}$, i.e.
    $\mathcal{N}_{k}(i)$ is a set of nodes reachable from node $v_{i}$ within $k$-steps.
    To simplify the notation, we omit the subscript for the immediate neighborhood,
    i.e., $\mathcal{N}(i)=\mathcal{N}_{1}(i)$.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基础知识。无向图的拉普拉斯矩阵定义为 $\mathbf{L}=\mathbf{D}-\mathbf{A}$，其中 $\mathbf{D}\in\mathbb{R}^{N\times
    N}$ 是一个对角度矩阵，满足 $\mathbf{D}(i,i)=\sum_{j}\mathbf{A}(i,j)$。它的特征分解表示为 $\mathbf{L}=\mathbf{Q\Lambda
    Q^{T}}$，其中 $\mathbf{\Lambda}\in\mathbb{R}^{N\times N}$ 是一个按升序排列的特征值对角矩阵，$\mathbf{Q}\in\mathbb{R}^{N\times
    N}$ 是相应的特征向量。转移矩阵定义为 $\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$，其中 $\mathbf{P}(i,j)$
    表示从节点 $v_{i}$ 开始的随机游走到达节点 $v_{j}$ 的概率。节点 $v_{i}$ 的 $k$ 步邻居定义为 $\mathcal{N}_{k}(i)=\left\{j|\mathcal{D}(i,j)\leq
    k\right\}$，其中 $\mathcal{D}(i,j)$ 是从节点 $v_{i}$ 到 $v_{j}$ 的最短距离，即 $\mathcal{N}_{k}(i)$
    是从节点 $v_{i}$ 在 $k$ 步内可以到达的节点集合。为了简化符号，我们省略了立即邻域的下标，即 $\mathcal{N}(i)=\mathcal{N}_{1}(i)$。
- en: 'For a deep learning model, we use superscripts to denote layers, e.g., $\mathbf{H}^{l}$.
    We use $f_{l}$ to denote the dimensionality of the layer $l$ (i.e., $\mathbf{H}^{l}\in\mathbb{R}^{N\times
    f_{l}}$). The sigmoid activation function is defined as $\sigma(x)=1/\left(1+e^{-x}\right)$
    and the rectified linear unit (ReLU) is defined as $\text{ReLU}(x)=max(0,x)$.
    A general element-wise nonlinear activation function is denoted as $\rho(\cdot)$.
    In this paper, unless stated otherwise, we assume all functions are differentiable,
    allowing the model parameters $\mathbf{\Theta}$ to be learned through back-propagation [[18](#bib.bib18)]
    using commonly adopted optimizers such as Adam [[19](#bib.bib19)] and training
    techniques such as dropout [[20](#bib.bib20)]. We denote the sample size as $s$
    if a sampling technique is adopted. We summarize the notations in Table [II](#S2.T2
    "TABLE II ‣ 2 Notations and Preliminaries ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '对于深度学习模型，我们使用上标来表示层次，例如，$\mathbf{H}^{l}$。我们用 $f_{l}$ 来表示层 $l$ 的维度（即，$\mathbf{H}^{l}\in\mathbb{R}^{N\times
    f_{l}}$）。Sigmoid 激活函数定义为 $\sigma(x)=1/\left(1+e^{-x}\right)$，而修正线性单元（ReLU）定义为
    $\text{ReLU}(x)=max(0,x)$。一般的元素级非线性激活函数表示为 $\rho(\cdot)$。在本文中，除非另有说明，我们假设所有函数都是可微的，从而允许通过反向传播[[18](#bib.bib18)]
    使用常见的优化器，如 Adam [[19](#bib.bib19)]，以及训练技术，如 dropout [[20](#bib.bib20)]，来学习模型参数
    $\mathbf{\Theta}$。如果采用了采样技术，我们将样本大小表示为 $s$。我们在表 [II](#S2.T2 "TABLE II ‣ 2 Notations
    and Preliminaries ‣ Deep Learning on Graphs: A Survey") 中总结了符号说明。'
- en: 'The tasks for learning a deep model on graphs can be broadly divided into two
    categories:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在图上学习深度模型的任务大致可以分为两类：
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Node-focused tasks: These tasks are associated with individual nodes in the
    graph. Examples include node classification, link prediction, and node recommendation.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 节点聚焦任务：这些任务与图中的个别节点相关。示例包括节点分类、链接预测和节点推荐。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph-focused tasks: These tasks are associated with the entire graph. Examples
    include graph classification, estimating various graph properties, and generating
    graphs.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图聚焦任务：这些任务与整个图相关。示例包括图分类、估计各种图属性和生成图。
- en: Note that such distinctions are more conceptually than mathematically rigorous.
    Some existing tasks are associated with mesoscopic structures such as community
    detection [[21](#bib.bib21)]. In addition, node-focused problems can sometimes
    be studied as graph-focused problems by transforming the former into egocentric
    networks [[22](#bib.bib22)]. Nevertheless, we will explain the differences in
    algorithm designs for these two categories when necessary.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些区分更多的是概念上的，而非数学上的严格。一些现有的任务与中观结构相关，例如社区检测 [[21](#bib.bib21)]。此外，节点聚焦问题有时可以通过将前者转化为自我中心网络
    [[22](#bib.bib22)] 来作为图聚焦问题进行研究。不过，我们将在必要时解释这两类任务在算法设计上的差异。
- en: 'TABLE III: The Main Characteristics of Graph Recurrent Neural Network (Graph
    RNNs)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：图形递归神经网络（Graph RNNs）的主要特征
- en: '| Category | Method | Recursive/sequential patterns of graphs | Time Complexity
    | Other Improvements |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 图的递归/顺序模式 | 时间复杂度 | 其他改进 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Node-level | GNN [[23](#bib.bib23)] | A recursive definition of node states
    | $O(MI_{f})$ | - |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 节点级别 | GNN [[23](#bib.bib23)] | 节点状态的递归定义 | $O(MI_{f})$ | - |'
- en: '| GGS-NNs [[24](#bib.bib24)] | $O(MT)$ | Sequence outputs |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| GGS-NNs [[24](#bib.bib24)] | $O(MT)$ | 序列输出 |'
- en: '| SSE [[25](#bib.bib25)] | $O(d_{\text{avg}}S)$ | - |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| SSE [[25](#bib.bib25)] | $O(d_{\text{avg}}S)$ | - |'
- en: '| Graph-level | You et al. [[26](#bib.bib26)] | Generate nodes and edges in
    an autoregressive manner | $O(N^{2})$ | - |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 图级别 | You 等人 [[26](#bib.bib26)] | 以自回归方式生成节点和边 | $O(N^{2})$ | - |'
- en: '| DGNN [[27](#bib.bib27)] | Capture the time dynamics of the formation of nodes
    and edges | $O(Md_{\text{avg}})$ | - |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| DGNN [[27](#bib.bib27)] | 捕捉节点和边形成的时间动态 | $O(Md_{\text{avg}})$ | - |'
- en: '| RMGCNN [[28](#bib.bib28)] | Recursively reconstruct the graph | $O(M)$ or
    $O(MN)$ | GCN layers |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| RMGCNN [[28](#bib.bib28)] | 递归重建图 | $O(M)$ 或 $O(MN)$ | GCN 层 |'
- en: '| Dynamic GCN [[29](#bib.bib29)] | Gather node representations in different
    time slices | $O(Mt)$ | GCN layers |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 动态 GCN [[29](#bib.bib29)] | 收集不同时间片的节点表示 | $O(Mt)$ | GCN 层 |'
- en: 3 Graph Recurrent Neural Networks
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 图形递归神经网络
- en: 'Recurrent neural networks (RNNs) such as gated recurrent units (GRU) [[30](#bib.bib30)]
    or long short-term memory (LSTM) [[31](#bib.bib31)] are de facto standards in
    modeling sequential data. In this section, we review Graph RNNs which can capture
    recursive and sequential patterns of graphs. Graph RNNs can be broadly divided
    into two categories: node-level RNNs and graph-level RNNs. The main distinction
    lies in whether the patterns lie at the node-level and are modeled by node states,
    or at the graph-level and are modeled by a common graph state. The main characteristics
    of the methods surveyed are summarized in Table [III](#S2.T3 "TABLE III ‣ 2 Notations
    and Preliminaries ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '循环神经网络（RNNs），如门控递归单元（GRU） [[30](#bib.bib30)] 或长短期记忆（LSTM） [[31](#bib.bib31)]，在建模顺序数据中是事实上的标准。在本节中，我们回顾了能够捕捉图的递归和顺序模式的图形
    RNNs。图形 RNNs 可以大致分为两类：节点级 RNNs 和图级 RNNs。主要区别在于模式是否位于节点级别并由节点状态建模，还是位于图级别并由公共图状态建模。所调查方法的主要特征总结在表 [III](#S2.T3
    "TABLE III ‣ 2 Notations and Preliminaries ‣ Deep Learning on Graphs: A Survey")。'
- en: 3.1 Node-level RNNs
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 节点级 RNNs
- en: 'Node-level RNNs for graphs, which are also referred to as graph neural networks
    (GNNs)³³3Recently, GNNs have also been used to refer to general neural networks
    for graph data. We follow the traditional naming convention and use GNNs to refer
    to this specific type of Graph RNNs., can be dated back to the ”pre-deep-learning”
    era [[32](#bib.bib32), [23](#bib.bib23)]. The idea behind a GNN is simple: to
    encode graph structural information, each node $v_{i}$ is represented by a low-dimensional
    state vector $\mathbf{s}_{i}$. Motivated by recursive neural networks [[33](#bib.bib33)],
    a recursive definition of states is adopted [[23](#bib.bib23)]:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图的节点级 RNNs，也被称为图神经网络（GNNs）³³3 最近，GNNs 也被用来指代图数据的通用神经网络。我们遵循传统命名惯例，将 GNNs 用于指代这种特定类型的图形
    RNNs。可以追溯到“深度学习之前”时代 [[32](#bib.bib32), [23](#bib.bib23)]。GNN 的思想很简单：为了编码图的结构信息，每个节点
    $v_{i}$ 由一个低维状态向量 $\mathbf{s}_{i}$ 表示。受到递归神经网络 [[33](#bib.bib33)] 的启发，采用递归状态定义 [[23](#bib.bib23)]：
- en: '|  | $\mathbf{s}_{i}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}(\mathbf{s}_{i},\mathbf{s}_{j},\mathbf{F}^{V}_{i},\mathbf{F}^{V}_{j},\mathbf{F}^{E}_{i,j}),$
    |  | (1) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{s}_{i}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}(\mathbf{s}_{i},\mathbf{s}_{j},\mathbf{F}^{V}_{i},\mathbf{F}^{V}_{j},\mathbf{F}^{E}_{i,j}),$
    |  | (1) |'
- en: 'where $\mathcal{F}(\cdot)$ is a parametric function to be learned. After obtaining
    $\mathbf{s}_{i}$, another function $\mathcal{O}(\cdot)$ is applied to get the
    final outputs:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}(\cdot)$ 是一个待学习的参数函数。在获得 $\mathbf{s}_{i}$ 后，应用另一个函数 $\mathcal{O}(\cdot)$
    来获取最终输出：
- en: '|  | $\hat{y}_{i}=\mathcal{O}(\mathbf{s}_{i},\mathbf{F}^{V}_{i}).$ |  | (2)
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}_{i}=\mathcal{O}(\mathbf{s}_{i},\mathbf{F}^{V}_{i}).$ |  | (2)
    |'
- en: 'For graph-focused tasks, the authors of [[23](#bib.bib23)] suggested adding
    a special node with unique attributes to represent the entire graph. To learn
    the model parameters, the following semi-supervised⁴⁴4It is called semi-supervised
    because all the graph structures and some subset of the node or graph labels is
    used during training. method is adopted: after iteratively solving Eq. ([1](#S3.E1
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")) to a stable point using the Jacobi method [[34](#bib.bib34)],
    one gradient descent step is performed using the Almeida-Pineda algorithm [[35](#bib.bib35),
    [36](#bib.bib36)] to minimize a task-specific objective function, for example,
    the squared loss between the predicted values and the ground-truth for regression
    tasks; then, this process is repeated until convergence.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图相关任务，[[23](#bib.bib23)]的作者建议添加一个具有独特属性的特殊节点来表示整个图。为了学习模型参数，采用以下半监督⁴⁴4这是所谓的半监督，因为在训练过程中使用了所有的图结构和一部分节点或图标签的方法：在使用雅可比方法[[34](#bib.bib34)]迭代求解方程式([1](#S3.E1
    "在3.1节点级RNNs ‣ 3图形递归神经网络 ‣ 深度学习在图形上的调查"))直到稳定点后，使用 Almeida-Pineda 算法[[35](#bib.bib35),
    [36](#bib.bib36)] 执行一步梯度下降，以最小化任务特定的目标函数，例如回归任务中预测值与真实值之间的平方损失；然后，重复这个过程直到收敛。
- en: 'Using the two simple equations in Eqs. ([1](#S3.E1 "In 3.1 Node-level RNNs
    ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on Graphs: A Survey"))([2](#S3.E2
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")), GNN plays two important roles. In retrospect, a GNN unifies
    some of the early methods used for processing graph data, such as recursive neural
    networks and Markov chains [[23](#bib.bib23)]. Looking toward the future, the
    general idea underlying GNNs has profound inspirations: as will be shown later,
    many state-of-the-art GCNs actually have a formulation similar to Eq. ([1](#S3.E1
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")) and follow the same framework of exchanging information within
    the immediate node neighborhoods. In fact, GNNs and GCNs can be unified into some
    common frameworks, and a GNN is equivalent to a GCN that uses identical layers
    to reach stable states. More discussion will be provided in Section 4.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方程式([1](#S3.E1 "在3.1节点级RNNs ‣ 3图形递归神经网络 ‣ 深度学习在图形上的调查"))([2](#S3.E2 "在3.1节点级RNNs
    ‣ 3图形递归神经网络 ‣ 深度学习在图形上的调查"))中的两个简单方程，GNN 执行两个重要的角色。回顾过去，GNN 统一了用于处理图数据的一些早期方法，例如递归神经网络和马尔可夫链[[23](#bib.bib23)]。展望未来，GNN
    的基本理念具有深远的启示：如后文所示，许多最先进的 GCN 实际上具有与方程式([1](#S3.E1 "在3.1节点级RNNs ‣ 3图形递归神经网络 ‣
    深度学习在图形上的调查"))类似的公式，并遵循在直接节点邻域内交换信息的相同框架。实际上，GNN 和 GCN 可以统一为一些通用框架，而 GNN 相当于一个使用相同层到达稳定状态的
    GCN。更多讨论将在第4节提供。
- en: 'Although they are conceptually important, GNNs have several drawbacks. First,
    to ensure that Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural
    Networks ‣ Deep Learning on Graphs: A Survey")) has a unique solution, $\mathcal{F}(\cdot)$
    must be a “contraction map” [[37](#bib.bib37)], i.e., $\exists\mu,0<\mu<1$ so
    that'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们在概念上很重要，但 GNN 存在几个缺点。首先，为了确保方程式([1](#S3.E1 "在3.1节点级RNNs ‣ 3图形递归神经网络 ‣ 深度学习在图形上的调查"))有唯一解，$\mathcal{F}(\cdot)$
    必须是一个“收缩映射”[[37](#bib.bib37)]，即，$\exists\mu,0<\mu<1$ 使得
- en: '|  | $\left\&#124;\mathcal{F}(x)-\mathcal{F}(y)\right\&#124;\leq\mu\left\&#124;x-y\right\&#124;,\forall
    x,y.$ |  | (3) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\&#124;\mathcal{F}(x)-\mathcal{F}(y)\right\&#124;\leq\mu\left\&#124;x-y\right\&#124;,\forall
    x,y.$ |  | (3) |'
- en: Intuitively, a “contraction map” requires that the distance between any two
    points can only “contract” after the $\mathcal{F}(\cdot)$ operation, which severely
    limits the modeling ability. Second, because many iterations are needed to reach
    a stable state between gradient descend steps, GNNs are computationally expensive.
    Because of these drawbacks and perhaps a lack of computational power (e.g., the
    graphics processing unit, GPU, was not widely used for deep learning in those
    days) and lack of research interests, GNNs did not become a focus of general research.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上讲，“收缩映射”要求任何两点之间的距离在$\mathcal{F}(\cdot)$操作后只能“收缩”，这严重限制了建模能力。其次，由于需要许多迭代步骤才能在梯度下降步骤之间达到稳定状态，GNN
    的计算开销很大。由于这些缺点以及可能的计算能力不足（例如，在那些日子里图形处理单元GPU尚未广泛用于深度学习）和缺乏研究兴趣，GNN 并没有成为一般研究的重点。
- en: 'A notable improvement to GNNs is gated graph sequence neural networks (GGS-NNs) [[24](#bib.bib24)]
    with the following modifications. Most importantly, the authors replaced the recursive
    definition in Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural
    Networks ‣ Deep Learning on Graphs: A Survey")) with a GRU, thus removing the
    “contraction map” requirement and supporting modern optimization techniques. Specifically,
    Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣
    Deep Learning on Graphs: A Survey")) is adapted as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '对于图神经网络（GNNs）的一个显著改进是门控图序列神经网络（GGS-NNs）[[24](#bib.bib24)]，其进行了以下修改。最重要的是，作者将方程式 ([1](#S3.E1
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")) 中的递归定义替换为 GRU，从而去除了“收缩映射”要求并支持现代优化技术。具体而言，方程式 ([1](#S3.E1
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")) 被调整为如下形式：'
- en: '|  | $\mathbf{s}_{i}^{(t)}=(1-\mathbf{z}_{i}^{(t)})\odot\mathbf{s}_{i}^{(t-1)}+\mathbf{z}_{i}^{(t)}\odot\widetilde{\mathbf{s}}_{i}^{(t)},$
    |  | (4) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{s}_{i}^{(t)}=(1-\mathbf{z}_{i}^{(t)})\odot\mathbf{s}_{i}^{(t-1)}+\mathbf{z}_{i}^{(t)}\odot\widetilde{\mathbf{s}}_{i}^{(t)},$
    |  | (4) |'
- en: where $\mathbf{z}$ is calculated by the update gate, $\widetilde{\mathbf{s}}$
    is the candidate for updating, and $t$ is the pseudo time. Second, the authors
    proposed using several such networks operating in sequence to produce sequence
    outputs and showed that their method could be applied to sequence-based tasks
    such as program verification [[38](#bib.bib38)].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{z}$ 是由更新门计算的，$\widetilde{\mathbf{s}}$ 是更新的候选项，而 $t$ 是伪时间。其次，作者提出使用几个这样的网络按序列操作以产生序列输出，并表明他们的方法可以应用于基于序列的任务，如程序验证
    [[38](#bib.bib38)]。
- en: 'SSE [[25](#bib.bib25)] took a similar approach as Eq. ([4](#S3.E4 "In 3.1 Node-level
    RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on Graphs: A Survey")).
    However, instead of using a GRU in the calculation, SSE adopted stochastic fixed-point
    gradient descent to accelerate the training process. This scheme basically alternates
    between calculating steady node states using local neighborhoods and optimizing
    the model parameters, with both calculations in stochastic mini-batches.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'SSE [[25](#bib.bib25)] 采用了与方程式 ([4](#S3.E4 "In 3.1 Node-level RNNs ‣ 3 Graph
    Recurrent Neural Networks ‣ Deep Learning on Graphs: A Survey")) 相似的方法。然而，SSE
    在计算中并没有使用 GRU，而是采用了随机固定点梯度下降以加速训练过程。该方案基本上在计算稳态节点状态和优化模型参数之间交替进行，两者都在随机小批量中计算。'
- en: 3.2 Graph-level RNNs
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 图级 RNNs
- en: In this subsection, we review how to apply RNNs to capture graph-level patterns,
    e.g., temporal patterns of dynamic graphs or sequential patterns at different
    levels of graph granularities. In graph-level RNNs, instead of applying one RNN
    to each node to learn the node states, a single RNN is applied to the entire graph
    to encode the graph states.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们回顾了如何应用 RNN 来捕捉图级模式，例如动态图的时间模式或不同图粒度层次的序列模式。在图级 RNN 中，代替对每个节点应用一个 RNN
    来学习节点状态，应用一个单一的 RNN 对整个图进行编码图状态。
- en: 'You et al. [[26](#bib.bib26)] applied Graph RNNs to the graph generation problem.
    Specifically, they adopted two RNNs: one to generate new nodes and the other to
    generate edges for the newly added node in an autoregressive manner. They showed
    that such hierarchical RNN architectures learn more effectively from input graphs
    than do the traditional rule-based graph generative models while having a reasonable
    time complexity.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你等人 [[26](#bib.bib26)] 将图 RNN 应用于图生成问题。具体而言，他们采用了两个 RNN：一个用于生成新节点，另一个用于以自回归方式生成新添加节点的边。他们表明，这种层次
    RNN 架构比传统的基于规则的图生成模型更有效地从输入图中学习，同时具有合理的时间复杂度。
- en: To capture the temporal information of dynamic graphs, dynamic graph neural
    network (DGNN) [[27](#bib.bib27)] was proposed that used a time-aware LSTM [[39](#bib.bib39)]
    to learn node representations. When a new edge is established, DGNN used the LSTM
    to update the representation of the two interacting nodes as well as their immediate
    neighbors, i.e., considering the one-step propagation effect. The authors showed
    that the time-aware LSTM could model the establishing orders and time intervals
    of edge formations well, which in turn benefited a range of graph applications.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉动态图的时间信息，提出了动态图神经网络（DGNN）[[27](#bib.bib27)]，该网络使用了时间感知 LSTM [[39](#bib.bib39)]
    来学习节点表示。当建立新的边时，DGNN 使用 LSTM 更新两个交互节点及其直接邻居的表示，即考虑了一步传播效应。作者表明，时间感知 LSTM 能够很好地建模边形成的建立顺序和时间间隔，从而有利于一系列图应用。
- en: 'Graph RNN can also be combined with other architectures, such as GCNs or GAEs.
    For example, aiming to tackle the graph sparsity problem, RMGCNN [[28](#bib.bib28)]
    applied an LSTM to the results of GCNs to progressively reconstruct a graph as
    illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Graph-level RNNs ‣ 3 Graph Recurrent
    Neural Networks ‣ Deep Learning on Graphs: A Survey"). By using an LSTM, the information
    from different parts of the graph can diffuse across long ranges without requiring
    as many GCN layers. Dynamic GCN [[29](#bib.bib29)] applied an LSTM to gather the
    results of GCNs from different time slices in dynamic networks to capture both
    the spatial and temporal graph information.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 'Graph RNN 还可以与其他架构结合，如 GCNs 或 GAEs。例如，针对图稀疏问题，RMGCNN [[28](#bib.bib28)] 将 LSTM
    应用于 GCNs 的结果，以逐步重建图，如图 [2](#S3.F2 "Figure 2 ‣ 3.2 Graph-level RNNs ‣ 3 Graph Recurrent
    Neural Networks ‣ Deep Learning on Graphs: A Survey")所示。通过使用 LSTM，图的不同部分的信息可以在长距离上传播，而不需要那么多
    GCN 层。动态 GCN [[29](#bib.bib29)] 应用了 LSTM 来收集动态网络中不同时间片段的 GCN 结果，以捕捉空间和时间图信息。'
- en: '![Refer to caption](img/e9b530023d6cbdcfb3e9a58ae41909dc.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9b530023d6cbdcfb3e9a58ae41909dc.png)'
- en: 'Figure 2: The framework of RMGCNN (reprinted from [[28](#bib.bib28)] with permission).
    RMGCNN includes an LSTM in the GCN to progressively reconstruct the graph. $\mathbf{X}^{t}$,
    $\mathbf{\tilde{X}}^{t}$, and $d\mathbf{X}^{t}$ represent the estimated matrix,
    the outputs of GCNs, and the incremental updates produced by the RNN at iteration
    $t$, respectively. MGCNN refers to a multigraph CNN.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：RMGCNN 的框架（经 [[28](#bib.bib28)]授权转载）。RMGCNN 在 GCN 中包括一个 LSTM 以逐步重建图。$\mathbf{X}^{t}$、$\mathbf{\tilde{X}}^{t}$
    和 $d\mathbf{X}^{t}$ 分别表示估计矩阵、GCNs 的输出和迭代 $t$ 中 RNN 产生的增量更新。MGCNN 指的是多图 CNN。
- en: 'TABLE IV: A Comparison among Different Graph Convolutional Networks (GCNs).
    T.C. = Time Complexity, M.G. = Multiple Graphs'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：不同图卷积网络（GCNs）之间的比较。T.C. = 时间复杂度，M.G. = 多图
- en: '| Method | Type | Convolution | Readout | T.C. | M.G. | Other Characteristics
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类型 | 卷积 | 读出 | T.C. | M.G. | 其他特征 |'
- en: '| Bruna et al. [[40](#bib.bib40)] | Spectral | Interpolation kernel | Hierarchical
    clustering + FC | $O(N^{3})$ | No | - |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Bruna et al. [[40](#bib.bib40)] | 频谱 | 插值核 | 层次聚类 + FC | $O(N^{3})$ | 否 |
    - |'
- en: '| Henaff et al. [[41](#bib.bib41)] | Spectral | Interpolation kernel | Hierarchical
    clustering + FC | $O(N^{3})$ | No | Constructing the graph |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Henaff et al. [[41](#bib.bib41)] | 频谱 | 插值核 | 层次聚类 + FC | $O(N^{3})$ | 否
    | 构建图 |'
- en: '| ChebNet [[42](#bib.bib42)] | Spectral/Spatial | Polynomial | Hierarchical
    clustering | $O(M)$ | Yes | - |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ChebNet [[42](#bib.bib42)] | 频谱/空间 | 多项式 | 层次聚类 | $O(M)$ | 是 | - |'
- en: '| Kipf&Welling [[43](#bib.bib43)] | Spectral/Spatial | First-order | - | $O(M)$
    | - | - |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Kipf&Welling [[43](#bib.bib43)] | 频谱/空间 | 一阶 | - | $O(M)$ | - | - |'
- en: '| CayletNet [[44](#bib.bib44)] | Spectral | Polynomial | Hierarchical clustering
    + FC | $O(M)$ | No | - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CayletNet [[44](#bib.bib44)] | 频谱 | 多项式 | 层次聚类 + FC | $O(M)$ | 否 | - |'
- en: '| GWNN [[45](#bib.bib45)] | Spectral | Wavelet transform | - | $O(M)$ | No
    | - |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GWNN [[45](#bib.bib45)] | 频谱 | 小波变换 | - | $O(M)$ | 否 | - |'
- en: '| Neural FPs [[46](#bib.bib46)] | Spatial | First-order | Sum | $O(M)$ | Yes
    | - |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Neural FPs [[46](#bib.bib46)] | 空间 | 一阶 | 总和 | $O(M)$ | 是 | - |'
- en: '| PATCHY-SAN [[47](#bib.bib47)] | Spatial | Polynomial + an order | An order
    + pooling | $O(M\log N)$ | Yes | A neighbor order |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| PATCHY-SAN [[47](#bib.bib47)] | 空间 | 多项式 + 一阶 | 一阶 + 池化 | $O(M\log N)$ |
    是 | 一个邻居顺序 |'
- en: '| LGCN [[48](#bib.bib48)] | Spatial | First-order + an order | - | $O(M)$ |
    Yes | A neighbor order |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LGCN [[48](#bib.bib48)] | 空间 | 一阶 + 一阶 | - | $O(M)$ | 是 | 一个邻居顺序 |'
- en: '| SortPooling [[49](#bib.bib49)] | Spatial | First-order | An order + pooling
    | $O(M)$ | Yes | A node order |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| SortPooling [[49](#bib.bib49)] | 空间 | 一阶 | 一阶 + 池化 | $O(M)$ | 是 | 一个节点顺序
    |'
- en: '| DCNN [[50](#bib.bib50)] | Spatial | Polynomial diffusion | Mean | $O(N^{2})$
    | Yes | Edge features |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| DCNN [[50](#bib.bib50)] | 空间 | 多项式扩散 | 平均 | $O(N^{2})$ | 是 | 边特征 |'
- en: '| DGCN [[51](#bib.bib51)] | Spatial | First-order + diffusion | - | $O(N^{2})$
    | - | - |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| DGCN [[51](#bib.bib51)] | 空间 | 一阶 + 扩散 | - | $O(N^{2})$ | - | - |'
- en: '| MPNNs [[52](#bib.bib52)] | Spatial | First-order | Set2set | $O(M)$ | Yes
    | A general framework |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MPNNs [[52](#bib.bib52)] | 空间 | 一阶 | Set2set | $O(M)$ | 是 | 一个通用框架 |'
- en: '| GraphSAGE [[53](#bib.bib53)] | Spatial | First-order + sampling | - | $O(Ns^{L})$
    | Yes | A general framework |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GraphSAGE [[53](#bib.bib53)] | 空间 | 一阶 + 采样 | - | $O(Ns^{L})$ | 是 | 一个通用框架
    |'
- en: '| MoNet [[54](#bib.bib54)] | Spatial | First-order | Hierarchical clustering
    | $O(M)$ | Yes | A general framework |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MoNet [[54](#bib.bib54)] | 空间 | 一阶 | 层次聚类 | $O(M)$ | 是 | 一个通用框架 |'
- en: '| GNs [[9](#bib.bib9)] | Spatial | First-order | A graph representation | $O(M)$
    | Yes | A general framework |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| GNs [[9](#bib.bib9)] | 空间 | 一阶 | 图表示 | $O(M)$ | 是 | 通用框架 |'
- en: '| Kearnes et al. [[55](#bib.bib55)] | Spatial | Weave module | Fuzzy histogram
    | $O(M)$ | Yes | Edge features |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Kearnes et al. [[55](#bib.bib55)] | 空间 | 编织模块 | 模糊直方图 | $O(M)$ | 是 | 边缘特征
    |'
- en: '| DiffPool [[56](#bib.bib56)] | Spatial | Various | Hierarchical clustering
    | $O(N^{2})$ | Yes | Differentiable pooling |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| DiffPool [[56](#bib.bib56)] | 空间 | 各种 | 层次聚类 | $O(N^{2})$ | 是 | 可微池化 |'
- en: '| GAT [[57](#bib.bib57)] | Spatial | First-order | - | $O(M)$ | Yes | Attention
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GAT [[57](#bib.bib57)] | 空间 | 一阶 | - | $O(M)$ | 是 | 注意力 |'
- en: '| GaAN [[58](#bib.bib58)] | Spatial | First-order | - | $O(Ns^{L})$ | Yes |
    Attention |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GaAN [[58](#bib.bib58)] | 空间 | 一阶 | - | $O(Ns^{L})$ | 是 | 注意力 |'
- en: '| HAN [[59](#bib.bib59)] | Spatial | Meta-path neighbors | - | $O(M_{\phi})$
    | Yes | Attention |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| HAN [[59](#bib.bib59)] | 空间 | 元路径邻居 | - | $O(M_{\phi})$ | 是 | 注意力 |'
- en: '| CLN [[60](#bib.bib60)] | Spatial | First-order | - | $O(M)$ | - | - |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| CLN [[60](#bib.bib60)] | 空间 | 一阶 | - | $O(M)$ | - | - |'
- en: '| PPNP [[61](#bib.bib61)] | Spatial | First-order | - | $O(M)$ | - | Teleportation
    connections |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| PPNP [[61](#bib.bib61)] | 空间 | 一阶 | - | $O(M)$ | - | 传送连接 |'
- en: '| JK-Nets [[62](#bib.bib62)] | Spatial | Various | - | $O(M)$ | Yes | Jumping
    connections |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| JK-Nets [[62](#bib.bib62)] | 空间 | 各种 | - | $O(M)$ | 是 | 跳跃连接 |'
- en: '| ECC [[63](#bib.bib63)] | Spatial | First-order | Hierarchical clustering
    | $O(M)$ | Yes | Edge features |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ECC [[63](#bib.bib63)] | 空间 | 一阶 | 层次聚类 | $O(M)$ | 是 | 边缘特征 |'
- en: '| R-GCNs [[64](#bib.bib64)] | Spatial | First-order | - | $O(M)$ | - | Edge
    features |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| R-GCNs [[64](#bib.bib64)] | 空间 | 一阶 | - | $O(M)$ | - | 边缘特征 |'
- en: '| LGNN [[65](#bib.bib65)] | Spatial | First-order + LINE graph | - | $O(M)$
    | - | Edge features |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| LGNN [[65](#bib.bib65)] | 空间 | 一阶 + LINE 图 | - | $O(M)$ | - | 边缘特征 |'
- en: '| PinSage [[66](#bib.bib66)] | Spatial | Random walk | - | $O(Ns^{L})$ | -
    | Neighborhood sampling |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| PinSage [[66](#bib.bib66)] | 空间 | 随机游走 | - | $O(Ns^{L})$ | - | 邻域采样 |'
- en: '| StochasticGCN [[67](#bib.bib67)] | Spatial | First-order + sampling | - |
    $O(Ns^{L})$ | - | Neighborhood sampling |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| StochasticGCN [[67](#bib.bib67)] | 空间 | 一阶 + 采样 | - | $O(Ns^{L})$ | - | 邻域采样
    |'
- en: '| FastGCN [[68](#bib.bib68)] | Spatial | First-order + sampling | - | $O(NsL)$
    | Yes | Layer-wise sampling |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| FastGCN [[68](#bib.bib68)] | 空间 | 一阶 + 采样 | - | $O(NsL)$ | 是 | 层级采样 |'
- en: '| Adapt [[69](#bib.bib69)] | Spatial | First-order + sampling | - | $O(NsL)$
    | Yes | Layer-wise sampling |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Adapt [[69](#bib.bib69)] | 空间 | 一阶 + 采样 | - | $O(NsL)$ | 是 | 层级采样 |'
- en: '| Li et al. [[70](#bib.bib70)] | Spatial | First-order | - | $O(M)$ | - | Theoretical
    analysis |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. [[70](#bib.bib70)] | 空间 | 一阶 | - | $O(M)$ | - | 理论分析 |'
- en: '| SGC [[71](#bib.bib71)] | Spatial | Polynomial | - | $O(M)$ | Yes | Theoretical
    analysis |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| SGC [[71](#bib.bib71)] | 空间 | 多项式 | - | $O(M)$ | 是 | 理论分析 |'
- en: '| GFNN [[72](#bib.bib72)] | Spatial | Polynomial | - | $O(M)$ | Yes | Theoretical
    analysis |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| GFNN [[72](#bib.bib72)] | 空间 | 多项式 | - | $O(M)$ | 是 | 理论分析 |'
- en: '| GIN [[73](#bib.bib73)] | Spatial | First-order | Sum + MLP | $O(M)$ | Yes
    | Theoretical analysis |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| GIN [[73](#bib.bib73)] | 空间 | 一阶 | 求和 + MLP | $O(M)$ | 是 | 理论分析 |'
- en: '| DGI [[74](#bib.bib74)] | Spatial | First-order | - | $O(M)$ | Yes | Unsupervised
    training |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| DGI [[74](#bib.bib74)] | 空间 | 一阶 | - | $O(M)$ | 是 | 无监督训练 |'
- en: 4 Graph Convolutional Networks
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 图卷积网络
- en: 'Graph convolutional networks (GCNs) are inarguably the hottest topic in graph-based
    deep learning. Mimicking CNNs, modern GCNs learn the common local and global structural
    patterns of graphs through designed convolution and readout functions. Because
    most GCNs can be trained with task-specific loss via backpropagation (with a few
    exceptions such as the unsupervised training method in [[74](#bib.bib74)]), we
    focus on the adopted architectures. We first discuss the convolution operations,
    then move to the readout operations and some other improvements. We summarize
    the main characteristics of GCNs surveyed in this paper in Table [IV](#S3.T4 "TABLE
    IV ‣ 3.2 Graph-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning
    on Graphs: A Survey").'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '图卷积网络（GCNs）无疑是基于图的深度学习中最热门的主题。现代GCNs模仿CNN，通过设计的卷积和读取函数来学习图的常见局部和全局结构模式。由于大多数GCNs可以通过反向传播使用任务特定的损失进行训练（有少数例外，如[[74](#bib.bib74)]中的无监督训练方法），我们专注于采用的架构。我们首先讨论卷积操作，然后转到读取操作和一些其他改进。我们在表[IV](#S3.T4
    "TABLE IV ‣ 3.2 Graph-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning
    on Graphs: A Survey")中总结了本文调查的GCNs的主要特征。'
- en: 4.1 Convolution Operations
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 卷积操作
- en: 'Graph convolutions can be divided into two groups: *spectral convolutions*,
    which perform convolution by transforming node representations into the spectral
    domain using the graph Fourier transform or its extensions, and *spatial convolutions*,
    which perform convolution by considering node neighborhoods. Note that these two
    groups can overlap, for example, when using a polynomial spectral kernel (please
    refer to Section [4.1.2](#S4.SS1.SSS2 "4.1.2 The Efficiency Aspect ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")
    for details).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积可以分为两类：*谱卷积*，通过使用图傅里叶变换或其扩展将节点表示转换到谱域来执行卷积；以及*空间卷积*，通过考虑节点邻域来执行卷积。请注意，这两类可能会重叠，例如，当使用多项式谱核时（请参见第
    [4.1.2](#S4.SS1.SSS2 "4.1.2 效率方面 ‣ 4.1 卷积操作 ‣ 4 图卷积网络 ‣ 图上的深度学习：综述") 节的详细信息）。
- en: 4.1.1 Spectral Methods
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 谱方法
- en: 'Convolution is the most fundamental operation in CNNs. However, the standard
    convolution operation used for images or text cannot be directly applied to graphs
    because graphs lack a grid structure [[6](#bib.bib6)]. Bruna et al. [[40](#bib.bib40)]
    first introduced convolution for graph data from the spectral domain using the
    graph Laplacian matrix $\mathbf{L}$ [[75](#bib.bib75)], which plays a similar
    role as the Fourier basis in signal processing [[6](#bib.bib6)]. The graph convolution
    operation, $*_{G}$, is defined as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是卷积神经网络中的最基本操作。然而，标准的卷积操作用于图像或文本的操作无法直接应用于图，因为图缺乏网格结构 [[6](#bib.bib6)]。Bruna
    等人 [[40](#bib.bib40)] 首次从谱域引入了图数据的卷积，使用了图拉普拉斯矩阵 $\mathbf{L}$ [[75](#bib.bib75)]，它在信号处理中的作用类似于傅里叶基底
    [[6](#bib.bib6)]。图卷积操作 $*_{G}$ 定义如下：
- en: '|  | $\mathbf{u}_{1}*_{G}\mathbf{u}_{2}=\mathbf{Q}\left(\left(\mathbf{Q}^{T}\mathbf{u}_{1}\right)\odot\left(\mathbf{Q}^{T}\mathbf{u}_{2}\right)\right),$
    |  | (5) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{u}_{1}*_{G}\mathbf{u}_{2}=\mathbf{Q}\left(\left(\mathbf{Q}^{T}\mathbf{u}_{1}\right)\odot\left(\mathbf{Q}^{T}\mathbf{u}_{2}\right)\right),$
    |  | (5) |'
- en: 'where $\mathbf{u}_{1},\mathbf{u}_{2}\in\mathbb{R}^{N}$ are two signals⁵⁵5We
    give an example of graph signals in Appendix [D](#A4 "Appendix D An Example of
    Graph Signals ‣ Deep Learning on Graphs: A Survey"). defined on nodes and $\mathbf{Q}$
    are the eigenvectors of $\mathbf{L}$. Briefly, multiplying $\mathbf{Q}^{T}$ transforms
    the graph signals $\mathbf{u}_{1},\mathbf{u}_{2}$ into the spectral domain (i.e.,
    the graph Fourier transform), while multiplying $\mathbf{Q}$ performs the inverse
    transform. The validity of this definition is based on the convolution theorem,
    i.e., the Fourier transform of a convolution operation is the element-wise product
    of their Fourier transforms. Then, a signal $\mathbf{u}$ can be filtered by'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{u}_{1},\mathbf{u}_{2}\in\mathbb{R}^{N}$ 是定义在节点上的两个信号⁵⁵5我们在附录 [D](#A4
    "附录 D 图信号示例 ‣ 图上的深度学习：综述")中给出了图信号的示例。 $\mathbf{Q}$ 是 $\mathbf{L}$ 的特征向量。简而言之，乘以
    $\mathbf{Q}^{T}$ 将图信号 $\mathbf{u}_{1},\mathbf{u}_{2}$ 转换到谱域（即图傅里叶变换），而乘以 $\mathbf{Q}$
    执行逆变换。这个定义的有效性基于卷积定理，即卷积操作的傅里叶变换是它们傅里叶变换的逐元素乘积。然后，一个信号 $\mathbf{u}$ 可以通过以下方式进行滤波：
- en: '|  | $\mathbf{u}^{\prime}=\mathbf{Q}\mathbf{\Theta}\mathbf{Q}^{T}\mathbf{u},$
    |  | (6) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{u}^{\prime}=\mathbf{Q}\mathbf{\Theta}\mathbf{Q}^{T}\mathbf{u},$
    |  | (6) |'
- en: 'where $\mathbf{u}^{\prime}$ is the output signal, $\mathbf{\Theta}=\mathbf{\Theta}(\mathbf{\Lambda})\in\mathbb{R}^{N\times
    N}$ is a diagonal matrix of learnable filters and $\mathbf{\Lambda}$ are the eigenvalues
    of $\mathbf{L}$. A convolutional layer is defined by applying different filters
    to different input-output signal pairs as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{u}^{\prime}$ 是输出信号，$\mathbf{\Theta}=\mathbf{\Theta}(\mathbf{\Lambda})\in\mathbb{R}^{N\times
    N}$ 是一个可学习滤波器的对角矩阵，$\mathbf{\Lambda}$ 是 $\mathbf{L}$ 的特征值。一个卷积层通过对不同的输入-输出信号对应用不同的滤波器来定义，如下所示：
- en: '|  | $\mathbf{u}^{l+1}_{j}=\rho\left(\sum\nolimits_{i=1}^{f_{l}}\mathbf{Q}\mathbf{\Theta}^{l}_{i,j}\mathbf{Q}^{T}\mathbf{u}^{l}_{i}\right)\;j=1,...,f_{l+1},$
    |  | (7) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{u}^{l+1}_{j}=\rho\left(\sum\nolimits_{i=1}^{f_{l}}\mathbf{Q}\mathbf{\Theta}^{l}_{i,j}\mathbf{Q}^{T}\mathbf{u}^{l}_{i}\right)\;j=1,...,f_{l+1},$
    |  | (7) |'
- en: 'where $l$ is the layer, $\mathbf{u}^{l}_{j}\in\mathbb{R}^{N}$ is the $j^{th}$
    hidden representation (i.e., the signal) for the nodes in the $l^{th}$ layer,
    and $\mathbf{\Theta}^{l}_{i,j}$ are learnable filters. The idea behind Eq. ([7](#S4.E7
    "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) is similar to a conventional convolution:
    it passes the input signals through a set of learnable filters to aggregate the
    information, followed by some nonlinear transformation. By using the node features
    $\mathbf{F}^{V}$ as the input layer and stacking multiple convolutional layers,
    the overall architecture is similar to that of a CNN. Theoretical analysis has
    shown that such a definition of the graph convolution operation can mimic certain
    geometric properties of CNNs and we refer readers to [[7](#bib.bib7)] for a comprehensive
    survey.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$l$是层，$\mathbf{u}^{l}_{j}\in\mathbb{R}^{N}$是第$l$层节点的第$j^{th}$隐藏表示（即信号），$\mathbf{\Theta}^{l}_{i,j}$是可学习的滤波器。公式 ([7](#S4.E7
    "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey"))背后的思想类似于传统卷积：它将输入信号通过一组可学习的滤波器来聚合信息，然后进行一些非线性变换。通过使用节点特征$\mathbf{F}^{V}$作为输入层并堆叠多个卷积层，整体架构类似于CNN。理论分析表明，这种图卷积操作的定义可以模拟CNN的某些几何属性，我们建议读者参考[[7](#bib.bib7)]以获取全面的综述。'
- en: 'However, directly using Eq. ([7](#S4.E7 "In 4.1.1 Spectral Methods ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    requires learning $O(N)$ parameters, which may not be feasible in practice. Besides,
    the filters in the spectral domain may not be localized in the spatial domain,
    i.e., each node may be affected by all the other nodes rather than only the nodes
    in a small region. To alleviate these problems, Bruna et al. [[40](#bib.bib40)]
    suggested using the following smoothing filters:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，直接使用公式 ([7](#S4.E7 "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))需要学习$O(N)$个参数，这在实际中可能不可行。此外，谱域中的滤波器可能在空间域中并不局部化，即每个节点可能会受到所有其他节点的影响，而不仅仅是小区域内的节点。为了缓解这些问题，Bruna等人[[40](#bib.bib40)]建议使用以下平滑滤波器：'
- en: '|  | $diag\left(\mathbf{\Theta}^{l}_{i,j}\right)=\mathcal{K}\;\alpha_{l,i,j},$
    |  | (8) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $diag\left(\mathbf{\Theta}^{l}_{i,j}\right)=\mathcal{K}\;\alpha_{l,i,j},$
    |  | (8) |'
- en: where $\mathcal{K}$ is a fixed interpolation kernel and $\alpha_{l,i,j}$ are
    learnable interpolation coefficients. The authors also generalized this idea to
    the setting where the graph is not given but constructed from raw features using
    either a supervised or an unsupervised method [[41](#bib.bib41)].
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{K}$是固定的插值核，$\alpha_{l,i,j}$是可学习的插值系数。作者还将这一思想推广到了图未给出而是通过有监督或无监督方法从原始特征构建的情形[[41](#bib.bib41)]。
- en: However, two fundamental problems remain unsolved. First, because the full eigenvectors
    of the Laplacian matrix are needed during each calculation, the time complexity
    is at least $O(N^{2})$ for each forward and backward pass, not to mention the
    $O(N^{3})$ complexity required to calculate the eigendecomposition, meaning that
    this approach is not scalable to large-scale graphs. Second, because the filters
    depend on the eigenbasis $\mathbf{Q}$ of the graph, the parameters cannot be shared
    across multiple graphs with different sizes and structures.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，两个根本性问题仍未解决。首先，由于在每次计算中都需要拉普拉斯矩阵的完整特征向量，因此每次前向和后向传播的时间复杂度至少为$O(N^{2})$，更不用说计算特征分解所需的$O(N^{3})$复杂度，这意味着这种方法在大规模图上不可扩展。其次，由于滤波器依赖于图的特征基$\mathbf{Q}$，因此这些参数不能在具有不同规模和结构的多个图之间共享。
- en: Next, we review two lines of works trying to solve these limitations and then
    unify them using some common frameworks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们回顾两条试图解决这些限制的工作线，并利用一些通用框架将它们统一起来。
- en: 4.1.2 The Efficiency Aspect
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 效率方面
- en: 'To solve the efficiency problem, ChebNet [[42](#bib.bib42)] was proposed to
    use a polynomial filter as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决效率问题，提出了ChebNet[[42](#bib.bib42)]，其使用多项式滤波器，如下所示：
- en: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\sum\nolimits_{k=0}^{K}\theta_{k}\mathbf{\Lambda}^{k},$
    |  | (9) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\sum\nolimits_{k=0}^{K}\theta_{k}\mathbf{\Lambda}^{k},$
    |  | (9) |'
- en: 'where $\theta_{0},...,\theta_{K}$ are the learnable parameters and $K$ is the
    polynomial order. Then, instead of performing the eigendecomposition, the authors
    rewrote Eq. ([9](#S4.E9 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) using
    the Chebyshev expansion [[76](#bib.bib76)]:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\theta_{0},...,\theta_{K}$ 是可学习的参数，$K$ 是多项式的阶数。然后，作者们使用切比雪夫展开 [[76](#bib.bib76)]
    替代特征分解重写了公式 ([9](#S4.E9 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))：'
- en: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\sum\nolimits_{k=0}^{K}\theta_{k}\mathcal{T}_{k}(\tilde{\mathbf{\Lambda}}),$
    |  | (10) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\sum\nolimits_{k=0}^{K}\theta_{k}\mathcal{T}_{k}(\tilde{\mathbf{\Lambda}}),$
    |  | (10) |'
- en: 'where $\tilde{\mathbf{\Lambda}}=2\mathbf{\Lambda}/\lambda_{max}-\mathbf{I}$
    are the rescaled eigenvalues, $\lambda_{max}$ is the maximum eigenvalue, $\mathbf{I}\in\mathbb{R}^{N\times
    N}$ is the identity matrix, and $\mathcal{T}_{k}(x)$ is the Chebyshev polynomial
    of order $k$. The rescaling is necessary because of the orthonormal basis of Chebyshev
    polynomials. Using the fact that a polynomial of the Laplacian matrix acts as
    a polynomial of its eigenvalues, i.e., $\mathbf{L}^{k}=\mathbf{Q}\mathbf{\Lambda}^{k}\mathbf{Q}^{T}$,
    the filter operation in Eq. ([6](#S4.E6 "In 4.1.1 Spectral Methods ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    can be rewritten as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{\mathbf{\Lambda}}=2\mathbf{\Lambda}/\lambda_{max}-\mathbf{I}$ 是缩放后的特征值，$\lambda_{max}$
    是最大特征值，$\mathbf{I}\in\mathbb{R}^{N\times N}$ 是单位矩阵，$\mathcal{T}_{k}(x)$ 是阶数为 $k$
    的切比雪夫多项式。由于切比雪夫多项式的正交归一基，这种缩放是必要的。利用拉普拉斯矩阵的多项式作用于其特征值的性质，即 $\mathbf{L}^{k}=\mathbf{Q}\mathbf{\Lambda}^{k}\mathbf{Q}^{T}$，公式中的滤波操作可以重写如下：
- en: '|  | $\displaystyle\mathbf{u}^{\prime}=\mathbf{Q}\mathbf{\Theta}(\mathbf{\Lambda})\mathbf{Q}^{T}\mathbf{u}=$
    | $\displaystyle\sum\nolimits_{k=0}^{K}\theta_{k}\mathbf{Q}\mathcal{T}_{k}(\tilde{\mathbf{\Lambda}})\mathbf{Q}^{T}\mathbf{u}$
    |  | (11) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{u}^{\prime}=\mathbf{Q}\mathbf{\Theta}(\mathbf{\Lambda})\mathbf{Q}^{T}\mathbf{u}=$
    | $\displaystyle\sum\nolimits_{k=0}^{K}\theta_{k}\mathbf{Q}\mathcal{T}_{k}(\tilde{\mathbf{\Lambda}})\mathbf{Q}^{T}\mathbf{u}$
    |  | (11) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum\nolimits_{k=0}^{K}\theta_{k}\mathcal{T}_{k}(\tilde{\mathbf{L}})\mathbf{u}=\sum\nolimits_{k=0}^{K}\theta_{k}\bar{\mathbf{u}}_{k},$
    |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum\nolimits_{k=0}^{K}\theta_{k}\mathcal{T}_{k}(\tilde{\mathbf{L}})\mathbf{u}=\sum\nolimits_{k=0}^{K}\theta_{k}\bar{\mathbf{u}}_{k},$
    |  |'
- en: 'where $\bar{\mathbf{u}}_{k}=\mathcal{T}_{k}(\tilde{\mathbf{L}})\mathbf{u}$
    and $\tilde{\mathbf{L}}=2\mathbf{L}/\lambda_{max}-\mathbf{I}$. Using the recurrence
    relation of the Chebyshev polynomial $\mathcal{T}_{k}(x)=2x\mathcal{T}_{k-1}(x)-\mathcal{T}_{k-2}(x)$
    and $\mathcal{T}_{0}(x)=1,\mathcal{T}_{1}(x)=x$, $\bar{\mathbf{u}}_{k}$ can also
    be calculated recursively:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bar{\mathbf{u}}_{k}=\mathcal{T}_{k}(\tilde{\mathbf{L}})\mathbf{u}$ 和 $\tilde{\mathbf{L}}=2\mathbf{L}/\lambda_{max}-\mathbf{I}$。使用切比雪夫多项式的递推关系
    $\mathcal{T}_{k}(x)=2x\mathcal{T}_{k-1}(x)-\mathcal{T}_{k-2}(x)$ 和 $\mathcal{T}_{0}(x)=1,\mathcal{T}_{1}(x)=x$，$\bar{\mathbf{u}}_{k}$
    也可以递归计算：
- en: '|  | $\bar{\mathbf{u}}_{k}=2\tilde{\mathbf{L}}\bar{\mathbf{u}}_{k-1}-\bar{\mathbf{u}}_{k-2}$
    |  | (12) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{\mathbf{u}}_{k}=2\tilde{\mathbf{L}}\bar{\mathbf{u}}_{k-1}-\bar{\mathbf{u}}_{k-2}$
    |  | (12) |'
- en: 'with $\bar{\mathbf{u}}_{0}=\mathbf{u}$ and $\bar{\mathbf{u}}_{1}=\tilde{\mathbf{L}}\mathbf{u}$.
    Now, because only the matrix multiplication of a sparse matrix $\tilde{\mathbf{L}}$
    and some vectors need to be calculated, the time complexity becomes $O(KM)$ when
    using sparse matrix multiplication, where $M$ is the number of edges and $K$ is
    the polynomial order, i.e., the time complexity is linear with respect to the
    number of edges. It is also easy to see that such a polynomial filter is strictly
    $K$-localized: after one convolution, the representation of node $v_{i}$ will
    be affected only by its $K$-step neighborhoods $\mathcal{N}_{K}(i)$. Interestingly,
    this idea is used independently in network embedding to preserve the high-order
    proximity [[77](#bib.bib77)], of which we omit the details for brevity.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bar{\mathbf{u}}_{0}=\mathbf{u}$ 和 $\bar{\mathbf{u}}_{1}=\tilde{\mathbf{L}}\mathbf{u}$。现在，因为只需计算稀疏矩阵
    $\tilde{\mathbf{L}}$ 和一些向量的矩阵乘法，使用稀疏矩阵乘法时时间复杂度变为 $O(KM)$，其中 $M$ 是边的数量，$K$ 是多项式的阶数，即时间复杂度与边的数量线性相关。也可以很容易看出，这种多项式滤波器是严格的
    $K$-局部化的：经过一次卷积后，节点 $v_{i}$ 的表示只会受到其 $K$ 步邻域 $\mathcal{N}_{K}(i)$ 的影响。有趣的是，这一思想在网络嵌入中被独立使用，以保持高阶邻近性 [[77](#bib.bib77)]，详细信息我们为简洁起见略去。
- en: 'Kipf and Welling [[43](#bib.bib43)] further simplified the filtering by using
    only the first-order neighbors:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Kipf 和 Welling [[43](#bib.bib43)] 通过仅使用一阶邻居进一步简化了滤波：
- en: '|  | $\mathbf{h}^{l+1}_{i}=\rho\left(\sum_{j\in\tilde{\mathcal{N}}(i)}\frac{1}{\sqrt{\tilde{\mathbf{D}}(i,i)\tilde{\mathbf{D}}(j,j)}}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right),$
    |  | (13) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}^{l+1}_{i}=\rho\left(\sum_{j\in\tilde{\mathcal{N}}(i)}\frac{1}{\sqrt{\tilde{\mathbf{D}}(i,i)\tilde{\mathbf{D}}(j,j)}}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right),$
    |  | (13) |'
- en: 'where $\mathbf{h}^{l}_{i}\in\mathbb{R}^{f_{l}}$ is the hidden representation
    of node $v_{i}$ in the $l^{th}$ layer⁶⁶6We use a different letter because $\mathbf{h}^{l}\in\mathbb{R}^{f_{l}}$
    is the hidden representation of one node, while $\mathbf{u}^{l}\in\mathbb{R}^{N}$
    represents a dimension for all nodes., $\tilde{\mathbf{D}}=\mathbf{D}+\mathbf{I}$,
    and $\tilde{\mathcal{N}}(i)=\mathcal{N}(i)\cup\{i\}$. This can be written equivalently
    in an matrix form as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{h}^{l}_{i}\in\mathbb{R}^{f_{l}}$ 是第 $l^{th}$ 层中节点 $v_{i}$ 的隐藏表示⁶⁶6我们使用不同的字母，因为
    $\mathbf{h}^{l}\in\mathbb{R}^{f_{l}}$ 是一个节点的隐藏表示，而 $\mathbf{u}^{l}\in\mathbb{R}^{N}$
    表示所有节点的一个维度。,$ $\tilde{\mathbf{D}}=\mathbf{D}+\mathbf{I}$，且 $\tilde{\mathcal{N}}(i)=\mathcal{N}(i)\cup\{i\}$。这可以等效地写成矩阵形式如下：
- en: '|  | $\mathbf{H}^{l+1}=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (14) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{l+1}=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (14) |'
- en: 'where $\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}$, i.e., adding a self-connection.
    The authors showed that Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency Aspect ‣ 4.1
    Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs:
    A Survey")) is a special case of Eq. ([9](#S4.E9 "In 4.1.2 The Efficiency Aspect
    ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) by setting $K=1$ with a few minor changes. Then, the authors
    argued that stacking an adequate number of layers as illustrated in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey") has a modeling capacity
    similar to ChebNet but leads to better results.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}$，即添加自连接。作者展示了方程 ([14](#S4.E14
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) 是方程 ([9](#S4.E9 "In 4.1.2 The
    Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")) 的一个特例，通过设置 $K=1$ 和一些小的修改。随后，作者认为，正如图 [3](#S4.F3
    "Figure 3 ‣ 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey") 所示，堆叠足够数量的层具有类似于
    ChebNet 的建模能力，但能获得更好的结果。'
- en: '![Refer to caption](img/b9b235bfb30b41141764c3552d5dd636.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b9b235bfb30b41141764c3552d5dd636.png)'
- en: 'Figure 3: An illustrative example of the spatial convolution operation proposed
    by Kipf and Welling [[43](#bib.bib43)] (reprinted with permission). Nodes are
    affected only by their immediate neighbors in each convolutional layer.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: Kipf 和 Welling [[43](#bib.bib43)] 提出的空间卷积操作的示例（经许可转载）。每个卷积层中，节点仅受其直接邻居的影响。'
- en: 'An important insight of ChebNet and its extension is that they connect the
    spectral graph convolution with the spatial architecture. Specifically, they show
    that when the spectral convolution function is polynomial or first-order, the
    spectral graph convolution is equivalent to a spatial convolution. In addition,
    the convolution in Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    is highly similar to the state definition in a GNN in Eq. ([1](#S3.E1 "In 3.1
    Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on Graphs:
    A Survey")), except that the convolution definition replaces the recursive definition.
    From this aspect, a GNN can be regarded as a GCN with a large number of identical
    layers to reach stable states [[7](#bib.bib7)], i.e., a GNN uses a fixed function
    with fixed parameters to iteratively update the node hidden states until reaching
    an equilibrium, while a GCN has a preset number of layers and each layer contains
    different parameters.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'ChebNet 及其扩展的一个重要见解是，它们将光谱图卷积与空间架构连接起来。具体而言，它们展示了当光谱卷积函数为多项式或一阶时，光谱图卷积等同于空间卷积。此外，Eq.
    ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4
    Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) 中的卷积与 GNN
    中 Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks
    ‣ Deep Learning on Graphs: A Survey")) 中的状态定义高度相似，只是卷积定义取代了递归定义。从这一方面看，GNN 可以被视为具有大量相同层的
    GCN，以达到稳定状态 [[7](#bib.bib7)]，即 GNN 使用具有固定参数的固定函数来迭代更新节点隐藏状态，直到达到平衡，而 GCN 有预设数量的层，每层包含不同的参数。'
- en: 'Some spectral methods have also been proposed to solve the efficiency problem.
    For example, instead of using the Chebyshev expansion as in Eq. ([10](#S4.E10
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")), CayleyNet [[44](#bib.bib44)]
    adopted Cayley polynomials to define graph convolutions:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '一些光谱方法也被提出以解决效率问题。例如，CayleyNet [[44](#bib.bib44)] 采用 Cayley 多项式来定义图卷积，而不是像
    Eq. ([10](#S4.E10 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) 中使用切比雪夫展开：'
- en: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\theta_{0}+2Re\left\{\sum\nolimits_{k=1}^{K}\theta_{k}\left(\theta_{h}\mathbf{\Lambda}-i\mathbf{I}\right)^{k}\left(\theta_{h}\mathbf{\Lambda}+i\mathbf{I}\right)^{k}\right\},$
    |  | (15) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\theta_{0}+2Re\left\{\sum\nolimits_{k=1}^{K}\theta_{k}\left(\theta_{h}\mathbf{\Lambda}-i\mathbf{I}\right)^{k}\left(\theta_{h}\mathbf{\Lambda}+i\mathbf{I}\right)^{k}\right\},$
    |  | (15) |'
- en: 'where $i=\sqrt{-1}$ denotes the imaginary unit and $\theta_{h}$ is another
    spectral zoom parameter. In addition to showing that CayleyNet is as efficient
    as ChebNet, the authors demonstrated that the Cayley polynomials can detect “narrow
    frequency bands of importance” to achieve better results. Graph wavelet neural
    network (GWNN) [[45](#bib.bib45)] was further proposed to replace the Fourier
    transform in spectral filters by the graph wavelet transform by rewriting Eq. ([5](#S4.E5
    "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $i=\sqrt{-1}$ 表示虚数单位，$\theta_{h}$ 是另一个谱缩放参数。除了表明 CayleyNet 与 ChebNet 一样高效之外，作者们还展示了
    Cayley 多项式可以检测“重要的狭频带”以获得更好的结果。图小波神经网络（GWNN）[[45](#bib.bib45)] 被进一步提出，以通过图小波变换取代光谱滤波器中的傅里叶变换，将
    Eq. ([5](#S4.E5 "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) 重写为以下形式：'
- en: '|  | $\mathbf{u}_{1}*_{G}\mathbf{u}_{2}=\mathbf{\psi}\left(\left(\mathbf{\psi}^{-1}\mathbf{u}_{1}\right)\odot\left(\mathbf{\psi}^{-1}\mathbf{u}_{2}\right)\right),$
    |  | (16) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{u}_{1}*_{G}\mathbf{u}_{2}=\mathbf{\psi}\left(\left(\mathbf{\psi}^{-1}\mathbf{u}_{1}\right)\odot\left(\mathbf{\psi}^{-1}\mathbf{u}_{2}\right)\right),$
    |  | (16) |'
- en: where $\mathbf{\psi}$ denotes the graph wavelet bases. By using fast approximating
    algorithms to calculate $\mathbf{\psi}$ and $\mathbf{\psi}^{-1}$, GWNN’s computational
    complexity is also $O(KM)$, i.e., linear with respect to the number of edges.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{\psi}$ 表示图小波基。通过使用快速近似算法计算 $\mathbf{\psi}$ 和 $\mathbf{\psi}^{-1}$，GWNN
    的计算复杂度也为 $O(KM)$，即与边的数量线性相关。
- en: 4.1.3 The Aspect of Multiple Graphs
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 多图的方面
- en: 'A parallel series of works has focuses on generalizing graph convolutions to
    multiple graphs of arbitrary sizes. Neural FPs [[46](#bib.bib46)] proposed a spatial
    method that also used the first-order neighbors:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列并行的研究专注于将图卷积推广到任意大小的多个图。Neural FPs [[46](#bib.bib46)] 提出了一个空间方法，该方法也使用了第一阶邻居：
- en: '|  | $\mathbf{h}^{l+1}_{i}=\sigma\left(\sum\nolimits_{j\in\hat{\mathcal{N}}(i)}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right).$
    |  | (17) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}^{l+1}_{i}=\sigma\left(\sum\nolimits_{j\in\hat{\mathcal{N}}(i)}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right).$
    |  | (17) |'
- en: 'Because the parameters $\mathbf{\Theta}$ can be shared across different graphs
    and are independent of the graph size, Neural FPs can handle multiple graphs of
    arbitrary sizes. Note that Eq. ([17](#S4.E17 "In 4.1.3 The Aspect of Multiple
    Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) is very similar to Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency
    Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")). However, instead of considering the influence of node
    degree by adding a normalization term, Neural FPs proposed learning different
    parameters $\mathbf{\Theta}$ for nodes with different degrees. This strategy performed
    well for small graphs such as molecular graphs (i.e., atoms as nodes and bonds
    as edges), but may not be scalable to larger graphs.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '由于参数 $\mathbf{\Theta}$ 可以在不同图之间共享，并且与图的大小无关，Neural FPs 可以处理任意大小的多个图。请注意，公式 ([17](#S4.E17
    "In 4.1.3 The Aspect of Multiple Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) 与公式 ([13](#S4.E13
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) 非常相似。然而，Neural FPs 并没有通过添加归一化项来考虑节点度数的影响，而是提出为不同度数的节点学习不同的参数
    $\mathbf{\Theta}$。这种策略在小型图（如分子图，即原子作为节点，键作为边）上表现良好，但可能无法扩展到更大的图。'
- en: PATCHY-SAN [[47](#bib.bib47)] adopted a different idea. It assigned a unique
    node order using a graph labeling procedure such as the Weisfeiler-Lehman kernel [[78](#bib.bib78)]
    and then arranged node neighbors in a line using this pre-defined order. In addition,
    PATCHY-SAN defined a “receptive field” for each node $v_{i}$ by selecting a fixed
    number of nodes from its $k$-step neighborhoods $\mathcal{N}_{k}(i)$. Then a standard
    1-D CNN with proper normalization was adopted. Using this approach, nodes in different
    graphs all have a “receptive field” with a fixed size and order; thus, PATCHY-SAN
    can learn from multiple graphs like normal CNNs learn from multiple images. The
    drawbacks are that the convolution depends heavily on the graph labeling procedure
    which is a preprocessing step that is not learned. LGCN [[48](#bib.bib48)] further
    proposed to simplify the sorting process by using a lexicographical order (i.e.,
    sorting neighbors based on their hidden representation in the final layer $\mathbf{H}^{L}$).
    Instead of using a single order, the authors sorted different channels of $\mathbf{H}^{L}$
    separately. SortPooling [[49](#bib.bib49)] took a similar approach, but rather
    than sorting the neighbors of each node, the authors proposed to sort all the
    nodes (i.e., using a single order for all the neighborhoods). Despite the differences
    among these methods, enforcing a 1-D node order may not be a natural choice for
    graphs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: PATCHY-SAN [[47](#bib.bib47)] 采用了不同的想法。它通过图标记程序（如 Weisfeiler-Lehman 核 [[78](#bib.bib78)]）分配了唯一的节点顺序，然后使用这个预定义的顺序将节点邻居排成一行。此外，PATCHY-SAN
    为每个节点 $v_{i}$ 定义了一个“感受野”，通过从其 $k$-步邻域 $\mathcal{N}_{k}(i)$ 中选择固定数量的节点来实现。然后，采用了标准的
    1-D CNN，并进行了适当的归一化。采用这种方法，不同图中的节点都具有固定大小和顺序的“感受野”；因此，PATCHY-SAN 可以像正常 CNN 从多个图像中学习一样，从多个图中学习。缺点是卷积严重依赖于图标记程序，这是一种未学习的预处理步骤。LGCN [[48](#bib.bib48)]
    进一步提出通过使用字典序（即根据最终层 $\mathbf{H}^{L}$ 中的隐藏表示对邻居进行排序）来简化排序过程。作者并没有使用单一的顺序，而是分别对
    $\mathbf{H}^{L}$ 的不同通道进行排序。SortPooling [[49](#bib.bib49)] 采取了类似的方法，但与其排序每个节点的邻居不同，作者提出对所有节点进行排序（即对所有邻域使用单一的顺序）。尽管这些方法存在差异，但强制使用
    1-D 节点顺序可能不是图的自然选择。
- en: 'DCNN [[50](#bib.bib50)] adopted another approach by replacing the eigenbasis
    of the graph convolution with a diffusion-basis, i.e., the neighborhoods of nodes
    were determined by the diffusion transition probability between nodes. Specifically,
    the convolution was defined as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: DCNN [[50](#bib.bib50)] 采用了另一种方法，通过用扩散基替换图卷积的特征基，即节点的邻域是通过节点之间的扩散转移概率确定的。具体而言，卷积定义如下：
- en: '|  | $\mathbf{H}^{l+1}=\rho\left(\mathbf{P}^{K}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (18) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{l+1}=\rho\left(\mathbf{P}^{K}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (18) |'
- en: where $\mathbf{P}^{K}=\left(\mathbf{P}\right)^{K}$ is the transition probability
    of a length-$K$ diffusion process (i.e., random walks), $K$ is a preset diffusion
    length, and $\mathbf{\Theta}^{l}$ are learnable parameters. Because only $\mathbf{P}^{K}$
    depends on the graph structure, the parameters $\mathbf{\Theta}^{l}$ can be shared
    across graphs of arbitrary sizes. However, calculating $\mathbf{P}^{K}$ has a
    time complexity of $O\left(N^{2}K\right)$; thus, this method is not scalable to
    large graphs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{P}^{K}=\left(\mathbf{P}\right)^{K}$ 是长度为 $K$ 的扩散过程（即随机游走）的转移概率，$K$
    是预设的扩散长度，而 $\mathbf{\Theta}^{l}$ 是可学习的参数。由于只有 $\mathbf{P}^{K}$ 依赖于图结构，因此参数 $\mathbf{\Theta}^{l}$
    可以在任意大小的图中共享。然而，计算 $\mathbf{P}^{K}$ 的时间复杂度为 $O\left(N^{2}K\right)$；因此，这种方法无法扩展到大图。
- en: 'DGCN [[51](#bib.bib51)] was further proposed to jointly adopt the diffusion
    and the adjacency bases using a dual graph convolutional network. Specifically,
    DGCN used two convolutions: one was Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency
    Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")), and the other replaced the adjacency matrix with the positive
    pointwise mutual information (PPMI) matrix [[79](#bib.bib79)] of the transition
    probability as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: DGCN [[51](#bib.bib51)] 进一步提出了使用双图卷积网络联合采用扩散和邻接基础。具体而言，DGCN 使用了两种卷积：一种是 Eq.
    ([14](#S4.E14 "在 4.1.2 效率方面 ‣ 4.1 卷积操作 ‣ 4 图卷积网络 ‣ 深度学习在图上的应用：综述"))，另一种则用转移概率的正点互信息（PPMI）矩阵
    [[79](#bib.bib79)] 替换了邻接矩阵，如下所示：
- en: '|  | $\mathbf{Z}^{l+1}=\rho\left(\mathbf{D}^{-\frac{1}{2}}_{P}\mathbf{X}_{P}\mathbf{D}^{-\frac{1}{2}}_{P}\mathbf{Z}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (19) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{Z}^{l+1}=\rho\left(\mathbf{D}^{-\frac{1}{2}}_{P}\mathbf{X}_{P}\mathbf{D}^{-\frac{1}{2}}_{P}\mathbf{Z}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (19) |'
- en: 'where $\mathbf{X}_{P}$ is the PPMI matrix calculated as:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{X}_{P}$ 是计算出的 PPMI 矩阵：
- en: '|  | $\mathbf{X}_{P}(i,j)=\max\left(\log\left(\frac{\mathbf{P}(i,j)\sum_{i,j}\mathbf{P}(i,j)}{\sum_{i}\mathbf{P}(i,j)\sum_{j}\mathbf{P}(i,j)}\right),0\right),$
    |  | (20) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{X}_{P}(i,j)=\max\left(\log\left(\frac{\mathbf{P}(i,j)\sum_{i,j}\mathbf{P}(i,j)}{\sum_{i}\mathbf{P}(i,j)\sum_{j}\mathbf{P}(i,j)}\right),0\right),$
    |  | (20) |'
- en: and $\mathbf{D}_{P}(i,i)=\sum_{j}\mathbf{X}_{P}(i,j)$ is the diagonal degree
    matrix of $\mathbf{X}_{P}$. Then, these two convolutions were ensembled by minimizing
    the mean square differences between $\mathbf{H}$ and $\mathbf{Z}$. DGCN adopted
    a random walk sampling technique to accelerate the transition probability calculation.
    The experiments demonstrated that such dual convolutions were effective even for
    single-graph problems.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 而 $\mathbf{D}_{P}(i,i)=\sum_{j}\mathbf{X}_{P}(i,j)$ 是 $\mathbf{X}_{P}$ 的对角度矩阵。然后，这两种卷积通过最小化
    $\mathbf{H}$ 和 $\mathbf{Z}$ 之间的均方差来进行集成。DGCN 采用了一种随机游走采样技术来加速转移概率的计算。实验表明，即使在单图问题中，这种双重卷积也是有效的。
- en: 4.1.4 Frameworks
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 框架
- en: 'Based on the above two lines of works, MPNNs [[52](#bib.bib52)] were proposed
    as a unified framework for the graph convolution operation in the spatial domain
    using message-passing functions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述两行工作，MPNNs [[52](#bib.bib52)] 被提出作为一个统一的框架，用于在空间域中使用消息传递函数进行图卷积操作：
- en: '|  | $\begin{gathered}\mathbf{m}_{i}^{l+1}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}^{l}\left(\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{F}^{E}_{i,j}\right)\\
    \mathbf{h}_{i}^{l+1}=\mathcal{G}^{l}\left(\mathbf{h}_{i}^{l},\mathbf{m}_{i}^{l+1}\right),\end{gathered}$
    |  | (21) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}\mathbf{m}_{i}^{l+1}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}^{l}\left(\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{F}^{E}_{i,j}\right)\\
    \mathbf{h}_{i}^{l+1}=\mathcal{G}^{l}\left(\mathbf{h}_{i}^{l},\mathbf{m}_{i}^{l+1}\right),\end{gathered}$
    |  | (21) |'
- en: where $\mathcal{F}^{l}(\cdot)$ and $\mathcal{G}^{l}(\cdot)$ are the message
    functions and vertex update functions to be learned, respectively, and $\mathbf{m}^{l}$
    denotes the “messages” passed between nodes. Conceptually, MPNNs are a framework
    in which each node sends messages based on its states and updates its states based
    on messages received from the immediate neighbors. The authors showed that the
    above framework had included many existing methods such as GGS-NNs [[24](#bib.bib24)],
    Bruna et al. [[40](#bib.bib40)], Henaff et al. [[41](#bib.bib41)], Neural FPs [[46](#bib.bib46)],
    Kipf and Welling [[43](#bib.bib43)] and Kearnes et al. [[55](#bib.bib55)] as special
    cases. In addition, the authors proposed adding a “master” node that was connected
    to all the nodes to accelerate the message-passing across long distances, and
    they split the hidden representations into different “towers” to improve the generalization
    ability. The authors showed that a specific variant of MPNNs could achieve state-of-the-art
    performance in predicting molecular properties.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}^{l}(\cdot)$ 和 $\mathcal{G}^{l}(\cdot)$ 分别是需要学习的消息函数和顶点更新函数，而
    $\mathbf{m}^{l}$ 表示节点之间传递的“消息”。 从概念上讲，MPNNs 是一个框架，其中每个节点基于其状态发送消息，并根据从直接邻居接收到的消息更新其状态。作者表明，上述框架包含了许多现有方法，如
    GGS-NNs [[24](#bib.bib24)]、Bruna 等人 [[40](#bib.bib40)]、Henaff 等人 [[41](#bib.bib41)]、Neural
    FPs [[46](#bib.bib46)]、Kipf 和 Welling [[43](#bib.bib43)] 以及 Kearnes 等人 [[55](#bib.bib55)]
    作为特殊情况。此外，作者提出添加一个与所有节点连接的“主”节点，以加速跨长距离的消息传递，并将隐藏表示拆分成不同的“塔”以提高泛化能力。作者表明，MPNNs
    的特定变体在预测分子属性方面可以达到最先进的性能。
- en: 'Concurrently, GraphSAGE [[53](#bib.bib53)] took a similar idea as Eq. ([21](#S4.E21
    "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")) using multiple aggregating functions as
    follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，GraphSAGE [[53](#bib.bib53)] 采用了类似的思想，如 Eq. ([21](#S4.E21 "在 4.1.4 框架 ‣
    4.1 卷积操作 ‣ 4 图卷积网络 ‣ 图上的深度学习：综述"))，使用了多种聚合函数，如下所示：
- en: '|  | $\begin{gathered}\mathbf{m}_{i}^{l+1}=\text{AGGREGATE}^{l}(\{\mathbf{h}_{j}^{l},\forall
    j\in\mathcal{N}(i)\})\\ \mathbf{h}_{i}^{l+1}=\rho\left(\mathbf{\Theta}^{l}\left[\mathbf{h}_{i}^{l},\mathbf{m}_{i}^{l+1}\right]\right),\end{gathered}$
    |  | (22) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}\mathbf{m}_{i}^{l+1}=\text{AGGREGATE}^{l}(\{\mathbf{h}_{j}^{l},\forall
    j\in\mathcal{N}(i)\})\\ \mathbf{h}_{i}^{l+1}=\rho\left(\mathbf{\Theta}^{l}\left[\mathbf{h}_{i}^{l},\mathbf{m}_{i}^{l+1}\right]\right),\end{gathered}$
    |  | (22) |'
- en: 'where $\left[\cdot,\cdot\right]$ is the concatenation operation and $\text{AGGREGATE}(\cdot)$
    represents the aggregating function. The authors suggested three aggregating functions:
    the element-wise mean, an LSTM, and max-pooling as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\left[\cdot,\cdot\right]$ 是拼接操作，而 $\text{AGGREGATE}(\cdot)$ 代表聚合函数。作者建议了三种聚合函数：逐元素均值、LSTM
    和最大池化，如下所示：
- en: '|  | $\text{AGGREGATE}^{l}=\max\{\rho(\mathbf{\Theta}_{\text{pool}}\mathbf{h}_{j}^{l}+\mathbf{b}_{\text{pool}}),\forall
    j\in\mathcal{N}(i)\},$ |  | (23) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{AGGREGATE}^{l}=\max\{\rho(\mathbf{\Theta}_{\text{pool}}\mathbf{h}_{j}^{l}+\mathbf{b}_{\text{pool}}),\forall
    j\in\mathcal{N}(i)\},$ |  | (23) |'
- en: where $\mathbf{\Theta}_{\text{pool}}$ and $\mathbf{b}_{\text{pool}}$ are the
    parameters to be learned and $\max\left\{\cdot\right\}$ is the element-wise maximum.
    For the LSTM aggregating function, because an neighbors order is needed, the authors
    adopted a simple random order.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{\Theta}_{\text{pool}}$ 和 $\mathbf{b}_{\text{pool}}$ 是待学习的参数，而 $\max\left\{\cdot\right\}$
    是逐元素最大值。对于 LSTM 聚合函数，由于需要邻居的顺序，作者采用了简单的随机顺序。
- en: 'Mixture model network (MoNet) [[54](#bib.bib54)] also tried to unify the existing
    GCN models as well as CNNs for manifolds into a common framework using “template
    matching”:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型网络（MoNet）[[54](#bib.bib54)] 还尝试将现有的 GCN 模型以及用于流形的 CNN 统一为一个通用框架，使用了“模板匹配”：
- en: '|  | $h^{l+1}_{ik}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}^{l}_{k}(\mathbf{u}(i,j))\mathbf{h}^{l}_{j},k=1,...,f_{l+1},$
    |  | (24) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | $h^{l+1}_{ik}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}^{l}_{k}(\mathbf{u}(i,j))\mathbf{h}^{l}_{j},k=1,...,f_{l+1},$
    |  | (24) |'
- en: 'where $\mathbf{u}(i,j)$ are the pseudo-coordinates of the node pair $(v_{i},v_{j})$,
    $\mathcal{F}^{l}_{k}(\mathbf{u})$ is a parametric function to be learned, and
    $h^{l}_{ik}$ is the $k^{th}$ dimension of $\mathbf{h}^{l}_{i}$. In other words,
    $\mathcal{F}^{l}_{k}(\mathbf{u})$ served as a weighting kernel for combining neighborhoods.
    Then, MoNet adopted the following Gaussian kernel:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{u}(i,j)$ 是节点对 $(v_{i},v_{j})$ 的伪坐标，$\mathcal{F}^{l}_{k}(\mathbf{u})$
    是待学习的参数函数，而 $h^{l}_{ik}$ 是 $\mathbf{h}^{l}_{i}$ 的第 $k^{th}$ 维度。 换句话说，$\mathcal{F}^{l}_{k}(\mathbf{u})$
    作为加权核用于组合邻域。然后，MoNet 采用了以下高斯核：
- en: '|  | $\mathcal{F}^{l}_{k}(\mathbf{u})=\exp\left(-\frac{1}{2}(\mathbf{u}-\bm{\mu}^{l}_{k})^{T}(\mathbf{\Sigma}^{l}_{k})^{-1}(\mathbf{u}-\bm{\mu}^{l}_{k})\right),$
    |  | (25) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| | $\mathcal{F}^{l}_{k}(\mathbf{u})=\exp\left(-\frac{1}{2}(\mathbf{u}-\bm{\mu}^{l}_{k})^{T}(\mathbf{\Sigma}^{l}_{k})^{-1}(\mathbf{u}-\bm{\mu}^{l}_{k})\right),$
    | | (25) |'
- en: where $\bm{\mu}^{l}_{k}$ and $\mathbf{\Sigma}^{l}_{k}$ are the mean vectors
    and diagonal covariance matrices to be learned, respectively. The pseudo-coordinates
    were degrees as in Kipf and Welling [[43](#bib.bib43)], i.e.,
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bm{\mu}^{l}_{k}$和$\mathbf{\Sigma}^{l}_{k}$分别是要学习的均值向量和对角协方差矩阵。伪坐标是度，就像Kipf和Welling[[43](#bib.bib43)]中那样，
- en: '|  | $\mathbf{u}(i,j)=(\frac{1}{\sqrt{\mathbf{D}(i,i)}},\frac{1}{\sqrt{\mathbf{D}(j,j)}}).$
    |  | (26) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  |$\mathbf{u}(i,j)=(\frac{1}{\sqrt{\mathbf{D}(i,i)}},\frac{1}{\sqrt{\mathbf{D}(j,j)}}).$
    | |(26) |'
- en: 'Graph networks (GNs) [[9](#bib.bib9)] proposed a more general framework for
    both GCNs and GNNs that learned three sets of representations: $\mathbf{h}_{i}^{l},\mathbf{e}_{ij}^{l}$,
    and $\mathbf{z}^{l}$ as the representation for nodes, edges, and the entire graph,
    respectively. These representations were learned using three aggregation and three
    updating functions:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图网络（GNs）[[9](#bib.bib9)]提出了一个更通用的框架，用于学习三组表示：$\mathbf{h}_{i}^{l},\mathbf{e}_{ij}^{l}$和$\mathbf{z}^{l}$，分别作为节点、边和整个图的表示。这些表示是使用三个聚合和三个更新函数学习的：
- en: '|  | <math   alttext="\begin{gathered}\mathbf{m}_{i}^{l}=\mathcal{G}^{E\rightarrow
    V}(\{\mathbf{e}_{ij}^{l},\forall j\in\mathcal{N}(i)\}),\mathbf{m}_{V}^{l}=\mathcal{G}^{V\rightarrow
    G}(\{\mathbf{h}_{i}^{l},\forall v_{i}\in V\})\\ \mathbf{m}_{E}^{l}=\mathcal{G}^{E\rightarrow
    G}(\{\mathbf{e}_{ij}^{l},\forall(v_{i},v_{j})\in E\}),\mathbf{h}_{i}^{l+1}=\mathcal{F}^{V}(\mathbf{m}_{i}^{l},\mathbf{h}_{i}^{l},\mathbf{z}^{l})\\'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |<math alttext="\begin{gathered}\mathbf{m}_{i}^{l}=\mathcal{G}^{E\rightarrow
    V}(\{\mathbf{e}_{ij}^{l},\forall j\in\mathcal{N}(i)\}),\mathbf{m}_{V}^{l}=\mathcal{G}^{V\rightarrow
    G}(\{\mathbf{h}_{i}^{l},\forall v_{i}\in V\})\\ \mathbf{m}_{E}^{l}=\mathcal{G}^{E\rightarrow
    G}(\{\mathbf{e}_{ij}^{l},\forall(v_{i},v_{j})\in E\}),\mathbf{h}_{i}^{l+1}=\mathcal{F}^{V}(\mathbf{m}_{i}^{l},\mathbf{h}_{i}^{l},\mathbf{z}^{l})
    \\ '
- en: \mathbf{e}_{ij}^{l+1}=\mathcal{F}^{E}(\mathbf{e}_{ij}^{l},\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{z}^{l}),\mathbf{z}^{l+1}=\mathcal{F}^{G}(\mathbf{m}_{E}^{l},\mathbf{m}_{V}^{l},\mathbf{z}^{l}),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><mrow ><msubsup ><mi  >𝐦</mi><mi >i</mi><mi >l</mi></msubsup><mo
    >=</mo><mrow ><msup ><mi >𝒢</mi><mrow ><mi  >E</mi><mo stretchy="false"  >→</mo><mi
    >V</mi></mrow></msup><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mo stretchy="false" >{</mo><mrow ><mrow ><msubsup ><mi >𝐞</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo
    >,</mo><mrow ><mo rspace="0.167em"  >∀</mo><mi >j</mi></mrow></mrow><mo >∈</mo><mrow
    ><mi >𝒩</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mi
    >i</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo stretchy="false" >}</mo></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐦</mi><mi
    >V</mi><mi >l</mi></msubsup><mo >=</mo><mrow ><msup ><mi >𝒢</mi><mrow ><mi >V</mi><mo
    stretchy="false"  >→</mo><mi >G</mi></mrow></msup><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mo stretchy="false" >{</mo><mrow
    ><mrow ><msubsup ><mi >𝐡</mi><mi >i</mi><mi >l</mi></msubsup><mo >,</mo><mrow
    ><mo rspace="0.167em" >∀</mo><msub ><mi >v</mi><mi >i</mi></msub></mrow></mrow><mo
    >∈</mo><mi >V</mi></mrow><mo stretchy="false" >}</mo></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><mrow ><msubsup
    ><mi >𝐦</mi><mi >E</mi><mi >l</mi></msubsup><mo >=</mo><mrow ><msup ><mi >𝒢</mi><mrow
    ><mi >E</mi><mo stretchy="false" >→</mo><mi >G</mi></mrow></msup><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mo stretchy="false"
    >{</mo><mrow ><mrow ><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >j</mi></mrow><mi >l</mi></msubsup><mo >,</mo><mrow ><mo >∀</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >v</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >v</mi><mi >j</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo >∈</mo><mi >E</mi></mrow><mo
    stretchy="false" >}</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo><mrow ><msubsup ><mi >𝐡</mi><mi >i</mi><mrow ><mi >l</mi><mo >+</mo><mn
    >1</mn></mrow></msubsup><mo >=</mo><mrow ><msup ><mi >ℱ</mi><mi >V</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi
    >𝐦</mi><mi >i</mi><mi >l</mi></msubsup><mo >,</mo><msubsup ><mi >𝐡</mi><mi >i</mi><mi
    >l</mi></msubsup><mo >,</mo><msup ><mi >𝐳</mi><mi >l</mi></msup><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><mrow ><mrow
    ><msubsup ><mi >𝐞</mi><mrow ><mi  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >j</mi></mrow><mrow ><mi >l</mi><mo >+</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow
    ><msup ><mi >ℱ</mi><mi >E</mi></msup><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo >,</mo><msubsup
    ><mi >𝐡</mi><mi >i</mi><mi >l</mi></msubsup><mo >,</mo><msubsup ><mi >𝐡</mi><mi
    >j</mi><mi >l</mi></msubsup><mo >,</mo><msup ><mi >𝐳</mi><mi >l</mi></msup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msup ><mi >𝐳</mi><mrow
    ><mi >l</mi><mo >+</mo><mn >1</mn></mrow></msup><mo >=</mo><mrow ><msup ><mi >ℱ</mi><mi
    >G</mi></msup><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msubsup ><mi >𝐦</mi><mi >E</mi><mi >l</mi></msubsup><mo >,</mo><msubsup
    ><mi >𝐦</mi><mi >V</mi><mi >l</mi></msubsup><mo >,</mo><msup ><mi >𝐳</mi><mi >l</mi></msup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐦</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝒢</ci><apply ><ci  >→</ci><ci >𝐸</ci><ci >𝑉</ci></apply></apply><set
    ><apply ><list ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐞</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="latexml"  >for-all</csymbol><ci >𝑗</ci></apply></list><apply
    ><ci >𝒩</ci><ci >𝑖</ci></apply></apply></set></apply></apply><apply ><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐦</ci><ci >𝑉</ci></apply><ci >𝑙</ci></apply><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝒢</ci><apply ><ci >→</ci><ci
    >𝑉</ci><ci >𝐺</ci></apply></apply><set ><apply ><list ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐡</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply><apply ><csymbol cd="latexml" >for-all</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑣</ci><ci >𝑖</ci></apply></apply></list><ci
    >𝑉</ci></apply></set><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐦</ci><ci >𝐸</ci></apply><ci
    >𝑙</ci></apply></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝒢</ci><apply ><ci >→</ci><ci >𝐸</ci><ci >𝐺</ci></apply></apply><set
    ><apply ><list ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐞</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="latexml" >for-all</csymbol><interval closure="open"
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑣</ci><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑣</ci><ci >𝑗</ci></apply></interval></apply></list><ci
    >𝐸</ci></apply></set></apply></apply></apply></apply><apply ><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐡</ci><ci >𝑖</ci></apply><apply ><ci >𝑙</ci><cn type="integer"  >1</cn></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℱ</ci><ci >𝑉</ci></apply><vector
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐦</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐡</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐳</ci><ci >𝑙</ci></apply></vector><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐞</ci><apply ><ci >𝑖</ci><ci
    >𝑗</ci></apply></apply><apply ><ci  >𝑙</ci><cn type="integer"  >1</cn></apply></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℱ</ci><ci
    >𝐸</ci></apply><vector ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐞</ci><apply ><ci >𝑖</ci><ci
    >𝑗</ci></apply></apply><ci >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐡</ci><ci >𝑖</ci></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐡</ci><ci >𝑗</ci></apply><ci >𝑙</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐳</ci><ci >𝑙</ci></apply></vector></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐳</ci><apply ><ci
    >𝑙</ci><cn type="integer"  >1</cn></apply></apply><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >ℱ</ci><ci >𝐺</ci></apply><vector ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐦</ci><ci >𝐸</ci></apply><ci >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐦</ci><ci >𝑉</ci></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐳</ci><ci
    >𝑙</ci></apply></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{gathered}\mathbf{m}_{i}^{l}=\mathcal{G}^{E\rightarrow V}(\{\mathbf{e}_{ij}^{l},\forall
    j\in\mathcal{N}(i)\}),\mathbf{m}_{V}^{l}=\mathcal{G}^{V\rightarrow G}(\{\mathbf{h}_{i}^{l},\forall
    v_{i}\in V\})\\ \mathbf{m}_{E}^{l}=\mathcal{G}^{E\rightarrow G}(\{\mathbf{e}_{ij}^{l},\forall(v_{i},v_{j})\in
    E\}),\mathbf{h}_{i}^{l+1}=\mathcal{F}^{V}(\mathbf{m}_{i}^{l},\mathbf{h}_{i}^{l},\mathbf{z}^{l})\\
    \mathbf{e}_{ij}^{l+1}=\mathcal{F}^{E}(\mathbf{e}_{ij}^{l},\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{z}^{l}),\mathbf{z}^{l+1}=\mathcal{F}^{G}(\mathbf{m}_{E}^{l},\mathbf{m}_{V}^{l},\mathbf{z}^{l}),\end{gathered}</annotation></semantics></math>
    |  | (27) |
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{m}_{i}^{l}=\mathcal{G}^{E\rightarrow V}(\{\mathbf{e}_{ij}^{l},\forall
    j\in\mathcal{N}(i)\}),\mathbf{m}_{V}^{l}=\mathcal{G}^{V\rightarrow G}(\{\mathbf{h}_{i}^{l},\forall
    v_{i}\in V\})\\ \mathbf{m}_{E}^{l}=\mathcal{G}^{E\rightarrow G}(\{\mathbf{e}_{ij}^{l},\forall(v_{i},v_{j})\in
    E\}),\mathbf{h}_{i}^{l+1}=\mathcal{F}^{V}(\mathbf{m}_{i}^{l},\mathbf{h}_{i}^{l},\mathbf{z}^{l})\\
    \mathbf{e}_{ij}^{l+1}=\mathcal{F}^{E}(\mathbf{e}_{ij}^{l},\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{z}^{l}),\mathbf{z}^{l+1}=\mathcal{F}^{G}(\mathbf{m}_{E}^{l},\mathbf{m}_{V}^{l},\mathbf{z}^{l}),
- en: where $\mathcal{F}^{V}(\cdot),\mathcal{F}^{E}(\cdot)$, and $\mathcal{F}^{G}(\cdot)$
    are the corresponding updating functions for nodes, edges, and the entire graph,
    respectively, and $\mathcal{G}(\cdot)$ represents message-passing functions whose
    superscripts denote message-passing directions. Note that the message-passing
    functions all take a set as the input, thus their arguments are variable in length
    and these functions should be invariant to input permutations; some examples include
    the element-wise summation, mean, and maximum. Compared with MPNNs, GNs introduced
    the edge representations and the representation of the entire graph, thus making
    the framework more general.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{F}^{V}(\cdot)$、$\mathcal{F}^{E}(\cdot)$ 和 $\mathcal{F}^{G}(\cdot)$
    分别是节点、边和整个图的相应更新函数，而 $\mathcal{G}(\cdot)$ 表示消息传递函数，其上标表示消息传递的方向。注意，消息传递函数的输入都是一个集合，因此其参数长度可变，这些函数应该对输入的排列不变；一些例子包括逐元素求和、平均值和最大值。与MPNNs相比，GNs引入了边表示和整个图的表示，从而使得框架更具通用性。
- en: 'In summary, the convolution operations have evolved from the spectral domain
    to the spatial domain and from multistep neighbors to the immediate neighbors.
    Currently, gathering information from the immediate neighbors (as in Eq. ([14](#S4.E14
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey"))) and following the framework of
    Eqs. ([21](#S4.E21 "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))([22](#S4.E22 "In
    4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey"))([27](#S4.E27 "In 4.1.4 Frameworks ‣ 4.1
    Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs:
    A Survey")) are the most common choices for graph convolution operations.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，卷积操作已经从谱域发展到空间域，从多步邻居发展到直接邻居。目前，从直接邻居中收集信息（如在 Eq. ([14](#S4.E14 "In 4.1.2
    The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey"))) 并遵循 Eqs. ([21](#S4.E21 "In 4.1.4 Frameworks
    ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey"))([22](#S4.E22 "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))([27](#S4.E27
    "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")) 的框架是图卷积操作中最常见的选择。'
- en: 4.2 Readout Operations
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 读出操作
- en: Using graph convolution operations, useful node features can be learned to solve
    many node-focused tasks. However, to tackle graph-focused tasks, node information
    needs to be aggregated to form a graph-level representation. In the literature,
    such procedures are usually called the readout operations⁷⁷7Readout operations
    are also related to graph coarsening, i.e., reducing a large graph to a smaller
    graph, because a graph-level representation can be obtained by coarsening the
    graph to a single node. Some papers use these two terms interchangeably.. Based
    on a regular and local neighborhood, standard CNNs conduct multiple stride convolutions
    or poolings to gradually reduce the resolution. Since graphs lack a grid structure,
    these existing methods cannot be used directly.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图卷积操作，可以学习到有用的节点特征来解决许多以节点为重点的任务。然而，要解决以图为重点的任务，节点信息需要被聚合以形成图级别的表示。在文献中，这种过程通常称为读出操作⁷⁷7读出操作也与图粗化有关，即将大图缩减为小图，因为通过将图粗化为一个节点可以获得图级别的表示。一些论文将这两个术语互换使用..
    基于常规和局部邻域，标准CNN进行多次步幅卷积或池化操作以逐渐降低分辨率。由于图缺乏网格结构，这些现有方法不能直接使用。
- en: Order invariance. A critical requirement for the graph readout operations is
    that the operation should be invariant to the node order, i.e., if we change the
    indices of nodes and edges using a bijective function between two node sets, the
    representation of the entire graph should not change. For example, whether a drug
    can treat certain diseases depends on its inherent structure; thus, we should
    get identical results if we represent the drug using different node indices. Note
    that because this problem is related to the graph isomorphism problem, of which
    the best-known algorithm is quasipolynomial [[80](#bib.bib80)], we only can find
    a function that is order-invariant but not vice versa in a polynomial time, i.e.,
    even two structurally different graphs may have the same representation.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序不变性。图读出操作的一个关键要求是操作应对节点顺序不变，即，如果我们使用一个双射函数在两个节点集之间更改节点和边的索引，则整个图的表示不应改变。例如，药物是否能治疗某些疾病取决于其固有结构；因此，如果我们使用不同的节点索引表示药物，则应得到相同的结果。注意，由于这个问题与图同构问题有关，而已知的最佳算法是准多项式
    [[80](#bib.bib80)]，我们只能在多项式时间内找到一个顺序不变的函数，但不能找到一个反之亦然的函数，即即使两个结构不同的图也可能具有相同的表示。
- en: 4.2.1 Statistics
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 统计
- en: The most basic order-invariant operations involve simple statistics such as
    summation, averaging or max-pooling [[46](#bib.bib46), [50](#bib.bib50)], i.e.,
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的顺序不变操作包括简单的统计，如求和、平均或最大池化 [[46](#bib.bib46), [50](#bib.bib50)]，即，
- en: '|  | $\mathbf{h}_{G}=\sum_{i=1}^{N}\mathbf{h}^{L}_{i}\;\text{or}\;\mathbf{h}_{G}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{h}^{L}_{i}\;\text{or}\;\mathbf{h}_{G}=\max\left\{\mathbf{h}^{L}_{i},\forall
    i\right\},$ |  | (28) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}_{G}=\sum_{i=1}^{N}\mathbf{h}^{L}_{i}\;\text{或}\;\mathbf{h}_{G}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{h}^{L}_{i}\;\text{或}\;\mathbf{h}_{G}=\max\left\{\mathbf{h}^{L}_{i},\forall
    i\right\},$ |  | (28) |'
- en: where $\mathbf{h}_{G}$ is the representation of the graph $G$ and $\mathbf{h}^{L}_{i}$
    is the representation of node $v_{i}$ in the final layer $L$. However, such first-moment
    statistics may not be sufficiently representative to distinguish different graphs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{h}_{G}$ 是图 $G$ 的表示，$\mathbf{h}^{L}_{i}$ 是最终层 $L$ 中节点 $v_{i}$ 的表示。然而，这种一阶统计可能不足以区分不同的图。
- en: Kearnes et al. [[55](#bib.bib55)] suggested considering the distribution of
    node representations by using fuzzy histograms [[81](#bib.bib81)]. The basic idea
    behind fuzzy histograms is to construct several “histogram bins” and then calculate
    the memberships of $\mathbf{h}^{L}_{i}$ to these bins, i.e., by regarding node
    representations as samples and matching them to some pre-defined templates, and
    finally return the concatenation of the final histograms. In this way, nodes with
    the same sum/average/maximum but with different distributions can be distinguished.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Kearnes 等人 [[55](#bib.bib55)] 建议通过使用模糊直方图 [[81](#bib.bib81)] 来考虑节点表示的分布。模糊直方图的基本思想是构建几个“直方图箱”，然后计算
    $\mathbf{h}^{L}_{i}$ 对这些箱的隶属度，即将节点表示视为样本，并将其匹配到一些预定义的模板上，最后返回最终直方图的拼接。通过这种方式，可以区分具有相同和/平均/最大值但分布不同的节点。
- en: Another commonly used approach for aggregating node representation is to add
    a fully connected (FC) layer as the final layer [[40](#bib.bib40)], i.e.,
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常用的节点表示聚合方法是添加一个全连接（FC）层作为最终层 [[40](#bib.bib40)]，即，
- en: '|  | $\mathbf{h}_{G}=\rho\left(\left[\mathbf{H}^{L}\right]\mathbf{\Theta}_{FC}\right),$
    |  | (29) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}_{G}=\rho\left(\left[\mathbf{H}^{L}\right]\mathbf{\Theta}_{FC}\right),$
    |  | (29) |'
- en: 'where $\left[\mathbf{H}^{L}\right]\in\mathbb{R}^{Nf_{L}}$ is the concatenation
    of the final node representation $\mathbf{H}^{L}$, $\mathbf{\Theta}_{FC}\in\mathbb{R}^{Nf_{L}\times
    f_{\text{output}}}$ are parameters, and $f_{\text{output}}$ is the dimensionality
    of the output. Eq. ([29](#S4.E29 "In 4.2.1 Statistics ‣ 4.2 Readout Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) can be
    regarded as a weighted sum of node-level features. One advantage is that the model
    can learn different weights for different nodes; however, this ability comes at
    the cost of being unable to guarantee order invariance.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\left[\mathbf{H}^{L}\right]\in\mathbb{R}^{Nf_{L}}$ 是最终节点表示 $\mathbf{H}^{L}$
    的拼接，$\mathbf{\Theta}_{FC}\in\mathbb{R}^{Nf_{L}\times f_{\text{output}}}$ 是参数，$f_{\text{output}}$
    是输出的维度。公式 ([29](#S4.E29 "In 4.2.1 Statistics ‣ 4.2 Readout Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) 可以看作是节点级特征的加权和。一个优点是模型可以为不同的节点学习不同的权重；然而，这种能力的代价是无法保证顺序不变性。'
- en: 4.2.2 Hierarchical Clustering
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 层次聚类
- en: 'Rather than a dichotomy between node and graph level structures, graphs are
    known to exhibit rich hierarchical structures [[82](#bib.bib82)], which can be
    explored by hierarchical clustering methods as shown in Figure [4](#S4.F4 "Figure
    4 ‣ 4.2.2 Hierarchical Clustering ‣ 4.2 Readout Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey"). For example, a density-based agglomerative
    clustering [[83](#bib.bib83)] was used in Bruna et al. [[40](#bib.bib40)] and
    multi-resolution spectral clustering [[84](#bib.bib84)] was used in Henaff et al. [[41](#bib.bib41)].
    ChebNet [[42](#bib.bib42)] and MoNet [[54](#bib.bib54)] adopted another greedy
    hierarchical clustering algorithm, Graclus [[85](#bib.bib85)], to merge two nodes
    at a time, along with a fast pooling method to rearrange the nodes into a balanced
    binary tree. ECC [[63](#bib.bib63)] adopted another hierarchical clustering method
    by performing eigendecomposition [[86](#bib.bib86)]. However, these hierarchical
    clustering methods are all independent of the graph convolutions (i.e., they can
    be performed as a preprocessing step and are not trained in an end-to-end fashion).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 与节点和图层次结构之间的二分法不同，图形被认为展现了丰富的层次结构 [[82](#bib.bib82)]，可以通过如图 [4](#S4.F4 "图 4
    ‣ 4.2.2 层次聚类 ‣ 4.2 读取操作 ‣ 4 图卷积网络 ‣ 图上的深度学习：综述") 所示的层次聚类方法进行探索。例如，Bruna 等人 [[40](#bib.bib40)]
    使用了基于密度的聚合聚类 [[83](#bib.bib83)]，Henaff 等人 [[41](#bib.bib41)] 使用了多分辨率谱聚类 [[84](#bib.bib84)]。ChebNet
    [[42](#bib.bib42)] 和 MoNet [[54](#bib.bib54)] 采用了另一种贪婪层次聚类算法 Graclus [[85](#bib.bib85)]，该算法每次合并两个节点，并配有快速池化方法将节点重新排列成平衡的二叉树。ECC
    [[63](#bib.bib63)] 采用了另一种层次聚类方法，通过进行特征分解 [[86](#bib.bib86)]。然而，这些层次聚类方法都与图卷积无关（即，它们可以作为预处理步骤进行，而不是以端到端的方式训练）。
- en: '![Refer to caption](img/eaf892d57e3d64a555f9f149a708169e.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eaf892d57e3d64a555f9f149a708169e.png)'
- en: 'Figure 4: An example of performing a hierarchical clustering algorithm. Reprinted
    from [[56](#bib.bib56)] with permission.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：执行层次聚类算法的示例。经 [[56](#bib.bib56)] 许可转载。
- en: 'To solve that problem, DiffPool [[56](#bib.bib56)] proposed a differentiable
    hierarchical clustering algorithm jointly trained with the graph convolutions.
    Specifically, the authors proposed learning a soft cluster assignment matrix in
    each layer using the hidden representations as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，DiffPool [[56](#bib.bib56)] 提出了一个与图卷积共同训练的可微层次聚类算法。具体来说，作者提出了在每一层使用隐藏表示来学习软聚类分配矩阵，如下所示：
- en: '|  | $\mathbf{S}^{l}=\mathcal{F}\left(\mathbf{A}^{l},\mathbf{H}^{l}\right),$
    |  | (30) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{S}^{l}=\mathcal{F}\left(\mathbf{A}^{l},\mathbf{H}^{l}\right),$
    |  | (30) |'
- en: 'where $\mathbf{S}^{l}\in\mathbb{R}^{N_{l}\times N_{l+1}}$ is the cluster assignment
    matrix, $N_{l}$ is the number of clusters in the layer $l$ and $\mathcal{F}(\cdot)$
    is a function to be learned. Then, the node representations and the new adjacency
    matrix for this “coarsened” graph can be obtained by taking the average according
    to $\mathbf{S}^{l}$ as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{S}^{l}\in\mathbb{R}^{N_{l}\times N_{l+1}}$ 是聚类分配矩阵，$N_{l}$ 是第 $l$
    层的聚类数量，$\mathcal{F}(\cdot)$ 是待学习的函数。然后，可以通过根据 $\mathbf{S}^{l}$ 取平均来获得这个“粗化”图的节点表示和新的邻接矩阵，如下所示：
- en: '|  | $\mathbf{H}^{l+1}=(\mathbf{S}^{l})^{T}\hat{\mathbf{H}}^{l+1},\mathbf{A}^{l+1}=(\mathbf{S}^{l})^{T}\mathbf{A}^{l}\mathbf{S}^{l},$
    |  | (31) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{l+1}=(\mathbf{S}^{l})^{T}\hat{\mathbf{H}}^{l+1},\mathbf{A}^{l+1}=(\mathbf{S}^{l})^{T}\mathbf{A}^{l}\mathbf{S}^{l},$
    |  | (31) |'
- en: where $\hat{\mathbf{H}}^{l+1}$ is obtained by applying a graph convolution layer
    to $\mathbf{H}^{l}$, i.e., coarsening the graph from $N_{l}$ nodes to $N_{l+1}$
    nodes in each layer after the convolution operation. The initial number of nodes
    is $N_{0}=N$ and the last layer is $N_{L}=1$, i.e., a single node that represents
    the entire graph. Because the cluster assignment operation is soft, the connections
    between clusters are not sparse; thus the time complexity of the method is $O(N^{2})$
    in principle.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{\mathbf{H}}^{l+1}$ 是通过对 $\mathbf{H}^{l}$ 应用图卷积层得到的，即在卷积操作后，将图从 $N_{l}$
    节点粗化到每层的 $N_{l+1}$ 节点。初始节点数量为 $N_{0}=N$，最后一层为 $N_{L}=1$，即表示整个图的单个节点。由于聚类分配操作是软性的，聚类之间的连接不是稀疏的，因此该方法的时间复杂度原则上是
    $O(N^{2})$。
- en: 4.2.3 Imposing Orders and Others
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 强加顺序及其他
- en: 'As mentioned in Section [4.1.3](#S4.SS1.SSS3 "4.1.3 The Aspect of Multiple
    Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey"), PATCHY-SAN [[47](#bib.bib47)] and SortPooling [[49](#bib.bib49)]
    took the idea of imposing a node order and then resorted to standard 1-D pooling
    as in CNNs. Whether these methods can preserve order invariance depends on how
    the order is imposed, which is another research field that we refer readers to [[87](#bib.bib87)]
    for a survey. However, whether imposing a node order is a natural choice for graphs
    and if so, what the best node orders are constitute still on-going research topics.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the aforementioned methods, there are some heuristics. In GNNs [[23](#bib.bib23)],
    the authors suggested adding a special node connected to all nodes to represent
    the entire graph. Similarly, GNs [[9](#bib.bib9)] proposed to directly learn the
    representation of the entire graph by receiving messages from all nodes and edges.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: MPNNs adopted set2set [[88](#bib.bib88)], a modification of the seq2seq model.
    Specifically, set2set uses a “Read-Process-and-Write” model that receives all
    inputs simultaneously, computes internal memories using an attention mechanism
    and an LSTM, and then writes the outputs. Unlike seq2seq which is order-sensitive,
    set2set is invariant to the input order.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Summary
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In short, statistics such as averaging or summation are the most simple readout
    operations, while hierarchical clustering algorithms jointly trained with graph
    convolutions are more advanced but are also more sophisticated. Other methods
    such as adding a pseudo node or imposing a node order have also been investigated.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Improvements and Discussions
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many techniques have been introduced to further improve GCNs. Note that some
    of these methods are general and could be applied to other deep learning models
    on graphs as well.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Attention Mechanism
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the aforementioned GCNs, the node neighborhoods are aggregated with equal
    or pre-defined weights. However, the influences of neighbors can vary greatly;
    thus, they should be learned during training rather than being predetermined.
    Inspired by the attention mechanism [[89](#bib.bib89)], graph attention network
    (GAT) [[57](#bib.bib57)] introduces the attention mechanism into GCNs by modifying
    the convolution operation in Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect
    ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}^{l+1}_{i}=\rho\left(\sum\nolimits_{j\in\hat{\mathcal{N}}(i)}\alpha_{ij}^{l}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right),$
    |  | (32) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: 'where $\alpha_{ij}^{l}$ is node $v_{i}$’s attention to node $v_{j}$ in the
    $l^{th}$ layer:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\alpha_{ij}^{l}=\frac{\exp\left(\text{LeakyReLU}\left(\mathcal{F}\left(\mathbf{h}^{l}_{i}\mathbf{\Theta}^{l},\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right)\right)\right)}{\sum_{k\in\hat{\mathcal{N}}(i)}\exp\left(\text{LeakyReLU}\left(\mathcal{F}\left(\mathbf{h}^{l}_{i}\mathbf{\Theta}^{l},\mathbf{h}^{l}_{k}\mathbf{\Theta}^{l}\right)\right)\right)},$
    |  | (33) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{ij}^{l}=\frac{\exp\left(\text{LeakyReLU}\left(\mathcal{F}\left(\mathbf{h}^{l}_{i}\mathbf{\Theta}^{l},\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right)\right)\right)}{\sum_{k\in\hat{\mathcal{N}}(i)}\exp\left(\text{LeakyReLU}\left(\mathcal{F}\left(\mathbf{h}^{l}_{i}\mathbf{\Theta}^{l},\mathbf{h}^{l}_{k}\mathbf{\Theta}^{l}\right)\right)\right)},$
    |  | (33) |'
- en: 'where $\mathcal{F}(\cdot,\cdot)$ is another function to be learned such as
    a multi-layer perceptron (MLP). To improve model capacity and stability, the authors
    also suggested using multiple independent attentions and concatenating the results,
    i.e., the multi-head attention mechanism [[89](#bib.bib89)] as illustrated in
    Figure [5](#S4.F5 "Figure 5 ‣ 4.3.1 Attention Mechanism ‣ 4.3 Improvements and
    Discussions ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey").
    GaAN [[58](#bib.bib58)] further proposed learning different weights for different
    heads and applied such a method to the traffic forecasting problem.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\mathcal{F}(\cdot,\cdot)$ 是另一个需要学习的函数，例如多层感知机（MLP）。为了提高模型的容量和稳定性，作者还建议使用多个独立的注意力并将结果进行拼接，即多头注意力机制 [[89](#bib.bib89)]，如图 [5](#S4.F5
    "Figure 5 ‣ 4.3.1 Attention Mechanism ‣ 4.3 Improvements and Discussions ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey") 所示。GaAN [[58](#bib.bib58)]
    进一步提出了对不同头学习不同权重的方法，并将这种方法应用于交通预测问题。'
- en: HAN [[59](#bib.bib59)] proposed a two-level attention mechanism, i.e., a node-level
    and a semantic-level attention mechanism, for heterogeneous graphs. Specifically,
    the node-level attention mechanism was similar to a GAT, but also considerd node
    types; therefore, it could assign different weights to aggregating meta-path-based
    neighbors. The semantic-level attention then learned the importance of different
    meta-paths and outputed the final results.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: HAN [[59](#bib.bib59)] 提出了一个两级注意力机制，即节点级和语义级注意力机制，用于异构图。具体来说，节点级注意力机制类似于 GAT，但还考虑了节点类型，因此可以为基于元路径的邻居分配不同的权重。语义级注意力机制则学习不同元路径的重要性并输出最终结果。
- en: '![Refer to caption](img/ed0729d23fce33a1585746df5caacb26.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ed0729d23fce33a1585746df5caacb26.png)'
- en: 'Figure 5: An illustration of the multi-head attention mechanism proposed in
    GAT [[57](#bib.bib57)] (reprinted with permission). Each color denotes an independent
    attention vector.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：GAT [[57](#bib.bib57)] 提出的多头注意力机制的示意图（经许可转载）。每种颜色表示一个独立的注意力向量。
- en: 4.3.2 Residual and Jumping Connections
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 残差和跳跃连接
- en: 'Many researches have observed that the most suitable depth for the existing
    GCNs is often very limited, e.g., 2 or 3 layers. This problem is potentially due
    to the practical difficulties involved in training deep GCNs or the over-smoothing
    problem, i.e., all nodes in deeper layers have the same representation [[70](#bib.bib70),
    [62](#bib.bib62)]. To remedy this problem, residual connections similar to ResNet [[90](#bib.bib90)]
    can be added to GCNs. For example, Kipf and Welling [[43](#bib.bib43)] added residual
    connections into Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '许多研究观察到，现有 GCNs 的最适合深度通常非常有限，例如 2 层或 3 层。这个问题可能是由于训练深层 GCNs 的实际困难或过度平滑问题，即所有节点在更深层次上具有相同的表示 [[70](#bib.bib70),
    [62](#bib.bib62)]。为了解决这个问题，可以将类似 ResNet [[90](#bib.bib90)] 的残差连接添加到 GCNs 中。例如，Kipf
    和 Welling [[43](#bib.bib43)] 将残差连接添加到 Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency
    Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) 中，如下所示：'
- en: '|  | $\mathbf{H}^{l+1}=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right)+\mathbf{H}^{l}.$
    |  | (34) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{l+1}=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right)+\mathbf{H}^{l}.$
    |  | (34) |'
- en: They showed experimentally that adding such residual connections could allow
    the depth of the network to increase, which is similar to the results of ResNet.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 他们通过实验证明，添加这样的残差连接可以使网络的深度增加，这与 ResNet 的结果类似。
- en: 'Column network (CLN) [[60](#bib.bib60)] adopted a similar idea by using the
    following residual connections with learnable weights:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Column network (CLN) [[60](#bib.bib60)] 采用了类似的思想，通过使用以下具有可学习权重的残差连接：
- en: '|  | $\mathbf{h}^{l+1}_{i}=\bm{\alpha}^{l}_{i}\odot\widetilde{\mathbf{h}}^{l+1}_{i}+(1-\bm{\alpha}^{l}_{i})\odot\mathbf{h}^{l}_{i},$
    |  | (35) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}^{l+1}_{i}=\bm{\alpha}^{l}_{i}\odot\widetilde{\mathbf{h}}^{l+1}_{i}+(1-\bm{\alpha}^{l}_{i})\odot\mathbf{h}^{l}_{i},$
    |  | (35) |'
- en: 'where $\widetilde{\mathbf{h}}^{l+1}_{i}$ is calculated similar to Eq. ([14](#S4.E14
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) and $\bm{\alpha}^{l}_{i}$ is a
    set of weights calculated as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\widetilde{\mathbf{h}}^{l+1}_{i}$ 是类似于方程 ([14](#S4.E14 "In 4.1.2 The Efficiency
    Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) 计算的，$\bm{\alpha}^{l}_{i}$ 是一组权重，其计算方式如下：'
- en: '|  | $\bm{\alpha}^{l}_{i}=\rho\left(\mathbf{b}^{l}_{\alpha}+\mathbf{\Theta}_{\alpha}^{l}\mathbf{h}_{i}^{l}+\mathbf{\Theta}_{\alpha}^{{}^{\prime}l}\sum\nolimits_{j\in\mathcal{N}(i)}\mathbf{h}_{j}^{l}\right),$
    |  | (36) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{\alpha}^{l}_{i}=\rho\left(\mathbf{b}^{l}_{\alpha}+\mathbf{\Theta}_{\alpha}^{l}\mathbf{h}_{i}^{l}+\mathbf{\Theta}_{\alpha}^{{}^{\prime}l}\sum\nolimits_{j\in\mathcal{N}(i)}\mathbf{h}_{j}^{l}\right),$
    |  | (36) |'
- en: 'where $\mathbf{b}^{l}_{\alpha},\mathbf{\Theta}_{\alpha}^{l},\mathbf{\Theta}_{\alpha}^{{}^{\prime}l}$
    are parameters. Note that Eq. ([35](#S4.E35 "In 4.3.2 Residual and Jumping Connections
    ‣ 4.3 Improvements and Discussions ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) is very similar to the GRU as in GGS-NNs [[24](#bib.bib24)].
    The differences are that in a CLN, the superscripts denote the number of layers,
    and different layers contain different parameters, while in GGS-NNs, the superscript
    denotes the pseudo time and a single set of parameters is used across time steps.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{b}^{l}_{\alpha},\mathbf{\Theta}_{\alpha}^{l},\mathbf{\Theta}_{\alpha}^{{}^{\prime}l}$
    是参数。注意，方程 ([35](#S4.E35 "In 4.3.2 Residual and Jumping Connections ‣ 4.3 Improvements
    and Discussions ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A
    Survey")) 与 GGS-NNs [[24](#bib.bib24)] 中的 GRU 非常相似。不同之处在于，CLN 中的上标表示层数，不同层包含不同的参数，而
    GGS-NNs 中的上标表示伪时间，并且在时间步长之间使用一组参数。'
- en: 'Inspired by personalized PageRank, PPNP [[61](#bib.bib61)] defined graph convolutions
    with teleportation to the initial layer:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 受到个性化 PageRank 的启发，PPNP [[61](#bib.bib61)] 定义了带有传送到初始层的图卷积：
- en: '|  | $\mathbf{H}^{l+1}=(1-\alpha)\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}+\alpha\mathbf{H}^{0},$
    |  | (37) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{l+1}=(1-\alpha)\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}+\alpha\mathbf{H}^{0},$
    |  | (37) |'
- en: where $\mathbf{H}_{0}=\mathcal{F}_{\theta}(\mathbf{F}^{V})$ and $\alpha$ is
    a hyper-parameter. Note that all the parameters are in $\mathcal{F}_{\theta}(\cdot)$
    rather than in the graph convolutions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{H}_{0}=\mathcal{F}_{\theta}(\mathbf{F}^{V})$ 且 $\alpha$ 是一个超参数。注意，所有的参数都在
    $\mathcal{F}_{\theta}(\cdot)$ 中，而不是在图卷积中。
- en: 'Jumping knowledge networks (JK-Nets) [[62](#bib.bib62)] proposed another architecture
    to connect the last layer of the network with all the lower hidden layers, i.e.,
    by “jumping” all the representations to the final output, as illustrated in Figure [6](#S4.F6
    "Figure 6 ‣ 4.3.2 Residual and Jumping Connections ‣ 4.3 Improvements and Discussions
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"). In this
    way, the model can learn to selectively exploit information from different layers.
    Formally, JK-Nets was formulated as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '跳跃知识网络（JK-Nets）[[62](#bib.bib62)] 提出了另一种架构，通过将网络的最后一层与所有较低的隐藏层连接起来，即通过“跳跃”所有表示到最终输出，如图
    [6](#S4.F6 "Figure 6 ‣ 4.3.2 Residual and Jumping Connections ‣ 4.3 Improvements
    and Discussions ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A
    Survey") 所示。这样，模型可以学会选择性地利用来自不同层的信息。形式上，JK-Nets 被表述如下：'
- en: '|  | $\mathbf{h}_{i}^{\text{final}}=\text{AGGREGATE}(\mathbf{h}_{i}^{0},\mathbf{h}_{i}^{1},...,\mathbf{h}_{i}^{L}),$
    |  | (38) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}_{i}^{\text{final}}=\text{AGGREGATE}(\mathbf{h}_{i}^{0},\mathbf{h}_{i}^{1},...,\mathbf{h}_{i}^{L}),$
    |  | (38) |'
- en: 'where $\mathbf{h}_{i}^{\text{final}}$ is the final representation for node
    $v_{i}$, AGGREGATE$(\cdot)$ is the aggregating function, and $L$ is the number
    of hidden layers. JK-Nets used three aggregating functions similar to GraphSAGE [[53](#bib.bib53)]:
    concatenation, max-pooling, and the LSTM attention. The experimental results showed
    that adding jump connections could improve the performance of multiple GCNs.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{h}_{i}^{\text{final}}$ 是节点 $v_{i}$ 的最终表示，AGGREGATE$(\cdot)$ 是聚合函数，$L$
    是隐藏层的数量。JK-Nets 使用了三种类似于 GraphSAGE [[53](#bib.bib53)] 的聚合函数：连接、最大池化和 LSTM 注意力。实验结果表明，添加跳跃连接可以提高多个
    GCN 的性能。
- en: '![Refer to caption](img/f01752a8f1cbff94a422cef1749168f0.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f01752a8f1cbff94a422cef1749168f0.png)'
- en: 'Figure 6: Jumping knowledge networks proposed in [[62](#bib.bib62)] in which
    the last layer is connected to all the other layers to selectively exploit different
    information from different layers. GC denotes graph convolutions. Reprinted with
    permission.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在 [[62](#bib.bib62)] 中提出的跳跃知识网络，其中最后一层与所有其他层连接，以选择性地利用来自不同层的信息。GC 表示图卷积。经授权转载。
- en: 4.3.3 Edge Features
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 边特征
- en: 'The aforementioned GCNs mostly focus on utilizing node features and graph structures.
    In this subsection, we briefly discuss how to use another important source of
    information: the edge features.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 GCN 主要集中在利用节点特征和图结构。在本小节中，我们简要讨论如何使用另一个重要的信息来源：边特征。
- en: For simple edge features with discrete values such as the edge type, a straightforward
    method is to train different parameters for different edge types and aggregate
    the results. For example, Neural FPs [[46](#bib.bib46)] trained different parameters
    for nodes with different degrees, which corresponds to the implicit edge feature
    of bond types in a molecular graph, and then summed over the results. CLN [[60](#bib.bib60)]
    trained different parameters for different edge types in a heterogeneous graph
    and averaged the results. Edge-conditioned convolution (ECC) [[63](#bib.bib63)]
    also trained different parameters based on edge types and applied them to graph
    classification. Relational GCNs (R-GCNs) [[64](#bib.bib64)] adopted a similar
    idea for knowledge graphs by training different weights for different relation
    types. However, these methods are suitable only for a limited number of discrete
    edge features.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散值的简单边特征，如边的类型，一种直接的方法是为不同的边类型训练不同的参数，并聚合结果。例如，Neural FPs [[46](#bib.bib46)]
    为具有不同度数的节点训练了不同的参数，这对应于分子图中的隐式边特征（键类型），然后对结果进行求和。CLN [[60](#bib.bib60)] 为异构图中的不同边类型训练了不同的参数，并对结果进行了平均。边条件卷积
    (ECC) [[63](#bib.bib63)] 也基于边类型训练了不同的参数，并将其应用于图分类。关系 GCNs (R-GCNs) [[64](#bib.bib64)]
    为知识图谱采用了类似的思想，通过为不同的关系类型训练不同的权重。然而，这些方法仅适用于有限数量的离散边特征。
- en: DCNN [[50](#bib.bib50)] proposed another method to convert each edge into a
    node connected to the head and tail node of that edge. After this conversion,
    edge features can be treated as node features.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: DCNN [[50](#bib.bib50)] 提出了另一种方法，将每条边转换为连接到该边头节点和尾节点的节点。经过这种转换，边特征可以被视为节点特征。
- en: 'LGCN [[65](#bib.bib65)] constructed a line graph $\mathbf{B}\in\mathbb{R}^{2M\times
    2M}$ to incorporate edge features as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: LGCN [[65](#bib.bib65)] 构建了一个线图 $\mathbf{B}\in\mathbb{R}^{2M\times 2M}$ 来整合边特征，如下所示：
- en: '|  | $\mathbf{B}_{i\rightarrow j,i^{\prime}\rightarrow j^{\prime}}=\left\{\begin{aligned}
    &amp;1\quad\text{if}\;j=i^{\prime}\;\text{and}\;j^{\prime}\neq i,\\ &amp;0\quad\text{otherwise}.\end{aligned}\right.$
    |  | (39) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{B}_{i\rightarrow j,i^{\prime}\rightarrow j^{\prime}}=\left\{\begin{aligned}
    &1\quad\text{if}\;j=i^{\prime}\;\text{and}\;j^{\prime}\neq i,\\ &0\quad\text{otherwise}.\end{aligned}\right.$
    |  | (39) |'
- en: 'In other words, nodes in the line graph are directed edges in the original
    graph, and two nodes in the line graph are connected if information can flow through
    their corresponding edges in the original graph. Then, LGCN adopted two GCNs:
    one on the original graph and one on the line graph.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，线图中的节点是原图中的有向边，如果信息可以通过原图中的相应边流动，则线图中的两个节点相连。然后，LGCN 采用了两个 GCN：一个在原图上，另一个在线图上。
- en: 'Kearnes et al. [[55](#bib.bib55)] proposed an architecture using a “weave module”.
    Specifically, they learned representations for both nodes and edges and exchanged
    information between them in each weave module using four different functions:
    node-to-node (NN), node-to-edge (NE), edge-to-edge (EE) and edge-to-node (EN):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Kearnes 等人 [[55](#bib.bib55)] 提出了一个使用“编织模块”的架构。具体而言，他们为节点和边学习了表示，并在每个编织模块中使用四种不同的函数交换信息：节点到节点
    (NN)、节点到边 (NE)、边到边 (EE) 和边到节点 (EN)：
- en: '|  | <math   alttext="\begin{gathered}\mathbf{h}^{l^{\prime}}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{0}_{i},\mathbf{h}^{1}_{i},...,\mathbf{h}^{l}_{i}),\mathbf{h}^{l^{\prime\prime}}_{i}=\mathcal{F}_{EN}(\{\mathbf{e}^{l}_{ij}&#124;j\in\mathcal{N}(i)\})\\
    \mathbf{e}^{l^{\prime}}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{0}_{ij},\mathbf{e}^{1}_{ij},...,\mathbf{e}^{l}_{ij}),\mathbf{e}^{l^{\prime\prime}}_{ij}=\mathcal{F}_{NE}(\mathbf{h}^{l}_{i},\mathbf{h}^{l}_{j})\\'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{gathered}\mathbf{h}^{l^{\prime}}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{0}_{i},\mathbf{h}^{1}_{i},...,\mathbf{h}^{l}_{i}),\mathbf{h}^{l^{\prime\prime}}_{i}=\mathcal{F}_{EN}(\{\mathbf{e}^{l}_{ij}&#124;j\in\mathcal{N}(i)\})\\
    \mathbf{e}^{l^{\prime}}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{0}_{ij},\mathbf{e}^{1}_{ij},...,\mathbf{e}^{l}_{ij}),\mathbf{e}^{l^{\prime\prime}}_{ij}=\mathcal{F}_{NE}(\mathbf{h}^{l}_{i},\mathbf{h}^{l}_{j})\\'
- en: \mathbf{h}^{l+1}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{l^{\prime}}_{i},\mathbf{h}^{l^{\prime\prime}}_{i}),\mathbf{e}^{l+1}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{l^{\prime}}_{ij},\mathbf{e}^{l^{\prime\prime}}_{ij}),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><mrow ><msubsup ><mi  >𝐡</mi><mi >i</mi><msup ><mi  >l</mi><mo >′</mo></msup></msubsup><mo
    >=</mo><mrow ><msub ><mi >ℱ</mi><mrow ><mi  >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >N</mi></mrow></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msubsup ><mi >𝐡</mi><mi >i</mi><mn >0</mn></msubsup><mo >,</mo><msubsup
    ><mi >𝐡</mi><mi >i</mi><mn >1</mn></msubsup><mo >,</mo><mi mathvariant="normal"  >…</mi><mo
    >,</mo><msubsup ><mi >𝐡</mi><mi >i</mi><mi >l</mi></msubsup><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐡</mi><mi >i</mi><msup
    ><mi >l</mi><mo >′′</mo></msup></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow
    ><mi >E</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >N</mi></mrow></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mo
    stretchy="false" >{</mo><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo lspace="0em" rspace="0em"
    >&#124;</mo><mrow ><mi >j</mi><mo >∈</mo><mrow ><mi >𝒩</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mi >i</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    stretchy="false" >}</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow ><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >j</mi></mrow><msup ><mi  >l</mi><mo >′</mo></msup></msubsup><mo >=</mo><mrow
    ><msub ><mi >ℱ</mi><mrow ><mi >E</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >E</mi></mrow></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >j</mi></mrow><mn >0</mn></msubsup><mo >,</mo><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow><mn >1</mn></msubsup><mo
    >,</mo><mi mathvariant="normal" >…</mi><mo >,</mo><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐞</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><msup ><mi
    >l</mi><mo >′′</mo></msup></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow
    ><mi >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >E</mi></mrow></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi
    >𝐡</mi><mi >i</mi><mi >l</mi></msubsup><mo >,</mo><msubsup ><mi >𝐡</mi><mi >j</mi><mi
    >l</mi></msubsup><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow ><mrow ><msubsup ><mi >𝐡</mi><mi >i</mi><mrow ><mi  >l</mi><mo
    >+</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow ><mi
    >N</mi><mo lspace="0em" rspace="0em" >​</mo><mi >N</mi></mrow></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >𝐡</mi><mi
    >i</mi><msup ><mi  >l</mi><mo >′</mo></msup></msubsup><mo >,</mo><msubsup ><mi
    >𝐡</mi><mi >i</mi><msup ><mi >l</mi><mo >′′</mo></msup></msubsup><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><mrow ><mi >l</mi><mo
    >+</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow ><mi
    >E</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >E</mi></mrow></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >𝐞</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><msup ><mi
    >l</mi><mo >′</mo></msup></msubsup><mo >,</mo><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow><msup ><mi >l</mi><mo
    >′′</mo></msup></msubsup><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" ><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><csymbol cd="ambiguous"
    >formulae-sequence</csymbol><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑙</ci><ci >′</ci></apply></apply><ci >𝑖</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℱ</ci><apply ><ci  >𝑁</ci><ci
    >𝑁</ci></apply></apply><vector ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><cn type="integer" >0</cn></apply><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><cn type="integer"  >1</cn></apply><ci
    >𝑖</ci></apply><ci >…</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><ci >𝑙</ci></apply><ci
    >𝑖</ci></apply></vector></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐡</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑙</ci><ci >′′</ci></apply></apply><ci
    >𝑖</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℱ</ci><apply ><ci >𝐸</ci><ci >𝑁</ci></apply></apply><apply ><csymbol cd="latexml"
    >conditional-set</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐞</ci><ci >𝑙</ci></apply><apply
    ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply ><ci >𝑗</ci><apply ><ci >𝒩</ci><ci
    >𝑖</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐞</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑙</ci><ci >′</ci></apply></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℱ</ci><apply
    ><ci >𝐸</ci><ci >𝐸</ci></apply></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐞</ci><cn type="integer" >0</cn></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐞</ci><cn type="integer"  >1</cn></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><ci
    >…</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝐞</ci><ci >𝑙</ci></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply></vector></apply></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐞</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑙</ci><ci >′′</ci></apply></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℱ</ci><apply ><ci >𝑁</ci><ci
    >𝐸</ci></apply></apply><interval closure="open" ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐡</ci><ci >𝑙</ci></apply><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><ci >𝑙</ci></apply><ci
    >𝑗</ci></apply></interval><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><apply ><ci >𝑙</ci><cn
    type="integer" >1</cn></apply></apply><ci >𝑖</ci></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℱ</ci><apply
    ><ci >𝑁</ci><ci >𝑁</ci></apply></apply><interval closure="open" ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐡</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑙</ci><ci >′</ci></apply></apply><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑙</ci><ci >′′</ci></apply></apply><ci >𝑖</ci></apply></interval></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝐞</ci><apply ><ci >𝑙</ci><cn type="integer"  >1</cn></apply></apply><apply
    ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ℱ</ci><apply ><ci >𝐸</ci><ci >𝐸</ci></apply></apply><interval
    closure="open" ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐞</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑙</ci><ci >′</ci></apply></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐞</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑙</ci><ci >′′</ci></apply></apply><apply
    ><ci >𝑖</ci><ci >𝑗</ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{gathered}\mathbf{h}^{l^{\prime}}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{0}_{i},\mathbf{h}^{1}_{i},...,\mathbf{h}^{l}_{i}),\mathbf{h}^{l^{\prime\prime}}_{i}=\mathcal{F}_{EN}(\{\mathbf{e}^{l}_{ij}&#124;j\in\mathcal{N}(i)\})\\
    \mathbf{e}^{l^{\prime}}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{0}_{ij},\mathbf{e}^{1}_{ij},...,\mathbf{e}^{l}_{ij}),\mathbf{e}^{l^{\prime\prime}}_{ij}=\mathcal{F}_{NE}(\mathbf{h}^{l}_{i},\mathbf{h}^{l}_{j})\\
    \mathbf{h}^{l+1}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{l^{\prime}}_{i},\mathbf{h}^{l^{\prime\prime}}_{i}),\mathbf{e}^{l+1}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{l^{\prime}}_{ij},\mathbf{e}^{l^{\prime\prime}}_{ij}),\end{gathered}</annotation></semantics></math>
    |  | (40) |
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'where $\mathbf{e}_{ij}^{l}$ is the representation of edge $(v_{i},v_{j})$ in
    the $l^{th}$ layer and $\mathcal{F}(\cdot)$ are learnable functions whose subscripts
    represent message-passing directions. By stacking multiple such modules, information
    can propagate by alternately passing between node and edge representations. Note
    that in the node-to-node and edge-to-edge functions, jump connections similar
    to those in JK-Nets [[62](#bib.bib62)] are implicitly added. GNs [[9](#bib.bib9)]
    also proposed learning an edge representation and updating both node and edge
    representations using message-passing functions as shown in Eq. ([27](#S4.E27
    "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")) in Section [4.1.4](#S4.SS1.SSS4 "4.1.4
    Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep
    Learning on Graphs: A Survey"). In this aspect, the “weave module” is a special
    case of GNs that does not a representation of the entire graph.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{e}_{ij}^{l}$ 是第 $l^{th}$ 层中边 $(v_{i},v_{j})$ 的表示，$\mathcal{F}(\cdot)$
    是可学习的函数，其下标表示消息传递方向。通过堆叠多个此类模块，信息可以通过在节点和边表示之间交替传递来传播。请注意，在节点到节点和边到边的函数中，隐式地添加了类似于
    JK-Nets [[62](#bib.bib62)] 的跳跃连接。GNs [[9](#bib.bib9)] 还提出了学习边表示并使用消息传递函数更新节点和边表示，如第[27](#S4.E27
    "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")节中的公式所示。在这方面，“编织模块”是 GNs 的一个特殊情况，它没有整个图的表示。'
- en: 4.3.4 Sampling Methods
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 采样方法
- en: 'One critical bottleneck when training GCNs for large-scale graphs is efficiency.
    As shown in Section [4.1.4](#S4.SS1.SSS4 "4.1.4 Frameworks ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"), many GCNs
    follow a neighborhood aggregation scheme. However, because many real graphs follow
    a power-law distribution [[91](#bib.bib91)] (i.e., a few nodes have very large
    degrees), the number of neighbors can expand extremely quickly. To deal with this
    problem, two types of sampling methods have been proposed: neighborhood samplings
    and layer-wise samplings, as illustrated in Figure [7](#S4.F7 "Figure 7 ‣ 4.3.4
    Sampling Methods ‣ 4.3 Improvements and Discussions ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '在大规模图上训练 GCNs 的一个关键瓶颈是效率。如[4.1.4](#S4.SS1.SSS4 "4.1.4 Frameworks ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")节所示，许多
    GCNs 遵循邻域聚合方案。然而，由于许多真实图遵循幂律分布 [[91](#bib.bib91)]（即少数节点具有非常大的度数），邻居的数量可以极快地增加。为解决这个问题，提出了两种采样方法：邻域采样和层级采样，如图[7](#S4.F7
    "Figure 7 ‣ 4.3.4 Sampling Methods ‣ 4.3 Improvements and Discussions ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")所示。'
- en: In neighborhood samplings, the sampling is performed for each node during the
    calculations. GraphSAGE [[53](#bib.bib53)] uniformly sampled a fixed number of
    neighbors for each node during training. PinSage [[66](#bib.bib66)] proposed sampling
    neighbors using random walks on graphs along with several implementation improvements
    including coordination between the CPU and GPU, a map-reduce inference pipeline,
    and so on. PinSage was shown to be capable of handling a real billion-scale graph.
    StochasticGCN [[67](#bib.bib67)] further proposed reducing the sampling variances
    by using the historical activations of the last batches as a control variate,
    allowing for arbitrarily small sample sizes with a theoretical guarantee.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在邻域采样中，采样在计算过程中对每个节点进行。GraphSAGE [[53](#bib.bib53)] 在训练过程中均匀地为每个节点采样固定数量的邻居。PinSage
    [[66](#bib.bib66)] 提出了使用图上的随机游走进行邻居采样，并进行了若干实施改进，包括 CPU 和 GPU 之间的协调、一个 map-reduce
    推断管道等。研究表明，PinSage 能够处理实际的十亿规模图。StochasticGCN [[67](#bib.bib67)] 进一步提出通过使用上一个批次的历史激活作为控制变量来减少采样方差，从而在理论上保证可以使用任意小的样本大小。
- en: 'Instead of sampling neighbors of nodes, FastGCN [[68](#bib.bib68)] adopted
    a different strategy: it sampled nodes in each convolutional layer (i.e., a layer-wise
    sampling) by interpreting the nodes as i.i.d. samples and the graph convolutions
    as integral transforms under probability measures. FastGCN also showed that sampling
    nodes via their normalized degrees could reduce variances and lead to better performance.
    Adapt [[69](#bib.bib69)] further proposed sampling nodes in the lower layers conditioned
    on their top layer; this approach was more adaptive and applicable to explicitly
    reduce variances.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4e12bde8d6457c8599967b3e1df050da.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Different node sampling methods, in which the blue nodes indicate
    samples from one batch and the arrows indicate the sampling directions. The red
    nodes in (B) represent historical samples.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 Inductive Setting
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another important aspect of GCNs is that whether they can be applied to an inductive
    setting, i.e., training on a set of nodes or graphs and testing on another unseen
    set of nodes or graphs. In principle, this goal is achieved by learning a mapping
    function on the given features that are not dependent on the graph basis and can
    be transferred across nodes or graphs. The inductive setting was verified in GraphSAGE [[53](#bib.bib53)],
    GAT [[57](#bib.bib57)], GaAN [[58](#bib.bib58)], and FastGCN [[68](#bib.bib68)].
    However, the existing inductive GCNs are suitable only for graphs with explicit
    features. How to conduct inductive learnings for graphs without explicit features,
    usually called the out-of-sample problem [[92](#bib.bib92)], remains largely open
    in the literature.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.6 Theoretical Analysis
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To understand the effectiveness of GCNs, some theoretical analyses have been
    proposed that can be divided into three categories: node-focused tasks, graph-focused
    tasks, and general analysis.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'For node-focused tasks, Li et al. [[70](#bib.bib70)] first analyzed the performance
    of GCNs by using a special form of Laplacian smoothing, which makes the features
    of nodes in the same cluster similar. The original Laplacian smoothing operation
    is formulated as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}^{\prime}_{i}=(1-\gamma)\mathbf{h}_{i}+\gamma\sum\nolimits_{j\in\mathcal{N}(i)}\frac{1}{d_{i}}\mathbf{h}_{j},$
    |  | (41) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{h}_{i}$ and $\mathbf{h}^{\prime}_{i}$ are the original and smoothed
    features of node $v_{i}$, respectively. We can see that Eq. ([41](#S4.E41 "In
    4.3.6 Theoretical Analysis ‣ 4.3 Improvements and Discussions ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) is very similar to the graph convolution
    in Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")). Based
    on this insight, Li et al. also proposed a co-training and a self-training method
    for GCNs.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Wu et al. [[71](#bib.bib71)] analyzed GCNs from a signal processing
    perspective. By regarding node features as graph signals, they showed that Eq. ([13](#S4.E13
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) is basically a fixed low-pass
    filter. Using this insight, they proposed an extremely simplified graph convolution
    (SGC) architecture by removing all the nonlinearities and collapsing the learning
    parameters into one matrix:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{H}^{L}=\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\right)^{L}\mathbf{F}_{V}\mathbf{\Theta}.$
    |  | (42) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: The authors showed that such a “non-deep-learning” GCN variant achieved comparable
    performance to existing GCNs in many tasks. Maehara [[72](#bib.bib72)] enhanced
    this result by showing that the low-pass filtering operation did not equip GCNs
    with a nonlinear manifold learning ability, and further proposed GFNN model to
    remedy this problem by adding a MLP after the graph convolution layers.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: For graph-focused tasks, Kipf and Welling [[43](#bib.bib43)] and the authors
    of SortPooling [[49](#bib.bib49)] both considered the relationship between GCNs
    and graph kernels such as the Weisfeiler-Lehman (WL) kernel [[78](#bib.bib78)],
    which is widely used in graph isomorphism tests. They showed that GCNs are conceptually
    a generalization of the WL kernel because both methods iteratively aggregate information
    from node neighbors. Xu et al. [[73](#bib.bib73)] formalized this idea by proving
    that the WL kernel provides an upper bound for GCNs in terms of distinguishing
    graph structures. Based on this analysis, they proposed graph isomorphism network
    (GIN) and showed that a readout operation using summation and a MLP can achieve
    provably maximum discriminative power, i.e., the highest training accuracy in
    graph classification tasks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: For general analysis, Scarselli et al. [[93](#bib.bib93)] showed that the Vapnik-Chervonenkis
    dimension (VC-dim) of GCNs with different activation functions has the same scale
    as the existing RNNs. Chen et al. [[65](#bib.bib65)] analyzed the optimization
    landscape of linear GCNs and showed that any local minimum is relatively close
    to the global minimum under certain simplifications. Verma and Zhang [[94](#bib.bib94)]
    analyzed the algorithmic stability and generalization bound of GCNs. They showed
    that single-layer GCNs satisfy the strong notion of uniform stability if the largest
    absolute eigenvalue of the graph convolution filters is independent of the graph
    size.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 5 Graph Autoencoders
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The autoencoder (AE) and its variations have been widely applied in unsupervised
    learning tasks [[95](#bib.bib95)] and are suitable for learning node representations
    for graphs. The implicit assumption is that graphs have an inherent, potentially
    nonlinear low-rank structure. In this section, we first elaborate graph autoencoders
    and then introduce graph variational autoencoders and other improvements. The
    main characteristics of GAEs are summarized in Table [V](#S5.T5 "TABLE V ‣ 5 Graph
    Autoencoders ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: A Comparison among Different Graph Autoencoders (GAEs). T.C. = Time
    Complexity'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Type | Objective Function | T.C. | Node Features | Other Characteristics
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| SAE [[96](#bib.bib96)] | AE | L2-reconstruction | $O(M)$ | No | - |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| SDNE [[97](#bib.bib97)] | AE | L2-reconstruction + Laplacian eigenmaps |
    $O(M)$ | No | - |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| DNGR [[98](#bib.bib98)] | AE | L2-reconstruction | $O(N^{2})$ | No | - |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| GC-MC [[99](#bib.bib99)] | AE | L2-reconstruction | $O(M)$ | Yes | GCN encoder
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| DRNE [[100](#bib.bib100)] | AE | Recursive reconstruction | $O(Ns)$ | No
    | LSTM aggregator |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| G2G [[101](#bib.bib101)] | AE | KL + ranking | $O(M)$ | Yes | Nodes as distributions
    |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| VGAE [[102](#bib.bib102)] | VAE | Pairwise reconstruction | $O(N^{2})$ |
    Yes | GCN encoder |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| DVNE [[103](#bib.bib103)] | VAE | Wasserstein + ranking | $O(M)$ | No | Nodes
    as distributions |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| ARGA/ARVGA [[104](#bib.bib104)] | AE/VAE | L2-reconstruction + GAN | $O(N^{2})$
    | Yes | GCN encoder |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| NetRA [[105](#bib.bib105)] | AE | Recursive reconstruction + Laplacian eigenmaps
    + GAN | $O(M)$ | No | LSTM encoder |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: 5.1 Autoencoders
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The use of AEs for graphs originated from sparse autoencoder (SAE) [[96](#bib.bib96)].
    The basic idea is that, by regarding the adjacency matrix or its variations as
    the raw features of nodes, AEs can be leveraged as a dimensionality reduction
    technique to learn low-dimensional node representations. Specifically, SAE adopted
    the following L2-reconstruction loss:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\min_{\mathbf{\Theta}}\mathcal{L}_{2}=\sum\nolimits_{i=1}^{N}\left\&#124;\mathbf{P}\left(i,:\right)-\hat{\mathbf{P}}\left(i,:\right)\right\&#124;_{2}\\
    \hat{\mathbf{P}}\left(i,:\right)=\mathcal{G}\left(\mathbf{h}_{i}\right),\mathbf{h}_{i}=\mathcal{F}\left(\mathbf{P}\left(i,:\right)\right),\end{gathered}$
    |  | (43) |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{P}$ is the transition matrix, $\hat{\mathbf{P}}$ is the reconstructed
    matrix, $\mathbf{h}_{i}\in\mathbb{R}^{d}$ is the low-dimensional representation
    of node $v_{i}$, $\mathcal{F}(\cdot)$ is the encoder, $\mathcal{G}(\cdot)$ is
    the decoder, $d\ll N$ is the dimensionality, and $\mathbf{\Theta}$ are parameters.
    Both the encoder and decoder are an MLP with many hidden layers. In other words,
    a SAE compresses the information of $\mathbf{P}(i,:)$ into a low-dimensional vector
    $\mathbf{h}_{i}$ and then reconstructs the original feature from that vector.
    Another sparsity regularization term was also added. After obtaining the low-dimensional
    representation $\mathbf{h}_{i}$, k-means [[106](#bib.bib106)] was applied for
    the node clustering task. The experiments prove that SAEs outperform non-deep
    learning baselines. However, SAE was based on an incorrect theoretical analysis.⁸⁸8SAE [[96](#bib.bib96)]
    motivated the problem by analyzing the connection between spectral clustering
    and singular value decomposition, which is mathematically incorrect as pointed
    out in [[107](#bib.bib107)]. The mechanism underlying its effectiveness remained
    unexplained.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Structure deep network embedding (SDNE) [[97](#bib.bib97)] filled in the puzzle
    by showing that the L2-reconstruction loss in Eq. ([43](#S5.E43 "In 5.1 Autoencoders
    ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")) actually corresponds
    to the second-order proximity between nodes, i.e., two nodes share similar latten
    representations if they have similar neighborhoods, which is a well-studied concept
    in network science known as collaborative filtering or triangle closure [[5](#bib.bib5)].
    Motivated by network embedding methods showing that the first-order proximity
    is also important [[108](#bib.bib108)], SDNE modified the objective function by
    adding another Laplacian eigenmaps term [[75](#bib.bib75)]:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\mathbf{\Theta}}\mathcal{L}_{2}+\alpha\sum\nolimits_{i,j=1}^{N}\mathbf{A}(i,j)\left\&#124;\mathbf{h}_{i}-\mathbf{h}_{j}\right\&#124;_{2},$
    |  | (44) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: 'i.e., two nodes also share similar latent representations if they are directly
    connected. The authors also modified the L2-reconstruction loss by using the adjacency
    matrix and assigning different weights to zero and non-zero elements:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{2}=\sum\nolimits_{i=1}^{N}\left\&#124;\left(\mathbf{A}\left(i,:\right)-\mathcal{G}\left(\mathbf{h}_{i}\right)\right)\odot\mathbf{b}_{i}\right\&#124;_{2},$
    |  | (45) |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{h}_{i}=\mathcal{F}\left(\mathbf{A}\left(i,:\right)\right)$,
    $b_{ij}=1$ if $\mathbf{A}(i,j)=0$; otherwise $b_{ij}=\beta>1$, and $\beta$ is
    another hyper-parameter. The overall architecture of SDNE is shown in Figure [8](#S5.F8
    "Figure 8 ‣ 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs:
    A Survey").'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by another line of studies, a contemporary work DNGR [[98](#bib.bib98)]
    replaced the transition matrix $\mathbf{P}$ in Eq. ([43](#S5.E43 "In 5.1 Autoencoders
    ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")) with the positive
    pointwise mutual information (PPMI) [[79](#bib.bib79)] matrix defined in Eq. ([20](#S4.E20
    "In 4.1.3 The Aspect of Multiple Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")). In this way, the
    raw features can be associated with some random walk probability of the graph [[109](#bib.bib109)].
    However, constructing the input matrix has a time complexity of $O(N^{2})$, which
    is not scalable to large-scale graphs.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/602bff1359bba0787232cfa4f2cc13d2.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The framework of SDNE [[97](#bib.bib97)]. Both the first and second-order
    proximities of nodes are preserved using deep autoencoders.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'GC-MC [[99](#bib.bib99)] took a different approach by using the GCN proposed
    by Kipf and Welling [[43](#bib.bib43)] as the encoder:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{H}=GCN\left(\mathbf{F}^{V},\mathbf{A}\right),$ |  | (46) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: 'and using a simple bilinear function as the decoder:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{A}}(i,j)=\mathbf{H}(i,:)\mathbf{\Theta}_{de}\mathbf{H}(j,:)^{T},$
    |  | (47) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{\Theta}_{de}$ are the decoder parameters. Using this approach,
    node features were naturally incorporated. For graphs without node features, a
    one-hot encoding of node IDs was utilized. The authors demonstrated the effectiveness
    of GC-MC on the recommendation problem on bipartite graphs.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of reconstructing the adjacency matrix or its variations, DRNE [[100](#bib.bib100)]
    proposed another modification that directly reconstructed the low-dimensional
    node vectors by aggregating neighborhood information using an LSTM. Specifically,
    DRNE adopted the following objective function:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\sum\nolimits_{i=1}^{N}\left\&#124;\mathbf{h}_{i}-\text{LSTM}\left(\{\mathbf{h}_{j}&#124;j\in\mathcal{N}(i)\}\right)\right\&#124;.$
    |  | (48) |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: Because an LSTM requires its inputs to be a sequence, the authors suggested
    ordering the node neighborhoods based on their degrees. They also adopted a neighborhood
    sampling technique for nodes with large degrees to prevent an overlong memory.
    The authors proved that such a method can preserve regular equivalence as well
    as many centrality measures of nodes, such as PageRank [[110](#bib.bib110)].
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the above works that map nodes into a low-dimensional vector, Graph2Gauss
    (G2G) [[101](#bib.bib101)] proposed encoding each node as a Gaussian distribution
    $\mathbf{h}_{i}=\mathcal{N}\left(\mathbf{M}(i,:),diag\left(\mathbf{\Sigma}(i,:)\right)\right)$
    to capture the uncertainties of nodes. Specifically, the authors used a deep mapping
    from the node attributes to the means and variances of the Gaussian distribution
    as the encoder:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{M}(i,:)=\mathcal{F}_{\mathbf{M}}(\mathbf{F}^{V}(i,:)),\mathbf{\Sigma}(i,:)=\mathcal{F}_{\mathbf{\Sigma}}(\mathbf{F}^{V}(i,:)),$
    |  | (49) |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{F}_{\mathbf{M}}(\cdot)$ and $\mathcal{F}_{\mathbf{\Sigma}}(\cdot)$
    are the parametric functions that need to be learned. Then, instead of using an
    explicit decoder function, they used pairwise constraints to learn the model:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\text{KL}\left(\mathbf{h}_{j}&#124;&#124;\mathbf{h}_{i}\right)<\text{KL}\left(\mathbf{h}_{j^{\prime}}&#124;&#124;\mathbf{h}_{i}\right)\\
    \forall i,\forall j,\forall j^{\prime}\;s.t.\;d(i,j)<d(i,j^{\prime}),\end{gathered}$
    |  | (50) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: 'where $d(i,j)$ is the shortest distance from node $v_{i}$ to $v_{j}$ and $\text{KL}(q(\cdot)||p(\cdot))$
    is the Kullback-Leibler (KL) divergence between $q(\cdot)$ and $p(\cdot)$ [[111](#bib.bib111)].
    In other words, the constraints ensure that the KL-divergence between node representations
    has the same relative order as the graph distance. However, because Eq. ([50](#S5.E50
    "In 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey"))
    is hard to optimize, an energy-based loss [[112](#bib.bib112)] was adopted as
    a relaxation:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\sum\nolimits_{(i,j,j^{\prime})\in\mathcal{D}}\left(E_{ij}^{2}+\exp^{-E_{ij^{\prime}}}\right),$
    |  | (51) |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{D}=\left\{(i,j,j^{\prime})|d(i,j)<d(i,j^{\prime})\right\}$ and
    $E_{ij}=\text{KL}(\mathbf{h}_{j}||\mathbf{h}_{i})$. The authors further proposed
    an unbiased sampling strategy to accelerate the training process.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Variational Autoencoders
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Different from the aforementioned autoencoders, variational autoencoders (VAEs)
    are another type of deep learning method that combines dimensionality reduction
    with generative models. Its potential benefits include tolerating noise and learning
    smooth representations [[113](#bib.bib113)]. VAEs were first introduced to graph
    data in VGAE [[102](#bib.bib102)], where the decoder was a simple linear product:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p\left(\mathbf{A}&#124;\mathbf{H}\right)=\prod\nolimits_{i,j=1}^{N}\sigma\left(\mathbf{h}_{i}\mathbf{h}_{j}^{T}\right),$
    |  | (52) |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: 'in which the node representation was assumed to follow a Gaussian distribution
    $q\left(\mathbf{h}_{i}|\mathbf{M},\mathbf{\Sigma}\right)=\mathcal{N}\left(\mathbf{h}_{i}|\mathbf{M}(i,:),diag\left(\mathbf{\Sigma}(i,:)\right)\right)$.
    For the encoder of the mean and variance matrices, the authors also adopted the
    GCN proposed by Kipf and Welling [[43](#bib.bib43)]:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{M}=GCN_{\mathbf{M}}\left(\mathbf{F}^{V},\mathbf{A}\right),\log\mathbf{\Sigma}=GCN_{\mathbf{\Sigma}}\left(\mathbf{F}^{V},\mathbf{A}\right).$
    |  | (53) |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: 'Then, the model parameters were learned by minimizing the variational lower
    bound [[113](#bib.bib113)]:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\mathbb{E}_{q\left(\mathbf{H}&#124;\mathbf{F}^{V},\mathbf{A}\right)}\left[\log
    p\left(\mathbf{A}&#124;\mathbf{H}\right)\right]-\text{KL}\left(q\left(\mathbf{H}&#124;\mathbf{F}^{V},\mathbf{A}\right)&#124;&#124;p(\mathbf{H})\right).$
    |  | (54) |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: However, because this approach required reconstructing the full graph, its time
    complexity is $O(N^{2})$.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by SDNE and G2G, DVNE [[103](#bib.bib103)] proposed another VAE for
    graph data that also represented each node as a Gaussian distribution. Unlike
    the existing works that had adopted KL-divergence as the measurement, DVNE used
    the Wasserstein distance [[114](#bib.bib114)] to preserve the transitivity of
    the nodes similarities. Similar to SDNE and G2G, DVNE also preserved both the
    first and second-order proximity in its objective function:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\bf{\Theta}}\sum\nolimits_{(i,j,j^{\prime})\in\mathcal{D}}\left(E_{ij}^{2}+\exp^{-E_{ij^{\prime}}}\right)+\alpha\mathcal{L}_{2},$
    |  | (55) |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: 'where $E_{ij}=W_{2}\left(\mathbf{h}_{j}||\mathbf{h}_{i}\right)$ is the $2^{nd}$
    Wasserstein distance between two Gaussian distributions $\mathbf{h}_{j}$ and $\mathbf{h}_{i}$
    and $\mathcal{D}=\left\{(i,j,j^{\prime})|j\in\mathcal{N}(i),j^{\prime}\notin\mathcal{N}(i)\right\}$
    is a set of triples corresponding to the ranking loss of the first-order proximity.
    The reconstruction loss was defined as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{2}=\inf\nolimits_{q(\mathbf{Z}&#124;\mathbf{P})}\mathbb{E}_{p(\mathbf{P})}\mathbb{E}_{q(\mathbf{Z}&#124;\mathbf{P})}\left\&#124;\mathbf{P}\odot(\mathbf{P}-\mathcal{G}(\mathbf{Z}))\right\&#124;_{2}^{2},$
    |  | (56) |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{P}$ is the transition matrix and $\mathbf{Z}$ represents samples
    drawn from $\mathbf{H}$. The framework is shown in Figure [9](#S5.F9 "Figure 9
    ‣ 5.2 Variational Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs:
    A Survey"). Using this approach, the objective function can be minimized as in
    conventional VAEs using the reparameterization trick [[113](#bib.bib113)].'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/431f56c0c1fef5a24acfc611b0d1033c.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The framework of DVNE [[103](#bib.bib103)]. DVNE represents nodes
    as distributions using a VAE and adopts the Wasserstein distance to preserve the
    transitivity of the nodes similarities.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Improvements and Discussions
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several improvements have also been proposed for GAEs.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Adversarial Training
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An adversarial training scheme⁹⁹9We will discuss more adversarial methods for
    graphs in Section [7](#S7 "7 Graph Adversarial Methods ‣ Deep Learning on Graphs:
    A Survey"). was incorporated into GAEs as an additional regularization term in
    ARGA [[104](#bib.bib104)]. The overall architecture is shown in Figure [10](#S5.F10
    "Figure 10 ‣ 5.3.1 Adversarial Training ‣ 5.3 Improvements and Discussions ‣ 5
    Graph Autoencoders ‣ Deep Learning on Graphs: A Survey"). Specifically, the encoder
    of GAEs was used as the generator while the discriminator aimed to distinguish
    whether a latent representation came from the generator or from a prior distribution.
    In this way, the autoencoder was forced to match the prior distribution as a regularization.
    The objective function was:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\bf{\Theta}}\mathcal{L}_{2}+\alpha\mathcal{L}_{GAN},$ |  | (57)
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}_{2}$ is the reconstruction loss in GAEs and $\mathcal{L}_{GAN}$
    is
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\mathcal{G}}\max_{\mathcal{D}}\mathbb{E}_{\mathbf{h}\sim p_{\mathbf{h}}}\left[\log\mathcal{D}(\mathbf{h})\right]+\mathbb{E}_{\mathbf{z}\sim\mathcal{G}(\mathbf{F}^{V},\mathbf{A})}\left[\log\left(1-\mathcal{D}\left(\mathbf{z}\right)\right)\right],$
    |  | (58) |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{G}\left(\mathbf{F}^{V},\mathbf{A}\right)$ is a generator that
    uses the graph convolutional encoder from Eq. ([53](#S5.E53 "In 5.2 Variational
    Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")), $\mathcal{D}(\cdot)$
    is a discriminator based on the cross-entropy loss, and $p_{\mathbf{h}}$ is the
    prior distribution. The study adopted a simple Gaussian prior, and the experimental
    results demonstrated the effectiveness of the adversarial training scheme.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dbadf30d7fdac7eee97de8a214b9def1.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The framework of ARGA/ARVGA reprinted from [[104](#bib.bib104)]
    with permission. This model incorporates the adversarial training scheme into
    GAEs.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrently, NetRA [[105](#bib.bib105)] also proposed using a generative adversarial
    network (GAN) [[115](#bib.bib115)] to enhance the generalization ability of graph
    autoencoders. Specifically, the authors used the following objective function:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\bf{\Theta}}\mathcal{L}_{2}+\alpha_{1}\mathcal{L}_{LE}+\alpha_{2}\mathcal{L}_{GAN},$
    |  | (59) |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{L}_{LE}$ is the Laplacian eigenmaps objective function shown
    in Eq. ([44](#S5.E44 "In 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning
    on Graphs: A Survey")). In addition, the authors adopted an LSTM as the encoder
    to aggregate information from neighborhoods similar to Eq. ([48](#S5.E48 "In 5.1
    Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")). Instead
    of sampling only immediate neighbors and ordering the nodes using degrees as in
    DRNE [[100](#bib.bib100)], the authors used random walks to generate the input
    sequences. In contrast to ARGA, NetRA considered the representations in GAEs as
    the ground-truth and adopted random Gaussian noises followed by an MLP as the
    generator.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Inductive Learning
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to GCNs, GAEs can be applied to the inductive learning setting if node
    attributes are incorporated in the encoder. This can be achieved by using a GCN
    as the encoder, such as in GC-MC  [[99](#bib.bib99)], VGAE [[102](#bib.bib102)],
    and VGAE [[104](#bib.bib104)], or by directly learning a mapping function from
    node features as in G2G [[101](#bib.bib101)]. Because the edge information is
    utilized only when learning the parameters, the model can also be applied to nodes
    unseen during training. These works also show that although GCNs and GAEs are
    based on different architectures, it is possible to use them jointly, which we
    believe is a promising future direction.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Similarity Measures
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In GAEs, many similarity measures have been adopted, for example, L2-reconstruction
    loss, Laplacian eigenmaps, and the ranking loss for graph AEs, and KL divergence
    and Wasserstein distance for graph VAEs. Although these similarity measures are
    based on different motivations, how to choose an appropriate similarity measure
    for a given task and model architecture remains unstudied. More research is needed
    to understand the underlying differences between these metrics.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: The Main Characteristics of Graph Reinforcement Learning'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Task | Actions | Rewards | Time Complexity |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| GCPN [[116](#bib.bib116)] | Graph generation | Link prediction | GAN + domain
    knowledge | $O(MN)$ |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| MolGAN [[117](#bib.bib117)] | Graph generation | Generate the entire graph
    | GAN + domain knowledge | $O(N^{2})$ |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| GTPN [[118](#bib.bib118)] | Chemical reaction prediction | Predict node pairs
    and new bonding types | Prediction results | $O(N^{2})$ |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| GAM [[119](#bib.bib119)] | Graph classification | Predict graph labels and
    select the next node | Classification results | $O(d_{\text{avg}}sT)$ |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| DeepPath [[120](#bib.bib120)] | Knowledge graph reasoning | Predict the next
    node of the reasoning path | Reasoning results + diversity | $O(d_{\text{avg}}sT+s^{2}T)$
    |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| MINERVA [[121](#bib.bib121)] | Knowledge graph reasoning | Predict the next
    node of the reasoning path | Reasoning results | $O(d_{\text{avg}}sT)$ |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: 6 Graph Reinforcement Learning
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One aspect of deep learning not yet discussed is reinforcement learning (RL),
    which has been shown to be effective in AI tasks such as playing games [[122](#bib.bib122)].
    RL is known to be good at learning from feedbacks, especially when dealing with
    non-differentiable objectives and constraints. In this section, we review Graph
    RL methods. Their main characteristics are summarized in Table [VI](#S5.T6 "TABLE
    VI ‣ 5.3.3 Similarity Measures ‣ 5.3 Improvements and Discussions ‣ 5 Graph Autoencoders
    ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: GCPN [[116](#bib.bib116)] utilized RL to generate goal-directed molecular graphs
    while considering non-differential objectives and constraints. Specifically, the
    graph generation is modeled as a Markov decision process of adding nodes and edges,
    and the generative model is regarded as an RL agent operating in the graph generation
    environment. By treating agent actions as link predictions, using domain-specific
    as well as adversarial rewards, and using GCNs to learn the node representations,
    GCPN can be trained in an end-to-end manner using a policy gradient [[123](#bib.bib123)].
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: A concurrent work, MolGAN [[117](#bib.bib117)], adopted a similar idea of using
    RL for generating molecular graphs. However, rather than generating the graph
    through a sequence of actions, MolGAN proposed directly generating the full graph;
    this approach worked particularly well for small molecules.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: GTPN [[118](#bib.bib118)] adopted RL to predict chemical reaction products.
    Specifically, the agent acted to select node pairs in the molecule graph and predicted
    their new bonding types, and rewards were given both immediately and at the end
    based on whether the predictions were correct. GTPN used a GCN to learn the node
    representations and an RNN to memorize the prediction sequence.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'GAM [[119](#bib.bib119)] applied RL to graph classification by using random
    walks. The authors modeled the generation of random walks as a partially observable
    Markov decision process (POMDP). The agent performed two actions: first, it predicted
    the label of the graph; then, it selected the next node in the random walk. The
    reward was determined simply by whether the agent correctly classified the graph,
    i.e.,'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{J}(\theta)=\mathbb{E}_{P(S_{1:T};\theta)}\sum\nolimits_{t=1}^{T}r_{t},$
    |  | (60) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: where $r_{t}=1$ represents a correct prediction; otherwise, $r_{t}=-1$. $T$
    is the total time steps and $S_{t}$ is the environment.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: DeepPath [[120](#bib.bib120)] and MINERVA [[121](#bib.bib121)] both adopted
    RL for knowledge graph (KG) reasoning. Specifically, DeepPath targeted at pathfinding,
    i.e., find the most informative path between two target nodes, while MINERVA tackled
    question-answering tasks, i.e., find the correct answer node given a question
    node and a relation. In both methods, the RL agents need to predict the next node
    in the path at each step and output a reasoning path in the KG. Agents receive
    rewards if the paths reach the correct destinations. DeepPath also added a regularization
    term to encourage the path diversity.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: The Main Characteristics of Graph Adversarial Methods'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method |  Adversarial Methods | Time Complexity | Node Features
    |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| Adversarial Training | ARGA/ARVGA [[104](#bib.bib104)] | Regularization for
    GAEs | $O(N^{2})$ | Yes |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| NetRA [[105](#bib.bib105)] | Regularization for GAEs | $O(M)$ | No |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| GCPN [[116](#bib.bib116)] | Rewards for Graph RL | $O(MN)$ | Yes |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| MolGAN [[117](#bib.bib117)] | Rewards for Graph RL | $O(N^{2})$ | Yes |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| GraphGAN [[124](#bib.bib124)] | Generation of negative samples (i.e., node
    pairs) | $O(MN)$ | No |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| ANE [[125](#bib.bib125)] | Regularization for network embedding | $O(N)$
    | No |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| GraphSGAN [[126](#bib.bib126)] | Enhancing semi-supervised learning on graphs
    | $O(N^{2})$ | Yes |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| NetGAN [[127](#bib.bib127)] | Generation of graphs via random walks | $O(M)$
    | No |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| Adversarial Attack | Nettack [[128](#bib.bib128)] | Targeted attacks of graph
    structures and node attributes | $O(Nd_{0}^{2})$ | Yes |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. [[129](#bib.bib129)] | Targeted attacks of graph structures |
    $O(M)$ | No |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| Zugner and Gunnemann [[130](#bib.bib130)] | Non-targeted attacks of graph
    structures | $O(N^{2})$ | No |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: 7 Graph Adversarial Methods
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adversarial methods such as GANs [[115](#bib.bib115)] and adversarial attacks
    have drawn increasing attention in the machine learning community in recent years.
    In this section, we review how to apply adversarial methods to graphs. The main
    characteristics of graph adversarial methods are summarized in Table [VII](#S6.T7
    "TABLE VII ‣ 6 Graph Reinforcement Learning ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Adversarial Training
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The basic idea behind a GAN is to build two linked models: a discriminator
    and a generator. The goal of the generator is to “fool” the discriminator by generating
    fake data, while the discriminator aims to distinguish whether a sample comes
    from real data or is generated by the generator. Subsequently, both models benefit
    from each other by joint training using a minimax game. Adversarial training has
    been shown to be effective in generative models and enhancing the generalization
    ability of discriminative models. In Section [5.3.1](#S5.SS3.SSS1 "5.3.1 Adversarial
    Training ‣ 5.3 Improvements and Discussions ‣ 5 Graph Autoencoders ‣ Deep Learning
    on Graphs: A Survey") and Section [6](#S6 "6 Graph Reinforcement Learning ‣ Deep
    Learning on Graphs: A Survey"), we reviewed how adversarial training schemes are
    used in GAEs and Graph RL, respectively. Here, we review several other adversarial
    training methods on graphs in detail.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'GraphGAN [[124](#bib.bib124)] proposed using a GAN to enhance graph embedding
    methods [[17](#bib.bib17)] with the following objective function:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{\mathcal{G}}\max_{\mathcal{D}}\sum\nolimits_{i=1}^{N}$
    | $\displaystyle\left(\mathbb{E}_{v\sim p_{graph}(\cdot&#124;v_{i})}\left[\log\mathcal{D}(v,v_{i})\right]\right.$
    |  | (61) |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle+$ | $\displaystyle\left.\mathbb{E}_{v\sim\mathcal{G}(\cdot&#124;v_{i})}\left[\log\left(1-\mathcal{D}\left(v,v_{i}\right)\right)\right]\right).$
    |  |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: 'The discriminator $\mathcal{D}(\cdot)$ and the generator $\mathcal{G}(\cdot)$
    are as follows:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\mathcal{D}(v,v_{i})=\sigma(\mathbf{d}_{v}\mathbf{d}_{v_{i}}^{T}),\mathcal{G}(v&#124;v_{i})=\frac{\exp(\mathbf{g}_{v}\mathbf{g}_{v_{i}}^{T})}{\sum_{v^{\prime}\neq
    v_{i}}\exp(\mathbf{g}_{v^{\prime}}\mathbf{g}_{v_{i}}^{T})},\end{gathered}$ |  |
    (62) |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{d}_{v}$ and $\mathbf{g}_{v}$ are the low-dimensional embedding
    vectors for node $v$ in the discriminator and the generator, respectively. Combining
    the above equations, the discriminator actually has two objectives: the node pairs
    in the original graph should possess large similarities, while the node pairs
    generated by the generator should possess small similarities. This architecture
    is similar to network embedding methods such as LINE [[108](#bib.bib108)], except
    that negative node pairs are generated by the generator $\mathcal{G}(\cdot)$ instead
    of by random samplings. The authors showed that this method enhanced the inference
    abilities of the node embedding vectors.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial network embedding (ANE) [[125](#bib.bib125)] also adopted an adversarial
    training scheme to improve network embedding methods. Similar to ARGA [[104](#bib.bib104)],
    ANE used a GAN as an additional regularization term to existing network embedding
    methods such as DeepWalk [[131](#bib.bib131)] by imposing a prior distribution
    as the real data and regarding the embedding vectors as generated samples.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: GraphSGAN [[126](#bib.bib126)] used a GAN to enhance semi-supervised learning
    on graphs. Specifically, the authors observed that fake nodes should be generated
    in the density gaps between subgraphs to weaken the propagation effect across
    different clusters of the existing models. To achieve that goal, the authors designed
    a novel optimization objective with elaborate loss terms to ensure that the generator
    generated samples in the density gaps at equilibrium.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: NetGAN [[127](#bib.bib127)] adopted a GAN for graph generation tasks. Specifically,
    the authors regarded graph generation as a task to learn the distribution of biased
    random walks and adopted a GAN framework to generate and discriminate among random
    walks using an LSTM. The experiments showed that using random walks could also
    learn global network patterns.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Adversarial Attacks
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial attacks are another class of adversarial methods intended to deliberately
    “fool” the targeted methods by adding small perturbations to data. Studying adversarial
    attacks can deepen our understanding of the existing models and inspire more robust
    architectures. We review the graph-based adversarial attacks below.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'Nettack [[128](#bib.bib128)] first proposed attacking node classification models
    such as GCNs by modifying graph structures and node attributes. Denoting the targeted
    node as $v_{0}$ and its true class as $c_{true}$, the targeted model as $\mathcal{F}(\mathbf{A},\mathbf{F}^{V})$
    and its loss function as $\mathcal{L}_{\mathcal{F}}(\mathbf{A},\mathbf{F}^{V})$,
    the model adopted the following objective function:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\operatorname*{argmax}_{\left(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}\right)\in\mathcal{P}}\;\max_{c\neq
    c_{true}}\log\mathbf{Z}^{*}_{v_{0},c}-\log\mathbf{Z}^{*}_{v_{0},c_{true}}\\ s.t.\;\mathbf{Z}^{*}=\mathcal{F}_{\theta^{*}}(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}),\theta^{*}=\operatorname*{argmin}\nolimits_{\theta}\mathcal{L}_{\mathcal{F}}(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}),\end{gathered}$
    |  | (63) |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{A}^{\prime}$ and $\mathbf{F}^{V\prime}$ are the modified adjacency
    matrix and node feature matrix, respectively, $\mathbf{Z}$ represents the classification
    probabilities predicted by $\mathcal{F}(\cdot)$, and $\mathcal{P}$ is the space
    determined by the attack constraints. Simply speaking, the optimization aims to
    find the best legitimate changes in graph structures and node attributes to cause
    $v_{0}$ to be misclassified. The $\theta^{*}$ indicates that the attack is causative,
    i.e., the attack occurs before training the targeted model. The authors proposed
    several constraints for the attacks. The most important constraint is that the
    attack should be “unnoticeable”, i.e., it should make only small changes. Specifically,
    the authors proposed to preserve data characteristics such as node degree distributions
    and feature co-occurrences. The authors also proposed two attacking scenarios,
    direct attack (directly attacking $v_{0}$) and influence attack (only attacking
    other nodes), and several relaxations to make the optimization tractable.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrently, Dai et al. [[129](#bib.bib129)] studied adversarial attacks for
    graphs with an objective function similar to Eq. ([63](#S7.E63 "In 7.2 Adversarial
    Attacks ‣ 7 Graph Adversarial Methods ‣ Deep Learning on Graphs: A Survey"));
    however, they focused on the case in which only graph structures were changed.
    Instead of assuming that the attacker possessed all the information, the authors
    considered several settings in which different amounts of information were available.
    The most effective strategy, RL-S2V, adopted structure2vec [[132](#bib.bib132)]
    to learn the node and graph representations and used reinforcement learning to
    solve the optimization. The experimental results showed that the attacks were
    effective for both node and graph classification tasks.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned two attacks are targeted, i.e., they are intended to cause
    misclassification of some targeted node $v_{0}$. Zugner and Gunnemann [[130](#bib.bib130)]
    were the first to study non-targeted attacks, which were intended to reduce the
    overall model performance. They treated the graph structure as hyper-parameters
    to be optimized and adopted meta-gradients in the optimization process, along
    with several techniques to approximate the meta-gradients.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 8 Discussions and Conclusion
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thus far, we have reviewed the different graph-based deep learning architectures
    as well as their similarities and differences. Next, we briefly discuss their
    applications, implementations, and future directions before summarizing this paper.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Applications
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to standard graph inference tasks such as node or graph classification^(10)^(10)10A
    collection of methods for common tasks is listed in Appendix [B](#A2 "Appendix
    B Applicability for Common Tasks ‣ Deep Learning on Graphs: A Survey")., graph-based
    deep learning methods have also been applied to a wide range of disciplines, including
    modeling social influence [[133](#bib.bib133)], recommendation [[28](#bib.bib28),
    [66](#bib.bib66), [99](#bib.bib99), [134](#bib.bib134)], chemistry and biology [[55](#bib.bib55),
    [52](#bib.bib52), [46](#bib.bib46), [116](#bib.bib116), [117](#bib.bib117)], physics [[135](#bib.bib135),
    [136](#bib.bib136)], disease and drug prediction [[137](#bib.bib137), [138](#bib.bib138),
    [139](#bib.bib139)], gene expression [[140](#bib.bib140)], natural language processing
    (NLP) [[141](#bib.bib141), [142](#bib.bib142)], computer vision [[143](#bib.bib143),
    [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146), [147](#bib.bib147)],
    traffic forecasting [[148](#bib.bib148), [149](#bib.bib149)], program induction [[150](#bib.bib150)],
    solving graph-based NP problems [[151](#bib.bib151), [152](#bib.bib152)], and
    multi-agent AI systems [[153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155)].'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: A thorough review of these methods is beyond the scope of this paper due to
    the sheer diversity of these applications; however, we list several key inspirations.
    First, it is important to incorporate domain knowledge into the model when constructing
    a graph or choosing architectures. For example, building a graph based on the
    relative distance may be suitable for traffic forecasting problems, but may not
    work well for a weather prediction problem where the geographical location is
    also important. Second, a graph-based model can usually be built on top of other
    architectures rather than as a stand-alone model. For example, the computer vision
    community usually adopts CNNs for detecting objects and then uses graph-based
    deep learning as a reasoning module [[156](#bib.bib156)]. For NLP problems, GCNs
    can be adopted as syntactic constraints [[141](#bib.bib141)]. As a result, key
    key challenge is how to integrate different models. These applications also show
    that graph-based deep learning not only enables mining the rich value underlying
    the existing graph data but also helps to naturally model relational data as graphs,
    greatly widening the applicability of graph-based deep learning models.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Implementations
  id: totrans-426
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recently, several open libraries have been made available for developing deep
    learning models on graphs. These libraries are listed in Table [VIII](#S8.T8 "TABLE
    VIII ‣ 8.3 Future Directions ‣ 8 Discussions and Conclusion ‣ Deep Learning on
    Graphs: A Survey"). We also collected a list of source code (mostly from their
    original authors) for the studies discussed in this paper. This repository is
    included in Appendix [A](#A1 "Appendix A Source Codes ‣ Deep Learning on Graphs:
    A Survey"). These open implementations make it easy to learn, compare, and improve
    different methods. Some implementations also address the problem of distributed
    computing, which we do not discuss in this paper.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Future Directions
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several ongoing or future research directions which are also worthy
    of discussion:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New models for unstudied graph structures. Due to the extremely diverse structures
    of graph data, the existing methods are not suitable for all of them. For example,
    most methods focus on homogeneous graphs, while heterogeneous graphs are seldom
    studied, especially those containing different modalities such as those in [[157](#bib.bib157)].
    Signed networks, in which negative edges represent conflicts between nodes, also
    have unique structures, and they pose additional challenges to the existing methods [[158](#bib.bib158)].
    Hypergraphs, which represent complex relations between more than two objects [[159](#bib.bib159)],
    are also understudied. Thus, an important next step is to design specific deep
    learning models to handle these types of graphs.
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compositionality of existing models. As shown multiple times in this paper,
    many of the existing architectures can be integrated: for example, using a GCN
    as a layer in GAEs or Graph RL. In addition to designing new building blocks,
    how to systematically composite these architectures is an interesting future direction.
    In this process, how to incorporate interdisciplinary knowledge in a principled
    way rather than on a case-by-case basis is also an open problem. One recent work,
    graph networks [[9](#bib.bib9)], takes the first step and focuses on using a general
    framework of GNNs and GCNs for relational reasoning problems. AutoML may also
    be helpful by reducing the human burden of assembling different components and
    choosing hyper-parameters [[160](#bib.bib160)].'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic graphs. Most of the existing methods focus on static graphs. However,
    many real graphs are dynamic in nature: their nodes, edges, and features can change
    over time. For example, in social networks, people may establish new social relations,
    remove old relations, and their features, such as hobbies and occupations, can
    change over time. New users may join the network and existing users may leave.
    How to model the evolving characteristics of dynamic graphs and support incremental
    updates to model parameters remain largely unaddressed. Some preliminary works
    have obtained encouraging results by using Graph RNNs [[29](#bib.bib29), [27](#bib.bib27)].'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpretability and robustness. Because graphs are often related to other
    risk-sensitive scenarios, the ability to interpret the results of deep learning
    models on graphs is critical in decision-making problems. For example, in medicine
    or disease-related problems, interpretability is essential in transforming computer
    experiments into applications for clinical use. However, interpretability for
    graph-based deep learning is even more challenging than are other black-box models
    because graph nodes and edges are often heavily interconnected. In addition, because
    many existing deep learning models on graphs are sensitive to adversarial attacks
    as shown in Section [7.2](#S7.SS2 "7.2 Adversarial Attacks ‣ 7 Graph Adversarial
    Methods ‣ Deep Learning on Graphs: A Survey"), enhancing the robustness of the
    existing methods is another important issue. Some pioneering works regarding interpretability
    and robustness can be found in [[161](#bib.bib161)] and [[162](#bib.bib162), [163](#bib.bib163)],
    respectively.'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE VIII: Libraries of Deep Learning on Graphs'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | URL | Language/Framework | Key Characteristics |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| PyTorch Geometric [[164](#bib.bib164)] | https://github.com/rusty1s/pytorch_geometric
    | PyTorch |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '&#124; Improved efficiency, unified operations, &#124;'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; comprehensive existing methods &#124;'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '| Deep Graph Library [[165](#bib.bib165)] | https://github.com/dmlc/dgl | PyTorch
    | Improved efficiency, unified operations, scalability |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: '| AliGraph [[166](#bib.bib166)] | https://github.com/alibaba/aligraph | Unknown
    | Distributed environment, scalability, in-house algorithms |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '| Euler | https://github.com/alibaba/euler | C++/TensorFlow | Distributed environment,
    scalability |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: 8.4 Summary
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The above survey shows that deep learning on graphs is a promising and fast-developing
    research field that both offers exciting opportunities and presents many challenges.
    Studying deep learning on graphs constitutes a critical building block in modeling
    relational data, and it is an important step towards a future with better machine
    learning and artificial intelligence techniques.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors thank Jianfei Chen, Jie Chen, William L. Hamilton, Wenbing Huang,
    Thomas Kipf, Federico Monti, Shirui Pan, Petar Velickovic, Keyulu Xu, Rex Ying
    for allowing us to use their figures. This work was supported in part by National
    Program on Key Basic Research Project (No. 2015CB352300), National Key R&D Program
    of China under Grand 2018AAA0102004, National Natural Science Foundation of China
    (No. U1936219, No. U1611461, No. 61772304), and Beijing Academy of Artificial
    Intelligence (BAAI). All opinions, findings, conclusions, and recommendations
    in this paper are those of the authors and do not necessarily reflect the views
    of the funding agencies.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-452
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, 2015.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *et al.*, “Deep neural networks for acoustic
    modeling in speech recognition: The shared views of four research groups,” *IEEE
    Signal Processing Magazine*, 2012.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in Neural Information Processing
    Systems*, 2012.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” in *Proceedings of the 4th International Conference
    on Learning Representations*, 2015.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A.-L. Barabasi, *Network science*.   Cambridge university press, 2016.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,
    “The emerging field of signal processing on graphs: Extending high-dimensional
    data analysis to networks and other irregular domains,” *IEEE Signal Processing
    Magazine*, 2013.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, “Geometric
    deep learning: going beyond euclidean data,” *IEEE Signal Processing Magazine*,
    2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] C. Zang, P. Cui, and C. Faloutsos, “Beyond sigmoids: The nettide model
    for social network growth, and its applications,” in *Proceedings of the 22nd
    ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 2016.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi,
    M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre,
    F. Song, A. Ballard, J. Gilmer, G. Dahl, A. Vaswani, K. Allen, C. Nash, V. Langston,
    C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, and
    R. Pascanu, “Relational inductive biases, deep learning, and graph networks,”
    *arXiv preprint arXiv:1806.01261*, 2018.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, “Attention models
    in graphs: A survey,” *ACM Transactions on Knowledge Discovery from Data*, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Zhang, H. Tong, J. Xu, and R. Maciejewski, “Graph convolutional networks:
    Algorithms, applications and open challenges,” in *International Conference on
    Computational Social Networks*, 2019.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] L. Sun, J. Wang, P. S. Yu, and B. Li, “Adversarial attack and defense
    on graph data: A survey,” *arXiv preprint arXiv:1812.10528*, 2018.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, and M. Sun, “Graph neural
    networks: A review of methods and applications,” *arXiv preprint arXiv:1812.08434*,
    2018.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, “A comprehensive
    survey on graph neural networks,” *arXiv preprint arXiv:1901.00596*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin, “Graph embedding
    and extensions: A general framework for dimensionality reduction,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2007.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] W. L. Hamilton, R. Ying, and J. Leskovec, “Representation learning on
    graphs: Methods and applications,” *arXiv preprint arXiv:1709.05584*, 2017.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] P. Cui, X. Wang, J. Pei, and W. Zhu, “A survey on network embedding,”
    *IEEE Transactions on Knowledge and Data Engineering*, 2018.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations
    by back-propagating errors,” *Nature*, 1986.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
    in *Proceedings of the 3rd International Conference on Learning Representations*,
    2014.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *Journal
    of Machine Learning Research*, 2014.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving
    network embedding,” in *Proceedings of the 31st AAAI Conference on Artificial
    Intelligence*, 2017.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Leskovec and J. J. Mcauley, “Learning to discover social circles in
    ego networks,” in *NeurIPS*, 2012.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE Transactions on Neural Networks*, 2009.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, “Gated graph sequence
    neural networks,” in *Proceedings of the 5th International Conference on Learning
    Representations*, 2016.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] H. Dai, Z. Kozareva, B. Dai, A. Smola, and L. Song, “Learning steady-states
    of iterative algorithms over graphs,” in *International Conference on Machine
    Learning*, 2018.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. You, R. Ying, X. Ren, W. Hamilton, and J. Leskovec, “Graphrnn: Generating
    realistic graphs with deep auto-regressive models,” in *International Conference
    on Machine Learning*, 2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Ma, Z. Guo, Z. Ren, E. Zhao, J. Tang, and D. Yin, “Streaming graph
    neural networks,” *arXiv preprint arXiv:1810.10627*, 2018.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] F. Monti, M. Bronstein, and X. Bresson, “Geometric matrix completion with
    recurrent multi-graph neural networks,” in *Advances in Neural Information Processing
    Systems*, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] F. Manessi, A. Rozza, and M. Manzo, “Dynamic graph convolutional networks,”
    *arXiv preprint arXiv:1704.06199*, 2017.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder–decoder for
    statistical machine translation,” in *Proceedings of the 2014 Conference on Empirical
    Methods in Natural Language Processing*, 2014.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    1997.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] M. Gori, G. Monfardini, and F. Scarselli, “A new model for learning in
    graph domains,” in *IEEE International Joint Conference on Neural Networks Proceedings*,
    2005.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P. Frasconi, M. Gori, and A. Sperduti, “A general framework for adaptive
    processing of data structures,” *IEEE transactions on Neural Networks*, 1998.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] M. J. Powell, “An efficient method for finding the minimum of a function
    of several variables without calculating derivatives,” *The computer journal*,
    1964.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] L. B. Almeida, “A learning rule for asynchronous perceptrons with feedback
    in a combinatorial environment.” in *Proceedings, 1st First International Conference
    on Neural Networks*, 1987.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] F. J. Pineda, “Generalization of back-propagation to recurrent neural
    networks,” *Physical Review Letters*, 1987.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. A. Khamsi and W. A. Kirk, *An introduction to metric spaces and fixed
    point theory*.   John Wiley & Sons, 2011.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] M. Brockschmidt, Y. Chen, B. Cook, P. Kohli, and D. Tarlow, “Learning
    to decipher the heap for program verification,” in *Workshop on Constructive Machine
    Learning at the International Conference on Machine Learning*, 2015.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] I. M. Baytas, C. Xiao, X. Zhang, F. Wang, A. K. Jain, and J. Zhou, “Patient
    subtyping via time-aware lstm networks,” in *Proceedings of the 23rd ACM SIGKDD
    International Conference on Knowledge Discovery and Data Mining*, 2017.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Bruna, W. Zaremba, A. Szlam, and Y. Lecun, “Spectral networks and locally
    connected networks on graphs,” in *Proceedings of the 3rd International Conference
    on Learning Representations*, 2014.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] M. Henaff, J. Bruna, and Y. LeCun, “Deep convolutional networks on graph-structured
    data,” *arXiv preprint arXiv:1506.05163*, 2015.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural
    networks on graphs with fast localized spectral filtering,” in *Advances in Neural
    Information Processing Systems*, 2016.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *Proceedings of the 6th International Conference on
    Learning Representations*, 2017.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, “Cayleynets: Graph
    convolutional neural networks with complex rational spectral filters,” *IEEE Transactions
    on Signal Processing*, 2017.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] B. Xu, H. Shen, Q. Cao, Y. Qiu, and X. Cheng, “Graph wavelet neural network,”
    in *Proceedings of the 8th International Conference on Learning Representations*,
    2019.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
    A. Aspuru-Guzik, and R. P. Adams, “Convolutional networks on graphs for learning
    molecular fingerprints,” in *Advances in Neural Information Processing Systems*,
    2015.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Niepert, M. Ahmed, and K. Kutzkov, “Learning convolutional neural networks
    for graphs,” in *International Conference on Machine Learning*, 2016.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] H. Gao, Z. Wang, and S. Ji, “Large-scale learnable graph convolutional
    networks,” in *Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery &amp; Data Mining*, 2018.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, “An end-to-end deep learning
    architecture for graph classification,” in *Thirty-Second AAAI Conference on Artificial
    Intelligence*, 2018.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Atwood and D. Towsley, “Diffusion-convolutional neural networks,” in
    *Advances in Neural Information Processing Systems*, 2016.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C. Zhuang and Q. Ma, “Dual graph convolutional networks for graph-based
    semi-supervised classification,” in *Proceedings of the 2018 World Wide Web Conference*,
    2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
    “Neural message passing for quantum chemistry,” in *International Conference on
    Machine Learning*, 2017.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning
    on large graphs,” in *NeurIPS*, 2017.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein,
    “Geometric deep learning on graphs and manifolds using mixture model cnns,” in
    *CVPR*, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley, “Molecular
    graph convolutions: moving beyond fingerprints,” *Journal of Computer-Aided Molecular
    Design*, 2016.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] R. Ying, J. You, C. Morris, X. Ren, W. L. Hamilton, and J. Leskovec, “Hierarchical
    graph representation learning with differentiable pooling,” in *Advances in Neural
    Information Processing Systems*, 2018.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio,
    “Graph attention networks,” in *Proceedings of the 7th International Conference
    on Learning Representations*, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D.-Y. Yeung, “Gaan: Gated
    attention networks for learning on large and spatiotemporal graphs,” in *Proceedings
    of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence*, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. S. Yu, “Heterogeneous
    graph attention network,” in *The World Wide Web Conference*, 2019.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] T. Pham, T. Tran, D. Q. Phung, and S. Venkatesh, “Column networks for
    collective classification.” in *Proceedings of the 31st AAAI Conference on Artificial
    Intelligence*, 2017.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] J. Klicpera, A. Bojchevski, and S. Günnemann, “Predict then propagate:
    Graph neural networks meet personalized pagerank,” in *Proceedings of the 8th
    International Conference on Learning Representations*, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] K. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka,
    “Representation learning on graphs with jumping knowledge networks,” in *International
    Conference on Machine Learning*, 2018.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. Simonovsky and N. Komodakis, “Dynamic edgeconditioned filters in convolutional
    neural networks on graphs,” in *2017 IEEE Conference on Computer Vision and Pattern
    Recognition*, 2017.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. V. D. Berg, I. Titov, and M. Welling,
    “Modeling relational data with graph convolutional networks,” in *European Semantic
    Web Conference*, 2018.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Z. Chen, L. Li, and J. Bruna, “Supervised community detection with line
    graph neural networks,” in *Proceedings of the 8th International Conference on
    Learning Representations*, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec,
    “Graph convolutional neural networks for web-scale recommender systems,” in *Proceedings
    of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining*, 2018.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] J. Chen, J. Zhu, and L. Song, “Stochastic training of graph convolutional
    networks with variance reduction,” in *International Conference on Machine Learning*,
    2018.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. Chen, T. Ma, and C. Xiao, “Fastgcn: fast learning with graph convolutional
    networks via importance sampling,” in *Proceedings of the 7th International Conference
    on Learning Representations*, 2018.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] W. Huang, T. Zhang, Y. Rong, and J. Huang, “Adaptive sampling towards
    fast graph representation learning,” in *Advances in Neural Information Processing
    Systems*, 2018.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional
    networks for semi-supervised learning,” in *Proceedings of the Thirty-Second AAAI
    Conference on Artificial Intelligence*, 2018.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger, “Simplifying
    graph convolutional networks,” in *International Conference on Machine Learning*,
    2019.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] T. Maehara, “Revisiting graph neural networks: All we have is low-pass
    filters,” *arXiv preprint arXiv:1905.09550*, 2019.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural
    networks?” in *Proceedings of the 8th International Conference on Learning Representations*,
    2019.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] P. Veličković, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D.
    Hjelm, “Deep graph infomax,” in *Proceedings of the 8th International Conference
    on Learning Representations*, 2019.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques
    for embedding and clustering,” in *Advances in neural information processing systems*,
    2002.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] D. K. Hammond, P. Vandergheynst, and R. Gribonval, “Wavelets on graphs
    via spectral graph theory,” *Applied and Computational Harmonic Analysis*, 2011.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary-order
    proximity preserved network embedding,” in *KDD*, 2018.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] N. Shervashidze, P. Schweitzer, E. J. v. Leeuwen, K. Mehlhorn, and K. M.
    Borgwardt, “Weisfeiler-lehman graph kernels,” *Journal of Machine Learning Research*,
    2011.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] O. Levy and Y. Goldberg, “Neural word embedding as implicit matrix factorization,”
    in *Advances in Neural Information Processing Systems*, 2014.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] L. Babai, “Graph isomorphism in quasipolynomial time,” in *Proceedings
    of the forty-eighth annual ACM symposium on Theory of Computing*, 2016.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] G. Klir and B. Yuan, *Fuzzy sets and fuzzy logic*.   Prentice hall New
    Jersey, 1995.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] J. Ma, P. Cui, X. Wang, and W. Zhu, “Hierarchical taxonomy aware network
    embedding,” in *Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery & Data Mining*, 2018.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] D. Ruppert, “The elements of statistical learning: Data mining, inference,
    and prediction,” *Journal of the Royal Statistical Society*, 2010.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] U. Von Luxburg, “A tutorial on spectral clustering,” *Statistics and computing*,
    2007.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] I. S. Dhillon, Y. Guan, and B. Kulis, “Weighted graph cuts without eigenvectors
    a multilevel approach,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2007.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] D. I. Shuman, M. J. Faraji, and P. Vandergheynst, “A multiscale pyramid
    transform for graph signals,” *IEEE Transactions on Signal Processing*, 2016.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] B. D. Mckay and A. Piperno, *Practical graph isomorphism, II*.   Academic
    Press, Inc., 2014.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] O. Vinyals, S. Bengio, and M. Kudlur, “Order matters: Sequence to sequence
    for sets,” *Proceedings of the 5th International Conference on Learning Representations*,
    2016.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Advances in Neural
    Information Processing Systems*, 2017.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on Computer Vision and Pattern
    Recognition*, 2016.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] A.-L. Barabási and R. Albert, “Emergence of scaling in random networks,”
    *Science*, 1999.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Ma, P. Cui, and W. Zhu, “Depthlgp: Learning embeddings of out-of-sample
    nodes in dynamic networks,” in *Proceedings of the 32nd AAAI Conference on Artificial
    Intelligence*, 2018.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] F. Scarselli, A. C. Tsoi, and M. Hagenbuchner, “The vapnik–chervonenkis
    dimension of graph and recursive neural networks,” *Neural Networks*, 2018.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] S. Verma and Z.-L. Zhang, “Stability and generalization of graph convolutional
    neural networks,” in *KDD*, 2019.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] P. Vincent, H. Larochelle, Y. Bengio, and P. A. Manzagol, “Extracting
    and composing robust features with denoising autoencoders,” in *International
    Conference on Machine Learning*, 2008.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu, “Learning deep representations
    for graph clustering.” in *Proceedings of the Twenty-Eighth AAAI Conference on
    Artificial Intelligence*, 2014.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in *KDD*,
    2016.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations.”
    in *Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence*,
    2016.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] R. v. d. Berg, T. N. Kipf, and M. Welling, “Graph convolutional matrix
    completion,” *KDD’18 Deep Learning Day*, 2018.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] K. Tu, P. Cui, X. Wang, P. S. Yu, and W. Zhu, “Deep recursive network
    embedding with regular equivalence,” in *KDD*, 2018.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] A. Bojchevski and S. Günnemann, “Deep gaussian embedding of graphs: Unsupervised
    inductive learning via ranking,” in *Proceedings of the 7th International Conference
    on Learning Representations*, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] T. N. Kipf and M. Welling, “Variational graph auto-encoders,” *NIPS Workshop
    on Bayesian Deep Learning*, 2016.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] D. Zhu, P. Cui, D. Wang, and W. Zhu, “Deep variational network embedding
    in wasserstein space,” in *Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*, 2018.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, “Adversarially
    regularized graph autoencoder for graph embedding.” in *Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence*, 2018.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] W. Yu, C. Zheng, W. Cheng, C. C. Aggarwal, D. Song, B. Zong, H. Chen,
    and W. Wang, “Learning deep network representations with adversarially regularized
    autoencoders,” in *Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*, 2018.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. MacQueen *et al.*, “Some methods for classification and analysis of
    multivariate observations,” in *Proceedings of the fifth Berkeley symposium on
    mathematical statistics and probability*, 1967.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Z. Zhang, “A note on spectral clustering and svd of graph data,” *arXiv
    preprint arXiv:1809.11029*, 2018.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale
    information network embedding,” in *Proceedings of the 24th International Conference
    on World Wide Web*, 2015.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] L. Lovász *et al.*, “Random walks on graphs: A survey,” *Combinatorics*,
    1993.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank citation
    ranking: Bringing order to the web.” Stanford InfoLab, Tech. Rep., 1999.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] S. Kullback and R. A. Leibler, “On information and sufficiency,” *The
    annals of mathematical statistics*, 1951.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang, “A tutorial
    on energy-based learning,” *Predicting structured data*, 2006.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in *Proceedings
    of the 3rd International Conference on Learning Representations*, 2014.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] S. Vallender, “Calculation of the wasserstein distance between probability
    distributions on the line,” *Theory of Probability & Its Applications*, 1974.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems*, 2014.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec, “Graph convolutional
    policy network for goal-directed molecular graph generation,” in *Advances in
    Neural Information Processing Systems*, 2018.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] N. De Cao and T. Kipf, “MolGAN: An implicit generative model for small
    molecular graphs,” *ICML 2018 workshop on Theoretical Foundations and Applications
    of Deep Generative Models*, 2018.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] K. Do, T. Tran, and S. Venkatesh, “Graph transformation policy network
    for chemical reaction prediction,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] J. B. Lee, R. Rossi, and X. Kong, “Graph classification using structural
    attention,” in *Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery & Data Mining*, 2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] W. Xiong, T. Hoang, and W. Y. Wang, “Deeppath: A reinforcement learning
    method for knowledge graph reasoning,” in *Proceedings of the 2017 Conference
    on Empirical Methods in Natural Language Processing*, 2017.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] R. Das, S. Dhuliawala, M. Zaheer, L. Vilnis, I. Durugkar, A. Krishnamurthy,
    A. Smola, and A. McCallum, “Go for a walk and arrive at the answer: Reasoning
    over paths in knowledge bases using reinforcement learning,” in *Proceedings of
    the 7th International Conference on Learning Representations*, 2018.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,
    T. Hubert, L. Baker, M. Lai, A. Bolton *et al.*, “Mastering the game of go without
    human knowledge,” *Nature*, 2017.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, “Policy
    gradient methods for reinforcement learning with function approximation,” in *Advances
    in Neural Information Processing systems*, 2000.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, X. Xie, and M. Guo,
    “Graphgan: Graph representation learning with generative adversarial nets,” in
    *Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence*,
    2018.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Q. Dai, Q. Li, J. Tang, and D. Wang, “Adversarial network embedding,”
    in *Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence*,
    2018.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. Ding, J. Tang, and J. Zhang, “Semi-supervised learning on graphs with
    generative adversarial nets,” in *Proceedings of the 27th ACM International Conference
    on Information and Knowledge Management*, 2018.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] A. Bojchevski, O. Shchur, D. Zügner, and S. Günnemann, “Netgan: Generating
    graphs via random walks,” in *International Conference on Machine Learning*, 2018.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] D. Zügner, A. Akbarnejad, and S. Günnemann, “Adversarial attacks on neural
    networks for graph data,” in *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery &amp; Data Mining*, 2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu, and L. Song, “Adversarial
    attack on graph structured data,” in *Proceedings of the 35th International Conference
    on Machine Learning*, 2018.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] D. Zügner and S. Günnemann, “Adversarial attacks on graph neural networks
    via meta learning,” in *Proceedings of the 8th International Conference on Learning
    Representations*, 2019.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of
    social representations,” in *Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining*, 2014.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] H. Dai, B. Dai, and L. Song, “Discriminative embeddings of latent variable
    models for structured data,” in *International Conference on Machine Learning*,
    2016.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Qiu, J. Tang, H. Ma, Y. Dong, K. Wang, and J. Tang, “Deepinf: Modeling
    influence locality in large social networks,” in *Proceedings of the 24th ACM
    SIGKDD International Conference on Knowledge Discovery and Data Mining*, 2018.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] J. Ma, C. Zhou, P. Cui, H. Yang, and W. Zhu, “Learning disentangled representations
    for recommendation,” in *Advances in Neural Information Processing Systems*, 2019.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] C. W. Coley, R. Barzilay, W. H. Green, T. S. Jaakkola, and K. F. Jensen,
    “Convolutional embedding of attributed molecular graphs for physical property
    prediction,” *Journal of chemical information and modeling*, 2017.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] T. Xie and J. C. Grossman, “Crystal graph convolutional neural networks
    for an accurate and interpretable prediction of material properties,” *Physical
    Review Letters*, 2018.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] S. I. Ktena, S. Parisot, E. Ferrante, M. Rajchl, M. Lee, B. Glocker,
    and D. Rueckert, “Distance metric learning using graph convolutional networks:
    Application to functional brain networks,” in *International Conference on Medical
    Image Computing and Computer-Assisted Intervention*, 2017.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] M. Zitnik, M. Agrawal, and J. Leskovec, “Modeling polypharmacy side effects
    with graph convolutional networks,” *Bioinformatics*, 2018.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] S. Parisot, S. I. Ktena, E. Ferrante, M. Lee, R. G. Moreno, B. Glocker,
    and D. Rueckert, “Spectral graph convolutions for population-based disease prediction,”
    in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, 2017.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] F. Dutil, J. P. Cohen, M. Weiss, G. Derevyanko, and Y. Bengio, “Towards
    gene expression convolutions using gene interaction graphs,” in *International
    Conference on Machine Learning Workshop on Computational Biology*, 2018.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] J. Bastings, I. Titov, W. Aziz, D. Marcheggiani, and K. Simaan, “Graph
    convolutional encoders for syntax-aware neural machine translation,” in *Proceedings
    of the 2017 Conference on Empirical Methods in Natural Language Processing*, 2017.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] D. Marcheggiani and I. Titov, “Encoding sentences with graph convolutional
    networks for semantic role labeling,” in *EMNLP*, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] V. Garcia and J. Bruna, “Few-shot learning with graph neural networks,”
    in *Proceedings of the 7th International Conference on Learning Representations*,
    2018.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena, “Structural-rnn: Deep
    learning on spatio-temporal graphs,” in *Computer Vision and Pattern Recognition*,
    2016.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun, “3d graph neural networks
    for rgbd semantic segmentation,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] K. Marino, R. Salakhutdinov, and A. Gupta, “The more you know: Using
    knowledge graphs for image classification,” in *2017 IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] S. Qi, W. Wang, B. Jia, J. Shen, and S.-C. Zhu, “Learning human-object
    interactions by graph parsing neural networks,” in *Proceedings of the European
    Conference on Computer Vision*, 2018.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph convolutional networks:
    A deep learning framework for traffic forecasting,” in *Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence*, 2018.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent
    neural network: Data-driven traffic forecasting,” in *Proceedings of the 7th International
    Conference on Learning Representations*, 2018.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] M. Allamanis, M. Brockschmidt, and M. Khademi, “Learning to represent
    programs with graphs,” in *Proceedings of the 7th International Conference on
    Learning Representations*, 2018.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Z. Li, Q. Chen, and V. Koltun, “Combinatorial optimization with graph
    convolutional networks and guided tree search,” in *NeurIPS*, 2018.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] M. Prates, P. H. Avelar, H. Lemos, L. C. Lamb, and M. Y. Vardi, “Learning
    to solve np-complete problems: A graph neural network for decision tsp,” in *AAAI*,
    2019.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] S. Sukhbaatar, R. Fergus *et al.*, “Learning multiagent communication
    with backpropagation,” in *Advances in Neural Information Processing Systems*,
    2016.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] P. W. Battaglia, R. Pascanu, M. Lai, D. Rezende, and K. Kavukcuoglu,
    “Interaction networks for learning about objects, relations and physics,” in *Advances
    in Neural Information Processing Systems*, 2016.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Y. Hoshen, “Vain: Attentional multi-agent predictive modeling,” in *Advances
    in Neural Information Processing Systems*, 2017.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] A. Santoro, D. Raposo, D. G. T. Barrett, M. Malinowski, R. Pascanu, P. Battaglia,
    and T. Lillicrap, “A simple neural network module for relational reasoning,” in
    *NeuRIPS*, 2017.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang,
    “Heterogeneous network embedding via deep architectures,” in *Proceedings of the
    21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*,
    2015.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] T. Derr, Y. Ma, and J. Tang, “Signed graph convolutional network,” in
    *Data Mining, 2018 IEEE International Conference on*, 2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] K. Tu, P. Cui, X. Wang, F. Wang, and W. Zhu, “Structural deep embedding
    for hyper-networks,” in *AAAI*, 2018.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] K. Tu, J. Ma, P. Cui, J. Pei, and W. Zhu, “Autone: Hyperparameter optimization
    for massive network embedding,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] R. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec, “Gnn explainer:
    A tool for post-hoc explanation of graph neural networks,” in *Advances in Neural
    Information Processing Systems*, 2019.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] D. Zhu, Z. Zhang, P. Cui, and W. Zhu, “Robust graph convolutional networks
    against adversarial attacks,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] M. Jin, H. Chang, W. Zhu, and S. Sojoudi, “Power up! robust graph convolutional
    network against evasion attacks based on graph powering,” *arXiv preprint arXiv:1905.10029*,
    2019.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] M. Fey and J. E. Lenssen, “Fast graph representation learning with PyTorch
    Geometric,” in *ICLR Workshop on Representation Learning on Graphs and Manifolds*,
    2019.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] M. Wang, L. Yu, D. Zheng, Q. Gan, Y. Gai, Z. Ye, M. Li, J. Zhou, Q. Huang,
    C. Ma, Z. Huang, Q. Guo, H. Zhang, H. Lin, J. Zhao, J. Li, A. J. Smola, and Z. Zhang,
    “Deep graph library: Towards efficient and scalable deep learning on graphs,”
    *ICLR Workshop on Representation Learning on Graphs and Manifolds*, 2019.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] R. Zhu, K. Zhao, H. Yang, W. Lin, C. Zhou, B. Ai, Y. Li, and J. Zhou,
    “Aligraph: A comprehensive graph neural network platform,” in *Proceedings of
    the 45th International Conference on Very Large Data Bases*, 2019.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] O. Shchur, M. Mumme, A. Bojchevski, and S. Günnemann, “Pitfalls of graph
    neural network evaluation,” *Relational Representation Learning Workshop, NeurIPS
    2018*, 2018.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, “Learning deep
    generative models of graphs,” *arXiv preprint arXiv:1803.03324*, 2018.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad,
    “Collective classification in network data,” *AI magazine*, 2008.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] A. Ortega, P. Frossard, J. Kovačević, J. M. Moura, and P. Vandergheynst,
    “Graph signal processing: Overview, challenges, and applications,” *Proceedings
    of the IEEE*, 2018.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/d160855baae52eba5d18410a64452a06.png) | Ziwei
    Zhang received his B.S. from the Department of Physics, Tsinghua University in
    2016\. He is currently pursuing a Ph.D. Degree in the Department of Computer Science
    and Technology at Tsinghua University. His research interests focus on network
    embedding and machine learning on graph data, especially in developing scalable
    algorithms for large-scale networks. He has published several papers in prestigious
    conferences and journals, including KDD, AAAI, IJCAI, and TKDE. |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/3e973fc1fa996149438faaad3dfbd44a.png) | Peng Cui
    received the Ph.D. degree from Tsinghua University in 2010\. He is currently an
    associate professor with tenure at Tsinghua University. His research interests
    include network representation learning, human behavioral modeling, and social-sensed
    multimedia computing. He has published more than 100 papers in prestigious conferences
    and journals in data mining and multimedia. His recent research efforts have received
    the SIGKDD 2016 Best Paper Finalist, the ICDM 2015 Best Student Paper Award, the
    SIGKDD 2014 Best Paper Finalist, the IEEE ICME 2014 Best Paper Award, the ACM
    MM12 Grand Challenge Multimodal Award, and the MMM13 Best Paper Award. He is an
    associate editor of IEEE Transactions on Knowledge and Data Engineering, the IEEE
    Transactions on Big Data, the ACM Transactions on Multimedia Computing, Communications,
    and Applications, the Elsevier Journal on Neurocomputing, etc. He was the recipient
    of the ACM China Rising Star Award in 2015. |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/653d001f3ce84903606150601d1b90de.png) | Wenwu
    Zhu is currently a Professor and Deputy Head of the Computer Science Department
    of Tsinghua University and Vice Dean of National Research Center on Information
    Science and Technology. Prior to his current post, he was a Senior Researcher
    and Research Manager at Microsoft Research Asia. He was the Chief Scientist and
    Director at Intel Research China from 2004 to 2008\. He worked at Bell Labs New
    Jersey as a Member of Technical Staff during 1996-1999\. He received his Ph.D.
    degree from New York University in 1996. He served as the Editor-in-Chief for
    the IEEE Transactions on Multimedia (T-MM) from January 1, 2017, to December 31,
    2019\. He has been serving as Vice EiC for IEEE Transactions on Circuits and Systems
    for Video Technology (TCSVT) and the chair of the steering committee for IEEE
    T-MM since January 1, 2020\. His current research interests are in the areas of
    multimedia computing and networking, and big data. He has published over 400 papers
    in the referred journals and received nine Best Paper Awards including IEEE TCSVT
    in 2001 and 2019, and ACM Multimedia 2012\. He is an IEEE Fellow, AAAS Fellow,
    SPIE Fellow and a member of the European Academy of Sciences (Academia Europaea).
    |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
- en: 'TABLE IX: A collection of published source code. O.A. = Original Authors'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method | URL | O.A. | Language/Framework |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
- en: '| Graph RNNs | GGS-NNs [[24](#bib.bib24)] | https://github.com/yujiali/ggnn
    | Yes | Lua/Torch |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
- en: '| SSE [[25](#bib.bib25)] | https://github.com/Hanjun-Dai/steady_state_embedding
    | Yes | C |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
- en: '| You et al. [[26](#bib.bib26)] | https://github.com/JiaxuanYou/graph-generation
    | Yes | Python/PyTorch |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
- en: '| RMGCNN [[28](#bib.bib28)] | https://github.com/fmonti/mgcnn | Yes | Python/TensorFlow
    |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
- en: '| GCNs | ChebNet [[42](#bib.bib42)] | https://github.com/mdeff/cnn_graph |
    Yes | Python/TensorFlow |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
- en: '| Kipf&Welling [[43](#bib.bib43)] | https://github.com/tkipf/gcn | Yes | Python/TensorFlow
    |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
- en: '| CayletNet [[44](#bib.bib44)] | https://github.com/amoliu/CayleyNet | Yes
    | Python/TensorFlow |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
- en: '| GWNN [[45](#bib.bib45)] | https://github.com/Eilene/GWNN | Yes | Python/TensorFlow
    |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
- en: '| Neural FPs [[46](#bib.bib46)] | https://github.com/HIPS/neural-fingerprint
    | Yes | Python |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
- en: '| PATCHY-SAN [[47](#bib.bib47)] | https://github.com/seiya-kumada/patchy-san
    | No | Python |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
- en: '| LGCN [[48](#bib.bib48)] | https://github.com/divelab/lgcn/ | Yes | Python/TensorFlow
    |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
- en: '| SortPooling [[49](#bib.bib49)] | https://github.com/muhanzhang/DGCNN | Yes
    | Lua/Torch |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
- en: '| DCNN [[50](#bib.bib50)] | https://github.com/jcatw/dcnn | Yes | Python/Theano
    |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
- en: '| DGCN [[51](#bib.bib51)] | https://github.com/ZhuangCY/Coding-NN | Yes | Python/Theano
    |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
- en: '| MPNNs [[52](#bib.bib52)] | https://github.com/brain-research/mpnn | Yes |
    Python/TensorFlow |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
- en: '| GraphSAGE [[53](#bib.bib53)] | https://github.com/williamleif/GraphSAGE |
    Yes | Python/TensorFlow |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
- en: '| GNs [[9](#bib.bib9)] | https://github.com/deepmind/graph_nets | Yes | Python/TensorFlow
    |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
- en: '| DiffPool [[56](#bib.bib56)] | https://github.com/RexYing/graph-pooling |
    Yes | Python/PyTorch |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
- en: '| GAT [[57](#bib.bib57)] | https://github.com/PetarV-/GAT | Yes | Python/TensorFlow
    |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
- en: '| GaAN [[58](#bib.bib58)] | https://github.com/jennyzhang0215/GaAN | Yes |
    Python/MXNet |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
- en: '| HAN [[59](#bib.bib59)] | https://github.com/Jhy1993/HAN | Yes | Python/TensorFlow
    |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
- en: '| CLN [[60](#bib.bib60)] | https://github.com/trangptm/Column_networks | Yes
    | Python/Keras |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
- en: '| PPNP [[61](#bib.bib61)] | https://github.com/klicperajo/ppnp | Yes | Python/TensorFlow
    |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
- en: '| JK-Nets [[62](#bib.bib62)] | https://github.com/mori97/JKNet-dgl | No | Python/DGL
    |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
- en: '| ECC [[63](#bib.bib63)] | https://github.com/mys007/ecc | Yes | Python/PyTorch
    |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
- en: '| R-GCNs [[64](#bib.bib64)] | https://github.com/tkipf/relational-gcn | Yes
    | Python/Keras |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
- en: '| LGNN [[65](#bib.bib65)] | https://github.com/joanbruna/GNN_community | Yes
    | Lua/Torch |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
- en: '| StochasticGCN [[67](#bib.bib67)] | https://github.com/thu-ml/stochastic_gcn
    | Yes | Python/TensorFlow |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
- en: '| FastGCN [[68](#bib.bib68)] | https://github.com/matenure/FastGCN | Yes |
    Python/TensorFlow |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
- en: '| Adapt [[69](#bib.bib69)] | https://github.com/huangwb/AS-GCN | Yes | Python/TensorFlow
    |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[70](#bib.bib70)] | https://github.com/liqimai/gcn | Yes | Python/TensorFlow
    |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
- en: '| SGC [[71](#bib.bib71)] | https://github.com/Tiiiger/SGC | Yes | Python/PyTorch
    |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
- en: '| GFNN [[72](#bib.bib72)] | https://github.com/gear/gfnn | Yes | Python/PyTorch
    |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
- en: '| GIN [[73](#bib.bib73)] | https://github.com/weihua916/powerful-gnns | Yes
    | Python/PyTorch |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
- en: '| DGI [[74](#bib.bib74)] | https://github.com/PetarV-/DGI | Yes | Python/PyTorch
    |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
- en: '| GAEs | SAE [[96](#bib.bib96)] | https://github.com/quinngroup/deep-representations-clustering
    | No | Python/Keras |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
- en: '| SDNE [[97](#bib.bib97)] | https://github.com/suanrong/SDNE | Yes | Python/TensorFlow
    |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
- en: '| DNGR [[98](#bib.bib98)] | https://github.com/ShelsonCao/DNGR | Yes | Matlab
    |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
- en: '| GC-MC [[99](#bib.bib99)] | https://github.com/riannevdberg/gc-mc | Yes |
    Python/TensorFlow |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
- en: '| DRNE [[100](#bib.bib100)] | https://github.com/tadpole/DRNE | Yes | Python/TensorFlow
    |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
- en: '| G2G [[101](#bib.bib101)] | https://github.com/abojchevski/graph2gauss | Yes
    | Python/TensorFlow |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
- en: '| VGAE [[102](#bib.bib102)] | https://github.com/tkipf/gae | Yes | Python/TensorFlow
    |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
- en: '| DVNE [[103](#bib.bib103)] | http://nrl.thumedialab.com | Yes | Python/TensorFlow
    |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
- en: '| ARGA/ARVGA [[104](#bib.bib104)] | https://github.com/Ruiqi-Hu/ARGA | Yes
    | Python/TensorFlow |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
- en: '| NetRA [[105](#bib.bib105)] | https://github.com/chengw07/NetRA | Yes | Python/PyTorch
    |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
- en: '| Graph RLs | GCPN [[116](#bib.bib116)] | https://github.com/bowenliu16/rl_graph_generation
    | Yes | Python/TensorFlow |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
- en: '| MolGAN [[117](#bib.bib117)] | https://github.com/nicola-decao/MolGAN | Yes
    | Python/TensorFlow |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
- en: '| GAM [[119](#bib.bib119)] | https://github.com/benedekrozemberczki/GAM | Yes
    | Python/Pytorhc |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
- en: '| DeepPath [[120](#bib.bib120)] | https://github.com/xwhan/DeepPath | Yes |
    Python/TensorFlow |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
- en: '| MINERVA [[121](#bib.bib121)] | https://github.com/shehzaadzd/MINERVA | Yes
    | Python/TensorFlow |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
- en: '| Graph adversarial methods | GraphGAN [[124](#bib.bib124)] | https://github.com/hwwang55/GraphGAN
    | Yes | Python/TensorFlow |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
- en: '| GraphSGAN [[126](#bib.bib126)] | https://github.com/dm-thu/GraphSGAN | Yes
    | Python/PyTorch |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
- en: '| NetGAN [[127](#bib.bib127)] | https://github.com/danielzuegner/netgan | Yes
    | Python/TensorFlow |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
- en: '| Nettack [[128](#bib.bib128)] | https://github.com/danielzuegner/nettack |
    Yes | Python/TensorFlow |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. [[129](#bib.bib129)] | https://github.com/Hanjun-Dai/graph_adversarial_attack
    | Yes | Python/PyTorch |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
- en: '| Zugner&Gunnemann [[130](#bib.bib130)] | https://github.com/danielzuegner/gnn-meta-attack
    | Yes | Python/TensorFlow |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
- en: '| Applications | DeepInf [[133](#bib.bib133)] | https://github.com/xptree/DeepInf
    | Yes | Python/PyTorch |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
- en: '| Ma et al. [[134](#bib.bib134)] | https://jianxinma.github.io/assets/disentangle-recsys-v1.zip
    | Yes | Python/TensorFlow |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
- en: '| CGCNN [[136](#bib.bib136)] | https://github.com/txie-93/cgcnn | Yes | Python/PyTorch
    |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
- en: '| Ktena et al. [[137](#bib.bib137)] | https://github.com/sk1712/gcn_metric_learning
    | Yes | Python |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
- en: '| Decagon [[138](#bib.bib138)] | https://github.com/mims-harvard/decagon |
    Yes | Python/PyTorch |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
- en: '| Parisot et al. [[139](#bib.bib139)] | https://github.com/parisots/population-gcn
    | Yes | Python/TensorFlow |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
- en: '| Dutil et al. [[140](#bib.bib140)] | https://github.com/mila-iqia/gene-graph-conv
    | Yes | Python/PyTorch |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
- en: '| Bastings et al. [[141](#bib.bib141)] | https://github.com/bastings/neuralmonkey/tree/emnlp_gcn
    | Yes | Python/TensorFlow |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
- en: '| Neural-dep-srl [[142](#bib.bib142)] | https://github.com/diegma/neural-dep-srl
    | Yes | Python/Therano |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
- en: '| Garcia&Bruna [[143](#bib.bib143)] | https://github.com/vgsatorras/few-shot-gnn
    | Yes | Python/PyTorch |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
- en: '| S-RNN [[144](#bib.bib144)] | https://github.com/asheshjain399/RNNexp | Yes
    | Python/Therano |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
- en: '| 3DGNN [[145](#bib.bib145)] | https://github.com/xjqicuhk/3DGNN | Yes | Matlab/Caffe
    |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
- en: '| GPNN [[147](#bib.bib147)] | https://github.com/SiyuanQi/gpnn | Yes | Python/PyTorch
    |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
- en: '| STGCN [[148](#bib.bib148)] | https://github.com/VeritasYin/STGCN_IJCAI-18
    | Yes | Python/TensorFlow |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
- en: '| DCRNN [[149](#bib.bib149)] | https://github.com/liyaguang/DCRNN | Yes | Python/TensorFlow
    |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
- en: '| Allamanis et al. [[150](#bib.bib150)] | https://github.com/microsoft/tf-gnn-samples
    | Yes | Python/TensorFlow |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[151](#bib.bib151)] | https://github.com/intel-isl/NPHard | Yes
    | Python/TensorFlow |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
- en: '| TSPGNN [[152](#bib.bib152)] | https://github.com/machine-reasoning-ufrgs/TSP-GNN
    | Yes | Python/TensorFlow |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
- en: '| CommNet [[153](#bib.bib153)] | https://github.com/facebookresearch/CommNet
    | Yes | Lua/Torch |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
- en: '| Interaction network [[154](#bib.bib154)] | https://github.com/jaesik817/Interaction-networks_tensorflow
    | No | Python/TensorFlow |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
- en: '| Relation networks [[156](#bib.bib156)] | https://github.com/kimhc6028/relational-networks
    | No | Python/PyTorch |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
- en: '| Miscellaneous | SGCN [[158](#bib.bib158)] | http://www.cse.msu.edu/~derrtyle/
    | Yes | Python/PyTorch |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
- en: '| DHNE [[159](#bib.bib159)] | https://github.com/tadpole/DHNE | Yes | Python/TensorFlow
    |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
- en: '| AutoNE [[160](#bib.bib160)] | https://github.com/tadpole/AutoNE | Yes | Python
    |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
- en: '| Gnn-explainer [[161](#bib.bib161)] | https://github.com/RexYing/gnn-model-explainer
    | Yes | Python/PyTorch |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
- en: '| RGCN [[162](#bib.bib162)] | https://github.com/thumanlab/nrlweb | Yes | Python/TensorFlow
    |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
- en: '| GNN-benchmark [[167](#bib.bib167)] | https://github.com/shchur/gnn-benchmark
    | Yes | Python/TensorFlow |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
- en: 'TABLE X: A Table of Methods for Six Common Tasks'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Task | Methods |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
- en: '| Node-focused Tasks | Node Clustering | [[96](#bib.bib96), [59](#bib.bib59),
    [157](#bib.bib157), [97](#bib.bib97), [124](#bib.bib124), [104](#bib.bib104),
    [44](#bib.bib44), [98](#bib.bib98)] |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
- en: '| Node Classification | Transductive |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
- en: '&#124; [[41](#bib.bib41), [23](#bib.bib23), [25](#bib.bib25), [27](#bib.bib27),
    [42](#bib.bib42), [48](#bib.bib48), [54](#bib.bib54), [45](#bib.bib45), [51](#bib.bib51),
    [43](#bib.bib43), [50](#bib.bib50), [53](#bib.bib53), [44](#bib.bib44), [29](#bib.bib29)]
    &#124;'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[64](#bib.bib64), [61](#bib.bib61), [65](#bib.bib65), [57](#bib.bib57),
    [59](#bib.bib59), [60](#bib.bib60), [68](#bib.bib68), [67](#bib.bib67), [58](#bib.bib58),
    [70](#bib.bib70), [62](#bib.bib62), [69](#bib.bib69), [71](#bib.bib71), [72](#bib.bib72)]
    &#124;'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[103](#bib.bib103), [101](#bib.bib101), [74](#bib.bib74), [100](#bib.bib100),
    [126](#bib.bib126), [124](#bib.bib124), [125](#bib.bib125), [162](#bib.bib162),
    [157](#bib.bib157), [97](#bib.bib97), [105](#bib.bib105)] &#124;'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: '| Inductive | [[74](#bib.bib74), [48](#bib.bib48), [57](#bib.bib57), [53](#bib.bib53),
    [67](#bib.bib67), [58](#bib.bib58), [62](#bib.bib62), [25](#bib.bib25), [71](#bib.bib71),
    [101](#bib.bib101), [72](#bib.bib72), [68](#bib.bib68), [69](#bib.bib69)] |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
- en: '| Network Reconstruction | [[157](#bib.bib157), [97](#bib.bib97), [103](#bib.bib103),
    [105](#bib.bib105)] |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
- en: '| Link Prediction | [[102](#bib.bib102), [99](#bib.bib99), [97](#bib.bib97),
    [101](#bib.bib101), [124](#bib.bib124), [103](#bib.bib103), [66](#bib.bib66),
    [28](#bib.bib28), [64](#bib.bib64), [104](#bib.bib104), [44](#bib.bib44), [27](#bib.bib27),
    [105](#bib.bib105)] |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
- en: '| Graph-focused Tasks | Graph Classification | [[40](#bib.bib40), [41](#bib.bib41),
    [29](#bib.bib29), [44](#bib.bib44), [49](#bib.bib49), [56](#bib.bib56), [55](#bib.bib55),
    [119](#bib.bib119), [132](#bib.bib132), [47](#bib.bib47), [50](#bib.bib50), [73](#bib.bib73),
    [71](#bib.bib71), [63](#bib.bib63), [42](#bib.bib42), [23](#bib.bib23), [54](#bib.bib54)]
    |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
- en: '| Graph Generation | Structure-only | [[127](#bib.bib127), [26](#bib.bib26)]
    |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
- en: '| Structure+features | [[117](#bib.bib117), [116](#bib.bib116), [168](#bib.bib168)]
    |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
- en: Appendix A Source Codes
  id: totrans-726
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [IX](#A0.T9 "TABLE IX ‣ Deep Learning on Graphs: A Survey") shows a collection
    and summary of the source code we collected for the papers discussed in this manuscript.
    In addition to method names and links, the table also lists the programming language
    used and the frameworks adopted as well as whether the code was published by the
    original authors of the paper.'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Applicability for Common Tasks
  id: totrans-728
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [X](#A0.T10 "TABLE X ‣ Deep Learning on Graphs: A Survey") summarizes
    the applicability of different models for six common graph tasks, including node
    clustering, node classification, network reconstruction, link prediction, graph
    classification, and graph generation. Note that these results are based on whether
    the experiments were reported in the original papers.'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Node Classification Results on Benchmark Datasets
  id: totrans-730
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As shown in Appendix [B](#A2 "Appendix B Applicability for Common Tasks ‣ Deep
    Learning on Graphs: A Survey"), node classification is the most common task for
    graph-based deep learning models. Here, we report the results of different methods
    on five node classification benchmark datasets^(11)^(11)11These five benchmark
    datasets are publicly available at https://github.com/tkipf/gcn or http://snap.stanford.edu/graphsage/.:'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cora, Citeseer, PubMed [[169](#bib.bib169)]: These are citation graphs with
    nodes representing papers, edges representing citations between papers, and papers
    associated with bag-of-words features and ground-truth topics as labels.'
  id: totrans-733
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reddit [[53](#bib.bib53)]: Reddit is an online discussion forum in which nodes
    represent posts and two nodes are connected when they are commented by the same
    user, and each post contains a low-dimensional word vector as features and a label
    indicating the Reddit community in which it was posted.'
  id: totrans-735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PPI [[53](#bib.bib53)]: PPI is a collection of protein-protein interaction
    graphs for different human tissues. It includes features that represent biological
    signatures and labels that represent the roles of proteins.'
  id: totrans-737
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Cora, Citeseer, and Pubmed each include one graph, and the same graph structure
    is used for both training and testing, thus the tasks are considered transductive.
    In Reddit and PPI, because the training and testing graphs are different, these
    two datasets are considered to be inductive node classification benchmarks.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [XI](#A3.T11 "TABLE XI ‣ Appendix C Node Classification Results on
    Benchmark Datasets ‣ Deep Learning on Graphs: A Survey"), we report the results
    of different models on these benchmark datasets. The results were extracted from
    their original papers when a fixed dataset split was adopted. The table shows
    that many state-of-the-art methods achieve roughly comparable performance on these
    benchmarks, with differences smaller than one percent. Shchur et al. [[167](#bib.bib167)]
    also found that a fixed dataset split can easily result in spurious comparisons.
    As a result, although these benchmarks have been widely adopted to compare different
    models, more comprehensive evaluation setups are critically needed.'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XI: Statistics of the benchmark datasets and the node classification
    results of different methods when a fixed dataset split is adopted. A hyphen (’-’)
    indicates that the result is unavailable in the paper.'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Cora | Citeseer | Pubmed | Reddit | PPI |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
- en: '| Type | Citation | Citation | Citation | Social | Biology |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
- en: '| Nodes | 2,708 | 3,327 | 19,717 | 232,965 | 56,944 (24 graphs) |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
- en: '| Edges | 5,429 | 4,732 | 44,338 | 11,606,919 | 818,716 |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
- en: '| Classes | 7 | 6 | 3 | 41 | 121 |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
- en: '| Features | 1,433 | 3,703 | 500 | 602 | 50 |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
- en: '| Task | Transductive | Transductive | Transductive | Inductive | Inductive
    |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
- en: '| Bruna et al. [[40](#bib.bib40)]^(12)^(12)footnotemark: 12 | 73.3 | 58.9 |
    73.9 | - | - |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
- en: '| ChebNet [[42](#bib.bib42)]^(13)^(13)13The results were reported in GWNN [[45](#bib.bib45)].
    | 81.2 | 69.8 | 74.4 | - | - |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
- en: '| GCN [[43](#bib.bib43)] | 81.5 | 70.3 | 79.0 | - | - |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
- en: '| CayleyNets [[44](#bib.bib44)] | 81.9$\pm$0.7 | - | - | - | - |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
- en: '| GWNN [[45](#bib.bib45)] | 82.8 | 71.7 | 79.1 | - | - |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
- en: '| LGCN [[48](#bib.bib48)] | 83.3$\pm$0.5 | 73.0$\pm$0.6 | 79.5$\pm$0.2 | -
    | 77.2$\pm$0.2 |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
- en: '| DGCN [[51](#bib.bib51)] | 83.5 | 72.6 | 80.0 | - | - |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
- en: '| GraphSAGE [[53](#bib.bib53)] | - | - | - | 95.4 | 61.2 |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
- en: '| MoNet [[54](#bib.bib54)] | 81.7$\pm$0.5 | - | 78.8$\pm$0.4 | - | - |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
- en: '| GAT [[57](#bib.bib57)] | 83.0$\pm$0.7 | 72.5$\pm$0.7 | 79.0$\pm$0.3 | - |
    97.3$\pm$0.2 |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
- en: '| GaAN [[58](#bib.bib58)] | - | - | - | 96.4$\pm$0.0 | 98.7$\pm$0.0 |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
- en: '| JK-Nets [[62](#bib.bib62)] | - | - | - | 96.5 | 97.6$\pm$0.7 |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
- en: '| StochasticGCN [[67](#bib.bib67)] | 82.0$\pm$0.8 | 70.9$\pm$0.2 | 79.0$\pm$0.4
    | 96.3$\pm$0.0 | 97.9$\pm$0.0 |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
- en: '| FastGCN [[68](#bib.bib68)] | 72.3 | - | 72.1 | 93.7 | - |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
- en: '| Adapt [[69](#bib.bib69)] | - | - | - | 96.3$\pm$0.3 | - |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
- en: '| SGC [[71](#bib.bib71)] | 81.0$\pm$0.0 | 71.9$\pm$0.1 | 78.9$\pm$0.0 | 94.9
    | - |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
- en: '| DGI [[74](#bib.bib74)] | 82.3$\pm$0.6 | 71.8$\pm$0.7 | 76.8$\pm$0.6 | 94.0$\pm$0.1
    | 63.8$\pm$0.2 |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
- en: '| SSE [[25](#bib.bib25)] | - | - | - | - | 83.6 |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
- en: '| GraphSGAN [[126](#bib.bib126)] | 83.0$\pm$1.3 | 73.1$\pm$1.8 | - | - | -
    |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
- en: '| RGCN [[162](#bib.bib162)] | 82.8$\pm$0.6 | 71.2$\pm$0.5 | 79.1$\pm$0.3 |
    - | - |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/00113f5c975bd00b305a3b9a4a5d1dbf.png)'
  id: totrans-768
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: An example of graph signals and its spectral representations transformed
    using the eigenvectors of the graph Laplacian matrix. The upward-pointing red
    lines represent positive values and the downward-pointing green lines represent
    negative values. These images were adapted from [[6](#bib.bib6)].'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D An Example of Graph Signals
  id: totrans-770
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To help understanding GCNs, we provide an example of graph signals and refer
    readers to [[6](#bib.bib6), [7](#bib.bib7), [170](#bib.bib170)] for more comprehensive
    surveys.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a graph $G=(V,E)$, a graph signal $\mathbf{f}$ corresponds to a collection
    of numbers: one number for each node in the graph. For undirected graphs, we usually
    assume that the signal takes real values, i.e., $\mathbf{f}\in\mathbb{R}^{N}$,
    where $N$ is the number of nodes. Any node feature satisfying the above requirement
    can be regarded as a graph signal, with an example shown in Fig [11](#A3.F11 "Figure
    11 ‣ Appendix C Node Classification Results on Benchmark Datasets ‣ Deep Learning
    on Graphs: A Survey") (A). Both the signal values and the underlying graph structure
    are important in processing and analyzing graph signals. For example, we can transform
    a graph signal into the spectral domain using the eigenvectors of the graph Laplacian
    matrix:'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{f}}=\mathbf{Q}^{T}\mathbf{f}$ |  | (64) |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
- en: or equivalently
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{f}}_{i}=\mathbf{Q}^{T}(i,:)\mathbf{f}.$ |  | (65) |'
  id: totrans-775
  prefs: []
  type: TYPE_TB
- en: 'Because the eigenvectors $\mathbf{Q}^{T}$ are sorted in ascending order based
    on their corresponding eigenvalues, it has been shown [[6](#bib.bib6)] that they
    form a basis for graph signals based on different “smoothness”. Specifically,
    eigenvectors corresponding to small eigenvalues represent smooth signals and low
    frequencies, while eigenvectors corresponding to large eigenvalues represent non-smooth
    signals and high frequencies, as shown in Fig [11](#A3.F11 "Figure 11 ‣ Appendix
    C Node Classification Results on Benchmark Datasets ‣ Deep Learning on Graphs:
    A Survey") (B). Note that the smoothness is measured with respect to the graph
    structure, i.e., whether the signals oscillate across edges in the graph. As a
    result, $\hat{\mathbf{f}}$ provides a spectral representation of the signal $\mathbf{f}$
    as shown in Fig [11](#A3.F11 "Figure 11 ‣ Appendix C Node Classification Results
    on Benchmark Datasets ‣ Deep Learning on Graphs: A Survey") (C). This is similar
    to the Fourier transform in Euclidean spaces. Using $\hat{\mathbf{f}}$, we can
    design various signal processing operations. For example, if we apply a low-pass
    filter, the resulted signal will be more smooth, as shown in Fig [11](#A3.F11
    "Figure 11 ‣ Appendix C Node Classification Results on Benchmark Datasets ‣ Deep
    Learning on Graphs: A Survey") (D) (in this example, we set the frequency threshold
    as 2, i.e., only keeping the lowest 4 frequencies).'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Time Complexity
  id: totrans-777
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '^(13)^(13)footnotetext: The results were reported in Kipf and Welling [[102](#bib.bib102)].'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explain how we obtained the time complexity in all the tables.
    Specifically, we mainly focus on the time complexity with respect to the graph
    size, e.g., the number of nodes $N$ and the number of edges $M$, and omit other
    factors, e.g., the number of hidden dimensions $f_{l}$ or the number of iterations,
    since the latter terms are usually set as small constants and are less dominant.
    Note that we focus on the theoretical results, while the exact efficiency of one
    algorithm also depends heavily on its implementations and techniques to reduce
    the constants in the time complexity.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GNN [[23](#bib.bib23)]: $O(MI_{f})$, where $I_{f}$ is the number of iterations
    for Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks
    ‣ Deep Learning on Graphs: A Survey")) to reach stable points, as shown in the
    paper.'
  id: totrans-781
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GGS-NNs [[24](#bib.bib24)]: $O(MT)$, where $T$ is a preset maximum pseudo time
    since the method utilizes all the edges in each updating.'
  id: totrans-783
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SSE [[25](#bib.bib25)]: $O(d_{\text{avg}}S)$, where $d_{\text{avg}}$ is the
    average degree and $S$ is the total number of samples, as shown in the paper.'
  id: totrans-785
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. [[26](#bib.bib26)]: $O(N^{2})$, as shown in the paper.'
  id: totrans-787
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DGNN [[27](#bib.bib27)]: $O(Md_{\text{avg}})$, where $d_{\text{avg}}$ is the
    average degree since the effect of the one-step propagation of each edge is considered.'
  id: totrans-789
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RMGCNN [[28](#bib.bib28)]: $O(MN)$ or $O(M)$, depending on whether an approximation
    technique is adopted, as shown in the paper.'
  id: totrans-791
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic GCN [[29](#bib.bib29)]: $O(Mt)$, where $t$ denotes the number of time
    slices since the model runs one GCN at each time slice.'
  id: totrans-793
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bruna et al. [[40](#bib.bib40)] and Henaff et al. [[41](#bib.bib41)]: $O(N^{3})$,
    due to the time complexity of the eigendecomposition.'
  id: totrans-795
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ChebNet [[42](#bib.bib42)], Kipf and Welling [[43](#bib.bib43)], CayletNet [[44](#bib.bib44)],
    GWNN [[45](#bib.bib45)], and Neural FPs [[46](#bib.bib46)]: $O(M)$, as shown in
    the corresponding papers.'
  id: totrans-797
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PATCHY-SAN [[47](#bib.bib47)]: $O(M\log N)$, assuming the method adopts WL
    to label nodes, as shown in the paper.'
  id: totrans-799
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LGCN [[48](#bib.bib48)]: $O(M)$ since all the neighbors of each node are sorted
    in the method.'
  id: totrans-801
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SortPooling [[49](#bib.bib49)]: $O(M)$, due to the time complexity of adopted
    graph convolution layers.'
  id: totrans-803
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DCNN [[50](#bib.bib50)]: $O(N^{2})$, as reported in [[43](#bib.bib43)].'
  id: totrans-805
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DGCN [[51](#bib.bib51)]: $O(N^{2})$ since the PPMI matrix is not sparse.'
  id: totrans-807
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MPNNs [[52](#bib.bib52)]: $O(M)$, as shown in the paper.'
  id: totrans-809
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GraphSAGE [[53](#bib.bib53)]: $O(Ns^{L})$, where $s$ is the size of the sampled
    neighborhoods and $L$ is the number of layers, as shown in the paper.'
  id: totrans-811
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MoNet [[54](#bib.bib54)]: $O(M)$ since only the existing node pairs are involved
    in the calculation.'
  id: totrans-813
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GNs [[9](#bib.bib9)]: $O(M)$ since only the existing node pairs are involved
    in the calculation.'
  id: totrans-815
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kearnes et al. [[55](#bib.bib55)]: $O(M)$, since only the existing node pairs
    are used in the calculation.'
  id: totrans-817
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DiffPool [[56](#bib.bib56)]: $O(N^{2})$ since the coarsened graph is not sparse.'
  id: totrans-819
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GAT [[57](#bib.bib57)]: $O(M)$, as shown in the paper.'
  id: totrans-821
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GaAN [[58](#bib.bib58)]: $O(Ns^{L})$, where $s$ is a preset maximum neighborhood
    length and $L$ is the number of layers, as shown in the paper.'
  id: totrans-823
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAN [[59](#bib.bib59)]: $O(M_{\phi})$, the number of meta-path-based node pairs,
    as shown in the paper.'
  id: totrans-825
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CLN [[60](#bib.bib60)]: $O(M)$ since only the existing node pairs are involved
    in the calculation.'
  id: totrans-827
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PPNP [[61](#bib.bib61)]: $O(M)$, as shown in the paper.'
  id: totrans-829
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JK-Nets [[62](#bib.bib62)]: $O(M)$, due to the time complexity in adopted graph
    convolutional layers.'
  id: totrans-831
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ECC [[63](#bib.bib63)]: $O(M)$, as shown in the paper.'
  id: totrans-833
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'R-GCNs [[64](#bib.bib64)]: $O(M)$ since the edges of different types sum up
    to the total number of edges of the graph.'
  id: totrans-835
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LGNN [[65](#bib.bib65)]: $O(M)$, as shown in the paper.'
  id: totrans-837
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PinSage [[66](#bib.bib66)]: $O(Ns^{L})$, where $s$ is the size of the sampled
    neighborhoods and $L$ is the number of layers since a sampling strategy similar
    to that of GraphSAGE [[53](#bib.bib53)] is adopted.'
  id: totrans-839
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'StochasticGCN [[67](#bib.bib67)]: $O(Ns^{L})$, as shown in the paper.'
  id: totrans-841
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FastGCN [[68](#bib.bib68)] and Adapt [[69](#bib.bib69)]: $O(NsL)$ since the
    samples are drawn in each layer instead of in the neighborhoods, as shown in the
    paper.'
  id: totrans-843
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [[70](#bib.bib70)]: $O(M)$, due to the time complexity in adopted
    graph convolutional layers.'
  id: totrans-845
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SGC [[71](#bib.bib71)]: $O(M)$ since the calculation is the same as Kipf and
    Welling [[43](#bib.bib43)] by not adopting nonlinear activations.'
  id: totrans-847
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GFNN [[72](#bib.bib72)]: $O(M)$ since the calculation is the same as SGC [[71](#bib.bib71)]
    by adding an extra MLP layer.'
  id: totrans-849
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GIN [[73](#bib.bib73)]: $O(M)$, due to the time complexity in adopted graph
    convolutional layers.'
  id: totrans-851
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DGI [[74](#bib.bib74)]: $O(M)$, due to the time complexity in adopted graph
    convolutional layers.'
  id: totrans-853
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SAE [[96](#bib.bib96)] and SDNE [[97](#bib.bib97)]: $O(M)$, as shown in the
    corresponding papers.'
  id: totrans-855
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DNGR [[98](#bib.bib98)]: $O(N^{2})$, due to the time complexity of calculating
    the PPMI matrix.'
  id: totrans-857
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GC-MC [[99](#bib.bib99)]: $O(M)$ since the encoder adopts the GCN proposed
    by Kipf and Welling [[43](#bib.bib43)] and only the non-zero elements of the graph
    are considered in the decoder.'
  id: totrans-859
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DRNE [[100](#bib.bib100)]: $O(Ns)$, where $s$ is a preset maximum neighborhood
    length, as shown in the paper.'
  id: totrans-861
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'G2G [[101](#bib.bib101)]: $O(M)$, due to the definition of the ranking loss.'
  id: totrans-863
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VGAE [[102](#bib.bib102)]: $O(N^{2})$, due to the reconstruction of all the
    node pairs.'
  id: totrans-865
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DVNE [[103](#bib.bib103)]: Though the original paper reported to have a time
    complexity of $O(Md_{\text{avg}})$ where $d_{\text{avg}}$ is the average degree,
    we have confirmed that it can be easily improved to $O(M)$ through personal communications
    with the authors.'
  id: totrans-867
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ARGA/ARVGA [[104](#bib.bib104)]: $O(N^{2})$, due to the reconstruction of all
    the node pairs.'
  id: totrans-869
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NetRA [[105](#bib.bib105)]: $O(M)$, as shown in the paper.'
  id: totrans-871
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GCPN [[116](#bib.bib116)]: $O(MN)$ since the embedding of all the nodes are
    used when generating each edge.'
  id: totrans-873
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MolGAN [[117](#bib.bib117)] and GTPN [[118](#bib.bib118)]: $O(N^{2})$ since
    the scores for all the node pairs have to be calculated.'
  id: totrans-875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GAM [[119](#bib.bib119)]: $O(d_{\text{avg}}sT)$, where $d_{\text{avg}}$ is
    the average degree, $s$ is the number of sampled random walks, and $T$ is the
    walk length, as shown in the paper.'
  id: totrans-877
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DeepPath [[120](#bib.bib120)]: $O(d_{\text{avg}}sT+s^{2}T)$, where $d_{\text{avg}}$
    is the average degree, $s$ is the number of sampled paths, and $T$ is the path
    length. The former term corresponds to finding paths and the latter term results
    from the diversity constraint.'
  id: totrans-879
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MINERVA [[121](#bib.bib121)]: $O(d_{\text{avg}}sT)$, where $d_{\text{avg}}$
    is the average degree, $s$ is the number of sampled paths, and $T$ is the path
    length, similar to the pathfinding method in DeepPath [[120](#bib.bib120)].'
  id: totrans-881
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GraphGAN [[124](#bib.bib124)]: $O(MN)$, as shown in the paper.'
  id: totrans-883
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANE [[125](#bib.bib125)]: $O(N)$, which is the extra time complexity introduced
    by the model in the generator and the discriminator.'
  id: totrans-885
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GraphSGAN [[126](#bib.bib126)]: $O(N^{2})$, due to the time complexity in the
    objective function.'
  id: totrans-887
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NetGAN [[127](#bib.bib127)]: $O(M)$, as shown in the paper.'
  id: totrans-889
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nettack [[128](#bib.bib128)]: $O(Nd_{0}^{2})$, where $d_{0}$ is the degree
    of the targeted node, as shown in the paper.'
  id: totrans-891
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. [[129](#bib.bib129)]: $O(M)$, which is the time complexity of the
    most effective strategy RL-S2V, as shown in the paper.'
  id: totrans-893
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zugner and Gunnemann [[130](#bib.bib130)]: $O(N^{2})$, as shown in the paper.'
  id: totrans-895
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
