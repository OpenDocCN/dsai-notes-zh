- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:06:57'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1812.04202] Deep Learning on Graphs: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1812.04202](https://ar5iv.labs.arxiv.org/html/1812.04202)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning on Graphs: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ziwei Zhang, Peng Cui and Wenwu Zhu Z. Zhang, P. Cui, and W. Zhu are with the
    Department of Computer Science and Technology, Tsinghua University, Beijing, China.
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: zw-zhang16@mails.tsinghua.edu.cn, cuip@tsinghua.edu.cn,'
  prefs: []
  type: TYPE_NORMAL
- en: wwzhu@tsinghua.edu.cn. P. Cui and W. Zhu are corresponding authors.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Deep learning has been shown to be successful in a number of domains, ranging
    from acoustics, images, to natural language processing. However, applying deep
    learning to the ubiquitous graph data is non-trivial because of the unique characteristics
    of graphs. Recently, substantial research efforts have been devoted to applying
    deep learning methods to graphs, resulting in beneficial advances in graph analysis
    techniques. In this survey, we comprehensively review the different types of deep
    learning methods on graphs. We divide the existing methods into five categories
    based on their model architectures and training strategies: graph recurrent neural
    networks, graph convolutional networks, graph autoencoders, graph reinforcement
    learning, and graph adversarial methods. We then provide a comprehensive overview
    of these methods in a systematic manner mainly by following their development
    history. We also analyze the differences and compositions of different methods.
    Finally, we briefly outline the applications in which they have been used and
    discuss potential future research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Graph Data, Deep Learning, Graph Neural Network, Graph Convolutional Network,
    Graph Autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Over the past decade, deep learning has become the “crown jewel” of artificial
    intelligence and machine learning [[1](#bib.bib1)], showing superior performance
    in acoustics [[2](#bib.bib2)], images [[3](#bib.bib3)] and natural language processing [[4](#bib.bib4)],
    etc. The expressive power of deep learning to extract complex patterns from underlying
    data is well recognized. On the other hand, graphs¹¹1Graphs are also called *networks*
    such as in social networks. In this paper, we use two terms interchangeably. are
    ubiquitous in the real world, representing objects and their relationships in
    varied domains, including social networks, e-commerce networks, biology networks,
    traffic networks, and so on. Graphs are also known to have complicated structures
    that can contain rich underlying values [[5](#bib.bib5)]. As a result, how to
    utilize deep learning methods to analyze graph data has attracted considerable
    research attention over the past few years. This problem is non-trivial because
    several challenges exist in applying traditional deep learning architectures to
    graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irregular structures of graphs. Unlike images, audio, and text, which have a
    clear grid structure, graphs have irregular structures, making it hard to generalize
    some of the basic mathematical operations to graphs [[6](#bib.bib6)]. For example,
    defining convolution and pooling operations, which are the fundamental operations
    in convolutional neural networks (CNNs), for graph data is not straightforward.
    This problem is often referred to as the geometric deep learning problem [[7](#bib.bib7)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heterogeneity and diversity of graphs. A graph itself can be complicated, containing
    diverse types and properties. For example, graphs can be heterogeneous or homogenous,
    weighted or unweighted, and signed or unsigned. In addition, the tasks of graphs
    also vary widely, ranging from node-focused problems such as node classification
    and link prediction to graph-focused problems such as graph classification and
    graph generation. These diverse types, properties, and tasks require different
    model architectures to tackle specific problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-scale graphs. In the big-data era, real graphs can easily have millions
    or billions of nodes and edges; some well-known examples are social networks and
    e-commerce networks [[8](#bib.bib8)]. Therefore, how to design scalable models,
    preferably models that have a linear time complexity with respect to the graph
    size, is a key problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Incorporating interdisciplinary knowledge. Graphs are often connected to other
    disciplines, such as biology, chemistry, and social sciences. This interdisciplinary
    nature provides both opportunities and challenges: domain knowledge can be leveraged
    to solve specific problems but integrating domain knowledge can complicate model
    designs. For example, when generating molecular graphs, the objective function
    and chemical constraints are often non-differentiable; therefore gradient-based
    training methods cannot easily be applied.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To tackle these challenges, tremendous efforts have been made in this area,
    resulting in a rich literature of related papers and methods. The adopted architectures
    and training strategies also vary greatly, ranging from supervised to unsupervised
    and from convolutional to recursive. However, to the best of our knowledge, little
    effort has been made to systematically summarize the differences and connections
    between these diverse methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/761eb80733568f5b76cfa0590e399272.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A categorization of deep learning methods on graphs. We divide the
    existing methods into five categories: graph recurrent neural networks, graph
    convolutional networks, graph autoencoders, graph reinforcement learning, and
    graph adversarial methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Main Distinctions among Deep Learning Methods on Graphs'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Basic Assumptions/Aims | Main Functions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Graph recurrent neural networks | Recursive and sequential patterns of graphs
    | Definitions of states for nodes or graphs |'
  prefs: []
  type: TYPE_TB
- en: '| Graph convolutional networks | Common local and global structural patterns
    of graphs | Graph convolution and readout operations |'
  prefs: []
  type: TYPE_TB
- en: '| Graph autoencoders | Low-rank structures of graphs | Unsupervised node representation
    learning |'
  prefs: []
  type: TYPE_TB
- en: '| Graph reinforcement learning | Feedbacks and constraints of graph tasks |
    Graph-based actions and rewards |'
  prefs: []
  type: TYPE_TB
- en: '| Graph adversarial methods | The generalization ability and robustness of
    graph-based models | Graph adversarial trainings and attacks |'
  prefs: []
  type: TYPE_TB
- en: 'In this paper, we try to fill this knowledge gap by comprehensively reviewing
    deep learning methods on graphs. Specifically, as shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Deep Learning on Graphs: A Survey"), we divide the existing
    methods into five categories based on their model architectures and training strategies:
    graph recurrent neural networks (Graph RNNs), graph convolutional networks (GCNs),
    graph autoencoders (GAEs), graph reinforcement learning (Graph RL), and graph
    adversarial methods. We summarize some of the main characteristics of these categories
    in Table [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Deep Learning on Graphs: A Survey")
    based on the following high-level distinctions. Graph RNNs capture recursive and
    sequential patterns of graphs by modeling states at either the node-level or the
    graph-level. GCNs define convolution and readout operations on irregular graph
    structures to capture common local and global structural patterns. GAEs assume
    low-rank graph structures and adopt unsupervised methods for node representation
    learning. Graph RL defines graph-based actions and rewards to obtain feedbacks
    on graph tasks while following constraints. Graph adversarial methods adopt adversarial
    training techniques to enhance the generalization ability of graph-based models
    and test their robustness by adversarial attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we provide a comprehensive and detailed overview
    of these methods, mainly by following their development history and the various
    ways these methods solve the challenges posed by graphs. We also analyze the differences
    between these models and delve into how to composite different architectures.
    Finally, we briefly outline the applications of these models, introduce several
    open libraries, and discuss potential future research directions. In the appendix,
    we provide a source code repository, analyze the time complexity of various methods
    discussed in the paper, and summarize some common applications.
  prefs: []
  type: TYPE_NORMAL
- en: Related works. Several previous surveys are related to our paper. Bronstein et al. [[7](#bib.bib7)]
    summarized some early GCN methods as well as CNNs on manifolds and studied them
    comprehensively through geometric deep learning. Battaglia et al. [[9](#bib.bib9)]
    summarized how to use GNNs and GCNs for relational reasoning using a unified framework
    called graph networks, Lee et al. [[10](#bib.bib10)] reviewed the attention models
    for graphs, Zhang et al. [[11](#bib.bib11)] summarized some GCNs, and Sun et al. [[12](#bib.bib12)]
    briefly surveyed adversarial attacks on graphs. Our work differs from these previous
    works in that we systematically and comprehensively review different deep learning
    architectures on graphs rather than focusing on one specific branch. Concurrent
    to our work, Zhou et al. [[13](#bib.bib13)] and Wu  et al. [[14](#bib.bib14)]
    surveyed this field from different viewpoints and categorizations. Specifically,
    neither of their works consider graph reinforcement learning or graph adversarial
    methods, which are covered in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: Another closely related topic is network embedding, aiming to embed nodes into
    a low-dimensional vector space [[15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)].
    The main distinction between network embedding and our paper is that we focus
    on how different deep learning models are applied to graphs, and network embedding
    can be recognized as a concrete application example that uses some of these models
    (and it uses non-deep-learning methods as well).
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this paper is organized as follows. In Section 2, we introduce the
    notations used in this paper and provide preliminaries. Then, we review Graph
    RNNs, GCNs, GAEs, Graph RL, and graph adversarial methods in Section 3 to 7, respectively.
    We conclude with a discussion in Section 8.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Notations and Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Notations. In this paper, a graph²²2We consider only graphs without self-loops
    or multiple edges. is represented as $G=\left(V,E\right)$ where $V=\left\{v_{1},...,v_{N}\right\}$
    is a set of $N=\left|V\right|$ nodes and $E\subseteq V\times V$ is a set of $M=\left|E\right|$
    edges between nodes. We use $\mathbf{A}\in\mathbb{R}^{N\times N}$ to denote the
    adjacency matrix, whose $i^{th}$ row, $j^{th}$ column, and an element are denoted
    as $\mathbf{A}(i,:),\mathbf{A}(:,j),\mathbf{A}(i,j)$, respectively. The graph
    can be either directed or undirected and weighted or unweighted. In this paper,
    we mainly consider unsigned graphs; therefore, $\mathbf{A}(i,j)\geq 0$. Signed
    graphs will be discussed in future research directions. We use $\mathbf{F}^{V}$
    and $\mathbf{F}^{E}$ to denote features of nodes and edges, respectively. For
    other variables, we use bold uppercase characters to denote matrices and bold
    lowercase characters to denote vectors, e.g., a matrix $\mathbf{X}$ and a vector
    $\mathbf{x}$. The transpose of a matrix is denoted as $\mathbf{X}^{T}$ and the
    element-wise multiplication is denoted as $\mathbf{X}_{1}\odot\mathbf{X}_{2}$.
    Functions are marked with curlicues, e.g., $\mathcal{F}(\cdot)$.
  prefs: []
  type: TYPE_NORMAL
- en: To better illustrate the notations, we take social networks as an example. Each
    node $v_{i}\in V$ corresponds to a user, and the edges $E$ correspond to relations
    between users. The profiles of users (e.g., age, gender, and location) can be
    represented as node features $\mathbf{F}^{V}$ and interaction data (e.g., sending
    messages and comments) can be represented as edge features $\mathbf{F}^{E}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: A Table for Commonly Used Notations'
  prefs: []
  type: TYPE_NORMAL
- en: '| $G=(V,E)$ | A graph |'
  prefs: []
  type: TYPE_TB
- en: '| $N,M$ | The number of nodes and edges |'
  prefs: []
  type: TYPE_TB
- en: '| $V=\left\{v_{1},...,v_{N}\right\}$ | The set of nodes |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{F}^{V},\mathbf{F}^{E}$ | The attributes/features of nodes and edges
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{A}$ | The adjacency matrix |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{D}(i,i)=\sum_{j}\mathbf{A}(i,j)$ | The diagonal degree matrix |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{L}=\mathbf{D}-\mathbf{A}$ | The Laplacian matrix |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{T}=\mathbf{L}$ | The eigendecomposition
    of $\mathbf{L}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$ | The transition matrix |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{N}_{k}(i),\mathcal{N}(i)$ | The k-step and 1-step neighbors of
    $v_{i}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{H}^{l}$ | The hidden representation in the $l^{th}$ layer |'
  prefs: []
  type: TYPE_TB
- en: '| $f_{l}$ | The dimensionality of $\mathbf{H}^{l}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\rho(\cdot)$ | Some non-linear activation function |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{X}_{1}\odot\mathbf{X}_{2}$ | The element-wise multiplication |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\Theta}$ | Learnable parameters |'
  prefs: []
  type: TYPE_TB
- en: '| $s$ | The sample size |'
  prefs: []
  type: TYPE_TB
- en: Preliminaries. The Laplacian matrix of an undirected graph is defined as $\mathbf{L}=\mathbf{D}-\mathbf{A}$,
    where $\mathbf{D}\in\mathbb{R}^{N\times N}$ is a diagonal degree matrix with $\mathbf{D}(i,i)=\sum_{j}\mathbf{A}(i,j)$.
    Its eigendecomposition is denoted as $\mathbf{L}=\mathbf{Q\Lambda Q^{T}}$, where
    $\mathbf{\Lambda}\in\mathbb{R}^{N\times N}$ is a diagonal matrix of eigenvalues
    sorted in ascending order and $\mathbf{Q}\in\mathbb{R}^{N\times N}$ are the corresponding
    eigenvectors. The transition matrix is defined as $\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$,
    where $\mathbf{P}(i,j)$ represents the probability of a random walk starting from
    node $v_{i}$ landing at node $v_{j}$. The $k$-step neighbors of node $v_{i}$ are
    defined as $\mathcal{N}_{k}(i)=\left\{j|\mathcal{D}(i,j)\leq k\right\}$, where
    $\mathcal{D}(i,j)$ is the shortest distance from node $v_{i}$ to $v_{j}$, i.e.
    $\mathcal{N}_{k}(i)$ is a set of nodes reachable from node $v_{i}$ within $k$-steps.
    To simplify the notation, we omit the subscript for the immediate neighborhood,
    i.e., $\mathcal{N}(i)=\mathcal{N}_{1}(i)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a deep learning model, we use superscripts to denote layers, e.g., $\mathbf{H}^{l}$.
    We use $f_{l}$ to denote the dimensionality of the layer $l$ (i.e., $\mathbf{H}^{l}\in\mathbb{R}^{N\times
    f_{l}}$). The sigmoid activation function is defined as $\sigma(x)=1/\left(1+e^{-x}\right)$
    and the rectified linear unit (ReLU) is defined as $\text{ReLU}(x)=max(0,x)$.
    A general element-wise nonlinear activation function is denoted as $\rho(\cdot)$.
    In this paper, unless stated otherwise, we assume all functions are differentiable,
    allowing the model parameters $\mathbf{\Theta}$ to be learned through back-propagation [[18](#bib.bib18)]
    using commonly adopted optimizers such as Adam [[19](#bib.bib19)] and training
    techniques such as dropout [[20](#bib.bib20)]. We denote the sample size as $s$
    if a sampling technique is adopted. We summarize the notations in Table [II](#S2.T2
    "TABLE II ‣ 2 Notations and Preliminaries ‣ Deep Learning on Graphs: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The tasks for learning a deep model on graphs can be broadly divided into two
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Node-focused tasks: These tasks are associated with individual nodes in the
    graph. Examples include node classification, link prediction, and node recommendation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graph-focused tasks: These tasks are associated with the entire graph. Examples
    include graph classification, estimating various graph properties, and generating
    graphs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that such distinctions are more conceptually than mathematically rigorous.
    Some existing tasks are associated with mesoscopic structures such as community
    detection [[21](#bib.bib21)]. In addition, node-focused problems can sometimes
    be studied as graph-focused problems by transforming the former into egocentric
    networks [[22](#bib.bib22)]. Nevertheless, we will explain the differences in
    algorithm designs for these two categories when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: The Main Characteristics of Graph Recurrent Neural Network (Graph
    RNNs)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method | Recursive/sequential patterns of graphs | Time Complexity
    | Other Improvements |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Node-level | GNN [[23](#bib.bib23)] | A recursive definition of node states
    | $O(MI_{f})$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| GGS-NNs [[24](#bib.bib24)] | $O(MT)$ | Sequence outputs |'
  prefs: []
  type: TYPE_TB
- en: '| SSE [[25](#bib.bib25)] | $O(d_{\text{avg}}S)$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-level | You et al. [[26](#bib.bib26)] | Generate nodes and edges in
    an autoregressive manner | $O(N^{2})$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| DGNN [[27](#bib.bib27)] | Capture the time dynamics of the formation of nodes
    and edges | $O(Md_{\text{avg}})$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| RMGCNN [[28](#bib.bib28)] | Recursively reconstruct the graph | $O(M)$ or
    $O(MN)$ | GCN layers |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamic GCN [[29](#bib.bib29)] | Gather node representations in different
    time slices | $O(Mt)$ | GCN layers |'
  prefs: []
  type: TYPE_TB
- en: 3 Graph Recurrent Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recurrent neural networks (RNNs) such as gated recurrent units (GRU) [[30](#bib.bib30)]
    or long short-term memory (LSTM) [[31](#bib.bib31)] are de facto standards in
    modeling sequential data. In this section, we review Graph RNNs which can capture
    recursive and sequential patterns of graphs. Graph RNNs can be broadly divided
    into two categories: node-level RNNs and graph-level RNNs. The main distinction
    lies in whether the patterns lie at the node-level and are modeled by node states,
    or at the graph-level and are modeled by a common graph state. The main characteristics
    of the methods surveyed are summarized in Table [III](#S2.T3 "TABLE III ‣ 2 Notations
    and Preliminaries ‣ Deep Learning on Graphs: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Node-level RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Node-level RNNs for graphs, which are also referred to as graph neural networks
    (GNNs)³³3Recently, GNNs have also been used to refer to general neural networks
    for graph data. We follow the traditional naming convention and use GNNs to refer
    to this specific type of Graph RNNs., can be dated back to the ”pre-deep-learning”
    era [[32](#bib.bib32), [23](#bib.bib23)]. The idea behind a GNN is simple: to
    encode graph structural information, each node $v_{i}$ is represented by a low-dimensional
    state vector $\mathbf{s}_{i}$. Motivated by recursive neural networks [[33](#bib.bib33)],
    a recursive definition of states is adopted [[23](#bib.bib23)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{s}_{i}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}(\mathbf{s}_{i},\mathbf{s}_{j},\mathbf{F}^{V}_{i},\mathbf{F}^{V}_{j},\mathbf{F}^{E}_{i,j}),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{F}(\cdot)$ is a parametric function to be learned. After obtaining
    $\mathbf{s}_{i}$, another function $\mathcal{O}(\cdot)$ is applied to get the
    final outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}_{i}=\mathcal{O}(\mathbf{s}_{i},\mathbf{F}^{V}_{i}).$ |  | (2)
    |'
  prefs: []
  type: TYPE_TB
- en: 'For graph-focused tasks, the authors of [[23](#bib.bib23)] suggested adding
    a special node with unique attributes to represent the entire graph. To learn
    the model parameters, the following semi-supervised⁴⁴4It is called semi-supervised
    because all the graph structures and some subset of the node or graph labels is
    used during training. method is adopted: after iteratively solving Eq. ([1](#S3.E1
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")) to a stable point using the Jacobi method [[34](#bib.bib34)],
    one gradient descent step is performed using the Almeida-Pineda algorithm [[35](#bib.bib35),
    [36](#bib.bib36)] to minimize a task-specific objective function, for example,
    the squared loss between the predicted values and the ground-truth for regression
    tasks; then, this process is repeated until convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the two simple equations in Eqs. ([1](#S3.E1 "In 3.1 Node-level RNNs
    ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on Graphs: A Survey"))([2](#S3.E2
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")), GNN plays two important roles. In retrospect, a GNN unifies
    some of the early methods used for processing graph data, such as recursive neural
    networks and Markov chains [[23](#bib.bib23)]. Looking toward the future, the
    general idea underlying GNNs has profound inspirations: as will be shown later,
    many state-of-the-art GCNs actually have a formulation similar to Eq. ([1](#S3.E1
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")) and follow the same framework of exchanging information within
    the immediate node neighborhoods. In fact, GNNs and GCNs can be unified into some
    common frameworks, and a GNN is equivalent to a GCN that uses identical layers
    to reach stable states. More discussion will be provided in Section 4.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although they are conceptually important, GNNs have several drawbacks. First,
    to ensure that Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural
    Networks ‣ Deep Learning on Graphs: A Survey")) has a unique solution, $\mathcal{F}(\cdot)$
    must be a “contraction map” [[37](#bib.bib37)], i.e., $\exists\mu,0<\mu<1$ so
    that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left\&#124;\mathcal{F}(x)-\mathcal{F}(y)\right\&#124;\leq\mu\left\&#124;x-y\right\&#124;,\forall
    x,y.$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Intuitively, a “contraction map” requires that the distance between any two
    points can only “contract” after the $\mathcal{F}(\cdot)$ operation, which severely
    limits the modeling ability. Second, because many iterations are needed to reach
    a stable state between gradient descend steps, GNNs are computationally expensive.
    Because of these drawbacks and perhaps a lack of computational power (e.g., the
    graphics processing unit, GPU, was not widely used for deep learning in those
    days) and lack of research interests, GNNs did not become a focus of general research.
  prefs: []
  type: TYPE_NORMAL
- en: 'A notable improvement to GNNs is gated graph sequence neural networks (GGS-NNs) [[24](#bib.bib24)]
    with the following modifications. Most importantly, the authors replaced the recursive
    definition in Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural
    Networks ‣ Deep Learning on Graphs: A Survey")) with a GRU, thus removing the
    “contraction map” requirement and supporting modern optimization techniques. Specifically,
    Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣
    Deep Learning on Graphs: A Survey")) is adapted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{s}_{i}^{(t)}=(1-\mathbf{z}_{i}^{(t)})\odot\mathbf{s}_{i}^{(t-1)}+\mathbf{z}_{i}^{(t)}\odot\widetilde{\mathbf{s}}_{i}^{(t)},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{z}$ is calculated by the update gate, $\widetilde{\mathbf{s}}$
    is the candidate for updating, and $t$ is the pseudo time. Second, the authors
    proposed using several such networks operating in sequence to produce sequence
    outputs and showed that their method could be applied to sequence-based tasks
    such as program verification [[38](#bib.bib38)].
  prefs: []
  type: TYPE_NORMAL
- en: 'SSE [[25](#bib.bib25)] took a similar approach as Eq. ([4](#S3.E4 "In 3.1 Node-level
    RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on Graphs: A Survey")).
    However, instead of using a GRU in the calculation, SSE adopted stochastic fixed-point
    gradient descent to accelerate the training process. This scheme basically alternates
    between calculating steady node states using local neighborhoods and optimizing
    the model parameters, with both calculations in stochastic mini-batches.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Graph-level RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we review how to apply RNNs to capture graph-level patterns,
    e.g., temporal patterns of dynamic graphs or sequential patterns at different
    levels of graph granularities. In graph-level RNNs, instead of applying one RNN
    to each node to learn the node states, a single RNN is applied to the entire graph
    to encode the graph states.
  prefs: []
  type: TYPE_NORMAL
- en: 'You et al. [[26](#bib.bib26)] applied Graph RNNs to the graph generation problem.
    Specifically, they adopted two RNNs: one to generate new nodes and the other to
    generate edges for the newly added node in an autoregressive manner. They showed
    that such hierarchical RNN architectures learn more effectively from input graphs
    than do the traditional rule-based graph generative models while having a reasonable
    time complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: To capture the temporal information of dynamic graphs, dynamic graph neural
    network (DGNN) [[27](#bib.bib27)] was proposed that used a time-aware LSTM [[39](#bib.bib39)]
    to learn node representations. When a new edge is established, DGNN used the LSTM
    to update the representation of the two interacting nodes as well as their immediate
    neighbors, i.e., considering the one-step propagation effect. The authors showed
    that the time-aware LSTM could model the establishing orders and time intervals
    of edge formations well, which in turn benefited a range of graph applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph RNN can also be combined with other architectures, such as GCNs or GAEs.
    For example, aiming to tackle the graph sparsity problem, RMGCNN [[28](#bib.bib28)]
    applied an LSTM to the results of GCNs to progressively reconstruct a graph as
    illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Graph-level RNNs ‣ 3 Graph Recurrent
    Neural Networks ‣ Deep Learning on Graphs: A Survey"). By using an LSTM, the information
    from different parts of the graph can diffuse across long ranges without requiring
    as many GCN layers. Dynamic GCN [[29](#bib.bib29)] applied an LSTM to gather the
    results of GCNs from different time slices in dynamic networks to capture both
    the spatial and temporal graph information.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9b530023d6cbdcfb3e9a58ae41909dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The framework of RMGCNN (reprinted from [[28](#bib.bib28)] with permission).
    RMGCNN includes an LSTM in the GCN to progressively reconstruct the graph. $\mathbf{X}^{t}$,
    $\mathbf{\tilde{X}}^{t}$, and $d\mathbf{X}^{t}$ represent the estimated matrix,
    the outputs of GCNs, and the incremental updates produced by the RNN at iteration
    $t$, respectively. MGCNN refers to a multigraph CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: A Comparison among Different Graph Convolutional Networks (GCNs).
    T.C. = Time Complexity, M.G. = Multiple Graphs'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Type | Convolution | Readout | T.C. | M.G. | Other Characteristics
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bruna et al. [[40](#bib.bib40)] | Spectral | Interpolation kernel | Hierarchical
    clustering + FC | $O(N^{3})$ | No | - |'
  prefs: []
  type: TYPE_TB
- en: '| Henaff et al. [[41](#bib.bib41)] | Spectral | Interpolation kernel | Hierarchical
    clustering + FC | $O(N^{3})$ | No | Constructing the graph |'
  prefs: []
  type: TYPE_TB
- en: '| ChebNet [[42](#bib.bib42)] | Spectral/Spatial | Polynomial | Hierarchical
    clustering | $O(M)$ | Yes | - |'
  prefs: []
  type: TYPE_TB
- en: '| Kipf&Welling [[43](#bib.bib43)] | Spectral/Spatial | First-order | - | $O(M)$
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CayletNet [[44](#bib.bib44)] | Spectral | Polynomial | Hierarchical clustering
    + FC | $O(M)$ | No | - |'
  prefs: []
  type: TYPE_TB
- en: '| GWNN [[45](#bib.bib45)] | Spectral | Wavelet transform | - | $O(M)$ | No
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| Neural FPs [[46](#bib.bib46)] | Spatial | First-order | Sum | $O(M)$ | Yes
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| PATCHY-SAN [[47](#bib.bib47)] | Spatial | Polynomial + an order | An order
    + pooling | $O(M\log N)$ | Yes | A neighbor order |'
  prefs: []
  type: TYPE_TB
- en: '| LGCN [[48](#bib.bib48)] | Spatial | First-order + an order | - | $O(M)$ |
    Yes | A neighbor order |'
  prefs: []
  type: TYPE_TB
- en: '| SortPooling [[49](#bib.bib49)] | Spatial | First-order | An order + pooling
    | $O(M)$ | Yes | A node order |'
  prefs: []
  type: TYPE_TB
- en: '| DCNN [[50](#bib.bib50)] | Spatial | Polynomial diffusion | Mean | $O(N^{2})$
    | Yes | Edge features |'
  prefs: []
  type: TYPE_TB
- en: '| DGCN [[51](#bib.bib51)] | Spatial | First-order + diffusion | - | $O(N^{2})$
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MPNNs [[52](#bib.bib52)] | Spatial | First-order | Set2set | $O(M)$ | Yes
    | A general framework |'
  prefs: []
  type: TYPE_TB
- en: '| GraphSAGE [[53](#bib.bib53)] | Spatial | First-order + sampling | - | $O(Ns^{L})$
    | Yes | A general framework |'
  prefs: []
  type: TYPE_TB
- en: '| MoNet [[54](#bib.bib54)] | Spatial | First-order | Hierarchical clustering
    | $O(M)$ | Yes | A general framework |'
  prefs: []
  type: TYPE_TB
- en: '| GNs [[9](#bib.bib9)] | Spatial | First-order | A graph representation | $O(M)$
    | Yes | A general framework |'
  prefs: []
  type: TYPE_TB
- en: '| Kearnes et al. [[55](#bib.bib55)] | Spatial | Weave module | Fuzzy histogram
    | $O(M)$ | Yes | Edge features |'
  prefs: []
  type: TYPE_TB
- en: '| DiffPool [[56](#bib.bib56)] | Spatial | Various | Hierarchical clustering
    | $O(N^{2})$ | Yes | Differentiable pooling |'
  prefs: []
  type: TYPE_TB
- en: '| GAT [[57](#bib.bib57)] | Spatial | First-order | - | $O(M)$ | Yes | Attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| GaAN [[58](#bib.bib58)] | Spatial | First-order | - | $O(Ns^{L})$ | Yes |
    Attention |'
  prefs: []
  type: TYPE_TB
- en: '| HAN [[59](#bib.bib59)] | Spatial | Meta-path neighbors | - | $O(M_{\phi})$
    | Yes | Attention |'
  prefs: []
  type: TYPE_TB
- en: '| CLN [[60](#bib.bib60)] | Spatial | First-order | - | $O(M)$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PPNP [[61](#bib.bib61)] | Spatial | First-order | - | $O(M)$ | - | Teleportation
    connections |'
  prefs: []
  type: TYPE_TB
- en: '| JK-Nets [[62](#bib.bib62)] | Spatial | Various | - | $O(M)$ | Yes | Jumping
    connections |'
  prefs: []
  type: TYPE_TB
- en: '| ECC [[63](#bib.bib63)] | Spatial | First-order | Hierarchical clustering
    | $O(M)$ | Yes | Edge features |'
  prefs: []
  type: TYPE_TB
- en: '| R-GCNs [[64](#bib.bib64)] | Spatial | First-order | - | $O(M)$ | - | Edge
    features |'
  prefs: []
  type: TYPE_TB
- en: '| LGNN [[65](#bib.bib65)] | Spatial | First-order + LINE graph | - | $O(M)$
    | - | Edge features |'
  prefs: []
  type: TYPE_TB
- en: '| PinSage [[66](#bib.bib66)] | Spatial | Random walk | - | $O(Ns^{L})$ | -
    | Neighborhood sampling |'
  prefs: []
  type: TYPE_TB
- en: '| StochasticGCN [[67](#bib.bib67)] | Spatial | First-order + sampling | - |
    $O(Ns^{L})$ | - | Neighborhood sampling |'
  prefs: []
  type: TYPE_TB
- en: '| FastGCN [[68](#bib.bib68)] | Spatial | First-order + sampling | - | $O(NsL)$
    | Yes | Layer-wise sampling |'
  prefs: []
  type: TYPE_TB
- en: '| Adapt [[69](#bib.bib69)] | Spatial | First-order + sampling | - | $O(NsL)$
    | Yes | Layer-wise sampling |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[70](#bib.bib70)] | Spatial | First-order | - | $O(M)$ | - | Theoretical
    analysis |'
  prefs: []
  type: TYPE_TB
- en: '| SGC [[71](#bib.bib71)] | Spatial | Polynomial | - | $O(M)$ | Yes | Theoretical
    analysis |'
  prefs: []
  type: TYPE_TB
- en: '| GFNN [[72](#bib.bib72)] | Spatial | Polynomial | - | $O(M)$ | Yes | Theoretical
    analysis |'
  prefs: []
  type: TYPE_TB
- en: '| GIN [[73](#bib.bib73)] | Spatial | First-order | Sum + MLP | $O(M)$ | Yes
    | Theoretical analysis |'
  prefs: []
  type: TYPE_TB
- en: '| DGI [[74](#bib.bib74)] | Spatial | First-order | - | $O(M)$ | Yes | Unsupervised
    training |'
  prefs: []
  type: TYPE_TB
- en: 4 Graph Convolutional Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Graph convolutional networks (GCNs) are inarguably the hottest topic in graph-based
    deep learning. Mimicking CNNs, modern GCNs learn the common local and global structural
    patterns of graphs through designed convolution and readout functions. Because
    most GCNs can be trained with task-specific loss via backpropagation (with a few
    exceptions such as the unsupervised training method in [[74](#bib.bib74)]), we
    focus on the adopted architectures. We first discuss the convolution operations,
    then move to the readout operations and some other improvements. We summarize
    the main characteristics of GCNs surveyed in this paper in Table [IV](#S3.T4 "TABLE
    IV ‣ 3.2 Graph-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning
    on Graphs: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Convolution Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Graph convolutions can be divided into two groups: *spectral convolutions*,
    which perform convolution by transforming node representations into the spectral
    domain using the graph Fourier transform or its extensions, and *spatial convolutions*,
    which perform convolution by considering node neighborhoods. Note that these two
    groups can overlap, for example, when using a polynomial spectral kernel (please
    refer to Section [4.1.2](#S4.SS1.SSS2 "4.1.2 The Efficiency Aspect ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")
    for details).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Spectral Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Convolution is the most fundamental operation in CNNs. However, the standard
    convolution operation used for images or text cannot be directly applied to graphs
    because graphs lack a grid structure [[6](#bib.bib6)]. Bruna et al. [[40](#bib.bib40)]
    first introduced convolution for graph data from the spectral domain using the
    graph Laplacian matrix $\mathbf{L}$ [[75](#bib.bib75)], which plays a similar
    role as the Fourier basis in signal processing [[6](#bib.bib6)]. The graph convolution
    operation, $*_{G}$, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{u}_{1}*_{G}\mathbf{u}_{2}=\mathbf{Q}\left(\left(\mathbf{Q}^{T}\mathbf{u}_{1}\right)\odot\left(\mathbf{Q}^{T}\mathbf{u}_{2}\right)\right),$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{u}_{1},\mathbf{u}_{2}\in\mathbb{R}^{N}$ are two signals⁵⁵5We
    give an example of graph signals in Appendix [D](#A4 "Appendix D An Example of
    Graph Signals ‣ Deep Learning on Graphs: A Survey"). defined on nodes and $\mathbf{Q}$
    are the eigenvectors of $\mathbf{L}$. Briefly, multiplying $\mathbf{Q}^{T}$ transforms
    the graph signals $\mathbf{u}_{1},\mathbf{u}_{2}$ into the spectral domain (i.e.,
    the graph Fourier transform), while multiplying $\mathbf{Q}$ performs the inverse
    transform. The validity of this definition is based on the convolution theorem,
    i.e., the Fourier transform of a convolution operation is the element-wise product
    of their Fourier transforms. Then, a signal $\mathbf{u}$ can be filtered by'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{u}^{\prime}=\mathbf{Q}\mathbf{\Theta}\mathbf{Q}^{T}\mathbf{u},$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{u}^{\prime}$ is the output signal, $\mathbf{\Theta}=\mathbf{\Theta}(\mathbf{\Lambda})\in\mathbb{R}^{N\times
    N}$ is a diagonal matrix of learnable filters and $\mathbf{\Lambda}$ are the eigenvalues
    of $\mathbf{L}$. A convolutional layer is defined by applying different filters
    to different input-output signal pairs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{u}^{l+1}_{j}=\rho\left(\sum\nolimits_{i=1}^{f_{l}}\mathbf{Q}\mathbf{\Theta}^{l}_{i,j}\mathbf{Q}^{T}\mathbf{u}^{l}_{i}\right)\;j=1,...,f_{l+1},$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'where $l$ is the layer, $\mathbf{u}^{l}_{j}\in\mathbb{R}^{N}$ is the $j^{th}$
    hidden representation (i.e., the signal) for the nodes in the $l^{th}$ layer,
    and $\mathbf{\Theta}^{l}_{i,j}$ are learnable filters. The idea behind Eq. ([7](#S4.E7
    "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) is similar to a conventional convolution:
    it passes the input signals through a set of learnable filters to aggregate the
    information, followed by some nonlinear transformation. By using the node features
    $\mathbf{F}^{V}$ as the input layer and stacking multiple convolutional layers,
    the overall architecture is similar to that of a CNN. Theoretical analysis has
    shown that such a definition of the graph convolution operation can mimic certain
    geometric properties of CNNs and we refer readers to [[7](#bib.bib7)] for a comprehensive
    survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, directly using Eq. ([7](#S4.E7 "In 4.1.1 Spectral Methods ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    requires learning $O(N)$ parameters, which may not be feasible in practice. Besides,
    the filters in the spectral domain may not be localized in the spatial domain,
    i.e., each node may be affected by all the other nodes rather than only the nodes
    in a small region. To alleviate these problems, Bruna et al. [[40](#bib.bib40)]
    suggested using the following smoothing filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $diag\left(\mathbf{\Theta}^{l}_{i,j}\right)=\mathcal{K}\;\alpha_{l,i,j},$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{K}$ is a fixed interpolation kernel and $\alpha_{l,i,j}$ are
    learnable interpolation coefficients. The authors also generalized this idea to
    the setting where the graph is not given but constructed from raw features using
    either a supervised or an unsupervised method [[41](#bib.bib41)].
  prefs: []
  type: TYPE_NORMAL
- en: However, two fundamental problems remain unsolved. First, because the full eigenvectors
    of the Laplacian matrix are needed during each calculation, the time complexity
    is at least $O(N^{2})$ for each forward and backward pass, not to mention the
    $O(N^{3})$ complexity required to calculate the eigendecomposition, meaning that
    this approach is not scalable to large-scale graphs. Second, because the filters
    depend on the eigenbasis $\mathbf{Q}$ of the graph, the parameters cannot be shared
    across multiple graphs with different sizes and structures.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we review two lines of works trying to solve these limitations and then
    unify them using some common frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 The Efficiency Aspect
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To solve the efficiency problem, ChebNet [[42](#bib.bib42)] was proposed to
    use a polynomial filter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\sum\nolimits_{k=0}^{K}\theta_{k}\mathbf{\Lambda}^{k},$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\theta_{0},...,\theta_{K}$ are the learnable parameters and $K$ is the
    polynomial order. Then, instead of performing the eigendecomposition, the authors
    rewrote Eq. ([9](#S4.E9 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) using
    the Chebyshev expansion [[76](#bib.bib76)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\sum\nolimits_{k=0}^{K}\theta_{k}\mathcal{T}_{k}(\tilde{\mathbf{\Lambda}}),$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\tilde{\mathbf{\Lambda}}=2\mathbf{\Lambda}/\lambda_{max}-\mathbf{I}$
    are the rescaled eigenvalues, $\lambda_{max}$ is the maximum eigenvalue, $\mathbf{I}\in\mathbb{R}^{N\times
    N}$ is the identity matrix, and $\mathcal{T}_{k}(x)$ is the Chebyshev polynomial
    of order $k$. The rescaling is necessary because of the orthonormal basis of Chebyshev
    polynomials. Using the fact that a polynomial of the Laplacian matrix acts as
    a polynomial of its eigenvalues, i.e., $\mathbf{L}^{k}=\mathbf{Q}\mathbf{\Lambda}^{k}\mathbf{Q}^{T}$,
    the filter operation in Eq. ([6](#S4.E6 "In 4.1.1 Spectral Methods ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    can be rewritten as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{u}^{\prime}=\mathbf{Q}\mathbf{\Theta}(\mathbf{\Lambda})\mathbf{Q}^{T}\mathbf{u}=$
    | $\displaystyle\sum\nolimits_{k=0}^{K}\theta_{k}\mathbf{Q}\mathcal{T}_{k}(\tilde{\mathbf{\Lambda}})\mathbf{Q}^{T}\mathbf{u}$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum\nolimits_{k=0}^{K}\theta_{k}\mathcal{T}_{k}(\tilde{\mathbf{L}})\mathbf{u}=\sum\nolimits_{k=0}^{K}\theta_{k}\bar{\mathbf{u}}_{k},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\bar{\mathbf{u}}_{k}=\mathcal{T}_{k}(\tilde{\mathbf{L}})\mathbf{u}$
    and $\tilde{\mathbf{L}}=2\mathbf{L}/\lambda_{max}-\mathbf{I}$. Using the recurrence
    relation of the Chebyshev polynomial $\mathcal{T}_{k}(x)=2x\mathcal{T}_{k-1}(x)-\mathcal{T}_{k-2}(x)$
    and $\mathcal{T}_{0}(x)=1,\mathcal{T}_{1}(x)=x$, $\bar{\mathbf{u}}_{k}$ can also
    be calculated recursively:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bar{\mathbf{u}}_{k}=2\tilde{\mathbf{L}}\bar{\mathbf{u}}_{k-1}-\bar{\mathbf{u}}_{k-2}$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 'with $\bar{\mathbf{u}}_{0}=\mathbf{u}$ and $\bar{\mathbf{u}}_{1}=\tilde{\mathbf{L}}\mathbf{u}$.
    Now, because only the matrix multiplication of a sparse matrix $\tilde{\mathbf{L}}$
    and some vectors need to be calculated, the time complexity becomes $O(KM)$ when
    using sparse matrix multiplication, where $M$ is the number of edges and $K$ is
    the polynomial order, i.e., the time complexity is linear with respect to the
    number of edges. It is also easy to see that such a polynomial filter is strictly
    $K$-localized: after one convolution, the representation of node $v_{i}$ will
    be affected only by its $K$-step neighborhoods $\mathcal{N}_{K}(i)$. Interestingly,
    this idea is used independently in network embedding to preserve the high-order
    proximity [[77](#bib.bib77)], of which we omit the details for brevity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kipf and Welling [[43](#bib.bib43)] further simplified the filtering by using
    only the first-order neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}^{l+1}_{i}=\rho\left(\sum_{j\in\tilde{\mathcal{N}}(i)}\frac{1}{\sqrt{\tilde{\mathbf{D}}(i,i)\tilde{\mathbf{D}}(j,j)}}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right),$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{h}^{l}_{i}\in\mathbb{R}^{f_{l}}$ is the hidden representation
    of node $v_{i}$ in the $l^{th}$ layer⁶⁶6We use a different letter because $\mathbf{h}^{l}\in\mathbb{R}^{f_{l}}$
    is the hidden representation of one node, while $\mathbf{u}^{l}\in\mathbb{R}^{N}$
    represents a dimension for all nodes., $\tilde{\mathbf{D}}=\mathbf{D}+\mathbf{I}$,
    and $\tilde{\mathcal{N}}(i)=\mathcal{N}(i)\cup\{i\}$. This can be written equivalently
    in an matrix form as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{H}^{l+1}=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}$, i.e., adding a self-connection.
    The authors showed that Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency Aspect ‣ 4.1
    Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs:
    A Survey")) is a special case of Eq. ([9](#S4.E9 "In 4.1.2 The Efficiency Aspect
    ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) by setting $K=1$ with a few minor changes. Then, the authors
    argued that stacking an adequate number of layers as illustrated in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey") has a modeling capacity
    similar to ChebNet but leads to better results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b9b235bfb30b41141764c3552d5dd636.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An illustrative example of the spatial convolution operation proposed
    by Kipf and Welling [[43](#bib.bib43)] (reprinted with permission). Nodes are
    affected only by their immediate neighbors in each convolutional layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An important insight of ChebNet and its extension is that they connect the
    spectral graph convolution with the spatial architecture. Specifically, they show
    that when the spectral convolution function is polynomial or first-order, the
    spectral graph convolution is equivalent to a spatial convolution. In addition,
    the convolution in Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    is highly similar to the state definition in a GNN in Eq. ([1](#S3.E1 "In 3.1
    Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on Graphs:
    A Survey")), except that the convolution definition replaces the recursive definition.
    From this aspect, a GNN can be regarded as a GCN with a large number of identical
    layers to reach stable states [[7](#bib.bib7)], i.e., a GNN uses a fixed function
    with fixed parameters to iteratively update the node hidden states until reaching
    an equilibrium, while a GCN has a preset number of layers and each layer contains
    different parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some spectral methods have also been proposed to solve the efficiency problem.
    For example, instead of using the Chebyshev expansion as in Eq. ([10](#S4.E10
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")), CayleyNet [[44](#bib.bib44)]
    adopted Cayley polynomials to define graph convolutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\theta_{0}+2Re\left\{\sum\nolimits_{k=1}^{K}\theta_{k}\left(\theta_{h}\mathbf{\Lambda}-i\mathbf{I}\right)^{k}\left(\theta_{h}\mathbf{\Lambda}+i\mathbf{I}\right)^{k}\right\},$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: 'where $i=\sqrt{-1}$ denotes the imaginary unit and $\theta_{h}$ is another
    spectral zoom parameter. In addition to showing that CayleyNet is as efficient
    as ChebNet, the authors demonstrated that the Cayley polynomials can detect “narrow
    frequency bands of importance” to achieve better results. Graph wavelet neural
    network (GWNN) [[45](#bib.bib45)] was further proposed to replace the Fourier
    transform in spectral filters by the graph wavelet transform by rewriting Eq. ([5](#S4.E5
    "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{u}_{1}*_{G}\mathbf{u}_{2}=\mathbf{\psi}\left(\left(\mathbf{\psi}^{-1}\mathbf{u}_{1}\right)\odot\left(\mathbf{\psi}^{-1}\mathbf{u}_{2}\right)\right),$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{\psi}$ denotes the graph wavelet bases. By using fast approximating
    algorithms to calculate $\mathbf{\psi}$ and $\mathbf{\psi}^{-1}$, GWNN’s computational
    complexity is also $O(KM)$, i.e., linear with respect to the number of edges.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 The Aspect of Multiple Graphs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A parallel series of works has focuses on generalizing graph convolutions to
    multiple graphs of arbitrary sizes. Neural FPs [[46](#bib.bib46)] proposed a spatial
    method that also used the first-order neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}^{l+1}_{i}=\sigma\left(\sum\nolimits_{j\in\hat{\mathcal{N}}(i)}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right).$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: 'Because the parameters $\mathbf{\Theta}$ can be shared across different graphs
    and are independent of the graph size, Neural FPs can handle multiple graphs of
    arbitrary sizes. Note that Eq. ([17](#S4.E17 "In 4.1.3 The Aspect of Multiple
    Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) is very similar to Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency
    Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")). However, instead of considering the influence of node
    degree by adding a normalization term, Neural FPs proposed learning different
    parameters $\mathbf{\Theta}$ for nodes with different degrees. This strategy performed
    well for small graphs such as molecular graphs (i.e., atoms as nodes and bonds
    as edges), but may not be scalable to larger graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: PATCHY-SAN [[47](#bib.bib47)] adopted a different idea. It assigned a unique
    node order using a graph labeling procedure such as the Weisfeiler-Lehman kernel [[78](#bib.bib78)]
    and then arranged node neighbors in a line using this pre-defined order. In addition,
    PATCHY-SAN defined a “receptive field” for each node $v_{i}$ by selecting a fixed
    number of nodes from its $k$-step neighborhoods $\mathcal{N}_{k}(i)$. Then a standard
    1-D CNN with proper normalization was adopted. Using this approach, nodes in different
    graphs all have a “receptive field” with a fixed size and order; thus, PATCHY-SAN
    can learn from multiple graphs like normal CNNs learn from multiple images. The
    drawbacks are that the convolution depends heavily on the graph labeling procedure
    which is a preprocessing step that is not learned. LGCN [[48](#bib.bib48)] further
    proposed to simplify the sorting process by using a lexicographical order (i.e.,
    sorting neighbors based on their hidden representation in the final layer $\mathbf{H}^{L}$).
    Instead of using a single order, the authors sorted different channels of $\mathbf{H}^{L}$
    separately. SortPooling [[49](#bib.bib49)] took a similar approach, but rather
    than sorting the neighbors of each node, the authors proposed to sort all the
    nodes (i.e., using a single order for all the neighborhoods). Despite the differences
    among these methods, enforcing a 1-D node order may not be a natural choice for
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'DCNN [[50](#bib.bib50)] adopted another approach by replacing the eigenbasis
    of the graph convolution with a diffusion-basis, i.e., the neighborhoods of nodes
    were determined by the diffusion transition probability between nodes. Specifically,
    the convolution was defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{H}^{l+1}=\rho\left(\mathbf{P}^{K}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{P}^{K}=\left(\mathbf{P}\right)^{K}$ is the transition probability
    of a length-$K$ diffusion process (i.e., random walks), $K$ is a preset diffusion
    length, and $\mathbf{\Theta}^{l}$ are learnable parameters. Because only $\mathbf{P}^{K}$
    depends on the graph structure, the parameters $\mathbf{\Theta}^{l}$ can be shared
    across graphs of arbitrary sizes. However, calculating $\mathbf{P}^{K}$ has a
    time complexity of $O\left(N^{2}K\right)$; thus, this method is not scalable to
    large graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'DGCN [[51](#bib.bib51)] was further proposed to jointly adopt the diffusion
    and the adjacency bases using a dual graph convolutional network. Specifically,
    DGCN used two convolutions: one was Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency
    Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")), and the other replaced the adjacency matrix with the positive
    pointwise mutual information (PPMI) matrix [[79](#bib.bib79)] of the transition
    probability as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{Z}^{l+1}=\rho\left(\mathbf{D}^{-\frac{1}{2}}_{P}\mathbf{X}_{P}\mathbf{D}^{-\frac{1}{2}}_{P}\mathbf{Z}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{X}_{P}$ is the PPMI matrix calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{X}_{P}(i,j)=\max\left(\log\left(\frac{\mathbf{P}(i,j)\sum_{i,j}\mathbf{P}(i,j)}{\sum_{i}\mathbf{P}(i,j)\sum_{j}\mathbf{P}(i,j)}\right),0\right),$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: and $\mathbf{D}_{P}(i,i)=\sum_{j}\mathbf{X}_{P}(i,j)$ is the diagonal degree
    matrix of $\mathbf{X}_{P}$. Then, these two convolutions were ensembled by minimizing
    the mean square differences between $\mathbf{H}$ and $\mathbf{Z}$. DGCN adopted
    a random walk sampling technique to accelerate the transition probability calculation.
    The experiments demonstrated that such dual convolutions were effective even for
    single-graph problems.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the above two lines of works, MPNNs [[52](#bib.bib52)] were proposed
    as a unified framework for the graph convolution operation in the spatial domain
    using message-passing functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\mathbf{m}_{i}^{l+1}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}^{l}\left(\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{F}^{E}_{i,j}\right)\\
    \mathbf{h}_{i}^{l+1}=\mathcal{G}^{l}\left(\mathbf{h}_{i}^{l},\mathbf{m}_{i}^{l+1}\right),\end{gathered}$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{F}^{l}(\cdot)$ and $\mathcal{G}^{l}(\cdot)$ are the message
    functions and vertex update functions to be learned, respectively, and $\mathbf{m}^{l}$
    denotes the “messages” passed between nodes. Conceptually, MPNNs are a framework
    in which each node sends messages based on its states and updates its states based
    on messages received from the immediate neighbors. The authors showed that the
    above framework had included many existing methods such as GGS-NNs [[24](#bib.bib24)],
    Bruna et al. [[40](#bib.bib40)], Henaff et al. [[41](#bib.bib41)], Neural FPs [[46](#bib.bib46)],
    Kipf and Welling [[43](#bib.bib43)] and Kearnes et al. [[55](#bib.bib55)] as special
    cases. In addition, the authors proposed adding a “master” node that was connected
    to all the nodes to accelerate the message-passing across long distances, and
    they split the hidden representations into different “towers” to improve the generalization
    ability. The authors showed that a specific variant of MPNNs could achieve state-of-the-art
    performance in predicting molecular properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrently, GraphSAGE [[53](#bib.bib53)] took a similar idea as Eq. ([21](#S4.E21
    "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")) using multiple aggregating functions as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\mathbf{m}_{i}^{l+1}=\text{AGGREGATE}^{l}(\{\mathbf{h}_{j}^{l},\forall
    j\in\mathcal{N}(i)\})\\ \mathbf{h}_{i}^{l+1}=\rho\left(\mathbf{\Theta}^{l}\left[\mathbf{h}_{i}^{l},\mathbf{m}_{i}^{l+1}\right]\right),\end{gathered}$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\left[\cdot,\cdot\right]$ is the concatenation operation and $\text{AGGREGATE}(\cdot)$
    represents the aggregating function. The authors suggested three aggregating functions:
    the element-wise mean, an LSTM, and max-pooling as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{AGGREGATE}^{l}=\max\{\rho(\mathbf{\Theta}_{\text{pool}}\mathbf{h}_{j}^{l}+\mathbf{b}_{\text{pool}}),\forall
    j\in\mathcal{N}(i)\},$ |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{\Theta}_{\text{pool}}$ and $\mathbf{b}_{\text{pool}}$ are the
    parameters to be learned and $\max\left\{\cdot\right\}$ is the element-wise maximum.
    For the LSTM aggregating function, because an neighbors order is needed, the authors
    adopted a simple random order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mixture model network (MoNet) [[54](#bib.bib54)] also tried to unify the existing
    GCN models as well as CNNs for manifolds into a common framework using “template
    matching”:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h^{l+1}_{ik}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}^{l}_{k}(\mathbf{u}(i,j))\mathbf{h}^{l}_{j},k=1,...,f_{l+1},$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{u}(i,j)$ are the pseudo-coordinates of the node pair $(v_{i},v_{j})$,
    $\mathcal{F}^{l}_{k}(\mathbf{u})$ is a parametric function to be learned, and
    $h^{l}_{ik}$ is the $k^{th}$ dimension of $\mathbf{h}^{l}_{i}$. In other words,
    $\mathcal{F}^{l}_{k}(\mathbf{u})$ served as a weighting kernel for combining neighborhoods.
    Then, MoNet adopted the following Gaussian kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{F}^{l}_{k}(\mathbf{u})=\exp\left(-\frac{1}{2}(\mathbf{u}-\bm{\mu}^{l}_{k})^{T}(\mathbf{\Sigma}^{l}_{k})^{-1}(\mathbf{u}-\bm{\mu}^{l}_{k})\right),$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{\mu}^{l}_{k}$ and $\mathbf{\Sigma}^{l}_{k}$ are the mean vectors
    and diagonal covariance matrices to be learned, respectively. The pseudo-coordinates
    were degrees as in Kipf and Welling [[43](#bib.bib43)], i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{u}(i,j)=(\frac{1}{\sqrt{\mathbf{D}(i,i)}},\frac{1}{\sqrt{\mathbf{D}(j,j)}}).$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: 'Graph networks (GNs) [[9](#bib.bib9)] proposed a more general framework for
    both GCNs and GNNs that learned three sets of representations: $\mathbf{h}_{i}^{l},\mathbf{e}_{ij}^{l}$,
    and $\mathbf{z}^{l}$ as the representation for nodes, edges, and the entire graph,
    respectively. These representations were learned using three aggregation and three
    updating functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S4.E27.m1.131" class="ltx_Math" alttext="\begin{gathered}\mathbf{m}_{i}^{l}=\mathcal{G}^{E\rightarrow
    V}(\{\mathbf{e}_{ij}^{l},\forall j\in\mathcal{N}(i)\}),\mathbf{m}_{V}^{l}=\mathcal{G}^{V\rightarrow
    G}(\{\mathbf{h}_{i}^{l},\forall v_{i}\in V\})\\ \mathbf{m}_{E}^{l}=\mathcal{G}^{E\rightarrow
    G}(\{\mathbf{e}_{ij}^{l},\forall(v_{i},v_{j})\in E\}),\mathbf{h}_{i}^{l+1}=\mathcal{F}^{V}(\mathbf{m}_{i}^{l},\mathbf{h}_{i}^{l},\mathbf{z}^{l})\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{e}_{ij}^{l+1}=\mathcal{F}^{E}(\mathbf{e}_{ij}^{l},\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{z}^{l}),\mathbf{z}^{l+1}=\mathcal{F}^{G}(\mathbf{m}_{E}^{l},\mathbf{m}_{V}^{l},\mathbf{z}^{l}),\end{gathered}"
    display="block"><semantics id="S4.E27.m1.131a"><mtable displaystyle="true" rowspacing="0pt"
    id="S4.E27.m1.131.131.6"><mtr id="S4.E27.m1.131.131.6a"><mtd id="S4.E27.m1.131.131.6b"><mrow
    id="S4.E27.m1.128.128.3.127.43.43.43"><mrow id="S4.E27.m1.127.127.2.126.42.42.42.1"><msubsup
    id="S4.E27.m1.127.127.2.126.42.42.42.1.2"><mi id="S4.E27.m1.1.1.1.1.1.1" xref="S4.E27.m1.1.1.1.1.1.1.cmml">𝐦</mi><mi
    id="S4.E27.m1.2.2.2.2.2.2.1" xref="S4.E27.m1.2.2.2.2.2.2.1.cmml">i</mi><mi id="S4.E27.m1.3.3.3.3.3.3.1"
    xref="S4.E27.m1.3.3.3.3.3.3.1.cmml">l</mi></msubsup><mo id="S4.E27.m1.4.4.4.4.4.4"
    xref="S4.E27.m1.4.4.4.4.4.4.cmml">=</mo><mrow id="S4.E27.m1.127.127.2.126.42.42.42.1.1"><msup
    id="S4.E27.m1.127.127.2.126.42.42.42.1.1.3"><mi class="ltx_font_mathcaligraphic"
    id="S4.E27.m1.5.5.5.5.5.5" xref="S4.E27.m1.5.5.5.5.5.5.cmml">𝒢</mi><mrow id="S4.E27.m1.6.6.6.6.6.6.1"
    xref="S4.E27.m1.6.6.6.6.6.6.1.cmml"><mi id="S4.E27.m1.6.6.6.6.6.6.1.2" xref="S4.E27.m1.6.6.6.6.6.6.1.2.cmml">E</mi><mo
    stretchy="false" id="S4.E27.m1.6.6.6.6.6.6.1.1" xref="S4.E27.m1.6.6.6.6.6.6.1.1.cmml">→</mo><mi
    id="S4.E27.m1.6.6.6.6.6.6.1.3" xref="S4.E27.m1.6.6.6.6.6.6.1.3.cmml">V</mi></mrow></msup><mo
    lspace="0em" rspace="0em" id="S4.E27.m1.127.127.2.126.42.42.42.1.1.2">​</mo><mrow
    id="S4.E27.m1.127.127.2.126.42.42.42.1.1.1.1"><mo stretchy="false" id="S4.E27.m1.7.7.7.7.7.7">(</mo><mrow
    id="S4.E27.m1.127.127.2.126.42.42.42.1.1.1.1.1"><mo stretchy="false" id="S4.E27.m1.8.8.8.8.8.8">{</mo><mrow
    id="S4.E27.m1.127.127.2.126.42.42.42.1.1.1.1.1.1.1"><mrow id="S4.E27.m1.127.127.2.126.42.42.42.1.1.1.1.1.1.1.2.2"><msubsup
    id="S4.E27.m1.127.127.2.126.42.42.42.1.1.1.1.1.1.1.1.1.1"><mi id="S4.E27.m1.9.9.9.9.9.9"
    xref="S4.E27.m1.9.9.9.9.9.9.cmml">𝐞</mi><mrow id="S4.E27.m1.10.10.10.10.10.10.1"
    xref="S4.E27.m1.10.10.10.10.10.10.1.cmml"><mi id="S4.E27.m1.10.10.10.10.10.10.1.2"
    xref="S4.E27.m1.10.10.10.10.10.10.1.2.cmml">i</mi><mo lspace="0em" rspace="0em"
    id="S4.E27.m1.10.10.10.10.10.10.1.1" xref="S4.E27.m1.10.10.10.10.10.10.1.1.cmml">​</mo><mi
    id="S4.E27.m1.10.10.10.10.10.10.1.3" xref="S4.E27.m1.10.10.10.10.10.10.1.3.cmml">j</mi></mrow><mi
    id="S4.E27.m1.11.11.11.11.11.11.1" xref="S4.E27.m1.11.11.11.11.11.11.1.cmml">l</mi></msubsup><mo
    id="S4.E27.m1.12.12.12.12.12.12">,</mo><mrow id="S4.E27.m1.127.127.2.126.42.42.42.1.1.1.1.1.1.1.2.2.2"><mo
    rspace="0.167em" id="S4.E27.m1.13.13.13.13.13.13" xref="S4.E27.m1.13.13.13.13.13.13.cmml">∀</mo><mi
    id="S4.E27.m1.14.14.14.14.14.14" xref="S4.E27.m1.14.14.14.14.14.14.cmml">j</mi></mrow></mrow><mo
    id="S4.E27.m1.15.15.15.15.15.15" xref="S4.E27.m1.15.15.15.15.15.15.cmml">∈</mo><mrow
    id="S4.E27.m1.127.127.2.126.42.42.42.1.1.1.1.1.1.1.3"><mi class="ltx_font_mathcaligraphic"
    id="S4.E27.m1.16.16.16.16.16.16" xref="S4.E27.m1.16.16.16.16.16.16.cmml">𝒩</mi><mo
    lspace="0em" rspace="0em" id="S4.E27.m1.127.127.2.126.42.42.42.1.1.1.1.1.1.1.3.1">​</mo><mrow
    id="S4.E27.m1.127.127.2.126.42.42.42.1.1.1.1.1.1.1.3.2"><mo stretchy="false" id="S4.E27.m1.17.17.17.17.17.17">(</mo><mi
    id="S4.E27.m1.18.18.18.18.18.18" xref="S4.E27.m1.18.18.18.18.18.18.cmml">i</mi><mo
    stretchy="false" id="S4.E27.m1.19.19.19.19.19.19">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S4.E27.m1.20.20.20.20.20.20">}</mo></mrow><mo stretchy="false"
    id="S4.E27.m1.21.21.21.21.21.21">)</mo></mrow></mrow></mrow><mo id="S4.E27.m1.22.22.22.22.22.22">,</mo><mrow
    id="S4.E27.m1.128.128.3.127.43.43.43.2"><msubsup id="S4.E27.m1.128.128.3.127.43.43.43.2.2"><mi
    id="S4.E27.m1.23.23.23.23.23.23" xref="S4.E27.m1.23.23.23.23.23.23.cmml">𝐦</mi><mi
    id="S4.E27.m1.24.24.24.24.24.24.1" xref="S4.E27.m1.24.24.24.24.24.24.1.cmml">V</mi><mi
    id="S4.E27.m1.25.25.25.25.25.25.1" xref="S4.E27.m1.25.25.25.25.25.25.1.cmml">l</mi></msubsup><mo
    id="S4.E27.m1.26.26.26.26.26.26" xref="S4.E27.m1.26.26.26.26.26.26.cmml">=</mo><mrow
    id="S4.E27.m1.128.128.3.127.43.43.43.2.1"><msup id="S4.E27.m1.128.128.3.127.43.43.43.2.1.3"><mi
    class="ltx_font_mathcaligraphic" id="S4.E27.m1.27.27.27.27.27.27" xref="S4.E27.m1.27.27.27.27.27.27.cmml">𝒢</mi><mrow
    id="S4.E27.m1.28.28.28.28.28.28.1" xref="S4.E27.m1.28.28.28.28.28.28.1.cmml"><mi
    id="S4.E27.m1.28.28.28.28.28.28.1.2" xref="S4.E27.m1.28.28.28.28.28.28.1.2.cmml">V</mi><mo
    stretchy="false" id="S4.E27.m1.28.28.28.28.28.28.1.1" xref="S4.E27.m1.28.28.28.28.28.28.1.1.cmml">→</mo><mi
    id="S4.E27.m1.28.28.28.28.28.28.1.3" xref="S4.E27.m1.28.28.28.28.28.28.1.3.cmml">G</mi></mrow></msup><mo
    lspace="0em" rspace="0em" id="S4.E27.m1.128.128.3.127.43.43.43.2.1.2">​</mo><mrow
    id="S4.E27.m1.128.128.3.127.43.43.43.2.1.1.1"><mo stretchy="false" id="S4.E27.m1.29.29.29.29.29.29">(</mo><mrow
    id="S4.E27.m1.128.128.3.127.43.43.43.2.1.1.1.1"><mo stretchy="false" id="S4.E27.m1.30.30.30.30.30.30">{</mo><mrow
    id="S4.E27.m1.128.128.3.127.43.43.43.2.1.1.1.1.1.1"><mrow id="S4.E27.m1.128.128.3.127.43.43.43.2.1.1.1.1.1.1.2.2"><msubsup
    id="S4.E27.m1.128.128.3.127.43.43.43.2.1.1.1.1.1.1.1.1.1"><mi id="S4.E27.m1.31.31.31.31.31.31"
    xref="S4.E27.m1.31.31.31.31.31.31.cmml">𝐡</mi><mi id="S4.E27.m1.32.32.32.32.32.32.1"
    xref="S4.E27.m1.32.32.32.32.32.32.1.cmml">i</mi><mi id="S4.E27.m1.33.33.33.33.33.33.1"
    xref="S4.E27.m1.33.33.33.33.33.33.1.cmml">l</mi></msubsup><mo id="S4.E27.m1.34.34.34.34.34.34">,</mo><mrow
    id="S4.E27.m1.128.128.3.127.43.43.43.2.1.1.1.1.1.1.2.2.2"><mo rspace="0.167em"
    id="S4.E27.m1.35.35.35.35.35.35" xref="S4.E27.m1.35.35.35.35.35.35.cmml">∀</mo><msub
    id="S4.E27.m1.128.128.3.127.43.43.43.2.1.1.1.1.1.1.2.2.2.1"><mi id="S4.E27.m1.36.36.36.36.36.36"
    xref="S4.E27.m1.36.36.36.36.36.36.cmml">v</mi><mi id="S4.E27.m1.37.37.37.37.37.37.1"
    xref="S4.E27.m1.37.37.37.37.37.37.1.cmml">i</mi></msub></mrow></mrow><mo id="S4.E27.m1.38.38.38.38.38.38"
    xref="S4.E27.m1.38.38.38.38.38.38.cmml">∈</mo><mi id="S4.E27.m1.39.39.39.39.39.39"
    xref="S4.E27.m1.39.39.39.39.39.39.cmml">V</mi></mrow><mo stretchy="false" id="S4.E27.m1.40.40.40.40.40.40">}</mo></mrow><mo
    stretchy="false" id="S4.E27.m1.41.41.41.41.41.41">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    id="S4.E27.m1.131.131.6c"><mtd id="S4.E27.m1.131.131.6d"><mrow id="S4.E27.m1.130.130.5.129.45.45.45"><mrow
    id="S4.E27.m1.129.129.4.128.44.44.44.1"><msubsup id="S4.E27.m1.129.129.4.128.44.44.44.1.2"><mi
    id="S4.E27.m1.42.42.42.1.1.1" xref="S4.E27.m1.42.42.42.1.1.1.cmml">𝐦</mi><mi id="S4.E27.m1.43.43.43.2.2.2.1"
    xref="S4.E27.m1.43.43.43.2.2.2.1.cmml">E</mi><mi id="S4.E27.m1.44.44.44.3.3.3.1"
    xref="S4.E27.m1.44.44.44.3.3.3.1.cmml">l</mi></msubsup><mo id="S4.E27.m1.45.45.45.4.4.4"
    xref="S4.E27.m1.45.45.45.4.4.4.cmml">=</mo><mrow id="S4.E27.m1.129.129.4.128.44.44.44.1.1"><msup
    id="S4.E27.m1.129.129.4.128.44.44.44.1.1.3"><mi class="ltx_font_mathcaligraphic"
    id="S4.E27.m1.46.46.46.5.5.5" xref="S4.E27.m1.46.46.46.5.5.5.cmml">𝒢</mi><mrow
    id="S4.E27.m1.47.47.47.6.6.6.1" xref="S4.E27.m1.47.47.47.6.6.6.1.cmml"><mi id="S4.E27.m1.47.47.47.6.6.6.1.2"
    xref="S4.E27.m1.47.47.47.6.6.6.1.2.cmml">E</mi><mo stretchy="false" id="S4.E27.m1.47.47.47.6.6.6.1.1"
    xref="S4.E27.m1.47.47.47.6.6.6.1.1.cmml">→</mo><mi id="S4.E27.m1.47.47.47.6.6.6.1.3"
    xref="S4.E27.m1.47.47.47.6.6.6.1.3.cmml">G</mi></mrow></msup><mo lspace="0em"
    rspace="0em" id="S4.E27.m1.129.129.4.128.44.44.44.1.1.2">​</mo><mrow id="S4.E27.m1.129.129.4.128.44.44.44.1.1.1.1"><mo
    stretchy="false" id="S4.E27.m1.48.48.48.7.7.7">(</mo><mrow id="S4.E27.m1.129.129.4.128.44.44.44.1.1.1.1.1"><mo
    stretchy="false" id="S4.E27.m1.49.49.49.8.8.8">{</mo><mrow id="S4.E27.m1.129.129.4.128.44.44.44.1.1.1.1.1.1.1"><mrow
    id="S4.E27.m1.129.129.4.128.44.44.44.1.1.1.1.1.1.1.2.2"><msubsup id="S4.E27.m1.129.129.4.128.44.44.44.1.1.1.1.1.1.1.1.1.1"><mi
    id="S4.E27.m1.50.50.50.9.9.9" xref="S4.E27.m1.50.50.50.9.9.9.cmml">𝐞</mi><mrow
    id="S4.E27.m1.51.51.51.10.10.10.1" xref="S4.E27.m1.51.51.51.10.10.10.1.cmml"><mi
    id="S4.E27.m1.51.51.51.10.10.10.1.2" xref="S4.E27.m1.51.51.51.10.10.10.1.2.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S4.E27.m1.51.51.51.10.10.10.1.1" xref="S4.E27.m1.51.51.51.10.10.10.1.1.cmml">​</mo><mi
    id="S4.E27.m1.51.51.51.10.10.10.1.3" xref="S4.E27.m1.51.51.51.10.10.10.1.3.cmml">j</mi></mrow><mi
    id="S4.E27.m1.52.52.52.11.11.11.1" xref="S4.E27.m1.52.52.52.11.11.11.1.cmml">l</mi></msubsup><mo
    id="S4.E27.m1.53.53.53.12.12.12">,</mo><mrow id="S4.E27.m1.129.129.4.128.44.44.44.1.1.1.1.1.1.1.2.2.2"><mo
    id="S4.E27.m1.54.54.54.13.13.13" xref="S4.E27.m1.54.54.54.13.13.13.cmml">∀</mo><mrow
    id="S4.E27.m1.129.129.4.128.44.44.44.1.1.1.1.1.1.1.2.2.2.2.2"><mo stretchy="false"
    id="S4.E27.m1.55.55.55.14.14.14">(</mo><msub id="S4.E27.m1.129.129.4.128.44.44.44.1.1.1.1.1.1.1.2.2.2.1.1.1"><mi
    id="S4.E27.m1.56.56.56.15.15.15" xref="S4.E27.m1.56.56.56.15.15.15.cmml">v</mi><mi
    id="S4.E27.m1.57.57.57.16.16.16.1" xref="S4.E27.m1.57.57.57.16.16.16.1.cmml">i</mi></msub><mo
    id="S4.E27.m1.58.58.58.17.17.17">,</mo><msub id="S4.E27.m1.129.129.4.128.44.44.44.1.1.1.1.1.1.1.2.2.2.2.2.2"><mi
    id="S4.E27.m1.59.59.59.18.18.18" xref="S4.E27.m1.59.59.59.18.18.18.cmml">v</mi><mi
    id="S4.E27.m1.60.60.60.19.19.19.1" xref="S4.E27.m1.60.60.60.19.19.19.1.cmml">j</mi></msub><mo
    stretchy="false" id="S4.E27.m1.61.61.61.20.20.20">)</mo></mrow></mrow></mrow><mo
    id="S4.E27.m1.62.62.62.21.21.21" xref="S4.E27.m1.62.62.62.21.21.21.cmml">∈</mo><mi
    id="S4.E27.m1.63.63.63.22.22.22" xref="S4.E27.m1.63.63.63.22.22.22.cmml">E</mi></mrow><mo
    stretchy="false" id="S4.E27.m1.64.64.64.23.23.23">}</mo></mrow><mo stretchy="false"
    id="S4.E27.m1.65.65.65.24.24.24">)</mo></mrow></mrow></mrow><mo id="S4.E27.m1.66.66.66.25.25.25">,</mo><mrow
    id="S4.E27.m1.130.130.5.129.45.45.45.2"><msubsup id="S4.E27.m1.130.130.5.129.45.45.45.2.4"><mi
    id="S4.E27.m1.67.67.67.26.26.26" xref="S4.E27.m1.67.67.67.26.26.26.cmml">𝐡</mi><mi
    id="S4.E27.m1.68.68.68.27.27.27.1" xref="S4.E27.m1.68.68.68.27.27.27.1.cmml">i</mi><mrow
    id="S4.E27.m1.69.69.69.28.28.28.1" xref="S4.E27.m1.69.69.69.28.28.28.1.cmml"><mi
    id="S4.E27.m1.69.69.69.28.28.28.1.2" xref="S4.E27.m1.69.69.69.28.28.28.1.2.cmml">l</mi><mo
    id="S4.E27.m1.69.69.69.28.28.28.1.1" xref="S4.E27.m1.69.69.69.28.28.28.1.1.cmml">+</mo><mn
    id="S4.E27.m1.69.69.69.28.28.28.1.3" xref="S4.E27.m1.69.69.69.28.28.28.1.3.cmml">1</mn></mrow></msubsup><mo
    id="S4.E27.m1.70.70.70.29.29.29" xref="S4.E27.m1.70.70.70.29.29.29.cmml">=</mo><mrow
    id="S4.E27.m1.130.130.5.129.45.45.45.2.3"><msup id="S4.E27.m1.130.130.5.129.45.45.45.2.3.5"><mi
    class="ltx_font_mathcaligraphic" id="S4.E27.m1.71.71.71.30.30.30" xref="S4.E27.m1.71.71.71.30.30.30.cmml">ℱ</mi><mi
    id="S4.E27.m1.72.72.72.31.31.31.1" xref="S4.E27.m1.72.72.72.31.31.31.1.cmml">V</mi></msup><mo
    lspace="0em" rspace="0em" id="S4.E27.m1.130.130.5.129.45.45.45.2.3.4">​</mo><mrow
    id="S4.E27.m1.130.130.5.129.45.45.45.2.3.3.3"><mo stretchy="false" id="S4.E27.m1.73.73.73.32.32.32">(</mo><msubsup
    id="S4.E27.m1.130.130.5.129.45.45.45.2.1.1.1.1"><mi id="S4.E27.m1.74.74.74.33.33.33"
    xref="S4.E27.m1.74.74.74.33.33.33.cmml">𝐦</mi><mi id="S4.E27.m1.75.75.75.34.34.34.1"
    xref="S4.E27.m1.75.75.75.34.34.34.1.cmml">i</mi><mi id="S4.E27.m1.76.76.76.35.35.35.1"
    xref="S4.E27.m1.76.76.76.35.35.35.1.cmml">l</mi></msubsup><mo id="S4.E27.m1.77.77.77.36.36.36">,</mo><msubsup
    id="S4.E27.m1.130.130.5.129.45.45.45.2.2.2.2.2"><mi id="S4.E27.m1.78.78.78.37.37.37"
    xref="S4.E27.m1.78.78.78.37.37.37.cmml">𝐡</mi><mi id="S4.E27.m1.79.79.79.38.38.38.1"
    xref="S4.E27.m1.79.79.79.38.38.38.1.cmml">i</mi><mi id="S4.E27.m1.80.80.80.39.39.39.1"
    xref="S4.E27.m1.80.80.80.39.39.39.1.cmml">l</mi></msubsup><mo id="S4.E27.m1.81.81.81.40.40.40">,</mo><msup
    id="S4.E27.m1.130.130.5.129.45.45.45.2.3.3.3.3"><mi id="S4.E27.m1.82.82.82.41.41.41"
    xref="S4.E27.m1.82.82.82.41.41.41.cmml">𝐳</mi><mi id="S4.E27.m1.83.83.83.42.42.42.1"
    xref="S4.E27.m1.83.83.83.42.42.42.1.cmml">l</mi></msup><mo stretchy="false" id="S4.E27.m1.84.84.84.43.43.43">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    id="S4.E27.m1.131.131.6e"><mtd id="S4.E27.m1.131.131.6f"><mrow id="S4.E27.m1.131.131.6.130.42.42.42"><mrow
    id="S4.E27.m1.131.131.6.130.42.42.42.1"><mrow id="S4.E27.m1.131.131.6.130.42.42.42.1.1.1"><msubsup
    id="S4.E27.m1.131.131.6.130.42.42.42.1.1.1.5"><mi id="S4.E27.m1.85.85.85.1.1.1"
    xref="S4.E27.m1.85.85.85.1.1.1.cmml">𝐞</mi><mrow id="S4.E27.m1.86.86.86.2.2.2.1"
    xref="S4.E27.m1.86.86.86.2.2.2.1.cmml"><mi id="S4.E27.m1.86.86.86.2.2.2.1.2" xref="S4.E27.m1.86.86.86.2.2.2.1.2.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S4.E27.m1.86.86.86.2.2.2.1.1" xref="S4.E27.m1.86.86.86.2.2.2.1.1.cmml">​</mo><mi
    id="S4.E27.m1.86.86.86.2.2.2.1.3" xref="S4.E27.m1.86.86.86.2.2.2.1.3.cmml">j</mi></mrow><mrow
    id="S4.E27.m1.87.87.87.3.3.3.1" xref="S4.E27.m1.87.87.87.3.3.3.1.cmml"><mi id="S4.E27.m1.87.87.87.3.3.3.1.2"
    xref="S4.E27.m1.87.87.87.3.3.3.1.2.cmml">l</mi><mo id="S4.E27.m1.87.87.87.3.3.3.1.1"
    xref="S4.E27.m1.87.87.87.3.3.3.1.1.cmml">+</mo><mn id="S4.E27.m1.87.87.87.3.3.3.1.3"
    xref="S4.E27.m1.87.87.87.3.3.3.1.3.cmml">1</mn></mrow></msubsup><mo id="S4.E27.m1.88.88.88.4.4.4"
    xref="S4.E27.m1.88.88.88.4.4.4.cmml">=</mo><mrow id="S4.E27.m1.131.131.6.130.42.42.42.1.1.1.4"><msup
    id="S4.E27.m1.131.131.6.130.42.42.42.1.1.1.4.6"><mi class="ltx_font_mathcaligraphic"
    id="S4.E27.m1.89.89.89.5.5.5" xref="S4.E27.m1.89.89.89.5.5.5.cmml">ℱ</mi><mi id="S4.E27.m1.90.90.90.6.6.6.1"
    xref="S4.E27.m1.90.90.90.6.6.6.1.cmml">E</mi></msup><mo lspace="0em" rspace="0em"
    id="S4.E27.m1.131.131.6.130.42.42.42.1.1.1.4.5">​</mo><mrow id="S4.E27.m1.131.131.6.130.42.42.42.1.1.1.4.4.4"><mo
    stretchy="false" id="S4.E27.m1.91.91.91.7.7.7">(</mo><msubsup id="S4.E27.m1.131.131.6.130.42.42.42.1.1.1.1.1.1.1"><mi
    id="S4.E27.m1.92.92.92.8.8.8" xref="S4.E27.m1.92.92.92.8.8.8.cmml">𝐞</mi><mrow
    id="S4.E27.m1.93.93.93.9.9.9.1" xref="S4.E27.m1.93.93.93.9.9.9.1.cmml"><mi id="S4.E27.m1.93.93.93.9.9.9.1.2"
    xref="S4.E27.m1.93.93.93.9.9.9.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E27.m1.93.93.93.9.9.9.1.1"
    xref="S4.E27.m1.93.93.93.9.9.9.1.1.cmml">​</mo><mi id="S4.E27.m1.93.93.93.9.9.9.1.3"
    xref="S4.E27.m1.93.93.93.9.9.9.1.3.cmml">j</mi></mrow><mi id="S4.E27.m1.94.94.94.10.10.10.1"
    xref="S4.E27.m1.94.94.94.10.10.10.1.cmml">l</mi></msubsup><mo id="S4.E27.m1.95.95.95.11.11.11">,</mo><msubsup
    id="S4.E27.m1.131.131.6.130.42.42.42.1.1.1.2.2.2.2"><mi id="S4.E27.m1.96.96.96.12.12.12"
    xref="S4.E27.m1.96.96.96.12.12.12.cmml">𝐡</mi><mi id="S4.E27.m1.97.97.97.13.13.13.1"
    xref="S4.E27.m1.97.97.97.13.13.13.1.cmml">i</mi><mi id="S4.E27.m1.98.98.98.14.14.14.1"
    xref="S4.E27.m1.98.98.98.14.14.14.1.cmml">l</mi></msubsup><mo id="S4.E27.m1.99.99.99.15.15.15">,</mo><msubsup
    id="S4.E27.m1.131.131.6.130.42.42.42.1.1.1.3.3.3.3"><mi id="S4.E27.m1.100.100.100.16.16.16"
    xref="S4.E27.m1.100.100.100.16.16.16.cmml">𝐡</mi><mi id="S4.E27.m1.101.101.101.17.17.17.1"
    xref="S4.E27.m1.101.101.101.17.17.17.1.cmml">j</mi><mi id="S4.E27.m1.102.102.102.18.18.18.1"
    xref="S4.E27.m1.102.102.102.18.18.18.1.cmml">l</mi></msubsup><mo id="S4.E27.m1.103.103.103.19.19.19">,</mo><msup
    id="S4.E27.m1.131.131.6.130.42.42.42.1.1.1.4.4.4.4"><mi id="S4.E27.m1.104.104.104.20.20.20"
    xref="S4.E27.m1.104.104.104.20.20.20.cmml">𝐳</mi><mi id="S4.E27.m1.105.105.105.21.21.21.1"
    xref="S4.E27.m1.105.105.105.21.21.21.1.cmml">l</mi></msup><mo stretchy="false"
    id="S4.E27.m1.106.106.106.22.22.22">)</mo></mrow></mrow></mrow><mo id="S4.E27.m1.107.107.107.23.23.23">,</mo><mrow
    id="S4.E27.m1.131.131.6.130.42.42.42.1.2.2"><msup id="S4.E27.m1.131.131.6.130.42.42.42.1.2.2.4"><mi
    id="S4.E27.m1.108.108.108.24.24.24" xref="S4.E27.m1.108.108.108.24.24.24.cmml">𝐳</mi><mrow
    id="S4.E27.m1.109.109.109.25.25.25.1" xref="S4.E27.m1.109.109.109.25.25.25.1.cmml"><mi
    id="S4.E27.m1.109.109.109.25.25.25.1.2" xref="S4.E27.m1.109.109.109.25.25.25.1.2.cmml">l</mi><mo
    id="S4.E27.m1.109.109.109.25.25.25.1.1" xref="S4.E27.m1.109.109.109.25.25.25.1.1.cmml">+</mo><mn
    id="S4.E27.m1.109.109.109.25.25.25.1.3" xref="S4.E27.m1.109.109.109.25.25.25.1.3.cmml">1</mn></mrow></msup><mo
    id="S4.E27.m1.110.110.110.26.26.26" xref="S4.E27.m1.110.110.110.26.26.26.cmml">=</mo><mrow
    id="S4.E27.m1.131.131.6.130.42.42.42.1.2.2.3"><msup id="S4.E27.m1.131.131.6.130.42.42.42.1.2.2.3.5"><mi
    class="ltx_font_mathcaligraphic" id="S4.E27.m1.111.111.111.27.27.27" xref="S4.E27.m1.111.111.111.27.27.27.cmml">ℱ</mi><mi
    id="S4.E27.m1.112.112.112.28.28.28.1" xref="S4.E27.m1.112.112.112.28.28.28.1.cmml">G</mi></msup><mo
    lspace="0em" rspace="0em" id="S4.E27.m1.131.131.6.130.42.42.42.1.2.2.3.4">​</mo><mrow
    id="S4.E27.m1.131.131.6.130.42.42.42.1.2.2.3.3.3"><mo stretchy="false" id="S4.E27.m1.113.113.113.29.29.29">(</mo><msubsup
    id="S4.E27.m1.131.131.6.130.42.42.42.1.2.2.1.1.1.1"><mi id="S4.E27.m1.114.114.114.30.30.30"
    xref="S4.E27.m1.114.114.114.30.30.30.cmml">𝐦</mi><mi id="S4.E27.m1.115.115.115.31.31.31.1"
    xref="S4.E27.m1.115.115.115.31.31.31.1.cmml">E</mi><mi id="S4.E27.m1.116.116.116.32.32.32.1"
    xref="S4.E27.m1.116.116.116.32.32.32.1.cmml">l</mi></msubsup><mo id="S4.E27.m1.117.117.117.33.33.33">,</mo><msubsup
    id="S4.E27.m1.131.131.6.130.42.42.42.1.2.2.2.2.2.2"><mi id="S4.E27.m1.118.118.118.34.34.34"
    xref="S4.E27.m1.118.118.118.34.34.34.cmml">𝐦</mi><mi id="S4.E27.m1.119.119.119.35.35.35.1"
    xref="S4.E27.m1.119.119.119.35.35.35.1.cmml">V</mi><mi id="S4.E27.m1.120.120.120.36.36.36.1"
    xref="S4.E27.m1.120.120.120.36.36.36.1.cmml">l</mi></msubsup><mo id="S4.E27.m1.121.121.121.37.37.37">,</mo><msup
    id="S4.E27.m1.131.131.6.130.42.42.42.1.2.2.3.3.3.3"><mi id="S4.E27.m1.122.122.122.38.38.38"
    xref="S4.E27.m1.122.122.122.38.38.38.cmml">𝐳</mi><mi id="S4.E27.m1.123.123.123.39.39.39.1"
    xref="S4.E27.m1.123.123.123.39.39.39.1.cmml">l</mi></msup><mo stretchy="false"
    id="S4.E27.m1.124.124.124.40.40.40">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E27.m1.125.125.125.41.41.41">,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S4.E27.m1.131b"><apply id="S4.E27.m1.126.126.1.1.1.4.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.4a.cmml">formulae-sequence</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.3a.cmml">formulae-sequence</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.1.1.cmml"><apply id="S4.E27.m1.126.126.1.1.1.1.1.1.1.3.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.1.1.3.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.1.1.3.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.1.1.3.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.1.1.1.1.1.1.cmml" xref="S4.E27.m1.1.1.1.1.1.1">𝐦</ci><ci id="S4.E27.m1.2.2.2.2.2.2.1.cmml"
    xref="S4.E27.m1.2.2.2.2.2.2.1">𝑖</ci></apply><ci id="S4.E27.m1.3.3.3.3.3.3.1.cmml"
    xref="S4.E27.m1.3.3.3.3.3.3.1">𝑙</ci></apply><apply id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.cmml"><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.3.1.cmml">superscript</csymbol><ci
    id="S4.E27.m1.5.5.5.5.5.5.cmml" xref="S4.E27.m1.5.5.5.5.5.5">𝒢</ci><apply id="S4.E27.m1.6.6.6.6.6.6.1.cmml"
    xref="S4.E27.m1.6.6.6.6.6.6.1"><ci id="S4.E27.m1.6.6.6.6.6.6.1.1.cmml" xref="S4.E27.m1.6.6.6.6.6.6.1.1">→</ci><ci
    id="S4.E27.m1.6.6.6.6.6.6.1.2.cmml" xref="S4.E27.m1.6.6.6.6.6.6.1.2">𝐸</ci><ci
    id="S4.E27.m1.6.6.6.6.6.6.1.3.cmml" xref="S4.E27.m1.6.6.6.6.6.6.1.3">𝑉</ci></apply></apply><set
    id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><apply id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><list
    id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><apply id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><csymbol cd="ambiguous"
    id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.9.9.9.9.9.9.cmml" xref="S4.E27.m1.9.9.9.9.9.9">𝐞</ci><apply id="S4.E27.m1.10.10.10.10.10.10.1.cmml"
    xref="S4.E27.m1.10.10.10.10.10.10.1"><ci id="S4.E27.m1.10.10.10.10.10.10.1.2.cmml"
    xref="S4.E27.m1.10.10.10.10.10.10.1.2">𝑖</ci><ci id="S4.E27.m1.10.10.10.10.10.10.1.3.cmml"
    xref="S4.E27.m1.10.10.10.10.10.10.1.3">𝑗</ci></apply></apply><ci id="S4.E27.m1.11.11.11.11.11.11.1.cmml"
    xref="S4.E27.m1.11.11.11.11.11.11.1">𝑙</ci></apply><apply id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><csymbol
    cd="latexml" id="S4.E27.m1.13.13.13.13.13.13.cmml" xref="S4.E27.m1.13.13.13.13.13.13">for-all</csymbol><ci
    id="S4.E27.m1.14.14.14.14.14.14.cmml" xref="S4.E27.m1.14.14.14.14.14.14">𝑗</ci></apply></list><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml"><ci id="S4.E27.m1.16.16.16.16.16.16.cmml"
    xref="S4.E27.m1.16.16.16.16.16.16">𝒩</ci><ci id="S4.E27.m1.18.18.18.18.18.18.cmml"
    xref="S4.E27.m1.18.18.18.18.18.18">𝑖</ci></apply></apply></set></apply></apply><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.cmml"><apply id="S4.E27.m1.126.126.1.1.1.1.1.2.2b.cmml"><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.4.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.2.2.4.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.4.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.2.2.4.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.23.23.23.23.23.23.cmml" xref="S4.E27.m1.23.23.23.23.23.23">𝐦</ci><ci
    id="S4.E27.m1.24.24.24.24.24.24.1.cmml" xref="S4.E27.m1.24.24.24.24.24.24.1">𝑉</ci></apply><ci
    id="S4.E27.m1.25.25.25.25.25.25.1.cmml" xref="S4.E27.m1.25.25.25.25.25.25.1">𝑙</ci></apply><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.cmml"><apply id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.3.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.3.1.cmml">superscript</csymbol><ci
    id="S4.E27.m1.27.27.27.27.27.27.cmml" xref="S4.E27.m1.27.27.27.27.27.27">𝒢</ci><apply
    id="S4.E27.m1.28.28.28.28.28.28.1.cmml" xref="S4.E27.m1.28.28.28.28.28.28.1"><ci
    id="S4.E27.m1.28.28.28.28.28.28.1.1.cmml" xref="S4.E27.m1.28.28.28.28.28.28.1.1">→</ci><ci
    id="S4.E27.m1.28.28.28.28.28.28.1.2.cmml" xref="S4.E27.m1.28.28.28.28.28.28.1.2">𝑉</ci><ci
    id="S4.E27.m1.28.28.28.28.28.28.1.3.cmml" xref="S4.E27.m1.28.28.28.28.28.28.1.3">𝐺</ci></apply></apply><set
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.1.1.1.2.cmml"><apply id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.1.1.1.1.1.cmml"><list
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.1.1.1.1.1.2.3.cmml"><apply id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.1.1.1.1.1.1.1.1.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><csymbol cd="ambiguous"
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.31.31.31.31.31.31.cmml" xref="S4.E27.m1.31.31.31.31.31.31">𝐡</ci><ci
    id="S4.E27.m1.32.32.32.32.32.32.1.cmml" xref="S4.E27.m1.32.32.32.32.32.32.1">𝑖</ci></apply><ci
    id="S4.E27.m1.33.33.33.33.33.33.1.cmml" xref="S4.E27.m1.33.33.33.33.33.33.1">𝑙</ci></apply><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.1.1.1.1.1.2.2.2.cmml"><csymbol cd="latexml"
    id="S4.E27.m1.35.35.35.35.35.35.cmml" xref="S4.E27.m1.35.35.35.35.35.35">for-all</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.1.1.1.1.1.2.2.2.2.cmml"><csymbol cd="ambiguous"
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.1.1.1.1.1.2.2.2.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.36.36.36.36.36.36.cmml" xref="S4.E27.m1.36.36.36.36.36.36">𝑣</ci><ci
    id="S4.E27.m1.37.37.37.37.37.37.1.cmml" xref="S4.E27.m1.37.37.37.37.37.37.1">𝑖</ci></apply></apply></list><ci
    id="S4.E27.m1.39.39.39.39.39.39.cmml" xref="S4.E27.m1.39.39.39.39.39.39">𝑉</ci></apply></set><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.4.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.4.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.4.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.2.2.1.4.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.42.42.42.1.1.1.cmml" xref="S4.E27.m1.42.42.42.1.1.1">𝐦</ci><ci id="S4.E27.m1.43.43.43.2.2.2.1.cmml"
    xref="S4.E27.m1.43.43.43.2.2.2.1">𝐸</ci></apply><ci id="S4.E27.m1.44.44.44.3.3.3.1.cmml"
    xref="S4.E27.m1.44.44.44.3.3.3.1">𝑙</ci></apply></apply></apply><apply id="S4.E27.m1.126.126.1.1.1.1.1.2.2c.cmml"><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.cmml"><apply id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.3.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.3.1.cmml">superscript</csymbol><ci
    id="S4.E27.m1.46.46.46.5.5.5.cmml" xref="S4.E27.m1.46.46.46.5.5.5">𝒢</ci><apply
    id="S4.E27.m1.47.47.47.6.6.6.1.cmml" xref="S4.E27.m1.47.47.47.6.6.6.1"><ci id="S4.E27.m1.47.47.47.6.6.6.1.1.cmml"
    xref="S4.E27.m1.47.47.47.6.6.6.1.1">→</ci><ci id="S4.E27.m1.47.47.47.6.6.6.1.2.cmml"
    xref="S4.E27.m1.47.47.47.6.6.6.1.2">𝐸</ci><ci id="S4.E27.m1.47.47.47.6.6.6.1.3.cmml"
    xref="S4.E27.m1.47.47.47.6.6.6.1.3">𝐺</ci></apply></apply><set id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.2.cmml"><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.cmml"><list id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.2.3.cmml"><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.1.1.1.cmml"><csymbol cd="ambiguous"
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.1.1.1.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.1.1.1.2.cmml"><csymbol cd="ambiguous"
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.1.1.1.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.50.50.50.9.9.9.cmml" xref="S4.E27.m1.50.50.50.9.9.9">𝐞</ci><apply
    id="S4.E27.m1.51.51.51.10.10.10.1.cmml" xref="S4.E27.m1.51.51.51.10.10.10.1"><ci
    id="S4.E27.m1.51.51.51.10.10.10.1.2.cmml" xref="S4.E27.m1.51.51.51.10.10.10.1.2">𝑖</ci><ci
    id="S4.E27.m1.51.51.51.10.10.10.1.3.cmml" xref="S4.E27.m1.51.51.51.10.10.10.1.3">𝑗</ci></apply></apply><ci
    id="S4.E27.m1.52.52.52.11.11.11.1.cmml" xref="S4.E27.m1.52.52.52.11.11.11.1">𝑙</ci></apply><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.2.2.2.cmml"><csymbol cd="latexml"
    id="S4.E27.m1.54.54.54.13.13.13.cmml" xref="S4.E27.m1.54.54.54.13.13.13">for-all</csymbol><interval
    closure="open" id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.2.2.2.2.3.cmml"><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.2.2.2.1.1.1.cmml"><csymbol cd="ambiguous"
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.2.2.2.1.1.1.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.56.56.56.15.15.15.cmml" xref="S4.E27.m1.56.56.56.15.15.15">𝑣</ci><ci
    id="S4.E27.m1.57.57.57.16.16.16.1.cmml" xref="S4.E27.m1.57.57.57.16.16.16.1">𝑖</ci></apply><apply
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.2.2.2.2.2.2.cmml"><csymbol cd="ambiguous"
    id="S4.E27.m1.126.126.1.1.1.1.1.2.2.2.1.1.1.1.1.2.2.2.2.2.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.59.59.59.18.18.18.cmml" xref="S4.E27.m1.59.59.59.18.18.18">𝑣</ci><ci
    id="S4.E27.m1.60.60.60.19.19.19.1.cmml" xref="S4.E27.m1.60.60.60.19.19.19.1">𝑗</ci></apply></interval></apply></list><ci
    id="S4.E27.m1.63.63.63.22.22.22.cmml" xref="S4.E27.m1.63.63.63.22.22.22">𝐸</ci></apply></set></apply></apply></apply></apply><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.cmml"><apply id="S4.E27.m1.126.126.1.1.1.2.2b.cmml"><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.9.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.9.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.9.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.9.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.67.67.67.26.26.26.cmml" xref="S4.E27.m1.67.67.67.26.26.26">𝐡</ci><ci
    id="S4.E27.m1.68.68.68.27.27.27.1.cmml" xref="S4.E27.m1.68.68.68.27.27.27.1">𝑖</ci></apply><apply
    id="S4.E27.m1.69.69.69.28.28.28.1.cmml" xref="S4.E27.m1.69.69.69.28.28.28.1"><ci
    id="S4.E27.m1.69.69.69.28.28.28.1.2.cmml" xref="S4.E27.m1.69.69.69.28.28.28.1.2">𝑙</ci><cn
    type="integer" id="S4.E27.m1.69.69.69.28.28.28.1.3.cmml" xref="S4.E27.m1.69.69.69.28.28.28.1.3">1</cn></apply></apply><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.3.cmml"><apply id="S4.E27.m1.126.126.1.1.1.2.2.3.5.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.3.5.1.cmml">superscript</csymbol><ci
    id="S4.E27.m1.71.71.71.30.30.30.cmml" xref="S4.E27.m1.71.71.71.30.30.30">ℱ</ci><ci
    id="S4.E27.m1.72.72.72.31.31.31.1.cmml" xref="S4.E27.m1.72.72.72.31.31.31.1">𝑉</ci></apply><vector
    id="S4.E27.m1.126.126.1.1.1.2.2.3.3.4.cmml"><apply id="S4.E27.m1.126.126.1.1.1.2.2.1.1.1.1.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.1.1.1.1.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.1.1.1.1.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.74.74.74.33.33.33.cmml" xref="S4.E27.m1.74.74.74.33.33.33">𝐦</ci><ci
    id="S4.E27.m1.75.75.75.34.34.34.1.cmml" xref="S4.E27.m1.75.75.75.34.34.34.1">𝑖</ci></apply><ci
    id="S4.E27.m1.76.76.76.35.35.35.1.cmml" xref="S4.E27.m1.76.76.76.35.35.35.1">𝑙</ci></apply><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.2.2.2.2.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.2.2.2.2.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.78.78.78.37.37.37.cmml" xref="S4.E27.m1.78.78.78.37.37.37">𝐡</ci><ci
    id="S4.E27.m1.79.79.79.38.38.38.1.cmml" xref="S4.E27.m1.79.79.79.38.38.38.1">𝑖</ci></apply><ci
    id="S4.E27.m1.80.80.80.39.39.39.1.cmml" xref="S4.E27.m1.80.80.80.39.39.39.1">𝑙</ci></apply><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.3.3.3.3.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.3.3.3.3.1.cmml">superscript</csymbol><ci
    id="S4.E27.m1.82.82.82.41.41.41.cmml" xref="S4.E27.m1.82.82.82.41.41.41">𝐳</ci><ci
    id="S4.E27.m1.83.83.83.42.42.42.1.cmml" xref="S4.E27.m1.83.83.83.42.42.42.1">𝑙</ci></apply></vector><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.3.6.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.3.6.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.3.6.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.3.6.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.85.85.85.1.1.1.cmml" xref="S4.E27.m1.85.85.85.1.1.1">𝐞</ci><apply
    id="S4.E27.m1.86.86.86.2.2.2.1.cmml" xref="S4.E27.m1.86.86.86.2.2.2.1"><ci id="S4.E27.m1.86.86.86.2.2.2.1.2.cmml"
    xref="S4.E27.m1.86.86.86.2.2.2.1.2">𝑖</ci><ci id="S4.E27.m1.86.86.86.2.2.2.1.3.cmml"
    xref="S4.E27.m1.86.86.86.2.2.2.1.3">𝑗</ci></apply></apply><apply id="S4.E27.m1.87.87.87.3.3.3.1.cmml"
    xref="S4.E27.m1.87.87.87.3.3.3.1"><ci id="S4.E27.m1.87.87.87.3.3.3.1.2.cmml" xref="S4.E27.m1.87.87.87.3.3.3.1.2">𝑙</ci><cn
    type="integer" id="S4.E27.m1.87.87.87.3.3.3.1.3.cmml" xref="S4.E27.m1.87.87.87.3.3.3.1.3">1</cn></apply></apply></apply></apply><apply
    id="S4.E27.m1.126.126.1.1.1.2.2c.cmml"><apply id="S4.E27.m1.126.126.1.1.1.2.2.7.cmml"><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.7.6.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.7.6.1.cmml">superscript</csymbol><ci
    id="S4.E27.m1.89.89.89.5.5.5.cmml" xref="S4.E27.m1.89.89.89.5.5.5">ℱ</ci><ci id="S4.E27.m1.90.90.90.6.6.6.1.cmml"
    xref="S4.E27.m1.90.90.90.6.6.6.1">𝐸</ci></apply><vector id="S4.E27.m1.126.126.1.1.1.2.2.7.4.5.cmml"><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.4.1.1.1.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.4.1.1.1.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.4.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.4.1.1.1.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.92.92.92.8.8.8.cmml" xref="S4.E27.m1.92.92.92.8.8.8">𝐞</ci><apply
    id="S4.E27.m1.93.93.93.9.9.9.1.cmml" xref="S4.E27.m1.93.93.93.9.9.9.1"><ci id="S4.E27.m1.93.93.93.9.9.9.1.2.cmml"
    xref="S4.E27.m1.93.93.93.9.9.9.1.2">𝑖</ci><ci id="S4.E27.m1.93.93.93.9.9.9.1.3.cmml"
    xref="S4.E27.m1.93.93.93.9.9.9.1.3">𝑗</ci></apply></apply><ci id="S4.E27.m1.94.94.94.10.10.10.1.cmml"
    xref="S4.E27.m1.94.94.94.10.10.10.1">𝑙</ci></apply><apply id="S4.E27.m1.126.126.1.1.1.2.2.5.2.2.2.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.5.2.2.2.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.5.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.5.2.2.2.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.96.96.96.12.12.12.cmml" xref="S4.E27.m1.96.96.96.12.12.12">𝐡</ci><ci
    id="S4.E27.m1.97.97.97.13.13.13.1.cmml" xref="S4.E27.m1.97.97.97.13.13.13.1">𝑖</ci></apply><ci
    id="S4.E27.m1.98.98.98.14.14.14.1.cmml" xref="S4.E27.m1.98.98.98.14.14.14.1">𝑙</ci></apply><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.6.3.3.3.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.6.3.3.3.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.6.3.3.3.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.6.3.3.3.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.100.100.100.16.16.16.cmml" xref="S4.E27.m1.100.100.100.16.16.16">𝐡</ci><ci
    id="S4.E27.m1.101.101.101.17.17.17.1.cmml" xref="S4.E27.m1.101.101.101.17.17.17.1">𝑗</ci></apply><ci
    id="S4.E27.m1.102.102.102.18.18.18.1.cmml" xref="S4.E27.m1.102.102.102.18.18.18.1">𝑙</ci></apply><apply
    id="S4.E27.m1.126.126.1.1.1.2.2.7.4.4.4.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.2.2.7.4.4.4.1.cmml">superscript</csymbol><ci
    id="S4.E27.m1.104.104.104.20.20.20.cmml" xref="S4.E27.m1.104.104.104.20.20.20">𝐳</ci><ci
    id="S4.E27.m1.105.105.105.21.21.21.1.cmml" xref="S4.E27.m1.105.105.105.21.21.21.1">𝑙</ci></apply></vector></apply></apply></apply><apply
    id="S4.E27.m1.126.126.1.1.1.3.3.cmml"><apply id="S4.E27.m1.126.126.1.1.1.3.3.5.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.3.3.5.1.cmml">superscript</csymbol><ci
    id="S4.E27.m1.108.108.108.24.24.24.cmml" xref="S4.E27.m1.108.108.108.24.24.24">𝐳</ci><apply
    id="S4.E27.m1.109.109.109.25.25.25.1.cmml" xref="S4.E27.m1.109.109.109.25.25.25.1"><ci
    id="S4.E27.m1.109.109.109.25.25.25.1.2.cmml" xref="S4.E27.m1.109.109.109.25.25.25.1.2">𝑙</ci><cn
    type="integer" id="S4.E27.m1.109.109.109.25.25.25.1.3.cmml" xref="S4.E27.m1.109.109.109.25.25.25.1.3">1</cn></apply></apply><apply
    id="S4.E27.m1.126.126.1.1.1.3.3.3.cmml"><apply id="S4.E27.m1.126.126.1.1.1.3.3.3.5.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.3.3.3.5.1.cmml">superscript</csymbol><ci
    id="S4.E27.m1.111.111.111.27.27.27.cmml" xref="S4.E27.m1.111.111.111.27.27.27">ℱ</ci><ci
    id="S4.E27.m1.112.112.112.28.28.28.1.cmml" xref="S4.E27.m1.112.112.112.28.28.28.1">𝐺</ci></apply><vector
    id="S4.E27.m1.126.126.1.1.1.3.3.3.3.4.cmml"><apply id="S4.E27.m1.126.126.1.1.1.3.3.1.1.1.1.cmml"><csymbol
    cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.3.3.1.1.1.1.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.3.3.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.3.3.1.1.1.1.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.114.114.114.30.30.30.cmml" xref="S4.E27.m1.114.114.114.30.30.30">𝐦</ci><ci
    id="S4.E27.m1.115.115.115.31.31.31.1.cmml" xref="S4.E27.m1.115.115.115.31.31.31.1">𝐸</ci></apply><ci
    id="S4.E27.m1.116.116.116.32.32.32.1.cmml" xref="S4.E27.m1.116.116.116.32.32.32.1">𝑙</ci></apply><apply
    id="S4.E27.m1.126.126.1.1.1.3.3.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.3.3.2.2.2.2.1.cmml">superscript</csymbol><apply
    id="S4.E27.m1.126.126.1.1.1.3.3.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.3.3.2.2.2.2.2.1.cmml">subscript</csymbol><ci
    id="S4.E27.m1.118.118.118.34.34.34.cmml" xref="S4.E27.m1.118.118.118.34.34.34">𝐦</ci><ci
    id="S4.E27.m1.119.119.119.35.35.35.1.cmml" xref="S4.E27.m1.119.119.119.35.35.35.1">𝑉</ci></apply><ci
    id="S4.E27.m1.120.120.120.36.36.36.1.cmml" xref="S4.E27.m1.120.120.120.36.36.36.1">𝑙</ci></apply><apply
    id="S4.E27.m1.126.126.1.1.1.3.3.3.3.3.3.cmml"><csymbol cd="ambiguous" id="S4.E27.m1.126.126.1.1.1.3.3.3.3.3.3.1.cmml">superscript</csymbol><ci
    id="S4.E27.m1.122.122.122.38.38.38.cmml" xref="S4.E27.m1.122.122.122.38.38.38">𝐳</ci><ci
    id="S4.E27.m1.123.123.123.39.39.39.1.cmml" xref="S4.E27.m1.123.123.123.39.39.39.1">𝑙</ci></apply></vector></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S4.E27.m1.131c">\begin{gathered}\mathbf{m}_{i}^{l}=\mathcal{G}^{E\rightarrow
    V}(\{\mathbf{e}_{ij}^{l},\forall j\in\mathcal{N}(i)\}),\mathbf{m}_{V}^{l}=\mathcal{G}^{V\rightarrow
    G}(\{\mathbf{h}_{i}^{l},\forall v_{i}\in V\})\\ \mathbf{m}_{E}^{l}=\mathcal{G}^{E\rightarrow
    G}(\{\mathbf{e}_{ij}^{l},\forall(v_{i},v_{j})\in E\}),\mathbf{h}_{i}^{l+1}=\mathcal{F}^{V}(\mathbf{m}_{i}^{l},\mathbf{h}_{i}^{l},\mathbf{z}^{l})\\
    \mathbf{e}_{ij}^{l+1}=\mathcal{F}^{E}(\mathbf{e}_{ij}^{l},\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{z}^{l}),\mathbf{z}^{l+1}=\mathcal{F}^{G}(\mathbf{m}_{E}^{l},\mathbf{m}_{V}^{l},\mathbf{z}^{l}),\end{gathered}</annotation></semantics></math>
    |  | (27) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathcal{F}^{V}(\cdot),\mathcal{F}^{E}(\cdot)$, and $\mathcal{F}^{G}(\cdot)$
    are the corresponding updating functions for nodes, edges, and the entire graph,
    respectively, and $\mathcal{G}(\cdot)$ represents message-passing functions whose
    superscripts denote message-passing directions. Note that the message-passing
    functions all take a set as the input, thus their arguments are variable in length
    and these functions should be invariant to input permutations; some examples include
    the element-wise summation, mean, and maximum. Compared with MPNNs, GNs introduced
    the edge representations and the representation of the entire graph, thus making
    the framework more general.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the convolution operations have evolved from the spectral domain
    to the spatial domain and from multistep neighbors to the immediate neighbors.
    Currently, gathering information from the immediate neighbors (as in Eq. ([14](#S4.E14
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey"))) and following the framework of
    Eqs. ([21](#S4.E21 "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))([22](#S4.E22 "In
    4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey"))([27](#S4.E27 "In 4.1.4 Frameworks ‣ 4.1
    Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs:
    A Survey")) are the most common choices for graph convolution operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Readout Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using graph convolution operations, useful node features can be learned to solve
    many node-focused tasks. However, to tackle graph-focused tasks, node information
    needs to be aggregated to form a graph-level representation. In the literature,
    such procedures are usually called the readout operations⁷⁷7Readout operations
    are also related to graph coarsening, i.e., reducing a large graph to a smaller
    graph, because a graph-level representation can be obtained by coarsening the
    graph to a single node. Some papers use these two terms interchangeably.. Based
    on a regular and local neighborhood, standard CNNs conduct multiple stride convolutions
    or poolings to gradually reduce the resolution. Since graphs lack a grid structure,
    these existing methods cannot be used directly.
  prefs: []
  type: TYPE_NORMAL
- en: Order invariance. A critical requirement for the graph readout operations is
    that the operation should be invariant to the node order, i.e., if we change the
    indices of nodes and edges using a bijective function between two node sets, the
    representation of the entire graph should not change. For example, whether a drug
    can treat certain diseases depends on its inherent structure; thus, we should
    get identical results if we represent the drug using different node indices. Note
    that because this problem is related to the graph isomorphism problem, of which
    the best-known algorithm is quasipolynomial [[80](#bib.bib80)], we only can find
    a function that is order-invariant but not vice versa in a polynomial time, i.e.,
    even two structurally different graphs may have the same representation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Statistics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The most basic order-invariant operations involve simple statistics such as
    summation, averaging or max-pooling [[46](#bib.bib46), [50](#bib.bib50)], i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}_{G}=\sum_{i=1}^{N}\mathbf{h}^{L}_{i}\;\text{or}\;\mathbf{h}_{G}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{h}^{L}_{i}\;\text{or}\;\mathbf{h}_{G}=\max\left\{\mathbf{h}^{L}_{i},\forall
    i\right\},$ |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{h}_{G}$ is the representation of the graph $G$ and $\mathbf{h}^{L}_{i}$
    is the representation of node $v_{i}$ in the final layer $L$. However, such first-moment
    statistics may not be sufficiently representative to distinguish different graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Kearnes et al. [[55](#bib.bib55)] suggested considering the distribution of
    node representations by using fuzzy histograms [[81](#bib.bib81)]. The basic idea
    behind fuzzy histograms is to construct several “histogram bins” and then calculate
    the memberships of $\mathbf{h}^{L}_{i}$ to these bins, i.e., by regarding node
    representations as samples and matching them to some pre-defined templates, and
    finally return the concatenation of the final histograms. In this way, nodes with
    the same sum/average/maximum but with different distributions can be distinguished.
  prefs: []
  type: TYPE_NORMAL
- en: Another commonly used approach for aggregating node representation is to add
    a fully connected (FC) layer as the final layer [[40](#bib.bib40)], i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}_{G}=\rho\left(\left[\mathbf{H}^{L}\right]\mathbf{\Theta}_{FC}\right),$
    |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\left[\mathbf{H}^{L}\right]\in\mathbb{R}^{Nf_{L}}$ is the concatenation
    of the final node representation $\mathbf{H}^{L}$, $\mathbf{\Theta}_{FC}\in\mathbb{R}^{Nf_{L}\times
    f_{\text{output}}}$ are parameters, and $f_{\text{output}}$ is the dimensionality
    of the output. Eq. ([29](#S4.E29 "In 4.2.1 Statistics ‣ 4.2 Readout Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) can be
    regarded as a weighted sum of node-level features. One advantage is that the model
    can learn different weights for different nodes; however, this ability comes at
    the cost of being unable to guarantee order invariance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Hierarchical Clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rather than a dichotomy between node and graph level structures, graphs are
    known to exhibit rich hierarchical structures [[82](#bib.bib82)], which can be
    explored by hierarchical clustering methods as shown in Figure [4](#S4.F4 "Figure
    4 ‣ 4.2.2 Hierarchical Clustering ‣ 4.2 Readout Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey"). For example, a density-based agglomerative
    clustering [[83](#bib.bib83)] was used in Bruna et al. [[40](#bib.bib40)] and
    multi-resolution spectral clustering [[84](#bib.bib84)] was used in Henaff et al. [[41](#bib.bib41)].
    ChebNet [[42](#bib.bib42)] and MoNet [[54](#bib.bib54)] adopted another greedy
    hierarchical clustering algorithm, Graclus [[85](#bib.bib85)], to merge two nodes
    at a time, along with a fast pooling method to rearrange the nodes into a balanced
    binary tree. ECC [[63](#bib.bib63)] adopted another hierarchical clustering method
    by performing eigendecomposition [[86](#bib.bib86)]. However, these hierarchical
    clustering methods are all independent of the graph convolutions (i.e., they can
    be performed as a preprocessing step and are not trained in an end-to-end fashion).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eaf892d57e3d64a555f9f149a708169e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: An example of performing a hierarchical clustering algorithm. Reprinted
    from [[56](#bib.bib56)] with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve that problem, DiffPool [[56](#bib.bib56)] proposed a differentiable
    hierarchical clustering algorithm jointly trained with the graph convolutions.
    Specifically, the authors proposed learning a soft cluster assignment matrix in
    each layer using the hidden representations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{S}^{l}=\mathcal{F}\left(\mathbf{A}^{l},\mathbf{H}^{l}\right),$
    |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{S}^{l}\in\mathbb{R}^{N_{l}\times N_{l+1}}$ is the cluster assignment
    matrix, $N_{l}$ is the number of clusters in the layer $l$ and $\mathcal{F}(\cdot)$
    is a function to be learned. Then, the node representations and the new adjacency
    matrix for this “coarsened” graph can be obtained by taking the average according
    to $\mathbf{S}^{l}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{H}^{l+1}=(\mathbf{S}^{l})^{T}\hat{\mathbf{H}}^{l+1},\mathbf{A}^{l+1}=(\mathbf{S}^{l})^{T}\mathbf{A}^{l}\mathbf{S}^{l},$
    |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: where $\hat{\mathbf{H}}^{l+1}$ is obtained by applying a graph convolution layer
    to $\mathbf{H}^{l}$, i.e., coarsening the graph from $N_{l}$ nodes to $N_{l+1}$
    nodes in each layer after the convolution operation. The initial number of nodes
    is $N_{0}=N$ and the last layer is $N_{L}=1$, i.e., a single node that represents
    the entire graph. Because the cluster assignment operation is soft, the connections
    between clusters are not sparse; thus the time complexity of the method is $O(N^{2})$
    in principle.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Imposing Orders and Others
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned in Section [4.1.3](#S4.SS1.SSS3 "4.1.3 The Aspect of Multiple
    Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey"), PATCHY-SAN [[47](#bib.bib47)] and SortPooling [[49](#bib.bib49)]
    took the idea of imposing a node order and then resorted to standard 1-D pooling
    as in CNNs. Whether these methods can preserve order invariance depends on how
    the order is imposed, which is another research field that we refer readers to [[87](#bib.bib87)]
    for a survey. However, whether imposing a node order is a natural choice for graphs
    and if so, what the best node orders are constitute still on-going research topics.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the aforementioned methods, there are some heuristics. In GNNs [[23](#bib.bib23)],
    the authors suggested adding a special node connected to all nodes to represent
    the entire graph. Similarly, GNs [[9](#bib.bib9)] proposed to directly learn the
    representation of the entire graph by receiving messages from all nodes and edges.
  prefs: []
  type: TYPE_NORMAL
- en: MPNNs adopted set2set [[88](#bib.bib88)], a modification of the seq2seq model.
    Specifically, set2set uses a “Read-Process-and-Write” model that receives all
    inputs simultaneously, computes internal memories using an attention mechanism
    and an LSTM, and then writes the outputs. Unlike seq2seq which is order-sensitive,
    set2set is invariant to the input order.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In short, statistics such as averaging or summation are the most simple readout
    operations, while hierarchical clustering algorithms jointly trained with graph
    convolutions are more advanced but are also more sophisticated. Other methods
    such as adding a pseudo node or imposing a node order have also been investigated.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Improvements and Discussions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many techniques have been introduced to further improve GCNs. Note that some
    of these methods are general and could be applied to other deep learning models
    on graphs as well.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Attention Mechanism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the aforementioned GCNs, the node neighborhoods are aggregated with equal
    or pre-defined weights. However, the influences of neighbors can vary greatly;
    thus, they should be learned during training rather than being predetermined.
    Inspired by the attention mechanism [[89](#bib.bib89)], graph attention network
    (GAT) [[57](#bib.bib57)] introduces the attention mechanism into GCNs by modifying
    the convolution operation in Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect
    ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}^{l+1}_{i}=\rho\left(\sum\nolimits_{j\in\hat{\mathcal{N}}(i)}\alpha_{ij}^{l}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right),$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\alpha_{ij}^{l}$ is node $v_{i}$’s attention to node $v_{j}$ in the
    $l^{th}$ layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\alpha_{ij}^{l}=\frac{\exp\left(\text{LeakyReLU}\left(\mathcal{F}\left(\mathbf{h}^{l}_{i}\mathbf{\Theta}^{l},\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right)\right)\right)}{\sum_{k\in\hat{\mathcal{N}}(i)}\exp\left(\text{LeakyReLU}\left(\mathcal{F}\left(\mathbf{h}^{l}_{i}\mathbf{\Theta}^{l},\mathbf{h}^{l}_{k}\mathbf{\Theta}^{l}\right)\right)\right)},$
    |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{F}(\cdot,\cdot)$ is another function to be learned such as
    a multi-layer perceptron (MLP). To improve model capacity and stability, the authors
    also suggested using multiple independent attentions and concatenating the results,
    i.e., the multi-head attention mechanism [[89](#bib.bib89)] as illustrated in
    Figure [5](#S4.F5 "Figure 5 ‣ 4.3.1 Attention Mechanism ‣ 4.3 Improvements and
    Discussions ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey").
    GaAN [[58](#bib.bib58)] further proposed learning different weights for different
    heads and applied such a method to the traffic forecasting problem.'
  prefs: []
  type: TYPE_NORMAL
- en: HAN [[59](#bib.bib59)] proposed a two-level attention mechanism, i.e., a node-level
    and a semantic-level attention mechanism, for heterogeneous graphs. Specifically,
    the node-level attention mechanism was similar to a GAT, but also considerd node
    types; therefore, it could assign different weights to aggregating meta-path-based
    neighbors. The semantic-level attention then learned the importance of different
    meta-paths and outputed the final results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed0729d23fce33a1585746df5caacb26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An illustration of the multi-head attention mechanism proposed in
    GAT [[57](#bib.bib57)] (reprinted with permission). Each color denotes an independent
    attention vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Residual and Jumping Connections
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many researches have observed that the most suitable depth for the existing
    GCNs is often very limited, e.g., 2 or 3 layers. This problem is potentially due
    to the practical difficulties involved in training deep GCNs or the over-smoothing
    problem, i.e., all nodes in deeper layers have the same representation [[70](#bib.bib70),
    [62](#bib.bib62)]. To remedy this problem, residual connections similar to ResNet [[90](#bib.bib90)]
    can be added to GCNs. For example, Kipf and Welling [[43](#bib.bib43)] added residual
    connections into Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{H}^{l+1}=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right)+\mathbf{H}^{l}.$
    |  | (34) |'
  prefs: []
  type: TYPE_TB
- en: They showed experimentally that adding such residual connections could allow
    the depth of the network to increase, which is similar to the results of ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Column network (CLN) [[60](#bib.bib60)] adopted a similar idea by using the
    following residual connections with learnable weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}^{l+1}_{i}=\bm{\alpha}^{l}_{i}\odot\widetilde{\mathbf{h}}^{l+1}_{i}+(1-\bm{\alpha}^{l}_{i})\odot\mathbf{h}^{l}_{i},$
    |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\widetilde{\mathbf{h}}^{l+1}_{i}$ is calculated similar to Eq. ([14](#S4.E14
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) and $\bm{\alpha}^{l}_{i}$ is a
    set of weights calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{\alpha}^{l}_{i}=\rho\left(\mathbf{b}^{l}_{\alpha}+\mathbf{\Theta}_{\alpha}^{l}\mathbf{h}_{i}^{l}+\mathbf{\Theta}_{\alpha}^{{}^{\prime}l}\sum\nolimits_{j\in\mathcal{N}(i)}\mathbf{h}_{j}^{l}\right),$
    |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{b}^{l}_{\alpha},\mathbf{\Theta}_{\alpha}^{l},\mathbf{\Theta}_{\alpha}^{{}^{\prime}l}$
    are parameters. Note that Eq. ([35](#S4.E35 "In 4.3.2 Residual and Jumping Connections
    ‣ 4.3 Improvements and Discussions ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) is very similar to the GRU as in GGS-NNs [[24](#bib.bib24)].
    The differences are that in a CLN, the superscripts denote the number of layers,
    and different layers contain different parameters, while in GGS-NNs, the superscript
    denotes the pseudo time and a single set of parameters is used across time steps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by personalized PageRank, PPNP [[61](#bib.bib61)] defined graph convolutions
    with teleportation to the initial layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{H}^{l+1}=(1-\alpha)\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}+\alpha\mathbf{H}^{0},$
    |  | (37) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{H}_{0}=\mathcal{F}_{\theta}(\mathbf{F}^{V})$ and $\alpha$ is
    a hyper-parameter. Note that all the parameters are in $\mathcal{F}_{\theta}(\cdot)$
    rather than in the graph convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jumping knowledge networks (JK-Nets) [[62](#bib.bib62)] proposed another architecture
    to connect the last layer of the network with all the lower hidden layers, i.e.,
    by “jumping” all the representations to the final output, as illustrated in Figure [6](#S4.F6
    "Figure 6 ‣ 4.3.2 Residual and Jumping Connections ‣ 4.3 Improvements and Discussions
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"). In this
    way, the model can learn to selectively exploit information from different layers.
    Formally, JK-Nets was formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}_{i}^{\text{final}}=\text{AGGREGATE}(\mathbf{h}_{i}^{0},\mathbf{h}_{i}^{1},...,\mathbf{h}_{i}^{L}),$
    |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{h}_{i}^{\text{final}}$ is the final representation for node
    $v_{i}$, AGGREGATE$(\cdot)$ is the aggregating function, and $L$ is the number
    of hidden layers. JK-Nets used three aggregating functions similar to GraphSAGE [[53](#bib.bib53)]:
    concatenation, max-pooling, and the LSTM attention. The experimental results showed
    that adding jump connections could improve the performance of multiple GCNs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f01752a8f1cbff94a422cef1749168f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Jumping knowledge networks proposed in [[62](#bib.bib62)] in which
    the last layer is connected to all the other layers to selectively exploit different
    information from different layers. GC denotes graph convolutions. Reprinted with
    permission.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Edge Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The aforementioned GCNs mostly focus on utilizing node features and graph structures.
    In this subsection, we briefly discuss how to use another important source of
    information: the edge features.'
  prefs: []
  type: TYPE_NORMAL
- en: For simple edge features with discrete values such as the edge type, a straightforward
    method is to train different parameters for different edge types and aggregate
    the results. For example, Neural FPs [[46](#bib.bib46)] trained different parameters
    for nodes with different degrees, which corresponds to the implicit edge feature
    of bond types in a molecular graph, and then summed over the results. CLN [[60](#bib.bib60)]
    trained different parameters for different edge types in a heterogeneous graph
    and averaged the results. Edge-conditioned convolution (ECC) [[63](#bib.bib63)]
    also trained different parameters based on edge types and applied them to graph
    classification. Relational GCNs (R-GCNs) [[64](#bib.bib64)] adopted a similar
    idea for knowledge graphs by training different weights for different relation
    types. However, these methods are suitable only for a limited number of discrete
    edge features.
  prefs: []
  type: TYPE_NORMAL
- en: DCNN [[50](#bib.bib50)] proposed another method to convert each edge into a
    node connected to the head and tail node of that edge. After this conversion,
    edge features can be treated as node features.
  prefs: []
  type: TYPE_NORMAL
- en: 'LGCN [[65](#bib.bib65)] constructed a line graph $\mathbf{B}\in\mathbb{R}^{2M\times
    2M}$ to incorporate edge features as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{B}_{i\rightarrow j,i^{\prime}\rightarrow j^{\prime}}=\left\{\begin{aligned}
    &amp;1\quad\text{if}\;j=i^{\prime}\;\text{and}\;j^{\prime}\neq i,\\ &amp;0\quad\text{otherwise}.\end{aligned}\right.$
    |  | (39) |'
  prefs: []
  type: TYPE_TB
- en: 'In other words, nodes in the line graph are directed edges in the original
    graph, and two nodes in the line graph are connected if information can flow through
    their corresponding edges in the original graph. Then, LGCN adopted two GCNs:
    one on the original graph and one on the line graph.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kearnes et al. [[55](#bib.bib55)] proposed an architecture using a “weave module”.
    Specifically, they learned representations for both nodes and edges and exchanged
    information between them in each weave module using four different functions:
    node-to-node (NN), node-to-edge (NE), edge-to-edge (EE) and edge-to-node (EN):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S4.E40.m1.117" class="ltx_Math" alttext="\begin{gathered}\mathbf{h}^{l^{\prime}}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{0}_{i},\mathbf{h}^{1}_{i},...,\mathbf{h}^{l}_{i}),\mathbf{h}^{l^{\prime\prime}}_{i}=\mathcal{F}_{EN}(\{\mathbf{e}^{l}_{ij}&#124;j\in\mathcal{N}(i)\})\\
    \mathbf{e}^{l^{\prime}}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{0}_{ij},\mathbf{e}^{1}_{ij},...,\mathbf{e}^{l}_{ij}),\mathbf{e}^{l^{\prime\prime}}_{ij}=\mathcal{F}_{NE}(\mathbf{h}^{l}_{i},\mathbf{h}^{l}_{j})\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{h}^{l+1}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{l^{\prime}}_{i},\mathbf{h}^{l^{\prime\prime}}_{i}),\mathbf{e}^{l+1}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{l^{\prime}}_{ij},\mathbf{e}^{l^{\prime\prime}}_{ij}),\end{gathered}"
    display="block"><semantics id="S4.E40.m1.117a"><mtable displaystyle="true" rowspacing="0pt"
    id="S4.E40.m1.117.117.6"><mtr id="S4.E40.m1.117.117.6a"><mtd id="S4.E40.m1.117.117.6b"><mrow
    id="S4.E40.m1.114.114.3.113.44.44.44"><mrow id="S4.E40.m1.113.113.2.112.43.43.43.1"><msubsup
    id="S4.E40.m1.113.113.2.112.43.43.43.1.4"><mi id="S4.E40.m1.1.1.1.1.1.1" xref="S4.E40.m1.1.1.1.1.1.1.cmml">𝐡</mi><mi
    id="S4.E40.m1.3.3.3.3.3.3.1" xref="S4.E40.m1.3.3.3.3.3.3.1.cmml">i</mi><msup id="S4.E40.m1.2.2.2.2.2.2.1"
    xref="S4.E40.m1.2.2.2.2.2.2.1.cmml"><mi id="S4.E40.m1.2.2.2.2.2.2.1.2" xref="S4.E40.m1.2.2.2.2.2.2.1.2.cmml">l</mi><mo
    id="S4.E40.m1.2.2.2.2.2.2.1.3" xref="S4.E40.m1.2.2.2.2.2.2.1.3.cmml">′</mo></msup></msubsup><mo
    id="S4.E40.m1.4.4.4.4.4.4" xref="S4.E40.m1.4.4.4.4.4.4.cmml">=</mo><mrow id="S4.E40.m1.113.113.2.112.43.43.43.1.3"><msub
    id="S4.E40.m1.113.113.2.112.43.43.43.1.3.5"><mi class="ltx_font_mathcaligraphic"
    id="S4.E40.m1.5.5.5.5.5.5" xref="S4.E40.m1.5.5.5.5.5.5.cmml">ℱ</mi><mrow id="S4.E40.m1.6.6.6.6.6.6.1"
    xref="S4.E40.m1.6.6.6.6.6.6.1.cmml"><mi id="S4.E40.m1.6.6.6.6.6.6.1.2" xref="S4.E40.m1.6.6.6.6.6.6.1.2.cmml">N</mi><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.6.6.6.6.6.6.1.1" xref="S4.E40.m1.6.6.6.6.6.6.1.1.cmml">​</mo><mi
    id="S4.E40.m1.6.6.6.6.6.6.1.3" xref="S4.E40.m1.6.6.6.6.6.6.1.3.cmml">N</mi></mrow></msub><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.113.113.2.112.43.43.43.1.3.4">​</mo><mrow
    id="S4.E40.m1.113.113.2.112.43.43.43.1.3.3.3"><mo stretchy="false" id="S4.E40.m1.7.7.7.7.7.7">(</mo><msubsup
    id="S4.E40.m1.113.113.2.112.43.43.43.1.1.1.1.1"><mi id="S4.E40.m1.8.8.8.8.8.8"
    xref="S4.E40.m1.8.8.8.8.8.8.cmml">𝐡</mi><mi id="S4.E40.m1.10.10.10.10.10.10.1"
    xref="S4.E40.m1.10.10.10.10.10.10.1.cmml">i</mi><mn id="S4.E40.m1.9.9.9.9.9.9.1"
    xref="S4.E40.m1.9.9.9.9.9.9.1.cmml">0</mn></msubsup><mo id="S4.E40.m1.11.11.11.11.11.11">,</mo><msubsup
    id="S4.E40.m1.113.113.2.112.43.43.43.1.2.2.2.2"><mi id="S4.E40.m1.12.12.12.12.12.12"
    xref="S4.E40.m1.12.12.12.12.12.12.cmml">𝐡</mi><mi id="S4.E40.m1.14.14.14.14.14.14.1"
    xref="S4.E40.m1.14.14.14.14.14.14.1.cmml">i</mi><mn id="S4.E40.m1.13.13.13.13.13.13.1"
    xref="S4.E40.m1.13.13.13.13.13.13.1.cmml">1</mn></msubsup><mo id="S4.E40.m1.15.15.15.15.15.15">,</mo><mi
    mathvariant="normal" id="S4.E40.m1.16.16.16.16.16.16" xref="S4.E40.m1.16.16.16.16.16.16.cmml">…</mi><mo
    id="S4.E40.m1.17.17.17.17.17.17">,</mo><msubsup id="S4.E40.m1.113.113.2.112.43.43.43.1.3.3.3.3"><mi
    id="S4.E40.m1.18.18.18.18.18.18" xref="S4.E40.m1.18.18.18.18.18.18.cmml">𝐡</mi><mi
    id="S4.E40.m1.20.20.20.20.20.20.1" xref="S4.E40.m1.20.20.20.20.20.20.1.cmml">i</mi><mi
    id="S4.E40.m1.19.19.19.19.19.19.1" xref="S4.E40.m1.19.19.19.19.19.19.1.cmml">l</mi></msubsup><mo
    stretchy="false" id="S4.E40.m1.21.21.21.21.21.21">)</mo></mrow></mrow></mrow><mo
    id="S4.E40.m1.22.22.22.22.22.22">,</mo><mrow id="S4.E40.m1.114.114.3.113.44.44.44.2"><msubsup
    id="S4.E40.m1.114.114.3.113.44.44.44.2.2"><mi id="S4.E40.m1.23.23.23.23.23.23"
    xref="S4.E40.m1.23.23.23.23.23.23.cmml">𝐡</mi><mi id="S4.E40.m1.25.25.25.25.25.25.1"
    xref="S4.E40.m1.25.25.25.25.25.25.1.cmml">i</mi><msup id="S4.E40.m1.24.24.24.24.24.24.1"
    xref="S4.E40.m1.24.24.24.24.24.24.1.cmml"><mi id="S4.E40.m1.24.24.24.24.24.24.1.2"
    xref="S4.E40.m1.24.24.24.24.24.24.1.2.cmml">l</mi><mo id="S4.E40.m1.24.24.24.24.24.24.1.3"
    xref="S4.E40.m1.24.24.24.24.24.24.1.3.cmml">′′</mo></msup></msubsup><mo id="S4.E40.m1.26.26.26.26.26.26"
    xref="S4.E40.m1.26.26.26.26.26.26.cmml">=</mo><mrow id="S4.E40.m1.114.114.3.113.44.44.44.2.1"><msub
    id="S4.E40.m1.114.114.3.113.44.44.44.2.1.3"><mi class="ltx_font_mathcaligraphic"
    id="S4.E40.m1.27.27.27.27.27.27" xref="S4.E40.m1.27.27.27.27.27.27.cmml">ℱ</mi><mrow
    id="S4.E40.m1.28.28.28.28.28.28.1" xref="S4.E40.m1.28.28.28.28.28.28.1.cmml"><mi
    id="S4.E40.m1.28.28.28.28.28.28.1.2" xref="S4.E40.m1.28.28.28.28.28.28.1.2.cmml">E</mi><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.28.28.28.28.28.28.1.1" xref="S4.E40.m1.28.28.28.28.28.28.1.1.cmml">​</mo><mi
    id="S4.E40.m1.28.28.28.28.28.28.1.3" xref="S4.E40.m1.28.28.28.28.28.28.1.3.cmml">N</mi></mrow></msub><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.114.114.3.113.44.44.44.2.1.2">​</mo><mrow
    id="S4.E40.m1.114.114.3.113.44.44.44.2.1.1.1"><mo stretchy="false" id="S4.E40.m1.29.29.29.29.29.29">(</mo><mrow
    id="S4.E40.m1.114.114.3.113.44.44.44.2.1.1.1.1"><mo stretchy="false" id="S4.E40.m1.30.30.30.30.30.30">{</mo><msubsup
    id="S4.E40.m1.114.114.3.113.44.44.44.2.1.1.1.1.1.1"><mi id="S4.E40.m1.31.31.31.31.31.31"
    xref="S4.E40.m1.31.31.31.31.31.31.cmml">𝐞</mi><mrow id="S4.E40.m1.33.33.33.33.33.33.1"
    xref="S4.E40.m1.33.33.33.33.33.33.1.cmml"><mi id="S4.E40.m1.33.33.33.33.33.33.1.2"
    xref="S4.E40.m1.33.33.33.33.33.33.1.2.cmml">i</mi><mo lspace="0em" rspace="0em"
    id="S4.E40.m1.33.33.33.33.33.33.1.1" xref="S4.E40.m1.33.33.33.33.33.33.1.1.cmml">​</mo><mi
    id="S4.E40.m1.33.33.33.33.33.33.1.3" xref="S4.E40.m1.33.33.33.33.33.33.1.3.cmml">j</mi></mrow><mi
    id="S4.E40.m1.32.32.32.32.32.32.1" xref="S4.E40.m1.32.32.32.32.32.32.1.cmml">l</mi></msubsup><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.34.34.34.34.34.34">&#124;</mo><mrow id="S4.E40.m1.114.114.3.113.44.44.44.2.1.1.1.1.2.2"><mi
    id="S4.E40.m1.35.35.35.35.35.35" xref="S4.E40.m1.35.35.35.35.35.35.cmml">j</mi><mo
    id="S4.E40.m1.36.36.36.36.36.36" xref="S4.E40.m1.36.36.36.36.36.36.cmml">∈</mo><mrow
    id="S4.E40.m1.114.114.3.113.44.44.44.2.1.1.1.1.2.2.1"><mi class="ltx_font_mathcaligraphic"
    id="S4.E40.m1.37.37.37.37.37.37" xref="S4.E40.m1.37.37.37.37.37.37.cmml">𝒩</mi><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.114.114.3.113.44.44.44.2.1.1.1.1.2.2.1.1">​</mo><mrow
    id="S4.E40.m1.114.114.3.113.44.44.44.2.1.1.1.1.2.2.1.2"><mo stretchy="false" id="S4.E40.m1.38.38.38.38.38.38">(</mo><mi
    id="S4.E40.m1.39.39.39.39.39.39" xref="S4.E40.m1.39.39.39.39.39.39.cmml">i</mi><mo
    stretchy="false" id="S4.E40.m1.40.40.40.40.40.40">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S4.E40.m1.41.41.41.41.41.41">}</mo></mrow><mo stretchy="false"
    id="S4.E40.m1.42.42.42.42.42.42">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    id="S4.E40.m1.117.117.6c"><mtd id="S4.E40.m1.117.117.6d"><mrow id="S4.E40.m1.116.116.5.115.39.39.39"><mrow
    id="S4.E40.m1.115.115.4.114.38.38.38.1"><msubsup id="S4.E40.m1.115.115.4.114.38.38.38.1.4"><mi
    id="S4.E40.m1.43.43.43.1.1.1" xref="S4.E40.m1.43.43.43.1.1.1.cmml">𝐞</mi><mrow
    id="S4.E40.m1.45.45.45.3.3.3.1" xref="S4.E40.m1.45.45.45.3.3.3.1.cmml"><mi id="S4.E40.m1.45.45.45.3.3.3.1.2"
    xref="S4.E40.m1.45.45.45.3.3.3.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E40.m1.45.45.45.3.3.3.1.1"
    xref="S4.E40.m1.45.45.45.3.3.3.1.1.cmml">​</mo><mi id="S4.E40.m1.45.45.45.3.3.3.1.3"
    xref="S4.E40.m1.45.45.45.3.3.3.1.3.cmml">j</mi></mrow><msup id="S4.E40.m1.44.44.44.2.2.2.1"
    xref="S4.E40.m1.44.44.44.2.2.2.1.cmml"><mi id="S4.E40.m1.44.44.44.2.2.2.1.2" xref="S4.E40.m1.44.44.44.2.2.2.1.2.cmml">l</mi><mo
    id="S4.E40.m1.44.44.44.2.2.2.1.3" xref="S4.E40.m1.44.44.44.2.2.2.1.3.cmml">′</mo></msup></msubsup><mo
    id="S4.E40.m1.46.46.46.4.4.4" xref="S4.E40.m1.46.46.46.4.4.4.cmml">=</mo><mrow
    id="S4.E40.m1.115.115.4.114.38.38.38.1.3"><msub id="S4.E40.m1.115.115.4.114.38.38.38.1.3.5"><mi
    class="ltx_font_mathcaligraphic" id="S4.E40.m1.47.47.47.5.5.5" xref="S4.E40.m1.47.47.47.5.5.5.cmml">ℱ</mi><mrow
    id="S4.E40.m1.48.48.48.6.6.6.1" xref="S4.E40.m1.48.48.48.6.6.6.1.cmml"><mi id="S4.E40.m1.48.48.48.6.6.6.1.2"
    xref="S4.E40.m1.48.48.48.6.6.6.1.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E40.m1.48.48.48.6.6.6.1.1"
    xref="S4.E40.m1.48.48.48.6.6.6.1.1.cmml">​</mo><mi id="S4.E40.m1.48.48.48.6.6.6.1.3"
    xref="S4.E40.m1.48.48.48.6.6.6.1.3.cmml">E</mi></mrow></msub><mo lspace="0em"
    rspace="0em" id="S4.E40.m1.115.115.4.114.38.38.38.1.3.4">​</mo><mrow id="S4.E40.m1.115.115.4.114.38.38.38.1.3.3.3"><mo
    stretchy="false" id="S4.E40.m1.49.49.49.7.7.7">(</mo><msubsup id="S4.E40.m1.115.115.4.114.38.38.38.1.1.1.1.1"><mi
    id="S4.E40.m1.50.50.50.8.8.8" xref="S4.E40.m1.50.50.50.8.8.8.cmml">𝐞</mi><mrow
    id="S4.E40.m1.52.52.52.10.10.10.1" xref="S4.E40.m1.52.52.52.10.10.10.1.cmml"><mi
    id="S4.E40.m1.52.52.52.10.10.10.1.2" xref="S4.E40.m1.52.52.52.10.10.10.1.2.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.52.52.52.10.10.10.1.1" xref="S4.E40.m1.52.52.52.10.10.10.1.1.cmml">​</mo><mi
    id="S4.E40.m1.52.52.52.10.10.10.1.3" xref="S4.E40.m1.52.52.52.10.10.10.1.3.cmml">j</mi></mrow><mn
    id="S4.E40.m1.51.51.51.9.9.9.1" xref="S4.E40.m1.51.51.51.9.9.9.1.cmml">0</mn></msubsup><mo
    id="S4.E40.m1.53.53.53.11.11.11">,</mo><msubsup id="S4.E40.m1.115.115.4.114.38.38.38.1.2.2.2.2"><mi
    id="S4.E40.m1.54.54.54.12.12.12" xref="S4.E40.m1.54.54.54.12.12.12.cmml">𝐞</mi><mrow
    id="S4.E40.m1.56.56.56.14.14.14.1" xref="S4.E40.m1.56.56.56.14.14.14.1.cmml"><mi
    id="S4.E40.m1.56.56.56.14.14.14.1.2" xref="S4.E40.m1.56.56.56.14.14.14.1.2.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.56.56.56.14.14.14.1.1" xref="S4.E40.m1.56.56.56.14.14.14.1.1.cmml">​</mo><mi
    id="S4.E40.m1.56.56.56.14.14.14.1.3" xref="S4.E40.m1.56.56.56.14.14.14.1.3.cmml">j</mi></mrow><mn
    id="S4.E40.m1.55.55.55.13.13.13.1" xref="S4.E40.m1.55.55.55.13.13.13.1.cmml">1</mn></msubsup><mo
    id="S4.E40.m1.57.57.57.15.15.15">,</mo><mi mathvariant="normal" id="S4.E40.m1.58.58.58.16.16.16"
    xref="S4.E40.m1.58.58.58.16.16.16.cmml">…</mi><mo id="S4.E40.m1.59.59.59.17.17.17">,</mo><msubsup
    id="S4.E40.m1.115.115.4.114.38.38.38.1.3.3.3.3"><mi id="S4.E40.m1.60.60.60.18.18.18"
    xref="S4.E40.m1.60.60.60.18.18.18.cmml">𝐞</mi><mrow id="S4.E40.m1.62.62.62.20.20.20.1"
    xref="S4.E40.m1.62.62.62.20.20.20.1.cmml"><mi id="S4.E40.m1.62.62.62.20.20.20.1.2"
    xref="S4.E40.m1.62.62.62.20.20.20.1.2.cmml">i</mi><mo lspace="0em" rspace="0em"
    id="S4.E40.m1.62.62.62.20.20.20.1.1" xref="S4.E40.m1.62.62.62.20.20.20.1.1.cmml">​</mo><mi
    id="S4.E40.m1.62.62.62.20.20.20.1.3" xref="S4.E40.m1.62.62.62.20.20.20.1.3.cmml">j</mi></mrow><mi
    id="S4.E40.m1.61.61.61.19.19.19.1" xref="S4.E40.m1.61.61.61.19.19.19.1.cmml">l</mi></msubsup><mo
    stretchy="false" id="S4.E40.m1.63.63.63.21.21.21">)</mo></mrow></mrow></mrow><mo
    id="S4.E40.m1.64.64.64.22.22.22">,</mo><mrow id="S4.E40.m1.116.116.5.115.39.39.39.2"><msubsup
    id="S4.E40.m1.116.116.5.115.39.39.39.2.3"><mi id="S4.E40.m1.65.65.65.23.23.23"
    xref="S4.E40.m1.65.65.65.23.23.23.cmml">𝐞</mi><mrow id="S4.E40.m1.67.67.67.25.25.25.1"
    xref="S4.E40.m1.67.67.67.25.25.25.1.cmml"><mi id="S4.E40.m1.67.67.67.25.25.25.1.2"
    xref="S4.E40.m1.67.67.67.25.25.25.1.2.cmml">i</mi><mo lspace="0em" rspace="0em"
    id="S4.E40.m1.67.67.67.25.25.25.1.1" xref="S4.E40.m1.67.67.67.25.25.25.1.1.cmml">​</mo><mi
    id="S4.E40.m1.67.67.67.25.25.25.1.3" xref="S4.E40.m1.67.67.67.25.25.25.1.3.cmml">j</mi></mrow><msup
    id="S4.E40.m1.66.66.66.24.24.24.1" xref="S4.E40.m1.66.66.66.24.24.24.1.cmml"><mi
    id="S4.E40.m1.66.66.66.24.24.24.1.2" xref="S4.E40.m1.66.66.66.24.24.24.1.2.cmml">l</mi><mo
    id="S4.E40.m1.66.66.66.24.24.24.1.3" xref="S4.E40.m1.66.66.66.24.24.24.1.3.cmml">′′</mo></msup></msubsup><mo
    id="S4.E40.m1.68.68.68.26.26.26" xref="S4.E40.m1.68.68.68.26.26.26.cmml">=</mo><mrow
    id="S4.E40.m1.116.116.5.115.39.39.39.2.2"><msub id="S4.E40.m1.116.116.5.115.39.39.39.2.2.4"><mi
    class="ltx_font_mathcaligraphic" id="S4.E40.m1.69.69.69.27.27.27" xref="S4.E40.m1.69.69.69.27.27.27.cmml">ℱ</mi><mrow
    id="S4.E40.m1.70.70.70.28.28.28.1" xref="S4.E40.m1.70.70.70.28.28.28.1.cmml"><mi
    id="S4.E40.m1.70.70.70.28.28.28.1.2" xref="S4.E40.m1.70.70.70.28.28.28.1.2.cmml">N</mi><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.70.70.70.28.28.28.1.1" xref="S4.E40.m1.70.70.70.28.28.28.1.1.cmml">​</mo><mi
    id="S4.E40.m1.70.70.70.28.28.28.1.3" xref="S4.E40.m1.70.70.70.28.28.28.1.3.cmml">E</mi></mrow></msub><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.116.116.5.115.39.39.39.2.2.3">​</mo><mrow
    id="S4.E40.m1.116.116.5.115.39.39.39.2.2.2.2"><mo stretchy="false" id="S4.E40.m1.71.71.71.29.29.29">(</mo><msubsup
    id="S4.E40.m1.116.116.5.115.39.39.39.2.1.1.1.1"><mi id="S4.E40.m1.72.72.72.30.30.30"
    xref="S4.E40.m1.72.72.72.30.30.30.cmml">𝐡</mi><mi id="S4.E40.m1.74.74.74.32.32.32.1"
    xref="S4.E40.m1.74.74.74.32.32.32.1.cmml">i</mi><mi id="S4.E40.m1.73.73.73.31.31.31.1"
    xref="S4.E40.m1.73.73.73.31.31.31.1.cmml">l</mi></msubsup><mo id="S4.E40.m1.75.75.75.33.33.33">,</mo><msubsup
    id="S4.E40.m1.116.116.5.115.39.39.39.2.2.2.2.2"><mi id="S4.E40.m1.76.76.76.34.34.34"
    xref="S4.E40.m1.76.76.76.34.34.34.cmml">𝐡</mi><mi id="S4.E40.m1.78.78.78.36.36.36.1"
    xref="S4.E40.m1.78.78.78.36.36.36.1.cmml">j</mi><mi id="S4.E40.m1.77.77.77.35.35.35.1"
    xref="S4.E40.m1.77.77.77.35.35.35.1.cmml">l</mi></msubsup><mo stretchy="false"
    id="S4.E40.m1.79.79.79.37.37.37">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    id="S4.E40.m1.117.117.6e"><mtd id="S4.E40.m1.117.117.6f"><mrow id="S4.E40.m1.117.117.6.116.33.33.33"><mrow
    id="S4.E40.m1.117.117.6.116.33.33.33.1"><mrow id="S4.E40.m1.117.117.6.116.33.33.33.1.1.1"><msubsup
    id="S4.E40.m1.117.117.6.116.33.33.33.1.1.1.3"><mi id="S4.E40.m1.80.80.80.1.1.1"
    xref="S4.E40.m1.80.80.80.1.1.1.cmml">𝐡</mi><mi id="S4.E40.m1.82.82.82.3.3.3.1"
    xref="S4.E40.m1.82.82.82.3.3.3.1.cmml">i</mi><mrow id="S4.E40.m1.81.81.81.2.2.2.1"
    xref="S4.E40.m1.81.81.81.2.2.2.1.cmml"><mi id="S4.E40.m1.81.81.81.2.2.2.1.2" xref="S4.E40.m1.81.81.81.2.2.2.1.2.cmml">l</mi><mo
    id="S4.E40.m1.81.81.81.2.2.2.1.1" xref="S4.E40.m1.81.81.81.2.2.2.1.1.cmml">+</mo><mn
    id="S4.E40.m1.81.81.81.2.2.2.1.3" xref="S4.E40.m1.81.81.81.2.2.2.1.3.cmml">1</mn></mrow></msubsup><mo
    id="S4.E40.m1.83.83.83.4.4.4" xref="S4.E40.m1.83.83.83.4.4.4.cmml">=</mo><mrow
    id="S4.E40.m1.117.117.6.116.33.33.33.1.1.1.2"><msub id="S4.E40.m1.117.117.6.116.33.33.33.1.1.1.2.4"><mi
    class="ltx_font_mathcaligraphic" id="S4.E40.m1.84.84.84.5.5.5" xref="S4.E40.m1.84.84.84.5.5.5.cmml">ℱ</mi><mrow
    id="S4.E40.m1.85.85.85.6.6.6.1" xref="S4.E40.m1.85.85.85.6.6.6.1.cmml"><mi id="S4.E40.m1.85.85.85.6.6.6.1.2"
    xref="S4.E40.m1.85.85.85.6.6.6.1.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S4.E40.m1.85.85.85.6.6.6.1.1"
    xref="S4.E40.m1.85.85.85.6.6.6.1.1.cmml">​</mo><mi id="S4.E40.m1.85.85.85.6.6.6.1.3"
    xref="S4.E40.m1.85.85.85.6.6.6.1.3.cmml">N</mi></mrow></msub><mo lspace="0em"
    rspace="0em" id="S4.E40.m1.117.117.6.116.33.33.33.1.1.1.2.3">​</mo><mrow id="S4.E40.m1.117.117.6.116.33.33.33.1.1.1.2.2.2"><mo
    stretchy="false" id="S4.E40.m1.86.86.86.7.7.7">(</mo><msubsup id="S4.E40.m1.117.117.6.116.33.33.33.1.1.1.1.1.1.1"><mi
    id="S4.E40.m1.87.87.87.8.8.8" xref="S4.E40.m1.87.87.87.8.8.8.cmml">𝐡</mi><mi id="S4.E40.m1.89.89.89.10.10.10.1"
    xref="S4.E40.m1.89.89.89.10.10.10.1.cmml">i</mi><msup id="S4.E40.m1.88.88.88.9.9.9.1"
    xref="S4.E40.m1.88.88.88.9.9.9.1.cmml"><mi id="S4.E40.m1.88.88.88.9.9.9.1.2" xref="S4.E40.m1.88.88.88.9.9.9.1.2.cmml">l</mi><mo
    id="S4.E40.m1.88.88.88.9.9.9.1.3" xref="S4.E40.m1.88.88.88.9.9.9.1.3.cmml">′</mo></msup></msubsup><mo
    id="S4.E40.m1.90.90.90.11.11.11">,</mo><msubsup id="S4.E40.m1.117.117.6.116.33.33.33.1.1.1.2.2.2.2"><mi
    id="S4.E40.m1.91.91.91.12.12.12" xref="S4.E40.m1.91.91.91.12.12.12.cmml">𝐡</mi><mi
    id="S4.E40.m1.93.93.93.14.14.14.1" xref="S4.E40.m1.93.93.93.14.14.14.1.cmml">i</mi><msup
    id="S4.E40.m1.92.92.92.13.13.13.1" xref="S4.E40.m1.92.92.92.13.13.13.1.cmml"><mi
    id="S4.E40.m1.92.92.92.13.13.13.1.2" xref="S4.E40.m1.92.92.92.13.13.13.1.2.cmml">l</mi><mo
    id="S4.E40.m1.92.92.92.13.13.13.1.3" xref="S4.E40.m1.92.92.92.13.13.13.1.3.cmml">′′</mo></msup></msubsup><mo
    stretchy="false" id="S4.E40.m1.94.94.94.15.15.15">)</mo></mrow></mrow></mrow><mo
    id="S4.E40.m1.95.95.95.16.16.16">,</mo><mrow id="S4.E40.m1.117.117.6.116.33.33.33.1.2.2"><msubsup
    id="S4.E40.m1.117.117.6.116.33.33.33.1.2.2.3"><mi id="S4.E40.m1.96.96.96.17.17.17"
    xref="S4.E40.m1.96.96.96.17.17.17.cmml">𝐞</mi><mrow id="S4.E40.m1.98.98.98.19.19.19.1"
    xref="S4.E40.m1.98.98.98.19.19.19.1.cmml"><mi id="S4.E40.m1.98.98.98.19.19.19.1.2"
    xref="S4.E40.m1.98.98.98.19.19.19.1.2.cmml">i</mi><mo lspace="0em" rspace="0em"
    id="S4.E40.m1.98.98.98.19.19.19.1.1" xref="S4.E40.m1.98.98.98.19.19.19.1.1.cmml">​</mo><mi
    id="S4.E40.m1.98.98.98.19.19.19.1.3" xref="S4.E40.m1.98.98.98.19.19.19.1.3.cmml">j</mi></mrow><mrow
    id="S4.E40.m1.97.97.97.18.18.18.1" xref="S4.E40.m1.97.97.97.18.18.18.1.cmml"><mi
    id="S4.E40.m1.97.97.97.18.18.18.1.2" xref="S4.E40.m1.97.97.97.18.18.18.1.2.cmml">l</mi><mo
    id="S4.E40.m1.97.97.97.18.18.18.1.1" xref="S4.E40.m1.97.97.97.18.18.18.1.1.cmml">+</mo><mn
    id="S4.E40.m1.97.97.97.18.18.18.1.3" xref="S4.E40.m1.97.97.97.18.18.18.1.3.cmml">1</mn></mrow></msubsup><mo
    id="S4.E40.m1.99.99.99.20.20.20" xref="S4.E40.m1.99.99.99.20.20.20.cmml">=</mo><mrow
    id="S4.E40.m1.117.117.6.116.33.33.33.1.2.2.2"><msub id="S4.E40.m1.117.117.6.116.33.33.33.1.2.2.2.4"><mi
    class="ltx_font_mathcaligraphic" id="S4.E40.m1.100.100.100.21.21.21" xref="S4.E40.m1.100.100.100.21.21.21.cmml">ℱ</mi><mrow
    id="S4.E40.m1.101.101.101.22.22.22.1" xref="S4.E40.m1.101.101.101.22.22.22.1.cmml"><mi
    id="S4.E40.m1.101.101.101.22.22.22.1.2" xref="S4.E40.m1.101.101.101.22.22.22.1.2.cmml">E</mi><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.101.101.101.22.22.22.1.1" xref="S4.E40.m1.101.101.101.22.22.22.1.1.cmml">​</mo><mi
    id="S4.E40.m1.101.101.101.22.22.22.1.3" xref="S4.E40.m1.101.101.101.22.22.22.1.3.cmml">E</mi></mrow></msub><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.117.117.6.116.33.33.33.1.2.2.2.3">​</mo><mrow
    id="S4.E40.m1.117.117.6.116.33.33.33.1.2.2.2.2.2"><mo stretchy="false" id="S4.E40.m1.102.102.102.23.23.23">(</mo><msubsup
    id="S4.E40.m1.117.117.6.116.33.33.33.1.2.2.1.1.1.1"><mi id="S4.E40.m1.103.103.103.24.24.24"
    xref="S4.E40.m1.103.103.103.24.24.24.cmml">𝐞</mi><mrow id="S4.E40.m1.105.105.105.26.26.26.1"
    xref="S4.E40.m1.105.105.105.26.26.26.1.cmml"><mi id="S4.E40.m1.105.105.105.26.26.26.1.2"
    xref="S4.E40.m1.105.105.105.26.26.26.1.2.cmml">i</mi><mo lspace="0em" rspace="0em"
    id="S4.E40.m1.105.105.105.26.26.26.1.1" xref="S4.E40.m1.105.105.105.26.26.26.1.1.cmml">​</mo><mi
    id="S4.E40.m1.105.105.105.26.26.26.1.3" xref="S4.E40.m1.105.105.105.26.26.26.1.3.cmml">j</mi></mrow><msup
    id="S4.E40.m1.104.104.104.25.25.25.1" xref="S4.E40.m1.104.104.104.25.25.25.1.cmml"><mi
    id="S4.E40.m1.104.104.104.25.25.25.1.2" xref="S4.E40.m1.104.104.104.25.25.25.1.2.cmml">l</mi><mo
    id="S4.E40.m1.104.104.104.25.25.25.1.3" xref="S4.E40.m1.104.104.104.25.25.25.1.3.cmml">′</mo></msup></msubsup><mo
    id="S4.E40.m1.106.106.106.27.27.27">,</mo><msubsup id="S4.E40.m1.117.117.6.116.33.33.33.1.2.2.2.2.2.2"><mi
    id="S4.E40.m1.107.107.107.28.28.28" xref="S4.E40.m1.107.107.107.28.28.28.cmml">𝐞</mi><mrow
    id="S4.E40.m1.109.109.109.30.30.30.1" xref="S4.E40.m1.109.109.109.30.30.30.1.cmml"><mi
    id="S4.E40.m1.109.109.109.30.30.30.1.2" xref="S4.E40.m1.109.109.109.30.30.30.1.2.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S4.E40.m1.109.109.109.30.30.30.1.1" xref="S4.E40.m1.109.109.109.30.30.30.1.1.cmml">​</mo><mi
    id="S4.E40.m1.109.109.109.30.30.30.1.3" xref="S4.E40.m1.109.109.109.30.30.30.1.3.cmml">j</mi></mrow><msup
    id="S4.E40.m1.108.108.108.29.29.29.1" xref="S4.E40.m1.108.108.108.29.29.29.1.cmml"><mi
    id="S4.E40.m1.108.108.108.29.29.29.1.2" xref="S4.E40.m1.108.108.108.29.29.29.1.2.cmml">l</mi><mo
    id="S4.E40.m1.108.108.108.29.29.29.1.3" xref="S4.E40.m1.108.108.108.29.29.29.1.3.cmml">′′</mo></msup></msubsup><mo
    stretchy="false" id="S4.E40.m1.110.110.110.31.31.31">)</mo></mrow></mrow></mrow></mrow><mo
    id="S4.E40.m1.111.111.111.32.32.32">,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S4.E40.m1.117b"><apply id="S4.E40.m1.112.112.1.1.1.4.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.4a.cmml">formulae-sequence</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.3a.cmml">formulae-sequence</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.1.1.cmml"><apply id="S4.E40.m1.112.112.1.1.1.1.1.1.1.5.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.1.1.5.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.1.1.5.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.1.1.5.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.1.1.1.1.1.1.cmml" xref="S4.E40.m1.1.1.1.1.1.1">𝐡</ci><apply id="S4.E40.m1.2.2.2.2.2.2.1.cmml"
    xref="S4.E40.m1.2.2.2.2.2.2.1"><csymbol cd="ambiguous" id="S4.E40.m1.2.2.2.2.2.2.1.1.cmml"
    xref="S4.E40.m1.2.2.2.2.2.2.1">superscript</csymbol><ci id="S4.E40.m1.2.2.2.2.2.2.1.2.cmml"
    xref="S4.E40.m1.2.2.2.2.2.2.1.2">𝑙</ci><ci id="S4.E40.m1.2.2.2.2.2.2.1.3.cmml"
    xref="S4.E40.m1.2.2.2.2.2.2.1.3">′</ci></apply></apply><ci id="S4.E40.m1.3.3.3.3.3.3.1.cmml"
    xref="S4.E40.m1.3.3.3.3.3.3.1">𝑖</ci></apply><apply id="S4.E40.m1.112.112.1.1.1.1.1.1.1.3.cmml"><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.1.1.3.5.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.1.1.3.5.1.cmml">subscript</csymbol><ci
    id="S4.E40.m1.5.5.5.5.5.5.cmml" xref="S4.E40.m1.5.5.5.5.5.5">ℱ</ci><apply id="S4.E40.m1.6.6.6.6.6.6.1.cmml"
    xref="S4.E40.m1.6.6.6.6.6.6.1"><ci id="S4.E40.m1.6.6.6.6.6.6.1.2.cmml" xref="S4.E40.m1.6.6.6.6.6.6.1.2">𝑁</ci><ci
    id="S4.E40.m1.6.6.6.6.6.6.1.3.cmml" xref="S4.E40.m1.6.6.6.6.6.6.1.3">𝑁</ci></apply></apply><vector
    id="S4.E40.m1.112.112.1.1.1.1.1.1.1.3.3.4.cmml"><apply id="S4.E40.m1.112.112.1.1.1.1.1.1.1.1.1.1.1.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.1.1.1.1.1.1.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.8.8.8.8.8.8.cmml" xref="S4.E40.m1.8.8.8.8.8.8">𝐡</ci><cn type="integer"
    id="S4.E40.m1.9.9.9.9.9.9.1.cmml" xref="S4.E40.m1.9.9.9.9.9.9.1">0</cn></apply><ci
    id="S4.E40.m1.10.10.10.10.10.10.1.cmml" xref="S4.E40.m1.10.10.10.10.10.10.1">𝑖</ci></apply><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.1.1.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.1.1.2.2.2.2.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.1.1.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.1.1.2.2.2.2.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.12.12.12.12.12.12.cmml" xref="S4.E40.m1.12.12.12.12.12.12">𝐡</ci><cn
    type="integer" id="S4.E40.m1.13.13.13.13.13.13.1.cmml" xref="S4.E40.m1.13.13.13.13.13.13.1">1</cn></apply><ci
    id="S4.E40.m1.14.14.14.14.14.14.1.cmml" xref="S4.E40.m1.14.14.14.14.14.14.1">𝑖</ci></apply><ci
    id="S4.E40.m1.16.16.16.16.16.16.cmml" xref="S4.E40.m1.16.16.16.16.16.16">…</ci><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.1.1.3.3.3.3.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.1.1.3.3.3.3.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.1.1.3.3.3.3.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.1.1.3.3.3.3.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.18.18.18.18.18.18.cmml" xref="S4.E40.m1.18.18.18.18.18.18">𝐡</ci><ci
    id="S4.E40.m1.19.19.19.19.19.19.1.cmml" xref="S4.E40.m1.19.19.19.19.19.19.1">𝑙</ci></apply><ci
    id="S4.E40.m1.20.20.20.20.20.20.1.cmml" xref="S4.E40.m1.20.20.20.20.20.20.1">𝑖</ci></apply></vector></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.cmml"><apply id="S4.E40.m1.112.112.1.1.1.1.1.2.2b.cmml"><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.6.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.6.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.6.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.6.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.23.23.23.23.23.23.cmml" xref="S4.E40.m1.23.23.23.23.23.23">𝐡</ci><apply
    id="S4.E40.m1.24.24.24.24.24.24.1.cmml" xref="S4.E40.m1.24.24.24.24.24.24.1"><csymbol
    cd="ambiguous" id="S4.E40.m1.24.24.24.24.24.24.1.1.cmml" xref="S4.E40.m1.24.24.24.24.24.24.1">superscript</csymbol><ci
    id="S4.E40.m1.24.24.24.24.24.24.1.2.cmml" xref="S4.E40.m1.24.24.24.24.24.24.1.2">𝑙</ci><ci
    id="S4.E40.m1.24.24.24.24.24.24.1.3.cmml" xref="S4.E40.m1.24.24.24.24.24.24.1.3">′′</ci></apply></apply><ci
    id="S4.E40.m1.25.25.25.25.25.25.1.cmml" xref="S4.E40.m1.25.25.25.25.25.25.1">𝑖</ci></apply><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.cmml"><apply id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.3.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.3.1.cmml">subscript</csymbol><ci
    id="S4.E40.m1.27.27.27.27.27.27.cmml" xref="S4.E40.m1.27.27.27.27.27.27">ℱ</ci><apply
    id="S4.E40.m1.28.28.28.28.28.28.1.cmml" xref="S4.E40.m1.28.28.28.28.28.28.1"><ci
    id="S4.E40.m1.28.28.28.28.28.28.1.2.cmml" xref="S4.E40.m1.28.28.28.28.28.28.1.2">𝐸</ci><ci
    id="S4.E40.m1.28.28.28.28.28.28.1.3.cmml" xref="S4.E40.m1.28.28.28.28.28.28.1.3">𝑁</ci></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.1.1.1.3.cmml"><csymbol cd="latexml" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.1.1.1.3.1.cmml">conditional-set</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.1.1.1.1.1.cmml"><csymbol cd="ambiguous"
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.1.1.1.1.1.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.1.1.1.1.1.2.cmml"><csymbol cd="ambiguous"
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.1.1.1.1.1.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.31.31.31.31.31.31.cmml" xref="S4.E40.m1.31.31.31.31.31.31">𝐞</ci><ci
    id="S4.E40.m1.32.32.32.32.32.32.1.cmml" xref="S4.E40.m1.32.32.32.32.32.32.1">𝑙</ci></apply><apply
    id="S4.E40.m1.33.33.33.33.33.33.1.cmml" xref="S4.E40.m1.33.33.33.33.33.33.1"><ci
    id="S4.E40.m1.33.33.33.33.33.33.1.2.cmml" xref="S4.E40.m1.33.33.33.33.33.33.1.2">𝑖</ci><ci
    id="S4.E40.m1.33.33.33.33.33.33.1.3.cmml" xref="S4.E40.m1.33.33.33.33.33.33.1.3">𝑗</ci></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.1.1.1.2.2.cmml"><ci id="S4.E40.m1.35.35.35.35.35.35.cmml"
    xref="S4.E40.m1.35.35.35.35.35.35">𝑗</ci><apply id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.1.1.1.2.2.3.cmml"><ci
    id="S4.E40.m1.37.37.37.37.37.37.cmml" xref="S4.E40.m1.37.37.37.37.37.37">𝒩</ci><ci
    id="S4.E40.m1.39.39.39.39.39.39.cmml" xref="S4.E40.m1.39.39.39.39.39.39">𝑖</ci></apply></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.4.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.4.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.4.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.1.4.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.43.43.43.1.1.1.cmml" xref="S4.E40.m1.43.43.43.1.1.1">𝐞</ci><apply
    id="S4.E40.m1.44.44.44.2.2.2.1.cmml" xref="S4.E40.m1.44.44.44.2.2.2.1"><csymbol
    cd="ambiguous" id="S4.E40.m1.44.44.44.2.2.2.1.1.cmml" xref="S4.E40.m1.44.44.44.2.2.2.1">superscript</csymbol><ci
    id="S4.E40.m1.44.44.44.2.2.2.1.2.cmml" xref="S4.E40.m1.44.44.44.2.2.2.1.2">𝑙</ci><ci
    id="S4.E40.m1.44.44.44.2.2.2.1.3.cmml" xref="S4.E40.m1.44.44.44.2.2.2.1.3">′</ci></apply></apply><apply
    id="S4.E40.m1.45.45.45.3.3.3.1.cmml" xref="S4.E40.m1.45.45.45.3.3.3.1"><ci id="S4.E40.m1.45.45.45.3.3.3.1.2.cmml"
    xref="S4.E40.m1.45.45.45.3.3.3.1.2">𝑖</ci><ci id="S4.E40.m1.45.45.45.3.3.3.1.3.cmml"
    xref="S4.E40.m1.45.45.45.3.3.3.1.3">𝑗</ci></apply></apply></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2c.cmml"><apply id="S4.E40.m1.112.112.1.1.1.1.1.2.2.4.cmml"><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.4.5.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.4.5.1.cmml">subscript</csymbol><ci
    id="S4.E40.m1.47.47.47.5.5.5.cmml" xref="S4.E40.m1.47.47.47.5.5.5">ℱ</ci><apply
    id="S4.E40.m1.48.48.48.6.6.6.1.cmml" xref="S4.E40.m1.48.48.48.6.6.6.1"><ci id="S4.E40.m1.48.48.48.6.6.6.1.2.cmml"
    xref="S4.E40.m1.48.48.48.6.6.6.1.2">𝐸</ci><ci id="S4.E40.m1.48.48.48.6.6.6.1.3.cmml"
    xref="S4.E40.m1.48.48.48.6.6.6.1.3">𝐸</ci></apply></apply><vector id="S4.E40.m1.112.112.1.1.1.1.1.2.2.4.3.4.cmml"><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.2.1.1.1.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.2.1.1.1.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.2.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.2.1.1.1.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.50.50.50.8.8.8.cmml" xref="S4.E40.m1.50.50.50.8.8.8">𝐞</ci><cn type="integer"
    id="S4.E40.m1.51.51.51.9.9.9.1.cmml" xref="S4.E40.m1.51.51.51.9.9.9.1">0</cn></apply><apply
    id="S4.E40.m1.52.52.52.10.10.10.1.cmml" xref="S4.E40.m1.52.52.52.10.10.10.1"><ci
    id="S4.E40.m1.52.52.52.10.10.10.1.2.cmml" xref="S4.E40.m1.52.52.52.10.10.10.1.2">𝑖</ci><ci
    id="S4.E40.m1.52.52.52.10.10.10.1.3.cmml" xref="S4.E40.m1.52.52.52.10.10.10.1.3">𝑗</ci></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.3.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.3.2.2.2.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.3.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.3.2.2.2.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.54.54.54.12.12.12.cmml" xref="S4.E40.m1.54.54.54.12.12.12">𝐞</ci><cn
    type="integer" id="S4.E40.m1.55.55.55.13.13.13.1.cmml" xref="S4.E40.m1.55.55.55.13.13.13.1">1</cn></apply><apply
    id="S4.E40.m1.56.56.56.14.14.14.1.cmml" xref="S4.E40.m1.56.56.56.14.14.14.1"><ci
    id="S4.E40.m1.56.56.56.14.14.14.1.2.cmml" xref="S4.E40.m1.56.56.56.14.14.14.1.2">𝑖</ci><ci
    id="S4.E40.m1.56.56.56.14.14.14.1.3.cmml" xref="S4.E40.m1.56.56.56.14.14.14.1.3">𝑗</ci></apply></apply><ci
    id="S4.E40.m1.58.58.58.16.16.16.cmml" xref="S4.E40.m1.58.58.58.16.16.16">…</ci><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.4.3.3.3.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.4.3.3.3.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.1.1.2.2.4.3.3.3.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.1.1.2.2.4.3.3.3.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.60.60.60.18.18.18.cmml" xref="S4.E40.m1.60.60.60.18.18.18">𝐞</ci><ci
    id="S4.E40.m1.61.61.61.19.19.19.1.cmml" xref="S4.E40.m1.61.61.61.19.19.19.1">𝑙</ci></apply><apply
    id="S4.E40.m1.62.62.62.20.20.20.1.cmml" xref="S4.E40.m1.62.62.62.20.20.20.1"><ci
    id="S4.E40.m1.62.62.62.20.20.20.1.2.cmml" xref="S4.E40.m1.62.62.62.20.20.20.1.2">𝑖</ci><ci
    id="S4.E40.m1.62.62.62.20.20.20.1.3.cmml" xref="S4.E40.m1.62.62.62.20.20.20.1.3">𝑗</ci></apply></apply></vector></apply></apply></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.cmml"><apply id="S4.E40.m1.112.112.1.1.1.2.2b.cmml"><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.6.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.6.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.6.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.6.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.65.65.65.23.23.23.cmml" xref="S4.E40.m1.65.65.65.23.23.23">𝐞</ci><apply
    id="S4.E40.m1.66.66.66.24.24.24.1.cmml" xref="S4.E40.m1.66.66.66.24.24.24.1"><csymbol
    cd="ambiguous" id="S4.E40.m1.66.66.66.24.24.24.1.1.cmml" xref="S4.E40.m1.66.66.66.24.24.24.1">superscript</csymbol><ci
    id="S4.E40.m1.66.66.66.24.24.24.1.2.cmml" xref="S4.E40.m1.66.66.66.24.24.24.1.2">𝑙</ci><ci
    id="S4.E40.m1.66.66.66.24.24.24.1.3.cmml" xref="S4.E40.m1.66.66.66.24.24.24.1.3">′′</ci></apply></apply><apply
    id="S4.E40.m1.67.67.67.25.25.25.1.cmml" xref="S4.E40.m1.67.67.67.25.25.25.1"><ci
    id="S4.E40.m1.67.67.67.25.25.25.1.2.cmml" xref="S4.E40.m1.67.67.67.25.25.25.1.2">𝑖</ci><ci
    id="S4.E40.m1.67.67.67.25.25.25.1.3.cmml" xref="S4.E40.m1.67.67.67.25.25.25.1.3">𝑗</ci></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.2.cmml"><apply id="S4.E40.m1.112.112.1.1.1.2.2.2.4.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.2.4.1.cmml">subscript</csymbol><ci
    id="S4.E40.m1.69.69.69.27.27.27.cmml" xref="S4.E40.m1.69.69.69.27.27.27">ℱ</ci><apply
    id="S4.E40.m1.70.70.70.28.28.28.1.cmml" xref="S4.E40.m1.70.70.70.28.28.28.1"><ci
    id="S4.E40.m1.70.70.70.28.28.28.1.2.cmml" xref="S4.E40.m1.70.70.70.28.28.28.1.2">𝑁</ci><ci
    id="S4.E40.m1.70.70.70.28.28.28.1.3.cmml" xref="S4.E40.m1.70.70.70.28.28.28.1.3">𝐸</ci></apply></apply><interval
    closure="open" id="S4.E40.m1.112.112.1.1.1.2.2.2.2.3.cmml"><apply id="S4.E40.m1.112.112.1.1.1.2.2.1.1.1.1.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.1.1.1.1.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.1.1.1.1.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.72.72.72.30.30.30.cmml" xref="S4.E40.m1.72.72.72.30.30.30">𝐡</ci><ci
    id="S4.E40.m1.73.73.73.31.31.31.1.cmml" xref="S4.E40.m1.73.73.73.31.31.31.1">𝑙</ci></apply><ci
    id="S4.E40.m1.74.74.74.32.32.32.1.cmml" xref="S4.E40.m1.74.74.74.32.32.32.1">𝑖</ci></apply><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.2.2.2.2.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.2.2.2.2.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.76.76.76.34.34.34.cmml" xref="S4.E40.m1.76.76.76.34.34.34">𝐡</ci><ci
    id="S4.E40.m1.77.77.77.35.35.35.1.cmml" xref="S4.E40.m1.77.77.77.35.35.35.1">𝑙</ci></apply><ci
    id="S4.E40.m1.78.78.78.36.36.36.1.cmml" xref="S4.E40.m1.78.78.78.36.36.36.1">𝑗</ci></apply></interval><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.2.5.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.2.5.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.2.5.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.2.5.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.80.80.80.1.1.1.cmml" xref="S4.E40.m1.80.80.80.1.1.1">𝐡</ci><apply
    id="S4.E40.m1.81.81.81.2.2.2.1.cmml" xref="S4.E40.m1.81.81.81.2.2.2.1"><ci id="S4.E40.m1.81.81.81.2.2.2.1.2.cmml"
    xref="S4.E40.m1.81.81.81.2.2.2.1.2">𝑙</ci><cn type="integer" id="S4.E40.m1.81.81.81.2.2.2.1.3.cmml"
    xref="S4.E40.m1.81.81.81.2.2.2.1.3">1</cn></apply></apply><ci id="S4.E40.m1.82.82.82.3.3.3.1.cmml"
    xref="S4.E40.m1.82.82.82.3.3.3.1">𝑖</ci></apply></apply></apply><apply id="S4.E40.m1.112.112.1.1.1.2.2c.cmml"><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.4.cmml"><apply id="S4.E40.m1.112.112.1.1.1.2.2.4.4.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.4.4.1.cmml">subscript</csymbol><ci
    id="S4.E40.m1.84.84.84.5.5.5.cmml" xref="S4.E40.m1.84.84.84.5.5.5">ℱ</ci><apply
    id="S4.E40.m1.85.85.85.6.6.6.1.cmml" xref="S4.E40.m1.85.85.85.6.6.6.1"><ci id="S4.E40.m1.85.85.85.6.6.6.1.2.cmml"
    xref="S4.E40.m1.85.85.85.6.6.6.1.2">𝑁</ci><ci id="S4.E40.m1.85.85.85.6.6.6.1.3.cmml"
    xref="S4.E40.m1.85.85.85.6.6.6.1.3">𝑁</ci></apply></apply><interval closure="open"
    id="S4.E40.m1.112.112.1.1.1.2.2.4.2.3.cmml"><apply id="S4.E40.m1.112.112.1.1.1.2.2.3.1.1.1.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.3.1.1.1.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.3.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.3.1.1.1.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.87.87.87.8.8.8.cmml" xref="S4.E40.m1.87.87.87.8.8.8">𝐡</ci><apply
    id="S4.E40.m1.88.88.88.9.9.9.1.cmml" xref="S4.E40.m1.88.88.88.9.9.9.1"><csymbol
    cd="ambiguous" id="S4.E40.m1.88.88.88.9.9.9.1.1.cmml" xref="S4.E40.m1.88.88.88.9.9.9.1">superscript</csymbol><ci
    id="S4.E40.m1.88.88.88.9.9.9.1.2.cmml" xref="S4.E40.m1.88.88.88.9.9.9.1.2">𝑙</ci><ci
    id="S4.E40.m1.88.88.88.9.9.9.1.3.cmml" xref="S4.E40.m1.88.88.88.9.9.9.1.3">′</ci></apply></apply><ci
    id="S4.E40.m1.89.89.89.10.10.10.1.cmml" xref="S4.E40.m1.89.89.89.10.10.10.1">𝑖</ci></apply><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.4.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.4.2.2.2.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.2.2.4.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.2.2.4.2.2.2.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.91.91.91.12.12.12.cmml" xref="S4.E40.m1.91.91.91.12.12.12">𝐡</ci><apply
    id="S4.E40.m1.92.92.92.13.13.13.1.cmml" xref="S4.E40.m1.92.92.92.13.13.13.1"><csymbol
    cd="ambiguous" id="S4.E40.m1.92.92.92.13.13.13.1.1.cmml" xref="S4.E40.m1.92.92.92.13.13.13.1">superscript</csymbol><ci
    id="S4.E40.m1.92.92.92.13.13.13.1.2.cmml" xref="S4.E40.m1.92.92.92.13.13.13.1.2">𝑙</ci><ci
    id="S4.E40.m1.92.92.92.13.13.13.1.3.cmml" xref="S4.E40.m1.92.92.92.13.13.13.1.3">′′</ci></apply></apply><ci
    id="S4.E40.m1.93.93.93.14.14.14.1.cmml" xref="S4.E40.m1.93.93.93.14.14.14.1">𝑖</ci></apply></interval></apply></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.3.3.cmml"><apply id="S4.E40.m1.112.112.1.1.1.3.3.4.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.3.3.4.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.3.3.4.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.3.3.4.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.96.96.96.17.17.17.cmml" xref="S4.E40.m1.96.96.96.17.17.17">𝐞</ci><apply
    id="S4.E40.m1.97.97.97.18.18.18.1.cmml" xref="S4.E40.m1.97.97.97.18.18.18.1"><ci
    id="S4.E40.m1.97.97.97.18.18.18.1.2.cmml" xref="S4.E40.m1.97.97.97.18.18.18.1.2">𝑙</ci><cn
    type="integer" id="S4.E40.m1.97.97.97.18.18.18.1.3.cmml" xref="S4.E40.m1.97.97.97.18.18.18.1.3">1</cn></apply></apply><apply
    id="S4.E40.m1.98.98.98.19.19.19.1.cmml" xref="S4.E40.m1.98.98.98.19.19.19.1"><ci
    id="S4.E40.m1.98.98.98.19.19.19.1.2.cmml" xref="S4.E40.m1.98.98.98.19.19.19.1.2">𝑖</ci><ci
    id="S4.E40.m1.98.98.98.19.19.19.1.3.cmml" xref="S4.E40.m1.98.98.98.19.19.19.1.3">𝑗</ci></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.3.3.2.cmml"><apply id="S4.E40.m1.112.112.1.1.1.3.3.2.4.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.3.3.2.4.1.cmml">subscript</csymbol><ci
    id="S4.E40.m1.100.100.100.21.21.21.cmml" xref="S4.E40.m1.100.100.100.21.21.21">ℱ</ci><apply
    id="S4.E40.m1.101.101.101.22.22.22.1.cmml" xref="S4.E40.m1.101.101.101.22.22.22.1"><ci
    id="S4.E40.m1.101.101.101.22.22.22.1.2.cmml" xref="S4.E40.m1.101.101.101.22.22.22.1.2">𝐸</ci><ci
    id="S4.E40.m1.101.101.101.22.22.22.1.3.cmml" xref="S4.E40.m1.101.101.101.22.22.22.1.3">𝐸</ci></apply></apply><interval
    closure="open" id="S4.E40.m1.112.112.1.1.1.3.3.2.2.3.cmml"><apply id="S4.E40.m1.112.112.1.1.1.3.3.1.1.1.1.cmml"><csymbol
    cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.3.3.1.1.1.1.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.3.3.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.3.3.1.1.1.1.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.103.103.103.24.24.24.cmml" xref="S4.E40.m1.103.103.103.24.24.24">𝐞</ci><apply
    id="S4.E40.m1.104.104.104.25.25.25.1.cmml" xref="S4.E40.m1.104.104.104.25.25.25.1"><csymbol
    cd="ambiguous" id="S4.E40.m1.104.104.104.25.25.25.1.1.cmml" xref="S4.E40.m1.104.104.104.25.25.25.1">superscript</csymbol><ci
    id="S4.E40.m1.104.104.104.25.25.25.1.2.cmml" xref="S4.E40.m1.104.104.104.25.25.25.1.2">𝑙</ci><ci
    id="S4.E40.m1.104.104.104.25.25.25.1.3.cmml" xref="S4.E40.m1.104.104.104.25.25.25.1.3">′</ci></apply></apply><apply
    id="S4.E40.m1.105.105.105.26.26.26.1.cmml" xref="S4.E40.m1.105.105.105.26.26.26.1"><ci
    id="S4.E40.m1.105.105.105.26.26.26.1.2.cmml" xref="S4.E40.m1.105.105.105.26.26.26.1.2">𝑖</ci><ci
    id="S4.E40.m1.105.105.105.26.26.26.1.3.cmml" xref="S4.E40.m1.105.105.105.26.26.26.1.3">𝑗</ci></apply></apply><apply
    id="S4.E40.m1.112.112.1.1.1.3.3.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.3.3.2.2.2.2.1.cmml">subscript</csymbol><apply
    id="S4.E40.m1.112.112.1.1.1.3.3.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S4.E40.m1.112.112.1.1.1.3.3.2.2.2.2.2.1.cmml">superscript</csymbol><ci
    id="S4.E40.m1.107.107.107.28.28.28.cmml" xref="S4.E40.m1.107.107.107.28.28.28">𝐞</ci><apply
    id="S4.E40.m1.108.108.108.29.29.29.1.cmml" xref="S4.E40.m1.108.108.108.29.29.29.1"><csymbol
    cd="ambiguous" id="S4.E40.m1.108.108.108.29.29.29.1.1.cmml" xref="S4.E40.m1.108.108.108.29.29.29.1">superscript</csymbol><ci
    id="S4.E40.m1.108.108.108.29.29.29.1.2.cmml" xref="S4.E40.m1.108.108.108.29.29.29.1.2">𝑙</ci><ci
    id="S4.E40.m1.108.108.108.29.29.29.1.3.cmml" xref="S4.E40.m1.108.108.108.29.29.29.1.3">′′</ci></apply></apply><apply
    id="S4.E40.m1.109.109.109.30.30.30.1.cmml" xref="S4.E40.m1.109.109.109.30.30.30.1"><ci
    id="S4.E40.m1.109.109.109.30.30.30.1.2.cmml" xref="S4.E40.m1.109.109.109.30.30.30.1.2">𝑖</ci><ci
    id="S4.E40.m1.109.109.109.30.30.30.1.3.cmml" xref="S4.E40.m1.109.109.109.30.30.30.1.3">𝑗</ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S4.E40.m1.117c">\begin{gathered}\mathbf{h}^{l^{\prime}}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{0}_{i},\mathbf{h}^{1}_{i},...,\mathbf{h}^{l}_{i}),\mathbf{h}^{l^{\prime\prime}}_{i}=\mathcal{F}_{EN}(\{\mathbf{e}^{l}_{ij}&#124;j\in\mathcal{N}(i)\})\\
    \mathbf{e}^{l^{\prime}}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{0}_{ij},\mathbf{e}^{1}_{ij},...,\mathbf{e}^{l}_{ij}),\mathbf{e}^{l^{\prime\prime}}_{ij}=\mathcal{F}_{NE}(\mathbf{h}^{l}_{i},\mathbf{h}^{l}_{j})\\
    \mathbf{h}^{l+1}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{l^{\prime}}_{i},\mathbf{h}^{l^{\prime\prime}}_{i}),\mathbf{e}^{l+1}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{l^{\prime}}_{ij},\mathbf{e}^{l^{\prime\prime}}_{ij}),\end{gathered}</annotation></semantics></math>
    |  | (40) |
  prefs: []
  type: TYPE_NORMAL
- en: 'where $\mathbf{e}_{ij}^{l}$ is the representation of edge $(v_{i},v_{j})$ in
    the $l^{th}$ layer and $\mathcal{F}(\cdot)$ are learnable functions whose subscripts
    represent message-passing directions. By stacking multiple such modules, information
    can propagate by alternately passing between node and edge representations. Note
    that in the node-to-node and edge-to-edge functions, jump connections similar
    to those in JK-Nets [[62](#bib.bib62)] are implicitly added. GNs [[9](#bib.bib9)]
    also proposed learning an edge representation and updating both node and edge
    representations using message-passing functions as shown in Eq. ([27](#S4.E27
    "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")) in Section [4.1.4](#S4.SS1.SSS4 "4.1.4
    Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep
    Learning on Graphs: A Survey"). In this aspect, the “weave module” is a special
    case of GNs that does not a representation of the entire graph.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Sampling Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One critical bottleneck when training GCNs for large-scale graphs is efficiency.
    As shown in Section [4.1.4](#S4.SS1.SSS4 "4.1.4 Frameworks ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"), many GCNs
    follow a neighborhood aggregation scheme. However, because many real graphs follow
    a power-law distribution [[91](#bib.bib91)] (i.e., a few nodes have very large
    degrees), the number of neighbors can expand extremely quickly. To deal with this
    problem, two types of sampling methods have been proposed: neighborhood samplings
    and layer-wise samplings, as illustrated in Figure [7](#S4.F7 "Figure 7 ‣ 4.3.4
    Sampling Methods ‣ 4.3 Improvements and Discussions ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: In neighborhood samplings, the sampling is performed for each node during the
    calculations. GraphSAGE [[53](#bib.bib53)] uniformly sampled a fixed number of
    neighbors for each node during training. PinSage [[66](#bib.bib66)] proposed sampling
    neighbors using random walks on graphs along with several implementation improvements
    including coordination between the CPU and GPU, a map-reduce inference pipeline,
    and so on. PinSage was shown to be capable of handling a real billion-scale graph.
    StochasticGCN [[67](#bib.bib67)] further proposed reducing the sampling variances
    by using the historical activations of the last batches as a control variate,
    allowing for arbitrarily small sample sizes with a theoretical guarantee.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of sampling neighbors of nodes, FastGCN [[68](#bib.bib68)] adopted
    a different strategy: it sampled nodes in each convolutional layer (i.e., a layer-wise
    sampling) by interpreting the nodes as i.i.d. samples and the graph convolutions
    as integral transforms under probability measures. FastGCN also showed that sampling
    nodes via their normalized degrees could reduce variances and lead to better performance.
    Adapt [[69](#bib.bib69)] further proposed sampling nodes in the lower layers conditioned
    on their top layer; this approach was more adaptive and applicable to explicitly
    reduce variances.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4e12bde8d6457c8599967b3e1df050da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Different node sampling methods, in which the blue nodes indicate
    samples from one batch and the arrows indicate the sampling directions. The red
    nodes in (B) represent historical samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 Inductive Setting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another important aspect of GCNs is that whether they can be applied to an inductive
    setting, i.e., training on a set of nodes or graphs and testing on another unseen
    set of nodes or graphs. In principle, this goal is achieved by learning a mapping
    function on the given features that are not dependent on the graph basis and can
    be transferred across nodes or graphs. The inductive setting was verified in GraphSAGE [[53](#bib.bib53)],
    GAT [[57](#bib.bib57)], GaAN [[58](#bib.bib58)], and FastGCN [[68](#bib.bib68)].
    However, the existing inductive GCNs are suitable only for graphs with explicit
    features. How to conduct inductive learnings for graphs without explicit features,
    usually called the out-of-sample problem [[92](#bib.bib92)], remains largely open
    in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.6 Theoretical Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To understand the effectiveness of GCNs, some theoretical analyses have been
    proposed that can be divided into three categories: node-focused tasks, graph-focused
    tasks, and general analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For node-focused tasks, Li et al. [[70](#bib.bib70)] first analyzed the performance
    of GCNs by using a special form of Laplacian smoothing, which makes the features
    of nodes in the same cluster similar. The original Laplacian smoothing operation
    is formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}^{\prime}_{i}=(1-\gamma)\mathbf{h}_{i}+\gamma\sum\nolimits_{j\in\mathcal{N}(i)}\frac{1}{d_{i}}\mathbf{h}_{j},$
    |  | (41) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{h}_{i}$ and $\mathbf{h}^{\prime}_{i}$ are the original and smoothed
    features of node $v_{i}$, respectively. We can see that Eq. ([41](#S4.E41 "In
    4.3.6 Theoretical Analysis ‣ 4.3 Improvements and Discussions ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) is very similar to the graph convolution
    in Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")). Based
    on this insight, Li et al. also proposed a co-training and a self-training method
    for GCNs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Wu et al. [[71](#bib.bib71)] analyzed GCNs from a signal processing
    perspective. By regarding node features as graph signals, they showed that Eq. ([13](#S4.E13
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) is basically a fixed low-pass
    filter. Using this insight, they proposed an extremely simplified graph convolution
    (SGC) architecture by removing all the nonlinearities and collapsing the learning
    parameters into one matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{H}^{L}=\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\right)^{L}\mathbf{F}_{V}\mathbf{\Theta}.$
    |  | (42) |'
  prefs: []
  type: TYPE_TB
- en: The authors showed that such a “non-deep-learning” GCN variant achieved comparable
    performance to existing GCNs in many tasks. Maehara [[72](#bib.bib72)] enhanced
    this result by showing that the low-pass filtering operation did not equip GCNs
    with a nonlinear manifold learning ability, and further proposed GFNN model to
    remedy this problem by adding a MLP after the graph convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: For graph-focused tasks, Kipf and Welling [[43](#bib.bib43)] and the authors
    of SortPooling [[49](#bib.bib49)] both considered the relationship between GCNs
    and graph kernels such as the Weisfeiler-Lehman (WL) kernel [[78](#bib.bib78)],
    which is widely used in graph isomorphism tests. They showed that GCNs are conceptually
    a generalization of the WL kernel because both methods iteratively aggregate information
    from node neighbors. Xu et al. [[73](#bib.bib73)] formalized this idea by proving
    that the WL kernel provides an upper bound for GCNs in terms of distinguishing
    graph structures. Based on this analysis, they proposed graph isomorphism network
    (GIN) and showed that a readout operation using summation and a MLP can achieve
    provably maximum discriminative power, i.e., the highest training accuracy in
    graph classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For general analysis, Scarselli et al. [[93](#bib.bib93)] showed that the Vapnik-Chervonenkis
    dimension (VC-dim) of GCNs with different activation functions has the same scale
    as the existing RNNs. Chen et al. [[65](#bib.bib65)] analyzed the optimization
    landscape of linear GCNs and showed that any local minimum is relatively close
    to the global minimum under certain simplifications. Verma and Zhang [[94](#bib.bib94)]
    analyzed the algorithmic stability and generalization bound of GCNs. They showed
    that single-layer GCNs satisfy the strong notion of uniform stability if the largest
    absolute eigenvalue of the graph convolution filters is independent of the graph
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Graph Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The autoencoder (AE) and its variations have been widely applied in unsupervised
    learning tasks [[95](#bib.bib95)] and are suitable for learning node representations
    for graphs. The implicit assumption is that graphs have an inherent, potentially
    nonlinear low-rank structure. In this section, we first elaborate graph autoencoders
    and then introduce graph variational autoencoders and other improvements. The
    main characteristics of GAEs are summarized in Table [V](#S5.T5 "TABLE V ‣ 5 Graph
    Autoencoders ‣ Deep Learning on Graphs: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: A Comparison among Different Graph Autoencoders (GAEs). T.C. = Time
    Complexity'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Type | Objective Function | T.C. | Node Features | Other Characteristics
    |'
  prefs: []
  type: TYPE_TB
- en: '| SAE [[96](#bib.bib96)] | AE | L2-reconstruction | $O(M)$ | No | - |'
  prefs: []
  type: TYPE_TB
- en: '| SDNE [[97](#bib.bib97)] | AE | L2-reconstruction + Laplacian eigenmaps |
    $O(M)$ | No | - |'
  prefs: []
  type: TYPE_TB
- en: '| DNGR [[98](#bib.bib98)] | AE | L2-reconstruction | $O(N^{2})$ | No | - |'
  prefs: []
  type: TYPE_TB
- en: '| GC-MC [[99](#bib.bib99)] | AE | L2-reconstruction | $O(M)$ | Yes | GCN encoder
    |'
  prefs: []
  type: TYPE_TB
- en: '| DRNE [[100](#bib.bib100)] | AE | Recursive reconstruction | $O(Ns)$ | No
    | LSTM aggregator |'
  prefs: []
  type: TYPE_TB
- en: '| G2G [[101](#bib.bib101)] | AE | KL + ranking | $O(M)$ | Yes | Nodes as distributions
    |'
  prefs: []
  type: TYPE_TB
- en: '| VGAE [[102](#bib.bib102)] | VAE | Pairwise reconstruction | $O(N^{2})$ |
    Yes | GCN encoder |'
  prefs: []
  type: TYPE_TB
- en: '| DVNE [[103](#bib.bib103)] | VAE | Wasserstein + ranking | $O(M)$ | No | Nodes
    as distributions |'
  prefs: []
  type: TYPE_TB
- en: '| ARGA/ARVGA [[104](#bib.bib104)] | AE/VAE | L2-reconstruction + GAN | $O(N^{2})$
    | Yes | GCN encoder |'
  prefs: []
  type: TYPE_TB
- en: '| NetRA [[105](#bib.bib105)] | AE | Recursive reconstruction + Laplacian eigenmaps
    + GAN | $O(M)$ | No | LSTM encoder |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The use of AEs for graphs originated from sparse autoencoder (SAE) [[96](#bib.bib96)].
    The basic idea is that, by regarding the adjacency matrix or its variations as
    the raw features of nodes, AEs can be leveraged as a dimensionality reduction
    technique to learn low-dimensional node representations. Specifically, SAE adopted
    the following L2-reconstruction loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\min_{\mathbf{\Theta}}\mathcal{L}_{2}=\sum\nolimits_{i=1}^{N}\left\&#124;\mathbf{P}\left(i,:\right)-\hat{\mathbf{P}}\left(i,:\right)\right\&#124;_{2}\\
    \hat{\mathbf{P}}\left(i,:\right)=\mathcal{G}\left(\mathbf{h}_{i}\right),\mathbf{h}_{i}=\mathcal{F}\left(\mathbf{P}\left(i,:\right)\right),\end{gathered}$
    |  | (43) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{P}$ is the transition matrix, $\hat{\mathbf{P}}$ is the reconstructed
    matrix, $\mathbf{h}_{i}\in\mathbb{R}^{d}$ is the low-dimensional representation
    of node $v_{i}$, $\mathcal{F}(\cdot)$ is the encoder, $\mathcal{G}(\cdot)$ is
    the decoder, $d\ll N$ is the dimensionality, and $\mathbf{\Theta}$ are parameters.
    Both the encoder and decoder are an MLP with many hidden layers. In other words,
    a SAE compresses the information of $\mathbf{P}(i,:)$ into a low-dimensional vector
    $\mathbf{h}_{i}$ and then reconstructs the original feature from that vector.
    Another sparsity regularization term was also added. After obtaining the low-dimensional
    representation $\mathbf{h}_{i}$, k-means [[106](#bib.bib106)] was applied for
    the node clustering task. The experiments prove that SAEs outperform non-deep
    learning baselines. However, SAE was based on an incorrect theoretical analysis.⁸⁸8SAE [[96](#bib.bib96)]
    motivated the problem by analyzing the connection between spectral clustering
    and singular value decomposition, which is mathematically incorrect as pointed
    out in [[107](#bib.bib107)]. The mechanism underlying its effectiveness remained
    unexplained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Structure deep network embedding (SDNE) [[97](#bib.bib97)] filled in the puzzle
    by showing that the L2-reconstruction loss in Eq. ([43](#S5.E43 "In 5.1 Autoencoders
    ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")) actually corresponds
    to the second-order proximity between nodes, i.e., two nodes share similar latten
    representations if they have similar neighborhoods, which is a well-studied concept
    in network science known as collaborative filtering or triangle closure [[5](#bib.bib5)].
    Motivated by network embedding methods showing that the first-order proximity
    is also important [[108](#bib.bib108)], SDNE modified the objective function by
    adding another Laplacian eigenmaps term [[75](#bib.bib75)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\mathbf{\Theta}}\mathcal{L}_{2}+\alpha\sum\nolimits_{i,j=1}^{N}\mathbf{A}(i,j)\left\&#124;\mathbf{h}_{i}-\mathbf{h}_{j}\right\&#124;_{2},$
    |  | (44) |'
  prefs: []
  type: TYPE_TB
- en: 'i.e., two nodes also share similar latent representations if they are directly
    connected. The authors also modified the L2-reconstruction loss by using the adjacency
    matrix and assigning different weights to zero and non-zero elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{2}=\sum\nolimits_{i=1}^{N}\left\&#124;\left(\mathbf{A}\left(i,:\right)-\mathcal{G}\left(\mathbf{h}_{i}\right)\right)\odot\mathbf{b}_{i}\right\&#124;_{2},$
    |  | (45) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{h}_{i}=\mathcal{F}\left(\mathbf{A}\left(i,:\right)\right)$,
    $b_{ij}=1$ if $\mathbf{A}(i,j)=0$; otherwise $b_{ij}=\beta>1$, and $\beta$ is
    another hyper-parameter. The overall architecture of SDNE is shown in Figure [8](#S5.F8
    "Figure 8 ‣ 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by another line of studies, a contemporary work DNGR [[98](#bib.bib98)]
    replaced the transition matrix $\mathbf{P}$ in Eq. ([43](#S5.E43 "In 5.1 Autoencoders
    ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")) with the positive
    pointwise mutual information (PPMI) [[79](#bib.bib79)] matrix defined in Eq. ([20](#S4.E20
    "In 4.1.3 The Aspect of Multiple Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")). In this way, the
    raw features can be associated with some random walk probability of the graph [[109](#bib.bib109)].
    However, constructing the input matrix has a time complexity of $O(N^{2})$, which
    is not scalable to large-scale graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/602bff1359bba0787232cfa4f2cc13d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The framework of SDNE [[97](#bib.bib97)]. Both the first and second-order
    proximities of nodes are preserved using deep autoencoders.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GC-MC [[99](#bib.bib99)] took a different approach by using the GCN proposed
    by Kipf and Welling [[43](#bib.bib43)] as the encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{H}=GCN\left(\mathbf{F}^{V},\mathbf{A}\right),$ |  | (46) |'
  prefs: []
  type: TYPE_TB
- en: 'and using a simple bilinear function as the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{A}}(i,j)=\mathbf{H}(i,:)\mathbf{\Theta}_{de}\mathbf{H}(j,:)^{T},$
    |  | (47) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{\Theta}_{de}$ are the decoder parameters. Using this approach,
    node features were naturally incorporated. For graphs without node features, a
    one-hot encoding of node IDs was utilized. The authors demonstrated the effectiveness
    of GC-MC on the recommendation problem on bipartite graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of reconstructing the adjacency matrix or its variations, DRNE [[100](#bib.bib100)]
    proposed another modification that directly reconstructed the low-dimensional
    node vectors by aggregating neighborhood information using an LSTM. Specifically,
    DRNE adopted the following objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\sum\nolimits_{i=1}^{N}\left\&#124;\mathbf{h}_{i}-\text{LSTM}\left(\{\mathbf{h}_{j}&#124;j\in\mathcal{N}(i)\}\right)\right\&#124;.$
    |  | (48) |'
  prefs: []
  type: TYPE_TB
- en: Because an LSTM requires its inputs to be a sequence, the authors suggested
    ordering the node neighborhoods based on their degrees. They also adopted a neighborhood
    sampling technique for nodes with large degrees to prevent an overlong memory.
    The authors proved that such a method can preserve regular equivalence as well
    as many centrality measures of nodes, such as PageRank [[110](#bib.bib110)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the above works that map nodes into a low-dimensional vector, Graph2Gauss
    (G2G) [[101](#bib.bib101)] proposed encoding each node as a Gaussian distribution
    $\mathbf{h}_{i}=\mathcal{N}\left(\mathbf{M}(i,:),diag\left(\mathbf{\Sigma}(i,:)\right)\right)$
    to capture the uncertainties of nodes. Specifically, the authors used a deep mapping
    from the node attributes to the means and variances of the Gaussian distribution
    as the encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{M}(i,:)=\mathcal{F}_{\mathbf{M}}(\mathbf{F}^{V}(i,:)),\mathbf{\Sigma}(i,:)=\mathcal{F}_{\mathbf{\Sigma}}(\mathbf{F}^{V}(i,:)),$
    |  | (49) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{F}_{\mathbf{M}}(\cdot)$ and $\mathcal{F}_{\mathbf{\Sigma}}(\cdot)$
    are the parametric functions that need to be learned. Then, instead of using an
    explicit decoder function, they used pairwise constraints to learn the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\text{KL}\left(\mathbf{h}_{j}&#124;&#124;\mathbf{h}_{i}\right)<\text{KL}\left(\mathbf{h}_{j^{\prime}}&#124;&#124;\mathbf{h}_{i}\right)\\
    \forall i,\forall j,\forall j^{\prime}\;s.t.\;d(i,j)<d(i,j^{\prime}),\end{gathered}$
    |  | (50) |'
  prefs: []
  type: TYPE_TB
- en: 'where $d(i,j)$ is the shortest distance from node $v_{i}$ to $v_{j}$ and $\text{KL}(q(\cdot)||p(\cdot))$
    is the Kullback-Leibler (KL) divergence between $q(\cdot)$ and $p(\cdot)$ [[111](#bib.bib111)].
    In other words, the constraints ensure that the KL-divergence between node representations
    has the same relative order as the graph distance. However, because Eq. ([50](#S5.E50
    "In 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey"))
    is hard to optimize, an energy-based loss [[112](#bib.bib112)] was adopted as
    a relaxation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\sum\nolimits_{(i,j,j^{\prime})\in\mathcal{D}}\left(E_{ij}^{2}+\exp^{-E_{ij^{\prime}}}\right),$
    |  | (51) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{D}=\left\{(i,j,j^{\prime})|d(i,j)<d(i,j^{\prime})\right\}$ and
    $E_{ij}=\text{KL}(\mathbf{h}_{j}||\mathbf{h}_{i})$. The authors further proposed
    an unbiased sampling strategy to accelerate the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Variational Autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Different from the aforementioned autoencoders, variational autoencoders (VAEs)
    are another type of deep learning method that combines dimensionality reduction
    with generative models. Its potential benefits include tolerating noise and learning
    smooth representations [[113](#bib.bib113)]. VAEs were first introduced to graph
    data in VGAE [[102](#bib.bib102)], where the decoder was a simple linear product:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p\left(\mathbf{A}&#124;\mathbf{H}\right)=\prod\nolimits_{i,j=1}^{N}\sigma\left(\mathbf{h}_{i}\mathbf{h}_{j}^{T}\right),$
    |  | (52) |'
  prefs: []
  type: TYPE_TB
- en: 'in which the node representation was assumed to follow a Gaussian distribution
    $q\left(\mathbf{h}_{i}|\mathbf{M},\mathbf{\Sigma}\right)=\mathcal{N}\left(\mathbf{h}_{i}|\mathbf{M}(i,:),diag\left(\mathbf{\Sigma}(i,:)\right)\right)$.
    For the encoder of the mean and variance matrices, the authors also adopted the
    GCN proposed by Kipf and Welling [[43](#bib.bib43)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{M}=GCN_{\mathbf{M}}\left(\mathbf{F}^{V},\mathbf{A}\right),\log\mathbf{\Sigma}=GCN_{\mathbf{\Sigma}}\left(\mathbf{F}^{V},\mathbf{A}\right).$
    |  | (53) |'
  prefs: []
  type: TYPE_TB
- en: 'Then, the model parameters were learned by minimizing the variational lower
    bound [[113](#bib.bib113)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\mathbb{E}_{q\left(\mathbf{H}&#124;\mathbf{F}^{V},\mathbf{A}\right)}\left[\log
    p\left(\mathbf{A}&#124;\mathbf{H}\right)\right]-\text{KL}\left(q\left(\mathbf{H}&#124;\mathbf{F}^{V},\mathbf{A}\right)&#124;&#124;p(\mathbf{H})\right).$
    |  | (54) |'
  prefs: []
  type: TYPE_TB
- en: However, because this approach required reconstructing the full graph, its time
    complexity is $O(N^{2})$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by SDNE and G2G, DVNE [[103](#bib.bib103)] proposed another VAE for
    graph data that also represented each node as a Gaussian distribution. Unlike
    the existing works that had adopted KL-divergence as the measurement, DVNE used
    the Wasserstein distance [[114](#bib.bib114)] to preserve the transitivity of
    the nodes similarities. Similar to SDNE and G2G, DVNE also preserved both the
    first and second-order proximity in its objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\bf{\Theta}}\sum\nolimits_{(i,j,j^{\prime})\in\mathcal{D}}\left(E_{ij}^{2}+\exp^{-E_{ij^{\prime}}}\right)+\alpha\mathcal{L}_{2},$
    |  | (55) |'
  prefs: []
  type: TYPE_TB
- en: 'where $E_{ij}=W_{2}\left(\mathbf{h}_{j}||\mathbf{h}_{i}\right)$ is the $2^{nd}$
    Wasserstein distance between two Gaussian distributions $\mathbf{h}_{j}$ and $\mathbf{h}_{i}$
    and $\mathcal{D}=\left\{(i,j,j^{\prime})|j\in\mathcal{N}(i),j^{\prime}\notin\mathcal{N}(i)\right\}$
    is a set of triples corresponding to the ranking loss of the first-order proximity.
    The reconstruction loss was defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{2}=\inf\nolimits_{q(\mathbf{Z}&#124;\mathbf{P})}\mathbb{E}_{p(\mathbf{P})}\mathbb{E}_{q(\mathbf{Z}&#124;\mathbf{P})}\left\&#124;\mathbf{P}\odot(\mathbf{P}-\mathcal{G}(\mathbf{Z}))\right\&#124;_{2}^{2},$
    |  | (56) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{P}$ is the transition matrix and $\mathbf{Z}$ represents samples
    drawn from $\mathbf{H}$. The framework is shown in Figure [9](#S5.F9 "Figure 9
    ‣ 5.2 Variational Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs:
    A Survey"). Using this approach, the objective function can be minimized as in
    conventional VAEs using the reparameterization trick [[113](#bib.bib113)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/431f56c0c1fef5a24acfc611b0d1033c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The framework of DVNE [[103](#bib.bib103)]. DVNE represents nodes
    as distributions using a VAE and adopts the Wasserstein distance to preserve the
    transitivity of the nodes similarities.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Improvements and Discussions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several improvements have also been proposed for GAEs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Adversarial Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An adversarial training scheme⁹⁹9We will discuss more adversarial methods for
    graphs in Section [7](#S7 "7 Graph Adversarial Methods ‣ Deep Learning on Graphs:
    A Survey"). was incorporated into GAEs as an additional regularization term in
    ARGA [[104](#bib.bib104)]. The overall architecture is shown in Figure [10](#S5.F10
    "Figure 10 ‣ 5.3.1 Adversarial Training ‣ 5.3 Improvements and Discussions ‣ 5
    Graph Autoencoders ‣ Deep Learning on Graphs: A Survey"). Specifically, the encoder
    of GAEs was used as the generator while the discriminator aimed to distinguish
    whether a latent representation came from the generator or from a prior distribution.
    In this way, the autoencoder was forced to match the prior distribution as a regularization.
    The objective function was:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\bf{\Theta}}\mathcal{L}_{2}+\alpha\mathcal{L}_{GAN},$ |  | (57)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}_{2}$ is the reconstruction loss in GAEs and $\mathcal{L}_{GAN}$
    is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\mathcal{G}}\max_{\mathcal{D}}\mathbb{E}_{\mathbf{h}\sim p_{\mathbf{h}}}\left[\log\mathcal{D}(\mathbf{h})\right]+\mathbb{E}_{\mathbf{z}\sim\mathcal{G}(\mathbf{F}^{V},\mathbf{A})}\left[\log\left(1-\mathcal{D}\left(\mathbf{z}\right)\right)\right],$
    |  | (58) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{G}\left(\mathbf{F}^{V},\mathbf{A}\right)$ is a generator that
    uses the graph convolutional encoder from Eq. ([53](#S5.E53 "In 5.2 Variational
    Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")), $\mathcal{D}(\cdot)$
    is a discriminator based on the cross-entropy loss, and $p_{\mathbf{h}}$ is the
    prior distribution. The study adopted a simple Gaussian prior, and the experimental
    results demonstrated the effectiveness of the adversarial training scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dbadf30d7fdac7eee97de8a214b9def1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The framework of ARGA/ARVGA reprinted from [[104](#bib.bib104)]
    with permission. This model incorporates the adversarial training scheme into
    GAEs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrently, NetRA [[105](#bib.bib105)] also proposed using a generative adversarial
    network (GAN) [[115](#bib.bib115)] to enhance the generalization ability of graph
    autoencoders. Specifically, the authors used the following objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\bf{\Theta}}\mathcal{L}_{2}+\alpha_{1}\mathcal{L}_{LE}+\alpha_{2}\mathcal{L}_{GAN},$
    |  | (59) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{L}_{LE}$ is the Laplacian eigenmaps objective function shown
    in Eq. ([44](#S5.E44 "In 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning
    on Graphs: A Survey")). In addition, the authors adopted an LSTM as the encoder
    to aggregate information from neighborhoods similar to Eq. ([48](#S5.E48 "In 5.1
    Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")). Instead
    of sampling only immediate neighbors and ordering the nodes using degrees as in
    DRNE [[100](#bib.bib100)], the authors used random walks to generate the input
    sequences. In contrast to ARGA, NetRA considered the representations in GAEs as
    the ground-truth and adopted random Gaussian noises followed by an MLP as the
    generator.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Inductive Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to GCNs, GAEs can be applied to the inductive learning setting if node
    attributes are incorporated in the encoder. This can be achieved by using a GCN
    as the encoder, such as in GC-MC  [[99](#bib.bib99)], VGAE [[102](#bib.bib102)],
    and VGAE [[104](#bib.bib104)], or by directly learning a mapping function from
    node features as in G2G [[101](#bib.bib101)]. Because the edge information is
    utilized only when learning the parameters, the model can also be applied to nodes
    unseen during training. These works also show that although GCNs and GAEs are
    based on different architectures, it is possible to use them jointly, which we
    believe is a promising future direction.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Similarity Measures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In GAEs, many similarity measures have been adopted, for example, L2-reconstruction
    loss, Laplacian eigenmaps, and the ranking loss for graph AEs, and KL divergence
    and Wasserstein distance for graph VAEs. Although these similarity measures are
    based on different motivations, how to choose an appropriate similarity measure
    for a given task and model architecture remains unstudied. More research is needed
    to understand the underlying differences between these metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: The Main Characteristics of Graph Reinforcement Learning'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Task | Actions | Rewards | Time Complexity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GCPN [[116](#bib.bib116)] | Graph generation | Link prediction | GAN + domain
    knowledge | $O(MN)$ |'
  prefs: []
  type: TYPE_TB
- en: '| MolGAN [[117](#bib.bib117)] | Graph generation | Generate the entire graph
    | GAN + domain knowledge | $O(N^{2})$ |'
  prefs: []
  type: TYPE_TB
- en: '| GTPN [[118](#bib.bib118)] | Chemical reaction prediction | Predict node pairs
    and new bonding types | Prediction results | $O(N^{2})$ |'
  prefs: []
  type: TYPE_TB
- en: '| GAM [[119](#bib.bib119)] | Graph classification | Predict graph labels and
    select the next node | Classification results | $O(d_{\text{avg}}sT)$ |'
  prefs: []
  type: TYPE_TB
- en: '| DeepPath [[120](#bib.bib120)] | Knowledge graph reasoning | Predict the next
    node of the reasoning path | Reasoning results + diversity | $O(d_{\text{avg}}sT+s^{2}T)$
    |'
  prefs: []
  type: TYPE_TB
- en: '| MINERVA [[121](#bib.bib121)] | Knowledge graph reasoning | Predict the next
    node of the reasoning path | Reasoning results | $O(d_{\text{avg}}sT)$ |'
  prefs: []
  type: TYPE_TB
- en: 6 Graph Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One aspect of deep learning not yet discussed is reinforcement learning (RL),
    which has been shown to be effective in AI tasks such as playing games [[122](#bib.bib122)].
    RL is known to be good at learning from feedbacks, especially when dealing with
    non-differentiable objectives and constraints. In this section, we review Graph
    RL methods. Their main characteristics are summarized in Table [VI](#S5.T6 "TABLE
    VI ‣ 5.3.3 Similarity Measures ‣ 5.3 Improvements and Discussions ‣ 5 Graph Autoencoders
    ‣ Deep Learning on Graphs: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: GCPN [[116](#bib.bib116)] utilized RL to generate goal-directed molecular graphs
    while considering non-differential objectives and constraints. Specifically, the
    graph generation is modeled as a Markov decision process of adding nodes and edges,
    and the generative model is regarded as an RL agent operating in the graph generation
    environment. By treating agent actions as link predictions, using domain-specific
    as well as adversarial rewards, and using GCNs to learn the node representations,
    GCPN can be trained in an end-to-end manner using a policy gradient [[123](#bib.bib123)].
  prefs: []
  type: TYPE_NORMAL
- en: A concurrent work, MolGAN [[117](#bib.bib117)], adopted a similar idea of using
    RL for generating molecular graphs. However, rather than generating the graph
    through a sequence of actions, MolGAN proposed directly generating the full graph;
    this approach worked particularly well for small molecules.
  prefs: []
  type: TYPE_NORMAL
- en: GTPN [[118](#bib.bib118)] adopted RL to predict chemical reaction products.
    Specifically, the agent acted to select node pairs in the molecule graph and predicted
    their new bonding types, and rewards were given both immediately and at the end
    based on whether the predictions were correct. GTPN used a GCN to learn the node
    representations and an RNN to memorize the prediction sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'GAM [[119](#bib.bib119)] applied RL to graph classification by using random
    walks. The authors modeled the generation of random walks as a partially observable
    Markov decision process (POMDP). The agent performed two actions: first, it predicted
    the label of the graph; then, it selected the next node in the random walk. The
    reward was determined simply by whether the agent correctly classified the graph,
    i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{J}(\theta)=\mathbb{E}_{P(S_{1:T};\theta)}\sum\nolimits_{t=1}^{T}r_{t},$
    |  | (60) |'
  prefs: []
  type: TYPE_TB
- en: where $r_{t}=1$ represents a correct prediction; otherwise, $r_{t}=-1$. $T$
    is the total time steps and $S_{t}$ is the environment.
  prefs: []
  type: TYPE_NORMAL
- en: DeepPath [[120](#bib.bib120)] and MINERVA [[121](#bib.bib121)] both adopted
    RL for knowledge graph (KG) reasoning. Specifically, DeepPath targeted at pathfinding,
    i.e., find the most informative path between two target nodes, while MINERVA tackled
    question-answering tasks, i.e., find the correct answer node given a question
    node and a relation. In both methods, the RL agents need to predict the next node
    in the path at each step and output a reasoning path in the KG. Agents receive
    rewards if the paths reach the correct destinations. DeepPath also added a regularization
    term to encourage the path diversity.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: The Main Characteristics of Graph Adversarial Methods'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method |  Adversarial Methods | Time Complexity | Node Features
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial Training | ARGA/ARVGA [[104](#bib.bib104)] | Regularization for
    GAEs | $O(N^{2})$ | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| NetRA [[105](#bib.bib105)] | Regularization for GAEs | $O(M)$ | No |'
  prefs: []
  type: TYPE_TB
- en: '| GCPN [[116](#bib.bib116)] | Rewards for Graph RL | $O(MN)$ | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| MolGAN [[117](#bib.bib117)] | Rewards for Graph RL | $O(N^{2})$ | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| GraphGAN [[124](#bib.bib124)] | Generation of negative samples (i.e., node
    pairs) | $O(MN)$ | No |'
  prefs: []
  type: TYPE_TB
- en: '| ANE [[125](#bib.bib125)] | Regularization for network embedding | $O(N)$
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| GraphSGAN [[126](#bib.bib126)] | Enhancing semi-supervised learning on graphs
    | $O(N^{2})$ | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| NetGAN [[127](#bib.bib127)] | Generation of graphs via random walks | $O(M)$
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial Attack | Nettack [[128](#bib.bib128)] | Targeted attacks of graph
    structures and node attributes | $O(Nd_{0}^{2})$ | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. [[129](#bib.bib129)] | Targeted attacks of graph structures |
    $O(M)$ | No |'
  prefs: []
  type: TYPE_TB
- en: '| Zugner and Gunnemann [[130](#bib.bib130)] | Non-targeted attacks of graph
    structures | $O(N^{2})$ | No |'
  prefs: []
  type: TYPE_TB
- en: 7 Graph Adversarial Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adversarial methods such as GANs [[115](#bib.bib115)] and adversarial attacks
    have drawn increasing attention in the machine learning community in recent years.
    In this section, we review how to apply adversarial methods to graphs. The main
    characteristics of graph adversarial methods are summarized in Table [VII](#S6.T7
    "TABLE VII ‣ 6 Graph Reinforcement Learning ‣ Deep Learning on Graphs: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Adversarial Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The basic idea behind a GAN is to build two linked models: a discriminator
    and a generator. The goal of the generator is to “fool” the discriminator by generating
    fake data, while the discriminator aims to distinguish whether a sample comes
    from real data or is generated by the generator. Subsequently, both models benefit
    from each other by joint training using a minimax game. Adversarial training has
    been shown to be effective in generative models and enhancing the generalization
    ability of discriminative models. In Section [5.3.1](#S5.SS3.SSS1 "5.3.1 Adversarial
    Training ‣ 5.3 Improvements and Discussions ‣ 5 Graph Autoencoders ‣ Deep Learning
    on Graphs: A Survey") and Section [6](#S6 "6 Graph Reinforcement Learning ‣ Deep
    Learning on Graphs: A Survey"), we reviewed how adversarial training schemes are
    used in GAEs and Graph RL, respectively. Here, we review several other adversarial
    training methods on graphs in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GraphGAN [[124](#bib.bib124)] proposed using a GAN to enhance graph embedding
    methods [[17](#bib.bib17)] with the following objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{\mathcal{G}}\max_{\mathcal{D}}\sum\nolimits_{i=1}^{N}$
    | $\displaystyle\left(\mathbb{E}_{v\sim p_{graph}(\cdot&#124;v_{i})}\left[\log\mathcal{D}(v,v_{i})\right]\right.$
    |  | (61) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle+$ | $\displaystyle\left.\mathbb{E}_{v\sim\mathcal{G}(\cdot&#124;v_{i})}\left[\log\left(1-\mathcal{D}\left(v,v_{i}\right)\right)\right]\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The discriminator $\mathcal{D}(\cdot)$ and the generator $\mathcal{G}(\cdot)$
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\mathcal{D}(v,v_{i})=\sigma(\mathbf{d}_{v}\mathbf{d}_{v_{i}}^{T}),\mathcal{G}(v&#124;v_{i})=\frac{\exp(\mathbf{g}_{v}\mathbf{g}_{v_{i}}^{T})}{\sum_{v^{\prime}\neq
    v_{i}}\exp(\mathbf{g}_{v^{\prime}}\mathbf{g}_{v_{i}}^{T})},\end{gathered}$ |  |
    (62) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{d}_{v}$ and $\mathbf{g}_{v}$ are the low-dimensional embedding
    vectors for node $v$ in the discriminator and the generator, respectively. Combining
    the above equations, the discriminator actually has two objectives: the node pairs
    in the original graph should possess large similarities, while the node pairs
    generated by the generator should possess small similarities. This architecture
    is similar to network embedding methods such as LINE [[108](#bib.bib108)], except
    that negative node pairs are generated by the generator $\mathcal{G}(\cdot)$ instead
    of by random samplings. The authors showed that this method enhanced the inference
    abilities of the node embedding vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial network embedding (ANE) [[125](#bib.bib125)] also adopted an adversarial
    training scheme to improve network embedding methods. Similar to ARGA [[104](#bib.bib104)],
    ANE used a GAN as an additional regularization term to existing network embedding
    methods such as DeepWalk [[131](#bib.bib131)] by imposing a prior distribution
    as the real data and regarding the embedding vectors as generated samples.
  prefs: []
  type: TYPE_NORMAL
- en: GraphSGAN [[126](#bib.bib126)] used a GAN to enhance semi-supervised learning
    on graphs. Specifically, the authors observed that fake nodes should be generated
    in the density gaps between subgraphs to weaken the propagation effect across
    different clusters of the existing models. To achieve that goal, the authors designed
    a novel optimization objective with elaborate loss terms to ensure that the generator
    generated samples in the density gaps at equilibrium.
  prefs: []
  type: TYPE_NORMAL
- en: NetGAN [[127](#bib.bib127)] adopted a GAN for graph generation tasks. Specifically,
    the authors regarded graph generation as a task to learn the distribution of biased
    random walks and adopted a GAN framework to generate and discriminate among random
    walks using an LSTM. The experiments showed that using random walks could also
    learn global network patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Adversarial Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial attacks are another class of adversarial methods intended to deliberately
    “fool” the targeted methods by adding small perturbations to data. Studying adversarial
    attacks can deepen our understanding of the existing models and inspire more robust
    architectures. We review the graph-based adversarial attacks below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nettack [[128](#bib.bib128)] first proposed attacking node classification models
    such as GCNs by modifying graph structures and node attributes. Denoting the targeted
    node as $v_{0}$ and its true class as $c_{true}$, the targeted model as $\mathcal{F}(\mathbf{A},\mathbf{F}^{V})$
    and its loss function as $\mathcal{L}_{\mathcal{F}}(\mathbf{A},\mathbf{F}^{V})$,
    the model adopted the following objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\operatorname*{argmax}_{\left(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}\right)\in\mathcal{P}}\;\max_{c\neq
    c_{true}}\log\mathbf{Z}^{*}_{v_{0},c}-\log\mathbf{Z}^{*}_{v_{0},c_{true}}\\ s.t.\;\mathbf{Z}^{*}=\mathcal{F}_{\theta^{*}}(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}),\theta^{*}=\operatorname*{argmin}\nolimits_{\theta}\mathcal{L}_{\mathcal{F}}(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}),\end{gathered}$
    |  | (63) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{A}^{\prime}$ and $\mathbf{F}^{V\prime}$ are the modified adjacency
    matrix and node feature matrix, respectively, $\mathbf{Z}$ represents the classification
    probabilities predicted by $\mathcal{F}(\cdot)$, and $\mathcal{P}$ is the space
    determined by the attack constraints. Simply speaking, the optimization aims to
    find the best legitimate changes in graph structures and node attributes to cause
    $v_{0}$ to be misclassified. The $\theta^{*}$ indicates that the attack is causative,
    i.e., the attack occurs before training the targeted model. The authors proposed
    several constraints for the attacks. The most important constraint is that the
    attack should be “unnoticeable”, i.e., it should make only small changes. Specifically,
    the authors proposed to preserve data characteristics such as node degree distributions
    and feature co-occurrences. The authors also proposed two attacking scenarios,
    direct attack (directly attacking $v_{0}$) and influence attack (only attacking
    other nodes), and several relaxations to make the optimization tractable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrently, Dai et al. [[129](#bib.bib129)] studied adversarial attacks for
    graphs with an objective function similar to Eq. ([63](#S7.E63 "In 7.2 Adversarial
    Attacks ‣ 7 Graph Adversarial Methods ‣ Deep Learning on Graphs: A Survey"));
    however, they focused on the case in which only graph structures were changed.
    Instead of assuming that the attacker possessed all the information, the authors
    considered several settings in which different amounts of information were available.
    The most effective strategy, RL-S2V, adopted structure2vec [[132](#bib.bib132)]
    to learn the node and graph representations and used reinforcement learning to
    solve the optimization. The experimental results showed that the attacks were
    effective for both node and graph classification tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned two attacks are targeted, i.e., they are intended to cause
    misclassification of some targeted node $v_{0}$. Zugner and Gunnemann [[130](#bib.bib130)]
    were the first to study non-targeted attacks, which were intended to reduce the
    overall model performance. They treated the graph structure as hyper-parameters
    to be optimized and adopted meta-gradients in the optimization process, along
    with several techniques to approximate the meta-gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Discussions and Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thus far, we have reviewed the different graph-based deep learning architectures
    as well as their similarities and differences. Next, we briefly discuss their
    applications, implementations, and future directions before summarizing this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to standard graph inference tasks such as node or graph classification^(10)^(10)10A
    collection of methods for common tasks is listed in Appendix [B](#A2 "Appendix
    B Applicability for Common Tasks ‣ Deep Learning on Graphs: A Survey")., graph-based
    deep learning methods have also been applied to a wide range of disciplines, including
    modeling social influence [[133](#bib.bib133)], recommendation [[28](#bib.bib28),
    [66](#bib.bib66), [99](#bib.bib99), [134](#bib.bib134)], chemistry and biology [[55](#bib.bib55),
    [52](#bib.bib52), [46](#bib.bib46), [116](#bib.bib116), [117](#bib.bib117)], physics [[135](#bib.bib135),
    [136](#bib.bib136)], disease and drug prediction [[137](#bib.bib137), [138](#bib.bib138),
    [139](#bib.bib139)], gene expression [[140](#bib.bib140)], natural language processing
    (NLP) [[141](#bib.bib141), [142](#bib.bib142)], computer vision [[143](#bib.bib143),
    [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146), [147](#bib.bib147)],
    traffic forecasting [[148](#bib.bib148), [149](#bib.bib149)], program induction [[150](#bib.bib150)],
    solving graph-based NP problems [[151](#bib.bib151), [152](#bib.bib152)], and
    multi-agent AI systems [[153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155)].'
  prefs: []
  type: TYPE_NORMAL
- en: A thorough review of these methods is beyond the scope of this paper due to
    the sheer diversity of these applications; however, we list several key inspirations.
    First, it is important to incorporate domain knowledge into the model when constructing
    a graph or choosing architectures. For example, building a graph based on the
    relative distance may be suitable for traffic forecasting problems, but may not
    work well for a weather prediction problem where the geographical location is
    also important. Second, a graph-based model can usually be built on top of other
    architectures rather than as a stand-alone model. For example, the computer vision
    community usually adopts CNNs for detecting objects and then uses graph-based
    deep learning as a reasoning module [[156](#bib.bib156)]. For NLP problems, GCNs
    can be adopted as syntactic constraints [[141](#bib.bib141)]. As a result, key
    key challenge is how to integrate different models. These applications also show
    that graph-based deep learning not only enables mining the rich value underlying
    the existing graph data but also helps to naturally model relational data as graphs,
    greatly widening the applicability of graph-based deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Implementations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recently, several open libraries have been made available for developing deep
    learning models on graphs. These libraries are listed in Table [VIII](#S8.T8 "TABLE
    VIII ‣ 8.3 Future Directions ‣ 8 Discussions and Conclusion ‣ Deep Learning on
    Graphs: A Survey"). We also collected a list of source code (mostly from their
    original authors) for the studies discussed in this paper. This repository is
    included in Appendix [A](#A1 "Appendix A Source Codes ‣ Deep Learning on Graphs:
    A Survey"). These open implementations make it easy to learn, compare, and improve
    different methods. Some implementations also address the problem of distributed
    computing, which we do not discuss in this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Future Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several ongoing or future research directions which are also worthy
    of discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New models for unstudied graph structures. Due to the extremely diverse structures
    of graph data, the existing methods are not suitable for all of them. For example,
    most methods focus on homogeneous graphs, while heterogeneous graphs are seldom
    studied, especially those containing different modalities such as those in [[157](#bib.bib157)].
    Signed networks, in which negative edges represent conflicts between nodes, also
    have unique structures, and they pose additional challenges to the existing methods [[158](#bib.bib158)].
    Hypergraphs, which represent complex relations between more than two objects [[159](#bib.bib159)],
    are also understudied. Thus, an important next step is to design specific deep
    learning models to handle these types of graphs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compositionality of existing models. As shown multiple times in this paper,
    many of the existing architectures can be integrated: for example, using a GCN
    as a layer in GAEs or Graph RL. In addition to designing new building blocks,
    how to systematically composite these architectures is an interesting future direction.
    In this process, how to incorporate interdisciplinary knowledge in a principled
    way rather than on a case-by-case basis is also an open problem. One recent work,
    graph networks [[9](#bib.bib9)], takes the first step and focuses on using a general
    framework of GNNs and GCNs for relational reasoning problems. AutoML may also
    be helpful by reducing the human burden of assembling different components and
    choosing hyper-parameters [[160](#bib.bib160)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic graphs. Most of the existing methods focus on static graphs. However,
    many real graphs are dynamic in nature: their nodes, edges, and features can change
    over time. For example, in social networks, people may establish new social relations,
    remove old relations, and their features, such as hobbies and occupations, can
    change over time. New users may join the network and existing users may leave.
    How to model the evolving characteristics of dynamic graphs and support incremental
    updates to model parameters remain largely unaddressed. Some preliminary works
    have obtained encouraging results by using Graph RNNs [[29](#bib.bib29), [27](#bib.bib27)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpretability and robustness. Because graphs are often related to other
    risk-sensitive scenarios, the ability to interpret the results of deep learning
    models on graphs is critical in decision-making problems. For example, in medicine
    or disease-related problems, interpretability is essential in transforming computer
    experiments into applications for clinical use. However, interpretability for
    graph-based deep learning is even more challenging than are other black-box models
    because graph nodes and edges are often heavily interconnected. In addition, because
    many existing deep learning models on graphs are sensitive to adversarial attacks
    as shown in Section [7.2](#S7.SS2 "7.2 Adversarial Attacks ‣ 7 Graph Adversarial
    Methods ‣ Deep Learning on Graphs: A Survey"), enhancing the robustness of the
    existing methods is another important issue. Some pioneering works regarding interpretability
    and robustness can be found in [[161](#bib.bib161)] and [[162](#bib.bib162), [163](#bib.bib163)],
    respectively.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE VIII: Libraries of Deep Learning on Graphs'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | URL | Language/Framework | Key Characteristics |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch Geometric [[164](#bib.bib164)] | https://github.com/rusty1s/pytorch_geometric
    | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Improved efficiency, unified operations, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; comprehensive existing methods &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Deep Graph Library [[165](#bib.bib165)] | https://github.com/dmlc/dgl | PyTorch
    | Improved efficiency, unified operations, scalability |'
  prefs: []
  type: TYPE_TB
- en: '| AliGraph [[166](#bib.bib166)] | https://github.com/alibaba/aligraph | Unknown
    | Distributed environment, scalability, in-house algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| Euler | https://github.com/alibaba/euler | C++/TensorFlow | Distributed environment,
    scalability |'
  prefs: []
  type: TYPE_TB
- en: 8.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The above survey shows that deep learning on graphs is a promising and fast-developing
    research field that both offers exciting opportunities and presents many challenges.
    Studying deep learning on graphs constitutes a critical building block in modeling
    relational data, and it is an important step towards a future with better machine
    learning and artificial intelligence techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors thank Jianfei Chen, Jie Chen, William L. Hamilton, Wenbing Huang,
    Thomas Kipf, Federico Monti, Shirui Pan, Petar Velickovic, Keyulu Xu, Rex Ying
    for allowing us to use their figures. This work was supported in part by National
    Program on Key Basic Research Project (No. 2015CB352300), National Key R&D Program
    of China under Grand 2018AAA0102004, National Natural Science Foundation of China
    (No. U1936219, No. U1611461, No. 61772304), and Beijing Academy of Artificial
    Intelligence (BAAI). All opinions, findings, conclusions, and recommendations
    in this paper are those of the authors and do not necessarily reflect the views
    of the funding agencies.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *et al.*, “Deep neural networks for acoustic
    modeling in speech recognition: The shared views of four research groups,” *IEEE
    Signal Processing Magazine*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in Neural Information Processing
    Systems*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” in *Proceedings of the 4th International Conference
    on Learning Representations*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A.-L. Barabasi, *Network science*.   Cambridge university press, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,
    “The emerging field of signal processing on graphs: Extending high-dimensional
    data analysis to networks and other irregular domains,” *IEEE Signal Processing
    Magazine*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, “Geometric
    deep learning: going beyond euclidean data,” *IEEE Signal Processing Magazine*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] C. Zang, P. Cui, and C. Faloutsos, “Beyond sigmoids: The nettide model
    for social network growth, and its applications,” in *Proceedings of the 22nd
    ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi,
    M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre,
    F. Song, A. Ballard, J. Gilmer, G. Dahl, A. Vaswani, K. Allen, C. Nash, V. Langston,
    C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, and
    R. Pascanu, “Relational inductive biases, deep learning, and graph networks,”
    *arXiv preprint arXiv:1806.01261*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, “Attention models
    in graphs: A survey,” *ACM Transactions on Knowledge Discovery from Data*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Zhang, H. Tong, J. Xu, and R. Maciejewski, “Graph convolutional networks:
    Algorithms, applications and open challenges,” in *International Conference on
    Computational Social Networks*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] L. Sun, J. Wang, P. S. Yu, and B. Li, “Adversarial attack and defense
    on graph data: A survey,” *arXiv preprint arXiv:1812.10528*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, and M. Sun, “Graph neural
    networks: A review of methods and applications,” *arXiv preprint arXiv:1812.08434*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, “A comprehensive
    survey on graph neural networks,” *arXiv preprint arXiv:1901.00596*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin, “Graph embedding
    and extensions: A general framework for dimensionality reduction,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] W. L. Hamilton, R. Ying, and J. Leskovec, “Representation learning on
    graphs: Methods and applications,” *arXiv preprint arXiv:1709.05584*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] P. Cui, X. Wang, J. Pei, and W. Zhu, “A survey on network embedding,”
    *IEEE Transactions on Knowledge and Data Engineering*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations
    by back-propagating errors,” *Nature*, 1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
    in *Proceedings of the 3rd International Conference on Learning Representations*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *Journal
    of Machine Learning Research*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving
    network embedding,” in *Proceedings of the 31st AAAI Conference on Artificial
    Intelligence*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Leskovec and J. J. Mcauley, “Learning to discover social circles in
    ego networks,” in *NeurIPS*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE Transactions on Neural Networks*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, “Gated graph sequence
    neural networks,” in *Proceedings of the 5th International Conference on Learning
    Representations*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] H. Dai, Z. Kozareva, B. Dai, A. Smola, and L. Song, “Learning steady-states
    of iterative algorithms over graphs,” in *International Conference on Machine
    Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. You, R. Ying, X. Ren, W. Hamilton, and J. Leskovec, “Graphrnn: Generating
    realistic graphs with deep auto-regressive models,” in *International Conference
    on Machine Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Ma, Z. Guo, Z. Ren, E. Zhao, J. Tang, and D. Yin, “Streaming graph
    neural networks,” *arXiv preprint arXiv:1810.10627*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] F. Monti, M. Bronstein, and X. Bresson, “Geometric matrix completion with
    recurrent multi-graph neural networks,” in *Advances in Neural Information Processing
    Systems*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] F. Manessi, A. Rozza, and M. Manzo, “Dynamic graph convolutional networks,”
    *arXiv preprint arXiv:1704.06199*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder–decoder for
    statistical machine translation,” in *Proceedings of the 2014 Conference on Empirical
    Methods in Natural Language Processing*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] M. Gori, G. Monfardini, and F. Scarselli, “A new model for learning in
    graph domains,” in *IEEE International Joint Conference on Neural Networks Proceedings*,
    2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P. Frasconi, M. Gori, and A. Sperduti, “A general framework for adaptive
    processing of data structures,” *IEEE transactions on Neural Networks*, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] M. J. Powell, “An efficient method for finding the minimum of a function
    of several variables without calculating derivatives,” *The computer journal*,
    1964.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] L. B. Almeida, “A learning rule for asynchronous perceptrons with feedback
    in a combinatorial environment.” in *Proceedings, 1st First International Conference
    on Neural Networks*, 1987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] F. J. Pineda, “Generalization of back-propagation to recurrent neural
    networks,” *Physical Review Letters*, 1987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. A. Khamsi and W. A. Kirk, *An introduction to metric spaces and fixed
    point theory*.   John Wiley & Sons, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] M. Brockschmidt, Y. Chen, B. Cook, P. Kohli, and D. Tarlow, “Learning
    to decipher the heap for program verification,” in *Workshop on Constructive Machine
    Learning at the International Conference on Machine Learning*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] I. M. Baytas, C. Xiao, X. Zhang, F. Wang, A. K. Jain, and J. Zhou, “Patient
    subtyping via time-aware lstm networks,” in *Proceedings of the 23rd ACM SIGKDD
    International Conference on Knowledge Discovery and Data Mining*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Bruna, W. Zaremba, A. Szlam, and Y. Lecun, “Spectral networks and locally
    connected networks on graphs,” in *Proceedings of the 3rd International Conference
    on Learning Representations*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] M. Henaff, J. Bruna, and Y. LeCun, “Deep convolutional networks on graph-structured
    data,” *arXiv preprint arXiv:1506.05163*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural
    networks on graphs with fast localized spectral filtering,” in *Advances in Neural
    Information Processing Systems*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *Proceedings of the 6th International Conference on
    Learning Representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, “Cayleynets: Graph
    convolutional neural networks with complex rational spectral filters,” *IEEE Transactions
    on Signal Processing*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] B. Xu, H. Shen, Q. Cao, Y. Qiu, and X. Cheng, “Graph wavelet neural network,”
    in *Proceedings of the 8th International Conference on Learning Representations*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
    A. Aspuru-Guzik, and R. P. Adams, “Convolutional networks on graphs for learning
    molecular fingerprints,” in *Advances in Neural Information Processing Systems*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Niepert, M. Ahmed, and K. Kutzkov, “Learning convolutional neural networks
    for graphs,” in *International Conference on Machine Learning*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] H. Gao, Z. Wang, and S. Ji, “Large-scale learnable graph convolutional
    networks,” in *Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery &amp; Data Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, “An end-to-end deep learning
    architecture for graph classification,” in *Thirty-Second AAAI Conference on Artificial
    Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Atwood and D. Towsley, “Diffusion-convolutional neural networks,” in
    *Advances in Neural Information Processing Systems*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C. Zhuang and Q. Ma, “Dual graph convolutional networks for graph-based
    semi-supervised classification,” in *Proceedings of the 2018 World Wide Web Conference*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
    “Neural message passing for quantum chemistry,” in *International Conference on
    Machine Learning*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning
    on large graphs,” in *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein,
    “Geometric deep learning on graphs and manifolds using mixture model cnns,” in
    *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley, “Molecular
    graph convolutions: moving beyond fingerprints,” *Journal of Computer-Aided Molecular
    Design*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] R. Ying, J. You, C. Morris, X. Ren, W. L. Hamilton, and J. Leskovec, “Hierarchical
    graph representation learning with differentiable pooling,” in *Advances in Neural
    Information Processing Systems*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio,
    “Graph attention networks,” in *Proceedings of the 7th International Conference
    on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D.-Y. Yeung, “Gaan: Gated
    attention networks for learning on large and spatiotemporal graphs,” in *Proceedings
    of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. S. Yu, “Heterogeneous
    graph attention network,” in *The World Wide Web Conference*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] T. Pham, T. Tran, D. Q. Phung, and S. Venkatesh, “Column networks for
    collective classification.” in *Proceedings of the 31st AAAI Conference on Artificial
    Intelligence*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] J. Klicpera, A. Bojchevski, and S. Günnemann, “Predict then propagate:
    Graph neural networks meet personalized pagerank,” in *Proceedings of the 8th
    International Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] K. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka,
    “Representation learning on graphs with jumping knowledge networks,” in *International
    Conference on Machine Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. Simonovsky and N. Komodakis, “Dynamic edgeconditioned filters in convolutional
    neural networks on graphs,” in *2017 IEEE Conference on Computer Vision and Pattern
    Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. V. D. Berg, I. Titov, and M. Welling,
    “Modeling relational data with graph convolutional networks,” in *European Semantic
    Web Conference*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Z. Chen, L. Li, and J. Bruna, “Supervised community detection with line
    graph neural networks,” in *Proceedings of the 8th International Conference on
    Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec,
    “Graph convolutional neural networks for web-scale recommender systems,” in *Proceedings
    of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] J. Chen, J. Zhu, and L. Song, “Stochastic training of graph convolutional
    networks with variance reduction,” in *International Conference on Machine Learning*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. Chen, T. Ma, and C. Xiao, “Fastgcn: fast learning with graph convolutional
    networks via importance sampling,” in *Proceedings of the 7th International Conference
    on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] W. Huang, T. Zhang, Y. Rong, and J. Huang, “Adaptive sampling towards
    fast graph representation learning,” in *Advances in Neural Information Processing
    Systems*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional
    networks for semi-supervised learning,” in *Proceedings of the Thirty-Second AAAI
    Conference on Artificial Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger, “Simplifying
    graph convolutional networks,” in *International Conference on Machine Learning*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] T. Maehara, “Revisiting graph neural networks: All we have is low-pass
    filters,” *arXiv preprint arXiv:1905.09550*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural
    networks?” in *Proceedings of the 8th International Conference on Learning Representations*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] P. Veličković, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D.
    Hjelm, “Deep graph infomax,” in *Proceedings of the 8th International Conference
    on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques
    for embedding and clustering,” in *Advances in neural information processing systems*,
    2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] D. K. Hammond, P. Vandergheynst, and R. Gribonval, “Wavelets on graphs
    via spectral graph theory,” *Applied and Computational Harmonic Analysis*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary-order
    proximity preserved network embedding,” in *KDD*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] N. Shervashidze, P. Schweitzer, E. J. v. Leeuwen, K. Mehlhorn, and K. M.
    Borgwardt, “Weisfeiler-lehman graph kernels,” *Journal of Machine Learning Research*,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] O. Levy and Y. Goldberg, “Neural word embedding as implicit matrix factorization,”
    in *Advances in Neural Information Processing Systems*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] L. Babai, “Graph isomorphism in quasipolynomial time,” in *Proceedings
    of the forty-eighth annual ACM symposium on Theory of Computing*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] G. Klir and B. Yuan, *Fuzzy sets and fuzzy logic*.   Prentice hall New
    Jersey, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] J. Ma, P. Cui, X. Wang, and W. Zhu, “Hierarchical taxonomy aware network
    embedding,” in *Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery & Data Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] D. Ruppert, “The elements of statistical learning: Data mining, inference,
    and prediction,” *Journal of the Royal Statistical Society*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] U. Von Luxburg, “A tutorial on spectral clustering,” *Statistics and computing*,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] I. S. Dhillon, Y. Guan, and B. Kulis, “Weighted graph cuts without eigenvectors
    a multilevel approach,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] D. I. Shuman, M. J. Faraji, and P. Vandergheynst, “A multiscale pyramid
    transform for graph signals,” *IEEE Transactions on Signal Processing*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] B. D. Mckay and A. Piperno, *Practical graph isomorphism, II*.   Academic
    Press, Inc., 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] O. Vinyals, S. Bengio, and M. Kudlur, “Order matters: Sequence to sequence
    for sets,” *Proceedings of the 5th International Conference on Learning Representations*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Advances in Neural
    Information Processing Systems*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on Computer Vision and Pattern
    Recognition*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] A.-L. Barabási and R. Albert, “Emergence of scaling in random networks,”
    *Science*, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Ma, P. Cui, and W. Zhu, “Depthlgp: Learning embeddings of out-of-sample
    nodes in dynamic networks,” in *Proceedings of the 32nd AAAI Conference on Artificial
    Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] F. Scarselli, A. C. Tsoi, and M. Hagenbuchner, “The vapnik–chervonenkis
    dimension of graph and recursive neural networks,” *Neural Networks*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] S. Verma and Z.-L. Zhang, “Stability and generalization of graph convolutional
    neural networks,” in *KDD*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] P. Vincent, H. Larochelle, Y. Bengio, and P. A. Manzagol, “Extracting
    and composing robust features with denoising autoencoders,” in *International
    Conference on Machine Learning*, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu, “Learning deep representations
    for graph clustering.” in *Proceedings of the Twenty-Eighth AAAI Conference on
    Artificial Intelligence*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in *KDD*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations.”
    in *Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] R. v. d. Berg, T. N. Kipf, and M. Welling, “Graph convolutional matrix
    completion,” *KDD’18 Deep Learning Day*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] K. Tu, P. Cui, X. Wang, P. S. Yu, and W. Zhu, “Deep recursive network
    embedding with regular equivalence,” in *KDD*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] A. Bojchevski and S. Günnemann, “Deep gaussian embedding of graphs: Unsupervised
    inductive learning via ranking,” in *Proceedings of the 7th International Conference
    on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] T. N. Kipf and M. Welling, “Variational graph auto-encoders,” *NIPS Workshop
    on Bayesian Deep Learning*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] D. Zhu, P. Cui, D. Wang, and W. Zhu, “Deep variational network embedding
    in wasserstein space,” in *Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, “Adversarially
    regularized graph autoencoder for graph embedding.” in *Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] W. Yu, C. Zheng, W. Cheng, C. C. Aggarwal, D. Song, B. Zong, H. Chen,
    and W. Wang, “Learning deep network representations with adversarially regularized
    autoencoders,” in *Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. MacQueen *et al.*, “Some methods for classification and analysis of
    multivariate observations,” in *Proceedings of the fifth Berkeley symposium on
    mathematical statistics and probability*, 1967.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Z. Zhang, “A note on spectral clustering and svd of graph data,” *arXiv
    preprint arXiv:1809.11029*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale
    information network embedding,” in *Proceedings of the 24th International Conference
    on World Wide Web*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] L. Lovász *et al.*, “Random walks on graphs: A survey,” *Combinatorics*,
    1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank citation
    ranking: Bringing order to the web.” Stanford InfoLab, Tech. Rep., 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] S. Kullback and R. A. Leibler, “On information and sufficiency,” *The
    annals of mathematical statistics*, 1951.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang, “A tutorial
    on energy-based learning,” *Predicting structured data*, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in *Proceedings
    of the 3rd International Conference on Learning Representations*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] S. Vallender, “Calculation of the wasserstein distance between probability
    distributions on the line,” *Theory of Probability & Its Applications*, 1974.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec, “Graph convolutional
    policy network for goal-directed molecular graph generation,” in *Advances in
    Neural Information Processing Systems*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] N. De Cao and T. Kipf, “MolGAN: An implicit generative model for small
    molecular graphs,” *ICML 2018 workshop on Theoretical Foundations and Applications
    of Deep Generative Models*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] K. Do, T. Tran, and S. Venkatesh, “Graph transformation policy network
    for chemical reaction prediction,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] J. B. Lee, R. Rossi, and X. Kong, “Graph classification using structural
    attention,” in *Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery & Data Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] W. Xiong, T. Hoang, and W. Y. Wang, “Deeppath: A reinforcement learning
    method for knowledge graph reasoning,” in *Proceedings of the 2017 Conference
    on Empirical Methods in Natural Language Processing*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] R. Das, S. Dhuliawala, M. Zaheer, L. Vilnis, I. Durugkar, A. Krishnamurthy,
    A. Smola, and A. McCallum, “Go for a walk and arrive at the answer: Reasoning
    over paths in knowledge bases using reinforcement learning,” in *Proceedings of
    the 7th International Conference on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,
    T. Hubert, L. Baker, M. Lai, A. Bolton *et al.*, “Mastering the game of go without
    human knowledge,” *Nature*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, “Policy
    gradient methods for reinforcement learning with function approximation,” in *Advances
    in Neural Information Processing systems*, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, X. Xie, and M. Guo,
    “Graphgan: Graph representation learning with generative adversarial nets,” in
    *Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Q. Dai, Q. Li, J. Tang, and D. Wang, “Adversarial network embedding,”
    in *Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. Ding, J. Tang, and J. Zhang, “Semi-supervised learning on graphs with
    generative adversarial nets,” in *Proceedings of the 27th ACM International Conference
    on Information and Knowledge Management*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] A. Bojchevski, O. Shchur, D. Zügner, and S. Günnemann, “Netgan: Generating
    graphs via random walks,” in *International Conference on Machine Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] D. Zügner, A. Akbarnejad, and S. Günnemann, “Adversarial attacks on neural
    networks for graph data,” in *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery &amp; Data Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu, and L. Song, “Adversarial
    attack on graph structured data,” in *Proceedings of the 35th International Conference
    on Machine Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] D. Zügner and S. Günnemann, “Adversarial attacks on graph neural networks
    via meta learning,” in *Proceedings of the 8th International Conference on Learning
    Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of
    social representations,” in *Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] H. Dai, B. Dai, and L. Song, “Discriminative embeddings of latent variable
    models for structured data,” in *International Conference on Machine Learning*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Qiu, J. Tang, H. Ma, Y. Dong, K. Wang, and J. Tang, “Deepinf: Modeling
    influence locality in large social networks,” in *Proceedings of the 24th ACM
    SIGKDD International Conference on Knowledge Discovery and Data Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] J. Ma, C. Zhou, P. Cui, H. Yang, and W. Zhu, “Learning disentangled representations
    for recommendation,” in *Advances in Neural Information Processing Systems*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] C. W. Coley, R. Barzilay, W. H. Green, T. S. Jaakkola, and K. F. Jensen,
    “Convolutional embedding of attributed molecular graphs for physical property
    prediction,” *Journal of chemical information and modeling*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] T. Xie and J. C. Grossman, “Crystal graph convolutional neural networks
    for an accurate and interpretable prediction of material properties,” *Physical
    Review Letters*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] S. I. Ktena, S. Parisot, E. Ferrante, M. Rajchl, M. Lee, B. Glocker,
    and D. Rueckert, “Distance metric learning using graph convolutional networks:
    Application to functional brain networks,” in *International Conference on Medical
    Image Computing and Computer-Assisted Intervention*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] M. Zitnik, M. Agrawal, and J. Leskovec, “Modeling polypharmacy side effects
    with graph convolutional networks,” *Bioinformatics*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] S. Parisot, S. I. Ktena, E. Ferrante, M. Lee, R. G. Moreno, B. Glocker,
    and D. Rueckert, “Spectral graph convolutions for population-based disease prediction,”
    in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] F. Dutil, J. P. Cohen, M. Weiss, G. Derevyanko, and Y. Bengio, “Towards
    gene expression convolutions using gene interaction graphs,” in *International
    Conference on Machine Learning Workshop on Computational Biology*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] J. Bastings, I. Titov, W. Aziz, D. Marcheggiani, and K. Simaan, “Graph
    convolutional encoders for syntax-aware neural machine translation,” in *Proceedings
    of the 2017 Conference on Empirical Methods in Natural Language Processing*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] D. Marcheggiani and I. Titov, “Encoding sentences with graph convolutional
    networks for semantic role labeling,” in *EMNLP*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] V. Garcia and J. Bruna, “Few-shot learning with graph neural networks,”
    in *Proceedings of the 7th International Conference on Learning Representations*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena, “Structural-rnn: Deep
    learning on spatio-temporal graphs,” in *Computer Vision and Pattern Recognition*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun, “3d graph neural networks
    for rgbd semantic segmentation,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] K. Marino, R. Salakhutdinov, and A. Gupta, “The more you know: Using
    knowledge graphs for image classification,” in *2017 IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] S. Qi, W. Wang, B. Jia, J. Shen, and S.-C. Zhu, “Learning human-object
    interactions by graph parsing neural networks,” in *Proceedings of the European
    Conference on Computer Vision*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph convolutional networks:
    A deep learning framework for traffic forecasting,” in *Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent
    neural network: Data-driven traffic forecasting,” in *Proceedings of the 7th International
    Conference on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] M. Allamanis, M. Brockschmidt, and M. Khademi, “Learning to represent
    programs with graphs,” in *Proceedings of the 7th International Conference on
    Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Z. Li, Q. Chen, and V. Koltun, “Combinatorial optimization with graph
    convolutional networks and guided tree search,” in *NeurIPS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] M. Prates, P. H. Avelar, H. Lemos, L. C. Lamb, and M. Y. Vardi, “Learning
    to solve np-complete problems: A graph neural network for decision tsp,” in *AAAI*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] S. Sukhbaatar, R. Fergus *et al.*, “Learning multiagent communication
    with backpropagation,” in *Advances in Neural Information Processing Systems*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] P. W. Battaglia, R. Pascanu, M. Lai, D. Rezende, and K. Kavukcuoglu,
    “Interaction networks for learning about objects, relations and physics,” in *Advances
    in Neural Information Processing Systems*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Y. Hoshen, “Vain: Attentional multi-agent predictive modeling,” in *Advances
    in Neural Information Processing Systems*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] A. Santoro, D. Raposo, D. G. T. Barrett, M. Malinowski, R. Pascanu, P. Battaglia,
    and T. Lillicrap, “A simple neural network module for relational reasoning,” in
    *NeuRIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang,
    “Heterogeneous network embedding via deep architectures,” in *Proceedings of the
    21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] T. Derr, Y. Ma, and J. Tang, “Signed graph convolutional network,” in
    *Data Mining, 2018 IEEE International Conference on*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] K. Tu, P. Cui, X. Wang, F. Wang, and W. Zhu, “Structural deep embedding
    for hyper-networks,” in *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] K. Tu, J. Ma, P. Cui, J. Pei, and W. Zhu, “Autone: Hyperparameter optimization
    for massive network embedding,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] R. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec, “Gnn explainer:
    A tool for post-hoc explanation of graph neural networks,” in *Advances in Neural
    Information Processing Systems*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] D. Zhu, Z. Zhang, P. Cui, and W. Zhu, “Robust graph convolutional networks
    against adversarial attacks,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] M. Jin, H. Chang, W. Zhu, and S. Sojoudi, “Power up! robust graph convolutional
    network against evasion attacks based on graph powering,” *arXiv preprint arXiv:1905.10029*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] M. Fey and J. E. Lenssen, “Fast graph representation learning with PyTorch
    Geometric,” in *ICLR Workshop on Representation Learning on Graphs and Manifolds*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] M. Wang, L. Yu, D. Zheng, Q. Gan, Y. Gai, Z. Ye, M. Li, J. Zhou, Q. Huang,
    C. Ma, Z. Huang, Q. Guo, H. Zhang, H. Lin, J. Zhao, J. Li, A. J. Smola, and Z. Zhang,
    “Deep graph library: Towards efficient and scalable deep learning on graphs,”
    *ICLR Workshop on Representation Learning on Graphs and Manifolds*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] R. Zhu, K. Zhao, H. Yang, W. Lin, C. Zhou, B. Ai, Y. Li, and J. Zhou,
    “Aligraph: A comprehensive graph neural network platform,” in *Proceedings of
    the 45th International Conference on Very Large Data Bases*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] O. Shchur, M. Mumme, A. Bojchevski, and S. Günnemann, “Pitfalls of graph
    neural network evaluation,” *Relational Representation Learning Workshop, NeurIPS
    2018*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, “Learning deep
    generative models of graphs,” *arXiv preprint arXiv:1803.03324*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad,
    “Collective classification in network data,” *AI magazine*, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] A. Ortega, P. Frossard, J. Kovačević, J. M. Moura, and P. Vandergheynst,
    “Graph signal processing: Overview, challenges, and applications,” *Proceedings
    of the IEEE*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/d160855baae52eba5d18410a64452a06.png) | Ziwei
    Zhang received his B.S. from the Department of Physics, Tsinghua University in
    2016\. He is currently pursuing a Ph.D. Degree in the Department of Computer Science
    and Technology at Tsinghua University. His research interests focus on network
    embedding and machine learning on graph data, especially in developing scalable
    algorithms for large-scale networks. He has published several papers in prestigious
    conferences and journals, including KDD, AAAI, IJCAI, and TKDE. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/3e973fc1fa996149438faaad3dfbd44a.png) | Peng Cui
    received the Ph.D. degree from Tsinghua University in 2010\. He is currently an
    associate professor with tenure at Tsinghua University. His research interests
    include network representation learning, human behavioral modeling, and social-sensed
    multimedia computing. He has published more than 100 papers in prestigious conferences
    and journals in data mining and multimedia. His recent research efforts have received
    the SIGKDD 2016 Best Paper Finalist, the ICDM 2015 Best Student Paper Award, the
    SIGKDD 2014 Best Paper Finalist, the IEEE ICME 2014 Best Paper Award, the ACM
    MM12 Grand Challenge Multimodal Award, and the MMM13 Best Paper Award. He is an
    associate editor of IEEE Transactions on Knowledge and Data Engineering, the IEEE
    Transactions on Big Data, the ACM Transactions on Multimedia Computing, Communications,
    and Applications, the Elsevier Journal on Neurocomputing, etc. He was the recipient
    of the ACM China Rising Star Award in 2015. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/653d001f3ce84903606150601d1b90de.png) | Wenwu
    Zhu is currently a Professor and Deputy Head of the Computer Science Department
    of Tsinghua University and Vice Dean of National Research Center on Information
    Science and Technology. Prior to his current post, he was a Senior Researcher
    and Research Manager at Microsoft Research Asia. He was the Chief Scientist and
    Director at Intel Research China from 2004 to 2008\. He worked at Bell Labs New
    Jersey as a Member of Technical Staff during 1996-1999\. He received his Ph.D.
    degree from New York University in 1996. He served as the Editor-in-Chief for
    the IEEE Transactions on Multimedia (T-MM) from January 1, 2017, to December 31,
    2019\. He has been serving as Vice EiC for IEEE Transactions on Circuits and Systems
    for Video Technology (TCSVT) and the chair of the steering committee for IEEE
    T-MM since January 1, 2020\. His current research interests are in the areas of
    multimedia computing and networking, and big data. He has published over 400 papers
    in the referred journals and received nine Best Paper Awards including IEEE TCSVT
    in 2001 and 2019, and ACM Multimedia 2012\. He is an IEEE Fellow, AAAS Fellow,
    SPIE Fellow and a member of the European Academy of Sciences (Academia Europaea).
    |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IX: A collection of published source code. O.A. = Original Authors'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method | URL | O.A. | Language/Framework |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Graph RNNs | GGS-NNs [[24](#bib.bib24)] | https://github.com/yujiali/ggnn
    | Yes | Lua/Torch |'
  prefs: []
  type: TYPE_TB
- en: '| SSE [[25](#bib.bib25)] | https://github.com/Hanjun-Dai/steady_state_embedding
    | Yes | C |'
  prefs: []
  type: TYPE_TB
- en: '| You et al. [[26](#bib.bib26)] | https://github.com/JiaxuanYou/graph-generation
    | Yes | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| RMGCNN [[28](#bib.bib28)] | https://github.com/fmonti/mgcnn | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| GCNs | ChebNet [[42](#bib.bib42)] | https://github.com/mdeff/cnn_graph |
    Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| Kipf&Welling [[43](#bib.bib43)] | https://github.com/tkipf/gcn | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| CayletNet [[44](#bib.bib44)] | https://github.com/amoliu/CayleyNet | Yes
    | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| GWNN [[45](#bib.bib45)] | https://github.com/Eilene/GWNN | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| Neural FPs [[46](#bib.bib46)] | https://github.com/HIPS/neural-fingerprint
    | Yes | Python |'
  prefs: []
  type: TYPE_TB
- en: '| PATCHY-SAN [[47](#bib.bib47)] | https://github.com/seiya-kumada/patchy-san
    | No | Python |'
  prefs: []
  type: TYPE_TB
- en: '| LGCN [[48](#bib.bib48)] | https://github.com/divelab/lgcn/ | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| SortPooling [[49](#bib.bib49)] | https://github.com/muhanzhang/DGCNN | Yes
    | Lua/Torch |'
  prefs: []
  type: TYPE_TB
- en: '| DCNN [[50](#bib.bib50)] | https://github.com/jcatw/dcnn | Yes | Python/Theano
    |'
  prefs: []
  type: TYPE_TB
- en: '| DGCN [[51](#bib.bib51)] | https://github.com/ZhuangCY/Coding-NN | Yes | Python/Theano
    |'
  prefs: []
  type: TYPE_TB
- en: '| MPNNs [[52](#bib.bib52)] | https://github.com/brain-research/mpnn | Yes |
    Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| GraphSAGE [[53](#bib.bib53)] | https://github.com/williamleif/GraphSAGE |
    Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| GNs [[9](#bib.bib9)] | https://github.com/deepmind/graph_nets | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| DiffPool [[56](#bib.bib56)] | https://github.com/RexYing/graph-pooling |
    Yes | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| GAT [[57](#bib.bib57)] | https://github.com/PetarV-/GAT | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| GaAN [[58](#bib.bib58)] | https://github.com/jennyzhang0215/GaAN | Yes |
    Python/MXNet |'
  prefs: []
  type: TYPE_TB
- en: '| HAN [[59](#bib.bib59)] | https://github.com/Jhy1993/HAN | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| CLN [[60](#bib.bib60)] | https://github.com/trangptm/Column_networks | Yes
    | Python/Keras |'
  prefs: []
  type: TYPE_TB
- en: '| PPNP [[61](#bib.bib61)] | https://github.com/klicperajo/ppnp | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| JK-Nets [[62](#bib.bib62)] | https://github.com/mori97/JKNet-dgl | No | Python/DGL
    |'
  prefs: []
  type: TYPE_TB
- en: '| ECC [[63](#bib.bib63)] | https://github.com/mys007/ecc | Yes | Python/PyTorch
    |'
  prefs: []
  type: TYPE_TB
- en: '| R-GCNs [[64](#bib.bib64)] | https://github.com/tkipf/relational-gcn | Yes
    | Python/Keras |'
  prefs: []
  type: TYPE_TB
- en: '| LGNN [[65](#bib.bib65)] | https://github.com/joanbruna/GNN_community | Yes
    | Lua/Torch |'
  prefs: []
  type: TYPE_TB
- en: '| StochasticGCN [[67](#bib.bib67)] | https://github.com/thu-ml/stochastic_gcn
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| FastGCN [[68](#bib.bib68)] | https://github.com/matenure/FastGCN | Yes |
    Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| Adapt [[69](#bib.bib69)] | https://github.com/huangwb/AS-GCN | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[70](#bib.bib70)] | https://github.com/liqimai/gcn | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| SGC [[71](#bib.bib71)] | https://github.com/Tiiiger/SGC | Yes | Python/PyTorch
    |'
  prefs: []
  type: TYPE_TB
- en: '| GFNN [[72](#bib.bib72)] | https://github.com/gear/gfnn | Yes | Python/PyTorch
    |'
  prefs: []
  type: TYPE_TB
- en: '| GIN [[73](#bib.bib73)] | https://github.com/weihua916/powerful-gnns | Yes
    | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| DGI [[74](#bib.bib74)] | https://github.com/PetarV-/DGI | Yes | Python/PyTorch
    |'
  prefs: []
  type: TYPE_TB
- en: '| GAEs | SAE [[96](#bib.bib96)] | https://github.com/quinngroup/deep-representations-clustering
    | No | Python/Keras |'
  prefs: []
  type: TYPE_TB
- en: '| SDNE [[97](#bib.bib97)] | https://github.com/suanrong/SDNE | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| DNGR [[98](#bib.bib98)] | https://github.com/ShelsonCao/DNGR | Yes | Matlab
    |'
  prefs: []
  type: TYPE_TB
- en: '| GC-MC [[99](#bib.bib99)] | https://github.com/riannevdberg/gc-mc | Yes |
    Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| DRNE [[100](#bib.bib100)] | https://github.com/tadpole/DRNE | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| G2G [[101](#bib.bib101)] | https://github.com/abojchevski/graph2gauss | Yes
    | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| VGAE [[102](#bib.bib102)] | https://github.com/tkipf/gae | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| DVNE [[103](#bib.bib103)] | http://nrl.thumedialab.com | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| ARGA/ARVGA [[104](#bib.bib104)] | https://github.com/Ruiqi-Hu/ARGA | Yes
    | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| NetRA [[105](#bib.bib105)] | https://github.com/chengw07/NetRA | Yes | Python/PyTorch
    |'
  prefs: []
  type: TYPE_TB
- en: '| Graph RLs | GCPN [[116](#bib.bib116)] | https://github.com/bowenliu16/rl_graph_generation
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| MolGAN [[117](#bib.bib117)] | https://github.com/nicola-decao/MolGAN | Yes
    | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| GAM [[119](#bib.bib119)] | https://github.com/benedekrozemberczki/GAM | Yes
    | Python/Pytorhc |'
  prefs: []
  type: TYPE_TB
- en: '| DeepPath [[120](#bib.bib120)] | https://github.com/xwhan/DeepPath | Yes |
    Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| MINERVA [[121](#bib.bib121)] | https://github.com/shehzaadzd/MINERVA | Yes
    | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| Graph adversarial methods | GraphGAN [[124](#bib.bib124)] | https://github.com/hwwang55/GraphGAN
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| GraphSGAN [[126](#bib.bib126)] | https://github.com/dm-thu/GraphSGAN | Yes
    | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| NetGAN [[127](#bib.bib127)] | https://github.com/danielzuegner/netgan | Yes
    | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| Nettack [[128](#bib.bib128)] | https://github.com/danielzuegner/nettack |
    Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. [[129](#bib.bib129)] | https://github.com/Hanjun-Dai/graph_adversarial_attack
    | Yes | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| Zugner&Gunnemann [[130](#bib.bib130)] | https://github.com/danielzuegner/gnn-meta-attack
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| Applications | DeepInf [[133](#bib.bib133)] | https://github.com/xptree/DeepInf
    | Yes | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| Ma et al. [[134](#bib.bib134)] | https://jianxinma.github.io/assets/disentangle-recsys-v1.zip
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| CGCNN [[136](#bib.bib136)] | https://github.com/txie-93/cgcnn | Yes | Python/PyTorch
    |'
  prefs: []
  type: TYPE_TB
- en: '| Ktena et al. [[137](#bib.bib137)] | https://github.com/sk1712/gcn_metric_learning
    | Yes | Python |'
  prefs: []
  type: TYPE_TB
- en: '| Decagon [[138](#bib.bib138)] | https://github.com/mims-harvard/decagon |
    Yes | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| Parisot et al. [[139](#bib.bib139)] | https://github.com/parisots/population-gcn
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| Dutil et al. [[140](#bib.bib140)] | https://github.com/mila-iqia/gene-graph-conv
    | Yes | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| Bastings et al. [[141](#bib.bib141)] | https://github.com/bastings/neuralmonkey/tree/emnlp_gcn
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| Neural-dep-srl [[142](#bib.bib142)] | https://github.com/diegma/neural-dep-srl
    | Yes | Python/Therano |'
  prefs: []
  type: TYPE_TB
- en: '| Garcia&Bruna [[143](#bib.bib143)] | https://github.com/vgsatorras/few-shot-gnn
    | Yes | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| S-RNN [[144](#bib.bib144)] | https://github.com/asheshjain399/RNNexp | Yes
    | Python/Therano |'
  prefs: []
  type: TYPE_TB
- en: '| 3DGNN [[145](#bib.bib145)] | https://github.com/xjqicuhk/3DGNN | Yes | Matlab/Caffe
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPNN [[147](#bib.bib147)] | https://github.com/SiyuanQi/gpnn | Yes | Python/PyTorch
    |'
  prefs: []
  type: TYPE_TB
- en: '| STGCN [[148](#bib.bib148)] | https://github.com/VeritasYin/STGCN_IJCAI-18
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| DCRNN [[149](#bib.bib149)] | https://github.com/liyaguang/DCRNN | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| Allamanis et al. [[150](#bib.bib150)] | https://github.com/microsoft/tf-gnn-samples
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[151](#bib.bib151)] | https://github.com/intel-isl/NPHard | Yes
    | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| TSPGNN [[152](#bib.bib152)] | https://github.com/machine-reasoning-ufrgs/TSP-GNN
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| CommNet [[153](#bib.bib153)] | https://github.com/facebookresearch/CommNet
    | Yes | Lua/Torch |'
  prefs: []
  type: TYPE_TB
- en: '| Interaction network [[154](#bib.bib154)] | https://github.com/jaesik817/Interaction-networks_tensorflow
    | No | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| Relation networks [[156](#bib.bib156)] | https://github.com/kimhc6028/relational-networks
    | No | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| Miscellaneous | SGCN [[158](#bib.bib158)] | http://www.cse.msu.edu/~derrtyle/
    | Yes | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| DHNE [[159](#bib.bib159)] | https://github.com/tadpole/DHNE | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| AutoNE [[160](#bib.bib160)] | https://github.com/tadpole/AutoNE | Yes | Python
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gnn-explainer [[161](#bib.bib161)] | https://github.com/RexYing/gnn-model-explainer
    | Yes | Python/PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| RGCN [[162](#bib.bib162)] | https://github.com/thumanlab/nrlweb | Yes | Python/TensorFlow
    |'
  prefs: []
  type: TYPE_TB
- en: '| GNN-benchmark [[167](#bib.bib167)] | https://github.com/shchur/gnn-benchmark
    | Yes | Python/TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE X: A Table of Methods for Six Common Tasks'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Task | Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Node-focused Tasks | Node Clustering | [[96](#bib.bib96), [59](#bib.bib59),
    [157](#bib.bib157), [97](#bib.bib97), [124](#bib.bib124), [104](#bib.bib104),
    [44](#bib.bib44), [98](#bib.bib98)] |'
  prefs: []
  type: TYPE_TB
- en: '| Node Classification | Transductive |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[41](#bib.bib41), [23](#bib.bib23), [25](#bib.bib25), [27](#bib.bib27),
    [42](#bib.bib42), [48](#bib.bib48), [54](#bib.bib54), [45](#bib.bib45), [51](#bib.bib51),
    [43](#bib.bib43), [50](#bib.bib50), [53](#bib.bib53), [44](#bib.bib44), [29](#bib.bib29)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[64](#bib.bib64), [61](#bib.bib61), [65](#bib.bib65), [57](#bib.bib57),
    [59](#bib.bib59), [60](#bib.bib60), [68](#bib.bib68), [67](#bib.bib67), [58](#bib.bib58),
    [70](#bib.bib70), [62](#bib.bib62), [69](#bib.bib69), [71](#bib.bib71), [72](#bib.bib72)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[103](#bib.bib103), [101](#bib.bib101), [74](#bib.bib74), [100](#bib.bib100),
    [126](#bib.bib126), [124](#bib.bib124), [125](#bib.bib125), [162](#bib.bib162),
    [157](#bib.bib157), [97](#bib.bib97), [105](#bib.bib105)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Inductive | [[74](#bib.bib74), [48](#bib.bib48), [57](#bib.bib57), [53](#bib.bib53),
    [67](#bib.bib67), [58](#bib.bib58), [62](#bib.bib62), [25](#bib.bib25), [71](#bib.bib71),
    [101](#bib.bib101), [72](#bib.bib72), [68](#bib.bib68), [69](#bib.bib69)] |'
  prefs: []
  type: TYPE_TB
- en: '| Network Reconstruction | [[157](#bib.bib157), [97](#bib.bib97), [103](#bib.bib103),
    [105](#bib.bib105)] |'
  prefs: []
  type: TYPE_TB
- en: '| Link Prediction | [[102](#bib.bib102), [99](#bib.bib99), [97](#bib.bib97),
    [101](#bib.bib101), [124](#bib.bib124), [103](#bib.bib103), [66](#bib.bib66),
    [28](#bib.bib28), [64](#bib.bib64), [104](#bib.bib104), [44](#bib.bib44), [27](#bib.bib27),
    [105](#bib.bib105)] |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-focused Tasks | Graph Classification | [[40](#bib.bib40), [41](#bib.bib41),
    [29](#bib.bib29), [44](#bib.bib44), [49](#bib.bib49), [56](#bib.bib56), [55](#bib.bib55),
    [119](#bib.bib119), [132](#bib.bib132), [47](#bib.bib47), [50](#bib.bib50), [73](#bib.bib73),
    [71](#bib.bib71), [63](#bib.bib63), [42](#bib.bib42), [23](#bib.bib23), [54](#bib.bib54)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Graph Generation | Structure-only | [[127](#bib.bib127), [26](#bib.bib26)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Structure+features | [[117](#bib.bib117), [116](#bib.bib116), [168](#bib.bib168)]
    |'
  prefs: []
  type: TYPE_TB
- en: Appendix A Source Codes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [IX](#A0.T9 "TABLE IX ‣ Deep Learning on Graphs: A Survey") shows a collection
    and summary of the source code we collected for the papers discussed in this manuscript.
    In addition to method names and links, the table also lists the programming language
    used and the frameworks adopted as well as whether the code was published by the
    original authors of the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Applicability for Common Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [X](#A0.T10 "TABLE X ‣ Deep Learning on Graphs: A Survey") summarizes
    the applicability of different models for six common graph tasks, including node
    clustering, node classification, network reconstruction, link prediction, graph
    classification, and graph generation. Note that these results are based on whether
    the experiments were reported in the original papers.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Node Classification Results on Benchmark Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As shown in Appendix [B](#A2 "Appendix B Applicability for Common Tasks ‣ Deep
    Learning on Graphs: A Survey"), node classification is the most common task for
    graph-based deep learning models. Here, we report the results of different methods
    on five node classification benchmark datasets^(11)^(11)11These five benchmark
    datasets are publicly available at https://github.com/tkipf/gcn or http://snap.stanford.edu/graphsage/.:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cora, Citeseer, PubMed [[169](#bib.bib169)]: These are citation graphs with
    nodes representing papers, edges representing citations between papers, and papers
    associated with bag-of-words features and ground-truth topics as labels.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reddit [[53](#bib.bib53)]: Reddit is an online discussion forum in which nodes
    represent posts and two nodes are connected when they are commented by the same
    user, and each post contains a low-dimensional word vector as features and a label
    indicating the Reddit community in which it was posted.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PPI [[53](#bib.bib53)]: PPI is a collection of protein-protein interaction
    graphs for different human tissues. It includes features that represent biological
    signatures and labels that represent the roles of proteins.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Cora, Citeseer, and Pubmed each include one graph, and the same graph structure
    is used for both training and testing, thus the tasks are considered transductive.
    In Reddit and PPI, because the training and testing graphs are different, these
    two datasets are considered to be inductive node classification benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [XI](#A3.T11 "TABLE XI ‣ Appendix C Node Classification Results on
    Benchmark Datasets ‣ Deep Learning on Graphs: A Survey"), we report the results
    of different models on these benchmark datasets. The results were extracted from
    their original papers when a fixed dataset split was adopted. The table shows
    that many state-of-the-art methods achieve roughly comparable performance on these
    benchmarks, with differences smaller than one percent. Shchur et al. [[167](#bib.bib167)]
    also found that a fixed dataset split can easily result in spurious comparisons.
    As a result, although these benchmarks have been widely adopted to compare different
    models, more comprehensive evaluation setups are critically needed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XI: Statistics of the benchmark datasets and the node classification
    results of different methods when a fixed dataset split is adopted. A hyphen (’-’)
    indicates that the result is unavailable in the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Cora | Citeseer | Pubmed | Reddit | PPI |'
  prefs: []
  type: TYPE_TB
- en: '| Type | Citation | Citation | Citation | Social | Biology |'
  prefs: []
  type: TYPE_TB
- en: '| Nodes | 2,708 | 3,327 | 19,717 | 232,965 | 56,944 (24 graphs) |'
  prefs: []
  type: TYPE_TB
- en: '| Edges | 5,429 | 4,732 | 44,338 | 11,606,919 | 818,716 |'
  prefs: []
  type: TYPE_TB
- en: '| Classes | 7 | 6 | 3 | 41 | 121 |'
  prefs: []
  type: TYPE_TB
- en: '| Features | 1,433 | 3,703 | 500 | 602 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| Task | Transductive | Transductive | Transductive | Inductive | Inductive
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bruna et al. [[40](#bib.bib40)]^(12)^(12)footnotemark: 12 | 73.3 | 58.9 |
    73.9 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ChebNet [[42](#bib.bib42)]^(13)^(13)13The results were reported in GWNN [[45](#bib.bib45)].
    | 81.2 | 69.8 | 74.4 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GCN [[43](#bib.bib43)] | 81.5 | 70.3 | 79.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CayleyNets [[44](#bib.bib44)] | 81.9$\pm$0.7 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GWNN [[45](#bib.bib45)] | 82.8 | 71.7 | 79.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LGCN [[48](#bib.bib48)] | 83.3$\pm$0.5 | 73.0$\pm$0.6 | 79.5$\pm$0.2 | -
    | 77.2$\pm$0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DGCN [[51](#bib.bib51)] | 83.5 | 72.6 | 80.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GraphSAGE [[53](#bib.bib53)] | - | - | - | 95.4 | 61.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MoNet [[54](#bib.bib54)] | 81.7$\pm$0.5 | - | 78.8$\pm$0.4 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GAT [[57](#bib.bib57)] | 83.0$\pm$0.7 | 72.5$\pm$0.7 | 79.0$\pm$0.3 | - |
    97.3$\pm$0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| GaAN [[58](#bib.bib58)] | - | - | - | 96.4$\pm$0.0 | 98.7$\pm$0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| JK-Nets [[62](#bib.bib62)] | - | - | - | 96.5 | 97.6$\pm$0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| StochasticGCN [[67](#bib.bib67)] | 82.0$\pm$0.8 | 70.9$\pm$0.2 | 79.0$\pm$0.4
    | 96.3$\pm$0.0 | 97.9$\pm$0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| FastGCN [[68](#bib.bib68)] | 72.3 | - | 72.1 | 93.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Adapt [[69](#bib.bib69)] | - | - | - | 96.3$\pm$0.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SGC [[71](#bib.bib71)] | 81.0$\pm$0.0 | 71.9$\pm$0.1 | 78.9$\pm$0.0 | 94.9
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| DGI [[74](#bib.bib74)] | 82.3$\pm$0.6 | 71.8$\pm$0.7 | 76.8$\pm$0.6 | 94.0$\pm$0.1
    | 63.8$\pm$0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| SSE [[25](#bib.bib25)] | - | - | - | - | 83.6 |'
  prefs: []
  type: TYPE_TB
- en: '| GraphSGAN [[126](#bib.bib126)] | 83.0$\pm$1.3 | 73.1$\pm$1.8 | - | - | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| RGCN [[162](#bib.bib162)] | 82.8$\pm$0.6 | 71.2$\pm$0.5 | 79.1$\pm$0.3 |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/00113f5c975bd00b305a3b9a4a5d1dbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: An example of graph signals and its spectral representations transformed
    using the eigenvectors of the graph Laplacian matrix. The upward-pointing red
    lines represent positive values and the downward-pointing green lines represent
    negative values. These images were adapted from [[6](#bib.bib6)].'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D An Example of Graph Signals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To help understanding GCNs, we provide an example of graph signals and refer
    readers to [[6](#bib.bib6), [7](#bib.bib7), [170](#bib.bib170)] for more comprehensive
    surveys.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a graph $G=(V,E)$, a graph signal $\mathbf{f}$ corresponds to a collection
    of numbers: one number for each node in the graph. For undirected graphs, we usually
    assume that the signal takes real values, i.e., $\mathbf{f}\in\mathbb{R}^{N}$,
    where $N$ is the number of nodes. Any node feature satisfying the above requirement
    can be regarded as a graph signal, with an example shown in Fig [11](#A3.F11 "Figure
    11 ‣ Appendix C Node Classification Results on Benchmark Datasets ‣ Deep Learning
    on Graphs: A Survey") (A). Both the signal values and the underlying graph structure
    are important in processing and analyzing graph signals. For example, we can transform
    a graph signal into the spectral domain using the eigenvectors of the graph Laplacian
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{f}}=\mathbf{Q}^{T}\mathbf{f}$ |  | (64) |'
  prefs: []
  type: TYPE_TB
- en: or equivalently
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{f}}_{i}=\mathbf{Q}^{T}(i,:)\mathbf{f}.$ |  | (65) |'
  prefs: []
  type: TYPE_TB
- en: 'Because the eigenvectors $\mathbf{Q}^{T}$ are sorted in ascending order based
    on their corresponding eigenvalues, it has been shown [[6](#bib.bib6)] that they
    form a basis for graph signals based on different “smoothness”. Specifically,
    eigenvectors corresponding to small eigenvalues represent smooth signals and low
    frequencies, while eigenvectors corresponding to large eigenvalues represent non-smooth
    signals and high frequencies, as shown in Fig [11](#A3.F11 "Figure 11 ‣ Appendix
    C Node Classification Results on Benchmark Datasets ‣ Deep Learning on Graphs:
    A Survey") (B). Note that the smoothness is measured with respect to the graph
    structure, i.e., whether the signals oscillate across edges in the graph. As a
    result, $\hat{\mathbf{f}}$ provides a spectral representation of the signal $\mathbf{f}$
    as shown in Fig [11](#A3.F11 "Figure 11 ‣ Appendix C Node Classification Results
    on Benchmark Datasets ‣ Deep Learning on Graphs: A Survey") (C). This is similar
    to the Fourier transform in Euclidean spaces. Using $\hat{\mathbf{f}}$, we can
    design various signal processing operations. For example, if we apply a low-pass
    filter, the resulted signal will be more smooth, as shown in Fig [11](#A3.F11
    "Figure 11 ‣ Appendix C Node Classification Results on Benchmark Datasets ‣ Deep
    Learning on Graphs: A Survey") (D) (in this example, we set the frequency threshold
    as 2, i.e., only keeping the lowest 4 frequencies).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Time Complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '^(13)^(13)footnotetext: The results were reported in Kipf and Welling [[102](#bib.bib102)].'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explain how we obtained the time complexity in all the tables.
    Specifically, we mainly focus on the time complexity with respect to the graph
    size, e.g., the number of nodes $N$ and the number of edges $M$, and omit other
    factors, e.g., the number of hidden dimensions $f_{l}$ or the number of iterations,
    since the latter terms are usually set as small constants and are less dominant.
    Note that we focus on the theoretical results, while the exact efficiency of one
    algorithm also depends heavily on its implementations and techniques to reduce
    the constants in the time complexity.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GNN [[23](#bib.bib23)]: $O(MI_{f})$, where $I_{f}$ is the number of iterations
    for Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks
    ‣ Deep Learning on Graphs: A Survey")) to reach stable points, as shown in the
    paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GGS-NNs [[24](#bib.bib24)]: $O(MT)$, where $T$ is a preset maximum pseudo time
    since the method utilizes all the edges in each updating.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SSE [[25](#bib.bib25)]: $O(d_{\text{avg}}S)$, where $d_{\text{avg}}$ is the
    average degree and $S$ is the total number of samples, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. [[26](#bib.bib26)]: $O(N^{2})$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DGNN [[27](#bib.bib27)]: $O(Md_{\text{avg}})$, where $d_{\text{avg}}$ is the
    average degree since the effect of the one-step propagation of each edge is considered.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RMGCNN [[28](#bib.bib28)]: $O(MN)$ or $O(M)$, depending on whether an approximation
    technique is adopted, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic GCN [[29](#bib.bib29)]: $O(Mt)$, where $t$ denotes the number of time
    slices since the model runs one GCN at each time slice.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bruna et al. [[40](#bib.bib40)] and Henaff et al. [[41](#bib.bib41)]: $O(N^{3})$,
    due to the time complexity of the eigendecomposition.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ChebNet [[42](#bib.bib42)], Kipf and Welling [[43](#bib.bib43)], CayletNet [[44](#bib.bib44)],
    GWNN [[45](#bib.bib45)], and Neural FPs [[46](#bib.bib46)]: $O(M)$, as shown in
    the corresponding papers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PATCHY-SAN [[47](#bib.bib47)]: $O(M\log N)$, assuming the method adopts WL
    to label nodes, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LGCN [[48](#bib.bib48)]: $O(M)$ since all the neighbors of each node are sorted
    in the method.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SortPooling [[49](#bib.bib49)]: $O(M)$, due to the time complexity of adopted
    graph convolution layers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DCNN [[50](#bib.bib50)]: $O(N^{2})$, as reported in [[43](#bib.bib43)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DGCN [[51](#bib.bib51)]: $O(N^{2})$ since the PPMI matrix is not sparse.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MPNNs [[52](#bib.bib52)]: $O(M)$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GraphSAGE [[53](#bib.bib53)]: $O(Ns^{L})$, where $s$ is the size of the sampled
    neighborhoods and $L$ is the number of layers, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MoNet [[54](#bib.bib54)]: $O(M)$ since only the existing node pairs are involved
    in the calculation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GNs [[9](#bib.bib9)]: $O(M)$ since only the existing node pairs are involved
    in the calculation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kearnes et al. [[55](#bib.bib55)]: $O(M)$, since only the existing node pairs
    are used in the calculation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DiffPool [[56](#bib.bib56)]: $O(N^{2})$ since the coarsened graph is not sparse.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GAT [[57](#bib.bib57)]: $O(M)$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GaAN [[58](#bib.bib58)]: $O(Ns^{L})$, where $s$ is a preset maximum neighborhood
    length and $L$ is the number of layers, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAN [[59](#bib.bib59)]: $O(M_{\phi})$, the number of meta-path-based node pairs,
    as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CLN [[60](#bib.bib60)]: $O(M)$ since only the existing node pairs are involved
    in the calculation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PPNP [[61](#bib.bib61)]: $O(M)$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JK-Nets [[62](#bib.bib62)]: $O(M)$, due to the time complexity in adopted graph
    convolutional layers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ECC [[63](#bib.bib63)]: $O(M)$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'R-GCNs [[64](#bib.bib64)]: $O(M)$ since the edges of different types sum up
    to the total number of edges of the graph.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LGNN [[65](#bib.bib65)]: $O(M)$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PinSage [[66](#bib.bib66)]: $O(Ns^{L})$, where $s$ is the size of the sampled
    neighborhoods and $L$ is the number of layers since a sampling strategy similar
    to that of GraphSAGE [[53](#bib.bib53)] is adopted.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'StochasticGCN [[67](#bib.bib67)]: $O(Ns^{L})$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FastGCN [[68](#bib.bib68)] and Adapt [[69](#bib.bib69)]: $O(NsL)$ since the
    samples are drawn in each layer instead of in the neighborhoods, as shown in the
    paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [[70](#bib.bib70)]: $O(M)$, due to the time complexity in adopted
    graph convolutional layers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SGC [[71](#bib.bib71)]: $O(M)$ since the calculation is the same as Kipf and
    Welling [[43](#bib.bib43)] by not adopting nonlinear activations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GFNN [[72](#bib.bib72)]: $O(M)$ since the calculation is the same as SGC [[71](#bib.bib71)]
    by adding an extra MLP layer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GIN [[73](#bib.bib73)]: $O(M)$, due to the time complexity in adopted graph
    convolutional layers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DGI [[74](#bib.bib74)]: $O(M)$, due to the time complexity in adopted graph
    convolutional layers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SAE [[96](#bib.bib96)] and SDNE [[97](#bib.bib97)]: $O(M)$, as shown in the
    corresponding papers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DNGR [[98](#bib.bib98)]: $O(N^{2})$, due to the time complexity of calculating
    the PPMI matrix.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GC-MC [[99](#bib.bib99)]: $O(M)$ since the encoder adopts the GCN proposed
    by Kipf and Welling [[43](#bib.bib43)] and only the non-zero elements of the graph
    are considered in the decoder.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DRNE [[100](#bib.bib100)]: $O(Ns)$, where $s$ is a preset maximum neighborhood
    length, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'G2G [[101](#bib.bib101)]: $O(M)$, due to the definition of the ranking loss.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VGAE [[102](#bib.bib102)]: $O(N^{2})$, due to the reconstruction of all the
    node pairs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DVNE [[103](#bib.bib103)]: Though the original paper reported to have a time
    complexity of $O(Md_{\text{avg}})$ where $d_{\text{avg}}$ is the average degree,
    we have confirmed that it can be easily improved to $O(M)$ through personal communications
    with the authors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ARGA/ARVGA [[104](#bib.bib104)]: $O(N^{2})$, due to the reconstruction of all
    the node pairs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NetRA [[105](#bib.bib105)]: $O(M)$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GCPN [[116](#bib.bib116)]: $O(MN)$ since the embedding of all the nodes are
    used when generating each edge.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MolGAN [[117](#bib.bib117)] and GTPN [[118](#bib.bib118)]: $O(N^{2})$ since
    the scores for all the node pairs have to be calculated.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GAM [[119](#bib.bib119)]: $O(d_{\text{avg}}sT)$, where $d_{\text{avg}}$ is
    the average degree, $s$ is the number of sampled random walks, and $T$ is the
    walk length, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DeepPath [[120](#bib.bib120)]: $O(d_{\text{avg}}sT+s^{2}T)$, where $d_{\text{avg}}$
    is the average degree, $s$ is the number of sampled paths, and $T$ is the path
    length. The former term corresponds to finding paths and the latter term results
    from the diversity constraint.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MINERVA [[121](#bib.bib121)]: $O(d_{\text{avg}}sT)$, where $d_{\text{avg}}$
    is the average degree, $s$ is the number of sampled paths, and $T$ is the path
    length, similar to the pathfinding method in DeepPath [[120](#bib.bib120)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GraphGAN [[124](#bib.bib124)]: $O(MN)$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANE [[125](#bib.bib125)]: $O(N)$, which is the extra time complexity introduced
    by the model in the generator and the discriminator.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GraphSGAN [[126](#bib.bib126)]: $O(N^{2})$, due to the time complexity in the
    objective function.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NetGAN [[127](#bib.bib127)]: $O(M)$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nettack [[128](#bib.bib128)]: $O(Nd_{0}^{2})$, where $d_{0}$ is the degree
    of the targeted node, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. [[129](#bib.bib129)]: $O(M)$, which is the time complexity of the
    most effective strategy RL-S2V, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zugner and Gunnemann [[130](#bib.bib130)]: $O(N^{2})$, as shown in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
