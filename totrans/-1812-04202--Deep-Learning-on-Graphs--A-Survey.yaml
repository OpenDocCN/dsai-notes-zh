- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:06:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:57
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1812.04202] Deep Learning on Graphs: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1812.04202] 图上的深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1812.04202](https://ar5iv.labs.arxiv.org/html/1812.04202)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1812.04202](https://ar5iv.labs.arxiv.org/html/1812.04202)
- en: 'Deep Learning on Graphs: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图上的深度学习：综述
- en: Ziwei Zhang, Peng Cui and Wenwu Zhu Z. Zhang, P. Cui, and W. Zhu are with the
    Department of Computer Science and Technology, Tsinghua University, Beijing, China.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张子伟、崔鹏和朱文武 Z. Zhang、P. Cui 和 W. Zhu 现为清华大学计算机科学与技术系，北京，中国。
- en: 'E-mail: zw-zhang16@mails.tsinghua.edu.cn, cuip@tsinghua.edu.cn,'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：zw-zhang16@mails.tsinghua.edu.cn, cuip@tsinghua.edu.cn,
- en: wwzhu@tsinghua.edu.cn. P. Cui and W. Zhu are corresponding authors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: wwzhu@tsinghua.edu.cn。P. Cui 和 W. Zhu 是通讯作者。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep learning has been shown to be successful in a number of domains, ranging
    from acoustics, images, to natural language processing. However, applying deep
    learning to the ubiquitous graph data is non-trivial because of the unique characteristics
    of graphs. Recently, substantial research efforts have been devoted to applying
    deep learning methods to graphs, resulting in beneficial advances in graph analysis
    techniques. In this survey, we comprehensively review the different types of deep
    learning methods on graphs. We divide the existing methods into five categories
    based on their model architectures and training strategies: graph recurrent neural
    networks, graph convolutional networks, graph autoencoders, graph reinforcement
    learning, and graph adversarial methods. We then provide a comprehensive overview
    of these methods in a systematic manner mainly by following their development
    history. We also analyze the differences and compositions of different methods.
    Finally, we briefly outline the applications in which they have been used and
    discuss potential future research directions.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已在多个领域取得成功，包括声学、图像和自然语言处理。然而，由于图的独特特性，将深度学习应用于普遍存在的图数据并非易事。近年来，大量研究致力于将深度学习方法应用于图，从而在图分析技术上取得了有益的进展。在这项综述中，我们全面回顾了图上不同类型的深度学习方法。我们根据模型架构和训练策略将现有方法分为五类：图递归神经网络、图卷积网络、图自编码器、图强化学习和图对抗方法。然后，我们系统地概述了这些方法，主要按照其发展历程进行。我们还分析了不同方法的差异和组成。最后，我们简要概述了这些方法的应用，并讨论了潜在的未来研究方向。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Graph Data, Deep Learning, Graph Neural Network, Graph Convolutional Network,
    Graph Autoencoder.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据、深度学习、图神经网络、图卷积网络、图自编码器。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Over the past decade, deep learning has become the “crown jewel” of artificial
    intelligence and machine learning [[1](#bib.bib1)], showing superior performance
    in acoustics [[2](#bib.bib2)], images [[3](#bib.bib3)] and natural language processing [[4](#bib.bib4)],
    etc. The expressive power of deep learning to extract complex patterns from underlying
    data is well recognized. On the other hand, graphs¹¹1Graphs are also called *networks*
    such as in social networks. In this paper, we use two terms interchangeably. are
    ubiquitous in the real world, representing objects and their relationships in
    varied domains, including social networks, e-commerce networks, biology networks,
    traffic networks, and so on. Graphs are also known to have complicated structures
    that can contain rich underlying values [[5](#bib.bib5)]. As a result, how to
    utilize deep learning methods to analyze graph data has attracted considerable
    research attention over the past few years. This problem is non-trivial because
    several challenges exist in applying traditional deep learning architectures to
    graphs:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，深度学习已成为人工智能和机器学习的“皇冠上的明珠”[[1](#bib.bib1)]，在声学[[2](#bib.bib2)]、图像[[3](#bib.bib3)]和自然语言处理[[4](#bib.bib4)]等领域表现优越。深度学习提取潜在数据中的复杂模式的表达能力得到广泛认可。另一方面，图¹¹1图也被称为*网络*，例如社交网络。本文中，我们将这两个术语交替使用。在现实世界中，图是普遍存在的，表示不同领域中的对象及其关系，包括社交网络、电子商务网络、生物网络、交通网络等。图的结构复杂，包含丰富的潜在值[[5](#bib.bib5)]。因此，如何利用深度学习方法分析图数据在过去几年中引起了大量研究关注。这一问题并非易事，因为将传统深度学习架构应用于图存在多个挑战：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Irregular structures of graphs. Unlike images, audio, and text, which have a
    clear grid structure, graphs have irregular structures, making it hard to generalize
    some of the basic mathematical operations to graphs [[6](#bib.bib6)]. For example,
    defining convolution and pooling operations, which are the fundamental operations
    in convolutional neural networks (CNNs), for graph data is not straightforward.
    This problem is often referred to as the geometric deep learning problem [[7](#bib.bib7)].
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图的非规则结构。与图像、音频和文本具有清晰的网格结构不同，图具有非规则结构，这使得将一些基本数学操作推广到图上变得困难[[6](#bib.bib6)]。例如，为图数据定义卷积和池化操作，这些是卷积神经网络（CNNs）中的基本操作，并不是很简单。这个问题通常被称为几何深度学习问题[[7](#bib.bib7)]。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Heterogeneity and diversity of graphs. A graph itself can be complicated, containing
    diverse types and properties. For example, graphs can be heterogeneous or homogenous,
    weighted or unweighted, and signed or unsigned. In addition, the tasks of graphs
    also vary widely, ranging from node-focused problems such as node classification
    and link prediction to graph-focused problems such as graph classification and
    graph generation. These diverse types, properties, and tasks require different
    model architectures to tackle specific problems.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图的异质性和多样性。图本身可能非常复杂，包含多种类型和属性。例如，图可以是异质的或同质的，加权或未加权，以及有符号或无符号。此外，图的任务也广泛多样，从节点分类和链接预测等以节点为中心的问题，到图分类和图生成等以图为中心的问题。这些多样的类型、属性和任务需要不同的模型架构来解决特定问题。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Large-scale graphs. In the big-data era, real graphs can easily have millions
    or billions of nodes and edges; some well-known examples are social networks and
    e-commerce networks [[8](#bib.bib8)]. Therefore, how to design scalable models,
    preferably models that have a linear time complexity with respect to the graph
    size, is a key problem.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大规模图。在大数据时代，真实的图可以拥有数百万或数十亿的节点和边；一些著名的例子包括社交网络和电子商务网络[[8](#bib.bib8)]。因此，如何设计可扩展的模型，最好是与图大小成线性时间复杂度的模型，是一个关键问题。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Incorporating interdisciplinary knowledge. Graphs are often connected to other
    disciplines, such as biology, chemistry, and social sciences. This interdisciplinary
    nature provides both opportunities and challenges: domain knowledge can be leveraged
    to solve specific problems but integrating domain knowledge can complicate model
    designs. For example, when generating molecular graphs, the objective function
    and chemical constraints are often non-differentiable; therefore gradient-based
    training methods cannot easily be applied.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨学科知识的融合。图常常与其他学科相关联，如生物学、化学和社会科学。这种跨学科的性质带来了机会和挑战：可以利用领域知识来解决特定问题，但整合领域知识可能会使模型设计复杂化。例如，在生成分子图时，目标函数和化学约束通常是不可微分的；因此，基于梯度的训练方法难以应用。
- en: To tackle these challenges, tremendous efforts have been made in this area,
    resulting in a rich literature of related papers and methods. The adopted architectures
    and training strategies also vary greatly, ranging from supervised to unsupervised
    and from convolutional to recursive. However, to the best of our knowledge, little
    effort has been made to systematically summarize the differences and connections
    between these diverse methods.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，在这一领域已经付出了巨大的努力，产生了丰富的相关文献和方法。采用的架构和训练策略也大相径庭，从监督到无监督，从卷积到递归。然而，据我们所知，尚未有系统总结这些不同方法之间差异和联系的工作。
- en: '![Refer to caption](img/761eb80733568f5b76cfa0590e399272.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/761eb80733568f5b76cfa0590e399272.png)'
- en: 'Figure 1: A categorization of deep learning methods on graphs. We divide the
    existing methods into five categories: graph recurrent neural networks, graph
    convolutional networks, graph autoencoders, graph reinforcement learning, and
    graph adversarial methods.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：图上的深度学习方法的分类。我们将现有方法分为五类：图的递归神经网络、图卷积网络、图自编码器、图强化学习和图对抗方法。
- en: 'TABLE I: Main Distinctions among Deep Learning Methods on Graphs'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：图上的深度学习方法主要区别
- en: '| Category | Basic Assumptions/Aims | Main Functions |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 基本假设/目标 | 主要功能 |'
- en: '| --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Graph recurrent neural networks | Recursive and sequential patterns of graphs
    | Definitions of states for nodes or graphs |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 图的递归神经网络 | 图的递归和顺序模式 | 节点或图的状态定义 |'
- en: '| Graph convolutional networks | Common local and global structural patterns
    of graphs | Graph convolution and readout operations |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 图形卷积网络 | 图形的常见局部和全局结构模式 | 图形卷积和读取操作 |'
- en: '| Graph autoencoders | Low-rank structures of graphs | Unsupervised node representation
    learning |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 图形自编码器 | 图形的低秩结构 | 无监督的节点表示学习 |'
- en: '| Graph reinforcement learning | Feedbacks and constraints of graph tasks |
    Graph-based actions and rewards |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 图形强化学习 | 图形任务的反馈和约束 | 基于图形的动作和奖励 |'
- en: '| Graph adversarial methods | The generalization ability and robustness of
    graph-based models | Graph adversarial trainings and attacks |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 图形对抗方法 | 基于图形模型的泛化能力和鲁棒性 | 图形对抗训练和攻击 |'
- en: 'In this paper, we try to fill this knowledge gap by comprehensively reviewing
    deep learning methods on graphs. Specifically, as shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Deep Learning on Graphs: A Survey"), we divide the existing
    methods into five categories based on their model architectures and training strategies:
    graph recurrent neural networks (Graph RNNs), graph convolutional networks (GCNs),
    graph autoencoders (GAEs), graph reinforcement learning (Graph RL), and graph
    adversarial methods. We summarize some of the main characteristics of these categories
    in Table [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Deep Learning on Graphs: A Survey")
    based on the following high-level distinctions. Graph RNNs capture recursive and
    sequential patterns of graphs by modeling states at either the node-level or the
    graph-level. GCNs define convolution and readout operations on irregular graph
    structures to capture common local and global structural patterns. GAEs assume
    low-rank graph structures and adopt unsupervised methods for node representation
    learning. Graph RL defines graph-based actions and rewards to obtain feedbacks
    on graph tasks while following constraints. Graph adversarial methods adopt adversarial
    training techniques to enhance the generalization ability of graph-based models
    and test their robustness by adversarial attacks.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们试图通过全面回顾图形上的深度学习方法来填补这一知识空白。具体来说，如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep Learning on Graphs: A Survey")所示，我们根据模型架构和训练策略将现有的方法分为五类：图形递归神经网络（Graph
    RNNs）、图形卷积网络（GCNs）、图形自编码器（GAEs）、图形强化学习（Graph RL）和图形对抗方法。我们在表 [I](#S1.T1 "TABLE
    I ‣ 1 Introduction ‣ Deep Learning on Graphs: A Survey")中基于以下高层次的区别总结了这些类别的一些主要特点。图形递归神经网络通过在节点级或图级建模状态来捕捉图形的递归和序列模式。图形卷积网络在不规则的图形结构上定义卷积和读取操作，以捕捉常见的局部和全局结构模式。图形自编码器假设图形结构是低秩的，并采用无监督的方法进行节点表示学习。图形强化学习定义基于图形的动作和奖励，以在遵循约束的同时获得图形任务的反馈。图形对抗方法采用对抗训练技术来增强基于图形模型的泛化能力，并通过对抗攻击测试其鲁棒性。'
- en: In the following sections, we provide a comprehensive and detailed overview
    of these methods, mainly by following their development history and the various
    ways these methods solve the challenges posed by graphs. We also analyze the differences
    between these models and delve into how to composite different architectures.
    Finally, we briefly outline the applications of these models, introduce several
    open libraries, and discuss potential future research directions. In the appendix,
    we provide a source code repository, analyze the time complexity of various methods
    discussed in the paper, and summarize some common applications.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们提供了这些方法的全面而详细的概述，主要通过追溯它们的发展历史以及这些方法解决图形问题的各种方式来进行分析。我们还分析了这些模型之间的差异，并探讨了如何组合不同的架构。最后，我们简要概述了这些模型的应用，介绍了几个开放库，并讨论了潜在的未来研究方向。在附录中，我们提供了源代码库，分析了论文中讨论的各种方法的时间复杂度，并总结了一些常见的应用。
- en: Related works. Several previous surveys are related to our paper. Bronstein et al. [[7](#bib.bib7)]
    summarized some early GCN methods as well as CNNs on manifolds and studied them
    comprehensively through geometric deep learning. Battaglia et al. [[9](#bib.bib9)]
    summarized how to use GNNs and GCNs for relational reasoning using a unified framework
    called graph networks, Lee et al. [[10](#bib.bib10)] reviewed the attention models
    for graphs, Zhang et al. [[11](#bib.bib11)] summarized some GCNs, and Sun et al. [[12](#bib.bib12)]
    briefly surveyed adversarial attacks on graphs. Our work differs from these previous
    works in that we systematically and comprehensively review different deep learning
    architectures on graphs rather than focusing on one specific branch. Concurrent
    to our work, Zhou et al. [[13](#bib.bib13)] and Wu  et al. [[14](#bib.bib14)]
    surveyed this field from different viewpoints and categorizations. Specifically,
    neither of their works consider graph reinforcement learning or graph adversarial
    methods, which are covered in this paper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作。若干先前的调查与我们的论文相关。Bronstein 等人 [[7](#bib.bib7)] 总结了一些早期的GCN方法以及流形上的CNN，并通过几何深度学习进行了全面研究。Battaglia 等人 [[9](#bib.bib9)]
    总结了如何使用GNN和GCN进行关系推理，采用了名为图网络的统一框架，Lee 等人 [[10](#bib.bib10)] 回顾了图的注意力模型，Zhang 等人 [[11](#bib.bib11)]
    总结了一些GCN，而Sun 等人 [[12](#bib.bib12)] 简要调查了对图的对抗攻击。我们的工作与这些先前的工作不同之处在于，我们系统地、全面地回顾了图上的不同深度学习架构，而不是专注于某个特定的分支。与我们的工作同时进行的还有Zhou 等人 [[13](#bib.bib13)]
    和Wu 等人 [[14](#bib.bib14)] 从不同的观点和分类进行调查。具体而言，他们的工作都没有考虑图强化学习或图对抗方法，而这些在本文中都有涉及。
- en: Another closely related topic is network embedding, aiming to embed nodes into
    a low-dimensional vector space [[15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)].
    The main distinction between network embedding and our paper is that we focus
    on how different deep learning models are applied to graphs, and network embedding
    can be recognized as a concrete application example that uses some of these models
    (and it uses non-deep-learning methods as well).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个密切相关的主题是网络嵌入，旨在将节点嵌入到低维向量空间中 [[15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)]。网络嵌入与我们论文的主要区别在于，我们关注的是不同深度学习模型如何应用于图，而网络嵌入可以被看作是一个具体的应用示例，它使用了这些模型中的一些（同时也使用了非深度学习方法）。
- en: The rest of this paper is organized as follows. In Section 2, we introduce the
    notations used in this paper and provide preliminaries. Then, we review Graph
    RNNs, GCNs, GAEs, Graph RL, and graph adversarial methods in Section 3 to 7, respectively.
    We conclude with a discussion in Section 8.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：在第2节中，我们介绍了本文中使用的符号并提供了初步知识。然后，在第3至第7节中，我们分别回顾了图RNN、GCN、GAE、图RL和图对抗方法。最后，在第8节中，我们进行总结讨论。
- en: 2 Notations and Preliminaries
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 符号和初步知识
- en: Notations. In this paper, a graph²²2We consider only graphs without self-loops
    or multiple edges. is represented as $G=\left(V,E\right)$ where $V=\left\{v_{1},...,v_{N}\right\}$
    is a set of $N=\left|V\right|$ nodes and $E\subseteq V\times V$ is a set of $M=\left|E\right|$
    edges between nodes. We use $\mathbf{A}\in\mathbb{R}^{N\times N}$ to denote the
    adjacency matrix, whose $i^{th}$ row, $j^{th}$ column, and an element are denoted
    as $\mathbf{A}(i,:),\mathbf{A}(:,j),\mathbf{A}(i,j)$, respectively. The graph
    can be either directed or undirected and weighted or unweighted. In this paper,
    we mainly consider unsigned graphs; therefore, $\mathbf{A}(i,j)\geq 0$. Signed
    graphs will be discussed in future research directions. We use $\mathbf{F}^{V}$
    and $\mathbf{F}^{E}$ to denote features of nodes and edges, respectively. For
    other variables, we use bold uppercase characters to denote matrices and bold
    lowercase characters to denote vectors, e.g., a matrix $\mathbf{X}$ and a vector
    $\mathbf{x}$. The transpose of a matrix is denoted as $\mathbf{X}^{T}$ and the
    element-wise multiplication is denoted as $\mathbf{X}_{1}\odot\mathbf{X}_{2}$.
    Functions are marked with curlicues, e.g., $\mathcal{F}(\cdot)$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 符号说明。本文中，图²²2我们只考虑没有自环或多重边的图。表示为 $G=\left(V,E\right)$，其中 $V=\left\{v_{1},...,v_{N}\right\}$
    是 $N=\left|V\right|$ 个节点的集合，$E\subseteq V\times V$ 是 $M=\left|E\right|$ 个节点之间的边的集合。我们使用
    $\mathbf{A}\in\mathbb{R}^{N\times N}$ 来表示邻接矩阵，其第 $i$ 行、第 $j$ 列和一个元素分别表示为 $\mathbf{A}(i,:),\mathbf{A}(:,j),\mathbf{A}(i,j)$。图可以是有向的或无向的，可以是加权的或无权的。本文中，我们主要考虑无符号图；因此，$\mathbf{A}(i,j)\geq
    0$。带符号的图将在未来的研究方向中讨论。我们使用 $\mathbf{F}^{V}$ 和 $\mathbf{F}^{E}$ 来分别表示节点和边的特征。对于其他变量，我们使用粗体大写字母表示矩阵，使用粗体小写字母表示向量，例如，矩阵
    $\mathbf{X}$ 和向量 $\mathbf{x}$。矩阵的转置表示为 $\mathbf{X}^{T}$，逐元素乘法表示为 $\mathbf{X}_{1}\odot\mathbf{X}_{2}$。函数用花体字母表示，例如，$\mathcal{F}(\cdot)$。
- en: To better illustrate the notations, we take social networks as an example. Each
    node $v_{i}\in V$ corresponds to a user, and the edges $E$ correspond to relations
    between users. The profiles of users (e.g., age, gender, and location) can be
    represented as node features $\mathbf{F}^{V}$ and interaction data (e.g., sending
    messages and comments) can be represented as edge features $\mathbf{F}^{E}$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地说明这些符号，我们以社交网络为例。每个节点 $v_{i}\in V$ 对应于一个用户，而边 $E$ 对应于用户之间的关系。用户的个人资料（例如，年龄、性别和位置）可以表示为节点特征
    $\mathbf{F}^{V}$，而交互数据（例如，发送消息和评论）可以表示为边特征 $\mathbf{F}^{E}$。
- en: 'TABLE II: A Table for Commonly Used Notations'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 常用符号表'
- en: '| $G=(V,E)$ | A graph |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| $G=(V,E)$ | 图 |'
- en: '| $N,M$ | The number of nodes and edges |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| $N,M$ | 节点和边的数量 |'
- en: '| $V=\left\{v_{1},...,v_{N}\right\}$ | The set of nodes |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| $V=\left\{v_{1},...,v_{N}\right\}$ | 节点集合 |'
- en: '| $\mathbf{F}^{V},\mathbf{F}^{E}$ | The attributes/features of nodes and edges
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{F}^{V},\mathbf{F}^{E}$ | 节点和边的属性/特征 |'
- en: '| $\mathbf{A}$ | The adjacency matrix |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{A}$ | 邻接矩阵 |'
- en: '| $\mathbf{D}(i,i)=\sum_{j}\mathbf{A}(i,j)$ | The diagonal degree matrix |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{D}(i,i)=\sum_{j}\mathbf{A}(i,j)$ | 对角度矩阵 |'
- en: '| $\mathbf{L}=\mathbf{D}-\mathbf{A}$ | The Laplacian matrix |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{L}=\mathbf{D}-\mathbf{A}$ | 拉普拉斯矩阵 |'
- en: '| $\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{T}=\mathbf{L}$ | The eigendecomposition
    of $\mathbf{L}$ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{T}=\mathbf{L}$ | $\mathbf{L}$ 的特征分解
    |'
- en: '| $\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$ | The transition matrix |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$ | 转移矩阵 |'
- en: '| $\mathcal{N}_{k}(i),\mathcal{N}(i)$ | The k-step and 1-step neighbors of
    $v_{i}$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{N}_{k}(i),\mathcal{N}(i)$ | $v_{i}$ 的 k 步邻居和 1 步邻居 |'
- en: '| $\mathbf{H}^{l}$ | The hidden representation in the $l^{th}$ layer |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{H}^{l}$ | 第 $l$ 层的隐藏表示 |'
- en: '| $f_{l}$ | The dimensionality of $\mathbf{H}^{l}$ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| $f_{l}$ | $\mathbf{H}^{l}$ 的维度 |'
- en: '| $\rho(\cdot)$ | Some non-linear activation function |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| $\rho(\cdot)$ | 一些非线性激活函数 |'
- en: '| $\mathbf{X}_{1}\odot\mathbf{X}_{2}$ | The element-wise multiplication |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{X}_{1}\odot\mathbf{X}_{2}$ | 逐元素乘法 |'
- en: '| $\mathbf{\Theta}$ | Learnable parameters |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{\Theta}$ | 可学习参数 |'
- en: '| $s$ | The sample size |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| $s$ | 样本大小 |'
- en: Preliminaries. The Laplacian matrix of an undirected graph is defined as $\mathbf{L}=\mathbf{D}-\mathbf{A}$,
    where $\mathbf{D}\in\mathbb{R}^{N\times N}$ is a diagonal degree matrix with $\mathbf{D}(i,i)=\sum_{j}\mathbf{A}(i,j)$.
    Its eigendecomposition is denoted as $\mathbf{L}=\mathbf{Q\Lambda Q^{T}}$, where
    $\mathbf{\Lambda}\in\mathbb{R}^{N\times N}$ is a diagonal matrix of eigenvalues
    sorted in ascending order and $\mathbf{Q}\in\mathbb{R}^{N\times N}$ are the corresponding
    eigenvectors. The transition matrix is defined as $\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$,
    where $\mathbf{P}(i,j)$ represents the probability of a random walk starting from
    node $v_{i}$ landing at node $v_{j}$. The $k$-step neighbors of node $v_{i}$ are
    defined as $\mathcal{N}_{k}(i)=\left\{j|\mathcal{D}(i,j)\leq k\right\}$, where
    $\mathcal{D}(i,j)$ is the shortest distance from node $v_{i}$ to $v_{j}$, i.e.
    $\mathcal{N}_{k}(i)$ is a set of nodes reachable from node $v_{i}$ within $k$-steps.
    To simplify the notation, we omit the subscript for the immediate neighborhood,
    i.e., $\mathcal{N}(i)=\mathcal{N}_{1}(i)$.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基础知识。无向图的拉普拉斯矩阵定义为 $\mathbf{L}=\mathbf{D}-\mathbf{A}$，其中 $\mathbf{D}\in\mathbb{R}^{N\times
    N}$ 是一个对角度矩阵，满足 $\mathbf{D}(i,i)=\sum_{j}\mathbf{A}(i,j)$。它的特征分解表示为 $\mathbf{L}=\mathbf{Q\Lambda
    Q^{T}}$，其中 $\mathbf{\Lambda}\in\mathbb{R}^{N\times N}$ 是一个按升序排列的特征值对角矩阵，$\mathbf{Q}\in\mathbb{R}^{N\times
    N}$ 是相应的特征向量。转移矩阵定义为 $\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$，其中 $\mathbf{P}(i,j)$
    表示从节点 $v_{i}$ 开始的随机游走到达节点 $v_{j}$ 的概率。节点 $v_{i}$ 的 $k$ 步邻居定义为 $\mathcal{N}_{k}(i)=\left\{j|\mathcal{D}(i,j)\leq
    k\right\}$，其中 $\mathcal{D}(i,j)$ 是从节点 $v_{i}$ 到 $v_{j}$ 的最短距离，即 $\mathcal{N}_{k}(i)$
    是从节点 $v_{i}$ 在 $k$ 步内可以到达的节点集合。为了简化符号，我们省略了立即邻域的下标，即 $\mathcal{N}(i)=\mathcal{N}_{1}(i)$。
- en: 'For a deep learning model, we use superscripts to denote layers, e.g., $\mathbf{H}^{l}$.
    We use $f_{l}$ to denote the dimensionality of the layer $l$ (i.e., $\mathbf{H}^{l}\in\mathbb{R}^{N\times
    f_{l}}$). The sigmoid activation function is defined as $\sigma(x)=1/\left(1+e^{-x}\right)$
    and the rectified linear unit (ReLU) is defined as $\text{ReLU}(x)=max(0,x)$.
    A general element-wise nonlinear activation function is denoted as $\rho(\cdot)$.
    In this paper, unless stated otherwise, we assume all functions are differentiable,
    allowing the model parameters $\mathbf{\Theta}$ to be learned through back-propagation [[18](#bib.bib18)]
    using commonly adopted optimizers such as Adam [[19](#bib.bib19)] and training
    techniques such as dropout [[20](#bib.bib20)]. We denote the sample size as $s$
    if a sampling technique is adopted. We summarize the notations in Table [II](#S2.T2
    "TABLE II ‣ 2 Notations and Preliminaries ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '对于深度学习模型，我们使用上标来表示层次，例如，$\mathbf{H}^{l}$。我们用 $f_{l}$ 来表示层 $l$ 的维度（即，$\mathbf{H}^{l}\in\mathbb{R}^{N\times
    f_{l}}$）。Sigmoid 激活函数定义为 $\sigma(x)=1/\left(1+e^{-x}\right)$，而修正线性单元（ReLU）定义为
    $\text{ReLU}(x)=max(0,x)$。一般的元素级非线性激活函数表示为 $\rho(\cdot)$。在本文中，除非另有说明，我们假设所有函数都是可微的，从而允许通过反向传播[[18](#bib.bib18)]
    使用常见的优化器，如 Adam [[19](#bib.bib19)]，以及训练技术，如 dropout [[20](#bib.bib20)]，来学习模型参数
    $\mathbf{\Theta}$。如果采用了采样技术，我们将样本大小表示为 $s$。我们在表 [II](#S2.T2 "TABLE II ‣ 2 Notations
    and Preliminaries ‣ Deep Learning on Graphs: A Survey") 中总结了符号说明。'
- en: 'The tasks for learning a deep model on graphs can be broadly divided into two
    categories:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在图上学习深度模型的任务大致可以分为两类：
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Node-focused tasks: These tasks are associated with individual nodes in the
    graph. Examples include node classification, link prediction, and node recommendation.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 节点聚焦任务：这些任务与图中的个别节点相关。示例包括节点分类、链接预测和节点推荐。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph-focused tasks: These tasks are associated with the entire graph. Examples
    include graph classification, estimating various graph properties, and generating
    graphs.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图聚焦任务：这些任务与整个图相关。示例包括图分类、估计各种图属性和生成图。
- en: Note that such distinctions are more conceptually than mathematically rigorous.
    Some existing tasks are associated with mesoscopic structures such as community
    detection [[21](#bib.bib21)]. In addition, node-focused problems can sometimes
    be studied as graph-focused problems by transforming the former into egocentric
    networks [[22](#bib.bib22)]. Nevertheless, we will explain the differences in
    algorithm designs for these two categories when necessary.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些区分更多的是概念上的，而非数学上的严格。一些现有的任务与中观结构相关，例如社区检测 [[21](#bib.bib21)]。此外，节点聚焦问题有时可以通过将前者转化为自我中心网络
    [[22](#bib.bib22)] 来作为图聚焦问题进行研究。不过，我们将在必要时解释这两类任务在算法设计上的差异。
- en: 'TABLE III: The Main Characteristics of Graph Recurrent Neural Network (Graph
    RNNs)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：图形递归神经网络（Graph RNNs）的主要特征
- en: '| Category | Method | Recursive/sequential patterns of graphs | Time Complexity
    | Other Improvements |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 图的递归/顺序模式 | 时间复杂度 | 其他改进 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Node-level | GNN [[23](#bib.bib23)] | A recursive definition of node states
    | $O(MI_{f})$ | - |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 节点级别 | GNN [[23](#bib.bib23)] | 节点状态的递归定义 | $O(MI_{f})$ | - |'
- en: '| GGS-NNs [[24](#bib.bib24)] | $O(MT)$ | Sequence outputs |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| GGS-NNs [[24](#bib.bib24)] | $O(MT)$ | 序列输出 |'
- en: '| SSE [[25](#bib.bib25)] | $O(d_{\text{avg}}S)$ | - |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| SSE [[25](#bib.bib25)] | $O(d_{\text{avg}}S)$ | - |'
- en: '| Graph-level | You et al. [[26](#bib.bib26)] | Generate nodes and edges in
    an autoregressive manner | $O(N^{2})$ | - |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 图级别 | You 等人 [[26](#bib.bib26)] | 以自回归方式生成节点和边 | $O(N^{2})$ | - |'
- en: '| DGNN [[27](#bib.bib27)] | Capture the time dynamics of the formation of nodes
    and edges | $O(Md_{\text{avg}})$ | - |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| DGNN [[27](#bib.bib27)] | 捕捉节点和边形成的时间动态 | $O(Md_{\text{avg}})$ | - |'
- en: '| RMGCNN [[28](#bib.bib28)] | Recursively reconstruct the graph | $O(M)$ or
    $O(MN)$ | GCN layers |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| RMGCNN [[28](#bib.bib28)] | 递归重建图 | $O(M)$ 或 $O(MN)$ | GCN 层 |'
- en: '| Dynamic GCN [[29](#bib.bib29)] | Gather node representations in different
    time slices | $O(Mt)$ | GCN layers |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 动态 GCN [[29](#bib.bib29)] | 收集不同时间片的节点表示 | $O(Mt)$ | GCN 层 |'
- en: 3 Graph Recurrent Neural Networks
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 图形递归神经网络
- en: 'Recurrent neural networks (RNNs) such as gated recurrent units (GRU) [[30](#bib.bib30)]
    or long short-term memory (LSTM) [[31](#bib.bib31)] are de facto standards in
    modeling sequential data. In this section, we review Graph RNNs which can capture
    recursive and sequential patterns of graphs. Graph RNNs can be broadly divided
    into two categories: node-level RNNs and graph-level RNNs. The main distinction
    lies in whether the patterns lie at the node-level and are modeled by node states,
    or at the graph-level and are modeled by a common graph state. The main characteristics
    of the methods surveyed are summarized in Table [III](#S2.T3 "TABLE III ‣ 2 Notations
    and Preliminaries ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '循环神经网络（RNNs），如门控递归单元（GRU） [[30](#bib.bib30)] 或长短期记忆（LSTM） [[31](#bib.bib31)]，在建模顺序数据中是事实上的标准。在本节中，我们回顾了能够捕捉图的递归和顺序模式的图形
    RNNs。图形 RNNs 可以大致分为两类：节点级 RNNs 和图级 RNNs。主要区别在于模式是否位于节点级别并由节点状态建模，还是位于图级别并由公共图状态建模。所调查方法的主要特征总结在表 [III](#S2.T3
    "TABLE III ‣ 2 Notations and Preliminaries ‣ Deep Learning on Graphs: A Survey")。'
- en: 3.1 Node-level RNNs
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 节点级 RNNs
- en: 'Node-level RNNs for graphs, which are also referred to as graph neural networks
    (GNNs)³³3Recently, GNNs have also been used to refer to general neural networks
    for graph data. We follow the traditional naming convention and use GNNs to refer
    to this specific type of Graph RNNs., can be dated back to the ”pre-deep-learning”
    era [[32](#bib.bib32), [23](#bib.bib23)]. The idea behind a GNN is simple: to
    encode graph structural information, each node $v_{i}$ is represented by a low-dimensional
    state vector $\mathbf{s}_{i}$. Motivated by recursive neural networks [[33](#bib.bib33)],
    a recursive definition of states is adopted [[23](#bib.bib23)]:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图的节点级 RNNs，也被称为图神经网络（GNNs）³³3 最近，GNNs 也被用来指代图数据的通用神经网络。我们遵循传统命名惯例，将 GNNs 用于指代这种特定类型的图形
    RNNs。可以追溯到“深度学习之前”时代 [[32](#bib.bib32), [23](#bib.bib23)]。GNN 的思想很简单：为了编码图的结构信息，每个节点
    $v_{i}$ 由一个低维状态向量 $\mathbf{s}_{i}$ 表示。受到递归神经网络 [[33](#bib.bib33)] 的启发，采用递归状态定义 [[23](#bib.bib23)]：
- en: '|  | $\mathbf{s}_{i}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}(\mathbf{s}_{i},\mathbf{s}_{j},\mathbf{F}^{V}_{i},\mathbf{F}^{V}_{j},\mathbf{F}^{E}_{i,j}),$
    |  | (1) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{s}_{i}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}(\mathbf{s}_{i},\mathbf{s}_{j},\mathbf{F}^{V}_{i},\mathbf{F}^{V}_{j},\mathbf{F}^{E}_{i,j}),$
    |  | (1) |'
- en: 'where $\mathcal{F}(\cdot)$ is a parametric function to be learned. After obtaining
    $\mathbf{s}_{i}$, another function $\mathcal{O}(\cdot)$ is applied to get the
    final outputs:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}(\cdot)$ 是一个待学习的参数函数。在获得 $\mathbf{s}_{i}$ 后，应用另一个函数 $\mathcal{O}(\cdot)$
    来获取最终输出：
- en: '|  | $\hat{y}_{i}=\mathcal{O}(\mathbf{s}_{i},\mathbf{F}^{V}_{i}).$ |  | (2)
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}_{i}=\mathcal{O}(\mathbf{s}_{i},\mathbf{F}^{V}_{i}).$ |  | (2)
    |'
- en: 'For graph-focused tasks, the authors of [[23](#bib.bib23)] suggested adding
    a special node with unique attributes to represent the entire graph. To learn
    the model parameters, the following semi-supervised⁴⁴4It is called semi-supervised
    because all the graph structures and some subset of the node or graph labels is
    used during training. method is adopted: after iteratively solving Eq. ([1](#S3.E1
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")) to a stable point using the Jacobi method [[34](#bib.bib34)],
    one gradient descent step is performed using the Almeida-Pineda algorithm [[35](#bib.bib35),
    [36](#bib.bib36)] to minimize a task-specific objective function, for example,
    the squared loss between the predicted values and the ground-truth for regression
    tasks; then, this process is repeated until convergence.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图相关任务，[[23](#bib.bib23)]的作者建议添加一个具有独特属性的特殊节点来表示整个图。为了学习模型参数，采用以下半监督⁴⁴4这是所谓的半监督，因为在训练过程中使用了所有的图结构和一部分节点或图标签的方法：在使用雅可比方法[[34](#bib.bib34)]迭代求解方程式([1](#S3.E1
    "在3.1节点级RNNs ‣ 3图形递归神经网络 ‣ 深度学习在图形上的调查"))直到稳定点后，使用 Almeida-Pineda 算法[[35](#bib.bib35),
    [36](#bib.bib36)] 执行一步梯度下降，以最小化任务特定的目标函数，例如回归任务中预测值与真实值之间的平方损失；然后，重复这个过程直到收敛。
- en: 'Using the two simple equations in Eqs. ([1](#S3.E1 "In 3.1 Node-level RNNs
    ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on Graphs: A Survey"))([2](#S3.E2
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")), GNN plays two important roles. In retrospect, a GNN unifies
    some of the early methods used for processing graph data, such as recursive neural
    networks and Markov chains [[23](#bib.bib23)]. Looking toward the future, the
    general idea underlying GNNs has profound inspirations: as will be shown later,
    many state-of-the-art GCNs actually have a formulation similar to Eq. ([1](#S3.E1
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")) and follow the same framework of exchanging information within
    the immediate node neighborhoods. In fact, GNNs and GCNs can be unified into some
    common frameworks, and a GNN is equivalent to a GCN that uses identical layers
    to reach stable states. More discussion will be provided in Section 4.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方程式([1](#S3.E1 "在3.1节点级RNNs ‣ 3图形递归神经网络 ‣ 深度学习在图形上的调查"))([2](#S3.E2 "在3.1节点级RNNs
    ‣ 3图形递归神经网络 ‣ 深度学习在图形上的调查"))中的两个简单方程，GNN 执行两个重要的角色。回顾过去，GNN 统一了用于处理图数据的一些早期方法，例如递归神经网络和马尔可夫链[[23](#bib.bib23)]。展望未来，GNN
    的基本理念具有深远的启示：如后文所示，许多最先进的 GCN 实际上具有与方程式([1](#S3.E1 "在3.1节点级RNNs ‣ 3图形递归神经网络 ‣
    深度学习在图形上的调查"))类似的公式，并遵循在直接节点邻域内交换信息的相同框架。实际上，GNN 和 GCN 可以统一为一些通用框架，而 GNN 相当于一个使用相同层到达稳定状态的
    GCN。更多讨论将在第4节提供。
- en: 'Although they are conceptually important, GNNs have several drawbacks. First,
    to ensure that Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural
    Networks ‣ Deep Learning on Graphs: A Survey")) has a unique solution, $\mathcal{F}(\cdot)$
    must be a “contraction map” [[37](#bib.bib37)], i.e., $\exists\mu,0<\mu<1$ so
    that'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们在概念上很重要，但 GNN 存在几个缺点。首先，为了确保方程式([1](#S3.E1 "在3.1节点级RNNs ‣ 3图形递归神经网络 ‣ 深度学习在图形上的调查"))有唯一解，$\mathcal{F}(\cdot)$
    必须是一个“收缩映射”[[37](#bib.bib37)]，即，$\exists\mu,0<\mu<1$ 使得
- en: '|  | $\left\&#124;\mathcal{F}(x)-\mathcal{F}(y)\right\&#124;\leq\mu\left\&#124;x-y\right\&#124;,\forall
    x,y.$ |  | (3) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\&#124;\mathcal{F}(x)-\mathcal{F}(y)\right\&#124;\leq\mu\left\&#124;x-y\right\&#124;,\forall
    x,y.$ |  | (3) |'
- en: Intuitively, a “contraction map” requires that the distance between any two
    points can only “contract” after the $\mathcal{F}(\cdot)$ operation, which severely
    limits the modeling ability. Second, because many iterations are needed to reach
    a stable state between gradient descend steps, GNNs are computationally expensive.
    Because of these drawbacks and perhaps a lack of computational power (e.g., the
    graphics processing unit, GPU, was not widely used for deep learning in those
    days) and lack of research interests, GNNs did not become a focus of general research.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上讲，“收缩映射”要求任何两点之间的距离在$\mathcal{F}(\cdot)$操作后只能“收缩”，这严重限制了建模能力。其次，由于需要许多迭代步骤才能在梯度下降步骤之间达到稳定状态，GNN
    的计算开销很大。由于这些缺点以及可能的计算能力不足（例如，在那些日子里图形处理单元GPU尚未广泛用于深度学习）和缺乏研究兴趣，GNN 并没有成为一般研究的重点。
- en: 'A notable improvement to GNNs is gated graph sequence neural networks (GGS-NNs) [[24](#bib.bib24)]
    with the following modifications. Most importantly, the authors replaced the recursive
    definition in Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural
    Networks ‣ Deep Learning on Graphs: A Survey")) with a GRU, thus removing the
    “contraction map” requirement and supporting modern optimization techniques. Specifically,
    Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣
    Deep Learning on Graphs: A Survey")) is adapted as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '对于图神经网络（GNNs）的一个显著改进是门控图序列神经网络（GGS-NNs）[[24](#bib.bib24)]，其进行了以下修改。最重要的是，作者将方程式 ([1](#S3.E1
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")) 中的递归定义替换为 GRU，从而去除了“收缩映射”要求并支持现代优化技术。具体而言，方程式 ([1](#S3.E1
    "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on
    Graphs: A Survey")) 被调整为如下形式：'
- en: '|  | $\mathbf{s}_{i}^{(t)}=(1-\mathbf{z}_{i}^{(t)})\odot\mathbf{s}_{i}^{(t-1)}+\mathbf{z}_{i}^{(t)}\odot\widetilde{\mathbf{s}}_{i}^{(t)},$
    |  | (4) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{s}_{i}^{(t)}=(1-\mathbf{z}_{i}^{(t)})\odot\mathbf{s}_{i}^{(t-1)}+\mathbf{z}_{i}^{(t)}\odot\widetilde{\mathbf{s}}_{i}^{(t)},$
    |  | (4) |'
- en: where $\mathbf{z}$ is calculated by the update gate, $\widetilde{\mathbf{s}}$
    is the candidate for updating, and $t$ is the pseudo time. Second, the authors
    proposed using several such networks operating in sequence to produce sequence
    outputs and showed that their method could be applied to sequence-based tasks
    such as program verification [[38](#bib.bib38)].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{z}$ 是由更新门计算的，$\widetilde{\mathbf{s}}$ 是更新的候选项，而 $t$ 是伪时间。其次，作者提出使用几个这样的网络按序列操作以产生序列输出，并表明他们的方法可以应用于基于序列的任务，如程序验证
    [[38](#bib.bib38)]。
- en: 'SSE [[25](#bib.bib25)] took a similar approach as Eq. ([4](#S3.E4 "In 3.1 Node-level
    RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on Graphs: A Survey")).
    However, instead of using a GRU in the calculation, SSE adopted stochastic fixed-point
    gradient descent to accelerate the training process. This scheme basically alternates
    between calculating steady node states using local neighborhoods and optimizing
    the model parameters, with both calculations in stochastic mini-batches.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'SSE [[25](#bib.bib25)] 采用了与方程式 ([4](#S3.E4 "In 3.1 Node-level RNNs ‣ 3 Graph
    Recurrent Neural Networks ‣ Deep Learning on Graphs: A Survey")) 相似的方法。然而，SSE
    在计算中并没有使用 GRU，而是采用了随机固定点梯度下降以加速训练过程。该方案基本上在计算稳态节点状态和优化模型参数之间交替进行，两者都在随机小批量中计算。'
- en: 3.2 Graph-level RNNs
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 图级 RNNs
- en: In this subsection, we review how to apply RNNs to capture graph-level patterns,
    e.g., temporal patterns of dynamic graphs or sequential patterns at different
    levels of graph granularities. In graph-level RNNs, instead of applying one RNN
    to each node to learn the node states, a single RNN is applied to the entire graph
    to encode the graph states.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们回顾了如何应用 RNN 来捕捉图级模式，例如动态图的时间模式或不同图粒度层次的序列模式。在图级 RNN 中，代替对每个节点应用一个 RNN
    来学习节点状态，应用一个单一的 RNN 对整个图进行编码图状态。
- en: 'You et al. [[26](#bib.bib26)] applied Graph RNNs to the graph generation problem.
    Specifically, they adopted two RNNs: one to generate new nodes and the other to
    generate edges for the newly added node in an autoregressive manner. They showed
    that such hierarchical RNN architectures learn more effectively from input graphs
    than do the traditional rule-based graph generative models while having a reasonable
    time complexity.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你等人 [[26](#bib.bib26)] 将图 RNN 应用于图生成问题。具体而言，他们采用了两个 RNN：一个用于生成新节点，另一个用于以自回归方式生成新添加节点的边。他们表明，这种层次
    RNN 架构比传统的基于规则的图生成模型更有效地从输入图中学习，同时具有合理的时间复杂度。
- en: To capture the temporal information of dynamic graphs, dynamic graph neural
    network (DGNN) [[27](#bib.bib27)] was proposed that used a time-aware LSTM [[39](#bib.bib39)]
    to learn node representations. When a new edge is established, DGNN used the LSTM
    to update the representation of the two interacting nodes as well as their immediate
    neighbors, i.e., considering the one-step propagation effect. The authors showed
    that the time-aware LSTM could model the establishing orders and time intervals
    of edge formations well, which in turn benefited a range of graph applications.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉动态图的时间信息，提出了动态图神经网络（DGNN）[[27](#bib.bib27)]，该网络使用了时间感知 LSTM [[39](#bib.bib39)]
    来学习节点表示。当建立新的边时，DGNN 使用 LSTM 更新两个交互节点及其直接邻居的表示，即考虑了一步传播效应。作者表明，时间感知 LSTM 能够很好地建模边形成的建立顺序和时间间隔，从而有利于一系列图应用。
- en: 'Graph RNN can also be combined with other architectures, such as GCNs or GAEs.
    For example, aiming to tackle the graph sparsity problem, RMGCNN [[28](#bib.bib28)]
    applied an LSTM to the results of GCNs to progressively reconstruct a graph as
    illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Graph-level RNNs ‣ 3 Graph Recurrent
    Neural Networks ‣ Deep Learning on Graphs: A Survey"). By using an LSTM, the information
    from different parts of the graph can diffuse across long ranges without requiring
    as many GCN layers. Dynamic GCN [[29](#bib.bib29)] applied an LSTM to gather the
    results of GCNs from different time slices in dynamic networks to capture both
    the spatial and temporal graph information.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 'Graph RNN 还可以与其他架构结合，如 GCNs 或 GAEs。例如，针对图稀疏问题，RMGCNN [[28](#bib.bib28)] 将 LSTM
    应用于 GCNs 的结果，以逐步重建图，如图 [2](#S3.F2 "Figure 2 ‣ 3.2 Graph-level RNNs ‣ 3 Graph Recurrent
    Neural Networks ‣ Deep Learning on Graphs: A Survey")所示。通过使用 LSTM，图的不同部分的信息可以在长距离上传播，而不需要那么多
    GCN 层。动态 GCN [[29](#bib.bib29)] 应用了 LSTM 来收集动态网络中不同时间片段的 GCN 结果，以捕捉空间和时间图信息。'
- en: '![Refer to caption](img/e9b530023d6cbdcfb3e9a58ae41909dc.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9b530023d6cbdcfb3e9a58ae41909dc.png)'
- en: 'Figure 2: The framework of RMGCNN (reprinted from [[28](#bib.bib28)] with permission).
    RMGCNN includes an LSTM in the GCN to progressively reconstruct the graph. $\mathbf{X}^{t}$,
    $\mathbf{\tilde{X}}^{t}$, and $d\mathbf{X}^{t}$ represent the estimated matrix,
    the outputs of GCNs, and the incremental updates produced by the RNN at iteration
    $t$, respectively. MGCNN refers to a multigraph CNN.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：RMGCNN 的框架（经 [[28](#bib.bib28)]授权转载）。RMGCNN 在 GCN 中包括一个 LSTM 以逐步重建图。$\mathbf{X}^{t}$、$\mathbf{\tilde{X}}^{t}$
    和 $d\mathbf{X}^{t}$ 分别表示估计矩阵、GCNs 的输出和迭代 $t$ 中 RNN 产生的增量更新。MGCNN 指的是多图 CNN。
- en: 'TABLE IV: A Comparison among Different Graph Convolutional Networks (GCNs).
    T.C. = Time Complexity, M.G. = Multiple Graphs'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：不同图卷积网络（GCNs）之间的比较。T.C. = 时间复杂度，M.G. = 多图
- en: '| Method | Type | Convolution | Readout | T.C. | M.G. | Other Characteristics
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类型 | 卷积 | 读出 | T.C. | M.G. | 其他特征 |'
- en: '| Bruna et al. [[40](#bib.bib40)] | Spectral | Interpolation kernel | Hierarchical
    clustering + FC | $O(N^{3})$ | No | - |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Bruna et al. [[40](#bib.bib40)] | 频谱 | 插值核 | 层次聚类 + FC | $O(N^{3})$ | 否 |
    - |'
- en: '| Henaff et al. [[41](#bib.bib41)] | Spectral | Interpolation kernel | Hierarchical
    clustering + FC | $O(N^{3})$ | No | Constructing the graph |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Henaff et al. [[41](#bib.bib41)] | 频谱 | 插值核 | 层次聚类 + FC | $O(N^{3})$ | 否
    | 构建图 |'
- en: '| ChebNet [[42](#bib.bib42)] | Spectral/Spatial | Polynomial | Hierarchical
    clustering | $O(M)$ | Yes | - |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ChebNet [[42](#bib.bib42)] | 频谱/空间 | 多项式 | 层次聚类 | $O(M)$ | 是 | - |'
- en: '| Kipf&Welling [[43](#bib.bib43)] | Spectral/Spatial | First-order | - | $O(M)$
    | - | - |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Kipf&Welling [[43](#bib.bib43)] | 频谱/空间 | 一阶 | - | $O(M)$ | - | - |'
- en: '| CayletNet [[44](#bib.bib44)] | Spectral | Polynomial | Hierarchical clustering
    + FC | $O(M)$ | No | - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CayletNet [[44](#bib.bib44)] | 频谱 | 多项式 | 层次聚类 + FC | $O(M)$ | 否 | - |'
- en: '| GWNN [[45](#bib.bib45)] | Spectral | Wavelet transform | - | $O(M)$ | No
    | - |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GWNN [[45](#bib.bib45)] | 频谱 | 小波变换 | - | $O(M)$ | 否 | - |'
- en: '| Neural FPs [[46](#bib.bib46)] | Spatial | First-order | Sum | $O(M)$ | Yes
    | - |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Neural FPs [[46](#bib.bib46)] | 空间 | 一阶 | 总和 | $O(M)$ | 是 | - |'
- en: '| PATCHY-SAN [[47](#bib.bib47)] | Spatial | Polynomial + an order | An order
    + pooling | $O(M\log N)$ | Yes | A neighbor order |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| PATCHY-SAN [[47](#bib.bib47)] | 空间 | 多项式 + 一阶 | 一阶 + 池化 | $O(M\log N)$ |
    是 | 一个邻居顺序 |'
- en: '| LGCN [[48](#bib.bib48)] | Spatial | First-order + an order | - | $O(M)$ |
    Yes | A neighbor order |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LGCN [[48](#bib.bib48)] | 空间 | 一阶 + 一阶 | - | $O(M)$ | 是 | 一个邻居顺序 |'
- en: '| SortPooling [[49](#bib.bib49)] | Spatial | First-order | An order + pooling
    | $O(M)$ | Yes | A node order |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| SortPooling [[49](#bib.bib49)] | 空间 | 一阶 | 一阶 + 池化 | $O(M)$ | 是 | 一个节点顺序
    |'
- en: '| DCNN [[50](#bib.bib50)] | Spatial | Polynomial diffusion | Mean | $O(N^{2})$
    | Yes | Edge features |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| DCNN [[50](#bib.bib50)] | 空间 | 多项式扩散 | 平均 | $O(N^{2})$ | 是 | 边特征 |'
- en: '| DGCN [[51](#bib.bib51)] | Spatial | First-order + diffusion | - | $O(N^{2})$
    | - | - |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| DGCN [[51](#bib.bib51)] | 空间 | 一阶 + 扩散 | - | $O(N^{2})$ | - | - |'
- en: '| MPNNs [[52](#bib.bib52)] | Spatial | First-order | Set2set | $O(M)$ | Yes
    | A general framework |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MPNNs [[52](#bib.bib52)] | 空间 | 一阶 | Set2set | $O(M)$ | 是 | 一个通用框架 |'
- en: '| GraphSAGE [[53](#bib.bib53)] | Spatial | First-order + sampling | - | $O(Ns^{L})$
    | Yes | A general framework |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GraphSAGE [[53](#bib.bib53)] | 空间 | 一阶 + 采样 | - | $O(Ns^{L})$ | 是 | 一个通用框架
    |'
- en: '| MoNet [[54](#bib.bib54)] | Spatial | First-order | Hierarchical clustering
    | $O(M)$ | Yes | A general framework |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MoNet [[54](#bib.bib54)] | 空间 | 一阶 | 层次聚类 | $O(M)$ | 是 | 一个通用框架 |'
- en: '| GNs [[9](#bib.bib9)] | Spatial | First-order | A graph representation | $O(M)$
    | Yes | A general framework |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| GNs [[9](#bib.bib9)] | 空间 | 一阶 | 图表示 | $O(M)$ | 是 | 通用框架 |'
- en: '| Kearnes et al. [[55](#bib.bib55)] | Spatial | Weave module | Fuzzy histogram
    | $O(M)$ | Yes | Edge features |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Kearnes et al. [[55](#bib.bib55)] | 空间 | 编织模块 | 模糊直方图 | $O(M)$ | 是 | 边缘特征
    |'
- en: '| DiffPool [[56](#bib.bib56)] | Spatial | Various | Hierarchical clustering
    | $O(N^{2})$ | Yes | Differentiable pooling |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| DiffPool [[56](#bib.bib56)] | 空间 | 各种 | 层次聚类 | $O(N^{2})$ | 是 | 可微池化 |'
- en: '| GAT [[57](#bib.bib57)] | Spatial | First-order | - | $O(M)$ | Yes | Attention
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GAT [[57](#bib.bib57)] | 空间 | 一阶 | - | $O(M)$ | 是 | 注意力 |'
- en: '| GaAN [[58](#bib.bib58)] | Spatial | First-order | - | $O(Ns^{L})$ | Yes |
    Attention |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GaAN [[58](#bib.bib58)] | 空间 | 一阶 | - | $O(Ns^{L})$ | 是 | 注意力 |'
- en: '| HAN [[59](#bib.bib59)] | Spatial | Meta-path neighbors | - | $O(M_{\phi})$
    | Yes | Attention |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| HAN [[59](#bib.bib59)] | 空间 | 元路径邻居 | - | $O(M_{\phi})$ | 是 | 注意力 |'
- en: '| CLN [[60](#bib.bib60)] | Spatial | First-order | - | $O(M)$ | - | - |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| CLN [[60](#bib.bib60)] | 空间 | 一阶 | - | $O(M)$ | - | - |'
- en: '| PPNP [[61](#bib.bib61)] | Spatial | First-order | - | $O(M)$ | - | Teleportation
    connections |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| PPNP [[61](#bib.bib61)] | 空间 | 一阶 | - | $O(M)$ | - | 传送连接 |'
- en: '| JK-Nets [[62](#bib.bib62)] | Spatial | Various | - | $O(M)$ | Yes | Jumping
    connections |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| JK-Nets [[62](#bib.bib62)] | 空间 | 各种 | - | $O(M)$ | 是 | 跳跃连接 |'
- en: '| ECC [[63](#bib.bib63)] | Spatial | First-order | Hierarchical clustering
    | $O(M)$ | Yes | Edge features |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ECC [[63](#bib.bib63)] | 空间 | 一阶 | 层次聚类 | $O(M)$ | 是 | 边缘特征 |'
- en: '| R-GCNs [[64](#bib.bib64)] | Spatial | First-order | - | $O(M)$ | - | Edge
    features |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| R-GCNs [[64](#bib.bib64)] | 空间 | 一阶 | - | $O(M)$ | - | 边缘特征 |'
- en: '| LGNN [[65](#bib.bib65)] | Spatial | First-order + LINE graph | - | $O(M)$
    | - | Edge features |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| LGNN [[65](#bib.bib65)] | 空间 | 一阶 + LINE 图 | - | $O(M)$ | - | 边缘特征 |'
- en: '| PinSage [[66](#bib.bib66)] | Spatial | Random walk | - | $O(Ns^{L})$ | -
    | Neighborhood sampling |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| PinSage [[66](#bib.bib66)] | 空间 | 随机游走 | - | $O(Ns^{L})$ | - | 邻域采样 |'
- en: '| StochasticGCN [[67](#bib.bib67)] | Spatial | First-order + sampling | - |
    $O(Ns^{L})$ | - | Neighborhood sampling |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| StochasticGCN [[67](#bib.bib67)] | 空间 | 一阶 + 采样 | - | $O(Ns^{L})$ | - | 邻域采样
    |'
- en: '| FastGCN [[68](#bib.bib68)] | Spatial | First-order + sampling | - | $O(NsL)$
    | Yes | Layer-wise sampling |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| FastGCN [[68](#bib.bib68)] | 空间 | 一阶 + 采样 | - | $O(NsL)$ | 是 | 层级采样 |'
- en: '| Adapt [[69](#bib.bib69)] | Spatial | First-order + sampling | - | $O(NsL)$
    | Yes | Layer-wise sampling |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Adapt [[69](#bib.bib69)] | 空间 | 一阶 + 采样 | - | $O(NsL)$ | 是 | 层级采样 |'
- en: '| Li et al. [[70](#bib.bib70)] | Spatial | First-order | - | $O(M)$ | - | Theoretical
    analysis |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. [[70](#bib.bib70)] | 空间 | 一阶 | - | $O(M)$ | - | 理论分析 |'
- en: '| SGC [[71](#bib.bib71)] | Spatial | Polynomial | - | $O(M)$ | Yes | Theoretical
    analysis |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| SGC [[71](#bib.bib71)] | 空间 | 多项式 | - | $O(M)$ | 是 | 理论分析 |'
- en: '| GFNN [[72](#bib.bib72)] | Spatial | Polynomial | - | $O(M)$ | Yes | Theoretical
    analysis |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| GFNN [[72](#bib.bib72)] | 空间 | 多项式 | - | $O(M)$ | 是 | 理论分析 |'
- en: '| GIN [[73](#bib.bib73)] | Spatial | First-order | Sum + MLP | $O(M)$ | Yes
    | Theoretical analysis |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| GIN [[73](#bib.bib73)] | 空间 | 一阶 | 求和 + MLP | $O(M)$ | 是 | 理论分析 |'
- en: '| DGI [[74](#bib.bib74)] | Spatial | First-order | - | $O(M)$ | Yes | Unsupervised
    training |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| DGI [[74](#bib.bib74)] | 空间 | 一阶 | - | $O(M)$ | 是 | 无监督训练 |'
- en: 4 Graph Convolutional Networks
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 图卷积网络
- en: 'Graph convolutional networks (GCNs) are inarguably the hottest topic in graph-based
    deep learning. Mimicking CNNs, modern GCNs learn the common local and global structural
    patterns of graphs through designed convolution and readout functions. Because
    most GCNs can be trained with task-specific loss via backpropagation (with a few
    exceptions such as the unsupervised training method in [[74](#bib.bib74)]), we
    focus on the adopted architectures. We first discuss the convolution operations,
    then move to the readout operations and some other improvements. We summarize
    the main characteristics of GCNs surveyed in this paper in Table [IV](#S3.T4 "TABLE
    IV ‣ 3.2 Graph-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning
    on Graphs: A Survey").'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '图卷积网络（GCNs）无疑是基于图的深度学习中最热门的主题。现代GCNs模仿CNN，通过设计的卷积和读取函数来学习图的常见局部和全局结构模式。由于大多数GCNs可以通过反向传播使用任务特定的损失进行训练（有少数例外，如[[74](#bib.bib74)]中的无监督训练方法），我们专注于采用的架构。我们首先讨论卷积操作，然后转到读取操作和一些其他改进。我们在表[IV](#S3.T4
    "TABLE IV ‣ 3.2 Graph-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning
    on Graphs: A Survey")中总结了本文调查的GCNs的主要特征。'
- en: 4.1 Convolution Operations
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 卷积操作
- en: 'Graph convolutions can be divided into two groups: *spectral convolutions*,
    which perform convolution by transforming node representations into the spectral
    domain using the graph Fourier transform or its extensions, and *spatial convolutions*,
    which perform convolution by considering node neighborhoods. Note that these two
    groups can overlap, for example, when using a polynomial spectral kernel (please
    refer to Section [4.1.2](#S4.SS1.SSS2 "4.1.2 The Efficiency Aspect ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")
    for details).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积可以分为两类：*谱卷积*，通过使用图傅里叶变换或其扩展将节点表示转换到谱域来执行卷积；以及*空间卷积*，通过考虑节点邻域来执行卷积。请注意，这两类可能会重叠，例如，当使用多项式谱核时（请参见第
    [4.1.2](#S4.SS1.SSS2 "4.1.2 效率方面 ‣ 4.1 卷积操作 ‣ 4 图卷积网络 ‣ 图上的深度学习：综述") 节的详细信息）。
- en: 4.1.1 Spectral Methods
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 谱方法
- en: 'Convolution is the most fundamental operation in CNNs. However, the standard
    convolution operation used for images or text cannot be directly applied to graphs
    because graphs lack a grid structure [[6](#bib.bib6)]. Bruna et al. [[40](#bib.bib40)]
    first introduced convolution for graph data from the spectral domain using the
    graph Laplacian matrix $\mathbf{L}$ [[75](#bib.bib75)], which plays a similar
    role as the Fourier basis in signal processing [[6](#bib.bib6)]. The graph convolution
    operation, $*_{G}$, is defined as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是卷积神经网络中的最基本操作。然而，标准的卷积操作用于图像或文本的操作无法直接应用于图，因为图缺乏网格结构 [[6](#bib.bib6)]。Bruna
    等人 [[40](#bib.bib40)] 首次从谱域引入了图数据的卷积，使用了图拉普拉斯矩阵 $\mathbf{L}$ [[75](#bib.bib75)]，它在信号处理中的作用类似于傅里叶基底
    [[6](#bib.bib6)]。图卷积操作 $*_{G}$ 定义如下：
- en: '|  | $\mathbf{u}_{1}*_{G}\mathbf{u}_{2}=\mathbf{Q}\left(\left(\mathbf{Q}^{T}\mathbf{u}_{1}\right)\odot\left(\mathbf{Q}^{T}\mathbf{u}_{2}\right)\right),$
    |  | (5) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{u}_{1}*_{G}\mathbf{u}_{2}=\mathbf{Q}\left(\left(\mathbf{Q}^{T}\mathbf{u}_{1}\right)\odot\left(\mathbf{Q}^{T}\mathbf{u}_{2}\right)\right),$
    |  | (5) |'
- en: 'where $\mathbf{u}_{1},\mathbf{u}_{2}\in\mathbb{R}^{N}$ are two signals⁵⁵5We
    give an example of graph signals in Appendix [D](#A4 "Appendix D An Example of
    Graph Signals ‣ Deep Learning on Graphs: A Survey"). defined on nodes and $\mathbf{Q}$
    are the eigenvectors of $\mathbf{L}$. Briefly, multiplying $\mathbf{Q}^{T}$ transforms
    the graph signals $\mathbf{u}_{1},\mathbf{u}_{2}$ into the spectral domain (i.e.,
    the graph Fourier transform), while multiplying $\mathbf{Q}$ performs the inverse
    transform. The validity of this definition is based on the convolution theorem,
    i.e., the Fourier transform of a convolution operation is the element-wise product
    of their Fourier transforms. Then, a signal $\mathbf{u}$ can be filtered by'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{u}_{1},\mathbf{u}_{2}\in\mathbb{R}^{N}$ 是定义在节点上的两个信号⁵⁵5我们在附录 [D](#A4
    "附录 D 图信号示例 ‣ 图上的深度学习：综述")中给出了图信号的示例。 $\mathbf{Q}$ 是 $\mathbf{L}$ 的特征向量。简而言之，乘以
    $\mathbf{Q}^{T}$ 将图信号 $\mathbf{u}_{1},\mathbf{u}_{2}$ 转换到谱域（即图傅里叶变换），而乘以 $\mathbf{Q}$
    执行逆变换。这个定义的有效性基于卷积定理，即卷积操作的傅里叶变换是它们傅里叶变换的逐元素乘积。然后，一个信号 $\mathbf{u}$ 可以通过以下方式进行滤波：
- en: '|  | $\mathbf{u}^{\prime}=\mathbf{Q}\mathbf{\Theta}\mathbf{Q}^{T}\mathbf{u},$
    |  | (6) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{u}^{\prime}=\mathbf{Q}\mathbf{\Theta}\mathbf{Q}^{T}\mathbf{u},$
    |  | (6) |'
- en: 'where $\mathbf{u}^{\prime}$ is the output signal, $\mathbf{\Theta}=\mathbf{\Theta}(\mathbf{\Lambda})\in\mathbb{R}^{N\times
    N}$ is a diagonal matrix of learnable filters and $\mathbf{\Lambda}$ are the eigenvalues
    of $\mathbf{L}$. A convolutional layer is defined by applying different filters
    to different input-output signal pairs as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{u}^{\prime}$ 是输出信号，$\mathbf{\Theta}=\mathbf{\Theta}(\mathbf{\Lambda})\in\mathbb{R}^{N\times
    N}$ 是一个可学习滤波器的对角矩阵，$\mathbf{\Lambda}$ 是 $\mathbf{L}$ 的特征值。一个卷积层通过对不同的输入-输出信号对应用不同的滤波器来定义，如下所示：
- en: '|  | $\mathbf{u}^{l+1}_{j}=\rho\left(\sum\nolimits_{i=1}^{f_{l}}\mathbf{Q}\mathbf{\Theta}^{l}_{i,j}\mathbf{Q}^{T}\mathbf{u}^{l}_{i}\right)\;j=1,...,f_{l+1},$
    |  | (7) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{u}^{l+1}_{j}=\rho\left(\sum\nolimits_{i=1}^{f_{l}}\mathbf{Q}\mathbf{\Theta}^{l}_{i,j}\mathbf{Q}^{T}\mathbf{u}^{l}_{i}\right)\;j=1,...,f_{l+1},$
    |  | (7) |'
- en: 'where $l$ is the layer, $\mathbf{u}^{l}_{j}\in\mathbb{R}^{N}$ is the $j^{th}$
    hidden representation (i.e., the signal) for the nodes in the $l^{th}$ layer,
    and $\mathbf{\Theta}^{l}_{i,j}$ are learnable filters. The idea behind Eq. ([7](#S4.E7
    "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) is similar to a conventional convolution:
    it passes the input signals through a set of learnable filters to aggregate the
    information, followed by some nonlinear transformation. By using the node features
    $\mathbf{F}^{V}$ as the input layer and stacking multiple convolutional layers,
    the overall architecture is similar to that of a CNN. Theoretical analysis has
    shown that such a definition of the graph convolution operation can mimic certain
    geometric properties of CNNs and we refer readers to [[7](#bib.bib7)] for a comprehensive
    survey.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$l$是层，$\mathbf{u}^{l}_{j}\in\mathbb{R}^{N}$是第$l$层节点的第$j^{th}$隐藏表示（即信号），$\mathbf{\Theta}^{l}_{i,j}$是可学习的滤波器。公式 ([7](#S4.E7
    "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey"))背后的思想类似于传统卷积：它将输入信号通过一组可学习的滤波器来聚合信息，然后进行一些非线性变换。通过使用节点特征$\mathbf{F}^{V}$作为输入层并堆叠多个卷积层，整体架构类似于CNN。理论分析表明，这种图卷积操作的定义可以模拟CNN的某些几何属性，我们建议读者参考[[7](#bib.bib7)]以获取全面的综述。'
- en: 'However, directly using Eq. ([7](#S4.E7 "In 4.1.1 Spectral Methods ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    requires learning $O(N)$ parameters, which may not be feasible in practice. Besides,
    the filters in the spectral domain may not be localized in the spatial domain,
    i.e., each node may be affected by all the other nodes rather than only the nodes
    in a small region. To alleviate these problems, Bruna et al. [[40](#bib.bib40)]
    suggested using the following smoothing filters:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，直接使用公式 ([7](#S4.E7 "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))需要学习$O(N)$个参数，这在实际中可能不可行。此外，谱域中的滤波器可能在空间域中并不局部化，即每个节点可能会受到所有其他节点的影响，而不仅仅是小区域内的节点。为了缓解这些问题，Bruna等人[[40](#bib.bib40)]建议使用以下平滑滤波器：'
- en: '|  | $diag\left(\mathbf{\Theta}^{l}_{i,j}\right)=\mathcal{K}\;\alpha_{l,i,j},$
    |  | (8) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $diag\left(\mathbf{\Theta}^{l}_{i,j}\right)=\mathcal{K}\;\alpha_{l,i,j},$
    |  | (8) |'
- en: where $\mathcal{K}$ is a fixed interpolation kernel and $\alpha_{l,i,j}$ are
    learnable interpolation coefficients. The authors also generalized this idea to
    the setting where the graph is not given but constructed from raw features using
    either a supervised or an unsupervised method [[41](#bib.bib41)].
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{K}$是固定的插值核，$\alpha_{l,i,j}$是可学习的插值系数。作者还将这一思想推广到了图未给出而是通过有监督或无监督方法从原始特征构建的情形[[41](#bib.bib41)]。
- en: However, two fundamental problems remain unsolved. First, because the full eigenvectors
    of the Laplacian matrix are needed during each calculation, the time complexity
    is at least $O(N^{2})$ for each forward and backward pass, not to mention the
    $O(N^{3})$ complexity required to calculate the eigendecomposition, meaning that
    this approach is not scalable to large-scale graphs. Second, because the filters
    depend on the eigenbasis $\mathbf{Q}$ of the graph, the parameters cannot be shared
    across multiple graphs with different sizes and structures.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，两个根本性问题仍未解决。首先，由于在每次计算中都需要拉普拉斯矩阵的完整特征向量，因此每次前向和后向传播的时间复杂度至少为$O(N^{2})$，更不用说计算特征分解所需的$O(N^{3})$复杂度，这意味着这种方法在大规模图上不可扩展。其次，由于滤波器依赖于图的特征基$\mathbf{Q}$，因此这些参数不能在具有不同规模和结构的多个图之间共享。
- en: Next, we review two lines of works trying to solve these limitations and then
    unify them using some common frameworks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们回顾两条试图解决这些限制的工作线，并利用一些通用框架将它们统一起来。
- en: 4.1.2 The Efficiency Aspect
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 效率方面
- en: 'To solve the efficiency problem, ChebNet [[42](#bib.bib42)] was proposed to
    use a polynomial filter as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决效率问题，提出了ChebNet[[42](#bib.bib42)]，其使用多项式滤波器，如下所示：
- en: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\sum\nolimits_{k=0}^{K}\theta_{k}\mathbf{\Lambda}^{k},$
    |  | (9) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\sum\nolimits_{k=0}^{K}\theta_{k}\mathbf{\Lambda}^{k},$
    |  | (9) |'
- en: 'where $\theta_{0},...,\theta_{K}$ are the learnable parameters and $K$ is the
    polynomial order. Then, instead of performing the eigendecomposition, the authors
    rewrote Eq. ([9](#S4.E9 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) using
    the Chebyshev expansion [[76](#bib.bib76)]:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\theta_{0},...,\theta_{K}$ 是可学习的参数，$K$ 是多项式的阶数。然后，作者们使用切比雪夫展开 [[76](#bib.bib76)]
    替代特征分解重写了公式 ([9](#S4.E9 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))：'
- en: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\sum\nolimits_{k=0}^{K}\theta_{k}\mathcal{T}_{k}(\tilde{\mathbf{\Lambda}}),$
    |  | (10) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\sum\nolimits_{k=0}^{K}\theta_{k}\mathcal{T}_{k}(\tilde{\mathbf{\Lambda}}),$
    |  | (10) |'
- en: 'where $\tilde{\mathbf{\Lambda}}=2\mathbf{\Lambda}/\lambda_{max}-\mathbf{I}$
    are the rescaled eigenvalues, $\lambda_{max}$ is the maximum eigenvalue, $\mathbf{I}\in\mathbb{R}^{N\times
    N}$ is the identity matrix, and $\mathcal{T}_{k}(x)$ is the Chebyshev polynomial
    of order $k$. The rescaling is necessary because of the orthonormal basis of Chebyshev
    polynomials. Using the fact that a polynomial of the Laplacian matrix acts as
    a polynomial of its eigenvalues, i.e., $\mathbf{L}^{k}=\mathbf{Q}\mathbf{\Lambda}^{k}\mathbf{Q}^{T}$,
    the filter operation in Eq. ([6](#S4.E6 "In 4.1.1 Spectral Methods ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    can be rewritten as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{\mathbf{\Lambda}}=2\mathbf{\Lambda}/\lambda_{max}-\mathbf{I}$ 是缩放后的特征值，$\lambda_{max}$
    是最大特征值，$\mathbf{I}\in\mathbb{R}^{N\times N}$ 是单位矩阵，$\mathcal{T}_{k}(x)$ 是阶数为 $k$
    的切比雪夫多项式。由于切比雪夫多项式的正交归一基，这种缩放是必要的。利用拉普拉斯矩阵的多项式作用于其特征值的性质，即 $\mathbf{L}^{k}=\mathbf{Q}\mathbf{\Lambda}^{k}\mathbf{Q}^{T}$，公式中的滤波操作可以重写如下：
- en: '|  | $\displaystyle\mathbf{u}^{\prime}=\mathbf{Q}\mathbf{\Theta}(\mathbf{\Lambda})\mathbf{Q}^{T}\mathbf{u}=$
    | $\displaystyle\sum\nolimits_{k=0}^{K}\theta_{k}\mathbf{Q}\mathcal{T}_{k}(\tilde{\mathbf{\Lambda}})\mathbf{Q}^{T}\mathbf{u}$
    |  | (11) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{u}^{\prime}=\mathbf{Q}\mathbf{\Theta}(\mathbf{\Lambda})\mathbf{Q}^{T}\mathbf{u}=$
    | $\displaystyle\sum\nolimits_{k=0}^{K}\theta_{k}\mathbf{Q}\mathcal{T}_{k}(\tilde{\mathbf{\Lambda}})\mathbf{Q}^{T}\mathbf{u}$
    |  | (11) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum\nolimits_{k=0}^{K}\theta_{k}\mathcal{T}_{k}(\tilde{\mathbf{L}})\mathbf{u}=\sum\nolimits_{k=0}^{K}\theta_{k}\bar{\mathbf{u}}_{k},$
    |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum\nolimits_{k=0}^{K}\theta_{k}\mathcal{T}_{k}(\tilde{\mathbf{L}})\mathbf{u}=\sum\nolimits_{k=0}^{K}\theta_{k}\bar{\mathbf{u}}_{k},$
    |  |'
- en: 'where $\bar{\mathbf{u}}_{k}=\mathcal{T}_{k}(\tilde{\mathbf{L}})\mathbf{u}$
    and $\tilde{\mathbf{L}}=2\mathbf{L}/\lambda_{max}-\mathbf{I}$. Using the recurrence
    relation of the Chebyshev polynomial $\mathcal{T}_{k}(x)=2x\mathcal{T}_{k-1}(x)-\mathcal{T}_{k-2}(x)$
    and $\mathcal{T}_{0}(x)=1,\mathcal{T}_{1}(x)=x$, $\bar{\mathbf{u}}_{k}$ can also
    be calculated recursively:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bar{\mathbf{u}}_{k}=\mathcal{T}_{k}(\tilde{\mathbf{L}})\mathbf{u}$ 和 $\tilde{\mathbf{L}}=2\mathbf{L}/\lambda_{max}-\mathbf{I}$。使用切比雪夫多项式的递推关系
    $\mathcal{T}_{k}(x)=2x\mathcal{T}_{k-1}(x)-\mathcal{T}_{k-2}(x)$ 和 $\mathcal{T}_{0}(x)=1,\mathcal{T}_{1}(x)=x$，$\bar{\mathbf{u}}_{k}$
    也可以递归计算：
- en: '|  | $\bar{\mathbf{u}}_{k}=2\tilde{\mathbf{L}}\bar{\mathbf{u}}_{k-1}-\bar{\mathbf{u}}_{k-2}$
    |  | (12) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{\mathbf{u}}_{k}=2\tilde{\mathbf{L}}\bar{\mathbf{u}}_{k-1}-\bar{\mathbf{u}}_{k-2}$
    |  | (12) |'
- en: 'with $\bar{\mathbf{u}}_{0}=\mathbf{u}$ and $\bar{\mathbf{u}}_{1}=\tilde{\mathbf{L}}\mathbf{u}$.
    Now, because only the matrix multiplication of a sparse matrix $\tilde{\mathbf{L}}$
    and some vectors need to be calculated, the time complexity becomes $O(KM)$ when
    using sparse matrix multiplication, where $M$ is the number of edges and $K$ is
    the polynomial order, i.e., the time complexity is linear with respect to the
    number of edges. It is also easy to see that such a polynomial filter is strictly
    $K$-localized: after one convolution, the representation of node $v_{i}$ will
    be affected only by its $K$-step neighborhoods $\mathcal{N}_{K}(i)$. Interestingly,
    this idea is used independently in network embedding to preserve the high-order
    proximity [[77](#bib.bib77)], of which we omit the details for brevity.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bar{\mathbf{u}}_{0}=\mathbf{u}$ 和 $\bar{\mathbf{u}}_{1}=\tilde{\mathbf{L}}\mathbf{u}$。现在，因为只需计算稀疏矩阵
    $\tilde{\mathbf{L}}$ 和一些向量的矩阵乘法，使用稀疏矩阵乘法时时间复杂度变为 $O(KM)$，其中 $M$ 是边的数量，$K$ 是多项式的阶数，即时间复杂度与边的数量线性相关。也可以很容易看出，这种多项式滤波器是严格的
    $K$-局部化的：经过一次卷积后，节点 $v_{i}$ 的表示只会受到其 $K$ 步邻域 $\mathcal{N}_{K}(i)$ 的影响。有趣的是，这一思想在网络嵌入中被独立使用，以保持高阶邻近性 [[77](#bib.bib77)]，详细信息我们为简洁起见略去。
- en: 'Kipf and Welling [[43](#bib.bib43)] further simplified the filtering by using
    only the first-order neighbors:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Kipf 和 Welling [[43](#bib.bib43)] 通过仅使用一阶邻居进一步简化了滤波：
- en: '|  | $\mathbf{h}^{l+1}_{i}=\rho\left(\sum_{j\in\tilde{\mathcal{N}}(i)}\frac{1}{\sqrt{\tilde{\mathbf{D}}(i,i)\tilde{\mathbf{D}}(j,j)}}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right),$
    |  | (13) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}^{l+1}_{i}=\rho\left(\sum_{j\in\tilde{\mathcal{N}}(i)}\frac{1}{\sqrt{\tilde{\mathbf{D}}(i,i)\tilde{\mathbf{D}}(j,j)}}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right),$
    |  | (13) |'
- en: 'where $\mathbf{h}^{l}_{i}\in\mathbb{R}^{f_{l}}$ is the hidden representation
    of node $v_{i}$ in the $l^{th}$ layer⁶⁶6We use a different letter because $\mathbf{h}^{l}\in\mathbb{R}^{f_{l}}$
    is the hidden representation of one node, while $\mathbf{u}^{l}\in\mathbb{R}^{N}$
    represents a dimension for all nodes., $\tilde{\mathbf{D}}=\mathbf{D}+\mathbf{I}$,
    and $\tilde{\mathcal{N}}(i)=\mathcal{N}(i)\cup\{i\}$. This can be written equivalently
    in an matrix form as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{h}^{l}_{i}\in\mathbb{R}^{f_{l}}$ 是第 $l^{th}$ 层中节点 $v_{i}$ 的隐藏表示⁶⁶6我们使用不同的字母，因为
    $\mathbf{h}^{l}\in\mathbb{R}^{f_{l}}$ 是一个节点的隐藏表示，而 $\mathbf{u}^{l}\in\mathbb{R}^{N}$
    表示所有节点的一个维度。,$ $\tilde{\mathbf{D}}=\mathbf{D}+\mathbf{I}$，且 $\tilde{\mathcal{N}}(i)=\mathcal{N}(i)\cup\{i\}$。这可以等效地写成矩阵形式如下：
- en: '|  | $\mathbf{H}^{l+1}=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (14) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{l+1}=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (14) |'
- en: 'where $\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}$, i.e., adding a self-connection.
    The authors showed that Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency Aspect ‣ 4.1
    Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs:
    A Survey")) is a special case of Eq. ([9](#S4.E9 "In 4.1.2 The Efficiency Aspect
    ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) by setting $K=1$ with a few minor changes. Then, the authors
    argued that stacking an adequate number of layers as illustrated in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey") has a modeling capacity
    similar to ChebNet but leads to better results.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}$，即添加自连接。作者展示了方程 ([14](#S4.E14
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) 是方程 ([9](#S4.E9 "In 4.1.2 The
    Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")) 的一个特例，通过设置 $K=1$ 和一些小的修改。随后，作者认为，正如图 [3](#S4.F3
    "Figure 3 ‣ 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey") 所示，堆叠足够数量的层具有类似于
    ChebNet 的建模能力，但能获得更好的结果。'
- en: '![Refer to caption](img/b9b235bfb30b41141764c3552d5dd636.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b9b235bfb30b41141764c3552d5dd636.png)'
- en: 'Figure 3: An illustrative example of the spatial convolution operation proposed
    by Kipf and Welling [[43](#bib.bib43)] (reprinted with permission). Nodes are
    affected only by their immediate neighbors in each convolutional layer.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: Kipf 和 Welling [[43](#bib.bib43)] 提出的空间卷积操作的示例（经许可转载）。每个卷积层中，节点仅受其直接邻居的影响。'
- en: 'An important insight of ChebNet and its extension is that they connect the
    spectral graph convolution with the spatial architecture. Specifically, they show
    that when the spectral convolution function is polynomial or first-order, the
    spectral graph convolution is equivalent to a spatial convolution. In addition,
    the convolution in Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    is highly similar to the state definition in a GNN in Eq. ([1](#S3.E1 "In 3.1
    Node-level RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on Graphs:
    A Survey")), except that the convolution definition replaces the recursive definition.
    From this aspect, a GNN can be regarded as a GCN with a large number of identical
    layers to reach stable states [[7](#bib.bib7)], i.e., a GNN uses a fixed function
    with fixed parameters to iteratively update the node hidden states until reaching
    an equilibrium, while a GCN has a preset number of layers and each layer contains
    different parameters.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'ChebNet 及其扩展的一个重要见解是，它们将光谱图卷积与空间架构连接起来。具体而言，它们展示了当光谱卷积函数为多项式或一阶时，光谱图卷积等同于空间卷积。此外，Eq.
    ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4
    Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) 中的卷积与 GNN
    中 Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks
    ‣ Deep Learning on Graphs: A Survey")) 中的状态定义高度相似，只是卷积定义取代了递归定义。从这一方面看，GNN 可以被视为具有大量相同层的
    GCN，以达到稳定状态 [[7](#bib.bib7)]，即 GNN 使用具有固定参数的固定函数来迭代更新节点隐藏状态，直到达到平衡，而 GCN 有预设数量的层，每层包含不同的参数。'
- en: 'Some spectral methods have also been proposed to solve the efficiency problem.
    For example, instead of using the Chebyshev expansion as in Eq. ([10](#S4.E10
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")), CayleyNet [[44](#bib.bib44)]
    adopted Cayley polynomials to define graph convolutions:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '一些光谱方法也被提出以解决效率问题。例如，CayleyNet [[44](#bib.bib44)] 采用 Cayley 多项式来定义图卷积，而不是像
    Eq. ([10](#S4.E10 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) 中使用切比雪夫展开：'
- en: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\theta_{0}+2Re\left\{\sum\nolimits_{k=1}^{K}\theta_{k}\left(\theta_{h}\mathbf{\Lambda}-i\mathbf{I}\right)^{k}\left(\theta_{h}\mathbf{\Lambda}+i\mathbf{I}\right)^{k}\right\},$
    |  | (15) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\Theta}(\mathbf{\Lambda})=\theta_{0}+2Re\left\{\sum\nolimits_{k=1}^{K}\theta_{k}\left(\theta_{h}\mathbf{\Lambda}-i\mathbf{I}\right)^{k}\left(\theta_{h}\mathbf{\Lambda}+i\mathbf{I}\right)^{k}\right\},$
    |  | (15) |'
- en: 'where $i=\sqrt{-1}$ denotes the imaginary unit and $\theta_{h}$ is another
    spectral zoom parameter. In addition to showing that CayleyNet is as efficient
    as ChebNet, the authors demonstrated that the Cayley polynomials can detect “narrow
    frequency bands of importance” to achieve better results. Graph wavelet neural
    network (GWNN) [[45](#bib.bib45)] was further proposed to replace the Fourier
    transform in spectral filters by the graph wavelet transform by rewriting Eq. ([5](#S4.E5
    "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $i=\sqrt{-1}$ 表示虚数单位，$\theta_{h}$ 是另一个谱缩放参数。除了表明 CayleyNet 与 ChebNet 一样高效之外，作者们还展示了
    Cayley 多项式可以检测“重要的狭频带”以获得更好的结果。图小波神经网络（GWNN）[[45](#bib.bib45)] 被进一步提出，以通过图小波变换取代光谱滤波器中的傅里叶变换，将
    Eq. ([5](#S4.E5 "In 4.1.1 Spectral Methods ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) 重写为以下形式：'
- en: '|  | $\mathbf{u}_{1}*_{G}\mathbf{u}_{2}=\mathbf{\psi}\left(\left(\mathbf{\psi}^{-1}\mathbf{u}_{1}\right)\odot\left(\mathbf{\psi}^{-1}\mathbf{u}_{2}\right)\right),$
    |  | (16) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{u}_{1}*_{G}\mathbf{u}_{2}=\mathbf{\psi}\left(\left(\mathbf{\psi}^{-1}\mathbf{u}_{1}\right)\odot\left(\mathbf{\psi}^{-1}\mathbf{u}_{2}\right)\right),$
    |  | (16) |'
- en: where $\mathbf{\psi}$ denotes the graph wavelet bases. By using fast approximating
    algorithms to calculate $\mathbf{\psi}$ and $\mathbf{\psi}^{-1}$, GWNN’s computational
    complexity is also $O(KM)$, i.e., linear with respect to the number of edges.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{\psi}$ 表示图小波基。通过使用快速近似算法计算 $\mathbf{\psi}$ 和 $\mathbf{\psi}^{-1}$，GWNN
    的计算复杂度也为 $O(KM)$，即与边的数量线性相关。
- en: 4.1.3 The Aspect of Multiple Graphs
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 多图的方面
- en: 'A parallel series of works has focuses on generalizing graph convolutions to
    multiple graphs of arbitrary sizes. Neural FPs [[46](#bib.bib46)] proposed a spatial
    method that also used the first-order neighbors:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列并行的研究专注于将图卷积推广到任意大小的多个图。Neural FPs [[46](#bib.bib46)] 提出了一个空间方法，该方法也使用了第一阶邻居：
- en: '|  | $\mathbf{h}^{l+1}_{i}=\sigma\left(\sum\nolimits_{j\in\hat{\mathcal{N}}(i)}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right).$
    |  | (17) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}^{l+1}_{i}=\sigma\left(\sum\nolimits_{j\in\hat{\mathcal{N}}(i)}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right).$
    |  | (17) |'
- en: 'Because the parameters $\mathbf{\Theta}$ can be shared across different graphs
    and are independent of the graph size, Neural FPs can handle multiple graphs of
    arbitrary sizes. Note that Eq. ([17](#S4.E17 "In 4.1.3 The Aspect of Multiple
    Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) is very similar to Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency
    Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")). However, instead of considering the influence of node
    degree by adding a normalization term, Neural FPs proposed learning different
    parameters $\mathbf{\Theta}$ for nodes with different degrees. This strategy performed
    well for small graphs such as molecular graphs (i.e., atoms as nodes and bonds
    as edges), but may not be scalable to larger graphs.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '由于参数 $\mathbf{\Theta}$ 可以在不同图之间共享，并且与图的大小无关，Neural FPs 可以处理任意大小的多个图。请注意，公式 ([17](#S4.E17
    "In 4.1.3 The Aspect of Multiple Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) 与公式 ([13](#S4.E13
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) 非常相似。然而，Neural FPs 并没有通过添加归一化项来考虑节点度数的影响，而是提出为不同度数的节点学习不同的参数
    $\mathbf{\Theta}$。这种策略在小型图（如分子图，即原子作为节点，键作为边）上表现良好，但可能无法扩展到更大的图。'
- en: PATCHY-SAN [[47](#bib.bib47)] adopted a different idea. It assigned a unique
    node order using a graph labeling procedure such as the Weisfeiler-Lehman kernel [[78](#bib.bib78)]
    and then arranged node neighbors in a line using this pre-defined order. In addition,
    PATCHY-SAN defined a “receptive field” for each node $v_{i}$ by selecting a fixed
    number of nodes from its $k$-step neighborhoods $\mathcal{N}_{k}(i)$. Then a standard
    1-D CNN with proper normalization was adopted. Using this approach, nodes in different
    graphs all have a “receptive field” with a fixed size and order; thus, PATCHY-SAN
    can learn from multiple graphs like normal CNNs learn from multiple images. The
    drawbacks are that the convolution depends heavily on the graph labeling procedure
    which is a preprocessing step that is not learned. LGCN [[48](#bib.bib48)] further
    proposed to simplify the sorting process by using a lexicographical order (i.e.,
    sorting neighbors based on their hidden representation in the final layer $\mathbf{H}^{L}$).
    Instead of using a single order, the authors sorted different channels of $\mathbf{H}^{L}$
    separately. SortPooling [[49](#bib.bib49)] took a similar approach, but rather
    than sorting the neighbors of each node, the authors proposed to sort all the
    nodes (i.e., using a single order for all the neighborhoods). Despite the differences
    among these methods, enforcing a 1-D node order may not be a natural choice for
    graphs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: PATCHY-SAN [[47](#bib.bib47)] 采用了不同的想法。它通过图标记程序（如 Weisfeiler-Lehman 核 [[78](#bib.bib78)]）分配了唯一的节点顺序，然后使用这个预定义的顺序将节点邻居排成一行。此外，PATCHY-SAN
    为每个节点 $v_{i}$ 定义了一个“感受野”，通过从其 $k$-步邻域 $\mathcal{N}_{k}(i)$ 中选择固定数量的节点来实现。然后，采用了标准的
    1-D CNN，并进行了适当的归一化。采用这种方法，不同图中的节点都具有固定大小和顺序的“感受野”；因此，PATCHY-SAN 可以像正常 CNN 从多个图像中学习一样，从多个图中学习。缺点是卷积严重依赖于图标记程序，这是一种未学习的预处理步骤。LGCN [[48](#bib.bib48)]
    进一步提出通过使用字典序（即根据最终层 $\mathbf{H}^{L}$ 中的隐藏表示对邻居进行排序）来简化排序过程。作者并没有使用单一的顺序，而是分别对
    $\mathbf{H}^{L}$ 的不同通道进行排序。SortPooling [[49](#bib.bib49)] 采取了类似的方法，但与其排序每个节点的邻居不同，作者提出对所有节点进行排序（即对所有邻域使用单一的顺序）。尽管这些方法存在差异，但强制使用
    1-D 节点顺序可能不是图的自然选择。
- en: 'DCNN [[50](#bib.bib50)] adopted another approach by replacing the eigenbasis
    of the graph convolution with a diffusion-basis, i.e., the neighborhoods of nodes
    were determined by the diffusion transition probability between nodes. Specifically,
    the convolution was defined as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: DCNN [[50](#bib.bib50)] 采用了另一种方法，通过用扩散基替换图卷积的特征基，即节点的邻域是通过节点之间的扩散转移概率确定的。具体而言，卷积定义如下：
- en: '|  | $\mathbf{H}^{l+1}=\rho\left(\mathbf{P}^{K}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (18) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{l+1}=\rho\left(\mathbf{P}^{K}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (18) |'
- en: where $\mathbf{P}^{K}=\left(\mathbf{P}\right)^{K}$ is the transition probability
    of a length-$K$ diffusion process (i.e., random walks), $K$ is a preset diffusion
    length, and $\mathbf{\Theta}^{l}$ are learnable parameters. Because only $\mathbf{P}^{K}$
    depends on the graph structure, the parameters $\mathbf{\Theta}^{l}$ can be shared
    across graphs of arbitrary sizes. However, calculating $\mathbf{P}^{K}$ has a
    time complexity of $O\left(N^{2}K\right)$; thus, this method is not scalable to
    large graphs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{P}^{K}=\left(\mathbf{P}\right)^{K}$ 是长度为 $K$ 的扩散过程（即随机游走）的转移概率，$K$
    是预设的扩散长度，而 $\mathbf{\Theta}^{l}$ 是可学习的参数。由于只有 $\mathbf{P}^{K}$ 依赖于图结构，因此参数 $\mathbf{\Theta}^{l}$
    可以在任意大小的图中共享。然而，计算 $\mathbf{P}^{K}$ 的时间复杂度为 $O\left(N^{2}K\right)$；因此，这种方法无法扩展到大图。
- en: 'DGCN [[51](#bib.bib51)] was further proposed to jointly adopt the diffusion
    and the adjacency bases using a dual graph convolutional network. Specifically,
    DGCN used two convolutions: one was Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency
    Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")), and the other replaced the adjacency matrix with the positive
    pointwise mutual information (PPMI) matrix [[79](#bib.bib79)] of the transition
    probability as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: DGCN [[51](#bib.bib51)] 进一步提出了使用双图卷积网络联合采用扩散和邻接基础。具体而言，DGCN 使用了两种卷积：一种是 Eq.
    ([14](#S4.E14 "在 4.1.2 效率方面 ‣ 4.1 卷积操作 ‣ 4 图卷积网络 ‣ 深度学习在图上的应用：综述"))，另一种则用转移概率的正点互信息（PPMI）矩阵
    [[79](#bib.bib79)] 替换了邻接矩阵，如下所示：
- en: '|  | $\mathbf{Z}^{l+1}=\rho\left(\mathbf{D}^{-\frac{1}{2}}_{P}\mathbf{X}_{P}\mathbf{D}^{-\frac{1}{2}}_{P}\mathbf{Z}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (19) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{Z}^{l+1}=\rho\left(\mathbf{D}^{-\frac{1}{2}}_{P}\mathbf{X}_{P}\mathbf{D}^{-\frac{1}{2}}_{P}\mathbf{Z}^{l}\mathbf{\Theta}^{l}\right),$
    |  | (19) |'
- en: 'where $\mathbf{X}_{P}$ is the PPMI matrix calculated as:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{X}_{P}$ 是计算出的 PPMI 矩阵：
- en: '|  | $\mathbf{X}_{P}(i,j)=\max\left(\log\left(\frac{\mathbf{P}(i,j)\sum_{i,j}\mathbf{P}(i,j)}{\sum_{i}\mathbf{P}(i,j)\sum_{j}\mathbf{P}(i,j)}\right),0\right),$
    |  | (20) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{X}_{P}(i,j)=\max\left(\log\left(\frac{\mathbf{P}(i,j)\sum_{i,j}\mathbf{P}(i,j)}{\sum_{i}\mathbf{P}(i,j)\sum_{j}\mathbf{P}(i,j)}\right),0\right),$
    |  | (20) |'
- en: and $\mathbf{D}_{P}(i,i)=\sum_{j}\mathbf{X}_{P}(i,j)$ is the diagonal degree
    matrix of $\mathbf{X}_{P}$. Then, these two convolutions were ensembled by minimizing
    the mean square differences between $\mathbf{H}$ and $\mathbf{Z}$. DGCN adopted
    a random walk sampling technique to accelerate the transition probability calculation.
    The experiments demonstrated that such dual convolutions were effective even for
    single-graph problems.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 而 $\mathbf{D}_{P}(i,i)=\sum_{j}\mathbf{X}_{P}(i,j)$ 是 $\mathbf{X}_{P}$ 的对角度矩阵。然后，这两种卷积通过最小化
    $\mathbf{H}$ 和 $\mathbf{Z}$ 之间的均方差来进行集成。DGCN 采用了一种随机游走采样技术来加速转移概率的计算。实验表明，即使在单图问题中，这种双重卷积也是有效的。
- en: 4.1.4 Frameworks
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 框架
- en: 'Based on the above two lines of works, MPNNs [[52](#bib.bib52)] were proposed
    as a unified framework for the graph convolution operation in the spatial domain
    using message-passing functions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述两行工作，MPNNs [[52](#bib.bib52)] 被提出作为一个统一的框架，用于在空间域中使用消息传递函数进行图卷积操作：
- en: '|  | $\begin{gathered}\mathbf{m}_{i}^{l+1}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}^{l}\left(\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{F}^{E}_{i,j}\right)\\
    \mathbf{h}_{i}^{l+1}=\mathcal{G}^{l}\left(\mathbf{h}_{i}^{l},\mathbf{m}_{i}^{l+1}\right),\end{gathered}$
    |  | (21) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}\mathbf{m}_{i}^{l+1}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}^{l}\left(\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{F}^{E}_{i,j}\right)\\
    \mathbf{h}_{i}^{l+1}=\mathcal{G}^{l}\left(\mathbf{h}_{i}^{l},\mathbf{m}_{i}^{l+1}\right),\end{gathered}$
    |  | (21) |'
- en: where $\mathcal{F}^{l}(\cdot)$ and $\mathcal{G}^{l}(\cdot)$ are the message
    functions and vertex update functions to be learned, respectively, and $\mathbf{m}^{l}$
    denotes the “messages” passed between nodes. Conceptually, MPNNs are a framework
    in which each node sends messages based on its states and updates its states based
    on messages received from the immediate neighbors. The authors showed that the
    above framework had included many existing methods such as GGS-NNs [[24](#bib.bib24)],
    Bruna et al. [[40](#bib.bib40)], Henaff et al. [[41](#bib.bib41)], Neural FPs [[46](#bib.bib46)],
    Kipf and Welling [[43](#bib.bib43)] and Kearnes et al. [[55](#bib.bib55)] as special
    cases. In addition, the authors proposed adding a “master” node that was connected
    to all the nodes to accelerate the message-passing across long distances, and
    they split the hidden representations into different “towers” to improve the generalization
    ability. The authors showed that a specific variant of MPNNs could achieve state-of-the-art
    performance in predicting molecular properties.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}^{l}(\cdot)$ 和 $\mathcal{G}^{l}(\cdot)$ 分别是需要学习的消息函数和顶点更新函数，而
    $\mathbf{m}^{l}$ 表示节点之间传递的“消息”。 从概念上讲，MPNNs 是一个框架，其中每个节点基于其状态发送消息，并根据从直接邻居接收到的消息更新其状态。作者表明，上述框架包含了许多现有方法，如
    GGS-NNs [[24](#bib.bib24)]、Bruna 等人 [[40](#bib.bib40)]、Henaff 等人 [[41](#bib.bib41)]、Neural
    FPs [[46](#bib.bib46)]、Kipf 和 Welling [[43](#bib.bib43)] 以及 Kearnes 等人 [[55](#bib.bib55)]
    作为特殊情况。此外，作者提出添加一个与所有节点连接的“主”节点，以加速跨长距离的消息传递，并将隐藏表示拆分成不同的“塔”以提高泛化能力。作者表明，MPNNs
    的特定变体在预测分子属性方面可以达到最先进的性能。
- en: 'Concurrently, GraphSAGE [[53](#bib.bib53)] took a similar idea as Eq. ([21](#S4.E21
    "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")) using multiple aggregating functions as
    follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，GraphSAGE [[53](#bib.bib53)] 采用了类似的思想，如 Eq. ([21](#S4.E21 "在 4.1.4 框架 ‣
    4.1 卷积操作 ‣ 4 图卷积网络 ‣ 图上的深度学习：综述"))，使用了多种聚合函数，如下所示：
- en: '|  | $\begin{gathered}\mathbf{m}_{i}^{l+1}=\text{AGGREGATE}^{l}(\{\mathbf{h}_{j}^{l},\forall
    j\in\mathcal{N}(i)\})\\ \mathbf{h}_{i}^{l+1}=\rho\left(\mathbf{\Theta}^{l}\left[\mathbf{h}_{i}^{l},\mathbf{m}_{i}^{l+1}\right]\right),\end{gathered}$
    |  | (22) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}\mathbf{m}_{i}^{l+1}=\text{AGGREGATE}^{l}(\{\mathbf{h}_{j}^{l},\forall
    j\in\mathcal{N}(i)\})\\ \mathbf{h}_{i}^{l+1}=\rho\left(\mathbf{\Theta}^{l}\left[\mathbf{h}_{i}^{l},\mathbf{m}_{i}^{l+1}\right]\right),\end{gathered}$
    |  | (22) |'
- en: 'where $\left[\cdot,\cdot\right]$ is the concatenation operation and $\text{AGGREGATE}(\cdot)$
    represents the aggregating function. The authors suggested three aggregating functions:
    the element-wise mean, an LSTM, and max-pooling as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\left[\cdot,\cdot\right]$ 是拼接操作，而 $\text{AGGREGATE}(\cdot)$ 代表聚合函数。作者建议了三种聚合函数：逐元素均值、LSTM
    和最大池化，如下所示：
- en: '|  | $\text{AGGREGATE}^{l}=\max\{\rho(\mathbf{\Theta}_{\text{pool}}\mathbf{h}_{j}^{l}+\mathbf{b}_{\text{pool}}),\forall
    j\in\mathcal{N}(i)\},$ |  | (23) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{AGGREGATE}^{l}=\max\{\rho(\mathbf{\Theta}_{\text{pool}}\mathbf{h}_{j}^{l}+\mathbf{b}_{\text{pool}}),\forall
    j\in\mathcal{N}(i)\},$ |  | (23) |'
- en: where $\mathbf{\Theta}_{\text{pool}}$ and $\mathbf{b}_{\text{pool}}$ are the
    parameters to be learned and $\max\left\{\cdot\right\}$ is the element-wise maximum.
    For the LSTM aggregating function, because an neighbors order is needed, the authors
    adopted a simple random order.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{\Theta}_{\text{pool}}$ 和 $\mathbf{b}_{\text{pool}}$ 是待学习的参数，而 $\max\left\{\cdot\right\}$
    是逐元素最大值。对于 LSTM 聚合函数，由于需要邻居的顺序，作者采用了简单的随机顺序。
- en: 'Mixture model network (MoNet) [[54](#bib.bib54)] also tried to unify the existing
    GCN models as well as CNNs for manifolds into a common framework using “template
    matching”:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型网络（MoNet）[[54](#bib.bib54)] 还尝试将现有的 GCN 模型以及用于流形的 CNN 统一为一个通用框架，使用了“模板匹配”：
- en: '|  | $h^{l+1}_{ik}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}^{l}_{k}(\mathbf{u}(i,j))\mathbf{h}^{l}_{j},k=1,...,f_{l+1},$
    |  | (24) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | $h^{l+1}_{ik}=\sum\nolimits_{j\in\mathcal{N}(i)}\mathcal{F}^{l}_{k}(\mathbf{u}(i,j))\mathbf{h}^{l}_{j},k=1,...,f_{l+1},$
    |  | (24) |'
- en: 'where $\mathbf{u}(i,j)$ are the pseudo-coordinates of the node pair $(v_{i},v_{j})$,
    $\mathcal{F}^{l}_{k}(\mathbf{u})$ is a parametric function to be learned, and
    $h^{l}_{ik}$ is the $k^{th}$ dimension of $\mathbf{h}^{l}_{i}$. In other words,
    $\mathcal{F}^{l}_{k}(\mathbf{u})$ served as a weighting kernel for combining neighborhoods.
    Then, MoNet adopted the following Gaussian kernel:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{u}(i,j)$ 是节点对 $(v_{i},v_{j})$ 的伪坐标，$\mathcal{F}^{l}_{k}(\mathbf{u})$
    是待学习的参数函数，而 $h^{l}_{ik}$ 是 $\mathbf{h}^{l}_{i}$ 的第 $k^{th}$ 维度。 换句话说，$\mathcal{F}^{l}_{k}(\mathbf{u})$
    作为加权核用于组合邻域。然后，MoNet 采用了以下高斯核：
- en: '|  | $\mathcal{F}^{l}_{k}(\mathbf{u})=\exp\left(-\frac{1}{2}(\mathbf{u}-\bm{\mu}^{l}_{k})^{T}(\mathbf{\Sigma}^{l}_{k})^{-1}(\mathbf{u}-\bm{\mu}^{l}_{k})\right),$
    |  | (25) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| | $\mathcal{F}^{l}_{k}(\mathbf{u})=\exp\left(-\frac{1}{2}(\mathbf{u}-\bm{\mu}^{l}_{k})^{T}(\mathbf{\Sigma}^{l}_{k})^{-1}(\mathbf{u}-\bm{\mu}^{l}_{k})\right),$
    | | (25) |'
- en: where $\bm{\mu}^{l}_{k}$ and $\mathbf{\Sigma}^{l}_{k}$ are the mean vectors
    and diagonal covariance matrices to be learned, respectively. The pseudo-coordinates
    were degrees as in Kipf and Welling [[43](#bib.bib43)], i.e.,
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bm{\mu}^{l}_{k}$和$\mathbf{\Sigma}^{l}_{k}$分别是要学习的均值向量和对角协方差矩阵。伪坐标是度，就像Kipf和Welling[[43](#bib.bib43)]中那样，
- en: '|  | $\mathbf{u}(i,j)=(\frac{1}{\sqrt{\mathbf{D}(i,i)}},\frac{1}{\sqrt{\mathbf{D}(j,j)}}).$
    |  | (26) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  |$\mathbf{u}(i,j)=(\frac{1}{\sqrt{\mathbf{D}(i,i)}},\frac{1}{\sqrt{\mathbf{D}(j,j)}}).$
    | |(26) |'
- en: 'Graph networks (GNs) [[9](#bib.bib9)] proposed a more general framework for
    both GCNs and GNNs that learned three sets of representations: $\mathbf{h}_{i}^{l},\mathbf{e}_{ij}^{l}$,
    and $\mathbf{z}^{l}$ as the representation for nodes, edges, and the entire graph,
    respectively. These representations were learned using three aggregation and three
    updating functions:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图网络（GNs）[[9](#bib.bib9)]提出了一个更通用的框架，用于学习三组表示：$\mathbf{h}_{i}^{l},\mathbf{e}_{ij}^{l}$和$\mathbf{z}^{l}$，分别作为节点、边和整个图的表示。这些表示是使用三个聚合和三个更新函数学习的：
- en: '|  | <math   alttext="\begin{gathered}\mathbf{m}_{i}^{l}=\mathcal{G}^{E\rightarrow
    V}(\{\mathbf{e}_{ij}^{l},\forall j\in\mathcal{N}(i)\}),\mathbf{m}_{V}^{l}=\mathcal{G}^{V\rightarrow
    G}(\{\mathbf{h}_{i}^{l},\forall v_{i}\in V\})\\ \mathbf{m}_{E}^{l}=\mathcal{G}^{E\rightarrow
    G}(\{\mathbf{e}_{ij}^{l},\forall(v_{i},v_{j})\in E\}),\mathbf{h}_{i}^{l+1}=\mathcal{F}^{V}(\mathbf{m}_{i}^{l},\mathbf{h}_{i}^{l},\mathbf{z}^{l})\\'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |<math alttext="\begin{gathered}\mathbf{m}_{i}^{l}=\mathcal{G}^{E\rightarrow
    V}(\{\mathbf{e}_{ij}^{l},\forall j\in\mathcal{N}(i)\}),\mathbf{m}_{V}^{l}=\mathcal{G}^{V\rightarrow
    G}(\{\mathbf{h}_{i}^{l},\forall v_{i}\in V\})\\ \mathbf{m}_{E}^{l}=\mathcal{G}^{E\rightarrow
    G}(\{\mathbf{e}_{ij}^{l},\forall(v_{i},v_{j})\in E\}),\mathbf{h}_{i}^{l+1}=\mathcal{F}^{V}(\mathbf{m}_{i}^{l},\mathbf{h}_{i}^{l},\mathbf{z}^{l})
    \\ '
- en: \mathbf{e}_{ij}^{l+1}=\mathcal{F}^{E}(\mathbf{e}_{ij}^{l},\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{z}^{l}),\mathbf{z}^{l+1}=\mathcal{F}^{G}(\mathbf{m}_{E}^{l},\mathbf{m}_{V}^{l},\mathbf{z}^{l}),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><mrow ><msubsup ><mi  >𝐦</mi><mi >i</mi><mi >l</mi></msubsup><mo
    >=</mo><mrow ><msup ><mi >𝒢</mi><mrow ><mi  >E</mi><mo stretchy="false"  >→</mo><mi
    >V</mi></mrow></msup><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mo stretchy="false" >{</mo><mrow ><mrow ><msubsup ><mi >𝐞</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo
    >,</mo><mrow ><mo rspace="0.167em"  >∀</mo><mi >j</mi></mrow></mrow><mo >∈</mo><mrow
    ><mi >𝒩</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mi
    >i</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo stretchy="false" >}</mo></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐦</mi><mi
    >V</mi><mi >l</mi></msubsup><mo >=</mo><mrow ><msup ><mi >𝒢</mi><mrow ><mi >V</mi><mo
    stretchy="false"  >→</mo><mi >G</mi></mrow></msup><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mo stretchy="false" >{</mo><mrow
    ><mrow ><msubsup ><mi >𝐡</mi><mi >i</mi><mi >l</mi></msubsup><mo >,</mo><mrow
    ><mo rspace="0.167em" >∀</mo><msub ><mi >v</mi><mi >i</mi></msub></mrow></mrow><mo
    >∈</mo><mi >V</mi></mrow><mo stretchy="false" >}</mo></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><mrow ><msubsup
    ><mi >𝐦</mi><mi >E</mi><mi >l</mi></msubsup><mo >=</mo><mrow ><msup ><mi >𝒢</mi><mrow
    ><mi >E</mi><mo stretchy="false" >→</mo><mi >G</mi></mrow></msup><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mo stretchy="false"
    >{</mo><mrow ><mrow ><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >j</mi></mrow><mi >l</mi></msubsup><mo >,</mo><mrow ><mo >∀</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >v</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >v</mi><mi >j</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo >∈</mo><mi >E</mi></mrow><mo
    stretchy="false" >}</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo><mrow ><msubsup ><mi >𝐡</mi><mi >i</mi><mrow ><mi >l</mi><mo >+</mo><mn
    >1</mn></mrow></msubsup><mo >=</mo><mrow ><msup ><mi >ℱ</mi><mi >V</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi
    >𝐦</mi><mi >i</mi><mi >l</mi></msubsup><mo >,</mo><msubsup ><mi >𝐡</mi><mi >i</mi><mi
    >l</mi></msubsup><mo >,</mo><msup ><mi >𝐳</mi><mi >l</mi></msup><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><mrow ><mrow
    ><msubsup ><mi >𝐞</mi><mrow ><mi  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >j</mi></mrow><mrow ><mi >l</mi><mo >+</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow
    ><msup ><mi >ℱ</mi><mi >E</mi></msup><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo >,</mo><msubsup
    ><mi >𝐡</mi><mi >i</mi><mi >l</mi></msubsup><mo >,</mo><msubsup ><mi >𝐡</mi><mi
    >j</mi><mi >l</mi></msubsup><mo >,</mo><msup ><mi >𝐳</mi><mi >l</mi></msup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msup ><mi >𝐳</mi><mrow
    ><mi >l</mi><mo >+</mo><mn >1</mn></mrow></msup><mo >=</mo><mrow ><msup ><mi >ℱ</mi><mi
    >G</mi></msup><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msubsup ><mi >𝐦</mi><mi >E</mi><mi >l</mi></msubsup><mo >,</mo><msubsup
    ><mi >𝐦</mi><mi >V</mi><mi >l</mi></msubsup><mo >,</mo><msup ><mi >𝐳</mi><mi >l</mi></msup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐦</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝒢</ci><apply ><ci  >→</ci><ci >𝐸</ci><ci >𝑉</ci></apply></apply><set
    ><apply ><list ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐞</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="latexml"  >for-all</csymbol><ci >𝑗</ci></apply></list><apply
    ><ci >𝒩</ci><ci >𝑖</ci></apply></apply></set></apply></apply><apply ><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐦</ci><ci >𝑉</ci></apply><ci >𝑙</ci></apply><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝒢</ci><apply ><ci >→</ci><ci
    >𝑉</ci><ci >𝐺</ci></apply></apply><set ><apply ><list ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐡</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply><apply ><csymbol cd="latexml" >for-all</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑣</ci><ci >𝑖</ci></apply></apply></list><ci
    >𝑉</ci></apply></set><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐦</ci><ci >𝐸</ci></apply><ci
    >𝑙</ci></apply></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝒢</ci><apply ><ci >→</ci><ci >𝐸</ci><ci >𝐺</ci></apply></apply><set
    ><apply ><list ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐞</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="latexml" >for-all</csymbol><interval closure="open"
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑣</ci><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑣</ci><ci >𝑗</ci></apply></interval></apply></list><ci
    >𝐸</ci></apply></set></apply></apply></apply></apply><apply ><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐡</ci><ci >𝑖</ci></apply><apply ><ci >𝑙</ci><cn type="integer"  >1</cn></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℱ</ci><ci >𝑉</ci></apply><vector
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐦</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐡</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐳</ci><ci >𝑙</ci></apply></vector><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐞</ci><apply ><ci >𝑖</ci><ci
    >𝑗</ci></apply></apply><apply ><ci  >𝑙</ci><cn type="integer"  >1</cn></apply></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℱ</ci><ci
    >𝐸</ci></apply><vector ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐞</ci><apply ><ci >𝑖</ci><ci
    >𝑗</ci></apply></apply><ci >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐡</ci><ci >𝑖</ci></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐡</ci><ci >𝑗</ci></apply><ci >𝑙</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐳</ci><ci >𝑙</ci></apply></vector></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐳</ci><apply ><ci
    >𝑙</ci><cn type="integer"  >1</cn></apply></apply><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >ℱ</ci><ci >𝐺</ci></apply><vector ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐦</ci><ci >𝐸</ci></apply><ci >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐦</ci><ci >𝑉</ci></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐳</ci><ci
    >𝑙</ci></apply></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{gathered}\mathbf{m}_{i}^{l}=\mathcal{G}^{E\rightarrow V}(\{\mathbf{e}_{ij}^{l},\forall
    j\in\mathcal{N}(i)\}),\mathbf{m}_{V}^{l}=\mathcal{G}^{V\rightarrow G}(\{\mathbf{h}_{i}^{l},\forall
    v_{i}\in V\})\\ \mathbf{m}_{E}^{l}=\mathcal{G}^{E\rightarrow G}(\{\mathbf{e}_{ij}^{l},\forall(v_{i},v_{j})\in
    E\}),\mathbf{h}_{i}^{l+1}=\mathcal{F}^{V}(\mathbf{m}_{i}^{l},\mathbf{h}_{i}^{l},\mathbf{z}^{l})\\
    \mathbf{e}_{ij}^{l+1}=\mathcal{F}^{E}(\mathbf{e}_{ij}^{l},\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{z}^{l}),\mathbf{z}^{l+1}=\mathcal{F}^{G}(\mathbf{m}_{E}^{l},\mathbf{m}_{V}^{l},\mathbf{z}^{l}),\end{gathered}</annotation></semantics></math>
    |  | (27) |
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{m}_{i}^{l}=\mathcal{G}^{E\rightarrow V}(\{\mathbf{e}_{ij}^{l},\forall
    j\in\mathcal{N}(i)\}),\mathbf{m}_{V}^{l}=\mathcal{G}^{V\rightarrow G}(\{\mathbf{h}_{i}^{l},\forall
    v_{i}\in V\})\\ \mathbf{m}_{E}^{l}=\mathcal{G}^{E\rightarrow G}(\{\mathbf{e}_{ij}^{l},\forall(v_{i},v_{j})\in
    E\}),\mathbf{h}_{i}^{l+1}=\mathcal{F}^{V}(\mathbf{m}_{i}^{l},\mathbf{h}_{i}^{l},\mathbf{z}^{l})\\
    \mathbf{e}_{ij}^{l+1}=\mathcal{F}^{E}(\mathbf{e}_{ij}^{l},\mathbf{h}_{i}^{l},\mathbf{h}_{j}^{l},\mathbf{z}^{l}),\mathbf{z}^{l+1}=\mathcal{F}^{G}(\mathbf{m}_{E}^{l},\mathbf{m}_{V}^{l},\mathbf{z}^{l}),
- en: where $\mathcal{F}^{V}(\cdot),\mathcal{F}^{E}(\cdot)$, and $\mathcal{F}^{G}(\cdot)$
    are the corresponding updating functions for nodes, edges, and the entire graph,
    respectively, and $\mathcal{G}(\cdot)$ represents message-passing functions whose
    superscripts denote message-passing directions. Note that the message-passing
    functions all take a set as the input, thus their arguments are variable in length
    and these functions should be invariant to input permutations; some examples include
    the element-wise summation, mean, and maximum. Compared with MPNNs, GNs introduced
    the edge representations and the representation of the entire graph, thus making
    the framework more general.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{F}^{V}(\cdot)$、$\mathcal{F}^{E}(\cdot)$ 和 $\mathcal{F}^{G}(\cdot)$
    分别是节点、边和整个图的相应更新函数，而 $\mathcal{G}(\cdot)$ 表示消息传递函数，其上标表示消息传递的方向。注意，消息传递函数的输入都是一个集合，因此其参数长度可变，这些函数应该对输入的排列不变；一些例子包括逐元素求和、平均值和最大值。与MPNNs相比，GNs引入了边表示和整个图的表示，从而使得框架更具通用性。
- en: 'In summary, the convolution operations have evolved from the spectral domain
    to the spatial domain and from multistep neighbors to the immediate neighbors.
    Currently, gathering information from the immediate neighbors (as in Eq. ([14](#S4.E14
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey"))) and following the framework of
    Eqs. ([21](#S4.E21 "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))([22](#S4.E22 "In
    4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey"))([27](#S4.E27 "In 4.1.4 Frameworks ‣ 4.1
    Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs:
    A Survey")) are the most common choices for graph convolution operations.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，卷积操作已经从谱域发展到空间域，从多步邻居发展到直接邻居。目前，从直接邻居中收集信息（如在 Eq. ([14](#S4.E14 "In 4.1.2
    The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey"))) 并遵循 Eqs. ([21](#S4.E21 "In 4.1.4 Frameworks
    ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey"))([22](#S4.E22 "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))([27](#S4.E27
    "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")) 的框架是图卷积操作中最常见的选择。'
- en: 4.2 Readout Operations
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 读出操作
- en: Using graph convolution operations, useful node features can be learned to solve
    many node-focused tasks. However, to tackle graph-focused tasks, node information
    needs to be aggregated to form a graph-level representation. In the literature,
    such procedures are usually called the readout operations⁷⁷7Readout operations
    are also related to graph coarsening, i.e., reducing a large graph to a smaller
    graph, because a graph-level representation can be obtained by coarsening the
    graph to a single node. Some papers use these two terms interchangeably.. Based
    on a regular and local neighborhood, standard CNNs conduct multiple stride convolutions
    or poolings to gradually reduce the resolution. Since graphs lack a grid structure,
    these existing methods cannot be used directly.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图卷积操作，可以学习到有用的节点特征来解决许多以节点为重点的任务。然而，要解决以图为重点的任务，节点信息需要被聚合以形成图级别的表示。在文献中，这种过程通常称为读出操作⁷⁷7读出操作也与图粗化有关，即将大图缩减为小图，因为通过将图粗化为一个节点可以获得图级别的表示。一些论文将这两个术语互换使用..
    基于常规和局部邻域，标准CNN进行多次步幅卷积或池化操作以逐渐降低分辨率。由于图缺乏网格结构，这些现有方法不能直接使用。
- en: Order invariance. A critical requirement for the graph readout operations is
    that the operation should be invariant to the node order, i.e., if we change the
    indices of nodes and edges using a bijective function between two node sets, the
    representation of the entire graph should not change. For example, whether a drug
    can treat certain diseases depends on its inherent structure; thus, we should
    get identical results if we represent the drug using different node indices. Note
    that because this problem is related to the graph isomorphism problem, of which
    the best-known algorithm is quasipolynomial [[80](#bib.bib80)], we only can find
    a function that is order-invariant but not vice versa in a polynomial time, i.e.,
    even two structurally different graphs may have the same representation.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序不变性。图读出操作的一个关键要求是操作应对节点顺序不变，即，如果我们使用一个双射函数在两个节点集之间更改节点和边的索引，则整个图的表示不应改变。例如，药物是否能治疗某些疾病取决于其固有结构；因此，如果我们使用不同的节点索引表示药物，则应得到相同的结果。注意，由于这个问题与图同构问题有关，而已知的最佳算法是准多项式
    [[80](#bib.bib80)]，我们只能在多项式时间内找到一个顺序不变的函数，但不能找到一个反之亦然的函数，即即使两个结构不同的图也可能具有相同的表示。
- en: 4.2.1 Statistics
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 统计
- en: The most basic order-invariant operations involve simple statistics such as
    summation, averaging or max-pooling [[46](#bib.bib46), [50](#bib.bib50)], i.e.,
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的顺序不变操作包括简单的统计，如求和、平均或最大池化 [[46](#bib.bib46), [50](#bib.bib50)]，即，
- en: '|  | $\mathbf{h}_{G}=\sum_{i=1}^{N}\mathbf{h}^{L}_{i}\;\text{or}\;\mathbf{h}_{G}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{h}^{L}_{i}\;\text{or}\;\mathbf{h}_{G}=\max\left\{\mathbf{h}^{L}_{i},\forall
    i\right\},$ |  | (28) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}_{G}=\sum_{i=1}^{N}\mathbf{h}^{L}_{i}\;\text{或}\;\mathbf{h}_{G}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{h}^{L}_{i}\;\text{或}\;\mathbf{h}_{G}=\max\left\{\mathbf{h}^{L}_{i},\forall
    i\right\},$ |  | (28) |'
- en: where $\mathbf{h}_{G}$ is the representation of the graph $G$ and $\mathbf{h}^{L}_{i}$
    is the representation of node $v_{i}$ in the final layer $L$. However, such first-moment
    statistics may not be sufficiently representative to distinguish different graphs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{h}_{G}$ 是图 $G$ 的表示，$\mathbf{h}^{L}_{i}$ 是最终层 $L$ 中节点 $v_{i}$ 的表示。然而，这种一阶统计可能不足以区分不同的图。
- en: Kearnes et al. [[55](#bib.bib55)] suggested considering the distribution of
    node representations by using fuzzy histograms [[81](#bib.bib81)]. The basic idea
    behind fuzzy histograms is to construct several “histogram bins” and then calculate
    the memberships of $\mathbf{h}^{L}_{i}$ to these bins, i.e., by regarding node
    representations as samples and matching them to some pre-defined templates, and
    finally return the concatenation of the final histograms. In this way, nodes with
    the same sum/average/maximum but with different distributions can be distinguished.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Kearnes 等人 [[55](#bib.bib55)] 建议通过使用模糊直方图 [[81](#bib.bib81)] 来考虑节点表示的分布。模糊直方图的基本思想是构建几个“直方图箱”，然后计算
    $\mathbf{h}^{L}_{i}$ 对这些箱的隶属度，即将节点表示视为样本，并将其匹配到一些预定义的模板上，最后返回最终直方图的拼接。通过这种方式，可以区分具有相同和/平均/最大值但分布不同的节点。
- en: Another commonly used approach for aggregating node representation is to add
    a fully connected (FC) layer as the final layer [[40](#bib.bib40)], i.e.,
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常用的节点表示聚合方法是添加一个全连接（FC）层作为最终层 [[40](#bib.bib40)]，即，
- en: '|  | $\mathbf{h}_{G}=\rho\left(\left[\mathbf{H}^{L}\right]\mathbf{\Theta}_{FC}\right),$
    |  | (29) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}_{G}=\rho\left(\left[\mathbf{H}^{L}\right]\mathbf{\Theta}_{FC}\right),$
    |  | (29) |'
- en: 'where $\left[\mathbf{H}^{L}\right]\in\mathbb{R}^{Nf_{L}}$ is the concatenation
    of the final node representation $\mathbf{H}^{L}$, $\mathbf{\Theta}_{FC}\in\mathbb{R}^{Nf_{L}\times
    f_{\text{output}}}$ are parameters, and $f_{\text{output}}$ is the dimensionality
    of the output. Eq. ([29](#S4.E29 "In 4.2.1 Statistics ‣ 4.2 Readout Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) can be
    regarded as a weighted sum of node-level features. One advantage is that the model
    can learn different weights for different nodes; however, this ability comes at
    the cost of being unable to guarantee order invariance.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\left[\mathbf{H}^{L}\right]\in\mathbb{R}^{Nf_{L}}$ 是最终节点表示 $\mathbf{H}^{L}$
    的拼接，$\mathbf{\Theta}_{FC}\in\mathbb{R}^{Nf_{L}\times f_{\text{output}}}$ 是参数，$f_{\text{output}}$
    是输出的维度。公式 ([29](#S4.E29 "In 4.2.1 Statistics ‣ 4.2 Readout Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")) 可以看作是节点级特征的加权和。一个优点是模型可以为不同的节点学习不同的权重；然而，这种能力的代价是无法保证顺序不变性。'
- en: 4.2.2 Hierarchical Clustering
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 层次聚类
- en: 'Rather than a dichotomy between node and graph level structures, graphs are
    known to exhibit rich hierarchical structures [[82](#bib.bib82)], which can be
    explored by hierarchical clustering methods as shown in Figure [4](#S4.F4 "Figure
    4 ‣ 4.2.2 Hierarchical Clustering ‣ 4.2 Readout Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey"). For example, a density-based agglomerative
    clustering [[83](#bib.bib83)] was used in Bruna et al. [[40](#bib.bib40)] and
    multi-resolution spectral clustering [[84](#bib.bib84)] was used in Henaff et al. [[41](#bib.bib41)].
    ChebNet [[42](#bib.bib42)] and MoNet [[54](#bib.bib54)] adopted another greedy
    hierarchical clustering algorithm, Graclus [[85](#bib.bib85)], to merge two nodes
    at a time, along with a fast pooling method to rearrange the nodes into a balanced
    binary tree. ECC [[63](#bib.bib63)] adopted another hierarchical clustering method
    by performing eigendecomposition [[86](#bib.bib86)]. However, these hierarchical
    clustering methods are all independent of the graph convolutions (i.e., they can
    be performed as a preprocessing step and are not trained in an end-to-end fashion).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 与节点和图层次结构之间的二分法不同，图形被认为展现了丰富的层次结构 [[82](#bib.bib82)]，可以通过如图 [4](#S4.F4 "图 4
    ‣ 4.2.2 层次聚类 ‣ 4.2 读取操作 ‣ 4 图卷积网络 ‣ 图上的深度学习：综述") 所示的层次聚类方法进行探索。例如，Bruna 等人 [[40](#bib.bib40)]
    使用了基于密度的聚合聚类 [[83](#bib.bib83)]，Henaff 等人 [[41](#bib.bib41)] 使用了多分辨率谱聚类 [[84](#bib.bib84)]。ChebNet
    [[42](#bib.bib42)] 和 MoNet [[54](#bib.bib54)] 采用了另一种贪婪层次聚类算法 Graclus [[85](#bib.bib85)]，该算法每次合并两个节点，并配有快速池化方法将节点重新排列成平衡的二叉树。ECC
    [[63](#bib.bib63)] 采用了另一种层次聚类方法，通过进行特征分解 [[86](#bib.bib86)]。然而，这些层次聚类方法都与图卷积无关（即，它们可以作为预处理步骤进行，而不是以端到端的方式训练）。
- en: '![Refer to caption](img/eaf892d57e3d64a555f9f149a708169e.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eaf892d57e3d64a555f9f149a708169e.png)'
- en: 'Figure 4: An example of performing a hierarchical clustering algorithm. Reprinted
    from [[56](#bib.bib56)] with permission.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：执行层次聚类算法的示例。经 [[56](#bib.bib56)] 许可转载。
- en: 'To solve that problem, DiffPool [[56](#bib.bib56)] proposed a differentiable
    hierarchical clustering algorithm jointly trained with the graph convolutions.
    Specifically, the authors proposed learning a soft cluster assignment matrix in
    each layer using the hidden representations as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，DiffPool [[56](#bib.bib56)] 提出了一个与图卷积共同训练的可微层次聚类算法。具体来说，作者提出了在每一层使用隐藏表示来学习软聚类分配矩阵，如下所示：
- en: '|  | $\mathbf{S}^{l}=\mathcal{F}\left(\mathbf{A}^{l},\mathbf{H}^{l}\right),$
    |  | (30) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{S}^{l}=\mathcal{F}\left(\mathbf{A}^{l},\mathbf{H}^{l}\right),$
    |  | (30) |'
- en: 'where $\mathbf{S}^{l}\in\mathbb{R}^{N_{l}\times N_{l+1}}$ is the cluster assignment
    matrix, $N_{l}$ is the number of clusters in the layer $l$ and $\mathcal{F}(\cdot)$
    is a function to be learned. Then, the node representations and the new adjacency
    matrix for this “coarsened” graph can be obtained by taking the average according
    to $\mathbf{S}^{l}$ as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{S}^{l}\in\mathbb{R}^{N_{l}\times N_{l+1}}$ 是聚类分配矩阵，$N_{l}$ 是第 $l$
    层的聚类数量，$\mathcal{F}(\cdot)$ 是待学习的函数。然后，可以通过根据 $\mathbf{S}^{l}$ 取平均来获得这个“粗化”图的节点表示和新的邻接矩阵，如下所示：
- en: '|  | $\mathbf{H}^{l+1}=(\mathbf{S}^{l})^{T}\hat{\mathbf{H}}^{l+1},\mathbf{A}^{l+1}=(\mathbf{S}^{l})^{T}\mathbf{A}^{l}\mathbf{S}^{l},$
    |  | (31) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{l+1}=(\mathbf{S}^{l})^{T}\hat{\mathbf{H}}^{l+1},\mathbf{A}^{l+1}=(\mathbf{S}^{l})^{T}\mathbf{A}^{l}\mathbf{S}^{l},$
    |  | (31) |'
- en: where $\hat{\mathbf{H}}^{l+1}$ is obtained by applying a graph convolution layer
    to $\mathbf{H}^{l}$, i.e., coarsening the graph from $N_{l}$ nodes to $N_{l+1}$
    nodes in each layer after the convolution operation. The initial number of nodes
    is $N_{0}=N$ and the last layer is $N_{L}=1$, i.e., a single node that represents
    the entire graph. Because the cluster assignment operation is soft, the connections
    between clusters are not sparse; thus the time complexity of the method is $O(N^{2})$
    in principle.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{\mathbf{H}}^{l+1}$ 是通过对 $\mathbf{H}^{l}$ 应用图卷积层得到的，即在卷积操作后，将图从 $N_{l}$
    节点粗化到每层的 $N_{l+1}$ 节点。初始节点数量为 $N_{0}=N$，最后一层为 $N_{L}=1$，即表示整个图的单个节点。由于聚类分配操作是软性的，聚类之间的连接不是稀疏的，因此该方法的时间复杂度原则上是
    $O(N^{2})$。
- en: 4.2.3 Imposing Orders and Others
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 强加顺序及其他
- en: 'As mentioned in Section [4.1.3](#S4.SS1.SSS3 "4.1.3 The Aspect of Multiple
    Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey"), PATCHY-SAN [[47](#bib.bib47)] and SortPooling [[49](#bib.bib49)]
    took the idea of imposing a node order and then resorted to standard 1-D pooling
    as in CNNs. Whether these methods can preserve order invariance depends on how
    the order is imposed, which is another research field that we refer readers to [[87](#bib.bib87)]
    for a survey. However, whether imposing a node order is a natural choice for graphs
    and if so, what the best node orders are constitute still on-going research topics.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[4.1.3节](#S4.SS1.SSS3 "4.1.3 多图的方面 ‣ 4.1 卷积操作 ‣ 4 图卷积网络 ‣ 深度学习图卷积：综述")中提到的，PATCHY-SAN[[47](#bib.bib47)]和SortPooling[[49](#bib.bib49)]借鉴了施加节点顺序的想法，然后采用了类似于CNNs中的标准1-D池化。是否这些方法能够保持顺序不变取决于顺序的施加方式，这是另一个研究领域，我们参考读者[[87](#bib.bib87)]的综述。然而，施加节点顺序是否是图的自然选择，以及如果是的话，最佳节点顺序是什么，仍然是正在进行的研究课题。
- en: In addition to the aforementioned methods, there are some heuristics. In GNNs [[23](#bib.bib23)],
    the authors suggested adding a special node connected to all nodes to represent
    the entire graph. Similarly, GNs [[9](#bib.bib9)] proposed to directly learn the
    representation of the entire graph by receiving messages from all nodes and edges.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述方法外，还有一些启发式方法。在GNNs[[23](#bib.bib23)]中，作者建议添加一个与所有节点相连的特殊节点来表示整个图。同样，GNs[[9](#bib.bib9)]提出通过接收来自所有节点和边的消息，直接学习整个图的表示。
- en: MPNNs adopted set2set [[88](#bib.bib88)], a modification of the seq2seq model.
    Specifically, set2set uses a “Read-Process-and-Write” model that receives all
    inputs simultaneously, computes internal memories using an attention mechanism
    and an LSTM, and then writes the outputs. Unlike seq2seq which is order-sensitive,
    set2set is invariant to the input order.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: MPNNs采用了set2set[[88](#bib.bib88)]，这是seq2seq模型的一种改进。具体而言，set2set使用了一个“读-处理-写”模型，该模型同时接收所有输入，利用注意力机制和LSTM计算内部记忆，然后写入输出。与顺序敏感的seq2seq不同，set2set对输入顺序不变。
- en: 4.2.4 Summary
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 总结
- en: In short, statistics such as averaging or summation are the most simple readout
    operations, while hierarchical clustering algorithms jointly trained with graph
    convolutions are more advanced but are also more sophisticated. Other methods
    such as adding a pseudo node or imposing a node order have also been investigated.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，诸如平均或求和等统计方法是最简单的读出操作，而与图卷积联合训练的层次聚类算法则更为先进，但也更为复杂。其他方法如添加伪节点或施加节点顺序也已被研究。
- en: 4.3 Improvements and Discussions
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 改进与讨论
- en: Many techniques have been introduced to further improve GCNs. Note that some
    of these methods are general and could be applied to other deep learning models
    on graphs as well.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 已经引入了许多技术来进一步改进GCNs。需要注意的是，这些方法中的一些是通用的，也可以应用于其他图上的深度学习模型。
- en: 4.3.1 Attention Mechanism
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 注意力机制
- en: 'In the aforementioned GCNs, the node neighborhoods are aggregated with equal
    or pre-defined weights. However, the influences of neighbors can vary greatly;
    thus, they should be learned during training rather than being predetermined.
    Inspired by the attention mechanism [[89](#bib.bib89)], graph attention network
    (GAT) [[57](#bib.bib57)] introduces the attention mechanism into GCNs by modifying
    the convolution operation in Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect
    ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述的GCNs中，节点邻域的聚合是通过相等或预定义的权重进行的。然而，邻居的影响可能差异很大，因此这些影响应该在训练过程中学习而非预先确定。受注意力机制的启发[[89](#bib.bib89)]，图注意力网络（GAT）[[57](#bib.bib57)]通过修改公式中的卷积操作[[13](#S4.E13
    "在4.1.2 效率方面 ‣ 4.1 卷积操作 ‣ 4 图卷积网络 ‣ 深度学习图卷积：综述")]将注意力机制引入GCNs，如下所示：
- en: '|  | $\mathbf{h}^{l+1}_{i}=\rho\left(\sum\nolimits_{j\in\hat{\mathcal{N}}(i)}\alpha_{ij}^{l}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right),$
    |  | (32) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}^{l+1}_{i}=\rho\left(\sum\nolimits_{j\in\hat{\mathcal{N}}(i)}\alpha_{ij}^{l}\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right),$
    |  | (32) |'
- en: 'where $\alpha_{ij}^{l}$ is node $v_{i}$’s attention to node $v_{j}$ in the
    $l^{th}$ layer:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha_{ij}^{l}$是节点$v_{i}$在$l^{th}$层对节点$v_{j}$的注意力：
- en: '|  | $\alpha_{ij}^{l}=\frac{\exp\left(\text{LeakyReLU}\left(\mathcal{F}\left(\mathbf{h}^{l}_{i}\mathbf{\Theta}^{l},\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right)\right)\right)}{\sum_{k\in\hat{\mathcal{N}}(i)}\exp\left(\text{LeakyReLU}\left(\mathcal{F}\left(\mathbf{h}^{l}_{i}\mathbf{\Theta}^{l},\mathbf{h}^{l}_{k}\mathbf{\Theta}^{l}\right)\right)\right)},$
    |  | (33) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{ij}^{l}=\frac{\exp\left(\text{LeakyReLU}\left(\mathcal{F}\left(\mathbf{h}^{l}_{i}\mathbf{\Theta}^{l},\mathbf{h}^{l}_{j}\mathbf{\Theta}^{l}\right)\right)\right)}{\sum_{k\in\hat{\mathcal{N}}(i)}\exp\left(\text{LeakyReLU}\left(\mathcal{F}\left(\mathbf{h}^{l}_{i}\mathbf{\Theta}^{l},\mathbf{h}^{l}_{k}\mathbf{\Theta}^{l}\right)\right)\right)},$
    |  | (33) |'
- en: 'where $\mathcal{F}(\cdot,\cdot)$ is another function to be learned such as
    a multi-layer perceptron (MLP). To improve model capacity and stability, the authors
    also suggested using multiple independent attentions and concatenating the results,
    i.e., the multi-head attention mechanism [[89](#bib.bib89)] as illustrated in
    Figure [5](#S4.F5 "Figure 5 ‣ 4.3.1 Attention Mechanism ‣ 4.3 Improvements and
    Discussions ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey").
    GaAN [[58](#bib.bib58)] further proposed learning different weights for different
    heads and applied such a method to the traffic forecasting problem.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\mathcal{F}(\cdot,\cdot)$ 是另一个需要学习的函数，例如多层感知机（MLP）。为了提高模型的容量和稳定性，作者还建议使用多个独立的注意力并将结果进行拼接，即多头注意力机制 [[89](#bib.bib89)]，如图 [5](#S4.F5
    "Figure 5 ‣ 4.3.1 Attention Mechanism ‣ 4.3 Improvements and Discussions ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey") 所示。GaAN [[58](#bib.bib58)]
    进一步提出了对不同头学习不同权重的方法，并将这种方法应用于交通预测问题。'
- en: HAN [[59](#bib.bib59)] proposed a two-level attention mechanism, i.e., a node-level
    and a semantic-level attention mechanism, for heterogeneous graphs. Specifically,
    the node-level attention mechanism was similar to a GAT, but also considerd node
    types; therefore, it could assign different weights to aggregating meta-path-based
    neighbors. The semantic-level attention then learned the importance of different
    meta-paths and outputed the final results.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: HAN [[59](#bib.bib59)] 提出了一个两级注意力机制，即节点级和语义级注意力机制，用于异构图。具体来说，节点级注意力机制类似于 GAT，但还考虑了节点类型，因此可以为基于元路径的邻居分配不同的权重。语义级注意力机制则学习不同元路径的重要性并输出最终结果。
- en: '![Refer to caption](img/ed0729d23fce33a1585746df5caacb26.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ed0729d23fce33a1585746df5caacb26.png)'
- en: 'Figure 5: An illustration of the multi-head attention mechanism proposed in
    GAT [[57](#bib.bib57)] (reprinted with permission). Each color denotes an independent
    attention vector.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：GAT [[57](#bib.bib57)] 提出的多头注意力机制的示意图（经许可转载）。每种颜色表示一个独立的注意力向量。
- en: 4.3.2 Residual and Jumping Connections
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 残差和跳跃连接
- en: 'Many researches have observed that the most suitable depth for the existing
    GCNs is often very limited, e.g., 2 or 3 layers. This problem is potentially due
    to the practical difficulties involved in training deep GCNs or the over-smoothing
    problem, i.e., all nodes in deeper layers have the same representation [[70](#bib.bib70),
    [62](#bib.bib62)]. To remedy this problem, residual connections similar to ResNet [[90](#bib.bib90)]
    can be added to GCNs. For example, Kipf and Welling [[43](#bib.bib43)] added residual
    connections into Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"))
    as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '许多研究观察到，现有 GCNs 的最适合深度通常非常有限，例如 2 层或 3 层。这个问题可能是由于训练深层 GCNs 的实际困难或过度平滑问题，即所有节点在更深层次上具有相同的表示 [[70](#bib.bib70),
    [62](#bib.bib62)]。为了解决这个问题，可以将类似 ResNet [[90](#bib.bib90)] 的残差连接添加到 GCNs 中。例如，Kipf
    和 Welling [[43](#bib.bib43)] 将残差连接添加到 Eq. ([14](#S4.E14 "In 4.1.2 The Efficiency
    Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) 中，如下所示：'
- en: '|  | $\mathbf{H}^{l+1}=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right)+\mathbf{H}^{l}.$
    |  | (34) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{l+1}=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}\mathbf{\Theta}^{l}\right)+\mathbf{H}^{l}.$
    |  | (34) |'
- en: They showed experimentally that adding such residual connections could allow
    the depth of the network to increase, which is similar to the results of ResNet.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 他们通过实验证明，添加这样的残差连接可以使网络的深度增加，这与 ResNet 的结果类似。
- en: 'Column network (CLN) [[60](#bib.bib60)] adopted a similar idea by using the
    following residual connections with learnable weights:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Column network (CLN) [[60](#bib.bib60)] 采用了类似的思想，通过使用以下具有可学习权重的残差连接：
- en: '|  | $\mathbf{h}^{l+1}_{i}=\bm{\alpha}^{l}_{i}\odot\widetilde{\mathbf{h}}^{l+1}_{i}+(1-\bm{\alpha}^{l}_{i})\odot\mathbf{h}^{l}_{i},$
    |  | (35) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}^{l+1}_{i}=\bm{\alpha}^{l}_{i}\odot\widetilde{\mathbf{h}}^{l+1}_{i}+(1-\bm{\alpha}^{l}_{i})\odot\mathbf{h}^{l}_{i},$
    |  | (35) |'
- en: 'where $\widetilde{\mathbf{h}}^{l+1}_{i}$ is calculated similar to Eq. ([14](#S4.E14
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) and $\bm{\alpha}^{l}_{i}$ is a
    set of weights calculated as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\widetilde{\mathbf{h}}^{l+1}_{i}$ 是类似于方程 ([14](#S4.E14 "In 4.1.2 The Efficiency
    Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) 计算的，$\bm{\alpha}^{l}_{i}$ 是一组权重，其计算方式如下：'
- en: '|  | $\bm{\alpha}^{l}_{i}=\rho\left(\mathbf{b}^{l}_{\alpha}+\mathbf{\Theta}_{\alpha}^{l}\mathbf{h}_{i}^{l}+\mathbf{\Theta}_{\alpha}^{{}^{\prime}l}\sum\nolimits_{j\in\mathcal{N}(i)}\mathbf{h}_{j}^{l}\right),$
    |  | (36) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{\alpha}^{l}_{i}=\rho\left(\mathbf{b}^{l}_{\alpha}+\mathbf{\Theta}_{\alpha}^{l}\mathbf{h}_{i}^{l}+\mathbf{\Theta}_{\alpha}^{{}^{\prime}l}\sum\nolimits_{j\in\mathcal{N}(i)}\mathbf{h}_{j}^{l}\right),$
    |  | (36) |'
- en: 'where $\mathbf{b}^{l}_{\alpha},\mathbf{\Theta}_{\alpha}^{l},\mathbf{\Theta}_{\alpha}^{{}^{\prime}l}$
    are parameters. Note that Eq. ([35](#S4.E35 "In 4.3.2 Residual and Jumping Connections
    ‣ 4.3 Improvements and Discussions ‣ 4 Graph Convolutional Networks ‣ Deep Learning
    on Graphs: A Survey")) is very similar to the GRU as in GGS-NNs [[24](#bib.bib24)].
    The differences are that in a CLN, the superscripts denote the number of layers,
    and different layers contain different parameters, while in GGS-NNs, the superscript
    denotes the pseudo time and a single set of parameters is used across time steps.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{b}^{l}_{\alpha},\mathbf{\Theta}_{\alpha}^{l},\mathbf{\Theta}_{\alpha}^{{}^{\prime}l}$
    是参数。注意，方程 ([35](#S4.E35 "In 4.3.2 Residual and Jumping Connections ‣ 4.3 Improvements
    and Discussions ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A
    Survey")) 与 GGS-NNs [[24](#bib.bib24)] 中的 GRU 非常相似。不同之处在于，CLN 中的上标表示层数，不同层包含不同的参数，而
    GGS-NNs 中的上标表示伪时间，并且在时间步长之间使用一组参数。'
- en: 'Inspired by personalized PageRank, PPNP [[61](#bib.bib61)] defined graph convolutions
    with teleportation to the initial layer:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 受到个性化 PageRank 的启发，PPNP [[61](#bib.bib61)] 定义了带有传送到初始层的图卷积：
- en: '|  | $\mathbf{H}^{l+1}=(1-\alpha)\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}+\alpha\mathbf{H}^{0},$
    |  | (37) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{l+1}=(1-\alpha)\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{l}+\alpha\mathbf{H}^{0},$
    |  | (37) |'
- en: where $\mathbf{H}_{0}=\mathcal{F}_{\theta}(\mathbf{F}^{V})$ and $\alpha$ is
    a hyper-parameter. Note that all the parameters are in $\mathcal{F}_{\theta}(\cdot)$
    rather than in the graph convolutions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{H}_{0}=\mathcal{F}_{\theta}(\mathbf{F}^{V})$ 且 $\alpha$ 是一个超参数。注意，所有的参数都在
    $\mathcal{F}_{\theta}(\cdot)$ 中，而不是在图卷积中。
- en: 'Jumping knowledge networks (JK-Nets) [[62](#bib.bib62)] proposed another architecture
    to connect the last layer of the network with all the lower hidden layers, i.e.,
    by “jumping” all the representations to the final output, as illustrated in Figure [6](#S4.F6
    "Figure 6 ‣ 4.3.2 Residual and Jumping Connections ‣ 4.3 Improvements and Discussions
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"). In this
    way, the model can learn to selectively exploit information from different layers.
    Formally, JK-Nets was formulated as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '跳跃知识网络（JK-Nets）[[62](#bib.bib62)] 提出了另一种架构，通过将网络的最后一层与所有较低的隐藏层连接起来，即通过“跳跃”所有表示到最终输出，如图
    [6](#S4.F6 "Figure 6 ‣ 4.3.2 Residual and Jumping Connections ‣ 4.3 Improvements
    and Discussions ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A
    Survey") 所示。这样，模型可以学会选择性地利用来自不同层的信息。形式上，JK-Nets 被表述如下：'
- en: '|  | $\mathbf{h}_{i}^{\text{final}}=\text{AGGREGATE}(\mathbf{h}_{i}^{0},\mathbf{h}_{i}^{1},...,\mathbf{h}_{i}^{L}),$
    |  | (38) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}_{i}^{\text{final}}=\text{AGGREGATE}(\mathbf{h}_{i}^{0},\mathbf{h}_{i}^{1},...,\mathbf{h}_{i}^{L}),$
    |  | (38) |'
- en: 'where $\mathbf{h}_{i}^{\text{final}}$ is the final representation for node
    $v_{i}$, AGGREGATE$(\cdot)$ is the aggregating function, and $L$ is the number
    of hidden layers. JK-Nets used three aggregating functions similar to GraphSAGE [[53](#bib.bib53)]:
    concatenation, max-pooling, and the LSTM attention. The experimental results showed
    that adding jump connections could improve the performance of multiple GCNs.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{h}_{i}^{\text{final}}$ 是节点 $v_{i}$ 的最终表示，AGGREGATE$(\cdot)$ 是聚合函数，$L$
    是隐藏层的数量。JK-Nets 使用了三种类似于 GraphSAGE [[53](#bib.bib53)] 的聚合函数：连接、最大池化和 LSTM 注意力。实验结果表明，添加跳跃连接可以提高多个
    GCN 的性能。
- en: '![Refer to caption](img/f01752a8f1cbff94a422cef1749168f0.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f01752a8f1cbff94a422cef1749168f0.png)'
- en: 'Figure 6: Jumping knowledge networks proposed in [[62](#bib.bib62)] in which
    the last layer is connected to all the other layers to selectively exploit different
    information from different layers. GC denotes graph convolutions. Reprinted with
    permission.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在 [[62](#bib.bib62)] 中提出的跳跃知识网络，其中最后一层与所有其他层连接，以选择性地利用来自不同层的信息。GC 表示图卷积。经授权转载。
- en: 4.3.3 Edge Features
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 边特征
- en: 'The aforementioned GCNs mostly focus on utilizing node features and graph structures.
    In this subsection, we briefly discuss how to use another important source of
    information: the edge features.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 GCN 主要集中在利用节点特征和图结构。在本小节中，我们简要讨论如何使用另一个重要的信息来源：边特征。
- en: For simple edge features with discrete values such as the edge type, a straightforward
    method is to train different parameters for different edge types and aggregate
    the results. For example, Neural FPs [[46](#bib.bib46)] trained different parameters
    for nodes with different degrees, which corresponds to the implicit edge feature
    of bond types in a molecular graph, and then summed over the results. CLN [[60](#bib.bib60)]
    trained different parameters for different edge types in a heterogeneous graph
    and averaged the results. Edge-conditioned convolution (ECC) [[63](#bib.bib63)]
    also trained different parameters based on edge types and applied them to graph
    classification. Relational GCNs (R-GCNs) [[64](#bib.bib64)] adopted a similar
    idea for knowledge graphs by training different weights for different relation
    types. However, these methods are suitable only for a limited number of discrete
    edge features.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散值的简单边特征，如边的类型，一种直接的方法是为不同的边类型训练不同的参数，并聚合结果。例如，Neural FPs [[46](#bib.bib46)]
    为具有不同度数的节点训练了不同的参数，这对应于分子图中的隐式边特征（键类型），然后对结果进行求和。CLN [[60](#bib.bib60)] 为异构图中的不同边类型训练了不同的参数，并对结果进行了平均。边条件卷积
    (ECC) [[63](#bib.bib63)] 也基于边类型训练了不同的参数，并将其应用于图分类。关系 GCNs (R-GCNs) [[64](#bib.bib64)]
    为知识图谱采用了类似的思想，通过为不同的关系类型训练不同的权重。然而，这些方法仅适用于有限数量的离散边特征。
- en: DCNN [[50](#bib.bib50)] proposed another method to convert each edge into a
    node connected to the head and tail node of that edge. After this conversion,
    edge features can be treated as node features.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: DCNN [[50](#bib.bib50)] 提出了另一种方法，将每条边转换为连接到该边头节点和尾节点的节点。经过这种转换，边特征可以被视为节点特征。
- en: 'LGCN [[65](#bib.bib65)] constructed a line graph $\mathbf{B}\in\mathbb{R}^{2M\times
    2M}$ to incorporate edge features as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: LGCN [[65](#bib.bib65)] 构建了一个线图 $\mathbf{B}\in\mathbb{R}^{2M\times 2M}$ 来整合边特征，如下所示：
- en: '|  | $\mathbf{B}_{i\rightarrow j,i^{\prime}\rightarrow j^{\prime}}=\left\{\begin{aligned}
    &amp;1\quad\text{if}\;j=i^{\prime}\;\text{and}\;j^{\prime}\neq i,\\ &amp;0\quad\text{otherwise}.\end{aligned}\right.$
    |  | (39) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{B}_{i\rightarrow j,i^{\prime}\rightarrow j^{\prime}}=\left\{\begin{aligned}
    &1\quad\text{if}\;j=i^{\prime}\;\text{and}\;j^{\prime}\neq i,\\ &0\quad\text{otherwise}.\end{aligned}\right.$
    |  | (39) |'
- en: 'In other words, nodes in the line graph are directed edges in the original
    graph, and two nodes in the line graph are connected if information can flow through
    their corresponding edges in the original graph. Then, LGCN adopted two GCNs:
    one on the original graph and one on the line graph.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，线图中的节点是原图中的有向边，如果信息可以通过原图中的相应边流动，则线图中的两个节点相连。然后，LGCN 采用了两个 GCN：一个在原图上，另一个在线图上。
- en: 'Kearnes et al. [[55](#bib.bib55)] proposed an architecture using a “weave module”.
    Specifically, they learned representations for both nodes and edges and exchanged
    information between them in each weave module using four different functions:
    node-to-node (NN), node-to-edge (NE), edge-to-edge (EE) and edge-to-node (EN):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Kearnes 等人 [[55](#bib.bib55)] 提出了一个使用“编织模块”的架构。具体而言，他们为节点和边学习了表示，并在每个编织模块中使用四种不同的函数交换信息：节点到节点
    (NN)、节点到边 (NE)、边到边 (EE) 和边到节点 (EN)：
- en: '|  | <math   alttext="\begin{gathered}\mathbf{h}^{l^{\prime}}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{0}_{i},\mathbf{h}^{1}_{i},...,\mathbf{h}^{l}_{i}),\mathbf{h}^{l^{\prime\prime}}_{i}=\mathcal{F}_{EN}(\{\mathbf{e}^{l}_{ij}&#124;j\in\mathcal{N}(i)\})\\
    \mathbf{e}^{l^{\prime}}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{0}_{ij},\mathbf{e}^{1}_{ij},...,\mathbf{e}^{l}_{ij}),\mathbf{e}^{l^{\prime\prime}}_{ij}=\mathcal{F}_{NE}(\mathbf{h}^{l}_{i},\mathbf{h}^{l}_{j})\\'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{gathered}\mathbf{h}^{l^{\prime}}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{0}_{i},\mathbf{h}^{1}_{i},...,\mathbf{h}^{l}_{i}),\mathbf{h}^{l^{\prime\prime}}_{i}=\mathcal{F}_{EN}(\{\mathbf{e}^{l}_{ij}&#124;j\in\mathcal{N}(i)\})\\
    \mathbf{e}^{l^{\prime}}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{0}_{ij},\mathbf{e}^{1}_{ij},...,\mathbf{e}^{l}_{ij}),\mathbf{e}^{l^{\prime\prime}}_{ij}=\mathcal{F}_{NE}(\mathbf{h}^{l}_{i},\mathbf{h}^{l}_{j})\\'
- en: \mathbf{h}^{l+1}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{l^{\prime}}_{i},\mathbf{h}^{l^{\prime\prime}}_{i}),\mathbf{e}^{l+1}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{l^{\prime}}_{ij},\mathbf{e}^{l^{\prime\prime}}_{ij}),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><mrow ><msubsup ><mi  >𝐡</mi><mi >i</mi><msup ><mi  >l</mi><mo >′</mo></msup></msubsup><mo
    >=</mo><mrow ><msub ><mi >ℱ</mi><mrow ><mi  >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >N</mi></mrow></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msubsup ><mi >𝐡</mi><mi >i</mi><mn >0</mn></msubsup><mo >,</mo><msubsup
    ><mi >𝐡</mi><mi >i</mi><mn >1</mn></msubsup><mo >,</mo><mi mathvariant="normal"  >…</mi><mo
    >,</mo><msubsup ><mi >𝐡</mi><mi >i</mi><mi >l</mi></msubsup><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐡</mi><mi >i</mi><msup
    ><mi >l</mi><mo >′′</mo></msup></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow
    ><mi >E</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >N</mi></mrow></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mo
    stretchy="false" >{</mo><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo lspace="0em" rspace="0em"
    >&#124;</mo><mrow ><mi >j</mi><mo >∈</mo><mrow ><mi >𝒩</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mi >i</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    stretchy="false" >}</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow ><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >j</mi></mrow><msup ><mi  >l</mi><mo >′</mo></msup></msubsup><mo >=</mo><mrow
    ><msub ><mi >ℱ</mi><mrow ><mi >E</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >E</mi></mrow></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >j</mi></mrow><mn >0</mn></msubsup><mo >,</mo><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow><mn >1</mn></msubsup><mo
    >,</mo><mi mathvariant="normal" >…</mi><mo >,</mo><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐞</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><msup ><mi
    >l</mi><mo >′′</mo></msup></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow
    ><mi >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >E</mi></mrow></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi
    >𝐡</mi><mi >i</mi><mi >l</mi></msubsup><mo >,</mo><msubsup ><mi >𝐡</mi><mi >j</mi><mi
    >l</mi></msubsup><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow ><mrow ><msubsup ><mi >𝐡</mi><mi >i</mi><mrow ><mi  >l</mi><mo
    >+</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow ><mi
    >N</mi><mo lspace="0em" rspace="0em" >​</mo><mi >N</mi></mrow></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >𝐡</mi><mi
    >i</mi><msup ><mi  >l</mi><mo >′</mo></msup></msubsup><mo >,</mo><msubsup ><mi
    >𝐡</mi><mi >i</mi><msup ><mi >l</mi><mo >′′</mo></msup></msubsup><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><mrow ><mi >l</mi><mo
    >+</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow ><mi
    >E</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >E</mi></mrow></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >𝐞</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><msup ><mi
    >l</mi><mo >′</mo></msup></msubsup><mo >,</mo><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow><msup ><mi >l</mi><mo
    >′′</mo></msup></msubsup><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" ><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><csymbol cd="ambiguous"
    >formulae-sequence</csymbol><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑙</ci><ci >′</ci></apply></apply><ci >𝑖</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℱ</ci><apply ><ci  >𝑁</ci><ci
    >𝑁</ci></apply></apply><vector ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><cn type="integer" >0</cn></apply><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><cn type="integer"  >1</cn></apply><ci
    >𝑖</ci></apply><ci >…</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><ci >𝑙</ci></apply><ci
    >𝑖</ci></apply></vector></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐡</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑙</ci><ci >′′</ci></apply></apply><ci
    >𝑖</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℱ</ci><apply ><ci >𝐸</ci><ci >𝑁</ci></apply></apply><apply ><csymbol cd="latexml"
    >conditional-set</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐞</ci><ci >𝑙</ci></apply><apply
    ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply ><ci >𝑗</ci><apply ><ci >𝒩</ci><ci
    >𝑖</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐞</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑙</ci><ci >′</ci></apply></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℱ</ci><apply
    ><ci >𝐸</ci><ci >𝐸</ci></apply></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐞</ci><cn type="integer" >0</cn></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐞</ci><cn type="integer"  >1</cn></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><ci
    >…</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝐞</ci><ci >𝑙</ci></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply></vector></apply></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐞</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑙</ci><ci >′′</ci></apply></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℱ</ci><apply ><ci >𝑁</ci><ci
    >𝐸</ci></apply></apply><interval closure="open" ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐡</ci><ci >𝑙</ci></apply><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><ci >𝑙</ci></apply><ci
    >𝑗</ci></apply></interval><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><apply ><ci >𝑙</ci><cn
    type="integer" >1</cn></apply></apply><ci >𝑖</ci></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℱ</ci><apply
    ><ci >𝑁</ci><ci >𝑁</ci></apply></apply><interval closure="open" ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐡</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑙</ci><ci >′</ci></apply></apply><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑙</ci><ci >′′</ci></apply></apply><ci >𝑖</ci></apply></interval></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝐞</ci><apply ><ci >𝑙</ci><cn type="integer"  >1</cn></apply></apply><apply
    ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ℱ</ci><apply ><ci >𝐸</ci><ci >𝐸</ci></apply></apply><interval
    closure="open" ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐞</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑙</ci><ci >′</ci></apply></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐞</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑙</ci><ci >′′</ci></apply></apply><apply
    ><ci >𝑖</ci><ci >𝑗</ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{gathered}\mathbf{h}^{l^{\prime}}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{0}_{i},\mathbf{h}^{1}_{i},...,\mathbf{h}^{l}_{i}),\mathbf{h}^{l^{\prime\prime}}_{i}=\mathcal{F}_{EN}(\{\mathbf{e}^{l}_{ij}&#124;j\in\mathcal{N}(i)\})\\
    \mathbf{e}^{l^{\prime}}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{0}_{ij},\mathbf{e}^{1}_{ij},...,\mathbf{e}^{l}_{ij}),\mathbf{e}^{l^{\prime\prime}}_{ij}=\mathcal{F}_{NE}(\mathbf{h}^{l}_{i},\mathbf{h}^{l}_{j})\\
    \mathbf{h}^{l+1}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{l^{\prime}}_{i},\mathbf{h}^{l^{\prime\prime}}_{i}),\mathbf{e}^{l+1}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{l^{\prime}}_{ij},\mathbf{e}^{l^{\prime\prime}}_{ij}),\end{gathered}</annotation></semantics></math>
    |  | (40) |
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{h}^{l+1}_{i}=\mathcal{F}_{NN}(\mathbf{h}^{l^{\prime}}_{i},\mathbf{h}^{l^{\prime\prime}}_{i}),\mathbf{e}^{l+1}_{ij}=\mathcal{F}_{EE}(\mathbf{e}^{l^{\prime}}_{ij},\mathbf{e}^{l^{\prime\prime}}_{ij}),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><mrow ><msubsup ><mi  >𝐡</mi><mi >i</mi><msup ><mi  >l</mi><mo >′</mo></msup></msubsup><mo
    >=</mo><mrow ><msub ><mi >ℱ</mi><mrow ><mi  >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >N</mi></mrow></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msubsup ><mi >𝐡</mi><mi >i</mi><mn >0</mn></msubsup><mo >,</mo><msubsup
    ><mi >𝐡</mi><mi >i</mi><mn >1</mn></msubsup><mo >,</mo><mi mathvariant="normal"  >…</mi><mo
    >,</mo><msubsup ><mi >𝐡</mi><mi >i</mi><mi >l</mi></msubsup><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐡</mi><mi >i</mi><msup
    ><mi >l</mi><mo >′′</mo></msup></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow
    ><mi >E</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >N</mi></mrow></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mo
    stretchy="false" >{</mo><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo lspace="0em" rspace="0em"
    >&#124;</mo><mrow ><mi >j</mi><mo >∈</mo><mrow ><mi >𝒩</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mi >i</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    stretchy="false" >}</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow ><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >j</mi></mrow><msup ><mi  >l</mi><mo >′</mo></msup></msubsup><mo >=</mo><mrow
    ><msub ><mi >ℱ</mi><mrow ><mi >E</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >E</mi></mrow></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msubsup ><mi >𝐞</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >j</mi></mrow><mn >0</mn></msubsup><mo >,</mo><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow><mn >1</mn></msubsup><mo
    >,</mo><mi mathvariant="normal" >…</mi><mo >,</mo><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐞</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><msup ><mi
    >l</mi><mo >′′</mo></msup></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow
    ><mi >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >E</mi></mrow></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi
    >𝐡</mi><mi >i</mi><mi >l</mi></msubsup><mo >,</mo><msubsup ><mi >𝐡</mi><mi >j</mi><mi
    >l</mi></msubsup><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow ><mrow ><msubsup ><mi >𝐡</mi><mi >i</mi><mrow ><mi  >l</mi><mo
    >+</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow ><mi
    >N</mi><mo lspace="0em" rspace="0em" >​</mo><mi >N</mi></mrow></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >𝐡</mi><mi
    >i</mi><msup ><mi  >l</mi><mo >′</mo></msup></msubsup><mo >,</mo><msubsup ><mi
    >𝐡</mi><mi >i</mi><msup ><mi >l</mi><mo >′′</mo></msup></msubsup><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo >,</mo><mrow ><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><mrow ><mi >l</mi><mo
    >+</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow ><msub ><mi >ℱ</mi><mrow ><mi
    >E</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >E</mi></mrow></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >𝐞</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow><msup ><mi
    >l</mi><mo >′</mo></msup></msubsup><mo >,</mo><msubsup ><mi >𝐞</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow><msup ><mi >l</mi><mo
    >′′</mo></msup></msubsup><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" ><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><csymbol cd="ambiguous"
    >formulae-sequence</csymbol><apply ><apply ><csymbol cd="amb
- en: 'where $\mathbf{e}_{ij}^{l}$ is the representation of edge $(v_{i},v_{j})$ in
    the $l^{th}$ layer and $\mathcal{F}(\cdot)$ are learnable functions whose subscripts
    represent message-passing directions. By stacking multiple such modules, information
    can propagate by alternately passing between node and edge representations. Note
    that in the node-to-node and edge-to-edge functions, jump connections similar
    to those in JK-Nets [[62](#bib.bib62)] are implicitly added. GNs [[9](#bib.bib9)]
    also proposed learning an edge representation and updating both node and edge
    representations using message-passing functions as shown in Eq. ([27](#S4.E27
    "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")) in Section [4.1.4](#S4.SS1.SSS4 "4.1.4
    Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks ‣ Deep
    Learning on Graphs: A Survey"). In this aspect, the “weave module” is a special
    case of GNs that does not a representation of the entire graph.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{e}_{ij}^{l}$ 是第 $l^{th}$ 层中边 $(v_{i},v_{j})$ 的表示，$\mathcal{F}(\cdot)$
    是可学习的函数，其下标表示消息传递方向。通过堆叠多个此类模块，信息可以通过在节点和边表示之间交替传递来传播。请注意，在节点到节点和边到边的函数中，隐式地添加了类似于
    JK-Nets [[62](#bib.bib62)] 的跳跃连接。GNs [[9](#bib.bib9)] 还提出了学习边表示并使用消息传递函数更新节点和边表示，如第[27](#S4.E27
    "In 4.1.4 Frameworks ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey")节中的公式所示。在这方面，“编织模块”是 GNs 的一个特殊情况，它没有整个图的表示。'
- en: 4.3.4 Sampling Methods
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 采样方法
- en: 'One critical bottleneck when training GCNs for large-scale graphs is efficiency.
    As shown in Section [4.1.4](#S4.SS1.SSS4 "4.1.4 Frameworks ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey"), many GCNs
    follow a neighborhood aggregation scheme. However, because many real graphs follow
    a power-law distribution [[91](#bib.bib91)] (i.e., a few nodes have very large
    degrees), the number of neighbors can expand extremely quickly. To deal with this
    problem, two types of sampling methods have been proposed: neighborhood samplings
    and layer-wise samplings, as illustrated in Figure [7](#S4.F7 "Figure 7 ‣ 4.3.4
    Sampling Methods ‣ 4.3 Improvements and Discussions ‣ 4 Graph Convolutional Networks
    ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '在大规模图上训练 GCNs 的一个关键瓶颈是效率。如[4.1.4](#S4.SS1.SSS4 "4.1.4 Frameworks ‣ 4.1 Convolution
    Operations ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")节所示，许多
    GCNs 遵循邻域聚合方案。然而，由于许多真实图遵循幂律分布 [[91](#bib.bib91)]（即少数节点具有非常大的度数），邻居的数量可以极快地增加。为解决这个问题，提出了两种采样方法：邻域采样和层级采样，如图[7](#S4.F7
    "Figure 7 ‣ 4.3.4 Sampling Methods ‣ 4.3 Improvements and Discussions ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")所示。'
- en: In neighborhood samplings, the sampling is performed for each node during the
    calculations. GraphSAGE [[53](#bib.bib53)] uniformly sampled a fixed number of
    neighbors for each node during training. PinSage [[66](#bib.bib66)] proposed sampling
    neighbors using random walks on graphs along with several implementation improvements
    including coordination between the CPU and GPU, a map-reduce inference pipeline,
    and so on. PinSage was shown to be capable of handling a real billion-scale graph.
    StochasticGCN [[67](#bib.bib67)] further proposed reducing the sampling variances
    by using the historical activations of the last batches as a control variate,
    allowing for arbitrarily small sample sizes with a theoretical guarantee.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在邻域采样中，采样在计算过程中对每个节点进行。GraphSAGE [[53](#bib.bib53)] 在训练过程中均匀地为每个节点采样固定数量的邻居。PinSage
    [[66](#bib.bib66)] 提出了使用图上的随机游走进行邻居采样，并进行了若干实施改进，包括 CPU 和 GPU 之间的协调、一个 map-reduce
    推断管道等。研究表明，PinSage 能够处理实际的十亿规模图。StochasticGCN [[67](#bib.bib67)] 进一步提出通过使用上一个批次的历史激活作为控制变量来减少采样方差，从而在理论上保证可以使用任意小的样本大小。
- en: 'Instead of sampling neighbors of nodes, FastGCN [[68](#bib.bib68)] adopted
    a different strategy: it sampled nodes in each convolutional layer (i.e., a layer-wise
    sampling) by interpreting the nodes as i.i.d. samples and the graph convolutions
    as integral transforms under probability measures. FastGCN also showed that sampling
    nodes via their normalized degrees could reduce variances and lead to better performance.
    Adapt [[69](#bib.bib69)] further proposed sampling nodes in the lower layers conditioned
    on their top layer; this approach was more adaptive and applicable to explicitly
    reduce variances.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 与节点邻居采样不同，FastGCN [[68](#bib.bib68)] 采用了一种不同的策略：它在每个卷积层中进行节点采样（即逐层采样），将节点视为独立同分布的样本，并将图卷积解释为概率测度下的积分变换。FastGCN
    还表明，通过节点的归一化度数进行采样可以减少方差并提高性能。Adapt [[69](#bib.bib69)] 进一步提出在下层节点上进行条件采样，以其顶层为条件；这种方法更具适应性，能够显著减少方差。
- en: '![Refer to caption](img/4e12bde8d6457c8599967b3e1df050da.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4e12bde8d6457c8599967b3e1df050da.png)'
- en: 'Figure 7: Different node sampling methods, in which the blue nodes indicate
    samples from one batch and the arrows indicate the sampling directions. The red
    nodes in (B) represent historical samples.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：不同的节点采样方法，其中蓝色节点表示来自一个批次的样本，箭头表示采样方向。图 (B) 中的红色节点表示历史样本。
- en: 4.3.5 Inductive Setting
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5 归纳设置
- en: Another important aspect of GCNs is that whether they can be applied to an inductive
    setting, i.e., training on a set of nodes or graphs and testing on another unseen
    set of nodes or graphs. In principle, this goal is achieved by learning a mapping
    function on the given features that are not dependent on the graph basis and can
    be transferred across nodes or graphs. The inductive setting was verified in GraphSAGE [[53](#bib.bib53)],
    GAT [[57](#bib.bib57)], GaAN [[58](#bib.bib58)], and FastGCN [[68](#bib.bib68)].
    However, the existing inductive GCNs are suitable only for graphs with explicit
    features. How to conduct inductive learnings for graphs without explicit features,
    usually called the out-of-sample problem [[92](#bib.bib92)], remains largely open
    in the literature.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: GCNs的另一个重要方面是它们是否可以应用于归纳设置，即在一组节点或图上进行训练并在另一组未见过的节点或图上进行测试。从原则上讲，这个目标是通过学习一个在给定特征上不依赖于图基的映射函数来实现的，这个映射函数可以跨节点或图进行转移。GraphSAGE
    [[53](#bib.bib53)]、GAT [[57](#bib.bib57)]、GaAN [[58](#bib.bib58)] 和 FastGCN [[68](#bib.bib68)]
    验证了归纳设置。然而，现有的归纳GCNs 仅适用于具有显式特征的图。如何对没有显式特征的图进行归纳学习，通常称为样本外问题 [[92](#bib.bib92)]，在文献中仍然大多未解决。
- en: 4.3.6 Theoretical Analysis
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.6 理论分析
- en: 'To understand the effectiveness of GCNs, some theoretical analyses have been
    proposed that can be divided into three categories: node-focused tasks, graph-focused
    tasks, and general analysis.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解GCNs的有效性，提出了一些理论分析，这些分析可以分为三类：以节点为中心的任务、以图为中心的任务和通用分析。
- en: 'For node-focused tasks, Li et al. [[70](#bib.bib70)] first analyzed the performance
    of GCNs by using a special form of Laplacian smoothing, which makes the features
    of nodes in the same cluster similar. The original Laplacian smoothing operation
    is formulated as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以节点为中心的任务，Li 等人 [[70](#bib.bib70)] 首先通过使用一种特殊形式的拉普拉斯平滑分析了GCNs的性能，该形式使得同一簇中的节点特征相似。原始的拉普拉斯平滑操作公式如下：
- en: '|  | $\mathbf{h}^{\prime}_{i}=(1-\gamma)\mathbf{h}_{i}+\gamma\sum\nolimits_{j\in\mathcal{N}(i)}\frac{1}{d_{i}}\mathbf{h}_{j},$
    |  | (41) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}^{\prime}_{i}=(1-\gamma)\mathbf{h}_{i}+\gamma\sum\nolimits_{j\in\mathcal{N}(i)}\frac{1}{d_{i}}\mathbf{h}_{j},$
    |  | (41) |'
- en: 'where $\mathbf{h}_{i}$ and $\mathbf{h}^{\prime}_{i}$ are the original and smoothed
    features of node $v_{i}$, respectively. We can see that Eq. ([41](#S4.E41 "In
    4.3.6 Theoretical Analysis ‣ 4.3 Improvements and Discussions ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) is very similar to the graph convolution
    in Eq. ([13](#S4.E13 "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations
    ‣ 4 Graph Convolutional Networks ‣ Deep Learning on Graphs: A Survey")). Based
    on this insight, Li et al. also proposed a co-training and a self-training method
    for GCNs.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{h}_{i}$ 和 $\mathbf{h}^{\prime}_{i}$ 分别是节点 $v_{i}$ 的原始特征和光滑特征。我们可以看到公式 ([41](#S4.E41
    "在 4.3.6 理论分析 ‣ 4.3 改进和讨论 ‣ 4 图卷积网络 ‣ 图上的深度学习：综述")) 与公式 ([13](#S4.E13 "在 4.1.2
    效率方面 ‣ 4.1 卷积操作 ‣ 4 图卷积网络 ‣ 图上的深度学习：综述")) 中的图卷积非常相似。基于这一见解，Li 等人还提出了GCNs的联合训练和自我训练方法。
- en: 'Recently, Wu et al. [[71](#bib.bib71)] analyzed GCNs from a signal processing
    perspective. By regarding node features as graph signals, they showed that Eq. ([13](#S4.E13
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) is basically a fixed low-pass
    filter. Using this insight, they proposed an extremely simplified graph convolution
    (SGC) architecture by removing all the nonlinearities and collapsing the learning
    parameters into one matrix:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，Wu 等人 [[71](#bib.bib71)] 从信号处理的角度分析了 GCNs。通过将节点特征视为图信号，他们表明公式 ([13](#S4.E13
    "In 4.1.2 The Efficiency Aspect ‣ 4.1 Convolution Operations ‣ 4 Graph Convolutional
    Networks ‣ Deep Learning on Graphs: A Survey")) 基本上是一个固定的低通滤波器。基于这一洞察，他们提出了一种极其简化的图卷积（SGC）架构，通过去除所有非线性并将学习参数合并到一个矩阵中：'
- en: '|  | $\mathbf{H}^{L}=\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\right)^{L}\mathbf{F}_{V}\mathbf{\Theta}.$
    |  | (42) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}^{L}=\left(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\right)^{L}\mathbf{F}_{V}\mathbf{\Theta}.$
    |  | (42) |'
- en: The authors showed that such a “non-deep-learning” GCN variant achieved comparable
    performance to existing GCNs in many tasks. Maehara [[72](#bib.bib72)] enhanced
    this result by showing that the low-pass filtering operation did not equip GCNs
    with a nonlinear manifold learning ability, and further proposed GFNN model to
    remedy this problem by adding a MLP after the graph convolution layers.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们展示了这种“非深度学习” GCN 变体在许多任务中实现了与现有 GCNs 相当的性能。Maehara [[72](#bib.bib72)] 通过展示低通滤波操作并未赋予
    GCNs 非线性流形学习能力，进一步增强了这一结果，并提出了 GFNN 模型，通过在图卷积层后添加 MLP 来解决这个问题。
- en: For graph-focused tasks, Kipf and Welling [[43](#bib.bib43)] and the authors
    of SortPooling [[49](#bib.bib49)] both considered the relationship between GCNs
    and graph kernels such as the Weisfeiler-Lehman (WL) kernel [[78](#bib.bib78)],
    which is widely used in graph isomorphism tests. They showed that GCNs are conceptually
    a generalization of the WL kernel because both methods iteratively aggregate information
    from node neighbors. Xu et al. [[73](#bib.bib73)] formalized this idea by proving
    that the WL kernel provides an upper bound for GCNs in terms of distinguishing
    graph structures. Based on this analysis, they proposed graph isomorphism network
    (GIN) and showed that a readout operation using summation and a MLP can achieve
    provably maximum discriminative power, i.e., the highest training accuracy in
    graph classification tasks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图相关任务，Kipf 和 Welling [[43](#bib.bib43)] 以及 SortPooling 的作者 [[49](#bib.bib49)]
    都考虑了 GCNs 与图核（如 Weisfeiler-Lehman (WL) 核 [[78](#bib.bib78)]）之间的关系，该核在图同构测试中被广泛使用。他们表明，GCNs
    从概念上讲是 WL 核的一个推广，因为这两种方法都通过迭代聚合来自节点邻居的信息。Xu 等人 [[73](#bib.bib73)] 通过证明 WL 核在区分图结构方面为
    GCNs 提供了一个上界，从而形式化了这一观点。基于此分析，他们提出了图同构网络（GIN），并表明使用求和和多层感知机（MLP）的读取操作可以实现可证明的最大区分能力，即在图分类任务中达到最高的训练准确度。
- en: For general analysis, Scarselli et al. [[93](#bib.bib93)] showed that the Vapnik-Chervonenkis
    dimension (VC-dim) of GCNs with different activation functions has the same scale
    as the existing RNNs. Chen et al. [[65](#bib.bib65)] analyzed the optimization
    landscape of linear GCNs and showed that any local minimum is relatively close
    to the global minimum under certain simplifications. Verma and Zhang [[94](#bib.bib94)]
    analyzed the algorithmic stability and generalization bound of GCNs. They showed
    that single-layer GCNs satisfy the strong notion of uniform stability if the largest
    absolute eigenvalue of the graph convolution filters is independent of the graph
    size.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般分析，Scarselli 等人 [[93](#bib.bib93)] 表明，不同激活函数的图卷积网络（GCNs）的 Vapnik-Chervonenkis
    维度（VC-dim）与现有的递归神经网络（RNNs）具有相同的量级。Chen 等人 [[65](#bib.bib65)] 分析了线性 GCNs 的优化景观，并表明在某些简化条件下，任何局部最小值都相对接近全局最小值。Verma
    和 Zhang [[94](#bib.bib94)] 分析了 GCNs 的算法稳定性和泛化界限。他们展示了单层 GCNs 满足强均匀稳定性的定义，前提是图卷积滤波器的最大绝对特征值与图的大小无关。
- en: 5 Graph Autoencoders
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 图自编码器
- en: 'The autoencoder (AE) and its variations have been widely applied in unsupervised
    learning tasks [[95](#bib.bib95)] and are suitable for learning node representations
    for graphs. The implicit assumption is that graphs have an inherent, potentially
    nonlinear low-rank structure. In this section, we first elaborate graph autoencoders
    and then introduce graph variational autoencoders and other improvements. The
    main characteristics of GAEs are summarized in Table [V](#S5.T5 "TABLE V ‣ 5 Graph
    Autoencoders ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '自编码器 (AE) 及其变种已广泛应用于无监督学习任务 [[95](#bib.bib95)]，并适用于学习图的节点表示。隐含的假设是图具有固有的、可能是非线性的低秩结构。在本节中，我们首先详细阐述图自编码器，然后介绍图变分自编码器及其他改进。GAEs
    的主要特征总结在表 [V](#S5.T5 "TABLE V ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs:
    A Survey")。'
- en: 'TABLE V: A Comparison among Different Graph Autoencoders (GAEs). T.C. = Time
    Complexity'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 不同图自编码器 (GAEs) 的比较。T.C. = 时间复杂度'
- en: '| Method | Type | Objective Function | T.C. | Node Features | Other Characteristics
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类型 | 目标函数 | 时间复杂度 | 节点特征 | 其他特征 |'
- en: '| SAE [[96](#bib.bib96)] | AE | L2-reconstruction | $O(M)$ | No | - |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| SAE [[96](#bib.bib96)] | AE | L2-重构 | $O(M)$ | 否 | - |'
- en: '| SDNE [[97](#bib.bib97)] | AE | L2-reconstruction + Laplacian eigenmaps |
    $O(M)$ | No | - |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| SDNE [[97](#bib.bib97)] | AE | L2-重构 + 拉普拉斯特征图 | $O(M)$ | 否 | - |'
- en: '| DNGR [[98](#bib.bib98)] | AE | L2-reconstruction | $O(N^{2})$ | No | - |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| DNGR [[98](#bib.bib98)] | AE | L2-重构 | $O(N^{2})$ | 否 | - |'
- en: '| GC-MC [[99](#bib.bib99)] | AE | L2-reconstruction | $O(M)$ | Yes | GCN encoder
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| GC-MC [[99](#bib.bib99)] | AE | L2-重构 | $O(M)$ | 是 | GCN 编码器 |'
- en: '| DRNE [[100](#bib.bib100)] | AE | Recursive reconstruction | $O(Ns)$ | No
    | LSTM aggregator |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| DRNE [[100](#bib.bib100)] | AE | 递归重构 | $O(Ns)$ | 否 | LSTM 聚合器 |'
- en: '| G2G [[101](#bib.bib101)] | AE | KL + ranking | $O(M)$ | Yes | Nodes as distributions
    |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| G2G [[101](#bib.bib101)] | AE | KL + 排序 | $O(M)$ | 是 | 节点作为分布 |'
- en: '| VGAE [[102](#bib.bib102)] | VAE | Pairwise reconstruction | $O(N^{2})$ |
    Yes | GCN encoder |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| VGAE [[102](#bib.bib102)] | VAE | 成对重构 | $O(N^{2})$ | 是 | GCN 编码器 |'
- en: '| DVNE [[103](#bib.bib103)] | VAE | Wasserstein + ranking | $O(M)$ | No | Nodes
    as distributions |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| DVNE [[103](#bib.bib103)] | VAE | Wasserstein + 排序 | $O(M)$ | 否 | 节点作为分布
    |'
- en: '| ARGA/ARVGA [[104](#bib.bib104)] | AE/VAE | L2-reconstruction + GAN | $O(N^{2})$
    | Yes | GCN encoder |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| ARGA/ARVGA [[104](#bib.bib104)] | AE/VAE | L2-重构 + GAN | $O(N^{2})$ | 是 |
    GCN 编码器 |'
- en: '| NetRA [[105](#bib.bib105)] | AE | Recursive reconstruction + Laplacian eigenmaps
    + GAN | $O(M)$ | No | LSTM encoder |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| NetRA [[105](#bib.bib105)] | AE | 递归重构 + 拉普拉斯特征图 + GAN | $O(M)$ | 否 | LSTM
    编码器 |'
- en: 5.1 Autoencoders
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 自编码器
- en: 'The use of AEs for graphs originated from sparse autoencoder (SAE) [[96](#bib.bib96)].
    The basic idea is that, by regarding the adjacency matrix or its variations as
    the raw features of nodes, AEs can be leveraged as a dimensionality reduction
    technique to learn low-dimensional node representations. Specifically, SAE adopted
    the following L2-reconstruction loss:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 对图应用 AEs 的起源来自于稀疏自编码器 (SAE) [[96](#bib.bib96)]。基本思想是，通过将邻接矩阵或其变体视为节点的原始特征，AEs
    可以被用作一种降维技术来学习低维的节点表示。具体来说，SAE 采用了以下 L2-重构损失：
- en: '|  | $\begin{gathered}\min_{\mathbf{\Theta}}\mathcal{L}_{2}=\sum\nolimits_{i=1}^{N}\left\&#124;\mathbf{P}\left(i,:\right)-\hat{\mathbf{P}}\left(i,:\right)\right\&#124;_{2}\\
    \hat{\mathbf{P}}\left(i,:\right)=\mathcal{G}\left(\mathbf{h}_{i}\right),\mathbf{h}_{i}=\mathcal{F}\left(\mathbf{P}\left(i,:\right)\right),\end{gathered}$
    |  | (43) |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}\min_{\mathbf{\Theta}}\mathcal{L}_{2}=\sum\nolimits_{i=1}^{N}\left\|
    \mathbf{P}\left(i,:\right)-\hat{\mathbf{P}}\left(i,:\right)\right\|_{2}\\ \hat{\mathbf{P}}\left(i,:\right)=\mathcal{G}\left(\mathbf{h}_{i}\right),\mathbf{h}_{i}=\mathcal{F}\left(\mathbf{P}\left(i,:\right)\right),\end{gathered}$
    |  | (43) |'
- en: where $\mathbf{P}$ is the transition matrix, $\hat{\mathbf{P}}$ is the reconstructed
    matrix, $\mathbf{h}_{i}\in\mathbb{R}^{d}$ is the low-dimensional representation
    of node $v_{i}$, $\mathcal{F}(\cdot)$ is the encoder, $\mathcal{G}(\cdot)$ is
    the decoder, $d\ll N$ is the dimensionality, and $\mathbf{\Theta}$ are parameters.
    Both the encoder and decoder are an MLP with many hidden layers. In other words,
    a SAE compresses the information of $\mathbf{P}(i,:)$ into a low-dimensional vector
    $\mathbf{h}_{i}$ and then reconstructs the original feature from that vector.
    Another sparsity regularization term was also added. After obtaining the low-dimensional
    representation $\mathbf{h}_{i}$, k-means [[106](#bib.bib106)] was applied for
    the node clustering task. The experiments prove that SAEs outperform non-deep
    learning baselines. However, SAE was based on an incorrect theoretical analysis.⁸⁸8SAE [[96](#bib.bib96)]
    motivated the problem by analyzing the connection between spectral clustering
    and singular value decomposition, which is mathematically incorrect as pointed
    out in [[107](#bib.bib107)]. The mechanism underlying its effectiveness remained
    unexplained.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{P}$ 是转移矩阵，$\hat{\mathbf{P}}$ 是重建矩阵，$\mathbf{h}_{i}\in\mathbb{R}^{d}$
    是节点 $v_{i}$ 的低维表示，$\mathcal{F}(\cdot)$ 是编码器，$\mathcal{G}(\cdot)$ 是解码器，$d\ll N$
    是维度，$\mathbf{\Theta}$ 是参数。编码器和解码器都是具有多个隐藏层的 MLP。换句话说，SAE 将 $\mathbf{P}(i,:)$ 的信息压缩成低维向量
    $\mathbf{h}_{i}$，然后从该向量重建原始特征。还增加了另一个稀疏性正则化项。在获得低维表示 $\mathbf{h}_{i}$ 后，使用 k-means
    [[106](#bib.bib106)] 进行了节点聚类任务。实验证明 SAEs 优于非深度学习基准。然而，SAE 基于错误的理论分析。⁸⁸8SAE [[96](#bib.bib96)]
    通过分析谱聚类与奇异值分解之间的关系激发了这个问题，但正如 [[107](#bib.bib107)] 所指出的那样，这在数学上是错误的。其有效性背后的机制仍未解释。
- en: 'Structure deep network embedding (SDNE) [[97](#bib.bib97)] filled in the puzzle
    by showing that the L2-reconstruction loss in Eq. ([43](#S5.E43 "In 5.1 Autoencoders
    ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")) actually corresponds
    to the second-order proximity between nodes, i.e., two nodes share similar latten
    representations if they have similar neighborhoods, which is a well-studied concept
    in network science known as collaborative filtering or triangle closure [[5](#bib.bib5)].
    Motivated by network embedding methods showing that the first-order proximity
    is also important [[108](#bib.bib108)], SDNE modified the objective function by
    adding another Laplacian eigenmaps term [[75](#bib.bib75)]:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '结构深度网络嵌入（SDNE） [[97](#bib.bib97)] 通过显示公式 ([43](#S5.E43 "In 5.1 Autoencoders
    ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")) 中的 L2 重建损失实际上对应于节点之间的二阶接近度，即如果两个节点具有相似的邻域，则它们共享相似的潜在表示，这是网络科学中的一个研究充分的概念，称为协同过滤或三角形闭合
    [[5](#bib.bib5)]。受到网络嵌入方法的启发，这些方法表明一阶接近度也很重要 [[108](#bib.bib108)]，SDNE 通过添加另一个拉普拉斯特征映射项
    [[75](#bib.bib75)] 修改了目标函数：'
- en: '|  | $\min_{\mathbf{\Theta}}\mathcal{L}_{2}+\alpha\sum\nolimits_{i,j=1}^{N}\mathbf{A}(i,j)\left\&#124;\mathbf{h}_{i}-\mathbf{h}_{j}\right\&#124;_{2},$
    |  | (44) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\mathbf{\Theta}}\mathcal{L}_{2}+\alpha\sum\nolimits_{i,j=1}^{N}\mathbf{A}(i,j)\left\&#124;\mathbf{h}_{i}-\mathbf{h}_{j}\right\&#124;_{2},$
    |  | (44) |'
- en: 'i.e., two nodes also share similar latent representations if they are directly
    connected. The authors also modified the L2-reconstruction loss by using the adjacency
    matrix and assigning different weights to zero and non-zero elements:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 即，如果两个节点直接连接，它们也共享相似的潜在表示。作者还通过使用邻接矩阵并为零和非零元素分配不同的权重来修改 L2 重建损失。
- en: '|  | $\mathcal{L}_{2}=\sum\nolimits_{i=1}^{N}\left\&#124;\left(\mathbf{A}\left(i,:\right)-\mathcal{G}\left(\mathbf{h}_{i}\right)\right)\odot\mathbf{b}_{i}\right\&#124;_{2},$
    |  | (45) |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{2}=\sum\nolimits_{i=1}^{N}\left\&#124;\left(\mathbf{A}\left(i,:\right)-\mathcal{G}\left(\mathbf{h}_{i}\right)\right)\odot\mathbf{b}_{i}\right\&#124;_{2},$
    |  | (45) |'
- en: 'where $\mathbf{h}_{i}=\mathcal{F}\left(\mathbf{A}\left(i,:\right)\right)$,
    $b_{ij}=1$ if $\mathbf{A}(i,j)=0$; otherwise $b_{ij}=\beta>1$, and $\beta$ is
    another hyper-parameter. The overall architecture of SDNE is shown in Figure [8](#S5.F8
    "Figure 8 ‣ 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs:
    A Survey").'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{h}_{i}=\mathcal{F}\left(\mathbf{A}\left(i,:\right)\right)$，如果 $\mathbf{A}(i,j)=0$，则
    $b_{ij}=1$；否则 $b_{ij}=\beta>1$，其中 $\beta$ 是另一个超参数。SDNE 的整体架构如图 [8](#S5.F8 "Figure
    8 ‣ 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")
    所示。'
- en: 'Motivated by another line of studies, a contemporary work DNGR [[98](#bib.bib98)]
    replaced the transition matrix $\mathbf{P}$ in Eq. ([43](#S5.E43 "In 5.1 Autoencoders
    ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")) with the positive
    pointwise mutual information (PPMI) [[79](#bib.bib79)] matrix defined in Eq. ([20](#S4.E20
    "In 4.1.3 The Aspect of Multiple Graphs ‣ 4.1 Convolution Operations ‣ 4 Graph
    Convolutional Networks ‣ Deep Learning on Graphs: A Survey")). In this way, the
    raw features can be associated with some random walk probability of the graph [[109](#bib.bib109)].
    However, constructing the input matrix has a time complexity of $O(N^{2})$, which
    is not scalable to large-scale graphs.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 受到另一类研究的启发，现代工作DNGR [[98](#bib.bib98)]将公式 ([43](#S5.E43 "在5.1节 自编码器 ‣ 5 图自编码器
    ‣ 图上的深度学习：综述"))中的转移矩阵 $\mathbf{P}$ 替换为公式 ([20](#S4.E20 "在4.1.3节 多图的视角 ‣ 4.1 卷积操作
    ‣ 4 图卷积网络 ‣ 图上的深度学习：综述"))中定义的正点互信息（PPMI） [[79](#bib.bib79)] 矩阵。通过这种方式，原始特征可以与图的一些随机游走概率相关联 [[109](#bib.bib109)]。然而，构建输入矩阵的时间复杂度为
    $O(N^{2})$，不适用于大规模图。
- en: '![Refer to caption](img/602bff1359bba0787232cfa4f2cc13d2.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/602bff1359bba0787232cfa4f2cc13d2.png)'
- en: 'Figure 8: The framework of SDNE [[97](#bib.bib97)]. Both the first and second-order
    proximities of nodes are preserved using deep autoencoders.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：SDNE [[97](#bib.bib97)]的框架。使用深度自编码器保留了节点的第一和第二阶邻近性。
- en: 'GC-MC [[99](#bib.bib99)] took a different approach by using the GCN proposed
    by Kipf and Welling [[43](#bib.bib43)] as the encoder:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: GC-MC [[99](#bib.bib99)]采用了不同的方法，使用Kipf和Welling [[43](#bib.bib43)]提出的GCN作为编码器：
- en: '|  | $\mathbf{H}=GCN\left(\mathbf{F}^{V},\mathbf{A}\right),$ |  | (46) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}=GCN\left(\mathbf{F}^{V},\mathbf{A}\right),$ |  | (46) |'
- en: 'and using a simple bilinear function as the decoder:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用一个简单的双线性函数作为解码器：
- en: '|  | $\hat{\mathbf{A}}(i,j)=\mathbf{H}(i,:)\mathbf{\Theta}_{de}\mathbf{H}(j,:)^{T},$
    |  | (47) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{A}}(i,j)=\mathbf{H}(i,:)\mathbf{\Theta}_{de}\mathbf{H}(j,:)^{T},$
    |  | (47) |'
- en: where $\mathbf{\Theta}_{de}$ are the decoder parameters. Using this approach,
    node features were naturally incorporated. For graphs without node features, a
    one-hot encoding of node IDs was utilized. The authors demonstrated the effectiveness
    of GC-MC on the recommendation problem on bipartite graphs.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{\Theta}_{de}$ 是解码器参数。采用这种方法，自然地融入了节点特征。对于没有节点特征的图，使用了节点ID的one-hot编码。作者展示了GC-MC在二分图推荐问题上的有效性。
- en: 'Instead of reconstructing the adjacency matrix or its variations, DRNE [[100](#bib.bib100)]
    proposed another modification that directly reconstructed the low-dimensional
    node vectors by aggregating neighborhood information using an LSTM. Specifically,
    DRNE adopted the following objective function:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: DRNE [[100](#bib.bib100)] 提出了另一种修改方法，直接通过使用LSTM汇聚邻域信息来重建低维节点向量。具体而言，DRNE采用了以下目标函数：
- en: '|  | $\mathcal{L}=\sum\nolimits_{i=1}^{N}\left\&#124;\mathbf{h}_{i}-\text{LSTM}\left(\{\mathbf{h}_{j}&#124;j\in\mathcal{N}(i)\}\right)\right\&#124;.$
    |  | (48) |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\sum\nolimits_{i=1}^{N}\left\| \mathbf{h}_{i}-\text{LSTM}\left(\{\mathbf{h}_{j}|j\in\mathcal{N}(i)\}\right)\right\|.$
    |  | (48) |'
- en: Because an LSTM requires its inputs to be a sequence, the authors suggested
    ordering the node neighborhoods based on their degrees. They also adopted a neighborhood
    sampling technique for nodes with large degrees to prevent an overlong memory.
    The authors proved that such a method can preserve regular equivalence as well
    as many centrality measures of nodes, such as PageRank [[110](#bib.bib110)].
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因为LSTM需要其输入为序列，作者建议根据节点的度来排序节点邻域。他们还采用了邻域采样技术，以防止过长的记忆。作者证明了这种方法可以保留常规等价性以及许多节点中心性测度，如PageRank [[110](#bib.bib110)]。
- en: 'Unlike the above works that map nodes into a low-dimensional vector, Graph2Gauss
    (G2G) [[101](#bib.bib101)] proposed encoding each node as a Gaussian distribution
    $\mathbf{h}_{i}=\mathcal{N}\left(\mathbf{M}(i,:),diag\left(\mathbf{\Sigma}(i,:)\right)\right)$
    to capture the uncertainties of nodes. Specifically, the authors used a deep mapping
    from the node attributes to the means and variances of the Gaussian distribution
    as the encoder:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述将节点映射到低维向量的工作不同，Graph2Gauss (G2G) [[101](#bib.bib101)]提出将每个节点编码为高斯分布 $\mathbf{h}_{i}=\mathcal{N}\left(\mathbf{M}(i,:),diag\left(\mathbf{\Sigma}(i,:)\right)\right)$
    以捕捉节点的不确定性。具体而言，作者使用了从节点属性到高斯分布的均值和方差的深度映射作为编码器：
- en: '|  | $\mathbf{M}(i,:)=\mathcal{F}_{\mathbf{M}}(\mathbf{F}^{V}(i,:)),\mathbf{\Sigma}(i,:)=\mathcal{F}_{\mathbf{\Sigma}}(\mathbf{F}^{V}(i,:)),$
    |  | (49) |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{M}(i,:)=\mathcal{F}_{\mathbf{M}}(\mathbf{F}^{V}(i,:)),\mathbf{\Sigma}(i,:)=\mathcal{F}_{\mathbf{\Sigma}}(\mathbf{F}^{V}(i,:)),$
    |  | (49) |'
- en: 'where $\mathcal{F}_{\mathbf{M}}(\cdot)$ and $\mathcal{F}_{\mathbf{\Sigma}}(\cdot)$
    are the parametric functions that need to be learned. Then, instead of using an
    explicit decoder function, they used pairwise constraints to learn the model:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}_{\mathbf{M}}(\cdot)$ 和 $\mathcal{F}_{\mathbf{\Sigma}}(\cdot)$
    是需要学习的参数函数。然后，他们使用成对约束来学习模型，而不是使用显式的解码器函数：
- en: '|  | $\begin{gathered}\text{KL}\left(\mathbf{h}_{j}&#124;&#124;\mathbf{h}_{i}\right)<\text{KL}\left(\mathbf{h}_{j^{\prime}}&#124;&#124;\mathbf{h}_{i}\right)\\
    \forall i,\forall j,\forall j^{\prime}\;s.t.\;d(i,j)<d(i,j^{\prime}),\end{gathered}$
    |  | (50) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}\text{KL}\left(\mathbf{h}_{j}&#124;&#124;\mathbf{h}_{i}\right)<\text{KL}\left(\mathbf{h}_{j^{\prime}}&#124;&#124;\mathbf{h}_{i}\right)\\
    \forall i,\forall j,\forall j^{\prime}\;s.t.\;d(i,j)<d(i,j^{\prime}),\end{gathered}$
    |  | (50) |'
- en: 'where $d(i,j)$ is the shortest distance from node $v_{i}$ to $v_{j}$ and $\text{KL}(q(\cdot)||p(\cdot))$
    is the Kullback-Leibler (KL) divergence between $q(\cdot)$ and $p(\cdot)$ [[111](#bib.bib111)].
    In other words, the constraints ensure that the KL-divergence between node representations
    has the same relative order as the graph distance. However, because Eq. ([50](#S5.E50
    "In 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey"))
    is hard to optimize, an energy-based loss [[112](#bib.bib112)] was adopted as
    a relaxation:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $d(i,j)$ 是从节点 $v_{i}$ 到 $v_{j}$ 的最短距离，$\text{KL}(q(\cdot)||p(\cdot))$ 是
    $q(\cdot)$ 和 $p(\cdot)$ 之间的Kullback-Leibler (KL)散度[[111](#bib.bib111)]。换句话说，约束确保了节点表示之间的KL散度具有与图距离相同的相对顺序。然而，由于
    Eq. ([50](#S5.E50 "In 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning
    on Graphs: A Survey")) 难以优化，因此采用了基于能量的损失[[112](#bib.bib112)]作为放松：'
- en: '|  | $\mathcal{L}=\sum\nolimits_{(i,j,j^{\prime})\in\mathcal{D}}\left(E_{ij}^{2}+\exp^{-E_{ij^{\prime}}}\right),$
    |  | (51) |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\sum\nolimits_{(i,j,j^{\prime})\in\mathcal{D}}\left(E_{ij}^{2}+\exp^{-E_{ij^{\prime}}}\right),$
    |  | (51) |'
- en: where $\mathcal{D}=\left\{(i,j,j^{\prime})|d(i,j)<d(i,j^{\prime})\right\}$ and
    $E_{ij}=\text{KL}(\mathbf{h}_{j}||\mathbf{h}_{i})$. The authors further proposed
    an unbiased sampling strategy to accelerate the training process.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}=\left\{(i,j,j^{\prime})|d(i,j)<d(i,j^{\prime})\right\}$ 和 $E_{ij}=\text{KL}(\mathbf{h}_{j}||\mathbf{h}_{i})$。作者进一步提出了一种无偏采样策略，以加速训练过程。
- en: 5.2 Variational Autoencoders
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 变分自编码器
- en: 'Different from the aforementioned autoencoders, variational autoencoders (VAEs)
    are another type of deep learning method that combines dimensionality reduction
    with generative models. Its potential benefits include tolerating noise and learning
    smooth representations [[113](#bib.bib113)]. VAEs were first introduced to graph
    data in VGAE [[102](#bib.bib102)], where the decoder was a simple linear product:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述自编码器不同，变分自编码器（VAEs）是一种将降维与生成模型结合的深度学习方法。它的潜在好处包括容忍噪声和学习平滑表示[[113](#bib.bib113)]。VAEs
    首次引入图数据是在 VGAE [[102](#bib.bib102)] 中，其中解码器是一个简单的线性乘积：
- en: '|  | $p\left(\mathbf{A}&#124;\mathbf{H}\right)=\prod\nolimits_{i,j=1}^{N}\sigma\left(\mathbf{h}_{i}\mathbf{h}_{j}^{T}\right),$
    |  | (52) |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | $p\left(\mathbf{A}&#124;\mathbf{H}\right)=\prod\nolimits_{i,j=1}^{N}\sigma\left(\mathbf{h}_{i}\mathbf{h}_{j}^{T}\right),$
    |  | (52) |'
- en: 'in which the node representation was assumed to follow a Gaussian distribution
    $q\left(\mathbf{h}_{i}|\mathbf{M},\mathbf{\Sigma}\right)=\mathcal{N}\left(\mathbf{h}_{i}|\mathbf{M}(i,:),diag\left(\mathbf{\Sigma}(i,:)\right)\right)$.
    For the encoder of the mean and variance matrices, the authors also adopted the
    GCN proposed by Kipf and Welling [[43](#bib.bib43)]:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，节点表示被假设为遵循高斯分布 $q\left(\mathbf{h}_{i}|\mathbf{M},\mathbf{\Sigma}\right)=\mathcal{N}\left(\mathbf{h}_{i}|\mathbf{M}(i,:),diag\left(\mathbf{\Sigma}(i,:)\right)\right)$。对于均值和方差矩阵的编码器，作者还采用了Kipf和Welling提出的GCN[[43](#bib.bib43)]：
- en: '|  | $\mathbf{M}=GCN_{\mathbf{M}}\left(\mathbf{F}^{V},\mathbf{A}\right),\log\mathbf{\Sigma}=GCN_{\mathbf{\Sigma}}\left(\mathbf{F}^{V},\mathbf{A}\right).$
    |  | (53) |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{M}=GCN_{\mathbf{M}}\left(\mathbf{F}^{V},\mathbf{A}\right),\log\mathbf{\Sigma}=GCN_{\mathbf{\Sigma}}\left(\mathbf{F}^{V},\mathbf{A}\right).$
    |  | (53) |'
- en: 'Then, the model parameters were learned by minimizing the variational lower
    bound [[113](#bib.bib113)]:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过最小化变分下界来学习模型参数[[113](#bib.bib113)]：
- en: '|  | $\mathcal{L}=\mathbb{E}_{q\left(\mathbf{H}&#124;\mathbf{F}^{V},\mathbf{A}\right)}\left[\log
    p\left(\mathbf{A}&#124;\mathbf{H}\right)\right]-\text{KL}\left(q\left(\mathbf{H}&#124;\mathbf{F}^{V},\mathbf{A}\right)&#124;&#124;p(\mathbf{H})\right).$
    |  | (54) |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathbb{E}_{q\left(\mathbf{H}&#124;\mathbf{F}^{V},\mathbf{A}\right)}\left[\log
    p\left(\mathbf{A}&#124;\mathbf{H}\right)\right]-\text{KL}\left(q\left(\mathbf{H}&#124;\mathbf{F}^{V},\mathbf{A}\right)&#124;&#124;p(\mathbf{H})\right).$
    |  | (54) |'
- en: However, because this approach required reconstructing the full graph, its time
    complexity is $O(N^{2})$.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于这种方法需要重建整个图，因此其时间复杂度为 $O(N^{2})$。
- en: 'Motivated by SDNE and G2G, DVNE [[103](#bib.bib103)] proposed another VAE for
    graph data that also represented each node as a Gaussian distribution. Unlike
    the existing works that had adopted KL-divergence as the measurement, DVNE used
    the Wasserstein distance [[114](#bib.bib114)] to preserve the transitivity of
    the nodes similarities. Similar to SDNE and G2G, DVNE also preserved both the
    first and second-order proximity in its objective function:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 受 SDNE 和 G2G 启发，DVNE [[103](#bib.bib103)] 提出了另一种用于图数据的变分自编码器（VAE），它也将每个节点表示为高斯分布。与采用
    KL 散度作为度量的现有方法不同，DVNE 使用 Wasserstein 距离 [[114](#bib.bib114)] 来保持节点相似性的传递性。与 SDNE
    和 G2G 类似，DVNE 在其目标函数中也保留了第一阶和第二阶的接近度：
- en: '|  | $\min_{\bf{\Theta}}\sum\nolimits_{(i,j,j^{\prime})\in\mathcal{D}}\left(E_{ij}^{2}+\exp^{-E_{ij^{\prime}}}\right)+\alpha\mathcal{L}_{2},$
    |  | (55) |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\bf{\Theta}}\sum\nolimits_{(i,j,j^{\prime})\in\mathcal{D}}\left(E_{ij}^{2}+\exp^{-E_{ij^{\prime}}}\right)+\alpha\mathcal{L}_{2},$
    |  | (55) |'
- en: 'where $E_{ij}=W_{2}\left(\mathbf{h}_{j}||\mathbf{h}_{i}\right)$ is the $2^{nd}$
    Wasserstein distance between two Gaussian distributions $\mathbf{h}_{j}$ and $\mathbf{h}_{i}$
    and $\mathcal{D}=\left\{(i,j,j^{\prime})|j\in\mathcal{N}(i),j^{\prime}\notin\mathcal{N}(i)\right\}$
    is a set of triples corresponding to the ranking loss of the first-order proximity.
    The reconstruction loss was defined as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E_{ij}=W_{2}\left(\mathbf{h}_{j}||\mathbf{h}_{i}\right)$ 是两个高斯分布 $\mathbf{h}_{j}$
    和 $\mathbf{h}_{i}$ 之间的 $2^{nd}$ Wasserstein 距离，$\mathcal{D}=\left\{(i,j,j^{\prime})|j\in\mathcal{N}(i),j^{\prime}\notin\mathcal{N}(i)\right\}$
    是一组三元组，对应于第一阶接近度的排序损失。重建损失定义如下：
- en: '|  | $\mathcal{L}_{2}=\inf\nolimits_{q(\mathbf{Z}&#124;\mathbf{P})}\mathbb{E}_{p(\mathbf{P})}\mathbb{E}_{q(\mathbf{Z}&#124;\mathbf{P})}\left\&#124;\mathbf{P}\odot(\mathbf{P}-\mathcal{G}(\mathbf{Z}))\right\&#124;_{2}^{2},$
    |  | (56) |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{2}=\inf\nolimits_{q(\mathbf{Z}&#124;\mathbf{P})}\mathbb{E}_{p(\mathbf{P})}\mathbb{E}_{q(\mathbf{Z}&#124;\mathbf{P})}\left\&#124;\mathbf{P}\odot(\mathbf{P}-\mathcal{G}(\mathbf{Z}))\right\&#124;_{2}^{2},$
    |  | (56) |'
- en: 'where $\mathbf{P}$ is the transition matrix and $\mathbf{Z}$ represents samples
    drawn from $\mathbf{H}$. The framework is shown in Figure [9](#S5.F9 "Figure 9
    ‣ 5.2 Variational Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs:
    A Survey"). Using this approach, the objective function can be minimized as in
    conventional VAEs using the reparameterization trick [[113](#bib.bib113)].'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{P}$ 是转移矩阵，$\mathbf{Z}$ 表示从 $\mathbf{H}$ 中抽取的样本。该框架如图 [9](#S5.F9
    "Figure 9 ‣ 5.2 Variational Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning
    on Graphs: A Survey") 所示。使用这种方法，目标函数可以像常规 VAE 一样通过重参数化技巧 [[113](#bib.bib113)]
    最小化。'
- en: '![Refer to caption](img/431f56c0c1fef5a24acfc611b0d1033c.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/431f56c0c1fef5a24acfc611b0d1033c.png)'
- en: 'Figure 9: The framework of DVNE [[103](#bib.bib103)]. DVNE represents nodes
    as distributions using a VAE and adopts the Wasserstein distance to preserve the
    transitivity of the nodes similarities.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：DVNE 的框架 [[103](#bib.bib103)]。DVNE 使用 VAE 将节点表示为分布，并采用 Wasserstein 距离来保持节点相似性的传递性。
- en: 5.3 Improvements and Discussions
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 改进与讨论
- en: Several improvements have also been proposed for GAEs.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 也提出了几项对图自编码器（GAEs）的改进。
- en: 5.3.1 Adversarial Training
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 对抗训练
- en: 'An adversarial training scheme⁹⁹9We will discuss more adversarial methods for
    graphs in Section [7](#S7 "7 Graph Adversarial Methods ‣ Deep Learning on Graphs:
    A Survey"). was incorporated into GAEs as an additional regularization term in
    ARGA [[104](#bib.bib104)]. The overall architecture is shown in Figure [10](#S5.F10
    "Figure 10 ‣ 5.3.1 Adversarial Training ‣ 5.3 Improvements and Discussions ‣ 5
    Graph Autoencoders ‣ Deep Learning on Graphs: A Survey"). Specifically, the encoder
    of GAEs was used as the generator while the discriminator aimed to distinguish
    whether a latent representation came from the generator or from a prior distribution.
    In this way, the autoencoder was forced to match the prior distribution as a regularization.
    The objective function was:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '一种对抗训练方案⁹⁹9 我们将在第 [7](#S7 "7 Graph Adversarial Methods ‣ Deep Learning on Graphs:
    A Survey") 节进一步讨论更多图的对抗方法。作为 ARGA [[104](#bib.bib104)] 的附加正则化项被纳入了 GAEs。总体架构如图
    [10](#S5.F10 "Figure 10 ‣ 5.3.1 Adversarial Training ‣ 5.3 Improvements and Discussions
    ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey") 所示。具体而言，GAEs 的编码器被用作生成器，而鉴别器则旨在区分潜在表示是来自生成器还是来自先验分布。通过这种方式，自动编码器被迫将其匹配到先验分布作为正则化。目标函数为：'
- en: '|  | $\min_{\bf{\Theta}}\mathcal{L}_{2}+\alpha\mathcal{L}_{GAN},$ |  | (57)
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\bf{\Theta}}\mathcal{L}_{2}+\alpha\mathcal{L}_{GAN},$ |  | (57)
    |'
- en: where $\mathcal{L}_{2}$ is the reconstruction loss in GAEs and $\mathcal{L}_{GAN}$
    is
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{2}$ 是 GAEs 中的重构损失，$\mathcal{L}_{GAN}$ 是
- en: '|  | $\min_{\mathcal{G}}\max_{\mathcal{D}}\mathbb{E}_{\mathbf{h}\sim p_{\mathbf{h}}}\left[\log\mathcal{D}(\mathbf{h})\right]+\mathbb{E}_{\mathbf{z}\sim\mathcal{G}(\mathbf{F}^{V},\mathbf{A})}\left[\log\left(1-\mathcal{D}\left(\mathbf{z}\right)\right)\right],$
    |  | (58) |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\mathcal{G}}\max_{\mathcal{D}}\mathbb{E}_{\mathbf{h}\sim p_{\mathbf{h}}}\left[\log\mathcal{D}(\mathbf{h})\right]+\mathbb{E}_{\mathbf{z}\sim\mathcal{G}(\mathbf{F}^{V},\mathbf{A})}\left[\log\left(1-\mathcal{D}\left(\mathbf{z}\right)\right)\right],$
    |  | (58) |'
- en: 'where $\mathcal{G}\left(\mathbf{F}^{V},\mathbf{A}\right)$ is a generator that
    uses the graph convolutional encoder from Eq. ([53](#S5.E53 "In 5.2 Variational
    Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")), $\mathcal{D}(\cdot)$
    is a discriminator based on the cross-entropy loss, and $p_{\mathbf{h}}$ is the
    prior distribution. The study adopted a simple Gaussian prior, and the experimental
    results demonstrated the effectiveness of the adversarial training scheme.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathcal{G}\left(\mathbf{F}^{V},\mathbf{A}\right)$ 是一个生成器，使用了来自 Eq. ([53](#S5.E53
    "In 5.2 Variational Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs:
    A Survey")) 的图卷积编码器，$\mathcal{D}(\cdot)$ 是基于交叉熵损失的鉴别器，$p_{\mathbf{h}}$ 是先验分布。研究采用了简单的高斯先验，实验结果表明对抗训练方案的有效性。'
- en: '![Refer to caption](img/dbadf30d7fdac7eee97de8a214b9def1.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dbadf30d7fdac7eee97de8a214b9def1.png)'
- en: 'Figure 10: The framework of ARGA/ARVGA reprinted from [[104](#bib.bib104)]
    with permission. This model incorporates the adversarial training scheme into
    GAEs.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：AGA/ARVGA 框架转载自 [[104](#bib.bib104)]，经许可使用。该模型将对抗训练方案融入了 GAEs 中。
- en: 'Concurrently, NetRA [[105](#bib.bib105)] also proposed using a generative adversarial
    network (GAN) [[115](#bib.bib115)] to enhance the generalization ability of graph
    autoencoders. Specifically, the authors used the following objective function:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，NetRA [[105](#bib.bib105)] 也提出使用生成对抗网络 (GAN) [[115](#bib.bib115)] 来增强图自编码器的泛化能力。具体而言，作者使用了以下目标函数：
- en: '|  | $\min_{\bf{\Theta}}\mathcal{L}_{2}+\alpha_{1}\mathcal{L}_{LE}+\alpha_{2}\mathcal{L}_{GAN},$
    |  | (59) |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\bf{\Theta}}\mathcal{L}_{2}+\alpha_{1}\mathcal{L}_{LE}+\alpha_{2}\mathcal{L}_{GAN},$
    |  | (59) |'
- en: 'where $\mathcal{L}_{LE}$ is the Laplacian eigenmaps objective function shown
    in Eq. ([44](#S5.E44 "In 5.1 Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning
    on Graphs: A Survey")). In addition, the authors adopted an LSTM as the encoder
    to aggregate information from neighborhoods similar to Eq. ([48](#S5.E48 "In 5.1
    Autoencoders ‣ 5 Graph Autoencoders ‣ Deep Learning on Graphs: A Survey")). Instead
    of sampling only immediate neighbors and ordering the nodes using degrees as in
    DRNE [[100](#bib.bib100)], the authors used random walks to generate the input
    sequences. In contrast to ARGA, NetRA considered the representations in GAEs as
    the ground-truth and adopted random Gaussian noises followed by an MLP as the
    generator.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{L}_{LE}$是拉普拉斯特征图目标函数，如公式([44](#S5.E44 "在5.1自编码器 ‣ 5图自编码器 ‣ 图上的深度学习：综述"))所示。此外，作者采用了LSTM作为编码器，以汇聚来自邻域的信息，类似于公式([48](#S5.E48
    "在5.1自编码器 ‣ 5图自编码器 ‣ 图上的深度学习：综述"))。与DRNE [[100](#bib.bib100)]中仅采样直接邻居并使用度排序节点不同，作者使用随机游走生成输入序列。与ARGA相比，NetRA将GAEs中的表示视为真实值，并采用随机高斯噪声，然后使用MLP作为生成器。
- en: 5.3.2 Inductive Learning
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 归纳学习
- en: Similar to GCNs, GAEs can be applied to the inductive learning setting if node
    attributes are incorporated in the encoder. This can be achieved by using a GCN
    as the encoder, such as in GC-MC  [[99](#bib.bib99)], VGAE [[102](#bib.bib102)],
    and VGAE [[104](#bib.bib104)], or by directly learning a mapping function from
    node features as in G2G [[101](#bib.bib101)]. Because the edge information is
    utilized only when learning the parameters, the model can also be applied to nodes
    unseen during training. These works also show that although GCNs and GAEs are
    based on different architectures, it is possible to use them jointly, which we
    believe is a promising future direction.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GCNs，GAEs也可以应用于归纳学习设置，只要在编码器中包含节点属性。这可以通过使用GCN作为编码器来实现，例如在GC-MC [[99](#bib.bib99)]、VGAE [[102](#bib.bib102)]和VGAE [[104](#bib.bib104)]中，或者通过直接从节点特征中学习映射函数，如G2G [[101](#bib.bib101)]。由于边信息仅在学习参数时使用，模型也可以应用于训练期间未见过的节点。这些工作还表明，尽管GCNs和GAEs基于不同的架构，但它们可以联合使用，我们相信这是一个有前景的未来方向。
- en: 5.3.3 Similarity Measures
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 相似度度量
- en: In GAEs, many similarity measures have been adopted, for example, L2-reconstruction
    loss, Laplacian eigenmaps, and the ranking loss for graph AEs, and KL divergence
    and Wasserstein distance for graph VAEs. Although these similarity measures are
    based on different motivations, how to choose an appropriate similarity measure
    for a given task and model architecture remains unstudied. More research is needed
    to understand the underlying differences between these metrics.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAEs中，已经采用了许多相似度度量，例如L2重建损失、拉普拉斯特征图和图AE的排序损失，以及图VAE的KL散度和Wasserstein距离。尽管这些相似度度量基于不同的动机，但如何为特定任务和模型架构选择合适的相似度度量仍未被研究清楚。需要更多的研究来理解这些度量之间的根本差异。
- en: 'TABLE VI: The Main Characteristics of Graph Reinforcement Learning'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 表VI：图强化学习的主要特征
- en: '| Method | Task | Actions | Rewards | Time Complexity |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 任务 | 动作 | 奖励 | 时间复杂度 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GCPN [[116](#bib.bib116)] | Graph generation | Link prediction | GAN + domain
    knowledge | $O(MN)$ |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| GCPN [[116](#bib.bib116)] | 图生成 | 链接预测 | GAN + 领域知识 | $O(MN)$ |'
- en: '| MolGAN [[117](#bib.bib117)] | Graph generation | Generate the entire graph
    | GAN + domain knowledge | $O(N^{2})$ |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| MolGAN [[117](#bib.bib117)] | 图生成 | 生成整个图 | GAN + 领域知识 | $O(N^{2})$ |'
- en: '| GTPN [[118](#bib.bib118)] | Chemical reaction prediction | Predict node pairs
    and new bonding types | Prediction results | $O(N^{2})$ |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| GTPN [[118](#bib.bib118)] | 化学反应预测 | 预测节点对和新键类型 | 预测结果 | $O(N^{2})$ |'
- en: '| GAM [[119](#bib.bib119)] | Graph classification | Predict graph labels and
    select the next node | Classification results | $O(d_{\text{avg}}sT)$ |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| GAM [[119](#bib.bib119)] | 图分类 | 预测图标签和选择下一个节点 | 分类结果 | $O(d_{\text{avg}}sT)$
    |'
- en: '| DeepPath [[120](#bib.bib120)] | Knowledge graph reasoning | Predict the next
    node of the reasoning path | Reasoning results + diversity | $O(d_{\text{avg}}sT+s^{2}T)$
    |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| DeepPath [[120](#bib.bib120)] | 知识图谱推理 | 预测推理路径上的下一个节点 | 推理结果 + 多样性 | $O(d_{\text{avg}}sT+s^{2}T)$
    |'
- en: '| MINERVA [[121](#bib.bib121)] | Knowledge graph reasoning | Predict the next
    node of the reasoning path | Reasoning results | $O(d_{\text{avg}}sT)$ |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| MINERVA [[121](#bib.bib121)] | 知识图谱推理 | 预测推理路径上的下一个节点 | 推理结果 | $O(d_{\text{avg}}sT)$
    |'
- en: 6 Graph Reinforcement Learning
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 图强化学习
- en: 'One aspect of deep learning not yet discussed is reinforcement learning (RL),
    which has been shown to be effective in AI tasks such as playing games [[122](#bib.bib122)].
    RL is known to be good at learning from feedbacks, especially when dealing with
    non-differentiable objectives and constraints. In this section, we review Graph
    RL methods. Their main characteristics are summarized in Table [VI](#S5.T6 "TABLE
    VI ‣ 5.3.3 Similarity Measures ‣ 5.3 Improvements and Discussions ‣ 5 Graph Autoencoders
    ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '尚未讨论的深度学习一个方面是强化学习（RL），它已被证明在AI任务中，如玩游戏，效果显著[[122](#bib.bib122)]。RL 被认为擅长从反馈中学习，特别是在处理不可微分目标和约束时。在这一节中，我们回顾了图RL方法。其主要特征总结在表 [VI](#S5.T6
    "TABLE VI ‣ 5.3.3 Similarity Measures ‣ 5.3 Improvements and Discussions ‣ 5 Graph
    Autoencoders ‣ Deep Learning on Graphs: A Survey")。'
- en: GCPN [[116](#bib.bib116)] utilized RL to generate goal-directed molecular graphs
    while considering non-differential objectives and constraints. Specifically, the
    graph generation is modeled as a Markov decision process of adding nodes and edges,
    and the generative model is regarded as an RL agent operating in the graph generation
    environment. By treating agent actions as link predictions, using domain-specific
    as well as adversarial rewards, and using GCNs to learn the node representations,
    GCPN can be trained in an end-to-end manner using a policy gradient [[123](#bib.bib123)].
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: GCPN [[116](#bib.bib116)] 利用RL生成目标导向的分子图，同时考虑不可微分目标和约束。具体而言，图生成被建模为添加节点和边的马尔可夫决策过程，生成模型被视为在图生成环境中操作的RL代理。通过将代理动作视为链接预测，使用领域特定和对抗奖励，以及使用GCN来学习节点表示，GCPN可以使用策略梯度[[123](#bib.bib123)]以端到端的方式进行训练。
- en: A concurrent work, MolGAN [[117](#bib.bib117)], adopted a similar idea of using
    RL for generating molecular graphs. However, rather than generating the graph
    through a sequence of actions, MolGAN proposed directly generating the full graph;
    this approach worked particularly well for small molecules.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的工作，MolGAN [[117](#bib.bib117)]，采用了类似的使用RL生成分子图的想法。然而，MolGAN提出直接生成完整图形，而不是通过一系列动作生成图形；这种方法在处理小分子时特别有效。
- en: GTPN [[118](#bib.bib118)] adopted RL to predict chemical reaction products.
    Specifically, the agent acted to select node pairs in the molecule graph and predicted
    their new bonding types, and rewards were given both immediately and at the end
    based on whether the predictions were correct. GTPN used a GCN to learn the node
    representations and an RNN to memorize the prediction sequence.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: GTPN [[118](#bib.bib118)] 采用了RL来预测化学反应产物。具体来说，代理行动以选择分子图中的节点对并预测它们的新键合类型，奖励根据预测是否正确在立即和结束时给予。GTPN
    使用了GCN来学习节点表示，并使用RNN来记忆预测序列。
- en: 'GAM [[119](#bib.bib119)] applied RL to graph classification by using random
    walks. The authors modeled the generation of random walks as a partially observable
    Markov decision process (POMDP). The agent performed two actions: first, it predicted
    the label of the graph; then, it selected the next node in the random walk. The
    reward was determined simply by whether the agent correctly classified the graph,
    i.e.,'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: GAM [[119](#bib.bib119)] 通过使用随机游走将强化学习（RL）应用于图分类。作者将随机游走的生成建模为部分可观察马尔可夫决策过程（POMDP）。代理执行了两个动作：首先，它预测图的标签；然后，它选择随机游走中的下一个节点。奖励仅由代理是否正确分类图来决定，即：
- en: '|  | $\mathcal{J}(\theta)=\mathbb{E}_{P(S_{1:T};\theta)}\sum\nolimits_{t=1}^{T}r_{t},$
    |  | (60) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{J}(\theta)=\mathbb{E}_{P(S_{1:T};\theta)}\sum\nolimits_{t=1}^{T}r_{t},$
    |  | (60) |'
- en: where $r_{t}=1$ represents a correct prediction; otherwise, $r_{t}=-1$. $T$
    is the total time steps and $S_{t}$ is the environment.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r_{t}=1$ 表示正确预测；否则，$r_{t}=-1$。$T$ 是总时间步数，$S_{t}$ 是环境。
- en: DeepPath [[120](#bib.bib120)] and MINERVA [[121](#bib.bib121)] both adopted
    RL for knowledge graph (KG) reasoning. Specifically, DeepPath targeted at pathfinding,
    i.e., find the most informative path between two target nodes, while MINERVA tackled
    question-answering tasks, i.e., find the correct answer node given a question
    node and a relation. In both methods, the RL agents need to predict the next node
    in the path at each step and output a reasoning path in the KG. Agents receive
    rewards if the paths reach the correct destinations. DeepPath also added a regularization
    term to encourage the path diversity.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: DeepPath [[120](#bib.bib120)] 和 MINERVA [[121](#bib.bib121)] 都采用了强化学习（RL）进行知识图谱（KG）推理。具体来说，DeepPath
    旨在路径查找，即找到两个目标节点之间最具信息性的路径，而 MINERVA 处理问答任务，即给定一个问题节点和一个关系，找到正确的答案节点。在这两种方法中，RL
    代理需要在每一步预测路径中的下一个节点，并输出 KG 中的推理路径。如果路径到达正确的目标，代理将获得奖励。DeepPath 还添加了一个正则化项来鼓励路径的多样性。
- en: 'TABLE VII: The Main Characteristics of Graph Adversarial Methods'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 VII: 图对抗方法的主要特点'
- en: '| Category | Method |  Adversarial Methods | Time Complexity | Node Features
    |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 |  对抗方法 | 时间复杂度 | 节点特征 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Adversarial Training | ARGA/ARVGA [[104](#bib.bib104)] | Regularization for
    GAEs | $O(N^{2})$ | Yes |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 对抗训练 | ARGA/ARVGA [[104](#bib.bib104)] | GAEs 的正则化 | $O(N^{2})$ | 是 |'
- en: '| NetRA [[105](#bib.bib105)] | Regularization for GAEs | $O(M)$ | No |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| NetRA [[105](#bib.bib105)] | GAEs 的正则化 | $O(M)$ | 否 |'
- en: '| GCPN [[116](#bib.bib116)] | Rewards for Graph RL | $O(MN)$ | Yes |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| GCPN [[116](#bib.bib116)] | 图强化学习的奖励 | $O(MN)$ | 是 |'
- en: '| MolGAN [[117](#bib.bib117)] | Rewards for Graph RL | $O(N^{2})$ | Yes |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| MolGAN [[117](#bib.bib117)] | 图强化学习的奖励 | $O(N^{2})$ | 是 |'
- en: '| GraphGAN [[124](#bib.bib124)] | Generation of negative samples (i.e., node
    pairs) | $O(MN)$ | No |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| GraphGAN [[124](#bib.bib124)] | 生成负样本（即节点对） | $O(MN)$ | 否 |'
- en: '| ANE [[125](#bib.bib125)] | Regularization for network embedding | $O(N)$
    | No |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| ANE [[125](#bib.bib125)] | 网络嵌入的正则化 | $O(N)$ | 否 |'
- en: '| GraphSGAN [[126](#bib.bib126)] | Enhancing semi-supervised learning on graphs
    | $O(N^{2})$ | Yes |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| GraphSGAN [[126](#bib.bib126)] | 增强图上的半监督学习 | $O(N^{2})$ | 是 |'
- en: '| NetGAN [[127](#bib.bib127)] | Generation of graphs via random walks | $O(M)$
    | No |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| NetGAN [[127](#bib.bib127)] | 通过随机游走生成图 | $O(M)$ | 否 |'
- en: '| Adversarial Attack | Nettack [[128](#bib.bib128)] | Targeted attacks of graph
    structures and node attributes | $O(Nd_{0}^{2})$ | Yes |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 对抗攻击 | Nettack [[128](#bib.bib128)] | 针对图结构和节点属性的攻击 | $O(Nd_{0}^{2})$ | 是
    |'
- en: '| Dai et al. [[129](#bib.bib129)] | Targeted attacks of graph structures |
    $O(M)$ | No |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| Dai et al. [[129](#bib.bib129)] | 针对图结构的攻击 | $O(M)$ | 否 |'
- en: '| Zugner and Gunnemann [[130](#bib.bib130)] | Non-targeted attacks of graph
    structures | $O(N^{2})$ | No |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| Zugner 和 Gunnemann [[130](#bib.bib130)] | 非针对图结构的攻击 | $O(N^{2})$ | 否 |'
- en: 7 Graph Adversarial Methods
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 图对抗方法
- en: 'Adversarial methods such as GANs [[115](#bib.bib115)] and adversarial attacks
    have drawn increasing attention in the machine learning community in recent years.
    In this section, we review how to apply adversarial methods to graphs. The main
    characteristics of graph adversarial methods are summarized in Table [VII](#S6.T7
    "TABLE VII ‣ 6 Graph Reinforcement Learning ‣ Deep Learning on Graphs: A Survey").'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '对抗方法如GANs [[115](#bib.bib115)] 和对抗攻击近年来在机器学习社区中引起了越来越多的关注。在这一部分，我们回顾了如何将对抗方法应用于图数据。图对抗方法的主要特点总结在表格 [VII](#S6.T7
    "TABLE VII ‣ 6 Graph Reinforcement Learning ‣ Deep Learning on Graphs: A Survey")中。'
- en: 7.1 Adversarial Training
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 对抗训练
- en: 'The basic idea behind a GAN is to build two linked models: a discriminator
    and a generator. The goal of the generator is to “fool” the discriminator by generating
    fake data, while the discriminator aims to distinguish whether a sample comes
    from real data or is generated by the generator. Subsequently, both models benefit
    from each other by joint training using a minimax game. Adversarial training has
    been shown to be effective in generative models and enhancing the generalization
    ability of discriminative models. In Section [5.3.1](#S5.SS3.SSS1 "5.3.1 Adversarial
    Training ‣ 5.3 Improvements and Discussions ‣ 5 Graph Autoencoders ‣ Deep Learning
    on Graphs: A Survey") and Section [6](#S6 "6 Graph Reinforcement Learning ‣ Deep
    Learning on Graphs: A Survey"), we reviewed how adversarial training schemes are
    used in GAEs and Graph RL, respectively. Here, we review several other adversarial
    training methods on graphs in detail.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 的基本思想是建立两个关联模型：判别器和生成器。生成器的目标是通过生成虚假数据来“欺骗”判别器，而判别器则旨在区分样本是来自真实数据还是由生成器生成。随后，通过使用极小极大游戏进行联合训练，两个模型互相受益。对抗训练已被证明在生成模型中有效，并增强了判别模型的泛化能力。在第
    [5.3.1](#S5.SS3.SSS1 "5.3.1 对抗训练 ‣ 5.3 改进与讨论 ‣ 5 图自动编码器 ‣ 图上的深度学习：综述") 节和第 [6](#S6
    "6 图强化学习 ‣ 图上的深度学习：综述") 节中，我们回顾了对抗训练方案在 GAEs 和图强化学习中的应用。这里，我们详细回顾了其他几种图上的对抗训练方法。
- en: 'GraphGAN [[124](#bib.bib124)] proposed using a GAN to enhance graph embedding
    methods [[17](#bib.bib17)] with the following objective function:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: GraphGAN [[124](#bib.bib124)] 提出了使用 GAN 来增强图嵌入方法 [[17](#bib.bib17)]，其目标函数如下：
- en: '|  | $\displaystyle\min_{\mathcal{G}}\max_{\mathcal{D}}\sum\nolimits_{i=1}^{N}$
    | $\displaystyle\left(\mathbb{E}_{v\sim p_{graph}(\cdot&#124;v_{i})}\left[\log\mathcal{D}(v,v_{i})\right]\right.$
    |  | (61) |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\mathcal{G}}\max_{\mathcal{D}}\sum\nolimits_{i=1}^{N}$
    | $\displaystyle\left(\mathbb{E}_{v\sim p_{graph}(\cdot&#124;v_{i})}\left[\log\mathcal{D}(v,v_{i})\right]\right.$
    |  | (61) |'
- en: '|  | $\displaystyle+$ | $\displaystyle\left.\mathbb{E}_{v\sim\mathcal{G}(\cdot&#124;v_{i})}\left[\log\left(1-\mathcal{D}\left(v,v_{i}\right)\right)\right]\right).$
    |  |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+$ | $\displaystyle\left.\mathbb{E}_{v\sim\mathcal{G}(\cdot&#124;v_{i})}\left[\log\left(1-\mathcal{D}\left(v,v_{i}\right)\right)\right]\right).$
    |  |'
- en: 'The discriminator $\mathcal{D}(\cdot)$ and the generator $\mathcal{G}(\cdot)$
    are as follows:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器 $\mathcal{D}(\cdot)$ 和生成器 $\mathcal{G}(\cdot)$ 如下：
- en: '|  | $\begin{gathered}\mathcal{D}(v,v_{i})=\sigma(\mathbf{d}_{v}\mathbf{d}_{v_{i}}^{T}),\mathcal{G}(v&#124;v_{i})=\frac{\exp(\mathbf{g}_{v}\mathbf{g}_{v_{i}}^{T})}{\sum_{v^{\prime}\neq
    v_{i}}\exp(\mathbf{g}_{v^{\prime}}\mathbf{g}_{v_{i}}^{T})},\end{gathered}$ |  |
    (62) |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}\mathcal{D}(v,v_{i})=\sigma(\mathbf{d}_{v}\mathbf{d}_{v_{i}}^{T}),\mathcal{G}(v&#124;v_{i})=\frac{\exp(\mathbf{g}_{v}\mathbf{g}_{v_{i}}^{T})}{\sum_{v^{\prime}\neq
    v_{i}}\exp(\mathbf{g}_{v^{\prime}}\mathbf{g}_{v_{i}}^{T})},\end{gathered}$ |  |
    (62) |'
- en: 'where $\mathbf{d}_{v}$ and $\mathbf{g}_{v}$ are the low-dimensional embedding
    vectors for node $v$ in the discriminator and the generator, respectively. Combining
    the above equations, the discriminator actually has two objectives: the node pairs
    in the original graph should possess large similarities, while the node pairs
    generated by the generator should possess small similarities. This architecture
    is similar to network embedding methods such as LINE [[108](#bib.bib108)], except
    that negative node pairs are generated by the generator $\mathcal{G}(\cdot)$ instead
    of by random samplings. The authors showed that this method enhanced the inference
    abilities of the node embedding vectors.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{d}_{v}$ 和 $\mathbf{g}_{v}$ 分别是判别器和生成器中节点 $v$ 的低维嵌入向量。结合上述方程，判别器实际上有两个目标：原始图中的节点对应具有较大的相似度，而生成器生成的节点对则具有较小的相似度。该架构类似于网络嵌入方法如
    LINE [[108](#bib.bib108)]，不同之处在于负节点对由生成器 $\mathcal{G}(\cdot)$ 生成，而不是通过随机采样。作者表明，这种方法增强了节点嵌入向量的推理能力。
- en: Adversarial network embedding (ANE) [[125](#bib.bib125)] also adopted an adversarial
    training scheme to improve network embedding methods. Similar to ARGA [[104](#bib.bib104)],
    ANE used a GAN as an additional regularization term to existing network embedding
    methods such as DeepWalk [[131](#bib.bib131)] by imposing a prior distribution
    as the real data and regarding the embedding vectors as generated samples.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗网络嵌入（ANE） [[125](#bib.bib125)] 也采用了一种对抗训练方案来改进网络嵌入方法。类似于 ARGA [[104](#bib.bib104)]，ANE
    通过将生成对抗网络（GAN）作为现有网络嵌入方法（如 DeepWalk [[131](#bib.bib131)]）的额外正则化项，引入先验分布作为真实数据，并将嵌入向量视为生成样本。
- en: GraphSGAN [[126](#bib.bib126)] used a GAN to enhance semi-supervised learning
    on graphs. Specifically, the authors observed that fake nodes should be generated
    in the density gaps between subgraphs to weaken the propagation effect across
    different clusters of the existing models. To achieve that goal, the authors designed
    a novel optimization objective with elaborate loss terms to ensure that the generator
    generated samples in the density gaps at equilibrium.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: GraphSGAN [[126](#bib.bib126)] 使用GAN来增强图上的半监督学习。具体来说，作者观察到，假节点应在子图之间的密度间隙中生成，以减弱现有模型中不同簇之间的传播效应。为实现这一目标，作者设计了一个新颖的优化目标，配以精细的损失项，以确保生成器在密度间隙中生成平衡的样本。
- en: NetGAN [[127](#bib.bib127)] adopted a GAN for graph generation tasks. Specifically,
    the authors regarded graph generation as a task to learn the distribution of biased
    random walks and adopted a GAN framework to generate and discriminate among random
    walks using an LSTM. The experiments showed that using random walks could also
    learn global network patterns.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: NetGAN [[127](#bib.bib127)] 采用了生成对抗网络（GAN）进行图生成任务。具体来说，作者将图生成视为学习偏置随机游走分布的任务，并采用GAN框架来生成和区分随机游走，使用了LSTM。实验表明，使用随机游走也可以学习到全局网络模式。
- en: 7.2 Adversarial Attacks
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 对抗攻击
- en: Adversarial attacks are another class of adversarial methods intended to deliberately
    “fool” the targeted methods by adding small perturbations to data. Studying adversarial
    attacks can deepen our understanding of the existing models and inspire more robust
    architectures. We review the graph-based adversarial attacks below.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击是另一类对抗性方法，其目的是通过向数据中添加小的扰动来故意“欺骗”目标方法。研究对抗攻击可以加深我们对现有模型的理解，并激发出更稳健的架构。我们在下面回顾了基于图的对抗攻击。
- en: 'Nettack [[128](#bib.bib128)] first proposed attacking node classification models
    such as GCNs by modifying graph structures and node attributes. Denoting the targeted
    node as $v_{0}$ and its true class as $c_{true}$, the targeted model as $\mathcal{F}(\mathbf{A},\mathbf{F}^{V})$
    and its loss function as $\mathcal{L}_{\mathcal{F}}(\mathbf{A},\mathbf{F}^{V})$,
    the model adopted the following objective function:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: Nettack [[128](#bib.bib128)] 首次提出通过修改图结构和节点属性来攻击节点分类模型，如GCNs。设定目标节点为$v_{0}$，其真实类别为$c_{true}$，目标模型为$\mathcal{F}(\mathbf{A},\mathbf{F}^{V})$，其损失函数为$\mathcal{L}_{\mathcal{F}}(\mathbf{A},\mathbf{F}^{V})$，该模型采用了以下目标函数：
- en: '|  | $\begin{gathered}\operatorname*{argmax}_{\left(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}\right)\in\mathcal{P}}\;\max_{c\neq
    c_{true}}\log\mathbf{Z}^{*}_{v_{0},c}-\log\mathbf{Z}^{*}_{v_{0},c_{true}}\\ s.t.\;\mathbf{Z}^{*}=\mathcal{F}_{\theta^{*}}(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}),\theta^{*}=\operatorname*{argmin}\nolimits_{\theta}\mathcal{L}_{\mathcal{F}}(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}),\end{gathered}$
    |  | (63) |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}\operatorname*{argmax}_{\left(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}\right)\in\mathcal{P}}\;\max_{c\neq
    c_{true}}\log\mathbf{Z}^{*}_{v_{0},c}-\log\mathbf{Z}^{*}_{v_{0},c_{true}}\\ s.t.\;\mathbf{Z}^{*}=\mathcal{F}_{\theta^{*}}(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}),\theta^{*}=\operatorname*{argmin}\nolimits_{\theta}\mathcal{L}_{\mathcal{F}}(\mathbf{A}^{\prime},\mathbf{F}^{V\prime}),\end{gathered}$
    |  | (63) |'
- en: where $\mathbf{A}^{\prime}$ and $\mathbf{F}^{V\prime}$ are the modified adjacency
    matrix and node feature matrix, respectively, $\mathbf{Z}$ represents the classification
    probabilities predicted by $\mathcal{F}(\cdot)$, and $\mathcal{P}$ is the space
    determined by the attack constraints. Simply speaking, the optimization aims to
    find the best legitimate changes in graph structures and node attributes to cause
    $v_{0}$ to be misclassified. The $\theta^{*}$ indicates that the attack is causative,
    i.e., the attack occurs before training the targeted model. The authors proposed
    several constraints for the attacks. The most important constraint is that the
    attack should be “unnoticeable”, i.e., it should make only small changes. Specifically,
    the authors proposed to preserve data characteristics such as node degree distributions
    and feature co-occurrences. The authors also proposed two attacking scenarios,
    direct attack (directly attacking $v_{0}$) and influence attack (only attacking
    other nodes), and several relaxations to make the optimization tractable.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{A}^{\prime}$ 和 $\mathbf{F}^{V\prime}$ 分别是修改后的邻接矩阵和节点特征矩阵，$\mathbf{Z}$
    代表 $\mathcal{F}(\cdot)$ 预测的分类概率，$\mathcal{P}$ 是攻击约束决定的空间。简而言之，优化旨在找到图结构和节点属性中的最佳合法变化，以使
    $v_{0}$ 被误分类。$\theta^{*}$ 表示攻击是因果的，即攻击发生在训练目标模型之前。作者提出了几种攻击约束。最重要的约束是攻击应“不可察觉”，即只应做出小的变化。具体来说，作者建议保留数据特征，例如节点度分布和特征共现。作者还提出了两种攻击场景，直接攻击（直接攻击
    $v_{0}$）和影响攻击（仅攻击其他节点），以及几种放宽条件以使优化可行。
- en: 'Concurrently, Dai et al. [[129](#bib.bib129)] studied adversarial attacks for
    graphs with an objective function similar to Eq. ([63](#S7.E63 "In 7.2 Adversarial
    Attacks ‣ 7 Graph Adversarial Methods ‣ Deep Learning on Graphs: A Survey"));
    however, they focused on the case in which only graph structures were changed.
    Instead of assuming that the attacker possessed all the information, the authors
    considered several settings in which different amounts of information were available.
    The most effective strategy, RL-S2V, adopted structure2vec [[132](#bib.bib132)]
    to learn the node and graph representations and used reinforcement learning to
    solve the optimization. The experimental results showed that the attacks were
    effective for both node and graph classification tasks.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '与此同时，Dai 等人 [[129](#bib.bib129)] 研究了具有类似目标函数的图对抗攻击，如 Eq. ([63](#S7.E63 "In
    7.2 Adversarial Attacks ‣ 7 Graph Adversarial Methods ‣ Deep Learning on Graphs:
    A Survey"))；然而，他们专注于仅改变图结构的情况。与假设攻击者掌握所有信息不同，作者考虑了几种不同信息量的设置。最有效的策略，RL-S2V，采用了
    structure2vec [[132](#bib.bib132)] 来学习节点和图表示，并使用强化学习解决优化问题。实验结果表明，这些攻击在节点和图分类任务中都是有效的。'
- en: The aforementioned two attacks are targeted, i.e., they are intended to cause
    misclassification of some targeted node $v_{0}$. Zugner and Gunnemann [[130](#bib.bib130)]
    were the first to study non-targeted attacks, which were intended to reduce the
    overall model performance. They treated the graph structure as hyper-parameters
    to be optimized and adopted meta-gradients in the optimization process, along
    with several techniques to approximate the meta-gradients.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 前述两种攻击都是针对性的，即它们旨在导致某个目标节点 $v_{0}$ 被误分类。Zugner 和 Gunnemann [[130](#bib.bib130)]
    首次研究了非针对性攻击，这些攻击旨在降低整体模型性能。他们将图结构视为需要优化的超参数，并在优化过程中采用了元梯度，结合几种技术来近似元梯度。
- en: 8 Discussions and Conclusion
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论与结论
- en: Thus far, we have reviewed the different graph-based deep learning architectures
    as well as their similarities and differences. Next, we briefly discuss their
    applications, implementations, and future directions before summarizing this paper.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回顾了不同的图基深度学习架构及其相似性和差异。接下来，我们简要讨论它们的应用、实现和未来方向，然后总结本文。
- en: 8.1 Applications
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 应用
- en: 'In addition to standard graph inference tasks such as node or graph classification^(10)^(10)10A
    collection of methods for common tasks is listed in Appendix [B](#A2 "Appendix
    B Applicability for Common Tasks ‣ Deep Learning on Graphs: A Survey")., graph-based
    deep learning methods have also been applied to a wide range of disciplines, including
    modeling social influence [[133](#bib.bib133)], recommendation [[28](#bib.bib28),
    [66](#bib.bib66), [99](#bib.bib99), [134](#bib.bib134)], chemistry and biology [[55](#bib.bib55),
    [52](#bib.bib52), [46](#bib.bib46), [116](#bib.bib116), [117](#bib.bib117)], physics [[135](#bib.bib135),
    [136](#bib.bib136)], disease and drug prediction [[137](#bib.bib137), [138](#bib.bib138),
    [139](#bib.bib139)], gene expression [[140](#bib.bib140)], natural language processing
    (NLP) [[141](#bib.bib141), [142](#bib.bib142)], computer vision [[143](#bib.bib143),
    [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146), [147](#bib.bib147)],
    traffic forecasting [[148](#bib.bib148), [149](#bib.bib149)], program induction [[150](#bib.bib150)],
    solving graph-based NP problems [[151](#bib.bib151), [152](#bib.bib152)], and
    multi-agent AI systems [[153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155)].'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准的图推断任务，如节点或图分类^(10)^(10)10一些常见任务的方法在附录[B](#A2 "附录B 通用任务的适用性 ‣ 图上的深度学习：一项调查")中列出。图形深度学习方法还被应用到广泛的学科领域，包括建模社会影响[[133](#bib.bib133)]、推荐[[28](#bib.bib28),
    [66](#bib.bib66), [99](#bib.bib99), [134](#bib.bib134)]、化学和生物学[[55](#bib.bib55),
    [52](#bib.bib52), [46](#bib.bib46), [116](#bib.bib116), [117](#bib.bib117)]、物理学[[135](#bib.bib135),
    [136](#bib.bib136)]、疾病和药物预测[[137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139)]、基因表达[[140](#bib.bib140)]、自然语言处理（NLP）[[141](#bib.bib141),
    [142](#bib.bib142)]、计算机视觉[[143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145),
    [146](#bib.bib146), [147](#bib.bib147)]、交通预测[[148](#bib.bib148), [149](#bib.bib149)]、程序归纳[[150](#bib.bib150)]、解决基于图的
    NP 问题[[151](#bib.bib151), [152](#bib.bib152)]以及多智能体人工智能系统[[153](#bib.bib153),
    [154](#bib.bib154), [155](#bib.bib155)]。
- en: A thorough review of these methods is beyond the scope of this paper due to
    the sheer diversity of these applications; however, we list several key inspirations.
    First, it is important to incorporate domain knowledge into the model when constructing
    a graph or choosing architectures. For example, building a graph based on the
    relative distance may be suitable for traffic forecasting problems, but may not
    work well for a weather prediction problem where the geographical location is
    also important. Second, a graph-based model can usually be built on top of other
    architectures rather than as a stand-alone model. For example, the computer vision
    community usually adopts CNNs for detecting objects and then uses graph-based
    deep learning as a reasoning module [[156](#bib.bib156)]. For NLP problems, GCNs
    can be adopted as syntactic constraints [[141](#bib.bib141)]. As a result, key
    key challenge is how to integrate different models. These applications also show
    that graph-based deep learning not only enables mining the rich value underlying
    the existing graph data but also helps to naturally model relational data as graphs,
    greatly widening the applicability of graph-based deep learning models.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些应用的多样性，本文无法对这些方法进行全面审查；然而，我们列出了一些关键的灵感。首先，在构建图形或选择架构时，将领域知识纳入模型是很重要的。例如，基于相对距离构建图形可能适用于交通预测问题，但可能不适用于地理位置也很重要的天气预测问题。其次，图形模型通常可以建立在其他架构的基础之上，而不是作为独立的模型。例如，计算机视觉社区通常采用
    CNN 用于检测对象，然后使用基于图的深度学习作为推理模块[[156](#bib.bib156)]。对于 NLP 问题，GCNs 可以被采用作为句法约束[[141](#bib.bib141)]。因此，关键挑战是如何整合不同的模型。这些应用还表明，基于图的深度学习不仅可以挖掘现有图数据中丰富的价值，还可以帮助自然地将关系数据建模为图，极大地扩大了基于图的深度学习模型的适用范围。
- en: 8.2 Implementations
  id: totrans-426
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 实施方式
- en: 'Recently, several open libraries have been made available for developing deep
    learning models on graphs. These libraries are listed in Table [VIII](#S8.T8 "TABLE
    VIII ‣ 8.3 Future Directions ‣ 8 Discussions and Conclusion ‣ Deep Learning on
    Graphs: A Survey"). We also collected a list of source code (mostly from their
    original authors) for the studies discussed in this paper. This repository is
    included in Appendix [A](#A1 "Appendix A Source Codes ‣ Deep Learning on Graphs:
    A Survey"). These open implementations make it easy to learn, compare, and improve
    different methods. Some implementations also address the problem of distributed
    computing, which we do not discuss in this paper.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，一些开放的库已可用于在图上开发深度学习模型。这些库列在表[VIII](#S8.T8 "TABLE VIII ‣ 8.3 Future Directions
    ‣ 8 Discussions and Conclusion ‣ Deep Learning on Graphs: A Survey")中。我们还收集了本文讨论的研究的源代码列表（主要来自原作者）。该代码库包括在附录[A](#A1
    "Appendix A Source Codes ‣ Deep Learning on Graphs: A Survey")中。这些开放实现使学习、比较和改进不同方法变得容易。一些实现还解决了分布式计算的问题，但本文没有讨论这一点。'
- en: 8.3 Future Directions
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 未来方向
- en: 'There are several ongoing or future research directions which are also worthy
    of discussion:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有几个正在进行或未来的研究方向也值得讨论：
- en: •
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: New models for unstudied graph structures. Due to the extremely diverse structures
    of graph data, the existing methods are not suitable for all of them. For example,
    most methods focus on homogeneous graphs, while heterogeneous graphs are seldom
    studied, especially those containing different modalities such as those in [[157](#bib.bib157)].
    Signed networks, in which negative edges represent conflicts between nodes, also
    have unique structures, and they pose additional challenges to the existing methods [[158](#bib.bib158)].
    Hypergraphs, which represent complex relations between more than two objects [[159](#bib.bib159)],
    are also understudied. Thus, an important next step is to design specific deep
    learning models to handle these types of graphs.
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 针对未研究的图结构的新模型。由于图数据结构的极端多样性，现有的方法并不适用于所有结构。例如，大多数方法侧重于同质图，而异质图却很少被研究，尤其是那些包含不同模态的图，如[[157](#bib.bib157)]中的图。带有负边表示节点间冲突的签名网络也具有独特的结构，并对现有方法提出了额外挑战[[158](#bib.bib158)]。表示两个以上对象之间复杂关系的超图[[159](#bib.bib159)]也被研究不足。因此，下一步的重要任务是设计专门的深度学习模型来处理这些类型的图。
- en: •
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Compositionality of existing models. As shown multiple times in this paper,
    many of the existing architectures can be integrated: for example, using a GCN
    as a layer in GAEs or Graph RL. In addition to designing new building blocks,
    how to systematically composite these architectures is an interesting future direction.
    In this process, how to incorporate interdisciplinary knowledge in a principled
    way rather than on a case-by-case basis is also an open problem. One recent work,
    graph networks [[9](#bib.bib9)], takes the first step and focuses on using a general
    framework of GNNs and GCNs for relational reasoning problems. AutoML may also
    be helpful by reducing the human burden of assembling different components and
    choosing hyper-parameters [[160](#bib.bib160)].'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有模型的组合性。正如本文多次提到的，许多现有架构可以进行集成：例如，使用GCN作为GAEs或Graph RL中的一个层。除了设计新的构建块外，如何系统性地组合这些架构是一个有趣的未来方向。在这个过程中，如何以有原则的方式而不是逐案处理的方式将跨学科知识融入也是一个未解问题。一项近期工作，图网络[[9](#bib.bib9)]，迈出了第一步，专注于使用GNNs和GCNs的通用框架来解决关系推理问题。AutoML也可能通过减少组装不同组件和选择超参数的人工负担而有所帮助[[160](#bib.bib160)]。
- en: •
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dynamic graphs. Most of the existing methods focus on static graphs. However,
    many real graphs are dynamic in nature: their nodes, edges, and features can change
    over time. For example, in social networks, people may establish new social relations,
    remove old relations, and their features, such as hobbies and occupations, can
    change over time. New users may join the network and existing users may leave.
    How to model the evolving characteristics of dynamic graphs and support incremental
    updates to model parameters remain largely unaddressed. Some preliminary works
    have obtained encouraging results by using Graph RNNs [[29](#bib.bib29), [27](#bib.bib27)].'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动态图。大多数现有方法集中于静态图。然而，许多真实的图本质上是动态的：它们的节点、边和特征会随时间变化。例如，在社交网络中，人们可能建立新的社交关系、删除旧的关系，他们的特征，如爱好和职业，也可能随时间变化。新的用户可能加入网络，现有用户可能离开。如何建模动态图的演变特征并支持模型参数的增量更新仍然没有得到很好的解决。一些初步工作通过使用图
    RNNs [[29](#bib.bib29), [27](#bib.bib27)]获得了令人鼓舞的结果。
- en: •
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Interpretability and robustness. Because graphs are often related to other
    risk-sensitive scenarios, the ability to interpret the results of deep learning
    models on graphs is critical in decision-making problems. For example, in medicine
    or disease-related problems, interpretability is essential in transforming computer
    experiments into applications for clinical use. However, interpretability for
    graph-based deep learning is even more challenging than are other black-box models
    because graph nodes and edges are often heavily interconnected. In addition, because
    many existing deep learning models on graphs are sensitive to adversarial attacks
    as shown in Section [7.2](#S7.SS2 "7.2 Adversarial Attacks ‣ 7 Graph Adversarial
    Methods ‣ Deep Learning on Graphs: A Survey"), enhancing the robustness of the
    existing methods is another important issue. Some pioneering works regarding interpretability
    and robustness can be found in [[161](#bib.bib161)] and [[162](#bib.bib162), [163](#bib.bib163)],
    respectively.'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '可解释性和鲁棒性。由于图常常与其他风险敏感场景相关，因此对图上深度学习模型结果的解释能力在决策问题中至关重要。例如，在医学或疾病相关问题中，可解释性对于将计算机实验转化为临床应用是必不可少的。然而，对于基于图的深度学习，解释性比其他黑箱模型更具挑战性，因为图中的节点和边通常高度互联。此外，由于许多现有的图上深度学习模型对对抗攻击敏感，如第[7.2](#S7.SS2
    "7.2 Adversarial Attacks ‣ 7 Graph Adversarial Methods ‣ Deep Learning on Graphs:
    A Survey")节所示，增强现有方法的鲁棒性是另一个重要问题。一些关于可解释性和鲁棒性的开创性工作可以在[[161](#bib.bib161)]和[[162](#bib.bib162),
    [163](#bib.bib163)]中找到。'
- en: 'TABLE VIII: Libraries of Deep Learning on Graphs'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VIII: 图上的深度学习库'
- en: '| Name | URL | Language/Framework | Key Characteristics |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 网址 | 语言/框架 | 主要特征 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| PyTorch Geometric [[164](#bib.bib164)] | https://github.com/rusty1s/pytorch_geometric
    | PyTorch |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch Geometric [[164](#bib.bib164)] | https://github.com/rusty1s/pytorch_geometric
    | PyTorch |'
- en: '&#124; Improved efficiency, unified operations, &#124;'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提高效率，统一操作，&#124;'
- en: '&#124; comprehensive existing methods &#124;'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 综合现有方法 &#124;'
- en: '|'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Deep Graph Library [[165](#bib.bib165)] | https://github.com/dmlc/dgl | PyTorch
    | Improved efficiency, unified operations, scalability |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| Deep Graph Library [[165](#bib.bib165)] | https://github.com/dmlc/dgl | PyTorch
    | 提高效率，统一操作，可扩展性 |'
- en: '| AliGraph [[166](#bib.bib166)] | https://github.com/alibaba/aligraph | Unknown
    | Distributed environment, scalability, in-house algorithms |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| AliGraph [[166](#bib.bib166)] | https://github.com/alibaba/aligraph | 未知
    | 分布式环境，可扩展性，内部算法 |'
- en: '| Euler | https://github.com/alibaba/euler | C++/TensorFlow | Distributed environment,
    scalability |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| Euler | https://github.com/alibaba/euler | C++/TensorFlow | 分布式环境，可扩展性 |'
- en: 8.4 Summary
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 总结
- en: The above survey shows that deep learning on graphs is a promising and fast-developing
    research field that both offers exciting opportunities and presents many challenges.
    Studying deep learning on graphs constitutes a critical building block in modeling
    relational data, and it is an important step towards a future with better machine
    learning and artificial intelligence techniques.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 上述调查表明，图上的深度学习是一个有前途且快速发展的研究领域，它既提供了令人兴奋的机会，也带来了许多挑战。研究图上的深度学习构成了建模关系数据的关键基础，是迈向更好机器学习和人工智能技术的一个重要步骤。
- en: Acknowledgement
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors thank Jianfei Chen, Jie Chen, William L. Hamilton, Wenbing Huang,
    Thomas Kipf, Federico Monti, Shirui Pan, Petar Velickovic, Keyulu Xu, Rex Ying
    for allowing us to use their figures. This work was supported in part by National
    Program on Key Basic Research Project (No. 2015CB352300), National Key R&D Program
    of China under Grand 2018AAA0102004, National Natural Science Foundation of China
    (No. U1936219, No. U1611461, No. 61772304), and Beijing Academy of Artificial
    Intelligence (BAAI). All opinions, findings, conclusions, and recommendations
    in this paper are those of the authors and do not necessarily reflect the views
    of the funding agencies.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢Jianfei Chen, Jie Chen, William L. Hamilton, Wenbing Huang, Thomas Kipf,
    Federico Monti, Shirui Pan, Petar Velickovic, Keyulu Xu, Rex Ying 允许我们使用他们的图表。本工作部分由国家重点基础研究发展计划（项目编号：2015CB352300）、中国国家重点研发计划资助（资助编号：2018AAA0102004）、国家自然科学基金（项目编号：U1936219,
    U1611461, 61772304）和北京人工智能学院（BAAI）资助。本文中的所有观点、发现、结论和建议均为作者个人意见，不一定反映资助机构的观点。
- en: References
  id: totrans-452
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, 2015.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. LeCun, Y. Bengio, 和 G. Hinton, “深度学习，” *自然*，2015年。'
- en: '[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *et al.*, “Deep neural networks for acoustic
    modeling in speech recognition: The shared views of four research groups,” *IEEE
    Signal Processing Magazine*, 2012.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *等*，“用于语音识别的深度神经网络：四个研究组的共同观点，” *IEEE信号处理杂志*，2012年。'
- en: '[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in Neural Information Processing
    Systems*, 2012.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行ImageNet分类，”
    在 *神经信息处理系统进展*，2012年。'
- en: '[4] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” in *Proceedings of the 4th International Conference
    on Learning Representations*, 2015.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] D. Bahdanau, K. Cho, 和 Y. Bengio, “通过联合学习对齐和翻译进行神经机器翻译，” 在 *第4届国际学习表征会议*，2015年。'
- en: '[5] A.-L. Barabasi, *Network science*.   Cambridge university press, 2016.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A.-L. Barabasi, *网络科学*。剑桥大学出版社，2016年。'
- en: '[6] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,
    “The emerging field of signal processing on graphs: Extending high-dimensional
    data analysis to networks and other irregular domains,” *IEEE Signal Processing
    Magazine*, 2013.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, 和 P. Vandergheynst,
    “图信号处理的新兴领域：将高维数据分析扩展到网络和其他不规则领域，” *IEEE信号处理杂志*，2013年。'
- en: '[7] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, “Geometric
    deep learning: going beyond euclidean data,” *IEEE Signal Processing Magazine*,
    2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, 和 P. Vandergheynst, “几何深度学习：超越欧几里得数据，”
    *IEEE信号处理杂志*，2017年。'
- en: '[8] C. Zang, P. Cui, and C. Faloutsos, “Beyond sigmoids: The nettide model
    for social network growth, and its applications,” in *Proceedings of the 22nd
    ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 2016.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. Zang, P. Cui, 和 C. Faloutsos, “超越Sigmoids：社交网络增长的nettide模型及其应用，” 在 *第22届ACM
    SIGKDD国际知识发现与数据挖掘会议*，2016年。'
- en: '[9] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi,
    M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre,
    F. Song, A. Ballard, J. Gilmer, G. Dahl, A. Vaswani, K. Allen, C. Nash, V. Langston,
    C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, and
    R. Pascanu, “Relational inductive biases, deep learning, and graph networks,”
    *arXiv preprint arXiv:1806.01261*, 2018.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi,
    M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre,
    F. Song, A. Ballard, J. Gilmer, G. Dahl, A. Vaswani, K. Allen, C. Nash, V. Langston,
    C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, 和 R.
    Pascanu, “关系归纳偏置、深度学习和图网络，” *arXiv预印本 arXiv:1806.01261*，2018年。'
- en: '[10] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, “Attention models
    in graphs: A survey,” *ACM Transactions on Knowledge Discovery from Data*, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, 和 E. Koh, “图中的注意力模型：综述，”
    *ACM数据知识发现交易*，2019年。'
- en: '[11] S. Zhang, H. Tong, J. Xu, and R. Maciejewski, “Graph convolutional networks:
    Algorithms, applications and open challenges,” in *International Conference on
    Computational Social Networks*, 2019.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Zhang, H. Tong, J. Xu, 和 R. Maciejewski, “图卷积网络：算法、应用及开放挑战，” 在 *计算社会网络国际会议*，2019年。'
- en: '[12] L. Sun, J. Wang, P. S. Yu, and B. Li, “Adversarial attack and defense
    on graph data: A survey,” *arXiv preprint arXiv:1812.10528*, 2018.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] L. Sun, J. Wang, P. S. Yu, 和 B. Li, “图数据上的对抗攻击与防御：综述，” *arXiv 预印本 arXiv:1812.10528*，2018。'
- en: '[13] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, and M. Sun, “Graph neural
    networks: A review of methods and applications,” *arXiv preprint arXiv:1812.08434*,
    2018.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, 和 M. Sun, “图神经网络：方法与应用综述，”
    *arXiv 预印本 arXiv:1812.08434*，2018。'
- en: '[14] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, “A comprehensive
    survey on graph neural networks,” *arXiv preprint arXiv:1901.00596*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, 和 P. S. Yu, “图神经网络的全面综述，” *arXiv
    预印本 arXiv:1901.00596*，2019。'
- en: '[15] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin, “Graph embedding
    and extensions: A general framework for dimensionality reduction,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2007.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, 和 S. Lin, “图嵌入及扩展：一种用于降维的一般框架，”
    *IEEE 模式分析与机器智能事务*，2007。'
- en: '[16] W. L. Hamilton, R. Ying, and J. Leskovec, “Representation learning on
    graphs: Methods and applications,” *arXiv preprint arXiv:1709.05584*, 2017.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] W. L. Hamilton, R. Ying, 和 J. Leskovec, “图上的表示学习：方法与应用，” *arXiv 预印本 arXiv:1709.05584*，2017。'
- en: '[17] P. Cui, X. Wang, J. Pei, and W. Zhu, “A survey on network embedding,”
    *IEEE Transactions on Knowledge and Data Engineering*, 2018.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] P. Cui, X. Wang, J. Pei, 和 W. Zhu, “网络嵌入综述，” *IEEE 知识与数据工程事务*，2018。'
- en: '[18] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations
    by back-propagating errors,” *Nature*, 1986.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] D. E. Rumelhart, G. E. Hinton, 和 R. J. Williams, “通过反向传播错误学习表示，” *自然*，1986。'
- en: '[19] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
    in *Proceedings of the 3rd International Conference on Learning Representations*,
    2014.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] D. P. Kingma 和 J. Ba, “Adam：一种随机优化方法，” 见于 *第3届国际学习表示会议论文集*，2014。'
- en: '[20] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *Journal
    of Machine Learning Research*, 2014.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, 和 R. Salakhutdinov,
    “Dropout：一种防止神经网络过拟合的简单方法，” *机器学习研究杂志*，2014。'
- en: '[21] X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving
    network embedding,” in *Proceedings of the 31st AAAI Conference on Artificial
    Intelligence*, 2017.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, 和 S. Yang, “社区保留的网络嵌入，” 见于 *第31届美国人工智能协会会议论文集*，2017。'
- en: '[22] J. Leskovec and J. J. Mcauley, “Learning to discover social circles in
    ego networks,” in *NeurIPS*, 2012.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Leskovec 和 J. J. Mcauley, “学习发现自我网络中的社交圈，” 见于 *NeurIPS*，2012。'
- en: '[23] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE Transactions on Neural Networks*, 2009.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, 和 G. Monfardini, “图神经网络模型，”
    *IEEE 神经网络事务*，2009。'
- en: '[24] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, “Gated graph sequence
    neural networks,” in *Proceedings of the 5th International Conference on Learning
    Representations*, 2016.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Y. Li, D. Tarlow, M. Brockschmidt, 和 R. Zemel, “门控图序列神经网络，” 见于 *第5届国际学习表示会议论文集*，2016。'
- en: '[25] H. Dai, Z. Kozareva, B. Dai, A. Smola, and L. Song, “Learning steady-states
    of iterative algorithms over graphs,” in *International Conference on Machine
    Learning*, 2018.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] H. Dai, Z. Kozareva, B. Dai, A. Smola, 和 L. Song, “图上的迭代算法稳定状态学习，” 见于
    *国际机器学习会议*，2018。'
- en: '[26] J. You, R. Ying, X. Ren, W. Hamilton, and J. Leskovec, “Graphrnn: Generating
    realistic graphs with deep auto-regressive models,” in *International Conference
    on Machine Learning*, 2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. You, R. Ying, X. Ren, W. Hamilton, 和 J. Leskovec, “Graphrnn：使用深度自回归模型生成现实图，”
    见于 *国际机器学习会议*，2018。'
- en: '[27] Y. Ma, Z. Guo, Z. Ren, E. Zhao, J. Tang, and D. Yin, “Streaming graph
    neural networks,” *arXiv preprint arXiv:1810.10627*, 2018.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Ma, Z. Guo, Z. Ren, E. Zhao, J. Tang, 和 D. Yin, “流式图神经网络，” *arXiv 预印本
    arXiv:1810.10627*，2018。'
- en: '[28] F. Monti, M. Bronstein, and X. Bresson, “Geometric matrix completion with
    recurrent multi-graph neural networks,” in *Advances in Neural Information Processing
    Systems*, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] F. Monti, M. Bronstein, 和 X. Bresson, “具有递归多图神经网络的几何矩阵补全，” 见于 *神经信息处理系统进展*，2017。'
- en: '[29] F. Manessi, A. Rozza, and M. Manzo, “Dynamic graph convolutional networks,”
    *arXiv preprint arXiv:1704.06199*, 2017.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] F. Manessi, A. Rozza, 和 M. Manzo, “动态图卷积网络，” *arXiv 预印本 arXiv:1704.06199*，2017。'
- en: '[30] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder–decoder for
    statistical machine translation,” in *Proceedings of the 2014 Conference on Empirical
    Methods in Natural Language Processing*, 2014.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk
    和 Y. Bengio，"使用RNN编码器-解码器学习短语表示用于统计机器翻译"，发表于*2014年自然语言处理经验方法会议论文集*，2014年。'
- en: '[31] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    1997.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Hochreiter 和 J. Schmidhuber，"长短期记忆"，*神经计算*，1997年。'
- en: '[32] M. Gori, G. Monfardini, and F. Scarselli, “A new model for learning in
    graph domains,” in *IEEE International Joint Conference on Neural Networks Proceedings*,
    2005.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. Gori, G. Monfardini 和 F. Scarselli，"一种用于图域学习的新模型"，发表于*IEEE国际联合神经网络会议论文集*，2005年。'
- en: '[33] P. Frasconi, M. Gori, and A. Sperduti, “A general framework for adaptive
    processing of data structures,” *IEEE transactions on Neural Networks*, 1998.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Frasconi, M. Gori 和 A. Sperduti，"数据结构自适应处理的一般框架"，*IEEE神经网络汇刊*，1998年。'
- en: '[34] M. J. Powell, “An efficient method for finding the minimum of a function
    of several variables without calculating derivatives,” *The computer journal*,
    1964.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. J. Powell，"一种高效的多变量函数最小值查找方法，无需计算导数"，*计算机期刊*，1964年。'
- en: '[35] L. B. Almeida, “A learning rule for asynchronous perceptrons with feedback
    in a combinatorial environment.” in *Proceedings, 1st First International Conference
    on Neural Networks*, 1987.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] L. B. Almeida，"用于组合环境中具有反馈的异步感知机的学习规则"，发表于*第1届国际神经网络会议论文集*，1987年。'
- en: '[36] F. J. Pineda, “Generalization of back-propagation to recurrent neural
    networks,” *Physical Review Letters*, 1987.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] F. J. Pineda，"反向传播在递归神经网络中的推广"，*物理评论快报*，1987年。'
- en: '[37] M. A. Khamsi and W. A. Kirk, *An introduction to metric spaces and fixed
    point theory*.   John Wiley & Sons, 2011.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] M. A. Khamsi 和 W. A. Kirk，*度量空间与不动点理论导论*，John Wiley & Sons，2011年。'
- en: '[38] M. Brockschmidt, Y. Chen, B. Cook, P. Kohli, and D. Tarlow, “Learning
    to decipher the heap for program verification,” in *Workshop on Constructive Machine
    Learning at the International Conference on Machine Learning*, 2015.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. Brockschmidt, Y. Chen, B. Cook, P. Kohli 和 D. Tarlow，"学习解码堆以进行程序验证"，发表于*国际机器学习会议上的构造性机器学习研讨会*，2015年。'
- en: '[39] I. M. Baytas, C. Xiao, X. Zhang, F. Wang, A. K. Jain, and J. Zhou, “Patient
    subtyping via time-aware lstm networks,” in *Proceedings of the 23rd ACM SIGKDD
    International Conference on Knowledge Discovery and Data Mining*, 2017.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] I. M. Baytas, C. Xiao, X. Zhang, F. Wang, A. K. Jain 和 J. Zhou，"通过时间感知LSTM网络进行患者亚型分类"，发表于*第23届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*，2017年。'
- en: '[40] J. Bruna, W. Zaremba, A. Szlam, and Y. Lecun, “Spectral networks and locally
    connected networks on graphs,” in *Proceedings of the 3rd International Conference
    on Learning Representations*, 2014.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Bruna, W. Zaremba, A. Szlam 和 Y. Lecun，"图上的谱网络和局部连接网络"，发表于*第3届国际学习表征会议论文集*，2014年。'
- en: '[41] M. Henaff, J. Bruna, and Y. LeCun, “Deep convolutional networks on graph-structured
    data,” *arXiv preprint arXiv:1506.05163*, 2015.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. Henaff, J. Bruna 和 Y. LeCun，"图结构数据上的深度卷积网络"，*arXiv预印本 arXiv:1506.05163*，2015年。'
- en: '[42] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural
    networks on graphs with fast localized spectral filtering,” in *Advances in Neural
    Information Processing Systems*, 2016.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. Defferrard, X. Bresson 和 P. Vandergheynst，"具有快速局部谱滤波的图卷积神经网络"，发表于*神经信息处理系统进展*，2016年。'
- en: '[43] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *Proceedings of the 6th International Conference on
    Learning Representations*, 2017.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] T. N. Kipf 和 M. Welling，"基于图卷积网络的半监督分类"，发表于*第6届国际学习表征会议论文集*，2017年。'
- en: '[44] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, “Cayleynets: Graph
    convolutional neural networks with complex rational spectral filters,” *IEEE Transactions
    on Signal Processing*, 2017.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Levie, F. Monti, X. Bresson 和 M. M. Bronstein，"Cayleynets：具有复杂有理谱滤波器的图卷积神经网络"，*IEEE信号处理汇刊*，2017年。'
- en: '[45] B. Xu, H. Shen, Q. Cao, Y. Qiu, and X. Cheng, “Graph wavelet neural network,”
    in *Proceedings of the 8th International Conference on Learning Representations*,
    2019.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] B. Xu, H. Shen, Q. Cao, Y. Qiu 和 X. Cheng，"图小波神经网络"，发表于*第8届国际学习表征会议论文集*，2019年。'
- en: '[46] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
    A. Aspuru-Guzik, and R. P. Adams, “Convolutional networks on graphs for learning
    molecular fingerprints,” in *Advances in Neural Information Processing Systems*,
    2015.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
    A. Aspuru-Guzik, 和 R. P. Adams， “用于学习分子指纹的图卷积网络”，发表于 *神经信息处理系统进展*，2015年。'
- en: '[47] M. Niepert, M. Ahmed, and K. Kutzkov, “Learning convolutional neural networks
    for graphs,” in *International Conference on Machine Learning*, 2016.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. Niepert, M. Ahmed, 和 K. Kutzkov， “图的卷积神经网络学习”，发表于 *国际机器学习会议*，2016年。'
- en: '[48] H. Gao, Z. Wang, and S. Ji, “Large-scale learnable graph convolutional
    networks,” in *Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery &amp; Data Mining*, 2018.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] H. Gao, Z. Wang, 和 S. Ji， “大规模可学习的图卷积网络”，发表于 *第24届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，2018年。'
- en: '[49] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, “An end-to-end deep learning
    architecture for graph classification,” in *Thirty-Second AAAI Conference on Artificial
    Intelligence*, 2018.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. Zhang, Z. Cui, M. Neumann, 和 Y. Chen， “一种端到端的图分类深度学习架构”，发表于 *第三十二届人工智能AAAI会议*，2018年。'
- en: '[50] J. Atwood and D. Towsley, “Diffusion-convolutional neural networks,” in
    *Advances in Neural Information Processing Systems*, 2016.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Atwood 和 D. Towsley， “扩散卷积神经网络”，发表于 *神经信息处理系统进展*，2016年。'
- en: '[51] C. Zhuang and Q. Ma, “Dual graph convolutional networks for graph-based
    semi-supervised classification,” in *Proceedings of the 2018 World Wide Web Conference*,
    2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] C. Zhuang 和 Q. Ma， “用于基于图的半监督分类的双图卷积网络”，发表于 *2018年全球互联网大会论文集*，2018年。'
- en: '[52] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
    “Neural message passing for quantum chemistry,” in *International Conference on
    Machine Learning*, 2017.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, 和 G. E. Dahl， “用于量子化学的神经信息传递”，发表于
    *国际机器学习会议*，2017年。'
- en: '[53] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning
    on large graphs,” in *NeurIPS*, 2017.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] W. Hamilton, Z. Ying, 和 J. Leskovec， “在大规模图上的归纳表示学习”，发表于 *NeurIPS*，2017年。'
- en: '[54] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein,
    “Geometric deep learning on graphs and manifolds using mixture model cnns,” in
    *CVPR*, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, 和 M. M. Bronstein，
    “使用混合模型卷积神经网络的图形和流形上的几何深度学习”，发表于 *CVPR*，2017年。'
- en: '[55] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley, “Molecular
    graph convolutions: moving beyond fingerprints,” *Journal of Computer-Aided Molecular
    Design*, 2016.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, 和 P. Riley， “分子图卷积：超越指纹”，发表于
    *计算机辅助分子设计杂志*，2016年。'
- en: '[56] R. Ying, J. You, C. Morris, X. Ren, W. L. Hamilton, and J. Leskovec, “Hierarchical
    graph representation learning with differentiable pooling,” in *Advances in Neural
    Information Processing Systems*, 2018.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] R. Ying, J. You, C. Morris, X. Ren, W. L. Hamilton, 和 J. Leskovec， “具有可微分池化的层次图表示学习”，发表于
    *神经信息处理系统进展*，2018年。'
- en: '[57] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio,
    “Graph attention networks,” in *Proceedings of the 7th International Conference
    on Learning Representations*, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, 和 Y. Bengio，
    “图注意网络”，发表于 *第7届国际学习表征会议论文集*，2018年。'
- en: '[58] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D.-Y. Yeung, “Gaan: Gated
    attention networks for learning on large and spatiotemporal graphs,” in *Proceedings
    of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence*, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, 和 D.-Y. Yeung， “Gaan：用于大规模和时空图学习的门控注意网络”，发表于
    *第三十四届人工智能不确定性会议论文集*，2018年。'
- en: '[59] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. S. Yu, “Heterogeneous
    graph attention network,” in *The World Wide Web Conference*, 2019.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, 和 P. S. Yu， “异质图注意网络”，发表于
    *全球互联网大会*，2019年。'
- en: '[60] T. Pham, T. Tran, D. Q. Phung, and S. Venkatesh, “Column networks for
    collective classification.” in *Proceedings of the 31st AAAI Conference on Artificial
    Intelligence*, 2017.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] T. Pham, T. Tran, D. Q. Phung, 和 S. Venkatesh， “用于集体分类的列网络”，发表于 *第31届人工智能AAAI会议论文集*，2017年。'
- en: '[61] J. Klicpera, A. Bojchevski, and S. Günnemann, “Predict then propagate:
    Graph neural networks meet personalized pagerank,” in *Proceedings of the 8th
    International Conference on Learning Representations*, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. Klicpera, A. Bojchevski, 和 S. Günnemann， “预测再传播：图神经网络与个性化PageRank的结合”，发表于
    *第8届国际学习表征会议论文集*，2019年。'
- en: '[62] K. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka,
    “Representation learning on graphs with jumping knowledge networks,” in *International
    Conference on Machine Learning*, 2018.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] K. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, 和 S. Jegelka，“通过跳跃知识网络在图上进行表征学习，”
    见 *国际机器学习会议*，2018年。'
- en: '[63] M. Simonovsky and N. Komodakis, “Dynamic edgeconditioned filters in convolutional
    neural networks on graphs,” in *2017 IEEE Conference on Computer Vision and Pattern
    Recognition*, 2017.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] M. Simonovsky 和 N. Komodakis，“图上的卷积神经网络中的动态边条件滤波器，” 见 *2017年IEEE计算机视觉与模式识别会议*，2017年。'
- en: '[64] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. V. D. Berg, I. Titov, and M. Welling,
    “Modeling relational data with graph convolutional networks,” in *European Semantic
    Web Conference*, 2018.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. V. D. Berg, I. Titov, 和 M. Welling，“使用图卷积网络建模关系数据，”
    见 *欧洲语义网会议*，2018年。'
- en: '[65] Z. Chen, L. Li, and J. Bruna, “Supervised community detection with line
    graph neural networks,” in *Proceedings of the 8th International Conference on
    Learning Representations*, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Z. Chen, L. Li, 和 J. Bruna，“使用线图神经网络的监督社区检测，” 见 *第八届国际学习表征会议论文集*，2019年。'
- en: '[66] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec,
    “Graph convolutional neural networks for web-scale recommender systems,” in *Proceedings
    of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining*, 2018.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, 和 J. Leskovec，“面向网络规模推荐系统的图卷积神经网络，”
    见 *第24届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，2018年。'
- en: '[67] J. Chen, J. Zhu, and L. Song, “Stochastic training of graph convolutional
    networks with variance reduction,” in *International Conference on Machine Learning*,
    2018.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Chen, J. Zhu, 和 L. Song，“具有方差减少的图卷积网络随机训练，” 见 *国际机器学习会议*，2018年。'
- en: '[68] J. Chen, T. Ma, and C. Xiao, “Fastgcn: fast learning with graph convolutional
    networks via importance sampling,” in *Proceedings of the 7th International Conference
    on Learning Representations*, 2018.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Chen, T. Ma, 和 C. Xiao，“Fastgcn：通过重要性采样加速图卷积网络的学习，” 见 *第七届国际学习表征会议论文集*，2018年。'
- en: '[69] W. Huang, T. Zhang, Y. Rong, and J. Huang, “Adaptive sampling towards
    fast graph representation learning,” in *Advances in Neural Information Processing
    Systems*, 2018.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] W. Huang, T. Zhang, Y. Rong, 和 J. Huang，“朝向快速图表征学习的自适应采样，” 见 *神经信息处理系统进展*，2018年。'
- en: '[70] Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional
    networks for semi-supervised learning,” in *Proceedings of the Thirty-Second AAAI
    Conference on Artificial Intelligence*, 2018.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Q. Li, Z. Han, 和 X.-M. Wu，“对图卷积网络进行深入理解以用于半监督学习，” 见 *第三十二届AAAI人工智能会议论文集*，2018年。'
- en: '[71] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger, “Simplifying
    graph convolutional networks,” in *International Conference on Machine Learning*,
    2019.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, 和 K. Weinberger，“简化图卷积网络，”
    见 *国际机器学习会议*，2019年。'
- en: '[72] T. Maehara, “Revisiting graph neural networks: All we have is low-pass
    filters,” *arXiv preprint arXiv:1905.09550*, 2019.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] T. Maehara，“重新审视图神经网络：我们拥有的只是低通滤波器，” *arXiv预印本 arXiv:1905.09550*，2019年。'
- en: '[73] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural
    networks?” in *Proceedings of the 8th International Conference on Learning Representations*,
    2019.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] K. Xu, W. Hu, J. Leskovec, 和 S. Jegelka，“图神经网络有多强大？” 见 *第八届国际学习表征会议论文集*，2019年。'
- en: '[74] P. Veličković, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D.
    Hjelm, “Deep graph infomax,” in *Proceedings of the 8th International Conference
    on Learning Representations*, 2019.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] P. Veličković, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, 和 R. D. Hjelm，“深度图信息最大化，”
    见 *第八届国际学习表征会议论文集*，2019年。'
- en: '[75] M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques
    for embedding and clustering,” in *Advances in neural information processing systems*,
    2002.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. Belkin 和 P. Niyogi，“拉普拉斯特征图与谱技术在嵌入与聚类中的应用，” 见 *神经信息处理系统进展*，2002年。'
- en: '[76] D. K. Hammond, P. Vandergheynst, and R. Gribonval, “Wavelets on graphs
    via spectral graph theory,” *Applied and Computational Harmonic Analysis*, 2011.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] D. K. Hammond, P. Vandergheynst, 和 R. Gribonval， “通过谱图理论的图上的小波变换，” *应用与计算谐波分析*，2011年。'
- en: '[77] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary-order
    proximity preserved network embedding,” in *KDD*, 2018.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, 和 W. Zhu，“任意阶接近度保留的网络嵌入，” 见
    *KDD*，2018年。'
- en: '[78] N. Shervashidze, P. Schweitzer, E. J. v. Leeuwen, K. Mehlhorn, and K. M.
    Borgwardt, “Weisfeiler-lehman graph kernels,” *Journal of Machine Learning Research*,
    2011.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] N. Shervashidze, P. Schweitzer, E. J. v. Leeuwen, K. Mehlhorn, 和 K. M.
    Borgwardt，“Weisfeiler-Lehman图核”，*机器学习研究杂志*，2011年。'
- en: '[79] O. Levy and Y. Goldberg, “Neural word embedding as implicit matrix factorization,”
    in *Advances in Neural Information Processing Systems*, 2014.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] O. Levy 和 Y. Goldberg，“神经词嵌入作为隐式矩阵分解”，发表于 *神经信息处理系统进展*，2014年。'
- en: '[80] L. Babai, “Graph isomorphism in quasipolynomial time,” in *Proceedings
    of the forty-eighth annual ACM symposium on Theory of Computing*, 2016.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] L. Babai，“准多项式时间的图同构”，发表于 *第48届ACM理论计算机年会论文集*，2016年。'
- en: '[81] G. Klir and B. Yuan, *Fuzzy sets and fuzzy logic*.   Prentice hall New
    Jersey, 1995.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] G. Klir 和 B. Yuan，*模糊集和模糊逻辑*。  普伦蒂斯厅，新泽西州，1995年。'
- en: '[82] J. Ma, P. Cui, X. Wang, and W. Zhu, “Hierarchical taxonomy aware network
    embedding,” in *Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery & Data Mining*, 2018.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Ma, P. Cui, X. Wang, 和 W. Zhu，“层次分类感知的网络嵌入”，发表于 *第24届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，2018年。'
- en: '[83] D. Ruppert, “The elements of statistical learning: Data mining, inference,
    and prediction,” *Journal of the Royal Statistical Society*, 2010.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] D. Ruppert，“统计学习的要素：数据挖掘、推断和预测”，*皇家统计学会杂志*，2010年。'
- en: '[84] U. Von Luxburg, “A tutorial on spectral clustering,” *Statistics and computing*,
    2007.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] U. Von Luxburg，“谱聚类教程”，*统计与计算*，2007年。'
- en: '[85] I. S. Dhillon, Y. Guan, and B. Kulis, “Weighted graph cuts without eigenvectors
    a multilevel approach,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2007.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] I. S. Dhillon, Y. Guan, 和 B. Kulis，“没有特征向量的加权图割：一种多级方法”，*IEEE模式分析与机器智能汇刊*，2007年。'
- en: '[86] D. I. Shuman, M. J. Faraji, and P. Vandergheynst, “A multiscale pyramid
    transform for graph signals,” *IEEE Transactions on Signal Processing*, 2016.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] D. I. Shuman, M. J. Faraji, 和 P. Vandergheynst，“图信号的多尺度金字塔变换”，*IEEE信号处理汇刊*，2016年。'
- en: '[87] B. D. Mckay and A. Piperno, *Practical graph isomorphism, II*.   Academic
    Press, Inc., 2014.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] B. D. Mckay 和 A. Piperno，*实用图同构，II*。 学术出版社，2014年。'
- en: '[88] O. Vinyals, S. Bengio, and M. Kudlur, “Order matters: Sequence to sequence
    for sets,” *Proceedings of the 5th International Conference on Learning Representations*,
    2016.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] O. Vinyals, S. Bengio, 和 M. Kudlur，“顺序很重要：集的序列到序列”，发表于 *第五届国际学习表征会议论文集*，2016年。'
- en: '[89] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Advances in Neural
    Information Processing Systems*, 2017.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“注意力即你所需”，发表于 *神经信息处理系统进展*，2017年。'
- en: '[90] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on Computer Vision and Pattern
    Recognition*, 2016.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2016年。'
- en: '[91] A.-L. Barabási and R. Albert, “Emergence of scaling in random networks,”
    *Science*, 1999.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] A.-L. Barabási 和 R. Albert，“随机网络中的尺度涌现”，*科学*，1999年。'
- en: '[92] J. Ma, P. Cui, and W. Zhu, “Depthlgp: Learning embeddings of out-of-sample
    nodes in dynamic networks,” in *Proceedings of the 32nd AAAI Conference on Artificial
    Intelligence*, 2018.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Ma, P. Cui, 和 W. Zhu，“Depthlgp：学习动态网络中样本外节点的嵌入”，发表于 *第32届AAAI人工智能会议论文集*，2018年。'
- en: '[93] F. Scarselli, A. C. Tsoi, and M. Hagenbuchner, “The vapnik–chervonenkis
    dimension of graph and recursive neural networks,” *Neural Networks*, 2018.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] F. Scarselli, A. C. Tsoi, 和 M. Hagenbuchner，“图和递归神经网络的 Vapnik–Chervonenkis
    维度”，*神经网络*，2018年。'
- en: '[94] S. Verma and Z.-L. Zhang, “Stability and generalization of graph convolutional
    neural networks,” in *KDD*, 2019.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] S. Verma 和 Z.-L. Zhang，“图卷积神经网络的稳定性和泛化能力”，发表于 *KDD*，2019年。'
- en: '[95] P. Vincent, H. Larochelle, Y. Bengio, and P. A. Manzagol, “Extracting
    and composing robust features with denoising autoencoders,” in *International
    Conference on Machine Learning*, 2008.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] P. Vincent, H. Larochelle, Y. Bengio, 和 P. A. Manzagol，“用去噪自编码器提取和组合鲁棒特征”，发表于
    *国际机器学习会议*，2008年。'
- en: '[96] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu, “Learning deep representations
    for graph clustering.” in *Proceedings of the Twenty-Eighth AAAI Conference on
    Artificial Intelligence*, 2014.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] F. Tian, B. Gao, Q. Cui, E. Chen, 和 T.-Y. Liu，“图聚类的深度表示学习”，发表于 *第二十八届AAAI人工智能会议论文集*，2014年。'
- en: '[97] D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in *KDD*,
    2016.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations.”
    in *Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence*,
    2016.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] R. v. d. Berg, T. N. Kipf, and M. Welling, “Graph convolutional matrix
    completion,” *KDD’18 Deep Learning Day*, 2018.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] K. Tu, P. Cui, X. Wang, P. S. Yu, and W. Zhu, “Deep recursive network
    embedding with regular equivalence,” in *KDD*, 2018.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] A. Bojchevski and S. Günnemann, “Deep gaussian embedding of graphs: Unsupervised
    inductive learning via ranking,” in *Proceedings of the 7th International Conference
    on Learning Representations*, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] T. N. Kipf and M. Welling, “Variational graph auto-encoders,” *NIPS Workshop
    on Bayesian Deep Learning*, 2016.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] D. Zhu, P. Cui, D. Wang, and W. Zhu, “Deep variational network embedding
    in wasserstein space,” in *Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*, 2018.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, “Adversarially
    regularized graph autoencoder for graph embedding.” in *Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence*, 2018.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] W. Yu, C. Zheng, W. Cheng, C. C. Aggarwal, D. Song, B. Zong, H. Chen,
    and W. Wang, “Learning deep network representations with adversarially regularized
    autoencoders,” in *Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*, 2018.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. MacQueen *et al.*, “Some methods for classification and analysis of
    multivariate observations,” in *Proceedings of the fifth Berkeley symposium on
    mathematical statistics and probability*, 1967.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Z. Zhang, “A note on spectral clustering and svd of graph data,” *arXiv
    preprint arXiv:1809.11029*, 2018.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale
    information network embedding,” in *Proceedings of the 24th International Conference
    on World Wide Web*, 2015.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] L. Lovász *et al.*, “Random walks on graphs: A survey,” *Combinatorics*,
    1993.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank citation
    ranking: Bringing order to the web.” Stanford InfoLab, Tech. Rep., 1999.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] S. Kullback and R. A. Leibler, “On information and sufficiency,” *The
    annals of mathematical statistics*, 1951.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang, “A tutorial
    on energy-based learning,” *Predicting structured data*, 2006.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in *Proceedings
    of the 3rd International Conference on Learning Representations*, 2014.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] S. Vallender, “Calculation of the wasserstein distance between probability
    distributions on the line,” *Theory of Probability & Its Applications*, 1974.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems*, 2014.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络，”发表于*神经信息处理系统进展*，2014年。'
- en: '[116] J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec, “Graph convolutional
    policy network for goal-directed molecular graph generation,” in *Advances in
    Neural Information Processing Systems*, 2018.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] J. You, B. Liu, R. Ying, V. Pande, 和 J. Leskovec，“用于目标导向分子图生成的图卷积策略网络，”发表于*神经信息处理系统进展*，2018年。'
- en: '[117] N. De Cao and T. Kipf, “MolGAN: An implicit generative model for small
    molecular graphs,” *ICML 2018 workshop on Theoretical Foundations and Applications
    of Deep Generative Models*, 2018.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] N. De Cao 和 T. Kipf，“MolGAN: 一种用于小分子图的隐式生成模型，”*ICML 2018 深度生成模型理论基础与应用研讨会*，2018年。'
- en: '[118] K. Do, T. Tran, and S. Venkatesh, “Graph transformation policy network
    for chemical reaction prediction,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] K. Do, T. Tran, 和 S. Venkatesh，“用于化学反应预测的图转换策略网络，”发表于*第25届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，2019年。'
- en: '[119] J. B. Lee, R. Rossi, and X. Kong, “Graph classification using structural
    attention,” in *Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery & Data Mining*, 2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] J. B. Lee, R. Rossi, 和 X. Kong，“使用结构注意力的图分类，”发表于*第24届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，2018年。'
- en: '[120] W. Xiong, T. Hoang, and W. Y. Wang, “Deeppath: A reinforcement learning
    method for knowledge graph reasoning,” in *Proceedings of the 2017 Conference
    on Empirical Methods in Natural Language Processing*, 2017.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] W. Xiong, T. Hoang, 和 W. Y. Wang，“Deeppath: 一种用于知识图谱推理的强化学习方法，”发表于*2017年自然语言处理实证方法会议论文集*，2017年。'
- en: '[121] R. Das, S. Dhuliawala, M. Zaheer, L. Vilnis, I. Durugkar, A. Krishnamurthy,
    A. Smola, and A. McCallum, “Go for a walk and arrive at the answer: Reasoning
    over paths in knowledge bases using reinforcement learning,” in *Proceedings of
    the 7th International Conference on Learning Representations*, 2018.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] R. Das, S. Dhuliawala, M. Zaheer, L. Vilnis, I. Durugkar, A. Krishnamurthy,
    A. Smola, 和 A. McCallum，“去散步并得到答案：使用强化学习在知识库中进行路径推理，”发表于*第7届国际学习表征会议论文集*，2018年。'
- en: '[122] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,
    T. Hubert, L. Baker, M. Lai, A. Bolton *et al.*, “Mastering the game of go without
    human knowledge,” *Nature*, 2017.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A.
    Guez, T. Hubert, L. Baker, M. Lai, A. Bolton *等*，“在没有人类知识的情况下掌握围棋游戏，”*自然*，2017年。'
- en: '[123] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, “Policy
    gradient methods for reinforcement learning with function approximation,” in *Advances
    in Neural Information Processing systems*, 2000.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] R. S. Sutton, D. A. McAllester, S. P. Singh, 和 Y. Mansour，“带有函数逼近的强化学习策略梯度方法，”发表于*神经信息处理系统进展*，2000年。'
- en: '[124] H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, X. Xie, and M. Guo,
    “Graphgan: Graph representation learning with generative adversarial nets,” in
    *Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence*,
    2018.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, X. Xie, 和 M.
    Guo，“Graphgan: 基于生成对抗网络的图表示学习，”发表于*第三十二届AAAI人工智能会议论文集*，2018年。'
- en: '[125] Q. Dai, Q. Li, J. Tang, and D. Wang, “Adversarial network embedding,”
    in *Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence*,
    2018.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Q. Dai, Q. Li, J. Tang, 和 D. Wang，“对抗网络嵌入，”发表于*第三十二届AAAI人工智能会议论文集*，2018年。'
- en: '[126] M. Ding, J. Tang, and J. Zhang, “Semi-supervised learning on graphs with
    generative adversarial nets,” in *Proceedings of the 27th ACM International Conference
    on Information and Knowledge Management*, 2018.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] M. Ding, J. Tang, 和 J. Zhang，“基于生成对抗网络的图半监督学习，”发表于*第27届ACM国际信息与知识管理大会论文集*，2018年。'
- en: '[127] A. Bojchevski, O. Shchur, D. Zügner, and S. Günnemann, “Netgan: Generating
    graphs via random walks,” in *International Conference on Machine Learning*, 2018.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] A. Bojchevski, O. Shchur, D. Zügner, 和 S. Günnemann，“Netgan: 通过随机游走生成图，”发表于*国际机器学习会议*，2018年。'
- en: '[128] D. Zügner, A. Akbarnejad, and S. Günnemann, “Adversarial attacks on neural
    networks for graph data,” in *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery &amp; Data Mining*, 2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] D. Zügner, A. Akbarnejad, 和 S. Günnemann，“对图数据的神经网络对抗攻击，”发表于*第24届ACM
    SIGKDD国际知识发现与数据挖掘大会论文集*，2018年。'
- en: '[129] H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu, and L. Song, “Adversarial
    attack on graph structured data,” in *Proceedings of the 35th International Conference
    on Machine Learning*, 2018.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu, 和 L. Song，“对图结构数据的对抗攻击”，发表于*第35届国际机器学习会议论文集*，2018年。'
- en: '[130] D. Zügner and S. Günnemann, “Adversarial attacks on graph neural networks
    via meta learning,” in *Proceedings of the 8th International Conference on Learning
    Representations*, 2019.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] D. Zügner 和 S. Günnemann，“通过元学习对图神经网络的对抗攻击”，发表于*第八届国际学习表征会议论文集*，2019年。'
- en: '[131] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of
    social representations,” in *Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining*, 2014.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] B. Perozzi, R. Al-Rfou, 和 S. Skiena，“Deepwalk：社交表征的在线学习”，发表于*第20届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*，2014年。'
- en: '[132] H. Dai, B. Dai, and L. Song, “Discriminative embeddings of latent variable
    models for structured data,” in *International Conference on Machine Learning*,
    2016.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] H. Dai, B. Dai, 和 L. Song，“用于结构化数据的潜变量模型的判别性嵌入”，发表于*国际机器学习会议*，2016年。'
- en: '[133] J. Qiu, J. Tang, H. Ma, Y. Dong, K. Wang, and J. Tang, “Deepinf: Modeling
    influence locality in large social networks,” in *Proceedings of the 24th ACM
    SIGKDD International Conference on Knowledge Discovery and Data Mining*, 2018.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. Qiu, J. Tang, H. Ma, Y. Dong, K. Wang, 和 J. Tang，“Deepinf：在大型社交网络中建模影响力局部性”，发表于*第24届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*，2018年。'
- en: '[134] J. Ma, C. Zhou, P. Cui, H. Yang, and W. Zhu, “Learning disentangled representations
    for recommendation,” in *Advances in Neural Information Processing Systems*, 2019.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] J. Ma, C. Zhou, P. Cui, H. Yang, 和 W. Zhu，“学习解耦表示以进行推荐”，发表于*神经信息处理系统进展*，2019年。'
- en: '[135] C. W. Coley, R. Barzilay, W. H. Green, T. S. Jaakkola, and K. F. Jensen,
    “Convolutional embedding of attributed molecular graphs for physical property
    prediction,” *Journal of chemical information and modeling*, 2017.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] C. W. Coley, R. Barzilay, W. H. Green, T. S. Jaakkola, 和 K. F. Jensen，“用于物理属性预测的属性分子图的卷积嵌入”，*化学信息与建模期刊*，2017年。'
- en: '[136] T. Xie and J. C. Grossman, “Crystal graph convolutional neural networks
    for an accurate and interpretable prediction of material properties,” *Physical
    Review Letters*, 2018.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] T. Xie 和 J. C. Grossman，“用于准确且可解释的材料属性预测的晶体图卷积神经网络”，*物理评论快报*，2018年。'
- en: '[137] S. I. Ktena, S. Parisot, E. Ferrante, M. Rajchl, M. Lee, B. Glocker,
    and D. Rueckert, “Distance metric learning using graph convolutional networks:
    Application to functional brain networks,” in *International Conference on Medical
    Image Computing and Computer-Assisted Intervention*, 2017.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] S. I. Ktena, S. Parisot, E. Ferrante, M. Rajchl, M. Lee, B. Glocker,
    和 D. Rueckert，“利用图卷积网络的距离度量学习：应用于功能脑网络”，发表于*国际医学影像计算与计算机辅助干预会议*，2017年。'
- en: '[138] M. Zitnik, M. Agrawal, and J. Leskovec, “Modeling polypharmacy side effects
    with graph convolutional networks,” *Bioinformatics*, 2018.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] M. Zitnik, M. Agrawal, 和 J. Leskovec，“利用图卷积网络建模多重用药副作用”，*生物信息学*，2018年。'
- en: '[139] S. Parisot, S. I. Ktena, E. Ferrante, M. Lee, R. G. Moreno, B. Glocker,
    and D. Rueckert, “Spectral graph convolutions for population-based disease prediction,”
    in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, 2017.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] S. Parisot, S. I. Ktena, E. Ferrante, M. Lee, R. G. Moreno, B. Glocker,
    和 D. Rueckert，“用于基于人群的疾病预测的谱图卷积”，发表于*国际医学影像计算与计算机辅助干预会议*，2017年。'
- en: '[140] F. Dutil, J. P. Cohen, M. Weiss, G. Derevyanko, and Y. Bengio, “Towards
    gene expression convolutions using gene interaction graphs,” in *International
    Conference on Machine Learning Workshop on Computational Biology*, 2018.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] F. Dutil, J. P. Cohen, M. Weiss, G. Derevyanko, 和 Y. Bengio，“通过基因交互图实现基因表达卷积”，发表于*国际机器学习会议计算生物学研讨会*，2018年。'
- en: '[141] J. Bastings, I. Titov, W. Aziz, D. Marcheggiani, and K. Simaan, “Graph
    convolutional encoders for syntax-aware neural machine translation,” in *Proceedings
    of the 2017 Conference on Empirical Methods in Natural Language Processing*, 2017.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] J. Bastings, I. Titov, W. Aziz, D. Marcheggiani, 和 K. Simaan，“用于语法感知神经机器翻译的图卷积编码器”，发表于*2017年自然语言处理实证方法会议论文集*，2017年。'
- en: '[142] D. Marcheggiani and I. Titov, “Encoding sentences with graph convolutional
    networks for semantic role labeling,” in *EMNLP*, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] D. Marcheggiani 和 I. Titov，“使用图卷积网络对句子进行编码以进行语义角色标注”，发表于*EMNLP*，2017年。'
- en: '[143] V. Garcia and J. Bruna, “Few-shot learning with graph neural networks,”
    in *Proceedings of the 7th International Conference on Learning Representations*,
    2018.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] V. Garcia 和 J. Bruna，“图神经网络的少样本学习”，发表于 *第七届国际学习表征会议论文集*，2018 年。'
- en: '[144] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena, “Structural-rnn: Deep
    learning on spatio-temporal graphs,” in *Computer Vision and Pattern Recognition*,
    2016.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] A. Jain, A. R. Zamir, S. Savarese 和 A. Saxena，“结构化 RNN：在时空图上的深度学习”，发表于
    *计算机视觉与模式识别*，2016 年。'
- en: '[145] X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun, “3d graph neural networks
    for rgbd semantic segmentation,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] X. Qi, R. Liao, J. Jia, S. Fidler 和 R. Urtasun，“用于 RGBD 语义分割的 3D 图神经网络”，发表于
    *IEEE 计算机视觉与模式识别会议论文集*，2017 年。'
- en: '[146] K. Marino, R. Salakhutdinov, and A. Gupta, “The more you know: Using
    knowledge graphs for image classification,” in *2017 IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] K. Marino, R. Salakhutdinov 和 A. Gupta，“你知道得越多：使用知识图谱进行图像分类”，发表于 *2017
    年 IEEE 计算机视觉与模式识别会议*，2017 年。'
- en: '[147] S. Qi, W. Wang, B. Jia, J. Shen, and S.-C. Zhu, “Learning human-object
    interactions by graph parsing neural networks,” in *Proceedings of the European
    Conference on Computer Vision*, 2018.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] S. Qi, W. Wang, B. Jia, J. Shen 和 S.-C. Zhu，“通过图解析神经网络学习人类与物体的交互”，发表于
    *欧洲计算机视觉会议论文集*，2018 年。'
- en: '[148] B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph convolutional networks:
    A deep learning framework for traffic forecasting,” in *Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence*, 2018.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] B. Yu, H. Yin 和 Z. Zhu，“时空图卷积网络：用于交通预测的深度学习框架”，发表于 *第二十七届国际人工智能联合会议论文集*，2018
    年。'
- en: '[149] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent
    neural network: Data-driven traffic forecasting,” in *Proceedings of the 7th International
    Conference on Learning Representations*, 2018.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Li, R. Yu, C. Shahabi 和 Y. Liu，“扩散卷积递归神经网络：数据驱动的交通预测”，发表于 *第七届国际学习表征会议论文集*，2018
    年。'
- en: '[150] M. Allamanis, M. Brockschmidt, and M. Khademi, “Learning to represent
    programs with graphs,” in *Proceedings of the 7th International Conference on
    Learning Representations*, 2018.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] M. Allamanis, M. Brockschmidt 和 M. Khademi，“学习通过图表示程序”，发表于 *第七届国际学习表征会议论文集*，2018
    年。'
- en: '[151] Z. Li, Q. Chen, and V. Koltun, “Combinatorial optimization with graph
    convolutional networks and guided tree search,” in *NeurIPS*, 2018.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Z. Li, Q. Chen 和 V. Koltun，“结合图卷积网络和引导树搜索的组合优化”，发表于 *NeurIPS*，2018 年。'
- en: '[152] M. Prates, P. H. Avelar, H. Lemos, L. C. Lamb, and M. Y. Vardi, “Learning
    to solve np-complete problems: A graph neural network for decision tsp,” in *AAAI*,
    2019.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] M. Prates, P. H. Avelar, H. Lemos, L. C. Lamb 和 M. Y. Vardi，“学习解决 NP
    完全问题：用于决策 TSP 的图神经网络”，发表于 *AAAI*，2019 年。'
- en: '[153] S. Sukhbaatar, R. Fergus *et al.*, “Learning multiagent communication
    with backpropagation,” in *Advances in Neural Information Processing Systems*,
    2016.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] S. Sukhbaatar, R. Fergus *等*，“通过反向传播学习多智能体通信”，发表于 *神经信息处理系统进展*，2016 年。'
- en: '[154] P. W. Battaglia, R. Pascanu, M. Lai, D. Rezende, and K. Kavukcuoglu,
    “Interaction networks for learning about objects, relations and physics,” in *Advances
    in Neural Information Processing Systems*, 2016.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] P. W. Battaglia, R. Pascanu, M. Lai, D. Rezende 和 K. Kavukcuoglu，“用于学习对象、关系和物理的交互网络”，发表于
    *神经信息处理系统进展*，2016 年。'
- en: '[155] Y. Hoshen, “Vain: Attentional multi-agent predictive modeling,” in *Advances
    in Neural Information Processing Systems*, 2017.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Y. Hoshen，“Vain：关注的多智能体预测建模”，发表于 *神经信息处理系统进展*，2017 年。'
- en: '[156] A. Santoro, D. Raposo, D. G. T. Barrett, M. Malinowski, R. Pascanu, P. Battaglia,
    and T. Lillicrap, “A simple neural network module for relational reasoning,” in
    *NeuRIPS*, 2017.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] A. Santoro, D. Raposo, D. G. T. Barrett, M. Malinowski, R. Pascanu, P.
    Battaglia 和 T. Lillicrap，“用于关系推理的简单神经网络模块”，发表于 *NeuRIPS*，2017 年。'
- en: '[157] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang,
    “Heterogeneous network embedding via deep architectures,” in *Proceedings of the
    21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*,
    2015.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal 和 T. S. Huang，“通过深度架构的异质网络嵌入”，发表于
    *第21届 ACM SIGKDD 知识发现与数据挖掘国际会议论文集*，2015 年。'
- en: '[158] T. Derr, Y. Ma, and J. Tang, “Signed graph convolutional network,” in
    *Data Mining, 2018 IEEE International Conference on*, 2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] T. Derr, Y. Ma 和 J. Tang，“有符号图卷积网络”，发表于 *数据挖掘，2018 年 IEEE 国际会议*，2018
    年。'
- en: '[159] K. Tu, P. Cui, X. Wang, F. Wang, and W. Zhu, “Structural deep embedding
    for hyper-networks,” in *AAAI*, 2018.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] K. Tu, P. Cui, X. Wang, F. Wang 和 W. Zhu，“超网络的结构深嵌入，”在 *AAAI*，2018。'
- en: '[160] K. Tu, J. Ma, P. Cui, J. Pei, and W. Zhu, “Autone: Hyperparameter optimization
    for massive network embedding,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] K. Tu, J. Ma, P. Cui, J. Pei 和 W. Zhu，“Autone：大规模网络嵌入的超参数优化，”在 *第25届
    ACM SIGKDD 国际知识发现与数据挖掘会议论文集*，2019。'
- en: '[161] R. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec, “Gnn explainer:
    A tool for post-hoc explanation of graph neural networks,” in *Advances in Neural
    Information Processing Systems*, 2019.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] R. Ying, D. Bourgeois, J. You, M. Zitnik 和 J. Leskovec，“Gnn explainer：一种图神经网络后期解释工具，”在
    *神经信息处理系统进展*，2019。'
- en: '[162] D. Zhu, Z. Zhang, P. Cui, and W. Zhu, “Robust graph convolutional networks
    against adversarial attacks,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] D. Zhu, Z. Zhang, P. Cui 和 W. Zhu，“针对对抗攻击的鲁棒图卷积网络，”在 *第25届 ACM SIGKDD
    国际知识发现与数据挖掘会议论文集*，2019。'
- en: '[163] M. Jin, H. Chang, W. Zhu, and S. Sojoudi, “Power up! robust graph convolutional
    network against evasion attacks based on graph powering,” *arXiv preprint arXiv:1905.10029*,
    2019.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] M. Jin, H. Chang, W. Zhu 和 S. Sojoudi，“增强！基于图动力学的鲁棒图卷积网络对抗规避攻击，” *arXiv
    预印本 arXiv:1905.10029*，2019。'
- en: '[164] M. Fey and J. E. Lenssen, “Fast graph representation learning with PyTorch
    Geometric,” in *ICLR Workshop on Representation Learning on Graphs and Manifolds*,
    2019.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] M. Fey 和 J. E. Lenssen，“使用 PyTorch Geometric 进行快速图表示学习，”在 *ICLR 图形和流形上的表示学习研讨会*，2019。'
- en: '[165] M. Wang, L. Yu, D. Zheng, Q. Gan, Y. Gai, Z. Ye, M. Li, J. Zhou, Q. Huang,
    C. Ma, Z. Huang, Q. Guo, H. Zhang, H. Lin, J. Zhao, J. Li, A. J. Smola, and Z. Zhang,
    “Deep graph library: Towards efficient and scalable deep learning on graphs,”
    *ICLR Workshop on Representation Learning on Graphs and Manifolds*, 2019.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] M. Wang, L. Yu, D. Zheng, Q. Gan, Y. Gai, Z. Ye, M. Li, J. Zhou, Q. Huang,
    C. Ma, Z. Huang, Q. Guo, H. Zhang, H. Lin, J. Zhao, J. Li, A. J. Smola 和 Z. Zhang，“深度图书馆：面向高效可扩展的图深度学习，”在
    *ICLR 图形和流形上的表示学习研讨会*，2019。'
- en: '[166] R. Zhu, K. Zhao, H. Yang, W. Lin, C. Zhou, B. Ai, Y. Li, and J. Zhou,
    “Aligraph: A comprehensive graph neural network platform,” in *Proceedings of
    the 45th International Conference on Very Large Data Bases*, 2019.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] R. Zhu, K. Zhao, H. Yang, W. Lin, C. Zhou, B. Ai, Y. Li 和 J. Zhou，“Aligraph：一个全面的图神经网络平台，”在
    *第45届国际大数据会议论文集*，2019。'
- en: '[167] O. Shchur, M. Mumme, A. Bojchevski, and S. Günnemann, “Pitfalls of graph
    neural network evaluation,” *Relational Representation Learning Workshop, NeurIPS
    2018*, 2018.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] O. Shchur, M. Mumme, A. Bojchevski 和 S. Günnemann，“图神经网络评估的陷阱，” *关系表示学习研讨会，NeurIPS
    2018*，2018。'
- en: '[168] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, “Learning deep
    generative models of graphs,” *arXiv preprint arXiv:1803.03324*, 2018.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Li, O. Vinyals, C. Dyer, R. Pascanu 和 P. Battaglia，“学习图的深度生成模型，” *arXiv
    预印本 arXiv:1803.03324*，2018。'
- en: '[169] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad,
    “Collective classification in network data,” *AI magazine*, 2008.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher 和 T. Eliassi-Rad，“网络数据中的集体分类，”
    *AI 杂志*，2008。'
- en: '[170] A. Ortega, P. Frossard, J. Kovačević, J. M. Moura, and P. Vandergheynst,
    “Graph signal processing: Overview, challenges, and applications,” *Proceedings
    of the IEEE*, 2018.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] A. Ortega, P. Frossard, J. Kovačević, J. M. Moura 和 P. Vandergheynst，“图信号处理：概述、挑战和应用，”
    *IEEE 会议论文集*，2018。'
- en: '| ![[Uncaptioned image]](img/d160855baae52eba5d18410a64452a06.png) | Ziwei
    Zhang received his B.S. from the Department of Physics, Tsinghua University in
    2016\. He is currently pursuing a Ph.D. Degree in the Department of Computer Science
    and Technology at Tsinghua University. His research interests focus on network
    embedding and machine learning on graph data, especially in developing scalable
    algorithms for large-scale networks. He has published several papers in prestigious
    conferences and journals, including KDD, AAAI, IJCAI, and TKDE. |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/d160855baae52eba5d18410a64452a06.png) | Ziwei Zhang 于2016年从清华大学物理系获得学士学位。他目前在清华大学计算机科学与技术系攻读博士学位。他的研究兴趣集中在网络嵌入和图数据的机器学习上，特别是在开发大规模网络的可扩展算法方面。他在包括
    KDD、AAAI、IJCAI 和 TKDE 在内的多个著名会议和期刊上发表了多篇论文。 |'
- en: '| ![[Uncaptioned image]](img/3e973fc1fa996149438faaad3dfbd44a.png) | Peng Cui
    received the Ph.D. degree from Tsinghua University in 2010\. He is currently an
    associate professor with tenure at Tsinghua University. His research interests
    include network representation learning, human behavioral modeling, and social-sensed
    multimedia computing. He has published more than 100 papers in prestigious conferences
    and journals in data mining and multimedia. His recent research efforts have received
    the SIGKDD 2016 Best Paper Finalist, the ICDM 2015 Best Student Paper Award, the
    SIGKDD 2014 Best Paper Finalist, the IEEE ICME 2014 Best Paper Award, the ACM
    MM12 Grand Challenge Multimodal Award, and the MMM13 Best Paper Award. He is an
    associate editor of IEEE Transactions on Knowledge and Data Engineering, the IEEE
    Transactions on Big Data, the ACM Transactions on Multimedia Computing, Communications,
    and Applications, the Elsevier Journal on Neurocomputing, etc. He was the recipient
    of the ACM China Rising Star Award in 2015. |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/3e973fc1fa996149438faaad3dfbd44a.png) | Peng Cui 于2010年获得清华大学博士学位。目前，他是清华大学的终身副教授。他的研究兴趣包括网络表示学习、人类行为建模和社会感知多媒体计算。他在数据挖掘和多媒体领域的顶级会议和期刊上发表了100多篇论文。他近期的研究成果获得了SIGKDD
    2016最佳论文终选、ICDM 2015最佳学生论文奖、SIGKDD 2014最佳论文终选、IEEE ICME 2014最佳论文奖、ACM MM12大挑战多模态奖和MMM13最佳论文奖。他是《IEEE知识与数据工程汇刊》、《IEEE大数据汇刊》、《ACM多媒体计算、通信与应用汇刊》、《Elsevier神经计算期刊》等的副编辑。他于2015年获得ACM中国新星奖。'
- en: '| ![[Uncaptioned image]](img/653d001f3ce84903606150601d1b90de.png) | Wenwu
    Zhu is currently a Professor and Deputy Head of the Computer Science Department
    of Tsinghua University and Vice Dean of National Research Center on Information
    Science and Technology. Prior to his current post, he was a Senior Researcher
    and Research Manager at Microsoft Research Asia. He was the Chief Scientist and
    Director at Intel Research China from 2004 to 2008\. He worked at Bell Labs New
    Jersey as a Member of Technical Staff during 1996-1999\. He received his Ph.D.
    degree from New York University in 1996. He served as the Editor-in-Chief for
    the IEEE Transactions on Multimedia (T-MM) from January 1, 2017, to December 31,
    2019\. He has been serving as Vice EiC for IEEE Transactions on Circuits and Systems
    for Video Technology (TCSVT) and the chair of the steering committee for IEEE
    T-MM since January 1, 2020\. His current research interests are in the areas of
    multimedia computing and networking, and big data. He has published over 400 papers
    in the referred journals and received nine Best Paper Awards including IEEE TCSVT
    in 2001 and 2019, and ACM Multimedia 2012\. He is an IEEE Fellow, AAAS Fellow,
    SPIE Fellow and a member of the European Academy of Sciences (Academia Europaea).
    |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/653d001f3ce84903606150601d1b90de.png) | Wenwu Zhu 目前是清华大学计算机科学系教授、副主任和国家信息科学与技术研究中心副主任。在现职位之前，他曾是微软亚洲研究院的高级研究员和研究经理。他在2004年至2008年期间担任英特尔中国研究院的首席科学家和主任。他曾在1996年至1999年期间担任贝尔实验室新泽西分部的技术人员。他于1996年在纽约大学获得博士学位。他曾在2017年1月1日至2019年12月31日担任《IEEE多媒体汇刊（T-MM）》的主编。他自2020年1月1日起担任《IEEE视频技术电路与系统汇刊（TCSVT）》的副主编，并且是《IEEE
    T-MM》指导委员会主席。他当前的研究兴趣包括多媒体计算与网络和大数据。他在被审稿期刊上发表了400多篇论文，并获得了九个最佳论文奖，包括2001年和2019年的IEEE
    TCSVT奖以及2012年的ACM多媒体奖。他是IEEE Fellow、AAAS Fellow、SPIE Fellow，并且是欧洲科学院（Academia
    Europaea）的成员。'
- en: 'TABLE IX: A collection of published source code. O.A. = Original Authors'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IX：已发布源代码的集合。O.A. = 原作者
- en: '| Category | Method | URL | O.A. | Language/Framework |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 网址 | O.A. | 语言/框架 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Graph RNNs | GGS-NNs [[24](#bib.bib24)] | https://github.com/yujiali/ggnn
    | Yes | Lua/Torch |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| 图形 RNNs | GGS-NNs [[24](#bib.bib24)] | https://github.com/yujiali/ggnn |
    是 | Lua/Torch |'
- en: '| SSE [[25](#bib.bib25)] | https://github.com/Hanjun-Dai/steady_state_embedding
    | Yes | C |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| SSE [[25](#bib.bib25)] | https://github.com/Hanjun-Dai/steady_state_embedding
    | 是 | C |'
- en: '| You et al. [[26](#bib.bib26)] | https://github.com/JiaxuanYou/graph-generation
    | Yes | Python/PyTorch |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| 你等人[[26](#bib.bib26)] | https://github.com/JiaxuanYou/graph-generation |
    是 | Python/PyTorch |'
- en: '| RMGCNN [[28](#bib.bib28)] | https://github.com/fmonti/mgcnn | Yes | Python/TensorFlow
    |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| RMGCNN [[28](#bib.bib28)] | https://github.com/fmonti/mgcnn | 是 | Python/TensorFlow
    |'
- en: '| GCNs | ChebNet [[42](#bib.bib42)] | https://github.com/mdeff/cnn_graph |
    Yes | Python/TensorFlow |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| GCNs | ChebNet [[42](#bib.bib42)] | https://github.com/mdeff/cnn_graph |
    是 | Python/TensorFlow |'
- en: '| Kipf&Welling [[43](#bib.bib43)] | https://github.com/tkipf/gcn | Yes | Python/TensorFlow
    |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| Kipf&Welling [[43](#bib.bib43)] | https://github.com/tkipf/gcn | 是 | Python/TensorFlow
    |'
- en: '| CayletNet [[44](#bib.bib44)] | https://github.com/amoliu/CayleyNet | Yes
    | Python/TensorFlow |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| CayletNet [[44](#bib.bib44)] | https://github.com/amoliu/CayleyNet | 是 |
    Python/TensorFlow |'
- en: '| GWNN [[45](#bib.bib45)] | https://github.com/Eilene/GWNN | Yes | Python/TensorFlow
    |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| GWNN [[45](#bib.bib45)] | https://github.com/Eilene/GWNN | 是 | Python/TensorFlow
    |'
- en: '| Neural FPs [[46](#bib.bib46)] | https://github.com/HIPS/neural-fingerprint
    | Yes | Python |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| Neural FPs [[46](#bib.bib46)] | https://github.com/HIPS/neural-fingerprint
    | 是 | Python |'
- en: '| PATCHY-SAN [[47](#bib.bib47)] | https://github.com/seiya-kumada/patchy-san
    | No | Python |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| PATCHY-SAN [[47](#bib.bib47)] | https://github.com/seiya-kumada/patchy-san
    | 否 | Python |'
- en: '| LGCN [[48](#bib.bib48)] | https://github.com/divelab/lgcn/ | Yes | Python/TensorFlow
    |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| LGCN [[48](#bib.bib48)] | https://github.com/divelab/lgcn/ | 是 | Python/TensorFlow
    |'
- en: '| SortPooling [[49](#bib.bib49)] | https://github.com/muhanzhang/DGCNN | Yes
    | Lua/Torch |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| SortPooling [[49](#bib.bib49)] | https://github.com/muhanzhang/DGCNN | 是
    | Lua/Torch |'
- en: '| DCNN [[50](#bib.bib50)] | https://github.com/jcatw/dcnn | Yes | Python/Theano
    |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| DCNN [[50](#bib.bib50)] | https://github.com/jcatw/dcnn | 是 | Python/Theano
    |'
- en: '| DGCN [[51](#bib.bib51)] | https://github.com/ZhuangCY/Coding-NN | Yes | Python/Theano
    |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| DGCN [[51](#bib.bib51)] | https://github.com/ZhuangCY/Coding-NN | 是 | Python/Theano
    |'
- en: '| MPNNs [[52](#bib.bib52)] | https://github.com/brain-research/mpnn | Yes |
    Python/TensorFlow |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| MPNNs [[52](#bib.bib52)] | https://github.com/brain-research/mpnn | 是 | Python/TensorFlow
    |'
- en: '| GraphSAGE [[53](#bib.bib53)] | https://github.com/williamleif/GraphSAGE |
    Yes | Python/TensorFlow |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| GraphSAGE [[53](#bib.bib53)] | https://github.com/williamleif/GraphSAGE |
    是 | Python/TensorFlow |'
- en: '| GNs [[9](#bib.bib9)] | https://github.com/deepmind/graph_nets | Yes | Python/TensorFlow
    |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| GNs [[9](#bib.bib9)] | https://github.com/deepmind/graph_nets | 是 | Python/TensorFlow
    |'
- en: '| DiffPool [[56](#bib.bib56)] | https://github.com/RexYing/graph-pooling |
    Yes | Python/PyTorch |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| DiffPool [[56](#bib.bib56)] | https://github.com/RexYing/graph-pooling |
    是 | Python/PyTorch |'
- en: '| GAT [[57](#bib.bib57)] | https://github.com/PetarV-/GAT | Yes | Python/TensorFlow
    |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| GAT [[57](#bib.bib57)] | https://github.com/PetarV-/GAT | 是 | Python/TensorFlow
    |'
- en: '| GaAN [[58](#bib.bib58)] | https://github.com/jennyzhang0215/GaAN | Yes |
    Python/MXNet |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| GaAN [[58](#bib.bib58)] | https://github.com/jennyzhang0215/GaAN | 是 | Python/MXNet
    |'
- en: '| HAN [[59](#bib.bib59)] | https://github.com/Jhy1993/HAN | Yes | Python/TensorFlow
    |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| HAN [[59](#bib.bib59)] | https://github.com/Jhy1993/HAN | 是 | Python/TensorFlow
    |'
- en: '| CLN [[60](#bib.bib60)] | https://github.com/trangptm/Column_networks | Yes
    | Python/Keras |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| CLN [[60](#bib.bib60)] | https://github.com/trangptm/Column_networks | 是
    | Python/Keras |'
- en: '| PPNP [[61](#bib.bib61)] | https://github.com/klicperajo/ppnp | Yes | Python/TensorFlow
    |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| PPNP [[61](#bib.bib61)] | https://github.com/klicperajo/ppnp | 是 | Python/TensorFlow
    |'
- en: '| JK-Nets [[62](#bib.bib62)] | https://github.com/mori97/JKNet-dgl | No | Python/DGL
    |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| JK-Nets [[62](#bib.bib62)] | https://github.com/mori97/JKNet-dgl | 否 | Python/DGL
    |'
- en: '| ECC [[63](#bib.bib63)] | https://github.com/mys007/ecc | Yes | Python/PyTorch
    |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| ECC [[63](#bib.bib63)] | https://github.com/mys007/ecc | 是 | Python/PyTorch
    |'
- en: '| R-GCNs [[64](#bib.bib64)] | https://github.com/tkipf/relational-gcn | Yes
    | Python/Keras |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| R-GCNs [[64](#bib.bib64)] | https://github.com/tkipf/relational-gcn | 是 |
    Python/Keras |'
- en: '| LGNN [[65](#bib.bib65)] | https://github.com/joanbruna/GNN_community | Yes
    | Lua/Torch |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| LGNN [[65](#bib.bib65)] | https://github.com/joanbruna/GNN_community | 是
    | Lua/Torch |'
- en: '| StochasticGCN [[67](#bib.bib67)] | https://github.com/thu-ml/stochastic_gcn
    | Yes | Python/TensorFlow |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| StochasticGCN [[67](#bib.bib67)] | https://github.com/thu-ml/stochastic_gcn
    | 是 | Python/TensorFlow |'
- en: '| FastGCN [[68](#bib.bib68)] | https://github.com/matenure/FastGCN | Yes |
    Python/TensorFlow |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| FastGCN [[68](#bib.bib68)] | https://github.com/matenure/FastGCN | 是 | Python/TensorFlow
    |'
- en: '| Adapt [[69](#bib.bib69)] | https://github.com/huangwb/AS-GCN | Yes | Python/TensorFlow
    |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| Adapt [[69](#bib.bib69)] | https://github.com/huangwb/AS-GCN | 是 | Python/TensorFlow
    |'
- en: '| Li et al. [[70](#bib.bib70)] | https://github.com/liqimai/gcn | Yes | Python/TensorFlow
    |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. [[70](#bib.bib70)] | https://github.com/liqimai/gcn | 是 | Python/TensorFlow
    |'
- en: '| SGC [[71](#bib.bib71)] | https://github.com/Tiiiger/SGC | Yes | Python/PyTorch
    |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| SGC [[71](#bib.bib71)] | https://github.com/Tiiiger/SGC | 是 | Python/PyTorch
    |'
- en: '| GFNN [[72](#bib.bib72)] | https://github.com/gear/gfnn | Yes | Python/PyTorch
    |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| GFNN [[72](#bib.bib72)] | https://github.com/gear/gfnn | 是 | Python/PyTorch
    |'
- en: '| GIN [[73](#bib.bib73)] | https://github.com/weihua916/powerful-gnns | Yes
    | Python/PyTorch |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| GIN [[73](#bib.bib73)] | https://github.com/weihua916/powerful-gnns | 是 |
    Python/PyTorch |'
- en: '| DGI [[74](#bib.bib74)] | https://github.com/PetarV-/DGI | Yes | Python/PyTorch
    |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| DGI [[74](#bib.bib74)] | https://github.com/PetarV-/DGI | 是 | Python/PyTorch
    |'
- en: '| GAEs | SAE [[96](#bib.bib96)] | https://github.com/quinngroup/deep-representations-clustering
    | No | Python/Keras |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| GAEs | SAE [[96](#bib.bib96)] | https://github.com/quinngroup/deep-representations-clustering
    | 否 | Python/Keras |'
- en: '| SDNE [[97](#bib.bib97)] | https://github.com/suanrong/SDNE | Yes | Python/TensorFlow
    |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| SDNE [[97](#bib.bib97)] | https://github.com/suanrong/SDNE | 是 | Python/TensorFlow
    |'
- en: '| DNGR [[98](#bib.bib98)] | https://github.com/ShelsonCao/DNGR | Yes | Matlab
    |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| DNGR [[98](#bib.bib98)] | https://github.com/ShelsonCao/DNGR | 是 | Matlab
    |'
- en: '| GC-MC [[99](#bib.bib99)] | https://github.com/riannevdberg/gc-mc | Yes |
    Python/TensorFlow |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| GC-MC [[99](#bib.bib99)] | https://github.com/riannevdberg/gc-mc | 是 | Python/TensorFlow
    |'
- en: '| DRNE [[100](#bib.bib100)] | https://github.com/tadpole/DRNE | Yes | Python/TensorFlow
    |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| DRNE [[100](#bib.bib100)] | https://github.com/tadpole/DRNE | 是 | Python/TensorFlow
    |'
- en: '| G2G [[101](#bib.bib101)] | https://github.com/abojchevski/graph2gauss | Yes
    | Python/TensorFlow |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| G2G [[101](#bib.bib101)] | https://github.com/abojchevski/graph2gauss | 是
    | Python/TensorFlow |'
- en: '| VGAE [[102](#bib.bib102)] | https://github.com/tkipf/gae | Yes | Python/TensorFlow
    |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| VGAE [[102](#bib.bib102)] | https://github.com/tkipf/gae | 是 | Python/TensorFlow
    |'
- en: '| DVNE [[103](#bib.bib103)] | http://nrl.thumedialab.com | Yes | Python/TensorFlow
    |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| DVNE [[103](#bib.bib103)] | http://nrl.thumedialab.com | 是 | Python/TensorFlow
    |'
- en: '| ARGA/ARVGA [[104](#bib.bib104)] | https://github.com/Ruiqi-Hu/ARGA | Yes
    | Python/TensorFlow |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| ARGA/ARVGA [[104](#bib.bib104)] | https://github.com/Ruiqi-Hu/ARGA | 是 |
    Python/TensorFlow |'
- en: '| NetRA [[105](#bib.bib105)] | https://github.com/chengw07/NetRA | Yes | Python/PyTorch
    |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| NetRA [[105](#bib.bib105)] | https://github.com/chengw07/NetRA | 是 | Python/PyTorch
    |'
- en: '| Graph RLs | GCPN [[116](#bib.bib116)] | https://github.com/bowenliu16/rl_graph_generation
    | Yes | Python/TensorFlow |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| 图神经网络 | GCPN [[116](#bib.bib116)] | https://github.com/bowenliu16/rl_graph_generation
    | 是 | Python/TensorFlow |'
- en: '| MolGAN [[117](#bib.bib117)] | https://github.com/nicola-decao/MolGAN | Yes
    | Python/TensorFlow |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| MolGAN [[117](#bib.bib117)] | https://github.com/nicola-decao/MolGAN | 是
    | Python/TensorFlow |'
- en: '| GAM [[119](#bib.bib119)] | https://github.com/benedekrozemberczki/GAM | Yes
    | Python/Pytorhc |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| GAM [[119](#bib.bib119)] | https://github.com/benedekrozemberczki/GAM | 是
    | Python/Pytorhc |'
- en: '| DeepPath [[120](#bib.bib120)] | https://github.com/xwhan/DeepPath | Yes |
    Python/TensorFlow |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| DeepPath [[120](#bib.bib120)] | https://github.com/xwhan/DeepPath | 是 | Python/TensorFlow
    |'
- en: '| MINERVA [[121](#bib.bib121)] | https://github.com/shehzaadzd/MINERVA | Yes
    | Python/TensorFlow |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| MINERVA [[121](#bib.bib121)] | https://github.com/shehzaadzd/MINERVA | 是
    | Python/TensorFlow |'
- en: '| Graph adversarial methods | GraphGAN [[124](#bib.bib124)] | https://github.com/hwwang55/GraphGAN
    | Yes | Python/TensorFlow |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| 图对抗方法 | GraphGAN [[124](#bib.bib124)] | https://github.com/hwwang55/GraphGAN
    | 是 | Python/TensorFlow |'
- en: '| GraphSGAN [[126](#bib.bib126)] | https://github.com/dm-thu/GraphSGAN | Yes
    | Python/PyTorch |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| GraphSGAN [[126](#bib.bib126)] | https://github.com/dm-thu/GraphSGAN | 是
    | Python/PyTorch |'
- en: '| NetGAN [[127](#bib.bib127)] | https://github.com/danielzuegner/netgan | Yes
    | Python/TensorFlow |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| NetGAN [[127](#bib.bib127)] | https://github.com/danielzuegner/netgan | 是
    | Python/TensorFlow |'
- en: '| Nettack [[128](#bib.bib128)] | https://github.com/danielzuegner/nettack |
    Yes | Python/TensorFlow |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| Nettack [[128](#bib.bib128)] | https://github.com/danielzuegner/nettack |
    是 | Python/TensorFlow |'
- en: '| Dai et al. [[129](#bib.bib129)] | https://github.com/Hanjun-Dai/graph_adversarial_attack
    | Yes | Python/PyTorch |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| Dai 等人 [[129](#bib.bib129)] | https://github.com/Hanjun-Dai/graph_adversarial_attack
    | 是 | Python/PyTorch |'
- en: '| Zugner&Gunnemann [[130](#bib.bib130)] | https://github.com/danielzuegner/gnn-meta-attack
    | Yes | Python/TensorFlow |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| Zugner & Gunnemann [[130](#bib.bib130)] | https://github.com/danielzuegner/gnn-meta-attack
    | 是 | Python/TensorFlow |'
- en: '| Applications | DeepInf [[133](#bib.bib133)] | https://github.com/xptree/DeepInf
    | Yes | Python/PyTorch |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | DeepInf [[133](#bib.bib133)] | https://github.com/xptree/DeepInf | 是
    | Python/PyTorch |'
- en: '| Ma et al. [[134](#bib.bib134)] | https://jianxinma.github.io/assets/disentangle-recsys-v1.zip
    | Yes | Python/TensorFlow |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '| Ma 等人 [[134](#bib.bib134)] | https://jianxinma.github.io/assets/disentangle-recsys-v1.zip
    | 是 | Python/TensorFlow |'
- en: '| CGCNN [[136](#bib.bib136)] | https://github.com/txie-93/cgcnn | Yes | Python/PyTorch
    |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '| CGCNN [[136](#bib.bib136)] | https://github.com/txie-93/cgcnn | 是 | Python/PyTorch
    |'
- en: '| Ktena et al. [[137](#bib.bib137)] | https://github.com/sk1712/gcn_metric_learning
    | Yes | Python |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '| Ktena 等人 [[137](#bib.bib137)] | https://github.com/sk1712/gcn_metric_learning
    | 是 | Python |'
- en: '| Decagon [[138](#bib.bib138)] | https://github.com/mims-harvard/decagon |
    Yes | Python/PyTorch |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| Decagon [[138](#bib.bib138)] | https://github.com/mims-harvard/decagon |
    是 | Python/PyTorch |'
- en: '| Parisot et al. [[139](#bib.bib139)] | https://github.com/parisots/population-gcn
    | Yes | Python/TensorFlow |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| Parisot 等人 [[139](#bib.bib139)] | https://github.com/parisots/population-gcn
    | 是 | Python/TensorFlow |'
- en: '| Dutil et al. [[140](#bib.bib140)] | https://github.com/mila-iqia/gene-graph-conv
    | Yes | Python/PyTorch |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| Dutil 等人 [[140](#bib.bib140)] | https://github.com/mila-iqia/gene-graph-conv
    | 是 | Python/PyTorch |'
- en: '| Bastings et al. [[141](#bib.bib141)] | https://github.com/bastings/neuralmonkey/tree/emnlp_gcn
    | Yes | Python/TensorFlow |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| Bastings 等人 [[141](#bib.bib141)] | https://github.com/bastings/neuralmonkey/tree/emnlp_gcn
    | 是 | Python/TensorFlow |'
- en: '| Neural-dep-srl [[142](#bib.bib142)] | https://github.com/diegma/neural-dep-srl
    | Yes | Python/Therano |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| Neural-dep-srl [[142](#bib.bib142)] | https://github.com/diegma/neural-dep-srl
    | 是 | Python/Therano |'
- en: '| Garcia&Bruna [[143](#bib.bib143)] | https://github.com/vgsatorras/few-shot-gnn
    | Yes | Python/PyTorch |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| Garcia & Bruna [[143](#bib.bib143)] | https://github.com/vgsatorras/few-shot-gnn
    | 是 | Python/PyTorch |'
- en: '| S-RNN [[144](#bib.bib144)] | https://github.com/asheshjain399/RNNexp | Yes
    | Python/Therano |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| S-RNN [[144](#bib.bib144)] | https://github.com/asheshjain399/RNNexp | 是
    | Python/Therano |'
- en: '| 3DGNN [[145](#bib.bib145)] | https://github.com/xjqicuhk/3DGNN | Yes | Matlab/Caffe
    |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '| 3DGNN [[145](#bib.bib145)] | https://github.com/xjqicuhk/3DGNN | 是 | Matlab/Caffe
    |'
- en: '| GPNN [[147](#bib.bib147)] | https://github.com/SiyuanQi/gpnn | Yes | Python/PyTorch
    |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '| GPNN [[147](#bib.bib147)] | https://github.com/SiyuanQi/gpnn | 是 | Python/PyTorch
    |'
- en: '| STGCN [[148](#bib.bib148)] | https://github.com/VeritasYin/STGCN_IJCAI-18
    | Yes | Python/TensorFlow |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '| STGCN [[148](#bib.bib148)] | https://github.com/VeritasYin/STGCN_IJCAI-18
    | 是 | Python/TensorFlow |'
- en: '| DCRNN [[149](#bib.bib149)] | https://github.com/liyaguang/DCRNN | Yes | Python/TensorFlow
    |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| DCRNN [[149](#bib.bib149)] | https://github.com/liyaguang/DCRNN | 是 | Python/TensorFlow
    |'
- en: '| Allamanis et al. [[150](#bib.bib150)] | https://github.com/microsoft/tf-gnn-samples
    | Yes | Python/TensorFlow |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
  zh: '| Allamanis et al. [[150](#bib.bib150)] | https://github.com/microsoft/tf-gnn-samples
    | 是 | Python/TensorFlow |'
- en: '| Li et al. [[151](#bib.bib151)] | https://github.com/intel-isl/NPHard | Yes
    | Python/TensorFlow |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. [[151](#bib.bib151)] | https://github.com/intel-isl/NPHard | 是
    | Python/TensorFlow |'
- en: '| TSPGNN [[152](#bib.bib152)] | https://github.com/machine-reasoning-ufrgs/TSP-GNN
    | Yes | Python/TensorFlow |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '| TSPGNN [[152](#bib.bib152)] | https://github.com/machine-reasoning-ufrgs/TSP-GNN
    | 是 | Python/TensorFlow |'
- en: '| CommNet [[153](#bib.bib153)] | https://github.com/facebookresearch/CommNet
    | Yes | Lua/Torch |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| CommNet [[153](#bib.bib153)] | https://github.com/facebookresearch/CommNet
    | 是 | Lua/Torch |'
- en: '| Interaction network [[154](#bib.bib154)] | https://github.com/jaesik817/Interaction-networks_tensorflow
    | No | Python/TensorFlow |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '| Interaction network [[154](#bib.bib154)] | https://github.com/jaesik817/Interaction-networks_tensorflow
    | 否 | Python/TensorFlow |'
- en: '| Relation networks [[156](#bib.bib156)] | https://github.com/kimhc6028/relational-networks
    | No | Python/PyTorch |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| Relation networks [[156](#bib.bib156)] | https://github.com/kimhc6028/relational-networks
    | 否 | Python/PyTorch |'
- en: '| Miscellaneous | SGCN [[158](#bib.bib158)] | http://www.cse.msu.edu/~derrtyle/
    | Yes | Python/PyTorch |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | SGCN [[158](#bib.bib158)] | http://www.cse.msu.edu/~derrtyle/ | 是 |
    Python/PyTorch |'
- en: '| DHNE [[159](#bib.bib159)] | https://github.com/tadpole/DHNE | Yes | Python/TensorFlow
    |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '| DHNE [[159](#bib.bib159)] | https://github.com/tadpole/DHNE | 是 | Python/TensorFlow
    |'
- en: '| AutoNE [[160](#bib.bib160)] | https://github.com/tadpole/AutoNE | Yes | Python
    |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '| AutoNE [[160](#bib.bib160)] | https://github.com/tadpole/AutoNE | 是 | Python
    |'
- en: '| Gnn-explainer [[161](#bib.bib161)] | https://github.com/RexYing/gnn-model-explainer
    | Yes | Python/PyTorch |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
  zh: '| Gnn-explainer [[161](#bib.bib161)] | https://github.com/RexYing/gnn-model-explainer
    | 是 | Python/PyTorch |'
- en: '| RGCN [[162](#bib.bib162)] | https://github.com/thumanlab/nrlweb | Yes | Python/TensorFlow
    |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
  zh: '| RGCN [[162](#bib.bib162)] | https://github.com/thumanlab/nrlweb | 是 | Python/TensorFlow
    |'
- en: '| GNN-benchmark [[167](#bib.bib167)] | https://github.com/shchur/gnn-benchmark
    | Yes | Python/TensorFlow |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '| GNN-benchmark [[167](#bib.bib167)] | https://github.com/shchur/gnn-benchmark
    | 是 | Python/TensorFlow |'
- en: 'TABLE X: A Table of Methods for Six Common Tasks'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 表 X：六种常见任务的方法表
- en: '| Type | Task | Methods |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 任务 | 方法 |'
- en: '| Node-focused Tasks | Node Clustering | [[96](#bib.bib96), [59](#bib.bib59),
    [157](#bib.bib157), [97](#bib.bib97), [124](#bib.bib124), [104](#bib.bib104),
    [44](#bib.bib44), [98](#bib.bib98)] |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
  zh: '| 节点聚焦任务 | 节点聚类 | [[96](#bib.bib96), [59](#bib.bib59), [157](#bib.bib157),
    [97](#bib.bib97), [124](#bib.bib124), [104](#bib.bib104), [44](#bib.bib44), [98](#bib.bib98)]
    |'
- en: '| Node Classification | Transductive |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
  zh: '| 节点分类 | 转导 |'
- en: '&#124; [[41](#bib.bib41), [23](#bib.bib23), [25](#bib.bib25), [27](#bib.bib27),
    [42](#bib.bib42), [48](#bib.bib48), [54](#bib.bib54), [45](#bib.bib45), [51](#bib.bib51),
    [43](#bib.bib43), [50](#bib.bib50), [53](#bib.bib53), [44](#bib.bib44), [29](#bib.bib29)]
    &#124;'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[41](#bib.bib41), [23](#bib.bib23), [25](#bib.bib25), [27](#bib.bib27),
    [42](#bib.bib42), [48](#bib.bib48), [54](#bib.bib54), [45](#bib.bib45), [51](#bib.bib51),
    [43](#bib.bib43), [50](#bib.bib50), [53](#bib.bib53), [44](#bib.bib44), [29](#bib.bib29)]
    &#124;'
- en: '&#124; [[64](#bib.bib64), [61](#bib.bib61), [65](#bib.bib65), [57](#bib.bib57),
    [59](#bib.bib59), [60](#bib.bib60), [68](#bib.bib68), [67](#bib.bib67), [58](#bib.bib58),
    [70](#bib.bib70), [62](#bib.bib62), [69](#bib.bib69), [71](#bib.bib71), [72](#bib.bib72)]
    &#124;'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[64](#bib.bib64), [61](#bib.bib61), [65](#bib.bib65), [57](#bib.bib57),
    [59](#bib.bib59), [60](#bib.bib60), [68](#bib.bib68), [67](#bib.bib67), [58](#bib.bib58),
    [70](#bib.bib70), [62](#bib.bib62), [69](#bib.bib69), [71](#bib.bib71), [72](#bib.bib72)]
    &#124;'
- en: '&#124; [[103](#bib.bib103), [101](#bib.bib101), [74](#bib.bib74), [100](#bib.bib100),
    [126](#bib.bib126), [124](#bib.bib124), [125](#bib.bib125), [162](#bib.bib162),
    [157](#bib.bib157), [97](#bib.bib97), [105](#bib.bib105)] &#124;'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[103](#bib.bib103), [101](#bib.bib101), [74](#bib.bib74), [100](#bib.bib100),
    [126](#bib.bib126), [124](#bib.bib124), [125](#bib.bib125), [162](#bib.bib162),
    [157](#bib.bib157), [97](#bib.bib97), [105](#bib.bib105)] &#124;'
- en: '|'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Inductive | [[74](#bib.bib74), [48](#bib.bib48), [57](#bib.bib57), [53](#bib.bib53),
    [67](#bib.bib67), [58](#bib.bib58), [62](#bib.bib62), [25](#bib.bib25), [71](#bib.bib71),
    [101](#bib.bib101), [72](#bib.bib72), [68](#bib.bib68), [69](#bib.bib69)] |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '| 推断 | [[74](#bib.bib74), [48](#bib.bib48), [57](#bib.bib57), [53](#bib.bib53),
    [67](#bib.bib67), [58](#bib.bib58), [62](#bib.bib62), [25](#bib.bib25), [71](#bib.bib71),
    [101](#bib.bib101), [72](#bib.bib72), [68](#bib.bib68), [69](#bib.bib69)] |'
- en: '| Network Reconstruction | [[157](#bib.bib157), [97](#bib.bib97), [103](#bib.bib103),
    [105](#bib.bib105)] |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
  zh: '| 网络重建 | [[157](#bib.bib157), [97](#bib.bib97), [103](#bib.bib103), [105](#bib.bib105)]
    |'
- en: '| Link Prediction | [[102](#bib.bib102), [99](#bib.bib99), [97](#bib.bib97),
    [101](#bib.bib101), [124](#bib.bib124), [103](#bib.bib103), [66](#bib.bib66),
    [28](#bib.bib28), [64](#bib.bib64), [104](#bib.bib104), [44](#bib.bib44), [27](#bib.bib27),
    [105](#bib.bib105)] |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
  zh: '| 链接预测 | [[102](#bib.bib102), [99](#bib.bib99), [97](#bib.bib97), [101](#bib.bib101),
    [124](#bib.bib124), [103](#bib.bib103), [66](#bib.bib66), [28](#bib.bib28), [64](#bib.bib64),
    [104](#bib.bib104), [44](#bib.bib44), [27](#bib.bib27), [105](#bib.bib105)] |'
- en: '| Graph-focused Tasks | Graph Classification | [[40](#bib.bib40), [41](#bib.bib41),
    [29](#bib.bib29), [44](#bib.bib44), [49](#bib.bib49), [56](#bib.bib56), [55](#bib.bib55),
    [119](#bib.bib119), [132](#bib.bib132), [47](#bib.bib47), [50](#bib.bib50), [73](#bib.bib73),
    [71](#bib.bib71), [63](#bib.bib63), [42](#bib.bib42), [23](#bib.bib23), [54](#bib.bib54)]
    |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
  zh: '| 图相关任务 | 图分类 | [[40](#bib.bib40), [41](#bib.bib41), [29](#bib.bib29), [44](#bib.bib44),
    [49](#bib.bib49), [56](#bib.bib56), [55](#bib.bib55), [119](#bib.bib119), [132](#bib.bib132),
    [47](#bib.bib47), [50](#bib.bib50), [73](#bib.bib73), [71](#bib.bib71), [63](#bib.bib63),
    [42](#bib.bib42), [23](#bib.bib23), [54](#bib.bib54)] |'
- en: '| Graph Generation | Structure-only | [[127](#bib.bib127), [26](#bib.bib26)]
    |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '| 图生成 | 仅结构 | [[127](#bib.bib127), [26](#bib.bib26)] |'
- en: '| Structure+features | [[117](#bib.bib117), [116](#bib.bib116), [168](#bib.bib168)]
    |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
  zh: '| 结构+特征 | [[117](#bib.bib117), [116](#bib.bib116), [168](#bib.bib168)] |'
- en: Appendix A Source Codes
  id: totrans-726
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 源代码
- en: 'Table [IX](#A0.T9 "TABLE IX ‣ Deep Learning on Graphs: A Survey") shows a collection
    and summary of the source code we collected for the papers discussed in this manuscript.
    In addition to method names and links, the table also lists the programming language
    used and the frameworks adopted as well as whether the code was published by the
    original authors of the paper.'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [IX](#A0.T9 "TABLE IX ‣ 图上的深度学习：一项综述") 显示了我们为本文讨论的论文收集的源代码的集合和总结。除了方法名称和链接外，该表还列出了使用的编程语言和采用的框架，以及代码是否由论文的原作者发布。
- en: Appendix B Applicability for Common Tasks
  id: totrans-728
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 常见任务的适用性
- en: 'Table [X](#A0.T10 "TABLE X ‣ Deep Learning on Graphs: A Survey") summarizes
    the applicability of different models for six common graph tasks, including node
    clustering, node classification, network reconstruction, link prediction, graph
    classification, and graph generation. Note that these results are based on whether
    the experiments were reported in the original papers.'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [X](#A0.T10 "TABLE X ‣ 图上的深度学习：一项综述") 总结了不同模型在六个常见图任务中的适用性，包括节点聚类、节点分类、网络重建、链接预测、图分类和图生成。请注意，这些结果基于原始论文中是否报告了实验。
- en: Appendix C Node Classification Results on Benchmark Datasets
  id: totrans-730
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 基准数据集上的节点分类结果
- en: 'As shown in Appendix [B](#A2 "Appendix B Applicability for Common Tasks ‣ Deep
    Learning on Graphs: A Survey"), node classification is the most common task for
    graph-based deep learning models. Here, we report the results of different methods
    on five node classification benchmark datasets^(11)^(11)11These five benchmark
    datasets are publicly available at https://github.com/tkipf/gcn or http://snap.stanford.edu/graphsage/.:'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 如附录 [B](#A2 "附录 B 常见任务的适用性 ‣ 图上的深度学习：一项综述") 中所示，节点分类是基于图的深度学习模型中最常见的任务。这里，我们报告了不同方法在五个节点分类基准数据集上的结果^(11)^(11)11这五个基准数据集可以在
    https://github.com/tkipf/gcn 或 http://snap.stanford.edu/graphsage/ 上公开获得。：
- en: •
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cora, Citeseer, PubMed [[169](#bib.bib169)]: These are citation graphs with
    nodes representing papers, edges representing citations between papers, and papers
    associated with bag-of-words features and ground-truth topics as labels.'
  id: totrans-733
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Cora、Citeseer、PubMed [[169](#bib.bib169)]：这些是引文图，其中节点代表论文，边表示论文之间的引用，论文与词袋特征和真实主题作为标签相关联。
- en: •
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reddit [[53](#bib.bib53)]: Reddit is an online discussion forum in which nodes
    represent posts and two nodes are connected when they are commented by the same
    user, and each post contains a low-dimensional word vector as features and a label
    indicating the Reddit community in which it was posted.'
  id: totrans-735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Reddit [[53](#bib.bib53)]：Reddit 是一个在线讨论论坛，其中节点代表帖子，当两个节点由同一用户评论时，它们就会连接，每个帖子包含一个低维的词向量作为特征和一个标签，表示该帖子发布的
    Reddit 社区。
- en: •
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PPI [[53](#bib.bib53)]: PPI is a collection of protein-protein interaction
    graphs for different human tissues. It includes features that represent biological
    signatures and labels that represent the roles of proteins.'
  id: totrans-737
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PPI [[53](#bib.bib53)]：PPI 是一个包含不同人类组织的蛋白质相互作用图的集合。它包括表示生物学特征的特征和表示蛋白质角色的标签。
- en: Cora, Citeseer, and Pubmed each include one graph, and the same graph structure
    is used for both training and testing, thus the tasks are considered transductive.
    In Reddit and PPI, because the training and testing graphs are different, these
    two datasets are considered to be inductive node classification benchmarks.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: Cora、Citeseer 和 Pubmed 各包含一个图，并且训练和测试使用相同的图结构，因此这些任务被认为是传导性的。在 Reddit 和 PPI
    中，由于训练和测试图不同，这两个数据集被认为是外推性节点分类基准。
- en: 'In Table [XI](#A3.T11 "TABLE XI ‣ Appendix C Node Classification Results on
    Benchmark Datasets ‣ Deep Learning on Graphs: A Survey"), we report the results
    of different models on these benchmark datasets. The results were extracted from
    their original papers when a fixed dataset split was adopted. The table shows
    that many state-of-the-art methods achieve roughly comparable performance on these
    benchmarks, with differences smaller than one percent. Shchur et al. [[167](#bib.bib167)]
    also found that a fixed dataset split can easily result in spurious comparisons.
    As a result, although these benchmarks have been widely adopted to compare different
    models, more comprehensive evaluation setups are critically needed.'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [XI](#A3.T11 "表 XI ‣ 附录 C 基准数据集上的节点分类结果 ‣ 图上的深度学习：综述")中，我们报告了这些基准数据集上不同模型的结果。当采用固定数据集拆分时，这些结果从原始论文中提取。表中显示，许多最先进的方法在这些基准测试中的表现大致相当，差异小于百分之一。Shchur 等人 [[167](#bib.bib167)]
    还发现固定数据集拆分可能导致虚假的比较。因此，尽管这些基准被广泛用于比较不同模型，但仍然迫切需要更全面的评估设置。
- en: 'TABLE XI: Statistics of the benchmark datasets and the node classification
    results of different methods when a fixed dataset split is adopted. A hyphen (’-’)
    indicates that the result is unavailable in the paper.'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 表 XI：基准数据集的统计数据以及采用固定数据集拆分时不同方法的节点分类结果。破折号（’-’）表示论文中没有提供结果。
- en: '|  | Cora | Citeseer | Pubmed | Reddit | PPI |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '|  | Cora | Citeseer | Pubmed | Reddit | PPI |'
- en: '| Type | Citation | Citation | Citation | Social | Biology |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 引用 | 引用 | 引用 | 社会 | 生物 |'
- en: '| Nodes | 2,708 | 3,327 | 19,717 | 232,965 | 56,944 (24 graphs) |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '| 节点数 | 2,708 | 3,327 | 19,717 | 232,965 | 56,944（24个图） |'
- en: '| Edges | 5,429 | 4,732 | 44,338 | 11,606,919 | 818,716 |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
  zh: '| 边数 | 5,429 | 4,732 | 44,338 | 11,606,919 | 818,716 |'
- en: '| Classes | 7 | 6 | 3 | 41 | 121 |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '| 类别数 | 7 | 6 | 3 | 41 | 121 |'
- en: '| Features | 1,433 | 3,703 | 500 | 602 | 50 |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 1,433 | 3,703 | 500 | 602 | 50 |'
- en: '| Task | Transductive | Transductive | Transductive | Inductive | Inductive
    |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 传导性 | 传导性 | 传导性 | 外推性 | 外推性 |'
- en: '| Bruna et al. [[40](#bib.bib40)]^(12)^(12)footnotemark: 12 | 73.3 | 58.9 |
    73.9 | - | - |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '| Bruna et al. [[40](#bib.bib40)]^(12)^(12)脚注标记：12 | 73.3 | 58.9 | 73.9 | -
    | - |'
- en: '| ChebNet [[42](#bib.bib42)]^(13)^(13)13The results were reported in GWNN [[45](#bib.bib45)].
    | 81.2 | 69.8 | 74.4 | - | - |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '| ChebNet [[42](#bib.bib42)]^(13)^(13)13结果报告于GWNN [[45](#bib.bib45)]。 | 81.2
    | 69.8 | 74.4 | - | - |'
- en: '| GCN [[43](#bib.bib43)] | 81.5 | 70.3 | 79.0 | - | - |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '| GCN [[43](#bib.bib43)] | 81.5 | 70.3 | 79.0 | - | - |'
- en: '| CayleyNets [[44](#bib.bib44)] | 81.9$\pm$0.7 | - | - | - | - |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
  zh: '| CayleyNets [[44](#bib.bib44)] | 81.9$\pm$0.7 | - | - | - | - |'
- en: '| GWNN [[45](#bib.bib45)] | 82.8 | 71.7 | 79.1 | - | - |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
  zh: '| GWNN [[45](#bib.bib45)] | 82.8 | 71.7 | 79.1 | - | - |'
- en: '| LGCN [[48](#bib.bib48)] | 83.3$\pm$0.5 | 73.0$\pm$0.6 | 79.5$\pm$0.2 | -
    | 77.2$\pm$0.2 |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '| LGCN [[48](#bib.bib48)] | 83.3$\pm$0.5 | 73.0$\pm$0.6 | 79.5$\pm$0.2 | -
    | 77.2$\pm$0.2 |'
- en: '| DGCN [[51](#bib.bib51)] | 83.5 | 72.6 | 80.0 | - | - |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '| DGCN [[51](#bib.bib51)] | 83.5 | 72.6 | 80.0 | - | - |'
- en: '| GraphSAGE [[53](#bib.bib53)] | - | - | - | 95.4 | 61.2 |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '| GraphSAGE [[53](#bib.bib53)] | - | - | - | 95.4 | 61.2 |'
- en: '| MoNet [[54](#bib.bib54)] | 81.7$\pm$0.5 | - | 78.8$\pm$0.4 | - | - |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '| MoNet [[54](#bib.bib54)] | 81.7$\pm$0.5 | - | 78.8$\pm$0.4 | - | - |'
- en: '| GAT [[57](#bib.bib57)] | 83.0$\pm$0.7 | 72.5$\pm$0.7 | 79.0$\pm$0.3 | - |
    97.3$\pm$0.2 |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '| GAT [[57](#bib.bib57)] | 83.0$\pm$0.7 | 72.5$\pm$0.7 | 79.0$\pm$0.3 | - |
    97.3$\pm$0.2 |'
- en: '| GaAN [[58](#bib.bib58)] | - | - | - | 96.4$\pm$0.0 | 98.7$\pm$0.0 |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '| GaAN [[58](#bib.bib58)] | - | - | - | 96.4$\pm$0.0 | 98.7$\pm$0.0 |'
- en: '| JK-Nets [[62](#bib.bib62)] | - | - | - | 96.5 | 97.6$\pm$0.7 |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
  zh: '| JK-Nets [[62](#bib.bib62)] | - | - | - | 96.5 | 97.6$\pm$0.7 |'
- en: '| StochasticGCN [[67](#bib.bib67)] | 82.0$\pm$0.8 | 70.9$\pm$0.2 | 79.0$\pm$0.4
    | 96.3$\pm$0.0 | 97.9$\pm$0.0 |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
  zh: '| StochasticGCN [[67](#bib.bib67)] | 82.0$\pm$0.8 | 70.9$\pm$0.2 | 79.0$\pm$0.4
    | 96.3$\pm$0.0 | 97.9$\pm$0.0 |'
- en: '| FastGCN [[68](#bib.bib68)] | 72.3 | - | 72.1 | 93.7 | - |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
  zh: '| FastGCN [[68](#bib.bib68)] | 72.3 | - | 72.1 | 93.7 | - |'
- en: '| Adapt [[69](#bib.bib69)] | - | - | - | 96.3$\pm$0.3 | - |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
  zh: '| Adapt [[69](#bib.bib69)] | - | - | - | 96.3$\pm$0.3 | - |'
- en: '| SGC [[71](#bib.bib71)] | 81.0$\pm$0.0 | 71.9$\pm$0.1 | 78.9$\pm$0.0 | 94.9
    | - |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
  zh: '| SGC [[71](#bib.bib71)] | 81.0$\pm$0.0 | 71.9$\pm$0.1 | 78.9$\pm$0.0 | 94.9
    | - |'
- en: '| DGI [[74](#bib.bib74)] | 82.3$\pm$0.6 | 71.8$\pm$0.7 | 76.8$\pm$0.6 | 94.0$\pm$0.1
    | 63.8$\pm$0.2 |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
  zh: '| DGI [[74](#bib.bib74)] | 82.3$\pm$0.6 | 71.8$\pm$0.7 | 76.8$\pm$0.6 | 94.0$\pm$0.1
    | 63.8$\pm$0.2 |'
- en: '| SSE [[25](#bib.bib25)] | - | - | - | - | 83.6 |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
  zh: '| SSE [[25](#bib.bib25)] | - | - | - | - | 83.6 |'
- en: '| GraphSGAN [[126](#bib.bib126)] | 83.0$\pm$1.3 | 73.1$\pm$1.8 | - | - | -
    |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '| GraphSGAN [[126](#bib.bib126)] | 83.0$\pm$1.3 | 73.1$\pm$1.8 | - | - | -
    |'
- en: '| RGCN [[162](#bib.bib162)] | 82.8$\pm$0.6 | 71.2$\pm$0.5 | 79.1$\pm$0.3 |
    - | - |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '| RGCN [[162](#bib.bib162)] | 82.8$\pm$0.6 | 71.2$\pm$0.5 | 79.1$\pm$0.3 |
    - | - |'
- en: '![Refer to caption](img/00113f5c975bd00b305a3b9a4a5d1dbf.png)'
  id: totrans-768
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/00113f5c975bd00b305a3b9a4a5d1dbf.png)'
- en: 'Figure 11: An example of graph signals and its spectral representations transformed
    using the eigenvectors of the graph Laplacian matrix. The upward-pointing red
    lines represent positive values and the downward-pointing green lines represent
    negative values. These images were adapted from [[6](#bib.bib6)].'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：一个图信号及其通过图拉普拉斯矩阵特征向量变换后的谱表示的示例。向上的红线表示正值，而向下的绿线表示负值。这些图像改编自[[6](#bib.bib6)]。
- en: Appendix D An Example of Graph Signals
  id: totrans-770
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 图信号示例
- en: To help understanding GCNs, we provide an example of graph signals and refer
    readers to [[6](#bib.bib6), [7](#bib.bib7), [170](#bib.bib170)] for more comprehensive
    surveys.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 为帮助理解GCNs，我们提供了一个图信号的示例，并参考[[6](#bib.bib6), [7](#bib.bib7), [170](#bib.bib170)]以获取更全面的综述。
- en: 'Given a graph $G=(V,E)$, a graph signal $\mathbf{f}$ corresponds to a collection
    of numbers: one number for each node in the graph. For undirected graphs, we usually
    assume that the signal takes real values, i.e., $\mathbf{f}\in\mathbb{R}^{N}$,
    where $N$ is the number of nodes. Any node feature satisfying the above requirement
    can be regarded as a graph signal, with an example shown in Fig [11](#A3.F11 "Figure
    11 ‣ Appendix C Node Classification Results on Benchmark Datasets ‣ Deep Learning
    on Graphs: A Survey") (A). Both the signal values and the underlying graph structure
    are important in processing and analyzing graph signals. For example, we can transform
    a graph signal into the spectral domain using the eigenvectors of the graph Laplacian
    matrix:'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个图 $G=(V,E)$，图信号 $\mathbf{f}$ 对应于一组数字：每个图中的节点一个数字。对于无向图，我们通常假设信号取实值，即 $\mathbf{f}\in\mathbb{R}^{N}$，其中
    $N$ 是节点数量。任何满足上述要求的节点特征都可以视为图信号，图 [11](#A3.F11 "图11 ‣ 附录C 基准数据集上的节点分类结果 ‣ 图上的深度学习：综述") (A)
    中展示了一个示例。信号值和底层图结构在处理和分析图信号时都很重要。例如，我们可以通过图拉普拉斯矩阵的特征向量将图信号转换到谱域：
- en: '|  | $\hat{\mathbf{f}}=\mathbf{Q}^{T}\mathbf{f}$ |  | (64) |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{f}}=\mathbf{Q}^{T}\mathbf{f}$ |  | (64) |'
- en: or equivalently
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 或等效地
- en: '|  | $\hat{\mathbf{f}}_{i}=\mathbf{Q}^{T}(i,:)\mathbf{f}.$ |  | (65) |'
  id: totrans-775
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{f}}_{i}=\mathbf{Q}^{T}(i,:)\mathbf{f}.$ |  | (65) |'
- en: 'Because the eigenvectors $\mathbf{Q}^{T}$ are sorted in ascending order based
    on their corresponding eigenvalues, it has been shown [[6](#bib.bib6)] that they
    form a basis for graph signals based on different “smoothness”. Specifically,
    eigenvectors corresponding to small eigenvalues represent smooth signals and low
    frequencies, while eigenvectors corresponding to large eigenvalues represent non-smooth
    signals and high frequencies, as shown in Fig [11](#A3.F11 "Figure 11 ‣ Appendix
    C Node Classification Results on Benchmark Datasets ‣ Deep Learning on Graphs:
    A Survey") (B). Note that the smoothness is measured with respect to the graph
    structure, i.e., whether the signals oscillate across edges in the graph. As a
    result, $\hat{\mathbf{f}}$ provides a spectral representation of the signal $\mathbf{f}$
    as shown in Fig [11](#A3.F11 "Figure 11 ‣ Appendix C Node Classification Results
    on Benchmark Datasets ‣ Deep Learning on Graphs: A Survey") (C). This is similar
    to the Fourier transform in Euclidean spaces. Using $\hat{\mathbf{f}}$, we can
    design various signal processing operations. For example, if we apply a low-pass
    filter, the resulted signal will be more smooth, as shown in Fig [11](#A3.F11
    "Figure 11 ‣ Appendix C Node Classification Results on Benchmark Datasets ‣ Deep
    Learning on Graphs: A Survey") (D) (in this example, we set the frequency threshold
    as 2, i.e., only keeping the lowest 4 frequencies).'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: '由于特征向量$\mathbf{Q}^{T}$是根据对应的特征值按升序排序的，已证明 [[6](#bib.bib6)]，它们构成了基于不同“平滑度”的图信号的基。具体而言，对应于小特征值的特征向量表示平滑信号和低频率，而对应于大特征值的特征向量表示非平滑信号和高频率，如图 [11](#A3.F11
    "Figure 11 ‣ Appendix C Node Classification Results on Benchmark Datasets ‣ Deep
    Learning on Graphs: A Survey") (B)所示。请注意，平滑度是相对于图结构来测量的，即信号是否在图中的边缘上振荡。因此，$\hat{\mathbf{f}}$提供了信号$\mathbf{f}$的谱表示，如图 [11](#A3.F11
    "Figure 11 ‣ Appendix C Node Classification Results on Benchmark Datasets ‣ Deep
    Learning on Graphs: A Survey") (C)所示。这类似于欧几里得空间中的傅里叶变换。使用$\hat{\mathbf{f}}$，我们可以设计各种信号处理操作。例如，如果应用低通滤波器，结果信号将更平滑，如图 [11](#A3.F11
    "Figure 11 ‣ Appendix C Node Classification Results on Benchmark Datasets ‣ Deep
    Learning on Graphs: A Survey") (D)所示（在此示例中，我们将频率阈值设置为2，即仅保留最低的4个频率）。'
- en: Appendix E Time Complexity
  id: totrans-777
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 时间复杂度
- en: '^(13)^(13)footnotetext: The results were reported in Kipf and Welling [[102](#bib.bib102)].'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: '^(13)^(13)脚注: 结果在Kipf和Welling [[102](#bib.bib102)]中报告。'
- en: In this section, we explain how we obtained the time complexity in all the tables.
    Specifically, we mainly focus on the time complexity with respect to the graph
    size, e.g., the number of nodes $N$ and the number of edges $M$, and omit other
    factors, e.g., the number of hidden dimensions $f_{l}$ or the number of iterations,
    since the latter terms are usually set as small constants and are less dominant.
    Note that we focus on the theoretical results, while the exact efficiency of one
    algorithm also depends heavily on its implementations and techniques to reduce
    the constants in the time complexity.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们解释了如何获得所有表格中的时间复杂度。具体而言，我们主要关注图的规模的时间复杂度，例如节点数$N$和边数$M$，而忽略其他因素，例如隐藏维度数$f_{l}$或迭代次数，因为后者通常设为小常数且不具主导性。请注意，我们关注的是理论结果，而一个算法的实际效率还很大程度上依赖于其实现和减少时间复杂度常数的技术。
- en: •
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GNN [[23](#bib.bib23)]: $O(MI_{f})$, where $I_{f}$ is the number of iterations
    for Eq. ([1](#S3.E1 "In 3.1 Node-level RNNs ‣ 3 Graph Recurrent Neural Networks
    ‣ Deep Learning on Graphs: A Survey")) to reach stable points, as shown in the
    paper.'
  id: totrans-781
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GNN [[23](#bib.bib23)]: $O(MI_{f})$，其中$I_{f}$是Eq. ([1](#S3.E1 "In 3.1 Node-level
    RNNs ‣ 3 Graph Recurrent Neural Networks ‣ Deep Learning on Graphs: A Survey"))达到稳定点所需的迭代次数，如论文所示。'
- en: •
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GGS-NNs [[24](#bib.bib24)]: $O(MT)$, where $T$ is a preset maximum pseudo time
    since the method utilizes all the edges in each updating.'
  id: totrans-783
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GGS-NNs [[24](#bib.bib24)]: $O(MT)$，其中$T$是预设的最大伪时间，因为该方法在每次更新中利用了所有边。'
- en: •
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SSE [[25](#bib.bib25)]: $O(d_{\text{avg}}S)$, where $d_{\text{avg}}$ is the
    average degree and $S$ is the total number of samples, as shown in the paper.'
  id: totrans-785
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SSE [[25](#bib.bib25)]: $O(d_{\text{avg}}S)$，其中$d_{\text{avg}}$是平均度，$S$是样本总数，如论文所示。'
- en: •
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'You et al. [[26](#bib.bib26)]: $O(N^{2})$, as shown in the paper.'
  id: totrans-787
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'You et al. [[26](#bib.bib26)]: $O(N^{2})$，如论文所示。'
- en: •
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DGNN [[27](#bib.bib27)]: $O(Md_{\text{avg}})$, where $d_{\text{avg}}$ is the
    average degree since the effect of the one-step propagation of each edge is considered.'
  id: totrans-789
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DGNN [[27](#bib.bib27)]: $O(Md_{\text{avg}})$，其中$d_{\text{avg}}$是平均度，因为考虑了每条边的一步传播的效果。'
- en: •
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RMGCNN [[28](#bib.bib28)]: $O(MN)$ or $O(M)$, depending on whether an approximation
    technique is adopted, as shown in the paper.'
  id: totrans-791
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic GCN [[29](#bib.bib29)]: $O(Mt)$, where $t$ denotes the number of time
    slices since the model runs one GCN at each time slice.'
  id: totrans-793
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bruna et al. [[40](#bib.bib40)] and Henaff et al. [[41](#bib.bib41)]: $O(N^{3})$,
    due to the time complexity of the eigendecomposition.'
  id: totrans-795
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ChebNet [[42](#bib.bib42)], Kipf and Welling [[43](#bib.bib43)], CayletNet [[44](#bib.bib44)],
    GWNN [[45](#bib.bib45)], and Neural FPs [[46](#bib.bib46)]: $O(M)$, as shown in
    the corresponding papers.'
  id: totrans-797
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PATCHY-SAN [[47](#bib.bib47)]: $O(M\log N)$, assuming the method adopts WL
    to label nodes, as shown in the paper.'
  id: totrans-799
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LGCN [[48](#bib.bib48)]: $O(M)$ since all the neighbors of each node are sorted
    in the method.'
  id: totrans-801
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SortPooling [[49](#bib.bib49)]: $O(M)$, due to the time complexity of adopted
    graph convolution layers.'
  id: totrans-803
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DCNN [[50](#bib.bib50)]: $O(N^{2})$, as reported in [[43](#bib.bib43)].'
  id: totrans-805
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DGCN [[51](#bib.bib51)]: $O(N^{2})$ since the PPMI matrix is not sparse.'
  id: totrans-807
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MPNNs [[52](#bib.bib52)]: $O(M)$, as shown in the paper.'
  id: totrans-809
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GraphSAGE [[53](#bib.bib53)]: $O(Ns^{L})$, where $s$ is the size of the sampled
    neighborhoods and $L$ is the number of layers, as shown in the paper.'
  id: totrans-811
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MoNet [[54](#bib.bib54)]: $O(M)$ since only the existing node pairs are involved
    in the calculation.'
  id: totrans-813
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GNs [[9](#bib.bib9)]: $O(M)$ since only the existing node pairs are involved
    in the calculation.'
  id: totrans-815
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kearnes et al. [[55](#bib.bib55)]: $O(M)$, since only the existing node pairs
    are used in the calculation.'
  id: totrans-817
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DiffPool [[56](#bib.bib56)]: $O(N^{2})$ since the coarsened graph is not sparse.'
  id: totrans-819
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GAT [[57](#bib.bib57)]: $O(M)$, as shown in the paper.'
  id: totrans-821
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GaAN [[58](#bib.bib58)]: $O(Ns^{L})$, where $s$ is a preset maximum neighborhood
    length and $L$ is the number of layers, as shown in the paper.'
  id: totrans-823
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAN [[59](#bib.bib59)]: $O(M_{\phi})$, the number of meta-path-based node pairs,
    as shown in the paper.'
  id: totrans-825
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CLN [[60](#bib.bib60)]: $O(M)$ since only the existing node pairs are involved
    in the calculation.'
  id: totrans-827
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PPNP [[61](#bib.bib61)]: $O(M)$, as shown in the paper.'
  id: totrans-829
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JK-Nets [[62](#bib.bib62)]: $O(M)$, due to the time complexity in adopted graph
    convolutional layers.'
  id: totrans-831
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ECC [[63](#bib.bib63)]: $O(M)$, as shown in the paper.'
  id: totrans-833
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'R-GCNs [[64](#bib.bib64)]: $O(M)$ since the edges of different types sum up
    to the total number of edges of the graph.'
  id: totrans-835
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LGNN [[65](#bib.bib65)]: $O(M)$, as shown in the paper.'
  id: totrans-837
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PinSage [[66](#bib.bib66)]: $O(Ns^{L})$, where $s$ is the size of the sampled
    neighborhoods and $L$ is the number of layers since a sampling strategy similar
    to that of GraphSAGE [[53](#bib.bib53)] is adopted.'
  id: totrans-839
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'StochasticGCN [[67](#bib.bib67)]: $O(Ns^{L})$, as shown in the paper.'
  id: totrans-841
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FastGCN [[68](#bib.bib68)] and Adapt [[69](#bib.bib69)]: $O(NsL)$ since the
    samples are drawn in each layer instead of in the neighborhoods, as shown in the
    paper.'
  id: totrans-843
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [[70](#bib.bib70)]: $O(M)$, due to the time complexity in adopted
    graph convolutional layers.'
  id: totrans-845
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Li et al. [[70](#bib.bib70)]: $O(M)$，由于采用的图卷积层中的时间复杂度。'
- en: •
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SGC [[71](#bib.bib71)]: $O(M)$ since the calculation is the same as Kipf and
    Welling [[43](#bib.bib43)] by not adopting nonlinear activations.'
  id: totrans-847
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SGC [[71](#bib.bib71)]: $O(M)$，因为计算与Kipf和Welling [[43](#bib.bib43)]相同，不采用非线性激活函数。'
- en: •
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GFNN [[72](#bib.bib72)]: $O(M)$ since the calculation is the same as SGC [[71](#bib.bib71)]
    by adding an extra MLP layer.'
  id: totrans-849
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GFNN [[72](#bib.bib72)]: $O(M)$，因为计算与SGC [[71](#bib.bib71)]相同，仅增加了一个额外的MLP层。'
- en: •
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GIN [[73](#bib.bib73)]: $O(M)$, due to the time complexity in adopted graph
    convolutional layers.'
  id: totrans-851
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GIN [[73](#bib.bib73)]: $O(M)$，由于采用的图卷积层中的时间复杂度。'
- en: •
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DGI [[74](#bib.bib74)]: $O(M)$, due to the time complexity in adopted graph
    convolutional layers.'
  id: totrans-853
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DGI [[74](#bib.bib74)]: $O(M)$，由于采用的图卷积层中的时间复杂度。'
- en: •
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SAE [[96](#bib.bib96)] and SDNE [[97](#bib.bib97)]: $O(M)$, as shown in the
    corresponding papers.'
  id: totrans-855
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SAE [[96](#bib.bib96)]和SDNE [[97](#bib.bib97)]: $O(M)$，如相应论文中所示。'
- en: •
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DNGR [[98](#bib.bib98)]: $O(N^{2})$, due to the time complexity of calculating
    the PPMI matrix.'
  id: totrans-857
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DNGR [[98](#bib.bib98)]: $O(N^{2})$，由于计算PPMI矩阵的时间复杂度。'
- en: •
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GC-MC [[99](#bib.bib99)]: $O(M)$ since the encoder adopts the GCN proposed
    by Kipf and Welling [[43](#bib.bib43)] and only the non-zero elements of the graph
    are considered in the decoder.'
  id: totrans-859
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GC-MC [[99](#bib.bib99)]: $O(M)$，因为编码器采用了Kipf和Welling [[43](#bib.bib43)]提出的GCN，且解码器中仅考虑图的非零元素。'
- en: •
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DRNE [[100](#bib.bib100)]: $O(Ns)$, where $s$ is a preset maximum neighborhood
    length, as shown in the paper.'
  id: totrans-861
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DRNE [[100](#bib.bib100)]: $O(Ns)$，其中$s$是预设的最大邻域长度，如论文中所示。'
- en: •
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'G2G [[101](#bib.bib101)]: $O(M)$, due to the definition of the ranking loss.'
  id: totrans-863
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'G2G [[101](#bib.bib101)]: $O(M)$，由于排名损失的定义。'
- en: •
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VGAE [[102](#bib.bib102)]: $O(N^{2})$, due to the reconstruction of all the
    node pairs.'
  id: totrans-865
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'VGAE [[102](#bib.bib102)]: $O(N^{2})$，由于需要重建所有节点对。'
- en: •
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DVNE [[103](#bib.bib103)]: Though the original paper reported to have a time
    complexity of $O(Md_{\text{avg}})$ where $d_{\text{avg}}$ is the average degree,
    we have confirmed that it can be easily improved to $O(M)$ through personal communications
    with the authors.'
  id: totrans-867
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DVNE [[103](#bib.bib103)]: 尽管原论文报告的时间复杂度为$O(Md_{\text{avg}})$（其中$d_{\text{avg}}$是平均度），但我们通过与作者的个人交流确认，可以轻松改进为$O(M)$。'
- en: •
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ARGA/ARVGA [[104](#bib.bib104)]: $O(N^{2})$, due to the reconstruction of all
    the node pairs.'
  id: totrans-869
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ARGA/ARVGA [[104](#bib.bib104)]: $O(N^{2})$，由于需要重建所有节点对。'
- en: •
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'NetRA [[105](#bib.bib105)]: $O(M)$, as shown in the paper.'
  id: totrans-871
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'NetRA [[105](#bib.bib105)]: $O(M)$，如论文中所示。'
- en: •
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GCPN [[116](#bib.bib116)]: $O(MN)$ since the embedding of all the nodes are
    used when generating each edge.'
  id: totrans-873
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GCPN [[116](#bib.bib116)]: $O(MN)$，因为在生成每个边时使用了所有节点的嵌入。'
- en: •
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MolGAN [[117](#bib.bib117)] and GTPN [[118](#bib.bib118)]: $O(N^{2})$ since
    the scores for all the node pairs have to be calculated.'
  id: totrans-875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MolGAN [[117](#bib.bib117)]和GTPN [[118](#bib.bib118)]: $O(N^{2})$，因为必须计算所有节点对的得分。'
- en: •
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GAM [[119](#bib.bib119)]: $O(d_{\text{avg}}sT)$, where $d_{\text{avg}}$ is
    the average degree, $s$ is the number of sampled random walks, and $T$ is the
    walk length, as shown in the paper.'
  id: totrans-877
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GAM [[119](#bib.bib119)]: $O(d_{\text{avg}}sT)$，其中$d_{\text{avg}}$是平均度，$s$是采样随机游走的数量，$T$是游走长度，如论文中所示。'
- en: •
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DeepPath [[120](#bib.bib120)]: $O(d_{\text{avg}}sT+s^{2}T)$, where $d_{\text{avg}}$
    is the average degree, $s$ is the number of sampled paths, and $T$ is the path
    length. The former term corresponds to finding paths and the latter term results
    from the diversity constraint.'
  id: totrans-879
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DeepPath [[120](#bib.bib120)]: $O(d_{\text{avg}}sT+s^{2}T)$，其中$d_{\text{avg}}$是平均度，$s$是采样路径的数量，$T$是路径长度。前一个项对应路径查找，后一个项来自多样性约束。'
- en: •
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MINERVA [[121](#bib.bib121)]: $O(d_{\text{avg}}sT)$, where $d_{\text{avg}}$
    is the average degree, $s$ is the number of sampled paths, and $T$ is the path
    length, similar to the pathfinding method in DeepPath [[120](#bib.bib120)].'
  id: totrans-881
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MINERVA [[121](#bib.bib121)]: $O(d_{\text{avg}}sT)$，其中$d_{\text{avg}}$是平均度，$s$是采样路径的数量，$T$是路径长度，类似于DeepPath [[120](#bib.bib120)]中的路径查找方法。'
- en: •
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GraphGAN [[124](#bib.bib124)]: $O(MN)$, as shown in the paper.'
  id: totrans-883
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GraphGAN [[124](#bib.bib124)]: $O(MN)$，如论文中所示。'
- en: •
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ANE [[125](#bib.bib125)]: $O(N)$, which is the extra time complexity introduced
    by the model in the generator and the discriminator.'
  id: totrans-885
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ANE [[125](#bib.bib125)]: $O(N)$，这是生成器和判别器中模型引入的额外时间复杂度。'
- en: •
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GraphSGAN [[126](#bib.bib126)]: $O(N^{2})$, due to the time complexity in the
    objective function.'
  id: totrans-887
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GraphSGAN [[126](#bib.bib126)]: $O(N^{2})$，由于目标函数中的时间复杂度。'
- en: •
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'NetGAN [[127](#bib.bib127)]: $O(M)$, as shown in the paper.'
  id: totrans-889
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'NetGAN [[127](#bib.bib127)]: $O(M)$，如论文中所示。'
- en: •
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Nettack [[128](#bib.bib128)]: $O(Nd_{0}^{2})$, where $d_{0}$ is the degree
    of the targeted node, as shown in the paper.'
  id: totrans-891
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Nettack [[128](#bib.bib128)]：$O(Nd_{0}^{2})$，其中$d_{0}$是目标节点的度数，如论文中所示。
- en: •
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dai et al. [[129](#bib.bib129)]: $O(M)$, which is the time complexity of the
    most effective strategy RL-S2V, as shown in the paper.'
  id: totrans-893
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Dai 等 [[129](#bib.bib129)]：$O(M)$，这是最有效策略 RL-S2V 的时间复杂度，如论文中所示。
- en: •
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Zugner and Gunnemann [[130](#bib.bib130)]: $O(N^{2})$, as shown in the paper.'
  id: totrans-895
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Zugner 和 Gunnemann [[130](#bib.bib130)]：$O(N^{2})$，如论文中所示。
