- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:45:08'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2207.14191] Learning with Limited Annotations: A Survey on Deep Semi-supervised
    Learning for Medical Image Segmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2207.14191](https://ar5iv.labs.arxiv.org/html/2207.14191)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \cormark
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: \cormark
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: \cormark
  prefs: []
  type: TYPE_NORMAL
- en: '[2]'
  prefs: []
  type: TYPE_NORMAL
- en: \cormark
  prefs: []
  type: TYPE_NORMAL
- en: '[2]'
  prefs: []
  type: TYPE_NORMAL
- en: \cortext
  prefs: []
  type: TYPE_NORMAL
- en: '[1]Contribute equally to this work. \cortext[2]Corresponding author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning with Limited Annotations: A Survey on Deep Semi-supervised Learning
    for Medical Image Segmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rushi Jiao    Yichi Zhang    Le Ding    Bingsen Xue    Jicong Zhang    Rong
    Cai    Cheng Jin School of Biomedical Engineering, Shanghai Jiao Tong University,
    Shanghai, 200240, China School of Data Science, Fudan University, Shanghai, 200433,
    China School of Engineering Medicine, Beihang University, Beijing, 100191, China
    Artificial Intelligence Innovation and Incubation Institute, Fudan University,
    Shanghai, 200433, China School of Biological Science and Medical Engineering,
    Beihang University, Beijing, 100191, China Shanghai Artificial Intelligence Laboratory,
    Shanghai, 200232, China Beijing Anding Hospital, Capital Medical University, Beijing,
    100088, China Key Laboratory for Biomechanics and Mechanobiology of Ministry of
    Education, Beihang University, Beijing, 100191, China Hefei Innovation Research
    Institute, Beihang University, Hefei, 230012, China
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Medical image segmentation is a fundamental and critical step in many image-guided
    clinical approaches. Recent success of deep learning-based segmentation methods
    usually relies on a large amount of labeled data, which is particularly difficult
    and costly to obtain, especially in the medical imaging domain where only experts
    can provide reliable and accurate annotations. Semi-supervised learning has emerged
    as an appealing strategy and been widely applied to medical image segmentation
    tasks to train deep models with limited annotations. In this paper, we present
    a comprehensive review of recently proposed semi-supervised learning methods for
    medical image segmentation and summarize both the technical novelties and empirical
    results. Furthermore, we analyze and discuss the limitations and several unsolved
    problems of existing approaches. We hope this review can inspire the research
    community to explore solutions to this challenge and further advance the field
    of medical image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: \sepMedical Image Segmentation, \sepSemi-Supervised Learning,\sepConvolutional
    Neural Network, \sepSurvey.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Medical image segmentation aims to delineate the interested anatomical structures
    like organs and tumors from the original images by labeling each pixel into a
    certain class, which is a basic and important step for many clinical approaches
    like computer-aided diagnosis, treatment planning and radiation therapy [[1](#bib.bib1),
    [4](#bib.bib4)]. Accurate segmentation can provide reliable volumetric and shape
    information so as to assist in further clinical applications like disease diagnosis
    and quantitative analysis [[23](#bib.bib23), [24](#bib.bib24), [6](#bib.bib6)].
    According to the word cloud of paper titles in the 25rd International Conference
    and Medical Image Computing and Computer Assisted Intervention ¹¹1http://miccai2022.org
    (MICCAI 2022) in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Learning with
    Limited Annotations: A Survey on Deep Semi-supervised Learning for Medical Image
    Segmentation"), we can observe that "segmentation" is one of the most active research
    topics and has the highest frequency in medical image analysis community.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5f72365c5983cb0e57e5e0af715d91c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Word cloud of paper titles in MICCAI 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the introduction of U-Net [[38](#bib.bib38), [39](#bib.bib39)] for medical
    image segmentation in 2015, many variants of encoder-decoder architecture have
    been proposed to improve it by re-designing skip connections [[42](#bib.bib42)],
    incorporating residual/dense convolution blocks [[8](#bib.bib8), [41](#bib.bib41)],
    attention mechanisms [[52](#bib.bib52), [211](#bib.bib211), [43](#bib.bib43)],
    etc. Moreover, nnU-Net (no-new-U-Net) [[49](#bib.bib49)] can automatically configure
    the strategies of pre-processing, the network architecture, the training, the
    inference, and the post-processing to a given dataset for medical image segmentation
    based on the encoder-decoder structure of U-Net. Without manual intervention,
    nnU-Net surpasses most existing approaches and achieves the state-of-the-art performance
    in several fully supervised medical image segmentation tasks. Inspired by recent
    success of transformer architectures in the field of natural language processing,
    many transformer-based methods have been proposed and applied for medical image
    segmentation [[11](#bib.bib11), [13](#bib.bib13)]. Although these architectural
    advancements have shown encouraging results and achieved state-of-the-art performances
    in many medical image segmentation tasks [[10](#bib.bib10)], these methods still
    require relatively large amount of high-quality annotated data for training, more
    than ever. However, it is impractical to obtain large-scale carefully-labeled
    datasets to train segmentation models, particularly for medical imaging where
    it is hard and expensive to obtain well-annotated data where only experts can
    provide reliable and accurate annotations [[25](#bib.bib25)]. Besides, many commonly
    used medical images like computed tomography (CT) and magnetic resonance imaging
    (MRI) scans are 3D volumetric data, which further increase the burden of manual
    annotation compared with 2D images where experts need to delineate the object
    from the volume slice by slice [[64](#bib.bib64)].
  prefs: []
  type: TYPE_NORMAL
- en: 'To ease the manual labeling burden in response to these challenges, significant
    efforts have been devoted to annotation-efficient deep learning methods for medical
    image segmentation tasks by enlarging the training data through label generation[[12](#bib.bib12)],
    data augmentation [[7](#bib.bib7)], leveraging external related labeled datasets
    [[26](#bib.bib26)], and leveraging unlabeled data with semi-supervised learning.
    Among these approaches, semi-supervised segmentation is a more practical method
    by encouraging segmentation models to utilize unlabeled data which is much easier
    to acquire in conjunction with limited amount of labeled data for training, which
    has a high impact on real-world clinical applications. According to the statistics
    in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Learning with Limited Annotations:
    A Survey on Deep Semi-supervised Learning for Medical Image Segmentation"), semi-supervised
    medical image segmentation has obtained increasing attention from the medical
    imaging and computer vision community in recent years. However, without expert-examined
    annotations, it is still an open and challenging question on how to efficiently
    exploit useful information from these unlabeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3816d79d0722460cd2c29b1bf2c75db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Statistics of papers retrieved from Web of Science on semi-supervised
    medical image segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: Main contributions. Compared with related surveys[[27](#bib.bib27), [25](#bib.bib25)],
    we mainly focus on deep semi-supervised medical image segmentation. And we provide
    a comprehensive review of recent solutions, summarizing both the technical novelties
    and empirical results. Furthermore, we analyze and discussed the limitations and
    several unsolved problems of existing approaches. We hope this review could inspire
    the research community to explore solutions for this challenge and further promote
    the developments in medical image segmentation field.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb925a00af8748e311d06377245e09db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Example comparison of supervised learning and semi-supervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2effc8ed17f1cd224a91281afbf96b38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The overview of existing deep semi-supervised learning methods for
    medical image segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Basic Formulation of Semi-Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Semi-supervised learning aims to utilize large amount of unlabeled data in conjunction
    with labeled data to train higher-performing segmentation models. To ease the
    description in the following sections, we formulate the semi-supervised learning
    task as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Given a dataset $\mathcal{D}$ for training, we denote the labeled set with $M$
    labeled cases as $\mathcal{D}_{L}=\{x_{i}^{l},y_{i}\}_{i=1}^{M}$, and the unlabeled
    set with $N$ unlabeled cases as $\mathcal{D}_{U}=\{x_{i}^{u}\}_{i=1}^{N}$, where
    $x_{i}^{l}$ and $x_{i}^{u}$ denote the input images and $y_{i}$ denotes the corresponding
    ground truth of labeled data. Generally, $\mathcal{D}_{L}$ is a relative small
    subset of the entire dataset $\mathcal{D}$, which means $M\ll N$. For semi-supervised
    segmentation settings, we aim at building a data-efficient deep learning model
    with the combination of $\mathcal{D}_{L}$ and $\mathcal{D}_{U}$ and making the
    performance to be comparable to an optimal model trained over fully labeled dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on whether test data are wholly available in the training process, semi-supervised
    learning can be classified into two settings: the transductive learning and the
    inductive learning. For transductive learning, it is assumed that the unlabeled
    samples in the training process are exactly the data to be predicted (i.e. the
    test set), and the purpose of the transductive learning is to generalize the model
    over these unlabeled samples. While for inductive learning, the semi-supervised
    model will be applied to new unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Assumptions for Semi-Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For semi-supervised learning, an essential prerequisite is that the data distribution
    should be under some assumptions. Otherwise, it is impossible to generalize from
    a finite training set to an infinite invisible set. The three basic assumptions
    for semi-supervised learning include [[204](#bib.bib204), [205](#bib.bib205)]:'
  prefs: []
  type: TYPE_NORMAL
- en: The Cluster Assumption. When two samples $x_{1}$ and $x_{2}$ are similar or
    belong to the same cluster, their corresponding outputs $y_{1}$ and $y_{2}$ should
    also be similar or belong to the same category, and vice versa. This assumption
    implies that the samples in a single class tend to form a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Low-density Separation. The decision boundary should be positioned in low-density
    regions of the feature space rather than high-density regions. This assumption
    is closely tied to the cluster assumption as it implies that samples belonging
    to the same class tend to be concentrated in the same cluster. Therefore, a large
    amount of unlabeled data can be used to adjust the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: The Manifold Assumption. If two samples $x_{1}$ and $x_{2}$ reside in a local
    neighborhood within a low-dimensional manifold, they are likely to possess similar
    class labels. This assumption reflects the local smoothness of the decision boundary
    and encourages nearby samples in the feature space to have the same predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Related Work on Semi-Supervised Medical Image Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we mainly divide these semi-supervised medical image segmentation
    methods into three strategies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 1) semi-supervised learning with pseudo labels, where unlabeled images are firstly
    predicted and pseudo labeled by a segmentation model and then used as new examples
    for further training.
  prefs: []
  type: TYPE_NORMAL
- en: 2) semi-supervised learning with unsupervised regularization, where unlabeled
    images are used jointly with labeled data to train a segmentation model with unsupervised
    regularization. This section mainly contains consistency learning, co-training,
    adversarial learning, entropy minimization.
  prefs: []
  type: TYPE_NORMAL
- en: 3) semi-supervised learning with knowledge priors, where unlabeled images is
    utilized to enable the model with knowledge priors like the shape and position
    of the targets to improve the representation ability for medical image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The summarized review of semi-supervised medical image segmentation
    methods with pseudo labels.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | 2D/3D | Modality | Dataset | Label generation methods |'
  prefs: []
  type: TYPE_TB
- en: '| PLRS, Thompson et al. [[82](#bib.bib82)] | 3D | MRI | BraTS 2020 [[128](#bib.bib128)]
    | Superpixel maps calculated by simple linear iterative clustering (SLIC) algorithm
    [[83](#bib.bib83)] to refine pseudo labels [online]. |'
  prefs: []
  type: TYPE_TB
- en: '| SSA-Net, Wang et al. [[109](#bib.bib109)] | 2D | CT | COVID-19-CT-Seg dataset
    [[16](#bib.bib16)], COVID-19 CT Segmentation dataset ¹¹10 | Add a trust module
    to re-evaluate the pseudo labels from the model outputs [online]. |'
  prefs: []
  type: TYPE_TB
- en: '| CoraNet, Shi et al. [[87](#bib.bib87)] | 2D/3D | CT, MRI | Pancreas CT [[130](#bib.bib130)],
    MR Endocardium [[167](#bib.bib167)], ACDC [[22](#bib.bib22)] | Conservative-radical
    network to generate more reliable results [online]. |'
  prefs: []
  type: TYPE_TB
- en: '| ECLR, Zhang et al. [[72](#bib.bib72)] | 2D | Microscope | Gland Segmentation
    Challenge dataset [[189](#bib.bib189)], ColoRectal Adenocarcinoma Gland (CRAG)[[168](#bib.bib168)]
    | Add an error prediction network to divide segmentation errors into intra-class
    inconsistency or inter-class similarity problems [online]. |'
  prefs: []
  type: TYPE_TB
- en: '| SECT, Li et al. [[136](#bib.bib136)] | 2D | CT | UESTC-COVID-19 Dataset[[169](#bib.bib169)],
    COVID-19-CT-Seg dataset [[16](#bib.bib16)] | Self-ensembling strategy to build
    the up-to-date predictions via exponential moving average [online]. |'
  prefs: []
  type: TYPE_TB
- en: '| LoL-SSL, Han et al.[[69](#bib.bib69)] | 2D | CT | part of LiTS dataset[[17](#bib.bib17)]
    | Generate class representations from labeled data based on prototype learning
    [label propagation]. |'
  prefs: []
  type: TYPE_TB
- en: '| NM-SSL, Wang et al. [[92](#bib.bib92)] | 2D | X-Ray, Dermoscopic | ISIC Skin
    [[19](#bib.bib19)], Chexpert [[20](#bib.bib20)] | Neighbor matching to generate
    pseudo-labels on a weight basis according to the embedding similarity with neighboring
    labeled data [label propagation]. |'
  prefs: []
  type: TYPE_TB
- en: '| RPG, Seibold et al. [[86](#bib.bib86)] | 2D | X-Ray | JSRT dataset[[15](#bib.bib15)]
    | Generate pseudo labels through transferring semantics [label propagation]. |'
  prefs: []
  type: TYPE_TB
- en: 1\. https://medicalsegmentation.com/covid19/
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Semi-Supervised Medical Image Segmentation with Pseudo Labels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To utilize unlabeled data, a direct and intuitive method is assigning pseudo
    annotations for unlabeled images, and then using the pseudo labeled images in
    conjunction with labeled images to update the segmentation model. Pseudo labeling
    is commonly implemented in an iterative manner therefore the model can improve
    the quality of pseudo annotations iteratively. Algorithm [1](#alg1 "Algorithm
    1 ‣ 3.1 Semi-Supervised Medical Image Segmentation with Pseudo Labels ‣ 3 Related
    Work on Semi-Supervised Medical Image Segmentation ‣ Learning with Limited Annotations:
    A Survey on Deep Semi-supervised Learning for Medical Image Segmentation") presents
    the overall workflow of this strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, an initial segmentation model is trained using limited labeled data.
    The initial segmentation model is then applied to unlabeled data to generate pseudo
    segmentation masks. After that, pseudo-labeled dataset is then merged with labeled
    dataset to update the initial model. The training procedure alternates between
    the two steps introduced above, until a predefined iteration number.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Training procedure of semi-supervised learning with pseudo labels.
  prefs: []
  type: TYPE_NORMAL
- en: 0:   $\{x^{l},y^{l}\}$ from labeled dataset $D_{L}$, $\{x^{u}\}$ from unlabeled
    dataset $D_{U}$, initial segmentation model $\mathcal{M}_{0}$, iteration times
    $\mathcal{T}$0:  Trained segmentation model $\mathcal{M}_{\mathcal{T}}$1:  Training
    initial segmentation model $\mathcal{M}_{0}$ with $D_{L}$2:  for $i\leftarrow
    1$ to $\mathcal{T}$ do3:     Generate pseudo labels $\{\hat{y}^{u}\}$ of unlabeled
    cases $\{x^{u}\}$ with model $\mathcal{M}_{i-1}$4:     Generate new training dataset
    $D_{PLi}$ with the combination of labeled dataset $\{x^{l},y^{l}\}$ and pseudo
    labeled dataset with $\{x^{u},\hat{y}^{u}\}$5:     $\mathcal{M}_{i}$ $\leftarrow$
    Fine-tuning model $\mathcal{M}_{i-1}$ using $D_{PLi}$6:  end for7:  return  Updated
    model $\mathcal{M}_{\mathcal{T}}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Within this strategy for semi-supervised learning, these methods mainly differ
    in the model initialization, generation of pseudo labels, and how the noise in
    pseudo labels is handled. The outputs of an under-trained segmentation model with
    limited labeled data are noisy. If these noisy outputs are used as pseudo labels
    directly, it may make the subsequent training process unstable and hurt the performance
    [[145](#bib.bib145)]. For better leverage of the pseudo labels with potential
    noise, lots of methods have been proposed. In this section, we will explain the
    generation of pseudo-labels from two aspects: online generation followed by removing
    noisy predictions and label propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Online generation Pseudo labels are mostly generated through the predictions
    of a trained model in an online manner followed by some post-processing algorithms
    for refinement. A common method is to choose unlabeled pixels with maximum predicted
    probability greater than the setting threshold. However, the predictions can be
    noisy and unreliable, and may provide incorrect guidance. It is unreasonable to
    set the same threshold fit for all the samples. In [[213](#bib.bib213)], double-threshold
    pseudo labeling is introduced, in which predictions from the classification branch
    and the segmentation branch jointly determine the reliable pseudo labels. Based
    on the work in [[109](#bib.bib109), [145](#bib.bib145)], the pseudo labels with
    higher confidence are usually more effective. Therefore, many confidence- or uncertainty-aware
    methods are proposed to generate more stable and reliable pseudo labels. For example,
    Yao et al. [[68](#bib.bib68)] propose a confidence-aware cross pseudo supervision
    network to improve the pseudo label quality of unlabeled images from unknown distributions.
    Specifically, the input image from source domain is perturbed with the amplitude
    of the target domain through the Fourier transformation to generate the transformed
    image. The pixel-wise KL-divergence of the predictions of the original and transformed
    images is calculated as the variance $V$, which is then used to calculate the
    pixel-wise confidence. Pseudo labels with high confidence are selected for loss
    calculation. This process is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}V=E[P_{F}\log(\frac{P_{F}}{P_{O}})]\end{split}$ |  | (1)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\begin{split}confidence=e^{-V}\end{split}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where, $P_{F}$ and $P_{O}$ represent the predictions of the transformed images
    and original images. Wang et al. [[109](#bib.bib109)] add a trust module to re-evaluate
    the pseudo labels from the model outputs and set a threshold to choose high confidence
    values. Except adding confidence-aware modules, Li et al. [[136](#bib.bib136)]
    propose a self-ensembling strategy to build the up-to-date predictions via exponential
    moving average to avoid noisy and unstable pseudo labels. For post-processing
    algorithms, morphological methods, machine learning methods [[83](#bib.bib83)]
    and additional networks [[87](#bib.bib87), [72](#bib.bib72)] are usually used
    to further refine pseudo labels. For example, superpixel maps calculated by simple
    linear iterative clustering (SLIC) algorithm [[83](#bib.bib83)] are introduced
    to refine pseudo labels in [[82](#bib.bib82)]. This algorithm is suitable for
    segmentation of targets with irregular shapes. Shi et al. [[87](#bib.bib87)] propose
    a conservative-radical network. The object conservative setting tends to predict
    pixels into background while the object radical setting tends to predict pixels
    into foreground. The certain region in predictions of unlabeled data is the overlap
    between conservative and radical settings and employed as pseudo labels. Zhang
    et al. [[72](#bib.bib72)] rectify the segmentation results of unlabeled data through
    another error segmentation network followed by the main segmentation network.
    The segmentation errors are divided into intra-class inconsistency or inter-class
    similarity problems. This method is applicable for different segmentation models
    and tasks. Recently, vision foundation models such as Segment Anything Model (SAM)
    [[202](#bib.bib202)], have shown their amazing capabilities and generalization
    abilities. [[209](#bib.bib209)] hypothesized that reliable pseudo-labels usually
    make SAM [[202](#bib.bib202)] conduct predictions consistent with the SSL models.
    So predictions of the SSL models are used as prompts to the SAM [[202](#bib.bib202)]
    to select reliable pseudo-labels. Then the SSL models are retrained with the reliable
    sets. This method shows a superior performance compared with existing SSL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Label propagation Pseudo labels can be generated indirectly through label propagation
    e.g. prototype learning[[69](#bib.bib69)] and nearest-neighbor matching[[92](#bib.bib92),
    [86](#bib.bib86)]. However, these indirect generation ways are time-consuming
    and demand higher memory consumption, mostly in an offline manner. For example,
    Han et al. [[69](#bib.bib69)] generate class representations from labeled data
    based on prototype learning. Through calculating the distances between feature
    vectors of unlabeled images and each class representation followed by a series
    morphological operations, high-quality pseudo labels are then generated. However,
    this prototype learning-based label propagation strategy requests high quality
    and representative feature extraction. For neighbor matching methods, Wang et
    al. [[92](#bib.bib92)] generate pseudo-labels on a weight basis according to the
    embedding similarity with neighboring labeled data. [[86](#bib.bib86)] generate
    pseudo labels through transferring semantics that have a best fit with the unlabeled
    data in feature space among a pool of labeled reference images, as shown in Figure
    [5](#S3.F5 "Figure 5 ‣ 3.1 Semi-Supervised Medical Image Segmentation with Pseudo
    Labels ‣ 3 Related Work on Semi-Supervised Medical Image Segmentation ‣ Learning
    with Limited Annotations: A Survey on Deep Semi-supervised Learning for Medical
    Image Segmentation"). Compared with network prediction-based pseudo label generation
    methods, label propagation-based pseudo label generation can avoid confirmation
    bias. Confirmation bias, which refers to the tendency a model to favor information
    that confirms its existing assumptions, while disregarding information that contradicts
    them, can be caused by the unbalanced training data and usually exists in network
    prediction-based pseudo label generation methods. In conclusion, these label propagation
    methods can premeditate the relations among data points with labeled dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Along with adding more high-confidence pseudo labels, pseudo labeling encourages
    low-density separation between classes. The quality of pseudo labels is the main
    constraint for pseudo labeling strategy. A model is unable to correct its mistakes
    when it overfits to a small labeled data and has confirmation bias. Then wrong
    predictions can be quickly amplified resulting in confident but erroneous pseudo
    labels during the training process [[163](#bib.bib163)]. Thus, how to choose pseudo
    labels that will be added in the next training process and how many iterations
    to repeat need to be further considered.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b0da8a9e130c75d2ef884917d25c1e36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Reference-guided pseudo-label generation [[86](#bib.bib86)]. The
    framework extract features of each unlabeled data and a pool of sampled annotated
    images are employed to generate pseudo-labels. The pseudo-label generation process
    is illustrated on the right, which choose the top-k closest distances in feature
    space among a pool of labeled reference images and transfer their semantics.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Semi-Supervised Medical Image Segmentation with Unsupervised Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Different from generating pseudo labels and updating the segmentation model
    in an iterative manner, some recent progress in semi-supervised medical image
    segmentation has been focused on incorporating unlabeled data into the training
    procedure with unsupervised regularization like unsupervised loss functions. Algorithm
    [2](#alg2 "Algorithm 2 ‣ 3.2 Semi-Supervised Medical Image Segmentation with Unsupervised
    Regularization ‣ 3 Related Work on Semi-Supervised Medical Image Segmentation
    ‣ Learning with Limited Annotations: A Survey on Deep Semi-supervised Learning
    for Medical Image Segmentation") presents the overall workflow of this strategy.
    Different choices of the unsupervised loss functions and regularization terms
    lead to different semi-supervised models. Generally, unsupervised regularization
    can be formulated into three sub-categories: consistency learning, co-training
    and entropy minimization.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Training procedure of semi-supervised learning with unsupervised
    regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 0:   $\{x^{l},y^{l}\}$ from labeled dataset $D_{L}$, $\{x^{u}\}$ from unlabeled
    dataset $D_{U}$, segmentation model $\mathcal{M}$0:  Trained segmentation model
    $\mathcal{M}$1:  while not converge do2:     Calculate supervised segmentation
    loss $\mathcal{L}_{sup}(\theta;\mathcal{D}_{L})$3:     Calculate unsupervised
    loss $\mathcal{L}_{unsup}(\theta;\mathcal{D})$4:     Update the segmentation model
    $\mathcal{M}$ with the combination of supervised loss $\mathcal{L}_{sup}$ and
    unsupervised loss $\mathcal{L}_{unsup}$5:  end while6:  return  Trained segmentation
    model $\mathcal{M}$
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Unsupervised Regularization with Consistency Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For unsupervised regularization, consistency learning is widely applied by
    enforcing an invariance of predictions of input images under different perturbations
    and pushing the decision boundary to low-density regions, based on the assumptions
    that the perturbations should not change the output of the model. The consistency
    between two objects can be calculated as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Loss=D[p(x),p^{{}^{\prime}}(T(x)]$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '$D$ is the similarity measure function, typically using Kullback-Leibler (KL)
    divergence, mean squared error(MSE), Jensen-Shannon divergence(JS) and so on.
    $T(\cdot)$ is augmentation that adds random perturbations on data. $p$ and $p^{{}^{\prime}}$
    represent segmentation models, and their parameters can either be shared or establish
    a connection through certain transformations, such as exponential moving average
    (EMA), or they can be independent of each other. While consistency learning methods
    have shown promising results in semi-supervised medical image segmentation tasks
    due to their simplicity, it has some limitations that need to be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Sensitivity to noise: Consistency learning assumes that small perturbations
    in the input images should not affect the model’s output. However, in practice,
    this assumption may not always hold true as the input data can contain noise or
    outliers. This can lead to the model focusing on these noisy regions during training,
    which may reduce its generalization capability.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Hyperparameter tuning: The performance of consistency learning methods
    depends on the choice of hyperparameters. Selecting appropriate hyperparameters
    can be challenging and may require extensive experiments, making it difficult
    to apply these methods in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Appropriate perturbations: If the perturbations are too weak, consistency-based
    learning may not work, but strong perturbations may confuse the model, and lead
    to low performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Common architectures The common architectures used in consistency learning
    in Figure [6](#S3.F6 "Figure 6 ‣ 3.2.1 Unsupervised Regularization with Consistency
    Learning ‣ 3.2 Semi-Supervised Medical Image Segmentation with Unsupervised Regularization
    ‣ 3 Related Work on Semi-Supervised Medical Image Segmentation ‣ Learning with
    Limited Annotations: A Survey on Deep Semi-supervised Learning for Medical Image
    Segmentation") are illustrated as follows. Sajjadi et al. [[115](#bib.bib115)]
    propose $\Pi$ Model to create two random augmentations of a sample for both labeled
    and unlabeled data. In the training process, the model expects the output of same
    unlabeled sample propagates forward twice under different random perturbations
    to be consistent. Samuli et al. [[30](#bib.bib30)] propose temporal ensembling
    strategy to use EMA predictions for unlabeled data as the consistency targets.
    The basic idea behind temporal ensembling is to train multiple models at different
    time points, and then combine their predictions to make a final prediction. However,
    maintaining the EMA predictions during the training process is a heavy burden.
    To issue the problem, Tarvainen et al. [[29](#bib.bib29)] propose to use a teacher
    model with the EMA weights of the student model for training and enforce the consistency
    of predictions from perturbed inputs between student and teacher models. Thus,
    this mean-teacher architecture is widely employed due to its simplicity. Zeng
    et al. [[95](#bib.bib95)] improve the EMA weighted way in teacher models. They
    add a feedback signal from the performance of the student on the labeled set,
    through which the teacher model can be updated by gradient descent algorithm autonomously
    and purposefully.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5c2989eafc5a3fd7760d7821b78e8763.png)![Refer to caption](img/7993645b293816ba95594a824fa54685.png)![Refer
    to caption](img/5c6cfbd47ff6454710c1804e6dbb4841.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The classic architectures used in consistency learning. (a): $\Pi$
    Model [[115](#bib.bib115)] which creates two random augmentations of a sample
    and encourages consistent predictions. (b):Temporal ensembling strategy [[30](#bib.bib30)]
    to use EMA predictions for unlabeled data as the consistency targets. (c):The
    mean-teacher architecture [[29](#bib.bib29)], in which the teacher model is with
    the EMA weights of the student model.'
  prefs: []
  type: TYPE_NORMAL
- en: Perturbations utilized for consistency learning can be divided into input perturbations
    and feature map perturbations, which should be meaningful for corresponding task.
    The effect of perturbations on segmentation performance has an upper bound, when
    adding more perturbations, the segmentation performance won’t be further improved
    [[99](#bib.bib99)].
  prefs: []
  type: TYPE_NORMAL
- en: Input perturbations There are some commonly used input perturbations, such as
    Gaussian noise, Gaussian blurring, randomly rotation, scaling and contrast variations,
    and the segmentation network is encouraged to be transformation-consistent for
    unlabeled data [[61](#bib.bib61)]. Bortsova et al. [[118](#bib.bib118)] explore
    the equivariance to elastic deformations and encourage the segmentation consistency
    between the predictions of the two identical branches which receive differently
    transformed images. Huang et al. [[99](#bib.bib99)] add cutout and slice misalignment
    as input perturbations. Another common perturbation is mix-up augmentation [[146](#bib.bib146),
    [112](#bib.bib112), [76](#bib.bib76)], which encourages the segmentation of interpolation
    of two data to be consistent with the interpolation of segmentation results of
    those data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature map perturbations Apart from disturbances on input images, there are
    also many studies focusing on disturbances at feature map level. Zheng et al.
    [[89](#bib.bib89)] propose to add random noise to the parameter calculations of
    the teacher model. Xu et al. [[91](#bib.bib91)] propose morphological feature
    perturbations through designing different network architectures, as shown in Figure
    [7](#S3.F7 "Figure 7 ‣ 3.2.1 Unsupervised Regularization with Consistency Learning
    ‣ 3.2 Semi-Supervised Medical Image Segmentation with Unsupervised Regularization
    ‣ 3 Related Work on Semi-Supervised Medical Image Segmentation ‣ Learning with
    Limited Annotations: A Survey on Deep Semi-supervised Learning for Medical Image
    Segmentation"). Atrous convolutions can enlarge foreground features while skip-connections
    will shrink foreground features [[164](#bib.bib164), [165](#bib.bib165)]. Li et
    al. [[96](#bib.bib96)] add seven types of feature perturbations to seven extra
    decoders and require this seven predictions to be consistent with the main decoder.
    These feature level perturbations are feature noise, feature dropout, object masking,
    context masking, guided cutout, intermediate VAT, and random dropout, based on
    the work in [[156](#bib.bib156)]. Among them, object masking, context masking
    and guided cutout utilize the predictions of the decoder to mask objects or contexts
    in feature maps; intermediate VAT refers to using virtual adversarial training
    as a perturbation function for feature maps. Some studies apply perturbations
    both at the input and feature map levels. For example, Xu et al. [[77](#bib.bib77)]
    propose a novel shadow perturbation which contains shadow augmentation [8(a)](#S3.F8.sf1
    "In Figure 8 ‣ 3.2.1 Unsupervised Regularization with Consistency Learning ‣ 3.2
    Semi-Supervised Medical Image Segmentation with Unsupervised Regularization ‣
    3 Related Work on Semi-Supervised Medical Image Segmentation ‣ Learning with Limited
    Annotations: A Survey on Deep Semi-supervised Learning for Medical Image Segmentation")
    and shadow dropout [8(b)](#S3.F8.sf2 "In Figure 8 ‣ 3.2.1 Unsupervised Regularization
    with Consistency Learning ‣ 3.2 Semi-Supervised Medical Image Segmentation with
    Unsupervised Regularization ‣ 3 Related Work on Semi-Supervised Medical Image
    Segmentation ‣ Learning with Limited Annotations: A Survey on Deep Semi-supervised
    Learning for Medical Image Segmentation") to simulate the low image quality and
    shadow artifacts in medical images. Specifically, shadow augmentation is a perturbation
    through adding simulated shadow artifacts to the input images while shadow dropout
    will drop neural nodes according to the prior knowledge of the shadow artifacts,
    which is a disturbance acting directly on feature maps. However, if the perturbations
    are too weak, it may cause the student model to memorize these easy variations
    and fit the training data quickly. Finally, the student model fails to discover
    effective features, which is the Lazy Student Phenomenon. But strong perturbations
    may confuse the teacher and student, and lead to low performance. To avoid the
    large gap between the student model and teacher model, Shu et al. [[112](#bib.bib112)]
    add a transductive monitor for further knowledge distillation to narrow the semantic
    gap between the student model and teacher model. Some works [[207](#bib.bib207),
    [208](#bib.bib208), [206](#bib.bib206)] explicitly divide perturbations into strong
    and weak perturbations, and use the prediction from a weakly perturbed input to
    supervise the prediction from its strong perturbed version. These works hold the
    assumption that weakly perturbed inputs can provide reliable predictions whereas
    strongly perturbed ones can improve the learning process and model robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a0811ee5e3e95c76395aea7a72904ba5.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Atrous convolution [[164](#bib.bib164)] to enlarge foreground features
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d406be621482698e293cd2cf0ddf659d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Skip-connections [[166](#bib.bib166)] to shrink foreground features
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Morphological feature perturbations through designing different network
    architectures [[91](#bib.bib91)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a98416aaae88d2843849048fbee90987.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Shadow augmentation which imposes the shadow artifacts extracted from shadow
    source images on the original input images with different values of shadow threshold
    $\tau_{s}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/45fa5e794d5b5aada72951f421a64b93.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Shadow dropout which filters the features extracted from the shadow regions
    in feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Shadow augmentation and dropout [[77](#bib.bib77)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task-level regularization Other than utilizing data-level perturbations for
    consistency learning, some methods focus on building task-level regularization
    by adding auxiliary task to leverage geometric information. Li et al. [[34](#bib.bib34)]
    develop a multi-task network to build shape-aware constraints with adversarial
    regularization. Liu et al. [[143](#bib.bib143)] propose a shape-aware multi-task
    framework which contained segmentation, signed distance map prediction and organ
    contour prediction. Luo et al. [[35](#bib.bib35)] combine the level set function
    regression task with the segmentation task to form a dual-task consistency for
    semi-supervised learning. Zhang et al. [[36](#bib.bib36)] propose dual-task mutual
    learning framework by encouraging dual-task networks to explore useful knowledge
    from each other. Based on dual-task framework, Zhang et al. [[9](#bib.bib9)] utilize
    both segmentation task and regression task for self-ensembling and utilize estimated
    uncertainty to guide the mutual consistency learning and obtain further performance
    improvement, Shi et al. [[218](#bib.bib218)] propose to utilize segmentation task
    and regression task as student networks for competitive ensembling to the teacher
    network. Chen et al. [[113](#bib.bib113)] propose a dual-task consistency joint
    learning framework that encouraged the segmentation results to be consistent with
    the transformation of the signed distance map predictions. Wang et al. [[97](#bib.bib97)]
    inject multi-task learning into mean teacher architecture which contain the segmentation
    task, the reconstruction task, and the signed distance field prediction task so
    that the model can take account of the data-, model- and task-level consistency,
    as shown in Figure [9](#S3.F9 "Figure 9 ‣ 3.2.1 Unsupervised Regularization with
    Consistency Learning ‣ 3.2 Semi-Supervised Medical Image Segmentation with Unsupervised
    Regularization ‣ 3 Related Work on Semi-Supervised Medical Image Segmentation
    ‣ Learning with Limited Annotations: A Survey on Deep Semi-supervised Learning
    for Medical Image Segmentation"). In signed distance field prediction, a neural
    network is trained to predict the signed distance value of each pixel from the
    nearest foreground points. The sign of the distance indicates whether the point
    is inside or outside the region of interest, while the magnitude of the distance
    gives an estimate of the distance from the foreground. Besides, they propose an
    uncertainty weighted integration (UWI) strategy to estimate the uncertainty on
    all tasks and develop a triple-uncertainty based on these tasks to guide the student
    model to learn reliable information from teacher.'
  prefs: []
  type: TYPE_NORMAL
- en: Variants of consistency calculation methods There are multiple consistency calculation
    methods to avoid noisy pixel predictions, such as uncertainty-based consistency
    learning [[31](#bib.bib31), [141](#bib.bib141), [106](#bib.bib106), [50](#bib.bib50),
    [106](#bib.bib106), [50](#bib.bib50), [110](#bib.bib110)], multi-level consistency
    learning [[124](#bib.bib124)], attention-guided consistency learning [[81](#bib.bib81)]
    and so on. The predictions of the teacher model can be wrong at some locations
    and might confuse the student model in the mean-teacher architecture. So uncertainty
    or confidence estimation are utilized to learn from more meaningful and reliable
    targets during training. Yu et al. [[31](#bib.bib31)] extend the mean teacher
    paradigm with an uncertainty estimation strategy through Monte Carlo dropout [[90](#bib.bib90)].
    To use Monte Carlo dropout in semi-supervised learning, the labeled data is used
    to train the model with dropout turned on. Then, the model is used to make predictions
    on the unlabeled data, with dropout turned on. The multiple predictions of the
    same unlabeled sample are then used to compute an uncertainty. This uncertainty
    can be used to guide the pseudo-labeling process, by identifying pixels that are
    likely to be mislabeled or ambiguous. Xie et al. [[141](#bib.bib141)] add a confidence-aware
    module to learn the model confidence under the guidance of labeled data. Luo et
    al. [[106](#bib.bib106), [50](#bib.bib50)] calculate uncertainty using pyramid
    predictions in one forward pass and proposed an multi-level uncertainty rectified
    pyramid consistency regularization. Fang et al. [[110](#bib.bib110)] attach an
    error estimation network to predict the loss map of the teacher’s prediction.
    Then the consistency loss will be calculated on low loss pixels. Chen et al. [[124](#bib.bib124)]
    propose multi-level consistency loss which computes the similarities between multi-scale
    features in an additional discriminator, where the inputs are the segmentation
    regions by multiplying the unlabeled input image with predicted segmentation probability
    maps instead of segmentation probability maps. Hu et al. [[81](#bib.bib81)] propose
    attention guided consistency which encourages the attention maps from the student
    model and the teacher model to be consistent. Zhao et al. [[75](#bib.bib75)] introduce
    cross-level consistency constraint which is calculated between patches and the
    full image.Except encouraging consistency on network segmentation results directly,
    generative consistency [[101](#bib.bib101)] is proposed through a generation network
    that reconstructs medical images from its predictions of the segmentation network.
    Xu et al. [[84](#bib.bib84)] propose contour consistency and utilize Fourier series
    which contained a series of harmonics as an elliptical descriptor. Through minimizing
    the L2 distance of the parameters between the student and the teacher branch,
    the model is equipped with shape awareness. However, this method needs to choose
    different maximum harmonic numbers for the segmentation of targets with different
    irregularity. Each image contains the same class object, so different images share
    similar semantics in the feature space. Xie et al. [[135](#bib.bib135)] introduce
    intra- and inter-pair consistency to augment feature maps. The pixel-level relation
    between a pair of images in the feature space is first calculated to obtain the
    attention maps that highlight the regions with the same semantics but on different
    images. Then multiple attention maps are taken into account to filter the low-confidence
    regions and then merged with the original feature map to improve its representation
    ability. Liu et al. [[102](#bib.bib102)] propose contrastive consistency which
    encourages segmentation outputs to be consistent in class-level through foreground
    and background class-vectors generated from a classification network. Xu et al.
    [[70](#bib.bib70)] propose the cyclic prototype consistency learning (CPCL) framework
    which contains a labeled-to-unlabeled (L2U) prototypical forward process and an
    unlabeled-to-labeled (U2L) backward process. The L2U forward consistency can transfer
    the real label supervision signals to unlabeled data using labeled prototypes
    while the U2L backward consistency can directly using unlabeled prototypes to
    segment labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c2ce1e2535d2e3045a53f64f9b52fa16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Multi-task learning in mean teacher architecture which contains the
    segmentation task, the reconstruction task, and the signed distance field prediction
    task[[97](#bib.bib97)]. The inter-task consistency encourages consistent predictions
    between the three tasks and the inter-model consistency encourages consistent
    predictions between the teacher model and the student model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: The summarized review of semi-supervised medical image segmentation
    methods with consistency learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | 2D/3D | Modality | Dataset | Perturbations |'
  prefs: []
  type: TYPE_TB
- en: '| SSN-RCL, Huang et al. [[99](#bib.bib99)] | 3D | Microscopy | Kasthuri15 [[171](#bib.bib171)],
    CREMI ¹¹10 | Gaussian blurring, Gaussian noise, slice misalignment, contrast variations
    |'
  prefs: []
  type: TYPE_TB
- en: '| SCO-SSL, Xu et al.[[77](#bib.bib77)] | 3D | US | UCLA [[172](#bib.bib172)]
    | Shadow augmentation, shadow dropout |'
  prefs: []
  type: TYPE_TB
- en: '| SemiTC, Bortsova et al. [[118](#bib.bib118)] | 2D | X-Ray | JSRT dataset
    [[15](#bib.bib15)] | Elastic deformations |'
  prefs: []
  type: TYPE_TB
- en: '| GCS, Chen et al.[[101](#bib.bib101)] | 3D | TOF-MRA | MIDAS dataset [[18](#bib.bib18)]
    | Random perturbations |'
  prefs: []
  type: TYPE_TB
- en: '| DUW-SSL, Wang et al. [[32](#bib.bib32)] | 3D | CT, MRI | LA dataset [[66](#bib.bib66)],
    KiTS dataset [[175](#bib.bib175)] | Random noise, dropout |'
  prefs: []
  type: TYPE_TB
- en: '| URPC, Luo et al.[[50](#bib.bib50)] | 3D | CT | BraTS 2019[[128](#bib.bib128)],
    Pancreas CT[[130](#bib.bib130)] | Randomly cropped patches, multi-level pyramid
    predictions |'
  prefs: []
  type: TYPE_TB
- en: '| Mtans, Chen et al.[[124](#bib.bib124)] | 3D | MRI | Longitudinal Multiple
    Sclerosis Lesion Segmentation [[173](#bib.bib173)], ISLES 2015[[174](#bib.bib174)],
    BraTS 2018[[128](#bib.bib128)] | Multi-scale features |'
  prefs: []
  type: TYPE_TB
- en: '| CPCL, Xu et al.[[70](#bib.bib70)] | 3D | CT, MRI | BraTS 2019 [[128](#bib.bib128)],
    KiTS dataset [[175](#bib.bib175)] | Different input images |'
  prefs: []
  type: TYPE_TB
- en: '| AHDC, Chen et al.[[88](#bib.bib88)] | 3D | CT, MRI | LGE-CMR datasets from
    [[176](#bib.bib176), [177](#bib.bib177)], MM-WHS dataset [[178](#bib.bib178),
    [179](#bib.bib179)] | Different domain inputs |'
  prefs: []
  type: TYPE_TB
- en: '| UA-MT, Yu et al.[[31](#bib.bib31)] | 3D | MRI | LA dataset [[66](#bib.bib66)]
    | Random flipping, random rotating |'
  prefs: []
  type: TYPE_TB
- en: '| SASSNet, Zhang et al.[[34](#bib.bib34)] | 3D | MRI | LA dataset [[66](#bib.bib66)]
    | Task-level consistency |'
  prefs: []
  type: TYPE_TB
- en: '| DTC, Luo et al.[[35](#bib.bib35)] | 3D | CT, MRI | LA dataset [[66](#bib.bib66)],
    Pancreas CT [[130](#bib.bib130)] | Task-level consistency |'
  prefs: []
  type: TYPE_TB
- en: '| T-UncA, Wang et al.[[67](#bib.bib67)] | 2D | MRI | ACDC dataset [[22](#bib.bib22)],
    PROMISE [[182](#bib.bib182)] | Task-level consistency |'
  prefs: []
  type: TYPE_TB
- en: 1\. https://cremi.org/
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Unsupervised Regularization with Co-Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Co-training framework assumes that each data has two or more different views
    and each view has sufficient information to give predictions independently [[147](#bib.bib147)].
    It first learns a separate segmentation model for each view on labeled data, and
    then the predictions of the models on unlabeled data are gradually added to training
    set to continue the training. In co-training, one view is redundant to other views
    and the models are encouraged to have consistent predictions on all the views.
    Note that different from self-training methods, co-training methods add pseudo
    labels from one view to the training set and act as supervision signals to train
    models of other views. And the difference between co-training and consistency
    learning is that all the models in co-training will be updated through gradient
    descent algorithm whereas the consistency learning encourages the outputs for
    different perturbations to be consistent and only one main model is updated by
    gradient descent algorithm, such as the mean-teacher architecture [[29](#bib.bib29)].
    There are also some limitations that need to be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Sufficient independence between views: Co-training assumes that each view
    is sufficient and independent enough to make predictions on its own. However,
    in real-world scenarios, this assumption may not always hold true, leading to
    poor performance when the views are correlated or redundant.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Risk of model conflict: Co-training encourages consistency between the
    models’ predictions across different views. However, if the models are too similar,
    they may become overly specialized and fail to capture the underlying patterns
    in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Sensitivity to noisy pseudo labels: Co-training adds pseudo labels from
    one view to the training set as supervision signals for other views. If these
    pseudo labels from one view are noisy or incorrect, it can negatively impact the
    performance of other views.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Construction of different views The core of co-training is how to construct
    two (or more) deep models of approximately represent sufficiently independent
    views. The methods mainly contain using different sources of data, employing different
    network architectures and using special training methods to obtain diverse deep
    models. First, different sources of data includes data from different modalities
    [[94](#bib.bib94), [103](#bib.bib103)], medical centers [[60](#bib.bib60)] or
    anatomical planes [[148](#bib.bib148), [133](#bib.bib133)], which lead to different
    distributions. For example, Zhu et al. [[94](#bib.bib94)] propose a co-training
    framework for unpaired multi-modal learning. This framework contains two segmentation
    networks and two image translation networks across two modalities. They utilize
    the pseudo-labels (from unlabeled data) or labels (from labeled data) from one
    modality to train the segmentation network in the other modality after image translation.
    For one thing, it increases supervision signals. For another, it adds modality-level
    consistency. Chen et al. [[103](#bib.bib103)] leveraged unpaired multi-modality
    images to be cross-modal consistent in anatomy and semantic information. The multi
    modalities which are collaborative and complementary could encourage better modality-independent
    representation learning. Liu et al. [[60](#bib.bib60)] present a co-training framework
    for domain-adaptive medical image segmentation. This framework contains two segmentors
    used for semi-supervised segmentation task (labeled and unlabeled target domain
    data as inputs) and unsupervised domain adaptation task (labeled source domain
    data and unlabeled target domain data as inputs), respectively. [[148](#bib.bib148),
    [133](#bib.bib133)] use coronal, sagittal and axial views of 3D medical images
    as view difference at input level and [[148](#bib.bib148)] also use asymmetric
    3D kernels with 2D initialization as view difference at feature level. However,
    when there are only one source of data available, training two (or more) identical
    networks may lead to collapsed neural networks as the predictions from these models
    are encouraged to be similar. [[149](#bib.bib149), [151](#bib.bib151)] generate
    adversarial examples as another view. Second, as different models usually extract
    different representations, different models in co-training framework can focus
    on different views. Except using CNN as the backbones, there are also some transformer-based
    backbones [[114](#bib.bib114), [119](#bib.bib119)]. As shown in Figure [10](#S3.F10
    "Figure 10 ‣ 3.2.2 Unsupervised Regularization with Co-Training ‣ 3.2 Semi-Supervised
    Medical Image Segmentation with Unsupervised Regularization ‣ 3 Related Work on
    Semi-Supervised Medical Image Segmentation ‣ Learning with Limited Annotations:
    A Survey on Deep Semi-supervised Learning for Medical Image Segmentation"), Luo
    et al. [[132](#bib.bib132)] introduce the cross teaching between CNN- and transformer-based
    backbones which implicitly encourages the consistency and complementary between
    different networks. Liu et al. [[114](#bib.bib114)] combine CNN blocks and Swin
    Transformer blocks as the backbone. Xiao et al. [[119](#bib.bib119)] add another
    teacher model with the transformer-based architecture. The teacher models communicate
    with each other with consistency regularization and guide the student learning
    process. Third, diverse deep models can also be trained using special training
    methods. For instance, Chen et al. [[150](#bib.bib150)] use output smearing to
    generate different labeled data sets to initialize diverse models. To maintain
    the diversity in the subsequent training process, the modules are fine-tuned using
    the generated sets in specific rounds.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca193da4f61d34b00e0c555887e6f8e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: A co-training framework which uses CNN- and transformer-based backbones
    and encourages the consistency and complementary between different networks [[132](#bib.bib132)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Avoiding noisy pseudo labels in co-training is also important. Although consistent
    predictions are encouraged across the networks, they may contain noise leading
    to unstable training process. To overcome the third limitation mentioned in the
    Sec.[3.2.2](#S3.SS2.SSS2 "3.2.2 Unsupervised Regularization with Co-Training ‣
    3.2 Semi-Supervised Medical Image Segmentation with Unsupervised Regularization
    ‣ 3 Related Work on Semi-Supervised Medical Image Segmentation ‣ Learning with
    Limited Annotations: A Survey on Deep Semi-supervised Learning for Medical Image
    Segmentation"), an uncertainty-aware co-training framework [[148](#bib.bib148)]
    is proposed through estimating the confidence of each view and fusing the predictions
    from the other views to generate the pseudo labels for one view. Wang et al. [[100](#bib.bib100)]
    develop a self-paced and self-consistent co-training framework. The self-paced
    strategy can encourage the network to transfer the knowledge of easier-to-segment
    regions to the harder ones gradually through minimizing a generalized Jensen-Shannon
    divergence. Another way to alleviate the influence from noisy pseudo labels is
    through exponential mix-up decay to adjust the contribution of the supervision
    signals from both labels and pseudo labels across the training process [[60](#bib.bib60)].
    Except the methods mentioned above, adversarial learning in [3.2.3](#S3.SS2.SSS3
    "3.2.3 Unsupervised Regularization with Adversarial Learning ‣ 3.2 Semi-Supervised
    Medical Image Segmentation with Unsupervised Regularization ‣ 3 Related Work on
    Semi-Supervised Medical Image Segmentation ‣ Learning with Limited Annotations:
    A Survey on Deep Semi-supervised Learning for Medical Image Segmentation") is
    always conducted to generate pixel-wise confidence maps or uncertainty. The semi-supervised
    models will learn from high-confidence predictions [[210](#bib.bib210)], thus
    avoiding noisy pseudo labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: The summarized review of semi-supervised medical image segmentation
    methods with co-training.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | 2D/3D | Modality | Dataset | Diverse views from |'
  prefs: []
  type: TYPE_TB
- en: '| Spsco-Cot, Wang et al.[[100](#bib.bib100)] | 2D | CT, MRI | ACDC dataset
    [[22](#bib.bib22)], Spleen sub-task of Medical Segmentation Decathlon [[185](#bib.bib185)],
    PROMISE [[182](#bib.bib182)] | Perturbations |'
  prefs: []
  type: TYPE_TB
- en: '| DCT-Seg, Peng et al.[[151](#bib.bib151)] | 2D | CT, MRI | ACDC dataset[[22](#bib.bib22)],
    SCGM [[184](#bib.bib184)], Spleen dataset [[185](#bib.bib185)] | Perturbations
    |'
  prefs: []
  type: TYPE_TB
- en: '| MASS, Chen et al.[[103](#bib.bib103)] | 3D | CT, MRI | BTCV[[187](#bib.bib187)],
    CHAOS[[183](#bib.bib183)] | Different modalities |'
  prefs: []
  type: TYPE_TB
- en: '| SSUML, Zhu et al.[[94](#bib.bib94)] | 2D | CT, MRI | Cardiac substructure
    segmentation [[178](#bib.bib178)], Abdominal multi-organ segmentation [[186](#bib.bib186),
    [183](#bib.bib183)] | Different modalities |'
  prefs: []
  type: TYPE_TB
- en: '| CT_CNN&Trans, Luo et al.[[132](#bib.bib132)] | 2D | MRI | ACDC dataset [[22](#bib.bib22)]
    | Different segmentation networks |'
  prefs: []
  type: TYPE_TB
- en: '| Mmgl, Zhao et al.[[133](#bib.bib133)] | 3D | CT | MM-WHS dataset [[178](#bib.bib178),
    [179](#bib.bib179)] | Different transformations |'
  prefs: []
  type: TYPE_TB
- en: '| UMCT, Xia et al.[[148](#bib.bib148)] | 3D | CT | NIH Pancreas [[130](#bib.bib130)],
    LiTS dataset [[17](#bib.bib17)] | Different transformations |'
  prefs: []
  type: TYPE_TB
- en: 3.2.3 Unsupervised Regularization with Adversarial Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Adversarial methods is used to encourage the distribution of predictions from
    unlabeled images to be closer to that of labeled images in semi-supervised learning.
    These methods always contain a discriminator to distinguish the inputs from labeled
    annotations or unlabeled predictions [[45](#bib.bib45), [137](#bib.bib137), [124](#bib.bib124),
    [93](#bib.bib93)]. However, adversarial training may be challenging in terms of
    convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. [[45](#bib.bib45)] introduce adversarial learning to encourage
    the segmentations of unlabeled data to be similar with the annotations of labeled
    data. Chen et al. [[124](#bib.bib124)] add a discriminator following the segmentation
    network which is used to distinguish between the input signed distance maps from
    labeled images or unlabeled images. Peiris et al. [[93](#bib.bib93), [210](#bib.bib210)]
    add a critic network into the segmentation architecture which can perform the
    min-max game through discriminating between prediction masks and the ground truth
    masks. The experiments show that it could sharpen boundaries in prediction masks.
    The discriminator can also be used to generate pixel-wise confidence maps and
    select the trustworthy pixel predictions used for co-training. Wu et al. [[134](#bib.bib134)]
    add two discriminators for predicting confidence maps and distinguishing the segmentation
    results from labeled or unlabeled data. Through adding another auxiliary discriminator,
    the under trained primary discriminator due to limited labeled images can be alleviated.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Unsupervised Regularization with Entropy Minimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Entropy minimization encourages the model to output low-entropy predictions
    on unlabeled data and avoids the class overlap. Semi-supervised learning algorithms
    [[105](#bib.bib105), [154](#bib.bib154), [155](#bib.bib155)] are usually combined
    with entropy minimization based on the assumption that the decision boundary should
    lie in low-density regions. For instance, in [[154](#bib.bib154)], a loss term
    is added to minimize the entropy of the predictions of the model on unlabeled
    data and the objective function turns to be as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S3.E4.m1.92" class="ltx_Math" alttext="\begin{split}C(\theta,\lambda;\mathcal{L}_{n})&amp;=L(\theta;\mathcal{L}_{n})-\lambda
    H_{emp}(Y&#124;X,Z;\mathcal{L}_{n})\\ &amp;=\sum_{i=1}^{n}\log(\sum_{k=1}^{K}z_{ik}f_{k}(x_{i}))\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;+\lambda\sum_{i=1}^{n}\sum_{k=1}^{K}g_{k}(x_{i},z_{i})\log g_{k}(x_{i},z_{i})\end{split}"
    display="block"><semantics id="S3.E4.m1.92a"><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" id="S3.E4.m1.92.92.16" xref="S3.E4.m1.84.84.8.cmml"><mtr id="S3.E4.m1.92.92.16a"
    xref="S3.E4.m1.84.84.8.cmml"><mtd class="ltx_align_right" columnalign="right"
    id="S3.E4.m1.92.92.16b" xref="S3.E4.m1.84.84.8.cmml"><mrow id="S3.E4.m1.85.85.9.77.32.10"
    xref="S3.E4.m1.84.84.8.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml">C</mi><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.85.85.9.77.32.10.11" xref="S3.E4.m1.84.84.8a.cmml">​</mo><mrow
    id="S3.E4.m1.85.85.9.77.32.10.10.1" xref="S3.E4.m1.84.84.8.cmml"><mo stretchy="false"
    id="S3.E4.m1.2.2.2.2.2.2" xref="S3.E4.m1.84.84.8a.cmml">(</mo><mi id="S3.E4.m1.3.3.3.3.3.3"
    xref="S3.E4.m1.3.3.3.3.3.3.cmml">θ</mi><mo id="S3.E4.m1.4.4.4.4.4.4" xref="S3.E4.m1.84.84.8a.cmml">,</mo><mi
    id="S3.E4.m1.5.5.5.5.5.5" xref="S3.E4.m1.5.5.5.5.5.5.cmml">λ</mi><mo id="S3.E4.m1.6.6.6.6.6.6"
    xref="S3.E4.m1.84.84.8a.cmml">;</mo><msub id="S3.E4.m1.85.85.9.77.32.10.10.1.1"
    xref="S3.E4.m1.84.84.8.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.7.7.7.7.7.7"
    xref="S3.E4.m1.7.7.7.7.7.7.cmml">ℒ</mi><mi id="S3.E4.m1.8.8.8.8.8.8.1" xref="S3.E4.m1.8.8.8.8.8.8.1.cmml">n</mi></msub><mo
    stretchy="false" id="S3.E4.m1.9.9.9.9.9.9" xref="S3.E4.m1.84.84.8a.cmml">)</mo></mrow></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E4.m1.92.92.16c" xref="S3.E4.m1.84.84.8.cmml"><mrow
    id="S3.E4.m1.87.87.11.79.34.24" xref="S3.E4.m1.84.84.8.cmml"><mo id="S3.E4.m1.10.10.10.10.1.1"
    xref="S3.E4.m1.10.10.10.10.1.1.cmml">=</mo><mrow id="S3.E4.m1.87.87.11.79.34.24.24"
    xref="S3.E4.m1.84.84.8.cmml"><mrow id="S3.E4.m1.86.86.10.78.33.23.23.1" xref="S3.E4.m1.84.84.8.cmml"><mi
    id="S3.E4.m1.11.11.11.11.2.2" xref="S3.E4.m1.11.11.11.11.2.2.cmml">L</mi><mo lspace="0em"
    rspace="0em" id="S3.E4.m1.86.86.10.78.33.23.23.1.2" xref="S3.E4.m1.84.84.8a.cmml">​</mo><mrow
    id="S3.E4.m1.86.86.10.78.33.23.23.1.1.1" xref="S3.E4.m1.84.84.8.cmml"><mo stretchy="false"
    id="S3.E4.m1.12.12.12.12.3.3" xref="S3.E4.m1.84.84.8a.cmml">(</mo><mi id="S3.E4.m1.13.13.13.13.4.4"
    xref="S3.E4.m1.13.13.13.13.4.4.cmml">θ</mi><mo id="S3.E4.m1.14.14.14.14.5.5" xref="S3.E4.m1.84.84.8a.cmml">;</mo><msub
    id="S3.E4.m1.86.86.10.78.33.23.23.1.1.1.1" xref="S3.E4.m1.84.84.8.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S3.E4.m1.15.15.15.15.6.6" xref="S3.E4.m1.15.15.15.15.6.6.cmml">ℒ</mi><mi id="S3.E4.m1.16.16.16.16.7.7.1"
    xref="S3.E4.m1.16.16.16.16.7.7.1.cmml">n</mi></msub><mo stretchy="false" id="S3.E4.m1.17.17.17.17.8.8"
    xref="S3.E4.m1.84.84.8a.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.18.18.18.18.9.9"
    xref="S3.E4.m1.18.18.18.18.9.9.cmml">−</mo><mrow id="S3.E4.m1.87.87.11.79.34.24.24.2"
    xref="S3.E4.m1.84.84.8.cmml"><mi id="S3.E4.m1.19.19.19.19.10.10" xref="S3.E4.m1.19.19.19.19.10.10.cmml">λ</mi><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.87.87.11.79.34.24.24.2.2" xref="S3.E4.m1.84.84.8a.cmml">​</mo><msub
    id="S3.E4.m1.87.87.11.79.34.24.24.2.3" xref="S3.E4.m1.84.84.8.cmml"><mi id="S3.E4.m1.20.20.20.20.11.11"
    xref="S3.E4.m1.20.20.20.20.11.11.cmml">H</mi><mrow id="S3.E4.m1.21.21.21.21.12.12.1"
    xref="S3.E4.m1.21.21.21.21.12.12.1.cmml"><mi id="S3.E4.m1.21.21.21.21.12.12.1.2"
    xref="S3.E4.m1.21.21.21.21.12.12.1.2.cmml">e</mi><mo lspace="0em" rspace="0em"
    id="S3.E4.m1.21.21.21.21.12.12.1.1" xref="S3.E4.m1.21.21.21.21.12.12.1.1.cmml">​</mo><mi
    id="S3.E4.m1.21.21.21.21.12.12.1.3" xref="S3.E4.m1.21.21.21.21.12.12.1.3.cmml">m</mi><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.21.21.21.21.12.12.1.1a" xref="S3.E4.m1.21.21.21.21.12.12.1.1.cmml">​</mo><mi
    id="S3.E4.m1.21.21.21.21.12.12.1.4" xref="S3.E4.m1.21.21.21.21.12.12.1.4.cmml">p</mi></mrow></msub><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.87.87.11.79.34.24.24.2.2a" xref="S3.E4.m1.84.84.8a.cmml">​</mo><mrow
    id="S3.E4.m1.87.87.11.79.34.24.24.2.1.1" xref="S3.E4.m1.84.84.8.cmml"><mo stretchy="false"
    id="S3.E4.m1.22.22.22.22.13.13" xref="S3.E4.m1.84.84.8a.cmml">(</mo><mrow id="S3.E4.m1.87.87.11.79.34.24.24.2.1.1.1"
    xref="S3.E4.m1.84.84.8.cmml"><mi id="S3.E4.m1.23.23.23.23.14.14" xref="S3.E4.m1.23.23.23.23.14.14.cmml">Y</mi><mo
    fence="false" id="S3.E4.m1.24.24.24.24.15.15" xref="S3.E4.m1.24.24.24.24.15.15.cmml">&#124;</mo><mrow
    id="S3.E4.m1.87.87.11.79.34.24.24.2.1.1.1.1.1" xref="S3.E4.m1.84.84.8.cmml"><mi
    id="S3.E4.m1.25.25.25.25.16.16" xref="S3.E4.m1.25.25.25.25.16.16.cmml">X</mi><mo
    id="S3.E4.m1.26.26.26.26.17.17" xref="S3.E4.m1.84.84.8a.cmml">,</mo><mi id="S3.E4.m1.27.27.27.27.18.18"
    xref="S3.E4.m1.27.27.27.27.18.18.cmml">Z</mi><mo id="S3.E4.m1.28.28.28.28.19.19"
    xref="S3.E4.m1.84.84.8a.cmml">;</mo><msub id="S3.E4.m1.87.87.11.79.34.24.24.2.1.1.1.1.1.1"
    xref="S3.E4.m1.84.84.8.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.29.29.29.29.20.20"
    xref="S3.E4.m1.29.29.29.29.20.20.cmml">ℒ</mi><mi id="S3.E4.m1.30.30.30.30.21.21.1"
    xref="S3.E4.m1.30.30.30.30.21.21.1.cmml">n</mi></msub></mrow></mrow><mo stretchy="false"
    id="S3.E4.m1.31.31.31.31.22.22" xref="S3.E4.m1.84.84.8a.cmml">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    id="S3.E4.m1.92.92.16d" xref="S3.E4.m1.84.84.8.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E4.m1.92.92.16f" xref="S3.E4.m1.84.84.8.cmml"><mrow
    id="S3.E4.m1.88.88.12.80.19.19" xref="S3.E4.m1.84.84.8.cmml"><mo rspace="0.111em"
    id="S3.E4.m1.32.32.32.1.1.1" xref="S3.E4.m1.32.32.32.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.88.88.12.80.19.19.19"
    xref="S3.E4.m1.84.84.8.cmml"><munderover id="S3.E4.m1.88.88.12.80.19.19.19.2"
    xref="S3.E4.m1.84.84.8.cmml"><mo movablelimits="false" id="S3.E4.m1.33.33.33.2.2.2"
    xref="S3.E4.m1.33.33.33.2.2.2.cmml">∑</mo><mrow id="S3.E4.m1.34.34.34.3.3.3.1"
    xref="S3.E4.m1.34.34.34.3.3.3.1.cmml"><mi id="S3.E4.m1.34.34.34.3.3.3.1.2" xref="S3.E4.m1.34.34.34.3.3.3.1.2.cmml">i</mi><mo
    id="S3.E4.m1.34.34.34.3.3.3.1.1" xref="S3.E4.m1.34.34.34.3.3.3.1.1.cmml">=</mo><mn
    id="S3.E4.m1.34.34.34.3.3.3.1.3" xref="S3.E4.m1.34.34.34.3.3.3.1.3.cmml">1</mn></mrow><mi
    id="S3.E4.m1.35.35.35.4.4.4.1" xref="S3.E4.m1.35.35.35.4.4.4.1.cmml">n</mi></munderover><mrow
    id="S3.E4.m1.88.88.12.80.19.19.19.1.1" xref="S3.E4.m1.84.84.8.cmml"><mi id="S3.E4.m1.36.36.36.5.5.5"
    xref="S3.E4.m1.36.36.36.5.5.5.cmml">log</mi><mo id="S3.E4.m1.88.88.12.80.19.19.19.1.1a"
    xref="S3.E4.m1.84.84.8a.cmml">⁡</mo><mrow id="S3.E4.m1.88.88.12.80.19.19.19.1.1.1"
    xref="S3.E4.m1.84.84.8.cmml"><mo stretchy="false" id="S3.E4.m1.37.37.37.6.6.6"
    xref="S3.E4.m1.84.84.8a.cmml">(</mo><mrow id="S3.E4.m1.88.88.12.80.19.19.19.1.1.1.1"
    xref="S3.E4.m1.84.84.8.cmml"><munderover id="S3.E4.m1.88.88.12.80.19.19.19.1.1.1.1.2"
    xref="S3.E4.m1.84.84.8.cmml"><mo lspace="0em" movablelimits="false" id="S3.E4.m1.38.38.38.7.7.7"
    xref="S3.E4.m1.38.38.38.7.7.7.cmml">∑</mo><mrow id="S3.E4.m1.39.39.39.8.8.8.1"
    xref="S3.E4.m1.39.39.39.8.8.8.1.cmml"><mi id="S3.E4.m1.39.39.39.8.8.8.1.2" xref="S3.E4.m1.39.39.39.8.8.8.1.2.cmml">k</mi><mo
    id="S3.E4.m1.39.39.39.8.8.8.1.1" xref="S3.E4.m1.39.39.39.8.8.8.1.1.cmml">=</mo><mn
    id="S3.E4.m1.39.39.39.8.8.8.1.3" xref="S3.E4.m1.39.39.39.8.8.8.1.3.cmml">1</mn></mrow><mi
    id="S3.E4.m1.40.40.40.9.9.9.1" xref="S3.E4.m1.40.40.40.9.9.9.1.cmml">K</mi></munderover><mrow
    id="S3.E4.m1.88.88.12.80.19.19.19.1.1.1.1.1" xref="S3.E4.m1.84.84.8.cmml"><msub
    id="S3.E4.m1.88.88.12.80.19.19.19.1.1.1.1.1.3" xref="S3.E4.m1.84.84.8.cmml"><mi
    id="S3.E4.m1.41.41.41.10.10.10" xref="S3.E4.m1.41.41.41.10.10.10.cmml">z</mi><mrow
    id="S3.E4.m1.42.42.42.11.11.11.1" xref="S3.E4.m1.42.42.42.11.11.11.1.cmml"><mi
    id="S3.E4.m1.42.42.42.11.11.11.1.2" xref="S3.E4.m1.42.42.42.11.11.11.1.2.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.42.42.42.11.11.11.1.1" xref="S3.E4.m1.42.42.42.11.11.11.1.1.cmml">​</mo><mi
    id="S3.E4.m1.42.42.42.11.11.11.1.3" xref="S3.E4.m1.42.42.42.11.11.11.1.3.cmml">k</mi></mrow></msub><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.88.88.12.80.19.19.19.1.1.1.1.1.2" xref="S3.E4.m1.84.84.8a.cmml">​</mo><msub
    id="S3.E4.m1.88.88.12.80.19.19.19.1.1.1.1.1.4" xref="S3.E4.m1.84.84.8.cmml"><mi
    id="S3.E4.m1.43.43.43.12.12.12" xref="S3.E4.m1.43.43.43.12.12.12.cmml">f</mi><mi
    id="S3.E4.m1.44.44.44.13.13.13.1" xref="S3.E4.m1.44.44.44.13.13.13.1.cmml">k</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.88.88.12.80.19.19.19.1.1.1.1.1.2a" xref="S3.E4.m1.84.84.8a.cmml">​</mo><mrow
    id="S3.E4.m1.88.88.12.80.19.19.19.1.1.1.1.1.1.1" xref="S3.E4.m1.84.84.8.cmml"><mo
    stretchy="false" id="S3.E4.m1.45.45.45.14.14.14" xref="S3.E4.m1.84.84.8a.cmml">(</mo><msub
    id="S3.E4.m1.88.88.12.80.19.19.19.1.1.1.1.1.1.1.1" xref="S3.E4.m1.84.84.8.cmml"><mi
    id="S3.E4.m1.46.46.46.15.15.15" xref="S3.E4.m1.46.46.46.15.15.15.cmml">x</mi><mi
    id="S3.E4.m1.47.47.47.16.16.16.1" xref="S3.E4.m1.47.47.47.16.16.16.1.cmml">i</mi></msub><mo
    stretchy="false" id="S3.E4.m1.48.48.48.17.17.17" xref="S3.E4.m1.84.84.8a.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S3.E4.m1.49.49.49.18.18.18" xref="S3.E4.m1.84.84.8a.cmml">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    id="S3.E4.m1.92.92.16g" xref="S3.E4.m1.84.84.8.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E4.m1.92.92.16i" xref="S3.E4.m1.84.84.8.cmml"><mrow
    id="S3.E4.m1.92.92.16.84.31.31" xref="S3.E4.m1.84.84.8.cmml"><mo id="S3.E4.m1.92.92.16.84.31.31a"
    xref="S3.E4.m1.84.84.8a.cmml">+</mo><mrow id="S3.E4.m1.92.92.16.84.31.31.31" xref="S3.E4.m1.84.84.8.cmml"><mi
    id="S3.E4.m1.51.51.51.2.2.2" xref="S3.E4.m1.51.51.51.2.2.2.cmml">λ</mi><mo lspace="0em"
    rspace="0em" id="S3.E4.m1.92.92.16.84.31.31.31.5" xref="S3.E4.m1.84.84.8a.cmml">​</mo><mrow
    id="S3.E4.m1.92.92.16.84.31.31.31.4" xref="S3.E4.m1.84.84.8.cmml"><munderover
    id="S3.E4.m1.92.92.16.84.31.31.31.4.5" xref="S3.E4.m1.84.84.8.cmml"><mo movablelimits="false"
    rspace="0em" id="S3.E4.m1.52.52.52.3.3.3" xref="S3.E4.m1.52.52.52.3.3.3.cmml">∑</mo><mrow
    id="S3.E4.m1.53.53.53.4.4.4.1" xref="S3.E4.m1.53.53.53.4.4.4.1.cmml"><mi id="S3.E4.m1.53.53.53.4.4.4.1.2"
    xref="S3.E4.m1.53.53.53.4.4.4.1.2.cmml">i</mi><mo id="S3.E4.m1.53.53.53.4.4.4.1.1"
    xref="S3.E4.m1.53.53.53.4.4.4.1.1.cmml">=</mo><mn id="S3.E4.m1.53.53.53.4.4.4.1.3"
    xref="S3.E4.m1.53.53.53.4.4.4.1.3.cmml">1</mn></mrow><mi id="S3.E4.m1.54.54.54.5.5.5.1"
    xref="S3.E4.m1.54.54.54.5.5.5.1.cmml">n</mi></munderover><mrow id="S3.E4.m1.92.92.16.84.31.31.31.4.4"
    xref="S3.E4.m1.84.84.8.cmml"><munderover id="S3.E4.m1.92.92.16.84.31.31.31.4.4.5"
    xref="S3.E4.m1.84.84.8.cmml"><mo movablelimits="false" id="S3.E4.m1.55.55.55.6.6.6"
    xref="S3.E4.m1.55.55.55.6.6.6.cmml">∑</mo><mrow id="S3.E4.m1.56.56.56.7.7.7.1"
    xref="S3.E4.m1.56.56.56.7.7.7.1.cmml"><mi id="S3.E4.m1.56.56.56.7.7.7.1.2" xref="S3.E4.m1.56.56.56.7.7.7.1.2.cmml">k</mi><mo
    id="S3.E4.m1.56.56.56.7.7.7.1.1" xref="S3.E4.m1.56.56.56.7.7.7.1.1.cmml">=</mo><mn
    id="S3.E4.m1.56.56.56.7.7.7.1.3" xref="S3.E4.m1.56.56.56.7.7.7.1.3.cmml">1</mn></mrow><mi
    id="S3.E4.m1.57.57.57.8.8.8.1" xref="S3.E4.m1.57.57.57.8.8.8.1.cmml">K</mi></munderover><mrow
    id="S3.E4.m1.92.92.16.84.31.31.31.4.4.4" xref="S3.E4.m1.84.84.8.cmml"><msub id="S3.E4.m1.92.92.16.84.31.31.31.4.4.4.6"
    xref="S3.E4.m1.84.84.8.cmml"><mi id="S3.E4.m1.58.58.58.9.9.9" xref="S3.E4.m1.58.58.58.9.9.9.cmml">g</mi><mi
    id="S3.E4.m1.59.59.59.10.10.10.1" xref="S3.E4.m1.59.59.59.10.10.10.1.cmml">k</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.92.92.16.84.31.31.31.4.4.4.5" xref="S3.E4.m1.84.84.8a.cmml">​</mo><mrow
    id="S3.E4.m1.90.90.14.82.29.29.29.2.2.2.2.2" xref="S3.E4.m1.84.84.8.cmml"><mo
    stretchy="false" id="S3.E4.m1.60.60.60.11.11.11" xref="S3.E4.m1.84.84.8a.cmml">(</mo><msub
    id="S3.E4.m1.89.89.13.81.28.28.28.1.1.1.1.1.1" xref="S3.E4.m1.84.84.8.cmml"><mi
    id="S3.E4.m1.61.61.61.12.12.12" xref="S3.E4.m1.61.61.61.12.12.12.cmml">x</mi><mi
    id="S3.E4.m1.62.62.62.13.13.13.1" xref="S3.E4.m1.62.62.62.13.13.13.1.cmml">i</mi></msub><mo
    id="S3.E4.m1.63.63.63.14.14.14" xref="S3.E4.m1.84.84.8a.cmml">,</mo><msub id="S3.E4.m1.90.90.14.82.29.29.29.2.2.2.2.2.2"
    xref="S3.E4.m1.84.84.8.cmml"><mi id="S3.E4.m1.64.64.64.15.15.15" xref="S3.E4.m1.64.64.64.15.15.15.cmml">z</mi><mi
    id="S3.E4.m1.65.65.65.16.16.16.1" xref="S3.E4.m1.65.65.65.16.16.16.1.cmml">i</mi></msub><mo
    stretchy="false" id="S3.E4.m1.66.66.66.17.17.17" xref="S3.E4.m1.84.84.8a.cmml">)</mo></mrow><mo
    lspace="0.167em" rspace="0em" id="S3.E4.m1.92.92.16.84.31.31.31.4.4.4.5a" xref="S3.E4.m1.84.84.8a.cmml">​</mo><mrow
    id="S3.E4.m1.92.92.16.84.31.31.31.4.4.4.7" xref="S3.E4.m1.84.84.8.cmml"><mi id="S3.E4.m1.67.67.67.18.18.18"
    xref="S3.E4.m1.67.67.67.18.18.18.cmml">log</mi><mo lspace="0.167em" id="S3.E4.m1.92.92.16.84.31.31.31.4.4.4.7a"
    xref="S3.E4.m1.84.84.8a.cmml">⁡</mo><msub id="S3.E4.m1.92.92.16.84.31.31.31.4.4.4.7.1"
    xref="S3.E4.m1.84.84.8.cmml"><mi id="S3.E4.m1.68.68.68.19.19.19" xref="S3.E4.m1.68.68.68.19.19.19.cmml">g</mi><mi
    id="S3.E4.m1.69.69.69.20.20.20.1" xref="S3.E4.m1.69.69.69.20.20.20.1.cmml">k</mi></msub></mrow><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.92.92.16.84.31.31.31.4.4.4.5b" xref="S3.E4.m1.84.84.8a.cmml">​</mo><mrow
    id="S3.E4.m1.92.92.16.84.31.31.31.4.4.4.4.2" xref="S3.E4.m1.84.84.8.cmml"><mo
    stretchy="false" id="S3.E4.m1.70.70.70.21.21.21" xref="S3.E4.m1.84.84.8a.cmml">(</mo><msub
    id="S3.E4.m1.91.91.15.83.30.30.30.3.3.3.3.1.1" xref="S3.E4.m1.84.84.8.cmml"><mi
    id="S3.E4.m1.71.71.71.22.22.22" xref="S3.E4.m1.71.71.71.22.22.22.cmml">x</mi><mi
    id="S3.E4.m1.72.72.72.23.23.23.1" xref="S3.E4.m1.72.72.72.23.23.23.1.cmml">i</mi></msub><mo
    id="S3.E4.m1.73.73.73.24.24.24" xref="S3.E4.m1.84.84.8a.cmml">,</mo><msub id="S3.E4.m1.92.92.16.84.31.31.31.4.4.4.4.2.2"
    xref="S3.E4.m1.84.84.8.cmml"><mi id="S3.E4.m1.74.74.74.25.25.25" xref="S3.E4.m1.74.74.74.25.25.25.cmml">z</mi><mi
    id="S3.E4.m1.75.75.75.26.26.26.1" xref="S3.E4.m1.75.75.75.26.26.26.1.cmml">i</mi></msub><mo
    stretchy="false" id="S3.E4.m1.76.76.76.27.27.27" xref="S3.E4.m1.84.84.8a.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S3.E4.m1.92b"><apply id="S3.E4.m1.84.84.8.cmml"
    xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.84.84.8b.cmml" xref="S3.E4.m1.92.92.16"><apply
    id="S3.E4.m1.77.77.1.1.cmml" xref="S3.E4.m1.92.92.16"><ci id="S3.E4.m1.1.1.1.1.1.1.cmml"
    xref="S3.E4.m1.1.1.1.1.1.1">𝐶</ci><vector id="S3.E4.m1.77.77.1.1.1.2.cmml" xref="S3.E4.m1.92.92.16"><ci
    id="S3.E4.m1.3.3.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.3.3">𝜃</ci><ci id="S3.E4.m1.5.5.5.5.5.5.cmml"
    xref="S3.E4.m1.5.5.5.5.5.5">𝜆</ci><apply id="S3.E4.m1.77.77.1.1.1.1.1.cmml" xref="S3.E4.m1.92.92.16"><csymbol
    cd="ambiguous" id="S3.E4.m1.77.77.1.1.1.1.1.1.cmml" xref="S3.E4.m1.92.92.16">subscript</csymbol><ci
    id="S3.E4.m1.7.7.7.7.7.7.cmml" xref="S3.E4.m1.7.7.7.7.7.7">ℒ</ci><ci id="S3.E4.m1.8.8.8.8.8.8.1.cmml"
    xref="S3.E4.m1.8.8.8.8.8.8.1">𝑛</ci></apply></vector></apply><apply id="S3.E4.m1.79.79.3.3.cmml"
    xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.78.78.2.2.1.cmml" xref="S3.E4.m1.92.92.16"><ci
    id="S3.E4.m1.11.11.11.11.2.2.cmml" xref="S3.E4.m1.11.11.11.11.2.2">𝐿</ci><list
    id="S3.E4.m1.78.78.2.2.1.1.2.cmml" xref="S3.E4.m1.92.92.16"><ci id="S3.E4.m1.13.13.13.13.4.4.cmml"
    xref="S3.E4.m1.13.13.13.13.4.4">𝜃</ci><apply id="S3.E4.m1.78.78.2.2.1.1.1.1.cmml"
    xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous" id="S3.E4.m1.78.78.2.2.1.1.1.1.1.cmml"
    xref="S3.E4.m1.92.92.16">subscript</csymbol><ci id="S3.E4.m1.15.15.15.15.6.6.cmml"
    xref="S3.E4.m1.15.15.15.15.6.6">ℒ</ci><ci id="S3.E4.m1.16.16.16.16.7.7.1.cmml"
    xref="S3.E4.m1.16.16.16.16.7.7.1">𝑛</ci></apply></list></apply><apply id="S3.E4.m1.79.79.3.3.2.cmml"
    xref="S3.E4.m1.92.92.16"><ci id="S3.E4.m1.19.19.19.19.10.10.cmml" xref="S3.E4.m1.19.19.19.19.10.10">𝜆</ci><apply
    id="S3.E4.m1.79.79.3.3.2.4.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous"
    id="S3.E4.m1.79.79.3.3.2.4.1.cmml" xref="S3.E4.m1.85.85.9.77.32.10.11">subscript</csymbol><ci
    id="S3.E4.m1.20.20.20.20.11.11.cmml" xref="S3.E4.m1.20.20.20.20.11.11">𝐻</ci><apply
    id="S3.E4.m1.21.21.21.21.12.12.1.cmml" xref="S3.E4.m1.21.21.21.21.12.12.1"><ci
    id="S3.E4.m1.21.21.21.21.12.12.1.2.cmml" xref="S3.E4.m1.21.21.21.21.12.12.1.2">𝑒</ci><ci
    id="S3.E4.m1.21.21.21.21.12.12.1.3.cmml" xref="S3.E4.m1.21.21.21.21.12.12.1.3">𝑚</ci><ci
    id="S3.E4.m1.21.21.21.21.12.12.1.4.cmml" xref="S3.E4.m1.21.21.21.21.12.12.1.4">𝑝</ci></apply></apply><apply
    id="S3.E4.m1.79.79.3.3.2.1.1.1.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="latexml"
    id="S3.E4.m1.24.24.24.24.15.15.cmml" xref="S3.E4.m1.24.24.24.24.15.15">conditional</csymbol><ci
    id="S3.E4.m1.23.23.23.23.14.14.cmml" xref="S3.E4.m1.23.23.23.23.14.14">𝑌</ci><list
    id="S3.E4.m1.79.79.3.3.2.1.1.1.1.2.cmml" xref="S3.E4.m1.92.92.16"><ci id="S3.E4.m1.25.25.25.25.16.16.cmml"
    xref="S3.E4.m1.25.25.25.25.16.16">𝑋</ci><ci id="S3.E4.m1.27.27.27.27.18.18.cmml"
    xref="S3.E4.m1.27.27.27.27.18.18">𝑍</ci><apply id="S3.E4.m1.79.79.3.3.2.1.1.1.1.1.1.cmml"
    xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous" id="S3.E4.m1.79.79.3.3.2.1.1.1.1.1.1.1.cmml"
    xref="S3.E4.m1.92.92.16">subscript</csymbol><ci id="S3.E4.m1.29.29.29.29.20.20.cmml"
    xref="S3.E4.m1.29.29.29.29.20.20">ℒ</ci><ci id="S3.E4.m1.30.30.30.30.21.21.1.cmml"
    xref="S3.E4.m1.30.30.30.30.21.21.1">𝑛</ci></apply></list></apply></apply></apply></apply><apply
    id="S3.E4.m1.84.84.8c.cmml" xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.84.84.8.8.cmml"
    xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.80.80.4.4.1.cmml" xref="S3.E4.m1.92.92.16"><apply
    id="S3.E4.m1.80.80.4.4.1.2.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous"
    id="S3.E4.m1.80.80.4.4.1.2.1.cmml" xref="S3.E4.m1.85.85.9.77.32.10.11">superscript</csymbol><apply
    id="S3.E4.m1.80.80.4.4.1.2.2.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous"
    id="S3.E4.m1.80.80.4.4.1.2.2.1.cmml" xref="S3.E4.m1.85.85.9.77.32.10.11">subscript</csymbol><apply
    id="S3.E4.m1.34.34.34.3.3.3.1.cmml" xref="S3.E4.m1.34.34.34.3.3.3.1"><ci id="S3.E4.m1.34.34.34.3.3.3.1.2.cmml"
    xref="S3.E4.m1.34.34.34.3.3.3.1.2">𝑖</ci><cn type="integer" id="S3.E4.m1.34.34.34.3.3.3.1.3.cmml"
    xref="S3.E4.m1.34.34.34.3.3.3.1.3">1</cn></apply></apply><ci id="S3.E4.m1.35.35.35.4.4.4.1.cmml"
    xref="S3.E4.m1.35.35.35.4.4.4.1">𝑛</ci></apply><apply id="S3.E4.m1.80.80.4.4.1.1.2.cmml"
    xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.80.80.4.4.1.1.1.1.1.cmml" xref="S3.E4.m1.92.92.16"><apply
    id="S3.E4.m1.80.80.4.4.1.1.1.1.1.2.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous"
    id="S3.E4.m1.80.80.4.4.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.92.92.16">superscript</csymbol><apply
    id="S3.E4.m1.80.80.4.4.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous"
    id="S3.E4.m1.80.80.4.4.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.92.92.16">subscript</csymbol><apply
    id="S3.E4.m1.39.39.39.8.8.8.1.cmml" xref="S3.E4.m1.39.39.39.8.8.8.1"><ci id="S3.E4.m1.39.39.39.8.8.8.1.2.cmml"
    xref="S3.E4.m1.39.39.39.8.8.8.1.2">𝑘</ci><cn type="integer" id="S3.E4.m1.39.39.39.8.8.8.1.3.cmml"
    xref="S3.E4.m1.39.39.39.8.8.8.1.3">1</cn></apply></apply><ci id="S3.E4.m1.40.40.40.9.9.9.1.cmml"
    xref="S3.E4.m1.40.40.40.9.9.9.1">𝐾</ci></apply><apply id="S3.E4.m1.80.80.4.4.1.1.1.1.1.1.cmml"
    xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.80.80.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.92.92.16"><csymbol
    cd="ambiguous" id="S3.E4.m1.80.80.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.92.92.16">subscript</csymbol><ci
    id="S3.E4.m1.41.41.41.10.10.10.cmml" xref="S3.E4.m1.41.41.41.10.10.10">𝑧</ci><apply
    id="S3.E4.m1.42.42.42.11.11.11.1.cmml" xref="S3.E4.m1.42.42.42.11.11.11.1"><ci
    id="S3.E4.m1.42.42.42.11.11.11.1.2.cmml" xref="S3.E4.m1.42.42.42.11.11.11.1.2">𝑖</ci><ci
    id="S3.E4.m1.42.42.42.11.11.11.1.3.cmml" xref="S3.E4.m1.42.42.42.11.11.11.1.3">𝑘</ci></apply></apply><apply
    id="S3.E4.m1.80.80.4.4.1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous"
    id="S3.E4.m1.80.80.4.4.1.1.1.1.1.1.4.1.cmml" xref="S3.E4.m1.92.92.16">subscript</csymbol><ci
    id="S3.E4.m1.43.43.43.12.12.12.cmml" xref="S3.E4.m1.43.43.43.12.12.12">𝑓</ci><ci
    id="S3.E4.m1.44.44.44.13.13.13.1.cmml" xref="S3.E4.m1.44.44.44.13.13.13.1">𝑘</ci></apply><apply
    id="S3.E4.m1.80.80.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.92.92.16"><csymbol
    cd="ambiguous" id="S3.E4.m1.80.80.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.92.92.16">subscript</csymbol><ci
    id="S3.E4.m1.46.46.46.15.15.15.cmml" xref="S3.E4.m1.46.46.46.15.15.15">𝑥</ci><ci
    id="S3.E4.m1.47.47.47.16.16.16.1.cmml" xref="S3.E4.m1.47.47.47.16.16.16.1">𝑖</ci></apply></apply></apply></apply></apply><apply
    id="S3.E4.m1.84.84.8.8.5.cmml" xref="S3.E4.m1.92.92.16"><ci id="S3.E4.m1.51.51.51.2.2.2.cmml"
    xref="S3.E4.m1.51.51.51.2.2.2">𝜆</ci><apply id="S3.E4.m1.84.84.8.8.5.4.cmml" xref="S3.E4.m1.92.92.16"><apply
    id="S3.E4.m1.84.84.8.8.5.4.5.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous"
    id="S3.E4.m1.84.84.8.8.5.4.5.1.cmml" xref="S3.E4.m1.85.85.9.77.32.10.11">superscript</csymbol><apply
    id="S3.E4.m1.84.84.8.8.5.4.5.2.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous"
    id="S3.E4.m1.84.84.8.8.5.4.5.2.1.cmml" xref="S3.E4.m1.85.85.9.77.32.10.11">subscript</csymbol><apply
    id="S3.E4.m1.53.53.53.4.4.4.1.cmml" xref="S3.E4.m1.53.53.53.4.4.4.1"><ci id="S3.E4.m1.53.53.53.4.4.4.1.2.cmml"
    xref="S3.E4.m1.53.53.53.4.4.4.1.2">𝑖</ci><cn type="integer" id="S3.E4.m1.53.53.53.4.4.4.1.3.cmml"
    xref="S3.E4.m1.53.53.53.4.4.4.1.3">1</cn></apply></apply><ci id="S3.E4.m1.54.54.54.5.5.5.1.cmml"
    xref="S3.E4.m1.54.54.54.5.5.5.1">𝑛</ci></apply><apply id="S3.E4.m1.84.84.8.8.5.4.4.cmml"
    xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.84.84.8.8.5.4.4.5.cmml" xref="S3.E4.m1.92.92.16"><csymbol
    cd="ambiguous" id="S3.E4.m1.84.84.8.8.5.4.4.5.1.cmml" xref="S3.E4.m1.85.85.9.77.32.10.11">superscript</csymbol><apply
    id="S3.E4.m1.84.84.8.8.5.4.4.5.2.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous"
    id="S3.E4.m1.84.84.8.8.5.4.4.5.2.1.cmml" xref="S3.E4.m1.85.85.9.77.32.10.11">subscript</csymbol><apply
    id="S3.E4.m1.56.56.56.7.7.7.1.cmml" xref="S3.E4.m1.56.56.56.7.7.7.1"><ci id="S3.E4.m1.56.56.56.7.7.7.1.2.cmml"
    xref="S3.E4.m1.56.56.56.7.7.7.1.2">𝑘</ci><cn type="integer" id="S3.E4.m1.56.56.56.7.7.7.1.3.cmml"
    xref="S3.E4.m1.56.56.56.7.7.7.1.3">1</cn></apply></apply><ci id="S3.E4.m1.57.57.57.8.8.8.1.cmml"
    xref="S3.E4.m1.57.57.57.8.8.8.1">𝐾</ci></apply><apply id="S3.E4.m1.84.84.8.8.5.4.4.4.cmml"
    xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.84.84.8.8.5.4.4.4.6.cmml" xref="S3.E4.m1.92.92.16"><csymbol
    cd="ambiguous" id="S3.E4.m1.84.84.8.8.5.4.4.4.6.1.cmml" xref="S3.E4.m1.85.85.9.77.32.10.11">subscript</csymbol><ci
    id="S3.E4.m1.58.58.58.9.9.9.cmml" xref="S3.E4.m1.58.58.58.9.9.9">𝑔</ci><ci id="S3.E4.m1.59.59.59.10.10.10.1.cmml"
    xref="S3.E4.m1.59.59.59.10.10.10.1">𝑘</ci></apply><interval closure="open" id="S3.E4.m1.82.82.6.6.3.2.2.2.2.3.cmml"
    xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.81.81.5.5.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.92.92.16"><csymbol
    cd="ambiguous" id="S3.E4.m1.81.81.5.5.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.92.92.16">subscript</csymbol><ci
    id="S3.E4.m1.61.61.61.12.12.12.cmml" xref="S3.E4.m1.61.61.61.12.12.12">𝑥</ci><ci
    id="S3.E4.m1.62.62.62.13.13.13.1.cmml" xref="S3.E4.m1.62.62.62.13.13.13.1">𝑖</ci></apply><apply
    id="S3.E4.m1.82.82.6.6.3.2.2.2.2.2.2.cmml" xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous"
    id="S3.E4.m1.82.82.6.6.3.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.92.92.16">subscript</csymbol><ci
    id="S3.E4.m1.64.64.64.15.15.15.cmml" xref="S3.E4.m1.64.64.64.15.15.15">𝑧</ci><ci
    id="S3.E4.m1.65.65.65.16.16.16.1.cmml" xref="S3.E4.m1.65.65.65.16.16.16.1">𝑖</ci></apply></interval><apply
    id="S3.E4.m1.84.84.8.8.5.4.4.4.7.cmml" xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.84.84.8.8.5.4.4.4.7.2.cmml"
    xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous" id="S3.E4.m1.84.84.8.8.5.4.4.4.7.2.1.cmml"
    xref="S3.E4.m1.85.85.9.77.32.10.11">subscript</csymbol><ci id="S3.E4.m1.68.68.68.19.19.19.cmml"
    xref="S3.E4.m1.68.68.68.19.19.19">𝑔</ci><ci id="S3.E4.m1.69.69.69.20.20.20.1.cmml"
    xref="S3.E4.m1.69.69.69.20.20.20.1">𝑘</ci></apply></apply><interval closure="open"
    id="S3.E4.m1.84.84.8.8.5.4.4.4.4.3.cmml" xref="S3.E4.m1.92.92.16"><apply id="S3.E4.m1.83.83.7.7.4.3.3.3.3.1.1.cmml"
    xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous" id="S3.E4.m1.83.83.7.7.4.3.3.3.3.1.1.1.cmml"
    xref="S3.E4.m1.92.92.16">subscript</csymbol><ci id="S3.E4.m1.71.71.71.22.22.22.cmml"
    xref="S3.E4.m1.71.71.71.22.22.22">𝑥</ci><ci id="S3.E4.m1.72.72.72.23.23.23.1.cmml"
    xref="S3.E4.m1.72.72.72.23.23.23.1">𝑖</ci></apply><apply id="S3.E4.m1.84.84.8.8.5.4.4.4.4.2.2.cmml"
    xref="S3.E4.m1.92.92.16"><csymbol cd="ambiguous" id="S3.E4.m1.84.84.8.8.5.4.4.4.4.2.2.1.cmml"
    xref="S3.E4.m1.92.92.16">subscript</csymbol><ci id="S3.E4.m1.74.74.74.25.25.25.cmml"
    xref="S3.E4.m1.74.74.74.25.25.25">𝑧</ci><ci id="S3.E4.m1.75.75.75.26.26.26.1.cmml"
    xref="S3.E4.m1.75.75.75.26.26.26.1">𝑖</ci></apply></interval></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E4.m1.92c">\begin{split}C(\theta,\lambda;\mathcal{L}_{n})&=L(\theta;\mathcal{L}_{n})-\lambda
    H_{emp}(Y&#124;X,Z;\mathcal{L}_{n})\\ &=\sum_{i=1}^{n}\log(\sum_{k=1}^{K}z_{ik}f_{k}(x_{i}))\\
    &+\lambda\sum_{i=1}^{n}\sum_{k=1}^{K}g_{k}(x_{i},z_{i})\log g_{k}(x_{i},z_{i})\end{split}</annotation></semantics></math>
    |  | (4) |'
  prefs: []
  type: TYPE_NORMAL
- en: 'where $L(\theta;\mathcal{L}_{n})$ is the conditional log-likelihood and sensitive
    to the labeled data and $H_{emp}(Y|X,Z;\mathcal{L}_{n})$ is conditional entropy
    and only affected by the unlabeled data which works to minimize the class overlap.
    $x_{i}$ and $z_{i}$ represent inputs and corresponding labels. If $x_{i}$ is labeled
    $\omega_{k}$, then $z_{ik}=1$ and $z_{il}=0$ for $l\neq k$ ; if $X_{i}$ is unlabeled,
    then $z_{il}=1$ for $l=1...k$. $f_{k}(x_{i})$ and $g_{k}(x_{i},z_{i})$ denote
    the model of $P(\omega_{k}|x_{i})$ and the model of $P(\omega_{k}|x_{i},z_{i})$.
    Wu et al. [[105](#bib.bib105)] add entropy minimization technique in the student
    branch. Berthelot et al. [[146](#bib.bib146)] propose MixMatch to use a sharpening
    function on the target distribution of unlabeled data to minimize the entropy.
    The sharpening through adjusting the “temperature” of this categorical distribution
    is as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}Sharpen(p,T)_{i}=p_{i}^{\frac{1}{T}}/\sum_{j=1}^{L}p_{j}^{\frac{1}{T}}\end{split}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $p$ is input categorical distribution and $T$ is a hyperparameter. As
    $T\rightarrow{}$ 0 , the output of $Sharpen(p,T)$ will approach a Dirac (“one-hot”)
    distribution. Lowering temperature encourages model to produce lower-entropy predictions.
    However, the hyperparameter needs to be set carefully and different samples may
    have different $T$, so [[2](#bib.bib2)] propose an adaptive sharpening which can
    adjust T adaptively for each sample according to its uncertainty predicted by
    the model. [[159](#bib.bib159)] introduce a mutual exclusivity loss for multi-class
    problems that explicitly forces the predictions to be mutually exclusive and encourages
    the decision boundary to lie on the low density space between the manifolds corresponding
    to different classes of data, which has a better performance in object detection
    task compared with entropy minimization in [[154](#bib.bib154)].
  prefs: []
  type: TYPE_NORMAL
- en: Another application of entropy minimization is the use of hard label in the
    pseudo labeling. As argmax operation applied to a probability distribution can
    produce a valid “one-hot” low-entropy (i.e., high-confidence) distribution, both
    the entropy minimization and pseudo labeling encourages the decision boundary
    passing low-density regions. Therefore, the strategy of using hard label in the
    pseudo labeling is closely related with entropy minimization [[160](#bib.bib160)].
    However, a high capacity model that tends to overfit quickly can give high-confidence
    predictions which also have low entropy [[161](#bib.bib161)]. Therefore, entropy
    minimization doesn’t work in some cases [[154](#bib.bib154)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The summarized review of semi-supervised medical image segmentation
    methods with adversarial learning and entropy minimization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | 2D/3D | Modality | Dataset | Highlights | Class |'
  prefs: []
  type: TYPE_TB
- en: '| CAFD, Wu et al.[[134](#bib.bib134)] | 2D | Colonoscope | Kvasir-SEG [[196](#bib.bib196)],
    CVC-Clinic DB [[188](#bib.bib188)] | Introduce collaborative and adversarial learning
    of focused and dispersive representations | Adversarial learning |'
  prefs: []
  type: TYPE_TB
- en: '| SSTD-Aug, Chaitanya et al. [[117](#bib.bib117)] | 2D | MRI | ACDC dataset
    [[22](#bib.bib22)] | Task-driven data augmentation method to synthesize new training
    examples | Adversarial learning |'
  prefs: []
  type: TYPE_TB
- en: '| DAN, Zhang et al.[[45](#bib.bib45)] | 3D | Microscopy | Gland Segmentation
    Challenge dataset [[189](#bib.bib189)] | Introduce adversarial learning to encourage
    the segmentation output of unlabeled data to be similar with the annotations of
    labeled data. | Adversarial learning |'
  prefs: []
  type: TYPE_TB
- en: '| GAVA, Li et al.[[125](#bib.bib125)] | 2D | MRI | M&Ms dataset [[123](#bib.bib123)]
    | Employ U-net as encoder and conditional GAN as decoder | Adversarial learning
    |'
  prefs: []
  type: TYPE_TB
- en: '| LeakGAN_ssl, Hou et al.[[144](#bib.bib144)] | 2D | Fundus | DRIVE[[190](#bib.bib190)],
    STARE[[191](#bib.bib191)], CHASE_DB1[[192](#bib.bib192)] | Add a leaking GAN to
    pollute the discriminator by leaking information from the generator for more moderate
    generations | Adversarial learning |'
  prefs: []
  type: TYPE_TB
- en: '| LG-ER-MT, Hang et al.[[33](#bib.bib33)] | 3D | MRI | LA dataset [[66](#bib.bib66)]
    | Introduce the entropy minimization principle to the student network | Entropy
    minimization |'
  prefs: []
  type: TYPE_TB
- en: '| MC-Net, Wu et al.[[2](#bib.bib2)] | 3D | MRI | LA dataset [[66](#bib.bib66)]
    | Adjust sharpening temperature adaptively according to the uncertainty predicted
    by the model | Entropy minimization |'
  prefs: []
  type: TYPE_TB
- en: 3.3 Semi-Supervised Medical Image Segmentation with Knowledge Priors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Knowledge priors are the information that a learner already has before it learns
    new information, and sometimes are helpful for dealing with new tasks. Compared
    with non-medical images, medical images have many anatomical priors such as the
    shape and position of organs and incorporating the anatomical prior knowledge
    in deep learning can improve the performance for medical image segmentation [[153](#bib.bib153)].
    Some semi-supervised algorithms utilize knowledge priors to improve the representation
    ability for new tasks. While knowledge priors can be helpful in semi-supervised
    medical image segmentation, there are also several limitations to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Overfitting: If the prior knowledge is too specific to the training data,
    it may lead to overfitting, where the model performs well on the training data
    but poorly on new, unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Non-differentiable: Some complex priors, such as region connectivity, convexity
    and symmetry are usually non-differentiable and complex losses need to be designed.
    In this part, we categorize the knowledge priors as self-supervised tasks and
    anatomical priors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/31f7f9d158186cd48db3544ea363c547.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: DPA-DenseBiasNet [[152](#bib.bib152)] for fine renal artery segmentation.
    An auto-encoder of a reconstruction task is trained in stage 1 process. The deep
    prior anatomy (DPA) features extracted from the encoder, which contain representations
    of anatomy priors, are then embedded for the downstream segmentation task in stage
    2 process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-supervised tasks which employs large unlabeled data to train networks
    can provide useful representations and visual priors. An important role is to
    pretrain networks and provide better starting points for target tasks. For example,
    huang et al. [[99](#bib.bib99)] add a reconstruction pre-training from the counterparts
    to avoid networks being randomly initialized in a cold start stage. Wang et al.
    [[131](#bib.bib131)] use superpixels to separate an image into regions and learned
    intra- and inter-organ representation based on contrastive learning, then the
    model is used to initialize the semi-supervised framework, which boost the performance
    significantly. Self-supervised tasks can also be trained jointly with target semi-supervised
    tasks as regularization. Contrastive learning are the most popular methods to
    integrate with semi-supervised framework. For example, hu et al. [[98](#bib.bib98)]
    introduce the self-supervised image-level and pixel-level contrastive learning
    into the semi-supervised framework. [[80](#bib.bib80)] integrate self-paced contrastive
    learning. Wu et al. [[105](#bib.bib105)] add patch- and pixel-level dense contrastive
    loss to align the features from the teacher and student models. Zhao et al. [[133](#bib.bib133)]
    introduce the multi-scale multi-view global-local contrastive learning into co-training
    framework. However, in contrastive learning, negative samples may come from the
    similar features from anchors, which may confuse models during training. You et
    al. [[199](#bib.bib199)] integrate contrastive learning from a variance-reduction
    perspective, which uses stratified group sampling theory and generalize well in
    long-tail distribution. Except contrastive learning, jigsaw puzzle tasks [[120](#bib.bib120)],
    lesion region inpainting [[3](#bib.bib3)] and reconstruction tasks [[152](#bib.bib152)]
    can also be utilized easily into semi-supervised framework. [[3](#bib.bib3)] propose
    a dual-task network with a shared encoder and two independent decoders for lesion
    region inpainting and segmentation. They also add entropy minimization technique
    in the student branch. He et al. [[152](#bib.bib152)] train an auto-encoder through
    a reconstruction task and the deep prior anatomy (DPA) features extracted from
    it are then embedded for segmenting, as shown in Figure [11](#S3.F11 "Figure 11
    ‣ 3.3 Semi-Supervised Medical Image Segmentation with Knowledge Priors ‣ 3 Related
    Work on Semi-Supervised Medical Image Segmentation ‣ Learning with Limited Annotations:
    A Survey on Deep Semi-supervised Learning for Medical Image Segmentation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2d48bbbeeafff6af33b40139c9ae5d1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Illustration of the framework of $ABS^{3}Net$ [[79](#bib.bib79)]
    with the confidence map. The left is the $ABS^{3}Net$, in which atlas-based pixel
    selection module is introduced to select reliable pixel results based on pixel-wise
    confidence. The right shows the atlas-based confidence map. The high confidence
    (shown in red) represents both probability atlas map (PA) and the segmentation
    probability map $s_{output}$ are close to the prediction mask $s_{mask}$. The
    confidence decreases from red to blue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Anatomical priors include fixed locations, shapes, region sizes and anatomical
    relations and so on. Objects, such as organs, in medical segmentation usually
    have fixed locations and shapes. To take position information and shape prior
    into account, atlas maps are widely applied in medical image segmentation[[157](#bib.bib157),
    [158](#bib.bib158), [153](#bib.bib153), [79](#bib.bib79), [111](#bib.bib111)].
    As shown in Figure [13](#S3.F13 "Figure 13 ‣ 3.3 Semi-Supervised Medical Image
    Segmentation with Knowledge Priors ‣ 3 Related Work on Semi-Supervised Medical
    Image Segmentation ‣ Learning with Limited Annotations: A Survey on Deep Semi-supervised
    Learning for Medical Image Segmentation"), an atlas map, which indicates the probability
    of objects appearing at some location, can be generated as follows. First, annotated
    volumes need to be registered to a referenced volume. Then the probabilistic atlas
    (PA) can be generated through averaging manually masks after deformable of all
    annotated volumes. For example, Zheng et al. [[153](#bib.bib153)] calculate the
    liver PA and predefined the hard pixel samples with the atlas values close to
    0.5\. Huang et al. [[79](#bib.bib79)] utilize PA to give the unlabeled data segmentation
    pixel-wise confidence to select reliable pixel results, as shown in Figure [12](#S3.F12
    "Figure 12 ‣ 3.3 Semi-Supervised Medical Image Segmentation with Knowledge Priors
    ‣ 3 Related Work on Semi-Supervised Medical Image Segmentation ‣ Learning with
    Limited Annotations: A Survey on Deep Semi-supervised Learning for Medical Image
    Segmentation"). The pixel-wise confidence is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}Confidence=\exp(-\frac{(PA-s_{mask})^{2}+(s_{output}-s_{mask})^{2}}{2\sigma^{2}})\end{split}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\begin{split}s_{mask}=[s_{output}+0.5]\end{split}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'where $s_{output}$ and $s_{mask}$ refer to the segmentation probability map
    for the organ to be segmented and the prediction mask of unlabeled data, whose
    value is only 0 or 1\. $[\cdot]$ denotes the integer-valued function. As can be
    seen in Figure [12](#S3.F12 "Figure 12 ‣ 3.3 Semi-Supervised Medical Image Segmentation
    with Knowledge Priors ‣ 3 Related Work on Semi-Supervised Medical Image Segmentation
    ‣ Learning with Limited Annotations: A Survey on Deep Semi-supervised Learning
    for Medical Image Segmentation"), the confidence decreases from red to blue and
    the confidence is higher, when both PA and $s_{output}$ are close to $s_{mask}$,
    that is 0 or 1\. However, segmentation algorithms utilizing atlas maps may be
    unsuitable for targets that have large positional variance. Furthermore, the segmentation
    performance highly relies on accurate registration. Fixed locations and shapes
    are always utilized in organ segmentation whereas anatomical relations can be
    utilized in multi-type pathology segmentation. Anatomical relations represent
    relative locations of different objects. For example, MyoPS-Net [[200](#bib.bib200)]
    uses inclusiveness loss to represent relations between different types of pathologies,
    which constrains the pixels of scars to be included in the pixels of edema. [[217](#bib.bib217)]
    propose magic-cube partition and recovery, encouraging unlabeled images to learn
    organ semantics in relative locations from labeled images. The limitation of this
    magic-cube partition and recovery augmentation is that it may not work well on
    unaligned images. Another assumption is that objects from the same class across
    all the samples share the same anatomical adjacencies, despite their varying region
    geometries, thus an adjacency-graph based auxiliary training loss that penalizes
    outputs with anatomically incorrect region relationships is introduced in [[220](#bib.bib220)].
    For size priors, PaNN[[198](#bib.bib198)] constrains the predicted average distribution
    of organ sizes to be similar with the prior statistics from the labeled dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms mentioned above are typically simple whereas some complex priors,
    such as region connectivity, convexity, symmetry are usually non-differentiable.
    Therefore, specific losses need to be designed for these complex constraints.
    In [[215](#bib.bib215)], an out-of-box and differentiable way to consider complex
    anatomical priors is developed based on reinforce algorithm and adversarial samples.
    Experiments show that clinical-plausible segmentations are obtained. Another work
    in [[219](#bib.bib219)] introduces persistent homology, a concept from topological
    data analysis, to specify the desired topology of segmented objects in terms of
    their Betti numbers and then drive the predictions of unlabeled data to contain
    the specified topological features. The Betti numbers count the number of features
    of some dimension, such as the number of connected components, the number of loops
    or holes, the number of hollow voids and so on. This process does not require
    any ground-truth labels, just prior knowledge of the topology of the structure
    being segmented. The idea of persistent homology can be applicable for segmentation
    of objects with a fixed and regular shape, such as cardiac chambers and myocardium.
  prefs: []
  type: TYPE_NORMAL
- en: Rich knowledge priors make medical image segmentation different from natural
    image segmentation. In semi-supervised medical image segmentation with limited
    labeled data, more accurate and plausible results can be obtained by incorporating
    medical knowledge priors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/636bf48ac389f166ef9a112c41f1c743.png)![Refer to caption](img/5238d36313f4dc1d3438604f077e582a.png)![Refer
    to caption](img/02b2c17fa6c6dd84fe50b87e4cb9e5be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The 3D probabilistic atlas of liver organ [[79](#bib.bib79)], which
    indicates the probability of liver pixels appearing at some location. (a)-(c)
    are superior–inferior, left–right direction and anterior–posterior direction correspondingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: The summarized review of semi-supervised medical image segmentation
    methods with knowledge priors.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | 2D/3D | Modality | Dataset | Highlights | Class |'
  prefs: []
  type: TYPE_TB
- en: '| SepaReg, Wang et al.[[131](#bib.bib131)] | 3D | CT | PDDCA[[193](#bib.bib193)]
    | Initialization with pre-trained model based on intra- and inter-organ contrastive
    learning | Contrastive learning |'
  prefs: []
  type: TYPE_TB
- en: '| S4 ML, Kiyasseh et al.[[122](#bib.bib122)] | 2D | MRI | LA dataset [[66](#bib.bib66)]
    | Using dataform multi centers through meta-learning and contrastive learning
    task performed with unlabelled data | Contrastive learning |'
  prefs: []
  type: TYPE_TB
- en: '| Le-SSCL, Hu et al.[[98](#bib.bib98)] | 3D | CT, MRI | Hippocampus subset
    of Medical Segmentation Decathlon [[185](#bib.bib185)], MM-WHS dataset [[178](#bib.bib178),
    [179](#bib.bib179)] | Self-supervised image-level and supervised pixel-level contrastive
    pre-training | Contrastive learning |'
  prefs: []
  type: TYPE_TB
- en: '| SimCVD, You et al. [[47](#bib.bib47)] | 3D | CT, MRI | LA dataset [[66](#bib.bib66)],
    Pancreas CT [[130](#bib.bib130)] | Contrastive distillation of voxel-wise representation
    with signed distance maps | Contrastive learning |'
  prefs: []
  type: TYPE_TB
- en: '| CPDC, Wu et al.[[105](#bib.bib105)] | 2D | Microscope | DSB[[194](#bib.bib194)]
    , MoNuSeg[[195](#bib.bib195)] | Cross-patch dense contrastive learning framework
    | Contrastive learning |'
  prefs: []
  type: TYPE_TB
- en: '| Dt-DDCL, Zhang et al.[[3](#bib.bib3)] | 2D | Colonoscope | kvasir-SEG dataset[[196](#bib.bib196)],
    Skin lesion dataset [[197](#bib.bib197)] | Dual-task network for segmentation
    and lesion region inpainting. | Inpainting task |'
  prefs: []
  type: TYPE_TB
- en: '| RLS_SSL, Yang et al. [[120](#bib.bib120)] | 3D | OCT | Private | Add self-supervised
    jigsaw puzzle task into training | Jigsaw puzzle task |'
  prefs: []
  type: TYPE_TB
- en: '| MTL-ABS3Net, Huang et al. [[79](#bib.bib79)] | 3D | CT | LiTS dataset [[17](#bib.bib17)]
    | Utilize prior anatomy to give the unlabeled data segmentation pixel-wise confidence
    | Atlas priors |'
  prefs: []
  type: TYPE_TB
- en: '| DAP, Zheng et al. [[153](#bib.bib153)] | 3D | CT | LiTS dataset [[17](#bib.bib17)]
    | Semi-supervised adversarial learning with deep atlas prior | Atlas priors |'
  prefs: []
  type: TYPE_TB
- en: 3.4 Other Semi-Supervised Medical Image Segmentation Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A frequently encountered obstacle in medical imaging is that, in real-world
    applications, the acquired data and annotations may be difficult to meet the assumptions,
    thus affecting the performance of semi-supervised learning. Other than these methodological
    developments for semi-supervised segmentation methods mentioned above, We have
    also compiled some different concerns in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: As there is usually a large amount of unlabeled data in semi-supervised learning,
    the distribution of labeled and unlabeled data may be misaligned. For better leverage
    of large scale data from different distributions or medical centers, some methods
    are proposed to deal with distribution misalignment[[78](#bib.bib78), [88](#bib.bib88),
    [122](#bib.bib122)]. Zhang et al. [[78](#bib.bib78)] try to align labeled data
    distribution and unlabeled data distribution through minimising the L2 distance
    between the feature maps of them. Meanwhile, to remain discriminative for the
    segmentation of labeled and unlabeled data, further segmentation supervision is
    obtained through comparing the non-local semantic relation matrix in feature maps
    from the ground truth label mask and the student inputs. Another work in [[88](#bib.bib88)]
    propose adaptive hierarchical dual consistency to use the dataset from different
    centers, which learns mapping networks adversarially to align the distributions
    and extend consistency learning into intra- and inter-consistency in cross-domain
    segmentation. Another idea for using data from multi centers is through meta-learning.
    In [[122](#bib.bib122)], one distinct task is formulated for each medical centre
    such that a segmentation task is performed for a centre with labelled data while
    the contrastive learning task is performed with unlabelled data.
  prefs: []
  type: TYPE_NORMAL
- en: Another concern in semi-supervised learning is how to fuse different supervision
    signals for label-efficient semi-supervised learning. As existing public imaging
    datasets usually have different annotations for different tasks, like CT images
    singly labelled tumors or partially labelled organs. Zhang et al. [[126](#bib.bib126)]
    propose a dual-path semi-supervised conditional nnU-Net that can be trained on
    a union of partially labelled datasets, segmentation of organs at risk or tumors.
    Another situation is the integration of different levels of supervision signals.
    [[138](#bib.bib138)] propose multi-label deep supervision in semi-supervised framework,
    which leveraged image-level, box-level and pixel-level annotations. If only image-level
    or box-level labels exist, the pseudo labels would be constrained to the classes
    contained in that or to lie within coarse regions. Except that, the noisy pseudo
    labels generated from the teacher model is smoothed using max-pooling to match
    different level predictions from the decoder for multi-level consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Class imbalance is a common problem in segmentation. In semi-supervised learning,
    class imbalance and limited labeled data may further bring the confirmation bias
    and uncertainty imbalance problem. Recently, some researchers propose class-imbalanced
    methods in semi-supervised learning[[212](#bib.bib212), [109](#bib.bib109), [216](#bib.bib216)].
    Lin et al. [[212](#bib.bib212)] propose a dual uncertainty-aware sampling strategy
    to sample low-confident categories of pixels for unsupervised consistency learning.
    Another direction focuses on utilizing re-weighting strategies calculated by the
    pixel proportion of categories[[109](#bib.bib109), [216](#bib.bib216)].
  prefs: []
  type: TYPE_NORMAL
- en: Besides, most of previous semi-supervised frameworks are discriminative models,
    where labeled data is only used in the early training stage and the model may
    tend to overfit to the labeled data [[104](#bib.bib104)]. Wang et al. [[104](#bib.bib104)]
    proposed a Bayesian deep learning framework for semi-supervised segmentation.
    In that way, both labeled and unlabeled data are utilized to estimate the joint
    distribution, which alleviates potential overfitting problem caused by using labeled
    data for early training only.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Analysis of Empirical Results for Semi-Supervised Medical Image Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Common Evaluation Metrics for Medical Image Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For medical image segmentation tasks, Dice Similarity Coefficient (DSC) is
    a widely used metric to measure the region overlap ratio of the ground truth $G$
    and segmentation result $S$. Another similar metric IoU (or Jaccard) is used as
    an alternative for the evaluation. These two metrics are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $DSC=\frac{2&#124;G\cap S&#124;}{&#124;G&#124;+&#124;S&#124;},\quad IoU=\frac{&#124;G\cap
    S&#124;}{&#124;G\cup S&#124;}.$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'However, region-based metrics like DSC cannot well reflect the boundary error
    or small region of mis-segmentation. To issue this limitation, boundary-based
    evaluation metrics like Hausdorff Distance (HD) are applied to focus on the boundary
    distance error defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $HD(\partial G,\partial S)=\max(\max\limits_{x\in\partial G}\min\limits_{y\in\partial
    S}&#124;&#124;x-y&#124;&#124;_{2},\max\limits_{x\in\partial S}\min\limits_{y\in\partial
    G}&#124;&#124;x-y&#124;&#124;_{2}),$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $\partial G$ and $\partial S$ represent the boundary of the ground truth
    and the segmentation result, respectively. To eliminate the influence caused by
    small subsets of outliers, 95% Hausdorff Distance (95HD) is also widely used,
    which is based on the calculation of the 95th percentile of the distances between
    boundary points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Representative works and empirical results on semi-supervised LA MRI
    segmentation benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Highlights | $\mathcal{D}_{L}/\mathcal{D}_{U}$ | Dice | Publication&Year
    |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline V-Net [[40](#bib.bib40)] | Fully supervised baseline with only labeled
    data | 8/0 | 79.99 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 16/0 | 86.03 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Upper-bound V-Net [[40](#bib.bib40)] | Fully supervised upper bound with
    all annotations | 80/0 | 91.14 |  |'
  prefs: []
  type: TYPE_TB
- en: '| UA-MT, Yu et al. [[31](#bib.bib31)] | Teacher-student framework with the
    guidance of uncertainty | 8/72 | 84.25 | MICCAI 2019 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 16/64 | 88.88 |'
  prefs: []
  type: TYPE_TB
- en: '| SASS, Li et al. [[34](#bib.bib34)] | Incorporating signed distance maps for
    shape regularization | 8/72 | 87.32 | MICCAI 2020 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 16/64 | 89.54 |'
  prefs: []
  type: TYPE_TB
- en: '| DUWM, Wang et al. [[32](#bib.bib32)] | Utilizing both segmentation and feature
    uncertainty | 8/72 | 85.91 | MICCAI 2020 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 16/64 | 89.65 |'
  prefs: []
  type: TYPE_TB
- en: '| LG-ER-MT, Hang et al. [[33](#bib.bib33)] | Entropy minimization to produce
    high-confident predictions and local structural consistency to encourage inter-voxel
    similarities | 8/72 | 85.54 | MICCAI 2020 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 16/64 | 89.62 |'
  prefs: []
  type: TYPE_TB
- en: '| DTC, Luo et al. [[35](#bib.bib35)] | Encourage the consistency between output
    segmentation maps and signed distance map | 16/64 | 89.42 | AAAI 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| PDC-Net, Hao et al. [[46](#bib.bib46)] | Parameter decoupling to encourage
    consistent predictions from two branch network | 8/72 | 86.55 | ICMV 2021 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 16/64 | 89.76 |'
  prefs: []
  type: TYPE_TB
- en: '| HCR-MT, Li et al. [[85](#bib.bib85)] | Teacher-student framework with multi-scale
    deep supervision and hierarchical consistency regularization | 16/64 | 90.04 |
    EMBC 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| DTML, Zhang et al. [[36](#bib.bib36)] | Mutual learning of dual-task networks
    for generating segmentation and signed distance maps | 16/64 | 90.12 | PRCV 2021
    |'
  prefs: []
  type: TYPE_TB
- en: '| MC-Net, Wu et al. [[2](#bib.bib2)] | Consistency learning between outputs
    from two different decoders | 8/72 | 87.71 | MICCAI 2021 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 16/64 | 90.34 |'
  prefs: []
  type: TYPE_TB
- en: '| CASS, Liu et al. [[102](#bib.bib102)] | Contrastive consistency on class-level
    | 8/72 | 86.51 | CMIG 2022 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 16/64 | 89.81 |'
  prefs: []
  type: TYPE_TB
- en: '| SimCVD, You et al. [[47](#bib.bib47)] | Contrastive distillation of voxel-wise
    representation with signed distance maps | 8/72 | 89.03 | TMI 2022 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 16/64 | 90.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CMM, Shu et al. [[112](#bib.bib112)] | Asynchronously perform Cross-Mix Teaching
    and transductive monitor for active knowledge distillation | 8/72 | 85.92 | TMM
    2022 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 16/64 | 90.03 |'
  prefs: []
  type: TYPE_TB
- en: '| DTCJL, Chen et al. [[113](#bib.bib113)] | Semi-supervised dual-task consistent
    joint learning framework with task-level regularization | 16/64 | 90.32 | TCBB
    2022 |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Benchmark Datasets for Semi-Supervised Medical Image Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the promising progress in semi-supervised medical image segmentation
    methods, several segmentation benchmarks are also evolved to ensure a fair comparison
    of these methods with the same task setting on same public dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'LA dataset. The LA benchmark dataset [[66](#bib.bib66)] from the Left Atrium
    Segmentation Challenge ²²2http://atriaseg2018.cardiacatlas.org/data/ contains
    100 3D gadolinium-enhanced MR imaging scans (GE-MRIs) for training, with an isotropic
    resolution of $0.625\times 0.625\times 0.625mm^{3}$ . Since the testing set of
    LA does not include public annotations, for the settings in [[31](#bib.bib31)],
    the 100 training scans are splitted into 80 scans for training and 20 scans for
    testing. Out of the 80 training scans, 20% (i.e. 16 scans) are used as labeled
    data and the remaining as unlabeled data. V-Net [[40](#bib.bib40)] is used as
    the network backbone for all experiments with a joint cross-entropy loss and dice
    loss for training. For supervised comparisons, V-Net trained with only labeled
    data (i.e. 16 scans) and trained with all labeled data (i.e. 80 scans) is performed
    as lower bound and upper bound for semi-supervised learning. As shown in Table.
    [6](#S4.T6 "Table 6 ‣ 4.1 Common Evaluation Metrics for Medical Image Segmentation
    ‣ 4 Analysis of Empirical Results for Semi-Supervised Medical Image Segmentation
    ‣ Learning with Limited Annotations: A Survey on Deep Semi-supervised Learning
    for Medical Image Segmentation"), as one of the most popular benchmark dataset
    for semi-supervised medical image segmentation, many methods are further proposed
    and evaluated on the same dataset under the same task settings following the task
    design of [[31](#bib.bib31)]. Specifically, several researches further promote
    the benchmark with 10% (i.e. 8 scans) labeled scans to further evaluate the performance
    under the circumstance with fewer labeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: Pancreas CT dataset. The NIH Pancreas CT segmentation dataset [[130](#bib.bib130)]
    contains 82 3D abdominal contrast-enhanced CT volumes, which are collected from
    53 male and 27 female subjects at the National Institutes of Health Clinical Center
    ³³3https://wiki.cancerimagingarchive.net/display/Public/Pancreas-CT. The dataset
    are collected on Philips and Siemens MDCT scanners and have a fixed resolution
    of $512\times 512$ with varying thicknesses from 1.5 to 2.5 mm, while the axial
    view slice number can vary from 181 to 466\. In [[148](#bib.bib148)], the dataset
    is randomly split into 20 testing cases and 62 training cases. Experimental results
    with 10% labeled training cases (6 labeled and 56 unlabeled) and 20% labeled training
    cases (12 labeled and 50 unlabeled) is reported. Following the pre-processing
    in [[198](#bib.bib198)], the voxel values are clipped to the range of [-125,275]
    Hounsfield Units (HU) and further re-sampled to an isotropic resolution of $1\times
    1\times 1mm^{3}$. several researches further promote the benchmark with 10% (i.e.
    8 scans) labeled scans to further evaluate the performance under the circumstance
    with fewer labeled data. Several semi-supervised approaches [[106](#bib.bib106),
    [127](#bib.bib127), [47](#bib.bib47)] are evaluated on Pancreas CT dataset.
  prefs: []
  type: TYPE_NORMAL
- en: BraTS dataset. The Brain Tumor Segmentation (BraTS) 2019 dataset [[128](#bib.bib128)]
    contains multi-institutional preoperative MRI of 335 glioma patients, where each
    patient has four modalities of MRI scans including T1, T1Gd, T2 and T2-FLAIR with
    neuroradiologist-examined labels. For several existing approaches [[70](#bib.bib70),
    [9](#bib.bib9), [106](#bib.bib106)], T2-FLAIR for whole tumor segmentation is
    used since such modality can better manifest the malignant tumors [[129](#bib.bib129)].
    All the scans are re-sampled to the same resolution of $1\times 1\times 1mm^{3}$
    with intensity normalized to zero mean and unit variance. For semi-supervised
    settings, the dataset is splitted into 250 scans for training, 25 scans for validation
    and the remaining 60 scans for testing. Among the 250 training scans, two different
    settings are performed with 10%/25 and 20%/50 scans as labeled data and the remaining
    scans as unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: ACDC dataset. The ACDC (Automated Cardiac Diagnosis Challenge) dataset [[22](#bib.bib22)]
    was collected from real clinical exams acquired at the University Hospital of
    Dijon ⁴⁴4https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html. The dataset
    contains multi-slice 2D cine cardiac MR imaging samples from 100 patients for
    training. For semi-supervised settings, the dataset is splitted into 70 scans
    for training, 10 scans for validation and 20 scans for testing. Unlike previous
    3D binary segmentation benchmark datasets, ACDC is a 2D multi-class segmentation
    task including RV cavity, myocardium and the LV cavity.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Existing Challenges and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although considerable performance has been achieved for semi-supervised medical
    image segmentation tasks, there are still several open questions for future work.
    In this section, we outline some of these challenges and potential future directions
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Misaligned distribution and class imbalance. As described in Section [4.2](#S4.SS2
    "4.2 Benchmark Datasets for Semi-Supervised Medical Image Segmentation ‣ 4 Analysis
    of Empirical Results for Semi-Supervised Medical Image Segmentation ‣ Learning
    with Limited Annotations: A Survey on Deep Semi-supervised Learning for Medical
    Image Segmentation"), existing semi-supervised medical image segmentation approaches
    have achieved comparable results with upper-bound fully supervised results in
    some benchmark datasets like LA segmentation [[66](#bib.bib66)]. However, these
    benchmarks are relatively "simple" tasks, with small amount of experimental data
    where the training and test set are from the same domain/medical center. However,
    a clinical applicable deep learning model should be generalized suitably across
    multiple centres and scanner vendors from different domains [[123](#bib.bib123)].
    As there is usually a large amount of unlabeled data in semi-supervised learning,
    the distribution of labeled and unlabeled data may be misaligned. This limitation
    is also highlighted by recent semi-supervised medical segmentation benchmarks
    like [[5](#bib.bib5)] and FLARE 22 challenge ⁵⁵5https://flare22.grand-challenge.org.
    Based on the work in [[161](#bib.bib161)], adding unlabeled data from a mismatched
    distribution from labeled data can lower the performance compared to not using
    any unlabeled data. Therefore, it is of great importance to issue the challenge
    of misaligned distribution for semi-supervised learning. As for class imbalance,
    when the training data is highly imbalanced, the trained model will show bias
    towards the majority classes, and may completely ignore the minority classes in
    some extreme cases [[142](#bib.bib142)]. Besides, for semi-supervised multi-class
    segmentation, there usually exists the uncertainty imbalance problem brought by
    class imbalance and limited labeled data. Recent studies [[74](#bib.bib74)] found
    that aleatoric uncertainty derived from the entropy of the predictions may lead
    to sub-optimal results in a multi-class context.'
  prefs: []
  type: TYPE_NORMAL
- en: Methodological analysis. Existing semi-supervised medical image segmentation
    approaches predominantly use unlabeled data to generate constraints, then the
    models are updated with supervised loss for labeled data and unsupervised loss/constraints
    for unlabeled data (or both labeled and unlabeled data). Generally, there is only
    a single weight to balance between supervised and unsupervised loss as described
    in many approaches [[31](#bib.bib31), [35](#bib.bib35), [36](#bib.bib36)]. In
    other words, all the unlabeled data are treated equally for semi-supervised learning.
    However, not all unlabeled data is equally appropriate for the learning procedure
    of the model. For example, when the estimation of an unlabeled case is incorrect,
    training on that particular label-estimate may hurt the overall performance. To
    issue this problem, it is important to encourage the model focusing on more challenging
    areas/cases and therefore exploit more useful information from unlabeled data
    like assigning different weights for each unlabeled example [[71](#bib.bib71)].
    Recent studies [[73](#bib.bib73)] also found that the quality of the perturbations
    is key to obtaining reasonable performances for semi-supervised learning, especially
    in the case of efficient data augmentations or perturbations schemes when the
    data lies in the neighborhood of low-dimensional manifolds.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with other annotation-efficient approaches. For existing semi-supervised
    learning approaches, we still need a small amount of well-annotated labeled data
    to guide the learning of unlabeled data. However, acquiring such fully annotated
    training data can still be costly, especially for the tasks of medical image segmentation.
    To further alleviate the annotation cost, some researches integrate semi-supervised
    learning with other annotation-efficient approaches like utilizing partially labelled
    datasets [[126](#bib.bib126)], leveraging image-level, box-level and pixel-level
    annotations [[138](#bib.bib138)] or scribble supervisions [[139](#bib.bib139)],
    or exploiting noisy labeled data [[140](#bib.bib140)]. Semi-supervised medical
    image segmentation could also be integrated with few-shot segmentation to improve
    the generalization ability with combination strategies to segment similar objects
    in unseen images. Both methods aim to improve the performance of a model when
    there is limited labeled data available. In semi-supervised learning, the model
    learns from both the labeled and unlabeled data by making assumptions about the
    distribution of the data, which is different from few-shot learning. Besides,
    with the recent introduction of SAM [[202](#bib.bib202)], which can serve as pseudo-label
    generator for image segmentation [[203](#bib.bib203), [209](#bib.bib209)], may
    provide some insights into the future development of semi-supervised learning
    for medical image segmentation [[201](#bib.bib201)].
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semi-supervised learning has been widely applied to medical image segmentation
    tasks since it alleviates the heavy burden of acquiring expert-examined annotations
    and takes the advantage of unlabeled data which is much easier to acquire. In
    this survey, we provide a taxonomy of existing deep semi-supervised learning methods
    for medical image segmentation tasks and group these methods into three main categories,
    namely, pseudo labels, unsupervised regularization, and knowledge priors. Other
    than summarizing technical novelties of these approaches, we also analyse and
    discuss the empirical results of these methods on several public benchmark datasets.
    Furthermore, we analysed and discussed the limitations and several unsolved problems
    of existing approaches. We hope this review could inspire the research community
    to explore solutions for this challenge and further promote the developments in
    this impactful area of research.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper was supported in part by the National Science Foundation under Grant
    32000687, and in part by the University Synergy Innovation Program of Anhui Province
    under Grant GXXT-2019-044 and Beijing Natural Science Foundation under Grant Z200024\.
    We also appreciate the efforts of literature collection and code implementations
    of SSL4MIS ⁶⁶6https://github.com/HiLab-git/SSL4MIS and several public benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] B. Van Ginneken, C. M. Schaefer-Prokop, and M. Prokop, “Computer-aided
    diagnosis: how to move from the laboratory to the clinic,” *Radiology*, vol. 261,
    no. 3, pp. 719–732, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y. Wu, M. Xu, Z. Ge, J. Cai, and L. Zhang, “Semi-supervised left atrium
    segmentation with mutual consistency training,” in *International Conference on
    Medical Image Computing and Computer-Assisted Intervention*.   Springer, 2021,
    pp. 297–306.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] R. Zhang, S. Liu, Y. Yu, and G. Li, “Self-supervised correction learning
    for semi-supervised biomedical image segmentation,” in *International Conference
    on Medical Image Computing and Computer-Assisted Intervention*.   Springer, 2021,
    pp. 134–144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, “A survey on deep learning
    in medical image analysis,” *Medical image analysis*, vol. 42, pp. 60–88, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. Ma, Y. Zhang, S. Gu, C. Zhu, C. Ge, Y. Zhang, X. An, C. Wang, Q. Wang,
    X. Liu, S. Cao, Q. Zhang, S. Liu, Y. Wang, Y. Li, J. He, and X. Yang, “Abdomenct-1k:
    Is abdominal organ segmentation a solved problem,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, pp. 1–1, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Lalande, Z. Chen, T. Pommier, T. Decourselle, A. Qayyum, M. Salomon,
    D. Ginhac, Y. Skandarani, A. Boucher, K. Brahim *et al.*, “Deep learning methods
    for automatic evaluation of delayed enhancement-mri. the results of the emidec
    challenge,” *Medical Image Analysis*, vol. 79, p. 102428, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] L. Zhang, X. Wang, D. Yang, T. Sanford, S. Harmon, B. Turkbey, B. J. Wood,
    H. Roth, A. Myronenko, D. Xu *et al.*, “Generalizing deep learning for medical
    image segmentation to unseen domains via deep stacked transformation,” *IEEE transactions
    on medical imaging*, vol. 39, no. 7, pp. 2531–2540, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] M. Z. Alom, C. Yakopcic, M. Hasan, T. M. Taha, and V. K. Asari, “Recurrent
    residual u-net for medical image segmentation,” *Journal of Medical Imaging*,
    vol. 6, no. 1, p. 014006, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Y. Zhang, R. Jiao, Q. Liao, D. Li, and J. Zhang, “Uncertainty-guided mutual
    consistency learning for semi-supervised medical image segmentation,” *Artificial
    Intelligence in Medicine*, vol. 138, p. 102476, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. Ma, “Cutting-edge 3d medical image segmentation methods in 2020: Are
    happy families all alike?” *arXiv preprint arXiv:2101.00232*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille,
    and Y. Zhou, “Transunet: Transformers make strong encoders for medical image segmentation,”
    *arXiv preprint arXiv:2102.04306*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Q. Yao, L. Xiao, P. Liu, and S. K. Zhou, “Label-free segmentation of covid-19
    lesions in lung ct,” *IEEE transactions on medical imaging*, vol. 40, no. 10,
    pp. 2808–2819, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Xie, J. Zhang, C. Shen, and Y. Xia, “Cotr: Efficiently bridging cnn
    and transformer for 3d medical image segmentation,” in *International conference
    on medical image computing and computer-assisted intervention*.   Springer, 2021,
    pp. 171–180.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider, B. A.
    Landman, G. Litjens, B. Menze, O. Ronneberger, R. M. Summers *et al.*, “The medical
    segmentation decathlon,” *Nature Communications*, vol. 13, no. 1, pp. 1–13, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] J. Shiraishi, S. Katsuragawa, J. Ikezoe, T. Matsumoto, T. Kobayashi, K.-i.
    Komatsu, M. Matsui, H. Fujita, Y. Kodera, and K. Doi, “Development of a digital
    image database for chest radiographs with and without a lung nodule: receiver
    operating characteristic analysis of radiologists’ detection of pulmonary nodules,”
    *American Journal of Roentgenology*, vol. 174, no. 1, pp. 71–74, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Ma, Y. Wang, X. An, C. Ge, Z. Yu, J. Chen, Q. Zhu, G. Dong, J. He,
    Z. He *et al.*, “Toward data-efficient learning: A benchmark for covid-19 ct lung
    and infection segmentation,” *Medical physics*, vol. 48, no. 3, pp. 1197–1210,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] P. Bilic, P. Christ, H. B. Li, E. Vorontsov, A. Ben-Cohen, G. Kaissis,
    A. Szeskin, C. Jacobs, G. E. H. Mamani, G. Chartrand *et al.*, “The liver tumor
    segmentation benchmark (lits),” *Medical Image Analysis*, vol. 84, p. 102680,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] E. Bullitt, D. Zeng, G. Gerig, S. Aylward, S. Joshi, J. K. Smith, W. Lin,
    and M. G. Ewend, “Vessel tortuosity and brain tumor malignancy: a blinded study1,”
    *Academic radiology*, vol. 12, no. 10, pp. 1232–1240, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman,
    B. Helba, A. Kalloo, K. Liopyris, M. Marchetti *et al.*, “Skin lesion analysis
    toward melanoma detection 2018: A challenge hosted by the international skin imaging
    collaboration (isic),” *arXiv preprint arXiv:1902.03368*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund,
    B. Haghgoo, R. Ball, K. Shpanskaya *et al.*, “Chexpert: A large chest radiograph
    dataset with uncertainty labels and expert comparison,” in *Proceedings of the
    AAAI conference on artificial intelligence*, vol. 33, no. 01, 2019, pp. 590–597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] G. Wang, S. Zhai, G. Lasio, B. Zhang, B. Yi, S. Chen, T. J. Macvittie,
    D. Metaxas, J. Zhou, and S. Zhang, “Semi-supervised segmentation of radiation-induced
    pulmonary fibrosis from lung ct scans with multi-scale guided dense attention,”
    *IEEE transactions on medical imaging*, vol. 41, no. 3, pp. 531–542, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng,
    I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester *et al.*, “Deep learning techniques
    for automatic mri cardiac multi-structures segmentation and diagnosis: is the
    problem solved?” *IEEE transactions on medical imaging*, vol. 37, no. 11, pp.
    2514–2525, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan,
    G. Mu, Z. Lin, M. Han *et al.*, “The state of the art in kidney and kidney tumor
    segmentation in contrast-enhanced ct imaging: Results of the kits19 challenge,”
    *Medical image analysis*, vol. 67, p. 101821, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] V. Oreiller, V. Andrearczyk, M. Jreige, S. Boughdad, H. Elhalawani, J. Castelli,
    M. Vallières, S. Zhu, J. Xie, Y. Peng *et al.*, “Head and neck tumor segmentation
    in pet/ct: the hecktor challenge,” *Medical image analysis*, vol. 77, p. 102336,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, and X. Ding,
    “Embracing imperfect datasets: A review of deep learning solutions for medical
    image segmentation,” *Medical Image Analysis*, vol. 63, p. 101693, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y. Zhang, Q. Liao, L. Yuan, H. Zhu, J. Xing, and J. Zhang, “Exploiting
    shared knowledge from non-covid lesions for annotation-efficient covid-19 ct lung
    infection segmentation,” *IEEE journal of biomedical and health informatics*,
    vol. 25, no. 11, pp. 4152–4162, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] V. Cheplygina, M. de Bruijne, and J. P. Pluim, “Not-so-supervised: a survey
    of semi-supervised, multi-instance, and transfer learning in medical image analysis,”
    *Medical image analysis*, vol. 54, pp. 280–296, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Min, X. Chen, Z.-J. Zha, F. Wu, and Y. Zhang, “A two-stream mutual
    attention network for semi-supervised biomedical segmentation with noisy labels,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 33, no. 01,
    2019, pp. 4578–4585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] A. Tarvainen and H. Valpola, “Mean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results,” *Advances
    in neural information processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] S. Laine and T. Aila, “Temporal ensembling for semi-supervised learning,”
    *arXiv preprint arXiv:1610.02242*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] L. Yu, S. Wang, X. Li, C.-W. Fu, and P.-A. Heng, “Uncertainty-aware self-ensembling
    model for semi-supervised 3d left atrium segmentation,” in *International Conference
    on Medical Image Computing and Computer-Assisted Intervention*.   Springer, 2019,
    pp. 605–613.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Wang, Y. Zhang, J. Tian, C. Zhong, Z. Shi, Y. Zhang, and Z. He, “Double-uncertainty
    weighted method for semi-supervised learning,” in *International Conference on
    Medical Image Computing and Computer-Assisted Intervention*.   Springer, 2020,
    pp. 542–551.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] W. Hang, W. Feng, S. Liang, L. Yu, Q. Wang, K.-S. Choi, and J. Qin, “Local
    and global structure-aware entropy regularized mean teacher model for 3d left
    atrium segmentation,” in *International Conference on Medical Image Computing
    and Computer-Assisted Intervention*.   Springer, 2020, pp. 562–571.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Li, C. Zhang, and X. He, “Shape-aware semi-supervised 3d semantic segmentation
    for medical images,” in *International Conference on Medical Image Computing and
    Computer-Assisted Intervention*.   Springer, 2020, pp. 552–561.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] X. Luo, J. Chen, T. Song, and G. Wang, “Semi-supervised medical image
    segmentation through dual-task consistency,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 35, no. 10, 2021, pp. 8801–8809.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Y. Zhang and J. Zhang, “Dual-task mutual learning for semi-supervised
    medical image segmentation,” in *Chinese Conference on Pattern Recognition and
    Computer Vision (PRCV)*.   Springer, 2021, pp. 548–559.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2015, pp. 3431–3440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
    “3d u-net: learning dense volumetric segmentation from sparse annotation,” in
    *International conference on medical image computing and computer-assisted intervention*.   Springer,
    2016, pp. 424–432.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
    neural networks for volumetric medical image segmentation,” in *2016 fourth international
    conference on 3D vision (3DV)*.   IEEE, 2016, pp. 565–571.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, and P.-A. Heng, “H-denseunet:
    hybrid densely connected unet for liver and tumor segmentation from ct volumes,”
    *IEEE transactions on medical imaging*, vol. 37, no. 12, pp. 2663–2674, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: Redesigning
    skip connections to exploit multiscale features in image segmentation,” *IEEE
    transactions on medical imaging*, vol. 39, no. 6, pp. 1856–1867, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Y. Zhang, L. Yuan, Y. Wang, and J. Zhang, “Sau-net: efficient 3d spine
    mri segmentation using inter-slice attention,” in *Medical Imaging With Deep Learning*.   PMLR,
    2020, pp. 903–913.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] W. Bai, O. Oktay, M. Sinclair, H. Suzuki, M. Rajchl, G. Tarroni, B. Glocker,
    A. King, P. M. Matthews, and D. Rueckert, “Semi-supervised learning for network-based
    cardiac mr image segmentation,” in *International Conference on Medical Image
    Computing and Computer-Assisted Intervention*.   Springer, 2017, pp. 253–260.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Y. Zhang, L. Yang, J. Chen, M. Fredericksen, D. P. Hughes, and D. Z. Chen,
    “Deep adversarial networks for biomedical image segmentation utilizing unannotated
    images,” in *International conference on medical image computing and computer-assisted
    intervention*.   Springer, 2017, pp. 408–416.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] X. Hao, S. Gao, L. Sheng, and J. Zhang, “Parameter decoupling strategy
    for semi-supervised 3d left atrium segmentation,” in *Fourteenth International
    Conference on Machine Vision (ICMV 2021)*, vol. 12084.   SPIE, 2022, pp. 118–124.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] C. You, Y. Zhou, R. Zhao, L. Staib, and J. S. Duncan, “Simcvd: Simple
    contrastive voxel-wise representation distillation for semi-supervised medical
    image segmentation,” *IEEE Transactions on Medical Imaging*, pp. 1–1, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. Ma, Y. Wang, X. An, C. Ge, Z. Yu, J. Chen, Q. Zhu, G. Dong, J. He,
    Z. He *et al.*, “Toward data-efficient learning: A benchmark for covid-19 ct lung
    and infection segmentation,” *Medical physics*, vol. 48, no. 3, pp. 1197–1210,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein,
    “nnu-net: a self-configuring method for deep learning-based biomedical image segmentation,”
    *Nature methods*, vol. 18, no. 2, pp. 203–211, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. Luo, W. Liao, J. Chen, T. Song, Y. Chen, S. Zhang, N. Chen, G. Wang,
    and S. Zhang, “Efficient semi-supervised gross target volume of nasopharyngeal
    carcinoma segmentation via uncertainty rectified pyramid consistency,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2021, pp. 318–329.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Q. Jin, Z. Meng, C. Sun, H. Cui, and R. Su, “Ra-unet: A hybrid deep attention-aware
    network to extract liver and tumor in ct scans,” *Frontiers in Bioengineering
    and Biotechnology*, p. 1471, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,
    K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz *et al.*, “Attention u-net: Learning
    where to look for the pancreas,” *arXiv preprint arXiv:1804.03999*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] X. Luo, W. Liao, J. Xiao, J. Chen, T. Song, X. Zhang, K. Li, D. N. Metaxas,
    G. Wang, and S. Zhang, “Word: A large scale dataset, benchmark and clinical applicable
    study for abdominal organ segmentation from ct image,” *Medical Image Analysis*,
    vol. 82, p. 102642, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable
    predictive uncertainty estimation using deep ensembles,” *Advances in neural information
    processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation: Representing
    model uncertainty in deep learning,” in *international conference on machine learning*.   PMLR,
    2016, pp. 1050–1059.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Y. Wang, X. Wei, F. Liu, J. Chen, Y. Zhou, W. Shen, E. K. Fishman, and
    A. L. Yuille, “Deep distance transform for tubular structure segmentation in ct
    scans,” pp. 3833–3842, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] S. Dangi, C. A. Linte, and Z. Yaniv, “A distance map regularized cnn for
    cardiac cine mr image segmentation,” *Medical physics*, vol. 46, no. 12, pp. 5637–5651,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] J. Ma, Z. Wei, Y. Zhang, Y. Wang, R. Lv, C. Zhu, C. Gaoxiang, J. Liu,
    C. Peng, L. Wang *et al.*, “How distance transform maps boost segmentation cnns:
    an empirical study,” in *Medical Imaging with Deep Learning*.   PMLR, 2020, pp.
    479–492.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] F. Navarro, S. Shit, I. Ezhov, J. Paetzold, A. Gafita, J. C. Peeken, S. E.
    Combs, and B. H. Menze, “Shape-aware complementary-task learning for multi-organ
    segmentation,” in *International Workshop on Machine Learning in Medical Imaging*.   Springer,
    2019, pp. 620–627.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] X. Liu, F. Xing, N. Shusharina, R. Lim, C.-C. Jay Kuo, G. El Fakhri, and
    J. Woo, “Act: Semi-supervised domain-adaptive medical image segmentation with
    asymmetric co-training,” in *International Conference on Medical Image Computing
    and Computer-Assisted Intervention*.   Springer, 2022, pp. 66–76.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] X. Li, L. Yu, H. Chen, C.-W. Fu, L. Xing, and P.-A. Heng, “Transformation-consistent
    self-ensembling model for semisupervised medical image segmentation,” *IEEE Transactions
    on Neural Networks and Learning Systems*, vol. 32, no. 2, pp. 523–534, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu, “Deep mutual learning,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2018, pp. 4320–4328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] L. Yu, J.-Z. Cheng, Q. Dou, X. Yang, H. Chen, J. Qin, and P.-A. Heng,
    “Automatic 3d cardiovascular mr segmentation with densely-connected volumetric
    convnets,” in *International conference on medical image computing and computer-assisted
    intervention*.   Springer, 2017, pp. 287–295.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Zhang, Q. Liao, L. Ding, and J. Zhang, “Bridging 2d and 3d segmentation
    networks for computation-efficient volumetric medical image segmentation: An empirical
    study of 2.5 d solutions,” *Computerized Medical Imaging and Graphics*, p. 102088,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] T.-H. Vu, H. Jain, M. Bucher, M. Cord, and P. Pérez, “Advent: Adversarial
    entropy minimization for domain adaptation in semantic segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 2517–2526.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Z. Xiong, Q. Xia, Z. Hu, N. Huang, C. Bian, Y. Zheng, S. Vesal, N. Ravikumar,
    A. Maier, X. Yang *et al.*, “A global benchmark of algorithms for segmenting the
    left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging,”
    *Medical Image Analysis*, vol. 67, p. 101832, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] K. Wang, B. Zhan, C. Zu, X. Wu, J. Zhou, L. Zhou, and Y. Wang, “Semi-supervised
    medical image segmentation via a tripled-uncertainty guided mean teacher model
    with contrastive learning,” *Medical Image Analysis*, vol. 79, p. 102447, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] H. Yao, X. Hu, and X. Li, “Enhancing pseudo label quality for semi-supervised
    domain-generalized medical image segmentation,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 36, no. 3, 2022, pp. 3099–3107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] K. Han, L. Liu, Y. Song, Y. Liu, C. Qiu, Y. Tang, Q. Teng, and Z. Liu,
    “An effective semi-supervised approach for liver ct image segmentation,” *IEEE
    Journal of Biomedical and Health Informatics*, vol. 26, no. 8, pp. 3999–4007,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Z. Xu, Y. Wang, D. Lu, L. Yu, J. Yan, J. Luo, K. Ma, Y. Zheng, and R. K.-y.
    Tong, “All-around real label supervision: Cyclic prototype consistency learning
    for semi-supervised medical image segmentation,” *IEEE Journal of Biomedical and
    Health Informatics*, vol. 26, no. 7, pp. 3174–3184, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Z. Ren, R. Yeh, and A. Schwing, “Not all unlabeled data are equal: Learning
    to weight data in semi-supervised learning,” *Advances in Neural Information Processing
    Systems*, vol. 33, pp. 21 786–21 797, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Z. Zhang, C. Tian, H. X. Bai, Z. Jiao, and X. Tian, “Discriminative error
    prediction network for semi-supervised colon gland segmentation,” *Medical Image
    Analysis*, vol. 79, p. 102458, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] A. Ghosh and A. H. Thiery, “On data-augmentation and consistency-based
    semi-supervised learning,” in *International Conference on Learning Representations*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] M. Van Waerebeke, G. Lodygensky, and J. Dolz, “On the pitfalls of entropy-based
    uncertainty for multi-class semi-supervised segmentation,” in *International Workshop
    on Uncertainty for Safe Utilization of Machine Learning in Medical Imaging*.   Springer,
    2022, pp. 36–46.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] X. Zhao, C. Fang, D.-J. Fan, X. Lin, F. Gao, and G. Li, “Cross-level contrastive
    learning and consistency constraint for semi-supervised medical image segmentation,”
    in *2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)*.   IEEE,
    2022, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] H. Basak, R. Bhattacharya, R. Hussain, and A. Chatterjee, “An embarrassingly
    simple consistency regularization method for semi-supervised medical image segmentation,”
    *arXiv preprint arXiv:2202.00677*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] X. Xu, T. Sanford, B. Turkbey, S. Xu, B. J. Wood, and P. Yan, “Shadow-consistent
    semi-supervised learning for prostate ultrasound segmentation,” *IEEE Transactions
    on Medical Imaging*, vol. 41, no. 6, pp. 1331–1345, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] N. Zhang, J. Hou, R.-W. Zhao, R. Feng, and Y. Zhang, “Semi-supervised
    medical image segmentation with distribution calibration and non-local semantic
    constraint,” in *2021 IEEE International Conference on Bioinformatics and Biomedicine
    (BIBM)*.   IEEE, 2021, pp. 1171–1178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] H. Huang, Q. Chen, L. Lin, M. Cai, Q. Zhang, Y. Iwamoto, X. Han, A. Furukawa,
    S. Kanasaki, Y.-W. Chen, R. Tong, and H. Hu, “Mtl-abs3net: Atlas-based semi-supervised
    organ segmentation network with multi-task learning for medical images,” *IEEE
    Journal of Biomedical and Health Informatics*, vol. 26, no. 8, pp. 3988–3998,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] J. Peng, P. Wang, C. Desrosiers, and M. Pedersoli, “Self-paced contrastive
    learning for semi-supervised medical image segmentation with meta-labels,” *Advances
    in Neural Information Processing Systems*, vol. 34, pp. 16 686–16 699, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] L. Hu, J. Li, X. Peng, J. Xiao, B. Zhan, C. Zu, X. Wu, J. Zhou, and Y. Wang,
    “Semi-supervised npc segmentation with uncertainty and attention guided consistency,”
    *Knowledge-Based Systems*, vol. 239, p. 108021, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] B. H. Thompson, G. Di Caterina, and J. P. Voisey, “Pseudo-label refinement
    using superpixels for semi-supervised brain tumour segmentation,” in *2022 IEEE
    19th International Symposium on Biomedical Imaging (ISBI)*.   IEEE, 2022, pp.
    1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk, “Slic
    superpixels compared to state-of-the-art superpixel methods,” *IEEE transactions
    on pattern analysis and machine intelligence*, vol. 34, no. 11, pp. 2274–2282,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] A. Xu, S. Wang, S. Ye, J. Fan, X. Shi, and X. Xia, “Ca-mt: A self-ensembling
    model for semi-supervised cardiac segmentation with elliptical descriptor based
    contour-aware,” in *2022 IEEE 19th International Symposium on Biomedical Imaging
    (ISBI)*.   IEEE, 2022, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Li, Z. Zhao, K. Xu, Z. Zeng, and C. Guan, “Hierarchical consistency
    regularized mean teacher for semi-supervised 3d left atrium segmentation,” in
    *2021 43rd Annual International Conference of the IEEE Engineering in Medicine
    & Biology Society (EMBC)*.   IEEE, 2021, pp. 3395–3398.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] C. M. Seibold, S. Reiß, J. Kleesiek, and R. Stiefelhagen, “Reference-guided
    pseudo-label generation for medical semantic segmentation,” in *Proceedings of
    the AAAI Conference on Artificial Intelligence*, vol. 36, no. 2, 2022, pp. 2171–2179.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y. Shi, J. Zhang, T. Ling, J. Lu, Y. Zheng, Q. Yu, L. Qi, and Y. Gao,
    “Inconsistency-aware uncertainty estimation for semi-supervised medical image
    segmentation,” *IEEE Transactions on Medical Imaging*, vol. 41, no. 3, pp. 608–620,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] J. Chen, H. Zhang, R. Mohiaddin, T. Wong, D. Firmin, J. Keegan, and G. Yang,
    “Adaptive hierarchical dual consistency for semi-supervised left atrium segmentation
    on cross-domain data,” *IEEE Transactions on Medical Imaging*, vol. 41, no. 2,
    pp. 420–433, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] K. Zheng, J. Xu, and J. Wei, “Double noise mean teacher self-ensembling
    model for semi-supervised tumor segmentation,” in *ICASSP 2022-2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE, 2022,
    pp. 1446–1450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian deep
    learning for computer vision?” *Advances in neural information processing systems*,
    vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] M.-C. Xu, Y.-K. Zhou, C. Jin, S. B. Blumberg, F. J. Wilson, M. deGroot,
    D. C. Alexander, N. P. Oxtoby, and J. Jacob, “Learning morphological feature perturbations
    for calibrated semi-supervised segmentation,” in *International Conference on
    Medical Imaging with Deep Learning*.   PMLR, 2022, pp. 1413–1429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] R. Wang, Y. Wu, H. Chen, L. Wang, and D. Meng, “Neighbor matching for
    semi-supervised learning,” in *International Conference on Medical Image Computing
    and Computer-Assisted Intervention*.   Springer, 2021, pp. 439–449.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] H. Peiris, Z. Chen, G. Egan, and M. Harandi, “Duo-segnet: adversarial
    dual-views for semi-supervised medical image segmentation,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2021, pp. 428–438.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] L. Zhu, K. Yang, M. Zhang, L. L. Chan, T. K. Ng, and B. C. Ooi, “Semi-supervised
    unpaired multi-modal learning for label-efficient medical image segmentation,”
    in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2021, pp. 394–404.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] X. Zeng, R. Huang, Y. Zhong, D. Sun, C. Han, D. Lin, D. Ni, and Y. Wang,
    “Reciprocal learning for semi-supervised segmentation,” in *International Conference
    on Medical Image Computing and Computer-Assisted Intervention*.   Springer, 2021,
    pp. 352–361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Y. Li, L. Luo, H. Lin, H. Chen, and P.-A. Heng, “Dual-consistency semi-supervised
    learning with uncertainty quantification for covid-19 lesion segmentation from
    ct images,” in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2021, pp. 199–209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] K. Wang, B. Zhan, C. Zu, X. Wu, J. Zhou, L. Zhou, and Y. Wang, “Tripled-uncertainty
    guided mean teacher model for semi-supervised medical image segmentation,” in
    *International Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2021, pp. 450–460.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] X. Hu, D. Zeng, X. Xu, and Y. Shi, “Semi-supervised contrastive learning
    for label-efficient medical image segmentation,” in *International Conference
    on Medical Image Computing and Computer-Assisted Intervention*.   Springer, 2021,
    pp. 481–490.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] W. Huang, C. Chen, Z. Xiong, Y. Zhang, X. Chen, X. Sun, and F. Wu, “Semi-supervised
    neuron segmentation via reinforced consistency learning,” *IEEE Transactions on
    Medical Imaging*, pp. 1–1, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] P. Wang, J. Peng, M. Pedersoli, Y. Zhou, C. Zhang, and C. Desrosiers,
    “Self-paced and self-consistent co-training for semi-supervised image segmentation,”
    *Medical Image Analysis*, vol. 73, p. 102146, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] C. Chen, K. Zhou, Z. Wang, and R. Xiao, “Generative consistency for semi-supervised
    cerebrovascular segmentation from tof-mra,” *IEEE Transactions on Medical Imaging*,
    pp. 1–1, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. Liu, W. Wang, G. Luo, K. Wang, and S. Li, “A contrastive consistency
    semi-supervised left atrium segmentation model,” *Computerized Medical Imaging
    and Graphics*, p. 102092, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] X. Chen, H.-Y. Zhou, F. Liu, J. Guo, L. Wang, and Y. Yu, “Mass: Modality-collaborative
    semi-supervised segmentation by exploiting cross-modal consistency from unpaired
    ct and mri images,” *Medical Image Analysis*, p. 102506, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] J. Wang and T. Lukasiewicz, “Rethinking bayesian deep learning methods
    for semi-supervised volumetric medical image segmentation,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp.
    182–190.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] H. Wu, Z. Wang, Y. Song, L. Yang, and J. Qin, “Cross-patch dense contrastive
    learning for semi-supervised segmentation of cellular nuclei in histopathologic
    images,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, 2022, pp. 11 666–11 675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] X. Luo, G. Wang, W. Liao, J. Chen, T. Song, Y. Chen, S. Zhang, D. N.
    Metaxas, and S. Zhang, “Semi-supervised medical image segmentation via uncertainty
    rectified pyramid consistency,” *Medical Image Analysis*, p. 102517, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Y. Lin, H. Yao, Z. Li, G. Zheng, and X. Li, “Calibrating label distribution
    for class-imbalanced barely-supervised knee segmentation,” in *International Conference
    on Medical Image Computing and Computer-Assisted Intervention*.   Springer, 2022,
    pp. 109–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] H. Wu, J. Liu, F. Xiao, Z. Wen, L. Cheng, and J. Qin, “Semi-supervised
    segmentation of echocardiography videos via noise-resilient spatiotemporal semantic
    calibration and fusion,” *Medical Image Analysis*, vol. 78, p. 102397, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] X. Wang, Y. Yuan, D. Guo, X. Huang, Y. Cui, M. Xia, Z. Wang, C. Bai,
    and S. Chen, “Ssa-net: Spatial self-attention network for covid-19 pneumonia infection
    segmentation with semi-supervised few-shot learning,” *Medical Image Analysis*,
    vol. 79, p. 102459, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Z. Fang, J. Bai, X. Guo, X. Wang, F. Gao, H.-Y. Yang, B. Kong, Y. Hou,
    K. Cao, Q. Song *et al.*, “Annotation-efficient covid-19 pneumonia lesion segmentation
    using error-aware unified semi-supervised and active learning,” *IEEE Transactions
    on Artificial Intelligence*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Z. Li, Z. Li, R. Liu, Z. Luo, and X. Fan, “Coupling deep deformable registration
    with contextual refinement for semi-supervised medical image segmentation,” in
    *2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)*.   IEEE,
    2022, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. Shu, H. Li, B. Xiao, X. Bi, and W. Li, “Cross-mix monitoring for medical
    image segmentation with limited supervision,” *IEEE Transactions on Multimedia*,
    pp. 1–1, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Q.-Q. Chen, Z.-H. Sun, C.-F. Wei, E. Q. Wu, and D. Ming, “Semi-supervised
    3d medical image segmentation based on dual-task consistent joint learning and
    task-level regularization,” *IEEE/ACM Transactions on Computational Biology and
    Bioinformatics*, pp. 1–1, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] M. Liu, L. Xiao, H. Jiang, and Q. He, “Ccat-net: A novel transformer
    based semi-supervised framework for covid-19 lung lesion segmentation,” in *2022
    IEEE 19th International Symposium on Biomedical Imaging (ISBI)*.   IEEE, 2022,
    pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] M. Sajjadi, M. Javanmardi, and T. Tasdizen, “Regularization with stochastic
    transformations and perturbations for deep semi-supervised learning,” *Advances
    in neural information processing systems*, vol. 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] D. Nie, Y. Gao, L. Wang, and D. Shen, “Asdnet: attention based semi-supervised
    deep networks for medical image segmentation,” in *International conference on
    medical image computing and computer-assisted intervention*.   Springer, 2018,
    pp. 370–378.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] K. Chaitanya, N. Karani, C. F. Baumgartner, A. Becker, O. Donati, and
    E. Konukoglu, “Semi-supervised and task-driven data augmentation,” in *International
    conference on information processing in medical imaging*.   Springer, 2019, pp.
    29–41.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] G. Bortsova, F. Dubost, L. Hogeweg, I. Katramados, and M. d. Bruijne,
    “Semi-supervised medical image segmentation via learning consistency under transformations,”
    in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2019, pp. 810–818.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Z. Xiao, Y. Su, Z. Deng, and W. Zhang, “Efficient combination of cnn
    and transformer for dual-teacher uncertainty-aware guided semi-supervised medical
    image segmentation,” *Available at SSRN 4081789*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Yang, Y. Tao, Q. Xu, Y. Zhang, X. Ma, S. Yuan, and Q. Chen, “Self-supervised
    sequence recovery for semi-supervised retinal layer segmentation,” *IEEE Journal
    of Biomedical and Health Informatics*, vol. 26, no. 8, pp. 3872–3883, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] H. He, A. Banerjee, M. Beetz, R. P. Choudhury, and V. Grau, “Semi-supervised
    coronary vessels segmentation from invasive coronary angiography with connectivity-preserving
    loss function,” in *2022 IEEE 19th International Symposium on Biomedical Imaging
    (ISBI)*, 2022, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] D. Kiyasseh, A. Swiston, R. Chen, and A. Chen, “Segmentation of left
    atrial mr images via self-supervised semi-supervised meta-learning,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2021, pp. 13–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] V. M. Campello, P. Gkontra, C. Izquierdo, C. Martin-Isla, A. Sojoudi,
    P. M. Full, K. Maier-Hein, Y. Zhang, Z. He, J. Ma *et al.*, “Multi-centre, multi-vendor
    and multi-disease cardiac segmentation: the m&ms challenge,” *IEEE Transactions
    on Medical Imaging*, vol. 40, no. 12, pp. 3543–3554, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] G. Chen, J. Ru, Y. Zhou, I. Rekik, Z. Pan, X. Liu, Y. Lin, B. Lu, and
    J. Shi, “Mtans: Multi-scale mean teacher combined adversarial network with shape-aware
    embedding for semi-supervised brain lesion segmentation,” *NeuroImage*, vol. 244,
    p. 118568, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] S. Li, Y. Zhang, and X. Yang, “Semi-supervised cardiac mri segmentation
    based on generative adversarial network and variational auto-encoder,” in *2021
    IEEE International Conference on Bioinformatics and Biomedicine (BIBM)*.   IEEE,
    2021, pp. 1402–1405.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] G. Zhang, Z. Yang, B. Huo, S. Chai, and S. Jiang, “Automatic segmentation
    of organs at risk and tumors in ct images of lung cancer from partially labelled
    datasets with a semi-supervised conditional nnu-net,” *Computer methods and programs
    in biomedicine*, vol. 211, p. 106419, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Y. Wu, Z. Ge, D. Zhang, M. Xu, L. Zhang, Y. Xia, and J. Cai, “Mutual
    consistency learning for semi-supervised medical image segmentation,” *Medical
    Image Analysis*, p. 102530, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby,
    Y. Burren, N. Porz, J. Slotboom, R. Wiest *et al.*, “The multimodal brain tumor
    image segmentation benchmark (brats),” *IEEE transactions on medical imaging*,
    vol. 34, no. 10, pp. 1993–2024, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] R. A. Zeineldin, M. E. Karar, J. Coburger, C. R. Wirtz, and O. Burgert,
    “Deepseg: deep neural network framework for automatic brain tumor segmentation
    using magnetic resonance flair images,” *International journal of computer assisted
    radiology and surgery*, vol. 15, no. 6, pp. 909–920, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore,
    S. Phillips, D. Maffitt, M. Pringle *et al.*, “The cancer imaging archive (tcia):
    maintaining and operating a public information repository,” *Journal of digital
    imaging*, vol. 26, no. 6, pp. 1045–1057, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Wang, X. Li, Y. Han, J. Qin, L. Wang, and Z. Qichao, “Separated contrastive
    learning for organ-at-risk and gross-tumor-volume segmentation with limited annotation,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 36, no. 3,
    2022, pp. 2459–2467.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] X. Luo, M. Hu, T. Song, G. Wang, and S. Zhang, “Semi-supervised medical
    image segmentation via cross teaching between cnn and transformer,” in *Medical
    Imaging With Deep Learning*.   PMLR, 2022, pp. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Z. Zhao, J. Hu, Z. Zeng, X. Yang, P. Qian, B. Veeravalli, and C. Guan,
    “Mmgl: Multi-scale multi-view global-local contrastive learning for semi-supervised
    cardiac image segmentation,” in *2022 IEEE international conference on image processing
    (ICIP)*.   IEEE, 2022, pp. 401–405.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] H. Wu, G. Chen, Z. Wen, and J. Qin, “Collaborative and adversarial learning
    of focused and dispersive representations for semi-supervised polyp segmentation,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2021, pp. 3489–3498.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Y. Xie, J. Zhang, Z. Liao, J. Verjans, C. Shen, and Y. Xia, “Intra-and
    inter-pair consistency for semi-supervised gland segmentation,” *IEEE Transactions
    on Image Processing*, vol. 31, pp. 894–905, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] C. Li, L. Dong, Q. Dou, F. Lin, K. Zhang, Z. Feng, W. Si, X. Deng, Z. Deng,
    and P.-A. Heng, “Self-ensembling co-training framework for semi-supervised covid-19
    ct segmentation,” *IEEE Journal of Biomedical and Health Informatics*, vol. 25,
    no. 11, pp. 4140–4151, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] H. Yang, C. Shan, A. Bouwman, L. R. Dekker, A. F. Kolen, and P. H. de With,
    “Medical instrument segmentation in 3d us by hybrid constrained semi-supervised
    learning,” *IEEE Journal of Biomedical and Health Informatics*, vol. 26, no. 2,
    pp. 762–773, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] S. Reiß, C. Seibold, A. Freytag, E. Rodner, and R. Stiefelhagen, “Every
    annotation counts: Multi-label deep supervision for medical image segmentation,”
    in *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    2021, pp. 9532–9542.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] K. Zhang and X. Zhuang, “Cyclemix: A holistic strategy for medical image
    segmentation from scribble supervision,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 11 656–11 665.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Z. Xu, D. Lu, Y. Wang, J. Luo, J. Jayender, K. Ma, Y. Zheng, and X. Li,
    “Noisy labels are treasure: mean-teacher-assisted confident learning for hepatic
    vessel segmentation,” in *International Conference on Medical Image Computing
    and Computer-Assisted Intervention*.   Springer, 2021, pp. 3–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Z. Xie, E. Tu, H. Zheng, Y. Gu, and J. Yang, “Semi-supervised skin lesion
    segmentation with learning model confidence,” in *ICASSP 2021-2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE, 2021,
    pp. 1135–1139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] J. M. Johnson and T. M. Khoshgoftaar, “Survey on deep learning with class
    imbalance,” *Journal of Big Data*, vol. 6, no. 1, pp. 1–54, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] S. Liu, Y. Li, X. Li, and G. Cao, “Shape-aware multi-task learning for
    semi-supervised 3d medical image segmentation,” in *2021 IEEE International Conference
    on Bioinformatics and Biomedicine (BIBM)*.   IEEE, 2021, pp. 1418–1423.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J. Hou, X. Ding, and J. D. Deng, “Semi-supervised semantic segmentation
    of vessel images using leaking perturbations,” in *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, 2022, pp. 2625–2634.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] D.-H. Lee *et al.*, “Pseudo-label: The simple and efficient semi-supervised
    learning method for deep neural networks,” in *Workshop on challenges in representation
    learning, ICML*, vol. 3, no. 2, 2013, p. 896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and
    C. A. Raffel, “Mixmatch: A holistic approach to semi-supervised learning,” *Advances
    in neural information processing systems*, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] A. Blum and T. Mitchell, “Combining labeled and unlabeled data with co-training,”
    in *Proceedings of the eleventh annual conference on Computational learning theory*,
    1998, pp. 92–100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Y. Xia, F. Liu, D. Yang, J. Cai, L. Yu, Z. Zhu, D. Xu, A. Yuille, and
    H. Roth, “3d semi-supervised learning with uncertainty-aware multi-view co-training,”
    in *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision*, 2020, pp. 3646–3655.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] S. Qiao, W. Shen, Z. Zhang, B. Wang, and A. Yuille, “Deep co-training
    for semi-supervised image recognition,” in *Proceedings of the european conference
    on computer vision (eccv)*, 2018, pp. 135–152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] W. Dong-DongChen and Z.-H. WeiGao, “Tri-net for semi-supervised deep
    learning,” in *Proceedings of twenty-seventh international joint conference on
    artificial intelligence*, 2018, pp. 2014–2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] J. Peng, G. Estrada, M. Pedersoli, and C. Desrosiers, “Deep co-training
    for semi-supervised image segmentation,” *Pattern Recognition*, vol. 107, p. 107269,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Y. He, G. Yang, J. Yang, Y. Chen, Y. Kong, J. Wu, L. Tang, X. Zhu, J.-L.
    Dillenseger, P. Shao *et al.*, “Dense biased networks with deep priori anatomy
    and hard region adaptation: Semi-supervised learning for fine renal artery segmentation,”
    *Medical image analysis*, vol. 63, p. 101722, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] H. Zheng, L. Lin, H. Hu, Q. Zhang, Q. Chen, Y. Iwamoto, X. Han, Y.-W.
    Chen, R. Tong, and J. Wu, “Semi-supervised segmentation of liver using adversarial
    learning with deep atlas prior,” in *International Conference on Medical Image
    Computing and Computer-Assisted Intervention*.   Springer, 2019, pp. 148–156.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy minimization,”
    *Advances in neural information processing systems*, vol. 17, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] J. Wu, H. Fan, X. Zhang, S. Lin, and Z. Li, “Semi-supervised semantic
    segmentation via entropy minimization,” in *2021 IEEE International Conference
    on Multimedia and Expo (ICME)*.   IEEE, 2021, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Y. Ouali, C. Hudelot, and M. Tami, “Semi-supervised semantic segmentation
    with cross-consistency training,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2020, pp. 12 674–12 684.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] C. Dong, Y.-w. Chen, A. H. Foruzan, L. Lin, X.-h. Han, T. Tateyama, X. Wu,
    G. Xu, and H. Jiang, “Segmentation of liver and spleen based on computational
    anatomy models,” *Computers in biology and medicine*, vol. 67, pp. 146–160, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] H. Park, P. H. Bland, and C. R. Meyer, “Construction of an abdominal
    probabilistic atlas and its application in segmentation,” *IEEE Transactions on
    medical imaging*, vol. 22, no. 4, pp. 483–492, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] M. Sajjadi, M. Javanmardi, and T. Tasdizen, “Mutual exclusivity loss
    for semi-supervised deep learning,” in *2016 IEEE International Conference on
    Image Processing (ICIP)*.   IEEE, 2016, pp. 1908–1912.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel,
    E. D. Cubuk, A. Kurakin, and C.-L. Li, “Fixmatch: Simplifying semi-supervised
    learning with consistency and confidence,” *Advances in neural information processing
    systems*, vol. 33, pp. 596–608, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] A. Oliver, A. Odena, C. A. Raffel, E. D. Cubuk, and I. Goodfellow, “Realistic
    evaluation of deep semi-supervised learning algorithms,” *Advances in neural information
    processing systems*, vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii, “Virtual adversarial
    training: a regularization method for supervised and semi-supervised learning,”
    *IEEE transactions on pattern analysis and machine intelligence*, vol. 41, no. 8,
    pp. 1979–1993, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Y. Ouali, C. Hudelot, and M. Tami, “An overview of deep semi-supervised
    learning,” *ArXiv*, vol. abs/2006.05278, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] W. Luo, Y. Li, R. Urtasun, and R. S. Zemel, “Understanding the effective
    receptive field in deep convolutional neural networks,” in *NIPS*, 2016, p. 4905–4913.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] M. Xu, N. P. Oxtoby, D. C. Alexander, and J. Jacob, “Learning to pay
    attention to mistakes,” *ArXiv*, vol. abs/2007.15131, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] A. Andreopoulos and J. K. Tsotsos, “Efficient and generalizable statistical
    models of shape and appearance for analysis of cardiac mri,” *Medical image analysis*,
    vol. 12, no. 3, pp. 335–357, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] R. Awan, K. Sirinukunwattana, D. Epstein, S. Jefferyes, U. Qidwai, Z. Aftab,
    I. Mujeeb, D. Snead, and N. Rajpoot, “Glandular morphometrics for objective grading
    of colorectal adenocarcinoma histology images,” *Scientific reports*, vol. 7,
    no. 1, pp. 1–12, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] G. Wang, X. Liu, C. Li, Z. Xu, J. Ruan, H. Zhu, T. Meng, K. Li, N. Huang,
    and S. Zhang, “A noise-robust framework for automatic segmentation of covid-19
    pneumonia lesions from ct images,” *IEEE Transactions on Medical Imaging*, vol. 39,
    no. 8, pp. 2653–2663, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] M. Jun, G. Cheng, W. Yixin, A. Xingle, G. Jiantao, Y. Ziqi, and H. Jian,
    “Covid-19 ct lung and infection segmentation dataset (version verson 1.0)[data
    set]. zenodo,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] N. Kasthuri, K. J. Hayworth, D. R. Berger, R. L. Schalek, J. A. Conchello,
    S. Knowles-Barley, D. Lee, A. Vázquez-Reina, V. Kaynig, T. R. Jones *et al.*,
    “Saturated reconstruction of a volume of neocortex,” *Cell*, vol. 162, no. 3,
    pp. 648–661, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] G. A. Sonn, S. Natarajan, D. J. Margolis, M. MacAiran, P. Lieu, J. Huang,
    F. J. Dorey, and L. S. Marks, “Targeted biopsy in the detection of prostate cancer
    using an office based magnetic resonance ultrasound fusion device,” *The Journal
    of urology*, vol. 189, no. 1, pp. 86–92, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] A. Carass, S. Roy, A. Jog, J. L. Cuzzocreo, E. Magrath, A. Gherman, J. Button,
    J. Nguyen, F. Prados, C. H. Sudre *et al.*, “Longitudinal multiple sclerosis lesion
    segmentation: resource and challenge,” *NeuroImage*, vol. 148, pp. 77–102, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] O. Maier, B. H. Menze, J. von der Gablentz, L. Häni, M. P. Heinrich,
    M. Liebrand, S. Winzeck, A. Basit, P. Bentley, L. Chen *et al.*, “Isles 2015-a
    public evaluation benchmark for ischemic stroke lesion segmentation from multispectral
    mri,” *Medical image analysis*, vol. 35, pp. 250–269, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] N. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore, H. Kaluzniak,
    J. Rosenberg, P. Blake, Z. Rengel, M. Oestreich *et al.*, “The kits19 challenge
    data: 300 kidney tumor cases with clinical context, ct semantic segmentations,
    and surgical outcomes,” *arXiv preprint arXiv:1904.00445*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] R. Karim, R. J. Housden, M. Balasubramaniam, Z. Chen, D. Perry, A. Uddin,
    Y. Al-Beyatti, E. Palkhi, P. Acheampong, S. Obom *et al.*, “Evaluation of current
    algorithms for segmentation of scar tissue from late gadolinium enhancement cardiovascular
    magnetic resonance of the left atrium: an open-access grand challenge,” *Journal
    of Cardiovascular Magnetic Resonance*, vol. 15, no. 1, pp. 1–17, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] L. Li, V. A. Zimmer, J. A. Schnabel, and X. Zhuang, “Atrialgeneral: Domain
    generalization for left atrial segmentation of multi-center lge mris,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2021, pp. 557–566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] X. Zhuang and J. Shen, “Multi-scale patch and multi-modality atlases
    for whole heart segmentation of mri,” *Medical image analysis*, vol. 31, pp. 77–87,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] X. Zhuang, “Challenges and methodologies of fully automatic whole heart
    segmentation: a review,” *Journal of healthcare engineering*, vol. 4, no. 3, pp.
    371–407, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] X. Zhuang, W. Bai, J. Song, S. Zhan, X. Qian, W. Shi, Y. Lian, and D. Rueckert,
    “Multiatlas whole heart segmentation of ct data using conditional entropy for
    atlas ranking and selection,” *Medical physics*, vol. 42, no. 7, pp. 3822–3833,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] X. Zhuang, K. S. Rhode, R. S. Razavi, D. J. Hawkes, and S. Ourselin,
    “A registration-based propagation framework for automatic whole heart segmentation
    of cardiac mri,” *IEEE transactions on medical imaging*, vol. 29, no. 9, pp. 1612–1625,
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] G. Litjens, R. Toth, W. van de Ven, C. Hoeks, S. Kerkstra, B. van Ginneken,
    G. Vincent, G. Guillard, N. Birbeck, J. Zhang *et al.*, “Evaluation of prostate
    segmentation algorithms for mri: the promise12 challenge,” *Medical image analysis*,
    vol. 18, no. 2, pp. 359–373, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] A. E. Kavur, N. S. Gezer, M. Barış, S. Aslan, P.-H. Conze, V. Groza,
    D. D. Pham, S. Chatterjee, P. Ernst, S. Özkan *et al.*, “Chaos challenge-combined
    (ct-mr) healthy abdominal organ segmentation,” *Medical Image Analysis*, vol. 69,
    p. 101950, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] F. Prados, J. Ashburner, C. Blaiotta, T. Brosch, J. Carballido-Gamio,
    M. J. Cardoso, B. N. Conrad, E. Datta, G. Dávid, B. De Leener *et al.*, “Spinal
    cord grey matter segmentation challenge,” *Neuroimage*, vol. 152, pp. 312–329,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. Van Ginneken,
    A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze *et al.*, “A large annotated
    medical image dataset for the development and evaluation of segmentation algorithms,”
    *arXiv preprint arXiv:1902.09063*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] B. Landman, Z. Xu, J. Igelsias, M. Styner, T. Langerak, and A. Klein,
    “Multi-atlas labeling beyond the cranial vault,” *URL: https://www. synapse. org*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] ——, “Miccai multi-atlas labeling beyond the cranial vault–workshop and
    challenge,” in *Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault—Workshop
    Challenge*, vol. 5, 2015, p. 12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, D. Gil, C. Rodríguez,
    and F. Vilariño, “Wm-dova maps for accurate polyp highlighting in colonoscopy:
    Validation vs. saliency maps from physicians,” *Computerized medical imaging and
    graphics*, vol. 43, pp. 99–111, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] K. Sirinukunwattana, J. P. Pluim, H. Chen, X. Qi, P.-A. Heng, Y. B. Guo,
    L. Y. Wang, B. J. Matuszewski, E. Bruni, U. Sanchez *et al.*, “Gland segmentation
    in colon histology images: The glas challenge contest,” *Medical image analysis*,
    vol. 35, pp. 489–502, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] J. Staal, M. D. Abràmoff, M. Niemeijer, M. A. Viergever, and B. Van Ginneken,
    “Ridge-based vessel segmentation in color images of the retina,” *IEEE transactions
    on medical imaging*, vol. 23, no. 4, pp. 501–509, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] A. Hoover, V. Kouznetsova, and M. Goldbaum, “Locating blood vessels in
    retinal images by piecewise threshold probing of a matched filter response,” *IEEE
    Transactions on Medical imaging*, vol. 19, no. 3, pp. 203–210, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] M. M. Fraz, P. Remagnino, A. Hoppe, B. Uyyanonvara, A. R. Rudnicka, C. G.
    Owen, and S. A. Barman, “An ensemble classification-based approach applied to
    retinal blood vessel segmentation,” *IEEE Transactions on Biomedical Engineering*,
    vol. 59, no. 9, pp. 2538–2548, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] P. F. Raudaschl, P. Zaffino, G. C. Sharp, M. F. Spadea, A. Chen, B. M.
    Dawant, T. Albrecht, T. Gass, C. Langguth, M. Lüthi *et al.*, “Evaluation of segmentation
    methods on head and neck ct: auto-segmentation challenge 2015,” *Medical physics*,
    vol. 44, no. 5, pp. 2020–2036, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] J. C. Caicedo, A. Goodman, K. W. Karhohs, B. A. Cimini, J. Ackerman,
    M. Haghighi, C. Heng, T. Becker, M. Doan, C. McQuin *et al.*, “Nucleus segmentation
    across imaging experiments: the 2018 data science bowl,” *Nature methods*, vol. 16,
    no. 12, pp. 1247–1253, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] N. Kumar, R. Verma, D. Anand, Y. Zhou, O. F. Onder, E. Tsougenis, H. Chen,
    P.-A. Heng, J. Li, Z. Hu *et al.*, “A multi-organ nucleus segmentation challenge,”
    *IEEE transactions on medical imaging*, vol. 39, no. 5, pp. 1380–1391, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. d. Lange, D. Johansen,
    and H. D. Johansen, “Kvasir-seg: A segmented polyp dataset,” in *International
    Conference on Multimedia Modeling*.   Springer, 2020, pp. 451–462.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] D. Gutman, N. C. Codella, E. Celebi, B. Helba, M. Marchetti, N. Mishra,
    and A. Halpern, “Skin lesion analysis toward melanoma detection: A challenge at
    the international symposium on biomedical imaging (isbi) 2016, hosted by the international
    skin imaging collaboration (isic),” *arXiv preprint arXiv:1605.01397*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Y. Zhou, Z. Li, S. Bai, C. Wang, X. Chen, M. Han, E. Fishman, and A. L.
    Yuille, “Prior-aware neural network for partially-supervised multi-organ segmentation,”
    in *Proceedings of the IEEE/CVF international conference on computer vision*,
    2019, pp. 10 672–10 681.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] C. You, W. Dai, Y. Min, F. Liu, X. Zhang, C. Feng, D. A. Clifton, S. K.
    Zhou, L. H. Staib, and J. S. Duncan, “Rethinking semi-supervised medical image
    segmentation: A variance-reduction perspective,” *arXiv preprint arXiv:2302.01735*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] J. Qiu, L. Li, S. Wang, K. Zhang, Y. Chen, S. Yang, and X. Zhuang, “Myops-net:
    Myocardial pathology segmentation with flexible combination of multi-sequence
    cmr images,” *Medical Image Analysis*, vol. 84, p. 102694, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Y. Zhang and R. Jiao, “How segment anything model (sam) boost medical
    image segmentation?” *arXiv preprint arXiv:2305.03678*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo *et al.*, “Segment anything,” *arXiv preprint
    arXiv:2304.02643*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] P.-T. Jiang and Y. Yang, “Segment anything is a good pseudo-label generator
    for weakly supervised semantic segmentation,” *arXiv preprint arXiv:2305.01275*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] O. Chapelle, B. Schölkopf, and A. Zien, *Introduction to Semi-Supervised
    Learning*, 2006, pp. 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] X. Yang, Z. Song, I. King, and Z. Xu, “A survey on deep semi-supervised
    learning,” *IEEE Transactions on Knowledge and Data Engineering*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] L. Yang, L. Qi, L. Feng, W. Zhang, and Y. Shi, “Revisiting weak-to-strong
    consistency in semi-supervised semantic segmentation,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023, pp. 7236–7246.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel,
    E. D. Cubuk, A. Kurakin, and C.-L. Li, “Fixmatch: Simplifying semi-supervised
    learning with consistency and confidence,” in *Advances in Neural Information
    Processing Systems*, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin,
    Eds., vol. 33.   Curran Associates, Inc., 2020, pp. 596–608\. [Online]. Available:
    [https://proceedings.neurips.cc/paper_files/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] J. Kim, Y. Min, D. Kim, G. Lee, J. Seo, K. Ryoo, and S. Kim, “Conmatch:
    Semi-supervised learning with confidence-guided consistency regularization,” in
    *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October
    23–27, 2022, Proceedings, Part XXX*.   Springer, 2022, pp. 674–690.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] N. Li, L. Xiong, W. Qiu, Y. Pan, Y. Luo, and Y. Zhang, “Segment anything
    model for semi-supervised medical image segmentation via selecting reliable pseudo-labels,”
    *Available at SSRN 4477443*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] H. Peiris, M. Hayat, Z. Chen, G. Egan, and M. Harandi, “Uncertainty-guided
    dual-views for semi-supervised volumetric medical image segmentation,” *Nature
    Machine Intelligence*, Jul 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] J. Schlemper, O. Oktay, M. Schaap, M. Heinrich, B. Kainz, B. Glocker,
    and D. Rueckert, “Attention gated networks: Learning to leverage salient regions
    in medical images,” *Medical image analysis*, vol. 53, pp. 197–207, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Y. Lin, H. Yao, Z. Li, G. Zheng, and X. Li, “Calibrating label distribution
    for class-imbalanced barely-supervised knee segmentation,” in *Medical Image Computing
    and Computer Assisted Intervention – MICCAI 2022*, L. Wang, Q. Dou, P. T. Fletcher,
    S. Speidel, and S. Li, Eds.   Cham: Springer Nature Switzerland, 2022, pp. 109–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] L.-L. Zeng, K. Gao, D. Hu, Z. Feng, C. Hou, P. Rong, and W. Wang, “Ss-tbn:
    A semi-supervised tri-branch network for covid-19 screening and lesion segmentation,”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] A. Lou, K. Tawfik, X. Yao, Z. Liu, and J. Noble, “Min-max similarity:
    A contrastive semi-supervised deep learning network for surgical tools segmentation,”
    *IEEE Transactions on Medical Imaging*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] P. Wang, J. Peng, M. Pedersoli, Y. Zhou, C. Zhang, and C. Desrosiers,
    “Cat: Constrained adversarial training for anatomically-plausible semi-supervised
    segmentation,” *IEEE Transactions on Medical Imaging*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] H. Wang and X. Li, “Dhc: Dual-debiased heterogeneous co-training framework
    for class-imbalanced semi-supervised medical image segmentation,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2023, pp. 582–591.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] D. Chen, Y. Bai, W. Shen, Q. Li, L. Yu, and Y. Wang, “Magicnet: Semi-supervised
    multi-organ segmentation via magic-cube partition and recovery,” in *2023 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, 2023, pp. 23 869–23 878.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Y. Shi, Y. Zhang, and S. Wang, “Competitive ensembling teacher-student
    framework for semi-supervised left atrium mri segmentation,” *arXiv preprint arXiv:2310.13955*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] J. R. Clough, N. Byrne, I. Oksuz, V. A. Zimmer, J. A. Schnabel, and A. P.
    King, “A topological loss function for deep-learning based image segmentation
    using persistent homology,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 44, no. 12, pp. 8766–8778, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] P.-A. Ganaye, M. Sdika, B. Triggs, and H. Benoit-Cattin, “Removing segmentation
    inconsistencies with semi-supervised non-adjacency constraint,” *Medical image
    analysis*, vol. 58, p. 101551, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
