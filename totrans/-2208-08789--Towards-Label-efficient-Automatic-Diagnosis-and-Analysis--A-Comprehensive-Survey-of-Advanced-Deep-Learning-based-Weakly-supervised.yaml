- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-09-06 19:44:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:44:52'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2208.08789] Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive
    Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and
    Self-supervised Techniques in Histopathological Image Analysis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2208.08789] é¢å‘æ ‡ç­¾é«˜æ•ˆçš„è‡ªåŠ¨è¯Šæ–­å’Œåˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„ç»¼åˆè°ƒæŸ¥'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2208.08789](https://ar5iv.labs.arxiv.org/html/2208.08789)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2208.08789](https://ar5iv.labs.arxiv.org/html/2208.08789)
- en: 'Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive Survey
    of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and Self-supervised
    Techniques in Histopathological Image Analysis'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¢å‘æ ‡ç­¾é«˜æ•ˆçš„è‡ªåŠ¨è¯Šæ–­å’Œåˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„ç»¼åˆè°ƒæŸ¥
- en: 'Linhao Qu, Siyu Liu, Xiaoyu Liu, Manning WangÂ¹Â¹1Corresponding Author., Zhijian
    SongÂ²Â²footnotemark: 2 Digital Medical Research Center, School of Basic Medical
    Science, Fudan University, Shanghai Key Lab of Medical Image Computing and Computer
    Assisted Intervention, Shanghai 200032, China [{lhqu20, mnwang, zjsong}@fudan.edu.cn.](mailto:%7Blhqu20,%20mnwang,%20zjsong%7D@fudan.edu.cn.)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Linhao Qu, Siyu Liu, Xiaoyu Liu, Manning WangÂ¹Â¹1Corresponding Author., Zhijian
    SongÂ²Â²footnotemark: 2 æ•°å­—åŒ»å­¦ç ”ç©¶ä¸­å¿ƒï¼ŒåŸºç¡€åŒ»å­¦å­¦é™¢ï¼Œå¤æ—¦å¤§å­¦ï¼Œä¸Šæµ·åŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„é‡ç‚¹å®éªŒå®¤ï¼Œä¸Šæµ· 200032ï¼Œä¸­å›½
    [{lhqu20, mnwang, zjsong}@fudan.edu.cn.](mailto:%7Blhqu20,%20mnwang,%20zjsong%7D@fudan.edu.cn.)'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Abstract
- en: Histopathological images contain abundant phenotypic information and pathological
    patterns, which are the gold standards for disease diagnosis and essential for
    the prediction of patient prognosis and treatment outcome. In recent years, computer-automated
    analysis techniques for histopathological images have been urgently required in
    clinical practice, and deep learning methods represented by convolutional neural
    networks have gradually become the mainstream in the field of digital pathology.
    However, obtaining large numbers of fine-grained annotated data in this field
    is a very expensive and difficult task, which hinders the further development
    of traditional supervised algorithms based on large numbers of annotated data.
    More recent studies have started to liberate from the traditional supervised paradigm,
    and the most representative ones are the studies on weakly supervised learning
    paradigm based on weak annotation, semi-supervised learning paradigm based on
    limited annotation, and self-supervised learning paradigm based on pathological
    image representation learning. These new methods have led a new wave of automatic
    pathological image diagnosis and analysis targeted at annotation efficiency. With
    a survey of over 130 papers, we present a comprehensive and systematic review
    of the latest studies on weakly supervised learning, semi-supervised learning,
    and self-supervised learning in the field of computational pathology from both
    technical and methodological perspectives. Finally, we present the key challenges
    and future trends for these techniques.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç»„ç»‡ç—…ç†å›¾åƒåŒ…å«ä¸°å¯Œçš„è¡¨å‹ä¿¡æ¯å’Œç—…ç†æ¨¡å¼ï¼Œè¿™äº›æ˜¯ç–¾ç—…è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œå¯¹äºæ‚£è€…é¢„åå’Œæ²»ç–—ç»“æœçš„é¢„æµ‹è‡³å…³é‡è¦ã€‚è¿‘å¹´æ¥ï¼Œä¸´åºŠå®è·µä¸­æ€¥éœ€ç»„ç»‡ç—…ç†å›¾åƒçš„è®¡ç®—æœºè‡ªåŠ¨åˆ†ææŠ€æœ¯ï¼Œå·ç§¯ç¥ç»ç½‘ç»œä»£è¡¨çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é€æ¸æˆä¸ºæ•°å­—ç—…ç†å­¦é¢†åŸŸçš„ä¸»æµã€‚ç„¶è€Œï¼Œåœ¨è¯¥é¢†åŸŸè·å–å¤§é‡ç»†ç²’åº¦æ ‡æ³¨æ•°æ®æ˜¯ä¸€é¡¹éå¸¸æ˜‚è´µå’Œå›°éš¾çš„ä»»åŠ¡ï¼Œè¿™é˜»ç¢äº†åŸºäºå¤§é‡æ ‡æ³¨æ•°æ®çš„ä¼ ç»Ÿç›‘ç£ç®—æ³•çš„è¿›ä¸€æ­¥å‘å±•ã€‚æ›´è¿‘æœŸçš„ç ”ç©¶å¼€å§‹æ‘†è„±ä¼ ç»Ÿçš„ç›‘ç£èŒƒå¼ï¼Œæœ€å…·ä»£è¡¨æ€§çš„ç ”ç©¶åŒ…æ‹¬åŸºäºå¼±æ ‡æ³¨çš„å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ã€åŸºäºæœ‰é™æ ‡æ³¨çš„åŠç›‘ç£å­¦ä¹ èŒƒå¼å’ŒåŸºäºç—…ç†å›¾åƒè¡¨ç¤ºå­¦ä¹ çš„è‡ªç›‘ç£å­¦ä¹ èŒƒå¼ã€‚è¿™äº›æ–°æ–¹æ³•å¼•é¢†äº†ä¸€æ³¢é’ˆå¯¹æ ‡æ³¨æ•ˆç‡çš„è‡ªåŠ¨ç—…ç†å›¾åƒè¯Šæ–­å’Œåˆ†æçš„æ–°æ½®æµã€‚é€šè¿‡å¯¹130å¤šç¯‡è®ºæ–‡çš„è°ƒæŸ¥ï¼Œæˆ‘ä»¬ä»æŠ€æœ¯å’Œæ–¹æ³•å­¦çš„è§’åº¦å¯¹è®¡ç®—ç—…ç†å­¦é¢†åŸŸä¸­çš„æœ€æ–°ç ”ç©¶è¿›è¡Œäº†å…¨é¢å’Œç³»ç»Ÿçš„å›é¡¾ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ˆç°äº†è¿™äº›æŠ€æœ¯çš„å…³é”®æŒ‘æˆ˜å’Œæœªæ¥è¶‹åŠ¿ã€‚
- en: \useunder
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \useunder
- en: 'Keywords: histopathological images, automatic analysis, deep learning'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'Keywords: ç»„ç»‡ç—…ç†å›¾åƒï¼Œè‡ªåŠ¨åˆ†æï¼Œæ·±åº¦å­¦ä¹ '
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: Histopathological images contain abundant phenotypic information and pathological
    patterns, which are the gold standards for disease diagnosis and essential for
    the prediction of patient prognosis and treatment outcome (Myronenko *et al.*Â [2021](#bib.bib116),
    Wang *et al.*Â [2019](#bib.bib181), Srinidhi *et al.*Â [2021](#bib.bib159)). For
    clinical diagnosis, experienced pathologists usually require exhaustive examination
    and interpretation of hematoxylin-eosin-stained (H&E) tissue slides under a high
    magnification microscope, including differentiation of tumor areas from large
    areas of normal tissues, elaborate grading of tumors, and detailed assessment
    of tumor progression and invasion (e.g., presence of invasive carcinoma or proliferative
    changes, etc.). This is a highly time-consuming and labor-intensive task, and
    for example, it usually takes an experienced histopathologist 15 to 30 minutes
    to examine a complete slide (Wang *et al.*Â [2019](#bib.bib181)). Moreover, even
    an experienced pathologist may not be able to accurately determine the deep features
    hidden in the pathological images, such as predicting lymph node metastasis and
    prognosis from the primary lesion. Therefore, computer-assisted automatic analysis
    techniques for histopathological images are in urgent need in clinical practice.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç»„ç»‡ç—…ç†å›¾åƒåŒ…å«ä¸°å¯Œçš„è¡¨å‹ä¿¡æ¯å’Œç—…ç†æ¨¡å¼ï¼Œè¿™äº›æ˜¯ç–¾ç—…è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œå¹¶ä¸”å¯¹æ‚£è€…é¢„åå’Œæ²»ç–—ç»“æœçš„é¢„æµ‹è‡³å…³é‡è¦ï¼ˆMyronenko *et al.* [2021](#bib.bib116),
    Wang *et al.* [2019](#bib.bib181), Srinidhi *et al.* [2021](#bib.bib159)ï¼‰ã€‚å¯¹äºä¸´åºŠè¯Šæ–­ï¼Œç»éªŒä¸°å¯Œçš„ç—…ç†å­¦å®¶é€šå¸¸éœ€è¦åœ¨é«˜å€æ˜¾å¾®é•œä¸‹å¯¹è‹æœ¨ç²¾-ä¼Šçº¢æŸ“è‰²ï¼ˆH&Eï¼‰ç»„ç»‡åˆ‡ç‰‡è¿›è¡Œè¯¦å°½çš„æ£€æŸ¥å’Œè§£è¯»ï¼ŒåŒ…æ‹¬åŒºåˆ†è‚¿ç˜¤åŒºåŸŸä¸å¤§é¢ç§¯æ­£å¸¸ç»„ç»‡ã€å¯¹è‚¿ç˜¤è¿›è¡Œè¯¦ç»†åˆ†çº§ï¼Œä»¥åŠè¯„ä¼°è‚¿ç˜¤çš„è¿›å±•å’Œä¾µè¢­ï¼ˆä¾‹å¦‚ï¼Œä¾µè¢­æ€§ç™Œç—‡çš„å­˜åœ¨æˆ–å¢ç”Ÿæ€§å˜åŒ–ç­‰ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸è€—æ—¶å’ŒåŠ³åŠ¨å¯†é›†çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼Œç»éªŒä¸°å¯Œçš„ç»„ç»‡ç—…ç†å­¦å®¶é€šå¸¸éœ€è¦15åˆ°30åˆ†é’Ÿæ¥æ£€æŸ¥ä¸€ä¸ªå®Œæ•´çš„åˆ‡ç‰‡ï¼ˆWang
    *et al.* [2019](#bib.bib181)ï¼‰ã€‚æ­¤å¤–ï¼Œå³ä½¿æ˜¯ç»éªŒä¸°å¯Œçš„ç—…ç†å­¦å®¶ä¹Ÿå¯èƒ½æ— æ³•å‡†ç¡®ç¡®å®šç—…ç†å›¾åƒä¸­éšè—çš„æ·±å±‚ç‰¹å¾ï¼Œä¾‹å¦‚é¢„æµ‹æ·‹å·´ç»“è½¬ç§»å’ŒåŸå‘ç—…ç¶çš„é¢„åã€‚å› æ­¤ï¼Œä¸´åºŠå®è·µä¸­è¿«åˆ‡éœ€è¦è®¡ç®—æœºè¾…åŠ©çš„è‡ªåŠ¨åˆ†ææŠ€æœ¯æ¥å¤„ç†ç»„ç»‡ç—…ç†å›¾åƒã€‚
- en: With the advent and development of digital slide scanners in the past two decades,
    tissues on biopsies can be converted into digital whole slide images (WSIs) that
    fully preserve the original tissue structure, laying the foundation for automatic
    pathological image analysis. Early studies in the field of digital pathology diagnosis
    primarily focused on extracting hand-crafted features from manually selected regions
    of interest (ROI) by pathologists (Jafari *et al.*Â [2003](#bib.bib78), Basavanhally
    *et al.*Â [2013](#bib.bib7), Mercan *et al.*Â [2017](#bib.bib113), Yu *et al.*Â [2016](#bib.bib203),
    Luo *et al.*Â [2017](#bib.bib106), Qaiser *et al.*Â [2016](#bib.bib127)) and using
    machine learning methods (Doyle *et al.*Â [2007](#bib.bib52), Rajpoot *et al.*Â [2004](#bib.bib134),
    Qureshi *et al.*Â [2008](#bib.bib132), Doyle *et al.*Â [2006](#bib.bib53)) for automatic
    analysis and diagnosis. In this regard, Gurcan *et al.*Â [2009](#bib.bib69) and
    Madabhushi *et al.*Â [2016](#bib.bib107) have presented an elaborate review.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€è¿‡å»äºŒåå¹´æ•°å­—åˆ‡ç‰‡æ‰«æä»ªçš„å‡ºç°å’Œå‘å±•ï¼Œæ´»æ£€ç»„ç»‡å¯ä»¥è½¬æ¢ä¸ºå®Œå…¨ä¿ç•™åŸå§‹ç»„ç»‡ç»“æ„çš„æ•°å­—å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰ï¼Œä¸ºè‡ªåŠ¨ç—…ç†å›¾åƒåˆ†æå¥ å®šäº†åŸºç¡€ã€‚æ•°å­—ç—…ç†è¯Šæ–­é¢†åŸŸçš„æ—©æœŸç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä»ç—…ç†å­¦å®¶æ‰‹åŠ¨é€‰æ‹©çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ä¸­æå–æ‰‹å·¥ç‰¹å¾ï¼ˆJafari
    *et al.* [2003](#bib.bib78), Basavanhally *et al.* [2013](#bib.bib7), Mercan *et
    al.* [2017](#bib.bib113), Yu *et al.* [2016](#bib.bib203), Luo *et al.* [2017](#bib.bib106),
    Qaiser *et al.* [2016](#bib.bib127)ï¼‰ï¼Œå¹¶ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆDoyle *et al.* [2007](#bib.bib52),
    Rajpoot *et al.* [2004](#bib.bib134), Qureshi *et al.* [2008](#bib.bib132), Doyle
    *et al.* [2006](#bib.bib53)ï¼‰è¿›è¡Œè‡ªåŠ¨åˆ†æå’Œè¯Šæ–­ã€‚åœ¨è¿™æ–¹é¢ï¼ŒGurcan *et al.* [2009](#bib.bib69)
    å’Œ Madabhushi *et al.* [2016](#bib.bib107) æä¾›äº†è¯¦å°½çš„ç»¼è¿°ã€‚
- en: In recent years, thanks to the powerful and automatic feature extraction capability,
    deep learning methods represented by Convolutional Neural Network (CNN) have gradually
    become the mainstream in the field of digital pathology. However, a major challenge
    is the huge size of WSIs, typically reaching 100000$\times" display="inline"><semantics
    ><mo xref=$100000 pixels at the highest resolution, which prevents the direct
    use of the entire WSIs as the input to deep learning models. Therefore, when using
    CNNs to process pathological images, WSIs are usually tiled into many small patches
    to reduce the computational burden. Earlier studies usually adopted a strongly
    supervised approach based on these patches to train the network and perform the
    corresponding classification (Cruz-Roa *et al.*Â [2014](#bib.bib40), Cruz-Roa *et
    al.*Â [2017](#bib.bib41), Wei *et al.*Â [2019](#bib.bib185), Ehteshami *et al.*Â [2018](#bib.bib54),
    Nagpal *et al.*Â [2019](#bib.bib117), Shaban *et al.*Â [2019](#bib.bib145), Halicek
    *et al.*Â [2019](#bib.bib71)) and segmentation tasks (Chen *et al.*Â [2017](#bib.bib22),
    Gu *et al.*Â [2018](#bib.bib67), Swiderska *et al.*Â [2019](#bib.bib165)). In these
    works, detailed patch-level annotation is essential, e.g., supervised classification
    problems require pathologists to give detailed class labels for each patch, and
    segmentation problems require pathologists to give more detailed pixel-level annotation
    for each patch.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘å‡ å¹´ï¼Œç”±äºå¼ºå¤§çš„è‡ªåŠ¨ç‰¹å¾æå–èƒ½åŠ›ï¼Œä»¥å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸ºä»£è¡¨çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é€æ¸æˆä¸ºæ•°å­—ç—…ç†å­¦é¢†åŸŸçš„ä¸»æµã€‚ç„¶è€Œï¼Œä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯WSIsçš„å·¨å¤§å°ºå¯¸ï¼Œé€šå¸¸åœ¨æœ€é«˜åˆ†è¾¨ç‡ä¸‹è¾¾åˆ°`100000`åƒç´ ï¼Œè¿™é˜»ç¢äº†å°†æ•´ä¸ªWSIsç›´æ¥ä½œä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹è¾“å…¥ã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨CNNå¤„ç†ç—…ç†å›¾åƒæ—¶ï¼ŒWSIsé€šå¸¸ä¼šè¢«åˆ‡å‰²æˆè®¸å¤šå°å—ï¼Œä»¥å‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚æ—©æœŸçš„ç ”ç©¶é€šå¸¸é‡‡ç”¨åŸºäºè¿™äº›å°å—çš„å¼ºç›‘ç£æ–¹æ³•æ¥è®­ç»ƒç½‘ç»œå¹¶æ‰§è¡Œç›¸åº”çš„åˆ†ç±»ï¼ˆCruz-Roa
    *et al.* [2014](#bib.bib40), Cruz-Roa *et al.* [2017](#bib.bib41), Wei *et al.*
    [2019](#bib.bib185), Ehteshami *et al.* [2018](#bib.bib54), Nagpal *et al.* [2019](#bib.bib117),
    Shaban *et al.* [2019](#bib.bib145), Halicek *et al.* [2019](#bib.bib71)ï¼‰å’Œåˆ†å‰²ä»»åŠ¡ï¼ˆChen
    *et al.* [2017](#bib.bib22), Gu *et al.* [2018](#bib.bib67), Swiderska *et al.*
    [2019](#bib.bib165)ï¼‰ã€‚åœ¨è¿™äº›å·¥ä½œä¸­ï¼Œè¯¦ç»†çš„å°å—çº§æ³¨é‡Šæ˜¯å¿…ä¸å¯å°‘çš„ï¼Œä¾‹å¦‚ï¼Œç›‘ç£åˆ†ç±»é—®é¢˜éœ€è¦ç—…ç†å­¦å®¶ä¸ºæ¯ä¸ªå°å—æä¾›è¯¦ç»†çš„ç±»åˆ«æ ‡ç­¾ï¼Œè€Œåˆ†å‰²é—®é¢˜éœ€è¦ç—…ç†å­¦å®¶ä¸ºæ¯ä¸ªå°å—æä¾›æ›´è¯¦ç»†çš„åƒç´ çº§æ³¨é‡Šã€‚
- en: 'Although supervised deep learning methods have achieved unprecedented success
    in digital pathology, they share a common drawback: they all require large amounts
    of high-quality fine-grained labeled data (patch-level labeled data for classification
    problems or pixel-level labeled data for segmentation problems) for training.
    Unfortunately, in the field of digital pathology, obtaining a large amount of
    data with fine-grained annotation is a very expensive and challenging task, mainly
    because 1) only experienced pathologists can perform the annotation, and these
    pathologists are scarce; 2) histopathological images often contain complex and
    diverse instances of objects, resulting in a large amount of time-consuming and
    laborious manual annotation effort (Tajbakhsh *et al.*Â [2020](#bib.bib166), Yang
    *et al.*Â [2017](#bib.bib199), Srinidhi *et al.*Â [2021](#bib.bib159)). Arguably,
    the lack of a large amount of annotated data limits the application of deep learning
    techniques in computational pathology. For this reason, some new studies have
    recently attempted to liberate from the traditional strongly supervised paradigms,
    the most representative of which are the weakly supervised learning paradigm based
    on weak annotations, the semi-supervised learning paradigm based on limited annotations,
    and the self-supervised paradigm based on the representation learning of pathological
    images.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç›‘ç£æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ•°å­—ç—…ç†å­¦ä¸­å–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸï¼Œä½†å®ƒä»¬éƒ½æœ‰ä¸€ä¸ªå…±åŒçš„ç¼ºç‚¹ï¼šéƒ½éœ€è¦å¤§é‡é«˜è´¨é‡çš„ç»†ç²’åº¦æ ‡æ³¨æ•°æ®ï¼ˆåˆ†ç±»é—®é¢˜çš„å—çº§æ ‡æ³¨æ•°æ®æˆ–åˆ†å‰²é—®é¢˜çš„åƒç´ çº§æ ‡æ³¨æ•°æ®ï¼‰è¿›è¡Œè®­ç»ƒã€‚ä¸å¹¸çš„æ˜¯ï¼Œåœ¨æ•°å­—ç—…ç†å­¦é¢†åŸŸï¼Œè·å–å¤§é‡ç»†ç²’åº¦æ³¨é‡Šçš„æ•°æ®æ˜¯ä¸€é¡¹éå¸¸æ˜‚è´µä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸»è¦å› ä¸º1ï¼‰åªæœ‰ç»éªŒä¸°å¯Œçš„ç—…ç†å­¦å®¶æ‰èƒ½è¿›è¡Œæ³¨é‡Šï¼Œè€Œè¿™äº›ç—…ç†å­¦å®¶ç¨€ç¼ºï¼›2ï¼‰ç»„ç»‡ç—…ç†å›¾åƒé€šå¸¸åŒ…å«å¤æ‚å¤šæ ·çš„å¯¹è±¡å®ä¾‹ï¼Œå¯¼è‡´è€—æ—¶ä¸”åŠ³åŠ¨å¯†é›†çš„æ‰‹åŠ¨æ³¨é‡Šå·¥ä½œï¼ˆTajbakhsh
    *et al.* [2020](#bib.bib166), Yang *et al.* [2017](#bib.bib199), Srinidhi *et
    al.* [2021](#bib.bib159)ï¼‰ã€‚å¯ä»¥è¯´ï¼Œç¼ºä¹å¤§é‡æ³¨é‡Šæ•°æ®é™åˆ¶äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨è®¡ç®—ç—…ç†å­¦ä¸­çš„åº”ç”¨ã€‚å› æ­¤ï¼Œä¸€äº›æ–°ç ”ç©¶æœ€è¿‘å°è¯•æ‘†è„±ä¼ ç»Ÿçš„å¼ºç›‘ç£èŒƒå¼ï¼Œå…¶ä¸­æœ€å…·ä»£è¡¨æ€§çš„æ˜¯åŸºäºå¼±æ³¨é‡Šçš„å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ã€åŸºäºæœ‰é™æ³¨é‡Šçš„åŠç›‘ç£å­¦ä¹ èŒƒå¼å’ŒåŸºäºç—…ç†å›¾åƒè¡¨ç¤ºå­¦ä¹ çš„è‡ªç›‘ç£èŒƒå¼ã€‚
- en: The weakly supervised learning paradigm no longer requires pathologists to give
    annotations of all pixels or regions on the entire WSI, but only class labels
    or sparse region annotations on the entire WSI; the semi-supervised learning paradigm
    no longer requires pathologists to give fine-grained annotations of a large amount
    of data, but only a small fraction of fine-grained labeled data and a large amount
    of unlabeled data; while the self-supervised learning paradigm can create supervised
    information through a large amount of unlabeled data for self-supervised training
    to learn an accurate feature representation of the data. In the process of training
    with limited labeled data, using the features trained by self-supervised learning
    to determine the initial model weights can significantly improve the performance
    of the model. Therefore, weakly supervised learning, semi-supervised learning
    and self-supervised learning are leading a new study direction of the automatic
    diagnosis and analysis for pathological images.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ä¸å†è¦æ±‚ç—…ç†å­¦å®¶å¯¹æ•´ä¸ªWSIä¸­çš„æ‰€æœ‰åƒç´ æˆ–åŒºåŸŸè¿›è¡Œæ ‡æ³¨ï¼Œåªéœ€å¯¹æ•´ä¸ªWSIè¿›è¡Œç±»åˆ«æ ‡ç­¾æˆ–ç¨€ç–åŒºåŸŸæ ‡æ³¨ï¼›åŠç›‘ç£å­¦ä¹ èŒƒå¼ä¸å†è¦æ±‚ç—…ç†å­¦å®¶å¯¹å¤§é‡æ•°æ®è¿›è¡Œç»†ç²’åº¦æ ‡æ³¨ï¼Œåªéœ€å°‘é‡ç»†ç²’åº¦æ ‡æ³¨æ•°æ®å’Œå¤§é‡æœªæ ‡æ³¨æ•°æ®ï¼›è€Œè‡ªç›‘ç£å­¦ä¹ èŒƒå¼åˆ™å¯ä»¥é€šè¿‡å¤§é‡æœªæ ‡æ³¨æ•°æ®åˆ›å»ºç›‘ç£ä¿¡æ¯è¿›è¡Œè‡ªç›‘ç£è®­ç»ƒï¼Œä»è€Œå­¦ä¹ æ•°æ®çš„å‡†ç¡®ç‰¹å¾è¡¨ç¤ºã€‚åœ¨æœ‰é™æ ‡æ³¨æ•°æ®çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒçš„ç‰¹å¾æ¥ç¡®å®šåˆå§‹æ¨¡å‹æƒé‡ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚å› æ­¤ï¼Œå¼±ç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ æ­£åœ¨å¼•é¢†ç—…ç†å›¾åƒè‡ªåŠ¨è¯Šæ–­ä¸åˆ†æçš„æ–°ç ”ç©¶æ–¹å‘ã€‚
- en: However, there are very few related reviews. Srinidhi *et al.*Â [2021](#bib.bib159)
    reviewed representative supervised learning, weakly supervised learning, unsupervised
    learning, and transfer learning studies in the field of computational pathology
    until December 2019\. Rony *et al.*Â [2019](#bib.bib139) reviewed representative
    weakly supervised learning studies until 2020\. Nevertheless, in recent years,
    deep learning techniques have been developing rapidly and the new techniques continue
    to emerge. Therefore, a review regarding the applications of these techniques
    in the automatic diagnosis of pathological images has important theoretical value
    and clinical significance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç›¸å…³çš„ç»¼è¿°éå¸¸å°‘ã€‚Srinidhi *ç­‰äºº* [2021](#bib.bib159) å›é¡¾äº†æˆªè‡³2019å¹´12æœˆåœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„ä»£è¡¨æ€§ç›‘ç£å­¦ä¹ ã€å¼±ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œè¿ç§»å­¦ä¹ ç ”ç©¶ã€‚Rony
    *ç­‰äºº* [2019](#bib.bib139) å›é¡¾äº†æˆªè‡³2020å¹´çš„ä»£è¡¨æ€§å¼±ç›‘ç£å­¦ä¹ ç ”ç©¶ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¿‘å¹´æ¥æ·±åº¦å­¦ä¹ æŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œæ–°æŠ€æœ¯ä¸æ–­æ¶Œç°ã€‚å› æ­¤ï¼Œå…³äºè¿™äº›æŠ€æœ¯åœ¨ç—…ç†å›¾åƒè‡ªåŠ¨è¯Šæ–­ä¸­çš„åº”ç”¨çš„ç»¼è¿°å…·æœ‰é‡è¦çš„ç†è®ºä»·å€¼å’Œä¸´åºŠæ„ä¹‰ã€‚
- en: In this review, we summarize more than 130 recent technical studies systematically
    on weakly supervised learning, semi-supervised learning, and self-supervised learning
    in the field of computational pathology. We performed this extensive review by
    searching Google Scholar, PubMed, and arXiv for papers including keywords such
    as (â€deep learningâ€ or â€weakly supervised learningâ€ or â€semi-supervised learningâ€
    or â€self-supervised learning â€) and (â€digital pathologyâ€ or â€histopathologyâ€ or
    â€computational pathologyâ€). Notably, on the one hand, we focus on papers presenting
    novel techniques and theories with high impact (h-index, citations and impact
    factors of journals), thus we concentrate more on studies published in top conferences
    (including CVPR, NeurIPS, MICCAI, ISBI, MIDL, IPMI, AAAI, ICCV, ECCV, etc.) and
    top journals (including TPAMI, TMI, MIA, etc.) on weakly supervised, semi-supervised,
    and self-supervised learning in the field of computational pathology. On the other
    hand, since technical research in this area is growing rapidly and more new techniques
    have been proposed, we mainly cover papers published in 2019-2021\. On the other
    hand, we also present a meticulous summary of the disease types, tasks, datasets,
    and performance covered by these papers. In total, this review contains more than
    200 relevant references.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ€»ç»“äº†130å¤šé¡¹è¿‘æœŸåœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„å¼±ç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ çš„æŠ€æœ¯ç ”ç©¶ã€‚æˆ‘ä»¬é€šè¿‡æœç´¢Google Scholarã€PubMedå’ŒarXivï¼ŒæŸ¥æ‰¾åŒ…å«ï¼ˆâ€œæ·±åº¦å­¦ä¹ â€æˆ–â€œå¼±ç›‘ç£å­¦ä¹ â€æˆ–â€œåŠç›‘ç£å­¦ä¹ â€æˆ–â€œè‡ªç›‘ç£å­¦ä¹ â€ï¼‰ä»¥åŠï¼ˆâ€œæ•°å­—ç—…ç†å­¦â€æˆ–â€œç»„ç»‡ç—…ç†å­¦â€æˆ–â€œè®¡ç®—ç—…ç†å­¦â€ï¼‰ç­‰å…³é”®è¯çš„è®ºæ–‡æ¥è¿›è¡Œè¿™é¡¹å¹¿æ³›çš„ç»¼è¿°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸€æ–¹é¢ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨é‚£äº›å‘ˆç°å‡ºé«˜å½±å“åŠ›çš„æ–°æŠ€æœ¯å’Œç†è®ºçš„è®ºæ–‡ï¼ˆå¦‚hæŒ‡æ•°ã€å¼•ç”¨æ¬¡æ•°å’ŒæœŸåˆŠçš„å½±å“å› å­ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬æ›´åŠ å…³æ³¨åœ¨é¡¶çº§ä¼šè®®ï¼ˆåŒ…æ‹¬CVPRã€NeurIPSã€MICCAIã€ISBIã€MIDLã€IPMIã€AAAIã€ICCVã€ECCVç­‰ï¼‰å’Œé¡¶çº§æœŸåˆŠï¼ˆåŒ…æ‹¬TPAMIã€TMIã€MIAç­‰ï¼‰ä¸Šå‘è¡¨çš„å…³äºè®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„å¼±ç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ çš„ç ”ç©¶ã€‚å¦ä¸€æ–¹é¢ï¼Œç”±äºè¿™ä¸€é¢†åŸŸçš„æŠ€æœ¯ç ”ç©¶å‘å±•è¿…é€Ÿï¼Œæå‡ºäº†æ›´å¤šçš„æ–°æŠ€æœ¯ï¼Œæˆ‘ä»¬ä¸»è¦æ¶µç›–äº†2019å¹´è‡³2021å¹´é—´å‘è¡¨çš„è®ºæ–‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹è¿™äº›è®ºæ–‡æ¶µç›–çš„ç–¾ç—…ç±»å‹ã€ä»»åŠ¡ã€æ•°æ®é›†å’Œæ€§èƒ½è¿›è¡Œäº†ç»†è‡´çš„æ€»ç»“ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™ç¯‡ç»¼è¿°åŒ…å«äº†200å¤šç¯‡ç›¸å…³å‚è€ƒæ–‡çŒ®ã€‚
- en: 'The rest of the paper is organized as follows: Section [2](#S2 "2 Overview
    of Learning Paradigms and Problem Formulation â€£ Towards Label-efficient Automatic
    Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis") expounds a general overview of the weakly supervised, semi-supervised,
    and self-supervised learning paradigms in the context of computational pathology;
    Section [3](#S3 "3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and
    Analysis: A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised,
    Semi-supervised and Self-supervised Techniques in Histopathological Image Analysis")
    includes a detailed review of the weakly supervised (Section [3.1](#S3.SS1 "3.1
    Weakly Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic
    Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis")), semi-supervised (Section [3.2](#S3.SS2 "3.2 Semi-Supervised
    Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis
    and Analysis: A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised,
    Semi-supervised and Self-supervised Techniques in Histopathological Image Analysis")),
    and self-supervised (Section [3.3](#S3.SS3 "3.3 Self-Supervised Learning Paradigm
    â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive
    Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and
    Self-supervised Techniques in Histopathological Image Analysis")) learning paradigms;
    We discuss the three learning paradigms and their future trends in Section [4](#S4
    "4 Discussion and Future Trends â€£ Towards Label-efficient Automatic Diagnosis
    and Analysis: A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised,
    Semi-supervised and Self-supervised Techniques in Histopathological Image Analysis"),
    and conclude the whole paper in Section [5](#S5 "5 Conclusion â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis"). The list of all the acronyms used in this review is shown in
    Table [1](#S1.T1 "Table 1 â€£ 1 Introduction â€£ Towards Label-efficient Automatic
    Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis").'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡çš„å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ï¼šç¬¬[2](#S2 "2 ç›‘ç£å­¦ä¹ å’Œé—®é¢˜è¡¨è¿°æ¦‚è¿° â€£ é¢å‘æ ‡ç­¾æ•ˆç‡çš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢ç»¼è¿°")èŠ‚é˜è¿°äº†è®¡ç®—ç—…ç†å­¦èƒŒæ™¯ä¸‹çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£å­¦ä¹ èŒƒå¼çš„ä¸€èˆ¬æ¦‚è¿°ï¼›ç¬¬[3](#S3
    "3 èŒƒå¼ â€£ é¢å‘æ ‡ç­¾æ•ˆç‡çš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢ç»¼è¿°")èŠ‚åŒ…æ‹¬å¯¹å¼±ç›‘ç£ï¼ˆç¬¬[3.1](#S3.SS1
    "3.1 å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ â€£ 3 èŒƒå¼ â€£ é¢å‘æ ‡ç­¾æ•ˆç‡çš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢ç»¼è¿°")èŠ‚ï¼‰ã€åŠç›‘ç£ï¼ˆç¬¬[3.2](#S3.SS2
    "3.2 åŠç›‘ç£å­¦ä¹ èŒƒå¼ â€£ 3 èŒƒå¼ â€£ é¢å‘æ ‡ç­¾æ•ˆç‡çš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢ç»¼è¿°")èŠ‚ï¼‰å’Œè‡ªç›‘ç£ï¼ˆç¬¬[3.3](#S3.SS3
    "3.3 è‡ªç›‘ç£å­¦ä¹ èŒƒå¼ â€£ 3 èŒƒå¼ â€£ é¢å‘æ ‡ç­¾æ•ˆç‡çš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢ç»¼è¿°")èŠ‚ï¼‰å­¦ä¹ èŒƒå¼çš„è¯¦ç»†å›é¡¾ï¼›æˆ‘ä»¬åœ¨ç¬¬[4](#S4
    "4 è®¨è®ºä¸æœªæ¥è¶‹åŠ¿ â€£ é¢å‘æ ‡ç­¾æ•ˆç‡çš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢ç»¼è¿°")èŠ‚è®¨è®ºäº†è¿™ä¸‰ç§å­¦ä¹ èŒƒå¼åŠå…¶æœªæ¥è¶‹åŠ¿ï¼Œå¹¶åœ¨ç¬¬[5](#S5
    "5 ç»“è®º â€£ é¢å‘æ ‡ç­¾æ•ˆç‡çš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢ç»¼è¿°")èŠ‚æ€»ç»“äº†æ•´ç¯‡è®ºæ–‡ã€‚æœ¬ç»¼è¿°ä¸­ä½¿ç”¨çš„æ‰€æœ‰ç¼©å†™çš„åˆ—è¡¨è§è¡¨[1](#S1.T1
    "è¡¨ 1 â€£ 1 å¼•è¨€ â€£ é¢å‘æ ‡ç­¾æ•ˆç‡çš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢ç»¼è¿°")ã€‚
- en: 'Table 1: List of all the acronyms in this review.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 1: æœ¬ç»¼è¿°ä¸­æ‰€æœ‰ç¼©å†™çš„åˆ—è¡¨ã€‚'
- en: '| Full Name | Acronyms | Full Name | Acronyms |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| å…¨ç§° | ç¼©å†™ | å…¨ç§° | ç¼©å†™ |'
- en: '| Area Under ROC Curve | AUC | Graph Neural Network | GNN |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| ROCæ›²çº¿ä¸‹é¢ç§¯ | AUC | å›¾ç¥ç»ç½‘ç»œ | GNN |'
- en: '| Auxiliary Classier Generative Adversarial Networks | AC-GAN | Hematoxylin-Eosin-Stained
    | H&E |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| è¾…åŠ©åˆ†ç±»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ | AC-GAN | è‹æœ¨ç²¾-ä¼Šçº¢æŸ“è‰² | H&E |'
- en: '| Average Hausdorff Distance | AHD | Magnication Prior Contrastive Similarity
    | MPCS |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| å¹³å‡Hausdorffè·ç¦» | AHD | æ”¾å¤§ä¼˜å…ˆå¯¹æ¯”ç›¸ä¼¼åº¦ | MPCS |'
- en: '| Average Jaccard Index | AJI | Mean Average Precision | MAP |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| å¹³å‡JaccardæŒ‡æ•° | AJI | å¹³å‡ç²¾åº¦å‡å€¼ | MAP |'
- en: '| Calinski-Harabaz Index | CHI | Mean Teachers | MT |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Calinski-HarabazæŒ‡æ•° | CHI | å¹³å‡æ•™å¸ˆ | MT |'
- en: '| Contrastive Predictive Coding | CPC | Microsatellite Instability | MSI |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| å¯¹æ¯”é¢„æµ‹ç¼–ç  | CPC | å¾®å«æ˜Ÿä¸ç¨³å®šæ€§ | MSI |'
- en: '| Convolutional Autoencoder | CAE | Multiple Instance Fully Convolutional Network
    | MI-FCN |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| å·ç§¯è‡ªç¼–ç å™¨ | CAE | å¤šå®ä¾‹å…¨å·ç§¯ç½‘ç»œ | MI-FCN |'
- en: '| Convolutional Neural Network | CNN | Multiple Instance Learning | MIL |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| å·ç§¯ç¥ç»ç½‘ç»œ | CNN | å¤šå®ä¾‹å­¦ä¹  | MIL |'
- en: '| Deep Learning Hashing | DLH | Noise Contrastive Estimation | NCE |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| æ·±åº¦å­¦ä¹ å“ˆå¸Œ | DLH | å™ªå£°å¯¹æ¯”ä¼°è®¡ | NCE |'
- en: '| Deformation Representation Learning | DRL | Percentage Of Tumor Cellularity
    | TC |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| å˜å½¢è¡¨ç¤ºå­¦ä¹  | DRL | è‚¿ç˜¤ç»†èƒç™¾åˆ†æ¯” | TC |'
- en: '| Diffusion-Convolutional Neural Networks | DCNNs | Recurrent Neural Network
    | RNN |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| æ‰©æ•£å·ç§¯ç¥ç»ç½‘ç»œ | DCNNs | å¾ªç¯ç¥ç»ç½‘ç»œ | RNN |'
- en: '| Dual-Stream Multiple Instance Learning | DSMIL | Regions Of Interest | ROI
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| åŒæµå¤šå®ä¾‹å­¦ä¹  | DSMIL | æ„Ÿå…´è¶£åŒºåŸŸ | ROI |'
- en: '| Expectation-Maximization | EM | Resolution Sequence Prediction | RSP |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| æœŸæœ›æœ€å¤§åŒ– | EM | åˆ†è¾¨ç‡åºåˆ—é¢„æµ‹ | RSP |'
- en: '| Exponential Moving Average | EMA | Silhouette Index | SI |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| æŒ‡æ•°ç§»åŠ¨å¹³å‡ | EMA | è½®å»“æŒ‡æ•° | SI |'
- en: '| Focal-Aware Module | FAM | Support Vector Machines | SVM |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ç„¦ç‚¹æ„ŸçŸ¥æ¨¡å— | FAM | æ”¯æŒå‘é‡æœº | SVM |'
- en: '| Frechet Inception Distance | FID | Temporal Ensembling | TE |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Frechetæ„ŸçŸ¥è·ç¦» | FID | æ—¶é—´é›†æˆ | TE |'
- en: '| Generative Adversarial Networks | GAN | The Cancer Genome Atlas Program |
    TCGA |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ | GAN | ç™Œç—‡åŸºå› ç»„å›¾è°±è®¡åˆ’ | TCGA |'
- en: '| Graph Convolutional Neural Network | GCN | Whole Slide Images | WSI |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| å›¾å·ç§¯ç¥ç»ç½‘ç»œ | GCN | æ•´å¼ å¹»ç¯ç‰‡å›¾åƒ | WSI |'
- en: 2 Overview of Learning Paradigms and Problem Formulation
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 å­¦ä¹ èŒƒå¼å’Œé—®é¢˜å®šä¹‰æ¦‚è¿°
- en: 'In this section, we provide a general overview and problem formulation of the
    three learning paradigms reviewed in this paper, and compare them with the traditional
    strongly supervised paradigm. To make the description more specific and vivid,
    we present an example of accurately classifying normal and cancerous tissues in
    a WSI, as shown in Figure [1](#S2.F1 "Figure 1 â€£ 2 Overview of Learning Paradigms
    and Problem Formulation â€£ Towards Label-efficient Automatic Diagnosis and Analysis:
    A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised
    and Self-supervised Techniques in Histopathological Image Analysis"). The raw
    data for this example WSI comes from a study on predicting lymph node metastasis
    in breast cancer using deep learning (Bejnordi *et al.*Â [2017a](#bib.bib9)). We
    also intuitively compare and summarize these paradigms in Table [2](#S2.T2 "Table
    2 â€£ 2 Overview of Learning Paradigms and Problem Formulation â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†æœ¬æ–‡å›é¡¾çš„ä¸‰ç§å­¦ä¹ èŒƒå¼çš„æ€»ä½“æ¦‚è¿°å’Œé—®é¢˜å®šä¹‰ï¼Œå¹¶å°†å®ƒä»¬ä¸ä¼ ç»Ÿçš„å¼ºç›‘ç£èŒƒå¼è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºäº†ä½¿æè¿°æ›´å…·ä½“å’Œç”ŸåŠ¨ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªåœ¨WSIä¸­å‡†ç¡®åˆ†ç±»æ­£å¸¸ç»„ç»‡å’Œç™Œç»„ç»‡çš„ç¤ºä¾‹ï¼Œå¦‚å›¾[1](#S2.F1
    "å›¾ 1 â€£ 2 å­¦ä¹ èŒƒå¼å’Œé—®é¢˜å®šä¹‰æ¦‚è¿° â€£ é¢å‘æ ‡ç­¾æ•ˆç‡è‡ªåŠ¨è¯Šæ–­å’Œåˆ†æï¼šå…ˆè¿›æ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„ç»¼åˆè°ƒæŸ¥")æ‰€ç¤ºã€‚è¯¥ç¤ºä¾‹WSIçš„åŸå§‹æ•°æ®æ¥è‡ªäºä¸€é¡¹ä½¿ç”¨æ·±åº¦å­¦ä¹ é¢„æµ‹ä¹³è…ºç™Œæ·‹å·´ç»“è½¬ç§»çš„ç ”ç©¶ï¼ˆBejnordi
    *et al.* [2017a](#bib.bib9)ï¼‰ã€‚æˆ‘ä»¬è¿˜åœ¨è¡¨[2](#S2.T2 "è¡¨ 2 â€£ 2 å­¦ä¹ èŒƒå¼å’Œé—®é¢˜å®šä¹‰æ¦‚è¿° â€£ é¢å‘æ ‡ç­¾æ•ˆç‡è‡ªåŠ¨è¯Šæ–­å’Œåˆ†æï¼šå…ˆè¿›æ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„ç»¼åˆè°ƒæŸ¥")ä¸­ç›´è§‚åœ°æ¯”è¾ƒå’Œæ€»ç»“äº†è¿™äº›èŒƒå¼ã€‚
- en: 'For the dataset <math   alttext="W={\{W_{i}\}}_{i=1}^{N}" display="inline"><semantics
    ><mrow  ><mi >W</mi><mo >=</mo><msubsup  ><mrow ><mo stretchy="false" >{</mo><msub
    ><mi  >W</mi><mi >i</mi></msub><mo stretchy="false"  >}</mo></mrow><mrow ><mi
    >i</mi><mo  >=</mo><mn >1</mn></mrow><mi >N</mi></msubsup></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><ci >ğ‘Š</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><set ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Š</ci><ci >ğ‘–</ci></apply></set><apply ><ci >ğ‘–</ci><cn type="integer" >1</cn></apply></apply><ci
    >ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >W={\{W_{i}\}}_{i=1}^{N}</annotation></semantics></math> consisting of <math   alttext="N"
    display="inline"><semantics ><mi  >N</mi><annotation-xml encoding="MathML-Content"
    ><ci  >ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" >N</annotation></semantics></math>
    WSIs, each WSI <math   alttext="W_{i}" display="inline"><semantics ><msub  ><mi
    >W</mi><mi  >i</mi></msub><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘Š</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" >W_{i}</annotation></semantics></math> is now cut
    into patches <math   alttext="\{p_{i,j},j=1,2,\ldots n_{i}\}" display="inline"><semantics
    ><mrow  ><mo stretchy="false"  >{</mo><mrow ><mrow ><mrow  ><msub ><mi >p</mi><mrow
    ><mi >i</mi><mo  >,</mo><mi >j</mi></mrow></msub><mo >,</mo><mi >j</mi></mrow><mo
    >=</mo><mn  >1</mn></mrow><mo >,</mo><mrow ><mn  >2</mn><mo >,</mo><mrow ><mi
    mathvariant="normal" >â€¦</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >n</mi><mi
    >i</mi></msub></mrow></mrow></mrow><mo stretchy="false"  >}</mo></mrow><annotation-xml
    encoding="MathML-Content" ><set  ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><list ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><list ><ci  >ğ‘–</ci><ci
    >ğ‘—</ci></list></apply><ci >ğ‘—</ci></list><cn type="integer" >1</cn></apply><list
    ><cn type="integer" >2</cn><apply ><ci  >â€¦</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘›</ci><ci >ğ‘–</ci></apply></apply></list></apply></set></annotation-xml><annotation
    encoding="application/x-tex" >\{p_{i,j},j=1,2,\ldots n_{i}\}</annotation></semantics></math>,
    and <math   alttext="n_{i}" display="inline"><semantics ><msub  ><mi >n</mi><mi  >i</mi></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘›</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >n_{i}</annotation></semantics></math> is the number of patches cut out of <math   alttext="W_{i}"
    display="inline"><semantics ><msub  ><mi >W</mi><mi  >i</mi></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Š</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >W_{i}</annotation></semantics></math>. In the supervised learning paradigm, a
    large number of patches with fine-grained labels are available for training, so
    each patch is given a label <math alttext="y_{i,j}\in\mathbb{R}^{C}" display="inline"><semantics
    ><mrow ><msub  ><mi >y</mi><mrow ><mi  >i</mi><mo >,</mo><mi >j</mi></mrow></msub><mo  >âˆˆ</mo><msup
    ><mi >â„</mi><mi  >C</mi></msup></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >ğ‘¦</ci><list
    ><ci >ğ‘–</ci><ci  >ğ‘—</ci></list></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >â„</ci><ci  >ğ¶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >y_{i,j}\in\mathbb{R}^{C}</annotation></semantics></math>, and <math   alttext="C"
    display="inline"><semantics ><mi  >C</mi><annotation-xml encoding="MathML-Content"
    ><ci  >ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" >C</annotation></semantics></math>
    denotes the possible class. For example, in the binary classification task, <math
    alttext="C=2" display="inline"><semantics ><mrow ><mi  >C</mi><mo >=</mo><mn >2</mn></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><ci >ğ¶</ci><cn type="integer"  >2</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" >C=2</annotation></semantics></math> and the label
    takes the scalar form {0, 1} while in the regression task, <math alttext="C" display="inline"><semantics
    ><mi >C</mi><annotation-xml encoding="MathML-Content" ><ci  >ğ¶</ci></annotation-xml><annotation
    encoding="application/x-tex" >C</annotation></semantics></math> takes the form
    of a continuous set of real numbers $\mathbb{R}" display="inline"><semantics ><mi
    >â„</mi><annotation-xml encoding="MathML-Content" ><ci >â„</ci></annotation-xml><annotation
    encoding=$. The goal of the supervised learning paradigm is to train a model <math
    alttext="f_{\theta}:x\rightarrow y" display="inline"><semantics ><mrow  ><msub
    ><mi >f</mi><mi  >Î¸</mi></msub><mo lspace="0.278em" rspace="0.278em"  >:</mo><mrow
    ><mi >x</mi><mo stretchy="false" >â†’</mo><mi  >y</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><ci  >:</ci><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘“</ci><ci >ğœƒ</ci></apply><apply ><ci  >â†’</ci><ci >ğ‘¥</ci><ci
    >ğ‘¦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >f_{\theta}:x\rightarrow y</annotation></semantics></math> to optimally predict
    the labels <math   alttext="y_{i,j}" display="inline"><semantics ><msub  ><mi
    >y</mi><mrow ><mi  >i</mi><mo >,</mo><mi >j</mi></mrow></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘¦</ci><list ><ci  >ğ‘–</ci><ci >ğ‘—</ci></list></apply></annotation-xml><annotation
    encoding="application/x-tex" >y_{i,j}</annotation></semantics></math> of the unknown
    patches <math   alttext="p_{i,j}" display="inline"><semantics ><msub  ><mi >p</mi><mrow
    ><mi  >i</mi><mo >,</mo><mi >j</mi></mrow></msub><annotation-xml encoding="MathML-Content"
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><list ><ci  >ğ‘–</ci><ci
    >ğ‘—</ci></list></apply></annotation-xml><annotation encoding="application/x-tex"
    >p_{i,j}</annotation></semantics></math> in the test WSI based on the loss function
    $\mathcal{L}" display="inline"><semantics ><mi >â„’</mi><annotation-xml encoding="MathML-Content"
    ><ci  >â„’</ci></annotation-xml><annotation encoding=$. Figure [1](#S2.F1 "Figure
    1 â€£ 2 Overview of Learning Paradigms and Problem Formulation â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis") (a) illustrates the main process of this paradigm. During training,
    the model is trained in a supervised manner using patches cut out of the training
    WSIs and their labels (green for negative and red for positive) by pathologists;
    during testing, the trained model is used to predict the labels of the patches
    cut out of the unseen test WSIs.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°æ®é›†<math   alttext="W={\{W_{i}\}}_{i=1}^{N}" display="inline"><semantics ><mrow  ><mi
    >W</mi><mo >=</mo><msubsup  ><mrow ><mo stretchy="false" >{</mo><msub ><mi  >W</mi><mi
    >i</mi></msub><mo stretchy="false"  >}</mo></mrow><mrow ><mi >i</mi><mo  >=</mo><mn
    >1</mn></mrow><mi >N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><ci >ğ‘Š</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><set ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Š</ci><ci >ğ‘–</ci></apply></set><apply ><ci >ğ‘–</ci><cn type="integer" >1</cn></apply></apply><ci
    >ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >W={\{W_{i}\}}_{i=1}^{N}</annotation></semantics></math>åŒ…å«<math   alttext="N"
    display="inline"><semantics ><mi  >N</mi><annotation-xml encoding="MathML-Content"
    ><ci  >ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" >N</annotation></semantics></math>ä¸ªWSIï¼Œæ¯ä¸ªWSI
    <math   alttext="W_{i}" display="inline"><semantics ><msub  ><mi >W</mi><mi  >i</mi></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Š</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >W_{i}</annotation></semantics></math> ç°åœ¨è¢«åˆ‡å‰²æˆè¡¥ä¸<math   alttext="\{p_{i,j},j=1,2,\ldots
    n_{i}\}" display="inline"><semantics ><mrow  ><mo stretchy="false"  >{</mo><mrow
    ><mrow ><mrow  ><msub ><mi >p</mi><mrow ><mi >i</mi><mo  >,</mo><mi >j</mi></mrow></msub><mo
    >,</mo><mi >j</mi></mrow><mo >=</mo><mn  >1</mn></mrow><mo >,</mo><mrow ><mn  >2</mn><mo
    >,</mo><mrow ><mi mathvariant="normal" >â€¦</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >n</mi><mi >i</mi></msub></mrow></mrow></mrow><mo stretchy="false"  >}</mo></mrow><annotation-xml
    encoding="MathML-Content" ><set  ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><list ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><list ><ci  >ğ‘–</ci><ci
    >ğ‘—</ci></list></apply><ci >ğ‘—</ci></list><cn type="integer" >1</cn></apply><list
    ><cn type="integer" >2</cn><apply ><ci  >â€¦</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘›</ci><ci >ğ‘–</ci></apply></apply></list></apply></set></annotation-xml><annotation
    encoding="application/x-tex" >\{p_{i,j},j=1,2,\ldots n_{i}\}</annotation></semantics></math>ï¼Œå…¶ä¸­<math   alttext="n_{i}"
    display="inline"><semantics ><msub  ><mi >n</mi><mi  >i</mi></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘›</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >n_{i}</annotation></semantics></math>æ˜¯ä»<math   alttext="W_{i}" display="inline"><semantics
    ><msub  ><mi >W</mi><mi  >i</mi></msub><annotation-xml encoding="MathML-Content"
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘Š</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" >W_{i}</annotation></semantics></math>ä¸­åˆ‡å‰²å‡ºçš„è¡¥ä¸æ•°é‡ã€‚åœ¨ç›‘ç£å­¦ä¹ èŒƒå¼ä¸­ï¼Œå¤§é‡å¸¦æœ‰ç»†ç²’åº¦æ ‡ç­¾çš„è¡¥ä¸ç”¨äºè®­ç»ƒï¼Œå› æ­¤æ¯ä¸ªè¡¥ä¸éƒ½è¢«èµ‹äºˆä¸€ä¸ªæ ‡ç­¾<math
    alttext="y_{i,j}\in\mathbb{R}^{C}" display="inline"><semantics ><mrow ><msub  ><mi
    >y</mi><mrow ><mi  >i</mi><mo >,</mo><mi >j</mi></mrow></msub><mo  >âˆˆ</mo><msup
    ><mi >â„</mi><mi  >C</mi></msup></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >ğ‘¦</ci><list
    ><ci >ğ‘–</ci><ci >ğ‘—</ci></list></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >â„</ci><ci  >ğ¶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >y_{i,j}\in\mathbb{R}^{C}</annotation></semantics></math>ï¼Œå…¶ä¸­<math   alttext="C"
    display="inline"><semantics ><mi  >C</mi><annotation-xml encoding="MathML-Content"
    ><ci  >ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" >C</annotation></semantics></math>è¡¨ç¤ºå¯èƒ½çš„ç±»åˆ«ã€‚ä¾‹å¦‚ï¼Œåœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œ<math
    alttext="C=2" display="inline"><semantics ><mrow ><mi  >C</mi><mo >=</mo><mn >2</mn></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><ci >ğ¶</ci><cn type="integer"  >2</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" >C=2</annotation></semantics></math>ï¼Œæ ‡ç­¾é‡‡ç”¨æ ‡é‡å½¢å¼{0,
    1}ï¼›è€Œåœ¨å›å½’ä»»åŠ¡ä¸­ï¼Œ<math alttext="C" display="inline"><semantics ><mi >C</mi><annotation-xml
    encoding="MathML-Content" ><ci  >ğ¶</ci></annotation-xml><annotation encoding="application/x-tex"
    >C</annotation></semantics></math>åˆ™é‡‡ç”¨å®æ•°çš„è¿ç»­é›†åˆå½¢å¼<math alttext="â„" display="inline"><semantics
    ><mi >â„</mi><annotation-xml encoding="MathML-Content" ><ci >â„</ci></annotation-xml><annotation
    encoding="application/x-tex" >â„</annotation></semantics></math>ã€‚ç›‘ç£å­¦ä¹ èŒƒå¼çš„ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªæ¨¡å‹<math
    alttext="f_{\theta}:x\rightarrow y" display="inline"><semantics ><mrow  ><msub
    ><mi >f</mi><mi  >Î¸</mi></msub><mo lspace="0.278em" rspace="0.278em"  >:</mo><mrow
    ><mi >x</mi><mo stretchy="false" >â†’</mo><mi  >y</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><ci  >:</ci><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘“</ci><ci >ğœƒ</ci></apply><apply ><ci  >â†’</ci><ci >ğ‘¥</ci><ci
    >ğ‘¦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >f_{\theta}:x\rightarrow y</annotation></semantics></math>ï¼Œä»¥**æœ€ä½³**åœ°é¢„æµ‹æœªçŸ¥è¡¥ä¸<math   alttext="y_{i,j}"
    display="inline"><semantics ><msub  ><mi >y</mi><mrow ><mi  >i</mi><mo >,</mo><mi
    >j</mi></mrow></msub><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol
- en: In the weakly supervised learning paradigm, the label <math alttext="y_{i,j}"
    display="inline"><semantics ><msub ><mi  >y</mi><mrow ><mi >i</mi><mo  >,</mo><mi
    >j</mi></mrow></msub><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘¦</ci><list ><ci  >ğ‘–</ci><ci >ğ‘—</ci></list></apply></annotation-xml><annotation
    encoding="application/x-tex" >y_{i,j}</annotation></semantics></math> of each
    patch is typically unknown, while only the label of each WSI is available, and
    thus the traditional strongly supervised learning paradigm cannot work. In this
    review, we focus on the most dominant weakly supervised paradigm currently used
    in computational pathology, the deep multiple instance learning (MIL) approach.
    In MIL, each WSI is considered as a bag containing many patches (also called instances).
    if a WSI (bag) is labeled as disease-positive, then at least one patch (instance)
    in that WSI is disease-positive; if a WSI is disease-negative, then all patches
    in that WSI are negative. The relationship between a WSI (bag) and its patches
    (instances) can be expressed mathematically as follows.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ä¸­ï¼Œæ¯ä¸ªè¡¥ä¸çš„æ ‡ç­¾<math alttext="y_{i,j}" display="inline"><semantics ><msub
    ><mi  >y</mi><mrow ><mi >i</mi><mo  >,</mo><mi >j</mi></mrow></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘¦</ci><list ><ci  >ğ‘–</ci><ci >ğ‘—</ci></list></apply></annotation-xml><annotation
    encoding="application/x-tex" >y_{i,j}</annotation></semantics></math> é€šå¸¸æ˜¯æœªçŸ¥çš„ï¼Œè€Œåªæœ‰æ¯ä¸ª
    WSI çš„æ ‡ç­¾æ˜¯å·²çŸ¥çš„ï¼Œå› æ­¤ä¼ ç»Ÿçš„å¼ºç›‘ç£å­¦ä¹ èŒƒå¼æ— æ³•ä½¿ç”¨ã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹ä»‹ç»äº†ç›®å‰åœ¨è®¡ç®—ç—…ç†å­¦ä¸­ä½¿ç”¨çš„æœ€ä¸»è¦çš„å¼±ç›‘ç£èŒƒå¼ï¼Œå³æ·±åº¦å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•ã€‚åœ¨
    MIL ä¸­ï¼Œæ¯ä¸ª WSI è¢«è§†ä¸ºåŒ…å«å¤šä¸ªè¡¥ä¸ï¼ˆä¹Ÿç§°ä¸ºå®ä¾‹ï¼‰çš„åŒ…ã€‚å¦‚æœä¸€ä¸ª WSIï¼ˆåŒ…ï¼‰è¢«æ ‡è®°ä¸ºç–¾ç—…é˜³æ€§ï¼Œåˆ™è¯¥ WSI ä¸­è‡³å°‘æœ‰ä¸€ä¸ªè¡¥ä¸ï¼ˆå®ä¾‹ï¼‰æ˜¯ç–¾ç—…é˜³æ€§ï¼›å¦‚æœä¸€ä¸ª
    WSI æ˜¯ç–¾ç—…é˜´æ€§ï¼Œåˆ™è¯¥ WSI ä¸­çš„æ‰€æœ‰è¡¥ä¸éƒ½æ˜¯é˜´æ€§ã€‚WSIï¼ˆåŒ…ï¼‰ä¸å…¶è¡¥ä¸ï¼ˆå®ä¾‹ï¼‰ä¹‹é—´çš„å…³ç³»å¯ä»¥ç”¨æ•°å­¦è¡¨è¾¾å¦‚ä¸‹ã€‚
- en: 'Given a dataset <math   alttext="W={\{W_{i}\}}_{i=1}^{N}" display="inline"><semantics
    ><mrow  ><mi >W</mi><mo >=</mo><msubsup  ><mrow ><mo stretchy="false" >{</mo><msub
    ><mi  >W</mi><mi >i</mi></msub><mo stretchy="false"  >}</mo></mrow><mrow ><mi
    >i</mi><mo  >=</mo><mn >1</mn></mrow><mi >N</mi></msubsup></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><ci >ğ‘Š</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><set ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Š</ci><ci >ğ‘–</ci></apply></set><apply ><ci >ğ‘–</ci><cn type="integer" >1</cn></apply></apply><ci
    >ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >W={\{W_{i}\}}_{i=1}^{N}</annotation></semantics></math> consisting of <math   alttext="N"
    display="inline"><semantics ><mi  >N</mi><annotation-xml encoding="MathML-Content"
    ><ci  >ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" >N</annotation></semantics></math>
    WSIs, each image <math   alttext="W_{i}" display="inline"><semantics ><msub  ><mi
    >W</mi><mi  >i</mi></msub><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘Š</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" >W_{i}</annotation></semantics></math> has a corresponding
    label <math   alttext="Y_{i}\in\left\{0,1\right\},\ i=\{1,2,...N\}" display="inline"><semantics
    ><mrow ><mrow  ><msub ><mi >Y</mi><mi  >i</mi></msub><mo >âˆˆ</mo><mrow ><mo  >{</mo><mn
    >0</mn><mo >,</mo><mn  >1</mn><mo >}</mo></mrow></mrow><mo rspace="0.667em"  >,</mo><mrow
    ><mi >i</mi><mo  >=</mo><mrow ><mo stretchy="false" >{</mo><mn >1</mn><mo  >,</mo><mn
    >2</mn><mo >,</mo><mrow  ><mi mathvariant="normal"  >â€¦</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >N</mi></mrow><mo stretchy="false"  >}</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous"  >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘Œ</ci><ci  >ğ‘–</ci></apply><set
    ><cn type="integer" >0</cn><cn type="integer" >1</cn></set></apply><apply ><ci  >ğ‘–</ci><set
    ><cn type="integer" >1</cn><cn type="integer" >2</cn><apply  ><ci >â€¦</ci><ci >ğ‘</ci></apply></set></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >Y_{i}\in\left\{0,1\right\},\ i=\{1,2,...N\}</annotation></semantics></math>.
    Now each WSI <math   alttext="W_{i}" display="inline"><semantics ><msub  ><mi
    >W</mi><mi  >i</mi></msub><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘Š</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" >W_{i}</annotation></semantics></math> is cut into
    small patches <math   alttext="\{p_{i,j},j=1,2,\ldots,n_{i}\}" display="inline"><semantics
    ><mrow  ><mo stretchy="false"  >{</mo><mrow ><mrow ><mrow  ><msub ><mi >p</mi><mrow
    ><mi >i</mi><mo  >,</mo><mi >j</mi></mrow></msub><mo >,</mo><mi >j</mi></mrow><mo
    >=</mo><mn  >1</mn></mrow><mo >,</mo><mrow ><mn  >2</mn><mo >,</mo><mi mathvariant="normal"  >â€¦</mi><mo
    >,</mo><msub ><mi >n</mi><mi >i</mi></msub></mrow></mrow><mo stretchy="false"
    >}</mo></mrow><annotation-xml encoding="MathML-Content" ><set  ><apply ><csymbol
    cd="ambiguous" >formulae-sequence</csymbol><apply ><list ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘</ci><list ><ci  >ğ‘–</ci><ci >ğ‘—</ci></list></apply><ci
    >ğ‘—</ci></list><cn type="integer" >1</cn></apply><list ><cn type="integer" >2</cn><ci
    >â€¦</ci><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘›</ci><ci >ğ‘–</ci></apply></list></apply></set></annotation-xml><annotation
    encoding="application/x-tex" >\{p_{i,j},j=1,2,\ldots,n_{i}\}</annotation></semantics></math>
    without overlapping each other, and <math   alttext="n_{i}" display="inline"><semantics
    ><msub  ><mi >n</mi><mi >i</mi></msub><annotation-xml encoding="MathML-Content"
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘›</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" >n_{i}</annotation></semantics></math> is the number
    of patches. All patches <math   alttext="\{p_{i,j},j=1,2,\ldots,n_{i}\}" display="inline"><semantics
    ><mrow  ><mo stretchy="false"  >{</mo><mrow ><mrow ><mrow  ><msub ><mi >p</mi><mrow
    ><mi >i</mi><mo  >,</mo><mi >j</mi></mrow></msub><mo >,</mo><mi >j</mi></mrow><mo
    >=</mo><mn  >1</mn></mrow><mo >,</mo><mrow ><mn  >2</mn><mo >,</mo><mi mathvariant="normal"  >â€¦</mi><mo
    >,</mo><msub ><mi >n</mi><mi >i</mi></msub></mrow></mrow><mo stretchy="false"
    >}</mo></mrow><annotation-xml encoding="MathML-Content" ><set  ><apply ><csymbol
    cd="ambiguous" >formulae-sequence</csymbol><apply ><list ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘</ci><list ><ci  >ğ‘–</ci><ci >ğ‘—</ci></list></apply><ci
    >ğ‘—</ci></list><cn type="integer" >1</cn></apply><list ><cn type="integer" >2</cn><ci
    >â€¦</ci><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘›</ci><ci >ğ‘–</ci></apply></list></apply></set></annotation-xml><annotation
    encoding="application/x-tex" >\{p_{i,j},j=1,2,\ldots,n_{i}\}</annotation></semantics></math>
    in <math   alttext="W_{i}" display="inline"><semantics ><msub  ><mi >W</mi><mi  >i</mi></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Š</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >W_{i}</annotation></semantics></math> form a bag, the bag-level label is the
    label <math   alttext="Y_{i}" display="inline"><semantics ><msub  ><mi >Y</mi><mi
    >i</mi></msub><annotation-xml encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Œ</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >Y_{i}</annotation></semantics></math> of <math alttext="W_{i}" display="inline"><semantics
    ><msub ><mi >W</mi><mi  >i</mi></msub><annotation-xml encoding="MathML-Content"
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘Š</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" >W_{i}</annotation></semantics></math>, and each
    small patch is called an instance of this bag, while the instance-level label
    <math alttext="y_{i,j}" display="inline"><semantics ><msub ><mi >y</mi><mrow  ><mi
    >i</mi><mo >,</mo><mi  >j</mi></mrow></msub><annotation-xml encoding="MathML-Content"
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘¦</ci><list ><ci  >ğ‘–</ci><ci
    >ğ‘—</ci></list></apply></annotation-xml><annotation encoding="application/x-tex"
    >y_{i,j}</annotation></semantics></math> and its corresponding bag-level label
    <math alttext="Y_{i}" display="inline"><semantics ><msub ><mi  >Y</mi><mi >i</mi></msub><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Œ</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >Y_{i}</annotation></semantics></math> have the following relationship:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªæ•°æ®é›† <math   alttext="W={\{W_{i}\}}_{i=1}^{N}" display="inline"><semantics
    ><mrow  ><mi >W</mi><mo >=</mo><msubsup  ><mrow ><mo stretchy="false" >{</mo><msub
    ><mi  >W</mi><mi >i</mi></msub><mo stretchy="false"  >}</mo></mrow><mrow ><mi
    >i</mi><mo  >=</mo><mn >1</mn></mrow><mi >N</mi></msubsup></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><ci >ğ‘Š</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><set ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Š</ci><ci >ğ‘–</ci></apply></set><apply ><ci >ğ‘–</ci><cn type="integer" >1</cn></apply></apply><ci
    >ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >W={\{W_{i}\}}_{i=1}^{N}</annotation></semantics></math> åŒ…å« <math   alttext="N"
    display="inline"><semantics ><mi  >N</mi><annotation-xml encoding="MathML-Content"
    ><ci  >ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" >N</annotation></semantics></math>
    ä¸ª WSIsï¼Œæ¯ä¸ªå›¾åƒ <math   alttext="W_{i}" display="inline"><semantics ><msub  ><mi >W</mi><mi  >i</mi></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Š</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >W_{i}</annotation></semantics></math> éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„æ ‡ç­¾ <math   alttext="Y_{i}\in\left\{0,1\right\},\
    i=\{1,2,...N\}" display="inline"><semantics ><mrow ><mrow  ><msub ><mi >Y</mi><mi  >i</mi></msub><mo
    >âˆˆ</mo><mrow ><mo  >{</mo><mn >0</mn><mo >,</mo><mn  >1</mn><mo >}</mo></mrow></mrow><mo
    rspace="0.667em"  >,</mo><mrow ><mi >i</mi><mo  >=</mo><mrow ><mo stretchy="false"
    >{</mo><mn >1</mn><mo  >,</mo><mn >2</mn><mo >,</mo><mrow  ><mi mathvariant="normal"  >â€¦</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >N</mi></mrow><mo stretchy="false"  >}</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous"  >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘Œ</ci><ci  >ğ‘–</ci></apply><set
    ><cn type="integer" >0</cn><cn type="integer" >1</cn></set></apply><apply ><ci  >ğ‘–</ci><set
    ><cn type="integer" >1</cn><cn type="integer" >2</cn><apply  ><ci >â€¦</ci><ci >ğ‘</ci></apply></set></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >Y_{i}\in\left\{0,1\right\},\ i=\{1,2,...N\}</annotation></semantics></math>ã€‚ç°åœ¨ï¼Œæ¯ä¸ª
    WSI <math   alttext="W_{i}" display="inline"><semantics ><msub  ><mi >W</mi><mi  >i</mi></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘Š</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >W_{i}</annotation></semantics></math> è¢«åˆ‡å‰²æˆå°å— <math   alttext="\{p_{i,j},j=1,2,\ldots,n_{i}\}"
    display="inline"><semantics ><mrow  ><mo stretchy="false"  >{</mo><mrow ><mrow
    ><mrow  ><msub ><mi >p</mi><mrow ><mi >i</mi><mo  >,</mo><mi >j</mi></mrow></msub><mo
    >,</mo><mi >j</mi></mrow><mo >=</mo><mn  >1</mn></mrow><mo >,</mo><mrow ><mn  >2</mn><mo
    >,</mo><mi mathvariant="normal"  >â€¦</mi><mo >,</mo><msub ><mi >n</mi><mi >i</mi></msub></mrow></mrow><mo
    stretchy="false" >}</mo></mrow><annotation-xml encoding="MathML-Content" ><set  ><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><list ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><list ><ci  >ğ‘–</ci><ci >ğ‘—</ci></list></apply><ci
    >ğ‘—</ci></list><cn type="integer" >1</cn></apply><list ><cn type="integer" >2</cn><ci
    >â€¦</ci><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘›</ci><ci >ğ‘–</ci></apply></list></apply></set></annotation-xml><annotation
    encoding="application/x-tex" >\{p_{i,j},j=1,2,\ldots,n_{i}\}</annotation></semantics></math>ï¼Œè¿™äº›å°å—ä¹‹é—´ä¸é‡å ï¼Œå…¶ä¸­
    <math   alttext="n_{i}" display="inline"><semantics ><msub  ><mi >n</mi><mi >i</mi></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘›</ci><ci >ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >n_{i}</annotation></semantics></math> æ˜¯å°å—çš„æ•°é‡ã€‚æ‰€æœ‰çš„å°å— <math   alttext="\{p_{i,j},j=1,2,\ldots,n_{i}\}"
    display="inline"><semantics ><mrow  ><mo stretchy="false"  >{</mo><mrow ><mrow
    ><mrow  ><msub ><mi >p</mi><mrow ><mi >i</mi><mo  >,</mo><mi >j</mi></mrow></msub><mo
    >,</mo><mi >j</mi></mrow><mo >=</mo><mn  >1</mn></mrow><mo >,</mo><mrow ><mn  >2</mn><mo
    >,</mo><mi mathvariant="normal"  >â€¦</mi><mo >,</mo><msub ><mi >n</mi><mi >i</mi></msub></mrow></mrow><mo
    stretchy="false" >}</mo></mrow><annotation-xml encoding="MathML-Content" ><set  ><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><list ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><list ><ci  >ğ‘–</ci><ci >ğ‘—</ci></list></apply><ci
    >ğ‘—</ci></list><cn type="integer" >1</cn></apply><list ><cn type="integer" >2</cn><ci
    >â€¦</ci><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘›</ci><ci >ğ‘–</ci></apply></list></apply></set></annotation-xml><annotation
    encoding="application/x-tex" >\{p_{i,j},j=1,2,\ldots,n_{i}\}</annotation></semantics></math>
    åœ¨ <math   alttext="W_{i}" display="inline"><semantics ><msub  ><mi >W</mi><mi  >i</mi></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="
- en: '|  | <math   alttext="Y_{i}=\left\{\begin{array}[]{cc}&amp;0,\text{ if }\sum_{j}y_{i,j}=0\\
    &amp;1,\text{ else }\end{array}\right." display="block"><semantics ><mrow  ><msub
    ><mi  >Y</mi><mi >i</mi></msub><mo >=</mo><mrow  ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  ><mrow ><mrow ><mn  >0</mn><mo
    >,</mo><mrow ><mtext  >Â ifÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mstyle displaystyle="false"  ><msub ><mo >âˆ‘</mo><mi >j</mi></msub></mstyle><msub
    ><mi >y</mi><mrow ><mi >i</mi><mo >,</mo><mi  >j</mi></mrow></msub></mrow></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow></mtd></mtr><mtr ><mtd  ><mrow ><mn >1</mn><mo  >,</mo><mtext
    >Â elseÂ </mtext></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >ğ‘Œ</ci><ci
    >ğ‘–</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow
    ><cerror  ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><apply
    ><list ><cn type="integer" >0</cn><apply  ><ci ><mtext >Â ifÂ </mtext></ci><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘—</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘¦</ci><list ><ci >ğ‘–</ci><ci  >ğ‘—</ci></list></apply></apply></apply></list><cn
    type="integer"  >0</cn></apply></matrixrow><matrixrow ><cerror  ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><list
    ><cn type="integer" >1</cn><ci ><mtext  >Â elseÂ </mtext></ci></list></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >Y_{i}=\left\{\begin{array}[]{cc}&0,\text{ if }\sum_{j}y_{i,j}=0\\
    &1,\text{ else }\end{array}\right.</annotation></semantics></math> |  | (1) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | <math   alttext="Y_{i}=\left\{\begin{array}[]{cc}&amp;0,\text{ if }\sum_{j}y_{i,j}=0\\
    &amp;1,\text{ else }\end{array}\right." display="block"><semantics ><mrow  ><msub
    ><mi  >Y</mi><mi >i</mi></msub><mo >=</mo><mrow  ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  ><mrow ><mrow ><mn  >0</mn><mo
    >,</mo><mrow ><mtext  >Â ifÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mstyle displaystyle="false"  ><msub ><mo >âˆ‘</mo><mi >j</mi></msub></mstyle><msub
    ><mi >y</mi><mrow ><mi >i</mi><mo >,</mo><mi  >j</mi></mrow></msub></mrow></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow></mtd></mtr><mtr ><mtd  ><mrow ><mn >1</mn><mo  >,</mo><mtext
    >Â elseÂ </mtext></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >ğ‘Œ</ci><ci
    >ğ‘–</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow
    ><cerror  ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><apply
    ><list ><cn type="integer" >0</cn><apply  ><ci ><mtext >Â ifÂ </mtext></ci><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘—</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘¦</ci><list ><ci >ğ‘–</ci><ci  >ğ‘—</ci></list></apply></apply></apply></list><cn
    type="integer"  >0</cn></apply></matrixrow><matrixrow ><cerror  ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><list
    ><cn type="integer" >1</cn><ci ><mtext  >Â elseÂ </mtext></ci></list></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >Y_{i}=\left\{\begin{array}[]{cc}&0,\text{ if }\sum_{j}y_{i,j}=0\\
    &1,\text{ else }\end{array}\right.</annotation></semantics></math> |  | (1) |'
- en: It means that the labels of all instances in the negative bag are negative,
    while at least one positive instance exists in the positive bag and the labels
    of instances <math   alttext="y_{i,j}" display="inline"><semantics ><msub  ><mi
    >y</mi><mrow  ><mi >i</mi><mo >,</mo><mi  >j</mi></mrow></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘¦</ci><list ><ci  >ğ‘–</ci><ci >ğ‘—</ci></list></apply></annotation-xml><annotation
    encoding="application/x-tex" >y_{i,j}</annotation></semantics></math> are unknown.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€è´Ÿè¢‹ä¸­çš„æ‰€æœ‰å®ä¾‹æ ‡ç­¾éƒ½æ˜¯è´Ÿçš„ï¼Œè€Œæ­£è¢‹ä¸­è‡³å°‘å­˜åœ¨ä¸€ä¸ªæ­£å®ä¾‹ï¼Œä¸”å®ä¾‹çš„æ ‡ç­¾<math   alttext="y_{i,j}" display="inline"><semantics
    ><msub  ><mi >y</mi><mrow  ><mi >i</mi><mo >,</mo><mi  >j</mi></mrow></msub><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘¦</ci><list ><ci  >ğ‘–</ci><ci >ğ‘—</ci></list></apply></annotation-xml><annotation
    encoding="application/x-tex" >y_{i,j}</annotation></semantics></math> æ˜¯æœªçŸ¥çš„ã€‚
- en: '![Refer to caption](img/7568353672e6498d941fd536fe5ff599.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/7568353672e6498d941fd536fe5ff599.png)'
- en: 'Figure 1: General overview of the learning paradigms reviewed in this paper,
    depicted as an example of classifying normal tissue (green) and cancerous tissue
    (red) in a WSI. Note that the training data and testing data in this figure are
    used for description only and are not necessarily the real case. (a) Supervised
    learning paradigm. (b) Weakly Supervised learning paradigm. (c) Semi-supervised
    learning paradigm. (d) Self-supervised learning paradigm.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šæœ¬æ–‡å›é¡¾çš„å­¦ä¹ èŒƒå¼çš„ä¸€èˆ¬æ¦‚è¿°ï¼Œå±•ç¤ºäº†åœ¨ WSI ä¸­åˆ†ç±»æ­£å¸¸ç»„ç»‡ï¼ˆç»¿è‰²ï¼‰å’Œç™Œç»„ç»‡ï¼ˆçº¢è‰²ï¼‰çš„ç¤ºä¾‹ã€‚è¯·æ³¨æ„ï¼Œè¿™å›¾ä¸­çš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä»…ç”¨äºæè¿°ï¼Œå¹¶ä¸ä¸€å®šæ˜¯çœŸå®æƒ…å†µã€‚
    (a) ç›‘ç£å­¦ä¹ èŒƒå¼ã€‚ (b) å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ã€‚ (c) åŠç›‘ç£å­¦ä¹ èŒƒå¼ã€‚ (d) è‡ªç›‘ç£å­¦ä¹ èŒƒå¼ã€‚
- en: 'As shown in Figure [1](#S2.F1 "Figure 1 â€£ 2 Overview of Learning Paradigms
    and Problem Formulation â€£ Towards Label-efficient Automatic Diagnosis and Analysis:
    A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised
    and Self-supervised Techniques in Histopathological Image Analysis") (b), generally,
    there are two main goals of deep learning-based WSI analysis, one is global slide
    classification, i.e., to accurately classify each WSI, and the other is positive
    patch localization, i.e., to accurately classify each instance in positive bags.
    A review of the current state-of-the-art weakly supervised learning methods is
    presented in Section [3.1](#S3.SS1 "3.1 Weakly Supervised Learning Paradigm â€£
    3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive
    Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and
    Self-supervised Techniques in Histopathological Image Analysis").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚å›¾[1](#S2.F1 "Figure 1 â€£ 2 Overview of Learning Paradigms and Problem Formulation
    â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive Survey
    of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and Self-supervised
    Techniques in Histopathological Image Analysis") (b)æ‰€ç¤ºï¼Œæ·±åº¦å­¦ä¹ åŸºç¡€çš„WSIåˆ†æé€šå¸¸æœ‰ä¸¤ä¸ªä¸»è¦ç›®æ ‡ï¼Œä¸€ä¸ªæ˜¯å…¨å±€å¹»ç¯ç‰‡åˆ†ç±»ï¼Œå³å‡†ç¡®åˆ†ç±»æ¯ä¸ªWSIï¼Œå¦ä¸€ä¸ªæ˜¯æ­£æ ·æœ¬å®šä½ï¼Œå³å‡†ç¡®åˆ†ç±»æ¯ä¸ªæ­£æ ·æœ¬åŒ…ä¸­çš„å®ä¾‹ã€‚æœ‰å…³å½“å‰æœ€å…ˆè¿›çš„å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•çš„ç»¼è¿°è§[3.1](#S3.SS1
    "3.1 Weakly Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis")èŠ‚ã€‚'
- en: 'In the semi-supervised learning paradigm, we only have a very small number
    of patches with labels, in addition to a large number of unlabeled patches that
    can also be used for training. Therefore, the main goal of the semi-supervised
    learning paradigm is how to use the unlabeled data to improve the performance
    of the models trained with limited labeled data. As shown in Figure [1](#S2.F1
    "Figure 1 â€£ 2 Overview of Learning Paradigms and Problem Formulation â€£ Towards
    Label-efficient Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced
    Deep Learning-based Weakly-supervised, Semi-supervised and Self-supervised Techniques
    in Histopathological Image Analysis") (c), in contrast to the supervised learning
    paradigm, the semi-supervised learning paradigm makes use of a large amount of
    unlabeled data while training with the labeled data. During testing, the trained
    model is used to predict the labels of the patches in test WSIs. See Section [3.2](#S3.SS2
    "3.2 Semi-Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis") for a detailed review of the semi-supervised learning methods.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨åŠç›‘ç£å­¦ä¹ èŒƒå¼ä¸­ï¼Œæˆ‘ä»¬ä»…æœ‰å°‘é‡å¸¦æ ‡ç­¾çš„æ ·æœ¬ï¼Œä»¥åŠå¤§é‡æœªæ ‡è®°çš„æ ·æœ¬ä¹Ÿå¯ä»¥ç”¨äºè®­ç»ƒã€‚å› æ­¤ï¼ŒåŠç›‘ç£å­¦ä¹ èŒƒå¼çš„ä¸»è¦ç›®æ ‡æ˜¯å¦‚ä½•åˆ©ç”¨æœªæ ‡è®°çš„æ•°æ®æ¥æé«˜åœ¨æœ‰é™å¸¦æ ‡ç­¾æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„æ€§èƒ½ã€‚å¦‚å›¾[1](#S2.F1
    "Figure 1 â€£ 2 Overview of Learning Paradigms and Problem Formulation â€£ Towards
    Label-efficient Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced
    Deep Learning-based Weakly-supervised, Semi-supervised and Self-supervised Techniques
    in Histopathological Image Analysis") (c)æ‰€ç¤ºï¼Œä¸ç›‘ç£å­¦ä¹ èŒƒå¼ç›¸æ¯”ï¼ŒåŠç›‘ç£å­¦ä¹ èŒƒå¼åœ¨ä½¿ç”¨å¸¦æ ‡ç­¾æ•°æ®è¿›è¡Œè®­ç»ƒçš„åŒæ—¶ï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„æ•°æ®ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œè®­ç»ƒå¥½çš„æ¨¡å‹è¢«ç”¨äºé¢„æµ‹æµ‹è¯•WSIä¸­æ ·æœ¬çš„æ ‡ç­¾ã€‚æœ‰å…³åŠç›‘ç£å­¦ä¹ æ–¹æ³•çš„è¯¦ç»†ç»¼è¿°è§[3.2](#S3.SS2
    "3.2 Semi-Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis")èŠ‚ã€‚'
- en: Self-supervised learning is a hybrid learning approach, which combines unsupervised
    and supervised learning paradigms in a pre-training and fine-tuning manner. The
    aim is to get better results of supervised training though generating supervised
    information from a large amount of unlabeled data, which can learn better feature
    representations, and can reduce manual annotation in the subsequent tasks. Due
    to the small amount of annotated data, it is not sufficient to use these data
    directly to train the model. Therefore, the self-supervised learning paradigm
    first learns a primary feature representation from a large amount of unlabeled
    data, which is called the pre-training process. The feature representations learned
    in the self-supervised auxiliary tasks are then transferred for further training
    in downstream tasks using limited labeled data, which is called the fine-tuning
    process. In this way, the primary feature representations can effectively help
    the network to achieve an effective training result with less labeled data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æ··åˆå­¦ä¹ æ–¹æ³•ï¼Œç»“åˆäº†æ— ç›‘ç£å­¦ä¹ å’Œç›‘ç£å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡é¢„è®­ç»ƒå’Œå¾®è°ƒçš„æ–¹å¼è¿›è¡Œã€‚å…¶ç›®çš„æ˜¯é€šè¿‡ä»å¤§é‡æœªæ ‡è®°æ•°æ®ä¸­ç”Ÿæˆç›‘ç£ä¿¡æ¯æ¥è·å¾—æ›´å¥½çš„ç›‘ç£è®­ç»ƒç»“æœï¼Œè¿™å¯ä»¥å­¦åˆ°æ›´å¥½çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶å‡å°‘åç»­ä»»åŠ¡ä¸­çš„äººå·¥æ ‡æ³¨ã€‚ç”±äºæ ‡æ³¨æ•°æ®é‡å°‘ï¼Œç›´æ¥ä½¿ç”¨è¿™äº›æ•°æ®è®­ç»ƒæ¨¡å‹æ˜¯ä¸å¤Ÿçš„ã€‚å› æ­¤ï¼Œè‡ªç›‘ç£å­¦ä¹ èŒƒå¼é¦–å…ˆä»å¤§é‡æœªæ ‡è®°æ•°æ®ä¸­å­¦ä¹ ä¸»è¦çš„ç‰¹å¾è¡¨ç¤ºï¼Œè¿™ç§°ä¸ºé¢„è®­ç»ƒè¿‡ç¨‹ã€‚ç„¶åï¼Œå°†åœ¨è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡ä¸­å­¦åˆ°çš„ç‰¹å¾è¡¨ç¤ºè½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨æœ‰é™çš„æ ‡è®°æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œè¿™ç§°ä¸ºå¾®è°ƒè¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä¸»è¦çš„ç‰¹å¾è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆåœ°å¸®åŠ©ç½‘ç»œåœ¨å°‘é‡æ ‡è®°æ•°æ®ä¸‹å®ç°æœ‰æ•ˆçš„è®­ç»ƒç»“æœã€‚
- en: 'As shown in Figure [1](#S2.F1 "Figure 1 â€£ 2 Overview of Learning Paradigms
    and Problem Formulation â€£ Towards Label-efficient Automatic Diagnosis and Analysis:
    A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised
    and Self-supervised Techniques in Histopathological Image Analysis") (d), the
    pre-training process of the self-supervised learning paradigm is typically performed
    through self-supervised auxiliary tasks. In the self-supervised auxiliary tasks,
    certain inherent properties of the unlabeled data are first utilized to generate
    supervised information, and then the network is trained by the self-supervised
    information, such as self-reconstruction, random rotation followed by angle prediction,
    color information discarding followed by colorization, and patch position disruption
    followed by restoration. Once accomplishing these self-supervised auxiliary tasks,
    the effective feature representations can be extracted. The fine-tuning process
    of self-supervised learning is done in the downstream tasks. During the fine-tuning
    process, a small amount of labeled data is used to perform the supervised training,
    and the model is not trained from scratch, but is further trained using the feature
    representations learned in the auxiliary tasks as the initial weights of the network.
    Finally, the trained network is used for testing. A review of the state-of-the-art
    self-supervised learning methods is presented in Section [3.3](#S3.SS3 "3.3 Self-Supervised
    Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis
    and Analysis: A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised,
    Semi-supervised and Self-supervised Techniques in Histopathological Image Analysis").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å›¾ [1](#S2.F1 "å›¾ 1 â€£ 2 å­¦ä¹ èŒƒå¼å’Œé—®é¢˜è¡¨è¿°æ¦‚è¿° â€£ é¢å‘æ ‡ç­¾é«˜æ•ˆçš„è‡ªåŠ¨è¯Šæ–­å’Œåˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å…ˆè¿›å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢è°ƒæŸ¥")
    (d) æ‰€ç¤ºï¼Œè‡ªç›‘ç£å­¦ä¹ èŒƒå¼çš„é¢„è®­ç»ƒè¿‡ç¨‹é€šå¸¸é€šè¿‡è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡æ¥å®Œæˆã€‚åœ¨è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡ä¸­ï¼Œé¦–å…ˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„æŸäº›å›ºæœ‰ç‰¹æ€§ç”Ÿæˆç›‘ç£ä¿¡æ¯ï¼Œç„¶åé€šè¿‡è‡ªç›‘ç£ä¿¡æ¯ï¼ˆä¾‹å¦‚è‡ªæˆ‘é‡å»ºã€éšæœºæ—‹è½¬åè§’åº¦é¢„æµ‹ã€é¢œè‰²ä¿¡æ¯ä¸¢å¼ƒåé¢œè‰²åŒ–ä»¥åŠè¡¥ä¸ä½ç½®ç ´ååæ¢å¤ï¼‰æ¥è®­ç»ƒç½‘ç»œã€‚ä¸€æ—¦å®Œæˆè¿™äº›è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡ï¼Œå°±å¯ä»¥æå–æœ‰æ•ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚è‡ªç›‘ç£å­¦ä¹ çš„å¾®è°ƒè¿‡ç¨‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿›è¡Œã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å°‘é‡æ ‡è®°æ•°æ®è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œæ¨¡å‹ä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè€Œæ˜¯åˆ©ç”¨åœ¨è¾…åŠ©ä»»åŠ¡ä¸­å­¦åˆ°çš„ç‰¹å¾è¡¨ç¤ºä½œä¸ºç½‘ç»œçš„åˆå§‹æƒé‡è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒã€‚æœ€åï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„ç½‘ç»œè¿›è¡Œæµ‹è¯•ã€‚æœ€å…ˆè¿›çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•çš„ç»¼è¿°è§ç¬¬
    [3.3](#S3.SS3 "3.3 è‡ªç›‘ç£å­¦ä¹ èŒƒå¼ â€£ 3 èŒƒå¼ â€£ é¢å‘æ ‡ç­¾é«˜æ•ˆçš„è‡ªåŠ¨è¯Šæ–­å’Œåˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å…ˆè¿›å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢è°ƒæŸ¥")
    èŠ‚ã€‚
- en: 'Table 2: Intuitive summary and comparison of the four paradigms.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 2ï¼šå››ç§èŒƒå¼çš„ç›´è§‚æ€»ç»“å’Œæ¯”è¾ƒã€‚
- en: '| Methods | Input | Suitable tasks | Technical paradigms | Strengths | Weaknesses
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | è¾“å…¥ | é€‚ç”¨ä»»åŠ¡ | æŠ€æœ¯èŒƒå¼ | ä¼˜åŠ¿ | åŠ£åŠ¿ |'
- en: '| Supervised learning paradigm | A large number of small patches (tiled from
    WSIs) with fine-grained labels | WSI-level and patch-level classification/segmentation/regression
    | - | Broad application, effective and simple training | Require large amount
    of fine-grained labeled data |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| ç›‘ç£å­¦ä¹ èŒƒå¼ | å¤§é‡çš„å°å—ï¼ˆä»WSIsæ‹¼æ¥è€Œæ¥ï¼‰å¸¦æœ‰ç»†ç²’åº¦æ ‡ç­¾ | WSIçº§åˆ«å’Œå°å—çº§åˆ«çš„åˆ†ç±»/åˆ†å‰²/å›å½’ | - | å¹¿æ³›åº”ç”¨ï¼Œæœ‰æ•ˆä¸”ç®€å•çš„è®­ç»ƒ
    | éœ€è¦å¤§é‡çš„ç»†ç²’åº¦æ ‡æ³¨æ•°æ® |'
- en: '| Weakly Supervised learning paradigm | Entire WSIs with overall labels or
    sparse labels | WSI-level classification/segmentation/regression, Patch-level
    coarse-grained localization | Instance-based approach, Bag-based approach, Hybrid
    approach | No need for fine-grained annotation, effectively reduce the burden
    of data annotation | Achieve limited performance for fine-grained tasks |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ | æ•´ä¸ªWSIå¸¦æœ‰æ•´ä½“æ ‡ç­¾æˆ–ç¨€ç–æ ‡ç­¾ | WSIçº§åˆ«åˆ†ç±»/åˆ†å‰²/å›å½’ï¼Œå°å—çº§åˆ«ç²—ç²’åº¦å®šä½ | åŸºäºå®ä¾‹çš„æ–¹æ³•ï¼ŒåŸºäºåŒ…çš„æ–¹æ³•ï¼Œæ··åˆæ–¹æ³•
    | ä¸éœ€è¦ç»†ç²’åº¦æ ‡æ³¨ï¼Œæœ‰æ•ˆå‡å°‘æ•°æ®æ ‡æ³¨è´Ÿæ‹… | å¯¹ç»†ç²’åº¦ä»»åŠ¡çš„è¡¨ç°æœ‰é™ |'
- en: '| Semi-supervised learning paradigm | A limited number of small patches (tiled
    from WSIs) with fine-grained labels | WSI-level and patch-level classification/segmentation/regression
    | Pseudo-labelling-based approach, Consistency-based approach, Graph-based approach,
    Unsupervised-preprocessing-based approach, GAN-based approach and others | Require
    only a small amount of fine-grained annotation, effectively reduce the burden
    of data annotation | Need to satisfy various semi-supervised assumptions |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| åŠç›‘ç£å­¦ä¹ èŒƒå¼ | æœ‰é™æ•°é‡çš„å°å—ï¼ˆä»WSIsæ‹¼æ¥è€Œæ¥ï¼‰å¸¦æœ‰ç»†ç²’åº¦æ ‡ç­¾ | WSIçº§åˆ«å’Œå°å—çº§åˆ«çš„åˆ†ç±»/åˆ†å‰²/å›å½’ | åŸºäºä¼ªæ ‡æ³¨çš„æ–¹æ³•ï¼Œä¸€è‡´æ€§æ–¹æ³•ï¼Œå›¾å½¢æ–¹æ³•ï¼Œæ— ç›‘ç£é¢„å¤„ç†æ–¹æ³•ï¼ŒGANæ–¹æ³•ç­‰
    | åªéœ€å°‘é‡ç»†ç²’åº¦æ ‡æ³¨ï¼Œæœ‰æ•ˆå‡å°‘æ•°æ®æ ‡æ³¨è´Ÿæ‹… | éœ€è¦æ»¡è¶³å„ç§åŠç›‘ç£å‡è®¾ |'
- en: '| Self-supervised learning paradigm | A large number of small patches (tiled
    from WSIs) without labels | Patch-level feature representations, Multiple related
    down-stream tasks | Predictive approach, Generative approach, Contrastive approach,
    Hybrid approach | Efficiently extract image features from a large amount of unsupervised
    data, effectively reduce the data annotation burden | May result in information
    loss when the extracted features are not applicable to downstream tasks |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| è‡ªç›‘ç£å­¦ä¹ èŒƒå¼ | å¤§é‡çš„å°å—ï¼ˆä»WSIsæ‹¼æ¥è€Œæ¥ï¼‰æ²¡æœ‰æ ‡ç­¾ | å°å—çº§åˆ«çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¤šä¸ªç›¸å…³çš„ä¸‹æ¸¸ä»»åŠ¡ | é¢„æµ‹æ–¹æ³•ï¼Œç”Ÿæˆæ–¹æ³•ï¼Œå¯¹æ¯”æ–¹æ³•ï¼Œæ··åˆæ–¹æ³•
    | ä»å¤§é‡æ— ç›‘ç£æ•°æ®ä¸­é«˜æ•ˆæå–å›¾åƒç‰¹å¾ï¼Œæœ‰æ•ˆå‡å°‘æ•°æ®æ ‡æ³¨è´Ÿæ‹… | å½“æå–çš„ç‰¹å¾ä¸é€‚ç”¨äºä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤± |'
- en: 3 Paradigms
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 ä¸ªèŒƒå¼
- en: 3.1 Weakly Supervised Learning Paradigm
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 å¼±ç›‘ç£å­¦ä¹ èŒƒå¼
- en: In this section, we provide a comprehensive review of the primary deep multiple
    instance learning (MIL) methods currently used in the weakly supervised learning
    paradigm for computational pathology. In MIL, each WSI is considered as a bag
    containing many patches (also called instances). If a WSI (bag) is labeled disease-positive,
    then at least one patch (instance) in that WSI is disease-positive; if a WSI is
    disease-negative, then all patches in that WSI are negative.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å…¨é¢å›é¡¾äº†ç›®å‰åœ¨è®¡ç®—ç—…ç†å­¦çš„å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ä¸­ä½¿ç”¨çš„ä¸»è¦æ·±åº¦å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•ã€‚åœ¨MILä¸­ï¼Œæ¯ä¸ªWSIè¢«è§†ä¸ºä¸€ä¸ªåŒ…å«å¤šä¸ªå°å—ï¼ˆä¹Ÿç§°ä¸ºå®ä¾‹ï¼‰çš„åŒ…ã€‚å¦‚æœWSIï¼ˆåŒ…ï¼‰è¢«æ ‡è®°ä¸ºç–¾ç—…é˜³æ€§ï¼Œé‚£ä¹ˆè¯¥WSIä¸­çš„è‡³å°‘ä¸€ä¸ªå°å—ï¼ˆå®ä¾‹ï¼‰ä¹Ÿæ˜¯ç–¾ç—…é˜³æ€§ï¼›å¦‚æœWSIæ˜¯ç–¾ç—…é˜´æ€§ï¼Œåˆ™è¯¥WSIä¸­çš„æ‰€æœ‰å°å—éƒ½æ˜¯é˜´æ€§ã€‚
- en: 'We categorize the current deep MIL methods for WSI analysis into instance-based
    methods, bag-based methods, and hybrid methods. Our categorization is mainly based
    on whether the methods contain an instance classifier or a bag classifier, i.e.,
    instance-based methods contain only an instance classifier; bag-based methods
    contain only a bag classifier; while hybrid methods contain both an instance classifier
    and a bag classifier. In this way, the categories clearly cover almost current
    deep MIL methods for WSI analysis. A diagram of the three methods above is shown
    in Figure [2](#S3.F2 "Figure 2 â€£ 3.1.1 Instance-based Approach â€£ 3.1 Weakly Supervised
    Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis
    and Analysis: A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised,
    Semi-supervised and Self-supervised Techniques in Histopathological Image Analysis").
    The detailed literatures in this section are summarized in Table [3](#S3.T3 "Table
    3 â€£ 3.1.4 Representative Clinical Studies â€£ 3.1 Weakly Supervised Learning Paradigm
    â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive
    Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and
    Self-supervised Techniques in Histopathological Image Analysis").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å½“å‰ç”¨äºWSIåˆ†æçš„æ·±åº¦MILæ–¹æ³•åˆ†ä¸ºåŸºäºå®ä¾‹çš„æ–¹æ³•ã€åŸºäºåŒ…çš„æ–¹æ³•å’Œæ··åˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆ†ç±»ä¸»è¦ä¾æ®æ–¹æ³•æ˜¯å¦åŒ…å«å®ä¾‹åˆ†ç±»å™¨æˆ–åŒ…åˆ†ç±»å™¨ï¼Œå³ï¼ŒåŸºäºå®ä¾‹çš„æ–¹æ³•ä»…åŒ…å«å®ä¾‹åˆ†ç±»å™¨ï¼›åŸºäºåŒ…çš„æ–¹æ³•ä»…åŒ…å«åŒ…åˆ†ç±»å™¨ï¼›è€Œæ··åˆæ–¹æ³•åˆ™åŒæ—¶åŒ…å«å®ä¾‹åˆ†ç±»å™¨å’ŒåŒ…åˆ†ç±»å™¨ã€‚è¿™æ ·ï¼Œç±»åˆ«å°±æ¸…æ™°åœ°è¦†ç›–äº†å‡ ä¹æ‰€æœ‰å½“å‰ç”¨äºWSIåˆ†æçš„æ·±åº¦MILæ–¹æ³•ã€‚ä¸Šè¿°ä¸‰ç§æ–¹æ³•çš„å›¾ç¤ºè§å›¾[2](#S3.F2
    "å›¾ 2 â€£ 3.1.1 åŸºäºå®ä¾‹çš„æ–¹æ³• â€£ 3.1 å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ â€£ 3 ç§èŒƒå¼ â€£ é¢å‘æ ‡ç­¾é«˜æ•ˆçš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢è°ƒæŸ¥")ã€‚æœ¬èŠ‚çš„è¯¦ç»†æ–‡çŒ®æ€»ç»“åœ¨è¡¨[3](#S3.T3
    "è¡¨ 3 â€£ 3.1.4 ä»£è¡¨æ€§ä¸´åºŠç ”ç©¶ â€£ 3.1 å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ â€£ 3 ç§èŒƒå¼ â€£ é¢å‘æ ‡ç­¾é«˜æ•ˆçš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢è°ƒæŸ¥")ã€‚
- en: 3.1.1 Instance-based Approach
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 åŸºäºå®ä¾‹çš„æ–¹æ³•
- en: 'The main idea of the instance-based approach is to train a good instance classifier
    to accurately predict the potential labels of instances in each bag, and then
    use MIL-pooling to aggregate the predictions of all instances in each bag to obtain
    the prediction of the bag. The details are shown in Figure [2](#S3.F2 "Figure
    2 â€£ 3.1.1 Instance-based Approach â€£ 3.1 Weakly Supervised Learning Paradigm â€£
    3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive
    Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and
    Self-supervised Techniques in Histopathological Image Analysis") (a). Since the
    true labels of each instance are unknown, these approaches usually first assign
    the labels of each instance with their corresponding bags as the pseudo labels
    (i.e., all instances in a positive bag are given positive labels, and all instances
    in a negative bag are given negative labels), and then train the instance classifier
    using a supervised way until it converges. The loss function is usually the cross-entropy
    function defined between the predictions of the instance classifier and the pseudo
    labels. After training, the instance classifier is used to make predictions for
    all instances in the test bag, and then the predictions of each instance are aggregated
    to obtain the prediction of the bag, and this aggregation process is called MIL-pooling.
    Commonly used MIL pooling methods include Mean-pooling (Wang *et al.*Â [2018](#bib.bib182)),
    Max-pooling (Feng *et al.*Â [2017](#bib.bib56), Wang *et al.*Â [2018](#bib.bib182),
    Wu *et al.*Â [2015](#bib.bib189)), Voting (Cruz-Roa *et al.*Â [2014](#bib.bib40)),
    log-sum-exp-pooling (Ramon *et al.*Â [2000](#bib.bib135)), Noisy-or-pooling (Maron
    *et al.*Â [1997](#bib.bib110)), Noisy-and-pooling (Kraus *et al.*Â [2016](#bib.bib90)),
    and Dynamic pooling (Yan *et al.*Â [2018](#bib.bib197)) among others.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'å®ä¾‹åŒ–æ–¹æ³•çš„ä¸»è¦æ€æƒ³æ˜¯è®­ç»ƒä¸€ä¸ªä¼˜ç§€çš„å®ä¾‹åˆ†ç±»å™¨ï¼Œä»¥å‡†ç¡®é¢„æµ‹æ¯ä¸ªè¢‹å­ä¸­å®ä¾‹çš„æ½œåœ¨æ ‡ç­¾ï¼Œç„¶åä½¿ç”¨MIL-poolingå°†æ¯ä¸ªè¢‹å­ä¸­æ‰€æœ‰å®ä¾‹çš„é¢„æµ‹ç»“æœè¿›è¡Œèšåˆï¼Œä»è€Œè·å¾—è¢‹å­çš„é¢„æµ‹ç»“æœã€‚è¯¦ç»†ä¿¡æ¯è§å›¾[2](#S3.F2
    "Figure 2 â€£ 3.1.1 Instance-based Approach â€£ 3.1 Weakly Supervised Learning Paradigm
    â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive
    Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and
    Self-supervised Techniques in Histopathological Image Analysis")ï¼ˆaï¼‰ã€‚ç”±äºæ¯ä¸ªå®ä¾‹çš„çœŸå®æ ‡ç­¾æœªçŸ¥ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é¦–å…ˆå°†æ¯ä¸ªå®ä¾‹çš„æ ‡ç­¾ä¸å…¶å¯¹åº”çš„è¢‹å­ä½œä¸ºä¼ªæ ‡ç­¾ï¼ˆå³ï¼Œæ­£è¢‹ä¸­çš„æ‰€æœ‰å®ä¾‹è¢«èµ‹äºˆæ­£æ ‡ç­¾ï¼Œè´Ÿè¢‹ä¸­çš„æ‰€æœ‰å®ä¾‹è¢«èµ‹äºˆè´Ÿæ ‡ç­¾ï¼‰ï¼Œç„¶åä½¿ç”¨ç›‘ç£æ–¹å¼è®­ç»ƒå®ä¾‹åˆ†ç±»å™¨ï¼Œç›´åˆ°å…¶æ”¶æ•›ã€‚æŸå¤±å‡½æ•°é€šå¸¸æ˜¯å®ä¾‹åˆ†ç±»å™¨é¢„æµ‹ç»“æœä¸ä¼ªæ ‡ç­¾ä¹‹é—´çš„äº¤å‰ç†µå‡½æ•°ã€‚è®­ç»ƒåï¼Œå®ä¾‹åˆ†ç±»å™¨ç”¨äºå¯¹æµ‹è¯•è¢‹ä¸­çš„æ‰€æœ‰å®ä¾‹è¿›è¡Œé¢„æµ‹ï¼Œç„¶åå°†æ¯ä¸ªå®ä¾‹çš„é¢„æµ‹ç»“æœè¿›è¡Œèšåˆä»¥è·å¾—è¢‹å­çš„é¢„æµ‹ç»“æœï¼Œè¿™ä¸€èšåˆè¿‡ç¨‹ç§°ä¸ºMIL-poolingã€‚å¸¸ç”¨çš„MIL
    poolingæ–¹æ³•åŒ…æ‹¬å‡å€¼æ± åŒ–ï¼ˆWang *et al.* [2018](#bib.bib182)ï¼‰ã€æœ€å¤§æ± åŒ–ï¼ˆFeng *et al.* [2017](#bib.bib56),
    Wang *et al.* [2018](#bib.bib182), Wu *et al.* [2015](#bib.bib189)ï¼‰ã€æŠ•ç¥¨ï¼ˆCruz-Roa
    *et al.* [2014](#bib.bib40)ï¼‰ã€å¯¹æ•°å’ŒæŒ‡æ•°æ± åŒ–ï¼ˆRamon *et al.* [2000](#bib.bib135)ï¼‰ã€å™ªå£°æˆ–æ± åŒ–ï¼ˆMaron
    *et al.* [1997](#bib.bib110)ï¼‰ã€å™ªå£°ä¸æ± åŒ–ï¼ˆKraus *et al.* [2016](#bib.bib90)ï¼‰å’ŒåŠ¨æ€æ± åŒ–ï¼ˆYan
    *et al.* [2018](#bib.bib197)ï¼‰ç­‰ã€‚'
- en: '![Refer to caption](img/b98364eaa5ad47a7afa0b4eecc6a86ba.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/b98364eaa5ad47a7afa0b4eecc6a86ba.png)'
- en: 'Figure 2: Overview of multiple instance learning methods. (a) Instance-based
    Approach. (b) Bag-based Approach. (c) Two-stage Hybrid Approach. (c) End-to-End
    Hybrid Approach.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šå¤šå®ä¾‹å­¦ä¹ æ–¹æ³•æ¦‚è§ˆã€‚ï¼ˆaï¼‰å®ä¾‹åŒ–æ–¹æ³•ã€‚ï¼ˆbï¼‰è¢‹å­æ–¹æ³•ã€‚ï¼ˆcï¼‰ä¸¤é˜¶æ®µæ··åˆæ–¹æ³•ã€‚ï¼ˆdï¼‰ç«¯åˆ°ç«¯æ··åˆæ–¹æ³•ã€‚
- en: Instance-based approach is more common in early studies, and its main advantage
    lies in the direct prediction of each instance so that the localization task can
    be performed conveniently. However, it has two major drawbacks. First, since the
    true labels of each instance in the positive bags are not necessarily all positive,
    the pseudo labels assigned to the instances in the positive bags are noisy, which
    will lead to inaccurate training of the instance classifier; Second, the MIL-pooling
    method, which aggregates the predictions of instances in each bag, is manually
    designed and non-trainable, making it less flexible and robust. Therefore, the
    performance of these methods is usually limited.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å®ä¾‹åŒ–æ–¹æ³•åœ¨æ—©æœŸç ”ç©¶ä¸­æ›´ä¸ºå¸¸è§ï¼Œå…¶ä¸»è¦ä¼˜ç‚¹åœ¨äºèƒ½å¤Ÿç›´æ¥é¢„æµ‹æ¯ä¸ªå®ä¾‹ï¼Œä»è€Œæ–¹ä¾¿è¿›è¡Œå®šä½ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒæœ‰ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ã€‚é¦–å…ˆï¼Œç”±äºæ­£è¢‹ä¸­æ¯ä¸ªå®ä¾‹çš„çœŸå®æ ‡ç­¾ä¸ä¸€å®šå…¨éƒ¨ä¸ºæ­£æ ‡ç­¾ï¼Œå› æ­¤èµ‹äºˆæ­£è¢‹ä¸­å®ä¾‹çš„ä¼ªæ ‡ç­¾æ˜¯å˜ˆæ‚çš„ï¼Œè¿™ä¼šå¯¼è‡´å®ä¾‹åˆ†ç±»å™¨è®­ç»ƒä¸å‡†ç¡®ï¼›å…¶æ¬¡ï¼ŒMIL-poolingæ–¹æ³•å°†æ¯ä¸ªè¢‹å­ä¸­å®ä¾‹çš„é¢„æµ‹ç»“æœè¿›è¡Œèšåˆï¼Œæ˜¯æ‰‹åŠ¨è®¾è®¡ä¸”ä¸å¯è®­ç»ƒçš„ï¼Œä½¿å…¶çµæ´»æ€§å’Œé²æ£’æ€§è¾ƒå·®ã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•çš„æ€§èƒ½é€šå¸¸å—é™ã€‚
- en: 3.1.2 Bag-based Approach
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 è¢‹å­æ–¹æ³•
- en: 'The main idea of the bag-based approaches is to first extract the features
    of each instance in a bag using shared instance-level feature extractors, then
    use MIL-pooling to aggregate the instance-level features to obtain the bag-level
    features, and then train the bag classifier in a supervised manner until it converges.
    The specific diagram is shown in Figure [2](#S3.F2 "Figure 2 â€£ 3.1.1 Instance-based
    Approach â€£ 3.1 Weakly Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis") (b). The loss function is usually defined as the cross-entropy
    loss between the predictions of the bag classifier and the true bag labels.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºåŒ…çš„æ–¹æ³•çš„ä¸»è¦æ€è·¯æ˜¯é¦–å…ˆä½¿ç”¨å…±äº«çš„å®ä¾‹çº§ç‰¹å¾æå–å™¨æå–åŒ…ä¸­æ¯ä¸ªå®ä¾‹çš„ç‰¹å¾ï¼Œç„¶åä½¿ç”¨ MIL-pooling èšåˆå®ä¾‹çº§ç‰¹å¾ä»¥è·å¾—åŒ…çº§ç‰¹å¾ï¼Œç„¶åä»¥ç›‘ç£æ–¹å¼è®­ç»ƒåŒ…åˆ†ç±»å™¨ç›´è‡³æ”¶æ•›ã€‚å…·ä½“ç¤ºæ„å›¾è§å›¾
    [2](#S3.F2 "å›¾ 2 â€£ 3.1.1 åŸºäºå®ä¾‹çš„æ–¹æ³• â€£ 3.1 å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ â€£ 3 èŒƒå¼ â€£ é¢å‘æ ‡ç­¾æ•ˆç‡è‡ªåŠ¨è¯Šæ–­å’Œåˆ†æï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„å…¨é¢è°ƒæŸ¥")
    (b)ã€‚æŸå¤±å‡½æ•°é€šå¸¸å®šä¹‰ä¸ºåŒ…åˆ†ç±»å™¨é¢„æµ‹ä¸çœŸå®åŒ…æ ‡ç­¾ä¹‹é—´çš„äº¤å‰ç†µæŸå¤±ã€‚
- en: MIL-pooling also exists in bag-based methods, but unlike instance-based methods,
    MIL-pooling here aggregates not the predictions of instances, but the features
    of instances. Mean-pooling, Max-pooling and other aggregation methods can also
    be used as aggregation methods for instance features, but their drawbacks remain,
    i.e., they cannot be trained and adjusted adaptively, so they are often not flexible
    enough.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: MIL-pooling åœ¨åŸºäºåŒ…çš„æ–¹æ³•ä¸­ä¹Ÿå­˜åœ¨ï¼Œä½†ä¸åŒäºåŸºäºå®ä¾‹çš„æ–¹æ³•ï¼ŒMIL-pooling åœ¨è¿™é‡Œèšåˆçš„ä¸æ˜¯å®ä¾‹çš„é¢„æµ‹ç»“æœï¼Œè€Œæ˜¯å®ä¾‹çš„ç‰¹å¾ã€‚å‡å€¼æ± åŒ–ã€æœ€å¤§æ± åŒ–åŠå…¶ä»–èšåˆæ–¹æ³•ä¹Ÿå¯ä»¥ç”¨ä½œå®ä¾‹ç‰¹å¾çš„èšåˆæ–¹æ³•ï¼Œä½†å®ƒä»¬çš„ç¼ºç‚¹ä»ç„¶å­˜åœ¨ï¼Œå³å®ƒä»¬æ— æ³•è‡ªé€‚åº”åœ°è®­ç»ƒå’Œè°ƒæ•´ï¼Œå› æ­¤é€šå¸¸ä¸å¤Ÿçµæ´»ã€‚
- en: The key of the bag-based methods is the training of the bag classifier. Since
    the true labels of the bags are available, there is no noise in their training
    process, so these methods tend to be more accurate than instance-based methods
    in bag classification. However, a serious problem of the bag-based approaches
    is that they cannot perform the localization task easily. Furthermore, the aggregation
    functions for instance features are not flexible enough to show the contribution
    of different instances to bag classification.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºåŒ…çš„æ–¹æ³•çš„å…³é”®åœ¨äºåŒ…åˆ†ç±»å™¨çš„è®­ç»ƒã€‚ç”±äºåŒ…çš„çœŸå®æ ‡ç­¾æ˜¯å·²çŸ¥çš„ï¼Œå› æ­¤åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰å™ªå£°ï¼Œå› æ­¤è¿™äº›æ–¹æ³•åœ¨åŒ…åˆ†ç±»ä¸­å¾€å¾€æ¯”åŸºäºå®ä¾‹çš„æ–¹æ³•æ›´å‡†ç¡®ã€‚ç„¶è€Œï¼ŒåŸºäºåŒ…çš„æ–¹æ³•çš„ä¸€ä¸ªä¸¥é‡é—®é¢˜æ˜¯å®ƒä»¬æ— æ³•è½»æ˜“åœ°æ‰§è¡Œå®šä½ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå®ä¾‹ç‰¹å¾çš„èšåˆå‡½æ•°ä¸å¤Ÿçµæ´»ï¼Œæ— æ³•å±•ç¤ºä¸åŒå®ä¾‹å¯¹åŒ…åˆ†ç±»çš„è´¡çŒ®ã€‚
- en: Attention-based Approach
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•
- en: Ilse *et al.*Â [2018](#bib.bib77) have alleviated these dilemmas. They first
    proposed to use the trainable attention mechanism to aggregate instance features,
    and started a wave of study on attention-based aggregation methods by subsequent
    bag-based methods. They trained both the instance-level feature extractor and
    a bag-level classifier using an end-to-end manner, and used the attention mechanism
    to aggregate the features and measure the significance of each instance. Tu *et
    al.*Â [2019](#bib.bib173) proposed a new end-to-end graph neural network (GNN)
    for instance aggregation. This work is the first GNN-based MIL work. Hashimoto
    *et al.*Â [2020](#bib.bib72) proposed a novel end-to-end method for cancer subtype
    classification by combining MIL, domain adversarial and multiscale learning frameworks.
    Yao, Zhu *et al.*Â [2017](#bib.bib212), [2020](#bib.bib202) proposed a deep attention
    guided MIL framework for cancer survival analysis. They first used a pre-trained
    model from ImageNet (Deng *et al.*Â [2009](#bib.bib47)) to extract the features
    of instances in each bag, and then used K-means algorithm to cluster the instances
    in each bag to obtain the phenotypic patterns, and finally applied attention mechanism
    to aggregate the features of these patterns and performed prediction.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼Šå°”æ–¯*ç­‰äºº* [2018](#bib.bib77) ç¼“è§£äº†è¿™äº›å›°å¢ƒã€‚ä»–ä»¬é¦–æ¬¡æå‡ºä½¿ç”¨å¯è®­ç»ƒçš„æ³¨æ„åŠ›æœºåˆ¶æ¥èšåˆå®ä¾‹ç‰¹å¾ï¼Œå¹¶é€šè¿‡åç»­çš„åŒ…çº§æ–¹æ³•å¼€å¯äº†åŸºäºæ³¨æ„åŠ›çš„èšåˆæ–¹æ³•çš„ç ”ç©¶æµªæ½®ã€‚ä»–ä»¬é‡‡ç”¨ç«¯åˆ°ç«¯æ–¹å¼è®­ç»ƒäº†å®ä¾‹çº§ç‰¹å¾æå–å™¨å’ŒåŒ…çº§åˆ†ç±»å™¨ï¼Œå¹¶ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èšåˆç‰¹å¾å¹¶è¡¡é‡æ¯ä¸ªå®ä¾‹çš„é‡è¦æ€§ã€‚å›¾*ç­‰äºº*
    [2019](#bib.bib173) æå‡ºäº†ç”¨äºå®ä¾‹èšåˆçš„æ–°å‹ç«¯åˆ°ç«¯å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ã€‚è¿™é¡¹å·¥ä½œæ˜¯é¦–ä¸ªåŸºäºGNNçš„MILå·¥ä½œã€‚æ¡¥æœ¬*ç­‰äºº* [2020](#bib.bib72)
    æå‡ºäº†é€šè¿‡ç»“åˆMILã€é¢†åŸŸå¯¹æŠ—å’Œå¤šå°ºåº¦å­¦ä¹ æ¡†æ¶çš„ç™Œç—‡äºšå‹åˆ†ç±»çš„æ–°å‹ç«¯åˆ°ç«¯æ–¹æ³•ã€‚å§šã€æœ±*ç­‰äºº* [2017](#bib.bib212), [2020](#bib.bib202)
    æå‡ºäº†ç”¨äºç™Œç—‡ç”Ÿå­˜åˆ†æçš„æ·±åº¦æ³¨æ„åŠ›å¼•å¯¼MILæ¡†æ¶ã€‚ä»–ä»¬é¦–å…ˆä½¿ç”¨æ¥è‡ªImageNetçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆé‚“*ç­‰äºº* [2009](#bib.bib47)ï¼‰æå–æ¯ä¸ªåŒ…ä¸­å®ä¾‹çš„ç‰¹å¾ï¼Œç„¶åä½¿ç”¨K-meansç®—æ³•å¯¹æ¯ä¸ªåŒ…ä¸­çš„å®ä¾‹è¿›è¡Œèšç±»ï¼Œä»¥è·å¾—è¡¨å‹æ¨¡å¼ï¼Œæœ€ååº”ç”¨æ³¨æ„åŠ›æœºåˆ¶èšåˆè¿™äº›æ¨¡å¼çš„ç‰¹å¾å¹¶è¿›è¡Œé¢„æµ‹ã€‚
- en: Self-supervised Pre-training-based Approach
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åŸºäºè‡ªç›‘ç£é¢„è®­ç»ƒçš„æ–¹æ³•
- en: Due to the extremely large size of WSIs and the large number of instances cut
    out, direct end-to-end training of all instances is easily limited by computational
    resources. Therefore, some studies first use advanced self-supervised pre-training
    methods to characterize each instance and then perform subsequent training. Lu
    *et al.*Â [2019](#bib.bib104) first proposed to obtain instance-level feature representations
    by self-supervised contrastive predictive coding (CPC), and then used the attention-based
    MIL method for instance aggregation to perform bag-level classification. This
    is the first MIL study using self-supervised contrastive learning. Zhao *et al.*Â [2020](#bib.bib206)
    used a pre-trained VAE-GAN (Larsen *et al.*Â [2016](#bib.bib93)) to extract instance-level
    features, and then used GNN to aggregate instance features and perform bag-level
    classification. Li *et al.*Â [2021](#bib.bib97) proposed DSMIL, where they used
    contrastive pre-training (Chen *et al.*Â [2020](#bib.bib26)) to obtain the instance
    features, and then proposed the masked non-local operation-based dual-stream aggregator
    to perform both instance-level classification and bag-level classification.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºWSIsçš„æå¤§å°ºå¯¸å’Œå¤§é‡åˆ‡å‰²å‡ºçš„å®ä¾‹ï¼Œç›´æ¥å¯¹æ‰€æœ‰å®ä¾‹è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒå¾ˆå®¹æ˜“å—åˆ°è®¡ç®—èµ„æºçš„é™åˆ¶ã€‚å› æ­¤ï¼Œä¸€äº›ç ”ç©¶é¦–å…ˆä½¿ç”¨å…ˆè¿›çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•æ¥åˆ»ç”»æ¯ä¸ªå®ä¾‹ï¼Œç„¶åè¿›è¡Œåç»­è®­ç»ƒã€‚å¢*ç­‰äºº*
    [2019](#bib.bib104) é¦–æ¬¡æå‡ºé€šè¿‡è‡ªç›‘ç£å¯¹æ¯”é¢„æµ‹ç¼–ç ï¼ˆCPCï¼‰è·å¾—å®ä¾‹çº§ç‰¹å¾è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„MILæ–¹æ³•è¿›è¡Œå®ä¾‹èšåˆï¼Œä»¥æ‰§è¡ŒåŒ…çº§åˆ†ç±»ã€‚è¿™æ˜¯é¦–ä¸ªä½¿ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„MILç ”ç©¶ã€‚èµµ*ç­‰äºº*
    [2020](#bib.bib206) ä½¿ç”¨é¢„è®­ç»ƒçš„VAE-GANï¼ˆæ‹‰å°”æ£®*ç­‰äºº* [2016](#bib.bib93)ï¼‰æ¥æå–å®ä¾‹çº§ç‰¹å¾ï¼Œç„¶åä½¿ç”¨GNNèšåˆå®ä¾‹ç‰¹å¾å¹¶è¿›è¡ŒåŒ…çº§åˆ†ç±»ã€‚æ*ç­‰äºº*
    [2021](#bib.bib97) æå‡ºäº†DSMILï¼Œä»–ä»¬ä½¿ç”¨å¯¹æ¯”é¢„è®­ç»ƒï¼ˆé™ˆ*ç­‰äºº* [2020](#bib.bib26)ï¼‰æ¥è·å–å®ä¾‹ç‰¹å¾ï¼Œç„¶åæå‡ºäº†åŸºäºæ©è”½éå±€éƒ¨æ“ä½œçš„åŒæµèšåˆå™¨ï¼Œä»¥è¿›è¡Œå®ä¾‹çº§åˆ†ç±»å’ŒåŒ…çº§åˆ†ç±»ã€‚
- en: Transformer Based Approach
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åŸºäºTransformerçš„æ–¹æ³•
- en: In MIL-based WSI analysis, not only the contribution of different instances
    to bag classification should be considered, the relationships among different
    instances should also be fully explored, because different instances in a WSI
    are not isolated from each other, but have strong correlation. To address this
    issue, Shao *et al.*Â [2021](#bib.bib146) and Li *et al.*Â [2021](#bib.bib99) et
    al. used Transformer-based architectures to aggregate instances and both achieved
    promising results. The former designed a Transformer-based correlated MIL framework
    to explore the morphological and spatial information among different instances
    and provided related proofs. The latter presented a MIL framework based on the
    deformable transformer and convolutional layers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸºäºMILçš„WSIåˆ†æä¸­ï¼Œé™¤äº†è€ƒè™‘ä¸åŒå®ä¾‹å¯¹åŒ…åˆ†ç±»çš„è´¡çŒ®å¤–ï¼Œè¿˜åº”å……åˆ†æ¢ç´¢ä¸åŒå®ä¾‹ä¹‹é—´çš„å…³ç³»ï¼Œå› ä¸ºWSIä¸­çš„ä¸åŒå®ä¾‹å¹¶ä¸æ˜¯å½¼æ­¤å­¤ç«‹çš„ï¼Œè€Œæ˜¯æœ‰å¼ºå…³è”çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒShao
    *et al.* [2021](#bib.bib146) å’Œ Li *et al.* [2021](#bib.bib99) ç­‰ä½¿ç”¨äº†åŸºäºTransformerçš„æ¶æ„æ¥èšåˆå®ä¾‹ï¼Œå¹¶å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚å‰è€…è®¾è®¡äº†ä¸€ç§åŸºäºTransformerçš„ç›¸å…³MILæ¡†æ¶æ¥æ¢ç´¢ä¸åŒå®ä¾‹ä¹‹é—´çš„å½¢æ€å’Œç©ºé—´ä¿¡æ¯ï¼Œå¹¶æä¾›äº†ç›¸å…³è¯æ˜ã€‚åè€…æå‡ºäº†ä¸€ç§åŸºäºå¯å˜å½¢å˜æ¢å™¨å’Œå·ç§¯å±‚çš„MILæ¡†æ¶ã€‚
- en: 3.1.3 Hybrid Approach
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 æ··åˆæ–¹æ³•
- en: The hybrid approach combines the advantages of the above two approaches. It
    trains both the instance-level classifier and the bag-level classifier, and uses
    the former to predict the instance-level results while the latter for bag-level
    results. Overall, there are two types of the hybrid approaches. One is the two-stage
    approach and the other is the end-to-end approach.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆæ–¹æ³•ç»“åˆäº†ä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ã€‚å®ƒåŒæ—¶è®­ç»ƒå®ä¾‹çº§åˆ†ç±»å™¨å’ŒåŒ…çº§åˆ†ç±»å™¨ï¼Œä½¿ç”¨å‰è€…é¢„æµ‹å®ä¾‹çº§ç»“æœï¼Œè€Œåè€…ç”¨äºåŒ…çº§ç»“æœã€‚æ€»ä½“è€Œè¨€ï¼Œæ··åˆæ–¹æ³•æœ‰ä¸¤ç§ç±»å‹ï¼Œä¸€ç§æ˜¯ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯ç«¯åˆ°ç«¯æ–¹æ³•ã€‚
- en: Two-stage Hybrid Approach
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ä¸¤é˜¶æ®µæ··åˆæ–¹æ³•
- en: 'The two-stage hybrid approach generally trains the instance classifier by assigning
    each instance in each bag with their corresponding bag labels as pseudo labels,
    and then trains the bag classifier to complete the bag classification based on
    the predictions of the instance classifier. Some studies have also attempted to
    select the key instances in each bag based on the predictions of the instance
    classifier, and then train the bag classifier based on these key instances. The
    specific diagram is shown in Figure [2](#S3.F2 "Figure 2 â€£ 3.1.1 Instance-based
    Approach â€£ 3.1 Weakly Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis") (c). Hou *et al.*Â [2016](#bib.bib76) proposed a new Expectation-Maximization
    (EM) based model. They selected discriminative instances based on spatial relationship
    to train the instance classifier and fed the histogram of instance predictions
    into the multiclass logistic regression model and the SVM model (Chang *et al.*Â [2011](#bib.bib20))
    for bag prediction. Campanella *et al.*Â [2019](#bib.bib17) first selected key
    instances with the maximum prediction probability of the instance classifier in
    the current iteration and assigned pseudo labels of the corresponding bag labels
    to them. Then they fed the features of these key instances into the recurrent
    neural network (RNN) to perform the aggregation and prediction of the bags. Wang
    *et al.*Â [2019](#bib.bib181) selected key instances based on the predictions of
    positive instance probability and fed their features into the global feature descriptor
    and used the random forest algorithm to classify the bags. Chen *et al.*Â [2021](#bib.bib30)
    proposed a focal-aware module (FAM) and used thumbnails of WSI to automatically
    estimate the key regions associated with the diagnosis. Then, the instance features
    at different scales were extracted based on these key regions and aggregated using
    GNN to perform the bag classification.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤é˜¶æ®µæ··åˆæ–¹æ³•é€šå¸¸é€šè¿‡å°†æ¯ä¸ªè¢‹ä¸­çš„æ¯ä¸ªå®ä¾‹åˆ†é…å…¶å¯¹åº”çš„è¢‹æ ‡ç­¾ä½œä¸ºä¼ªæ ‡ç­¾æ¥è®­ç»ƒå®ä¾‹åˆ†ç±»å™¨ï¼Œç„¶åè®­ç»ƒè¢‹åˆ†ç±»å™¨ï¼Œæ ¹æ®å®ä¾‹åˆ†ç±»å™¨çš„é¢„æµ‹å®Œæˆè¢‹åˆ†ç±»ã€‚ä¸€äº›ç ”ç©¶ä¹Ÿå°è¯•åŸºäºå®ä¾‹åˆ†ç±»å™¨çš„é¢„æµ‹é€‰æ‹©æ¯ä¸ªè¢‹ä¸­çš„å…³é”®å®ä¾‹ï¼Œç„¶ååŸºäºè¿™äº›å…³é”®å®ä¾‹è®­ç»ƒè¢‹åˆ†ç±»å™¨ã€‚å…·ä½“å›¾ç¤ºè§å›¾
    [2](#S3.F2 "å›¾ 2 â€£ 3.1.1 å®ä¾‹åŸºç¡€æ–¹æ³• â€£ 3.1 å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ â€£ 3 ä¸ªèŒƒå¼ â€£ è¿ˆå‘æ ‡ç­¾é«˜æ•ˆçš„è‡ªåŠ¨è¯Šæ–­ä¸åˆ†æï¼šå…ˆè¿›æ·±åº¦å­¦ä¹ åŸºç¡€çš„å¼±ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£æŠ€æœ¯åœ¨ç»„ç»‡ç—…ç†å›¾åƒåˆ†æä¸­çš„ç»¼åˆè°ƒæŸ¥")
    (c)ã€‚Hou *ç­‰* [2016](#bib.bib76) æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºäºæœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰çš„æ¨¡å‹ã€‚ä»–ä»¬åŸºäºç©ºé—´å…³ç³»é€‰æ‹©äº†å…·æœ‰åŒºåˆ†æ€§çš„å®ä¾‹æ¥è®­ç»ƒå®ä¾‹åˆ†ç±»å™¨ï¼Œå¹¶å°†å®ä¾‹é¢„æµ‹çš„ç›´æ–¹å›¾è¾“å…¥å¤šç±»é€»è¾‘å›å½’æ¨¡å‹å’ŒSVMæ¨¡å‹ï¼ˆChang
    *ç­‰* [2011](#bib.bib20)ï¼‰è¿›è¡Œè¢‹é¢„æµ‹ã€‚Campanella *ç­‰* [2019](#bib.bib17) é¦–å…ˆåœ¨å½“å‰è¿­ä»£ä¸­é€‰æ‹©äº†å®ä¾‹åˆ†ç±»å™¨é¢„æµ‹æ¦‚ç‡æœ€å¤§çš„å…³é”®å®ä¾‹ï¼Œå¹¶å°†ç›¸åº”è¢‹æ ‡ç­¾çš„ä¼ªæ ‡ç­¾åˆ†é…ç»™å®ƒä»¬ã€‚ç„¶åï¼Œä»–ä»¬å°†è¿™äº›å…³é”®å®ä¾‹çš„ç‰¹å¾è¾“å…¥é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œä»¥æ‰§è¡Œè¢‹çš„èšåˆå’Œé¢„æµ‹ã€‚Wang
    *ç­‰* [2019](#bib.bib181) åŸºäºæ­£å®ä¾‹æ¦‚ç‡çš„é¢„æµ‹é€‰æ‹©äº†å…³é”®å®ä¾‹ï¼Œå¹¶å°†å…¶ç‰¹å¾è¾“å…¥å…¨å±€ç‰¹å¾æè¿°ç¬¦ï¼Œä½¿ç”¨éšæœºæ£®æ—ç®—æ³•å¯¹è¢‹è¿›è¡Œåˆ†ç±»ã€‚Chen *ç­‰*
    [2021](#bib.bib30) æå‡ºäº†ä¸€ä¸ªå…³æ³¨ç„¦ç‚¹æ¨¡å—ï¼ˆFAMï¼‰ï¼Œå¹¶ä½¿ç”¨WSIçš„ç¼©ç•¥å›¾è‡ªåŠ¨ä¼°è®¡ä¸è¯Šæ–­ç›¸å…³çš„å…³é”®åŒºåŸŸã€‚ç„¶åï¼ŒåŸºäºè¿™äº›å…³é”®åŒºåŸŸæå–ä¸åŒå°ºåº¦çš„å®ä¾‹ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨GNNè¿›è¡Œèšåˆä»¥æ‰§è¡Œè¢‹åˆ†ç±»ã€‚
- en: End-to-end Hybrid Approach
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç«¯åˆ°ç«¯æ··åˆæ–¹æ³•
- en: 'The end-to-end hybrid approach generally trains the instance-level classifier
    and the bag-level classifier at the same time. A common approach is to train the
    two classifiers simultaneously by assigning each instance the corresponding bag
    labels as pseudo labels on top of the bag classifier. Some studies also train
    the instance classifier to select the key instances in an epoch first, and then
    train the bag classifier after aggregating the instance features. The specific
    diagram is shown in Figure [2](#S3.F2 "Figure 2 â€£ 3.1.1 Instance-based Approach
    â€£ 3.1 Weakly Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis") (d). Shi *et al.*Â [2020](#bib.bib151) proposed loss-based attention
    MIL. They added an instance-level loss function weighted by the instance attention
    scores based on AB-MIL (Ilse *et al.*Â [2018](#bib.bib77)) as a regularization
    term to improve the recall of instances and used consistency constraints to smooth
    the training process to improve the generalization ability. Chikontwe *et al.*Â [2020](#bib.bib33)
    combined top-k instance selection, instance-level representation learning, and
    bag-level representation in an end-to-end framework. Sharma *et al.*Â [2021](#bib.bib147)
    also combined instance selection, instance-level representation learning and bag-level
    representation in an end-to-end framework. Unlike (Chikontwe *et al.*Â [2020](#bib.bib33)),
    they proposed to use a clustering-based sampling method to select key instances.
    Lu *et al.*Â [2021](#bib.bib105) also proposed a MIL framework based on clustering
    and attention mechanisms. They selected the instances with the largest and smallest
    attention scores in the current bag for clustering to enhance the learning of
    feature space. Myronenko *et al.*Â [2021](#bib.bib116) proposed a MIL framework
    combining the Transformer and CNN architectures to compute the interrelationships
    between instances and aggregate the instances features to accomplish the bag classification.
    They added the instance loss to assist the optimization process.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç«¯åˆ°ç«¯æ··åˆæ–¹æ³•é€šå¸¸åŒæ—¶è®­ç»ƒå®ä¾‹çº§åˆ†ç±»å™¨å’ŒåŒ…çº§åˆ†ç±»å™¨ã€‚ä¸€ä¸ªå¸¸è§çš„æ–¹æ³•æ˜¯é€šè¿‡å°†æ¯ä¸ªå®ä¾‹åˆ†é…åˆ°åŒ…åˆ†ç±»å™¨ä¸Šä½œä¸ºä¼ªæ ‡ç­¾æ¥åŒæ—¶è®­ç»ƒè¿™ä¸¤ä¸ªåˆ†ç±»å™¨ã€‚ä¸€äº›ç ”ç©¶è¿˜è®­ç»ƒå®ä¾‹åˆ†ç±»å™¨é¦–å…ˆåœ¨ä¸€ä¸ªæ—¶æœŸå†…é€‰æ‹©å…³é”®å®ä¾‹ï¼Œç„¶ååœ¨æ±‡æ€»å®ä¾‹ç‰¹å¾åè®­ç»ƒåŒ…åˆ†ç±»å™¨ã€‚å…·ä½“å›¾ç¤ºè§å›¾[2](#S3.F2
    "Figure 2 â€£ 3.1.1 Instance-based Approach â€£ 3.1 Weakly Supervised Learning Paradigm
    â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive
    Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and
    Self-supervised Techniques in Histopathological Image Analysis") (d)ã€‚Shi *et al.*
    [2020](#bib.bib151) æå‡ºäº†åŸºäºæŸå¤±çš„æ³¨æ„åŠ›MILã€‚ä»–ä»¬åœ¨AB-MIL (Ilse *et al.* [2018](#bib.bib77))
    çš„åŸºç¡€ä¸Šå¢åŠ äº†ä¸€ä¸ªæŒ‰å®ä¾‹æ³¨æ„åŠ›åˆ†æ•°åŠ æƒçš„å®ä¾‹çº§æŸå¤±å‡½æ•°ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œä»¥æé«˜å®ä¾‹çš„å¬å›ç‡ï¼Œå¹¶ä½¿ç”¨ä¸€è‡´æ€§çº¦æŸæ¥å¹³æ»‘è®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚Chikontwe *et
    al.* [2020](#bib.bib33) åœ¨ç«¯åˆ°ç«¯æ¡†æ¶ä¸­ç»“åˆäº† top-k å®ä¾‹é€‰æ‹©ã€å®ä¾‹çº§è¡¨ç¤ºå­¦ä¹ å’ŒåŒ…çº§è¡¨ç¤ºã€‚Sharma *et al.* [2021](#bib.bib147)
    ä¹Ÿåœ¨ç«¯åˆ°ç«¯æ¡†æ¶ä¸­ç»“åˆäº†å®ä¾‹é€‰æ‹©ã€å®ä¾‹çº§è¡¨ç¤ºå­¦ä¹ å’ŒåŒ…çº§è¡¨ç¤ºã€‚ä¸ (Chikontwe *et al.* [2020](#bib.bib33)) ä¸åŒï¼Œä»–ä»¬æå‡ºä½¿ç”¨åŸºäºèšç±»çš„é‡‡æ ·æ–¹æ³•æ¥é€‰æ‹©å…³é”®å®ä¾‹ã€‚Lu
    *et al.* [2021](#bib.bib105) è¿˜æå‡ºäº†ä¸€ä¸ªåŸºäºèšç±»å’Œæ³¨æ„åŠ›æœºåˆ¶çš„MILæ¡†æ¶ã€‚ä»–ä»¬åœ¨å½“å‰åŒ…ä¸­é€‰æ‹©æ³¨æ„åŠ›åˆ†æ•°æœ€å¤§å’Œæœ€å°çš„å®ä¾‹è¿›è¡Œèšç±»ï¼Œä»¥å¢å¼ºç‰¹å¾ç©ºé—´çš„å­¦ä¹ ã€‚Myronenko
    *et al.* [2021](#bib.bib116) æå‡ºäº†ä¸€ä¸ªç»“åˆäº† Transformer å’Œ CNN æ¶æ„çš„MILæ¡†æ¶ï¼Œä»¥è®¡ç®—å®ä¾‹ä¹‹é—´çš„ç›¸äº’å…³ç³»å¹¶æ±‡æ€»å®ä¾‹ç‰¹å¾ä»¥å®ŒæˆåŒ…åˆ†ç±»ã€‚ä»–ä»¬å¢åŠ äº†å®ä¾‹æŸå¤±ä»¥è¾…åŠ©ä¼˜åŒ–è¿‡ç¨‹ã€‚'
- en: 3.1.4 Representative Clinical Studies
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 ä»£è¡¨æ€§ä¸´åºŠç ”ç©¶
- en: A large number of outstanding studies have been dedicated to address significant
    clinical problems using weakly supervised methods. For example, Coudray *et al.*Â [2018](#bib.bib39)
    et al. developed deep learning models for accurate prediction of cancer subtypes
    and genetic mutations and sparked the whole field of weakly supervised computational
    pathology. Naik *et al.*Â [2020](#bib.bib118) et al. presented an attention-based
    deep MIL framework to predict directly estrogen receptor status from H&E slices.
    Another typical clinical work comes from Tomita *et al.*Â [2019](#bib.bib172),
    who proposed a grid-based attention network to perform 4-class classification
    of high-resolution endoscopic esophagus and gastroesophageal junction mucosal
    biopsy images from 379 patients. Skrede *et al.*Â [2020](#bib.bib156) developed
    a multi-scale deep MIL-based model to analyze conventional HE-stained slides and
    developed a model that can effectively predict the prognosis of patients after
    colorectal cancer surgery. Another gastrointestinal tract oncology study (Kather
    *et al.*Â [2019](#bib.bib85)) predicted microsatellite instability (MSI) based
    on a deep MIL model directly on HE-stained slides. Currently, weakly supervised
    deep-learning models for digital pathological analysis has been applied in a wide
    range of cancer types including breast, colorectal, lung, liver, cervical, thyroid,
    and bladder cancers (Coudray *et al.*Â [2018](#bib.bib39), Chaudhary *et al.*Â [2018](#bib.bib21),
    Wessels *et al.*Â [2021](#bib.bib186), Campanella *et al.*Â [2019](#bib.bib17),
    Anand *et al.*Â [2021](#bib.bib3), Yang *et al.*Â [2022](#bib.bib198), Li *et al.*Â [2021](#bib.bib98),
    Saillard *et al.*Â [2020](#bib.bib143), Velmahos *et al.*Â [2021](#bib.bib177),
    Woerl *et al.*Â [2020](#bib.bib188)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§é‡æ°å‡ºçš„ç ”ç©¶è‡´åŠ›äºä½¿ç”¨å¼±ç›‘ç£æ–¹æ³•è§£å†³é‡å¤§ä¸´åºŠé—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒCoudray *et al.* [2018](#bib.bib39) ç­‰äººå¼€å‘äº†ç”¨äºç²¾ç¡®é¢„æµ‹ç™Œç—‡äºšå‹å’ŒåŸºå› çªå˜çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ¨åŠ¨äº†æ•´ä¸ªå¼±ç›‘ç£è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„å‘å±•ã€‚Naik
    *et al.* [2020](#bib.bib118) ç­‰äººæå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„æ·±åº¦ MIL æ¡†æ¶ï¼Œä»¥ä» H&E åˆ‡ç‰‡ä¸­ç›´æ¥é¢„æµ‹é›Œæ¿€ç´ å—ä½“çŠ¶æ€ã€‚å¦ä¸€ä¸ªå…¸å‹çš„ä¸´åºŠå·¥ä½œæ¥è‡ª
    Tomita *et al.* [2019](#bib.bib172)ï¼Œä»–ä»¬æå‡ºäº†ä¸€ç§åŸºäºç½‘æ ¼çš„æ³¨æ„åŠ›ç½‘ç»œï¼Œä»¥å¯¹æ¥è‡ª 379 åæ‚£è€…çš„é«˜åˆ†è¾¨ç‡å†…çª¥é•œé£Ÿç®¡å’Œèƒƒé£Ÿç®¡äº¤ç•Œå¤„é»è†œæ´»æ£€å›¾åƒè¿›è¡Œ
    4 ç±»åˆ†ç±»ã€‚Skrede *et al.* [2020](#bib.bib156) å¼€å‘äº†ä¸€ç§å¤šå°ºåº¦æ·±åº¦ MIL åŸºæ¨¡å‹æ¥åˆ†æä¼ ç»Ÿçš„ HE æŸ“è‰²åˆ‡ç‰‡ï¼Œå¹¶å¼€å‘äº†ä¸€ç§å¯ä»¥æœ‰æ•ˆé¢„æµ‹ç»“ç›´è‚ ç™Œæ‰‹æœ¯åæ‚£è€…é¢„åçš„æ¨¡å‹ã€‚å¦ä¸€ä¸ªèƒƒè‚ é“è‚¿ç˜¤ç ”ç©¶ï¼ˆKather
    *et al.* [2019](#bib.bib85)ï¼‰åŸºäºæ·±åº¦ MIL æ¨¡å‹ç›´æ¥åœ¨ HE æŸ“è‰²åˆ‡ç‰‡ä¸Šé¢„æµ‹å¾®å«æ˜Ÿä¸ç¨³å®šæ€§ï¼ˆMSIï¼‰ã€‚ç›®å‰ï¼Œå¼±ç›‘ç£æ·±åº¦å­¦ä¹ æ¨¡å‹å·²ç»å¹¿æ³›åº”ç”¨äºåŒ…æ‹¬ä¹³è…ºç™Œã€ç»“ç›´è‚ ç™Œã€è‚ºç™Œã€è‚ç™Œã€å®«é¢ˆç™Œã€ç”²çŠ¶è…ºç™Œå’Œè†€èƒ±ç™Œåœ¨å†…çš„å¤šç§ç™Œç—‡ç±»å‹ï¼ˆCoudray
    *et al.* [2018](#bib.bib39), Chaudhary *et al.* [2018](#bib.bib21), Wessels *et
    al.* [2021](#bib.bib186), Campanella *et al.* [2019](#bib.bib17), Anand *et al.*
    [2021](#bib.bib3), Yang *et al.* [2022](#bib.bib198), Li *et al.* [2021](#bib.bib98),
    Saillard *et al.* [2020](#bib.bib143), Velmahos *et al.* [2021](#bib.bib177),
    Woerl *et al.* [2020](#bib.bib188))ã€‚
- en: 'Table 3: List of literatures in the weakly supervised learning section.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 3ï¼šå¼±ç›‘ç£å­¦ä¹ éƒ¨åˆ†æ–‡çŒ®åˆ—è¡¨ã€‚
- en: '| Â Â Â Â Â Reference | Â Â Â Â Â Approach | Â Â Â Â Â Disease Type | Â Â Â Â Â Staining | Â Â Â Â Â Task
    | Â Â Â Â Â Dataset | Â Â Â Â Â Dataset Scale | Â Â Â Â Â Dataset Link | Â Â Â Â Â Performance |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â å‚è€ƒæ–‡çŒ® | Â Â Â Â Â æ–¹æ³• | Â Â Â Â Â ç–¾ç—…ç±»å‹ | Â Â Â Â Â æŸ“è‰² | Â Â Â Â Â ä»»åŠ¡ | Â Â Â Â Â æ•°æ®é›† | Â Â Â Â Â æ•°æ®é›†è§„æ¨¡
    | Â Â Â Â Â æ•°æ®é›†é“¾æ¥ | Â Â Â Â Â æ€§èƒ½ |'
- en: '| Â Â Â Â Â Yan etÂ al. ([2018](#bib.bib197)) | Â Â Â Â Â Instance-based | Â Â Â Â Â Breast
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â  Benign and Â Â Â Â Â malignant classification | Â Â Â Â Â UCSB
    breast dataset | Â Â Â Â Â 58 cases | Â Â Â Â Â Kandemir etÂ al. ([2014](#bib.bib80)) | Â Â Â Â Â Accuracy:
    0.927 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Yan ç­‰äºº ([2018](#bib.bib197)) | Â Â Â Â Â åŸºäºå®ä¾‹ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â 
    è‰¯æ€§å’Œæ¶æ€§åˆ†ç±» | Â Â Â Â Â UCSB ä¹³è…ºæ•°æ®é›† | Â Â Â Â Â 58 ä¸ªç—…ä¾‹ | Â Â Â Â Â Kandemir ç­‰äºº ([2014](#bib.bib80))
    | Â Â Â Â Â å‡†ç¡®ç‡ï¼š0.927 |'
- en: '| Â Â Â Â Â  Diabetes (from eye Â Â Â Â Â fundus images) | Â Â Â Â Â Messidor dataset | Â Â Â Â Â 1200
    cases | Â Â Â Â Â DecenciÃ¨re etÂ al. ([2014](#bib.bib44)) | Â Â Â Â Â Accuracy: 0.740 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â  ç³–å°¿ç—…ï¼ˆæ¥è‡ªçœ¼åº•å›¾åƒï¼‰ | Â Â Â Â Â Messidor æ•°æ®é›† | Â Â Â Â Â 1200 ä¸ªç—…ä¾‹ | Â Â Â Â Â DecenciÃ¨re ç­‰äºº
    ([2014](#bib.bib44)) | Â Â Â Â Â å‡†ç¡®ç‡ï¼š0.740 |'
- en: '| Â Â Â Â Â Kraus etÂ al. ([2016](#bib.bib90)) | Â Â Â Â Â Instance-based | Â Â Â Â Â Breast
    Cancer | Â Â Â Â Â  Three channels with Â Â Â Â Â fluorescent markers for Â Â Â Â Â DNA, actin
    filaments, Â Â Â Â Â and b-tubulin | Â Â Â Â Â  Classification of 12 Â Â Â Â Â distinct categories
    | Â Â Â Â Â  Broad Bioimage Benchmark Â Â Â Â Â Collection (BBBC021v1) Dataset | Â Â Â Â Â 340
    cases | Â Â Â Â Â Ljosa etÂ al. ([2012](#bib.bib103)) | Â Â Â Â Â  Accuracy: 0.958 for full
    Â Â Â Â Â image, 0.971 for treatment |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Kraus ç­‰äºº ([2016](#bib.bib90)) | Â Â Â Â Â åŸºäºå®ä¾‹ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â  ä¸‰é€šé“è§å…‰æ ‡è®°ç”¨äº
    DNAã€è‚ŒåŠ¨è›‹ç™½çº¤ç»´å’Œ b-å¾®ç®¡ | Â Â Â Â Â  12 ä¸ªä¸åŒç±»åˆ«çš„åˆ†ç±» | Â Â Â Â Â  Broad Bioimage Benchmark Collection
    (BBBC021v1) æ•°æ®é›† | Â Â Â Â Â 340 ä¸ªç—…ä¾‹ | Â Â Â Â Â Ljosa ç­‰äºº ([2012](#bib.bib103)) | Â Â Â Â Â  å‡†ç¡®ç‡ï¼šå®Œæ•´å›¾åƒ
    0.958ï¼Œå¤„ç†å›¾åƒ 0.971 |'
- en: '| Â Â Â Â Â Cruz-Roa etÂ al. ([2014](#bib.bib40)) | Â Â Â Â Â Instance-based | Â Â Â Â Â Breast
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â  Automatic detection of Â Â Â Â Â invasive ductal carcinoma
    Â Â Â Â Â tissue regions | Â Â Â Â Â  Clinical histopathology dataset Â Â Â Â Â collected from
    multiple hospitals | Â Â Â Â Â 162 cases | Â Â Â Â Â inhouse | Â Â Â Â Â Accuracy: 0.842 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Cruz-Roa ç­‰äºº ([2014](#bib.bib40)) | Â Â Â Â Â åŸºäºå®ä¾‹çš„ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E
    | Â Â Â Â Â  ä¾µè¢­æ€§å¯¼ç®¡ç™Œç»„ç»‡åŒºåŸŸçš„è‡ªåŠ¨æ£€æµ‹ | Â Â Â Â Â  ä¸´åºŠç»„ç»‡ç—…ç†æ•°æ®é›† æ”¶é›†è‡ªå¤šä¸ªåŒ»é™¢ | Â Â Â Â Â 162 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â å†…éƒ¨ | Â Â Â Â Â å‡†ç¡®ç‡:
    0.842 |'
- en: '| Â Â Â Â Â Ilse etÂ al. ([2018](#bib.bib77)) | Â Â Â Â Â Bag-based | Â Â Â Â Â Breast Cancer
    | Â Â Â Â Â H&E | Â Â Â Â Â  Automatic detection Â Â Â Â Â of cancerous regions | Â Â Â Â Â Breast
    cancer dataset | Â Â Â Â Â 58 cases | Â Â Â Â Â Gelasca etÂ al. ([2008](#bib.bib59)) | Â Â Â Â Â Accuracy:
    0.755 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Ilse ç­‰äºº ([2018](#bib.bib77)) | Â Â Â Â Â åŸºäºåŒ…çš„ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â 
    ç™Œå˜åŒºåŸŸçš„è‡ªåŠ¨æ£€æµ‹ | Â Â Â Â Â ä¹³è…ºç™Œæ•°æ®é›† | Â Â Â Â Â 58 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â Gelasca ç­‰äºº ([2008](#bib.bib59)) |
    Â Â Â Â Â å‡†ç¡®ç‡: 0.755 |'
- en: '| Â Â Â Â Â Colon Cancer | Â Â Â Â Â Colon cancer dataset | Â Â Â Â Â 100 cases | Â Â Â Â Â Sirinukunwattana
    etÂ al. ([2016](#bib.bib155)) | Â Â Â Â Â Accuracy: 0.904 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â ç»“è‚ ç™Œ | Â Â Â Â Â ç»“è‚ ç™Œæ•°æ®é›† | Â Â Â Â Â 100 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â Sirinukunwattana ç­‰äºº ([2016](#bib.bib155))
    | Â Â Â Â Â å‡†ç¡®ç‡: 0.904 |'
- en: '| Â Â Â Â Â Tu etÂ al. ([2019](#bib.bib173)) | Â Â Â Â Â Bag-based | Â Â Â Â Â  Diabetes (from
    eye Â Â Â Â Â fundus images) | Â Â Â Â Â H&E | Â Â Â Â Â  Diagnosing diabetes from Â Â Â Â Â weakly
    labeled retinal images | Â Â Â Â Â Messidor dataset | Â Â Â Â Â 1200 cases | Â Â Â Â Â DecenciÃ¨re
    etÂ al. ([2014](#bib.bib44)) | Â Â Â Â Â Accuracy: 0.742 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â æ¶‚ç­‰äºº ([2019](#bib.bib173)) | Â Â Â Â Â åŸºäºåŒ…çš„ | Â Â Â Â Â  ç³–å°¿ç—…ï¼ˆæ¥è‡ªçœ¼åº•å›¾åƒï¼‰ | Â Â Â Â Â H&E
    | Â Â Â Â Â  ä»å¼±æ ‡è®°çš„è§†ç½‘è†œå›¾åƒä¸­è¯Šæ–­ç³–å°¿ç—… | Â Â Â Â Â Messidor æ•°æ®é›† | Â Â Â Â Â 1200 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â DecenciÃ¨re
    ç­‰äºº ([2014](#bib.bib44)) | Â Â Â Â Â å‡†ç¡®ç‡: 0.742 |'
- en: '| Â Â Â Â Â Hashimoto etÂ al. ([2020](#bib.bib72)) | Â Â Â Â Â Bag-based | Â Â Â Â Â Malignant
    Lymphoma | Â Â Â Â Â H&E | Â Â Â Â Â  Classification of malignant Â Â Â Â Â lymphoma sub-types
    | Â Â Â Â Â  Clinical histopathology dataset Â Â Â Â Â collected from multiple hospitals
    | Â Â Â Â Â 196 cases | Â Â Â Â Â inhouse | Â Â Â Â Â Accuracy: 0.871 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Hashimoto ç­‰äºº ([2020](#bib.bib72)) | Â Â Â Â Â åŸºäºåŒ…çš„ | Â Â Â Â Â æ¶æ€§æ·‹å·´ç˜¤ | Â Â Â Â Â H&E
    | Â Â Â Â Â  æ¶æ€§æ·‹å·´ç˜¤äºšå‹åˆ†ç±» | Â Â Â Â Â  ä¸´åºŠç»„ç»‡ç—…ç†æ•°æ®é›† æ”¶é›†è‡ªå¤šä¸ªåŒ»é™¢ | Â Â Â Â Â 196 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â å†…éƒ¨ | Â Â Â Â Â å‡†ç¡®ç‡:
    0.871 |'
- en: '| Â Â Â Â Â Yao etÂ al. ([2020](#bib.bib202)) | Â Â Â Â Â Bag-based | Â Â Â Â Â Lung Cancer
    | Â Â Â Â Â H&E | Â Â Â Â Â Cancer survival prediction | Â Â Â Â Â  National Lung Screening Â Â Â Â Â Trial
    (NLST) dataset | Â Â Â Â Â 387 cases | Â Â Â Â Â Team etÂ al. ([2011](#bib.bib169)) | Â Â Â Â Â AUC:
    0.652 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â å§šç­‰äºº ([2020](#bib.bib202)) | Â Â Â Â Â åŸºäºåŒ…çš„ | Â Â Â Â Â è‚ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â ç™Œç—‡ç”Ÿå­˜é¢„æµ‹
    | Â Â Â Â Â  å›½å®¶è‚ºéƒ¨ç­›æŸ¥è¯•éªŒ (NLST) æ•°æ®é›† | Â Â Â Â Â 387 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â å›¢é˜Ÿç­‰äºº ([2011](#bib.bib169)) |
    Â Â Â Â Â AUC: 0.652 |'
- en: '| Â Â Â Â Â Colorectal Cancer | Â Â Â Â Â  Molecular and Cellular Â Â Â Â Â Oncology (MCO)
    dataset | Â Â Â Â Â 1146 cases | Â Â Â Â Â Ward andÂ Hawkins ([2015](#bib.bib184)) | Â Â Â Â Â AUC:
    0.7143 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â ç»“ç›´è‚ ç™Œ | Â Â Â Â Â  åˆ†å­å’Œç»†èƒè‚¿ç˜¤å­¦ (MCO) æ•°æ®é›† | Â Â Â Â Â 1146 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â Ward å’Œ Hawkins
    ([2015](#bib.bib184)) | Â Â Â Â Â AUC: 0.7143 |'
- en: '| Â Â Â Â Â Lu etÂ al. ([2019](#bib.bib104)) | Â Â Â Â Â Bag-based | Â Â Â Â Â Breast Cancer
    | Â Â Â Â Â H&E | Â Â Â Â Â Classification of normal or benign | Â Â Â Â Â BACH dataset | Â Â Â Â Â 400
    cases | Â Â Â Â Â Aresta etÂ al. ([2019](#bib.bib5)) | Â Â Â Â Â Accuracy: 0.95 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â å¢ç­‰äºº ([2019](#bib.bib104)) | Â Â Â Â Â åŸºäºåŒ…çš„ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â æ­£å¸¸æˆ–è‰¯æ€§åˆ†ç±»
    | Â Â Â Â Â BACH æ•°æ®é›† | Â Â Â Â Â 400 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â Aresta ç­‰äºº ([2019](#bib.bib5)) | Â Â Â Â Â å‡†ç¡®ç‡:
    0.95 |'
- en: '| Â Â Â Â Â Zhao etÂ al. ([2020](#bib.bib206)) | Â Â Â Â Â Bag-based | Â Â Â Â Â Colon Adenocarcinoma
    | Â Â Â Â Â H&E | Â Â Â Â Â Prediction of lymph node metastasis | Â Â Â Â Â  The Cancer Genome
    Â Â Â Â Â Atlas (TCGA) dataset | Â Â Â Â Â 425 cases | Â Â Â Â Â Kandoth etÂ al. ([2013](#bib.bib81))
    | Â Â Â Â Â Accuracy: 0.6761 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â èµµç­‰äºº ([2020](#bib.bib206)) | Â Â Â Â Â åŸºäºåŒ…çš„ | Â Â Â Â Â ç»“è‚ è…ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â æ·‹å·´ç»“è½¬ç§»é¢„æµ‹
    | Â Â Â Â Â  ç™Œç—‡åŸºå› ç»„å›¾è°± (TCGA) æ•°æ®é›† | Â Â Â Â Â 425 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â Kandoth ç­‰äºº ([2013](#bib.bib81))
    | Â Â Â Â Â å‡†ç¡®ç‡: 0.6761 |'
- en: '| Â Â Â Â Â Li, Li andÂ Eliceiri ([2021](#bib.bib97)) | Â Â Â Â Â Bag-based | Â Â Â Â Â Breast
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Detection of lymph node metastases | Â Â Â Â Â Camelyon16
    dataset | Â Â Â Â Â 400 cases | Â Â Â Â Â Bejnordi etÂ al. ([2017b](#bib.bib10)) | Â Â Â Â Â Accuracy:
    0.8992 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â æã€æå’Œ Eliceiri ([2021](#bib.bib97)) | Â Â Â Â Â åŸºäºåŒ…çš„ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E
    | Â Â Â Â Â æ·‹å·´ç»“è½¬ç§»æ£€æµ‹ | Â Â Â Â Â Camelyon16 æ•°æ®é›† | Â Â Â Â Â 400 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â Bejnordi ç­‰äºº ([2017b](#bib.bib10))
    | Â Â Â Â Â å‡†ç¡®ç‡: 0.8992 |'
- en: '| Â Â Â Â Â Lung Cancer | Â Â Â Â Â  Diagnosis of lung Â Â Â Â Â cancer subtypes | Â Â Â Â Â  The
    Cancer Genome Atlas Â Â Â Â Â (TCGA) lung cancer dataset | Â Â Â Â Â 1054 cases | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â Accuracy: 0.9571 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â è‚ºç™Œ | Â Â Â Â Â  è‚ºç™Œäºšå‹çš„è¯Šæ–­ | Â Â Â Â Â  ç™Œç—‡åŸºå› ç»„å›¾è°± (TCGA) è‚ºç™Œæ•°æ®é›† | Â Â Â Â Â 1054 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â å‡†ç¡®ç‡: 0.9571 |'
- en: '| Â Â Â Â Â Shao etÂ al. ([2021](#bib.bib146)) | Â Â Â Â Â Bag-based | Â Â Â Â Â Breast Cancer
    | Â Â Â Â Â H&E | Â Â Â Â Â Detection of lymph node metastases | Â Â Â Â Â Camelyon16 dataset
    | Â Â Â Â Â 400 cases | Â Â Â Â Â Bejnordi etÂ al. ([2017b](#bib.bib10)) | Â Â Â Â Â Accuracy:
    0.8837 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â é‚µç­‰äºº ([2021](#bib.bib146)) | Â Â Â Â Â åŸºäºåŒ…çš„ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â æ·‹å·´ç»“è½¬ç§»æ£€æµ‹
    | Â Â Â Â Â Camelyon16 æ•°æ®é›† | Â Â Â Â Â 400 ä¸ªæ¡ˆä¾‹ | Â Â Â Â Â Bejnordi ç­‰äºº ([2017b](#bib.bib10))
    | Â Â Â Â Â å‡†ç¡®ç‡: 0.8837 |'
- en: '| Â Â Â Â Â Lung Cancer | Â Â Â Â Â Diagnosis of cancer subtypes | Â Â Â Â Â TCGA-NSCLC dataset
    | Â Â Â Â Â 993 cases | Â Â Â Â Â https://portal.gdc.cancer.gov/ | Â Â Â Â Â Accuracy: 0.8835
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â è‚ºç™Œ | Â Â Â Â Â ç™Œç—‡äºšå‹è¯Šæ–­ | Â Â Â Â Â TCGA-NSCLC æ•°æ®é›† | Â Â Â Â Â 993 ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â å‡†ç¡®ç‡: 0.8835 |'
- en: '| Â Â Â Â Â Kidney Cancer | Â Â Â Â Â Diagnosis of cancer subtypes | Â Â Â Â Â TCGA-RCC dataset
    | Â Â Â Â Â 884 cases | Â Â Â Â Â https://portal.gdc.cancer.gov/ | Â Â Â Â Â Accuracy: 0.9466
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â è‚¾ç™Œ | Â Â Â Â Â ç™Œç—‡äºšå‹è¯Šæ–­ | Â Â Â Â Â TCGA-RCC æ•°æ®é›† | Â Â Â Â Â 884 ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â å‡†ç¡®ç‡: 0.9466 |'
- en: '| Â Â Â Â Â Li, Yang, Zhao andÂ Yao ([2021](#bib.bib99)) | Â Â Â Â Â Bag-based | Â Â Â Â Â Breast
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Detection of lymph node metastases | Â Â Â Â Â BREAST-LNM
    dataset | Â Â Â Â Â 3957 cases | Â Â Â Â Â inhouse | Â Â Â Â Â AUC: 0.7288 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Li, Yang, Zhao å’Œ Yao ([2021](#bib.bib99)) | Â Â Â Â Â åŸºäºè¢‹çš„ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E
    | Â Â Â Â Â æ·‹å·´ç»“è½¬ç§»æ£€æµ‹ | Â Â Â Â Â BREAST-LNM æ•°æ®é›† | Â Â Â Â Â 3,957 ä¾‹ | Â Â Â Â Â å†…éƒ¨ | Â Â Â Â Â AUC: 0.7288
    |'
- en: '| Â Â Â Â Â Lung Cancer | Â Â Â Â Â Diagnosis of lung cancer subtypes | Â Â Â Â Â CPTAC-LUAD
    dataset | Â Â Â Â Â 1065 cases | Â Â Â Â Â Clark etÂ al. ([2013](#bib.bib37)) | Â Â Â Â Â AUC:
    0.9906 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â è‚ºç™Œ | Â Â Â Â Â è‚ºç™Œäºšå‹è¯Šæ–­ | Â Â Â Â Â CPTAC-LUAD æ•°æ®é›† | Â Â Â Â Â 1065 ä¾‹ | Â Â Â Â Â Clark ç­‰äºº
    ([2013](#bib.bib37)) | Â Â Â Â Â AUC: 0.9906 |'
- en: '| Â Â Â Â Â Hou etÂ al. ([2016](#bib.bib76)) | Â Â Â Â Â Hybrid | Â Â Â Â Â Glioma | Â Â Â Â Â H&E
    | Â Â Â Â Â Classification of glioma | Â Â Â Â Â  The Cancer Genome Atlas (TCGA) dataset
    | Â Â Â Â Â 209cases | Â Â Â Â Â https://portal.gdc.cancer.gov/ | Â Â Â Â Â Accuracy: 0.771 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Hou ç­‰äºº ([2016](#bib.bib76)) | Â Â Â Â Â æ··åˆ | Â Â Â Â Â èƒ¶è´¨ç˜¤ | Â Â Â Â Â H&E | Â Â Â Â Â èƒ¶è´¨ç˜¤åˆ†ç±»
    | Â Â Â Â Â  ç™Œç—‡åŸºå› ç»„å›¾è°± (TCGA) æ•°æ®é›† | Â Â Â Â Â 209 ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â å‡†ç¡®ç‡: 0.771 |'
- en: '| Â Â Â Â Â Lung Cancer | Â Â Â Â Â  Diagnosis of non-small-cell Â Â Â Â Â lung carcinoma
    subtypes | Â Â Â Â Â 316cases | Â Â Â Â Â Accuracy: 0.798 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â è‚ºç™Œ | Â Â Â Â Â  éå°ç»†èƒè‚ºç™Œäºšå‹è¯Šæ–­ | Â Â Â Â Â 316 ä¾‹ | Â Â Â Â Â å‡†ç¡®ç‡: 0.798 |'
- en: '| Â Â Â Â Â Campanella etÂ al. ([2019](#bib.bib17)) | Â Â Â Â Â Hybrid | Â Â Â Â Â Prostate
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Benign and malignant classification | Â Â Â Â Â Prostate core
    biopsy dataset | Â Â Â Â Â 24859 cases | Â Â Â Â Â in house | Â Â Â Â Â AUC: 0.986 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Campanella ç­‰äºº ([2019](#bib.bib17)) | Â Â Â Â Â æ··åˆ | Â Â Â Â Â å‰åˆ—è…ºç™Œ | Â Â Â Â Â H&E
    | Â Â Â Â Â è‰¯æ€§ä¸æ¶æ€§åˆ†ç±» | Â Â Â Â Â å‰åˆ—è…ºæ ¸å¿ƒæ´»æ£€æ•°æ®é›† | Â Â Â Â Â 24,859 ä¾‹ | Â Â Â Â Â å†…éƒ¨ | Â Â Â Â Â AUC: 0.986 |'
- en: '| Â Â Â Â Â Skin Cancer | Â Â Â Â Â Benign and malignant classification | Â Â Â Â Â Skin dataset
    | Â Â Â Â Â 9,962 cases | Â Â Â Â Â in house | Â Â Â Â Â AUC: 0.986 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â çš®è‚¤ç™Œ | Â Â Â Â Â è‰¯æ€§ä¸æ¶æ€§åˆ†ç±» | Â Â Â Â Â çš®è‚¤æ•°æ®é›† | Â Â Â Â Â 9,962 ä¾‹ | Â Â Â Â Â å†…éƒ¨ | Â Â Â Â Â AUC:
    0.986 |'
- en: '| Â Â Â Â Â Breast Cancer | Â Â Â Â Â Detection of lymph node metastases | Â Â Â Â Â Breast
    dataset | Â Â Â Â Â 9894 cases | Â Â Â Â Â  MSK breast cancer: Â Â Â Â Â http://thomasfuchslab.org/data/.
    | Â Â Â Â Â AUC: 0.965 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â æ·‹å·´ç»“è½¬ç§»æ£€æµ‹ | Â Â Â Â Â ä¹³è…ºæ•°æ®é›† | Â Â Â Â Â 9894 ä¾‹ | Â Â Â Â Â  MSK ä¹³è…ºç™Œ: Â Â Â Â Â http://thomasfuchslab.org/data/.
    | Â Â Â Â Â AUC: 0.965 |'
- en: '| Â Â Â Â Â Wang etÂ al. ([2019](#bib.bib181)) | Â Â Â Â Â Hybrid | Â Â Â Â Â Lung Cancer |
    Â Â Â Â Â H&E | Â Â Â Â Â Diagnosis of lung cancer subtypes | Â Â Â Â Â Lung cancer dataset |
    Â Â Â Â Â 939 cases | Â Â Â Â Â inhouse | Â Â Â Â Â Accuracy: 0.973 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Wang ç­‰äºº ([2019](#bib.bib181)) | Â Â Â Â Â æ··åˆ | Â Â Â Â Â è‚ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â è‚ºç™Œäºšå‹è¯Šæ–­
    | Â Â Â Â Â è‚ºç™Œæ•°æ®é›† | Â Â Â Â Â 939 ä¾‹ | Â Â Â Â Â å†…éƒ¨ | Â Â Â Â Â å‡†ç¡®ç‡: 0.973 |'
- en: '| Â Â Â Â Â Chen etÂ al. ([2021](#bib.bib30)) | Â Â Â Â Â Hybrid | Â Â Â Â Â Breast Cancer
    | Â Â Â Â Â IHC | Â Â Â Â Â  HER2 scoring (negative (0/1+), Â Â Â Â Â equivocal (2+) and positive
    (3+)) | Â Â Â Â Â HER2 scoring dataset | Â Â Â Â Â 1105 cases | Â Â Â Â Â inhouse | Â Â Â Â Â Accuracy:
    0.8970 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Chen ç­‰äºº ([2021](#bib.bib30)) | Â Â Â Â Â æ··åˆ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â IHC | Â Â Â Â Â 
    HER2 è¯„åˆ† (é˜´æ€§ (0/1+)ï¼Œå¯ç–‘ (2+) å’Œé˜³æ€§ (3+)) | Â Â Â Â Â HER2 è¯„åˆ†æ•°æ®é›† | Â Â Â Â Â 1,105 ä¾‹ | Â Â Â Â Â å†…éƒ¨
    | Â Â Â Â Â å‡†ç¡®ç‡: 0.8970 |'
- en: '| Â Â Â Â Â Chikontwe etÂ al. ([2020](#bib.bib33)) | Â Â Â Â Â Hybrid | Â Â Â Â Â Colectoral
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â  Prediction of normal Â Â Â Â Â and malignant tissues | Â Â Â Â Â CRC
    WSI Dataset I | Â Â Â Â Â 173 cases | Â Â Â Â Â inhouse | Â Â Â Â Â Accuracy: 0.9231 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Chikontwe ç­‰äºº ([2020](#bib.bib33)) | Â Â Â Â Â æ··åˆ | Â Â Â Â Â ç»“è‚ ç™Œ | Â Â Â Â Â H&E |
    Â Â Â Â Â  é¢„æµ‹æ­£å¸¸å’Œæ¶æ€§ç»„ç»‡ | Â Â Â Â Â CRC WSI æ•°æ®é›† I | Â Â Â Â Â 173 ä¾‹ | Â Â Â Â Â å†…éƒ¨ | Â Â Â Â Â å‡†ç¡®ç‡: 0.9231
    |'
- en: '| Â Â Â Â Â CRC WSI Dataset II | Â Â Â Â Â 193 cases | Â Â Â Â Â Accuracy: 0.9872 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â CRC WSI æ•°æ®é›† II | Â Â Â Â Â 193 ä¾‹ | Â Â Â Â Â å‡†ç¡®ç‡: 0.9872 |'
- en: '| Â Â Â Â Â Sharma etÂ al. ([2021](#bib.bib147)) | Â Â Â Â Â Hybrid | Â Â Â Â Â  Gastrointestinal
    Â Â Â Â Â Celiac Disease | Â Â Â Â Â H&E | Â Â Â Â Â  Prediction of patients with Â Â Â Â Â celiac
    disease or being healthy | Â Â Â Â Â Gastrointestinal dataset | Â Â Â Â Â 413 cases | Â Â Â Â Â inhouse
    | Â Â Â Â Â Accuracy: 0.862 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Sharma ç­‰äºº ([2021](#bib.bib147)) | Â Â Â Â Â æ··åˆ | Â Â Â Â Â  èƒƒè‚ é“ Â Â Â Â Â ä¹³ç³œæ³» | Â Â Â Â Â H&E
    | Â Â Â Â Â  é¢„æµ‹ä¹³ç³œæ³»æ‚£è€…æˆ–å¥åº·äºº | Â Â Â Â Â èƒƒè‚ é“æ•°æ®é›† | Â Â Â Â Â 413 ä¾‹ | Â Â Â Â Â å†…éƒ¨ | Â Â Â Â Â å‡†ç¡®ç‡: 0.862 |'
- en: '| Â Â Â Â Â Breast Cancer | Â Â Â Â Â Detection of lymph node metastases | Â Â Â Â Â Camelyon16
    dataset | Â Â Â Â Â 400 cases | Â Â Â Â Â Bejnordi etÂ al. ([2017b](#bib.bib10)) | Â Â Â Â Â AUC:
    0.9112 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â æ·‹å·´ç»“è½¬ç§»æ£€æµ‹ | Â Â Â Â Â Camelyon16 æ•°æ®é›† | Â Â Â Â Â 400 ä¾‹ | Â Â Â Â Â Bejnordi
    ç­‰äºº ([2017b](#bib.bib10)) | Â Â Â Â Â AUC: 0.9112 |'
- en: '| Â Â Â Â Â Lu etÂ al. ([2021](#bib.bib105)) | Â Â Â Â Â Hybrid | Â Â Â Â Â Renal Cell Carcinoma
    | Â Â Â Â Â H&E | Â Â Â Â Â  subtyping and the detection Â Â Â Â Â of lymph node metastasis |
    Â Â Â Â Â RCC dataset | Â Â Â Â Â 884 cases | Â Â Â Â Â https://portal.gdc.cancer.gov | Â Â Â Â Â AUC:
    0.991 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Lu ç­‰ ([2021](#bib.bib105)) | Â Â Â Â Â æ··åˆ | Â Â Â Â Â è‚¾ç»†èƒç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â 
    äºšå‹åˆ†ç±»åŠæ·‹å·´ç»“è½¬ç§»çš„æ£€æµ‹ | Â Â Â Â Â RCC æ•°æ®é›† | Â Â Â Â Â 884 ä¸ªç—…ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov
    | Â Â Â Â Â AUC: 0.991 |'
- en: '| Â Â Â Â Â Non-small-cell Lung Cancer | Â Â Â Â Â NSCLC dataset | Â Â Â Â Â 993 cases | Â Â Â Â Â https://cancerimagingarchive.net/datascope/cptac
    | Â Â Â Â Â AUC: 0.956 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â éå°ç»†èƒè‚ºç™Œ | Â Â Â Â Â NSCLC æ•°æ®é›† | Â Â Â Â Â 993 ä¸ªç—…ä¾‹ | Â Â Â Â Â https://cancerimagingarchive.net/datascope/cptac
    | Â Â Â Â Â AUC: 0.956 |'
- en: '| Â Â Â Â Â Breast Cancer | Â Â Â Â Â CAMELYON16 and CAMELYON17 dataset | Â Â Â Â Â 899 cases
    | Â Â Â Â Â https://camelyon17.grand-challenge.org/Data | Â Â Â Â Â AUC: 0.936 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â CAMELYON16 å’Œ CAMELYON17 æ•°æ®é›† | Â Â Â Â Â 899 ä¸ªç—…ä¾‹ | Â Â Â Â Â https://camelyon17.grand-challenge.org/Data
    | Â Â Â Â Â AUC: 0.936 |'
- en: '| Â Â Â Â Â Myronenko etÂ al. ([2021](#bib.bib116)) | Â Â Â Â Â Hybrid | Â Â Â Â Â Prostate
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â  Classifying cancer tissue Â Â Â Â Â into Gleason patterns
    | Â Â Â Â Â  Prostate cANcer graDe Assessment Â Â Â Â Â (PANDA) challenge dataset | Â Â Â Â Â 11,000
    cases | Â Â Â Â Â https://panda.grandchallenge.org/home/ | Â Â Â Â Â Accuracy: 0.805 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Myronenko ç­‰ ([2021](#bib.bib116)) | Â Â Â Â Â æ··åˆ | Â Â Â Â Â å‰åˆ—è…ºç™Œ | Â Â Â Â Â H&E |
    Â Â Â Â Â  ç™Œç»„ç»‡åˆ†ç±» Â Â Â Â Â ä¸º Gleason æ¨¡å¼ | Â Â Â Â Â  å‰åˆ—è…ºç™Œç­‰çº§è¯„ä¼° Â Â Â Â Â (PANDA) æŒ‘æˆ˜æ•°æ®é›† | Â Â Â Â Â 11,000
    ä¸ªç—…ä¾‹ | Â Â Â Â Â https://panda.grandchallenge.org/home/ | Â Â Â Â Â å‡†ç¡®åº¦: 0.805 |'
- en: '| Â Â Â Â Â Naik etÂ al. ([2020](#bib.bib118)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Breast
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â  Determination of Â Â Â Â Â hormonal receptor status | Â Â Â Â Â 
    Australian Breast Cancer Â Â Â Â Â Tissue Bank (ABCTB) dataset | Â Â Â Â Â 2535 cases |
    Â Â Â Â Â https://abctb.org.au/abctbNew2/ACCESSPOLICY.pdf | Â Â Â Â Â AUC: 0.92 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Naik ç­‰ ([2020](#bib.bib118)) | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â 
    æ¿€ç´ å—ä½“çŠ¶æ€çš„ç¡®å®š | Â Â Â Â Â  æ¾³å¤§åˆ©äºšä¹³è…ºç™Œç»„ç»‡åº“ Â Â Â Â Â (ABCTB) æ•°æ®é›† | Â Â Â Â Â 2535 ä¸ªç—…ä¾‹ | Â Â Â Â Â https://abctb.org.au/abctbNew2/ACCESSPOLICY.pdf
    | Â Â Â Â Â AUC: 0.92 |'
- en: '| Â Â Â Â Â  The Cancer Genome Atlas (TCGA) dataset | Â Â Â Â Â 1014 cases | Â Â Â Â Â https://portal.gdc.cancer.gov
    | Â Â Â Â Â AUC: 0.861 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â  ç™Œç—‡åŸºå› ç»„å›¾è°± (TCGA) æ•°æ®é›† | Â Â Â Â Â 1014 ä¸ªç—…ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov
    | Â Â Â Â Â AUC: 0.861 |'
- en: '| Â Â Â Â Â Tomita etÂ al. ([2019](#bib.bib172)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Esophagus
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â  Detection of cancerous and Â Â Â Â Â precancerous esophagus
    tissue | Â Â Â Â Â Esophagus cancer dataset | Â Â Â Â Â 180 cases | Â Â Â Â Â inhouse | Â Â Â Â Â Accuracy:
    0.83 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Tomita ç­‰ ([2019](#bib.bib172)) | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â é£Ÿç®¡ç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â 
    ç™Œæ€§å’Œç™Œå‰é£Ÿç®¡ç»„ç»‡çš„æ£€æµ‹ | Â Â Â Â Â é£Ÿç®¡ç™Œæ•°æ®é›† | Â Â Â Â Â 180 ä¸ªç—…ä¾‹ | Â Â Â Â Â å†…éƒ¨æ•°æ® | Â Â Â Â Â å‡†ç¡®åº¦: 0.83 |'
- en: '| Â Â Â Â Â Skrede etÂ al. ([2020](#bib.bib156)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Colorectal
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Prediction of colorectal cancer outcome | Â Â Â Â Â Colorectal
    cancer dataset | Â Â Â Â Â 2473 cases | Â Â Â Â Â inhouse | Â Â Â Â Â  Ratio for poor versus
    Â Â Â Â Â good prognosis: 3.84 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Skrede ç­‰ ([2020](#bib.bib156)) | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â ç»“ç›´è‚ ç™Œ | Â Â Â Â Â H&E |
    Â Â Â Â Â ç»“ç›´è‚ ç™Œé¢„åé¢„æµ‹ | Â Â Â Â Â ç»“ç›´è‚ ç™Œæ•°æ®é›† | Â Â Â Â Â 2473 ä¸ªç—…ä¾‹ | Â Â Â Â Â å†…éƒ¨æ•°æ® | Â Â Â Â Â  ä¸è‰¯é¢„åä¸ Â Â Â Â Â è‰¯å¥½é¢„åçš„æ¯”ä¾‹:
    3.84 |'
- en: '| Â Â Â Â Â Kather *et al.*Â ([2019](#bib.bib85)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Gastrointestinal
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Prediction of microsatellite instability | Â Â Â Â Â TCGA-STAD
    dataset | Â Â Â Â Â 315 cases | Â Â Â Â Â https://portal.gdc.cancer.gov/. | Â Â Â Â Â AUC: 0.81
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Kather *et al.* ([2019](#bib.bib85)) | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â èƒƒè‚ ç™Œ | Â Â Â Â Â H&E
    | Â Â Â Â Â å¾®å«æ˜Ÿä¸ç¨³å®šæ€§çš„é¢„æµ‹ | Â Â Â Â Â TCGA-STAD æ•°æ®é›† | Â Â Â Â Â 315 ä¸ªç—…ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov/.
    | Â Â Â Â Â AUC: 0.81 |'
- en: '| Â Â Â Â Â TCGA-CRC-DX dataset | Â Â Â Â Â 360 cases | Â Â Â Â Â AUC: 0.84 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â TCGA-CRC-DX æ•°æ®é›† | Â Â Â Â Â 360 ä¸ªç—…ä¾‹ | Â Â Â Â Â AUC: 0.84 |'
- en: '| Â Â Â Â Â TCGA-CRC-KR dataset | Â Â Â Â Â 378 cases | Â Â Â Â Â AUC: 0.77 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â TCGA-CRC-KR æ•°æ®é›† | Â Â Â Â Â 378 ä¸ªç—…ä¾‹ | Â Â Â Â Â AUC: 0.77 |'
- en: '| Â Â Â Â Â Coudray etÂ al. ([2018](#bib.bib39)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Lung
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Classification of subtypes | Â Â Â Â Â  The Cancer Genome
    Atlas (TCGA) dataset | Â Â Â Â Â 1634 cases | Â Â Â Â Â https://portal.gdc.cancer.gov/ |
    Â Â Â Â Â AUC: 0.97 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Coudray ç­‰ ([2018](#bib.bib39)) | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â è‚ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â äºšå‹åˆ†ç±»
    | Â Â Â Â Â  ç™Œç—‡åŸºå› ç»„å›¾è°± (TCGA) æ•°æ®é›† | Â Â Â Â Â 1634 ä¸ªç—…ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â AUC: 0.97 |'
- en: '| Â Â Â Â Â  Prediction of mutation from Â Â Â Â Â non-small cell lung cancer | Â Â Â Â Â 
    AUC of six of commonly mutated Â Â Â Â Â genes from 0.733 to 0.856 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â  ä»éå°ç»†èƒè‚ºç™Œä¸­é¢„æµ‹çªå˜ | Â Â Â Â Â  å…­ç§å¸¸è§çªå˜åŸºå› çš„ AUC ä» 0.733 åˆ° 0.856 |'
- en: '| Â Â Â Â Â Bejnordi etÂ al. ([2017a](#bib.bib9)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Breast
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Detection of lymph node metastases | Â Â Â Â Â CAMELYON16
    dataset | Â Â Â Â Â 400 cases | Â Â Â Â Â https://camelyon16.grand-challenge.org/ | Â Â Â Â Â AUC:
    0.994 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Bejnordi ç­‰ ([2017a](#bib.bib9)) | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E |
    Â Â Â Â Â æ·‹å·´ç»“è½¬ç§»çš„æ£€æµ‹ | Â Â Â Â Â CAMELYON16 æ•°æ®é›† | Â Â Â Â Â 400 ä¸ªç—…ä¾‹ | Â Â Â Â Â https://camelyon16.grand-challenge.org/
    | Â Â Â Â Â AUC: 0.994 |'
- en: '| Â Â Â Â Â Wessels etÂ al. ([2021](#bib.bib186)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Prostate
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Prediction lymph node metastasis | Â Â Â Â Â Prostate cancer
    dataset | Â Â Â Â Â 218 cases | Â Â Â Â Â inhouse | Â Â Â Â Â AUC: 0.68 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Wesselsç­‰äººï¼ˆ[2021](#bib.bib186)ï¼‰ | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â å‰åˆ—è…ºç™Œ | Â Â Â Â Â H&E |
    Â Â Â Â Â é¢„æµ‹æ·‹å·´ç»“è½¬ç§» | Â Â Â Â Â å‰åˆ—è…ºç™Œæ•°æ®é›† | Â Â Â Â Â 218ä¾‹ | Â Â Â Â Â å†…éƒ¨æ•°æ® | Â Â Â Â Â AUC: 0.68 |'
- en: '| Â Â Â Â Â Anand etÂ al. ([2021](#bib.bib3)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Thyroid
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Prediction of BRAF mutation | Â Â Â Â Â  ISBI 2017 Thyroid
    Tissue Â Â Â Â Â Microarray (TH-TMA17) dataset | Â Â Â Â Â 85 cases | Wang *et al.*Â ([2018](#bib.bib180))
    | Â Â Â Â Â AUC: 0.96 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Anandç­‰äººï¼ˆ[2021](#bib.bib3)ï¼‰ | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â ç”²çŠ¶è…ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â é¢„æµ‹BRAFçªå˜
    | Â Â Â Â Â  ISBI 2017ç”²çŠ¶è…ºç»„ç»‡å¾®é˜µåˆ—ï¼ˆTH-TMA17ï¼‰æ•°æ®é›† | Â Â Â Â Â 85ä¾‹ | Wang *ç­‰äºº*ï¼ˆ[2018](#bib.bib180)ï¼‰
    | Â Â Â Â Â AUC: 0.96 |'
- en: '| Â Â Â Â Â TCGA-THCA dataset | Â Â Â Â Â 444 cases | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â AUC: 0.98 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â TCGA-THCAæ•°æ®é›† | Â Â Â Â Â 444ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov/ | Â Â Â Â Â AUC:
    0.98 |'
- en: '| Â Â Â Â Â Yang etÂ al. ([2022](#bib.bib198)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Breast
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â  Prediction of HER2-positive breast Â Â Â Â Â cancer recurrence
    and metastasis risk | Â Â Â Â Â HER2-positive breast cancer dataset | Â Â Â Â Â 127 cases
    | Â Â Â Â Â https://github.com/bensteven2/HE_breast_recurrence | Â Â Â Â Â AUC: 0.76 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â æ¨ç­‰äººï¼ˆ[2022](#bib.bib198)ï¼‰ | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â  é¢„æµ‹HER2é˜³æ€§ä¹³è…ºç™Œå¤å‘å’Œè½¬ç§»é£é™©
    | Â Â Â Â Â HER2é˜³æ€§ä¹³è…ºç™Œæ•°æ®é›† | Â Â Â Â Â 127ä¾‹ | Â Â Â Â Â https://github.com/bensteven2/HE_breast_recurrence
    | Â Â Â Â Â AUC: 0.76 |'
- en: '| Â Â Â Â Â The Cancer Genome Atlas (TCGA) dataset | Â Â Â Â Â 123 cases | Â Â Â Â Â AUC:
    0.72 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â ç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰æ•°æ®é›† | Â Â Â Â Â 123ä¾‹ | Â Â Â Â Â AUC: 0.72 |'
- en: '| Li *et al.*Â ([2021](#bib.bib98)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Breast Cancer
    | Â Â Â Â Â H&E | Â Â Â Â Â  Predicting biomarker of Â Â Â Â Â pathological complete response
    Â Â Â Â Â to neoadjuvant chemotherapy | Â Â Â Â Â Breast cancer dataset | Â Â Â Â Â 540 cases
    | Â Â Â Â Â inhouse | Â Â Â Â Â AUC: 0.847 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| æ*ç­‰äºº*ï¼ˆ[2021](#bib.bib98)ï¼‰ | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â ä¹³è…ºç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â  é¢„æµ‹æ–°è¾…åŠ©åŒ–ç–—çš„ç—…ç†å®Œå…¨ååº”ç”Ÿç‰©æ ‡å¿—ç‰©
    | Â Â Â Â Â ä¹³è…ºç™Œæ•°æ®é›† | Â Â Â Â Â 540ä¾‹ | Â Â Â Â Â å†…éƒ¨æ•°æ® | Â Â Â Â Â AUC: 0.847 |'
- en: '| Â Â Â Â Â Saillard etÂ al. ([2020](#bib.bib143)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Hepatocellular
    Carcinoma | Â Â Â Â Â H&E | Â Â Â Â Â  Predicting survival after Â Â Â Â Â hepatocellular carcinoma
    resection | Â Â Â Â Â Discovery set | Â Â Â Â Â 194 cases | Â Â Â Â Â inhouse | Â Â Â Â Â C-Indices:
    0.78 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Saillardç­‰äººï¼ˆ[2020](#bib.bib143)ï¼‰ | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â è‚ç»†èƒç™Œ | Â Â Â Â Â H&E |
    Â Â Â Â Â  é¢„æµ‹è‚ç»†èƒç™Œåˆ‡é™¤åçš„ç”Ÿå­˜ç‡ | Â Â Â Â Â å‘ç°é›† | Â Â Â Â Â 194ä¾‹ | Â Â Â Â Â å†…éƒ¨æ•°æ® | Â Â Â Â Â C-æŒ‡æ•°ï¼š0.78 |'
- en: '| Â Â Â Â Â The Cancer Genome Atlas (TCGA) dataset | Â Â Â Â Â 328 cases | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â C-Indices: 0.70 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â ç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰æ•°æ®é›† | Â Â Â Â Â 328ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov/ |
    Â Â Â Â Â C-æŒ‡æ•°ï¼š0.70 |'
- en: '| Â Â Â Â Â Velmahos etÂ al. ([2021](#bib.bib177)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Bladder
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Identifying FGFR-activating mutations | Â Â Â Â Â  The Cancer
    Genome Â Â Â Â Â Atlas (TCGA) dataset | Â Â Â Â Â 418 cases | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â AUC = 0.76 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Velmahosç­‰äººï¼ˆ[2021](#bib.bib177)ï¼‰ | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â è†€èƒ±ç™Œ | Â Â Â Â Â H&E |
    Â Â Â Â Â è¯†åˆ«FGFRæ¿€æ´»çªå˜ | Â Â Â Â Â  ç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰æ•°æ®é›† | Â Â Â Â Â 418ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â AUC = 0.76 |'
- en: '| Â Â Â Â Â Woerl etÂ al. ([2020](#bib.bib188)) | Â Â Â Â Â Clinical Studies | Â Â Â Â Â Bladder
    Cancer | Â Â Â Â Â H&E | Â Â Â Â Â Prediction of molecular subtypes | Â Â Â Â Â  The Cancer Genome
    Atlas (TCGA) Â Â Â Â Â Urothelial Bladder Carcinoma Dataset | Â Â Â Â Â 407 cases | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â AUC = 0.89 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Woerlç­‰äººï¼ˆ[2020](#bib.bib188)ï¼‰ | Â Â Â Â Â ä¸´åºŠç ”ç©¶ | Â Â Â Â Â è†€èƒ±ç™Œ | Â Â Â Â Â H&E | Â Â Â Â Â é¢„æµ‹åˆ†å­äºšå‹
    | Â Â Â Â Â  ç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰å°¿è·¯ä¸Šçš®è†€èƒ±ç™Œæ•°æ®é›† | Â Â Â Â Â 407ä¾‹ | Â Â Â Â Â https://portal.gdc.cancer.gov/
    | Â Â Â Â Â AUC = 0.89 |'
- en: '| Â Â Â Â Â CCC-EMN cohort | Â Â Â Â Â 16 cases | Â Â Â Â Â inhouse | Â Â Â Â Â AUC = 0.85 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â CCC-EMNé˜Ÿåˆ— | Â Â Â Â Â 16ä¾‹ | Â Â Â Â Â å†…éƒ¨æ•°æ® | Â Â Â Â Â AUC = 0.85 |'
- en: 3.2 Semi-Supervised Learning Paradigm
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 åŠç›‘ç£å­¦ä¹ èŒƒå¼
- en: Semi-supervised learning is a branch of machine learning that combines both
    supervised and unsupervised learning tasks and improves model performance by exploiting
    the information associated between tasks (Zhu *et al.*Â [2005](#bib.bib211), Van
    *et al.*Â [2020](#bib.bib175)). In semi-supervised learning, only a small amount
    of labeled data is generally available, and besides that, a large amount of unlabeled
    data can be utilized for network training. Consequently, the main goal of semi-supervised
    learning is how to use these unlabeled data to improve the performance of the
    model trained with limited labeled data. Scenarios of the semi-supervised learning
    paradigm are very common in the field of pathological image analysis, both in
    diagnostic tasks and in segmentation tasks. Due to the expensive and time-consuming
    fine-grained annotation, pathologists often can only provide a small number of
    precise annotations for supervised training of the models, while a large amount
    of unannotated data cannot be used. Training deep models with only these limited
    labeled data can easily lead to over-fitting, thus significantly harming the performance
    and generalization of the models. In the semi-supervised learning paradigm, a
    large number of unlabeled images can be used to assist in training and thus further
    improve the performance, generalization, and robustness of the models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: åŠç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒç»“åˆäº†ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶é€šè¿‡åˆ©ç”¨ä»»åŠ¡é—´çš„ä¿¡æ¯æ¥æé«˜æ¨¡å‹æ€§èƒ½ï¼ˆZhu *et al.* [2005](#bib.bib211),
    Van *et al.* [2020](#bib.bib175)ï¼‰ã€‚åœ¨åŠç›‘ç£å­¦ä¹ ä¸­ï¼Œé€šå¸¸åªæœ‰å°‘é‡æ ‡è®°æ•°æ®å¯ç”¨ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œè¿˜å¯ä»¥åˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„æ•°æ®è¿›è¡Œç½‘ç»œè®­ç»ƒã€‚å› æ­¤ï¼ŒåŠç›‘ç£å­¦ä¹ çš„ä¸»è¦ç›®æ ‡æ˜¯å¦‚ä½•åˆ©ç”¨è¿™äº›æœªæ ‡è®°çš„æ•°æ®æ¥æé«˜ç”¨æœ‰é™æ ‡è®°æ•°æ®è®­ç»ƒçš„æ¨¡å‹çš„æ€§èƒ½ã€‚åŠç›‘ç£å­¦ä¹ èŒƒå¼çš„åœºæ™¯åœ¨ç—…ç†å›¾åƒåˆ†æé¢†åŸŸéå¸¸æ™®éï¼Œæ— è®ºæ˜¯åœ¨è¯Šæ–­ä»»åŠ¡è¿˜æ˜¯åˆ†å‰²ä»»åŠ¡ä¸­ã€‚ç”±äºç²¾ç»†æ ‡æ³¨çš„æˆæœ¬é«˜ä¸”è€—æ—¶ï¼Œç—…ç†å­¦å®¶é€šå¸¸åªèƒ½æä¾›å°‘é‡ç²¾ç¡®çš„æ ‡æ³¨æ¥è¿›è¡Œæ¨¡å‹çš„ç›‘ç£è®­ç»ƒï¼Œè€Œå¤§é‡æœªæ ‡æ³¨çš„æ•°æ®æ— æ³•è¢«åˆ©ç”¨ã€‚ä»…ç”¨è¿™äº›æœ‰é™çš„æ ‡è®°æ•°æ®è®­ç»ƒæ·±åº¦æ¨¡å‹å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œä»è€Œæ˜¾è‘—æŸå®³æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨åŠç›‘ç£å­¦ä¹ èŒƒå¼ä¸­ï¼Œå¤§é‡æœªæ ‡è®°å›¾åƒå¯ä»¥ç”¨äºè¾…åŠ©è®­ç»ƒï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚
- en: 'In the past two decades, numerous semi-supervised learning algorithms have
    been proposed and widely used in the fields of natural image processing and pathological
    image analysis. The representative approaches in the field of semi-supervised
    learning are divided into five categories, namely pseudo-labelling-based approach
    (Section [3.2.1](#S3.SS2.SSS1 "3.2.1 Pseudo-labelling-based Approach â€£ 3.2 Semi-Supervised
    Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis
    and Analysis: A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised,
    Semi-supervised and Self-supervised Techniques in Histopathological Image Analysis")),
    consistency-based approach (Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Consistency-based
    Approach â€£ 3.2 Semi-Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis")), graph-based approach (Section [3.2.3](#S3.SS2.SSS3 "3.2.3 Graph-based
    Approach â€£ 3.2 Semi-Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis")), unsupervised-preprocessing approach (Section [3.2.4](#S3.SS2.SSS4
    "3.2.4 Unsupervised-preprocessing-based Approach â€£ 3.2 Semi-Supervised Learning
    Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis:
    A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised
    and Self-supervised Techniques in Histopathological Image Analysis")), and other
    approaches (Section [3.2.5](#S3.SS2.SSS5 "3.2.5 Other Approaches â€£ 3.2 Semi-Supervised
    Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis
    and Analysis: A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised,
    Semi-supervised and Self-supervised Techniques in Histopathological Image Analysis")).
    We introduce these methods below, respectively. For each category, we first describe
    their fundamental principles and then elaborate on their representative studies
    in the field of pathological image analysis. For a systematic review of the assumptions,
    concepts and representative methods of semi-supervised learning in the field of
    natural images, we recommend the review by Van *et al.*Â [2020](#bib.bib175). Table
    [4](#S3.T4 "Table 4 â€£ 3.2.5 Other Approaches â€£ 3.2 Semi-Supervised Learning Paradigm
    â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive
    Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and
    Self-supervised Techniques in Histopathological Image Analysis") summarizes the
    detailed list of literatures in this section.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¿‡å»çš„äºŒåå¹´ä¸­ï¼Œè®¸å¤šåŠç›‘ç£å­¦ä¹ ç®—æ³•è¢«æå‡ºå¹¶å¹¿æ³›åº”ç”¨äºè‡ªç„¶å›¾åƒå¤„ç†å’Œç—…ç†å›¾åƒåˆ†æé¢†åŸŸã€‚åŠç›‘ç£å­¦ä¹ é¢†åŸŸçš„ä»£è¡¨æ€§æ–¹æ³•åˆ†ä¸ºäº”ç±»ï¼Œå³åŸºäºä¼ªæ ‡è®°çš„æ–¹æ³•ï¼ˆç¬¬[3.2.1](#S3.SS2.SSS1
    "3.2.1 Pseudo-labelling-based Approach â€£ 3.2 Semi-Supervised Learning Paradigm
    â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive
    Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and
    Self-supervised Techniques in Histopathological Image Analysis")èŠ‚ï¼‰ï¼ŒåŸºäºä¸€è‡´æ€§çš„æ–¹æ³•ï¼ˆç¬¬[3.2.2](#S3.SS2.SSS2
    "3.2.2 Consistency-based Approach â€£ 3.2 Semi-Supervised Learning Paradigm â€£ 3
    Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive
    Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and
    Self-supervised Techniques in Histopathological Image Analysis")èŠ‚ï¼‰ï¼ŒåŸºäºå›¾çš„æ–¹æ³•ï¼ˆç¬¬[3.2.3](#S3.SS2.SSS3
    "3.2.3 Graph-based Approach â€£ 3.2 Semi-Supervised Learning Paradigm â€£ 3 Paradigms
    â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive Survey
    of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and Self-supervised
    Techniques in Histopathological Image Analysis")èŠ‚ï¼‰ï¼Œæ— ç›‘ç£é¢„å¤„ç†æ–¹æ³•ï¼ˆç¬¬[3.2.4](#S3.SS2.SSS4
    "3.2.4 Unsupervised-preprocessing-based Approach â€£ 3.2 Semi-Supervised Learning
    Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis:
    A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised
    and Self-supervised Techniques in Histopathological Image Analysis")èŠ‚ï¼‰ï¼Œä»¥åŠå…¶ä»–æ–¹æ³•ï¼ˆç¬¬[3.2.5](#S3.SS2.SSS5
    "3.2.5 Other Approaches â€£ 3.2 Semi-Supervised Learning Paradigm â€£ 3 Paradigms
    â€£ Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive Survey
    of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and Self-supervised
    Techniques in Histopathological Image Analysis")èŠ‚ï¼‰ã€‚æˆ‘ä»¬å°†åˆ†åˆ«ä»‹ç»è¿™äº›æ–¹æ³•ã€‚å¯¹äºæ¯ä¸€ç±»ï¼Œæˆ‘ä»¬é¦–å…ˆæè¿°å…¶åŸºæœ¬åŸç†ï¼Œç„¶åè¯¦ç»†é˜è¿°å…¶åœ¨ç—…ç†å›¾åƒåˆ†æé¢†åŸŸçš„ä»£è¡¨æ€§ç ”ç©¶ã€‚æœ‰å…³è‡ªç„¶å›¾åƒé¢†åŸŸåŠç›‘ç£å­¦ä¹ çš„å‡è®¾ã€æ¦‚å¿µå’Œä»£è¡¨æ€§æ–¹æ³•çš„ç³»ç»Ÿç»¼è¿°ï¼Œæˆ‘ä»¬æ¨èVan
    *et al.* [2020](#bib.bib175)çš„ç»¼è¿°ã€‚è¡¨[4](#S3.T4 "Table 4 â€£ 3.2.5 Other Approaches
    â€£ 3.2 Semi-Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis")æ€»ç»“äº†æœ¬èŠ‚æ–‡çŒ®çš„è¯¦ç»†åˆ—è¡¨ã€‚'
- en: 3.2.1 Pseudo-labelling-based Approach
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 åŸºäºä¼ªæ ‡è®°çš„æ–¹æ³•
- en: Fundamental Principles
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åŸºæœ¬åŸç†
- en: The pseudo-labeling-based approach is a classical and well-known semi-supervised
    method (Zhu *et al.*Â [2005](#bib.bib211)), which mainly consists of two alternating
    processes, training and pseudo-labeling. Taking the classification problem as
    an example, in the training process, one or more classifiers are first trained
    in a supervised manner on the labeled data. The labeled data may be derived from
    the initial accurately labeled data or from the pseudo-labeled data from the previous
    iterations. In the pseudo-labeling process, all the unlabeled data are first predicted
    using the classifier trained in the previous process, and then the most confidently
    predicted portion of the data are selected for pseudo-labeling. Finally, these
    pseudo-labeled data are added to the labeled data for the next iteration. This
    process is repeated until no data with high confidence are found or all data are
    labeled.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºä¼ªæ ‡ç­¾çš„æ–¹æ³•æ˜¯ä¸€ç§ç»å…¸ä¸”çŸ¥åçš„åŠç›‘ç£æ–¹æ³•ï¼ˆZhu *ç­‰äºº* [2005](#bib.bib211)ï¼‰ï¼Œä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªäº¤æ›¿çš„è¿‡ç¨‹ï¼šè®­ç»ƒå’Œä¼ªæ ‡ç­¾ã€‚ä»¥åˆ†ç±»é—®é¢˜ä¸ºä¾‹ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªæˆ–å¤šä¸ªåˆ†ç±»å™¨é¦–å…ˆåœ¨æ ‡è®°æ•°æ®ä¸Šä»¥ç›‘ç£æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚è¿™äº›æ ‡è®°æ•°æ®å¯èƒ½æ¥æºäºåˆå§‹å‡†ç¡®æ ‡è®°çš„æ•°æ®ï¼Œæˆ–æ¥è‡ªäºä¹‹å‰è¿­ä»£ä¸­äº§ç”Ÿçš„ä¼ªæ ‡ç­¾æ•°æ®ã€‚åœ¨ä¼ªæ ‡ç­¾è¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆä½¿ç”¨åœ¨å‰ä¸€è¿‡ç¨‹è®­ç»ƒçš„åˆ†ç±»å™¨å¯¹æ‰€æœ‰æœªæ ‡è®°çš„æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œç„¶åé€‰æ‹©é¢„æµ‹æœ€ä¸ºè‡ªä¿¡çš„æ•°æ®éƒ¨åˆ†è¿›è¡Œä¼ªæ ‡ç­¾ã€‚æœ€åï¼Œè¿™äº›ä¼ªæ ‡ç­¾æ•°æ®è¢«æ·»åŠ åˆ°æ ‡è®°æ•°æ®ä¸­è¿›è¡Œä¸‹ä¸€è½®è¿­ä»£ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤ï¼Œç›´åˆ°æ²¡æœ‰é«˜ç½®ä¿¡åº¦çš„æ•°æ®æˆ–æ‰€æœ‰æ•°æ®éƒ½è¢«æ ‡è®°ã€‚
- en: The pseudo-labeling-based methods are firstly applied to the field of natural
    image processing and typically contain self-training methods (Lee *et al.*Â [2013](#bib.bib94))
    and co-training methods (Blum *et al.*Â [1998](#bib.bib14), Zhou *et al.*Â [2005](#bib.bib209)).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºä¼ªæ ‡ç­¾çš„æ–¹æ³•æœ€åˆåº”ç”¨äºè‡ªç„¶å›¾åƒå¤„ç†é¢†åŸŸï¼Œé€šå¸¸åŒ…æ‹¬è‡ªè®­ç»ƒæ–¹æ³•ï¼ˆLee *ç­‰äºº* [2013](#bib.bib94)ï¼‰å’ŒååŒè®­ç»ƒæ–¹æ³•ï¼ˆBlum *ç­‰äºº*
    [1998](#bib.bib14)ï¼ŒZhou *ç­‰äºº* [2005](#bib.bib209)ï¼‰ã€‚
- en: Study in Pathological Image Analysis
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç—…ç†å›¾åƒåˆ†æç ”ç©¶
- en: In pathological image analysis, Singh *et al.*Â [2011](#bib.bib153) proposed
    a semi-supervised method of learning distance metrics from labeled data and performing
    label propagation for identifying the subtypes of nuclei, which was locally adaptive
    and could fully consider the heterogeneity of the data. Bulten *et al.*Â [2020](#bib.bib16)
    developed a deep learning system for Gleason scoring of prostate biopsies based
    on semi-supervised learning. They first trained the network on a small training
    dataset with pure Gleason scores, and then applied the trained network to other
    internal training datasets to set reference standards. Then, the labels were corrected
    and relabeled using reports from pathologists. Tolkach *et al.*Â [2020](#bib.bib171)
    used a pseudo-labeling-based semi-supervised strategy to train the CNN network
    to accomplish Gleason pattern classification. Jasiwal *et al.*Â [2019](#bib.bib79)
    proposed a semi-supervised method based on pseudo-labeling and entropy regularization
    for breast cancer pathological image classification. Shaw *et al.*Â [2020](#bib.bib148)
    extended the study of Yalniz *et al.*Â [2019](#bib.bib196) by proposing a semi-supervised
    teacher-student distillation method for the classification of colorectal cancer
    pathological images. Marini *et al.*Â [2021](#bib.bib109) proposed a deep pseudo-labeling-based
    semi-supervised learning method for strongly heterogeneous pathology data containing
    only a small number of local annotations. Their method consists of a high-volume
    teacher model and a small-volume student model, where the teacher model is automatically
    labeled with pseudo labels for the training of the student model. Cheng *et al.*Â [2020](#bib.bib31)
    proposed a semi-supervised learning framework based on a teacher-student model
    with similarity learning for the segmentation of breast cancer lesions containing
    a small number of annotations and noisy annotations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç—…ç†å›¾åƒåˆ†æä¸­ï¼ŒSingh *ç­‰äºº* [2011](#bib.bib153) æå‡ºäº†ä¸€ç§åŠç›‘ç£å­¦ä¹ è·ç¦»åº¦é‡çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»æ ‡è®°æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶æ‰§è¡Œæ ‡ç­¾ä¼ æ’­ä»¥è¯†åˆ«ç»†èƒæ ¸çš„äºšå‹ï¼Œè¯¥æ–¹æ³•åœ¨æœ¬åœ°è‡ªé€‚åº”ï¼Œå¹¶èƒ½å¤Ÿå……åˆ†è€ƒè™‘æ•°æ®çš„å¼‚è´¨æ€§ã€‚Bulten
    *ç­‰äºº* [2020](#bib.bib16) å¼€å‘äº†ä¸€ç§åŸºäºåŠç›‘ç£å­¦ä¹ çš„å‰åˆ—è…ºæ´»æ£€Gleasonè¯„åˆ†çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿã€‚ä»–ä»¬é¦–å…ˆåœ¨ä¸€ä¸ªåŒ…å«çº¯Gleasonè¯„åˆ†çš„å°è®­ç»ƒæ•°æ®é›†ä¸Šè®­ç»ƒç½‘ç»œï¼Œç„¶åå°†è®­ç»ƒå¥½çš„ç½‘ç»œåº”ç”¨äºå…¶ä»–å†…éƒ¨è®­ç»ƒæ•°æ®é›†ä»¥è®¾å®šå‚è€ƒæ ‡å‡†ã€‚ä¹‹åï¼Œä½¿ç”¨ç—…ç†å­¦å®¶çš„æŠ¥å‘Šæ¥ä¿®æ­£å’Œé‡æ–°æ ‡è®°æ ‡ç­¾ã€‚Tolkach
    *ç­‰äºº* [2020](#bib.bib171) é‡‡ç”¨åŸºäºä¼ªæ ‡ç­¾çš„åŠç›‘ç£ç­–ç•¥æ¥è®­ç»ƒCNNç½‘ç»œï¼Œä»¥å®ŒæˆGleasonæ¨¡å¼åˆ†ç±»ã€‚Jasiwal *ç­‰äºº* [2019](#bib.bib79)
    æå‡ºäº†ä¸€ç§åŸºäºä¼ªæ ‡ç­¾å’Œç†µæ­£åˆ™åŒ–çš„åŠç›‘ç£æ–¹æ³•ç”¨äºä¹³è…ºç™Œç—…ç†å›¾åƒåˆ†ç±»ã€‚Shaw *ç­‰äºº* [2020](#bib.bib148) é€šè¿‡æå‡ºä¸€ç§åŠç›‘ç£æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦æ–¹æ³•æ¥æ‰©å±•Yalniz
    *ç­‰äºº* [2019](#bib.bib196) çš„ç ”ç©¶ï¼Œç”¨äºç»“ç›´è‚ ç™Œç—…ç†å›¾åƒçš„åˆ†ç±»ã€‚Marini *ç­‰äºº* [2021](#bib.bib109) æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ä¼ªæ ‡ç­¾çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåŒ…å«ä»…å°‘é‡æœ¬åœ°æ³¨é‡Šçš„å¼ºå¼‚è´¨æ€§ç—…ç†æ•°æ®ã€‚ä»–ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªå¤§å®¹é‡æ•™å¸ˆæ¨¡å‹å’Œä¸€ä¸ªå°å®¹é‡å­¦ç”Ÿæ¨¡å‹ï¼Œå…¶ä¸­æ•™å¸ˆæ¨¡å‹ä½¿ç”¨ä¼ªæ ‡ç­¾è‡ªåŠ¨æ ‡è®°ï¼Œä»¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚Cheng
    *ç­‰äºº* [2020](#bib.bib31) æå‡ºäº†ä¸€ä¸ªåŸºäºæ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆç›¸ä¼¼æ€§å­¦ä¹ ï¼Œç”¨äºåˆ†å‰²åŒ…å«å°‘é‡æ³¨é‡Šå’Œå™ªå£°æ³¨é‡Šçš„ä¹³è…ºç™Œç—…å˜ã€‚
- en: 3.2.2 Consistency-based Approach
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 åŸºäºä¸€è‡´æ€§çš„æ–¹æ³•
- en: Fundamental Principles
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åŸºæœ¬åŸç†
- en: The consistency-based semi-supervised learning approach is mainly based on the
    smoothing assumption. In the smoothing assumption, the prediction model should
    be robust to local perturbations within its input. This means that when we perturb
    the data points with a small amount of noise, the networkâ€™s predictions for the
    perturbed data points and the clean original data points should be similar. In
    the implementation of deep neural networks, the consistency-based approach can
    be easily extended to a semi-supervised learning setup by directly adding unsupervised
    consistency loss functions to the original supervised loss functions. In the field
    of natural image processing, typical methods include <math alttext="\pi" display="inline"><semantics
    ><mi >Ï€</mi><annotation-xml encoding="MathML-Content" ><ci >ğœ‹</ci></annotation-xml><annotation
    encoding="application/x-tex" >\pi</annotation></semantics></math>-model (Laine
    *et al.*Â [2016](#bib.bib92)), Temporal Ensembling model (Laine *et al.*Â [2016](#bib.bib92)),
    Mean Teachers (Tarvainen *et al.*Â [2017](#bib.bib167)) and UDA (Xie *et al.*Â [2020](#bib.bib190)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€è‡´æ€§åŸºåŠç›‘ç£å­¦ä¹ æ–¹æ³•ä¸»è¦åŸºäºå¹³æ»‘å‡è®¾ã€‚åœ¨å¹³æ»‘å‡è®¾ä¸­ï¼Œé¢„æµ‹æ¨¡å‹åº”è¯¥å¯¹è¾“å…¥çš„å±€éƒ¨æ‰°åŠ¨å…·æœ‰é²æ£’æ€§ã€‚è¿™æ„å‘³ç€å½“æˆ‘ä»¬å¯¹æ•°æ®ç‚¹æ–½åŠ å°‘é‡å™ªå£°æ—¶ï¼Œç½‘ç»œå¯¹æ‰°åŠ¨æ•°æ®ç‚¹å’Œå¹²å‡€åŸå§‹æ•°æ®ç‚¹çš„é¢„æµ‹åº”è¯¥æ˜¯ç›¸ä¼¼çš„ã€‚åœ¨æ·±åº¦ç¥ç»ç½‘ç»œçš„å®ç°ä¸­ï¼Œä¸€è‡´æ€§åŸºæ–¹æ³•å¯ä»¥é€šè¿‡ç›´æ¥å°†æ— ç›‘ç£ä¸€è‡´æ€§æŸå¤±å‡½æ•°æ·»åŠ åˆ°åŸæœ‰çš„ç›‘ç£æŸå¤±å‡½æ•°ä¸­ï¼Œè½»æ¾æ‰©å±•åˆ°åŠç›‘ç£å­¦ä¹ è®¾ç½®ã€‚åœ¨è‡ªç„¶å›¾åƒå¤„ç†é¢†åŸŸï¼Œå…¸å‹çš„æ–¹æ³•åŒ…æ‹¬
    <math alttext="\pi" display="inline"><semantics ><mi >Ï€</mi><annotation-xml encoding="MathML-Content"
    ><ci >ğœ‹</ci></annotation-xml><annotation encoding="application/x-tex" >\pi</annotation></semantics></math>-model
    (Laine *et al.* [2016](#bib.bib92))ï¼ŒTemporal Ensembling model (Laine *et al.*
    [2016](#bib.bib92))ï¼ŒMean Teachers (Tarvainen *et al.* [2017](#bib.bib167)) å’Œ UDA
    (Xie *et al.* [2020](#bib.bib190))ã€‚
- en: Study in Pathological Image Analysis
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç—…ç†å›¾åƒåˆ†æç ”ç©¶
- en: In pathological image analysis, Zhou *et al.*Â [2020](#bib.bib208) proposed a
    new Mean-teacher (MT) framework based on template-guided perturbation-sensitive
    sample mining. This framework consists of a teacher network and a student network.
    The teacher network is an integrated prediction network from K-times randomly
    augmented data, which is used to guide the student network to remain invariant
    to small perturbations at both feature and semantic levels. Su *et al.*Â [2019](#bib.bib162)
    proposed a novel global and local consistency loss and performed the nuclei classification
    task based on the Mean-Teacher framework.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç—…ç†å›¾åƒåˆ†æä¸­ï¼ŒZhou *et al.* [2020](#bib.bib208) æå‡ºäº†ä¸€ä¸ªåŸºäºæ¨¡æ¿å¼•å¯¼çš„æ‰°åŠ¨æ•æ„Ÿæ ·æœ¬æŒ–æ˜çš„æ–°å‹ Mean-teacher
    (MT) æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¸€ä¸ªæ•™å¸ˆç½‘ç»œå’Œä¸€ä¸ªå­¦ç”Ÿç½‘ç»œç»„æˆã€‚æ•™å¸ˆç½‘ç»œæ˜¯ä¸€ä¸ªä»Kæ¬¡éšæœºå¢å¼ºæ•°æ®ä¸­é›†æˆçš„é¢„æµ‹ç½‘ç»œï¼Œç”¨äºæŒ‡å¯¼å­¦ç”Ÿç½‘ç»œåœ¨ç‰¹å¾å’Œè¯­ä¹‰å±‚é¢ä¸Šå¯¹å°æ‰°åŠ¨ä¿æŒä¸å˜ã€‚Su
    *et al.* [2019](#bib.bib162) æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å…¨å±€å’Œå±€éƒ¨ä¸€è‡´æ€§æŸå¤±ï¼Œå¹¶åŸºäº Mean-Teacher æ¡†æ¶æ‰§è¡Œäº†æ ¸åˆ†ç±»ä»»åŠ¡ã€‚
- en: 3.2.3 Graph-based Approach
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 åŸºäºå›¾çš„æ–¹æ³•
- en: Fundamental Principles
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åŸºæœ¬åŸç†
- en: Methods of graph-based semi-supervised learning typically construct graphs to
    preserve the relationships of neighboring nodes, and use the graph transformations
    to simultaneously exploit information from labeled data and explore the underlying
    structure of unlabeled data. The key step of the graph-based semi-supervised learning
    methods is to construct a better graph to represent the original data structure.
    They usually define a graph on all data points (both labeled and unlabeled data
    points) and use weights to encode the similarity between pairs of the data points.
    In this way, the labeled information can be propagated through the graph to the
    unlabeled data points. For labeled data points, the predicted labels should match
    the true labels; similar data points defined by a similarity graph should have
    the same predictions. Graph-based semi-supervised methods are a relatively complex
    and long-developed field, and we recommend (Van *et al.*Â [2020](#bib.bib175),
    Chong *et al.*Â [2020](#bib.bib34)) for a more thorough understanding.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åŸºåŠç›‘ç£å­¦ä¹ æ–¹æ³•é€šå¸¸æ„å»ºå›¾ä»¥ä¿æŒé‚»è¿‘èŠ‚ç‚¹çš„å…³ç³»ï¼Œå¹¶åˆ©ç”¨å›¾çš„å˜æ¢åŒæ—¶åˆ©ç”¨æ ‡è®°æ•°æ®çš„ä¿¡æ¯ä»¥åŠæ¢ç´¢æœªæ ‡è®°æ•°æ®çš„æ½œåœ¨ç»“æ„ã€‚å›¾åŸºåŠç›‘ç£å­¦ä¹ æ–¹æ³•çš„å…³é”®æ­¥éª¤æ˜¯æ„å»ºä¸€ä¸ªæ›´å¥½çš„å›¾æ¥è¡¨ç¤ºåŸå§‹æ•°æ®ç»“æ„ã€‚å®ƒä»¬é€šå¸¸åœ¨æ‰€æœ‰æ•°æ®ç‚¹ï¼ˆåŒ…æ‹¬æ ‡è®°å’Œæœªæ ‡è®°çš„æ•°æ®ç‚¹ï¼‰ä¸Šå®šä¹‰å›¾ï¼Œå¹¶ä½¿ç”¨æƒé‡æ¥ç¼–ç æ•°æ®ç‚¹å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ ‡è®°çš„ä¿¡æ¯å¯ä»¥é€šè¿‡å›¾ä¼ æ’­åˆ°æœªæ ‡è®°çš„æ•°æ®ç‚¹ã€‚å¯¹äºæ ‡è®°çš„æ•°æ®ç‚¹ï¼Œé¢„æµ‹çš„æ ‡ç­¾åº”è¯¥ä¸çœŸå®æ ‡ç­¾åŒ¹é…ï¼›ç”±ç›¸ä¼¼æ€§å›¾å®šä¹‰çš„ç›¸ä¼¼æ•°æ®ç‚¹åº”è¯¥æœ‰ç›¸åŒçš„é¢„æµ‹ã€‚å›¾åŸºåŠç›‘ç£æ–¹æ³•æ˜¯ä¸€ä¸ªç›¸å¯¹å¤æ‚ä¸”å‘å±•è¾ƒé•¿çš„é¢†åŸŸï¼Œæˆ‘ä»¬æ¨èï¼ˆVan
    *et al.* [2020](#bib.bib175)ï¼ŒChong *et al.* [2020](#bib.bib34)ï¼‰ä»¥è·å¾—æ›´å…¨é¢çš„ç†è§£ã€‚
- en: Study in Pathological Image Analysis
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç—…ç†å›¾åƒåˆ†æç ”ç©¶
- en: In pathological image analysis, Xu *et al.*Â [2016](#bib.bib194) proposed a new
    framework that combines a CNN with a semi-supervised regularization term. They
    first generated a hypothetical label for each unlabeled sample, then proposed
    a graph-based smoothing term for regularization. Su *et al.*Â [2015](#bib.bib163)
    proposed an active learning and graph-based semi-supervised learning method for
    interactive cell segmentation. Inspired by the Temporal Ensembling model (Laine
    *et al.*Â [2016](#bib.bib92)), Shi *et al.*Â [2020](#bib.bib150) proposed a graph-based
    temporal ensembling model GTE. This method creates ensemble targets for both features
    and label predictions for each training sample, and encourages the model to form
    consistent predictions under different perturbations to exploit the semantic information
    of unlabeled data and improve the robustness of the model to noisy labels.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç—…ç†å›¾åƒåˆ†æä¸­ï¼Œå¾*ç­‰äºº* [2016](#bib.bib194) æå‡ºäº†ä¸€ä¸ªç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒåŠç›‘ç£æ­£åˆ™åŒ–é¡¹çš„æ–°æ¡†æ¶ã€‚ä»–ä»¬é¦–å…ˆä¸ºæ¯ä¸ªæœªæ ‡è®°æ ·æœ¬ç”Ÿæˆäº†ä¸€ä¸ªå‡è®¾æ ‡ç­¾ï¼Œç„¶åæå‡ºäº†ä¸€ç§åŸºäºå›¾çš„å¹³æ»‘é¡¹è¿›è¡Œæ­£åˆ™åŒ–ã€‚è‹*ç­‰äºº*
    [2015](#bib.bib163) æå‡ºäº†ä¸€ç§ç”¨äºäº¤äº’å¼ç»†èƒåˆ†å‰²çš„ä¸»åŠ¨å­¦ä¹ å’ŒåŸºäºå›¾çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚å—åˆ°æ—¶é—´é›†æˆæ¨¡å‹ï¼ˆLaine*ç­‰äºº* [2016](#bib.bib92)ï¼‰çš„å¯å‘ï¼ŒçŸ³*ç­‰äºº*
    [2020](#bib.bib150) æå‡ºäº†åŸºäºå›¾çš„æ—¶é—´é›†æˆæ¨¡å‹GTEã€‚è¯¥æ–¹æ³•ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬åˆ›å»ºç‰¹å¾å’Œæ ‡ç­¾é¢„æµ‹çš„é›†æˆç›®æ ‡ï¼Œå¹¶é¼“åŠ±æ¨¡å‹åœ¨ä¸åŒæ‰°åŠ¨ä¸‹å½¢æˆä¸€è‡´çš„é¢„æµ‹ï¼Œä»è€Œåˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹å¯¹å™ªå£°æ ‡ç­¾çš„é²æ£’æ€§ã€‚
- en: 3.2.4 Unsupervised-preprocessing-based Approach
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 æ— ç›‘ç£é¢„å¤„ç†æ–¹æ³•
- en: Fundamental Principles
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åŸºæœ¬åŸç†
- en: Unlike the previous approaches, unsupervised preprocessing-based approaches
    are typically dedicated to the unsupervised feature extraction, clustering (cluster-then-label),
    or initialization of the parameters of the subsequent supervised learning process
    (pre-training) from a large amount of unlabeled data. The most popular methods
    include autoencoders and their variants (Vincent *et al.*Â [2008](#bib.bib179),
    [2011](#bib.bib138)). Clustering is another method that enables adequate learning
    of the overall data distribution, thus many semi-supervised learning algorithms
    (Goldberg *et al.*Â [2009](#bib.bib62), Demiriz *et al.*Â [1999](#bib.bib46), Dara
    *et al.*Â [2002](#bib.bib43)) guide the subsequent classification process through
    clustering. The idea of the pre-training is to first pre-train a model using unsupervised
    methods with unlabeled data, and then use the parameters of this model as the
    initial parameters of the subsequent supervised training model, i.e., the subsequent
    supervised training is fine-tuned on the basis of these initial parameters. On
    this basis, the large number of unlabeled data can fully guide the subsequent
    classification models with limited labeled data thus improving the performance
    of semi-supervised learning (Erhan *et al.*Â [2010](#bib.bib55)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼Œæ— ç›‘ç£é¢„å¤„ç†æ–¹æ³•é€šå¸¸ä¸“æ³¨äºä»å¤§é‡æœªæ ‡è®°æ•°æ®ä¸­è¿›è¡Œæ— ç›‘ç£ç‰¹å¾æå–ã€èšç±»ï¼ˆå…ˆèšç±»åæ ‡è®°ï¼‰æˆ–åç»­ç›‘ç£å­¦ä¹ è¿‡ç¨‹çš„å‚æ•°åˆå§‹åŒ–ï¼ˆé¢„è®­ç»ƒï¼‰ã€‚æœ€æµè¡Œçš„æ–¹æ³•åŒ…æ‹¬è‡ªç¼–ç å™¨åŠå…¶å˜ä½“ï¼ˆVincent*ç­‰äºº*
    [2008](#bib.bib179), [2011](#bib.bib138)ï¼‰ã€‚èšç±»æ˜¯å¦ä¸€ç§æ–¹æ³•ï¼Œèƒ½å¤Ÿå……åˆ†å­¦ä¹ æ•´ä½“æ•°æ®åˆ†å¸ƒï¼Œå› æ­¤è®¸å¤šåŠç›‘ç£å­¦ä¹ ç®—æ³•ï¼ˆGoldberg*ç­‰äºº*
    [2009](#bib.bib62), Demiriz*ç­‰äºº* [1999](#bib.bib46), Dara*ç­‰äºº* [2002](#bib.bib43)ï¼‰é€šè¿‡èšç±»å¼•å¯¼åç»­çš„åˆ†ç±»è¿‡ç¨‹ã€‚é¢„è®­ç»ƒçš„æ€æƒ³æ˜¯é¦–å…ˆä½¿ç”¨æœªæ ‡è®°æ•°æ®é€šè¿‡æ— ç›‘ç£æ–¹æ³•é¢„è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åå°†è¯¥æ¨¡å‹çš„å‚æ•°ä½œä¸ºåç»­ç›‘ç£è®­ç»ƒæ¨¡å‹çš„åˆå§‹å‚æ•°ï¼Œå³åç»­çš„ç›‘ç£è®­ç»ƒæ˜¯åœ¨è¿™äº›åˆå§‹å‚æ•°çš„åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¤§é‡æœªæ ‡è®°æ•°æ®å¯ä»¥å……åˆ†å¼•å¯¼å…·æœ‰æœ‰é™æ ‡è®°æ•°æ®çš„åç»­åˆ†ç±»æ¨¡å‹ï¼Œä»è€Œæé«˜åŠç›‘ç£å­¦ä¹ çš„æ€§èƒ½ï¼ˆErhan*ç­‰äºº*
    [2010](#bib.bib55)ï¼‰ã€‚
- en: Study in Pathological Image Analysis
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç—…ç†å›¾åƒåˆ†æä¸­çš„ç ”ç©¶
- en: In pathological image analysis, Peikari *et al.*Â [2018](#bib.bib125) proposed
    a cluster-then-label semi-supervised learning method for identifying high-density
    regions in the data space and then utilized these regions to help support vector
    machines find decision boundaries. Lu *et al.*Â [2019](#bib.bib104) proposed a
    semi-supervised method based on feature extraction and pre-training for the WSI-level
    breast cancer classification task, which is the first work that relies on self-supervised
    feature learning using contrastive predictive coding for weakly supervised histopathological
    image classification. Koohbanani *et al.*Â [2021](#bib.bib89) proposed a joint
    framework of self-supervised learning and semi-supervised learning for pathological
    images. They proposed three pathology-specific self-supervised tasks, magnification
    prediction, magnification jigsaw prediction and hematoxylin channel prediction,
    to learn high-level semantic information and domain invariant information in pathological
    images. Srinidhi *et al.*Â [2022](#bib.bib160) also proposed a framework that combines
    self-supervised learning with semi-supervised learning. They first proposed the
    resolution sequence prediction (RSP) self-supervised auxiliary task to pre-train
    the model through unlabeled data, and then they performed fine-tuning of the model
    on the labeled data. After that they used the trained model from the above two
    steps as the initial weights of the model for further semi-supervised training
    based on the teacher-student consistency framework.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç—…ç†å›¾åƒåˆ†æä¸­ï¼ŒPeikari *ç­‰äºº* [2018](#bib.bib125) æå‡ºäº†ä¸€ä¸ªå…ˆèšç±»åæ ‡è®°çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«æ•°æ®ç©ºé—´ä¸­çš„é«˜å¯†åº¦åŒºåŸŸï¼Œå¹¶åˆ©ç”¨è¿™äº›åŒºåŸŸå¸®åŠ©æ”¯æŒå‘é‡æœºæ‰¾åˆ°å†³ç­–è¾¹ç•Œã€‚Lu
    *ç­‰äºº* [2019](#bib.bib104) æå‡ºäº†ä¸€ä¸ªåŸºäºç‰¹å¾æå–å’Œé¢„è®­ç»ƒçš„åŠç›‘ç£æ–¹æ³•ï¼Œç”¨äºWSIçº§åˆ«çš„ä¹³è…ºç™Œåˆ†ç±»ä»»åŠ¡ï¼Œè¿™æ˜¯é¦–ä¸ªä¾èµ–å¯¹æ¯”é¢„æµ‹ç¼–ç è¿›è¡Œè‡ªç›‘ç£ç‰¹å¾å­¦ä¹ çš„å¼±ç›‘ç£ç—…ç†å›¾åƒåˆ†ç±»å·¥ä½œã€‚Koohbanani
    *ç­‰äºº* [2021](#bib.bib89) æå‡ºäº†ä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ ä¸åŠç›‘ç£å­¦ä¹ ç›¸ç»“åˆçš„ç—…ç†å›¾åƒè”åˆæ¡†æ¶ã€‚ä»–ä»¬æå‡ºäº†ä¸‰ç§ç‰¹å®šäºç—…ç†å­¦çš„è‡ªç›‘ç£ä»»åŠ¡ï¼šæ”¾å¤§é¢„æµ‹ã€æ”¾å¤§æ‹¼å›¾é¢„æµ‹å’Œè‹æœ¨ç²¾é€šé“é¢„æµ‹ï¼Œä»¥å­¦ä¹ ç—…ç†å›¾åƒä¸­çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯å’Œé¢†åŸŸä¸å˜ä¿¡æ¯ã€‚Srinidhi
    *ç­‰äºº* [2022](#bib.bib160) ä¹Ÿæå‡ºäº†ä¸€ä¸ªç»“åˆè‡ªç›‘ç£å­¦ä¹ ä¸åŠç›‘ç£å­¦ä¹ çš„æ¡†æ¶ã€‚ä»–ä»¬é¦–å…ˆæå‡ºäº†è§£æåºåˆ—é¢„æµ‹ï¼ˆRSPï¼‰è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡ï¼Œé€šè¿‡æœªæ ‡è®°æ•°æ®é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨æ ‡è®°æ•°æ®ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä¹‹åï¼Œä»–ä»¬ä½¿ç”¨ä¸Šè¿°ä¸¤æ­¥å¾—åˆ°çš„è®­ç»ƒæ¨¡å‹ä½œä¸ºæ¨¡å‹çš„åˆå§‹æƒé‡ï¼Œè¿›è¡ŒåŸºäºå¸ˆç”Ÿä¸€è‡´æ€§æ¡†æ¶çš„è¿›ä¸€æ­¥åŠç›‘ç£è®­ç»ƒã€‚
- en: 3.2.5 Other Approaches
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5 å…¶ä»–æ–¹æ³•
- en: Among semi-supervised learning, there are many other approaches, such as the
    methods based on generative adversarial networks (GAN) (Goodfellow *et al.*Â [2014](#bib.bib65),
    [2016](#bib.bib63), Salimans *et al.*Â [2016](#bib.bib144), Odena *et al.*Â [2016](#bib.bib121),
    Dai *et al.*Â [2017](#bib.bib42)), Manifold-based methods (Belkin *et al.*Â [2005](#bib.bib12),
    [2006](#bib.bib13), Weston *et al.*Â [2012](#bib.bib187), Rifai *et al.*Â [2011](#bib.bib137),
    [2011](#bib.bib138)) and Association learning based methods (Haeusser *et al.*Â [2017](#bib.bib70)).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŠç›‘ç£å­¦ä¹ ä¸­ï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–æ–¹æ³•ï¼Œå¦‚åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–¹æ³•ï¼ˆGoodfellow *ç­‰äºº* [2014](#bib.bib65), [2016](#bib.bib63),
    Salimans *ç­‰äºº* [2016](#bib.bib144), Odena *ç­‰äºº* [2016](#bib.bib121), Dai *ç­‰äºº* [2017](#bib.bib42)ï¼‰ã€æµå½¢æ–¹æ³•ï¼ˆBelkin
    *ç­‰äºº* [2005](#bib.bib12), [2006](#bib.bib13), Weston *ç­‰äºº* [2012](#bib.bib187),
    Rifai *ç­‰äºº* [2011](#bib.bib137), [2011](#bib.bib138)ï¼‰å’ŒåŸºäºå…³è”å­¦ä¹ çš„æ–¹æ³•ï¼ˆHaeusser *ç­‰äºº* [2017](#bib.bib70)ï¼‰ã€‚
- en: In pathological image analysis, Kapil *et al.*Â [2018](#bib.bib82) first used
    auxiliary classifier generative adversarial networks (AC-GAN) for the pathological
    image semi-supervised classification task and achieved favorable results. Cong
    *et al.*Â [2021](#bib.bib38) proposed to use a GAN-based semi-supervised learning
    method to accomplish the stain normalization problem for pathological images.
    Sparks *et al.*Â [2016](#bib.bib158) proposed a semi-supervised method based on
    epidemic learning to accomplish a content-based histopathological image retrieval
    task. Li *et al.*Â [2018](#bib.bib100) developed an Expectation-Maximization (EM)-based
    semi-supervised method for the semantic segmentation task of radical prostatectomy
    histopathological images. Su *et al.*Â [2021](#bib.bib164) proposed a new semi-supervised
    method based on association learning for pathological image classification task
    inspired by Haeusser *et al.*Â [2017](#bib.bib70). Some studies (Foucart *et al.*Â [2019](#bib.bib57))
    have also attempted to analyze the weaknesses and effectiveness of semi-supervised,
    noisy learning and weak label learning based on deep learning for pathological
    image analysis.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç—…ç†å›¾åƒåˆ†æä¸­ï¼ŒKapil *ç­‰* [2018](#bib.bib82) é¦–æ¬¡ä½¿ç”¨äº†è¾…åŠ©åˆ†ç±»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (AC-GAN) æ¥å¤„ç†ç—…ç†å›¾åƒåŠç›‘ç£åˆ†ç±»ä»»åŠ¡ï¼Œå¹¶å–å¾—äº†è‰¯å¥½ç»“æœã€‚Cong
    *ç­‰* [2021](#bib.bib38) æå‡ºäº†ä½¿ç”¨åŸºäº GAN çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•æ¥è§£å†³ç—…ç†å›¾åƒçš„æŸ“è‰²æ ‡å‡†åŒ–é—®é¢˜ã€‚Sparks *ç­‰* [2016](#bib.bib158)
    æå‡ºäº†åŸºäºæµè¡Œç—…å­¦ä¹ çš„åŠç›‘ç£æ–¹æ³•æ¥å®ŒæˆåŸºäºå†…å®¹çš„ç»„ç»‡ç—…ç†å›¾åƒæ£€ç´¢ä»»åŠ¡ã€‚Li *ç­‰* [2018](#bib.bib100) å¼€å‘äº†ä¸€ç§åŸºäºæœŸæœ›æœ€å¤§åŒ– (EM)
    çš„åŠç›‘ç£æ–¹æ³•ï¼Œç”¨äºå‰åˆ—è…ºåˆ‡é™¤æœ¯ç—…ç†å›¾åƒçš„è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚Su *ç­‰* [2021](#bib.bib164) æå‡ºäº†åŸºäºå…³è”å­¦ä¹ çš„æ–°å‹åŠç›‘ç£æ–¹æ³•ï¼Œç”¨äºç—…ç†å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œçµæ„Ÿæ¥æºäº
    Haeusser *ç­‰* [2017](#bib.bib70)ã€‚ä¸€äº›ç ”ç©¶ (Foucart *ç­‰* [2019](#bib.bib57)) ä¹Ÿå°è¯•åˆ†æäº†åŸºäºæ·±åº¦å­¦ä¹ çš„åŠç›‘ç£ã€å™ªå£°å­¦ä¹ å’Œå¼±æ ‡ç­¾å­¦ä¹ åœ¨ç—…ç†å›¾åƒåˆ†æä¸­çš„å¼±ç‚¹å’Œæœ‰æ•ˆæ€§ã€‚
- en: 'Table 4: List of literatures in the semi-supervised learning section.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 4: åŠç›‘ç£å­¦ä¹ éƒ¨åˆ†æ–‡çŒ®åˆ—è¡¨ã€‚'
- en: '| Reference | Approach | Disease Type | Staining | Task | Dataset | Dataset
    Scale | Dataset Link | Performance |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| å‚è€ƒæ–‡çŒ® | æ–¹æ³• | ç–¾ç—…ç±»å‹ | æŸ“è‰² | ä»»åŠ¡ | æ•°æ®é›† | æ•°æ®é›†è§„æ¨¡ | æ•°æ®é›†é“¾æ¥ | æ€§èƒ½ |'
- en: '| Singh etÂ al. ([2011](#bib.bib153)) | Pseudo- labelling-based | Breast Cancer
    | 3D fluorescence microscopy | Identifying nuclear phenotypes | Nuclei image dataset
    | 984 images | Inhouse | Mean Accuracy: 0.8 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Singh ç­‰ ([2011](#bib.bib153)) | åŸºäºä¼ªæ ‡ç­¾ | ä¹³è…ºç™Œ | 3D è§å…‰æ˜¾å¾®é•œ | è¯†åˆ«æ ¸è¡¨å‹ | æ ¸å›¾åƒæ•°æ®é›† |
    984 å¼ å›¾åƒ | å†…éƒ¨ | å¹³å‡å‡†ç¡®ç‡: 0.8 |'
- en: '| Bulten etÂ al. ([2020](#bib.bib16)) | Pseudo- labelling-based | Prostate Cancer
    | H&E | Gleason grading | Inhouse dataset | 5759 biopsies from 1243 patients |
    Inhouse | AUC = 0.99 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Bulten ç­‰ ([2020](#bib.bib16)) | åŸºäºä¼ªæ ‡ç­¾ | å‰åˆ—è…ºç™Œ | H&E | æ ¼é‡Œæ£®è¯„åˆ† | å†…éƒ¨æ•°æ®é›† | 1243
    åæ‚£è€…çš„ 5759 ä¸ªæ´»æ£€æ ·æœ¬ | å†…éƒ¨ | AUC = 0.99 |'
- en: '| Tolkach etÂ al. ([2020](#bib.bib171)) | Pseudo- labelling-based | Prostate
    Cancer | H&E | Detection of prostate cancer tissue | The Cancer Genome Atlas Program
    (TCGA) dataset | 1.67 million patches | http://portal.gdc.cancer.gov | Accuracy
    = 0.967 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Tolkach ç­‰ ([2020](#bib.bib171)) | åŸºäºä¼ªæ ‡ç­¾ | å‰åˆ—è…ºç™Œ | H&E | å‰åˆ—è…ºç™Œç»„ç»‡çš„æ£€æµ‹ | ç™Œç—‡åŸºå› ç»„å›¾è°±è®¡åˆ’
    (TCGA) æ•°æ®é›† | 167 ä¸‡ä¸ªè¡¥ä¸ | http://portal.gdc.cancer.gov | å‡†ç¡®ç‡ = 0.967 |'
- en: '| Gleason grading of prostatic adenocarcinomas | https://zenodo.org/deposit/3825933
    | Accuracy = 0.98 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| å‰åˆ—è…ºè…ºç™Œçš„æ ¼é‡Œæ£®è¯„åˆ† | https://zenodo.org/deposit/3825933 | å‡†ç¡®ç‡ = 0.98 |'
- en: '| Jaiswal etÂ al. ([2019](#bib.bib79)) | Pseudo- labelling-based | Breast Cancer
    | H&E | Detection of lymph node metastases | PatchCamelyon dataset | 327680 patches
    | https://camelyon16.grand-challenge.org/Data/ | AUC = 0.9816 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Jaiswal ç­‰ ([2019](#bib.bib79)) | åŸºäºä¼ªæ ‡ç­¾ | ä¹³è…ºç™Œ | H&E | æ·‹å·´ç»“è½¬ç§»çš„æ£€æµ‹ | PatchCamelyon
    æ•°æ®é›† | 327680 ä¸ªè¡¥ä¸ | https://camelyon16.grand-challenge.org/Data/ | AUC = 0.9816
    |'
- en: '| Shaw etÂ al. ([2020](#bib.bib148)) | Pseudo- labelling-based | Colorectal
    Cancer | H&E | Classification of 9 categories of pathology patterns | Public dataset
    | 100000 patches | https://zenodo.org/record/1214456#.YvyiX3ZByw4 | Mean Accuracy
    = 0.943 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Shaw ç­‰ ([2020](#bib.bib148)) | åŸºäºä¼ªæ ‡ç­¾ | ç»“ç›´è‚ ç™Œ | H&E | 9 ç±»ç—…ç†æ¨¡å¼çš„åˆ†ç±» | å…¬å¼€æ•°æ®é›† |
    100000 ä¸ªè¡¥ä¸ | https://zenodo.org/record/1214456#.YvyiX3ZByw4 | å¹³å‡å‡†ç¡®ç‡ = 0.943 |'
- en: '| Marini etÂ al. ([2021](#bib.bib109)) | Pseudo- labelling-based | Prostate
    Cancer | H&E | Gleason grading | Tissue MicroArray dataset Zurich dataset | 886
    cases | Inhouse | <math   alttext="\kappa" display="inline"><semantics ><mi mathsize="80%"
    >Îº</mi><annotation-xml encoding="MathML-Content" ><ci  >ğœ…</ci></annotation-xml><annotation
    encoding="application/x-tex" >\kappa</annotation></semantics></math>-score: 0.7645
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Marini et al. ([2021](#bib.bib109)) | åŸºäºä¼ªæ ‡ç­¾çš„æ–¹æ³• | å‰åˆ—è…ºç™Œ | H&E | Gleason åˆ†çº§
    | Tissue MicroArray æ•°æ®é›† Zurich æ•°æ®é›† | 886 ä¸ªæ¡ˆä¾‹ | å†…éƒ¨ | <math alttext="\kappa" display="inline"><semantics
    ><mi mathsize="80%" >Îº</mi><annotation-xml encoding="MathML-Content" ><ci  >ğœ…</ci></annotation-xml><annotation
    encoding="application/x-tex" >\kappa</annotation></semantics></math>åˆ†æ•°ï¼š0.7645
    |'
- en: '| TCGA-PRAD dataset | 449 cases | http://portal.gdc.cancer.gov | <math alttext="\kappa"
    display="inline"><semantics ><mi mathsize="80%"  >Îº</mi><annotation-xml encoding="MathML-Content"
    ><ci >ğœ…</ci></annotation-xml><annotation encoding="application/x-tex" >\kappa</annotation></semantics></math>-score:
    0.4529 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| TCGA-PRAD æ•°æ®é›† | 449 ä¸ªæ¡ˆä¾‹ | http://portal.gdc.cancer.gov | <math alttext="\kappa"
    display="inline"><semantics ><mi mathsize="80%"  >Îº</mi><annotation-xml encoding="MathML-Content"
    ><ci >ğœ…</ci></annotation-xml><annotation encoding="application/x-tex" >\kappa</annotation></semantics></math>åˆ†æ•°ï¼š0.4529
    |'
- en: '| Cheng etÂ al. ([2020](#bib.bib31)) | Pseudo- labelling-based | Breast Cancer
    | H&E | Automated segmentation of cancerous regions | CAMELYON16 dataset | 400
    cases | https://camelyon16.grand-challenge.org/Data/ | Dice: 93.76 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Cheng et al. ([2020](#bib.bib31)) | åŸºäºä¼ªæ ‡ç­¾çš„æ–¹æ³• | ä¹³è…ºç™Œ | H&E | è‡ªåŠ¨åŒ–ç™ŒåŒºåˆ†å‰² | CAMELYON16
    æ•°æ®é›† | 400 ä¸ªæ¡ˆä¾‹ | https://camelyon16.grand-challenge.org/Data/ | Diceï¼š93.76 |'
- en: '| Prostate Cancer | TVGH TURP dataset | 71 cases | Inhouse | Dice: 77.24 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| å‰åˆ—è…ºç™Œ | TVGH TURP æ•°æ®é›† | 71 ä¸ªæ¡ˆä¾‹ | å†…éƒ¨ | Diceï¼š77.24 |'
- en: '| Zhou etÂ al. ([2020](#bib.bib208)) | Consistency-based | - | Liquid-based
    pap test specimen | Cervical cell instance segmentation | liquid-based Pap test
    specimen dataset | 4439 cytoplasm | Inhouse | AJI: 73.45, MAP: 46.01 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Zhou et al. ([2020](#bib.bib208)) | åŸºäºä¸€è‡´æ€§çš„æ–¹æ³• | - | æ¶²ä½“å·´æ°æµ‹è¯•æ ‡æœ¬ | å®«é¢ˆç»†èƒå®ä¾‹åˆ†å‰² |
    æ¶²ä½“å·´æ°æµ‹è¯•æ ‡æœ¬æ•°æ®é›† | 4439 ä¸ªç»†èƒè´¨ | å†…éƒ¨ | AJIï¼š73.45ï¼ŒMAPï¼š46.01 |'
- en: '| Su etÂ al. ([2019](#bib.bib162)) | Consistency-based | - | H&E | Nuclei classification
    | MoNuseg dataset | 22462 nuclei | Sirinukunwattana etÂ al. ([2016](#bib.bib155))
    | F1 score: 75.02 (5% labels) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Su et al. ([2019](#bib.bib162)) | åŸºäºä¸€è‡´æ€§çš„æ–¹æ³• | - | H&E | ç»†èƒæ ¸åˆ†ç±» | MoNuseg æ•°æ®é›†
    | 22462 ä¸ªç»†èƒæ ¸ | Sirinukunwattana et al. ([2016](#bib.bib155)) | F1 åˆ†æ•°ï¼š75.02ï¼ˆ5%
    æ ‡ç­¾ï¼‰ |'
- en: '| Ki-67 nucleus dataset | 17516 nuclei | Inhouse | F1 score: 79.32 (5% labels)
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Ki-67 æ ¸æ•°æ®é›† | 17516 ä¸ªæ ¸ | å†…éƒ¨ | F1 åˆ†æ•°ï¼š79.32ï¼ˆ5% æ ‡ç­¾ï¼‰ |'
- en: '| Srinidhi etÂ al. ([2022](#bib.bib160)) | Consistency-based | Breast Cancer,
    Colorectal Cancer | H&E | Detection of tumor metastasis | BreastPathQ dataset
    | 2579 patches | Martel etÂ al. ([2019](#bib.bib111)) | TC: 0.876 (10% labels)
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Srinidhi et al. ([2022](#bib.bib160)) | åŸºäºä¸€è‡´æ€§çš„æ–¹æ³• | ä¹³è…ºç™Œã€ç»“ç›´è‚ ç™Œ | H&E | è‚¿ç˜¤è½¬ç§»æ£€æµ‹
    | BreastPathQ æ•°æ®é›† | 2579 ä¸ªè¡¥ä¸ | Martel et al. ([2019](#bib.bib111)) | TCï¼š0.876ï¼ˆ10%
    æ ‡ç­¾ï¼‰ |'
- en: '| Classification of tissue types | Camelyon16 dataset | 399 WSIs | https://camelyon16.grand-challenge.org/Data/
    | AUC: 0.855 (10% labels) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| ç»„ç»‡ç±»å‹åˆ†ç±» | Camelyon16 æ•°æ®é›† | 399 ä¸ª WSIs | https://camelyon16.grand-challenge.org/Data/
    | AUCï¼š0.855ï¼ˆ10% æ ‡ç­¾ï¼‰ |'
- en: '| Quantification of tumor cellularity | Kather multiclass dataset | 100K patches
    | Kather *et al.*Â ([2019](#bib.bib84)) | Accuracy: 0.982 (10% labels) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| è‚¿ç˜¤ç»†èƒæ€§å®šé‡ | Kather å¤šç±»æ•°æ®é›† | 100K ä¸ªè¡¥ä¸ | Kather *et al.* ([2019](#bib.bib84))
    | å‡†ç¡®ç‡ï¼š0.982ï¼ˆ10% æ ‡ç­¾ï¼‰ |'
- en: '| Xu etÂ al. ([2016](#bib.bib194)) | Graph-based | - | Microscopy images | Neuron
    segmentation | Neural morphology image dataset | 2000 neuron regions with with
    annotations | Inhouse | F1 score: 0.96 (40% labels) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. ([2016](#bib.bib194)) | åŸºäºå›¾çš„æ–¹æ³• | - | æ˜¾å¾®é•œå›¾åƒ | ç¥ç»å…ƒåˆ†å‰² | ç¥ç»å½¢æ€å›¾åƒæ•°æ®é›†
    | 2000ä¸ªç¥ç»å…ƒåŒºåŸŸå¸¦æ³¨é‡Š | å†…éƒ¨ | F1 åˆ†æ•°ï¼š0.96ï¼ˆ40% æ ‡ç­¾ï¼‰ |'
- en: '| Su etÂ al. ([2015](#bib.bib163)) | Graph-based | - | Microscopy images | Cell
    segmentation | Phase contrast microscopy image dataset | Multiple sequences of
    total 1404 frames | http://www.celltracking.ri.cmu.edu/downloads.html. | TC: 0.9813
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Su et al. ([2015](#bib.bib163)) | åŸºäºå›¾çš„æ–¹æ³• | - | æ˜¾å¾®é•œå›¾åƒ | ç»†èƒåˆ†å‰² | ç›¸ä½å¯¹æ¯”æ˜¾å¾®é•œå›¾åƒæ•°æ®é›†
    | å¤šä¸ªåºåˆ—ï¼Œæ€»è®¡ 1404 å¸§ | http://www.celltracking.ri.cmu.edu/downloads.html | TCï¼š0.9813
    |'
- en: '| Shi, Su, Xing andÂ Yang ([2020](#bib.bib150)) | Graph-based | Lung Cancer
    | H&E | Predictions of subtypes | The Cancer Genome Altas (TCGA) | 2904 patches
    | http://portal.gdc.cancer.gov | Accuracy: 0.905 (20% labels) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Shi, Su, Xing å’Œ Yang ([2020](#bib.bib150)) | åŸºäºå›¾çš„æ–¹æ³• | è‚ºç™Œ | H&E | äºšå‹é¢„æµ‹ | ç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰
    | 2904 ä¸ªè¡¥ä¸ | http://portal.gdc.cancer.gov | å‡†ç¡®ç‡ï¼š0.905ï¼ˆ20% æ ‡ç­¾ï¼‰ |'
- en: '| Breast Cancer | 1763 patches | Accuracy: 0.895 (20% labels) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| ä¹³è…ºç™Œ | 1763 ä¸ªè¡¥ä¸ | å‡†ç¡®ç‡ï¼š0.895ï¼ˆ20% æ ‡ç­¾ï¼‰ |'
- en: '| Peikari etÂ al. ([2018](#bib.bib125)) | Unsupervised- preprocessing-based
    | Breast Cancer | H&E | Identifying different breast tissue regions | Pathology
    triaging image dataset | 4402 patches | Inhouse | AUC: 0.86 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Peikari ç­‰ ([2018](#bib.bib125)) | æ— ç›‘ç£é¢„å¤„ç†åŸºç¡€çš„ | ä¹³è…ºç™Œ | H&E | è¯†åˆ«ä¸åŒä¹³è…ºç»„ç»‡åŒºåŸŸ | ç—…ç†ç­›æŸ¥å›¾åƒæ•°æ®é›†
    | 4402 ä¸ªè¡¥ä¸ | å†…éƒ¨æ•°æ® | AUC: 0.86 |'
- en: '| Nuclei figure classification dataset | 30,000 figures | AUC: 0.95 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| æ ¸å›¾å½¢åˆ†ç±»æ•°æ®é›† | 30,000 ä¸ªå›¾å½¢ | AUC: 0.95 |'
- en: '| Lu etÂ al. ([2019](#bib.bib104)) | Unsupervised- preprocessing-based | Breast
    Cancer | H&E | Benign and malignant classification | BACH dataset | 400 cases
    | BACH: Grand challenge on Breast Cancer histology images | Accuracy: 0.95 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Lu ç­‰ ([2019](#bib.bib104)) | æ— ç›‘ç£é¢„å¤„ç†åŸºç¡€çš„ | ä¹³è…ºç™Œ | H&E | è‰¯æ€§ä¸æ¶æ€§åˆ†ç±» | BACH æ•°æ®é›† |
    400 ä¸ªç—…ä¾‹ | BACH: ä¹³è…ºç™Œç»„ç»‡å›¾åƒå¤§æŒ‘æˆ˜ | å‡†ç¡®ç‡: 0.95 |'
- en: '| Koohbanani etÂ al. ([2021](#bib.bib89)) | Unsupervised- preprocessing-based
    | Breast Cancer | H&E | Detection of tumor regions | Camelyon16 dataset | 399
    slides | https://camelyon16.grand-challenge.org/Data/ | AUC: 0.817 (1% labeled)
    |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| Koohbanani ç­‰ ([2021](#bib.bib89)) | æ— ç›‘ç£é¢„å¤„ç†åŸºç¡€çš„ | ä¹³è…ºç™Œ | H&E | è‚¿ç˜¤åŒºåŸŸæ£€æµ‹ | Camelyon16
    æ•°æ®é›† | 399 å¼ å¹»ç¯ç‰‡ | https://camelyon16.grand-challenge.org/Data/ | AUC: 0.817 (1%
    æ ‡è®°)'
- en: '| Oral Squamous Cell Carcinoma | Prediction of metastases in the cervical lymph
    nodes | LNM-OSCC dataset | 217 slides | Inhouse | AUC: 0.806 (1% labeled) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| å£è…”é³çŠ¶ç»†èƒç™Œ | é¢ˆéƒ¨æ·‹å·´ç»“è½¬ç§»é¢„æµ‹ | LNM-OSCC æ•°æ®é›† | 217 å¼ å¹»ç¯ç‰‡ | å†…éƒ¨æ•°æ® | AUC: 0.806 (1% æ ‡è®°)
    |'
- en: '| Colorectal Cancer | Classification of tissue types | Kather multiclass dataset
    | 100K patches | Kather *et al.*Â ([2019](#bib.bib84)) | AUC: 0.903 (1%labeled)
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| ç»“ç›´è‚ ç™Œ | ç»„ç»‡ç±»å‹åˆ†ç±» | Kather å¤šç±»åˆ«æ•°æ®é›† | 100K ä¸ªè¡¥ä¸ | Kather *ç­‰* ([2019](#bib.bib84))
    | AUC: 0.903 (1% æ ‡è®°) |'
- en: '| Srinidhi etÂ al. ([2022](#bib.bib160)) | Unsupervised- preprocessing-based
    | Breast Cancer, Colorectal Cancer | H&E | Detection of tumor metastasis | BreastPathQ
    dataset | 2579 patches | Martel etÂ al. ([2019](#bib.bib111)) | TC: 0.876 (10%
    labels) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Srinidhi ç­‰ ([2022](#bib.bib160)) | æ— ç›‘ç£é¢„å¤„ç†åŸºç¡€çš„ | ä¹³è…ºç™Œ, ç»“ç›´è‚ ç™Œ | H&E | è‚¿ç˜¤è½¬ç§»æ£€æµ‹ |
    BreastPathQ æ•°æ®é›† | 2579 ä¸ªè¡¥ä¸ | Martel ç­‰ ([2019](#bib.bib111)) | TC: 0.876 (10% æ ‡ç­¾)
    |'
- en: '| Classification of tissue type | Camelyon16 dataset | 399 WSIs | https://camelyon16.grand-challenge.org/Data/
    | AUC: 0.855 (10% labels) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| ç»„ç»‡ç±»å‹åˆ†ç±» | Camelyon16 æ•°æ®é›† | 399 å¼  WSIs | https://camelyon16.grand-challenge.org/Data/
    | AUC: 0.855 (10% æ ‡ç­¾) |'
- en: '| Quantification of tumor cellularity | Kather multiclass dataset | 100K patches
    | Kather *et al.*Â ([2019](#bib.bib84)) | ACC: 0.982 (10% labels) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| è‚¿ç˜¤ç»†èƒæ€§å®šé‡ | Kather å¤šç±»åˆ«æ•°æ®é›† | 100K ä¸ªè¡¥ä¸ | Kather *ç­‰* ([2019](#bib.bib84)) | ACC:
    0.982 (10% æ ‡ç­¾) |'
- en: '| Kapil etÂ al. ([2018](#bib.bib82)) | GAN-based | Lung Cancer | Ventana PD-L1
    (SP263) assay | Automated tumor proportion scoring | NSCLC needle biopsy dataset
    | 270 slides | Inhouse | Ratio of the number of tumor positive cell pixels to
    the total number of tumor cell pixels: 0.94 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Kapil ç­‰ ([2018](#bib.bib82)) | åŸºäº GAN çš„ | è‚ºç™Œ | Ventana PD-L1 (SP263) æ£€æµ‹ |
    è‡ªåŠ¨è‚¿ç˜¤æ¯”ä¾‹è¯„åˆ† | NSCLC é’ˆåˆºæ´»æ£€æ•°æ®é›† | 270 å¼ å¹»ç¯ç‰‡ | å†…éƒ¨æ•°æ® | è‚¿ç˜¤é˜³æ€§ç»†èƒåƒç´ ä¸è‚¿ç˜¤ç»†èƒåƒç´ æ€»æ•°çš„æ¯”ç‡: 0.94 |'
- en: '| Cong etÂ al. ([2021](#bib.bib38)) | GAN-based | Brain Cancer | H&E | Stain
    normalisation | TCGA1 glioma cohort | 22,229 images | Liu etÂ al. ([2020](#bib.bib101))
    | F1 score: 0.937 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Cong ç­‰ ([2021](#bib.bib38)) | åŸºäº GAN çš„ | è„‘ç™Œ | H&E | æŸ“è‰²æ ‡å‡†åŒ– | TCGA1 ç¥ç»èƒ¶è´¨ç˜¤é˜Ÿåˆ—
    | 22,229 å¼ å›¾åƒ | Liu ç­‰ ([2020](#bib.bib101)) | F1 åˆ†æ•°: 0.937 |'
- en: '| Breast Cancer | BreakHis database | 7,909 images | Spanhol etÂ al. ([2015](#bib.bib157))
    | F1 score: 0.980 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| ä¹³è…ºç™Œ | BreakHis æ•°æ®åº“ | 7,909 å¼ å›¾åƒ | Spanhol ç­‰ ([2015](#bib.bib157)) | F1 åˆ†æ•°:
    0.980 |'
- en: '| Sparks andÂ Madabhushi ([2016](#bib.bib158)) | Manifold- learning-based |
    Prostate Cancer | H&E | Image retrieval | Prostate histpathology dataset | 58
    patients | Inhouse | SI: 0.14 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Sparks å’Œ Madabhushi ([2016](#bib.bib158)) | æµå½¢å­¦ä¹ åŸºç¡€çš„ | å‰åˆ—è…ºç™Œ | H&E | å›¾åƒæ£€ç´¢ |
    å‰åˆ—è…ºç»„ç»‡ç—…ç†æ•°æ®é›† | 58 ä½æ‚£è€… | å†…éƒ¨æ•°æ® | SI: 0.14 |'
- en: '| Li etÂ al. ([2018](#bib.bib100)) | Expectation- Maximization-based | Prostate
    Cancer | H&E | Semantic segmentation | Prostate dataset | 135 fully annotated
    and 1800 weakly annotated tiles | Gertych etÂ al. ([2015](#bib.bib60)) | AJI: 0.495
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Li ç­‰ ([2018](#bib.bib100)) | åŸºäºæœŸæœ›æœ€å¤§åŒ–çš„ | å‰åˆ—è…ºç™Œ | H&E | è¯­ä¹‰åˆ†å‰² | å‰åˆ—è…ºæ•°æ®é›† | 135
    ä¸ªå®Œå…¨æ³¨é‡Šå’Œ 1800 ä¸ªå¼±æ³¨é‡Šå›¾å— | Gertych ç­‰ ([2015](#bib.bib60)) | AJI: 0.495 |'
- en: '| Su etÂ al. ([2021](#bib.bib164)) | Association- learning-based | Breast Cancer
    | H&E | Classification of cancerous and non-cancerous slides | Bioimaging 2015
    challenge dataset | 285 images | AraÃºjo etÂ al. ([2017](#bib.bib4)) | F1 score:
    0.75 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Su ç­‰ ([2021](#bib.bib164)) | å…³è”å­¦ä¹ åŸºç¡€çš„ | ä¹³è…ºç™Œ | H&E | ç™Œæ€§å’Œéç™Œæ€§å¹»ç¯ç‰‡åˆ†ç±» | Bioimaging
    2015 æŒ‘æˆ˜æ•°æ®é›† | 285 å¼ å›¾åƒ | AraÃºjo ç­‰ ([2017](#bib.bib4)) | F1 åˆ†æ•°: 0.75 |'
- en: '| BACH dataset | 400 images | Aresta etÂ al. ([2019](#bib.bib5)) | F1 score:
    0.77 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| BACH æ•°æ®é›† | 400 å¼ å›¾åƒ | Aresta ç­‰ ([2019](#bib.bib5)) | F1 åˆ†æ•°: 0.77 |'
- en: 3.3 Self-Supervised Learning Paradigm
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 è‡ªç›‘ç£å­¦ä¹ èŒƒå¼
- en: Unlike the former two paradigms, the self-supervised learning paradigm does
    not perform the classification or segmentation of pathological images directly,
    but in a two-stage â€™pre-training and fine-tuningâ€™ approach. Due to the small number
    of annotated pathological images, it is not enough to use these data to directly
    train the model. Therefore, the self-supervised learning paradigm aims to first
    learn effective feature representations from a large amount of unlabeled data,
    which is called the pre-training process. Afterwards, the feature representations
    learned in the self-supervised auxiliary tasks are used to be transferred to train
    the downstream tasks using limited labeled data, which is called the fine-tuning
    process. In this way, good feature representations can effectively help the model
    to achieve good results even if it is trained with only a small amount of labeled
    data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å‰ä¸¤ç§èŒƒå¼ä¸åŒï¼Œè‡ªç›‘ç£å­¦ä¹ èŒƒå¼å¹¶ä¸ç›´æ¥å¯¹ç—…ç†å›¾åƒè¿›è¡Œåˆ†ç±»æˆ–åˆ†å‰²ï¼Œè€Œæ˜¯é‡‡ç”¨â€œä¸¤é˜¶æ®µâ€˜é¢„è®­ç»ƒå’Œå¾®è°ƒâ€™â€çš„æ–¹æ³•ã€‚ç”±äºæ ‡è®°çš„ç—…ç†å›¾åƒæ•°é‡è¾ƒå°‘ï¼Œæ— æ³•ä»…ç”¨è¿™äº›æ•°æ®ç›´æ¥è®­ç»ƒæ¨¡å‹ã€‚å› æ­¤ï¼Œè‡ªç›‘ç£å­¦ä¹ èŒƒå¼æ—¨åœ¨é¦–å…ˆä»å¤§é‡æœªæ ‡è®°æ•°æ®ä¸­å­¦ä¹ æœ‰æ•ˆçš„ç‰¹å¾è¡¨ç¤ºï¼Œè¿™è¢«ç§°ä¸ºé¢„è®­ç»ƒè¿‡ç¨‹ã€‚ä¹‹åï¼Œåˆ©ç”¨åœ¨è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡ä¸­å­¦ä¹ åˆ°çš„ç‰¹å¾è¡¨ç¤ºï¼Œå°†å…¶è¿ç§»ç”¨äºä½¿ç”¨æœ‰é™æ ‡è®°æ•°æ®è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡ï¼Œè¿™è¢«ç§°ä¸ºå¾®è°ƒè¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå³ä½¿ä»…ç”¨å°‘é‡æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè‰¯å¥½çš„ç‰¹å¾è¡¨ç¤ºä¹Ÿèƒ½æœ‰æ•ˆå¸®åŠ©æ¨¡å‹è·å¾—è‰¯å¥½çš„ç»“æœã€‚
- en: The process of pre-training, i.e., the learning process of good feature representations,
    is the key to self-supervised learning. Typically, self-supervised learning learns
    good feature representations by performing self-supervised auxiliary tasks. In
    a self-supervised auxiliary task, certain inherent properties of the unlabeled
    data are first used to generate supervised signals, and then the network is trained
    by these self-supervised signals. Different studies usually focus on designing
    different self-supervised auxiliary tasks to perform feature representation learning
    efficiently. According to the properties of the auxiliary tasks, existing self-supervised
    learning paradigms can be mainly classified into predictive self-supervised learning,
    generative self-supervised learning, and contrastive self-supervised learning.
    Predictive self-supervised learning learns good feature representations by constructing
    the auxiliary tasks as classification problems with unlabeled data; generative
    self-supervised learning learns good feature representations by reconstructing
    the input images; and contrastive self-supervised learning learns good feature
    representations by learning to distinguish between similar samples (positive samples)
    and dissimilar samples (negative samples). For a systematic review of self-supervised
    methods in the natural image domain and medical image domain, we recommend the
    reviews by Liu *et al.*Â [2021](#bib.bib102) and Shurrab *et al.*Â [2021](#bib.bib152).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œå³è‰¯å¥½ç‰¹å¾è¡¨ç¤ºçš„å­¦ä¹ è¿‡ç¨‹ï¼Œæ˜¯è‡ªç›‘ç£å­¦ä¹ çš„å…³é”®ã€‚é€šå¸¸ï¼Œè‡ªç›‘ç£å­¦ä¹ é€šè¿‡æ‰§è¡Œè‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡æ¥å­¦ä¹ è‰¯å¥½çš„ç‰¹å¾è¡¨ç¤ºã€‚åœ¨è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡ä¸­ï¼Œé¦–å…ˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„æŸäº›å›ºæœ‰å±æ€§ç”Ÿæˆç›‘ç£ä¿¡å·ï¼Œç„¶åé€šè¿‡è¿™äº›è‡ªç›‘ç£ä¿¡å·æ¥è®­ç»ƒç½‘ç»œã€‚ä¸åŒçš„ç ”ç©¶é€šå¸¸é›†ä¸­äºè®¾è®¡ä¸åŒçš„è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡ï¼Œä»¥æœ‰æ•ˆåœ°è¿›è¡Œç‰¹å¾è¡¨ç¤ºå­¦ä¹ ã€‚æ ¹æ®è¾…åŠ©ä»»åŠ¡çš„æ€§è´¨ï¼Œç°æœ‰çš„è‡ªç›‘ç£å­¦ä¹ èŒƒå¼ä¸»è¦å¯ä»¥åˆ†ä¸ºé¢„æµ‹æ€§è‡ªç›‘ç£å­¦ä¹ ã€ç”Ÿæˆæ€§è‡ªç›‘ç£å­¦ä¹ å’Œå¯¹æ¯”æ€§è‡ªç›‘ç£å­¦ä¹ ã€‚é¢„æµ‹æ€§è‡ªç›‘ç£å­¦ä¹ é€šè¿‡å°†è¾…åŠ©ä»»åŠ¡æ„å»ºä¸ºå¸¦æœ‰æœªæ ‡è®°æ•°æ®çš„åˆ†ç±»é—®é¢˜æ¥å­¦ä¹ è‰¯å¥½çš„ç‰¹å¾è¡¨ç¤ºï¼›ç”Ÿæˆæ€§è‡ªç›‘ç£å­¦ä¹ é€šè¿‡é‡å»ºè¾“å…¥å›¾åƒæ¥å­¦ä¹ è‰¯å¥½çš„ç‰¹å¾è¡¨ç¤ºï¼›å¯¹æ¯”æ€§è‡ªç›‘ç£å­¦ä¹ é€šè¿‡å­¦ä¹ åŒºåˆ†ç›¸ä¼¼æ ·æœ¬ï¼ˆæ­£æ ·æœ¬ï¼‰å’Œä¸ç›¸ä¼¼æ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬ï¼‰æ¥å­¦ä¹ è‰¯å¥½çš„ç‰¹å¾è¡¨ç¤ºã€‚æœ‰å…³è‡ªç„¶å›¾åƒé¢†åŸŸå’ŒåŒ»å­¦å›¾åƒé¢†åŸŸè‡ªç›‘ç£æ–¹æ³•çš„ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œæˆ‘ä»¬æ¨èåˆ˜*ç­‰*
    [2021](#bib.bib102) å’Œ Shurrab*ç­‰* [2021](#bib.bib152)çš„ç»¼è¿°ã€‚
- en: 'In this section, we provide a detailed review of the studies on self-supervised
    learning for pathological image analysis. Currently, some studies focus on proposing
    innovative self-supervised frameworks for pathological images (we call them study
    on novel self-supervised frameworks), while others attempt to apply existing self-supervised
    learning methods to pathological image analysis (we call them study on application
    of self-supervised frameworks). We introduce studies on novel self-supervised
    frameworks in Section [3.3.1](#S3.SS3.SSS1 "3.3.1 Study on Novel Self-supervised
    Frameworks â€£ 3.3 Self-Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis"), where we focus on predictive self-supervised learning, generative
    self-supervised learning, contrastive self-supervised learning, and hybrid self-supervised
    learning and their state-of-the-art approaches in pathological image analysis.
    We introduce the study on application of self-supervised frameworks in Section
    [3.3.2](#S3.SS3.SSS2 "3.3.2 Study on Applications of Self-supervised Frameworks
    â€£ 3.3 Self-Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis"). Table [5](#S3.T5 "Table 5 â€£ 3.3.2 Study on Applications of Self-supervised
    Frameworks â€£ 3.3 Self-Supervised Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient
    Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based
    Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological
    Image Analysis") summarizes a detailed list of literatures in this section.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†å›é¡¾äº†ç”¨äºç—…ç†å›¾åƒåˆ†æçš„è‡ªç›‘ç£å­¦ä¹ ç ”ç©¶ã€‚ç›®å‰ï¼Œä¸€äº›ç ”ç©¶ä¸“æ³¨äºæå‡ºåˆ›æ–°çš„è‡ªç›‘ç£æ¡†æ¶ç”¨äºç—…ç†å›¾åƒï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºæ–°é¢–è‡ªç›‘ç£æ¡†æ¶ç ”ç©¶ï¼‰ï¼Œè€Œå¦ä¸€äº›åˆ™å°è¯•å°†ç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åº”ç”¨äºç—…ç†å›¾åƒåˆ†æï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºè‡ªç›‘ç£æ¡†æ¶åº”ç”¨ç ”ç©¶ï¼‰ã€‚æˆ‘ä»¬åœ¨ç¬¬[3.3.1](#S3.SS3.SSS1
    "3.3.1 Study on Novel Self-supervised Frameworks â€£ 3.3 Self-Supervised Learning
    Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis and Analysis:
    A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised
    and Self-supervised Techniques in Histopathological Image Analysis")èŠ‚ä»‹ç»äº†æ–°é¢–è‡ªç›‘ç£æ¡†æ¶çš„ç ”ç©¶ï¼Œå…¶ä¸­æˆ‘ä»¬é‡ç‚¹è®¨è®ºäº†é¢„æµ‹æ€§è‡ªç›‘ç£å­¦ä¹ ã€ç”Ÿæˆæ€§è‡ªç›‘ç£å­¦ä¹ ã€å¯¹æ¯”æ€§è‡ªç›‘ç£å­¦ä¹ å’Œæ··åˆè‡ªç›‘ç£å­¦ä¹ åŠå…¶åœ¨ç—…ç†å›¾åƒåˆ†æä¸­çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ç¬¬[3.3.2](#S3.SS3.SSS2
    "3.3.2 Study on Applications of Self-supervised Frameworks â€£ 3.3 Self-Supervised
    Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis
    and Analysis: A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised,
    Semi-supervised and Self-supervised Techniques in Histopathological Image Analysis")èŠ‚ä»‹ç»äº†è‡ªç›‘ç£æ¡†æ¶åº”ç”¨ç ”ç©¶ã€‚è¡¨[5](#S3.T5
    "Table 5 â€£ 3.3.2 Study on Applications of Self-supervised Frameworks â€£ 3.3 Self-Supervised
    Learning Paradigm â€£ 3 Paradigms â€£ Towards Label-efficient Automatic Diagnosis
    and Analysis: A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised,
    Semi-supervised and Self-supervised Techniques in Histopathological Image Analysis")æ€»ç»“äº†æœ¬èŠ‚çš„è¯¦ç»†æ–‡çŒ®åˆ—è¡¨ã€‚'
- en: 3.3.1 Study on Novel Self-supervised Frameworks
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 æ–°é¢–è‡ªç›‘ç£æ¡†æ¶çš„ç ”ç©¶
- en: Predictive Self-supervised Learning Approach
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ€§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•
- en: Fundamental Principles
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: åŸºæœ¬åŸç†
- en: Predictive self-supervised learning learns good feature representations by constructing
    the auxiliary tasks as classification problems with unlabeled data, and the class
    labels for classification are constructed from the unlabeled data itself. Currently,
    predictive self-supervised auxiliary tasks widely applied in natural image processing
    are relative position prediction (Doersch *et al.*Â [2015](#bib.bib49)), solving
    Jigsaw puzzles (Noroozi *et al.*Â [2016](#bib.bib120)), and rotation angle prediction
    (Gidaris *et al.*Â [2018](#bib.bib61)), etc.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ€§è‡ªç›‘ç£å­¦ä¹ é€šè¿‡å°†è¾…åŠ©ä»»åŠ¡æ„å»ºä¸ºæ— æ ‡ç­¾æ•°æ®çš„åˆ†ç±»é—®é¢˜æ¥å­¦ä¹ è‰¯å¥½çš„ç‰¹å¾è¡¨ç¤ºï¼Œåˆ†ç±»çš„ç±»åˆ«æ ‡ç­¾æ˜¯ä»æ— æ ‡ç­¾æ•°æ®æœ¬èº«æ„å»ºçš„ã€‚ç›®å‰ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶å›¾åƒå¤„ç†çš„é¢„æµ‹æ€§è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡åŒ…æ‹¬ç›¸å¯¹ä½ç½®é¢„æµ‹ï¼ˆDoersch
    *et al.*Â [2015](#bib.bib49)ï¼‰ã€æ‹¼å›¾è§£å†³ï¼ˆNoroozi *et al.*Â [2016](#bib.bib120)ï¼‰å’Œæ—‹è½¬è§’åº¦é¢„æµ‹ï¼ˆGidaris
    *et al.*Â [2018](#bib.bib61)ï¼‰ç­‰ã€‚
- en: Study in Pathological Image Analysis
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ç—…ç†å›¾åƒåˆ†æç ”ç©¶
- en: In the field of pathological image processing, Sahasrabudhe *et al.*Â [2020](#bib.bib141)
    proposed the auxiliary task of predicting patch magnification for cell nuclei
    segmentation. Their main idea is that given WSIs of different magnification classes
    (e.g., 5$\times" display="inline"><semantics ><mo >Ã—</mo><annotation encoding="application/x-tex"
    id=$, 10$\times" display="inline"><semantics ><mo >Ã—</mo><annotation encoding="application/x-tex"
    id=$, 20$\times" display="inline"><semantics ><mo >Ã—</mo><annotation encoding="application/x-tex"
    id=$), they first obtained patches of different magnifications from them and then
    predicted the magnification class of those patches by examining the size and texture
    of the cell nuclei in the patches. Srinidhi *et al.*Â [2022](#bib.bib160) proposed
    the resolution sequence prediction (RSP) auxiliary task. First they used patches
    with different magnifications to construct different combinations of resolution
    sequences, and then trained the network to predict the order of the resolution
    sequences. Koohbanani *et al.*Â [2021](#bib.bib89) proposed magnification prediction
    and solving magnification puzzles auxiliary tasks for pathological images. They
    first trained the network to accurately predict the magnification category, and
    then trained the network to predict the order of the patches with different magnifications.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç—…ç†å›¾åƒå¤„ç†é¢†åŸŸï¼ŒSahasrabudhe *ç­‰* [2020](#bib.bib141) æå‡ºäº†é¢„æµ‹ç»†èƒæ ¸åˆ†å‰²çš„è¡¥å……ä»»åŠ¡ã€‚ä»–ä»¬çš„ä¸»è¦æ€è·¯æ˜¯ï¼Œå¯¹äºä¸åŒæ”¾å¤§å€æ•°çš„WSIï¼ˆä¾‹å¦‚ï¼Œ5$\times$ã€10$\times$ã€20$\times$ï¼‰ï¼Œä»–ä»¬é¦–å…ˆä»ä¸­è·å–ä¸åŒæ”¾å¤§å€æ•°çš„å›¾å—ï¼Œç„¶åé€šè¿‡æ£€æŸ¥å›¾å—ä¸­ç»†èƒæ ¸çš„å¤§å°å’Œçº¹ç†æ¥é¢„æµ‹è¿™äº›å›¾å—çš„æ”¾å¤§å€æ•°ç±»åˆ«ã€‚Srinidhi
    *ç­‰* [2022](#bib.bib160) æå‡ºäº†åˆ†è¾¨ç‡åºåˆ—é¢„æµ‹ï¼ˆRSPï¼‰è¡¥å……ä»»åŠ¡ã€‚ä»–ä»¬é¦–å…ˆä½¿ç”¨ä¸åŒæ”¾å¤§å€æ•°çš„å›¾å—æ„å»ºä¸åŒçš„åˆ†è¾¨ç‡åºåˆ—ç»„åˆï¼Œç„¶åè®­ç»ƒç½‘ç»œé¢„æµ‹åˆ†è¾¨ç‡åºåˆ—çš„é¡ºåºã€‚Koohbanani
    *ç­‰* [2021](#bib.bib89) æå‡ºäº†æ”¾å¤§å€æ•°é¢„æµ‹å’Œè§£å†³æ”¾å¤§å€æ•°éš¾é¢˜çš„è¡¥å……ä»»åŠ¡ã€‚ä»–ä»¬é¦–å…ˆè®­ç»ƒç½‘ç»œå‡†ç¡®é¢„æµ‹æ”¾å¤§å€æ•°ç±»åˆ«ï¼Œç„¶åè®­ç»ƒç½‘ç»œé¢„æµ‹ä¸åŒæ”¾å¤§å€æ•°å›¾å—çš„é¡ºåºã€‚
- en: Generative Self-supervised Learning Approach
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç”Ÿæˆè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•
- en: Fundamental Principles
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: åŸºæœ¬åŸç†
- en: Generative self-supervised learning learns good feature representations by reconstructing
    the input images. They argue that the image itself is a useful self-supervised
    information and that the network can learn the potential feature representations
    of the generated image during the image reconstruction process. In natural image
    processing, autoencoders (Goodfellow *et al.*Â [2016](#bib.bib64)) are representative
    of early work on generative self-supervised feature representation learning. Later,
    denoising autoencoders (Vincent *et al.*Â [2008](#bib.bib179)) enhanced the feature
    representation capability of the model by introducing noise. Subsequently, researchers
    proposed a series of reconstructive self-supervised auxiliary tasks, including
    inpainting (Pathak *et al.*Â [2016](#bib.bib124)), colorization (Zhang *et al.*Â [2016](#bib.bib204)),
    patch shuffling and restoration (Chen *et al.*Â [2019](#bib.bib23), Zhou *et al.*Â [2021](#bib.bib210))
    to further enhance the feature representation capability of the network and achieved
    promising results. On the other hand, a series of GAN-based models (e.g., DCGAN
    [2015](#bib.bib133), BiGAN [2016](#bib.bib50)) have also been used to perform
    self-supervised representation learning. In the latest self-supervised studies
    on natural images, a series (e.g., BEiT [2021](#bib.bib6), MAE [2021](#bib.bib73),
    PeCo [2021](#bib.bib51), etc.) of self-supervised studies based on masked image
    blocks and reconstruction using Transformer achieved the highest performance,
    which is expected to start a new wave of research on reconstruction-based self-supervised
    representation learning.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆè‡ªç›‘ç£å­¦ä¹ é€šè¿‡é‡å»ºè¾“å…¥å›¾åƒæ¥å­¦ä¹ è‰¯å¥½çš„ç‰¹å¾è¡¨ç¤ºã€‚ä»–ä»¬è®¤ä¸ºå›¾åƒæœ¬èº«æ˜¯ä¸€ç§æœ‰ç”¨çš„è‡ªç›‘ç£ä¿¡æ¯ï¼Œç½‘ç»œå¯ä»¥åœ¨å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­å­¦ä¹ ç”Ÿæˆå›¾åƒçš„æ½œåœ¨ç‰¹å¾è¡¨ç¤ºã€‚åœ¨è‡ªç„¶å›¾åƒå¤„ç†ä¸­ï¼Œè‡ªç¼–ç å™¨ï¼ˆGoodfellow
    *ç­‰* [2016](#bib.bib64)ï¼‰æ˜¯ç”Ÿæˆè‡ªç›‘ç£ç‰¹å¾è¡¨ç¤ºå­¦ä¹ çš„æ—©æœŸä»£è¡¨å·¥ä½œã€‚éšåï¼Œå»å™ªè‡ªç¼–ç å™¨ï¼ˆVincent *ç­‰* [2008](#bib.bib179)ï¼‰é€šè¿‡å¼•å…¥å™ªå£°å¢å¼ºäº†æ¨¡å‹çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚éšåï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç³»åˆ—é‡å»ºè‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä¿®è¡¥ï¼ˆPathak
    *ç­‰* [2016](#bib.bib124)ï¼‰ã€ç€è‰²ï¼ˆZhang *ç­‰* [2016](#bib.bib204)ï¼‰ã€è¡¥ä¸æ‰“ä¹±å’Œæ¢å¤ï¼ˆChen *ç­‰* [2019](#bib.bib23)ï¼ŒZhou
    *ç­‰* [2021](#bib.bib210)ï¼‰ï¼Œä»¥è¿›ä¸€æ­¥æå‡ç½‘ç»œçš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶å–å¾—äº†è‰¯å¥½çš„ç»“æœã€‚å¦ä¸€æ–¹é¢ï¼Œä¸€ç³»åˆ—åŸºäºGANçš„æ¨¡å‹ï¼ˆå¦‚DCGAN [2015](#bib.bib133)ï¼ŒBiGAN
    [2016](#bib.bib50)ï¼‰ä¹Ÿè¢«ç”¨äºæ‰§è¡Œè‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€‚åœ¨æœ€æ–°çš„è‡ªç„¶å›¾åƒè‡ªç›‘ç£ç ”ç©¶ä¸­ï¼Œä¸€ç³»åˆ—ï¼ˆå¦‚BEiT [2021](#bib.bib6)ï¼ŒMAE
    [2021](#bib.bib73)ï¼ŒPeCo [2021](#bib.bib51)ç­‰ï¼‰åŸºäºé®ç½©å›¾åƒå—å’Œä½¿ç”¨Transformerè¿›è¡Œé‡å»ºçš„è‡ªç›‘ç£ç ”ç©¶è¾¾åˆ°äº†æœ€é«˜çš„æ€§èƒ½ï¼Œé¢„è®¡å°†å¼•å‘æ–°ä¸€æ³¢åŸºäºé‡å»ºçš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ç ”ç©¶ã€‚
- en: Study in Pathological Image Analysis
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ç—…ç†å›¾åƒåˆ†æä¸­çš„ç ”ç©¶
- en: In pathological image analysis, Muhammad *et al.*Â [2019](#bib.bib114) proposed
    a new deep convolutional autoencoder-based clustering model to learn the feature
    representations of pathological images. Mahapatra *et al.*Â [2020](#bib.bib108)
    incorporated semantic information into a GAN-based generative model for self-supervised
    feature representation learning and used it for the stain normalization task of
    pathological images. Quiros *et al.*Â [2019](#bib.bib131), [2021](#bib.bib130)
    designed GANs for pathological images to extract key feature representations of
    tissues. Boyd et al *et al.*Â [2021](#bib.bib15) proposed a new generative auxiliary
    task which performs representation learning by extending the view of image patches.
    Hou et al *et al.*Â [2019](#bib.bib75) proposed a sparse convolutional autoencoder
    (CAE) for simultaneous nuclei detection and feature extraction in histopathological
    images. Koohbanani *et al.*Â [2021](#bib.bib89) proposed the hematoxylin channel
    prediction auxiliary task, where they used hematoxylin and eosin (H&E) stained
    images to predict the hematoxylin channel pixel by pixel.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç—…ç†å›¾åƒåˆ†æä¸­ï¼Œç©†ç½•é»˜å¾· *ç­‰* [2019](#bib.bib114) æå‡ºäº†ä¸€ä¸ªåŸºäºæ·±åº¦å·ç§¯è‡ªç¼–ç å™¨çš„æ–°å‹èšç±»æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ ç—…ç†å›¾åƒçš„ç‰¹å¾è¡¨ç¤ºã€‚é©¬å“ˆå¸•ç‰¹æ‹‰
    *ç­‰* [2020](#bib.bib108) å°†è¯­ä¹‰ä¿¡æ¯èå…¥åˆ°åŸºäºGANçš„ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œç”¨äºè‡ªç›‘ç£ç‰¹å¾è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶å°†å…¶åº”ç”¨äºç—…ç†å›¾åƒçš„æŸ“è‰²ä½“æ ‡å‡†åŒ–ä»»åŠ¡ã€‚åŸºç½—æ–¯
    *ç­‰* [2019](#bib.bib131)ï¼Œ[2021](#bib.bib130) è®¾è®¡äº†ç”¨äºç—…ç†å›¾åƒçš„GANï¼Œæå–ç»„ç»‡çš„å…³é”®ç‰¹å¾è¡¨ç¤ºã€‚åšä¼Šå¾· *ç­‰*
    [2021](#bib.bib15) æå‡ºäº†ä¸€ä¸ªæ–°çš„ç”Ÿæˆè¾…åŠ©ä»»åŠ¡ï¼Œé€šè¿‡æ‰©å±•å›¾åƒè¡¥ä¸çš„è§†è§’æ¥è¿›è¡Œè¡¨ç¤ºå­¦ä¹ ã€‚ä¾¯ *ç­‰* [2019](#bib.bib75) æå‡ºäº†ä¸€ä¸ªç¨€ç–å·ç§¯è‡ªç¼–ç å™¨ï¼ˆCAEï¼‰ï¼Œç”¨äºåœ¨ç»„ç»‡ç—…ç†å›¾åƒä¸­åŒæ—¶è¿›è¡Œæ ¸æ£€æµ‹å’Œç‰¹å¾æå–ã€‚åº“ç­å°¼
    *ç­‰* [2021](#bib.bib89) æå‡ºäº†è¡€çº¢ç´ é€šé“é¢„æµ‹è¾…åŠ©ä»»åŠ¡ï¼Œä»–ä»¬ä½¿ç”¨è¡€çº¢ç´ å’Œä¼Šçº¢ï¼ˆH&Eï¼‰æŸ“è‰²å›¾åƒé€åƒç´ é¢„æµ‹è¡€çº¢ç´ é€šé“ã€‚
- en: Contrastive Self-supervised Learning Approach
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•
- en: Fundamental Principles
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: åŸºæœ¬åŸç†
- en: The contrastive self-supervised approach is one of the most popular self-supervised
    paradigms, which focuses on learning good feature representations by encouraging
    the model to learn to distinguish between similar samples (positive samples) and
    dissimilar samples (negative samples).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ¯”è‡ªç›‘ç£æ–¹æ³•æ˜¯æœ€å—æ¬¢è¿çš„è‡ªç›‘ç£èŒƒå¼ä¹‹ä¸€ï¼Œå®ƒé€šè¿‡é¼“åŠ±æ¨¡å‹å­¦ä¹ åŒºåˆ†ç›¸ä¼¼æ ·æœ¬ï¼ˆæ­£æ ·æœ¬ï¼‰å’Œä¸ç›¸ä¼¼æ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬ï¼‰æ¥å­¦ä¹ è‰¯å¥½çš„ç‰¹å¾è¡¨ç¤ºã€‚
- en: Contrast predictive coding (CPC) (Van *et al.*Â [2018](#bib.bib174)) is an early
    contrastive self-supervised method applied to natural image processing whose goal
    is to maximize the mutual information between patches (positive samples) from
    the same image and minimize the mutual information between patches (negative samples)
    from different images within a mini-batch. Typical subsequent studies have been
    devoted to constructing negative samples. MoCo (He *et al.*Â [2020](#bib.bib74))
    is a momentum-based contrastive self-supervised framework, which is mainly based
    on the ideas of dynamic dictionary-lookup and queues. SimCLR (Chen *et al.*Â [2020](#bib.bib26))
    is a simple contrastive learning framework that aims to maximize the cosine similarity
    between two augmented views of the same image (positive samples) and minimize
    the similarity between different images in a minibatch (negative samples).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ¯”é¢„æµ‹ç¼–ç ï¼ˆCPCï¼‰ï¼ˆVan *et al.* [2018](#bib.bib174)ï¼‰æ˜¯ä¸€ç§æ—©æœŸåº”ç”¨äºè‡ªç„¶å›¾åƒå¤„ç†çš„å¯¹æ¯”è‡ªç›‘ç£æ–¹æ³•ï¼Œå…¶ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ¥è‡ªåŒä¸€å›¾åƒçš„è¡¥ä¸ï¼ˆæ­£æ ·æœ¬ï¼‰ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œå¹¶æœ€å°åŒ–æ¥è‡ªä¸åŒå›¾åƒçš„è¡¥ä¸ï¼ˆè´Ÿæ ·æœ¬ï¼‰ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚å…¸å‹çš„åç»­ç ”ç©¶è‡´åŠ›äºæ„é€ è´Ÿæ ·æœ¬ã€‚MoCoï¼ˆHe
    *et al.* [2020](#bib.bib74)ï¼‰æ˜¯åŸºäºåŠ¨é‡çš„å¯¹æ¯”è‡ªç›‘ç£æ¡†æ¶ï¼Œä¸»è¦åŸºäºåŠ¨æ€å­—å…¸æŸ¥æ‰¾å’Œé˜Ÿåˆ—çš„æ€æƒ³ã€‚SimCLRï¼ˆChen *et al.*
    [2020](#bib.bib26)ï¼‰æ˜¯ä¸€ä¸ªç®€å•çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æœ€å¤§åŒ–åŒä¸€å›¾åƒçš„ä¸¤ä¸ªå¢å¼ºè§†å›¾ï¼ˆæ­£æ ·æœ¬ï¼‰ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶æœ€å°åŒ–å°æ‰¹é‡ä¸­ä¸åŒå›¾åƒä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆè´Ÿæ ·æœ¬ï¼‰ã€‚
- en: These methods rely heavily on a large number of negative samples since only
    positive samples will easily lead to model degeneration, i.e., mapping the features
    of all samples to an identical vector. However, recent studies have shown that
    negative samples are not necessary. Caron *et al.*Â [2020](#bib.bib18) introduced
    clustering into contrastive learning, thus eliminating the need for negative samples.
    Chen *et al.*Â [2021](#bib.bib28) explored stop-gradient operation applied to siamese
    networks without the need for a large number of negative samples. Grill *et al.*Â [2020](#bib.bib66),
    Caron *et al.*Â [2021](#bib.bib19) proposed a self-supervised learning model based
    on a teacher-student knowledge distillation framework that achieves state-of-the-art
    performance without any negative samples.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ–¹æ³•ä¸¥é‡ä¾èµ–å¤§é‡è´Ÿæ ·æœ¬ï¼Œå› ä¸ºä»…æœ‰æ­£æ ·æœ¬å®¹æ˜“å¯¼è‡´æ¨¡å‹é€€åŒ–ï¼Œå³å°†æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾æ˜ å°„åˆ°ç›¸åŒçš„å‘é‡ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜è´Ÿæ ·æœ¬å¹¶éå¿…éœ€ã€‚Caron *et
    al.* [2020](#bib.bib18) å°†èšç±»å¼•å…¥å¯¹æ¯”å­¦ä¹ ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹è´Ÿæ ·æœ¬çš„éœ€æ±‚ã€‚Chen *et al.* [2021](#bib.bib28)
    æ¢ç´¢äº†åº”ç”¨äºå­ªç”Ÿç½‘ç»œçš„åœæ­¢æ¢¯åº¦æ“ä½œï¼Œæ— éœ€å¤§é‡è´Ÿæ ·æœ¬ã€‚Grill *et al.* [2020](#bib.bib66)ï¼ŒCaron *et al.* [2021](#bib.bib19)
    æå‡ºäº†åŸºäºæ•™å¸ˆ-å­¦ç”ŸçŸ¥è¯†è’¸é¦æ¡†æ¶çš„è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œåœ¨æ²¡æœ‰ä»»ä½•è´Ÿæ ·æœ¬çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚
- en: Study in Pathological Image Analysis
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ç—…ç†å›¾åƒåˆ†æç ”ç©¶
- en: In pathological image analysis, Xie *et al.*Â [2020](#bib.bib191) employed patches
    from different magnifications as positive samples and patches from different magnifications
    as negative samples and constructed scale-wise triplet loss to perform contrastive
    learning for the nuclei segmentation. Chhipa *et al.*Â [2022](#bib.bib32) proposed
    Magnification Prior Contrastive Similarity (MPCS) to construct contrastive loss.
    Xu *et al.*Â [2020](#bib.bib193) proposed a self-supervised Deformation Representation
    Learning (DRL) framework to learn semantic features from unlabeled pathological
    images. They used mutual information to train the network to distinguish original
    histopathological images from those deformed in local structure, while consistent
    global contextual information was maintained using noise contrastive estimation
    (NCE). Wang *et al.*Â [2021](#bib.bib183) proposed Transpath based on the BYOL
    framework [2020](#bib.bib66). They first collected the current largest histopathological
    image dataset for self-supervised pre-training, which includes about 2.7 million
    images from 32529 WSIs. Then they proposed a hybrid framework combining CNN and
    Transformer to extract both local structural features and global contextual features,
    and proposed a TAE module to further enhance the feature extraction capability.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç—…ç†å›¾åƒåˆ†æä¸­ï¼ŒXie *ç­‰äºº* [2020](#bib.bib191) ä½¿ç”¨ä¸åŒæ”¾å¤§å€æ•°çš„å›¾å—ä½œä¸ºæ­£æ ·æœ¬ï¼Œè€Œç”¨ä¸åŒæ”¾å¤§å€æ•°çš„å›¾å—ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œå¹¶æ„å»ºäº†å°ºåº¦çº§ä¸‰å…ƒç»„æŸå¤±ï¼Œä»¥è¿›è¡Œå¯¹æ¯”å­¦ä¹ ä»¥è¿›è¡Œæ ¸åˆ†å‰²ã€‚Chhipa
    *ç­‰äºº* [2022](#bib.bib32) æå‡ºäº†æ”¾å¤§ä¼˜å…ˆå¯¹æ¯”ç›¸ä¼¼åº¦ï¼ˆMPCSï¼‰æ¥æ„å»ºå¯¹æ¯”æŸå¤±ã€‚Xu *ç­‰äºº* [2020](#bib.bib193)
    æå‡ºäº†ä¸€ä¸ªè‡ªç›‘ç£å˜å½¢è¡¨ç¤ºå­¦ä¹ ï¼ˆDRLï¼‰æ¡†æ¶ï¼Œä»æœªæ ‡è®°çš„ç—…ç†å›¾åƒä¸­å­¦ä¹ è¯­ä¹‰ç‰¹å¾ã€‚ä»–ä»¬ä½¿ç”¨äº’ä¿¡æ¯è®­ç»ƒç½‘ç»œï¼Œä»¥åŒºåˆ†åŸå§‹çš„ç»„ç»‡ç—…ç†å›¾åƒå’Œå±€éƒ¨ç»“æ„å˜å½¢çš„å›¾åƒï¼ŒåŒæ—¶ä½¿ç”¨å™ªå£°å¯¹æ¯”ä¼°è®¡ï¼ˆNCEï¼‰ä¿æŒä¸€è‡´çš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚Wang
    *ç­‰äºº* [2021](#bib.bib183) æå‡ºäº†åŸºäº BYOL æ¡†æ¶ [2020](#bib.bib66) çš„ Transpathã€‚ä»–ä»¬é¦–å…ˆæ”¶é›†äº†å½“å‰æœ€å¤§çš„ç—…ç†å›¾åƒæ•°æ®é›†ç”¨äºè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå…¶ä¸­åŒ…æ‹¬çº¦
    270 ä¸‡å¼ æ¥è‡ª 32529 ä¸ª WSIs çš„å›¾åƒã€‚ç„¶åä»–ä»¬æå‡ºäº†ä¸€ä¸ªç»“åˆ CNN å’Œ Transformer çš„æ··åˆæ¡†æ¶ï¼Œä»¥æå–å±€éƒ¨ç»“æ„ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œå¹¶æå‡ºäº†
    TAE æ¨¡å—ä»¥è¿›ä¸€æ­¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚
- en: Hybrid Self-supervised Learning Approach
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ··åˆè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•
- en: Many studies have also presented hybrid self-supervised methods for pathological
    images. Abbet *et al.*Â [2020](#bib.bib2) proposed a combination of generative
    and contrastive self-supervised representation learning method for pathological
    images. They first applied colorization as a generative auxiliary task. Then,
    they constructed the contrastive loss using spatially neighboring patches as positive
    samples and distant patches as negative samples. Yang *et al.*Â [2021](#bib.bib200)
    also proposed a self-supervised representation method combining generative and
    contrastive approaches for pathological images. They first proposed a generative-based
    self-supervised task called cross-stain prediction, in which they defined two
    encoders and decoders to predict the E-channel and H-channel, respectively, and
    then they used the encoders trained in the previous task to perform further contrastive
    training.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šç ”ç©¶è¿˜æå‡ºäº†ç”¨äºç—…ç†å›¾åƒçš„æ··åˆè‡ªç›‘ç£æ–¹æ³•ã€‚Abbet *ç­‰äºº* [2020](#bib.bib2) æå‡ºäº†ç”¨äºç—…ç†å›¾åƒçš„ç”Ÿæˆå¯¹æ¯”è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹æ³•çš„ç»„åˆã€‚ä»–ä»¬é¦–å…ˆå°†é¢œè‰²åŒ–ä½œä¸ºç”Ÿæˆè¾…åŠ©ä»»åŠ¡ã€‚ç„¶åï¼Œä»–ä»¬ä½¿ç”¨ç©ºé—´ä¸Šç›¸é‚»çš„å›¾å—ä½œä¸ºæ­£æ ·æœ¬ï¼Œè€Œè¿œç¦»çš„å›¾å—ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œæ„å»ºå¯¹æ¯”æŸå¤±ã€‚Yang
    *ç­‰äºº* [2021](#bib.bib200) è¿˜æå‡ºäº†ä¸€ç§ç»“åˆç”Ÿæˆå’Œå¯¹æ¯”æ–¹æ³•çš„è‡ªç›‘ç£è¡¨ç¤ºæ–¹æ³•ç”¨äºç—…ç†å›¾åƒã€‚ä»–ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆçš„è‡ªç›‘ç£ä»»åŠ¡ï¼Œç§°ä¸ºäº¤å‰æŸ“è‰²é¢„æµ‹ï¼Œå…¶ä¸­å®šä¹‰äº†ä¸¤ä¸ªç¼–ç å™¨å’Œè§£ç å™¨åˆ†åˆ«é¢„æµ‹
    E é€šé“å’Œ H é€šé“ï¼Œç„¶åä»–ä»¬ä½¿ç”¨åœ¨å‰ä¸€ä¸ªä»»åŠ¡ä¸­è®­ç»ƒçš„ç¼–ç å™¨è¿›è¡Œè¿›ä¸€æ­¥çš„å¯¹æ¯”è®­ç»ƒã€‚
- en: 3.3.2 Study on Applications of Self-supervised Frameworks
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 è‡ªç›‘ç£æ¡†æ¶çš„åº”ç”¨ç ”ç©¶
- en: In addition to studies that aim to propose innovative self-supervised frameworks
    for pathological images, more studies have attempted to apply existing self-supervised
    learning methods to various pathological image analysis tasks. Chen *et al.*Â [2020](#bib.bib25)
    proposed an end-to-end multimodal fusion framework for histopathological images
    and genomic data for survival prognosis prediction, in which they used contrastive
    predictive coding (CPC) pre-trained self-supervised features for initialization
    of the network model. Ciga *et al.*Â [2022](#bib.bib36) showed through extensive
    experiments that using self-supervised pre-training methods can yield better features
    to improve performance on several downstream tasks. They found that the success
    of contrastive self-supervised pre-training methods depended heavily on the diversity
    of the unlabeled training set rather than the number of images. On the other hand,
    positive and negative samples that are visually significantly different facilitate
    contrastive self-supervised learning, while positive and negative sample that
    contain only minor differences but are generally similar (e.g., normal patches
    versus patches containing only a small percentage of tumor regions) are not conducive
    to contrastive learning. However, this is uncommon in natural images, so it is
    particularly important to design targeted self-supervised tasks for the characteristics
    of pathological images. Tellez *et al.*Â [2019](#bib.bib170) used the variational
    autoencoder [2013](#bib.bib87), contrastive learning [2016](#bib.bib112) and BiGAN
    [2016](#bib.bib50) for the compression of gigapixel pathological images and evaluated
    the performance on a synthetic dataset and two public histopathology datasets,
    respectively, achieving promising results. Stacke *et al.*Â [2021](#bib.bib161)
    investigated how SimCLR [2020](#bib.bib26) could be extended for pathological
    images to learn useful feature representations. They systematically compared the
    differences between ImageNet data and histopathology data and how this affected
    the goals of self-supervised learning, and pointed out the impact that designing
    for different self-supervised goals would have on the results. Chen *et al.*Â [2022](#bib.bib24)
    comprehensively compared the performance of ImageNet pre-trained features, SimCLR
    pre-trained features, and DINO [2021](#bib.bib19) pre-trained features in weakly
    supervised classification and fully supervised classification tasks for histopathological
    images. They found that the DINO-based knowledge distillation framework could
    better learn effective and interpretable features in pathological images.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ—¨åœ¨æå‡ºåˆ›æ–°çš„è‡ªç›‘ç£æ¡†æ¶ä»¥å¤„ç†ç—…ç†å›¾åƒçš„ç ”ç©¶ï¼Œæ›´å¤šçš„ç ”ç©¶å°è¯•å°†ç°æœ‰çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åº”ç”¨äºå„ç§ç—…ç†å›¾åƒåˆ†æä»»åŠ¡ã€‚Chen *ç­‰äºº* [2020](#bib.bib25)
    æå‡ºäº†ä¸€ä¸ªç”¨äºç”Ÿå­˜é¢„åé¢„æµ‹çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç»„ç»‡ç—…ç†å›¾åƒå’ŒåŸºå› ç»„æ•°æ®ï¼Œå…¶ä¸­ä½¿ç”¨äº†å¯¹æ¯”é¢„æµ‹ç¼–ç ï¼ˆCPCï¼‰é¢„è®­ç»ƒçš„è‡ªç›‘ç£ç‰¹å¾æ¥åˆå§‹åŒ–ç½‘ç»œæ¨¡å‹ã€‚Ciga
    *ç­‰äºº* [2022](#bib.bib36) é€šè¿‡å¤§é‡å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•å¯ä»¥äº§ç”Ÿæ›´å¥½çš„ç‰¹å¾ï¼Œä»è€Œæå‡å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚ä»–ä»¬å‘ç°ï¼Œå¯¹æ¯”è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæœªæ ‡è®°è®­ç»ƒé›†çš„å¤šæ ·æ€§ï¼Œè€Œä¸æ˜¯å›¾åƒçš„æ•°é‡ã€‚å¦ä¸€æ–¹é¢ï¼Œè§†è§‰ä¸Šæ˜¾è‘—ä¸åŒçš„æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬æœ‰åŠ©äºå¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ ï¼Œè€Œåªæœ‰å¾®å°å·®å¼‚ä½†æ€»ä½“ä¸Šç›¸ä¼¼çš„æ­£è´Ÿæ ·æœ¬ï¼ˆä¾‹å¦‚æ­£å¸¸çš„è¡¥ä¸ä¸ä»…åŒ…å«å°‘é‡è‚¿ç˜¤åŒºåŸŸçš„è¡¥ä¸ï¼‰åˆ™ä¸åˆ©äºå¯¹æ¯”å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™åœ¨è‡ªç„¶å›¾åƒä¸­å¹¶ä¸å¸¸è§ï¼Œå› æ­¤é’ˆå¯¹ç—…ç†å›¾åƒç‰¹å¾è®¾è®¡æœ‰é’ˆå¯¹æ€§çš„è‡ªç›‘ç£ä»»åŠ¡å°¤ä¸ºé‡è¦ã€‚Tellez
    *ç­‰äºº* [2019](#bib.bib170) ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ [2013](#bib.bib87)ã€å¯¹æ¯”å­¦ä¹  [2016](#bib.bib112)
    å’Œ BiGAN [2016](#bib.bib50) å¯¹ gigapixel ç—…ç†å›¾åƒè¿›è¡Œå‹ç¼©ï¼Œå¹¶åˆ†åˆ«åœ¨ä¸€ä¸ªåˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªå…¬å…±ç»„ç»‡ç—…ç†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ€§èƒ½ï¼Œå–å¾—äº†ä»¤äººæ»¡æ„çš„ç»“æœã€‚Stacke
    *ç­‰äºº* [2021](#bib.bib161) ç ”ç©¶äº†å¦‚ä½•å°† SimCLR [2020](#bib.bib26) æ‰©å±•åˆ°ç—…ç†å›¾åƒä¸Šï¼Œä»¥å­¦ä¹ æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤ºã€‚ä»–ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†
    ImageNet æ•°æ®å’Œç»„ç»‡ç—…ç†æ•°æ®ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥åŠè¿™äº›å·®å¼‚å¦‚ä½•å½±å“è‡ªç›‘ç£å­¦ä¹ çš„ç›®æ ‡ï¼Œå¹¶æŒ‡å‡ºäº†ä¸ºä¸åŒè‡ªç›‘ç£ç›®æ ‡è®¾è®¡å¯¹ç»“æœçš„å½±å“ã€‚Chen *ç­‰äºº* [2022](#bib.bib24)
    å¯¹ ImageNet é¢„è®­ç»ƒç‰¹å¾ã€SimCLR é¢„è®­ç»ƒç‰¹å¾å’Œ DINO [2021](#bib.bib19) é¢„è®­ç»ƒç‰¹å¾åœ¨ç—…ç†å›¾åƒçš„å¼±ç›‘ç£åˆ†ç±»å’Œå®Œå…¨ç›‘ç£åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒã€‚ä»–ä»¬å‘ç°åŸºäº
    DINO çš„çŸ¥è¯†è’¸é¦æ¡†æ¶èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ ç—…ç†å›¾åƒä¸­çš„æœ‰æ•ˆå’Œå¯è§£é‡Šçš„ç‰¹å¾ã€‚
- en: Saillard *et al.*Â [2021](#bib.bib142) and Dehaene *et al.*Â [2020](#bib.bib45)
    used the MoCo V2 [2020](#bib.bib27) self-supervised learning method to train pathological
    images and the experimental results showed that the results using the self-supervised
    pre-trained features were consistently better than those using features pre-trained
    on ImageNet under the same conditions. Lu *et al.*Â [2019](#bib.bib104), Zhao *et
    al.*Â [2020](#bib.bib206), and Li *et al.*Â [2021](#bib.bib97) used contrastive
    predictive coding (CPC) [2018](#bib.bib174), VAE-GAN [2016](#bib.bib93), and SimCLR
    [2020](#bib.bib26) self-supervised pre-trained features for weakly supervised
    WSI classification, respectively, and achieved the current state-of-the-art performance.
    Koohbanani *et al.*Â [2021](#bib.bib89) developed a semi-supervised learning framework
    facilitated by self-supervised learning with a multi-task learning approach for
    training, i.e., training with a small amount of labeled data as the main task
    and self-supervised tasks as auxiliary tasks. In their study, they also compared
    the effectiveness of various commonly used pathology-agnostic self-supervised
    auxiliary tasks (including rotation, flipping, auto-encoder, real/fake prediction,
    domain prediction, etc.) to facilitate semi-supervised learning. Srinidhi *et
    al.*Â [2022](#bib.bib160) also attempted to use self-supervised pre-trained features
    to enhance semi-supervised learning. They first proposed the resolution sequence
    prediction (RSP) self-supervised auxiliary task to pre-train the model through
    unlabeled data, and then they fine-tuned the model on the labeled data. After
    that, they used the trained model from the above two steps as the initial weights
    of the model for further semi-supervised training based on the teacher-student
    consistency framework.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Saillard *et al.* [2021](#bib.bib142) å’Œ Dehaene *et al.* [2020](#bib.bib45)
    ä½¿ç”¨äº† MoCo V2 [2020](#bib.bib27) è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒç—…ç†å›¾åƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç›¸åŒæ¡ä»¶ä¸‹ï¼Œä½¿ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒç‰¹å¾çš„ç»“æœå§‹ç»ˆä¼˜äºä½¿ç”¨åœ¨
    ImageNet ä¸Šé¢„è®­ç»ƒçš„ç‰¹å¾ã€‚Lu *et al.* [2019](#bib.bib104), Zhao *et al.* [2020](#bib.bib206),
    å’Œ Li *et al.* [2021](#bib.bib97) åˆ†åˆ«ä½¿ç”¨äº†å¯¹æ¯”é¢„æµ‹ç¼–ç ï¼ˆCPCï¼‰ [2018](#bib.bib174), VAE-GAN
    [2016](#bib.bib93), å’Œ SimCLR [2020](#bib.bib26) è‡ªç›‘ç£é¢„è®­ç»ƒç‰¹å¾è¿›è¡Œå¼±ç›‘ç£ WSI åˆ†ç±»ï¼Œå¹¶å–å¾—äº†å½“å‰çš„æœ€æ–°æ€§èƒ½ã€‚Koohbanani
    *et al.* [2021](#bib.bib89) å¼€å‘äº†ä¸€ä¸ªé€šè¿‡è‡ªç›‘ç£å­¦ä¹ å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ä¿ƒè¿›çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œå³ä»¥å°‘é‡æ ‡æ³¨æ•°æ®ä½œä¸ºä¸»è¦ä»»åŠ¡ï¼Œè‡ªç›‘ç£ä»»åŠ¡ä½œä¸ºè¾…åŠ©ä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚åœ¨ä»–ä»¬çš„ç ”ç©¶ä¸­ï¼Œä»–ä»¬è¿˜æ¯”è¾ƒäº†å„ç§å¸¸ç”¨çš„ç—…ç†æ— å…³è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡ï¼ˆåŒ…æ‹¬æ—‹è½¬ã€ç¿»è½¬ã€è‡ªç¼–ç å™¨ã€çœŸ/å‡é¢„æµ‹ã€é¢†åŸŸé¢„æµ‹ç­‰ï¼‰ä»¥ä¿ƒè¿›åŠç›‘ç£å­¦ä¹ ã€‚Srinidhi
    *et al.* [2022](#bib.bib160) ä¹Ÿå°è¯•ä½¿ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒç‰¹å¾æ¥å¢å¼ºåŠç›‘ç£å­¦ä¹ ã€‚ä»–ä»¬é¦–å…ˆæå‡ºäº†åˆ†è¾¨ç‡åºåˆ—é¢„æµ‹ï¼ˆRSPï¼‰è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡ï¼Œé€šè¿‡æœªæ ‡è®°çš„æ•°æ®é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨æ ‡è®°æ•°æ®ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä¹‹åï¼Œä»–ä»¬ä½¿ç”¨ä¸Šè¿°ä¸¤ä¸ªæ­¥éª¤å¾—åˆ°çš„è®­ç»ƒæ¨¡å‹ä½œä¸ºæ¨¡å‹çš„åˆå§‹æƒé‡ï¼Œè¿›è¡ŒåŸºäºæ•™å¸ˆ-å­¦ç”Ÿä¸€è‡´æ€§æ¡†æ¶çš„è¿›ä¸€æ­¥åŠç›‘ç£è®­ç»ƒã€‚
- en: In addition, self-supervised learning has been used for a variety of other pathological
    tasks, such as pathological image retrieval (Shi *et al.*Â [2018](#bib.bib149),
    Yang *et al.*Â [2020](#bib.bib201)), active learning (Zheng *et al.*Â [2019](#bib.bib207)),
    and molecular signature prediction (Ding *et al.*Â [2020](#bib.bib48), Fu *et al.*Â [2020](#bib.bib58),
    Kather *et al.*Â [2020](#bib.bib83)), etc.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè‡ªç›‘ç£å­¦ä¹ è¿˜è¢«ç”¨äºå„ç§å…¶ä»–ç—…ç†ä»»åŠ¡ï¼Œå¦‚ç—…ç†å›¾åƒæ£€ç´¢ï¼ˆShi *et al.* [2018](#bib.bib149), Yang *et al.*
    [2020](#bib.bib201)ï¼‰ï¼Œä¸»åŠ¨å­¦ä¹ ï¼ˆZheng *et al.* [2019](#bib.bib207)ï¼‰ï¼Œä»¥åŠåˆ†å­ç‰¹å¾é¢„æµ‹ï¼ˆDing *et
    al.* [2020](#bib.bib48), Fu *et al.* [2020](#bib.bib58), Kather *et al.* [2020](#bib.bib83)ï¼‰ç­‰ã€‚
- en: 'Table 5: List of literatures in the self-supervised learning section.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 5ï¼šè‡ªç›‘ç£å­¦ä¹ éƒ¨åˆ†æ–‡çŒ®åˆ—è¡¨ã€‚
- en: '| Reference | Approach | Disease Type | Staining | Dataset | Dataset Scale
    | Dataset Link | Self-supervised Method | Downstream Task | Downstream Performance
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| å‚è€ƒæ–‡çŒ® | æ–¹æ³• | ç–¾ç—…ç±»å‹ | æŸ“è‰² | æ•°æ®é›† | æ•°æ®é›†è§„æ¨¡ | æ•°æ®é›†é“¾æ¥ | è‡ªç›‘ç£æ–¹æ³• | ä¸‹æ¸¸ä»»åŠ¡ | ä¸‹æ¸¸æ€§èƒ½ |'
- en: '| Sahasrabudhe etÂ al. ([2020](#bib.bib141)) | Predictive | - | H&E | MoNuSeg
    database | 1,125,737 tiles | Kumar etÂ al. ([2017](#bib.bib91)) | Identification
    of the magnification levels for tiles | Nuclei segmentation | AJI: 0.5354, AHD:
    7.7502, Dice: 0.7477 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Sahasrabudhe ç­‰ ([2020](#bib.bib141)) | é¢„æµ‹ | - | H&E | MoNuSeg æ•°æ®åº“ | 1,125,737
    ä¸ªå›¾å— | Kumar ç­‰ ([2017](#bib.bib91)) | è¯†åˆ«å›¾å—çš„æ”¾å¤§çº§åˆ« | ç»†èƒæ ¸åˆ†å‰² | AJI: 0.5354, AHD: 7.7502,
    Dice: 0.7477 |'
- en: '| Srinidhi etÂ al. ([2022](#bib.bib160)) | Predictive | Breast Cancer, Colorectal
    Cancer | H&E | BreastPathQ dataset | 2579 patches | Martel etÂ al. ([2019](#bib.bib111))
    | Predicting the resolution sequences | Detection of tumor metastasis | TC: 0.876
    (10% labels) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Srinidhi ç­‰ ([2022](#bib.bib160)) | é¢„æµ‹ | ä¹³è…ºç™Œã€ç»“ç›´è‚ ç™Œ | H&E | BreastPathQ æ•°æ®é›†
    | 2579 ä¸ªè¡¥ä¸ | Martel ç­‰ ([2019](#bib.bib111)) | é¢„æµ‹åˆ†è¾¨ç‡åºåˆ— | è‚¿ç˜¤è½¬ç§»æ£€æµ‹ | TC: 0.876 (10%
    æ ‡ç­¾) |'
- en: '|  |  |  |  | Camelyon16 dataset | 399 WSIs | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    |  | Classification of tissue types | AUC: 0.855 (10% labels) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | Camelyon16 æ•°æ®é›† | 399 å¼ å…¨åˆ‡ç‰‡å›¾åƒ | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    |  | ç»„ç»‡ç±»å‹åˆ†ç±» | AUC: 0.855 (10% æ ‡ç­¾) |'
- en: '|  |  |  |  | Kather multiclass dataset | 100K patches | Kather *et al.*Â ([2019](#bib.bib84))
    |  | Quantification of tumor cellularity | Accuracy: 0.982 (10% labels) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | Kather å¤šç±»åˆ«æ•°æ®é›† | 100K è´´ç‰‡ | Kather *et al.* ([2019](#bib.bib84))
    |  | è‚¿ç˜¤ç»†èƒé‡åŒ– | å‡†ç¡®ç‡: 0.982 (10% æ ‡ç­¾) |'
- en: '| Koohbanani etÂ al. ([2021](#bib.bib89)) | Predictive | Breast Cancer | H&E
    | Camelyon16 dataset | 399 slides | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | Magnification prediction and solving magnification puzzles | Detection of tumor
    regions | AUC: 0.817 (1% labeled) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Koohbanani et al. ([2021](#bib.bib89)) | é¢„æµ‹æ¨¡å‹ | ä¹³è…ºç™Œ | H&E | Camelyon16 æ•°æ®é›†
    | 399 å¼ å¹»ç¯ç‰‡ | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | æ”¾å¤§é¢„æµ‹å’Œè§£å†³æ”¾å¤§éš¾é¢˜ | è‚¿ç˜¤åŒºåŸŸæ£€æµ‹ | AUC: 0.817 (1% æ ‡è®°) |'
- en: '|  |  | oral Squamous Cell Carcinoma |  | LNM-OSCC dataset | 217 slides | Inhouse
    |  | Prediction of metastases in the cervical lymph nodes | AUC: 0.806 (1% labeled)
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  |  | å£è…”é³çŠ¶ç»†èƒç™Œ |  | LNM-OSCC æ•°æ®é›† | 217 å¼ å¹»ç¯ç‰‡ | å†…éƒ¨æ•°æ® |  | é¢„æµ‹é¢ˆéƒ¨æ·‹å·´ç»“ä¸­çš„è½¬ç§» | AUC:
    0.806 (1% æ ‡è®°) |'
- en: '|  |  | Colorectal Cancer |  | Kather multiclass dataset | 100K patches | Kather
    *et al.*Â ([2019](#bib.bib84)) |  | Classification of tissue types | AUC: 0.903
    (1%labeled) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ç»“ç›´è‚ ç™Œ |  | Kather å¤šç±»åˆ«æ•°æ®é›† | 100K è´´ç‰‡ | Kather *et al.* ([2019](#bib.bib84))
    |  | ç»„ç»‡ç±»å‹åˆ†ç±» | AUC: 0.903 (1% æ ‡è®°) |'
- en: '| Muhammad etÂ al. ([2019](#bib.bib114)) | Generative | Cholangi-ocarcinoma
    | H&E | Intrahepatic cholangiocarcinoma (ICC) dataset | 246 patients | Inhouse
    | Deep clustering convolutional autoencoder | Subtyping of cholangiocarcinoma
    | CHI: 3863(5 clusters) and 4314 (clutsering weight = 0.2) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| Muhammad et al. ([2019](#bib.bib114)) | ç”Ÿæˆæ¨¡å‹ | èƒ†ç®¡ç™Œ | H&E | è‚å†…èƒ†ç®¡ç™Œ (ICC) æ•°æ®é›†
    | 246 åæ‚£è€… | å†…éƒ¨æ•°æ® | æ·±åº¦èšç±»å·ç§¯è‡ªç¼–ç å™¨ | èƒ†ç®¡ç™Œçš„äºšå‹åˆ’åˆ† | CHI: 3863ï¼ˆ5 ä¸ªç°‡ï¼‰å’Œ 4314ï¼ˆç°‡æƒé‡ = 0.2ï¼‰ |'
- en: '| Mahapatra etÂ al. ([2020](#bib.bib108)) | Generative | Breast Cancer | H&E
    | CAMELYON16 dataset | 100, 000 patches | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | Using pre-trained networks for semantic guidance | Stain normalization | Average
    AUC: 0.9320 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Mahapatra et al. ([2020](#bib.bib108)) | ç”Ÿæˆæ¨¡å‹ | ä¹³è…ºç™Œ | H&E | CAMELYON16 æ•°æ®é›†
    | 100,000 è´´ç‰‡ | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | ä½¿ç”¨é¢„è®­ç»ƒç½‘ç»œè¿›è¡Œè¯­ä¹‰å¼•å¯¼ | æŸ“è‰²æ ‡å‡†åŒ– | å¹³å‡ AUC: 0.9320 |'
- en: '|  |  | Breast Cancer |  | CAMELYON17 dataset | 100, 000 patches | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/),
    inhouse |  |  |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ä¹³è…ºç™Œ |  | CAMELYON17 æ•°æ®é›† | 100,000 è´´ç‰‡ | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)ï¼Œå†…éƒ¨æ•°æ®
    |  |  |  |'
- en: '| Quiros etÂ al. ([2019](#bib.bib131)) | Generative | Colorectal Cancer | H&E
    | National Center for Tumor diseases (NCT) dataset | 86 slides | [https://zenodo.org/record/1214456#.Yvzd-nZBxhE](https://zenodo.org/record/1214456#.Yvzd-nZBxhE)
    | Using Generative Adversarial Networks (GANs) to capture key tissue features
    and structure information | Count of cancer, lymphocytes, or stromal cells | FID:
    16.65 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Quiros et al. ([2019](#bib.bib131)) | ç”Ÿæˆæ¨¡å‹ | ç»“ç›´è‚ ç™Œ | H&E | å›½å®¶è‚¿ç˜¤ç–¾ç—…ä¸­å¿ƒ (NCT)
    æ•°æ®é›† | 86 å¼ å¹»ç¯ç‰‡ | [https://zenodo.org/record/1214456#.Yvzd-nZBxhE](https://zenodo.org/record/1214456#.Yvzd-nZBxhE)
    | ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GANs) æ•æ‰å…³é”®ç»„ç»‡ç‰¹å¾å’Œç»“æ„ä¿¡æ¯ | ç™Œç—‡ã€æ·‹å·´ç»†èƒæˆ–åŸºè´¨ç»†èƒçš„è®¡æ•° | FID: 16.65 |'
- en: '|  |  | Breast Cancer |  | Netherlands Cancer Institute (NKI) dataset and Vancouver
    General Hospital (VGH) dataset | 576 tissue micro-arrays (TMAs) | Beck etÂ al.
    ([2011](#bib.bib8)) |  |  | FID: 32.05 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ä¹³è…ºç™Œ |  | è·å…°ç™Œç—‡ç ”ç©¶æ‰€ (NKI) æ•°æ®é›† å’Œ æ¸©å“¥åç»¼åˆåŒ»é™¢ (VGH) æ•°æ®é›† | 576 ä¸ªç»„ç»‡å¾®é˜µåˆ— (TMAs)
    | Beck et al. ([2011](#bib.bib8)) |  |  | FID: 32.05 |'
- en: '| Quiros etÂ al. ([2021](#bib.bib130)) | Generative | Breast Cancer | H&E |
    Netherlands Cancer Institute (NKI, Netherlands) and Vancouver General Hospital
    (VGH, Canada) cohorts | Total of 576 patients | Beck etÂ al. ([2011](#bib.bib8))
    | Presenting an adversarial learning model to extract feature representations
    of cancer tissue | Classifying tissue types and predicting the presence of tumor
    in Whole Slide Images (WSIs) using multiple instance learning (MIL) | AUC: 0.97
    and Accuracy: 0.85; AUC: 0.98 and Accuracy: 0.94 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Quiros et al. ([2021](#bib.bib130)) | ç”Ÿæˆæ¨¡å‹ | ä¹³è…ºç™Œ | H&E | è·å…°ç™Œç—‡ç ”ç©¶æ‰€ (NKI, è·å…°)
    å’Œ æ¸©å“¥åç»¼åˆåŒ»é™¢ (VGH, åŠ æ‹¿å¤§) é˜Ÿåˆ— | æ€»è®¡ 576 åæ‚£è€… | Beck et al. ([2011](#bib.bib8)) | æå‡ºå¯¹æŠ—å­¦ä¹ æ¨¡å‹ä»¥æå–ç™Œç—‡ç»„ç»‡çš„ç‰¹å¾è¡¨ç¤º
    | ä½¿ç”¨å¤šå®ä¾‹å­¦ä¹  (MIL) åˆ†ç±»ç»„ç»‡ç±»å‹å’Œé¢„æµ‹å…¨åˆ‡ç‰‡å›¾åƒ (WSIs) ä¸­çš„è‚¿ç˜¤å­˜åœ¨ | AUC: 0.97 å’Œ å‡†ç¡®ç‡: 0.85; AUC: 0.98
    å’Œ å‡†ç¡®ç‡: 0.94 |'
- en: '|  |  | Colon cancer |  | National Center for Tumor diseases (NCT, Germany)
    dataset | 100K tissue tiles | [https://zenodo.org/record/1214456#.Yvzd-nZBxhE](https://zenodo.org/record/1214456#.Yvzd-nZBxhE)
    |  |  |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ç»“è‚ ç™Œ |  | å›½å®¶è‚¿ç˜¤ä¸­å¿ƒ (NCT, å¾·å›½) æ•°æ®é›† | 100K ç»„ç»‡å— | [https://zenodo.org/record/1214456#.Yvzd-nZBxhE](https://zenodo.org/record/1214456#.Yvzd-nZBxhE)
    |  |  |  |'
- en: '|  |  | Lung Cancer |  | TCGA LUAD, LUSC dataset | 1184 patients | [http://portal.gdc.cancer.gov](http://portal.gdc.cancer.gov)
    |  |  |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  |  | è‚ºç™Œ |  | TCGA LUAD, LUSC æ•°æ®é›† | 1184 åæ‚£è€… | [http://portal.gdc.cancer.gov](http://portal.gdc.cancer.gov)
    |  |  |  |'
- en: '| Boyd etÂ al. ([2021](#bib.bib15)) | Generative | Breast Cancer | H&E | CAMELYON17
    dataset | 500 slides | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | Visual field expansion | Binary classification of tiles into metastatic and
    non-metastatic classes | Accuracy: 0.8569 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Boyd ç­‰äºº ([2021](#bib.bib15)) | ç”Ÿæˆå‹ | ä¹³è…ºç™Œ | H&E | CAMELYON17 æ•°æ®é›† | 500 å¼ åˆ‡ç‰‡
    | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | è§†åœºæ‰©å±• | å°†åˆ‡ç‰‡äºŒåˆ†ç±»ä¸ºè½¬ç§»æ€§å’Œéè½¬ç§»æ€§ç±»åˆ« | å‡†ç¡®ç‡: 0.8569 |'
- en: '|  |  | Colorectal Cancer |  | CRC benchmark dataset | 100K image tiles | [https://doi.org/10.5281/zenodo.1214456](https://doi.org/10.5281/zenodo.1214456)
    |  | Classification of tiles into the 9 tissue types | Accuracy: 0.8511 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ç»“ç›´è‚ ç™Œ |  | CRC åŸºå‡†æ•°æ®é›† | 100K å›¾åƒå— | [https://doi.org/10.5281/zenodo.1214456](https://doi.org/10.5281/zenodo.1214456)
    |  | åˆ‡ç‰‡åˆ†ç±»ä¸º 9 ç§ç»„ç»‡ç±»å‹ | å‡†ç¡®ç‡: 0.8511 |'
- en: '| Koohbanani etÂ al. ([2021](#bib.bib89)) | Generative | Breast Cancer | H&E
    | Camelyon16 dataset | 399 slides | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | Hematoxylin channel prediction auxiliary task | Detection of tumor regions |
    AUC: 0.817 (1% labeled) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Koohbanani ç­‰äºº ([2021](#bib.bib89)) | ç”Ÿæˆå‹ | ä¹³è…ºç™Œ | H&E | Camelyon16 æ•°æ®é›† | 399
    å¼ åˆ‡ç‰‡ | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | è‹æœ¨ç²¾é€šé“é¢„æµ‹è¾…åŠ©ä»»åŠ¡ | è‚¿ç˜¤åŒºåŸŸæ£€æµ‹ | AUC: 0.817 (1% æ ‡è®°) |'
- en: '|  |  | Oral Squamous Cell Carcinoma |  | LNM-OSCC dataset | 217 slides | Inhouse
    |  | Prediction of metastases in the cervical lymph nodes | AUC: 0.806 (1% labeled)
    |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |  | å£è…”é³çŠ¶ç»†èƒç™Œ |  | LNM-OSCC æ•°æ®é›† | 217 å¼ åˆ‡ç‰‡ | å†…éƒ¨æ”¶é›† |  | é¢ˆéƒ¨æ·‹å·´ç»“è½¬ç§»é¢„æµ‹ | AUC: 0.806
    (1% æ ‡è®°) |'
- en: '|  |  | Colorectal Cancer |  | Kather multiclass dataset | 100K patches | Kather
    *et al.*Â ([2019](#bib.bib84)) |  | Classification of tissue types | AUC: 0.903
    (1%labeled) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ç»“ç›´è‚ ç™Œ |  | Kather å¤šç±»åˆ«æ•°æ®é›† | 100K è¡¥ä¸ | Kather *ç­‰äºº* ([2019](#bib.bib84))
    |  | ç»„ç»‡ç±»å‹åˆ†ç±» | AUC: 0.903 (1% æ ‡è®°) |'
- en: '| Hou etÂ al. ([2019](#bib.bib75)) | Generative | - | H&E | Self-collected lymphocyte
    classification dataset | 1785 images | Inhouse | Sparse Convolutional Autoencoder
    (CAE) | Nucleus detection | Nucleus Classification: Lymphocyte Classification
    AUC 0.7856 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Hou ç­‰äºº ([2019](#bib.bib75)) | ç”Ÿæˆå‹ | - | H&E | è‡ªæ”¶é›†æ·‹å·´ç»†èƒåˆ†ç±»æ•°æ®é›† | 1785 å¼ å›¾åƒ | å†…éƒ¨æ”¶é›†
    | ç¨€ç–å·ç§¯è‡ªç¼–ç å™¨ï¼ˆCAEï¼‰ | æ ¸æ£€æµ‹ | æ ¸åˆ†ç±»ï¼šæ·‹å·´ç»†èƒåˆ†ç±» AUC 0.7856 |'
- en: '|  |  |  |  | Nuclear shape and attribute classification dataset | 2000 images
    | Murthy etÂ al. ([2017](#bib.bib115)) |  |  | Nuclear Attribute &Shape AUC 0.8788
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | æ ¸å½¢çŠ¶å’Œå±æ€§åˆ†ç±»æ•°æ®é›† | 2000 å¼ å›¾åƒ | Murthy ç­‰äºº ([2017](#bib.bib115)) |  |  |
    æ ¸å±æ€§ä¸å½¢çŠ¶ AUC 0.8788 |'
- en: '|  |  |  |  | CRCHistoPhenotypes nucleus detection dataset | 100 images | Sirinukunwattana
    etÂ al. ([2016](#bib.bib155)) |  |  | Nucleus detection: F-measure: 0.8345 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | CRCHistoPhenotypes æ ¸æ£€æµ‹æ•°æ®é›† | 100 å¼ å›¾åƒ | Sirinukunwattana ç­‰äºº ([2016](#bib.bib155))
    |  |  | æ ¸æ£€æµ‹ï¼šF-measure: 0.8345 |'
- en: '|  |  |  |  | MICCAI 2015 nucleus segmentation challenge dataset | 763 images
    | https://wiki.cancerimagingarchive.net/ pages/viewpage.action?pageId=20644646
    |  |  | Lymphocyte classification: AUC 0.7856 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | MICCAI 2015 æ ¸åˆ†å‰²æŒ‘æˆ˜æ•°æ®é›† | 763 å¼ å›¾åƒ | [https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=20644646](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=20644646)
    |  |  | æ·‹å·´ç»†èƒåˆ†ç±»ï¼šAUC 0.7856 |'
- en: '|  |  |  |  | TCGA lung cancer dataset | 0.5 million images | [https://cancergenome.nih.gov/](https://cancergenome.nih.gov/)
    |  |  | Nucleus segmentation: DICE: 0.8362 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | TCGA è‚ºç™Œæ•°æ®é›† | 50 ä¸‡å¼ å›¾åƒ | [https://cancergenome.nih.gov/](https://cancergenome.nih.gov/)
    |  |  | æ ¸åˆ†å‰²ï¼šDICE: 0.8362 |'
- en: '| Xie, Chen, Li andÂ Zheng ([2020](#bib.bib191)) | Contrastive | - | H&E | MoNuSeg
    dataset | 44 images | Naylor etÂ al. ([2018](#bib.bib119)) | Scale-wise triplet
    learning and count ranking | Nuclei segmentation | AJI: 0.7063 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Xieã€Chenã€Li å’Œ Zheng ([2020](#bib.bib191)) | å¯¹æ¯”å‹ | - | H&E | MoNuSeg æ•°æ®é›† |
    44 å¼ å›¾åƒ | Naylor ç­‰äºº ([2018](#bib.bib119)) | å°ºåº¦ä¸‰é‡å­¦ä¹ ä¸è®¡æ•°æ’åº | æ ¸åˆ†å‰² | AJI: 0.7063 |'
- en: '| Chhipa etÂ al. ([2022](#bib.bib32)) | Contrastive | Breast Cancer | H&E |
    BreakHis dataset | 7909 images | Spanhol etÂ al. ([2015](#bib.bib157)) | Magnification
    prior contrastive similarity | Classifying histopathological images | Mean Accuracy:
    0.9233 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| Chhipa ç­‰äºº ([2022](#bib.bib32)) | å¯¹æ¯”å‹ | ä¹³è…ºç™Œ | H&E | BreakHis æ•°æ®é›† | 7909 å¼ å›¾åƒ
    | Spanhol ç­‰äºº ([2015](#bib.bib157)) | æ”¾å¤§ä¼˜å…ˆå¯¹æ¯”ç›¸ä¼¼æ€§ | åˆ†ç±»ç»„ç»‡ç—…ç†å›¾åƒ | å¹³å‡å‡†ç¡®ç‡: 0.9233 |'
- en: '| Xu etÂ al. ([2020](#bib.bib193)) | Contrastive | Breast Cancer | H&E | MICCAI
    2015 Gland Segmentation Challenge (GLaS) dataset | 165 images | Sirinukunwattana
    etÂ al. ([2017](#bib.bib154)) | Deformation representation learning | Gland segmentation
    | F1-score 0.900, Accuracy 0.8548 (10% labeled) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Xu ç­‰äºº ([2020](#bib.bib193)) | å¯¹æ¯” | ä¹³è…ºç™Œ | H&E | MICCAI 2015 è…ºä½“åˆ†å‰²æŒ‘æˆ˜èµ› (GLaS)
    æ•°æ®é›† | 165 å¼ å›¾åƒ | Sirinukunwattana ç­‰äºº ([2017](#bib.bib154)) | å˜å½¢è¡¨ç¤ºå­¦ä¹  | è…ºä½“åˆ†å‰² | F1-score
    0.900, å‡†ç¡®ç‡ 0.8548 (10% æ ‡æ³¨) |'
- en: '|  |  | Colon Cancer |  | Patch Camelyon (PCam) image classification dataset
    | 327,680 patches | Veeling etÂ al. ([2018](#bib.bib176)) |  | Semi-supervised
    classification |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ç»“è‚ ç™Œ |  | Patch Camelyon (PCam) å›¾åƒåˆ†ç±»æ•°æ®é›† | 327,680 ä¸ªè¡¥ä¸ | Veeling ç­‰äºº ([2018](#bib.bib176))
    |  | åŠç›‘ç£åˆ†ç±» |  |'
- en: '| Wang etÂ al. ([2021](#bib.bib183)) | Contrastive | Liver, Renal, Colorectal,
    Prostatic, Pancreatic, and Cholangio Breast Cancers | H&E | Multiple histopathological
    image datasets including MHIST, NCT-CRC-HE, PatchCamelyon dataset | 2.7 million
    images | [https://github.com/Xiyue-Wang/TransPath](https://github.com/Xiyue-Wang/TransPath)
    | Contrastive learning like BYOL (Bootstrap your own latent: a new approach to
    self-supervised learning) | Histopathological image classification tasks | F1-score:
    0.8993, 0.9582, 0.8983 on MHIST, NCT-CRC-HE, PatchCamelyon dataset |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Wang ç­‰äºº ([2021](#bib.bib183)) | å¯¹æ¯” | è‚ç™Œã€è‚¾ç™Œã€ç»“ç›´è‚ ç™Œã€å‰åˆ—è…ºç™Œã€èƒ°è…ºç™Œå’Œèƒ†ç®¡ç™Œ | H&E | åŒ…æ‹¬ MHISTã€NCT-CRC-HEã€PatchCamelyon
    æ•°æ®é›†åœ¨å†…çš„å¤šä¸ªç»„ç»‡ç—…ç†å›¾åƒæ•°æ®é›† | 270 ä¸‡å¼ å›¾åƒ | [https://github.com/Xiyue-Wang/TransPath](https://github.com/Xiyue-Wang/TransPath)
    | å¯¹æ¯”å­¦ä¹ ï¼Œå¦‚ BYOLï¼ˆè‡ªç›‘ç£å­¦ä¹ çš„æ–°æ–¹æ³•ï¼‰ | ç»„ç»‡ç—…ç†å›¾åƒåˆ†ç±»ä»»åŠ¡ | F1-score: 0.8993ã€0.9582ã€0.8983ï¼ˆåœ¨ MHISTã€NCT-CRC-HEã€PatchCamelyon
    æ•°æ®é›†ä¸Šï¼‰ |'
- en: '| Abbet etÂ al. ([2020](#bib.bib2)) | Generative + Contrastive | Colorectal
    Cancer | H&E | Clinicopathological dataset | 660 WSIs | Inhouse | Colorization,
    Image reconstrucation and Contrastive learning | Survival analysis | C-Index:
    0.6943 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Abbet ç­‰äºº ([2020](#bib.bib2)) | ç”Ÿæˆ + å¯¹æ¯” | ç»“ç›´è‚ ç™Œ | H&E | ä¸´åºŠç—…ç†æ•°æ®é›† | 660 ä¸ªWSI
    | å†…éƒ¨ | ç€è‰²ã€å›¾åƒé‡å»ºå’Œå¯¹æ¯”å­¦ä¹  | ç”Ÿå­˜åˆ†æ | C-Index: 0.6943 |'
- en: '| Yang etÂ al. ([2021](#bib.bib200)) | Generative + Contrastive | Colorectal
    Cancer | H&E | NCTCRC-HE-100K dataset | 100K images | [https://zenodo.org/record/1214456#.Yvzd-nZBxhE](https://zenodo.org/record/1214456#.Yvzd-nZBxhE)
    | Cross-stain prediction, Contrastive training | Nine-class classification of
    histopathological images | Accuracy of eight-class classification with only 1,000
    labeled data: 0.915 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Yang ç­‰äºº ([2021](#bib.bib200)) | ç”Ÿæˆ + å¯¹æ¯” | ç»“ç›´è‚ ç™Œ | H&E | NCTCRC-HE-100K æ•°æ®é›†
    | 100K å¼ å›¾åƒ | [https://zenodo.org/record/1214456#.Yvzd-nZBxhE](https://zenodo.org/record/1214456#.Yvzd-nZBxhE)
    | è·¨æŸ“è‰²é¢„æµ‹ã€å¯¹æ¯”è®­ç»ƒ | ä¹ç±»ç»„ç»‡ç—…ç†å›¾åƒåˆ†ç±» | ä»…ç”¨1,000ä¸ªæ ‡æ³¨æ•°æ®çš„å…«ç±»åˆ†ç±»å‡†ç¡®ç‡: 0.915 |'
- en: '| Chen, Lu andÂ Mahmood ([2020](#bib.bib25)) | Application | Glioma and Cell
    Carcinoma | H&E | The Cancer Genome Atlas (TCGA) dataset | 1505 images | [http://portal.gdc.cancer.gov](http://portal.gdc.cancer.gov)
    | Contrastive predictive coding (CPC) | Survival prognosis prediction | C-Index:
    0.826 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Chen, Lu å’Œ Mahmood ([2020](#bib.bib25)) | åº”ç”¨ | ç¥ç»èƒ¶è´¨ç˜¤å’Œç»†èƒç™Œ | H&E | ç™Œç—‡åŸºå› ç»„å›¾è°±
    (TCGA) æ•°æ®é›† | 1505 å¼ å›¾åƒ | [http://portal.gdc.cancer.gov](http://portal.gdc.cancer.gov)
    | å¯¹æ¯”é¢„æµ‹ç¼–ç  (CPC) | ç”Ÿå­˜é¢„åé¢„æµ‹ | C-Index: 0.826 |'
- en: '| Ciga etÂ al. ([2022](#bib.bib36)) | Application | Multiple Types | H&E | Out
    of the total 57 datasets from various institutions | A large number of images
    | [https://github.com/ozanciga/self-supervised-histopathology](https://github.com/ozanciga/self-supervised-histopathology)
    | Contrastive learning | Classification, Regression, and Segmentation | Multiple
    results |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| Ciga ç­‰äºº ([2022](#bib.bib36)) | åº”ç”¨ | å¤šç§ç±»å‹ | H&E | æ¥è‡ªä¸åŒæœºæ„çš„æ€»å…±57ä¸ªæ•°æ®é›† | å¤§é‡å›¾åƒ |
    [https://github.com/ozanciga/self-supervised-histopathology](https://github.com/ozanciga/self-supervised-histopathology)
    | å¯¹æ¯”å­¦ä¹  | åˆ†ç±»ã€å›å½’å’Œåˆ†å‰² | å¤šé¡¹ç»“æœ |'
- en: '| Tellez etÂ al. ([2019](#bib.bib170)) | Application | Breast Cancer | H&E |
    Camelyon16 dataset | 400 WSIs | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | Variational autoencoder, Contrastive learning and BiGAN | Predicting the presence
    of metastasis | AUC: 0.725 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Tellez ç­‰äºº ([2019](#bib.bib170)) | åº”ç”¨ | ä¹³è…ºç™Œ | H&E | Camelyon16 æ•°æ®é›† | 400 ä¸ªWSI
    | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | å˜åˆ†è‡ªç¼–ç å™¨ã€å¯¹æ¯”å­¦ä¹ å’Œ BiGAN | é¢„æµ‹è½¬ç§»çš„å­˜åœ¨ | AUC: 0.725 |'
- en: '|  |  |  |  | TUPAC16 dataset | 492 WSIs | Veta etÂ al. ([2019](#bib.bib178))
    |  | Predicting tumor proliferation speed | Spearman correlation: 0.522 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | TUPAC16 æ•°æ®é›† | 492 ä¸ªWSI | Veta ç­‰äºº ([2019](#bib.bib178)) |  | é¢„æµ‹è‚¿ç˜¤å¢æ®–é€Ÿåº¦
    | Spearman ç›¸å…³ç³»æ•°: 0.522 |'
- en: '| Stacke etÂ al. ([2021](#bib.bib161)) | Application | Multiple Types | H&E
    | Camelyon16 dataset | 400 slides | [https://github.com/k-stacke/ssl-pathology](https://github.com/k-stacke/ssl-pathology)
    | Contrastive learning | Binary tumor classification | Multiple results |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Stacke ç­‰äºº ([2021](#bib.bib161)) | åº”ç”¨ | å¤šç§ç±»å‹ | H&E | Camelyon16 æ•°æ®é›† | 400
    å¼ å¹»ç¯ç‰‡ | [https://github.com/k-stacke/ssl-pathology](https://github.com/k-stacke/ssl-pathology)
    | å¯¹æ¯”å­¦ä¹  | äºŒå…ƒè‚¿ç˜¤åˆ†ç±» | å¤šé‡ç»“æœ |'
- en: '|  |  |  |  | AIDA-LNSK dataset | 96 slides |  |  |  |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | AIDA-LNSK æ•°æ®é›† | 96 å¼ å¹»ç¯ç‰‡ |  |  |  |  |'
- en: '|  |  |  |  | Multidata (samples from 60 publicly available datasets) | A large
    number of images |  |  |  |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | Multidataï¼ˆæ¥è‡ª 60 ä¸ªå…¬å¼€æ•°æ®é›†çš„æ ·æœ¬ï¼‰ | å¤§é‡å›¾åƒ |  |  |  |  |'
- en: '| Chen andÂ Krishnan ([2022](#bib.bib24)) | Application | Colorectal Cancer
    | H&E | CRC-100K dataset | 100K images | Kather etÂ al. ([2016](#bib.bib86)) |
    Contrastive learning | Weakly-supervised cancer subtyping | AUC: 0.886 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Chen å’Œ Krishnan ([2022](#bib.bib24)) | åº”ç”¨ | ç»“ç›´è‚ ç™Œ | H&E | CRC-100K æ•°æ®é›† | 10
    ä¸‡å¼ å›¾åƒ | Kather ç­‰äºº ([2016](#bib.bib86)) | å¯¹æ¯”å­¦ä¹  | å¼±ç›‘ç£ç™Œç—‡äºšå‹åˆ†ç±» | AUC: 0.886 |'
- en: '|  |  | Breast Cancer |  | BreastPathQ dataset | 2766 patches | Petrick etÂ al.
    ([2021](#bib.bib126)) |  | Patch-level tissue phenotyping | AUC: 0.987 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ä¹³è…ºç™Œ |  | BreastPathQ æ•°æ®é›† | 2766 ä¸ªè¡¥ä¸ | Petrick ç­‰äºº ([2021](#bib.bib126))
    |  | è¡¥ä¸çº§ç»„ç»‡è¡¨å‹åˆ†æ | AUC: 0.987 |'
- en: '| Saillard etÂ al. ([2021](#bib.bib142)) | Application | Colorectal Cancer |
    H&E | TCGA-CRC dataset | 555 patients | [http://portal.gdc.cancer.gov](http://portal.gdc.cancer.gov)
    | Contrastive learning | Microsatellite instability | AUC: 0.92 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Saillard ç­‰äºº ([2021](#bib.bib142)) | åº”ç”¨ | ç»“ç›´è‚ ç™Œ | H&E | TCGA-CRC æ•°æ®é›† | 555
    åæ‚£è€… | [http://portal.gdc.cancer.gov](http://portal.gdc.cancer.gov) | å¯¹æ¯”å­¦ä¹  | å¾®å«æ˜Ÿä¸ç¨³å®šæ€§
    | AUC: 0.92 |'
- en: '|  |  | Gastric Cancer |  | TCGA-Gastric dataset | 375 patients |  |  |  |
    AUC: 0.83 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |  | èƒƒç™Œ |  | TCGA-èƒƒç™Œæ•°æ®é›† | 375 åæ‚£è€… |  |  |  | AUC: 0.83 |'
- en: '| Dehaene etÂ al. ([2020](#bib.bib45)) | Application | Colorectal Cancer | H&E
    | Camelyon16 dataset | 400 slides | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | Contrastive learning | Predicting lymph node metastasis in Breast Cancer | AUC:
    0.987 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Dehaene ç­‰äºº ([2020](#bib.bib45)) | åº”ç”¨ | ç»“ç›´è‚ ç™Œ | H&E | Camelyon16 æ•°æ®é›† | 400
    å¼ å¹»ç¯ç‰‡ | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | å¯¹æ¯”å­¦ä¹  | é¢„æµ‹ä¹³è…ºç™Œæ·‹å·´ç»“è½¬ç§» | AUC: 0.987 |'
- en: '|  |  | Breast Cancer |  | TCGA-COAD dataset | 461 slides | Guinney etÂ al.
    ([2015](#bib.bib68)) |  | Colorectal Cancer subtyping | AUC: 0.882 (CMS1) and
    AUC: 0.829 (CMS3) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ä¹³è…ºç™Œ |  | TCGA-COAD æ•°æ®é›† | 461 å¼ å¹»ç¯ç‰‡ | Guinney ç­‰äºº ([2015](#bib.bib68))
    |  | ç»“ç›´è‚ ç™Œäºšå‹åˆ†ç±» | AUC: 0.882 (CMS1) å’Œ AUC: 0.829 (CMS3) |'
- en: '| Lu etÂ al. ([2019](#bib.bib104)) | Application | Breast Cancer | H&E | BACH
    dataset | 400 cases | Aresta etÂ al. ([2019](#bib.bib5)) | Contrastive predictive
    coding (CPC) | classification and localization of clinically relevant histopathological
    classes | Accuracy: 0.95 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Lu ç­‰äºº ([2019](#bib.bib104)) | åº”ç”¨ | ä¹³è…ºç™Œ | H&E | BACH æ•°æ®é›† | 400 ä¾‹ | Aresta
    ç­‰äºº ([2019](#bib.bib5)) | å¯¹æ¯”é¢„æµ‹ç¼–ç  (CPC) | ä¸´åºŠç›¸å…³ç»„ç»‡ç—…ç†å­¦ç±»åˆ«çš„åˆ†ç±»å’Œå®šä½ | å‡†ç¡®ç‡: 0.95 |'
- en: '| Zhao etÂ al. ([2020](#bib.bib206)) | Application | Colon Adenocarcinoma |
    H&E | The Cancer Genome Atlas (TCGA) dataset | 425 patients | [http://portal.gdc.cancer.gov](http://portal.gdc.cancer.gov)
    | Variational Auto Encoder and Generative Adversial Network (VAE-GAN) | Predicting
    lymph node metastasis | Accuracy: 0.6761 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Zhao ç­‰äºº ([2020](#bib.bib206)) | åº”ç”¨ | ç»“è‚ è…ºç™Œ | H&E | ç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰æ•°æ®é›† | 425
    åæ‚£è€… | [http://portal.gdc.cancer.gov](http://portal.gdc.cancer.gov) | å˜åˆ†è‡ªç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œ
    (VAE-GAN) | é¢„æµ‹æ·‹å·´ç»“è½¬ç§» | å‡†ç¡®ç‡: 0.6761 |'
- en: '| Li, Li andÂ Eliceiri ([2021](#bib.bib97)) | Application | Breast Cancer |
    H&E | Camelyon16 dataset | 400 cases | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | Contrastive learning | Detection of lymph node metastases | Accuracy: 0.8992
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Li, Li å’Œ Eliceiri ([2021](#bib.bib97)) | åº”ç”¨ | ä¹³è…ºç™Œ | H&E | Camelyon16 æ•°æ®é›†
    | 400 ä¾‹ | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | å¯¹æ¯”å­¦ä¹  | ä¹³è…ºç™Œæ·‹å·´ç»“è½¬ç§»æ£€æµ‹ | å‡†ç¡®ç‡: 0.8992 |'
- en: '|  |  | Lung Cancer |  | TCGA lung cancer dataset | 1054 cases | [https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga](https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga)
    |  | Diagnosis of lung cancer subtypes | Accuracy: 0.9571 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |  | è‚ºç™Œ |  | TCGA è‚ºç™Œæ•°æ®é›† | 1054 ä¾‹ | [https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga](https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga)
    |  | è‚ºç™Œäºšå‹è¯Šæ–­ | å‡†ç¡®ç‡: 0.9571 |'
- en: '| Koohbanani etÂ al. ([2021](#bib.bib89)) | Application | Breast Cancer | H&E
    | Camelyon16 dataset | 399 slides | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | Magnification prediction, JigMag prediction and Hematoxylin channel prediction
    | Detection of tumor regions | AUC: 0.817 (1% labeled) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Koohbanani ç­‰äºº ([2021](#bib.bib89)) | åº”ç”¨ | ä¹³è…ºç™Œ | H&E | Camelyon16 æ•°æ®é›† | 399
    ç»ç‰‡ | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    | æ”¾å¤§å€ç‡é¢„æµ‹, JigMag é¢„æµ‹å’Œè‹æœ¨ç²¾é€šé“é¢„æµ‹ | è‚¿ç˜¤åŒºåŸŸæ£€æµ‹ | AUC: 0.817 (1% æ ‡æ³¨) |'
- en: '|  |  | Oral Squamous Cell Carcinoma |  | LNM-OSCC dataset | 217 slides | Inhouse
    |  | Prediction of metastases in the cervical lymph nodes | AUC: 0.806 (1% labeled)
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  |  | å£è…”é³çŠ¶ç»†èƒç™Œ |  | LNM-OSCC æ•°æ®é›† | 217 ç»ç‰‡ | å†…éƒ¨æ•°æ® |  | é¢ˆéƒ¨æ·‹å·´ç»“è½¬ç§»é¢„æµ‹ | AUC: 0.806
    (1% æ ‡æ³¨) |'
- en: '|  |  | Colorectal Cancer |  | Kather multiclass dataset | 100K patches | Kather
    *et al.*Â ([2019](#bib.bib84)) |  | Classification of tissue types | AUC: 0.903
    (1%labeled) |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ç»“ç›´è‚ ç™Œ |  | Kather å¤šç±»åˆ«æ•°æ®é›† | 100K ç‰‡æ®µ | Kather *ç­‰äºº* ([2019](#bib.bib84))
    |  | ç»„ç»‡ç±»å‹åˆ†ç±» | AUC: 0.903 (1% æ ‡æ³¨) |'
- en: '| Srinidhi etÂ al. ([2022](#bib.bib160)) | Application | Breast Cancer, Colorectal
    Cancer | H&E | BreastPathQ dataset | 2579 patches | Martel etÂ al. ([2019](#bib.bib111))
    | Resolution sequence prediction | Detection of tumor metastasis | TC: 0.876 (10%
    labels) |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Srinidhi ç­‰äºº ([2022](#bib.bib160)) | åº”ç”¨ | ä¹³è…ºç™Œ, ç»“ç›´è‚ ç™Œ | H&E | BreastPathQ æ•°æ®é›†
    | 2579 ç‰‡æ®µ | Martel ç­‰äºº ([2019](#bib.bib111)) | åˆ†è¾¨ç‡åºåˆ—é¢„æµ‹ | è‚¿ç˜¤è½¬ç§»æ£€æµ‹ | TC: 0.876 (10%
    æ ‡æ³¨) |'
- en: '|  |  |  |  | Camelyon16 dataset | 399 WSIs | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    |  | Classification of tissue types | AUC: 0.855 (10% labels) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | Camelyon16 æ•°æ®é›† | 399 WSIs | [https://camelyon16.grand-challenge.org/Data/](https://camelyon16.grand-challenge.org/Data/)
    |  | ç»„ç»‡ç±»å‹åˆ†ç±» | AUC: 0.855 (10% æ ‡æ³¨) |'
- en: '|  |  |  |  | Kather multiclass dataset | 100K patches | Kather *et al.*Â ([2019](#bib.bib84))
    |  | Quantification of tumor cellularity | ACC: 0.982 (10% labels) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | Kather å¤šç±»åˆ«æ•°æ®é›† | 100K ç‰‡æ®µ | Kather *ç­‰äºº* ([2019](#bib.bib84)) |  |
    è‚¿ç˜¤ç»†èƒå¯†åº¦å®šé‡ | ACC: 0.982 (10% æ ‡æ³¨) |'
- en: '| Zheng etÂ al. ([2019](#bib.bib207)) | Application | Colon Cancer | H&E | MICCAI
    2015 Gland Segmentation Challenge (GlaS) dataset | 165 images | Sirinukunwattana
    etÂ al. ([2017](#bib.bib154)) | Variational Auto Encoder (VAE) | Active learning
    in biomedical image segmentation | F1 score: 0.909, 0.9252 (30% labeled) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Zheng ç­‰äºº ([2019](#bib.bib207)) | åº”ç”¨ | ç»“è‚ ç™Œ | H&E | MICCAI 2015 è…ºä½“åˆ†å‰²æŒ‘æˆ˜ï¼ˆGlaSï¼‰æ•°æ®é›†
    | 165 å¼ å›¾åƒ | Sirinukunwattana ç­‰äºº ([2017](#bib.bib154)) | å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ | ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¸»åŠ¨å­¦ä¹ 
    | F1 åˆ†æ•°: 0.909, 0.9252 (30% æ ‡æ³¨) |'
- en: '|  |  |  |  | Fungus dataset | 84 images | Zhang etÂ al. ([2017](#bib.bib205))
    |  |  |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | çœŸèŒæ•°æ®é›† | 84 å¼ å›¾åƒ | Zhang ç­‰äºº ([2017](#bib.bib205)) |  |  |  |'
- en: 4 Discussion and Future Trends
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 è®¨è®ºä¸æœªæ¥è¶‹åŠ¿
- en: 4.1 For Weakly Supervised Learning Paradigm
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 å¯¹äºå¼±ç›‘ç£å­¦ä¹ èŒƒå¼
- en: The two main goals of WSI analysis using the weakly supervised learning paradigm
    are global slide classification, which aims to accurately predict the labels of
    each WSI, and positive patch localization, which aims to accurately predict the
    labels of each positive patch in the positive bags. Among above two tasks, the
    former can be used for rapid automatic diagnosis of clinical pathology slides,
    such as early clinical screening, and the latter can be used for precise localization
    of tumor cells, as well as interpretable analysis of clinical diagnosis by deep
    learning networks. Based on the diagnostic results obtained from the whole slides,
    pathologists are often more interested in the precise location of tumor cells,
    the cell morphology and other microstructures for further analysis and corroboration.
    On the other hand, pathologists also expect new knowledge from the diagnosis of
    the deep neural networks, such as discovering new pathological patterns and structures,
    etc. A few current algorithms can perform the task of global slide classification
    well, but the task of positive patch localization is another challenge for most
    algorithms. A primary reason is that the loss functions of most bag-based deep
    MIL algorithms are defined only at the bag-level, and although mechanisms such
    as attention (Ilse *et al.*Â [2018](#bib.bib77)) can be used to measure the contribution
    of each instance to the bag-level classification, the network does not have enough
    motivation to classify all instances accurately (Shi *et al.*Â [2020](#bib.bib151),
    Qu *et al.*Â [2022](#bib.bib129)). On the other hand, instance-based methods and
    hybrid methods, although defining instance-level classifiers, usually face a high
    risk of errors in pseudo-labeling or key instance selection. Therefore, it is
    a new challenge for the weakly supervised learning paradigm to further improve
    the ability to classify instances while obtaining a better slide-level diagnosis.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¼±ç›‘ç£å­¦ä¹ èŒƒå¼è¿›è¡ŒWSIåˆ†æçš„ä¸¤ä¸ªä¸»è¦ç›®æ ‡æ˜¯å…¨çƒå¹»ç¯ç‰‡åˆ†ç±»ï¼Œå…¶ç›®çš„æ˜¯å‡†ç¡®é¢„æµ‹æ¯ä¸ªWSIçš„æ ‡ç­¾ï¼Œä»¥åŠæ­£æ ·æœ¬è¡¥ä¸å®šä½ï¼Œå…¶ç›®çš„æ˜¯å‡†ç¡®é¢„æµ‹æ­£æ ·æœ¬åŒ…ä¸­æ¯ä¸ªæ­£æ ·æœ¬è¡¥ä¸çš„æ ‡ç­¾ã€‚åœ¨è¿™ä¸¤ä¸ªä»»åŠ¡ä¸­ï¼Œå‰è€…å¯ä»¥ç”¨äºä¸´åºŠç—…ç†å¹»ç¯ç‰‡çš„å¿«é€Ÿè‡ªåŠ¨è¯Šæ–­ï¼Œä¾‹å¦‚æ—©æœŸä¸´åºŠç­›æŸ¥ï¼Œè€Œåè€…å¯ä»¥ç”¨äºç²¾ç¡®å®šä½è‚¿ç˜¤ç»†èƒï¼Œä»¥åŠé€šè¿‡æ·±åº¦å­¦ä¹ ç½‘ç»œè¿›è¡Œå¯è§£é‡Šçš„ä¸´åºŠè¯Šæ–­åˆ†æã€‚åŸºäºä»æ•´ä¸ªå¹»ç¯ç‰‡ä¸­è·å¾—çš„è¯Šæ–­ç»“æœï¼Œç—…ç†å­¦å®¶é€šå¸¸å¯¹è‚¿ç˜¤ç»†èƒçš„ç²¾ç¡®ä½ç½®ã€ç»†èƒå½¢æ€åŠå…¶ä»–å¾®è§‚ç»“æ„è¿›è¡Œè¿›ä¸€æ­¥åˆ†æå’Œç¡®è®¤æ›´åŠ æ„Ÿå…´è¶£ã€‚å¦ä¸€æ–¹é¢ï¼Œç—…ç†å­¦å®¶ä¹ŸæœŸæœ›ä»æ·±åº¦ç¥ç»ç½‘ç»œçš„è¯Šæ–­ä¸­è·å¾—æ–°çŸ¥è¯†ï¼Œä¾‹å¦‚å‘ç°æ–°çš„ç—…ç†æ¨¡å¼å’Œç»“æ„ç­‰ã€‚ç›®å‰ä¸€äº›ç®—æ³•å¯ä»¥å¾ˆå¥½åœ°æ‰§è¡Œå…¨çƒå¹»ç¯ç‰‡åˆ†ç±»ä»»åŠ¡ï¼Œä½†æ­£æ ·æœ¬è¡¥ä¸å®šä½ä»»åŠ¡å¯¹äºå¤§å¤šæ•°ç®—æ³•æ¥è¯´ä»ç„¶æ˜¯å¦ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸»è¦åŸå› æ˜¯å¤§å¤šæ•°åŸºäºè¢‹çš„æ·±åº¦MILç®—æ³•çš„æŸå¤±å‡½æ•°ä»…åœ¨è¢‹çº§åˆ«å®šä¹‰ï¼Œå°½ç®¡å¯ä»¥ä½¿ç”¨æ³¨æ„åŠ›ç­‰æœºåˆ¶ï¼ˆIlse
    *et al.* [2018](#bib.bib77)ï¼‰æ¥æµ‹é‡æ¯ä¸ªå®ä¾‹å¯¹è¢‹çº§åˆ«åˆ†ç±»çš„è´¡çŒ®ï¼Œä½†ç½‘ç»œæ²¡æœ‰è¶³å¤Ÿçš„åŠ¨åŠ›æ¥å‡†ç¡®åˆ†ç±»æ‰€æœ‰å®ä¾‹ï¼ˆShi *et al.*
    [2020](#bib.bib151)ï¼ŒQu *et al.* [2022](#bib.bib129)ï¼‰ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŸºäºå®ä¾‹çš„æ–¹æ³•å’Œæ··åˆæ–¹æ³•è™½ç„¶å®šä¹‰äº†å®ä¾‹çº§åˆ†ç±»å™¨ï¼Œä½†é€šå¸¸é¢ä¸´ä¼ªæ ‡ç­¾æˆ–å…³é”®å®ä¾‹é€‰æ‹©çš„é«˜é”™è¯¯é£é™©ã€‚å› æ­¤ï¼Œåœ¨è·å¾—æ›´å¥½çš„å¹»ç¯ç‰‡çº§è¯Šæ–­çš„åŒæ—¶ï¼Œè¿›ä¸€æ­¥æé«˜åˆ†ç±»å®ä¾‹çš„èƒ½åŠ›æ˜¯å¼±ç›‘ç£å­¦ä¹ èŒƒå¼é¢ä¸´çš„æ–°æŒ‘æˆ˜ã€‚
- en: Further, with the emergence of the methods of the weakly supervised segmentation
    in the natural image processing field (Ru *et al.*Â [2022](#bib.bib140), Xu *et
    al.*Â [2022](#bib.bib195), Pan *et al.*Â [2022](#bib.bib122), Lee *et al.*Â [2021](#bib.bib95),
    Chen *et al.*Â [2022](#bib.bib29)), a new challenging direction for WSI analysis
    is to perform pixel-level semantic segmentation of the entire WSI based on weak
    or sparse labels. The task of the positive patch localization, which described
    in the previous section is still based on the classification of patches, and it
    is a more challenging task to further obtain pixel-level segmentation results
    based on the weak labels. A few current studies (Xu *et al.*Â [2019](#bib.bib192),
    Qu *et al.*Â [2020](#bib.bib128), Belharbi *et al.*Â [2021](#bib.bib11), Lerousseau
    *et al.*Â [2020](#bib.bib96)) have made attempts in this new direction, but they
    still face many problems such as lack of details and precision on the segmentation
    results. Overall, for the weakly supervised learning paradigm, how to obtain the
    most detailed segmentation results as possible with weak labels is another promising
    study direction.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œéšç€è‡ªç„¶å›¾åƒå¤„ç†é¢†åŸŸä¸­å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•çš„å‡ºç°ï¼ˆRu *et al.*Â [2022](#bib.bib140)ï¼ŒXu *et al.*Â [2022](#bib.bib195)ï¼ŒPan
    *et al.*Â [2022](#bib.bib122)ï¼ŒLee *et al.*Â [2021](#bib.bib95)ï¼ŒChen *et al.*Â [2022](#bib.bib29)ï¼‰ï¼ŒWSIåˆ†æçš„æ–°æŒ‘æˆ˜æ–¹å‘æ˜¯åŸºäºå¼±æˆ–ç¨€ç–æ ‡ç­¾å¯¹æ•´ä¸ªWSIè¿›è¡Œåƒç´ çº§è¯­ä¹‰åˆ†å‰²ã€‚ä¸Šä¸€èŠ‚æè¿°çš„æ­£æ ·æœ¬è¡¥ä¸å®šä½ä»ç„¶åŸºäºè¡¥ä¸åˆ†ç±»ï¼Œè€ŒåŸºäºå¼±æ ‡ç­¾è¿›ä¸€æ­¥è·å¾—åƒç´ çº§åˆ†å‰²ç»“æœæ˜¯ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä¸€äº›å½“å‰ç ”ç©¶ï¼ˆXu
    *et al.*Â [2019](#bib.bib192)ï¼ŒQu *et al.*Â [2020](#bib.bib128)ï¼ŒBelharbi *et al.*Â [2021](#bib.bib11)ï¼ŒLerousseau
    *et al.*Â [2020](#bib.bib96)ï¼‰åœ¨è¿™ä¸ªæ–°æ–¹å‘ä¸Šåšå‡ºäº†å°è¯•ï¼Œä½†ä»é¢ä¸´è¯¸å¦‚åˆ†å‰²ç»“æœç¼ºä¹ç»†èŠ‚å’Œç²¾åº¦ç­‰è®¸å¤šé—®é¢˜ã€‚æ€»ä½“è€Œè¨€ï¼Œå¯¹äºå¼±ç›‘ç£å­¦ä¹ èŒƒå¼ï¼Œå¦‚ä½•åˆ©ç”¨å¼±æ ‡ç­¾è·å¾—å°½å¯èƒ½è¯¦ç»†çš„åˆ†å‰²ç»“æœæ˜¯å¦ä¸€ä¸ªæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚
- en: Another urgent need is the publicly available WSI datasets with fine-grained
    annotations at the patch level. As we all know, the scarcity of the publicly available
    pathological image datasets is an important factor hindering the development of
    the field. In recent years, we are grateful for the support of large public pathology
    datasets such as TCGA ([2019](#bib.bib168)), but public pathology datasets with
    fine-grained annotations are still in short supply for deeper research. To our
    knowledge, the large public WSI dataset with detailed annotation at the patch
    level is merely CAMELYON (Bejnordi *et al.*Â [2017a](#bib.bib9)). We should encourage
    an individual or organization to provide more public WSI datasets with detailed
    patch-level annotations to promote the development of this study field.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªè¿«åˆ‡éœ€è¦çš„æ˜¯å…¬å¼€çš„å…·æœ‰ç»†ç²’åº¦æ³¨é‡Šçš„WSIæ•°æ®é›†ã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œå…¬å¼€çš„ç—…ç†å›¾åƒæ•°æ®é›†ç¨€ç¼ºæ˜¯åˆ¶çº¦è¯¥é¢†åŸŸå‘å±•çš„é‡è¦å› ç´ ã€‚è¿‘å¹´æ¥ï¼Œæˆ‘ä»¬æ„Ÿè°¢TCGAç­‰å¤§å‹å…¬å¼€ç—…ç†æ•°æ®é›†çš„æ”¯æŒï¼ˆ[2019](#bib.bib168)ï¼‰ï¼Œä½†ä»ç„¶ç¼ºä¹å…·æœ‰ç»†ç²’åº¦æ³¨é‡Šçš„å…¬å¼€ç—…ç†æ•°æ®é›†ä»¥è¿›è¡Œæ›´æ·±å…¥çš„ç ”ç©¶ã€‚æ®æˆ‘ä»¬äº†è§£ï¼Œå…·æœ‰è¯¦ç»†æ³¨é‡Šçš„å¤§å‹å…¬å¼€WSIæ•°æ®é›†ä»…æœ‰CAMELYONï¼ˆBejnordi
    *et al.*Â [2017a](#bib.bib9)ï¼‰ã€‚æˆ‘ä»¬åº”é¼“åŠ±ä¸ªäººæˆ–ç»„ç»‡æä¾›æ›´å¤šå…·æœ‰è¯¦ç»†è¡¥ä¸çº§åˆ«æ³¨é‡Šçš„å…¬å¼€WSIæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¯¥ç ”ç©¶é¢†åŸŸçš„å‘å±•ã€‚
- en: 4.2 For Semi-Supervised Learning Paradigm
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 å¯¹äºåŠç›‘ç£å­¦ä¹ èŒƒå¼
- en: For semi-supervised learning paradigm, a new study direction is the combination
    with active learning, the purpose of which is to use the most effective labeled
    data to obtain the highest performance. Active learning aims to find the most
    valuable samples in the unlabeled dataset to be annotated through iterative interactions
    with experts, which allows to further exploit the effects of semi-supervised learning.
    There are already a lot of studies on pathological image analysis with the help
    of active learning (Zheng *et al.*Â [2019](#bib.bib207), Yang *et al.*Â [2017](#bib.bib199))
    or combination with semi-supervised learning and active learning (Su *et al.*Â [2015](#bib.bib163),
    Parag *et al.*Â [2014](#bib.bib123)).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåŠç›‘ç£å­¦ä¹ èŒƒå¼ï¼Œä¸€ä¸ªæ–°çš„ç ”ç©¶æ–¹å‘æ˜¯ä¸ä¸»åŠ¨å­¦ä¹ çš„ç»“åˆï¼Œå…¶ç›®çš„æ˜¯åˆ©ç”¨æœ€æœ‰æ•ˆçš„æ ‡è®°æ•°æ®è·å¾—æœ€é«˜çš„æ€§èƒ½ã€‚ä¸»åŠ¨å­¦ä¹ æ—¨åœ¨é€šè¿‡ä¸ä¸“å®¶çš„è¿­ä»£äº’åŠ¨ï¼Œæ‰¾åˆ°æœªæ ‡è®°æ•°æ®é›†ä¸­æœ€æœ‰ä»·å€¼çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œè¿™è¿›ä¸€æ­¥å‘æŒ¥äº†åŠç›‘ç£å­¦ä¹ çš„æ•ˆæœã€‚å·²ç»æœ‰å¾ˆå¤šç ”ç©¶åˆ©ç”¨ä¸»åŠ¨å­¦ä¹ è¿›è¡Œç—…ç†å›¾åƒåˆ†æï¼ˆZheng
    *et al.*Â [2019](#bib.bib207)ï¼ŒYang *et al.*Â [2017](#bib.bib199)ï¼‰æˆ–å°†åŠç›‘ç£å­¦ä¹ ä¸ä¸»åŠ¨å­¦ä¹ ç»“åˆï¼ˆSu
    *et al.*Â [2015](#bib.bib163)ï¼ŒParag *et al.*Â [2014](#bib.bib123)ï¼‰ã€‚
- en: Another challenge is the effect that noisy data and domain variation have on
    the performance of semi-supervised learning algorithms. In the field of computational
    pathology, noisy annotations are very common, because the instance features of
    pathological images are very complex and variable, and their sizes are so huge
    that doctors are likely to suffer from missing and mislabeling during annotation.
    When performing multicenter validation, significant staining variation between
    the slides from different centers is also very common as there is no uniform standard
    for staining pathological images among different centers. Both the noisy labels
    and the domain variation are powerful factors that affect the performance of semi-supervised
    learning in real-world scenarios. Recent studies (Koohbanani *et al.*Â [2021](#bib.bib89),
    Cheng *et al.*Â [2020](#bib.bib31), Shi *et al.*Â [2020](#bib.bib150), Foucart *et
    al.*Â [2019](#bib.bib57), Marini *et al.*Â [2021](#bib.bib109)) have made efforts
    on these two problems, and more studies in this field are expected.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯å™ªå£°æ•°æ®å’Œé¢†åŸŸå˜åŒ–å¯¹åŠç›‘ç£å­¦ä¹ ç®—æ³•æ€§èƒ½çš„å½±å“ã€‚åœ¨è®¡ç®—ç—…ç†é¢†åŸŸï¼Œå™ªå£°æ ‡æ³¨éå¸¸æ™®éï¼Œå› ä¸ºç—…ç†å›¾åƒçš„å®ä¾‹ç‰¹å¾éå¸¸å¤æ‚ä¸”å¤šå˜ï¼Œå…¶å°ºå¯¸åºå¤§ï¼ŒåŒ»ç”Ÿåœ¨æ ‡æ³¨è¿‡ç¨‹ä¸­å®¹æ˜“å‡ºç°é—æ¼å’Œé”™è¯¯æ ‡æ³¨ã€‚åœ¨è¿›è¡Œå¤šä¸­å¿ƒéªŒè¯æ—¶ï¼Œä¸åŒä¸­å¿ƒä¹‹é—´çš„åˆ‡ç‰‡æŸ“è‰²å˜åŒ–ä¹Ÿéå¸¸å¸¸è§ï¼Œå› ä¸ºä¸åŒä¸­å¿ƒæ²¡æœ‰ç»Ÿä¸€çš„ç—…ç†å›¾åƒæŸ“è‰²æ ‡å‡†ã€‚å™ªå£°æ ‡ç­¾å’Œé¢†åŸŸå˜åŒ–éƒ½æ˜¯å½±å“ç°å®ä¸–ç•Œåœºæ™¯ä¸­åŠç›‘ç£å­¦ä¹ æ€§èƒ½çš„é‡è¦å› ç´ ã€‚è¿‘æœŸçš„ç ”ç©¶ï¼ˆKoohbanani
    *ç­‰* [2021](#bib.bib89)ï¼ŒCheng *ç­‰* [2020](#bib.bib31)ï¼ŒShi *ç­‰* [2020](#bib.bib150)ï¼ŒFoucart
    *ç­‰* [2019](#bib.bib57)ï¼ŒMarini *ç­‰* [2021](#bib.bib109)ï¼‰å·²ç»å¯¹è¿™ä¸¤ä¸ªé—®é¢˜è¿›è¡Œäº†ç ”ç©¶ï¼Œæœªæ¥å¯¹æ­¤é¢†åŸŸçš„æ›´å¤šç ”ç©¶å€¼å¾—æœŸå¾…ã€‚
- en: 4.3 For Self-Supervised Learning Paradigm
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 è‡ªç›‘ç£å­¦ä¹ èŒƒå¼
- en: For self-supervised learning paradigm, although current relevant studies in
    the field of natural images are developing rapidly, the direct applications of
    these methods to pathological images will be hindered by the strong domain discrepancy
    (Ciga *et al.*Â [2022](#bib.bib36), Koohbanani *et al.*Â [2021](#bib.bib89)). Therefore,
    how to design more effective self-supervised auxiliary tasks for pathological
    images is a promising direction.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè‡ªç›‘ç£å­¦ä¹ èŒƒå¼ï¼Œå°½ç®¡å½“å‰åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸçš„ç›¸å…³ç ”ç©¶å‘å±•è¿…é€Ÿï¼Œä½†è¿™äº›æ–¹æ³•ç›´æ¥åº”ç”¨äºç—…ç†å›¾åƒå°†å—åˆ°å¼ºçƒˆé¢†åŸŸå·®å¼‚çš„é˜»ç¢ï¼ˆCiga *ç­‰* [2022](#bib.bib36)ï¼ŒKoohbanani
    *ç­‰* [2021](#bib.bib89)ï¼‰ã€‚å› æ­¤ï¼Œå¦‚ä½•ä¸ºç—…ç†å›¾åƒè®¾è®¡æ›´æœ‰æ•ˆçš„è‡ªç›‘ç£è¾…åŠ©ä»»åŠ¡æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚
- en: On the other hand, self-supervised learning has been promoting the development
    of weakly supervised learning and semi-supervised learning in pathological image
    analysis. As we all know, it is difficult for a network to learn effective feature
    representations with very limited annotations. In contrast, self-supervised learning
    is very suitable for learning effective feature representations from a lot of
    unlabeled data. Therefore, it will be a popular way to combine the features extracted
    by self-supervised pre-training with the weakly supervised or semi-supervised
    downstream tasks in the future. On the one hand, the efficient feature representations
    obtained from self-supervised pre-training will greatly improve the efficiency
    of weakly supervised learning and semi-supervised learning, and on the other hand,
    weakly supervised learning or semi-supervised learning will fully release the
    new potential of self-supervised learning in the field of computational pathology.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œè‡ªç›‘ç£å­¦ä¹ æ­£åœ¨æ¨åŠ¨ç—…ç†å›¾åƒåˆ†æä¸­å¼±ç›‘ç£å­¦ä¹ å’ŒåŠç›‘ç£å­¦ä¹ çš„å‘å±•ã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œç½‘ç»œå¾ˆéš¾åœ¨éå¸¸æœ‰é™çš„æ ‡æ³¨ä¸‹å­¦ä¹ æœ‰æ•ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè‡ªç›‘ç£å­¦ä¹ éå¸¸é€‚åˆä»å¤§é‡æœªæ ‡è®°çš„æ•°æ®ä¸­å­¦ä¹ æœ‰æ•ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚å› æ­¤ï¼Œå°†è‡ªç›‘ç£é¢„è®­ç»ƒæå–çš„ç‰¹å¾ä¸å¼±ç›‘ç£æˆ–åŠç›‘ç£ä¸‹æ¸¸ä»»åŠ¡ç»“åˆèµ·æ¥ï¼Œå°†æ˜¯æœªæ¥çš„ä¸€ç§çƒ­é—¨æ–¹å¼ã€‚ä¸€æ–¹é¢ï¼Œè‡ªç›‘ç£é¢„è®­ç»ƒè·å¾—çš„é«˜æ•ˆç‰¹å¾è¡¨ç¤ºå°†å¤§å¤§æé«˜å¼±ç›‘ç£å­¦ä¹ å’ŒåŠç›‘ç£å­¦ä¹ çš„æ•ˆç‡ï¼Œå¦ä¸€æ–¹é¢ï¼Œå¼±ç›‘ç£å­¦ä¹ æˆ–åŠç›‘ç£å­¦ä¹ å°†å……åˆ†é‡Šæ”¾è‡ªç›‘ç£å­¦ä¹ åœ¨è®¡ç®—ç—…ç†é¢†åŸŸçš„æ–°æ½œåŠ›ã€‚
- en: 4.4 Limitations
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 é™åˆ¶
- en: This review also has several limitations. First, due to space limitations, this
    review does not include more clinical studies. We focus more on top technical
    conferences and journals and do not include more excellent papers published in
    clinical journals. For more systematic reviews of clinical studies, see (Cifci
    *et al.*Â [2022](#bib.bib35)) and (Kleppe *et al.*Â [2021](#bib.bib88)) for details.
    In addition, since there are so many technical studies on artificial intelligence
    applied to computational pathology, it is difficult to summarize them all, and
    due to space limitations, we have tried to include as many recent articles as
    possible, while some of them have not been included.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç»¼è¿°ä¹Ÿå­˜åœ¨è‹¥å¹²å±€é™æ€§ã€‚é¦–å…ˆï¼Œç”±äºç¯‡å¹…é™åˆ¶ï¼Œæœ¬ç»¼è¿°æœªåŒ…æ‹¬æ›´å¤šçš„ä¸´åºŠç ”ç©¶ã€‚æˆ‘ä»¬æ›´å¤šå…³æ³¨é¡¶çº§æŠ€æœ¯ä¼šè®®å’ŒæœŸåˆŠï¼Œè€Œæœªçº³å…¥æ›´å¤šåœ¨ä¸´åºŠæœŸåˆŠä¸Šå‘è¡¨çš„ä¼˜ç§€è®ºæ–‡ã€‚æœ‰å…³ä¸´åºŠç ”ç©¶çš„æ›´ç³»ç»Ÿç»¼è¿°ï¼Œè¯·å‚è§(Cifci
    *et al.* [2022](#bib.bib35))å’Œ(Kleppe *et al.* [2021](#bib.bib88))ã€‚æ­¤å¤–ï¼Œç”±äºäººå·¥æ™ºèƒ½åº”ç”¨äºè®¡ç®—ç—…ç†å­¦çš„æŠ€æœ¯ç ”ç©¶ä¼—å¤šï¼Œéš¾ä»¥ä¸€ä¸€æ€»ç»“ï¼Œä¸”ç”±äºç¯‡å¹…é™åˆ¶ï¼Œæˆ‘ä»¬å°½åŠ›åŒ…å«äº†å°½å¯èƒ½å¤šçš„æœ€æ–°æ–‡ç« ï¼Œéƒ¨åˆ†æ–‡ç« æœªè¢«çº³å…¥ã€‚
- en: 5 Conclusion
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 ç»“è®º
- en: In this review, we provide a systematic summary of recent studies on weakly
    supervised learning, semi-supervised learning, and self-supervised learning in
    the field of computational pathology from the theoretical and methodological perspectives.
    On this basis, we also present targeted solutions to some current difficulties
    and shortcomings in this field, and illustrate its future trends. Through a survey
    of over 130 papers, we find that the field of computational pathology is marching
    at high speed into a new era, which is automatic diagnosis and analysis with fewer
    annotation needs, wider application scope, and higher prediction accuracy.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬ä»ç†è®ºå’Œæ–¹æ³•è®ºçš„è§’åº¦ç³»ç»Ÿæ€»ç»“äº†æœ€è¿‘åœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„å¼±ç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ çš„ç ”ç©¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€äº›é’ˆå¯¹å½“å‰é¢†åŸŸå›°éš¾å’Œä¸è¶³çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶é˜è¿°äº†å…¶æœªæ¥è¶‹åŠ¿ã€‚é€šè¿‡å¯¹è¶…è¿‡130ç¯‡è®ºæ–‡çš„è°ƒæŸ¥ï¼Œæˆ‘ä»¬å‘ç°è®¡ç®—ç—…ç†å­¦é¢†åŸŸæ­£ä»¥é«˜é€Ÿè¿ˆå…¥ä¸€ä¸ªæ–°æ—¶ä»£ï¼Œå³è‡ªåŠ¨è¯Šæ–­å’Œåˆ†æï¼Œéœ€æ±‚çš„æ ‡æ³¨æ›´å°‘ï¼Œåº”ç”¨èŒƒå›´æ›´å¹¿ï¼Œé¢„æµ‹å‡†ç¡®åº¦æ›´é«˜ã€‚
- en: Acknowledgments
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡´è°¢
- en: This work was supported by National Natural Science Foundation of China under
    Grant 82072021.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬å·¥ä½œå¾—åˆ°äº†ä¸­å›½å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ï¼ˆèµ„åŠ©ç¼–å·82072021ï¼‰çš„æ”¯æŒã€‚
- en: References
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: (1)
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abbet etÂ al. (2020) Abbet, C., Zlobec, I., Bozorgtabar, B. andÂ Thiran, J.-P.
    (2020). Divide-and-rule: self-supervised learning for survival analysis in colorectal
    cancer, International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, pp.Â 480â€“489.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbetç­‰ï¼ˆ2020ï¼‰Abbet, C., Zlobec, I., Bozorgtabar, B. å’Œ Thiran, J.-P.ï¼ˆ2020ï¼‰ã€‚åˆ†è€Œæ²»ä¹‹ï¼šç”¨äºç»“ç›´è‚ ç™Œç”Ÿå­˜åˆ†æçš„è‡ªç›‘ç£å­¦ä¹ ï¼Œã€Šå›½é™…åŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬480â€“489é¡µã€‚
- en: Anand etÂ al. (2021) Anand, D., Yashashwi, K., Kumar, N., Rane, S., Gann, P.Â H.
    andÂ Sethi, A. (2021). Weakly supervised learning on unannotated h&e-stained slides
    predicts braf mutation in thyroid cancer with high accuracy, The Journal of Pathology  255(3):Â 232â€“242.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Anandç­‰ï¼ˆ2021ï¼‰Anand, D., Yashashwi, K., Kumar, N., Rane, S., Gann, P. H. å’Œ Sethi,
    A.ï¼ˆ2021ï¼‰ã€‚åœ¨æœªæ ‡æ³¨çš„H&EæŸ“è‰²åˆ‡ç‰‡ä¸Šè¿›è¡Œå¼±ç›‘ç£å­¦ä¹ ï¼Œä»¥é«˜å‡†ç¡®åº¦é¢„æµ‹ç”²çŠ¶è…ºç™Œä¸­çš„BRAFçªå˜ï¼Œã€ŠThe Journal of Pathologyã€‹255(3):
    232â€“242ã€‚'
- en: AraÃºjo etÂ al. (2017) AraÃºjo, T., Aresta, G., Castro, E., Rouco, J., Aguiar,
    P., Eloy, C., PolÃ³nia, A. andÂ Campilho, A. (2017). Classification of breast cancer
    histology images using convolutional neural networks, PloS One  12(6):Â e0177544.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AraÃºjoç­‰ï¼ˆ2017ï¼‰AraÃºjo, T., Aresta, G., Castro, E., Rouco, J., Aguiar, P., Eloy,
    C., PolÃ³nia, A. å’Œ Campilho, A.ï¼ˆ2017ï¼‰ã€‚ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œå¯¹ä¹³è…ºç™Œç»„ç»‡å­¦å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œã€ŠPloS Oneã€‹12(6): e0177544ã€‚'
- en: 'Aresta etÂ al. (2019) Aresta, G., AraÃºjo, T., Kwok, S., Chennamsetty, S.Â S.,
    Safwan, M., Alex, V., Marami, B., Prastawa, M., Chan, M., Donovan, M. etÂ al. (2019).
    Bach: Grand challenge on breast cancer histology images, Medical Image Analysis  56:Â 122â€“139.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arestaç­‰ï¼ˆ2019ï¼‰Aresta, G., AraÃºjo, T., Kwok, S., Chennamsetty, S. S., Safwan,
    M., Alex, V., Marami, B., Prastawa, M., Chan, M., Donovan, M. ç­‰ï¼ˆ2019ï¼‰ã€‚Bachï¼šä¹³è…ºç™Œç»„ç»‡å­¦å›¾åƒå¤§æŒ‘æˆ˜ï¼Œã€ŠåŒ»å­¦å›¾åƒåˆ†æã€‹56:
    122â€“139ã€‚'
- en: 'Bao etÂ al. (2021) Bao, H., Dong, L. andÂ Wei, F. (2021). Beit: Bert pre-training
    of image transformers, arXiv preprint arXiv:2106.08254 .'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baoç­‰ï¼ˆ2021ï¼‰Bao, H., Dong, L. å’Œ Wei, F.ï¼ˆ2021ï¼‰ã€‚Beitï¼šå›¾åƒå˜æ¢å™¨çš„Berté¢„è®­ç»ƒï¼ŒarXivé¢„å°æœ¬arXiv:2106.08254ã€‚
- en: Basavanhally etÂ al. (2013) Basavanhally, A., Ganesan, S., Feldman, M., Shih,
    N., Mies, C., Tomaszewski, J. andÂ Madabhushi, A. (2013). Multi-field-of-view framework
    for distinguishing tumor grade in er+ breast cancer from entire histopathology
    slides, IEEE Transactions on Biomedical Engineering  60(8):Â 2089â€“2099.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Basavanhallyç­‰ï¼ˆ2013ï¼‰Basavanhally, A., Ganesan, S., Feldman, M., Shih, N., Mies,
    C., Tomaszewski, J. å’Œ Madabhushi, A.ï¼ˆ2013ï¼‰ã€‚ç”¨äºåŒºåˆ†ER+ä¹³è…ºç™Œè‚¿ç˜¤ç­‰çº§çš„å¤šè§†è§’æ¡†æ¶ï¼Œä»æ•´ä¸ªç»„ç»‡ç—…ç†åˆ‡ç‰‡ä¸­åŒºåˆ†è‚¿ç˜¤ç­‰çº§ï¼Œã€ŠIEEEç”Ÿç‰©åŒ»å­¦å·¥ç¨‹å­¦æŠ¥ã€‹60(8):
    2089â€“2099ã€‚'
- en: Beck etÂ al. (2011) Beck, A.Â H., Sangoi, A.Â R., Leung, S., Marinelli, R.Â J.,
    Nielsen, T.Â O., Van DeÂ Vijver, M.Â J., West, R.Â B., Van DeÂ Rijn, M. andÂ Koller,
    D. (2011). Systematic analysis of breast cancer morphology uncovers stromal features
    associated with survival, Science Translational Medicine  3(108):Â 108ra113â€“108ra113.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beck et al. (2011) Beck, A. H., Sangoi, A. R., Leung, S., Marinelli, R. J.,
    Nielsen, T. O., Van De Vijver, M. J., West, R. B., Van De Rijn, M. å’Œ Koller, D.
    (2011). ä¹³è…ºç™Œå½¢æ€çš„ç³»ç»Ÿåˆ†ææ­ç¤ºä¸ç”Ÿå­˜ç›¸å…³çš„åŸºè´¨ç‰¹å¾ï¼Œã€Šç§‘å­¦è½¬åŒ–åŒ»å­¦ã€‹ 3(108): 108ra113â€“108ra113ã€‚'
- en: Bejnordi etÂ al. (2017a) Bejnordi, B.Â E., Veta, M., VanÂ Diest, P.Â J., VanÂ Ginneken,
    B., Karssemeijer, N., Litjens, G., Van DerÂ Laak, J.Â A., Hermsen, M., Manson, Q.Â F.,
    Balkenhol, M. etÂ al. (2017a). Diagnostic assessment of deep learning algorithms
    for detection of lymph node metastases in women with breast cancer, Jama  318(22):Â 2199â€“2210.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bejnordi et al. (2017a) Bejnordi, B. E., Veta, M., Van Diest, P. J., Van Ginneken,
    B., Karssemeijer, N., Litjens, G., Van Der Laak, J. A., Hermsen, M., Manson, Q.
    F., Balkenhol, M. ç­‰ (2017a). å¯¹ä¹³è…ºç™Œå¥³æ€§æ·‹å·´ç»“è½¬ç§»æ£€æµ‹çš„æ·±åº¦å­¦ä¹ ç®—æ³•çš„è¯Šæ–­è¯„ä¼°ï¼Œã€ŠJamaã€‹ 318(22): 2199â€“2210ã€‚'
- en: Bejnordi etÂ al. (2017b) Bejnordi, B.Â E., Veta, M., VanÂ Diest, P.Â J., VanÂ Ginneken,
    B., Karssemeijer, N., Litjens, G., Van DerÂ Laak, J.Â A., Hermsen, M., Manson, Q.Â F.,
    Balkenhol, M. etÂ al. (2017b). Diagnostic assessment of deep learning algorithms
    for detection of lymph node metastases in women with breast cancer, JAMA  318(22):Â 2199â€“2210.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bejnordi et al. (2017b) Bejnordi, B. E., Veta, M., Van Diest, P. J., Van Ginneken,
    B., Karssemeijer, N., Litjens, G., Van Der Laak, J. A., Hermsen, M., Manson, Q.
    F., Balkenhol, M. ç­‰ (2017b). å¯¹ä¹³è…ºç™Œå¥³æ€§æ·‹å·´ç»“è½¬ç§»æ£€æµ‹çš„æ·±åº¦å­¦ä¹ ç®—æ³•çš„è¯Šæ–­è¯„ä¼°ï¼Œã€ŠJAMAã€‹ 318(22): 2199â€“2210ã€‚'
- en: Belharbi etÂ al. (2021) Belharbi, S., Rony, J., Dolz, J., Ayed, I.Â B., McCaffrey,
    L. andÂ Granger, E. (2021). Deep interpretable classification and weakly-supervised
    segmentation of histology images via max-min uncertainty, IEEE Transactions on
    Medical Imaging .
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belharbi et al. (2021) Belharbi, S., Rony, J., Dolz, J., Ayed, I. B., McCaffrey,
    L. å’Œ Granger, E. (2021). é€šè¿‡æœ€å¤§-æœ€å°ä¸ç¡®å®šæ€§è¿›è¡Œæ·±åº¦å¯è§£é‡Šåˆ†ç±»å’Œå¼±ç›‘ç£åˆ†å‰²ã€ŠåŒ»å­¦å½±åƒIEEEäº¤æ˜“ã€‹ã€‚
- en: Belkin etÂ al. (2005) Belkin, M., Niyogi, P. andÂ Sindhwani, V. (2005). On manifold
    regularization, International Workshop on Artificial Intelligence and Statistics,
    PMLR, pp.Â 17â€“24.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belkin et al. (2005) Belkin, M., Niyogi, P. å’Œ Sindhwani, V. (2005). å…³äºæµå½¢æ­£åˆ™åŒ–ï¼Œã€Šäººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡å›½é™…ç ”è®¨ä¼šã€‹ï¼ŒPMLRï¼Œç¬¬17â€“24é¡µã€‚
- en: 'Belkin etÂ al. (2006) Belkin, M., Niyogi, P. andÂ Sindhwani, V. (2006). Manifold
    regularization: A geometric framework for learning from labeled and unlabeled
    examples., Journal of Machine Learning Research  7(11).'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belkin et al. (2006) Belkin, M., Niyogi, P. å’Œ Sindhwani, V. (2006). æµå½¢æ­£åˆ™åŒ–ï¼šä»æ ‡è®°å’Œæœªæ ‡è®°æ ·æœ¬ä¸­å­¦ä¹ çš„å‡ ä½•æ¡†æ¶ï¼Œã€Šæœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—ã€‹
    7(11)ã€‚
- en: Blum andÂ Mitchell (1998) Blum, A. andÂ Mitchell, T. (1998). Combining labeled
    and unlabeled data with co-training, Proceedings of the Eleventh Annual Conference
    on Computational Learning Theory, pp.Â 92â€“100.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blum å’Œ Mitchell (1998) Blum, A. å’Œ Mitchell, T. (1998). ç»“åˆæ ‡è®°å’Œæœªæ ‡è®°æ•°æ®çš„å…±åŒè®­ç»ƒï¼Œã€Šç¬¬åä¸€å±Šè®¡ç®—å­¦ä¹ ç†è®ºå¹´ä¼šè®ºæ–‡é›†ã€‹ï¼Œç¬¬92â€“100é¡µã€‚
- en: Boyd etÂ al. (2021) Boyd, J., Liashuha, M., Deutsch, E., Paragios, N., Christodoulidis,
    S. andÂ Vakalopoulou, M. (2021). Self-supervised representation learning using
    visual field expansion on digital pathology, Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp.Â 639â€“647.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boyd et al. (2021) Boyd, J., Liashuha, M., Deutsch, E., Paragios, N., Christodoulidis,
    S. å’Œ Vakalopoulou, M. (2021). ä½¿ç”¨è§†è§‰é¢†åŸŸæ‰©å±•çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ åœ¨æ•°å­—ç—…ç†å­¦ä¸­çš„åº”ç”¨ï¼Œã€ŠIEEE/CVFå›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡é›†ã€‹ï¼Œç¬¬639â€“647é¡µã€‚
- en: 'Bulten etÂ al. (2020) Bulten, W., Pinckaers, H., van Boven, H., Vink, R., deÂ Bel,
    T., van Ginneken, B., vanÂ der Laak, J., Hulsbergen-vanÂ de Kaa, C. andÂ Litjens,
    G. (2020). Automated deep-learning system for gleason grading of prostate cancer
    using biopsies: a diagnostic study, The Lancet Oncology  21(2):Â 233â€“241.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bulten et al. (2020) Bulten, W., Pinckaers, H., van Boven, H., Vink, R., de
    Bel, T., van Ginneken, B., van der Laak, J., Hulsbergen-van de Kaa, C. å’Œ Litjens,
    G. (2020). ç”¨äºå‰åˆ—è…ºç™ŒGleasonè¯„åˆ†çš„è‡ªåŠ¨æ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼šä¸€é¡¹è¯Šæ–­ç ”ç©¶ï¼Œã€ŠæŸ³å¶åˆ€è‚¿ç˜¤å­¦ã€‹ 21(2): 233â€“241ã€‚'
- en: Campanella etÂ al. (2019) Campanella, G., Hanna, M.Â G., Geneslaw, L., Miraflor,
    A., Werneck KraussÂ Silva, V., Busam, K.Â J., Brogi, E., Reuter, V.Â E., Klimstra,
    D.Â S. andÂ Fuchs, T.Â J. (2019). Clinical-grade computational pathology using weakly
    supervised deep learning on whole slide images, Nature Medicine  25(8):Â 1301â€“1309.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Campanella et al. (2019) Campanella, G., Hanna, M. G., Geneslaw, L., Miraflor,
    A., Werneck Krauss Silva, V., Busam, K. J., Brogi, E., Reuter, V. E., Klimstra,
    D. S. å’Œ Fuchs, T. J. (2019). ä½¿ç”¨å¼±ç›‘ç£æ·±åº¦å­¦ä¹ çš„ä¸´åºŠçº§è®¡ç®—ç—…ç†å­¦ï¼Œã€Šè‡ªç„¶åŒ»å­¦ã€‹ 25(8): 1301â€“1309ã€‚'
- en: Caron etÂ al. (2020) Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski,
    P. andÂ Joulin, A. (2020). Unsupervised learning of visual features by contrasting
    cluster assignments, Advances in Neural Information Processing Systems  33:Â 9912â€“9924.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¡éš†ç­‰ï¼ˆ2020ï¼‰å¡éš†ï¼Œç±³æ–¯æ‹‰ï¼Œé©¬ä¼Šæ‹‰å°”ï¼Œæˆˆäºšå°”ï¼Œåšæ‰¬è¯ºå¤«æ–¯åŸºå’Œæœ±åˆ©å®‰ï¼ˆ2020ï¼‰ã€‚é€šè¿‡å¯¹æ¯”é›†ç¾¤åˆ†é…è¿›è¡Œçš„è§†è§‰ç‰¹å¾æ— ç›‘ç£å­¦ä¹ ï¼Œç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•33ï¼š9912â€“9924ã€‚
- en: Caron etÂ al. (2021) Caron, M., Touvron, H., Misra, I., JÃ©gou, H., Mairal, J.,
    Bojanowski, P. andÂ Joulin, A. (2021). Emerging properties in self-supervised vision
    transformers, Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp.Â 9650â€“9660.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¡éš†ç­‰ï¼ˆ2021ï¼‰å¡éš†ï¼Œå›¾å¼—é¾™ï¼Œç±³æ–¯æ‹‰ï¼Œæ°å¤ï¼Œé©¬ä¼Šæ‹‰å°”ï¼Œåšæ‰¬è¯ºå¤«æ–¯åŸºå’Œæœ±åˆ©å®‰ï¼ˆ2021ï¼‰ã€‚è‡ªç›‘ç£è§†è§‰å˜æ¢å™¨ä¸­çš„æ–°å…´ç‰¹æ€§ï¼ŒIEEE/CVFå›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬9650â€“9660é¡µã€‚
- en: 'Chang andÂ Lin (2011) Chang, C.-C. andÂ Lin, C.-J. (2011). Libsvm: a library
    for support vector machines, ACM Transactions on Intelligent Systems and Technology
    (TIST)  2(3):Â 1â€“27.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼ å’Œæ—ï¼ˆ2011ï¼‰å¼ å¿—æˆå’Œæ—å¿—åšï¼ˆ2011ï¼‰ã€‚Libsvmï¼šæ”¯æŒå‘é‡æœºåº“ï¼ŒACMæ™ºèƒ½ç³»ç»Ÿä¸æŠ€æœ¯äº¤æ˜“ï¼ˆTISTï¼‰2ï¼ˆ3ï¼‰ï¼š1â€“27ã€‚
- en: Chaudhary etÂ al. (2018) Chaudhary, K., Poirion, O.Â B., Lu, L. andÂ Garmire, L.Â X.
    (2018). Deep learningâ€“based multi-omics integration robustly predicts survival
    in liver cancer, Clinical Cancer Research  24(6):Â 1248â€“1259.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŸ¥ä¹Œè¾¾é‡Œç­‰ï¼ˆ2018ï¼‰æŸ¥ä¹Œè¾¾é‡Œï¼Œæ™®ç“¦é‡Œæ˜‚ï¼Œå¢ä¸½å’ŒåŠ å°”ç±³å°”ï¼ˆ2018ï¼‰ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šç»„å­¦æ•´åˆç¨³å¥é¢„æµ‹è‚ç™Œç”Ÿå­˜ï¼Œä¸´åºŠç™Œç—‡ç ”ç©¶24ï¼ˆ6ï¼‰ï¼š1248â€“1259ã€‚
- en: 'Chen etÂ al. (2017) Chen, H., Qi, X., Yu, L., Dou, Q., Qin, J. andÂ Heng, P.-A.
    (2017). Dcan: Deep contour-aware networks for object instance segmentation from
    histology images, Medical Image Analysis  36:Â 135â€“146.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™ˆç­‰ï¼ˆ2017ï¼‰é™ˆæµ©ï¼Œé½é‘«ï¼Œä½™äº®ï¼Œçª¦åº†ï¼Œç§¦å¥å’Œæ¨ªé¹å®‰ï¼ˆ2017ï¼‰ã€‚Dcanï¼šç”¨äºç»„ç»‡å­¦å›¾åƒå¯¹è±¡å®ä¾‹åˆ†å‰²çš„æ·±åº¦è½®å»“æ„ŸçŸ¥ç½‘ç»œï¼ŒåŒ»å­¦å›¾åƒåˆ†æ36ï¼š135â€“146ã€‚
- en: Chen etÂ al. (2019) Chen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M.
    andÂ Rueckert, D. (2019). Self-supervised learning for medical image analysis using
    image context restoration, Medical Image Analysis  58:Â 101539.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™ˆç­‰ï¼ˆ2019ï¼‰é™ˆç«‹ï¼Œå½­åˆ©ï¼Œæ£®æœ¬ï¼Œä¸‰æ³½ä½³å’Œè—¤åŸç¾ï¼ˆ2019ï¼‰ã€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†æçš„è‡ªç›‘ç£å­¦ä¹ ï¼Œé€šè¿‡å›¾åƒä¸Šä¸‹æ–‡æ¢å¤ï¼ŒåŒ»å­¦å›¾åƒåˆ†æ58ï¼š101539ã€‚
- en: Chen andÂ Krishnan (2022) Chen, R.Â J. andÂ Krishnan, R.Â G. (2022). Self-supervised
    vision transformers learn visual concepts in histopathology, arXiv preprint arXiv:2203.00585
    .
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™ˆå’Œå…‹é‡Œå¸Œå—ï¼ˆ2022ï¼‰é™ˆç‘æ°å’Œå…‹é‡Œå¸Œå—æ‹‰ç»´ï¼ˆ2022ï¼‰ã€‚è‡ªç›‘ç£è§†è§‰å˜æ¢å™¨åœ¨ç»„ç»‡ç—…ç†å­¦ä¸­çš„è§†è§‰æ¦‚å¿µå­¦ä¹ ï¼ŒarXivé¢„å°æœ¬arXiv:2203.00585ã€‚
- en: 'Chen, Lu andÂ Mahmood (2020) Chen, R.Â J., Lu, N.Â I. andÂ Mahmood, F. (2020).
    Pathomic fusion: an integrated framework for fusing histopathology and genomic
    features for cancer diagnosis and prognosis, IEEE Transactions on Medical Imaging
    .'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™ˆï¼Œå¢å’Œé©¬å“ˆèŒ‚å¾·ï¼ˆ2020ï¼‰é™ˆç‘æ°ï¼Œå¢å®å’Œé©¬å“ˆèŒ‚å¾·ï¼ˆ2020ï¼‰ã€‚ç—…ç†ç»„å­¦èåˆï¼šç”¨äºç™Œç—‡è¯Šæ–­å’Œé¢„åçš„ç»„ç»‡ç—…ç†å­¦å’ŒåŸºå› ç»„ç‰¹å¾èåˆçš„é›†æˆæ¡†æ¶ï¼ŒIEEEåŒ»å­¦æˆåƒäº‹åŠ¡ã€‚
- en: Chen, Kornblith, Norouzi andÂ Hinton (2020) Chen, T., Kornblith, S., Norouzi,
    M. andÂ Hinton, G. (2020). A simple framework for contrastive learning of visual
    representations, International Conference on Machine Learning, PMLR, pp.Â 1597â€“1607.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™ˆï¼Œç§‘æ©å¸ƒåˆ©æ–¯ï¼Œè¯ºé²å…¹å’Œè¾›é¡¿ï¼ˆ2020ï¼‰é™ˆå¤©ï¼Œç§‘æ©å¸ƒåˆ©æ–¯ï¼Œè¯ºé²å…¹å’Œè¾›é¡¿ï¼ˆ2020ï¼‰ã€‚è§†è§‰è¡¨å¾å¯¹æ¯”å­¦ä¹ çš„ç®€å•æ¡†æ¶ï¼Œå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ŒPMLRï¼Œç¬¬1597â€“1607é¡µã€‚
- en: Chen, Fan, Girshick andÂ He (2020) Chen, X., Fan, H., Girshick, R. andÂ He, K.
    (2020). Improved baselines with momentum contrastive learning, arXiv preprint
    arXiv:2003.04297 .
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™ˆï¼ŒèŒƒï¼Œå‰å°”å¸Œå…‹å’Œä½•ï¼ˆ2020ï¼‰é™ˆè½©ï¼ŒèŒƒæµ©ï¼Œå‰å°”å¸Œå…‹ç‘å’Œä½•å‡¯ï¼ˆ2020ï¼‰ã€‚åŸºäºåŠ¨é‡å¯¹æ¯”å­¦ä¹ çš„æ”¹è¿›åŸºçº¿ï¼ŒarXivé¢„å°æœ¬arXiv:2003.04297ã€‚
- en: Chen andÂ He (2021) Chen, X. andÂ He, K. (2021). Exploring simple siamese representation
    learning, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp.Â 15750â€“15758.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™ˆå’Œä½•ï¼ˆ2021ï¼‰é™ˆè½©å’Œä½•å‡¯ï¼ˆ2021ï¼‰ã€‚æ¢ç´¢ç®€å•çš„å­ªç”Ÿè¡¨ç¤ºå­¦ä¹ ï¼ŒIEEE/CVFè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬15750â€“15758é¡µã€‚
- en: Chen etÂ al. (2022) Chen, Z., Wang, T., Wu, X., Hua, X.-S., Zhang, H. andÂ Sun,
    Q. (2022). Class re-activation maps for weakly-supervised semantic segmentation,
    arXiv preprint arXiv:2203.00962 .
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™ˆç­‰ï¼ˆ2022ï¼‰é™ˆå­ï¼Œç‹è…¾ï¼Œå´ç¿”ï¼Œåæ™“æ™Ÿï¼Œå¼ å®å’Œå­™å¼ºï¼ˆ2022ï¼‰ã€‚ç”¨äºå¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²çš„ç±»åˆ«é‡æ¿€æ´»å›¾ï¼ŒarXivé¢„å°æœ¬arXiv:2203.00962ã€‚
- en: 'Chen etÂ al. (2021) Chen, Z., Zhang, J., Che, S., Huang, J., Han, X. andÂ Yuan,
    Y. (2021). Diagnose like a pathologist: Weakly-supervised pathologist-tree network
    for slide-level immunohistochemical scoring, 35th AAAI Conference on Artificial
    Intelligence (AAAI-21), AAAI Press, pp.Â 47â€“54.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™ˆç­‰ï¼ˆ2021ï¼‰é™ˆå­ï¼Œå¼ æ°ï¼Œè½¦èˆ’ï¼Œé»„ä¿Šï¼ŒéŸ©æ™“å’Œè¢é˜³ï¼ˆ2021ï¼‰ã€‚åƒç—…ç†å­¦å®¶ä¸€æ ·è¯Šæ–­ï¼šç”¨äºæ»‘ç‰‡çº§å…ç–«ç»„ç»‡åŒ–å­¦è¯„åˆ†çš„å¼±ç›‘ç£ç—…ç†å­¦å®¶æ ‘ç½‘ç»œï¼Œç¬¬35å±ŠAAAIäººå·¥æ™ºèƒ½ä¼šè®®ï¼ˆAAAI-21ï¼‰ï¼ŒAAAIå‡ºç‰ˆç¤¾ï¼Œç¬¬47â€“54é¡µã€‚
- en: Cheng etÂ al. (2020) Cheng, H.-T., Yeh, C.-F., Kuo, P.-C., Wei, A., Liu, K.-C.,
    Ko, M.-C., Chao, K.-H., Peng, Y.-C. andÂ Liu, T.-L. (2020). Self-similarity student
    for partial label histopathology image segmentation, European Conference on Computer
    Vision, Springer, pp.Â 117â€“132.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng ç­‰äºº (2020) Cheng, H.-T., Yeh, C.-F., Kuo, P.-C., Wei, A., Liu, K.-C., Ko,
    M.-C., Chao, K.-H., Peng, Y.-C. å’Œ Liu, T.-L. (2020)ã€‚ç”¨äºéƒ¨åˆ†æ ‡ç­¾ç»„ç»‡ç—…ç†å›¾åƒåˆ†å‰²çš„è‡ªç›¸ä¼¼å­¦ç”Ÿï¼Œæ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®ï¼ŒSpringerï¼Œç¬¬117â€“132é¡µã€‚
- en: 'Chhipa etÂ al. (2022) Chhipa, P.Â C., Upadhyay, R., Pihlgren, G.Â G., Saini, R.,
    Uchida, S. andÂ Liwicki, M. (2022). Magnification prior: A self-supervised method
    for learning representations on breast cancer histopathological images, arXiv
    preprint arXiv:2203.07707 .'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chhipa ç­‰äºº (2022) Chhipa, P. C., Upadhyay, R., Pihlgren, G. G., Saini, R., Uchida,
    S. å’Œ Liwicki, M. (2022)ã€‚æ”¾å¤§å…ˆéªŒï¼šç”¨äºå­¦ä¹ ä¹³è…ºç™Œç»„ç»‡ç—…ç†å›¾åƒè¡¨ç¤ºçš„è‡ªç›‘ç£æ–¹æ³•ï¼ŒarXiv é¢„å°æœ¬ arXiv:2203.07707ã€‚
- en: Chikontwe etÂ al. (2020) Chikontwe, P., Kim, M., Nam, S.Â J., Go, H. andÂ Park,
    S.Â H. (2020). Multiple instance learning with center embeddings for histopathology
    classification, International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, pp.Â 519â€“528.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chikontwe ç­‰äºº (2020) Chikontwe, P., Kim, M., Nam, S. J., Go, H. å’Œ Park, S. H.
    (2020)ã€‚åŸºäºä¸­å¿ƒåµŒå…¥çš„å¤šå®ä¾‹å­¦ä¹ ç”¨äºç»„ç»‡ç—…ç†å­¦åˆ†ç±»ï¼ŒåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯å›½é™…ä¼šè®®ï¼ŒSpringerï¼Œç¬¬519â€“528é¡µã€‚
- en: 'Chong etÂ al. (2020) Chong, Y., Ding, Y., Yan, Q. andÂ Pan, S. (2020). Graph-based
    semi-supervised learning: A review, Neurocomputing  408:Â 216â€“230.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chong ç­‰äºº (2020) Chong, Y., Ding, Y., Yan, Q. å’Œ Pan, S. (2020)ã€‚åŸºäºå›¾çš„åŠç›‘ç£å­¦ä¹ ï¼šç»¼è¿°ï¼Œç¥ç»è®¡ç®—
    408: 216â€“230ã€‚'
- en: Cifci etÂ al. (2022) Cifci, D., Foersch, S. andÂ Kather, J.Â N. (2022). Artificial
    intelligence to identify genetic alterations in conventional histopathology, The
    Journal of Pathology .
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cifci ç­‰äºº (2022) Cifci, D., Foersch, S. å’Œ Kather, J. N. (2022)ã€‚äººå·¥æ™ºèƒ½è¯†åˆ«å¸¸è§„ç»„ç»‡ç—…ç†å­¦ä¸­çš„é—ä¼ å˜å¼‚ï¼Œã€Šç—…ç†å­¦æ‚å¿—ã€‹ã€‚
- en: Ciga etÂ al. (2022) Ciga, O., Xu, T. andÂ Martel, A.Â L. (2022). Self supervised
    contrastive learning for digital histopathology, Machine Learning with Applications  7:Â 100198.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ciga ç­‰äºº (2022) Ciga, O., Xu, T. å’Œ Martel, A. L. (2022)ã€‚ç”¨äºæ•°å­—ç»„ç»‡ç—…ç†å­¦çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œæœºå™¨å­¦ä¹ åº”ç”¨
    7: 100198ã€‚'
- en: 'Clark etÂ al. (2013) Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J.,
    Koppel, P., Moore, S., Phillips, S., Maffitt, D., Pringle, M. etÂ al. (2013). The
    cancer imaging archive (tcia): maintaining and operating a public information
    repository, Journal of Digital Imaging  26(6):Â 1045â€“1057.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark ç­‰äºº (2013) Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel,
    P., Moore, S., Phillips, S., Maffitt, D., Pringle, M. ç­‰ (2013)ã€‚ç™Œç—‡å½±åƒæ¡£æ¡ˆï¼ˆtciaï¼‰ï¼šç»´æŠ¤å’Œè¿è¥å…¬å…±ä¿¡æ¯åº“ï¼Œæ•°å­—æˆåƒæ‚å¿—
    26(6): 1045â€“1057ã€‚'
- en: Cong etÂ al. (2021) Cong, C., Liu, S., Ieva, A.Â D., Pagnucco, M., Berkovsky,
    S. andÂ Song, Y. (2021). Semi-supervised adversarial learning for stain normalisation
    in histopathology images, International Conference on Medical Image Computing
    and Computer-Assisted Intervention, Springer, pp.Â 581â€“591.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cong ç­‰äºº (2021) Cong, C., Liu, S., Ieva, A. D., Pagnucco, M., Berkovsky, S. å’Œ
    Song, Y. (2021)ã€‚ç”¨äºç»„ç»‡ç—…ç†å›¾åƒæŸ“è‰²æ ‡å‡†åŒ–çš„åŠç›‘ç£å¯¹æŠ—å­¦ä¹ ï¼ŒåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯å›½é™…ä¼šè®®ï¼ŒSpringerï¼Œç¬¬581â€“591é¡µã€‚
- en: Coudray etÂ al. (2018) Coudray, N., Ocampo, P.Â S., Sakellaropoulos, T., Narula,
    N., Snuderl, M., FenyÃ¶, D., Moreira, A.Â L., Razavian, N. andÂ Tsirigos, A. (2018).
    Classification and mutation prediction from nonâ€“small cell lung cancer histopathology
    images using deep learning, Nature Medicine  24(10):Â 1559â€“1567.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Coudray ç­‰äºº (2018) Coudray, N., Ocampo, P. S., Sakellaropoulos, T., Narula,
    N., Snuderl, M., FenyÃ¶, D., Moreira, A. L., Razavian, N. å’Œ Tsirigos, A. (2018)ã€‚ä½¿ç”¨æ·±åº¦å­¦ä¹ ä»éå°ç»†èƒè‚ºç™Œç»„ç»‡ç—…ç†å›¾åƒä¸­è¿›è¡Œåˆ†ç±»å’Œçªå˜é¢„æµ‹ï¼ŒNature
    Medicine 24(10): 1559â€“1567ã€‚'
- en: 'Cruz-Roa etÂ al. (2014) Cruz-Roa, A., Basavanhally, A., GonzÃ¡lez, F., Gilmore,
    H., Feldman, M., Ganesan, S., Shih, N., Tomaszewski, J. andÂ Madabhushi, A. (2014).
    Automatic detection of invasive ductal carcinoma in whole slide images with convolutional
    neural networks, Medical Imaging 2014: Digital Pathology, Vol. 9041, SPIE, p.Â 904103.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cruz-Roa ç­‰äºº (2014) Cruz-Roa, A., Basavanhally, A., GonzÃ¡lez, F., Gilmore, H.,
    Feldman, M., Ganesan, S., Shih, N., Tomaszewski, J. å’Œ Madabhushi, A. (2014)ã€‚ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè‡ªåŠ¨æ£€æµ‹å…¨ç‰‡å›¾åƒä¸­çš„ä¾µè¢­æ€§å¯¼ç®¡ç™Œï¼ŒåŒ»å­¦æˆåƒ2014ï¼šæ•°å­—ç—…ç†å­¦ï¼Œç¬¬9041å·ï¼ŒSPIEï¼Œç¬¬904103é¡µã€‚
- en: 'Cruz-Roa etÂ al. (2017) Cruz-Roa, A., Gilmore, H., Basavanhally, A., Feldman,
    M., Ganesan, S., Shih, N.Â N., Tomaszewski, J., GonzÃ¡lez, F.Â A. andÂ Madabhushi,
    A. (2017). Accurate and reproducible invasive breast cancer detection in whole-slide
    images: a deep learning approach for quantifying tumor extent, Scientific Reports  7(1):Â 1â€“14.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cruz-Roa ç­‰äºº (2017) Cruz-Roa, A., Gilmore, H., Basavanhally, A., Feldman, M.,
    Ganesan, S., Shih, N. N., Tomaszewski, J., GonzÃ¡lez, F. A. å’Œ Madabhushi, A. (2017)ã€‚å…¨ç‰‡å›¾åƒä¸­ä¾µè¢­æ€§ä¹³è…ºç™Œçš„å‡†ç¡®å’Œå¯é‡å¤æ£€æµ‹ï¼šä¸€ç§ç”¨äºé‡åŒ–è‚¿ç˜¤èŒƒå›´çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ŒScientific
    Reports 7(1): 1â€“14ã€‚'
- en: Dai etÂ al. (2017) Dai, Z., Yang, Z., Yang, F., Cohen, W.Â W. andÂ Salakhutdinov,
    R.Â R. (2017). Good semi-supervised learning that requires a bad gan, Advances
    in Neural Information Processing Systems  30.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daiç­‰ï¼ˆ2017ï¼‰Dai, Z., Yang, Z., Yang, F., Cohen, W. W. å’ŒSalakhutdinov, R. R.ï¼ˆ2017ï¼‰ã€‚éœ€è¦ä¸€ä¸ªç³Ÿç³•çš„GANçš„è‰¯å¥½åŠç›‘ç£å­¦ä¹ ï¼Œã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹30ã€‚
- en: Dara etÂ al. (2002) Dara, R., Kremer, S.Â C. andÂ Stacey, D.Â A. (2002). Clustering
    unlabeled data with soms improves classification of labeled real-world data, Proceedings
    of the 2002 International Joint Conference on Neural Networks. IJCNNâ€™02 (Cat.
    No. 02CH37290), Vol.Â 3, IEEE, pp.Â 2237â€“2242.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daraç­‰ï¼ˆ2002ï¼‰Dara, R., Kremer, S. C. å’ŒStacey, D. A.ï¼ˆ2002ï¼‰ã€‚ä½¿ç”¨SOMså¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œèšç±»æ”¹å–„æ ‡è®°çœŸå®æ•°æ®çš„åˆ†ç±»ï¼Œ2002å¹´å›½é™…ç¥ç»ç½‘ç»œè”åˆä¼šè®®è®ºæ–‡é›†ã€‚IJCNNâ€™02ï¼ˆCat.
    No. 02CH37290ï¼‰ï¼Œç¬¬3å·ï¼ŒIEEEï¼Œç¬¬2237â€“2242é¡µã€‚
- en: 'DecenciÃ¨re etÂ al. (2014) DecenciÃ¨re, E., Zhang, X., Cazuguel, G., Lay, B.,
    Cochener, B., Trone, C., Gain, P., Ordonez, R., Massin, P., Erginay, A. etÂ al.
    (2014). Feedback on a publicly distributed image database: the messidor database,
    Image Analysis & Stereology  33(3):Â 231â€“234.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DecenciÃ¨reç­‰ï¼ˆ2014ï¼‰DecenciÃ¨re, E., Zhang, X., Cazuguel, G., Lay, B., Cochener,
    B., Trone, C., Gain, P., Ordonez, R., Massin, P., Erginay, A. ç­‰ï¼ˆ2014ï¼‰ã€‚å¯¹å…¬å¼€åˆ†å‘çš„å›¾åƒæ•°æ®åº“çš„åé¦ˆï¼šMessidoræ•°æ®åº“ï¼Œå›¾åƒåˆ†æä¸ç«‹ä½“è§†è§‰
    33(3)ï¼š231â€“234ã€‚
- en: Dehaene etÂ al. (2020) Dehaene, O., Camara, A., Moindrot, O., deÂ Lavergne, A.
    andÂ Courtiol, P. (2020). Self-supervision closes the gap between weak and strong
    supervision in histology, arXiv preprint arXiv:2012.03583 .
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dehaeneç­‰ï¼ˆ2020ï¼‰Dehaene, O., Camara, A., Moindrot, O., de Lavergne, A. å’ŒCourtiol,
    P.ï¼ˆ2020ï¼‰ã€‚è‡ªç›‘ç£ç¼©å°äº†ç»„ç»‡å­¦ä¸­å¼±ç›‘ç£å’Œå¼ºç›‘ç£ä¹‹é—´çš„å·®è·ï¼ŒarXivé¢„å°æœ¬arXiv:2012.03583ã€‚
- en: Demiriz etÂ al. (1999) Demiriz, A., Bennett, K.Â P. andÂ Embrechts, M.Â J. (1999).
    Semi-supervised clustering using genetic algorithms, Artificial Neural Networks
    in Engineering (ANNIE-99) pp.Â 809â€“814.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demirizç­‰ï¼ˆ1999ï¼‰Demiriz, A., Bennett, K. P. å’ŒEmbrechts, M. J.ï¼ˆ1999ï¼‰ã€‚ä½¿ç”¨é—ä¼ ç®—æ³•çš„åŠç›‘ç£èšç±»ï¼Œäººå·¥ç¥ç»ç½‘ç»œå·¥ç¨‹ï¼ˆANNIE-99ï¼‰ç¬¬809â€“814é¡µã€‚
- en: 'Deng etÂ al. (2009) Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K. andÂ Fei-Fei,
    L. (2009). Imagenet: A large-scale hierarchical image database, 2009 IEEE Conference
    on Computer Vision and Pattern Recognition, Ieee, pp.Â 248â€“255.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dengç­‰ï¼ˆ2009ï¼‰Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K. å’ŒFei-Fei, L.ï¼ˆ2009ï¼‰ã€‚ImageNetï¼šå¤§è§„æ¨¡å±‚æ¬¡åŒ–å›¾åƒæ•°æ®åº“ï¼Œ2009å¹´IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ŒIEEEï¼Œç¬¬248â€“255é¡µã€‚
- en: Ding etÂ al. (2020) Ding, K., Liu, Q., Lee, E., Zhou, M., Lu, A. andÂ Zhang, S.
    (2020). Feature-enhanced graph networks for genetic mutational prediction using
    histopathological images in colon cancer, International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer, pp.Â 294â€“304.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dingç­‰ï¼ˆ2020ï¼‰Ding, K., Liu, Q., Lee, E., Zhou, M., Lu, A. å’ŒZhang, S.ï¼ˆ2020ï¼‰ã€‚ç”¨äºç»“è‚ ç™Œç»„ç»‡ç—…ç†å›¾åƒçš„é—ä¼ çªå˜é¢„æµ‹çš„ç‰¹å¾å¢å¼ºå›¾ç½‘ç»œï¼ŒåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ï¼ŒSpringerï¼Œç¬¬294â€“304é¡µã€‚
- en: Doersch etÂ al. (2015) Doersch, C., Gupta, A. andÂ Efros, A.Â A. (2015). Unsupervised
    visual representation learning by context prediction, Proceedings of the IEEE
    International Conference on Computer Vision, pp.Â 1422â€“1430.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doerschç­‰ï¼ˆ2015ï¼‰Doersch, C., Gupta, A. å’ŒEfros, A. A.ï¼ˆ2015ï¼‰ã€‚é€šè¿‡ä¸Šä¸‹æ–‡é¢„æµ‹è¿›è¡Œæ— ç›‘ç£è§†è§‰è¡¨ç¤ºå­¦ä¹ ï¼ŒIEEEå›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬1422â€“1430é¡µã€‚
- en: Donahue etÂ al. (2016) Donahue, J., KrÃ¤henbÃ¼hl, P. andÂ Darrell, T. (2016). Adversarial
    feature learning, arXiv preprint arXiv:1605.09782 .
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Donahueç­‰ï¼ˆ2016ï¼‰Donahue, J., KrÃ¤henbÃ¼hl, P. å’ŒDarrell, T.ï¼ˆ2016ï¼‰ã€‚å¯¹æŠ—æ€§ç‰¹å¾å­¦ä¹ ï¼ŒarXivé¢„å°æœ¬arXiv:1605.09782ã€‚
- en: 'Dong etÂ al. (2021) Dong, X., Bao, J., Zhang, T., Chen, D., Zhang, W., Yuan,
    L., Chen, D., Wen, F. andÂ Yu, N. (2021). Peco: Perceptual codebook for bert pre-training
    of vision transformers, arXiv preprint arXiv:2111.12710 .'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dongç­‰ï¼ˆ2021ï¼‰Dong, X., Bao, J., Zhang, T., Chen, D., Zhang, W., Yuan, L., Chen,
    D., Wen, F. å’ŒYu, N.ï¼ˆ2021ï¼‰ã€‚PECOï¼šç”¨äºBERTé¢„è®­ç»ƒè§†è§‰å˜æ¢å™¨çš„æ„ŸçŸ¥ç¼–ç æœ¬ï¼ŒarXivé¢„å°æœ¬arXiv:2111.12710ã€‚
- en: 'Doyle etÂ al. (2007) Doyle, S., Hwang, M., Shah, K., Madabhushi, A., Feldman,
    M. andÂ Tomaszeweski, J. (2007). Automated grading of prostate cancer using architectural
    and textural image features, 2007 4th IEEE International Symposium on Biomedical
    Imaging: From Nano to Macro, IEEE, pp.Â 1284â€“1287.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doyleç­‰ï¼ˆ2007ï¼‰Doyle, S., Hwang, M., Shah, K., Madabhushi, A., Feldman, M. å’ŒTomaszeweski,
    J.ï¼ˆ2007ï¼‰ã€‚åˆ©ç”¨å»ºç­‘å­¦å’Œçº¹ç†å›¾åƒç‰¹å¾å¯¹å‰åˆ—è…ºç™Œè¿›è¡Œè‡ªåŠ¨è¯„åˆ†ï¼Œ2007å¹´ç¬¬4å±ŠIEEEå›½é™…ç”Ÿç‰©åŒ»å­¦æˆåƒç ”è®¨ä¼šï¼šä»çº³ç±³åˆ°å®è§‚ï¼ŒIEEEï¼Œç¬¬1284â€“1287é¡µã€‚
- en: Doyle etÂ al. (2006) Doyle, S., Rodriguez, C., Madabhushi, A., Tomaszeweski,
    J. andÂ Feldman, M. (2006). Detecting prostatic adenocarcinoma from digitized histology
    using a multi-scale hierarchical classification approach, 2006 International Conference
    of the IEEE Engineering in Medicine and Biology Society, IEEE, pp.Â 4759â€“4762.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doyleç­‰ï¼ˆ2006ï¼‰Doyle, S., Rodriguez, C., Madabhushi, A., Tomaszeweski, J. å’ŒFeldman,
    M.ï¼ˆ2006ï¼‰ã€‚ä½¿ç”¨å¤šå°ºåº¦åˆ†å±‚åˆ†ç±»æ–¹æ³•ä»æ•°å­—åŒ–ç»„ç»‡å­¦ä¸­æ£€æµ‹å‰åˆ—è…ºè…ºç™Œï¼Œ2006å¹´IEEEåŒ»å­¦ä¸ç”Ÿç‰©å·¥ç¨‹å­¦ä¼šå›½é™…ä¼šè®®ï¼ŒIEEEï¼Œç¬¬4759â€“4762é¡µã€‚
- en: EhteshamiÂ Bejnordi etÂ al. (2018) EhteshamiÂ Bejnordi, B., Mullooly, M., Pfeiffer,
    R.Â M., Fan, S., Vacek, P.Â M., Weaver, D.Â L., Herschorn, S., Brinton, L.Â A., van
    Ginneken, B., Karssemeijer, N. etÂ al. (2018). Using deep convolutional neural
    networks to identify and classify tumor-associated stroma in diagnostic breast
    biopsies, Modern Pathology  31(10):Â 1502â€“1512.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ehteshami Bejnordi ç­‰ (2018) Ehteshami Bejnordi, B., Mullooly, M., Pfeiffer,
    R. M., Fan, S., Vacek, P. M., Weaver, D. L., Herschorn, S., Brinton, L. A., van
    Ginneken, B., Karssemeijer, N. ç­‰ (2018). ä½¿ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œè¯†åˆ«å’Œåˆ†ç±»è¯Šæ–­ä¹³è…ºæ´»æ£€ä¸­çš„è‚¿ç˜¤ç›¸å…³åŸºè´¨ï¼Œç°ä»£ç—…ç†å­¦ 31(10):
    1502â€“1512ã€‚'
- en: Erhan etÂ al. (2010) Erhan, D., Courville, A., Bengio, Y. andÂ Vincent, P. (2010).
    Why does unsupervised pre-training help deep learning?, Proceedings of the Thirteenth
    International Conference on Artificial Intelligence and Statistics, JMLR Workshop
    and Conference Proceedings, pp.Â 201â€“208.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erhan ç­‰ (2010) Erhan, D., Courville, A., Bengio, Y. å’Œ Vincent, P. (2010). ä¸ºä»€ä¹ˆæ— ç›‘ç£é¢„è®­ç»ƒæœ‰åŠ©äºæ·±åº¦å­¦ä¹ ï¼Ÿï¼Œç¬¬åä¸‰å±Šå›½é™…äººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ä¼šè®®è®ºæ–‡é›†ï¼ŒJMLRç ”è®¨ä¼šå’Œä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬201â€“208é¡µã€‚
- en: Feng andÂ Zhou (2017) Feng, J. andÂ Zhou, Z.-H. (2017). Deep miml network, Proceedings
    of the AAAI Conference on Artificial Intelligence, Vol.Â 31.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng å’Œ Zhou (2017) Feng, J. å’Œ Zhou, Z.-H. (2017). æ·±åº¦MIMLç½‘ç»œï¼Œç¬¬31å±ŠAAAIäººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬31å·ã€‚
- en: 'Foucart etÂ al. (2019) Foucart, A., Debeir, O. andÂ Decaestecker, C. (2019).
    Snow: Semi-supervised, noisy and/or weak data for deep learning in digital pathology,
    2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), IEEE,
    pp.Â 1869â€“1872.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Foucart ç­‰ (2019) Foucart, A., Debeir, O. å’Œ Decaestecker, C. (2019). Snow: ç”¨äºæ•°å­—ç—…ç†å­¦çš„åŠç›‘ç£ã€å™ªå£°å’Œ/æˆ–å¼±æ•°æ®çš„æ·±åº¦å­¦ä¹ ï¼Œ2019å¹´ç¬¬16å±ŠIEEEç”Ÿç‰©åŒ»å­¦æˆåƒå›½é™…ç ”è®¨ä¼š
    (ISBI 2019)ï¼ŒIEEEï¼Œç¬¬1869â€“1872é¡µã€‚'
- en: Fu etÂ al. (2020) Fu, Y., Jung, A.Â W., Torne, R.Â V., Gonzalez, S., VÃ¶hringer,
    H., Shmatko, A., Yates, L.Â R., Jimenez-Linan, M., Moore, L. andÂ Gerstung, M. (2020).
    Pan-cancer computational histopathology reveals mutations, tumor composition and
    prognosis, Nature Cancer  1(8):Â 800â€“810.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu ç­‰ (2020) Fu, Y., Jung, A. W., Torne, R. V., Gonzalez, S., VÃ¶hringer, H.,
    Shmatko, A., Yates, L. R., Jimenez-Linan, M., Moore, L. å’Œ Gerstung, M. (2020).
    æ³›ç™Œè®¡ç®—ç»„ç»‡ç—…ç†å­¦æ­ç¤ºäº†çªå˜ã€è‚¿ç˜¤ç»„æˆå’Œé¢„åï¼Œè‡ªç„¶ç™Œç—‡ 1(8): 800â€“810ã€‚'
- en: Gelasca etÂ al. (2008) Gelasca, E.Â D., Byun, J., Obara, B. andÂ Manjunath, B.
    (2008). Evaluation and benchmark for biological image segmentation, 2008 15th
    IEEE International Conference on Image Processing, IEEE, pp.Â 1816â€“1819.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gelasca ç­‰ (2008) Gelasca, E. D., Byun, J., Obara, B. å’Œ Manjunath, B. (2008).
    ç”Ÿç‰©å›¾åƒåˆ†å‰²çš„è¯„ä¼°å’ŒåŸºå‡†ï¼Œ2008å¹´ç¬¬15å±ŠIEEEå›½é™…å›¾åƒå¤„ç†ä¼šè®®ï¼ŒIEEEï¼Œç¬¬1816â€“1819é¡µã€‚
- en: Gertych etÂ al. (2015) Gertych, A., Ing, N., Ma, Z., Fuchs, T.Â J., Salman, S.,
    Mohanty, S., Bhele, S., VelÃ¡squez-Vacca, A., Amin, M.Â B. andÂ Knudsen, B.Â S. (2015).
    Machine learning approaches to analyze histological images of tissues from radical
    prostatectomies, Computerized Medical Imaging and Graphics  46:Â 197â€“208.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gertych ç­‰ (2015) Gertych, A., Ing, N., Ma, Z., Fuchs, T. J., Salman, S., Mohanty,
    S., Bhele, S., VelÃ¡squez-Vacca, A., Amin, M. B. å’Œ Knudsen, B. S. (2015). åˆ†ææ ¹æ²»æ€§å‰åˆ—è…ºåˆ‡é™¤æœ¯ç»„ç»‡çš„ç»„ç»‡å­¦å›¾åƒçš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œè®¡ç®—æœºåŒ–åŒ»å­¦æˆåƒä¸å›¾å½¢
    46: 197â€“208ã€‚'
- en: Gidaris etÂ al. (2018) Gidaris, S., Singh, P. andÂ Komodakis, N. (2018). Unsupervised
    representation learning by predicting image rotations, arXiv preprint arXiv:1803.07728
    .
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gidaris ç­‰ (2018) Gidaris, S., Singh, P. å’Œ Komodakis, N. (2018). é€šè¿‡é¢„æµ‹å›¾åƒæ—‹è½¬è¿›è¡Œæ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼ŒarXivé¢„å°æœ¬
    arXiv:1803.07728ã€‚
- en: Goldberg etÂ al. (2009) Goldberg, A., Zhu, X., Singh, A., Xu, Z. andÂ Nowak, R.
    (2009). Multi-manifold semi-supervised learning, Artificial Intelligence and Statistics,
    PMLR, pp.Â 169â€“176.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldberg ç­‰ (2009) Goldberg, A., Zhu, X., Singh, A., Xu, Z. å’Œ Nowak, R. (2009).
    å¤šæµå½¢åŠç›‘ç£å­¦ä¹ ï¼Œäººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ï¼ŒPMLRï¼Œç¬¬169â€“176é¡µã€‚
- en: 'Goodfellow (2016) Goodfellow, I. (2016). Nips 2016 tutorial: Generative adversarial
    networks, arXiv preprint arXiv:1701.00160 .'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow (2016) Goodfellow, I. (2016). Nips 2016æ•™ç¨‹ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ŒarXivé¢„å°æœ¬ arXiv:1701.00160ã€‚
- en: Goodfellow etÂ al. (2016) Goodfellow, I., Bengio, Y., Courville, A. andÂ Bengio,
    Y. (2016). Deep learning, volume 1.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow ç­‰ (2016) Goodfellow, I., Bengio, Y., Courville, A. å’Œ Bengio, Y. (2016).
    æ·±åº¦å­¦ä¹ ï¼Œç¬¬1å·ã€‚
- en: Goodfellow etÂ al. (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., Courville, A. andÂ Bengio, Y. (2014). Generative adversarial
    nets, Advances in Neural Information Processing Systems  27.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow ç­‰ (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,
    D., Ozair, S., Courville, A. å’Œ Bengio, Y. (2014). ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±• 27ã€‚
- en: Grill etÂ al. (2020) Grill, J.-B., Strub, F., AltchÃ©, F., Tallec, C., Richemond,
    P., Buchatskaya, E., Doersch, C., AvilaÂ Pires, B., Guo, Z., GheshlaghiÂ Azar, M.
    etÂ al. (2020). Bootstrap your own latent-a new approach to self-supervised learning,
    Advances in Neural Information Processing Systems  33:Â 21271â€“21284.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Grillç­‰äººï¼ˆ2020ï¼‰Grill, J.-B., Strub, F., AltchÃ©, F., Tallec, C., Richemond, P.,
    Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M.ç­‰ï¼ˆ2020ï¼‰ã€‚è‡ªä¸¾è‡ªå·±çš„æ½œåœ¨ç©ºé—´â€”â€”ä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œ*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*
    33: 21271â€“21284ã€‚'
- en: Gu etÂ al. (2018) Gu, F., Burlutskiy, N., Andersson, M. andÂ WilÃ©n, L.Â K. (2018).
    Multi-resolution networks for semantic segmentation in whole slide images, Computational
    Pathology and Ophthalmic Medical Image Analysis, Springer, pp.Â 11â€“18.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guç­‰äººï¼ˆ2018ï¼‰Gu, F., Burlutskiy, N., Andersson, M. å’Œ WilÃ©n, L. K.ï¼ˆ2018ï¼‰ã€‚ç”¨äºå…¨å¹»ç¯ç‰‡å›¾åƒçš„å¤šåˆ†è¾¨ç‡ç½‘ç»œè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œ*è®¡ç®—ç—…ç†å­¦ä¸çœ¼ç§‘åŒ»å­¦å›¾åƒåˆ†æ*ï¼ŒSpringerï¼Œç¬¬11â€“18é¡µã€‚
- en: Guinney etÂ al. (2015) Guinney, J., Dienstmann, R., Wang, X., DeÂ Reynies, A.,
    Schlicker, A., Soneson, C., Marisa, L., Roepman, P., Nyamundanda, G., Angelino,
    P. etÂ al. (2015). The consensus molecular subtypes of colorectal cancer, Nature
    Medicine  21(11):Â 1350â€“1356.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guinneyç­‰äººï¼ˆ2015ï¼‰Guinney, J., Dienstmann, R., Wang, X., De Reynies, A., Schlicker,
    A., Soneson, C., Marisa, L., Roepman, P., Nyamundanda, G., Angelino, P. ç­‰ï¼ˆ2015ï¼‰ã€‚ç»“ç›´è‚ ç™Œçš„å…±è¯†åˆ†å­äºšå‹ï¼Œ*è‡ªç„¶åŒ»å­¦*
    21(11): 1350â€“1356ã€‚'
- en: 'Gurcan etÂ al. (2009) Gurcan, M.Â N., Boucheron, L.Â E., Can, A., Madabhushi,
    A., Rajpoot, N.Â M. andÂ Yener, B. (2009). Histopathological image analysis: A review,
    IEEE Reviews in Biomedical Engineering  2:Â 147â€“171.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gurcanç­‰äººï¼ˆ2009ï¼‰Gurcan, M. N., Boucheron, L. E., Can, A., Madabhushi, A., Rajpoot,
    N. M. å’Œ Yener, B.ï¼ˆ2009ï¼‰ã€‚ç»„ç»‡ç—…ç†å›¾åƒåˆ†æï¼šç»¼è¿°ï¼Œ*IEEEç”Ÿç‰©åŒ»å­¦å·¥ç¨‹è¯„è®º* 2: 147â€“171ã€‚'
- en: Haeusser etÂ al. (2017) Haeusser, P., Mordvintsev, A. andÂ Cremers, D. (2017).
    Learning by associationâ€“a versatile semi-supervised training method for neural
    networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp.Â 89â€“98.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haeusserç­‰äººï¼ˆ2017ï¼‰Haeusser, P., Mordvintsev, A. å’Œ Cremers, D.ï¼ˆ2017ï¼‰ã€‚é€šè¿‡å…³è”å­¦ä¹ â€”â€”ä¸€ç§é€šç”¨çš„åŠç›‘ç£ç¥ç»ç½‘ç»œè®­ç»ƒæ–¹æ³•ï¼Œ*IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬89â€“98é¡µã€‚
- en: Halicek etÂ al. (2019) Halicek, M., Shahedi, M., Little, J.Â V., Chen, A.Â Y.,
    Myers, L.Â L., Sumer, B.Â D. andÂ Fei, B. (2019). Head and neck cancer detection
    in digitized whole-slide histology using convolutional neural networks, Scientific
    Reports  9(1):Â 1â€“11.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Halicekç­‰äººï¼ˆ2019ï¼‰Halicek, M., Shahedi, M., Little, J. V., Chen, A. Y., Myers,
    L. L., Sumer, B. D. å’Œ Fei, B.ï¼ˆ2019ï¼‰ã€‚ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œæ•°å­—åŒ–å…¨å¹»ç¯ç‰‡ç»„ç»‡å­¦ä¸­çš„å¤´é¢ˆç™Œæ£€æµ‹ï¼Œ*ç§‘å­¦æŠ¥å‘Š* 9(1): 1â€“11ã€‚'
- en: Hashimoto etÂ al. (2020) Hashimoto, N., Fukushima, D., Koga, R., Takagi, Y.,
    Ko, K., Kohno, K., Nakaguro, M., Nakamura, S., Hontani, H. andÂ Takeuchi, I. (2020).
    Multi-scale domain-adversarial multiple-instance cnn for cancer subtype classification
    with unannotated histopathological images, Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp.Â 3852â€“3861.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hashimotoç­‰äººï¼ˆ2020ï¼‰Hashimoto, N., Fukushima, D., Koga, R., Takagi, Y., Ko, K.,
    Kohno, K., Nakaguro, M., Nakamura, S., Hontani, H. å’Œ Takeuchi, I.ï¼ˆ2020ï¼‰ã€‚ç”¨äºç™Œç—‡äºšå‹åˆ†ç±»çš„å¤šå°ºåº¦é¢†åŸŸå¯¹æŠ—å¤šå®ä¾‹CNNï¼Œ*IEEE/CVFè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬3852â€“3861é¡µã€‚
- en: He etÂ al. (2021) He, K., Chen, X., Xie, S., Li, Y., DollÃ¡r, P. andÂ Girshick,
    R. (2021). Masked autoencoders are scalable vision learners, arXiv preprint arXiv:2111.06377
    .
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heç­‰äººï¼ˆ2021ï¼‰He, K., Chen, X., Xie, S., Li, Y., DollÃ¡r, P. å’Œ Girshick, R.ï¼ˆ2021ï¼‰ã€‚æ©ç è‡ªç¼–ç å™¨æ˜¯å¯æ‰©å±•çš„è§†è§‰å­¦ä¹ è€…ï¼Œ*arXivé¢„å°æœ¬*
    arXiv:2111.06377ã€‚
- en: He etÂ al. (2020) He, K., Fan, H., Wu, Y., Xie, S. andÂ Girshick, R. (2020). Momentum
    contrast for unsupervised visual representation learning, Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp.Â 9729â€“9738.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heç­‰äººï¼ˆ2020ï¼‰He, K., Fan, H., Wu, Y., Xie, S. å’Œ Girshick, R.ï¼ˆ2020ï¼‰ã€‚åŠ¨é‡å¯¹æ¯”ç”¨äºæ— ç›‘ç£è§†è§‰è¡¨å¾å­¦ä¹ ï¼Œ*IEEE/CVFè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬9729â€“9738é¡µã€‚
- en: Hou etÂ al. (2019) Hou, L., Nguyen, V., Kanevsky, A.Â B., Samaras, D., Kurc, T.Â M.,
    Zhao, T., Gupta, R.Â R., Gao, Y., Chen, W., Foran, D. etÂ al. (2019). Sparse autoencoder
    for unsupervised nucleus detection and representation in histopathology images,
    Pattern Recognition  86:Â 188â€“200.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Houç­‰äººï¼ˆ2019ï¼‰Hou, L., Nguyen, V., Kanevsky, A. B., Samaras, D., Kurc, T. M.,
    Zhao, T., Gupta, R. R., Gao, Y., Chen, W., Foran, D.ç­‰ï¼ˆ2019ï¼‰ã€‚ç”¨äºæ— ç›‘ç£ç»†èƒæ ¸æ£€æµ‹å’Œè¡¨å¾çš„ç¨€ç–è‡ªç¼–ç å™¨ï¼Œ*æ¨¡å¼è¯†åˆ«*
    86: 188â€“200ã€‚'
- en: Hou etÂ al. (2016) Hou, L., Samaras, D., Kurc, T.Â M., Gao, Y., Davis, J.Â E. andÂ Saltz,
    J.Â H. (2016). Patch-based convolutional neural network for whole slide tissue
    image classification, Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pp.Â 2424â€“2433.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houç­‰äººï¼ˆ2016ï¼‰Hou, L., Samaras, D., Kurc, T. M., Gao, Y., Davis, J. E. å’Œ Saltz,
    J. H.ï¼ˆ2016ï¼‰ã€‚åŸºäºå—çš„å·ç§¯ç¥ç»ç½‘ç»œç”¨äºå…¨å¹»ç¯ç‰‡ç»„ç»‡å›¾åƒåˆ†ç±»ï¼Œ*IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬2424â€“2433é¡µã€‚
- en: Ilse etÂ al. (2018) Ilse, M., Tomczak, J. andÂ Welling, M. (2018). Attention-based
    deep multiple instance learning, International Conference on Machine Learning,
    PMLR, pp.Â 2127â€“2136.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilse ç­‰ï¼ˆ2018ï¼‰ Ilse, M., Tomczak, J. å’Œ Welling, M.ï¼ˆ2018ï¼‰ã€‚åŸºäºæ³¨æ„åŠ›çš„æ·±åº¦å¤šå®ä¾‹å­¦ä¹ ï¼Œã€Šæœºå™¨å­¦ä¹ å›½é™…ä¼šè®®ã€‹ï¼ŒPMLRï¼Œç¬¬
    2127â€“2136 é¡µã€‚
- en: Jafari-Khouzani andÂ Soltanian-Zadeh (2003) Jafari-Khouzani, K. andÂ Soltanian-Zadeh,
    H. (2003). Multiwavelet grading of pathological images of prostate, IEEE Transactions
    on Biomedical Engineering  50(6):Â 697â€“704.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jafari-Khouzani å’Œ Soltanian-Zadehï¼ˆ2003ï¼‰ Jafari-Khouzani, K. å’Œ Soltanian-Zadeh,
    H.ï¼ˆ2003ï¼‰ã€‚å‰åˆ—è…ºç—…ç†å›¾åƒçš„å¤šå°æ³¢åˆ†çº§ï¼Œã€ŠIEEE ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹å­¦æŠ¥ã€‹ 50(6): 697â€“704ã€‚'
- en: Jaiswal etÂ al. (2019) Jaiswal, A.Â K., Panshin, I., Shulkin, D., Aneja, N. andÂ Abramov,
    S. (2019). Semi-supervised learning for cancer detection of lymph node metastases,
    arXiv preprint arXiv:1906.09587 .
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal ç­‰ï¼ˆ2019ï¼‰ Jaiswal, A. K., Panshin, I., Shulkin, D., Aneja, N. å’Œ Abramov,
    S.ï¼ˆ2019ï¼‰ã€‚ç”¨äºç™Œç—‡æ£€æµ‹çš„åŠç›‘ç£å­¦ä¹ æ·‹å·´ç»“è½¬ç§»ï¼Œã€ŠarXiv é¢„å°æœ¬ arXiv:1906.09587ã€‹ã€‚
- en: Kandemir etÂ al. (2014) Kandemir, M., Zhang, C. andÂ Hamprecht, F.Â A. (2014).
    Empowering multiple instance histopathology cancer diagnosis by cell graphs, International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer,
    pp.Â 228â€“235.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kandemir ç­‰ï¼ˆ2014ï¼‰ Kandemir, M., Zhang, C. å’Œ Hamprecht, F. A.ï¼ˆ2014ï¼‰ã€‚é€šè¿‡ç»†èƒå›¾æå‡å¤šå®ä¾‹ç»„ç»‡ç—…ç†å­¦ç™Œç—‡è¯Šæ–­ï¼Œã€ŠåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬
    228â€“235 é¡µã€‚
- en: Kandoth etÂ al. (2013) Kandoth, C., McLellan, M.Â D., Vandin, F., Ye, K., Niu,
    B., Lu, C., Xie, M., Zhang, Q., McMichael, J.Â F., Wyczalkowski, M.Â A. etÂ al. (2013).
    Mutational landscape and significance across 12 major cancer types, Nature  502(7471):Â 333â€“339.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kandoth ç­‰ï¼ˆ2013ï¼‰ Kandoth, C., McLellan, M. D., Vandin, F., Ye, K., Niu, B.,
    Lu, C., Xie, M., Zhang, Q., McMichael, J. F., Wyczalkowski, M. A. ç­‰ï¼ˆ2013ï¼‰ã€‚12 ç§ä¸»è¦ç™Œç—‡ç±»å‹çš„çªå˜æ™¯è§‚åŠå…¶æ„ä¹‰ï¼Œã€Šè‡ªç„¶ã€‹
    502(7471): 333â€“339ã€‚'
- en: Kapil etÂ al. (2018) Kapil, A., Meier, A., Zuraw, A., Steele, K.Â E., Rebelatto,
    M.Â C., Schmidt, G. andÂ Brieu, N. (2018). Deep semi supervised generative learning
    for automated tumor proportion scoring on nsclc tissue needle biopsies, Scientific
    Reports  8(1):Â 1â€“10.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kapil ç­‰ï¼ˆ2018ï¼‰ Kapil, A., Meier, A., Zuraw, A., Steele, K. E., Rebelatto, M.
    C., Schmidt, G. å’Œ Brieu, N.ï¼ˆ2018ï¼‰ã€‚ç”¨äº nsclc ç»„ç»‡é’ˆåˆºæ´»æ£€çš„æ·±åº¦åŠç›‘ç£ç”Ÿæˆå­¦ä¹ è‡ªåŠ¨è‚¿ç˜¤æ¯”ä¾‹è¯„åˆ†ï¼Œã€Šç§‘å­¦æŠ¥å‘Šã€‹ 8(1):
    1â€“10ã€‚'
- en: Kather etÂ al. (2020) Kather, J.Â N., Heij, L.Â R., Grabsch, H.Â I., Loeffler, C.,
    Echle, A., Muti, H.Â S., Krause, J., Niehues, J.Â M., Sommer, K.Â A., Bankhead, P.
    etÂ al. (2020). Pan-cancer image-based detection of clinically actionable genetic
    alterations, Nature Cancer  1(8):Â 789â€“799.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kather ç­‰ï¼ˆ2020ï¼‰ Kather, J. N., Heij, L. R., Grabsch, H. I., Loeffler, C., Echle,
    A., Muti, H. S., Krause, J., Niehues, J. M., Sommer, K. A., Bankhead, P. ç­‰ï¼ˆ2020ï¼‰ã€‚åŸºäºå›¾åƒçš„å…¨ç™Œç§ä¸´åºŠå¯æ“ä½œåŸºå› å˜å¼‚æ£€æµ‹ï¼Œã€Šè‡ªç„¶ç™Œç—‡ã€‹
    1(8): 789â€“799ã€‚'
- en: 'Kather, Krisam, Charoentong, Luedde, Herpel, Weis, Gaiser, Marx, Valous, Ferber
    etÂ al. (2019) Kather, J.Â N., Krisam, J., Charoentong, P., Luedde, T., Herpel,
    E., Weis, C.-A., Gaiser, T., Marx, A., Valous, N.Â A., Ferber, D. etÂ al. (2019).
    Predicting survival from colorectal cancer histology slides using deep learning:
    A retrospective multicenter study, PLoS Medicine  16(1):Â e1002730.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kather, Krisam, Charoentong, Luedde, Herpel, Weis, Gaiser, Marx, Valous, Ferber
    ç­‰ï¼ˆ2019ï¼‰ Kather, J. N., Krisam, J., Charoentong, P., Luedde, T., Herpel, E., Weis,
    C.-A., Gaiser, T., Marx, A., Valous, N. A., Ferber, D. ç­‰ï¼ˆ2019ï¼‰ã€‚é€šè¿‡æ·±åº¦å­¦ä¹ é¢„æµ‹ç»“ç›´è‚ ç™Œç»„ç»‡å­¦åˆ‡ç‰‡çš„ç”Ÿå­˜æœŸï¼šä¸€é¡¹å›é¡¾æ€§å¤šä¸­å¿ƒç ”ç©¶ï¼Œã€ŠPLoS
    åŒ»å­¦ã€‹ 16(1): e1002730ã€‚'
- en: Kather, Pearson, Halama, JÃ¤ger, Krause, Loosen, Marx, Boor, Tacke, Neumann etÂ al.
    (2019) Kather, J.Â N., Pearson, A.Â T., Halama, N., JÃ¤ger, D., Krause, J., Loosen,
    S.Â H., Marx, A., Boor, P., Tacke, F., Neumann, U.Â P. etÂ al. (2019). Deep learning
    can predict microsatellite instability directly from histology in gastrointestinal
    cancer, Nature Medicine  25(7):Â 1054â€“1056.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kather, Pearson, Halama, JÃ¤ger, Krause, Loosen, Marx, Boor, Tacke, Neumann
    ç­‰ï¼ˆ2019ï¼‰ Kather, J. N., Pearson, A. T., Halama, N., JÃ¤ger, D., Krause, J., Loosen,
    S. H., Marx, A., Boor, P., Tacke, F., Neumann, U. P. ç­‰ï¼ˆ2019ï¼‰ã€‚æ·±åº¦å­¦ä¹ å¯ä»¥ç›´æ¥ä»èƒƒè‚ ç™Œçš„ç»„ç»‡å­¦ä¸­é¢„æµ‹å¾®å«æ˜Ÿä¸ç¨³å®šæ€§ï¼Œã€Šè‡ªç„¶åŒ»å­¦ã€‹
    25(7): 1054â€“1056ã€‚'
- en: Kather etÂ al. (2016) Kather, J.Â N., Weis, C.-A., Bianconi, F., Melchers, S.Â M.,
    Schad, L.Â R., Gaiser, T., Marx, A. andÂ ZÃ¶llner, F.Â G. (2016). Multi-class texture
    analysis in colorectal cancer histology, Scientific Reports  6(1):Â 1â€“11.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kather ç­‰ï¼ˆ2016ï¼‰ Kather, J. N., Weis, C.-A., Bianconi, F., Melchers, S. M., Schad,
    L. R., Gaiser, T., Marx, A. å’Œ ZÃ¶llner, F. G.ï¼ˆ2016ï¼‰ã€‚ç»“ç›´è‚ ç™Œç»„ç»‡å­¦ä¸­çš„å¤šç±»çº¹ç†åˆ†æï¼Œã€Šç§‘å­¦æŠ¥å‘Šã€‹ 6(1):
    1â€“11ã€‚'
- en: Kingma andÂ Welling (2013) Kingma, D.Â P. andÂ Welling, M. (2013). Auto-encoding
    variational bayes, arXiv preprint arXiv:1312.6114 .
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma å’Œ Wellingï¼ˆ2013ï¼‰ Kingma, D. P. å’Œ Welling, M.ï¼ˆ2013ï¼‰ã€‚è‡ªç¼–ç å˜åˆ†è´å¶æ–¯ï¼ŒarXiv é¢„å°æœ¬
    arXiv:1312.6114ã€‚
- en: Kleppe etÂ al. (2021) Kleppe, A., Skrede, O.-J., DeÂ Raedt, S., LiestÃ¸l, K., Kerr,
    D.Â J. andÂ Danielsen, H.Â E. (2021). Designing deep learning studies in cancer diagnostics,
    Nature Reviews Cancer  21(3):Â 199â€“211.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kleppe ç­‰ï¼ˆ2021ï¼‰ Kleppe, A., Skrede, O.-J., De Raedt, S., LiestÃ¸l, K., Kerr,
    D. J. å’Œ Danielsen, H. E.ï¼ˆ2021ï¼‰ã€‚ç™Œç—‡è¯Šæ–­ä¸­çš„æ·±åº¦å­¦ä¹ ç ”ç©¶è®¾è®¡ï¼Œã€Šè‡ªç„¶è¯„è®ºç™Œç—‡ã€‹ 21(3): 199â€“211ã€‚'
- en: 'Koohbanani etÂ al. (2021) Koohbanani, N.Â A., Unnikrishnan, B., Khurram, S.Â A.,
    Krishnaswamy, P. andÂ Rajpoot, N. (2021). Self-path: Self-supervision for classification
    of pathology images with limited annotations, IEEE Transactions on Medical Imaging  40(10):Â 2845â€“2856.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Koohbanani et al. (2021) Koohbanani, N. A., Unnikrishnan, B., Khurram, S. A.,
    Krishnaswamy, P. å’Œ Rajpoot, N. (2021). Self-pathï¼šæœ‰é™æ³¨é‡Šçš„ç—…ç†å›¾åƒåˆ†ç±»çš„è‡ªç›‘ç£ï¼Œã€ŠIEEE åŒ»å­¦æˆåƒæ±‡åˆŠã€‹
    40(10): 2845â€“2856ã€‚'
- en: Kraus etÂ al. (2016) Kraus, O.Â Z., Ba, J.Â L. andÂ Frey, B.Â J. (2016). Classifying
    and segmenting microscopy images with deep multiple instance learning, Bioinformatics  32(12):Â i52â€“i59.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kraus et al. (2016) Kraus, O. Z., Ba, J. L. å’Œ Frey, B. J. (2016). ä½¿ç”¨æ·±åº¦å¤šå®ä¾‹å­¦ä¹ è¿›è¡Œæ˜¾å¾®é•œå›¾åƒçš„åˆ†ç±»å’Œåˆ†å‰²ï¼Œã€Šç”Ÿç‰©ä¿¡æ¯å­¦ã€‹
    32(12): i52â€“i59ã€‚'
- en: Kumar etÂ al. (2017) Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane,
    A. andÂ Sethi, A. (2017). A dataset and a technique for generalized nuclear segmentation
    for computational pathology, IEEE Transactions on Medical Imaging  36(7):Â 1550â€“1560.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kumar et al. (2017) Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane,
    A. å’Œ Sethi, A. (2017). ä¸€ç§ç”¨äºè®¡ç®—ç—…ç†å­¦çš„å¹¿ä¹‰æ ¸åˆ†å‰²çš„æ•°æ®é›†å’ŒæŠ€æœ¯ï¼Œã€ŠIEEE åŒ»å­¦æˆåƒæ±‡åˆŠã€‹ 36(7): 1550â€“1560ã€‚'
- en: Laine andÂ Aila (2016) Laine, S. andÂ Aila, T. (2016). Temporal ensembling for
    semi-supervised learning, arXiv preprint arXiv:1610.02242 .
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laine å’Œ Aila (2016) Laine, S. å’Œ Aila, T. (2016). åŠç›‘ç£å­¦ä¹ çš„æ—¶é—´é›†åˆï¼Œã€ŠarXiv é¢„å°æœ¬ arXiv:1610.02242ã€‹ã€‚
- en: Larsen etÂ al. (2016) Larsen, A. B.Â L., SÃ¸nderby, S.Â K., Larochelle, H. andÂ Winther,
    O. (2016). Autoencoding beyond pixels using a learned similarity metric, International
    Conference on Machine Learning, PMLR, pp.Â 1558â€“1566.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Larsen et al. (2016) Larsen, A. B. L., SÃ¸nderby, S. K., Larochelle, H. å’Œ Winther,
    O. (2016). ä½¿ç”¨å­¦ä¹ ç›¸ä¼¼æ€§åº¦é‡çš„åƒç´ è¶…ç¼–ç ï¼Œã€Šæœºå™¨å­¦ä¹ å›½é™…ä¼šè®®ã€‹ï¼ŒPMLRï¼Œé¡µç ï¼š1558â€“1566ã€‚
- en: 'Lee etÂ al. (2013) Lee, D.-H. etÂ al. (2013). Pseudo-label: The simple and efficient
    semi-supervised learning method for deep neural networks, Workshop on Challenges
    in Representation Learning, ICML, Vol.Â 3, p.Â 896.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2013) Lee, D.-H. et al. (2013). ä¼ªæ ‡ç­¾ï¼šæ·±åº¦ç¥ç»ç½‘ç»œçš„ç®€å•é«˜æ•ˆåŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œã€Šè¡¨ç¤ºå­¦ä¹ æŒ‘æˆ˜ç ”è®¨ä¼šã€‹ï¼ŒICMLï¼Œå·
    3ï¼Œé¡µç ï¼š896ã€‚
- en: Lee etÂ al. (2021) Lee, J., Kim, E. andÂ Yoon, S. (2021). Anti-adversarially manipulated
    attributions for weakly and semi-supervised semantic segmentation, Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.Â 4071â€“4080.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2021) Lee, J., Kim, E. å’Œ Yoon, S. (2021). é’ˆå¯¹å¼±ç›‘ç£å’ŒåŠç›‘ç£è¯­ä¹‰åˆ†å‰²çš„åå¯¹æŠ—æ“æ§å½’å› ï¼Œã€ŠIEEE/CVF
    è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ã€‹ï¼Œé¡µç ï¼š4071â€“4080ã€‚
- en: Lerousseau etÂ al. (2020) Lerousseau, M., Vakalopoulou, M., Classe, M., Adam,
    J., Battistella, E., CarrÃ©, A., Estienne, T., Henry, T., Deutsch, E. andÂ Paragios,
    N. (2020). Weakly supervised multiple instance learning histopathological tumor
    segmentation, International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, pp.Â 470â€“479.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lerousseau et al. (2020) Lerousseau, M., Vakalopoulou, M., Classe, M., Adam,
    J., Battistella, E., CarrÃ©, A., Estienne, T., Henry, T., Deutsch, E. å’Œ Paragios,
    N. (2020). å¼±ç›‘ç£å¤šå®ä¾‹å­¦ä¹ çš„ç»„ç»‡ç—…ç†å­¦è‚¿ç˜¤åˆ†å‰²ï¼Œã€ŠåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ã€‹ï¼ŒSpringerï¼Œé¡µç ï¼š470â€“479ã€‚
- en: Li, Li andÂ Eliceiri (2021) Li, B., Li, Y. andÂ Eliceiri, K.Â W. (2021). Dual-stream
    multiple instance learning network for whole slide image classification with self-supervised
    contrastive learning, Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp.Â 14318â€“14328.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li, Li å’Œ Eliceiri (2021) Li, B., Li, Y. å’Œ Eliceiri, K. W. (2021). ç”¨äºå…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»çš„åŒæµå¤šå®ä¾‹å­¦ä¹ ç½‘ç»œä¸è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œã€ŠIEEE/CVF
    è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ã€‹ï¼Œé¡µç ï¼š14318â€“14328ã€‚
- en: Li, Yang, Wei, He, Chen, Zheng andÂ Bu (2021) Li, F., Yang, Y., Wei, Y., He,
    P., Chen, J., Zheng, Z. andÂ Bu, H. (2021). Deep learning-based predictive biomarker
    of pathological complete response to neoadjuvant chemotherapy from histological
    images in breast cancer, Journal of Translational Medicine  19(1):Â 1â€“13.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li, Yang, Wei, He, Chen, Zheng å’Œ Bu (2021) Li, F., Yang, Y., Wei, Y., He, P.,
    Chen, J., Zheng, Z. å’Œ Bu, H. (2021). åŸºäºæ·±åº¦å­¦ä¹ çš„ä¹³è…ºç™Œæ–°è¾…åŠ©åŒ–ç–—ç—…ç†å®Œå…¨åº”ç­”çš„é¢„æµ‹ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œã€Šè½¬åŒ–åŒ»å­¦æ‚å¿—ã€‹ 19(1):
    1â€“13ã€‚'
- en: 'Li, Yang, Zhao andÂ Yao (2021) Li, H., Yang, F., Zhao andÂ Yao, J. (2021). Dt-mil:
    Deformable transformer for multi-instance learning on histopathological image,
    International Conference on Medical Image Computing and Computer-Assisted Intervention,
    Springer, pp.Â 206â€“216.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li, Yang, Zhao å’Œ Yao (2021) Li, H., Yang, F., Zhao å’Œ Yao, J. (2021). Dt-milï¼šç”¨äºç»„ç»‡ç—…ç†å›¾åƒçš„å¯å˜å½¢å˜æ¢å™¨å¤šå®ä¾‹å­¦ä¹ ï¼Œã€ŠåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ã€‹ï¼ŒSpringerï¼Œé¡µç ï¼š206â€“216ã€‚
- en: Li etÂ al. (2018) Li, J., Speier, W., Ho, K.Â C., Sarma, K.Â V., Gertych, A., Knudsen,
    B.Â S. andÂ Arnold, C.Â W. (2018). An em-based semi-supervised deep learning approach
    for semantic segmentation of histopathological images from radical prostatectomies,
    Computerized Medical Imaging and Graphics  69:Â 125â€“133.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æç­‰ï¼ˆ2018ï¼‰æï¼ŒJ.ï¼Œæ–¯çš®å°”ï¼ŒW.ï¼Œéœï¼ŒK. C.ï¼Œè¨å°”é©¬ï¼ŒK. V.ï¼Œæ ¼å°”è’‚å¥‡ï¼ŒA.ï¼Œå…‹åŠªæ£®ï¼ŒB. S. å’Œ é˜¿è¯ºå¾·ï¼ŒC. W.ï¼ˆ2018ï¼‰ã€‚ä¸€ç§åŸºäºEMçš„åŠç›‘ç£æ·±åº¦å­¦ä¹ æ–¹æ³•ç”¨äºä»æ ¹æ²»æ€§å‰åˆ—è…ºåˆ‡é™¤æœ¯çš„ç»„ç»‡ç—…ç†å›¾åƒä¸­è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œè®¡ç®—æœºåŒ–åŒ»å­¦æˆåƒä¸å›¾å½¢
    69ï¼š125â€“133ã€‚
- en: Liu etÂ al. (2020) Liu, S., Shah, Z., Sav, A., Russo, C., Berkovsky, S., Qian,
    Y., Coiera, E. andÂ DiÂ Ieva, A. (2020). Isocitrate dehydrogenase (idh) status prediction
    in histopathology images of gliomas using deep learning, Scientific Reports  10(1):Â 1â€“11.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ˜ç­‰ï¼ˆ2020ï¼‰åˆ˜ï¼ŒS.ï¼Œæ²™ï¼ŒZ.ï¼Œè¨å¤«ï¼ŒA.ï¼Œé²ç´¢ï¼ŒC.ï¼Œè´å°”ç§‘å¤«æ–¯åŸºï¼ŒS.ï¼Œé’±ï¼ŒY.ï¼Œç§‘åŸƒæ‹‰ï¼ŒE. å’Œ è¿ªÂ·è€¶ç“¦ï¼ŒA.ï¼ˆ2020ï¼‰ã€‚åˆ©ç”¨æ·±åº¦å­¦ä¹ é¢„æµ‹èƒ¶è´¨ç˜¤ç»„ç»‡ç—…ç†å›¾åƒä¸­çš„å¼‚æŸ æª¬é…¸è„±æ°¢é…¶ï¼ˆidhï¼‰çŠ¶æ€ï¼Œç§‘å­¦æŠ¥å‘Š
    10(1)ï¼š1â€“11ã€‚
- en: 'Liu etÂ al. (2021) Liu, X., Zhang, F., Hou, Z., Mian, L., Wang, Z., Zhang, J.
    andÂ Tang, J. (2021). Self-supervised learning: Generative or contrastive, IEEE
    Transactions on Knowledge and Data Engineering .'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ˜ç­‰ï¼ˆ2021ï¼‰åˆ˜ï¼ŒX.ï¼Œå¼ ï¼ŒF.ï¼Œä¾¯ï¼ŒZ.ï¼Œç¼…ï¼ŒL.ï¼Œç‹ï¼ŒZ.ï¼Œå¼ ï¼ŒJ. å’Œ å”ï¼ŒJ.ï¼ˆ2021ï¼‰ã€‚è‡ªç›‘ç£å­¦ä¹ ï¼šç”Ÿæˆè¿˜æ˜¯å¯¹æ¯”ï¼ŒIEEEçŸ¥è¯†ä¸æ•°æ®å·¥ç¨‹å­¦æŠ¥ã€‚
- en: Ljosa etÂ al. (2012) Ljosa, V., Sokolnicki, K.Â L. andÂ Carpenter, A.Â E. (2012).
    Annotated high-throughput microscopy image sets for validation, Nature methods  9(7):Â 637â€“637.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ©ä¹”è¨ç­‰ï¼ˆ2012ï¼‰åˆ©ä¹”è¨ï¼ŒV.ï¼Œç´¢ç§‘å°”å°¼åŸºï¼ŒK. L. å’Œ å¡å½­ç‰¹ï¼ŒA. E.ï¼ˆ2012ï¼‰ã€‚ç”¨äºéªŒè¯çš„æ ‡æ³¨é«˜é€šé‡æ˜¾å¾®é•œå›¾åƒé›†ï¼Œè‡ªç„¶æ–¹æ³• 9(7)ï¼š637â€“637ã€‚
- en: Lu etÂ al. (2019) Lu, M.Â Y., Chen, R.Â J., Wang, J., Dillon, D. andÂ Mahmood, F.
    (2019). Semi-supervised histology classification using deep multiple instance
    learning and contrastive predictive coding, arXiv preprint arXiv:1910.10825 .
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™†ç­‰ï¼ˆ2019ï¼‰é™†ï¼ŒM. Y.ï¼Œé™ˆï¼ŒR. J.ï¼Œç‹ï¼ŒJ.ï¼Œè¿ªé¾™ï¼ŒD. å’Œ é©¬èµ«ç©†å¾·ï¼ŒF.ï¼ˆ2019ï¼‰ã€‚ä½¿ç”¨æ·±åº¦å¤šå®ä¾‹å­¦ä¹ å’Œå¯¹æ¯”é¢„æµ‹ç¼–ç çš„åŠç›‘ç£ç»„ç»‡å­¦åˆ†ç±»ï¼ŒarXivé¢„å°æœ¬
    arXiv:1910.10825ã€‚
- en: Lu etÂ al. (2021) Lu, M.Â Y., Williamson, D.Â F., Chen, T.Â Y., Chen, R.Â J., Barbieri,
    M. andÂ Mahmood, F. (2021). Data-efficient and weakly supervised computational
    pathology on whole-slide images, Nature Biomedical Engineering  5(6):Â 555â€“570.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™†ç­‰ï¼ˆ2021ï¼‰é™†ï¼ŒM. Y.ï¼Œå¨å»‰å§†æ£®ï¼ŒD. F.ï¼Œé™ˆï¼ŒT. Y.ï¼Œé™ˆï¼ŒR. J.ï¼Œå·´æ¯”è€¶é‡Œï¼ŒM. å’Œ é©¬èµ«ç©†å¾·ï¼ŒF.ï¼ˆ2021ï¼‰ã€‚åŸºäºå…¨åˆ‡ç‰‡å›¾åƒçš„æ•°æ®é«˜æ•ˆå’Œå¼±ç›‘ç£è®¡ç®—ç—…ç†ï¼Œè‡ªç„¶ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹
    5(6)ï¼š555â€“570ã€‚
- en: Luo etÂ al. (2017) Luo, X., Zang, X., Yang, L., Huang, J., Liang, F., Rodriguez-Canales,
    J., Wistuba, I.Â I., Gazdar, A., Xie, Y. andÂ Xiao, G. (2017). Comprehensive computational
    pathological image analysis predicts lung cancer prognosis, Journal of Thoracic
    Oncology  12(3):Â 501â€“509.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç½—ç­‰ï¼ˆ2017ï¼‰ç½—ï¼ŒX.ï¼Œè‡§ï¼ŒX.ï¼Œæ¨ï¼ŒL.ï¼Œé»„ï¼ŒJ.ï¼Œæ¢ï¼ŒF.ï¼Œç½—å¾·é‡Œæ ¼æ–¯-å¡çº³è±æ–¯ï¼ŒJ.ï¼Œç»´æ–¯å›¾å·´ï¼ŒI. I.ï¼ŒåŠ å…¹è¾¾ï¼ŒA.ï¼Œè°¢ï¼ŒY. å’Œ è‚–ï¼ŒG.ï¼ˆ2017ï¼‰ã€‚ç»¼åˆè®¡ç®—ç—…ç†å›¾åƒåˆ†æé¢„æµ‹è‚ºç™Œé¢„åï¼Œèƒ¸å¤–ç§‘è‚¿ç˜¤å­¦æ‚å¿—
    12(3)ï¼š501â€“509ã€‚
- en: 'Madabhushi andÂ Lee (2016) Madabhushi, A. andÂ Lee, G. (2016). Image analysis
    and machine learning in digital pathology: Challenges and opportunities, Medical
    Image Analysis  33:Â 170â€“175.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é©¬è¾¾å¸ƒå¸Œå’Œ æï¼ˆ2016ï¼‰é©¬è¾¾å¸ƒå¸Œï¼ŒA. å’Œ æï¼ŒG.ï¼ˆ2016ï¼‰ã€‚æ•°å­—ç—…ç†å­¦ä¸­çš„å›¾åƒåˆ†æå’Œæœºå™¨å­¦ä¹ ï¼šæŒ‘æˆ˜ä¸æœºé‡ï¼ŒåŒ»å­¦å›¾åƒåˆ†æ 33ï¼š170â€“175ã€‚
- en: Mahapatra etÂ al. (2020) Mahapatra, D., Bozorgtabar, B., Thiran, J.-P. andÂ Shao,
    L. (2020). Structure preserving stain normalization of histopathology images using
    self supervised semantic guidance, International Conference on Medical Image Computing
    and Computer-Assisted Intervention, Springer, pp.Â 309â€“319.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é©¬å“ˆå¸•ç‰¹æ‹‰ç­‰ï¼ˆ2020ï¼‰é©¬å“ˆå¸•ç‰¹æ‹‰ï¼ŒD.ï¼Œåšä½æ ¼å¡”å·´å°”ï¼ŒB.ï¼Œæå…°ï¼ŒJ.-P. å’Œ é‚µï¼ŒL.ï¼ˆ2020ï¼‰ã€‚ä½¿ç”¨è‡ªç›‘ç£è¯­ä¹‰æŒ‡å¯¼çš„ç»“æ„ä¿æŒæŸ“è‰²è§„èŒƒåŒ–ç»„ç»‡ç—…ç†å›¾åƒï¼ŒåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ï¼ŒSpringerï¼Œç¬¬309â€“319é¡µã€‚
- en: 'Marini etÂ al. (2021) Marini, N., OtÃ¡lora, S., MÃ¼ller, H. andÂ Atzori, M. (2021).
    Semi-supervised training of deep convolutional neural networks with heterogeneous
    data and few local annotations: An experiment on prostate histopathology image
    classification, Medical Image Analysis  73:Â 102165.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é©¬é‡Œå°¼ç­‰ï¼ˆ2021ï¼‰é©¬é‡Œå°¼ï¼ŒN.ï¼Œå¥¥å¡”æ´›æ‹‰ï¼ŒS.ï¼Œç©†å‹’ï¼ŒH. å’Œ é˜¿ä½é‡Œï¼ŒM.ï¼ˆ2021ï¼‰ã€‚å…·æœ‰å¼‚è´¨æ•°æ®å’Œå°‘é‡å±€éƒ¨æ³¨é‡Šçš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„åŠç›‘ç£è®­ç»ƒï¼šå‰åˆ—è…ºç»„ç»‡ç—…ç†å›¾åƒåˆ†ç±»å®éªŒï¼ŒåŒ»å­¦å›¾åƒåˆ†æ
    73ï¼š102165ã€‚
- en: Maron andÂ Lozano-PÃ©rez (1997) Maron, O. andÂ Lozano-PÃ©rez, T. (1997). A framework
    for multiple-instance learning, Advances in Neural Information Processing Systems  10.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é©¬é¾™å’Œ æ´›è¨è¯º-ä½©é›·æ–¯ï¼ˆ1997ï¼‰é©¬é¾™ï¼ŒO. å’Œ æ´›è¨è¯º-ä½©é›·æ–¯ï¼ŒT.ï¼ˆ1997ï¼‰ã€‚å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶ï¼Œç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±• 10ã€‚
- en: Martel etÂ al. (2019) Martel, A., Nofech-Mozes, S., Salama, S., Akbar, S. andÂ Peikari,
    M. (2019). Assessment of residual breast cancer cellularity after neoadjuvant
    chemotherapy using digital pathology [data set], The Cancer Imaging Archive .
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é©¬ç‰¹å°”ç­‰ï¼ˆ2019ï¼‰é©¬ç‰¹å°”ï¼ŒA.ï¼Œè¯ºè´¹èµ«-è«æ³½æ–¯ï¼ŒS.ï¼Œè¨æ‹‰é©¬ï¼ŒS.ï¼Œé˜¿å…‹å·´å°”ï¼ŒS. å’Œ ä½©å¡é‡Œï¼ŒM.ï¼ˆ2019ï¼‰ã€‚ä½¿ç”¨æ•°å­—ç—…ç†å­¦[æ•°æ®é›†]è¯„ä¼°æ–°è¾…åŠ©åŒ–ç–—åçš„æ®‹ä½™ä¹³è…ºç™Œç»†èƒæ€§ï¼Œç™Œç—‡å½±åƒæ¡£æ¡ˆé¦†ã€‚
- en: Melekhov etÂ al. (2016) Melekhov, I., Kannala, J. andÂ Rahtu, E. (2016). Siamese
    network features for image matching, 2016 23rd International Conference on Pattern
    Recognition (ICPR), IEEE, pp.Â 378â€“383.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Melekhov et al. (2016) Melekhov, I., Kannala, J. å’Œ Rahtu, E. (2016). ç”¨äºå›¾åƒåŒ¹é…çš„å­ªç”Ÿç½‘ç»œç‰¹å¾ï¼Œ2016
    ç¬¬ 23 å±Šå›½é™…æ¨¡å¼è¯†åˆ«å¤§ä¼š (ICPR)ï¼ŒIEEEï¼Œç¬¬ 378â€“383 é¡µã€‚
- en: Mercan etÂ al. (2017) Mercan, C., Aksoy, S., Mercan, E., Shapiro, L.Â G., Weaver,
    D.Â L. andÂ Elmore, J.Â G. (2017). Multi-instance multi-label learning for multi-class
    classification of whole slide breast histopathology images, IEEE Transactions
    on Medical Imaging  37(1):Â 316â€“325.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mercan et al. (2017) Mercan, C., Aksoy, S., Mercan, E., Shapiro, L. G., Weaver,
    D. L. å’Œ Elmore, J. G. (2017). ç”¨äºå…¨åˆ‡ç‰‡ä¹³è…ºç»„ç»‡ç—…ç†å›¾åƒçš„å¤šå®ä¾‹å¤šæ ‡ç­¾å­¦ä¹ ï¼Œã€ŠIEEE åŒ»å­¦æˆåƒäº¤æ˜“ã€‹ 37(1): 316â€“325ã€‚'
- en: Muhammad etÂ al. (2019) Muhammad, H., Sigel, C.Â S., Campanella, G., Boerner,
    T., Pak, L.Â M., BÃ¼ttner, S., IJzermans, J.Â N., Koerkamp, B.Â G., Doukas, M., Jarnagin,
    W.Â R. etÂ al. (2019). Unsupervised subtyping of cholangiocarcinoma using a deep
    clustering convolutional autoencoder, International Conference on Medical Image
    Computing and Computer-Assisted Intervention, Springer, pp.Â 604â€“612.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muhammad et al. (2019) Muhammad, H., Sigel, C. S., Campanella, G., Boerner,
    T., Pak, L. M., BÃ¼ttner, S., IJzermans, J. N., Koerkamp, B. G., Doukas, M., Jarnagin,
    W. R. ç­‰ (2019). ä½¿ç”¨æ·±åº¦èšç±»å·ç§¯è‡ªç¼–ç å™¨å¯¹èƒ†ç®¡ç™Œè¿›è¡Œæ— ç›‘ç£äºšå‹åˆ†ç±»ï¼Œã€ŠåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯å›½é™…ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬ 604â€“612
    é¡µã€‚
- en: Murthy etÂ al. (2017) Murthy, V., Hou, L., Samaras, D., Kurc, T.Â M. andÂ Saltz,
    J.Â H. (2017). Center-focusing multi-task cnn with injected features for classification
    of glioma nuclear images, 2017 IEEE Winter Conference on Applications of Computer
    Vision (WACV), IEEE, pp.Â 834â€“841.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murthy et al. (2017) Murthy, V., Hou, L., Samaras, D., Kurc, T. M. å’Œ Saltz,
    J. H. (2017). é€šè¿‡æ³¨å…¥ç‰¹å¾çš„ä¸­å¿ƒèšç„¦å¤šä»»åŠ¡ CNN å¯¹èƒ¶è´¨ç˜¤æ ¸å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œ2017 IEEE å†¬å­£è®¡ç®—æœºè§†è§‰åº”ç”¨ä¼šè®® (WACV)ï¼ŒIEEEï¼Œç¬¬
    834â€“841 é¡µã€‚
- en: Myronenko etÂ al. (2021) Myronenko, A., Xu, Z., Yang, D., Roth, H.Â R. andÂ Xu,
    D. (2021). Accounting for dependencies in deep learning based multiple instance
    learning for whole slide imaging, International Conference on Medical Image Computing
    and Computer-Assisted Intervention, Springer, pp.Â 329â€“338.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Myronenko et al. (2021) Myronenko, A., Xu, Z., Yang, D., Roth, H. R. å’Œ Xu, D.
    (2021). è®¡ç®—æ·±åº¦å­¦ä¹ åŸºç¡€çš„å¤šå®ä¾‹å­¦ä¹ ä¸­å¯¹æ•´ä½“åˆ‡ç‰‡æˆåƒçš„ä¾èµ–æ€§ï¼Œã€ŠåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯å›½é™…ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬ 329â€“338 é¡µã€‚
- en: Nagpal etÂ al. (2019) Nagpal, K., Foote, D., Liu, Y., Chen, P.-H.Â C., Wulczyn,
    E., Tan, F., Olson, N., Smith, J.Â L., Mohtashamian, A., Wren, J.Â H. etÂ al. (2019).
    Development and validation of a deep learning algorithm for improving gleason
    scoring of prostate cancer, NPJ Digital Medicine  2(1):Â 1â€“10.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nagpal et al. (2019) Nagpal, K., Foote, D., Liu, Y., Chen, P.-H. C., Wulczyn,
    E., Tan, F., Olson, N., Smith, J. L., Mohtashamian, A., Wren, J. H. ç­‰ (2019).
    æé«˜å‰åˆ—è…ºç™Œ Gleason è¯„åˆ†çš„æ·±åº¦å­¦ä¹ ç®—æ³•çš„å¼€å‘ä¸éªŒè¯ï¼Œã€ŠNPJ æ•°å­—åŒ»å­¦ã€‹ 2(1): 1â€“10ã€‚'
- en: Naik etÂ al. (2020) Naik, N., Madani, A., Esteva, A., Keskar, N.Â S., Press, M.Â F.,
    Ruderman, D., Agus, D.Â B. andÂ Socher, R. (2020). Deep learning-enabled breast
    cancer hormonal receptor status determination from base-level h&e stains, Nature
    Communications  11(1):Â 1â€“8.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Naik et al. (2020) Naik, N., Madani, A., Esteva, A., Keskar, N. S., Press,
    M. F., Ruderman, D., Agus, D. B. å’Œ Socher, R. (2020). åŸºäºæ·±åº¦å­¦ä¹ çš„ä¹³è…ºç™Œæ¿€ç´ å—ä½“çŠ¶æ€ç¡®å®šï¼ŒåŸºäºåŸºç¡€çº§åˆ«
    H&E æŸ“è‰²ï¼Œã€Šè‡ªç„¶é€šè®¯ã€‹ 11(1): 1â€“8ã€‚'
- en: Naylor etÂ al. (2018) Naylor, P., LaÃ©, M., Reyal, F. andÂ Walter, T. (2018). Segmentation
    of nuclei in histopathology images by deep regression of the distance map, IEEE
    Transactions on Medical Imaging  38(2):Â 448â€“459.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Naylor et al. (2018) Naylor, P., LaÃ©, M., Reyal, F. å’Œ Walter, T. (2018). é€šè¿‡æ·±åº¦å›å½’è·ç¦»å›¾å¯¹ç»„ç»‡ç—…ç†å›¾åƒä¸­çš„ç»†èƒæ ¸è¿›è¡Œåˆ†å‰²ï¼Œã€ŠIEEE
    åŒ»å­¦æˆåƒäº¤æ˜“ã€‹ 38(2): 448â€“459ã€‚'
- en: Noroozi andÂ Favaro (2016) Noroozi, M. andÂ Favaro, P. (2016). Unsupervised learning
    of visual representations by solving jigsaw puzzles, European Conference on Computer
    Vision, Springer, pp.Â 69â€“84.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noroozi and Favaro (2016) Noroozi, M. å’Œ Favaro, P. (2016). é€šè¿‡è§£å†³æ‹¼å›¾æ¥æ— ç›‘ç£å­¦ä¹ è§†è§‰è¡¨ç¤ºï¼Œã€Šæ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬
    69â€“84 é¡µã€‚
- en: Odena (2016) Odena, A. (2016). Semi-supervised learning with generative adversarial
    networks, arXiv preprint arXiv:1606.01583 .
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Odena (2016) Odena, A. (2016). ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„åŠç›‘ç£å­¦ä¹ ï¼ŒarXiv é¢„å°æœ¬ arXiv:1606.01583ã€‚
- en: Pan etÂ al. (2022) Pan, J., Bi, Q., Yang, Y., Zhu, P. andÂ Bian, C. (2022). Label-efficient
    hybrid-supervised learning for medical image segmentation, arXiv preprint arXiv:2203.05956
    .
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. (2022) Pan, J., Bi, Q., Yang, Y., Zhu, P. å’Œ Bian, C. (2022). ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ ‡ç­¾é«˜æ•ˆæ··åˆç›‘ç£å­¦ä¹ ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:2203.05956ã€‚
- en: Parag etÂ al. (2014) Parag, T., Plaza, S. andÂ Scheffer, L. (2014). Small sample
    learning of superpixel classifiers for em segmentation, International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, pp.Â 389â€“397.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parag et al. (2014) Parag, T., Plaza, S. å’Œ Scheffer, L. (2014). ç”¨äº EM åˆ†å‰²çš„è¶…åƒç´ åˆ†ç±»å™¨çš„å°æ ·æœ¬å­¦ä¹ ï¼Œã€ŠåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯å›½é™…ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬
    389â€“397 é¡µã€‚
- en: 'Pathak etÂ al. (2016) Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T. andÂ Efros,
    A.Â A. (2016). Context encoders: Feature learning by inpainting, Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp.Â 2536â€“2544.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathakç­‰ï¼ˆ2016ï¼‰Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T. å’Œ Efros, A.
    A.ï¼ˆ2016ï¼‰ã€‚ä¸Šä¸‹æ–‡ç¼–ç å™¨ï¼šé€šè¿‡ä¿®å¤è¿›è¡Œç‰¹å¾å­¦ä¹ ï¼ŒIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬2536â€“2544é¡µã€‚
- en: Peikari etÂ al. (2018) Peikari, M., Salama, S., Nofech-Mozes, S. andÂ Martel,
    A.Â L. (2018). A cluster-then-label semi-supervised learning approach for pathology
    image classification, Scientific Reports  8(1):Â 1â€“13.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peikariç­‰ï¼ˆ2018ï¼‰Peikari, M., Salama, S., Nofech-Mozes, S. å’Œ Martel, A. L.ï¼ˆ2018ï¼‰ã€‚ä¸€ç§å…ˆèšç±»åæ ‡æ³¨çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•ç”¨äºç—…ç†å›¾åƒåˆ†ç±»ï¼ŒScientific
    Reports 8(1): 1â€“13ã€‚'
- en: 'Petrick etÂ al. (2021) Petrick, N.Â A., Akbar, S., Cha, K.Â H., Nofech-Mozes,
    S., Sahiner, B., Gavrielides, M.Â A., Kalpathy-Cramer, J., Drukker, K., Martel,
    A.Â L. etÂ al. (2021). Spie-aapm-nci breastpathq challenge: an image analysis challenge
    for quantitative tumor cellularity assessment in breast cancer histology images
    following neoadjuvant treatment, Journal of Medical Imaging  8(3):Â 034501.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Petrickç­‰ï¼ˆ2021ï¼‰Petrick, N. A., Akbar, S., Cha, K. H., Nofech-Mozes, S., Sahiner,
    B., Gavrielides, M. A., Kalpathy-Cramer, J., Drukker, K., Martel, A. L. ç­‰ï¼ˆ2021ï¼‰ã€‚Spie-aapm-nci
    breastpathqæŒ‘æˆ˜èµ›ï¼šç”¨äºå®šé‡è¯„ä¼°ä¹³è…ºç™Œç»„ç»‡å­¦å›¾åƒä¸­è‚¿ç˜¤ç»†èƒæ€§çš„ä¸€é¡¹å›¾åƒåˆ†ææŒ‘æˆ˜ï¼ŒJournal of Medical Imaging 8(3):
    034501ã€‚'
- en: Qaiser etÂ al. (2016) Qaiser, T., Sirinukunwattana, K., Nakane, K., Tsang, Y.-W.,
    Epstein, D. andÂ Rajpoot, N. (2016). Persistent homology for fast tumor segmentation
    in whole slide histology images, Procedia Computer Science  90:Â 119â€“124.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qaiserç­‰ï¼ˆ2016ï¼‰Qaiser, T., Sirinukunwattana, K., Nakane, K., Tsang, Y.-W., Epstein,
    D. å’Œ Rajpoot, N.ï¼ˆ2016ï¼‰ã€‚ç”¨äºå¿«é€Ÿè‚¿ç˜¤åˆ†å‰²çš„æŒä¹…åŒè°ƒï¼ŒProcedia Computer Science 90: 119â€“124ã€‚'
- en: Qu etÂ al. (2020) Qu, H., Wu, P., Huang, Q., Yi, J., Yan, Z., Li, K., Riedlinger,
    G.Â M., De, S., Zhang, S. andÂ Metaxas, D.Â N. (2020). Weakly supervised deep nuclei
    segmentation using partial points annotation in histopathology images, IEEE Transactions
    on Medical Imaging  39(11):Â 3655â€“3666.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Quç­‰ï¼ˆ2020ï¼‰Qu, H., Wu, P., Huang, Q., Yi, J., Yan, Z., Li, K., Riedlinger, G.
    M., De, S., Zhang, S. å’Œ Metaxas, D. N.ï¼ˆ2020ï¼‰ã€‚ä½¿ç”¨éƒ¨åˆ†ç‚¹æ³¨é‡Šçš„å¼±ç›‘ç£æ·±æ ¸åˆ†å‰²ï¼ŒIEEE Transactions
    on Medical Imaging 39(11): 3655â€“3666ã€‚'
- en: 'Qu etÂ al. (2022) Qu, L., Luo, X., Liu, S., Wang, M. andÂ Song, Z. (2022). Dgmil:
    Distribution guided multiple instance learning for whole slide image classification,
    arXiv preprint arXiv:2206.08861 .'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Quç­‰ï¼ˆ2022ï¼‰Qu, L., Luo, X., Liu, S., Wang, M. å’Œ Song, Z.ï¼ˆ2022ï¼‰ã€‚Dgmil: åˆ†å¸ƒå¼•å¯¼çš„å¤šå®ä¾‹å­¦ä¹ ç”¨äºå…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»ï¼ŒarXivé¢„å°æœ¬
    arXiv:2206.08861ã€‚'
- en: Quiros etÂ al. (2021) Quiros, A.Â C., Coudray, N., Yeaton, A., Sunhem, W., Murray-Smith,
    R., Tsirigos, A. andÂ Yuan, K. (2021). Adversarial learning of cancer tissue representations,
    arXiv preprint arXiv:2108.02223 .
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quirosç­‰ï¼ˆ2021ï¼‰Quiros, A. C., Coudray, N., Yeaton, A., Sunhem, W., Murray-Smith,
    R., Tsirigos, A. å’Œ Yuan, K.ï¼ˆ2021ï¼‰ã€‚ç™Œç—‡ç»„ç»‡è¡¨ç¤ºçš„å¯¹æŠ—æ€§å­¦ä¹ ï¼ŒarXivé¢„å°æœ¬ arXiv:2108.02223ã€‚
- en: 'Quiros etÂ al. (2019) Quiros, A.Â C., Murray-Smith, R. andÂ Yuan, K. (2019). Pathologygan:
    Learning deep representations of cancer tissue, arXiv preprint arXiv:1907.02644
    .'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Quirosç­‰ï¼ˆ2019ï¼‰Quiros, A. C., Murray-Smith, R. å’Œ Yuan, K.ï¼ˆ2019ï¼‰ã€‚Pathologygan:
    å­¦ä¹ ç™Œç—‡ç»„ç»‡çš„æ·±åº¦è¡¨ç¤ºï¼ŒarXivé¢„å°æœ¬ arXiv:1907.02644ã€‚'
- en: Qureshi etÂ al. (2008) Qureshi, H., Sertel, O., Rajpoot, N., Wilson, R. andÂ Gurcan,
    M. (2008). Adaptive discriminant wavelet packet transform and local binary patterns
    for meningioma subtype classification, International Conference on Medical Image
    Computing and Computer-Assisted Intervention, Springer, pp.Â 196â€“204.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qureshiç­‰ï¼ˆ2008ï¼‰Qureshi, H., Sertel, O., Rajpoot, N., Wilson, R. å’Œ Gurcan, M.ï¼ˆ2008ï¼‰ã€‚ç”¨äºè„‘è†œç˜¤äºšå‹åˆ†ç±»çš„è‡ªé€‚åº”åˆ¤åˆ«å°æ³¢åŒ…å˜æ¢å’Œå±€éƒ¨äºŒå€¼æ¨¡å¼ï¼Œå›½é™…åŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„ä¼šè®®ï¼ŒSpringerï¼Œç¬¬196â€“204é¡µã€‚
- en: Radford etÂ al. (2015) Radford, A., Metz, L. andÂ Chintala, S. (2015). Unsupervised
    representation learning with deep convolutional generative adversarial networks,
    arXiv preprint arXiv:1511.06434 .
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radfordç­‰ï¼ˆ2015ï¼‰Radford, A., Metz, L. å’Œ Chintala, S.ï¼ˆ2015ï¼‰ã€‚ä½¿ç”¨æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„æ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼ŒarXivé¢„å°æœ¬
    arXiv:1511.06434ã€‚
- en: Rajpoot andÂ Rajpoot (2004) Rajpoot, K. andÂ Rajpoot, N. (2004). Svm optimization
    for hyperspectral colon tissue cell classification, International Conference on
    Medical Image Computing and Computer-Assisted Intervention, Springer, pp.Â 829â€“837.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpootå’ŒRajpootï¼ˆ2004ï¼‰Rajpoot, K. å’Œ Rajpoot, N.ï¼ˆ2004ï¼‰ã€‚ç”¨äºé«˜å…‰è°±ç»“è‚ ç»„ç»‡ç»†èƒåˆ†ç±»çš„æ”¯æŒå‘é‡æœºä¼˜åŒ–ï¼Œå›½é™…åŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„ä¼šè®®ï¼ŒSpringerï¼Œç¬¬829â€“837é¡µã€‚
- en: Ramon andÂ DeÂ Raedt (2000) Ramon, J. andÂ DeÂ Raedt, L. (2000). Multi instance
    neural networks, Proceedings of the ICML-2000 Workshop on Attribute-value and
    Relational Learning, pp.Â 53â€“60.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramonå’ŒDe Raedtï¼ˆ2000ï¼‰Ramon, J. å’Œ De Raedt, L.ï¼ˆ2000ï¼‰ã€‚å¤šå®ä¾‹ç¥ç»ç½‘ç»œï¼ŒICML-2000å±æ€§å€¼ä¸å…³ç³»å­¦ä¹ ç ”è®¨ä¼šè®ºæ–‡é›†ï¼Œç¬¬53â€“60é¡µã€‚
- en: 'Rethlefsen etÂ al. (2021) Rethlefsen, M.Â L., Kirtley, S., Waffenschmidt, S.,
    Ayala, A.Â P., Moher, D., Page, M.Â J. andÂ Koffel, J.Â B. (2021). Prisma-s: an extension
    to the prisma statement for reporting literature searches in systematic reviews,
    Systematic Reviews  10(1):Â 1â€“19.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rethlefsenç­‰ï¼ˆ2021ï¼‰Rethlefsen, M. L., Kirtley, S., Waffenschmidt, S., Ayala,
    A. P., Moher, D., Page, M. J. å’Œ Koffel, J. B.ï¼ˆ2021ï¼‰ã€‚Prisma-sï¼šå¯¹PRISMAå£°æ˜çš„æ‰©å±•ï¼Œç”¨äºç³»ç»Ÿè¯„ä»·ä¸­çš„æ–‡çŒ®æ£€ç´¢æŠ¥å‘Šï¼Œã€Šç³»ç»Ÿè¯„ä»·ã€‹10(1):
    1â€“19ã€‚'
- en: Rifai, Dauphin, Vincent, Bengio andÂ Muller (2011) Rifai, S., Dauphin, Y.Â N.,
    Vincent, P., Bengio, Y. andÂ Muller, X. (2011). The manifold tangent classifier,
    Advances in Neural Information Processing Systems  24.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rifai, Dauphin, Vincent, Bengio å’Œ Mullerï¼ˆ2011ï¼‰Rifai, S., Dauphin, Y. N., Vincent,
    P., Bengio, Y. å’Œ Muller, X.ï¼ˆ2011ï¼‰ã€‚æµå½¢åˆ‡çº¿åˆ†ç±»å™¨ï¼Œã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹24ã€‚
- en: 'Rifai, Vincent, Muller, Glorot andÂ Bengio (2011) Rifai, S., Vincent, P., Muller,
    X., Glorot, X. andÂ Bengio, Y. (2011). Contractive auto-encoders: Explicit invariance
    during feature extraction, International Conference on Machine Learning.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rifai, Vincent, Muller, Glorot å’Œ Bengioï¼ˆ2011ï¼‰Rifai, S., Vincent, P., Muller,
    X., Glorot, X. å’Œ Bengio, Y.ï¼ˆ2011ï¼‰ã€‚å‹ç¼©è‡ªç¼–ç å™¨ï¼šç‰¹å¾æå–ä¸­çš„æ˜¾å¼ä¸å˜æ€§ï¼Œã€Šæœºå™¨å­¦ä¹ å›½é™…ä¼šè®®ã€‹ã€‚
- en: 'Rony etÂ al. (2019) Rony, J., Belharbi, S., Dolz, J., Ayed, I.Â B., McCaffrey,
    L. andÂ Granger, E. (2019). Deep weakly-supervised learning methods for classification
    and localization in histology images: a survey, arXiv preprint arXiv:1909.03354
    .'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ronyç­‰ï¼ˆ2019ï¼‰Rony, J., Belharbi, S., Dolz, J., Ayed, I. B., McCaffrey, L. å’Œ Granger,
    E.ï¼ˆ2019ï¼‰ã€‚ç”¨äºç»„ç»‡å­¦å›¾åƒåˆ†ç±»å’Œå®šä½çš„æ·±åº¦å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼šç»¼è¿°ï¼ŒarXivé¢„å°æœ¬ arXiv:1909.03354ã€‚
- en: 'Ru etÂ al. (2022) Ru, L., Zhan, Y., Yu, B. andÂ Du, B. (2022). Learning affinity
    from attention: End-to-end weakly-supervised semantic segmentation with transformers,
    arXiv preprint arXiv:2203.02664 .'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruç­‰ï¼ˆ2022ï¼‰Ru, L., Zhan, Y., Yu, B. å’Œ Du, B.ï¼ˆ2022ï¼‰ã€‚ä»æ³¨æ„åŠ›ä¸­å­¦ä¹ äº²å’ŒåŠ›ï¼šåŸºäºå˜æ¢å™¨çš„ç«¯åˆ°ç«¯å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ŒarXivé¢„å°æœ¬
    arXiv:2203.02664ã€‚
- en: Sahasrabudhe etÂ al. (2020) Sahasrabudhe, M., Christodoulidis, S., Salgado, R.,
    Michiels, S., Loi, S., AndrÃ©, F., Paragios, N. andÂ Vakalopoulou, M. (2020). Self-supervised
    nuclei segmentation in histopathological images using attention, International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer,
    pp.Â 393â€“402.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahasrabudheç­‰ï¼ˆ2020ï¼‰Sahasrabudhe, M., Christodoulidis, S., Salgado, R., Michiels,
    S., Loi, S., AndrÃ©, F., Paragios, N. å’Œ Vakalopoulou, M.ï¼ˆ2020ï¼‰ã€‚ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç»„ç»‡ç—…ç†å›¾åƒçš„è‡ªç›‘ç£ç»†èƒæ ¸åˆ†å‰²ï¼Œã€ŠåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬393â€“402é¡µã€‚
- en: Saillard etÂ al. (2021) Saillard, C., Dehaene, O., Marchand, T., Moindrot, O.,
    Kamoun, A., Schmauch, B. andÂ Jegou, S. (2021). Self supervised learning improves
    dmmr/msi detection from histology slides across multiple cancers, arXiv preprint
    arXiv:2109.05819 .
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saillardç­‰ï¼ˆ2021ï¼‰Saillard, C., Dehaene, O., Marchand, T., Moindrot, O., Kamoun,
    A., Schmauch, B. å’Œ Jegou, S.ï¼ˆ2021ï¼‰ã€‚è‡ªç›‘ç£å­¦ä¹ æé«˜äº†å¤šç™Œç—‡ç»„ç»‡åˆ‡ç‰‡ä¸­çš„DMMR/MSIæ£€æµ‹ï¼ŒarXivé¢„å°æœ¬ arXiv:2109.05819ã€‚
- en: Saillard etÂ al. (2020) Saillard, C., Schmauch, B., Laifa, O., Moarii, M., Toldo,
    S., Zaslavskiy, M., Pronier, E., Laurent, A., Amaddeo, G., Regnault, H. etÂ al.
    (2020). Predicting survival after hepatocellular carcinoma resection using deep
    learning on histological slides, Hepatology  72(6):Â 2000â€“2013.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saillardç­‰ï¼ˆ2020ï¼‰Saillard, C., Schmauch, B., Laifa, O., Moarii, M., Toldo, S.,
    Zaslavskiy, M., Pronier, E., Laurent, A., Amaddeo, G., Regnault, H. ç­‰ï¼ˆ2020ï¼‰ã€‚åˆ©ç”¨æ·±åº¦å­¦ä¹ åœ¨ç»„ç»‡å­¦åˆ‡ç‰‡ä¸Šé¢„æµ‹è‚ç»†èƒç™Œåˆ‡é™¤åçš„ç”Ÿå­˜ç‡ï¼Œã€Šè‚ç—…å­¦ã€‹72(6):
    2000â€“2013ã€‚'
- en: Salimans etÂ al. (2016) Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V.,
    Radford, A. andÂ Chen, X. (2016). Improved techniques for training gans, Advances
    in Neural Information Processing Systems  29.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salimansç­‰ï¼ˆ2016ï¼‰Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford,
    A. å’Œ Chen, X.ï¼ˆ2016ï¼‰ã€‚æ”¹è¿›çš„GANè®­ç»ƒæŠ€æœ¯ï¼Œã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹29ã€‚
- en: Shaban etÂ al. (2019) Shaban, M., Khurram, S.Â A., Fraz, M.Â M., Alsubaie, N.,
    Masood, I., Mushtaq, S., Hassan, M., Loya, A. andÂ Rajpoot, N.Â M. (2019). A novel
    digital score for abundance of tumour infiltrating lymphocytes predicts disease
    free survival in oral squamous cell carcinoma, Scientific Reports  9(1):Â 1â€“13.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shabanç­‰ï¼ˆ2019ï¼‰Shaban, M., Khurram, S. A., Fraz, M. M., Alsubaie, N., Masood,
    I., Mushtaq, S., Hassan, M., Loya, A. å’Œ Rajpoot, N. M.ï¼ˆ2019ï¼‰ã€‚ä¸€ç§æ–°å‹æ•°å­—è¯„åˆ†ç”¨äºé¢„æµ‹å£è…”é³çŠ¶ç»†èƒç™Œä¸­çš„è‚¿ç˜¤æµ¸æ¶¦æ·‹å·´ç»†èƒä¸°åº¦ï¼Œä»è€Œé¢„æµ‹æ— ç—…ç”Ÿå­˜ï¼Œã€Šç§‘å­¦æŠ¥å‘Šã€‹9(1):
    1â€“13ã€‚'
- en: 'Shao etÂ al. (2021) Shao, Z., Bian, H., Chen, Y., Wang, Y., Zhang, J., Ji, X.
    etÂ al. (2021). Transmil: Transformer based correlated multiple instance learning
    for whole slide image classification, Advances in Neural Information Processing
    Systems  34.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaoç­‰ï¼ˆ2021ï¼‰Shao, Z., Bian, H., Chen, Y., Wang, Y., Zhang, J., Ji, X. ç­‰ï¼ˆ2021ï¼‰ã€‚Transmilï¼šåŸºäºå˜æ¢å™¨çš„ç›¸å…³å¤šå®ä¾‹å­¦ä¹ ç”¨äºå…¨åˆ‡ç‰‡å›¾åƒåˆ†ç±»ï¼Œã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹34ã€‚
- en: 'Sharma etÂ al. (2021) Sharma, Y., Shrivastava, A., Ehsan, L., Moskaluk, C.Â A.,
    Syed, S. andÂ Brown, D. (2021). Cluster-to-conquer: A framework for end-to-end
    multi-instance learning for whole slide image classification, Medical Imaging
    with Deep Learning, PMLR, pp.Â 682â€“698.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma ç­‰ï¼ˆ2021ï¼‰Sharma, Y., Shrivastava, A., Ehsan, L., Moskaluk, C. A., Syed,
    S. å’Œ Brown, D.ï¼ˆ2021ï¼‰ã€‚Cluster-to-conquerï¼šä¸€ç§ç”¨äºå…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»çš„ç«¯åˆ°ç«¯å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶ï¼ŒåŒ»å­¦å½±åƒæ·±åº¦å­¦ä¹ ï¼ŒPMLRï¼Œç¬¬682â€“698é¡µã€‚
- en: Shaw etÂ al. (2020) Shaw, S., Pajak, M., Lisowska, A., Tsaftaris, S.Â A. andÂ Oâ€™Neil,
    A.Â Q. (2020). Teacher-student chain for efficient semi-supervised histology image
    classification, arXiv preprint arXiv:2003.08797 .
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaw ç­‰ï¼ˆ2020ï¼‰Shaw, S., Pajak, M., Lisowska, A., Tsaftaris, S. A. å’Œ Oâ€™Neil, A.
    Q.ï¼ˆ2020ï¼‰ã€‚é«˜æ•ˆçš„åŠç›‘ç£ç»„ç»‡å­¦å›¾åƒåˆ†ç±»çš„å¸ˆç”Ÿé“¾ï¼ŒarXiv é¢„å°æœ¬ arXiv:2003.08797ã€‚
- en: Shi etÂ al. (2018) Shi, X., Sapkota, M., Xing, F., Liu, F., Cui, L. andÂ Yang,
    L. (2018). Pairwise based deep ranking hashing for histopathology image classification
    and retrieval, Pattern Recognition  81:Â 14â€“22.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi ç­‰ï¼ˆ2018ï¼‰Shi, X., Sapkota, M., Xing, F., Liu, F., Cui, L. å’Œ Yang, L.ï¼ˆ2018ï¼‰ã€‚åŸºäºå¯¹çš„æ·±åº¦æ’åºå“ˆå¸Œç”¨äºç»„ç»‡ç—…ç†å›¾åƒåˆ†ç±»å’Œæ£€ç´¢ï¼Œæ¨¡å¼è¯†åˆ«
    81: 14â€“22ã€‚'
- en: Shi, Su, Xing andÂ Yang (2020) Shi, X., Su, H., Xing, G. andÂ Yang, L. (2020).
    Graph temporal ensembling based semi-supervised convolutional neural network with
    noisy labels for histopathology image analysis, Medical Image Analysis  60:Â 101624.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi, Su, Xing å’Œ Yangï¼ˆ2020ï¼‰Shi, X., Su, H., Xing, G. å’Œ Yang, L.ï¼ˆ2020ï¼‰ã€‚åŸºäºå›¾çš„æ—¶é—´é›†æˆåŠç›‘ç£å·ç§¯ç¥ç»ç½‘ç»œä¸å™ªå£°æ ‡ç­¾ç”¨äºç»„ç»‡ç—…ç†å›¾åƒåˆ†æï¼ŒåŒ»å­¦å›¾åƒåˆ†æ
    60: 101624ã€‚'
- en: Shi, Xing, Xie, Zhang, Cui andÂ Yang (2020) Shi, X., Xing, F., Xie, Y., Zhang,
    Z., Cui, L. andÂ Yang, L. (2020). Loss-based attention for deep multiple instance
    learning, Proceedings of the AAAI Conference on Artificial Intelligence, Vol.Â 34,
    pp.Â 5742â€“5749.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi, Xing, Xie, Zhang, Cui å’Œ Yangï¼ˆ2020ï¼‰Shi, X., Xing, F., Xie, Y., Zhang, Z.,
    Cui, L. å’Œ Yang, L.ï¼ˆ2020ï¼‰ã€‚åŸºäºæŸå¤±çš„æ³¨æ„åŠ›ç”¨äºæ·±åº¦å¤šå®ä¾‹å­¦ä¹ ï¼ŒAAAI äººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†ï¼Œå·34ï¼Œç¬¬5742â€“5749é¡µã€‚
- en: 'Shurrab andÂ Duwairi (2021) Shurrab, S. andÂ Duwairi, R. (2021). Self-supervised
    learning methods and applications in medical imaging analysis: A survey, arXiv
    preprint arXiv:2109.08685 .'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shurrab å’Œ Duwairiï¼ˆ2021ï¼‰Shurrab, S. å’Œ Duwairi, R.ï¼ˆ2021ï¼‰ã€‚è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åŠå…¶åœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„åº”ç”¨ï¼šç»¼è¿°ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:2109.08685ã€‚
- en: Singh etÂ al. (2011) Singh, S., Janoos, F., PÃ©cot, T., Caserta, E., Leone, G.,
    Rittscher, J. andÂ Machiraju, R. (2011). Identifying nuclear phenotypes using semi-supervised
    metric learning, Biennial International Conference on Information Processing in
    Medical Imaging, Springer, pp.Â 398â€“410.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh ç­‰ï¼ˆ2011ï¼‰Singh, S., Janoos, F., PÃ©cot, T., Caserta, E., Leone, G., Rittscher,
    J. å’Œ Machiraju, R.ï¼ˆ2011ï¼‰ã€‚ä½¿ç”¨åŠç›‘ç£åº¦é‡å­¦ä¹ è¯†åˆ«æ ¸è¡¨å‹ï¼ŒåŒ»å­¦å½±åƒä¿¡æ¯å¤„ç†å›½é™…ä¼šè®®ï¼ˆBiennial International Conference
    on Information Processing in Medical Imagingï¼‰ï¼ŒSpringerï¼Œç¬¬398â€“410é¡µã€‚
- en: 'Sirinukunwattana etÂ al. (2017) Sirinukunwattana, K., Pluim, J.Â P., Chen, H.,
    Qi, X., Heng, P.-A., Guo, Y.Â B., Wang, L.Â Y., Matuszewski, B.Â J., Bruni, E., Sanchez,
    U. etÂ al. (2017). Gland segmentation in colon histology images: The glas challenge
    contest, Medical Image Analysis  35:Â 489â€“502.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sirinukunwattana ç­‰ï¼ˆ2017ï¼‰Sirinukunwattana, K., Pluim, J. P., Chen, H., Qi, X.,
    Heng, P.-A., Guo, Y. B., Wang, L. Y., Matuszewski, B. J., Bruni, E., Sanchez,
    U. ç­‰ï¼ˆ2017ï¼‰ã€‚ç»“è‚ ç»„ç»‡å­¦å›¾åƒä¸­çš„è…ºä½“åˆ†å‰²ï¼šGLASæŒ‘æˆ˜èµ›ï¼ŒåŒ»å­¦å›¾åƒåˆ†æ 35: 489â€“502ã€‚'
- en: Sirinukunwattana etÂ al. (2016) Sirinukunwattana, K., Raza, S. E.Â A., Tsang,
    Y.-W., Snead, D.Â R., Cree, I.Â A. andÂ Rajpoot, N.Â M. (2016). Locality sensitive
    deep learning for detection and classification of nuclei in routine colon cancer
    histology images, IEEE transactions on medical imaging  35(5):Â 1196â€“1206.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sirinukunwattana ç­‰ï¼ˆ2016ï¼‰Sirinukunwattana, K., Raza, S. E. A., Tsang, Y.-W.,
    Snead, D. R., Cree, I. A. å’Œ Rajpoot, N. M.ï¼ˆ2016ï¼‰ã€‚ç”¨äºå¸¸è§„ç»“è‚ ç™Œç»„ç»‡å­¦å›¾åƒä¸­ç»†èƒæ ¸æ£€æµ‹å’Œåˆ†ç±»çš„å±€éƒ¨æ•æ„Ÿæ·±åº¦å­¦ä¹ ï¼ŒIEEEåŒ»å­¦å½±åƒå­¦æ‚å¿—
    35(5): 1196â€“1206ã€‚'
- en: 'Skrede etÂ al. (2020) Skrede, O.-J., DeÂ Raedt, S., Kleppe, A., Hveem, T.Â S.,
    LiestÃ¸l, K., Maddison, J., Askautrud, H.Â A., Pradhan, M., Nesheim, J.Â A., Albregtsen,
    F. etÂ al. (2020). Deep learning for prediction of colorectal cancer outcome: a
    discovery and validation study, The Lancet  395(10221):Â 350â€“360.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Skrede ç­‰ï¼ˆ2020ï¼‰Skrede, O.-J., De Raedt, S., Kleppe, A., Hveem, T. S., LiestÃ¸l,
    K., Maddison, J., Askautrud, H. A., Pradhan, M., Nesheim, J. A., Albregtsen, F.
    ç­‰ï¼ˆ2020ï¼‰ã€‚ç”¨äºé¢„æµ‹ç»“ç›´è‚ ç™Œç»“æœçš„æ·±åº¦å­¦ä¹ ï¼šå‘ç°å’ŒéªŒè¯ç ”ç©¶ï¼Œã€ŠæŸ³å¶åˆ€ã€‹ 395(10221): 350â€“360ã€‚'
- en: Spanhol etÂ al. (2015) Spanhol, F.Â A., Oliveira, L.Â S., Petitjean, C. andÂ Heutte,
    L. (2015). A dataset for breast cancer histopathological image classification,
    IEEE Transactions on Biomedical Engineering  63(7):Â 1455â€“1462.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Spanhol ç­‰ï¼ˆ2015ï¼‰Spanhol, F. A., Oliveira, L. S., Petitjean, C. å’Œ Heutte, L.ï¼ˆ2015ï¼‰ã€‚ä¹³è…ºç™Œç»„ç»‡ç—…ç†å›¾åƒåˆ†ç±»çš„æ•°æ®é›†ï¼ŒIEEEç”Ÿç‰©åŒ»å­¦å·¥ç¨‹å­¦æŠ¥
    63(7): 1455â€“1462ã€‚'
- en: 'Sparks andÂ Madabhushi (2016) Sparks, R. andÂ Madabhushi, A. (2016). Out-of-sample
    extrapolation utilizing semi-supervised manifold learning (ose-ssl): content based
    image retrieval for histopathology images, Scientific Reports  6(1):Â 1â€“15.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sparks å’Œ Madabhushiï¼ˆ2016ï¼‰Sparks, R. å’Œ Madabhushi, A.ï¼ˆ2016ï¼‰ã€‚åˆ©ç”¨åŠç›‘ç£æµå½¢å­¦ä¹ ï¼ˆose-sslï¼‰çš„æ ·æœ¬å¤–å¤–æ¨ï¼šåŸºäºå†…å®¹çš„ç»„ç»‡ç—…ç†å›¾åƒæ£€ç´¢ï¼Œã€Šç§‘å­¦æŠ¥å‘Šã€‹6(1):
    1â€“15ã€‚'
- en: 'Srinidhi etÂ al. (2021) Srinidhi, C.Â L., Ciga, O. andÂ Martel, A.Â L. (2021).
    Deep neural network models for computational histopathology: A survey, Medical
    Image Analysis  67:Â 101813.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Srinidhi ç­‰äººï¼ˆ2021ï¼‰Srinidhi, C. L., Ciga, O. å’Œ Martel, A. L.ï¼ˆ2021ï¼‰ã€‚è®¡ç®—ç»„ç»‡ç—…ç†å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼šç»¼è¿°ï¼Œã€ŠåŒ»å­¦å›¾åƒåˆ†æã€‹67:
    101813ã€‚'
- en: Srinidhi etÂ al. (2022) Srinidhi, C.Â L., Kim, S.Â W., Chen, F.-D. andÂ Martel,
    A.Â L. (2022). Self-supervised driven consistency training for annotation efficient
    histopathology image analysis, Medical Image Analysis  75:Â 102256.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Srinidhi ç­‰äººï¼ˆ2022ï¼‰Srinidhi, C. L., Kim, S. W., Chen, F.-D. å’Œ Martel, A. L.ï¼ˆ2022ï¼‰ã€‚è‡ªç›‘ç£é©±åŠ¨çš„ä¸€è‡´æ€§è®­ç»ƒç”¨äºæ³¨é‡Šé«˜æ•ˆçš„ç»„ç»‡ç—…ç†å›¾åƒåˆ†æï¼Œã€ŠåŒ»å­¦å›¾åƒåˆ†æã€‹75:
    102256ã€‚'
- en: Stacke etÂ al. (2021) Stacke, K., Unger, J., LundstrÃ¶m, C. andÂ Eilertsen, G.
    (2021). Learning representations with contrastive self-supervised learning for
    histopathology applications, arXiv preprint arXiv:2112.05760 .
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stacke ç­‰äººï¼ˆ2021ï¼‰Stacke, K., Unger, J., LundstrÃ¶m, C. å’Œ Eilertsen, G.ï¼ˆ2021ï¼‰ã€‚ä½¿ç”¨å¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œç»„ç»‡ç—…ç†åº”ç”¨çš„è¡¨ç¤ºå­¦ä¹ ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:2112.05760ã€‚
- en: Su etÂ al. (2019) Su, H., Shi, X., Cai, J. andÂ Yang, L. (2019). Local and global
    consistency regularized mean teacher for semi-supervised nuclei classification,
    International Conference on Medical Image Computing and Computer-Assisted Intervention,
    Springer, pp.Â 559â€“567.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su ç­‰äººï¼ˆ2019ï¼‰Su, H., Shi, X., Cai, J. å’Œ Yang, L.ï¼ˆ2019ï¼‰ã€‚å±€éƒ¨å’Œå…¨å±€ä¸€è‡´æ€§æ­£åˆ™åŒ–çš„å‡å€¼æ•™å¸ˆç”¨äºåŠç›‘ç£æ ¸åˆ†ç±»ï¼Œå›½é™…åŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„ä¼šè®®ï¼ŒSpringerï¼Œç¬¬559â€“567é¡µã€‚
- en: Su etÂ al. (2015) Su, H., Yin, Z., Huh, S., Kanade, T. andÂ Zhu, J. (2015). Interactive
    cell segmentation based on active and semi-supervised learning, IEEE Transactions
    on Medical Imaging  35(3):Â 762â€“777.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su ç­‰äººï¼ˆ2015ï¼‰Su, H., Yin, Z., Huh, S., Kanade, T. å’Œ Zhu, J.ï¼ˆ2015ï¼‰ã€‚åŸºäºä¸»åŠ¨å’ŒåŠç›‘ç£å­¦ä¹ çš„äº¤äº’å¼ç»†èƒåˆ†å‰²ï¼Œã€ŠIEEE
    åŒ»å­¦æˆåƒæ±‡åˆŠã€‹35(3): 762â€“777ã€‚'
- en: 'Su etÂ al. (2021) Su, L., Liu, Y., Wang, M. andÂ Li, A. (2021). Semi-hic: A novel
    semi-supervised deep learning method for histopathological image classification,
    Computers in Biology and Medicine  137:Â 104788.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su ç­‰äººï¼ˆ2021ï¼‰Su, L., Liu, Y., Wang, M. å’Œ Li, A.ï¼ˆ2021ï¼‰ã€‚Semi-hicï¼šä¸€ç§æ–°é¢–çš„åŠç›‘ç£æ·±åº¦å­¦ä¹ æ–¹æ³•ç”¨äºç»„ç»‡ç—…ç†å›¾åƒåˆ†ç±»ï¼Œã€Šç”Ÿç‰©åŒ»å­¦è®¡ç®—æœºã€‹137:
    104788ã€‚'
- en: Swiderska-Chadaj etÂ al. (2019) Swiderska-Chadaj, Z., Pinckaers, H., van Rijthoven,
    M., Balkenhol, M., Melnikova, M., Geessink, O., Manson, Q., Sherman, M., Polonia,
    A., Parry, J. etÂ al. (2019). Learning to detect lymphocytes in immunohistochemistry
    with deep learning, Medical Image Analysis  58:Â 101547.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Swiderska-Chadaj ç­‰äººï¼ˆ2019ï¼‰Swiderska-Chadaj, Z., Pinckaers, H., van Rijthoven,
    M., Balkenhol, M., Melnikova, M., Geessink, O., Manson, Q., Sherman, M., Polonia,
    A., Parry, J. ç­‰äººï¼ˆ2019ï¼‰ã€‚åœ¨å…ç–«ç»„ç»‡åŒ–å­¦ä¸­ä½¿ç”¨æ·±åº¦å­¦ä¹ æ£€æµ‹æ·‹å·´ç»†èƒï¼Œã€ŠåŒ»å­¦å›¾åƒåˆ†æã€‹58: 101547ã€‚'
- en: 'Tajbakhsh etÂ al. (2020) Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J.Â N.,
    Wu, Z. andÂ Ding, X. (2020). Embracing imperfect datasets: A review of deep learning
    solutions for medical image segmentation, Medical Image Analysis  63:Â 101693.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tajbakhsh ç­‰äººï¼ˆ2020ï¼‰Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J. N., Wu,
    Z. å’Œ Ding, X.ï¼ˆ2020ï¼‰ã€‚æ‹¥æŠ±ä¸å®Œç¾çš„æ•°æ®é›†ï¼šæ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨ç»¼è¿°ï¼Œã€ŠåŒ»å­¦å›¾åƒåˆ†æã€‹63: 101693ã€‚'
- en: 'Tarvainen andÂ Valpola (2017) Tarvainen, A. andÂ Valpola, H. (2017). Mean teachers
    are better role models: Weight-averaged consistency targets improve semi-supervised
    deep learning results, Advances in Neural Information Processing Systems  30.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarvainen å’Œ Valpolaï¼ˆ2017ï¼‰Tarvainen, A. å’Œ Valpola, H.ï¼ˆ2017ï¼‰ã€‚å‡å€¼æ•™å¸ˆæ˜¯æ›´å¥½çš„æ¦œæ ·ï¼šæƒé‡å¹³å‡çš„ä¸€è‡´æ€§ç›®æ ‡æ”¹å–„åŠç›‘ç£æ·±åº¦å­¦ä¹ ç»“æœï¼Œã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹30ã€‚
- en: TCGA (2019) TCGA (2019). The cancer genome atlas, [https://www.cancer.gov/tcga](https://www.cancer.gov/tcga).
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TCGAï¼ˆ2019ï¼‰TCGAï¼ˆ2019ï¼‰ã€‚ç™Œç—‡åŸºå› ç»„å›¾è°±ï¼Œ[https://www.cancer.gov/tcga](https://www.cancer.gov/tcga)ã€‚
- en: 'Team etÂ al. (2011) Team, N. L. S. T.Â R. etÂ al. (2011). The national lung screening
    trial: overview and study design, Radiology  258(1):Â 243.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team ç­‰äººï¼ˆ2011ï¼‰Team, N. L. S. T. R. ç­‰äººï¼ˆ2011ï¼‰ã€‚å›½å®¶è‚ºç­›æŸ¥è¯•éªŒï¼šæ¦‚è¿°å’Œç ”ç©¶è®¾è®¡ï¼Œã€Šæ”¾å°„å­¦ã€‹258(1): 243ã€‚'
- en: Tellez etÂ al. (2019) Tellez, D., Litjens, G., vanÂ der Laak, J. andÂ Ciompi, F.
    (2019). Neural image compression for gigapixel histopathology image analysis,
    IEEE Transactions on Pattern Analysis and Machine Intelligence  43(2):Â 567â€“578.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tellez ç­‰äººï¼ˆ2019ï¼‰Tellez, D., Litjens, G., van der Laak, J. å’Œ Ciompi, F.ï¼ˆ2019ï¼‰ã€‚ç”¨äºåƒå…†åƒç´ ç»„ç»‡ç—…ç†å›¾åƒåˆ†æçš„ç¥ç»å›¾åƒå‹ç¼©ï¼Œã€ŠIEEE
    å›¾æ¡ˆåˆ†æä¸æœºå™¨æ™ºèƒ½æ±‡åˆŠã€‹43(2): 567â€“578ã€‚'
- en: Tolkach etÂ al. (2020) Tolkach, Y., DohmgÃ¶rgen, T., Toma, M. andÂ Kristiansen,
    G. (2020). High-accuracy prostate cancer pathology using deep learning, Nature
    Machine Intelligence  2(7):Â 411â€“418.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tolkach ç­‰äºº (2020) Tolkach, Y., DohmgÃ¶rgen, T., Toma, M. å’Œ Kristiansen, G. (2020).
    ä½¿ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œé«˜å‡†ç¡®åº¦å‰åˆ—è…ºç™Œç—…ç†åˆ†æï¼ŒNature Machine Intelligence 2(7): 411â€“418ã€‚'
- en: Tomita etÂ al. (2019) Tomita, N., Abdollahi, B., Wei, J., Ren, B., Suriawinata,
    A. andÂ Hassanpour, S. (2019). Attention-based deep neural networks for detection
    of cancerous and precancerous esophagus tissue on histopathological slides, JAMA
    Network Open  2(11):Â e1914645â€“e1914645.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tomita ç­‰äºº (2019) Tomita, N., Abdollahi, B., Wei, J., Ren, B., Suriawinata,
    A. å’Œ Hassanpour, S. (2019). åŸºäºæ³¨æ„åŠ›çš„æ·±åº¦ç¥ç»ç½‘ç»œç”¨äºæ£€æµ‹ç™Œå˜å’Œå‰ç™Œå˜çš„é£Ÿé“ç»„ç»‡åœ¨ç»„ç»‡ç—…ç†åˆ‡ç‰‡ä¸Šçš„è¡¨ç°ï¼ŒJAMA Network
    Open 2(11): e1914645â€“e1914645ã€‚'
- en: Tu etÂ al. (2019) Tu, M., Huang, J., He, X. andÂ Zhou, B. (2019). Multiple instance
    learning with graph neural networks, arXiv preprint arXiv:1906.04881 .
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu ç­‰äºº (2019) Tu, M., Huang, J., He, X. å’Œ Zhou, B. (2019). åŸºäºå›¾ç¥ç»ç½‘ç»œçš„å¤šå®ä¾‹å­¦ä¹ ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:1906.04881ã€‚
- en: VanÂ den Oord etÂ al. (2018) VanÂ den Oord, A., Li, Y. andÂ Vinyals, O. (2018).
    Representation learning with contrastive predictive coding, arXiv e-prints pp.Â arXivâ€“1807.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van den Oord ç­‰äºº (2018) Van den Oord, A., Li, Y. å’Œ Vinyals, O. (2018). ä½¿ç”¨å¯¹æ¯”é¢„æµ‹ç¼–ç è¿›è¡Œè¡¨å¾å­¦ä¹ ï¼ŒarXiv
    e-printsï¼Œç¬¬arXivâ€“1807é¡µã€‚
- en: VanÂ Engelen andÂ Hoos (2020) VanÂ Engelen, J.Â E. andÂ Hoos, H.Â H. (2020). A survey
    on semi-supervised learning, Machine Learning  109(2):Â 373â€“440.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Van Engelen å’Œ Hoos (2020) Van Engelen, J. E. å’Œ Hoos, H. H. (2020). åŠç›‘ç£å­¦ä¹ çš„è°ƒæŸ¥ï¼ŒMachine
    Learning 109(2): 373â€“440ã€‚'
- en: Veeling etÂ al. (2018) Veeling, B.Â S., Linmans, J., Winkens, J., Cohen, T. andÂ Welling,
    M. (2018). Rotation equivariant cnns for digital pathology, International Conference
    on Medical Image Computing and Computer-assisted Intervention, Springer, pp.Â 210â€“218.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veeling ç­‰äºº (2018) Veeling, B. S., Linmans, J., Winkens, J., Cohen, T. å’Œ Welling,
    M. (2018). é’ˆå¯¹æ•°å­—ç—…ç†çš„æ—‹è½¬ç­‰å˜å·ç§¯ç¥ç»ç½‘ç»œï¼Œå›½é™…åŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„ä¼šè®®ï¼ŒSpringerï¼Œç¬¬210â€“218é¡µã€‚
- en: Velmahos etÂ al. (2021) Velmahos, C.Â S., Badgeley, M. andÂ Lo, Y.-C. (2021). Using
    deep learning to identify bladder cancers with fgfr-activating mutations from
    histology images, Cancer Medicine  10(14):Â 4805â€“4813.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Velmahos ç­‰äºº (2021) Velmahos, C. S., Badgeley, M. å’Œ Lo, Y.-C. (2021). ä½¿ç”¨æ·±åº¦å­¦ä¹ ä»ç»„ç»‡å­¦å›¾åƒä¸­è¯†åˆ«å…·æœ‰
    fgfr æ¿€æ´»çªå˜çš„è†€èƒ±ç™Œï¼ŒCancer Medicine 10(14): 4805â€“4813ã€‚'
- en: 'Veta etÂ al. (2019) Veta, M., Heng, Y.Â J., Stathonikos, N., Bejnordi, B.Â E.,
    Beca, F., Wollmann, T., Rohr, K., Shah, M.Â A., Wang, D., Rousson, M. etÂ al. (2019).
    Predicting breast tumor proliferation from whole-slide images: the tupac16 challenge,
    Medical Image Analysis  54:Â 111â€“121.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Veta ç­‰äºº (2019) Veta, M., Heng, Y. J., Stathonikos, N., Bejnordi, B. E., Beca,
    F., Wollmann, T., Rohr, K., Shah, M. A., Wang, D., Rousson, M. ç­‰ (2019). ä»å…¨åˆ‡ç‰‡å›¾åƒé¢„æµ‹ä¹³è…ºè‚¿ç˜¤å¢æ®–ï¼štupac16
    æŒ‘æˆ˜ï¼ŒMedical Image Analysis 54: 111â€“121ã€‚'
- en: Vincent etÂ al. (2008) Vincent, P., Larochelle, H., Bengio, Y. andÂ Manzagol,
    P.-A. (2008). Extracting and composing robust features with denoising autoencoders,
    Proceedings of the 25th International Conference on Machine Learning, pp.Â 1096â€“1103.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent ç­‰äºº (2008) Vincent, P., Larochelle, H., Bengio, Y. å’Œ Manzagol, P.-A.
    (2008). ä½¿ç”¨å»å™ªè‡ªç¼–ç å™¨æå–å’Œç»„åˆé²æ£’ç‰¹å¾ï¼Œç¬¬25å±Šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬1096â€“1103é¡µã€‚
- en: Wang, Lee, Calista, Zhou, Zhu, Suzuki, Komura, Ishikawa andÂ Cheng (2018) Wang,
    C.-W., Lee, Y.-C., Calista, E., Zhou, F., Zhu, H., Suzuki, R., Komura, D., Ishikawa,
    S. andÂ Cheng, S.-P. (2018). A benchmark for comparing precision medicine methods
    in thyroid cancer diagnosis using tissue microarrays, Bioinformatics  34(10):Â 1767â€“1773.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang, Lee, Calista, Zhou, Zhu, Suzuki, Komura, Ishikawa å’Œ Cheng (2018) Wang,
    C.-W., Lee, Y.-C., Calista, E., Zhou, F., Zhu, H., Suzuki, R., Komura, D., Ishikawa,
    S. å’Œ Cheng, S.-P. (2018). ä½¿ç”¨ç»„ç»‡å¾®é˜µåˆ—æ¯”è¾ƒç”²çŠ¶è…ºç™Œè¯Šæ–­ä¸­çš„ç²¾å‡†åŒ»å­¦æ–¹æ³•çš„åŸºå‡†ï¼ŒBioinformatics 34(10): 1767â€“1773ã€‚'
- en: Wang etÂ al. (2019) Wang, X., Chen, H., Gan, C., Lin, H., Dou, Q., Tsougenis,
    E., Huang, Q., Cai, M. andÂ Heng, P.-A. (2019). Weakly supervised deep learning
    for whole slide lung cancer image analysis, IEEE Transactions on Cybernetics  50(9):Â 3950â€“3962.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang ç­‰äºº (2019) Wang, X., Chen, H., Gan, C., Lin, H., Dou, Q., Tsougenis, E.,
    Huang, Q., Cai, M. å’Œ Heng, P.-A. (2019). å¼±ç›‘ç£æ·±åº¦å­¦ä¹ ç”¨äºå…¨åˆ‡ç‰‡è‚ºç™Œå›¾åƒåˆ†æï¼ŒIEEE Transactions
    on Cybernetics 50(9): 3950â€“3962ã€‚'
- en: Wang, Yan, Tang, Bai andÂ Liu (2018) Wang, X., Yan, Y., Tang, P., Bai, X. andÂ Liu,
    W. (2018). Revisiting multiple instance neural networks, Pattern Recognition  74:Â 15â€“24.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang, Yan, Tang, Bai å’Œ Liu (2018) Wang, X., Yan, Y., Tang, P., Bai, X. å’Œ Liu,
    W. (2018). é‡æ–°å®¡è§†å¤šå®ä¾‹ç¥ç»ç½‘ç»œï¼ŒPattern Recognition 74: 15â€“24ã€‚'
- en: 'Wang etÂ al. (2021) Wang, X., Yang, S., Zhang, J., Wang, M., Zhang, J., Huang,
    J., Yang, W. andÂ Han, X. (2021). Transpath: Transformer-based self-supervised
    learning for histopathological image classification, International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, pp.Â 186â€“195.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‹ç­‰ï¼ˆ2021ï¼‰ç‹æ—­ã€æ¨ç¿ã€å¼ æ°ã€ç‹ç¾ã€å¼ æ·ã€é»„æ°ã€æ¨å¨å’ŒéŸ©æ—­ï¼ˆ2021ï¼‰ã€‚Transpathï¼šåŸºäºå˜æ¢å™¨çš„è‡ªç›‘ç£å­¦ä¹ ç”¨äºç»„ç»‡ç—…ç†å›¾åƒåˆ†ç±»ï¼Œã€Šå›½é™…åŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬186â€“195é¡µã€‚
- en: Ward andÂ Hawkins (2015) Ward, R.Â L. andÂ Hawkins, N.Â J. (2015). Molecular and
    cellular oncology (mco) study tumour collection, UNSW Australia .
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ²ƒå¾·å’Œéœé‡‘æ–¯ï¼ˆ2015ï¼‰æ²ƒå¾·Â·R. L.å’Œéœé‡‘æ–¯Â·N. J.ï¼ˆ2015ï¼‰ã€‚åˆ†å­å’Œç»†èƒè‚¿ç˜¤å­¦ï¼ˆmcoï¼‰ç ”ç©¶è‚¿ç˜¤æ”¶é›†ï¼Œæ¾³å¤§åˆ©äºšæ–°å—å¨å°”å£«å¤§å­¦ã€‚
- en: Wei etÂ al. (2019) Wei, J.Â W., Tafe, L.Â J., Linnik, Y.Â A., Vaickus, L.Â J., Tomita,
    N. andÂ Hassanpour, S. (2019). Pathologist-level classification of histologic patterns
    on resected lung adenocarcinoma slides with deep neural networks, Scientific Reports  9(1):Â 1â€“8.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'é­ç­‰ï¼ˆ2019ï¼‰é­é™æ–‡ã€å¡”å¤«Â·L. J.ã€æ—å°¼å…‹Â·Y. A.ã€ç“¦ä¼Šå…‹æ–¯Â·L. J.ã€å¯Œç”°ç›´å’Œå“ˆæ¡‘æ™®å°”ï¼ˆ2019ï¼‰ã€‚ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå¯¹åˆ‡é™¤çš„è‚ºè…ºç™Œåˆ‡ç‰‡è¿›è¡Œç—…ç†å­¦æ¨¡å¼çš„ç—…ç†å­¦å®¶çº§åˆ†ç±»ï¼Œã€Šç§‘å­¦æŠ¥å‘Šã€‹9(1):
    1â€“8ã€‚'
- en: Wessels etÂ al. (2021) Wessels, F., Schmitt, M., Krieghoff-Henning, E., Jutzi,
    T., Worst, T.Â S., Waldbillig, F., Neuberger, M., Maron, R.Â C., Steeg, M., Gaiser,
    T. etÂ al. (2021). Deep learning approach to predict lymph node metastasis directly
    from primary tumour histology in prostate cancer, BJU International  128(3):Â 352â€“360.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'éŸ¦å¡å°”æ–¯ç­‰ï¼ˆ2021ï¼‰éŸ¦å¡å°”æ–¯ã€æ–½å¯†ç‰¹ã€å…‹é›·éœå¤«-äº¨å®ã€æœ±èŒ¨ã€æ²ƒæ–¯Â·T. S.ã€ç“¦å°”æ¯”åˆ©å¸Œã€çº½ä¼¯æ ¼ã€é©¬é¾™Â·R. C.ã€æ–¯è’‚æ ¼ã€ç›–ç‘Ÿç­‰ï¼ˆ2021ï¼‰ã€‚ä¸€ç§æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºç›´æ¥ä»å‰åˆ—è…ºç™ŒåŸå‘è‚¿ç˜¤ç»„ç»‡å­¦é¢„æµ‹æ·‹å·´ç»“è½¬ç§»ï¼Œã€ŠBJUå›½é™…ã€‹128(3):
    352â€“360ã€‚'
- en: 'Weston etÂ al. (2012) Weston, J., Ratle, F., Mobahi, H. andÂ Collobert, R. (2012).
    Deep learning via semi-supervised embedding, Neural Networks: Tricks of the Trade,
    Springer, pp.Â 639â€“655.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éŸ¦æ–¯é¡¿ç­‰ï¼ˆ2012ï¼‰éŸ¦æ–¯é¡¿ã€æ‹‰ç‰¹å°”ã€è«å·´å¸Œå’Œç§‘æ´›è´å°”ï¼ˆ2012ï¼‰ã€‚é€šè¿‡åŠç›‘ç£åµŒå…¥è¿›è¡Œæ·±åº¦å­¦ä¹ ï¼Œã€Šç¥ç»ç½‘ç»œï¼šæŠ€æœ¯è¯€çªã€‹ï¼ŒSpringerï¼Œç¬¬639â€“655é¡µã€‚
- en: Woerl etÂ al. (2020) Woerl, A.-C., Eckstein, M., Geiger, J., Wagner, D.Â C., Daher,
    T., Stenzel, P., Fernandez, A., Hartmann, A., Wand, M., Roth, W. etÂ al. (2020).
    Deep learning predicts molecular subtype of muscle-invasive bladder cancer from
    conventional histopathological slides, European Urology  78(2):Â 256â€“264.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'æ²ƒå°”ç­‰ï¼ˆ2020ï¼‰æ²ƒå°”Â·A.-C.ã€è‰¾å…‹æ–¯å¦ã€ç›–æ ¼å°”ã€ç“¦æ ¼çº³Â·D. C.ã€è¾¾èµ«ã€æ–¯æ»•æ³½å°”ã€è´¹å°”å—å¾·æ–¯ã€å“ˆç‰¹æ›¼ã€ä¸‡å¾·ã€ç½—æ–¯ç­‰ï¼ˆ2020ï¼‰ã€‚æ·±åº¦å­¦ä¹ é¢„æµ‹è‚Œä¾µè¢­æ€§è†€èƒ±ç™Œçš„åˆ†å­äºšå‹ï¼Œä»ä¼ ç»Ÿç»„ç»‡ç—…ç†åˆ‡ç‰‡ä¸­ï¼Œã€Šæ¬§æ´²æ³Œå°¿å­¦ã€‹78(2):
    256â€“264ã€‚'
- en: Wu etÂ al. (2015) Wu, J., Yu, Y., Huang, C. andÂ Yu, K. (2015). Deep multiple
    instance learning for image classification and auto-annotation, Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp.Â 3460â€“3469.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å´ç­‰ï¼ˆ2015ï¼‰å´å³»ã€ä½™é˜³ã€é»„æ¥šå’Œä½™å‡¯ï¼ˆ2015ï¼‰ã€‚ç”¨äºå›¾åƒåˆ†ç±»å’Œè‡ªåŠ¨æ ‡æ³¨çš„æ·±åº¦å¤šå®ä¾‹å­¦ä¹ ï¼Œã€ŠIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ã€‹ï¼Œç¬¬3460â€“3469é¡µã€‚
- en: Xie, Dai, Hovy, Luong andÂ Le (2020) Xie, Q., Dai, Z., Hovy, E., Luong, T. andÂ Le,
    Q. (2020). Unsupervised data augmentation for consistency training, Advances in
    Neural Information Processing Systems  33:Â 6256â€“6268.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'è°¢ã€æˆ´ã€éœç»´ã€éš†å’Œä¹ï¼ˆ2020ï¼‰è°¢å¼ºã€æˆ´å¿—ä¼Ÿã€éœç»´ã€éš†æ¶›å’Œä¹å¼ºï¼ˆ2020ï¼‰ã€‚ç”¨äºä¸€è‡´æ€§è®­ç»ƒçš„æ— ç›‘ç£æ•°æ®å¢å¼ºï¼Œã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹33: 6256â€“6268ã€‚'
- en: Xie, Chen, Li andÂ Zheng (2020) Xie, X., Chen, J., Li, K. andÂ Zheng, Y. (2020).
    Instance-aware self-supervised learning for nuclei segmentation, International
    Conference on Medical Image Computing and Computer-assisted Intervention, Springer,
    pp.Â 341â€“350.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°¢ã€é™ˆã€æå’Œéƒ‘ï¼ˆ2020ï¼‰è°¢é‘«ã€é™ˆä½³ã€æå‡¯å’Œéƒ‘é¢–ï¼ˆ2020ï¼‰ã€‚åŸºäºå®ä¾‹çš„è‡ªç›‘ç£å­¦ä¹ ç”¨äºç»†èƒæ ¸åˆ†å‰²ï¼Œã€Šå›½é™…åŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬341â€“350é¡µã€‚
- en: 'Xu etÂ al. (2019) Xu, G., Song, Z., Sun, Z., Ku, C., Yang, Z., Liu, C., Wang,
    S., Ma, J. andÂ Xu, W. (2019). Camel: A weakly supervised learning framework for
    histopathology image segmentation, Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp.Â 10682â€“10691.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾ç­‰ï¼ˆ2019ï¼‰å¾å…‰ã€å®‹å¿—ã€å­™æ³½ã€åº“æˆã€æ¨å¿—ã€åˆ˜è¶…ã€ç‹æ™Ÿã€é©¬æ°å’Œå¾ä¼Ÿï¼ˆ2019ï¼‰ã€‚Camelï¼šç”¨äºç»„ç»‡ç—…ç†å›¾åƒåˆ†å‰²çš„å¼±ç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œã€ŠIEEE/CVFè®¡ç®—æœºè§†è§‰å›½é™…ä¼šè®®è®ºæ–‡é›†ã€‹ï¼Œç¬¬10682â€“10691é¡µã€‚
- en: Xu etÂ al. (2020) Xu, J., Hou, J., Zhang, Y., Feng, R., Ruan, C., Zhang, T. andÂ Fan,
    W. (2020). Data-efficient histopathology image analysis with deformation representation
    learning, 2020 IEEE International Conference on Bioinformatics and Biomedicine
    (BIBM), IEEE, pp.Â 857â€“864.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾ç­‰ï¼ˆ2020ï¼‰å¾ä½³ã€ä¾¯ä¿Šã€å¼ è‰ºã€å†¯ç¿ã€é˜®æ™¨ã€å¼ å©·å’ŒèŒƒä¼Ÿï¼ˆ2020ï¼‰ã€‚å…·æœ‰å˜å½¢è¡¨ç¤ºå­¦ä¹ çš„æ•°æ®é«˜æ•ˆç»„ç»‡ç—…ç†å›¾åƒåˆ†æï¼Œ2020 IEEEå›½é™…ç”Ÿç‰©ä¿¡æ¯å­¦ä¸ç”Ÿç‰©åŒ»å­¦ä¼šè®®ï¼ˆBIBMï¼‰ï¼ŒIEEEï¼Œç¬¬857â€“864é¡µã€‚
- en: Xu etÂ al. (2016) Xu, K., Su, H., Zhu, J., Guan, J.-S. andÂ Zhang, B. (2016).
    Neuron segmentation based on cnn with semi-supervised regularization, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp.Â 20â€“28.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu ç­‰äººï¼ˆ2016ï¼‰ Xu, K., Su, H., Zhu, J., Guan, J.-S. å’Œ Zhang, B.ï¼ˆ2016ï¼‰ã€‚åŸºäºCNNçš„ç¥ç»å…ƒåˆ†å‰²ä¸åŠç›‘ç£æ­£åˆ™åŒ–ï¼Œã€ŠIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ã€‹ï¼Œç¬¬20â€“28é¡µã€‚
- en: Xu etÂ al. (2022) Xu, L., Ouyang, W., Bennamoun, M., Boussaid, F. andÂ Xu, D.
    (2022). Multi-class token transformer for weakly supervised semantic segmentation,
    arXiv preprint arXiv:2203.02891 .
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu ç­‰äººï¼ˆ2022ï¼‰ Xu, L., Ouyang, W., Bennamoun, M., Boussaid, F. å’Œ Xu, D.ï¼ˆ2022ï¼‰ã€‚ç”¨äºå¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²çš„å¤šç±»æ ‡è®°è½¬æ¢å™¨ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:2203.02891ã€‚
- en: Yalniz etÂ al. (2019) Yalniz, I.Â Z., JÃ©gou, H., Chen, K., Paluri, M. andÂ Mahajan,
    D. (2019). Billion-scale semi-supervised learning for image classification, arXiv
    preprint arXiv:1905.00546 .
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yalniz ç­‰äººï¼ˆ2019ï¼‰ Yalniz, I. Z., JÃ©gou, H., Chen, K., Paluri, M. å’Œ Mahajan, D.ï¼ˆ2019ï¼‰ã€‚ç”¨äºå›¾åƒåˆ†ç±»çš„åäº¿è§„æ¨¡åŠç›‘ç£å­¦ä¹ ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:1905.00546ã€‚
- en: Yan etÂ al. (2018) Yan, Y., Wang, X., Guo, X., Fang, J., Liu, W. andÂ Huang, J.
    (2018). Deep multi-instance learning with dynamic pooling, Asian Conference on
    Machine Learning, PMLR, pp.Â 662â€“677.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan ç­‰äººï¼ˆ2018ï¼‰ Yan, Y., Wang, X., Guo, X., Fang, J., Liu, W. å’Œ Huang, J.ï¼ˆ2018ï¼‰ã€‚å…·æœ‰åŠ¨æ€æ± åŒ–çš„æ·±åº¦å¤šå®ä¾‹å­¦ä¹ ï¼Œã€Šäºšæ´²æœºå™¨å­¦ä¹ ä¼šè®®ã€‹ï¼ŒPMLRï¼Œç¬¬662â€“677é¡µã€‚
- en: Yang etÂ al. (2022) Yang, J., Ju, J., Guo, L., Ji, B., Shi, S., Yang, Z., Gao,
    S., Yuan, X., Tian, G., Liang, Y. etÂ al. (2022). Prediction of her2-positive breast
    cancer recurrence and metastasis risk from histopathological images and clinical
    information via multimodal deep learning, Computational and Structural Biotechnology
    Journal  20:Â 333â€“342.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang ç­‰äººï¼ˆ2022ï¼‰ Yang, J., Ju, J., Guo, L., Ji, B., Shi, S., Yang, Z., Gao, S.,
    Yuan, X., Tian, G., Liang, Y. ç­‰äººï¼ˆ2022ï¼‰ã€‚é€šè¿‡å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ ä»ç»„ç»‡ç—…ç†å›¾åƒå’Œä¸´åºŠä¿¡æ¯ä¸­é¢„æµ‹ HER2 é˜³æ€§ä¹³è…ºç™Œå¤å‘å’Œè½¬ç§»é£é™©ï¼Œã€Šè®¡ç®—ä¸ç»“æ„ç”Ÿç‰©æŠ€æœ¯æœŸåˆŠã€‹
    20: 333â€“342ã€‚'
- en: 'Yang etÂ al. (2017) Yang, L., Zhang, Y., Chen, J., Zhang, S. andÂ Chen, D.Â Z.
    (2017). Suggestive annotation: A deep active learning framework for biomedical
    image segmentation, International Conference on Medical Image Computing and Computer-assisted
    Intervention, Springer, pp.Â 399â€“407.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang ç­‰äººï¼ˆ2017ï¼‰ Yang, L., Zhang, Y., Chen, J., Zhang, S. å’Œ Chen, D. Z.ï¼ˆ2017ï¼‰ã€‚å»ºè®®æ€§æ³¨é‡Šï¼šä¸€ç§ç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ·±åº¦ä¸»åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œã€ŠåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬399â€“407é¡µã€‚
- en: Yang etÂ al. (2021) Yang, P., Hong, Z., Yin, X., Zhu, C. andÂ Jiang, R. (2021).
    Self-supervised visual representation learning for histopathological images, International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer,
    pp.Â 47â€“57.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang ç­‰äººï¼ˆ2021ï¼‰ Yang, P., Hong, Z., Yin, X., Zhu, C. å’Œ Jiang, R.ï¼ˆ2021ï¼‰ã€‚ç”¨äºç»„ç»‡ç—…ç†å›¾åƒçš„è‡ªç›‘ç£è§†è§‰è¡¨ç¤ºå­¦ä¹ ï¼Œã€ŠåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬47â€“57é¡µã€‚
- en: Yang etÂ al. (2020) Yang, P., Zhai, Y., Li, L., Lv, H., Wang, J., Zhu, C. andÂ Jiang,
    R. (2020). A deep metric learning approach for histopathological image retrieval,
    Methods  179:Â 14â€“25.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang ç­‰äººï¼ˆ2020ï¼‰ Yang, P., Zhai, Y., Li, L., Lv, H., Wang, J., Zhu, C. å’Œ Jiang,
    R.ï¼ˆ2020ï¼‰ã€‚ä¸€ç§ç”¨äºç»„ç»‡ç—…ç†å›¾åƒæ£€ç´¢çš„æ·±åº¦åº¦é‡å­¦ä¹ æ–¹æ³•ï¼Œã€Šæ–¹æ³•ã€‹ 179: 14â€“25ã€‚'
- en: Yao etÂ al. (2020) Yao, J., Zhu, X., Jonnagaddala, J., Hawkins, N. andÂ Huang,
    J. (2020). Whole slide images based cancer survival prediction using attention
    guided deep multiple instance learning networks, Medical Image Analysis  65:Â 101789.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao ç­‰äººï¼ˆ2020ï¼‰ Yao, J., Zhu, X., Jonnagaddala, J., Hawkins, N. å’Œ Huang, J.ï¼ˆ2020ï¼‰ã€‚åŸºäºå…¨åˆ‡ç‰‡å›¾åƒçš„ç™Œç—‡ç”Ÿå­˜é¢„æµ‹ï¼Œä½¿ç”¨æ³¨æ„åŠ›å¼•å¯¼çš„æ·±åº¦å¤šå®ä¾‹å­¦ä¹ ç½‘ç»œï¼Œã€ŠåŒ»å­¦å›¾åƒåˆ†æã€‹
    65: 101789ã€‚'
- en: Yu etÂ al. (2016) Yu, K.-H., Zhang, C., Berry, G.Â J., Altman, R.Â B., RÃ©, C.,
    Rubin, D.Â L. andÂ Snyder, M. (2016). Predicting non-small cell lung cancer prognosis
    by fully automated microscopic pathology image features, Nature Communications  7(1):Â 1â€“10.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu ç­‰äººï¼ˆ2016ï¼‰ Yu, K.-H., Zhang, C., Berry, G. J., Altman, R. B., RÃ©, C., Rubin,
    D. L. å’Œ Snyder, M.ï¼ˆ2016ï¼‰ã€‚é€šè¿‡å®Œå…¨è‡ªåŠ¨åŒ–çš„æ˜¾å¾®ç—…ç†å›¾åƒç‰¹å¾é¢„æµ‹éå°ç»†èƒè‚ºç™Œé¢„åï¼Œã€Šè‡ªç„¶é€šè®¯ã€‹ 7(1): 1â€“10ã€‚'
- en: Zhang etÂ al. (2016) Zhang, R., Isola, P. andÂ Efros, A.Â A. (2016). Colorful image
    colorization, European Conference on Computer Vision, Springer, pp.Â 649â€“666.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰äººï¼ˆ2016ï¼‰ Zhang, R., Isola, P. å’Œ Efros, A. A.ï¼ˆ2016ï¼‰ã€‚è‰²å½©ä¸°å¯Œçš„å›¾åƒä¸Šè‰²ï¼Œã€Šæ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬649â€“666é¡µã€‚
- en: Zhang etÂ al. (2017) Zhang, Y., Yang, L., Chen, J., Fredericksen, M., Hughes,
    D.Â P. andÂ Chen, D.Â Z. (2017). Deep adversarial networks for biomedical image segmentation
    utilizing unannotated images, International Conference on Medical Image Computing
    and Computer-assisted Intervention, Springer, pp.Â 408â€“416.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰äººï¼ˆ2017ï¼‰ Zhang, Y., Yang, L., Chen, J., Fredericksen, M., Hughes, D. P.
    å’Œ Chen, D. Z.ï¼ˆ2017ï¼‰ã€‚åˆ©ç”¨æœªæ ‡æ³¨å›¾åƒçš„æ·±åº¦å¯¹æŠ—ç½‘ç»œç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œã€ŠåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ã€‹ï¼ŒSpringerï¼Œç¬¬408â€“416é¡µã€‚
- en: Zhao etÂ al. (2020) Zhao, Y., Yang, F., Fang, Y., Liu, H., Zhou, N., Zhang, J.,
    Sun, J., Yang, S., Menze, B., Fan, X. etÂ al. (2020). Predicting lymph node metastasis
    using histopathological images based on multiple instance learning with deep graph
    convolution, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp.Â 4837â€“4846.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao ç­‰ (2020) Zhao, Y., Yang, F., Fang, Y., Liu, H., Zhou, N., Zhang, J., Sun,
    J., Yang, S., Menze, B., Fan, X. ç­‰ (2020). åŸºäºå¤šå®ä¾‹å­¦ä¹ ä¸æ·±åº¦å›¾å·ç§¯çš„ç»„ç»‡ç—…ç†å›¾åƒé¢„æµ‹æ·‹å·´ç»“è½¬ç§»ï¼ŒIEEE/CVFè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬4837â€“4846é¡µã€‚
- en: Zheng etÂ al. (2019) Zheng, H., Yang, L., Chen, J., Han, J., Zhang, Y., Liang,
    P., Zhao, Z., Wang, C. andÂ Chen, D.Â Z. (2019). Biomedical image segmentation via
    representative annotation, Proceedings of the AAAI Conference on Artificial Intelligence,
    Vol.Â 33, pp.Â 5901â€“5908.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng ç­‰ (2019) Zheng, H., Yang, L., Chen, J., Han, J., Zhang, Y., Liang, P.,
    Zhao, Z., Wang, C. å’Œ Chen, D. Z. (2019). é€šè¿‡ä»£è¡¨æ€§æ³¨é‡Šè¿›è¡Œç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ŒAAAIäººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬33å·ï¼Œç¬¬5901â€“5908é¡µã€‚
- en: Zhou etÂ al. (2020) Zhou, Y., Chen, H., Lin, H. andÂ Heng, P.-A. (2020). Deep
    semi-supervised knowledge distillation for overlapping cervical cell instance
    segmentation, International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, pp.Â 521â€“531.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou ç­‰ (2020) Zhou, Y., Chen, H., Lin, H. å’Œ Heng, P.-A. (2020). ç”¨äºé‡å å®«é¢ˆç»†èƒå®ä¾‹åˆ†å‰²çš„æ·±åº¦åŠç›‘ç£çŸ¥è¯†è’¸é¦ï¼ŒåŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ï¼ŒSpringerï¼Œç¬¬521â€“531é¡µã€‚
- en: 'Zhou andÂ Li (2005) Zhou, Z.-H. andÂ Li, M. (2005). Tri-training: Exploiting
    unlabeled data using three classifiers, IEEE Transactions on Knowledge and Data
    Engineering  17(11):Â 1529â€“1541.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou å’Œ Li (2005) Zhou, Z.-H. å’Œ Li, M. (2005). ä¸‰é‡è®­ç»ƒï¼šåˆ©ç”¨ä¸‰ä¸ªåˆ†ç±»å™¨çš„æ— æ ‡ç­¾æ•°æ®ï¼ŒIEEEçŸ¥è¯†ä¸æ•°æ®å·¥ç¨‹å­¦æŠ¥
    17(11): 1529â€“1541ã€‚'
- en: Zhou etÂ al. (2021) Zhou, Z., Sodha, V., Pang, J., Gotway, M.Â B. andÂ Liang, J.
    (2021). Models genesis, Medical Image Analysis  67:Â 101840.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou ç­‰ (2021) Zhou, Z., Sodha, V., Pang, J., Gotway, M. B. å’Œ Liang, J. (2021).
    æ¨¡å‹èµ·æºï¼ŒåŒ»å­¦å›¾åƒåˆ†æ 67: 101840ã€‚'
- en: Zhu (2005) Zhu, X.Â J. (2005). Semi-supervised learning literature survey, CS
    Technical Reports .
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu (2005) Zhu, X. J. (2005). åŠç›‘ç£å­¦ä¹ æ–‡çŒ®ç»¼è¿°ï¼Œè®¡ç®—æœºç§‘å­¦æŠ€æœ¯æŠ¥å‘Šã€‚
- en: 'Zhu etÂ al. (2017) Zhu, X., Yao, J., Zhu, F. andÂ Huang, J. (2017). Wsisa: Making
    survival prediction from whole slide histopathological images, Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp.Â 7234â€“7242.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu ç­‰ (2017) Zhu, X., Yao, J., Zhu, F. å’Œ Huang, J. (2017). Wsisaï¼šä»å…¨åˆ‡ç‰‡ç»„ç»‡ç—…ç†å›¾åƒä¸­è¿›è¡Œç”Ÿå­˜é¢„æµ‹ï¼ŒIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬7234â€“7242é¡µã€‚
- en: '[â—„](/html/2208.08786) [![ar5iv homepage](img/ed0f3cf5a019c4f8e48e41de62929bb0.png)](/)
    [Feeling'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '[â—„](/html/2208.08786) [![ar5ivé¦–é¡µ](img/ed0f3cf5a019c4f8e48e41de62929bb0.png)](/)
    [æ„Ÿè§‰]'
- en: lucky?](/feeling_lucky) [Conversion
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¹¸è¿ï¼Ÿ](/feeling_lucky) [è½¬æ¢]'
- en: report](/log/2208.08789) [Report
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŠ¥å‘Š](/log/2208.08789) [æŠ¥å‘Š]'
- en: an issue](https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2208.08789)
    [ViewÂ original
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '[æå‡ºé—®é¢˜](https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2208.08789)
    [æŸ¥çœ‹åŸæ–‡]'
- en: onÂ arXiv](https://arxiv.org/abs/2208.08789)[â–º](/html/2208.08791)[](javascript:toggleColorScheme()
    "Toggle ar5iv color scheme")[Copyright](https://arxiv.org/help/license) [Privacy
    Policy](https://arxiv.org/help/policies/privacy_policy)Generated on Wed Mar 13
    16:26:27 2024 by [LaTeXML![Mascot Sammy](img/70e087b9e50c3aa663763c3075b0d6c5.png)](http://dlmf.nist.gov/LaTeXML/)
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ arXiv ä¸Š](https://arxiv.org/abs/2208.08789)[â–º](/html/2208.08791)[](javascript:toggleColorScheme()
    "åˆ‡æ¢ ar5iv é¢œè‰²æ–¹æ¡ˆ") [ç‰ˆæƒ](https://arxiv.org/help/license) [éšç§æ”¿ç­–](https://arxiv.org/help/policies/privacy_policy)
    ç”± [LaTeXML ![å‰ç¥¥ç‰© Sammy](img/70e087b9e50c3aa663763c3075b0d6c5.png)](http://dlmf.nist.gov/LaTeXML/)
    ç”Ÿæˆäº 2024å¹´3æœˆ13æ—¥ æ˜ŸæœŸä¸‰ 16:26:27
