- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:34:25'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2402.15490] A Comprehensive Survey of Convolutions in Deep Learning: Applications,
    Challenges, and Future Trends'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15490](https://ar5iv.labs.arxiv.org/html/2402.15490)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Abolfazl Younesi1, Mohsen Ansari1, MohammadAmin Fazli1, Alireza Ejlali1, Muhammad
    Shafique2, and Jörg Henkel3 A. Younesi, M. Ansari, A. Ejlali, and M. A. Fazli
    are with the Department of Computer Engineering, Sharif University of Technology,
    Tehran, Iran. E-mail: {abolfazl.yunesi, ansari, ejlali, fazli}@sharif.edu.M. Shafique
    is with the Department of Computer Engineering, New York University (NYU) Abu
    Dhabi, United Arab Emirates. E-mail: muhammad.shafique@nyu.edu).J. Henkel is with
    the Karlsruhe Institute of Technology, Karlsruhe 76131, Germany. E-mail: henkel@kit.edu.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In today’s digital age, Convolutional Neural Networks (CNNs), a subset of Deep
    Learning (DL), are widely used for various computer vision tasks such as image
    classification, object detection, and image segmentation. There are numerous types
    of CNNs designed to meet specific needs and requirements, including 1D, 2D, and
    3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS,
    among others. Each type of CNN has its unique structure and characteristics, making
    it suitable for specific tasks. It’s crucial to gain a thorough understanding
    and perform a comparative analysis of these different CNN types to understand
    their strengths and weaknesses. Furthermore, studying the performance, limitations,
    and practical applications of each type of CNN can aid in the development of new
    and improved architectures in the future. We also dive into the platforms and
    frameworks that researchers utilize for their research or development from various
    perspectives. Additionally, we explore the main research fields of CNN like 6D
    vision, generative models, and meta-learning. This survey paper provides a comprehensive
    examination and comparison of various CNN architectures, highlighting their architectural
    differences and emphasizing their respective advantages, disadvantages, applications,
    challenges, and future trends.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning, DNN, CNN, Machine learning, Vision Transformers, GAN, Attention,
    Computer Vision, LLM, Large Language Model, Transformer, Dilated Convolution,
    Depthwise, NAS, NAT, Object Detection, 6D Vision, Vision Language Model
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IN today’s world, as technology continues to evolve, deep learning (DL) has
    become an integral part of our lives [[1](#bib.bib1)]. From voice assistants like
    Siri and Alexa to personalized recommendations on social media platforms, DL algorithms
    are constantly working behind the scenes to understand our preferences and make
    our lives easier [[2](#bib.bib2)]. With advancements in technology, DL is also
    being used in various fields such as healthcare, finance, and transportation,
    revolutionizing the way we approach these industries [[3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)]. As research and development in the field of DL continue to progress,
    even more innovative applications that will further enhance our daily lives can
    be expected. DL has ushered in a transformative era in artificial intelligence,
    empowering machines to assimilate vast datasets and make informed predictions
    [[6](#bib.bib6)][[8](#bib.bib8)]. The development of CNNs has received attention
    among deep learning’s significant advancements. Their impact has been felt in
    some areas, including generative AI, examining medical images, identifying objects
    [[9](#bib.bib9)], and finding anomalies [[10](#bib.bib10)]. CNNs, constituting
    a feedforward neural network, integrate convolution operations into their architecture
    [[7](#bib.bib7)][[11](#bib.bib11)]. These operations enable CNNs to adeptly capture
    intricate spatial and hierarchical patterns, rendering them exceptionally well-suited
    for image analysis tasks [[12](#bib.bib12)].
  prefs: []
  type: TYPE_NORMAL
- en: However, CNNs are often burdened by their computational complexity during training
    and deployment, particularly when operating on resource-constrained devices like
    mobile phones and wearables [[12](#bib.bib12)][[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Comparison of existing surveys; +* means conditional cosideration'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | Year | No. of included studies | Research Questions and Objective
    | Taxonomy | Datasets | Challanges | Comparison of Simulators | Evaluation |'
  prefs: []
  type: TYPE_TB
- en: '| [[117](#bib.bib117)] | 2023 | 210 | - | +* | - | - | + | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[118](#bib.bib118)] | 2021 | 343 | - | + | + | + | + | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[119](#bib.bib119)] | 2022 | 202 | - | + | - | + | - | + |'
  prefs: []
  type: TYPE_TB
- en: '| [[120](#bib.bib120)] | 2020 | 243 | - | +* | + | +* | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Our survey | 2024 | 465 | + | + | + | + | + | + |'
  prefs: []
  type: TYPE_TB
- en: 'Two principal avenues have emerged to reinforce the energy efficiency of CNNs:
    Employing Lightweight CNN Architectures: These architectures are deliberately
    engineered to achieve computational efficiency without compromising accuracy.
    For instance, the MobileNet family of CNNs has been meticulously tailored for
    mobile devices and has demonstrated state-of-the-art accuracy across various image
    classification Applications [[13](#bib.bib13)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Embracing Compression Techniques: These methods facilitate the reduction of
    CNN model size and consequently diminish the volume of data transfers between
    devices. A noteworthy example is the TensorFlow Lite framework, which offers a
    suite of compression techniques tailored for compressing CNN models for mobile
    devices [[14](#bib.bib14)].'
  prefs: []
  type: TYPE_NORMAL
- en: The fusion of lightweight CNN architectures and compression techniques yields
    a substantial boost in the energy efficiency of CNNs. Training and deploying CNNs
    on resource-constrained devices become feasible, thereby unlocking novel opportunities
    for employing CNNs in diverse applications like healthcare, agriculture, and environmental
    monitoring [[12](#bib.bib12)][[16](#bib.bib16)].
  prefs: []
  type: TYPE_NORMAL
- en: 'How different convolutional techniques cater to various AI applications. Convolutions
    play a fundamental role in contemporary DL architectures and are especially crucial
    when dealing with data organized in grid-like structures, such as images, audio
    signals, and sequential data [[23](#bib.bib23)]. The convolutional operation entails
    moving a small filter, also known as a kernel, across the input data, performing
    element-wise multiplications and aggregations. This process extracts essential
    features from the input data [[24](#bib.bib24)]. The main significance of convolutions
    lies in their capability to efficiently capture local patterns and spatial relationships
    within the data. This localization property makes convolutions highly suitable
    for tasks like image recognition, as objects can be identified based on their
    local structures. Additionally, convolutions introduce parameter sharing, which
    results in a significant reduction in the number of trainable parameters, leading
    to more efficient and scalable models [[25](#bib.bib25)]. Existing surveys: Previous
    survey papers on CNN architectures such as [[118](#bib.bib118)] and [[120](#bib.bib120)]
    provided good overviews of popular architectures from a certain period. However,
    they lacked a clear Research question and objective, evaluation, and challenges
    based on their design patterns. They mostly discussed architectures chronologically.'
  prefs: []
  type: TYPE_NORMAL
- en: Earlier surveys like [[119](#bib.bib119)] and [[120](#bib.bib120)] focused on
    explaining core CNN components and popular architectures up to a certain year.
    they also lacked research questions and objectives, analysis of datasets, and
    special types of taxonomy that were not considered complete overviews like large
    vision models, and large language models, and lacked of multipoint view for challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Previous work discussed the challenges in some specific concepts and applications
    of CNNs but did not extensively cover the intrinsic taxonomy present in newer
    CNN architectures. So this caused us to write a survey paper that aims to address
    the gaps in previous work by proposing a taxonomy to clearly classify CNN architectures
    based on their intrinsic design patterns rather than release year.
  prefs: []
  type: TYPE_NORMAL
- en: We focus on architectural innovations from 2012 onwards and discuss the recent
    developments in greater depth than earlier surveys. Discussing the latest trends
    and challenges provides an updated perspective for researchers.
  prefs: []
  type: TYPE_NORMAL
- en: This comprehensive survey of CNN’s history, taxonomy, applications, and challenges
    is needed to accelerate research progress in this domain further.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, the key questions we seek to address include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do state-of-the-art CNN models like ResNet, Inception, and MobileNet perform
    on the target hardware compared to constrained baselines? What are the impacts
    on accuracy, latency, and memory usage?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What techniques like pruning, quantization, distillation, and architecture design
    can help reduce the model size and computational complexity the most while retaining
    prediction quality?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do multi-stage optimization approaches that combine different techniques
    compare to single methods? Can we achieve better trade-offs between accuracy,
    latency, and memory?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a target application like embedded vision, what are the best practices for
    benchmarking, tuning, and deploying optimized CNN models considering their unique
    constraints and specifications?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which pruning and quantization techniques work best for our target application
    and hardware? How does this compare to baselines?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4939a02c4d84d1c713199e81aeee249c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Represents the section-by-section structure of the paper that provides
    a clear and organized framework for presenting the research findings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our overview makes several key contributions to the DL and CV communities:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Analyzing multiple types of existing CNNs: The survey provides a comprehensive
    and detailed analysis of various DL models and algorithms used in CV Applications.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Comparing the CNN models with various parameters and architectures: The overview
    offers insights into the performance and efficiency trade-offs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Identifying the strengths and weaknesses of different CNN models: Aiding researchers
    in selecting the most suitable model for their specific applications.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overview highlights the challenges and future directions for further improvement
    in the fields of DL and computer vision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exploring the trends in neural network architecture: This emphasizes the practical
    application and exciting nature of the advancements.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'comprehensive overview of the Main research fields: This covers the primary
    fields of research that are actively pursued by researchers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ffbfe5858ad65fb12047d8a2b424779f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A text-based visual reading map that helps individuals navigate and
    comprehend the paper'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of our review paper follows (See Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")): Section 2 of the paper will delve into the fundamentals
    of convolutions, elucidating their mathematical formulation, operational mechanics,
    and the role they play in the architecture of neural networks. Section 3 describes
    the basic parts of CNNs. In Section 4, The exploration will cover 2D convolutions,
    1D convolutions for sequential data, and 3D convolutions for volumetric data.
    Section 5 of the research paper will investigate advanced convolutional techniques
    that have emerged in recent years. This will encompass topics such as transposed
    convolutions for upsampling, depthwise separable convolutions for efficiency,
    spatial pyramid pooling, and attention mechanisms within convolutions. Section
    6 of the paper will highlight the real-world applications of different convolution
    types, showcasing their utility in image recognition, object detection, NLP, audio
    processing, and medical image analysis. In section 7 we discuss future trends
    and some open questions about CNNs. Section 8 is about the performance consideration
    of CNNs. In Section 9, we are going to talk about the platforms that are mostly
    used by researchers and developers, and in Section 10 about research fields that
    are popular or trending, then we have discussion in Section 11\. By the end of
    this research in Section 8, readers will gain a profound understanding of the
    importance of convolutions in DL and Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends") represents a reader map to visualize the flow of information
    within a text. It shows the connections between various sections, assisting readers
    in comprehending the overall structure of their preferred section following their
    needs.'
  prefs: []
  type: TYPE_NORMAL
- en: II Fundamentals of Convolutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convolutions form the foundation of crucial mathematical operations used to
    process data structured in grids, such as images, videos, and time series data
    [[26](#bib.bib26)]. Originally used in signal processing, convolutions were used
    for analyzing and manipulating signals [[27](#bib.bib27)]. In deep learning, convolutions
    serve as powerful feature extractors, enabling neural networks to efficiently
    learn from raw data [[26](#bib.bib26)][[27](#bib.bib27)]. The essence of a convolution
    involves the sliding of a small filter, commonly known as a kernel, over the input
    data. At each position of this sliding operation, the kernel performs element-wise
    multiplication with the corresponding input values [[28](#bib.bib28)]. Through
    this process, local patterns and relationships within the data are captured, enabling
    the model to acquire essential features like edges, textures, and shapes.
  prefs: []
  type: TYPE_NORMAL
- en: II-A Mathematical Formulation of Convolutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Mathematically, a 2D convolution between an input matrix (often representing
    an image) and a kernel can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Output}(i,j)=\sum_{(x,y)}\text{Input}(x,y)\cdot\text{Kernel}(i-x,j-y)$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Here, Output denotes the resulting feature map, and Input represents the input
    matrix. The kernel, usually a small square matrix, defines the convolutional filter’s
    weights. The convolution operation is performed by sliding the kernel over the
    input matrix, and at each position, the element-wise multiplication and summation
    are computed as described in the formula [[29](#bib.bib29)]. For 1D convolutions,
    the mathematical formulation is similar, with the kernel sliding along a one-dimensional
    sequence, such as a time series or text data [[30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B Convolutional Operations in DL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional operations form the core of CNNs, a highly prominent class of
    DL models widely utilized for various CV applications. Within a CNN, convolutions
    are typically integrated into specific layers referred to as convolutional layers
    [[31](#bib.bib31)]. These layers are composed of multiple filters, each responsible
    for detecting distinct patterns in the input data [[139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144),
    [145](#bib.bib145), [146](#bib.bib146)]. During the training phase, the model
    goes through the process of backpropagation and gradient descent to learn the
    optimal weights of the convolutional filters. This enables the model to automatically
    discern meaningful patterns within the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4f21392d14824235012fd1bb5ca53792.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: . A graphical representation of CNN architectures from 1998 to 2023'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, CNN architectures (See Fig. [3](#S2.F3 "Figure 3 ‣ II-B Convolutional
    Operations in DL ‣ II Fundamentals of Convolutions ‣ A Comprehensive Survey of
    Convolutions in Deep Learning: Applications, Challenges, and Future Trends") and
    Fig. [4](#S2.F4 "Figure 4 ‣ II-B Convolutional Operations in DL ‣ II Fundamentals
    of Convolutions ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications,
    Challenges, and Future Trends")) often incorporate pooling layers following the
    convolutional layers. As a result of pooling layers, feature maps generated by
    convolutions are downsampled, reducing computational complexity. Common pooling
    techniques include max-pooling and average pooling, which we will discuss about
    them in Section 3\. B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3c6633a4539462c2db1f1b48c6280160.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The flow of CNN architectures from 1998-2020 with their pros and
    cons represents that each CNN model is efficient for a specific application'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Wavelets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Wavelets are an important mathematical tool that has numerous applications in
    fields such as signal processing and computer graphics. At their core, wavelets
    rely on convolution to analyze functions or continuous-time signals [[104](#bib.bib104)].
    By convolving the target function with wavelet basis functions at different scales,
    wavelets are capable of representing data with varying degrees of resolution [[109](#bib.bib109)].
  prefs: []
  type: TYPE_NORMAL
- en: Wavelet analysis uses small waves, called wavelets, as basis functions instead
    of the sine and cosine functions used in Fourier analysis [[105](#bib.bib105)].
    Wavelets have the advantage of analyzing properties of data locally in time and
    frequency instead of globally. This makes them well-suited for tasks such as edge
    detection, noise removal, and texture identification. The wavelet basis can also
    be adapted to the input signal or data being analyzed [[105](#bib.bib105)][[106](#bib.bib106)].
  prefs: []
  type: TYPE_NORMAL
- en: CNNs naturally lend themselves to wavelet analysis due to their intrinsic use
    of convolution operations [[107](#bib.bib107)][[108](#bib.bib108)]. During training,
    the convolutional filters within CNNs can learn wavelet-like basis functions tailored
    to meaningfully represent the given input data distribution at multiple resolutions.
    By adopting the wavelet bases through gradient descent and backpropagation, CNNs
    gain an efficient multi-scale representation of patterns in the data [[108](#bib.bib108)][[109](#bib.bib109)].
  prefs: []
  type: TYPE_NORMAL
- en: A key characteristic of wavelets is their ability to decompose a signal into
    different frequency components, with high frequencies corresponding to detailed
    information and low frequencies corresponding to overall trends [[108](#bib.bib108)].
    A single-level wavelet decomposition breaks down the original signal into approximation
    and detail coefficients. The approximation contains lower frequency information,
    while the detail contains higher frequency or detailed information [[109](#bib.bib109)].
  prefs: []
  type: TYPE_NORMAL
- en: CNNs can utilize this multi-resolution decomposition property of wavelets by
    using convolutions to learn wavelet filters at each level [[108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110)]. The output of each level becomes the
    input to the next, with the filters extracting more detailed features at higher
    levels after the removal of coarse information. This convolutional learning of
    adapted wavelet bases enables CNNs to hierarchically capture patterns across different
    scales for improved data representation [[110](#bib.bib110)].
  prefs: []
  type: TYPE_NORMAL
- en: In various image processing and computer vision tasks, the use of convolutional
    wavelets within CNNs has shown promising results. For applications like denoising,
    super-resolution, and texture synthesis, CNNs equipped with learned wavelet filters
    have achieved state-of-the-art performance by effectively representing key multi-scale
    characteristics of visual data [[110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112),
    [113](#bib.bib113)]. Convolutional wavelets also benefit segmentation, detection,
    and classification when combined with traditional convolutional filters within
    CNNs [[109](#bib.bib109)]. In summary, wavelets provide a powerful tool for multi-scale
    analysis that CNNs can leverage through their inherent ability to learn localized
    basis functions via convolution operations.
  prefs: []
  type: TYPE_NORMAL
- en: III Basic Convolutional Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cca30847185f7f980a4826af630c37e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A graphical representation of Section 3'
  prefs: []
  type: TYPE_NORMAL
- en: The CNN architecture typically consists of an initial input layer, followed
    by several critical components, including convolutional layers, pooling layers,
    and fully connected layers. This organized structure allows for the systematic
    processing of raw data, such as images, through a series of layers, which in turn
    enables the extraction of relevant features and facilitates making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layers hold a central position in this architecture, as they
    employ learnable filters to process the input data. This operation is instrumental
    in detecting diverse patterns and features, thereby enhancing the network’s ability
    to understand the underlying data. Following the convolutional layers, the pooling
    layers come into play, downsampling the output from the previous layers. This
    downsampling process reduces the spatial dimensions while retaining crucial information.
    By focusing on the most significant details, these layers contribute to translational
    invariance, a valuable aspect in applications like image recognition where object
    positions may vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [II](#S3.T2 "TABLE II ‣ III-A Background of Deep Learning ‣ III Basic
    Convolutional Neural Networks ‣ A Comprehensive Survey of Convolutions in Deep
    Learning: Applications, Challenges, and Future Trends"), a comprehensive overview
    of the core components of basic CNNs is presented (also See Fig. [5](#S3.F5 "Figure
    5 ‣ III Basic Convolutional Neural Networks ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends")), encompassing
    convolutional layers, pooling layers, and activation functions. The table provides
    insights into their individual purposes, functionalities, dependencies on input
    size, parameters, feature maps, translational invariance, computational efficiency,
    output size, roles in the CNN architecture, and impact on model performance. Analyzing
    these aspects provides profound insights into the elements that contribute to
    the effectiveness and performance of CNNs, making it a valuable reference for
    researchers and practitioners in the field.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Background of Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning, a prominent form of machine learning, encompasses the use of
    neural networks composed of multiple layers to acquire hierarchical representations
    of data [[17](#bib.bib17)]. Taking inspiration from the intricate workings of
    the human brain, where neurons engage in processing and transmitting information
    to forge elaborate depictions of the world, DL models, also known as deep neural
    networks, showcase remarkable prowess in assimilating hierarchical features from
    raw data. This exceptional ability enables them to discern intricate patterns
    and achieve remarkable precision in predictions [[18](#bib.bib18)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: The Different Aspects of the Basic Convolutional Neural Networks'
  prefs: []
  type: TYPE_NORMAL
- en: '| Aspect | Convolutional Layers | Pooling Layers | Activation Functions | Batch
    Normalization |'
  prefs: []
  type: TYPE_TB
- en: '| Purpose | Feature extraction | Feature reduction | Introduce non-linearity
    | Training stabilization |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineFunctionality | Detect patterns and textures | Downsample feature
    maps | Add non-linearity | Normalizing activations |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineInput size dependency | Depends on input dimensions | Reduces spatial
    dimensions | Independent of input | Depends on input size |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineParameters | Learnable weights (kernels) | No parameters | No parameters
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Learnable scaling & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; shifting parameters &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| \hdashlineFeature maps | Produce feature maps | No feature maps | No feature
    maps | No feature maps |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineTranslational invariance | Not inherently invariant | Introduces
    some invariance | Independent of input | No Translational invariance |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineComputational efficiency | Computationally intensive |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Reduces computation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; complexity &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Low computation cost | Enhanced training stability |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineOutput size |'
  prefs: []
  type: TYPE_TB
- en: '&#124; May or may not &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; match the input size &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reduced size | Unchanged | Unchanged |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Role in CNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; architecture &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Central component |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Interposed between &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; convolutions &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Enable learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; complex relationships &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Improve convergence, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ease of tuning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| \hdashline'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Influence on &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; model performance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Significantly &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; impacts performance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Affects model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; efficiency &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Crucial for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Significantly &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; impacts performance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| \hdashlineInterpretability | Low | Low | Low | Normal |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineTraining complexity | High | Low | Low | Normal |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineMemory usage | Normal | Low | Low | Normal |'
  prefs: []
  type: TYPE_TB
- en: The roots of DL can be traced back to the nascent endeavors surrounding artificial
    neural networks in the 1940s. However, the true resurgence and substantial remarkable
    materialized in the 1980s and 1990s, paving the way for its remarkable revival
    in the 21 century [[19](#bib.bib19)]. Key catalysts driving this resurgence were
    the strides made in computational power, the vast availability of datasets, and
    the advent of efficient training algorithms, most notably backpropagation, which
    played a pivotal role [[20](#bib.bib20)]. By harnessing these advancements, DL
    models attained the ability to process and analyze vast repositories of data,
    thus acquiring an aptitude for deciphering intricate patterns and making precise
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The convergence of powerful hardware and sophisticated algorithms ushered in
    an era of remarkable accomplishments across diverse domains. Computer Vision (CV),
    natural language processing (NLP), and speech recognition (SR), among others,
    have witnessed remarkable strides through the transformative power of DL [[73](#bib.bib73)].
    This dynamic discipline’s capacity to overcome more difficult problems and promote
    innovation across various industries is becoming more and more clear as it develops
    and advances.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Introduction to Convolutional Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CNNs, an influential category of DL models, have emerged as a preeminent and
    extensively utilized algorithm within the realm of DL [[21](#bib.bib21)]. Distinctive
    to CNNs is their capacity to engage in convolution calculations and operate proficiently
    on intricate structures. This characteristic has propelled CNNs to achieve remarkable
    breakthroughs in image analysis and feature extraction, bestowing upon them the
    ability to discern and efficiently classify features in images. Moreover, CNNs
    are renowned as shift-invariant artificial neural networks, a nomenclature that
    accentuates their capability to classify input information based on its hierarchical
    arrangement [[22](#bib.bib22)].
  prefs: []
  type: TYPE_NORMAL
- en: The hierarchical architecture of CNNs empowers them to process and extract features
    from input data in a shift-invariant manner [[22](#bib.bib22)]. This implies that
    CNNs can adeptly recognize and classify objects within images, irrespective of
    their position or orientation. The realization of this shift-invariant attribute
    is accomplished through the application of convolutional layers, which employ
    filters in a sliding window fashion. These filters acquire the ability to detect
    specific patterns or features at various spatial scales, thereby enabling the
    network to encapsulate both local and global information. Consequently, CNNs exhibit
    profound proficiency in extracting meaningful features from images, facilitating
    a wide array of applications encompassing object detection, image recognition,
    and even image generation [[74](#bib.bib74)].
  prefs: []
  type: TYPE_NORMAL
- en: III-C Convolutional Layers and Their Functionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each convolutional layer comprises multiple filters, also referred to as kernels,
    which are small windows that slide over the input data [[32](#bib.bib32)]. During
    the training phase, the weights of these filters are learned, and they function
    as feature extractors, identifying specific patterns, edges, and textures present
    in the input [[33](#bib.bib33)]. When the filters move across the input, they
    create feature maps that emphasize important parts of the data as region of interest
    (ROI). These maps show where specific patterns in the input become active, helping
    the CNN recognize significant features crucial for later tasks like classification
    or detection [[34](#bib.bib34)].
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a CNN trained to identify cats in images, the filters may learn
    to recognize the patterns of fur, whiskers, and ears. As the filters convolve
    across an image of a cat, they generate feature maps that highlight these specific
    regions of interest. These feature maps indicate the activation of these cat-specific
    patterns and aid in accurately classifying the image as containing a cat.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Pooling Layers and Feature Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pooling layers are incorporated following convolutional layers to decrease the
    spatial dimensions of the feature maps, thereby reducing the computational complexity
    of the network [[35](#bib.bib35)]. The most frequently utilized pooling techniques
    in CNNs are max-pooling and average-pooling [[37](#bib.bib37)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Max-pooling entails selecting the maximum value from a small region of the
    feature map, while average-pooling computes the average value. Pooling offers
    two primary advantages: first, it effectively reduces the number of parameters
    in the network, resulting in improved computational efficiency. Second, it introduces
    a level of translational invariance, signifying that minor spatial translations
    in the input data do not substantially impact the pooled outputs. This property
    enhances the CNN’s ability to generalize better to variations in the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in image classification applications, after several convolutional
    and activation layers, a pooling layer can be used to downsample the feature map.
    This downsampling reduces the spatial resolution of the features, making it more
    computationally efficient to process and reducing the risk of overfitting. Additionally,
    because pooling computes either maximum or average values, it can capture the
    dominant features in an image regardless of their exact location, making the network
    more robust to slight variations in object position or orientation.
  prefs: []
  type: TYPE_NORMAL
- en: III-E Activation Functions in CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Activation functions play a vital role in CNNs as they are applied to the output
    of each neuron, introducing nonlinearity to the network and facilitating the learning
    of complex relationships between input data and their corresponding features.
    Within CNNs, several commonly used activation functions include Rectified Linear
    Units (ReLU) [[36](#bib.bib36)], which set negative values to zero while preserving
    positive values unchanged. Variants like Leaky ReLU [[36](#bib.bib36)] and Parametric
    ReLU [[39](#bib.bib39)] are also widely employed. The selection of the activation
    function is of great importance as it directly impacts the network’s capacity
    to learn and make accurate predictions. By introducing nonlinearity, activation
    functions allow CNN to model intricate patterns and decision boundaries, thereby
    enhancing its performance across a range of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in image classification applications, the ReLU activation function
    has been shown to effectively remove negative pixel values and emphasize positive
    pixel values, allowing CNN to identify important features and learn discriminative
    patterns. This enables the CNN to accurately classify different objects in images,
    such as correctly identifying whether an image contains a cat or a dog.
  prefs: []
  type: TYPE_NORMAL
- en: III-F Batch Normalization in CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Batch Normalization is a technique that helps stabilize and accelerate the training
    of CNNs [[78](#bib.bib78)]. It normalizes the activations of each layer by centering
    and scaling the values using the mean and variance of each mini-batch during training.
    This process reduces internal covariate shifts, making the optimization process
    smoother and enabling the use of higher learning rates.
  prefs: []
  type: TYPE_NORMAL
- en: By normalizing activations, Batch Normalization allows for more aggressive learning
    rates, which leads to faster convergence and improved model generalization. Additionally,
    it acts as a regularizer, reducing the need for other regularization techniques
    like dropout.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, Batch Normalization has become a standard component in CNN architectures,
    contributing to faster training, improved model performance, and increased ease
    of hyperparameter tuning. Its widespread adoption has significantly contributed
    to the success of modern CNNs in various CV and NLP applications. For example,
    in image classification applications, Batch Normalization helps reduce overfitting
    by normalizing the input for each mini-batch during training. This ensures that
    the network learns robust features and avoids relying on specific pixel values
    or noise in the input data. As a result, the model becomes more generalized and
    performs better on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: IV Types of Convolution in Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4145c6ab56a402b2b7df7d520333ac8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An overview of Section 4 structure'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, our goal is to comprehensively explore the different convolution
    methods (See Fig. [6](#S4.F6 "Figure 6 ‣ IV Types of Convolution in Deep Learning
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")) commonly used in deep learning models. Table LABEL:tab:Characteristics
    presents a condensed overview of these convolution types, providing important
    information such as input data type, dimensionality, receptive field, computational
    cost, primary use case, memory consumption, parallelization capability, consideration
    of temporal information, and computational efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d987ea6235c57d37d02e224fbba34c11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The Basic structure of CNN. a) represents CNN without Padding which
    causes the output image to become smaller. b) represents CNN without Padding which
    the output image is the same size as the input image'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to highlight that selecting the appropriate convolutional type
    relies on the particular task and dataset under consideration. For instance, when
    working with diverse data types, such as images or text, it may be necessary to
    employ distinct convolutional types to effectively capture relevant features.
    Moreover, considering the computational efficiency of each convolutional type
    becomes important for real-time applications or settings with limited resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: The Comparison Provides an Overview of The Characteristics and Functionalities
    of Different Convolution Types'
  prefs: []
  type: TYPE_NORMAL
- en: '| Convolution Type | 2D Convolutions | 1D Convolutions | 3D Convolutions |
    Dilated Convolutions | Grouped Convolutions |'
  prefs: []
  type: TYPE_TB
- en: '| Input Data Type | Images |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Sequential Data &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (e.g., Text) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Volumetric Data &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (e.g., Videos) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Images | Images |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineDimensionality | 2D | 1D | 3D | 1D, 2D | 2D |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineReceptive Field | Local | Local | Volumetric | Local | Local |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineComputational Cost | Medium | Low | High | Low | High |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineMain Use Case |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image recognition, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Text classification, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sentiment analysis &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Semantic segmentation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D medical imaging &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image Filtering, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image generation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Large-scale &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN architectures &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| \hdashlineMemory Consumption | Medium | Low | High | Low | Low |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineParallelization | Limited | Limited | Limited | Limited | High
    |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Use of Temporal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Not applicable |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Captures temporal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; patterns &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Captures spatial &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; temporal patterns &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Not applicable | Not applicable |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Computational &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Efficiency &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Medium | High | Medium | High | High |'
  prefs: []
  type: TYPE_TB
- en: IV-A 2D Convolutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '2D convolutions (See Fig. [7](#S4.F7 "Figure 7 ‣ IV Types of Convolution in
    Deep Learning ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications,
    Challenges, and Future Trends")) serve as the foundational elements in CNNs, particularly
    for applications related to CV. They are predominantly utilized for processing
    two-dimensional data, such as images, which can be represented as a grid of pixels.
    During this convolutional operation, a 2D kernel slides over the input image,
    enabling the capture of local patterns and the extraction of relevant features
    [[27](#bib.bib27)]. The primary application of 2D convolutions lies in image recognition,
    wherein the model learns to identify essential patterns, including edges, textures,
    and object components, thereby facilitating high-level recognition applications
    [[40](#bib.bib40)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2D convolutions have found use in a variety of fields, including signal processing,
    CV, and NLP in addition to image recognition. CNNs have completely changed CV
    processes like object detection, image segmentation, and facial recognition. CNNs
    can more accurately and efficiently analyze the spatial relationships and hierarchical
    structures present in images by using 2D convolutions. When learned filters slide
    across the input image, a CNN can learn to find and locate different objects in
    images, such as in object detection tasks. This helps the network accurately detect
    objects even in complicated scenes, as it can identify important patterns of various
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, CNNs can also be learned to categorize and compare faces by analyzing
    facial features using 2D convolutions in facial recognition. This makes it possible
    to create systems like access control and identity verification.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B 1D Convolutions for Sequential Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One-dimensional (1D) convolutions (See Fig. [8](#S4.F8 "Figure 8 ‣ IV-B 1D
    Convolutions for Sequential Data ‣ IV Types of Convolution in Deep Learning ‣
    A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")) are specially designed for working with sequential data like
    time series, audio signals, and natural language. Unlike their two-dimensional
    counterparts, 1D convolutions operate on a single line, allowing them to detect
    patterns that develop over time [[41](#bib.bib41)]. In the field of natural language
    processing, 1D convolutions are widely used in tasks such as classifying text
    and analyzing sentiments. They help the model identify complex patterns in sequences
    of words and understand how these words are related to each other [[42](#bib.bib42)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/202c37eda19fd5c0d864a106c6b14bcf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: An overview to simple one-dimensional (1D) Convolution Neural Network
    with Two Convolution layer'
  prefs: []
  type: TYPE_NORMAL
- en: 1D convolutions have also been successfully applied to audio signal processing
    applications such as SR and music analysis. By analyzing the temporal patterns
    of audio signals, these models can extract meaningful features that capture the
    underlying structure and characteristics of the sound. This has proven to be particularly
    useful in applications like speaker identification and emotion recognition, where
    the sequential nature of the audio data is sequential.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in speaker identification, 1D convolution can analyze the sequential
    patterns of an individual’s voice and learn to associate certain patterns with
    specific speakers. This allows the model to accurately identify and differentiate
    between different speakers in an audio recording. In emotion recognition, 1D convolutions
    can analyze the temporal changes in pitch, tone, and intensity of an audio signal
    to classify the emotional state of the speaker, such as happiness, sadness, or
    anger. This helps in detecting and understanding the underlying emotions conveyed
    through speech, which can be useful in various applications like customer sentiment
    analysis, virtual assistants, and mental health monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C 3D Convolutions for Volumetric Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Three-dimensional (3D) convolutions are specifically designed to handle volumetric
    data, such as 3D medical images or video data [[43](#bib.bib43)]. 3D convolutions
    possess the capability to simultaneously process spatial and temporal dimensions,
    thereby capturing intricate patterns and distinctive features across all three
    dimensions. In medical imaging, 3D convolutions are vital in jobs like finding
    where tumors are. The model uses 3D medical scans to figure out where the important
    spatial and surrounding details are, which helps accurately locate and describe
    tumors [[44](#bib.bib44)][[45](#bib.bib45)].
  prefs: []
  type: TYPE_NORMAL
- en: The use of 3D convolutions has gone beyond just tumors and is used in various
    medical imaging tasks like picking out different parts of the body, spotting issues,
    and classifying diseases. This method lets the model see the whole volume of a
    medical scan, rather than just individual parts, and consider how different slices
    are related in space. This comprehensive approach allows the model to effectively
    capture the overall structure of the target organ or an anomaly, resulting in
    improved diagnostic accuracy and better patient outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in tumor segmentation, 3D convolutions can be used to analyze
    a series of consecutive medical scans to identify the size and location of tumors
    over time, allowing doctors to track their growth and plan targeted treatments.
    This helps improve the accuracy and efficiency of tumor identification, leading
    to better patient outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to operating on raw medical images and videos, 3D convolutions can
    be applied to process point cloud data through voxelization [[101](#bib.bib101)].
    As point clouds represent 3D geometry as an unordered set of points without connectivity,
    a common approach is to first discretize the continuous 3D space into regular
    volumetric grids called voxels. Each voxel is assigned a feature vector, such
    as the number of points or aggregated point properties within its volume.
  prefs: []
  type: TYPE_NORMAL
- en: Voxelizing the point cloud allows existing 3D convolutional kernel operations
    to be directly applied. Early works divided the spatial domain into coarse voxels
    and maxpooled point features inside each voxel [[101](#bib.bib101)]. More advanced
    methods utilize sparse convolutions over fine-grained voxels or use dilated kernels
    with gaps to control the receptive field size. Multi-scale voxels have also been
    explored to capture both local and global point features [[126](#bib.bib126)][[127](#bib.bib127)].
  prefs: []
  type: TYPE_NORMAL
- en: After 3D convolution and pooling, the extracted voxel features can be decoded
    back to the original point cloud domain for subsequent 3D fully connected or Transformer
    layers [[130](#bib.bib130)]. Voxel representation serves as an efficient intermediary
    that not only maintains the spatial structure required by CNNs but also allows
    points of variable density[[128](#bib.bib128)][[129](#bib.bib129)][[130](#bib.bib130)].
    This two-stage voxel-based approach enables end-to-end training of 3D CNNs for
    point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Dilated Convolutions and Their Advantages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dilated convolutions (See Fig. [9](#S4.F9 "Figure 9 ‣ IV-D Dilated Convolutions
    and Their Advantages ‣ IV Types of Convolution in Deep Learning ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends")), also known as atrous convolutions, are a variant of traditional convolutions
    that introduce gaps (dilation) between kernel elements. This gap enables for an
    increased receptive field without increasing the number of parameters, making
    dilated convolutions more computationally efficient [[46](#bib.bib46)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8e3673677dfbd79cfdd215b598f3fda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Dilation Convolution with multiple dilation rate with 3 x 3 kernel
    size [[74](#bib.bib74)]'
  prefs: []
  type: TYPE_NORMAL
- en: Dilated convolutions find application in applications like semantic segmentation,
    where they enable the model to capture broader contextual information without
    compromising computational efficiency [[47](#bib.bib47)].
  prefs: []
  type: TYPE_NORMAL
- en: In semantic segmentation applications, dilated convolutions are particularly
    useful because they enable the model to capture broader contextual information.
    By introducing gaps between kernel elements, dilated convolutions increase the
    receptive field without adding more parameters. This means that the model can
    understand the surrounding context of each pixel or object in the image without
    sacrificing computational efficiency. This value is important in applications
    like semantic segmentation, where accurately identifying and classifying objects
    within an image is essential.
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Grouped Convolutions for Efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Grouped convolutions (See Fig. [10](#S4.F10 "Figure 10 ‣ IV-E Grouped Convolutions
    for Efficiency ‣ IV Types of Convolution in Deep Learning ‣ A Comprehensive Survey
    of Convolutions in Deep Learning: Applications, Challenges, and Future Trends"))
    involve dividing the input and output channels of a convolutional layer into groups.
    Within each group, separate convolutions are performed, which are then concatenated
    to produce the final output. This technique significantly reduces computational
    cost and memory consumption while promoting model parallelism [[48](#bib.bib48)].
    Grouped convolutions are commonly used in large-scale CNN architectures to reduce
    training time and enhance the scalability of DL models [[49](#bib.bib49)].'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to reducing computational cost and memory consumption, grouped convolutions
    also offer other advantages. One of the main benefits is improved model parallelism,
    which provides for better utilization of parallel computing resources. This is
    especially important in large-scale CNN architectures where training time can
    be a bottleneck. By dividing the input and output channels into groups, the convolutions
    can be performed in parallel, speeding up the entire training process. Furthermore,
    the scalability of DL models is enhanced with grouped convolutions, making it
    easier to deal with larger datasets and more complex applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb515e8024203151c7f5d27635f276d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Grouped convolution involves dividing the channels of a convolutional
    layer into 3 groups'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/106508688b05f0efeac7bb3b6192c26d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The detailed overview of advanced convolutions techniques'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in image classification applications, a large-scale CNN architecture
    such as ResNet can benefit from model parallelism using grouped convolutions.
    By dividing the input and output channels into groups, different subsets of the
    model can be trained in parallel on multiple GPUs or distributed systems. This
    not only reduces the training time but also allows for better resource utilization,
    eventually improving the scalability of the DL model to handle larger datasets
    and more complex image recognition applications.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, DL offers a diverse range of convolutional techniques to accommodate
    different data types and applications. From 2D convolutions for image recognition
    to 1D convolutions for sequential data and 3D convolutions for volumetric data,
    each convolution type has its unique advantages. Additionally, dilated convolutions
    and grouped convolutions serve as efficient alternatives, addressing specific
    challenges in DL models. Understanding the characteristics and applications of
    these convolution types empowers researchers and practitioners to design efficient
    and effective models for a wide array of applications.
  prefs: []
  type: TYPE_NORMAL
- en: IV-F Evolution of CNN Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the early origins of CNNs, there has been a rapid evolution in CNN architectures
    (See Fig. [11](#S4.F11 "Figure 11 ‣ IV-E Grouped Convolutions for Efficiency ‣
    IV Types of Convolution in Deep Learning ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends")) [[49](#bib.bib49)]
    over the past decade to enhance performance and efficiency [51]. Some key developments
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception modules (2014) - The Inception architecture introduced convolutional
    blocks with multiple filter sizes to capture features at various scales [[52](#bib.bib52)].
    This improves both accuracy and computational efficiency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNets (2015) - Residual networks allow the training of much deeper CNNs through
    shortcut connections that bypass multiple layers [[53](#bib.bib53)]. They reduce
    degradation in very deep models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseNets (2016) - These connect each layer to all subsequent layers for maximum
    information flow and feature reuse. This reduces the number of parameters [[54](#bib.bib54)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MobileNets (2017) - Designed specifically for mobile applications, they use
    depthwise separable convolutions to minimize model size and latency [[55](#bib.bib55)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EfficientNets (2019) - By systematically scaling network dimensions, these achieve
    much better efficiency-accuracy trade-offs [[55](#bib.bib55)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The evolution of CNN architectures (See Fig. [11](#S4.F11 "Figure 11 ‣ IV-E
    Grouped Convolutions for Efficiency ‣ IV Types of Convolution in Deep Learning
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")) has been crucial to their widespread adoption across vision
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: V Advanced Convolutional Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides a detailed overview of advanced convolutional techniques
    (See Fig. [12](#S5.F12 "Figure 12 ‣ V Advanced Convolutional Techniques ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends")). A clear and informative summary of these techniques is available in
    Table [IV](#S5.T4 "TABLE IV ‣ V-A Transposed Convolutions and Upsampling ‣ V Advanced
    Convolutional Techniques ‣ A Comprehensive Survey of Convolutions in Deep Learning:
    Applications, Challenges, and Future Trends"). By reviewing this table, readers
    can gain a better understanding of the state-of-the-art convolutional techniques
    and their potential uses.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/47c61927d83cfadaec28e28f2371ca42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The trend of CNNs over time based on the released year and amount
    of parameters and their types'
  prefs: []
  type: TYPE_NORMAL
- en: V-A Transposed Convolutions and Upsampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transposed convolutions—also referred to as deconvolutions or fractionally stridden
    convolutions—are sophisticated methods for upsampling feature maps [[57](#bib.bib57)].
    Transposed convolutions, as opposed to conventional convolutions, increase the
    feature map size, enabling the model to reconstruct higher-resolution representations
    from lower-resolution inputs [[58](#bib.bib58)]. Traditional convolutions reduce
    spatial dimensions. In processes like image segmentation [[59](#bib.bib59)], image
    creation [[60](#bib.bib60)], and image-to-image translation [[61](#bib.bib61)],
    they are essential. Transposed convolutions employ padding and stride values to
    regulate the upsampling process and learnable parameters to choose the output
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Transposed convolution can create artifacts or checkerboard patterns in generated
    feature maps, due to overlapping receptive fields. To prevent this, stride, padding,
    and dilation are used to control the output resolution and reduce these artifacts.
    In the field of image generation, transposed convolutions are used to upscale
    low-resolution images into high-resolution ones. To ensure the generated images
    are free of artifacts or checkerboard patterns, stride, padding, and dilation
    are adjusted to control the output resolution and enhance the quality of the generated
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: The Comparison Provides an Overview of The Characteristics and Functionalities
    of Different Convolution Types - Part 1'
  prefs: []
  type: TYPE_NORMAL
- en: '| Convolution Technique | Transposed Convolutions | DSC | SPP | Attention Mechanism
    | Shift-Invariant |'
  prefs: []
  type: TYPE_TB
- en: '| Purpose | Upsampling | Parameter Reduction |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Handling Varying &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input Sizes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Focus on Relevant &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Features &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Invariance |'
  prefs: []
  type: TYPE_TB
- en: '| Parameters | Learnable | Learnable | No parameters | Learnable | Learnable
    |'
  prefs: []
  type: TYPE_TB
- en: '| Computational Cost | High | Low | Low | Normal | High |'
  prefs: []
  type: TYPE_TB
- en: '| Parameter Efficiency | Low | High | High | Low | Normal |'
  prefs: []
  type: TYPE_TB
- en: '| Upsampling | Yes | No | No | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Spatial Handling | Spatially Invariant | Spatially Invariant | Variable regions
    | Spatially Invariant | Spatially Invariant |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Long-range &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dependencies &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| No | No | No | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Translation Invariance | Yes | Yes | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Rotation Invariance | No | No | No | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Interpretability | Low | Low | Low | Low | Low |'
  prefs: []
  type: TYPE_TB
- en: '| Model Size | Large | Small | Small | Small | Large |'
  prefs: []
  type: TYPE_TB
- en: '| Versatility | Normal | High | High | High | Normal |'
  prefs: []
  type: TYPE_TB
- en: '| Practical Applications |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image Segmentation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image Super-Resolution, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image Generation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mobile Vision Applications, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Real-time Object Detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image Classification, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object Detection, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Semantic Segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image Captioning, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Visual Question Answering &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image Recognition, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object Detection, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image Filtering &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: The Comparison Provides an Overview of The Characteristics and Functionalities
    of Different Convolution Types - Part 2'
  prefs: []
  type: TYPE_NORMAL
- en: '| Convolution Technique | Steerable Convolution | Capsule Networks | NAS |
    GAN | VIT |'
  prefs: []
  type: TYPE_TB
- en: '| Purpose | Efficiency and Invariance | Invariance | Efficiency | Synthesis
    | Long-range dependencies |'
  prefs: []
  type: TYPE_TB
- en: '| Parameters | Learnable | Learnable capsules | Architecture search | Learnable
    | Learnable |'
  prefs: []
  type: TYPE_TB
- en: '| Computational Cost | Low | High | High | High | Higher |'
  prefs: []
  type: TYPE_TB
- en: '| Parameter Efficiency | High | Normal | High | Low | Normal |'
  prefs: []
  type: TYPE_TB
- en: '| Upsampling | No | No | No | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Spatial Handling | Spatially Invariant | Spatially Invariant | Spatially
    variant | Spatially Invariant | Spatially Invariant |'
  prefs: []
  type: TYPE_TB
- en: '| Long-range Dependencies | No | No | No | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Translation Invariance | Yes | Yes | Yes | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Rotation Invariance | Yes | Yes | No | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Interpretability | Low | Low | Low | Low | High |'
  prefs: []
  type: TYPE_TB
- en: '| Model Size | Normal | Normal | Large | Large | Large |'
  prefs: []
  type: TYPE_TB
- en: '| Versatility | Low | Low | Low | Low | High |'
  prefs: []
  type: TYPE_TB
- en: '| Practical Applications |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image Filtering, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Edge Detection, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pattern Recognition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object Recognition, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image Segmentation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Medical Imaging &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Customized CNN Architectures, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Resource-Constrained Devices &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image Synthesis, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Style Transfer, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Data Augmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image recognition, NLP, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; diverse tasks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: V-B Depthwise Separable Convolutions (DSC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Depthwise separable convolutions (See the purple box in Fig. [13](#S5.F13 "Figure
    13 ‣ V-B Depthwise Separable Convolutions (DSC) ‣ V Advanced Convolutional Techniques
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")) are an efficient alternative to traditional convolutions,
    particularly in resource-constrained environments [[62](#bib.bib62)][[63](#bib.bib63)].
    They split the convolution process into two steps (See Fig. [13](#S5.F13 "Figure
    13 ‣ V-B Depthwise Separable Convolutions (DSC) ‣ V Advanced Convolutional Techniques
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")) depthwise convolutions [[64](#bib.bib64)] and pointwise convolutions
    [[65](#bib.bib65)] [[276](#bib.bib276), [277](#bib.bib277), [278](#bib.bib278),
    [279](#bib.bib279)]. Depthwise convolutions apply a separate kernel to each input
    channel, capturing spatial patterns independently for each channel. Pointwise
    convolutions then use 1x1 convolutions to combine the output channels from the
    depthwise step, effectively aggregating the information [[66](#bib.bib66)]. Depthwise
    separable convolutions significantly reduce the number of parameters and computation
    while maintaining model performance, making them popular in mobile and embedded
    applications [[67](#bib.bib67)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cd6d866e2a320d92f8e63a6f46e16a8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The Box with Purple color represents the Depthwise Convolution and
    the box with red color represents Pointwise Convolution (in pointwise a 1 x 1
    convolution is used)'
  prefs: []
  type: TYPE_NORMAL
- en: By decoupling spatial filtering from cross-channel filtering, depthwise convolution
    achieves higher computational efficiency and is well-suited for resource-constrained
    environments. MobileNet and Xception are popular CNN architectures that use depthwise
    convolution to reduce model size and improve inference speed without compromising
    performance significantly.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Spatial Pyramid Pooling (SPP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spatial pyramid pooling (SPP) is a technique used to handle inputs of varying
    sizes and aspect ratios in CNNs [[68](#bib.bib68)][[280](#bib.bib280), [281](#bib.bib281),
    [282](#bib.bib282), [283](#bib.bib283), [284](#bib.bib284), [285](#bib.bib285)].
    It divides the input feature maps into different regions of interest and applies
    max-pooling or average-pooling to each region independently. The resulting pooled
    features are then concatenated to form a fixed-length representation, which is
    fed into fully connected layers for further processing. SPP enables the CNN to
    accept input images of different sizes and produces consistent feature maps, making
    it useful in object detection and image segmentation applications [[69](#bib.bib69)].
  prefs: []
  type: TYPE_NORMAL
- en: V-D Attention Mechanisms in Convolutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attention mechanisms in convolutions allow the model to focus on relevant parts
    of the input, emphasizing specific regions during feature extraction [[70](#bib.bib70)].
    These mechanisms assign weights to different spatial locations based on their
    importance. Self-attention mechanisms [[70](#bib.bib70)], like those used in transformers,
    have been adapted for use in convolutions. They enable the network to capture
    long-range dependencies and context, improving the model’s ability to recognize
    complex patterns and relationships.
  prefs: []
  type: TYPE_NORMAL
- en: V-E Shift-Invariant and Steerable Convolutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shift-invariant convolutions are designed to be insensitive to small translations
    in the input data [[71](#bib.bib71)] [[286](#bib.bib286), [287](#bib.bib287),
    [288](#bib.bib288)]. They ensure that the learned features remain consistent regardless
    of the object’s position within the input image. This property is crucial for
    object detection applications, where the object’s location might vary within the
    image [[27](#bib.bib27)]. Steerable convolutions are filters that can be rotated
    to different angles, allowing the model to learn orientation-sensitive features
    in an orientation-invariant manner [[289](#bib.bib289), [290](#bib.bib290), [291](#bib.bib291)].
    These convolutions are often used in applications like text recognition, where
    the orientation of text can vary.
  prefs: []
  type: TYPE_NORMAL
- en: V-F Recent Advancements and Innovations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-F1 Capsule Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Capsule Networks, introduced by Geoffrey Hinton and his team, is a revolutionary
    advancement in CNNs [[75](#bib.bib75)]. They aim to address the limitations of
    traditional CNNs, particularly in handling spatial hierarchies and viewpoint variations
    [[292](#bib.bib292), [293](#bib.bib293), [294](#bib.bib294), [295](#bib.bib295),
    [296](#bib.bib296), [297](#bib.bib297), [298](#bib.bib298)]. Capsule Networks
    use capsules as fundamental units, which are groups of neurons that represent
    various properties of an entity, such as its pose, deformation, and parts.
  prefs: []
  type: TYPE_NORMAL
- en: Capsule Networks offer dynamic routing mechanisms to route information between
    capsules, allowing them to model complex hierarchical relationships more effectively.
    This enables the network to recognize objects with various poses and appearances,
    making Capsule Networks more robust to transformations and occlusions.
  prefs: []
  type: TYPE_NORMAL
- en: V-F2 Neural Architecture Search for Convolutions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neural Architecture Search (NAS) is an automated approach to designing CNN architectures
    [[76](#bib.bib76)][[81](#bib.bib81)]. Instead of relying on human-designed architectures,
    NAS employs search algorithms and neural networks to discover architectures that
    perform well on specific applications [[76](#bib.bib76)]. This technique has led
    to the development of state-of-the-art CNNs that outperform hand-crafted models
    [[299](#bib.bib299), [300](#bib.bib300), [301](#bib.bib301), [302](#bib.bib302),
    [303](#bib.bib303), [304](#bib.bib304), [305](#bib.bib305), [306](#bib.bib306),
    [307](#bib.bib307), [308](#bib.bib308), [309](#bib.bib309)].
  prefs: []
  type: TYPE_NORMAL
- en: NAS for convolutions involves exploring various convolutional designs, including
    different kernel sizes, depths, and connectivity patterns [[82](#bib.bib82)].
    It evaluates each architecture on a validation set, and through a process of evolution
    or optimization, identifies the best-performing architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In the scenario of self-autonomous vehicle navigation, NAS for convolutions
    could be used to design an optimal convolutional neural network architecture specifically
    tailored for processing and analyzing various types of visual data collected by
    the vehicle’s sensors. By exploring different convolutional designs, such as varying
    kernel sizes, depths, and connectivity patterns, NAS could identify the most effective
    architecture for accurately detecting objects and recognizing road signs in real-time.
    This would ultimately improve the vehicle’s ability to navigate autonomously and
    make informed decisions based on its visual perception.
  prefs: []
  type: TYPE_NORMAL
- en: V-F3 Generative Adversarial Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Generative Adversarial Networks (GANs) are a class of DL models used for generative
    applications, such as image synthesis, style transfer, and data augmentation [[310](#bib.bib310),
    [311](#bib.bib311), [312](#bib.bib312), [313](#bib.bib313), [314](#bib.bib314),
    [315](#bib.bib315), [316](#bib.bib316)]. GANs utilize CNNs as key components to
    model the generator and discriminator (See Fig. [14](#S5.F14 "Figure 14 ‣ V-F3
    Generative Adversarial Networks ‣ V-F Recent Advancements and Innovations ‣ V
    Advanced Convolutional Techniques ‣ A Comprehensive Survey of Convolutions in
    Deep Learning: Applications, Challenges, and Future Trends")) [[77](#bib.bib77)][[83](#bib.bib83)][[84](#bib.bib84)].
    The generator is a CNN that generates new samples, such as realistic images, while
    the discriminator is another CNN that aims to distinguish between real and fake
    samples [[77](#bib.bib77)]. These networks are trained adversarially, where the
    generator’s goal is to produce samples that deceive the discriminator, and the
    discriminator’s goal is to become better at distinguishing real from fake [[71](#bib.bib71)][[84](#bib.bib84)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8c7343fd1954b03e4c6b4269b91f9418.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: A simple GAN architecture represented to detect real and fake data
    which generator has generated'
  prefs: []
  type: TYPE_NORMAL
- en: GANs with convolution have revolutionized the field of image generation and
    have produced impressive results in generating high-quality images and realistic
    textures [[266](#bib.bib266), [267](#bib.bib267), [268](#bib.bib268), [269](#bib.bib269),
    [270](#bib.bib270), [271](#bib.bib271), [272](#bib.bib272), [273](#bib.bib273),
    [274](#bib.bib274), [275](#bib.bib275)]. They have also been extended to other
    domains like NLP, audio generation, and video synthesis. This technology has also
    been applied to other areas such as medical imaging, where GANs have been used
    to generate high-resolution and accurate images for diagnostic purposes. Additionally,
    GANs have shown promising results in the field of data augmentation, where they
    can generate synthetic data to increase the size and diversity of training datasets,
    improving the performance of machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the field of image generation, GANs with convolutional networks
    have been used to create realistic images of non-existent landscapes. The generator
    network creates visually convincing images, while the discriminator network learns
    to identify any flaws or inconsistencies in these generated images, pushing the
    generator to improve its output. This adversarial training process ultimately
    leads to the creation of high-quality and believable images that are indistinguishable
    from real photographs.
  prefs: []
  type: TYPE_NORMAL
- en: V-G Vision Transformers and Self-Attention Mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Through the use of self-attention mechanisms [[85](#bib.bib85)], Vision Transformers
    [[244](#bib.bib244), [245](#bib.bib245), [246](#bib.bib246), [247](#bib.bib247),
    [248](#bib.bib248), [249](#bib.bib249), [250](#bib.bib250), [251](#bib.bib251),
    [252](#bib.bib252), [253](#bib.bib253), [254](#bib.bib254), [255](#bib.bib255),
    [256](#bib.bib256), [257](#bib.bib257), [258](#bib.bib258), [259](#bib.bib259),
    [260](#bib.bib260), [261](#bib.bib261), [262](#bib.bib262), [263](#bib.bib263),
    [264](#bib.bib264), [265](#bib.bib265)] represent an important evolutionary step
    away from traditional computer vision architectures [[86](#bib.bib86), [87](#bib.bib87)]
    . Rather than solely relying on convolutional filters to process visual inputs,
    as has predominantly been the case, they segment images into distinct finite parts
    known as patches [[87](#bib.bib87)]. Each patch focuses on and extracts features
    from a different localized region of the photographic scene. This division of
    images into discrete patches is a major conceptual divergence from how most previous
    approaches operate.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, advanced convolutional techniques have significantly expanded
    the capabilities of CNNs and revolutionized various fields like CV, image synthesis,
    and NLP. From transposed convolution for upsampling to capsule networks for handling
    spatial hierarchies, these innovations have enhanced the efficiency, robustness,
    and expressiveness of CNNs, making them powerful tools for a wide range of applications.
    Moreover, recent advancements, such as NAS and GANs, continue to drive progress
    in the field of DL and hold promise for further breakthroughs in the future.
  prefs: []
  type: TYPE_NORMAL
- en: VI Applications of Different Convolution Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We provide a thorough overview of the numerous applications of different convolutional
    types in this section (See Fig. [15](#S6.F15 "Figure 15 ‣ VI Applications of Different
    Convolution Types ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications,
    Challenges, and Future Trends")). Table [VI](#S6.T6 "TABLE VI ‣ VI-A Image Recognition
    and Classification ‣ VI Applications of Different Convolution Types ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends") provides a brief but comprehensive overview of these applications. Convolutions
    of various types are used in a variety of contexts, demonstrating the flexibility
    and strength of CNNs. Convolutional techniques enable machines to understand and
    interact with complex data, facilitating advancements in a variety of fields and
    enhancing our daily lives. Examples include image recognition, object detection,
    NLP, and medical image analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56472502fac659f26439f7933dc48cc5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: The applications of CNN techniques which we have discussed in Section
    VI'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Image Recognition and Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many uses for CNNs, including image recognition and classification.
    Traditional 2D convolutions are especially useful in these applications. They
    make it possible for deep learning models to accurately classify images into various
    groups and learn crucial features from images. The network’s convolutional layers
    recognize edges, textures, and shapes. The pooling layers reduce the size of the
    image while preserving the data needed for classification. Image recognition and
    classification are used for various tasks, including optical character recognition
    (OCR) [[203](#bib.bib203), [204](#bib.bib204), [205](#bib.bib205), [206](#bib.bib206),
    [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209), [210](#bib.bib210),
    [211](#bib.bib211)], classifying different animal species, and recognizing handwritten
    numbers [[88](#bib.bib88)]. In competitions like ImageNet, CNNs have displayed
    impressive results, showcasing their abilities for handling wide image classification
    [[89](#bib.bib89)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: The Compact Table Highlights the Main Applications of Each Convolution
    Type'
  prefs: []
  type: TYPE_NORMAL
- en: '| Convolution Type | Traditional 2D Convolutions | 1D Convolutions | 3D Convolutions
    | Dilated Convolutions | Grouped Convolutions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Image Recognition | Image categorization | Time series analysis | Action
    recognition | Image segmentation | Real-time recognition |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineObject Detection | Object detection | Event detection | 3D object
    detection | Semantic segmentation | Efficient detection |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineNLP | Sentiment analysis | Text classification | Textual entailment
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Hierarchical &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; document classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter reduction |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineASPR |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Voice activity &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Speech recognition |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Environmental sound &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Robust speech &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; recognition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Low-latency &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; speech recognition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| \hdashlineMedical Image Analysis | Tumor segmentation | ECG signal processing
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Brain Tumor &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Enhanced image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Faster medical analysis |'
  prefs: []
  type: TYPE_TB
- en: VI-B Object Detection and Localization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multiple objects within an image must be located and identified during object
    detection [[90](#bib.bib90)]. In this application, both conventional 2D convolutions
    and 3D convolutions are crucial [[178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180),
    [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183), [184](#bib.bib184),
    [185](#bib.bib185), [186](#bib.bib186), [187](#bib.bib187), [188](#bib.bib188),
    [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)]. While 3D convolutions
    are used for video object detection, 2D convolutions are used to process individual
    image frames. CNNs can detect objects at different scales and aspect ratios thanks
    to their region proposal mechanisms and anchor-based methods [[192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194), [195](#bib.bib195), [196](#bib.bib196),
    [197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200),
    [201](#bib.bib201), [202](#bib.bib202)].
  prefs: []
  type: TYPE_NORMAL
- en: Accurate localization of object bounding boxes is made possible by the use of
    pooling layers and convolutional sliding windows. Robotics, surveillance technology,
    and autonomous vehicles all use object detection to better understand and interact
    with their surroundings [[91](#bib.bib91)][[92](#bib.bib92)].
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Natural Language Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For sequential data, such as text processing and sentiment analysis, NLP uses
    1D convolutions. 1D convolutions are used in NLP applications to extract pertinent
    patterns and relationships from sentences, enabling models to understand semantic
    meaning and context [[212](#bib.bib212), [213](#bib.bib213), [214](#bib.bib214),
    [215](#bib.bib215), [216](#bib.bib216)]. Sentiment analysis for understanding
    customer opinions, named entity recognition to extract specific information from
    text, and text classification to classify news articles or product reviews are
    examples of NLP applications using 1D convolutions. Applications like machine
    translation and text summarization have benefited from the successful integration
    of CNNs and recurrent neural networks (RNNs).
  prefs: []
  type: TYPE_NORMAL
- en: VI-D Audio Processing and Speech Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Audio Processing and Speech Recognition (APSR) benefit from 1D convolutions,
    which analyze and process sequential audio data such as speech signals or audio
    waveforms [[217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219), [220](#bib.bib220),
    [221](#bib.bib221), [222](#bib.bib222), [223](#bib.bib223)]. By extracting temporal
    patterns and acoustic features, CNNs can learn to recognize spoken words and transcribe
    audio into text. SR systems, often built upon convolutional and recurrent neural
    networks, enable voice assistants like Siri and Google Assistant to understand
    and respond to user commands.
  prefs: []
  type: TYPE_NORMAL
- en: VI-E Medical Image Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Medical image analysis involves the examination and interpretation of medical
    images, such as MRI scans, CT scans, and X-rays [[92](#bib.bib92)][[224](#bib.bib224),
    [225](#bib.bib225), [226](#bib.bib226), [227](#bib.bib227), [228](#bib.bib228),
    [229](#bib.bib229), [230](#bib.bib230), [231](#bib.bib231), [232](#bib.bib232),
    [233](#bib.bib233), [234](#bib.bib234), [235](#bib.bib235), [236](#bib.bib236),
    [237](#bib.bib237), [238](#bib.bib238), [239](#bib.bib239), [240](#bib.bib240),
    [241](#bib.bib241)]. In this domain, 3D convolutions and dilated convolutions
    are frequently used. 3D convolutions process volumetric medical data, allowing
    CNNs to extract spatial and contextual information for applications like tumor
    segmentation, organ localization, and disease classification [[92](#bib.bib92)][[93](#bib.bib93)].
    Dilated convolutions enhance feature extraction and semantic segmentation in medical
    images, enabling precise identification of abnormal tissues and structures. The
    applications of convolution types in medical image analysis have led to significant
    advancements in healthcare, assisting doctors in diagnosis and treatment planning.
  prefs: []
  type: TYPE_NORMAL
- en: VII Future Trends in CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNNs continue to be a hot topic of research and have achieved remarkable success
    in various CV applications. Future trends and open research questions in the field
    of CNNs are emerging as technology develops and deep learning techniques become
    increasingly complex.
  prefs: []
  type: TYPE_NORMAL
- en: The investigation of more effective architectures that can achieve comparable
    performance with fewer parameters and computational resources is one future trend
    in CNN research. How to make CNNs more interpretable is another unanswered research
    question, as the reasoning behind CNN decisions is frequently difficult to comprehend
    due to the internal complexity of these systems. Another crucial area for future
    research is finding ways to strengthen CNNs and make them less vulnerable to hostile
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: One active area of research looks at designing efficient CNN architectures optimized
    for edge and mobile computing. As CV moves from data centers to cameras, smartphones,
    and IoT at the network’s edge, models need to operate within strict constraints
    on latency, memory, and power. Techniques including network pruning, compact operators,
    knowledge distillation, and adaptive quantization help derive lightweight CNN
    variants suitable for these low-resource scenarios [[121](#bib.bib121)]. This
    focus on efficiency ties into work on improving CNN interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: While today’s complex CNNs achieve top accuracy, their decision-making remains
    poorly understood. Work on saliency mapping, activation clustering, modular CNNs,
    and other explanatory methods aims to shine light into the ”black box” and address
    concerns around reliability, bias, and accountability - important considerations
    for safety-critical domains like healthcare. New types of CNN modules also aim
    to expand what these models can represent by incorporating flexible self-attention
    and capturing non-Euclidean structures.
  prefs: []
  type: TYPE_NORMAL
- en: A particularly compelling avenue involves tackling large-scale vision multimodal
    (LVM) challenges, which builds upon this work on expanding CNN capabilities. Vast
    datasets merging diverse visual media with language, audio, and other inputs present
    unprecedented complexity. However, they also offer unprecedented opportunities
    to develop general, comprehensive models of multisensory scene understanding.
  prefs: []
  type: TYPE_NORMAL
- en: VII-A Interpretability and Explainability of CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The interpretability and explainability of CNNs is a significant open research
    question. Understanding the decision-making process of these models gets harder
    as CNNs get deeper and more complex. Particularly in critical applications like
    healthcare and autonomous systems, researchers are investigating ways to interpret
    and explain CNN predictions. To increase trust and reliability in CNN-based systems,
    methods such as attention visualization, saliency maps, and attribution methods
    seek to reveal which areas of the input contribute most to the model’s conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Incorporating Domain Knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Incorporating domain knowledge into CNN architectures is another important research
    direction. While CNNs have shown exceptional generalization abilities, they may
    not fully exploit domain-specific characteristics. Research focuses on developing
    architectures that can efficiently utilize domain knowledge or constraints, such
    as physics-based priors in medical imaging or geometric constraints in robotics,
    to improve performance and reduce data requirements.
  prefs: []
  type: TYPE_NORMAL
- en: VII-C Robustness and Adversarial Defense
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enhancing the robustness of CNNs against adversarial attacks remains a significant
    challenge. Adversarial attacks involve adding carefully crafted perturbations
    to inputs, leading to incorrect predictions by the CNN model. Researchers are
    investigating techniques for adversarial defense, such as adversarial training,
    robust optimization, and input transformations, to make CNNs more resilient against
    these attacks.
  prefs: []
  type: TYPE_NORMAL
- en: VII-D Efficient Model Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using CNNs on devices with limited resources, such as smartphones and edge
    devices, efficiency in terms of computation, memory, and power consumption is
    important [[242](#bib.bib242), [243](#bib.bib243)]. Creating lightweight architectures,
    knowledge distillation methods, and effective model compression techniques will
    be future trends in CNN research to decrease the model size and increase inference
    speed while maintaining accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Comparison of Pruning Technique'
  prefs: []
  type: TYPE_NORMAL
- en: '| Technique | Sparsity Type | Pruning Granularity | Hardware Friendly | Accuracy
    Impact | Compression Ratio | Iterative Training | Requires Retraining |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude Pruning | Unstructured | Weight level | No | Medium | 2-10x | Yes
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| Filter Pruning | Channel-wise | Filter level | Yes | Low | 5-10x | No | Yes
    |'
  prefs: []
  type: TYPE_TB
- en: '| Block Pruning | Block-level | Block level | Yes | Low | 2-5x | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Network Slimming | Channel-wise | Channel level | Yes | Low | 2-5x | Yes
    | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Lottery Ticket | Unstructured | Weight level | No | Low | 2-10x | Yes | Yes
    |'
  prefs: []
  type: TYPE_TB
- en: '| Iterative Magnitude | Unstructured | Weight level | No | Medium | 2-5x |
    Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Pruning-at-Init | Channel-wise | Filter level | Yes | Low | 5-10x | No |
    No |'
  prefs: []
  type: TYPE_TB
- en: '| One-Shot Pruning | Channel-wise | Filter level | Yes | Low | 5-10x | No |
    No |'
  prefs: []
  type: TYPE_TB
- en: 'Model compression techniques play a crucial role in designing efficient deep
    learning models suitable for deployment on resource-constrained edge devices.
    Several methods (See Table [VII](#S7.T7 "TABLE VII ‣ VII-D Efficient Model Design
    ‣ VII Future Trends in CNN ‣ A Comprehensive Survey of Convolutions in Deep Learning:
    Applications, Challenges, and Future Trends")) have been proposed to reduce model
    size and computations without significantly impacting predictive performance.
    Network pruning and quantization are two widely used compression approaches[[102](#bib.bib102)][[103](#bib.bib103)].'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning techniques aim to sparsify neural networks by removing redundant connections
    with minimal impact on functionality [[121](#bib.bib121)]. Early methods relied
    on unstructured pruning where connections were simply set to zero based on their
    magnitude or importance ranking. However, such arbitrary pruning leads to non-standard
    sparse matrices thereby preventing hardware acceleration. More recent structured
    pruning techniques induce channel-wise, filter-wise, or block-wise sparsity to
    yield compact models amendable to efficient implementations [[121](#bib.bib121),
    [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: Comparison of Quantization Technique'
  prefs: []
  type: TYPE_NORMAL
- en: '| Technique | Quantization Level | Bit Width | Hardware Friendly | Accuracy
    Impact | Compression Ratio | Iterative Training | Requires Calibration |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Weight Quantization | Weight values | 8-bit | Yes | Low | Up to 8x | No |
    Yes |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Activation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Quantization &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Activations | 8-bit | Yes | Low | Up to 8x | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Tensor Quantization | Tensors | 4-8 bit | Yes | Low | Up to 32x | No | Yes
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tensor Decomposition | Tensors | 4-bit | Yes | Medium | Up to 32x | No |
    No |'
  prefs: []
  type: TYPE_TB
- en: '| Huffman Coding | Weights | Variable | No | Low | Up to 10x | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Log Quantization | Activations | 1 bit | Yes | Low | Up to 16x | No | No
    |'
  prefs: []
  type: TYPE_TB
- en: '| BNN Quantization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Weights/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Activations &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 bit | Yes | High | Up to 32x | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Floating Point &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Quantization &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Weights/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Activations &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 16-bit | Yes | Low | Up to 2x | No | No |'
  prefs: []
  type: TYPE_TB
- en: Filter pruning refers to removing entire convolutional filters, thereby achieving
    channel-wise sparsity [[116](#bib.bib116)][[123](#bib.bib123)]. It has been shown
    that up to 90% of filters can be removed from VGG16 without accuracy degradation.
    One method, termed “Pruning-at-Initialization” prunes filters with the lowest
    sum values at the start of training itself. Alternatively, “One-Shot” prunes filters
    once based on their first-order Taylor expansion. These filter-level pruning methods
    lead to uniform sparsity across layers and reduce computation by 5̃x.
  prefs: []
  type: TYPE_NORMAL
- en: Another structured approach is to prune blocks of connections rather than individual
    weights [[124](#bib.bib124)]. For example, in “Block Level Pruning”, a number
    of convolution blocks are removed from blocks 1, 2, and 3 of ResNet50, reducing
    computations without retraining. The block structure ensures layout sparsity,
    maintaining original convolution block shapes for hardware friendliness. Network
    slimming is a channel-pruning method that enforces L1-norm regularization during
    training itself to gradually remove channels with low importance scores.
  prefs: []
  type: TYPE_NORMAL
- en: In unstructured variants, magnitude-based pruning removes weights below a threshold
    while iterative magnitude pruning alternates between weight updates and pruning
    based on a dynamic threshold [[121](#bib.bib121)][[125](#bib.bib125)]. These maintain
    sparsity throughout the architecture but induce non-zero filler weights. Lottery
    ticket hypothesis experiments have demonstrated that dense, randomly-initialized,
    sub-networks can achieve the accuracy of their original networks if trained in
    isolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from pruning, quantization is another effective technique to compress
    models (See Table [VIII](#S7.T8 "TABLE VIII ‣ VII-D Efficient Model Design ‣ VII
    Future Trends in CNN ‣ A Comprehensive Survey of Convolutions in Deep Learning:
    Applications, Challenges, and Future Trends")). Weight and activation quantization
    methods map weights/activations to a small set of discrete values, reducing the
    number of bits required for representation [[114](#bib.bib114)][[115](#bib.bib115)].
    For example, 8-bit quantization reduces model size by 4x without accuracy loss
    for many architectures. Tensor decomposition-based quantization further compresses
    models by decomposing weight tensors into low-rank approximations.'
  prefs: []
  type: TYPE_NORMAL
- en: Some recent works have combined multiple compression approaches in a multi-stage
    pipeline. One example jointly employs weight quantization, pruning, and Huffman
    coding on ResNet50, achieving over 10x compression with a minor accuracy drop.
    Another uses a two-phase pipeline consisting of filtering-based pruning followed
    by quantization to design efficient MobileNet variants. Such composite methods
    achieve better accuracy-efficiency tradeoffs than individual techniques alone.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, network pruning and quantization offer promising avenues to design
    compact models for edge and mobile applications. While early methods relied on
    unstructured sparsing, recent techniques induce structure for hardware friendliness.
    Looking ahead, continued research on model compression holds the key to facilitating
    the adoption of deep learning across myriad resource-constrained environments.
  prefs: []
  type: TYPE_NORMAL
- en: VII-E Multi-Task Learning and Transfer Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CNNs are well suited for multi-task learning, in which a single model is trained
    to carry out several related applications concurrently [[162](#bib.bib162), [163](#bib.bib163),
    [164](#bib.bib164), [165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167),
    [168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170), [171](#bib.bib171),
    [172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176), [177](#bib.bib177)]. The need for large amounts of labeled
    data for each individual task is being reduced as researchers investigate ways
    to take advantage of shared representations across applications and enhance generalization
    by transferring knowledge learned from one task to another [[147](#bib.bib147),
    [148](#bib.bib148), [149](#bib.bib149), [150](#bib.bib150), [151](#bib.bib151),
    [152](#bib.bib152), [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155),
    [156](#bib.bib156), [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159),
    [160](#bib.bib160), [161](#bib.bib161)].
  prefs: []
  type: TYPE_NORMAL
- en: VII-F Integration with Uncertainty Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding model uncertainty is essential for safety-critical applications.
    Integrating uncertainty estimation into CNNs would allow models to quantify their
    confidence in predictions and prevent costly errors, which is an area of open
    research. To improve the uncertainty measures in CNNs, researchers are investigating
    Bayesian neural networks (BNNs), dropout-based uncertainty estimation, and Bayesian
    optimization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: VII-G Generalization to Small Data Regimes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A constant problem in the CNN research area is the generalization to small data
    regimes, where labeled training data are hard to come by. Essentially using data
    from related applications or domains, techniques like transfer learning, few-shot
    learning, and meta-learning work to increase CNNs’ capacity to learn from sparse
    data.
  prefs: []
  type: TYPE_NORMAL
- en: VII-H Evolution of Language Models and Multimodal LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent epochs, the domain of large language models (LLMs) for natural language
    processing has witnessed a precipitous progression. Prototypes such as BERT, GPT-3,
    and PaLM have demonstrated exceptional aptitude in language apprehension and generation,
    courtesy of self-supervised pretraining on voluminous text corpora [[85](#bib.bib85)].
    As LLMs expand in magnitude and range, incorporating additional modalities beyond
    text is a burgeoning field of study. Multimodal LLMs strive to amalgamate language,
    vision, and other sensory inputs within a singular model architecture. They hold
    the potential to attain a more holistic understanding of the world by concurrently
    learning representations across diverse data types [[96](#bib.bib96)]. A significant
    hurdle is the effective fusion of the strengths of CNNs for computer vision and
    transformer architectures for language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: One strategy involves employing a dual-stream architecture with distinct CNN
    and transformer encoders interacting via co-attentional transformer layers [[97](#bib.bib97)].
    The CNN extracts visual features from images, providing contextual information
    that can guide language generation and comprehension. The transformer architecture
    models the semantics and syntax of text. Their interaction enables the generation
    of captions based on image content or the retrieval of pertinent images for textual
    queries. Alternative methods directly incorporate CNNs within the transformer
    architecture as visual token encoders that operate with text token encoders [[98](#bib.bib98)].
    The CNN projections of image patches are appended to text token embeddings as
    inputs to the transformer layers. This unified architecture allows for end-to-end
    optimization of parameters for both vision and language tasks. Self-supervised
    pretraining continues to be vital for multimodal LLMs to learn effective joint
    representations before downstream task tuning. Contrastive learning objectives
    that predict associations between modalities have proven highly effective [[99](#bib.bib99)].
    Models pre-trained on large datasets of image-text pairs have demonstrated robust
    zero-shot transfer performance on multimodal tasks.
  prefs: []
  type: TYPE_NORMAL
- en: As multimodal LLMs increase in scale, the efficient combination of diverse convolution
    types and attention mechanisms will be crucial. Compact CNN architectures could
    help to reduce the cost of computing. Sparse attention and memory compression
    techniques can assist with scalability.
  prefs: []
  type: TYPE_NORMAL
- en: VIII Performance and Efficiency Consideration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considerations for performance and efficiency (See Figs. 17-20) in CNNs are
    critical in developing high-performing and resource-efficient models. Researchers
    can make informed decisions about optimizing their CNN architectures for various
    applications and deployment scenarios by analyzing computational complexity, trade-offs
    between accuracy and speed, memory requirements, and benchmarking on standard
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-A Computational Complexity of Different Convolutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The computational complexity of different convolutional techniques (See Table
    [IX](#S8.T9 "TABLE IX ‣ VIII-A Computational Complexity of Different Convolutions
    ‣ VIII Performance and Efficiency Consideration ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends")) is a critical
    aspect to consider when designing CNNs. It refers to the amount of computation
    required to perform a convolution operation on input data. The computational complexity
    is influenced by various factors, including the size of the input data, the size
    of the convolutional filters, and the number of channels in the feature maps.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional convolutional layers, such as the standard convolution and depthwise
    separable convolution, generally have higher computational complexity compared
    to other techniques. This is because they involve a large number of convolution
    operations, especially when dealing with high-resolution images or complex data.
    On the other hand, techniques like pointwise convolution and transposed convolution
    tend to have lower computational complexity, making them more suitable for certain
    resource-constrained applications.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the computational complexity of different convolution types is
    crucial for optimizing the performance of CNNs. By selecting convolution techniques
    that align with the available computational resources, researchers can build efficient
    models that achieve a good balance between accuracy and speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in Figs. [17](#S8.F17 "Figure 17 ‣ VIII-B Trade-offs between
    Accuracy and Speed ‣ VIII Performance and Efficiency Consideration ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends") to [19](#S8.F19 "Figure 19 ‣ VIII-B Trade-offs between Accuracy and Speed
    ‣ VIII Performance and Efficiency Consideration ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends") the Adam optimizer
    performed well, as evidenced by key observations ① through ⑥, in both accuracy
    and loss metrics. Overall, the use of CNN techniques such as VGG, ResNet, and
    LeNet resulted in improved accuracy and reduced loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, as depicted in Figure [20](#S8.F20 "Figure 20 ‣ VIII-C Memory and Storage
    Requirements ‣ VIII Performance and Efficiency Consideration ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends"), and based on key observation ①,②, and ③, it is evident that the Adam
    optimizer exhibits less CPU usage in comparison to five other optimizers - RMSprop,
    Adamax, Adagrad, SGD, and Nadam. This observation holds true when using LeNet-5,
    VGG16, and ResNet-50\. Additionally, the memory usage of the Adam optimizer is
    among the lowest (See key observation ④).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e87d17eb1851cedb0402c62ba54ecba4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The trade-off curve between accuracy and speed of a deep learning
    model [[75](#bib.bib75)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: Comparison on LeNet-5, VGG16, and ResNet-50 with 7 types of optimizers
    on Cifar-10 dataset, CU: CPU Utilization, MU: Memory Utilization'
  prefs: []
  type: TYPE_NORMAL
- en: '| Optimizer Type | CNN Model | Accuracy | Loss | CU | MU |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeNet-5 | 0.547 | 1.277 | 71 | 50.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | VGG16 | 0.87 | 0.776 | 57 | 55.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SGD | ResNet-50 | 0.789 | 1.1212 | 63 | 53.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeNet-5 | 0.629 | 1.153 | 46.2 | 44.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | VGG16 | 0.805 | 0.821 | 54.2 | 51.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Adam | ResNet-50 | 0.760 | 1.016 | 60.5 | 51.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeNet-5 | 0.624 | 1.22 | 58.3 | 57.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | VGG16 | 0.776 | 1.109 | 61.1 | 63.5 |'
  prefs: []
  type: TYPE_TB
- en: '| NAdam | ResNet-50 | 0.789 | 0.89 | 66.4 | 57.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeNet-5 | 0.605 | 1.288 | 50.3 | 42.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | VGG16 | 0.755 | 22.286 | 61.2 | 49.7 |'
  prefs: []
  type: TYPE_TB
- en: '| RSMProp | ResNet-50 | 0.78 | 1.151 | 61.7 | 49.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeNet-5 | 0.603 | 1.132 | 69.8 | 56.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | VGG16 | 0.8506 | 0.885 | 55.8 | 64.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Adamax | ResNet-50 | 0.8123 | 1.002 | 62.1 | 56.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeNet-5 | 0.412 | 1.65 | 67.6 | 44.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | VGG16 | 0.822 | 0.708 | 55.3 | 50.3 |'
  prefs: []
  type: TYPE_TB
- en: '| AdaGrad | ResNet-50 | 0.75 | 0.999 | 62.4 | 50.6 |'
  prefs: []
  type: TYPE_TB
- en: VIII-B Trade-offs between Accuracy and Speed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key challenging aspects of designing CNNs is balancing model accuracy
    and inference speed (see Fig. 16). The inference time increases as the complexity
    of convolutional layers increases to capture more complex features. Using simpler
    convolutional techniques, on the other hand, may result in lower accuracy. The
    depth and width of the network, the number of parameters, the choice of convolutional
    techniques, and the hardware on which the model is deployed all have an impact
    on the trade-offs between accuracy and speed. For real-time applications or resource-constrained
    environments, sacrificing some accuracy to achieve faster inference may be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec1997a7188290ec8bd8a5d623ad5c9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Comparison of various optimizers on LeNet-5 with Cifar-10 dataset.
    a) represents the accuracy of LeNet-5 architecture, b) represents loss of LeNet-5
    architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model pruning, quantization, and low-rank approximations are commonly used
    by researchers to reduce model size (See Section [VII](#S7 "VII Future Trends
    in CNN ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications,
    Challenges, and Future Trends") -¿ Subsection D) and improve inference speed without
    significantly compromising accuracy. Furthermore, attention-based convolutions
    and other techniques that prioritize important regions of the input can be used
    to focus computational efforts where they are most needed, improving the balance
    between accuracy and speed even further.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d6e7c606b589069871faebd320f4395.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Comparison of various optimizers on VGG16 with Cifar-10 dataset.
    a) represents the accuracy of VGG16 architecture, b) represents loss of VGG16
    architecture with various range of optimizers'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1234d55fe1057ef41b681851fbeb6a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Comparison of various optimizers on ResNet-50 with Cifar-10 dataset.
    a) represents the accuracy of ResNet-50 architecture, b) represents loss of ResNet
    -50 architecture'
  prefs: []
  type: TYPE_NORMAL
- en: VIII-C Memory and Storage Requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ee5916de1d599d32554281f0281f2bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: The CPU and Memory Utilization used by each model. a) The Average
    CPU Utilization of LeNet-5, VGG16, and ResNet-50 with six types of optimizer (Better
    value Recognition depends on use-case), b) The Average Memory Utilization of LeNet-5,
    VGG16, and ResNet-50 with six types of optimizer (Better value Recognition depends
    on Usecase)'
  prefs: []
  type: TYPE_NORMAL
- en: Memory and storage requirements are crucial considerations in deep learning,
    especially when deploying models on edge devices or in cloud environments with
    limited resources. Convolutional models, particularly those with a large number
    of layers and parameters, can demand substantial memory and storage resources
    during training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional convolutional layers often have higher memory requirements due to
    the need to store intermediate feature maps and gradients during backpropagation.
    Depthwise separable convolutions and pointwise convolutions can reduce memory
    usage by reducing the number of parameters and intermediate feature maps. Memory-efficient
    CNN design involves strategies like using smaller batch sizes, employing mixed-precision
    training, and optimizing memory usage during inference. Additionally, model compression
    techniques, such as knowledge distillation and model quantization, can significantly
    reduce the size of the model without significant loss in performance.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-D Benchmarking on Standard Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Benchmarking convolutional techniques on standard datasets is a crucial step
    in evaluating their performance and efficiency. Standard datasets, such as ImageNet
    [[95](#bib.bib95)] for image recognition or COCO [[94](#bib.bib94)] for object
    detection, provide a common ground for fair comparison of different models and
    techniques. By benchmarking convolutional techniques, researchers can objectively
    assess their effectiveness in various applications and compare their performance
    with state-of-the-art models. The benchmarks consider metrics like accuracy, inference
    speed, memory usage, and energy efficiency, allowing for a comprehensive evaluation
    of the models.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking helps the DL community identify the strengths and weaknesses of
    different convolutional techniques, paving the way for improvements and advancements.
    It also aids practitioners in selecting the most suitable convolutional techniques
    for their specific use cases and desired trade-offs between performance and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: IX Frameworks and Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section will provide an overview of some of the popular platforms (See
    Table [X](#S9.T10 "TABLE X ‣ IX Frameworks and Libraries ‣ A Comprehensive Survey
    of Convolutions in Deep Learning: Applications, Challenges, and Future Trends"))
    available for developing deep learning applications. We will compare the frameworks
    from aspects like their architecture, programming models, supported hardware,
    and key features. Choosing the right tool is crucial for deep learning success.
    That’s why exploring framework capabilities is key for researchers and engineers'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE X: Comparison of existing popular frameworks and libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '| Aspect | Caffe | TensorFlow | Keras | PyTorch | OpenCV | Deeplearning4j |
    MXNet | Chainer |'
  prefs: []
  type: TYPE_TB
- en: '| Year released | 2013 | 2015 | 2015 | 2016 | 1999 | 2014 | 2015 | 2015 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Programming &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; language &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| C++/Python | Python, C++ | Python | Python | C++, Python, Java | Java, Scala
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Python, C++, R, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Scala, Perl, Julia &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Python |'
  prefs: []
  type: TYPE_TB
- en: '| License | BSD 3-Clause | Apache 2.0 | MIT | BSD 3-Clause | BSD 3-Clause |
    Apache 2.0 | Apache 2.0 | MIT |'
  prefs: []
  type: TYPE_TB
- en: '| Model definition | Layered | Graph-based |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Sequential & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; functional &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dynamic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; computations graphs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| N/A |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Sequential, compute &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; graphs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Symbolic |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Imperative and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; declarative &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ease of use | Intermediate | Intermediate | High | High | Low | Intermediate
    | Intermediate | High |'
  prefs: []
  type: TYPE_TB
- en: '| Speed | Fast | Fast | Intermediate | Fast | Very fast | Fast | Fast | Fast
    |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Support for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; computer vision &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Very good | Excellent | Good | Excellent | Excellent (library) | Good | Goo
    | Good |'
  prefs: []
  type: TYPE_TB
- en: '| Focus |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Research &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prototyping &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Production & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; research &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User-friendly &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; research &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Research &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prototyping &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Traditional &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; algorithms &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Enterprise &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; production &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Distributed training &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; at scale &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Intuitive high-level &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; APIs for research &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Distributed training | No | Yes | No | No | No | Yes | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; deployment &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| No | Yes | Yes | Yes | No | Yes | Yes | Limited |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hardware &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; support &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| CPU, GPU | CPU, GPU, TPU | CPU, GPU | CPU, GPU, TPU | CPU, GPU | CPU, GPU
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CPU, GPU, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| CPU, GPU |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Documentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; quality &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Good | Excellent | Good | Excellent | Excellent | Good | Good | Good |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Community &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; support &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Limited | Very active | Very active | Very active | Very active | Active
    | Active | Active |'
  prefs: []
  type: TYPE_TB
- en: 'Table [X](#S9.T10 "TABLE X ‣ IX Frameworks and Libraries ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends") provides a comparison of several popular frameworks and libraries used
    in deep learning. It evaluates key aspects such as the year of release, programming
    languages supported, license type, model definition approaches, ease of use, speed,
    and focus or strength of each framework.'
  prefs: []
  type: TYPE_NORMAL
- en: IX-A Caffe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Caffe was one of the earliest and most influential deep learning frameworks
    developed specifically for CV tasks [[131](#bib.bib131)]. Released in 2013 by
    the Berkeley Vision and Learning Center (BVLC), Caffe made training convolutional
    neural networks much faster and more accessible. It has an easy-to-use C++/Python
    interface and was designed for speed and modularity. Caffe adopted a layered structure
    that greatly simplified model definition and training. This helped drive wider
    adoption and enabled researchers to rapidly iterate on vision models. While development
    has slowed in recent years, Caffe laid important groundwork and is still used
    for CV research.
  prefs: []
  type: TYPE_NORMAL
- en: IX-B TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TensorFlow is an end-to-end open-source machine learning platform developed
    by Google [[132](#bib.bib132)]. While not strictly a CV library, it has become
    one of the most popular and full-featured frameworks for building and training
    complex deep learning models. TensorFlow has excellent support for CV including
    pre-trained models, image loading and preprocessing utilities, object detection
    APIs, and more. Its flexibility has led to it being used for a very wide range
    of applications from image classification to semantic segmentation. TensorFlow
    also works seamlessly across CPUs and GPUs and can be easily deployed to production.
  prefs: []
  type: TYPE_NORMAL
- en: IX-C Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keras is a high-level deep learning API that runs on top of popular frameworks
    like TensorFlow and CNTK [[133](#bib.bib133)]. Keras was developed with a focus
    on user-friendliness, modularity and extensibility. It provides excellent abstractions
    and tools for developing and evaluating deep learning models quickly. For CV,
    Keras ships with the ImageDataGenertator for real-time data augmentation as well
    as pre-defined models like VGG16\. It also supports popular CV tasks like image
    segmentation, object detection, and feature extraction through convenient APIs.
    Keras’ simplicity has made it very approachable for developers.
  prefs: []
  type: TYPE_NORMAL
- en: IX-D PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch is an open-source deep learning platform developed by Facebook’s AI
    Research Lab (FAIR) [[134](#bib.bib134)]. In recent years it has emerged as a
    leading alternative to TensorFlow especially for CV and NLP applications. PyTorch
    has a strong focus on dynamic neural networks and shares similarities to MATLAB
    and Numpy. This makes for an intuitive, Pythonic interface that is well-suited
    to CV prototyping and experimentation. PyTorch also supports GPU/TPU training
    along with production deployment. It has a growing ecosystem of 3rd party libraries
    and community support. Like Keras, PyTorch integrates tightly with common CV tasks
    and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: IX-E OpenCV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenCV (Open Source Computer Vision Library) is a popular CV and machine learning
    software library [[135](#bib.bib135)]. While not specifically designed for deep
    learning, OpenCV contains many traditional CV algorithms and an extensive collection
    of image processing functions. These include capabilities like image filtering,
    morphological operations, feature detection and extraction, object segmentation,
    and face and gesture recognition among others. OpenCV integrates with deep learning
    frameworks and is frequently used for simpler CV tasks or as a pre-processing
    step before feeding data into neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: IX-F MXNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MXNet is a flexible, efficient, and scalable deep learning framework [[136](#bib.bib136)].
    Similar to TensorFlow, it supports a wide variety of programming languages and
    hardware environments. MXNet excels at distributed training and supports training
    models containing billions of parameters across hundreds of GPUs. It also includes
    algorithms for CV like image recognition, object detection, and semantic segmentation.
    Overall, MXNet strikes a good balance between flexibility, performance, and ease
    of use making it suitable for large-scale CV problems.
  prefs: []
  type: TYPE_NORMAL
- en: IX-G Chainer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chainer is an open-source deep learning framework created by preferred networks
    in Japan [[137](#bib.bib137)]. It provides straightforward neural network abstraction
    similar to Keras with imperative and declarative model definitions. Chainer focuses
    on intuitive high-level APIs combined with low-level performance. It includes
    CV functionality like image loading, augmentation, pre-trained models, and model
    export. Chainer supports GPU and multi-GPU training and deployment. Overall it
    provides a performant and productive environment for CV development.
  prefs: []
  type: TYPE_NORMAL
- en: IX-H Deeplearning4j
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deeplearning4j (Dl4j) was launched in 2014 as an open-source deep learning library
    for Java and Scala on the JVM [[138](#bib.bib138)]. It enables large-scale distributed
    training on GPUs and CPUs. For CV tasks, Deeplearning4j offers tools like image
    loading, pre-trained models, model import from Keras and ONNX, and the samediff
    for dynamic model construction. Deeplearning4j focuses on production-ready deployment
    with capabilities like model serving, online prediction, and on-device inference
    via Android or iOS apps.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, these libraries and frameworks represent the forefront of open-source
    tools transforming CV through deep learning. Each offers different strengths and
    tradeoffs between flexibility, performance, ease of use, and supported features.
    As CV tasks continue advancing, we can expect these projects to further incorporate
    state-of-the-art research while also lowering the barrier to development through
    improved tools and abstractions. CV is sure to remain a major application domain
    for deep learning innovation in both research and industry.
  prefs: []
  type: TYPE_NORMAL
- en: X Main Research Fields
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: X-A Image Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Image classification was one of the earliest successes of CNNs. The seminal
    AlexNet achieved record-breaking results on the ImageNet challenge in 2012 by
    drastically improving upon prior techniques. Today, state-of-the-art CNNs for
    image classification routinely achieve human-level or better accuracy on standardized
    datasets. Architectures like ResNet, Inception, Xception, and EfficientNets optimize
    parameters, layer connectivity, and computation to classify thousands of object
    categories at superhuman performance levels [[52](#bib.bib52), [53](#bib.bib53),
    [56](#bib.bib56), [276](#bib.bib276)]. Beyond static images, video classification
    CNNs also extract spatial-temporal features to recognize complex activities and
    events.
  prefs: []
  type: TYPE_NORMAL
- en: X-B Object Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object detection is another major CV application that relies heavily on convolutional
    modeling. Two-stage detectors like Faster R-CNN and one-stage detectors like YOLO
    leverage region proposal networks and anchor boxes trained via priors to simultaneously
    localize and classify objects within images [[317](#bib.bib317), [318](#bib.bib318),
    [319](#bib.bib319), [320](#bib.bib320), [321](#bib.bib321), [322](#bib.bib322),
    [323](#bib.bib323), [324](#bib.bib324), [325](#bib.bib325), [326](#bib.bib326),
    [327](#bib.bib327), [328](#bib.bib328), [329](#bib.bib329), [330](#bib.bib330),
    [331](#bib.bib331)]. Recent works further optimize speed and accuracy, enabling
    real-time object detection on billions of parameter models. Techniques like mobile
    object detection address embedded constraints by designing lightweight CNN backbones
    and feature extractors optimized for on-device inference [[332](#bib.bib332)].
  prefs: []
  type: TYPE_NORMAL
- en: X-C Image Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Semantic segmentation tasks require dense pixel-level labeling of image content.
    FCN and U-Net CNNs employ skip connections and encoder-decoder mirrors to preserve
    spatial information across resolutions [[333](#bib.bib333), [334](#bib.bib334),
    [335](#bib.bib335), [336](#bib.bib336), [337](#bib.bib337), [338](#bib.bib338),
    [339](#bib.bib339), [340](#bib.bib340), [341](#bib.bib341), [342](#bib.bib342),
    [343](#bib.bib343), [344](#bib.bib344), [345](#bib.bib345), [346](#bib.bib346),
    [347](#bib.bib347), [348](#bib.bib348)]. PSPNet and DeepLab introduce pyramid
    spatial pooling modules to capture multi-scale contextual cues [[349](#bib.bib349)].
    GANs and conditional random fields further refine coarse segmentations from CNNs.
    Advances in medical imaging also apply segmentation CNNs to understand organ structures,
    localize pathologies, and aid diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: X-D Vision transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vision transformers have also emerged as a compelling alternative to traditional
    CNNs for CV tasks. Inspired by the success of language models, vision transformers
    divide images into discrete patches which are embedded and processed with self-attention.
    This allows them to capture long-range dependencies and multi-scale contextual
    information more effectively than CNNs. Models like ViT, DeiT, and Visual BERT
    demonstrate state-of-the-art results in tasks like image classification when pre-trained
    on large datasets [[350](#bib.bib350), [351](#bib.bib351), [352](#bib.bib352),
    [353](#bib.bib353), [354](#bib.bib354), [355](#bib.bib355), [356](#bib.bib356),
    [357](#bib.bib357)]. Research now focuses on optimizing transformer efficiency
    for real-time CV applications.
  prefs: []
  type: TYPE_NORMAL
- en: X-E One-shot/few-shot/Zero-shot learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One-shot and few-shot learning aim to address challenges posed by limited labeled
    training examples. Through metric learning and prototypical networks that learn
    robust representations from extensive base classes, models can effectively recognize
    new concepts from just one or a handful of examples without catastrophic forgetting
    [[358](#bib.bib358), [359](#bib.bib359), [360](#bib.bib360), [361](#bib.bib361),
    [362](#bib.bib362), [363](#bib.bib363), [364](#bib.bib364), [365](#bib.bib365),
    [366](#bib.bib366), [367](#bib.bib367), [368](#bib.bib368), [369](#bib.bib369),
    [370](#bib.bib370), [371](#bib.bib371), [372](#bib.bib372)]. This opens up CV
    to new long-tailed and incremental learning paradigms. Matching networks and prototypical
    networks efficiently compare test samples to prototype representations of base
    classes to generalize from limited exposures.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot learning emerges as a promising area where CNNs imagine possibilities
    beyond the limitations of labeled data [[373](#bib.bib373), [374](#bib.bib374),
    [375](#bib.bib375), [376](#bib.bib376), [377](#bib.bib377)]. Descriptors like
    attributes or semantic relationships introduce inductive biases facilitating generalization
    without example. SAE, DeViSE, and contemporary models transfer knowledge by aligning
    embeddings between seen and unseen categories connected through auxiliary descriptors.
    Knowledge graphs also provide structural inductive biases through entity and relation
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: X-F Weakly-supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Weakly supervised learning techniques also help alleviate dependence on labor-intensive
    annotations [[378](#bib.bib378), [379](#bib.bib379), [380](#bib.bib380), [381](#bib.bib381),
    [382](#bib.bib382), [383](#bib.bib383)]. Models can be trained end-to-end from
    weaker input signals like image-level tags or bounding box object locations instead
    of explicit pixel-level segmentation maps. Multi-instance learning approaches
    cluster image regions corresponding to each label to iteratively refine local
    predictions. Expectation-maximization (EM) and multiple instance learning jointly
    infer labels and recognize discriminative regions, enabling training from cheaper
    forms of weak supervision.
  prefs: []
  type: TYPE_NORMAL
- en: X-G Self-supervised/unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Self-supervised learning has also gained vast attention in CV by enabling pre-training
    from sheer ubiquity of unlabeled visual data [[384](#bib.bib384), [385](#bib.bib385),
    [386](#bib.bib386), [387](#bib.bib387), [388](#bib.bib388), [389](#bib.bib389),
    [390](#bib.bib390), [391](#bib.bib391), [392](#bib.bib392), [393](#bib.bib393)].
    Pretext tasks like predicting image rotations, solving jigsaw puzzles, or counting
    pixel colors allow models to learn rich visual representations applicable to downstream
    tasks. Recent contrastive self-supervised models like SimCLR, SwAV, and MoCo demonstrate
    that unlabeled pre-training rivals or exceeds supervised pre-training in various
    vision benchmarks, enabling more data-efficient fine-tuning or transfer to new
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: X-H Lifelong/Continual learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lifelong and continual learning aim to simulate open-world scenarios where models
    learn lifelong with non-stationary data distributions [[51](#bib.bib51)]. Models
    must avoid catastrophic forgetting when presented with new classes or shifts in
    existing class definitions without revisiting historical data [[394](#bib.bib394),
    [395](#bib.bib395), [396](#bib.bib396), [397](#bib.bib397), [398](#bib.bib398),
    [399](#bib.bib399), [400](#bib.bib400), [401](#bib.bib401), [402](#bib.bib402),
    [403](#bib.bib403)]. Elastic weight consolidation and incremental moment matching
    regularization preserve knowledge while accommodating new tasks. Research now
    explores task-aware architectures, dual-memory systems, and replay buffers that
    emulate memory reconsolidation to model lifelong visual learning.
  prefs: []
  type: TYPE_NORMAL
- en: X-I Vision language model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vision-language models (VLMs) have also emerged at the intersection of NLP and
    CV by grounding language in visual contexts. Models fuse multimodal inputs through
    attention and generate captions conditioned on images, or localize and describe
    visual entities based on linguistic context. Large pre-trained models such as
    CLIP, ALIGN, and Oscar demonstrate exciting capabilities like zero-shot classification,
    question-answering (QA), and visual dialog with potential applications in education,
    assistive technologies, and more.
  prefs: []
  type: TYPE_NORMAL
- en: X-J Medical image analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Medical imaging epitomizes the necessity of collaboration between deep learning
    and domain experts. Segmenting organs in volumetric scans, localizing anomalies
    across imaging modalities, and tracking patients longitudinally all leverage 3D/2D
    CNNs [[404](#bib.bib404), [405](#bib.bib405), [406](#bib.bib406), [407](#bib.bib407),
    [408](#bib.bib408), [409](#bib.bib409), [410](#bib.bib410), [411](#bib.bib411),
    [412](#bib.bib412), [413](#bib.bib413), [414](#bib.bib414), [415](#bib.bib415),
    [416](#bib.bib416), [417](#bib.bib417), [418](#bib.bib418)]. Advanced models exploit
    anatomical priors by enforcing smoothness, and preservation of edges and surfaces
    in predictions. Self-supervision further enables pre-training from non-private
    data before fine-tuning target tasks. Model interpretation especially matters
    here to ensure trust among clinicians [[410](#bib.bib410), [411](#bib.bib411),
    [412](#bib.bib412), [413](#bib.bib413)]. Beyond diagnosis, CNNs can also simulate
    novel views to aid surgical planning. Efficiency additionally matters for on-device
    deployment and assisting underserved populations lacking infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: X-K Video understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond images, video understanding presents unique challenges in modeling spatial-temporal
    relationships across consecutive frames. C3D and I3D CNNs introduce 3D convolutions
    directly learning from video volumes. Advanced techniques in video captioning
    and action recognition fuse language models and attention to jointly reason about
    visual content and linguistic semantics over time. Self-supervised learning from
    large unlabeled video repositories also emerges as a promising pretraining paradigm
    before fine-tuning downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: X-L Multi-task learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-task learning aims to improve generalization by jointly training CNNs
    on multiple related tasks using shared representations. This has proven successful
    across numerous applications by leveraging commonalities while mitigating overfitting
    individual tasks’ limited data [[419](#bib.bib419), [420](#bib.bib420), [421](#bib.bib421),
    [422](#bib.bib422)]. For example, YOLO trains object detection alongside other
    auxiliary predictions like segmentation and counting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-task CNNs outperform independent models in low-data regimes (See Section
    [VII](#S7 "VII Future Trends in CNN ‣ A Comprehensive Survey of Convolutions in
    Deep Learning: Applications, Challenges, and Future Trends") -¿ Sub-section G.)
    by borrowing statistical strength across related problems. Dense captioning localizes
    objects and describes scenes simultaneously. A single network predicts keypoints,
    normals, and semantic part segmentation. Deeper tasks benefit substantially from
    representations learned for more general shallow tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Progressively growing into new problem spaces via related auxiliary objectives
    also prevents catastrophic forgetting. Self-supervised pre-training establishes
    features broadly useful across downstream tasks, including those without annotations.
    Measuring and maximizing modularity in multi-task architectures additionally reduces
    interference between domains.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques like multi-granularity, multi-level, and heterogeneous multi-task
    learning further craft diverse objectives to progressively refine semantics captured
    at differing levels of granularity [[423](#bib.bib423), [424](#bib.bib424), [425](#bib.bib425),
    [426](#bib.bib426)]. Task relations range from independent, and cooperative where
    tasks improve each other, to completely shared exploiting identical representations.
    Properly designed, multi-task CNNs deliver state-of-the-art performance while
    improving generalizability, efficiency, and real-world applicability.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-task models combine CNNs with other modalities like language. For captioning,
    CNN-RNN fusion grounds generated text within visual contexts. For retrieval, ranking
    loss trains CNN-LSTM encoders to map semantically aligned vision-text pairs to
    nearby embeddings. Multi-modal pre-training on enormous unlabeled multimedia collections
    has proven highly beneficial via self-supervised alignment of domains.
  prefs: []
  type: TYPE_NORMAL
- en: X-M 6D vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6D vision aims to recover the full 6D pose (3D position, 3D orientation) of
    objects directly from monocular RGB images. This is a challenging problem due
    to the loss of depth information when projecting 3D scenes onto 2D images [[427](#bib.bib427),
    [428](#bib.bib428), [429](#bib.bib429), [430](#bib.bib430), [431](#bib.bib431),
    [432](#bib.bib432)]. Early works relied on CAD models and rendered synthetic data
    which lacked photorealism, while more recent approaches leverage large amounts
    of real training data.
  prefs: []
  type: TYPE_NORMAL
- en: CNN-based regression networks are commonly used which take images as input and
    directly predict the 6D pose values. PoseCNN showed this can achieve competitive
    accuracy to model-based regression if trained end-to-end on real data. Due to
    the complex, multi-modal nature of the target distribution, losses that ensure
    consistent predictions under different poses like reprojection or angular are
    beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Iterative refinement approaches first detect the object, then iteratively update
    the pose estimate based on 2D-3D correspondences. DeepIM predicts shape coefficients
    and refines using PnP. DPOD leverages deep features combined with geometric constraints
    in a RANSAC framework. Dense representations also help by reasoning about object
    parts independently.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-view and RGB-D sensors provide additional cues to leverage. MVD helps
    constrain the problem by training separate networks for each view and fusing results.
    Using both RGB and depth as input allows Depth-PoseNet to lift 2D predictions
    to 3D space. Multitask models predicting bounding boxes, keypoints, and poses
    jointly demonstrate accuracies approaching marker-based motion capture.
  prefs: []
  type: TYPE_NORMAL
- en: X-N Neural Architecture Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neural architecture search (NAS) aims to automate the design of neural networks
    leveraging the power of evolution and reinforcement learning. Rather than relying
    on human experts to laboriously craft CNN architectures, NAS approaches evolve
    architectures directly on target datasets and tasks. This has led to state-of-the-art
    vision models developed without human design choices [[433](#bib.bib433), [434](#bib.bib434),
    [435](#bib.bib435), [436](#bib.bib436), [437](#bib.bib437), [438](#bib.bib438),
    [439](#bib.bib439), [440](#bib.bib440)].
  prefs: []
  type: TYPE_NORMAL
- en: Early NAS works explored various search spaces defined by units, operations,
    and connections between them. Combining concepts like pruning, sharing weights
    across child models during evolution helped scaling search to larger spaces [[295](#bib.bib295),
    [299](#bib.bib299)]. Performance predictors further reduced costs by guiding search
    towards promising regions. Novel methods evolved filters, activation functions,
    and batch normalization layers for particular domains.
  prefs: []
  type: TYPE_NORMAL
- en: Recent efforts evolve entire sections or blocks, expanding applicable search
    spaces. Single-path one-shot approaches drastically sped up search without compromising
    quality. ProxylessNAS found efficient mobile architectures directly on target
    devices. NAS approaches also discover non-CNN models suiting problems beyond CV.
  prefs: []
  type: TYPE_NORMAL
- en: Once identified, the best architectures can be trained from scratch to further
    improve upon proxy accuracies predicted during the search. Late phase evolution
    also enhances architectures initially identified, while architecture parameters
    themselves may evolve. Overall, NAS technologies continuously push forward state-of-the-art
    for vision tasks given diverse data, constraints, or objectives.
  prefs: []
  type: TYPE_NORMAL
- en: X-O Neural Architecture Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neural architecture transformers (NAT) replace CNNs’ fixed topology with self-attention,
    replacing convolutional filtering with axial self-attention [[436](#bib.bib436),
    [441](#bib.bib441)]. This increased flexibility allows modeling long-range pixel
    dependencies crucial for vision tasks like segmentation. Vil-BERT introduced a
    multi-stage training procedure enabling pre-trained models to learn visual representations
    as well as natural language tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Early works divided input images into uniform patches processed independently
    by attention layers. More sophisticated designs aim to capture visual locality
    through hierarchical patch divisions better. Rotary positional embeddings and
    attention patterns help encode translation equivariance. Architectures like CoAtNet
    cascade blocks with increased resolution, improving accuracy and interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-scale vision transformers (MViT) incorporate prior convolutional inductive
    biases in hybrid models jointly benefiting from attention and translation equivariance.
    Combining vision transformers with convolutional networks particularly benefits
    medical image segmentation leveraging anatomical priors. Swin Transformers introduces
    a shifted window mechanism to focus computation locally across higher-resolution
    feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: Though still an emerging direction, neural architecture transformers open new
    pathways for CV by bringing the full generality of self-attention to bear on visual
    problems. Their continued development will surely impact future CV research by
    unlocking novel representational abilities. Alongside NAS, they hold promise for
    pushing boundaries through data-driven discovery operating directly within much
    broader algorithmic search spaces.
  prefs: []
  type: TYPE_NORMAL
- en: X-P Generative Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative models have made large strides in the area of CV through techniques
    like GANs and diffusion models [[442](#bib.bib442), [443](#bib.bib443), [444](#bib.bib444)].
    GANs pair a generator network against a discriminator network in an adversarial
    training procedure. This drives the generator to synthesize increasingly realistic
    fake images that can fool the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: GANs have produced impressive results generating photos that are near-indistinguishable
    from real images. Applications include image-to-image translation, super-resolution,
    and manipulating image attributes like style [[444](#bib.bib444), [445](#bib.bib445),
    [446](#bib.bib446), [447](#bib.bib447)]. However, GAN training remains tricky
    to stabilize. Issues like mode collapse require careful architecture and hyperparameter
    choices.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models provide an alternative generative framework gaining popularity.
    They utilize denoising diffusion probabilistic models (DDPMs) which gradually
    corrupt data with Gaussian noise before reversing the process [[442](#bib.bib442),
    [443](#bib.bib443), [444](#bib.bib444), [447](#bib.bib447), [448](#bib.bib448),
    [449](#bib.bib449)]. During generation, the model adds noise to a blank canvas
    and then predicts the noise-reduced output iteratively. This diffusion process
    proves more stable than adversarial training.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling from DDPMs follows an ancestral sampling approach regressing the noise
    at each step conditioned on the previous denoised output. Advanced techniques
    like score-based sampling further improve sample quality by maximizing the model’s
    density rather than following ancestral noise. Generative diffusion models (GDMs)
    also maximize a denoising score objective specifically for a generation [[449](#bib.bib449)].
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models have proven highly effective at synthesizing crisp, detailed
    images across varied datasets. Large-scale vision diffusion models (LVMs) like
    DALL-E 2 and DALL-E 3 demonstrate unparalleled capabilities of generating images
    from text prompts, and can even fuse language and vision to answer trivia questions
    about synthetic images.
  prefs: []
  type: TYPE_NORMAL
- en: By generating synthetic training data, generative models also benefit downstream
    classification, detection, and segmentation tasks through data augmentation. As
    generative diffusion models continue advancing, they will surely establish new
    frontiers in CV domains ranging from image editing to scientific discovery through
    computational experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: X-Q Meta Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Meta-learning, also known as learning to learn, aims to develop models that
    can rapidly adapt to new tasks and environments using only a few training examples.
    This is achieved by learning inductive biases about learning itself on a variety
    of related tasks during a meta-training phase. These biases are then leveraged
    during meta-test time on novel tasks [[450](#bib.bib450), [451](#bib.bib451)].
  prefs: []
  type: TYPE_NORMAL
- en: In CV, meta-learning enables CNNs to generalize beyond the restrictions of limited
    labeled examples through fast adaptation. Model-agnostic meta-learning (MAML)
    trains initial model parameters such that a few gradient steps fine-tune into
    new tasks. This learns efficient parameter initialization rather than solutions
    for any specific task [[450](#bib.bib450), [451](#bib.bib451), [452](#bib.bib452),
    [453](#bib.bib453), [454](#bib.bib454), [455](#bib.bib455), [456](#bib.bib456),
    [457](#bib.bib457)].
  prefs: []
  type: TYPE_NORMAL
- en: Metric-based approaches represent classes using prototypes that summarize inter/intra-class
    relationships independent of tasks [[450](#bib.bib450), [451](#bib.bib451), [452](#bib.bib452)].
    Matching networks compare new examples to prototypes, providing fast adaptation
    through learned metric space similarities. Meta-Dataset consolidates many few-shot
    image classification datasets, advancing state-of-the-art and evaluation protocols
    in this challenging zero/few-shot regime[[450](#bib.bib450), [451](#bib.bib451),
    [452](#bib.bib452), [457](#bib.bib457)].
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised auxiliary tasks like prediction, rotation, and context modeling
    further enhance generalization when used alongside supervised meta-learning objectives.
    Temporal ensemble models aggregate diverse predictions over time from a generator
    network, improving robustness to noise and outliers. Reinforcement meta-learning
    successfully trains visuomotor policies for robotic control from only a handful
    of demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: X-R Federated Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Federated learning (FL) enables distributed training across decentralized edge
    devices without exchanging private user data like images, videos or medical scans
    . It aims to collaboratively learn a shared global model tailored to non-IID user
    distributions through coordinated local updates. This paradigm attracts increased
    interest due to growing concerns around data privacy and security.
  prefs: []
  type: TYPE_NORMAL
- en: FL trains a centralized CNN model through an iterative process where devices
    download the latest parameters, contribute updates computed over shards of local
    data, and then push weights back. A parameter server aggregates updates to globally
    improve the model. A key challenge arises from heterogeneity in non-IID data distributions,
    devices, and unreliable network connectivity. FedVision applies FL to object detection
    directly over fragmented client videos.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques like personalized, multi-task, and meta-learning help address statistical
    heterogeneity in FL. Continual learning aspects prevent catastrophic forgetting
    when populations change over disseminated rounds. Differentially private algorithms
    and secure aggregation schemes ensure strong privacy in collaborative updates,
    advancing FL under stringent privacy constraints beyond vision to sensitive domains
    like healthcare.
  prefs: []
  type: TYPE_NORMAL
- en: XI Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have methodically explored the various CNN variations that have become more
    and more popular in recent years across a wide range of application sectors through
    this thorough survey. Our goal in this discussion part is to summarize the most
    significant findings from our evaluation of the literature and offer an analytical
    viewpoint on significant problems regarding the development and prospects of this
    area of study.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers are well-suited for grid-like data types, like images because
    they have proven highly capable of capturing spatial relationships and extracting
    hierarchical patterns. At the core of CNNs, commonly used for computer vision
    tasks such as object identification and image classification, remain traditional
    2D convolutions. However, as the field has evolved, additional specialized convolution
    approaches have emerged to handle different data modalities more effectively.
    One notable application of 1D convolutions is in sequential data domains including
    time series analysis and natural language processing. Their ability to capture
    temporal dependencies has enabled state-of-the-art accuracy on various language
    and audio processing problems. Likewise, 3D convolutions allow CNNs to effectively
    model volumetric medical images and video inputs by accounting for both spatial
    and temporal dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: While basic convolution varieties such as 2D and 3D continue powering many top
    models, more efficient variants have also been developed. Dilated convolutions
    utilize dilations to widen receptive fields without loss of resolution, aiding
    high-level semantic tasks such as segmentation. Grouped convolutions offer a means
    of factorizing convolutions to dramatically reduce computation and memory usage,
    enabling large, deep architectures. However, their representational abilities
    may remain limited compared to standard convolutions for advanced analysis. Depthwise
    separable convolutions, as used in MobileNets, have achieved tremendous success
    in deploying efficient CNNs on embedded and mobile devices via their channel-wise
    decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to novel convolution designs, the field is witnessing increasingly
    innovative integration of concepts from parallel research areas. For example,
    vision transformer models incorporate attention mechanisms to replace convolutional
    building blocks entirely, achieving strong results, especially on large datasets.
    Techniques like capsule networks aim to overcome CNN limitations through dynamic
    routing between feature vectors. Generative models such as Pix2Pix employ convolutional
    decoders to generate high-fidelity images from semantic maps or sketches. Advances
    in self-supervised learning provide alternative pretraining paradigms bypassing
    the need for vast annotated datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Further combining of deep learning techniques seems poised to yield fruitful
    synergies. For instance, incorporating attention into convolutional pipelines
    could endow them with the benefits of both approaches. Moreover, self-supervised
    mechanisms may help the unsupervised discovery of interpretable convolutional
    filters well-suited to specific domains. Despite remarkable achievements, open
    challenges remain regarding robustness, sparse data scenarios, model interpretability,
    and trustworthiness. Future progress relies on close collaboration between academia
    and industry to define real-world needs and expand deep learning’s positive societal
    impact.
  prefs: []
  type: TYPE_NORMAL
- en: Some convolution types have proven more enduring than others based on their
    flexibility and ability to adaptively fit diverse applications. While LeNet certainly
    played an instrumental pioneering role, more recent architectures better capture
    inherent data properties through principled network designs and optimizations.
    Meanwhile, innovation continues on all fronts, suggesting no single solution has
    emerged as definitive. Success hinges on judiciously combining innovations tailored
    to particular contexts rather than wholesale replacement of existing paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: A promising outlook envisions continued refinement of core CNN building blocks
    and their harmonious integration with new algorithmic concepts from self-supervised
    learning, attention mechanisms, and generative models. In conclusion, this survey
    highlights both the remarkable advances of convolutional neural networks to date
    and their vast unrealized potential through the future intersection of ideas across
    deep learning’s constantly evolving landscape.
  prefs: []
  type: TYPE_NORMAL
- en: XII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this comprehensive study of different convolution types in deep learning,
    we have gained valuable insights into these techniques’ diverse applications and
    strengths. CNNs have proven to be highly effective in various domains, ranging
    from image recognition to natural language processing. We compared various types
    of CNNS in various aspects, allowing us to understand their unique characteristics
    and advantages for specific tasks. Overall, this study emphasizes the importance
    of convolution in deep learning and its potential for future advances and improvements
    in artificial intelligence. Furthermore, the findings suggest that CNNs’ versatility
    makes them suitable for various applications beyond traditional computer vision
    tasks. Furthermore, the study emphasizes the importance of additional research
    and development to optimize and refine these techniques for specific domains and
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This work was partially supported by the NYUAD Center for Artificial Intelligence
    and Robotics (CAIR), funded by Tamkeen under the NYUAD Research Institute Award
    CG010\. This work was also partially supported by the project “eDLAuto: An Automated
    Framework for Energy-Efficient Embedded Deep Learning in Autonomous Systems”,
    funded by the NYUAD Research Enhancement Fund (REF).'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] I. H. Sarker, ”Deep Learning: A Comprehensive Overview on Techniques, Taxonomy,
    Applications and Research Directions,” SN Computer Science, vol. 2, no. 6, Aug.
    2021, doi: 10.1007/s42979-021-00815-1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] G. Hong, A. Folcarelli, J. Less, C. Wang, N. Erbasi, and S. Lin, ”Voice
    Assistants and Cancer Screening: A Comparison of Alexa, Siri, Google Assistant,
    and Cortana,” The Annals of Family Medicine, vol. 19, no. 5, pp. 447–449, Sep.
    2021, doi: 10.1370/afm.2713.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Kumar, S. Gadag, and U. Y. Nayak, ”The Beginning of a New Era: Artificial
    Intelligence in Healthcare,” Advanced Pharmaceutical Bulletin, vol. 11, no. 3,
    pp. 414–425, Jul. 2020, doi: 10.34172/apb.2021.049.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. B. Heaton and N. Polson, ”Deep Learning for Finance: Deep Portfolios,”
    SSRN Electronic Journal, 2016, doi: 10.2139/ssrn.2838013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. Veres and M. Moussa, ”Deep Learning for Intelligent Transportation Systems:
    A Survey of Emerging Trends,” IEEE Transactions on Intelligent Transportation
    Systems, vol. 21, no. 8, pp. 3152-3168, Aug. 2020, doi: 10.1109/TITS.2019.2929020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. Ghaderzadeh and F. Asadi, ”Deep Learning in the Detection and Diagnosis
    of COVID-19 Using Radiology Modalities: A Systematic Review,” Journal of Healthcare
    Engineering, vol. 2021, pp. 1-10, Mar. 2021, doi: 10.1155/2021/6677314.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] I. Goodfellow, Y. Bengio, and A. Courville, ”Deep learning,” MIT press,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Schmidhuber, ”Deep learning in neural networks: An overview,” Neural
    Networks, vol. 61, pp. 85–117, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] N. K. Logothetis and D. L. Sheinberg, ”Visual Object Recognition,” Mar.
    1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] H. Zhou et al., ”A deep learning approach for medical waste classification,”
    Feb. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Yang and J. Anjie, ”Recognition of Oil and Gas Reservoir Space Based
    on Deep Learning,” Jan. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] D. Jimenez-Carretero et al., ”Tox_(R)CNN: Deep learning-based nuclei profiling
    tool for drug toxicity screening,” Plos Computational Biology, vol. 14, no. 11,
    pp. e1006238, Nov. 2018, doi: 10.1371/journal.pcbi.1006238.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] A. G. Howard et al., ”Mobilenets: Efficient convolutional neural networks
    for mobile vision applications,” arXiv preprint arXiv:1704.04861, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] TensorFlow Lite. https://www.tensorflow.org/lite/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] R. Kavuluru, ”An end-to-end deep learning architecture for extracting
    protein–protein interactions affected by genetic mutations,” Jan. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] A. Kamilaris and F. X. Prenafeta-Boldú, ”A review of the use of convolutional
    neural networks in agriculture,” The Journal of Agricultural Science, vol. 156,
    no. 3, pp. 312–322, Apr. 2018, doi: 10.1017/s0021859618000436.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Klos and J. Patalas-Maliszewska, ”A Model for the Intelligent Supervision
    of Production for Industry 4.0,” May. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] R. Nakajo, S. Murata, H. Arie, and T. Ogata, ”Acquisition of Viewpoint
    Transformation and Action Mappings via Sequence to Sequence Imitative Learning
    by Deep Neural Networks,” Jul. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] V. D. Veksler, B. E. Hoffman, and N. Buchler, ”Symbolic Deep Networks:
    A Psychologically Inspired Lightweight and Efficient Approach to Deep Learning,”
    Topics in Cognitive Science, Oct. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] C. Xue et al., ”Detection of dementia on voice recordings using deep learning:
    a Framingham Heart Study,” Aug. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] C. Lai et al., ”The Use of Convolutional Neural Networks and Digital Camera
    Images in Cataract Detection,” Mar. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Ahmed and M. Kashmola, ”A proposed architecture for convolutional neural
    networks to detect skin cancers,” Iaes International Journal of Artificial Intelligence
    (Ij-Ai), vol. 11, no. 2, pp. 485, Jun. 2022, doi: 10.11591/ijai.v11.i2.pp485-493.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] X. Jiang, L. Teng and L. Teng, ”A novel Gauss-Laplace operator based on
    multi-scale convolution for dance motion image enhancement,” Jul. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Shuai, Y. Zhou and X. Song, ”A Stochastic Max Pooling Strategy for
    Convolutional Neural Network Trained by Noisy Samples,” International Journal
    of Computers Communications & Control, vol. 15, no. 1, Feb. 2020, doi: 10.15837/ijccc.2020.1.3712.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Zhang, J. Hou, Q. Wang, A. Hou and Y. Liu, ”Application of Transfer
    Learning and Feature Fusion Algorithms to Improve the Identification and Prediction
    Efficiency of Premature Ovarian Failure,” Mar. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Younesi, R. Afrouzian, and Y. Seyfari, ”A transfer learning approach
    with convolutional neural network for Face Mask Detection,” Journal of Advanced
    Signal Processing, vol. 5, no. 1, Jan. 2021, doi: 10.22034/jasp.2022.48447.1167.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] H. M. Jalajamony, H. M. Jalajamony and R. E. Fernandez, ”Deep-Learning
    Based Estimation of Dielectrophoretic Force,” Micromachines, vol. 13, no. 1, pp.
    41, Dec. 2021, doi: 10.3390/mi13010041.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] W. Liang, H. Zhang, G. Zhang and H. Cao, ”Rice Blast Disease Recognition
    Using a Deep Convolutional Neural Network,” Feb. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] G. Yang, X. Liang, S. Deng and X. D. Chen, ”Principal Component Research
    of the Teaching Model Based on Multimodal Neural Network Algorithm,” Jun. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Rong, Y. Chen and J. Yang, ”CNN-LSTM Hybrid Model for Kinematic Feature
    Analysis and Parabolic Radian Prediction in Basketball Videos,” Sep. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] A. Tiwari, M. Silver and A. Karnieli, ”A deep learning approach for automatic
    identification of ancient agricultural water harvesting systems,” Apr. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] I. Ahmed, B. T. Hammad and N. Jamil, ”A comparative analysis of image
    copy-move forgery detection algorithms based on hand and machine-crafted features,”
    *Indonesian Journal of Electrical Engineering and Computer Science*, vol. 22,
    no. 2, pp. 1177, May. 2021, doi: 10.11591/ijeecs.v22.i2.pp1177-1190.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] E. Setyati, S. Az, S. A. Hudiono and F. Kurniawan, ”CNN based Face Recognition
    System for Patients with Down and William Syndrome,” *Knowledge Engineering and
    Data Science*, vol. 4, no. 2, pp. 138, Dec. 2021, doi: 10.17977/um018v4i22021p138-144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] G. Hu, K. Wang and L. Liu, ”Underwater Acoustic Target Recognition Based
    on Depthwise Separable Convolution Neural Networks,” *Sensors*, vol. 21, no. 4,
    pp. 1429, Feb. 2021, doi: 10.3390/s21041429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Z. Jiang, Z. Dong, L. Wang and W. Jiang, ”Method for Diagnosis of Acute
    Lymphoblastic Leukemia Based on ViT-CNN Ensemble Model,” Aug. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] L. Wang, W. Zhou, Q. Chang, J. Chen and X. Zhou, ”Deep Ensemble Detection
    of Congestive Heart Failure Using Short-Term RR Intervals,” *IEEE Access*, vol.
    7, pp. 69559-69574, Jan. 2019, doi: 10.1109/access.2019.2912226.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] A. Zafar et al., “A Comparison of Pooling Methods for Convolutional Neural
    Networks,” *Applied Sciences*, vol. 12, no. 17, p. 8643, Jan. 2022, doi: 10.3390/app12178643.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] J. Zhu, J. Jang-Jaccard and P. A. Watters, ”Multi-Loss Siamese Neural
    Network With Batch Normalization Layer for Malware Detection,” Jan. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] A. Maniatopoulos and N. Mitianoudis, ”Learnable Leaky ReLU (LeLeLU): An
    Alternative Accuracy-Optimized Activation Function,” Dec. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] *”Delving Deep into Rectifiers: Surpassing Human-Level …”*, arxiv.org,
    (Accessed 18 Jul. 2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] L. E. V. Dyck, R. Kwitt, S. J. Denzler and W. R. Gruber, ”Comparing Object
    Recognition in Humans and Deep Convolutional Neural Networks—An Eye Tracking Study,”
    *Frontiers in Neuroscience*, vol. 15, Oct. 2021, doi: 10.3389/fnins.2021.750639.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Z. Wu, Y. Guo, W. Lin, S. Yu and Y. Ji, ”A Weighted Deep Representation
    Learning Model for Imbalanced Fault Diagnosis in Cyber-Physical Systems,” *Sensors*,
    vol. 18, no. 4, pp. 1096, Apr. 2018, doi: 10.3390/s18041096.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] H. Xia, C. Ding and Y. Liu, ”Sentiment Analysis Model Based on Self-Attention
    and Character-Level Embedding,” *IEEE Access*, vol. 8, pp. 184614-184620, Jan.
    2020, doi: 10.1109/access.2020.3029694.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Z. Zhao, X. Xie, W. Liu and Q. Pan, ”A Hybrid-3D Convolutional Network
    for Video Compressive Sensing,” Jan. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Z. Liu et al., ”Deep learning based brain tumor segmentation: a survey,”
    Jul. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] H. Chen et al., ”A Supervised Video Hashing Method Based on a Deep 3D
    Convolutional Neural Network for Large-Scale Video Retrieval,” *Sensors*, vol.
    21, no. 9, pp. 3094, Apr. 2021, doi: 10.3390/s21093094.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] G. Li, S. Jiang, I. Yun and J. Kim, ”Depth-Wise Asymmetric Bottleneck
    With Point-Wise Aggregation Decoder for Real-Time Semantic Segmentation in Urban
    Scenes,” *IEEE Access*, vol. 8, pp. 27495-27506, Jan. 2020, doi: 10.1109/access.2020.2971760.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Z. Lu et al., ”Fast Single Image Super-Resolution Via Dilated Residual
    Networks,” Jan. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] R. Kolaghassi, M. K. Al-Hares and K. Sirlantzis, ”Systematic Review of
    Intelligent Algorithms in Gait Analysis and Prediction for Lower Limb Robotic
    Systems,” Jan. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] M. Capra, B. Bussolino, A. Marchisio, G. Masera, M. Martina and M. Shafique,
    ”Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey
    of Current Trends, Challenges, and the Road Ahead,” in *IEEE Access*, vol. 8,
    pp. 225134-225180, 2020, doi: 10.1109/ACCESS.2020.3039858.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] K. Shaheen, M. A. Hanif, O. Hasan, and M. Shafique, “Continual Learning
    for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks,” *Journal
    of Intelligent & Robotic Systems*, vol. 105, no. 1, Apr. 2022, doi: 1007/s10846-022-01603-6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] C. Szegedy et al., “Going deeper with convolutions,” *CVPR 2015*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] K. He et al., “Deep residual learning for image recognition,” *CVPR 2016*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] G. Huang et al., “Densely connected convolutional networks,” *CVPR 2017*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. Howard et al., “MobileNets: Efficient CNNs for mobile vision applications,”
    *arXiv 2017*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] M. Tan and Q. Le, “EfficientNet: Rethinking model scaling for CNNs,” *ICML
    2019*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] B. Jin, L. Cruz and N. Goncalves, ”Deep Facial Diagnosis: Deep Transfer
    Learning From Face Recognition to Facial Diagnosis,” *IEEE Access*, vol. 8, pp.
    123649-123661, Jan. 2020, doi: 10.1109/access.2020.3005687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Yang, D. Kim and B. H. Oh, ”Deep Convolutional Grid Warping Network
    for Joint Depth Map Upsampling,” Jan. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] P. Lang, X. Fu, C. Feng, J. Dong, R. Qin and M. Martorella, ”LW-CMDANet:
    A Novel Attention Network for SAR Automatic Target Recognition,” *IEEE Journal
    of Selected Topics in Applied Earth Observations and Remote Sensing*, vol. 15,
    pp. 6615-6630, Jan. 2022, doi: 10.1109/jstars.2022.3195074.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] L. Li, J. Wang and X. Li, ”Efficiency Analysis of Machine Learning Intelligent
    Investment Based on K-Means Algorithm,” *IEEE Access*, vol. 8, pp. 147463-147470,
    Jan. 2020, doi: 10.1109/access.2020.3011366.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] H. Tomosada, T. Kudo, T. Fujisawa and M. Ikehara, ”GAN-Based Image Deblurring
    Using DCT Loss With Customized Datasets,” Jan. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] T. Wang *et al.*, ”Pretraining is All You Need for Image-to-Image Translation,”
    May. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] B. Kaddar, H. Fizazi, M. Hernandez-Cabronero, V. Sanchez and J. Serra-Sagrista,
    ”DivNet: Efficient Convolutional Neural Network via Multilevel Hierarchical Architecture
    Design,” Jan. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] R. Yu and J. Sun, ”Learning Polynomial-Based Separable Convolution for
    3D Point Cloud Analysis,” Jun. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] A. Kara, ”A Hybrid Prognostic Approach Based on Deep Learning for the
    Degradation Prediction of Machinery,” *Sakarya University Journal of Computer
    and Information Sciences*, vol. 4, no. 2, pp. 216-226, Aug. 2021, doi: 10.35377/saucis.04.02.912154.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] X. Zhang, J. Zhou, W. Sun and S. K. Jha, ”A Lightweight CNN Based on Transfer
    Learning for COVID-19 Diagnosis,” *Computers Materials & Continua*, vol. 72, no.
    1, pp. 1123-1137, Jan. 2022, doi: 10.32604/cmc.2022.024589.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] F. Sultonov, J. Park, S. Yun, D. Lim and J. Kang, ”Mixer U-Net: An Improved
    Automatic Road Extraction from UAV Imagery,” Feb. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. Shao, C. Qu, J. Li and S. Peng, ”A Lightweight Convolutional Neural
    Network Based on Visual Attention for SAR Image Target Classification,” *Sensors*,
    vol. 18, no. 9, pp. 3039, Sep. 2018, doi: 10.3390/s18093039.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] D. AL-Alimi, Y. Shao, R. Feng, M. A. A. Al-qaness, T. Seppänen and S.
    Kim, ”Multi-Scale Geospatial Object Detection Based on Shallow-Deep Feature Extraction,”
    *Remote Sensing*, vol. 11, no. 21, pp. 2525, Oct. 2019, doi: 10.3390/rs11212525.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] X. Yang, A. Chen, G. Zhou, W. Chen, Y. Gao and R. Jiang, ”Instance Segmentation
    and Classification Method for Plant Leaf Images Based on ISC-MRCNN and APS-DCCNN,”
    Jan. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Q. Liu *et al.*, ”Hybrid Attention Based Residual Network for Pansharpening,”
    *Remote Sensing*, vol. 13, no. 10, pp. 1962, May. 2021, doi: 10.3390/rs13101962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Ju and Z. Wang, ”Convolutional block attention module based on visual
    mechanism for robot image edge detection,” *Icst Transactions on Scalable Information
    Systems*, pp. 172214, Jul. 2018, doi: 10.4108/eai.19-11-2021.172214.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] M. Alam, M. D. Samad, L. Vidyaratne, A. Glandon, and K. M. Iftekharuddin,
    “Survey on Deep Neural Networks in Speech and Vision Systems,” *Neurocomputing*,
    vol. 417, pp. 302–321, Dec. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Q. Zhang, X. Wang, Y. Wu, H. Zhou, and S.-C. Zhu, “Interpretable CNNs
    for Object Classification,” vol. 43, no. 10, pp. 3416–3431, Oct. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. Kwabena Patrick, A. Felix Adekoya, A. Abra Mighty, and B. Y. Edward,
    “Capsule Networks – A survey,” *Journal of King Saud University - Computer and
    Information Sciences*, Sep. 2019, doi: 10.1016/j.jksuci.2019.09.014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] C. White *et al.*, “Neural Architecture Search: Insights from 1000 Papers,”
    Jan. 2023, doi: 10.48550/arxiv.2301.08727.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] I. J. Goodfellow *et al.*, “Generative Adversarial Networks,” *arXiv (Cornell
    University)*, Jun. 2014, doi: 10.48550/arxiv.1406.2661.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] X. Yuan, Z. Feng, M. Norton, and X. Li, “Generalized Batch Normalization:
    Towards Accelerating Deep Neural Networks,” *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 33, pp. 1682–1689, Jul. 2019, doi: 10.1609/aaai.v33i01.33011682.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Y. Wang, G. Wang, C. Chen, and Z. Pan, “Multi-scale dilated convolution
    of convolutional neural network for image denoising,” *Multimedia Tools and Applications*,
    Feb. 2019, doi: 10.1007/s11042-019-7377-y.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] J. Huang *et al.*, “Speed/accuracy trade-offs for modern convolutional
    object detectors,” *arXiv (Cornell University)*, Nov. 2016, doi: 10.48550/arxiv.1611.10012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] H. Zhu, H. Zhang, and Y. Jin, “From federated learning to federated neural
    architecture search: a survey,” *Complex & Intelligent Systems*, Jan. 2021, doi:
    10.1007/s40747-020-00247-z.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] O. N. Oyelade and A. E. Ezugwu, “A bioinspired neural architecture search
    based convolutional neural network for breast cancer detection using histopathology
    images,” *Scientific Reports*, vol. 11, no. 1, Oct. 2021, doi: 10.1038/s41598-021-98978-7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] F. Zhan, H. Zhu, and S. Lu, “Spatial Fusion GAN for Image Synthesis,”
    Jun. 2019, doi: 10.1109/cvpr.2019.00377.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] A. Singh *et al.*, “Neural Style Transfer: A Critical Review,” *IEEE Access*,
    vol. 9, pp. 131583–131613, 2021, doi: 10.1109/access.2021.3112996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] A. Vaswani *et al.*, “Attention Is All You Need,” *arXiv.org*, Dec. 05,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] K. Han *et al.*, ”A Survey on Vision Transformer,” in *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, vol. 45, no. 1, pp. 87-110, 1 Jan.
    2023, doi: 10.1109/TPAMI.2022.3152247.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Khan *et al.*, “A survey of the Vision Transformers and its CNN-Transformer
    based Variants,” *arXiv.org*, Aug. 08, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] J. Memon, M. Sami, R. A. Khan and M. Uddin, ”Handwritten Optical Character
    Recognition (OCR): A Comprehensive Systematic Literature Review (SLR),” in *IEEE
    Access*, vol. 8, pp. 142642-142668, 2020, doi: 10.1109/ACCESS.2020.3012542.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Tal Ridnik, E. Ben-Baruch, A. Noy, and Lihi Zelnik-Manor, “ImageNet-21K
    Pretraining for the Masses,” *arXiv (Cornell University)*, Apr. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M. Hnewa and H. Radha, ”Object Detection Under Rainy Conditions for Autonomous
    Vehicles: A Review of State-of-the-Art and Emerging Techniques,” in *IEEE Signal
    Processing Magazine*, vol. 38, no. 1, pp. 53-67, Jan. 2021, doi: 10.1109/MSP.2020.2984801.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] M. Elhoseny, “Multi-object Detection and Tracking (MODT) Machine Learning
    Model for Real-Time Video Surveillance Systems,” *Circuits, Systems, and Signal
    Processing*, Aug. 2019, doi: 10.1007/s00034-019-01234-7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] S. Thakur and A. Kumar, “X-ray and CT-scan-based automated detection and
    classification of covid-19 using convolutional neural networks (CNN),” *Biomedical
    Signal Processing and Control*, vol. 69, p. 102920, Aug. 2021, doi: 10.1016/j.bspc.2021.102920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] F. Ramzan, M. U. G. Khan, S. Iqbal, T. Saba and A. Rehman, ”Volumetric
    Segmentation of Brain Regions From MRI Scans Using 3D Convolutional Neural Networks,”
    in *IEEE Access*, vol. 8, pp. 103697-103709, 2020, doi: 10.1109/ACCESS .2020.2998901.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] T.-Y. Lin *et al.*, “Microsoft COCO: Common Objects in Context,” *arXiv
    (Cornell University)*, May 2014, doi: 10.48550/arxiv.1405.0312.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] J. Deng *et al.*, ”ImageNet: A large-scale hierarchical image database,”
    *2009 IEEE Conference on Computer Vision and Pattern Recognition*, Miami, FL,
    USA, 2009, pp. 248-255, doi: 10.1109/CVPR.2009.5206848.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Y. Li *et al.*, ”Unified multimodal transformers for joint video-language
    modeling,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] H. Tan and M. Bansal, “LXMERT: Learning Cross-Modality Encoder Representations
    from Transformers,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] A. Radford *et al.*, “Learning Transferable Visual Models From Natural
    Language Supervision,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] J.-B. Alayrac *et al.*, “Self-Supervised Multimodal Versatile Networks,”
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] G. O. Young, “Synthetic structure of industrial plastics,” in Plastics,
    vol. 3, Polymers of Hexadromicon, J. Peters, Ed., 2^(nd) ed. New York, NY, USA:
    McGraw-Hill, 1964, pp. 15-64. [Online]. Available: http://www.bookref.com.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Zhou and Oncel Tuzel, “VoxelNet: End-to-End Learning for Point Cloud
    Based 3D Object Detection,” Nov. 2017, doi: 10.48550/arxiv.1711.06396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. He and L. Xiao, ”Structured Pruning for Deep Convolutional Neural
    Networks: A Survey,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    doi: 10.1109/TPAMI.2023.3334614.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] I. Oguntola, S. Olubeko and C. Sweeney, ”SlimNets: An Exploration of
    Deep Model Compression and Acceleration,” 2018 IEEE High Performance extreme Computing
    Conference (HPEC), Waltham, MA, USA, 2018, pp. 1-6, doi: 10.1109/HPEC.2018.8547604.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] T. Guo, T. Zhang, E. Lim, M. López-Benítez, F. Ma and L. Yu, ”A Review
    of Wavelet Analysis and Its Applications: Challenges and Opportunities,” in IEEE
    Access, vol. 10, pp. 58869-58903, 2022, doi: 10.1109/ACCESS.2022.3179517.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] G. Othman and D. Q. Zeebaree, “The Applications of Discrete Wavelet Transform
    in Image Processing: A Review”, jscdm, vol. 1, no. 2, pp. 31–43, Dec. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] A. Saxena, A. Khanna, and D. Gupta, “Emotion Recognition and Detection
    Methods: A Comprehensive Survey,” Journal of Artificial Intelligence and Systems,
    vol. 2, no. 1, pp. 53–79, 2020, doi: 10.33969/ais.2020.21005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] T. Williams and R. Li, ”Advanced Image Classification Using Wavelets
    and Convolutional Neural Networks,” 2016 15th IEEE International Conference on
    Machine Learning and Applications (ICMLA), Anaheim, CA, USA, 2016, pp. 233-239,
    doi: 10.1109/ICMLA.2016.0046.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] P. Liu, H. Zhang, W. Lian and W. Zuo, ”Multi-Level Wavelet Convolutional
    Neural Networks,” in IEEE Access, vol. 7, pp. 74973-74985, 2019, doi: 10.1109/ACCESS.2019.2921451.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] S. Fujieda, K. Takayama, and T. Hachisuka, “Wavelet Convolutional Neural
    Networks,” arXiv.org, May 20, 2018\. https://arxiv.org/abs/1805.08620 (accessed
    Nov. 08, 2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] W. Zhang et al., ”A Wavelet-Based Asymmetric Convolution Network for
    Single Image Super-Resolution,” in IEEE Access, vol. 9, pp. 28976-28986, 2021,
    doi: 10.1109/ACCESS.2021.3058648.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Y. Wu, P. Qian and X. Zhang, ”Two-Level Wavelet-Based Convolutional Neural
    Network for Image Deblurring,” in IEEE Access, vol. 9, pp. 45853-45863, 2021,
    doi: 10.1109/ACCESS.2021.3067055.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Z. Tao, T. Wei and J. Li, ”Wavelet Multi-Level Attention Capsule Network
    for Texture Classification,” in IEEE Signal Processing Letters, vol. 28, pp. 1215-1219,
    2021, doi: 10.1109/LSP.2021.3088052.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] M. C. Kim, J. H. Park, and M. H. Sunwoo, ”Multilevel Feature Extraction
    Using Wavelet Attention for Deep Joint Demosaicking and Denoising,” in IEEE Access,
    vol. 10, pp. 77099-77109, 2022, doi: 10.1109/ACCESS.2022.3192451.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Z. Xie, Z. Wen, J. Liu, Z. Liu, X. Wu, and M. Tan, “Deep Transferring
    Quantization,” Lecture Notes in Computer Science, pp. 625–642, Jan. 2020, doi:
    https://doi.org/10.1007/978-3-030-58598-3_37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] H. Li et al., “Hard Sample Matters a Lot in Zero-Shot Quantization,”
    Jun. 2023, doi: 10.1109/cvpr52729.2023.02339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] M. Lin et al., “HRank: Filter Pruning Using High-Rank Feature Map,” Jun.
    2020, doi: 10.1109/cvpr42600.2020.00160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Moez Krichen, “Convolutional Neural Networks: A Survey,” Computers, vol.
    12, no. 8, pp. 151–151, Jul. 2023, doi: 10.3390/computers12080151'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] L. Alzubaidi et al., “Review of deep learning: concepts, CNN architectures,
    challenges, applications, future directions,” Journal of Big Data, vol. 8, no.
    1, Mar. 2021, doi: 10.1186/s40537-021-00444-8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Z. Li, F. Liu, W. Yang, S. Peng and J. Zhou, ”A Survey of Convolutional
    Neural Networks: Analysis, Applications, and Prospects,” in IEEE Transactions
    on Neural Networks and Learning Systems, vol. 33, no. 12, pp. 6999-7019, Dec.
    2022, doi: 10.1109/TNNLS.2021.3084827.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] A. Khan, A. Sohail, U. Zahoora, and A. S. Qureshi, “A survey of the recent
    architectures of deep convolutional neural networks,” Artificial Intelligence
    Review, vol. 53, Apr. 2020, doi: 10.1007/s10462-020-09825-6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] S. Xu, A. Huang, L. Chen and B. Zhang, ”Convolutional Neural Network
    Pruning: A Survey,” 2020 39th Chinese Control Conference (CCC), Shenyang, China,
    2020, pp. 7458-7463, doi: 10.23919/CCC50068.2020.9189610.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Y. He and L. Xiao, ”Structured Pruning for Deep Convolutional Neural
    Networks: A Survey,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    doi: 10.1109/TPAMI.2023.3334614.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] U. Kulkarni, S. S. Hallad, A. Patil, T. Bhujannavar, S. Kulkarni and
    S. M. Meena, ”A Survey on Filter Pruning Techniques for Optimization of Deep Neural
    Networks,” 2022 Sixth International Conference on I-SMAC (IoT in Social, Mobile,
    Analytics and Cloud) (I-SMAC), Dharan, Nepal, 2022, pp. 610-617, doi: 10.1109/I-SMAC55078.2022.9987264.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] S. Vadera and S. Ameen, ”Methods for Pruning Deep Neural Networks,” in
    IEEE Access, vol. 10, pp. 63280-63300, 2022, doi: 10.1109/ACCESS.2022.3182659.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] A. R. Aswani, C. R and A. P. James, ”Unstructured Weight Pruning in Variability-Aware
    Memristive Crossbar Neural Networks,” 2022 IEEE International Symposium on Circuits
    and Systems (ISCAS), Austin, TX, USA, 2022, pp. 3458-3462, doi: 10.1109/ISCAS48785.2022.9937284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] A. Liang, H. Zhang, H. Hua and W. Chen, ”To Drop or to Select: Reduce
    the Negative Effects of Disturbance Features for Point Cloud Classification From
    an Interpretable Perspective,” in IEEE Access, vol. 11, pp. 36184-36202, 2023,
    doi: 10.1109/ACCESS.2023.3266340.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Y. Peng et al., ”Sparse-to-Dense Multi-Encoder Shape Completion of Unstructured
    Point Cloud,” in IEEE Access, vol. 8, pp. 30969-30978, 2020, doi: 10.1109/ACCESS.2020.2973003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Zhang, S. Li, J. Wu, S. Li and B. Zhang, ”Exploring Semantic Information
    Extraction From Different Data Forms in 3D Point Cloud Semantic Segmentation,”
    in IEEE Access, vol. 11, pp. 61929-61949, 2023, doi: 10.1109/ACCESS.2023.3287940.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Y. Wang, X. Tang, and C. Yue, ”Enhancing the Local Graph Semantic Feature
    for 3D Point Cloud Classification and Segmentation,” in IEEE Access, vol. 10,
    pp. 74620-74628, 2022, doi: 10.1109/ACCESS.2022.3190966.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] J. Zeng, D. Wang, and P. Chen, ”A Survey on Transformers for Point Cloud
    Processing: An Updated Overview,” in IEEE Access, vol. 10, pp. 86510-86527, 2022,
    doi: 10.1109/ACCESS.2022.3198999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] “Caffe — Deep Learning Framework,” Berkeleyvision.org, 2012\. https://caffe.berkeleyvision.org/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] PyTorch, “PyTorch,” Pytorch.org, 2023\. https://pytorch.org/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] TensorFlow, “TensorFlow,” TensorFlow, 2019\. https://www.tensorflow.org/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Keras, “Home - Keras Documentation,” Keras.io, 2019\. https://keras.io/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] OpenCV, “OpenCV library,” Opencv.org, 2019\. https://opencv.org/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] “apache/mxnet,” GitHub, Jan. 09, 2024\. https://github.com/apache/mxnet
    (accessed Jan. 09, 2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] “Chainer: A flexible framework for neural networks,” Chainer. https://chainer.org/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] “Eclipse DeepLearning4J,” deeplearning4j.konduit.ai. https://deeplearning4j.konduit.ai/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] F. Ullah, I. Ullah, R. U. Khan, S. Khan, K. Khan and G. Pau, ”Conventional
    to Deep Ensemble Methods for Hyperspectral Image Classification: A Comprehensive
    Survey,” in IEEE Journal of Selected Topics in Applied Earth Observations and
    Remote Sensing, doi: 10.1109/JSTARS.2024.3353551.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] V. A. Ashwath, O. K. Sikha and R. Benitez, ”TS-CNN: A Three-Tier Self-Interpretable
    CNN for Multi-Region Medical Image Classification,” in IEEE Access, vol. 11, pp.
    78402-78418, 2023, doi: 10.1109/ACCESS.2023.3299850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] X. He and Y. Chen, ”Optimized Input for CNN-Based Hyperspectral Image
    Classification Using Spatial Transformer Network,” in IEEE Geoscience and Remote
    Sensing Letters, vol. 16, no. 12, pp. 1884-1888, Dec. 2019, doi: 10.1109/LGRS.2019.2911322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Y. Pei, Y. Huang, Q. Zou, X. Zhang and S. Wang, ”Effects of Image Degradation
    and Degradation Removal to CNN-Based Image Classification,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 43, no. 4, pp. 1239-1253, 1
    April 2021, doi: 10.1109/TPAMI.2019.2950923.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] L. Song et al., ”A Deep Multi-Modal CNN for Multi-Instance Multi-Label
    Image Classification,” in IEEE Transactions on Image Processing, vol. 27, no.
    12, pp. 6025-6038, Dec. 2018, doi: 10.1109/TIP.2018.2864920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] C. Shi, L. Fang and H. Shen, ”Convolutional Neural Networks With Class-Driven
    Loss for Multiscale VHR Remote Sensing Image Classification,” in IEEE Access,
    vol. 8, pp. 149162-149175, 2020, doi: 10.1109/ACCESS.2020.3014975.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] D. Wang, J. Zhang, B. Du, L. Zhang and D. Tao, ”DCN-T: Dual Context Network
    With Transformer for Hyperspectral Image Classification,” in IEEE Transactions
    on Image Processing, vol. 32, pp. 2536-2551, 2023, doi: 10.1109/TIP.2023.3270104.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] S. Khan, M. Sajjad, T. Hussain, A. Ullah and A. S. Imran, ”A Review on
    Traditional Machine Learning and Deep Learning Models for WBCs Classification
    in Blood Smear Images,” in IEEE Access, vol. 9, pp. 10657-10673, 2021, doi: 10.1109/ACCESS.2020.3048172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] S. A. H. Minoofam, A. Bastanfard and M. R. Keyvanpour, ”TRCLA: A Transfer
    Learning Approach to Reduce Negative Transfer for Cellular Learning Automata,”
    in IEEE Transactions on Neural Networks and Learning Systems, vol. 34, no. 5,
    pp. 2480-2489, May 2023, doi: 10.1109/TNNLS.2021.3106705.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Hao, ”Deep learning-based medical image analysis with explainable
    transfer learning,” 2023 International Conference on Computer Engineering and
    Distance Learning (CEDL), Shanghai, China, 2023, pp. 106-109, doi: 10.1109/CEDL60560.2023.00029.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] L. Shao, F. Zhu and X. Li, ”Transfer Learning for Visual Categorization:
    A Survey,” in IEEE Transactions on Neural Networks and Learning Systems, vol.
    26, no. 5, pp. 1019-1034, May 2015, doi: 10.1109/TNNLS.2014.2330900.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] E. Chalmers, E. B. Contreras, B. Robertson, A. Luczak and A. Gruber,
    ”Learning to Predict Consequences as a Method of Knowledge Transfer in Reinforcement
    Learning,” in IEEE Transactions on Neural Networks and Learning Systems, vol.
    29, no. 6, pp. 2259-2270, June 2018, doi: 10.1109/TNNLS.2017.2690910.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] T. V. Phan, S. Sultana, T. G. Nguyen and T. Bauschert, ”$Q$ - TRANSFER:
    A Novel Framework for Efficient Deep Transfer Learning in Networking,” 2020 International
    Conference on Artificial Intelligence in Information and Communication (ICAIIC),
    Fukuoka, Japan, 2020, pp. 146-151, doi: 10.1109/ICAIIC48513.2020.9065240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] T. T. Chungath, A. M. Nambiar and A. Mittal, ”Transfer Learning and Few-Shot
    Learning Based Deep Neural Network Models for Underwater Sonar Image Classification
    With a Few Samples,” in IEEE Journal of Oceanic Engineering, doi: 10.1109/JOE.2022.3221127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] A. M. Nagib, H. Abou-zeid and H. S. Hassanein, ”Safe and Accelerated
    Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach,”
    in IEEE Journal on Selected Areas in Communications, doi: 10.1109/JSAC.2023.3336191.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] M. Biehler, Y. Sun, S. Kode, J. Li and J. Shi, ”PLURAL: 3D Point Cloud
    Transfer Learning via Contrastive Learning With Augmentations,” in IEEE Transactions
    on Automation Science and Engineering, doi: 10.1109/TASE.2023.3345807.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] H. Li, Z. Wang, C. Lan, P. Wu and N. Zeng, ”A Novel Dynamic Multiobjective
    Optimization Algorithm With Non-Inductive Transfer Learning Based on Multi-Strategy
    Adaptive Selection,” in IEEE Transactions on Neural Networks and Learning Systems,
    doi: 10.1109/TNNLS.2023.3295461.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] H. Chen, H. Luo, B. Huang, B. Jiang and O. Kaynak, ”Transfer Learning-Motivated
    Intelligent Fault Diagnosis Designs: A Survey, Insights, and Perspectives,” in
    IEEE Transactions on Neural Networks and Learning Systems, doi: 10.1109/TNNLS.2023.3290974.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] H. S. Mputu, A. Abdel-Mawgood, A. Shimada and M. S. Sayed, ”Tomato Quality
    Classification based on Transfer Learning Feature Extraction and Machine Learning
    Algorithm Classifiers,” in IEEE Access, doi: 10.1109/ACCESS.2024.3352745.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] T. Zhou et al., ”Cooperative Multi-Agent Transfer Learning with Coalition
    Pattern Decomposition,” in IEEE Transactions on Games, doi: 10.1109/TG.2023.3272386.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] L. He, Q. Wei, M. Gong, X. Yang and J. Wei, ”Transfer Learning Based
    Center-of-mass Positioning Methods for Cultural Relics,” in IEEE Access, doi:
    10.1109/ACCESS.2023.3349017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] M. S. Azari, F. Flammini, S. Santini and M. Caporuscio, ”A Systematic
    Literature Review on Transfer Learning for Predictive Maintenance in Industry
    4.0,” in IEEE Access, vol. 11, pp. 12887-12910, 2023, doi: 10.1109/ACCESS.2023.3239784.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] D. Onita, ”Active Learning Based on Transfer Learning Techniques for
    Text Classification,” in IEEE Access, vol. 11, pp. 28751-28761, 2023, doi: 10.1109/ACCESS.2023.3260771.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Q. Li et al., ”A multi-task learning based approach to biomedical entity
    relation extraction,” 2018 IEEE International Conference on Bioinformatics and
    Biomedicine (BIBM), Madrid, Spain, 2018, pp. 680-682, doi: 10.1109/BIBM.2018.8621284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] H. Li and J. Qi, ”A Multi-Task Learning and Data Augmentation-Based Pose
    Estimation Algorithm,” 2023 8th International Conference on Information Systems
    Engineering (ICISE), Dalian, China, 2023, pp. 358-361, doi: 10.1109/ICISE60366.2023.00082.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] N. Jin, J. Wu, X. Ma, K. Yan and Y. Mo, ”Multi-Task Learning Model Based
    on Multi-Scale CNN and LSTM for Sentiment Classification,” in IEEE Access, vol.
    8, pp. 77060-77072, 2020, doi: 10.1109/ACCESS.2020.2989428.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] S. Liu, F. Yang, F. Kang and J. Yang, ”A Multi-Task Learning Method for
    Weakly Supervised Sound Event Detection,” ICASSP 2022 - 2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore,
    2022, pp. 8802-8806, doi: 10.1109/ICASSP43922.2022.9746947.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] X. Ouyang et al., ”A 3D-CNN and LSTM Based Multi-Task Learning Architecture
    for Action Recognition,” in IEEE Access, vol. 7, pp. 40757-40770, 2019, doi: 10.1109/ACCESS.2019.2906654.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] L. Yunxiang and Z. Kexin, ”Design of Efficient Speech Emotion Recognition
    Based on Multi Task Learning,” in IEEE Access, vol. 11, pp. 5528-5537, 2023, doi:
    10.1109/ACCESS.2023.3237268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Q. Zhou and Q. Zhao, ”Flexible Clustered Multi-Task Learning by Learning
    Representative Tasks,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 38, no. 2, pp. 266-278, 1 Feb. 2016, doi: 10.1109/TPAMI.2015.2452911.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Y. Yan, E. Ricci, R. Subramanian, G. Liu, O. Lanz and N. Sebe, ”A Multi-Task
    Learning Framework for Head Pose Estimation under Target Motion,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 38, no. 6, pp. 1070-1083, 1
    June 2016, doi: 10.1109/TPAMI.2015.2477843.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] A. -A. Liu, Y. -T. Su, W. -Z. Nie and M. Kankanhalli, ”Hierarchical Clustering
    Multi-Task Learning for Joint Human Action Grouping and Recognition,” in IEEE
    Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 1, pp.
    102-114, 1 Jan. 2017, doi: 10.1109/TPAMI.2016.2537337.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Q. Chen, W. Liu and X. Yu, ”A Viewpoint Aware Multi-Task Learning Framework
    for Fine-Grained Vehicle Recognition,” in IEEE Access, vol. 8, pp. 171912-171923,
    2020, doi: 10.1109/ACCESS.2020.3024658.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] G. Buroni, B. Lebichot and G. Bontempi, ”AST-MTL: An Attention-Based
    Multi-Task Learning Strategy for Traffic Forecasting,” in IEEE Access, vol. 9,
    pp. 77359-77370, 2021, doi: 10.1109/ACCESS.2021.3083412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] C. Ding, Z. Lu, S. Wang, R. Cheng and V. N. Boddeti, ”Mitigating Task
    Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable
    Primitives,” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), Vancouver, BC, Canada, 2023, pp. 7756-7765, doi: 10.1109/CVPR52729.2023.00749.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] W. Choi and S. Im, ”Dynamic Neural Network for Multi-Task Learning Searching
    across Diverse Network Topologies,” 2023 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 3779-3788, doi:
    10.1109/CVPR52729.2023.00368.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] R. Ranjan, V. M. Patel and R. Chellappa, ”HyperFace: A Deep Multi-Task
    Learning Framework for Face Detection, Landmark Localization, Pose Estimation,
    and Gender Recognition,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 41, no. 1, pp. 121-135, 1 Jan. 2019, doi: 10.1109/TPAMI.2017.2781233.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] H. Han, A. K. Jain, F. Wang, S. Shan and X. Chen, ”Heterogeneous Face
    Attribute Estimation: A Deep Multi-Task Learning Approach,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 40, no. 11, pp. 2597-2609,
    1 Nov. 2018, doi: 10.1109/TPAMI.2017.2738004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] G. He, Y. Huo, M. He, H. Zhang and J. Fan, ”A Novel Orthogonality Loss
    for Deep Hierarchical Multi-Task Learning,” in IEEE Access, vol. 8, pp. 67735-67744,
    2020, doi: 10.1109/ACCESS.2020.2985991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Z. -Q. Zhao, P. Zheng, S. -T. Xu and X. Wu, ”Object Detection With Deep
    Learning: A Review,” in IEEE Transactions on Neural Networks and Learning Systems,
    vol. 30, no. 11, pp. 3212-3232, Nov. 2019, doi: 10.1109/TNNLS.2018.2876865.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] K. He, G. Gkioxari, P. Dollár and R. Girshick, ”Mask R-CNN,” in IEEE
    Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 2, pp.
    386-397, 1 Feb. 2020, doi: 10.1109/TPAMI.2018.2844175.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] R. Girshick, J. Donahue, T. Darrell and J. Malik, ”Region-Based Convolutional
    Networks for Accurate Object Detection and Segmentation,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 38, no. 1, pp. 142-158, 1 Jan.
    2016, doi: 10.1109/TPAMI.2015.2437384.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] S. -H. Gao, M. -M. Cheng, K. Zhao, X. -Y. Zhang, M. -H. Yang and P. Torr,
    ”Res2Net: A New Multi-Scale Backbone Architecture,” in IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 43, no. 2, pp. 652-662, 1 Feb. 2021, doi:
    10.1109/TPAMI.2019.2938758.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Z. Wu, J. Wen, Y. Xu, J. Yang, X. Li and D. Zhang, ”Enhanced Spatial
    Feature Learning for Weakly Supervised Object Detection,” in IEEE Transactions
    on Neural Networks and Learning Systems, vol. 35, no. 1, pp. 961-972, Jan. 2024,
    doi: 10.1109/TNNLS.2022.3178180.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] L. Jiao et al., ”A Survey of Deep Learning-Based Object Detection,” in
    IEEE Access, vol. 7, pp. 128837-128868, 2019, doi: 10.1109/ACCESS.2019.2939201.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] J. Kang, S. Tariq, H. Oh and S. S. Woo, ”A Survey of Deep Learning-Based
    Object Detection Methods and Datasets for Overhead Imagery,” in IEEE Access, vol.
    10, pp. 20118-20134, 2022, doi: 10.1109/ACCESS.2022.3149052.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] S. Hoque, M. Y. Arafat, S. Xu, A. Maiti and Y. Wei, ”A Comprehensive
    Review on 3D Object Detection and 6D Pose Estimation With Deep Learning,” in IEEE
    Access, vol. 9, pp. 143746-143770, 2021, doi: 10.1109/ACCESS.2021.3114399.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Y. -L. Li and S. Wang, ”HAR-Net: Joint Learning of Hybrid Attention for
    Single-Stage Object Detection,” in IEEE Transactions on Image Processing, vol.
    29, pp. 3092-3103, 2020, doi: 10.1109/TIP.2019.2957850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Z. Yuan, X. Song, L. Bai, Z. Wang and W. Ouyang, ”Temporal-Channel Transformer
    for 3D Lidar-Based Video Object Detection for Autonomous Driving,” in IEEE Transactions
    on Circuits and Systems for Video Technology, vol. 32, no. 4, pp. 2068-2078, April
    2022, doi: 10.1109/TCSVT.2021.3082763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] H. Ibrahem, A. D. A. Salem and H. -S. Kang, ”Real-Time Weakly Supervised
    Object Detection Using Center-of-Features Localization,” in IEEE Access, vol.
    9, pp. 38742-38756, 2021, doi: 10.1109/ACCESS.2021.3064372.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] A. B. Amjoud and M. Amrouch, ”Object Detection Using Deep Learning, CNNs,
    and Vision Transformers: A Review,” in IEEE Access, vol. 11, pp. 35479-35516,
    2023, doi: 10.1109/ACCESS.2023.3266093.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] H. Wang, Q. Wang, H. Zhang, Q. Hu, and W. Zuo, ”CrabNet: Fully Task-Specific
    Feature Learning for One-Stage Object Detection,” in IEEE Transactions on Image
    Processing, vol. 31, pp. 2962-2974, 2022, doi: 10.1109/TIP.2022.3162099.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] T. Gao, H. Pan and H. Gao, ”Monocular 3D Object Detection With Sequential
    Feature Association and Depth Hint Augmentation,” in IEEE Transactions on Intelligent
    Vehicles, vol. 7, no. 2, pp. 240-250, June 2022, doi: 10.1109/TIV.2022.3143954.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Z. Zhang and T. D. Bui, ”Attention-based Selection Strategy for Weakly
    Supervised Object Localization,” 2020 25th International Conference on Pattern
    Recognition (ICPR), Milan, Italy, 2021, pp. 10305-10311, doi: 10.1109/ICPR48806.2021.9412173.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] J. Wei, Q. Wang, Z. Li, S. Wang, S. K. Zhou and S. Cui, ”Shallow Feature
    Matters for Weakly Supervised Object Localization,” 2021 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp.
    5989-5997, doi: 10.1109/CVPR46437.2021.00593.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] L. Zhu, Q. She, Q. Chen, Y. You, B. Wang and Y. Lu, ”Weakly Supervised
    Object Localization as Domain Adaption,” 2022 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 14617-14626,
    doi: 10.1109/CVPR52688.2022.01423.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] W. Gao et al., ”TS-CAM: Token Semantic Coupled Attention Map for Weakly
    Supervised Object Localization,” 2021 IEEE/CVF International Conference on Computer
    Vision (ICCV), Montreal, QC, Canada, 2021, pp. 2866-2875, doi: 10.1109/ICCV48922.2021.00288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] X. Pan et al., ”Unveiling the Potential of Structure Preserving for Weakly
    Supervised Object Localization,” 2021 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp. 11637-11646, doi: 10.1109/CVPR46437.2021.01147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] G. Guo, J. Han, F. Wan and D. Zhang, ”Strengthen Learning Tolerance for
    Weakly Supervised Object Localization,” 2021 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp. 7399-7408, doi:
    10.1109/CVPR46437.2021.00732.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] J. Xie, C. Luo, X. Zhu, Z. Jin, W. Lu and L. Shen, ”Online Refinement
    of Low-level Feature Based Activation Map for Weakly Supervised Object Localization,”
    2021 IEEE/CVF International Conference on Computer Vision (ICCV), Montreal, QC,
    Canada, 2021, pp. 132-141, doi: 10.1109/ICCV48922.2021.00020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] J. Xu et al., ”CREAM: Weakly Supervised Object Localization via Class
    RE-Activation Mapping,” 2022 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 9427-9436, doi: 10.1109/CVPR52688.2022.00922.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Z. Min, B. Zhuang, S. Schulter, B. Liu, E. Dunn and M. Chandraker, ”NeurOCS:
    Neural NOCS Supervision for Monocular 3D Object Localization,” 2023 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023,
    pp. 21404-21414, doi: 10.1109/CVPR52729.2023.02050.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] X. Yu et al., ”Object Localization under Single Coarse Point Supervision,”
    2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New
    Orleans, LA, USA, 2022, pp. 4858-4867, doi: 10.1109/CVPR52688.2022.00482.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] V. Gaudillière, L. Pauly, A. Rathinam, A. G. Sanchez, M. A. Musallam
    and D. Aouada, ”3D-Aware Object Localization using Gaussian Implicit Occupancy
    Function,” 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems
    (IROS), Detroit, MI, USA, 2023, pp. 5858-5863, doi: 10.1109/IROS55552.2023.10342399.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Gang Lv, Y. Sun, Fudong Nian, M. Zhu, W. Tang, and Z. Hu, “COME: Clip-OCR
    and Master ObjEct for text image captioning,” Image and Vision Computing, vol.
    136, pp. 104751–104751, Aug. 2023, doi: 10.1016/j.imavis.2023.104751.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] M. R. Gupta, N. P. Jacobson, and E. K. Garcia, “OCR binarization and
    image pre-processing for searching historical documents,” Pattern Recognition,
    vol. 40, no. 2, pp. 389–397, Feb. 2007, doi: 10.1016/j.patcog.2006.04.043.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] I. Bazzi, R. Schwartz and J. Makhoul, ”An omnifont open-vocabulary OCR
    system for English and Arabic,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 21, no. 6, pp. 495-504, June 1999, doi: 10.1109/34.771314.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Jaehwa Park, V. Govindaraju and S. N. Srihari, ”OCR in a hierarchical
    feature space,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 22, no. 4, pp. 400-407, April 2000, doi: 10.1109/34.845383.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] J. Memon, M. Sami, R. A. Khan and M. Uddin, ”Handwritten Optical Character
    Recognition (OCR): A Comprehensive Systematic Literature Review (SLR),” in IEEE
    Access, vol. 8, pp. 142642-142668, 2020, doi: 10.1109/ACCESS.2020.3012542.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] G. Nagy, S. Seth and K. Einspahr, ”Decoding Substitution Ciphers by Means
    of Word Matching with Application to OCR,” in IEEE Transactions on Pattern Analysis
    and Machine Intelligence, vol. PAMI-9, no. 5, pp. 710-715, Sept. 1987, doi: 10.1109/TPAMI.1987.4767969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Agung Yuwono Sugiyono, Kendricko Adrio, K. Tanuwijaya, and Kristien Margi
    Suryaningrum, “Extracting Information from Vehicle Registration Plate using OCR
    Tesseract,” Procedia Computer Science, vol. 227, pp. 932–938, Jan. 2023, doi:
    10.1016/j.procs.2023.10.600.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] C. C. Paglinawan, M. Hannah M. Caliolio and J. B. Frias, ”Medicine Classification
    Using YOLOv4 and Tesseract OCR,” 2023 15th International Conference on Computer
    and Automation Engineering (ICCAE), Sydney, Australia, 2023, pp. 260-263, doi:
    10.1109/ICCAE56788.2023.10111387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] T. Thapliyal, S. Bhatt, V. Rawat and S. Maurya, ”Automatic License Plate
    Recognition (ALPR) using YOLOv5 model and Tesseract OCR engine,” 2023 First International
    Conference on Advances in Electrical, Electronics and Computational Intelligence
    (ICAEECI), Tiruchengode, India, 2023, pp. 1-5, doi: 10.1109/ICAEECI58247.2023.10370919.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] S. K. Ladi, G. K. Panda, R. Dash and P. K. Ladi, ”A Pioneering Approach
    of Hyperspectral Image Classification Employing the Cooperative Efforts of 3D,
    2D and Depthwise Separable-1D Convolutions,” 2022 IEEE 2nd International Symposium
    on Sustainable Energy, Signal Processing and Cyber Security (iSSSC), Gunupur,
    Odisha, India, 2022, pp. 1-6, doi: 10.1109/iSSSC56467.2022.10051566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] K. J. Han, R. Prieto and T. Ma, ”State-of-the-Art Speech Recognition
    Using Multi-Stream Self-Attention with Dilated 1D Convolutions,” 2019 IEEE Automatic
    Speech Recognition and Understanding Workshop (ASRU), Singapore, 2019, pp. 54-61,
    doi: 10.1109/ASRU46091.2019.9003730.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] S. Kiranyaz, T. Ince, O. Abdeljaber, O. Avci and M. Gabbouj, ”1-D Convolutional
    Neural Networks for Signal Processing Applications,” ICASSP 2019 - 2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK,
    2019, pp. 8360-8364, doi: 10.1109/ICASSP.2019.8682194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] W. Huang and G. Liu, ”Hierarchical Text Classification Based on the End-to-End
    MCHA-BERT,” 2021 IEEE 3rd International Conference on Frontiers Technology of
    Information and Computer (ICFTIC), Greenville, SC, USA, 2021, pp. 301-306, doi:
    10.1109/ICFTIC54370.2021.9647279.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] J. Abdelnour, J. Rouat and G. Salvi, ”NAAQA: A Neural Architecture for
    Acoustic Question Answering,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 45, no. 4, pp. 4997-5009, 1 April 2023, doi: 10.1109/TPAMI.2022.3194311.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] J. Rämö and V. Välimäki, ”Optimizing a High-Order Graphic Equalizer for
    Audio Processing,” in IEEE Signal Processing Letters, vol. 21, no. 3, pp. 301-305,
    March 2014, doi: 10.1109/LSP.2014.2301557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Y. Yamazaki, C. Premachandra and C. J. Perea, ”Audio-Processing-Based
    Human Detection at Disaster Sites With Unmanned Aerial Vehicle,” in IEEE Access,
    vol. 8, pp. 101398-101405, 2020, doi: 10.1109/ACCESS.2020.2998776.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] K. Kumar, R. Pandey, S. S. Bhattacharjee and N. V. George, ”Exponential
    Hyperbolic Cosine Robust Adaptive Filters for Audio Signal Processing,” in IEEE
    Signal Processing Letters, vol. 28, pp. 1410-1414, 2021, doi: 10.1109/LSP.2021.3093862.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] D. Comminiello, M. Scarpiniti, R. Parisi and A. Uncini, ”Frequency-domain
    Adaptive Filtering: from Real to Hypercomplex Signal Processing,” ICASSP 2019
    - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP), Brighton, UK, 2019, pp. 7745-7749, doi: 10.1109/ICASSP.2019.8683403.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] W. Fan, K. Chen, J. Lu and J. Tao, ”Effective Improvement of Under-Modeling
    Frequency-Domain Kalman Filter,” in IEEE Signal Processing Letters, vol. 26, no.
    2, pp. 342-346, Feb. 2019, doi: 10.1109/LSP.2019.2890965.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] É. Bavu, A. Ramamonjy, H. Pujol and A. Garcia, ”Timescalenet : A Multiresolution
    Approach for Raw Audio Recognition,” ICASSP 2019 - 2019 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, 2019, pp. 5686-5690,
    doi: 10.1109/ICASSP.2019.8682378.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Ç. Bilen, A. Ozerov and P. Pérez, ”Solving Time-Domain Audio Inverse
    Problems Using Nonnegative Tensor Factorization,” in IEEE Transactions on Signal
    Processing, vol. 66, no. 21, pp. 5604-5617, 1 Nov.1, 2018, doi: 10.1109/TSP.2018.2869113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] W. Cai, L. Xie, W. Yang, Y. Li, Y. Gao and T. Wang, ”DFTNet: Dual-Path
    Feature Transfer Network for Weakly Supervised Medical Image Segmentation,” in
    IEEE/ACM Transactions on Computational Biology and Bioinformatics, vol. 20, no.
    4, pp. 2530-2540, 1 July-Aug. 2023, doi: 10.1109/TCBB.2022.3198284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] X. Dai, T. Ma, H. Cai and Y. Wen, ”Unsupervised Hierarchical Translation-Based
    Model for Multi-Modal Medical Image Registration,” ICASSP 2022 - 2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore,
    2022, pp. 1261-1265, doi: 10.1109/ICASSP43922.2022.9746324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] L. Xie, W. Cai and Y. Gao, ”DMCGNet: A Novel Network for Medical Image
    Segmentation With Dense Self-Mimic and Channel Grouping Mechanism,” in IEEE Journal
    of Biomedical and Health Informatics, vol. 26, no. 10, pp. 5013-5024, Oct. 2022,
    doi: 10.1109/JBHI.2022.3192277.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Z. Gu et al., ”CE-Net: Context Encoder Network for 2D Medical Image Segmentation,”
    in IEEE Transactions on Medical Imaging, vol. 38, no. 10, pp. 2281-2292, Oct.
    2019, doi: 10.1109/TMI.2019.2903562.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] X. Bing, W. Zhang, L. Zheng and Y. Zhang, ”Medical Image Super Resolution
    Using Improved Generative Adversarial Networks,” in IEEE Access, vol. 7, pp. 145030-145038,
    2019, doi: 10.1109/ACCESS.2019.2944862.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] M. Z. Khan, M. K. Gajendran, Y. Lee and M. A. Khan, ”Deep Neural Architectures
    for Medical Image Semantic Segmentation: Review,” in IEEE Access, vol. 9, pp.
    83002-83024, 2021, doi: 10.1109/ACCESS.2021.3086530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] J. Duan, S. Mao, J. Jin, Z. Zhou, L. Chen and C. L. P. Chen, ”A Novel
    GA-Based Optimized Approach for Regional Multimodal Medical Image Fusion With
    Superpixel Segmentation,” in IEEE Access, vol. 9, pp. 96353-96366, 2021, doi:
    10.1109/ACCESS.2021.3094972.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Y. Weng, T. Zhou, Y. Li and X. Qiu, ”NAS-Unet: Neural Architecture Search
    for Medical Image Segmentation,” in IEEE Access, vol. 7, pp. 44247-44257, 2019,
    doi: 10.1109/ACCESS.2019.2908991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] C. You, Y. Zhou, R. Zhao, L. Staib and J. S. Duncan, ”SimCVD: Simple
    Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical
    Image Segmentation,” in IEEE Transactions on Medical Imaging, vol. 41, no. 9,
    pp. 2228-2237, Sept. 2022, doi: 10.1109/TMI.2022.3161829.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] N. Wang et al., ”MISSU: 3D Medical Image Segmentation via Self-Distilling
    TransUNet,” in IEEE Transactions on Medical Imaging, vol. 42, no. 9, pp. 2740-2750,
    Sept. 2023, doi: 10.1109/TMI.2023.3264433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Y. Zhao, K. Lu, J. Xue, S. Wang and J. Lu, ”Semi-Supervised Medical Image
    Segmentation With Voxel Stability and Reliability Constraints,” in IEEE Journal
    of Biomedical and Health Informatics, vol. 27, no. 8, pp. 3912-3923, Aug. 2023,
    doi: 10.1109/JBHI.2023.3273609.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] L. Wang, J. Zhang, Y. Liu, J. Mi and J. Zhang, ”Multimodal Medical Image
    Fusion Based on Gabor Representation Combination of Multi-CNN and Fuzzy Neural
    Network,” in IEEE Access, vol. 9, pp. 67634-67647, 2021, doi: 10.1109/ACCESS.2021.3075953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] N. Hilmizen, A. Bustamam and D. Sarwinda, ”The Multimodal Deep Learning
    for Diagnosing COVID-19 Pneumonia from Chest CT-Scan and X-Ray Images,” 2020 3rd
    International Seminar on Research of Information Technology and Intelligent Systems
    (ISRITI), Yogyakarta, Indonesia, 2020, pp. 26-31, doi: 10.1109/ISRITI51436.2020.9315478.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] T. Anwar and S. Zakir, ”Deep learning based diagnosis of COVID-19 using
    chest CT-scan images,” 2020 IEEE 23rd International Multitopic Conference (INMIC),
    Bahawalpur, Pakistan, 2020, pp. 1-5, doi: 10.1109/INMIC50486.2020.9318212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Y. F. Riti, H. A. Nugroho, S. Wibirama, B. Windarta and L. Choridah,
    ”Feature extraction for lesion margin characteristic classification from CT Scan
    lungs image,” 2016 1st International Conference on Information Technology, Information
    Systems and Electrical Engineering (ICITISEE), Yogyakarta, Indonesia, 2016, pp.
    54-58, doi: 10.1109/ICITISEE.2016.7803047.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] A. Seum, A. H. Raj, S. Sakib and T. Hossain, ”A Comparative Study of
    CNN Transfer Learning Classification Algorithms with Segmentation for COVID-19
    Detection from CT Scan Images,” 2020 11th International Conference on Electrical
    and Computer Engineering (ICECE), Dhaka, Bangladesh, 2020, pp. 234-237, doi: 10.1109/ICECE51571.2020.9393129.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] N. Vani and D. Vinod, ”A Comparative Analysis on Random Forest Algorithm
    Over K-Means for Identifying the Brain Tumor Anomalies Using Novel CT Scan with
    MRI Scan,” 2022 International Conference on Business Analytics for Technology
    and Security (ICBATS), Dubai, United Arab Emirates, 2022, pp. 1-6, doi: 10.1109/ICBATS54253.2022.9759036.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] A. Hoque, A. K. M. A. Farabi, F. Ahmed and M. Z. Islam, ”Automated Detection
    of Lung Cancer Using CT Scan Images,” 2020 IEEE Region 10 Symposium (TENSYMP),
    Dhaka, Bangladesh, 2020, pp. 1030-1033, doi: 10.1109/TENSYMP50017.2020.9230861.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] W. Cao et al., ”CNN-based intelligent safety surveillance in green IoT
    applications,” in China Communications, vol. 18, no. 1, pp. 108-119, Jan. 2021,
    doi: 10.23919/JCC.2021.01.010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] J. Xu, W. Zhou, Z. Fu, H. Zhang, and L. Li, “A Survey on Green Deep Learning,”
    arXiv (Cornell University), Nov. 2021, doi: 10.48550/arxiv.2111.05193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Y. Rao, Z. Liu, W. Zhao, J. Zhou and J. Lu, ”Dynamic Spatial Sparsification
    for Efficient Vision Transformers and Convolutional Neural Networks,” in IEEE
    Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp.
    10883-10897, 1 Sept. 2023, doi: 10.1109/TPAMI.2023.3263826.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Z. Li, M. Chen, J. Xiao and Q. Gu, ”PSAQ-ViT V2: Toward Accurate and
    General Data-Free Quantization for Vision Transformers,” in IEEE Transactions
    on Neural Networks and Learning Systems, doi: 10.1109/TNNLS.2023.3301007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] R. Garcia-Martin and R. Sanchez-Reillo, ”Vision Transformers for Vein
    Biometric Recognition,” in IEEE Access, vol. 11, pp. 22060-22080, 2023, doi: 10.1109/ACCESS.2023.3252009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] K. L. Ong, C. P. Lee, H. S. Lim, K. M. Lim and A. Alqahtani, ”Mel-MViTv2:
    Enhanced Speech Emotion Recognition With Mel Spectrogram and Improved Multiscale
    Vision Transformers,” in IEEE Access, vol. 11, pp. 108571-108579, 2023, doi: 10.1109/ACCESS.2023.3321122.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] L. Meng et al., ”AdaViT: Adaptive Vision Transformers for Efficient Image
    Recognition,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), New Orleans, LA, USA, 2022, pp. 12299-12308, doi: 10.1109/CVPR52688.2022.01199.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] J. Liu, X. Huang, J. Zheng, Y. Liu and H. Li, ”MixMAE: Mixed and Masked
    Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers,” 2023
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver,
    BC, Canada, 2023, pp. 6252-6261, doi: 10.1109/CVPR52729.2023.00605.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] J. -N. Chen, S. Sun, J. He, P. Torr, A. Yuille and S. Bai, ”TransMix:
    Attend to Mix for Vision Transformers,” 2022 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 12125-12134, doi:
    10.1109/CVPR52688.2022.01182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] J. -N. Chen, S. Sun, J. He, P. Torr, A. Yuille and S. Bai, ”TransMix:
    Attend to Mix for Vision Transformers,” 2022 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 12125-12134, doi:
    10.1109/CVPR52688.2022.01182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] Y. Tang et al., ”Patch Slimming for Efficient Vision Transformers,” 2022
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans,
    LA, USA, 2022, pp. 12155-12164, doi: 10.1109/CVPR52688.2022.01185.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] A. Hatamizadeh et al., ”GradViT: Gradient Inversion of Vision Transformers,”
    2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New
    Orleans, LA, USA, 2022, pp. 10011-10020, doi: 10.1109/CVPR52688.2022.00978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Y. He et al., ”BiViT: Extremely Compressed Binary Vision Transformers,”
    2023 IEEE/CVF International Conference on Computer Vision (ICCV), Paris, France,
    2023, pp. 5628-5640, doi: 10.1109/ICCV51070.2023.00520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Y. Li et al., ”Rethinking Vision Transformers for MobileNet Size and
    Speed,” 2023 IEEE/CVF International Conference on Computer Vision (ICCV), Paris,
    France, 2023, pp. 16843-16854, doi: 10.1109/ICCV51070.2023.01549.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] T. Sun, C. Zhang, Y. Ji and Z. Hu, ”MSnet: Multi-Head Self-Attention
    Network for Distantly Supervised Relation Extraction,” in IEEE Access, vol. 7,
    pp. 54472-54482, 2019, doi: 10.1109/ACCESS.2019.2913316.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] Z. Xie, G. Zheng, L. Miao and W. Huang, ”STGL-GCN: Spatial–Temporal Mixing
    of Global and Local Self-Attention Graph Convolutional Networks for Human Action
    Recognition,” in IEEE Access, vol. 11, pp. 16526-16532, 2023, doi: 10.1109/ACCESS.2023.3246127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] C. -X. Zhang, Y. -L. Zhang and X. -Y. Gao, ”Multi-Head Self-Attention
    Gated-Dilated Convolutional Neural Network for Word Sense Disambiguation,” in
    IEEE Access, vol. 11, pp. 14202-14210, 2023, doi: 10.1109/ACCESS.2023.3243574.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] K. Zhao, W. Guo, F. Qin and X. Wang, ”D3-SACNN: DGA Domain Detection
    With Self-Attention Convolutional Network,” in IEEE Access, vol. 10, pp. 69250-69263,
    2022, doi: 10.1109/ACCESS.2021.3127913.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] F. Zhang, A. Panahi and G. Gao, ”FsaNet: Frequency Self-Attention for
    Semantic Segmentation,” in IEEE Transactions on Image Processing, vol. 32, pp.
    4757-4772, 2023, doi: 10.1109/TIP.2023.3305090.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] B. Kelenyi and L. Tamas, ”D3GATTEN: Dense 3D Geometric Features Extraction
    and Pose Estimation Using Self-Attention,” in IEEE Access, vol. 11, pp. 7947-7958,
    2023, doi: 10.1109/ACCESS.2023.3238901.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] Y. Wang, Y. Xie, X. Ji, Z. Liu and X. Liu, ”RacPixGAN: An Enhanced Sketch-to-Face
    Synthesis GAN Based on Residual modules, Multi-Head Self-Attention Mechanisms,
    and CLIP Loss,” 2023 4th International Conference on Electronic Communication
    and Artificial Intelligence (ICECAI), Guangzhou, China, 2023, pp. 336-342, doi:
    10.1109/ICECAI58670.2023.10176715.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] C. Yang et al., ”Lite Vision Transformer with Enhanced Self-Attention,”
    2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New
    Orleans, LA, USA, 2022, pp. 11988-11998, doi: 10.1109/CVPR52688.2022.01169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] S. Park and Y. S. Choi, ”Image Super-Resolution Using Dilated Window
    Transformer,” in IEEE Access, vol. 11, pp. 60028-60039, 2023, doi: 10.1109/ACCESS.2023.3284539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] F. Wang, X. Wang, D. Lv, L. Zhou and G. Shi, ”Separable Self-Attention
    Mechanism for Point Cloud Local and Global Feature Modeling,” in IEEE Access,
    vol. 10, pp. 129823-129831, 2022, doi: 10.1109/ACCESS.2022.3228044.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] S. Lamba, A. Baliyan and V. Kukreja, ”GAN based image augmentation for
    increased CNN performance in Paddy leaf disease classification,” 2022 2nd International
    Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),
    Greater Noida, India, 2022, pp. 2054-2059, doi: 10.1109/ICACITE53722.2022.9823799.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] B. Sandhiya, R. Priyatharshini, B. Ramya, S. Monish and G. R. Sai Raja,
    ”Reconstruction, Identification and Classification of Brain Tumor Using Gan and
    Faster Regional-CNN,” 2021 3rd International Conference on Signal Processing and
    Communication (ICPSC), Coimbatore, India, 2021, pp. 238-242, doi: 10.1109/ICSPC51351.2021.9451747.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] N. Sasipriyaa, P. Natesan, R. S. Mohana, P. Arvindkumar, R. S. Arwin
    Prakadis and K. Aswin Surya, ”Recognizing Handwritten Offline Tamil Character
    using VAE-GAN & CNN,” 2023 International Conference on Computer Communication
    and Informatics (ICCCI), Coimbatore, India, 2023, pp. 1-7, doi: 10.1109/ICCCI56745.2023.10128520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] S. K. Singh, M. H. Anisi, S. Clough, T. Blyth and D. Jarchi, ”CNN-BiLSTM
    based GAN for Anamoly Detection from Multivariate Time Series Data,” 2023 24th
    International Conference on Digital Signal Processing (DSP), Rhodes (Rodos), Greece,
    2023, pp. 1-4, doi: 10.1109/DSP58604.2023.10167937.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] Y. Fu, T. Sun, X. Jiang, K. Xu and P. He, ”Robust GAN-Face Detection
    Based on Dual-Channel CNN Network,” 2019 12th International Congress on Image
    and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), Suzhou,
    China, 2019, pp. 1-5, doi: 10.1109/CISP-BMEI48845.2019.8965991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] A. Akram, N. Wang, X. Gao and J. Li, ”Integrating GAN with CNN for Face
    Sketch Synthesis,” 2018 IEEE 4th International Conference on Computer and Communications
    (ICCC), Chengdu, China, 2018, pp. 1483-1487, doi: 10.1109/CompComm.2018.8780648.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] C. -H. Rhee and C. H. Lee, ”Estimating Physically-Based Reflectance Parameters
    From a Single Image With GAN-Guided CNN,” in IEEE Access, vol. 10, pp. 13259-13269,
    2022, doi: 10.1109/ACCESS.2022.3147483.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] C. Mao, L. Huang, Y. Xiao, F. He and Y. Liu, ”Target Recognition of SAR
    Image Based on CN-GAN and CNN in Complex Environment,” in IEEE Access, vol. 9,
    pp. 39608-39617, 2021, doi: 10.1109/ACCESS.2021.3064362.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] J. Wang, M. Xu, X. Deng, L. Shen and Y. Song, ”MW-GAN+ for Perceptual
    Quality Enhancement on Compressed Video,” in IEEE Transactions on Circuits and
    Systems for Video Technology, vol. 32, no. 7, pp. 4224-4237, July 2022, doi: 10.1109/TCSVT.2021.3128275.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] L. Wei, S. Zhang, W. Gao and Q. Tian, ”Person Transfer GAN to Bridge
    Domain Gap for Person Re-identification,” 2018 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, Salt Lake City, UT, USA, 2018, pp. 79-88, doi:
    10.1109/CVPR.2018.00016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] F. Chollet, ”Xception: Deep Learning with Depthwise Separable Convolutions,”
    2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu,
    HI, USA, 2017, pp. 1800-1807, doi: 10.1109/CVPR.2017.195.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] S. Ma, W. Liu, W. Cai, Z. Shang and G. Liu, ”Lightweight Deep Residual
    CNN for Fault Diagnosis of Rotating Machinery Based on Depthwise Separable Convolutions,”
    in IEEE Access, vol. 7, pp. 57023-57036, 2019, doi: 10.1109/ACCESS.2019.2912072.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] D. Haase and M. Amthor, ”Rethinking Depthwise Separable Convolutions:
    How Intra-Kernel Correlations Lead to Improved MobileNets,” in 2020 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp.
    14588-14597, doi: 10.1109/CVPR42600.2020.01461.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] A. Batool and Y. -C. Byun, ”Lightweight EfficientNetB3 Model Based on
    Depthwise Separable Convolutions for Enhancing Classification of Leukemia White
    Blood Cell Images,” in IEEE Access, vol. 11, pp. 37203-37215, 2023, doi: 10.1109/ACCESS.2023.3266511.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] N. A. Mohamed, M. A. Zulkifley and S. R. Abdani, ”Spatial Pyramid Pooling
    with Atrous Convolutional for MobileNet,” 2020 IEEE Student Conference on Research
    and Development (SCOReD), Batu Pahat, Malaysia, 2020, pp. 333-336, doi: 10.1109/SCOReD50371.2020.9250928.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] S. Sriram, R. Vinayakumar, V. Sowmya, M. Alazab and K. P. Soman, ”Multi-scale
    Learning based Malware Variant Detection using Spatial Pyramid Pooling Network,”
    IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM
    WKSHPS), Toronto, ON, Canada, 2020, pp. 740-745, doi: 10.1109/INFOCOMWKSHPS50562.2020.9162661.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] K. He, X. Zhang, S. Ren and J. Sun, ”Spatial Pyramid Pooling in Deep
    Convolutional Networks for Visual Recognition,” in IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 37, no. 9, pp. 1904-1916, 1 Sept. 2015,
    doi: 10.1109/TPAMI.2015.2389824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] E. Prasetyo, N. Suciati and C. Fatichah, ”Yolov4-tiny and Spatial Pyramid
    Pooling for Detecting Head and Tail of Fish,” 2021 International Conference on
    Artificial Intelligence and Computer Science Technology (ICAICST), Yogyakarta,
    Indonesia, 2021, pp. 157-161, doi: 10.1109/ICAICST53116.2021.9497822.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] Y. Tian, F. Chen, H. Wang and S. Zhang, ”Real-Time Semantic Segmentation
    Network Based on Lite Reduced Atrous Spatial Pyramid Pooling Module Group,” 2020
    5th International Conference on Control, Robotics and Cybernetics (CRC), Wuhan,
    China, 2020, pp. 139-143, doi: 10.1109/CRC51253.2020.9253492.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] A. Qayyum, I. Ahmad, W. Mumtaz, M. O. Alassafi, R. Alghamdi and M. Mazher,
    ”Automatic Segmentation Using a Hybrid Dense Network Integrated With an 3D-Atrous
    Spatial Pyramid Pooling Module for Computed Tomography (CT) Imaging,” in IEEE
    Access, vol. 8, pp. 169794-169803, 2020, doi: 10.1109/ACCESS.2020.3024277.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] Chi-Man Pun and Moon-Chuen Lee, ”Extraction of shift invariant wavelet
    features for classification of images with different sizes,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 26, no. 9, pp. 1228-1233, Sept.
    2004, doi: 10.1109/TPAMI.2004.67.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] L. Liang and H. Liu, ”Dual-Tree Cosine-Modulated Filter Bank With Linear-Phase
    Individual Filters: An Alternative Shift-Invariant and Directional-Selective Transform,”
    in IEEE Transactions on Image Processing, vol. 22, no. 12, pp. 5168-5180, Dec.
    2013, doi: 10.1109/TIP.2013.2283146.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] L. -D. Kuang, Q. -H. Lin, X. -F. Gong, F. Cong, Y. -P. Wang and V. D.
    Calhoun, ”Shift-Invariant Canonical Polyadic Decomposition of Complex-Valued Multi-Subject
    fMRI Data With a Phase Sparsity Constraint,” in IEEE Transactions on Medical Imaging,
    vol. 39, no. 4, pp. 844-853, April 2020, doi: 10.1109/TMI.2019.2936046.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] G. Papari, P. Campisi and N. Petkov, ”New Families of Fourier Eigenfunctions
    for Steerable Filtering,” in IEEE Transactions on Image Processing, vol. 21, no.
    6, pp. 2931-2943, June 2012, doi: 10.1109/TIP.2011.2179060.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] T. Zhao and T. Blu, ”The Fourier-Argand Representation: An Optimal Basis
    of Steerable Patterns,” in IEEE Transactions on Image Processing, vol. 29, pp.
    6357-6371, 2020, doi: 10.1109/TIP.2020.2990483.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] A. Depeursinge, Z. Püspöki, J. P. Ward and M. Unser, ”Steerable Wavelet
    Machines (SWM): Learning Moving Frames for Texture Classification,” in IEEE Transactions
    on Image Processing, vol. 26, no. 4, pp. 1626-1636, April 2017, doi: 10.1109/TIP.2017.2655438.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] A. M. Alhassan and W. M. N. W. Zainon, ”BAT Algorithm With fuzzy C-Ordered
    Means (BAFCOM) Clustering Segmentation and Enhanced Capsule Networks (ECN) for
    Brain Cancer MRI Images Classification,” in IEEE Access, vol. 8, pp. 201741-201751,
    2020, doi: 10.1109/ACCESS.2020.3035803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] P. Haridas, G. Chennupati, N. Santhi, P. Romero and S. Eidenbenz, ”Code
    Characterization With Graph Convolutions and Capsule Networks,” in IEEE Access,
    vol. 8, pp. 136307-136315, 2020, doi: 10.1109/ACCESS.2020.3011909.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] B. Kakillioglu, A. Ren, Y. Wang and S. Velipasalar, ”3D Capsule Networks
    for Object Classification With Weight Pruning,” in IEEE Access, vol. 8, pp. 27393-27405,
    2020, doi: 10.1109/ACCESS.2020.2971950.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] A. Marchisio, V. Mrazek, A. Massa, B. Bussolino, M. Martina and M. Shafique,
    ”RoHNAS: A Neural Architecture Search Framework With Conjoint Optimization for
    Adversarial Robustness and Hardware Efficiency of Convolutional and Capsule Networks,”
    in IEEE Access, vol. 10, pp. 109043-109055, 2022, doi: 10.1109/ACCESS.2022.3214312.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] Y. Zhao, T. Birdal, H. Deng and F. Tombari, ”3D Point Capsule Networks,”
    2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long
    Beach, CA, USA, 2019, pp. 1009-1018, doi: 10.1109/CVPR.2019.00110.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] M. -H. Ha and O. T. -C. Chen, ”Deep Neural Networks Using Capsule Networks
    and Skeleton-Based Attentions for Action Recognition,” in IEEE Access, vol. 9,
    pp. 6164-6178, 2021, doi: 10.1109/ACCESS.2020.3048741.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] J. Rajasegaran, V. Jayasundara, S. Jayasekara, H. Jayasekara, S. Seneviratne
    and R. Rodrigo, ”DeepCaps: Going Deeper With Capsule Networks,” 2019 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA,
    USA, 2019, pp. 10717-10725, doi: 10.1109/CVPR.2019.01098.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] Z. Yan, X. Dai, P. Zhang, Y. Tian, B. Wu and M. Feiszli, ”FP-NAS: Fast
    Probabilistic Neural Architecture Search,” 2021 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp. 15134-15143,
    doi: 10.1109/CVPR46437.2021.01489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] M. Zhang, H. Li, S. Pan, X. Chang, and S. Su, ”Overcoming Multi-Model
    Forgetting in One-Shot NAS With Diversity Maximization,” 2020 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp.
    7806-7815, doi: 10.1109/CVPR42600.2020.00783.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] X. Li et al., ”Improving One-Shot NAS by Suppressing the Posterior Fading,”
    2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle,
    WA, USA, 2020, pp. 13833-13842, doi: 10.1109/CVPR42600.2020.01385.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] Z. Li, T. Xi, J. Deng, G. Zhang, S. Wen and R. He, ”GP-NAS: Gaussian
    Process Based Neural Architecture Search,” 2020 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp. 11930-11939,
    doi: 10.1109/CVPR42600.2020.01195.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] C. Jiang, H. Xu, W. Zhang, X. Liang and Z. Li, ”SP-NAS: Serial-to-Parallel
    Backbone Search for Object Detection,” 2020 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp. 11860-11869, doi:
    10.1109/CVPR42600.2020.01188.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] Y. Gao, H. Bai, Z. Jie, J. Ma, K. Jia and W. Liu, ”MTL-NAS: Task-Agnostic
    Neural Architecture Search Towards General-Purpose Multi-Task Learning,” 2020
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle,
    WA, USA, 2020, pp. 11540-11549, doi: 10.1109/CVPR42600.2020.01156.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] Y. Liu, Y. Sun, B. Xue, M. Zhang, G. G. Yen and K. C. Tan, ”A Survey
    on Evolutionary Neural Architecture Search,” in IEEE Transactions on Neural Networks
    and Learning Systems, vol. 34, no. 2, pp. 550-570, Feb. 2023, doi: 10.1109/TNNLS.2021.3100554.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] K. T. Chitty-Venkata, M. Emani, V. Vishwanath and A. K. Somani, ”Neural
    Architecture Search Benchmarks: Insights and Survey,” in IEEE Access, vol. 11,
    pp. 25217-25236, 2023, doi: 10.1109/ACCESS.2023.3253818.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] Y. Weng, T. Zhou, L. Liu and C. Xia, ”Automatic Convolutional Neural
    Architecture Search for Image Classification Under Different Scenes,” in IEEE
    Access, vol. 7, pp. 38495-38506, 2019, doi: 10.1109/ACCESS.2019.2906369.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] X. Zheng et al., ”MIGO-NAS: Towards Fast and Generalizable Neural Architecture
    Search,” in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
    43, no. 9, pp. 2936-2952, 1 Sept. 2021, doi: 10.1109/TPAMI.2021.3065138.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] C. Wei, C. Niu, Y. Tang, Y. Wang, H. Hu and J. Liang, ”NPENAS: Neural
    Predictor Guided Evolution for Neural Architecture Search,” in IEEE Transactions
    on Neural Networks and Learning Systems, vol. 34, no. 11, pp. 8441-8455, Nov.
    2023, doi: 10.1109/TNNLS.2022.3151160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] Y. Li, S. Tang, R. Zhang, Y. Zhang, J. Li and S. Yan, ”Asymmetric GAN
    for Unpaired Image-to-Image Translation,” in IEEE Transactions on Image Processing,
    vol. 28, no. 12, pp. 5881-5896, Dec. 2019, doi: 10.1109/TIP.2019.2922854.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] C. D. Prakash and L. J. Karam, ”It GAN Do Better: GAN-Based Detection
    of Objects on Images With Varying Quality,” in IEEE Transactions on Image Processing,
    vol. 30, pp. 9220-9230, 2021, doi: 10.1109/TIP.2021.3124155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] Y. Huang, F. Zheng, R. Cong, W. Huang, M. R. Scott and L. Shao, ”MCMT-GAN:
    Multi-Task Coherent Modality Transferable GAN for 3D Brain Image Synthesis,” in
    IEEE Transactions on Image Processing, vol. 29, pp. 8187-8198, 2020, doi: 10.1109/TIP.2020.3011557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] T. Chen, Y. Zhang, X. Huo, S. Wu, Y. Xu and H. S. Wong, ”SphericGAN:
    Semi-supervised Hyper-spherical Generative Adversarial Networks for Fine-grained
    Image Synthesis,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), New Orleans, LA, USA, 2022, pp. 9991-10000, doi: 10.1109/CVPR52688.2022.00976.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] Z. Liu et al., ”Fine-Grained Face Swapping Via Regional GAN Inversion,”
    2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver,
    BC, Canada, 2023, pp. 8578-8587, doi: 10.1109/CVPR52729.2023.00829.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] Y. Dong, W. Tan, D. Tao, L. Zheng, and X. Li, ”CartoonLossGAN: Learning
    Surface and Coloring of Images for Cartoonization,” in IEEE Transactions on Image
    Processing, vol. 31, pp. 485-498, 2022, doi: 10.1109/TIP.2021.3130539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] Z. Pan, W. Yu, X. Yi, A. Khan, F. Yuan and Y. Zheng, ”Recent Progress
    on Generative Adversarial Networks (GANs): A Survey,” in IEEE Access, vol. 7,
    pp. 36322-36333, 2019, doi: 10.1109/ACCESS.2019.2905015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-Time
    Object Detection with Region Proposal Networks,” arXiv (Cornell University), Jun.
    2015, doi: 10.48550/arxiv.1506.01497.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] R. Joseph, Divvala Santosh, G. Ross, and F. Ali, “You Only Look Once:
    Unified, Real-Time Object Detection,” arXiv (Cornell University), Jan. 2016, doi:
    10.48550/arxiv.1506.02640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] J. Terven, D.-M. Córdova-Esparza, and J.-A. Romero-González, “A Comprehensive
    Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS,”
    Machine Learning and Knowledge Extraction, vol. 5, no. 4, pp. 1680–1716, Dec.
    2023, doi: 10.3390/make5040083.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Y. Lu, L. Zhang and W. Xie, ”YOLO-compact: An Efficient YOLO Network
    for Single Category Real-time Object Detection,” 2020 Chinese Control And Decision
    Conference (CCDC), Hefei, China, 2020, pp. 1931-1936, doi: 10.1109/CCDC49329.2020.9164580.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] T. -H. Wu, T. -W. Wang and Y. -Q. Liu, ”Real-Time Vehicle and Distance
    Detection Based on Improved Yolo v5 Network,” 2021 3rd World Symposium on Artificial
    Intelligence (WSAI), Guangzhou, China, 2021, pp. 24-28, doi: 10.1109/WSAI51899.2021.9486316.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] H. Yu, Y. Li and D. Zhang, ”An Improved YOLO v3 Small-Scale Ship Target
    Detection Algorithm,” 2021 6th International Conference on Smart Grid and Electrical
    Automation (ICSGEA), Kunming, China, 2021, pp. 560-563, doi: 10.1109/ICSGEA53208.2021.00132.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] W. Lan, J. Dang, Y. Wang and S. Wang, ”Pedestrian Detection Based on
    YOLO Network Model,” 2018 IEEE International Conference on Mechatronics and Automation
    (ICMA), Changchun, China, 2018, pp. 1547-1551, doi: 10.1109/ICMA.2018.8484698.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] J. -H. Kim, N. Kim and C. S. Won, ”High-Speed Drone Detection Based On
    Yolo-V8,” ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-2, doi: 10.1109/ICASSP49357.2023.10095516.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] Z. Li, Z. Liu and X. Wang, ”On-Board Real-Time Pedestrian Detection for
    Micro Unmanned Aerial Vehicles Based on YOLO-v8,” 2023 2nd International Conference
    on Machine Learning, Cloud Computing and Intelligent Mining (MLCCIM), Jiuzhaigou,
    China, 2023, pp. 250-255, doi: 10.1109/MLCCIM60412.2023.00042.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] N. Madhasu and S. D. Pande, ”Chrometect GAYO: Classification and Colorization
    using PIX2PIX and YOLOV8,” 2023 7th International Conference on Computation System
    and Information Technology for Sustainable Solutions (CSITSS), Bangalore, India,
    2023, pp. 1-6, doi: 10.1109/CSITSS60515.2023.10384657.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] Y. Wang, H. Wang and Z. Xin, ”Efficient Detection Model of Steel Strip
    Surface Defects Based on YOLO-V7,” in IEEE Access, vol. 10, pp. 133936-133944,
    2022, doi: 10.1109/ACCESS.2022.3230894.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] F. Fang, L. Li, H. Zhu and J. -H. Lim, ”Combining Faster R-CNN and Model-Driven
    Clustering for Elongated Object Detection,” in IEEE Transactions on Image Processing,
    vol. 29, pp. 2052-2065, 2020, doi: 10.1109/TIP.2019.2947792.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] X. Chen et al., ”Fast and Accurate Craniomaxillofacial Landmark Detection
    via 3D Faster R-CNN,” in IEEE Transactions on Medical Imaging, vol. 40, no. 12,
    pp. 3867-3878, Dec. 2021, doi: 10.1109/TMI.2021.3099509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] S. -L. Chen et al., ”Detection of Various Dental Conditions on Dental
    Panoramic Radiography Using Faster R-CNN,” in IEEE Access, vol. 11, pp. 127388-127401,
    2023, doi: 10.1109/ACCESS.2023.3332269.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] F. Selamet, S. Cakar and M. Kotan, ”Automatic Detection and Classification
    of Defective Areas on Metal Parts by Using Adaptive Fusion of Faster R-CNN and
    Shape From Shading,” in IEEE Access, vol. 10, pp. 126030-126038, 2022, doi: 10.1109/ACCESS.2022.3224037.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] A. Omid-Zohoor, C. Young, D. Ta and B. Murmann, ”Toward Always-On Mobile
    Object Detection: Energy Versus Performance Tradeoffs for Embedded HOG Feature
    Extraction,” in IEEE Transactions on Circuits and Systems for Video Technology,
    vol. 28, no. 5, pp. 1102-1115, May 2018, doi: 10.1109/TCSVT.2017.2653187.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] Y. Guo and B. Yang, ”A Survey of Semantic Segmentation Methods in Traffic
    Scenarios,” 2022 International Conference on Machine Learning, Cloud Computing
    and Intelligent Mining (MLCCIM), Xiamen, China, 2022, pp. 452-457, doi: 10.1109/MLCCIM55934.2022.00083.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] X. Xu, S. Huang and H. Lai, ”Lightweight Semantic Segmentation Network
    Leveraging Class-Aware Contextual Information,” in IEEE Access, vol. 11, pp. 144722-144734,
    2023, doi: 10.1109/ACCESS.2023.3345790.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] S. Abdigapporov, S. Miraliev, V. Kakani and H. Kim, ”Joint Multiclass
    Object Detection and Semantic Segmentation for Autonomous Driving,” in IEEE Access,
    vol. 11, pp. 37637-37649, 2023, doi: 10.1109/ACCESS.2023.3266284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] Y. Zheng, Y. Xu, S. Qiu, W. Li, G. Zhong and M. Sarem, ”A Novel Semantic
    Segmentation Algorithm for RGB-D Images Based on Non-Symmetry and Anti-Packing
    Pattern Representation Model,” in IEEE Access, vol. 11, pp. 36290-36299, 2023,
    doi: 10.1109/ACCESS.2023.3266251.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] Z. Cao et al., ”Meta-Seg: A Generalized Meta-Learning Framework for Multi-Class
    Few-Shot Semantic Segmentation,” in IEEE Access, vol. 7, pp. 166109-166121, 2019,
    doi: 10.1109/ACCESS.2019.2953465.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] F. S. Saleh, M. S. Aliakbarian, M. Salzmann, L. Petersson, J. M. Alvarez
    and S. Gould, ”Incorporating Network Built-in Priors in Weakly-Supervised Semantic
    Segmentation,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 40, no. 6, pp. 1382-1396, 1 June 2018, doi: 10.1109/TPAMI.2017.2713785.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] P. Anilkumar et al., ”An Adaptive DeepLabv3+ for Semantic Segmentation
    of Aerial Images Using Improved Golden Eagle Optimization Algorithm,” in IEEE
    Access, vol. 11, pp. 106688-106705, 2023, doi: 10.1109/ACCESS.2023.3318867.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] W. Zhou, J. Liu, J. Lei, L. Yu and J. -N. Hwang, ”GMNet: Graded-Feature
    Multilabel-Learning Network for RGB-Thermal Urban Scene Semantic Segmentation,”
    in IEEE Transactions on Image Processing, vol. 30, pp. 7790-7802, 2021, doi: 10.1109/TIP.2021.3109518.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] A. Sohail, N. A. Nawaz, A. A. Shah, S. Rasheed, S. Ilyas and M. K. Ehsan,
    ”A Systematic Literature Review on Machine Learning and Deep Learning Methods
    for Semantic Segmentation,” in IEEE Access, vol. 10, pp. 134557-134570, 2022,
    doi: 10.1109/ACCESS.2022.3230983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] W. Ji et al., ”Multispectral Video Semantic Segmentation: A Benchmark
    Dataset and Baseline,” 2023 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 1094-1104, doi: 10.1109/CVPR52729.2023.00112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] J. Zhang, X. Zhao, Z. Chen and Z. Lu, ”A Review of Deep Learning-Based
    Semantic Segmentation for Point Cloud,” in IEEE Access, vol. 7, pp. 179118-179133,
    2019, doi: 10.1109/ACCESS.2019.2958671.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] B. Woo and M. Lee, ”Comparison of tissue segmentation performance between
    2D U-Net and 3D U-Net on brain MR Images,” 2021 International Conference on Electronics,
    Information, and Communication (ICEIC), Jeju, Korea (South), 2021, pp. 1-4, doi:
    10.1109/ICEIC51217.2021.9369797.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] P. Harsh, R. Chakraborty, S. Tripathi and K. Sharma, ”Attention U-Net
    Architecture for Dental Image Segmentation,” 2021 International Conference on
    Intelligent Technologies (CONIT), Hubli, India, 2021, pp. 1-5, doi: 10.1109/CONIT51480.2021.9498422.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[346] G. B. Kande et al., ”MSR U-Net: An Improved U-Net Model for Retinal Blood
    Vessel Segmentation,” in IEEE Access, vol. 12, pp. 534-551, 2024, doi: 10.1109/ACCESS.2023.3347196.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] N. Siddique, S. Paheding, C. P. Elkin and V. Devabhaktuni, ”U-Net and
    Its Variants for Medical Image Segmentation: A Review of Theory and Applications,”
    in IEEE Access, vol. 9, pp. 82031-82057, 2021, doi: 10.1109/ACCESS.2021.3086020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] Y. -J. Huang et al., ”HL-FCN: Hybrid loss guided FCN for colorectal cancer
    segmentation,” 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI
    2018), Washington, DC, USA, 2018, pp. 195-198, doi: 10.1109/ISBI.2018.8363553.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid Scene Parsing Network,”
    arXiv.org, Apr. 27, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] H. Tang, ”Vision Question Answering System Based on Roberta and Vit Model,”
    2022 International Conference on Image Processing, Computer Vision and Machine
    Learning (ICICML), Xi’an, China, 2022, pp. 258-261, doi: 10.1109/ICICML57342.2022.10009711.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] F. Bao et al., ”All are Worth Words: A ViT Backbone for Diffusion Models,”
    2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver,
    BC, Canada, 2023, pp. 22669-22679, doi: 10.1109/CVPR52729.2023.02171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] Z. Li and Q. Gu, ”I-ViT: Integer-only Quantization for Efficient Vision
    Transformer Inference,” 2023 IEEE/CVF International Conference on Computer Vision
    (ICCV), Paris, France, 2023, pp. 17019-17029, doi: 10.1109/ICCV51070.2023.01565.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] H. Dong, C. Chen, J. Wang, F. Shen and Y. Pang, ”ViT-SAPS: Detail-Aware
    Transformer for Mechanical Assembly Semantic Segmentation,” in IEEE Access, vol.
    11, pp. 41467-41479, 2023, doi: 10.1109/ACCESS.2023.3270807.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] L. Zou, Z. Huang, N. Gu and G. Wang, ”6D-ViT: Category-Level 6D Object
    Pose Estimation via Transformer-Based Instance Representation Learning,” in IEEE
    Transactions on Image Processing, vol. 31, pp. 6907-6921, 2022, doi: 10.1109/TIP.2022.3216980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] H. Bao, L. Dong, and F. Wei, “BEiT: BERT Pre-Training of Image Transformers,”
    arXiv (Cornell University), Jun. 2021, doi: 10.48550/arxiv.2106.08254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, “Transformer in Transformer,”
    arXiv (Cornell University), Feb. 2021, doi: 10.48550/arxiv.2103.00112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] S. Mehta and M. Rastegari, “MobileViT: Light-weight, General-purpose,
    and Mobile-friendly Vision Transformer,” arXiv (Cornell University), Oct. 2021,
    doi: 10.48550/arxiv.2110.02178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] Z. Hu, Z. Gan, W. Li, J. Z. Wen, D. Zhou and X. Wang, ”Two-Stage Model-Agnostic
    Meta-Learning With Noise Mechanism for One-Shot Imitation,” in IEEE Access, vol.
    8, pp. 182720-182730, 2020, doi: 10.1109/ACCESS.2020.3029220.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] L. Xie, Y. Yang, Z. Fu and S. M. Naqvi, ”One-Shot Medical Action Recognition
    With A Cross-Attention Mechanism And Dynamic Time Warping,” ICASSP 2023 - 2023
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10097186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] C. Huang, Y. Dang, P. Chen, X. Yang and K. -T. Cheng, ”One-Shot Imitation
    Drone Filming of Human Motion Videos,” in IEEE Transactions on Pattern Analysis
    and Machine Intelligence, vol. 44, no. 9, pp. 5335-5348, 1 Sept. 2022, doi: 10.1109/TPAMI.2021.3067359.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] S. K. Biswas and P. Milanfar, ”One Shot Detection with Laplacian Object
    and Fast Matrix Cosine Similarity,” in IEEE Transactions on Pattern Analysis and
    Machine Intelligence, vol. 38, no. 3, pp. 546-562, 1 March 2016, doi: 10.1109/TPAMI.2015.2453950.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[362] S. -. Y. Huang and W. -. Ta Chu, ”Searching by Generating: Flexible and
    Efficient One-Shot NAS with Architecture Generator,” 2021 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp.
    983-992, doi: 10.1109/CVPR46437.2021.00104.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[363] Y. Chen, T. Huang, Y. Niu, X. Ke and Y. Lin, ”Pose-Guided Spatial Alignment
    and Key Frame Selection for One-Shot Video-Based Person Re-Identification,” in
    IEEE Access, vol. 7, pp. 78991-79004, 2019, doi: 10.1109/ACCESS.2019.2922679.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[364] S. K. Roy, P. Kar, M. E. Paoletti, J. M. Haut, R. Pastor-Vargas and A.
    Robles-Gómez, ”SiCoDeF² Net: Siamese Convolution Deconvolution Feature Fusion
    Network for One-Shot Classification,” in IEEE Access, vol. 9, pp. 118419-118434,
    2021, doi: 10.1109/ACCESS.2021.3107626.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] Y. Song, T. Wang, Subrota Kumar Mondal, and Jyoti Prakash Sahoo, “A Comprehensive
    Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities,”
    arXiv (Cornell University), May 2022, doi: 10.48550/arxiv.2205.06743.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] Y. Wang, Q. Yao, J. Kwok, and L. M. Ni, “Generalizing from a Few Examples:
    A Survey on Few-Shot Learning,” arXiv.org, Mar. 29, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] W. Wang, V. W. Zheng, H. Yu, and C. Miao, “A Survey of Zero-Shot Learning,”
    ACM Transactions on Intelligent Systems and Technology, vol. 10, no. 2, pp. 1–37,
    Jan. 2019, doi: 10.1145/3293318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] R. Xu et al., ”GCT: Graph Co-Training for Semi-Supervised Few-Shot Learning,”
    in IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no.
    12, pp. 8674-8687, Dec. 2022, doi: 10.1109/TCSVT.2022.3196550.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[369] G. Cheng, C. Lang and J. Han, ”Holistic Prototype Activation for Few-Shot
    Segmentation,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 45, no. 4, pp. 4650-4666, 1 April 2023, doi: 10.1109/TPAMI.2022.3193587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[370] L. Zhu and Y. Yang, ”Label Independent Memory for Semi-Supervised Few-Shot
    Video Classification,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 44, no. 1, pp. 273-285, 1 Jan. 2022, doi: 10.1109/TPAMI.2020.3007511.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[371] Z. Yang, C. Zhang, R. Li, Y. Xu and G. Lin, ”Efficient Few-Shot Object
    Detection via Knowledge Inheritance,” in IEEE Transactions on Image Processing,
    vol. 32, pp. 321-334, 2023, doi: 10.1109/TIP.2022.3228162.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[372] Z. Tian, H. Zhao, M. Shu, Z. Yang, R. Li and J. Jia, ”Prior Guided Feature
    Enrichment Network for Few-Shot Segmentation,” in IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 44, no. 2, pp. 1050-1065, 1 Feb. 2022,
    doi: 10.1109/TPAMI.2020.3013717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[373] C. Yan, X. Chang, M. Luo, H. Liu, X. Zhang and Q. Zheng, ”Semantics-Guided
    Contrastive Network for Zero-Shot Object detection,” in IEEE Transactions on Pattern
    Analysis and Machine Intelligence, doi: 10.1109/TPAMI.2021.3140070.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[374] S. Chen et al., ”GNDAN: Graph Navigated Dual Attention Network for Zero-Shot
    Learning,” in IEEE Transactions on Neural Networks and Learning Systems, doi:
    10.1109/TNNLS.2022.3155602.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[375] P. Ma, Z. He, W. Ran and H. Lu, ”A Transferable Generative Framework
    for Multi-Label Zero-Shot Learning,” in IEEE Transactions on Circuits and Systems
    for Video Technology, doi: 10.1109/TCSVT.2023.3324648.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[376] L. Feng and C. Zhao, ”Transfer Increment for Generalized Zero-Shot Learning,”
    in IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 6,
    pp. 2506-2520, June 2021, doi: 10.1109/TNNLS.2020.3006322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[377] Y. Fu, T. M. Hospedales, T. Xiang and S. Gong, ”Transductive Multi-View
    Zero-Shot Learning,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 37, no. 11, pp. 2332-2345, 1 Nov. 2015, doi: 10.1109/TPAMI.2015.2408354.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[378] C. Gong, J. Yang, J. You and M. Sugiyama, ”Centroid Estimation With Guaranteed
    Efficiency: A General Framework for Weakly Supervised Learning,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 44, no. 6, pp. 2841-2855, 1
    June 2022, doi: 10.1109/TPAMI.2020.3044997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[379] D. Ienco, Y. J. E. Gbodjo, R. Gaetano and R. Interdonato, ”Weakly Supervised
    Learning for Land Cover Mapping of Satellite Image Time Series via Attention-Based
    CNN,” in IEEE Access, vol. 8, pp. 179547-179560, 2020, doi: 10.1109/ACCESS.2020.3024133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[380] J. Han, Y. Yang, D. Zhang, D. Huang, D. Xu and F. De La Torre, ”Weakly-Supervised
    Learning of Category-Specific 3D Object Shapes,” in IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 43, no. 4, pp. 1423-1437, 1 April 2021,
    doi: 10.1109/TPAMI.2019.2949562.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[381] Y. Feng, L. Wang and M. Zhang, ”Weakly-Supervised Learning of a Deep
    Convolutional Neural Networks for Semantic Segmentation,” in IEEE Access, vol.
    7, pp. 91009-91018, 2019, doi: 10.1109/ACCESS.2019.2926972.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[382] A. Prest, C. Schmid and V. Ferrari, ”Weakly Supervised Learning of Interactions
    between Humans and Objects,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 34, no. 3, pp. 601-614, March 2012, doi: 10.1109/TPAMI.2011.158.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[383] R. Cong et al., ”A Weakly Supervised Learning Framework for Salient Object
    Detection via Hybrid Labels,” in IEEE Transactions on Circuits and Systems for
    Video Technology, vol. 33, no. 2, pp. 534-548, Feb. 2023, doi: 10.1109/TCSVT.2022.3205182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[384] X. Zhou, A. Sun, Y. Liu, J. Zhang, and C. Miao, “SelfCF: A Simple Framework
    for Self-supervised Collaborative Filtering,” ACM Transactions on Recommender
    Systems, Apr. 2023, doi: 10.1145/3591469. ‌'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[385] L. Tian, Z. Tu, D. Zhang, J. Liu, B. Li and J. Yuan, ”Unsupervised Learning
    of Optical Flow With CNN-Based Non-Local Filtering,” in IEEE Transactions on Image
    Processing, vol. 29, pp. 8429-8442, 2020, doi: 10.1109/TIP.2020.3013168.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[386] Y. A. D. Djilali, T. Krishna, K. McGuinness and N. E. O’Connor, ”Rethinking
    360° Image Visual Attention Modelling with Unsupervised Learning,” 2021 IEEE/CVF
    International Conference on Computer Vision (ICCV), Montreal, QC, Canada, 2021,
    pp. 15394-15404, doi: 10.1109/ICCV48922.2021.01513.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[387] J. Xu, Z. Zhang and X. Hu, ”Extracting Semantic Knowledge From GANs With
    Unsupervised Learning,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 45, no. 8, pp. 9654-9668, Aug. 2023, doi: 10.1109/TPAMI.2023.3262140.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[388] Y. Shan, H. S. Sawhney and R. Kumar, ”Unsupervised Learning of Discriminative
    Edge Measures for Vehicle Matching between Nonoverlapping Cameras,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 30, no. 4, pp. 700-711, April
    2008, doi: 10.1109/TPAMI.2007.70728.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[389] W. Kim, A. Kanezaki and M. Tanaka, ”Unsupervised Learning of Image Segmentation
    Based on Differentiable Feature Clustering,” in IEEE Transactions on Image Processing,
    vol. 29, pp. 8055-8068, 2020, doi: 10.1109/TIP.2020.3011269.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[390] K. Song, J. Xie, S. Zhang and Z. Luo, ”Multi-Mode Online Knowledge Distillation
    for Self-Supervised Visual Representation Learning,” 2023 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023,
    pp. 11848-11857, doi: 10.1109/CVPR52729.2023.01140.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[391] S. Zare and H. Van Nguyen, ”Evaluating and Improving Domain Invariance
    in Contrastive Self-Supervised Learning by Extrapolating the Loss Function,” in
    IEEE Access, vol. 11, pp. 137758-137768, 2023, doi: 10.1109/ACCESS.2023.3339775.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[392] R. Li, C. Zhang, G. Lin, Z. Wang and C. Shen, ”RigidFlow: Self-Supervised
    Scene Flow Learning on Point Clouds by Local Rigidity Prior,” 2022 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022,
    pp. 16938-16947, doi: 10.1109/CVPR52688.2022.01645.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[393] F. D. Pup and M. Atzori, ”Applications of Self-Supervised Learning to
    Biomedical Signals: A Survey,” in IEEE Access, vol. 11, pp. 144180-144203, 2023,
    doi: 10.1109/ACCESS.2023.3344531.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[394] L. Wang, X. Zhang, H. Su, and J. Zhu, “A Comprehensive Survey of Continual
    Learning: Theory, Method and Application,” arXiv.org, Jun. 10, 2023\. https://arxiv.org/abs/2302.00487
    (accessed Sep. 04, 2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[395] J. He and F. Zhu, ”Out-Of-Distribution Detection In Unsupervised Continual
    Learning,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops (CVPRW), New Orleans, LA, USA, 2022, pp. 3849-3854, doi: 10.1109/CVPRW56347.2022.00430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[396] B. Kwon and T. Kim, ”Toward an Online Continual Learning Architecture
    for Intrusion Detection of Video Surveillance,” in IEEE Access, vol. 10, pp. 89732-89744,
    2022, doi: 10.1109/ACCESS.2022.3201139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[397] Z. Cai, O. Sener and V. Koltun, ”Online Continual Learning with Natural
    Distribution Shifts: An Empirical Study with Visual Data,” 2021 IEEE/CVF International
    Conference on Computer Vision (ICCV), Montreal, QC, Canada, 2021, pp. 8261-8270,
    doi: 10.1109/ICCV48922.2021.00817.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[398] Y. Zhao, D. Saxena and J. Cao, ”AdaptCL: Adaptive Continual Learning
    for Tackling Heterogeneity in Sequential Datasets,” in IEEE Transactions on Neural
    Networks and Learning Systems, doi: 10.1109/TNNLS.2023.3341841.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[399] X. Li and W. Wang, ”GopGAN: Gradients Orthogonal Projection Generative
    Adversarial Network With Continual Learning,” in IEEE Transactions on Neural Networks
    and Learning Systems, vol. 34, no. 1, pp. 215-227, Jan. 2023, doi: 10.1109/TNNLS.2021.3093319.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[400] M. De Lange et al., ”A Continual Learning Survey: Defying Forgetting
    in Classification Tasks,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 44, no. 7, pp. 3366-3385, 1 July 2022, doi: 10.1109/TPAMI.2021.3057446.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[401] L. Wang, B. Lei, Q. Li, H. Su, J. Zhu and Y. Zhong, ”Triple-Memory Networks:
    A Brain-Inspired Method for Continual Learning,” in IEEE Transactions on Neural
    Networks and Learning Systems, vol. 33, no. 5, pp. 1925-1934, May 2022, doi: 10.1109/TNNLS.2021.3111019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[402] M. Xue, H. Zhang, J. Song and M. Song, ”Meta-attention for ViT-backed
    Continual Learning,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), New Orleans, LA, USA, 2022, pp. 150-159, doi: 10.1109/CVPR52688.2022.00025.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[403] X. Wang, L. Yao, X. Wang, H. -Y. Paik and S. Wang, ”Uncertainty Estimation
    With Neural Processes for Meta-Continual Learning,” in IEEE Transactions on Neural
    Networks and Learning Systems, vol. 34, no. 10, pp. 6887-6897, Oct. 2023, doi:
    10.1109/TNNLS.2022.3215633.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[404] B. Sistaninejhad, H. Rasi, and P. Nayeri, “A Review Paper about Deep
    Learning for Medical Image Analysis,” Computational and Mathematical Methods in
    Medicine, vol. 2023, p. e7091301, May 2023, doi: 10.1155/2023/7091301.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[405] Giorgos Papanastasiou, $\text{Ni}\kappa\text{o}\lambda\alpha\text{o}\varsigma\
    \Delta\text{i}\kappa\alpha\text{i}\text{o}\varsigma$, J. Huang, C. Wang, and G.
    Yang, “Is attention all you need in medical image analysis? A review,” arXiv (Cornell
    University), Jul. 2023, doi: 10.48550/arxiv.2307.12775.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[406] Z. M. C. Baum, Y. Hu and D. C. Barratt, ”Meta-Learning Initializations
    for Interactive Medical Image Registration,” in IEEE Transactions on Medical Imaging,
    vol. 42, no. 3, pp. 823-833, March 2023, doi: 10.1109/TMI.2022.3218147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[407] M. T. Irshad and H. U. Rehman, ”Gradient Compass-Based Adaptive Multimodal
    Medical Image Fusion,” in IEEE Access, vol. 9, pp. 22662-22670, 2021, doi: 10.1109/ACCESS.2021.3054843.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[408] J. S. Duncan and N. Ayache, ”Medical image analysis: progress over two
    decades and the challenges ahead,” in IEEE Transactions on Pattern Analysis and
    Machine Intelligence, vol. 22, no. 1, pp. 85-106, Jan. 2000, doi: 10.1109/34.824822.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[409] J. Ker, L. Wang, J. Rao and T. Lim, ”Deep Learning Applications in Medical
    Image Analysis,” in IEEE Access, vol. 6, pp. 9375-9389, 2018, doi: 10.1109/ACCESS.2017.2788044.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[410] G. Litjens et al., “A Survey on Deep Learning in Medical Image Analysis,”
    Medical Image Analysis, vol. 42, pp. 60–88, Dec. 2017, doi: 10.1016/j.media.2017.07.005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[411] S. M. Anwar, M. Majid, A. Qayyum, M. Awais, M. Alnowami, and M. K. Khan,
    “Medical Image Analysis using Convolutional Neural Networks: A Review,” Journal
    of Medical Systems, vol. 42, no. 11, Oct. 2018, doi: 10.1007/s10916-018-1088-1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[412] D. Shen, G. Wu, and H.-I. Suk, “Deep Learning in Medical Image Analysis,”
    Annual Review of Biomedical Engineering, vol. 19, no. 1, pp. 221–248, Jun. 2017,
    doi: 10.1146/annurev-bioeng-071516-044442.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[413] J. Jiang, P. Trundle, and J. Ren, “Medical image analysis with artificial
    neural networks,” Computerized Medical Imaging and Graphics, vol. 34, no. 8, pp.
    617–631, Dec. 2010, doi: 10.1016/j.compmedimag.2010.07.003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[414] Z. Gu et al., ”CE-Net: Context Encoder Network for 2D Medical Image Segmentation,”
    in IEEE Transactions on Medical Imaging, vol. 38, no. 10, pp. 2281-2292, Oct.
    2019, doi: 10.1109/TMI.2019.2903562.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[415] M. Z. Khan, M. K. Gajendran, Y. Lee and M. A. Khan, ”Deep Neural Architectures
    for Medical Image Semantic Segmentation: Review,” in IEEE Access, vol. 9, pp.
    83002-83024, 2021, doi: 10.1109/ACCESS.2021.3086530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[416] J. Duan, S. Mao, J. Jin, Z. Zhou, L. Chen and C. L. P. Chen, ”A Novel
    GA-Based Optimized Approach for Regional Multimodal Medical Image Fusion With
    Superpixel Segmentation,” in IEEE Access, vol. 9, pp. 96353-96366, 2021, doi:
    10.1109/ACCESS.2021.3094972.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[417] C. You, Y. Zhou, R. Zhao, L. Staib and J. S. Duncan, ”SimCVD: Simple
    Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical
    Image Segmentation,” in IEEE Transactions on Medical Imaging, vol. 41, no. 9,
    pp. 2228-2237, Sept. 2022, doi: 10.1109/TMI.2022.3161829.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[418] S. Ye, T. Wang, M. Ding and X. Zhang, ”F-DARTS: Foveated Differentiable
    Architecture Search Based Multimodal Medical Image Fusion,” in IEEE Transactions
    on Medical Imaging, vol. 42, no. 11, pp. 3348-3361, Nov. 2023, doi: 10.1109/TMI.2023.3283517.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[419] S. Liu, E. Johns and A. J. Davison, ”End-To-End Multi-Task Learning With
    Attention,” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), Long Beach, CA, USA, 2019, pp. 1871-1880, doi: 10.1109/CVPR.2019.00197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[420] Y. Zhang and Q. Yang, ”A Survey on Multi-Task Learning,” in IEEE Transactions
    on Knowledge and Data Engineering, vol. 34, no. 12, pp. 5586-5609, 1 Dec. 2022,
    doi: 10.1109/TKDE.2021.3070203.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[421] I. Misra, A. Shrivastava, A. Gupta and M. Hebert, ”Cross-Stitch Networks
    for Multi-task Learning,” 2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 3994-4003, doi: 10.1109/CVPR.2016.433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[422] S. Vandenhende, S. Georgoulis, W. Van Gansbeke, M. Proesmans, D. Dai
    and L. Van Gool, ”Multi-Task Learning for Dense Prediction Tasks: A Survey,” in
    IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 7,
    pp. 3614-3633, 1 July 2022, doi: 10.1109/TPAMI.2021.3054719.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[423] F. Zhao, Y. Li, L. Bai, Z. Tian and X. Wang, ”Semi-Supervised Multi-Granularity
    CNNs for Text Classification: An Application in Human-Car Interaction,” in IEEE
    Access, vol. 8, pp. 68000-68012, 2020, doi: 10.1109/ACCESS.2020.2985098.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[424] H. Chen, Y. Wang and Q. Hu, ”Multi-Granularity Regularized Re-Balancing
    for Class Incremental Learning,” in IEEE Transactions on Knowledge and Data Engineering,
    vol. 35, no. 7, pp. 7263-7277, 1 July 2023, doi: 10.1109/TKDE.2022.3188335.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[425] K. Niu, Y. Huang, W. Ouyang and L. Wang, ”Improving Description-Based
    Person Re-Identification by Multi-Granularity Image-Text Alignments,” in IEEE
    Transactions on Image Processing, vol. 29, pp. 5542-5556, 2020, doi: 10.1109/TIP.2020.2984883.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[426] J. -F. Hu, W. -S. Zheng, J. Lai and J. Zhang, ”Jointly Learning Heterogeneous
    Features for RGB-D Activity Recognition,” in IEEE Transactions on Pattern Analysis
    and Machine Intelligence, vol. 39, no. 11, pp. 2186-2200, 1 Nov. 2017, doi: 10.1109/TPAMI.2016.2640292.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[427] C. Sahin, G. Garcia-Hernando, J. Sock, and T.-K. Kim, “A review on object
    pose recovery: From 3D bounding box detectors to full 6D pose estimators,” Image
    and Vision Computing, vol. 96, p. 103898, Apr. 2020, doi: https://doi.org/10.1016/j.imavis.2020.103898.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[428] T. Hodaň, D. Baráth and J. Matas, ”EPOS: Estimating 6D Pose of Objects
    With Symmetries,” 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), Seattle, WA, USA, 2020, pp. 11700-11709, doi: 10.1109/CVPR42600.2020.01172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[429] W. Kehl, F. Manhardt, F. Tombari, S. Ilic and N. Navab, ”SSD-6D: Making
    RGB-Based 3D Detection and 6D Pose Estimation Great Again,” 2017 IEEE International
    Conference on Computer Vision (ICCV), Venice, Italy, 2017, pp. 1530-1538, doi:
    10.1109/ICCV.2017.169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[430] T. Vaudrey, A. Wedel, C. Rabe, J. Klappstein and R. Klette, ”Evaluation
    of moving object segmentation comparing 6D-vision and monocular motion constraints,”
    2008 23rd International Conference Image and Vision Computing New Zealand, Christchurch,
    New Zealand, 2008, pp. 1-6, doi: 10.1109/IVCNZ.2008.4762126.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[431] Z. He, W. Feng, X. Zhao, and Y. Lv, “6D Pose Estimation of Objects: Recent
    Technologies and Challenges,” Applied Sciences, vol. 11, no. 1, p. 228, Jan. 2021,
    doi: 10.3390/app11010228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[432] Y. Li, G. Wang, X. Ji, Y. Xiang, and D. Fox, “DeepIM: Deep Iterative
    Matching for 6D Pose Estimation,” International Journal of Computer Vision, vol.
    128, no. 3, pp. 657–678, Nov. 2019, doi: 10.1007/s11263-019-01250-9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[433] T. Elsken, Jan Hendrik Metzen, and F. Hutter, “Neural Architecture Search:
    A Survey,” arXiv (Cornell University), vol. 20, no. 55, pp. 1–21, Jan. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[434] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, “Neural Architecture
    Search without Training,” arXiv (Cornell University), Jun. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[435] L. Sekanina, “Neural Architecture Search and Hardware Accelerator Co-Search:
    A Survey,” IEEE Access, vol. 9, pp. 151337–151362, 2021, doi: 10.1109/access.2021.3126685.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[436] K. T. Chitty-Venkata, M. Emani, V. Vishwanath and A. K. Somani, ”Neural
    Architecture Search for Transformers: A Survey,” in IEEE Access, vol. 10, pp.
    108374-108412, 2022, doi: 10.1109/ACCESS.2022.3212767.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[437] K. G. Mills et al., ”Exploring Neural Architecture Search Space via Deep
    Deterministic Sampling,” in IEEE Access, vol. 9, pp. 110962-110974, 2021, doi:
    10.1109/ACCESS.2021.3101975.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[438] Z. Ding, Y. Chen, N. Li, D. Zhao, Z. Sun and C. L. P. Chen, ”BNAS: Efficient
    Neural Architecture Search Using Broad Scalable Architecture,” in IEEE Transactions
    on Neural Networks and Learning Systems, vol. 33, no. 9, pp. 5004-5018, Sept.
    2022, doi: 10.1109/TNNLS.2021.3067028.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[439] Z. Ma, Z. Zhou, Y. Liu, Y. Lei and H. Yan, ”Auto-ORVNet: Orientation-Boosted
    Volumetric Neural Architecture Search for 3D Shape Classification,” in IEEE Access,
    vol. 8, pp. 12942-12954, 2020, doi: 10.1109/ACCESS.2019.2961715.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[440] X. Zhang, Z. Huang, N. Wang, S. Xiang and C. Pan, ”You Only Search Once:
    Single Shot Neural Architecture Search via Direct Sparse Optimization,” in IEEE
    Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 9, pp.
    2891-2904, 1 Sept. 2021, doi: 10.1109/TPAMI.2020.3020300.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[441] Y. Guo et al., ”Towards Accurate and Compact Architectures via Neural
    Architecture Transformer,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 44, no. 10, pp. 6501-6516, 1 Oct. 2022, doi: 10.1109/TPAMI.2021.3086914.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[442] H. Cao, C. Tan, Z. Gao, G. Chen, P. Heng, and S. Z. Li, “A Survey on
    Generative Diffusion Model,” arXiv (Cornell University), Sep. 2022, doi: 10.48550/arxiv.2209.02646.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[443] W. Mao, B. Han and Z. Wang, ”Sketchffusion: Sketch-Guided Image Editing
    with Diffusion Model,” 2023 IEEE International Conference on Image Processing
    (ICIP), Kuala Lumpur, Malaysia, 2023, pp. 790-794, doi: 10.1109/ICIP49359.2023.10222365.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[444] X. P. Ooi and C. Seng Chan, ”LLDE: Enhancing Low-Light Images with Diffusion
    Model,” 2023 IEEE International Conference on Image Processing (ICIP), Kuala Lumpur,
    Malaysia, 2023, pp. 1305-1309, doi: 10.1109/ICIP49359.2023.10222446.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[445] T. Roque et al., ”A DCE-MRI Driven 3-D Reaction-Diffusion Model of Solid
    Tumor Growth,” in IEEE Transactions on Medical Imaging, vol. 37, no. 3, pp. 724-732,
    March 2018, doi: 10.1109/TMI.2017.2779811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[446] T. Roque et al., ”A DCE-MRI Driven 3-D Reaction-Diffusion Model of Solid
    Tumor Growth,” in IEEE Transactions on Medical Imaging, vol. 37, no. 3, pp. 724-732,
    March 2018, doi: 10.1109/TMI.2017.2779811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[447] D. Kim, E. Lee, D. Yoo and H. Lee, ”Fine-Grained Human Hair Segmentation
    Using a Text-to-Image Diffusion Model,” in IEEE Access, doi: 10.1109/ACCESS.2024.3355542.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[448] A. Karnewar, A. Vedaldi, D. Novotny and N. J. Mitra, ”HOLODIFFUSION:
    Training a 3D Diffusion Model Using 2D Images,” 2023 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 18423-18433,
    doi: 10.1109/CVPR52729.2023.01767.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[449] W. Ran, W. Yuan and R. Shibasaki, ”Few-Shot Depth Completion Using Denoising
    Diffusion Probabilistic Model,” 2023 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition Workshops (CVPRW), Vancouver, BC, Canada, 2023, pp. 6559-6567,
    doi: 10.1109/CVPRW59228.2023.00697.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[450] T. Hospedales, A. Antoniou, P. Micaelli and A. Storkey, ”Meta-Learning
    in Neural Networks: A Survey,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 44, no. 9, pp. 5149-5169, 1 Sept. 2022, doi: 10.1109/TPAMI.2021.3079209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[451] I. Khan, X. Zhang, M. Rehman and R. Ali, ”A Literature Survey and Empirical
    Study of Meta-Learning for Classifier Selection,” in IEEE Access, vol. 8, pp.
    10262-10281, 2020, doi: 10.1109/ACCESS.2020.2964726.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[452] P. Zhang, C. Liu, X. Chang, Y. Li and M. Li, ”Metric-based Meta-Learning
    Model for Few-Shot PolSAR Image Terrain Classification,” 2021 CIE International
    Conference on Radar (Radar), Haikou, Hainan, China, 2021, pp. 2529-2533, doi:
    10.1109/Radar53847.2021.10027883.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[453] X. Zhang, D. Meng, H. Gouk and T. Hospedales, ”Shallow Bayesian Meta
    Learning for Real-World Few-Shot Recognition,” 2021 IEEE/CVF International Conference
    on Computer Vision (ICCV), Montreal, QC, Canada, 2021, pp. 631-640, doi: 10.1109/ICCV48922.2021.00069.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[454] K. Gao, B. Liu, X. Yu and A. Yu, ”Unsupervised Meta Learning With Multiview
    Constraints for Hyperspectral Image Small Sample set Classification,” in IEEE
    Transactions on Image Processing, vol. 31, pp. 3449-3462, 2022, doi: 10.1109/TIP.2022.3169689.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[455] H. Cho, Y. Cho, J. Yu and J. Kim, ”Camera Distortion-aware 3D Human Pose
    Estimation in Video with Optimization-based Meta-Learning,” 2021 IEEE/CVF International
    Conference on Computer Vision (ICCV), Montreal, QC, Canada, 2021, pp. 11149-11158,
    doi: 10.1109/ICCV48922.2021.01098.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[456] L. Zhang, Z. Liu, W. Zhang and D. Zhang, ”Style Uncertainty Based Self-Paced
    Meta Learning for Generalizable Person Re-Identification,” in IEEE Transactions
    on Image Processing, vol. 32, pp. 2107-2119, 2023, doi: 10.1109/TIP.2023.3263112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[457] H. Coskun et al., ”Domain-Specific Priors and Meta Learning for Few-Shot
    First-Person Action Recognition,” in IEEE Transactions on Pattern Analysis and
    Machine Intelligence, vol. 45, no. 6, pp. 6659-6673, 1 June 2023, doi: 10.1109/TPAMI.2021.3058606.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[458] Y. Deng, T. Han and N. Ansari, ”FedVision: Federated Video Analytics
    With Edge Computing,” in IEEE Open Journal of the Computer Society, vol. 1, pp.
    62-72, 2020, doi: 10.1109/OJCS.2020.2996184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[459] K. Doshi and Y. Yilmaz, ”Privacy-Preserving Video Understanding via Transformer-based
    Federated Learning,” 2023 IEEE Conference on Dependable and Secure Computing (DSC),
    Tampa, FL, USA, 2023, pp. 1-8, doi: 10.1109/DSC61021.2023.10354099.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[460] Y. Liu et al., “Federated Learning-Powered Visual Object Detection for
    Safety Monitoring,” AI Magazine, vol. 42, no. 2, pp. 19–27, Oct. 2021, doi: 10.1609/aimag.v42i2.15095.
    ‌'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[461] Y. Liu et al., “FedVision: An Online Visual Object Detection Platform
    Powered by Federated Learning,” Proceedings of the AAAI Conference on Artificial
    Intelligence, vol. 34, no. 08, pp. 13172–13179, Apr. 2020, doi: 10.1609/aaai.v34i08.7021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[462] K. Zhou and Xin Eric Wang, “FedVLN: Privacy-preserving Federated Vision-and-Language
    Navigation,” arXiv (Cornell University), Mar. 2022, doi: 10.48550/arxiv.2203.14936.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[463] H. Li, K. Yin, X. Ji, Y. Liu, T. Huang and G. Yin, ”Improved YOLOV3 Surveillance
    Device Object Detection Method Based on Federated Learning,” 2022 4th International
    Conference on Data-driven Optimization of Complex Systems (DOCS), Chengdu, China,
    2022, pp. 1-6, doi: 10.1109/DOCS55193.2022.9967481.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[464] M. Alazab, S. P. RM, P. M, P. K. R. Maddikunta, T. R. Gadekallu and Q.
    -V. Pham, ”Federated Learning for Cybersecurity: Concepts, Challenges, and Future
    Directions,” in IEEE Transactions on Industrial Informatics, vol. 18, no. 5, pp.
    3501-3509, May 2022, doi: 10.1109/TII.2021.3119038.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[465] P. K. Mandal, Carter De Leo, and C. Hurley, “Horizontal Federated Computer
    Vision,” arXiv (Cornell University), Dec. 2023, doi: 10.48550/arxiv.2401.00390.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/ac14f5baae3653fe8d2a513f332d2931.png) | Abolfazl
    Younesi received a B.Sc. degree in computer engineering from the Tabriz University,
    Tabriz, Iran, in 2021\. He is currently working toward an M.Sc. degree in computer
    engineering at Sharif University of Technology (SUT), Tehran, Iran, from Oct.
    2021 until now. He is currently a member of the Embedded Systems Research Laboratory
    (ESR-LAB) at the Department of Computer Engineering, Sharif University of Technology.
    His research interests include the Internet of Things (IoT) and Cyber-Physical
    Systems (CPS), low-power design, machine learning, and computer vision. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/01164c65ef5a2b63f5742725459f9797.png) | Mohsen
    Ansari received the Ph.D. degree in computer engineering from the Sharif University
    of Technology, Tehran, Iran, in 2021\. He is currently an Assistant Professor
    of computer engineering at the Sharif University of Technology, Tehran, Iran.
    He was a visiting researcher with the chair for Embedded Systems (CES), Karlsruhe
    Institute of Technology (KIT), Germany, from 2019 to 2021\. His research interests
    include cyber-physical systems, embedded machine learning, edge, fog, and cloud
    computing, and thermal and low-power design of CPSs. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/dca3ea20497b602fe404bad5820db6df.png) | MohammadAmin
    Fazli received the B.Sc. degree in hardware engineering and the M.Sc. and Ph.D.
    degrees in software engineering from the Sharif University of Technology, Tehran,
    Iran, in 2009, 2011, and 2015, respectively. He is currently an Assistant Professor
    at the Sharif University of Technology, where he is also a Research and Development
    Supervisor with the Intelligent Information Solutions Center. His current research
    interests include game theory, combinatorial optimization, computational business
    and economics, graphs and combinatorics, complex networks, and dynamical systems.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/c023ba39b014f151e794963e9f1a81df.png) | Alireza
    Ejlali received the Ph.D. degree in computer engineering from the Sharif University
    of Technology (SUT), Tehran, Iran, in 2006, where he is currently an Associate
    Professor of computer engineering. From 2005 to 2006, he was a Visiting Researcher
    with the Electronic Systems Design Group, University of Southampton, Southampton,
    U.K. He is currently the Director of the Embedded Systems Research Laboratory
    with the Department of Computer Engineering, Sharif University of Technology.
    His research interests include low-power design, real-time, and fault-tolerant
    embedded systems. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/9f84fb5f5f8a635efb69ea0a53b4acce.png) | Muhammad
    Shafique (Senior Member, IEEE) received the Ph.D. degree in computer science from
    Karlsruhe Institute of Technology, Karlsruhe, Germany, in 2011\. He was a Full
    Professor at the Institute of Computer Engineering, TU Wien, Vienna, Austria,
    from October 2016 to August 2020\. Since September 2020, he has been with the
    Division of Engineering, New York University Abu Dhabi, Abu Dhabi, UAE, and is
    a Global Network Faculty Member of the NYU Tandon School of Engineering, Brooklyn,
    NY, USA. His research interests are in system-level design for brain-inspired
    computing, AI/machine learning hardware, wearables, autonomous systems, energy-efficient
    and robust computing, IoT, and smart CPS. Dr. Shafique received the 2015 ACM/SIGDA
    Outstanding New Faculty Award, the AI 2000 Chip Technology Most Influential Scholar
    Award in 2020, 2022, and 2023, six gold medals, and several best paper awards
    and nominations. He has given several keynotes, talks, and tutorials and organized
    special sessions at premier venues. He has served as the PC chair, general chair,
    track chair, and PC member for several conferences. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/67acb84df6d491c378695937997f4373.png) | Jörg Henkel
    (Fellow, IEEE) received the Diploma and Ph.D. degrees (summa cum laude) from the
    Technical University of Braunschweig. He is currently the Chair Professor of embedded
    systems with the Karlsruhe Institute of Technology (KIT). Before that, he was
    a Research Staff Member with NEC Laboratories, Princeton, NJ, USA. His research
    interests include co-design for embedded hardware/software systems with respect
    to power security and means of embedded machine learning. He is the Vice President
    of Publications at IEEE CEDA and a fellow of the ACM. He has led several conferences
    as the General Chair, including ICCAD and ESWeek, and is currently the General
    of DAC’60\. He serves as a steering committee chair/member for leading conferences
    and journals for embedded and cyber-physical systems. He has coordinated the DFG
    Program SPP 1500 “Dependable Embedded Systems” and is a Site Coordinator of the
    DFG TR89 Collaborative Research Center on “Invasive Computing.” He is the Chairman
    of the IEEE Computer Society, Germany Chapter. He has received six best paper
    awards throughout his career from, among others, ICCAD, ESWeek, and DATE. For
    two consecutive terms each, he served as the Editor-in-Chief for both the ACM
    Transactions on Embedded Computing Systems and the IEEE Design & Test magazine.
    |'
  prefs: []
  type: TYPE_TB
