- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:00:31'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[2007.00095] Deep Learning for Vision-based Prediction: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2007.00095](https://ar5iv.labs.arxiv.org/html/2007.00095)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Vision-based Prediction: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amir Rasouli A. Rasouli is with Noah’s Ark Laboratory at Huawei Technologies
    Canada, 19 Allstate Pkwy, Markham, ON L3R 5A4
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: amir.rasouli@huawei.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Vision-based prediction algorithms have a wide range of applications including
    autonomous driving, surveillance, human-robot interaction, weather prediction.
    The objective of this paper is to provide an overview of the field in the past
    five years with a particular focus on deep learning approaches. For this purpose,
    we categorize these algorithms into video prediction, action prediction, trajectory
    prediction, body motion prediction, and other prediction applications. For each
    category, we highlight the common architectures, training methods and types of
    data used. In addition, we discuss the common evaluation metrics and datasets
    used for vision-based prediction tasks. A database of all the information presented
    in this survey, cross-referenced according to papers, datasets and metrics, can
    be found online at [https://github.com/aras62/vision-based-prediction](https://github.com/aras62/vision-based-prediction).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Video Prediction, Action Prediction , Trajectory Prediction, Motion Prediction,
    Survey.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ability to predict the changes in the environment and the behavior of objects
    is fundamental in many applications such as surveillance, autonomous driving,
    scene understanding, etc. Prediction is a widely studied field in various artificial
    intelligence communities. A subset of these algorithms relies primarily on visual
    appearances of the objects and the scene to reason about the future. Other approaches
    use different forms of sensors such as wearable or environmental sensors to learn
    about the past states of the environment or objects.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The focus of this report is on vision-based prediction algorithms, which primarily
    use visual information to observe the changes in the environment and predict the
    future. In this context, prediction can be in the form of generating future scenes
    or reasoning about specific aspects of the objects, e.g. their trajectories, poses,
    etc.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: For this review, we divide the prediction algorithms into five groups, namely
    video prediction, action prediction, trajectory prediction, motion (pose) prediction,
    and others which involve various applications of prediction such as trend prediction,
    visual weather prediction, map prediction, semantic prediction, etc. In addition,
    we briefly discuss algorithms that use a form of prediction as an intermediate
    step to perform tasks such as object detection, action detection, and recognition,
    etc. Moreover, for each group of prediction algorithms, we will talk about the
    common datasets and metrics and discuss of their characteristics. It should be
    noted that due to the broad scope of this review and the large body of work on
    the vision-based prediction, this review will only focus on works that had been
    published since five years ago in major computer vision, robotics and machine
    learning venues. In addition, as the title of the paper suggests, the main focus
    of the discussion will be on deep learning methods given their popularity in recent
    years.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这次审查，我们将预测算法分为五组，即视频预测、动作预测、轨迹预测、运动（姿态）预测和其他涉及各种预测应用的组，如趋势预测、视觉天气预测、地图预测、语义预测等。此外，我们还简要讨论了使用预测形式作为中间步骤以执行诸如目标检测、动作检测和识别等任务的算法。此外，对于每组预测算法，我们将讨论常见的数据集和指标，并讨论其特征。需要注意的是，由于本审查的范围广泛以及基于视觉的预测工作的庞大，这次审查仅关注自五年前在主要计算机视觉、机器人和机器学习领域发表的工作。此外，正如论文标题所示，讨论的主要焦点将是深度学习方法，因为它们在近年来非常流行。
- en: 2 Vision-based Prediction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基于视觉的预测
- en: Before reviewing the works on vision-based prediction algorithms, there are
    a number of points that should be considered.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在审阅基于视觉的预测算法之前，需要考虑一些要点。
- en: 2.1 Applications
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 应用
- en: Based on our review, we have identified four major vision-based applications
    namely, video prediction, action prediction, trajectory prediction, and motion
    prediction. We discuss each of the studies in each category in a dedicated section.
    Some of the prediction works, such as visual weather prediction, semantic prediction,
    contests outcome prediction, that do not fit to any of the four major categories
    are presented in other application section.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的审查，我们确定了四个主要的基于视觉的应用，即视频预测、动作预测、轨迹预测和运动预测。我们在专门的部分讨论每个类别中的每项研究。一些预测工作，如视觉天气预测、语义预测、竞赛结果预测，不适合任何四个主要类别，会在其他应用部分中介绍。
- en: Some works address multiple prediction tasks, e.g. predicting trajectories and
    actions simultaneously, and therefore might fall in more than one category. It
    should be noted that we only include an algorithm in each category if the corresponding
    task is directly evaluated. For instance, if an algorithm performs video prediction
    for future action classification, and only evaluates the accuracy of predicted
    actions, it will only appear in the action prediction category. Furthermore, some
    works that are reviewed in this paper propose multiple architectures, e.g. recurrent
    and feedforward, for solving the same problem. In architecture-based categorizations,
    these algorithms may appear more than once.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作涉及多个预测任务，例如同时预测轨迹和动作，因此可能属于多个类别。需要注意的是，我们只在每个类别中包含对应任务直接评估的算法。例如，如果一个算法执行未来动作分类的视频预测，并且仅评估预测动作的准确性，它将仅出现在动作预测类别中。此外，本文中审阅的一些工作提出了多个架构，例如递归和前馈，以解决相同的问题。在基于架构的分类中，这些算法可能会出现多次。
- en: 2.2 Methods
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 方法
- en: 2.2.1 Algorithms
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 算法
- en: 'This work focuses on vision-based algorithms, which use some form of visual
    input such as RGB camera images or active sensors such as LIDAR. It should be
    noted that many algorithms, especially trajectory prediction ones, only use ground
    truth data such as object trajectories without actual visual processing, e.g.
    for detection of objects. However, as long as these algorithms are evaluated on
    vision datasets, they are included in this paper. Note that a completer list of
    papers with published code can be found in Appendix [A](#A1 "Appendix A Papers
    with code ‣ Deep Learning for Vision-based Prediction: A Survey").'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Architectures
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned earlier, we focus on algorithms that have a deep learning component,
    either in the stage of visual representation generation (e.g. using convolutional
    features) or reasoning (e.g. using an MultiLayer Preceptron (MLP) for classification).
    We will, however, acknowledge the classical methods by mentioning some of the
    main techniques and including them in the datasets and metrics sections of this
    paper.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: We classify the algorithms in terms of training techniques and architectures.
    In practice, this is very challenging as the majority of algorithms use a combination
    of different approaches. For example, recurrent networks often rely on a form
    of Convolutional Neural Networks (CNNs) to generate feature representations for
    scenes, poses of agents, etc. To better distinguish between different classes
    of algorithms, we only focus on the core component of each algorithm, i.e. the
    parts that are used for reasoning about the future. Hence, for example, if an
    algorithm uses a CNN model for pre-processing input data and a recurrent network
    for temporal reasoning, we consider this algorithm as recurrent. On the other
    hand, if the features are used with a fully connected network, we categorize this
    algorithm as feedforward or one-shot method. A few algorithms propose the use
    of both architectures for reasoning. We address those methods as hybrid. In addition,
    it should be noted that many works propose alternative approaches using each architecture.
    Therefore, we categorize them in more than one group.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Data type
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As one would expect, vision-based algorithms primarily rely on visual information.
    However, many algorithms use pre-trained off-the-shelf algorithms to transform
    the input to some explicit feature spaces, e.g. poses, trajectories, action labels
    and perform reasoning in those feature spaces. If pre-processing is not part of
    the main algorithm, we consider those secondary features as different types of
    data inputs to the algorithms. If some basic processing, e.g. generating convolutional
    features for a scene is used, we consider the data type of the original input,
    e.g. RGB images.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 3 Video prediction
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Video or future scene prediction can be regarded as the most generic form of
    prediction. The objective of video prediction algorithms is to generate future
    scenes, often in the form of RGB images [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)] and/or optical flow maps [[5](#bib.bib5), [6](#bib.bib6), [1](#bib.bib1),
    [7](#bib.bib7)]. The generated images in turn can be used for various tasks such
    as action prediction [[8](#bib.bib8)], event prediction [[9](#bib.bib9)], flow
    estimation [[6](#bib.bib6)], semantic segmentation [[10](#bib.bib10)], etc.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 视频或未来场景预测可以视为最通用的预测形式。视频预测算法的目标是生成未来场景，通常以RGB图像[[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)]和/或光流图[[5](#bib.bib5), [6](#bib.bib6), [1](#bib.bib1),
    [7](#bib.bib7)]的形式呈现。生成的图像反过来可以用于各种任务，如动作预测[[8](#bib.bib8)]、事件预测[[9](#bib.bib9)]、流估计[[6](#bib.bib6)]、语义分割[[10](#bib.bib10)]等。
- en: Video prediction applications rely on generative models whose task is to predict
    future scene(s) based on a short observation of input sequences (or in some cases
    only a single image [[11](#bib.bib11)]). Although many approaches use feedforard
    architectures [[2](#bib.bib2), [1](#bib.bib1), [4](#bib.bib4), [5](#bib.bib5),
    [9](#bib.bib9), [12](#bib.bib12), [11](#bib.bib11), [7](#bib.bib7), [13](#bib.bib13),
    [14](#bib.bib14), [8](#bib.bib8), [15](#bib.bib15)], the majority of algorithms
    take advantage of Recurrent Neural Networks (RNNs) such as Gated Recurrent Units
    (GRUs) [[16](#bib.bib16)], Long-Short Term Memory (LSTM) networks [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)], its variation Convolutional
    LSTMs (ConvLSTMs)[[3](#bib.bib3), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [6](#bib.bib6), [29](#bib.bib29)]
    or a combination of these [[37](#bib.bib37), [27](#bib.bib27)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 视频预测应用依赖于生成模型，其任务是基于对输入序列的短时间观察（或在某些情况下仅一张图像[[11](#bib.bib11)]）来预测未来场景。尽管许多方法使用前馈架构[[2](#bib.bib2),
    [1](#bib.bib1), [4](#bib.bib4), [5](#bib.bib5), [9](#bib.bib9), [12](#bib.bib12),
    [11](#bib.bib11), [7](#bib.bib7), [13](#bib.bib13), [14](#bib.bib14), [8](#bib.bib8),
    [15](#bib.bib15)]，大多数算法利用递归神经网络（RNNs），例如门控递归单元（GRUs）[[16](#bib.bib16)]、长短期记忆（LSTM）网络[[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]、其变体卷积LSTM（ConvLSTMs）[[3](#bib.bib3),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [6](#bib.bib6), [29](#bib.bib29)]或这些的组合[[37](#bib.bib37), [27](#bib.bib27)]。
- en: 'Generative Adversarial Networks (GANs) are particularly popular in the video
    prediction community. [[2](#bib.bib2), [18](#bib.bib18), [31](#bib.bib31), [19](#bib.bib19),
    [21](#bib.bib21), [22](#bib.bib22), [24](#bib.bib24), [13](#bib.bib13), [14](#bib.bib14),
    [36](#bib.bib36), [6](#bib.bib6), [26](#bib.bib26), [15](#bib.bib15)]. In these
    adversarial training frameworks, there are two compoents: A generative network
    that produces future representations and a discriminator whose objective is to
    distinguish between the predicted representations (e.g. optical flow [[6](#bib.bib6)],
    frames [[18](#bib.bib18)], motion [[31](#bib.bib31)]) or their temporal consistency
    [[2](#bib.bib2), [19](#bib.bib19)] and the actual ground truth data by producing
    a binary classification score that indicates whether the prediction is real or
    fake. While many algorithms use discriminators to judge how realistic the final
    generated images [[18](#bib.bib18), [21](#bib.bib21), [22](#bib.bib22), [24](#bib.bib24),
    [13](#bib.bib13), [36](#bib.bib36)] are or intermediate features (e.g. poses [[26](#bib.bib26)]),
    others use multiple discriminators at different stages of processing. For example,
    the authors of [[2](#bib.bib2), [19](#bib.bib19)] use two discriminators, one
    is responsible for judging the temporal consistency of the generated frames (i.e.
    whether the order of generated frames is real) and the other assesses whether
    the generated frames are real or not. Lee et al. [[31](#bib.bib31)] use three
    discriminators to assess the quality of generated frames and the intermediate
    motion and content features. Using a two-stream approach, the method in [[6](#bib.bib6)]
    produces both the next frame and optical flow and each stream is trained with
    a separate discriminator. The prediction network of [[15](#bib.bib15)] uses a
    discriminator for intermediate features generated from input scenes and another
    discriminator for the final results.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoders (VAEs) [[38](#bib.bib38)] or Conditional VAEs (CVAEs)
    [[39](#bib.bib39)] are also used in some approaches [[3](#bib.bib3), [18](#bib.bib18),
    [11](#bib.bib11), [23](#bib.bib23), [26](#bib.bib26)]. VAEs model uncertainty
    in generated future frames by defining a posterior distribution over some latent
    variable space [[3](#bib.bib3), [23](#bib.bib23), [26](#bib.bib26)]. In CVAEs,
    the posterior is conditioned on an additional parameter such as the observed action
    in the scenes [[18](#bib.bib18)] or initial observation [[11](#bib.bib11)]. Using
    VAEs, at inference time, a random sample is drawn from the posterior to generate
    the future frame.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Many video prediction algorithms operate solely on input images and propose
    various architectural innovations for encoding the content and generating future
    images [[2](#bib.bib2), [3](#bib.bib3), [32](#bib.bib32), [33](#bib.bib33), [16](#bib.bib16),
    [34](#bib.bib34), [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [8](#bib.bib8),
    [15](#bib.bib15), [27](#bib.bib27)]. For example, the method in [[2](#bib.bib2)]
    performs a two-way prediction, forward and backward. Each prediction relies on
    two discriminators for assessing the quality of the generated images and temporal
    consistency. The model presented in [[3](#bib.bib3)] trains a context network
    by inputting an image sequence into a ConvLSTM whose output is used to initialize
    convolutional networks responsible for generating the next frames. Xu et al. [[32](#bib.bib32)],
    in addition to raw pixel values, encode the output of a high pass filter applied
    to the image as a means of maintaining the structural integrity of the objects
    in the scene. In [[15](#bib.bib15)], the authors use a two-step approach in which
    they first perform a coarse frame prediction followed by a fine frame prediction.
    In [[13](#bib.bib13)], the algorithm learns in two stages. A discriminator is
    applied after features are generated from the scenes and another one after the
    final generated frames.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 许多视频预测算法仅基于输入图像操作，并提出了各种架构创新来编码内容和生成未来图像 [[2](#bib.bib2), [3](#bib.bib3), [32](#bib.bib32),
    [33](#bib.bib33), [16](#bib.bib16), [34](#bib.bib34), [13](#bib.bib13), [14](#bib.bib14),
    [35](#bib.bib35), [8](#bib.bib8), [15](#bib.bib15), [27](#bib.bib27)]。例如，[[2](#bib.bib2)]
    中的方法执行双向预测，即前向和后向预测。每个预测依赖于两个判别器来评估生成图像的质量和时间一致性。[[3](#bib.bib3)] 中提出的模型通过将图像序列输入到
    ConvLSTM 中来训练一个上下文网络，其输出用于初始化负责生成下一帧的卷积网络。Xu 等人 [[32](#bib.bib32)] 除了原始像素值，还对图像应用高通滤波器的输出进行编码，以保持场景中物体的结构完整性。在
    [[15](#bib.bib15)] 中，作者使用了一个两步方法，首先进行粗略的帧预测，然后进行精细的帧预测。在 [[13](#bib.bib13)] 中，该算法分两个阶段进行学习。一个判别器在从场景生成特征后应用，另一个则在最终生成帧后应用。
- en: 'Optical flow prediction has been widely used as an intermediate step in video
    prediction algorithms [[1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [20](#bib.bib20),
    [5](#bib.bib5), [11](#bib.bib11), [25](#bib.bib25), [36](#bib.bib36), [6](#bib.bib6)].
    For example, to deal with occlusion in dynamic scenes, Gao et al. [[1](#bib.bib1)]
    disentangle flow and pixel-level predictions into two steps: the algorithm first
    predicts the flow of the scene, and then uses it, in conjunction with the input
    frames, to predict the future. Similar multi-step approaches have also been used
    in [[6](#bib.bib6), [36](#bib.bib36), [11](#bib.bib11), [31](#bib.bib31)]. In
    [[31](#bib.bib31)], the authors use two separate branches: one branch receives
    two consecutive frames $(t,t+1)$ and produces context information. The second
    branch produces motion information by receiving two frames that are $k$ steps
    apart (i.e. $t+1$, $t+k$). The outputs of these two branches are fused and fed
    into the final scene generator. The method in [[6](#bib.bib6)] simultaneously
    produces the next future frame and the corresponding optical flow map. In this
    architecture, two additional networks are used: A flow estimator which uses the
    output of the frame generator and the last observation to estimate a flow map
    and a warping layer which performs differential 2D spatial transformation to warp
    the last observed image into the future predicted frame according to the predicted
    flow map.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 光流预测已广泛用于视频预测算法的中间步骤 [[1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [20](#bib.bib20),
    [5](#bib.bib5), [11](#bib.bib11), [25](#bib.bib25), [36](#bib.bib36), [6](#bib.bib6)]。例如，为了处理动态场景中的遮挡，Gao
    等人 [[1](#bib.bib1)] 将光流和像素级预测分解为两个步骤：算法首先预测场景的光流，然后将其与输入帧结合，预测未来。类似的多步骤方法也在 [[6](#bib.bib6),
    [36](#bib.bib36), [11](#bib.bib11), [31](#bib.bib31)] 中使用。在 [[31](#bib.bib31)]
    中，作者使用了两个独立的分支：一个分支接收两个连续帧 $(t,t+1)$ 并生成上下文信息。第二个分支通过接收两个相隔 $k$ 步的帧（即 $t+1$, $t+k$）生成运动信息。这两个分支的输出被融合并输入到最终的场景生成器中。在
    [[6](#bib.bib6)] 中，方法同时生成下一帧和相应的光流图。在该架构中，使用了两个额外的网络：一个流估计器，它利用帧生成器的输出和最后的观察来估计流图，以及一个变形层，它根据预测的流图执行差分二维空间变换，将最后观察到的图像扭曲到未来预测的帧中。
- en: Some algorithms rely on various intermediate steps for video prediction[[17](#bib.bib17),
    [18](#bib.bib18), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [7](#bib.bib7), [14](#bib.bib14), [35](#bib.bib35)]. For instance, the method
    in [[17](#bib.bib17)], reasons about the locations and features of individual
    entities (e.g. cubes) for final scene predictions. Kim et al. [[18](#bib.bib18)]
    first identify keypoints, which may correspond to important structures such as
    joints, and then predict their motion. For videos involving humans, in [[21](#bib.bib21),
    [22](#bib.bib22), [7](#bib.bib7)] the authors identify and reason about the changes
    in poses, and use this information to generate future frames. In [[14](#bib.bib14),
    [35](#bib.bib35)], in addition to raw input images, the differences between consecutive
    frames used in the learning process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Prediction networks can also be provided with additional information to guide
    future frame generation. In [[19](#bib.bib19), [12](#bib.bib12)] an optical flow
    network and in [[26](#bib.bib26), [28](#bib.bib28)] a pose estimation network
    are used in addition to RGB images. Using a CVAE architecture, in [[18](#bib.bib18)]
    the authors use the action lables as conditional input for frame generation. In
    the context of active tasks, e.g. object manipulation with robotic arms, in which
    the consequences of actions influence the future scene configuration, it is common
    to condition the future scene generation on the current or future intended actions
    [[37](#bib.bib37), [29](#bib.bib29), [30](#bib.bib30)].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Summary
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Video prediction algorithms are based on generative models that produce future
    images given a short observation, or in extreme cases a single view of the scene.
    Both recurrent and feedforward models are widely used in the field, with recurrent
    ones being slightly more favorable. The architectural designs and training strategies
    such as the VAEs or GANs are very common. However, it is hard to establish which
    one of these approaches is superior given that the majority of the video prediction
    algorithms are application-agnostic, meaning that they are evaluated on a wide
    range of video datasets with very different characteristics such as traffic scenes,
    activities, games, object manipulations, etc. (more on this in Section [11](#S11
    "11 Datasets ‣ Deep Learning for Vision-based Prediction: A Survey")).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Despite the great progress in the field, video prediction algorithms are still
    facing some major challenges. One of them is the ability to hallucinate, which
    is to generate visual representations for parts of the scenes that were not visible
    during observation phase, e.g. due to occlusions. This is particularly an issue
    for more complex images such as traffic scenes, movies, etc. The complexity of
    the scenes also determines how fast the generated images would degrade. Although
    these algorithms show promising results in simple synthetic videos or action sequences,
    they still struggle in real practical applications. In addition, many of these
    algorithms cannot reason about the expected presence or absence of objects in
    the future. For example, if a moving object is present in the observations and
    is about to exit the field of view in near future, the algorithms account for
    it in the future scenes as long as parts of it are visible in the observation
    stage. This can be an issue for safety-critical applications such as autonomous
    driving in which the presence or absence of traffic elements and the interactions
    between them are essential for action planning.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该领域取得了重大进展，视频预测算法仍面临一些主要挑战。其中之一是生成视觉表现的能力，即对观察阶段不可见的场景部分进行生成，例如由于遮挡。这在复杂图像如交通场景、电影等中尤为突出。场景的复杂性也决定了生成图像的退化速度。虽然这些算法在简单的合成视频或动作序列中显示出有希望的结果，但在实际应用中仍面临困难。此外，许多这些算法无法推测未来对象的预期存在或缺失。例如，如果在观察中存在一个移动物体并且它即将退出视野，算法会在未来的场景中考虑它，只要观察阶段中可见它的一部分。这在安全关键的应用中，如自动驾驶，其中交通元素的存在或缺失以及它们之间的互动对动作规划至关重要，可能会成为问题。
- en: 4 Action prediction
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 动作预测
- en: 'Action prediction algorithms can be categorized into two groups: Next action
    or event prediction (or action anticipation) and early action prediction. In the
    former category, the algorithms use the observation of current activities or scene
    configurations and predict what will happen next. Early action prediction algorithms,
    on the other hand, observe parts of the current action in progress and predict
    what this action is. The classical learning approaches such as Conditional Random
    Fields (CRFs) [[40](#bib.bib40)], Support Vector Machines (SVMs) with hand-crafted
    features [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47)], Markov models [[48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)],
    Bayesian networks [[54](#bib.bib54), [55](#bib.bib55)] and other statistical methods
    [[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62)] have been widely used in recent years. However,
    as mentioned earlier, we will only focus on deep learning approaches.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 动作预测算法可以分为两类：下一步动作或事件预测（或动作预期）和早期动作预测。在前者类别中，算法使用当前活动或场景配置的观察结果来预测接下来会发生什么。另一方面，早期动作预测算法观察当前进行中的部分动作，并预测这个动作是什么。经典的学习方法如条件随机场（CRFs）[[40](#bib.bib40)]，支持向量机（SVMs）与手工特征[[41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47)]，马尔可夫模型[[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)]，贝叶斯网络[[54](#bib.bib54),
    [55](#bib.bib55)]以及其他统计方法[[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62)]在近年来被广泛使用。然而，如前所述，我们将只关注深度学习方法。
- en: 4.1 Action anticipation
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 动作预期
- en: Action prediction algorithms are used in a wide range of applications including
    cooking activities [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)], traffic
    understanding [[71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [73](#bib.bib73),
    [74](#bib.bib74), [75](#bib.bib75), [74](#bib.bib74), [76](#bib.bib76), [77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)], accident prediction [[81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)], sports [[85](#bib.bib85),
    [86](#bib.bib86)] and other forms of activities [[87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91), [8](#bib.bib8), [92](#bib.bib92),
    [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]. Although the majority of
    these algorithms use sequences in which the objects and agents are fully observable,
    a number of methods rely on egocentric scenes [[63](#bib.bib63), [64](#bib.bib64),
    [68](#bib.bib68), [89](#bib.bib89), [85](#bib.bib85), [95](#bib.bib95)] which
    are recorded from the point of view of the acting agents and only parts of their
    bodies (e.g. hands) are observable.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 动作预测算法被广泛应用于各种场景，包括烹饪活动 [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)]，交通理解
    [[71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75), [74](#bib.bib74), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80)]，事故预测 [[81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [84](#bib.bib84)]，体育 [[85](#bib.bib85), [86](#bib.bib86)] 以及其他形式的活动
    [[87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91),
    [8](#bib.bib8), [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]。虽然大多数这些算法使用的是对象和行为者完全可观察的序列，但一些方法依赖于自我中心的场景
    [[63](#bib.bib63), [64](#bib.bib64), [68](#bib.bib68), [89](#bib.bib89), [85](#bib.bib85),
    [95](#bib.bib95)]，这些场景是从行为者的视角记录的，只能观察到他们身体的一部分（例如手）。
- en: 'Action prediction methods predominantly use a variation of RNN-based architectures
    including LSTMs [[87](#bib.bib87), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [74](#bib.bib74), [67](#bib.bib67), [68](#bib.bib68), [82](#bib.bib82), [89](#bib.bib89),
    [75](#bib.bib75), [74](#bib.bib74), [90](#bib.bib90), [91](#bib.bib91), [85](#bib.bib85),
    [84](#bib.bib84), [86](#bib.bib86), [70](#bib.bib70), [92](#bib.bib92), [96](#bib.bib96),
    [79](#bib.bib79), [80](#bib.bib80)], GRUs [[88](#bib.bib88), [71](#bib.bib71),
    [72](#bib.bib72), [69](#bib.bib69)], ConvLSTMs [[76](#bib.bib76)], and Quasi-RNNs
    (QRNNs) [[83](#bib.bib83)]. For instance, in [[88](#bib.bib88), [67](#bib.bib67)]
    the authors use a graph-based RNN architecture in which the nodes represent actions
    and the edges of the graph represent the transitions between the actions. The
    method in [[69](#bib.bib69)] employs a two-step approach: using a recognition
    algorithm, the observed actions and their durations are recognized. These form
    a one-hot encoding vector which is fed into GRUs for the prediction of the future
    activities, their corresponding start time and length. In the context of vehicle
    behavior prediction, Ding et al. [[72](#bib.bib72)] uses a two-stream GRU-based
    architecture to encode the trajectory of two vehicles and a shared activation
    unit to encode the vehicles mutual interactions. Scheel et al. [[97](#bib.bib97)]
    encode the relationship between the ego-vehicle and surrounding vehicles in terms
    of their mutual distances. The vectorized encoding is then fed into a bi-directional
    LSTM. At each time step, the output of the LSTM is classified, using a softmax
    activation, into a binary value indicating whether it is safe for the ego-vehicle
    to change lane. In [[83](#bib.bib83)] the authors use a QRNN network to capture
    the relationships between road users in order to predict the likelihood of a traffic
    accident. To train the model, the authors propose an adaptive loss function that
    assigns penalty weights depending on how early the model can predict accidents.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 行为预测方法主要使用基于RNN的架构变体，包括LSTMs [[87](#bib.bib87), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66), [74](#bib.bib74), [67](#bib.bib67), [68](#bib.bib68), [82](#bib.bib82),
    [89](#bib.bib89), [75](#bib.bib75), [74](#bib.bib74), [90](#bib.bib90), [91](#bib.bib91),
    [85](#bib.bib85), [84](#bib.bib84), [86](#bib.bib86), [70](#bib.bib70), [92](#bib.bib92),
    [96](#bib.bib96), [79](#bib.bib79), [80](#bib.bib80)], GRUs [[88](#bib.bib88),
    [71](#bib.bib71), [72](#bib.bib72), [69](#bib.bib69)], ConvLSTMs [[76](#bib.bib76)]，以及Quasi-RNNs
    (QRNNs) [[83](#bib.bib83)]。例如，在[[88](#bib.bib88), [67](#bib.bib67)]中，作者使用了一种基于图的RNN架构，其中节点代表动作，图的边代表动作之间的转移。[[69](#bib.bib69)]中的方法采用两步法：使用识别算法识别观察到的动作及其持续时间。这些形成一个一热编码向量，并输入到GRUs中，以预测未来的活动、相应的开始时间和长度。在车辆行为预测的背景下，Ding等人[[72](#bib.bib72)]使用基于两流GRU的架构来编码两辆车的轨迹，以及一个共享激活单元来编码车辆之间的相互作用。Scheel等人[[97](#bib.bib97)]将自车与周围车辆之间的关系编码为相互距离。向量化编码随后输入到双向LSTM中。在每个时间步，LSTM的输出使用softmax激活进行分类，得到一个二元值，指示自车是否可以安全变道。在[[83](#bib.bib83)]中，作者使用QRNN网络捕捉道路使用者之间的关系，以预测交通事故的可能性。为了训练模型，作者提出了一种自适应损失函数，根据模型预测事故的提前程度分配惩罚权重。
- en: As an alternative to recurrent architectures, some algorithms use feedforward
    architectures using both 3D [[63](#bib.bib63), [9](#bib.bib9), [73](#bib.bib73)]
    and 2D [[81](#bib.bib81), [69](#bib.bib69), [77](#bib.bib77), [86](#bib.bib86),
    [78](#bib.bib78), [8](#bib.bib8), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]
    convolutional networks. For example, in the context of pedestrian crossing prediction,
    in [[9](#bib.bib9)] the authors use a generative 3D CNN model that produces future
    scenes and is followed by a classifier. The method of [[73](#bib.bib73)] detects
    and tracks pedestrians in the scenes, and then feeds the visual representations
    of the tracks, in the form of an image sequence, into a 3D CNN architecture, which
    directly classifies how likely the pedestrian will cross the road. To predict
    the time of traffic accidents, the method in [[81](#bib.bib81)] processes each
    input image using a 2D CNN model and then combines the representations followed
    by a fully-conntected (fc) layer for prediction. Farha et al. [[69](#bib.bib69)]
    create a 2D matrix by stacking one-hot encodings of actions for each segment of
    observation and use a 2D convolutional net to generate future actions encodings.
    Casas et al. [[77](#bib.bib77)] use a two-stream 2D CNN, each processing the stacked
    voxelized LIDAR scans and the scene map. The feature maps obtained from each stream
    are fused and fed into a backbone network followed by three headers responsible
    for the detection of the vehicles and predicting their intentions and trajectories.
    For sports forecasting, Felsen et al. [[86](#bib.bib86)] concatenate 5 image observations
    channel-wise and feed the resulting output into a 2D CNN network comprised of
    4 convolutional layers and an fc layer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作为递归架构的替代方案，一些算法使用前馈架构，利用了3D [[63](#bib.bib63), [9](#bib.bib9), [73](#bib.bib73)]
    和2D [[81](#bib.bib81), [69](#bib.bib69), [77](#bib.bib77), [86](#bib.bib86), [78](#bib.bib78),
    [8](#bib.bib8), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)] 卷积网络。例如，在行人过马路预测的背景下，[[9](#bib.bib9)]
    中的作者使用生成的3D CNN模型来生成未来场景，并随后使用分类器。[[73](#bib.bib73)] 中的方法检测并跟踪场景中的行人，然后将这些跟踪的视觉表示（以图像序列的形式）输入到3D
    CNN架构中，该架构直接分类行人过马路的可能性。为了预测交通事故的时间，[[81](#bib.bib81)] 中的方法使用2D CNN模型处理每张输入图像，然后结合表示并通过一个全连接（fc）层进行预测。Farha等人[[69](#bib.bib69)]
    通过对每个观察段的动作进行独热编码堆叠，创建一个2D矩阵，并使用2D卷积网络生成未来动作的编码。Casas等人[[77](#bib.bib77)] 使用一个双流2D
    CNN，每个流处理堆叠的体素化LIDAR扫描和场景地图。来自每个流的特征图被融合并输入到一个主干网络中，然后经过三个头部，负责检测车辆及预测其意图和轨迹。在体育预测方面，Felsen等人[[86](#bib.bib86)]
    按通道将5个图像观察结果串联起来，并将得到的输出输入到一个由4个卷积层和一个全连接层组成的2D CNN网络中。
- en: 'Although some algorithms rely on a single source of information, e.g. a set
    of pre-processed features from RGB images [[88](#bib.bib88), [82](#bib.bib82),
    [83](#bib.bib83), [91](#bib.bib91), [84](#bib.bib84), [86](#bib.bib86), [70](#bib.bib70),
    [92](#bib.bib92), [96](#bib.bib96)] or trajectories [[72](#bib.bib72)], many algorithms
    use a multimodal approach by using various sources of information such as optical
    flow maps [[64](#bib.bib64), [67](#bib.bib67), [68](#bib.bib68), [75](#bib.bib75),
    [95](#bib.bib95)], poses [[87](#bib.bib87), [71](#bib.bib71), [67](#bib.bib67),
    [90](#bib.bib90), [80](#bib.bib80)], scene attributes (e.g. road structure, semantics)
    [[87](#bib.bib87), [74](#bib.bib74), [89](#bib.bib89), [77](#bib.bib77)], text
    [[65](#bib.bib65)], action labels [[66](#bib.bib66)], length of actions [[69](#bib.bib69)],
    speed (e.g. ego-vehicle or surrounding agents) [[71](#bib.bib71), [74](#bib.bib74),
    [75](#bib.bib75), [97](#bib.bib97), [85](#bib.bib85)], gaze [[89](#bib.bib89),
    [90](#bib.bib90)], current activities [[78](#bib.bib78)] and the time [[63](#bib.bib63)]
    of the actions. For example, the method in [[65](#bib.bib65)] uses a multi-stream
    LSTM in which two LSTMs encode visual features and cooking recipes and an LSTM
    decodes them for final predictions. To capture the relationships within and between
    sequences, Gammulle et al. [[66](#bib.bib66)] propose a two-stream LSTM network
    with external neural memory units. Each stream is responsible for encoding visual
    features and action labels. In [[71](#bib.bib71)], a multi-layer GRU structure
    is used in which features with different modalities enter the network at different
    levels and are fused with the previous level encodings. The fusion process is
    taking place according to the complexity of the data modality, e.g. more complex
    features such as encodings of pedestrian appearances enter the network at the
    bottom layer, whereas location and speed features enter at the second-last and
    last layers respectively. Farha et al. [[69](#bib.bib69)] use a two-layer stacked
    GRU architecture which receives as input a feature tuple of the length of the
    activity and its corresponding one-hot vector encoding. In [[75](#bib.bib75)],
    the method uses a two-stage architecture: First information regarding the appearance
    of the scene, optical flow (pre-processed using a CNN) and vehicle dynamics are
    fed into individual LSTM units. Then, the output of these units is combined and
    passed through an fc layer to create a representation of the context. This representation
    is used by another LSTM network to predict future traffic actions. In the context
    of human-robot interaction, the authors of [[90](#bib.bib90)] combine the information
    regarding the gaze and pose of the humans using an encoder-decoder LSTM architecture
    to predict their next actions. Jain et al. [[80](#bib.bib80)] use a fusion network
    to combine head pose information of the driver, outside scene features, GPS information,
    and vehicle dynamics to predict the driver’s next action.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Before concluding this section, it is important to discuss the use of attention
    modules which have gained popularity in recent years [[63](#bib.bib63), [87](#bib.bib87),
    [88](#bib.bib88), [64](#bib.bib64), [74](#bib.bib74), [68](#bib.bib68), [82](#bib.bib82),
    [89](#bib.bib89), [84](#bib.bib84), [79](#bib.bib79)]. As the name implies, the
    objective of attention modules is to determine what has to be given more importance
    at a given time. These modules can come in different froms and can be applied
    to different dimensions of data and at various processing. Some of these modules
    are temporal attention [[63](#bib.bib63), [82](#bib.bib82), [89](#bib.bib89)]
    for identifying keyframes, modality attention [[64](#bib.bib64), [68](#bib.bib68)]
    to prioritize between different modalities of data input, spatial attention [[84](#bib.bib84),
    [79](#bib.bib79)] for highlighting the important parts of the scenes, and graph
    attention [[88](#bib.bib88)] for weighting nodes of the graph. In some cases,
    a combination of different attention mechanisms is used [[87](#bib.bib87), [74](#bib.bib74)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结本节内容之前，有必要讨论近年来颇受欢迎的注意力模块的使用[[63](#bib.bib63), [87](#bib.bib87), [88](#bib.bib88),
    [64](#bib.bib64), [74](#bib.bib74), [68](#bib.bib68), [82](#bib.bib82), [89](#bib.bib89),
    [84](#bib.bib84), [79](#bib.bib79)]。顾名思义，注意力模块的目标是确定在特定时间需要给予更多关注的内容。这些模块可以有不同的形式，并可应用于数据的不同维度以及各种处理过程。其中一些模块包括用于识别关键帧的时间注意力[[63](#bib.bib63),
    [82](#bib.bib82), [89](#bib.bib89)]，用于在不同数据输入模态之间进行优先级排序的模态注意力[[64](#bib.bib64),
    [68](#bib.bib68)]，用于突出场景重要部分的空间注意力[[84](#bib.bib84), [79](#bib.bib79)]，以及用于加权图节点的图注意力[[88](#bib.bib88)]。在某些情况下，使用了不同注意力机制的组合[[87](#bib.bib87),
    [74](#bib.bib74)]。
- en: 4.1.1 Summary
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 总结
- en: In the field of action anticipation, RNN architectures are strongly preferred.
    Compared to feedforward algorithms, recurrent methods have the flexibility of
    dealing with variable observation lengths and multi-modal data, in particular,
    when they are significantly different, e.g. trajectories and RGB images. However,
    basic recurrent architectures such as LSTMs and GRUs rely on some forms of pre-processing,
    especially when dealing with high dimensional data such as RGB images, which requires
    the use of various convolutional networks, a process that can be computationally
    costly. Feedforward models, on the other hand, can perform prediction in one shot,
    meaning that they can simultaneously perform temporal reasoning and spatial feature
    generation in a single framework, and as a result, potentially have a shorter
    processing time.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在动作预测领域，RNN 架构受到强烈青睐。与前馈算法相比，递归方法在处理变长观察序列和多模态数据时具有灵活性，特别是当这些数据显著不同，例如轨迹和 RGB
    图像时。然而，基本的递归架构如 LSTM 和 GRU 依赖于某些形式的预处理，尤其是在处理高维数据如 RGB 图像时，这需要使用各种卷积网络，这一过程可能计算开销较大。另一方面，前馈模型可以一蹴而就地进行预测，这意味着它们可以在一个框架内同时进行时间推理和空间特征生成，因此，处理时间可能更短。
- en: Many of the approaches mentioned earlier are generative in nature. They generate
    representations in some feature space and then using these representations predict
    what will happen next. Some algorithms go one step further and generate the actual
    future images and use them for prediction. Although such an approach seems effective
    for single actor events, e.g. cooking scenes, human-robot interaction, it is not
    a feasible approach for multi-agent predictions such as reasoning about behaviors
    of pedestrians or cars in traffic scenes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 许多前面提到的方法本质上是生成性的。它们在某些特征空间中生成表示，然后利用这些表示预测接下来会发生什么。一些算法更进一步，生成实际的未来图像并利用这些图像进行预测。虽然这种方法对于单一演员事件如烹饪场景和人机互动看似有效，但对于多代理预测（例如推理行人或交通场景中汽车的行为）而言，这不是一种可行的方法。
- en: The majority of the methods reviewed in this section use multi-modal data input.
    This seems to be a very effective approach, especially in high dimensional problems
    such as predicting road user behavior where the state (e.g. location and speed)
    of the road user, observer, and other agents, as well as scene structure, lighting
    conditions, and many other factors, play a role in predicting the future behavior.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾的大多数方法使用多模态数据输入。这似乎是一种非常有效的方法，特别是在诸如预测道路用户行为等高维问题中，在这些问题中，道路用户、观察者以及其他代理的状态（例如位置和速度），以及场景结构、光照条件和许多其他因素，都在预测未来行为中发挥作用。
- en: Multi-tasking, e.g. predicting actions and trajectories, are an effective way
    to predict future actions. For instance, trajectories can imply the possibility
    of certain actions, e.g. moving towards the road implies the possibility that
    the person might cross the street. As a result, the simultaneous learning of different
    tasks can be beneficial.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least is the use of attention modules. These modules are deemed
    to be very effective , in particular for tasks with high complexity in terms of
    the modality of input data, the scene structure and temporal relations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Early action prediction
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to action anticipation methods, early action prediction algorithms widely
    use recurrent network architectures [[88](#bib.bib88), [98](#bib.bib98), [99](#bib.bib99),
    [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102), [103](#bib.bib103),
    [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)]. Although many of
    these algorithms use similar aproaches to action anticipation algorithms, some
    propose new approaches. For example, in [[98](#bib.bib98)] the authors use a teacher-student
    learning scheme where the teacher learns to recognize full sequences using a bi-directional
    LSTM and the student relies on partial videos using an LSTM network. They perform
    knowledge distillation by linking feature representations of both networks. Using
    GAN frameworks, the methods in [[99](#bib.bib99), [102](#bib.bib102)] predict
    future feature representations of videos in order to predict actions. Zhao et
    al. [[100](#bib.bib100)] implement a Kalman filter using an LSTM architecture.
    In this method, actions are predcted after observing each frame and corrections
    are made if the next observation provide additional information. The method of
    [[105](#bib.bib105)] uses a two-step LSTM architecture which first generates an
    encoding of the context using context-aware convolutional features and then combines
    these encodings with action-aware convolutional features to predict the action.
    The authors of this method propose a new loss function that penalizes false negatives
    with the same strength at any point in time and false positives with a strength
    that increases linearly over time, to reach the same weight as that on false negatives.
    In [[106](#bib.bib106)] the authors perform coarse-to-fine-grained predictions
    depending on the amount of evidence available for the type of action.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Many early action prediction methods adopt feedforward architectures [[107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109), [104](#bib.bib104), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113)]. The authors of [[107](#bib.bib107)]
    predict actions from a single image by transforming it into a new representation
    called Ranked Saliency Map and Predicted Optical Flow. This representation is
    passed through a 2D convent and fc layers for final prediction. In [[108](#bib.bib108),
    [104](#bib.bib104)], the authors use temporal CNN (TCNNs) architectures, which
    are a series of 1D dilated convolutional layers designed to capture the temporal
    dependencies of feature representations, e.g. in the form of a vector representation
    of poses [[108](#bib.bib108)] or word-embeddings [[104](#bib.bib104)] describing
    the video frames. Chen et al. [[109](#bib.bib109)] use features of body parts
    generated by a CNN model and train an attention module whose objective is to activate
    only features that contribute to the prediction of the action. In [[112](#bib.bib112)],
    the authors use an action detection framework, which incrementally predicts the
    locations of the actors and the action classes based on the current detections.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The majority of the early action prediction algorithms pre-process the entire
    observed scenes using, e.g. different forms of convolutional neural networks [[98](#bib.bib98),
    [100](#bib.bib100), [107](#bib.bib107), [102](#bib.bib102), [104](#bib.bib104),
    [111](#bib.bib111), [105](#bib.bib105), [113](#bib.bib113)] or other forms of
    features [[106](#bib.bib106)]. Some algorithms complement these features using
    optical flow maps [[99](#bib.bib99), [104](#bib.bib104), [112](#bib.bib112)].
    Another group of action prediction methods focuses on specific parts of the observations.
    For example, in [[108](#bib.bib108), [101](#bib.bib101), [103](#bib.bib103), [110](#bib.bib110)]
    the authors use the changes in the poses of actors to predict their actions. The
    method in [[109](#bib.bib109)] uses body parts extracted by cropping a local patch
    around identified joints of the actors. The authors of [[113](#bib.bib113)] only
    use the visual appearances of actors extracted from detected bounding boxes, and
    the relationship between them.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Summary
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Early action detection methods have many commonalities with action anticipation
    algorithms in terms of architectural design. However, there are some exceptions
    that are more applicable in this domain. These exceptions are teacher-student
    training schemes for knowledge distillation, identifying discriminative features,
    and recursive prediction/correction mechanisms. In addition, early action prediction
    algorithm, with a few exceptions, rely on single modal data for prediction and
    rarely use refinement frameworks such as attention modules. Adopting these techniques
    and operating on multi-modal feature spaces can further improve the performance
    of early action prediction algorithms. Unlike the action anticipation methods,
    there is no strong preference for recurrent or feedforward approaches. Some approaches
    take advantage of architectures such as temporal CNNs which are popular in the
    language processing domain and show their effectiveness for early action prediction
    tasks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 5 Trajectory prediction
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name implies, trajectory prediction algorithms predict the future trajectories
    of objects, i.e. the future positions of the objects over time. These approaches
    are particularly popular for applications such as intelligent driving and surveillance.
    Predicted trajectories can be used directly, e.g. in route planning for autonomous
    vehicles, or used for predicting future events, anomalies, or actions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we follow the same routine as before and focus on algorithms
    that have a deep learning component while acknowledging many recent works that
    have used classical approaches including Gaussian mixture models [[114](#bib.bib114),
    [115](#bib.bib115)] and processes [[116](#bib.bib116), [117](#bib.bib117)], Markov
    decision processes (MDPs) [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126)], Markov chains [[127](#bib.bib127), [128](#bib.bib128)]
    and other techniques [[129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory prediction applications like many other sequence prediction tasks
    heavily rely on recurrent architectures such as LSTMs [[137](#bib.bib137), [87](#bib.bib87),
    [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145),
    [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153),
    [154](#bib.bib154), [155](#bib.bib155), [101](#bib.bib101), [156](#bib.bib156),
    [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)],
    and GRUs [[161](#bib.bib161), [162](#bib.bib162), [163](#bib.bib163), [164](#bib.bib164),
    [165](#bib.bib165), [166](#bib.bib166)]. These methods often use an encoder-decoder
    architecture in which a network, e.g. an LSTM encodes single- or multi-modal observations
    of the scenes for some time, and another network generates future trajectories
    given the encoding of the observations. Depending on the complexity of input data,
    these algorithms may rely on some form of pre-processing for generating features
    or embedding mechanisms to minimize the dimensionality of the data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The feedforward algorithms [[161](#bib.bib161), [167](#bib.bib167), [168](#bib.bib168),
    [169](#bib.bib169), [170](#bib.bib170), [171](#bib.bib171), [172](#bib.bib172),
    [173](#bib.bib173), [77](#bib.bib77), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176)] often use whole views of the scenes (i.e. the environment
    and moving objects) and encode them using convolutional layers followed by a regression
    layer to predict trajectories. A few algorithms use hybrid approaches in which
    both convolutional and recurrent reasoning are also used [[177](#bib.bib177),
    [178](#bib.bib178)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the prediction task, algorithms may rely on single- or multi-modal
    observations. For example, in the context of visual surveillance where a fixed
    camera provide a top-down or bird-eye viewing angle, many algorithms only use
    past trajectories of the agents in either actual 2D frame coordinates or their
    velocities calculated by the changes from each time step to another [[178](#bib.bib178),
    [139](#bib.bib139), [143](#bib.bib143), [145](#bib.bib145), [149](#bib.bib149),
    [151](#bib.bib151), [179](#bib.bib179), [153](#bib.bib153), [155](#bib.bib155),
    [164](#bib.bib164), [156](#bib.bib156), [180](#bib.bib180), [152](#bib.bib152),
    [159](#bib.bib159), [160](#bib.bib160)]. In addition to observations of individual
    trajectories of agents, these algorithms focus on modeling the interaction between
    the agents and how they impact each other. For example, Zhang et al. [[139](#bib.bib139)]
    use a state refinement module that aligns all pedestrians in the scene with a
    message passing mechanism that receives as input the current locations of the
    subjects and their encodings from an LSTM unit. In [[143](#bib.bib143)] a graph-based
    approach is used where pedestrians are considered as nodes and the interactions
    between them as edges of the graph. By aggregating information from neighboring
    nodes, the network learns to assign a different level of importance to each node
    for a given subject. The authors of [[153](#bib.bib153), [160](#bib.bib160)] perform
    a pooling operation on the generated representations by sharing the state of individual
    LSTMs that have spatial proximity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: As shown in some works, other sources of information are used in surveilling
    objects [[87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [142](#bib.bib142),
    [146](#bib.bib146), [163](#bib.bib163), [154](#bib.bib154), [101](#bib.bib101),
    [156](#bib.bib156), [158](#bib.bib158), [165](#bib.bib165), [175](#bib.bib175),
    [176](#bib.bib176)]. For example, in addition to encoding the interactions with
    the environment, Liang et al. [[87](#bib.bib87)] use the semantic information
    of the scene as well as changes in the poses of the pedestrians. In [[138](#bib.bib138),
    [140](#bib.bib140), [142](#bib.bib142), [146](#bib.bib146), [163](#bib.bib163),
    [158](#bib.bib158), [165](#bib.bib165), [176](#bib.bib176)] the visual representations
    of the layout of the environment and the appearances of the subjects are included.
    The authors of [[156](#bib.bib156)] use an occupancy map which highlights the
    potential traversable locations for the subjects. The method in [[154](#bib.bib154)]
    takes into account pedestrians’ head orientations to estimate their fields of
    view in order to predict which subjects would potentially interact with one another.
    To predict interactions between humans, in [[101](#bib.bib101)] the authors use
    both poses and trajectories of the agents. Ma et al. [[175](#bib.bib175)] go one
    step further and take into account the pedestrians’ characteristics (e.g. age,
    gender) within a game-theoretic perspective to determine how the trajectory of
    one pedestrians impact each other.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of traffic understanding, predicting trajectories can be more
    challenging due to the fact that there is camera ego-motion involved (e.g. the
    prediction is from the perspective of a moving vehicle), there are interactions
    between different types of objects (e.g. vehicles and pedestrians), and there
    are certain constraints involved such as traffic rules, signals, etc. To achieve
    robustness, many methods in this domain take advantage of multi-modal data for
    trajectory prediction [[177](#bib.bib177), [137](#bib.bib137), [161](#bib.bib161),
    [141](#bib.bib141), [144](#bib.bib144), [162](#bib.bib162), [169](#bib.bib169),
    [147](#bib.bib147), [170](#bib.bib170), [181](#bib.bib181), [182](#bib.bib182),
    [163](#bib.bib163), [150](#bib.bib150), [168](#bib.bib168), [172](#bib.bib172),
    [183](#bib.bib183), [166](#bib.bib166), [173](#bib.bib173), [77](#bib.bib77),
    [165](#bib.bib165)]. In addition to using past trajectories, all these algorithms
    account for the road structure (whether it is from the perspective of the ego-vehicle
    or a top-down view) often in the form of raw visual inputs or, in some cases,
    as an occupancy map [[141](#bib.bib141), [173](#bib.bib173)]. The scene layout
    can implicitly capture the structure of the road, the appearances of the objects
    (e.g. shape) and the dynamics (e.g. velocity or locations of subjects). Such implicit
    information can be further augmented by explicit data such as the shapes of the
    objects (in the case of vehicles) [[177](#bib.bib177)], the speed [[144](#bib.bib144),
    [170](#bib.bib170), [183](#bib.bib183)] and steering angle [[170](#bib.bib170),
    [183](#bib.bib183)] of the ego-vehicle, the distance between the objects [[137](#bib.bib137),
    [182](#bib.bib182)], traffic rules [[182](#bib.bib182)] and signals [[77](#bib.bib77)],
    and kinematic constraints [[174](#bib.bib174)]. For example, the method in [[183](#bib.bib183)]
    uses a two-stream LSTM encoder-decoder scheme: first stream encodes the current
    ego-vehicle’s odometry (steering angle and speed) and the last observation of
    the scene and predicts future odometry of the vehicle. The second stream is a
    trajectory stream that jointly encodes location information of pedestrians and
    the ego-vehicle’s odometry and then combines the encoding with the prediction
    of the odometry stream to predict the future trajectories of the pedestrians.
    The method in [[144](#bib.bib144)] further extends this approach and adds an intention
    prediction stream which outputs how likely the observed pedestrian intends to
    cross the street. The intention likelihood is produced using an LSTM network that
    encodes the dynamics of the pedestrian, their appearances and their surroundings.
    Chandra et al. [[177](#bib.bib177)] create embeddings of contextual information
    by taking into account the shape and velocity of the road users and their spatial
    coordinates within a neighboring region. These embeddings are then fed into some
    LSTM networks followed by a number of convolutional layers to capture the dynamics
    of the scenes. In [[141](#bib.bib141)] the authors use separate LSTMs for encoding
    the trajectories of pedestrians and vehicles (as orientated bounding boxes) and
    then combine them into a unified framework by generating an occupancy map of the
    scene centered at each agent, followed by a pooling operation to capture the interactions
    between different subjects. Lee et al. [[165](#bib.bib165)] predict the future
    trajectories of vehicles in two steps: First, an encoder-decoder GRU architecture
    predicts future trajectories by observing the past ones. Then a refinement network
    adjusts the predicted trajectories by taking into account the contextual information
    in the forms of social interactions, dynamics of the agents involved, and the
    road structure.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Similar to action prediction algorithms, attention modules have been widely
    used in trajectory prediction methods [[87](#bib.bib87), [138](#bib.bib138), [139](#bib.bib139),
    [143](#bib.bib143), [144](#bib.bib144), [146](#bib.bib146), [163](#bib.bib163),
    [152](#bib.bib152), [180](#bib.bib180), [164](#bib.bib164)]. For example, in [[87](#bib.bib87),
    [143](#bib.bib143)], the attention module jointly measures spatial and temporal
    interactions. The authors of [[138](#bib.bib138), [139](#bib.bib139), [146](#bib.bib146),
    [180](#bib.bib180)] propose the use of social attention modules which estimate
    the relative importance of interactions between the subject of interest and its
    neighboring subjects. The method in [[144](#bib.bib144)] uses two attention mechanisms,
    a temporal attention module that measures the importance of each timestep of the
    observed trajectories and a series of self-attention modules which are applied
    to encodings of observations prior to predictions. Xue et al. [[152](#bib.bib152)]
    propose an attention mechanism to measure the relative importance between different
    data modalities, namely the locations and velocities of subjects.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: One of the major challenges in trajectory prediction is to model uncertainty
    of predictions, especially in scenarios where many possibilities exist (e.g. pedestrians
    at intersections). Some algorithms such as [[87](#bib.bib87), [139](#bib.bib139),
    [144](#bib.bib144), [152](#bib.bib152), [153](#bib.bib153), [160](#bib.bib160)]
    train the models by directly measuring the error between the ground truth trajectories
    and predicted ones, e.g. by using an L2 objective function. At the inference time,
    these algorithms generate deterministic predictions. To measure the uncertainty
    of predictions, these models are trained multiple times with randomly initialized
    parameters. Alternatively, uncertainty can be estimated via probabilistic objective
    functions, e.g. Gaussian log-likelihood, as in [[177](#bib.bib177), [161](#bib.bib161),
    [172](#bib.bib172), [142](#bib.bib142), [154](#bib.bib154), [183](#bib.bib183)]
    Instead of a single point in space, these algorithms predict a distribution that
    captures the uncertainty of the predictions. VAEs are another technique that can
    be used to estimate uncertainty [[161](#bib.bib161), [182](#bib.bib182), [163](#bib.bib163),
    [184](#bib.bib184), [165](#bib.bib165)]. Using these methods, at training time
    a posterior distribution over some latent space $z$ is learned by conditioning,
    for example, over future trajectories. At the inference time, a random sample
    is drawn from the latent space for the predictions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Since trajectory prediction algorithms are generative by nature, many approaches
    rely on adversarial training techniques [[178](#bib.bib178), [138](#bib.bib138),
    [140](#bib.bib140), [145](#bib.bib145), [148](#bib.bib148), [149](#bib.bib149),
    [163](#bib.bib163), [153](#bib.bib153), [164](#bib.bib164)] in which at training
    time a discriminator is used to predict whether the generated trajectories are
    real or fake. Kosaraju et al. [[146](#bib.bib146)] extend this approach by using
    two discriminators: A local discriminator which predicts the results on the output
    of the prediction using only past trajectories, and one global discriminator which
    operates on the output of the entire network, i.e. the prediction results based
    on trajectories and scene information.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Summary
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Trajectory prediction is a widely studied field in the computer vision community.
    Although these works dominantly use recurrent network architectures, many approaches,
    such as those used in the field of traffic scene understanding, use feedforward
    networks. Trajectory prediction algorithms rely on one or more sources of information
    such as the past trajectories of subjects, surrounding visual context, object
    attributes, vehicle sensor readings, etc. One factor that is common in many of
    trajectory prediction algorithms is modeling the interactions between dynamic
    or dynamic and static objects. Relationships are captured explicitly or implicitly
    via encoding the scenes as a whole. Like many other prediction approaches, trajectory
    prediction algorithms benefit from various forms of attention mechanisms to learn
    the importance of spatial, temporal or social interactions between objects. To
    model uncertainty, techniques such as probabilistic objectives and variational
    encodings are used.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory prediction algorithms are predominantly rely on past trajectory information
    to predict the future. Although past motion observations are very informative,
    in some context, e.g. traffic scenes, they are simply not enough. There is a need
    for a more explicit encoding of contextual information such as road conditions,
    the subject’s attributes, rules and constraints, scene structure, etc. A number
    of approaches successfully have included a subset of these factors, but a more
    comprehensive approach should be considered.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 6 Motion prediction
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the term “motion prediction” in many cases is used to refer to future
    trajectory prediction, here we only consider the algorithms that are designed
    to predict changes in human pose. Motion prediction play a fundamental role in
    all prediction approaches as an intermediate step, e.g. to reflect how the future
    visual representations would look like or the types of actions to anticipate.
    Like many other prediction applications, this field is dominated by deep learning
    models, even though some methods still rely on classical techniques [[56](#bib.bib56),
    [48](#bib.bib48), [185](#bib.bib185)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Similar to other prediction algorithms, motion prediction methods widely use
    recurrent architectures such as LSTMs [[186](#bib.bib186), [187](#bib.bib187),
    [188](#bib.bib188), [101](#bib.bib101), [103](#bib.bib103), [189](#bib.bib189),
    [26](#bib.bib26)] and GRUs [[190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194), [96](#bib.bib96), [195](#bib.bib195)]
    or a combination of both [[196](#bib.bib196)]. For example, in [[190](#bib.bib190)]
    the authors use a two-layer GRU model in which the top layer operates backward
    to learn noise processes and the bottom level is used to predict the poses given
    the past pose observations and the output of the top layer. Chiu et al. [[187](#bib.bib187)]
    propose a hierarchical LSTM architecture in which each layer of the network encodes
    the observed poses at different time-scales. In the context of 3D pose prediction,
    some algorithms rely on a two-stage process where the visual inputs, either as
    a single image [[189](#bib.bib189)] or a sequence of images [[188](#bib.bib188)],
    are fed into a recurrent network to predict 2D poses of the agent. This is followed
    by a refinement procedure that transforms the 2D poses into 3D.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Some approaches adopt feedforward architectures [[197](#bib.bib197), [198](#bib.bib198),
    [199](#bib.bib199), [200](#bib.bib200), [110](#bib.bib110)]. For example, the
    method in [[198](#bib.bib198)] uses two feedforward networks in a two-stage process.
    First, the input poses are fed into an autoencoder which is comprised of fully
    connected layerss (implemented by 1D convolutions with a kernel size of 1) and
    self-attention blocks. The encodings are then used by multi-level 2D convolutional
    blocks for final predictions. Zhang et al. [[199](#bib.bib199)] predict 3D poses
    from RGB videos. In their method, the images are converted to a feature space
    using a convolutional network, and then the features are used to learn a latent
    3D representation of 3D human dynamics. The representation is used by a network
    to predict future 3D poses. To capture movement patterns, a method proposed in
    [[197](#bib.bib197)] converts poses into a trajectory space using discrete cosine
    transformation. The newly formed representations are then used in a Graph-CNN
    framework to learn the dependencies between different joint trajectories.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: To train motion prediction models, some authors use adversarial training methods
    in which a discriminator is used to classify whether the predicted poses are real
    or fake [[198](#bib.bib198), [193](#bib.bib193)]. The discrimination procedure
    can also be applied to evaluating the continuity, i.e. correct order of, predictions
    as demonstrated in [[192](#bib.bib192)]. In [[196](#bib.bib196)] the discrimination
    score is used to generate a policy for future action predictions in the context
    of imitation learning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Summary
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Motion prediction algorithms primarily focus on the prediction of changes in
    the dynamics (i.e. poses) of observed agents. Such predictions can be fundamental
    to many other applications such as video or trajectory prediction tasks some of
    which were discussed previously.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: In recent works, recurrent network architectures are strongly preferred. The
    architecture of the choice often depends on the representation of the input data,
    e.g. whether joint coordinates are directly used or are encoded into a high-dimensional
    representation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Despite the development of many successful motion prediction algorithms, the
    majority of these methods rely on a single source of information, for example,
    poses or scenes. Encoding higher-level contextual information, such as scene semantics,
    interactions, etc. can potentially result in more robust predictions, as shown
    in other prediction applications. Attention modules also, except for one instance,
    haven’t been adopted within motion prediction algorithms. Given the success of
    using attention in other prediction applications, motion prediction algorithms
    may benefit from the use of attention mechanisms.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 7 Other applications
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of autonomous robotics, some algorithms are designed to predict
    occupancy grid maps (OGMs) that are grayscale representations of the robot’s surroundings
    showing which parts of the environment are traversable. These approaches are often
    object-agnostic and are concerned with generating future OGMs which are used by
    an autonomous agent to perform path planning. In recent years both classical [[201](#bib.bib201),
    [202](#bib.bib202), [203](#bib.bib203), [204](#bib.bib204)] and deep learning
    [[205](#bib.bib205), [206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [209](#bib.bib209)] methods are used. The deep learning approaches, in essence,
    are similar to video prediction methods in which the model receives as input a
    sequence of OGMs and predicts the future ones over some period of time. In this
    context both recurrent [[205](#bib.bib205), [207](#bib.bib207), [209](#bib.bib209)]
    and feedforward [[206](#bib.bib206), [208](#bib.bib208)] methods were common.
    Another group of generative approaches is semantic map prediction algorithms [[10](#bib.bib10),
    [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212)]. These algorithms
    receive as inputs RGB images of the scenes and predict future segmentation maps.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Some of the other vision-based prediction applications include weather [[213](#bib.bib213),
    [214](#bib.bib214)] and Solar irradiance forecasting [[215](#bib.bib215)], steering
    angle prediction [[212](#bib.bib212)], predicting the popularities of tweets based
    on tweeted images used and the users’ histories [[216](#bib.bib216)], forecasting
    fashion trends [[217](#bib.bib217)], storyline prediction [[8](#bib.bib8)], pain
    anticipation [[218](#bib.bib218)], predicting the effect of force after manipulating
    objects [[219](#bib.bib219)], forecasting the winner of Miss Universe given the
    appearances of contestants’ gowns [[220](#bib.bib220)], and predicting election
    results given the facial attributes of candidates [[221](#bib.bib221)]. These
    algorithms rely on a combination of techniques discussed earlier in this paper.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 8 Prediction in other vision applications
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before concluding our discussion on vision-prediction methods, it is worth mentioning
    that prediction techniques are also widely used in other visual processing tasks
    such as video summarization [[222](#bib.bib222)], anomaly detection [[223](#bib.bib223)],
    tracking [[224](#bib.bib224)], active object recognition [[225](#bib.bib225)],
    action detection [[226](#bib.bib226), [227](#bib.bib227)] and recognition [[228](#bib.bib228)].
    For example, tracking algorithms are very closely related to trajectory prediction
    ones and often rely on short term predictions to deal with gaps, e.g. due to occlusions,
    in tracking. For example, in [[224](#bib.bib224)] the method uses a recurrent
    framework to generate future frames in order to localize pedestrians in next frames.
    In the context of action detection, some methods rely on a future frame [[226](#bib.bib226)]
    or trajectory prediction of objects to detect actions [[227](#bib.bib227)]. In
    [[225](#bib.bib225)], a method is used for detecting an object in 3D by relying
    on predicting next best viewing angle of the object. Liu et al. [[223](#bib.bib223)]
    uses a video prediction framework to predict future motion flow maps and images.
    The future predictions that do not conform with expectations will be identified
    as abnormal.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 9 The evaluation of state-of-the-art
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to the evaluation of algorithms, there are two important factors:
    metrics and datasets. They highlight the strengths and weaknesses of the algorithms
    and provide a means to compare the relative performances of the methods. Given
    the importance of these two factors in the design of algorithms, we dedicate the
    following sections to discussing the common metrics and datasets used for vision-based
    prediction tasks. Since the datasets and metrics used in these applications are
    highly diverse, we will focus our discussion on some of the main ones for each
    prediction category while providing visual aids to summarize what the past works
    used for evaluation purposes.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 10 Metrics
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/123c23fed9399d1b22083411fe9de376.png)![Refer to caption](img/5a7e73c63d3321a43fda47325af9d3a0.png)![Refer
    to caption](img/6643993049cdc15ab18e9731688f7e11.png)![Refer to caption](img/422842051f851883ea64ff8a39b50505.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Metrics used in vision-based prediction applications. From left to
    right: Video, action, trajectory and motion prediction.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we follow the same routine as the discussion on the past works
    and divide the metrics into different categories. A summary of the metrics can
    be found in Figure [1](#S10.F1 "Figure 1 ‣ 10 Metrics ‣ Deep Learning for Vision-based
    Prediction: A Survey"). The interested readers can also refer to Appendix [B](#A2
    "Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey") for further information regarding the metrics and the the
    papers that used them. Note while we discuss the metrics in each category and
    we only provide mathematical expressions of the most popular metrics in Appendix
    [C](#A3 "Appendix C Metric formulas ‣ Deep Learning for Vision-based Prediction:
    A Survey").'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Video prediction
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Video prediction is about generating realistic images, hence the best performance
    is achieved when the disparities between the generated images and groundtruth
    images are minimal. The most straightforward way of computing the disparity is
    to measure pixel-wise error using a Mean Square Error (MSE) [[2](#bib.bib2), [4](#bib.bib4),
    [20](#bib.bib20), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12), [229](#bib.bib229),
    [23](#bib.bib23), [14](#bib.bib14), [7](#bib.bib7), [6](#bib.bib6), [27](#bib.bib27),
    [30](#bib.bib30)], which computes average squared intensity differences between
    pixels. Another more popular metric related to MSE is Peak Signal-to-Noise Ratio
    (PSNR) [[2](#bib.bib2), [1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [5](#bib.bib5), [32](#bib.bib32), [33](#bib.bib33),
    [22](#bib.bib22), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12), [229](#bib.bib229),
    [24](#bib.bib24), [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [36](#bib.bib36),
    [6](#bib.bib6), [15](#bib.bib15), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)].
    PSNR is the ratio of the maximum pixel value (i.e. possible signal power), e.g.
    255 in 8-bit images, divided by the MSE (or power of distorting noise) measure
    of two images. The lower the error between two images, the higher the value of
    PSNR, and consequently, the higher the quality of the generated images. Because
    of the wide dynamic range of signals, PSNR value is expressed in the logarithmic
    decibel scale.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Although MSE, and PSNR metrics are easy to calculate, they cannot measure the
    perceived visual quality of a generated image. An alternative metric to address
    this issue is Structural SIMilarity (SSIM) index ([[230](#bib.bib230)]) [[2](#bib.bib2),
    [3](#bib.bib3), [1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [5](#bib.bib5), [32](#bib.bib32), [33](#bib.bib33),
    [22](#bib.bib22), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12), [24](#bib.bib24),
    [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [6](#bib.bib6), [15](#bib.bib15),
    [27](#bib.bib27), [29](#bib.bib29)] which is designed to model image distortion.
    To capture the structural differences between the two images, SSIM separates illumination
    information as it is independent of objects’ structures. As a result, the similarity
    is measured by a combination of three comparisons, namely luminance, contrast,
    and structure.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Higher-level contextual similarities may not be captured by distance measures
    on pixel values. More recently proposed metric, Learned Perceptual Image Patch
    Similarity (LPIPS), ([[231](#bib.bib231)])[[3](#bib.bib3), [17](#bib.bib17), [37](#bib.bib37),
    [11](#bib.bib11)] measures the similarity between two images by comparing internal
    activations of convolutional networks trained for high-level classification tasks.
    The value is calculated by an average L2 distance over normalized deep features.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Some other metrics that have been used in the literature are qualitative human
    judgment [[11](#bib.bib11), [25](#bib.bib25), [8](#bib.bib8), [28](#bib.bib28)],
    Frechet Video Distance (FVD) ([[232](#bib.bib232)]) [[3](#bib.bib3), [18](#bib.bib18)],
    Maximum Mean Discrepancy (MMD) [[26](#bib.bib26)], Inception Scores (IS) ([[233](#bib.bib233)])
    [[26](#bib.bib26)], Binary Cross Entropy (BCE) [[23](#bib.bib23)], L1 [[9](#bib.bib9),
    [12](#bib.bib12)], and Root MSE (RMSE) [[11](#bib.bib11)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Action prediction
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to classification tasks, many action prediction algorithms use accuracy
    measure to report on the performance that is the ratio of the correct predictions
    with respect to the total number of predictions [[56](#bib.bib56), [63](#bib.bib63),
    [64](#bib.bib64), [99](#bib.bib99), [100](#bib.bib100), [66](#bib.bib66), [71](#bib.bib71),
    [9](#bib.bib9), [74](#bib.bib74), [48](#bib.bib48), [49](#bib.bib49), [107](#bib.bib107),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [108](#bib.bib108), [83](#bib.bib83),
    [101](#bib.bib101), [109](#bib.bib109), [89](#bib.bib89), [102](#bib.bib102),
    [75](#bib.bib75), [103](#bib.bib103), [97](#bib.bib97), [76](#bib.bib76), [91](#bib.bib91),
    [110](#bib.bib110), [111](#bib.bib111), [85](#bib.bib85), [105](#bib.bib105),
    [86](#bib.bib86), [70](#bib.bib70), [50](#bib.bib50), [112](#bib.bib112), [8](#bib.bib8),
    [104](#bib.bib104), [93](#bib.bib93), [41](#bib.bib41), [94](#bib.bib94), [52](#bib.bib52),
    [59](#bib.bib59), [42](#bib.bib42), [106](#bib.bib106), [61](#bib.bib61), [113](#bib.bib113),
    [54](#bib.bib54), [43](#bib.bib43), [44](#bib.bib44), [95](#bib.bib95), [62](#bib.bib62),
    [45](#bib.bib45), [55](#bib.bib55), [46](#bib.bib46), [40](#bib.bib40), [47](#bib.bib47)].
    Despite being used widely, accuracy on its own is not a strong indicator of performance,
    especially when we are dealing with class-imbalance data. This is because, for
    example, the model can simply favor the more represented class and predict every
    input as that class. This then would result in a high accuracy measure because
    the metric only considers the ratio of correct predictions. To address these shortcomings,
    some works use complimentary metrics that, in addition to correct predictions,
    account for different types of false predictions. These metrics are precision
    [[63](#bib.bib63), [71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [74](#bib.bib74),
    [67](#bib.bib67), [82](#bib.bib82), [83](#bib.bib83), [70](#bib.bib70), [58](#bib.bib58),
    [51](#bib.bib51), [96](#bib.bib96), [80](#bib.bib80), [52](#bib.bib52), [42](#bib.bib42),
    [53](#bib.bib53)], recall [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65),
    [71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [74](#bib.bib74), [67](#bib.bib67),
    [68](#bib.bib68), [82](#bib.bib82), [83](#bib.bib83), [77](#bib.bib77), [70](#bib.bib70),
    [58](#bib.bib58), [51](#bib.bib51), [96](#bib.bib96), [80](#bib.bib80), [52](#bib.bib52),
    [42](#bib.bib42), [53](#bib.bib53)], and Area Under the Curve (AUC) [[71](#bib.bib71),
    [112](#bib.bib112), [54](#bib.bib54)] of precision-recall graph. Precision and
    recall also form the basis for the calculation of some higher level metrics such
    as F1-score [[71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [83](#bib.bib83),
    [90](#bib.bib90), [58](#bib.bib58), [96](#bib.bib96), [52](#bib.bib52), [42](#bib.bib42)],
    Average Precision (AP) [[88](#bib.bib88), [9](#bib.bib9), [73](#bib.bib73), [82](#bib.bib82),
    [79](#bib.bib79)] and its variations mean AP (mAP) [[87](#bib.bib87), [107](#bib.bib107),
    [83](#bib.bib83), [77](#bib.bib77), [84](#bib.bib84), [78](#bib.bib78), [92](#bib.bib92),
    [112](#bib.bib112)] and calibrated AP (cAP) [[92](#bib.bib92)]. Some of the less
    common performance metrics are Matthews Correlation Coefficient (MCC) [[76](#bib.bib76)],
    False positive (FP)[[53](#bib.bib53)], True Positive Rate (TPR) and False Positive
    Rate (FPR) [[47](#bib.bib47)], Prediction Power (PP) [[57](#bib.bib57)], and Mean
    Reciprocal Rank (MRR) [[44](#bib.bib44)].
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the application, some algorithms evaluate the timing factor in
    terms of the Run Time (RT) of the model [[9](#bib.bib9), [73](#bib.bib73), [43](#bib.bib43)]
    or time of the event, e.g. beginning of the next activity [[70](#bib.bib70)],
    Time To Accident (or collision) (TTA) [[81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [84](#bib.bib84)], and, in the context of driving, Time To Maneuver (TTM) [[74](#bib.bib74),
    [49](#bib.bib49), [96](#bib.bib96), [80](#bib.bib80), [53](#bib.bib53)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Trajectory prediction
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Perhaps the most popular performance measure for trajectory prediction is Average
    Displacement Error (ADE) [[182](#bib.bib182), [171](#bib.bib171), [101](#bib.bib101),
    [234](#bib.bib234), [159](#bib.bib159)] calculated as the average error between
    the prediction location and the ground truth over all time steps. Some methods
    complement ADE measure with its extension Final Displacement Error (FDE) [[177](#bib.bib177),
    [87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [146](#bib.bib146), [163](#bib.bib163),
    [168](#bib.bib168), [172](#bib.bib172), [152](#bib.bib152), [153](#bib.bib153),
    [155](#bib.bib155), [164](#bib.bib164), [180](#bib.bib180), [158](#bib.bib158),
    [160](#bib.bib160)]. As the name suggests, FDE only measures the error between
    the ground truth and the generated trajectory for the final time step.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Many other works use the same metric as ADE [[161](#bib.bib161), [170](#bib.bib170),
    [148](#bib.bib148), [150](#bib.bib150), [129](#bib.bib129), [183](#bib.bib183),
    [184](#bib.bib184), [118](#bib.bib118), [119](#bib.bib119), [121](#bib.bib121),
    [127](#bib.bib127), [115](#bib.bib115), [132](#bib.bib132), [117](#bib.bib117),
    [123](#bib.bib123)] or ADE/FDE [[139](#bib.bib139), [144](#bib.bib144), [169](#bib.bib169),
    [151](#bib.bib151), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [176](#bib.bib176)] without using the same terminology. It is also a common practice
    that instead of using average or final time step measures, to calculate the error
    at different time steps over a period of time [[140](#bib.bib140), [147](#bib.bib147),
    [114](#bib.bib114), [173](#bib.bib173), [130](#bib.bib130), [156](#bib.bib156),
    [120](#bib.bib120), [77](#bib.bib77), [165](#bib.bib165), [124](#bib.bib124),
    [133](#bib.bib133), [125](#bib.bib125), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: To measure displacement error in probablistic trajectory prediction algorithms,
    some works generate a set number of samples and report the best measure (i.e.
    minimum error) [[161](#bib.bib161), [165](#bib.bib165), [178](#bib.bib178), [168](#bib.bib168),
    [162](#bib.bib162), [166](#bib.bib166)] or average over all samples [[174](#bib.bib174),
    [162](#bib.bib162), [166](#bib.bib166)]. Depending on the error metric used, some
    refer to these measures as Minimum ADE/FDE (MinADE/FDE)[[137](#bib.bib137), [178](#bib.bib178),
    [168](#bib.bib168)] (using Euclidean distance) or Mean/Minimum Mean Square Displacement
    (Mean/MinMSD) [[162](#bib.bib162), [166](#bib.bib166)] (using MSE). Some of the
    other probabilistic measures are Log-Likelihood (LL) [[145](#bib.bib145), [181](#bib.bib181),
    [120](#bib.bib120)], Negative Log-Likelihood (NLL) [[172](#bib.bib172), [183](#bib.bib183),
    [174](#bib.bib174), [175](#bib.bib175), [123](#bib.bib123), [125](#bib.bib125)],
    Kullback–Leibler Divergence (KLD) [[181](#bib.bib181), [127](#bib.bib127), [125](#bib.bib125)],
    Negative Log-Probability (NLP)[[118](#bib.bib118), [119](#bib.bib119)], Cross
    Entropy (CE) [[166](#bib.bib166)], Average Prediction Probability (APP) [[157](#bib.bib157)].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Performance can also be evaluated using common classification metrics. For example,
    in [[161](#bib.bib161), [128](#bib.bib128)] Hit Rate (HR) and in [[184](#bib.bib184),
    [165](#bib.bib165)] Miss Rate (MR) metrics are used. In these cases, if the predicted
    trajectory is below (or above) a certain distance threshold from the groundtruth,
    it is considered as a hit or miss. Following a similar approach, some authors
    calculate accuracy [[167](#bib.bib167), [127](#bib.bib127)] or precision [[115](#bib.bib115)]
    of predictions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Some of the other metrics used in the literature are Run Time (RT) [[147](#bib.bib147),
    [118](#bib.bib118), [121](#bib.bib121), [123](#bib.bib123), [135](#bib.bib135)],
    Average Non-linear Displacement Error (ANDE) [[155](#bib.bib155), [160](#bib.bib160)],
    Maximum Distance (MaxD) [[165](#bib.bib165), [184](#bib.bib184)], State collision
    rate (SCR) [[175](#bib.bib175)], Percentage Deviated (PD) [[122](#bib.bib122)],
    Distance to Goal (DtG) [[125](#bib.bib125)], Fraction of Near Misses (FNM) [[126](#bib.bib126)],
    Expected Calibration Error (ECE) [[172](#bib.bib172)], and qualitative (Q) [[116](#bib.bib116)].
    A few works predict the orientations of pedestrians [[154](#bib.bib154), [234](#bib.bib234),
    [131](#bib.bib131)] or vehicles [[77](#bib.bib77)], therefore also report performance
    using Mean angular error (MAnE).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Pitfalls of trajectory prediction metrics
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike video and action prediction fields, performance measures for trajectory
    prediction algorithms are not standardized in terms error metrics used and units
    of measure. For example, for measuring displacement error, although many algorithms
    use Euclidean Distance (ED) (aka L2-distance, L2-norm, Euclidean norm) [[87](#bib.bib87),
    [138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141), [146](#bib.bib146),
    [163](#bib.bib163), [168](#bib.bib168), [172](#bib.bib172), [152](#bib.bib152),
    [153](#bib.bib153), [155](#bib.bib155), [180](#bib.bib180), [158](#bib.bib158),
    [182](#bib.bib182), [234](#bib.bib234), [159](#bib.bib159), [150](#bib.bib150),
    [184](#bib.bib184), [115](#bib.bib115), [139](#bib.bib139), [169](#bib.bib169),
    [151](#bib.bib151), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [114](#bib.bib114), [173](#bib.bib173), [130](#bib.bib130), [156](#bib.bib156),
    [77](#bib.bib77), [165](#bib.bib165), [124](#bib.bib124), [133](#bib.bib133),
    [125](#bib.bib125), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [161](#bib.bib161), [168](#bib.bib168), [155](#bib.bib155), [176](#bib.bib176),
    [125](#bib.bib125)], many others rely on different error metrics including MSE
    [[143](#bib.bib143), [160](#bib.bib160), [171](#bib.bib171), [101](#bib.bib101),
    [170](#bib.bib170), [129](#bib.bib129), [183](#bib.bib183), [127](#bib.bib127),
    [144](#bib.bib144), [162](#bib.bib162), [166](#bib.bib166)], RMSE [[177](#bib.bib177),
    [161](#bib.bib161), [140](#bib.bib140), [170](#bib.bib170), [140](#bib.bib140),
    [147](#bib.bib147)], Weighted RMSE [[120](#bib.bib120)], Mean Absolute Error (MAE)[[77](#bib.bib77),
    [148](#bib.bib148)], Hausdorff Distance (HD)[[174](#bib.bib174)], Modified HD
    (MHD) [[118](#bib.bib118), [119](#bib.bib119), [121](#bib.bib121), [115](#bib.bib115),
    [132](#bib.bib132), [123](#bib.bib123), [125](#bib.bib125)], and discrete Fréchet
    distance (DFD) [[179](#bib.bib179)]. Moreover, trajectory prediction algorithms
    use different units for measuring displacement error. These are meter [[177](#bib.bib177),
    [87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [146](#bib.bib146),
    [168](#bib.bib168), [172](#bib.bib172), [153](#bib.bib153), [180](#bib.bib180),
    [182](#bib.bib182), [161](#bib.bib161), [170](#bib.bib170), [148](#bib.bib148),
    [150](#bib.bib150), [129](#bib.bib129), [121](#bib.bib121), [139](#bib.bib139),
    [169](#bib.bib169), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [140](#bib.bib140), [147](#bib.bib147), [114](#bib.bib114), [173](#bib.bib173),
    [130](#bib.bib130), [156](#bib.bib156), [77](#bib.bib77), [165](#bib.bib165),
    [124](#bib.bib124), [133](#bib.bib133), [135](#bib.bib135), [136](#bib.bib136),
    [161](#bib.bib161), [165](#bib.bib165), [168](#bib.bib168), [166](#bib.bib166)],
    pixel [[138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142),
    [163](#bib.bib163), [152](#bib.bib152), [183](#bib.bib183), [132](#bib.bib132),
    [165](#bib.bib165)], normalized pixel [[142](#bib.bib142), [158](#bib.bib158),
    [125](#bib.bib125)] and feet [[184](#bib.bib184)].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Although such discrepancy between error metrics and units is expected across
    different applications, the problem arises when the proposed works do not specify
    the error metric [[142](#bib.bib142), [178](#bib.bib178)], the unit of measure
    [[143](#bib.bib143), [155](#bib.bib155), [160](#bib.bib160), [171](#bib.bib171),
    [101](#bib.bib101), [118](#bib.bib118), [119](#bib.bib119), [127](#bib.bib127),
    [115](#bib.bib115), [123](#bib.bib123), [151](#bib.bib151), [176](#bib.bib176),
    [120](#bib.bib120), [174](#bib.bib174), [178](#bib.bib178), [155](#bib.bib155)]
    or both [[164](#bib.bib164), [117](#bib.bib117)]. Despite the fact that the reported
    results might imply the choice of the metrics and units, the lack of specification
    can cause erroneous comparisons, specially because many authors use the results
    of previous works directly as reported in the papers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, metric and unit discrepancy exists within the same applications
    and the same error measuring techniques. For instance, in the case of ADE measure,
    this metric is originally proposed in [[235](#bib.bib235)] in terms of ED, and
    was referred to as ADE by the authors of [[160](#bib.bib160)] despite the fact
    that they used MSE instead. This is also apparent in many subsequent works that
    employed ADE measure. For example, the majority of methods use the original metric
    and report the results in terms of ED [[87](#bib.bib87), [138](#bib.bib138), [141](#bib.bib141),
    [152](#bib.bib152), [153](#bib.bib153), [155](#bib.bib155), [146](#bib.bib146),
    [163](#bib.bib163), [168](#bib.bib168), [172](#bib.bib172), [180](#bib.bib180),
    [158](#bib.bib158), [182](#bib.bib182), [234](#bib.bib234), [159](#bib.bib159)]
    whereas some works use MSE [[143](#bib.bib143), [160](#bib.bib160), [171](#bib.bib171),
    [101](#bib.bib101)] and RMSE[[177](#bib.bib177), [140](#bib.bib140)] or do not
    specify the metric [[142](#bib.bib142), [164](#bib.bib164)]. Although the formulation
    of these metrics look similar, they produce different results. ADE using ED, for
    example, is square-root of squared differences averaged over all samples and time
    steps. Unlike ED, in RMSE, the averaging takes place inside square-root operation.
    MSE, on the other hand, is very different from the other two metrics, and does
    not calculate the root of the error. As we can also see in some of the past works,
    this discrepancy may cause confusion about the intended and actual metric that
    is used. For example, in [[140](#bib.bib140)] the authors propose to use MAE metric
    while presenting mathematical formulation of Euclidean distance. The authors of
    [[159](#bib.bib159), [176](#bib.bib176)] make a similar mistake and define ED
    formulation but refer to it as MSE.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: In addition, some algorithms within the same applications and using the same
    datasets use different measuring unit. For instance, in the context of surveillance,
    ETH [[235](#bib.bib235)] is one of the most commonly used datasets. Many works
    use this dataset for benchmarking the performance of their proposed algorithms,
    however, they either use different units, e.g. meter [[87](#bib.bib87), [138](#bib.bib138),
    [139](#bib.bib139), [146](#bib.bib146), [149](#bib.bib149), [153](#bib.bib153),
    [156](#bib.bib156), [180](#bib.bib180)], pixel [[163](#bib.bib163), [140](#bib.bib140),
    [142](#bib.bib142), [152](#bib.bib152)], normalized pixel [[158](#bib.bib158),
    [142](#bib.bib142)], or do not specify the unit used [[143](#bib.bib143), [178](#bib.bib178),
    [151](#bib.bib151), [155](#bib.bib155), [164](#bib.bib164), [160](#bib.bib160)].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, another potential source of error in performance evaluation
    is in the design of the experiments. Taking surveillance applications as an example,
    it is a common practice to evaluate algorithms with 8 frames observations of the
    past and prediction 12 steps in the future [[178](#bib.bib178), [87](#bib.bib87),
    [138](#bib.bib138), [139](#bib.bib139), [153](#bib.bib153), [180](#bib.bib180),
    [160](#bib.bib160)]. However, in some cases the performance of state-of-the-art
    is reported under the standard 8/12 condition, but the proposed algorithms are
    tested under different conditions. For instance, in [[155](#bib.bib155)] the authors
    incorrectly compared the performance of their proposed algorithm using 5 observations
    and 5 predictions with the results of the previous works evaluated under the standard
    8/12 condition.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Motion prediction
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the inherent stochasticity of human body movement, motion prediction
    algorithms often limit their prediction horizon to approximately $500ms$. To measure
    the error between corresponding ground truth and predicted poses, these algorithms
    use mean average error, either in angle space (MAnE) [[190](#bib.bib190), [236](#bib.bib236),
    [197](#bib.bib197), [198](#bib.bib198), [196](#bib.bib196), [187](#bib.bib187),
    [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193), [194](#bib.bib194),
    [96](#bib.bib96), [195](#bib.bib195)] or joint space (MJE) in terms of joint coordinates
    [[236](#bib.bib236), [188](#bib.bib188), [101](#bib.bib101), [103](#bib.bib103),
    [56](#bib.bib56), [103](#bib.bib103), [26](#bib.bib26), [110](#bib.bib110), [187](#bib.bib187),
    [186](#bib.bib186), [200](#bib.bib200)]. In the 3D motion prediction domain, a
    metric known as Mean Per Joint Prediction Error (MPJPE) [[197](#bib.bib197), [199](#bib.bib199)]
    is used which is the error over joints normalized with respect to the root joint.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative to distance error metrics, Percentage of Correct Keypoints
    (PCK) [[199](#bib.bib199), [187](#bib.bib187), [189](#bib.bib189), [188](#bib.bib188)]
    measures how many of the keypoints (e.g. joints) are predicted correctly. The
    correct predictions are those that are below a certain error threshold (e.g. 0.05).
    Some works also use the accuracy metric to report on how well the algorithm can
    localize the position of a particular joint within an error tolerance region [[48](#bib.bib48),
    [195](#bib.bib195)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Other metrics used in the literature include Normalized Power Spectrum Similarity
    (NPSS) [[190](#bib.bib190)], Reconstruction Error (RE) [[199](#bib.bib199)], Limb
    Orientation (LO) [[56](#bib.bib56)], PoSe Entropy (PSEnt), PoSe KL (PSKL) [[198](#bib.bib198)],
    qualitative human judgment [[198](#bib.bib198), [192](#bib.bib192)] and method
    Run Time (RT) [[236](#bib.bib236), [188](#bib.bib188)].
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.1 Pitfalls of motion prediction metrics
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to trajectory methods, motion prediction algorithms are evaluated using
    distance-based methods that calculate the error between pose vectors. In the case
    of MAnE measure, some methods use ED metric [[197](#bib.bib197), [196](#bib.bib196),
    [193](#bib.bib193), [194](#bib.bib194), [96](#bib.bib96), [195](#bib.bib195)]
    while others use MSE [[190](#bib.bib190), [236](#bib.bib236), [198](#bib.bib198),
    [191](#bib.bib191), [192](#bib.bib192)]. Sometimes no metric is specified [[187](#bib.bib187)].
    The same holds for MJE measure where metrics used include MSE [[236](#bib.bib236),
    [101](#bib.bib101), [103](#bib.bib103), [103](#bib.bib103)], RMSE [[188](#bib.bib188),
    [200](#bib.bib200)], ED [[26](#bib.bib26), [110](#bib.bib110), [186](#bib.bib186)],
    MAE [[187](#bib.bib187)], or no metric is specified [[56](#bib.bib56)].
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The added challenge in coordinate-based error measures, e.g. MJE, MPJPE, is
    the error unit. While many approaches do not specify the unit explicitly [[236](#bib.bib236),
    [103](#bib.bib103), [103](#bib.bib103), [26](#bib.bib26), [110](#bib.bib110),
    [187](#bib.bib187), [186](#bib.bib186)], others clearly state whether the unit
    is in pixel [[188](#bib.bib188), [101](#bib.bib101)], centimeter [[56](#bib.bib56)],
    meter [[200](#bib.bib200)] or millimeter [[197](#bib.bib197), [199](#bib.bib199)].
    As was the case before, here many algorithms that benchmark on the same datasets,
    may use different performance metrics, e.g. using popular human datasset Human
    3.6M [[237](#bib.bib237)], MAnE (ED), MAnE (MSE)[[191](#bib.bib191)] and MJE (ED)
    [[193](#bib.bib193)] are used.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Other prediction applications
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending on the task objectives, the metrics used in other prediction applications
    are similar to the ones discussed thus far. For instance, the applications that
    classify future events or outcomes, e.g. contest or an election winner, next image
    index for storytelling, severe weather, and pain, use common metrics such as accuracy
    [[220](#bib.bib220), [221](#bib.bib221), [8](#bib.bib8)], precision, recall [[213](#bib.bib213),
    [218](#bib.bib218)], percentage of correct predictions (PCP) [[219](#bib.bib219)],
    and Matthews correlation coefficient (MCC) [[218](#bib.bib218)] which predicts
    the quality of binary classification by taking into account both false and true
    predictions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Regression based methods, such as temperature, trends, or steering prediction,
    use distance metrics including Euclidean Distance (ED) [[202](#bib.bib202), [238](#bib.bib238)],
    RMSE [[214](#bib.bib214)], MSE [[212](#bib.bib212)], MAE [[239](#bib.bib239),
    [217](#bib.bib217), [204](#bib.bib204)], Mean Absolute Percentage Error (MAPE)
    [[216](#bib.bib216), [217](#bib.bib217)], normalized MAPE (nMAPE) [[215](#bib.bib215)],
    and the Spearman’s ranking Correlation (SRC) [[216](#bib.bib216)] which measures
    the strength and direction of relationship between two variables.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Of particular interest are metrics used for evaluating generative models that
    predict Occupancy Grid Maps (OGMs) and segmentation maps. OGMs are grayscale images
    that highlight the likelihood of a certain region (represented as a cell in the
    grid) that is occupied. The generated map can be compared to ground truth by using
    image similarity metrics such as SSIM [[205](#bib.bib205), [206](#bib.bib206)],
    PSNR [[206](#bib.bib206)] or psi ($\psi$) [[203](#bib.bib203)]. Alternatively,
    OGM can be evaluated using a binary classification metric. Here, the grid cells
    are classified as occupied or free by applying a threshold and then can be evaluated
    as a whole by using metrics such as True Positive (TP), True Negative (TN) [[205](#bib.bib205)],
    Receiver Operator Characteristic (ROC) curve over TP and TN [[207](#bib.bib207),
    [208](#bib.bib208)], F1-score [[201](#bib.bib201), [207](#bib.bib207)], precision,
    recall, and their corresponding AUC [[209](#bib.bib209)]. Given that OGM prediction
    algorithms are mainly used in safety-critical applications such as autonomous
    driving, some algorithms are also evaluated in terms of their Run Time (RT) [[205](#bib.bib205),
    [206](#bib.bib206), [209](#bib.bib209)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Image similarity metrics such as PSNR and SSIM can also be used in the segmentation
    prediction domain [[211](#bib.bib211)]. The most common metric, however, is Intersection
    over Union (IoU)[[10](#bib.bib10), [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212)]
    which measures the average overlap of segmented instances with the ground truth
    segments. In addition, by applying a threshold to IoU scores, the true matches
    can be identified and used to calculate the Average Precision (AP) scores as in
    [[210](#bib.bib210)]. Other metrics used for segmentation prediction tasks include
    EndPoint error (EPE) [[212](#bib.bib212)], Probabilistic Rand Index (RI), Global
    Consistency Error (GCE), and Variation of Information (VoI) [[210](#bib.bib210)].
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 11 Datasets
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Year | Dataset | Type | Annotations | Application |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| V | A | T | M | O |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| 2019 | ARGOVerse [[137](#bib.bib137)] | Traffic | RGB, LIDAR, 3D BB |  |  |
    x |  |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| CARLA [[162](#bib.bib162)] | Traffic (sim) | RGB |  | x |  |  |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| EgoPose [[186](#bib.bib186)] | Pose (ego) | RGB, 3D Pose |  |  |  | x |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| Future Motion (FM) [[167](#bib.bib167)] | Mix | RGB, BB, Attrib. |  |  |
    x |  |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| InstaVariety [[240](#bib.bib240)] | Activities | RGB, BB, Pose |  |  |  |
    x |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| INTEARCTION [[241](#bib.bib241)] | Traffic | Map, Traj. |  |  | x |  |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| Luggage [[81](#bib.bib81)] | Robot | Stereo RGB, BB |  | x |  |  |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| MGIF [[242](#bib.bib242)] | Activities | RGB | x |  |  |  |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| Pedestrian Intention Estimation (PIE) [[144](#bib.bib144)] | Traffic | RGB,
    BB, Class, Attrib., Temporal seg., Vehicle sensors |  | x | x |  |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| nuScenes [[243](#bib.bib243)] | Traffic | RGB, LIDAR, 3D BB, Vehicle sensors
    |  |  | x |  |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| Vehicle-Pedestrian-Mixed (VPM) [[141](#bib.bib141)] | Traffic | RGB, BB |  |  |
    x |  |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| TRAF [[177](#bib.bib177)] | Traffic | RGB, BN, Class, Time-of-day |  |  |
    x |  |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| 2018 | 3D POSES IN THE WILD (3DPW) [[244](#bib.bib244)] | Outdoor | RGB,
    2D/3D Pose, Models |  |  |  | x |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| ActEV/VIRAT [[245](#bib.bib245)] | Surveillance | RGB, BB, Activity, Temporal
    seg. |  | x | x |  |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| ACTICIPATE [[246](#bib.bib246)] | Interaction | RGB, Gaze, Pose |  | x |  |  |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| Atomic Visual Actions (AVA) [[247](#bib.bib247)] | Activities | RGB, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| Epic-Kitchen [[248](#bib.bib248)] | Cooking (ego) | RGB, Audio, BB, Class,
    Text, Temporal seg. |  | x |  |  |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| EGTEA Gaze+ [[249](#bib.bib249)] | Cooking (ego) | RGB, Gaze, Mask, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| ShanghaiTech Campus (STC) [[223](#bib.bib223)] | Surveillance | RGB, Anomaly
    | x |  |  |  |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| ShapeStack [[250](#bib.bib250)] | Objects (sim) | RGBD, Mask, Stability |
    x |  |  |  |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| VIENA [[75](#bib.bib75)] | Traffic (sim) | RGB, Activity, Vehicle sensors
    |  | x |  |  |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| YouCook2 [[251](#bib.bib251)] | Cooking | RGB, Audio, Text, Activity, Temporal
    seg. |  | x |  |  |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| 2017 | BU Action (BUA) [[252](#bib.bib252)] | Activities | RGB (image), Activity
    |  | x |  |  |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| CityPerson [[253](#bib.bib253)] | Traffic | Stereo RGB, BB, Semantic seg.
    |  |  | x |  |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| Epic-Fail [[84](#bib.bib84)] | Risk assessment | RGB, BB, Traj., Temporal
    seg. |  | x |  |  |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| Joint Attention in Autonomous Driving (JAAD) [[78](#bib.bib78)] | Traffic
    | RGB, BB, Attrib., Temporal seg. | x | x | x |  | x |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| L-CAS [[254](#bib.bib254)] | Traffic | LIDAR, 3D BB, Attrib. |  |  | x |  |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| Mouse Fish [[255](#bib.bib255)] | Animals | Depth, 3D Pose |  |  |  | x |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| Oxford Robot Car (ORC) [[256](#bib.bib256)] | Traffic | Stereo RGB, LIDAR,
    Vehicle sensors |  |  | x |  |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| PKU-MMD [[257](#bib.bib257)] | Activities, interactions | RGBD, IR, 3D Pose,
    Multiview, Temporal seg. |  | x |  |  |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| Recipe1M [[258](#bib.bib258)] | Cooking | RGB(image), Text |  | x |  |  |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| STRANDS [[259](#bib.bib259)] | Traffic | RGBD, 3DBB |  |  | x |  |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| 2016 | BAIR Push [[29](#bib.bib29)] | Object manipulation | RGB | x |  |  |  |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| Bouncing Ball (BB) [[260](#bib.bib260)] | Simulation | RGB | x |  |  |  |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| Miss Universe (MU) [[220](#bib.bib220)] | Miss universe | RGB, BB, Scores
    |  |  |  |  | x |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes [[261](#bib.bib261)] | Traffic | Stereo RGB, BB, Semantic seg.,
    Vehicle Sensors | x |  | x |  | x |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| CMU Mocap [[262](#bib.bib262)] | Activities | 3D Pose, Activity |  | x |  |
    x |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| Dashcam Accident Dataset (DAD) [[79](#bib.bib79)] | Traffic, accidents |
    RGB, BB, Class, Temporal seg. |  | x |  |  |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| NTU RGB-D [[263](#bib.bib263)] | Activities | RGBD, IR, 3D Pose, Activity
    |  | x |  |  |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| Ongoing Activity (OA) [[106](#bib.bib106)] | Actvities | RGB, Activity |  |
    x |  |  |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| OAD [[264](#bib.bib264)] | Activities | RGBD, 3D Pose, Activity, Temporal
    seg. |  | x |  |  |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| Stanford Drone (SD) [[265](#bib.bib265)] | Surveillance | RGB, BB, Class
    |  |  | x |  |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| TV Series [[266](#bib.bib266)] | Activities | RGB, Activity, Temporal seg.
    |  | x |  |  |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| Visual StoryTelling (VIST) [[267](#bib.bib267)] | Visual story | RGB, Text
    |  |  |  |  | x |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| Youtube-8M [[268](#bib.bib268)] | Activities | RGB, Activity, Temporal seg.
    | x |  |  |  |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Amazon [[269](#bib.bib269)] | Fashion | Features, Attrib., Text |  |  |  |  |
    x |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| Atari [[30](#bib.bib30)] | Games | RGB | x |  |  |  |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| Brain4Cars [[53](#bib.bib53)] | Traffic, Driver | RGB, BB, Attrib., Temporal
    seg., Vehicle sensors |  | x |  |  |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| CMU Panoptic [[270](#bib.bib270)] | Interaction | RGBD, Multiview, 3D Pose,
    3D facial landmark | x | x |  | x |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| First Person Personalized Activities (FPPA) [[95](#bib.bib95)] | Activities
    (ego) | RGB, Activity, Temporal seg. |  | x |  |  |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze + [[271](#bib.bib271)] | Cooking (ego) | RGB, Gaze, Mask, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| MicroBlog-Images (MBI-1M) [[272](#bib.bib272)] | Tweets | RGB (image), Attrib.,
    Text |  |  |  |  | x |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| MOT [[273](#bib.bib273)] | Surveillance | RGB, BB |  |  | x |  |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| Moving MNIST (MMNIST) [[274](#bib.bib274)] | Digits | Grayscale | x |  |  |  |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| SUN RGB-D [[275](#bib.bib275)] | Places | RGBD, 3D BB , Class |  |  |  |  |
    x |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| SYSU 3DHOI [[276](#bib.bib276)] | Object interaction | RGBD, 3D Pose, Activity
    |  | x |  |  |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| THUMOS [[277](#bib.bib277)] | Activities | RGB, Activity, Temporal seg. |
    x | x |  |  |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| Watch-n-Push (WnP) [[278](#bib.bib278)] | Activities | RGBD, 3D Pose, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| Wider [[279](#bib.bib279)] | Activities | RGB (image), Activity |  | x |  |  |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| 2014 | Breakfast [[280](#bib.bib280)] | Cooking | RGB, Activity, Temporal
    seg. |  | x |  |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| Human3.6M [[237](#bib.bib237)] | Activities | RGB, 3D Pose, Activity | x
    | x |  | x |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| MPII Human Pose [[281](#bib.bib281)] | Activities | RGB, Pose, Activity |  |  |  |
    x |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| Online RGBD Action Dataset (ORGBD) [[282](#bib.bib282)] | Activities | RGBD,
    BB, 3D Pose, Activity |  | x |  |  |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| Sports-1M [[283](#bib.bib283)] | Sports | RGB, Activity | x | x |  |  |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: A summary of common datasets from years 2014-2019 used in vision-based
    prediction applications, namely video (V), action (A), trajectory (T), motion
    (M) and others (O). The annotation column specifies the type of data (e.g. RGB,
    Infrared(IR)) and annotation types. All datasets contain image sequences unless
    specified by “image”. As for annotations, BB stands for bounding box. Attributes
    include any object characteristics (e.g. for pedestrians demographics, behavior).
    Vehicle sensors may include speed, steering angle, GPS, etc. Temporal seg. identifies
    the datasets that specify the start and end of the events.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Dataset | Type | Annotations | Application |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| V | A | T | M | O |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| 2013 | 50Salads [[284](#bib.bib284)] | Cooking (ego) | RGBD, Activity, Temporal
    seg. |  | x |  |  |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| ATC [[285](#bib.bib285)] | Surveillance | RGB, Traj., Attrib. |  |  | x |  |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| CAD-120 [[286](#bib.bib286)] | Activities | RGBD, 3D Pose, Activity |  |
    x |  |  |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| CHUK Avenue [[287](#bib.bib287)] | Surveillance | RGB, BB, Anomaly, Temporal
    seg. | x |  | x |  |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| Daimler Path [[288](#bib.bib288)] | Traffic | Stereo Grayscale, BB, Temporal
    seg. , Vehicle sensors |  | x |  |  |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| Joint-annotated HMDB (JHMDB) [[289](#bib.bib289)] | Activities | RGB, Mask,
    Activity, Pose, Optical flow | x | x |  |  |  |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| Penn Action [[290](#bib.bib290)] | Activities | RGB, BB, Pose, Activity |
    x |  |  | x |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| 2012 | BIT [[291](#bib.bib291)] | Interaction | RGB, Activity |  | x |  |  |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze [[292](#bib.bib292)] | Cooking (ego) | RGB, Gaze, Mask, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| KITTI [[293](#bib.bib293)] | Traffic | Stereo RGB, LIDAR, BB, Optical flow,
    Vehicle sensors | x |  | x |  | x |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| MANIAC [[294](#bib.bib294)] | Object manipulation | RGBD, Semantic seg.,
    Activity |  | x |  |  |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| MPII-Cooking [[295](#bib.bib295)] | Cooking | RGB, 3D Pose, Activity, Temporal
    seg. |  | x |  |  |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| MSR Daily Activity (MSRDA) [[296](#bib.bib296)] | Activities | Depth, Activity
    |  | x |  |  |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| New York Grand Central (GC) [[297](#bib.bib297)] | Surveillance | RGB, Traj.
    |  |  | x |  |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| SBU Kinetic Interction (SBUKI) [[298](#bib.bib298)] | Interaction | RGBD,
    3D Pose, Activity |  | x | x | x |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| UCF-101 [[299](#bib.bib299)] | Activities | RGB, Activity | x | x |  | x
    |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| UTKinect-Action (UTKA) [[300](#bib.bib300)] | Activities | RGBD, 3D Pose,
    Activity, Temporal seg. |  | x |  |  |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| UvA-NEMO [[301](#bib.bib301)] | Smiles | RGB | x |  |  |  |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| 2011 | Ford campus vision LiDAR (FCVL) [[302](#bib.bib302)] | Traffic | RGB,
    LIDAR, Vehicle sensors |  |  |  |  | x |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| Human Motion Database (HMDB) [[303](#bib.bib303)] | Activities | RGB, BB,
    Mask, Activity |  | x |  |  |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| Stanford 40 [[304](#bib.bib304)] | Activities | RGB (image), BB, Activity
    |  | x |  |  |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| Town Center [[305](#bib.bib305)] | Surveillance | RGB, BB |  |  | x |  |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| VIRAT [[306](#bib.bib306)] | Surveillance, Activities | RGB, BB, Activity,
    Temporal seg. |  | x | x |  |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| 2010 | DISPLECS [[307](#bib.bib307)] | Traffic | RGB, Vehicle sensors |  |  |  |  |
    x |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| MSR [[308](#bib.bib308)] | Activities | Depth, Activity | x |  |  |  |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| MUG [[309](#bib.bib309)] | Facial expressions | RGB, Keypoints, Label | x
    |  |  |  |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| PROST [[310](#bib.bib310)] | Objects | RGB, BB | x |  |  |  |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| TV Human Interaction (THI) [[311](#bib.bib311)] | Interaction | RGB, BB,
    Head pose, Activity |  | x |  |  |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| UT Interaction (UTI) [[312](#bib.bib312)] | Interaction | RGB, BB, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| VISOR [[313](#bib.bib313)] | Surveillance | RGB, BB, Pose, Attrib. | x |  |  |  |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| Willow Action [[314](#bib.bib314)] | Activiites | RGB (image), Activity |  |
    x |  |  |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| 2009 | Caltech Pedestrian [[315](#bib.bib315)] | Traffic | RGB, BB | x |
    x |  |  |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| Collective Activity (CA) [[316](#bib.bib316)] | Interaction | RGB, BB, Attrib.,
    Activity, Temporal seg. |  | x | x | x |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| Edinburgh IFP [[317](#bib.bib317)] | Surveillance | RGB, BB |  |  | x |  |  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| ETH [[235](#bib.bib235)] | Surveillance | RGB, Traj. |  |  | x |  |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| OSU [[318](#bib.bib318)] | Sports | RGB, BB, Attrib. |  |  | x |  |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| PETS2009 [[319](#bib.bib319)] | Surveillance | RGB, BB |  |  | x |  |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| QMUL [[320](#bib.bib320)] | Traffic, anomaly | RGB, Traj. |  |  | x |  |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| TUM Kitchen [[321](#bib.bib321)] | Activities | RGB, RFID, 3D Pose, Activity,
    Temporal seg. |  |  | x |  |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| YUV Videos [[322](#bib.bib322)] | Mix videos | RGB | x |  |  |  |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| 2008 | Daimler [[323](#bib.bib323)] | Traffic | Grayscale, BB |  | x |  |  |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| MIT Trajectory (MITT) [[324](#bib.bib324)] | Surveillance | RGB, Traj. |  |  |
    x |  |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| 2007 | AMOS [[325](#bib.bib325)] | Weather | RGB, Temperature, Time |  |  |  |  |
    x |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| ETH Pedestrian [[326](#bib.bib326)] | Traffic | RGB, BB |  | x |  |  |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| Lankershim Boulevard [[327](#bib.bib327)] | Traffic | RGB, Traj. |  |  |
    x |  |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| Next Generation Simulation (NGSIM) [[328](#bib.bib328)] | Traffic | Map,
    Traj. |  | x | x |  |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| UCY [[329](#bib.bib329)] | Surveillance | RGB, Traj., Gaze |  |  | x |  |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| 2006 | Tuscan Arizona [[330](#bib.bib330)] | Weather | RGB |  |  |  |  |
    x |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| 2004 | KTH [[331](#bib.bib331)] | Activities | Grayscale, Activity | x |  |  |  |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| 1981 | Golden Colorado [[332](#bib.bib332)] | Weather | RGB |  |  |  |  |
    x |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: 'TABLE II: A summary of common datasets from years 2013 and earlier used in
    vision-based prediction applications, namely video (V), action (A), trajectory
    (T), motion (M) and others (O). The annotation column specifies the type of data
    (e.g. RGB, Infrared(IR)) and annotation types. All datasets contain sequences
    unless specified by “image”. As for annotations, BB stands for bounding box. Attributes
    include any object characteristics (e.g. for pedestrians demographics, behavior).
    Vehicle sensors may include speed, steering angle, GPS, etc. Temporal seg. identifies
    the datasets that specify the start and end of the events.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/98db438ea0b637bb5c4683dc3b666846.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An illustration of datasets and papers that use them.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'We have identified more than 100 datasets that are used in the vision-based
    prediction literature. Discussing all datasets in detail is beyond the scope of
    this paper. We provide a summary of the datasets and their characteristics in
    Tables [I](#S11.T1 "TABLE I ‣ 11 Datasets ‣ Deep Learning for Vision-based Prediction:
    A Survey") and [II](#S11.T2 "TABLE II ‣ 11 Datasets ‣ Deep Learning for Vision-based
    Prediction: A Survey") and briefly discuss more popular datasets in each field.
    Figure [2](#S11.F2 "Figure 2 ‣ 11 Datasets ‣ Deep Learning for Vision-based Prediction:
    A Survey") illustrates the list of papers and corresponding datasets used for
    evaluation. Note that the papers that do not use publicly available datasets are
    not listed in this figure. For further information, the readers can also refer
    to Appendices [D](#A4 "Appendix D Links to the datasets ‣ Deep Learning for Vision-based
    Prediction: A Survey") and [E](#A5 "Appendix E Datasets and corresponding papers
    ‣ Deep Learning for Vision-based Prediction: A Survey").'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Video prediction. Almost any forms of sequential RGB images can be used for
    evaluation of video prediction algorithms. Among the most common datasets are
    traffic datasets such a KITTI [[293](#bib.bib293)], and Caltech Pedestrians [[315](#bib.bib315)].
    KITTI is a dataset recorded from inside of a vehicle and contains images of urban
    roads annotated with bounding box information. It also contains depth maps, LIDAR
    point clouds and semantic segmentation maps. Caltech Pedestrian is a similar dataset
    with the difference of only containing RGB images and bounding boxes for pedestrians.
    It also contains occlusion bounding boxes highlighting the visible portions of
    the pedestrians. Activity datasets such as UCF-101 [[299](#bib.bib299)] and Human3.6M
    [[237](#bib.bib237)] are also widely used. UCF-101 contains videos of various
    types of activities such as sports, applying makeup, playing music instruments
    annotated with activity labels per video. Human3.6M consists of 3.6 million 3D
    human poses and corresponding images recorded from 11 professional actors. This
    dataset contains 17 generic scenarios such as discussion, smoking, and taking
    photos.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Action prediction. The algorithms in this domain are evaluated on a wide range
    of datasets. For anticipation tasks, traffic datasets such as Next Generation
    Simulation (NGSIM) [[328](#bib.bib328)] and Joint Attention in Autonomous Driving
    (JAAD) [[78](#bib.bib78)] are used. NGSIM contains trajectories of vehicles driving
    on highways in the United States. The trajectories are accompanied by the top-down
    views of the corresponding road structures. The JAAD dataset contains videos of
    pedestrians crossing the road recorded using an on-board camera. This dataset
    contains the frame-wise pedestrian bounding boxes, and action labels as well as
    pedestrians’ and roads’ attributes. A similar dataset to JAAD is Pedestrian Intention
    Estimation (PIE) [[144](#bib.bib144)] which, in addition, provides the ego-vehicle
    sensor data and spatial annotations for traffic elements.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Another popular category of datasets in this domain is those containing videos
    of cooking activities. These datasets are Epic-Kitchen [[248](#bib.bib248)], 50salads
    [[284](#bib.bib284)], Breakfast [[280](#bib.bib280)] and MPII-Cooking [[295](#bib.bib295)].
    These datasets contain videos showing sequences of different cooking actions of
    preparing meals. All videos in the datasets have temporal segments with corresponding
    activity labels. Some datasets also provide additional annotations such as object
    bounding boxes, voice and text in Epic-Kitchen, and the poses of the actors in
    MPII-Cooking.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Early action prediction works widely use the popular UCF-101 dataset [[299](#bib.bib299)]
    and interaction datasets such as UT Interaction (UTI) [[312](#bib.bib312)] and
    BIT [[291](#bib.bib291)]. UTI and BIT contain videos of people engaged in interaction
    with the corresponding label for the types of interactions. In addition, UTI has
    the added temporal segment annotations detailing different stages of interactions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory prediction. The most common datasets in this domain are ETH [[235](#bib.bib235)]
    and UCY [[329](#bib.bib329)] which contain surveillance videos of pedestrians
    walking on sidewalks annotated with their position coordinates. UCY also provides
    the gaze directions to capture the viewing angle of pedestrians. Another popular
    dataset is Stanford Aerial Pedestrian (SAP), also known as Stanford Drone (SD)
    [[265](#bib.bib265)]. This dataset has the footage of road users from a top-down
    view recorded by a drone. The annotations include bounding boxes and object class
    labels.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Motion prediction. The algorithms in this domain are mainly evaluated on the
    widely popular dataset Human 3.6M [[237](#bib.bib237)] described earlier. This
    dataset is particularly suitable for these applications because it contains accurate
    3D poses of the actors recorded by a high-speed motion capture system. Using this
    dataset, the background can be accurately removed allowing the algorithms to focus
    purely on changes in the poses. Another popular dataset in this field is Penn
    Action [[290](#bib.bib290)] which contains RGB videos of various activities with
    corresponding activity labels and poses of the actors involved.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Other applications. The most notable datasets are KITTI [[293](#bib.bib293)]
    which is used by the OGM prediction algorithms and CityScapes [[261](#bib.bib261)]
    that is used by the segmentation prediction algorithms. CityScapes contains video
    footage of urban environments recorded by an on-board camera. The data is annotated
    with semantic masks of the traffic objects and corresponding bounding boxes.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 12 Summary and Discussion
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 12.1 Architecture
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many factors that define the choice of architecture for vision-based
    prediction tasks. These factors include the types of input data and expected output,
    computational efficiency, application-specific constrains, etc. For instance,
    in terms of network choice, whether it is feedforward and recurrent, no preference
    is observed in video applications. However, in the case of action, trajectory
    and motion predictions, recurrent architectures are strongly preferred. This can
    be due to the fact that these applications often rely on multi-modal data which
    can be combined easier in a recurrent framework. In the case of trajectory prediction,
    recurrent architectures give the flexibility of varying observation and prediction
    lengths without the need for architectural modifications.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks (GANs) are widely used in video prediction applications
    and to some extent in trajectory prediction methods. Some of the major challenges
    using generative models is to deal with inherent uncertainty of future representations,
    in particular, this an issue in the context of trajectory prediction due to high
    unpredictability of human movement. To remedy this issue and to capture uncertainty
    of movement, techniques such as variational auto encoders, in which uncertainty
    is modeled as a latent distribution, and the use of probabilistic objective functions
    are common.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: A more recent trend in the field of vision-based prediction (and perhaps in
    other computer vision applications) is the use of attention modules. These modules
    can be applied at spatial or temporal level or even to adjust the impact of different
    modalities of data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Data representation and processing
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The type of data and methods of processing vary across different applications.
    For instance, video applications mainly rely on images but also take advantage
    of alternative representations, such as optical flow, poses, object-based keypoints,
    and report improved results. Similarly, many action prediction algorithms use
    different sources of information such as optical flow, poses, scene attributes,
    text, complementary sensor readings (e.g. speed in vehicles), gaze and time of
    the actions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory prediction algorithms, predominantly rely on trajectory information,
    with some exceptions that use scene layouts, complimentary sensors’ readings or
    other constrains. One of the main applications in this domain, in particular surveillance,
    is modeling the social interaction between the dynamic agents. Unlike other vision-based
    applications, motion prediction algorithms are mainly single-modal and use only
    poses and the images of agents as inputs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Evaluation
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 12.3.1 Metrics
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Metrics may vary across different applications of vision-based prediction. Video
    prediction algorithms, for instance, are mainly evaluated using MSE, SSIM, and
    PSNR, whereas in the case of action prediction algorithms the main metrics are
    accuracy, precision, and recall. Trajectory prediction works often measure the
    average distance (ADE) or final distance (FDE) between the actual and predicted
    locations of the agents. The models with probabilistic outputs are also evaluated
    using NLL and KLD metrics. Distance-based metrics are used in motion prediction
    methods where the error in joint prediction is either calculated on average (MJE)
    or per joint (MPJPE). In addition, joint accuracy can be reported in terms of
    the percentage of correct prediction using PCK metric. In this case, a tolerance
    threshold is defined to determine whether a predicted joint is correct.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: While calculating performance error for video and action prediction algorithms
    are fairly standardized, there are major discrepancies across different works
    in the way error is computed for trajectory and motion prediction algorithms.
    For example, in trajectory prediction, distance error is calculated by using metrics
    such as MSE, RMSE, ED, etc. and units such as pixels and meters. Such discrepancy,
    and the fact that many works omit mentioning the choice of error metrics and units,
    increases the chance of incorrect comparisons between models.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 Datasets
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The choice of datasets depends on the objective of the applications. For example,
    action prediction algorithms for cooking activities are evaluated on datasets
    such as Epic-Kitchen, 50Salads, Breakfast, and MPII-Cooking and the ones for traffic
    events evaluated on JAAD, NSGIM, and PIE. Similarly, trajectory prediction works
    for surveillance widely use UCY, ETH, and SD and for traffic NGSIM. Motion prediction
    algorithms are more focusing on individual movements in diverse context, therefore
    predominantly use Human3.6M and Penn Action datasets.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Compared to other applications, video prediction is an exception. The algorithms
    in this group are evaluated on almost any datasets with video content. The algorithms
    in this domain are often task agnostic meaning that the same approaches are evaluated
    on datasets with traffic scenes (e.g. KITTI, Caltech Pedestrian), general activities
    (e.g. UCF-101, Penn Action), basic actions (e.g. Human3.6M, KTH) and synthetic
    data (e.g. MMNIST, Atari games). Although such generalizability across different
    domains is a desirable feature in video prediction algorithms, it is often the
    case that the reason behind the choice of datasets is not discussed raising the
    question of whether the decision for selecting particular datasets is motivated
    by the limitations of the algorithms.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 What’s next
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years we have witnessed an exponential growth in the number of works
    published in the field of vision-based prediction. There are still, however, many
    open research problems in the field that need to be addressed.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The ability to hallucinate or generate parts of the image that were not previously
    observed is still a major challenge in video prediction applications. In addition,
    the algorithms in this domain cannot deal with cases where some objects go out
    of view in future time frames. The performances of action prediction algorithms
    are still sub-optimal, especially in safety critical and complex tasks such as
    event prediction in traffic scenes. To make predictions in such cases, many modalities
    of data and the relationships between them should be considered which is often
    not the case in the proposed approaches.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory prediction algorithms mainly rely on changes in the location of the
    agents to predict their future states. Although, this might be an effective approach
    for tasks such as surveillance, in many other cases it might not be sufficient.
    For example, in order to predict trajectories of pedestrians in traffic scenes,
    many other sources of information, such as their poses and orientation, road structure,
    interactions, road conditions, traffic flow, etc., are potentially relevant. Such
    contextual analysis can also be beneficial for motion prediction algorithms which
    manly rely on the changes in poses to predict the future.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the choice of learning architectures and training schemes, a systematic
    comparison of different approaches, e.g. using feedforward vs recurrent networks,
    the benefits of using adversarial training schemes, various uncertainty modeling
    approaches, etc. is missing. Such information can be partially extracted from
    the existing literature, however, in many cases it is not possible due to the
    lack of standard evaluation procedures and metrics, unavailability of corresponding
    implementation code and the datasets used for comparison.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] H. Gao, H. Xu, Q.-Z. Cai, R. Wang, F. Yu, and T. Darrell, “Disentangling
    propagation and generation for video prediction,” in *ICCV*, 2019.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y.-H. Kwon and M.-G. Park, “Predicting future frames using retrospective
    cycle gan,” in *CVPR*, 2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] L. Castrejon, N. Ballas, and A. Courville, “Improved conditional vrnns
    for video prediction,” in *ICCV*, 2019.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Y.-H. Ho, C.-Y. Cho, W.-H. Peng, and G.-L. Jin, “Sme-net: Sparse motion
    estimation for parametric video prediction through reinforcement learning,” in
    *ICCV*, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] C. Zhang, T. Chen, H. Liu, Q. Shen, and Z. Ma, “Looking-ahead: Neural future
    video frame prediction,” in *ICIP*, 2019.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] X. Liang, L. Lee, W. Dai, and E. P. Xing, “Dual motion gan for future-flow
    embedded video prediction,” in *ICCV*, 2017.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. Ji, Z. Wei, E. Dunn, and J. M. Frahm, “Dynamic visual sequence prediction
    with motion flow networks,” in *WACV*, 2018.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] K.-H. Zeng, W. B. Shen, D.-A. Huang, M. Sun, and J. Carlos Niebles, “Visual
    forecasting by imitating dynamics in natural sequences,” in *ICCV*, 2017.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] P. Gujjar and R. Vaughan, “Classifying pedestrian actions in advance using
    predicted video of urban driving scenes,” in *ICRA*, 2019.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Terwilliger, G. Brazil, and X. Liu, “Recurrent flow-guided semantic
    forecasting,” in *WACV*, 2019.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang, “Flow-grounded
    spatial-temporal video prediction from still images,” in *ECCV*, 2018.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] F. A. Reda, G. Liu, K. J. Shih, R. Kirby, J. Barker, D. Tarjan, A. Tao,
    and B. Catanzaro, “Sdc-net: Video prediction using spatially-displaced convolution,”
    in *ECCV*, 2018.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] P. Bhattacharjee and S. Das, “Predicting video frames using feature based
    locally guided objectives,” in *ACCV*, C. Jawahar, H. Li, G. Mori, and K. Schindler,
    Eds., 2019.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] G. Ying, Y. Zou, L. Wan, Y. Hu, and J. Feng, “Better guider predicts future
    better: Difference guided generative adversarial networks,” in *ACCV*, C. Jawahar,
    H. Li, G. Mori, and K. Schindler, Eds., 2018.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] P. Bhattacharjee and S. Das, “Temporal coherency based criteria for predicting
    video frames using deep multi-stage generative adversarial networks,” in *NeurIPS*,
    2017.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Oliu, J. Selva, and S. Escalera, “Folded recurrent neural networks
    for future video prediction,” in *ECCV*, 2018.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y. Ye, M. Singh, A. Gupta, and S. Tulsiani, “Compositional video prediction,”
    in *ICCV*, 2019.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Y. Kim, S. Nam, I. Cho, and S. J. Kim, “Unsupervised keypoint learning
    for guiding class-conditional video prediction,” in *NeurIPS*, 2019.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Wang, B. Hu, Y. Long, and Y. Guan, “Order matters: Shuffling sequence
    generation for video prediction,” in *BMVC*, 2019.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Y. Ho, C. Cho, and W. Peng, “Deep reinforcement learning for video prediction,”
    in *ICIP*, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. Tang, H. Hu, Q. Zhou, H. Shan, C. Tian, and T. Q. S. Quek, “Pose guided
    global and local gan for appearance preserving human video prediction,” in *ICIP*,
    2019.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Cai, C. Bai, Y.-W. Tai, and C.-K. Tang, “Deep video generation, prediction
    and completion of human action sequences,” in *ECCV*, 2018.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J.-T. Hsieh, B. Liu, D.-A. Huang, L. F. Fei-Fei, and J. C. Niebles, “Learning
    to decompose and disentangle representations for video prediction,” in *NeurIPS*,
    2018.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Xu, B. Ni, and X. Yang, “Video prediction via selective sampling,”
    in *NeurIPS*, 2018.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] N. Wichers, R. Villegas, D. Erhan, and H. Lee, “Hierarchical long-term
    video prediction without supervision,” in *ICML*, 2018.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. Walker, K. Marino, A. Gupta, and M. Hebert, “The pose knows: Video
    forecasting by generating pose futures,” in *ICCV*, 2017.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Wang, M. Long, J. Wang, Z. Gao, and P. S. Yu, “Predrnn: Recurrent neural
    networks for predictive learning using spatiotemporal lstms,” in *NeurIPS*, 2017.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] R. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and H. Lee, “Learning to
    generate long-term future via hierarchical prediction,” in *ICML*, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] C. Finn, I. Goodfellow, and S. Levine, “Unsupervised learning for physical
    interaction through video prediction,” in *NeurIPS*, 2016.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh, “Action-conditional
    video prediction using deep networks in atari games,” in *NeurIPS*, 2015.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Lee, J. Lee, S. Lee, and S. Yoon, “Mutual suppression network for video
    prediction using disentangled features,” in *BMVC*, 2019.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J. Xu, B. Ni, Z. Li, S. Cheng, and X. Yang, “Structure preserving video
    prediction,” in *CVPR*, 2018.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] W. Byeon, Q. Wang, R. Kumar Srivastava, and P. Koumoutsakos, “Contextvp:
    Fully context-aware video prediction,” in *ECCV*, 2018.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] W. Liu, A. Sharma, O. Camps, and M. Sznaier, “Dyan: A dynamical atoms-based
    network for video prediction,” in *ECCV*, 2018.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] B. Jin, Y. Hu, Y. Zeng, Q. Tang, S. Liu, and J. Ye, “Varnet: Exploring
    variations for unsupervised video prediction,” in *IROS*, 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] C. Lu, M. Hirsch, and B. Scholkopf, “Flexible spatio-temporal networks
    for video prediction,” in *CVPR*, 2017.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. Jung, T. Matsumoto, and J. Tani, “Goal-directed behavior under variational
    predictive coding: Dynamic organization of visual attention and working memory,”
    in *IROS*, 2019.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv:1312.6114*,
    2013.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic backpropagation
    and approximate inference in deep generative models,” *arXiv:1401.4082*, 2014.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. T. Schulz and R. Stiefelhagen, “Pedestrian intention recognition using
    latent-dynamic conditional random fields,” in *IV*, 2015.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J.-F. Hu, W.-S. Zheng, L. Ma, G. Wang, and J. Lai, “Real-time rgb-d activity
    prediction by soft regression,” in *ECCV*, B. Leibe, J. Matas, N. Sebe, and M. Welling,
    Eds., 2016.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] F. Schneemann and P. Heinemann, “Context-based detection of pedestrian
    crossing intention for autonomous driving in urban environments,” in *IROS*, 2016.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] B. Völz, K. Behrendt, H. Mielenz, I. Gilitschenski, R. Siegwart, and J. Nieto,
    “A data-driven approach for pedestrian intention estimation,” in *ITSC*, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Z. Xu, L. Qing, and J. Miao, “Activity auto-completion: Predicting human
    activities from partial videos,” in *ICCV*, 2015.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] H. Zhang and L. E. Parker, “Bio-inspired predictive orientation decomposition
    of skeleton trajectories for real-time human activity prediction,” in *ICRA*,
    2015.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Köhler, M. Goldhammer, K. Zindler, K. Doll, and K. Dietmeyer, “Stereo-vision-based
    pedestrian’s intention detection in a moving vehicle,” in *ITSC*, 2015.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] B. Völz, H. Mielenz, G. Agamennoni, and R. Siegwart, “Feature relevance
    estimation for learning pedestrian behavior at crosswalks,” in *ITSC*, 2015.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] R. C. Luo and L. Mai, “Human intention inference and on-line human hand
    motion prediction for human-robot collaboration,” in *IROS*, 2019.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Wu, T. Louw, M. Lahijanian, W. Ruan, X. Huang, N. Merat, and M. Kwiatkowska,
    “Gaze-based intention anticipation over driving manoeuvres in semi-autonomous
    vehicles,” in *IROS*, 2019.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Rhinehart and K. M. Kitani, “First-person activity forecasting with
    online inverse reinforcement learning,” in *ICCV*, 2017.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] J.-Y. Kwak, B. C. Ko, and J.-Y. Nam, “Pedestrian intention prediction
    based on dynamic fuzzy automata for vehicle driving at nighttime,” *Infrared Physics
    & Technology*, vol. 81, pp. 41–51, 2017.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] N. Hu, A. Bestick, G. Englebienne, R. Bajscy, and B. Kröse, “Human intent
    forecasting using intrinsic kinematic constraints,” in *IROS*, 2016.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A. Jain, H. S. Koppula, B. Raghavan, S. Soh, and A. Saxena, “Car that
    knows before you do: Anticipating maneuvers via learning temporal driving models,”
    in *ICCV*, 2015.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] J. Hariyono, A. Shahbaz, L. Kurnianggoro, and K.-H. Jo, “Estimation of
    collision risk for improving driver’s safety,” in *IECON*, 2016.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Hashimoto, Y. Gu, L.-T. Hsu, and S. Kamijo, “Probability estimation
    for pedestrian crossing intention at signalized crosswalks,” in *ICVES*, 2015.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] H. Joo, T. Simon, M. Cikara, and Y. Sheikh, “Towards social artificial
    intelligence: Nonverbal social signal prediction in a triadic interaction,” in
    *CVPR*, 2019.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] F. Ziaeetabar, T. Kulvicius, M. Tamosiunaite, and F. Wörgötter, “Prediction
    of manipulation action classes using semantic spatial reasoning,” in *IROS*, 2018.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] S. Qi, S. Huang, P. Wei, and S.-C. Zhu, “Predicting human activities using
    stochastic grammar,” in *ICCV*, 2017.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] C. Park, J. Ondřej, M. Gilbert, K. Freeman, and C. O’Sullivan, “Hi robot:
    Human intention-aware robot planning for safe and efficient navigation in crowds,”
    in *IROS*, 2016.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] T. Mahmud, M. Hasan, A. Chakraborty, and A. K. Roy-Chowdhury, “A poisson
    process model for activity forecasting,” in *ICIP*, 2016.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] K. Xu, Z. Qin, and G. Wang, “Human activities prediction by learning combinatorial
    sparse representations,” in *ICIP*, 2016.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] C. Pérez-D’Arpino and J. A. Shah, “Fast target prediction of human reaching
    motion for cooperative human-robot manipulation tasks using time series classification,”
    in *ICRA*, 2015.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Q. Ke, M. Fritz, and B. Schiele, “Time-conditioned action anticipation
    in one shot,” in *CVPR*, 2019.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] A. Furnari and G. M. Farinella, “What would you expect? anticipating egocentric
    actions with rolling-unrolling lstms and modality attention,” in *ICCV*, 2019.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] F. Sener and A. Yao, “Zero-shot anticipation for instructional activities,”
    in *ICCV*, 2019.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Forecasting future
    action sequences with neural memory networks,” in *BMVC*, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] E. Alati, L. Mauro, V. Ntouskos, and F. Pirri, “Help by predicting what
    to do,” in *ICIP*, 2019.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] A. Furnari and G. M. Farinella, “Egocentric action anticipation by disentangling
    encoding and inference,” in *ICIP*, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Abu Farha, A. Richard, and J. Gall, “When will you do what? - anticipating
    temporal occurrences of activities,” in *CVPR*, 2018.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] T. Mahmud, M. Hasan, and A. K. Roy-Chowdhury, “Joint prediction of activity
    labels and starting times in untrimmed videos,” in *ICCV*, 2017.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] A. Rasouli, I. Kotseruba, and J. K. Tsotsos, “Pedestrian action anticipation
    using contextual feature fusion in stacked rnns,” in *BMVC*, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] W. Ding, J. Chen, and S. Shen, “Predicting vehicle behaviors over an extended
    horizon using behavior interaction network,” in *ICRA*, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] K. Saleh, M. Hossny, and S. Nahavandi, “Real-time intent prediction of
    pedestrians for autonomous ground vehicles via spatio-temporal densenet,” in *ICRA*,
    2019.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] O. Scheel, N. S. Nagaraja, L. Schwarz, N. Navab, and F. Tombari, “Attention-based
    lane change prediction,” in *ICRA*, 2019.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. S. Aliakbarian, F. S. Saleh, M. Salzmann, B. Fernando, L. Petersson,
    and L. Andersson, “Viena: A driving anticipation dataset,” in *ACCV*, C. V. Jawahar,
    H. Li, G. Mori, and K. Schindler, Eds., 2019.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] M. Strickland, G. Fainekos, and H. B. Amor, “Deep predictive models for
    collision risk assessment in autonomous driving,” in *ICRA*, 2018.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S. Casas, W. Luo, and R. Urtasun, “Intentnet: Learning to predict intention
    from raw sensor data,” in *CORL*, 2018.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] A. Rasouli, I. Kotseruba, and J. K. Tsotsos, “Are they going to cross?
    a benchmark dataset and baseline for pedestrian crosswalk behavior,” in *ICCVW*,
    2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] F.-H. Chan, Y.-T. Chen, Y. Xiang, and M. Sun, “Anticipating accidents
    in dashcam videos,” in *ACCV*, S.-H. Lai, V. Lepetit, K. Nishino, and Y. Sato,
    Eds., 2017.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Jain, A. Singh, H. S. Koppula, S. Soh, and A. Saxena, “Recurrent neural
    networks for driver activity anticipation via sensory-fusion architecture,” in
    *ICRA*, 2016.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] A. Manglik, X. Weng, E. Ohn-Bar, and K. M. Kitani, “Forecasting time-to-collision
    from monocular video: Feasibility, dataset, and challenges,” in *IROS*, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] P. Wang, S. Lien, and M. Lee, “A learning-based prediction model for baby
    accidents,” in *ICIP*, 2019.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] T. Suzuki, H. Kataoka, Y. Aoki, and Y. Satoh, “Anticipating traffic accidents
    with adaptive loss and large-scale incident db,” in *The CVPR*, 2018.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] K.-H. Zeng, S.-H. Chou, F.-H. Chan, J. Carlos Niebles, and M. Sun, “Agent-centric
    risk assessment: Accident anticipation and risky region localization,” in *CVPR*,
    2017.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Su, J. Pyo Hong, J. Shi, and H. Soo Park, “Predicting behaviors of
    basketball players from first person videos,” in *CVPR*, 2017.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] P. Felsen, P. Agrawal, and J. Malik, “What will happen next? forecasting
    player moves in sports videos,” in *ICCV*, 2017.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] J. Liang, L. Jiang, J. C. Niebles, A. G. Hauptmann, and L. Fei-Fei, “Peeking
    into the future: Predicting future person activities and locations in videos,”
    in *CVPR*, 2019.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C. Sun, A. Shrivastava, C. Vondrick, R. Sukthankar, K. Murphy, and C. Schmid,
    “Relational action forecasting,” in *CVPR*, 2019.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. Shen, B. Ni, Z. Li, and N. Zhuang, “Egocentric activity prediction
    via event modulated attention,” in *ECCV*, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] P. Schydlo, M. Rakovic, L. Jamone, and J. Santos-Victor, “Anticipation
    in human-robot cooperation: A recurrent neural network approach for multiple action
    sequences prediction,” in *ICRA*, 2018.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Zhong and W. Zheng, “Unsupervised learning for forecasting action representations,”
    in *ICIP*, 2018.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Gao, Z. Yang, and R. Nevatia, “Red: Reinforced encoder-decoder networks
    for action anticipation,” in *BMVC*, 2017.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] C. Vondrick, H. Pirsiavash, and A. Torralba, “Anticipating visual representations
    from unlabeled video,” in *CVPR*, 2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] H. Kataoka, Y. Miyashita, M. Hayashi, K. Iwata, and Y. Satoh, “Recognition
    of transitional action for short-term action prediction using discriminative temporal
    cnn feature,” in *BMVC*, 2016.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Y. Zhou and T. L. Berg, “Temporal perception and prediction in ego-centric
    video,” in *ICCV*, 2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena, “Structural-rnn: Deep
    learning on spatio-temporal graphs,” in *CVPR*, 2016.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] O. Scheel, L. Schwarz, N. Navab, and F. Tombari, “Situation assessment
    for planning lane changes: Combining recurrent models and prediction,” in *ICRA*,
    2018.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] X. Wang, J.-F. Hu, J.-H. Lai, J. Zhang, and W.-S. Zheng, “Progressive
    teacher-student learning for early action prediction,” in *CVPR*, 2019.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Predicting the future:
    A jointly learnt model for action anticipation,” in *ICCV*, 2019.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Zhao and R. P. Wildes, “Spatiotemporal feature residual propagation
    for action prediction,” in *ICCV*, 2019.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] T. Yao, M. Wang, B. Ni, H. Wei, and X. Yang, “Multiple granularity group
    interaction prediction,” in *CVPR*, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. Shi, B. Fernando, and R. Hartley, “Action anticipation with rbf kernelized
    feature mapping rnn,” in *ECCV*, 2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] J. Bütepage, H. Kjellström, and D. Kragic, “Anticipating many futures:
    Online human motion prediction and generation for human-robot interaction,” in
    *ICRA*, 2018.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Cho and H. Foroosh, “A temporal sequence learning for action recognition
    and prediction,” in *WACV*, 2018.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] M. Sadegh Aliakbarian, F. Sadat Saleh, M. Salzmann, B. Fernando, L. Petersson,
    and L. Andersson, “Encouraging lstms to anticipate actions very early,” in *ICCV*,
    2017.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] W. Li and M. Fritz, “Recognition of ongoing complex activities by sequence
    prediction over a hierarchical label space,” in *WACV*, 2016.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] M. Safaei and H. Foroosh, “Still image action recognition by predicting
    spatial-temporal pixel evolution,” in *WACV*, 2019.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. Liu, A. Shahroudy, G. Wang, L.-Y. Duan, and A. C. Kot, “Ssnet: Scale
    selection network for online 3d action prediction,” in *CVPR*, 2018.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] L. Chen, J. Lu, Z. Song, and J. Zhou, “Part-activated deep reinforcement
    learning for action prediction,” in *ECCV*, 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] J. Butepage, M. J. Black, D. Kragic, and H. Kjellstrom, “Deep representation
    learning for human motion prediction and classification,” in *CVPR*, 2017.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Y. Kong, Z. Tao, and Y. Fu, “Deep sequential context networks for action
    prediction,” in *CVPR*, 2017.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] G. Singh, S. Saha, M. Sapienza, P. H. S. Torr, and F. Cuzzolin, “Online
    real-time multiple spatiotemporal action localisation and prediction,” in *ICCV*,
    2017.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Dong-Gyu Lee and Seong-Whan Lee, “Human activity prediction based on
    sub-volume relationship descriptor,” in *ICPR*, 2016.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] J. F. Carvalho, M. Vejdemo-Johansson, F. T. Pokorny, and D. Kragic, “Long-term
    prediction of motion trajectories using path homology clusters,” in *IROS*, 2019.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Y. Yoo, K. Yun, S. Yun, J. Hong, H. Jeong, and J. Young Choi, “Visual
    path prediction in complex scenes with crowded moving objects,” in *CVPR*, 2016.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] A. Møgelmose, M. M. Trivedi, and T. B. Moeslund, “Trajectory analysis
    and prediction for improved pedestrian safety: Integrated framework and evaluations,”
    in *IV*, 2015.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Chenghui Zhou, B. Balle, and J. Pineau, “Learning time series models
    for pedestrian motion prediction,” in *ICRA*, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] A. Rudenko, L. Palmieri, and K. O. Arras, “Joint long-term prediction
    of human motion using a planning-based social force approach,” in *ICRA*, 2018.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] A. Rudenko, L. Palmieri, A. J. Lilienthal, and K. O. Arras, “Human motion
    prediction under social grouping constraints,” in *IROS*, 2018.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Schulz, C. Hubmann, J. Löchner, and D. Burschka, “Interaction-aware
    probabilistic behavior prediction in urban environments,” in *IROS*, 2018.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M. Shen, G. Habibi, and J. P. How, “Transferable pedestrian motion prediction
    models at intersections,” in *IROS*, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] F. Shkurti and G. Dudek, “Topologically distinct trajectory predictions
    for probabilistic pursuit,” in *IROS*, 2017.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] D. Vasquez, “Novel planning-based algorithms for human motion prediction,”
    in *ICRA*, 2016.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] V. Karasev, A. Ayvaci, B. Heisele, and S. Soatto, “Intent-aware long-term
    prediction of pedestrian motion,” in *ICRA*, 2016.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] N. Lee and K. M. Kitani, “Predicting wide receiver trajectories in american
    football,” in *WACV*, 2016.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] H. Bai, S. Cai, N. Ye, D. Hsu, and W. S. Lee, “Intention-aware online
    pomdp planning for autonomous driving in a crowd,” in *ICRA*, 2015.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] S. Solaimanpour and P. Doshi, “A layered hmm for predicting motion of
    a leader in multi-robot settings,” in *ICRA*, 2017.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Y. F. Chen, M. Liu, and J. P. How, “Augmented dictionary learning for
    motion prediction,” in *ICRA*, 2016.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] R. Sanchez-Matilla and A. Cavallaro, “A predictor of moving objects for
    first-person vision,” in *ICIP*, 2019.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] B. Lee, J. Choi, C. Baek, and B. Zhang, “Robust human following by deep
    bayesian trajectory prediction for home service robots,” in *ICRA*, 2018.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] I. Hasan, F. Setti, T. Tsesmelis, A. Del Bue, M. Cristani, and F. Galasso,
    ““seeing is believing”: Pedestrian trajectory forecasting using visual frustum
    of attention,” in *WACV*, 2018.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] L. Ballan, F. Castaldo, A. Alahi, F. Palmieri, and S. Savarese, “Knowledge
    transfer for scene-specific motion prediction,” in *ECCV*, B. Leibe, J. Matas,
    N. Sebe, and M. Welling, Eds., 2016.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] M. Pfeiffer, U. Schwesinger, H. Sommer, E. Galceran, and R. Siegwart,
    “Predicting actions to act predictably: Cooperative partial motion planning with
    maximum entropy models,” in *IROS*, 2016.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] N. N. Vo and A. F. Bobick, “Augmenting physical state prediction through
    structured activity inference,” in *ICRA*, 2015.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] V. Akbarzadeh, C. Gagné, and M. Parizeau, “Kernel density estimation
    for target trajectory prediction,” in *IROS*, 2015.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. Schulz and R. Stiefelhagen, “A controlled interactive multiple model
    filter for combined pedestrian intention recognition and path prediction,” in
    *ITSC*, 2015.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,
    D. Wang, P. Carr, S. Lucey, D. Ramanan, and J. Hays, “Argoverse: 3d tracking and
    forecasting with rich maps,” in *CVPR*, 2019.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Sadeghian, V. Kosaraju, A. Sadeghian, N. Hirose, H. Rezatofighi, and
    S. Savarese, “Sophie: An attentive gan for predicting paths compliant to social
    and physical constraints,” in *CVPR*, 2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] P. Zhang, W. Ouyang, P. Zhang, J. Xue, and N. Zheng, “Sr-lstm: State
    refinement for lstm towards pedestrian trajectory prediction,” in *CVPR*, 2019.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] T. Zhao, Y. Xu, M. Monfort, W. Choi, C. Baker, Y. Zhao, Y. Wang, and
    Y. N. Wu, “Multi-agent tensor fusion for contextual trajectory prediction,” in
    *CVPR*, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] H. Bi, Z. Fang, T. Mao, Z. Wang, and Z. Deng, “Joint prediction for kinematic
    trajectories in vehicle-pedestrian-mixed scenes,” in *ICCV*, 2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C. Choi and B. Dariush, “Looking to relations for future trajectory forecast,”
    in *ICCV*, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Y. Huang, H. Bi, Z. Li, T. Mao, and Z. Wang, “Stgat: Modeling spatial-temporal
    interactions for human trajectory prediction,” in *ICCV*, 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] A. Rasouli, I. Kotseruba, T. Kunic, and J. K. Tsotsos, “Pie: A large-scale
    dataset and models for pedestrian intention estimation and trajectory prediction,”
    in *ICCV*, 2019.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] L. A. Thiede and P. P. Brahma, “Analyzing the variety loss in the context
    of probabilistic trajectory prediction,” in *ICCV*, 2019.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] V. Kosaraju, A. Sadeghian, R. Martín-Martín, I. Reid, H. Rezatofighi,
    and S. Savarese, “Social-bigat: Multimodal trajectory forecasting using bicycle-gan
    and graph attention networks,” in *NeurIPS*, 2019.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] W. Ding and S. Shen, “Online vehicle trajectory prediction using policy
    anticipation network and optimization-based context reasoning,” in *ICRA*, 2019.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Li, H. Ma, and M. Tomizuka, “Interaction-aware multi-agent tracking
    and probabilistic behavior prediction via adversarial learning,” in *ICRA*, 2019.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] C. Anderson, X. Du, R. Vasudevan, and M. Johnson-Roberson, “Stochastic
    sampling simulation for pedestrian trajectory prediction,” in *IROS*, 2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. Srikanth, J. A. Ansari, S. Sharma *et al.*, “Infer: Intermediate representations
    for future prediction,” in *IROS*, 2019.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Y. Zhu, D. Qian, D. Ren, and H. Xia, “Starnet: Pedestrian trajectory
    prediction using deep neural network in star topology,” in *IROS*, 2019.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] H. Xue, D. Huynh, and M. Reynolds, “Location-velocity attention for pedestrian
    trajectory prediction,” in *WACV*, 2019.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, “Social
    gan: Socially acceptable trajectories with generative adversarial networks,” in
    *CVPR*, 2018.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] I. Hasan, F. Setti, T. Tsesmelis, A. Del Bue, F. Galasso, and M. Cristani,
    “Mx-lstm: Mixing tracklets and vislets to jointly forecast trajectories and head
    poses,” in *CVPR*, 2018.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Y. Xu, Z. Piao, and S. Gao, “Encoding crowd interaction with deep neural
    network for pedestrian trajectory prediction,” in *CVPR*, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] M. Pfeiffer, G. Paolo, H. Sommer, J. Nieto, R. Siegwart, and C. Cadena,
    “A data-driven model for interaction-aware pedestrian motion prediction in object
    cluttered environments,” in *ICRA*, 2018.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] E. Rehder, F. Wirth, M. Lauer, and C. Stiller, “Pedestrian prediction
    by planning using deep neural networks,” in *ICRA*, 2018.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] H. Xue, D. Q. Huynh, and M. Reynolds, “Ss-lstm: A hierarchical lstm model
    for pedestrian trajectory prediction,” in *WACV*, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] F. Bartoli, G. Lisanti, L. Ballan, and A. Del Bimbo, “Context-aware trajectory
    prediction,” in *ICPR*, 2018.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese,
    “Social lstm: Human trajectory prediction in crowded spaces,” in *CVPR*, 2016.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] J. Hong, B. Sapp, and J. Philbin, “Rules of the road: Predicting driving
    behavior with a convolutional model of semantic interactions,” in *CVPR*, 2019.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] N. Rhinehart, R. McAllister, K. Kitani, and S. Levine, “Precog: Prediction
    conditioned on goals in visual multi-agent settings,” in *ICCV*, 2019.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] J. Li, H. Ma, and M. Tomizuka, “Conditional generative neural system
    for probabilistic trajectory prediction,” in *IROS*, 2019.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] T. Fernando, S. Denman, S. Sridharan, and C. Fookes, “Gd-gan: Generative
    adversarial networks for trajectory prediction and group detection in crowds,”
    in *ACCV*, C. V. Jawahar, H. Li, G. Mori, and K. Schindler, Eds., 2019.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] N. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. S. Torr, and M. Chandraker,
    “Desire: Distant future prediction in dynamic scenes with interacting agents,”
    in *CVPR*, 2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] N. Rhinehart, K. M. Kitani, and P. Vernaza, “R2p2: A reparameterized
    pushforward policy for diverse, precise generative path forecasting,” in *ECCV*,
    2018.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] K.-R. Kim, W. Choi, Y. J. Koh, S.-G. Jeong, and C.-S. Kim, “Instance-level
    future motion estimation in a single image based on ordinal regression,” in *ICCV*,
    2019.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov, “Multipath: Multiple probabilistic
    anchor trajectory hypotheses for behavior prediction,” in *CoRL*, 2019.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] H. Cui, V. Radosavljevic, F. Chou, T. Lin, T. Nguyen, T. Huang, J. Schneider,
    and N. Djuric, “Multimodal trajectory predictions for autonomous driving using
    deep convolutional networks,” in *ICRA*, 2019.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] X. Huang, S. G. McGill, B. C. Williams, L. Fletcher, and G. Rosman, “Uncertainty-aware
    driver trajectory prediction at urban intersections,” in *ICRA*, 2019.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] S. Zhou, M. J. Phielipp, J. A. Sefair, S. I. Walker, and H. B. Amor,
    “Clone swarms: Learning to predict and control multi-robot systems by imitation,”
    in *IROS*, 2019.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] A. Jain, S. Casas, R. Liao, Y. Xiong, S. Feng, S. Segal, and R. Urtasun,
    “Discrete residual flow for probabilistic pedestrian behavior prediction,” in
    *CoRL*, 2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] U. Baumann, C. Guiser, M. Herman, and J. M. Zollner, “Predicting ego-vehicle
    paths from environmental observations with a deep neural network,” in *ICRA*,
    2018.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Y. Zhang, W. Wang, R. Bonatti, D. Maturana, and S. Scherer, “Integrating
    kinematics and environment context into deep inverse reinforcement learning for
    predicting off-road vehicle trajectories,” in *CoRL*, 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] W.-C. Ma, D.-A. Huang, N. Lee, and K. M. Kitani, “Forecasting interactive
    dynamics of pedestrians with fictitious play,” in *CVPR*, 2017.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] S. Yi, H. Li, and X. Wang, “Pedestrian behavior understanding and prediction
    with deep neural networks,” in *ECCV*, B. Leibe, J. Matas, N. Sebe, and M. Welling,
    Eds., 2016.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] R. Chandra, U. Bhattacharya, A. Bera, and D. Manocha, “Traphic: Trajectory
    prediction in dense and heterogeneous traffic using weighted interactions,” in
    *CVPR*, 2019.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Y. Li, “Which way are you going? imitative decision learning for path
    forecasting in dynamic scenes,” in *CVPR*, 2019.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] W. Zhi, L. Ott, and F. Ramos, “Kernel trajectory maps for multi-modal
    probabilistic motion prediction,” in *CoRL*, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] A. Vemula, K. Muelling, and J. Oh, “Social attention: Modeling attention
    in human crowds,” in *ICRA*, 2018.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] C. Tang, J. Chen, and M. Tomizuka, “Adaptive probabilistic vehicle trajectory
    prediction through physically feasible bayesian recurrent neural network,” in
    *ICRA*, 2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] K. Cho, T. Ha, G. Lee, and S. Oh, “Deep predictive autonomous driving
    using multi-agent joint trajectory prediction and traffic rules,” in *IROS*, 2019.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] A. Bhattacharyya, M. Fritz, and B. Schiele, “Long-term on-board prediction
    of people in traffic scenes under uncertainty,” in *CVPR*, 2018.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] P. Felsen, P. Lucey, and S. Ganguly, “Where will they go? predicting
    fine-grained adversarial multi-agent motion using conditional variational autoencoders,”
    in *ECCV*, 2018.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] S. Cao and R. Nevatia, “Forecasting human pose and motion with multibody
    dynamic model,” in *WACV*, 2015.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Y. Yuan and K. Kitani, “Ego-pose estimation and forecasting as real-time
    pd control,” in *ICCV*, 2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] H. Chiu, E. Adeli, B. Wang, D. Huang, and J. C. Niebles, “Action-agnostic
    human pose forecasting,” in *WACV*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] E. Wu and H. Koike, “Futurepose - mixed reality martial arts training
    using real-time 3d human pose forecasting with a rgb camera,” in *WACV*, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Y.-W. Chao, J. Yang, B. Price, S. Cohen, and J. Deng, “Forecasting human
    dynamics from static images,” in *CVPR*, 2017.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] A. Gopalakrishnan, A. Mali, D. Kifer, L. Giles, and A. G. Ororbia, “A
    neural temporal model for human motion prediction,” in *CVPR*, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] L.-Y. Gui, Y.-X. Wang, D. Ramanan, and J. M. F. Moura, “Few-shot human
    motion prediction via meta-learning,” in *ECCV*, 2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] L.-Y. Gui, Y.-X. Wang, X. Liang, and J. M. F. Moura, “Adversarial geometry-aware
    human motion prediction,” in *ECCV*, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] L. Gui, K. Zhang, Y. Wang, X. Liang, J. M. F. Moura, and M. Veloso, “Teaching
    robots to predict human motion,” in *IROS*, 2018.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] J. Martinez, M. J. Black, and J. Romero, “On human motion prediction
    using recurrent neural networks,” in *CVPR*, 2017.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] K. Fragkiadaki, S. Levine, P. Felsen, and J. Malik, “Recurrent network
    models for human dynamics,” in *ICCV*, 2015.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] B. Wang, E. Adeli, H.-k. Chiu, D.-A. Huang, and J. C. Niebles, “Imitation
    learning for human pose prediction,” in *ICCV*, 2019.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] W. Mao, M. Liu, M. Salzmann, and H. Li, “Learning trajectory dependencies
    for human motion prediction,” in *ICCV*, 2019.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] A. Hernandez, J. Gall, and F. Moreno-Noguer, “Human motion prediction
    via spatio-temporal inpainting,” in *ICCV*, 2019.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] J. Y. Zhang, P. Felsen, A. Kanazawa, and J. Malik, “Predicting 3d human
    dynamics from video,” in *ICCV*, 2019.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] C. Talignani Landi, Y. Cheng, F. Ferraguti, M. Bonfe, C. Secchi, and
    M. Tomizuka, “Prediction of human arm target for robot reaching movements,” in
    *IROS*, 2019.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] V. Guizilini, R. Senanayake, and F. Ramos, “Dynamic hilbert maps: Real-time
    occupancy predictions in changing environments,” in *ICRA*, 2019.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] D. Graves, K. Rezaee, and S. Scheideman, “Perception as prediction using
    general value functions in autonomous driving applications,” in *IROS*, 2019.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] O. Afolabi, K. Driggs–Campbell, R. Dong, M. J. Kochenderfer, and S. S.
    Sastry, “People as sensors: Imputing maps from human actions,” in *IROS*, 2018.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] G. N. Wilson, A. Ramirez-Serrano, and Q. Sun, “Vehicle state prediction
    for outdoor autonomous high-speed off-road ugvs,” in *ICRA*, 2015.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] N. Mohajerin and M. Rohani, “Multi-step prediction of occupancy grid
    maps with recurrent neural networks,” in *CVPR*, 2019.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] K. Katyal, K. Popek, C. Paxton, P. Burlina, and G. D. Hager, “Uncertainty-aware
    occupancy map prediction using generative networks for robot navigation,” in *ICRA*,
    2019.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] M. Schreiber, S. Hoermann, and K. Dietmayer, “Long-term occupancy grid
    prediction using recurrent neural networks,” in *ICRA*, 2019.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] S. Hoermann, M. Bach, and K. Dietmayer, “Dynamic occupancy grid prediction
    for urban autonomous driving: A deep learning approach with fully automatic labeling,”
    in *ICRA*, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] S. Choi, K. Lee, and S. Oh, “Robust modeling and prediction in dynamic
    environments using recurrent flow networks,” in *IROS*, 2016.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] P. Luc, C. Couprie, Y. LeCun, and J. Verbeek, “Predicting future instance
    segmentation by forecasting convolutional features,” in *ECCV*, 2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] P. Luc, N. Neverova, C. Couprie, J. Verbeek, and Y. LeCun, “Predicting
    deeper into the future of semantic segmentation,” in *ICCV*, 2017.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] X. Jin, H. Xiao, X. Shen, J. Yang, Z. Lin, Y. Chen, Z. Jie, J. Feng,
    and S. Yan, “Predicting scene parsing and motion dynamics in the future,” in *NeurIPS*,
    2017.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] S. Kim, H. Kim, J. Lee, S. Yoon, S. E. Kahou, K. Kashinath, and M. Prabhat,
    “Deep-hurricane-tracker: Tracking and forecasting extreme climate events,” in
    *WACV*, 2019.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] W. Chu, K. Ho, and A. Borji, “Visual weather temperature prediction,”
    in *WACV*, 2018.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] T. A. Siddiqui, S. Bharadwaj, and S. Kalyanaraman, “A deep learning approach
    to solar-irradiance forecasting in sky-videos,” in *WACV*, 2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] K. Wang, M. Bansal, and J. Frahm, “Retweet wars: Tweet popularity prediction
    via dynamic multimodal regression,” in *WACV*, 2018.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Z. Al-Halah, R. Stiefelhagen, and K. Grauman, “Fashion forward: Forecasting
    visual style in fashion,” in *ICCV*, 2017.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] I. Sur and H. Ben Amor, “Robots that anticipate pain: Anticipating physical
    perturbations from visual cues through deep predictive models,” in *IROS*, 2017.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] R. Mottaghi, M. Rastegari, A. Gupta, and A. Farhadi, ““what happens if…”
    learning to predict the effect of forces in images,” in *ECCV*, B. Leibe, J. Matas,
    N. Sebe, and M. Welling, Eds., 2016.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] J. Carvajal, A. Wiliem, C. Sanderson, and B. Lovell, “Towards miss universe
    automatic prediction: The evening gown competition,” in *ICPR*, 2016.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] J. Joo, F. F. Steen, and S.-C. Zhu, “Automated facial trait judgment
    and election outcome prediction: Social dimensions of face,” in *ICCV*, 2015.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] S. Lal, S. Duggal, and I. Sreedevi, “Online video summarization: Predicting
    future to better summarize present,” in *WACV*, 2019.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] W. Liu, W. Luo, D. Lian, and S. Gao, “Future frame prediction for anomaly
    detection – a new baseline,” in *CVPR*, 2018.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] T. Fernando, S. Denman, S. Sridharan, and C. Fookes, “Tracking by prediction:
    A deep generative model for mutli-person localisation and tracking,” in *WACV*,
    2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] D. Jayaraman and K. Grauman, “Look-ahead before you leap: End-to-end
    active recognition by forecasting the effect of motion,” in *ECCV*, B. Leibe,
    J. Matas, N. Sebe, and M. Welling, Eds., 2016.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] A. Dave, O. Russakovsky, and D. Ramanan, “Predictive-corrective networks
    for action detection,” in *CVPR*, 2017.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Z. Yang, J. Gao, and R. Nevatia, “Spatio-temporal action detection with
    cascade proposal and location anticipation,” in *BMVC*, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] M. Ziaeefard, R. Bergevin, and L.-P. Morency, “Time-slice prediction
    of dyadic human activities,” in *the British Machine Vision Conference (BMVC)*,
    2015.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. Metaxas, “Learning to forecast
    and refine residual motion for image-to-video generation,” in *ECCV*, 2018.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli *et al.*, “Image
    quality assessment: from error visibility to structural similarity,” *IEEE transactions
    on image processing*, vol. 13, no. 4, pp. 600–612, 2004.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *CVPR*, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski,
    and S. Gelly, “Towards accurate generative models of video: A new metric & challenges,”
    *arXiv:1812.01717*, 2018.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen,
    “Improved techniques for training gans,” in *NeurIPS*, 2016.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] L. Sun, Z. Yan, S. M. Mellado, M. Hanheide, and T. Duckett, “3dof pedestrian
    trajectory prediction learned from long-term autonomous mobile robot deployment
    data,” in *ICRA*, 2018.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] S. Pellegrini, A. Ess, K. Schindler, and L. Van Gool, “You’ll never walk
    alone: Modeling social behavior for multi-target tracking,” in *ICCV*, 2009.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Z. Liu, S. Wu, S. Jin, Q. Liu, S. Lu, R. Zimmermann, and L. Cheng, “Towards
    natural and accurate future motion prediction of humans and animals,” in *CVPR*,
    2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.6m: Large
    scale datasets and predictive methods for 3d human sensing in natural environments,”
    *PAMI*, vol. 36, no. 7, pp. 1325–1339, 2014.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] M. P. Zapf, M. Kawanabe, and L. Y. Morales Saiki, “Pedestrian density
    prediction for efficient mobile robot exploration,” in *IROS*, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] S. He, D. Kangin, Y. Mi, and N. Pugeault, “Aggregated sparse attention
    for steering angle prediction,” in *ICPR*, 2018.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] A. Kanazawa, J. Y. Zhang, P. Felsen, and J. Malik, “Learning 3d human
    dynamics from video,” in *CVPR*, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. Kummerle,
    H. Konigshof, C. Stiller, A. de La Fortelle *et al.*, “Interaction dataset: An
    international, adversarial and cooperative motion dataset in interactive driving
    scenarios with semantic maps,” *arXiv:1910.03088*, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] A. Siarohin, S. Lathuilière, S. Tulyakov, E. Ricci, and N. Sebe, “Animating
    arbitrary objects via deep motion transfer,” in *CVPR*, 2019.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous
    driving,” *arXiv:1903.11027*, 2019.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] T. von Marcard, R. Henschel, M. Black, B. Rosenhahn, and G. Pons-Moll,
    “Recovering accurate 3d human pose in the wild using imus and a moving camera,”
    in *ECCV*, 2018.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] G. Awad, A. Butt, K. Curtis, Y. Lee, J. Fiscus, A. Godil, D. Joy, A. Delgado,
    A. Smeaton, Y. Graham *et al.*, “Benchmarking video activity detection, video
    captioning and matching, video storytelling linking and video search,” in *TRECVID*,
    2018.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] P. Schydlo, M. Rakovic, L. Jamone, and J. Santos-Victor, “Anticipation
    in human-robot cooperation: A recurrent neural network approach for multiple action
    sequences prediction,” in *ICRA*, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco, R. Sukthankar *et al.*, “Ava: A video dataset of spatio-temporally
    localized atomic visual actions,” in *CVPR*, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “Scaling egocentric
    vision: The epic-kitchens dataset,” in *ECCV*, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] Y. Li, M. Liu, and J. M. Rehg, “In the eye of beholder: Joint learning
    of gaze and actions in first person video,” in *ECCV*, 2018.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] O. Groth, F. B. Fuchs, I. Posner, and A. Vedaldi, “Shapestacks: Learning
    vision-based physical intuition for generalised object stacking,” in *ECCV*, 2018.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] L. Zhou, C. Xu, and J. J. Corso, “Towards automatic learning of procedures
    from web instructional videos,” in *AI*, 2018.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] S. Ma, S. A. Bargal, J. Zhang, L. Sigal, and S. Sclaroff, “Do less and
    achieve more: Training cnns for action recognition utilizing action images from
    the web,” *Pattern Recognition*, vol. 68, pp. 334–345, 2017.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] S. Zhang, R. Benenson, and B. Schiele, “Citypersons: A diverse dataset
    for pedestrian detection,” in *CVPR*, 2017.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Z. Yan, T. Duckett, and N. Bellotto, “Online learning for human classification
    in 3d lidar-based tracking,” in *IROS*, 2017.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] C. Xu, L. N. Govindarajan, Y. Zhang, and L. Cheng, “Lie-x: Depth image
    based articulated object pose estimation, tracking, and action recognition on
    lie groups,” *IJCV*, vol. 123, no. 3, pp. 454–478, 2017.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year, 1000km: The
    Oxford RobotCar Dataset,” *IJRR*, vol. 36, no. 1, pp. 3–15, 2017.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] L. Chunhui, H. Yueyu, L. Yanghao, S. Sijie, and L. Jiaying, “Pku-mmd:
    A large scale benchmark for continuous multi-modal human action understanding,”
    *arXiv:1703.07475*, 2017.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] A. Salvador, N. Hynes, Y. Aytar, J. Marin, F. Ofli, I. Weber, and A. Torralba,
    “Learning cross-modal embeddings for cooking recipes and food images,” in *CVPR*,
    2017.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] N. Hawes, C. Burbridge, F. Jovan, L. Kunze, B. Lacerda, L. Mudrova, J. Young,
    J. Wyatt, D. Hebesberger, T. Kortner *et al.*, “The strands project: Long-term
    autonomy in everyday environments,” *IEEE Robotics & Automation Magazine*, vol. 24,
    no. 3, pp. 146–156, 2017.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum, “A compositional
    object-based approach to learning physical dynamics,” *arXiv:1612.00341*, 2016.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *CVPR*, 2016.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] CMU, “Cmu graphics lab motion capture database,” http://mocap.cs.cmu.edu/,
    2016.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, “Ntu rgb+d: A large scale
    dataset for 3d human activity analysis,” in *CVPR*, 2016.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] Y. Li, C. Lan, J. Xing, W. Zeng, C. Yuan, and J. Liu, “Online human action
    detection using joint classification-regression recurrent neural networks,” *ECCV*,
    2016.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learning social
    etiquette: Human trajectory understanding in crowded scenes,” in *ECCV*, 2016.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] R. De Geest, E. Gavves, A. Ghodrati, Z. Li, C. Snoek, and T. Tuytelaars,
    “Online action detection,” in *ECCV*, 2016.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] T.-H. K. Huang, F. Ferraro, N. Mostafazadeh, I. Misra, J. Devlin, A. Agrawal,
    R. Girshick, X. He, P. Kohli, D. Batra *et al.*, “Visual storytelling,” in *NAACL*,
    2016.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan,
    and S. Vijayanarasimhan, “Youtube-8m: A large-scale video classification benchmark,”
    *arXiv:1609.08675*, 2016.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] J. McAuley, C. Targett, Q. Shi, and A. Van Den Hengel, “Image-based recommendations
    on styles and substitutes,” in *SIGIR*, 2015.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] H. Joo, H. Liu, L. Tan, L. Gui, B. Nabbe, I. Matthews, T. Kanade, S. Nobuhara,
    and Y. Sheikh, “Panoptic studio: A massively multiview system for social motion
    capture,” in *ICCV*, 2015.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Y. Li, Z. Ye, and J. M. Rehg, “Delving into egocentric actions,” in *CVPR*,
    2015.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] S. Cappallo, T. Mensink, and C. G. Snoek, “Latent factors of visual popularity
    prediction,” in *ICMR*, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] L. Leal-Taixé, A. Milan, I. Reid, S. Roth, and K. Schindler, “Motchallenge
    2015: Towards a benchmark for multi-target tracking,” *arXiv:1504.01942*, 2015.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] N. Srivastava, E. Mansimov, and R. Salakhudinov, “Unsupervised learning
    of video representations using lstms,” in *ICML*, 2015.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene understanding
    benchmark suite,” in *CVPR*, 2015.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] J.-F. Hu, W.-S. Zheng, J. Lai, and J. Zhang, “Jointly learning heterogeneous
    features for rgb-d activity recognition,” in *CVPR*, 2015.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] A. Gorban, H. Idrees, Y.-G. Jiang, A. Roshan Zamir, I. Laptev, M. Shah,
    and R. Sukthankar, “THUMOS challenge: Action recognition with a large number of
    classes,” [http://www.thumos.info/](http://www.thumos.info/), 2015.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] C. Wu, J. Zhang, S. Savarese, and A. Saxena, “Watch-n-patch: Unsupervised
    understanding of actions and relations,” in *CVPR*, 2015.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] Y. Xiong, K. Zhu, D. Lin, and X. Tang, “Recognize complex events from
    static images by fusing deep channels,” in *CVPR*, 2015.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] H. Kuehne, A. B. Arslan, and T. Serre, “The language of actions: Recovering
    the syntax and semantics of goal-directed human activities,” in *CVPR*, 2014.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele, “2d human pose
    estimation: New benchmark and state of the art analysis,” in *CVPR*, 2014.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] G. Yu, Z. Liu, and J. Yuan, “Discriminative orderlet mining for real-time
    recognition of human-object interaction,” in *ACCV*, D. Cremers, I. Reid, H. Saito,
    and M.-H. Yang, Eds., 2015.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei,
    “Large-scale video classification with convolutional neural networks,” in *CVPR*,
    2014.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] S. Stein and S. J. McKenna, “Combining embedded accelerometers with computer
    vision for recognizing food preparation activities,” in *UbiComp*, 2013.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] D. Brščić, T. Kanda, T. Ikeda, and T. Miyashita, “Person tracking in
    large public spaces using 3-d range sensors,” *Transactions on Human-Machine Systems*,
    vol. 43, no. 6, pp. 522–534, 2013.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] H. S. Koppula, R. Gupta, and A. Saxena, “Learning human activities and
    object affordances from rgb-d videos,” *IJRR*, vol. 32, no. 8, pp. 951–970, 2013.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] C. Lu, J. Shi, and J. Jia, “Abnormal event detection at 150 fps in matlab,”
    in *ICCV*, 2013.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] N. Schneider and D. M. Gavrila, “Pedestrian path prediction with recursive
    bayesian filters: A comparative study,” in *GCPR*, 2013.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, “Towards understanding
    action recognition,” in *ICCV*, 2013.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] W. Zhang, M. Zhu, and K. G. Derpanis, “From actemes to action: A strongly-supervised
    representation for detailed action understanding,” in *ICCV*, 2013.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] Y. Kong, Y. Jia, and Y. Fu, “Learning human interaction by interactive
    phrases,” in *ECCV*, 2012.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] A. Fathi, Y. Li, and J. M. Rehg, “Learning to recognize daily actions
    using gaze,” in *ECCV*, 2012.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *CVPR*, 2012.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] A. Abramov, K. Pauwels, J. Papon, F. Wörgötter, and B. Dellen, “Depth-supported
    real-time video segmentation with the kinect,” in *WACV*, 2012.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele, “A database for fine
    grained activity detection of cooking activities,” in *CVPR*, 2012.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] J. Wang, Z. Liu, Y. Wu, and J. Yuan, “Mining actionlet ensemble for action
    recognition with depth cameras,” in *CVPR*, 2012.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] B. Zhou, X. Wang, and X. Tang, “Understanding collective crowd behaviors:
    Learning a mixture model of dynamic pedestrian-agents,” in *CVPR*, 2012.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] K. Yun, J. Honorio, D. Chattopadhyay, T. L. Berg, and D. Samaras, “Two-person
    interaction detection using body-pose features and multiple instance learning,”
    in *CVPRW*, 2012.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101 human
    actions classes from videos in the wild,” *arXiv:1212.0402*, 2012.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] L. Xia, C. Chen, and J. Aggarwal, “View invariant human action recognition
    using histograms of 3d joints,” in *CVPRW*, 2012.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] H. Dibeklioğlu, A. A. Salah, and T. Gevers, “Are you really smiling at
    me? spontaneous versus posed enjoyment smiles,” in *ECCV*, A. Fitzgibbon, S. Lazebnik,
    P. Perona, Y. Sato, and C. Schmid, Eds., 2012.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] G. Pandey, J. R. McBride, and R. M. Eustice, “Ford campus vision and
    lidar data set,” *The International Journal of Robotics Research (IJRR)*, vol. 30,
    no. 13, pp. 1543–1552, 2011.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, “HMDB: a large
    video database for human motion recognition,” in *ICCV*, 2011.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] B. Yao, X. Jiang, A. Khosla, A. L. Lin, L. Guibas, and L. Fei-Fei, “Human
    action recognition by learning bases of action attributes and parts,” in *ICCV*,
    2011.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] B. Benfold and I. Reid, “Stable multi-target tracking in real-time surveillance
    video,” in *CVPR*, 2011.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee, S. Mukherjee,
    J. Aggarwal, H. Lee, L. Davis *et al.*, “A large-scale benchmark dataset for event
    recognition in surveillance video,” in *CVPR*, 2011.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] N. Pugeault and R. Bowden, “Learning pre-attentive driving behaviour
    from holistic visual features,” in *ECCV*, 2010.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] W. Li, Z. Zhang, and Z. Liu, “Action recognition based on a bag of 3d
    points,” in *CVPRW*, 2010.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] N. Aifanti, C. Papachristou, and A. Delopoulos, “The mug facial expression
    database,” *WIAMIS*, 2010.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] J. Santner, C. Leistner, A. Saffari, T. Pock, and H. Bischof, “Prost:
    Parallel robust online simple tracking,” in *CVPR*, 2010.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] A. Patron-Perez, M. Marszalek, A. Zisserman, and I. D. Reid, “High five:
    Recognising human interactions in tv shows.” in *BMVC*, 2010.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] M. S. Ryoo and J. K. Aggarwal, “UT-Interaction Dataset, ICPR contest
    on Semantic Description of Human Activities (SDHA),” 2010. [Online]. Available:
    [http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] R. Vezzani and R. Cucchiara, “Video surveillance online repository (visor):
    an integrated framework,” *Multimedia Tools and Applications*, vol. 50, no. 2,
    pp. 359–380, 2010.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] V. Delaitre, I. Laptev, and J. Sivic, “Recognizing human actions in still
    images: a study of bag-of-features and part-based representations,” in *BMVC*,
    2010.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] P. Dollár, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection:
    A benchmark,” in *CVPR*, 2009.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] W. Choi, K. Shahid, and S. Savarese, “What are they doing? : Collective
    activity classification using spatio-temporal relationship among people,” in *ICCVW*,
    2009.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] B. Majecka, “Statistical models of pedestrian behaviour in the forum,”
    Master’s thesis, School of Informatics, University of Edinburgh, 2009.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] R. Hess and A. Fern, “Discriminatively trained particle filters for complex
    multi-object tracking,” in *CVPR*, 2009.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] J. Ferryman and A. Shahrokni, “Pets2009: Dataset and challenge,” in *PETS*,
    2009.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] C. C. Loy, T. Xiang, and S. Gong, “Modelling multi-object activity by
    gaussian processes.” in *BMVC*, 2009.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] M. Tenorth, J. Bandouch, and M. Beetz, “The tum kitchen data set of everyday
    manipulation activities for motion tracking and action recognition,” in *ICCVW*,
    2009.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] A. V. T. Library, “YUV video sequences,” http://trace.kom.aau.dk/yuv/index.html,
    2009.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] M. Enzweiler and D. M. Gavrila, “Monocular pedestrian detection: Survey
    and experiments,” *transactions on pattern analysis and machine intelligence (PAMI)*,
    vol. 31, no. 12, pp. 2179–2195, 2008.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] E. Grimson, X. Wang, G.-W. Ng, and K. T. Ma, “Trajectory analysis and
    semantic region modeling using a nonparametric bayesian model,” in *CVPR*, 2008.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] N. Jacobs, N. Roman, and R. Pless, “Consistent temporal variations in
    many outdoor scenes,” in *CVPR*, 2007.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] A. Ess, B. Leibe, and L. Van Gool, “Depth and appearance for mobile scene
    analysis,” in *ICCV*, 2007.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] U. D. of Transportation, “Lankershim boulevard dataset,” 2007\. [Online].
    Available: [https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm](https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm)'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] U. D. of Transporation, “Next generation simulation (ngsim),” Online,
    2007.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] A. Lerner, Y. Chrysanthou, and D. Lischinski, “Crowds by example,” *Computer
    graphics forum*, vol. 26, no. 3, pp. 655–664, 2007.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] T. Pickering, “The mmt all-sky camera,” *Ground-based and Airborne Telescopes*,
    vol. 6267, p. 62671A, 2006.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions: a local
    svm approach,” in *ICPR*, vol. 3, 2004.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] T. Stoffel and A. Andreas, “NREL solar radiation research laboratory
    (srrl): Baseline measurement system (bms); golden, colorado (data),” National
    Renewable Energy Lab.(NREL), Tech. Rep., 1981.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] Yale Song, J. Vallmitjana, A. Stent, and A. Jaimes, “Tvsum: Summarizing
    web videos using titles,” in *CVPR*, 2015.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Papers with code
  id: totrans-613
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| App | Paper | Link |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
- en: '| Video | [[19](#bib.bib19)] | [https://github.com/andrewjywang/SEENet](https://github.com/andrewjywang/SEENet)
    |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
- en: '| [[11](#bib.bib11)] | [https://github.com/Yijunmaverick/FlowGrounded-VideoPrediction](https://github.com/Yijunmaverick/FlowGrounded-VideoPrediction)
    |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
- en: '| [[34](#bib.bib34)] | [https://github.com/liuem607/DYAN](https://github.com/liuem607/DYAN)
    |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
- en: '| [[229](#bib.bib229)] | [https://github.com/garyzhao/FRGAN](https://github.com/garyzhao/FRGAN)
    |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
- en: '| [[23](#bib.bib23)] | [https://github.com/jthsieh/DDPAE-video-prediction](https://github.com/jthsieh/DDPAE-video-prediction)
    |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
- en: '| [[24](#bib.bib24)] | [https://github.com/xjwxjw/VPSS](https://github.com/xjwxjw/VPSS)
    |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
- en: '| [[35](#bib.bib35)] | [https://github.com/jinbeibei/VarNet](https://github.com/jinbeibei/VarNet)
    |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
- en: '| [[25](#bib.bib25)] | [https://bit.ly/2HqiHqx](https://bit.ly/2HqiHqx) |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
- en: '| [[27](#bib.bib27)] | [https://github.com/ujjax/pred-rnn](https://github.com/ujjax/pred-rnn)
    |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
- en: '| [[28](#bib.bib28)] | [https://github.com/rubenvillegas/icml2017hierchvid](https://github.com/rubenvillegas/icml2017hierchvid)
    |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bib29)] | [https://github.com/tensorflow/models/tree/master/research/video_prediction](https://github.com/tensorflow/models/tree/master/research/video_prediction)
    |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
- en: '| [[30](#bib.bib30)] | [https://github.com/junhyukoh/nips2015-action-conditional-video-prediction](https://github.com/junhyukoh/nips2015-action-conditional-video-prediction)
    |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
- en: '| Action | [[87](#bib.bib87)] | [https://github.com/google/next-prediction](https://github.com/google/next-prediction)
    |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bib64)] | [https://github.com/fpv-iplab/rulstm](https://github.com/fpv-iplab/rulstm)
    |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
- en: '| [[71](#bib.bib71)] | [https://github.com/aras62/SF-GRU](https://github.com/aras62/SF-GRU)
    |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
- en: '| [[81](#bib.bib81)] | [https://github.com/aashi7/NearCollision](https://github.com/aashi7/NearCollision)
    |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] | [https://github.com/yabufarha/anticipating-activities](https://github.com/yabufarha/anticipating-activities)
    |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
- en: '| [[112](#bib.bib112)] | [https://github.com/gurkirt/realtime-action-detection](https://github.com/gurkirt/realtime-action-detection)
    |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
- en: '| [[80](#bib.bib80)] | [https://github.com/asheshjain399/RNNexp](https://github.com/asheshjain399/RNNexp)
    |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
- en: '| [[95](#bib.bib95)] | [https://github.com/aditya7874/Activity-Prediction-in-EgoCentric-Videos](https://github.com/aditya7874/Activity-Prediction-in-EgoCentric-Videos)
    |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
- en: '|  | [[100](#bib.bib100)] | [https://github.com/JoeHEZHAO/Spatiotemporal-Residual-Propagation](https://github.com/JoeHEZHAO/Spatiotemporal-Residual-Propagation)
    |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
- en: '| Trajectory | [[177](#bib.bib177)] | [https://go.umd.edu/TraPHic](https://go.umd.edu/TraPHic)
    |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
- en: '| [[87](#bib.bib87)] | [https://github.com/google/next-prediction](https://github.com/google/next-prediction)
    |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
- en: '| [[139](#bib.bib139)] | [https://github.com/zhangpur/SR-LSTM](https://github.com/zhangpur/SR-LSTM)
    |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
- en: '| [[144](#bib.bib144)] | [https://github.com/aras62/PIEPredict](https://github.com/aras62/PIEPredict)
    |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
- en: '| [[162](#bib.bib162)] | [https://sites.google.com/view/precog](https://sites.google.com/view/precog)
    |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
- en: '| [[150](#bib.bib150)] | [https://rebrand.ly/INFER-results](https://rebrand.ly/INFER-results)
    |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
- en: '| [[179](#bib.bib179)] | [https://github.com/wzhi/KernelTrajectoryMaps](https://github.com/wzhi/KernelTrajectoryMaps)
    |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
- en: '| [[183](#bib.bib183)] | [https://github.com/apratimbhattacharyya18/onboard_long_term_prediction](https://github.com/apratimbhattacharyya18/onboard_long_term_prediction)
    |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
- en: '| [[153](#bib.bib153)] | [https://github.com/agrimgupta92/sgan](https://github.com/agrimgupta92/sgan)
    |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
- en: '| [[155](#bib.bib155)] | [https://github.com/svip-lab/CIDNN](https://github.com/svip-lab/CIDNN)
    |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
- en: '| [[174](#bib.bib174)] | [https://github.com/yfzhang/vehicle-motion-forecasting](https://github.com/yfzhang/vehicle-motion-forecasting)
    |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
- en: '| [[165](#bib.bib165)] | [https://github.com/yadrimz/DESIRE](https://github.com/yadrimz/DESIRE)
    |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
- en: '| [[160](#bib.bib160)] | [https://github.com/quancore/social-lstm](https://github.com/quancore/social-lstm)
    |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
- en: '| Motion | [[190](#bib.bib190)] | [https://github.com/cr7anand/neural_temporal_models](https://github.com/cr7anand/neural_temporal_models)
    |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
- en: '| [[236](#bib.bib236)] | [https://github.com/BII-wushuang/Lie-Group-Motion-Prediction](https://github.com/BII-wushuang/Lie-Group-Motion-Prediction)
    |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
- en: '| [[197](#bib.bib197)] | [https://github.com/wei-mao-2019/LearnTrajDep](https://github.com/wei-mao-2019/LearnTrajDep)
    |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
- en: '| [[198](#bib.bib198)] | [https://github.com/magnux/MotionGAN](https://github.com/magnux/MotionGAN)
    |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
- en: '| [[186](#bib.bib186)] | [https://github.com/Khrylx/EgoPose](https://github.com/Khrylx/EgoPose)
    |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
- en: '| [[199](#bib.bib199)] | [https://jasonyzhang.com/phd/](https://jasonyzhang.com/phd/)
    |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
- en: '| [[187](#bib.bib187)] | [https://github.com/eddyhkchiu/pose_forecast_wacv/.](https://github.com/eddyhkchiu/pose_forecast_wacv/.)
    |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
- en: '| [[189](#bib.bib189)] | [https://github.com/ywchao/image-play](https://github.com/ywchao/image-play)
    |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
- en: '| [[194](#bib.bib194)] | [https://github.com/una-dinosauria/human-motion-prediction](https://github.com/una-dinosauria/human-motion-prediction)
    |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
- en: '| Others | [[201](#bib.bib201)] | [https://bitbucket.org/vguizilini/cvpp/src](https://bitbucket.org/vguizilini/cvpp/src)
    |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
- en: '| [[211](#bib.bib211)] | [https://github.com/facebookresearch/SegmPred](https://github.com/facebookresearch/SegmPred)
    |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: A summary of vision-based prediction papers with published code.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of papers with official published code can be found in Table [III](#A1.T3
    "TABLE III ‣ Appendix A Papers with code ‣ Deep Learning for Vision-based Prediction:
    A Survey").'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Metrics and corresponding papers
  id: totrans-662
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Metric | Papers |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
- en: '| BCE | [[23](#bib.bib23)] |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
- en: '| FVD | [[3](#bib.bib3)],[[18](#bib.bib18)] |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
- en: '| Human | [[11](#bib.bib11)],[[25](#bib.bib25)],[[8](#bib.bib8)],[[28](#bib.bib28)]
    |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
- en: '| IS | [[26](#bib.bib26)] |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
- en: '| L1 | [[9](#bib.bib9)],[[12](#bib.bib12)] |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
- en: '| LPIPS | [[3](#bib.bib3)],[[17](#bib.bib17)],[[37](#bib.bib37)],[[11](#bib.bib11)]
    |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
- en: '| MMD | [[26](#bib.bib26)] |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
- en: '| MSE | [[2](#bib.bib2)],[[4](#bib.bib4)],[[20](#bib.bib20)],[[34](#bib.bib34)],[[16](#bib.bib16)],
    [[12](#bib.bib12)],[[229](#bib.bib229)],[[23](#bib.bib23)],[[14](#bib.bib14)],[[7](#bib.bib7)],
    [[6](#bib.bib6)],[[27](#bib.bib27)],[[30](#bib.bib30)] |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
- en: '| PSNR | [[2](#bib.bib2)],[[1](#bib.bib1)],[[4](#bib.bib4)],[[31](#bib.bib31)],[[19](#bib.bib19)],
    [[20](#bib.bib20)],[[21](#bib.bib21)],[[5](#bib.bib5)],[[32](#bib.bib32)],[[33](#bib.bib33)],
    [[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[12](#bib.bib12)],[[229](#bib.bib229)],
    [[24](#bib.bib24)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[36](#bib.bib36)],
    [[6](#bib.bib6)],[[15](#bib.bib15)],[[27](#bib.bib27)],[[28](#bib.bib28)],[[29](#bib.bib29)]
    |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
- en: '| RMSE | [[11](#bib.bib11)] |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
- en: '| SSIM | [[2](#bib.bib2)],[[3](#bib.bib3)],[[1](#bib.bib1)],[[4](#bib.bib4)],[[31](#bib.bib31)],
    [[19](#bib.bib19)],[[20](#bib.bib20)],[[21](#bib.bib21)],[[5](#bib.bib5)],[[32](#bib.bib32)],
    [[33](#bib.bib33)],[[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[12](#bib.bib12)],
    [[24](#bib.bib24)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[6](#bib.bib6)],
    [[15](#bib.bib15)],[[27](#bib.bib27)],[[29](#bib.bib29)] |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Metrics used in video prediction applications.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics | Papers |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
- en: '| AP | [[88](#bib.bib88)],[[9](#bib.bib9)],[[73](#bib.bib73)],[[82](#bib.bib82)],[[79](#bib.bib79)],
    [[88](#bib.bib88)] |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
- en: '| ATTA | [[82](#bib.bib82)] |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
- en: '| ATTC | [[83](#bib.bib83)] |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
- en: '| AUC | [[71](#bib.bib71)],[[54](#bib.bib54)],[[112](#bib.bib112)] |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | [[56](#bib.bib56)],[[63](#bib.bib63)],[[64](#bib.bib64)],[[66](#bib.bib66)],[[71](#bib.bib71)],
    [[9](#bib.bib9)],[[74](#bib.bib74)],[[48](#bib.bib48)],[[49](#bib.bib49)],[[67](#bib.bib67)],
    [[68](#bib.bib68)],[[69](#bib.bib69)],[[83](#bib.bib83)],[[89](#bib.bib89)],[[75](#bib.bib75)],
    [[97](#bib.bib97)],[[90](#bib.bib90)],[[76](#bib.bib76)],[[91](#bib.bib91)],[[85](#bib.bib85)],
    [[86](#bib.bib86)],[[70](#bib.bib70)],[[50](#bib.bib50)],[[8](#bib.bib8)],[[93](#bib.bib93)],
    [[94](#bib.bib94)],[[52](#bib.bib52)],[[59](#bib.bib59)],[[42](#bib.bib42)],[[54](#bib.bib54)],
    [[43](#bib.bib43)],[[95](#bib.bib95)],[[55](#bib.bib55)],[[46](#bib.bib46)],[[40](#bib.bib40)],
    [[47](#bib.bib47)],[[98](#bib.bib98)],[[99](#bib.bib99)],[[100](#bib.bib100)],[[107](#bib.bib107)],
    [[108](#bib.bib108)],[[101](#bib.bib101)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[103](#bib.bib103)],
    [[104](#bib.bib104)],[[110](#bib.bib110)],[[111](#bib.bib111)],[[105](#bib.bib105)],[[112](#bib.bib112)],[[41](#bib.bib41)],[[106](#bib.bib106)],[[61](#bib.bib61)],[[113](#bib.bib113)],[[44](#bib.bib44)],
    [[62](#bib.bib62)],[[45](#bib.bib45)] |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
- en: '| F1 | [[71](#bib.bib71)],[[72](#bib.bib72)],[[9](#bib.bib9)],[[83](#bib.bib83)],[[90](#bib.bib90)],[[58](#bib.bib58)],[[96](#bib.bib96)],[[52](#bib.bib52)],[[42](#bib.bib42)]
    |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
- en: '| FP | [[53](#bib.bib53)] |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
- en: '| MAE | [[81](#bib.bib81)] |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
- en: '| MCC | [[76](#bib.bib76)] |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
- en: '| MRR | [[44](#bib.bib44)] |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
- en: '| PP | [[57](#bib.bib57)] |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
- en: '| Precision | [[63](#bib.bib63)],[[71](#bib.bib71)],[[72](#bib.bib72)],[[9](#bib.bib9)],[[74](#bib.bib74)],
    [[67](#bib.bib67)],[[82](#bib.bib82)],[[83](#bib.bib83)],[[70](#bib.bib70)],[[58](#bib.bib58)],
    [[51](#bib.bib51)],[[96](#bib.bib96)],[[80](#bib.bib80)],[[52](#bib.bib52)],[[42](#bib.bib42)],
    [[53](#bib.bib53)] |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
- en: '| RMSE | [[70](#bib.bib70)],[[43](#bib.bib43)],[[60](#bib.bib60)] |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
- en: '| Recall | [[63](#bib.bib63)],[[64](#bib.bib64)],[[65](#bib.bib65)],[[71](#bib.bib71)],[[9](#bib.bib9)],
    [[74](#bib.bib74)],[[67](#bib.bib67)],[[68](#bib.bib68)],[[82](#bib.bib82)],[[83](#bib.bib83)],
    [[77](#bib.bib77)],[[70](#bib.bib70)],[[51](#bib.bib51)],[[96](#bib.bib96)],[[80](#bib.bib80)],
    [[52](#bib.bib52)],[[42](#bib.bib42)],[[53](#bib.bib53)] |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
- en: '| Run time | [[9](#bib.bib9)],[[73](#bib.bib73)] |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
- en: '| TNR | [[47](#bib.bib47)] |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
- en: '| TPR | [[47](#bib.bib47)] |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
- en: '| TTA | [[84](#bib.bib84)] |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
- en: '| TTM | [[74](#bib.bib74)],[[49](#bib.bib49)],[[96](#bib.bib96)],[[80](#bib.bib80)],[[53](#bib.bib53)]
    |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
- en: '| cAP | [[92](#bib.bib92)] |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
- en: '| mAP | [[87](#bib.bib87)],[[83](#bib.bib83)],[[77](#bib.bib77)],[[84](#bib.bib84)],[[78](#bib.bib78)],[[92](#bib.bib92)],[[107](#bib.bib107)],[[112](#bib.bib112)]
    |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
- en: '| recall | [[72](#bib.bib72)],[[58](#bib.bib58)] |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Metrics used in action prediction applications.'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics | Papers |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
- en: '| ADE | [[177](#bib.bib177)],[[161](#bib.bib161)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],
    [[140](#bib.bib140)],[[141](#bib.bib141)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[144](#bib.bib144)],[[146](#bib.bib146)],[[169](#bib.bib169)],[[170](#bib.bib170)],[[148](#bib.bib148)],[[149](#bib.bib149)],
    [[182](#bib.bib182)],[[163](#bib.bib163)],[[150](#bib.bib150)],[[171](#bib.bib171)],[[151](#bib.bib151)],
    [[168](#bib.bib168)],[[172](#bib.bib172)],[[179](#bib.bib179)],[[152](#bib.bib152)],[[129](#bib.bib129)],[[183](#bib.bib183)],[[153](#bib.bib153)],[[154](#bib.bib154)],[[155](#bib.bib155)],[[101](#bib.bib101)],
    [[184](#bib.bib184)],[[164](#bib.bib164)],[[234](#bib.bib234)],[[180](#bib.bib180)],[[121](#bib.bib121)],
    [[174](#bib.bib174)],[[131](#bib.bib131)],[[158](#bib.bib158)],[[159](#bib.bib159)],[[127](#bib.bib127)],[[160](#bib.bib160)],[[115](#bib.bib115)],[[132](#bib.bib132)],[[176](#bib.bib176)],[[123](#bib.bib123)],
    [[117](#bib.bib117)],[[136](#bib.bib136)] |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
- en: '| AEDE | [[234](#bib.bib234)] |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
- en: '| ANDE | [[155](#bib.bib155)],[[160](#bib.bib160)] |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
- en: '| APP | [[157](#bib.bib157)] |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | [[167](#bib.bib167)],[[127](#bib.bib127)] |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
- en: '| CE | [[166](#bib.bib166)] |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
- en: '| DtG | [[125](#bib.bib125)] |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
- en: '| ECE | [[172](#bib.bib172)] |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
- en: '| ED | [[114](#bib.bib114)],[[173](#bib.bib173)],[[130](#bib.bib130)],[[156](#bib.bib156)],[[165](#bib.bib165)],[[124](#bib.bib124)],[[133](#bib.bib133)],[[134](#bib.bib134)],[[135](#bib.bib135)],[[136](#bib.bib136)]
    |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
- en: '| FDE | [[177](#bib.bib177)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],
    [[141](#bib.bib141)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[144](#bib.bib144)],[[146](#bib.bib146)],
    [[169](#bib.bib169)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[168](#bib.bib168)],
    [[172](#bib.bib172)],[[179](#bib.bib179)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[154](#bib.bib154)],
    [[155](#bib.bib155)],[[164](#bib.bib164)],[[180](#bib.bib180)],[[131](#bib.bib131)],[[158](#bib.bib158)],
    [[160](#bib.bib160)],[[176](#bib.bib176)] |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
- en: '| FNM | [[126](#bib.bib126)] |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
- en: '| Hit rate | [[161](#bib.bib161)],[[128](#bib.bib128)] |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
- en: '| KLD | [[181](#bib.bib181)],[[127](#bib.bib127)],[[125](#bib.bib125)] |'
  id: totrans-716
  prefs: []
  type: TYPE_TB
- en: '| L1 | [[77](#bib.bib77)] |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
- en: '| LL | [[145](#bib.bib145)],[[181](#bib.bib181)],[[168](#bib.bib168)],[[120](#bib.bib120)]
    |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
- en: '| MAnE | [[131](#bib.bib131)] |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
- en: '| MHD | [[118](#bib.bib118)],[[119](#bib.bib119)],[[125](#bib.bib125)] |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
- en: '| MSE | [[77](#bib.bib77)],[[125](#bib.bib125)] |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
- en: '| Miss rate | [[184](#bib.bib184)],[[165](#bib.bib165)] |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
- en: '| NLL | [[172](#bib.bib172)],[[183](#bib.bib183)],[[174](#bib.bib174)],[[175](#bib.bib175)],[[123](#bib.bib123)],[[125](#bib.bib125)]
    |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
- en: '| NLP | [[118](#bib.bib118)],[[119](#bib.bib119)] |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
- en: '| None | [[116](#bib.bib116)] |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
- en: '| PD | [[122](#bib.bib122)] |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
- en: '| Precision | [[115](#bib.bib115)] |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
- en: '| RMSE | [[140](#bib.bib140)],[[147](#bib.bib147)] |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
- en: '| Run time | [[147](#bib.bib147)],[[118](#bib.bib118)],[[121](#bib.bib121)],[[123](#bib.bib123)],[[135](#bib.bib135)]
    |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
- en: '| SCR | [[175](#bib.bib175)] |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
- en: '| WRMSE | [[120](#bib.bib120)] |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
- en: '| maxD | [[184](#bib.bib184)],[[165](#bib.bib165)] |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
- en: '| meanMSD | [[162](#bib.bib162)],[[166](#bib.bib166)] |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
- en: '| minADE | [[137](#bib.bib137)],[[178](#bib.bib178)],[[149](#bib.bib149)],[[168](#bib.bib168)]
    |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
- en: '| minED | [[161](#bib.bib161)],[[165](#bib.bib165)] |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
- en: '| minFDE | [[137](#bib.bib137)],[[178](#bib.bib178)] |'
  id: totrans-736
  prefs: []
  type: TYPE_TB
- en: '| minMSD | [[162](#bib.bib162)],[[166](#bib.bib166)] |'
  id: totrans-737
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: Metrics used in trajectory prediction applications.'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics | Papers |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | [[48](#bib.bib48)],[[195](#bib.bib195)] |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
- en: '| Human | [[198](#bib.bib198)],[[192](#bib.bib192)] |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
- en: '| LO | [[56](#bib.bib56)] |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
- en: '| MAnE | [[190](#bib.bib190)],[[197](#bib.bib197)],[[198](#bib.bib198)],[[196](#bib.bib196)],[[191](#bib.bib191)],[[192](#bib.bib192)],[[193](#bib.bib193)],[[194](#bib.bib194)],[[96](#bib.bib96)],[[195](#bib.bib195)]
    |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
- en: '| MJE | [[56](#bib.bib56)],[[236](#bib.bib236)],[[186](#bib.bib186)],[[200](#bib.bib200)],[[187](#bib.bib187)],[[188](#bib.bib188)],[[101](#bib.bib101)],[[103](#bib.bib103)],[[110](#bib.bib110)],[[26](#bib.bib26)],[[185](#bib.bib185)]
    |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
- en: '| MPJPE | [[197](#bib.bib197)],[[199](#bib.bib199)] |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
- en: '| NPSS | [[190](#bib.bib190)] |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
- en: '| PCK | [[199](#bib.bib199)],[[187](#bib.bib187)],[[188](#bib.bib188)],[[189](#bib.bib189)]
    |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
- en: '| PSEnt | [[198](#bib.bib198)] |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
- en: '| PSKL | [[198](#bib.bib198)] |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
- en: '| RE | [[199](#bib.bib199)] |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
- en: '| Run time | [[236](#bib.bib236)],[[188](#bib.bib188)] |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
- en: 'TABLE VII: Metrics used in motion prediction applications.'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: 'Lists of metrics and corresponding papers can be found in Tables [IV](#A2.T4
    "TABLE IV ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey"), [V](#A2.T5 "TABLE V ‣ Appendix B Metrics and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey"), [VI](#A2.T6 "TABLE
    VI ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey"), [VII](#A2.T7 "TABLE VII ‣ Appendix B Metrics and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey"), and [VIII](#A2.T8
    "TABLE VIII ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for
    Vision-based Prediction: A Survey").'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics | Papers |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
- en: '| AUC | [[209](#bib.bib209)] |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | [[8](#bib.bib8)],[[220](#bib.bib220)],[[221](#bib.bib221)] |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
- en: '| ED | [[202](#bib.bib202)],[[238](#bib.bib238)] |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
- en: '| EPE | [[212](#bib.bib212)] |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
- en: '| F1 | [[201](#bib.bib201)],[[207](#bib.bib207)] |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
- en: '| GCE | [[210](#bib.bib210)] |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
- en: '| ISM | [[203](#bib.bib203)] |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
- en: '| IoU | [[10](#bib.bib10)],[[210](#bib.bib210)],[[211](#bib.bib211)] |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
- en: '| MAE | [[239](#bib.bib239)],[[217](#bib.bib217)],[[204](#bib.bib204)] |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
- en: '| MAPE | [[216](#bib.bib216)],[[217](#bib.bib217)] |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
- en: '| MCC | [[218](#bib.bib218)] |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
- en: '| MIoU | [[212](#bib.bib212)] |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
- en: '| MSE | [[212](#bib.bib212)] |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
- en: '| PCP | [[219](#bib.bib219)] |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
- en: '| PSNR | [[206](#bib.bib206)],[[211](#bib.bib211)] |'
  id: totrans-771
  prefs: []
  type: TYPE_TB
- en: '| Precision | [[209](#bib.bib209)],[[213](#bib.bib213)],[[218](#bib.bib218)]
    |'
  id: totrans-772
  prefs: []
  type: TYPE_TB
- en: '| Psi | [[203](#bib.bib203)] |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
- en: '| RI | [[210](#bib.bib210)] |'
  id: totrans-774
  prefs: []
  type: TYPE_TB
- en: '| RMSE | [[214](#bib.bib214)] |'
  id: totrans-775
  prefs: []
  type: TYPE_TB
- en: '| ROC | [[207](#bib.bib207)],[[208](#bib.bib208)] |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
- en: '| Recall | [[209](#bib.bib209)],[[213](#bib.bib213)],[[218](#bib.bib218)] |'
  id: totrans-777
  prefs: []
  type: TYPE_TB
- en: '| Run time | [[205](#bib.bib205)],[[206](#bib.bib206)],[[209](#bib.bib209)]
    |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
- en: '| SRC | [[216](#bib.bib216)] |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
- en: '| SSIM | [[205](#bib.bib205)],[[206](#bib.bib206)],[[211](#bib.bib211)] |'
  id: totrans-780
  prefs: []
  type: TYPE_TB
- en: '| TN | [[205](#bib.bib205)] |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
- en: '| TP | [[205](#bib.bib205)] |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
- en: '| VoI | [[210](#bib.bib210)] |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
- en: '| nMAPE | [[215](#bib.bib215)] |'
  id: totrans-784
  prefs: []
  type: TYPE_TB
- en: 'TABLE VIII: Metrics used in other prediction applications.'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Metric formulas
  id: totrans-786
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 Video prediction
  id: totrans-787
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | $MSE=\frac{1}{MN}\sum_{i=1}^{M}\sum_{j=1}^{N}(I(i,j)-\tilde{I}(i,j))^{2}$
    |  |'
  id: totrans-788
  prefs: []
  type: TYPE_TB
- en: '|  | $PSNR=20\log\left(\frac{MAX_{I}}{\sqrt{MSE}}\right)$ |  |'
  id: totrans-789
  prefs: []
  type: TYPE_TB
- en: Structural Similarity (SSIM)
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $luminance(l)(x,y)=\frac{2\mu_{x}\mu_{y}+C_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+C_{1}}$
    |  |'
  id: totrans-791
  prefs: []
  type: TYPE_TB
- en: '|  | $\mu_{x}=\frac{1}{N}\sum_{i=1}^{N}x_{i}$ |  |'
  id: totrans-792
  prefs: []
  type: TYPE_TB
- en: '|  | $C_{1}=(K_{1}L)^{2}$ |  |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
- en: where $L$ is dynamic range of pixel values (e.g. 255) and $K_{1}\ll 1$ is a
    small constant.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $contrast(c)(x,y)=\frac{2\sigma_{x}\sigma_{y}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}}$
    |  |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
- en: '|  | $\sigma_{x}=\left(\frac{1}{N-1}\sum_{i=1}^{N}(x_{i}-\mu_{x})^{2}\right)^{1/2}$
    |  |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
- en: '|  | $C_{2}=(K_{2}L)^{2}$ |  |'
  id: totrans-797
  prefs: []
  type: TYPE_TB
- en: '|  | $K_{2}\ll 1$ |  |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
- en: '|  | $structure(s)(x,y)=\frac{\sigma_{xy}+C_{3}}{\sigma_{x}\sigma_{y}+C_{3}}$
    |  |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
- en: '|  | $C_{3}=(K_{3}L)^{2}$ |  |'
  id: totrans-800
  prefs: []
  type: TYPE_TB
- en: '|  | $K_{3}\ll 1$ |  |'
  id: totrans-801
  prefs: []
  type: TYPE_TB
- en: '|  | $SSIM(x,y)=[l(x,y)]^{\alpha}.[c(x,y)]^{\beta}.[s(x,y)]^{\gamma}$ |  |'
  id: totrans-802
  prefs: []
  type: TYPE_TB
- en: where $\alpha,\beta,\gamma>0$ are parameters to choose in order to adjust the
    importance.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: Learned Perceptual Image Patch Similarity (LPIPS)
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: Assume features are extracted from $L$ layers and unit-normalized in channel
    dimension, for layer l
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}^{l},\hat{y}^{l}\in R^{H_{l}\times W_{l}\times C_{l}}$ |  |'
  id: totrans-806
  prefs: []
  type: TYPE_TB
- en: .
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
- en: The distance between reference $x$ and distorted patches $x_{0}$ is given by,
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d(x,x_{0})=\sum_{l}\frac{1}{H_{l}W_{l}}\sum_{w,l}\parallel w_{l}\odot(\hat{y}^{l}_{hw},\hat{y}^{l}_{0hw})\parallel^{2}_{2}$
    |  |'
  id: totrans-809
  prefs: []
  type: TYPE_TB
- en: C.2 Action prediction
  id: totrans-810
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are 4 possibilities for classification: True positive (TP) and True Negative
    (TN) when the algorithm correctly classifies positive and negative samples, and
    False Positive (FP) and False Negative (FN) when the algorithm incorrectly classifies
    negative samples as positive and vice versa.'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Accuracy=\frac{TN+TP}{TP+TN+FP+FN}$ |  |'
  id: totrans-812
  prefs: []
  type: TYPE_TB
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  |'
  id: totrans-813
  prefs: []
  type: TYPE_TB
- en: '|  | $Recall=\frac{TP}{TP+FN}$ |  |'
  id: totrans-814
  prefs: []
  type: TYPE_TB
- en: '|  | $F1-score=2\times\frac{Precision\times Recall}{Precision+Recall}$ |  |'
  id: totrans-815
  prefs: []
  type: TYPE_TB
- en: '|  | $RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})}$ |  |'
  id: totrans-816
  prefs: []
  type: TYPE_TB
- en: Let $p(r)$ be the precision-recall curve. Then,
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $AP=\int_{0}^{1}p(r)dr$ |  |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
- en: C.3 Trajectory prediction
  id: totrans-819
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: C.3.1 Distance metrics
  id: totrans-820
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '|  | $\text{Euclidean Distance}(ED)=\parallel y-\tilde{y}\parallel=\parallel
    y-\tilde{y}\parallel_{2}$ |  |'
  id: totrans-821
  prefs: []
  type: TYPE_TB
- en: '|  | $=\sqrt{\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}}$ |  |'
  id: totrans-822
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{Mean Absolute Error}(MAE)=\frac{1}{n}\sum_{i=1}^{n}&#124;y_{i}-\tilde{y_{i}}&#124;$
    |  |'
  id: totrans-823
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{Mean Square Error}(MSE)=\parallel y-\tilde{y}\parallel^{2}$ |  |'
  id: totrans-824
  prefs: []
  type: TYPE_TB
- en: '|  | $=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}$ |  |'
  id: totrans-825
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{Root MSE}(RMSE)=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}}$
    |  |'
  id: totrans-826
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{Hausdorff Distance}(HD)=max_{y\in Y}min_{\tilde{y}\in\tilde{Y}}\parallel
    y-\tilde{y}\parallel$ |  |'
  id: totrans-827
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{Modified HD}(MHD)=max(d(Y,\tilde{Y}),d(\tilde{Y},Y))$ |  |'
  id: totrans-828
  prefs: []
  type: TYPE_TB
- en: '|  | $d(Y,\tilde{Y})=\frac{1}{N_{y}}\sum_{y\in Y}min_{\tilde{y}\in\tilde{Y}}\parallel
    y-\tilde{y}\parallel$ |  |'
  id: totrans-829
  prefs: []
  type: TYPE_TB
- en: '|  | $ADE=\frac{\sum^{N}_{i=1}\sum^{T_{pred}}_{t=1}\parallel\tilde{y}^{i}_{t}-y^{i}_{t}\parallel}{N\times
    T_{pred}}$ |  |'
  id: totrans-830
  prefs: []
  type: TYPE_TB
- en: where $N$ is the number of samples and $T_{pred}$ is the prediction steps.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $FDE=\frac{\sum^{N}_{i=1}\parallel\tilde{y}^{i}_{T_{pred}}-y^{i}_{T_{pred}}\parallel}{N}$
    |  |'
  id: totrans-832
  prefs: []
  type: TYPE_TB
- en: '|  | $minMSD=\mathbb{E}_{\tilde{Y}_{k}\sim q_{\theta}}min_{\tilde{y}\in\tilde{Y}_{k}}\parallel
    y-\tilde{y}\parallel^{2}$ |  |'
  id: totrans-833
  prefs: []
  type: TYPE_TB
- en: where $q_{\theta}$ is the sampling space and $K$ number of samples.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $meanMSD=\frac{1}{K}\sum_{k=1}^{K}\parallel y-\tilde{y}\parallel^{2}$
    |  |'
  id: totrans-835
  prefs: []
  type: TYPE_TB
- en: '|  | $NLL=\mathbb{E}_{p(Y&#124;X)}\left[-\log\prod_{t=1}^{T_{pred}}p(y_{t}&#124;X)\right]$
    |  |'
  id: totrans-836
  prefs: []
  type: TYPE_TB
- en: C.4 Motion prediction
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | $MPJPE=\frac{1}{N\times T_{pred}}\sum_{t=1}^{T_{pred}}\sum_{i=1}^{N}\parallel(J_{i}^{t}-J_{root}^{t})-(\tilde{J}_{i}^{t}-\tilde{J}_{root}^{t})\parallel$
    |  |'
  id: totrans-838
  prefs: []
  type: TYPE_TB
- en: Appendix D Links to the datasets
  id: totrans-839
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Year | Dataset | Links |'
  id: totrans-840
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-841
  prefs: []
  type: TYPE_TB
- en: '| 2019 | ARGOVerse[[137](#bib.bib137)] | [https://www.argoverse.org/data.html](https://www.argoverse.org/data.html)
    |'
  id: totrans-842
  prefs: []
  type: TYPE_TB
- en: '| CARLA[[162](#bib.bib162)] | [https://sites.google.com/view/precog](https://sites.google.com/view/precog)
    |'
  id: totrans-843
  prefs: []
  type: TYPE_TB
- en: '| EgoPose[[186](#bib.bib186)] | [https://github.com/Khrylx/EgoPose](https://github.com/Khrylx/EgoPose)
    |'
  id: totrans-844
  prefs: []
  type: TYPE_TB
- en: '| FM[[167](#bib.bib167)] | [https://mcl.korea.ac.kr/$∼$krkim/iccv2019/index.html](https://mcl.korea.ac.kr/%24%E2%88%BC%24krkim/iccv2019/index.html)
    |'
  id: totrans-845
  prefs: []
  type: TYPE_TB
- en: '| InstaVariety[[240](#bib.bib240)] | [https://github.com/akanazawa/human_dynamics](https://github.com/akanazawa/human_dynamics)
    |'
  id: totrans-846
  prefs: []
  type: TYPE_TB
- en: '| INTEARCTION[[241](#bib.bib241)] | [https://interaction-dataset.com](https://interaction-dataset.com)
    |'
  id: totrans-847
  prefs: []
  type: TYPE_TB
- en: '| Luggage[[81](#bib.bib81)] | [https://aashi7.github.io/NearCollision.html](https://aashi7.github.io/NearCollision.html)
    |'
  id: totrans-848
  prefs: []
  type: TYPE_TB
- en: '| MGIF[[242](#bib.bib242)] | [https://github.com/AliaksandrSiarohin/monkey-net](https://github.com/AliaksandrSiarohin/monkey-net)
    |'
  id: totrans-849
  prefs: []
  type: TYPE_TB
- en: '| PIE[[144](#bib.bib144)] | [http://data.nvision2.eecs.yorku.ca/PIE_dataset/](http://data.nvision2.eecs.yorku.ca/PIE_dataset/)
    |'
  id: totrans-850
  prefs: []
  type: TYPE_TB
- en: '| nuScenes[[243](#bib.bib243)] | [https://www.nuscenes.org/](https://www.nuscenes.org/)
    |'
  id: totrans-851
  prefs: []
  type: TYPE_TB
- en: '| Vehicle-Pedestrian-Mixed (VPM)[[141](#bib.bib141)] | [http://vr.ict.ac.cn/vp-lstm.](http://vr.ict.ac.cn/vp-lstm.)
    |'
  id: totrans-852
  prefs: []
  type: TYPE_TB
- en: '| TRAF[[177](#bib.bib177)] | [https://drive.google.com/drive/folders/1LqzJuRkx5yhOcjWFORO5WZ97v6jg8RHN](https://drive.google.com/drive/folders/1LqzJuRkx5yhOcjWFORO5WZ97v6jg8RHN)
    |'
  id: totrans-853
  prefs: []
  type: TYPE_TB
- en: '| 2018 | 3DPW[[244](#bib.bib244)] | [https://virtualhumans.mpi-inf.mpg.de/3DPW/](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
    |'
  id: totrans-854
  prefs: []
  type: TYPE_TB
- en: '| ActEV/VIRAT[[245](#bib.bib245)] | [https://actev.nist.gov/trecvid19](https://actev.nist.gov/trecvid19)
    |'
  id: totrans-855
  prefs: []
  type: TYPE_TB
- en: '| ACTICIPATE[[246](#bib.bib246)] | [http://vislab.isr.tecnico.ulisboa.pt/datasets/](http://vislab.isr.tecnico.ulisboa.pt/datasets/)
    |'
  id: totrans-856
  prefs: []
  type: TYPE_TB
- en: '| AVA[[247](#bib.bib247)] | [https://research.google.com/ava/](https://research.google.com/ava/)
    |'
  id: totrans-857
  prefs: []
  type: TYPE_TB
- en: '| Epic-Kitchen[[248](#bib.bib248)] | [https://epic-kitchens.github.io/2019](https://epic-kitchens.github.io/2019)
    |'
  id: totrans-858
  prefs: []
  type: TYPE_TB
- en: '| EGTEA Gaze+[[249](#bib.bib249)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
  id: totrans-859
  prefs: []
  type: TYPE_TB
- en: '| STC[[223](#bib.bib223)] | [https://svip-lab.github.io/dataset/campus_dataset.html](https://svip-lab.github.io/dataset/campus_dataset.html)
    |'
  id: totrans-860
  prefs: []
  type: TYPE_TB
- en: '| ShapeStack[[250](#bib.bib250)] | [https://shapestacks.robots.ox.ac.uk/](https://shapestacks.robots.ox.ac.uk/)
    |'
  id: totrans-861
  prefs: []
  type: TYPE_TB
- en: '| VIENA[[75](#bib.bib75)] | [https://sites.google.com/view/viena2-project/home](https://sites.google.com/view/viena2-project/home)
    |'
  id: totrans-862
  prefs: []
  type: TYPE_TB
- en: '| YouCook2[[251](#bib.bib251)] | [http://youcook2.eecs.umich.edu/](http://youcook2.eecs.umich.edu/)
    |'
  id: totrans-863
  prefs: []
  type: TYPE_TB
- en: '| 2017 | BUA[[252](#bib.bib252)] | [http://cs-people.bu.edu/sbargal/BU-action/](http://cs-people.bu.edu/sbargal/BU-action/)
    |'
  id: totrans-864
  prefs: []
  type: TYPE_TB
- en: '| CityPerson[[253](#bib.bib253)] | [https://bitbucket.org/shanshanzhang/citypersons/src/default/](https://bitbucket.org/shanshanzhang/citypersons/src/default/)
    |'
  id: totrans-865
  prefs: []
  type: TYPE_TB
- en: '| Epic-fail[[84](#bib.bib84)] | [http://aliensunmin.github.io/project/video-Forecasting/](http://aliensunmin.github.io/project/video-Forecasting/)
    |'
  id: totrans-866
  prefs: []
  type: TYPE_TB
- en: '| JAAD[[78](#bib.bib78)] | [http://data.nvision2.eecs.yorku.ca/JAAD_dataset/](http://data.nvision2.eecs.yorku.ca/JAAD_dataset/)
    |'
  id: totrans-867
  prefs: []
  type: TYPE_TB
- en: '| L-CAS[[254](#bib.bib254)] | [https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/](https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/)
    |'
  id: totrans-868
  prefs: []
  type: TYPE_TB
- en: '| Mouse Fish [[255](#bib.bib255)] | [https://web.bii.a-star.edu.sg/archive/machine_learning/Projects/behaviorAnalysis/Lie-X/Lie-X.html](https://web.bii.a-star.edu.sg/archive/machine_learning/Projects/behaviorAnalysis/Lie-X/Lie-X.html)
    |'
  id: totrans-869
  prefs: []
  type: TYPE_TB
- en: '| ORC[[256](#bib.bib256)] | [https://robotcar-dataset.robots.ox.ac.uk/](https://robotcar-dataset.robots.ox.ac.uk/)
    |'
  id: totrans-870
  prefs: []
  type: TYPE_TB
- en: '| PKU-MMD[[257](#bib.bib257)] | [http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html](http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html)
    |'
  id: totrans-871
  prefs: []
  type: TYPE_TB
- en: '| Recipe1M[[258](#bib.bib258)] | [http://pic2recipe.csail.mit.edu/](http://pic2recipe.csail.mit.edu/)
    |'
  id: totrans-872
  prefs: []
  type: TYPE_TB
- en: '| STRANDS[[259](#bib.bib259)] | [https://strands.readthedocs.io/en/latest/datasets/](https://strands.readthedocs.io/en/latest/datasets/)
    |'
  id: totrans-873
  prefs: []
  type: TYPE_TB
- en: '| 2016 | BAIR Push[[29](#bib.bib29)] | [https://sites.google.com/site/brainrobotdata/home/push-dataset](https://sites.google.com/site/brainrobotdata/home/push-dataset)
    |'
  id: totrans-874
  prefs: []
  type: TYPE_TB
- en: '| BB[[260](#bib.bib260)] | [https://github.com/mbchang/dynamics](https://github.com/mbchang/dynamics)
    |'
  id: totrans-875
  prefs: []
  type: TYPE_TB
- en: '| MU[[220](#bib.bib220)] | [http://staff.itee.uq.edu.au/lovell/MissUniverse/](http://staff.itee.uq.edu.au/lovell/MissUniverse/)
    |'
  id: totrans-876
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes[[261](#bib.bib261)] | [https://www.cityscapes-dataset.com/](https://www.cityscapes-dataset.com/)
    |'
  id: totrans-877
  prefs: []
  type: TYPE_TB
- en: '| CMU mocap[[262](#bib.bib262)] | [http://mocap.cs.cmu.edu/](http://mocap.cs.cmu.edu/)
    |'
  id: totrans-878
  prefs: []
  type: TYPE_TB
- en: '| DAD[[79](#bib.bib79)] | [https://aliensunmin.github.io/project/dashcam/](https://aliensunmin.github.io/project/dashcam/)
    |'
  id: totrans-879
  prefs: []
  type: TYPE_TB
- en: '| NTU RGB-D[[263](#bib.bib263)] | [http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp](http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp)
    |'
  id: totrans-880
  prefs: []
  type: TYPE_TB
- en: '| OA[[106](#bib.bib106)] | [http://www.mpii.de/ongoing-activity](http://www.mpii.de/ongoing-activity)
    |'
  id: totrans-881
  prefs: []
  type: TYPE_TB
- en: '| OAD[[264](#bib.bib264)] | [http://www.icst.pku.edu.cn/struct/Projects/OAD.html](http://www.icst.pku.edu.cn/struct/Projects/OAD.html)
    |'
  id: totrans-882
  prefs: []
  type: TYPE_TB
- en: '| SD[[265](#bib.bib265)] | [http://cvgl.stanford.edu/projects/uav_data/](http://cvgl.stanford.edu/projects/uav_data/)
    |'
  id: totrans-883
  prefs: []
  type: TYPE_TB
- en: '| TV Series[[266](#bib.bib266)] | [https://github.com/zhenyangli/online_action](https://github.com/zhenyangli/online_action)
    |'
  id: totrans-884
  prefs: []
  type: TYPE_TB
- en: '| VIST[[267](#bib.bib267)] | [http://visionandlanguage.net/VIST/](http://visionandlanguage.net/VIST/)
    |'
  id: totrans-885
  prefs: []
  type: TYPE_TB
- en: '| Youtube-8M[[268](#bib.bib268)] | [https://research.google.com/youtube8m/](https://research.google.com/youtube8m/)
    |'
  id: totrans-886
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Amazon[[269](#bib.bib269)] | [http://jmcauley.ucsd.edu/data/amazon/index_2014.html](http://jmcauley.ucsd.edu/data/amazon/index_2014.html)
    |'
  id: totrans-887
  prefs: []
  type: TYPE_TB
- en: '| Atari[[30](#bib.bib30)] | [https://github.com/junhyukoh/nips2015-action-conditional-video-prediction](https://github.com/junhyukoh/nips2015-action-conditional-video-prediction)
    |'
  id: totrans-888
  prefs: []
  type: TYPE_TB
- en: '| Brain4Cars[[53](#bib.bib53)] | [https://github.com/asheshjain399/ICCV2015_Brain4Cars](https://github.com/asheshjain399/ICCV2015_Brain4Cars)
    |'
  id: totrans-889
  prefs: []
  type: TYPE_TB
- en: '| CMU Panoptic[[270](#bib.bib270)] | [http://domedb.perception.cs.cmu.edu/dataset.html](http://domedb.perception.cs.cmu.edu/dataset.html)
    |'
  id: totrans-890
  prefs: []
  type: TYPE_TB
- en: '| FPPA[[95](#bib.bib95)] | [http://bvision11.cs.unc.edu/bigpen/yipin/ICCV2015/prediction_webpage/Prediction.html](http://bvision11.cs.unc.edu/bigpen/yipin/ICCV2015/prediction_webpage/Prediction.html)
    |'
  id: totrans-891
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze+[[271](#bib.bib271)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
  id: totrans-892
  prefs: []
  type: TYPE_TB
- en: '| MBI-1M[[272](#bib.bib272)] | [http://academic.mywebsiteontheinternet.com/data/](http://academic.mywebsiteontheinternet.com/data/)
    |'
  id: totrans-893
  prefs: []
  type: TYPE_TB
- en: '| MOT[[273](#bib.bib273)] | [https://motchallenge.net/](https://motchallenge.net/)
    |'
  id: totrans-894
  prefs: []
  type: TYPE_TB
- en: '| MMNIST[[274](#bib.bib274)] | [http://www.cs.toronto.edu/$∼$nitish/unsupervised_video/](http://www.cs.toronto.edu/%24%E2%88%BC%24nitish/unsupervised_video/)
    |'
  id: totrans-895
  prefs: []
  type: TYPE_TB
- en: '| SUN RGB-D[[333](#bib.bib333)] | [http://rgbd.cs.princeton.edu/](http://rgbd.cs.princeton.edu/)
    |'
  id: totrans-896
  prefs: []
  type: TYPE_TB
- en: '| SYSU 3DHOI[[276](#bib.bib276)] | [http://www.isee-ai.cn/$∼$hujianfang/ProjectJOULE.html](http://www.isee-ai.cn/%24%E2%88%BC%24hujianfang/ProjectJOULE.html)
    |'
  id: totrans-897
  prefs: []
  type: TYPE_TB
- en: '| THUMOS[[277](#bib.bib277)] | [http://www.thumos.info/home.html](http://www.thumos.info/home.html)
    |'
  id: totrans-898
  prefs: []
  type: TYPE_TB
- en: '| WnP[[278](#bib.bib278)] | [http://watchnpatch.cs.cornell.edu/](http://watchnpatch.cs.cornell.edu/)
    |'
  id: totrans-899
  prefs: []
  type: TYPE_TB
- en: '| Wider[[279](#bib.bib279)] | [http://yjxiong.me/event_recog/WIDER/](http://yjxiong.me/event_recog/WIDER/)
    |'
  id: totrans-900
  prefs: []
  type: TYPE_TB
- en: '| 2014 | Breakfast[[280](#bib.bib280)] | [http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/](http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)
    |'
  id: totrans-901
  prefs: []
  type: TYPE_TB
- en: '| Human3.6M[[237](#bib.bib237)] | [http://vision.imar.ro/human3.6m/description.php](http://vision.imar.ro/human3.6m/description.php)
    |'
  id: totrans-902
  prefs: []
  type: TYPE_TB
- en: '| MPII Human Pose[[281](#bib.bib281)] | [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)
    |'
  id: totrans-903
  prefs: []
  type: TYPE_TB
- en: '| ORGBD[[282](#bib.bib282)] | [https://sites.google.com/site/skicyyu/orgbd](https://sites.google.com/site/skicyyu/orgbd)
    |'
  id: totrans-904
  prefs: []
  type: TYPE_TB
- en: '| Sports-1M[[283](#bib.bib283)] | [https://cs.stanford.edu/people/karpathy/deepvideo/](https://cs.stanford.edu/people/karpathy/deepvideo/)
    |'
  id: totrans-905
  prefs: []
  type: TYPE_TB
- en: 'TABLE IX: A summary of datasets (from year 2014-2019) used in vision-based
    prediction papers and corresponding links.'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Dataset | Links |'
  id: totrans-907
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-908
  prefs: []
  type: TYPE_TB
- en: '| 2013 | 50 salads[[284](#bib.bib284)] | [https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/](https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/)
    |'
  id: totrans-909
  prefs: []
  type: TYPE_TB
- en: '| ATC [[285](#bib.bib285)] | [https://irc.atr.jp/crest2010_HRI/ATC_dataset/](https://irc.atr.jp/crest2010_HRI/ATC_dataset/)
    |'
  id: totrans-910
  prefs: []
  type: TYPE_TB
- en: '| CAD-120[[286](#bib.bib286)] | [http://pr.cs.cornell.edu/humanactivities/data.php](http://pr.cs.cornell.edu/humanactivities/data.php)
    |'
  id: totrans-911
  prefs: []
  type: TYPE_TB
- en: '| CHUK Avenue[[287](#bib.bib287)] | [http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html](http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html)
    |'
  id: totrans-912
  prefs: []
  type: TYPE_TB
- en: '| Daimler path[[288](#bib.bib288)] | [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Pedestrian_Path_Predict_GCPR_1/pedestrian_path_predict_gcpr_1.html](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Pedestrian_Path_Predict_GCPR_1/pedestrian_path_predict_gcpr_1.html)
    |'
  id: totrans-913
  prefs: []
  type: TYPE_TB
- en: '| JHMDB[[289](#bib.bib289)] | [http://jhmdb.is.tue.mpg.de/](http://jhmdb.is.tue.mpg.de/)
    |'
  id: totrans-914
  prefs: []
  type: TYPE_TB
- en: '| Penn Action[[290](#bib.bib290)] | [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)
    |'
  id: totrans-915
  prefs: []
  type: TYPE_TB
- en: '| 2012 | BIT[[291](#bib.bib291)] | [https://sites.google.com/site/alexkongy/software](https://sites.google.com/site/alexkongy/software)
    |'
  id: totrans-916
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze[[292](#bib.bib292)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
  id: totrans-917
  prefs: []
  type: TYPE_TB
- en: '| KITTI[[293](#bib.bib293)] | [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)
    |'
  id: totrans-918
  prefs: []
  type: TYPE_TB
- en: '| MANIAC[[294](#bib.bib294)] | [https://alexandria.physik3.uni-goettingen.de/cns-group/datasets/maniac/](https://alexandria.physik3.uni-goettingen.de/cns-group/datasets/maniac/)
    |'
  id: totrans-919
  prefs: []
  type: TYPE_TB
- en: '| MPII-cooking[[295](#bib.bib295)] | [https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset/)
    |'
  id: totrans-920
  prefs: []
  type: TYPE_TB
- en: '| MSRDA[[296](#bib.bib296)] | [https://documents.uow.edu.au/$∼$wanqing/#MSRAction3DDatasets](https://documents.uow.edu.au/%24%E2%88%BC%24wanqing/#MSRAction3DDatasets)
    |'
  id: totrans-921
  prefs: []
  type: TYPE_TB
- en: '| GC[[297](#bib.bib297)] | [http://www.ee.cuhk.edu.hk/$∼$xgwang/grandcentral.html](http://www.ee.cuhk.edu.hk/%24%E2%88%BC%24xgwang/grandcentral.html)
    |'
  id: totrans-922
  prefs: []
  type: TYPE_TB
- en: '| SBUKI[[298](#bib.bib298)] | [https://www3.cs.stonybrook.edu/$∼$kyun/research/kinect_interaction/index.html](https://www3.cs.stonybrook.edu/%24%E2%88%BC%24kyun/research/kinect_interaction/index.html)
    |'
  id: totrans-923
  prefs: []
  type: TYPE_TB
- en: '| UCF-101[[299](#bib.bib299)] | [https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php)
    |'
  id: totrans-924
  prefs: []
  type: TYPE_TB
- en: '| UTKA[[300](#bib.bib300)] | [http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html)
    |'
  id: totrans-925
  prefs: []
  type: TYPE_TB
- en: '| UvA-NEMO[[301](#bib.bib301)] | [https://www.uva-nemo.org/](https://www.uva-nemo.org/)
    |'
  id: totrans-926
  prefs: []
  type: TYPE_TB
- en: '| 2011 | FCVL[[302](#bib.bib302)] | [http://robots.engin.umich.edu/SoftwareData/Ford](http://robots.engin.umich.edu/SoftwareData/Ford)
    |'
  id: totrans-927
  prefs: []
  type: TYPE_TB
- en: '| HMDB[[303](#bib.bib303)] | [http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)
    |'
  id: totrans-928
  prefs: []
  type: TYPE_TB
- en: '| Stanford 40[[304](#bib.bib304)] | [http://vision.stanford.edu/Datasets/40actions.html](http://vision.stanford.edu/Datasets/40actions.html)
    |'
  id: totrans-929
  prefs: []
  type: TYPE_TB
- en: '| Town Center[[305](#bib.bib305)] | [http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets](http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets)
    |'
  id: totrans-930
  prefs: []
  type: TYPE_TB
- en: '| VIRAT[[306](#bib.bib306)] | [http://viratdata.org/](http://viratdata.org/)
    |'
  id: totrans-931
  prefs: []
  type: TYPE_TB
- en: '| 2010 | DISPLECS[[307](#bib.bib307)] | [https://cvssp.org/data/diplecs/](https://cvssp.org/data/diplecs/)
    |'
  id: totrans-932
  prefs: []
  type: TYPE_TB
- en: '| MSR[[308](#bib.bib308)] | [https://www.microsoft.com/en-us/download/details.aspx?id=52315](https://www.microsoft.com/en-us/download/details.aspx?id=52315)
    |'
  id: totrans-933
  prefs: []
  type: TYPE_TB
- en: '| MUG Facial Expression[[309](#bib.bib309)] | [https://mug.ee.auth.gr/fed/](https://mug.ee.auth.gr/fed/)
    |'
  id: totrans-934
  prefs: []
  type: TYPE_TB
- en: '| PROST[[310](#bib.bib310)] | [www.gpu4vision.com](www.gpu4vision.com) |'
  id: totrans-935
  prefs: []
  type: TYPE_TB
- en: '| THI[[311](#bib.bib311)] | [http://www.robots.ox.ac.uk/$∼$alonso/tv_human_interactions.html](http://www.robots.ox.ac.uk/%24%E2%88%BC%24alonso/tv_human_interactions.html)
    |'
  id: totrans-936
  prefs: []
  type: TYPE_TB
- en: '| UTI[[312](#bib.bib312)] | [http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)
    |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
- en: '| VISOR[[313](#bib.bib313)] | [imagelab.ing.unimore.it/visor](imagelab.ing.unimore.it/visor)
    |'
  id: totrans-938
  prefs: []
  type: TYPE_TB
- en: '| Willow Action[[314](#bib.bib314)] | [https://www.di.ens.fr/willow/research/stillactions/](https://www.di.ens.fr/willow/research/stillactions/)
    |'
  id: totrans-939
  prefs: []
  type: TYPE_TB
- en: '| 2009 | Caltech Pedestrian[[315](#bib.bib315)] | [http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)
    |'
  id: totrans-940
  prefs: []
  type: TYPE_TB
- en: '| Collective Activity (CA)[[316](#bib.bib316)] | [http://www-personal.umich.edu/$∼$wgchoi/eccv12/wongun_eccv12.html](http://www-personal.umich.edu/%24%E2%88%BC%24wgchoi/eccv12/wongun_eccv12.html)
    |'
  id: totrans-941
  prefs: []
  type: TYPE_TB
- en: '| EIFP[[317](#bib.bib317)] | [http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/](http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/)
    |'
  id: totrans-942
  prefs: []
  type: TYPE_TB
- en: '| ETH[[235](#bib.bib235)] | [http://www.vision.ee.ethz.ch/en/datasets/](http://www.vision.ee.ethz.ch/en/datasets/)
    |'
  id: totrans-943
  prefs: []
  type: TYPE_TB
- en: '| OSU[[318](#bib.bib318)] | [http://eecs.oregonstate.edu/football/tracking/dataset](http://eecs.oregonstate.edu/football/tracking/dataset)
    |'
  id: totrans-944
  prefs: []
  type: TYPE_TB
- en: '| PETS2009[[319](#bib.bib319)] | [http://www.cvg.reading.ac.uk/PETS2009/a.html](http://www.cvg.reading.ac.uk/PETS2009/a.html)
    |'
  id: totrans-945
  prefs: []
  type: TYPE_TB
- en: '| QMUL[[320](#bib.bib320)] | [http://personal.ie.cuhk.edu.hk/$∼$ccloy/downloads_qmul_junction.html](http://personal.ie.cuhk.edu.hk/%24%E2%88%BC%24ccloy/downloads_qmul_junction.html)
    |'
  id: totrans-946
  prefs: []
  type: TYPE_TB
- en: '| TUM Kitchen[[321](#bib.bib321)] | [https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data](https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data)
    |'
  id: totrans-947
  prefs: []
  type: TYPE_TB
- en: '| YUV Videos[[322](#bib.bib322)] | [http://trace.kom.aau.dk/yuv/index.html](http://trace.kom.aau.dk/yuv/index.html)
    |'
  id: totrans-948
  prefs: []
  type: TYPE_TB
- en: '| 2008 | Daimler[[323](#bib.bib323)] | [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html)
    |'
  id: totrans-949
  prefs: []
  type: TYPE_TB
- en: '| MITT[[324](#bib.bib324)] | [http://www.ee.cuhk.edu.hk/$∼$xgwang/MITtrajsingle.html](http://www.ee.cuhk.edu.hk/%24%E2%88%BC%24xgwang/MITtrajsingle.html)
    |'
  id: totrans-950
  prefs: []
  type: TYPE_TB
- en: '| 2007 | AMOS[[325](#bib.bib325)] | [http://amos.cse.wustl.edu/](http://amos.cse.wustl.edu/)
    |'
  id: totrans-951
  prefs: []
  type: TYPE_TB
- en: '| ETH pedestrian[[326](#bib.bib326)] | [https://data.vision.ee.ethz.ch/cvl/aess/](https://data.vision.ee.ethz.ch/cvl/aess/)
    |'
  id: totrans-952
  prefs: []
  type: TYPE_TB
- en: '| Lankershim Boulevard[[327](#bib.bib327)] | [https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm](https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm)
    |'
  id: totrans-953
  prefs: []
  type: TYPE_TB
- en: '| NGSIM[[328](#bib.bib328)] | [https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm](https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm)
    |'
  id: totrans-954
  prefs: []
  type: TYPE_TB
- en: '| UCY[[329](#bib.bib329)] | [https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data](https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data)
    |'
  id: totrans-955
  prefs: []
  type: TYPE_TB
- en: '| 2006 | Tuscan Arizona[[330](#bib.bib330)] | [http://www.mmto.org/](http://www.mmto.org/)
    |'
  id: totrans-956
  prefs: []
  type: TYPE_TB
- en: '| 2004 | KTH[[331](#bib.bib331)] | [http://www.nada.kth.se/cvap/actions/](http://www.nada.kth.se/cvap/actions/)
    |'
  id: totrans-957
  prefs: []
  type: TYPE_TB
- en: '| 1981 | Golden Colorado[[332](#bib.bib332)] | [https://www.osti.gov/dataexplorer/biblio/dataset/1052221](https://www.osti.gov/dataexplorer/biblio/dataset/1052221)
    |'
  id: totrans-958
  prefs: []
  type: TYPE_TB
- en: 'TABLE X: A summary of datasets (from year 2013 and earlier) used in vision-based
    prediction papers and corresponding links.'
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
- en: 'Lists of datasets with associated repository links can be found in Tables [IX](#A4.T9
    "TABLE IX ‣ Appendix D Links to the datasets ‣ Deep Learning for Vision-based
    Prediction: A Survey") and [X](#A4.T10 "TABLE X ‣ Appendix D Links to the datasets
    ‣ Deep Learning for Vision-based Prediction: A Survey").'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Datasets and corresponding papers
  id: totrans-961
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lists of datasets and corresponding papers can be found in Tables [XI](#A5.T11
    "TABLE XI ‣ Appendix E Datasets and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey"), [XII](#A5.T12 "TABLE XII ‣ Appendix E Datasets and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey"), [XIII](#A5.T13
    "TABLE XIII ‣ Appendix E Datasets and corresponding papers ‣ Deep Learning for
    Vision-based Prediction: A Survey"), [XIV](#A5.T14 "TABLE XIV ‣ Appendix E Datasets
    and corresponding papers ‣ Deep Learning for Vision-based Prediction: A Survey"),
    and [XV](#A5.T15 "TABLE XV ‣ Appendix E Datasets and corresponding papers ‣ Deep
    Learning for Vision-based Prediction: A Survey").'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Papers |'
  id: totrans-963
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
- en: '| Atari | [[30](#bib.bib30)] |'
  id: totrans-965
  prefs: []
  type: TYPE_TB
- en: '| BAIR Push | [[3](#bib.bib3)] ,[[24](#bib.bib24)],[[29](#bib.bib29)] |'
  id: totrans-966
  prefs: []
  type: TYPE_TB
- en: '| Bouncing Ball | [[23](#bib.bib23)] |'
  id: totrans-967
  prefs: []
  type: TYPE_TB
- en: '| CHUK Avenue | [[2](#bib.bib2)] |'
  id: totrans-968
  prefs: []
  type: TYPE_TB
- en: '| Caltech Pedestrian | [[20](#bib.bib20)],[[33](#bib.bib33)] , [[2](#bib.bib2)],[[1](#bib.bib1)],[[34](#bib.bib34)],[[12](#bib.bib12)],[[6](#bib.bib6)],
    [[4](#bib.bib4)] |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes | [[3](#bib.bib3)],[[32](#bib.bib32)] |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
- en: '| Human 3.6M | [[32](#bib.bib32)],[[33](#bib.bib33)],[[22](#bib.bib22)],[[24](#bib.bib24)],[[14](#bib.bib14)],[[25](#bib.bib25)],[[7](#bib.bib7)],[[28](#bib.bib28)],[[29](#bib.bib29)]
    |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
- en: '| JAAD | [[9](#bib.bib9)] |'
  id: totrans-972
  prefs: []
  type: TYPE_TB
- en: '| JHMDB | [[21](#bib.bib21)] |'
  id: totrans-973
  prefs: []
  type: TYPE_TB
- en: '| KITTI | [[2](#bib.bib2)],[[1](#bib.bib1)],[[20](#bib.bib20)],[[33](#bib.bib33)],[[34](#bib.bib34)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[6](#bib.bib6)],[[15](#bib.bib15)]
    |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
- en: '| KTH | [[31](#bib.bib31)],[[19](#bib.bib19)],[[11](#bib.bib11)],[[16](#bib.bib16)],[[13](#bib.bib13)],[[35](#bib.bib35)],[[27](#bib.bib27)]
    |'
  id: totrans-975
  prefs: []
  type: TYPE_TB
- en: '| MGIF | [[18](#bib.bib18)] |'
  id: totrans-976
  prefs: []
  type: TYPE_TB
- en: '| MMNIST | [[3](#bib.bib3)],[[31](#bib.bib31)],[[19](#bib.bib19)],[[16](#bib.bib16)],[[23](#bib.bib23)],[[24](#bib.bib24)],[[36](#bib.bib36)],[[8](#bib.bib8)],[[27](#bib.bib27)]
    |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
- en: '| MSR | [[19](#bib.bib19)] |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
- en: '| MUG | [[229](#bib.bib229)] |'
  id: totrans-979
  prefs: []
  type: TYPE_TB
- en: '| Own | [[37](#bib.bib37)],[[25](#bib.bib25)] |'
  id: totrans-980
  prefs: []
  type: TYPE_TB
- en: '| PROST | [[36](#bib.bib36)] |'
  id: totrans-981
  prefs: []
  type: TYPE_TB
- en: '| Penn Action | [[21](#bib.bib21)],[[229](#bib.bib229)],[[26](#bib.bib26)]
    |'
  id: totrans-982
  prefs: []
  type: TYPE_TB
- en: '| Penn action | [[17](#bib.bib17)],[[18](#bib.bib18)],[[28](#bib.bib28)] |'
  id: totrans-983
  prefs: []
  type: TYPE_TB
- en: '| ShanghaiTech Campus | [[2](#bib.bib2)] |'
  id: totrans-984
  prefs: []
  type: TYPE_TB
- en: '| ShapeStack | [[17](#bib.bib17)] |'
  id: totrans-985
  prefs: []
  type: TYPE_TB
- en: '| Sports-1M | [[36](#bib.bib36)],[[15](#bib.bib15)] |'
  id: totrans-986
  prefs: []
  type: TYPE_TB
- en: '| THUMOS | [[6](#bib.bib6)] |'
  id: totrans-987
  prefs: []
  type: TYPE_TB
- en: '| UCF-101 | [[2](#bib.bib2)],[[4](#bib.bib4)],[[5](#bib.bib5)],[[32](#bib.bib32)],[[33](#bib.bib33)],[[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[36](#bib.bib36)],[[6](#bib.bib6)],[[26](#bib.bib26)],[[15](#bib.bib15)]
    |'
  id: totrans-988
  prefs: []
  type: TYPE_TB
- en: '| UvA-NEMO | [[18](#bib.bib18)] |'
  id: totrans-989
  prefs: []
  type: TYPE_TB
- en: '| ViSOR | [[36](#bib.bib36)] |'
  id: totrans-990
  prefs: []
  type: TYPE_TB
- en: '| YUV | [[4](#bib.bib4)],[[20](#bib.bib20)] |'
  id: totrans-991
  prefs: []
  type: TYPE_TB
- en: '| Youtube-8M | [[12](#bib.bib12)] |'
  id: totrans-992
  prefs: []
  type: TYPE_TB
- en: '| pedestrian | [[4](#bib.bib4)] |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
- en: 'TABLE XI: Datasets used in video prediction applications.'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Papers |'
  id: totrans-995
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-996
  prefs: []
  type: TYPE_TB
- en: '| 50Salad | [[63](#bib.bib63)],[[66](#bib.bib66)],[[69](#bib.bib69)] |'
  id: totrans-997
  prefs: []
  type: TYPE_TB
- en: '| ANTICIPATE | [[90](#bib.bib90)] |'
  id: totrans-998
  prefs: []
  type: TYPE_TB
- en: '| AVA | [[88](#bib.bib88)],[[88](#bib.bib88)] |'
  id: totrans-999
  prefs: []
  type: TYPE_TB
- en: '| ActEV/VIRAT | [[87](#bib.bib87)] |'
  id: totrans-1000
  prefs: []
  type: TYPE_TB
- en: '| BIT | [[100](#bib.bib100)],[[109](#bib.bib109)],[[111](#bib.bib111)],[[113](#bib.bib113)]
    |'
  id: totrans-1001
  prefs: []
  type: TYPE_TB
- en: '| BU Action | [[107](#bib.bib107)] |'
  id: totrans-1002
  prefs: []
  type: TYPE_TB
- en: '| Brain4Cars | [[96](#bib.bib96)],[[80](#bib.bib80)],[[53](#bib.bib53)] |'
  id: totrans-1003
  prefs: []
  type: TYPE_TB
- en: '| Breakfast | [[66](#bib.bib66)],[[67](#bib.bib67)],[[69](#bib.bib69)] |'
  id: totrans-1004
  prefs: []
  type: TYPE_TB
- en: '| CA | [[101](#bib.bib101)] |'
  id: totrans-1005
  prefs: []
  type: TYPE_TB
- en: '| CAD-120 | [[67](#bib.bib67)],[[90](#bib.bib90)],[[58](#bib.bib58)],[[96](#bib.bib96)],[[52](#bib.bib52)]
    |'
  id: totrans-1006
  prefs: []
  type: TYPE_TB
- en: '| CMU Panoptic | [[56](#bib.bib56)] |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
- en: '| CMU Mocap | [[110](#bib.bib110)] |'
  id: totrans-1008
  prefs: []
  type: TYPE_TB
- en: '| Caltech Pedestrian | [[54](#bib.bib54)] |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
- en: '| DAD | [[83](#bib.bib83)],[[84](#bib.bib84)] |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
- en: '| Daimler | [[54](#bib.bib54)] |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
- en: '| Daimler Path | [[40](#bib.bib40)] |'
  id: totrans-1012
  prefs: []
  type: TYPE_TB
- en: '| EGTEA Gaze+ | [[64](#bib.bib64)] |'
  id: totrans-1013
  prefs: []
  type: TYPE_TB
- en: '| ETH Pedestrian | [[54](#bib.bib54)] |'
  id: totrans-1014
  prefs: []
  type: TYPE_TB
- en: '| Epic-fail | [[84](#bib.bib84)] |'
  id: totrans-1015
  prefs: []
  type: TYPE_TB
- en: '| Epic-Kitchen | [[63](#bib.bib63)] ,[[64](#bib.bib64)],[[68](#bib.bib68)]
    |'
  id: totrans-1016
  prefs: []
  type: TYPE_TB
- en: '| FPPA | [[95](#bib.bib95)] |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze | [[89](#bib.bib89)] |'
  id: totrans-1018
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze+ | [[89](#bib.bib89)] |'
  id: totrans-1019
  prefs: []
  type: TYPE_TB
- en: '| HMDB | [[104](#bib.bib104)] |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
- en: '| Human 3.6M | [[110](#bib.bib110)] |'
  id: totrans-1021
  prefs: []
  type: TYPE_TB
- en: '| JAAD | [[9](#bib.bib9)],[[73](#bib.bib73)],[[75](#bib.bib75)],[[78](#bib.bib78)]
    |'
  id: totrans-1022
  prefs: []
  type: TYPE_TB
- en: '| JHMDB | [[88](#bib.bib88)],[[88](#bib.bib88)],[[100](#bib.bib100)],[[102](#bib.bib102)],[[105](#bib.bib105)],[[112](#bib.bib112)]
    |'
  id: totrans-1023
  prefs: []
  type: TYPE_TB
- en: '| Luggage | [[81](#bib.bib81)] |'
  id: totrans-1024
  prefs: []
  type: TYPE_TB
- en: '| MANIAC | [[57](#bib.bib57)] |'
  id: totrans-1025
  prefs: []
  type: TYPE_TB
- en: '| MPII Cooking | [[67](#bib.bib67)],[[70](#bib.bib70)],[[60](#bib.bib60)] |'
  id: totrans-1026
  prefs: []
  type: TYPE_TB
- en: '| MSRDA | [[45](#bib.bib45)] |'
  id: totrans-1027
  prefs: []
  type: TYPE_TB
- en: '| NGSIM | [[72](#bib.bib72)],[[74](#bib.bib74)],[[97](#bib.bib97)] |'
  id: totrans-1028
  prefs: []
  type: TYPE_TB
- en: '| NTU RGB-D | [[98](#bib.bib98)] |'
  id: totrans-1029
  prefs: []
  type: TYPE_TB
- en: '| OA | [[106](#bib.bib106)] |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
- en: '| OAD | [[108](#bib.bib108)] |'
  id: totrans-1031
  prefs: []
  type: TYPE_TB
- en: '| ORGBD | [[41](#bib.bib41)] |'
  id: totrans-1032
  prefs: []
  type: TYPE_TB
- en: '| PIE | [[71](#bib.bib71)] |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
- en: '| PKU-MMD | [[108](#bib.bib108)] |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
- en: '| Recipe1M | [[65](#bib.bib65)] |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
- en: '| SBUIK | [[101](#bib.bib101)] |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
- en: '| SYSU 3DHOI | [[98](#bib.bib98)],[[41](#bib.bib41)] |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
- en: '| Stanford-40 | [[107](#bib.bib107)] |'
  id: totrans-1038
  prefs: []
  type: TYPE_TB
- en: '| THUMOS | [[91](#bib.bib91)],[[92](#bib.bib92)],[[93](#bib.bib93)] |'
  id: totrans-1039
  prefs: []
  type: TYPE_TB
- en: '| TV Human Interaction | [[99](#bib.bib99)] , [[91](#bib.bib91)],[[8](#bib.bib8)],[[92](#bib.bib92)],[[93](#bib.bib93)]
    |'
  id: totrans-1040
  prefs: []
  type: TYPE_TB
- en: '| TV Series | [[92](#bib.bib92)] |'
  id: totrans-1041
  prefs: []
  type: TYPE_TB
- en: '| UCF-101 | [[98](#bib.bib98)],[[99](#bib.bib99)],[[100](#bib.bib100)],[[107](#bib.bib107)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[104](#bib.bib104)],[[111](#bib.bib111)],[[105](#bib.bib105)],[[112](#bib.bib112)],[[61](#bib.bib61)]
    |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
- en: '| UTI | [[99](#bib.bib99)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[105](#bib.bib105)],[[61](#bib.bib61)],[[113](#bib.bib113)],[[44](#bib.bib44)]
    |'
  id: totrans-1043
  prefs: []
  type: TYPE_TB
- en: '| UTKA | [[94](#bib.bib94)] |'
  id: totrans-1044
  prefs: []
  type: TYPE_TB
- en: '| VIENA | [[75](#bib.bib75)] |'
  id: totrans-1045
  prefs: []
  type: TYPE_TB
- en: '| VIRAT | [[70](#bib.bib70)] |'
  id: totrans-1046
  prefs: []
  type: TYPE_TB
- en: '| WIDER | [[107](#bib.bib107)] |'
  id: totrans-1047
  prefs: []
  type: TYPE_TB
- en: '| Willow Action | [[107](#bib.bib107)] |'
  id: totrans-1048
  prefs: []
  type: TYPE_TB
- en: '| WnP | [[94](#bib.bib94)] |'
  id: totrans-1049
  prefs: []
  type: TYPE_TB
- en: '| YouCook2 | [[65](#bib.bib65)] |'
  id: totrans-1050
  prefs: []
  type: TYPE_TB
- en: '| Sports-1M | [[111](#bib.bib111)] |'
  id: totrans-1051
  prefs: []
  type: TYPE_TB
- en: 'TABLE XII: Datasets used in action prediction applications.'
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Papers |'
  id: totrans-1053
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-1054
  prefs: []
  type: TYPE_TB
- en: '| ARGOVerse | [[137](#bib.bib137)] |'
  id: totrans-1055
  prefs: []
  type: TYPE_TB
- en: '| ATC | [[118](#bib.bib118)] |'
  id: totrans-1056
  prefs: []
  type: TYPE_TB
- en: '| ActEV/VIRAT | [[87](#bib.bib87)] |'
  id: totrans-1057
  prefs: []
  type: TYPE_TB
- en: '| CA | [[101](#bib.bib101)] |'
  id: totrans-1058
  prefs: []
  type: TYPE_TB
- en: '| CARLA | [[162](#bib.bib162)],[[147](#bib.bib147)] |'
  id: totrans-1059
  prefs: []
  type: TYPE_TB
- en: '| CHUK | [[155](#bib.bib155)] |'
  id: totrans-1060
  prefs: []
  type: TYPE_TB
- en: '| CityPerson | [[183](#bib.bib183)] |'
  id: totrans-1061
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes | [[150](#bib.bib150)] |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
- en: '| Daimler Path | [[136](#bib.bib136)] |'
  id: totrans-1063
  prefs: []
  type: TYPE_TB
- en: '| ETH | [[178](#bib.bib178)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[146](#bib.bib146)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[155](#bib.bib155)],[[164](#bib.bib164)],[[156](#bib.bib156)],[[180](#bib.bib180)],[[158](#bib.bib158)],[[160](#bib.bib160)]
    |'
  id: totrans-1064
  prefs: []
  type: TYPE_TB
- en: '| Edinburgh (IFP) | [[114](#bib.bib114)] , [[179](#bib.bib179)] |'
  id: totrans-1065
  prefs: []
  type: TYPE_TB
- en: '| FM | [[167](#bib.bib167)] |'
  id: totrans-1066
  prefs: []
  type: TYPE_TB
- en: '| GC | [[155](#bib.bib155)],[[115](#bib.bib115)],[[176](#bib.bib176)],[[135](#bib.bib135)]
    |'
  id: totrans-1067
  prefs: []
  type: TYPE_TB
- en: '| INTEARCTION | [[163](#bib.bib163)] |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
- en: '| JAAD | [[144](#bib.bib144)] |'
  id: totrans-1069
  prefs: []
  type: TYPE_TB
- en: '| KITTI | [[150](#bib.bib150)],[[166](#bib.bib166)],[[165](#bib.bib165)] |'
  id: totrans-1070
  prefs: []
  type: TYPE_TB
- en: '| L-CAS | [[234](#bib.bib234)] |'
  id: totrans-1071
  prefs: []
  type: TYPE_TB
- en: '| Lankershim Boulevard | [[179](#bib.bib179)] |'
  id: totrans-1072
  prefs: []
  type: TYPE_TB
- en: '| MITT | [[135](#bib.bib135)] |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
- en: '| MOT | [[129](#bib.bib129)] |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
- en: '| NGSIM | [[177](#bib.bib177)],[[140](#bib.bib140)],[[141](#bib.bib141)],[[145](#bib.bib145)],[[148](#bib.bib148)],[[181](#bib.bib181)],[[182](#bib.bib182)]
    |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
- en: '| OSU | [[125](#bib.bib125)] |'
  id: totrans-1076
  prefs: []
  type: TYPE_TB
- en: '| Oxford | [[150](#bib.bib150)] |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
- en: '| PETS2009 | [[152](#bib.bib152)] |'
  id: totrans-1078
  prefs: []
  type: TYPE_TB
- en: '| PIE | [[144](#bib.bib144)] |'
  id: totrans-1079
  prefs: []
  type: TYPE_TB
- en: '| QMUL | [[115](#bib.bib115)] |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
- en: '| SBUIK | [[101](#bib.bib101)] |'
  id: totrans-1081
  prefs: []
  type: TYPE_TB
- en: '| SD | [[178](#bib.bib178)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[163](#bib.bib163)],[[152](#bib.bib152)],[[165](#bib.bib165)],[[132](#bib.bib132)]
    |'
  id: totrans-1082
  prefs: []
  type: TYPE_TB
- en: '| STRANDS | [[234](#bib.bib234)] |'
  id: totrans-1083
  prefs: []
  type: TYPE_TB
- en: '| TRAF | [[177](#bib.bib177)] |'
  id: totrans-1084
  prefs: []
  type: TYPE_TB
- en: '| TUM Kitchen | [[134](#bib.bib134)] |'
  id: totrans-1085
  prefs: []
  type: TYPE_TB
- en: '| Town Center | [[154](#bib.bib154)],[[131](#bib.bib131)],[[175](#bib.bib175)]
    |'
  id: totrans-1086
  prefs: []
  type: TYPE_TB
- en: '| UCY | [[178](#bib.bib178)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[145](#bib.bib145)],[[146](#bib.bib146)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[154](#bib.bib154)],[[155](#bib.bib155)],[[164](#bib.bib164)],[[180](#bib.bib180)],[[131](#bib.bib131)],[[158](#bib.bib158)],[[159](#bib.bib159)],[[175](#bib.bib175)],[[160](#bib.bib160)],[[132](#bib.bib132)]
    |'
  id: totrans-1087
  prefs: []
  type: TYPE_TB
- en: '| VIRAT | [[123](#bib.bib123)] |'
  id: totrans-1088
  prefs: []
  type: TYPE_TB
- en: '| VPM | [[141](#bib.bib141)] |'
  id: totrans-1089
  prefs: []
  type: TYPE_TB
- en: '| nuScenes | [[162](#bib.bib162)] |'
  id: totrans-1090
  prefs: []
  type: TYPE_TB
- en: 'TABLE XIII: Datasets used in trajectory prediction applications.'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Papers |'
  id: totrans-1092
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-1093
  prefs: []
  type: TYPE_TB
- en: '| 3DPW | [[197](#bib.bib197)] |'
  id: totrans-1094
  prefs: []
  type: TYPE_TB
- en: '| CA | [[101](#bib.bib101)] |'
  id: totrans-1095
  prefs: []
  type: TYPE_TB
- en: '| CMU Mocap | [[197](#bib.bib197)] |'
  id: totrans-1096
  prefs: []
  type: TYPE_TB
- en: '| CMU Panoptic | [[56](#bib.bib56)] |'
  id: totrans-1097
  prefs: []
  type: TYPE_TB
- en: '| Egopose | [[186](#bib.bib186)] |'
  id: totrans-1098
  prefs: []
  type: TYPE_TB
- en: '| Human 3.6M | [[187](#bib.bib187)],[[193](#bib.bib193)],[[96](#bib.bib96)],[[195](#bib.bib195)],[[190](#bib.bib190)],[[236](#bib.bib236)],[[197](#bib.bib197)],[[198](#bib.bib198)],[[196](#bib.bib196)],[[199](#bib.bib199)],[[191](#bib.bib191)],[[192](#bib.bib192)],[[110](#bib.bib110)],[[189](#bib.bib189)],[[194](#bib.bib194)]
    |'
  id: totrans-1099
  prefs: []
  type: TYPE_TB
- en: '| InstaVariety | [[199](#bib.bib199)] |'
  id: totrans-1100
  prefs: []
  type: TYPE_TB
- en: '| MPII Human Pose | [[189](#bib.bib189)] |'
  id: totrans-1101
  prefs: []
  type: TYPE_TB
- en: '| Mouse Fish | [[236](#bib.bib236)] |'
  id: totrans-1102
  prefs: []
  type: TYPE_TB
- en: '| Own | [[48](#bib.bib48)],[[200](#bib.bib200)],[[188](#bib.bib188)],[[103](#bib.bib103)],[[185](#bib.bib185)]
    |'
  id: totrans-1103
  prefs: []
  type: TYPE_TB
- en: '| Penn Action | [[199](#bib.bib199)],[[187](#bib.bib187)],[[26](#bib.bib26)],[[189](#bib.bib189)]
    |'
  id: totrans-1104
  prefs: []
  type: TYPE_TB
- en: '| SBUIK | [[101](#bib.bib101)] |'
  id: totrans-1105
  prefs: []
  type: TYPE_TB
- en: '| UCF-101 | [[26](#bib.bib26)] |'
  id: totrans-1106
  prefs: []
  type: TYPE_TB
- en: 'TABLE XIV: Datasets used in motion prediction applications.'
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Papers |'
  id: totrans-1108
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-1109
  prefs: []
  type: TYPE_TB
- en: '| AMOS | [[214](#bib.bib214)] |'
  id: totrans-1110
  prefs: []
  type: TYPE_TB
- en: '| Amazon | [[217](#bib.bib217)] |'
  id: totrans-1111
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes | [[10](#bib.bib10)],[[210](#bib.bib210)],[[211](#bib.bib211)],[[212](#bib.bib212)]
    |'
  id: totrans-1112
  prefs: []
  type: TYPE_TB
- en: '| DIPLECS | [[239](#bib.bib239)] |'
  id: totrans-1113
  prefs: []
  type: TYPE_TB
- en: '| FCVL | [[209](#bib.bib209)] |'
  id: totrans-1114
  prefs: []
  type: TYPE_TB
- en: '| Golden Colorado | [[215](#bib.bib215)] |'
  id: totrans-1115
  prefs: []
  type: TYPE_TB
- en: '| JAAD | [[203](#bib.bib203)] |'
  id: totrans-1116
  prefs: []
  type: TYPE_TB
- en: '| KITTI | [[205](#bib.bib205)],[[201](#bib.bib201)] |'
  id: totrans-1117
  prefs: []
  type: TYPE_TB
- en: '| MBI-1M | [[216](#bib.bib216)] |'
  id: totrans-1118
  prefs: []
  type: TYPE_TB
- en: '| MU | [[220](#bib.bib220)] |'
  id: totrans-1119
  prefs: []
  type: TYPE_TB
- en: '| SUN RGB-D | [[219](#bib.bib219)] |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
- en: '| Tuscan Arizona | [[215](#bib.bib215)] |'
  id: totrans-1121
  prefs: []
  type: TYPE_TB
- en: '| VIST | [[8](#bib.bib8)] |'
  id: totrans-1122
  prefs: []
  type: TYPE_TB
- en: 'TABLE XV: Datasets used in other prediction applications.'
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
