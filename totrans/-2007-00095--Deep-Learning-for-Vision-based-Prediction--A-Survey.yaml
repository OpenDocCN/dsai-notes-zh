- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:00:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2007.00095] Deep Learning for Vision-based Prediction: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2007.00095](https://ar5iv.labs.arxiv.org/html/2007.00095)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Vision-based Prediction: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amir Rasouli A. Rasouli is with Noah’s Ark Laboratory at Huawei Technologies
    Canada, 19 Allstate Pkwy, Markham, ON L3R 5A4
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: amir.rasouli@huawei.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Vision-based prediction algorithms have a wide range of applications including
    autonomous driving, surveillance, human-robot interaction, weather prediction.
    The objective of this paper is to provide an overview of the field in the past
    five years with a particular focus on deep learning approaches. For this purpose,
    we categorize these algorithms into video prediction, action prediction, trajectory
    prediction, body motion prediction, and other prediction applications. For each
    category, we highlight the common architectures, training methods and types of
    data used. In addition, we discuss the common evaluation metrics and datasets
    used for vision-based prediction tasks. A database of all the information presented
    in this survey, cross-referenced according to papers, datasets and metrics, can
    be found online at [https://github.com/aras62/vision-based-prediction](https://github.com/aras62/vision-based-prediction).
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Video Prediction, Action Prediction , Trajectory Prediction, Motion Prediction,
    Survey.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ability to predict the changes in the environment and the behavior of objects
    is fundamental in many applications such as surveillance, autonomous driving,
    scene understanding, etc. Prediction is a widely studied field in various artificial
    intelligence communities. A subset of these algorithms relies primarily on visual
    appearances of the objects and the scene to reason about the future. Other approaches
    use different forms of sensors such as wearable or environmental sensors to learn
    about the past states of the environment or objects.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of this report is on vision-based prediction algorithms, which primarily
    use visual information to observe the changes in the environment and predict the
    future. In this context, prediction can be in the form of generating future scenes
    or reasoning about specific aspects of the objects, e.g. their trajectories, poses,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: For this review, we divide the prediction algorithms into five groups, namely
    video prediction, action prediction, trajectory prediction, motion (pose) prediction,
    and others which involve various applications of prediction such as trend prediction,
    visual weather prediction, map prediction, semantic prediction, etc. In addition,
    we briefly discuss algorithms that use a form of prediction as an intermediate
    step to perform tasks such as object detection, action detection, and recognition,
    etc. Moreover, for each group of prediction algorithms, we will talk about the
    common datasets and metrics and discuss of their characteristics. It should be
    noted that due to the broad scope of this review and the large body of work on
    the vision-based prediction, this review will only focus on works that had been
    published since five years ago in major computer vision, robotics and machine
    learning venues. In addition, as the title of the paper suggests, the main focus
    of the discussion will be on deep learning methods given their popularity in recent
    years.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Vision-based Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before reviewing the works on vision-based prediction algorithms, there are
    a number of points that should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on our review, we have identified four major vision-based applications
    namely, video prediction, action prediction, trajectory prediction, and motion
    prediction. We discuss each of the studies in each category in a dedicated section.
    Some of the prediction works, such as visual weather prediction, semantic prediction,
    contests outcome prediction, that do not fit to any of the four major categories
    are presented in other application section.
  prefs: []
  type: TYPE_NORMAL
- en: Some works address multiple prediction tasks, e.g. predicting trajectories and
    actions simultaneously, and therefore might fall in more than one category. It
    should be noted that we only include an algorithm in each category if the corresponding
    task is directly evaluated. For instance, if an algorithm performs video prediction
    for future action classification, and only evaluates the accuracy of predicted
    actions, it will only appear in the action prediction category. Furthermore, some
    works that are reviewed in this paper propose multiple architectures, e.g. recurrent
    and feedforward, for solving the same problem. In architecture-based categorizations,
    these algorithms may appear more than once.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.2.1 Algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This work focuses on vision-based algorithms, which use some form of visual
    input such as RGB camera images or active sensors such as LIDAR. It should be
    noted that many algorithms, especially trajectory prediction ones, only use ground
    truth data such as object trajectories without actual visual processing, e.g.
    for detection of objects. However, as long as these algorithms are evaluated on
    vision datasets, they are included in this paper. Note that a completer list of
    papers with published code can be found in Appendix [A](#A1 "Appendix A Papers
    with code ‣ Deep Learning for Vision-based Prediction: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Architectures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned earlier, we focus on algorithms that have a deep learning component,
    either in the stage of visual representation generation (e.g. using convolutional
    features) or reasoning (e.g. using an MultiLayer Preceptron (MLP) for classification).
    We will, however, acknowledge the classical methods by mentioning some of the
    main techniques and including them in the datasets and metrics sections of this
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: We classify the algorithms in terms of training techniques and architectures.
    In practice, this is very challenging as the majority of algorithms use a combination
    of different approaches. For example, recurrent networks often rely on a form
    of Convolutional Neural Networks (CNNs) to generate feature representations for
    scenes, poses of agents, etc. To better distinguish between different classes
    of algorithms, we only focus on the core component of each algorithm, i.e. the
    parts that are used for reasoning about the future. Hence, for example, if an
    algorithm uses a CNN model for pre-processing input data and a recurrent network
    for temporal reasoning, we consider this algorithm as recurrent. On the other
    hand, if the features are used with a fully connected network, we categorize this
    algorithm as feedforward or one-shot method. A few algorithms propose the use
    of both architectures for reasoning. We address those methods as hybrid. In addition,
    it should be noted that many works propose alternative approaches using each architecture.
    Therefore, we categorize them in more than one group.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Data type
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As one would expect, vision-based algorithms primarily rely on visual information.
    However, many algorithms use pre-trained off-the-shelf algorithms to transform
    the input to some explicit feature spaces, e.g. poses, trajectories, action labels
    and perform reasoning in those feature spaces. If pre-processing is not part of
    the main algorithm, we consider those secondary features as different types of
    data inputs to the algorithms. If some basic processing, e.g. generating convolutional
    features for a scene is used, we consider the data type of the original input,
    e.g. RGB images.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Video prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Video or future scene prediction can be regarded as the most generic form of
    prediction. The objective of video prediction algorithms is to generate future
    scenes, often in the form of RGB images [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)] and/or optical flow maps [[5](#bib.bib5), [6](#bib.bib6), [1](#bib.bib1),
    [7](#bib.bib7)]. The generated images in turn can be used for various tasks such
    as action prediction [[8](#bib.bib8)], event prediction [[9](#bib.bib9)], flow
    estimation [[6](#bib.bib6)], semantic segmentation [[10](#bib.bib10)], etc.
  prefs: []
  type: TYPE_NORMAL
- en: Video prediction applications rely on generative models whose task is to predict
    future scene(s) based on a short observation of input sequences (or in some cases
    only a single image [[11](#bib.bib11)]). Although many approaches use feedforard
    architectures [[2](#bib.bib2), [1](#bib.bib1), [4](#bib.bib4), [5](#bib.bib5),
    [9](#bib.bib9), [12](#bib.bib12), [11](#bib.bib11), [7](#bib.bib7), [13](#bib.bib13),
    [14](#bib.bib14), [8](#bib.bib8), [15](#bib.bib15)], the majority of algorithms
    take advantage of Recurrent Neural Networks (RNNs) such as Gated Recurrent Units
    (GRUs) [[16](#bib.bib16)], Long-Short Term Memory (LSTM) networks [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)], its variation Convolutional
    LSTMs (ConvLSTMs)[[3](#bib.bib3), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [6](#bib.bib6), [29](#bib.bib29)]
    or a combination of these [[37](#bib.bib37), [27](#bib.bib27)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative Adversarial Networks (GANs) are particularly popular in the video
    prediction community. [[2](#bib.bib2), [18](#bib.bib18), [31](#bib.bib31), [19](#bib.bib19),
    [21](#bib.bib21), [22](#bib.bib22), [24](#bib.bib24), [13](#bib.bib13), [14](#bib.bib14),
    [36](#bib.bib36), [6](#bib.bib6), [26](#bib.bib26), [15](#bib.bib15)]. In these
    adversarial training frameworks, there are two compoents: A generative network
    that produces future representations and a discriminator whose objective is to
    distinguish between the predicted representations (e.g. optical flow [[6](#bib.bib6)],
    frames [[18](#bib.bib18)], motion [[31](#bib.bib31)]) or their temporal consistency
    [[2](#bib.bib2), [19](#bib.bib19)] and the actual ground truth data by producing
    a binary classification score that indicates whether the prediction is real or
    fake. While many algorithms use discriminators to judge how realistic the final
    generated images [[18](#bib.bib18), [21](#bib.bib21), [22](#bib.bib22), [24](#bib.bib24),
    [13](#bib.bib13), [36](#bib.bib36)] are or intermediate features (e.g. poses [[26](#bib.bib26)]),
    others use multiple discriminators at different stages of processing. For example,
    the authors of [[2](#bib.bib2), [19](#bib.bib19)] use two discriminators, one
    is responsible for judging the temporal consistency of the generated frames (i.e.
    whether the order of generated frames is real) and the other assesses whether
    the generated frames are real or not. Lee et al. [[31](#bib.bib31)] use three
    discriminators to assess the quality of generated frames and the intermediate
    motion and content features. Using a two-stream approach, the method in [[6](#bib.bib6)]
    produces both the next frame and optical flow and each stream is trained with
    a separate discriminator. The prediction network of [[15](#bib.bib15)] uses a
    discriminator for intermediate features generated from input scenes and another
    discriminator for the final results.'
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoders (VAEs) [[38](#bib.bib38)] or Conditional VAEs (CVAEs)
    [[39](#bib.bib39)] are also used in some approaches [[3](#bib.bib3), [18](#bib.bib18),
    [11](#bib.bib11), [23](#bib.bib23), [26](#bib.bib26)]. VAEs model uncertainty
    in generated future frames by defining a posterior distribution over some latent
    variable space [[3](#bib.bib3), [23](#bib.bib23), [26](#bib.bib26)]. In CVAEs,
    the posterior is conditioned on an additional parameter such as the observed action
    in the scenes [[18](#bib.bib18)] or initial observation [[11](#bib.bib11)]. Using
    VAEs, at inference time, a random sample is drawn from the posterior to generate
    the future frame.
  prefs: []
  type: TYPE_NORMAL
- en: Many video prediction algorithms operate solely on input images and propose
    various architectural innovations for encoding the content and generating future
    images [[2](#bib.bib2), [3](#bib.bib3), [32](#bib.bib32), [33](#bib.bib33), [16](#bib.bib16),
    [34](#bib.bib34), [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [8](#bib.bib8),
    [15](#bib.bib15), [27](#bib.bib27)]. For example, the method in [[2](#bib.bib2)]
    performs a two-way prediction, forward and backward. Each prediction relies on
    two discriminators for assessing the quality of the generated images and temporal
    consistency. The model presented in [[3](#bib.bib3)] trains a context network
    by inputting an image sequence into a ConvLSTM whose output is used to initialize
    convolutional networks responsible for generating the next frames. Xu et al. [[32](#bib.bib32)],
    in addition to raw pixel values, encode the output of a high pass filter applied
    to the image as a means of maintaining the structural integrity of the objects
    in the scene. In [[15](#bib.bib15)], the authors use a two-step approach in which
    they first perform a coarse frame prediction followed by a fine frame prediction.
    In [[13](#bib.bib13)], the algorithm learns in two stages. A discriminator is
    applied after features are generated from the scenes and another one after the
    final generated frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optical flow prediction has been widely used as an intermediate step in video
    prediction algorithms [[1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [20](#bib.bib20),
    [5](#bib.bib5), [11](#bib.bib11), [25](#bib.bib25), [36](#bib.bib36), [6](#bib.bib6)].
    For example, to deal with occlusion in dynamic scenes, Gao et al. [[1](#bib.bib1)]
    disentangle flow and pixel-level predictions into two steps: the algorithm first
    predicts the flow of the scene, and then uses it, in conjunction with the input
    frames, to predict the future. Similar multi-step approaches have also been used
    in [[6](#bib.bib6), [36](#bib.bib36), [11](#bib.bib11), [31](#bib.bib31)]. In
    [[31](#bib.bib31)], the authors use two separate branches: one branch receives
    two consecutive frames $(t,t+1)$ and produces context information. The second
    branch produces motion information by receiving two frames that are $k$ steps
    apart (i.e. $t+1$, $t+k$). The outputs of these two branches are fused and fed
    into the final scene generator. The method in [[6](#bib.bib6)] simultaneously
    produces the next future frame and the corresponding optical flow map. In this
    architecture, two additional networks are used: A flow estimator which uses the
    output of the frame generator and the last observation to estimate a flow map
    and a warping layer which performs differential 2D spatial transformation to warp
    the last observed image into the future predicted frame according to the predicted
    flow map.'
  prefs: []
  type: TYPE_NORMAL
- en: Some algorithms rely on various intermediate steps for video prediction[[17](#bib.bib17),
    [18](#bib.bib18), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [7](#bib.bib7), [14](#bib.bib14), [35](#bib.bib35)]. For instance, the method
    in [[17](#bib.bib17)], reasons about the locations and features of individual
    entities (e.g. cubes) for final scene predictions. Kim et al. [[18](#bib.bib18)]
    first identify keypoints, which may correspond to important structures such as
    joints, and then predict their motion. For videos involving humans, in [[21](#bib.bib21),
    [22](#bib.bib22), [7](#bib.bib7)] the authors identify and reason about the changes
    in poses, and use this information to generate future frames. In [[14](#bib.bib14),
    [35](#bib.bib35)], in addition to raw input images, the differences between consecutive
    frames used in the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction networks can also be provided with additional information to guide
    future frame generation. In [[19](#bib.bib19), [12](#bib.bib12)] an optical flow
    network and in [[26](#bib.bib26), [28](#bib.bib28)] a pose estimation network
    are used in addition to RGB images. Using a CVAE architecture, in [[18](#bib.bib18)]
    the authors use the action lables as conditional input for frame generation. In
    the context of active tasks, e.g. object manipulation with robotic arms, in which
    the consequences of actions influence the future scene configuration, it is common
    to condition the future scene generation on the current or future intended actions
    [[37](#bib.bib37), [29](#bib.bib29), [30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Video prediction algorithms are based on generative models that produce future
    images given a short observation, or in extreme cases a single view of the scene.
    Both recurrent and feedforward models are widely used in the field, with recurrent
    ones being slightly more favorable. The architectural designs and training strategies
    such as the VAEs or GANs are very common. However, it is hard to establish which
    one of these approaches is superior given that the majority of the video prediction
    algorithms are application-agnostic, meaning that they are evaluated on a wide
    range of video datasets with very different characteristics such as traffic scenes,
    activities, games, object manipulations, etc. (more on this in Section [11](#S11
    "11 Datasets ‣ Deep Learning for Vision-based Prediction: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the great progress in the field, video prediction algorithms are still
    facing some major challenges. One of them is the ability to hallucinate, which
    is to generate visual representations for parts of the scenes that were not visible
    during observation phase, e.g. due to occlusions. This is particularly an issue
    for more complex images such as traffic scenes, movies, etc. The complexity of
    the scenes also determines how fast the generated images would degrade. Although
    these algorithms show promising results in simple synthetic videos or action sequences,
    they still struggle in real practical applications. In addition, many of these
    algorithms cannot reason about the expected presence or absence of objects in
    the future. For example, if a moving object is present in the observations and
    is about to exit the field of view in near future, the algorithms account for
    it in the future scenes as long as parts of it are visible in the observation
    stage. This can be an issue for safety-critical applications such as autonomous
    driving in which the presence or absence of traffic elements and the interactions
    between them are essential for action planning.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Action prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Action prediction algorithms can be categorized into two groups: Next action
    or event prediction (or action anticipation) and early action prediction. In the
    former category, the algorithms use the observation of current activities or scene
    configurations and predict what will happen next. Early action prediction algorithms,
    on the other hand, observe parts of the current action in progress and predict
    what this action is. The classical learning approaches such as Conditional Random
    Fields (CRFs) [[40](#bib.bib40)], Support Vector Machines (SVMs) with hand-crafted
    features [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47)], Markov models [[48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)],
    Bayesian networks [[54](#bib.bib54), [55](#bib.bib55)] and other statistical methods
    [[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62)] have been widely used in recent years. However,
    as mentioned earlier, we will only focus on deep learning approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Action anticipation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Action prediction algorithms are used in a wide range of applications including
    cooking activities [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)], traffic
    understanding [[71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [73](#bib.bib73),
    [74](#bib.bib74), [75](#bib.bib75), [74](#bib.bib74), [76](#bib.bib76), [77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)], accident prediction [[81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)], sports [[85](#bib.bib85),
    [86](#bib.bib86)] and other forms of activities [[87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91), [8](#bib.bib8), [92](#bib.bib92),
    [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]. Although the majority of
    these algorithms use sequences in which the objects and agents are fully observable,
    a number of methods rely on egocentric scenes [[63](#bib.bib63), [64](#bib.bib64),
    [68](#bib.bib68), [89](#bib.bib89), [85](#bib.bib85), [95](#bib.bib95)] which
    are recorded from the point of view of the acting agents and only parts of their
    bodies (e.g. hands) are observable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Action prediction methods predominantly use a variation of RNN-based architectures
    including LSTMs [[87](#bib.bib87), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [74](#bib.bib74), [67](#bib.bib67), [68](#bib.bib68), [82](#bib.bib82), [89](#bib.bib89),
    [75](#bib.bib75), [74](#bib.bib74), [90](#bib.bib90), [91](#bib.bib91), [85](#bib.bib85),
    [84](#bib.bib84), [86](#bib.bib86), [70](#bib.bib70), [92](#bib.bib92), [96](#bib.bib96),
    [79](#bib.bib79), [80](#bib.bib80)], GRUs [[88](#bib.bib88), [71](#bib.bib71),
    [72](#bib.bib72), [69](#bib.bib69)], ConvLSTMs [[76](#bib.bib76)], and Quasi-RNNs
    (QRNNs) [[83](#bib.bib83)]. For instance, in [[88](#bib.bib88), [67](#bib.bib67)]
    the authors use a graph-based RNN architecture in which the nodes represent actions
    and the edges of the graph represent the transitions between the actions. The
    method in [[69](#bib.bib69)] employs a two-step approach: using a recognition
    algorithm, the observed actions and their durations are recognized. These form
    a one-hot encoding vector which is fed into GRUs for the prediction of the future
    activities, their corresponding start time and length. In the context of vehicle
    behavior prediction, Ding et al. [[72](#bib.bib72)] uses a two-stream GRU-based
    architecture to encode the trajectory of two vehicles and a shared activation
    unit to encode the vehicles mutual interactions. Scheel et al. [[97](#bib.bib97)]
    encode the relationship between the ego-vehicle and surrounding vehicles in terms
    of their mutual distances. The vectorized encoding is then fed into a bi-directional
    LSTM. At each time step, the output of the LSTM is classified, using a softmax
    activation, into a binary value indicating whether it is safe for the ego-vehicle
    to change lane. In [[83](#bib.bib83)] the authors use a QRNN network to capture
    the relationships between road users in order to predict the likelihood of a traffic
    accident. To train the model, the authors propose an adaptive loss function that
    assigns penalty weights depending on how early the model can predict accidents.'
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative to recurrent architectures, some algorithms use feedforward
    architectures using both 3D [[63](#bib.bib63), [9](#bib.bib9), [73](#bib.bib73)]
    and 2D [[81](#bib.bib81), [69](#bib.bib69), [77](#bib.bib77), [86](#bib.bib86),
    [78](#bib.bib78), [8](#bib.bib8), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]
    convolutional networks. For example, in the context of pedestrian crossing prediction,
    in [[9](#bib.bib9)] the authors use a generative 3D CNN model that produces future
    scenes and is followed by a classifier. The method of [[73](#bib.bib73)] detects
    and tracks pedestrians in the scenes, and then feeds the visual representations
    of the tracks, in the form of an image sequence, into a 3D CNN architecture, which
    directly classifies how likely the pedestrian will cross the road. To predict
    the time of traffic accidents, the method in [[81](#bib.bib81)] processes each
    input image using a 2D CNN model and then combines the representations followed
    by a fully-conntected (fc) layer for prediction. Farha et al. [[69](#bib.bib69)]
    create a 2D matrix by stacking one-hot encodings of actions for each segment of
    observation and use a 2D convolutional net to generate future actions encodings.
    Casas et al. [[77](#bib.bib77)] use a two-stream 2D CNN, each processing the stacked
    voxelized LIDAR scans and the scene map. The feature maps obtained from each stream
    are fused and fed into a backbone network followed by three headers responsible
    for the detection of the vehicles and predicting their intentions and trajectories.
    For sports forecasting, Felsen et al. [[86](#bib.bib86)] concatenate 5 image observations
    channel-wise and feed the resulting output into a 2D CNN network comprised of
    4 convolutional layers and an fc layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although some algorithms rely on a single source of information, e.g. a set
    of pre-processed features from RGB images [[88](#bib.bib88), [82](#bib.bib82),
    [83](#bib.bib83), [91](#bib.bib91), [84](#bib.bib84), [86](#bib.bib86), [70](#bib.bib70),
    [92](#bib.bib92), [96](#bib.bib96)] or trajectories [[72](#bib.bib72)], many algorithms
    use a multimodal approach by using various sources of information such as optical
    flow maps [[64](#bib.bib64), [67](#bib.bib67), [68](#bib.bib68), [75](#bib.bib75),
    [95](#bib.bib95)], poses [[87](#bib.bib87), [71](#bib.bib71), [67](#bib.bib67),
    [90](#bib.bib90), [80](#bib.bib80)], scene attributes (e.g. road structure, semantics)
    [[87](#bib.bib87), [74](#bib.bib74), [89](#bib.bib89), [77](#bib.bib77)], text
    [[65](#bib.bib65)], action labels [[66](#bib.bib66)], length of actions [[69](#bib.bib69)],
    speed (e.g. ego-vehicle or surrounding agents) [[71](#bib.bib71), [74](#bib.bib74),
    [75](#bib.bib75), [97](#bib.bib97), [85](#bib.bib85)], gaze [[89](#bib.bib89),
    [90](#bib.bib90)], current activities [[78](#bib.bib78)] and the time [[63](#bib.bib63)]
    of the actions. For example, the method in [[65](#bib.bib65)] uses a multi-stream
    LSTM in which two LSTMs encode visual features and cooking recipes and an LSTM
    decodes them for final predictions. To capture the relationships within and between
    sequences, Gammulle et al. [[66](#bib.bib66)] propose a two-stream LSTM network
    with external neural memory units. Each stream is responsible for encoding visual
    features and action labels. In [[71](#bib.bib71)], a multi-layer GRU structure
    is used in which features with different modalities enter the network at different
    levels and are fused with the previous level encodings. The fusion process is
    taking place according to the complexity of the data modality, e.g. more complex
    features such as encodings of pedestrian appearances enter the network at the
    bottom layer, whereas location and speed features enter at the second-last and
    last layers respectively. Farha et al. [[69](#bib.bib69)] use a two-layer stacked
    GRU architecture which receives as input a feature tuple of the length of the
    activity and its corresponding one-hot vector encoding. In [[75](#bib.bib75)],
    the method uses a two-stage architecture: First information regarding the appearance
    of the scene, optical flow (pre-processed using a CNN) and vehicle dynamics are
    fed into individual LSTM units. Then, the output of these units is combined and
    passed through an fc layer to create a representation of the context. This representation
    is used by another LSTM network to predict future traffic actions. In the context
    of human-robot interaction, the authors of [[90](#bib.bib90)] combine the information
    regarding the gaze and pose of the humans using an encoder-decoder LSTM architecture
    to predict their next actions. Jain et al. [[80](#bib.bib80)] use a fusion network
    to combine head pose information of the driver, outside scene features, GPS information,
    and vehicle dynamics to predict the driver’s next action.'
  prefs: []
  type: TYPE_NORMAL
- en: Before concluding this section, it is important to discuss the use of attention
    modules which have gained popularity in recent years [[63](#bib.bib63), [87](#bib.bib87),
    [88](#bib.bib88), [64](#bib.bib64), [74](#bib.bib74), [68](#bib.bib68), [82](#bib.bib82),
    [89](#bib.bib89), [84](#bib.bib84), [79](#bib.bib79)]. As the name implies, the
    objective of attention modules is to determine what has to be given more importance
    at a given time. These modules can come in different froms and can be applied
    to different dimensions of data and at various processing. Some of these modules
    are temporal attention [[63](#bib.bib63), [82](#bib.bib82), [89](#bib.bib89)]
    for identifying keyframes, modality attention [[64](#bib.bib64), [68](#bib.bib68)]
    to prioritize between different modalities of data input, spatial attention [[84](#bib.bib84),
    [79](#bib.bib79)] for highlighting the important parts of the scenes, and graph
    attention [[88](#bib.bib88)] for weighting nodes of the graph. In some cases,
    a combination of different attention mechanisms is used [[87](#bib.bib87), [74](#bib.bib74)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the field of action anticipation, RNN architectures are strongly preferred.
    Compared to feedforward algorithms, recurrent methods have the flexibility of
    dealing with variable observation lengths and multi-modal data, in particular,
    when they are significantly different, e.g. trajectories and RGB images. However,
    basic recurrent architectures such as LSTMs and GRUs rely on some forms of pre-processing,
    especially when dealing with high dimensional data such as RGB images, which requires
    the use of various convolutional networks, a process that can be computationally
    costly. Feedforward models, on the other hand, can perform prediction in one shot,
    meaning that they can simultaneously perform temporal reasoning and spatial feature
    generation in a single framework, and as a result, potentially have a shorter
    processing time.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the approaches mentioned earlier are generative in nature. They generate
    representations in some feature space and then using these representations predict
    what will happen next. Some algorithms go one step further and generate the actual
    future images and use them for prediction. Although such an approach seems effective
    for single actor events, e.g. cooking scenes, human-robot interaction, it is not
    a feasible approach for multi-agent predictions such as reasoning about behaviors
    of pedestrians or cars in traffic scenes.
  prefs: []
  type: TYPE_NORMAL
- en: The majority of the methods reviewed in this section use multi-modal data input.
    This seems to be a very effective approach, especially in high dimensional problems
    such as predicting road user behavior where the state (e.g. location and speed)
    of the road user, observer, and other agents, as well as scene structure, lighting
    conditions, and many other factors, play a role in predicting the future behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-tasking, e.g. predicting actions and trajectories, are an effective way
    to predict future actions. For instance, trajectories can imply the possibility
    of certain actions, e.g. moving towards the road implies the possibility that
    the person might cross the street. As a result, the simultaneous learning of different
    tasks can be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least is the use of attention modules. These modules are deemed
    to be very effective , in particular for tasks with high complexity in terms of
    the modality of input data, the scene structure and temporal relations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Early action prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to action anticipation methods, early action prediction algorithms widely
    use recurrent network architectures [[88](#bib.bib88), [98](#bib.bib98), [99](#bib.bib99),
    [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102), [103](#bib.bib103),
    [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)]. Although many of
    these algorithms use similar aproaches to action anticipation algorithms, some
    propose new approaches. For example, in [[98](#bib.bib98)] the authors use a teacher-student
    learning scheme where the teacher learns to recognize full sequences using a bi-directional
    LSTM and the student relies on partial videos using an LSTM network. They perform
    knowledge distillation by linking feature representations of both networks. Using
    GAN frameworks, the methods in [[99](#bib.bib99), [102](#bib.bib102)] predict
    future feature representations of videos in order to predict actions. Zhao et
    al. [[100](#bib.bib100)] implement a Kalman filter using an LSTM architecture.
    In this method, actions are predcted after observing each frame and corrections
    are made if the next observation provide additional information. The method of
    [[105](#bib.bib105)] uses a two-step LSTM architecture which first generates an
    encoding of the context using context-aware convolutional features and then combines
    these encodings with action-aware convolutional features to predict the action.
    The authors of this method propose a new loss function that penalizes false negatives
    with the same strength at any point in time and false positives with a strength
    that increases linearly over time, to reach the same weight as that on false negatives.
    In [[106](#bib.bib106)] the authors perform coarse-to-fine-grained predictions
    depending on the amount of evidence available for the type of action.
  prefs: []
  type: TYPE_NORMAL
- en: Many early action prediction methods adopt feedforward architectures [[107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109), [104](#bib.bib104), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113)]. The authors of [[107](#bib.bib107)]
    predict actions from a single image by transforming it into a new representation
    called Ranked Saliency Map and Predicted Optical Flow. This representation is
    passed through a 2D convent and fc layers for final prediction. In [[108](#bib.bib108),
    [104](#bib.bib104)], the authors use temporal CNN (TCNNs) architectures, which
    are a series of 1D dilated convolutional layers designed to capture the temporal
    dependencies of feature representations, e.g. in the form of a vector representation
    of poses [[108](#bib.bib108)] or word-embeddings [[104](#bib.bib104)] describing
    the video frames. Chen et al. [[109](#bib.bib109)] use features of body parts
    generated by a CNN model and train an attention module whose objective is to activate
    only features that contribute to the prediction of the action. In [[112](#bib.bib112)],
    the authors use an action detection framework, which incrementally predicts the
    locations of the actors and the action classes based on the current detections.
  prefs: []
  type: TYPE_NORMAL
- en: The majority of the early action prediction algorithms pre-process the entire
    observed scenes using, e.g. different forms of convolutional neural networks [[98](#bib.bib98),
    [100](#bib.bib100), [107](#bib.bib107), [102](#bib.bib102), [104](#bib.bib104),
    [111](#bib.bib111), [105](#bib.bib105), [113](#bib.bib113)] or other forms of
    features [[106](#bib.bib106)]. Some algorithms complement these features using
    optical flow maps [[99](#bib.bib99), [104](#bib.bib104), [112](#bib.bib112)].
    Another group of action prediction methods focuses on specific parts of the observations.
    For example, in [[108](#bib.bib108), [101](#bib.bib101), [103](#bib.bib103), [110](#bib.bib110)]
    the authors use the changes in the poses of actors to predict their actions. The
    method in [[109](#bib.bib109)] uses body parts extracted by cropping a local patch
    around identified joints of the actors. The authors of [[113](#bib.bib113)] only
    use the visual appearances of actors extracted from detected bounding boxes, and
    the relationship between them.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Early action detection methods have many commonalities with action anticipation
    algorithms in terms of architectural design. However, there are some exceptions
    that are more applicable in this domain. These exceptions are teacher-student
    training schemes for knowledge distillation, identifying discriminative features,
    and recursive prediction/correction mechanisms. In addition, early action prediction
    algorithm, with a few exceptions, rely on single modal data for prediction and
    rarely use refinement frameworks such as attention modules. Adopting these techniques
    and operating on multi-modal feature spaces can further improve the performance
    of early action prediction algorithms. Unlike the action anticipation methods,
    there is no strong preference for recurrent or feedforward approaches. Some approaches
    take advantage of architectures such as temporal CNNs which are popular in the
    language processing domain and show their effectiveness for early action prediction
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Trajectory prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name implies, trajectory prediction algorithms predict the future trajectories
    of objects, i.e. the future positions of the objects over time. These approaches
    are particularly popular for applications such as intelligent driving and surveillance.
    Predicted trajectories can be used directly, e.g. in route planning for autonomous
    vehicles, or used for predicting future events, anomalies, or actions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we follow the same routine as before and focus on algorithms
    that have a deep learning component while acknowledging many recent works that
    have used classical approaches including Gaussian mixture models [[114](#bib.bib114),
    [115](#bib.bib115)] and processes [[116](#bib.bib116), [117](#bib.bib117)], Markov
    decision processes (MDPs) [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126)], Markov chains [[127](#bib.bib127), [128](#bib.bib128)]
    and other techniques [[129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)].
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory prediction applications like many other sequence prediction tasks
    heavily rely on recurrent architectures such as LSTMs [[137](#bib.bib137), [87](#bib.bib87),
    [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145),
    [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153),
    [154](#bib.bib154), [155](#bib.bib155), [101](#bib.bib101), [156](#bib.bib156),
    [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)],
    and GRUs [[161](#bib.bib161), [162](#bib.bib162), [163](#bib.bib163), [164](#bib.bib164),
    [165](#bib.bib165), [166](#bib.bib166)]. These methods often use an encoder-decoder
    architecture in which a network, e.g. an LSTM encodes single- or multi-modal observations
    of the scenes for some time, and another network generates future trajectories
    given the encoding of the observations. Depending on the complexity of input data,
    these algorithms may rely on some form of pre-processing for generating features
    or embedding mechanisms to minimize the dimensionality of the data.
  prefs: []
  type: TYPE_NORMAL
- en: The feedforward algorithms [[161](#bib.bib161), [167](#bib.bib167), [168](#bib.bib168),
    [169](#bib.bib169), [170](#bib.bib170), [171](#bib.bib171), [172](#bib.bib172),
    [173](#bib.bib173), [77](#bib.bib77), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176)] often use whole views of the scenes (i.e. the environment
    and moving objects) and encode them using convolutional layers followed by a regression
    layer to predict trajectories. A few algorithms use hybrid approaches in which
    both convolutional and recurrent reasoning are also used [[177](#bib.bib177),
    [178](#bib.bib178)].
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the prediction task, algorithms may rely on single- or multi-modal
    observations. For example, in the context of visual surveillance where a fixed
    camera provide a top-down or bird-eye viewing angle, many algorithms only use
    past trajectories of the agents in either actual 2D frame coordinates or their
    velocities calculated by the changes from each time step to another [[178](#bib.bib178),
    [139](#bib.bib139), [143](#bib.bib143), [145](#bib.bib145), [149](#bib.bib149),
    [151](#bib.bib151), [179](#bib.bib179), [153](#bib.bib153), [155](#bib.bib155),
    [164](#bib.bib164), [156](#bib.bib156), [180](#bib.bib180), [152](#bib.bib152),
    [159](#bib.bib159), [160](#bib.bib160)]. In addition to observations of individual
    trajectories of agents, these algorithms focus on modeling the interaction between
    the agents and how they impact each other. For example, Zhang et al. [[139](#bib.bib139)]
    use a state refinement module that aligns all pedestrians in the scene with a
    message passing mechanism that receives as input the current locations of the
    subjects and their encodings from an LSTM unit. In [[143](#bib.bib143)] a graph-based
    approach is used where pedestrians are considered as nodes and the interactions
    between them as edges of the graph. By aggregating information from neighboring
    nodes, the network learns to assign a different level of importance to each node
    for a given subject. The authors of [[153](#bib.bib153), [160](#bib.bib160)] perform
    a pooling operation on the generated representations by sharing the state of individual
    LSTMs that have spatial proximity.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in some works, other sources of information are used in surveilling
    objects [[87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [142](#bib.bib142),
    [146](#bib.bib146), [163](#bib.bib163), [154](#bib.bib154), [101](#bib.bib101),
    [156](#bib.bib156), [158](#bib.bib158), [165](#bib.bib165), [175](#bib.bib175),
    [176](#bib.bib176)]. For example, in addition to encoding the interactions with
    the environment, Liang et al. [[87](#bib.bib87)] use the semantic information
    of the scene as well as changes in the poses of the pedestrians. In [[138](#bib.bib138),
    [140](#bib.bib140), [142](#bib.bib142), [146](#bib.bib146), [163](#bib.bib163),
    [158](#bib.bib158), [165](#bib.bib165), [176](#bib.bib176)] the visual representations
    of the layout of the environment and the appearances of the subjects are included.
    The authors of [[156](#bib.bib156)] use an occupancy map which highlights the
    potential traversable locations for the subjects. The method in [[154](#bib.bib154)]
    takes into account pedestrians’ head orientations to estimate their fields of
    view in order to predict which subjects would potentially interact with one another.
    To predict interactions between humans, in [[101](#bib.bib101)] the authors use
    both poses and trajectories of the agents. Ma et al. [[175](#bib.bib175)] go one
    step further and take into account the pedestrians’ characteristics (e.g. age,
    gender) within a game-theoretic perspective to determine how the trajectory of
    one pedestrians impact each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of traffic understanding, predicting trajectories can be more
    challenging due to the fact that there is camera ego-motion involved (e.g. the
    prediction is from the perspective of a moving vehicle), there are interactions
    between different types of objects (e.g. vehicles and pedestrians), and there
    are certain constraints involved such as traffic rules, signals, etc. To achieve
    robustness, many methods in this domain take advantage of multi-modal data for
    trajectory prediction [[177](#bib.bib177), [137](#bib.bib137), [161](#bib.bib161),
    [141](#bib.bib141), [144](#bib.bib144), [162](#bib.bib162), [169](#bib.bib169),
    [147](#bib.bib147), [170](#bib.bib170), [181](#bib.bib181), [182](#bib.bib182),
    [163](#bib.bib163), [150](#bib.bib150), [168](#bib.bib168), [172](#bib.bib172),
    [183](#bib.bib183), [166](#bib.bib166), [173](#bib.bib173), [77](#bib.bib77),
    [165](#bib.bib165)]. In addition to using past trajectories, all these algorithms
    account for the road structure (whether it is from the perspective of the ego-vehicle
    or a top-down view) often in the form of raw visual inputs or, in some cases,
    as an occupancy map [[141](#bib.bib141), [173](#bib.bib173)]. The scene layout
    can implicitly capture the structure of the road, the appearances of the objects
    (e.g. shape) and the dynamics (e.g. velocity or locations of subjects). Such implicit
    information can be further augmented by explicit data such as the shapes of the
    objects (in the case of vehicles) [[177](#bib.bib177)], the speed [[144](#bib.bib144),
    [170](#bib.bib170), [183](#bib.bib183)] and steering angle [[170](#bib.bib170),
    [183](#bib.bib183)] of the ego-vehicle, the distance between the objects [[137](#bib.bib137),
    [182](#bib.bib182)], traffic rules [[182](#bib.bib182)] and signals [[77](#bib.bib77)],
    and kinematic constraints [[174](#bib.bib174)]. For example, the method in [[183](#bib.bib183)]
    uses a two-stream LSTM encoder-decoder scheme: first stream encodes the current
    ego-vehicle’s odometry (steering angle and speed) and the last observation of
    the scene and predicts future odometry of the vehicle. The second stream is a
    trajectory stream that jointly encodes location information of pedestrians and
    the ego-vehicle’s odometry and then combines the encoding with the prediction
    of the odometry stream to predict the future trajectories of the pedestrians.
    The method in [[144](#bib.bib144)] further extends this approach and adds an intention
    prediction stream which outputs how likely the observed pedestrian intends to
    cross the street. The intention likelihood is produced using an LSTM network that
    encodes the dynamics of the pedestrian, their appearances and their surroundings.
    Chandra et al. [[177](#bib.bib177)] create embeddings of contextual information
    by taking into account the shape and velocity of the road users and their spatial
    coordinates within a neighboring region. These embeddings are then fed into some
    LSTM networks followed by a number of convolutional layers to capture the dynamics
    of the scenes. In [[141](#bib.bib141)] the authors use separate LSTMs for encoding
    the trajectories of pedestrians and vehicles (as orientated bounding boxes) and
    then combine them into a unified framework by generating an occupancy map of the
    scene centered at each agent, followed by a pooling operation to capture the interactions
    between different subjects. Lee et al. [[165](#bib.bib165)] predict the future
    trajectories of vehicles in two steps: First, an encoder-decoder GRU architecture
    predicts future trajectories by observing the past ones. Then a refinement network
    adjusts the predicted trajectories by taking into account the contextual information
    in the forms of social interactions, dynamics of the agents involved, and the
    road structure.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to action prediction algorithms, attention modules have been widely
    used in trajectory prediction methods [[87](#bib.bib87), [138](#bib.bib138), [139](#bib.bib139),
    [143](#bib.bib143), [144](#bib.bib144), [146](#bib.bib146), [163](#bib.bib163),
    [152](#bib.bib152), [180](#bib.bib180), [164](#bib.bib164)]. For example, in [[87](#bib.bib87),
    [143](#bib.bib143)], the attention module jointly measures spatial and temporal
    interactions. The authors of [[138](#bib.bib138), [139](#bib.bib139), [146](#bib.bib146),
    [180](#bib.bib180)] propose the use of social attention modules which estimate
    the relative importance of interactions between the subject of interest and its
    neighboring subjects. The method in [[144](#bib.bib144)] uses two attention mechanisms,
    a temporal attention module that measures the importance of each timestep of the
    observed trajectories and a series of self-attention modules which are applied
    to encodings of observations prior to predictions. Xue et al. [[152](#bib.bib152)]
    propose an attention mechanism to measure the relative importance between different
    data modalities, namely the locations and velocities of subjects.
  prefs: []
  type: TYPE_NORMAL
- en: One of the major challenges in trajectory prediction is to model uncertainty
    of predictions, especially in scenarios where many possibilities exist (e.g. pedestrians
    at intersections). Some algorithms such as [[87](#bib.bib87), [139](#bib.bib139),
    [144](#bib.bib144), [152](#bib.bib152), [153](#bib.bib153), [160](#bib.bib160)]
    train the models by directly measuring the error between the ground truth trajectories
    and predicted ones, e.g. by using an L2 objective function. At the inference time,
    these algorithms generate deterministic predictions. To measure the uncertainty
    of predictions, these models are trained multiple times with randomly initialized
    parameters. Alternatively, uncertainty can be estimated via probabilistic objective
    functions, e.g. Gaussian log-likelihood, as in [[177](#bib.bib177), [161](#bib.bib161),
    [172](#bib.bib172), [142](#bib.bib142), [154](#bib.bib154), [183](#bib.bib183)]
    Instead of a single point in space, these algorithms predict a distribution that
    captures the uncertainty of the predictions. VAEs are another technique that can
    be used to estimate uncertainty [[161](#bib.bib161), [182](#bib.bib182), [163](#bib.bib163),
    [184](#bib.bib184), [165](#bib.bib165)]. Using these methods, at training time
    a posterior distribution over some latent space $z$ is learned by conditioning,
    for example, over future trajectories. At the inference time, a random sample
    is drawn from the latent space for the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since trajectory prediction algorithms are generative by nature, many approaches
    rely on adversarial training techniques [[178](#bib.bib178), [138](#bib.bib138),
    [140](#bib.bib140), [145](#bib.bib145), [148](#bib.bib148), [149](#bib.bib149),
    [163](#bib.bib163), [153](#bib.bib153), [164](#bib.bib164)] in which at training
    time a discriminator is used to predict whether the generated trajectories are
    real or fake. Kosaraju et al. [[146](#bib.bib146)] extend this approach by using
    two discriminators: A local discriminator which predicts the results on the output
    of the prediction using only past trajectories, and one global discriminator which
    operates on the output of the entire network, i.e. the prediction results based
    on trajectories and scene information.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Trajectory prediction is a widely studied field in the computer vision community.
    Although these works dominantly use recurrent network architectures, many approaches,
    such as those used in the field of traffic scene understanding, use feedforward
    networks. Trajectory prediction algorithms rely on one or more sources of information
    such as the past trajectories of subjects, surrounding visual context, object
    attributes, vehicle sensor readings, etc. One factor that is common in many of
    trajectory prediction algorithms is modeling the interactions between dynamic
    or dynamic and static objects. Relationships are captured explicitly or implicitly
    via encoding the scenes as a whole. Like many other prediction approaches, trajectory
    prediction algorithms benefit from various forms of attention mechanisms to learn
    the importance of spatial, temporal or social interactions between objects. To
    model uncertainty, techniques such as probabilistic objectives and variational
    encodings are used.
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory prediction algorithms are predominantly rely on past trajectory information
    to predict the future. Although past motion observations are very informative,
    in some context, e.g. traffic scenes, they are simply not enough. There is a need
    for a more explicit encoding of contextual information such as road conditions,
    the subject’s attributes, rules and constraints, scene structure, etc. A number
    of approaches successfully have included a subset of these factors, but a more
    comprehensive approach should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Motion prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the term “motion prediction” in many cases is used to refer to future
    trajectory prediction, here we only consider the algorithms that are designed
    to predict changes in human pose. Motion prediction play a fundamental role in
    all prediction approaches as an intermediate step, e.g. to reflect how the future
    visual representations would look like or the types of actions to anticipate.
    Like many other prediction applications, this field is dominated by deep learning
    models, even though some methods still rely on classical techniques [[56](#bib.bib56),
    [48](#bib.bib48), [185](#bib.bib185)].
  prefs: []
  type: TYPE_NORMAL
- en: Similar to other prediction algorithms, motion prediction methods widely use
    recurrent architectures such as LSTMs [[186](#bib.bib186), [187](#bib.bib187),
    [188](#bib.bib188), [101](#bib.bib101), [103](#bib.bib103), [189](#bib.bib189),
    [26](#bib.bib26)] and GRUs [[190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194), [96](#bib.bib96), [195](#bib.bib195)]
    or a combination of both [[196](#bib.bib196)]. For example, in [[190](#bib.bib190)]
    the authors use a two-layer GRU model in which the top layer operates backward
    to learn noise processes and the bottom level is used to predict the poses given
    the past pose observations and the output of the top layer. Chiu et al. [[187](#bib.bib187)]
    propose a hierarchical LSTM architecture in which each layer of the network encodes
    the observed poses at different time-scales. In the context of 3D pose prediction,
    some algorithms rely on a two-stage process where the visual inputs, either as
    a single image [[189](#bib.bib189)] or a sequence of images [[188](#bib.bib188)],
    are fed into a recurrent network to predict 2D poses of the agent. This is followed
    by a refinement procedure that transforms the 2D poses into 3D.
  prefs: []
  type: TYPE_NORMAL
- en: Some approaches adopt feedforward architectures [[197](#bib.bib197), [198](#bib.bib198),
    [199](#bib.bib199), [200](#bib.bib200), [110](#bib.bib110)]. For example, the
    method in [[198](#bib.bib198)] uses two feedforward networks in a two-stage process.
    First, the input poses are fed into an autoencoder which is comprised of fully
    connected layerss (implemented by 1D convolutions with a kernel size of 1) and
    self-attention blocks. The encodings are then used by multi-level 2D convolutional
    blocks for final predictions. Zhang et al. [[199](#bib.bib199)] predict 3D poses
    from RGB videos. In their method, the images are converted to a feature space
    using a convolutional network, and then the features are used to learn a latent
    3D representation of 3D human dynamics. The representation is used by a network
    to predict future 3D poses. To capture movement patterns, a method proposed in
    [[197](#bib.bib197)] converts poses into a trajectory space using discrete cosine
    transformation. The newly formed representations are then used in a Graph-CNN
    framework to learn the dependencies between different joint trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: To train motion prediction models, some authors use adversarial training methods
    in which a discriminator is used to classify whether the predicted poses are real
    or fake [[198](#bib.bib198), [193](#bib.bib193)]. The discrimination procedure
    can also be applied to evaluating the continuity, i.e. correct order of, predictions
    as demonstrated in [[192](#bib.bib192)]. In [[196](#bib.bib196)] the discrimination
    score is used to generate a policy for future action predictions in the context
    of imitation learning.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Motion prediction algorithms primarily focus on the prediction of changes in
    the dynamics (i.e. poses) of observed agents. Such predictions can be fundamental
    to many other applications such as video or trajectory prediction tasks some of
    which were discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: In recent works, recurrent network architectures are strongly preferred. The
    architecture of the choice often depends on the representation of the input data,
    e.g. whether joint coordinates are directly used or are encoded into a high-dimensional
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the development of many successful motion prediction algorithms, the
    majority of these methods rely on a single source of information, for example,
    poses or scenes. Encoding higher-level contextual information, such as scene semantics,
    interactions, etc. can potentially result in more robust predictions, as shown
    in other prediction applications. Attention modules also, except for one instance,
    haven’t been adopted within motion prediction algorithms. Given the success of
    using attention in other prediction applications, motion prediction algorithms
    may benefit from the use of attention mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Other applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of autonomous robotics, some algorithms are designed to predict
    occupancy grid maps (OGMs) that are grayscale representations of the robot’s surroundings
    showing which parts of the environment are traversable. These approaches are often
    object-agnostic and are concerned with generating future OGMs which are used by
    an autonomous agent to perform path planning. In recent years both classical [[201](#bib.bib201),
    [202](#bib.bib202), [203](#bib.bib203), [204](#bib.bib204)] and deep learning
    [[205](#bib.bib205), [206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [209](#bib.bib209)] methods are used. The deep learning approaches, in essence,
    are similar to video prediction methods in which the model receives as input a
    sequence of OGMs and predicts the future ones over some period of time. In this
    context both recurrent [[205](#bib.bib205), [207](#bib.bib207), [209](#bib.bib209)]
    and feedforward [[206](#bib.bib206), [208](#bib.bib208)] methods were common.
    Another group of generative approaches is semantic map prediction algorithms [[10](#bib.bib10),
    [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212)]. These algorithms
    receive as inputs RGB images of the scenes and predict future segmentation maps.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the other vision-based prediction applications include weather [[213](#bib.bib213),
    [214](#bib.bib214)] and Solar irradiance forecasting [[215](#bib.bib215)], steering
    angle prediction [[212](#bib.bib212)], predicting the popularities of tweets based
    on tweeted images used and the users’ histories [[216](#bib.bib216)], forecasting
    fashion trends [[217](#bib.bib217)], storyline prediction [[8](#bib.bib8)], pain
    anticipation [[218](#bib.bib218)], predicting the effect of force after manipulating
    objects [[219](#bib.bib219)], forecasting the winner of Miss Universe given the
    appearances of contestants’ gowns [[220](#bib.bib220)], and predicting election
    results given the facial attributes of candidates [[221](#bib.bib221)]. These
    algorithms rely on a combination of techniques discussed earlier in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Prediction in other vision applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before concluding our discussion on vision-prediction methods, it is worth mentioning
    that prediction techniques are also widely used in other visual processing tasks
    such as video summarization [[222](#bib.bib222)], anomaly detection [[223](#bib.bib223)],
    tracking [[224](#bib.bib224)], active object recognition [[225](#bib.bib225)],
    action detection [[226](#bib.bib226), [227](#bib.bib227)] and recognition [[228](#bib.bib228)].
    For example, tracking algorithms are very closely related to trajectory prediction
    ones and often rely on short term predictions to deal with gaps, e.g. due to occlusions,
    in tracking. For example, in [[224](#bib.bib224)] the method uses a recurrent
    framework to generate future frames in order to localize pedestrians in next frames.
    In the context of action detection, some methods rely on a future frame [[226](#bib.bib226)]
    or trajectory prediction of objects to detect actions [[227](#bib.bib227)]. In
    [[225](#bib.bib225)], a method is used for detecting an object in 3D by relying
    on predicting next best viewing angle of the object. Liu et al. [[223](#bib.bib223)]
    uses a video prediction framework to predict future motion flow maps and images.
    The future predictions that do not conform with expectations will be identified
    as abnormal.
  prefs: []
  type: TYPE_NORMAL
- en: 9 The evaluation of state-of-the-art
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to the evaluation of algorithms, there are two important factors:
    metrics and datasets. They highlight the strengths and weaknesses of the algorithms
    and provide a means to compare the relative performances of the methods. Given
    the importance of these two factors in the design of algorithms, we dedicate the
    following sections to discussing the common metrics and datasets used for vision-based
    prediction tasks. Since the datasets and metrics used in these applications are
    highly diverse, we will focus our discussion on some of the main ones for each
    prediction category while providing visual aids to summarize what the past works
    used for evaluation purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 10 Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/123c23fed9399d1b22083411fe9de376.png)![Refer to caption](img/5a7e73c63d3321a43fda47325af9d3a0.png)![Refer
    to caption](img/6643993049cdc15ab18e9731688f7e11.png)![Refer to caption](img/422842051f851883ea64ff8a39b50505.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Metrics used in vision-based prediction applications. From left to
    right: Video, action, trajectory and motion prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we follow the same routine as the discussion on the past works
    and divide the metrics into different categories. A summary of the metrics can
    be found in Figure [1](#S10.F1 "Figure 1 ‣ 10 Metrics ‣ Deep Learning for Vision-based
    Prediction: A Survey"). The interested readers can also refer to Appendix [B](#A2
    "Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey") for further information regarding the metrics and the the
    papers that used them. Note while we discuss the metrics in each category and
    we only provide mathematical expressions of the most popular metrics in Appendix
    [C](#A3 "Appendix C Metric formulas ‣ Deep Learning for Vision-based Prediction:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Video prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Video prediction is about generating realistic images, hence the best performance
    is achieved when the disparities between the generated images and groundtruth
    images are minimal. The most straightforward way of computing the disparity is
    to measure pixel-wise error using a Mean Square Error (MSE) [[2](#bib.bib2), [4](#bib.bib4),
    [20](#bib.bib20), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12), [229](#bib.bib229),
    [23](#bib.bib23), [14](#bib.bib14), [7](#bib.bib7), [6](#bib.bib6), [27](#bib.bib27),
    [30](#bib.bib30)], which computes average squared intensity differences between
    pixels. Another more popular metric related to MSE is Peak Signal-to-Noise Ratio
    (PSNR) [[2](#bib.bib2), [1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [5](#bib.bib5), [32](#bib.bib32), [33](#bib.bib33),
    [22](#bib.bib22), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12), [229](#bib.bib229),
    [24](#bib.bib24), [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [36](#bib.bib36),
    [6](#bib.bib6), [15](#bib.bib15), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)].
    PSNR is the ratio of the maximum pixel value (i.e. possible signal power), e.g.
    255 in 8-bit images, divided by the MSE (or power of distorting noise) measure
    of two images. The lower the error between two images, the higher the value of
    PSNR, and consequently, the higher the quality of the generated images. Because
    of the wide dynamic range of signals, PSNR value is expressed in the logarithmic
    decibel scale.
  prefs: []
  type: TYPE_NORMAL
- en: Although MSE, and PSNR metrics are easy to calculate, they cannot measure the
    perceived visual quality of a generated image. An alternative metric to address
    this issue is Structural SIMilarity (SSIM) index ([[230](#bib.bib230)]) [[2](#bib.bib2),
    [3](#bib.bib3), [1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [5](#bib.bib5), [32](#bib.bib32), [33](#bib.bib33),
    [22](#bib.bib22), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12), [24](#bib.bib24),
    [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [6](#bib.bib6), [15](#bib.bib15),
    [27](#bib.bib27), [29](#bib.bib29)] which is designed to model image distortion.
    To capture the structural differences between the two images, SSIM separates illumination
    information as it is independent of objects’ structures. As a result, the similarity
    is measured by a combination of three comparisons, namely luminance, contrast,
    and structure.
  prefs: []
  type: TYPE_NORMAL
- en: Higher-level contextual similarities may not be captured by distance measures
    on pixel values. More recently proposed metric, Learned Perceptual Image Patch
    Similarity (LPIPS), ([[231](#bib.bib231)])[[3](#bib.bib3), [17](#bib.bib17), [37](#bib.bib37),
    [11](#bib.bib11)] measures the similarity between two images by comparing internal
    activations of convolutional networks trained for high-level classification tasks.
    The value is calculated by an average L2 distance over normalized deep features.
  prefs: []
  type: TYPE_NORMAL
- en: Some other metrics that have been used in the literature are qualitative human
    judgment [[11](#bib.bib11), [25](#bib.bib25), [8](#bib.bib8), [28](#bib.bib28)],
    Frechet Video Distance (FVD) ([[232](#bib.bib232)]) [[3](#bib.bib3), [18](#bib.bib18)],
    Maximum Mean Discrepancy (MMD) [[26](#bib.bib26)], Inception Scores (IS) ([[233](#bib.bib233)])
    [[26](#bib.bib26)], Binary Cross Entropy (BCE) [[23](#bib.bib23)], L1 [[9](#bib.bib9),
    [12](#bib.bib12)], and Root MSE (RMSE) [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Action prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to classification tasks, many action prediction algorithms use accuracy
    measure to report on the performance that is the ratio of the correct predictions
    with respect to the total number of predictions [[56](#bib.bib56), [63](#bib.bib63),
    [64](#bib.bib64), [99](#bib.bib99), [100](#bib.bib100), [66](#bib.bib66), [71](#bib.bib71),
    [9](#bib.bib9), [74](#bib.bib74), [48](#bib.bib48), [49](#bib.bib49), [107](#bib.bib107),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [108](#bib.bib108), [83](#bib.bib83),
    [101](#bib.bib101), [109](#bib.bib109), [89](#bib.bib89), [102](#bib.bib102),
    [75](#bib.bib75), [103](#bib.bib103), [97](#bib.bib97), [76](#bib.bib76), [91](#bib.bib91),
    [110](#bib.bib110), [111](#bib.bib111), [85](#bib.bib85), [105](#bib.bib105),
    [86](#bib.bib86), [70](#bib.bib70), [50](#bib.bib50), [112](#bib.bib112), [8](#bib.bib8),
    [104](#bib.bib104), [93](#bib.bib93), [41](#bib.bib41), [94](#bib.bib94), [52](#bib.bib52),
    [59](#bib.bib59), [42](#bib.bib42), [106](#bib.bib106), [61](#bib.bib61), [113](#bib.bib113),
    [54](#bib.bib54), [43](#bib.bib43), [44](#bib.bib44), [95](#bib.bib95), [62](#bib.bib62),
    [45](#bib.bib45), [55](#bib.bib55), [46](#bib.bib46), [40](#bib.bib40), [47](#bib.bib47)].
    Despite being used widely, accuracy on its own is not a strong indicator of performance,
    especially when we are dealing with class-imbalance data. This is because, for
    example, the model can simply favor the more represented class and predict every
    input as that class. This then would result in a high accuracy measure because
    the metric only considers the ratio of correct predictions. To address these shortcomings,
    some works use complimentary metrics that, in addition to correct predictions,
    account for different types of false predictions. These metrics are precision
    [[63](#bib.bib63), [71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [74](#bib.bib74),
    [67](#bib.bib67), [82](#bib.bib82), [83](#bib.bib83), [70](#bib.bib70), [58](#bib.bib58),
    [51](#bib.bib51), [96](#bib.bib96), [80](#bib.bib80), [52](#bib.bib52), [42](#bib.bib42),
    [53](#bib.bib53)], recall [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65),
    [71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [74](#bib.bib74), [67](#bib.bib67),
    [68](#bib.bib68), [82](#bib.bib82), [83](#bib.bib83), [77](#bib.bib77), [70](#bib.bib70),
    [58](#bib.bib58), [51](#bib.bib51), [96](#bib.bib96), [80](#bib.bib80), [52](#bib.bib52),
    [42](#bib.bib42), [53](#bib.bib53)], and Area Under the Curve (AUC) [[71](#bib.bib71),
    [112](#bib.bib112), [54](#bib.bib54)] of precision-recall graph. Precision and
    recall also form the basis for the calculation of some higher level metrics such
    as F1-score [[71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [83](#bib.bib83),
    [90](#bib.bib90), [58](#bib.bib58), [96](#bib.bib96), [52](#bib.bib52), [42](#bib.bib42)],
    Average Precision (AP) [[88](#bib.bib88), [9](#bib.bib9), [73](#bib.bib73), [82](#bib.bib82),
    [79](#bib.bib79)] and its variations mean AP (mAP) [[87](#bib.bib87), [107](#bib.bib107),
    [83](#bib.bib83), [77](#bib.bib77), [84](#bib.bib84), [78](#bib.bib78), [92](#bib.bib92),
    [112](#bib.bib112)] and calibrated AP (cAP) [[92](#bib.bib92)]. Some of the less
    common performance metrics are Matthews Correlation Coefficient (MCC) [[76](#bib.bib76)],
    False positive (FP)[[53](#bib.bib53)], True Positive Rate (TPR) and False Positive
    Rate (FPR) [[47](#bib.bib47)], Prediction Power (PP) [[57](#bib.bib57)], and Mean
    Reciprocal Rank (MRR) [[44](#bib.bib44)].
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the application, some algorithms evaluate the timing factor in
    terms of the Run Time (RT) of the model [[9](#bib.bib9), [73](#bib.bib73), [43](#bib.bib43)]
    or time of the event, e.g. beginning of the next activity [[70](#bib.bib70)],
    Time To Accident (or collision) (TTA) [[81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [84](#bib.bib84)], and, in the context of driving, Time To Maneuver (TTM) [[74](#bib.bib74),
    [49](#bib.bib49), [96](#bib.bib96), [80](#bib.bib80), [53](#bib.bib53)].
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Trajectory prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Perhaps the most popular performance measure for trajectory prediction is Average
    Displacement Error (ADE) [[182](#bib.bib182), [171](#bib.bib171), [101](#bib.bib101),
    [234](#bib.bib234), [159](#bib.bib159)] calculated as the average error between
    the prediction location and the ground truth over all time steps. Some methods
    complement ADE measure with its extension Final Displacement Error (FDE) [[177](#bib.bib177),
    [87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [146](#bib.bib146), [163](#bib.bib163),
    [168](#bib.bib168), [172](#bib.bib172), [152](#bib.bib152), [153](#bib.bib153),
    [155](#bib.bib155), [164](#bib.bib164), [180](#bib.bib180), [158](#bib.bib158),
    [160](#bib.bib160)]. As the name suggests, FDE only measures the error between
    the ground truth and the generated trajectory for the final time step.
  prefs: []
  type: TYPE_NORMAL
- en: Many other works use the same metric as ADE [[161](#bib.bib161), [170](#bib.bib170),
    [148](#bib.bib148), [150](#bib.bib150), [129](#bib.bib129), [183](#bib.bib183),
    [184](#bib.bib184), [118](#bib.bib118), [119](#bib.bib119), [121](#bib.bib121),
    [127](#bib.bib127), [115](#bib.bib115), [132](#bib.bib132), [117](#bib.bib117),
    [123](#bib.bib123)] or ADE/FDE [[139](#bib.bib139), [144](#bib.bib144), [169](#bib.bib169),
    [151](#bib.bib151), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [176](#bib.bib176)] without using the same terminology. It is also a common practice
    that instead of using average or final time step measures, to calculate the error
    at different time steps over a period of time [[140](#bib.bib140), [147](#bib.bib147),
    [114](#bib.bib114), [173](#bib.bib173), [130](#bib.bib130), [156](#bib.bib156),
    [120](#bib.bib120), [77](#bib.bib77), [165](#bib.bib165), [124](#bib.bib124),
    [133](#bib.bib133), [125](#bib.bib125), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)].
  prefs: []
  type: TYPE_NORMAL
- en: To measure displacement error in probablistic trajectory prediction algorithms,
    some works generate a set number of samples and report the best measure (i.e.
    minimum error) [[161](#bib.bib161), [165](#bib.bib165), [178](#bib.bib178), [168](#bib.bib168),
    [162](#bib.bib162), [166](#bib.bib166)] or average over all samples [[174](#bib.bib174),
    [162](#bib.bib162), [166](#bib.bib166)]. Depending on the error metric used, some
    refer to these measures as Minimum ADE/FDE (MinADE/FDE)[[137](#bib.bib137), [178](#bib.bib178),
    [168](#bib.bib168)] (using Euclidean distance) or Mean/Minimum Mean Square Displacement
    (Mean/MinMSD) [[162](#bib.bib162), [166](#bib.bib166)] (using MSE). Some of the
    other probabilistic measures are Log-Likelihood (LL) [[145](#bib.bib145), [181](#bib.bib181),
    [120](#bib.bib120)], Negative Log-Likelihood (NLL) [[172](#bib.bib172), [183](#bib.bib183),
    [174](#bib.bib174), [175](#bib.bib175), [123](#bib.bib123), [125](#bib.bib125)],
    Kullback–Leibler Divergence (KLD) [[181](#bib.bib181), [127](#bib.bib127), [125](#bib.bib125)],
    Negative Log-Probability (NLP)[[118](#bib.bib118), [119](#bib.bib119)], Cross
    Entropy (CE) [[166](#bib.bib166)], Average Prediction Probability (APP) [[157](#bib.bib157)].
  prefs: []
  type: TYPE_NORMAL
- en: Performance can also be evaluated using common classification metrics. For example,
    in [[161](#bib.bib161), [128](#bib.bib128)] Hit Rate (HR) and in [[184](#bib.bib184),
    [165](#bib.bib165)] Miss Rate (MR) metrics are used. In these cases, if the predicted
    trajectory is below (or above) a certain distance threshold from the groundtruth,
    it is considered as a hit or miss. Following a similar approach, some authors
    calculate accuracy [[167](#bib.bib167), [127](#bib.bib127)] or precision [[115](#bib.bib115)]
    of predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the other metrics used in the literature are Run Time (RT) [[147](#bib.bib147),
    [118](#bib.bib118), [121](#bib.bib121), [123](#bib.bib123), [135](#bib.bib135)],
    Average Non-linear Displacement Error (ANDE) [[155](#bib.bib155), [160](#bib.bib160)],
    Maximum Distance (MaxD) [[165](#bib.bib165), [184](#bib.bib184)], State collision
    rate (SCR) [[175](#bib.bib175)], Percentage Deviated (PD) [[122](#bib.bib122)],
    Distance to Goal (DtG) [[125](#bib.bib125)], Fraction of Near Misses (FNM) [[126](#bib.bib126)],
    Expected Calibration Error (ECE) [[172](#bib.bib172)], and qualitative (Q) [[116](#bib.bib116)].
    A few works predict the orientations of pedestrians [[154](#bib.bib154), [234](#bib.bib234),
    [131](#bib.bib131)] or vehicles [[77](#bib.bib77)], therefore also report performance
    using Mean angular error (MAnE).
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Pitfalls of trajectory prediction metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike video and action prediction fields, performance measures for trajectory
    prediction algorithms are not standardized in terms error metrics used and units
    of measure. For example, for measuring displacement error, although many algorithms
    use Euclidean Distance (ED) (aka L2-distance, L2-norm, Euclidean norm) [[87](#bib.bib87),
    [138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141), [146](#bib.bib146),
    [163](#bib.bib163), [168](#bib.bib168), [172](#bib.bib172), [152](#bib.bib152),
    [153](#bib.bib153), [155](#bib.bib155), [180](#bib.bib180), [158](#bib.bib158),
    [182](#bib.bib182), [234](#bib.bib234), [159](#bib.bib159), [150](#bib.bib150),
    [184](#bib.bib184), [115](#bib.bib115), [139](#bib.bib139), [169](#bib.bib169),
    [151](#bib.bib151), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [114](#bib.bib114), [173](#bib.bib173), [130](#bib.bib130), [156](#bib.bib156),
    [77](#bib.bib77), [165](#bib.bib165), [124](#bib.bib124), [133](#bib.bib133),
    [125](#bib.bib125), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [161](#bib.bib161), [168](#bib.bib168), [155](#bib.bib155), [176](#bib.bib176),
    [125](#bib.bib125)], many others rely on different error metrics including MSE
    [[143](#bib.bib143), [160](#bib.bib160), [171](#bib.bib171), [101](#bib.bib101),
    [170](#bib.bib170), [129](#bib.bib129), [183](#bib.bib183), [127](#bib.bib127),
    [144](#bib.bib144), [162](#bib.bib162), [166](#bib.bib166)], RMSE [[177](#bib.bib177),
    [161](#bib.bib161), [140](#bib.bib140), [170](#bib.bib170), [140](#bib.bib140),
    [147](#bib.bib147)], Weighted RMSE [[120](#bib.bib120)], Mean Absolute Error (MAE)[[77](#bib.bib77),
    [148](#bib.bib148)], Hausdorff Distance (HD)[[174](#bib.bib174)], Modified HD
    (MHD) [[118](#bib.bib118), [119](#bib.bib119), [121](#bib.bib121), [115](#bib.bib115),
    [132](#bib.bib132), [123](#bib.bib123), [125](#bib.bib125)], and discrete Fréchet
    distance (DFD) [[179](#bib.bib179)]. Moreover, trajectory prediction algorithms
    use different units for measuring displacement error. These are meter [[177](#bib.bib177),
    [87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [146](#bib.bib146),
    [168](#bib.bib168), [172](#bib.bib172), [153](#bib.bib153), [180](#bib.bib180),
    [182](#bib.bib182), [161](#bib.bib161), [170](#bib.bib170), [148](#bib.bib148),
    [150](#bib.bib150), [129](#bib.bib129), [121](#bib.bib121), [139](#bib.bib139),
    [169](#bib.bib169), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [140](#bib.bib140), [147](#bib.bib147), [114](#bib.bib114), [173](#bib.bib173),
    [130](#bib.bib130), [156](#bib.bib156), [77](#bib.bib77), [165](#bib.bib165),
    [124](#bib.bib124), [133](#bib.bib133), [135](#bib.bib135), [136](#bib.bib136),
    [161](#bib.bib161), [165](#bib.bib165), [168](#bib.bib168), [166](#bib.bib166)],
    pixel [[138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142),
    [163](#bib.bib163), [152](#bib.bib152), [183](#bib.bib183), [132](#bib.bib132),
    [165](#bib.bib165)], normalized pixel [[142](#bib.bib142), [158](#bib.bib158),
    [125](#bib.bib125)] and feet [[184](#bib.bib184)].
  prefs: []
  type: TYPE_NORMAL
- en: Although such discrepancy between error metrics and units is expected across
    different applications, the problem arises when the proposed works do not specify
    the error metric [[142](#bib.bib142), [178](#bib.bib178)], the unit of measure
    [[143](#bib.bib143), [155](#bib.bib155), [160](#bib.bib160), [171](#bib.bib171),
    [101](#bib.bib101), [118](#bib.bib118), [119](#bib.bib119), [127](#bib.bib127),
    [115](#bib.bib115), [123](#bib.bib123), [151](#bib.bib151), [176](#bib.bib176),
    [120](#bib.bib120), [174](#bib.bib174), [178](#bib.bib178), [155](#bib.bib155)]
    or both [[164](#bib.bib164), [117](#bib.bib117)]. Despite the fact that the reported
    results might imply the choice of the metrics and units, the lack of specification
    can cause erroneous comparisons, specially because many authors use the results
    of previous works directly as reported in the papers.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, metric and unit discrepancy exists within the same applications
    and the same error measuring techniques. For instance, in the case of ADE measure,
    this metric is originally proposed in [[235](#bib.bib235)] in terms of ED, and
    was referred to as ADE by the authors of [[160](#bib.bib160)] despite the fact
    that they used MSE instead. This is also apparent in many subsequent works that
    employed ADE measure. For example, the majority of methods use the original metric
    and report the results in terms of ED [[87](#bib.bib87), [138](#bib.bib138), [141](#bib.bib141),
    [152](#bib.bib152), [153](#bib.bib153), [155](#bib.bib155), [146](#bib.bib146),
    [163](#bib.bib163), [168](#bib.bib168), [172](#bib.bib172), [180](#bib.bib180),
    [158](#bib.bib158), [182](#bib.bib182), [234](#bib.bib234), [159](#bib.bib159)]
    whereas some works use MSE [[143](#bib.bib143), [160](#bib.bib160), [171](#bib.bib171),
    [101](#bib.bib101)] and RMSE[[177](#bib.bib177), [140](#bib.bib140)] or do not
    specify the metric [[142](#bib.bib142), [164](#bib.bib164)]. Although the formulation
    of these metrics look similar, they produce different results. ADE using ED, for
    example, is square-root of squared differences averaged over all samples and time
    steps. Unlike ED, in RMSE, the averaging takes place inside square-root operation.
    MSE, on the other hand, is very different from the other two metrics, and does
    not calculate the root of the error. As we can also see in some of the past works,
    this discrepancy may cause confusion about the intended and actual metric that
    is used. For example, in [[140](#bib.bib140)] the authors propose to use MAE metric
    while presenting mathematical formulation of Euclidean distance. The authors of
    [[159](#bib.bib159), [176](#bib.bib176)] make a similar mistake and define ED
    formulation but refer to it as MSE.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, some algorithms within the same applications and using the same
    datasets use different measuring unit. For instance, in the context of surveillance,
    ETH [[235](#bib.bib235)] is one of the most commonly used datasets. Many works
    use this dataset for benchmarking the performance of their proposed algorithms,
    however, they either use different units, e.g. meter [[87](#bib.bib87), [138](#bib.bib138),
    [139](#bib.bib139), [146](#bib.bib146), [149](#bib.bib149), [153](#bib.bib153),
    [156](#bib.bib156), [180](#bib.bib180)], pixel [[163](#bib.bib163), [140](#bib.bib140),
    [142](#bib.bib142), [152](#bib.bib152)], normalized pixel [[158](#bib.bib158),
    [142](#bib.bib142)], or do not specify the unit used [[143](#bib.bib143), [178](#bib.bib178),
    [151](#bib.bib151), [155](#bib.bib155), [164](#bib.bib164), [160](#bib.bib160)].
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, another potential source of error in performance evaluation
    is in the design of the experiments. Taking surveillance applications as an example,
    it is a common practice to evaluate algorithms with 8 frames observations of the
    past and prediction 12 steps in the future [[178](#bib.bib178), [87](#bib.bib87),
    [138](#bib.bib138), [139](#bib.bib139), [153](#bib.bib153), [180](#bib.bib180),
    [160](#bib.bib160)]. However, in some cases the performance of state-of-the-art
    is reported under the standard 8/12 condition, but the proposed algorithms are
    tested under different conditions. For instance, in [[155](#bib.bib155)] the authors
    incorrectly compared the performance of their proposed algorithm using 5 observations
    and 5 predictions with the results of the previous works evaluated under the standard
    8/12 condition.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Motion prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the inherent stochasticity of human body movement, motion prediction
    algorithms often limit their prediction horizon to approximately $500ms$. To measure
    the error between corresponding ground truth and predicted poses, these algorithms
    use mean average error, either in angle space (MAnE) [[190](#bib.bib190), [236](#bib.bib236),
    [197](#bib.bib197), [198](#bib.bib198), [196](#bib.bib196), [187](#bib.bib187),
    [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193), [194](#bib.bib194),
    [96](#bib.bib96), [195](#bib.bib195)] or joint space (MJE) in terms of joint coordinates
    [[236](#bib.bib236), [188](#bib.bib188), [101](#bib.bib101), [103](#bib.bib103),
    [56](#bib.bib56), [103](#bib.bib103), [26](#bib.bib26), [110](#bib.bib110), [187](#bib.bib187),
    [186](#bib.bib186), [200](#bib.bib200)]. In the 3D motion prediction domain, a
    metric known as Mean Per Joint Prediction Error (MPJPE) [[197](#bib.bib197), [199](#bib.bib199)]
    is used which is the error over joints normalized with respect to the root joint.
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative to distance error metrics, Percentage of Correct Keypoints
    (PCK) [[199](#bib.bib199), [187](#bib.bib187), [189](#bib.bib189), [188](#bib.bib188)]
    measures how many of the keypoints (e.g. joints) are predicted correctly. The
    correct predictions are those that are below a certain error threshold (e.g. 0.05).
    Some works also use the accuracy metric to report on how well the algorithm can
    localize the position of a particular joint within an error tolerance region [[48](#bib.bib48),
    [195](#bib.bib195)].
  prefs: []
  type: TYPE_NORMAL
- en: Other metrics used in the literature include Normalized Power Spectrum Similarity
    (NPSS) [[190](#bib.bib190)], Reconstruction Error (RE) [[199](#bib.bib199)], Limb
    Orientation (LO) [[56](#bib.bib56)], PoSe Entropy (PSEnt), PoSe KL (PSKL) [[198](#bib.bib198)],
    qualitative human judgment [[198](#bib.bib198), [192](#bib.bib192)] and method
    Run Time (RT) [[236](#bib.bib236), [188](#bib.bib188)].
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.1 Pitfalls of motion prediction metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to trajectory methods, motion prediction algorithms are evaluated using
    distance-based methods that calculate the error between pose vectors. In the case
    of MAnE measure, some methods use ED metric [[197](#bib.bib197), [196](#bib.bib196),
    [193](#bib.bib193), [194](#bib.bib194), [96](#bib.bib96), [195](#bib.bib195)]
    while others use MSE [[190](#bib.bib190), [236](#bib.bib236), [198](#bib.bib198),
    [191](#bib.bib191), [192](#bib.bib192)]. Sometimes no metric is specified [[187](#bib.bib187)].
    The same holds for MJE measure where metrics used include MSE [[236](#bib.bib236),
    [101](#bib.bib101), [103](#bib.bib103), [103](#bib.bib103)], RMSE [[188](#bib.bib188),
    [200](#bib.bib200)], ED [[26](#bib.bib26), [110](#bib.bib110), [186](#bib.bib186)],
    MAE [[187](#bib.bib187)], or no metric is specified [[56](#bib.bib56)].
  prefs: []
  type: TYPE_NORMAL
- en: The added challenge in coordinate-based error measures, e.g. MJE, MPJPE, is
    the error unit. While many approaches do not specify the unit explicitly [[236](#bib.bib236),
    [103](#bib.bib103), [103](#bib.bib103), [26](#bib.bib26), [110](#bib.bib110),
    [187](#bib.bib187), [186](#bib.bib186)], others clearly state whether the unit
    is in pixel [[188](#bib.bib188), [101](#bib.bib101)], centimeter [[56](#bib.bib56)],
    meter [[200](#bib.bib200)] or millimeter [[197](#bib.bib197), [199](#bib.bib199)].
    As was the case before, here many algorithms that benchmark on the same datasets,
    may use different performance metrics, e.g. using popular human datasset Human
    3.6M [[237](#bib.bib237)], MAnE (ED), MAnE (MSE)[[191](#bib.bib191)] and MJE (ED)
    [[193](#bib.bib193)] are used.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Other prediction applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending on the task objectives, the metrics used in other prediction applications
    are similar to the ones discussed thus far. For instance, the applications that
    classify future events or outcomes, e.g. contest or an election winner, next image
    index for storytelling, severe weather, and pain, use common metrics such as accuracy
    [[220](#bib.bib220), [221](#bib.bib221), [8](#bib.bib8)], precision, recall [[213](#bib.bib213),
    [218](#bib.bib218)], percentage of correct predictions (PCP) [[219](#bib.bib219)],
    and Matthews correlation coefficient (MCC) [[218](#bib.bib218)] which predicts
    the quality of binary classification by taking into account both false and true
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Regression based methods, such as temperature, trends, or steering prediction,
    use distance metrics including Euclidean Distance (ED) [[202](#bib.bib202), [238](#bib.bib238)],
    RMSE [[214](#bib.bib214)], MSE [[212](#bib.bib212)], MAE [[239](#bib.bib239),
    [217](#bib.bib217), [204](#bib.bib204)], Mean Absolute Percentage Error (MAPE)
    [[216](#bib.bib216), [217](#bib.bib217)], normalized MAPE (nMAPE) [[215](#bib.bib215)],
    and the Spearman’s ranking Correlation (SRC) [[216](#bib.bib216)] which measures
    the strength and direction of relationship between two variables.
  prefs: []
  type: TYPE_NORMAL
- en: Of particular interest are metrics used for evaluating generative models that
    predict Occupancy Grid Maps (OGMs) and segmentation maps. OGMs are grayscale images
    that highlight the likelihood of a certain region (represented as a cell in the
    grid) that is occupied. The generated map can be compared to ground truth by using
    image similarity metrics such as SSIM [[205](#bib.bib205), [206](#bib.bib206)],
    PSNR [[206](#bib.bib206)] or psi ($\psi$) [[203](#bib.bib203)]. Alternatively,
    OGM can be evaluated using a binary classification metric. Here, the grid cells
    are classified as occupied or free by applying a threshold and then can be evaluated
    as a whole by using metrics such as True Positive (TP), True Negative (TN) [[205](#bib.bib205)],
    Receiver Operator Characteristic (ROC) curve over TP and TN [[207](#bib.bib207),
    [208](#bib.bib208)], F1-score [[201](#bib.bib201), [207](#bib.bib207)], precision,
    recall, and their corresponding AUC [[209](#bib.bib209)]. Given that OGM prediction
    algorithms are mainly used in safety-critical applications such as autonomous
    driving, some algorithms are also evaluated in terms of their Run Time (RT) [[205](#bib.bib205),
    [206](#bib.bib206), [209](#bib.bib209)].
  prefs: []
  type: TYPE_NORMAL
- en: Image similarity metrics such as PSNR and SSIM can also be used in the segmentation
    prediction domain [[211](#bib.bib211)]. The most common metric, however, is Intersection
    over Union (IoU)[[10](#bib.bib10), [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212)]
    which measures the average overlap of segmented instances with the ground truth
    segments. In addition, by applying a threshold to IoU scores, the true matches
    can be identified and used to calculate the Average Precision (AP) scores as in
    [[210](#bib.bib210)]. Other metrics used for segmentation prediction tasks include
    EndPoint error (EPE) [[212](#bib.bib212)], Probabilistic Rand Index (RI), Global
    Consistency Error (GCE), and Variation of Information (VoI) [[210](#bib.bib210)].
  prefs: []
  type: TYPE_NORMAL
- en: 11 Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Year | Dataset | Type | Annotations | Application |'
  prefs: []
  type: TYPE_TB
- en: '| V | A | T | M | O |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | ARGOVerse [[137](#bib.bib137)] | Traffic | RGB, LIDAR, 3D BB |  |  |
    x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CARLA [[162](#bib.bib162)] | Traffic (sim) | RGB |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| EgoPose [[186](#bib.bib186)] | Pose (ego) | RGB, 3D Pose |  |  |  | x |  |'
  prefs: []
  type: TYPE_TB
- en: '| Future Motion (FM) [[167](#bib.bib167)] | Mix | RGB, BB, Attrib. |  |  |
    x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| InstaVariety [[240](#bib.bib240)] | Activities | RGB, BB, Pose |  |  |  |
    x |  |'
  prefs: []
  type: TYPE_TB
- en: '| INTEARCTION [[241](#bib.bib241)] | Traffic | Map, Traj. |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Luggage [[81](#bib.bib81)] | Robot | Stereo RGB, BB |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MGIF [[242](#bib.bib242)] | Activities | RGB | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Pedestrian Intention Estimation (PIE) [[144](#bib.bib144)] | Traffic | RGB,
    BB, Class, Attrib., Temporal seg., Vehicle sensors |  | x | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| nuScenes [[243](#bib.bib243)] | Traffic | RGB, LIDAR, 3D BB, Vehicle sensors
    |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Vehicle-Pedestrian-Mixed (VPM) [[141](#bib.bib141)] | Traffic | RGB, BB |  |  |
    x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TRAF [[177](#bib.bib177)] | Traffic | RGB, BN, Class, Time-of-day |  |  |
    x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | 3D POSES IN THE WILD (3DPW) [[244](#bib.bib244)] | Outdoor | RGB,
    2D/3D Pose, Models |  |  |  | x |  |'
  prefs: []
  type: TYPE_TB
- en: '| ActEV/VIRAT [[245](#bib.bib245)] | Surveillance | RGB, BB, Activity, Temporal
    seg. |  | x | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ACTICIPATE [[246](#bib.bib246)] | Interaction | RGB, Gaze, Pose |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Atomic Visual Actions (AVA) [[247](#bib.bib247)] | Activities | RGB, Activity,
    Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Epic-Kitchen [[248](#bib.bib248)] | Cooking (ego) | RGB, Audio, BB, Class,
    Text, Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| EGTEA Gaze+ [[249](#bib.bib249)] | Cooking (ego) | RGB, Gaze, Mask, Activity,
    Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ShanghaiTech Campus (STC) [[223](#bib.bib223)] | Surveillance | RGB, Anomaly
    | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeStack [[250](#bib.bib250)] | Objects (sim) | RGBD, Mask, Stability |
    x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| VIENA [[75](#bib.bib75)] | Traffic (sim) | RGB, Activity, Vehicle sensors
    |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| YouCook2 [[251](#bib.bib251)] | Cooking | RGB, Audio, Text, Activity, Temporal
    seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | BU Action (BUA) [[252](#bib.bib252)] | Activities | RGB (image), Activity
    |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CityPerson [[253](#bib.bib253)] | Traffic | Stereo RGB, BB, Semantic seg.
    |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Epic-Fail [[84](#bib.bib84)] | Risk assessment | RGB, BB, Traj., Temporal
    seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Joint Attention in Autonomous Driving (JAAD) [[78](#bib.bib78)] | Traffic
    | RGB, BB, Attrib., Temporal seg. | x | x | x |  | x |'
  prefs: []
  type: TYPE_TB
- en: '| L-CAS [[254](#bib.bib254)] | Traffic | LIDAR, 3D BB, Attrib. |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mouse Fish [[255](#bib.bib255)] | Animals | Depth, 3D Pose |  |  |  | x |  |'
  prefs: []
  type: TYPE_TB
- en: '| Oxford Robot Car (ORC) [[256](#bib.bib256)] | Traffic | Stereo RGB, LIDAR,
    Vehicle sensors |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| PKU-MMD [[257](#bib.bib257)] | Activities, interactions | RGBD, IR, 3D Pose,
    Multiview, Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Recipe1M [[258](#bib.bib258)] | Cooking | RGB(image), Text |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| STRANDS [[259](#bib.bib259)] | Traffic | RGBD, 3DBB |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | BAIR Push [[29](#bib.bib29)] | Object manipulation | RGB | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Bouncing Ball (BB) [[260](#bib.bib260)] | Simulation | RGB | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Miss Universe (MU) [[220](#bib.bib220)] | Miss universe | RGB, BB, Scores
    |  |  |  |  | x |'
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes [[261](#bib.bib261)] | Traffic | Stereo RGB, BB, Semantic seg.,
    Vehicle Sensors | x |  | x |  | x |'
  prefs: []
  type: TYPE_TB
- en: '| CMU Mocap [[262](#bib.bib262)] | Activities | 3D Pose, Activity |  | x |  |
    x |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dashcam Accident Dataset (DAD) [[79](#bib.bib79)] | Traffic, accidents |
    RGB, BB, Class, Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| NTU RGB-D [[263](#bib.bib263)] | Activities | RGBD, IR, 3D Pose, Activity
    |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ongoing Activity (OA) [[106](#bib.bib106)] | Actvities | RGB, Activity |  |
    x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| OAD [[264](#bib.bib264)] | Activities | RGBD, 3D Pose, Activity, Temporal
    seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Stanford Drone (SD) [[265](#bib.bib265)] | Surveillance | RGB, BB, Class
    |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TV Series [[266](#bib.bib266)] | Activities | RGB, Activity, Temporal seg.
    |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Visual StoryTelling (VIST) [[267](#bib.bib267)] | Visual story | RGB, Text
    |  |  |  |  | x |'
  prefs: []
  type: TYPE_TB
- en: '| Youtube-8M [[268](#bib.bib268)] | Activities | RGB, Activity, Temporal seg.
    | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Amazon [[269](#bib.bib269)] | Fashion | Features, Attrib., Text |  |  |  |  |
    x |'
  prefs: []
  type: TYPE_TB
- en: '| Atari [[30](#bib.bib30)] | Games | RGB | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Brain4Cars [[53](#bib.bib53)] | Traffic, Driver | RGB, BB, Attrib., Temporal
    seg., Vehicle sensors |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CMU Panoptic [[270](#bib.bib270)] | Interaction | RGBD, Multiview, 3D Pose,
    3D facial landmark | x | x |  | x |  |'
  prefs: []
  type: TYPE_TB
- en: '| First Person Personalized Activities (FPPA) [[95](#bib.bib95)] | Activities
    (ego) | RGB, Activity, Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze + [[271](#bib.bib271)] | Cooking (ego) | RGB, Gaze, Mask, Activity,
    Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MicroBlog-Images (MBI-1M) [[272](#bib.bib272)] | Tweets | RGB (image), Attrib.,
    Text |  |  |  |  | x |'
  prefs: []
  type: TYPE_TB
- en: '| MOT [[273](#bib.bib273)] | Surveillance | RGB, BB |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Moving MNIST (MMNIST) [[274](#bib.bib274)] | Digits | Grayscale | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SUN RGB-D [[275](#bib.bib275)] | Places | RGBD, 3D BB , Class |  |  |  |  |
    x |'
  prefs: []
  type: TYPE_TB
- en: '| SYSU 3DHOI [[276](#bib.bib276)] | Object interaction | RGBD, 3D Pose, Activity
    |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| THUMOS [[277](#bib.bib277)] | Activities | RGB, Activity, Temporal seg. |
    x | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Watch-n-Push (WnP) [[278](#bib.bib278)] | Activities | RGBD, 3D Pose, Activity,
    Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wider [[279](#bib.bib279)] | Activities | RGB (image), Activity |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2014 | Breakfast [[280](#bib.bib280)] | Cooking | RGB, Activity, Temporal
    seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Human3.6M [[237](#bib.bib237)] | Activities | RGB, 3D Pose, Activity | x
    | x |  | x |  |'
  prefs: []
  type: TYPE_TB
- en: '| MPII Human Pose [[281](#bib.bib281)] | Activities | RGB, Pose, Activity |  |  |  |
    x |  |'
  prefs: []
  type: TYPE_TB
- en: '| Online RGBD Action Dataset (ORGBD) [[282](#bib.bib282)] | Activities | RGBD,
    BB, 3D Pose, Activity |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sports-1M [[283](#bib.bib283)] | Sports | RGB, Activity | x | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: A summary of common datasets from years 2014-2019 used in vision-based
    prediction applications, namely video (V), action (A), trajectory (T), motion
    (M) and others (O). The annotation column specifies the type of data (e.g. RGB,
    Infrared(IR)) and annotation types. All datasets contain image sequences unless
    specified by “image”. As for annotations, BB stands for bounding box. Attributes
    include any object characteristics (e.g. for pedestrians demographics, behavior).
    Vehicle sensors may include speed, steering angle, GPS, etc. Temporal seg. identifies
    the datasets that specify the start and end of the events.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Dataset | Type | Annotations | Application |'
  prefs: []
  type: TYPE_TB
- en: '| V | A | T | M | O |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | 50Salads [[284](#bib.bib284)] | Cooking (ego) | RGBD, Activity, Temporal
    seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ATC [[285](#bib.bib285)] | Surveillance | RGB, Traj., Attrib. |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CAD-120 [[286](#bib.bib286)] | Activities | RGBD, 3D Pose, Activity |  |
    x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CHUK Avenue [[287](#bib.bib287)] | Surveillance | RGB, BB, Anomaly, Temporal
    seg. | x |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Daimler Path [[288](#bib.bib288)] | Traffic | Stereo Grayscale, BB, Temporal
    seg. , Vehicle sensors |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Joint-annotated HMDB (JHMDB) [[289](#bib.bib289)] | Activities | RGB, Mask,
    Activity, Pose, Optical flow | x | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Penn Action [[290](#bib.bib290)] | Activities | RGB, BB, Pose, Activity |
    x |  |  | x |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | BIT [[291](#bib.bib291)] | Interaction | RGB, Activity |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze [[292](#bib.bib292)] | Cooking (ego) | RGB, Gaze, Mask, Activity,
    Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI [[293](#bib.bib293)] | Traffic | Stereo RGB, LIDAR, BB, Optical flow,
    Vehicle sensors | x |  | x |  | x |'
  prefs: []
  type: TYPE_TB
- en: '| MANIAC [[294](#bib.bib294)] | Object manipulation | RGBD, Semantic seg.,
    Activity |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MPII-Cooking [[295](#bib.bib295)] | Cooking | RGB, 3D Pose, Activity, Temporal
    seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MSR Daily Activity (MSRDA) [[296](#bib.bib296)] | Activities | Depth, Activity
    |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| New York Grand Central (GC) [[297](#bib.bib297)] | Surveillance | RGB, Traj.
    |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SBU Kinetic Interction (SBUKI) [[298](#bib.bib298)] | Interaction | RGBD,
    3D Pose, Activity |  | x | x | x |  |'
  prefs: []
  type: TYPE_TB
- en: '| UCF-101 [[299](#bib.bib299)] | Activities | RGB, Activity | x | x |  | x
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| UTKinect-Action (UTKA) [[300](#bib.bib300)] | Activities | RGBD, 3D Pose,
    Activity, Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| UvA-NEMO [[301](#bib.bib301)] | Smiles | RGB | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2011 | Ford campus vision LiDAR (FCVL) [[302](#bib.bib302)] | Traffic | RGB,
    LIDAR, Vehicle sensors |  |  |  |  | x |'
  prefs: []
  type: TYPE_TB
- en: '| Human Motion Database (HMDB) [[303](#bib.bib303)] | Activities | RGB, BB,
    Mask, Activity |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Stanford 40 [[304](#bib.bib304)] | Activities | RGB (image), BB, Activity
    |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Town Center [[305](#bib.bib305)] | Surveillance | RGB, BB |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| VIRAT [[306](#bib.bib306)] | Surveillance, Activities | RGB, BB, Activity,
    Temporal seg. |  | x | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2010 | DISPLECS [[307](#bib.bib307)] | Traffic | RGB, Vehicle sensors |  |  |  |  |
    x |'
  prefs: []
  type: TYPE_TB
- en: '| MSR [[308](#bib.bib308)] | Activities | Depth, Activity | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MUG [[309](#bib.bib309)] | Facial expressions | RGB, Keypoints, Label | x
    |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| PROST [[310](#bib.bib310)] | Objects | RGB, BB | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TV Human Interaction (THI) [[311](#bib.bib311)] | Interaction | RGB, BB,
    Head pose, Activity |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| UT Interaction (UTI) [[312](#bib.bib312)] | Interaction | RGB, BB, Activity,
    Temporal seg. |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| VISOR [[313](#bib.bib313)] | Surveillance | RGB, BB, Pose, Attrib. | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Willow Action [[314](#bib.bib314)] | Activiites | RGB (image), Activity |  |
    x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2009 | Caltech Pedestrian [[315](#bib.bib315)] | Traffic | RGB, BB | x |
    x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Collective Activity (CA) [[316](#bib.bib316)] | Interaction | RGB, BB, Attrib.,
    Activity, Temporal seg. |  | x | x | x |  |'
  prefs: []
  type: TYPE_TB
- en: '| Edinburgh IFP [[317](#bib.bib317)] | Surveillance | RGB, BB |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ETH [[235](#bib.bib235)] | Surveillance | RGB, Traj. |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| OSU [[318](#bib.bib318)] | Sports | RGB, BB, Attrib. |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| PETS2009 [[319](#bib.bib319)] | Surveillance | RGB, BB |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| QMUL [[320](#bib.bib320)] | Traffic, anomaly | RGB, Traj. |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TUM Kitchen [[321](#bib.bib321)] | Activities | RGB, RFID, 3D Pose, Activity,
    Temporal seg. |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| YUV Videos [[322](#bib.bib322)] | Mix videos | RGB | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2008 | Daimler [[323](#bib.bib323)] | Traffic | Grayscale, BB |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MIT Trajectory (MITT) [[324](#bib.bib324)] | Surveillance | RGB, Traj. |  |  |
    x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2007 | AMOS [[325](#bib.bib325)] | Weather | RGB, Temperature, Time |  |  |  |  |
    x |'
  prefs: []
  type: TYPE_TB
- en: '| ETH Pedestrian [[326](#bib.bib326)] | Traffic | RGB, BB |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Lankershim Boulevard [[327](#bib.bib327)] | Traffic | RGB, Traj. |  |  |
    x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Next Generation Simulation (NGSIM) [[328](#bib.bib328)] | Traffic | Map,
    Traj. |  | x | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| UCY [[329](#bib.bib329)] | Surveillance | RGB, Traj., Gaze |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2006 | Tuscan Arizona [[330](#bib.bib330)] | Weather | RGB |  |  |  |  |
    x |'
  prefs: []
  type: TYPE_TB
- en: '| 2004 | KTH [[331](#bib.bib331)] | Activities | Grayscale, Activity | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1981 | Golden Colorado [[332](#bib.bib332)] | Weather | RGB |  |  |  |  |
    x |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE II: A summary of common datasets from years 2013 and earlier used in
    vision-based prediction applications, namely video (V), action (A), trajectory
    (T), motion (M) and others (O). The annotation column specifies the type of data
    (e.g. RGB, Infrared(IR)) and annotation types. All datasets contain sequences
    unless specified by “image”. As for annotations, BB stands for bounding box. Attributes
    include any object characteristics (e.g. for pedestrians demographics, behavior).
    Vehicle sensors may include speed, steering angle, GPS, etc. Temporal seg. identifies
    the datasets that specify the start and end of the events.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/98db438ea0b637bb5c4683dc3b666846.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An illustration of datasets and papers that use them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have identified more than 100 datasets that are used in the vision-based
    prediction literature. Discussing all datasets in detail is beyond the scope of
    this paper. We provide a summary of the datasets and their characteristics in
    Tables [I](#S11.T1 "TABLE I ‣ 11 Datasets ‣ Deep Learning for Vision-based Prediction:
    A Survey") and [II](#S11.T2 "TABLE II ‣ 11 Datasets ‣ Deep Learning for Vision-based
    Prediction: A Survey") and briefly discuss more popular datasets in each field.
    Figure [2](#S11.F2 "Figure 2 ‣ 11 Datasets ‣ Deep Learning for Vision-based Prediction:
    A Survey") illustrates the list of papers and corresponding datasets used for
    evaluation. Note that the papers that do not use publicly available datasets are
    not listed in this figure. For further information, the readers can also refer
    to Appendices [D](#A4 "Appendix D Links to the datasets ‣ Deep Learning for Vision-based
    Prediction: A Survey") and [E](#A5 "Appendix E Datasets and corresponding papers
    ‣ Deep Learning for Vision-based Prediction: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Video prediction. Almost any forms of sequential RGB images can be used for
    evaluation of video prediction algorithms. Among the most common datasets are
    traffic datasets such a KITTI [[293](#bib.bib293)], and Caltech Pedestrians [[315](#bib.bib315)].
    KITTI is a dataset recorded from inside of a vehicle and contains images of urban
    roads annotated with bounding box information. It also contains depth maps, LIDAR
    point clouds and semantic segmentation maps. Caltech Pedestrian is a similar dataset
    with the difference of only containing RGB images and bounding boxes for pedestrians.
    It also contains occlusion bounding boxes highlighting the visible portions of
    the pedestrians. Activity datasets such as UCF-101 [[299](#bib.bib299)] and Human3.6M
    [[237](#bib.bib237)] are also widely used. UCF-101 contains videos of various
    types of activities such as sports, applying makeup, playing music instruments
    annotated with activity labels per video. Human3.6M consists of 3.6 million 3D
    human poses and corresponding images recorded from 11 professional actors. This
    dataset contains 17 generic scenarios such as discussion, smoking, and taking
    photos.
  prefs: []
  type: TYPE_NORMAL
- en: Action prediction. The algorithms in this domain are evaluated on a wide range
    of datasets. For anticipation tasks, traffic datasets such as Next Generation
    Simulation (NGSIM) [[328](#bib.bib328)] and Joint Attention in Autonomous Driving
    (JAAD) [[78](#bib.bib78)] are used. NGSIM contains trajectories of vehicles driving
    on highways in the United States. The trajectories are accompanied by the top-down
    views of the corresponding road structures. The JAAD dataset contains videos of
    pedestrians crossing the road recorded using an on-board camera. This dataset
    contains the frame-wise pedestrian bounding boxes, and action labels as well as
    pedestrians’ and roads’ attributes. A similar dataset to JAAD is Pedestrian Intention
    Estimation (PIE) [[144](#bib.bib144)] which, in addition, provides the ego-vehicle
    sensor data and spatial annotations for traffic elements.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular category of datasets in this domain is those containing videos
    of cooking activities. These datasets are Epic-Kitchen [[248](#bib.bib248)], 50salads
    [[284](#bib.bib284)], Breakfast [[280](#bib.bib280)] and MPII-Cooking [[295](#bib.bib295)].
    These datasets contain videos showing sequences of different cooking actions of
    preparing meals. All videos in the datasets have temporal segments with corresponding
    activity labels. Some datasets also provide additional annotations such as object
    bounding boxes, voice and text in Epic-Kitchen, and the poses of the actors in
    MPII-Cooking.
  prefs: []
  type: TYPE_NORMAL
- en: Early action prediction works widely use the popular UCF-101 dataset [[299](#bib.bib299)]
    and interaction datasets such as UT Interaction (UTI) [[312](#bib.bib312)] and
    BIT [[291](#bib.bib291)]. UTI and BIT contain videos of people engaged in interaction
    with the corresponding label for the types of interactions. In addition, UTI has
    the added temporal segment annotations detailing different stages of interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory prediction. The most common datasets in this domain are ETH [[235](#bib.bib235)]
    and UCY [[329](#bib.bib329)] which contain surveillance videos of pedestrians
    walking on sidewalks annotated with their position coordinates. UCY also provides
    the gaze directions to capture the viewing angle of pedestrians. Another popular
    dataset is Stanford Aerial Pedestrian (SAP), also known as Stanford Drone (SD)
    [[265](#bib.bib265)]. This dataset has the footage of road users from a top-down
    view recorded by a drone. The annotations include bounding boxes and object class
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Motion prediction. The algorithms in this domain are mainly evaluated on the
    widely popular dataset Human 3.6M [[237](#bib.bib237)] described earlier. This
    dataset is particularly suitable for these applications because it contains accurate
    3D poses of the actors recorded by a high-speed motion capture system. Using this
    dataset, the background can be accurately removed allowing the algorithms to focus
    purely on changes in the poses. Another popular dataset in this field is Penn
    Action [[290](#bib.bib290)] which contains RGB videos of various activities with
    corresponding activity labels and poses of the actors involved.
  prefs: []
  type: TYPE_NORMAL
- en: Other applications. The most notable datasets are KITTI [[293](#bib.bib293)]
    which is used by the OGM prediction algorithms and CityScapes [[261](#bib.bib261)]
    that is used by the segmentation prediction algorithms. CityScapes contains video
    footage of urban environments recorded by an on-board camera. The data is annotated
    with semantic masks of the traffic objects and corresponding bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 12 Summary and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 12.1 Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many factors that define the choice of architecture for vision-based
    prediction tasks. These factors include the types of input data and expected output,
    computational efficiency, application-specific constrains, etc. For instance,
    in terms of network choice, whether it is feedforward and recurrent, no preference
    is observed in video applications. However, in the case of action, trajectory
    and motion predictions, recurrent architectures are strongly preferred. This can
    be due to the fact that these applications often rely on multi-modal data which
    can be combined easier in a recurrent framework. In the case of trajectory prediction,
    recurrent architectures give the flexibility of varying observation and prediction
    lengths without the need for architectural modifications.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks (GANs) are widely used in video prediction applications
    and to some extent in trajectory prediction methods. Some of the major challenges
    using generative models is to deal with inherent uncertainty of future representations,
    in particular, this an issue in the context of trajectory prediction due to high
    unpredictability of human movement. To remedy this issue and to capture uncertainty
    of movement, techniques such as variational auto encoders, in which uncertainty
    is modeled as a latent distribution, and the use of probabilistic objective functions
    are common.
  prefs: []
  type: TYPE_NORMAL
- en: A more recent trend in the field of vision-based prediction (and perhaps in
    other computer vision applications) is the use of attention modules. These modules
    can be applied at spatial or temporal level or even to adjust the impact of different
    modalities of data.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Data representation and processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The type of data and methods of processing vary across different applications.
    For instance, video applications mainly rely on images but also take advantage
    of alternative representations, such as optical flow, poses, object-based keypoints,
    and report improved results. Similarly, many action prediction algorithms use
    different sources of information such as optical flow, poses, scene attributes,
    text, complementary sensor readings (e.g. speed in vehicles), gaze and time of
    the actions.
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory prediction algorithms, predominantly rely on trajectory information,
    with some exceptions that use scene layouts, complimentary sensors’ readings or
    other constrains. One of the main applications in this domain, in particular surveillance,
    is modeling the social interaction between the dynamic agents. Unlike other vision-based
    applications, motion prediction algorithms are mainly single-modal and use only
    poses and the images of agents as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 12.3.1 Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Metrics may vary across different applications of vision-based prediction. Video
    prediction algorithms, for instance, are mainly evaluated using MSE, SSIM, and
    PSNR, whereas in the case of action prediction algorithms the main metrics are
    accuracy, precision, and recall. Trajectory prediction works often measure the
    average distance (ADE) or final distance (FDE) between the actual and predicted
    locations of the agents. The models with probabilistic outputs are also evaluated
    using NLL and KLD metrics. Distance-based metrics are used in motion prediction
    methods where the error in joint prediction is either calculated on average (MJE)
    or per joint (MPJPE). In addition, joint accuracy can be reported in terms of
    the percentage of correct prediction using PCK metric. In this case, a tolerance
    threshold is defined to determine whether a predicted joint is correct.
  prefs: []
  type: TYPE_NORMAL
- en: While calculating performance error for video and action prediction algorithms
    are fairly standardized, there are major discrepancies across different works
    in the way error is computed for trajectory and motion prediction algorithms.
    For example, in trajectory prediction, distance error is calculated by using metrics
    such as MSE, RMSE, ED, etc. and units such as pixels and meters. Such discrepancy,
    and the fact that many works omit mentioning the choice of error metrics and units,
    increases the chance of incorrect comparisons between models.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The choice of datasets depends on the objective of the applications. For example,
    action prediction algorithms for cooking activities are evaluated on datasets
    such as Epic-Kitchen, 50Salads, Breakfast, and MPII-Cooking and the ones for traffic
    events evaluated on JAAD, NSGIM, and PIE. Similarly, trajectory prediction works
    for surveillance widely use UCY, ETH, and SD and for traffic NGSIM. Motion prediction
    algorithms are more focusing on individual movements in diverse context, therefore
    predominantly use Human3.6M and Penn Action datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to other applications, video prediction is an exception. The algorithms
    in this group are evaluated on almost any datasets with video content. The algorithms
    in this domain are often task agnostic meaning that the same approaches are evaluated
    on datasets with traffic scenes (e.g. KITTI, Caltech Pedestrian), general activities
    (e.g. UCF-101, Penn Action), basic actions (e.g. Human3.6M, KTH) and synthetic
    data (e.g. MMNIST, Atari games). Although such generalizability across different
    domains is a desirable feature in video prediction algorithms, it is often the
    case that the reason behind the choice of datasets is not discussed raising the
    question of whether the decision for selecting particular datasets is motivated
    by the limitations of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 What’s next
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years we have witnessed an exponential growth in the number of works
    published in the field of vision-based prediction. There are still, however, many
    open research problems in the field that need to be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to hallucinate or generate parts of the image that were not previously
    observed is still a major challenge in video prediction applications. In addition,
    the algorithms in this domain cannot deal with cases where some objects go out
    of view in future time frames. The performances of action prediction algorithms
    are still sub-optimal, especially in safety critical and complex tasks such as
    event prediction in traffic scenes. To make predictions in such cases, many modalities
    of data and the relationships between them should be considered which is often
    not the case in the proposed approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory prediction algorithms mainly rely on changes in the location of the
    agents to predict their future states. Although, this might be an effective approach
    for tasks such as surveillance, in many other cases it might not be sufficient.
    For example, in order to predict trajectories of pedestrians in traffic scenes,
    many other sources of information, such as their poses and orientation, road structure,
    interactions, road conditions, traffic flow, etc., are potentially relevant. Such
    contextual analysis can also be beneficial for motion prediction algorithms which
    manly rely on the changes in poses to predict the future.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the choice of learning architectures and training schemes, a systematic
    comparison of different approaches, e.g. using feedforward vs recurrent networks,
    the benefits of using adversarial training schemes, various uncertainty modeling
    approaches, etc. is missing. Such information can be partially extracted from
    the existing literature, however, in many cases it is not possible due to the
    lack of standard evaluation procedures and metrics, unavailability of corresponding
    implementation code and the datasets used for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] H. Gao, H. Xu, Q.-Z. Cai, R. Wang, F. Yu, and T. Darrell, “Disentangling
    propagation and generation for video prediction,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y.-H. Kwon and M.-G. Park, “Predicting future frames using retrospective
    cycle gan,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] L. Castrejon, N. Ballas, and A. Courville, “Improved conditional vrnns
    for video prediction,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Y.-H. Ho, C.-Y. Cho, W.-H. Peng, and G.-L. Jin, “Sme-net: Sparse motion
    estimation for parametric video prediction through reinforcement learning,” in
    *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] C. Zhang, T. Chen, H. Liu, Q. Shen, and Z. Ma, “Looking-ahead: Neural future
    video frame prediction,” in *ICIP*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] X. Liang, L. Lee, W. Dai, and E. P. Xing, “Dual motion gan for future-flow
    embedded video prediction,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. Ji, Z. Wei, E. Dunn, and J. M. Frahm, “Dynamic visual sequence prediction
    with motion flow networks,” in *WACV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] K.-H. Zeng, W. B. Shen, D.-A. Huang, M. Sun, and J. Carlos Niebles, “Visual
    forecasting by imitating dynamics in natural sequences,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] P. Gujjar and R. Vaughan, “Classifying pedestrian actions in advance using
    predicted video of urban driving scenes,” in *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Terwilliger, G. Brazil, and X. Liu, “Recurrent flow-guided semantic
    forecasting,” in *WACV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang, “Flow-grounded
    spatial-temporal video prediction from still images,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] F. A. Reda, G. Liu, K. J. Shih, R. Kirby, J. Barker, D. Tarjan, A. Tao,
    and B. Catanzaro, “Sdc-net: Video prediction using spatially-displaced convolution,”
    in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] P. Bhattacharjee and S. Das, “Predicting video frames using feature based
    locally guided objectives,” in *ACCV*, C. Jawahar, H. Li, G. Mori, and K. Schindler,
    Eds., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] G. Ying, Y. Zou, L. Wan, Y. Hu, and J. Feng, “Better guider predicts future
    better: Difference guided generative adversarial networks,” in *ACCV*, C. Jawahar,
    H. Li, G. Mori, and K. Schindler, Eds., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] P. Bhattacharjee and S. Das, “Temporal coherency based criteria for predicting
    video frames using deep multi-stage generative adversarial networks,” in *NeurIPS*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Oliu, J. Selva, and S. Escalera, “Folded recurrent neural networks
    for future video prediction,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y. Ye, M. Singh, A. Gupta, and S. Tulsiani, “Compositional video prediction,”
    in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Y. Kim, S. Nam, I. Cho, and S. J. Kim, “Unsupervised keypoint learning
    for guiding class-conditional video prediction,” in *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Wang, B. Hu, Y. Long, and Y. Guan, “Order matters: Shuffling sequence
    generation for video prediction,” in *BMVC*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Y. Ho, C. Cho, and W. Peng, “Deep reinforcement learning for video prediction,”
    in *ICIP*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. Tang, H. Hu, Q. Zhou, H. Shan, C. Tian, and T. Q. S. Quek, “Pose guided
    global and local gan for appearance preserving human video prediction,” in *ICIP*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Cai, C. Bai, Y.-W. Tai, and C.-K. Tang, “Deep video generation, prediction
    and completion of human action sequences,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J.-T. Hsieh, B. Liu, D.-A. Huang, L. F. Fei-Fei, and J. C. Niebles, “Learning
    to decompose and disentangle representations for video prediction,” in *NeurIPS*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Xu, B. Ni, and X. Yang, “Video prediction via selective sampling,”
    in *NeurIPS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] N. Wichers, R. Villegas, D. Erhan, and H. Lee, “Hierarchical long-term
    video prediction without supervision,” in *ICML*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. Walker, K. Marino, A. Gupta, and M. Hebert, “The pose knows: Video
    forecasting by generating pose futures,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Wang, M. Long, J. Wang, Z. Gao, and P. S. Yu, “Predrnn: Recurrent neural
    networks for predictive learning using spatiotemporal lstms,” in *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] R. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and H. Lee, “Learning to
    generate long-term future via hierarchical prediction,” in *ICML*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] C. Finn, I. Goodfellow, and S. Levine, “Unsupervised learning for physical
    interaction through video prediction,” in *NeurIPS*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh, “Action-conditional
    video prediction using deep networks in atari games,” in *NeurIPS*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Lee, J. Lee, S. Lee, and S. Yoon, “Mutual suppression network for video
    prediction using disentangled features,” in *BMVC*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J. Xu, B. Ni, Z. Li, S. Cheng, and X. Yang, “Structure preserving video
    prediction,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] W. Byeon, Q. Wang, R. Kumar Srivastava, and P. Koumoutsakos, “Contextvp:
    Fully context-aware video prediction,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] W. Liu, A. Sharma, O. Camps, and M. Sznaier, “Dyan: A dynamical atoms-based
    network for video prediction,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] B. Jin, Y. Hu, Y. Zeng, Q. Tang, S. Liu, and J. Ye, “Varnet: Exploring
    variations for unsupervised video prediction,” in *IROS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] C. Lu, M. Hirsch, and B. Scholkopf, “Flexible spatio-temporal networks
    for video prediction,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. Jung, T. Matsumoto, and J. Tani, “Goal-directed behavior under variational
    predictive coding: Dynamic organization of visual attention and working memory,”
    in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv:1312.6114*,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic backpropagation
    and approximate inference in deep generative models,” *arXiv:1401.4082*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. T. Schulz and R. Stiefelhagen, “Pedestrian intention recognition using
    latent-dynamic conditional random fields,” in *IV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J.-F. Hu, W.-S. Zheng, L. Ma, G. Wang, and J. Lai, “Real-time rgb-d activity
    prediction by soft regression,” in *ECCV*, B. Leibe, J. Matas, N. Sebe, and M. Welling,
    Eds., 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] F. Schneemann and P. Heinemann, “Context-based detection of pedestrian
    crossing intention for autonomous driving in urban environments,” in *IROS*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] B. Völz, K. Behrendt, H. Mielenz, I. Gilitschenski, R. Siegwart, and J. Nieto,
    “A data-driven approach for pedestrian intention estimation,” in *ITSC*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Z. Xu, L. Qing, and J. Miao, “Activity auto-completion: Predicting human
    activities from partial videos,” in *ICCV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] H. Zhang and L. E. Parker, “Bio-inspired predictive orientation decomposition
    of skeleton trajectories for real-time human activity prediction,” in *ICRA*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Köhler, M. Goldhammer, K. Zindler, K. Doll, and K. Dietmeyer, “Stereo-vision-based
    pedestrian’s intention detection in a moving vehicle,” in *ITSC*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] B. Völz, H. Mielenz, G. Agamennoni, and R. Siegwart, “Feature relevance
    estimation for learning pedestrian behavior at crosswalks,” in *ITSC*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] R. C. Luo and L. Mai, “Human intention inference and on-line human hand
    motion prediction for human-robot collaboration,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Wu, T. Louw, M. Lahijanian, W. Ruan, X. Huang, N. Merat, and M. Kwiatkowska,
    “Gaze-based intention anticipation over driving manoeuvres in semi-autonomous
    vehicles,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Rhinehart and K. M. Kitani, “First-person activity forecasting with
    online inverse reinforcement learning,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] J.-Y. Kwak, B. C. Ko, and J.-Y. Nam, “Pedestrian intention prediction
    based on dynamic fuzzy automata for vehicle driving at nighttime,” *Infrared Physics
    & Technology*, vol. 81, pp. 41–51, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] N. Hu, A. Bestick, G. Englebienne, R. Bajscy, and B. Kröse, “Human intent
    forecasting using intrinsic kinematic constraints,” in *IROS*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A. Jain, H. S. Koppula, B. Raghavan, S. Soh, and A. Saxena, “Car that
    knows before you do: Anticipating maneuvers via learning temporal driving models,”
    in *ICCV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] J. Hariyono, A. Shahbaz, L. Kurnianggoro, and K.-H. Jo, “Estimation of
    collision risk for improving driver’s safety,” in *IECON*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Hashimoto, Y. Gu, L.-T. Hsu, and S. Kamijo, “Probability estimation
    for pedestrian crossing intention at signalized crosswalks,” in *ICVES*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] H. Joo, T. Simon, M. Cikara, and Y. Sheikh, “Towards social artificial
    intelligence: Nonverbal social signal prediction in a triadic interaction,” in
    *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] F. Ziaeetabar, T. Kulvicius, M. Tamosiunaite, and F. Wörgötter, “Prediction
    of manipulation action classes using semantic spatial reasoning,” in *IROS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] S. Qi, S. Huang, P. Wei, and S.-C. Zhu, “Predicting human activities using
    stochastic grammar,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] C. Park, J. Ondřej, M. Gilbert, K. Freeman, and C. O’Sullivan, “Hi robot:
    Human intention-aware robot planning for safe and efficient navigation in crowds,”
    in *IROS*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] T. Mahmud, M. Hasan, A. Chakraborty, and A. K. Roy-Chowdhury, “A poisson
    process model for activity forecasting,” in *ICIP*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] K. Xu, Z. Qin, and G. Wang, “Human activities prediction by learning combinatorial
    sparse representations,” in *ICIP*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] C. Pérez-D’Arpino and J. A. Shah, “Fast target prediction of human reaching
    motion for cooperative human-robot manipulation tasks using time series classification,”
    in *ICRA*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Q. Ke, M. Fritz, and B. Schiele, “Time-conditioned action anticipation
    in one shot,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] A. Furnari and G. M. Farinella, “What would you expect? anticipating egocentric
    actions with rolling-unrolling lstms and modality attention,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] F. Sener and A. Yao, “Zero-shot anticipation for instructional activities,”
    in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Forecasting future
    action sequences with neural memory networks,” in *BMVC*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] E. Alati, L. Mauro, V. Ntouskos, and F. Pirri, “Help by predicting what
    to do,” in *ICIP*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] A. Furnari and G. M. Farinella, “Egocentric action anticipation by disentangling
    encoding and inference,” in *ICIP*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Abu Farha, A. Richard, and J. Gall, “When will you do what? - anticipating
    temporal occurrences of activities,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] T. Mahmud, M. Hasan, and A. K. Roy-Chowdhury, “Joint prediction of activity
    labels and starting times in untrimmed videos,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] A. Rasouli, I. Kotseruba, and J. K. Tsotsos, “Pedestrian action anticipation
    using contextual feature fusion in stacked rnns,” in *BMVC*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] W. Ding, J. Chen, and S. Shen, “Predicting vehicle behaviors over an extended
    horizon using behavior interaction network,” in *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] K. Saleh, M. Hossny, and S. Nahavandi, “Real-time intent prediction of
    pedestrians for autonomous ground vehicles via spatio-temporal densenet,” in *ICRA*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] O. Scheel, N. S. Nagaraja, L. Schwarz, N. Navab, and F. Tombari, “Attention-based
    lane change prediction,” in *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. S. Aliakbarian, F. S. Saleh, M. Salzmann, B. Fernando, L. Petersson,
    and L. Andersson, “Viena: A driving anticipation dataset,” in *ACCV*, C. V. Jawahar,
    H. Li, G. Mori, and K. Schindler, Eds., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] M. Strickland, G. Fainekos, and H. B. Amor, “Deep predictive models for
    collision risk assessment in autonomous driving,” in *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S. Casas, W. Luo, and R. Urtasun, “Intentnet: Learning to predict intention
    from raw sensor data,” in *CORL*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] A. Rasouli, I. Kotseruba, and J. K. Tsotsos, “Are they going to cross?
    a benchmark dataset and baseline for pedestrian crosswalk behavior,” in *ICCVW*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] F.-H. Chan, Y.-T. Chen, Y. Xiang, and M. Sun, “Anticipating accidents
    in dashcam videos,” in *ACCV*, S.-H. Lai, V. Lepetit, K. Nishino, and Y. Sato,
    Eds., 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Jain, A. Singh, H. S. Koppula, S. Soh, and A. Saxena, “Recurrent neural
    networks for driver activity anticipation via sensory-fusion architecture,” in
    *ICRA*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] A. Manglik, X. Weng, E. Ohn-Bar, and K. M. Kitani, “Forecasting time-to-collision
    from monocular video: Feasibility, dataset, and challenges,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] P. Wang, S. Lien, and M. Lee, “A learning-based prediction model for baby
    accidents,” in *ICIP*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] T. Suzuki, H. Kataoka, Y. Aoki, and Y. Satoh, “Anticipating traffic accidents
    with adaptive loss and large-scale incident db,” in *The CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] K.-H. Zeng, S.-H. Chou, F.-H. Chan, J. Carlos Niebles, and M. Sun, “Agent-centric
    risk assessment: Accident anticipation and risky region localization,” in *CVPR*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Su, J. Pyo Hong, J. Shi, and H. Soo Park, “Predicting behaviors of
    basketball players from first person videos,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] P. Felsen, P. Agrawal, and J. Malik, “What will happen next? forecasting
    player moves in sports videos,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] J. Liang, L. Jiang, J. C. Niebles, A. G. Hauptmann, and L. Fei-Fei, “Peeking
    into the future: Predicting future person activities and locations in videos,”
    in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C. Sun, A. Shrivastava, C. Vondrick, R. Sukthankar, K. Murphy, and C. Schmid,
    “Relational action forecasting,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. Shen, B. Ni, Z. Li, and N. Zhuang, “Egocentric activity prediction
    via event modulated attention,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] P. Schydlo, M. Rakovic, L. Jamone, and J. Santos-Victor, “Anticipation
    in human-robot cooperation: A recurrent neural network approach for multiple action
    sequences prediction,” in *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Zhong and W. Zheng, “Unsupervised learning for forecasting action representations,”
    in *ICIP*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Gao, Z. Yang, and R. Nevatia, “Red: Reinforced encoder-decoder networks
    for action anticipation,” in *BMVC*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] C. Vondrick, H. Pirsiavash, and A. Torralba, “Anticipating visual representations
    from unlabeled video,” in *CVPR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] H. Kataoka, Y. Miyashita, M. Hayashi, K. Iwata, and Y. Satoh, “Recognition
    of transitional action for short-term action prediction using discriminative temporal
    cnn feature,” in *BMVC*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Y. Zhou and T. L. Berg, “Temporal perception and prediction in ego-centric
    video,” in *ICCV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena, “Structural-rnn: Deep
    learning on spatio-temporal graphs,” in *CVPR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] O. Scheel, L. Schwarz, N. Navab, and F. Tombari, “Situation assessment
    for planning lane changes: Combining recurrent models and prediction,” in *ICRA*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] X. Wang, J.-F. Hu, J.-H. Lai, J. Zhang, and W.-S. Zheng, “Progressive
    teacher-student learning for early action prediction,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Predicting the future:
    A jointly learnt model for action anticipation,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Zhao and R. P. Wildes, “Spatiotemporal feature residual propagation
    for action prediction,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] T. Yao, M. Wang, B. Ni, H. Wei, and X. Yang, “Multiple granularity group
    interaction prediction,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. Shi, B. Fernando, and R. Hartley, “Action anticipation with rbf kernelized
    feature mapping rnn,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] J. Bütepage, H. Kjellström, and D. Kragic, “Anticipating many futures:
    Online human motion prediction and generation for human-robot interaction,” in
    *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Cho and H. Foroosh, “A temporal sequence learning for action recognition
    and prediction,” in *WACV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] M. Sadegh Aliakbarian, F. Sadat Saleh, M. Salzmann, B. Fernando, L. Petersson,
    and L. Andersson, “Encouraging lstms to anticipate actions very early,” in *ICCV*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] W. Li and M. Fritz, “Recognition of ongoing complex activities by sequence
    prediction over a hierarchical label space,” in *WACV*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] M. Safaei and H. Foroosh, “Still image action recognition by predicting
    spatial-temporal pixel evolution,” in *WACV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. Liu, A. Shahroudy, G. Wang, L.-Y. Duan, and A. C. Kot, “Ssnet: Scale
    selection network for online 3d action prediction,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] L. Chen, J. Lu, Z. Song, and J. Zhou, “Part-activated deep reinforcement
    learning for action prediction,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] J. Butepage, M. J. Black, D. Kragic, and H. Kjellstrom, “Deep representation
    learning for human motion prediction and classification,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Y. Kong, Z. Tao, and Y. Fu, “Deep sequential context networks for action
    prediction,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] G. Singh, S. Saha, M. Sapienza, P. H. S. Torr, and F. Cuzzolin, “Online
    real-time multiple spatiotemporal action localisation and prediction,” in *ICCV*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Dong-Gyu Lee and Seong-Whan Lee, “Human activity prediction based on
    sub-volume relationship descriptor,” in *ICPR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] J. F. Carvalho, M. Vejdemo-Johansson, F. T. Pokorny, and D. Kragic, “Long-term
    prediction of motion trajectories using path homology clusters,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Y. Yoo, K. Yun, S. Yun, J. Hong, H. Jeong, and J. Young Choi, “Visual
    path prediction in complex scenes with crowded moving objects,” in *CVPR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] A. Møgelmose, M. M. Trivedi, and T. B. Moeslund, “Trajectory analysis
    and prediction for improved pedestrian safety: Integrated framework and evaluations,”
    in *IV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Chenghui Zhou, B. Balle, and J. Pineau, “Learning time series models
    for pedestrian motion prediction,” in *ICRA*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] A. Rudenko, L. Palmieri, and K. O. Arras, “Joint long-term prediction
    of human motion using a planning-based social force approach,” in *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] A. Rudenko, L. Palmieri, A. J. Lilienthal, and K. O. Arras, “Human motion
    prediction under social grouping constraints,” in *IROS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Schulz, C. Hubmann, J. Löchner, and D. Burschka, “Interaction-aware
    probabilistic behavior prediction in urban environments,” in *IROS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M. Shen, G. Habibi, and J. P. How, “Transferable pedestrian motion prediction
    models at intersections,” in *IROS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] F. Shkurti and G. Dudek, “Topologically distinct trajectory predictions
    for probabilistic pursuit,” in *IROS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] D. Vasquez, “Novel planning-based algorithms for human motion prediction,”
    in *ICRA*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] V. Karasev, A. Ayvaci, B. Heisele, and S. Soatto, “Intent-aware long-term
    prediction of pedestrian motion,” in *ICRA*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] N. Lee and K. M. Kitani, “Predicting wide receiver trajectories in american
    football,” in *WACV*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] H. Bai, S. Cai, N. Ye, D. Hsu, and W. S. Lee, “Intention-aware online
    pomdp planning for autonomous driving in a crowd,” in *ICRA*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] S. Solaimanpour and P. Doshi, “A layered hmm for predicting motion of
    a leader in multi-robot settings,” in *ICRA*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Y. F. Chen, M. Liu, and J. P. How, “Augmented dictionary learning for
    motion prediction,” in *ICRA*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] R. Sanchez-Matilla and A. Cavallaro, “A predictor of moving objects for
    first-person vision,” in *ICIP*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] B. Lee, J. Choi, C. Baek, and B. Zhang, “Robust human following by deep
    bayesian trajectory prediction for home service robots,” in *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] I. Hasan, F. Setti, T. Tsesmelis, A. Del Bue, M. Cristani, and F. Galasso,
    ““seeing is believing”: Pedestrian trajectory forecasting using visual frustum
    of attention,” in *WACV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] L. Ballan, F. Castaldo, A. Alahi, F. Palmieri, and S. Savarese, “Knowledge
    transfer for scene-specific motion prediction,” in *ECCV*, B. Leibe, J. Matas,
    N. Sebe, and M. Welling, Eds., 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] M. Pfeiffer, U. Schwesinger, H. Sommer, E. Galceran, and R. Siegwart,
    “Predicting actions to act predictably: Cooperative partial motion planning with
    maximum entropy models,” in *IROS*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] N. N. Vo and A. F. Bobick, “Augmenting physical state prediction through
    structured activity inference,” in *ICRA*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] V. Akbarzadeh, C. Gagné, and M. Parizeau, “Kernel density estimation
    for target trajectory prediction,” in *IROS*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. Schulz and R. Stiefelhagen, “A controlled interactive multiple model
    filter for combined pedestrian intention recognition and path prediction,” in
    *ITSC*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,
    D. Wang, P. Carr, S. Lucey, D. Ramanan, and J. Hays, “Argoverse: 3d tracking and
    forecasting with rich maps,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Sadeghian, V. Kosaraju, A. Sadeghian, N. Hirose, H. Rezatofighi, and
    S. Savarese, “Sophie: An attentive gan for predicting paths compliant to social
    and physical constraints,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] P. Zhang, W. Ouyang, P. Zhang, J. Xue, and N. Zheng, “Sr-lstm: State
    refinement for lstm towards pedestrian trajectory prediction,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] T. Zhao, Y. Xu, M. Monfort, W. Choi, C. Baker, Y. Zhao, Y. Wang, and
    Y. N. Wu, “Multi-agent tensor fusion for contextual trajectory prediction,” in
    *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] H. Bi, Z. Fang, T. Mao, Z. Wang, and Z. Deng, “Joint prediction for kinematic
    trajectories in vehicle-pedestrian-mixed scenes,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C. Choi and B. Dariush, “Looking to relations for future trajectory forecast,”
    in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Y. Huang, H. Bi, Z. Li, T. Mao, and Z. Wang, “Stgat: Modeling spatial-temporal
    interactions for human trajectory prediction,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] A. Rasouli, I. Kotseruba, T. Kunic, and J. K. Tsotsos, “Pie: A large-scale
    dataset and models for pedestrian intention estimation and trajectory prediction,”
    in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] L. A. Thiede and P. P. Brahma, “Analyzing the variety loss in the context
    of probabilistic trajectory prediction,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] V. Kosaraju, A. Sadeghian, R. Martín-Martín, I. Reid, H. Rezatofighi,
    and S. Savarese, “Social-bigat: Multimodal trajectory forecasting using bicycle-gan
    and graph attention networks,” in *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] W. Ding and S. Shen, “Online vehicle trajectory prediction using policy
    anticipation network and optimization-based context reasoning,” in *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Li, H. Ma, and M. Tomizuka, “Interaction-aware multi-agent tracking
    and probabilistic behavior prediction via adversarial learning,” in *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] C. Anderson, X. Du, R. Vasudevan, and M. Johnson-Roberson, “Stochastic
    sampling simulation for pedestrian trajectory prediction,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. Srikanth, J. A. Ansari, S. Sharma *et al.*, “Infer: Intermediate representations
    for future prediction,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Y. Zhu, D. Qian, D. Ren, and H. Xia, “Starnet: Pedestrian trajectory
    prediction using deep neural network in star topology,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] H. Xue, D. Huynh, and M. Reynolds, “Location-velocity attention for pedestrian
    trajectory prediction,” in *WACV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, “Social
    gan: Socially acceptable trajectories with generative adversarial networks,” in
    *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] I. Hasan, F. Setti, T. Tsesmelis, A. Del Bue, F. Galasso, and M. Cristani,
    “Mx-lstm: Mixing tracklets and vislets to jointly forecast trajectories and head
    poses,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Y. Xu, Z. Piao, and S. Gao, “Encoding crowd interaction with deep neural
    network for pedestrian trajectory prediction,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] M. Pfeiffer, G. Paolo, H. Sommer, J. Nieto, R. Siegwart, and C. Cadena,
    “A data-driven model for interaction-aware pedestrian motion prediction in object
    cluttered environments,” in *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] E. Rehder, F. Wirth, M. Lauer, and C. Stiller, “Pedestrian prediction
    by planning using deep neural networks,” in *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] H. Xue, D. Q. Huynh, and M. Reynolds, “Ss-lstm: A hierarchical lstm model
    for pedestrian trajectory prediction,” in *WACV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] F. Bartoli, G. Lisanti, L. Ballan, and A. Del Bimbo, “Context-aware trajectory
    prediction,” in *ICPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese,
    “Social lstm: Human trajectory prediction in crowded spaces,” in *CVPR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] J. Hong, B. Sapp, and J. Philbin, “Rules of the road: Predicting driving
    behavior with a convolutional model of semantic interactions,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] N. Rhinehart, R. McAllister, K. Kitani, and S. Levine, “Precog: Prediction
    conditioned on goals in visual multi-agent settings,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] J. Li, H. Ma, and M. Tomizuka, “Conditional generative neural system
    for probabilistic trajectory prediction,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] T. Fernando, S. Denman, S. Sridharan, and C. Fookes, “Gd-gan: Generative
    adversarial networks for trajectory prediction and group detection in crowds,”
    in *ACCV*, C. V. Jawahar, H. Li, G. Mori, and K. Schindler, Eds., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] N. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. S. Torr, and M. Chandraker,
    “Desire: Distant future prediction in dynamic scenes with interacting agents,”
    in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] N. Rhinehart, K. M. Kitani, and P. Vernaza, “R2p2: A reparameterized
    pushforward policy for diverse, precise generative path forecasting,” in *ECCV*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] K.-R. Kim, W. Choi, Y. J. Koh, S.-G. Jeong, and C.-S. Kim, “Instance-level
    future motion estimation in a single image based on ordinal regression,” in *ICCV*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov, “Multipath: Multiple probabilistic
    anchor trajectory hypotheses for behavior prediction,” in *CoRL*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] H. Cui, V. Radosavljevic, F. Chou, T. Lin, T. Nguyen, T. Huang, J. Schneider,
    and N. Djuric, “Multimodal trajectory predictions for autonomous driving using
    deep convolutional networks,” in *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] X. Huang, S. G. McGill, B. C. Williams, L. Fletcher, and G. Rosman, “Uncertainty-aware
    driver trajectory prediction at urban intersections,” in *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] S. Zhou, M. J. Phielipp, J. A. Sefair, S. I. Walker, and H. B. Amor,
    “Clone swarms: Learning to predict and control multi-robot systems by imitation,”
    in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] A. Jain, S. Casas, R. Liao, Y. Xiong, S. Feng, S. Segal, and R. Urtasun,
    “Discrete residual flow for probabilistic pedestrian behavior prediction,” in
    *CoRL*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] U. Baumann, C. Guiser, M. Herman, and J. M. Zollner, “Predicting ego-vehicle
    paths from environmental observations with a deep neural network,” in *ICRA*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Y. Zhang, W. Wang, R. Bonatti, D. Maturana, and S. Scherer, “Integrating
    kinematics and environment context into deep inverse reinforcement learning for
    predicting off-road vehicle trajectories,” in *CoRL*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] W.-C. Ma, D.-A. Huang, N. Lee, and K. M. Kitani, “Forecasting interactive
    dynamics of pedestrians with fictitious play,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] S. Yi, H. Li, and X. Wang, “Pedestrian behavior understanding and prediction
    with deep neural networks,” in *ECCV*, B. Leibe, J. Matas, N. Sebe, and M. Welling,
    Eds., 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] R. Chandra, U. Bhattacharya, A. Bera, and D. Manocha, “Traphic: Trajectory
    prediction in dense and heterogeneous traffic using weighted interactions,” in
    *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Y. Li, “Which way are you going? imitative decision learning for path
    forecasting in dynamic scenes,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] W. Zhi, L. Ott, and F. Ramos, “Kernel trajectory maps for multi-modal
    probabilistic motion prediction,” in *CoRL*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] A. Vemula, K. Muelling, and J. Oh, “Social attention: Modeling attention
    in human crowds,” in *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] C. Tang, J. Chen, and M. Tomizuka, “Adaptive probabilistic vehicle trajectory
    prediction through physically feasible bayesian recurrent neural network,” in
    *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] K. Cho, T. Ha, G. Lee, and S. Oh, “Deep predictive autonomous driving
    using multi-agent joint trajectory prediction and traffic rules,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] A. Bhattacharyya, M. Fritz, and B. Schiele, “Long-term on-board prediction
    of people in traffic scenes under uncertainty,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] P. Felsen, P. Lucey, and S. Ganguly, “Where will they go? predicting
    fine-grained adversarial multi-agent motion using conditional variational autoencoders,”
    in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] S. Cao and R. Nevatia, “Forecasting human pose and motion with multibody
    dynamic model,” in *WACV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Y. Yuan and K. Kitani, “Ego-pose estimation and forecasting as real-time
    pd control,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] H. Chiu, E. Adeli, B. Wang, D. Huang, and J. C. Niebles, “Action-agnostic
    human pose forecasting,” in *WACV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] E. Wu and H. Koike, “Futurepose - mixed reality martial arts training
    using real-time 3d human pose forecasting with a rgb camera,” in *WACV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Y.-W. Chao, J. Yang, B. Price, S. Cohen, and J. Deng, “Forecasting human
    dynamics from static images,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] A. Gopalakrishnan, A. Mali, D. Kifer, L. Giles, and A. G. Ororbia, “A
    neural temporal model for human motion prediction,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] L.-Y. Gui, Y.-X. Wang, D. Ramanan, and J. M. F. Moura, “Few-shot human
    motion prediction via meta-learning,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] L.-Y. Gui, Y.-X. Wang, X. Liang, and J. M. F. Moura, “Adversarial geometry-aware
    human motion prediction,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] L. Gui, K. Zhang, Y. Wang, X. Liang, J. M. F. Moura, and M. Veloso, “Teaching
    robots to predict human motion,” in *IROS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] J. Martinez, M. J. Black, and J. Romero, “On human motion prediction
    using recurrent neural networks,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] K. Fragkiadaki, S. Levine, P. Felsen, and J. Malik, “Recurrent network
    models for human dynamics,” in *ICCV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] B. Wang, E. Adeli, H.-k. Chiu, D.-A. Huang, and J. C. Niebles, “Imitation
    learning for human pose prediction,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] W. Mao, M. Liu, M. Salzmann, and H. Li, “Learning trajectory dependencies
    for human motion prediction,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] A. Hernandez, J. Gall, and F. Moreno-Noguer, “Human motion prediction
    via spatio-temporal inpainting,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] J. Y. Zhang, P. Felsen, A. Kanazawa, and J. Malik, “Predicting 3d human
    dynamics from video,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] C. Talignani Landi, Y. Cheng, F. Ferraguti, M. Bonfe, C. Secchi, and
    M. Tomizuka, “Prediction of human arm target for robot reaching movements,” in
    *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] V. Guizilini, R. Senanayake, and F. Ramos, “Dynamic hilbert maps: Real-time
    occupancy predictions in changing environments,” in *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] D. Graves, K. Rezaee, and S. Scheideman, “Perception as prediction using
    general value functions in autonomous driving applications,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] O. Afolabi, K. Driggs–Campbell, R. Dong, M. J. Kochenderfer, and S. S.
    Sastry, “People as sensors: Imputing maps from human actions,” in *IROS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] G. N. Wilson, A. Ramirez-Serrano, and Q. Sun, “Vehicle state prediction
    for outdoor autonomous high-speed off-road ugvs,” in *ICRA*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] N. Mohajerin and M. Rohani, “Multi-step prediction of occupancy grid
    maps with recurrent neural networks,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] K. Katyal, K. Popek, C. Paxton, P. Burlina, and G. D. Hager, “Uncertainty-aware
    occupancy map prediction using generative networks for robot navigation,” in *ICRA*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] M. Schreiber, S. Hoermann, and K. Dietmayer, “Long-term occupancy grid
    prediction using recurrent neural networks,” in *ICRA*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] S. Hoermann, M. Bach, and K. Dietmayer, “Dynamic occupancy grid prediction
    for urban autonomous driving: A deep learning approach with fully automatic labeling,”
    in *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] S. Choi, K. Lee, and S. Oh, “Robust modeling and prediction in dynamic
    environments using recurrent flow networks,” in *IROS*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] P. Luc, C. Couprie, Y. LeCun, and J. Verbeek, “Predicting future instance
    segmentation by forecasting convolutional features,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] P. Luc, N. Neverova, C. Couprie, J. Verbeek, and Y. LeCun, “Predicting
    deeper into the future of semantic segmentation,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] X. Jin, H. Xiao, X. Shen, J. Yang, Z. Lin, Y. Chen, Z. Jie, J. Feng,
    and S. Yan, “Predicting scene parsing and motion dynamics in the future,” in *NeurIPS*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] S. Kim, H. Kim, J. Lee, S. Yoon, S. E. Kahou, K. Kashinath, and M. Prabhat,
    “Deep-hurricane-tracker: Tracking and forecasting extreme climate events,” in
    *WACV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] W. Chu, K. Ho, and A. Borji, “Visual weather temperature prediction,”
    in *WACV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] T. A. Siddiqui, S. Bharadwaj, and S. Kalyanaraman, “A deep learning approach
    to solar-irradiance forecasting in sky-videos,” in *WACV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] K. Wang, M. Bansal, and J. Frahm, “Retweet wars: Tweet popularity prediction
    via dynamic multimodal regression,” in *WACV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Z. Al-Halah, R. Stiefelhagen, and K. Grauman, “Fashion forward: Forecasting
    visual style in fashion,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] I. Sur and H. Ben Amor, “Robots that anticipate pain: Anticipating physical
    perturbations from visual cues through deep predictive models,” in *IROS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] R. Mottaghi, M. Rastegari, A. Gupta, and A. Farhadi, ““what happens if…”
    learning to predict the effect of forces in images,” in *ECCV*, B. Leibe, J. Matas,
    N. Sebe, and M. Welling, Eds., 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] J. Carvajal, A. Wiliem, C. Sanderson, and B. Lovell, “Towards miss universe
    automatic prediction: The evening gown competition,” in *ICPR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] J. Joo, F. F. Steen, and S.-C. Zhu, “Automated facial trait judgment
    and election outcome prediction: Social dimensions of face,” in *ICCV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] S. Lal, S. Duggal, and I. Sreedevi, “Online video summarization: Predicting
    future to better summarize present,” in *WACV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] W. Liu, W. Luo, D. Lian, and S. Gao, “Future frame prediction for anomaly
    detection – a new baseline,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] T. Fernando, S. Denman, S. Sridharan, and C. Fookes, “Tracking by prediction:
    A deep generative model for mutli-person localisation and tracking,” in *WACV*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] D. Jayaraman and K. Grauman, “Look-ahead before you leap: End-to-end
    active recognition by forecasting the effect of motion,” in *ECCV*, B. Leibe,
    J. Matas, N. Sebe, and M. Welling, Eds., 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] A. Dave, O. Russakovsky, and D. Ramanan, “Predictive-corrective networks
    for action detection,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Z. Yang, J. Gao, and R. Nevatia, “Spatio-temporal action detection with
    cascade proposal and location anticipation,” in *BMVC*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] M. Ziaeefard, R. Bergevin, and L.-P. Morency, “Time-slice prediction
    of dyadic human activities,” in *the British Machine Vision Conference (BMVC)*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. Metaxas, “Learning to forecast
    and refine residual motion for image-to-video generation,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli *et al.*, “Image
    quality assessment: from error visibility to structural similarity,” *IEEE transactions
    on image processing*, vol. 13, no. 4, pp. 600–612, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski,
    and S. Gelly, “Towards accurate generative models of video: A new metric & challenges,”
    *arXiv:1812.01717*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen,
    “Improved techniques for training gans,” in *NeurIPS*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] L. Sun, Z. Yan, S. M. Mellado, M. Hanheide, and T. Duckett, “3dof pedestrian
    trajectory prediction learned from long-term autonomous mobile robot deployment
    data,” in *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] S. Pellegrini, A. Ess, K. Schindler, and L. Van Gool, “You’ll never walk
    alone: Modeling social behavior for multi-target tracking,” in *ICCV*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Z. Liu, S. Wu, S. Jin, Q. Liu, S. Lu, R. Zimmermann, and L. Cheng, “Towards
    natural and accurate future motion prediction of humans and animals,” in *CVPR*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.6m: Large
    scale datasets and predictive methods for 3d human sensing in natural environments,”
    *PAMI*, vol. 36, no. 7, pp. 1325–1339, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] M. P. Zapf, M. Kawanabe, and L. Y. Morales Saiki, “Pedestrian density
    prediction for efficient mobile robot exploration,” in *IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] S. He, D. Kangin, Y. Mi, and N. Pugeault, “Aggregated sparse attention
    for steering angle prediction,” in *ICPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] A. Kanazawa, J. Y. Zhang, P. Felsen, and J. Malik, “Learning 3d human
    dynamics from video,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. Kummerle,
    H. Konigshof, C. Stiller, A. de La Fortelle *et al.*, “Interaction dataset: An
    international, adversarial and cooperative motion dataset in interactive driving
    scenarios with semantic maps,” *arXiv:1910.03088*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] A. Siarohin, S. Lathuilière, S. Tulyakov, E. Ricci, and N. Sebe, “Animating
    arbitrary objects via deep motion transfer,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous
    driving,” *arXiv:1903.11027*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] T. von Marcard, R. Henschel, M. Black, B. Rosenhahn, and G. Pons-Moll,
    “Recovering accurate 3d human pose in the wild using imus and a moving camera,”
    in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] G. Awad, A. Butt, K. Curtis, Y. Lee, J. Fiscus, A. Godil, D. Joy, A. Delgado,
    A. Smeaton, Y. Graham *et al.*, “Benchmarking video activity detection, video
    captioning and matching, video storytelling linking and video search,” in *TRECVID*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] P. Schydlo, M. Rakovic, L. Jamone, and J. Santos-Victor, “Anticipation
    in human-robot cooperation: A recurrent neural network approach for multiple action
    sequences prediction,” in *ICRA*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco, R. Sukthankar *et al.*, “Ava: A video dataset of spatio-temporally
    localized atomic visual actions,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “Scaling egocentric
    vision: The epic-kitchens dataset,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] Y. Li, M. Liu, and J. M. Rehg, “In the eye of beholder: Joint learning
    of gaze and actions in first person video,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] O. Groth, F. B. Fuchs, I. Posner, and A. Vedaldi, “Shapestacks: Learning
    vision-based physical intuition for generalised object stacking,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] L. Zhou, C. Xu, and J. J. Corso, “Towards automatic learning of procedures
    from web instructional videos,” in *AI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] S. Ma, S. A. Bargal, J. Zhang, L. Sigal, and S. Sclaroff, “Do less and
    achieve more: Training cnns for action recognition utilizing action images from
    the web,” *Pattern Recognition*, vol. 68, pp. 334–345, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] S. Zhang, R. Benenson, and B. Schiele, “Citypersons: A diverse dataset
    for pedestrian detection,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Z. Yan, T. Duckett, and N. Bellotto, “Online learning for human classification
    in 3d lidar-based tracking,” in *IROS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] C. Xu, L. N. Govindarajan, Y. Zhang, and L. Cheng, “Lie-x: Depth image
    based articulated object pose estimation, tracking, and action recognition on
    lie groups,” *IJCV*, vol. 123, no. 3, pp. 454–478, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year, 1000km: The
    Oxford RobotCar Dataset,” *IJRR*, vol. 36, no. 1, pp. 3–15, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] L. Chunhui, H. Yueyu, L. Yanghao, S. Sijie, and L. Jiaying, “Pku-mmd:
    A large scale benchmark for continuous multi-modal human action understanding,”
    *arXiv:1703.07475*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] A. Salvador, N. Hynes, Y. Aytar, J. Marin, F. Ofli, I. Weber, and A. Torralba,
    “Learning cross-modal embeddings for cooking recipes and food images,” in *CVPR*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] N. Hawes, C. Burbridge, F. Jovan, L. Kunze, B. Lacerda, L. Mudrova, J. Young,
    J. Wyatt, D. Hebesberger, T. Kortner *et al.*, “The strands project: Long-term
    autonomy in everyday environments,” *IEEE Robotics & Automation Magazine*, vol. 24,
    no. 3, pp. 146–156, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum, “A compositional
    object-based approach to learning physical dynamics,” *arXiv:1612.00341*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *CVPR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] CMU, “Cmu graphics lab motion capture database,” http://mocap.cs.cmu.edu/,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, “Ntu rgb+d: A large scale
    dataset for 3d human activity analysis,” in *CVPR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] Y. Li, C. Lan, J. Xing, W. Zeng, C. Yuan, and J. Liu, “Online human action
    detection using joint classification-regression recurrent neural networks,” *ECCV*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learning social
    etiquette: Human trajectory understanding in crowded scenes,” in *ECCV*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] R. De Geest, E. Gavves, A. Ghodrati, Z. Li, C. Snoek, and T. Tuytelaars,
    “Online action detection,” in *ECCV*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] T.-H. K. Huang, F. Ferraro, N. Mostafazadeh, I. Misra, J. Devlin, A. Agrawal,
    R. Girshick, X. He, P. Kohli, D. Batra *et al.*, “Visual storytelling,” in *NAACL*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan,
    and S. Vijayanarasimhan, “Youtube-8m: A large-scale video classification benchmark,”
    *arXiv:1609.08675*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] J. McAuley, C. Targett, Q. Shi, and A. Van Den Hengel, “Image-based recommendations
    on styles and substitutes,” in *SIGIR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] H. Joo, H. Liu, L. Tan, L. Gui, B. Nabbe, I. Matthews, T. Kanade, S. Nobuhara,
    and Y. Sheikh, “Panoptic studio: A massively multiview system for social motion
    capture,” in *ICCV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Y. Li, Z. Ye, and J. M. Rehg, “Delving into egocentric actions,” in *CVPR*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] S. Cappallo, T. Mensink, and C. G. Snoek, “Latent factors of visual popularity
    prediction,” in *ICMR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] L. Leal-Taixé, A. Milan, I. Reid, S. Roth, and K. Schindler, “Motchallenge
    2015: Towards a benchmark for multi-target tracking,” *arXiv:1504.01942*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] N. Srivastava, E. Mansimov, and R. Salakhudinov, “Unsupervised learning
    of video representations using lstms,” in *ICML*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene understanding
    benchmark suite,” in *CVPR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] J.-F. Hu, W.-S. Zheng, J. Lai, and J. Zhang, “Jointly learning heterogeneous
    features for rgb-d activity recognition,” in *CVPR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] A. Gorban, H. Idrees, Y.-G. Jiang, A. Roshan Zamir, I. Laptev, M. Shah,
    and R. Sukthankar, “THUMOS challenge: Action recognition with a large number of
    classes,” [http://www.thumos.info/](http://www.thumos.info/), 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] C. Wu, J. Zhang, S. Savarese, and A. Saxena, “Watch-n-patch: Unsupervised
    understanding of actions and relations,” in *CVPR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] Y. Xiong, K. Zhu, D. Lin, and X. Tang, “Recognize complex events from
    static images by fusing deep channels,” in *CVPR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] H. Kuehne, A. B. Arslan, and T. Serre, “The language of actions: Recovering
    the syntax and semantics of goal-directed human activities,” in *CVPR*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele, “2d human pose
    estimation: New benchmark and state of the art analysis,” in *CVPR*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] G. Yu, Z. Liu, and J. Yuan, “Discriminative orderlet mining for real-time
    recognition of human-object interaction,” in *ACCV*, D. Cremers, I. Reid, H. Saito,
    and M.-H. Yang, Eds., 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei,
    “Large-scale video classification with convolutional neural networks,” in *CVPR*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] S. Stein and S. J. McKenna, “Combining embedded accelerometers with computer
    vision for recognizing food preparation activities,” in *UbiComp*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] D. Brščić, T. Kanda, T. Ikeda, and T. Miyashita, “Person tracking in
    large public spaces using 3-d range sensors,” *Transactions on Human-Machine Systems*,
    vol. 43, no. 6, pp. 522–534, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] H. S. Koppula, R. Gupta, and A. Saxena, “Learning human activities and
    object affordances from rgb-d videos,” *IJRR*, vol. 32, no. 8, pp. 951–970, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] C. Lu, J. Shi, and J. Jia, “Abnormal event detection at 150 fps in matlab,”
    in *ICCV*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] N. Schneider and D. M. Gavrila, “Pedestrian path prediction with recursive
    bayesian filters: A comparative study,” in *GCPR*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, “Towards understanding
    action recognition,” in *ICCV*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] W. Zhang, M. Zhu, and K. G. Derpanis, “From actemes to action: A strongly-supervised
    representation for detailed action understanding,” in *ICCV*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] Y. Kong, Y. Jia, and Y. Fu, “Learning human interaction by interactive
    phrases,” in *ECCV*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] A. Fathi, Y. Li, and J. M. Rehg, “Learning to recognize daily actions
    using gaze,” in *ECCV*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *CVPR*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] A. Abramov, K. Pauwels, J. Papon, F. Wörgötter, and B. Dellen, “Depth-supported
    real-time video segmentation with the kinect,” in *WACV*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele, “A database for fine
    grained activity detection of cooking activities,” in *CVPR*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] J. Wang, Z. Liu, Y. Wu, and J. Yuan, “Mining actionlet ensemble for action
    recognition with depth cameras,” in *CVPR*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] B. Zhou, X. Wang, and X. Tang, “Understanding collective crowd behaviors:
    Learning a mixture model of dynamic pedestrian-agents,” in *CVPR*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] K. Yun, J. Honorio, D. Chattopadhyay, T. L. Berg, and D. Samaras, “Two-person
    interaction detection using body-pose features and multiple instance learning,”
    in *CVPRW*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101 human
    actions classes from videos in the wild,” *arXiv:1212.0402*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] L. Xia, C. Chen, and J. Aggarwal, “View invariant human action recognition
    using histograms of 3d joints,” in *CVPRW*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] H. Dibeklioğlu, A. A. Salah, and T. Gevers, “Are you really smiling at
    me? spontaneous versus posed enjoyment smiles,” in *ECCV*, A. Fitzgibbon, S. Lazebnik,
    P. Perona, Y. Sato, and C. Schmid, Eds., 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] G. Pandey, J. R. McBride, and R. M. Eustice, “Ford campus vision and
    lidar data set,” *The International Journal of Robotics Research (IJRR)*, vol. 30,
    no. 13, pp. 1543–1552, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, “HMDB: a large
    video database for human motion recognition,” in *ICCV*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] B. Yao, X. Jiang, A. Khosla, A. L. Lin, L. Guibas, and L. Fei-Fei, “Human
    action recognition by learning bases of action attributes and parts,” in *ICCV*,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] B. Benfold and I. Reid, “Stable multi-target tracking in real-time surveillance
    video,” in *CVPR*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee, S. Mukherjee,
    J. Aggarwal, H. Lee, L. Davis *et al.*, “A large-scale benchmark dataset for event
    recognition in surveillance video,” in *CVPR*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] N. Pugeault and R. Bowden, “Learning pre-attentive driving behaviour
    from holistic visual features,” in *ECCV*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] W. Li, Z. Zhang, and Z. Liu, “Action recognition based on a bag of 3d
    points,” in *CVPRW*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] N. Aifanti, C. Papachristou, and A. Delopoulos, “The mug facial expression
    database,” *WIAMIS*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] J. Santner, C. Leistner, A. Saffari, T. Pock, and H. Bischof, “Prost:
    Parallel robust online simple tracking,” in *CVPR*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] A. Patron-Perez, M. Marszalek, A. Zisserman, and I. D. Reid, “High five:
    Recognising human interactions in tv shows.” in *BMVC*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] M. S. Ryoo and J. K. Aggarwal, “UT-Interaction Dataset, ICPR contest
    on Semantic Description of Human Activities (SDHA),” 2010. [Online]. Available:
    [http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] R. Vezzani and R. Cucchiara, “Video surveillance online repository (visor):
    an integrated framework,” *Multimedia Tools and Applications*, vol. 50, no. 2,
    pp. 359–380, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] V. Delaitre, I. Laptev, and J. Sivic, “Recognizing human actions in still
    images: a study of bag-of-features and part-based representations,” in *BMVC*,
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] P. Dollár, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection:
    A benchmark,” in *CVPR*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] W. Choi, K. Shahid, and S. Savarese, “What are they doing? : Collective
    activity classification using spatio-temporal relationship among people,” in *ICCVW*,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] B. Majecka, “Statistical models of pedestrian behaviour in the forum,”
    Master’s thesis, School of Informatics, University of Edinburgh, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] R. Hess and A. Fern, “Discriminatively trained particle filters for complex
    multi-object tracking,” in *CVPR*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] J. Ferryman and A. Shahrokni, “Pets2009: Dataset and challenge,” in *PETS*,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] C. C. Loy, T. Xiang, and S. Gong, “Modelling multi-object activity by
    gaussian processes.” in *BMVC*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] M. Tenorth, J. Bandouch, and M. Beetz, “The tum kitchen data set of everyday
    manipulation activities for motion tracking and action recognition,” in *ICCVW*,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] A. V. T. Library, “YUV video sequences,” http://trace.kom.aau.dk/yuv/index.html,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] M. Enzweiler and D. M. Gavrila, “Monocular pedestrian detection: Survey
    and experiments,” *transactions on pattern analysis and machine intelligence (PAMI)*,
    vol. 31, no. 12, pp. 2179–2195, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] E. Grimson, X. Wang, G.-W. Ng, and K. T. Ma, “Trajectory analysis and
    semantic region modeling using a nonparametric bayesian model,” in *CVPR*, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] N. Jacobs, N. Roman, and R. Pless, “Consistent temporal variations in
    many outdoor scenes,” in *CVPR*, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] A. Ess, B. Leibe, and L. Van Gool, “Depth and appearance for mobile scene
    analysis,” in *ICCV*, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] U. D. of Transportation, “Lankershim boulevard dataset,” 2007\. [Online].
    Available: [https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm](https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] U. D. of Transporation, “Next generation simulation (ngsim),” Online,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] A. Lerner, Y. Chrysanthou, and D. Lischinski, “Crowds by example,” *Computer
    graphics forum*, vol. 26, no. 3, pp. 655–664, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] T. Pickering, “The mmt all-sky camera,” *Ground-based and Airborne Telescopes*,
    vol. 6267, p. 62671A, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions: a local
    svm approach,” in *ICPR*, vol. 3, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] T. Stoffel and A. Andreas, “NREL solar radiation research laboratory
    (srrl): Baseline measurement system (bms); golden, colorado (data),” National
    Renewable Energy Lab.(NREL), Tech. Rep., 1981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] Yale Song, J. Vallmitjana, A. Stent, and A. Jaimes, “Tvsum: Summarizing
    web videos using titles,” in *CVPR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Papers with code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| App | Paper | Link |'
  prefs: []
  type: TYPE_TB
- en: '| Video | [[19](#bib.bib19)] | [https://github.com/andrewjywang/SEENet](https://github.com/andrewjywang/SEENet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[11](#bib.bib11)] | [https://github.com/Yijunmaverick/FlowGrounded-VideoPrediction](https://github.com/Yijunmaverick/FlowGrounded-VideoPrediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[34](#bib.bib34)] | [https://github.com/liuem607/DYAN](https://github.com/liuem607/DYAN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[229](#bib.bib229)] | [https://github.com/garyzhao/FRGAN](https://github.com/garyzhao/FRGAN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[23](#bib.bib23)] | [https://github.com/jthsieh/DDPAE-video-prediction](https://github.com/jthsieh/DDPAE-video-prediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[24](#bib.bib24)] | [https://github.com/xjwxjw/VPSS](https://github.com/xjwxjw/VPSS)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[35](#bib.bib35)] | [https://github.com/jinbeibei/VarNet](https://github.com/jinbeibei/VarNet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[25](#bib.bib25)] | [https://bit.ly/2HqiHqx](https://bit.ly/2HqiHqx) |'
  prefs: []
  type: TYPE_TB
- en: '| [[27](#bib.bib27)] | [https://github.com/ujjax/pred-rnn](https://github.com/ujjax/pred-rnn)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[28](#bib.bib28)] | [https://github.com/rubenvillegas/icml2017hierchvid](https://github.com/rubenvillegas/icml2017hierchvid)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bib29)] | [https://github.com/tensorflow/models/tree/master/research/video_prediction](https://github.com/tensorflow/models/tree/master/research/video_prediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[30](#bib.bib30)] | [https://github.com/junhyukoh/nips2015-action-conditional-video-prediction](https://github.com/junhyukoh/nips2015-action-conditional-video-prediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Action | [[87](#bib.bib87)] | [https://github.com/google/next-prediction](https://github.com/google/next-prediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bib64)] | [https://github.com/fpv-iplab/rulstm](https://github.com/fpv-iplab/rulstm)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[71](#bib.bib71)] | [https://github.com/aras62/SF-GRU](https://github.com/aras62/SF-GRU)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[81](#bib.bib81)] | [https://github.com/aashi7/NearCollision](https://github.com/aashi7/NearCollision)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] | [https://github.com/yabufarha/anticipating-activities](https://github.com/yabufarha/anticipating-activities)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[112](#bib.bib112)] | [https://github.com/gurkirt/realtime-action-detection](https://github.com/gurkirt/realtime-action-detection)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[80](#bib.bib80)] | [https://github.com/asheshjain399/RNNexp](https://github.com/asheshjain399/RNNexp)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[95](#bib.bib95)] | [https://github.com/aditya7874/Activity-Prediction-in-EgoCentric-Videos](https://github.com/aditya7874/Activity-Prediction-in-EgoCentric-Videos)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[100](#bib.bib100)] | [https://github.com/JoeHEZHAO/Spatiotemporal-Residual-Propagation](https://github.com/JoeHEZHAO/Spatiotemporal-Residual-Propagation)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Trajectory | [[177](#bib.bib177)] | [https://go.umd.edu/TraPHic](https://go.umd.edu/TraPHic)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[87](#bib.bib87)] | [https://github.com/google/next-prediction](https://github.com/google/next-prediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[139](#bib.bib139)] | [https://github.com/zhangpur/SR-LSTM](https://github.com/zhangpur/SR-LSTM)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[144](#bib.bib144)] | [https://github.com/aras62/PIEPredict](https://github.com/aras62/PIEPredict)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[162](#bib.bib162)] | [https://sites.google.com/view/precog](https://sites.google.com/view/precog)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[150](#bib.bib150)] | [https://rebrand.ly/INFER-results](https://rebrand.ly/INFER-results)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[179](#bib.bib179)] | [https://github.com/wzhi/KernelTrajectoryMaps](https://github.com/wzhi/KernelTrajectoryMaps)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[183](#bib.bib183)] | [https://github.com/apratimbhattacharyya18/onboard_long_term_prediction](https://github.com/apratimbhattacharyya18/onboard_long_term_prediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[153](#bib.bib153)] | [https://github.com/agrimgupta92/sgan](https://github.com/agrimgupta92/sgan)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[155](#bib.bib155)] | [https://github.com/svip-lab/CIDNN](https://github.com/svip-lab/CIDNN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[174](#bib.bib174)] | [https://github.com/yfzhang/vehicle-motion-forecasting](https://github.com/yfzhang/vehicle-motion-forecasting)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[165](#bib.bib165)] | [https://github.com/yadrimz/DESIRE](https://github.com/yadrimz/DESIRE)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[160](#bib.bib160)] | [https://github.com/quancore/social-lstm](https://github.com/quancore/social-lstm)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Motion | [[190](#bib.bib190)] | [https://github.com/cr7anand/neural_temporal_models](https://github.com/cr7anand/neural_temporal_models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[236](#bib.bib236)] | [https://github.com/BII-wushuang/Lie-Group-Motion-Prediction](https://github.com/BII-wushuang/Lie-Group-Motion-Prediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[197](#bib.bib197)] | [https://github.com/wei-mao-2019/LearnTrajDep](https://github.com/wei-mao-2019/LearnTrajDep)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[198](#bib.bib198)] | [https://github.com/magnux/MotionGAN](https://github.com/magnux/MotionGAN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[186](#bib.bib186)] | [https://github.com/Khrylx/EgoPose](https://github.com/Khrylx/EgoPose)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[199](#bib.bib199)] | [https://jasonyzhang.com/phd/](https://jasonyzhang.com/phd/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[187](#bib.bib187)] | [https://github.com/eddyhkchiu/pose_forecast_wacv/.](https://github.com/eddyhkchiu/pose_forecast_wacv/.)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[189](#bib.bib189)] | [https://github.com/ywchao/image-play](https://github.com/ywchao/image-play)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[194](#bib.bib194)] | [https://github.com/una-dinosauria/human-motion-prediction](https://github.com/una-dinosauria/human-motion-prediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Others | [[201](#bib.bib201)] | [https://bitbucket.org/vguizilini/cvpp/src](https://bitbucket.org/vguizilini/cvpp/src)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[211](#bib.bib211)] | [https://github.com/facebookresearch/SegmPred](https://github.com/facebookresearch/SegmPred)
    |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: A summary of vision-based prediction papers with published code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of papers with official published code can be found in Table [III](#A1.T3
    "TABLE III ‣ Appendix A Papers with code ‣ Deep Learning for Vision-based Prediction:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Metrics and corresponding papers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Metric | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BCE | [[23](#bib.bib23)] |'
  prefs: []
  type: TYPE_TB
- en: '| FVD | [[3](#bib.bib3)],[[18](#bib.bib18)] |'
  prefs: []
  type: TYPE_TB
- en: '| Human | [[11](#bib.bib11)],[[25](#bib.bib25)],[[8](#bib.bib8)],[[28](#bib.bib28)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| IS | [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: '| L1 | [[9](#bib.bib9)],[[12](#bib.bib12)] |'
  prefs: []
  type: TYPE_TB
- en: '| LPIPS | [[3](#bib.bib3)],[[17](#bib.bib17)],[[37](#bib.bib37)],[[11](#bib.bib11)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| MMD | [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: '| MSE | [[2](#bib.bib2)],[[4](#bib.bib4)],[[20](#bib.bib20)],[[34](#bib.bib34)],[[16](#bib.bib16)],
    [[12](#bib.bib12)],[[229](#bib.bib229)],[[23](#bib.bib23)],[[14](#bib.bib14)],[[7](#bib.bib7)],
    [[6](#bib.bib6)],[[27](#bib.bib27)],[[30](#bib.bib30)] |'
  prefs: []
  type: TYPE_TB
- en: '| PSNR | [[2](#bib.bib2)],[[1](#bib.bib1)],[[4](#bib.bib4)],[[31](#bib.bib31)],[[19](#bib.bib19)],
    [[20](#bib.bib20)],[[21](#bib.bib21)],[[5](#bib.bib5)],[[32](#bib.bib32)],[[33](#bib.bib33)],
    [[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[12](#bib.bib12)],[[229](#bib.bib229)],
    [[24](#bib.bib24)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[36](#bib.bib36)],
    [[6](#bib.bib6)],[[15](#bib.bib15)],[[27](#bib.bib27)],[[28](#bib.bib28)],[[29](#bib.bib29)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| RMSE | [[11](#bib.bib11)] |'
  prefs: []
  type: TYPE_TB
- en: '| SSIM | [[2](#bib.bib2)],[[3](#bib.bib3)],[[1](#bib.bib1)],[[4](#bib.bib4)],[[31](#bib.bib31)],
    [[19](#bib.bib19)],[[20](#bib.bib20)],[[21](#bib.bib21)],[[5](#bib.bib5)],[[32](#bib.bib32)],
    [[33](#bib.bib33)],[[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[12](#bib.bib12)],
    [[24](#bib.bib24)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[6](#bib.bib6)],
    [[15](#bib.bib15)],[[27](#bib.bib27)],[[29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Metrics used in video prediction applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AP | [[88](#bib.bib88)],[[9](#bib.bib9)],[[73](#bib.bib73)],[[82](#bib.bib82)],[[79](#bib.bib79)],
    [[88](#bib.bib88)] |'
  prefs: []
  type: TYPE_TB
- en: '| ATTA | [[82](#bib.bib82)] |'
  prefs: []
  type: TYPE_TB
- en: '| ATTC | [[83](#bib.bib83)] |'
  prefs: []
  type: TYPE_TB
- en: '| AUC | [[71](#bib.bib71)],[[54](#bib.bib54)],[[112](#bib.bib112)] |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | [[56](#bib.bib56)],[[63](#bib.bib63)],[[64](#bib.bib64)],[[66](#bib.bib66)],[[71](#bib.bib71)],
    [[9](#bib.bib9)],[[74](#bib.bib74)],[[48](#bib.bib48)],[[49](#bib.bib49)],[[67](#bib.bib67)],
    [[68](#bib.bib68)],[[69](#bib.bib69)],[[83](#bib.bib83)],[[89](#bib.bib89)],[[75](#bib.bib75)],
    [[97](#bib.bib97)],[[90](#bib.bib90)],[[76](#bib.bib76)],[[91](#bib.bib91)],[[85](#bib.bib85)],
    [[86](#bib.bib86)],[[70](#bib.bib70)],[[50](#bib.bib50)],[[8](#bib.bib8)],[[93](#bib.bib93)],
    [[94](#bib.bib94)],[[52](#bib.bib52)],[[59](#bib.bib59)],[[42](#bib.bib42)],[[54](#bib.bib54)],
    [[43](#bib.bib43)],[[95](#bib.bib95)],[[55](#bib.bib55)],[[46](#bib.bib46)],[[40](#bib.bib40)],
    [[47](#bib.bib47)],[[98](#bib.bib98)],[[99](#bib.bib99)],[[100](#bib.bib100)],[[107](#bib.bib107)],
    [[108](#bib.bib108)],[[101](#bib.bib101)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[103](#bib.bib103)],
    [[104](#bib.bib104)],[[110](#bib.bib110)],[[111](#bib.bib111)],[[105](#bib.bib105)],[[112](#bib.bib112)],[[41](#bib.bib41)],[[106](#bib.bib106)],[[61](#bib.bib61)],[[113](#bib.bib113)],[[44](#bib.bib44)],
    [[62](#bib.bib62)],[[45](#bib.bib45)] |'
  prefs: []
  type: TYPE_TB
- en: '| F1 | [[71](#bib.bib71)],[[72](#bib.bib72)],[[9](#bib.bib9)],[[83](#bib.bib83)],[[90](#bib.bib90)],[[58](#bib.bib58)],[[96](#bib.bib96)],[[52](#bib.bib52)],[[42](#bib.bib42)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| FP | [[53](#bib.bib53)] |'
  prefs: []
  type: TYPE_TB
- en: '| MAE | [[81](#bib.bib81)] |'
  prefs: []
  type: TYPE_TB
- en: '| MCC | [[76](#bib.bib76)] |'
  prefs: []
  type: TYPE_TB
- en: '| MRR | [[44](#bib.bib44)] |'
  prefs: []
  type: TYPE_TB
- en: '| PP | [[57](#bib.bib57)] |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | [[63](#bib.bib63)],[[71](#bib.bib71)],[[72](#bib.bib72)],[[9](#bib.bib9)],[[74](#bib.bib74)],
    [[67](#bib.bib67)],[[82](#bib.bib82)],[[83](#bib.bib83)],[[70](#bib.bib70)],[[58](#bib.bib58)],
    [[51](#bib.bib51)],[[96](#bib.bib96)],[[80](#bib.bib80)],[[52](#bib.bib52)],[[42](#bib.bib42)],
    [[53](#bib.bib53)] |'
  prefs: []
  type: TYPE_TB
- en: '| RMSE | [[70](#bib.bib70)],[[43](#bib.bib43)],[[60](#bib.bib60)] |'
  prefs: []
  type: TYPE_TB
- en: '| Recall | [[63](#bib.bib63)],[[64](#bib.bib64)],[[65](#bib.bib65)],[[71](#bib.bib71)],[[9](#bib.bib9)],
    [[74](#bib.bib74)],[[67](#bib.bib67)],[[68](#bib.bib68)],[[82](#bib.bib82)],[[83](#bib.bib83)],
    [[77](#bib.bib77)],[[70](#bib.bib70)],[[51](#bib.bib51)],[[96](#bib.bib96)],[[80](#bib.bib80)],
    [[52](#bib.bib52)],[[42](#bib.bib42)],[[53](#bib.bib53)] |'
  prefs: []
  type: TYPE_TB
- en: '| Run time | [[9](#bib.bib9)],[[73](#bib.bib73)] |'
  prefs: []
  type: TYPE_TB
- en: '| TNR | [[47](#bib.bib47)] |'
  prefs: []
  type: TYPE_TB
- en: '| TPR | [[47](#bib.bib47)] |'
  prefs: []
  type: TYPE_TB
- en: '| TTA | [[84](#bib.bib84)] |'
  prefs: []
  type: TYPE_TB
- en: '| TTM | [[74](#bib.bib74)],[[49](#bib.bib49)],[[96](#bib.bib96)],[[80](#bib.bib80)],[[53](#bib.bib53)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| cAP | [[92](#bib.bib92)] |'
  prefs: []
  type: TYPE_TB
- en: '| mAP | [[87](#bib.bib87)],[[83](#bib.bib83)],[[77](#bib.bib77)],[[84](#bib.bib84)],[[78](#bib.bib78)],[[92](#bib.bib92)],[[107](#bib.bib107)],[[112](#bib.bib112)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| recall | [[72](#bib.bib72)],[[58](#bib.bib58)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Metrics used in action prediction applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ADE | [[177](#bib.bib177)],[[161](#bib.bib161)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],
    [[140](#bib.bib140)],[[141](#bib.bib141)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[144](#bib.bib144)],[[146](#bib.bib146)],[[169](#bib.bib169)],[[170](#bib.bib170)],[[148](#bib.bib148)],[[149](#bib.bib149)],
    [[182](#bib.bib182)],[[163](#bib.bib163)],[[150](#bib.bib150)],[[171](#bib.bib171)],[[151](#bib.bib151)],
    [[168](#bib.bib168)],[[172](#bib.bib172)],[[179](#bib.bib179)],[[152](#bib.bib152)],[[129](#bib.bib129)],[[183](#bib.bib183)],[[153](#bib.bib153)],[[154](#bib.bib154)],[[155](#bib.bib155)],[[101](#bib.bib101)],
    [[184](#bib.bib184)],[[164](#bib.bib164)],[[234](#bib.bib234)],[[180](#bib.bib180)],[[121](#bib.bib121)],
    [[174](#bib.bib174)],[[131](#bib.bib131)],[[158](#bib.bib158)],[[159](#bib.bib159)],[[127](#bib.bib127)],[[160](#bib.bib160)],[[115](#bib.bib115)],[[132](#bib.bib132)],[[176](#bib.bib176)],[[123](#bib.bib123)],
    [[117](#bib.bib117)],[[136](#bib.bib136)] |'
  prefs: []
  type: TYPE_TB
- en: '| AEDE | [[234](#bib.bib234)] |'
  prefs: []
  type: TYPE_TB
- en: '| ANDE | [[155](#bib.bib155)],[[160](#bib.bib160)] |'
  prefs: []
  type: TYPE_TB
- en: '| APP | [[157](#bib.bib157)] |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | [[167](#bib.bib167)],[[127](#bib.bib127)] |'
  prefs: []
  type: TYPE_TB
- en: '| CE | [[166](#bib.bib166)] |'
  prefs: []
  type: TYPE_TB
- en: '| DtG | [[125](#bib.bib125)] |'
  prefs: []
  type: TYPE_TB
- en: '| ECE | [[172](#bib.bib172)] |'
  prefs: []
  type: TYPE_TB
- en: '| ED | [[114](#bib.bib114)],[[173](#bib.bib173)],[[130](#bib.bib130)],[[156](#bib.bib156)],[[165](#bib.bib165)],[[124](#bib.bib124)],[[133](#bib.bib133)],[[134](#bib.bib134)],[[135](#bib.bib135)],[[136](#bib.bib136)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| FDE | [[177](#bib.bib177)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],
    [[141](#bib.bib141)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[144](#bib.bib144)],[[146](#bib.bib146)],
    [[169](#bib.bib169)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[168](#bib.bib168)],
    [[172](#bib.bib172)],[[179](#bib.bib179)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[154](#bib.bib154)],
    [[155](#bib.bib155)],[[164](#bib.bib164)],[[180](#bib.bib180)],[[131](#bib.bib131)],[[158](#bib.bib158)],
    [[160](#bib.bib160)],[[176](#bib.bib176)] |'
  prefs: []
  type: TYPE_TB
- en: '| FNM | [[126](#bib.bib126)] |'
  prefs: []
  type: TYPE_TB
- en: '| Hit rate | [[161](#bib.bib161)],[[128](#bib.bib128)] |'
  prefs: []
  type: TYPE_TB
- en: '| KLD | [[181](#bib.bib181)],[[127](#bib.bib127)],[[125](#bib.bib125)] |'
  prefs: []
  type: TYPE_TB
- en: '| L1 | [[77](#bib.bib77)] |'
  prefs: []
  type: TYPE_TB
- en: '| LL | [[145](#bib.bib145)],[[181](#bib.bib181)],[[168](#bib.bib168)],[[120](#bib.bib120)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| MAnE | [[131](#bib.bib131)] |'
  prefs: []
  type: TYPE_TB
- en: '| MHD | [[118](#bib.bib118)],[[119](#bib.bib119)],[[125](#bib.bib125)] |'
  prefs: []
  type: TYPE_TB
- en: '| MSE | [[77](#bib.bib77)],[[125](#bib.bib125)] |'
  prefs: []
  type: TYPE_TB
- en: '| Miss rate | [[184](#bib.bib184)],[[165](#bib.bib165)] |'
  prefs: []
  type: TYPE_TB
- en: '| NLL | [[172](#bib.bib172)],[[183](#bib.bib183)],[[174](#bib.bib174)],[[175](#bib.bib175)],[[123](#bib.bib123)],[[125](#bib.bib125)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| NLP | [[118](#bib.bib118)],[[119](#bib.bib119)] |'
  prefs: []
  type: TYPE_TB
- en: '| None | [[116](#bib.bib116)] |'
  prefs: []
  type: TYPE_TB
- en: '| PD | [[122](#bib.bib122)] |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | [[115](#bib.bib115)] |'
  prefs: []
  type: TYPE_TB
- en: '| RMSE | [[140](#bib.bib140)],[[147](#bib.bib147)] |'
  prefs: []
  type: TYPE_TB
- en: '| Run time | [[147](#bib.bib147)],[[118](#bib.bib118)],[[121](#bib.bib121)],[[123](#bib.bib123)],[[135](#bib.bib135)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| SCR | [[175](#bib.bib175)] |'
  prefs: []
  type: TYPE_TB
- en: '| WRMSE | [[120](#bib.bib120)] |'
  prefs: []
  type: TYPE_TB
- en: '| maxD | [[184](#bib.bib184)],[[165](#bib.bib165)] |'
  prefs: []
  type: TYPE_TB
- en: '| meanMSD | [[162](#bib.bib162)],[[166](#bib.bib166)] |'
  prefs: []
  type: TYPE_TB
- en: '| minADE | [[137](#bib.bib137)],[[178](#bib.bib178)],[[149](#bib.bib149)],[[168](#bib.bib168)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| minED | [[161](#bib.bib161)],[[165](#bib.bib165)] |'
  prefs: []
  type: TYPE_TB
- en: '| minFDE | [[137](#bib.bib137)],[[178](#bib.bib178)] |'
  prefs: []
  type: TYPE_TB
- en: '| minMSD | [[162](#bib.bib162)],[[166](#bib.bib166)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: Metrics used in trajectory prediction applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | [[48](#bib.bib48)],[[195](#bib.bib195)] |'
  prefs: []
  type: TYPE_TB
- en: '| Human | [[198](#bib.bib198)],[[192](#bib.bib192)] |'
  prefs: []
  type: TYPE_TB
- en: '| LO | [[56](#bib.bib56)] |'
  prefs: []
  type: TYPE_TB
- en: '| MAnE | [[190](#bib.bib190)],[[197](#bib.bib197)],[[198](#bib.bib198)],[[196](#bib.bib196)],[[191](#bib.bib191)],[[192](#bib.bib192)],[[193](#bib.bib193)],[[194](#bib.bib194)],[[96](#bib.bib96)],[[195](#bib.bib195)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| MJE | [[56](#bib.bib56)],[[236](#bib.bib236)],[[186](#bib.bib186)],[[200](#bib.bib200)],[[187](#bib.bib187)],[[188](#bib.bib188)],[[101](#bib.bib101)],[[103](#bib.bib103)],[[110](#bib.bib110)],[[26](#bib.bib26)],[[185](#bib.bib185)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| MPJPE | [[197](#bib.bib197)],[[199](#bib.bib199)] |'
  prefs: []
  type: TYPE_TB
- en: '| NPSS | [[190](#bib.bib190)] |'
  prefs: []
  type: TYPE_TB
- en: '| PCK | [[199](#bib.bib199)],[[187](#bib.bib187)],[[188](#bib.bib188)],[[189](#bib.bib189)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| PSEnt | [[198](#bib.bib198)] |'
  prefs: []
  type: TYPE_TB
- en: '| PSKL | [[198](#bib.bib198)] |'
  prefs: []
  type: TYPE_TB
- en: '| RE | [[199](#bib.bib199)] |'
  prefs: []
  type: TYPE_TB
- en: '| Run time | [[236](#bib.bib236)],[[188](#bib.bib188)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VII: Metrics used in motion prediction applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lists of metrics and corresponding papers can be found in Tables [IV](#A2.T4
    "TABLE IV ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey"), [V](#A2.T5 "TABLE V ‣ Appendix B Metrics and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey"), [VI](#A2.T6 "TABLE
    VI ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey"), [VII](#A2.T7 "TABLE VII ‣ Appendix B Metrics and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey"), and [VIII](#A2.T8
    "TABLE VIII ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for
    Vision-based Prediction: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AUC | [[209](#bib.bib209)] |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | [[8](#bib.bib8)],[[220](#bib.bib220)],[[221](#bib.bib221)] |'
  prefs: []
  type: TYPE_TB
- en: '| ED | [[202](#bib.bib202)],[[238](#bib.bib238)] |'
  prefs: []
  type: TYPE_TB
- en: '| EPE | [[212](#bib.bib212)] |'
  prefs: []
  type: TYPE_TB
- en: '| F1 | [[201](#bib.bib201)],[[207](#bib.bib207)] |'
  prefs: []
  type: TYPE_TB
- en: '| GCE | [[210](#bib.bib210)] |'
  prefs: []
  type: TYPE_TB
- en: '| ISM | [[203](#bib.bib203)] |'
  prefs: []
  type: TYPE_TB
- en: '| IoU | [[10](#bib.bib10)],[[210](#bib.bib210)],[[211](#bib.bib211)] |'
  prefs: []
  type: TYPE_TB
- en: '| MAE | [[239](#bib.bib239)],[[217](#bib.bib217)],[[204](#bib.bib204)] |'
  prefs: []
  type: TYPE_TB
- en: '| MAPE | [[216](#bib.bib216)],[[217](#bib.bib217)] |'
  prefs: []
  type: TYPE_TB
- en: '| MCC | [[218](#bib.bib218)] |'
  prefs: []
  type: TYPE_TB
- en: '| MIoU | [[212](#bib.bib212)] |'
  prefs: []
  type: TYPE_TB
- en: '| MSE | [[212](#bib.bib212)] |'
  prefs: []
  type: TYPE_TB
- en: '| PCP | [[219](#bib.bib219)] |'
  prefs: []
  type: TYPE_TB
- en: '| PSNR | [[206](#bib.bib206)],[[211](#bib.bib211)] |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | [[209](#bib.bib209)],[[213](#bib.bib213)],[[218](#bib.bib218)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Psi | [[203](#bib.bib203)] |'
  prefs: []
  type: TYPE_TB
- en: '| RI | [[210](#bib.bib210)] |'
  prefs: []
  type: TYPE_TB
- en: '| RMSE | [[214](#bib.bib214)] |'
  prefs: []
  type: TYPE_TB
- en: '| ROC | [[207](#bib.bib207)],[[208](#bib.bib208)] |'
  prefs: []
  type: TYPE_TB
- en: '| Recall | [[209](#bib.bib209)],[[213](#bib.bib213)],[[218](#bib.bib218)] |'
  prefs: []
  type: TYPE_TB
- en: '| Run time | [[205](#bib.bib205)],[[206](#bib.bib206)],[[209](#bib.bib209)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| SRC | [[216](#bib.bib216)] |'
  prefs: []
  type: TYPE_TB
- en: '| SSIM | [[205](#bib.bib205)],[[206](#bib.bib206)],[[211](#bib.bib211)] |'
  prefs: []
  type: TYPE_TB
- en: '| TN | [[205](#bib.bib205)] |'
  prefs: []
  type: TYPE_TB
- en: '| TP | [[205](#bib.bib205)] |'
  prefs: []
  type: TYPE_TB
- en: '| VoI | [[210](#bib.bib210)] |'
  prefs: []
  type: TYPE_TB
- en: '| nMAPE | [[215](#bib.bib215)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VIII: Metrics used in other prediction applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Metric formulas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 Video prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | $MSE=\frac{1}{MN}\sum_{i=1}^{M}\sum_{j=1}^{N}(I(i,j)-\tilde{I}(i,j))^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $PSNR=20\log\left(\frac{MAX_{I}}{\sqrt{MSE}}\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: Structural Similarity (SSIM)
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $luminance(l)(x,y)=\frac{2\mu_{x}\mu_{y}+C_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+C_{1}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mu_{x}=\frac{1}{N}\sum_{i=1}^{N}x_{i}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $C_{1}=(K_{1}L)^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $L$ is dynamic range of pixel values (e.g. 255) and $K_{1}\ll 1$ is a
    small constant.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $contrast(c)(x,y)=\frac{2\sigma_{x}\sigma_{y}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\sigma_{x}=\left(\frac{1}{N-1}\sum_{i=1}^{N}(x_{i}-\mu_{x})^{2}\right)^{1/2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $C_{2}=(K_{2}L)^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $K_{2}\ll 1$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $structure(s)(x,y)=\frac{\sigma_{xy}+C_{3}}{\sigma_{x}\sigma_{y}+C_{3}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $C_{3}=(K_{3}L)^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $K_{3}\ll 1$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $SSIM(x,y)=[l(x,y)]^{\alpha}.[c(x,y)]^{\beta}.[s(x,y)]^{\gamma}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha,\beta,\gamma>0$ are parameters to choose in order to adjust the
    importance.
  prefs: []
  type: TYPE_NORMAL
- en: Learned Perceptual Image Patch Similarity (LPIPS)
  prefs: []
  type: TYPE_NORMAL
- en: Assume features are extracted from $L$ layers and unit-normalized in channel
    dimension, for layer l
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}^{l},\hat{y}^{l}\in R^{H_{l}\times W_{l}\times C_{l}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: The distance between reference $x$ and distorted patches $x_{0}$ is given by,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d(x,x_{0})=\sum_{l}\frac{1}{H_{l}W_{l}}\sum_{w,l}\parallel w_{l}\odot(\hat{y}^{l}_{hw},\hat{y}^{l}_{0hw})\parallel^{2}_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: C.2 Action prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are 4 possibilities for classification: True positive (TP) and True Negative
    (TN) when the algorithm correctly classifies positive and negative samples, and
    False Positive (FP) and False Negative (FN) when the algorithm incorrectly classifies
    negative samples as positive and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Accuracy=\frac{TN+TP}{TP+TN+FP+FN}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $Recall=\frac{TP}{TP+FN}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $F1-score=2\times\frac{Precision\times Recall}{Precision+Recall}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Let $p(r)$ be the precision-recall curve. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $AP=\int_{0}^{1}p(r)dr$ |  |'
  prefs: []
  type: TYPE_TB
- en: C.3 Trajectory prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: C.3.1 Distance metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '|  | $\text{Euclidean Distance}(ED)=\parallel y-\tilde{y}\parallel=\parallel
    y-\tilde{y}\parallel_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $=\sqrt{\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{Mean Absolute Error}(MAE)=\frac{1}{n}\sum_{i=1}^{n}&#124;y_{i}-\tilde{y_{i}}&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{Mean Square Error}(MSE)=\parallel y-\tilde{y}\parallel^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{Root MSE}(RMSE)=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{Hausdorff Distance}(HD)=max_{y\in Y}min_{\tilde{y}\in\tilde{Y}}\parallel
    y-\tilde{y}\parallel$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{Modified HD}(MHD)=max(d(Y,\tilde{Y}),d(\tilde{Y},Y))$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $d(Y,\tilde{Y})=\frac{1}{N_{y}}\sum_{y\in Y}min_{\tilde{y}\in\tilde{Y}}\parallel
    y-\tilde{y}\parallel$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $ADE=\frac{\sum^{N}_{i=1}\sum^{T_{pred}}_{t=1}\parallel\tilde{y}^{i}_{t}-y^{i}_{t}\parallel}{N\times
    T_{pred}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $N$ is the number of samples and $T_{pred}$ is the prediction steps.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $FDE=\frac{\sum^{N}_{i=1}\parallel\tilde{y}^{i}_{T_{pred}}-y^{i}_{T_{pred}}\parallel}{N}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $minMSD=\mathbb{E}_{\tilde{Y}_{k}\sim q_{\theta}}min_{\tilde{y}\in\tilde{Y}_{k}}\parallel
    y-\tilde{y}\parallel^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $q_{\theta}$ is the sampling space and $K$ number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $meanMSD=\frac{1}{K}\sum_{k=1}^{K}\parallel y-\tilde{y}\parallel^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $NLL=\mathbb{E}_{p(Y&#124;X)}\left[-\log\prod_{t=1}^{T_{pred}}p(y_{t}&#124;X)\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: C.4 Motion prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | $MPJPE=\frac{1}{N\times T_{pred}}\sum_{t=1}^{T_{pred}}\sum_{i=1}^{N}\parallel(J_{i}^{t}-J_{root}^{t})-(\tilde{J}_{i}^{t}-\tilde{J}_{root}^{t})\parallel$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Links to the datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Year | Dataset | Links |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | ARGOVerse[[137](#bib.bib137)] | [https://www.argoverse.org/data.html](https://www.argoverse.org/data.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CARLA[[162](#bib.bib162)] | [https://sites.google.com/view/precog](https://sites.google.com/view/precog)
    |'
  prefs: []
  type: TYPE_TB
- en: '| EgoPose[[186](#bib.bib186)] | [https://github.com/Khrylx/EgoPose](https://github.com/Khrylx/EgoPose)
    |'
  prefs: []
  type: TYPE_TB
- en: '| FM[[167](#bib.bib167)] | [https://mcl.korea.ac.kr/$∼$krkim/iccv2019/index.html](https://mcl.korea.ac.kr/%24%E2%88%BC%24krkim/iccv2019/index.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| InstaVariety[[240](#bib.bib240)] | [https://github.com/akanazawa/human_dynamics](https://github.com/akanazawa/human_dynamics)
    |'
  prefs: []
  type: TYPE_TB
- en: '| INTEARCTION[[241](#bib.bib241)] | [https://interaction-dataset.com](https://interaction-dataset.com)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Luggage[[81](#bib.bib81)] | [https://aashi7.github.io/NearCollision.html](https://aashi7.github.io/NearCollision.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MGIF[[242](#bib.bib242)] | [https://github.com/AliaksandrSiarohin/monkey-net](https://github.com/AliaksandrSiarohin/monkey-net)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PIE[[144](#bib.bib144)] | [http://data.nvision2.eecs.yorku.ca/PIE_dataset/](http://data.nvision2.eecs.yorku.ca/PIE_dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| nuScenes[[243](#bib.bib243)] | [https://www.nuscenes.org/](https://www.nuscenes.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vehicle-Pedestrian-Mixed (VPM)[[141](#bib.bib141)] | [http://vr.ict.ac.cn/vp-lstm.](http://vr.ict.ac.cn/vp-lstm.)
    |'
  prefs: []
  type: TYPE_TB
- en: '| TRAF[[177](#bib.bib177)] | [https://drive.google.com/drive/folders/1LqzJuRkx5yhOcjWFORO5WZ97v6jg8RHN](https://drive.google.com/drive/folders/1LqzJuRkx5yhOcjWFORO5WZ97v6jg8RHN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | 3DPW[[244](#bib.bib244)] | [https://virtualhumans.mpi-inf.mpg.de/3DPW/](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ActEV/VIRAT[[245](#bib.bib245)] | [https://actev.nist.gov/trecvid19](https://actev.nist.gov/trecvid19)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ACTICIPATE[[246](#bib.bib246)] | [http://vislab.isr.tecnico.ulisboa.pt/datasets/](http://vislab.isr.tecnico.ulisboa.pt/datasets/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| AVA[[247](#bib.bib247)] | [https://research.google.com/ava/](https://research.google.com/ava/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Epic-Kitchen[[248](#bib.bib248)] | [https://epic-kitchens.github.io/2019](https://epic-kitchens.github.io/2019)
    |'
  prefs: []
  type: TYPE_TB
- en: '| EGTEA Gaze+[[249](#bib.bib249)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| STC[[223](#bib.bib223)] | [https://svip-lab.github.io/dataset/campus_dataset.html](https://svip-lab.github.io/dataset/campus_dataset.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeStack[[250](#bib.bib250)] | [https://shapestacks.robots.ox.ac.uk/](https://shapestacks.robots.ox.ac.uk/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| VIENA[[75](#bib.bib75)] | [https://sites.google.com/view/viena2-project/home](https://sites.google.com/view/viena2-project/home)
    |'
  prefs: []
  type: TYPE_TB
- en: '| YouCook2[[251](#bib.bib251)] | [http://youcook2.eecs.umich.edu/](http://youcook2.eecs.umich.edu/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | BUA[[252](#bib.bib252)] | [http://cs-people.bu.edu/sbargal/BU-action/](http://cs-people.bu.edu/sbargal/BU-action/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CityPerson[[253](#bib.bib253)] | [https://bitbucket.org/shanshanzhang/citypersons/src/default/](https://bitbucket.org/shanshanzhang/citypersons/src/default/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Epic-fail[[84](#bib.bib84)] | [http://aliensunmin.github.io/project/video-Forecasting/](http://aliensunmin.github.io/project/video-Forecasting/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| JAAD[[78](#bib.bib78)] | [http://data.nvision2.eecs.yorku.ca/JAAD_dataset/](http://data.nvision2.eecs.yorku.ca/JAAD_dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| L-CAS[[254](#bib.bib254)] | [https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/](https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mouse Fish [[255](#bib.bib255)] | [https://web.bii.a-star.edu.sg/archive/machine_learning/Projects/behaviorAnalysis/Lie-X/Lie-X.html](https://web.bii.a-star.edu.sg/archive/machine_learning/Projects/behaviorAnalysis/Lie-X/Lie-X.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ORC[[256](#bib.bib256)] | [https://robotcar-dataset.robots.ox.ac.uk/](https://robotcar-dataset.robots.ox.ac.uk/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PKU-MMD[[257](#bib.bib257)] | [http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html](http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Recipe1M[[258](#bib.bib258)] | [http://pic2recipe.csail.mit.edu/](http://pic2recipe.csail.mit.edu/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| STRANDS[[259](#bib.bib259)] | [https://strands.readthedocs.io/en/latest/datasets/](https://strands.readthedocs.io/en/latest/datasets/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | BAIR Push[[29](#bib.bib29)] | [https://sites.google.com/site/brainrobotdata/home/push-dataset](https://sites.google.com/site/brainrobotdata/home/push-dataset)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BB[[260](#bib.bib260)] | [https://github.com/mbchang/dynamics](https://github.com/mbchang/dynamics)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MU[[220](#bib.bib220)] | [http://staff.itee.uq.edu.au/lovell/MissUniverse/](http://staff.itee.uq.edu.au/lovell/MissUniverse/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes[[261](#bib.bib261)] | [https://www.cityscapes-dataset.com/](https://www.cityscapes-dataset.com/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CMU mocap[[262](#bib.bib262)] | [http://mocap.cs.cmu.edu/](http://mocap.cs.cmu.edu/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DAD[[79](#bib.bib79)] | [https://aliensunmin.github.io/project/dashcam/](https://aliensunmin.github.io/project/dashcam/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| NTU RGB-D[[263](#bib.bib263)] | [http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp](http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp)
    |'
  prefs: []
  type: TYPE_TB
- en: '| OA[[106](#bib.bib106)] | [http://www.mpii.de/ongoing-activity](http://www.mpii.de/ongoing-activity)
    |'
  prefs: []
  type: TYPE_TB
- en: '| OAD[[264](#bib.bib264)] | [http://www.icst.pku.edu.cn/struct/Projects/OAD.html](http://www.icst.pku.edu.cn/struct/Projects/OAD.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SD[[265](#bib.bib265)] | [http://cvgl.stanford.edu/projects/uav_data/](http://cvgl.stanford.edu/projects/uav_data/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| TV Series[[266](#bib.bib266)] | [https://github.com/zhenyangli/online_action](https://github.com/zhenyangli/online_action)
    |'
  prefs: []
  type: TYPE_TB
- en: '| VIST[[267](#bib.bib267)] | [http://visionandlanguage.net/VIST/](http://visionandlanguage.net/VIST/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Youtube-8M[[268](#bib.bib268)] | [https://research.google.com/youtube8m/](https://research.google.com/youtube8m/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Amazon[[269](#bib.bib269)] | [http://jmcauley.ucsd.edu/data/amazon/index_2014.html](http://jmcauley.ucsd.edu/data/amazon/index_2014.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Atari[[30](#bib.bib30)] | [https://github.com/junhyukoh/nips2015-action-conditional-video-prediction](https://github.com/junhyukoh/nips2015-action-conditional-video-prediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Brain4Cars[[53](#bib.bib53)] | [https://github.com/asheshjain399/ICCV2015_Brain4Cars](https://github.com/asheshjain399/ICCV2015_Brain4Cars)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CMU Panoptic[[270](#bib.bib270)] | [http://domedb.perception.cs.cmu.edu/dataset.html](http://domedb.perception.cs.cmu.edu/dataset.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| FPPA[[95](#bib.bib95)] | [http://bvision11.cs.unc.edu/bigpen/yipin/ICCV2015/prediction_webpage/Prediction.html](http://bvision11.cs.unc.edu/bigpen/yipin/ICCV2015/prediction_webpage/Prediction.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze+[[271](#bib.bib271)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MBI-1M[[272](#bib.bib272)] | [http://academic.mywebsiteontheinternet.com/data/](http://academic.mywebsiteontheinternet.com/data/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MOT[[273](#bib.bib273)] | [https://motchallenge.net/](https://motchallenge.net/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MMNIST[[274](#bib.bib274)] | [http://www.cs.toronto.edu/$∼$nitish/unsupervised_video/](http://www.cs.toronto.edu/%24%E2%88%BC%24nitish/unsupervised_video/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SUN RGB-D[[333](#bib.bib333)] | [http://rgbd.cs.princeton.edu/](http://rgbd.cs.princeton.edu/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SYSU 3DHOI[[276](#bib.bib276)] | [http://www.isee-ai.cn/$∼$hujianfang/ProjectJOULE.html](http://www.isee-ai.cn/%24%E2%88%BC%24hujianfang/ProjectJOULE.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| THUMOS[[277](#bib.bib277)] | [http://www.thumos.info/home.html](http://www.thumos.info/home.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| WnP[[278](#bib.bib278)] | [http://watchnpatch.cs.cornell.edu/](http://watchnpatch.cs.cornell.edu/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wider[[279](#bib.bib279)] | [http://yjxiong.me/event_recog/WIDER/](http://yjxiong.me/event_recog/WIDER/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2014 | Breakfast[[280](#bib.bib280)] | [http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/](http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Human3.6M[[237](#bib.bib237)] | [http://vision.imar.ro/human3.6m/description.php](http://vision.imar.ro/human3.6m/description.php)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MPII Human Pose[[281](#bib.bib281)] | [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ORGBD[[282](#bib.bib282)] | [https://sites.google.com/site/skicyyu/orgbd](https://sites.google.com/site/skicyyu/orgbd)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sports-1M[[283](#bib.bib283)] | [https://cs.stanford.edu/people/karpathy/deepvideo/](https://cs.stanford.edu/people/karpathy/deepvideo/)
    |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IX: A summary of datasets (from year 2014-2019) used in vision-based
    prediction papers and corresponding links.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Dataset | Links |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | 50 salads[[284](#bib.bib284)] | [https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/](https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ATC [[285](#bib.bib285)] | [https://irc.atr.jp/crest2010_HRI/ATC_dataset/](https://irc.atr.jp/crest2010_HRI/ATC_dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CAD-120[[286](#bib.bib286)] | [http://pr.cs.cornell.edu/humanactivities/data.php](http://pr.cs.cornell.edu/humanactivities/data.php)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CHUK Avenue[[287](#bib.bib287)] | [http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html](http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Daimler path[[288](#bib.bib288)] | [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Pedestrian_Path_Predict_GCPR_1/pedestrian_path_predict_gcpr_1.html](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Pedestrian_Path_Predict_GCPR_1/pedestrian_path_predict_gcpr_1.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| JHMDB[[289](#bib.bib289)] | [http://jhmdb.is.tue.mpg.de/](http://jhmdb.is.tue.mpg.de/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Penn Action[[290](#bib.bib290)] | [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | BIT[[291](#bib.bib291)] | [https://sites.google.com/site/alexkongy/software](https://sites.google.com/site/alexkongy/software)
    |'
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze[[292](#bib.bib292)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI[[293](#bib.bib293)] | [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MANIAC[[294](#bib.bib294)] | [https://alexandria.physik3.uni-goettingen.de/cns-group/datasets/maniac/](https://alexandria.physik3.uni-goettingen.de/cns-group/datasets/maniac/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MPII-cooking[[295](#bib.bib295)] | [https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSRDA[[296](#bib.bib296)] | [https://documents.uow.edu.au/$∼$wanqing/#MSRAction3DDatasets](https://documents.uow.edu.au/%24%E2%88%BC%24wanqing/#MSRAction3DDatasets)
    |'
  prefs: []
  type: TYPE_TB
- en: '| GC[[297](#bib.bib297)] | [http://www.ee.cuhk.edu.hk/$∼$xgwang/grandcentral.html](http://www.ee.cuhk.edu.hk/%24%E2%88%BC%24xgwang/grandcentral.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SBUKI[[298](#bib.bib298)] | [https://www3.cs.stonybrook.edu/$∼$kyun/research/kinect_interaction/index.html](https://www3.cs.stonybrook.edu/%24%E2%88%BC%24kyun/research/kinect_interaction/index.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| UCF-101[[299](#bib.bib299)] | [https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php)
    |'
  prefs: []
  type: TYPE_TB
- en: '| UTKA[[300](#bib.bib300)] | [http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| UvA-NEMO[[301](#bib.bib301)] | [https://www.uva-nemo.org/](https://www.uva-nemo.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2011 | FCVL[[302](#bib.bib302)] | [http://robots.engin.umich.edu/SoftwareData/Ford](http://robots.engin.umich.edu/SoftwareData/Ford)
    |'
  prefs: []
  type: TYPE_TB
- en: '| HMDB[[303](#bib.bib303)] | [http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Stanford 40[[304](#bib.bib304)] | [http://vision.stanford.edu/Datasets/40actions.html](http://vision.stanford.edu/Datasets/40actions.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Town Center[[305](#bib.bib305)] | [http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets](http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets)
    |'
  prefs: []
  type: TYPE_TB
- en: '| VIRAT[[306](#bib.bib306)] | [http://viratdata.org/](http://viratdata.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2010 | DISPLECS[[307](#bib.bib307)] | [https://cvssp.org/data/diplecs/](https://cvssp.org/data/diplecs/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSR[[308](#bib.bib308)] | [https://www.microsoft.com/en-us/download/details.aspx?id=52315](https://www.microsoft.com/en-us/download/details.aspx?id=52315)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MUG Facial Expression[[309](#bib.bib309)] | [https://mug.ee.auth.gr/fed/](https://mug.ee.auth.gr/fed/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PROST[[310](#bib.bib310)] | [www.gpu4vision.com](www.gpu4vision.com) |'
  prefs: []
  type: TYPE_TB
- en: '| THI[[311](#bib.bib311)] | [http://www.robots.ox.ac.uk/$∼$alonso/tv_human_interactions.html](http://www.robots.ox.ac.uk/%24%E2%88%BC%24alonso/tv_human_interactions.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| UTI[[312](#bib.bib312)] | [http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| VISOR[[313](#bib.bib313)] | [imagelab.ing.unimore.it/visor](imagelab.ing.unimore.it/visor)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Willow Action[[314](#bib.bib314)] | [https://www.di.ens.fr/willow/research/stillactions/](https://www.di.ens.fr/willow/research/stillactions/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2009 | Caltech Pedestrian[[315](#bib.bib315)] | [http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Collective Activity (CA)[[316](#bib.bib316)] | [http://www-personal.umich.edu/$∼$wgchoi/eccv12/wongun_eccv12.html](http://www-personal.umich.edu/%24%E2%88%BC%24wgchoi/eccv12/wongun_eccv12.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| EIFP[[317](#bib.bib317)] | [http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/](http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ETH[[235](#bib.bib235)] | [http://www.vision.ee.ethz.ch/en/datasets/](http://www.vision.ee.ethz.ch/en/datasets/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| OSU[[318](#bib.bib318)] | [http://eecs.oregonstate.edu/football/tracking/dataset](http://eecs.oregonstate.edu/football/tracking/dataset)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PETS2009[[319](#bib.bib319)] | [http://www.cvg.reading.ac.uk/PETS2009/a.html](http://www.cvg.reading.ac.uk/PETS2009/a.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| QMUL[[320](#bib.bib320)] | [http://personal.ie.cuhk.edu.hk/$∼$ccloy/downloads_qmul_junction.html](http://personal.ie.cuhk.edu.hk/%24%E2%88%BC%24ccloy/downloads_qmul_junction.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| TUM Kitchen[[321](#bib.bib321)] | [https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data](https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data)
    |'
  prefs: []
  type: TYPE_TB
- en: '| YUV Videos[[322](#bib.bib322)] | [http://trace.kom.aau.dk/yuv/index.html](http://trace.kom.aau.dk/yuv/index.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2008 | Daimler[[323](#bib.bib323)] | [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MITT[[324](#bib.bib324)] | [http://www.ee.cuhk.edu.hk/$∼$xgwang/MITtrajsingle.html](http://www.ee.cuhk.edu.hk/%24%E2%88%BC%24xgwang/MITtrajsingle.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2007 | AMOS[[325](#bib.bib325)] | [http://amos.cse.wustl.edu/](http://amos.cse.wustl.edu/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ETH pedestrian[[326](#bib.bib326)] | [https://data.vision.ee.ethz.ch/cvl/aess/](https://data.vision.ee.ethz.ch/cvl/aess/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Lankershim Boulevard[[327](#bib.bib327)] | [https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm](https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm)
    |'
  prefs: []
  type: TYPE_TB
- en: '| NGSIM[[328](#bib.bib328)] | [https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm](https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm)
    |'
  prefs: []
  type: TYPE_TB
- en: '| UCY[[329](#bib.bib329)] | [https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data](https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2006 | Tuscan Arizona[[330](#bib.bib330)] | [http://www.mmto.org/](http://www.mmto.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2004 | KTH[[331](#bib.bib331)] | [http://www.nada.kth.se/cvap/actions/](http://www.nada.kth.se/cvap/actions/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1981 | Golden Colorado[[332](#bib.bib332)] | [https://www.osti.gov/dataexplorer/biblio/dataset/1052221](https://www.osti.gov/dataexplorer/biblio/dataset/1052221)
    |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE X: A summary of datasets (from year 2013 and earlier) used in vision-based
    prediction papers and corresponding links.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lists of datasets with associated repository links can be found in Tables [IX](#A4.T9
    "TABLE IX ‣ Appendix D Links to the datasets ‣ Deep Learning for Vision-based
    Prediction: A Survey") and [X](#A4.T10 "TABLE X ‣ Appendix D Links to the datasets
    ‣ Deep Learning for Vision-based Prediction: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Datasets and corresponding papers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lists of datasets and corresponding papers can be found in Tables [XI](#A5.T11
    "TABLE XI ‣ Appendix E Datasets and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey"), [XII](#A5.T12 "TABLE XII ‣ Appendix E Datasets and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey"), [XIII](#A5.T13
    "TABLE XIII ‣ Appendix E Datasets and corresponding papers ‣ Deep Learning for
    Vision-based Prediction: A Survey"), [XIV](#A5.T14 "TABLE XIV ‣ Appendix E Datasets
    and corresponding papers ‣ Deep Learning for Vision-based Prediction: A Survey"),
    and [XV](#A5.T15 "TABLE XV ‣ Appendix E Datasets and corresponding papers ‣ Deep
    Learning for Vision-based Prediction: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Atari | [[30](#bib.bib30)] |'
  prefs: []
  type: TYPE_TB
- en: '| BAIR Push | [[3](#bib.bib3)] ,[[24](#bib.bib24)],[[29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: '| Bouncing Ball | [[23](#bib.bib23)] |'
  prefs: []
  type: TYPE_TB
- en: '| CHUK Avenue | [[2](#bib.bib2)] |'
  prefs: []
  type: TYPE_TB
- en: '| Caltech Pedestrian | [[20](#bib.bib20)],[[33](#bib.bib33)] , [[2](#bib.bib2)],[[1](#bib.bib1)],[[34](#bib.bib34)],[[12](#bib.bib12)],[[6](#bib.bib6)],
    [[4](#bib.bib4)] |'
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes | [[3](#bib.bib3)],[[32](#bib.bib32)] |'
  prefs: []
  type: TYPE_TB
- en: '| Human 3.6M | [[32](#bib.bib32)],[[33](#bib.bib33)],[[22](#bib.bib22)],[[24](#bib.bib24)],[[14](#bib.bib14)],[[25](#bib.bib25)],[[7](#bib.bib7)],[[28](#bib.bib28)],[[29](#bib.bib29)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| JAAD | [[9](#bib.bib9)] |'
  prefs: []
  type: TYPE_TB
- en: '| JHMDB | [[21](#bib.bib21)] |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI | [[2](#bib.bib2)],[[1](#bib.bib1)],[[20](#bib.bib20)],[[33](#bib.bib33)],[[34](#bib.bib34)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[6](#bib.bib6)],[[15](#bib.bib15)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| KTH | [[31](#bib.bib31)],[[19](#bib.bib19)],[[11](#bib.bib11)],[[16](#bib.bib16)],[[13](#bib.bib13)],[[35](#bib.bib35)],[[27](#bib.bib27)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| MGIF | [[18](#bib.bib18)] |'
  prefs: []
  type: TYPE_TB
- en: '| MMNIST | [[3](#bib.bib3)],[[31](#bib.bib31)],[[19](#bib.bib19)],[[16](#bib.bib16)],[[23](#bib.bib23)],[[24](#bib.bib24)],[[36](#bib.bib36)],[[8](#bib.bib8)],[[27](#bib.bib27)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSR | [[19](#bib.bib19)] |'
  prefs: []
  type: TYPE_TB
- en: '| MUG | [[229](#bib.bib229)] |'
  prefs: []
  type: TYPE_TB
- en: '| Own | [[37](#bib.bib37)],[[25](#bib.bib25)] |'
  prefs: []
  type: TYPE_TB
- en: '| PROST | [[36](#bib.bib36)] |'
  prefs: []
  type: TYPE_TB
- en: '| Penn Action | [[21](#bib.bib21)],[[229](#bib.bib229)],[[26](#bib.bib26)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Penn action | [[17](#bib.bib17)],[[18](#bib.bib18)],[[28](#bib.bib28)] |'
  prefs: []
  type: TYPE_TB
- en: '| ShanghaiTech Campus | [[2](#bib.bib2)] |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeStack | [[17](#bib.bib17)] |'
  prefs: []
  type: TYPE_TB
- en: '| Sports-1M | [[36](#bib.bib36)],[[15](#bib.bib15)] |'
  prefs: []
  type: TYPE_TB
- en: '| THUMOS | [[6](#bib.bib6)] |'
  prefs: []
  type: TYPE_TB
- en: '| UCF-101 | [[2](#bib.bib2)],[[4](#bib.bib4)],[[5](#bib.bib5)],[[32](#bib.bib32)],[[33](#bib.bib33)],[[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[36](#bib.bib36)],[[6](#bib.bib6)],[[26](#bib.bib26)],[[15](#bib.bib15)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| UvA-NEMO | [[18](#bib.bib18)] |'
  prefs: []
  type: TYPE_TB
- en: '| ViSOR | [[36](#bib.bib36)] |'
  prefs: []
  type: TYPE_TB
- en: '| YUV | [[4](#bib.bib4)],[[20](#bib.bib20)] |'
  prefs: []
  type: TYPE_TB
- en: '| Youtube-8M | [[12](#bib.bib12)] |'
  prefs: []
  type: TYPE_TB
- en: '| pedestrian | [[4](#bib.bib4)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XI: Datasets used in video prediction applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 50Salad | [[63](#bib.bib63)],[[66](#bib.bib66)],[[69](#bib.bib69)] |'
  prefs: []
  type: TYPE_TB
- en: '| ANTICIPATE | [[90](#bib.bib90)] |'
  prefs: []
  type: TYPE_TB
- en: '| AVA | [[88](#bib.bib88)],[[88](#bib.bib88)] |'
  prefs: []
  type: TYPE_TB
- en: '| ActEV/VIRAT | [[87](#bib.bib87)] |'
  prefs: []
  type: TYPE_TB
- en: '| BIT | [[100](#bib.bib100)],[[109](#bib.bib109)],[[111](#bib.bib111)],[[113](#bib.bib113)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| BU Action | [[107](#bib.bib107)] |'
  prefs: []
  type: TYPE_TB
- en: '| Brain4Cars | [[96](#bib.bib96)],[[80](#bib.bib80)],[[53](#bib.bib53)] |'
  prefs: []
  type: TYPE_TB
- en: '| Breakfast | [[66](#bib.bib66)],[[67](#bib.bib67)],[[69](#bib.bib69)] |'
  prefs: []
  type: TYPE_TB
- en: '| CA | [[101](#bib.bib101)] |'
  prefs: []
  type: TYPE_TB
- en: '| CAD-120 | [[67](#bib.bib67)],[[90](#bib.bib90)],[[58](#bib.bib58)],[[96](#bib.bib96)],[[52](#bib.bib52)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| CMU Panoptic | [[56](#bib.bib56)] |'
  prefs: []
  type: TYPE_TB
- en: '| CMU Mocap | [[110](#bib.bib110)] |'
  prefs: []
  type: TYPE_TB
- en: '| Caltech Pedestrian | [[54](#bib.bib54)] |'
  prefs: []
  type: TYPE_TB
- en: '| DAD | [[83](#bib.bib83)],[[84](#bib.bib84)] |'
  prefs: []
  type: TYPE_TB
- en: '| Daimler | [[54](#bib.bib54)] |'
  prefs: []
  type: TYPE_TB
- en: '| Daimler Path | [[40](#bib.bib40)] |'
  prefs: []
  type: TYPE_TB
- en: '| EGTEA Gaze+ | [[64](#bib.bib64)] |'
  prefs: []
  type: TYPE_TB
- en: '| ETH Pedestrian | [[54](#bib.bib54)] |'
  prefs: []
  type: TYPE_TB
- en: '| Epic-fail | [[84](#bib.bib84)] |'
  prefs: []
  type: TYPE_TB
- en: '| Epic-Kitchen | [[63](#bib.bib63)] ,[[64](#bib.bib64)],[[68](#bib.bib68)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| FPPA | [[95](#bib.bib95)] |'
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze | [[89](#bib.bib89)] |'
  prefs: []
  type: TYPE_TB
- en: '| GTEA Gaze+ | [[89](#bib.bib89)] |'
  prefs: []
  type: TYPE_TB
- en: '| HMDB | [[104](#bib.bib104)] |'
  prefs: []
  type: TYPE_TB
- en: '| Human 3.6M | [[110](#bib.bib110)] |'
  prefs: []
  type: TYPE_TB
- en: '| JAAD | [[9](#bib.bib9)],[[73](#bib.bib73)],[[75](#bib.bib75)],[[78](#bib.bib78)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| JHMDB | [[88](#bib.bib88)],[[88](#bib.bib88)],[[100](#bib.bib100)],[[102](#bib.bib102)],[[105](#bib.bib105)],[[112](#bib.bib112)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Luggage | [[81](#bib.bib81)] |'
  prefs: []
  type: TYPE_TB
- en: '| MANIAC | [[57](#bib.bib57)] |'
  prefs: []
  type: TYPE_TB
- en: '| MPII Cooking | [[67](#bib.bib67)],[[70](#bib.bib70)],[[60](#bib.bib60)] |'
  prefs: []
  type: TYPE_TB
- en: '| MSRDA | [[45](#bib.bib45)] |'
  prefs: []
  type: TYPE_TB
- en: '| NGSIM | [[72](#bib.bib72)],[[74](#bib.bib74)],[[97](#bib.bib97)] |'
  prefs: []
  type: TYPE_TB
- en: '| NTU RGB-D | [[98](#bib.bib98)] |'
  prefs: []
  type: TYPE_TB
- en: '| OA | [[106](#bib.bib106)] |'
  prefs: []
  type: TYPE_TB
- en: '| OAD | [[108](#bib.bib108)] |'
  prefs: []
  type: TYPE_TB
- en: '| ORGBD | [[41](#bib.bib41)] |'
  prefs: []
  type: TYPE_TB
- en: '| PIE | [[71](#bib.bib71)] |'
  prefs: []
  type: TYPE_TB
- en: '| PKU-MMD | [[108](#bib.bib108)] |'
  prefs: []
  type: TYPE_TB
- en: '| Recipe1M | [[65](#bib.bib65)] |'
  prefs: []
  type: TYPE_TB
- en: '| SBUIK | [[101](#bib.bib101)] |'
  prefs: []
  type: TYPE_TB
- en: '| SYSU 3DHOI | [[98](#bib.bib98)],[[41](#bib.bib41)] |'
  prefs: []
  type: TYPE_TB
- en: '| Stanford-40 | [[107](#bib.bib107)] |'
  prefs: []
  type: TYPE_TB
- en: '| THUMOS | [[91](#bib.bib91)],[[92](#bib.bib92)],[[93](#bib.bib93)] |'
  prefs: []
  type: TYPE_TB
- en: '| TV Human Interaction | [[99](#bib.bib99)] , [[91](#bib.bib91)],[[8](#bib.bib8)],[[92](#bib.bib92)],[[93](#bib.bib93)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| TV Series | [[92](#bib.bib92)] |'
  prefs: []
  type: TYPE_TB
- en: '| UCF-101 | [[98](#bib.bib98)],[[99](#bib.bib99)],[[100](#bib.bib100)],[[107](#bib.bib107)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[104](#bib.bib104)],[[111](#bib.bib111)],[[105](#bib.bib105)],[[112](#bib.bib112)],[[61](#bib.bib61)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| UTI | [[99](#bib.bib99)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[105](#bib.bib105)],[[61](#bib.bib61)],[[113](#bib.bib113)],[[44](#bib.bib44)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| UTKA | [[94](#bib.bib94)] |'
  prefs: []
  type: TYPE_TB
- en: '| VIENA | [[75](#bib.bib75)] |'
  prefs: []
  type: TYPE_TB
- en: '| VIRAT | [[70](#bib.bib70)] |'
  prefs: []
  type: TYPE_TB
- en: '| WIDER | [[107](#bib.bib107)] |'
  prefs: []
  type: TYPE_TB
- en: '| Willow Action | [[107](#bib.bib107)] |'
  prefs: []
  type: TYPE_TB
- en: '| WnP | [[94](#bib.bib94)] |'
  prefs: []
  type: TYPE_TB
- en: '| YouCook2 | [[65](#bib.bib65)] |'
  prefs: []
  type: TYPE_TB
- en: '| Sports-1M | [[111](#bib.bib111)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XII: Datasets used in action prediction applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ARGOVerse | [[137](#bib.bib137)] |'
  prefs: []
  type: TYPE_TB
- en: '| ATC | [[118](#bib.bib118)] |'
  prefs: []
  type: TYPE_TB
- en: '| ActEV/VIRAT | [[87](#bib.bib87)] |'
  prefs: []
  type: TYPE_TB
- en: '| CA | [[101](#bib.bib101)] |'
  prefs: []
  type: TYPE_TB
- en: '| CARLA | [[162](#bib.bib162)],[[147](#bib.bib147)] |'
  prefs: []
  type: TYPE_TB
- en: '| CHUK | [[155](#bib.bib155)] |'
  prefs: []
  type: TYPE_TB
- en: '| CityPerson | [[183](#bib.bib183)] |'
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes | [[150](#bib.bib150)] |'
  prefs: []
  type: TYPE_TB
- en: '| Daimler Path | [[136](#bib.bib136)] |'
  prefs: []
  type: TYPE_TB
- en: '| ETH | [[178](#bib.bib178)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[146](#bib.bib146)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[155](#bib.bib155)],[[164](#bib.bib164)],[[156](#bib.bib156)],[[180](#bib.bib180)],[[158](#bib.bib158)],[[160](#bib.bib160)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Edinburgh (IFP) | [[114](#bib.bib114)] , [[179](#bib.bib179)] |'
  prefs: []
  type: TYPE_TB
- en: '| FM | [[167](#bib.bib167)] |'
  prefs: []
  type: TYPE_TB
- en: '| GC | [[155](#bib.bib155)],[[115](#bib.bib115)],[[176](#bib.bib176)],[[135](#bib.bib135)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| INTEARCTION | [[163](#bib.bib163)] |'
  prefs: []
  type: TYPE_TB
- en: '| JAAD | [[144](#bib.bib144)] |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI | [[150](#bib.bib150)],[[166](#bib.bib166)],[[165](#bib.bib165)] |'
  prefs: []
  type: TYPE_TB
- en: '| L-CAS | [[234](#bib.bib234)] |'
  prefs: []
  type: TYPE_TB
- en: '| Lankershim Boulevard | [[179](#bib.bib179)] |'
  prefs: []
  type: TYPE_TB
- en: '| MITT | [[135](#bib.bib135)] |'
  prefs: []
  type: TYPE_TB
- en: '| MOT | [[129](#bib.bib129)] |'
  prefs: []
  type: TYPE_TB
- en: '| NGSIM | [[177](#bib.bib177)],[[140](#bib.bib140)],[[141](#bib.bib141)],[[145](#bib.bib145)],[[148](#bib.bib148)],[[181](#bib.bib181)],[[182](#bib.bib182)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| OSU | [[125](#bib.bib125)] |'
  prefs: []
  type: TYPE_TB
- en: '| Oxford | [[150](#bib.bib150)] |'
  prefs: []
  type: TYPE_TB
- en: '| PETS2009 | [[152](#bib.bib152)] |'
  prefs: []
  type: TYPE_TB
- en: '| PIE | [[144](#bib.bib144)] |'
  prefs: []
  type: TYPE_TB
- en: '| QMUL | [[115](#bib.bib115)] |'
  prefs: []
  type: TYPE_TB
- en: '| SBUIK | [[101](#bib.bib101)] |'
  prefs: []
  type: TYPE_TB
- en: '| SD | [[178](#bib.bib178)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[163](#bib.bib163)],[[152](#bib.bib152)],[[165](#bib.bib165)],[[132](#bib.bib132)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| STRANDS | [[234](#bib.bib234)] |'
  prefs: []
  type: TYPE_TB
- en: '| TRAF | [[177](#bib.bib177)] |'
  prefs: []
  type: TYPE_TB
- en: '| TUM Kitchen | [[134](#bib.bib134)] |'
  prefs: []
  type: TYPE_TB
- en: '| Town Center | [[154](#bib.bib154)],[[131](#bib.bib131)],[[175](#bib.bib175)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| UCY | [[178](#bib.bib178)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[145](#bib.bib145)],[[146](#bib.bib146)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[154](#bib.bib154)],[[155](#bib.bib155)],[[164](#bib.bib164)],[[180](#bib.bib180)],[[131](#bib.bib131)],[[158](#bib.bib158)],[[159](#bib.bib159)],[[175](#bib.bib175)],[[160](#bib.bib160)],[[132](#bib.bib132)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| VIRAT | [[123](#bib.bib123)] |'
  prefs: []
  type: TYPE_TB
- en: '| VPM | [[141](#bib.bib141)] |'
  prefs: []
  type: TYPE_TB
- en: '| nuScenes | [[162](#bib.bib162)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XIII: Datasets used in trajectory prediction applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 3DPW | [[197](#bib.bib197)] |'
  prefs: []
  type: TYPE_TB
- en: '| CA | [[101](#bib.bib101)] |'
  prefs: []
  type: TYPE_TB
- en: '| CMU Mocap | [[197](#bib.bib197)] |'
  prefs: []
  type: TYPE_TB
- en: '| CMU Panoptic | [[56](#bib.bib56)] |'
  prefs: []
  type: TYPE_TB
- en: '| Egopose | [[186](#bib.bib186)] |'
  prefs: []
  type: TYPE_TB
- en: '| Human 3.6M | [[187](#bib.bib187)],[[193](#bib.bib193)],[[96](#bib.bib96)],[[195](#bib.bib195)],[[190](#bib.bib190)],[[236](#bib.bib236)],[[197](#bib.bib197)],[[198](#bib.bib198)],[[196](#bib.bib196)],[[199](#bib.bib199)],[[191](#bib.bib191)],[[192](#bib.bib192)],[[110](#bib.bib110)],[[189](#bib.bib189)],[[194](#bib.bib194)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| InstaVariety | [[199](#bib.bib199)] |'
  prefs: []
  type: TYPE_TB
- en: '| MPII Human Pose | [[189](#bib.bib189)] |'
  prefs: []
  type: TYPE_TB
- en: '| Mouse Fish | [[236](#bib.bib236)] |'
  prefs: []
  type: TYPE_TB
- en: '| Own | [[48](#bib.bib48)],[[200](#bib.bib200)],[[188](#bib.bib188)],[[103](#bib.bib103)],[[185](#bib.bib185)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Penn Action | [[199](#bib.bib199)],[[187](#bib.bib187)],[[26](#bib.bib26)],[[189](#bib.bib189)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| SBUIK | [[101](#bib.bib101)] |'
  prefs: []
  type: TYPE_TB
- en: '| UCF-101 | [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XIV: Datasets used in motion prediction applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AMOS | [[214](#bib.bib214)] |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon | [[217](#bib.bib217)] |'
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes | [[10](#bib.bib10)],[[210](#bib.bib210)],[[211](#bib.bib211)],[[212](#bib.bib212)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| DIPLECS | [[239](#bib.bib239)] |'
  prefs: []
  type: TYPE_TB
- en: '| FCVL | [[209](#bib.bib209)] |'
  prefs: []
  type: TYPE_TB
- en: '| Golden Colorado | [[215](#bib.bib215)] |'
  prefs: []
  type: TYPE_TB
- en: '| JAAD | [[203](#bib.bib203)] |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI | [[205](#bib.bib205)],[[201](#bib.bib201)] |'
  prefs: []
  type: TYPE_TB
- en: '| MBI-1M | [[216](#bib.bib216)] |'
  prefs: []
  type: TYPE_TB
- en: '| MU | [[220](#bib.bib220)] |'
  prefs: []
  type: TYPE_TB
- en: '| SUN RGB-D | [[219](#bib.bib219)] |'
  prefs: []
  type: TYPE_TB
- en: '| Tuscan Arizona | [[215](#bib.bib215)] |'
  prefs: []
  type: TYPE_TB
- en: '| VIST | [[8](#bib.bib8)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XV: Datasets used in other prediction applications.'
  prefs: []
  type: TYPE_NORMAL
