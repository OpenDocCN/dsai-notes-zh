- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:00:31'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:00:31
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2007.00095] Deep Learning for Vision-based Prediction: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2007.00095] 深度学习在基于视觉的预测中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2007.00095](https://ar5iv.labs.arxiv.org/html/2007.00095)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2007.00095](https://ar5iv.labs.arxiv.org/html/2007.00095)
- en: 'Deep Learning for Vision-based Prediction: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在基于视觉的预测中的应用：综述
- en: Amir Rasouli A. Rasouli is with Noah’s Ark Laboratory at Huawei Technologies
    Canada, 19 Allstate Pkwy, Markham, ON L3R 5A4
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Amir Rasouli A. Rasouli 目前在华为技术加拿大公司的诺亚方舟实验室工作，地址为19 Allstate Pkwy, Markham,
    ON L3R 5A4
- en: 'E-mail: amir.rasouli@huawei.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：amir.rasouli@huawei.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Vision-based prediction algorithms have a wide range of applications including
    autonomous driving, surveillance, human-robot interaction, weather prediction.
    The objective of this paper is to provide an overview of the field in the past
    five years with a particular focus on deep learning approaches. For this purpose,
    we categorize these algorithms into video prediction, action prediction, trajectory
    prediction, body motion prediction, and other prediction applications. For each
    category, we highlight the common architectures, training methods and types of
    data used. In addition, we discuss the common evaluation metrics and datasets
    used for vision-based prediction tasks. A database of all the information presented
    in this survey, cross-referenced according to papers, datasets and metrics, can
    be found online at [https://github.com/aras62/vision-based-prediction](https://github.com/aras62/vision-based-prediction).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视觉的预测算法在自动驾驶、监控、人机交互、天气预测等多个领域有广泛的应用。本文的目标是提供过去五年该领域的概述，特别关注深度学习方法。为此，我们将这些算法分类为视频预测、动作预测、轨迹预测、身体运动预测和其他预测应用。对于每个类别，我们突出常见的架构、训练方法和使用的数据类型。此外，我们还讨论了用于基于视觉的预测任务的常见评估指标和数据集。所有这些信息的数据库，按照论文、数据集和指标进行交叉引用，可以在[https://github.com/aras62/vision-based-prediction](https://github.com/aras62/vision-based-prediction)在线找到。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Video Prediction, Action Prediction , Trajectory Prediction, Motion Prediction,
    Survey.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 视频预测、动作预测、轨迹预测、运动预测、综述。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The ability to predict the changes in the environment and the behavior of objects
    is fundamental in many applications such as surveillance, autonomous driving,
    scene understanding, etc. Prediction is a widely studied field in various artificial
    intelligence communities. A subset of these algorithms relies primarily on visual
    appearances of the objects and the scene to reason about the future. Other approaches
    use different forms of sensors such as wearable or environmental sensors to learn
    about the past states of the environment or objects.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 预测环境变化和物体行为的能力在许多应用中至关重要，例如监控、自动驾驶、场景理解等。预测是各种人工智能社区广泛研究的领域。这些算法的一个子集主要依赖物体和场景的视觉外观来推断未来。其他方法则使用不同形式的传感器，如可穿戴传感器或环境传感器，以了解环境或物体的过去状态。
- en: The focus of this report is on vision-based prediction algorithms, which primarily
    use visual information to observe the changes in the environment and predict the
    future. In this context, prediction can be in the form of generating future scenes
    or reasoning about specific aspects of the objects, e.g. their trajectories, poses,
    etc.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本报告的重点是基于视觉的预测算法，这些算法主要使用视觉信息来观察环境变化并预测未来。在这种情况下，预测可以表现为生成未来场景或推理物体的特定方面，例如其轨迹、姿态等。
- en: For this review, we divide the prediction algorithms into five groups, namely
    video prediction, action prediction, trajectory prediction, motion (pose) prediction,
    and others which involve various applications of prediction such as trend prediction,
    visual weather prediction, map prediction, semantic prediction, etc. In addition,
    we briefly discuss algorithms that use a form of prediction as an intermediate
    step to perform tasks such as object detection, action detection, and recognition,
    etc. Moreover, for each group of prediction algorithms, we will talk about the
    common datasets and metrics and discuss of their characteristics. It should be
    noted that due to the broad scope of this review and the large body of work on
    the vision-based prediction, this review will only focus on works that had been
    published since five years ago in major computer vision, robotics and machine
    learning venues. In addition, as the title of the paper suggests, the main focus
    of the discussion will be on deep learning methods given their popularity in recent
    years.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这次审查，我们将预测算法分为五组，即视频预测、动作预测、轨迹预测、运动（姿态）预测和其他涉及各种预测应用的组，如趋势预测、视觉天气预测、地图预测、语义预测等。此外，我们还简要讨论了使用预测形式作为中间步骤以执行诸如目标检测、动作检测和识别等任务的算法。此外，对于每组预测算法，我们将讨论常见的数据集和指标，并讨论其特征。需要注意的是，由于本审查的范围广泛以及基于视觉的预测工作的庞大，这次审查仅关注自五年前在主要计算机视觉、机器人和机器学习领域发表的工作。此外，正如论文标题所示，讨论的主要焦点将是深度学习方法，因为它们在近年来非常流行。
- en: 2 Vision-based Prediction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基于视觉的预测
- en: Before reviewing the works on vision-based prediction algorithms, there are
    a number of points that should be considered.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在审阅基于视觉的预测算法之前，需要考虑一些要点。
- en: 2.1 Applications
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 应用
- en: Based on our review, we have identified four major vision-based applications
    namely, video prediction, action prediction, trajectory prediction, and motion
    prediction. We discuss each of the studies in each category in a dedicated section.
    Some of the prediction works, such as visual weather prediction, semantic prediction,
    contests outcome prediction, that do not fit to any of the four major categories
    are presented in other application section.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的审查，我们确定了四个主要的基于视觉的应用，即视频预测、动作预测、轨迹预测和运动预测。我们在专门的部分讨论每个类别中的每项研究。一些预测工作，如视觉天气预测、语义预测、竞赛结果预测，不适合任何四个主要类别，会在其他应用部分中介绍。
- en: Some works address multiple prediction tasks, e.g. predicting trajectories and
    actions simultaneously, and therefore might fall in more than one category. It
    should be noted that we only include an algorithm in each category if the corresponding
    task is directly evaluated. For instance, if an algorithm performs video prediction
    for future action classification, and only evaluates the accuracy of predicted
    actions, it will only appear in the action prediction category. Furthermore, some
    works that are reviewed in this paper propose multiple architectures, e.g. recurrent
    and feedforward, for solving the same problem. In architecture-based categorizations,
    these algorithms may appear more than once.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作涉及多个预测任务，例如同时预测轨迹和动作，因此可能属于多个类别。需要注意的是，我们只在每个类别中包含对应任务直接评估的算法。例如，如果一个算法执行未来动作分类的视频预测，并且仅评估预测动作的准确性，它将仅出现在动作预测类别中。此外，本文中审阅的一些工作提出了多个架构，例如递归和前馈，以解决相同的问题。在基于架构的分类中，这些算法可能会出现多次。
- en: 2.2 Methods
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 方法
- en: 2.2.1 Algorithms
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 算法
- en: 'This work focuses on vision-based algorithms, which use some form of visual
    input such as RGB camera images or active sensors such as LIDAR. It should be
    noted that many algorithms, especially trajectory prediction ones, only use ground
    truth data such as object trajectories without actual visual processing, e.g.
    for detection of objects. However, as long as these algorithms are evaluated on
    vision datasets, they are included in this paper. Note that a completer list of
    papers with published code can be found in Appendix [A](#A1 "Appendix A Papers
    with code ‣ Deep Learning for Vision-based Prediction: A Survey").'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究关注基于视觉的算法，这些算法使用某种形式的视觉输入，如RGB相机图像或LIDAR等主动传感器。值得注意的是，许多算法，特别是轨迹预测算法，仅使用真实数据，如对象轨迹，而没有实际的视觉处理，例如对象检测。然而，只要这些算法在视觉数据集上进行评估，它们就被包括在本文中。请注意，已发布代码的完整论文列表可以在附录[A](#A1
    "附录 A 发布代码的论文 ‣ 基于视觉的预测的深度学习：综述")中找到。
- en: 2.2.2 Architectures
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 架构
- en: As mentioned earlier, we focus on algorithms that have a deep learning component,
    either in the stage of visual representation generation (e.g. using convolutional
    features) or reasoning (e.g. using an MultiLayer Preceptron (MLP) for classification).
    We will, however, acknowledge the classical methods by mentioning some of the
    main techniques and including them in the datasets and metrics sections of this
    paper.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们关注具有深度学习组件的算法，无论是在视觉表示生成阶段（例如，使用卷积特征）还是推理阶段（例如，使用多层感知器（MLP）进行分类）。然而，我们会通过提及一些主要技术并将其包括在本文的数据集和指标部分，来承认经典方法。
- en: We classify the algorithms in terms of training techniques and architectures.
    In practice, this is very challenging as the majority of algorithms use a combination
    of different approaches. For example, recurrent networks often rely on a form
    of Convolutional Neural Networks (CNNs) to generate feature representations for
    scenes, poses of agents, etc. To better distinguish between different classes
    of algorithms, we only focus on the core component of each algorithm, i.e. the
    parts that are used for reasoning about the future. Hence, for example, if an
    algorithm uses a CNN model for pre-processing input data and a recurrent network
    for temporal reasoning, we consider this algorithm as recurrent. On the other
    hand, if the features are used with a fully connected network, we categorize this
    algorithm as feedforward or one-shot method. A few algorithms propose the use
    of both architectures for reasoning. We address those methods as hybrid. In addition,
    it should be noted that many works propose alternative approaches using each architecture.
    Therefore, we categorize them in more than one group.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据训练技术和架构对算法进行分类。在实践中，这非常具有挑战性，因为大多数算法使用不同方法的组合。例如，递归网络通常依赖卷积神经网络（CNNs）的某种形式来生成场景、代理姿势等的特征表示。为了更好地区分不同类别的算法，我们只关注每个算法的核心组件，即用于推理未来的部分。因此，例如，如果一个算法使用CNN模型进行输入数据的预处理，并使用递归网络进行时间推理，我们将该算法视为递归型。另一方面，如果特征与全连接网络一起使用，我们将该算法归类为前馈或一次性方法。一些算法提议使用两种架构进行推理。我们将这些方法称为混合型。此外，应注意，许多研究提出了使用每种架构的替代方法。因此，我们将它们分类到多个组中。
- en: 2.2.3 Data type
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 数据类型
- en: As one would expect, vision-based algorithms primarily rely on visual information.
    However, many algorithms use pre-trained off-the-shelf algorithms to transform
    the input to some explicit feature spaces, e.g. poses, trajectories, action labels
    and perform reasoning in those feature spaces. If pre-processing is not part of
    the main algorithm, we consider those secondary features as different types of
    data inputs to the algorithms. If some basic processing, e.g. generating convolutional
    features for a scene is used, we consider the data type of the original input,
    e.g. RGB images.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，基于视觉的算法主要依赖于视觉信息。然而，许多算法使用预训练的现成算法将输入转换为某些显式特征空间，例如姿势、轨迹、动作标签，并在这些特征空间中进行推理。如果预处理不是主要算法的一部分，我们将这些次要特征视为不同类型的数据输入。如果使用了基本处理，例如生成场景的卷积特征，我们将数据类型视为原始输入的类型，例如RGB图像。
- en: 3 Video prediction
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 视频预测
- en: Video or future scene prediction can be regarded as the most generic form of
    prediction. The objective of video prediction algorithms is to generate future
    scenes, often in the form of RGB images [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)] and/or optical flow maps [[5](#bib.bib5), [6](#bib.bib6), [1](#bib.bib1),
    [7](#bib.bib7)]. The generated images in turn can be used for various tasks such
    as action prediction [[8](#bib.bib8)], event prediction [[9](#bib.bib9)], flow
    estimation [[6](#bib.bib6)], semantic segmentation [[10](#bib.bib10)], etc.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 视频或未来场景预测可以视为最通用的预测形式。视频预测算法的目标是生成未来场景，通常以RGB图像[[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)]和/或光流图[[5](#bib.bib5), [6](#bib.bib6), [1](#bib.bib1),
    [7](#bib.bib7)]的形式呈现。生成的图像反过来可以用于各种任务，如动作预测[[8](#bib.bib8)]、事件预测[[9](#bib.bib9)]、流估计[[6](#bib.bib6)]、语义分割[[10](#bib.bib10)]等。
- en: Video prediction applications rely on generative models whose task is to predict
    future scene(s) based on a short observation of input sequences (or in some cases
    only a single image [[11](#bib.bib11)]). Although many approaches use feedforard
    architectures [[2](#bib.bib2), [1](#bib.bib1), [4](#bib.bib4), [5](#bib.bib5),
    [9](#bib.bib9), [12](#bib.bib12), [11](#bib.bib11), [7](#bib.bib7), [13](#bib.bib13),
    [14](#bib.bib14), [8](#bib.bib8), [15](#bib.bib15)], the majority of algorithms
    take advantage of Recurrent Neural Networks (RNNs) such as Gated Recurrent Units
    (GRUs) [[16](#bib.bib16)], Long-Short Term Memory (LSTM) networks [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)], its variation Convolutional
    LSTMs (ConvLSTMs)[[3](#bib.bib3), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [6](#bib.bib6), [29](#bib.bib29)]
    or a combination of these [[37](#bib.bib37), [27](#bib.bib27)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 视频预测应用依赖于生成模型，其任务是基于对输入序列的短时间观察（或在某些情况下仅一张图像[[11](#bib.bib11)]）来预测未来场景。尽管许多方法使用前馈架构[[2](#bib.bib2),
    [1](#bib.bib1), [4](#bib.bib4), [5](#bib.bib5), [9](#bib.bib9), [12](#bib.bib12),
    [11](#bib.bib11), [7](#bib.bib7), [13](#bib.bib13), [14](#bib.bib14), [8](#bib.bib8),
    [15](#bib.bib15)]，大多数算法利用递归神经网络（RNNs），例如门控递归单元（GRUs）[[16](#bib.bib16)]、长短期记忆（LSTM）网络[[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]、其变体卷积LSTM（ConvLSTMs）[[3](#bib.bib3),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [6](#bib.bib6), [29](#bib.bib29)]或这些的组合[[37](#bib.bib37), [27](#bib.bib27)]。
- en: 'Generative Adversarial Networks (GANs) are particularly popular in the video
    prediction community. [[2](#bib.bib2), [18](#bib.bib18), [31](#bib.bib31), [19](#bib.bib19),
    [21](#bib.bib21), [22](#bib.bib22), [24](#bib.bib24), [13](#bib.bib13), [14](#bib.bib14),
    [36](#bib.bib36), [6](#bib.bib6), [26](#bib.bib26), [15](#bib.bib15)]. In these
    adversarial training frameworks, there are two compoents: A generative network
    that produces future representations and a discriminator whose objective is to
    distinguish between the predicted representations (e.g. optical flow [[6](#bib.bib6)],
    frames [[18](#bib.bib18)], motion [[31](#bib.bib31)]) or their temporal consistency
    [[2](#bib.bib2), [19](#bib.bib19)] and the actual ground truth data by producing
    a binary classification score that indicates whether the prediction is real or
    fake. While many algorithms use discriminators to judge how realistic the final
    generated images [[18](#bib.bib18), [21](#bib.bib21), [22](#bib.bib22), [24](#bib.bib24),
    [13](#bib.bib13), [36](#bib.bib36)] are or intermediate features (e.g. poses [[26](#bib.bib26)]),
    others use multiple discriminators at different stages of processing. For example,
    the authors of [[2](#bib.bib2), [19](#bib.bib19)] use two discriminators, one
    is responsible for judging the temporal consistency of the generated frames (i.e.
    whether the order of generated frames is real) and the other assesses whether
    the generated frames are real or not. Lee et al. [[31](#bib.bib31)] use three
    discriminators to assess the quality of generated frames and the intermediate
    motion and content features. Using a two-stream approach, the method in [[6](#bib.bib6)]
    produces both the next frame and optical flow and each stream is trained with
    a separate discriminator. The prediction network of [[15](#bib.bib15)] uses a
    discriminator for intermediate features generated from input scenes and another
    discriminator for the final results.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）在视频预测领域特别受欢迎。[[2](#bib.bib2), [18](#bib.bib18), [31](#bib.bib31),
    [19](#bib.bib19), [21](#bib.bib21), [22](#bib.bib22), [24](#bib.bib24), [13](#bib.bib13),
    [14](#bib.bib14), [36](#bib.bib36), [6](#bib.bib6), [26](#bib.bib26), [15](#bib.bib15)]。在这些对抗训练框架中，有两个组件：一个生成网络负责生成未来的表示，另一个是判别器，其目标是通过生成一个二分类分数来区分预测的表示（例如光流[[6](#bib.bib6)]、帧[[18](#bib.bib18)]、运动[[31](#bib.bib31)]）或它们的时间一致性[[2](#bib.bib2),
    [19](#bib.bib19)]与实际的真实数据。许多算法使用判别器来判断最终生成图像[[18](#bib.bib18), [21](#bib.bib21),
    [22](#bib.bib22), [24](#bib.bib24), [13](#bib.bib13), [36](#bib.bib36)]的真实性或中间特征（例如姿态[[26](#bib.bib26)]），而其他算法在不同处理阶段使用多个判别器。例如，[[2](#bib.bib2),
    [19](#bib.bib19)]的作者使用两个判别器，一个负责判断生成帧的时间一致性（即生成帧的顺序是否真实），另一个评估生成的帧是否真实。Lee等人[[31](#bib.bib31)]使用三个判别器来评估生成帧的质量及其中间运动和内容特征。使用双流方法，[[6](#bib.bib6)]中的方法同时生成下一帧和光流，每个流都用一个单独的判别器进行训练。[[15](#bib.bib15)]的预测网络使用一个判别器来处理从输入场景生成的中间特征，另一个判别器处理最终结果。
- en: Variational Autoencoders (VAEs) [[38](#bib.bib38)] or Conditional VAEs (CVAEs)
    [[39](#bib.bib39)] are also used in some approaches [[3](#bib.bib3), [18](#bib.bib18),
    [11](#bib.bib11), [23](#bib.bib23), [26](#bib.bib26)]. VAEs model uncertainty
    in generated future frames by defining a posterior distribution over some latent
    variable space [[3](#bib.bib3), [23](#bib.bib23), [26](#bib.bib26)]. In CVAEs,
    the posterior is conditioned on an additional parameter such as the observed action
    in the scenes [[18](#bib.bib18)] or initial observation [[11](#bib.bib11)]. Using
    VAEs, at inference time, a random sample is drawn from the posterior to generate
    the future frame.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAEs）[[38](#bib.bib38)]或条件变分自编码器（CVAEs）[[39](#bib.bib39)]在一些方法中也有使用[[3](#bib.bib3),
    [18](#bib.bib18), [11](#bib.bib11), [23](#bib.bib23), [26](#bib.bib26)]。VAEs通过定义一些潜在变量空间上的后验分布来建模生成未来帧的不确定性[[3](#bib.bib3),
    [23](#bib.bib23), [26](#bib.bib26)]。在CVAEs中，后验分布以一个额外的参数为条件，例如场景中的观察动作[[18](#bib.bib18)]或初始观察[[11](#bib.bib11)]。使用VAEs时，在推理时，从后验分布中随机抽样生成未来帧。
- en: Many video prediction algorithms operate solely on input images and propose
    various architectural innovations for encoding the content and generating future
    images [[2](#bib.bib2), [3](#bib.bib3), [32](#bib.bib32), [33](#bib.bib33), [16](#bib.bib16),
    [34](#bib.bib34), [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [8](#bib.bib8),
    [15](#bib.bib15), [27](#bib.bib27)]. For example, the method in [[2](#bib.bib2)]
    performs a two-way prediction, forward and backward. Each prediction relies on
    two discriminators for assessing the quality of the generated images and temporal
    consistency. The model presented in [[3](#bib.bib3)] trains a context network
    by inputting an image sequence into a ConvLSTM whose output is used to initialize
    convolutional networks responsible for generating the next frames. Xu et al. [[32](#bib.bib32)],
    in addition to raw pixel values, encode the output of a high pass filter applied
    to the image as a means of maintaining the structural integrity of the objects
    in the scene. In [[15](#bib.bib15)], the authors use a two-step approach in which
    they first perform a coarse frame prediction followed by a fine frame prediction.
    In [[13](#bib.bib13)], the algorithm learns in two stages. A discriminator is
    applied after features are generated from the scenes and another one after the
    final generated frames.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 许多视频预测算法仅基于输入图像操作，并提出了各种架构创新来编码内容和生成未来图像 [[2](#bib.bib2), [3](#bib.bib3), [32](#bib.bib32),
    [33](#bib.bib33), [16](#bib.bib16), [34](#bib.bib34), [13](#bib.bib13), [14](#bib.bib14),
    [35](#bib.bib35), [8](#bib.bib8), [15](#bib.bib15), [27](#bib.bib27)]。例如，[[2](#bib.bib2)]
    中的方法执行双向预测，即前向和后向预测。每个预测依赖于两个判别器来评估生成图像的质量和时间一致性。[[3](#bib.bib3)] 中提出的模型通过将图像序列输入到
    ConvLSTM 中来训练一个上下文网络，其输出用于初始化负责生成下一帧的卷积网络。Xu 等人 [[32](#bib.bib32)] 除了原始像素值，还对图像应用高通滤波器的输出进行编码，以保持场景中物体的结构完整性。在
    [[15](#bib.bib15)] 中，作者使用了一个两步方法，首先进行粗略的帧预测，然后进行精细的帧预测。在 [[13](#bib.bib13)] 中，该算法分两个阶段进行学习。一个判别器在从场景生成特征后应用，另一个则在最终生成帧后应用。
- en: 'Optical flow prediction has been widely used as an intermediate step in video
    prediction algorithms [[1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [20](#bib.bib20),
    [5](#bib.bib5), [11](#bib.bib11), [25](#bib.bib25), [36](#bib.bib36), [6](#bib.bib6)].
    For example, to deal with occlusion in dynamic scenes, Gao et al. [[1](#bib.bib1)]
    disentangle flow and pixel-level predictions into two steps: the algorithm first
    predicts the flow of the scene, and then uses it, in conjunction with the input
    frames, to predict the future. Similar multi-step approaches have also been used
    in [[6](#bib.bib6), [36](#bib.bib36), [11](#bib.bib11), [31](#bib.bib31)]. In
    [[31](#bib.bib31)], the authors use two separate branches: one branch receives
    two consecutive frames $(t,t+1)$ and produces context information. The second
    branch produces motion information by receiving two frames that are $k$ steps
    apart (i.e. $t+1$, $t+k$). The outputs of these two branches are fused and fed
    into the final scene generator. The method in [[6](#bib.bib6)] simultaneously
    produces the next future frame and the corresponding optical flow map. In this
    architecture, two additional networks are used: A flow estimator which uses the
    output of the frame generator and the last observation to estimate a flow map
    and a warping layer which performs differential 2D spatial transformation to warp
    the last observed image into the future predicted frame according to the predicted
    flow map.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 光流预测已广泛用于视频预测算法的中间步骤 [[1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [20](#bib.bib20),
    [5](#bib.bib5), [11](#bib.bib11), [25](#bib.bib25), [36](#bib.bib36), [6](#bib.bib6)]。例如，为了处理动态场景中的遮挡，Gao
    等人 [[1](#bib.bib1)] 将光流和像素级预测分解为两个步骤：算法首先预测场景的光流，然后将其与输入帧结合，预测未来。类似的多步骤方法也在 [[6](#bib.bib6),
    [36](#bib.bib36), [11](#bib.bib11), [31](#bib.bib31)] 中使用。在 [[31](#bib.bib31)]
    中，作者使用了两个独立的分支：一个分支接收两个连续帧 $(t,t+1)$ 并生成上下文信息。第二个分支通过接收两个相隔 $k$ 步的帧（即 $t+1$, $t+k$）生成运动信息。这两个分支的输出被融合并输入到最终的场景生成器中。在
    [[6](#bib.bib6)] 中，方法同时生成下一帧和相应的光流图。在该架构中，使用了两个额外的网络：一个流估计器，它利用帧生成器的输出和最后的观察来估计流图，以及一个变形层，它根据预测的流图执行差分二维空间变换，将最后观察到的图像扭曲到未来预测的帧中。
- en: Some algorithms rely on various intermediate steps for video prediction[[17](#bib.bib17),
    [18](#bib.bib18), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [7](#bib.bib7), [14](#bib.bib14), [35](#bib.bib35)]. For instance, the method
    in [[17](#bib.bib17)], reasons about the locations and features of individual
    entities (e.g. cubes) for final scene predictions. Kim et al. [[18](#bib.bib18)]
    first identify keypoints, which may correspond to important structures such as
    joints, and then predict their motion. For videos involving humans, in [[21](#bib.bib21),
    [22](#bib.bib22), [7](#bib.bib7)] the authors identify and reason about the changes
    in poses, and use this information to generate future frames. In [[14](#bib.bib14),
    [35](#bib.bib35)], in addition to raw input images, the differences between consecutive
    frames used in the learning process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法依赖于视频预测的各种中间步骤[[17](#bib.bib17), [18](#bib.bib18), [21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [7](#bib.bib7), [14](#bib.bib14), [35](#bib.bib35)]。例如，[[17](#bib.bib17)]中的方法考虑了个体实体（如立方体）的位置和特征以进行最终场景预测。Kim等人[[18](#bib.bib18)]首先识别关键点，这些关键点可能对应于重要结构如关节，然后预测它们的运动。对于涉及人的视频，在[[21](#bib.bib21),
    [22](#bib.bib22), [7](#bib.bib7)]中，作者识别并推理姿势的变化，并利用这些信息生成未来帧。在[[14](#bib.bib14),
    [35](#bib.bib35)]中，除了原始输入图像外，还使用了学习过程中的连续帧差异。
- en: Prediction networks can also be provided with additional information to guide
    future frame generation. In [[19](#bib.bib19), [12](#bib.bib12)] an optical flow
    network and in [[26](#bib.bib26), [28](#bib.bib28)] a pose estimation network
    are used in addition to RGB images. Using a CVAE architecture, in [[18](#bib.bib18)]
    the authors use the action lables as conditional input for frame generation. In
    the context of active tasks, e.g. object manipulation with robotic arms, in which
    the consequences of actions influence the future scene configuration, it is common
    to condition the future scene generation on the current or future intended actions
    [[37](#bib.bib37), [29](#bib.bib29), [30](#bib.bib30)].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 预测网络还可以提供额外的信息以指导未来帧的生成。在[[19](#bib.bib19), [12](#bib.bib12)]中，除了RGB图像外，还使用了光流网络；在[[26](#bib.bib26),
    [28](#bib.bib28)]中，除了RGB图像外，还使用了姿态估计网络。在[[18](#bib.bib18)]中，作者利用CVAE架构将动作标签作为生成帧的条件输入。在主动任务的背景下，例如使用机器人手臂进行物体操作，其中行动的后果影响未来场景配置，通常将未来场景生成条件化于当前或未来预期的动作[[37](#bib.bib37),
    [29](#bib.bib29), [30](#bib.bib30)]。
- en: 3.1 Summary
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 摘要
- en: 'Video prediction algorithms are based on generative models that produce future
    images given a short observation, or in extreme cases a single view of the scene.
    Both recurrent and feedforward models are widely used in the field, with recurrent
    ones being slightly more favorable. The architectural designs and training strategies
    such as the VAEs or GANs are very common. However, it is hard to establish which
    one of these approaches is superior given that the majority of the video prediction
    algorithms are application-agnostic, meaning that they are evaluated on a wide
    range of video datasets with very different characteristics such as traffic scenes,
    activities, games, object manipulations, etc. (more on this in Section [11](#S11
    "11 Datasets ‣ Deep Learning for Vision-based Prediction: A Survey")).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 视频预测算法基于生成模型，这些模型在给定短时间观察，或在极端情况下只给定场景的单一视角的情况下生成未来图像。递归模型和前馈模型在该领域广泛使用，其中递归模型略微更受欢迎。架构设计和训练策略如VAEs或GANs非常常见。然而，很难确定这些方法中哪一种更优，因为大多数视频预测算法都是与应用无关的，即它们在具有非常不同特征的视频数据集上进行评估，如交通场景、活动、游戏、物体操作等（更多内容见第[11](#S11
    "11 数据集 ‣ 基于视觉的预测的深度学习：综述")节）。
- en: Despite the great progress in the field, video prediction algorithms are still
    facing some major challenges. One of them is the ability to hallucinate, which
    is to generate visual representations for parts of the scenes that were not visible
    during observation phase, e.g. due to occlusions. This is particularly an issue
    for more complex images such as traffic scenes, movies, etc. The complexity of
    the scenes also determines how fast the generated images would degrade. Although
    these algorithms show promising results in simple synthetic videos or action sequences,
    they still struggle in real practical applications. In addition, many of these
    algorithms cannot reason about the expected presence or absence of objects in
    the future. For example, if a moving object is present in the observations and
    is about to exit the field of view in near future, the algorithms account for
    it in the future scenes as long as parts of it are visible in the observation
    stage. This can be an issue for safety-critical applications such as autonomous
    driving in which the presence or absence of traffic elements and the interactions
    between them are essential for action planning.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该领域取得了重大进展，视频预测算法仍面临一些主要挑战。其中之一是生成视觉表现的能力，即对观察阶段不可见的场景部分进行生成，例如由于遮挡。这在复杂图像如交通场景、电影等中尤为突出。场景的复杂性也决定了生成图像的退化速度。虽然这些算法在简单的合成视频或动作序列中显示出有希望的结果，但在实际应用中仍面临困难。此外，许多这些算法无法推测未来对象的预期存在或缺失。例如，如果在观察中存在一个移动物体并且它即将退出视野，算法会在未来的场景中考虑它，只要观察阶段中可见它的一部分。这在安全关键的应用中，如自动驾驶，其中交通元素的存在或缺失以及它们之间的互动对动作规划至关重要，可能会成为问题。
- en: 4 Action prediction
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 动作预测
- en: 'Action prediction algorithms can be categorized into two groups: Next action
    or event prediction (or action anticipation) and early action prediction. In the
    former category, the algorithms use the observation of current activities or scene
    configurations and predict what will happen next. Early action prediction algorithms,
    on the other hand, observe parts of the current action in progress and predict
    what this action is. The classical learning approaches such as Conditional Random
    Fields (CRFs) [[40](#bib.bib40)], Support Vector Machines (SVMs) with hand-crafted
    features [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47)], Markov models [[48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)],
    Bayesian networks [[54](#bib.bib54), [55](#bib.bib55)] and other statistical methods
    [[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62)] have been widely used in recent years. However,
    as mentioned earlier, we will only focus on deep learning approaches.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 动作预测算法可以分为两类：下一步动作或事件预测（或动作预期）和早期动作预测。在前者类别中，算法使用当前活动或场景配置的观察结果来预测接下来会发生什么。另一方面，早期动作预测算法观察当前进行中的部分动作，并预测这个动作是什么。经典的学习方法如条件随机场（CRFs）[[40](#bib.bib40)]，支持向量机（SVMs）与手工特征[[41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47)]，马尔可夫模型[[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)]，贝叶斯网络[[54](#bib.bib54),
    [55](#bib.bib55)]以及其他统计方法[[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62)]在近年来被广泛使用。然而，如前所述，我们将只关注深度学习方法。
- en: 4.1 Action anticipation
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 动作预期
- en: Action prediction algorithms are used in a wide range of applications including
    cooking activities [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)], traffic
    understanding [[71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [73](#bib.bib73),
    [74](#bib.bib74), [75](#bib.bib75), [74](#bib.bib74), [76](#bib.bib76), [77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)], accident prediction [[81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)], sports [[85](#bib.bib85),
    [86](#bib.bib86)] and other forms of activities [[87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91), [8](#bib.bib8), [92](#bib.bib92),
    [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]. Although the majority of
    these algorithms use sequences in which the objects and agents are fully observable,
    a number of methods rely on egocentric scenes [[63](#bib.bib63), [64](#bib.bib64),
    [68](#bib.bib68), [89](#bib.bib89), [85](#bib.bib85), [95](#bib.bib95)] which
    are recorded from the point of view of the acting agents and only parts of their
    bodies (e.g. hands) are observable.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 动作预测算法被广泛应用于各种场景，包括烹饪活动 [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)]，交通理解
    [[71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75), [74](#bib.bib74), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80)]，事故预测 [[81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [84](#bib.bib84)]，体育 [[85](#bib.bib85), [86](#bib.bib86)] 以及其他形式的活动
    [[87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91),
    [8](#bib.bib8), [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]。虽然大多数这些算法使用的是对象和行为者完全可观察的序列，但一些方法依赖于自我中心的场景
    [[63](#bib.bib63), [64](#bib.bib64), [68](#bib.bib68), [89](#bib.bib89), [85](#bib.bib85),
    [95](#bib.bib95)]，这些场景是从行为者的视角记录的，只能观察到他们身体的一部分（例如手）。
- en: 'Action prediction methods predominantly use a variation of RNN-based architectures
    including LSTMs [[87](#bib.bib87), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [74](#bib.bib74), [67](#bib.bib67), [68](#bib.bib68), [82](#bib.bib82), [89](#bib.bib89),
    [75](#bib.bib75), [74](#bib.bib74), [90](#bib.bib90), [91](#bib.bib91), [85](#bib.bib85),
    [84](#bib.bib84), [86](#bib.bib86), [70](#bib.bib70), [92](#bib.bib92), [96](#bib.bib96),
    [79](#bib.bib79), [80](#bib.bib80)], GRUs [[88](#bib.bib88), [71](#bib.bib71),
    [72](#bib.bib72), [69](#bib.bib69)], ConvLSTMs [[76](#bib.bib76)], and Quasi-RNNs
    (QRNNs) [[83](#bib.bib83)]. For instance, in [[88](#bib.bib88), [67](#bib.bib67)]
    the authors use a graph-based RNN architecture in which the nodes represent actions
    and the edges of the graph represent the transitions between the actions. The
    method in [[69](#bib.bib69)] employs a two-step approach: using a recognition
    algorithm, the observed actions and their durations are recognized. These form
    a one-hot encoding vector which is fed into GRUs for the prediction of the future
    activities, their corresponding start time and length. In the context of vehicle
    behavior prediction, Ding et al. [[72](#bib.bib72)] uses a two-stream GRU-based
    architecture to encode the trajectory of two vehicles and a shared activation
    unit to encode the vehicles mutual interactions. Scheel et al. [[97](#bib.bib97)]
    encode the relationship between the ego-vehicle and surrounding vehicles in terms
    of their mutual distances. The vectorized encoding is then fed into a bi-directional
    LSTM. At each time step, the output of the LSTM is classified, using a softmax
    activation, into a binary value indicating whether it is safe for the ego-vehicle
    to change lane. In [[83](#bib.bib83)] the authors use a QRNN network to capture
    the relationships between road users in order to predict the likelihood of a traffic
    accident. To train the model, the authors propose an adaptive loss function that
    assigns penalty weights depending on how early the model can predict accidents.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 行为预测方法主要使用基于RNN的架构变体，包括LSTMs [[87](#bib.bib87), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66), [74](#bib.bib74), [67](#bib.bib67), [68](#bib.bib68), [82](#bib.bib82),
    [89](#bib.bib89), [75](#bib.bib75), [74](#bib.bib74), [90](#bib.bib90), [91](#bib.bib91),
    [85](#bib.bib85), [84](#bib.bib84), [86](#bib.bib86), [70](#bib.bib70), [92](#bib.bib92),
    [96](#bib.bib96), [79](#bib.bib79), [80](#bib.bib80)], GRUs [[88](#bib.bib88),
    [71](#bib.bib71), [72](#bib.bib72), [69](#bib.bib69)], ConvLSTMs [[76](#bib.bib76)]，以及Quasi-RNNs
    (QRNNs) [[83](#bib.bib83)]。例如，在[[88](#bib.bib88), [67](#bib.bib67)]中，作者使用了一种基于图的RNN架构，其中节点代表动作，图的边代表动作之间的转移。[[69](#bib.bib69)]中的方法采用两步法：使用识别算法识别观察到的动作及其持续时间。这些形成一个一热编码向量，并输入到GRUs中，以预测未来的活动、相应的开始时间和长度。在车辆行为预测的背景下，Ding等人[[72](#bib.bib72)]使用基于两流GRU的架构来编码两辆车的轨迹，以及一个共享激活单元来编码车辆之间的相互作用。Scheel等人[[97](#bib.bib97)]将自车与周围车辆之间的关系编码为相互距离。向量化编码随后输入到双向LSTM中。在每个时间步，LSTM的输出使用softmax激活进行分类，得到一个二元值，指示自车是否可以安全变道。在[[83](#bib.bib83)]中，作者使用QRNN网络捕捉道路使用者之间的关系，以预测交通事故的可能性。为了训练模型，作者提出了一种自适应损失函数，根据模型预测事故的提前程度分配惩罚权重。
- en: As an alternative to recurrent architectures, some algorithms use feedforward
    architectures using both 3D [[63](#bib.bib63), [9](#bib.bib9), [73](#bib.bib73)]
    and 2D [[81](#bib.bib81), [69](#bib.bib69), [77](#bib.bib77), [86](#bib.bib86),
    [78](#bib.bib78), [8](#bib.bib8), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]
    convolutional networks. For example, in the context of pedestrian crossing prediction,
    in [[9](#bib.bib9)] the authors use a generative 3D CNN model that produces future
    scenes and is followed by a classifier. The method of [[73](#bib.bib73)] detects
    and tracks pedestrians in the scenes, and then feeds the visual representations
    of the tracks, in the form of an image sequence, into a 3D CNN architecture, which
    directly classifies how likely the pedestrian will cross the road. To predict
    the time of traffic accidents, the method in [[81](#bib.bib81)] processes each
    input image using a 2D CNN model and then combines the representations followed
    by a fully-conntected (fc) layer for prediction. Farha et al. [[69](#bib.bib69)]
    create a 2D matrix by stacking one-hot encodings of actions for each segment of
    observation and use a 2D convolutional net to generate future actions encodings.
    Casas et al. [[77](#bib.bib77)] use a two-stream 2D CNN, each processing the stacked
    voxelized LIDAR scans and the scene map. The feature maps obtained from each stream
    are fused and fed into a backbone network followed by three headers responsible
    for the detection of the vehicles and predicting their intentions and trajectories.
    For sports forecasting, Felsen et al. [[86](#bib.bib86)] concatenate 5 image observations
    channel-wise and feed the resulting output into a 2D CNN network comprised of
    4 convolutional layers and an fc layer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作为递归架构的替代方案，一些算法使用前馈架构，利用了3D [[63](#bib.bib63), [9](#bib.bib9), [73](#bib.bib73)]
    和2D [[81](#bib.bib81), [69](#bib.bib69), [77](#bib.bib77), [86](#bib.bib86), [78](#bib.bib78),
    [8](#bib.bib8), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)] 卷积网络。例如，在行人过马路预测的背景下，[[9](#bib.bib9)]
    中的作者使用生成的3D CNN模型来生成未来场景，并随后使用分类器。[[73](#bib.bib73)] 中的方法检测并跟踪场景中的行人，然后将这些跟踪的视觉表示（以图像序列的形式）输入到3D
    CNN架构中，该架构直接分类行人过马路的可能性。为了预测交通事故的时间，[[81](#bib.bib81)] 中的方法使用2D CNN模型处理每张输入图像，然后结合表示并通过一个全连接（fc）层进行预测。Farha等人[[69](#bib.bib69)]
    通过对每个观察段的动作进行独热编码堆叠，创建一个2D矩阵，并使用2D卷积网络生成未来动作的编码。Casas等人[[77](#bib.bib77)] 使用一个双流2D
    CNN，每个流处理堆叠的体素化LIDAR扫描和场景地图。来自每个流的特征图被融合并输入到一个主干网络中，然后经过三个头部，负责检测车辆及预测其意图和轨迹。在体育预测方面，Felsen等人[[86](#bib.bib86)]
    按通道将5个图像观察结果串联起来，并将得到的输出输入到一个由4个卷积层和一个全连接层组成的2D CNN网络中。
- en: 'Although some algorithms rely on a single source of information, e.g. a set
    of pre-processed features from RGB images [[88](#bib.bib88), [82](#bib.bib82),
    [83](#bib.bib83), [91](#bib.bib91), [84](#bib.bib84), [86](#bib.bib86), [70](#bib.bib70),
    [92](#bib.bib92), [96](#bib.bib96)] or trajectories [[72](#bib.bib72)], many algorithms
    use a multimodal approach by using various sources of information such as optical
    flow maps [[64](#bib.bib64), [67](#bib.bib67), [68](#bib.bib68), [75](#bib.bib75),
    [95](#bib.bib95)], poses [[87](#bib.bib87), [71](#bib.bib71), [67](#bib.bib67),
    [90](#bib.bib90), [80](#bib.bib80)], scene attributes (e.g. road structure, semantics)
    [[87](#bib.bib87), [74](#bib.bib74), [89](#bib.bib89), [77](#bib.bib77)], text
    [[65](#bib.bib65)], action labels [[66](#bib.bib66)], length of actions [[69](#bib.bib69)],
    speed (e.g. ego-vehicle or surrounding agents) [[71](#bib.bib71), [74](#bib.bib74),
    [75](#bib.bib75), [97](#bib.bib97), [85](#bib.bib85)], gaze [[89](#bib.bib89),
    [90](#bib.bib90)], current activities [[78](#bib.bib78)] and the time [[63](#bib.bib63)]
    of the actions. For example, the method in [[65](#bib.bib65)] uses a multi-stream
    LSTM in which two LSTMs encode visual features and cooking recipes and an LSTM
    decodes them for final predictions. To capture the relationships within and between
    sequences, Gammulle et al. [[66](#bib.bib66)] propose a two-stream LSTM network
    with external neural memory units. Each stream is responsible for encoding visual
    features and action labels. In [[71](#bib.bib71)], a multi-layer GRU structure
    is used in which features with different modalities enter the network at different
    levels and are fused with the previous level encodings. The fusion process is
    taking place according to the complexity of the data modality, e.g. more complex
    features such as encodings of pedestrian appearances enter the network at the
    bottom layer, whereas location and speed features enter at the second-last and
    last layers respectively. Farha et al. [[69](#bib.bib69)] use a two-layer stacked
    GRU architecture which receives as input a feature tuple of the length of the
    activity and its corresponding one-hot vector encoding. In [[75](#bib.bib75)],
    the method uses a two-stage architecture: First information regarding the appearance
    of the scene, optical flow (pre-processed using a CNN) and vehicle dynamics are
    fed into individual LSTM units. Then, the output of these units is combined and
    passed through an fc layer to create a representation of the context. This representation
    is used by another LSTM network to predict future traffic actions. In the context
    of human-robot interaction, the authors of [[90](#bib.bib90)] combine the information
    regarding the gaze and pose of the humans using an encoder-decoder LSTM architecture
    to predict their next actions. Jain et al. [[80](#bib.bib80)] use a fusion network
    to combine head pose information of the driver, outside scene features, GPS information,
    and vehicle dynamics to predict the driver’s next action.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些算法依赖于单一的信息源，例如来自RGB图像的预处理特征集合[[88](#bib.bib88), [82](#bib.bib82), [83](#bib.bib83),
    [91](#bib.bib91), [84](#bib.bib84), [86](#bib.bib86), [70](#bib.bib70), [92](#bib.bib92),
    [96](#bib.bib96)]或轨迹[[72](#bib.bib72)]，许多算法则采用多模态的方法，通过使用各种信息源如光流图[[64](#bib.bib64),
    [67](#bib.bib67), [68](#bib.bib68), [75](#bib.bib75), [95](#bib.bib95)]，姿态[[87](#bib.bib87),
    [71](#bib.bib71), [67](#bib.bib67), [90](#bib.bib90), [80](#bib.bib80)]，场景属性（如道路结构、语义）[[87](#bib.bib87),
    [74](#bib.bib74), [89](#bib.bib89), [77](#bib.bib77)]，文本[[65](#bib.bib65)]，动作标签[[66](#bib.bib66)]，动作长度[[69](#bib.bib69)]，速度（如自车或周围代理）[[71](#bib.bib71),
    [74](#bib.bib74), [75](#bib.bib75), [97](#bib.bib97), [85](#bib.bib85)]，注视[[89](#bib.bib89),
    [90](#bib.bib90)]，当前活动[[78](#bib.bib78)]和动作的时间[[63](#bib.bib63)]。例如，[[65](#bib.bib65)]中的方法使用了一个多流LSTM，其中两个LSTM编码视觉特征和烹饪食谱，另一个LSTM对其进行解码以进行最终预测。为了捕捉序列内和序列之间的关系，Gammulle等[[66](#bib.bib66)]提出了一个具有外部神经记忆单元的双流LSTM网络。每个流负责编码视觉特征和动作标签。在[[71](#bib.bib71)]中，使用了一个多层GRU结构，其中具有不同模态的特征在不同层次进入网络，并与之前的层次编码进行融合。融合过程根据数据模态的复杂性进行，例如，更复杂的特征如行人外观的编码在底层进入网络，而位置和速度特征分别在倒数第二层和最后一层进入。Farha等[[69](#bib.bib69)]使用了一个两层堆叠的GRU架构，输入为活动长度的特征元组及其对应的one-hot向量编码。在[[75](#bib.bib75)]中，该方法使用了一个两阶段架构：首先将有关场景外观、光流（通过CNN预处理）和车辆动态的信息输入到各个LSTM单元中。然后，这些单元的输出被组合并通过一个fc层创建上下文表示。这个表示被另一个LSTM网络用来预测未来的交通动作。在人机交互的背景下，[[90](#bib.bib90)]的作者结合了人类的注视和姿态信息，使用编码器-解码器LSTM架构来预测他们的下一步动作。Jain等[[80](#bib.bib80)]使用了一个融合网络，将驾驶员的头部姿态信息、外部场景特征、GPS信息和车辆动态结合起来，以预测驾驶员的下一步动作。
- en: Before concluding this section, it is important to discuss the use of attention
    modules which have gained popularity in recent years [[63](#bib.bib63), [87](#bib.bib87),
    [88](#bib.bib88), [64](#bib.bib64), [74](#bib.bib74), [68](#bib.bib68), [82](#bib.bib82),
    [89](#bib.bib89), [84](#bib.bib84), [79](#bib.bib79)]. As the name implies, the
    objective of attention modules is to determine what has to be given more importance
    at a given time. These modules can come in different froms and can be applied
    to different dimensions of data and at various processing. Some of these modules
    are temporal attention [[63](#bib.bib63), [82](#bib.bib82), [89](#bib.bib89)]
    for identifying keyframes, modality attention [[64](#bib.bib64), [68](#bib.bib68)]
    to prioritize between different modalities of data input, spatial attention [[84](#bib.bib84),
    [79](#bib.bib79)] for highlighting the important parts of the scenes, and graph
    attention [[88](#bib.bib88)] for weighting nodes of the graph. In some cases,
    a combination of different attention mechanisms is used [[87](#bib.bib87), [74](#bib.bib74)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结本节内容之前，有必要讨论近年来颇受欢迎的注意力模块的使用[[63](#bib.bib63), [87](#bib.bib87), [88](#bib.bib88),
    [64](#bib.bib64), [74](#bib.bib74), [68](#bib.bib68), [82](#bib.bib82), [89](#bib.bib89),
    [84](#bib.bib84), [79](#bib.bib79)]。顾名思义，注意力模块的目标是确定在特定时间需要给予更多关注的内容。这些模块可以有不同的形式，并可应用于数据的不同维度以及各种处理过程。其中一些模块包括用于识别关键帧的时间注意力[[63](#bib.bib63),
    [82](#bib.bib82), [89](#bib.bib89)]，用于在不同数据输入模态之间进行优先级排序的模态注意力[[64](#bib.bib64),
    [68](#bib.bib68)]，用于突出场景重要部分的空间注意力[[84](#bib.bib84), [79](#bib.bib79)]，以及用于加权图节点的图注意力[[88](#bib.bib88)]。在某些情况下，使用了不同注意力机制的组合[[87](#bib.bib87),
    [74](#bib.bib74)]。
- en: 4.1.1 Summary
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 总结
- en: In the field of action anticipation, RNN architectures are strongly preferred.
    Compared to feedforward algorithms, recurrent methods have the flexibility of
    dealing with variable observation lengths and multi-modal data, in particular,
    when they are significantly different, e.g. trajectories and RGB images. However,
    basic recurrent architectures such as LSTMs and GRUs rely on some forms of pre-processing,
    especially when dealing with high dimensional data such as RGB images, which requires
    the use of various convolutional networks, a process that can be computationally
    costly. Feedforward models, on the other hand, can perform prediction in one shot,
    meaning that they can simultaneously perform temporal reasoning and spatial feature
    generation in a single framework, and as a result, potentially have a shorter
    processing time.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在动作预测领域，RNN 架构受到强烈青睐。与前馈算法相比，递归方法在处理变长观察序列和多模态数据时具有灵活性，特别是当这些数据显著不同，例如轨迹和 RGB
    图像时。然而，基本的递归架构如 LSTM 和 GRU 依赖于某些形式的预处理，尤其是在处理高维数据如 RGB 图像时，这需要使用各种卷积网络，这一过程可能计算开销较大。另一方面，前馈模型可以一蹴而就地进行预测，这意味着它们可以在一个框架内同时进行时间推理和空间特征生成，因此，处理时间可能更短。
- en: Many of the approaches mentioned earlier are generative in nature. They generate
    representations in some feature space and then using these representations predict
    what will happen next. Some algorithms go one step further and generate the actual
    future images and use them for prediction. Although such an approach seems effective
    for single actor events, e.g. cooking scenes, human-robot interaction, it is not
    a feasible approach for multi-agent predictions such as reasoning about behaviors
    of pedestrians or cars in traffic scenes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 许多前面提到的方法本质上是生成性的。它们在某些特征空间中生成表示，然后利用这些表示预测接下来会发生什么。一些算法更进一步，生成实际的未来图像并利用这些图像进行预测。虽然这种方法对于单一演员事件如烹饪场景和人机互动看似有效，但对于多代理预测（例如推理行人或交通场景中汽车的行为）而言，这不是一种可行的方法。
- en: The majority of the methods reviewed in this section use multi-modal data input.
    This seems to be a very effective approach, especially in high dimensional problems
    such as predicting road user behavior where the state (e.g. location and speed)
    of the road user, observer, and other agents, as well as scene structure, lighting
    conditions, and many other factors, play a role in predicting the future behavior.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾的大多数方法使用多模态数据输入。这似乎是一种非常有效的方法，特别是在诸如预测道路用户行为等高维问题中，在这些问题中，道路用户、观察者以及其他代理的状态（例如位置和速度），以及场景结构、光照条件和许多其他因素，都在预测未来行为中发挥作用。
- en: Multi-tasking, e.g. predicting actions and trajectories, are an effective way
    to predict future actions. For instance, trajectories can imply the possibility
    of certain actions, e.g. moving towards the road implies the possibility that
    the person might cross the street. As a result, the simultaneous learning of different
    tasks can be beneficial.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务处理，例如预测动作和轨迹，是预测未来动作的有效方法。例如，轨迹可以暗示某些动作的可能性，例如，朝着道路移动暗示该人可能会过马路。因此，同时学习不同任务可能是有益的。
- en: Last but not least is the use of attention modules. These modules are deemed
    to be very effective , in particular for tasks with high complexity in terms of
    the modality of input data, the scene structure and temporal relations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是使用注意力模块。这些模块被认为非常有效，特别是对于输入数据的模态、场景结构和时间关系具有高复杂性的任务。
- en: 4.2 Early action prediction
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 早期动作预测
- en: Similar to action anticipation methods, early action prediction algorithms widely
    use recurrent network architectures [[88](#bib.bib88), [98](#bib.bib98), [99](#bib.bib99),
    [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102), [103](#bib.bib103),
    [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)]. Although many of
    these algorithms use similar aproaches to action anticipation algorithms, some
    propose new approaches. For example, in [[98](#bib.bib98)] the authors use a teacher-student
    learning scheme where the teacher learns to recognize full sequences using a bi-directional
    LSTM and the student relies on partial videos using an LSTM network. They perform
    knowledge distillation by linking feature representations of both networks. Using
    GAN frameworks, the methods in [[99](#bib.bib99), [102](#bib.bib102)] predict
    future feature representations of videos in order to predict actions. Zhao et
    al. [[100](#bib.bib100)] implement a Kalman filter using an LSTM architecture.
    In this method, actions are predcted after observing each frame and corrections
    are made if the next observation provide additional information. The method of
    [[105](#bib.bib105)] uses a two-step LSTM architecture which first generates an
    encoding of the context using context-aware convolutional features and then combines
    these encodings with action-aware convolutional features to predict the action.
    The authors of this method propose a new loss function that penalizes false negatives
    with the same strength at any point in time and false positives with a strength
    that increases linearly over time, to reach the same weight as that on false negatives.
    In [[106](#bib.bib106)] the authors perform coarse-to-fine-grained predictions
    depending on the amount of evidence available for the type of action.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于动作预期方法，早期动作预测算法广泛使用递归网络架构[[88](#bib.bib88), [98](#bib.bib98), [99](#bib.bib99),
    [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102), [103](#bib.bib103),
    [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)]。虽然许多这些算法使用与动作预期算法类似的方法，但有些提出了新的方法。例如，在[[98](#bib.bib98)]中，作者使用教师-学生学习方案，其中教师通过双向LSTM学习识别完整序列，而学生则依靠部分视频使用LSTM网络。他们通过链接两个网络的特征表示来执行知识蒸馏。使用GAN框架，[[99](#bib.bib99),
    [102](#bib.bib102)]中的方法预测视频的未来特征表示以预测动作。赵等人[[100](#bib.bib100)]实现了一种使用LSTM架构的卡尔曼滤波器。在这种方法中，动作是在观察每一帧后预测的，并且如果下一次观察提供了额外的信息，则会进行修正。[[105](#bib.bib105)]的方法使用两步LSTM架构，首先使用上下文感知卷积特征生成上下文编码，然后将这些编码与动作感知卷积特征结合以预测动作。该方法的作者提出了一种新的损失函数，该函数以相同的强度惩罚假阴性，并以线性增加的强度惩罚假阳性，达到与假阴性相同的权重。在[[106](#bib.bib106)]中，作者根据可用证据的数量对动作类型进行粗到细的预测。
- en: Many early action prediction methods adopt feedforward architectures [[107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109), [104](#bib.bib104), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113)]. The authors of [[107](#bib.bib107)]
    predict actions from a single image by transforming it into a new representation
    called Ranked Saliency Map and Predicted Optical Flow. This representation is
    passed through a 2D convent and fc layers for final prediction. In [[108](#bib.bib108),
    [104](#bib.bib104)], the authors use temporal CNN (TCNNs) architectures, which
    are a series of 1D dilated convolutional layers designed to capture the temporal
    dependencies of feature representations, e.g. in the form of a vector representation
    of poses [[108](#bib.bib108)] or word-embeddings [[104](#bib.bib104)] describing
    the video frames. Chen et al. [[109](#bib.bib109)] use features of body parts
    generated by a CNN model and train an attention module whose objective is to activate
    only features that contribute to the prediction of the action. In [[112](#bib.bib112)],
    the authors use an action detection framework, which incrementally predicts the
    locations of the actors and the action classes based on the current detections.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 许多早期的动作预测方法采用前馈结构[[107](#bib.bib107)、[108](#bib.bib108)、[109](#bib.bib109)、[104](#bib.bib104)、[110](#bib.bib110)、[111](#bib.bib111)、[112](#bib.bib112)、[113](#bib.bib113)]。[[107](#bib.bib107)]的作者通过将单幅图像转化为称为排名显著性图和预测光流的新表示来预测动作。这一表示通过2D卷积层和全连接层进行最终预测。在[[108](#bib.bib108)、[104](#bib.bib104)]中，作者使用时序卷积神经网络（TCNNs）结构，这是一系列1D扩张卷积层，旨在捕捉特征表示的时间依赖性，例如姿势的向量表示[[108](#bib.bib108)]或描述视频帧的词嵌入[[104](#bib.bib104)]。陈等人[[109](#bib.bib109)]使用由CNN模型生成的身体部位特征，并训练一个注意力模块，目标是激活仅对动作预测有贡献的特征。在[[112](#bib.bib112)]中，作者使用一个动作检测框架，该框架基于当前检测逐步预测演员的位置和动作类别。
- en: The majority of the early action prediction algorithms pre-process the entire
    observed scenes using, e.g. different forms of convolutional neural networks [[98](#bib.bib98),
    [100](#bib.bib100), [107](#bib.bib107), [102](#bib.bib102), [104](#bib.bib104),
    [111](#bib.bib111), [105](#bib.bib105), [113](#bib.bib113)] or other forms of
    features [[106](#bib.bib106)]. Some algorithms complement these features using
    optical flow maps [[99](#bib.bib99), [104](#bib.bib104), [112](#bib.bib112)].
    Another group of action prediction methods focuses on specific parts of the observations.
    For example, in [[108](#bib.bib108), [101](#bib.bib101), [103](#bib.bib103), [110](#bib.bib110)]
    the authors use the changes in the poses of actors to predict their actions. The
    method in [[109](#bib.bib109)] uses body parts extracted by cropping a local patch
    around identified joints of the actors. The authors of [[113](#bib.bib113)] only
    use the visual appearances of actors extracted from detected bounding boxes, and
    the relationship between them.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的大多数动作预测算法通过例如不同形式的卷积神经网络[[98](#bib.bib98)、[100](#bib.bib100)、[107](#bib.bib107)、[102](#bib.bib102)、[104](#bib.bib104)、[111](#bib.bib111)、[105](#bib.bib105)、[113](#bib.bib113)]或其他特征形式[[106](#bib.bib106)]对整个观察场景进行预处理。一些算法通过光流图[[99](#bib.bib99)、[104](#bib.bib104)、[112](#bib.bib112)]来补充这些特征。另一组动作预测方法则专注于观察的特定部分。例如，在[[108](#bib.bib108)、[101](#bib.bib101)、[103](#bib.bib103)、[110](#bib.bib110)]中，作者利用演员姿势的变化来预测他们的动作。[[109](#bib.bib109)]中的方法使用通过裁剪识别关节周围的局部补丁提取的身体部位。[[113](#bib.bib113)]的作者仅使用从检测到的边界框中提取的演员的视觉外观以及它们之间的关系。
- en: 4.2.1 Summary
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 摘要
- en: Early action detection methods have many commonalities with action anticipation
    algorithms in terms of architectural design. However, there are some exceptions
    that are more applicable in this domain. These exceptions are teacher-student
    training schemes for knowledge distillation, identifying discriminative features,
    and recursive prediction/correction mechanisms. In addition, early action prediction
    algorithm, with a few exceptions, rely on single modal data for prediction and
    rarely use refinement frameworks such as attention modules. Adopting these techniques
    and operating on multi-modal feature spaces can further improve the performance
    of early action prediction algorithms. Unlike the action anticipation methods,
    there is no strong preference for recurrent or feedforward approaches. Some approaches
    take advantage of architectures such as temporal CNNs which are popular in the
    language processing domain and show their effectiveness for early action prediction
    tasks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 早期行动检测方法在架构设计方面与行动预测算法有许多共同之处。然而，在这个领域中，有一些例外情况更为适用。这些例外包括知识蒸馏的师生训练方案、识别区分特征和递归预测/校正机制。此外，早期行动预测算法，除了少数例外，通常依赖单一模态数据进行预测，很少使用像注意力模块这样的优化框架。采用这些技术并在多模态特征空间上操作可以进一步提升早期行动预测算法的性能。与行动预测方法不同，对于递归或前馈方法并没有强烈的偏好。一些方法利用了在语言处理领域流行的架构，例如时间卷积神经网络（CNN），并展示了它们在早期行动预测任务中的有效性。
- en: 5 Trajectory prediction
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 轨迹预测
- en: As the name implies, trajectory prediction algorithms predict the future trajectories
    of objects, i.e. the future positions of the objects over time. These approaches
    are particularly popular for applications such as intelligent driving and surveillance.
    Predicted trajectories can be used directly, e.g. in route planning for autonomous
    vehicles, or used for predicting future events, anomalies, or actions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名字所示，轨迹预测算法预测物体的未来轨迹，即物体随时间变化的未来位置。这些方法在智能驾驶和监控等应用中尤其受欢迎。预测的轨迹可以直接使用，例如用于自动驾驶车辆的路线规划，或者用于预测未来事件、异常或行动。
- en: In this section, we follow the same routine as before and focus on algorithms
    that have a deep learning component while acknowledging many recent works that
    have used classical approaches including Gaussian mixture models [[114](#bib.bib114),
    [115](#bib.bib115)] and processes [[116](#bib.bib116), [117](#bib.bib117)], Markov
    decision processes (MDPs) [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126)], Markov chains [[127](#bib.bib127), [128](#bib.bib128)]
    and other techniques [[129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们遵循之前的例程，重点关注具有深度学习组件的算法，同时承认许多最近使用经典方法的工作，包括高斯混合模型[[114](#bib.bib114)、[115](#bib.bib115)]和过程[[116](#bib.bib116)、[117](#bib.bib117)]、马尔可夫决策过程（MDP）[[118](#bib.bib118)、[119](#bib.bib119)、[120](#bib.bib120)、[121](#bib.bib121)、[122](#bib.bib122)、[123](#bib.bib123)、[124](#bib.bib124)、[125](#bib.bib125)、[126](#bib.bib126)]、马尔可夫链[[127](#bib.bib127)、[128](#bib.bib128)]和其他技术[[129](#bib.bib129)、[130](#bib.bib130)、[131](#bib.bib131)、[132](#bib.bib132)、[133](#bib.bib133)、[134](#bib.bib134)、[135](#bib.bib135)、[136](#bib.bib136)]。
- en: Trajectory prediction applications like many other sequence prediction tasks
    heavily rely on recurrent architectures such as LSTMs [[137](#bib.bib137), [87](#bib.bib87),
    [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145),
    [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153),
    [154](#bib.bib154), [155](#bib.bib155), [101](#bib.bib101), [156](#bib.bib156),
    [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)],
    and GRUs [[161](#bib.bib161), [162](#bib.bib162), [163](#bib.bib163), [164](#bib.bib164),
    [165](#bib.bib165), [166](#bib.bib166)]. These methods often use an encoder-decoder
    architecture in which a network, e.g. an LSTM encodes single- or multi-modal observations
    of the scenes for some time, and another network generates future trajectories
    given the encoding of the observations. Depending on the complexity of input data,
    these algorithms may rely on some form of pre-processing for generating features
    or embedding mechanisms to minimize the dimensionality of the data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹预测应用程序与许多其他序列预测任务一样，主要依赖于递归架构，例如 LSTMs [[137](#bib.bib137), [87](#bib.bib87),
    [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145),
    [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153),
    [154](#bib.bib154), [155](#bib.bib155), [101](#bib.bib101), [156](#bib.bib156),
    [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)]
    和 GRUs [[161](#bib.bib161), [162](#bib.bib162), [163](#bib.bib163), [164](#bib.bib164),
    [165](#bib.bib165), [166](#bib.bib166)]。这些方法通常使用编码器-解码器架构，其中一个网络，例如 LSTM，对场景的单模态或多模态观测进行编码，另一个网络则根据观测的编码生成未来的轨迹。根据输入数据的复杂性，这些算法可能依赖于某种形式的预处理来生成特征或嵌入机制，以最小化数据的维度。
- en: The feedforward algorithms [[161](#bib.bib161), [167](#bib.bib167), [168](#bib.bib168),
    [169](#bib.bib169), [170](#bib.bib170), [171](#bib.bib171), [172](#bib.bib172),
    [173](#bib.bib173), [77](#bib.bib77), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176)] often use whole views of the scenes (i.e. the environment
    and moving objects) and encode them using convolutional layers followed by a regression
    layer to predict trajectories. A few algorithms use hybrid approaches in which
    both convolutional and recurrent reasoning are also used [[177](#bib.bib177),
    [178](#bib.bib178)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈算法 [[161](#bib.bib161), [167](#bib.bib167), [168](#bib.bib168), [169](#bib.bib169),
    [170](#bib.bib170), [171](#bib.bib171), [172](#bib.bib172), [173](#bib.bib173),
    [77](#bib.bib77), [174](#bib.bib174), [175](#bib.bib175), [176](#bib.bib176)]
    通常使用整个场景的视图（即环境和移动对象），并通过卷积层进行编码，然后使用回归层来预测轨迹。一些算法使用混合方法，其中同时使用卷积和递归推理 [[177](#bib.bib177),
    [178](#bib.bib178)]。
- en: Depending on the prediction task, algorithms may rely on single- or multi-modal
    observations. For example, in the context of visual surveillance where a fixed
    camera provide a top-down or bird-eye viewing angle, many algorithms only use
    past trajectories of the agents in either actual 2D frame coordinates or their
    velocities calculated by the changes from each time step to another [[178](#bib.bib178),
    [139](#bib.bib139), [143](#bib.bib143), [145](#bib.bib145), [149](#bib.bib149),
    [151](#bib.bib151), [179](#bib.bib179), [153](#bib.bib153), [155](#bib.bib155),
    [164](#bib.bib164), [156](#bib.bib156), [180](#bib.bib180), [152](#bib.bib152),
    [159](#bib.bib159), [160](#bib.bib160)]. In addition to observations of individual
    trajectories of agents, these algorithms focus on modeling the interaction between
    the agents and how they impact each other. For example, Zhang et al. [[139](#bib.bib139)]
    use a state refinement module that aligns all pedestrians in the scene with a
    message passing mechanism that receives as input the current locations of the
    subjects and their encodings from an LSTM unit. In [[143](#bib.bib143)] a graph-based
    approach is used where pedestrians are considered as nodes and the interactions
    between them as edges of the graph. By aggregating information from neighboring
    nodes, the network learns to assign a different level of importance to each node
    for a given subject. The authors of [[153](#bib.bib153), [160](#bib.bib160)] perform
    a pooling operation on the generated representations by sharing the state of individual
    LSTMs that have spatial proximity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 根据预测任务的不同，算法可能依赖于单一或多模态的观察。例如，在视觉监控的背景下，当固定摄像头提供自上而下或鸟瞰角度时，许多算法仅使用代理的过去轨迹，这些轨迹可以是实际的2D帧坐标，或通过每个时间步变化计算出的速度[[178](#bib.bib178),
    [139](#bib.bib139), [143](#bib.bib143), [145](#bib.bib145), [149](#bib.bib149),
    [151](#bib.bib151), [179](#bib.bib179), [153](#bib.bib153), [155](#bib.bib155),
    [164](#bib.bib164), [156](#bib.bib156), [180](#bib.bib180), [152](#bib.bib152),
    [159](#bib.bib159), [160](#bib.bib160)]。除了对个体轨迹的观察，这些算法还重点建模代理之间的互动及其相互影响。例如，张等人[[139](#bib.bib139)]使用了一个状态细化模块，该模块通过接收当前主体位置及其来自LSTM单元的编码来对场景中的所有行人进行对齐，采用了消息传递机制。在[[143](#bib.bib143)]中，使用了基于图的方式，将行人视为节点，将它们之间的互动视为图的边。通过聚合来自邻近节点的信息，网络学习为每个节点分配不同的重要性等级。[[153](#bib.bib153),
    [160](#bib.bib160)]的作者通过共享具有空间接近性的各个LSTM的状态，对生成的表示进行池化操作。
- en: As shown in some works, other sources of information are used in surveilling
    objects [[87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [142](#bib.bib142),
    [146](#bib.bib146), [163](#bib.bib163), [154](#bib.bib154), [101](#bib.bib101),
    [156](#bib.bib156), [158](#bib.bib158), [165](#bib.bib165), [175](#bib.bib175),
    [176](#bib.bib176)]. For example, in addition to encoding the interactions with
    the environment, Liang et al. [[87](#bib.bib87)] use the semantic information
    of the scene as well as changes in the poses of the pedestrians. In [[138](#bib.bib138),
    [140](#bib.bib140), [142](#bib.bib142), [146](#bib.bib146), [163](#bib.bib163),
    [158](#bib.bib158), [165](#bib.bib165), [176](#bib.bib176)] the visual representations
    of the layout of the environment and the appearances of the subjects are included.
    The authors of [[156](#bib.bib156)] use an occupancy map which highlights the
    potential traversable locations for the subjects. The method in [[154](#bib.bib154)]
    takes into account pedestrians’ head orientations to estimate their fields of
    view in order to predict which subjects would potentially interact with one another.
    To predict interactions between humans, in [[101](#bib.bib101)] the authors use
    both poses and trajectories of the agents. Ma et al. [[175](#bib.bib175)] go one
    step further and take into account the pedestrians’ characteristics (e.g. age,
    gender) within a game-theoretic perspective to determine how the trajectory of
    one pedestrians impact each other.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如一些研究所示，监控对象时使用了其他信息来源[[87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140),
    [142](#bib.bib142), [146](#bib.bib146), [163](#bib.bib163), [154](#bib.bib154),
    [101](#bib.bib101), [156](#bib.bib156), [158](#bib.bib158), [165](#bib.bib165),
    [175](#bib.bib175), [176](#bib.bib176)]。例如，除了编码与环境的交互外，Liang等人[[87](#bib.bib87)]还使用了场景的语义信息以及行人的姿态变化。在[[138](#bib.bib138),
    [140](#bib.bib140), [142](#bib.bib142), [146](#bib.bib146), [163](#bib.bib163),
    [158](#bib.bib158), [165](#bib.bib165), [176](#bib.bib176)]中包含了环境布局的视觉表征和被监测对象的外观。[[156](#bib.bib156)]的作者使用了一个占用图，突出了被监测对象可能通过的区域。[[154](#bib.bib154)]中的方法考虑了行人的头部朝向，以估计他们的视野范围，从而预测哪些对象可能会相互互动。为了预测人类之间的互动，[[101](#bib.bib101)]的作者使用了代理的姿态和轨迹。Ma等人[[175](#bib.bib175)]进一步考虑了行人的特征（如年龄、性别），从博弈论的角度来确定一个行人的轨迹如何影响其他人。
- en: 'In the context of traffic understanding, predicting trajectories can be more
    challenging due to the fact that there is camera ego-motion involved (e.g. the
    prediction is from the perspective of a moving vehicle), there are interactions
    between different types of objects (e.g. vehicles and pedestrians), and there
    are certain constraints involved such as traffic rules, signals, etc. To achieve
    robustness, many methods in this domain take advantage of multi-modal data for
    trajectory prediction [[177](#bib.bib177), [137](#bib.bib137), [161](#bib.bib161),
    [141](#bib.bib141), [144](#bib.bib144), [162](#bib.bib162), [169](#bib.bib169),
    [147](#bib.bib147), [170](#bib.bib170), [181](#bib.bib181), [182](#bib.bib182),
    [163](#bib.bib163), [150](#bib.bib150), [168](#bib.bib168), [172](#bib.bib172),
    [183](#bib.bib183), [166](#bib.bib166), [173](#bib.bib173), [77](#bib.bib77),
    [165](#bib.bib165)]. In addition to using past trajectories, all these algorithms
    account for the road structure (whether it is from the perspective of the ego-vehicle
    or a top-down view) often in the form of raw visual inputs or, in some cases,
    as an occupancy map [[141](#bib.bib141), [173](#bib.bib173)]. The scene layout
    can implicitly capture the structure of the road, the appearances of the objects
    (e.g. shape) and the dynamics (e.g. velocity or locations of subjects). Such implicit
    information can be further augmented by explicit data such as the shapes of the
    objects (in the case of vehicles) [[177](#bib.bib177)], the speed [[144](#bib.bib144),
    [170](#bib.bib170), [183](#bib.bib183)] and steering angle [[170](#bib.bib170),
    [183](#bib.bib183)] of the ego-vehicle, the distance between the objects [[137](#bib.bib137),
    [182](#bib.bib182)], traffic rules [[182](#bib.bib182)] and signals [[77](#bib.bib77)],
    and kinematic constraints [[174](#bib.bib174)]. For example, the method in [[183](#bib.bib183)]
    uses a two-stream LSTM encoder-decoder scheme: first stream encodes the current
    ego-vehicle’s odometry (steering angle and speed) and the last observation of
    the scene and predicts future odometry of the vehicle. The second stream is a
    trajectory stream that jointly encodes location information of pedestrians and
    the ego-vehicle’s odometry and then combines the encoding with the prediction
    of the odometry stream to predict the future trajectories of the pedestrians.
    The method in [[144](#bib.bib144)] further extends this approach and adds an intention
    prediction stream which outputs how likely the observed pedestrian intends to
    cross the street. The intention likelihood is produced using an LSTM network that
    encodes the dynamics of the pedestrian, their appearances and their surroundings.
    Chandra et al. [[177](#bib.bib177)] create embeddings of contextual information
    by taking into account the shape and velocity of the road users and their spatial
    coordinates within a neighboring region. These embeddings are then fed into some
    LSTM networks followed by a number of convolutional layers to capture the dynamics
    of the scenes. In [[141](#bib.bib141)] the authors use separate LSTMs for encoding
    the trajectories of pedestrians and vehicles (as orientated bounding boxes) and
    then combine them into a unified framework by generating an occupancy map of the
    scene centered at each agent, followed by a pooling operation to capture the interactions
    between different subjects. Lee et al. [[165](#bib.bib165)] predict the future
    trajectories of vehicles in two steps: First, an encoder-decoder GRU architecture
    predicts future trajectories by observing the past ones. Then a refinement network
    adjusts the predicted trajectories by taking into account the contextual information
    in the forms of social interactions, dynamics of the agents involved, and the
    road structure.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在交通理解的背景下，由于涉及到摄像头自我运动（例如预测是从移动车辆的视角出发）、不同类型物体之间的互动（例如车辆和行人），以及交通规则、信号等特定约束，预测轨迹可能更具挑战性。为了实现鲁棒性，该领域的许多方法利用多模态数据进行轨迹预测[[177](#bib.bib177),
    [137](#bib.bib137), [161](#bib.bib161), [141](#bib.bib141), [144](#bib.bib144),
    [162](#bib.bib162), [169](#bib.bib169), [147](#bib.bib147), [170](#bib.bib170),
    [181](#bib.bib181), [182](#bib.bib182), [163](#bib.bib163), [150](#bib.bib150),
    [168](#bib.bib168), [172](#bib.bib172), [183](#bib.bib183), [166](#bib.bib166),
    [173](#bib.bib173), [77](#bib.bib77), [165](#bib.bib165)]。除了使用过去的轨迹，这些算法还考虑了道路结构（无论是从自我车辆的视角还是俯视图）通常以原始视觉输入的形式，或者在某些情况下，作为占用图[[141](#bib.bib141),
    [173](#bib.bib173)]。场景布局可以隐式捕捉道路的结构、物体的外观（例如形状）和动态（例如速度或位置）。这种隐式信息可以通过显式数据进一步增强，例如物体的形状（在车辆的情况下）[[177](#bib.bib177)]、自我车辆的速度[[144](#bib.bib144),
    [170](#bib.bib170), [183](#bib.bib183)]和转向角度[[170](#bib.bib170), [183](#bib.bib183)]、物体之间的距离[[137](#bib.bib137),
    [182](#bib.bib182)]、交通规则[[182](#bib.bib182)]和信号[[77](#bib.bib77)]，以及运动学约束[[174](#bib.bib174)]。例如，[[183](#bib.bib183)]中的方法使用了一个双流LSTM编码器-解码器方案：第一个流编码当前自我车辆的里程计（转向角度和速度）以及场景的最后观察，并预测车辆的未来里程计。第二个流是一个轨迹流，它联合编码行人的位置信息和自我车辆的里程计，然后将编码与里程计流的预测结合，以预测行人的未来轨迹。[[144](#bib.bib144)]中的方法进一步扩展了这种方法，并添加了一个意图预测流，输出观察到的行人打算过马路的可能性。意图可能性是通过一个LSTM网络生成的，该网络编码了行人的动态、外观和周围环境。Chandra等人[[177](#bib.bib177)]通过考虑道路使用者的形状和速度及其在邻近区域内的空间坐标，创建了上下文信息的嵌入。这些嵌入随后被输入到一些LSTM网络中，然后经过若干卷积层以捕捉场景的动态。在[[141](#bib.bib141)]中，作者使用独立的LSTM来编码行人和车辆（作为定向边界框）的轨迹，然后通过生成以每个代理为中心的场景占用图，将它们组合成一个统一的框架，接着进行池化操作以捕捉不同主体之间的互动。Lee等人[[165](#bib.bib165)]将车辆的未来轨迹预测分为两个步骤：首先，编码器-解码器GRU架构通过观察过去的轨迹预测未来轨迹。然后，细化网络通过考虑社会互动、相关代理的动态和道路结构的上下文信息来调整预测的轨迹。
- en: Similar to action prediction algorithms, attention modules have been widely
    used in trajectory prediction methods [[87](#bib.bib87), [138](#bib.bib138), [139](#bib.bib139),
    [143](#bib.bib143), [144](#bib.bib144), [146](#bib.bib146), [163](#bib.bib163),
    [152](#bib.bib152), [180](#bib.bib180), [164](#bib.bib164)]. For example, in [[87](#bib.bib87),
    [143](#bib.bib143)], the attention module jointly measures spatial and temporal
    interactions. The authors of [[138](#bib.bib138), [139](#bib.bib139), [146](#bib.bib146),
    [180](#bib.bib180)] propose the use of social attention modules which estimate
    the relative importance of interactions between the subject of interest and its
    neighboring subjects. The method in [[144](#bib.bib144)] uses two attention mechanisms,
    a temporal attention module that measures the importance of each timestep of the
    observed trajectories and a series of self-attention modules which are applied
    to encodings of observations prior to predictions. Xue et al. [[152](#bib.bib152)]
    propose an attention mechanism to measure the relative importance between different
    data modalities, namely the locations and velocities of subjects.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于动作预测算法，注意力模块在轨迹预测方法中得到了广泛应用[[87](#bib.bib87), [138](#bib.bib138), [139](#bib.bib139),
    [143](#bib.bib143), [144](#bib.bib144), [146](#bib.bib146), [163](#bib.bib163),
    [152](#bib.bib152), [180](#bib.bib180), [164](#bib.bib164)]。例如，在[[87](#bib.bib87),
    [143](#bib.bib143)]中，注意力模块共同测量空间和时间交互。[[138](#bib.bib138), [139](#bib.bib139),
    [146](#bib.bib146), [180](#bib.bib180)]的作者提出了使用社会注意力模块来估计目标与其邻近对象之间交互的相对重要性。[[144](#bib.bib144)]中的方法使用了两个注意力机制，一个时间注意力模块用于测量观察轨迹中每个时间步的重要性，另一个是一系列自注意力模块，这些模块应用于预测前的观察编码。[[152](#bib.bib152)]的Xue等人提出了一种注意力机制，用于测量不同数据模态之间的相对重要性，即对象的位置和速度。
- en: One of the major challenges in trajectory prediction is to model uncertainty
    of predictions, especially in scenarios where many possibilities exist (e.g. pedestrians
    at intersections). Some algorithms such as [[87](#bib.bib87), [139](#bib.bib139),
    [144](#bib.bib144), [152](#bib.bib152), [153](#bib.bib153), [160](#bib.bib160)]
    train the models by directly measuring the error between the ground truth trajectories
    and predicted ones, e.g. by using an L2 objective function. At the inference time,
    these algorithms generate deterministic predictions. To measure the uncertainty
    of predictions, these models are trained multiple times with randomly initialized
    parameters. Alternatively, uncertainty can be estimated via probabilistic objective
    functions, e.g. Gaussian log-likelihood, as in [[177](#bib.bib177), [161](#bib.bib161),
    [172](#bib.bib172), [142](#bib.bib142), [154](#bib.bib154), [183](#bib.bib183)]
    Instead of a single point in space, these algorithms predict a distribution that
    captures the uncertainty of the predictions. VAEs are another technique that can
    be used to estimate uncertainty [[161](#bib.bib161), [182](#bib.bib182), [163](#bib.bib163),
    [184](#bib.bib184), [165](#bib.bib165)]. Using these methods, at training time
    a posterior distribution over some latent space $z$ is learned by conditioning,
    for example, over future trajectories. At the inference time, a random sample
    is drawn from the latent space for the predictions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹预测的主要挑战之一是建模预测的不确定性，尤其是在存在多种可能性的场景中（例如，交叉路口的行人）。一些算法如[[87](#bib.bib87), [139](#bib.bib139),
    [144](#bib.bib144), [152](#bib.bib152), [153](#bib.bib153), [160](#bib.bib160)]通过直接测量真实轨迹和预测轨迹之间的误差来训练模型，例如，通过使用L2目标函数。在推理时，这些算法生成确定性的预测。为了测量预测的不确定性，这些模型通过随机初始化参数进行多次训练。另一种方法是通过概率目标函数来估计不确定性，例如，高斯对数似然，如[[177](#bib.bib177),
    [161](#bib.bib161), [172](#bib.bib172), [142](#bib.bib142), [154](#bib.bib154),
    [183](#bib.bib183)]。与单一空间点不同，这些算法预测一个捕捉预测不确定性的分布。变分自编码器（VAEs）是另一种可以用来估计不确定性的技术[[161](#bib.bib161),
    [182](#bib.bib182), [163](#bib.bib163), [184](#bib.bib184), [165](#bib.bib165)]。使用这些方法，在训练时，通过条件化（例如，未来轨迹）来学习一些潜在空间$z$的后验分布。在推理时，从潜在空间中随机抽取样本以进行预测。
- en: 'Since trajectory prediction algorithms are generative by nature, many approaches
    rely on adversarial training techniques [[178](#bib.bib178), [138](#bib.bib138),
    [140](#bib.bib140), [145](#bib.bib145), [148](#bib.bib148), [149](#bib.bib149),
    [163](#bib.bib163), [153](#bib.bib153), [164](#bib.bib164)] in which at training
    time a discriminator is used to predict whether the generated trajectories are
    real or fake. Kosaraju et al. [[146](#bib.bib146)] extend this approach by using
    two discriminators: A local discriminator which predicts the results on the output
    of the prediction using only past trajectories, and one global discriminator which
    operates on the output of the entire network, i.e. the prediction results based
    on trajectories and scene information.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于轨迹预测算法本质上是生成型的，许多方法依赖于对抗训练技术 [[178](#bib.bib178), [138](#bib.bib138), [140](#bib.bib140),
    [145](#bib.bib145), [148](#bib.bib148), [149](#bib.bib149), [163](#bib.bib163),
    [153](#bib.bib153), [164](#bib.bib164)]，在训练过程中，使用鉴别器来预测生成的轨迹是真实的还是伪造的。Kosaraju
    等人 [[146](#bib.bib146)] 通过使用两个鉴别器扩展了这一方法：一个局部鉴别器，仅使用过去的轨迹来预测预测结果，另一个全局鉴别器则作用于整个网络的输出，即基于轨迹和场景信息的预测结果。
- en: 5.1 Summary
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 总结
- en: Trajectory prediction is a widely studied field in the computer vision community.
    Although these works dominantly use recurrent network architectures, many approaches,
    such as those used in the field of traffic scene understanding, use feedforward
    networks. Trajectory prediction algorithms rely on one or more sources of information
    such as the past trajectories of subjects, surrounding visual context, object
    attributes, vehicle sensor readings, etc. One factor that is common in many of
    trajectory prediction algorithms is modeling the interactions between dynamic
    or dynamic and static objects. Relationships are captured explicitly or implicitly
    via encoding the scenes as a whole. Like many other prediction approaches, trajectory
    prediction algorithms benefit from various forms of attention mechanisms to learn
    the importance of spatial, temporal or social interactions between objects. To
    model uncertainty, techniques such as probabilistic objectives and variational
    encodings are used.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹预测是计算机视觉领域广泛研究的一个领域。尽管这些研究主要使用递归网络架构，但许多方法，例如在交通场景理解领域使用的方法，采用前馈网络。轨迹预测算法依赖于一个或多个信息源，例如主体的过去轨迹、周围的视觉上下文、物体属性、车辆传感器读数等。许多轨迹预测算法的共同因素是建模动态或动态与静态对象之间的交互。关系通过将场景整体进行显式或隐式编码来捕捉。像许多其他预测方法一样，轨迹预测算法通过各种形式的注意机制来学习物体之间的空间、时间或社会交互的重要性。为了建模不确定性，使用了概率目标和变分编码等技术。
- en: Trajectory prediction algorithms are predominantly rely on past trajectory information
    to predict the future. Although past motion observations are very informative,
    in some context, e.g. traffic scenes, they are simply not enough. There is a need
    for a more explicit encoding of contextual information such as road conditions,
    the subject’s attributes, rules and constraints, scene structure, etc. A number
    of approaches successfully have included a subset of these factors, but a more
    comprehensive approach should be considered.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹预测算法主要依赖于过去的轨迹信息来预测未来。尽管过去的运动观察非常有用，但在某些情况下，例如交通场景中，它们显然是不够的。需要更明确地编码上下文信息，例如道路条件、主体属性、规则和约束、场景结构等。许多方法成功地包括了这些因素的一个子集，但应该考虑更全面的方法。
- en: 6 Motion prediction
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 运动预测
- en: Although the term “motion prediction” in many cases is used to refer to future
    trajectory prediction, here we only consider the algorithms that are designed
    to predict changes in human pose. Motion prediction play a fundamental role in
    all prediction approaches as an intermediate step, e.g. to reflect how the future
    visual representations would look like or the types of actions to anticipate.
    Like many other prediction applications, this field is dominated by deep learning
    models, even though some methods still rely on classical techniques [[56](#bib.bib56),
    [48](#bib.bib48), [185](#bib.bib185)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在许多情况下，“运动预测”这一术语用于指代未来轨迹预测，但在这里我们仅考虑那些旨在预测人体姿态变化的算法。运动预测在所有预测方法中都扮演着基础性角色，例如，反映未来视觉表现的样子或预期的动作类型。像许多其他预测应用一样，这一领域主要由深度学习模型主导，尽管一些方法仍依赖于经典技术
    [[56](#bib.bib56), [48](#bib.bib48), [185](#bib.bib185)]。
- en: Similar to other prediction algorithms, motion prediction methods widely use
    recurrent architectures such as LSTMs [[186](#bib.bib186), [187](#bib.bib187),
    [188](#bib.bib188), [101](#bib.bib101), [103](#bib.bib103), [189](#bib.bib189),
    [26](#bib.bib26)] and GRUs [[190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194), [96](#bib.bib96), [195](#bib.bib195)]
    or a combination of both [[196](#bib.bib196)]. For example, in [[190](#bib.bib190)]
    the authors use a two-layer GRU model in which the top layer operates backward
    to learn noise processes and the bottom level is used to predict the poses given
    the past pose observations and the output of the top layer. Chiu et al. [[187](#bib.bib187)]
    propose a hierarchical LSTM architecture in which each layer of the network encodes
    the observed poses at different time-scales. In the context of 3D pose prediction,
    some algorithms rely on a two-stage process where the visual inputs, either as
    a single image [[189](#bib.bib189)] or a sequence of images [[188](#bib.bib188)],
    are fed into a recurrent network to predict 2D poses of the agent. This is followed
    by a refinement procedure that transforms the 2D poses into 3D.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于其他预测算法，运动预测方法广泛使用递归架构，如LSTM[[186](#bib.bib186), [187](#bib.bib187), [188](#bib.bib188),
    [101](#bib.bib101), [103](#bib.bib103), [189](#bib.bib189), [26](#bib.bib26)]和GRU[[190](#bib.bib190),
    [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193), [194](#bib.bib194),
    [96](#bib.bib96), [195](#bib.bib195)]，或者两者的结合[[196](#bib.bib196)]。例如，在[[190](#bib.bib190)]中，作者使用了一个两层GRU模型，其中顶层向后操作以学习噪声过程，底层用于根据过去的姿态观察和顶层的输出预测姿态。Chiu等人[[187](#bib.bib187)]提出了一种分层LSTM架构，其中网络的每一层在不同的时间尺度上编码观察到的姿态。在3D姿态预测的背景下，一些算法依赖于两阶段过程，其中视觉输入，无论是单张图像[[189](#bib.bib189)]还是一系列图像[[188](#bib.bib188)]，被输入到递归网络中以预测代理的2D姿态。然后进行一个精细化过程，将2D姿态转换为3D。
- en: Some approaches adopt feedforward architectures [[197](#bib.bib197), [198](#bib.bib198),
    [199](#bib.bib199), [200](#bib.bib200), [110](#bib.bib110)]. For example, the
    method in [[198](#bib.bib198)] uses two feedforward networks in a two-stage process.
    First, the input poses are fed into an autoencoder which is comprised of fully
    connected layerss (implemented by 1D convolutions with a kernel size of 1) and
    self-attention blocks. The encodings are then used by multi-level 2D convolutional
    blocks for final predictions. Zhang et al. [[199](#bib.bib199)] predict 3D poses
    from RGB videos. In their method, the images are converted to a feature space
    using a convolutional network, and then the features are used to learn a latent
    3D representation of 3D human dynamics. The representation is used by a network
    to predict future 3D poses. To capture movement patterns, a method proposed in
    [[197](#bib.bib197)] converts poses into a trajectory space using discrete cosine
    transformation. The newly formed representations are then used in a Graph-CNN
    framework to learn the dependencies between different joint trajectories.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法采用前馈架构[[197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200),
    [110](#bib.bib110)]。例如，[[198](#bib.bib198)]中的方法在两阶段过程中使用两个前馈网络。首先，将输入姿态输入到一个由完全连接层（通过1D卷积实现，卷积核大小为1）和自注意力块组成的自编码器中。然后，这些编码被多层2D卷积块用于最终预测。张等人[[199](#bib.bib199)]从RGB视频中预测3D姿态。在他们的方法中，图像通过卷积网络转换为特征空间，然后这些特征用于学习3D人类动态的潜在3D表示。这一表示被网络用来预测未来的3D姿态。为了捕捉运动模式，[[197](#bib.bib197)]中提出的方法将姿态转换为轨迹空间，使用离散余弦变换。新形成的表示随后被用于Graph-CNN框架中，以学习不同关节轨迹之间的依赖关系。
- en: To train motion prediction models, some authors use adversarial training methods
    in which a discriminator is used to classify whether the predicted poses are real
    or fake [[198](#bib.bib198), [193](#bib.bib193)]. The discrimination procedure
    can also be applied to evaluating the continuity, i.e. correct order of, predictions
    as demonstrated in [[192](#bib.bib192)]. In [[196](#bib.bib196)] the discrimination
    score is used to generate a policy for future action predictions in the context
    of imitation learning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练运动预测模型，一些作者使用对抗训练方法，其中使用判别器来分类预测的姿态是真实的还是虚假的[[198](#bib.bib198), [193](#bib.bib193)]。这一判别程序也可以用于评估预测的连续性，即预测的正确顺序，如[[192](#bib.bib192)]所示。在[[196](#bib.bib196)]中，判别评分用于生成未来动作预测的策略，这在模仿学习的背景下进行。
- en: 6.1 Summary
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 摘要
- en: Motion prediction algorithms primarily focus on the prediction of changes in
    the dynamics (i.e. poses) of observed agents. Such predictions can be fundamental
    to many other applications such as video or trajectory prediction tasks some of
    which were discussed previously.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 运动预测算法主要集中在预测观察到的代理的动态变化（即姿态）。这样的预测对许多其他应用，如视频或轨迹预测任务（其中一些已经讨论过），可能是基础性的。
- en: In recent works, recurrent network architectures are strongly preferred. The
    architecture of the choice often depends on the representation of the input data,
    e.g. whether joint coordinates are directly used or are encoded into a high-dimensional
    representation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的研究中，递归网络架构被强烈偏好。选择的架构通常取决于输入数据的表示方式，例如是否直接使用关节坐标或将其编码成高维表示。
- en: Despite the development of many successful motion prediction algorithms, the
    majority of these methods rely on a single source of information, for example,
    poses or scenes. Encoding higher-level contextual information, such as scene semantics,
    interactions, etc. can potentially result in more robust predictions, as shown
    in other prediction applications. Attention modules also, except for one instance,
    haven’t been adopted within motion prediction algorithms. Given the success of
    using attention in other prediction applications, motion prediction algorithms
    may benefit from the use of attention mechanisms.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多成功的运动预测算法已经得到开发，但这些方法中的大多数依赖于单一的信息源，例如姿态或场景。编码更高层次的上下文信息，如场景语义、交互等，可能会产生更为稳健的预测，这在其他预测应用中已有所示例。除了一个实例外，注意力模块也尚未被广泛应用于运动预测算法中。鉴于在其他预测应用中注意力机制的成功，运动预测算法可能会从使用注意力机制中受益。
- en: 7 Other applications
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 其他应用
- en: In the context of autonomous robotics, some algorithms are designed to predict
    occupancy grid maps (OGMs) that are grayscale representations of the robot’s surroundings
    showing which parts of the environment are traversable. These approaches are often
    object-agnostic and are concerned with generating future OGMs which are used by
    an autonomous agent to perform path planning. In recent years both classical [[201](#bib.bib201),
    [202](#bib.bib202), [203](#bib.bib203), [204](#bib.bib204)] and deep learning
    [[205](#bib.bib205), [206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [209](#bib.bib209)] methods are used. The deep learning approaches, in essence,
    are similar to video prediction methods in which the model receives as input a
    sequence of OGMs and predicts the future ones over some period of time. In this
    context both recurrent [[205](#bib.bib205), [207](#bib.bib207), [209](#bib.bib209)]
    and feedforward [[206](#bib.bib206), [208](#bib.bib208)] methods were common.
    Another group of generative approaches is semantic map prediction algorithms [[10](#bib.bib10),
    [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212)]. These algorithms
    receive as inputs RGB images of the scenes and predict future segmentation maps.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在自主机器人领域，一些算法被设计用来预测占用网格地图（OGMs），这些地图是机器人的周围环境的灰度表示，显示环境中哪些部分是可 traversable
    的。这些方法通常与物体无关，关注于生成未来的 OGMs，这些 OGMs 被自主代理用来进行路径规划。近年来，经典的 [[201](#bib.bib201),
    [202](#bib.bib202), [203](#bib.bib203), [204](#bib.bib204)] 和深度学习 [[205](#bib.bib205),
    [206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209)]
    方法都被使用。深度学习方法本质上类似于视频预测方法，其中模型接收一系列 OGMs 作为输入，并预测在一段时间内的未来 OGMs。在这种情况下，递归 [[205](#bib.bib205),
    [207](#bib.bib207), [209](#bib.bib209)] 和前馈 [[206](#bib.bib206), [208](#bib.bib208)]
    方法都很常见。另一组生成方法是语义地图预测算法 [[10](#bib.bib10), [210](#bib.bib210), [211](#bib.bib211),
    [212](#bib.bib212)]。这些算法接收场景的 RGB 图像作为输入，并预测未来的分割图。
- en: Some of the other vision-based prediction applications include weather [[213](#bib.bib213),
    [214](#bib.bib214)] and Solar irradiance forecasting [[215](#bib.bib215)], steering
    angle prediction [[212](#bib.bib212)], predicting the popularities of tweets based
    on tweeted images used and the users’ histories [[216](#bib.bib216)], forecasting
    fashion trends [[217](#bib.bib217)], storyline prediction [[8](#bib.bib8)], pain
    anticipation [[218](#bib.bib218)], predicting the effect of force after manipulating
    objects [[219](#bib.bib219)], forecasting the winner of Miss Universe given the
    appearances of contestants’ gowns [[220](#bib.bib220)], and predicting election
    results given the facial attributes of candidates [[221](#bib.bib221)]. These
    algorithms rely on a combination of techniques discussed earlier in this paper.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其他基于视觉的预测应用包括天气[[213](#bib.bib213), [214](#bib.bib214)]和太阳辐射预测[[215](#bib.bib215)]、转向角预测[[212](#bib.bib212)]、基于推文图像和用户历史的推文流行度预测[[216](#bib.bib216)]、时尚趋势预测[[217](#bib.bib217)]、故事情节预测[[8](#bib.bib8)]、疼痛预期[[218](#bib.bib218)]、操控物体后的力效应预测[[219](#bib.bib219)]、根据选手礼服预测环球小姐获胜者[[220](#bib.bib220)]以及根据候选人的面部特征预测选举结果[[221](#bib.bib221)]。这些算法依赖于本文前面讨论的多种技术的结合。
- en: 8 Prediction in other vision applications
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 其他视觉应用中的预测
- en: Before concluding our discussion on vision-prediction methods, it is worth mentioning
    that prediction techniques are also widely used in other visual processing tasks
    such as video summarization [[222](#bib.bib222)], anomaly detection [[223](#bib.bib223)],
    tracking [[224](#bib.bib224)], active object recognition [[225](#bib.bib225)],
    action detection [[226](#bib.bib226), [227](#bib.bib227)] and recognition [[228](#bib.bib228)].
    For example, tracking algorithms are very closely related to trajectory prediction
    ones and often rely on short term predictions to deal with gaps, e.g. due to occlusions,
    in tracking. For example, in [[224](#bib.bib224)] the method uses a recurrent
    framework to generate future frames in order to localize pedestrians in next frames.
    In the context of action detection, some methods rely on a future frame [[226](#bib.bib226)]
    or trajectory prediction of objects to detect actions [[227](#bib.bib227)]. In
    [[225](#bib.bib225)], a method is used for detecting an object in 3D by relying
    on predicting next best viewing angle of the object. Liu et al. [[223](#bib.bib223)]
    uses a video prediction framework to predict future motion flow maps and images.
    The future predictions that do not conform with expectations will be identified
    as abnormal.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结我们的视觉预测方法讨论之前，值得一提的是，预测技术也广泛应用于其他视觉处理任务，如视频摘要[[222](#bib.bib222)]、异常检测[[223](#bib.bib223)]、跟踪[[224](#bib.bib224)]、主动对象识别[[225](#bib.bib225)]、动作检测[[226](#bib.bib226),
    [227](#bib.bib227)]和识别[[228](#bib.bib228)]。例如，跟踪算法与轨迹预测算法密切相关，并且通常依赖于短期预测来处理跟踪中的间隙，例如由于遮挡。在[[224](#bib.bib224)]中，该方法使用递归框架生成未来帧以在下一帧中定位行人。在动作检测的背景下，一些方法依赖于未来帧[[226](#bib.bib226)]或对象的轨迹预测来检测动作[[227](#bib.bib227)]。在[[225](#bib.bib225)]中，采用一种方法通过预测对象的下一个最佳观察角度来检测3D对象。Liu等人[[223](#bib.bib223)]使用视频预测框架来预测未来的运动流图和图像。不符合预期的未来预测将被识别为异常。
- en: 9 The evaluation of state-of-the-art
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 最先进技术的评估
- en: 'When it comes to the evaluation of algorithms, there are two important factors:
    metrics and datasets. They highlight the strengths and weaknesses of the algorithms
    and provide a means to compare the relative performances of the methods. Given
    the importance of these two factors in the design of algorithms, we dedicate the
    following sections to discussing the common metrics and datasets used for vision-based
    prediction tasks. Since the datasets and metrics used in these applications are
    highly diverse, we will focus our discussion on some of the main ones for each
    prediction category while providing visual aids to summarize what the past works
    used for evaluation purposes.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法评估方面，有两个重要因素：度量标准和数据集。这些因素突出了算法的优缺点，并提供了比较方法相对性能的手段。鉴于这两个因素在算法设计中的重要性，我们将以下章节专门讨论用于基于视觉的预测任务的常见度量标准和数据集。由于这些应用中使用的数据集和度量标准高度多样，我们将重点讨论每个预测类别中的一些主要标准，并提供视觉辅助工具来总结过去的工作用于评估的情况。
- en: 10 Metrics
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 度量标准
- en: '![Refer to caption](img/123c23fed9399d1b22083411fe9de376.png)![Refer to caption](img/5a7e73c63d3321a43fda47325af9d3a0.png)![Refer
    to caption](img/6643993049cdc15ab18e9731688f7e11.png)![Refer to caption](img/422842051f851883ea64ff8a39b50505.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/123c23fed9399d1b22083411fe9de376.png)![参见说明](img/5a7e73c63d3321a43fda47325af9d3a0.png)![参见说明](img/6643993049cdc15ab18e9731688f7e11.png)![参见说明](img/422842051f851883ea64ff8a39b50505.png)'
- en: 'Figure 1: Metrics used in vision-based prediction applications. From left to
    right: Video, action, trajectory and motion prediction.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：视觉预测应用中使用的指标。从左到右：视频、动作、轨迹和运动预测。
- en: 'In this section, we follow the same routine as the discussion on the past works
    and divide the metrics into different categories. A summary of the metrics can
    be found in Figure [1](#S10.F1 "Figure 1 ‣ 10 Metrics ‣ Deep Learning for Vision-based
    Prediction: A Survey"). The interested readers can also refer to Appendix [B](#A2
    "Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey") for further information regarding the metrics and the the
    papers that used them. Note while we discuss the metrics in each category and
    we only provide mathematical expressions of the most popular metrics in Appendix
    [C](#A3 "Appendix C Metric formulas ‣ Deep Learning for Vision-based Prediction:
    A Survey").'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们遵循与之前工作讨论相同的例程，将指标划分为不同类别。有关指标的总结可以在图[1](#S10.F1 "Figure 1 ‣ 10 Metrics
    ‣ Deep Learning for Vision-based Prediction: A Survey")中找到。感兴趣的读者还可以参考附录[B](#A2
    "Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey")，获取有关指标及其应用论文的更多信息。请注意，在讨论每个类别的指标时，我们仅在附录[C](#A3 "Appendix
    C Metric formulas ‣ Deep Learning for Vision-based Prediction: A Survey")中提供了最流行指标的数学表达式。'
- en: 10.1 Video prediction
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1 视频预测
- en: Video prediction is about generating realistic images, hence the best performance
    is achieved when the disparities between the generated images and groundtruth
    images are minimal. The most straightforward way of computing the disparity is
    to measure pixel-wise error using a Mean Square Error (MSE) [[2](#bib.bib2), [4](#bib.bib4),
    [20](#bib.bib20), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12), [229](#bib.bib229),
    [23](#bib.bib23), [14](#bib.bib14), [7](#bib.bib7), [6](#bib.bib6), [27](#bib.bib27),
    [30](#bib.bib30)], which computes average squared intensity differences between
    pixels. Another more popular metric related to MSE is Peak Signal-to-Noise Ratio
    (PSNR) [[2](#bib.bib2), [1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [5](#bib.bib5), [32](#bib.bib32), [33](#bib.bib33),
    [22](#bib.bib22), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12), [229](#bib.bib229),
    [24](#bib.bib24), [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [36](#bib.bib36),
    [6](#bib.bib6), [15](#bib.bib15), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)].
    PSNR is the ratio of the maximum pixel value (i.e. possible signal power), e.g.
    255 in 8-bit images, divided by the MSE (or power of distorting noise) measure
    of two images. The lower the error between two images, the higher the value of
    PSNR, and consequently, the higher the quality of the generated images. Because
    of the wide dynamic range of signals, PSNR value is expressed in the logarithmic
    decibel scale.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 视频预测涉及生成逼真的图像，因此，当生成的图像与真实图像之间的差异最小时，性能最佳。计算差异最直接的方法是使用均方误差（MSE）[[2](#bib.bib2),
    [4](#bib.bib4), [20](#bib.bib20), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12),
    [229](#bib.bib229), [23](#bib.bib23), [14](#bib.bib14), [7](#bib.bib7), [6](#bib.bib6),
    [27](#bib.bib27), [30](#bib.bib30)]，该方法计算像素间的平均平方强度差异。与MSE相关的另一个更流行的指标是峰值信噪比（PSNR）[[2](#bib.bib2),
    [1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [5](#bib.bib5), [32](#bib.bib32), [33](#bib.bib33), [22](#bib.bib22),
    [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12), [229](#bib.bib229), [24](#bib.bib24),
    [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [36](#bib.bib36), [6](#bib.bib6),
    [15](#bib.bib15), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)]。PSNR是最大像素值（即可能的信号功率）的比率，例如在8位图像中为255，除以MSE（或图像失真噪声的功率）度量。两幅图像之间的误差越小，PSNR值越高，从而生成的图像质量也越高。由于信号的动态范围很广，PSNR值以对数分贝尺度表示。
- en: Although MSE, and PSNR metrics are easy to calculate, they cannot measure the
    perceived visual quality of a generated image. An alternative metric to address
    this issue is Structural SIMilarity (SSIM) index ([[230](#bib.bib230)]) [[2](#bib.bib2),
    [3](#bib.bib3), [1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [5](#bib.bib5), [32](#bib.bib32), [33](#bib.bib33),
    [22](#bib.bib22), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12), [24](#bib.bib24),
    [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [6](#bib.bib6), [15](#bib.bib15),
    [27](#bib.bib27), [29](#bib.bib29)] which is designed to model image distortion.
    To capture the structural differences between the two images, SSIM separates illumination
    information as it is independent of objects’ structures. As a result, the similarity
    is measured by a combination of three comparisons, namely luminance, contrast,
    and structure.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MSE和PSNR指标容易计算，但它们无法测量生成图像的感知视觉质量。解决这个问题的替代指标是结构相似性（SSIM）指数 ([[230](#bib.bib230)])
    [[2](#bib.bib2), [3](#bib.bib3), [1](#bib.bib1), [4](#bib.bib4), [31](#bib.bib31),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [5](#bib.bib5), [32](#bib.bib32),
    [33](#bib.bib33), [22](#bib.bib22), [34](#bib.bib34), [16](#bib.bib16), [12](#bib.bib12),
    [24](#bib.bib24), [13](#bib.bib13), [14](#bib.bib14), [35](#bib.bib35), [6](#bib.bib6),
    [15](#bib.bib15), [27](#bib.bib27), [29](#bib.bib29)]，它旨在模拟图像失真。为了捕捉两幅图像之间的结构差异，SSIM将光照信息分离，因为它与对象的结构无关。因此，相似度通过三种比较的组合来衡量，即亮度、对比度和结构。
- en: Higher-level contextual similarities may not be captured by distance measures
    on pixel values. More recently proposed metric, Learned Perceptual Image Patch
    Similarity (LPIPS), ([[231](#bib.bib231)])[[3](#bib.bib3), [17](#bib.bib17), [37](#bib.bib37),
    [11](#bib.bib11)] measures the similarity between two images by comparing internal
    activations of convolutional networks trained for high-level classification tasks.
    The value is calculated by an average L2 distance over normalized deep features.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级的上下文相似性可能无法通过像素值上的距离度量来捕捉。最近提出的指标，学习感知图像块相似性（LPIPS），([[231](#bib.bib231)])[[3](#bib.bib3),
    [17](#bib.bib17), [37](#bib.bib37), [11](#bib.bib11)]通过比较训练用于高层分类任务的卷积网络的内部激活来测量两幅图像之间的相似性。该值通过归一化深度特征上的平均L2距离计算得出。
- en: Some other metrics that have been used in the literature are qualitative human
    judgment [[11](#bib.bib11), [25](#bib.bib25), [8](#bib.bib8), [28](#bib.bib28)],
    Frechet Video Distance (FVD) ([[232](#bib.bib232)]) [[3](#bib.bib3), [18](#bib.bib18)],
    Maximum Mean Discrepancy (MMD) [[26](#bib.bib26)], Inception Scores (IS) ([[233](#bib.bib233)])
    [[26](#bib.bib26)], Binary Cross Entropy (BCE) [[23](#bib.bib23)], L1 [[9](#bib.bib9),
    [12](#bib.bib12)], and Root MSE (RMSE) [[11](#bib.bib11)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中使用的一些其他指标包括定性人工判断 [[11](#bib.bib11), [25](#bib.bib25), [8](#bib.bib8), [28](#bib.bib28)]，Frechet视频距离（FVD）
    ([[232](#bib.bib232)]) [[3](#bib.bib3), [18](#bib.bib18)]，最大均值差异（MMD） [[26](#bib.bib26)]，Inception分数（IS）
    ([[233](#bib.bib233)]) [[26](#bib.bib26)]，二元交叉熵（BCE） [[23](#bib.bib23)]，L1 [[9](#bib.bib9),
    [12](#bib.bib12)]，以及均方根误差（RMSE） [[11](#bib.bib11)]。
- en: 10.2 Action prediction
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2 动作预测
- en: Similar to classification tasks, many action prediction algorithms use accuracy
    measure to report on the performance that is the ratio of the correct predictions
    with respect to the total number of predictions [[56](#bib.bib56), [63](#bib.bib63),
    [64](#bib.bib64), [99](#bib.bib99), [100](#bib.bib100), [66](#bib.bib66), [71](#bib.bib71),
    [9](#bib.bib9), [74](#bib.bib74), [48](#bib.bib48), [49](#bib.bib49), [107](#bib.bib107),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [108](#bib.bib108), [83](#bib.bib83),
    [101](#bib.bib101), [109](#bib.bib109), [89](#bib.bib89), [102](#bib.bib102),
    [75](#bib.bib75), [103](#bib.bib103), [97](#bib.bib97), [76](#bib.bib76), [91](#bib.bib91),
    [110](#bib.bib110), [111](#bib.bib111), [85](#bib.bib85), [105](#bib.bib105),
    [86](#bib.bib86), [70](#bib.bib70), [50](#bib.bib50), [112](#bib.bib112), [8](#bib.bib8),
    [104](#bib.bib104), [93](#bib.bib93), [41](#bib.bib41), [94](#bib.bib94), [52](#bib.bib52),
    [59](#bib.bib59), [42](#bib.bib42), [106](#bib.bib106), [61](#bib.bib61), [113](#bib.bib113),
    [54](#bib.bib54), [43](#bib.bib43), [44](#bib.bib44), [95](#bib.bib95), [62](#bib.bib62),
    [45](#bib.bib45), [55](#bib.bib55), [46](#bib.bib46), [40](#bib.bib40), [47](#bib.bib47)].
    Despite being used widely, accuracy on its own is not a strong indicator of performance,
    especially when we are dealing with class-imbalance data. This is because, for
    example, the model can simply favor the more represented class and predict every
    input as that class. This then would result in a high accuracy measure because
    the metric only considers the ratio of correct predictions. To address these shortcomings,
    some works use complimentary metrics that, in addition to correct predictions,
    account for different types of false predictions. These metrics are precision
    [[63](#bib.bib63), [71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [74](#bib.bib74),
    [67](#bib.bib67), [82](#bib.bib82), [83](#bib.bib83), [70](#bib.bib70), [58](#bib.bib58),
    [51](#bib.bib51), [96](#bib.bib96), [80](#bib.bib80), [52](#bib.bib52), [42](#bib.bib42),
    [53](#bib.bib53)], recall [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65),
    [71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [74](#bib.bib74), [67](#bib.bib67),
    [68](#bib.bib68), [82](#bib.bib82), [83](#bib.bib83), [77](#bib.bib77), [70](#bib.bib70),
    [58](#bib.bib58), [51](#bib.bib51), [96](#bib.bib96), [80](#bib.bib80), [52](#bib.bib52),
    [42](#bib.bib42), [53](#bib.bib53)], and Area Under the Curve (AUC) [[71](#bib.bib71),
    [112](#bib.bib112), [54](#bib.bib54)] of precision-recall graph. Precision and
    recall also form the basis for the calculation of some higher level metrics such
    as F1-score [[71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [83](#bib.bib83),
    [90](#bib.bib90), [58](#bib.bib58), [96](#bib.bib96), [52](#bib.bib52), [42](#bib.bib42)],
    Average Precision (AP) [[88](#bib.bib88), [9](#bib.bib9), [73](#bib.bib73), [82](#bib.bib82),
    [79](#bib.bib79)] and its variations mean AP (mAP) [[87](#bib.bib87), [107](#bib.bib107),
    [83](#bib.bib83), [77](#bib.bib77), [84](#bib.bib84), [78](#bib.bib78), [92](#bib.bib92),
    [112](#bib.bib112)] and calibrated AP (cAP) [[92](#bib.bib92)]. Some of the less
    common performance metrics are Matthews Correlation Coefficient (MCC) [[76](#bib.bib76)],
    False positive (FP)[[53](#bib.bib53)], True Positive Rate (TPR) and False Positive
    Rate (FPR) [[47](#bib.bib47)], Prediction Power (PP) [[57](#bib.bib57)], and Mean
    Reciprocal Rank (MRR) [[44](#bib.bib44)].
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类任务类似，许多动作预测算法使用准确率来报告性能，即正确预测与总预测数量的比率 [[56](#bib.bib56), [63](#bib.bib63),
    [64](#bib.bib64), [99](#bib.bib99), [100](#bib.bib100), [66](#bib.bib66), [71](#bib.bib71),
    [9](#bib.bib9), [74](#bib.bib74), [48](#bib.bib48), [49](#bib.bib49), [107](#bib.bib107),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [108](#bib.bib108), [83](#bib.bib83),
    [101](#bib.bib101), [109](#bib.bib109), [89](#bib.bib89), [102](#bib.bib102),
    [75](#bib.bib75), [103](#bib.bib103), [97](#bib.bib97), [76](#bib.bib76), [91](#bib.bib91),
    [110](#bib.bib110), [111](#bib.bib111), [85](#bib.bib85), [105](#bib.bib105),
    [86](#bib.bib86), [70](#bib.bib70), [50](#bib.bib50), [112](#bib.bib112), [8](#bib.bib8),
    [104](#bib.bib104), [93](#bib.bib93), [41](#bib.bib41), [94](#bib.bib94), [52](#bib.bib52),
    [59](#bib.bib59), [42](#bib.bib42), [106](#bib.bib106), [61](#bib.bib61), [113](#bib.bib113),
    [54](#bib.bib54), [43](#bib.bib43), [44](#bib.bib44), [95](#bib.bib95), [62](#bib.bib62),
    [45](#bib.bib45), [55](#bib.bib55), [46](#bib.bib46), [40](#bib.bib40), [47](#bib.bib47)]。尽管使用广泛，但准确率本身并不是性能的强指标，特别是在处理类别不平衡的数据时。这是因为，例如，模型可以简单地偏向于更多表示的类别，并将每个输入预测为该类别。这将导致高准确率，因为该指标只考虑正确预测的比率。为了应对这些缺点，一些研究使用了补充指标，这些指标除了正确预测之外，还考虑了不同类型的错误预测。这些指标包括精确率
    [[63](#bib.bib63), [71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [74](#bib.bib74),
    [67](#bib.bib67), [82](#bib.bib82), [83](#bib.bib83), [70](#bib.bib70), [58](#bib.bib58),
    [51](#bib.bib51), [96](#bib.bib96), [80](#bib.bib80), [52](#bib.bib52), [42](#bib.bib42),
    [53](#bib.bib53)]、召回率 [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [71](#bib.bib71),
    [72](#bib.bib72), [9](#bib.bib9), [74](#bib.bib74), [67](#bib.bib67), [68](#bib.bib68),
    [82](#bib.bib82), [83](#bib.bib83), [77](#bib.bib77), [70](#bib.bib70), [58](#bib.bib58),
    [51](#bib.bib51), [96](#bib.bib96), [80](#bib.bib80), [52](#bib.bib52), [42](#bib.bib42),
    [53](#bib.bib53)]和精确召回曲线下面积（AUC） [[71](#bib.bib71), [112](#bib.bib112), [54](#bib.bib54)]。精确率和召回率也构成了计算一些更高级指标的基础，如F1分数
    [[71](#bib.bib71), [72](#bib.bib72), [9](#bib.bib9), [83](#bib.bib83), [90](#bib.bib90),
    [58](#bib.bib58), [96](#bib.bib96), [52](#bib.bib52), [42](#bib.bib42)]、平均精度（AP）
    [[88](#bib.bib88), [9](#bib.bib9), [73](#bib.bib73), [82](#bib.bib82), [79](#bib.bib79)]及其变体均值AP（mAP）
    [[87](#bib.bib87), [107](#bib.bib107), [83](#bib.bib83), [77](#bib.bib77), [84](#bib.bib84),
    [78](#bib.bib78), [92](#bib.bib92), [112](#bib.bib112)]和校准AP（cAP） [[92](#bib.bib92)]。一些不太常见的性能指标包括马修斯相关系数（MCC）
    [[76](#bib.bib76)]、假阳性（FP）[[53](#bib.bib53)]、真正率（TPR）和假阳性率（FPR） [[47](#bib.bib47)]、预测能力（PP）
    [[57](#bib.bib57)]以及均值倒数排名（MRR） [[44](#bib.bib44)]。
- en: Depending on the application, some algorithms evaluate the timing factor in
    terms of the Run Time (RT) of the model [[9](#bib.bib9), [73](#bib.bib73), [43](#bib.bib43)]
    or time of the event, e.g. beginning of the next activity [[70](#bib.bib70)],
    Time To Accident (or collision) (TTA) [[81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [84](#bib.bib84)], and, in the context of driving, Time To Maneuver (TTM) [[74](#bib.bib74),
    [49](#bib.bib49), [96](#bib.bib96), [80](#bib.bib80), [53](#bib.bib53)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用的不同，一些算法通过模型的运行时间（RT）[[9](#bib.bib9), [73](#bib.bib73), [43](#bib.bib43)]
    或事件时间来评估时间因素，例如下一活动的开始时间[[70](#bib.bib70)]，事故时间（或碰撞时间）（TTA）[[81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)]，以及在驾驶环境中的机动时间（TTM）[[74](#bib.bib74),
    [49](#bib.bib49), [96](#bib.bib96), [80](#bib.bib80), [53](#bib.bib53)]。
- en: 10.3 Trajectory prediction
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3 轨迹预测
- en: Perhaps the most popular performance measure for trajectory prediction is Average
    Displacement Error (ADE) [[182](#bib.bib182), [171](#bib.bib171), [101](#bib.bib101),
    [234](#bib.bib234), [159](#bib.bib159)] calculated as the average error between
    the prediction location and the ground truth over all time steps. Some methods
    complement ADE measure with its extension Final Displacement Error (FDE) [[177](#bib.bib177),
    [87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [146](#bib.bib146), [163](#bib.bib163),
    [168](#bib.bib168), [172](#bib.bib172), [152](#bib.bib152), [153](#bib.bib153),
    [155](#bib.bib155), [164](#bib.bib164), [180](#bib.bib180), [158](#bib.bib158),
    [160](#bib.bib160)]. As the name suggests, FDE only measures the error between
    the ground truth and the generated trajectory for the final time step.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最受欢迎的轨迹预测性能度量是平均位移误差（ADE）[[182](#bib.bib182), [171](#bib.bib171), [101](#bib.bib101),
    [234](#bib.bib234), [159](#bib.bib159)]，其计算为预测位置与实际位置在所有时间步长上的平均误差。一些方法通过其扩展的最终位移误差（FDE）[[177](#bib.bib177),
    [87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [146](#bib.bib146), [163](#bib.bib163),
    [168](#bib.bib168), [172](#bib.bib172), [152](#bib.bib152), [153](#bib.bib153),
    [155](#bib.bib155), [164](#bib.bib164), [180](#bib.bib180), [158](#bib.bib158),
    [160](#bib.bib160)]来补充ADE度量。顾名思义，FDE仅测量实际位置与生成轨迹在最终时间步长上的误差。
- en: Many other works use the same metric as ADE [[161](#bib.bib161), [170](#bib.bib170),
    [148](#bib.bib148), [150](#bib.bib150), [129](#bib.bib129), [183](#bib.bib183),
    [184](#bib.bib184), [118](#bib.bib118), [119](#bib.bib119), [121](#bib.bib121),
    [127](#bib.bib127), [115](#bib.bib115), [132](#bib.bib132), [117](#bib.bib117),
    [123](#bib.bib123)] or ADE/FDE [[139](#bib.bib139), [144](#bib.bib144), [169](#bib.bib169),
    [151](#bib.bib151), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [176](#bib.bib176)] without using the same terminology. It is also a common practice
    that instead of using average or final time step measures, to calculate the error
    at different time steps over a period of time [[140](#bib.bib140), [147](#bib.bib147),
    [114](#bib.bib114), [173](#bib.bib173), [130](#bib.bib130), [156](#bib.bib156),
    [120](#bib.bib120), [77](#bib.bib77), [165](#bib.bib165), [124](#bib.bib124),
    [133](#bib.bib133), [125](#bib.bib125), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他工作使用与ADE相同的度量[[161](#bib.bib161), [170](#bib.bib170), [148](#bib.bib148),
    [150](#bib.bib150), [129](#bib.bib129), [183](#bib.bib183), [184](#bib.bib184),
    [118](#bib.bib118), [119](#bib.bib119), [121](#bib.bib121), [127](#bib.bib127),
    [115](#bib.bib115), [132](#bib.bib132), [117](#bib.bib117), [123](#bib.bib123)]
    或 ADE/FDE [[139](#bib.bib139), [144](#bib.bib144), [169](#bib.bib169), [151](#bib.bib151),
    [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131), [176](#bib.bib176)]，但未使用相同的术语。还常见的做法是，不使用平均或最终时间步长度量，而是计算一段时间内不同时间步的误差[[140](#bib.bib140),
    [147](#bib.bib147), [114](#bib.bib114), [173](#bib.bib173), [130](#bib.bib130),
    [156](#bib.bib156), [120](#bib.bib120), [77](#bib.bib77), [165](#bib.bib165),
    [124](#bib.bib124), [133](#bib.bib133), [125](#bib.bib125), [134](#bib.bib134),
    [135](#bib.bib135), [136](#bib.bib136)]。
- en: To measure displacement error in probablistic trajectory prediction algorithms,
    some works generate a set number of samples and report the best measure (i.e.
    minimum error) [[161](#bib.bib161), [165](#bib.bib165), [178](#bib.bib178), [168](#bib.bib168),
    [162](#bib.bib162), [166](#bib.bib166)] or average over all samples [[174](#bib.bib174),
    [162](#bib.bib162), [166](#bib.bib166)]. Depending on the error metric used, some
    refer to these measures as Minimum ADE/FDE (MinADE/FDE)[[137](#bib.bib137), [178](#bib.bib178),
    [168](#bib.bib168)] (using Euclidean distance) or Mean/Minimum Mean Square Displacement
    (Mean/MinMSD) [[162](#bib.bib162), [166](#bib.bib166)] (using MSE). Some of the
    other probabilistic measures are Log-Likelihood (LL) [[145](#bib.bib145), [181](#bib.bib181),
    [120](#bib.bib120)], Negative Log-Likelihood (NLL) [[172](#bib.bib172), [183](#bib.bib183),
    [174](#bib.bib174), [175](#bib.bib175), [123](#bib.bib123), [125](#bib.bib125)],
    Kullback–Leibler Divergence (KLD) [[181](#bib.bib181), [127](#bib.bib127), [125](#bib.bib125)],
    Negative Log-Probability (NLP)[[118](#bib.bib118), [119](#bib.bib119)], Cross
    Entropy (CE) [[166](#bib.bib166)], Average Prediction Probability (APP) [[157](#bib.bib157)].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量概率轨迹预测算法中的位移误差，一些工作生成一定数量的样本，并报告最佳度量（即最小误差）[[161](#bib.bib161), [165](#bib.bib165),
    [178](#bib.bib178), [168](#bib.bib168), [162](#bib.bib162), [166](#bib.bib166)]，或者对所有样本取平均[[174](#bib.bib174),
    [162](#bib.bib162), [166](#bib.bib166)]。根据所使用的误差度量，一些研究将这些度量称为最小ADE/FDE（MinADE/FDE）[[137](#bib.bib137),
    [178](#bib.bib178), [168](#bib.bib168)]（使用欧几里得距离）或平均/最小均方位移（Mean/MinMSD）[[162](#bib.bib162),
    [166](#bib.bib166)]（使用MSE）。其他概率度量包括对数似然（LL）[[145](#bib.bib145), [181](#bib.bib181),
    [120](#bib.bib120)]、负对数似然（NLL）[[172](#bib.bib172), [183](#bib.bib183), [174](#bib.bib174),
    [175](#bib.bib175), [123](#bib.bib123), [125](#bib.bib125)]、Kullback–Leibler散度（KLD）[[181](#bib.bib181),
    [127](#bib.bib127), [125](#bib.bib125)]、负对数概率（NLP）[[118](#bib.bib118), [119](#bib.bib119)]、交叉熵（CE）[[166](#bib.bib166)]、平均预测概率（APP）[[157](#bib.bib157)]。
- en: Performance can also be evaluated using common classification metrics. For example,
    in [[161](#bib.bib161), [128](#bib.bib128)] Hit Rate (HR) and in [[184](#bib.bib184),
    [165](#bib.bib165)] Miss Rate (MR) metrics are used. In these cases, if the predicted
    trajectory is below (or above) a certain distance threshold from the groundtruth,
    it is considered as a hit or miss. Following a similar approach, some authors
    calculate accuracy [[167](#bib.bib167), [127](#bib.bib127)] or precision [[115](#bib.bib115)]
    of predictions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 性能还可以使用常见的分类度量进行评估。例如，在[[161](#bib.bib161), [128](#bib.bib128)]中使用了命中率（HR），而在[[184](#bib.bib184),
    [165](#bib.bib165)]中使用了漏检率（MR）度量。在这些情况下，如果预测轨迹与真实值的距离低于（或高于）某个阈值，则被视为命中或漏检。采用类似的方法，一些作者计算了准确率[[167](#bib.bib167),
    [127](#bib.bib127)]或预测的精确度[[115](#bib.bib115)]。
- en: Some of the other metrics used in the literature are Run Time (RT) [[147](#bib.bib147),
    [118](#bib.bib118), [121](#bib.bib121), [123](#bib.bib123), [135](#bib.bib135)],
    Average Non-linear Displacement Error (ANDE) [[155](#bib.bib155), [160](#bib.bib160)],
    Maximum Distance (MaxD) [[165](#bib.bib165), [184](#bib.bib184)], State collision
    rate (SCR) [[175](#bib.bib175)], Percentage Deviated (PD) [[122](#bib.bib122)],
    Distance to Goal (DtG) [[125](#bib.bib125)], Fraction of Near Misses (FNM) [[126](#bib.bib126)],
    Expected Calibration Error (ECE) [[172](#bib.bib172)], and qualitative (Q) [[116](#bib.bib116)].
    A few works predict the orientations of pedestrians [[154](#bib.bib154), [234](#bib.bib234),
    [131](#bib.bib131)] or vehicles [[77](#bib.bib77)], therefore also report performance
    using Mean angular error (MAnE).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中使用的一些其他度量包括运行时间（RT）[[147](#bib.bib147), [118](#bib.bib118), [121](#bib.bib121),
    [123](#bib.bib123), [135](#bib.bib135)]、平均非线性位移误差（ANDE）[[155](#bib.bib155), [160](#bib.bib160)]、最大距离（MaxD）[[165](#bib.bib165),
    [184](#bib.bib184)]、状态碰撞率（SCR）[[175](#bib.bib175)]、偏差百分比（PD）[[122](#bib.bib122)]、到目标的距离（DtG）[[125](#bib.bib125)]、接近错过的比例（FNM）[[126](#bib.bib126)]、期望校准误差（ECE）[[172](#bib.bib172)]，以及定性（Q）[[116](#bib.bib116)]。一些工作预测行人[[154](#bib.bib154),
    [234](#bib.bib234), [131](#bib.bib131)]或车辆[[77](#bib.bib77)]的方向，因此也报告了使用平均角误差（MAnE）的性能。
- en: 10.3.1 Pitfalls of trajectory prediction metrics
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.1 轨迹预测度量的陷阱
- en: Unlike video and action prediction fields, performance measures for trajectory
    prediction algorithms are not standardized in terms error metrics used and units
    of measure. For example, for measuring displacement error, although many algorithms
    use Euclidean Distance (ED) (aka L2-distance, L2-norm, Euclidean norm) [[87](#bib.bib87),
    [138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141), [146](#bib.bib146),
    [163](#bib.bib163), [168](#bib.bib168), [172](#bib.bib172), [152](#bib.bib152),
    [153](#bib.bib153), [155](#bib.bib155), [180](#bib.bib180), [158](#bib.bib158),
    [182](#bib.bib182), [234](#bib.bib234), [159](#bib.bib159), [150](#bib.bib150),
    [184](#bib.bib184), [115](#bib.bib115), [139](#bib.bib139), [169](#bib.bib169),
    [151](#bib.bib151), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [114](#bib.bib114), [173](#bib.bib173), [130](#bib.bib130), [156](#bib.bib156),
    [77](#bib.bib77), [165](#bib.bib165), [124](#bib.bib124), [133](#bib.bib133),
    [125](#bib.bib125), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [161](#bib.bib161), [168](#bib.bib168), [155](#bib.bib155), [176](#bib.bib176),
    [125](#bib.bib125)], many others rely on different error metrics including MSE
    [[143](#bib.bib143), [160](#bib.bib160), [171](#bib.bib171), [101](#bib.bib101),
    [170](#bib.bib170), [129](#bib.bib129), [183](#bib.bib183), [127](#bib.bib127),
    [144](#bib.bib144), [162](#bib.bib162), [166](#bib.bib166)], RMSE [[177](#bib.bib177),
    [161](#bib.bib161), [140](#bib.bib140), [170](#bib.bib170), [140](#bib.bib140),
    [147](#bib.bib147)], Weighted RMSE [[120](#bib.bib120)], Mean Absolute Error (MAE)[[77](#bib.bib77),
    [148](#bib.bib148)], Hausdorff Distance (HD)[[174](#bib.bib174)], Modified HD
    (MHD) [[118](#bib.bib118), [119](#bib.bib119), [121](#bib.bib121), [115](#bib.bib115),
    [132](#bib.bib132), [123](#bib.bib123), [125](#bib.bib125)], and discrete Fréchet
    distance (DFD) [[179](#bib.bib179)]. Moreover, trajectory prediction algorithms
    use different units for measuring displacement error. These are meter [[177](#bib.bib177),
    [87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [146](#bib.bib146),
    [168](#bib.bib168), [172](#bib.bib172), [153](#bib.bib153), [180](#bib.bib180),
    [182](#bib.bib182), [161](#bib.bib161), [170](#bib.bib170), [148](#bib.bib148),
    [150](#bib.bib150), [129](#bib.bib129), [121](#bib.bib121), [139](#bib.bib139),
    [169](#bib.bib169), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [140](#bib.bib140), [147](#bib.bib147), [114](#bib.bib114), [173](#bib.bib173),
    [130](#bib.bib130), [156](#bib.bib156), [77](#bib.bib77), [165](#bib.bib165),
    [124](#bib.bib124), [133](#bib.bib133), [135](#bib.bib135), [136](#bib.bib136),
    [161](#bib.bib161), [165](#bib.bib165), [168](#bib.bib168), [166](#bib.bib166)],
    pixel [[138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142),
    [163](#bib.bib163), [152](#bib.bib152), [183](#bib.bib183), [132](#bib.bib132),
    [165](#bib.bib165)], normalized pixel [[142](#bib.bib142), [158](#bib.bib158),
    [125](#bib.bib125)] and feet [[184](#bib.bib184)].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与视频和动作预测领域不同，轨迹预测算法的性能测量在误差指标和测量单位上并未标准化。例如，在测量位移误差时，虽然许多算法使用欧几里得距离（ED）（也称为L2-距离、L2-范数、欧几里得范数）[[87](#bib.bib87),
    [138](#bib.bib138), [140](#bib.bib140), [141](#bib.bib141), [146](#bib.bib146),
    [163](#bib.bib163), [168](#bib.bib168), [172](#bib.bib172), [152](#bib.bib152),
    [153](#bib.bib153), [155](#bib.bib155), [180](#bib.bib180), [158](#bib.bib158),
    [182](#bib.bib182), [234](#bib.bib234), [159](#bib.bib159), [150](#bib.bib150),
    [184](#bib.bib184), [115](#bib.bib115), [139](#bib.bib139), [169](#bib.bib169),
    [151](#bib.bib151), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [114](#bib.bib114), [173](#bib.bib173), [130](#bib.bib130), [156](#bib.bib156),
    [77](#bib.bib77), [165](#bib.bib165), [124](#bib.bib124), [133](#bib.bib133),
    [125](#bib.bib125), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [161](#bib.bib161), [168](#bib.bib168), [155](#bib.bib155), [176](#bib.bib176),
    [125](#bib.bib125)]，许多其他算法依赖于不同的误差指标，包括均方误差（MSE）[[143](#bib.bib143), [160](#bib.bib160),
    [171](#bib.bib171), [101](#bib.bib101), [170](#bib.bib170), [129](#bib.bib129),
    [183](#bib.bib183), [127](#bib.bib127), [144](#bib.bib144), [162](#bib.bib162),
    [166](#bib.bib166)]，均方根误差（RMSE）[[177](#bib.bib177), [161](#bib.bib161), [140](#bib.bib140),
    [170](#bib.bib170), [140](#bib.bib140), [147](#bib.bib147)]，加权均方根误差（Weighted RMSE）[[120](#bib.bib120)]，平均绝对误差（MAE）[[77](#bib.bib77),
    [148](#bib.bib148)]，豪斯多夫距离（HD）[[174](#bib.bib174)]，修正豪斯多夫距离（MHD）[[118](#bib.bib118),
    [119](#bib.bib119), [121](#bib.bib121), [115](#bib.bib115), [132](#bib.bib132),
    [123](#bib.bib123), [125](#bib.bib125)]，以及离散弗雷歇距离（DFD）[[179](#bib.bib179)]。此外，轨迹预测算法使用不同的单位来测量位移误差，这些单位包括米[[177](#bib.bib177),
    [87](#bib.bib87), [138](#bib.bib138), [140](#bib.bib140), [146](#bib.bib146),
    [168](#bib.bib168), [172](#bib.bib172), [153](#bib.bib153), [180](#bib.bib180),
    [182](#bib.bib182), [161](#bib.bib161), [170](#bib.bib170), [148](#bib.bib148),
    [150](#bib.bib150), [129](#bib.bib129), [121](#bib.bib121), [139](#bib.bib139),
    [169](#bib.bib169), [179](#bib.bib179), [154](#bib.bib154), [131](#bib.bib131),
    [140](#bib.bib140), [147](#bib.bib147), [114](#bib.bib114), [173](#bib.bib173),
    [130](#bib.bib130), [156](#bib.bib156), [77](#bib.bib77), [165](#bib.bib165),
    [124](#bib.bib124), [133](#bib.bib133), [135](#bib.bib135), [136](#bib.bib136),
    [161](#bib.bib161), [165](#bib.bib165), [168](#bib.bib168), [166](#bib.bib166)]，像素[[138](#bib.bib138),
    [140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142), [163](#bib.bib163),
    [152](#bib.bib152), [183](#bib.bib183), [132](#bib.bib132), [165](#bib.bib165)]，归一化像素[[142](#bib.bib142),
    [158](#bib.bib158), [125](#bib.bib125)]和英尺[[184](#bib.bib184)]。
- en: Although such discrepancy between error metrics and units is expected across
    different applications, the problem arises when the proposed works do not specify
    the error metric [[142](#bib.bib142), [178](#bib.bib178)], the unit of measure
    [[143](#bib.bib143), [155](#bib.bib155), [160](#bib.bib160), [171](#bib.bib171),
    [101](#bib.bib101), [118](#bib.bib118), [119](#bib.bib119), [127](#bib.bib127),
    [115](#bib.bib115), [123](#bib.bib123), [151](#bib.bib151), [176](#bib.bib176),
    [120](#bib.bib120), [174](#bib.bib174), [178](#bib.bib178), [155](#bib.bib155)]
    or both [[164](#bib.bib164), [117](#bib.bib117)]. Despite the fact that the reported
    results might imply the choice of the metrics and units, the lack of specification
    can cause erroneous comparisons, specially because many authors use the results
    of previous works directly as reported in the papers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不同应用中误差度量和单位之间的这种不一致是预期中的问题，但当提出的工作没有指定误差度量[[142](#bib.bib142), [178](#bib.bib178)]、测量单位[[143](#bib.bib143),
    [155](#bib.bib155), [160](#bib.bib160), [171](#bib.bib171), [101](#bib.bib101),
    [118](#bib.bib118), [119](#bib.bib119), [127](#bib.bib127), [115](#bib.bib115),
    [123](#bib.bib123), [151](#bib.bib151), [176](#bib.bib176), [120](#bib.bib120),
    [174](#bib.bib174), [178](#bib.bib178), [155](#bib.bib155)]或两者[[164](#bib.bib164),
    [117](#bib.bib117)]时，问题就会出现。尽管报告的结果可能暗示了度量和单位的选择，但缺乏规格说明可能导致错误的比较，特别是因为许多作者直接使用了前期工作中报告的结果。
- en: Unfortunately, metric and unit discrepancy exists within the same applications
    and the same error measuring techniques. For instance, in the case of ADE measure,
    this metric is originally proposed in [[235](#bib.bib235)] in terms of ED, and
    was referred to as ADE by the authors of [[160](#bib.bib160)] despite the fact
    that they used MSE instead. This is also apparent in many subsequent works that
    employed ADE measure. For example, the majority of methods use the original metric
    and report the results in terms of ED [[87](#bib.bib87), [138](#bib.bib138), [141](#bib.bib141),
    [152](#bib.bib152), [153](#bib.bib153), [155](#bib.bib155), [146](#bib.bib146),
    [163](#bib.bib163), [168](#bib.bib168), [172](#bib.bib172), [180](#bib.bib180),
    [158](#bib.bib158), [182](#bib.bib182), [234](#bib.bib234), [159](#bib.bib159)]
    whereas some works use MSE [[143](#bib.bib143), [160](#bib.bib160), [171](#bib.bib171),
    [101](#bib.bib101)] and RMSE[[177](#bib.bib177), [140](#bib.bib140)] or do not
    specify the metric [[142](#bib.bib142), [164](#bib.bib164)]. Although the formulation
    of these metrics look similar, they produce different results. ADE using ED, for
    example, is square-root of squared differences averaged over all samples and time
    steps. Unlike ED, in RMSE, the averaging takes place inside square-root operation.
    MSE, on the other hand, is very different from the other two metrics, and does
    not calculate the root of the error. As we can also see in some of the past works,
    this discrepancy may cause confusion about the intended and actual metric that
    is used. For example, in [[140](#bib.bib140)] the authors propose to use MAE metric
    while presenting mathematical formulation of Euclidean distance. The authors of
    [[159](#bib.bib159), [176](#bib.bib176)] make a similar mistake and define ED
    formulation but refer to it as MSE.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在相同的应用程序和相同的误差测量技术中存在度量和单位的不一致。例如，在ADE度量的情况下，该度量最初在[[235](#bib.bib235)]中以ED的形式提出，尽管[[160](#bib.bib160)]的作者使用了MSE，但他们称之为ADE。这在许多使用ADE度量的后续工作中也很明显。例如，大多数方法使用原始度量并以ED为单位报告结果[[87](#bib.bib87),
    [138](#bib.bib138), [141](#bib.bib141), [152](#bib.bib152), [153](#bib.bib153),
    [155](#bib.bib155), [146](#bib.bib146), [163](#bib.bib163), [168](#bib.bib168),
    [172](#bib.bib172), [180](#bib.bib180), [158](#bib.bib158), [182](#bib.bib182),
    [234](#bib.bib234), [159](#bib.bib159)]，而一些工作则使用MSE[[143](#bib.bib143), [160](#bib.bib160),
    [171](#bib.bib171), [101](#bib.bib101)]和RMSE[[177](#bib.bib177), [140](#bib.bib140)]，或者没有指定度量[[142](#bib.bib142),
    [164](#bib.bib164)]。虽然这些度量的公式看起来相似，但它们产生不同的结果。例如，使用ED的ADE是所有样本和时间步骤上平方差的平方根。与ED不同的是，RMSE的平均是在平方根操作内部进行的。另一方面，MSE与其他两个度量非常不同，不计算误差的平方根。正如我们在一些过去的工作中所见，这种不一致可能会导致对使用的预期和实际度量的困惑。例如，在[[140](#bib.bib140)]中，作者提出使用MAE度量，同时给出欧几里得距离的数学公式。[[159](#bib.bib159),
    [176](#bib.bib176)]的作者也犯了类似的错误，定义了ED公式但称之为MSE。
- en: In addition, some algorithms within the same applications and using the same
    datasets use different measuring unit. For instance, in the context of surveillance,
    ETH [[235](#bib.bib235)] is one of the most commonly used datasets. Many works
    use this dataset for benchmarking the performance of their proposed algorithms,
    however, they either use different units, e.g. meter [[87](#bib.bib87), [138](#bib.bib138),
    [139](#bib.bib139), [146](#bib.bib146), [149](#bib.bib149), [153](#bib.bib153),
    [156](#bib.bib156), [180](#bib.bib180)], pixel [[163](#bib.bib163), [140](#bib.bib140),
    [142](#bib.bib142), [152](#bib.bib152)], normalized pixel [[158](#bib.bib158),
    [142](#bib.bib142)], or do not specify the unit used [[143](#bib.bib143), [178](#bib.bib178),
    [151](#bib.bib151), [155](#bib.bib155), [164](#bib.bib164), [160](#bib.bib160)].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，同一应用程序中的一些算法使用相同的数据集，但测量单位不同。例如，在监控的背景下，ETH [[235](#bib.bib235)] 是最常用的数据集之一。许多研究使用该数据集来基准测试他们提出的算法性能，但它们要么使用不同的单位，如米
    [[87](#bib.bib87), [138](#bib.bib138), [139](#bib.bib139), [146](#bib.bib146),
    [149](#bib.bib149), [153](#bib.bib153), [156](#bib.bib156), [180](#bib.bib180)]，像素
    [[163](#bib.bib163), [140](#bib.bib140), [142](#bib.bib142), [152](#bib.bib152)]，归一化像素
    [[158](#bib.bib158), [142](#bib.bib142)]，或未指定使用的单位 [[143](#bib.bib143), [178](#bib.bib178),
    [151](#bib.bib151), [155](#bib.bib155), [164](#bib.bib164), [160](#bib.bib160)]。
- en: Last but not least, another potential source of error in performance evaluation
    is in the design of the experiments. Taking surveillance applications as an example,
    it is a common practice to evaluate algorithms with 8 frames observations of the
    past and prediction 12 steps in the future [[178](#bib.bib178), [87](#bib.bib87),
    [138](#bib.bib138), [139](#bib.bib139), [153](#bib.bib153), [180](#bib.bib180),
    [160](#bib.bib160)]. However, in some cases the performance of state-of-the-art
    is reported under the standard 8/12 condition, but the proposed algorithms are
    tested under different conditions. For instance, in [[155](#bib.bib155)] the authors
    incorrectly compared the performance of their proposed algorithm using 5 observations
    and 5 predictions with the results of the previous works evaluated under the standard
    8/12 condition.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，性能评估中的另一个潜在误差来源在于实验设计。以监控应用程序为例，通常的做法是用过去的 8 帧观察数据来评估算法，并预测未来的 12
    步 [[178](#bib.bib178), [87](#bib.bib87), [138](#bib.bib138), [139](#bib.bib139),
    [153](#bib.bib153), [180](#bib.bib180), [160](#bib.bib160)]。然而，在某些情况下，最先进的算法在标准
    8/12 条件下的性能报告是准确的，但提出的算法在不同条件下进行了测试。例如，在 [[155](#bib.bib155)] 中，作者错误地将他们提出的算法在
    5 个观察和 5 个预测下的性能与在标准 8/12 条件下评估的先前工作的结果进行了比较。
- en: 10.4 Motion prediction
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4 运动预测
- en: Due to the inherent stochasticity of human body movement, motion prediction
    algorithms often limit their prediction horizon to approximately $500ms$. To measure
    the error between corresponding ground truth and predicted poses, these algorithms
    use mean average error, either in angle space (MAnE) [[190](#bib.bib190), [236](#bib.bib236),
    [197](#bib.bib197), [198](#bib.bib198), [196](#bib.bib196), [187](#bib.bib187),
    [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193), [194](#bib.bib194),
    [96](#bib.bib96), [195](#bib.bib195)] or joint space (MJE) in terms of joint coordinates
    [[236](#bib.bib236), [188](#bib.bib188), [101](#bib.bib101), [103](#bib.bib103),
    [56](#bib.bib56), [103](#bib.bib103), [26](#bib.bib26), [110](#bib.bib110), [187](#bib.bib187),
    [186](#bib.bib186), [200](#bib.bib200)]. In the 3D motion prediction domain, a
    metric known as Mean Per Joint Prediction Error (MPJPE) [[197](#bib.bib197), [199](#bib.bib199)]
    is used which is the error over joints normalized with respect to the root joint.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人体运动的固有随机性，运动预测算法通常将其预测范围限制在大约 $500ms$。为了测量相应真实姿态与预测姿态之间的误差，这些算法使用均值绝对误差，既可以在角度空间
    (MAnE) [[190](#bib.bib190), [236](#bib.bib236), [197](#bib.bib197), [198](#bib.bib198),
    [196](#bib.bib196), [187](#bib.bib187), [191](#bib.bib191), [192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194), [96](#bib.bib96), [195](#bib.bib195)]，也可以在关节空间
    (MJE) 以关节坐标 [[236](#bib.bib236), [188](#bib.bib188), [101](#bib.bib101), [103](#bib.bib103),
    [56](#bib.bib56), [103](#bib.bib103), [26](#bib.bib26), [110](#bib.bib110), [187](#bib.bib187),
    [186](#bib.bib186), [200](#bib.bib200)] 的形式。 在 3D 运动预测领域，使用了一种称为每关节预测误差 (MPJPE)
    [[197](#bib.bib197), [199](#bib.bib199)] 的度量，它是相对于根关节归一化的关节误差。
- en: As an alternative to distance error metrics, Percentage of Correct Keypoints
    (PCK) [[199](#bib.bib199), [187](#bib.bib187), [189](#bib.bib189), [188](#bib.bib188)]
    measures how many of the keypoints (e.g. joints) are predicted correctly. The
    correct predictions are those that are below a certain error threshold (e.g. 0.05).
    Some works also use the accuracy metric to report on how well the algorithm can
    localize the position of a particular joint within an error tolerance region [[48](#bib.bib48),
    [195](#bib.bib195)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 作为距离误差度量的替代方案，正确关键点百分比 (PCK) [[199](#bib.bib199), [187](#bib.bib187), [189](#bib.bib189),
    [188](#bib.bib188)] 衡量的是预测正确的关键点（例如关节）数量。正确的预测是那些低于某个误差阈值（例如 0.05）的预测。一些工作还使用准确度度量来报告算法在误差容差区域内定位特定关节位置的能力
    [[48](#bib.bib48), [195](#bib.bib195)]。
- en: Other metrics used in the literature include Normalized Power Spectrum Similarity
    (NPSS) [[190](#bib.bib190)], Reconstruction Error (RE) [[199](#bib.bib199)], Limb
    Orientation (LO) [[56](#bib.bib56)], PoSe Entropy (PSEnt), PoSe KL (PSKL) [[198](#bib.bib198)],
    qualitative human judgment [[198](#bib.bib198), [192](#bib.bib192)] and method
    Run Time (RT) [[236](#bib.bib236), [188](#bib.bib188)].
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中使用的其他度量包括标准化功率谱相似度 (NPSS) [[190](#bib.bib190)]、重建误差 (RE) [[199](#bib.bib199)]、肢体方向
    (LO) [[56](#bib.bib56)]、PoSe 熵 (PSEnt)、PoSe KL (PSKL) [[198](#bib.bib198)]、定性人工判断
    [[198](#bib.bib198), [192](#bib.bib192)] 和方法运行时间 (RT) [[236](#bib.bib236), [188](#bib.bib188)]。
- en: 10.4.1 Pitfalls of motion prediction metrics
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.4.1 运动预测度量的陷阱
- en: Similar to trajectory methods, motion prediction algorithms are evaluated using
    distance-based methods that calculate the error between pose vectors. In the case
    of MAnE measure, some methods use ED metric [[197](#bib.bib197), [196](#bib.bib196),
    [193](#bib.bib193), [194](#bib.bib194), [96](#bib.bib96), [195](#bib.bib195)]
    while others use MSE [[190](#bib.bib190), [236](#bib.bib236), [198](#bib.bib198),
    [191](#bib.bib191), [192](#bib.bib192)]. Sometimes no metric is specified [[187](#bib.bib187)].
    The same holds for MJE measure where metrics used include MSE [[236](#bib.bib236),
    [101](#bib.bib101), [103](#bib.bib103), [103](#bib.bib103)], RMSE [[188](#bib.bib188),
    [200](#bib.bib200)], ED [[26](#bib.bib26), [110](#bib.bib110), [186](#bib.bib186)],
    MAE [[187](#bib.bib187)], or no metric is specified [[56](#bib.bib56)].
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于轨迹方法，运动预测算法使用基于距离的方法来评估，即计算姿态向量之间的误差。在 MAnE 测量的情况下，一些方法使用 ED 度量 [[197](#bib.bib197),
    [196](#bib.bib196), [193](#bib.bib193), [194](#bib.bib194), [96](#bib.bib96),
    [195](#bib.bib195)]，而其他方法使用 MSE [[190](#bib.bib190), [236](#bib.bib236), [198](#bib.bib198),
    [191](#bib.bib191), [192](#bib.bib192)]。有时没有指定度量 [[187](#bib.bib187)]。MJE 测量也是如此，使用的度量包括
    MSE [[236](#bib.bib236), [101](#bib.bib101), [103](#bib.bib103), [103](#bib.bib103)]、RMSE
    [[188](#bib.bib188), [200](#bib.bib200)]、ED [[26](#bib.bib26), [110](#bib.bib110),
    [186](#bib.bib186)]、MAE [[187](#bib.bib187)]，或者没有指定度量 [[56](#bib.bib56)]。
- en: The added challenge in coordinate-based error measures, e.g. MJE, MPJPE, is
    the error unit. While many approaches do not specify the unit explicitly [[236](#bib.bib236),
    [103](#bib.bib103), [103](#bib.bib103), [26](#bib.bib26), [110](#bib.bib110),
    [187](#bib.bib187), [186](#bib.bib186)], others clearly state whether the unit
    is in pixel [[188](#bib.bib188), [101](#bib.bib101)], centimeter [[56](#bib.bib56)],
    meter [[200](#bib.bib200)] or millimeter [[197](#bib.bib197), [199](#bib.bib199)].
    As was the case before, here many algorithms that benchmark on the same datasets,
    may use different performance metrics, e.g. using popular human datasset Human
    3.6M [[237](#bib.bib237)], MAnE (ED), MAnE (MSE)[[191](#bib.bib191)] and MJE (ED)
    [[193](#bib.bib193)] are used.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标基础误差度量（例如 MJE、MPJPE）中的额外挑战在于误差单位。虽然许多方法没有明确指定单位 [[236](#bib.bib236), [103](#bib.bib103),
    [103](#bib.bib103), [26](#bib.bib26), [110](#bib.bib110), [187](#bib.bib187),
    [186](#bib.bib186)]，但也有一些明确指出单位是像素 [[188](#bib.bib188), [101](#bib.bib101)]、厘米
    [[56](#bib.bib56)]、米 [[200](#bib.bib200)] 或毫米 [[197](#bib.bib197), [199](#bib.bib199)]。与之前一样，许多在相同数据集上进行基准测试的算法可能使用不同的性能度量，例如使用流行的人体数据集
    Human 3.6M [[237](#bib.bib237)]，使用 MAnE (ED)、MAnE (MSE) [[191](#bib.bib191)] 和
    MJE (ED) [[193](#bib.bib193)]。
- en: 10.5 Other prediction applications
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5 其他预测应用
- en: Depending on the task objectives, the metrics used in other prediction applications
    are similar to the ones discussed thus far. For instance, the applications that
    classify future events or outcomes, e.g. contest or an election winner, next image
    index for storytelling, severe weather, and pain, use common metrics such as accuracy
    [[220](#bib.bib220), [221](#bib.bib221), [8](#bib.bib8)], precision, recall [[213](#bib.bib213),
    [218](#bib.bib218)], percentage of correct predictions (PCP) [[219](#bib.bib219)],
    and Matthews correlation coefficient (MCC) [[218](#bib.bib218)] which predicts
    the quality of binary classification by taking into account both false and true
    predictions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 根据任务目标，其他预测应用中使用的度量标准与目前讨论的类似。例如，分类未来事件或结果的应用程序，如比赛或选举获胜者、讲故事的下一张图片索引、严重天气和疼痛，使用的常见度量标准包括准确率[[220](#bib.bib220),
    [221](#bib.bib221), [8](#bib.bib8)]、精确率、召回率[[213](#bib.bib213), [218](#bib.bib218)]、正确预测的百分比
    (PCP) [[219](#bib.bib219)]以及马修斯相关系数 (MCC) [[218](#bib.bib218)]，MCC通过考虑假阳性和真阳性来预测二分类的质量。
- en: Regression based methods, such as temperature, trends, or steering prediction,
    use distance metrics including Euclidean Distance (ED) [[202](#bib.bib202), [238](#bib.bib238)],
    RMSE [[214](#bib.bib214)], MSE [[212](#bib.bib212)], MAE [[239](#bib.bib239),
    [217](#bib.bib217), [204](#bib.bib204)], Mean Absolute Percentage Error (MAPE)
    [[216](#bib.bib216), [217](#bib.bib217)], normalized MAPE (nMAPE) [[215](#bib.bib215)],
    and the Spearman’s ranking Correlation (SRC) [[216](#bib.bib216)] which measures
    the strength and direction of relationship between two variables.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 基于回归的方法，如温度、趋势或方向预测，使用的距离度量包括欧几里得距离 (ED) [[202](#bib.bib202), [238](#bib.bib238)]、均方根误差
    (RMSE) [[214](#bib.bib214)]、均方误差 (MSE) [[212](#bib.bib212)]、平均绝对误差 (MAE) [[239](#bib.bib239),
    [217](#bib.bib217), [204](#bib.bib204)]、平均绝对百分比误差 (MAPE) [[216](#bib.bib216),
    [217](#bib.bib217)]、标准化平均绝对百分比误差 (nMAPE) [[215](#bib.bib215)]和斯皮尔曼等级相关系数 (SRC)
    [[216](#bib.bib216)]，后者衡量两个变量之间关系的强度和方向。
- en: Of particular interest are metrics used for evaluating generative models that
    predict Occupancy Grid Maps (OGMs) and segmentation maps. OGMs are grayscale images
    that highlight the likelihood of a certain region (represented as a cell in the
    grid) that is occupied. The generated map can be compared to ground truth by using
    image similarity metrics such as SSIM [[205](#bib.bib205), [206](#bib.bib206)],
    PSNR [[206](#bib.bib206)] or psi ($\psi$) [[203](#bib.bib203)]. Alternatively,
    OGM can be evaluated using a binary classification metric. Here, the grid cells
    are classified as occupied or free by applying a threshold and then can be evaluated
    as a whole by using metrics such as True Positive (TP), True Negative (TN) [[205](#bib.bib205)],
    Receiver Operator Characteristic (ROC) curve over TP and TN [[207](#bib.bib207),
    [208](#bib.bib208)], F1-score [[201](#bib.bib201), [207](#bib.bib207)], precision,
    recall, and their corresponding AUC [[209](#bib.bib209)]. Given that OGM prediction
    algorithms are mainly used in safety-critical applications such as autonomous
    driving, some algorithms are also evaluated in terms of their Run Time (RT) [[205](#bib.bib205),
    [206](#bib.bib206), [209](#bib.bib209)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 特别值得关注的是用于评估生成模型的度量标准，这些模型预测占用网格图 (OGMs) 和分割图。OGMs 是灰度图像，突出显示某个区域（在网格中表示为一个单元）被占用的可能性。生成的地图可以通过使用图像相似度度量标准来与实际情况进行比较，例如
    SSIM [[205](#bib.bib205), [206](#bib.bib206)]、PSNR [[206](#bib.bib206)] 或 psi
    ($\psi$) [[203](#bib.bib203)]。另外，OGM 也可以通过二分类度量标准进行评估。在这里，通过应用阈值将网格单元分类为已占用或空闲，然后可以使用诸如真正例
    (TP)、真负例 (TN) [[205](#bib.bib205)]、接收者操作特征 (ROC) 曲线覆盖 TP 和 TN [[207](#bib.bib207),
    [208](#bib.bib208)]、F1-score [[201](#bib.bib201), [207](#bib.bib207)]、精确率、召回率及其对应的
    AUC [[209](#bib.bib209)] 等度量标准对整体进行评估。鉴于 OGM 预测算法主要用于安全关键应用，如自动驾驶，一些算法也根据其运行时间
    (RT) [[205](#bib.bib205), [206](#bib.bib206), [209](#bib.bib209)] 进行评估。
- en: Image similarity metrics such as PSNR and SSIM can also be used in the segmentation
    prediction domain [[211](#bib.bib211)]. The most common metric, however, is Intersection
    over Union (IoU)[[10](#bib.bib10), [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212)]
    which measures the average overlap of segmented instances with the ground truth
    segments. In addition, by applying a threshold to IoU scores, the true matches
    can be identified and used to calculate the Average Precision (AP) scores as in
    [[210](#bib.bib210)]. Other metrics used for segmentation prediction tasks include
    EndPoint error (EPE) [[212](#bib.bib212)], Probabilistic Rand Index (RI), Global
    Consistency Error (GCE), and Variation of Information (VoI) [[210](#bib.bib210)].
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图像相似性度量，如 PSNR 和 SSIM 也可以用于分割预测领域 [[211](#bib.bib211)]。然而，最常见的度量是交并比 (IoU) [[10](#bib.bib10),
    [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212)]，它测量分割实例与真实分割的平均重叠度。此外，通过对
    IoU 分数应用阈值，可以识别真实匹配并用于计算平均精度 (AP) 分数，如 [[210](#bib.bib210)] 中所示。其他用于分割预测任务的度量包括端点误差
    (EPE) [[212](#bib.bib212)]、概率随机指数 (RI)、全局一致性误差 (GCE) 和信息变异 (VoI) [[210](#bib.bib210)]。
- en: 11 Datasets
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11 数据集
- en: '| Year | Dataset | Type | Annotations | Application |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 数据集 | 类型 | 注释 | 应用 |'
- en: '| V | A | T | M | O |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| V | A | T | M | O |'
- en: '| 2019 | ARGOVerse [[137](#bib.bib137)] | Traffic | RGB, LIDAR, 3D BB |  |  |
    x |  |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | ARGOVerse [[137](#bib.bib137)] | 交通 | RGB, LIDAR, 3D BB |  |  | x
    |  |  |'
- en: '| CARLA [[162](#bib.bib162)] | Traffic (sim) | RGB |  | x |  |  |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| CARLA [[162](#bib.bib162)] | 交通（模拟） | RGB |  | x |  |  |  |'
- en: '| EgoPose [[186](#bib.bib186)] | Pose (ego) | RGB, 3D Pose |  |  |  | x |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| EgoPose [[186](#bib.bib186)] | 姿态（自我） | RGB, 3D 姿态 |  |  |  | x |  |'
- en: '| Future Motion (FM) [[167](#bib.bib167)] | Mix | RGB, BB, Attrib. |  |  |
    x |  |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Future Motion (FM) [[167](#bib.bib167)] | 混合 | RGB, BB, 属性 |  |  | x |  |  |'
- en: '| InstaVariety [[240](#bib.bib240)] | Activities | RGB, BB, Pose |  |  |  |
    x |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| InstaVariety [[240](#bib.bib240)] | 活动 | RGB, BB, 姿态 |  |  |  | x |  |'
- en: '| INTEARCTION [[241](#bib.bib241)] | Traffic | Map, Traj. |  |  | x |  |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| INTEARCTION [[241](#bib.bib241)] | 交通 | 地图, 轨迹 |  |  | x |  |  |'
- en: '| Luggage [[81](#bib.bib81)] | Robot | Stereo RGB, BB |  | x |  |  |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Luggage [[81](#bib.bib81)] | 机器人 | 立体 RGB, BB |  | x |  |  |  |'
- en: '| MGIF [[242](#bib.bib242)] | Activities | RGB | x |  |  |  |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| MGIF [[242](#bib.bib242)] | 活动 | RGB | x |  |  |  |  |'
- en: '| Pedestrian Intention Estimation (PIE) [[144](#bib.bib144)] | Traffic | RGB,
    BB, Class, Attrib., Temporal seg., Vehicle sensors |  | x | x |  |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Pedestrian Intention Estimation (PIE) [[144](#bib.bib144)] | 交通 | RGB, BB,
    类别, 属性, 时间段, 车辆传感器 |  | x | x |  |  |'
- en: '| nuScenes [[243](#bib.bib243)] | Traffic | RGB, LIDAR, 3D BB, Vehicle sensors
    |  |  | x |  |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| nuScenes [[243](#bib.bib243)] | 交通 | RGB, LIDAR, 3D BB, 车辆传感器 |  |  | x |  |  |'
- en: '| Vehicle-Pedestrian-Mixed (VPM) [[141](#bib.bib141)] | Traffic | RGB, BB |  |  |
    x |  |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Vehicle-Pedestrian-Mixed (VPM) [[141](#bib.bib141)] | 交通 | RGB, BB |  |  |
    x |  |  |'
- en: '| TRAF [[177](#bib.bib177)] | Traffic | RGB, BN, Class, Time-of-day |  |  |
    x |  |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| TRAF [[177](#bib.bib177)] | 交通 | RGB, BN, 类别, 时间段 |  |  | x |  |  |'
- en: '| 2018 | 3D POSES IN THE WILD (3DPW) [[244](#bib.bib244)] | Outdoor | RGB,
    2D/3D Pose, Models |  |  |  | x |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 3D POSES IN THE WILD (3DPW) [[244](#bib.bib244)] | 户外 | RGB, 2D/3D
    姿态, 模型 |  |  |  | x |  |'
- en: '| ActEV/VIRAT [[245](#bib.bib245)] | Surveillance | RGB, BB, Activity, Temporal
    seg. |  | x | x |  |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| ActEV/VIRAT [[245](#bib.bib245)] | 监控 | RGB, BB, 活动, 时间段 |  | x | x |  |  |'
- en: '| ACTICIPATE [[246](#bib.bib246)] | Interaction | RGB, Gaze, Pose |  | x |  |  |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ACTICIPATE [[246](#bib.bib246)] | 互动 | RGB, 注视, 姿态 |  | x |  |  |  |'
- en: '| Atomic Visual Actions (AVA) [[247](#bib.bib247)] | Activities | RGB, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Atomic Visual Actions (AVA) [[247](#bib.bib247)] | 活动 | RGB, 活动, 时间段 |  |
    x |  |  |  |'
- en: '| Epic-Kitchen [[248](#bib.bib248)] | Cooking (ego) | RGB, Audio, BB, Class,
    Text, Temporal seg. |  | x |  |  |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Epic-Kitchen [[248](#bib.bib248)] | 烹饪（自我） | RGB, 音频, BB, 类别, 文本, 时间段 |  |
    x |  |  |  |'
- en: '| EGTEA Gaze+ [[249](#bib.bib249)] | Cooking (ego) | RGB, Gaze, Mask, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| EGTEA Gaze+ [[249](#bib.bib249)] | 烹饪（自我） | RGB, 注视, 掩码, 活动, 时间段 |  | x |  |  |  |'
- en: '| ShanghaiTech Campus (STC) [[223](#bib.bib223)] | Surveillance | RGB, Anomaly
    | x |  |  |  |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ShanghaiTech Campus (STC) [[223](#bib.bib223)] | 监控 | RGB, 异常 | x |  |  |  |  |'
- en: '| ShapeStack [[250](#bib.bib250)] | Objects (sim) | RGBD, Mask, Stability |
    x |  |  |  |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| ShapeStack [[250](#bib.bib250)] | 物体（模拟） | RGBD, 掩码, 稳定性 | x |  |  |  |  |'
- en: '| VIENA [[75](#bib.bib75)] | Traffic (sim) | RGB, Activity, Vehicle sensors
    |  | x |  |  |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| VIENA [[75](#bib.bib75)] | 交通（模拟） | RGB, 活动, 车辆传感器 |  | x |  |  |  |'
- en: '| YouCook2 [[251](#bib.bib251)] | Cooking | RGB, Audio, Text, Activity, Temporal
    seg. |  | x |  |  |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| YouCook2 [[251](#bib.bib251)] | 烹饪 | RGB, 音频, 文本, 活动, 时间段 |  | x |  |  |  |'
- en: '| 2017 | BU Action (BUA) [[252](#bib.bib252)] | Activities | RGB (image), Activity
    |  | x |  |  |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | BU 行动 (BUA) [[252](#bib.bib252)] | 活动 | RGB (图像), 活动 |  | x |  |  |  |'
- en: '| CityPerson [[253](#bib.bib253)] | Traffic | Stereo RGB, BB, Semantic seg.
    |  |  | x |  |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| CityPerson [[253](#bib.bib253)] | 交通 | 立体 RGB, BB, 语义分割 |  |  | x |  |  |'
- en: '| Epic-Fail [[84](#bib.bib84)] | Risk assessment | RGB, BB, Traj., Temporal
    seg. |  | x |  |  |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Epic-Fail [[84](#bib.bib84)] | 风险评估 | RGB, BB, 轨迹, 时间段 |  | x |  |  |  |'
- en: '| Joint Attention in Autonomous Driving (JAAD) [[78](#bib.bib78)] | Traffic
    | RGB, BB, Attrib., Temporal seg. | x | x | x |  | x |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 自动驾驶中的共同注意 (JAAD) [[78](#bib.bib78)] | 交通 | RGB, BB, 属性, 时间段 | x | x | x
    |  | x |'
- en: '| L-CAS [[254](#bib.bib254)] | Traffic | LIDAR, 3D BB, Attrib. |  |  | x |  |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| L-CAS [[254](#bib.bib254)] | 交通 | LIDAR, 3D BB, 属性 |  |  | x |  |  |'
- en: '| Mouse Fish [[255](#bib.bib255)] | Animals | Depth, 3D Pose |  |  |  | x |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 老鼠鱼 [[255](#bib.bib255)] | 动物 | 深度, 3D 姿态 |  |  |  | x |  |'
- en: '| Oxford Robot Car (ORC) [[256](#bib.bib256)] | Traffic | Stereo RGB, LIDAR,
    Vehicle sensors |  |  | x |  |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 牛津机器人车 (ORC) [[256](#bib.bib256)] | 交通 | 立体 RGB, LIDAR, 车辆传感器 |  |  | x |  |  |'
- en: '| PKU-MMD [[257](#bib.bib257)] | Activities, interactions | RGBD, IR, 3D Pose,
    Multiview, Temporal seg. |  | x |  |  |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| PKU-MMD [[257](#bib.bib257)] | 活动, 互动 | RGBD, 红外, 3D 姿态, 多视角, 时间段 |  | x
    |  |  |  |'
- en: '| Recipe1M [[258](#bib.bib258)] | Cooking | RGB(image), Text |  | x |  |  |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Recipe1M [[258](#bib.bib258)] | 烹饪 | RGB (图像), 文本 |  | x |  |  |  |'
- en: '| STRANDS [[259](#bib.bib259)] | Traffic | RGBD, 3DBB |  |  | x |  |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| STRANDS [[259](#bib.bib259)] | 交通 | RGBD, 3D BB |  |  | x |  |  |'
- en: '| 2016 | BAIR Push [[29](#bib.bib29)] | Object manipulation | RGB | x |  |  |  |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | BAIR 推动 [[29](#bib.bib29)] | 物体操控 | RGB | x |  |  |  |  |'
- en: '| Bouncing Ball (BB) [[260](#bib.bib260)] | Simulation | RGB | x |  |  |  |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 弹跳球 (BB) [[260](#bib.bib260)] | 模拟 | RGB | x |  |  |  |  |'
- en: '| Miss Universe (MU) [[220](#bib.bib220)] | Miss universe | RGB, BB, Scores
    |  |  |  |  | x |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 环球小姐 (MU) [[220](#bib.bib220)] | 环球小姐 | RGB, BB, 分数 |  |  |  |  | x |'
- en: '| Cityscapes [[261](#bib.bib261)] | Traffic | Stereo RGB, BB, Semantic seg.,
    Vehicle Sensors | x |  | x |  | x |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Cityscapes [[261](#bib.bib261)] | 交通 | 立体 RGB, BB, 语义分割, 车辆传感器 | x |  | x
    |  | x |'
- en: '| CMU Mocap [[262](#bib.bib262)] | Activities | 3D Pose, Activity |  | x |  |
    x |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| CMU 动作捕捉 [[262](#bib.bib262)] | 活动 | 3D 姿态, 活动 |  | x |  | x |  |'
- en: '| Dashcam Accident Dataset (DAD) [[79](#bib.bib79)] | Traffic, accidents |
    RGB, BB, Class, Temporal seg. |  | x |  |  |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 行车记录仪事故数据集 (DAD) [[79](#bib.bib79)] | 交通, 事故 | RGB, BB, 类别, 时间段 |  | x |  |  |  |'
- en: '| NTU RGB-D [[263](#bib.bib263)] | Activities | RGBD, IR, 3D Pose, Activity
    |  | x |  |  |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| NTU RGB-D [[263](#bib.bib263)] | 活动 | RGBD, 红外, 3D 姿态, 活动 |  | x |  |  |  |'
- en: '| Ongoing Activity (OA) [[106](#bib.bib106)] | Actvities | RGB, Activity |  |
    x |  |  |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 持续活动 (OA) [[106](#bib.bib106)] | 活动 | RGB, 活动 |  | x |  |  |  |'
- en: '| OAD [[264](#bib.bib264)] | Activities | RGBD, 3D Pose, Activity, Temporal
    seg. |  | x |  |  |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| OAD [[264](#bib.bib264)] | 活动 | RGBD, 3D 姿态, 活动, 时间段 |  | x |  |  |  |'
- en: '| Stanford Drone (SD) [[265](#bib.bib265)] | Surveillance | RGB, BB, Class
    |  |  | x |  |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 斯坦福无人机 (SD) [[265](#bib.bib265)] | 监控 | RGB, BB, 类别 |  |  | x |  |  |'
- en: '| TV Series [[266](#bib.bib266)] | Activities | RGB, Activity, Temporal seg.
    |  | x |  |  |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 电视剧 [[266](#bib.bib266)] | 活动 | RGB, 活动, 时间段 |  | x |  |  |  |'
- en: '| Visual StoryTelling (VIST) [[267](#bib.bib267)] | Visual story | RGB, Text
    |  |  |  |  | x |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 视觉讲故事 (VIST) [[267](#bib.bib267)] | 视觉故事 | RGB, 文本 |  |  |  |  | x |'
- en: '| Youtube-8M [[268](#bib.bib268)] | Activities | RGB, Activity, Temporal seg.
    | x |  |  |  |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Youtube-8M [[268](#bib.bib268)] | 活动 | RGB, 活动, 时间段 | x |  |  |  |  |'
- en: '| 2015 | Amazon [[269](#bib.bib269)] | Fashion | Features, Attrib., Text |  |  |  |  |
    x |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 亚马逊 [[269](#bib.bib269)] | 时尚 | 特征, 属性, 文本 |  |  |  |  | x |'
- en: '| Atari [[30](#bib.bib30)] | Games | RGB | x |  |  |  |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Atari [[30](#bib.bib30)] | 游戏 | RGB | x |  |  |  |  |'
- en: '| Brain4Cars [[53](#bib.bib53)] | Traffic, Driver | RGB, BB, Attrib., Temporal
    seg., Vehicle sensors |  | x |  |  |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Brain4Cars [[53](#bib.bib53)] | 交通, 驾驶员 | RGB, BB, 属性, 时间段, 车辆传感器 |  | x
    |  |  |  |'
- en: '| CMU Panoptic [[270](#bib.bib270)] | Interaction | RGBD, Multiview, 3D Pose,
    3D facial landmark | x | x |  | x |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| CMU 全景 [[270](#bib.bib270)] | 互动 | RGBD, 多视角, 3D 姿态, 3D 面部标志点 | x | x |  |
    x |  |'
- en: '| First Person Personalized Activities (FPPA) [[95](#bib.bib95)] | Activities
    (ego) | RGB, Activity, Temporal seg. |  | x |  |  |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 第一人称个性化活动 (FPPA) [[95](#bib.bib95)] | 活动 (自我) | RGB, 活动, 时间段 |  | x |  |  |  |'
- en: '| GTEA Gaze + [[271](#bib.bib271)] | Cooking (ego) | RGB, Gaze, Mask, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| GTEA 注视 + [[271](#bib.bib271)] | 烹饪 (自我) | RGB, 注视, 面具, 活动, 时间段 |  | x |  |  |  |'
- en: '| MicroBlog-Images (MBI-1M) [[272](#bib.bib272)] | Tweets | RGB (image), Attrib.,
    Text |  |  |  |  | x |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 微博图像 (MBI-1M) [[272](#bib.bib272)] | 推文 | RGB (图像), 属性, 文本 |  |  |  |  |
    x |'
- en: '| MOT [[273](#bib.bib273)] | Surveillance | RGB, BB |  |  | x |  |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| MOT [[273](#bib.bib273)] | 监控 | RGB, 边框 |  |  | x |  |  |'
- en: '| Moving MNIST (MMNIST) [[274](#bib.bib274)] | Digits | Grayscale | x |  |  |  |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Moving MNIST (MMNIST) [[274](#bib.bib274)] | 数字 | 灰度 | x |  |  |  |  |'
- en: '| SUN RGB-D [[275](#bib.bib275)] | Places | RGBD, 3D BB , Class |  |  |  |  |
    x |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| SUN RGB-D [[275](#bib.bib275)] | 场所 | RGBD, 3D边框, 分类 |  |  |  |  | x |'
- en: '| SYSU 3DHOI [[276](#bib.bib276)] | Object interaction | RGBD, 3D Pose, Activity
    |  | x |  |  |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| SYSU 3DHOI [[276](#bib.bib276)] | 物体交互 | RGBD, 3D姿态, 活动 |  | x |  |  |  |'
- en: '| THUMOS [[277](#bib.bib277)] | Activities | RGB, Activity, Temporal seg. |
    x | x |  |  |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| THUMOS [[277](#bib.bib277)] | 活动 | RGB, 活动, 时间段 | x | x |  |  |  |'
- en: '| Watch-n-Push (WnP) [[278](#bib.bib278)] | Activities | RGBD, 3D Pose, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Watch-n-Push (WnP) [[278](#bib.bib278)] | 活动 | RGBD, 3D姿态, 活动, 时间段 |  | x
    |  |  |  |'
- en: '| Wider [[279](#bib.bib279)] | Activities | RGB (image), Activity |  | x |  |  |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Wider [[279](#bib.bib279)] | 活动 | RGB（图像）, 活动 |  | x |  |  |  |'
- en: '| 2014 | Breakfast [[280](#bib.bib280)] | Cooking | RGB, Activity, Temporal
    seg. |  | x |  |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 早餐 [[280](#bib.bib280)] | 烹饪 | RGB, 活动, 时间段 |  | x |  |  |  |'
- en: '| Human3.6M [[237](#bib.bib237)] | Activities | RGB, 3D Pose, Activity | x
    | x |  | x |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Human3.6M [[237](#bib.bib237)] | 活动 | RGB, 3D姿态, 活动 | x | x |  | x |  |'
- en: '| MPII Human Pose [[281](#bib.bib281)] | Activities | RGB, Pose, Activity |  |  |  |
    x |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| MPII Human Pose [[281](#bib.bib281)] | 活动 | RGB, 姿态, 活动 |  |  |  | x |  |'
- en: '| Online RGBD Action Dataset (ORGBD) [[282](#bib.bib282)] | Activities | RGBD,
    BB, 3D Pose, Activity |  | x |  |  |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Online RGBD Action Dataset (ORGBD) [[282](#bib.bib282)] | 活动 | RGBD, 边框,
    3D姿态, 活动 |  | x |  |  |  |'
- en: '| Sports-1M [[283](#bib.bib283)] | Sports | RGB, Activity | x | x |  |  |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Sports-1M [[283](#bib.bib283)] | 体育 | RGB, 活动 | x | x |  |  |  |'
- en: 'TABLE I: A summary of common datasets from years 2014-2019 used in vision-based
    prediction applications, namely video (V), action (A), trajectory (T), motion
    (M) and others (O). The annotation column specifies the type of data (e.g. RGB,
    Infrared(IR)) and annotation types. All datasets contain image sequences unless
    specified by “image”. As for annotations, BB stands for bounding box. Attributes
    include any object characteristics (e.g. for pedestrians demographics, behavior).
    Vehicle sensors may include speed, steering angle, GPS, etc. Temporal seg. identifies
    the datasets that specify the start and end of the events.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE I: 一个总结2014-2019年常用数据集的表格，用于基于视觉的预测应用，包括视频（V）、动作（A）、轨迹（T）、运动（M）及其他（O）。注释列指定数据类型（例如RGB，红外(IR)）和注释类型。所有数据集都包含图像序列，除非标明“图像”。关于注释，BB代表边框。属性包括任何物体特征（例如行人特征，行为）。车辆传感器可能包括速度、转向角度、GPS等。时间段标识那些指定事件开始和结束的数据集。'
- en: '| Year | Dataset | Type | Annotations | Application |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 年 | 数据集 | 类型 | 注释 | 应用 |'
- en: '| V | A | T | M | O |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| V | A | T | M | O |'
- en: '| 2013 | 50Salads [[284](#bib.bib284)] | Cooking (ego) | RGBD, Activity, Temporal
    seg. |  | x |  |  |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | 50沙拉 [[284](#bib.bib284)] | 烹饪（自我） | RGBD, 活动, 时间段 |  | x |  |  |  |'
- en: '| ATC [[285](#bib.bib285)] | Surveillance | RGB, Traj., Attrib. |  |  | x |  |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| ATC [[285](#bib.bib285)] | 监控 | RGB, 轨迹, 属性 |  |  | x |  |  |'
- en: '| CAD-120 [[286](#bib.bib286)] | Activities | RGBD, 3D Pose, Activity |  |
    x |  |  |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| CAD-120 [[286](#bib.bib286)] | 活动 | RGBD, 3D姿态, 活动 |  | x |  |  |  |'
- en: '| CHUK Avenue [[287](#bib.bib287)] | Surveillance | RGB, BB, Anomaly, Temporal
    seg. | x |  | x |  |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| CHUK Avenue [[287](#bib.bib287)] | 监控 | RGB, 边框, 异常, 时间段 | x |  | x |  |  |'
- en: '| Daimler Path [[288](#bib.bib288)] | Traffic | Stereo Grayscale, BB, Temporal
    seg. , Vehicle sensors |  | x |  |  |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Daimler Path [[288](#bib.bib288)] | 交通 | 立体灰度, 边框, 时间段, 车辆传感器 |  | x |  |  |  |'
- en: '| Joint-annotated HMDB (JHMDB) [[289](#bib.bib289)] | Activities | RGB, Mask,
    Activity, Pose, Optical flow | x | x |  |  |  |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Joint-annotated HMDB (JHMDB) [[289](#bib.bib289)] | 活动 | RGB, 遮罩, 活动, 姿态,
    光流 | x | x |  |  |  |'
- en: '| Penn Action [[290](#bib.bib290)] | Activities | RGB, BB, Pose, Activity |
    x |  |  | x |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Penn Action [[290](#bib.bib290)] | 活动 | RGB, 边框, 姿态, 活动 | x |  |  | x |  |'
- en: '| 2012 | BIT [[291](#bib.bib291)] | Interaction | RGB, Activity |  | x |  |  |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | BIT [[291](#bib.bib291)] | 互动 | RGB, 活动 |  | x |  |  |  |'
- en: '| GTEA Gaze [[292](#bib.bib292)] | Cooking (ego) | RGB, Gaze, Mask, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| GTEA Gaze [[292](#bib.bib292)] | 烹饪（自我） | RGB, 注视, 遮罩, 活动, 时间段 |  | x |  |  |  |'
- en: '| KITTI [[293](#bib.bib293)] | Traffic | Stereo RGB, LIDAR, BB, Optical flow,
    Vehicle sensors | x |  | x |  | x |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| KITTI [[293](#bib.bib293)] | 交通 | 立体RGB, LIDAR, 边框, 光流, 车辆传感器 | x |  | x
    |  | x |'
- en: '| MANIAC [[294](#bib.bib294)] | Object manipulation | RGBD, Semantic seg.,
    Activity |  | x |  |  |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| MANIAC [[294](#bib.bib294)] | 物体操作 | RGBD, 语义分割, 活动 |  | x |  |  |  |'
- en: '| MPII-Cooking [[295](#bib.bib295)] | Cooking | RGB, 3D Pose, Activity, Temporal
    seg. |  | x |  |  |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| MPII-烹饪 [[295](#bib.bib295)] | 烹饪 | RGB, 3D 姿态, 活动, 时间段 |  | x |  |  |  |'
- en: '| MSR Daily Activity (MSRDA) [[296](#bib.bib296)] | Activities | Depth, Activity
    |  | x |  |  |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| MSR 日常活动 (MSRDA) [[296](#bib.bib296)] | 活动 | 深度, 活动 |  | x |  |  |  |'
- en: '| New York Grand Central (GC) [[297](#bib.bib297)] | Surveillance | RGB, Traj.
    |  |  | x |  |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 纽约大中央 (GC) [[297](#bib.bib297)] | 监控 | RGB, 轨迹 |  |  | x |  |  |'
- en: '| SBU Kinetic Interction (SBUKI) [[298](#bib.bib298)] | Interaction | RGBD,
    3D Pose, Activity |  | x | x | x |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| SBU 运动互动 (SBUKI) [[298](#bib.bib298)] | 互动 | RGBD, 3D 姿态, 活动 |  | x | x |
    x |  |'
- en: '| UCF-101 [[299](#bib.bib299)] | Activities | RGB, Activity | x | x |  | x
    |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| UCF-101 [[299](#bib.bib299)] | 活动 | RGB, 活动 | x | x |  | x |  |'
- en: '| UTKinect-Action (UTKA) [[300](#bib.bib300)] | Activities | RGBD, 3D Pose,
    Activity, Temporal seg. |  | x |  |  |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| UTKinect-Action (UTKA) [[300](#bib.bib300)] | 活动 | RGBD, 3D 姿态, 活动, 时间段 |  |
    x |  |  |  |'
- en: '| UvA-NEMO [[301](#bib.bib301)] | Smiles | RGB | x |  |  |  |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| UvA-NEMO [[301](#bib.bib301)] | 微笑 | RGB | x |  |  |  |  |'
- en: '| 2011 | Ford campus vision LiDAR (FCVL) [[302](#bib.bib302)] | Traffic | RGB,
    LIDAR, Vehicle sensors |  |  |  |  | x |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 2011 | 福特校园视觉激光雷达 (FCVL) [[302](#bib.bib302)] | 交通 | RGB, LIDAR, 车辆传感器 |  |  |  |  |
    x |'
- en: '| Human Motion Database (HMDB) [[303](#bib.bib303)] | Activities | RGB, BB,
    Mask, Activity |  | x |  |  |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 人体运动数据库 (HMDB) [[303](#bib.bib303)] | 活动 | RGB, BB, 掩模, 活动 |  | x |  |  |  |'
- en: '| Stanford 40 [[304](#bib.bib304)] | Activities | RGB (image), BB, Activity
    |  | x |  |  |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 斯坦福 40 [[304](#bib.bib304)] | 活动 | RGB (图像), BB, 活动 |  | x |  |  |  |'
- en: '| Town Center [[305](#bib.bib305)] | Surveillance | RGB, BB |  |  | x |  |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 市中心 [[305](#bib.bib305)] | 监控 | RGB, BB |  |  | x |  |  |'
- en: '| VIRAT [[306](#bib.bib306)] | Surveillance, Activities | RGB, BB, Activity,
    Temporal seg. |  | x | x |  |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| VIRAT [[306](#bib.bib306)] | 监控, 活动 | RGB, BB, 活动, 时间段 |  | x | x |  |  |'
- en: '| 2010 | DISPLECS [[307](#bib.bib307)] | Traffic | RGB, Vehicle sensors |  |  |  |  |
    x |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 2010 | DISPLECS [[307](#bib.bib307)] | 交通 | RGB, 车辆传感器 |  |  |  |  | x |'
- en: '| MSR [[308](#bib.bib308)] | Activities | Depth, Activity | x |  |  |  |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| MSR [[308](#bib.bib308)] | 活动 | 深度, 活动 | x |  |  |  |  |'
- en: '| MUG [[309](#bib.bib309)] | Facial expressions | RGB, Keypoints, Label | x
    |  |  |  |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| MUG [[309](#bib.bib309)] | 面部表情 | RGB, 关键点, 标签 | x |  |  |  |  |'
- en: '| PROST [[310](#bib.bib310)] | Objects | RGB, BB | x |  |  |  |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| PROST [[310](#bib.bib310)] | 物体 | RGB, BB | x |  |  |  |  |'
- en: '| TV Human Interaction (THI) [[311](#bib.bib311)] | Interaction | RGB, BB,
    Head pose, Activity |  | x |  |  |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| TV 人体互动 (THI) [[311](#bib.bib311)] | 互动 | RGB, BB, 头部姿态, 活动 |  | x |  |  |  |'
- en: '| UT Interaction (UTI) [[312](#bib.bib312)] | Interaction | RGB, BB, Activity,
    Temporal seg. |  | x |  |  |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| UT 互动 (UTI) [[312](#bib.bib312)] | 互动 | RGB, BB, 活动, 时间段 |  | x |  |  |  |'
- en: '| VISOR [[313](#bib.bib313)] | Surveillance | RGB, BB, Pose, Attrib. | x |  |  |  |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| VISOR [[313](#bib.bib313)] | 监控 | RGB, BB, 姿态, 属性 | x |  |  |  |  |'
- en: '| Willow Action [[314](#bib.bib314)] | Activiites | RGB (image), Activity |  |
    x |  |  |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Willow Action [[314](#bib.bib314)] | 活动 | RGB (图像), 活动 |  | x |  |  |  |'
- en: '| 2009 | Caltech Pedestrian [[315](#bib.bib315)] | Traffic | RGB, BB | x |
    x |  |  |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 2009 | 加州理工学院行人 [[315](#bib.bib315)] | 交通 | RGB, BB | x | x |  |  |  |'
- en: '| Collective Activity (CA) [[316](#bib.bib316)] | Interaction | RGB, BB, Attrib.,
    Activity, Temporal seg. |  | x | x | x |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 集体活动 (CA) [[316](#bib.bib316)] | 互动 | RGB, BB, 属性, 活动, 时间段 |  | x | x | x
    |  |'
- en: '| Edinburgh IFP [[317](#bib.bib317)] | Surveillance | RGB, BB |  |  | x |  |  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 爱丁堡 IFP [[317](#bib.bib317)] | 监控 | RGB, BB |  |  | x |  |  |'
- en: '| ETH [[235](#bib.bib235)] | Surveillance | RGB, Traj. |  |  | x |  |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| ETH [[235](#bib.bib235)] | 监控 | RGB, 轨迹 |  |  | x |  |  |'
- en: '| OSU [[318](#bib.bib318)] | Sports | RGB, BB, Attrib. |  |  | x |  |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| OSU [[318](#bib.bib318)] | 体育 | RGB, BB, 属性 |  |  | x |  |  |'
- en: '| PETS2009 [[319](#bib.bib319)] | Surveillance | RGB, BB |  |  | x |  |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| PETS2009 [[319](#bib.bib319)] | 监控 | RGB, BB |  |  | x |  |  |'
- en: '| QMUL [[320](#bib.bib320)] | Traffic, anomaly | RGB, Traj. |  |  | x |  |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| QMUL [[320](#bib.bib320)] | 交通, 异常 | RGB, 轨迹 |  |  | x |  |  |'
- en: '| TUM Kitchen [[321](#bib.bib321)] | Activities | RGB, RFID, 3D Pose, Activity,
    Temporal seg. |  |  | x |  |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| TUM 厨房 [[321](#bib.bib321)] | 活动 | RGB, RFID, 3D 姿态, 活动, 时间段 |  |  | x |  |  |'
- en: '| YUV Videos [[322](#bib.bib322)] | Mix videos | RGB | x |  |  |  |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| YUV 视频 [[322](#bib.bib322)] | 混合视频 | RGB | x |  |  |  |  |'
- en: '| 2008 | Daimler [[323](#bib.bib323)] | Traffic | Grayscale, BB |  | x |  |  |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 2008 | Daimler [[323](#bib.bib323)] | 交通 | 灰度, BB |  | x |  |  |  |'
- en: '| MIT Trajectory (MITT) [[324](#bib.bib324)] | Surveillance | RGB, Traj. |  |  |
    x |  |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| MIT 轨迹 (MITT) [[324](#bib.bib324)] | 监控 | RGB, 轨迹 |  |  | x |  |  |'
- en: '| 2007 | AMOS [[325](#bib.bib325)] | Weather | RGB, Temperature, Time |  |  |  |  |
    x |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 2007 | AMOS [[325](#bib.bib325)] | 天气 | RGB, 温度, 时间 |  |  |  |  | x |'
- en: '| ETH Pedestrian [[326](#bib.bib326)] | Traffic | RGB, BB |  | x |  |  |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| ETH Pedestrian [[326](#bib.bib326)] | 交通 | RGB, BB |  | x |  |  |  |'
- en: '| Lankershim Boulevard [[327](#bib.bib327)] | Traffic | RGB, Traj. |  |  |
    x |  |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Lankershim Boulevard [[327](#bib.bib327)] | 交通 | RGB, 轨迹 |  |  | x |  |  |'
- en: '| Next Generation Simulation (NGSIM) [[328](#bib.bib328)] | Traffic | Map,
    Traj. |  | x | x |  |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 下一代仿真（NGSIM）[[328](#bib.bib328)] | 交通 | 地图, 轨迹 |  | x | x |  |  |'
- en: '| UCY [[329](#bib.bib329)] | Surveillance | RGB, Traj., Gaze |  |  | x |  |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| UCY [[329](#bib.bib329)] | 监控 | RGB, 轨迹, 凝视 |  |  | x |  |  |'
- en: '| 2006 | Tuscan Arizona [[330](#bib.bib330)] | Weather | RGB |  |  |  |  |
    x |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 2006 | Tuscan Arizona [[330](#bib.bib330)] | 天气 | RGB |  |  |  |  | x |'
- en: '| 2004 | KTH [[331](#bib.bib331)] | Activities | Grayscale, Activity | x |  |  |  |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 2004 | KTH [[331](#bib.bib331)] | 活动 | 灰度, 活动 | x |  |  |  |  |'
- en: '| 1981 | Golden Colorado [[332](#bib.bib332)] | Weather | RGB |  |  |  |  |
    x |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 1981 | Golden Colorado [[332](#bib.bib332)] | 天气 | RGB |  |  |  |  | x |'
- en: 'TABLE II: A summary of common datasets from years 2013 and earlier used in
    vision-based prediction applications, namely video (V), action (A), trajectory
    (T), motion (M) and others (O). The annotation column specifies the type of data
    (e.g. RGB, Infrared(IR)) and annotation types. All datasets contain sequences
    unless specified by “image”. As for annotations, BB stands for bounding box. Attributes
    include any object characteristics (e.g. for pedestrians demographics, behavior).
    Vehicle sensors may include speed, steering angle, GPS, etc. Temporal seg. identifies
    the datasets that specify the start and end of the events.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 对于2013年及更早年份中用于基于视觉的预测应用的常见数据集的总结，即视频（V）、动作（A）、轨迹（T）、运动（M）以及其他（O）。注释列指定了数据的类型（例如
    RGB、红外线（IR））和注释类型。所有数据集都包含序列，除非标明为“图像”。在注释方面，BB 代表边界框。属性包括任何物体特征（例如行人特征、行为）。车辆传感器可能包括速度、转向角、GPS
    等。时间段标记识别那些指定事件开始和结束的数据集。'
- en: '![Refer to caption](img/98db438ea0b637bb5c4683dc3b666846.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/98db438ea0b637bb5c4683dc3b666846.png)'
- en: 'Figure 2: An illustration of datasets and papers that use them.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 使用数据集和论文的示意图。'
- en: 'We have identified more than 100 datasets that are used in the vision-based
    prediction literature. Discussing all datasets in detail is beyond the scope of
    this paper. We provide a summary of the datasets and their characteristics in
    Tables [I](#S11.T1 "TABLE I ‣ 11 Datasets ‣ Deep Learning for Vision-based Prediction:
    A Survey") and [II](#S11.T2 "TABLE II ‣ 11 Datasets ‣ Deep Learning for Vision-based
    Prediction: A Survey") and briefly discuss more popular datasets in each field.
    Figure [2](#S11.F2 "Figure 2 ‣ 11 Datasets ‣ Deep Learning for Vision-based Prediction:
    A Survey") illustrates the list of papers and corresponding datasets used for
    evaluation. Note that the papers that do not use publicly available datasets are
    not listed in this figure. For further information, the readers can also refer
    to Appendices [D](#A4 "Appendix D Links to the datasets ‣ Deep Learning for Vision-based
    Prediction: A Survey") and [E](#A5 "Appendix E Datasets and corresponding papers
    ‣ Deep Learning for Vision-based Prediction: A Survey").'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '我们已经识别出100多个在基于视觉的预测文献中使用的数据集。详细讨论所有数据集超出了本文的范围。我们在表格[I](#S11.T1 "表 I ‣ 11
    个数据集 ‣ 基于视觉的预测的深度学习: 综述")和[II](#S11.T2 "表 II ‣ 11 个数据集 ‣ 基于视觉的预测的深度学习: 综述")中提供了数据集及其特性的总结，并简要讨论了各领域中更受欢迎的数据集。图[2](#S11.F2
    "图 2 ‣ 11 个数据集 ‣ 基于视觉的预测的深度学习: 综述")展示了用于评估的论文列表和相应的数据集。请注意，未使用公开数据集的论文未在此图中列出。有关更多信息，读者还可以参考附录[D](#A4
    "附录 D 数据集链接 ‣ 基于视觉的预测的深度学习: 综述")和[E](#A5 "附录 E 数据集及相应论文 ‣ 基于视觉的预测的深度学习: 综述")。'
- en: Video prediction. Almost any forms of sequential RGB images can be used for
    evaluation of video prediction algorithms. Among the most common datasets are
    traffic datasets such a KITTI [[293](#bib.bib293)], and Caltech Pedestrians [[315](#bib.bib315)].
    KITTI is a dataset recorded from inside of a vehicle and contains images of urban
    roads annotated with bounding box information. It also contains depth maps, LIDAR
    point clouds and semantic segmentation maps. Caltech Pedestrian is a similar dataset
    with the difference of only containing RGB images and bounding boxes for pedestrians.
    It also contains occlusion bounding boxes highlighting the visible portions of
    the pedestrians. Activity datasets such as UCF-101 [[299](#bib.bib299)] and Human3.6M
    [[237](#bib.bib237)] are also widely used. UCF-101 contains videos of various
    types of activities such as sports, applying makeup, playing music instruments
    annotated with activity labels per video. Human3.6M consists of 3.6 million 3D
    human poses and corresponding images recorded from 11 professional actors. This
    dataset contains 17 generic scenarios such as discussion, smoking, and taking
    photos.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 视频预测。几乎任何形式的顺序 RGB 图像都可以用于视频预测算法的评估。在最常见的数据集中，有交通数据集如 KITTI [[293](#bib.bib293)]
    和 Caltech Pedestrians [[315](#bib.bib315)]。KITTI 是从车辆内部记录的数据集，包含城市道路的图像，图像中标注有边界框信息。它还包含深度图、LIDAR
    点云和语义分割图。Caltech Pedestrian 是一个类似的数据集，但仅包含 RGB 图像和行人的边界框。它还包含遮挡边界框，突出显示行人的可见部分。活动数据集如
    UCF-101 [[299](#bib.bib299)] 和 Human3.6M [[237](#bib.bib237)] 也被广泛使用。UCF-101 包含各种类型活动的视频，如运动、化妆、演奏乐器，并为每个视频标注了活动标签。Human3.6M
    包含360万个人体 3D 姿态和由11名专业演员录制的相应图像。该数据集包含 17 种通用场景，如讨论、吸烟和拍照。
- en: Action prediction. The algorithms in this domain are evaluated on a wide range
    of datasets. For anticipation tasks, traffic datasets such as Next Generation
    Simulation (NGSIM) [[328](#bib.bib328)] and Joint Attention in Autonomous Driving
    (JAAD) [[78](#bib.bib78)] are used. NGSIM contains trajectories of vehicles driving
    on highways in the United States. The trajectories are accompanied by the top-down
    views of the corresponding road structures. The JAAD dataset contains videos of
    pedestrians crossing the road recorded using an on-board camera. This dataset
    contains the frame-wise pedestrian bounding boxes, and action labels as well as
    pedestrians’ and roads’ attributes. A similar dataset to JAAD is Pedestrian Intention
    Estimation (PIE) [[144](#bib.bib144)] which, in addition, provides the ego-vehicle
    sensor data and spatial annotations for traffic elements.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 动作预测。在这个领域中的算法在各种数据集上进行评估。对于预判任务，使用了交通数据集，如 Next Generation Simulation (NGSIM)
    [[328](#bib.bib328)] 和 Joint Attention in Autonomous Driving (JAAD) [[78](#bib.bib78)]。NGSIM
    包含美国高速公路上车辆的轨迹，这些轨迹配有相应道路结构的俯视图。JAAD 数据集包含使用车载摄像头记录的行人过马路的视频。该数据集包含逐帧的行人边界框、动作标签以及行人和道路的属性。与
    JAAD 类似的数据集还有 Pedestrian Intention Estimation (PIE) [[144](#bib.bib144)]，它除了提供自车传感器数据和交通元素的空间注释外，还提供了额外的信息。
- en: Another popular category of datasets in this domain is those containing videos
    of cooking activities. These datasets are Epic-Kitchen [[248](#bib.bib248)], 50salads
    [[284](#bib.bib284)], Breakfast [[280](#bib.bib280)] and MPII-Cooking [[295](#bib.bib295)].
    These datasets contain videos showing sequences of different cooking actions of
    preparing meals. All videos in the datasets have temporal segments with corresponding
    activity labels. Some datasets also provide additional annotations such as object
    bounding boxes, voice and text in Epic-Kitchen, and the poses of the actors in
    MPII-Cooking.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域中，另一类受欢迎的数据集是包含烹饪活动视频的数据集。这些数据集包括 Epic-Kitchen [[248](#bib.bib248)]、50salads
    [[284](#bib.bib284)]、Breakfast [[280](#bib.bib280)] 和 MPII-Cooking [[295](#bib.bib295)]。这些数据集包含展示不同烹饪动作的准备餐点的序列视频。所有视频都有时间段以及对应的活动标签。有些数据集还提供额外的注释，例如
    Epic-Kitchen 中的物体边界框、语音和文本，以及 MPII-Cooking 中演员的姿势。
- en: Early action prediction works widely use the popular UCF-101 dataset [[299](#bib.bib299)]
    and interaction datasets such as UT Interaction (UTI) [[312](#bib.bib312)] and
    BIT [[291](#bib.bib291)]. UTI and BIT contain videos of people engaged in interaction
    with the corresponding label for the types of interactions. In addition, UTI has
    the added temporal segment annotations detailing different stages of interactions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 早期动作预测工作广泛使用流行的UCF-101数据集 [[299](#bib.bib299)] 和互动数据集如UT Interaction (UTI) [[312](#bib.bib312)]
    和BIT [[291](#bib.bib291)]。UTI和BIT包含了人们互动的视频，并附有互动类型的标签。此外，UTI还增加了时间段注释，详细描述了互动的不同阶段。
- en: Trajectory prediction. The most common datasets in this domain are ETH [[235](#bib.bib235)]
    and UCY [[329](#bib.bib329)] which contain surveillance videos of pedestrians
    walking on sidewalks annotated with their position coordinates. UCY also provides
    the gaze directions to capture the viewing angle of pedestrians. Another popular
    dataset is Stanford Aerial Pedestrian (SAP), also known as Stanford Drone (SD)
    [[265](#bib.bib265)]. This dataset has the footage of road users from a top-down
    view recorded by a drone. The annotations include bounding boxes and object class
    labels.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹预测。该领域最常见的数据集是ETH [[235](#bib.bib235)] 和UCY [[329](#bib.bib329)]，它们包含了行人在人行道上行走的监控视频，并注释了他们的位置坐标。UCY还提供了视线方向以捕捉行人的视角。另一个流行的数据集是斯坦福航空行人（SAP），也称为斯坦福无人机（SD）
    [[265](#bib.bib265)]。该数据集包含了由无人机从俯视角度记录的道路使用者视频。注释包括边界框和物体类别标签。
- en: Motion prediction. The algorithms in this domain are mainly evaluated on the
    widely popular dataset Human 3.6M [[237](#bib.bib237)] described earlier. This
    dataset is particularly suitable for these applications because it contains accurate
    3D poses of the actors recorded by a high-speed motion capture system. Using this
    dataset, the background can be accurately removed allowing the algorithms to focus
    purely on changes in the poses. Another popular dataset in this field is Penn
    Action [[290](#bib.bib290)] which contains RGB videos of various activities with
    corresponding activity labels and poses of the actors involved.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 动作预测。该领域的算法主要在之前描述的广泛流行的数据集Human 3.6M [[237](#bib.bib237)]上进行评估。该数据集特别适合这些应用，因为它包含由高速动作捕捉系统记录的演员准确的3D姿势。使用该数据集，背景可以被准确去除，使算法可以专注于姿势的变化。该领域的另一个流行数据集是Penn
    Action [[290](#bib.bib290)]，它包含各种活动的RGB视频，以及相应的活动标签和参与者的姿势。
- en: Other applications. The most notable datasets are KITTI [[293](#bib.bib293)]
    which is used by the OGM prediction algorithms and CityScapes [[261](#bib.bib261)]
    that is used by the segmentation prediction algorithms. CityScapes contains video
    footage of urban environments recorded by an on-board camera. The data is annotated
    with semantic masks of the traffic objects and corresponding bounding boxes.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 其他应用。最显著的数据集是KITTI [[293](#bib.bib293)]，它被OGM预测算法使用，以及CityScapes [[261](#bib.bib261)]，它被分割预测算法使用。CityScapes包含由车载摄像头记录的城市环境视频素材。数据被注释了交通物体的语义掩膜和相应的边界框。
- en: 12 Summary and Discussion
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12 总结与讨论
- en: 12.1 Architecture
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1 架构
- en: There are many factors that define the choice of architecture for vision-based
    prediction tasks. These factors include the types of input data and expected output,
    computational efficiency, application-specific constrains, etc. For instance,
    in terms of network choice, whether it is feedforward and recurrent, no preference
    is observed in video applications. However, in the case of action, trajectory
    and motion predictions, recurrent architectures are strongly preferred. This can
    be due to the fact that these applications often rely on multi-modal data which
    can be combined easier in a recurrent framework. In the case of trajectory prediction,
    recurrent architectures give the flexibility of varying observation and prediction
    lengths without the need for architectural modifications.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 选择视觉预测任务架构的因素有很多。这些因素包括输入数据的类型和预期输出、计算效率、应用特定的限制等。例如，在网络选择方面，无论是前馈型还是递归型，在视频应用中都没有观察到偏好。然而，在动作、轨迹和运动预测的情况下，递归架构被强烈偏好。这可能是因为这些应用通常依赖于多模态数据，递归框架更易于结合。在轨迹预测的情况下，递归架构提供了观察和预测长度变化的灵活性，而无需进行架构修改。
- en: Generative Adversarial Networks (GANs) are widely used in video prediction applications
    and to some extent in trajectory prediction methods. Some of the major challenges
    using generative models is to deal with inherent uncertainty of future representations,
    in particular, this an issue in the context of trajectory prediction due to high
    unpredictability of human movement. To remedy this issue and to capture uncertainty
    of movement, techniques such as variational auto encoders, in which uncertainty
    is modeled as a latent distribution, and the use of probabilistic objective functions
    are common.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）广泛应用于视频预测以及在某种程度上用于轨迹预测方法。使用生成模型的主要挑战之一是处理未来表示的固有不确定性，特别是在轨迹预测的背景下，由于人类运动的高度不可预测性，这成为了一个问题。为了解决这个问题并捕捉运动的不确定性，诸如变分自编码器等技术，其中不确定性被建模为潜在分布，以及使用概率目标函数都是常见的。
- en: A more recent trend in the field of vision-based prediction (and perhaps in
    other computer vision applications) is the use of attention modules. These modules
    can be applied at spatial or temporal level or even to adjust the impact of different
    modalities of data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉预测领域（以及其他计算机视觉应用领域），一个较新的趋势是使用注意力模块。这些模块可以应用于空间或时间层面，甚至调整不同数据模态的影响。
- en: 12.2 Data representation and processing
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2 数据表示与处理
- en: The type of data and methods of processing vary across different applications.
    For instance, video applications mainly rely on images but also take advantage
    of alternative representations, such as optical flow, poses, object-based keypoints,
    and report improved results. Similarly, many action prediction algorithms use
    different sources of information such as optical flow, poses, scene attributes,
    text, complementary sensor readings (e.g. speed in vehicles), gaze and time of
    the actions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型和处理方法因不同应用而异。例如，视频应用主要依赖于图像，但也利用了诸如光流、姿态、基于对象的关键点等替代表示，并报告了改进的结果。同样，许多动作预测算法使用不同的信息来源，如光流、姿态、场景属性、文本、补充传感器读数（如车辆速度）、注视和动作时间。
- en: Trajectory prediction algorithms, predominantly rely on trajectory information,
    with some exceptions that use scene layouts, complimentary sensors’ readings or
    other constrains. One of the main applications in this domain, in particular surveillance,
    is modeling the social interaction between the dynamic agents. Unlike other vision-based
    applications, motion prediction algorithms are mainly single-modal and use only
    poses and the images of agents as inputs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹预测算法主要依赖于轨迹信息，但也有一些例外使用场景布局、补充传感器读数或其他约束。在这一领域，特别是在监控方面的主要应用是建模动态代理之间的社会互动。与其他视觉应用不同，运动预测算法主要是单模态的，仅使用代理的姿态和图像作为输入。
- en: 12.3 Evaluation
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3 评估
- en: 12.3.1 Metrics
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.1 指标
- en: Metrics may vary across different applications of vision-based prediction. Video
    prediction algorithms, for instance, are mainly evaluated using MSE, SSIM, and
    PSNR, whereas in the case of action prediction algorithms the main metrics are
    accuracy, precision, and recall. Trajectory prediction works often measure the
    average distance (ADE) or final distance (FDE) between the actual and predicted
    locations of the agents. The models with probabilistic outputs are also evaluated
    using NLL and KLD metrics. Distance-based metrics are used in motion prediction
    methods where the error in joint prediction is either calculated on average (MJE)
    or per joint (MPJPE). In addition, joint accuracy can be reported in terms of
    the percentage of correct prediction using PCK metric. In this case, a tolerance
    threshold is defined to determine whether a predicted joint is correct.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 指标可能因视觉预测的不同应用而有所不同。例如，视频预测算法主要使用MSE、SSIM和PSNR进行评估，而在动作预测算法的情况下，主要指标是准确率、精确率和召回率。轨迹预测工作通常测量实际位置和预测位置之间的平均距离（ADE）或最终距离（FDE）。具有概率输出的模型也使用NLL和KLD指标进行评估。在运动预测方法中，距离相关的指标用于计算关节预测中的误差，通常计算为平均值（MJE）或每个关节（MPJPE）。此外，关节准确率可以通过PCK指标报告，PCK指标定义了一个容忍阈值来判断预测的关节是否正确。
- en: While calculating performance error for video and action prediction algorithms
    are fairly standardized, there are major discrepancies across different works
    in the way error is computed for trajectory and motion prediction algorithms.
    For example, in trajectory prediction, distance error is calculated by using metrics
    such as MSE, RMSE, ED, etc. and units such as pixels and meters. Such discrepancy,
    and the fact that many works omit mentioning the choice of error metrics and units,
    increases the chance of incorrect comparisons between models.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管视频和动作预测算法的性能误差计算相对标准化，但在轨迹和运动预测算法的误差计算方式上，各个研究之间存在较大差异。例如，在轨迹预测中，距离误差通过使用诸如
    MSE、RMSE、ED 等指标以及像素和米等单位来计算。这样的差异以及许多研究省略提及误差指标和单位的选择，增加了模型间不正确比较的可能性。
- en: 12.3.2 Datasets
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.2 数据集
- en: The choice of datasets depends on the objective of the applications. For example,
    action prediction algorithms for cooking activities are evaluated on datasets
    such as Epic-Kitchen, 50Salads, Breakfast, and MPII-Cooking and the ones for traffic
    events evaluated on JAAD, NSGIM, and PIE. Similarly, trajectory prediction works
    for surveillance widely use UCY, ETH, and SD and for traffic NGSIM. Motion prediction
    algorithms are more focusing on individual movements in diverse context, therefore
    predominantly use Human3.6M and Penn Action datasets.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的选择取决于应用的目标。例如，针对烹饪活动的动作预测算法在 Epic-Kitchen、50Salads、Breakfast 和 MPII-Cooking
    数据集上进行评估，而针对交通事件的算法在 JAAD、NSGIM 和 PIE 上进行评估。类似地，监控中的轨迹预测工作广泛使用 UCY、ETH 和 SD 数据集，交通预测则使用
    NGSIM 数据集。运动预测算法更侧重于不同背景下的个体动作，因此主要使用 Human3.6M 和 Penn Action 数据集。
- en: Compared to other applications, video prediction is an exception. The algorithms
    in this group are evaluated on almost any datasets with video content. The algorithms
    in this domain are often task agnostic meaning that the same approaches are evaluated
    on datasets with traffic scenes (e.g. KITTI, Caltech Pedestrian), general activities
    (e.g. UCF-101, Penn Action), basic actions (e.g. Human3.6M, KTH) and synthetic
    data (e.g. MMNIST, Atari games). Although such generalizability across different
    domains is a desirable feature in video prediction algorithms, it is often the
    case that the reason behind the choice of datasets is not discussed raising the
    question of whether the decision for selecting particular datasets is motivated
    by the limitations of the algorithms.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他应用相比，视频预测是一个例外。该组中的算法几乎在任何包含视频内容的数据集上进行评估。该领域的算法通常是任务不可知的，即相同的方法可以在包含交通场景（例如
    KITTI、Caltech Pedestrian）、一般活动（例如 UCF-101、Penn Action）、基本动作（例如 Human3.6M、KTH）和合成数据（例如
    MMNIST、Atari games）的数据集上进行评估。尽管这种跨领域的泛化能力是视频预测算法的一个可取特性，但往往没有讨论选择数据集的原因，这引发了选择特定数据集是否受到算法限制的质疑。
- en: 12.4 What’s next
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4 接下来做什么
- en: In recent years we have witnessed an exponential growth in the number of works
    published in the field of vision-based prediction. There are still, however, many
    open research problems in the field that need to be addressed.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们见证了基于视觉的预测领域作品数量的指数增长。然而，该领域仍存在许多亟待解决的研究问题。
- en: The ability to hallucinate or generate parts of the image that were not previously
    observed is still a major challenge in video prediction applications. In addition,
    the algorithms in this domain cannot deal with cases where some objects go out
    of view in future time frames. The performances of action prediction algorithms
    are still sub-optimal, especially in safety critical and complex tasks such as
    event prediction in traffic scenes. To make predictions in such cases, many modalities
    of data and the relationships between them should be considered which is often
    not the case in the proposed approaches.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在视频预测应用中，生成或虚拟化之前未观察到的图像部分仍然是一个重大挑战。此外，该领域的算法无法处理未来时间帧中某些对象消失的情况。动作预测算法的表现仍未达到最佳，特别是在如交通场景中的事件预测等安全关键和复杂任务中。要在这种情况下进行预测，应该考虑许多数据模态及其之间的关系，而这在提出的方法中往往没有被考虑到。
- en: Trajectory prediction algorithms mainly rely on changes in the location of the
    agents to predict their future states. Although, this might be an effective approach
    for tasks such as surveillance, in many other cases it might not be sufficient.
    For example, in order to predict trajectories of pedestrians in traffic scenes,
    many other sources of information, such as their poses and orientation, road structure,
    interactions, road conditions, traffic flow, etc., are potentially relevant. Such
    contextual analysis can also be beneficial for motion prediction algorithms which
    manly rely on the changes in poses to predict the future.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹预测算法主要依赖于代理位置的变化来预测其未来状态。然而，这种方法对于任务如监控可能有效，但在许多其他情况下可能不足以应对。例如，为了预测交通场景中行人的轨迹，许多其他信息来源，如他们的姿势和方向、道路结构、互动、道路状况、交通流量等，都可能相关。这种背景分析也对主要依赖姿势变化来预测未来的运动预测算法有利。
- en: In terms of the choice of learning architectures and training schemes, a systematic
    comparison of different approaches, e.g. using feedforward vs recurrent networks,
    the benefits of using adversarial training schemes, various uncertainty modeling
    approaches, etc. is missing. Such information can be partially extracted from
    the existing literature, however, in many cases it is not possible due to the
    lack of standard evaluation procedures and metrics, unavailability of corresponding
    implementation code and the datasets used for comparison.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习架构和训练方案的选择方面，系统性比较不同方法，如使用前馈与递归网络、使用对抗训练方案的好处、各种不确定性建模方法等，仍然缺失。这类信息可以从现有文献中部分提取，但由于缺乏标准评估程序和指标、相关实现代码及比较使用的数据集的缺失，许多情况下无法实现。
- en: References
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Gao, H. Xu, Q.-Z. Cai, R. Wang, F. Yu, and T. Darrell, “Disentangling
    propagation and generation for video prediction,” in *ICCV*, 2019.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Gao、H. Xu、Q.-Z. Cai、R. Wang、F. Yu 和 T. Darrell，“解开传播与生成的关系用于视频预测”，发表于*ICCV*，2019年。'
- en: '[2] Y.-H. Kwon and M.-G. Park, “Predicting future frames using retrospective
    cycle gan,” in *CVPR*, 2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Y.-H. Kwon 和 M.-G. Park，“使用回顾性循环生成对抗网络预测未来帧”，发表于*CVPR*，2019年。'
- en: '[3] L. Castrejon, N. Ballas, and A. Courville, “Improved conditional vrnns
    for video prediction,” in *ICCV*, 2019.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] L. Castrejon、N. Ballas 和 A. Courville，“改进的条件VRNNs用于视频预测”，发表于*ICCV*，2019年。'
- en: '[4] Y.-H. Ho, C.-Y. Cho, W.-H. Peng, and G.-L. Jin, “Sme-net: Sparse motion
    estimation for parametric video prediction through reinforcement learning,” in
    *ICCV*, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Y.-H. Ho、C.-Y. Cho、W.-H. Peng 和 G.-L. Jin，“Sme-net: 通过强化学习进行稀疏运动估计的参数视频预测”，发表于*ICCV*，2019年。'
- en: '[5] C. Zhang, T. Chen, H. Liu, Q. Shen, and Z. Ma, “Looking-ahead: Neural future
    video frame prediction,” in *ICIP*, 2019.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] C. 张、T. 陈、H. 刘、Q. 沈 和 Z. 马，“Looking-ahead: 神经网络未来视频帧预测”，发表于*ICIP*，2019年。'
- en: '[6] X. Liang, L. Lee, W. Dai, and E. P. Xing, “Dual motion gan for future-flow
    embedded video prediction,” in *ICCV*, 2017.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] X. Liang、L. Lee、W. Dai 和 E. P. Xing，“双重运动生成对抗网络用于未来流嵌入的视频预测”，发表于*ICCV*，2017年。'
- en: '[7] D. Ji, Z. Wei, E. Dunn, and J. M. Frahm, “Dynamic visual sequence prediction
    with motion flow networks,” in *WACV*, 2018.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. 吉、Z. 韦、E. 邓 和 J. M. Frahm，“动态视觉序列预测与运动流网络”，发表于*WACV*，2018年。'
- en: '[8] K.-H. Zeng, W. B. Shen, D.-A. Huang, M. Sun, and J. Carlos Niebles, “Visual
    forecasting by imitating dynamics in natural sequences,” in *ICCV*, 2017.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] K.-H. Zeng、W. B. Shen、D.-A. Huang、M. Sun 和 J. Carlos Niebles，“通过模仿自然序列中的动态进行视觉预测”，发表于*ICCV*，2017年。'
- en: '[9] P. Gujjar and R. Vaughan, “Classifying pedestrian actions in advance using
    predicted video of urban driving scenes,” in *ICRA*, 2019.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. Gujjar 和 R. Vaughan，“使用预测的城市驾驶场景视频提前分类行人动作”，发表于*ICRA*，2019年。'
- en: '[10] A. Terwilliger, G. Brazil, and X. Liu, “Recurrent flow-guided semantic
    forecasting,” in *WACV*, 2019.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Terwilliger、G. Brazil 和 X. Liu，“递归流引导的语义预测”，发表于*WACV*，2019年。'
- en: '[11] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang, “Flow-grounded
    spatial-temporal video prediction from still images,” in *ECCV*, 2018.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Y. Li、C. Fang、J. Yang、Z. Wang、X. Lu 和 M.-H. Yang，“基于流的空间-时间视频预测从静态图像”，发表于*ECCV*，2018年。'
- en: '[12] F. A. Reda, G. Liu, K. J. Shih, R. Kirby, J. Barker, D. Tarjan, A. Tao,
    and B. Catanzaro, “Sdc-net: Video prediction using spatially-displaced convolution,”
    in *ECCV*, 2018.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] F. A. Reda、G. Liu、K. J. Shih、R. Kirby、J. Barker、D. Tarjan、A. Tao 和 B.
    Catanzaro，“SDC-Net: 使用空间位移卷积的视频预测”，发表于*ECCV*，2018年。'
- en: '[13] P. Bhattacharjee and S. Das, “Predicting video frames using feature based
    locally guided objectives,” in *ACCV*, C. Jawahar, H. Li, G. Mori, and K. Schindler,
    Eds., 2019.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] P. Bhattacharjee 和 S. Das，“使用基于特征的局部引导目标预测视频帧”，发表于*ACCV*，C. Jawahar、H.
    Li、G. Mori 和 K. Schindler 主编，2019年。'
- en: '[14] G. Ying, Y. Zou, L. Wan, Y. Hu, and J. Feng, “Better guider predicts future
    better: Difference guided generative adversarial networks,” in *ACCV*, C. Jawahar,
    H. Li, G. Mori, and K. Schindler, Eds., 2018.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] G. Ying, Y. Zou, L. Wan, Y. Hu, 和 J. Feng，“更好的引导者预测未来更好：差异引导的生成对抗网络，”
    发表在 *ACCV*，C. Jawahar, H. Li, G. Mori, 和 K. Schindler 编辑，2018 年。'
- en: '[15] P. Bhattacharjee and S. Das, “Temporal coherency based criteria for predicting
    video frames using deep multi-stage generative adversarial networks,” in *NeurIPS*,
    2017.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] P. Bhattacharjee 和 S. Das，“基于时间一致性的标准，通过深度多阶段生成对抗网络预测视频帧，” 发表在 *NeurIPS*，2017
    年。'
- en: '[16] M. Oliu, J. Selva, and S. Escalera, “Folded recurrent neural networks
    for future video prediction,” in *ECCV*, 2018.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Oliu, J. Selva, 和 S. Escalera，“用于未来视频预测的折叠递归神经网络，” 发表在 *ECCV*，2018
    年。'
- en: '[17] Y. Ye, M. Singh, A. Gupta, and S. Tulsiani, “Compositional video prediction,”
    in *ICCV*, 2019.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Ye, M. Singh, A. Gupta, 和 S. Tulsiani，“组合视频预测，” 发表在 *ICCV*，2019 年。'
- en: '[18] Y. Kim, S. Nam, I. Cho, and S. J. Kim, “Unsupervised keypoint learning
    for guiding class-conditional video prediction,” in *NeurIPS*, 2019.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. Kim, S. Nam, I. Cho, 和 S. J. Kim，“用于指导类别条件视频预测的无监督关键点学习，” 发表在 *NeurIPS*，2019
    年。'
- en: '[19] J. Wang, B. Hu, Y. Long, and Y. Guan, “Order matters: Shuffling sequence
    generation for video prediction,” in *BMVC*, 2019.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Wang, B. Hu, Y. Long, 和 Y. Guan，“顺序重要：视频预测的序列打乱生成，” 发表在 *BMVC*，2019
    年。'
- en: '[20] Y. Ho, C. Cho, and W. Peng, “Deep reinforcement learning for video prediction,”
    in *ICIP*, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Ho, C. Cho, 和 W. Peng，“用于视频预测的深度强化学习，” 发表在 *ICIP*，2019 年。'
- en: '[21] J. Tang, H. Hu, Q. Zhou, H. Shan, C. Tian, and T. Q. S. Quek, “Pose guided
    global and local gan for appearance preserving human video prediction,” in *ICIP*,
    2019.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Tang, H. Hu, Q. Zhou, H. Shan, C. Tian, 和 T. Q. S. Quek，“姿态引导的全局和局部
    GAN 用于外观保留的人类视频预测，” 发表在 *ICIP*，2019 年。'
- en: '[22] H. Cai, C. Bai, Y.-W. Tai, and C.-K. Tang, “Deep video generation, prediction
    and completion of human action sequences,” in *ECCV*, 2018.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Cai, C. Bai, Y.-W. Tai, 和 C.-K. Tang，“深度视频生成、预测和人类动作序列的补全，” 发表在 *ECCV*，2018
    年。'
- en: '[23] J.-T. Hsieh, B. Liu, D.-A. Huang, L. F. Fei-Fei, and J. C. Niebles, “Learning
    to decompose and disentangle representations for video prediction,” in *NeurIPS*,
    2018.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J.-T. Hsieh, B. Liu, D.-A. Huang, L. F. Fei-Fei, 和 J. C. Niebles，“学习分解和解耦表示以进行视频预测，”
    发表在 *NeurIPS*，2018 年。'
- en: '[24] J. Xu, B. Ni, and X. Yang, “Video prediction via selective sampling,”
    in *NeurIPS*, 2018.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Xu, B. Ni, 和 X. Yang，“通过选择性采样的视频预测，” 发表在 *NeurIPS*，2018 年。'
- en: '[25] N. Wichers, R. Villegas, D. Erhan, and H. Lee, “Hierarchical long-term
    video prediction without supervision,” in *ICML*, 2018.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] N. Wichers, R. Villegas, D. Erhan, 和 H. Lee，“无监督的层次化长期视频预测，” 发表在 *ICML*，2018
    年。'
- en: '[26] J. Walker, K. Marino, A. Gupta, and M. Hebert, “The pose knows: Video
    forecasting by generating pose futures,” in *ICCV*, 2017.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Walker, K. Marino, A. Gupta, 和 M. Hebert，“姿态知道：通过生成姿态未来进行视频预测，” 发表在
    *ICCV*，2017 年。'
- en: '[27] Y. Wang, M. Long, J. Wang, Z. Gao, and P. S. Yu, “Predrnn: Recurrent neural
    networks for predictive learning using spatiotemporal lstms,” in *NeurIPS*, 2017.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Wang, M. Long, J. Wang, Z. Gao, 和 P. S. Yu，“Predrnn: 使用时空 LSTM 的递归神经网络预测学习，”
    发表在 *NeurIPS*，2017 年。'
- en: '[28] R. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and H. Lee, “Learning to
    generate long-term future via hierarchical prediction,” in *ICML*, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, 和 H. Lee，“通过层次化预测学习生成长期未来，”
    发表在 *ICML*，2017 年。'
- en: '[29] C. Finn, I. Goodfellow, and S. Levine, “Unsupervised learning for physical
    interaction through video prediction,” in *NeurIPS*, 2016.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] C. Finn, I. Goodfellow, 和 S. Levine，“通过视频预测进行物理交互的无监督学习，” 发表在 *NeurIPS*，2016
    年。'
- en: '[30] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh, “Action-conditional
    video prediction using deep networks in atari games,” in *NeurIPS*, 2015.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Oh, X. Guo, H. Lee, R. L. Lewis, 和 S. Singh，“使用深度网络在 Atari 游戏中的动作条件视频预测，”
    发表在 *NeurIPS*，2015 年。'
- en: '[31] J. Lee, J. Lee, S. Lee, and S. Yoon, “Mutual suppression network for video
    prediction using disentangled features,” in *BMVC*, 2019.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Lee, J. Lee, S. Lee, 和 S. Yoon，“基于解耦特征的视频预测互抑制网络，” 发表在 *BMVC*，2019
    年。'
- en: '[32] J. Xu, B. Ni, Z. Li, S. Cheng, and X. Yang, “Structure preserving video
    prediction,” in *CVPR*, 2018.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Xu, B. Ni, Z. Li, S. Cheng, 和 X. Yang，“结构保持的视频预测，” 发表在 *CVPR*，2018
    年。'
- en: '[33] W. Byeon, Q. Wang, R. Kumar Srivastava, and P. Koumoutsakos, “Contextvp:
    Fully context-aware video prediction,” in *ECCV*, 2018.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] W. Byeon, Q. Wang, R. Kumar Srivastava, 和 P. Koumoutsakos，“Contextvp:
    完全上下文感知的视频预测，” 发表在 *ECCV*，2018 年。'
- en: '[34] W. Liu, A. Sharma, O. Camps, and M. Sznaier, “Dyan: A dynamical atoms-based
    network for video prediction,” in *ECCV*, 2018.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] W. Liu, A. Sharma, O. Camps, 和 M. Sznaier，“Dyan: 一种基于动态原子的网络用于视频预测，” 发表在
    *ECCV*，2018 年。'
- en: '[35] B. Jin, Y. Hu, Y. Zeng, Q. Tang, S. Liu, and J. Ye, “Varnet: Exploring
    variations for unsupervised video prediction,” in *IROS*, 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] B. Jin, Y. Hu, Y. Zeng, Q. Tang, S. Liu, 和 J. Ye，“Varnet：探索无监督视频预测的变异性，”发表于
    *IROS*，2018。'
- en: '[36] C. Lu, M. Hirsch, and B. Scholkopf, “Flexible spatio-temporal networks
    for video prediction,” in *CVPR*, 2017.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] C. Lu, M. Hirsch, 和 B. Scholkopf，“用于视频预测的灵活时空网络，”发表于 *CVPR*，2017。'
- en: '[37] M. Jung, T. Matsumoto, and J. Tani, “Goal-directed behavior under variational
    predictive coding: Dynamic organization of visual attention and working memory,”
    in *IROS*, 2019.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] M. Jung, T. Matsumoto, 和 J. Tani，“在变分预测编码下的目标导向行为：视觉注意力和工作记忆的动态组织，”发表于
    *IROS*，2019。'
- en: '[38] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv:1312.6114*,
    2013.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] D. P. Kingma 和 M. Welling，“自编码变分贝叶斯，” *arXiv:1312.6114*，2013。'
- en: '[39] D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic backpropagation
    and approximate inference in deep generative models,” *arXiv:1401.4082*, 2014.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] D. J. Rezende, S. Mohamed, 和 D. Wierstra，“深度生成模型中的随机反向传播和近似推断，” *arXiv:1401.4082*，2014。'
- en: '[40] A. T. Schulz and R. Stiefelhagen, “Pedestrian intention recognition using
    latent-dynamic conditional random fields,” in *IV*, 2015.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. T. Schulz 和 R. Stiefelhagen，“基于潜在动态条件随机场的行人意图识别，”发表于 *IV*，2015。'
- en: '[41] J.-F. Hu, W.-S. Zheng, L. Ma, G. Wang, and J. Lai, “Real-time rgb-d activity
    prediction by soft regression,” in *ECCV*, B. Leibe, J. Matas, N. Sebe, and M. Welling,
    Eds., 2016.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J.-F. Hu, W.-S. Zheng, L. Ma, G. Wang, 和 J. Lai，“通过软回归进行实时 rgb-d 活动预测，”发表于
    *ECCV*，B. Leibe, J. Matas, N. Sebe, 和 M. Welling 编，2016。'
- en: '[42] F. Schneemann and P. Heinemann, “Context-based detection of pedestrian
    crossing intention for autonomous driving in urban environments,” in *IROS*, 2016.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] F. Schneemann 和 P. Heinemann，“基于上下文的行人过马路意图检测，用于城市环境中的自动驾驶，”发表于 *IROS*，2016。'
- en: '[43] B. Völz, K. Behrendt, H. Mielenz, I. Gilitschenski, R. Siegwart, and J. Nieto,
    “A data-driven approach for pedestrian intention estimation,” in *ITSC*, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] B. Völz, K. Behrendt, H. Mielenz, I. Gilitschenski, R. Siegwart, 和 J.
    Nieto，“一种数据驱动的行人意图估计方法，”发表于 *ITSC*，2016。'
- en: '[44] Z. Xu, L. Qing, and J. Miao, “Activity auto-completion: Predicting human
    activities from partial videos,” in *ICCV*, 2015.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Z. Xu, L. Qing, 和 J. Miao，“活动自动补全：从部分视频中预测人类活动，”发表于 *ICCV*，2015。'
- en: '[45] H. Zhang and L. E. Parker, “Bio-inspired predictive orientation decomposition
    of skeleton trajectories for real-time human activity prediction,” in *ICRA*,
    2015.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] H. Zhang 和 L. E. Parker，“生物启发的骨架轨迹预测的预测方向分解，用于实时人类活动预测，”发表于 *ICRA*，2015。'
- en: '[46] S. Köhler, M. Goldhammer, K. Zindler, K. Doll, and K. Dietmeyer, “Stereo-vision-based
    pedestrian’s intention detection in a moving vehicle,” in *ITSC*, 2015.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] S. Köhler, M. Goldhammer, K. Zindler, K. Doll, 和 K. Dietmeyer，“基于立体视觉的行人意图检测在移动车辆中，”发表于
    *ITSC*，2015。'
- en: '[47] B. Völz, H. Mielenz, G. Agamennoni, and R. Siegwart, “Feature relevance
    estimation for learning pedestrian behavior at crosswalks,” in *ITSC*, 2015.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] B. Völz, H. Mielenz, G. Agamennoni, 和 R. Siegwart，“学习行人过马路行为的特征相关性估计，”发表于
    *ITSC*，2015。'
- en: '[48] R. C. Luo and L. Mai, “Human intention inference and on-line human hand
    motion prediction for human-robot collaboration,” in *IROS*, 2019.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] R. C. Luo 和 L. Mai，“人类意图推断与人机协作中的在线手部运动预测，”发表于 *IROS*，2019。'
- en: '[49] M. Wu, T. Louw, M. Lahijanian, W. Ruan, X. Huang, N. Merat, and M. Kwiatkowska,
    “Gaze-based intention anticipation over driving manoeuvres in semi-autonomous
    vehicles,” in *IROS*, 2019.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. Wu, T. Louw, M. Lahijanian, W. Ruan, X. Huang, N. Merat, 和 M. Kwiatkowska，“基于凝视的意图预测在半自主车辆的驾驶动作中，”发表于
    *IROS*，2019。'
- en: '[50] N. Rhinehart and K. M. Kitani, “First-person activity forecasting with
    online inverse reinforcement learning,” in *ICCV*, 2017.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] N. Rhinehart 和 K. M. Kitani，“利用在线逆强化学习进行第一人称活动预测，”发表于 *ICCV*，2017。'
- en: '[51] J.-Y. Kwak, B. C. Ko, and J.-Y. Nam, “Pedestrian intention prediction
    based on dynamic fuzzy automata for vehicle driving at nighttime,” *Infrared Physics
    & Technology*, vol. 81, pp. 41–51, 2017.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] J.-Y. Kwak, B. C. Ko, 和 J.-Y. Nam，“基于动态模糊自动机的夜间驾驶行人意图预测，” *Infrared Physics
    & Technology*，第81卷，第41–51页，2017。'
- en: '[52] N. Hu, A. Bestick, G. Englebienne, R. Bajscy, and B. Kröse, “Human intent
    forecasting using intrinsic kinematic constraints,” in *IROS*, 2016.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] N. Hu, A. Bestick, G. Englebienne, R. Bajscy, 和 B. Kröse，“利用内在运动学约束进行人类意图预测，”发表于
    *IROS*，2016。'
- en: '[53] A. Jain, H. S. Koppula, B. Raghavan, S. Soh, and A. Saxena, “Car that
    knows before you do: Anticipating maneuvers via learning temporal driving models,”
    in *ICCV*, 2015.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] A. Jain, H. S. Koppula, B. Raghavan, S. Soh, 和 A. Saxena，“汽车预知你之前的行为：通过学习时间驾驶模型预测动作，”发表于
    *ICCV*，2015。'
- en: '[54] J. Hariyono, A. Shahbaz, L. Kurnianggoro, and K.-H. Jo, “Estimation of
    collision risk for improving driver’s safety,” in *IECON*, 2016.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] J. Hariyono, A. Shahbaz, L. Kurnianggoro, 和 K.-H. Jo，“为了提高驾驶员安全，碰撞风险估计”，发表于*IECON*，2016年。'
- en: '[55] Y. Hashimoto, Y. Gu, L.-T. Hsu, and S. Kamijo, “Probability estimation
    for pedestrian crossing intention at signalized crosswalks,” in *ICVES*, 2015.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Hashimoto, Y. Gu, L.-T. Hsu, 和 S. Kamijo，“在信号灯斑马线上预测行人过马路意图的概率估计”，发表于*ICVES*，2015年。'
- en: '[56] H. Joo, T. Simon, M. Cikara, and Y. Sheikh, “Towards social artificial
    intelligence: Nonverbal social signal prediction in a triadic interaction,” in
    *CVPR*, 2019.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] H. Joo, T. Simon, M. Cikara, 和 Y. Sheikh，“走向社会人工智能：三方互动中的非语言社会信号预测”，发表于*CVPR*，2019年。'
- en: '[57] F. Ziaeetabar, T. Kulvicius, M. Tamosiunaite, and F. Wörgötter, “Prediction
    of manipulation action classes using semantic spatial reasoning,” in *IROS*, 2018.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] F. Ziaeetabar, T. Kulvicius, M. Tamosiunaite, 和 F. Wörgötter，“利用语义空间推理预测操控动作类别”，发表于*IROS*，2018年。'
- en: '[58] S. Qi, S. Huang, P. Wei, and S.-C. Zhu, “Predicting human activities using
    stochastic grammar,” in *ICCV*, 2017.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] S. Qi, S. Huang, P. Wei, 和 S.-C. Zhu，“使用随机语法预测人类活动”，发表于*ICCV*，2017年。'
- en: '[59] C. Park, J. Ondřej, M. Gilbert, K. Freeman, and C. O’Sullivan, “Hi robot:
    Human intention-aware robot planning for safe and efficient navigation in crowds,”
    in *IROS*, 2016.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] C. Park, J. Ondřej, M. Gilbert, K. Freeman, 和 C. O’Sullivan，“嗨，机器人：基于人类意图的安全高效导航规划”，发表于*IROS*，2016年。'
- en: '[60] T. Mahmud, M. Hasan, A. Chakraborty, and A. K. Roy-Chowdhury, “A poisson
    process model for activity forecasting,” in *ICIP*, 2016.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] T. Mahmud, M. Hasan, A. Chakraborty, 和 A. K. Roy-Chowdhury，“一种用于活动预测的泊松过程模型”，发表于*ICIP*，2016年。'
- en: '[61] K. Xu, Z. Qin, and G. Wang, “Human activities prediction by learning combinatorial
    sparse representations,” in *ICIP*, 2016.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] K. Xu, Z. Qin, 和 G. Wang，“通过学习组合稀疏表示预测人类活动”，发表于*ICIP*，2016年。'
- en: '[62] C. Pérez-D’Arpino and J. A. Shah, “Fast target prediction of human reaching
    motion for cooperative human-robot manipulation tasks using time series classification,”
    in *ICRA*, 2015.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] C. Pérez-D’Arpino 和 J. A. Shah，“使用时间序列分类快速预测人类达动的目标用于合作人机操作任务”，发表于*ICRA*，2015年。'
- en: '[63] Q. Ke, M. Fritz, and B. Schiele, “Time-conditioned action anticipation
    in one shot,” in *CVPR*, 2019.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Q. Ke, M. Fritz, 和 B. Schiele，“一次性时间条件动作预测”，发表于*CVPR*，2019年。'
- en: '[64] A. Furnari and G. M. Farinella, “What would you expect? anticipating egocentric
    actions with rolling-unrolling lstms and modality attention,” in *ICCV*, 2019.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] A. Furnari 和 G. M. Farinella，“你会期待什么？利用滚动-解滚LSTM和模态注意力预测自我中心的动作”，发表于*ICCV*，2019年。'
- en: '[65] F. Sener and A. Yao, “Zero-shot anticipation for instructional activities,”
    in *ICCV*, 2019.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] F. Sener 和 A. Yao，“零样本预测指令活动”，发表于*ICCV*，2019年。'
- en: '[66] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Forecasting future
    action sequences with neural memory networks,” in *BMVC*, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] H. Gammulle, S. Denman, S. Sridharan, 和 C. Fookes，“使用神经记忆网络预测未来的动作序列”，发表于*BMVC*，2019年。'
- en: '[67] E. Alati, L. Mauro, V. Ntouskos, and F. Pirri, “Help by predicting what
    to do,” in *ICIP*, 2019.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] E. Alati, L. Mauro, V. Ntouskos, 和 F. Pirri，“通过预测该做什么来提供帮助”，发表于*ICIP*，2019年。'
- en: '[68] A. Furnari and G. M. Farinella, “Egocentric action anticipation by disentangling
    encoding and inference,” in *ICIP*, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Furnari 和 G. M. Farinella，“通过解开编码和推理预测自我中心的动作”，发表于*ICIP*，2019年。'
- en: '[69] Y. Abu Farha, A. Richard, and J. Gall, “When will you do what? - anticipating
    temporal occurrences of activities,” in *CVPR*, 2018.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Abu Farha, A. Richard, 和 J. Gall，“你什么时候会做什么？——预测活动的时间发生”，发表于*CVPR*，2018年。'
- en: '[70] T. Mahmud, M. Hasan, and A. K. Roy-Chowdhury, “Joint prediction of activity
    labels and starting times in untrimmed videos,” in *ICCV*, 2017.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] T. Mahmud, M. Hasan, 和 A. K. Roy-Chowdhury，“在未剪辑视频中联合预测活动标签和起始时间”，发表于*ICCV*，2017年。'
- en: '[71] A. Rasouli, I. Kotseruba, and J. K. Tsotsos, “Pedestrian action anticipation
    using contextual feature fusion in stacked rnns,” in *BMVC*, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] A. Rasouli, I. Kotseruba, 和 J. K. Tsotsos，“使用上下文特征融合在堆叠RNN中预测行人动作”，发表于*BMVC*，2019年。'
- en: '[72] W. Ding, J. Chen, and S. Shen, “Predicting vehicle behaviors over an extended
    horizon using behavior interaction network,” in *ICRA*, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] W. Ding, J. Chen, 和 S. Shen，“利用行为交互网络预测扩展时间范围内的车辆行为”，发表于*ICRA*，2019年。'
- en: '[73] K. Saleh, M. Hossny, and S. Nahavandi, “Real-time intent prediction of
    pedestrians for autonomous ground vehicles via spatio-temporal densenet,” in *ICRA*,
    2019.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] K. Saleh, M. Hossny, 和 S. Nahavandi，“通过时空密集网络实时预测自动驾驶车辆的行人意图”，发表于*ICRA*，2019年。'
- en: '[74] O. Scheel, N. S. Nagaraja, L. Schwarz, N. Navab, and F. Tombari, “Attention-based
    lane change prediction,” in *ICRA*, 2019.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] O. Scheel, N. S. Nagaraja, L. Schwarz, N. Navab, 和 F. Tombari，“基于注意力的车道变换预测”，发表于*ICRA*，2019年。'
- en: '[75] M. S. Aliakbarian, F. S. Saleh, M. Salzmann, B. Fernando, L. Petersson,
    and L. Andersson, “Viena: A driving anticipation dataset,” in *ACCV*, C. V. Jawahar,
    H. Li, G. Mori, and K. Schindler, Eds., 2019.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. S. Aliakbarian，F. S. Saleh，M. Salzmann，B. Fernando，L. Petersson 和 L. Andersson，“Viena：驾驶预测数据集”，在
    *ACCV*，C. V. Jawahar，H. Li，G. Mori 和 K. Schindler，Eds.，2019。'
- en: '[76] M. Strickland, G. Fainekos, and H. B. Amor, “Deep predictive models for
    collision risk assessment in autonomous driving,” in *ICRA*, 2018.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] M. Strickland，G. Fainekos 和 H. B. Amor，“自动驾驶碰撞风险评估的深度预测模型”，在 *ICRA*，2018。'
- en: '[77] S. Casas, W. Luo, and R. Urtasun, “Intentnet: Learning to predict intention
    from raw sensor data,” in *CORL*, 2018.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S. Casas，W. Luo 和 R. Urtasun，“Intentnet：从原始传感器数据学习预测意图”，在 *CORL*，2018。'
- en: '[78] A. Rasouli, I. Kotseruba, and J. K. Tsotsos, “Are they going to cross?
    a benchmark dataset and baseline for pedestrian crosswalk behavior,” in *ICCVW*,
    2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] A. Rasouli，I. Kotseruba 和 J. K. Tsotsos，“他们会过马路吗？行人过马路行为的基准数据集和基线”，在 *ICCVW*，2017。'
- en: '[79] F.-H. Chan, Y.-T. Chen, Y. Xiang, and M. Sun, “Anticipating accidents
    in dashcam videos,” in *ACCV*, S.-H. Lai, V. Lepetit, K. Nishino, and Y. Sato,
    Eds., 2017.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] F.-H. Chan，Y.-T. Chen，Y. Xiang 和 M. Sun，“在行车记录仪视频中预测事故”，在 *ACCV*，S.-H.
    Lai，V. Lepetit，K. Nishino 和 Y. Sato，Eds.，2017。'
- en: '[80] A. Jain, A. Singh, H. S. Koppula, S. Soh, and A. Saxena, “Recurrent neural
    networks for driver activity anticipation via sensory-fusion architecture,” in
    *ICRA*, 2016.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] A. Jain，A. Singh，H. S. Koppula，S. Soh 和 A. Saxena，“通过感官融合架构对驾驶员活动进行循环神经网络预测”，在
    *ICRA*，2016。'
- en: '[81] A. Manglik, X. Weng, E. Ohn-Bar, and K. M. Kitani, “Forecasting time-to-collision
    from monocular video: Feasibility, dataset, and challenges,” in *IROS*, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] A. Manglik，X. Weng，E. Ohn-Bar 和 K. M. Kitani，“从单眼视频预测碰撞时间的可行性、数据集和挑战”，在
    *IROS*，2019。'
- en: '[82] P. Wang, S. Lien, and M. Lee, “A learning-based prediction model for baby
    accidents,” in *ICIP*, 2019.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] P. Wang，S. Lien 和 M. Lee，“基于学习的婴儿意外预测模型”，在 *ICIP*，2019。'
- en: '[83] T. Suzuki, H. Kataoka, Y. Aoki, and Y. Satoh, “Anticipating traffic accidents
    with adaptive loss and large-scale incident db,” in *The CVPR*, 2018.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] T. Suzuki，H. Kataoka，Y. Aoki 和 Y. Satoh，“使用自适应损失和大规模事件数据库预测交通事故”，在 *CVPR*，2018。'
- en: '[84] K.-H. Zeng, S.-H. Chou, F.-H. Chan, J. Carlos Niebles, and M. Sun, “Agent-centric
    risk assessment: Accident anticipation and risky region localization,” in *CVPR*,
    2017.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] K.-H. Zeng，S.-H. Chou，F.-H. Chan，J. Carlos Niebles 和 M. Sun，“以代理为中心的风险评估：事故预测和危险区域定位”，在
    *CVPR*，2017。'
- en: '[85] S. Su, J. Pyo Hong, J. Shi, and H. Soo Park, “Predicting behaviors of
    basketball players from first person videos,” in *CVPR*, 2017.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Su，J. Pyo Hong，J. Shi 和 H. Soo Park，“从第一人称视频中预测篮球运动员的行为”，在 *CVPR*，2017。'
- en: '[86] P. Felsen, P. Agrawal, and J. Malik, “What will happen next? forecasting
    player moves in sports videos,” in *ICCV*, 2017.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] P. Felsen，P. Agrawal 和 J. Malik，“接下来会发生什么？预测体育视频中球员的动作”，在 *ICCV*，2017。'
- en: '[87] J. Liang, L. Jiang, J. C. Niebles, A. G. Hauptmann, and L. Fei-Fei, “Peeking
    into the future: Predicting future person activities and locations in videos,”
    in *CVPR*, 2019.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] J. Liang，L. Jiang，J. C. Niebles，A. G. Hauptmann 和 L. Fei-Fei，“窥探未来：预测视频中未来的个人活动和位置”，在
    *CVPR*，2019。'
- en: '[88] C. Sun, A. Shrivastava, C. Vondrick, R. Sukthankar, K. Murphy, and C. Schmid,
    “Relational action forecasting,” in *CVPR*, 2019.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C. Sun，A. Shrivastava，C. Vondrick，R. Sukthankar，K. Murphy 和 C. Schmid，“关系动作预测”，在
    *CVPR*，2019。'
- en: '[89] Y. Shen, B. Ni, Z. Li, and N. Zhuang, “Egocentric activity prediction
    via event modulated attention,” in *ECCV*, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. Shen，B. Ni，Z. Li 和 N. Zhuang，“通过事件调制注意预测自我中心活动”，在 *ECCV*，2018。'
- en: '[90] P. Schydlo, M. Rakovic, L. Jamone, and J. Santos-Victor, “Anticipation
    in human-robot cooperation: A recurrent neural network approach for multiple action
    sequences prediction,” in *ICRA*, 2018.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] P. Schydlo，M. Rakovic，L. Jamone 和 J. Santos-Victor，“人机合作中的预测：多动作序列预测的递归神经网络方法”，在
    *ICRA*，2018。'
- en: '[91] Y. Zhong and W. Zheng, “Unsupervised learning for forecasting action representations,”
    in *ICIP*, 2018.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Y. Zhong 和 W. Zheng，“无监督学习预测动作表现”，在 *ICIP*，2018。'
- en: '[92] J. Gao, Z. Yang, and R. Nevatia, “Red: Reinforced encoder-decoder networks
    for action anticipation,” in *BMVC*, 2017.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Gao，Z. Yang 和 R. Nevatia，“RED：强化编码器-解码器网络用于动作预测”，在 *BMVC*，2017。'
- en: '[93] C. Vondrick, H. Pirsiavash, and A. Torralba, “Anticipating visual representations
    from unlabeled video,” in *CVPR*, 2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] C. Vondrick，H. Pirsiavash 和 A. Torralba，“从未标记视频中预测视觉表现”，在 *CVPR*，2016。'
- en: '[94] H. Kataoka, Y. Miyashita, M. Hayashi, K. Iwata, and Y. Satoh, “Recognition
    of transitional action for short-term action prediction using discriminative temporal
    cnn feature,” in *BMVC*, 2016.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] H. Kataoka、Y. Miyashita、M. Hayashi、K. Iwata 和 Y. Satoh，“使用判别时间 CNN 特征进行短期动作预测的过渡动作识别”，发表于
    *BMVC*，2016 年。'
- en: '[95] Y. Zhou and T. L. Berg, “Temporal perception and prediction in ego-centric
    video,” in *ICCV*, 2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Y. Zhou 和 T. L. Berg，“自我中心视频中的时间感知与预测”，发表于 *ICCV*，2015 年。'
- en: '[96] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena, “Structural-rnn: Deep
    learning on spatio-temporal graphs,” in *CVPR*, 2016.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] A. Jain、A. R. Zamir、S. Savarese 和 A. Saxena，“结构化 RNN：在时空图上的深度学习”，发表于 *CVPR*，2016
    年。'
- en: '[97] O. Scheel, L. Schwarz, N. Navab, and F. Tombari, “Situation assessment
    for planning lane changes: Combining recurrent models and prediction,” in *ICRA*,
    2018.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] O. Scheel、L. Schwarz、N. Navab 和 F. Tombari，“车道变更计划的情况评估：结合递归模型与预测”，发表于
    *ICRA*，2018 年。'
- en: '[98] X. Wang, J.-F. Hu, J.-H. Lai, J. Zhang, and W.-S. Zheng, “Progressive
    teacher-student learning for early action prediction,” in *CVPR*, 2019.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] X. Wang、J.-F. Hu、J.-H. Lai、J. Zhang 和 W.-S. Zheng，“用于早期动作预测的渐进式师生学习”，发表于
    *CVPR*，2019 年。'
- en: '[99] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Predicting the future:
    A jointly learnt model for action anticipation,” in *ICCV*, 2019.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] H. Gammulle、S. Denman、S. Sridharan 和 C. Fookes，“预测未来：一种联合学习的动作预期模型”，发表于
    *ICCV*，2019 年。'
- en: '[100] H. Zhao and R. P. Wildes, “Spatiotemporal feature residual propagation
    for action prediction,” in *ICCV*, 2019.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] H. Zhao 和 R. P. Wildes，“用于动作预测的时空特征残差传播”，发表于 *ICCV*，2019 年。'
- en: '[101] T. Yao, M. Wang, B. Ni, H. Wei, and X. Yang, “Multiple granularity group
    interaction prediction,” in *CVPR*, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] T. Yao、M. Wang、B. Ni、H. Wei 和 X. Yang，“多粒度群体互动预测”，发表于 *CVPR*，2018 年。'
- en: '[102] Y. Shi, B. Fernando, and R. Hartley, “Action anticipation with rbf kernelized
    feature mapping rnn,” in *ECCV*, 2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Y. Shi、B. Fernando 和 R. Hartley，“使用 RBF 核映射 RNN 的动作预期”，发表于 *ECCV*，2018
    年。'
- en: '[103] J. Bütepage, H. Kjellström, and D. Kragic, “Anticipating many futures:
    Online human motion prediction and generation for human-robot interaction,” in
    *ICRA*, 2018.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. Bütepage、H. Kjellström 和 D. Kragic，“预见多种未来：用于人机交互的在线人类运动预测与生成”，发表于
    *ICRA*，2018 年。'
- en: '[104] S. Cho and H. Foroosh, “A temporal sequence learning for action recognition
    and prediction,” in *WACV*, 2018.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] S. Cho 和 H. Foroosh，“用于动作识别和预测的时间序列学习”，发表于 *WACV*，2018 年。'
- en: '[105] M. Sadegh Aliakbarian, F. Sadat Saleh, M. Salzmann, B. Fernando, L. Petersson,
    and L. Andersson, “Encouraging lstms to anticipate actions very early,” in *ICCV*,
    2017.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] M. Sadegh Aliakbarian、F. Sadat Saleh、M. Salzmann、B. Fernando、L. Petersson
    和 L. Andersson，“鼓励 LSTMs 早期预见动作”，发表于 *ICCV*，2017 年。'
- en: '[106] W. Li and M. Fritz, “Recognition of ongoing complex activities by sequence
    prediction over a hierarchical label space,” in *WACV*, 2016.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] W. Li 和 M. Fritz，“通过在分层标签空间上的序列预测识别正在进行的复杂活动”，发表于 *WACV*，2016 年。'
- en: '[107] M. Safaei and H. Foroosh, “Still image action recognition by predicting
    spatial-temporal pixel evolution,” in *WACV*, 2019.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] M. Safaei 和 H. Foroosh，“通过预测时空像素演变进行静态图像动作识别”，发表于 *WACV*，2019 年。'
- en: '[108] J. Liu, A. Shahroudy, G. Wang, L.-Y. Duan, and A. C. Kot, “Ssnet: Scale
    selection network for online 3d action prediction,” in *CVPR*, 2018.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J. Liu、A. Shahroudy、G. Wang、L.-Y. Duan 和 A. C. Kot，“SSNet：用于在线 3D 动作预测的尺度选择网络”，发表于
    *CVPR*，2018 年。'
- en: '[109] L. Chen, J. Lu, Z. Song, and J. Zhou, “Part-activated deep reinforcement
    learning for action prediction,” in *ECCV*, 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] L. Chen、J. Lu、Z. Song 和 J. Zhou，“基于部分激活深度强化学习的动作预测”，发表于 *ECCV*，2018 年。'
- en: '[110] J. Butepage, M. J. Black, D. Kragic, and H. Kjellstrom, “Deep representation
    learning for human motion prediction and classification,” in *CVPR*, 2017.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. Butepage、M. J. Black、D. Kragic 和 H. Kjellstrom，“用于人类运动预测与分类的深度表示学习”，发表于
    *CVPR*，2017 年。'
- en: '[111] Y. Kong, Z. Tao, and Y. Fu, “Deep sequential context networks for action
    prediction,” in *CVPR*, 2017.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Y. Kong、Z. Tao 和 Y. Fu，“用于动作预测的深度序列上下文网络”，发表于 *CVPR*，2017 年。'
- en: '[112] G. Singh, S. Saha, M. Sapienza, P. H. S. Torr, and F. Cuzzolin, “Online
    real-time multiple spatiotemporal action localisation and prediction,” in *ICCV*,
    2017.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] G. Singh、S. Saha、M. Sapienza、P. H. S. Torr 和 F. Cuzzolin，“在线实时多重时空动作定位与预测”，发表于
    *ICCV*，2017 年。'
- en: '[113] Dong-Gyu Lee and Seong-Whan Lee, “Human activity prediction based on
    sub-volume relationship descriptor,” in *ICPR*, 2016.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Dong-Gyu Lee 和 Seong-Whan Lee，“基于子体积关系描述符的人类活动预测”，发表于 *ICPR*，2016 年。'
- en: '[114] J. F. Carvalho, M. Vejdemo-Johansson, F. T. Pokorny, and D. Kragic, “Long-term
    prediction of motion trajectories using path homology clusters,” in *IROS*, 2019.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] J. F. Carvalho、M. Vejdemo-Johansson、F. T. Pokorny 和 D. Kragic，“使用路径同源簇进行运动轨迹的长期预测”，发表于
    *IROS*，2019 年。'
- en: '[115] Y. Yoo, K. Yun, S. Yun, J. Hong, H. Jeong, and J. Young Choi, “Visual
    path prediction in complex scenes with crowded moving objects,” in *CVPR*, 2016.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Y. Yoo, K. Yun, S. Yun, J. Hong, H. Jeong 和 J. Young Choi，“复杂场景中拥挤移动物体的视觉路径预测”，发表于
    *CVPR*，2016年。'
- en: '[116] A. Møgelmose, M. M. Trivedi, and T. B. Moeslund, “Trajectory analysis
    and prediction for improved pedestrian safety: Integrated framework and evaluations,”
    in *IV*, 2015.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] A. Møgelmose, M. M. Trivedi 和 T. B. Moeslund，“用于提升行人安全的轨迹分析与预测：集成框架与评估”，发表于
    *IV*，2015年。'
- en: '[117] Chenghui Zhou, B. Balle, and J. Pineau, “Learning time series models
    for pedestrian motion prediction,” in *ICRA*, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Chenghui Zhou, B. Balle 和 J. Pineau，“用于行人运动预测的时间序列模型学习”，发表于 *ICRA*，2016年。'
- en: '[118] A. Rudenko, L. Palmieri, and K. O. Arras, “Joint long-term prediction
    of human motion using a planning-based social force approach,” in *ICRA*, 2018.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] A. Rudenko, L. Palmieri 和 K. O. Arras，“使用基于规划的社会力方法进行人类运动的联合长期预测”，发表于
    *ICRA*，2018年。'
- en: '[119] A. Rudenko, L. Palmieri, A. J. Lilienthal, and K. O. Arras, “Human motion
    prediction under social grouping constraints,” in *IROS*, 2018.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] A. Rudenko, L. Palmieri, A. J. Lilienthal 和 K. O. Arras，“在社会分组约束下的人类运动预测”，发表于
    *IROS*，2018年。'
- en: '[120] J. Schulz, C. Hubmann, J. Löchner, and D. Burschka, “Interaction-aware
    probabilistic behavior prediction in urban environments,” in *IROS*, 2018.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] J. Schulz, C. Hubmann, J. Löchner 和 D. Burschka，“城市环境中的互动感知概率行为预测”，发表于
    *IROS*，2018年。'
- en: '[121] M. Shen, G. Habibi, and J. P. How, “Transferable pedestrian motion prediction
    models at intersections,” in *IROS*, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Shen, G. Habibi 和 J. P. How，“交叉口的可迁移行人运动预测模型”，发表于 *IROS*，2018年。'
- en: '[122] F. Shkurti and G. Dudek, “Topologically distinct trajectory predictions
    for probabilistic pursuit,” in *IROS*, 2017.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] F. Shkurti 和 G. Dudek，“用于概率追踪的拓扑独特轨迹预测”，发表于 *IROS*，2017年。'
- en: '[123] D. Vasquez, “Novel planning-based algorithms for human motion prediction,”
    in *ICRA*, 2016.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] D. Vasquez，“用于人类运动预测的新型基于规划的算法”，发表于 *ICRA*，2016年。'
- en: '[124] V. Karasev, A. Ayvaci, B. Heisele, and S. Soatto, “Intent-aware long-term
    prediction of pedestrian motion,” in *ICRA*, 2016.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] V. Karasev, A. Ayvaci, B. Heisele 和 S. Soatto，“行人运动的意图感知长期预测”，发表于 *ICRA*，2016年。'
- en: '[125] N. Lee and K. M. Kitani, “Predicting wide receiver trajectories in american
    football,” in *WACV*, 2016.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] N. Lee 和 K. M. Kitani，“预测美式足球中的宽接收者轨迹”，发表于 *WACV*，2016年。'
- en: '[126] H. Bai, S. Cai, N. Ye, D. Hsu, and W. S. Lee, “Intention-aware online
    pomdp planning for autonomous driving in a crowd,” in *ICRA*, 2015.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] H. Bai, S. Cai, N. Ye, D. Hsu 和 W. S. Lee，“针对拥挤环境中自主驾驶的意图感知在线POMDP规划”，发表于
    *ICRA*，2015年。'
- en: '[127] S. Solaimanpour and P. Doshi, “A layered hmm for predicting motion of
    a leader in multi-robot settings,” in *ICRA*, 2017.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] S. Solaimanpour 和 P. Doshi，“在多机器人环境中预测领导者运动的分层HMM”，发表于 *ICRA*，2017年。'
- en: '[128] Y. F. Chen, M. Liu, and J. P. How, “Augmented dictionary learning for
    motion prediction,” in *ICRA*, 2016.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Y. F. Chen, M. Liu 和 J. P. How，“用于运动预测的增强字典学习”，发表于 *ICRA*，2016年。'
- en: '[129] R. Sanchez-Matilla and A. Cavallaro, “A predictor of moving objects for
    first-person vision,” in *ICIP*, 2019.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] R. Sanchez-Matilla 和 A. Cavallaro，“用于第一人称视觉的移动物体预测器”，发表于 *ICIP*，2019年。'
- en: '[130] B. Lee, J. Choi, C. Baek, and B. Zhang, “Robust human following by deep
    bayesian trajectory prediction for home service robots,” in *ICRA*, 2018.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] B. Lee, J. Choi, C. Baek 和 B. Zhang，“通过深度贝叶斯轨迹预测实现鲁棒的人体跟随，适用于家用服务机器人”，发表于
    *ICRA*，2018年。'
- en: '[131] I. Hasan, F. Setti, T. Tsesmelis, A. Del Bue, M. Cristani, and F. Galasso,
    ““seeing is believing”: Pedestrian trajectory forecasting using visual frustum
    of attention,” in *WACV*, 2018.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] I. Hasan, F. Setti, T. Tsesmelis, A. Del Bue, M. Cristani 和 F. Galasso，“‘眼见为实’：使用视觉注意力体进行行人轨迹预测”，发表于
    *WACV*，2018年。'
- en: '[132] L. Ballan, F. Castaldo, A. Alahi, F. Palmieri, and S. Savarese, “Knowledge
    transfer for scene-specific motion prediction,” in *ECCV*, B. Leibe, J. Matas,
    N. Sebe, and M. Welling, Eds., 2016.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] L. Ballan, F. Castaldo, A. Alahi, F. Palmieri 和 S. Savarese，“场景特定运动预测的知识转移”，发表于
    *ECCV*，B. Leibe, J. Matas, N. Sebe 和 M. Welling 编，2016年。'
- en: '[133] M. Pfeiffer, U. Schwesinger, H. Sommer, E. Galceran, and R. Siegwart,
    “Predicting actions to act predictably: Cooperative partial motion planning with
    maximum entropy models,” in *IROS*, 2016.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] M. Pfeiffer, U. Schwesinger, H. Sommer, E. Galceran 和 R. Siegwart，“预测行动以实现可预测行为：使用最大熵模型进行合作性部分运动规划”，发表于
    *IROS*，2016年。'
- en: '[134] N. N. Vo and A. F. Bobick, “Augmenting physical state prediction through
    structured activity inference,” in *ICRA*, 2015.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] N. N. Vo 和 A. F. Bobick，“通过结构化活动推断增强物理状态预测”，发表于 *ICRA*，2015年。'
- en: '[135] V. Akbarzadeh, C. Gagné, and M. Parizeau, “Kernel density estimation
    for target trajectory prediction,” in *IROS*, 2015.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] V. Akbarzadeh, C. Gagné 和 M. Parizeau，“目标轨迹预测的核密度估计”，发表于 *IROS*，2015年。'
- en: '[136] A. Schulz and R. Stiefelhagen, “A controlled interactive multiple model
    filter for combined pedestrian intention recognition and path prediction,” in
    *ITSC*, 2015.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] A. Schulz 和 R. Stiefelhagen，“一种控制的交互式多模型滤波器用于行人意图识别和路径预测”，发表于*ITSC*，2015年。'
- en: '[137] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,
    D. Wang, P. Carr, S. Lucey, D. Ramanan, and J. Hays, “Argoverse: 3d tracking and
    forecasting with rich maps,” in *CVPR*, 2019.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,
    D. Wang, P. Carr, S. Lucey, D. Ramanan 和 J. Hays，“Argoverse：具有丰富地图的3D跟踪和预测”，发表于*CVPR*，2019年。'
- en: '[138] A. Sadeghian, V. Kosaraju, A. Sadeghian, N. Hirose, H. Rezatofighi, and
    S. Savarese, “Sophie: An attentive gan for predicting paths compliant to social
    and physical constraints,” in *CVPR*, 2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] A. Sadeghian, V. Kosaraju, A. Sadeghian, N. Hirose, H. Rezatofighi 和
    S. Savarese，“Sophie：一个用于预测符合社会和物理约束路径的注意力型GAN”，发表于*CVPR*，2019年。'
- en: '[139] P. Zhang, W. Ouyang, P. Zhang, J. Xue, and N. Zheng, “Sr-lstm: State
    refinement for lstm towards pedestrian trajectory prediction,” in *CVPR*, 2019.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] P. Zhang, W. Ouyang, P. Zhang, J. Xue 和 N. Zheng，“Sr-lstm：用于行人轨迹预测的LSTM状态优化”，发表于*CVPR*，2019年。'
- en: '[140] T. Zhao, Y. Xu, M. Monfort, W. Choi, C. Baker, Y. Zhao, Y. Wang, and
    Y. N. Wu, “Multi-agent tensor fusion for contextual trajectory prediction,” in
    *CVPR*, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] T. Zhao, Y. Xu, M. Monfort, W. Choi, C. Baker, Y. Zhao, Y. Wang 和 Y.
    N. Wu，“用于上下文轨迹预测的多智能体张量融合”，发表于*CVPR*，2019年。'
- en: '[141] H. Bi, Z. Fang, T. Mao, Z. Wang, and Z. Deng, “Joint prediction for kinematic
    trajectories in vehicle-pedestrian-mixed scenes,” in *ICCV*, 2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] H. Bi, Z. Fang, T. Mao, Z. Wang 和 Z. Deng，“在车辆-行人混合场景中的运动轨迹联合预测”，发表于*ICCV*，2019年。'
- en: '[142] C. Choi and B. Dariush, “Looking to relations for future trajectory forecast,”
    in *ICCV*, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] C. Choi 和 B. Dariush，“关注关系以预测未来轨迹”，发表于*ICCV*，2019年。'
- en: '[143] Y. Huang, H. Bi, Z. Li, T. Mao, and Z. Wang, “Stgat: Modeling spatial-temporal
    interactions for human trajectory prediction,” in *ICCV*, 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Y. Huang, H. Bi, Z. Li, T. Mao 和 Z. Wang，“Stgat：为人类轨迹预测建模时空交互”，发表于*ICCV*，2019年。'
- en: '[144] A. Rasouli, I. Kotseruba, T. Kunic, and J. K. Tsotsos, “Pie: A large-scale
    dataset and models for pedestrian intention estimation and trajectory prediction,”
    in *ICCV*, 2019.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] A. Rasouli, I. Kotseruba, T. Kunic 和 J. K. Tsotsos，“Pie：一个用于行人意图估计和轨迹预测的大规模数据集和模型”，发表于*ICCV*，2019年。'
- en: '[145] L. A. Thiede and P. P. Brahma, “Analyzing the variety loss in the context
    of probabilistic trajectory prediction,” in *ICCV*, 2019.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] L. A. Thiede 和 P. P. Brahma，“在概率轨迹预测背景下分析多样性损失”，发表于*ICCV*，2019年。'
- en: '[146] V. Kosaraju, A. Sadeghian, R. Martín-Martín, I. Reid, H. Rezatofighi,
    and S. Savarese, “Social-bigat: Multimodal trajectory forecasting using bicycle-gan
    and graph attention networks,” in *NeurIPS*, 2019.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] V. Kosaraju, A. Sadeghian, R. Martín-Martín, I. Reid, H. Rezatofighi
    和 S. Savarese，“Social-bigat：使用自行车-gan和图注意力网络的多模态轨迹预测”，发表于*NeurIPS*，2019年。'
- en: '[147] W. Ding and S. Shen, “Online vehicle trajectory prediction using policy
    anticipation network and optimization-based context reasoning,” in *ICRA*, 2019.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] W. Ding 和 S. Shen，“使用策略预测网络和基于优化的上下文推理进行在线车辆轨迹预测”，发表于*ICRA*，2019年。'
- en: '[148] J. Li, H. Ma, and M. Tomizuka, “Interaction-aware multi-agent tracking
    and probabilistic behavior prediction via adversarial learning,” in *ICRA*, 2019.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] J. Li, H. Ma 和 M. Tomizuka，“通过对抗学习的交互感知多智能体跟踪和概率行为预测”，发表于*ICRA*，2019年。'
- en: '[149] C. Anderson, X. Du, R. Vasudevan, and M. Johnson-Roberson, “Stochastic
    sampling simulation for pedestrian trajectory prediction,” in *IROS*, 2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] C. Anderson, X. Du, R. Vasudevan 和 M. Johnson-Roberson，“用于行人轨迹预测的随机采样模拟”，发表于*IROS*，2019年。'
- en: '[150] S. Srikanth, J. A. Ansari, S. Sharma *et al.*, “Infer: Intermediate representations
    for future prediction,” in *IROS*, 2019.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. Srikanth, J. A. Ansari, S. Sharma *等*，“Infer：用于未来预测的中间表示”，发表于*IROS*，2019年。'
- en: '[151] Y. Zhu, D. Qian, D. Ren, and H. Xia, “Starnet: Pedestrian trajectory
    prediction using deep neural network in star topology,” in *IROS*, 2019.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Y. Zhu, D. Qian, D. Ren 和 H. Xia，“Starnet：使用星形拓扑的深度神经网络进行行人轨迹预测”，发表于*IROS*，2019年。'
- en: '[152] H. Xue, D. Huynh, and M. Reynolds, “Location-velocity attention for pedestrian
    trajectory prediction,” in *WACV*, 2019.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] H. Xue, D. Huynh 和 M. Reynolds，“用于行人轨迹预测的位置-速度注意力”，发表于*WACV*，2019年。'
- en: '[153] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, “Social
    gan: Socially acceptable trajectories with generative adversarial networks,” in
    *CVPR*, 2018.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese 和 A. Alahi，“Social gan：使用生成对抗网络生成社会可接受的轨迹”，发表于*CVPR*，2018年。'
- en: '[154] I. Hasan, F. Setti, T. Tsesmelis, A. Del Bue, F. Galasso, and M. Cristani,
    “Mx-lstm: Mixing tracklets and vislets to jointly forecast trajectories and head
    poses,” in *CVPR*, 2018.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] I. Hasan, F. Setti, T. Tsesmelis, A. Del Bue, F. Galasso, 和 M. Cristani，“Mx-lstm:
    混合轨迹片段和视觉片段以联合预测轨迹和头部姿态”，发表于*CVPR*，2018年。'
- en: '[155] Y. Xu, Z. Piao, and S. Gao, “Encoding crowd interaction with deep neural
    network for pedestrian trajectory prediction,” in *CVPR*, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Y. Xu, Z. Piao, 和 S. Gao，“利用深度神经网络编码人群互动以进行行人轨迹预测”，发表于*CVPR*，2018年。'
- en: '[156] M. Pfeiffer, G. Paolo, H. Sommer, J. Nieto, R. Siegwart, and C. Cadena,
    “A data-driven model for interaction-aware pedestrian motion prediction in object
    cluttered environments,” in *ICRA*, 2018.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] M. Pfeiffer, G. Paolo, H. Sommer, J. Nieto, R. Siegwart, 和 C. Cadena，“一个数据驱动的模型用于在物体杂乱环境中进行交互感知的行人运动预测”，发表于*ICRA*，2018年。'
- en: '[157] E. Rehder, F. Wirth, M. Lauer, and C. Stiller, “Pedestrian prediction
    by planning using deep neural networks,” in *ICRA*, 2018.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] E. Rehder, F. Wirth, M. Lauer, 和 C. Stiller，“通过规划使用深度神经网络进行行人预测”，发表于*ICRA*，2018年。'
- en: '[158] H. Xue, D. Q. Huynh, and M. Reynolds, “Ss-lstm: A hierarchical lstm model
    for pedestrian trajectory prediction,” in *WACV*, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] H. Xue, D. Q. Huynh, 和 M. Reynolds，“Ss-lstm: 一种用于行人轨迹预测的分层lstm模型”，发表于*WACV*，2018年。'
- en: '[159] F. Bartoli, G. Lisanti, L. Ballan, and A. Del Bimbo, “Context-aware trajectory
    prediction,” in *ICPR*, 2018.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] F. Bartoli, G. Lisanti, L. Ballan, 和 A. Del Bimbo，“基于上下文的轨迹预测”，发表于*ICPR*，2018年。'
- en: '[160] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese,
    “Social lstm: Human trajectory prediction in crowded spaces,” in *CVPR*, 2016.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, 和 S. Savarese，“Social
    lstm: 在拥挤空间中进行人类轨迹预测”，发表于*CVPR*，2016年。'
- en: '[161] J. Hong, B. Sapp, and J. Philbin, “Rules of the road: Predicting driving
    behavior with a convolutional model of semantic interactions,” in *CVPR*, 2019.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] J. Hong, B. Sapp, 和 J. Philbin，“道路规则: 使用卷积模型预测驾驶行为中的语义互动”，发表于*CVPR*，2019年。'
- en: '[162] N. Rhinehart, R. McAllister, K. Kitani, and S. Levine, “Precog: Prediction
    conditioned on goals in visual multi-agent settings,” in *ICCV*, 2019.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] N. Rhinehart, R. McAllister, K. Kitani, 和 S. Levine，“Precog: 在视觉多智能体环境中基于目标的预测”，发表于*ICCV*，2019年。'
- en: '[163] J. Li, H. Ma, and M. Tomizuka, “Conditional generative neural system
    for probabilistic trajectory prediction,” in *IROS*, 2019.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] J. Li, H. Ma, 和 M. Tomizuka，“用于概率轨迹预测的条件生成神经系统”，发表于*IROS*，2019年。'
- en: '[164] T. Fernando, S. Denman, S. Sridharan, and C. Fookes, “Gd-gan: Generative
    adversarial networks for trajectory prediction and group detection in crowds,”
    in *ACCV*, C. V. Jawahar, H. Li, G. Mori, and K. Schindler, Eds., 2019.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] T. Fernando, S. Denman, S. Sridharan, 和 C. Fookes，“Gd-gan: 用于轨迹预测和人群组检测的生成对抗网络”，发表于*ACCV*，C.
    V. Jawahar, H. Li, G. Mori, 和 K. Schindler 编辑，2019年。'
- en: '[165] N. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. S. Torr, and M. Chandraker,
    “Desire: Distant future prediction in dynamic scenes with interacting agents,”
    in *CVPR*, 2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] N. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. S. Torr, 和 M. Chandraker，“Desire:
    在具有交互代理的动态场景中进行远期预测”，发表于*CVPR*，2017年。'
- en: '[166] N. Rhinehart, K. M. Kitani, and P. Vernaza, “R2p2: A reparameterized
    pushforward policy for diverse, precise generative path forecasting,” in *ECCV*,
    2018.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] N. Rhinehart, K. M. Kitani, 和 P. Vernaza，“R2p2: 一种重新参数化的推前策略用于多样、精确的生成路径预测”，发表于*ECCV*，2018年。'
- en: '[167] K.-R. Kim, W. Choi, Y. J. Koh, S.-G. Jeong, and C.-S. Kim, “Instance-level
    future motion estimation in a single image based on ordinal regression,” in *ICCV*,
    2019.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] K.-R. Kim, W. Choi, Y. J. Koh, S.-G. Jeong, 和 C.-S. Kim，“基于序数回归的单图像实例级未来运动估计”，发表于*ICCV*，2019年。'
- en: '[168] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov, “Multipath: Multiple probabilistic
    anchor trajectory hypotheses for behavior prediction,” in *CoRL*, 2019.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Chai, B. Sapp, M. Bansal, 和 D. Anguelov，“Multipath: 多种概率锚点轨迹假设用于行为预测”，发表于*CoRL*，2019年。'
- en: '[169] H. Cui, V. Radosavljevic, F. Chou, T. Lin, T. Nguyen, T. Huang, J. Schneider,
    and N. Djuric, “Multimodal trajectory predictions for autonomous driving using
    deep convolutional networks,” in *ICRA*, 2019.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] H. Cui, V. Radosavljevic, F. Chou, T. Lin, T. Nguyen, T. Huang, J. Schneider,
    和 N. Djuric，“利用深度卷积网络进行自动驾驶的多模态轨迹预测”，发表于*ICRA*，2019年。'
- en: '[170] X. Huang, S. G. McGill, B. C. Williams, L. Fletcher, and G. Rosman, “Uncertainty-aware
    driver trajectory prediction at urban intersections,” in *ICRA*, 2019.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] X. Huang, S. G. McGill, B. C. Williams, L. Fletcher, 和 G. Rosman，“在城市交叉口进行不确定性感知的驾驶轨迹预测”，发表于*ICRA*，2019年。'
- en: '[171] S. Zhou, M. J. Phielipp, J. A. Sefair, S. I. Walker, and H. B. Amor,
    “Clone swarms: Learning to predict and control multi-robot systems by imitation,”
    in *IROS*, 2019.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] S. Zhou, M. J. Phielipp, J. A. Sefair, S. I. Walker, 和 H. B. Amor，“Clone
    swarms: 通过模仿学习预测和控制多机器人系统”，发表于*IROS*，2019年。'
- en: '[172] A. Jain, S. Casas, R. Liao, Y. Xiong, S. Feng, S. Segal, and R. Urtasun,
    “Discrete residual flow for probabilistic pedestrian behavior prediction,” in
    *CoRL*, 2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] A. Jain, S. Casas, R. Liao, Y. Xiong, S. Feng, S. Segal, 和 R. Urtasun，“用于概率行人行为预测的离散残差流”，发表于
    *CoRL*，2019年。'
- en: '[173] U. Baumann, C. Guiser, M. Herman, and J. M. Zollner, “Predicting ego-vehicle
    paths from environmental observations with a deep neural network,” in *ICRA*,
    2018.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] U. Baumann, C. Guiser, M. Herman, 和 J. M. Zollner，“从环境观察中预测自车路径，使用深度神经网络”，发表于
    *ICRA*，2018年。'
- en: '[174] Y. Zhang, W. Wang, R. Bonatti, D. Maturana, and S. Scherer, “Integrating
    kinematics and environment context into deep inverse reinforcement learning for
    predicting off-road vehicle trajectories,” in *CoRL*, 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Y. Zhang, W. Wang, R. Bonatti, D. Maturana, 和 S. Scherer，“将运动学和环境背景整合到深度逆强化学习中，以预测越野车轨迹”，发表于
    *CoRL*，2018年。'
- en: '[175] W.-C. Ma, D.-A. Huang, N. Lee, and K. M. Kitani, “Forecasting interactive
    dynamics of pedestrians with fictitious play,” in *CVPR*, 2017.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] W.-C. Ma, D.-A. Huang, N. Lee, 和 K. M. Kitani，“通过虚拟博弈预测行人的互动动态”，发表于 *CVPR*，2017年。'
- en: '[176] S. Yi, H. Li, and X. Wang, “Pedestrian behavior understanding and prediction
    with deep neural networks,” in *ECCV*, B. Leibe, J. Matas, N. Sebe, and M. Welling,
    Eds., 2016.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] S. Yi, H. Li, 和 X. Wang，“使用深度神经网络理解和预测行人行为”，发表于 *ECCV*，B. Leibe, J. Matas,
    N. Sebe, 和 M. Welling 编，2016年。'
- en: '[177] R. Chandra, U. Bhattacharya, A. Bera, and D. Manocha, “Traphic: Trajectory
    prediction in dense and heterogeneous traffic using weighted interactions,” in
    *CVPR*, 2019.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] R. Chandra, U. Bhattacharya, A. Bera, 和 D. Manocha，“Traphic：使用加权交互在密集和异质交通中进行轨迹预测”，发表于
    *CVPR*，2019年。'
- en: '[178] Y. Li, “Which way are you going? imitative decision learning for path
    forecasting in dynamic scenes,” in *CVPR*, 2019.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Y. Li，“你要去哪儿？用于动态场景路径预测的模仿决策学习”，发表于 *CVPR*，2019年。'
- en: '[179] W. Zhi, L. Ott, and F. Ramos, “Kernel trajectory maps for multi-modal
    probabilistic motion prediction,” in *CoRL*, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] W. Zhi, L. Ott, 和 F. Ramos，“用于多模态概率运动预测的核轨迹图”，发表于 *CoRL*，2019年。'
- en: '[180] A. Vemula, K. Muelling, and J. Oh, “Social attention: Modeling attention
    in human crowds,” in *ICRA*, 2018.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] A. Vemula, K. Muelling, 和 J. Oh，“社会注意力：在人群中建模注意力”，发表于 *ICRA*，2018年。'
- en: '[181] C. Tang, J. Chen, and M. Tomizuka, “Adaptive probabilistic vehicle trajectory
    prediction through physically feasible bayesian recurrent neural network,” in
    *ICRA*, 2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] C. Tang, J. Chen, 和 M. Tomizuka，“通过物理可行的贝叶斯递归神经网络进行自适应概率车辆轨迹预测”，发表于 *ICRA*，2019年。'
- en: '[182] K. Cho, T. Ha, G. Lee, and S. Oh, “Deep predictive autonomous driving
    using multi-agent joint trajectory prediction and traffic rules,” in *IROS*, 2019.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] K. Cho, T. Ha, G. Lee, 和 S. Oh，“使用多智能体联合轨迹预测和交通规则的深度预测自主驾驶”，发表于 *IROS*，2019年。'
- en: '[183] A. Bhattacharyya, M. Fritz, and B. Schiele, “Long-term on-board prediction
    of people in traffic scenes under uncertainty,” in *CVPR*, 2018.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] A. Bhattacharyya, M. Fritz, 和 B. Schiele，“在交通场景中长期板载预测人群的不确定性”，发表于 *CVPR*，2018年。'
- en: '[184] P. Felsen, P. Lucey, and S. Ganguly, “Where will they go? predicting
    fine-grained adversarial multi-agent motion using conditional variational autoencoders,”
    in *ECCV*, 2018.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] P. Felsen, P. Lucey, 和 S. Ganguly，“他们会去哪儿？使用条件变分自编码器预测细粒度对抗性多智能体运动”，发表于
    *ECCV*，2018年。'
- en: '[185] S. Cao and R. Nevatia, “Forecasting human pose and motion with multibody
    dynamic model,” in *WACV*, 2015.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] S. Cao 和 R. Nevatia，“使用多体动态模型预测人类姿态和运动”，发表于 *WACV*，2015年。'
- en: '[186] Y. Yuan and K. Kitani, “Ego-pose estimation and forecasting as real-time
    pd control,” in *ICCV*, 2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Y. Yuan 和 K. Kitani，“自我姿态估计与预测作为实时 PD 控制”，发表于 *ICCV*，2019年。'
- en: '[187] H. Chiu, E. Adeli, B. Wang, D. Huang, and J. C. Niebles, “Action-agnostic
    human pose forecasting,” in *WACV*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] H. Chiu, E. Adeli, B. Wang, D. Huang, 和 J. C. Niebles，“与动作无关的人体姿态预测”，发表于
    *WACV*，2019年。'
- en: '[188] E. Wu and H. Koike, “Futurepose - mixed reality martial arts training
    using real-time 3d human pose forecasting with a rgb camera,” in *WACV*, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] E. Wu 和 H. Koike，“Futurepose - 使用 RGB 摄像头进行实时 3D 人体姿态预测的混合现实武术训练”，发表于
    *WACV*，2019年。'
- en: '[189] Y.-W. Chao, J. Yang, B. Price, S. Cohen, and J. Deng, “Forecasting human
    dynamics from static images,” in *CVPR*, 2017.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Y.-W. Chao, J. Yang, B. Price, S. Cohen, 和 J. Deng，“从静态图像中预测人类动态”，发表于
    *CVPR*，2017年。'
- en: '[190] A. Gopalakrishnan, A. Mali, D. Kifer, L. Giles, and A. G. Ororbia, “A
    neural temporal model for human motion prediction,” in *CVPR*, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] A. Gopalakrishnan, A. Mali, D. Kifer, L. Giles, 和 A. G. Ororbia，“用于人类运动预测的神经时间模型”，发表于
    *CVPR*，2019年。'
- en: '[191] L.-Y. Gui, Y.-X. Wang, D. Ramanan, and J. M. F. Moura, “Few-shot human
    motion prediction via meta-learning,” in *ECCV*, 2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] L.-Y. Gui, Y.-X. Wang, D. Ramanan, 和 J. M. F. Moura，“通过元学习进行少样本人类动作预测”，发表于
    *ECCV*，2018年。'
- en: '[192] L.-Y. Gui, Y.-X. Wang, X. Liang, and J. M. F. Moura, “Adversarial geometry-aware
    human motion prediction,” in *ECCV*, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] L.-Y. Gui, Y.-X. Wang, X. Liang, 和 J. M. F. Moura，“对抗几何感知的人类运动预测，”发表于
    *ECCV*，2018年。'
- en: '[193] L. Gui, K. Zhang, Y. Wang, X. Liang, J. M. F. Moura, and M. Veloso, “Teaching
    robots to predict human motion,” in *IROS*, 2018.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] L. Gui, K. Zhang, Y. Wang, X. Liang, J. M. F. Moura, 和 M. Veloso，“教机器人预测人类运动，”发表于
    *IROS*，2018年。'
- en: '[194] J. Martinez, M. J. Black, and J. Romero, “On human motion prediction
    using recurrent neural networks,” in *CVPR*, 2017.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] J. Martinez, M. J. Black, 和 J. Romero，“使用递归神经网络进行人体运动预测，”发表于 *CVPR*，2017年。'
- en: '[195] K. Fragkiadaki, S. Levine, P. Felsen, and J. Malik, “Recurrent network
    models for human dynamics,” in *ICCV*, 2015.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] K. Fragkiadaki, S. Levine, P. Felsen, 和 J. Malik，“用于人类动态的递归网络模型，”发表于
    *ICCV*，2015年。'
- en: '[196] B. Wang, E. Adeli, H.-k. Chiu, D.-A. Huang, and J. C. Niebles, “Imitation
    learning for human pose prediction,” in *ICCV*, 2019.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] B. Wang, E. Adeli, H.-k. Chiu, D.-A. Huang, 和 J. C. Niebles，“用于人体姿态预测的模仿学习，”发表于
    *ICCV*，2019年。'
- en: '[197] W. Mao, M. Liu, M. Salzmann, and H. Li, “Learning trajectory dependencies
    for human motion prediction,” in *ICCV*, 2019.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] W. Mao, M. Liu, M. Salzmann, 和 H. Li，“学习轨迹依赖性以进行人类运动预测，”发表于 *ICCV*，2019年。'
- en: '[198] A. Hernandez, J. Gall, and F. Moreno-Noguer, “Human motion prediction
    via spatio-temporal inpainting,” in *ICCV*, 2019.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] A. Hernandez, J. Gall, 和 F. Moreno-Noguer，“通过时空修补进行人类运动预测，”发表于 *ICCV*，2019年。'
- en: '[199] J. Y. Zhang, P. Felsen, A. Kanazawa, and J. Malik, “Predicting 3d human
    dynamics from video,” in *ICCV*, 2019.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] J. Y. Zhang, P. Felsen, A. Kanazawa, 和 J. Malik，“从视频中预测三维人类动态，”发表于 *ICCV*，2019年。'
- en: '[200] C. Talignani Landi, Y. Cheng, F. Ferraguti, M. Bonfe, C. Secchi, and
    M. Tomizuka, “Prediction of human arm target for robot reaching movements,” in
    *IROS*, 2019.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] C. Talignani Landi, Y. Cheng, F. Ferraguti, M. Bonfe, C. Secchi, 和 M.
    Tomizuka，“机器人抓取动作的人类手臂目标预测，”发表于 *IROS*，2019年。'
- en: '[201] V. Guizilini, R. Senanayake, and F. Ramos, “Dynamic hilbert maps: Real-time
    occupancy predictions in changing environments,” in *ICRA*, 2019.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] V. Guizilini, R. Senanayake, 和 F. Ramos，“动态希尔伯特图：在变化环境中的实时占据预测，”发表于 *ICRA*，2019年。'
- en: '[202] D. Graves, K. Rezaee, and S. Scheideman, “Perception as prediction using
    general value functions in autonomous driving applications,” in *IROS*, 2019.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] D. Graves, K. Rezaee, 和 S. Scheideman，“在自主驾驶应用中使用通用价值函数进行感知作为预测，”发表于
    *IROS*，2019年。'
- en: '[203] O. Afolabi, K. Driggs–Campbell, R. Dong, M. J. Kochenderfer, and S. S.
    Sastry, “People as sensors: Imputing maps from human actions,” in *IROS*, 2018.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] O. Afolabi, K. Driggs–Campbell, R. Dong, M. J. Kochenderfer, 和 S. S.
    Sastry，“人作为传感器：从人类动作中推断地图，”发表于 *IROS*，2018年。'
- en: '[204] G. N. Wilson, A. Ramirez-Serrano, and Q. Sun, “Vehicle state prediction
    for outdoor autonomous high-speed off-road ugvs,” in *ICRA*, 2015.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] G. N. Wilson, A. Ramirez-Serrano, 和 Q. Sun，“户外自主高速越野UGVs的车辆状态预测，”发表于
    *ICRA*，2015年。'
- en: '[205] N. Mohajerin and M. Rohani, “Multi-step prediction of occupancy grid
    maps with recurrent neural networks,” in *CVPR*, 2019.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] N. Mohajerin 和 M. Rohani，“使用递归神经网络进行占据网格地图的多步预测，”发表于 *CVPR*，2019年。'
- en: '[206] K. Katyal, K. Popek, C. Paxton, P. Burlina, and G. D. Hager, “Uncertainty-aware
    occupancy map prediction using generative networks for robot navigation,” in *ICRA*,
    2019.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] K. Katyal, K. Popek, C. Paxton, P. Burlina, 和 G. D. Hager，“使用生成网络进行不确定性感知的占据地图预测，用于机器人导航，”发表于
    *ICRA*，2019年。'
- en: '[207] M. Schreiber, S. Hoermann, and K. Dietmayer, “Long-term occupancy grid
    prediction using recurrent neural networks,” in *ICRA*, 2019.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] M. Schreiber, S. Hoermann, 和 K. Dietmayer，“使用递归神经网络的长期占据网格预测，”发表于 *ICRA*，2019年。'
- en: '[208] S. Hoermann, M. Bach, and K. Dietmayer, “Dynamic occupancy grid prediction
    for urban autonomous driving: A deep learning approach with fully automatic labeling,”
    in *ICRA*, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] S. Hoermann, M. Bach, 和 K. Dietmayer，“城市自主驾驶的动态占据网格预测：一种具有完全自动标记的深度学习方法，”发表于
    *ICRA*，2018年。'
- en: '[209] S. Choi, K. Lee, and S. Oh, “Robust modeling and prediction in dynamic
    environments using recurrent flow networks,” in *IROS*, 2016.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] S. Choi, K. Lee, 和 S. Oh，“在动态环境中使用递归流网络进行鲁棒建模和预测，”发表于 *IROS*，2016年。'
- en: '[210] P. Luc, C. Couprie, Y. LeCun, and J. Verbeek, “Predicting future instance
    segmentation by forecasting convolutional features,” in *ECCV*, 2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] P. Luc, C. Couprie, Y. LeCun, 和 J. Verbeek，“通过预测卷积特征预测未来实例分割，”发表于 *ECCV*，2018年。'
- en: '[211] P. Luc, N. Neverova, C. Couprie, J. Verbeek, and Y. LeCun, “Predicting
    deeper into the future of semantic segmentation,” in *ICCV*, 2017.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] P. Luc, N. Neverova, C. Couprie, J. Verbeek, 和 Y. LeCun，“预测语义分割的更深远未来，”发表于
    *ICCV*，2017年。'
- en: '[212] X. Jin, H. Xiao, X. Shen, J. Yang, Z. Lin, Y. Chen, Z. Jie, J. Feng,
    and S. Yan, “Predicting scene parsing and motion dynamics in the future,” in *NeurIPS*,
    2017.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] X. Jin, H. Xiao, X. Shen, J. Yang, Z. Lin, Y. Chen, Z. Jie, J. Feng,
    和 S. Yan，“预测未来的场景解析和运动动态，”发表于*NeurIPS*，2017年。'
- en: '[213] S. Kim, H. Kim, J. Lee, S. Yoon, S. E. Kahou, K. Kashinath, and M. Prabhat,
    “Deep-hurricane-tracker: Tracking and forecasting extreme climate events,” in
    *WACV*, 2019.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] S. Kim, H. Kim, J. Lee, S. Yoon, S. E. Kahou, K. Kashinath, 和 M. Prabhat，“深度飓风跟踪器：跟踪和预测极端气候事件，”发表于*WACV*，2019年。'
- en: '[214] W. Chu, K. Ho, and A. Borji, “Visual weather temperature prediction,”
    in *WACV*, 2018.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] W. Chu, K. Ho, 和 A. Borji，“视觉天气温度预测，”发表于*WACV*，2018年。'
- en: '[215] T. A. Siddiqui, S. Bharadwaj, and S. Kalyanaraman, “A deep learning approach
    to solar-irradiance forecasting in sky-videos,” in *WACV*, 2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] T. A. Siddiqui, S. Bharadwaj, 和 S. Kalyanaraman，“基于深度学习的天空视频辐射预报方法，”发表于*WACV*，2019年。'
- en: '[216] K. Wang, M. Bansal, and J. Frahm, “Retweet wars: Tweet popularity prediction
    via dynamic multimodal regression,” in *WACV*, 2018.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] K. Wang, M. Bansal, 和 J. Frahm，“转发战争：通过动态多模态回归预测推文的受欢迎程度，”发表于*WACV*，2018年。'
- en: '[217] Z. Al-Halah, R. Stiefelhagen, and K. Grauman, “Fashion forward: Forecasting
    visual style in fashion,” in *ICCV*, 2017.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Z. Al-Halah, R. Stiefelhagen, 和 K. Grauman，“时尚前沿：预测时尚中的视觉风格，”发表于*ICCV*，2017年。'
- en: '[218] I. Sur and H. Ben Amor, “Robots that anticipate pain: Anticipating physical
    perturbations from visual cues through deep predictive models,” in *IROS*, 2017.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] I. Sur 和 H. Ben Amor，“预见疼痛的机器人：通过深度预测模型从视觉线索中预测物理扰动，”发表于*IROS*，2017年。'
- en: '[219] R. Mottaghi, M. Rastegari, A. Gupta, and A. Farhadi, ““what happens if…”
    learning to predict the effect of forces in images,” in *ECCV*, B. Leibe, J. Matas,
    N. Sebe, and M. Welling, Eds., 2016.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] R. Mottaghi, M. Rastegari, A. Gupta, 和 A. Farhadi，““如果发生什么…”学习预测图像中力的效果，”发表于*ECCV*，B.
    Leibe, J. Matas, N. Sebe, 和 M. Welling, 编辑，2016年。'
- en: '[220] J. Carvajal, A. Wiliem, C. Sanderson, and B. Lovell, “Towards miss universe
    automatic prediction: The evening gown competition,” in *ICPR*, 2016.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] J. Carvajal, A. Wiliem, C. Sanderson, 和 B. Lovell，“走向宇宙小姐自动预测：晚礼服比赛，”发表于*ICPR*，2016年。'
- en: '[221] J. Joo, F. F. Steen, and S.-C. Zhu, “Automated facial trait judgment
    and election outcome prediction: Social dimensions of face,” in *ICCV*, 2015.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] J. Joo, F. F. Steen, 和 S.-C. Zhu，“自动面部特征判断与选举结果预测：面部的社会维度，”发表于*ICCV*，2015年。'
- en: '[222] S. Lal, S. Duggal, and I. Sreedevi, “Online video summarization: Predicting
    future to better summarize present,” in *WACV*, 2019.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] S. Lal, S. Duggal, 和 I. Sreedevi，“在线视频摘要：预测未来以更好地总结现在，”发表于*WACV*，2019年。'
- en: '[223] W. Liu, W. Luo, D. Lian, and S. Gao, “Future frame prediction for anomaly
    detection – a new baseline,” in *CVPR*, 2018.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] W. Liu, W. Luo, D. Lian, 和 S. Gao，“异常检测的未来帧预测——一个新的基准，”发表于*CVPR*，2018年。'
- en: '[224] T. Fernando, S. Denman, S. Sridharan, and C. Fookes, “Tracking by prediction:
    A deep generative model for mutli-person localisation and tracking,” in *WACV*,
    2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] T. Fernando, S. Denman, S. Sridharan, 和 C. Fookes，“通过预测进行跟踪：用于多人的位置定位和跟踪的深度生成模型，”发表于*WACV*，2018年。'
- en: '[225] D. Jayaraman and K. Grauman, “Look-ahead before you leap: End-to-end
    active recognition by forecasting the effect of motion,” in *ECCV*, B. Leibe,
    J. Matas, N. Sebe, and M. Welling, Eds., 2016.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] D. Jayaraman 和 K. Grauman，“跃前看：通过预测运动效果进行端到端主动识别，”发表于*ECCV*，B. Leibe,
    J. Matas, N. Sebe, 和 M. Welling, 编辑，2016年。'
- en: '[226] A. Dave, O. Russakovsky, and D. Ramanan, “Predictive-corrective networks
    for action detection,” in *CVPR*, 2017.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] A. Dave, O. Russakovsky, 和 D. Ramanan，“用于动作检测的预测-校正网络，”发表于*CVPR*，2017年。'
- en: '[227] Z. Yang, J. Gao, and R. Nevatia, “Spatio-temporal action detection with
    cascade proposal and location anticipation,” in *BMVC*, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] Z. Yang, J. Gao, 和 R. Nevatia，“级联提案和位置预测的时空动作检测，”发表于*BMVC*，2017年。'
- en: '[228] M. Ziaeefard, R. Bergevin, and L.-P. Morency, “Time-slice prediction
    of dyadic human activities,” in *the British Machine Vision Conference (BMVC)*,
    2015.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] M. Ziaeefard, R. Bergevin, 和 L.-P. Morency，“双人活动的时间切片预测，”发表于*英国机器视觉会议
    (BMVC)*，2015年。'
- en: '[229] L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. Metaxas, “Learning to forecast
    and refine residual motion for image-to-video generation,” in *ECCV*, 2018.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] L. Zhao, X. Peng, Y. Tian, M. Kapadia, 和 D. Metaxas，“学习预测和优化图像到视频生成的残差运动，”发表于*ECCV*，2018年。'
- en: '[230] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli *et al.*, “Image
    quality assessment: from error visibility to structural similarity,” *IEEE transactions
    on image processing*, vol. 13, no. 4, pp. 600–612, 2004.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli *等*，“图像质量评估：从错误可见性到结构相似性，”*IEEE
    图像处理学报*，第13卷，第4期，第600–612页，2004年。'
- en: '[231] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *CVPR*, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, 和 O. Wang，“深度特征作为感知度量的非凡有效性，”
    在 *CVPR*，2018 年。'
- en: '[232] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski,
    and S. Gelly, “Towards accurate generative models of video: A new metric & challenges,”
    *arXiv:1812.01717*, 2018.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski,
    和 S. Gelly，“迈向准确的视频生成模型：一种新度量及挑战，” *arXiv:1812.01717*，2018 年。'
- en: '[233] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen,
    “Improved techniques for training gans,” in *NeurIPS*, 2016.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, 和 X. Chen，“改进的
    GAN 训练技术，” 在 *NeurIPS*，2016 年。'
- en: '[234] L. Sun, Z. Yan, S. M. Mellado, M. Hanheide, and T. Duckett, “3dof pedestrian
    trajectory prediction learned from long-term autonomous mobile robot deployment
    data,” in *ICRA*, 2018.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] L. Sun, Z. Yan, S. M. Mellado, M. Hanheide, 和 T. Duckett，“从长期自主移动机器人部署数据中学习的
    3D 运动预测，” 在 *ICRA*，2018 年。'
- en: '[235] S. Pellegrini, A. Ess, K. Schindler, and L. Van Gool, “You’ll never walk
    alone: Modeling social behavior for multi-target tracking,” in *ICCV*, 2009.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] S. Pellegrini, A. Ess, K. Schindler, 和 L. Van Gool，“你永远不会独行：为多目标跟踪建模社交行为，”
    在 *ICCV*，2009 年。'
- en: '[236] Z. Liu, S. Wu, S. Jin, Q. Liu, S. Lu, R. Zimmermann, and L. Cheng, “Towards
    natural and accurate future motion prediction of humans and animals,” in *CVPR*,
    2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] Z. Liu, S. Wu, S. Jin, Q. Liu, S. Lu, R. Zimmermann, 和 L. Cheng，“迈向自然和准确的人类及动物未来运动预测，”
    在 *CVPR*，2019 年。'
- en: '[237] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.6m: Large
    scale datasets and predictive methods for 3d human sensing in natural environments,”
    *PAMI*, vol. 36, no. 7, pp. 1325–1339, 2014.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] C. Ionescu, D. Papava, V. Olaru, 和 C. Sminchisescu，“Human3.6m：大规模数据集和自然环境中
    3D 人体感知的预测方法，” *PAMI*，第 36 卷，第 7 期，页码 1325–1339，2014 年。'
- en: '[238] M. P. Zapf, M. Kawanabe, and L. Y. Morales Saiki, “Pedestrian density
    prediction for efficient mobile robot exploration,” in *IROS*, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] M. P. Zapf, M. Kawanabe, 和 L. Y. Morales Saiki，“用于高效移动机器人探索的行人密度预测，”
    在 *IROS*，2019 年。'
- en: '[239] S. He, D. Kangin, Y. Mi, and N. Pugeault, “Aggregated sparse attention
    for steering angle prediction,” in *ICPR*, 2018.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] S. He, D. Kangin, Y. Mi, 和 N. Pugeault，“用于转向角预测的聚合稀疏注意力，” 在 *ICPR*，2018
    年。'
- en: '[240] A. Kanazawa, J. Y. Zhang, P. Felsen, and J. Malik, “Learning 3d human
    dynamics from video,” in *CVPR*, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] A. Kanazawa, J. Y. Zhang, P. Felsen, 和 J. Malik，“从视频中学习 3D 人体动态，” 在 *CVPR*，2019
    年。'
- en: '[241] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. Kummerle,
    H. Konigshof, C. Stiller, A. de La Fortelle *et al.*, “Interaction dataset: An
    international, adversarial and cooperative motion dataset in interactive driving
    scenarios with semantic maps,” *arXiv:1910.03088*, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. Kummerle,
    H. Konigshof, C. Stiller, A. de La Fortelle *等*，“交互数据集：一个国际性的、对抗性和合作性的互动驾驶场景运动数据集，附带语义地图，”
    *arXiv:1910.03088*，2019 年。'
- en: '[242] A. Siarohin, S. Lathuilière, S. Tulyakov, E. Ricci, and N. Sebe, “Animating
    arbitrary objects via deep motion transfer,” in *CVPR*, 2019.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] A. Siarohin, S. Lathuilière, S. Tulyakov, E. Ricci, 和 N. Sebe，“通过深度运动迁移动画任意对象，”
    在 *CVPR*，2019 年。'
- en: '[243] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous
    driving,” *arXiv:1903.11027*, 2019.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, 和 O. Beijbom，“nuscenes：一个用于自动驾驶的多模态数据集，” *arXiv:1903.11027*，2019
    年。'
- en: '[244] T. von Marcard, R. Henschel, M. Black, B. Rosenhahn, and G. Pons-Moll,
    “Recovering accurate 3d human pose in the wild using imus and a moving camera,”
    in *ECCV*, 2018.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] T. von Marcard, R. Henschel, M. Black, B. Rosenhahn, 和 G. Pons-Moll，“使用
    IMU 和移动相机在野外恢复准确的 3D 人体姿态，” 在 *ECCV*，2018 年。'
- en: '[245] G. Awad, A. Butt, K. Curtis, Y. Lee, J. Fiscus, A. Godil, D. Joy, A. Delgado,
    A. Smeaton, Y. Graham *et al.*, “Benchmarking video activity detection, video
    captioning and matching, video storytelling linking and video search,” in *TRECVID*,
    2018.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] G. Awad, A. Butt, K. Curtis, Y. Lee, J. Fiscus, A. Godil, D. Joy, A.
    Delgado, A. Smeaton, Y. Graham *等*，“视频活动检测、视频字幕生成和匹配、视频讲故事链接和视频搜索的基准测试，” 在 *TRECVID*，2018
    年。'
- en: '[246] P. Schydlo, M. Rakovic, L. Jamone, and J. Santos-Victor, “Anticipation
    in human-robot cooperation: A recurrent neural network approach for multiple action
    sequences prediction,” in *ICRA*, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] P. Schydlo, M. Rakovic, L. Jamone, 和 J. Santos-Victor，“人机合作中的预期：一种用于多个动作序列预测的递归神经网络方法，”
    在 *ICRA*，2018 年。'
- en: '[247] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco, R. Sukthankar *et al.*, “Ava: A video dataset of spatio-temporally
    localized atomic visual actions,” in *CVPR*, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco, R. Sukthankar *等*，“Ava: 一个时空局部化的原子视觉动作视频数据集”，发表于 *CVPR*，2018
    年。'
- en: '[248] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “Scaling egocentric
    vision: The epic-kitchens dataset,” in *ECCV*, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price 和 M. Wray，“扩展自我中心视觉：Epic-Kitchens
    数据集”，发表于 *ECCV*，2018 年。'
- en: '[249] Y. Li, M. Liu, and J. M. Rehg, “In the eye of beholder: Joint learning
    of gaze and actions in first person video,” in *ECCV*, 2018.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] Y. Li, M. Liu 和 J. M. Rehg，“在观察者眼中：在第一人称视频中联合学习注视和动作”，发表于 *ECCV*，2018
    年。'
- en: '[250] O. Groth, F. B. Fuchs, I. Posner, and A. Vedaldi, “Shapestacks: Learning
    vision-based physical intuition for generalised object stacking,” in *ECCV*, 2018.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] O. Groth, F. B. Fuchs, I. Posner 和 A. Vedaldi，“Shapestacks: 学习基于视觉的物理直觉以进行通用对象堆叠”，发表于
    *ECCV*，2018 年。'
- en: '[251] L. Zhou, C. Xu, and J. J. Corso, “Towards automatic learning of procedures
    from web instructional videos,” in *AI*, 2018.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] L. Zhou, C. Xu 和 J. J. Corso，“自动学习来自网络教学视频的程序”，发表于 *AI*，2018 年。'
- en: '[252] S. Ma, S. A. Bargal, J. Zhang, L. Sigal, and S. Sclaroff, “Do less and
    achieve more: Training cnns for action recognition utilizing action images from
    the web,” *Pattern Recognition*, vol. 68, pp. 334–345, 2017.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] S. Ma, S. A. Bargal, J. Zhang, L. Sigal 和 S. Sclaroff，“少做更多事：利用来自网络的动作图像训练卷积神经网络进行动作识别”，*Pattern
    Recognition*，第 68 卷，第 334–345 页，2017 年。'
- en: '[253] S. Zhang, R. Benenson, and B. Schiele, “Citypersons: A diverse dataset
    for pedestrian detection,” in *CVPR*, 2017.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] S. Zhang, R. Benenson 和 B. Schiele，“Citypersons: 一个多样化的行人检测数据集”，发表于 *CVPR*，2017
    年。'
- en: '[254] Z. Yan, T. Duckett, and N. Bellotto, “Online learning for human classification
    in 3d lidar-based tracking,” in *IROS*, 2017.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] Z. Yan, T. Duckett 和 N. Bellotto，“基于 3D 激光雷达跟踪的人类分类在线学习”，发表于 *IROS*，2017
    年。'
- en: '[255] C. Xu, L. N. Govindarajan, Y. Zhang, and L. Cheng, “Lie-x: Depth image
    based articulated object pose estimation, tracking, and action recognition on
    lie groups,” *IJCV*, vol. 123, no. 3, pp. 454–478, 2017.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] C. Xu, L. N. Govindarajan, Y. Zhang 和 L. Cheng，“Lie-x: 基于深度图像的关节对象姿态估计、跟踪和动作识别”，*IJCV*，第
    123 卷，第 3 期，第 454–478 页，2017 年。'
- en: '[256] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year, 1000km: The
    Oxford RobotCar Dataset,” *IJRR*, vol. 36, no. 1, pp. 3–15, 2017.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] W. Maddern, G. Pascoe, C. Linegar 和 P. Newman，“1 年，1000 公里：牛津 RobotCar
    数据集”，*IJRR*，第 36 卷，第 1 期，第 3–15 页，2017 年。'
- en: '[257] L. Chunhui, H. Yueyu, L. Yanghao, S. Sijie, and L. Jiaying, “Pku-mmd:
    A large scale benchmark for continuous multi-modal human action understanding,”
    *arXiv:1703.07475*, 2017.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] L. Chunhui, H. Yueyu, L. Yanghao, S. Sijie 和 L. Jiaying，“Pku-mmd：一个用于连续多模态人类动作理解的大规模基准”，*arXiv:1703.07475*，2017
    年。'
- en: '[258] A. Salvador, N. Hynes, Y. Aytar, J. Marin, F. Ofli, I. Weber, and A. Torralba,
    “Learning cross-modal embeddings for cooking recipes and food images,” in *CVPR*,
    2017.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] A. Salvador, N. Hynes, Y. Aytar, J. Marin, F. Ofli, I. Weber 和 A. Torralba，“学习用于烹饪食谱和食物图像的跨模态嵌入”，发表于
    *CVPR*，2017 年。'
- en: '[259] N. Hawes, C. Burbridge, F. Jovan, L. Kunze, B. Lacerda, L. Mudrova, J. Young,
    J. Wyatt, D. Hebesberger, T. Kortner *et al.*, “The strands project: Long-term
    autonomy in everyday environments,” *IEEE Robotics & Automation Magazine*, vol. 24,
    no. 3, pp. 146–156, 2017.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] N. Hawes, C. Burbridge, F. Jovan, L. Kunze, B. Lacerda, L. Mudrova, J.
    Young, J. Wyatt, D. Hebesberger, T. Kortner *等*，“The Strands 项目：日常环境中的长期自主性”，*IEEE
    Robotics & Automation Magazine*，第 24 卷，第 3 期，第 146–156 页，2017 年。'
- en: '[260] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum, “A compositional
    object-based approach to learning physical dynamics,” *arXiv:1612.00341*, 2016.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] M. B. Chang, T. Ullman, A. Torralba 和 J. B. Tenenbaum，“一种基于组合对象的物理动态学习方法”，*arXiv:1612.00341*，2016
    年。'
- en: '[261] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *CVPR*, 2016.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth 和 B. Schiele，“用于语义城市场景理解的 cityscapes 数据集”，发表于 *CVPR*，2016 年。'
- en: '[262] CMU, “Cmu graphics lab motion capture database,” http://mocap.cs.cmu.edu/,
    2016.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] CMU，“CMU 图形实验室动作捕捉数据库”，http://mocap.cs.cmu.edu/，2016 年。'
- en: '[263] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, “Ntu rgb+d: A large scale
    dataset for 3d human activity analysis,” in *CVPR*, 2016.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] A. Shahroudy, J. Liu, T.-T. Ng 和 G. Wang，“Ntu rgb+d：一个大规模 3D 人类活动分析数据集”，发表于
    *CVPR*，2016 年。'
- en: '[264] Y. Li, C. Lan, J. Xing, W. Zeng, C. Yuan, and J. Liu, “Online human action
    detection using joint classification-regression recurrent neural networks,” *ECCV*,
    2016.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] Y. Li, C. Lan, J. Xing, W. Zeng, C. Yuan 和 J. Liu，“使用联合分类-回归递归神经网络的在线人类动作检测”，*ECCV*，2016。'
- en: '[265] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learning social
    etiquette: Human trajectory understanding in crowded scenes,” in *ECCV*, 2016.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] A. Robicquet, A. Sadeghian, A. Alahi 和 S. Savarese，“学习社交礼仪：拥挤场景中的人类轨迹理解”，在
    *ECCV*，2016。'
- en: '[266] R. De Geest, E. Gavves, A. Ghodrati, Z. Li, C. Snoek, and T. Tuytelaars,
    “Online action detection,” in *ECCV*, 2016.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] R. De Geest, E. Gavves, A. Ghodrati, Z. Li, C. Snoek 和 T. Tuytelaars，“在线动作检测”，在
    *ECCV*，2016。'
- en: '[267] T.-H. K. Huang, F. Ferraro, N. Mostafazadeh, I. Misra, J. Devlin, A. Agrawal,
    R. Girshick, X. He, P. Kohli, D. Batra *et al.*, “Visual storytelling,” in *NAACL*,
    2016.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] T.-H. K. Huang, F. Ferraro, N. Mostafazadeh, I. Misra, J. Devlin, A.
    Agrawal, R. Girshick, X. He, P. Kohli, D. Batra *等*，“视觉讲故事”，在 *NAACL*，2016。'
- en: '[268] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan,
    and S. Vijayanarasimhan, “Youtube-8m: A large-scale video classification benchmark,”
    *arXiv:1609.08675*, 2016.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan
    和 S. Vijayanarasimhan，“Youtube-8m: 一个大规模视频分类基准”，*arXiv:1609.08675*，2016。'
- en: '[269] J. McAuley, C. Targett, Q. Shi, and A. Van Den Hengel, “Image-based recommendations
    on styles and substitutes,” in *SIGIR*, 2015.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] J. McAuley, C. Targett, Q. Shi 和 A. Van Den Hengel，“基于图像的风格和替代品推荐”，在
    *SIGIR*，2015。'
- en: '[270] H. Joo, H. Liu, L. Tan, L. Gui, B. Nabbe, I. Matthews, T. Kanade, S. Nobuhara,
    and Y. Sheikh, “Panoptic studio: A massively multiview system for social motion
    capture,” in *ICCV*, 2015.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] H. Joo, H. Liu, L. Tan, L. Gui, B. Nabbe, I. Matthews, T. Kanade, S.
    Nobuhara 和 Y. Sheikh，“Panoptic studio: 一个大规模多视角社交动作捕捉系统”，在 *ICCV*，2015。'
- en: '[271] Y. Li, Z. Ye, and J. M. Rehg, “Delving into egocentric actions,” in *CVPR*,
    2015.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] Y. Li, Z. Ye 和 J. M. Rehg，“深入探讨自我中心动作”，在 *CVPR*，2015。'
- en: '[272] S. Cappallo, T. Mensink, and C. G. Snoek, “Latent factors of visual popularity
    prediction,” in *ICMR*, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] S. Cappallo, T. Mensink 和 C. G. Snoek，“视觉流行预测的潜在因素”，在 *ICMR*，2015。'
- en: '[273] L. Leal-Taixé, A. Milan, I. Reid, S. Roth, and K. Schindler, “Motchallenge
    2015: Towards a benchmark for multi-target tracking,” *arXiv:1504.01942*, 2015.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] L. Leal-Taixé, A. Milan, I. Reid, S. Roth 和 K. Schindler，“Motchallenge
    2015: 朝着多目标跟踪基准迈进”，*arXiv:1504.01942*，2015。'
- en: '[274] N. Srivastava, E. Mansimov, and R. Salakhudinov, “Unsupervised learning
    of video representations using lstms,” in *ICML*, 2015.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] N. Srivastava, E. Mansimov 和 R. Salakhudinov，“使用 lstms 的无监督视频表示学习”，在
    *ICML*，2015。'
- en: '[275] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene understanding
    benchmark suite,” in *CVPR*, 2015.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] S. Song, S. P. Lichtenberg 和 J. Xiao，“Sun rgb-d: 一个 rgb-d 场景理解基准套件”，在
    *CVPR*，2015。'
- en: '[276] J.-F. Hu, W.-S. Zheng, J. Lai, and J. Zhang, “Jointly learning heterogeneous
    features for rgb-d activity recognition,” in *CVPR*, 2015.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] J.-F. Hu, W.-S. Zheng, J. Lai 和 J. Zhang，“联合学习异质特征用于 rgb-d 活动识别”，在 *CVPR*，2015。'
- en: '[277] A. Gorban, H. Idrees, Y.-G. Jiang, A. Roshan Zamir, I. Laptev, M. Shah,
    and R. Sukthankar, “THUMOS challenge: Action recognition with a large number of
    classes,” [http://www.thumos.info/](http://www.thumos.info/), 2015.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] A. Gorban, H. Idrees, Y.-G. Jiang, A. Roshan Zamir, I. Laptev, M. Shah
    和 R. Sukthankar，“THUMOS 挑战：具有大量类别的动作识别”，[http://www.thumos.info/](http://www.thumos.info/)，2015。'
- en: '[278] C. Wu, J. Zhang, S. Savarese, and A. Saxena, “Watch-n-patch: Unsupervised
    understanding of actions and relations,” in *CVPR*, 2015.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] C. Wu, J. Zhang, S. Savarese 和 A. Saxena，“Watch-n-patch: 无监督理解动作和关系”，在
    *CVPR*，2015。'
- en: '[279] Y. Xiong, K. Zhu, D. Lin, and X. Tang, “Recognize complex events from
    static images by fusing deep channels,” in *CVPR*, 2015.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] Y. Xiong, K. Zhu, D. Lin 和 X. Tang，“通过融合深度通道从静态图像中识别复杂事件”，在 *CVPR*，2015。'
- en: '[280] H. Kuehne, A. B. Arslan, and T. Serre, “The language of actions: Recovering
    the syntax and semantics of goal-directed human activities,” in *CVPR*, 2014.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] H. Kuehne, A. B. Arslan 和 T. Serre，“行动的语言：恢复目标导向人类活动的语法和语义”，在 *CVPR*，2014。'
- en: '[281] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele, “2d human pose
    estimation: New benchmark and state of the art analysis,” in *CVPR*, 2014.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] M. Andriluka, L. Pishchulin, P. Gehler 和 B. Schiele，“2d 人体姿态估计：新的基准和现状分析”，在
    *CVPR*，2014。'
- en: '[282] G. Yu, Z. Liu, and J. Yuan, “Discriminative orderlet mining for real-time
    recognition of human-object interaction,” in *ACCV*, D. Cremers, I. Reid, H. Saito,
    and M.-H. Yang, Eds., 2015.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] G. Yu, Z. Liu 和 J. Yuan，“用于实时识别人类-物体交互的判别性 orderlet 挖掘”，在 *ACCV*，D. Cremers,
    I. Reid, H. Saito 和 M.-H. Yang 编，2015。'
- en: '[283] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei,
    “Large-scale video classification with convolutional neural networks,” in *CVPR*,
    2014.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, 和 L. Fei-Fei,
    “使用卷积神经网络进行大规模视频分类，” 在 *CVPR*，2014 年。'
- en: '[284] S. Stein and S. J. McKenna, “Combining embedded accelerometers with computer
    vision for recognizing food preparation activities,” in *UbiComp*, 2013.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] S. Stein 和 S. J. McKenna, “将嵌入式加速度计与计算机视觉结合用于识别食品准备活动，” 在 *UbiComp*，2013
    年。'
- en: '[285] D. Brščić, T. Kanda, T. Ikeda, and T. Miyashita, “Person tracking in
    large public spaces using 3-d range sensors,” *Transactions on Human-Machine Systems*,
    vol. 43, no. 6, pp. 522–534, 2013.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] D. Brščić, T. Kanda, T. Ikeda, 和 T. Miyashita, “在大型公共空间中使用三维范围传感器进行人员跟踪，”
    *人机系统交易*，第 43 卷，第 6 期，页码 522–534，2013 年。'
- en: '[286] H. S. Koppula, R. Gupta, and A. Saxena, “Learning human activities and
    object affordances from rgb-d videos,” *IJRR*, vol. 32, no. 8, pp. 951–970, 2013.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] H. S. Koppula, R. Gupta, 和 A. Saxena, “从 RGB-D 视频中学习人类活动和物体可用性，” *IJRR*，第
    32 卷，第 8 期，页码 951–970，2013 年。'
- en: '[287] C. Lu, J. Shi, and J. Jia, “Abnormal event detection at 150 fps in matlab,”
    in *ICCV*, 2013.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] C. Lu, J. Shi, 和 J. Jia, “在 MATLAB 中以 150 fps 检测异常事件，” 在 *ICCV*，2013
    年。'
- en: '[288] N. Schneider and D. M. Gavrila, “Pedestrian path prediction with recursive
    bayesian filters: A comparative study,” in *GCPR*, 2013.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] N. Schneider 和 D. M. Gavrila, “使用递归贝叶斯滤波器进行行人路径预测：一项比较研究，” 在 *GCPR*，2013
    年。'
- en: '[289] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, “Towards understanding
    action recognition,” in *ICCV*, 2013.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, 和 M. J. Black, “迈向理解动作识别，” 在
    *ICCV*，2013 年。'
- en: '[290] W. Zhang, M. Zhu, and K. G. Derpanis, “From actemes to action: A strongly-supervised
    representation for detailed action understanding,” in *ICCV*, 2013.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] W. Zhang, M. Zhu, 和 K. G. Derpanis, “从动作原语到动作：一种用于详细动作理解的强监督表示，” 在 *ICCV*，2013
    年。'
- en: '[291] Y. Kong, Y. Jia, and Y. Fu, “Learning human interaction by interactive
    phrases,” in *ECCV*, 2012.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] Y. Kong, Y. Jia, 和 Y. Fu, “通过互动短语学习人类互动，” 在 *ECCV*，2012 年。'
- en: '[292] A. Fathi, Y. Li, and J. M. Rehg, “Learning to recognize daily actions
    using gaze,” in *ECCV*, 2012.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] A. Fathi, Y. Li, 和 J. M. Rehg, “通过注视学习识别日常动作，” 在 *ECCV*，2012 年。'
- en: '[293] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *CVPR*, 2012.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] A. Geiger, P. Lenz, 和 R. Urtasun, “我们为自动驾驶做好准备了吗？KITTI 视觉基准套件，” 在 *CVPR*，2012
    年。'
- en: '[294] A. Abramov, K. Pauwels, J. Papon, F. Wörgötter, and B. Dellen, “Depth-supported
    real-time video segmentation with the kinect,” in *WACV*, 2012.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] A. Abramov, K. Pauwels, J. Papon, F. Wörgötter, 和 B. Dellen, “利用 Kinect
    进行深度支持的实时视频分割，” 在 *WACV*，2012 年。'
- en: '[295] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele, “A database for fine
    grained activity detection of cooking activities,” in *CVPR*, 2012.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] M. Rohrbach, S. Amin, M. Andriluka, 和 B. Schiele, “一个用于细粒度烹饪活动检测的数据库，”
    在 *CVPR*，2012 年。'
- en: '[296] J. Wang, Z. Liu, Y. Wu, and J. Yuan, “Mining actionlet ensemble for action
    recognition with depth cameras,” in *CVPR*, 2012.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] J. Wang, Z. Liu, Y. Wu, 和 J. Yuan, “利用深度相机挖掘动作集成用于动作识别，” 在 *CVPR*，2012
    年。'
- en: '[297] B. Zhou, X. Wang, and X. Tang, “Understanding collective crowd behaviors:
    Learning a mixture model of dynamic pedestrian-agents,” in *CVPR*, 2012.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] B. Zhou, X. Wang, 和 X. Tang, “理解集体人群行为：学习动态行人代理的混合模型，” 在 *CVPR*，2012
    年。'
- en: '[298] K. Yun, J. Honorio, D. Chattopadhyay, T. L. Berg, and D. Samaras, “Two-person
    interaction detection using body-pose features and multiple instance learning,”
    in *CVPRW*, 2012.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] K. Yun, J. Honorio, D. Chattopadhyay, T. L. Berg, 和 D. Samaras, “使用身体姿态特征和多实例学习进行两人互动检测，”
    在 *CVPRW*，2012 年。'
- en: '[299] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101 human
    actions classes from videos in the wild,” *arXiv:1212.0402*, 2012.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] K. Soomro, A. R. Zamir, 和 M. Shah, “Ucf101：一个包含 101 种人类动作类别的野外视频数据集，”
    *arXiv:1212.0402*，2012 年。'
- en: '[300] L. Xia, C. Chen, and J. Aggarwal, “View invariant human action recognition
    using histograms of 3d joints,” in *CVPRW*, 2012.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] L. Xia, C. Chen, 和 J. Aggarwal, “使用三维关节直方图进行视角不变的人类动作识别，” 在 *CVPRW*，2012
    年。'
- en: '[301] H. Dibeklioğlu, A. A. Salah, and T. Gevers, “Are you really smiling at
    me? spontaneous versus posed enjoyment smiles,” in *ECCV*, A. Fitzgibbon, S. Lazebnik,
    P. Perona, Y. Sato, and C. Schmid, Eds., 2012.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] H. Dibeklioğlu, A. A. Salah, 和 T. Gevers, “你真的在对我笑吗？自发与摆拍的愉悦笑容，” 在 *ECCV*，A. Fitzgibbon,
    S. Lazebnik, P. Perona, Y. Sato, 和 C. Schmid 编，2012 年。'
- en: '[302] G. Pandey, J. R. McBride, and R. M. Eustice, “Ford campus vision and
    lidar data set,” *The International Journal of Robotics Research (IJRR)*, vol. 30,
    no. 13, pp. 1543–1552, 2011.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] G. Pandey, J. R. McBride, 和 R. M. Eustice, “福特校园视觉与激光雷达数据集，” *国际机器人研究期刊
    (IJRR)*，第 30 卷，第 13 期，页码 1543–1552，2011 年。'
- en: '[303] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, “HMDB: a large
    video database for human motion recognition,” in *ICCV*, 2011.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, 和 T. Serre, “HMDB：一个大型视频数据库用于人类运动识别”，发表于
    *ICCV*，2011年。'
- en: '[304] B. Yao, X. Jiang, A. Khosla, A. L. Lin, L. Guibas, and L. Fei-Fei, “Human
    action recognition by learning bases of action attributes and parts,” in *ICCV*,
    2011.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] B. Yao, X. Jiang, A. Khosla, A. L. Lin, L. Guibas, 和 L. Fei-Fei, “通过学习动作属性和部件的基础进行人类动作识别”，发表于
    *ICCV*，2011年。'
- en: '[305] B. Benfold and I. Reid, “Stable multi-target tracking in real-time surveillance
    video,” in *CVPR*, 2011.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] B. Benfold 和 I. Reid, “实时监控视频中的稳定多目标跟踪”，发表于 *CVPR*，2011年。'
- en: '[306] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee, S. Mukherjee,
    J. Aggarwal, H. Lee, L. Davis *et al.*, “A large-scale benchmark dataset for event
    recognition in surveillance video,” in *CVPR*, 2011.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee, S. Mukherjee,
    J. Aggarwal, H. Lee, L. Davis *等*，“用于事件识别的大规模基准数据集”，发表于 *CVPR*，2011年。'
- en: '[307] N. Pugeault and R. Bowden, “Learning pre-attentive driving behaviour
    from holistic visual features,” in *ECCV*, 2010.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] N. Pugeault 和 R. Bowden, “从整体视觉特征中学习前注意驾驶行为”，发表于 *ECCV*，2010年。'
- en: '[308] W. Li, Z. Zhang, and Z. Liu, “Action recognition based on a bag of 3d
    points,” in *CVPRW*, 2010.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] W. Li, Z. Zhang, 和 Z. Liu, “基于 3D 点袋的动作识别”，发表于 *CVPRW*，2010年。'
- en: '[309] N. Aifanti, C. Papachristou, and A. Delopoulos, “The mug facial expression
    database,” *WIAMIS*, 2010.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] N. Aifanti, C. Papachristou, 和 A. Delopoulos, “Mug 面部表情数据库”，*WIAMIS*，2010年。'
- en: '[310] J. Santner, C. Leistner, A. Saffari, T. Pock, and H. Bischof, “Prost:
    Parallel robust online simple tracking,” in *CVPR*, 2010.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] J. Santner, C. Leistner, A. Saffari, T. Pock, 和 H. Bischof, “Prost：并行鲁棒在线简单跟踪”，发表于
    *CVPR*，2010年。'
- en: '[311] A. Patron-Perez, M. Marszalek, A. Zisserman, and I. D. Reid, “High five:
    Recognising human interactions in tv shows.” in *BMVC*, 2010.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] A. Patron-Perez, M. Marszalek, A. Zisserman, 和 I. D. Reid, “High five：识别电视节目中的人类互动”，发表于
    *BMVC*，2010年。'
- en: '[312] M. S. Ryoo and J. K. Aggarwal, “UT-Interaction Dataset, ICPR contest
    on Semantic Description of Human Activities (SDHA),” 2010. [Online]. Available:
    [http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] M. S. Ryoo 和 J. K. Aggarwal, “UT-Interaction 数据集，ICPR 人类活动语义描述（SDHA）竞赛”，2010年。[在线]。可用：
    [http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)'
- en: '[313] R. Vezzani and R. Cucchiara, “Video surveillance online repository (visor):
    an integrated framework,” *Multimedia Tools and Applications*, vol. 50, no. 2,
    pp. 359–380, 2010.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] R. Vezzani 和 R. Cucchiara, “视频监控在线资源库（visor）：一个集成框架”，*Multimedia Tools
    and Applications*，第 50 卷，第 2 期，359–380 页，2010年。'
- en: '[314] V. Delaitre, I. Laptev, and J. Sivic, “Recognizing human actions in still
    images: a study of bag-of-features and part-based representations,” in *BMVC*,
    2010.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] V. Delaitre, I. Laptev, 和 J. Sivic, “在静态图像中识别人体动作：对特征袋和基于部件表示的研究”，发表于
    *BMVC*，2010年。'
- en: '[315] P. Dollár, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection:
    A benchmark,” in *CVPR*, 2009.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] P. Dollár, C. Wojek, B. Schiele, 和 P. Perona, “行人检测：一个基准测试”，发表于 *CVPR*，2009年。'
- en: '[316] W. Choi, K. Shahid, and S. Savarese, “What are they doing? : Collective
    activity classification using spatio-temporal relationship among people,” in *ICCVW*,
    2009.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] W. Choi, K. Shahid, 和 S. Savarese, “他们在做什么？：使用空间时间关系进行集体活动分类”，发表于 *ICCVW*，2009年。'
- en: '[317] B. Majecka, “Statistical models of pedestrian behaviour in the forum,”
    Master’s thesis, School of Informatics, University of Edinburgh, 2009.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] B. Majecka, “论坛中的行人行为统计模型”，硕士论文，爱丁堡大学信息学学院，2009年。'
- en: '[318] R. Hess and A. Fern, “Discriminatively trained particle filters for complex
    multi-object tracking,” in *CVPR*, 2009.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] R. Hess 和 A. Fern, “用于复杂多目标跟踪的判别训练粒子滤波器”，发表于 *CVPR*，2009年。'
- en: '[319] J. Ferryman and A. Shahrokni, “Pets2009: Dataset and challenge,” in *PETS*,
    2009.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] J. Ferryman 和 A. Shahrokni, “Pets2009：数据集和挑战”，发表于 *PETS*，2009年。'
- en: '[320] C. C. Loy, T. Xiang, and S. Gong, “Modelling multi-object activity by
    gaussian processes.” in *BMVC*, 2009.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] C. C. Loy, T. Xiang, 和 S. Gong, “通过高斯过程建模多物体活动。”，发表于 *BMVC*，2009年。'
- en: '[321] M. Tenorth, J. Bandouch, and M. Beetz, “The tum kitchen data set of everyday
    manipulation activities for motion tracking and action recognition,” in *ICCVW*,
    2009.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] M. Tenorth, J. Bandouch, 和 M. Beetz, “TUM 厨房数据集：用于运动跟踪和动作识别的日常操作活动”，发表于
    *ICCVW*，2009年。'
- en: '[322] A. V. T. Library, “YUV video sequences,” http://trace.kom.aau.dk/yuv/index.html,
    2009.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] A. V. T. Library, “YUV 视频序列”，http://trace.kom.aau.dk/yuv/index.html，2009年。'
- en: '[323] M. Enzweiler and D. M. Gavrila, “Monocular pedestrian detection: Survey
    and experiments,” *transactions on pattern analysis and machine intelligence (PAMI)*,
    vol. 31, no. 12, pp. 2179–2195, 2008.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] M. Enzweiler 和 D. M. Gavrila，“单目行人检测：调查与实验”，*模式分析与机器智能（PAMI）*，第31卷，第12期，页码
    2179–2195，2008年。'
- en: '[324] E. Grimson, X. Wang, G.-W. Ng, and K. T. Ma, “Trajectory analysis and
    semantic region modeling using a nonparametric bayesian model,” in *CVPR*, 2008.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] E. Grimson, X. Wang, G.-W. Ng, 和 K. T. Ma，“使用非参数贝叶斯模型的轨迹分析与语义区域建模”，在*CVPR*，2008年。'
- en: '[325] N. Jacobs, N. Roman, and R. Pless, “Consistent temporal variations in
    many outdoor scenes,” in *CVPR*, 2007.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[325] N. Jacobs, N. Roman, 和 R. Pless，“许多户外场景中的一致时间变化”，在*CVPR*，2007年。'
- en: '[326] A. Ess, B. Leibe, and L. Van Gool, “Depth and appearance for mobile scene
    analysis,” in *ICCV*, 2007.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[326] A. Ess, B. Leibe, 和 L. Van Gool，“移动场景分析中的深度与外观”，在*ICCV*，2007年。'
- en: '[327] U. D. of Transportation, “Lankershim boulevard dataset,” 2007\. [Online].
    Available: [https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm](https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm)'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[327] 美国运输部，“Lankershim大道数据集”，2007年。[在线]。可用链接：[https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm](https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm)'
- en: '[328] U. D. of Transporation, “Next generation simulation (ngsim),” Online,
    2007.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[328] 美国运输部，“下一代模拟（ngsim）”，在线，2007年。'
- en: '[329] A. Lerner, Y. Chrysanthou, and D. Lischinski, “Crowds by example,” *Computer
    graphics forum*, vol. 26, no. 3, pp. 655–664, 2007.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[329] A. Lerner, Y. Chrysanthou, 和 D. Lischinski，“通过示例生成的人群”，*计算机图形学论坛*，第26卷，第3期，页码
    655–664，2007年。'
- en: '[330] T. Pickering, “The mmt all-sky camera,” *Ground-based and Airborne Telescopes*,
    vol. 6267, p. 62671A, 2006.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[330] T. Pickering，“MMT全天候相机”，*地面和空中望远镜*，第6267卷，页码 62671A，2006年。'
- en: '[331] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions: a local
    svm approach,” in *ICPR*, vol. 3, 2004.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[331] C. Schuldt, I. Laptev, 和 B. Caputo，“识别人体动作：一种局部SVM方法”，在*ICPR*，第3卷，2004年。'
- en: '[332] T. Stoffel and A. Andreas, “NREL solar radiation research laboratory
    (srrl): Baseline measurement system (bms); golden, colorado (data),” National
    Renewable Energy Lab.(NREL), Tech. Rep., 1981.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[332] T. Stoffel 和 A. Andreas，“NREL太阳辐射研究实验室（srrl）：基线测量系统（bms）；科罗拉多州金色数据”，国家可再生能源实验室（NREL），技术报告，1981年。'
- en: '[333] Yale Song, J. Vallmitjana, A. Stent, and A. Jaimes, “Tvsum: Summarizing
    web videos using titles,” in *CVPR*, 2015.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[333] Yale Song, J. Vallmitjana, A. Stent, 和 A. Jaimes，“Tvsum：使用标题总结网络视频”，在*CVPR*，2015年。'
- en: Appendix A Papers with code
  id: totrans-613
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 带代码的论文
- en: '| App | Paper | Link |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 论文 | 链接 |'
- en: '| Video | [[19](#bib.bib19)] | [https://github.com/andrewjywang/SEENet](https://github.com/andrewjywang/SEENet)
    |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| 视频 | [[19](#bib.bib19)] | [https://github.com/andrewjywang/SEENet](https://github.com/andrewjywang/SEENet)
    |'
- en: '| [[11](#bib.bib11)] | [https://github.com/Yijunmaverick/FlowGrounded-VideoPrediction](https://github.com/Yijunmaverick/FlowGrounded-VideoPrediction)
    |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| [[11](#bib.bib11)] | [https://github.com/Yijunmaverick/FlowGrounded-VideoPrediction](https://github.com/Yijunmaverick/FlowGrounded-VideoPrediction)
    |'
- en: '| [[34](#bib.bib34)] | [https://github.com/liuem607/DYAN](https://github.com/liuem607/DYAN)
    |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| [[34](#bib.bib34)] | [https://github.com/liuem607/DYAN](https://github.com/liuem607/DYAN)
    |'
- en: '| [[229](#bib.bib229)] | [https://github.com/garyzhao/FRGAN](https://github.com/garyzhao/FRGAN)
    |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| [[229](#bib.bib229)] | [https://github.com/garyzhao/FRGAN](https://github.com/garyzhao/FRGAN)
    |'
- en: '| [[23](#bib.bib23)] | [https://github.com/jthsieh/DDPAE-video-prediction](https://github.com/jthsieh/DDPAE-video-prediction)
    |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| [[23](#bib.bib23)] | [https://github.com/jthsieh/DDPAE-video-prediction](https://github.com/jthsieh/DDPAE-video-prediction)
    |'
- en: '| [[24](#bib.bib24)] | [https://github.com/xjwxjw/VPSS](https://github.com/xjwxjw/VPSS)
    |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| [[24](#bib.bib24)] | [https://github.com/xjwxjw/VPSS](https://github.com/xjwxjw/VPSS)
    |'
- en: '| [[35](#bib.bib35)] | [https://github.com/jinbeibei/VarNet](https://github.com/jinbeibei/VarNet)
    |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| [[35](#bib.bib35)] | [https://github.com/jinbeibei/VarNet](https://github.com/jinbeibei/VarNet)
    |'
- en: '| [[25](#bib.bib25)] | [https://bit.ly/2HqiHqx](https://bit.ly/2HqiHqx) |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| [[25](#bib.bib25)] | [https://bit.ly/2HqiHqx](https://bit.ly/2HqiHqx) |'
- en: '| [[27](#bib.bib27)] | [https://github.com/ujjax/pred-rnn](https://github.com/ujjax/pred-rnn)
    |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] | [https://github.com/ujjax/pred-rnn](https://github.com/ujjax/pred-rnn)
    |'
- en: '| [[28](#bib.bib28)] | [https://github.com/rubenvillegas/icml2017hierchvid](https://github.com/rubenvillegas/icml2017hierchvid)
    |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)] | [https://github.com/rubenvillegas/icml2017hierchvid](https://github.com/rubenvillegas/icml2017hierchvid)
    |'
- en: '| [[29](#bib.bib29)] | [https://github.com/tensorflow/models/tree/master/research/video_prediction](https://github.com/tensorflow/models/tree/master/research/video_prediction)
    |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] | [https://github.com/tensorflow/models/tree/master/research/video_prediction](https://github.com/tensorflow/models/tree/master/research/video_prediction)
    |'
- en: '| [[30](#bib.bib30)] | [https://github.com/junhyukoh/nips2015-action-conditional-video-prediction](https://github.com/junhyukoh/nips2015-action-conditional-video-prediction)
    |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| [[30](#bib.bib30)] | [https://github.com/junhyukoh/nips2015-action-conditional-video-prediction](https://github.com/junhyukoh/nips2015-action-conditional-video-prediction)
    |'
- en: '| Action | [[87](#bib.bib87)] | [https://github.com/google/next-prediction](https://github.com/google/next-prediction)
    |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| Action | [[87](#bib.bib87)] | [https://github.com/google/next-prediction](https://github.com/google/next-prediction)
    |'
- en: '| [[64](#bib.bib64)] | [https://github.com/fpv-iplab/rulstm](https://github.com/fpv-iplab/rulstm)
    |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| [[64](#bib.bib64)] | [https://github.com/fpv-iplab/rulstm](https://github.com/fpv-iplab/rulstm)
    |'
- en: '| [[71](#bib.bib71)] | [https://github.com/aras62/SF-GRU](https://github.com/aras62/SF-GRU)
    |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| [[71](#bib.bib71)] | [https://github.com/aras62/SF-GRU](https://github.com/aras62/SF-GRU)
    |'
- en: '| [[81](#bib.bib81)] | [https://github.com/aashi7/NearCollision](https://github.com/aashi7/NearCollision)
    |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| [[81](#bib.bib81)] | [https://github.com/aashi7/NearCollision](https://github.com/aashi7/NearCollision)
    |'
- en: '| [[69](#bib.bib69)] | [https://github.com/yabufarha/anticipating-activities](https://github.com/yabufarha/anticipating-activities)
    |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| [[69](#bib.bib69)] | [https://github.com/yabufarha/anticipating-activities](https://github.com/yabufarha/anticipating-activities)
    |'
- en: '| [[112](#bib.bib112)] | [https://github.com/gurkirt/realtime-action-detection](https://github.com/gurkirt/realtime-action-detection)
    |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| [[112](#bib.bib112)] | [https://github.com/gurkirt/realtime-action-detection](https://github.com/gurkirt/realtime-action-detection)
    |'
- en: '| [[80](#bib.bib80)] | [https://github.com/asheshjain399/RNNexp](https://github.com/asheshjain399/RNNexp)
    |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| [[80](#bib.bib80)] | [https://github.com/asheshjain399/RNNexp](https://github.com/asheshjain399/RNNexp)
    |'
- en: '| [[95](#bib.bib95)] | [https://github.com/aditya7874/Activity-Prediction-in-EgoCentric-Videos](https://github.com/aditya7874/Activity-Prediction-in-EgoCentric-Videos)
    |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| [[95](#bib.bib95)] | [https://github.com/aditya7874/Activity-Prediction-in-EgoCentric-Videos](https://github.com/aditya7874/Activity-Prediction-in-EgoCentric-Videos)
    |'
- en: '|  | [[100](#bib.bib100)] | [https://github.com/JoeHEZHAO/Spatiotemporal-Residual-Propagation](https://github.com/JoeHEZHAO/Spatiotemporal-Residual-Propagation)
    |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '|  | [[100](#bib.bib100)] | [https://github.com/JoeHEZHAO/Spatiotemporal-Residual-Propagation](https://github.com/JoeHEZHAO/Spatiotemporal-Residual-Propagation)
    |'
- en: '| Trajectory | [[177](#bib.bib177)] | [https://go.umd.edu/TraPHic](https://go.umd.edu/TraPHic)
    |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| Trajectory | [[177](#bib.bib177)] | [https://go.umd.edu/TraPHic](https://go.umd.edu/TraPHic)
    |'
- en: '| [[87](#bib.bib87)] | [https://github.com/google/next-prediction](https://github.com/google/next-prediction)
    |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| [[87](#bib.bib87)] | [https://github.com/google/next-prediction](https://github.com/google/next-prediction)
    |'
- en: '| [[139](#bib.bib139)] | [https://github.com/zhangpur/SR-LSTM](https://github.com/zhangpur/SR-LSTM)
    |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| [[139](#bib.bib139)] | [https://github.com/zhangpur/SR-LSTM](https://github.com/zhangpur/SR-LSTM)
    |'
- en: '| [[144](#bib.bib144)] | [https://github.com/aras62/PIEPredict](https://github.com/aras62/PIEPredict)
    |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| [[144](#bib.bib144)] | [https://github.com/aras62/PIEPredict](https://github.com/aras62/PIEPredict)
    |'
- en: '| [[162](#bib.bib162)] | [https://sites.google.com/view/precog](https://sites.google.com/view/precog)
    |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| [[162](#bib.bib162)] | [https://sites.google.com/view/precog](https://sites.google.com/view/precog)
    |'
- en: '| [[150](#bib.bib150)] | [https://rebrand.ly/INFER-results](https://rebrand.ly/INFER-results)
    |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| [[150](#bib.bib150)] | [https://rebrand.ly/INFER-results](https://rebrand.ly/INFER-results)
    |'
- en: '| [[179](#bib.bib179)] | [https://github.com/wzhi/KernelTrajectoryMaps](https://github.com/wzhi/KernelTrajectoryMaps)
    |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| [[179](#bib.bib179)] | [https://github.com/wzhi/KernelTrajectoryMaps](https://github.com/wzhi/KernelTrajectoryMaps)
    |'
- en: '| [[183](#bib.bib183)] | [https://github.com/apratimbhattacharyya18/onboard_long_term_prediction](https://github.com/apratimbhattacharyya18/onboard_long_term_prediction)
    |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| [[183](#bib.bib183)] | [https://github.com/apratimbhattacharyya18/onboard_long_term_prediction](https://github.com/apratimbhattacharyya18/onboard_long_term_prediction)
    |'
- en: '| [[153](#bib.bib153)] | [https://github.com/agrimgupta92/sgan](https://github.com/agrimgupta92/sgan)
    |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| [[153](#bib.bib153)] | [https://github.com/agrimgupta92/sgan](https://github.com/agrimgupta92/sgan)
    |'
- en: '| [[155](#bib.bib155)] | [https://github.com/svip-lab/CIDNN](https://github.com/svip-lab/CIDNN)
    |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| [[155](#bib.bib155)] | [https://github.com/svip-lab/CIDNN](https://github.com/svip-lab/CIDNN)
    |'
- en: '| [[174](#bib.bib174)] | [https://github.com/yfzhang/vehicle-motion-forecasting](https://github.com/yfzhang/vehicle-motion-forecasting)
    |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| [[174](#bib.bib174)] | [https://github.com/yfzhang/vehicle-motion-forecasting](https://github.com/yfzhang/vehicle-motion-forecasting)
    |'
- en: '| [[165](#bib.bib165)] | [https://github.com/yadrimz/DESIRE](https://github.com/yadrimz/DESIRE)
    |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| [[165](#bib.bib165)] | [https://github.com/yadrimz/DESIRE](https://github.com/yadrimz/DESIRE)
    |'
- en: '| [[160](#bib.bib160)] | [https://github.com/quancore/social-lstm](https://github.com/quancore/social-lstm)
    |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| [[160](#bib.bib160)] | [https://github.com/quancore/social-lstm](https://github.com/quancore/social-lstm)
    |'
- en: '| Motion | [[190](#bib.bib190)] | [https://github.com/cr7anand/neural_temporal_models](https://github.com/cr7anand/neural_temporal_models)
    |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| Motion | [[190](#bib.bib190)] | [https://github.com/cr7anand/neural_temporal_models](https://github.com/cr7anand/neural_temporal_models)
    |'
- en: '| [[236](#bib.bib236)] | [https://github.com/BII-wushuang/Lie-Group-Motion-Prediction](https://github.com/BII-wushuang/Lie-Group-Motion-Prediction)
    |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| [[236](#bib.bib236)] | [https://github.com/BII-wushuang/Lie-Group-Motion-Prediction](https://github.com/BII-wushuang/Lie-Group-Motion-Prediction)
    |'
- en: '| [[197](#bib.bib197)] | [https://github.com/wei-mao-2019/LearnTrajDep](https://github.com/wei-mao-2019/LearnTrajDep)
    |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| [[197](#bib.bib197)] | [https://github.com/wei-mao-2019/LearnTrajDep](https://github.com/wei-mao-2019/LearnTrajDep)
    |'
- en: '| [[198](#bib.bib198)] | [https://github.com/magnux/MotionGAN](https://github.com/magnux/MotionGAN)
    |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| [[198](#bib.bib198)] | [https://github.com/magnux/MotionGAN](https://github.com/magnux/MotionGAN)
    |'
- en: '| [[186](#bib.bib186)] | [https://github.com/Khrylx/EgoPose](https://github.com/Khrylx/EgoPose)
    |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| [[186](#bib.bib186)] | [https://github.com/Khrylx/EgoPose](https://github.com/Khrylx/EgoPose)
    |'
- en: '| [[199](#bib.bib199)] | [https://jasonyzhang.com/phd/](https://jasonyzhang.com/phd/)
    |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| [[199](#bib.bib199)] | [https://jasonyzhang.com/phd/](https://jasonyzhang.com/phd/)
    |'
- en: '| [[187](#bib.bib187)] | [https://github.com/eddyhkchiu/pose_forecast_wacv/.](https://github.com/eddyhkchiu/pose_forecast_wacv/.)
    |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| [[187](#bib.bib187)] | [https://github.com/eddyhkchiu/pose_forecast_wacv/.](https://github.com/eddyhkchiu/pose_forecast_wacv/.)
    |'
- en: '| [[189](#bib.bib189)] | [https://github.com/ywchao/image-play](https://github.com/ywchao/image-play)
    |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| [[189](#bib.bib189)] | [https://github.com/ywchao/image-play](https://github.com/ywchao/image-play)
    |'
- en: '| [[194](#bib.bib194)] | [https://github.com/una-dinosauria/human-motion-prediction](https://github.com/una-dinosauria/human-motion-prediction)
    |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| [[194](#bib.bib194)] | [https://github.com/una-dinosauria/human-motion-prediction](https://github.com/una-dinosauria/human-motion-prediction)
    |'
- en: '| Others | [[201](#bib.bib201)] | [https://bitbucket.org/vguizilini/cvpp/src](https://bitbucket.org/vguizilini/cvpp/src)
    |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | [[201](#bib.bib201)] | [https://bitbucket.org/vguizilini/cvpp/src](https://bitbucket.org/vguizilini/cvpp/src)
    |'
- en: '| [[211](#bib.bib211)] | [https://github.com/facebookresearch/SegmPred](https://github.com/facebookresearch/SegmPred)
    |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| [[211](#bib.bib211)] | [https://github.com/facebookresearch/SegmPred](https://github.com/facebookresearch/SegmPred)
    |'
- en: 'TABLE III: A summary of vision-based prediction papers with published code.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：具有发布代码的视觉预测论文汇总。
- en: 'A list of papers with official published code can be found in Table [III](#A1.T3
    "TABLE III ‣ Appendix A Papers with code ‣ Deep Learning for Vision-based Prediction:
    A Survey").'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '-   可以在表格[III](#A1.T3 "TABLE III ‣ Appendix A Papers with code ‣ Deep Learning
    for Vision-based Prediction: A Survey")中找到具有官方发布代码的论文列表。'
- en: Appendix B Metrics and corresponding papers
  id: totrans-662
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B  评估指标及对应论文
- en: '| Metric | Papers |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| Metric | Papers |'
- en: '| --- | --- |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| BCE | [[23](#bib.bib23)] |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| BCE | [[23](#bib.bib23)] |'
- en: '| FVD | [[3](#bib.bib3)],[[18](#bib.bib18)] |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| FVD | [[3](#bib.bib3)],[[18](#bib.bib18)] |'
- en: '| Human | [[11](#bib.bib11)],[[25](#bib.bib25)],[[8](#bib.bib8)],[[28](#bib.bib28)]
    |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | [[11](#bib.bib11)],[[25](#bib.bib25)],[[8](#bib.bib8)],[[28](#bib.bib28)]
    |'
- en: '| IS | [[26](#bib.bib26)] |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| IS | [[26](#bib.bib26)] |'
- en: '| L1 | [[9](#bib.bib9)],[[12](#bib.bib12)] |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| L1 | [[9](#bib.bib9)],[[12](#bib.bib12)] |'
- en: '| LPIPS | [[3](#bib.bib3)],[[17](#bib.bib17)],[[37](#bib.bib37)],[[11](#bib.bib11)]
    |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| LPIPS | [[3](#bib.bib3)],[[17](#bib.bib17)],[[37](#bib.bib37)],[[11](#bib.bib11)]
    |'
- en: '| MMD | [[26](#bib.bib26)] |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| MMD | [[26](#bib.bib26)] |'
- en: '| MSE | [[2](#bib.bib2)],[[4](#bib.bib4)],[[20](#bib.bib20)],[[34](#bib.bib34)],[[16](#bib.bib16)],
    [[12](#bib.bib12)],[[229](#bib.bib229)],[[23](#bib.bib23)],[[14](#bib.bib14)],[[7](#bib.bib7)],
    [[6](#bib.bib6)],[[27](#bib.bib27)],[[30](#bib.bib30)] |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| MSE | [[2](#bib.bib2)],[[4](#bib.bib4)],[[20](#bib.bib20)],[[34](#bib.bib34)],[[16](#bib.bib16)],
    [[12](#bib.bib12)],[[229](#bib.bib229)],[[23](#bib.bib23)],[[14](#bib.bib14)],[[7](#bib.bib7)],
    [[6](#bib.bib6)],[[27](#bib.bib27)],[[30](#bib.bib30)] |'
- en: '| PSNR | [[2](#bib.bib2)],[[1](#bib.bib1)],[[4](#bib.bib4)],[[31](#bib.bib31)],[[19](#bib.bib19)],
    [[20](#bib.bib20)],[[21](#bib.bib21)],[[5](#bib.bib5)],[[32](#bib.bib32)],[[33](#bib.bib33)],
    [[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[12](#bib.bib12)],[[229](#bib.bib229)],
    [[24](#bib.bib24)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[36](#bib.bib36)],
    [[6](#bib.bib6)],[[15](#bib.bib15)],[[27](#bib.bib27)],[[28](#bib.bib28)],[[29](#bib.bib29)]
    |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| PSNR | [[2](#bib.bib2)],[[1](#bib.bib1)],[[4](#bib.bib4)],[[31](#bib.bib31)],[[19](#bib.bib19)],
    [[20](#bib.bib20)],[[21](#bib.bib21)],[[5](#bib.bib5)],[[32](#bib.bib32)],[[33](#bib.bib33)],
    [[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[12](#bib.bib12)],[[229](#bib.bib229)],
    [[24](#bib.bib24)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[36](#bib.bib36)],
    [[6](#bib.bib6)],[[15](#bib.bib15)],[[27](#bib.bib27)],[[28](#bib.bib28)],[[29](#bib.bib29)]
    |'
- en: '| RMSE | [[11](#bib.bib11)] |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| RMSE | [[11](#bib.bib11)] |'
- en: '| SSIM | [[2](#bib.bib2)],[[3](#bib.bib3)],[[1](#bib.bib1)],[[4](#bib.bib4)],[[31](#bib.bib31)],
    [[19](#bib.bib19)],[[20](#bib.bib20)],[[21](#bib.bib21)],[[5](#bib.bib5)],[[32](#bib.bib32)],
    [[33](#bib.bib33)],[[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[12](#bib.bib12)],
    [[24](#bib.bib24)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[6](#bib.bib6)],
    [[15](#bib.bib15)],[[27](#bib.bib27)],[[29](#bib.bib29)] |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| SSIM | [[2](#bib.bib2)],[[3](#bib.bib3)],[[1](#bib.bib1)],[[4](#bib.bib4)],[[31](#bib.bib31)],
    [[19](#bib.bib19)],[[20](#bib.bib20)],[[21](#bib.bib21)],[[5](#bib.bib5)],[[32](#bib.bib32)],
    [[33](#bib.bib33)],[[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[12](#bib.bib12)],
    [[24](#bib.bib24)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[6](#bib.bib6)],
    [[15](#bib.bib15)],[[27](#bib.bib27)],[[29](#bib.bib29)] |'
- en: 'TABLE IV: Metrics used in video prediction applications.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：视频预测应用中使用的指标。
- en: '| Metrics | Papers |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 文献 |'
- en: '| --- | --- |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| AP | [[88](#bib.bib88)],[[9](#bib.bib9)],[[73](#bib.bib73)],[[82](#bib.bib82)],[[79](#bib.bib79)],
    [[88](#bib.bib88)] |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| AP | [[88](#bib.bib88)],[[9](#bib.bib9)],[[73](#bib.bib73)],[[82](#bib.bib82)],[[79](#bib.bib79)],
    [[88](#bib.bib88)] |'
- en: '| ATTA | [[82](#bib.bib82)] |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| ATTA | [[82](#bib.bib82)] |'
- en: '| ATTC | [[83](#bib.bib83)] |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| ATTC | [[83](#bib.bib83)] |'
- en: '| AUC | [[71](#bib.bib71)],[[54](#bib.bib54)],[[112](#bib.bib112)] |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| AUC | [[71](#bib.bib71)],[[54](#bib.bib54)],[[112](#bib.bib112)] |'
- en: '| Accuracy | [[56](#bib.bib56)],[[63](#bib.bib63)],[[64](#bib.bib64)],[[66](#bib.bib66)],[[71](#bib.bib71)],
    [[9](#bib.bib9)],[[74](#bib.bib74)],[[48](#bib.bib48)],[[49](#bib.bib49)],[[67](#bib.bib67)],
    [[68](#bib.bib68)],[[69](#bib.bib69)],[[83](#bib.bib83)],[[89](#bib.bib89)],[[75](#bib.bib75)],
    [[97](#bib.bib97)],[[90](#bib.bib90)],[[76](#bib.bib76)],[[91](#bib.bib91)],[[85](#bib.bib85)],
    [[86](#bib.bib86)],[[70](#bib.bib70)],[[50](#bib.bib50)],[[8](#bib.bib8)],[[93](#bib.bib93)],
    [[94](#bib.bib94)],[[52](#bib.bib52)],[[59](#bib.bib59)],[[42](#bib.bib42)],[[54](#bib.bib54)],
    [[43](#bib.bib43)],[[95](#bib.bib95)],[[55](#bib.bib55)],[[46](#bib.bib46)],[[40](#bib.bib40)],
    [[47](#bib.bib47)],[[98](#bib.bib98)],[[99](#bib.bib99)],[[100](#bib.bib100)],[[107](#bib.bib107)],
    [[108](#bib.bib108)],[[101](#bib.bib101)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[103](#bib.bib103)],
    [[104](#bib.bib104)],[[110](#bib.bib110)],[[111](#bib.bib111)],[[105](#bib.bib105)],[[112](#bib.bib112)],[[41](#bib.bib41)],[[106](#bib.bib106)],[[61](#bib.bib61)],[[113](#bib.bib113)],[[44](#bib.bib44)],
    [[62](#bib.bib62)],[[45](#bib.bib45)] |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | [[56](#bib.bib56)],[[63](#bib.bib63)],[[64](#bib.bib64)],[[66](#bib.bib66)],[[71](#bib.bib71)],
    [[9](#bib.bib9)],[[74](#bib.bib74)],[[48](#bib.bib48)],[[49](#bib.bib49)],[[67](#bib.bib67)],
    [[68](#bib.bib68)],[[69](#bib.bib69)],[[83](#bib.bib83)],[[89](#bib.bib89)],[[75](#bib.bib75)],
    [[97](#bib.bib97)],[[90](#bib.bib90)],[[76](#bib.bib76)],[[91](#bib.bib91)],[[85](#bib.bib85)],
    [[86](#bib.bib86)],[[70](#bib.bib70)],[[50](#bib.bib50)],[[8](#bib.bib8)],[[93](#bib.bib93)],
    [[94](#bib.bib94)],[[52](#bib.bib52)],[[59](#bib.bib59)],[[42](#bib.bib42)],[[54](#bib.bib54)],
    [[43](#bib.bib43)],[[95](#bib.bib95)],[[55](#bib.bib55)],[[46](#bib.bib46)],[[40](#bib.bib40)],
    [[47](#bib.bib47)],[[98](#bib.bib98)],[[99](#bib.bib99)],[[100](#bib.bib100)],[[107](#bib.bib107)],
    [[108](#bib.bib108)],[[101](#bib.bib101)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[103](#bib.bib103)],
    [[104](#bib.bib104)],[[110](#bib.bib110)],[[111](#bib.bib111)],[[105](#bib.bib105)],[[112](#bib.bib112)],[[41](#bib.bib41)],[[106](#bib.bib106)],[[61](#bib.bib61)],[[113](#bib.bib113)],[[44](#bib.bib44)],
    [[62](#bib.bib62)],[[45](#bib.bib45)] |'
- en: '| F1 | [[71](#bib.bib71)],[[72](#bib.bib72)],[[9](#bib.bib9)],[[83](#bib.bib83)],[[90](#bib.bib90)],[[58](#bib.bib58)],[[96](#bib.bib96)],[[52](#bib.bib52)],[[42](#bib.bib42)]
    |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| F1 | [[71](#bib.bib71)],[[72](#bib.bib72)],[[9](#bib.bib9)],[[83](#bib.bib83)],[[90](#bib.bib90)],[[58](#bib.bib58)],[[96](#bib.bib96)],[[52](#bib.bib52)],[[42](#bib.bib42)]
    |'
- en: '| FP | [[53](#bib.bib53)] |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| FP | [[53](#bib.bib53)] |'
- en: '| MAE | [[81](#bib.bib81)] |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '| MAE | [[81](#bib.bib81)] |'
- en: '| MCC | [[76](#bib.bib76)] |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '| MCC | [[76](#bib.bib76)] |'
- en: '| MRR | [[44](#bib.bib44)] |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '| MRR | [[44](#bib.bib44)] |'
- en: '| PP | [[57](#bib.bib57)] |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| PP | [[57](#bib.bib57)] |'
- en: '| Precision | [[63](#bib.bib63)],[[71](#bib.bib71)],[[72](#bib.bib72)],[[9](#bib.bib9)],[[74](#bib.bib74)],
    [[67](#bib.bib67)],[[82](#bib.bib82)],[[83](#bib.bib83)],[[70](#bib.bib70)],[[58](#bib.bib58)],
    [[51](#bib.bib51)],[[96](#bib.bib96)],[[80](#bib.bib80)],[[52](#bib.bib52)],[[42](#bib.bib42)],
    [[53](#bib.bib53)] |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | [[63](#bib.bib63)],[[71](#bib.bib71)],[[72](#bib.bib72)],[[9](#bib.bib9)],[[74](#bib.bib74)],
    [[67](#bib.bib67)],[[82](#bib.bib82)],[[83](#bib.bib83)],[[70](#bib.bib70)],[[58](#bib.bib58)],
    [[51](#bib.bib51)],[[96](#bib.bib96)],[[80](#bib.bib80)],[[52](#bib.bib52)],[[42](#bib.bib42)],
    [[53](#bib.bib53)] |'
- en: '| RMSE | [[70](#bib.bib70)],[[43](#bib.bib43)],[[60](#bib.bib60)] |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| RMSE | [[70](#bib.bib70)],[[43](#bib.bib43)],[[60](#bib.bib60)] |'
- en: '| Recall | [[63](#bib.bib63)],[[64](#bib.bib64)],[[65](#bib.bib65)],[[71](#bib.bib71)],[[9](#bib.bib9)],
    [[74](#bib.bib74)],[[67](#bib.bib67)],[[68](#bib.bib68)],[[82](#bib.bib82)],[[83](#bib.bib83)],
    [[77](#bib.bib77)],[[70](#bib.bib70)],[[51](#bib.bib51)],[[96](#bib.bib96)],[[80](#bib.bib80)],
    [[52](#bib.bib52)],[[42](#bib.bib42)],[[53](#bib.bib53)] |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | [[63](#bib.bib63)],[[64](#bib.bib64)],[[65](#bib.bib65)],[[71](#bib.bib71)],[[9](#bib.bib9)],
    [[74](#bib.bib74)],[[67](#bib.bib67)],[[68](#bib.bib68)],[[82](#bib.bib82)],[[83](#bib.bib83)],
    [[77](#bib.bib77)],[[70](#bib.bib70)],[[51](#bib.bib51)],[[96](#bib.bib96)],[[80](#bib.bib80)],
    [[52](#bib.bib52)],[[42](#bib.bib42)],[[53](#bib.bib53)] |'
- en: '| Run time | [[9](#bib.bib9)],[[73](#bib.bib73)] |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| 运行时间 | [[9](#bib.bib9)],[[73](#bib.bib73)] |'
- en: '| TNR | [[47](#bib.bib47)] |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| TNR | [[47](#bib.bib47)] |'
- en: '| TPR | [[47](#bib.bib47)] |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| TPR | [[47](#bib.bib47)] |'
- en: '| TTA | [[84](#bib.bib84)] |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '| TTA | [[84](#bib.bib84)] |'
- en: '| TTM | [[74](#bib.bib74)],[[49](#bib.bib49)],[[96](#bib.bib96)],[[80](#bib.bib80)],[[53](#bib.bib53)]
    |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '| TTM | [[74](#bib.bib74)],[[49](#bib.bib49)],[[96](#bib.bib96)],[[80](#bib.bib80)],[[53](#bib.bib53)]
    |'
- en: '| cAP | [[92](#bib.bib92)] |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '| cAP | [[92](#bib.bib92)] |'
- en: '| mAP | [[87](#bib.bib87)],[[83](#bib.bib83)],[[77](#bib.bib77)],[[84](#bib.bib84)],[[78](#bib.bib78)],[[92](#bib.bib92)],[[107](#bib.bib107)],[[112](#bib.bib112)]
    |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| mAP | [[87](#bib.bib87)],[[83](#bib.bib83)],[[77](#bib.bib77)],[[84](#bib.bib84)],[[78](#bib.bib78)],[[92](#bib.bib92)],[[107](#bib.bib107)],[[112](#bib.bib112)]
    |'
- en: '| recall | [[72](#bib.bib72)],[[58](#bib.bib58)] |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | [[72](#bib.bib72)],[[58](#bib.bib58)] |'
- en: 'TABLE V: Metrics used in action prediction applications.'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 在动作预测应用中使用的指标。'
- en: '| Metrics | Papers |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 论文 |'
- en: '| --- | --- |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ADE | [[177](#bib.bib177)],[[161](#bib.bib161)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],
    [[140](#bib.bib140)],[[141](#bib.bib141)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[144](#bib.bib144)],[[146](#bib.bib146)],[[169](#bib.bib169)],[[170](#bib.bib170)],[[148](#bib.bib148)],[[149](#bib.bib149)],
    [[182](#bib.bib182)],[[163](#bib.bib163)],[[150](#bib.bib150)],[[171](#bib.bib171)],[[151](#bib.bib151)],
    [[168](#bib.bib168)],[[172](#bib.bib172)],[[179](#bib.bib179)],[[152](#bib.bib152)],[[129](#bib.bib129)],[[183](#bib.bib183)],[[153](#bib.bib153)],[[154](#bib.bib154)],[[155](#bib.bib155)],[[101](#bib.bib101)],
    [[184](#bib.bib184)],[[164](#bib.bib164)],[[234](#bib.bib234)],[[180](#bib.bib180)],[[121](#bib.bib121)],
    [[174](#bib.bib174)],[[131](#bib.bib131)],[[158](#bib.bib158)],[[159](#bib.bib159)],[[127](#bib.bib127)],[[160](#bib.bib160)],[[115](#bib.bib115)],[[132](#bib.bib132)],[[176](#bib.bib176)],[[123](#bib.bib123)],
    [[117](#bib.bib117)],[[136](#bib.bib136)] |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '| ADE | [[177](#bib.bib177)],[[161](#bib.bib161)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],
    [[140](#bib.bib140)],[[141](#bib.bib141)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[144](#bib.bib144)],[[146](#bib.bib146)],[[169](#bib.bib169)],[[170](#bib.bib170)],[[148](#bib.bib148)],[[149](#bib.bib149)],
    [[182](#bib.bib182)],[[163](#bib.bib163)],[[150](#bib.bib150)],[[171](#bib.bib171)],[[151](#bib.bib151)],
    [[168](#bib.bib168)],[[172](#bib.bib172)],[[179](#bib.bib179)],[[152](#bib.bib152)],[[129](#bib.bib129)],[[183](#bib.bib183)],[[153](#bib.bib153)],[[154](#bib.bib154)],[[155](#bib.bib155)],[[101](#bib.bib101)],
    [[184](#bib.bib184)],[[164](#bib.bib164)],[[234](#bib.bib234)],[[180](#bib.bib180)],[[121](#bib.bib121)],
    [[174](#bib.bib174)],[[131](#bib.bib131)],[[158](#bib.bib158)],[[159](#bib.bib159)],[[127](#bib.bib127)],[[160](#bib.bib160)],[[115](#bib.bib115)],[[132](#bib.bib132)],[[176](#bib.bib176)],[[123](#bib.bib123)],
    [[117](#bib.bib117)],[[136](#bib.bib136)] |'
- en: '| AEDE | [[234](#bib.bib234)] |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| AEDE | [[234](#bib.bib234)] |'
- en: '| ANDE | [[155](#bib.bib155)],[[160](#bib.bib160)] |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
  zh: '| ANDE | [[155](#bib.bib155)],[[160](#bib.bib160)] |'
- en: '| APP | [[157](#bib.bib157)] |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '| APP | [[157](#bib.bib157)] |'
- en: '| Accuracy | [[167](#bib.bib167)],[[127](#bib.bib127)] |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | [[167](#bib.bib167)],[[127](#bib.bib127)] |'
- en: '| CE | [[166](#bib.bib166)] |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
  zh: '| CE | [[166](#bib.bib166)] |'
- en: '| DtG | [[125](#bib.bib125)] |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
  zh: '| DtG | [[125](#bib.bib125)] |'
- en: '| ECE | [[172](#bib.bib172)] |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '| ECE | [[172](#bib.bib172)] |'
- en: '| ED | [[114](#bib.bib114)],[[173](#bib.bib173)],[[130](#bib.bib130)],[[156](#bib.bib156)],[[165](#bib.bib165)],[[124](#bib.bib124)],[[133](#bib.bib133)],[[134](#bib.bib134)],[[135](#bib.bib135)],[[136](#bib.bib136)]
    |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
  zh: '| ED | [[114](#bib.bib114)],[[173](#bib.bib173)],[[130](#bib.bib130)],[[156](#bib.bib156)],[[165](#bib.bib165)],[[124](#bib.bib124)],[[133](#bib.bib133)],[[134](#bib.bib134)],[[135](#bib.bib135)],[[136](#bib.bib136)]
    |'
- en: '| FDE | [[177](#bib.bib177)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],
    [[141](#bib.bib141)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[144](#bib.bib144)],[[146](#bib.bib146)],
    [[169](#bib.bib169)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[168](#bib.bib168)],
    [[172](#bib.bib172)],[[179](#bib.bib179)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[154](#bib.bib154)],
    [[155](#bib.bib155)],[[164](#bib.bib164)],[[180](#bib.bib180)],[[131](#bib.bib131)],[[158](#bib.bib158)],
    [[160](#bib.bib160)],[[176](#bib.bib176)] |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
  zh: '| FDE | [[177](#bib.bib177)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],
    [[141](#bib.bib141)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[144](#bib.bib144)],[[146](#bib.bib146)],
    [[169](#bib.bib169)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[168](#bib.bib168)],
    [[172](#bib.bib172)],[[179](#bib.bib179)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[154](#bib.bib154)],
    [[155](#bib.bib155)],[[164](#bib.bib164)],[[180](#bib.bib180)],[[131](#bib.bib131)],[[158](#bib.bib158)],
    [[160](#bib.bib160)],[[176](#bib.bib176)] |'
- en: '| FNM | [[126](#bib.bib126)] |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
  zh: '| FNM | [[126](#bib.bib126)] |'
- en: '| Hit rate | [[161](#bib.bib161)],[[128](#bib.bib128)] |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
  zh: '| 命中率 | [[161](#bib.bib161)],[[128](#bib.bib128)] |'
- en: '| KLD | [[181](#bib.bib181)],[[127](#bib.bib127)],[[125](#bib.bib125)] |'
  id: totrans-716
  prefs: []
  type: TYPE_TB
  zh: '| KLD | [[181](#bib.bib181)],[[127](#bib.bib127)],[[125](#bib.bib125)] |'
- en: '| L1 | [[77](#bib.bib77)] |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
  zh: '| L1 | [[77](#bib.bib77)] |'
- en: '| LL | [[145](#bib.bib145)],[[181](#bib.bib181)],[[168](#bib.bib168)],[[120](#bib.bib120)]
    |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
  zh: '| LL | [[145](#bib.bib145)],[[181](#bib.bib181)],[[168](#bib.bib168)],[[120](#bib.bib120)]
    |'
- en: '| MAnE | [[131](#bib.bib131)] |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '| MAnE | [[131](#bib.bib131)] |'
- en: '| MHD | [[118](#bib.bib118)],[[119](#bib.bib119)],[[125](#bib.bib125)] |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '| MHD | [[118](#bib.bib118)],[[119](#bib.bib119)],[[125](#bib.bib125)] |'
- en: '| MSE | [[77](#bib.bib77)],[[125](#bib.bib125)] |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
  zh: '| MSE | [[77](#bib.bib77)],[[125](#bib.bib125)] |'
- en: '| Miss rate | [[184](#bib.bib184)],[[165](#bib.bib165)] |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
  zh: '| 漏检率 | [[184](#bib.bib184)],[[165](#bib.bib165)] |'
- en: '| NLL | [[172](#bib.bib172)],[[183](#bib.bib183)],[[174](#bib.bib174)],[[175](#bib.bib175)],[[123](#bib.bib123)],[[125](#bib.bib125)]
    |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
  zh: '| NLL | [[172](#bib.bib172)],[[183](#bib.bib183)],[[174](#bib.bib174)],[[175](#bib.bib175)],[[123](#bib.bib123)],[[125](#bib.bib125)]
    |'
- en: '| NLP | [[118](#bib.bib118)],[[119](#bib.bib119)] |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '| NLP | [[118](#bib.bib118)],[[119](#bib.bib119)] |'
- en: '| None | [[116](#bib.bib116)] |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
  zh: '| 无 | [[116](#bib.bib116)] |'
- en: '| PD | [[122](#bib.bib122)] |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
  zh: '| PD | [[122](#bib.bib122)] |'
- en: '| Precision | [[115](#bib.bib115)] |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | [[115](#bib.bib115)] |'
- en: '| RMSE | [[140](#bib.bib140)],[[147](#bib.bib147)] |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
  zh: '| RMSE | [[140](#bib.bib140)],[[147](#bib.bib147)] |'
- en: '| Run time | [[147](#bib.bib147)],[[118](#bib.bib118)],[[121](#bib.bib121)],[[123](#bib.bib123)],[[135](#bib.bib135)]
    |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
  zh: '| 运行时间 | [[147](#bib.bib147)],[[118](#bib.bib118)],[[121](#bib.bib121)],[[123](#bib.bib123)],[[135](#bib.bib135)]
    |'
- en: '| SCR | [[175](#bib.bib175)] |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
  zh: '| SCR | [[175](#bib.bib175)] |'
- en: '| WRMSE | [[120](#bib.bib120)] |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '| WRMSE | [[120](#bib.bib120)] |'
- en: '| maxD | [[184](#bib.bib184)],[[165](#bib.bib165)] |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
  zh: '| maxD | [[184](#bib.bib184)],[[165](#bib.bib165)] |'
- en: '| meanMSD | [[162](#bib.bib162)],[[166](#bib.bib166)] |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
  zh: '| meanMSD | [[162](#bib.bib162)],[[166](#bib.bib166)] |'
- en: '| minADE | [[137](#bib.bib137)],[[178](#bib.bib178)],[[149](#bib.bib149)],[[168](#bib.bib168)]
    |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
  zh: '| minADE | [[137](#bib.bib137)],[[178](#bib.bib178)],[[149](#bib.bib149)],[[168](#bib.bib168)]
    |'
- en: '| minED | [[161](#bib.bib161)],[[165](#bib.bib165)] |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
  zh: '| minED | [[161](#bib.bib161)],[[165](#bib.bib165)] |'
- en: '| minFDE | [[137](#bib.bib137)],[[178](#bib.bib178)] |'
  id: totrans-736
  prefs: []
  type: TYPE_TB
  zh: '| minFDE | [[137](#bib.bib137)],[[178](#bib.bib178)] |'
- en: '| minMSD | [[162](#bib.bib162)],[[166](#bib.bib166)] |'
  id: totrans-737
  prefs: []
  type: TYPE_TB
  zh: '| minMSD | [[162](#bib.bib162)],[[166](#bib.bib166)] |'
- en: 'TABLE VI: Metrics used in trajectory prediction applications.'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 用于轨迹预测应用的指标。'
- en: '| Metrics | Papers |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 论文 |'
- en: '| --- | --- |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Accuracy | [[48](#bib.bib48)],[[195](#bib.bib195)] |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | [[48](#bib.bib48)],[[195](#bib.bib195)] |'
- en: '| Human | [[198](#bib.bib198)],[[192](#bib.bib192)] |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '| 人体 | [[198](#bib.bib198)],[[192](#bib.bib192)] |'
- en: '| LO | [[56](#bib.bib56)] |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '| LO | [[56](#bib.bib56)] |'
- en: '| MAnE | [[190](#bib.bib190)],[[197](#bib.bib197)],[[198](#bib.bib198)],[[196](#bib.bib196)],[[191](#bib.bib191)],[[192](#bib.bib192)],[[193](#bib.bib193)],[[194](#bib.bib194)],[[96](#bib.bib96)],[[195](#bib.bib195)]
    |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
  zh: '| MAnE | [[190](#bib.bib190)],[[197](#bib.bib197)],[[198](#bib.bib198)],[[196](#bib.bib196)],[[191](#bib.bib191)],[[192](#bib.bib192)],[[193](#bib.bib193)],[[194](#bib.bib194)],[[96](#bib.bib96)],[[195](#bib.bib195)]
    |'
- en: '| MJE | [[56](#bib.bib56)],[[236](#bib.bib236)],[[186](#bib.bib186)],[[200](#bib.bib200)],[[187](#bib.bib187)],[[188](#bib.bib188)],[[101](#bib.bib101)],[[103](#bib.bib103)],[[110](#bib.bib110)],[[26](#bib.bib26)],[[185](#bib.bib185)]
    |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '| MJE | [[56](#bib.bib56)],[[236](#bib.bib236)],[[186](#bib.bib186)],[[200](#bib.bib200)],[[187](#bib.bib187)],[[188](#bib.bib188)],[[101](#bib.bib101)],[[103](#bib.bib103)],[[110](#bib.bib110)],[[26](#bib.bib26)],[[185](#bib.bib185)]
    |'
- en: '| MPJPE | [[197](#bib.bib197)],[[199](#bib.bib199)] |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '| MPJPE | [[197](#bib.bib197)],[[199](#bib.bib199)] |'
- en: '| NPSS | [[190](#bib.bib190)] |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '| NPSS | [[190](#bib.bib190)] |'
- en: '| PCK | [[199](#bib.bib199)],[[187](#bib.bib187)],[[188](#bib.bib188)],[[189](#bib.bib189)]
    |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '| PCK | [[199](#bib.bib199)],[[187](#bib.bib187)],[[188](#bib.bib188)],[[189](#bib.bib189)]
    |'
- en: '| PSEnt | [[198](#bib.bib198)] |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '| PSEnt | [[198](#bib.bib198)] |'
- en: '| PSKL | [[198](#bib.bib198)] |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '| PSKL | [[198](#bib.bib198)] |'
- en: '| RE | [[199](#bib.bib199)] |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
  zh: '| RE | [[199](#bib.bib199)] |'
- en: '| Run time | [[236](#bib.bib236)],[[188](#bib.bib188)] |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
  zh: '| 运行时间 | [[236](#bib.bib236)],[[188](#bib.bib188)] |'
- en: 'TABLE VII: Metrics used in motion prediction applications.'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VII: 用于运动预测应用的指标。'
- en: 'Lists of metrics and corresponding papers can be found in Tables [IV](#A2.T4
    "TABLE IV ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey"), [V](#A2.T5 "TABLE V ‣ Appendix B Metrics and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey"), [VI](#A2.T6 "TABLE
    VI ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey"), [VII](#A2.T7 "TABLE VII ‣ Appendix B Metrics and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey"), and [VIII](#A2.T8
    "TABLE VIII ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for
    Vision-based Prediction: A Survey").'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: '指标及对应论文的列表可以在表 [IV](#A2.T4 "TABLE IV ‣ Appendix B Metrics and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey")、[V](#A2.T5 "TABLE
    V ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey")、[VI](#A2.T6 "TABLE VI ‣ Appendix B Metrics and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey")、[VII](#A2.T7 "TABLE
    VII ‣ Appendix B Metrics and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey") 和 [VIII](#A2.T8 "TABLE VIII ‣ Appendix B Metrics and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey") 中找到。'
- en: '| Metrics | Papers |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 论文 |'
- en: '| --- | --- |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| AUC | [[209](#bib.bib209)] |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '| AUC | [[209](#bib.bib209)] |'
- en: '| Accuracy | [[8](#bib.bib8)],[[220](#bib.bib220)],[[221](#bib.bib221)] |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | [[8](#bib.bib8)],[[220](#bib.bib220)],[[221](#bib.bib221)] |'
- en: '| ED | [[202](#bib.bib202)],[[238](#bib.bib238)] |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
  zh: '| ED | [[202](#bib.bib202)],[[238](#bib.bib238)] |'
- en: '| EPE | [[212](#bib.bib212)] |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
  zh: '| EPE | [[212](#bib.bib212)] |'
- en: '| F1 | [[201](#bib.bib201)],[[207](#bib.bib207)] |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
  zh: '| F1 | [[201](#bib.bib201)],[[207](#bib.bib207)] |'
- en: '| GCE | [[210](#bib.bib210)] |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
  zh: '| GCE | [[210](#bib.bib210)] |'
- en: '| ISM | [[203](#bib.bib203)] |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
  zh: '| ISM | [[203](#bib.bib203)] |'
- en: '| IoU | [[10](#bib.bib10)],[[210](#bib.bib210)],[[211](#bib.bib211)] |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
  zh: '| IoU | [[10](#bib.bib10)],[[210](#bib.bib210)],[[211](#bib.bib211)] |'
- en: '| MAE | [[239](#bib.bib239)],[[217](#bib.bib217)],[[204](#bib.bib204)] |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
  zh: '| MAE | [[239](#bib.bib239)],[[217](#bib.bib217)],[[204](#bib.bib204)] |'
- en: '| MAPE | [[216](#bib.bib216)],[[217](#bib.bib217)] |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '| MAPE | [[216](#bib.bib216)],[[217](#bib.bib217)] |'
- en: '| MCC | [[218](#bib.bib218)] |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '| MCC | [[218](#bib.bib218)] |'
- en: '| MIoU | [[212](#bib.bib212)] |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '| MIoU | [[212](#bib.bib212)] |'
- en: '| MSE | [[212](#bib.bib212)] |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
  zh: '| MSE | [[212](#bib.bib212)] |'
- en: '| PCP | [[219](#bib.bib219)] |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
  zh: '| PCP | [[219](#bib.bib219)] |'
- en: '| PSNR | [[206](#bib.bib206)],[[211](#bib.bib211)] |'
  id: totrans-771
  prefs: []
  type: TYPE_TB
  zh: '| PSNR | [[206](#bib.bib206)],[[211](#bib.bib211)] |'
- en: '| Precision | [[209](#bib.bib209)],[[213](#bib.bib213)],[[218](#bib.bib218)]
    |'
  id: totrans-772
  prefs: []
  type: TYPE_TB
  zh: '| Precision | [[209](#bib.bib209)],[[213](#bib.bib213)],[[218](#bib.bib218)]
    |'
- en: '| Psi | [[203](#bib.bib203)] |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
  zh: '| Psi | [[203](#bib.bib203)] |'
- en: '| RI | [[210](#bib.bib210)] |'
  id: totrans-774
  prefs: []
  type: TYPE_TB
  zh: '| RI | [[210](#bib.bib210)] |'
- en: '| RMSE | [[214](#bib.bib214)] |'
  id: totrans-775
  prefs: []
  type: TYPE_TB
  zh: '| RMSE | [[214](#bib.bib214)] |'
- en: '| ROC | [[207](#bib.bib207)],[[208](#bib.bib208)] |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
  zh: '| ROC | [[207](#bib.bib207)],[[208](#bib.bib208)] |'
- en: '| Recall | [[209](#bib.bib209)],[[213](#bib.bib213)],[[218](#bib.bib218)] |'
  id: totrans-777
  prefs: []
  type: TYPE_TB
  zh: '| Recall | [[209](#bib.bib209)],[[213](#bib.bib213)],[[218](#bib.bib218)] |'
- en: '| Run time | [[205](#bib.bib205)],[[206](#bib.bib206)],[[209](#bib.bib209)]
    |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
  zh: '| 运行时间 | [[205](#bib.bib205)],[[206](#bib.bib206)],[[209](#bib.bib209)] |'
- en: '| SRC | [[216](#bib.bib216)] |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '| SRC | [[216](#bib.bib216)] |'
- en: '| SSIM | [[205](#bib.bib205)],[[206](#bib.bib206)],[[211](#bib.bib211)] |'
  id: totrans-780
  prefs: []
  type: TYPE_TB
  zh: '| SSIM | [[205](#bib.bib205)],[[206](#bib.bib206)],[[211](#bib.bib211)] |'
- en: '| TN | [[205](#bib.bib205)] |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
  zh: '| TN | [[205](#bib.bib205)] |'
- en: '| TP | [[205](#bib.bib205)] |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
  zh: '| TP | [[205](#bib.bib205)] |'
- en: '| VoI | [[210](#bib.bib210)] |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
  zh: '| VoI | [[210](#bib.bib210)] |'
- en: '| nMAPE | [[215](#bib.bib215)] |'
  id: totrans-784
  prefs: []
  type: TYPE_TB
  zh: '| nMAPE | [[215](#bib.bib215)] |'
- en: 'TABLE VIII: Metrics used in other prediction applications.'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VIII: 在其他预测应用中使用的度量标准。'
- en: Appendix C Metric formulas
  id: totrans-786
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 度量公式
- en: C.1 Video prediction
  id: totrans-787
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 视频预测
- en: '|  | $MSE=\frac{1}{MN}\sum_{i=1}^{M}\sum_{j=1}^{N}(I(i,j)-\tilde{I}(i,j))^{2}$
    |  |'
  id: totrans-788
  prefs: []
  type: TYPE_TB
  zh: '|  | $MSE=\frac{1}{MN}\sum_{i=1}^{M}\sum_{j=1}^{N}(I(i,j)-\tilde{I}(i,j))^{2}$
    |  |'
- en: '|  | $PSNR=20\log\left(\frac{MAX_{I}}{\sqrt{MSE}}\right)$ |  |'
  id: totrans-789
  prefs: []
  type: TYPE_TB
  zh: '|  | $PSNR=20\log\left(\frac{MAX_{I}}{\sqrt{MSE}}\right)$ |  |'
- en: Structural Similarity (SSIM)
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 结构相似性（SSIM）
- en: '|  | $luminance(l)(x,y)=\frac{2\mu_{x}\mu_{y}+C_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+C_{1}}$
    |  |'
  id: totrans-791
  prefs: []
  type: TYPE_TB
  zh: '|  | $luminance(l)(x,y)=\frac{2\mu_{x}\mu_{y}+C_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+C_{1}}$
    |  |'
- en: '|  | $\mu_{x}=\frac{1}{N}\sum_{i=1}^{N}x_{i}$ |  |'
  id: totrans-792
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu_{x}=\frac{1}{N}\sum_{i=1}^{N}x_{i}$ |  |'
- en: '|  | $C_{1}=(K_{1}L)^{2}$ |  |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{1}=(K_{1}L)^{2}$ |  |'
- en: where $L$ is dynamic range of pixel values (e.g. 255) and $K_{1}\ll 1$ is a
    small constant.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L$ 是像素值的动态范围（例如 255），$K_{1}\ll 1$ 是一个小常数。
- en: '|  | $contrast(c)(x,y)=\frac{2\sigma_{x}\sigma_{y}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}}$
    |  |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
  zh: '|  | $contrast(c)(x,y)=\frac{2\sigma_{x}\sigma_{y}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}}$
    |  |'
- en: '|  | $\sigma_{x}=\left(\frac{1}{N-1}\sum_{i=1}^{N}(x_{i}-\mu_{x})^{2}\right)^{1/2}$
    |  |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma_{x}=\left(\frac{1}{N-1}\sum_{i=1}^{N}(x_{i}-\mu_{x})^{2}\right)^{1/2}$
    |  |'
- en: '|  | $C_{2}=(K_{2}L)^{2}$ |  |'
  id: totrans-797
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{2}=(K_{2}L)^{2}$ |  |'
- en: '|  | $K_{2}\ll 1$ |  |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
  zh: '|  | $K_{2}\ll 1$ |  |'
- en: '|  | $structure(s)(x,y)=\frac{\sigma_{xy}+C_{3}}{\sigma_{x}\sigma_{y}+C_{3}}$
    |  |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
  zh: '|  | $structure(s)(x,y)=\frac{\sigma_{xy}+C_{3}}{\sigma_{x}\sigma_{y}+C_{3}}$
    |  |'
- en: '|  | $C_{3}=(K_{3}L)^{2}$ |  |'
  id: totrans-800
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{3}=(K_{3}L)^{2}$ |  |'
- en: '|  | $K_{3}\ll 1$ |  |'
  id: totrans-801
  prefs: []
  type: TYPE_TB
  zh: '|  | $K_{3}\ll 1$ |  |'
- en: '|  | $SSIM(x,y)=[l(x,y)]^{\alpha}.[c(x,y)]^{\beta}.[s(x,y)]^{\gamma}$ |  |'
  id: totrans-802
  prefs: []
  type: TYPE_TB
  zh: '|  | $SSIM(x,y)=[l(x,y)]^{\alpha}.[c(x,y)]^{\beta}.[s(x,y)]^{\gamma}$ |  |'
- en: where $\alpha,\beta,\gamma>0$ are parameters to choose in order to adjust the
    importance.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha,\beta,\gamma>0$ 是需要选择的参数，以调整重要性。
- en: Learned Perceptual Image Patch Similarity (LPIPS)
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 学习感知图像块相似度（LPIPS）
- en: Assume features are extracted from $L$ layers and unit-normalized in channel
    dimension, for layer l
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 假设特征从 $L$ 层提取，并在通道维度上单位归一化，对于层 l
- en: '|  | $\hat{y}^{l},\hat{y}^{l}\in R^{H_{l}\times W_{l}\times C_{l}}$ |  |'
  id: totrans-806
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}^{l},\hat{y}^{l}\in R^{H_{l}\times W_{l}\times C_{l}}$ |  |'
- en: .
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: The distance between reference $x$ and distorted patches $x_{0}$ is given by,
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 参考 $x$ 和失真块 $x_{0}$ 之间的距离由下式给出，
- en: '|  | $d(x,x_{0})=\sum_{l}\frac{1}{H_{l}W_{l}}\sum_{w,l}\parallel w_{l}\odot(\hat{y}^{l}_{hw},\hat{y}^{l}_{0hw})\parallel^{2}_{2}$
    |  |'
  id: totrans-809
  prefs: []
  type: TYPE_TB
  zh: '|  | $d(x,x_{0})=\sum_{l}\frac{1}{H_{l}W_{l}}\sum_{w,l}\parallel w_{l}\odot(\hat{y}^{l}_{hw},\hat{y}^{l}_{0hw})\parallel^{2}_{2}$
    |  |'
- en: C.2 Action prediction
  id: totrans-810
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 行为预测
- en: 'There are 4 possibilities for classification: True positive (TP) and True Negative
    (TN) when the algorithm correctly classifies positive and negative samples, and
    False Positive (FP) and False Negative (FN) when the algorithm incorrectly classifies
    negative samples as positive and vice versa.'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 分类有4种可能性：当算法正确分类正样本和负样本时为真正例（TP）和真负例（TN），当算法错误地将负样本分类为正样本或将正样本分类为负样本时为假正例（FP）和假负例（FN）。
- en: '|  | $Accuracy=\frac{TN+TP}{TP+TN+FP+FN}$ |  |'
  id: totrans-812
  prefs: []
  type: TYPE_TB
  zh: '|  | $Accuracy=\frac{TN+TP}{TP+TN+FP+FN}$ |  |'
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  |'
  id: totrans-813
  prefs: []
  type: TYPE_TB
  zh: '|  | $Precision=\frac{TP}{TP+FP}$ |  |'
- en: '|  | $Recall=\frac{TP}{TP+FN}$ |  |'
  id: totrans-814
  prefs: []
  type: TYPE_TB
  zh: '|  | $Recall=\frac{TP}{TP+FN}$ |  |'
- en: '|  | $F1-score=2\times\frac{Precision\times Recall}{Precision+Recall}$ |  |'
  id: totrans-815
  prefs: []
  type: TYPE_TB
  zh: '|  | $F1-score=2\times\frac{Precision\times Recall}{Precision+Recall}$ |  |'
- en: '|  | $RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})}$ |  |'
  id: totrans-816
  prefs: []
  type: TYPE_TB
  zh: '|  | $RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})}$ |  |'
- en: Let $p(r)$ be the precision-recall curve. Then,
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $p(r)$ 为精确率-召回率曲线。则，
- en: '|  | $AP=\int_{0}^{1}p(r)dr$ |  |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
  zh: '|  | $AP=\int_{0}^{1}p(r)dr$ |  |'
- en: C.3 Trajectory prediction
  id: totrans-819
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 轨迹预测
- en: C.3.1 Distance metrics
  id: totrans-820
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.3.1 距离度量
- en: '|  | $\text{Euclidean Distance}(ED)=\parallel y-\tilde{y}\parallel=\parallel
    y-\tilde{y}\parallel_{2}$ |  |'
  id: totrans-821
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Euclidean Distance}(ED)=\parallel y-\tilde{y}\parallel=\parallel
    y-\tilde{y}\parallel_{2}$ |  |'
- en: '|  | $=\sqrt{\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}}$ |  |'
  id: totrans-822
  prefs: []
  type: TYPE_TB
  zh: '|  | $=\sqrt{\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}}$ |  |'
- en: '|  | $\text{Mean Absolute Error}(MAE)=\frac{1}{n}\sum_{i=1}^{n}&#124;y_{i}-\tilde{y_{i}}&#124;$
    |  |'
  id: totrans-823
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Mean Absolute Error}(MAE)=\frac{1}{n}\sum_{i=1}^{n}&#124;y_{i}-\tilde{y_{i}}&#124;$
    |  |'
- en: '|  | $\text{Mean Square Error}(MSE)=\parallel y-\tilde{y}\parallel^{2}$ |  |'
  id: totrans-824
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Mean Square Error}(MSE)=\parallel y-\tilde{y}\parallel^{2}$ |  |'
- en: '|  | $=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}$ |  |'
  id: totrans-825
  prefs: []
  type: TYPE_TB
  zh: '|  | $=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}$ |  |'
- en: '|  | $\text{Root MSE}(RMSE)=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}}$
    |  |'
  id: totrans-826
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Root MSE}(RMSE)=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\tilde{y_{i}})^{2}}$
    |  |'
- en: '|  | $\text{Hausdorff Distance}(HD)=max_{y\in Y}min_{\tilde{y}\in\tilde{Y}}\parallel
    y-\tilde{y}\parallel$ |  |'
  id: totrans-827
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Hausdorff Distance}(HD)=max_{y\in Y}min_{\tilde{y}\in\tilde{Y}}\parallel
    y-\tilde{y}\parallel$ |  |'
- en: '|  | $\text{Modified HD}(MHD)=max(d(Y,\tilde{Y}),d(\tilde{Y},Y))$ |  |'
  id: totrans-828
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Modified HD}(MHD)=max(d(Y,\tilde{Y}),d(\tilde{Y},Y))$ |  |'
- en: '|  | $d(Y,\tilde{Y})=\frac{1}{N_{y}}\sum_{y\in Y}min_{\tilde{y}\in\tilde{Y}}\parallel
    y-\tilde{y}\parallel$ |  |'
  id: totrans-829
  prefs: []
  type: TYPE_TB
  zh: '|  | $d(Y,\tilde{Y})=\frac{1}{N_{y}}\sum_{y\in Y}min_{\tilde{y}\in\tilde{Y}}\parallel
    y-\tilde{y}\parallel$ |  |'
- en: '|  | $ADE=\frac{\sum^{N}_{i=1}\sum^{T_{pred}}_{t=1}\parallel\tilde{y}^{i}_{t}-y^{i}_{t}\parallel}{N\times
    T_{pred}}$ |  |'
  id: totrans-830
  prefs: []
  type: TYPE_TB
  zh: '|  | $ADE=\frac{\sum^{N}_{i=1}\sum^{T_{pred}}_{t=1}\parallel\tilde{y}^{i}_{t}-y^{i}_{t}\parallel}{N\times
    T_{pred}}$ |  |'
- en: where $N$ is the number of samples and $T_{pred}$ is the prediction steps.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 是样本数量，$T_{pred}$ 是预测步数。
- en: '|  | $FDE=\frac{\sum^{N}_{i=1}\parallel\tilde{y}^{i}_{T_{pred}}-y^{i}_{T_{pred}}\parallel}{N}$
    |  |'
  id: totrans-832
  prefs: []
  type: TYPE_TB
  zh: '|  | $FDE=\frac{\sum^{N}_{i=1}\parallel\tilde{y}^{i}_{T_{pred}}-y^{i}_{T_{pred}}\parallel}{N}$
    |  |'
- en: '|  | $minMSD=\mathbb{E}_{\tilde{Y}_{k}\sim q_{\theta}}min_{\tilde{y}\in\tilde{Y}_{k}}\parallel
    y-\tilde{y}\parallel^{2}$ |  |'
  id: totrans-833
  prefs: []
  type: TYPE_TB
  zh: '|  | $minMSD=\mathbb{E}_{\tilde{Y}_{k}\sim q_{\theta}}min_{\tilde{y}\in\tilde{Y}_{k}}\parallel
    y-\tilde{y}\parallel^{2}$ |  |'
- en: where $q_{\theta}$ is the sampling space and $K$ number of samples.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q_{\theta}$ 是采样空间，$K$ 是样本数量。
- en: '|  | $meanMSD=\frac{1}{K}\sum_{k=1}^{K}\parallel y-\tilde{y}\parallel^{2}$
    |  |'
  id: totrans-835
  prefs: []
  type: TYPE_TB
  zh: '|  | $meanMSD=\frac{1}{K}\sum_{k=1}^{K}\parallel y-\tilde{y}\parallel^{2}$
    |  |'
- en: '|  | $NLL=\mathbb{E}_{p(Y&#124;X)}\left[-\log\prod_{t=1}^{T_{pred}}p(y_{t}&#124;X)\right]$
    |  |'
  id: totrans-836
  prefs: []
  type: TYPE_TB
  zh: '|  | $NLL=\mathbb{E}_{p(Y&#124;X)}\left[-\log\prod_{t=1}^{T_{pred}}p(y_{t}&#124;X)\right]$
    |  |'
- en: C.4 Motion prediction
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 运动预测
- en: '|  | $MPJPE=\frac{1}{N\times T_{pred}}\sum_{t=1}^{T_{pred}}\sum_{i=1}^{N}\parallel(J_{i}^{t}-J_{root}^{t})-(\tilde{J}_{i}^{t}-\tilde{J}_{root}^{t})\parallel$
    |  |'
  id: totrans-838
  prefs: []
  type: TYPE_TB
  zh: '|  | $MPJPE=\frac{1}{N\times T_{pred}}\sum_{t=1}^{T_{pred}}\sum_{i=1}^{N}\parallel(J_{i}^{t}-J_{root}^{t})-(\tilde{J}_{i}^{t}-\tilde{J}_{root}^{t})\parallel$
    |  |'
- en: Appendix D Links to the datasets
  id: totrans-839
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 数据集链接
- en: '| Year | Dataset | Links |'
  id: totrans-840
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 数据集 | 链接 |'
- en: '| --- | --- | --- |'
  id: totrans-841
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2019 | ARGOVerse[[137](#bib.bib137)] | [https://www.argoverse.org/data.html](https://www.argoverse.org/data.html)
    |'
  id: totrans-842
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | ARGOVerse[[137](#bib.bib137)] | [https://www.argoverse.org/data.html](https://www.argoverse.org/data.html)
    |'
- en: '| CARLA[[162](#bib.bib162)] | [https://sites.google.com/view/precog](https://sites.google.com/view/precog)
    |'
  id: totrans-843
  prefs: []
  type: TYPE_TB
  zh: '| CARLA[[162](#bib.bib162)] | [https://sites.google.com/view/precog](https://sites.google.com/view/precog)
    |'
- en: '| EgoPose[[186](#bib.bib186)] | [https://github.com/Khrylx/EgoPose](https://github.com/Khrylx/EgoPose)
    |'
  id: totrans-844
  prefs: []
  type: TYPE_TB
  zh: '| EgoPose[[186](#bib.bib186)] | [https://github.com/Khrylx/EgoPose](https://github.com/Khrylx/EgoPose)
    |'
- en: '| FM[[167](#bib.bib167)] | [https://mcl.korea.ac.kr/$∼$krkim/iccv2019/index.html](https://mcl.korea.ac.kr/%24%E2%88%BC%24krkim/iccv2019/index.html)
    |'
  id: totrans-845
  prefs: []
  type: TYPE_TB
  zh: '| FM[[167](#bib.bib167)] | [https://mcl.korea.ac.kr/$∼$krkim/iccv2019/index.html](https://mcl.korea.ac.kr/%24%E2%88%BC%24krkim/iccv2019/index.html)
    |'
- en: '| InstaVariety[[240](#bib.bib240)] | [https://github.com/akanazawa/human_dynamics](https://github.com/akanazawa/human_dynamics)
    |'
  id: totrans-846
  prefs: []
  type: TYPE_TB
  zh: '| InstaVariety[[240](#bib.bib240)] | [https://github.com/akanazawa/human_dynamics](https://github.com/akanazawa/human_dynamics)
    |'
- en: '| INTEARCTION[[241](#bib.bib241)] | [https://interaction-dataset.com](https://interaction-dataset.com)
    |'
  id: totrans-847
  prefs: []
  type: TYPE_TB
  zh: '| INTEARCTION[[241](#bib.bib241)] | [https://interaction-dataset.com](https://interaction-dataset.com)
    |'
- en: '| Luggage[[81](#bib.bib81)] | [https://aashi7.github.io/NearCollision.html](https://aashi7.github.io/NearCollision.html)
    |'
  id: totrans-848
  prefs: []
  type: TYPE_TB
  zh: '| Luggage[[81](#bib.bib81)] | [https://aashi7.github.io/NearCollision.html](https://aashi7.github.io/NearCollision.html)
    |'
- en: '| MGIF[[242](#bib.bib242)] | [https://github.com/AliaksandrSiarohin/monkey-net](https://github.com/AliaksandrSiarohin/monkey-net)
    |'
  id: totrans-849
  prefs: []
  type: TYPE_TB
  zh: '| MGIF[[242](#bib.bib242)] | [https://github.com/AliaksandrSiarohin/monkey-net](https://github.com/AliaksandrSiarohin/monkey-net)
    |'
- en: '| PIE[[144](#bib.bib144)] | [http://data.nvision2.eecs.yorku.ca/PIE_dataset/](http://data.nvision2.eecs.yorku.ca/PIE_dataset/)
    |'
  id: totrans-850
  prefs: []
  type: TYPE_TB
  zh: '| PIE[[144](#bib.bib144)] | [http://data.nvision2.eecs.yorku.ca/PIE_dataset/](http://data.nvision2.eecs.yorku.ca/PIE_dataset/)
    |'
- en: '| nuScenes[[243](#bib.bib243)] | [https://www.nuscenes.org/](https://www.nuscenes.org/)
    |'
  id: totrans-851
  prefs: []
  type: TYPE_TB
  zh: '| nuScenes[[243](#bib.bib243)] | [https://www.nuscenes.org/](https://www.nuscenes.org/)
    |'
- en: '| Vehicle-Pedestrian-Mixed (VPM)[[141](#bib.bib141)] | [http://vr.ict.ac.cn/vp-lstm.](http://vr.ict.ac.cn/vp-lstm.)
    |'
  id: totrans-852
  prefs: []
  type: TYPE_TB
  zh: '| Vehicle-Pedestrian-Mixed (VPM)[[141](#bib.bib141)] | [http://vr.ict.ac.cn/vp-lstm.](http://vr.ict.ac.cn/vp-lstm.)
    |'
- en: '| TRAF[[177](#bib.bib177)] | [https://drive.google.com/drive/folders/1LqzJuRkx5yhOcjWFORO5WZ97v6jg8RHN](https://drive.google.com/drive/folders/1LqzJuRkx5yhOcjWFORO5WZ97v6jg8RHN)
    |'
  id: totrans-853
  prefs: []
  type: TYPE_TB
  zh: '| TRAF[[177](#bib.bib177)] | [https://drive.google.com/drive/folders/1LqzJuRkx5yhOcjWFORO5WZ97v6jg8RHN](https://drive.google.com/drive/folders/1LqzJuRkx5yhOcjWFORO5WZ97v6jg8RHN)
    |'
- en: '| 2018 | 3DPW[[244](#bib.bib244)] | [https://virtualhumans.mpi-inf.mpg.de/3DPW/](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
    |'
  id: totrans-854
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 3DPW[[244](#bib.bib244)] | [https://virtualhumans.mpi-inf.mpg.de/3DPW/](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
    |'
- en: '| ActEV/VIRAT[[245](#bib.bib245)] | [https://actev.nist.gov/trecvid19](https://actev.nist.gov/trecvid19)
    |'
  id: totrans-855
  prefs: []
  type: TYPE_TB
  zh: '| ActEV/VIRAT[[245](#bib.bib245)] | [https://actev.nist.gov/trecvid19](https://actev.nist.gov/trecvid19)
    |'
- en: '| ACTICIPATE[[246](#bib.bib246)] | [http://vislab.isr.tecnico.ulisboa.pt/datasets/](http://vislab.isr.tecnico.ulisboa.pt/datasets/)
    |'
  id: totrans-856
  prefs: []
  type: TYPE_TB
  zh: '| ACTICIPATE[[246](#bib.bib246)] | [http://vislab.isr.tecnico.ulisboa.pt/datasets/](http://vislab.isr.tecnico.ulisboa.pt/datasets/)
    |'
- en: '| AVA[[247](#bib.bib247)] | [https://research.google.com/ava/](https://research.google.com/ava/)
    |'
  id: totrans-857
  prefs: []
  type: TYPE_TB
  zh: '| AVA[[247](#bib.bib247)] | [https://research.google.com/ava/](https://research.google.com/ava/)
    |'
- en: '| Epic-Kitchen[[248](#bib.bib248)] | [https://epic-kitchens.github.io/2019](https://epic-kitchens.github.io/2019)
    |'
  id: totrans-858
  prefs: []
  type: TYPE_TB
  zh: '| Epic-Kitchen[[248](#bib.bib248)] | [https://epic-kitchens.github.io/2019](https://epic-kitchens.github.io/2019)
    |'
- en: '| EGTEA Gaze+[[249](#bib.bib249)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
  id: totrans-859
  prefs: []
  type: TYPE_TB
  zh: '| EGTEA Gaze+[[249](#bib.bib249)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
- en: '| STC[[223](#bib.bib223)] | [https://svip-lab.github.io/dataset/campus_dataset.html](https://svip-lab.github.io/dataset/campus_dataset.html)
    |'
  id: totrans-860
  prefs: []
  type: TYPE_TB
  zh: '| STC[[223](#bib.bib223)] | [https://svip-lab.github.io/dataset/campus_dataset.html](https://svip-lab.github.io/dataset/campus_dataset.html)
    |'
- en: '| ShapeStack[[250](#bib.bib250)] | [https://shapestacks.robots.ox.ac.uk/](https://shapestacks.robots.ox.ac.uk/)
    |'
  id: totrans-861
  prefs: []
  type: TYPE_TB
  zh: '| ShapeStack[[250](#bib.bib250)] | [https://shapestacks.robots.ox.ac.uk/](https://shapestacks.robots.ox.ac.uk/)
    |'
- en: '| VIENA[[75](#bib.bib75)] | [https://sites.google.com/view/viena2-project/home](https://sites.google.com/view/viena2-project/home)
    |'
  id: totrans-862
  prefs: []
  type: TYPE_TB
  zh: '| VIENA[[75](#bib.bib75)] | [https://sites.google.com/view/viena2-project/home](https://sites.google.com/view/viena2-project/home)
    |'
- en: '| YouCook2[[251](#bib.bib251)] | [http://youcook2.eecs.umich.edu/](http://youcook2.eecs.umich.edu/)
    |'
  id: totrans-863
  prefs: []
  type: TYPE_TB
  zh: '| YouCook2[[251](#bib.bib251)] | [http://youcook2.eecs.umich.edu/](http://youcook2.eecs.umich.edu/)
    |'
- en: '| 2017 | BUA[[252](#bib.bib252)] | [http://cs-people.bu.edu/sbargal/BU-action/](http://cs-people.bu.edu/sbargal/BU-action/)
    |'
  id: totrans-864
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | BUA[[252](#bib.bib252)] | [http://cs-people.bu.edu/sbargal/BU-action/](http://cs-people.bu.edu/sbargal/BU-action/)
    |'
- en: '| CityPerson[[253](#bib.bib253)] | [https://bitbucket.org/shanshanzhang/citypersons/src/default/](https://bitbucket.org/shanshanzhang/citypersons/src/default/)
    |'
  id: totrans-865
  prefs: []
  type: TYPE_TB
  zh: '| CityPerson[[253](#bib.bib253)] | [https://bitbucket.org/shanshanzhang/citypersons/src/default/](https://bitbucket.org/shanshanzhang/citypersons/src/default/)
    |'
- en: '| Epic-fail[[84](#bib.bib84)] | [http://aliensunmin.github.io/project/video-Forecasting/](http://aliensunmin.github.io/project/video-Forecasting/)
    |'
  id: totrans-866
  prefs: []
  type: TYPE_TB
  zh: '| Epic-fail[[84](#bib.bib84)] | [http://aliensunmin.github.io/project/video-Forecasting/](http://aliensunmin.github.io/project/video-Forecasting/)
    |'
- en: '| JAAD[[78](#bib.bib78)] | [http://data.nvision2.eecs.yorku.ca/JAAD_dataset/](http://data.nvision2.eecs.yorku.ca/JAAD_dataset/)
    |'
  id: totrans-867
  prefs: []
  type: TYPE_TB
  zh: '| JAAD[[78](#bib.bib78)] | [http://data.nvision2.eecs.yorku.ca/JAAD_dataset/](http://data.nvision2.eecs.yorku.ca/JAAD_dataset/)
    |'
- en: '| L-CAS[[254](#bib.bib254)] | [https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/](https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/)
    |'
  id: totrans-868
  prefs: []
  type: TYPE_TB
  zh: '| L-CAS[[254](#bib.bib254)] | [https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/](https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/)
    |'
- en: '| Mouse Fish [[255](#bib.bib255)] | [https://web.bii.a-star.edu.sg/archive/machine_learning/Projects/behaviorAnalysis/Lie-X/Lie-X.html](https://web.bii.a-star.edu.sg/archive/machine_learning/Projects/behaviorAnalysis/Lie-X/Lie-X.html)
    |'
  id: totrans-869
  prefs: []
  type: TYPE_TB
  zh: '| Mouse Fish [[255](#bib.bib255)] | [https://web.bii.a-star.edu.sg/archive/machine_learning/Projects/behaviorAnalysis/Lie-X/Lie-X.html](https://web.bii.a-star.edu.sg/archive/machine_learning/Projects/behaviorAnalysis/Lie-X/Lie-X.html)
    |'
- en: '| ORC[[256](#bib.bib256)] | [https://robotcar-dataset.robots.ox.ac.uk/](https://robotcar-dataset.robots.ox.ac.uk/)
    |'
  id: totrans-870
  prefs: []
  type: TYPE_TB
  zh: '| ORC[[256](#bib.bib256)] | [https://robotcar-dataset.robots.ox.ac.uk/](https://robotcar-dataset.robots.ox.ac.uk/)
    |'
- en: '| PKU-MMD[[257](#bib.bib257)] | [http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html](http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html)
    |'
  id: totrans-871
  prefs: []
  type: TYPE_TB
  zh: '| PKU-MMD[[257](#bib.bib257)] | [http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html](http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html)
    |'
- en: '| Recipe1M[[258](#bib.bib258)] | [http://pic2recipe.csail.mit.edu/](http://pic2recipe.csail.mit.edu/)
    |'
  id: totrans-872
  prefs: []
  type: TYPE_TB
  zh: '| Recipe1M[[258](#bib.bib258)] | [http://pic2recipe.csail.mit.edu/](http://pic2recipe.csail.mit.edu/)
    |'
- en: '| STRANDS[[259](#bib.bib259)] | [https://strands.readthedocs.io/en/latest/datasets/](https://strands.readthedocs.io/en/latest/datasets/)
    |'
  id: totrans-873
  prefs: []
  type: TYPE_TB
  zh: '| STRANDS[[259](#bib.bib259)] | [https://strands.readthedocs.io/en/latest/datasets/](https://strands.readthedocs.io/en/latest/datasets/)
    |'
- en: '| 2016 | BAIR Push[[29](#bib.bib29)] | [https://sites.google.com/site/brainrobotdata/home/push-dataset](https://sites.google.com/site/brainrobotdata/home/push-dataset)
    |'
  id: totrans-874
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | BAIR Push[[29](#bib.bib29)] | [https://sites.google.com/site/brainrobotdata/home/push-dataset](https://sites.google.com/site/brainrobotdata/home/push-dataset)
    |'
- en: '| BB[[260](#bib.bib260)] | [https://github.com/mbchang/dynamics](https://github.com/mbchang/dynamics)
    |'
  id: totrans-875
  prefs: []
  type: TYPE_TB
  zh: '| BB[[260](#bib.bib260)] | [https://github.com/mbchang/dynamics](https://github.com/mbchang/dynamics)
    |'
- en: '| MU[[220](#bib.bib220)] | [http://staff.itee.uq.edu.au/lovell/MissUniverse/](http://staff.itee.uq.edu.au/lovell/MissUniverse/)
    |'
  id: totrans-876
  prefs: []
  type: TYPE_TB
  zh: '| MU[[220](#bib.bib220)] | [http://staff.itee.uq.edu.au/lovell/MissUniverse/](http://staff.itee.uq.edu.au/lovell/MissUniverse/)
    |'
- en: '| Cityscapes[[261](#bib.bib261)] | [https://www.cityscapes-dataset.com/](https://www.cityscapes-dataset.com/)
    |'
  id: totrans-877
  prefs: []
  type: TYPE_TB
  zh: '| Cityscapes[[261](#bib.bib261)] | [https://www.cityscapes-dataset.com/](https://www.cityscapes-dataset.com/)
    |'
- en: '| CMU mocap[[262](#bib.bib262)] | [http://mocap.cs.cmu.edu/](http://mocap.cs.cmu.edu/)
    |'
  id: totrans-878
  prefs: []
  type: TYPE_TB
  zh: '| CMU mocap[[262](#bib.bib262)] | [http://mocap.cs.cmu.edu/](http://mocap.cs.cmu.edu/)
    |'
- en: '| DAD[[79](#bib.bib79)] | [https://aliensunmin.github.io/project/dashcam/](https://aliensunmin.github.io/project/dashcam/)
    |'
  id: totrans-879
  prefs: []
  type: TYPE_TB
  zh: '| DAD[[79](#bib.bib79)] | [https://aliensunmin.github.io/project/dashcam/](https://aliensunmin.github.io/project/dashcam/)
    |'
- en: '| NTU RGB-D[[263](#bib.bib263)] | [http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp](http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp)
    |'
  id: totrans-880
  prefs: []
  type: TYPE_TB
  zh: '| NTU RGB-D[[263](#bib.bib263)] | [http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp](http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp)
    |'
- en: '| OA[[106](#bib.bib106)] | [http://www.mpii.de/ongoing-activity](http://www.mpii.de/ongoing-activity)
    |'
  id: totrans-881
  prefs: []
  type: TYPE_TB
  zh: '| OA[[106](#bib.bib106)] | [http://www.mpii.de/ongoing-activity](http://www.mpii.de/ongoing-activity)
    |'
- en: '| OAD[[264](#bib.bib264)] | [http://www.icst.pku.edu.cn/struct/Projects/OAD.html](http://www.icst.pku.edu.cn/struct/Projects/OAD.html)
    |'
  id: totrans-882
  prefs: []
  type: TYPE_TB
  zh: '| OAD[[264](#bib.bib264)] | [http://www.icst.pku.edu.cn/struct/Projects/OAD.html](http://www.icst.pku.edu.cn/struct/Projects/OAD.html)
    |'
- en: '| SD[[265](#bib.bib265)] | [http://cvgl.stanford.edu/projects/uav_data/](http://cvgl.stanford.edu/projects/uav_data/)
    |'
  id: totrans-883
  prefs: []
  type: TYPE_TB
  zh: '| SD[[265](#bib.bib265)] | [http://cvgl.stanford.edu/projects/uav_data/](http://cvgl.stanford.edu/projects/uav_data/)
    |'
- en: '| TV Series[[266](#bib.bib266)] | [https://github.com/zhenyangli/online_action](https://github.com/zhenyangli/online_action)
    |'
  id: totrans-884
  prefs: []
  type: TYPE_TB
  zh: '| TV Series[[266](#bib.bib266)] | [https://github.com/zhenyangli/online_action](https://github.com/zhenyangli/online_action)
    |'
- en: '| VIST[[267](#bib.bib267)] | [http://visionandlanguage.net/VIST/](http://visionandlanguage.net/VIST/)
    |'
  id: totrans-885
  prefs: []
  type: TYPE_TB
  zh: '| VIST[[267](#bib.bib267)] | [http://visionandlanguage.net/VIST/](http://visionandlanguage.net/VIST/)
    |'
- en: '| Youtube-8M[[268](#bib.bib268)] | [https://research.google.com/youtube8m/](https://research.google.com/youtube8m/)
    |'
  id: totrans-886
  prefs: []
  type: TYPE_TB
  zh: '| Youtube-8M[[268](#bib.bib268)] | [https://research.google.com/youtube8m/](https://research.google.com/youtube8m/)
    |'
- en: '| 2015 | Amazon[[269](#bib.bib269)] | [http://jmcauley.ucsd.edu/data/amazon/index_2014.html](http://jmcauley.ucsd.edu/data/amazon/index_2014.html)
    |'
  id: totrans-887
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | Amazon[[269](#bib.bib269)] | [http://jmcauley.ucsd.edu/data/amazon/index_2014.html](http://jmcauley.ucsd.edu/data/amazon/index_2014.html)
    |'
- en: '| Atari[[30](#bib.bib30)] | [https://github.com/junhyukoh/nips2015-action-conditional-video-prediction](https://github.com/junhyukoh/nips2015-action-conditional-video-prediction)
    |'
  id: totrans-888
  prefs: []
  type: TYPE_TB
  zh: '| Atari[[30](#bib.bib30)] | [https://github.com/junhyukoh/nips2015-action-conditional-video-prediction](https://github.com/junhyukoh/nips2015-action-conditional-video-prediction)
    |'
- en: '| Brain4Cars[[53](#bib.bib53)] | [https://github.com/asheshjain399/ICCV2015_Brain4Cars](https://github.com/asheshjain399/ICCV2015_Brain4Cars)
    |'
  id: totrans-889
  prefs: []
  type: TYPE_TB
  zh: '| Brain4Cars[[53](#bib.bib53)] | [https://github.com/asheshjain399/ICCV2015_Brain4Cars](https://github.com/asheshjain399/ICCV2015_Brain4Cars)
    |'
- en: '| CMU Panoptic[[270](#bib.bib270)] | [http://domedb.perception.cs.cmu.edu/dataset.html](http://domedb.perception.cs.cmu.edu/dataset.html)
    |'
  id: totrans-890
  prefs: []
  type: TYPE_TB
  zh: '| CMU Panoptic[[270](#bib.bib270)] | [http://domedb.perception.cs.cmu.edu/dataset.html](http://domedb.perception.cs.cmu.edu/dataset.html)
    |'
- en: '| FPPA[[95](#bib.bib95)] | [http://bvision11.cs.unc.edu/bigpen/yipin/ICCV2015/prediction_webpage/Prediction.html](http://bvision11.cs.unc.edu/bigpen/yipin/ICCV2015/prediction_webpage/Prediction.html)
    |'
  id: totrans-891
  prefs: []
  type: TYPE_TB
  zh: '| FPPA[[95](#bib.bib95)] | [http://bvision11.cs.unc.edu/bigpen/yipin/ICCV2015/prediction_webpage/Prediction.html](http://bvision11.cs.unc.edu/bigpen/yipin/ICCV2015/prediction_webpage/Prediction.html)
    |'
- en: '| GTEA Gaze+[[271](#bib.bib271)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
  id: totrans-892
  prefs: []
  type: TYPE_TB
  zh: '| GTEA Gaze+[[271](#bib.bib271)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
- en: '| MBI-1M[[272](#bib.bib272)] | [http://academic.mywebsiteontheinternet.com/data/](http://academic.mywebsiteontheinternet.com/data/)
    |'
  id: totrans-893
  prefs: []
  type: TYPE_TB
  zh: '| MBI-1M[[272](#bib.bib272)] | [http://academic.mywebsiteontheinternet.com/data/](http://academic.mywebsiteontheinternet.com/data/)
    |'
- en: '| MOT[[273](#bib.bib273)] | [https://motchallenge.net/](https://motchallenge.net/)
    |'
  id: totrans-894
  prefs: []
  type: TYPE_TB
  zh: '| MOT[[273](#bib.bib273)] | [https://motchallenge.net/](https://motchallenge.net/)
    |'
- en: '| MMNIST[[274](#bib.bib274)] | [http://www.cs.toronto.edu/$∼$nitish/unsupervised_video/](http://www.cs.toronto.edu/%24%E2%88%BC%24nitish/unsupervised_video/)
    |'
  id: totrans-895
  prefs: []
  type: TYPE_TB
  zh: '| MMNIST[[274](#bib.bib274)] | [http://www.cs.toronto.edu/$∼$nitish/unsupervised_video/](http://www.cs.toronto.edu/%24%E2%88%BC%24nitish/unsupervised_video/)
    |'
- en: '| SUN RGB-D[[333](#bib.bib333)] | [http://rgbd.cs.princeton.edu/](http://rgbd.cs.princeton.edu/)
    |'
  id: totrans-896
  prefs: []
  type: TYPE_TB
  zh: '| SUN RGB-D[[333](#bib.bib333)] | [http://rgbd.cs.princeton.edu/](http://rgbd.cs.princeton.edu/)
    |'
- en: '| SYSU 3DHOI[[276](#bib.bib276)] | [http://www.isee-ai.cn/$∼$hujianfang/ProjectJOULE.html](http://www.isee-ai.cn/%24%E2%88%BC%24hujianfang/ProjectJOULE.html)
    |'
  id: totrans-897
  prefs: []
  type: TYPE_TB
  zh: '| SYSU 3DHOI[[276](#bib.bib276)] | [http://www.isee-ai.cn/$∼$hujianfang/ProjectJOULE.html](http://www.isee-ai.cn/%24%E2%88%BC%24hujianfang/ProjectJOULE.html)
    |'
- en: '| THUMOS[[277](#bib.bib277)] | [http://www.thumos.info/home.html](http://www.thumos.info/home.html)
    |'
  id: totrans-898
  prefs: []
  type: TYPE_TB
  zh: '| THUMOS[[277](#bib.bib277)] | [http://www.thumos.info/home.html](http://www.thumos.info/home.html)
    |'
- en: '| WnP[[278](#bib.bib278)] | [http://watchnpatch.cs.cornell.edu/](http://watchnpatch.cs.cornell.edu/)
    |'
  id: totrans-899
  prefs: []
  type: TYPE_TB
  zh: '| WnP[[278](#bib.bib278)] | [http://watchnpatch.cs.cornell.edu/](http://watchnpatch.cs.cornell.edu/)
    |'
- en: '| Wider[[279](#bib.bib279)] | [http://yjxiong.me/event_recog/WIDER/](http://yjxiong.me/event_recog/WIDER/)
    |'
  id: totrans-900
  prefs: []
  type: TYPE_TB
  zh: '| Wider[[279](#bib.bib279)] | [http://yjxiong.me/event_recog/WIDER/](http://yjxiong.me/event_recog/WIDER/)
    |'
- en: '| 2014 | Breakfast[[280](#bib.bib280)] | [http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/](http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)
    |'
  id: totrans-901
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 早餐[[280](#bib.bib280)] | [http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/](http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)
    |'
- en: '| Human3.6M[[237](#bib.bib237)] | [http://vision.imar.ro/human3.6m/description.php](http://vision.imar.ro/human3.6m/description.php)
    |'
  id: totrans-902
  prefs: []
  type: TYPE_TB
  zh: '| Human3.6M[[237](#bib.bib237)] | [http://vision.imar.ro/human3.6m/description.php](http://vision.imar.ro/human3.6m/description.php)
    |'
- en: '| MPII Human Pose[[281](#bib.bib281)] | [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)
    |'
  id: totrans-903
  prefs: []
  type: TYPE_TB
  zh: '| MPII 人体姿态[[281](#bib.bib281)] | [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)
    |'
- en: '| ORGBD[[282](#bib.bib282)] | [https://sites.google.com/site/skicyyu/orgbd](https://sites.google.com/site/skicyyu/orgbd)
    |'
  id: totrans-904
  prefs: []
  type: TYPE_TB
  zh: '| ORGBD[[282](#bib.bib282)] | [https://sites.google.com/site/skicyyu/orgbd](https://sites.google.com/site/skicyyu/orgbd)
    |'
- en: '| Sports-1M[[283](#bib.bib283)] | [https://cs.stanford.edu/people/karpathy/deepvideo/](https://cs.stanford.edu/people/karpathy/deepvideo/)
    |'
  id: totrans-905
  prefs: []
  type: TYPE_TB
  zh: '| Sports-1M[[283](#bib.bib283)] | [https://cs.stanford.edu/people/karpathy/deepvideo/](https://cs.stanford.edu/people/karpathy/deepvideo/)
    |'
- en: 'TABLE IX: A summary of datasets (from year 2014-2019) used in vision-based
    prediction papers and corresponding links.'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IX: 2014-2019 年间在基于视觉的预测论文中使用的数据集及相关链接的总结。'
- en: '| Year | Dataset | Links |'
  id: totrans-907
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 数据集 | 链接 |'
- en: '| --- | --- | --- |'
  id: totrans-908
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2013 | 50 salads[[284](#bib.bib284)] | [https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/](https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/)
    |'
  id: totrans-909
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | 50 salads[[284](#bib.bib284)] | [https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/](https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/)
    |'
- en: '| ATC [[285](#bib.bib285)] | [https://irc.atr.jp/crest2010_HRI/ATC_dataset/](https://irc.atr.jp/crest2010_HRI/ATC_dataset/)
    |'
  id: totrans-910
  prefs: []
  type: TYPE_TB
  zh: '| ATC [[285](#bib.bib285)] | [https://irc.atr.jp/crest2010_HRI/ATC_dataset/](https://irc.atr.jp/crest2010_HRI/ATC_dataset/)
    |'
- en: '| CAD-120[[286](#bib.bib286)] | [http://pr.cs.cornell.edu/humanactivities/data.php](http://pr.cs.cornell.edu/humanactivities/data.php)
    |'
  id: totrans-911
  prefs: []
  type: TYPE_TB
  zh: '| CAD-120[[286](#bib.bib286)] | [http://pr.cs.cornell.edu/humanactivities/data.php](http://pr.cs.cornell.edu/humanactivities/data.php)
    |'
- en: '| CHUK Avenue[[287](#bib.bib287)] | [http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html](http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html)
    |'
  id: totrans-912
  prefs: []
  type: TYPE_TB
  zh: '| CHUK Avenue[[287](#bib.bib287)] | [http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html](http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html)
    |'
- en: '| Daimler path[[288](#bib.bib288)] | [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Pedestrian_Path_Predict_GCPR_1/pedestrian_path_predict_gcpr_1.html](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Pedestrian_Path_Predict_GCPR_1/pedestrian_path_predict_gcpr_1.html)
    |'
  id: totrans-913
  prefs: []
  type: TYPE_TB
  zh: '| Daimler path[[288](#bib.bib288)] | [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Pedestrian_Path_Predict_GCPR_1/pedestrian_path_predict_gcpr_1.html](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Pedestrian_Path_Predict_GCPR_1/pedestrian_path_predict_gcpr_1.html)
    |'
- en: '| JHMDB[[289](#bib.bib289)] | [http://jhmdb.is.tue.mpg.de/](http://jhmdb.is.tue.mpg.de/)
    |'
  id: totrans-914
  prefs: []
  type: TYPE_TB
  zh: '| JHMDB[[289](#bib.bib289)] | [http://jhmdb.is.tue.mpg.de/](http://jhmdb.is.tue.mpg.de/)
    |'
- en: '| Penn Action[[290](#bib.bib290)] | [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)
    |'
  id: totrans-915
  prefs: []
  type: TYPE_TB
  zh: '| Penn Action[[290](#bib.bib290)] | [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)
    |'
- en: '| 2012 | BIT[[291](#bib.bib291)] | [https://sites.google.com/site/alexkongy/software](https://sites.google.com/site/alexkongy/software)
    |'
  id: totrans-916
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | BIT[[291](#bib.bib291)] | [https://sites.google.com/site/alexkongy/software](https://sites.google.com/site/alexkongy/software)
    |'
- en: '| GTEA Gaze[[292](#bib.bib292)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
  id: totrans-917
  prefs: []
  type: TYPE_TB
  zh: '| GTEA Gaze[[292](#bib.bib292)] | [http://www.cbi.gatech.edu/fpv/](http://www.cbi.gatech.edu/fpv/)
    |'
- en: '| KITTI[[293](#bib.bib293)] | [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)
    |'
  id: totrans-918
  prefs: []
  type: TYPE_TB
  zh: '| KITTI[[293](#bib.bib293)] | [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)
    |'
- en: '| MANIAC[[294](#bib.bib294)] | [https://alexandria.physik3.uni-goettingen.de/cns-group/datasets/maniac/](https://alexandria.physik3.uni-goettingen.de/cns-group/datasets/maniac/)
    |'
  id: totrans-919
  prefs: []
  type: TYPE_TB
  zh: '| MANIAC[[294](#bib.bib294)] | [https://alexandria.physik3.uni-goettingen.de/cns-group/datasets/maniac/](https://alexandria.physik3.uni-goettingen.de/cns-group/datasets/maniac/)
    |'
- en: '| MPII-cooking[[295](#bib.bib295)] | [https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset/)
    |'
  id: totrans-920
  prefs: []
  type: TYPE_TB
  zh: '| MPII-cooking[[295](#bib.bib295)] | [https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset/)
    |'
- en: '| MSRDA[[296](#bib.bib296)] | [https://documents.uow.edu.au/$∼$wanqing/#MSRAction3DDatasets](https://documents.uow.edu.au/%24%E2%88%BC%24wanqing/#MSRAction3DDatasets)
    |'
  id: totrans-921
  prefs: []
  type: TYPE_TB
  zh: '| MSRDA[[296](#bib.bib296)] | [https://documents.uow.edu.au/$∼$wanqing/#MSRAction3DDatasets](https://documents.uow.edu.au/%24%E2%88%BC%24wanqing/#MSRAction3DDatasets)
    |'
- en: '| GC[[297](#bib.bib297)] | [http://www.ee.cuhk.edu.hk/$∼$xgwang/grandcentral.html](http://www.ee.cuhk.edu.hk/%24%E2%88%BC%24xgwang/grandcentral.html)
    |'
  id: totrans-922
  prefs: []
  type: TYPE_TB
  zh: '| GC[[297](#bib.bib297)] | [http://www.ee.cuhk.edu.hk/$∼$xgwang/grandcentral.html](http://www.ee.cuhk.edu.hk/%24%E2%88%BC%24xgwang/grandcentral.html)
    |'
- en: '| SBUKI[[298](#bib.bib298)] | [https://www3.cs.stonybrook.edu/$∼$kyun/research/kinect_interaction/index.html](https://www3.cs.stonybrook.edu/%24%E2%88%BC%24kyun/research/kinect_interaction/index.html)
    |'
  id: totrans-923
  prefs: []
  type: TYPE_TB
  zh: '| SBUKI[[298](#bib.bib298)] | [https://www3.cs.stonybrook.edu/$∼$kyun/research/kinect_interaction/index.html](https://www3.cs.stonybrook.edu/%24%E2%88%BC%24kyun/research/kinect_interaction/index.html)
    |'
- en: '| UCF-101[[299](#bib.bib299)] | [https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php)
    |'
  id: totrans-924
  prefs: []
  type: TYPE_TB
  zh: '| UCF-101[[299](#bib.bib299)] | [https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php)
    |'
- en: '| UTKA[[300](#bib.bib300)] | [http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html)
    |'
  id: totrans-925
  prefs: []
  type: TYPE_TB
  zh: '| UTKA[[300](#bib.bib300)] | [http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html)
    |'
- en: '| UvA-NEMO[[301](#bib.bib301)] | [https://www.uva-nemo.org/](https://www.uva-nemo.org/)
    |'
  id: totrans-926
  prefs: []
  type: TYPE_TB
  zh: '| UvA-NEMO[[301](#bib.bib301)] | [https://www.uva-nemo.org/](https://www.uva-nemo.org/)
    |'
- en: '| 2011 | FCVL[[302](#bib.bib302)] | [http://robots.engin.umich.edu/SoftwareData/Ford](http://robots.engin.umich.edu/SoftwareData/Ford)
    |'
  id: totrans-927
  prefs: []
  type: TYPE_TB
  zh: '| 2011 | FCVL[[302](#bib.bib302)] | [http://robots.engin.umich.edu/SoftwareData/Ford](http://robots.engin.umich.edu/SoftwareData/Ford)
    |'
- en: '| HMDB[[303](#bib.bib303)] | [http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)
    |'
  id: totrans-928
  prefs: []
  type: TYPE_TB
  zh: '| HMDB[[303](#bib.bib303)] | [http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)
    |'
- en: '| Stanford 40[[304](#bib.bib304)] | [http://vision.stanford.edu/Datasets/40actions.html](http://vision.stanford.edu/Datasets/40actions.html)
    |'
  id: totrans-929
  prefs: []
  type: TYPE_TB
  zh: '| Stanford 40[[304](#bib.bib304)] | [http://vision.stanford.edu/Datasets/40actions.html](http://vision.stanford.edu/Datasets/40actions.html)
    |'
- en: '| Town Center[[305](#bib.bib305)] | [http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets](http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets)
    |'
  id: totrans-930
  prefs: []
  type: TYPE_TB
  zh: '| Town Center[[305](#bib.bib305)] | [http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets](http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets)
    |'
- en: '| VIRAT[[306](#bib.bib306)] | [http://viratdata.org/](http://viratdata.org/)
    |'
  id: totrans-931
  prefs: []
  type: TYPE_TB
  zh: '| VIRAT[[306](#bib.bib306)] | [http://viratdata.org/](http://viratdata.org/)
    |'
- en: '| 2010 | DISPLECS[[307](#bib.bib307)] | [https://cvssp.org/data/diplecs/](https://cvssp.org/data/diplecs/)
    |'
  id: totrans-932
  prefs: []
  type: TYPE_TB
  zh: '| 2010 | DISPLECS[[307](#bib.bib307)] | [https://cvssp.org/data/diplecs/](https://cvssp.org/data/diplecs/)
    |'
- en: '| MSR[[308](#bib.bib308)] | [https://www.microsoft.com/en-us/download/details.aspx?id=52315](https://www.microsoft.com/en-us/download/details.aspx?id=52315)
    |'
  id: totrans-933
  prefs: []
  type: TYPE_TB
  zh: '| MSR[[308](#bib.bib308)] | [https://www.microsoft.com/en-us/download/details.aspx?id=52315](https://www.microsoft.com/en-us/download/details.aspx?id=52315)
    |'
- en: '| MUG Facial Expression[[309](#bib.bib309)] | [https://mug.ee.auth.gr/fed/](https://mug.ee.auth.gr/fed/)
    |'
  id: totrans-934
  prefs: []
  type: TYPE_TB
  zh: '| MUG 面部表情[[309](#bib.bib309)] | [https://mug.ee.auth.gr/fed/](https://mug.ee.auth.gr/fed/)
    |'
- en: '| PROST[[310](#bib.bib310)] | [www.gpu4vision.com](www.gpu4vision.com) |'
  id: totrans-935
  prefs: []
  type: TYPE_TB
  zh: '| PROST[[310](#bib.bib310)] | [www.gpu4vision.com](www.gpu4vision.com) |'
- en: '| THI[[311](#bib.bib311)] | [http://www.robots.ox.ac.uk/$∼$alonso/tv_human_interactions.html](http://www.robots.ox.ac.uk/%24%E2%88%BC%24alonso/tv_human_interactions.html)
    |'
  id: totrans-936
  prefs: []
  type: TYPE_TB
  zh: '| THI[[311](#bib.bib311)] | [http://www.robots.ox.ac.uk/$∼$alonso/tv_human_interactions.html](http://www.robots.ox.ac.uk/%24%E2%88%BC%24alonso/tv_human_interactions.html)
    |'
- en: '| UTI[[312](#bib.bib312)] | [http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)
    |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
  zh: '| UTI[[312](#bib.bib312)] | [http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)
    |'
- en: '| VISOR[[313](#bib.bib313)] | [imagelab.ing.unimore.it/visor](imagelab.ing.unimore.it/visor)
    |'
  id: totrans-938
  prefs: []
  type: TYPE_TB
  zh: '| VISOR[[313](#bib.bib313)] | [imagelab.ing.unimore.it/visor](imagelab.ing.unimore.it/visor)
    |'
- en: '| Willow Action[[314](#bib.bib314)] | [https://www.di.ens.fr/willow/research/stillactions/](https://www.di.ens.fr/willow/research/stillactions/)
    |'
  id: totrans-939
  prefs: []
  type: TYPE_TB
  zh: '| Willow 动作[[314](#bib.bib314)] | [https://www.di.ens.fr/willow/research/stillactions/](https://www.di.ens.fr/willow/research/stillactions/)
    |'
- en: '| 2009 | Caltech Pedestrian[[315](#bib.bib315)] | [http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)
    |'
  id: totrans-940
  prefs: []
  type: TYPE_TB
  zh: '| 2009 | 加州理工学院行人数据集[[315](#bib.bib315)] | [http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)
    |'
- en: '| Collective Activity (CA)[[316](#bib.bib316)] | [http://www-personal.umich.edu/$∼$wgchoi/eccv12/wongun_eccv12.html](http://www-personal.umich.edu/%24%E2%88%BC%24wgchoi/eccv12/wongun_eccv12.html)
    |'
  id: totrans-941
  prefs: []
  type: TYPE_TB
  zh: '| 集体活动 (CA)[[316](#bib.bib316)] | [http://www-personal.umich.edu/$∼$wgchoi/eccv12/wongun_eccv12.html](http://www-personal.umich.edu/%24%E2%88%BC%24wgchoi/eccv12/wongun_eccv12.html)
    |'
- en: '| EIFP[[317](#bib.bib317)] | [http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/](http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/)
    |'
  id: totrans-942
  prefs: []
  type: TYPE_TB
  zh: '| EIFP[[317](#bib.bib317)] | [http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/](http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/)
    |'
- en: '| ETH[[235](#bib.bib235)] | [http://www.vision.ee.ethz.ch/en/datasets/](http://www.vision.ee.ethz.ch/en/datasets/)
    |'
  id: totrans-943
  prefs: []
  type: TYPE_TB
  zh: '| ETH[[235](#bib.bib235)] | [http://www.vision.ee.ethz.ch/en/datasets/](http://www.vision.ee.ethz.ch/en/datasets/)
    |'
- en: '| OSU[[318](#bib.bib318)] | [http://eecs.oregonstate.edu/football/tracking/dataset](http://eecs.oregonstate.edu/football/tracking/dataset)
    |'
  id: totrans-944
  prefs: []
  type: TYPE_TB
  zh: '| OSU[[318](#bib.bib318)] | [http://eecs.oregonstate.edu/football/tracking/dataset](http://eecs.oregonstate.edu/football/tracking/dataset)
    |'
- en: '| PETS2009[[319](#bib.bib319)] | [http://www.cvg.reading.ac.uk/PETS2009/a.html](http://www.cvg.reading.ac.uk/PETS2009/a.html)
    |'
  id: totrans-945
  prefs: []
  type: TYPE_TB
  zh: '| PETS2009[[319](#bib.bib319)] | [http://www.cvg.reading.ac.uk/PETS2009/a.html](http://www.cvg.reading.ac.uk/PETS2009/a.html)
    |'
- en: '| QMUL[[320](#bib.bib320)] | [http://personal.ie.cuhk.edu.hk/$∼$ccloy/downloads_qmul_junction.html](http://personal.ie.cuhk.edu.hk/%24%E2%88%BC%24ccloy/downloads_qmul_junction.html)
    |'
  id: totrans-946
  prefs: []
  type: TYPE_TB
  zh: '| QMUL[[320](#bib.bib320)] | [http://personal.ie.cuhk.edu.hk/$∼$ccloy/downloads_qmul_junction.html](http://personal.ie.cuhk.edu.hk/%24%E2%88%BC%24ccloy/downloads_qmul_junction.html)
    |'
- en: '| TUM Kitchen[[321](#bib.bib321)] | [https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data](https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data)
    |'
  id: totrans-947
  prefs: []
  type: TYPE_TB
  zh: '| TUM 厨房[[321](#bib.bib321)] | [https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data](https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data)
    |'
- en: '| YUV Videos[[322](#bib.bib322)] | [http://trace.kom.aau.dk/yuv/index.html](http://trace.kom.aau.dk/yuv/index.html)
    |'
  id: totrans-948
  prefs: []
  type: TYPE_TB
  zh: '| YUV 视频[[322](#bib.bib322)] | [http://trace.kom.aau.dk/yuv/index.html](http://trace.kom.aau.dk/yuv/index.html)
    |'
- en: '| 2008 | Daimler[[323](#bib.bib323)] | [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html)
    |'
  id: totrans-949
  prefs: []
  type: TYPE_TB
  zh: '| 2008 | 达姆勒[[323](#bib.bib323)] | [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html)
    |'
- en: '| MITT[[324](#bib.bib324)] | [http://www.ee.cuhk.edu.hk/$∼$xgwang/MITtrajsingle.html](http://www.ee.cuhk.edu.hk/%24%E2%88%BC%24xgwang/MITtrajsingle.html)
    |'
  id: totrans-950
  prefs: []
  type: TYPE_TB
  zh: '| MITT[[324](#bib.bib324)] | [http://www.ee.cuhk.edu.hk/$∼$xgwang/MITtrajsingle.html](http://www.ee.cuhk.edu.hk/%24%E2%88%BC%24xgwang/MITtrajsingle.html)
    |'
- en: '| 2007 | AMOS[[325](#bib.bib325)] | [http://amos.cse.wustl.edu/](http://amos.cse.wustl.edu/)
    |'
  id: totrans-951
  prefs: []
  type: TYPE_TB
  zh: '| 2007 | AMOS [[325](#bib.bib325)] | [http://amos.cse.wustl.edu/](http://amos.cse.wustl.edu/)
    |'
- en: '| ETH pedestrian[[326](#bib.bib326)] | [https://data.vision.ee.ethz.ch/cvl/aess/](https://data.vision.ee.ethz.ch/cvl/aess/)
    |'
  id: totrans-952
  prefs: []
  type: TYPE_TB
  zh: '| ETH 行人 [[326](#bib.bib326)] | [https://data.vision.ee.ethz.ch/cvl/aess/](https://data.vision.ee.ethz.ch/cvl/aess/)
    |'
- en: '| Lankershim Boulevard[[327](#bib.bib327)] | [https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm](https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm)
    |'
  id: totrans-953
  prefs: []
  type: TYPE_TB
  zh: '| 兰克什姆大道 [[327](#bib.bib327)] | [https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm](https://www.fhwa.dot.gov/publications/research/operations/07029/index.cfm)
    |'
- en: '| NGSIM[[328](#bib.bib328)] | [https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm](https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm)
    |'
  id: totrans-954
  prefs: []
  type: TYPE_TB
  zh: '| NGSIM [[328](#bib.bib328)] | [https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm](https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm)
    |'
- en: '| UCY[[329](#bib.bib329)] | [https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data](https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data)
    |'
  id: totrans-955
  prefs: []
  type: TYPE_TB
  zh: '| UCY [[329](#bib.bib329)] | [https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data](https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data)
    |'
- en: '| 2006 | Tuscan Arizona[[330](#bib.bib330)] | [http://www.mmto.org/](http://www.mmto.org/)
    |'
  id: totrans-956
  prefs: []
  type: TYPE_TB
  zh: '| 2006 | 托斯坎亚利桑那 [[330](#bib.bib330)] | [http://www.mmto.org/](http://www.mmto.org/)
    |'
- en: '| 2004 | KTH[[331](#bib.bib331)] | [http://www.nada.kth.se/cvap/actions/](http://www.nada.kth.se/cvap/actions/)
    |'
  id: totrans-957
  prefs: []
  type: TYPE_TB
  zh: '| 2004 | KTH [[331](#bib.bib331)] | [http://www.nada.kth.se/cvap/actions/](http://www.nada.kth.se/cvap/actions/)
    |'
- en: '| 1981 | Golden Colorado[[332](#bib.bib332)] | [https://www.osti.gov/dataexplorer/biblio/dataset/1052221](https://www.osti.gov/dataexplorer/biblio/dataset/1052221)
    |'
  id: totrans-958
  prefs: []
  type: TYPE_TB
  zh: '| 1981 | 黄金科罗拉多 [[332](#bib.bib332)] | [https://www.osti.gov/dataexplorer/biblio/dataset/1052221](https://www.osti.gov/dataexplorer/biblio/dataset/1052221)
    |'
- en: 'TABLE X: A summary of datasets (from year 2013 and earlier) used in vision-based
    prediction papers and corresponding links.'
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 表 X：用于基于视觉预测论文的数据集汇总（从2013年及以前）及相关链接。
- en: 'Lists of datasets with associated repository links can be found in Tables [IX](#A4.T9
    "TABLE IX ‣ Appendix D Links to the datasets ‣ Deep Learning for Vision-based
    Prediction: A Survey") and [X](#A4.T10 "TABLE X ‣ Appendix D Links to the datasets
    ‣ Deep Learning for Vision-based Prediction: A Survey").'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 带有相关存储库链接的数据集列表可以在表 [IX](#A4.T9 "TABLE IX ‣ 附录 D 数据集链接 ‣ 基于视觉预测的深度学习：一项调查")
    和 [X](#A4.T10 "TABLE X ‣ 附录 D 数据集链接 ‣ 基于视觉预测的深度学习：一项调查") 中找到。
- en: Appendix E Datasets and corresponding papers
  id: totrans-961
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 数据集及相关论文
- en: 'Lists of datasets and corresponding papers can be found in Tables [XI](#A5.T11
    "TABLE XI ‣ Appendix E Datasets and corresponding papers ‣ Deep Learning for Vision-based
    Prediction: A Survey"), [XII](#A5.T12 "TABLE XII ‣ Appendix E Datasets and corresponding
    papers ‣ Deep Learning for Vision-based Prediction: A Survey"), [XIII](#A5.T13
    "TABLE XIII ‣ Appendix E Datasets and corresponding papers ‣ Deep Learning for
    Vision-based Prediction: A Survey"), [XIV](#A5.T14 "TABLE XIV ‣ Appendix E Datasets
    and corresponding papers ‣ Deep Learning for Vision-based Prediction: A Survey"),
    and [XV](#A5.T15 "TABLE XV ‣ Appendix E Datasets and corresponding papers ‣ Deep
    Learning for Vision-based Prediction: A Survey").'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集及相关论文的列表可以在表 [XI](#A5.T11 "TABLE XI ‣ 附录 E 数据集及相关论文 ‣ 基于视觉预测的深度学习：一项调查")、[XII](#A5.T12
    "TABLE XII ‣ 附录 E 数据集及相关论文 ‣ 基于视觉预测的深度学习：一项调查")、[XIII](#A5.T13 "TABLE XIII ‣ 附录
    E 数据集及相关论文 ‣ 基于视觉预测的深度学习：一项调查")、[XIV](#A5.T14 "TABLE XIV ‣ 附录 E 数据集及相关论文 ‣ 基于视觉预测的深度学习：一项调查")
    和 [XV](#A5.T15 "TABLE XV ‣ 附录 E 数据集及相关论文 ‣ 基于视觉预测的深度学习：一项调查") 中找到。
- en: '| Datasets | Papers |'
  id: totrans-963
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 论文 |'
- en: '| --- | --- |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Atari | [[30](#bib.bib30)] |'
  id: totrans-965
  prefs: []
  type: TYPE_TB
  zh: '| Atari | [[30](#bib.bib30)] |'
- en: '| BAIR Push | [[3](#bib.bib3)] ,[[24](#bib.bib24)],[[29](#bib.bib29)] |'
  id: totrans-966
  prefs: []
  type: TYPE_TB
  zh: '| BAIR 推送 | [[3](#bib.bib3)], [[24](#bib.bib24)], [[29](#bib.bib29)] |'
- en: '| Bouncing Ball | [[23](#bib.bib23)] |'
  id: totrans-967
  prefs: []
  type: TYPE_TB
  zh: '| 弹跳球 | [[23](#bib.bib23)] |'
- en: '| CHUK Avenue | [[2](#bib.bib2)] |'
  id: totrans-968
  prefs: []
  type: TYPE_TB
  zh: '| CHUK Avenue | [[2](#bib.bib2)] |'
- en: '| Caltech Pedestrian | [[20](#bib.bib20)],[[33](#bib.bib33)] , [[2](#bib.bib2)],[[1](#bib.bib1)],[[34](#bib.bib34)],[[12](#bib.bib12)],[[6](#bib.bib6)],
    [[4](#bib.bib4)] |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
  zh: '| 加州理工学院行人 | [[20](#bib.bib20)], [[33](#bib.bib33)], [[2](#bib.bib2)], [[1](#bib.bib1)],
    [[34](#bib.bib34)], [[12](#bib.bib12)], [[6](#bib.bib6)], [[4](#bib.bib4)] |'
- en: '| Cityscapes | [[3](#bib.bib3)],[[32](#bib.bib32)] |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
  zh: '| Cityscapes | [[3](#bib.bib3)], [[32](#bib.bib32)] |'
- en: '| Human 3.6M | [[32](#bib.bib32)],[[33](#bib.bib33)],[[22](#bib.bib22)],[[24](#bib.bib24)],[[14](#bib.bib14)],[[25](#bib.bib25)],[[7](#bib.bib7)],[[28](#bib.bib28)],[[29](#bib.bib29)]
    |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
  zh: '| Human 3.6M | [[32](#bib.bib32)], [[33](#bib.bib33)], [[22](#bib.bib22)],
    [[24](#bib.bib24)], [[14](#bib.bib14)], [[25](#bib.bib25)], [[7](#bib.bib7)],
    [[28](#bib.bib28)], [[29](#bib.bib29)] |'
- en: '| JAAD | [[9](#bib.bib9)] |'
  id: totrans-972
  prefs: []
  type: TYPE_TB
  zh: '| JAAD | [[9](#bib.bib9)] |'
- en: '| JHMDB | [[21](#bib.bib21)] |'
  id: totrans-973
  prefs: []
  type: TYPE_TB
  zh: '| JHMDB | [[21](#bib.bib21)] |'
- en: '| KITTI | [[2](#bib.bib2)],[[1](#bib.bib1)],[[20](#bib.bib20)],[[33](#bib.bib33)],[[34](#bib.bib34)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[6](#bib.bib6)],[[15](#bib.bib15)]
    |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
  zh: '| KITTI | [[2](#bib.bib2)],[[1](#bib.bib1)],[[20](#bib.bib20)],[[33](#bib.bib33)],[[34](#bib.bib34)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[35](#bib.bib35)],[[6](#bib.bib6)],[[15](#bib.bib15)]
    |'
- en: '| KTH | [[31](#bib.bib31)],[[19](#bib.bib19)],[[11](#bib.bib11)],[[16](#bib.bib16)],[[13](#bib.bib13)],[[35](#bib.bib35)],[[27](#bib.bib27)]
    |'
  id: totrans-975
  prefs: []
  type: TYPE_TB
  zh: '| KTH | [[31](#bib.bib31)],[[19](#bib.bib19)],[[11](#bib.bib11)],[[16](#bib.bib16)],[[13](#bib.bib13)],[[35](#bib.bib35)],[[27](#bib.bib27)]
    |'
- en: '| MGIF | [[18](#bib.bib18)] |'
  id: totrans-976
  prefs: []
  type: TYPE_TB
  zh: '| MGIF | [[18](#bib.bib18)] |'
- en: '| MMNIST | [[3](#bib.bib3)],[[31](#bib.bib31)],[[19](#bib.bib19)],[[16](#bib.bib16)],[[23](#bib.bib23)],[[24](#bib.bib24)],[[36](#bib.bib36)],[[8](#bib.bib8)],[[27](#bib.bib27)]
    |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
  zh: '| MMNIST | [[3](#bib.bib3)],[[31](#bib.bib31)],[[19](#bib.bib19)],[[16](#bib.bib16)],[[23](#bib.bib23)],[[24](#bib.bib24)],[[36](#bib.bib36)],[[8](#bib.bib8)],[[27](#bib.bib27)]
    |'
- en: '| MSR | [[19](#bib.bib19)] |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
  zh: '| MSR | [[19](#bib.bib19)] |'
- en: '| MUG | [[229](#bib.bib229)] |'
  id: totrans-979
  prefs: []
  type: TYPE_TB
  zh: '| MUG | [[229](#bib.bib229)] |'
- en: '| Own | [[37](#bib.bib37)],[[25](#bib.bib25)] |'
  id: totrans-980
  prefs: []
  type: TYPE_TB
  zh: '| Own | [[37](#bib.bib37)],[[25](#bib.bib25)] |'
- en: '| PROST | [[36](#bib.bib36)] |'
  id: totrans-981
  prefs: []
  type: TYPE_TB
  zh: '| PROST | [[36](#bib.bib36)] |'
- en: '| Penn Action | [[21](#bib.bib21)],[[229](#bib.bib229)],[[26](#bib.bib26)]
    |'
  id: totrans-982
  prefs: []
  type: TYPE_TB
  zh: '| Penn Action | [[21](#bib.bib21)],[[229](#bib.bib229)],[[26](#bib.bib26)]
    |'
- en: '| Penn action | [[17](#bib.bib17)],[[18](#bib.bib18)],[[28](#bib.bib28)] |'
  id: totrans-983
  prefs: []
  type: TYPE_TB
  zh: '| Penn action | [[17](#bib.bib17)],[[18](#bib.bib18)],[[28](#bib.bib28)] |'
- en: '| ShanghaiTech Campus | [[2](#bib.bib2)] |'
  id: totrans-984
  prefs: []
  type: TYPE_TB
  zh: '| ShanghaiTech Campus | [[2](#bib.bib2)] |'
- en: '| ShapeStack | [[17](#bib.bib17)] |'
  id: totrans-985
  prefs: []
  type: TYPE_TB
  zh: '| ShapeStack | [[17](#bib.bib17)] |'
- en: '| Sports-1M | [[36](#bib.bib36)],[[15](#bib.bib15)] |'
  id: totrans-986
  prefs: []
  type: TYPE_TB
  zh: '| Sports-1M | [[36](#bib.bib36)],[[15](#bib.bib15)] |'
- en: '| THUMOS | [[6](#bib.bib6)] |'
  id: totrans-987
  prefs: []
  type: TYPE_TB
  zh: '| THUMOS | [[6](#bib.bib6)] |'
- en: '| UCF-101 | [[2](#bib.bib2)],[[4](#bib.bib4)],[[5](#bib.bib5)],[[32](#bib.bib32)],[[33](#bib.bib33)],[[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[36](#bib.bib36)],[[6](#bib.bib6)],[[26](#bib.bib26)],[[15](#bib.bib15)]
    |'
  id: totrans-988
  prefs: []
  type: TYPE_TB
  zh: '| UCF-101 | [[2](#bib.bib2)],[[4](#bib.bib4)],[[5](#bib.bib5)],[[32](#bib.bib32)],[[33](#bib.bib33)],[[22](#bib.bib22)],[[34](#bib.bib34)],[[16](#bib.bib16)],[[13](#bib.bib13)],[[14](#bib.bib14)],[[36](#bib.bib36)],[[6](#bib.bib6)],[[26](#bib.bib26)],[[15](#bib.bib15)]
    |'
- en: '| UvA-NEMO | [[18](#bib.bib18)] |'
  id: totrans-989
  prefs: []
  type: TYPE_TB
  zh: '| UvA-NEMO | [[18](#bib.bib18)] |'
- en: '| ViSOR | [[36](#bib.bib36)] |'
  id: totrans-990
  prefs: []
  type: TYPE_TB
  zh: '| ViSOR | [[36](#bib.bib36)] |'
- en: '| YUV | [[4](#bib.bib4)],[[20](#bib.bib20)] |'
  id: totrans-991
  prefs: []
  type: TYPE_TB
  zh: '| YUV | [[4](#bib.bib4)],[[20](#bib.bib20)] |'
- en: '| Youtube-8M | [[12](#bib.bib12)] |'
  id: totrans-992
  prefs: []
  type: TYPE_TB
  zh: '| Youtube-8M | [[12](#bib.bib12)] |'
- en: '| pedestrian | [[4](#bib.bib4)] |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '| pedestrian | [[4](#bib.bib4)] |'
- en: 'TABLE XI: Datasets used in video prediction applications.'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XI: 用于视频预测应用的数据集。'
- en: '| Datasets | Papers |'
  id: totrans-995
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 论文 |'
- en: '| --- | --- |'
  id: totrans-996
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 50Salad | [[63](#bib.bib63)],[[66](#bib.bib66)],[[69](#bib.bib69)] |'
  id: totrans-997
  prefs: []
  type: TYPE_TB
  zh: '| 50Salad | [[63](#bib.bib63)],[[66](#bib.bib66)],[[69](#bib.bib69)] |'
- en: '| ANTICIPATE | [[90](#bib.bib90)] |'
  id: totrans-998
  prefs: []
  type: TYPE_TB
  zh: '| ANTICIPATE | [[90](#bib.bib90)] |'
- en: '| AVA | [[88](#bib.bib88)],[[88](#bib.bib88)] |'
  id: totrans-999
  prefs: []
  type: TYPE_TB
  zh: '| AVA | [[88](#bib.bib88)],[[88](#bib.bib88)] |'
- en: '| ActEV/VIRAT | [[87](#bib.bib87)] |'
  id: totrans-1000
  prefs: []
  type: TYPE_TB
  zh: '| ActEV/VIRAT | [[87](#bib.bib87)] |'
- en: '| BIT | [[100](#bib.bib100)],[[109](#bib.bib109)],[[111](#bib.bib111)],[[113](#bib.bib113)]
    |'
  id: totrans-1001
  prefs: []
  type: TYPE_TB
  zh: '| BIT | [[100](#bib.bib100)],[[109](#bib.bib109)],[[111](#bib.bib111)],[[113](#bib.bib113)]
    |'
- en: '| BU Action | [[107](#bib.bib107)] |'
  id: totrans-1002
  prefs: []
  type: TYPE_TB
  zh: '| BU Action | [[107](#bib.bib107)] |'
- en: '| Brain4Cars | [[96](#bib.bib96)],[[80](#bib.bib80)],[[53](#bib.bib53)] |'
  id: totrans-1003
  prefs: []
  type: TYPE_TB
  zh: '| Brain4Cars | [[96](#bib.bib96)],[[80](#bib.bib80)],[[53](#bib.bib53)] |'
- en: '| Breakfast | [[66](#bib.bib66)],[[67](#bib.bib67)],[[69](#bib.bib69)] |'
  id: totrans-1004
  prefs: []
  type: TYPE_TB
  zh: '| Breakfast | [[66](#bib.bib66)],[[67](#bib.bib67)],[[69](#bib.bib69)] |'
- en: '| CA | [[101](#bib.bib101)] |'
  id: totrans-1005
  prefs: []
  type: TYPE_TB
  zh: '| CA | [[101](#bib.bib101)] |'
- en: '| CAD-120 | [[67](#bib.bib67)],[[90](#bib.bib90)],[[58](#bib.bib58)],[[96](#bib.bib96)],[[52](#bib.bib52)]
    |'
  id: totrans-1006
  prefs: []
  type: TYPE_TB
  zh: '| CAD-120 | [[67](#bib.bib67)],[[90](#bib.bib90)],[[58](#bib.bib58)],[[96](#bib.bib96)],[[52](#bib.bib52)]
    |'
- en: '| CMU Panoptic | [[56](#bib.bib56)] |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
  zh: '| CMU Panoptic | [[56](#bib.bib56)] |'
- en: '| CMU Mocap | [[110](#bib.bib110)] |'
  id: totrans-1008
  prefs: []
  type: TYPE_TB
  zh: '| CMU Mocap | [[110](#bib.bib110)] |'
- en: '| Caltech Pedestrian | [[54](#bib.bib54)] |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
  zh: '| Caltech Pedestrian | [[54](#bib.bib54)] |'
- en: '| DAD | [[83](#bib.bib83)],[[84](#bib.bib84)] |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
  zh: '| DAD | [[83](#bib.bib83)],[[84](#bib.bib84)] |'
- en: '| Daimler | [[54](#bib.bib54)] |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
  zh: '| Daimler | [[54](#bib.bib54)] |'
- en: '| Daimler Path | [[40](#bib.bib40)] |'
  id: totrans-1012
  prefs: []
  type: TYPE_TB
  zh: '| Daimler Path | [[40](#bib.bib40)] |'
- en: '| EGTEA Gaze+ | [[64](#bib.bib64)] |'
  id: totrans-1013
  prefs: []
  type: TYPE_TB
  zh: '| EGTEA Gaze+ | [[64](#bib.bib64)] |'
- en: '| ETH Pedestrian | [[54](#bib.bib54)] |'
  id: totrans-1014
  prefs: []
  type: TYPE_TB
  zh: '| ETH Pedestrian | [[54](#bib.bib54)] |'
- en: '| Epic-fail | [[84](#bib.bib84)] |'
  id: totrans-1015
  prefs: []
  type: TYPE_TB
  zh: '| Epic-fail | [[84](#bib.bib84)] |'
- en: '| Epic-Kitchen | [[63](#bib.bib63)] ,[[64](#bib.bib64)],[[68](#bib.bib68)]
    |'
  id: totrans-1016
  prefs: []
  type: TYPE_TB
  zh: '| Epic-Kitchen | [[63](#bib.bib63)] ,[[64](#bib.bib64)],[[68](#bib.bib68)]
    |'
- en: '| FPPA | [[95](#bib.bib95)] |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
  zh: '| FPPA | [[95](#bib.bib95)] |'
- en: '| GTEA Gaze | [[89](#bib.bib89)] |'
  id: totrans-1018
  prefs: []
  type: TYPE_TB
  zh: '| GTEA Gaze | [[89](#bib.bib89)] |'
- en: '| GTEA Gaze+ | [[89](#bib.bib89)] |'
  id: totrans-1019
  prefs: []
  type: TYPE_TB
  zh: '| GTEA Gaze+ | [[89](#bib.bib89)] |'
- en: '| HMDB | [[104](#bib.bib104)] |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '| HMDB | [[104](#bib.bib104)] |'
- en: '| Human 3.6M | [[110](#bib.bib110)] |'
  id: totrans-1021
  prefs: []
  type: TYPE_TB
  zh: '| Human 3.6M | [[110](#bib.bib110)] |'
- en: '| JAAD | [[9](#bib.bib9)],[[73](#bib.bib73)],[[75](#bib.bib75)],[[78](#bib.bib78)]
    |'
  id: totrans-1022
  prefs: []
  type: TYPE_TB
  zh: '| JAAD | [[9](#bib.bib9)],[[73](#bib.bib73)],[[75](#bib.bib75)],[[78](#bib.bib78)]
    |'
- en: '| JHMDB | [[88](#bib.bib88)],[[88](#bib.bib88)],[[100](#bib.bib100)],[[102](#bib.bib102)],[[105](#bib.bib105)],[[112](#bib.bib112)]
    |'
  id: totrans-1023
  prefs: []
  type: TYPE_TB
  zh: '| JHMDB | [[88](#bib.bib88)],[[88](#bib.bib88)],[[100](#bib.bib100)],[[102](#bib.bib102)],[[105](#bib.bib105)],[[112](#bib.bib112)]
    |'
- en: '| Luggage | [[81](#bib.bib81)] |'
  id: totrans-1024
  prefs: []
  type: TYPE_TB
  zh: '| Luggage | [[81](#bib.bib81)] |'
- en: '| MANIAC | [[57](#bib.bib57)] |'
  id: totrans-1025
  prefs: []
  type: TYPE_TB
  zh: '| MANIAC | [[57](#bib.bib57)] |'
- en: '| MPII Cooking | [[67](#bib.bib67)],[[70](#bib.bib70)],[[60](#bib.bib60)] |'
  id: totrans-1026
  prefs: []
  type: TYPE_TB
  zh: '| MPII Cooking | [[67](#bib.bib67)],[[70](#bib.bib70)],[[60](#bib.bib60)] |'
- en: '| MSRDA | [[45](#bib.bib45)] |'
  id: totrans-1027
  prefs: []
  type: TYPE_TB
  zh: '| MSRDA | [[45](#bib.bib45)] |'
- en: '| NGSIM | [[72](#bib.bib72)],[[74](#bib.bib74)],[[97](#bib.bib97)] |'
  id: totrans-1028
  prefs: []
  type: TYPE_TB
  zh: '| NGSIM | [[72](#bib.bib72)],[[74](#bib.bib74)],[[97](#bib.bib97)] |'
- en: '| NTU RGB-D | [[98](#bib.bib98)] |'
  id: totrans-1029
  prefs: []
  type: TYPE_TB
  zh: '| NTU RGB-D | [[98](#bib.bib98)] |'
- en: '| OA | [[106](#bib.bib106)] |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
  zh: '| OA | [[106](#bib.bib106)] |'
- en: '| OAD | [[108](#bib.bib108)] |'
  id: totrans-1031
  prefs: []
  type: TYPE_TB
  zh: '| OAD | [[108](#bib.bib108)] |'
- en: '| ORGBD | [[41](#bib.bib41)] |'
  id: totrans-1032
  prefs: []
  type: TYPE_TB
  zh: '| ORGBD | [[41](#bib.bib41)] |'
- en: '| PIE | [[71](#bib.bib71)] |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
  zh: '| PIE | [[71](#bib.bib71)] |'
- en: '| PKU-MMD | [[108](#bib.bib108)] |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
  zh: '| PKU-MMD | [[108](#bib.bib108)] |'
- en: '| Recipe1M | [[65](#bib.bib65)] |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
  zh: '| Recipe1M | [[65](#bib.bib65)] |'
- en: '| SBUIK | [[101](#bib.bib101)] |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
  zh: '| SBUIK | [[101](#bib.bib101)] |'
- en: '| SYSU 3DHOI | [[98](#bib.bib98)],[[41](#bib.bib41)] |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '| SYSU 3DHOI | [[98](#bib.bib98)],[[41](#bib.bib41)] |'
- en: '| Stanford-40 | [[107](#bib.bib107)] |'
  id: totrans-1038
  prefs: []
  type: TYPE_TB
  zh: '| Stanford-40 | [[107](#bib.bib107)] |'
- en: '| THUMOS | [[91](#bib.bib91)],[[92](#bib.bib92)],[[93](#bib.bib93)] |'
  id: totrans-1039
  prefs: []
  type: TYPE_TB
  zh: '| THUMOS | [[91](#bib.bib91)],[[92](#bib.bib92)],[[93](#bib.bib93)] |'
- en: '| TV Human Interaction | [[99](#bib.bib99)] , [[91](#bib.bib91)],[[8](#bib.bib8)],[[92](#bib.bib92)],[[93](#bib.bib93)]
    |'
  id: totrans-1040
  prefs: []
  type: TYPE_TB
  zh: '| TV Human Interaction | [[99](#bib.bib99)] , [[91](#bib.bib91)],[[8](#bib.bib8)],[[92](#bib.bib92)],[[93](#bib.bib93)]
    |'
- en: '| TV Series | [[92](#bib.bib92)] |'
  id: totrans-1041
  prefs: []
  type: TYPE_TB
  zh: '| TV Series | [[92](#bib.bib92)] |'
- en: '| UCF-101 | [[98](#bib.bib98)],[[99](#bib.bib99)],[[100](#bib.bib100)],[[107](#bib.bib107)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[104](#bib.bib104)],[[111](#bib.bib111)],[[105](#bib.bib105)],[[112](#bib.bib112)],[[61](#bib.bib61)]
    |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '| UCF-101 | [[98](#bib.bib98)],[[99](#bib.bib99)],[[100](#bib.bib100)],[[107](#bib.bib107)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[104](#bib.bib104)],[[111](#bib.bib111)],[[105](#bib.bib105)],[[112](#bib.bib112)],[[61](#bib.bib61)]
    |'
- en: '| UTI | [[99](#bib.bib99)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[105](#bib.bib105)],[[61](#bib.bib61)],[[113](#bib.bib113)],[[44](#bib.bib44)]
    |'
  id: totrans-1043
  prefs: []
  type: TYPE_TB
  zh: '| UTI | [[99](#bib.bib99)],[[109](#bib.bib109)],[[102](#bib.bib102)],[[105](#bib.bib105)],[[61](#bib.bib61)],[[113](#bib.bib113)],[[44](#bib.bib44)]
    |'
- en: '| UTKA | [[94](#bib.bib94)] |'
  id: totrans-1044
  prefs: []
  type: TYPE_TB
  zh: '| UTKA | [[94](#bib.bib94)] |'
- en: '| VIENA | [[75](#bib.bib75)] |'
  id: totrans-1045
  prefs: []
  type: TYPE_TB
  zh: '| VIENA | [[75](#bib.bib75)] |'
- en: '| VIRAT | [[70](#bib.bib70)] |'
  id: totrans-1046
  prefs: []
  type: TYPE_TB
  zh: '| VIRAT | [[70](#bib.bib70)] |'
- en: '| WIDER | [[107](#bib.bib107)] |'
  id: totrans-1047
  prefs: []
  type: TYPE_TB
  zh: '| WIDER | [[107](#bib.bib107)] |'
- en: '| Willow Action | [[107](#bib.bib107)] |'
  id: totrans-1048
  prefs: []
  type: TYPE_TB
  zh: '| Willow Action | [[107](#bib.bib107)] |'
- en: '| WnP | [[94](#bib.bib94)] |'
  id: totrans-1049
  prefs: []
  type: TYPE_TB
  zh: '| WnP | [[94](#bib.bib94)] |'
- en: '| YouCook2 | [[65](#bib.bib65)] |'
  id: totrans-1050
  prefs: []
  type: TYPE_TB
  zh: '| YouCook2 | [[65](#bib.bib65)] |'
- en: '| Sports-1M | [[111](#bib.bib111)] |'
  id: totrans-1051
  prefs: []
  type: TYPE_TB
  zh: '| Sports-1M | [[111](#bib.bib111)] |'
- en: 'TABLE XII: Datasets used in action prediction applications.'
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE XII: 用于动作预测应用的数据集。'
- en: '| Datasets | Papers |'
  id: totrans-1053
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 论文 |'
- en: '| --- | --- |'
  id: totrans-1054
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ARGOVerse | [[137](#bib.bib137)] |'
  id: totrans-1055
  prefs: []
  type: TYPE_TB
  zh: '| ARGOVerse | [[137](#bib.bib137)] |'
- en: '| ATC | [[118](#bib.bib118)] |'
  id: totrans-1056
  prefs: []
  type: TYPE_TB
  zh: '| ATC | [[118](#bib.bib118)] |'
- en: '| ActEV/VIRAT | [[87](#bib.bib87)] |'
  id: totrans-1057
  prefs: []
  type: TYPE_TB
  zh: '| ActEV/VIRAT | [[87](#bib.bib87)] |'
- en: '| CA | [[101](#bib.bib101)] |'
  id: totrans-1058
  prefs: []
  type: TYPE_TB
  zh: '| CA | [[101](#bib.bib101)] |'
- en: '| CARLA | [[162](#bib.bib162)],[[147](#bib.bib147)] |'
  id: totrans-1059
  prefs: []
  type: TYPE_TB
  zh: '| CARLA | [[162](#bib.bib162)],[[147](#bib.bib147)] |'
- en: '| CHUK | [[155](#bib.bib155)] |'
  id: totrans-1060
  prefs: []
  type: TYPE_TB
  zh: '| CHUK | [[155](#bib.bib155)] |'
- en: '| CityPerson | [[183](#bib.bib183)] |'
  id: totrans-1061
  prefs: []
  type: TYPE_TB
  zh: '| CityPerson | [[183](#bib.bib183)] |'
- en: '| Cityscapes | [[150](#bib.bib150)] |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
  zh: '| Cityscapes | [[150](#bib.bib150)] |'
- en: '| Daimler Path | [[136](#bib.bib136)] |'
  id: totrans-1063
  prefs: []
  type: TYPE_TB
  zh: '| Daimler Path | [[136](#bib.bib136)] |'
- en: '| ETH | [[178](#bib.bib178)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[146](#bib.bib146)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[155](#bib.bib155)],[[164](#bib.bib164)],[[156](#bib.bib156)],[[180](#bib.bib180)],[[158](#bib.bib158)],[[160](#bib.bib160)]
    |'
  id: totrans-1064
  prefs: []
  type: TYPE_TB
  zh: '| ETH | [[178](#bib.bib178)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[146](#bib.bib146)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[155](#bib.bib155)],[[164](#bib.bib164)],[[156](#bib.bib156)],[[180](#bib.bib180)],[[158](#bib.bib158)],[[160](#bib.bib160)]
    |'
- en: '| Edinburgh (IFP) | [[114](#bib.bib114)] , [[179](#bib.bib179)] |'
  id: totrans-1065
  prefs: []
  type: TYPE_TB
  zh: '| Edinburgh (IFP) | [[114](#bib.bib114)] , [[179](#bib.bib179)] |'
- en: '| FM | [[167](#bib.bib167)] |'
  id: totrans-1066
  prefs: []
  type: TYPE_TB
  zh: '| FM | [[167](#bib.bib167)] |'
- en: '| GC | [[155](#bib.bib155)],[[115](#bib.bib115)],[[176](#bib.bib176)],[[135](#bib.bib135)]
    |'
  id: totrans-1067
  prefs: []
  type: TYPE_TB
  zh: '| GC | [[155](#bib.bib155)],[[115](#bib.bib115)],[[176](#bib.bib176)],[[135](#bib.bib135)]
    |'
- en: '| INTEARCTION | [[163](#bib.bib163)] |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
  zh: '| INTEARCTION | [[163](#bib.bib163)] |'
- en: '| JAAD | [[144](#bib.bib144)] |'
  id: totrans-1069
  prefs: []
  type: TYPE_TB
  zh: '| JAAD | [[144](#bib.bib144)] |'
- en: '| KITTI | [[150](#bib.bib150)],[[166](#bib.bib166)],[[165](#bib.bib165)] |'
  id: totrans-1070
  prefs: []
  type: TYPE_TB
  zh: '| KITTI | [[150](#bib.bib150)],[[166](#bib.bib166)],[[165](#bib.bib165)] |'
- en: '| L-CAS | [[234](#bib.bib234)] |'
  id: totrans-1071
  prefs: []
  type: TYPE_TB
  zh: '| L-CAS | [[234](#bib.bib234)] |'
- en: '| Lankershim Boulevard | [[179](#bib.bib179)] |'
  id: totrans-1072
  prefs: []
  type: TYPE_TB
  zh: '| Lankershim Boulevard | [[179](#bib.bib179)] |'
- en: '| MITT | [[135](#bib.bib135)] |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
  zh: '| MITT | [[135](#bib.bib135)] |'
- en: '| MOT | [[129](#bib.bib129)] |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
  zh: '| MOT | [[129](#bib.bib129)] |'
- en: '| NGSIM | [[177](#bib.bib177)],[[140](#bib.bib140)],[[141](#bib.bib141)],[[145](#bib.bib145)],[[148](#bib.bib148)],[[181](#bib.bib181)],[[182](#bib.bib182)]
    |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
  zh: '| NGSIM | [[177](#bib.bib177)],[[140](#bib.bib140)],[[141](#bib.bib141)],[[145](#bib.bib145)],[[148](#bib.bib148)],[[181](#bib.bib181)],[[182](#bib.bib182)]
    |'
- en: '| OSU | [[125](#bib.bib125)] |'
  id: totrans-1076
  prefs: []
  type: TYPE_TB
  zh: '| OSU | [[125](#bib.bib125)] |'
- en: '| Oxford | [[150](#bib.bib150)] |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
  zh: '| Oxford | [[150](#bib.bib150)] |'
- en: '| PETS2009 | [[152](#bib.bib152)] |'
  id: totrans-1078
  prefs: []
  type: TYPE_TB
  zh: '| PETS2009 | [[152](#bib.bib152)] |'
- en: '| PIE | [[144](#bib.bib144)] |'
  id: totrans-1079
  prefs: []
  type: TYPE_TB
  zh: '| PIE | [[144](#bib.bib144)] |'
- en: '| QMUL | [[115](#bib.bib115)] |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '| QMUL | [[115](#bib.bib115)] |'
- en: '| SBUIK | [[101](#bib.bib101)] |'
  id: totrans-1081
  prefs: []
  type: TYPE_TB
  zh: '| SBUIK | [[101](#bib.bib101)] |'
- en: '| SD | [[178](#bib.bib178)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[163](#bib.bib163)],[[152](#bib.bib152)],[[165](#bib.bib165)],[[132](#bib.bib132)]
    |'
  id: totrans-1082
  prefs: []
  type: TYPE_TB
  zh: '| SD | [[178](#bib.bib178)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[163](#bib.bib163)],[[152](#bib.bib152)],[[165](#bib.bib165)],[[132](#bib.bib132)]
    |'
- en: '| STRANDS | [[234](#bib.bib234)] |'
  id: totrans-1083
  prefs: []
  type: TYPE_TB
  zh: '| STRANDS | [[234](#bib.bib234)] |'
- en: '| TRAF | [[177](#bib.bib177)] |'
  id: totrans-1084
  prefs: []
  type: TYPE_TB
  zh: '| TRAF | [[177](#bib.bib177)] |'
- en: '| TUM Kitchen | [[134](#bib.bib134)] |'
  id: totrans-1085
  prefs: []
  type: TYPE_TB
  zh: '| TUM Kitchen | [[134](#bib.bib134)] |'
- en: '| Town Center | [[154](#bib.bib154)],[[131](#bib.bib131)],[[175](#bib.bib175)]
    |'
  id: totrans-1086
  prefs: []
  type: TYPE_TB
  zh: '| Town Center | [[154](#bib.bib154)],[[131](#bib.bib131)],[[175](#bib.bib175)]
    |'
- en: '| UCY | [[178](#bib.bib178)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[145](#bib.bib145)],[[146](#bib.bib146)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[154](#bib.bib154)],[[155](#bib.bib155)],[[164](#bib.bib164)],[[180](#bib.bib180)],[[131](#bib.bib131)],[[158](#bib.bib158)],[[159](#bib.bib159)],[[175](#bib.bib175)],[[160](#bib.bib160)],[[132](#bib.bib132)]
    |'
  id: totrans-1087
  prefs: []
  type: TYPE_TB
  zh: '| UCY | [[178](#bib.bib178)],[[87](#bib.bib87)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)],[[142](#bib.bib142)],[[143](#bib.bib143)],[[145](#bib.bib145)],[[146](#bib.bib146)],[[149](#bib.bib149)],[[163](#bib.bib163)],[[151](#bib.bib151)],[[152](#bib.bib152)],[[153](#bib.bib153)],[[154](#bib.bib154)],[[155](#bib.bib155)],[[164](#bib.bib164)],[[180](#bib.bib180)],[[131](#bib.bib131)],[[158](#bib.bib158)],[[159](#bib.bib159)],[[175](#bib.bib175)],[[160](#bib.bib160)],[[132](#bib.bib132)]
    |'
- en: '| VIRAT | [[123](#bib.bib123)] |'
  id: totrans-1088
  prefs: []
  type: TYPE_TB
  zh: '| VIRAT | [[123](#bib.bib123)] |'
- en: '| VPM | [[141](#bib.bib141)] |'
  id: totrans-1089
  prefs: []
  type: TYPE_TB
  zh: '| VPM | [[141](#bib.bib141)] |'
- en: '| nuScenes | [[162](#bib.bib162)] |'
  id: totrans-1090
  prefs: []
  type: TYPE_TB
  zh: '| nuScenes | [[162](#bib.bib162)] |'
- en: 'TABLE XIII: Datasets used in trajectory prediction applications.'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: 表 XIII：用于轨迹预测应用的数据集。
- en: '| Datasets | Papers |'
  id: totrans-1092
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 论文 |'
- en: '| --- | --- |'
  id: totrans-1093
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 3DPW | [[197](#bib.bib197)] |'
  id: totrans-1094
  prefs: []
  type: TYPE_TB
  zh: '| 3DPW | [[197](#bib.bib197)] |'
- en: '| CA | [[101](#bib.bib101)] |'
  id: totrans-1095
  prefs: []
  type: TYPE_TB
  zh: '| CA | [[101](#bib.bib101)] |'
- en: '| CMU Mocap | [[197](#bib.bib197)] |'
  id: totrans-1096
  prefs: []
  type: TYPE_TB
  zh: '| CMU Mocap | [[197](#bib.bib197)] |'
- en: '| CMU Panoptic | [[56](#bib.bib56)] |'
  id: totrans-1097
  prefs: []
  type: TYPE_TB
  zh: '| CMU Panoptic | [[56](#bib.bib56)] |'
- en: '| Egopose | [[186](#bib.bib186)] |'
  id: totrans-1098
  prefs: []
  type: TYPE_TB
  zh: '| Egopose | [[186](#bib.bib186)] |'
- en: '| Human 3.6M | [[187](#bib.bib187)],[[193](#bib.bib193)],[[96](#bib.bib96)],[[195](#bib.bib195)],[[190](#bib.bib190)],[[236](#bib.bib236)],[[197](#bib.bib197)],[[198](#bib.bib198)],[[196](#bib.bib196)],[[199](#bib.bib199)],[[191](#bib.bib191)],[[192](#bib.bib192)],[[110](#bib.bib110)],[[189](#bib.bib189)],[[194](#bib.bib194)]
    |'
  id: totrans-1099
  prefs: []
  type: TYPE_TB
  zh: '| Human 3.6M | [[187](#bib.bib187)],[[193](#bib.bib193)],[[96](#bib.bib96)],[[195](#bib.bib195)],[[190](#bib.bib190)],[[236](#bib.bib236)],[[197](#bib.bib197)],[[198](#bib.bib198)],[[196](#bib.bib196)],[[199](#bib.bib199)],[[191](#bib.bib191)],[[192](#bib.bib192)],[[110](#bib.bib110)],[[189](#bib.bib189)],[[194](#bib.bib194)]
    |'
- en: '| InstaVariety | [[199](#bib.bib199)] |'
  id: totrans-1100
  prefs: []
  type: TYPE_TB
  zh: '| InstaVariety | [[199](#bib.bib199)] |'
- en: '| MPII Human Pose | [[189](#bib.bib189)] |'
  id: totrans-1101
  prefs: []
  type: TYPE_TB
  zh: '| MPII Human Pose | [[189](#bib.bib189)] |'
- en: '| Mouse Fish | [[236](#bib.bib236)] |'
  id: totrans-1102
  prefs: []
  type: TYPE_TB
  zh: '| Mouse Fish | [[236](#bib.bib236)] |'
- en: '| Own | [[48](#bib.bib48)],[[200](#bib.bib200)],[[188](#bib.bib188)],[[103](#bib.bib103)],[[185](#bib.bib185)]
    |'
  id: totrans-1103
  prefs: []
  type: TYPE_TB
  zh: '| Own | [[48](#bib.bib48)],[[200](#bib.bib200)],[[188](#bib.bib188)],[[103](#bib.bib103)],[[185](#bib.bib185)]
    |'
- en: '| Penn Action | [[199](#bib.bib199)],[[187](#bib.bib187)],[[26](#bib.bib26)],[[189](#bib.bib189)]
    |'
  id: totrans-1104
  prefs: []
  type: TYPE_TB
  zh: '| Penn Action | [[199](#bib.bib199)],[[187](#bib.bib187)],[[26](#bib.bib26)],[[189](#bib.bib189)]
    |'
- en: '| SBUIK | [[101](#bib.bib101)] |'
  id: totrans-1105
  prefs: []
  type: TYPE_TB
  zh: '| SBUIK | [[101](#bib.bib101)] |'
- en: '| UCF-101 | [[26](#bib.bib26)] |'
  id: totrans-1106
  prefs: []
  type: TYPE_TB
  zh: '| UCF-101 | [[26](#bib.bib26)] |'
- en: 'TABLE XIV: Datasets used in motion prediction applications.'
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 表 XIV：用于运动预测应用的数据集。
- en: '| Datasets | Papers |'
  id: totrans-1108
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 论文 |'
- en: '| --- | --- |'
  id: totrans-1109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| AMOS | [[214](#bib.bib214)] |'
  id: totrans-1110
  prefs: []
  type: TYPE_TB
  zh: '| AMOS | [[214](#bib.bib214)] |'
- en: '| Amazon | [[217](#bib.bib217)] |'
  id: totrans-1111
  prefs: []
  type: TYPE_TB
  zh: '| Amazon | [[217](#bib.bib217)] |'
- en: '| Cityscapes | [[10](#bib.bib10)],[[210](#bib.bib210)],[[211](#bib.bib211)],[[212](#bib.bib212)]
    |'
  id: totrans-1112
  prefs: []
  type: TYPE_TB
  zh: '| Cityscapes | [[10](#bib.bib10)],[[210](#bib.bib210)],[[211](#bib.bib211)],[[212](#bib.bib212)]
    |'
- en: '| DIPLECS | [[239](#bib.bib239)] |'
  id: totrans-1113
  prefs: []
  type: TYPE_TB
  zh: '| DIPLECS | [[239](#bib.bib239)] |'
- en: '| FCVL | [[209](#bib.bib209)] |'
  id: totrans-1114
  prefs: []
  type: TYPE_TB
  zh: '| FCVL | [[209](#bib.bib209)] |'
- en: '| Golden Colorado | [[215](#bib.bib215)] |'
  id: totrans-1115
  prefs: []
  type: TYPE_TB
  zh: '| Golden Colorado | [[215](#bib.bib215)] |'
- en: '| JAAD | [[203](#bib.bib203)] |'
  id: totrans-1116
  prefs: []
  type: TYPE_TB
  zh: '| JAAD | [[203](#bib.bib203)] |'
- en: '| KITTI | [[205](#bib.bib205)],[[201](#bib.bib201)] |'
  id: totrans-1117
  prefs: []
  type: TYPE_TB
  zh: '| KITTI | [[205](#bib.bib205)],[[201](#bib.bib201)] |'
- en: '| MBI-1M | [[216](#bib.bib216)] |'
  id: totrans-1118
  prefs: []
  type: TYPE_TB
  zh: '| MBI-1M | [[216](#bib.bib216)] |'
- en: '| MU | [[220](#bib.bib220)] |'
  id: totrans-1119
  prefs: []
  type: TYPE_TB
  zh: '| MU | [[220](#bib.bib220)] |'
- en: '| SUN RGB-D | [[219](#bib.bib219)] |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
  zh: '| SUN RGB-D | [[219](#bib.bib219)] |'
- en: '| Tuscan Arizona | [[215](#bib.bib215)] |'
  id: totrans-1121
  prefs: []
  type: TYPE_TB
  zh: '| Tuscan Arizona | [[215](#bib.bib215)] |'
- en: '| VIST | [[8](#bib.bib8)] |'
  id: totrans-1122
  prefs: []
  type: TYPE_TB
  zh: '| VIST | [[8](#bib.bib8)] |'
- en: 'TABLE XV: Datasets used in other prediction applications.'
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 XV：用于其他预测应用的数据集。
