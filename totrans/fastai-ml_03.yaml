- en: 'Machine Learning 1: Lesson 3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习1：第3课
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-3-fa4065d8cb1e](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-3-fa4065d8cb1e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-3-fa4065d8cb1e](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-3-fa4065d8cb1e)
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从*[*机器学习课程*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*中的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*[*Jeremy*](https://twitter.com/jeremyphoward)*和*[*Rachel*](https://twitter.com/math_rachel)*给了我这个学习的机会。*'
- en: 'What is covered in today’s lesson:'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 今天的课程内容：
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson2-rf_interpretation.ipynb)
    / [Video](https://youtu.be/YSFG_W8JxBo)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson2-rf_interpretation.ipynb)
    / [视频](https://youtu.be/YSFG_W8JxBo)'
- en: '**Understanding the data better by using machine learning**'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过使用机器学习更好地理解数据**'
- en: This idea is contrary to the common refrain that things like random forests
    are black boxes that hide meaning from us. The truth is quite the opposite. Random
    forests allow us to understand our data deeper and more quickly than traditional
    approaches.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个想法与常见的说法相反，即随机森林等东西是隐藏我们意义的黑匣子。事实恰恰相反。随机森林让我们比传统方法更深入更快地理解我们的数据。
- en: '**How to look at larger datasets**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何查看更大的数据集**'
- en: Dataset with over 100 million rows — [Grocery Forecasting](https://www.kaggle.com/c/favorita-grocery-sales-forecasting)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有超过1亿行的数据集 - [杂货销售预测](https://www.kaggle.com/c/favorita-grocery-sales-forecasting)
- en: '**Question**: When to use random forests [[2:41](https://youtu.be/YSFG_W8JxBo?t=2m41s)]?
    Cannot think of anything offhand that it is definitely not going to be at least
    somewhat useful. So it is always worth trying. The real question might be in what
    situation should we try other things as well, and the short answer to that is
    for unstructured data (image, sound, etc), you almost certainly want to try deep
    learning. For collaborative filtering model (groceries competition is of that
    kind), neither random forest nor deep learning approach is exactly what you want
    and you need to do some tweaks.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：何时使用随机森林[[2:41](https://youtu.be/YSFG_W8JxBo?t=2m41s)]？我无法想到任何绝对不会至少有些用处的情况。因此，值得一试。真正的问题可能是在什么情况下我们应该尝试其他方法，简短的答案是对于非结构化数据（图像，声音等），您几乎肯定要尝试深度学习。对于协同过滤模型（杂货竞赛属于这种类型），随机森林和深度学习方法都不是您想要的，您需要做一些调整。'
- en: Review of last week [[4:42](https://youtu.be/YSFG_W8JxBo?t=4m42s)]
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上周回顾[[4:42](https://youtu.be/YSFG_W8JxBo?t=4m42s)]
- en: Reading CSV took a minute or two, and we saved it to a feather format file.
    Feather format is almost the same format that it lives in RAM, so it is ridiculously
    fast to read and write. The first thing we do is in the lesson 2 notebook is to
    read in the feather format file.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 读取CSV花了一两分钟，我们将其保存为羽毛格式文件。羽毛格式几乎与RAM中的格式相同，因此读写速度非常快。我们在第2课笔记本中做的第一件事是读取羽毛格式文件。
- en: proc_df issue [[5:28](https://youtu.be/YSFG_W8JxBo?t=5m28s)]
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: proc_df问题[[5:28](https://youtu.be/YSFG_W8JxBo?t=5m28s)]
- en: 'An interesting little issue that was brought up during the week is in `proc_df`
    function. `proc_df` function does the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一周期间提出的一个有趣的小问题是在`proc_df`函数中。`proc_df`函数执行以下操作：
- en: Finds numeric columns which have missing values and create an additional boolean
    column as well as replacing the missing with medians.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找具有缺失值的数值列，并创建一个额外的布尔列，同时用中位数替换缺失值。
- en: Turn the categorical objects into integer codes.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分类对象转换为整数代码。
- en: '**Problem #1**: Your test set may have missing values in some columns that
    were not in your training set or vice versa. If that happens, you are going to
    get an error when you try to do the random forest since the “missing” boolean
    column appeared in your training set but not in the test set.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题＃1**：您的测试集中可能有一些列中的缺失值，这些列在训练集中不存在，反之亦然。如果发生这种情况，当您尝试进行随机森林时，您将会出现错误，因为“缺失”布尔列出现在训练集中，但不在测试集中。'
- en: '**Problem #2**: Median of the numeric value in the test set may be different
    from the training set. So it may process it into something which has different
    semantics.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题＃2**：测试集中数值的中位数可能与训练集不同。因此，它可能将其处理为具有不同语义的内容。'
- en: '**Solution**: There is now an additional return variable `nas` from `proc_df`
    which is a dictionary whose keys are the names of the columns that had missing
    values, and the values of the dictionary are the medians. Optionally, you can
    pass `nas` to `proc_df` as an argument to make sure that it adds those specific
    columns and uses those specific medians:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**：现在有一个额外的返回变量`nas`从`proc_df`，它是一个字典，其键是具有缺失值的列的名称，字典的值是中位数。可选地，您可以将`nas`作为参数传递给`proc_df`，以确保它添加这些特定列并使用这些特定中位数：'
- en: '`df, y, nas = proc_df(df_raw, ''SalePrice'', nas)`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`df, y, nas = proc_df(df_raw, ''SalePrice'', nas)`'
- en: Corporación Favorita Grocery Sales Forecasting [[9:25](https://youtu.be/YSFG_W8JxBo?t=9m25s)]
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Corporación Favorita杂货销售预测[[9:25](https://youtu.be/YSFG_W8JxBo?t=9m25s)]
- en: Let’s walk through the same process when you are working with a really large
    dataset. It is almost the same but there are a few cases where we cannot use the
    defaults because defaults run a little bit too slowly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们走一遍当您处理一个真正大的数据集时的相同过程。几乎相同，但有一些情况下我们不能使用默认值，因为默认值运行速度有点慢。
- en: 'It is important to be able to explain the problem you are working on. The key
    things to understand in a machine learning problem are:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 能够解释您正在处理的问题是很重要的。在机器学习问题中理解的关键事项是：
- en: What are the independent variables?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立变量是什么？
- en: What is the dependent variable (the thing you are trying to predict)?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是因变量（您试图预测的东西）？
- en: In this competition
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个比赛中
- en: Dependent variable — how many units of each kind of product were sold in each
    store on each day during the two week period.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因变量 — 在两周期间每天每个商店销售了多少种产品。
- en: Independent variables — how many units of each product at each store on each
    day were sold in the last few years. For each store, where it is located and what
    class of store it is (metadata). For each type of product, what category of product
    it is, etc. For each date, we have metadata such as what the oil price was.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自变量 — 过去几年每个产品在每个商店每天销售了多少单位。对于每个商店，它的位置在哪里以及它是什么类型的商店（元数据）。对于每种产品，它是什么类别的产品等。对于每个日期，我们有元数据，比如油价是多少。
- en: This is what we call a **relational dataset**. Relational dataset is one where
    we have a number of different pieces of information that we can join together.
    Specifically this kind of relational dataset is what we refer to as “star schema”
    where there is some central transactions table. In this competition, the central
    transactions table is `train.csv` which contains the number units that were sold
    by `date` , `store_nbr` , and `item_nbr`. From this, we can join various bits
    of metadata (hence the name “star” schema — there is also one called [“snowflake”
    schema](https://en.wikipedia.org/wiki/Snowflake_schema)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们所说的**关系数据集**。关系数据集是指我们可以将许多不同信息连接在一起的数据集。具体来说，这种关系数据集是我们所说的“星型模式”，其中有一张中心交易表。在这个比赛中，中心交易表是`train.csv`，其中包含了按`日期`、`store_nbr`和`item_nbr`销售的数量。通过这个表，我们可以连接各种元数据（因此称为“星型”模式
    — 还有一种叫做[“雪花”模式](https://en.wikipedia.org/wiki/Snowflake_schema)）。
- en: Reading Data[[15:12](https://youtu.be/YSFG_W8JxBo?t=15m12s)]
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取数据[[15:12](https://youtu.be/YSFG_W8JxBo?t=15m12s)]
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you set `low_memory=False`, it will run out of memory regardless of how much
    memory you have.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果设置`low_memory=False`，无论您有多少内存，它都会耗尽内存。
- en: In order to limit the amount of space that it takes up when you read in, we
    create a dictionary for each column name to the data type of that column. It is
    up to you to figure out the data types by running or `less` or `head` on the dataset.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了在读取时限制占用的空间量，我们为每个列名创建一个字典，指定该列的数据类型。您可以通过运行或在数据集上使用`less`或`head`来找出数据类型。
- en: With these tweaks, we can read in 125,497,040 rows in less than 2 minutes.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过这些调整，我们可以在不到2分钟内读取125,497,040行数据。
- en: Python itself is not fast, but almost everything we want to do in Python in
    data science has been written for us in C or more often in Cython which is a python
    like language that compiles to C. In Pandas, a lot of it is written in assembly
    language which is heavily optimized. Behind the scene, a lot of that is going
    back to calling Fortran based libraries for linear algebra.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python本身并不快，但几乎我们在Python中进行数据科学时想要做的一切都已经为我们用C或更常见的Cython编写好了，Cython是一种类似Python的语言，可以编译成C。在Pandas中，很多代码是用汇编语言编写的，这些代码经过了大量优化。在幕后，很多代码都是调用基于Fortran的线性代数库。
- en: '**Question**: Are there any performance consideration to specifying `int64`
    vs. `int` [[18:33](https://youtu.be/YSFG_W8JxBo?t=18m33s)]? The key performance
    here was to use the smallest number of bits that I could to fully represent the
    column. If we used `int8` for `item_nbr` , the maximum `item_nbr` is bigger than
    255 and it will not fit. On the other hand, if we used `int64` for the `store_nbr`
    , it is using more bits than necessary. Given that the whole purpose here was
    to avoid running out of RAM, we do not want to use up 8 times more memory than
    necessary. When you are working with large datasets, very often you will find
    that the slow piece is reading and writing to RAM, not the CPU operations. Also
    as a rule of thumb, smaller data types often will run faster particularly if you
    can use Single Instruction Multiple Data (SIMD) vectorized code, it can pack more
    numbers into a single vector to run at once.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：指定`int64`与`int`是否有任何性能考虑[[18:33](https://youtu.be/YSFG_W8JxBo?t=18m33s)]？这里的关键性能是使用尽可能少的位数来完全表示列。如果我们对`item_nbr`使用`int8`，最大的`item_nbr`大于255，将无法容纳。另一方面，如果我们对`store_nbr`使用`int64`，它使用的位数比必要的多。鉴于这里的整个目的是避免耗尽RAM，我们不希望使用比必要多8倍的内存。当您处理大型数据集时，很多时候最慢的部分是读取和写入RAM，而不是CPU操作。另外，作为一个经验法则，较小的数据类型通常会运行得更快，特别是如果您可以使用单指令多数据（SIMD）矢量化代码，它可以将更多数字打包到一个单独的矢量中一次运行。'
- en: '**Question**: Do we not have to shuffle the data anymore [[20:11](https://youtu.be/YSFG_W8JxBo?t=20m11s)]?
    Although here I have read in the whole thing, when I start I never start by reading
    in the whole thing. By using a UNIX command `shuf`, you can get a random sample
    of data at the command prompt and then you can just read that. This is a good
    way, for example, to find out what data types to use — read in a random sample
    and let Pandas figure it out for you. In general, I do as much work as possible
    on a sample until I feel confident that I understand the sample before I move
    on.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我们不再需要对数据进行洗牌了吗[[20:11](https://youtu.be/YSFG_W8JxBo?t=20m11s)]？尽管在这里我已经读取了整个数据，但当我开始时，我从不会一开始就读取整个数据。通过使用UNIX命令`shuf`，您可以在命令提示符下获取数据的随机样本，然后您可以直接读取该样本。这是一个很好的方法，例如，找出要使用的数据类型
    — 读取一个随机样本，让Pandas为您找出。通常情况下，我会尽可能多地在样本上工作，直到我确信我理解了样本后才会继续。'
- en: 'To pick a random line from a file using `shuf` use the `-n` option. This limits
    the output to the number specified. You can also specify the output file:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`shuf`从文件中随机选择一行，请使用`-n`选项。这将限制输出为指定的数量。您还可以指定输出文件：
- en: '`shuf -n 5 -o sample_training.csv train.csv`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`shuf -n 5 -o sample_training.csv train.csv`'
- en: '`''onpromotion'': ‘object''` [[21:28](https://youtu.be/YSFG_W8JxBo?t=21m28s)]—
    `object` is a general purpose Python datatype which is slow and memory heavy.
    The reason for this is it is a boolean which also has missing values, so we need
    to deal with this before we can turn it into a boolean as you see below:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`''onpromotion'': ‘object''` [[21:28](https://youtu.be/YSFG_W8JxBo?t=21m28s)]—
    `object`是一个通用的Python数据类型，速度慢且占用内存。原因是它是一个布尔值，还有缺失值，所以我们需要在将其转换为布尔值之前处理它，如下所示：'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`fillna(False)`: we would not do this without checking first, but some exploratory
    data analysis shows that it is probably an appropriate thing to do (i.e. missing
    means false).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fillna(False)`: 我们不会在没有先检查的情况下这样做，但一些探索性数据分析显示这可能是一个合适的做法（即缺失值表示false）。'
- en: '`map({‘False’: False, ‘True’: True})` : `object` usually reads in as string,
    so replace string `‘True’` and `‘False’` with actual booleans.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map({‘False’: False, ‘True’: True})`：`object`通常读取为字符串，所以用实际的布尔值替换字符串`‘True’`和`‘False’`。'
- en: '`astype(bool)` : Then finally convert it to boolean type.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`astype(bool)`：最后将其转换为布尔类型。'
- en: The feather file with over 125 million records takes up something under 2.5GB
    of memory.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有超过1.25亿条记录的feather文件占用了不到2.5GB的内存。
- en: Now it is in a nice fast format, we can save it to feather format in under 5
    seconds.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在它以一个很好的快速格式，我们可以在不到5秒的时间内将它保存为feather格式。
- en: 'Pandas is generally fast, so you can summarize every column of all 125 million
    records in 20 seconds:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas通常很快，所以你可以在20秒内总结所有1.25亿条记录的每一列：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/2568de2e53d9b99568f8fbdd2acddb1d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2568de2e53d9b99568f8fbdd2acddb1d.png)'
- en: First thing to look at is the dates. Dates are important because any models
    you put in in practice, you are going to be putting it in at some date that is
    later than the date that you trained it by definition. So if anything in the world
    changes, you need to know how your predictive accuracy changes as well. So for
    Kaggle or for your own project, you should always make sure that your dates do
    not overlap [[22:55](https://youtu.be/YSFG_W8JxBo?t=22m55s)].
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先要看的是日期。日期很重要，因为你在实践中放入的任何模型，都会在比你训练的日期晚的某个日期放入。所以如果世界上的任何事情发生变化，你需要知道你的预测准确性也会如何变化。所以对于Kaggle或你自己的项目，你应该始终确保你的日期不重叠。
- en: In this case, training set goes from 2013 to August 2017.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，训练集从2013年到2017年8月。
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/df1859cbc7988611eeea704f84096273.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df1859cbc7988611eeea704f84096273.png)'
- en: In our test set, they go from one day later until the end of the month.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的测试集中，它们从第二天开始直到月底。
- en: This is a key thing — you cannot really do any useful machine learning until
    you understand this basic piece. You have four years of data and you are trying
    to predict the next two weeks. This is a fundamental thing you need to understand
    before you can go and do a good job at this.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个关键的事情 —— 除非你理解这个基本的部分，否则你无法真正做出任何有用的机器学习。你有四年的数据，你正在尝试预测接下来的两周。在你能够做好这个工作之前，这是你需要理解的基本事情。
- en: If you want to use a smaller dataset, we should use the most recent — not random
    set.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想使用一个较小的数据集，我们应该使用最近的 —— 而不是随机的集合。
- en: '**Question**: Wouldn’t four years ago around the same time frame be important
    (e.g. around Christmas time)[[25:06](https://youtu.be/YSFG_W8JxBo?t=25m6s)]? Exactly.
    It is not that there is no useful information from four years ago so we do not
    want to entirely throw it away. But as a first step, if you were to submit the
    mean, you would not submit the mean of 2012 sales, but probably want to submit
    the mean of last month’s sale.And later on, we might want to weight more recent
    dates more highly since they are probably more relevant. But we should do bunch
    of exploratory data analysis to check that.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：四年前大约在同一时间段重要吗（例如在圣诞节左右）？确实。并不是说四年前没有有用的信息，所以我们不想完全抛弃它。但作为第一步，如果你要提交平均值，你不会提交2012年销售额的平均值，而可能想要提交上个月销售额的平均值。之后，我们可能希望更高权重最近的日期，因为它们可能更相关。但我们应该进行大量的探索性数据分析来检查。'
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/80c10c4942f06fb03e584e5065725c9e.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80c10c4942f06fb03e584e5065725c9e.png)'
- en: Here is what the bottom of the data looks like [[26:00](https://youtu.be/YSFG_W8JxBo?t=26m)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据底部的样子。
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We have to take a log of the sales because we are trying to predict something
    that varies according to the ratios and they told us, in this competition, that
    root mean squared log error is something they care about.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须对销售额取对数，因为我们正在尝试预测根据比率变化的某些东西，而他们告诉我们，在这个比赛中，均方根对数误差是他们关心的事情。
- en: '`np.clip(df_all.unit_sales, 0, None)`: there are some negative sales that represent
    returns and the organizer told us to consider them to be zero for the purpose
    of this competition. `clilp` truncates to specified min and max.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.clip(df_all.unit_sales, 0, None)`: 有一些代表退货的负销售额，组织者告诉我们在这个比赛中将它们视为零。`clip`截断到指定的最小值和最大值。'
- en: '`np.log1p` : log of the value plus 1\. The competition detail tells you that
    they are going to use root mean squared log plus 1 error because log(0) does not
    make sense.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.log1p`：值加1的对数。比赛细节告诉你他们将使用均方根对数加1误差，因为log(0)没有意义。'
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can add date part as usual. It takes a couple of minutes, so we should run
    through all this on sample first to make sure it works. Once you know everything
    is reasonable, then go back and run on a whole set.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像往常一样添加日期部分。这需要几分钟，所以我们应该先在样本上运行所有这些，以确保它有效。一旦你知道一切都是合理的，然后回去在整个集合上运行。
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These lines of code are identical to what we saw for bulldozers competition.
    We do not need to run `train_cats` or `apply_cats` since all of the data types
    are already numeric (remember `apply_cats` applies the same categorical codes
    to validation set as the training set) [[27:59](https://youtu.be/YSFG_W8JxBo?t=27m59s)].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码行与我们在推土机比赛中看到的代码行是相同的。我们不需要运行`train_cats`或`apply_cats`，因为所有的数据类型已经是数字的了（记住`apply_cats`将相同的分类代码应用于验证集和训练集）。
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Call `proc_df` to check the missing values and so forth.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`proc_df`来检查缺失值等。
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'These lines of code again are identical. Then there are two changes [[28:48](https://youtu.be/YSFG_W8JxBo?t=28m48s)]:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码行再次是相同的。然后有两个变化：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We have learned about `set_rf_samples` last week. We probably do not want to
    create a tree from 125 million records (not sure how long that will take). You
    can start with 10k or 100k and figure out how much you can run. There is no relationship
    between the size of the dataset and how long it takes to build the random forests
    — the relationship is between the number of estimators multiplied by the sample
    size.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上周学习了`set_rf_samples`。我们可能不想从1.25亿条记录中创建一棵树（不确定需要多长时间）。你可以从10k或100k开始，然后找出你可以运行多少。数据集的大小与构建随机森林所需时间之间没有关系，关系在于估计器数量乘以样本大小。
- en: '**Question:** What is `n_job`? In the past, it has always been `-1` [[29:42](https://youtu.be/YSFG_W8JxBo?t=29m42s)].
    The number of jobs is the number of cores to use. I was running this on a computer
    that has about 60 cores and if you try to use all of them, it spent so much time
    spinning out jobs and it was slower. If you have lots of cores on your computer,
    sometimes you want less (`-1` means use every single core).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：** `n_job`是什么？过去，它总是`-1`[[29:42](https://youtu.be/YSFG_W8JxBo?t=29m42s)]。作业数是要使用的核心数。我在一台大约有60个核心的计算机上运行这个，如果你尝试使用所有核心，它会花费很多时间来启动作业，速度会变慢。如果你的计算机有很多核心，有时你想要更少（`-1`表示使用每个核心）。'
- en: Another change was `x = np.array(trn, dtype=np.float32)`. This converts data
    frame into an array of floats and we fit it on that. Inside the random forest
    code, they do this anyway. Given that we want to run a few different random forests
    with a few different hyper parameters, doing this once myself saves 1 min 37 sec.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个变化是`x = np.array(trn, dtype=np.float32)`。这将数据框转换为浮点数组，然后我们在其上进行拟合。在随机森林代码内部，他们无论如何都会这样做。鉴于我们想要运行几个不同的随机森林，使用几种不同的超参数，自己做一次可以节省1分37秒。
- en: '`Profiler : %prun` [[30:40](https://youtu.be/YSFG_W8JxBo?t=30m40s)]'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`分析器：%prun`[[30:40](https://youtu.be/YSFG_W8JxBo?t=30m40s)]'
- en: If you run a line of code that takes quite a long time, you can put `%prun`
    in front.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行一行需要很长时间的代码，你可以在前面加上`%prun`。
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will run a profiler and tells you which lines of code took the most time.
    Here it was the code in scikit-learn that was the line of code that converts data
    frame to numpy array.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将运行一个分析器，并告诉你哪些代码行花费了最多的时间。这里是scikit-learn中将数据框转换为numpy数组的代码行。
- en: Looking to see which things is taking up the time is called “profiling” and
    in software engineering is one of the most important tool. But data scientists
    tend to under appreciate it.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看哪些事情占用了时间被称为“分析”，在软件工程中是最重要的工具之一。但数据科学家往往低估了它。
- en: For fun, try running `%prun` from time to time on code that takes 10–20 seconds
    and see if you can learn to interpret and use profiler outputs.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有趣的是，尝试不时运行`%prun`在需要10-20秒的代码上，看看你是否能学会解释和使用分析器输出。
- en: Something else Jeremy noticed in the profiler is we can’t use OOB score when
    we do `set_rf_samples` because if we do, it will use the other 124 million rows
    to calculate the OOB score. Besides, we want to use the validation set that is
    the most recent dates rather than random.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeremy在分析器中注意到的另一件事是，当我们使用`set_rf_samples`时，我们不能使用OOB分数，因为如果这样做，它将使用其他124百万行来计算OOB分数。此外，我们希望使用最近日期的验证集，而不是随机的。
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So this got us 0.76 validation root mean squared log error.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这让我们得到了0.76的验证均方根对数误差。
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This gets us down to 0.71 even though it took a little longer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们降到了0.71，尽管花了更长的时间。
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This brought this error down to 0.70\. `min_samples_leaf=1` did not really
    help. So we have a “reasonable” random forest here. But this does not give a good
    result on the leader board [[33:42](https://youtu.be/YSFG_W8JxBo?t=33m42s)]. Why?
    Let’s go back and see the data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这将错误降低到0.70。`min_samples_leaf=1`并没有真正帮助。所以我们在这里有一个“合理”的随机森林。但是这在排行榜上并没有取得好的结果[[33:42](https://youtu.be/YSFG_W8JxBo?t=33m42s)]。为什么？让我们回头看看数据：
- en: '![](../Images/882cf4f8fcac80db504a7cf053135380.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/882cf4f8fcac80db504a7cf053135380.png)'
- en: 'These are the columns we had to predict with (plus what were added by `add_datepart`).
    Most of the insight around how much of something you expect to sell tomorrow is
    likely to be wrapped up in the details about where the store is, what kind of
    things they tend to sell at the store, for a given item, what category of item
    it is. Random forest has no ability to do anything other than create binary splits
    on things like day of week, store number, item number. It does not know type of
    items or location of stores. Since its ability to understand what is going on
    is limited, we probably need to use the entire 4 years of data to even get some
    useful insights. But as soon as we start using the whole 4 years of data, a lot
    of the data we are using is really old. There is a Kaggle kernel that points out
    that what you could do is [[35:54](https://youtu.be/YSFG_W8JxBo?t=35m54s)]:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们必须用来预测的列（以及由`add_datepart`添加的内容）。关于明天预计销售多少的大部分见解可能都包含在关于商店位置、商店通常销售的物品种类以及给定物品的类别是什么的细节中。随机森林除了在诸如星期几、商店编号、物品编号等方面创建二元分割之外，没有其他能力。它不知道物品类型或商店位置。由于它理解正在发生的事情的能力有限，我们可能需要使用整整4年的数据才能得到一些有用的见解。但是一旦我们开始使用整整4年的数据，我们使用的数据中有很多是非常陈旧的。有一个Kaggle内核指出，你可以做的是[[35:54](https://youtu.be/YSFG_W8JxBo?t=35m54s)]：
- en: Take the last two weeks.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看最后两周。
- en: Take the average sales by store number, by item number, by on promotion, then
    take a mean across date.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按商店编号、物品编号、促销情况的平均销售额，然后跨日期取平均。
- en: Just submit that, and you come about 30th 🎉
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只需提交，你就能排在第30名左右🎉
- en: We will talk about this in the next class, but if you can figure out how you
    start with that model and make it a little bit better, you will be above 30th
    place.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一堂课上讨论这个问题，但如果你能找出如何从那个模型开始并使其变得更好一点，你将排在第30名以上。
- en: '**Question**: Could you try to capture seasonality and trend effects by creating
    new columns like average sales in the month of August [[38:10](https://youtu.be/YSFG_W8JxBo?t=38m10s)]?
    It is a great idea. The thing to figure out is how to do it because there are
    details to get right and are difficult- not intellectually difficult but they
    are difficult in a way that makes you headbutt your desk at 2am [[38:41](https://youtu.be/YSFG_W8JxBo?t=38m41s)].'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：您能否尝试通过创建新列来捕捉季节性和趋势效应，比如8月份的平均销售额？这是一个很好的主意。要解决的问题是如何做到这一点，因为有一些细节需要正确，这些细节很困难-不是在智力上困难，而是以一种让你在凌晨2点撞头的方式困难。'
- en: Coding you do for machine learning is incredibly frustrating and incredibly
    difficult. If you get a detail wrong, much of the time it is not going to give
    you an exception it will just silently be slightly less good than it otherwise
    would have been. If you are on Kaggle, you will know that you are not doing as
    well as other people. But otherwise you have nothing to compare against. You will
    not know if your company’s model is half as good as it could be because you made
    a little mistake. This is why practicing on Kaggle now is great.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为机器学习编码是非常令人沮丧和非常困难的。如果你弄错了一个细节，很多时候它不会给你一个异常，它只会默默地比原来稍微差一点。如果你在Kaggle上，你会知道你的表现不如其他人。但否则，你没有什么可以比较的。你不会知道你的公司模型是否只有它可能的一半好，因为你犯了一个小错误。这就是为什么现在在Kaggle上练习是很好的。
- en: You will get practice in finding all of the ways in which you can infuriatingly
    screw things up and you will be amazed [[39:38](https://youtu.be/YSFG_W8JxBo?t=39m38s)].
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你将练习找到所有可能令人恼火地搞砸事情的方法，你会感到惊讶。
- en: Even for Jeremy, there is an extraordinary array of them. As you get to get
    to know what they are, you will start to know how to check for them as you go.
    You should assume every button you press, you are going to press the wrong button.
    That is fine as long as you have a way to find out.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于Jeremy来说，这些验证集也是非常丰富的。当你开始了解它们是什么时，你将开始知道如何在进行时检查它们。你应该假设你按下的每个按钮都会按错按钮。只要你有一种找出来的方法就可以。
- en: Unfortunately there is not a set of specific things you should always do, you
    just have to think what you know about the results of this thing I am about to
    do. Here is a really simple example. If you created that basic entry where you
    take the mean by date, by store number, by on promotion, you submitted it, and
    got a reasonable score. Then you think you have something that is a little bit
    better and you do predictions for that. How about you create a scatter plot showing
    the prediction of your average model on one axis versus the predictions of your
    new model on the other axis. You should see that they just about form a line.
    If they do not, then that is a very strong suggestion that you screwed something
    up.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，没有一套你应该总是做的具体事情，你只需要考虑一下我即将做的事情的结果。这里有一个非常简单的例子。如果你创建了一个基本的条目，其中你按日期、店铺编号、促销状态取平均值，然后提交了它，并得到了一个合理的分数。然后你认为你有一些稍微好一点的东西，你为此做了预测。你可以创建一个散点图，显示你的平均模型预测在一个轴上，与你的新模型预测在另一个轴上。你应该看到它们几乎形成一条直线。如果不是，那么这非常明显地表明你搞砸了什么。
- en: '**Question**: How often do you pull in data from other sources to supplement
    dataset you have [[41:15](https://youtu.be/YSFG_W8JxBo?t=41m15s)]? Very often.
    The whole point of star schema is that you have a centric table, and you have
    other tables coming off it that provide metadata about it. On Kaggle, most competitions
    have the rule that you can use external data as long as post on the forum and
    is publicly available (double check the rule!). Outside of the Kaggle, you should
    always be looking for what external data you could possibly leverage.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：您多久从其他来源获取数据来补充您已有的数据集？非常频繁。星型模式的整个重点是你有一个中心表，你有其他表与之相连，提供关于它的元数据。在Kaggle上，大多数比赛的规则是你可以使用外部数据，只要在论坛上发布并且是公开可用的（双重检查规则！）。在Kaggle之外，你应该始终寻找可能利用的外部数据。'
- en: '**Question**: How about adding Ecuador’s holidays to supplement the data? [[42:52](https://youtu.be/YSFG_W8JxBo?t=42m52s)]
    That information is actually provided. In general, one way of tackling this kind
    of problem is to create lots of new columns containing things like average number
    of sales on holidays, average percent change in sale between January and February,
    etc. There has been [a pervious competition](https://www.kaggle.com/c/rossmann-store-sales)
    for a grocery chain in Germany that was almost identical. [The person who won](http://blog.kaggle.com/2015/12/21/rossmann-store-sales-winners-interview-1st-place-gert/)
    was a domain expert and specialist in doing logistics predictions. He created
    lots of columns based on his experience of what kinds of things tend to be useful
    for making predictions. So that is an approach that can work. The third place
    winner did almost no feature engineering, however, and they also had one big oversight
    which may have cost them the first place win. We will be learning a lot more about
    how to win this competition and ones like it as we go.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：如何添加厄瓜多尔的假期来补充数据？这个信息实际上是提供的。一种解决这种问题的一般方法是创建许多新列，其中包含假期销售平均数量，一月和二月之间销售平均百分比变化等。在德国的一个杂货连锁店曾经有一场[先前的比赛](https://www.kaggle.com/c/rossmann-store-sales)，几乎是一样的。[获胜者](http://blog.kaggle.com/2015/12/21/rossmann-store-sales-winners-interview-1st-place-gert/)是一个领域专家，擅长做物流预测。他根据自己的经验创建了许多列，这些列通常对于做预测是有用的。所以这是一个可以奏效的方法。然而，第三名获奖者几乎没有进行特征工程，而且他们也有一个大的疏忽，这可能导致他们失去第一名。随着比赛的进行，我们将学到更多关于如何赢得这场比赛以及类似比赛的知识。'
- en: Importance of good validation set [[44:53](https://youtu.be/YSFG_W8JxBo?t=44m53s)]
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 好的验证集的重要性
- en: If you do not have a good validation set, it is hard, if not impossible, to
    create a good model. If you are trying to predict next month’s sales and you build
    models. If you have no way of knowing whether the models you have built are good
    at predicting sales a month ahead of time, then you have no way of knowing whether
    it is actually going to be any good when you put your model in production. You
    need a validation set that you know is reliable at telling you whether or not
    your model is likely to work well when you put it in production or use it on the
    test set.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有一个好的验证集，要创建一个好的模型是困难的，甚至是不可能的。如果你试图预测下个月的销售额，并建立模型。如果你无法知道你建立的模型是否擅长提前一个月预测销售额，那么当你将模型投入生产或在测试集上使用时，你就无法知道它是否真的会很好。你需要一个可靠的验证集，告诉你你的模型是否有可能在投入生产或在测试集上使用时表现良好。
- en: Normally you should not use your test set for anything other than using it right
    at the end of the competition or right at the end of the project to find out how
    you did. But there is one thing you can use the test set for in addition — that
    is to **calibrate your validation set** [[46:02](https://youtu.be/YSFG_W8JxBo?t=46m2s)].
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，你不应该对测试集做任何其他操作，除非在比赛结束时或项目结束时使用它来查看你的表现。但是有一件事你可以在测试集中使用 —— 那就是**校准你的验证集**[[46:02](https://youtu.be/YSFG_W8JxBo?t=46m2s)]。
- en: '![](../Images/f8963cb5444c2b184ca2fd349c6a28b0.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8963cb5444c2b184ca2fd349c6a28b0.png)'
- en: What Terrance did here was that he built four different models and submitted
    each of the four models to Kaggle to find out its score. X-axis is the score Kaggle
    told us on the leaderboard, and y-axis he plotted the score on a particular validation
    set he was trying out to see whether the validation set was going to be any good.
    If your validation set is good, then the relationship between the leaderboards
    score (i.e. the test set score) should lie in a straight line. Ideally, it will
    lie on the `y = x` line, but honestly that does not matter too much as long as
    relatively speaking it tells you which models are better than which other models,
    then you know which model is the best. In this case, Terrance has managed to come
    up with a validation set which looks like it is going to predict the Kaggle leaderboard
    score well. That is really cool because he can go away and try a hundred different
    types of models, feature engineering, weighting, tweaks, hyper parameters, whatever
    else, see how they go on the validation set, and not have to submit to Kaggle.
    So you will get a lot more iterations, a lot more feedback. This is not just true
    for Kaggle but every machine learning project you do. In general, if your validation
    set is not showing nice fit line, you need think carefully [[48:02](https://youtu.be/YSFG_W8JxBo?t=48m2s)].
    How is the test set constructed? How is my validation set different? You will
    have to draw lots of charts and so forth to find out.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Terrance在这里做的是建立了四种不同的模型，并将这四种模型分别提交到Kaggle上，以找出它们的得分。X轴是Kaggle在排行榜上告诉我们的得分，y轴是他在一个特定的验证集上绘制的得分，他试图看看这个验证集是否会很好。如果你的验证集很好，那么排行榜得分（即测试集得分）之间的关系应该是一条直线。理想情况下，它将位于`y
    = x`线上，但老实说，这并不太重要，只要相对来说告诉你哪些模型比哪些模型更好，那么你就知道哪个模型是最好的。在这种情况下，Terrance设法找到了一个看起来能够很好地预测Kaggle排行榜得分的验证集。这真的很酷，因为他可以尝试一百种不同类型的模型、特征工程、加权、调整、超参数等等，看看它们在验证集上的表现，而不必提交到Kaggle。因此，你将得到更多的迭代，更多的反馈。这不仅适用于Kaggle，而且适用于你做的每一个机器学习项目。一般来说，如果你的验证集没有显示出良好的拟合线，你需要仔细思考[[48:02](https://youtu.be/YSFG_W8JxBo?t=48m2s)]。测试集是如何构建的？我的验证集有什么不同？你将不得不绘制很多图表等等来找出。
- en: '**Question:** How do you construct a validation set as close to the test set
    [[48:23](https://youtu.be/YSFG_W8JxBo?t=48m23s)]? Here are a few tips from Terrance:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：**如何构建一个与测试集尽可能接近的验证集[[48:23](https://youtu.be/YSFG_W8JxBo?t=48m23s)]？以下是Terrance的一些建议：'
- en: Close by date (i.e. most recent)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期接近（即最近的）
- en: First looked at the date range of the test set (16 days), then looked at the
    date range of the kernel which described how to get 0.58 on the leaderboard by
    taking an average (14 days).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先看一下测试集的日期范围（16天），然后看一下描述如何在排行榜上获得0.58分的内核的日期范围（14天）。
- en: Test set begins on the day after pay day and ends on a pay day.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集从发薪日的第二天开始，到下一个发薪日结束。
- en: Plot lots of pictures. Even if you did not know it was pay day, you want to
    draw the time series chart and hopefully see that every two weeks there is a spike
    and make sure that you have the same number of spikes in the validation set as
    the test set.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制很多图片。即使你不知道今天是发薪日，你也想绘制时间序列图，希望看到每两周有一个高峰，并确保验证集中有与测试集相同数量的高峰。
- en: Interpreting machine learning models [[50:38](https://youtu.be/YSFG_W8JxBo?t=50m38s)
    / [Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson2-rf_interpretation.ipynb)]
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释机器学习模型[[50:38](https://youtu.be/YSFG_W8JxBo?t=50m38s) / [笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson2-rf_interpretation.ipynb)]
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We start by reading in our feather files for Blue Books for Bulldozers competition.
    Reminder: we have already read in the CSV, processed it into categories, and save
    it in feather format. The next thing we do is call `proc_df` to turn categories
    into integers, deal with missing values, and pull out the dependent variable.
    Then create a validation set just like last week:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先读取蓝色书籍对推土机比赛的feather文件。提醒：我们已经读取了CSV文件，将其处理为类别，并保存为feather格式。接下来我们调用`proc_df`将类别转换为整数，处理缺失值，并提取出因变量。然后创建一个像上周一样的验证集：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Detour to lesson 1 notebook [[51:59](https://youtu.be/YSFG_W8JxBo?t=51m59s)]
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绕道到第1课笔记本[[51:59](https://youtu.be/YSFG_W8JxBo?t=51m59s)]
- en: Last week, there was a bug in `proc_df` that was shuffling the dataframe when
    `subset` gets passed in hence causing the validation set to be not the latest
    12000 records. This issue was fixed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 上周，在`proc_df`中有一个bug，当传入`subset`时会打乱数据框，导致验证集不是最新的12000条记录。这个问题已经修复。
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Question**: Why is `nas` both input and out put of this function [[53:03](https://youtu.be/YSFG_W8JxBo?t=53m3s)]?
    `proc_df` returns a dictionary telling you which columns were missing and for
    each of those columns what the median was.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：为什么`nas`既是该函数的输入又是输出[[53:03](https://youtu.be/YSFG_W8JxBo?t=53m3s)]？`proc_df`返回一个告诉您哪些列丢失以及每个丢失列的中位数的字典。'
- en: When you call `proc_df` on a larger dataset, you do not pass in `nas` but you
    want to keep that return value.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您在较大的数据集上调用`proc_df`时，不需要传入`nas`，但您希望保留该返回值。
- en: Later on, when you want to create a subset (by passing in `subset`), you want
    to use the same missing columns and medians, so you pass `nas` in.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稍后，当您想要创建一个子集（通过传入`subset`）时，您希望使用相同的丢失列和中位数，因此您传入`nas`。
- en: If it turns out that the subset was from a whole different dataset and had different
    missing columns, it would update the dictionary with additional key value.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果发现子集来自完全不同的数据集并且具有不同的丢失列，它将使用附加键值更新字典。
- en: It keeps track of any missing columns you came across in anything you passed
    to `proc_df` .
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它会跟踪您在传递给`proc_df`的任何内容中遇到的任何丢失列。
- en: Back to lesson 2 notebook [[54:40](https://youtu.be/YSFG_W8JxBo?t=54m40s)]
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回到第2课笔记本[[54:40](https://youtu.be/YSFG_W8JxBo?t=54m40s)]
- en: Once we have done `proc_df`, this is what it looks like. `SalePrice` is the
    log of the sale price.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了`proc_df`，它看起来是这样的。`SalePrice`是销售价格的对数。
- en: '![](../Images/2ea7c5b8ef1b437bd8868798dc7e61f6.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ea7c5b8ef1b437bd8868798dc7e61f6.png)'
- en: We already know how to get the prediction. We take the average value in each
    leaf node in each tree after running a particular row through each tree. Normally,
    we do not just want a prediction — we also want to know how confident we are of
    that prediction.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道如何进行预测。我们在通过每棵树运行特定行后，在每棵树的每个叶节点中取平均值。通常，我们不仅想要一个预测 - 我们还想知道我们对该预测的信心有多大。
- en: We would be less confident of a prediction if we have not seen many examples
    of rows like this one. In that case, we would not expect any of the trees to have
    a path through — which is designed to help us predict that row. So conceptually,
    you would expect then that as you pass this unusual row through different trees,
    it is going to end up in very different places. In other words, rather than just
    taking the mean of the predictions of the trees and saying that is our prediction,
    what if we took the standard deviation of the predictions of the trees? If the
    standard deviation is high, that means each tree is giving us a very different
    estimate of this row’s prediction. If this was a really common kind of row, the
    trees would have learned to make good predictions for it because it has seen lots
    of opportunities to split based on those kind of rows. So the standard deviation
    of the predictions across the trees gives us at least relative understanding of
    how confident we are of this prediction [[56:39](https://youtu.be/YSFG_W8JxBo?t=56m39s)].
    This is not something which exists in scikit-learn, so we have to create it. But
    we already have almost the exact code we need.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有看到许多类似这一行的示例，我们对预测会更不自信。在这种情况下，我们不希望任何树都通过 - 这有助于我们预测该行。因此，在概念上，您会期望当您通过不同树传递此不寻常的行时，它会最终出现在非常不同的位置。换句话说，与其只取树的预测平均值并说这是我们的预测，不如我们取树的预测标准差呢？如果标准差很高，这意味着每棵树都为我们提供了对该行预测的非常不同的估计。如果这是一种非常常见的行，树将已经学会为其做出良好的预测，因为它已经看到了许多基于这些行的分割机会。因此，跨树的预测标准差至少让我们相对了解我们对该预测的信心有多大[[56:39](https://youtu.be/YSFG_W8JxBo?t=56m39s)]。这在scikit-learn中不存在，因此我们必须创建它。但我们已经有几乎需要的确切代码。
- en: For model interpretation, there is no need to use the full dataset because we
    do not need a massively accurate random forest — we just need one which indicates
    the nature of relationships involved.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型解释，没有必要使用完整的数据集，因为我们不需要一个非常准确的随机森林 - 我们只需要一个指示所涉及关系性质的随机森林。
- en: Just make sure the sample size is large enough that if you call the same interpretation
    commands multiple times, you do not get different results back each time. In practice,
    50,000 is a high number and it would be surprising if that was not enough (and
    it runs in seconds).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 只需确保样本大小足够大，以便如果多次调用相同的解释命令，每次都不会得到不同的结果。在实践中，50,000是一个很高的数字，如果这还不够的话会令人惊讶（而且运行时间只需几秒）。
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is where we can do the exact same list comprehension as the last time
    [[58:35](https://youtu.be/YSFG_W8JxBo?t=58m35s)]:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们可以做与上次完全相同的列表推导[[58:35](https://youtu.be/YSFG_W8JxBo?t=58m35s)]：
- en: '[PRE19]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is how to do it for one observation. This takes quite a while and specifically,
    it is not taking advantage of the fact that my computer has lots of cores in it.
    List comprehensions itself if Python code and Python code (unless you are doing
    something special) runs in serial which means it runs on a single CPU and does
    not take advantage of your multi CPU hardware. If we wanted to run this on more
    trees and more data, the execution time goes up. Wall time (the amount of actual
    time it took) is roughly equal to the CPU time where else if it was running on
    lots of cores, the CPU time would be higher than the wall time [[1:00:05](https://youtu.be/YSFG_W8JxBo?t=1h5s)].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是针对一个观察结果的方法。这需要相当长的时间，特别是它没有充分利用我的计算机有很多核心这一事实。列表推导本身是Python代码，Python代码（除非您在做一些特殊的事情）运行在串行模式下，这意味着它在单个CPU上运行，不利用您的多CPU硬件。如果我们想在更多树和更多数据上运行此代码，执行时间会增加。墙上时间（实际花费的时间）大致等于CPU时间，否则如果它在许多核心上运行，CPU时间将高于墙上时间[[1:00:05](https://youtu.be/YSFG_W8JxBo?t=1h5s)]。
- en: 'It turns out Fast.ai library provides a handy function called `parallel_trees`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 原来Fast.ai库提供了一个方便的函数称为`parallel_trees`：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`parallel_trees` takes a random forest model `m` and some function to call
    (here, it is `get_preds`). This calls this function on every tree in parallel.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parallel_trees`接受一个随机森林模型`m`和要调用的某个函数（这里是`get_preds`）。这会并行在每棵树上调用此函数。'
- en: It will return a list of the result of applying that function to every tree.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用该函数对每棵树应用后返回一个结果列表。
- en: This will cut down the wall time to 500 milliseconds and giving exactly the
    same answer. Time permitting, we will talk about more general ways of writing
    code that runs in parallel which is super useful for data science, but here is
    one that we can use for random forests.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将把墙上的时间缩短到500毫秒，并给出完全相同的答案。如果时间允许，我们将讨论更一般的编写并行代码的方法，这对数据科学非常有用，但这里有一种我们可以用于随机森林的方法。
- en: Plotting [[1:02:02](https://youtu.be/YSFG_W8JxBo?t=1h2m2s)]
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制[[1:02:02](https://youtu.be/YSFG_W8JxBo?t=1h2m2s)]
- en: 'We will first create a copy of the data and add the standard deviation of the
    predictions and predictions themselves (the mean) as new columns:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将数据复制一份，并将预测的标准差和预测本身（均值）作为新列添加进去：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/d252129ec3961b66bd1bdb82125af420.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d252129ec3961b66bd1bdb82125af420.png)'
- en: You might remember from last lesson that one of the predictors we have is called
    `Enclosure` and this is an important one as we will see later. Let’s start by
    doing a histogram. One of the nice things about Pandas is it has built-in [plotting
    capabilities](https://pandas.pydata.org/pandas-docs/stable/visualization.html).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得上一课中我们有一个叫做`Enclosure`的预测变量，这是一个重要的变量，我们稍后会看到。让我们从做一个直方图开始。Pandas的一个好处是它具有内置的[绘图功能](https://pandas.pydata.org/pandas-docs/stable/visualization.html)。
- en: '**Question**: Can you remind me what enclosure is [[01:02:50](https://youtu.be/YSFG_W8JxBo?t=1h2m50s)]?
    We do not know what it means and it does not matter. The whole purpose of this
    process is that we are going to learn about what things are (or at least what
    things are important and later on figure out what they are and how they are important).
    So we will start out knowing nothing about this dataset. We are just going to
    look at something called `Enclosure` that has something called `EROPS` and `ROPS`
    and we do not even know what this is yet. All we know is that the only three that
    appear in any great quantity are `OROPS`, `EROPS w AC`, and `EROPS`. This is very
    common as a data scientist. You often find yourself looking at data that you are
    not that familiar with and you have to figure out which bits to study more carefully,
    which bits seem to matter, and so forth. In this case, at least know that `EROPS
    AC`, `NO ROPS`, and `None or Unspecified` we really do not care about because
    they basically do not exist. So we will focus on `OROPS`, `EROPS w AC`, and `EROPS`.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：你能提醒我围栏是什么吗[[01:02:50](https://youtu.be/YSFG_W8JxBo?t=1h2m50s)]？我们不知道它的意思，也不重要。这个过程的整个目的是我们将学习关于事物是什么（或者至少重要的事物，然后弄清楚它们是什么以及它们的重要性）。所以我们一开始对这个数据集一无所知。我们只是要看一下一个叫做`Enclosure`的东西，里面有一个叫做`EROPS`和`ROPS`的东西，我们甚至还不知道这是什么。我们只知道在任何大量出现的情况下，只有`OROPS`、`EROPS
    w AC`和`EROPS`。这在数据科学家中非常常见。你经常发现自己在看一些你不太熟悉的数据，并且你必须弄清楚哪些部分需要更仔细地研究，哪些部分似乎很重要，等等。在这种情况下，至少知道`EROPS
    AC`、`NO ROPS`和`None or Unspecified`我们真的不关心，因为它们基本上不存在。所以我们将关注`OROPS`、`EROPS w
    AC`和`EROPS`。'
- en: 'Here we took our data frame, grouped by `Enclosure`, then took average of 3
    fields [[1:04:00](https://youtu.be/YSFG_W8JxBo?t=1h4m)]:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们取了我们的数据框，按`Enclosure`分组，然后取了3个字段的平均值[[1:04:00](https://youtu.be/YSFG_W8JxBo?t=1h4m)]:'
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/bd58ac7ad5332548ad0bf6dd86a15ccd.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd58ac7ad5332548ad0bf6dd86a15ccd.png)'
- en: 'We can already start to learn a little here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经开始在这里学习一点：
- en: Prediction and the sale price are close to each other on average (good sign)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测和销售价格平均接近（好迹象）
- en: Standard deviation varies a little bit
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准差有些变化
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/5341d4e30fe46779f92e2e28d24e0753.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5341d4e30fe46779f92e2e28d24e0753.png)'
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/308feaabedab15f69099285ec08021c1.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/308feaabedab15f69099285ec08021c1.png)'
- en: 'We used the standard deviation of prediction for the error bars above. This
    will tell us if there is some groups or some rows that we are not very confident
    of at all. We could do something similar for product size:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了上面预测的标准差来绘制误差线。这将告诉我们是否有一些组或一些行我们并不是很有信心。我们可以对产品尺寸做类似的事情：
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/bf732a0447aa7def8c890114c0bd8e3e.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf732a0447aa7def8c890114c0bd8e3e.png)'
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/0375cf222fc5c14c245b092841beb128.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0375cf222fc5c14c245b092841beb128.png)'
- en: You expect, on average, when you are predicting something that is a bigger number
    your standard deviation would be higher. So you can sort by the ratio of the standard
    deviation of the predictions to the predictions themselves [[1:05:51](https://youtu.be/YSFG_W8JxBo?t=1h5m51s)].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你期望，平均而言，当你预测一个更大的数字时，你的标准差会更高。所以你可以按照预测的标准差与预测本身的比率排序[[1:05:51](https://youtu.be/YSFG_W8JxBo?t=1h5m51s)]。
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/ee1e1a26bc04f8cf89be9a391af7cbdb.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee1e1a26bc04f8cf89be9a391af7cbdb.png)'
- en: What this tells us is that product size `Large` and `Compact` , our predictions
    are less accurate (relatively speaking as a ratio of the total price). So if we
    go back and have a look, you see why. These are the smallest groups in the histogram.
    As you would expect, in small groups, we are doing a less good job.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们的是产品尺寸`Large`和`Compact`，我们的预测不太准确（相对于总价格的比率）。所以如果我们回头看一下，你会明白为什么。这些是直方图中最小的组。正如你所期望的，在小组中，我们的工作效果不太好。
- en: 'You can use this confidence interval for two main purposes:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用这个置信区间做两个主要目的：
- en: You can look at the average confidence interval by group to find out if there
    are groups you do not seem to have confidence about.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以按组查看平均置信区间，以找出你似乎不太有信心的组。
- en: Perhaps more importantly, you can look at them for specific rows. When you put
    it in production, you might always want to see the confidence interval. For example,
    if you are doing credit scoring to decide whether to give somebody a loan, you
    probably want to see not only what their level of risk is but how confident we
    are. If they want to borrow lots of money and we are not at all confident about
    our ability to predict whether they will pay back, we might want to give them
    a smaller loan.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 也许更重要的是，你可以查看特定行的重要性。当你投入生产时，你可能总是想看到置信区间。例如，如果你正在进行信用评分来决定是否给某人贷款，你可能不仅想知道他们的风险水平，还想知道我们有多大的信心。如果他们想借很多钱，而我们对我们的预测能力毫无信心，我们可能会给他们较小的贷款。
- en: Feature importance [[1:07:20](https://youtu.be/YSFG_W8JxBo?t=1h7m20s)]
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征重要性 [[1:07:20](https://youtu.be/YSFG_W8JxBo?t=1h7m20s)]
- en: I always look at feature importance first in practice. Whether I’m working on
    a Kaggle competition or a real world project, I build a random forest as fast
    as I can, trying to get it to the point that is significantly better than random
    but doesn’t have to be much better than that. And the next thing I do is to plot
    the feature importance.
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在实践中，我总是首先查看特征重要性。无论我是在参加 Kaggle 竞赛还是在进行真实世界项目，我都会尽快构建一个随机森林，试图让它达到明显优于随机的水平，但不必比那更好太多。接下来我要做的事情是绘制特征重要性。
- en: The feature importance tells us in this random forest, which columns mattered.
    We have dozens of columns in this dataset, and here, we are picking out the top
    10\. `rf_feat_importance` is part of Fast.ai library which takes a model `m` and
    dataframe `df_trn` (because we need to know names of columns) and it will give
    you back a Pandas dataframe showing you in order of importance how important each
    column was.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性告诉我们在这个随机森林中，哪些列很重要。在这个数据集中有几十列，而在这里，我们挑选出前十个。`rf_feat_importance` 是 Fast.ai
    库的一部分，它接受一个模型 `m` 和一个数据框 `df_trn`（因为我们需要知道列的名称），然后会返回一个 Pandas 数据框，按重要性顺序显示每列的重要性。
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](../Images/cac676a1c93aa74c8505e2ac05395602.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cac676a1c93aa74c8505e2ac05395602.png)'
- en: '[PRE29]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](../Images/c729d0eda2fd7ad4756792f82ad2673a.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c729d0eda2fd7ad4756792f82ad2673a.png)'
- en: Since `fi` is a `DataFrame`, we can use `DataFrame` plotting commands [[1:09:00](https://youtu.be/YSFG_W8JxBo?t=1h9m)].
    The important thing is to see that some columns are really important and most
    columns do not really matter at all. In nearly every dataset you use in real life,
    this is what your feature importance is going to look like. There is only a handful
    of columns that you care about, and this is why Jeremy always starts here. At
    this point, in terms of looking into learning about this domain of heavy industrial
    equipment auctions, we only have to care about learning about the columns which
    matter. Are we going to bother learning about `Enclosure`? Depends whether `Enclosure`
    is important. It turns out that it appears in top 10, so we are going to have
    to learn about `Enclosure`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `fi` 是一个 `DataFrame`，我们可以使用 `DataFrame` 绘图命令 [[1:09:00](https://youtu.be/YSFG_W8JxBo?t=1h9m)]。重要的是要看到一些列真的很重要，而大多数列实际上并不重要。在你在现实生活中使用的几乎每个数据集中，你的特征重要性都会是这个样子。只有少数几列是你关心的，这就是为什么
    Jeremy 总是从这里开始的原因。在这一点上，就学习这个重型工业设备拍卖领域，我们只需要关心那些重要的列。我们是否要去了解 `Enclosure`？取决于
    `Enclosure` 是否重要。结果表明它出现在前十名，所以我们需要了解 `Enclosure`。
- en: 'We can also plot this as a bar plot:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将其绘制为条形图：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](../Images/7fe9dce9aba9b05771f592a1d5bb56ac.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fe9dce9aba9b05771f592a1d5bb56ac.png)'
- en: The most important thing to do with this is to now sit down with your client,
    your data dictionary, or whatever your source of information is and say to then
    “okay, tell me about `YearMade`. What does that mean? Where does it come from?”
    [[1:10:31](https://youtu.be/YSFG_W8JxBo?t=1h10m31s)] Plot lots of things like
    histogram of `YearMade` and scatter plot of `YearMade` against price and learn
    everything you can because `YearMade` and `Coupler_System` — they are the things
    that matter.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在最重要的事情是和你的客户、数据字典，或者任何你的信息来源坐下来，然后说“好的，告诉我关于 `YearMade`。那是什么意思？它来自哪里？”[[1:10:31](https://youtu.be/YSFG_W8JxBo?t=1h10m31s)]
    绘制很多东西，比如 `YearMade` 的直方图和 `YearMade` 与价格的散点图，尽可能学到更多，因为 `YearMade` 和 `Coupler_System`
    —— 这些才是重要的事情。
- en: What will often happen in real-world projects is that you sit with the the client
    and you’ll say “it turns out the `Coupler_System` is the second most important
    thing” and they might say “that makes no sense.” That doesn’t mean that there
    is a problem with your model, it means there is a problem with their understanding
    of the data they gave you.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实项目中经常发生的情况是，你和客户坐在一起，你会说“结果 `Coupler_System` 是第二重要的事情”，而他们可能会说“这毫无意义”。这并不意味着你的模型有问题，而是意味着他们对他们给你的数据的理解有问题。
- en: Let me give you an example [[1:11:16](https://youtu.be/YSFG_W8JxBo?t=1h11m16s)].
    I entered a Kaggle competition where the goal was to predict which applications
    for grants at a university would be successful. I used this exact approach and
    I discovered a number of columns which were almost entirely predictive of the
    dependent variable. Specifically, when I then looked to see in what way they are
    predictive, it turned out whether they were missing or not was the only thing
    that mattered in his dataset. I ended up winning that competition thanks to this
    insight. Later on, I heard what had happened. It turns out that at that university,
    there is an administrative burden to fill any other database and so for a lot
    of the grant applications, they do not fill in the database for the folks whose
    applications were not accepted. In other words, these missing values in the dataset
    were saying this grand wasn’t accepted because if it was accepted then the admin
    folks will go in and type in that information. This is what we call **data leakage**.
    Data leakage means there is information in the dataset that I was modeling with
    which the university would not have had in real life at that point in time they
    were making a decision. When they are actually deciding which grant applications
    to prioritize, they do not know which ones the admin staff will later on going
    to add information to because it turns out that they were accepted.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: One of the key things you will find here is data leakage problems and that is
    a serious problem you need to deal with [[1:12:56](https://youtu.be/YSFG_W8JxBo?t=1h12m56s)].
    The other thing that will happen is you will often find its signs of collinearity.
    It seems like what happened with `Coupler_System`. `Coupler_System` tells you
    whether or not a particular kind of heavy industrial equipment has a particular
    feature on it. But if it is not that kind of industrial equipment at all, it will
    be missing. So it indicates whether or not it is a certain class of heavy industrial
    equipment. This is not data leakage. This is an actual information you actually
    have at the right time. You just have to be careful interpreting it. So you should
    go through at least the top 10 or look for where the natural break points are
    and really study these things carefully.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: To make life easier, it is sometimes good to throw some data away and see if
    it make any difference. In this case, we have a random forest which was .889 r².
    Here we filter out those where the importance is equal to or less than 0.005 (i.e.
    only keep the one whose importance is greater than 0.005).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The r² did not change much — it actually increased a tiny bit. Generally speaking,
    removing redundant columns should not make it worse. If f it makes it worse, they
    were not redundant after all. It might make it a little bit better because if
    you think about how we built these trees, when it is deciding what to split on,
    it has less things to worry about trying, it is less often going to accidentally
    find a crappy column. So there is slightly better opportunity to create a slightly
    better tree with slightly less data, but it is not going to change it by much.
    But it is going to make it a bit faster and it is going to let us focus on what
    matters. Let’s re-run feature importance on this new result [[1:15:49](https://youtu.be/YSFG_W8JxBo?t=1h15m49s)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](../Images/ec4d6bc1a56fcf8d47731b874bd338f5.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: Key thing that has happened is that when you remove redundant columns, you are
    also removing sources of collinearity. In other words, two columns that might
    be related to each other. Collinearity does not make your random forests less
    predictive, but if you have a column A is a little bit related to a column B,
    and B is a strong driver of the independent, what happens is that the importance
    is going to be split between A and B. By removing some of those columns with very
    little impact, it makes your feature importance plot clearer. Before `YearMade`
    was pretty close to `Coupler_System`. But there must have been a bunch of things
    that are collinear with `YearMade` and now you can see `YearMade` really matters.
    This feature importance plot is more reliable than the one before because it has
    a lot less collinearity to confuse us.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 发生的关键事情是，当你移除冗余列时，你也在移除共线性的来源。换句话说，可能彼此相关的两列。共线性不会使你的随机森林更少预测，但如果A列与B列稍微相关，而B是独立变量的一个强驱动因素，那么重要性将在A和B之间分配。通过移除一些对结果影响很小的列，使得你的特征重要性图更清晰。之前`YearMade`与`Coupler_System`相当接近。但肯定有一堆与`YearMade`共线的东西，现在你可以看到`YearMade`真的很重要。这个特征重要性图比之前更可靠，因为它减少了很多共线性，不会让我们感到困惑。
- en: Let’s talk about how this works [[1:17:21](https://youtu.be/YSFG_W8JxBo?t=1h17m21s)]
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们谈谈这是如何运作的[[1:17:21](https://youtu.be/YSFG_W8JxBo?t=1h17m21s)]
- en: Not only is it really simple, it is a technique you can use not just for random
    forests but for basically any kind of machine learning model. Interestingly, almost
    no one knows this. Many people will tell you there is no way of interpreting this
    particular kind of model (the most important interpretation of a model is knowing
    which things are important) and that is almost certainly not going to be true
    because the technique I am going to teach you actually works for any kind of models.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅非常简单，而且是一种你可以用于任何类型的机器学习模型的技术。有趣的是，几乎没有人知道这一点。许多人会告诉你，没有办法解释这种特定类型的模型（模型的最重要解释是知道哪些因素是重要的），这几乎肯定不会是真的，因为我要教给你的技术实际上适用于任何类型的模型。
- en: '![](../Images/65e55dcc799c070470c01756ad879a05.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65e55dcc799c070470c01756ad879a05.png)'
- en: We take our bulldozer data set and we have a column `Price` we are trying to
    predict (dependent variable).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们拿我们的推土机数据集，我们有一个列`Price`我们正在尝试预测（因变量）。
- en: We have 25 independent variables and one of them is `YearMade`.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有25个自变量，其中之一是`YearMade`。
- en: How do we figure out how important `YearMade` is? We have a whole random forest
    and we can find out our predictive accuracy. So we will put all these rows through
    our random forest, and it will spit out some predictions. We will then compare
    them to the actual price (in this case, we get our root mean squared error and
    r²). This is our starting point.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何确定`YearMade`有多重要？我们有一个完整的随机森林，我们可以找出我们的预测准确性。因此，我们将把所有这些行通过我们的随机森林，它将输出一些预测。然后我们将它们与实际价格进行比较（在这种情况下，我们得到我们的均方根误差和r²）。这是我们的起点。
- en: Let’s do exactly the same thing, but this time, take the `YearMade` column and
    randomly shuffle it (i.e. randomly permute just that column). Now `YearMade` has
    exactly the same distribution as before (same mean, same standard deviation).
    But it has no relationships with our dependent variable at all because we totally
    randomly reordered it.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们做完全相同的事情，但这次，拿`YearMade`列并随机洗牌它（即随机排列只是那一列）。现在`YearMade`与之前完全相同的分布（相同的均值，相同的标准差）。但它与我们的因变量没有任何关系，因为我们完全随机重新排序了它。
- en: Before, we might have found our r² was .89\. After we shuffle `YearMade`, we
    check again, and now r² is .80\. The score got much worse when we destroyed that
    variable.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前，我们可能发现我们的r²是0.89。在我们洗牌`YearMade`之后，我们再次检查，现在r²是0.80。当我们破坏那个变量时，得分变得更糟了。
- en: Okay, let’s try again. We put `YearMade` back to how it was, and this time let’s
    take `Enclosure` and shuffle that. This time, r²is .84 and we can say the amount
    of decrease in our score for `YearMade` was .09 and the amount of decrease for
    `Enclosure` was .05\. And this is going to give us our feature importances for
    each column.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好的，让我们再试一次。我们把`YearMade`恢复到原来的状态，这次让我们拿`Enclosure`来洗牌。这次，r²是0.84，我们可以说`YearMade`得分减少了0.09，而`Enclosure`的得分减少了0.05。这将为我们提供每一列的特征重要性。
- en: '**Question**: Can’t we just exclude the column and check the decay in performance
    [[1:20:31](https://youtu.be/YSFG_W8JxBo?t=1h20m31s)]? You could remove the column
    and train a whole new random forest, but that is going to be really slow. Where
    else this way, we can keep our random forest and just test the predictive accuracy
    of it again. So this is nice and fast by comparison. In this case, we just have
    to rerun every row forward through the forest for each shuffled column.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我们不能只排除这一列然后检查性能的下降吗[[1:20:31](https://youtu.be/YSFG_W8JxBo?t=1h20m31s)]？你可以删除该列并训练一个全新的随机森林，但那将会非常慢。而通过这种方式，我们可以保留我们的随机森林，再次测试其预测准确性。因此，相比之下，这种方式更快速。在这种情况下，我们只需将每个洗牌列的每一行再次通过森林运行一遍。'
- en: '**Question**: If you want to do multi-collinearity, would you do two of them
    and random shuffle and then three of them [[1:21:12](https://youtu.be/YSFG_W8JxBo?t=1h21m12s)]?
    I don’t think you mean multi-collinearity, I think you mean looking for interaction
    effects. So if you want to say which pairs of variables are most important, you
    could do exactly the same thing each pair in turn. In practice, there are better
    ways to do that because that is obviously computationally pretty expensive and
    so we will try to find time to do that if we can.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：如果你想做多重共线性，你会做两个然后随机洗牌，然后三个[[1:21:12](https://youtu.be/YSFG_W8JxBo?t=1h21m12s)]？我认为你不是指多重共线性，我认为你是指寻找交互效应。因此，如果你想知道哪些变量对是最重要的，你可以依次对每对变量做完全相同的事情。实际上，有更好的方法来做到这一点，因为显然这在计算上是非常昂贵的，所以如果可能的话，我们将尝试找时间来做到这一点。'
- en: 'We now have a model which is a little bit more accurate and we have learned
    a lot more about it. So we are out of time and what I would suggest you try doing
    now before next class for this bulldozers dataset is going through the top 5 or
    10 predictors and try and learn what you can about how to draw plots in Pandas
    and try to come back with some insights about things like:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个稍微更准确的模型，我们对它了解更多。所以我们时间不够了，我建议你在下一堂课之前尝试做的是，查看前5或10个预测变量，尝试学习如何在Pandas中绘制图表，并尝试回来带一些关于以下事项的见解：
- en: what is the relationship between `YearMade` and the dependent variable
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`YearMade`和因变量之间的关系是什么'
- en: what is the histogram of `YearMade`
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`YearMade`的直方图是什么样的'
- en: now that you know `YearMade` is really important, check if there is some noise
    in that column which we could fix
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在你知道`YearMade`非常重要，检查一下这一列中是否有一些噪音，我们可以修复。
- en: Check if there is some weird encoding in that column we can fix
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查一下这一列中是否有一些奇怪的编码问题，我们可以修复。
- en: This idea Jeremy had that maybe `Coupler_System` is there entirely because it
    is collinear with something else, you might want try and figure out if it’s true.
    If so, how would you do it?
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeremy提出的这个想法，也许`Coupler_System`完全存在是因为它与其他某些东西共线，你可能想要尝试弄清楚这是否属实。如果是这样，你会怎么做呢？
- en: '`fiProductClassDesc` that rings alarm bells — it sounds like it might be a
    high cardinality categorical variable. It might be something with lots and lots
    levels because it sounds like it is a model name. So go and have a look at that
    model name — does it have some order into it? Could you make it an ordinal variable
    to make it better? Does it have some kind of hierarchical structure in the string
    that we can split it on hyphen to create more sub columns.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fiProductClassDesc`这个让人警惕的名字——听起来可能是一个高基数分类变量。它可能是一个有很多级别的东西，因为它听起来像是一个型号名称。所以去看看那个型号名称——它有一定的顺序吗？你能把它变成一个有序变量吗？它在字符串中有一些层次结构，我们可以通过连字符拆分它来创建更多的子列。'
- en: Have a think about this. Try and make it so that by when you come back, you’ve
    got some new, ideally a better accuracy than what I just showed because you found
    some new insights or at least that you can tell the class about some things you
    have learnt about how heavy industrial equipment auctions work in practice.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想这个问题。试着让你回来时，你有一些新的，最好是比我刚刚展示的更准确的东西，因为你找到了一些新的见解，或者至少你可以告诉班上一些你学到的有关重型工业设备拍卖的实际工作方式的事情。
