- en: 'Machine Learning 1: Lesson 3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ 1ï¼šç¬¬3è¯¾
- en: åŸæ–‡ï¼š[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-3-fa4065d8cb1e](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-3-fa4065d8cb1e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-3-fa4065d8cb1e](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-3-fa4065d8cb1e)
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to â€œreallyâ€ understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»*[*æœºå™¨å­¦ä¹ è¯¾ç¨‹*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*ä¸­çš„ä¸ªäººç¬”è®°ã€‚éšç€æˆ‘ç»§ç»­å¤ä¹ è¯¾ç¨‹ä»¥â€œçœŸæ­£â€ç†è§£å®ƒï¼Œè¿™äº›ç¬”è®°å°†ç»§ç»­æ›´æ–°å’Œæ”¹è¿›ã€‚éå¸¸æ„Ÿè°¢*[*Jeremy*](https://twitter.com/jeremyphoward)*å’Œ*[*Rachel*](https://twitter.com/math_rachel)*ç»™äº†æˆ‘è¿™ä¸ªå­¦ä¹ çš„æœºä¼šã€‚*'
- en: 'What is covered in todayâ€™s lesson:'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»Šå¤©çš„è¯¾ç¨‹å†…å®¹ï¼š
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson2-rf_interpretation.ipynb)
    / [Video](https://youtu.be/YSFG_W8JxBo)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[ç¬”è®°æœ¬](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson2-rf_interpretation.ipynb)
    / [è§†é¢‘](https://youtu.be/YSFG_W8JxBo)'
- en: '**Understanding the data better by using machine learning**'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€šè¿‡ä½¿ç”¨æœºå™¨å­¦ä¹ æ›´å¥½åœ°ç†è§£æ•°æ®**'
- en: This idea is contrary to the common refrain that things like random forests
    are black boxes that hide meaning from us. The truth is quite the opposite. Random
    forests allow us to understand our data deeper and more quickly than traditional
    approaches.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæƒ³æ³•ä¸å¸¸è§çš„è¯´æ³•ç›¸åï¼Œå³éšæœºæ£®æ—ç­‰ä¸œè¥¿æ˜¯éšè—æˆ‘ä»¬æ„ä¹‰çš„é»‘åŒ£å­ã€‚äº‹å®æ°æ°ç›¸åã€‚éšæœºæ£®æ—è®©æˆ‘ä»¬æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´æ·±å…¥æ›´å¿«åœ°ç†è§£æˆ‘ä»¬çš„æ•°æ®ã€‚
- en: '**How to look at larger datasets**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•æŸ¥çœ‹æ›´å¤§çš„æ•°æ®é›†**'
- en: Dataset with over 100 million rows â€” [Grocery Forecasting](https://www.kaggle.com/c/favorita-grocery-sales-forecasting)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‹¥æœ‰è¶…è¿‡1äº¿è¡Œçš„æ•°æ®é›† - [æ‚è´§é”€å”®é¢„æµ‹](https://www.kaggle.com/c/favorita-grocery-sales-forecasting)
- en: '**Question**: When to use random forests [[2:41](https://youtu.be/YSFG_W8JxBo?t=2m41s)]?
    Cannot think of anything offhand that it is definitely not going to be at least
    somewhat useful. So it is always worth trying. The real question might be in what
    situation should we try other things as well, and the short answer to that is
    for unstructured data (image, sound, etc), you almost certainly want to try deep
    learning. For collaborative filtering model (groceries competition is of that
    kind), neither random forest nor deep learning approach is exactly what you want
    and you need to do some tweaks.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šä½•æ—¶ä½¿ç”¨éšæœºæ£®æ—[[2:41](https://youtu.be/YSFG_W8JxBo?t=2m41s)]ï¼Ÿæˆ‘æ— æ³•æƒ³åˆ°ä»»ä½•ç»å¯¹ä¸ä¼šè‡³å°‘æœ‰äº›ç”¨å¤„çš„æƒ…å†µã€‚å› æ­¤ï¼Œå€¼å¾—ä¸€è¯•ã€‚çœŸæ­£çš„é—®é¢˜å¯èƒ½æ˜¯åœ¨ä»€ä¹ˆæƒ…å†µä¸‹æˆ‘ä»¬åº”è¯¥å°è¯•å…¶ä»–æ–¹æ³•ï¼Œç®€çŸ­çš„ç­”æ¡ˆæ˜¯å¯¹äºéç»“æ„åŒ–æ•°æ®ï¼ˆå›¾åƒï¼Œå£°éŸ³ç­‰ï¼‰ï¼Œæ‚¨å‡ ä¹è‚¯å®šè¦å°è¯•æ·±åº¦å­¦ä¹ ã€‚å¯¹äºååŒè¿‡æ»¤æ¨¡å‹ï¼ˆæ‚è´§ç«èµ›å±äºè¿™ç§ç±»å‹ï¼‰ï¼Œéšæœºæ£®æ—å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•éƒ½ä¸æ˜¯æ‚¨æƒ³è¦çš„ï¼Œæ‚¨éœ€è¦åšä¸€äº›è°ƒæ•´ã€‚'
- en: Review of last week [[4:42](https://youtu.be/YSFG_W8JxBo?t=4m42s)]
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸Šå‘¨å›é¡¾[[4:42](https://youtu.be/YSFG_W8JxBo?t=4m42s)]
- en: Reading CSV took a minute or two, and we saved it to a feather format file.
    Feather format is almost the same format that it lives in RAM, so it is ridiculously
    fast to read and write. The first thing we do is in the lesson 2 notebook is to
    read in the feather format file.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¯»å–CSVèŠ±äº†ä¸€ä¸¤åˆ†é’Ÿï¼Œæˆ‘ä»¬å°†å…¶ä¿å­˜ä¸ºç¾½æ¯›æ ¼å¼æ–‡ä»¶ã€‚ç¾½æ¯›æ ¼å¼å‡ ä¹ä¸RAMä¸­çš„æ ¼å¼ç›¸åŒï¼Œå› æ­¤è¯»å†™é€Ÿåº¦éå¸¸å¿«ã€‚æˆ‘ä»¬åœ¨ç¬¬2è¯¾ç¬”è®°æœ¬ä¸­åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯è¯»å–ç¾½æ¯›æ ¼å¼æ–‡ä»¶ã€‚
- en: proc_df issue [[5:28](https://youtu.be/YSFG_W8JxBo?t=5m28s)]
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: proc_dfé—®é¢˜[[5:28](https://youtu.be/YSFG_W8JxBo?t=5m28s)]
- en: 'An interesting little issue that was brought up during the week is in `proc_df`
    function. `proc_df` function does the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€å‘¨æœŸé—´æå‡ºçš„ä¸€ä¸ªæœ‰è¶£çš„å°é—®é¢˜æ˜¯åœ¨`proc_df`å‡½æ•°ä¸­ã€‚`proc_df`å‡½æ•°æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: Finds numeric columns which have missing values and create an additional boolean
    column as well as replacing the missing with medians.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŸ¥æ‰¾å…·æœ‰ç¼ºå¤±å€¼çš„æ•°å€¼åˆ—ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé¢å¤–çš„å¸ƒå°”åˆ—ï¼ŒåŒæ—¶ç”¨ä¸­ä½æ•°æ›¿æ¢ç¼ºå¤±å€¼ã€‚
- en: Turn the categorical objects into integer codes.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†åˆ†ç±»å¯¹è±¡è½¬æ¢ä¸ºæ•´æ•°ä»£ç ã€‚
- en: '**Problem #1**: Your test set may have missing values in some columns that
    were not in your training set or vice versa. If that happens, you are going to
    get an error when you try to do the random forest since the â€œmissingâ€ boolean
    column appeared in your training set but not in the test set.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ï¼ƒ1**ï¼šæ‚¨çš„æµ‹è¯•é›†ä¸­å¯èƒ½æœ‰ä¸€äº›åˆ—ä¸­çš„ç¼ºå¤±å€¼ï¼Œè¿™äº›åˆ—åœ¨è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨ï¼Œåä¹‹äº¦ç„¶ã€‚å¦‚æœå‘ç”Ÿè¿™ç§æƒ…å†µï¼Œå½“æ‚¨å°è¯•è¿›è¡Œéšæœºæ£®æ—æ—¶ï¼Œæ‚¨å°†ä¼šå‡ºç°é”™è¯¯ï¼Œå› ä¸ºâ€œç¼ºå¤±â€å¸ƒå°”åˆ—å‡ºç°åœ¨è®­ç»ƒé›†ä¸­ï¼Œä½†ä¸åœ¨æµ‹è¯•é›†ä¸­ã€‚'
- en: '**Problem #2**: Median of the numeric value in the test set may be different
    from the training set. So it may process it into something which has different
    semantics.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ï¼ƒ2**ï¼šæµ‹è¯•é›†ä¸­æ•°å€¼çš„ä¸­ä½æ•°å¯èƒ½ä¸è®­ç»ƒé›†ä¸åŒã€‚å› æ­¤ï¼Œå®ƒå¯èƒ½å°†å…¶å¤„ç†ä¸ºå…·æœ‰ä¸åŒè¯­ä¹‰çš„å†…å®¹ã€‚'
- en: '**Solution**: There is now an additional return variable `nas` from `proc_df`
    which is a dictionary whose keys are the names of the columns that had missing
    values, and the values of the dictionary are the medians. Optionally, you can
    pass `nas` to `proc_df` as an argument to make sure that it adds those specific
    columns and uses those specific medians:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**è§£å†³æ–¹æ¡ˆ**ï¼šç°åœ¨æœ‰ä¸€ä¸ªé¢å¤–çš„è¿”å›å˜é‡`nas`ä»`proc_df`ï¼Œå®ƒæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶é”®æ˜¯å…·æœ‰ç¼ºå¤±å€¼çš„åˆ—çš„åç§°ï¼Œå­—å…¸çš„å€¼æ˜¯ä¸­ä½æ•°ã€‚å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥å°†`nas`ä½œä¸ºå‚æ•°ä¼ é€’ç»™`proc_df`ï¼Œä»¥ç¡®ä¿å®ƒæ·»åŠ è¿™äº›ç‰¹å®šåˆ—å¹¶ä½¿ç”¨è¿™äº›ç‰¹å®šä¸­ä½æ•°ï¼š'
- en: '`df, y, nas = proc_df(df_raw, ''SalePrice'', nas)`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`df, y, nas = proc_df(df_raw, ''SalePrice'', nas)`'
- en: CorporaciÃ³n Favorita Grocery Sales Forecasting [[9:25](https://youtu.be/YSFG_W8JxBo?t=9m25s)]
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CorporaciÃ³n Favoritaæ‚è´§é”€å”®é¢„æµ‹[[9:25](https://youtu.be/YSFG_W8JxBo?t=9m25s)]
- en: Letâ€™s walk through the same process when you are working with a really large
    dataset. It is almost the same but there are a few cases where we cannot use the
    defaults because defaults run a little bit too slowly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬èµ°ä¸€éå½“æ‚¨å¤„ç†ä¸€ä¸ªçœŸæ­£å¤§çš„æ•°æ®é›†æ—¶çš„ç›¸åŒè¿‡ç¨‹ã€‚å‡ ä¹ç›¸åŒï¼Œä½†æœ‰ä¸€äº›æƒ…å†µä¸‹æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨é»˜è®¤å€¼ï¼Œå› ä¸ºé»˜è®¤å€¼è¿è¡Œé€Ÿåº¦æœ‰ç‚¹æ…¢ã€‚
- en: 'It is important to be able to explain the problem you are working on. The key
    things to understand in a machine learning problem are:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿè§£é‡Šæ‚¨æ­£åœ¨å¤„ç†çš„é—®é¢˜æ˜¯å¾ˆé‡è¦çš„ã€‚åœ¨æœºå™¨å­¦ä¹ é—®é¢˜ä¸­ç†è§£çš„å…³é”®äº‹é¡¹æ˜¯ï¼š
- en: What are the independent variables?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‹¬ç«‹å˜é‡æ˜¯ä»€ä¹ˆï¼Ÿ
- en: What is the dependent variable (the thing you are trying to predict)?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å› å˜é‡ï¼ˆæ‚¨è¯•å›¾é¢„æµ‹çš„ä¸œè¥¿ï¼‰ï¼Ÿ
- en: In this competition
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ¯”èµ›ä¸­
- en: Dependent variable â€” how many units of each kind of product were sold in each
    store on each day during the two week period.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› å˜é‡ â€” åœ¨ä¸¤å‘¨æœŸé—´æ¯å¤©æ¯ä¸ªå•†åº—é”€å”®äº†å¤šå°‘ç§äº§å“ã€‚
- en: Independent variables â€” how many units of each product at each store on each
    day were sold in the last few years. For each store, where it is located and what
    class of store it is (metadata). For each type of product, what category of product
    it is, etc. For each date, we have metadata such as what the oil price was.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªå˜é‡ â€” è¿‡å»å‡ å¹´æ¯ä¸ªäº§å“åœ¨æ¯ä¸ªå•†åº—æ¯å¤©é”€å”®äº†å¤šå°‘å•ä½ã€‚å¯¹äºæ¯ä¸ªå•†åº—ï¼Œå®ƒçš„ä½ç½®åœ¨å“ªé‡Œä»¥åŠå®ƒæ˜¯ä»€ä¹ˆç±»å‹çš„å•†åº—ï¼ˆå…ƒæ•°æ®ï¼‰ã€‚å¯¹äºæ¯ç§äº§å“ï¼Œå®ƒæ˜¯ä»€ä¹ˆç±»åˆ«çš„äº§å“ç­‰ã€‚å¯¹äºæ¯ä¸ªæ—¥æœŸï¼Œæˆ‘ä»¬æœ‰å…ƒæ•°æ®ï¼Œæ¯”å¦‚æ²¹ä»·æ˜¯å¤šå°‘ã€‚
- en: This is what we call a **relational dataset**. Relational dataset is one where
    we have a number of different pieces of information that we can join together.
    Specifically this kind of relational dataset is what we refer to as â€œstar schemaâ€
    where there is some central transactions table. In this competition, the central
    transactions table is `train.csv` which contains the number units that were sold
    by `date` , `store_nbr` , and `item_nbr`. From this, we can join various bits
    of metadata (hence the name â€œstarâ€ schema â€” there is also one called [â€œsnowflakeâ€
    schema](https://en.wikipedia.org/wiki/Snowflake_schema)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„**å…³ç³»æ•°æ®é›†**ã€‚å…³ç³»æ•°æ®é›†æ˜¯æŒ‡æˆ‘ä»¬å¯ä»¥å°†è®¸å¤šä¸åŒä¿¡æ¯è¿æ¥åœ¨ä¸€èµ·çš„æ•°æ®é›†ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™ç§å…³ç³»æ•°æ®é›†æ˜¯æˆ‘ä»¬æ‰€è¯´çš„â€œæ˜Ÿå‹æ¨¡å¼â€ï¼Œå…¶ä¸­æœ‰ä¸€å¼ ä¸­å¿ƒäº¤æ˜“è¡¨ã€‚åœ¨è¿™ä¸ªæ¯”èµ›ä¸­ï¼Œä¸­å¿ƒäº¤æ˜“è¡¨æ˜¯`train.csv`ï¼Œå…¶ä¸­åŒ…å«äº†æŒ‰`æ—¥æœŸ`ã€`store_nbr`å’Œ`item_nbr`é”€å”®çš„æ•°é‡ã€‚é€šè¿‡è¿™ä¸ªè¡¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿æ¥å„ç§å…ƒæ•°æ®ï¼ˆå› æ­¤ç§°ä¸ºâ€œæ˜Ÿå‹â€æ¨¡å¼
    â€” è¿˜æœ‰ä¸€ç§å«åš[â€œé›ªèŠ±â€æ¨¡å¼](https://en.wikipedia.org/wiki/Snowflake_schema)ï¼‰ã€‚
- en: Reading Data[[15:12](https://youtu.be/YSFG_W8JxBo?t=15m12s)]
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯»å–æ•°æ®[[15:12](https://youtu.be/YSFG_W8JxBo?t=15m12s)]
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you set `low_memory=False`, it will run out of memory regardless of how much
    memory you have.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœè®¾ç½®`low_memory=False`ï¼Œæ— è®ºæ‚¨æœ‰å¤šå°‘å†…å­˜ï¼Œå®ƒéƒ½ä¼šè€—å°½å†…å­˜ã€‚
- en: In order to limit the amount of space that it takes up when you read in, we
    create a dictionary for each column name to the data type of that column. It is
    up to you to figure out the data types by running or `less` or `head` on the dataset.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†åœ¨è¯»å–æ—¶é™åˆ¶å ç”¨çš„ç©ºé—´é‡ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªåˆ—ååˆ›å»ºä¸€ä¸ªå­—å…¸ï¼ŒæŒ‡å®šè¯¥åˆ—çš„æ•°æ®ç±»å‹ã€‚æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œæˆ–åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨`less`æˆ–`head`æ¥æ‰¾å‡ºæ•°æ®ç±»å‹ã€‚
- en: With these tweaks, we can read in 125,497,040 rows in less than 2 minutes.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™äº›è°ƒæ•´ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸åˆ°2åˆ†é’Ÿå†…è¯»å–125,497,040è¡Œæ•°æ®ã€‚
- en: Python itself is not fast, but almost everything we want to do in Python in
    data science has been written for us in C or more often in Cython which is a python
    like language that compiles to C. In Pandas, a lot of it is written in assembly
    language which is heavily optimized. Behind the scene, a lot of that is going
    back to calling Fortran based libraries for linear algebra.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pythonæœ¬èº«å¹¶ä¸å¿«ï¼Œä½†å‡ ä¹æˆ‘ä»¬åœ¨Pythonä¸­è¿›è¡Œæ•°æ®ç§‘å­¦æ—¶æƒ³è¦åšçš„ä¸€åˆ‡éƒ½å·²ç»ä¸ºæˆ‘ä»¬ç”¨Cæˆ–æ›´å¸¸è§çš„Cythonç¼–å†™å¥½äº†ï¼ŒCythonæ˜¯ä¸€ç§ç±»ä¼¼Pythonçš„è¯­è¨€ï¼Œå¯ä»¥ç¼–è¯‘æˆCã€‚åœ¨Pandasä¸­ï¼Œå¾ˆå¤šä»£ç æ˜¯ç”¨æ±‡ç¼–è¯­è¨€ç¼–å†™çš„ï¼Œè¿™äº›ä»£ç ç»è¿‡äº†å¤§é‡ä¼˜åŒ–ã€‚åœ¨å¹•åï¼Œå¾ˆå¤šä»£ç éƒ½æ˜¯è°ƒç”¨åŸºäºFortrançš„çº¿æ€§ä»£æ•°åº“ã€‚
- en: '**Question**: Are there any performance consideration to specifying `int64`
    vs. `int` [[18:33](https://youtu.be/YSFG_W8JxBo?t=18m33s)]? The key performance
    here was to use the smallest number of bits that I could to fully represent the
    column. If we used `int8` for `item_nbr` , the maximum `item_nbr` is bigger than
    255 and it will not fit. On the other hand, if we used `int64` for the `store_nbr`
    , it is using more bits than necessary. Given that the whole purpose here was
    to avoid running out of RAM, we do not want to use up 8 times more memory than
    necessary. When you are working with large datasets, very often you will find
    that the slow piece is reading and writing to RAM, not the CPU operations. Also
    as a rule of thumb, smaller data types often will run faster particularly if you
    can use Single Instruction Multiple Data (SIMD) vectorized code, it can pack more
    numbers into a single vector to run at once.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šæŒ‡å®š`int64`ä¸`int`æ˜¯å¦æœ‰ä»»ä½•æ€§èƒ½è€ƒè™‘[[18:33](https://youtu.be/YSFG_W8JxBo?t=18m33s)]ï¼Ÿè¿™é‡Œçš„å…³é”®æ€§èƒ½æ˜¯ä½¿ç”¨å°½å¯èƒ½å°‘çš„ä½æ•°æ¥å®Œå…¨è¡¨ç¤ºåˆ—ã€‚å¦‚æœæˆ‘ä»¬å¯¹`item_nbr`ä½¿ç”¨`int8`ï¼Œæœ€å¤§çš„`item_nbr`å¤§äº255ï¼Œå°†æ— æ³•å®¹çº³ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬å¯¹`store_nbr`ä½¿ç”¨`int64`ï¼Œå®ƒä½¿ç”¨çš„ä½æ•°æ¯”å¿…è¦çš„å¤šã€‚é‰´äºè¿™é‡Œçš„æ•´ä¸ªç›®çš„æ˜¯é¿å…è€—å°½RAMï¼Œæˆ‘ä»¬ä¸å¸Œæœ›ä½¿ç”¨æ¯”å¿…è¦å¤š8å€çš„å†…å­˜ã€‚å½“æ‚¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ï¼Œå¾ˆå¤šæ—¶å€™æœ€æ…¢çš„éƒ¨åˆ†æ˜¯è¯»å–å’Œå†™å…¥RAMï¼Œè€Œä¸æ˜¯CPUæ“ä½œã€‚å¦å¤–ï¼Œä½œä¸ºä¸€ä¸ªç»éªŒæ³•åˆ™ï¼Œè¾ƒå°çš„æ•°æ®ç±»å‹é€šå¸¸ä¼šè¿è¡Œå¾—æ›´å¿«ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæ‚¨å¯ä»¥ä½¿ç”¨å•æŒ‡ä»¤å¤šæ•°æ®ï¼ˆSIMDï¼‰çŸ¢é‡åŒ–ä»£ç ï¼Œå®ƒå¯ä»¥å°†æ›´å¤šæ•°å­—æ‰“åŒ…åˆ°ä¸€ä¸ªå•ç‹¬çš„çŸ¢é‡ä¸­ä¸€æ¬¡è¿è¡Œã€‚'
- en: '**Question**: Do we not have to shuffle the data anymore [[20:11](https://youtu.be/YSFG_W8JxBo?t=20m11s)]?
    Although here I have read in the whole thing, when I start I never start by reading
    in the whole thing. By using a UNIX command `shuf`, you can get a random sample
    of data at the command prompt and then you can just read that. This is a good
    way, for example, to find out what data types to use â€” read in a random sample
    and let Pandas figure it out for you. In general, I do as much work as possible
    on a sample until I feel confident that I understand the sample before I move
    on.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šæˆ‘ä»¬ä¸å†éœ€è¦å¯¹æ•°æ®è¿›è¡Œæ´—ç‰Œäº†å—[[20:11](https://youtu.be/YSFG_W8JxBo?t=20m11s)]ï¼Ÿå°½ç®¡åœ¨è¿™é‡Œæˆ‘å·²ç»è¯»å–äº†æ•´ä¸ªæ•°æ®ï¼Œä½†å½“æˆ‘å¼€å§‹æ—¶ï¼Œæˆ‘ä»ä¸ä¼šä¸€å¼€å§‹å°±è¯»å–æ•´ä¸ªæ•°æ®ã€‚é€šè¿‡ä½¿ç”¨UNIXå‘½ä»¤`shuf`ï¼Œæ‚¨å¯ä»¥åœ¨å‘½ä»¤æç¤ºç¬¦ä¸‹è·å–æ•°æ®çš„éšæœºæ ·æœ¬ï¼Œç„¶åæ‚¨å¯ä»¥ç›´æ¥è¯»å–è¯¥æ ·æœ¬ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ï¼Œä¾‹å¦‚ï¼Œæ‰¾å‡ºè¦ä½¿ç”¨çš„æ•°æ®ç±»å‹
    â€” è¯»å–ä¸€ä¸ªéšæœºæ ·æœ¬ï¼Œè®©Pandasä¸ºæ‚¨æ‰¾å‡ºã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä¼šå°½å¯èƒ½å¤šåœ°åœ¨æ ·æœ¬ä¸Šå·¥ä½œï¼Œç›´åˆ°æˆ‘ç¡®ä¿¡æˆ‘ç†è§£äº†æ ·æœ¬åæ‰ä¼šç»§ç»­ã€‚'
- en: 'To pick a random line from a file using `shuf` use the `-n` option. This limits
    the output to the number specified. You can also specify the output file:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨`shuf`ä»æ–‡ä»¶ä¸­éšæœºé€‰æ‹©ä¸€è¡Œï¼Œè¯·ä½¿ç”¨`-n`é€‰é¡¹ã€‚è¿™å°†é™åˆ¶è¾“å‡ºä¸ºæŒ‡å®šçš„æ•°é‡ã€‚æ‚¨è¿˜å¯ä»¥æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼š
- en: '`shuf -n 5 -o sample_training.csv train.csv`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`shuf -n 5 -o sample_training.csv train.csv`'
- en: '`''onpromotion'': â€˜object''` [[21:28](https://youtu.be/YSFG_W8JxBo?t=21m28s)]â€”
    `object` is a general purpose Python datatype which is slow and memory heavy.
    The reason for this is it is a boolean which also has missing values, so we need
    to deal with this before we can turn it into a boolean as you see below:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`''onpromotion'': â€˜object''` [[21:28](https://youtu.be/YSFG_W8JxBo?t=21m28s)]â€”
    `object`æ˜¯ä¸€ä¸ªé€šç”¨çš„Pythonæ•°æ®ç±»å‹ï¼Œé€Ÿåº¦æ…¢ä¸”å ç”¨å†…å­˜ã€‚åŸå› æ˜¯å®ƒæ˜¯ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¿˜æœ‰ç¼ºå¤±å€¼ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨å°†å…¶è½¬æ¢ä¸ºå¸ƒå°”å€¼ä¹‹å‰å¤„ç†å®ƒï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`fillna(False)`: we would not do this without checking first, but some exploratory
    data analysis shows that it is probably an appropriate thing to do (i.e. missing
    means false).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fillna(False)`: æˆ‘ä»¬ä¸ä¼šåœ¨æ²¡æœ‰å…ˆæ£€æŸ¥çš„æƒ…å†µä¸‹è¿™æ ·åšï¼Œä½†ä¸€äº›æ¢ç´¢æ€§æ•°æ®åˆ†ææ˜¾ç¤ºè¿™å¯èƒ½æ˜¯ä¸€ä¸ªåˆé€‚çš„åšæ³•ï¼ˆå³ç¼ºå¤±å€¼è¡¨ç¤ºfalseï¼‰ã€‚'
- en: '`map({â€˜Falseâ€™: False, â€˜Trueâ€™: True})` : `object` usually reads in as string,
    so replace string `â€˜Trueâ€™` and `â€˜Falseâ€™` with actual booleans.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map({â€˜Falseâ€™: False, â€˜Trueâ€™: True})`ï¼š`object`é€šå¸¸è¯»å–ä¸ºå­—ç¬¦ä¸²ï¼Œæ‰€ä»¥ç”¨å®é™…çš„å¸ƒå°”å€¼æ›¿æ¢å­—ç¬¦ä¸²`â€˜Trueâ€™`å’Œ`â€˜Falseâ€™`ã€‚'
- en: '`astype(bool)` : Then finally convert it to boolean type.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`astype(bool)`ï¼šæœ€åå°†å…¶è½¬æ¢ä¸ºå¸ƒå°”ç±»å‹ã€‚'
- en: The feather file with over 125 million records takes up something under 2.5GB
    of memory.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‹¥æœ‰è¶…è¿‡1.25äº¿æ¡è®°å½•çš„featheræ–‡ä»¶å ç”¨äº†ä¸åˆ°2.5GBçš„å†…å­˜ã€‚
- en: Now it is in a nice fast format, we can save it to feather format in under 5
    seconds.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç°åœ¨å®ƒä»¥ä¸€ä¸ªå¾ˆå¥½çš„å¿«é€Ÿæ ¼å¼ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸åˆ°5ç§’çš„æ—¶é—´å†…å°†å®ƒä¿å­˜ä¸ºfeatheræ ¼å¼ã€‚
- en: 'Pandas is generally fast, so you can summarize every column of all 125 million
    records in 20 seconds:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Pandasé€šå¸¸å¾ˆå¿«ï¼Œæ‰€ä»¥ä½ å¯ä»¥åœ¨20ç§’å†…æ€»ç»“æ‰€æœ‰1.25äº¿æ¡è®°å½•çš„æ¯ä¸€åˆ—ï¼š
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/2568de2e53d9b99568f8fbdd2acddb1d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2568de2e53d9b99568f8fbdd2acddb1d.png)'
- en: First thing to look at is the dates. Dates are important because any models
    you put in in practice, you are going to be putting it in at some date that is
    later than the date that you trained it by definition. So if anything in the world
    changes, you need to know how your predictive accuracy changes as well. So for
    Kaggle or for your own project, you should always make sure that your dates do
    not overlap [[22:55](https://youtu.be/YSFG_W8JxBo?t=22m55s)].
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆè¦çœ‹çš„æ˜¯æ—¥æœŸã€‚æ—¥æœŸå¾ˆé‡è¦ï¼Œå› ä¸ºä½ åœ¨å®è·µä¸­æ”¾å…¥çš„ä»»ä½•æ¨¡å‹ï¼Œéƒ½ä¼šåœ¨æ¯”ä½ è®­ç»ƒçš„æ—¥æœŸæ™šçš„æŸä¸ªæ—¥æœŸæ”¾å…¥ã€‚æ‰€ä»¥å¦‚æœä¸–ç•Œä¸Šçš„ä»»ä½•äº‹æƒ…å‘ç”Ÿå˜åŒ–ï¼Œä½ éœ€è¦çŸ¥é“ä½ çš„é¢„æµ‹å‡†ç¡®æ€§ä¹Ÿä¼šå¦‚ä½•å˜åŒ–ã€‚æ‰€ä»¥å¯¹äºKaggleæˆ–ä½ è‡ªå·±çš„é¡¹ç›®ï¼Œä½ åº”è¯¥å§‹ç»ˆç¡®ä¿ä½ çš„æ—¥æœŸä¸é‡å ã€‚
- en: In this case, training set goes from 2013 to August 2017.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒé›†ä»2013å¹´åˆ°2017å¹´8æœˆã€‚
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/df1859cbc7988611eeea704f84096273.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df1859cbc7988611eeea704f84096273.png)'
- en: In our test set, they go from one day later until the end of the month.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æµ‹è¯•é›†ä¸­ï¼Œå®ƒä»¬ä»ç¬¬äºŒå¤©å¼€å§‹ç›´åˆ°æœˆåº•ã€‚
- en: This is a key thing â€” you cannot really do any useful machine learning until
    you understand this basic piece. You have four years of data and you are trying
    to predict the next two weeks. This is a fundamental thing you need to understand
    before you can go and do a good job at this.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„äº‹æƒ… â€”â€” é™¤éä½ ç†è§£è¿™ä¸ªåŸºæœ¬çš„éƒ¨åˆ†ï¼Œå¦åˆ™ä½ æ— æ³•çœŸæ­£åšå‡ºä»»ä½•æœ‰ç”¨çš„æœºå™¨å­¦ä¹ ã€‚ä½ æœ‰å››å¹´çš„æ•°æ®ï¼Œä½ æ­£åœ¨å°è¯•é¢„æµ‹æ¥ä¸‹æ¥çš„ä¸¤å‘¨ã€‚åœ¨ä½ èƒ½å¤Ÿåšå¥½è¿™ä¸ªå·¥ä½œä¹‹å‰ï¼Œè¿™æ˜¯ä½ éœ€è¦ç†è§£çš„åŸºæœ¬äº‹æƒ…ã€‚
- en: If you want to use a smaller dataset, we should use the most recent â€” not random
    set.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨æœ€è¿‘çš„ â€”â€” è€Œä¸æ˜¯éšæœºçš„é›†åˆã€‚
- en: '**Question**: Wouldnâ€™t four years ago around the same time frame be important
    (e.g. around Christmas time)[[25:06](https://youtu.be/YSFG_W8JxBo?t=25m6s)]? Exactly.
    It is not that there is no useful information from four years ago so we do not
    want to entirely throw it away. But as a first step, if you were to submit the
    mean, you would not submit the mean of 2012 sales, but probably want to submit
    the mean of last monthâ€™s sale.And later on, we might want to weight more recent
    dates more highly since they are probably more relevant. But we should do bunch
    of exploratory data analysis to check that.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šå››å¹´å‰å¤§çº¦åœ¨åŒä¸€æ—¶é—´æ®µé‡è¦å—ï¼ˆä¾‹å¦‚åœ¨åœ£è¯èŠ‚å·¦å³ï¼‰ï¼Ÿç¡®å®ã€‚å¹¶ä¸æ˜¯è¯´å››å¹´å‰æ²¡æœ‰æœ‰ç”¨çš„ä¿¡æ¯ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸æƒ³å®Œå…¨æŠ›å¼ƒå®ƒã€‚ä½†ä½œä¸ºç¬¬ä¸€æ­¥ï¼Œå¦‚æœä½ è¦æäº¤å¹³å‡å€¼ï¼Œä½ ä¸ä¼šæäº¤2012å¹´é”€å”®é¢çš„å¹³å‡å€¼ï¼Œè€Œå¯èƒ½æƒ³è¦æäº¤ä¸Šä¸ªæœˆé”€å”®é¢çš„å¹³å‡å€¼ã€‚ä¹‹åï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æ›´é«˜æƒé‡æœ€è¿‘çš„æ—¥æœŸï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½æ›´ç›¸å…³ã€‚ä½†æˆ‘ä»¬åº”è¯¥è¿›è¡Œå¤§é‡çš„æ¢ç´¢æ€§æ•°æ®åˆ†ææ¥æ£€æŸ¥ã€‚'
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/80c10c4942f06fb03e584e5065725c9e.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80c10c4942f06fb03e584e5065725c9e.png)'
- en: Here is what the bottom of the data looks like [[26:00](https://youtu.be/YSFG_W8JxBo?t=26m)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ•°æ®åº•éƒ¨çš„æ ·å­ã€‚
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We have to take a log of the sales because we are trying to predict something
    that varies according to the ratios and they told us, in this competition, that
    root mean squared log error is something they care about.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»å¯¹é”€å”®é¢å–å¯¹æ•°ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨å°è¯•é¢„æµ‹æ ¹æ®æ¯”ç‡å˜åŒ–çš„æŸäº›ä¸œè¥¿ï¼Œè€Œä»–ä»¬å‘Šè¯‰æˆ‘ä»¬ï¼Œåœ¨è¿™ä¸ªæ¯”èµ›ä¸­ï¼Œå‡æ–¹æ ¹å¯¹æ•°è¯¯å·®æ˜¯ä»–ä»¬å…³å¿ƒçš„äº‹æƒ…ã€‚
- en: '`np.clip(df_all.unit_sales, 0, None)`: there are some negative sales that represent
    returns and the organizer told us to consider them to be zero for the purpose
    of this competition. `clilp` truncates to specified min and max.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.clip(df_all.unit_sales, 0, None)`: æœ‰ä¸€äº›ä»£è¡¨é€€è´§çš„è´Ÿé”€å”®é¢ï¼Œç»„ç»‡è€…å‘Šè¯‰æˆ‘ä»¬åœ¨è¿™ä¸ªæ¯”èµ›ä¸­å°†å®ƒä»¬è§†ä¸ºé›¶ã€‚`clip`æˆªæ–­åˆ°æŒ‡å®šçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ã€‚'
- en: '`np.log1p` : log of the value plus 1\. The competition detail tells you that
    they are going to use root mean squared log plus 1 error because log(0) does not
    make sense.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.log1p`ï¼šå€¼åŠ 1çš„å¯¹æ•°ã€‚æ¯”èµ›ç»†èŠ‚å‘Šè¯‰ä½ ä»–ä»¬å°†ä½¿ç”¨å‡æ–¹æ ¹å¯¹æ•°åŠ 1è¯¯å·®ï¼Œå› ä¸ºlog(0)æ²¡æœ‰æ„ä¹‰ã€‚'
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can add date part as usual. It takes a couple of minutes, so we should run
    through all this on sample first to make sure it works. Once you know everything
    is reasonable, then go back and run on a whole set.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·æ·»åŠ æ—¥æœŸéƒ¨åˆ†ã€‚è¿™éœ€è¦å‡ åˆ†é’Ÿï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥å…ˆåœ¨æ ·æœ¬ä¸Šè¿è¡Œæ‰€æœ‰è¿™äº›ï¼Œä»¥ç¡®ä¿å®ƒæœ‰æ•ˆã€‚ä¸€æ—¦ä½ çŸ¥é“ä¸€åˆ‡éƒ½æ˜¯åˆç†çš„ï¼Œç„¶åå›å»åœ¨æ•´ä¸ªé›†åˆä¸Šè¿è¡Œã€‚
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These lines of code are identical to what we saw for bulldozers competition.
    We do not need to run `train_cats` or `apply_cats` since all of the data types
    are already numeric (remember `apply_cats` applies the same categorical codes
    to validation set as the training set) [[27:59](https://youtu.be/YSFG_W8JxBo?t=27m59s)].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä»£ç è¡Œä¸æˆ‘ä»¬åœ¨æ¨åœŸæœºæ¯”èµ›ä¸­çœ‹åˆ°çš„ä»£ç è¡Œæ˜¯ç›¸åŒçš„ã€‚æˆ‘ä»¬ä¸éœ€è¦è¿è¡Œ`train_cats`æˆ–`apply_cats`ï¼Œå› ä¸ºæ‰€æœ‰çš„æ•°æ®ç±»å‹å·²ç»æ˜¯æ•°å­—çš„äº†ï¼ˆè®°ä½`apply_cats`å°†ç›¸åŒçš„åˆ†ç±»ä»£ç åº”ç”¨äºéªŒè¯é›†å’Œè®­ç»ƒé›†ï¼‰ã€‚
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Call `proc_df` to check the missing values and so forth.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨`proc_df`æ¥æ£€æŸ¥ç¼ºå¤±å€¼ç­‰ã€‚
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'These lines of code again are identical. Then there are two changes [[28:48](https://youtu.be/YSFG_W8JxBo?t=28m48s)]:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä»£ç è¡Œå†æ¬¡æ˜¯ç›¸åŒçš„ã€‚ç„¶åæœ‰ä¸¤ä¸ªå˜åŒ–ï¼š
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We have learned about `set_rf_samples` last week. We probably do not want to
    create a tree from 125 million records (not sure how long that will take). You
    can start with 10k or 100k and figure out how much you can run. There is no relationship
    between the size of the dataset and how long it takes to build the random forests
    â€” the relationship is between the number of estimators multiplied by the sample
    size.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸Šå‘¨å­¦ä¹ äº†`set_rf_samples`ã€‚æˆ‘ä»¬å¯èƒ½ä¸æƒ³ä»1.25äº¿æ¡è®°å½•ä¸­åˆ›å»ºä¸€æ£µæ ‘ï¼ˆä¸ç¡®å®šéœ€è¦å¤šé•¿æ—¶é—´ï¼‰ã€‚ä½ å¯ä»¥ä»10kæˆ–100kå¼€å§‹ï¼Œç„¶åæ‰¾å‡ºä½ å¯ä»¥è¿è¡Œå¤šå°‘ã€‚æ•°æ®é›†çš„å¤§å°ä¸æ„å»ºéšæœºæ£®æ—æ‰€éœ€æ—¶é—´ä¹‹é—´æ²¡æœ‰å…³ç³»ï¼Œå…³ç³»åœ¨äºä¼°è®¡å™¨æ•°é‡ä¹˜ä»¥æ ·æœ¬å¤§å°ã€‚
- en: '**Question:** What is `n_job`? In the past, it has always been `-1` [[29:42](https://youtu.be/YSFG_W8JxBo?t=29m42s)].
    The number of jobs is the number of cores to use. I was running this on a computer
    that has about 60 cores and if you try to use all of them, it spent so much time
    spinning out jobs and it was slower. If you have lots of cores on your computer,
    sometimes you want less (`-1` means use every single core).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ï¼š** `n_job`æ˜¯ä»€ä¹ˆï¼Ÿè¿‡å»ï¼Œå®ƒæ€»æ˜¯`-1`[[29:42](https://youtu.be/YSFG_W8JxBo?t=29m42s)]ã€‚ä½œä¸šæ•°æ˜¯è¦ä½¿ç”¨çš„æ ¸å¿ƒæ•°ã€‚æˆ‘åœ¨ä¸€å°å¤§çº¦æœ‰60ä¸ªæ ¸å¿ƒçš„è®¡ç®—æœºä¸Šè¿è¡Œè¿™ä¸ªï¼Œå¦‚æœä½ å°è¯•ä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒï¼Œå®ƒä¼šèŠ±è´¹å¾ˆå¤šæ—¶é—´æ¥å¯åŠ¨ä½œä¸šï¼Œé€Ÿåº¦ä¼šå˜æ…¢ã€‚å¦‚æœä½ çš„è®¡ç®—æœºæœ‰å¾ˆå¤šæ ¸å¿ƒï¼Œæœ‰æ—¶ä½ æƒ³è¦æ›´å°‘ï¼ˆ`-1`è¡¨ç¤ºä½¿ç”¨æ¯ä¸ªæ ¸å¿ƒï¼‰ã€‚'
- en: Another change was `x = np.array(trn, dtype=np.float32)`. This converts data
    frame into an array of floats and we fit it on that. Inside the random forest
    code, they do this anyway. Given that we want to run a few different random forests
    with a few different hyper parameters, doing this once myself saves 1 min 37 sec.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå˜åŒ–æ˜¯`x = np.array(trn, dtype=np.float32)`ã€‚è¿™å°†æ•°æ®æ¡†è½¬æ¢ä¸ºæµ®ç‚¹æ•°ç»„ï¼Œç„¶åæˆ‘ä»¬åœ¨å…¶ä¸Šè¿›è¡Œæ‹Ÿåˆã€‚åœ¨éšæœºæ£®æ—ä»£ç å†…éƒ¨ï¼Œä»–ä»¬æ— è®ºå¦‚ä½•éƒ½ä¼šè¿™æ ·åšã€‚é‰´äºæˆ‘ä»¬æƒ³è¦è¿è¡Œå‡ ä¸ªä¸åŒçš„éšæœºæ£®æ—ï¼Œä½¿ç”¨å‡ ç§ä¸åŒçš„è¶…å‚æ•°ï¼Œè‡ªå·±åšä¸€æ¬¡å¯ä»¥èŠ‚çœ1åˆ†37ç§’ã€‚
- en: '`Profiler : %prun` [[30:40](https://youtu.be/YSFG_W8JxBo?t=30m40s)]'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`åˆ†æå™¨ï¼š%prun`[[30:40](https://youtu.be/YSFG_W8JxBo?t=30m40s)]'
- en: If you run a line of code that takes quite a long time, you can put `%prun`
    in front.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿è¡Œä¸€è¡Œéœ€è¦å¾ˆé•¿æ—¶é—´çš„ä»£ç ï¼Œä½ å¯ä»¥åœ¨å‰é¢åŠ ä¸Š`%prun`ã€‚
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will run a profiler and tells you which lines of code took the most time.
    Here it was the code in scikit-learn that was the line of code that converts data
    frame to numpy array.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™å°†è¿è¡Œä¸€ä¸ªåˆ†æå™¨ï¼Œå¹¶å‘Šè¯‰ä½ å“ªäº›ä»£ç è¡ŒèŠ±è´¹äº†æœ€å¤šçš„æ—¶é—´ã€‚è¿™é‡Œæ˜¯scikit-learnä¸­å°†æ•°æ®æ¡†è½¬æ¢ä¸ºnumpyæ•°ç»„çš„ä»£ç è¡Œã€‚
- en: Looking to see which things is taking up the time is called â€œprofilingâ€ and
    in software engineering is one of the most important tool. But data scientists
    tend to under appreciate it.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹å“ªäº›äº‹æƒ…å ç”¨äº†æ—¶é—´è¢«ç§°ä¸ºâ€œåˆ†æâ€ï¼Œåœ¨è½¯ä»¶å·¥ç¨‹ä¸­æ˜¯æœ€é‡è¦çš„å·¥å…·ä¹‹ä¸€ã€‚ä½†æ•°æ®ç§‘å­¦å®¶å¾€å¾€ä½ä¼°äº†å®ƒã€‚
- en: For fun, try running `%prun` from time to time on code that takes 10â€“20 seconds
    and see if you can learn to interpret and use profiler outputs.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œå°è¯•ä¸æ—¶è¿è¡Œ`%prun`åœ¨éœ€è¦10-20ç§’çš„ä»£ç ä¸Šï¼Œçœ‹çœ‹ä½ æ˜¯å¦èƒ½å­¦ä¼šè§£é‡Šå’Œä½¿ç”¨åˆ†æå™¨è¾“å‡ºã€‚
- en: Something else Jeremy noticed in the profiler is we canâ€™t use OOB score when
    we do `set_rf_samples` because if we do, it will use the other 124 million rows
    to calculate the OOB score. Besides, we want to use the validation set that is
    the most recent dates rather than random.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeremyåœ¨åˆ†æå™¨ä¸­æ³¨æ„åˆ°çš„å¦ä¸€ä»¶äº‹æ˜¯ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨`set_rf_samples`æ—¶ï¼Œæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨OOBåˆ†æ•°ï¼Œå› ä¸ºå¦‚æœè¿™æ ·åšï¼Œå®ƒå°†ä½¿ç”¨å…¶ä»–124ç™¾ä¸‡è¡Œæ¥è®¡ç®—OOBåˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨æœ€è¿‘æ—¥æœŸçš„éªŒè¯é›†ï¼Œè€Œä¸æ˜¯éšæœºçš„ã€‚
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So this got us 0.76 validation root mean squared log error.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™è®©æˆ‘ä»¬å¾—åˆ°äº†0.76çš„éªŒè¯å‡æ–¹æ ¹å¯¹æ•°è¯¯å·®ã€‚
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This gets us down to 0.71 even though it took a little longer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿æˆ‘ä»¬é™åˆ°äº†0.71ï¼Œå°½ç®¡èŠ±äº†æ›´é•¿çš„æ—¶é—´ã€‚
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This brought this error down to 0.70\. `min_samples_leaf=1` did not really
    help. So we have a â€œreasonableâ€ random forest here. But this does not give a good
    result on the leader board [[33:42](https://youtu.be/YSFG_W8JxBo?t=33m42s)]. Why?
    Letâ€™s go back and see the data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†é”™è¯¯é™ä½åˆ°0.70ã€‚`min_samples_leaf=1`å¹¶æ²¡æœ‰çœŸæ­£å¸®åŠ©ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œæœ‰ä¸€ä¸ªâ€œåˆç†â€çš„éšæœºæ£®æ—ã€‚ä½†æ˜¯è¿™åœ¨æ’è¡Œæ¦œä¸Šå¹¶æ²¡æœ‰å–å¾—å¥½çš„ç»“æœ[[33:42](https://youtu.be/YSFG_W8JxBo?t=33m42s)]ã€‚ä¸ºä»€ä¹ˆï¼Ÿè®©æˆ‘ä»¬å›å¤´çœ‹çœ‹æ•°æ®ï¼š
- en: '![](../Images/882cf4f8fcac80db504a7cf053135380.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/882cf4f8fcac80db504a7cf053135380.png)'
- en: 'These are the columns we had to predict with (plus what were added by `add_datepart`).
    Most of the insight around how much of something you expect to sell tomorrow is
    likely to be wrapped up in the details about where the store is, what kind of
    things they tend to sell at the store, for a given item, what category of item
    it is. Random forest has no ability to do anything other than create binary splits
    on things like day of week, store number, item number. It does not know type of
    items or location of stores. Since its ability to understand what is going on
    is limited, we probably need to use the entire 4 years of data to even get some
    useful insights. But as soon as we start using the whole 4 years of data, a lot
    of the data we are using is really old. There is a Kaggle kernel that points out
    that what you could do is [[35:54](https://youtu.be/YSFG_W8JxBo?t=35m54s)]:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯æˆ‘ä»¬å¿…é¡»ç”¨æ¥é¢„æµ‹çš„åˆ—ï¼ˆä»¥åŠç”±`add_datepart`æ·»åŠ çš„å†…å®¹ï¼‰ã€‚å…³äºæ˜å¤©é¢„è®¡é”€å”®å¤šå°‘çš„å¤§éƒ¨åˆ†è§è§£å¯èƒ½éƒ½åŒ…å«åœ¨å…³äºå•†åº—ä½ç½®ã€å•†åº—é€šå¸¸é”€å”®çš„ç‰©å“ç§ç±»ä»¥åŠç»™å®šç‰©å“çš„ç±»åˆ«æ˜¯ä»€ä¹ˆçš„ç»†èŠ‚ä¸­ã€‚éšæœºæ£®æ—é™¤äº†åœ¨è¯¸å¦‚æ˜ŸæœŸå‡ ã€å•†åº—ç¼–å·ã€ç‰©å“ç¼–å·ç­‰æ–¹é¢åˆ›å»ºäºŒå…ƒåˆ†å‰²ä¹‹å¤–ï¼Œæ²¡æœ‰å…¶ä»–èƒ½åŠ›ã€‚å®ƒä¸çŸ¥é“ç‰©å“ç±»å‹æˆ–å•†åº—ä½ç½®ã€‚ç”±äºå®ƒç†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…çš„èƒ½åŠ›æœ‰é™ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä½¿ç”¨æ•´æ•´4å¹´çš„æ•°æ®æ‰èƒ½å¾—åˆ°ä¸€äº›æœ‰ç”¨çš„è§è§£ã€‚ä½†æ˜¯ä¸€æ—¦æˆ‘ä»¬å¼€å§‹ä½¿ç”¨æ•´æ•´4å¹´çš„æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®ä¸­æœ‰å¾ˆå¤šæ˜¯éå¸¸é™ˆæ—§çš„ã€‚æœ‰ä¸€ä¸ªKaggleå†…æ ¸æŒ‡å‡ºï¼Œä½ å¯ä»¥åšçš„æ˜¯[[35:54](https://youtu.be/YSFG_W8JxBo?t=35m54s)]ï¼š
- en: Take the last two weeks.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: çœ‹çœ‹æœ€åä¸¤å‘¨ã€‚
- en: Take the average sales by store number, by item number, by on promotion, then
    take a mean across date.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‰å•†åº—ç¼–å·ã€ç‰©å“ç¼–å·ã€ä¿ƒé”€æƒ…å†µçš„å¹³å‡é”€å”®é¢ï¼Œç„¶åè·¨æ—¥æœŸå–å¹³å‡ã€‚
- en: Just submit that, and you come about 30th ğŸ‰
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åªéœ€æäº¤ï¼Œä½ å°±èƒ½æ’åœ¨ç¬¬30åå·¦å³ğŸ‰
- en: We will talk about this in the next class, but if you can figure out how you
    start with that model and make it a little bit better, you will be above 30th
    place.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€å ‚è¯¾ä¸Šè®¨è®ºè¿™ä¸ªé—®é¢˜ï¼Œä½†å¦‚æœä½ èƒ½æ‰¾å‡ºå¦‚ä½•ä»é‚£ä¸ªæ¨¡å‹å¼€å§‹å¹¶ä½¿å…¶å˜å¾—æ›´å¥½ä¸€ç‚¹ï¼Œä½ å°†æ’åœ¨ç¬¬30åä»¥ä¸Šã€‚
- en: '**Question**: Could you try to capture seasonality and trend effects by creating
    new columns like average sales in the month of August [[38:10](https://youtu.be/YSFG_W8JxBo?t=38m10s)]?
    It is a great idea. The thing to figure out is how to do it because there are
    details to get right and are difficult- not intellectually difficult but they
    are difficult in a way that makes you headbutt your desk at 2am [[38:41](https://youtu.be/YSFG_W8JxBo?t=38m41s)].'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šæ‚¨èƒ½å¦å°è¯•é€šè¿‡åˆ›å»ºæ–°åˆ—æ¥æ•æ‰å­£èŠ‚æ€§å’Œè¶‹åŠ¿æ•ˆåº”ï¼Œæ¯”å¦‚8æœˆä»½çš„å¹³å‡é”€å”®é¢ï¼Ÿè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¸»æ„ã€‚è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºæœ‰ä¸€äº›ç»†èŠ‚éœ€è¦æ­£ç¡®ï¼Œè¿™äº›ç»†èŠ‚å¾ˆå›°éš¾-ä¸æ˜¯åœ¨æ™ºåŠ›ä¸Šå›°éš¾ï¼Œè€Œæ˜¯ä»¥ä¸€ç§è®©ä½ åœ¨å‡Œæ™¨2ç‚¹æ’å¤´çš„æ–¹å¼å›°éš¾ã€‚'
- en: Coding you do for machine learning is incredibly frustrating and incredibly
    difficult. If you get a detail wrong, much of the time it is not going to give
    you an exception it will just silently be slightly less good than it otherwise
    would have been. If you are on Kaggle, you will know that you are not doing as
    well as other people. But otherwise you have nothing to compare against. You will
    not know if your companyâ€™s model is half as good as it could be because you made
    a little mistake. This is why practicing on Kaggle now is great.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæœºå™¨å­¦ä¹ ç¼–ç æ˜¯éå¸¸ä»¤äººæ²®ä¸§å’Œéå¸¸å›°éš¾çš„ã€‚å¦‚æœä½ å¼„é”™äº†ä¸€ä¸ªç»†èŠ‚ï¼Œå¾ˆå¤šæ—¶å€™å®ƒä¸ä¼šç»™ä½ ä¸€ä¸ªå¼‚å¸¸ï¼Œå®ƒåªä¼šé»˜é»˜åœ°æ¯”åŸæ¥ç¨å¾®å·®ä¸€ç‚¹ã€‚å¦‚æœä½ åœ¨Kaggleä¸Šï¼Œä½ ä¼šçŸ¥é“ä½ çš„è¡¨ç°ä¸å¦‚å…¶ä»–äººã€‚ä½†å¦åˆ™ï¼Œä½ æ²¡æœ‰ä»€ä¹ˆå¯ä»¥æ¯”è¾ƒçš„ã€‚ä½ ä¸ä¼šçŸ¥é“ä½ çš„å…¬å¸æ¨¡å‹æ˜¯å¦åªæœ‰å®ƒå¯èƒ½çš„ä¸€åŠå¥½ï¼Œå› ä¸ºä½ çŠ¯äº†ä¸€ä¸ªå°é”™è¯¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆç°åœ¨åœ¨Kaggleä¸Šç»ƒä¹ æ˜¯å¾ˆå¥½çš„ã€‚
- en: You will get practice in finding all of the ways in which you can infuriatingly
    screw things up and you will be amazed [[39:38](https://youtu.be/YSFG_W8JxBo?t=39m38s)].
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½ å°†ç»ƒä¹ æ‰¾åˆ°æ‰€æœ‰å¯èƒ½ä»¤äººæ¼ç«åœ°æç ¸äº‹æƒ…çš„æ–¹æ³•ï¼Œä½ ä¼šæ„Ÿåˆ°æƒŠè®¶ã€‚
- en: Even for Jeremy, there is an extraordinary array of them. As you get to get
    to know what they are, you will start to know how to check for them as you go.
    You should assume every button you press, you are going to press the wrong button.
    That is fine as long as you have a way to find out.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿å¯¹äºJeremyæ¥è¯´ï¼Œè¿™äº›éªŒè¯é›†ä¹Ÿæ˜¯éå¸¸ä¸°å¯Œçš„ã€‚å½“ä½ å¼€å§‹äº†è§£å®ƒä»¬æ˜¯ä»€ä¹ˆæ—¶ï¼Œä½ å°†å¼€å§‹çŸ¥é“å¦‚ä½•åœ¨è¿›è¡Œæ—¶æ£€æŸ¥å®ƒä»¬ã€‚ä½ åº”è¯¥å‡è®¾ä½ æŒ‰ä¸‹çš„æ¯ä¸ªæŒ‰é’®éƒ½ä¼šæŒ‰é”™æŒ‰é’®ã€‚åªè¦ä½ æœ‰ä¸€ç§æ‰¾å‡ºæ¥çš„æ–¹æ³•å°±å¯ä»¥ã€‚
- en: Unfortunately there is not a set of specific things you should always do, you
    just have to think what you know about the results of this thing I am about to
    do. Here is a really simple example. If you created that basic entry where you
    take the mean by date, by store number, by on promotion, you submitted it, and
    got a reasonable score. Then you think you have something that is a little bit
    better and you do predictions for that. How about you create a scatter plot showing
    the prediction of your average model on one axis versus the predictions of your
    new model on the other axis. You should see that they just about form a line.
    If they do not, then that is a very strong suggestion that you screwed something
    up.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œæ²¡æœ‰ä¸€å¥—ä½ åº”è¯¥æ€»æ˜¯åšçš„å…·ä½“äº‹æƒ…ï¼Œä½ åªéœ€è¦è€ƒè™‘ä¸€ä¸‹æˆ‘å³å°†åšçš„äº‹æƒ…çš„ç»“æœã€‚è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸ç®€å•çš„ä¾‹å­ã€‚å¦‚æœä½ åˆ›å»ºäº†ä¸€ä¸ªåŸºæœ¬çš„æ¡ç›®ï¼Œå…¶ä¸­ä½ æŒ‰æ—¥æœŸã€åº—é“ºç¼–å·ã€ä¿ƒé”€çŠ¶æ€å–å¹³å‡å€¼ï¼Œç„¶åæäº¤äº†å®ƒï¼Œå¹¶å¾—åˆ°äº†ä¸€ä¸ªåˆç†çš„åˆ†æ•°ã€‚ç„¶åä½ è®¤ä¸ºä½ æœ‰ä¸€äº›ç¨å¾®å¥½ä¸€ç‚¹çš„ä¸œè¥¿ï¼Œä½ ä¸ºæ­¤åšäº†é¢„æµ‹ã€‚ä½ å¯ä»¥åˆ›å»ºä¸€ä¸ªæ•£ç‚¹å›¾ï¼Œæ˜¾ç¤ºä½ çš„å¹³å‡æ¨¡å‹é¢„æµ‹åœ¨ä¸€ä¸ªè½´ä¸Šï¼Œä¸ä½ çš„æ–°æ¨¡å‹é¢„æµ‹åœ¨å¦ä¸€ä¸ªè½´ä¸Šã€‚ä½ åº”è¯¥çœ‹åˆ°å®ƒä»¬å‡ ä¹å½¢æˆä¸€æ¡ç›´çº¿ã€‚å¦‚æœä¸æ˜¯ï¼Œé‚£ä¹ˆè¿™éå¸¸æ˜æ˜¾åœ°è¡¨æ˜ä½ æç ¸äº†ä»€ä¹ˆã€‚
- en: '**Question**: How often do you pull in data from other sources to supplement
    dataset you have [[41:15](https://youtu.be/YSFG_W8JxBo?t=41m15s)]? Very often.
    The whole point of star schema is that you have a centric table, and you have
    other tables coming off it that provide metadata about it. On Kaggle, most competitions
    have the rule that you can use external data as long as post on the forum and
    is publicly available (double check the rule!). Outside of the Kaggle, you should
    always be looking for what external data you could possibly leverage.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šæ‚¨å¤šä¹…ä»å…¶ä»–æ¥æºè·å–æ•°æ®æ¥è¡¥å……æ‚¨å·²æœ‰çš„æ•°æ®é›†ï¼Ÿéå¸¸é¢‘ç¹ã€‚æ˜Ÿå‹æ¨¡å¼çš„æ•´ä¸ªé‡ç‚¹æ˜¯ä½ æœ‰ä¸€ä¸ªä¸­å¿ƒè¡¨ï¼Œä½ æœ‰å…¶ä»–è¡¨ä¸ä¹‹ç›¸è¿ï¼Œæä¾›å…³äºå®ƒçš„å…ƒæ•°æ®ã€‚åœ¨Kaggleä¸Šï¼Œå¤§å¤šæ•°æ¯”èµ›çš„è§„åˆ™æ˜¯ä½ å¯ä»¥ä½¿ç”¨å¤–éƒ¨æ•°æ®ï¼Œåªè¦åœ¨è®ºå›ä¸Šå‘å¸ƒå¹¶ä¸”æ˜¯å…¬å¼€å¯ç”¨çš„ï¼ˆåŒé‡æ£€æŸ¥è§„åˆ™ï¼ï¼‰ã€‚åœ¨Kaggleä¹‹å¤–ï¼Œä½ åº”è¯¥å§‹ç»ˆå¯»æ‰¾å¯èƒ½åˆ©ç”¨çš„å¤–éƒ¨æ•°æ®ã€‚'
- en: '**Question**: How about adding Ecuadorâ€™s holidays to supplement the data? [[42:52](https://youtu.be/YSFG_W8JxBo?t=42m52s)]
    That information is actually provided. In general, one way of tackling this kind
    of problem is to create lots of new columns containing things like average number
    of sales on holidays, average percent change in sale between January and February,
    etc. There has been [a pervious competition](https://www.kaggle.com/c/rossmann-store-sales)
    for a grocery chain in Germany that was almost identical. [The person who won](http://blog.kaggle.com/2015/12/21/rossmann-store-sales-winners-interview-1st-place-gert/)
    was a domain expert and specialist in doing logistics predictions. He created
    lots of columns based on his experience of what kinds of things tend to be useful
    for making predictions. So that is an approach that can work. The third place
    winner did almost no feature engineering, however, and they also had one big oversight
    which may have cost them the first place win. We will be learning a lot more about
    how to win this competition and ones like it as we go.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šå¦‚ä½•æ·»åŠ å„ç“œå¤šå°”çš„å‡æœŸæ¥è¡¥å……æ•°æ®ï¼Ÿè¿™ä¸ªä¿¡æ¯å®é™…ä¸Šæ˜¯æä¾›çš„ã€‚ä¸€ç§è§£å†³è¿™ç§é—®é¢˜çš„ä¸€èˆ¬æ–¹æ³•æ˜¯åˆ›å»ºè®¸å¤šæ–°åˆ—ï¼Œå…¶ä¸­åŒ…å«å‡æœŸé”€å”®å¹³å‡æ•°é‡ï¼Œä¸€æœˆå’ŒäºŒæœˆä¹‹é—´é”€å”®å¹³å‡ç™¾åˆ†æ¯”å˜åŒ–ç­‰ã€‚åœ¨å¾·å›½çš„ä¸€ä¸ªæ‚è´§è¿é”åº—æ›¾ç»æœ‰ä¸€åœº[å…ˆå‰çš„æ¯”èµ›](https://www.kaggle.com/c/rossmann-store-sales)ï¼Œå‡ ä¹æ˜¯ä¸€æ ·çš„ã€‚[è·èƒœè€…](http://blog.kaggle.com/2015/12/21/rossmann-store-sales-winners-interview-1st-place-gert/)æ˜¯ä¸€ä¸ªé¢†åŸŸä¸“å®¶ï¼Œæ“…é•¿åšç‰©æµé¢„æµ‹ã€‚ä»–æ ¹æ®è‡ªå·±çš„ç»éªŒåˆ›å»ºäº†è®¸å¤šåˆ—ï¼Œè¿™äº›åˆ—é€šå¸¸å¯¹äºåšé¢„æµ‹æ˜¯æœ‰ç”¨çš„ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå¯ä»¥å¥æ•ˆçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç¬¬ä¸‰åè·å¥–è€…å‡ ä¹æ²¡æœ‰è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Œè€Œä¸”ä»–ä»¬ä¹Ÿæœ‰ä¸€ä¸ªå¤§çš„ç–å¿½ï¼Œè¿™å¯èƒ½å¯¼è‡´ä»–ä»¬å¤±å»ç¬¬ä¸€åã€‚éšç€æ¯”èµ›çš„è¿›è¡Œï¼Œæˆ‘ä»¬å°†å­¦åˆ°æ›´å¤šå…³äºå¦‚ä½•èµ¢å¾—è¿™åœºæ¯”èµ›ä»¥åŠç±»ä¼¼æ¯”èµ›çš„çŸ¥è¯†ã€‚'
- en: Importance of good validation set [[44:53](https://youtu.be/YSFG_W8JxBo?t=44m53s)]
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¥½çš„éªŒè¯é›†çš„é‡è¦æ€§
- en: If you do not have a good validation set, it is hard, if not impossible, to
    create a good model. If you are trying to predict next monthâ€™s sales and you build
    models. If you have no way of knowing whether the models you have built are good
    at predicting sales a month ahead of time, then you have no way of knowing whether
    it is actually going to be any good when you put your model in production. You
    need a validation set that you know is reliable at telling you whether or not
    your model is likely to work well when you put it in production or use it on the
    test set.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ²¡æœ‰ä¸€ä¸ªå¥½çš„éªŒè¯é›†ï¼Œè¦åˆ›å»ºä¸€ä¸ªå¥½çš„æ¨¡å‹æ˜¯å›°éš¾çš„ï¼Œç”šè‡³æ˜¯ä¸å¯èƒ½çš„ã€‚å¦‚æœä½ è¯•å›¾é¢„æµ‹ä¸‹ä¸ªæœˆçš„é”€å”®é¢ï¼Œå¹¶å»ºç«‹æ¨¡å‹ã€‚å¦‚æœä½ æ— æ³•çŸ¥é“ä½ å»ºç«‹çš„æ¨¡å‹æ˜¯å¦æ“…é•¿æå‰ä¸€ä¸ªæœˆé¢„æµ‹é”€å”®é¢ï¼Œé‚£ä¹ˆå½“ä½ å°†æ¨¡å‹æŠ•å…¥ç”Ÿäº§æˆ–åœ¨æµ‹è¯•é›†ä¸Šä½¿ç”¨æ—¶ï¼Œä½ å°±æ— æ³•çŸ¥é“å®ƒæ˜¯å¦çœŸçš„ä¼šå¾ˆå¥½ã€‚ä½ éœ€è¦ä¸€ä¸ªå¯é çš„éªŒè¯é›†ï¼Œå‘Šè¯‰ä½ ä½ çš„æ¨¡å‹æ˜¯å¦æœ‰å¯èƒ½åœ¨æŠ•å…¥ç”Ÿäº§æˆ–åœ¨æµ‹è¯•é›†ä¸Šä½¿ç”¨æ—¶è¡¨ç°è‰¯å¥½ã€‚
- en: Normally you should not use your test set for anything other than using it right
    at the end of the competition or right at the end of the project to find out how
    you did. But there is one thing you can use the test set for in addition â€” that
    is to **calibrate your validation set** [[46:02](https://youtu.be/YSFG_W8JxBo?t=46m2s)].
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æƒ…å†µä¸‹ï¼Œä½ ä¸åº”è¯¥å¯¹æµ‹è¯•é›†åšä»»ä½•å…¶ä»–æ“ä½œï¼Œé™¤éåœ¨æ¯”èµ›ç»“æŸæ—¶æˆ–é¡¹ç›®ç»“æŸæ—¶ä½¿ç”¨å®ƒæ¥æŸ¥çœ‹ä½ çš„è¡¨ç°ã€‚ä½†æ˜¯æœ‰ä¸€ä»¶äº‹ä½ å¯ä»¥åœ¨æµ‹è¯•é›†ä¸­ä½¿ç”¨ â€”â€” é‚£å°±æ˜¯**æ ¡å‡†ä½ çš„éªŒè¯é›†**[[46:02](https://youtu.be/YSFG_W8JxBo?t=46m2s)]ã€‚
- en: '![](../Images/f8963cb5444c2b184ca2fd349c6a28b0.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8963cb5444c2b184ca2fd349c6a28b0.png)'
- en: What Terrance did here was that he built four different models and submitted
    each of the four models to Kaggle to find out its score. X-axis is the score Kaggle
    told us on the leaderboard, and y-axis he plotted the score on a particular validation
    set he was trying out to see whether the validation set was going to be any good.
    If your validation set is good, then the relationship between the leaderboards
    score (i.e. the test set score) should lie in a straight line. Ideally, it will
    lie on the `y = x` line, but honestly that does not matter too much as long as
    relatively speaking it tells you which models are better than which other models,
    then you know which model is the best. In this case, Terrance has managed to come
    up with a validation set which looks like it is going to predict the Kaggle leaderboard
    score well. That is really cool because he can go away and try a hundred different
    types of models, feature engineering, weighting, tweaks, hyper parameters, whatever
    else, see how they go on the validation set, and not have to submit to Kaggle.
    So you will get a lot more iterations, a lot more feedback. This is not just true
    for Kaggle but every machine learning project you do. In general, if your validation
    set is not showing nice fit line, you need think carefully [[48:02](https://youtu.be/YSFG_W8JxBo?t=48m2s)].
    How is the test set constructed? How is my validation set different? You will
    have to draw lots of charts and so forth to find out.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Terranceåœ¨è¿™é‡Œåšçš„æ˜¯å»ºç«‹äº†å››ç§ä¸åŒçš„æ¨¡å‹ï¼Œå¹¶å°†è¿™å››ç§æ¨¡å‹åˆ†åˆ«æäº¤åˆ°Kaggleä¸Šï¼Œä»¥æ‰¾å‡ºå®ƒä»¬çš„å¾—åˆ†ã€‚Xè½´æ˜¯Kaggleåœ¨æ’è¡Œæ¦œä¸Šå‘Šè¯‰æˆ‘ä»¬çš„å¾—åˆ†ï¼Œyè½´æ˜¯ä»–åœ¨ä¸€ä¸ªç‰¹å®šçš„éªŒè¯é›†ä¸Šç»˜åˆ¶çš„å¾—åˆ†ï¼Œä»–è¯•å›¾çœ‹çœ‹è¿™ä¸ªéªŒè¯é›†æ˜¯å¦ä¼šå¾ˆå¥½ã€‚å¦‚æœä½ çš„éªŒè¯é›†å¾ˆå¥½ï¼Œé‚£ä¹ˆæ’è¡Œæ¦œå¾—åˆ†ï¼ˆå³æµ‹è¯•é›†å¾—åˆ†ï¼‰ä¹‹é—´çš„å…³ç³»åº”è¯¥æ˜¯ä¸€æ¡ç›´çº¿ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œå®ƒå°†ä½äº`y
    = x`çº¿ä¸Šï¼Œä½†è€å®è¯´ï¼Œè¿™å¹¶ä¸å¤ªé‡è¦ï¼Œåªè¦ç›¸å¯¹æ¥è¯´å‘Šè¯‰ä½ å“ªäº›æ¨¡å‹æ¯”å“ªäº›æ¨¡å‹æ›´å¥½ï¼Œé‚£ä¹ˆä½ å°±çŸ¥é“å“ªä¸ªæ¨¡å‹æ˜¯æœ€å¥½çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒTerranceè®¾æ³•æ‰¾åˆ°äº†ä¸€ä¸ªçœ‹èµ·æ¥èƒ½å¤Ÿå¾ˆå¥½åœ°é¢„æµ‹Kaggleæ’è¡Œæ¦œå¾—åˆ†çš„éªŒè¯é›†ã€‚è¿™çœŸçš„å¾ˆé…·ï¼Œå› ä¸ºä»–å¯ä»¥å°è¯•ä¸€ç™¾ç§ä¸åŒç±»å‹çš„æ¨¡å‹ã€ç‰¹å¾å·¥ç¨‹ã€åŠ æƒã€è°ƒæ•´ã€è¶…å‚æ•°ç­‰ç­‰ï¼Œçœ‹çœ‹å®ƒä»¬åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ï¼Œè€Œä¸å¿…æäº¤åˆ°Kaggleã€‚å› æ­¤ï¼Œä½ å°†å¾—åˆ°æ›´å¤šçš„è¿­ä»£ï¼Œæ›´å¤šçš„åé¦ˆã€‚è¿™ä¸ä»…é€‚ç”¨äºKaggleï¼Œè€Œä¸”é€‚ç”¨äºä½ åšçš„æ¯ä¸€ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä½ çš„éªŒè¯é›†æ²¡æœ‰æ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ‹Ÿåˆçº¿ï¼Œä½ éœ€è¦ä»”ç»†æ€è€ƒ[[48:02](https://youtu.be/YSFG_W8JxBo?t=48m2s)]ã€‚æµ‹è¯•é›†æ˜¯å¦‚ä½•æ„å»ºçš„ï¼Ÿæˆ‘çš„éªŒè¯é›†æœ‰ä»€ä¹ˆä¸åŒï¼Ÿä½ å°†ä¸å¾—ä¸ç»˜åˆ¶å¾ˆå¤šå›¾è¡¨ç­‰ç­‰æ¥æ‰¾å‡ºã€‚
- en: '**Question:** How do you construct a validation set as close to the test set
    [[48:23](https://youtu.be/YSFG_W8JxBo?t=48m23s)]? Here are a few tips from Terrance:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ï¼š**å¦‚ä½•æ„å»ºä¸€ä¸ªä¸æµ‹è¯•é›†å°½å¯èƒ½æ¥è¿‘çš„éªŒè¯é›†[[48:23](https://youtu.be/YSFG_W8JxBo?t=48m23s)]ï¼Ÿä»¥ä¸‹æ˜¯Terranceçš„ä¸€äº›å»ºè®®ï¼š'
- en: Close by date (i.e. most recent)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ—¥æœŸæ¥è¿‘ï¼ˆå³æœ€è¿‘çš„ï¼‰
- en: First looked at the date range of the test set (16 days), then looked at the
    date range of the kernel which described how to get 0.58 on the leaderboard by
    taking an average (14 days).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆçœ‹ä¸€ä¸‹æµ‹è¯•é›†çš„æ—¥æœŸèŒƒå›´ï¼ˆ16å¤©ï¼‰ï¼Œç„¶åçœ‹ä¸€ä¸‹æè¿°å¦‚ä½•åœ¨æ’è¡Œæ¦œä¸Šè·å¾—0.58åˆ†çš„å†…æ ¸çš„æ—¥æœŸèŒƒå›´ï¼ˆ14å¤©ï¼‰ã€‚
- en: Test set begins on the day after pay day and ends on a pay day.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æµ‹è¯•é›†ä»å‘è–ªæ—¥çš„ç¬¬äºŒå¤©å¼€å§‹ï¼Œåˆ°ä¸‹ä¸€ä¸ªå‘è–ªæ—¥ç»“æŸã€‚
- en: Plot lots of pictures. Even if you did not know it was pay day, you want to
    draw the time series chart and hopefully see that every two weeks there is a spike
    and make sure that you have the same number of spikes in the validation set as
    the test set.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»˜åˆ¶å¾ˆå¤šå›¾ç‰‡ã€‚å³ä½¿ä½ ä¸çŸ¥é“ä»Šå¤©æ˜¯å‘è–ªæ—¥ï¼Œä½ ä¹Ÿæƒ³ç»˜åˆ¶æ—¶é—´åºåˆ—å›¾ï¼Œå¸Œæœ›çœ‹åˆ°æ¯ä¸¤å‘¨æœ‰ä¸€ä¸ªé«˜å³°ï¼Œå¹¶ç¡®ä¿éªŒè¯é›†ä¸­æœ‰ä¸æµ‹è¯•é›†ç›¸åŒæ•°é‡çš„é«˜å³°ã€‚
- en: Interpreting machine learning models [[50:38](https://youtu.be/YSFG_W8JxBo?t=50m38s)
    / [Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson2-rf_interpretation.ipynb)]
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹[[50:38](https://youtu.be/YSFG_W8JxBo?t=50m38s) / [ç¬”è®°æœ¬](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson2-rf_interpretation.ipynb)]
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We start by reading in our feather files for Blue Books for Bulldozers competition.
    Reminder: we have already read in the CSV, processed it into categories, and save
    it in feather format. The next thing we do is call `proc_df` to turn categories
    into integers, deal with missing values, and pull out the dependent variable.
    Then create a validation set just like last week:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆè¯»å–è“è‰²ä¹¦ç±å¯¹æ¨åœŸæœºæ¯”èµ›çš„featheræ–‡ä»¶ã€‚æé†’ï¼šæˆ‘ä»¬å·²ç»è¯»å–äº†CSVæ–‡ä»¶ï¼Œå°†å…¶å¤„ç†ä¸ºç±»åˆ«ï¼Œå¹¶ä¿å­˜ä¸ºfeatheræ ¼å¼ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬è°ƒç”¨`proc_df`å°†ç±»åˆ«è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤„ç†ç¼ºå¤±å€¼ï¼Œå¹¶æå–å‡ºå› å˜é‡ã€‚ç„¶ååˆ›å»ºä¸€ä¸ªåƒä¸Šå‘¨ä¸€æ ·çš„éªŒè¯é›†ï¼š
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Detour to lesson 1 notebook [[51:59](https://youtu.be/YSFG_W8JxBo?t=51m59s)]
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»•é“åˆ°ç¬¬1è¯¾ç¬”è®°æœ¬[[51:59](https://youtu.be/YSFG_W8JxBo?t=51m59s)]
- en: Last week, there was a bug in `proc_df` that was shuffling the dataframe when
    `subset` gets passed in hence causing the validation set to be not the latest
    12000 records. This issue was fixed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šå‘¨ï¼Œåœ¨`proc_df`ä¸­æœ‰ä¸€ä¸ªbugï¼Œå½“ä¼ å…¥`subset`æ—¶ä¼šæ‰“ä¹±æ•°æ®æ¡†ï¼Œå¯¼è‡´éªŒè¯é›†ä¸æ˜¯æœ€æ–°çš„12000æ¡è®°å½•ã€‚è¿™ä¸ªé—®é¢˜å·²ç»ä¿®å¤ã€‚
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Question**: Why is `nas` both input and out put of this function [[53:03](https://youtu.be/YSFG_W8JxBo?t=53m3s)]?
    `proc_df` returns a dictionary telling you which columns were missing and for
    each of those columns what the median was.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šä¸ºä»€ä¹ˆ`nas`æ—¢æ˜¯è¯¥å‡½æ•°çš„è¾“å…¥åˆæ˜¯è¾“å‡º[[53:03](https://youtu.be/YSFG_W8JxBo?t=53m3s)]ï¼Ÿ`proc_df`è¿”å›ä¸€ä¸ªå‘Šè¯‰æ‚¨å“ªäº›åˆ—ä¸¢å¤±ä»¥åŠæ¯ä¸ªä¸¢å¤±åˆ—çš„ä¸­ä½æ•°çš„å­—å…¸ã€‚'
- en: When you call `proc_df` on a larger dataset, you do not pass in `nas` but you
    want to keep that return value.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“æ‚¨åœ¨è¾ƒå¤§çš„æ•°æ®é›†ä¸Šè°ƒç”¨`proc_df`æ—¶ï¼Œä¸éœ€è¦ä¼ å…¥`nas`ï¼Œä½†æ‚¨å¸Œæœ›ä¿ç•™è¯¥è¿”å›å€¼ã€‚
- en: Later on, when you want to create a subset (by passing in `subset`), you want
    to use the same missing columns and medians, so you pass `nas` in.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¨åï¼Œå½“æ‚¨æƒ³è¦åˆ›å»ºä¸€ä¸ªå­é›†ï¼ˆé€šè¿‡ä¼ å…¥`subset`ï¼‰æ—¶ï¼Œæ‚¨å¸Œæœ›ä½¿ç”¨ç›¸åŒçš„ä¸¢å¤±åˆ—å’Œä¸­ä½æ•°ï¼Œå› æ­¤æ‚¨ä¼ å…¥`nas`ã€‚
- en: If it turns out that the subset was from a whole different dataset and had different
    missing columns, it would update the dictionary with additional key value.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœå‘ç°å­é›†æ¥è‡ªå®Œå…¨ä¸åŒçš„æ•°æ®é›†å¹¶ä¸”å…·æœ‰ä¸åŒçš„ä¸¢å¤±åˆ—ï¼Œå®ƒå°†ä½¿ç”¨é™„åŠ é”®å€¼æ›´æ–°å­—å…¸ã€‚
- en: It keeps track of any missing columns you came across in anything you passed
    to `proc_df` .
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒä¼šè·Ÿè¸ªæ‚¨åœ¨ä¼ é€’ç»™`proc_df`çš„ä»»ä½•å†…å®¹ä¸­é‡åˆ°çš„ä»»ä½•ä¸¢å¤±åˆ—ã€‚
- en: Back to lesson 2 notebook [[54:40](https://youtu.be/YSFG_W8JxBo?t=54m40s)]
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›åˆ°ç¬¬2è¯¾ç¬”è®°æœ¬[[54:40](https://youtu.be/YSFG_W8JxBo?t=54m40s)]
- en: Once we have done `proc_df`, this is what it looks like. `SalePrice` is the
    log of the sale price.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬å®Œæˆäº†`proc_df`ï¼Œå®ƒçœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ã€‚`SalePrice`æ˜¯é”€å”®ä»·æ ¼çš„å¯¹æ•°ã€‚
- en: '![](../Images/2ea7c5b8ef1b437bd8868798dc7e61f6.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ea7c5b8ef1b437bd8868798dc7e61f6.png)'
- en: We already know how to get the prediction. We take the average value in each
    leaf node in each tree after running a particular row through each tree. Normally,
    we do not just want a prediction â€” we also want to know how confident we are of
    that prediction.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çŸ¥é“å¦‚ä½•è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬åœ¨é€šè¿‡æ¯æ£µæ ‘è¿è¡Œç‰¹å®šè¡Œåï¼Œåœ¨æ¯æ£µæ ‘çš„æ¯ä¸ªå¶èŠ‚ç‚¹ä¸­å–å¹³å‡å€¼ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä¸ä»…æƒ³è¦ä¸€ä¸ªé¢„æµ‹ - æˆ‘ä»¬è¿˜æƒ³çŸ¥é“æˆ‘ä»¬å¯¹è¯¥é¢„æµ‹çš„ä¿¡å¿ƒæœ‰å¤šå¤§ã€‚
- en: We would be less confident of a prediction if we have not seen many examples
    of rows like this one. In that case, we would not expect any of the trees to have
    a path through â€” which is designed to help us predict that row. So conceptually,
    you would expect then that as you pass this unusual row through different trees,
    it is going to end up in very different places. In other words, rather than just
    taking the mean of the predictions of the trees and saying that is our prediction,
    what if we took the standard deviation of the predictions of the trees? If the
    standard deviation is high, that means each tree is giving us a very different
    estimate of this rowâ€™s prediction. If this was a really common kind of row, the
    trees would have learned to make good predictions for it because it has seen lots
    of opportunities to split based on those kind of rows. So the standard deviation
    of the predictions across the trees gives us at least relative understanding of
    how confident we are of this prediction [[56:39](https://youtu.be/YSFG_W8JxBo?t=56m39s)].
    This is not something which exists in scikit-learn, so we have to create it. But
    we already have almost the exact code we need.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ²¡æœ‰çœ‹åˆ°è®¸å¤šç±»ä¼¼è¿™ä¸€è¡Œçš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯¹é¢„æµ‹ä¼šæ›´ä¸è‡ªä¿¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›ä»»ä½•æ ‘éƒ½é€šè¿‡ - è¿™æœ‰åŠ©äºæˆ‘ä»¬é¢„æµ‹è¯¥è¡Œã€‚å› æ­¤ï¼Œåœ¨æ¦‚å¿µä¸Šï¼Œæ‚¨ä¼šæœŸæœ›å½“æ‚¨é€šè¿‡ä¸åŒæ ‘ä¼ é€’æ­¤ä¸å¯»å¸¸çš„è¡Œæ—¶ï¼Œå®ƒä¼šæœ€ç»ˆå‡ºç°åœ¨éå¸¸ä¸åŒçš„ä½ç½®ã€‚æ¢å¥è¯è¯´ï¼Œä¸å…¶åªå–æ ‘çš„é¢„æµ‹å¹³å‡å€¼å¹¶è¯´è¿™æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ï¼Œä¸å¦‚æˆ‘ä»¬å–æ ‘çš„é¢„æµ‹æ ‡å‡†å·®å‘¢ï¼Ÿå¦‚æœæ ‡å‡†å·®å¾ˆé«˜ï¼Œè¿™æ„å‘³ç€æ¯æ£µæ ‘éƒ½ä¸ºæˆ‘ä»¬æä¾›äº†å¯¹è¯¥è¡Œé¢„æµ‹çš„éå¸¸ä¸åŒçš„ä¼°è®¡ã€‚å¦‚æœè¿™æ˜¯ä¸€ç§éå¸¸å¸¸è§çš„è¡Œï¼Œæ ‘å°†å·²ç»å­¦ä¼šä¸ºå…¶åšå‡ºè‰¯å¥½çš„é¢„æµ‹ï¼Œå› ä¸ºå®ƒå·²ç»çœ‹åˆ°äº†è®¸å¤šåŸºäºè¿™äº›è¡Œçš„åˆ†å‰²æœºä¼šã€‚å› æ­¤ï¼Œè·¨æ ‘çš„é¢„æµ‹æ ‡å‡†å·®è‡³å°‘è®©æˆ‘ä»¬ç›¸å¯¹äº†è§£æˆ‘ä»¬å¯¹è¯¥é¢„æµ‹çš„ä¿¡å¿ƒæœ‰å¤šå¤§[[56:39](https://youtu.be/YSFG_W8JxBo?t=56m39s)]ã€‚è¿™åœ¨scikit-learnä¸­ä¸å­˜åœ¨ï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»åˆ›å»ºå®ƒã€‚ä½†æˆ‘ä»¬å·²ç»æœ‰å‡ ä¹éœ€è¦çš„ç¡®åˆ‡ä»£ç ã€‚
- en: For model interpretation, there is no need to use the full dataset because we
    do not need a massively accurate random forest â€” we just need one which indicates
    the nature of relationships involved.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¨¡å‹è§£é‡Šï¼Œæ²¡æœ‰å¿…è¦ä½¿ç”¨å®Œæ•´çš„æ•°æ®é›†ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦ä¸€ä¸ªéå¸¸å‡†ç¡®çš„éšæœºæ£®æ— - æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªæŒ‡ç¤ºæ‰€æ¶‰åŠå…³ç³»æ€§è´¨çš„éšæœºæ£®æ—ã€‚
- en: Just make sure the sample size is large enough that if you call the same interpretation
    commands multiple times, you do not get different results back each time. In practice,
    50,000 is a high number and it would be surprising if that was not enough (and
    it runs in seconds).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€ç¡®ä¿æ ·æœ¬å¤§å°è¶³å¤Ÿå¤§ï¼Œä»¥ä¾¿å¦‚æœå¤šæ¬¡è°ƒç”¨ç›¸åŒçš„è§£é‡Šå‘½ä»¤ï¼Œæ¯æ¬¡éƒ½ä¸ä¼šå¾—åˆ°ä¸åŒçš„ç»“æœã€‚åœ¨å®è·µä¸­ï¼Œ50,000æ˜¯ä¸€ä¸ªå¾ˆé«˜çš„æ•°å­—ï¼Œå¦‚æœè¿™è¿˜ä¸å¤Ÿçš„è¯ä¼šä»¤äººæƒŠè®¶ï¼ˆè€Œä¸”è¿è¡Œæ—¶é—´åªéœ€å‡ ç§’ï¼‰ã€‚
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is where we can do the exact same list comprehension as the last time
    [[58:35](https://youtu.be/YSFG_W8JxBo?t=58m35s)]:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘ä»¬å¯ä»¥åšä¸ä¸Šæ¬¡å®Œå…¨ç›¸åŒçš„åˆ—è¡¨æ¨å¯¼[[58:35](https://youtu.be/YSFG_W8JxBo?t=58m35s)]ï¼š
- en: '[PRE19]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is how to do it for one observation. This takes quite a while and specifically,
    it is not taking advantage of the fact that my computer has lots of cores in it.
    List comprehensions itself if Python code and Python code (unless you are doing
    something special) runs in serial which means it runs on a single CPU and does
    not take advantage of your multi CPU hardware. If we wanted to run this on more
    trees and more data, the execution time goes up. Wall time (the amount of actual
    time it took) is roughly equal to the CPU time where else if it was running on
    lots of cores, the CPU time would be higher than the wall time [[1:00:05](https://youtu.be/YSFG_W8JxBo?t=1h5s)].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é’ˆå¯¹ä¸€ä¸ªè§‚å¯Ÿç»“æœçš„æ–¹æ³•ã€‚è¿™éœ€è¦ç›¸å½“é•¿çš„æ—¶é—´ï¼Œç‰¹åˆ«æ˜¯å®ƒæ²¡æœ‰å……åˆ†åˆ©ç”¨æˆ‘çš„è®¡ç®—æœºæœ‰å¾ˆå¤šæ ¸å¿ƒè¿™ä¸€äº‹å®ã€‚åˆ—è¡¨æ¨å¯¼æœ¬èº«æ˜¯Pythonä»£ç ï¼ŒPythonä»£ç ï¼ˆé™¤éæ‚¨åœ¨åšä¸€äº›ç‰¹æ®Šçš„äº‹æƒ…ï¼‰è¿è¡Œåœ¨ä¸²è¡Œæ¨¡å¼ä¸‹ï¼Œè¿™æ„å‘³ç€å®ƒåœ¨å•ä¸ªCPUä¸Šè¿è¡Œï¼Œä¸åˆ©ç”¨æ‚¨çš„å¤šCPUç¡¬ä»¶ã€‚å¦‚æœæˆ‘ä»¬æƒ³åœ¨æ›´å¤šæ ‘å’Œæ›´å¤šæ•°æ®ä¸Šè¿è¡Œæ­¤ä»£ç ï¼Œæ‰§è¡Œæ—¶é—´ä¼šå¢åŠ ã€‚å¢™ä¸Šæ—¶é—´ï¼ˆå®é™…èŠ±è´¹çš„æ—¶é—´ï¼‰å¤§è‡´ç­‰äºCPUæ—¶é—´ï¼Œå¦åˆ™å¦‚æœå®ƒåœ¨è®¸å¤šæ ¸å¿ƒä¸Šè¿è¡Œï¼ŒCPUæ—¶é—´å°†é«˜äºå¢™ä¸Šæ—¶é—´[[1:00:05](https://youtu.be/YSFG_W8JxBo?t=1h5s)]ã€‚
- en: 'It turns out Fast.ai library provides a handy function called `parallel_trees`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: åŸæ¥Fast.aiåº“æä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„å‡½æ•°ç§°ä¸º`parallel_trees`ï¼š
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`parallel_trees` takes a random forest model `m` and some function to call
    (here, it is `get_preds`). This calls this function on every tree in parallel.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parallel_trees`æ¥å—ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹`m`å’Œè¦è°ƒç”¨çš„æŸä¸ªå‡½æ•°ï¼ˆè¿™é‡Œæ˜¯`get_preds`ï¼‰ã€‚è¿™ä¼šå¹¶è¡Œåœ¨æ¯æ£µæ ‘ä¸Šè°ƒç”¨æ­¤å‡½æ•°ã€‚'
- en: It will return a list of the result of applying that function to every tree.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒç”¨è¯¥å‡½æ•°å¯¹æ¯æ£µæ ‘åº”ç”¨åè¿”å›ä¸€ä¸ªç»“æœåˆ—è¡¨ã€‚
- en: This will cut down the wall time to 500 milliseconds and giving exactly the
    same answer. Time permitting, we will talk about more general ways of writing
    code that runs in parallel which is super useful for data science, but here is
    one that we can use for random forests.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™å°†æŠŠå¢™ä¸Šçš„æ—¶é—´ç¼©çŸ­åˆ°500æ¯«ç§’ï¼Œå¹¶ç»™å‡ºå®Œå…¨ç›¸åŒçš„ç­”æ¡ˆã€‚å¦‚æœæ—¶é—´å…è®¸ï¼Œæˆ‘ä»¬å°†è®¨è®ºæ›´ä¸€èˆ¬çš„ç¼–å†™å¹¶è¡Œä»£ç çš„æ–¹æ³•ï¼Œè¿™å¯¹æ•°æ®ç§‘å­¦éå¸¸æœ‰ç”¨ï¼Œä½†è¿™é‡Œæœ‰ä¸€ç§æˆ‘ä»¬å¯ä»¥ç”¨äºéšæœºæ£®æ—çš„æ–¹æ³•ã€‚
- en: Plotting [[1:02:02](https://youtu.be/YSFG_W8JxBo?t=1h2m2s)]
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»˜åˆ¶[[1:02:02](https://youtu.be/YSFG_W8JxBo?t=1h2m2s)]
- en: 'We will first create a copy of the data and add the standard deviation of the
    predictions and predictions themselves (the mean) as new columns:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå°†æ•°æ®å¤åˆ¶ä¸€ä»½ï¼Œå¹¶å°†é¢„æµ‹çš„æ ‡å‡†å·®å’Œé¢„æµ‹æœ¬èº«ï¼ˆå‡å€¼ï¼‰ä½œä¸ºæ–°åˆ—æ·»åŠ è¿›å»ï¼š
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/d252129ec3961b66bd1bdb82125af420.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d252129ec3961b66bd1bdb82125af420.png)'
- en: You might remember from last lesson that one of the predictors we have is called
    `Enclosure` and this is an important one as we will see later. Letâ€™s start by
    doing a histogram. One of the nice things about Pandas is it has built-in [plotting
    capabilities](https://pandas.pydata.org/pandas-docs/stable/visualization.html).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½è¿˜è®°å¾—ä¸Šä¸€è¯¾ä¸­æˆ‘ä»¬æœ‰ä¸€ä¸ªå«åš`Enclosure`çš„é¢„æµ‹å˜é‡ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å˜é‡ï¼Œæˆ‘ä»¬ç¨åä¼šçœ‹åˆ°ã€‚è®©æˆ‘ä»¬ä»åšä¸€ä¸ªç›´æ–¹å›¾å¼€å§‹ã€‚Pandasçš„ä¸€ä¸ªå¥½å¤„æ˜¯å®ƒå…·æœ‰å†…ç½®çš„[ç»˜å›¾åŠŸèƒ½](https://pandas.pydata.org/pandas-docs/stable/visualization.html)ã€‚
- en: '**Question**: Can you remind me what enclosure is [[01:02:50](https://youtu.be/YSFG_W8JxBo?t=1h2m50s)]?
    We do not know what it means and it does not matter. The whole purpose of this
    process is that we are going to learn about what things are (or at least what
    things are important and later on figure out what they are and how they are important).
    So we will start out knowing nothing about this dataset. We are just going to
    look at something called `Enclosure` that has something called `EROPS` and `ROPS`
    and we do not even know what this is yet. All we know is that the only three that
    appear in any great quantity are `OROPS`, `EROPS w AC`, and `EROPS`. This is very
    common as a data scientist. You often find yourself looking at data that you are
    not that familiar with and you have to figure out which bits to study more carefully,
    which bits seem to matter, and so forth. In this case, at least know that `EROPS
    AC`, `NO ROPS`, and `None or Unspecified` we really do not care about because
    they basically do not exist. So we will focus on `OROPS`, `EROPS w AC`, and `EROPS`.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šä½ èƒ½æé†’æˆ‘å›´æ æ˜¯ä»€ä¹ˆå—[[01:02:50](https://youtu.be/YSFG_W8JxBo?t=1h2m50s)]ï¼Ÿæˆ‘ä»¬ä¸çŸ¥é“å®ƒçš„æ„æ€ï¼Œä¹Ÿä¸é‡è¦ã€‚è¿™ä¸ªè¿‡ç¨‹çš„æ•´ä¸ªç›®çš„æ˜¯æˆ‘ä»¬å°†å­¦ä¹ å…³äºäº‹ç‰©æ˜¯ä»€ä¹ˆï¼ˆæˆ–è€…è‡³å°‘é‡è¦çš„äº‹ç‰©ï¼Œç„¶åå¼„æ¸…æ¥šå®ƒä»¬æ˜¯ä»€ä¹ˆä»¥åŠå®ƒä»¬çš„é‡è¦æ€§ï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸€å¼€å§‹å¯¹è¿™ä¸ªæ•°æ®é›†ä¸€æ— æ‰€çŸ¥ã€‚æˆ‘ä»¬åªæ˜¯è¦çœ‹ä¸€ä¸‹ä¸€ä¸ªå«åš`Enclosure`çš„ä¸œè¥¿ï¼Œé‡Œé¢æœ‰ä¸€ä¸ªå«åš`EROPS`å’Œ`ROPS`çš„ä¸œè¥¿ï¼Œæˆ‘ä»¬ç”šè‡³è¿˜ä¸çŸ¥é“è¿™æ˜¯ä»€ä¹ˆã€‚æˆ‘ä»¬åªçŸ¥é“åœ¨ä»»ä½•å¤§é‡å‡ºç°çš„æƒ…å†µä¸‹ï¼Œåªæœ‰`OROPS`ã€`EROPS
    w AC`å’Œ`EROPS`ã€‚è¿™åœ¨æ•°æ®ç§‘å­¦å®¶ä¸­éå¸¸å¸¸è§ã€‚ä½ ç»å¸¸å‘ç°è‡ªå·±åœ¨çœ‹ä¸€äº›ä½ ä¸å¤ªç†Ÿæ‚‰çš„æ•°æ®ï¼Œå¹¶ä¸”ä½ å¿…é¡»å¼„æ¸…æ¥šå“ªäº›éƒ¨åˆ†éœ€è¦æ›´ä»”ç»†åœ°ç ”ç©¶ï¼Œå“ªäº›éƒ¨åˆ†ä¼¼ä¹å¾ˆé‡è¦ï¼Œç­‰ç­‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè‡³å°‘çŸ¥é“`EROPS
    AC`ã€`NO ROPS`å’Œ`None or Unspecified`æˆ‘ä»¬çœŸçš„ä¸å…³å¿ƒï¼Œå› ä¸ºå®ƒä»¬åŸºæœ¬ä¸Šä¸å­˜åœ¨ã€‚æ‰€ä»¥æˆ‘ä»¬å°†å…³æ³¨`OROPS`ã€`EROPS w
    AC`å’Œ`EROPS`ã€‚'
- en: 'Here we took our data frame, grouped by `Enclosure`, then took average of 3
    fields [[1:04:00](https://youtu.be/YSFG_W8JxBo?t=1h4m)]:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å–äº†æˆ‘ä»¬çš„æ•°æ®æ¡†ï¼ŒæŒ‰`Enclosure`åˆ†ç»„ï¼Œç„¶åå–äº†3ä¸ªå­—æ®µçš„å¹³å‡å€¼[[1:04:00](https://youtu.be/YSFG_W8JxBo?t=1h4m)]:'
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/bd58ac7ad5332548ad0bf6dd86a15ccd.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd58ac7ad5332548ad0bf6dd86a15ccd.png)'
- en: 'We can already start to learn a little here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»å¼€å§‹åœ¨è¿™é‡Œå­¦ä¹ ä¸€ç‚¹ï¼š
- en: Prediction and the sale price are close to each other on average (good sign)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„æµ‹å’Œé”€å”®ä»·æ ¼å¹³å‡æ¥è¿‘ï¼ˆå¥½è¿¹è±¡ï¼‰
- en: Standard deviation varies a little bit
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡å‡†å·®æœ‰äº›å˜åŒ–
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/5341d4e30fe46779f92e2e28d24e0753.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5341d4e30fe46779f92e2e28d24e0753.png)'
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/308feaabedab15f69099285ec08021c1.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/308feaabedab15f69099285ec08021c1.png)'
- en: 'We used the standard deviation of prediction for the error bars above. This
    will tell us if there is some groups or some rows that we are not very confident
    of at all. We could do something similar for product size:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨äº†ä¸Šé¢é¢„æµ‹çš„æ ‡å‡†å·®æ¥ç»˜åˆ¶è¯¯å·®çº¿ã€‚è¿™å°†å‘Šè¯‰æˆ‘ä»¬æ˜¯å¦æœ‰ä¸€äº›ç»„æˆ–ä¸€äº›è¡Œæˆ‘ä»¬å¹¶ä¸æ˜¯å¾ˆæœ‰ä¿¡å¿ƒã€‚æˆ‘ä»¬å¯ä»¥å¯¹äº§å“å°ºå¯¸åšç±»ä¼¼çš„äº‹æƒ…ï¼š
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/bf732a0447aa7def8c890114c0bd8e3e.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf732a0447aa7def8c890114c0bd8e3e.png)'
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/0375cf222fc5c14c245b092841beb128.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0375cf222fc5c14c245b092841beb128.png)'
- en: You expect, on average, when you are predicting something that is a bigger number
    your standard deviation would be higher. So you can sort by the ratio of the standard
    deviation of the predictions to the predictions themselves [[1:05:51](https://youtu.be/YSFG_W8JxBo?t=1h5m51s)].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æœŸæœ›ï¼Œå¹³å‡è€Œè¨€ï¼Œå½“ä½ é¢„æµ‹ä¸€ä¸ªæ›´å¤§çš„æ•°å­—æ—¶ï¼Œä½ çš„æ ‡å‡†å·®ä¼šæ›´é«˜ã€‚æ‰€ä»¥ä½ å¯ä»¥æŒ‰ç…§é¢„æµ‹çš„æ ‡å‡†å·®ä¸é¢„æµ‹æœ¬èº«çš„æ¯”ç‡æ’åº[[1:05:51](https://youtu.be/YSFG_W8JxBo?t=1h5m51s)]ã€‚
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/ee1e1a26bc04f8cf89be9a391af7cbdb.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee1e1a26bc04f8cf89be9a391af7cbdb.png)'
- en: What this tells us is that product size `Large` and `Compact` , our predictions
    are less accurate (relatively speaking as a ratio of the total price). So if we
    go back and have a look, you see why. These are the smallest groups in the histogram.
    As you would expect, in small groups, we are doing a less good job.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å‘Šè¯‰æˆ‘ä»¬çš„æ˜¯äº§å“å°ºå¯¸`Large`å’Œ`Compact`ï¼Œæˆ‘ä»¬çš„é¢„æµ‹ä¸å¤ªå‡†ç¡®ï¼ˆç›¸å¯¹äºæ€»ä»·æ ¼çš„æ¯”ç‡ï¼‰ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬å›å¤´çœ‹ä¸€ä¸‹ï¼Œä½ ä¼šæ˜ç™½ä¸ºä»€ä¹ˆã€‚è¿™äº›æ˜¯ç›´æ–¹å›¾ä¸­æœ€å°çš„ç»„ã€‚æ­£å¦‚ä½ æ‰€æœŸæœ›çš„ï¼Œåœ¨å°ç»„ä¸­ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ•ˆæœä¸å¤ªå¥½ã€‚
- en: 'You can use this confidence interval for two main purposes:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ç”¨è¿™ä¸ªç½®ä¿¡åŒºé—´åšä¸¤ä¸ªä¸»è¦ç›®çš„ï¼š
- en: You can look at the average confidence interval by group to find out if there
    are groups you do not seem to have confidence about.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥æŒ‰ç»„æŸ¥çœ‹å¹³å‡ç½®ä¿¡åŒºé—´ï¼Œä»¥æ‰¾å‡ºä½ ä¼¼ä¹ä¸å¤ªæœ‰ä¿¡å¿ƒçš„ç»„ã€‚
- en: Perhaps more importantly, you can look at them for specific rows. When you put
    it in production, you might always want to see the confidence interval. For example,
    if you are doing credit scoring to decide whether to give somebody a loan, you
    probably want to see not only what their level of risk is but how confident we
    are. If they want to borrow lots of money and we are not at all confident about
    our ability to predict whether they will pay back, we might want to give them
    a smaller loan.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æ›´é‡è¦çš„æ˜¯ï¼Œä½ å¯ä»¥æŸ¥çœ‹ç‰¹å®šè¡Œçš„é‡è¦æ€§ã€‚å½“ä½ æŠ•å…¥ç”Ÿäº§æ—¶ï¼Œä½ å¯èƒ½æ€»æ˜¯æƒ³çœ‹åˆ°ç½®ä¿¡åŒºé—´ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æ­£åœ¨è¿›è¡Œä¿¡ç”¨è¯„åˆ†æ¥å†³å®šæ˜¯å¦ç»™æŸäººè´·æ¬¾ï¼Œä½ å¯èƒ½ä¸ä»…æƒ³çŸ¥é“ä»–ä»¬çš„é£é™©æ°´å¹³ï¼Œè¿˜æƒ³çŸ¥é“æˆ‘ä»¬æœ‰å¤šå¤§çš„ä¿¡å¿ƒã€‚å¦‚æœä»–ä»¬æƒ³å€Ÿå¾ˆå¤šé’±ï¼Œè€Œæˆ‘ä»¬å¯¹æˆ‘ä»¬çš„é¢„æµ‹èƒ½åŠ›æ¯«æ— ä¿¡å¿ƒï¼Œæˆ‘ä»¬å¯èƒ½ä¼šç»™ä»–ä»¬è¾ƒå°çš„è´·æ¬¾ã€‚
- en: Feature importance [[1:07:20](https://youtu.be/YSFG_W8JxBo?t=1h7m20s)]
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç‰¹å¾é‡è¦æ€§ [[1:07:20](https://youtu.be/YSFG_W8JxBo?t=1h7m20s)]
- en: I always look at feature importance first in practice. Whether Iâ€™m working on
    a Kaggle competition or a real world project, I build a random forest as fast
    as I can, trying to get it to the point that is significantly better than random
    but doesnâ€™t have to be much better than that. And the next thing I do is to plot
    the feature importance.
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œæˆ‘æ€»æ˜¯é¦–å…ˆæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§ã€‚æ— è®ºæˆ‘æ˜¯åœ¨å‚åŠ  Kaggle ç«èµ›è¿˜æ˜¯åœ¨è¿›è¡ŒçœŸå®ä¸–ç•Œé¡¹ç›®ï¼Œæˆ‘éƒ½ä¼šå°½å¿«æ„å»ºä¸€ä¸ªéšæœºæ£®æ—ï¼Œè¯•å›¾è®©å®ƒè¾¾åˆ°æ˜æ˜¾ä¼˜äºéšæœºçš„æ°´å¹³ï¼Œä½†ä¸å¿…æ¯”é‚£æ›´å¥½å¤ªå¤šã€‚æ¥ä¸‹æ¥æˆ‘è¦åšçš„äº‹æƒ…æ˜¯ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§ã€‚
- en: The feature importance tells us in this random forest, which columns mattered.
    We have dozens of columns in this dataset, and here, we are picking out the top
    10\. `rf_feat_importance` is part of Fast.ai library which takes a model `m` and
    dataframe `df_trn` (because we need to know names of columns) and it will give
    you back a Pandas dataframe showing you in order of importance how important each
    column was.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾é‡è¦æ€§å‘Šè¯‰æˆ‘ä»¬åœ¨è¿™ä¸ªéšæœºæ£®æ—ä¸­ï¼Œå“ªäº›åˆ—å¾ˆé‡è¦ã€‚åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­æœ‰å‡ ååˆ—ï¼Œè€Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŒ‘é€‰å‡ºå‰åä¸ªã€‚`rf_feat_importance` æ˜¯ Fast.ai
    åº“çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒæ¥å—ä¸€ä¸ªæ¨¡å‹ `m` å’Œä¸€ä¸ªæ•°æ®æ¡† `df_trn`ï¼ˆå› ä¸ºæˆ‘ä»¬éœ€è¦çŸ¥é“åˆ—çš„åç§°ï¼‰ï¼Œç„¶åä¼šè¿”å›ä¸€ä¸ª Pandas æ•°æ®æ¡†ï¼ŒæŒ‰é‡è¦æ€§é¡ºåºæ˜¾ç¤ºæ¯åˆ—çš„é‡è¦æ€§ã€‚
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](../Images/cac676a1c93aa74c8505e2ac05395602.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cac676a1c93aa74c8505e2ac05395602.png)'
- en: '[PRE29]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](../Images/c729d0eda2fd7ad4756792f82ad2673a.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c729d0eda2fd7ad4756792f82ad2673a.png)'
- en: Since `fi` is a `DataFrame`, we can use `DataFrame` plotting commands [[1:09:00](https://youtu.be/YSFG_W8JxBo?t=1h9m)].
    The important thing is to see that some columns are really important and most
    columns do not really matter at all. In nearly every dataset you use in real life,
    this is what your feature importance is going to look like. There is only a handful
    of columns that you care about, and this is why Jeremy always starts here. At
    this point, in terms of looking into learning about this domain of heavy industrial
    equipment auctions, we only have to care about learning about the columns which
    matter. Are we going to bother learning about `Enclosure`? Depends whether `Enclosure`
    is important. It turns out that it appears in top 10, so we are going to have
    to learn about `Enclosure`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº `fi` æ˜¯ä¸€ä¸ª `DataFrame`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `DataFrame` ç»˜å›¾å‘½ä»¤ [[1:09:00](https://youtu.be/YSFG_W8JxBo?t=1h9m)]ã€‚é‡è¦çš„æ˜¯è¦çœ‹åˆ°ä¸€äº›åˆ—çœŸçš„å¾ˆé‡è¦ï¼Œè€Œå¤§å¤šæ•°åˆ—å®é™…ä¸Šå¹¶ä¸é‡è¦ã€‚åœ¨ä½ åœ¨ç°å®ç”Ÿæ´»ä¸­ä½¿ç”¨çš„å‡ ä¹æ¯ä¸ªæ•°æ®é›†ä¸­ï¼Œä½ çš„ç‰¹å¾é‡è¦æ€§éƒ½ä¼šæ˜¯è¿™ä¸ªæ ·å­ã€‚åªæœ‰å°‘æ•°å‡ åˆ—æ˜¯ä½ å…³å¿ƒçš„ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆ
    Jeremy æ€»æ˜¯ä»è¿™é‡Œå¼€å§‹çš„åŸå› ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œå°±å­¦ä¹ è¿™ä¸ªé‡å‹å·¥ä¸šè®¾å¤‡æ‹å–é¢†åŸŸï¼Œæˆ‘ä»¬åªéœ€è¦å…³å¿ƒé‚£äº›é‡è¦çš„åˆ—ã€‚æˆ‘ä»¬æ˜¯å¦è¦å»äº†è§£ `Enclosure`ï¼Ÿå–å†³äº
    `Enclosure` æ˜¯å¦é‡è¦ã€‚ç»“æœè¡¨æ˜å®ƒå‡ºç°åœ¨å‰ååï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦äº†è§£ `Enclosure`ã€‚
- en: 'We can also plot this as a bar plot:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†å…¶ç»˜åˆ¶ä¸ºæ¡å½¢å›¾ï¼š
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](../Images/7fe9dce9aba9b05771f592a1d5bb56ac.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fe9dce9aba9b05771f592a1d5bb56ac.png)'
- en: The most important thing to do with this is to now sit down with your client,
    your data dictionary, or whatever your source of information is and say to then
    â€œokay, tell me about `YearMade`. What does that mean? Where does it come from?â€
    [[1:10:31](https://youtu.be/YSFG_W8JxBo?t=1h10m31s)] Plot lots of things like
    histogram of `YearMade` and scatter plot of `YearMade` against price and learn
    everything you can because `YearMade` and `Coupler_System` â€” they are the things
    that matter.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æœ€é‡è¦çš„äº‹æƒ…æ˜¯å’Œä½ çš„å®¢æˆ·ã€æ•°æ®å­—å…¸ï¼Œæˆ–è€…ä»»ä½•ä½ çš„ä¿¡æ¯æ¥æºåä¸‹æ¥ï¼Œç„¶åè¯´â€œå¥½çš„ï¼Œå‘Šè¯‰æˆ‘å…³äº `YearMade`ã€‚é‚£æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿå®ƒæ¥è‡ªå“ªé‡Œï¼Ÿâ€[[1:10:31](https://youtu.be/YSFG_W8JxBo?t=1h10m31s)]
    ç»˜åˆ¶å¾ˆå¤šä¸œè¥¿ï¼Œæ¯”å¦‚ `YearMade` çš„ç›´æ–¹å›¾å’Œ `YearMade` ä¸ä»·æ ¼çš„æ•£ç‚¹å›¾ï¼Œå°½å¯èƒ½å­¦åˆ°æ›´å¤šï¼Œå› ä¸º `YearMade` å’Œ `Coupler_System`
    â€”â€” è¿™äº›æ‰æ˜¯é‡è¦çš„äº‹æƒ…ã€‚
- en: What will often happen in real-world projects is that you sit with the the client
    and youâ€™ll say â€œit turns out the `Coupler_System` is the second most important
    thingâ€ and they might say â€œthat makes no sense.â€ That doesnâ€™t mean that there
    is a problem with your model, it means there is a problem with their understanding
    of the data they gave you.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç°å®é¡¹ç›®ä¸­ç»å¸¸å‘ç”Ÿçš„æƒ…å†µæ˜¯ï¼Œä½ å’Œå®¢æˆ·ååœ¨ä¸€èµ·ï¼Œä½ ä¼šè¯´â€œç»“æœ `Coupler_System` æ˜¯ç¬¬äºŒé‡è¦çš„äº‹æƒ…â€ï¼Œè€Œä»–ä»¬å¯èƒ½ä¼šè¯´â€œè¿™æ¯«æ— æ„ä¹‰â€ã€‚è¿™å¹¶ä¸æ„å‘³ç€ä½ çš„æ¨¡å‹æœ‰é—®é¢˜ï¼Œè€Œæ˜¯æ„å‘³ç€ä»–ä»¬å¯¹ä»–ä»¬ç»™ä½ çš„æ•°æ®çš„ç†è§£æœ‰é—®é¢˜ã€‚
- en: Let me give you an example [[1:11:16](https://youtu.be/YSFG_W8JxBo?t=1h11m16s)].
    I entered a Kaggle competition where the goal was to predict which applications
    for grants at a university would be successful. I used this exact approach and
    I discovered a number of columns which were almost entirely predictive of the
    dependent variable. Specifically, when I then looked to see in what way they are
    predictive, it turned out whether they were missing or not was the only thing
    that mattered in his dataset. I ended up winning that competition thanks to this
    insight. Later on, I heard what had happened. It turns out that at that university,
    there is an administrative burden to fill any other database and so for a lot
    of the grant applications, they do not fill in the database for the folks whose
    applications were not accepted. In other words, these missing values in the dataset
    were saying this grand wasnâ€™t accepted because if it was accepted then the admin
    folks will go in and type in that information. This is what we call **data leakage**.
    Data leakage means there is information in the dataset that I was modeling with
    which the university would not have had in real life at that point in time they
    were making a decision. When they are actually deciding which grant applications
    to prioritize, they do not know which ones the admin staff will later on going
    to add information to because it turns out that they were accepted.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: One of the key things you will find here is data leakage problems and that is
    a serious problem you need to deal with [[1:12:56](https://youtu.be/YSFG_W8JxBo?t=1h12m56s)].
    The other thing that will happen is you will often find its signs of collinearity.
    It seems like what happened with `Coupler_System`. `Coupler_System` tells you
    whether or not a particular kind of heavy industrial equipment has a particular
    feature on it. But if it is not that kind of industrial equipment at all, it will
    be missing. So it indicates whether or not it is a certain class of heavy industrial
    equipment. This is not data leakage. This is an actual information you actually
    have at the right time. You just have to be careful interpreting it. So you should
    go through at least the top 10 or look for where the natural break points are
    and really study these things carefully.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: To make life easier, it is sometimes good to throw some data away and see if
    it make any difference. In this case, we have a random forest which was .889 rÂ².
    Here we filter out those where the importance is equal to or less than 0.005 (i.e.
    only keep the one whose importance is greater than 0.005).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The rÂ² did not change much â€” it actually increased a tiny bit. Generally speaking,
    removing redundant columns should not make it worse. If f it makes it worse, they
    were not redundant after all. It might make it a little bit better because if
    you think about how we built these trees, when it is deciding what to split on,
    it has less things to worry about trying, it is less often going to accidentally
    find a crappy column. So there is slightly better opportunity to create a slightly
    better tree with slightly less data, but it is not going to change it by much.
    But it is going to make it a bit faster and it is going to let us focus on what
    matters. Letâ€™s re-run feature importance on this new result [[1:15:49](https://youtu.be/YSFG_W8JxBo?t=1h15m49s)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](../Images/ec4d6bc1a56fcf8d47731b874bd338f5.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: Key thing that has happened is that when you remove redundant columns, you are
    also removing sources of collinearity. In other words, two columns that might
    be related to each other. Collinearity does not make your random forests less
    predictive, but if you have a column A is a little bit related to a column B,
    and B is a strong driver of the independent, what happens is that the importance
    is going to be split between A and B. By removing some of those columns with very
    little impact, it makes your feature importance plot clearer. Before `YearMade`
    was pretty close to `Coupler_System`. But there must have been a bunch of things
    that are collinear with `YearMade` and now you can see `YearMade` really matters.
    This feature importance plot is more reliable than the one before because it has
    a lot less collinearity to confuse us.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ç”Ÿçš„å…³é”®äº‹æƒ…æ˜¯ï¼Œå½“ä½ ç§»é™¤å†—ä½™åˆ—æ—¶ï¼Œä½ ä¹Ÿåœ¨ç§»é™¤å…±çº¿æ€§çš„æ¥æºã€‚æ¢å¥è¯è¯´ï¼Œå¯èƒ½å½¼æ­¤ç›¸å…³çš„ä¸¤åˆ—ã€‚å…±çº¿æ€§ä¸ä¼šä½¿ä½ çš„éšæœºæ£®æ—æ›´å°‘é¢„æµ‹ï¼Œä½†å¦‚æœAåˆ—ä¸Båˆ—ç¨å¾®ç›¸å…³ï¼Œè€ŒBæ˜¯ç‹¬ç«‹å˜é‡çš„ä¸€ä¸ªå¼ºé©±åŠ¨å› ç´ ï¼Œé‚£ä¹ˆé‡è¦æ€§å°†åœ¨Aå’ŒBä¹‹é—´åˆ†é…ã€‚é€šè¿‡ç§»é™¤ä¸€äº›å¯¹ç»“æœå½±å“å¾ˆå°çš„åˆ—ï¼Œä½¿å¾—ä½ çš„ç‰¹å¾é‡è¦æ€§å›¾æ›´æ¸…æ™°ã€‚ä¹‹å‰`YearMade`ä¸`Coupler_System`ç›¸å½“æ¥è¿‘ã€‚ä½†è‚¯å®šæœ‰ä¸€å †ä¸`YearMade`å…±çº¿çš„ä¸œè¥¿ï¼Œç°åœ¨ä½ å¯ä»¥çœ‹åˆ°`YearMade`çœŸçš„å¾ˆé‡è¦ã€‚è¿™ä¸ªç‰¹å¾é‡è¦æ€§å›¾æ¯”ä¹‹å‰æ›´å¯é ï¼Œå› ä¸ºå®ƒå‡å°‘äº†å¾ˆå¤šå…±çº¿æ€§ï¼Œä¸ä¼šè®©æˆ‘ä»¬æ„Ÿåˆ°å›°æƒ‘ã€‚
- en: Letâ€™s talk about how this works [[1:17:21](https://youtu.be/YSFG_W8JxBo?t=1h17m21s)]
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è°ˆè°ˆè¿™æ˜¯å¦‚ä½•è¿ä½œçš„[[1:17:21](https://youtu.be/YSFG_W8JxBo?t=1h17m21s)]
- en: Not only is it really simple, it is a technique you can use not just for random
    forests but for basically any kind of machine learning model. Interestingly, almost
    no one knows this. Many people will tell you there is no way of interpreting this
    particular kind of model (the most important interpretation of a model is knowing
    which things are important) and that is almost certainly not going to be true
    because the technique I am going to teach you actually works for any kind of models.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ä»…éå¸¸ç®€å•ï¼Œè€Œä¸”æ˜¯ä¸€ç§ä½ å¯ä»¥ç”¨äºä»»ä½•ç±»å‹çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„æŠ€æœ¯ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå‡ ä¹æ²¡æœ‰äººçŸ¥é“è¿™ä¸€ç‚¹ã€‚è®¸å¤šäººä¼šå‘Šè¯‰ä½ ï¼Œæ²¡æœ‰åŠæ³•è§£é‡Šè¿™ç§ç‰¹å®šç±»å‹çš„æ¨¡å‹ï¼ˆæ¨¡å‹çš„æœ€é‡è¦è§£é‡Šæ˜¯çŸ¥é“å“ªäº›å› ç´ æ˜¯é‡è¦çš„ï¼‰ï¼Œè¿™å‡ ä¹è‚¯å®šä¸ä¼šæ˜¯çœŸçš„ï¼Œå› ä¸ºæˆ‘è¦æ•™ç»™ä½ çš„æŠ€æœ¯å®é™…ä¸Šé€‚ç”¨äºä»»ä½•ç±»å‹çš„æ¨¡å‹ã€‚
- en: '![](../Images/65e55dcc799c070470c01756ad879a05.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65e55dcc799c070470c01756ad879a05.png)'
- en: We take our bulldozer data set and we have a column `Price` we are trying to
    predict (dependent variable).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‹¿æˆ‘ä»¬çš„æ¨åœŸæœºæ•°æ®é›†ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåˆ—`Price`æˆ‘ä»¬æ­£åœ¨å°è¯•é¢„æµ‹ï¼ˆå› å˜é‡ï¼‰ã€‚
- en: We have 25 independent variables and one of them is `YearMade`.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰25ä¸ªè‡ªå˜é‡ï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯`YearMade`ã€‚
- en: How do we figure out how important `YearMade` is? We have a whole random forest
    and we can find out our predictive accuracy. So we will put all these rows through
    our random forest, and it will spit out some predictions. We will then compare
    them to the actual price (in this case, we get our root mean squared error and
    rÂ²). This is our starting point.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•ç¡®å®š`YearMade`æœ‰å¤šé‡è¦ï¼Ÿæˆ‘ä»¬æœ‰ä¸€ä¸ªå®Œæ•´çš„éšæœºæ£®æ—ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾å‡ºæˆ‘ä»¬çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æŠŠæ‰€æœ‰è¿™äº›è¡Œé€šè¿‡æˆ‘ä»¬çš„éšæœºæ£®æ—ï¼Œå®ƒå°†è¾“å‡ºä¸€äº›é¢„æµ‹ã€‚ç„¶åæˆ‘ä»¬å°†å®ƒä»¬ä¸å®é™…ä»·æ ¼è¿›è¡Œæ¯”è¾ƒï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¾—åˆ°æˆ‘ä»¬çš„å‡æ–¹æ ¹è¯¯å·®å’ŒrÂ²ï¼‰ã€‚è¿™æ˜¯æˆ‘ä»¬çš„èµ·ç‚¹ã€‚
- en: Letâ€™s do exactly the same thing, but this time, take the `YearMade` column and
    randomly shuffle it (i.e. randomly permute just that column). Now `YearMade` has
    exactly the same distribution as before (same mean, same standard deviation).
    But it has no relationships with our dependent variable at all because we totally
    randomly reordered it.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åšå®Œå…¨ç›¸åŒçš„äº‹æƒ…ï¼Œä½†è¿™æ¬¡ï¼Œæ‹¿`YearMade`åˆ—å¹¶éšæœºæ´—ç‰Œå®ƒï¼ˆå³éšæœºæ’åˆ—åªæ˜¯é‚£ä¸€åˆ—ï¼‰ã€‚ç°åœ¨`YearMade`ä¸ä¹‹å‰å®Œå…¨ç›¸åŒçš„åˆ†å¸ƒï¼ˆç›¸åŒçš„å‡å€¼ï¼Œç›¸åŒçš„æ ‡å‡†å·®ï¼‰ã€‚ä½†å®ƒä¸æˆ‘ä»¬çš„å› å˜é‡æ²¡æœ‰ä»»ä½•å…³ç³»ï¼Œå› ä¸ºæˆ‘ä»¬å®Œå…¨éšæœºé‡æ–°æ’åºäº†å®ƒã€‚
- en: Before, we might have found our rÂ² was .89\. After we shuffle `YearMade`, we
    check again, and now rÂ² is .80\. The score got much worse when we destroyed that
    variable.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¹‹å‰ï¼Œæˆ‘ä»¬å¯èƒ½å‘ç°æˆ‘ä»¬çš„rÂ²æ˜¯0.89ã€‚åœ¨æˆ‘ä»¬æ´—ç‰Œ`YearMade`ä¹‹åï¼Œæˆ‘ä»¬å†æ¬¡æ£€æŸ¥ï¼Œç°åœ¨rÂ²æ˜¯0.80ã€‚å½“æˆ‘ä»¬ç ´åé‚£ä¸ªå˜é‡æ—¶ï¼Œå¾—åˆ†å˜å¾—æ›´ç³Ÿäº†ã€‚
- en: Okay, letâ€™s try again. We put `YearMade` back to how it was, and this time letâ€™s
    take `Enclosure` and shuffle that. This time, rÂ²is .84 and we can say the amount
    of decrease in our score for `YearMade` was .09 and the amount of decrease for
    `Enclosure` was .05\. And this is going to give us our feature importances for
    each column.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡ã€‚æˆ‘ä»¬æŠŠ`YearMade`æ¢å¤åˆ°åŸæ¥çš„çŠ¶æ€ï¼Œè¿™æ¬¡è®©æˆ‘ä»¬æ‹¿`Enclosure`æ¥æ´—ç‰Œã€‚è¿™æ¬¡ï¼ŒrÂ²æ˜¯0.84ï¼Œæˆ‘ä»¬å¯ä»¥è¯´`YearMade`å¾—åˆ†å‡å°‘äº†0.09ï¼Œè€Œ`Enclosure`çš„å¾—åˆ†å‡å°‘äº†0.05ã€‚è¿™å°†ä¸ºæˆ‘ä»¬æä¾›æ¯ä¸€åˆ—çš„ç‰¹å¾é‡è¦æ€§ã€‚
- en: '**Question**: Canâ€™t we just exclude the column and check the decay in performance
    [[1:20:31](https://youtu.be/YSFG_W8JxBo?t=1h20m31s)]? You could remove the column
    and train a whole new random forest, but that is going to be really slow. Where
    else this way, we can keep our random forest and just test the predictive accuracy
    of it again. So this is nice and fast by comparison. In this case, we just have
    to rerun every row forward through the forest for each shuffled column.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šæˆ‘ä»¬ä¸èƒ½åªæ’é™¤è¿™ä¸€åˆ—ç„¶åæ£€æŸ¥æ€§èƒ½çš„ä¸‹é™å—[[1:20:31](https://youtu.be/YSFG_W8JxBo?t=1h20m31s)]ï¼Ÿä½ å¯ä»¥åˆ é™¤è¯¥åˆ—å¹¶è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„éšæœºæ£®æ—ï¼Œä½†é‚£å°†ä¼šéå¸¸æ…¢ã€‚è€Œé€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä¿ç•™æˆ‘ä»¬çš„éšæœºæ£®æ—ï¼Œå†æ¬¡æµ‹è¯•å…¶é¢„æµ‹å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œè¿™ç§æ–¹å¼æ›´å¿«é€Ÿã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªéœ€å°†æ¯ä¸ªæ´—ç‰Œåˆ—çš„æ¯ä¸€è¡Œå†æ¬¡é€šè¿‡æ£®æ—è¿è¡Œä¸€éã€‚'
- en: '**Question**: If you want to do multi-collinearity, would you do two of them
    and random shuffle and then three of them [[1:21:12](https://youtu.be/YSFG_W8JxBo?t=1h21m12s)]?
    I donâ€™t think you mean multi-collinearity, I think you mean looking for interaction
    effects. So if you want to say which pairs of variables are most important, you
    could do exactly the same thing each pair in turn. In practice, there are better
    ways to do that because that is obviously computationally pretty expensive and
    so we will try to find time to do that if we can.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šå¦‚æœä½ æƒ³åšå¤šé‡å…±çº¿æ€§ï¼Œä½ ä¼šåšä¸¤ä¸ªç„¶åéšæœºæ´—ç‰Œï¼Œç„¶åä¸‰ä¸ª[[1:21:12](https://youtu.be/YSFG_W8JxBo?t=1h21m12s)]ï¼Ÿæˆ‘è®¤ä¸ºä½ ä¸æ˜¯æŒ‡å¤šé‡å…±çº¿æ€§ï¼Œæˆ‘è®¤ä¸ºä½ æ˜¯æŒ‡å¯»æ‰¾äº¤äº’æ•ˆåº”ã€‚å› æ­¤ï¼Œå¦‚æœä½ æƒ³çŸ¥é“å“ªäº›å˜é‡å¯¹æ˜¯æœ€é‡è¦çš„ï¼Œä½ å¯ä»¥ä¾æ¬¡å¯¹æ¯å¯¹å˜é‡åšå®Œå…¨ç›¸åŒçš„äº‹æƒ…ã€‚å®é™…ä¸Šï¼Œæœ‰æ›´å¥½çš„æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºæ˜¾ç„¶è¿™åœ¨è®¡ç®—ä¸Šæ˜¯éå¸¸æ˜‚è´µçš„ï¼Œæ‰€ä»¥å¦‚æœå¯èƒ½çš„è¯ï¼Œæˆ‘ä»¬å°†å°è¯•æ‰¾æ—¶é—´æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚'
- en: 'We now have a model which is a little bit more accurate and we have learned
    a lot more about it. So we are out of time and what I would suggest you try doing
    now before next class for this bulldozers dataset is going through the top 5 or
    10 predictors and try and learn what you can about how to draw plots in Pandas
    and try to come back with some insights about things like:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªç¨å¾®æ›´å‡†ç¡®çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯¹å®ƒäº†è§£æ›´å¤šã€‚æ‰€ä»¥æˆ‘ä»¬æ—¶é—´ä¸å¤Ÿäº†ï¼Œæˆ‘å»ºè®®ä½ åœ¨ä¸‹ä¸€å ‚è¯¾ä¹‹å‰å°è¯•åšçš„æ˜¯ï¼ŒæŸ¥çœ‹å‰5æˆ–10ä¸ªé¢„æµ‹å˜é‡ï¼Œå°è¯•å­¦ä¹ å¦‚ä½•åœ¨Pandasä¸­ç»˜åˆ¶å›¾è¡¨ï¼Œå¹¶å°è¯•å›æ¥å¸¦ä¸€äº›å…³äºä»¥ä¸‹äº‹é¡¹çš„è§è§£ï¼š
- en: what is the relationship between `YearMade` and the dependent variable
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`YearMade`å’Œå› å˜é‡ä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆ'
- en: what is the histogram of `YearMade`
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`YearMade`çš„ç›´æ–¹å›¾æ˜¯ä»€ä¹ˆæ ·çš„'
- en: now that you know `YearMade` is really important, check if there is some noise
    in that column which we could fix
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ çŸ¥é“`YearMade`éå¸¸é‡è¦ï¼Œæ£€æŸ¥ä¸€ä¸‹è¿™ä¸€åˆ—ä¸­æ˜¯å¦æœ‰ä¸€äº›å™ªéŸ³ï¼Œæˆ‘ä»¬å¯ä»¥ä¿®å¤ã€‚
- en: Check if there is some weird encoding in that column we can fix
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ä¸€ä¸‹è¿™ä¸€åˆ—ä¸­æ˜¯å¦æœ‰ä¸€äº›å¥‡æ€ªçš„ç¼–ç é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä¿®å¤ã€‚
- en: This idea Jeremy had that maybe `Coupler_System` is there entirely because it
    is collinear with something else, you might want try and figure out if itâ€™s true.
    If so, how would you do it?
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeremyæå‡ºçš„è¿™ä¸ªæƒ³æ³•ï¼Œä¹Ÿè®¸`Coupler_System`å®Œå…¨å­˜åœ¨æ˜¯å› ä¸ºå®ƒä¸å…¶ä»–æŸäº›ä¸œè¥¿å…±çº¿ï¼Œä½ å¯èƒ½æƒ³è¦å°è¯•å¼„æ¸…æ¥šè¿™æ˜¯å¦å±å®ã€‚å¦‚æœæ˜¯è¿™æ ·ï¼Œä½ ä¼šæ€ä¹ˆåšå‘¢ï¼Ÿ
- en: '`fiProductClassDesc` that rings alarm bells â€” it sounds like it might be a
    high cardinality categorical variable. It might be something with lots and lots
    levels because it sounds like it is a model name. So go and have a look at that
    model name â€” does it have some order into it? Could you make it an ordinal variable
    to make it better? Does it have some kind of hierarchical structure in the string
    that we can split it on hyphen to create more sub columns.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fiProductClassDesc`è¿™ä¸ªè®©äººè­¦æƒ•çš„åå­—â€”â€”å¬èµ·æ¥å¯èƒ½æ˜¯ä¸€ä¸ªé«˜åŸºæ•°åˆ†ç±»å˜é‡ã€‚å®ƒå¯èƒ½æ˜¯ä¸€ä¸ªæœ‰å¾ˆå¤šçº§åˆ«çš„ä¸œè¥¿ï¼Œå› ä¸ºå®ƒå¬èµ·æ¥åƒæ˜¯ä¸€ä¸ªå‹å·åç§°ã€‚æ‰€ä»¥å»çœ‹çœ‹é‚£ä¸ªå‹å·åç§°â€”â€”å®ƒæœ‰ä¸€å®šçš„é¡ºåºå—ï¼Ÿä½ èƒ½æŠŠå®ƒå˜æˆä¸€ä¸ªæœ‰åºå˜é‡å—ï¼Ÿå®ƒåœ¨å­—ç¬¦ä¸²ä¸­æœ‰ä¸€äº›å±‚æ¬¡ç»“æ„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿å­—ç¬¦æ‹†åˆ†å®ƒæ¥åˆ›å»ºæ›´å¤šçš„å­åˆ—ã€‚'
- en: Have a think about this. Try and make it so that by when you come back, youâ€™ve
    got some new, ideally a better accuracy than what I just showed because you found
    some new insights or at least that you can tell the class about some things you
    have learnt about how heavy industrial equipment auctions work in practice.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³ä¸€æƒ³è¿™ä¸ªé—®é¢˜ã€‚è¯•ç€è®©ä½ å›æ¥æ—¶ï¼Œä½ æœ‰ä¸€äº›æ–°çš„ï¼Œæœ€å¥½æ˜¯æ¯”æˆ‘åˆšåˆšå±•ç¤ºçš„æ›´å‡†ç¡®çš„ä¸œè¥¿ï¼Œå› ä¸ºä½ æ‰¾åˆ°äº†ä¸€äº›æ–°çš„è§è§£ï¼Œæˆ–è€…è‡³å°‘ä½ å¯ä»¥å‘Šè¯‰ç­ä¸Šä¸€äº›ä½ å­¦åˆ°çš„æœ‰å…³é‡å‹å·¥ä¸šè®¾å¤‡æ‹å–çš„å®é™…å·¥ä½œæ–¹å¼çš„äº‹æƒ…ã€‚
