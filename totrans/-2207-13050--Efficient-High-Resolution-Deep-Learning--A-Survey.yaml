- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:45:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:45:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2207.13050] Efficient High-Resolution Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2207.13050] 高效高分辨率深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2207.13050](https://ar5iv.labs.arxiv.org/html/2207.13050)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2207.13050](https://ar5iv.labs.arxiv.org/html/2207.13050)
- en: 'Efficient High-Resolution Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效高分辨率深度学习：综述
- en: 'Arian Bakhtiarnia, Qi Zhang, and Alexandros Iosifidis Arian Bakhtiarnia, Qi
    Zhang and Alexandros Iosifidis are with DIGIT, the Department of Electrical and
    Computer Engineering, Aarhus University, Aarhus, Midtjylland, Denmark (e-mail:
    arianbakh@ece.au.dk; qz@ece.au.dk; ai@ece.au.dk).This work was funded by the European
    Union’s Horizon 2020 research and innovation programme under grant agreement No
    957337, and by the Danish Council for Independent Research under Grant No. 9131-00119B.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**Arian Bakhtiarnia**、**Qi Zhang** 和 **Alexandros Iosifidis** 现任职于 DIGIT，奥胡斯大学电气与计算机工程系，丹麦中日德兰地区奥胡斯（电子邮件：arianbakh@ece.au.dk；qz@ece.au.dk；ai@ece.au.dk）。该研究得到了欧洲联盟
    Horizon 2020 研究与创新计划的资助，资助协议编号为 957337，同时也得到了丹麦独立研究委员会的资助，资助编号为 9131-00119B。'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Cameras in modern devices such as smartphones, satellites and medical equipment
    are capable of capturing very high resolution images and videos. Such high-resolution
    data often need to be processed by deep learning models for cancer detection,
    automated road navigation, weather prediction, surveillance, optimizing agricultural
    processes and many other applications. Using high-resolution images and videos
    as direct inputs for deep learning models creates many challenges due to their
    high number of parameters, computation cost, inference latency and GPU memory
    consumption. Simple approaches such as resizing the images to a lower resolution
    are common in the literature, however, they typically significantly decrease accuracy.
    Several works in the literature propose better alternatives in order to deal with
    the challenges of high-resolution data and improve accuracy and speed while complying
    with hardware limitations and time restrictions. This survey describes such efficient
    high-resolution deep learning methods, summarizes real-world applications of high-resolution
    deep learning, and provides comprehensive information about available high-resolution
    datasets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现代设备如智能手机、卫星和医疗设备中的相机能够捕捉非常高分辨率的图像和视频。这些高分辨率数据通常需要通过深度学习模型进行处理，例如癌症检测、自动道路导航、天气预测、监控、优化农业过程等许多应用。使用高分辨率图像和视频作为深度学习模型的直接输入带来了许多挑战，如参数数量庞大、计算成本高、推理延迟和
    GPU 内存消耗。文献中常见的简单方法是将图像调整为较低分辨率，但这通常会显著降低准确性。文献中有几项工作提出了更好的替代方案，以应对高分辨率数据的挑战，并在遵守硬件限制和时间限制的同时提高准确性和速度。本综述描述了这些高效高分辨率深度学习方法，总结了高分辨率深度学习的实际应用，并提供了关于可用高分辨率数据集的全面信息。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: high-resolution deep learning, efficient deep learning, vision transformer,
    computer vision
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率深度学习、高效深度学习、视觉变换器、计算机视觉
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'Many modern devices such as smartphones, drones, augmented reality headsets,
    vehicles and other Internet of Things (IoT) devices are equipped with high-quality
    cameras that can capture high-resolution images and videos. With the help of image
    stitching techniques, camera arrays [[1](#bib.bib1), [2](#bib.bib2)], gigapixel
    acquisition robots [[3](#bib.bib3)] and whole-slide scanners [[4](#bib.bib4)],
    capture resolutions can be increased to billions of pixels (commonly referred
    to as gigapixels), such as the image depicted in Figure [1](#S1.F1 "Figure 1 ‣
    I Introduction ‣ Efficient High-Resolution Deep Learning: A Survey"). One could
    attempt to define high-resolution based on the capabilities of human visual system.
    However, many deep learning tasks rely on data captured by equipment which behaves
    very differently compared to the human eye, such as microscopes, satellite imagery
    and infrared cameras. Furthermore, utilizing more detail than the eye can sense
    is beneficial in many deep learning tasks, such as in the applications discussed
    in Section [II](#S2 "II Applications of High-Resolution Deep Learning ‣ Efficient
    High-Resolution Deep Learning: A Survey"). The amount of detail that can be captured
    and is useful if processed varies greatly from task to task. Therefore, the definition
    of high-resolution is task-dependent. For instance, in image classification and
    computed tomography (CT) scan processing, a resolution of 512$\times$512 pixels
    is considered to be high [[5](#bib.bib5), [6](#bib.bib6)]. In visual crowd counting,
    datasets with High-Definition (HD) resolutions or higher are common [[7](#bib.bib7)],
    and whole-slide images (WSIs) in histopathology, which is the study of diseases
    of the tissues, or remote sensing data, which are captured by aircrafts or satellites,
    can easily reach gigapixel resolutions [[8](#bib.bib8), [9](#bib.bib9)].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '许多现代设备，如智能手机、无人机、增强现实头戴设备、车辆和其他物联网（IoT）设备，都配备了能够拍摄高分辨率图像和视频的高质量相机。在图像拼接技术、相机阵列[[1](#bib.bib1),
    [2](#bib.bib2)]、千像素采集机器人[[3](#bib.bib3)]和全片扫描仪[[4](#bib.bib4)]的帮助下，拍摄的分辨率可以提高到数十亿像素（通常称为千像素），例如图像[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Efficient High-Resolution Deep Learning: A Survey")所示的图像。可以尝试根据人眼的能力来定义高分辨率。然而，许多深度学习任务依赖于设备捕获的数据，这些设备的行为与人眼非常不同，例如显微镜、卫星图像和红外相机。此外，在许多深度学习任务中，利用超过眼睛能够感知的细节是有益的，例如在第[II](#S2
    "II Applications of High-Resolution Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey")节中讨论的应用。可以捕获并在处理时有用的细节量因任务而异。因此，高分辨率的定义是任务依赖性的。例如，在图像分类和计算机断层扫描（CT）处理过程中，512$\times$512像素的分辨率被认为是高的[[5](#bib.bib5),
    [6](#bib.bib6)]。在视觉人群计数中，高清（HD）分辨率或更高分辨率的数据集很常见[[7](#bib.bib7)]，而在组织病理学（即研究组织疾病）中的全片图像（WSIs）或由飞机或卫星拍摄的遥感数据中，分辨率很容易达到千像素水平[[8](#bib.bib8),
    [9](#bib.bib9)]。'
- en: Moreover, with the constant advancement of hardware and methodologies, what
    deep learning literature considers high-resolution has shifted over time. For
    instance, in the late 1990s, processing the 32$\times$32-pixel MNIST images with
    neural networks was an accomplishment [[10](#bib.bib10)], whereas in early 2010s,
    the 256$\times$256-pixel images in ImageNet were considered high-resolution [[11](#bib.bib11)].
    This trend can also be seen in the consistent increase of the average resolution
    of images in popular deep learning datasets, such as crowd counting [[7](#bib.bib7)]
    and anomaly detection [[12](#bib.bib12)] datasets. Therefore, the definition of
    high-resolution is also period-dependent. Based on the task- and period-dependence
    properties, it is clear that the term “high-resolution” is technical, not fundamental
    or universal. Therefore, instead of trying to derive such a definition, we shift
    our focus to resolutions that create technical challenges in deep learning at
    the time of this writing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着硬件和方法学的不断进步，深度学习文献中认为的高分辨率标准也在不断变化。例如，在1990年代末期，用神经网络处理32$\times$32像素的MNIST图像是一项成就[[10](#bib.bib10)]，而在2010年代初期，256$\times$256像素的ImageNet图像被认为是高分辨率[[11](#bib.bib11)]。这种趋势也可以从流行深度学习数据集（如人群计数[[7](#bib.bib7)]和异常检测[[12](#bib.bib12)]数据集的平均图像分辨率的持续增加中看出。因此，高分辨率的定义也具有时期依赖性。基于任务和时期的依赖性，可以清楚地看出，“高分辨率”这一术语是技术性的，而非根本性或普遍性的。因此，我们不再尝试推导这样的定义，而是将重点转向在撰写本文时对深度学习构成技术挑战的分辨率。
- en: '| ![Refer to caption](img/8b0e32c02b51345c190e05620f0998dc.png) | ![Refer to
    caption](img/72b9d8266db4cf53252216efc868c83f.png) |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/8b0e32c02b51345c190e05620f0998dc.png) | ![参考说明](img/72b9d8266db4cf53252216efc868c83f.png)
    |'
- en: '| (a) | (b) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| (a) | (b) |'
- en: 'Figure 1: Example of a gigapixel image, taken from the PANDA-Crowd dataset
    [[13](#bib.bib13)], captured using an array of micro-cameras; (a) original image
    with a size of 26,558$\times$14,828 pixels, and (b) zoomed in to the location
    specified by the red rectangle in the original image, with a size of 2,516$\times$1,347
    pixels, which is more than 100 times smaller than the original image, yet still
    approximately 5 times larger than the image size processed by state-of-the-art
    deep learning models for crowd counting such as SASNet [[14](#bib.bib14)], which
    is 1024$\times$768, and around 50 times larger than the standard image size processed
    by image classification models, which is 224$\times$224.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：来自PANDA-Crowd数据集[[13](#bib.bib13)]的一个千亿像素图像示例，该图像使用微型相机阵列拍摄；(a) 原始图像，尺寸为26,558$\times$14,828像素，(b)
    放大到原始图像中红色矩形指定的位置，尺寸为2,516$\times$1,347像素，比原始图像小了100倍以上，但仍然大约是用于人群计数的最先进深度学习模型（如SASNet
    [[14](#bib.bib14)]）处理的图像尺寸的5倍（1024$\times$768），以及标准图像分类模型处理的图像尺寸的50倍（224$\times$224）。
- en: 'Using high-resolution images and videos directly as inputs to deep learning
    models creates challenges during both training and inference phases. With the
    exception of fully-convolutional networks (FCNs), the number of parameters in
    deep learning models typically increases with larger input sizes. Moreover, the
    amount of computation, which is commonly measured in terms of floating point operations
    (FLOPs), and therefore inference/training time, as well as GPU memory consumption
    increase with higher-resolution inputs, as shown in Figure [2](#S1.F2 "Figure
    2 ‣ I Introduction ‣ Efficient High-Resolution Deep Learning: A Survey"). This
    issue is especially problematic in Vision Transformer (ViT) architectures, which
    use the self-attention mechanism, where the inference speed and number of parameters
    scale quadratically with input size [[6](#bib.bib6), [15](#bib.bib15)]. These
    issues are exacerbated when the training or inference needs to be done on resource-constrained
    devices, such as smartphones, that have limited computational capabilities compared
    to high-end computing equipment, such as workstations or servers.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 直接使用高分辨率图像和视频作为深度学习模型的输入，在训练和推理阶段都会带来挑战。除了完全卷积网络（FCNs）之外，深度学习模型的参数数量通常会随着输入尺寸的增大而增加。此外，计算量（通常以浮点运算（FLOPs）来衡量），因此推理/训练时间以及GPU内存消耗随着高分辨率输入的增加而增加，如图[2](#S1.F2
    "图2 ‣ I 引言 ‣ 高分辨率深度学习：综述")所示。这个问题在使用自注意力机制的Vision Transformer (ViT)架构中尤其突出，其中推理速度和参数数量与输入尺寸的平方成正比[[6](#bib.bib6),
    [15](#bib.bib15)]。当训练或推理需要在资源受限的设备（如智能手机）上进行时，这些问题尤为严重，因为这些设备的计算能力有限，相比于高端计算设备（如工作站或服务器）。
- en: '| ![Refer to caption](img/c73099a058f3b225aec8f66ed7d7bcc7.png) | ![Refer to
    caption](img/e7b67eda50f1a15156e11d56f5a23637.png) | ![Refer to caption](img/7288a5ec18d0127c857604478a992dde.png)
    | ![Refer to caption](img/9a0631c4ab6e1aa797425b41bd9bf314.png) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/c73099a058f3b225aec8f66ed7d7bcc7.png) | ![参考说明](img/e7b67eda50f1a15156e11d56f5a23637.png)
    | ![参考说明](img/7288a5ec18d0127c857604478a992dde.png) | ![参考说明](img/9a0631c4ab6e1aa797425b41bd9bf314.png)
    |'
- en: '| (a) | (b) | (c) | (d) |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| (a) | (b) | (c) | (d) |'
- en: 'Figure 2: As the resolution of the input image increases, so does (a) the amount
    of computation, (b) inference time, and (c) GPU memory usage in the EfficientNet-B7
    [[16](#bib.bib16)]; and (d) the number of parameters in the ViT-B16 [[6](#bib.bib6)]
    architecture. The last layer of EfficientNet-B7 was removed to form a fully-convolutional
    feature extractor. Since accuracy is not considered in these figures, there is
    no need to use real images, thus randomly generated images are given to the models
    as input. All experiments were conducted on an Nvidia A6000 GPU.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：随着输入图像分辨率的增加，(a) 计算量，(b) 推理时间，和 (c) 在EfficientNet-B7 [[16](#bib.bib16)]中的GPU内存使用量也增加；以及
    (d) ViT-B16 [[6](#bib.bib6)]架构中的参数数量。EfficientNet-B7的最后一层被移除以形成一个完全卷积的特征提取器。由于这些图中的精度没有考虑，因此不需要使用真实图像，因此随机生成的图像被作为输入提供给模型。所有实验都在Nvidia
    A6000 GPU上进行。
- en: Even though methods such as model parallelism can be used to split the model
    between multiple GPUs during both the training [[17](#bib.bib17), [18](#bib.bib18)]
    and inference [[19](#bib.bib19)] phases, and thus avoid memory and latency issues,
    these methods require a large amount of resources, such as a large number of GPUs
    and servers, which can incur high costs, especially when working with extreme
    resolutions such as gigapixel images. Furthermore, in many applications, such
    as self-driving cars and drone image processing, there is a limit for the hardware
    that can be mounted, and offloading the computation to external servers is not
    always possible because of unreliability of the network connection due to movement
    and the time-critical nature of the application. Therefore, the most common approach
    for deep learning training and inference is to load the full model on each single
    GPU instance. Multi-GPU setups are instead typically used to speed up the training
    by increasing the overall batch size, to test multiple sets of hyper-parameters
    in parallel or to distribute the inference load. Consequently, in many cases,
    there is an effective maximum resolution that can be processed by deep learning
    models. As an example, the maximum resolution for inference using SASNet [[14](#bib.bib14)],
    which is the state-of-the-art model for crowd counting on the Shanghai Tech dataset
    [[20](#bib.bib20)] at the time of this writing, is around 1024$\times$768 (less
    than HD) on Nvidia 2080 Ti GPUs which have 11 GBs of video memory.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管诸如模型并行化的方法可以在训练[[17](#bib.bib17)、[18](#bib.bib18)]和推理[[19](#bib.bib19)]阶段将模型拆分到多个GPU上，从而避免内存和延迟问题，但这些方法需要大量的资源，如大量的GPU和服务器，这可能会带来高成本，尤其是在处理如千像素图像等极端分辨率时。此外，在许多应用场景中，如自动驾驶汽车和无人机图像处理，硬件的安装有一定限制，且由于网络连接的不可靠性和应用的时间敏感性，将计算任务卸载到外部服务器并不总是可行。因此，深度学习训练和推理最常见的方法是将完整模型加载到每个GPU实例上。多GPU设置通常用于通过增加总体批量大小来加速训练，测试多个超参数集并行处理，或分配推理负载。因此，在许多情况下，深度学习模型可以处理的最大分辨率是有效的。例如，使用SASNet[[14](#bib.bib14)]进行推理的最大分辨率（这是当前在上海科技数据集[[20](#bib.bib20)]上的前沿模型）在具有11
    GB显存的Nvidia 2080 Ti GPU上大约为1024$\times$768（低于HD）。
- en: 'Although newer generations of GPUs are getting faster and have more memory
    available, the resolution of images and videos captured by devices is also increasing.
    Figure [3](#S1.F3 "Figure 3 ‣ I Introduction ‣ Efficient High-Resolution Deep
    Learning: A Survey") shows this trend across recent years for multiple types of
    devices. Therefore, the aforementioned issues will likely persist even with advances
    in computation hardware technology. Furthermore, current imaging technologies
    are nowhere near the physical limits of image resolutions, which is estimated
    to be in petapixels [[21](#bib.bib21)].'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管新一代GPU变得更快且内存更大，但设备捕获的图像和视频分辨率也在增加。图[3](#S1.F3 "图3 ‣ 引言 ‣ 高效高分辨率深度学习：一项调查")展示了最近几年各种设备的这一趋势。因此，即使计算硬件技术有所进步，前述问题可能仍会存在。此外，目前的成像技术远未达到图像分辨率的物理极限，估计为拍摄像素[[21](#bib.bib21)]。
- en: '![Refer to caption](img/c8eb2c63625a4c873af4190b8eca09b7.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c8eb2c63625a4c873af4190b8eca09b7.png)'
- en: 'Figure 3: Trend of the maximum resolutions captured by smartphones (Apple iPhone
    and Samsung Galaxy S), drones (DJI Phantom), augmented reality headsets (Microsoft
    HoloLens) and IoT devices (Raspberry Pi) over time. Details and data sources are
    available in appendix [A](#A1 "Appendix A Data Sources ‣ Efficient High-Resolution
    Deep Learning: A Survey").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：智能手机（Apple iPhone和Samsung Galaxy S）、无人机（DJI Phantom）、增强现实头戴设备（Microsoft HoloLens）和物联网设备（Raspberry
    Pi）随时间变化的最大分辨率趋势。详细信息和数据来源见附录[A](#A1 "附录A 数据来源 ‣ 高效高分辨率深度学习：一项调查")。
- en: 'Whether or not capturing and processing a higher resolution leads to improvements
    depends on the particular problem at hand. For instance, in image classification,
    it is unlikely that increasing the resolution for images of objects or animals
    to gigapixels would reveal more beneficial details and improve the accuracy. On
    the other hand, if the goal is to count the total number of people in scenes such
    as the one presented in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Efficient
    High-Resolution Deep Learning: A Survey"), using an HD resolution instead of gigapixels
    would mean that several people could be represented by a single pixel, which significantly
    increases the error. Similarly, it has been shown that using higher resolutions
    in histopathology can lead to better results [[22](#bib.bib22)].'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '捕获和处理更高分辨率是否能带来改进取决于具体问题。例如，在图像分类中，将对象或动物的图像分辨率提高到吉像素级别不太可能揭示更多有益细节并提高准确度。另一方面，如果目标是计算图像中总人数，如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Efficient High-Resolution Deep Learning: A Survey")所示，使用HD分辨率而不是吉像素将意味着几个个人可能被表示为一个像素，这会显著增加误差。同样，已证明在组织病理学中使用更高分辨率可以得到更好的结果[[22](#bib.bib22)]。'
- en: Assuming there is an effective maximum resolution for a particular problem due
    to hardware limitations or latency requirements, there are two simple baseline
    approaches for processing the original captured inputs which are commonly used
    in deep learning literature [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)].
    The popularity of these baselines can be attributed to the simplicity of their
    implementation. The first one is to resize (downsample) the original input to
    the desired resolution, however, this will lead to a lower accuracy if any important
    details for the problem at hand are lost. This approach is called uniform downsampling
    (UD) since the quality is reduced uniformly throughout the image. The second approach
    is to cut up the original input into smaller patches that each have a maximum
    resolution, process the patches independently, and aggregate the results, for
    instance, by summing them up for regression problems and majority voting for classification
    problems. We call this approach cutting into patches (CIP). There are two issues
    with this approach. First, many deep learning models rely on global features which
    will be lost since features extracted from each patch will not be shared with
    other patches, leading to decreased accuracy. For instance, crowd counting methods
    typically heavily rely on global information such as perspective or illumination
    [[7](#bib.bib7)], and in object detection, objects near the boundaries may be
    split between multiple patches. Secondly, since multiple passes of inference are
    performed, that is, one pass for each patch, inference will take much longer.
    This issue is worse when patches overlap.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设由于硬件限制或延迟要求，某个特定问题有一个有效的最大分辨率，那么在深度学习文献中常用的两种简单基线方法用于处理原始捕获的输入[[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)]。这些基线方法的普及可以归因于其实现的简单性。第一种方法是将原始输入调整（下采样）到所需的分辨率，但如果丢失了任何对当前问题重要的细节，这将导致准确度降低。这种方法称为均匀下采样（UD），因为图像的质量在整个图像中均匀降低。第二种方法是将原始输入切分成较小的补丁，每个补丁具有最大分辨率，独立处理这些补丁，然后汇总结果，例如，对于回归问题，通过将结果求和来汇总，对于分类问题，通过多数投票来汇总。我们称这种方法为切割补丁（CIP）。这种方法有两个问题。首先，许多深度学习模型依赖于全局特征，这些特征会丢失，因为从每个补丁中提取的特征不会与其他补丁共享，导致准确度下降。例如，人群计数方法通常依赖于全局信息，如透视或照明[[7](#bib.bib7)]，而在目标检测中，接近边界的物体可能会被分割到多个补丁之间。其次，由于进行多个推理过程，即对每个补丁进行一次推理，推理时间会显著增加。当补丁重叠时，这一问题更为严重。
- en: To highlight these issues, we test the two baseline approaches (UD and CIP)
    on the Shanghai Tech Part B dataset [[20](#bib.bib20)] for crowd counting, which
    contains images of size 1024$\times$768 pixels. We reduce the original image size
    by factors of 4 and 16 and measure the mean absolute error (MAE) for both baselines.
    To test UD, we take a SASNet model [[14](#bib.bib14)] pre-trained on the Shanghai
    Tech Part B dataset [[20](#bib.bib20)] with input size of 1024$\times$768, and
    fine-tune it for the target input size using the AdamW optimizer [[26](#bib.bib26)]
    with a learning rate of $10^{-5}$ and weight decay of $10^{-4}$. Note that the
    original SASNet paper uses the Adam optimizer [[27](#bib.bib27)] with a learning
    rate of $10^{-5}$. We train the model for 100 epochs with batch size of 12 per
    GPU instance using 3$\times$Nvidia A6000 GPUs. We empirically found that fine-tuning
    does not improve the accuracy of cutting into patches, therefore, we cut the original
    image into 4 and 16 patches, and obtain the count for each patch using the pre-trained
    SASNet mentioned above, then aggregate the results by summing up the predicted
    count for each patch.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出这些问题，我们在上海科技B部分数据集 [[20](#bib.bib20)] 上测试了两种基线方法（UD和CIP），该数据集包含1024$\times$768像素的图像。我们将原始图像大小缩小了4倍和16倍，并测量了两个基线方法的平均绝对误差（MAE）。为了测试UD，我们使用了一个在上海科技B部分数据集
    [[20](#bib.bib20)] 上预训练的SASNet模型 [[14](#bib.bib14)]，输入大小为1024$\times$768，并使用AdamW优化器
    [[26](#bib.bib26)] 以$10^{-5}$的学习率和$10^{-4}$的权重衰减进行微调。注意，原始SASNet论文使用的是Adam优化器
    [[27](#bib.bib27)] 和$10^{-5}$的学习率。我们训练模型100个周期，每个GPU实例的批量大小为12，使用3$\times$Nvidia
    A6000 GPU。我们经验上发现，微调并未改善切割成补丁的准确性，因此，我们将原始图像切割成4个和16个补丁，使用上述预训练的SASNet获得每个补丁的计数，然后通过对每个补丁的预测计数求和来汇总结果。
- en: 'The results of these experiments are shown in Table [I](#S1.T1 "TABLE I ‣ I
    Introduction ‣ Efficient High-Resolution Deep Learning: A Survey"). It can be
    observed that uniform downsampling significantly increases the error compared
    to processing the original input size. Keep in mind that even though the increase
    in error is not as drastic with cutting into patches, the inference time of this
    approach is increased by the same factor (i.e., 4 and 16) since we assumed we
    are using the effective maximum resolution possible for our hardware, and thus
    patches cannot be processed in parallel as the entire hardware is required to
    process a single patch.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '这些实验的结果展示在表格 [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Efficient High-Resolution
    Deep Learning: A Survey") 中。可以观察到，与处理原始输入大小相比，均匀下采样显著增加了误差。请注意，尽管切割成补丁的误差增加不如均匀下采样那么剧烈，但这种方法的推断时间也以相同的因子（即4和16）增加，因为我们假设使用了我们硬件的有效最大分辨率，因此补丁不能并行处理，因为整个硬件都需要处理单个补丁。'
- en: 'TABLE I: Performance of baseline approaches on the Shanghai Tech Part B dataset.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：上海科技B部分数据集基线方法的性能
- en: '| Input Size | Original MAE | UD^∗ MAE | CIP^† MAE |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 输入大小 | 原始MAE | UD^∗ MAE | CIP^† MAE |'
- en: '| 1024$\times$768 (original) | 6.31 | - | - |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 1024$\times$768（原始） | 6.31 | - | - |'
- en: '| 512$\times$384 (reduced 4$\times$) | - | 9.01 (+43%) | 6.40 (+1%) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 512$\times$384（缩小4$\times$） | - | 9.01（+43%） | 6.40（+1%） |'
- en: '| 256$\times$192 (reduced 16$\times$) | - | 16.06 (+155%) | 6.67 (+6%) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 256$\times$192（缩小16$\times$） | - | 16.06（+155%） | 6.67（+6%） |'
- en: '| ^∗Uniform Downsampling |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| ^∗均匀下采样 |'
- en: '| ^†Cutting into Patches |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ^†切割成补丁 |'
- en: Since these baseline approaches are far from ideal, in recent years, several
    alternative methods have been proposed in the literature in order to improve accuracy
    and speed while complying with the maximum resolution limitation caused either
    by memory limitations or speed requirements. The goal of this survey is to summarize
    and categorize these contributions. To the best of our knowledge, no other survey
    on the topic of high-resolution deep learning exists. However, there are some
    surveys that include aspects relevant to this topic. A survey on methods for reducing
    the computational complexity of Transformer architectures is provided in [[15](#bib.bib15)],
    which discusses the issues related to the quadratic time and memory complexity
    of self-attention and analyzes various aspects of efficiency including memory
    footprint and computational cost. While reducing the computational complexity
    of Transformer models can contribute to efficient processing of high-resolution
    inputs, in this survey, we only include Vision Transformer methods that explicitly
    focus on high-resolution images. Some application-specific surveys include high-resolution
    datasets and methods that operate on such data. For instance, a survey on deep
    learning for histopathology, which mentions challenges with processing the giga-resolution
    of WSIs, is provided in [[28](#bib.bib28)]; a survey of methods that achieve greater
    spatial resolution in computed tomography (CT) is provided in [[29](#bib.bib29)],
    which highlights improved diagnostic accuracy with ultra high-resolution CT, and
    briefly discusses deep learning methods for noise reduction and reconstruction;
    a survey on crowd counting where many of the available datasets are high-resolution
    is provided in [[7](#bib.bib7)]; a survey on deep learning methods for land cover
    classification and object detection in high-resolution remote sensing imagery
    is provided in [[30](#bib.bib30)]; and a survey on deep learning-based change
    detection in high-resolution remote sensing images is provided in [[31](#bib.bib31)].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些基线方法远非理想，近年来，文献中提出了几种替代方法，以提高准确性和速度，同时遵守由于内存限制或速度要求导致的最大分辨率限制。本调查的目标是总结和分类这些贡献。据我们所知，关于高分辨率深度学习的其他调查尚不存在。然而，有一些调查涵盖了与此主题相关的方面。一项关于降低
    Transformer 架构计算复杂度的方法的调查见于 [[15](#bib.bib15)]，该调查讨论了自注意力的二次时间和内存复杂度相关的问题，并分析了包括内存占用和计算成本在内的各种效率方面。虽然降低
    Transformer 模型的计算复杂度可以有助于高分辨率输入的高效处理，但在本调查中，我们仅包括那些明确关注高分辨率图像的 Vision Transformer
    方法。一些特定应用的调查包括高分辨率数据集和处理这些数据的方法。例如，一项关于深度学习在组织病理学中的应用的调查，提到了处理 WSIs 的 giga 分辨率的挑战，见于
    [[28](#bib.bib28)]；一项关于在计算机断层扫描（CT）中实现更高空间分辨率的方法的调查见于 [[29](#bib.bib29)]，该调查强调了超高分辨率
    CT 提高了诊断准确性，并简要讨论了噪声减少和重建的深度学习方法；一项关于人群计数的调查，其中许多现有数据集为高分辨率的调查见于 [[7](#bib.bib7)]；一项关于深度学习方法在高分辨率遥感影像中的土地覆盖分类和物体检测的调查见于
    [[30](#bib.bib30)]；以及一项关于基于深度学习的高分辨率遥感影像变化检测的调查见于 [[31](#bib.bib31)]。
- en: 'It is important to mention that some methods operate on high-resolution inputs,
    yet do not make any effort to address the aforementioned challenges. For instance,
    multi-column (also known as multi-scale) networks [[7](#bib.bib7)] incorporate
    multiple columns of layers in their architecture, where each column is responsible
    for processing a specific scale as shown in Figure [4](#S1.F4 "Figure 4 ‣ I Introduction
    ‣ Efficient High-Resolution Deep Learning: A Survey"). However, since the columns
    process the same resolution as the original input, most of these methods in fact
    require even more memory and computation compared to the case where only the original
    scale is processed. The primary goal of these methods is instead to increase the
    accuracy by taking into account the scale variances that occur in high-resolution
    images, although there are some multi-scale methods that improve both accuracy
    and efficiency [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]. Therefore,
    these methods do not fall within the scope of this survey, unless they explicitly
    address the efficiency aspect for high-resolution inputs. ZoomCount [[35](#bib.bib35)],
    Locality-Aware Crowd Counting [[36](#bib.bib36)], RAZ-Net [[37](#bib.bib37)] and
    Learn to Scale [[38](#bib.bib38)] are all examples of multi-scale methods in crowd
    counting, and DMMN [[39](#bib.bib39)] and KGZNet [[40](#bib.bib40)] in medical
    image processing.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要提到，一些方法虽然处理高分辨率输入，但并没有采取任何措施来应对上述挑战。例如，多列（也称为多尺度）网络[[7](#bib.bib7)]在其架构中包含多个列层，每列负责处理特定的尺度，如图[4](#S1.F4
    "图 4 ‣ I 引言 ‣ 高分辨率深度学习：综述")所示。然而，由于这些列处理的分辨率与原始输入相同，这些方法实际上需要比仅处理原始尺度更多的内存和计算。
    这些方法的主要目标是通过考虑高分辨率图像中的尺度变异来提高准确性，尽管有些多尺度方法在提高准确性和效率方面都有所改进[[32](#bib.bib32)、[33](#bib.bib33)、[34](#bib.bib34)]。因此，这些方法不在本调查的范围内，除非它们明确解决了高分辨率输入的效率问题。ZoomCount
    [[35](#bib.bib35)]、Locality-Aware Crowd Counting [[36](#bib.bib36)]、RAZ-Net [[37](#bib.bib37)]
    和 Learn to Scale [[38](#bib.bib38)] 都是人群计数中的多尺度方法的例子，DMMN [[39](#bib.bib39)] 和
    KGZNet [[40](#bib.bib40)] 则是在医学图像处理中。
- en: '![Refer to caption](img/31d73833756304ebf5866438414ca17e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/31d73833756304ebf5866438414ca17e.png)'
- en: 'Figure 4: Schematic illustration of a multi-column architecture. If the original
    input to the DNN is a patch taken from a larger image, such as in [[36](#bib.bib36)],
    in addition to zooming in, it is also possible to zoom out.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：多列架构的示意图。如果DNN的原始输入是从更大图像中提取的一个补丁，如在[[36](#bib.bib36)]中所示，除了放大，还可以缩小。
- en: 'The primary purpose of this survey is to collect and describe methods that
    exist in deep learning literature, which can be used in situations where the high
    resolution of input images and videos create the aforementioned technical challenges
    regarding memory, computation and time. The rest of this paper is organized as
    follows: Section [II](#S2 "II Applications of High-Resolution Deep Learning ‣
    Efficient High-Resolution Deep Learning: A Survey") lists applications where high-resolution
    images and videos are processed using deep learning. Section [III](#S3 "III Methods
    for Efficient Processing of High-Resolution Inputs with Deep Learning ‣ Efficient
    High-Resolution Deep Learning: A Survey") categorizes efficient methods for high-resolution
    deep learning into five general categories and provides several examples for each
    category. This section also briefly discusses alternative approaches for solving
    the memory and processing time issues caused by high-resolution inputs. Section
    [IV](#S4 "IV High-Resolution Datasets ‣ Efficient High-Resolution Deep Learning:
    A Survey") lists existing high-resolution datasets for various deep learning problems
    and provides details for each of them. Section [V](#S5 "V Discussion and Open
    Issues ‣ Efficient High-Resolution Deep Learning: A Survey") discusses the advantages
    and disadvantages of using efficient high-resolution methods belonging to different
    categories and provides recommendations about which method to use in different
    situations. Finally, Section [VI](#S6 "VI Conclusion and Outlook ‣ Efficient High-Resolution
    Deep Learning: A Survey") concludes the paper by summarizing the current state
    and trends in high-resolution deep learning as well as suggestions for future
    research. The code for experiments conducted in this survey is available at [https://gitlab.au.dk/maleci/high-resolution-deep-learning](https://gitlab.au.dk/maleci/high-resolution-deep-learning).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查的主要目的是收集和描述深度学习文献中存在的方法，这些方法可用于解决输入图像和视频的高分辨率带来的技术挑战，涉及内存、计算和时间问题。本文其余部分组织如下：第[II](#S2
    "II Applications of High-Resolution Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey")节列出了使用深度学习处理高分辨率图像和视频的应用。第[III](#S3 "III Methods for
    Efficient Processing of High-Resolution Inputs with Deep Learning ‣ Efficient
    High-Resolution Deep Learning: A Survey")节将高分辨率深度学习的高效方法分为五个通用类别，并为每个类别提供了几个示例。该节还简要讨论了解决高分辨率输入导致的内存和处理时间问题的替代方法。第[IV](#S4
    "IV High-Resolution Datasets ‣ Efficient High-Resolution Deep Learning: A Survey")节列出了各种深度学习问题的现有高分辨率数据集，并提供了每个数据集的详细信息。第[V](#S5
    "V Discussion and Open Issues ‣ Efficient High-Resolution Deep Learning: A Survey")节讨论了使用不同类别的高效高分辨率方法的优缺点，并提供了不同情况下使用哪些方法的建议。最后，第[VI](#S6
    "VI Conclusion and Outlook ‣ Efficient High-Resolution Deep Learning: A Survey")节通过总结高分辨率深度学习的现状和趋势以及对未来研究的建议来结束本文。本文调查中进行实验的代码可以在[https://gitlab.au.dk/maleci/high-resolution-deep-learning](https://gitlab.au.dk/maleci/high-resolution-deep-learning)找到。'
- en: II Applications of High-Resolution Deep Learning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 高分辨率深度学习的应用
- en: In this section, we list some real-world applications where high-resolution
    images are processed with deep learning. Most of these methods do not focus on
    the efficiency angle, however, some of the methods address issues encountered
    with high-resolution images. For instance, [[41](#bib.bib41)] mentions that “it
    was not possible to train the model with the original 6000$\times$4000 pixels
    images because of GPU memory limitation” and [[42](#bib.bib42)], which uses the
    cutting into patches approach, states that “a raw remote image has millions of
    pixels and is difficult to process directly”.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们列出了一些使用深度学习处理高分辨率图像的实际应用。这些方法大多数并不专注于效率角度，但其中一些方法解决了高分辨率图像遇到的问题。例如，[[41](#bib.bib41)]提到“由于GPU内存限制，无法用原始6000$\times$4000像素的图像训练模型”，而[[42](#bib.bib42)]，使用切割成小块的方法，指出“原始遥感图像有数百万个像素，直接处理困难”。
- en: II-A Medical and Biomedical Image Analysis
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 医学和生物医学图像分析
- en: Multi-gigapixel whole-slide pathology images can be processed with deep learning
    in order to detect breast cancer [[43](#bib.bib43)], skin cancer [[44](#bib.bib44),
    [45](#bib.bib45)], prostate cancer [[45](#bib.bib45)], lung cancer [[45](#bib.bib45)],
    cervical cancer [[46](#bib.bib46)] and cancer in the digestive tract [[47](#bib.bib47)].
    Some methods are even able to detect the cancer subtypes [[45](#bib.bib45)] or
    detect the spread of cancer to lymph nodes (metastasis) [[48](#bib.bib48)]. Semantic
    segmentation of such images can be useful in neuropathology [[49](#bib.bib49)],
    which is the study of diseases of the nervous system, and identifying tissue components
    such as tumor, muscle, and debris in medical images [[50](#bib.bib50)].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 多吉兆像素的全切片病理图像可以通过深度学习处理，以检测乳腺癌 [[43](#bib.bib43)]、皮肤癌 [[44](#bib.bib44), [45](#bib.bib45)]、前列腺癌
    [[45](#bib.bib45)]、肺癌 [[45](#bib.bib45)]、宫颈癌 [[46](#bib.bib46)] 和消化道癌症 [[47](#bib.bib47)]。一些方法甚至能够检测癌症亚型
    [[45](#bib.bib45)] 或检测癌症是否扩散到淋巴结（转移） [[48](#bib.bib48)]。这种图像的语义分割在神经病理学 [[49](#bib.bib49)]
    中可能很有用，神经病理学是研究神经系统疾病的学科，并且可以在医学图像中识别组织成分，如肿瘤、肌肉和碎屑 [[50](#bib.bib50)]。
- en: Moreover, the processing of high-resolution computed tomography (CT) scans with
    deep learning is becoming more prevalent. The studies in [[51](#bib.bib51)] and
    [[5](#bib.bib5)] detect COVID-19 in high-resolution CT scans of the lung, and
    the study in [[52](#bib.bib52)] uses deep learning to improve the quality of captured
    ultra-high-resolution CT scans. In addition, the study in [[53](#bib.bib53)] performs
    semantic segmentation on high-resolution electron microscopy images from hearts
    and brains of mice, which is useful for fundamental biomedical research. Additionally,
    high-resolution deep learning can be used for reconstruction of CT images and
    reduction of image noise, which has been shown to obtain results similar to other
    conventional methods with clinically feasible speed [[54](#bib.bib54), [55](#bib.bib55)].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，利用深度学习处理高分辨率计算机断层扫描（CT）图像正变得越来越普遍。[[51](#bib.bib51)] 和 [[5](#bib.bib5)] 的研究在高分辨率的肺部CT扫描中检测
    COVID-19，而 [[52](#bib.bib52)] 的研究使用深度学习提高了捕获的超高分辨率CT图像的质量。此外，[[53](#bib.bib53)]
    的研究对来自小鼠心脏和大脑的高分辨率电子显微镜图像进行语义分割，这对基础生物医学研究很有帮助。此外，高分辨率深度学习还可以用于CT图像重建和图像噪声减少，研究表明，这种方法在临床可行的速度下获得了类似于其他传统方法的结果
    [[54](#bib.bib54), [55](#bib.bib55)]。
- en: Even though medical image analysis methods primarily focus on improving the
    accuracy of particular tasks, inference speed can be crucial in some applications,
    for instance, speed might be a requirement in clinical practice [[48](#bib.bib48)].
    Furthermore, real-time augmented reality under microscopes can provide suitable
    human–computer interaction for AI-assisted slide screening [[46](#bib.bib46)].
    Finally, there might be situations where the speed for processing a single input
    is acceptable, however, the sheer number of input data is so high that inputs
    collectively cannot be processed within a deadline. For instance, 55,000 high-resolution
    images are taken during the examination of a single patient using wireless capsule
    endoscopy, where a tiny wireless camera is swallowed to take pictures of the digestive
    tract, which can be used to detect lesions and inflammation [[56](#bib.bib56)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管医学图像分析方法主要关注于提高特定任务的准确性，但在某些应用中，推断速度可能至关重要，例如，在临床实践中速度可能是一个要求 [[48](#bib.bib48)]。此外，显微镜下的实时增强现实可以为AI辅助的切片筛查提供合适的人机交互
    [[46](#bib.bib46)]。最后，可能会有这样的情况，即单个输入的处理速度是可以接受的，但输入数据的数量非常庞大，以至于所有输入无法在截止日期内处理。例如，在使用无线胶囊内窥镜检查单个患者时，会拍摄55,000张高分辨率图像，无线摄像头被吞下以拍摄消化道的图像，这些图像可以用于检测病变和炎症
    [[56](#bib.bib56)]。
- en: II-B Remote Sensing
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 遥感
- en: Processing high-resolution aerial and satellite imagery with deep learning has
    various applications [[57](#bib.bib57)], such as detecting buildings [[58](#bib.bib58)],
    which is useful for urban planning and monitoring; detecting airplanes [[59](#bib.bib59)],
    which can be used for defense and military applications as well as airport surveillance;
    extracting road networks [[42](#bib.bib42)], which has applications in automated
    road navigation with unmanned vehicles, urban planning and real-time updating
    of geospatial databases; detecting areas in a forest that are damaged due to natural
    disasters such as storms [[60](#bib.bib60)]; identifying weed plants, which can
    be used for targeted spraying of pesticides in agricultural fields; semantic segmentation
    of satellite data which can help with crop monitoring, natural resource management
    and digital mapping [[61](#bib.bib61)]; and remote sensing image captioning which
    is useful for applications such as image retrieval and military intelligence generation
    [[62](#bib.bib62)]. Moreover, significant accuracy improvements can be obtained
    by taking low-resolution weather data as input and interpolating high-resolution
    data using super-resolution [[63](#bib.bib63)]. The motivation behind this approach
    is that high-resolution data are only available with a few days delay, and this
    method can be used to more accurately process low-resolution but up-to-date data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习处理高分辨率航空和卫星影像具有多种应用[[57](#bib.bib57)]，例如检测建筑物[[58](#bib.bib58)]，这对于城市规划和监控非常有用；检测飞机[[59](#bib.bib59)]，可用于国防和军事应用以及机场监控；提取道路网络[[42](#bib.bib42)]，在无人驾驶车辆的自动道路导航、城市规划和地理空间数据库的实时更新中都有应用；检测森林中因自然灾害如风暴[[60](#bib.bib60)]造成的受损区域；识别杂草植物，这可以用于农业领域的针对性喷洒农药；对卫星数据进行语义分割，这有助于作物监测、自然资源管理和数字地图制作[[61](#bib.bib61)]；以及遥感影像描述，这对图像检索和军事情报生成等应用非常有用[[62](#bib.bib62)]。此外，通过将低分辨率天气数据作为输入，并使用超分辨率插值高分辨率数据，可以获得显著的准确性提升[[63](#bib.bib63)]。这种方法的动机在于高分辨率数据通常会有几天的延迟，而这种方法可以更准确地处理低分辨率但最新的数据。
- en: II-C Surveillance
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 监控
- en: 'Capturing and processing gigapixel images for surveillance is becoming increasingly
    widespread, and such images can be processed with deep learning for searching
    and identifying people [[64](#bib.bib64), [65](#bib.bib65)] as well as detecting
    pedestrians [[66](#bib.bib66), [67](#bib.bib67)] which can be used for human behavior
    analysis and intelligent video surveillance such as enforcing social distancing
    restrictions during a pandemic [[68](#bib.bib68), [69](#bib.bib69)]. It should
    be noted that capturing gigapixel images for surveillance has several advantages
    over capturing lower resolutions with multiple cameras at different locations
    of the scene. First, cameras in a multi-camera setup typically have some overlap
    in their fields of view to avoid blindspots. This may result in errors for many
    applications, such as crowd counting, due to duplicates, as shown in Figure [5](#S2.F5
    "Figure 5 ‣ II-C Surveillance ‣ II Applications of High-Resolution Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey"). Reducing this error is
    not an easy task, since it requires information about the geometry of the scene
    and the use of re-identification methods for identifying and deduplicating people
    in multiple views of the same scene. Secondly, tracking the trajectory of people,
    vehicles and other moving objects is difficult with multiple cameras, since it
    also requires identifying them in multiple views of the scene. Finally, in many
    deep learning applications such as crowd counting, incorporating global information
    from the entire scene such as illumination and perspective improves the accuracy
    of the task [[7](#bib.bib7)]. Note that images captured from drastically different
    locations and perspectives, such as the ones in in Figure [5](#S2.F5 "Figure 5
    ‣ II-C Surveillance ‣ II Applications of High-Resolution Deep Learning ‣ Efficient
    High-Resolution Deep Learning: A Survey"), cannot be stitched together to form
    a single image.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获和处理千亿像素图像用于监控变得越来越普遍，这些图像可以通过深度学习进行人员搜索和识别[[64](#bib.bib64), [65](#bib.bib65)]，以及行人检测[[66](#bib.bib66),
    [67](#bib.bib67)]，这可以用于人类行为分析和智能视频监控，如在疫情期间实施社交距离限制[[68](#bib.bib68), [69](#bib.bib69)]。需要注意的是，捕获千亿像素图像相比于在场景不同位置使用多个相机捕获低分辨率图像有几个优点。首先，多相机设置中的相机通常有一定的视野重叠，以避免盲点。这可能会导致许多应用中的错误，例如人群计数，由于重复，如图[5](#S2.F5
    "图 5 ‣ II-C 监控 ‣ II 高分辨率深度学习应用 ‣ 高效高分辨率深度学习：综述")所示。减少这种错误不是一件容易的事，因为它需要场景几何信息以及用于识别和去重的重新识别方法。其次，追踪人员、车辆和其他移动物体的轨迹在多个相机下很困难，因为这也需要在多个视图中识别它们。最后，在许多深度学习应用中，如人群计数，结合整个场景的全局信息（如光照和透视）可以提高任务的准确性[[7](#bib.bib7)]。需要注意的是，从
    drastically 不同的位置和角度拍摄的图像，例如图[5](#S2.F5 "图 5 ‣ II-C 监控 ‣ II 高分辨率深度学习应用 ‣ 高效高分辨率深度学习：综述")中的图像，不能拼接在一起形成单一图像。
- en: '![Refer to caption](img/518aad973ff5519608112a43471a0a9c.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/518aad973ff5519608112a43471a0a9c.png)'
- en: 'Figure 5: Overlap in the field of view for multi-camera setups, which can result
    in duplicates in tasks such as crowd counting.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：多相机设置中视野重叠可能导致任务如人群计数中的重复。
- en: II-D Other Applications
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 其他应用
- en: High-resolution deep learning can be beneficial in many other applications and
    various domains of science. For instance, the study in [[41](#bib.bib41)] estimates
    the density of wheat ears, which are the grain-bearing parts of the plant, from
    high-resolution images taken from grain fields, which aids plant breeders in optimizing
    their yield; and the study in [[70](#bib.bib70)] introduces a deep learning method
    for segmentation of high-resolution electron microscopy images, which has applications
    in material science such as understanding the degradation process of industrial
    catalysts. [[71](#bib.bib71)] proposes a method for real-time high-resolution
    background replacement, which is useful in video calls and conferencing.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率的深度学习在许多其他应用和科学领域中具有重要作用。例如，研究[[41](#bib.bib41)]通过从谷物田地中拍摄的高分辨率图像来估算小麦穗的密度，这有助于植物育种者优化产量；而研究[[70](#bib.bib70)]介绍了一种用于高分辨率电子显微镜图像分割的深度学习方法，这在材料科学中有应用，例如理解工业催化剂的降解过程。[[71](#bib.bib71)]
    提出了一种用于实时高分辨率背景替换的方法，这在视频通话和会议中非常有用。
- en: III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 高分辨率输入的深度学习高效处理方法
- en: III-A Non-Uniform Downsampling
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 非均匀下采样
- en: 'Non-uniform downsampling (NUD) is based on the idea that for any deep learning
    task, some locations of an input image are more important than others. For instance,
    in gaze estimation, where the goal is to detect where a person is looking given
    an image including the person’s face, the image locations depicting the person’s
    eyes are much more important than other parts of the image. Therefore, when reducing
    the resolution of the image, it might be beneficial to sample more pixels from
    salient areas and less pixels from non-salient locations, resulting in a warped
    and distorted image. This operation requires salient areas to be determined before
    introducing the downsampled image to the task DNN. Therefore, a small saliency
    detection network is utilized in order to obtain this saliency map. Figure [6](#S3.F6
    "Figure 6 ‣ III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing
    of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution Deep
    Learning: A Survey") provides a schematic illustration of the non-uniform downsampling
    approach. Note that non-uniform downsampling is a broad process that encompasses
    any method that downsamples the input image in any manner other than uniform.
    [[23](#bib.bib23)] further subdivides non-uniform downsampling into three categories:
    attention mechanisms, saliency-based methods and adaptive image sampling methods.
    However, as the authors point out, there is a lot of overlap between these categories
    and it is difficult to draw a clear border between them.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 非均匀下采样（NUD）的基础思想是，对于任何深度学习任务，输入图像的某些位置比其他位置更重要。例如，在凝视估计中，目标是检测给定包含人脸的图像中的人正在注视的地方，描绘人眼的图像位置比图像的其他部分重要得多。因此，当降低图像的分辨率时，可能会有利于从显著区域采样更多的像素，从非显著位置采样更少的像素，从而得到一个扭曲和变形的图像。这个操作要求在将下采样图像引入任务
    DNN 之前确定显著区域。因此，使用一个小型显著性检测网络来获取这个显著性图。图 [6](#S3.F6 "图 6 ‣ III-A 非均匀下采样 ‣ III
    高分辨率输入深度学习的高效处理方法 ‣ 高效高分辨率深度学习：综述") 提供了非均匀下采样方法的示意图。请注意，非均匀下采样是一个广泛的过程，包含了以任何非均匀方式下采样输入图像的任何方法。[[23](#bib.bib23)]
    进一步将非均匀下采样细分为三类：注意力机制、基于显著性的方法和自适应图像采样方法。然而，正如作者指出的，这些类别之间有很大的重叠，很难划定明确的边界。
- en: '![Refer to caption](img/900fb6373b2602680322b3263ca19a75.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/900fb6373b2602680322b3263ca19a75.png)'
- en: 'Figure 6: Schematic illustration of the non-uniform downsampling approach.
    The saliency detector detects the cat’s right eye as a salient area, therefore,
    the non-uniform resampler samples more pixels from that area.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 非均匀下采样方法的示意图。显著性检测器检测到猫的右眼作为显著区域，因此，非均匀重采样器从该区域采样更多的像素。'
- en: Formally, the saliency map $S$ can be obtained by applying saliency detection
    network $f_{s}(\cdot)$ on a uniformly downsampled image $I_{l}$, that is, $S=f_{s}(I_{l})$.
    The input to the saliency detection network is downsampled in order to keep the
    overhead of the saliency detection process low. The non-uniformly downsampled
    image $J$ can the be obtained based on $J=g(I,S)$, where $g(\cdot)$ is the non-uniform
    resampler and $I$ is the original image. Essentially, the resampler should compute
    a mapping $J(x,y)=I(u_{c}(x,y),v_{c}(x,y))$ from the original image to the downsampled
    one. Functions $u_{c}(\cdot)$ and $v_{c}(\cdot)$ need to map pixels proportionally
    to the weight assigned to them in the saliency map. Assuming the saliency map
    is normalized and $\forall x,y:0\leq u_{c}(x,y)\leq 1$ and $\forall x,y:0\leq
    v_{c}(x,y)\leq 1$, this problem can be written as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，显著性图 $S$ 可以通过在均匀下采样图像 $I_{l}$ 上应用显著性检测网络 $f_{s}(\cdot)$ 来获得，即 $S=f_{s}(I_{l})$。输入到显著性检测网络的图像经过下采样，以保持显著性检测过程的开销较低。非均匀下采样图像
    $J$ 可以根据 $J=g(I,S)$ 获得，其中 $g(\cdot)$ 是非均匀重采样器，$I$ 是原始图像。实际上，重采样器应计算一个从原始图像到下采样图像的映射
    $J(x,y)=I(u_{c}(x,y),v_{c}(x,y))$。函数 $u_{c}(\cdot)$ 和 $v_{c}(\cdot)$ 需要将像素按分配给它们的显著性图中的权重比例映射。假设显著性图已归一化且
    $\forall x,y:0\leq u_{c}(x,y)\leq 1$ 和 $\forall x,y:0\leq v_{c}(x,y)\leq 1$，这个问题可以写成
- en: '|  | $\int_{0}^{u_{c}(x,y)}\int_{0}^{v_{c}(x,y)}S(x^{\prime},y^{\prime})dx^{\prime}dy^{\prime}=xy.$
    |  | (1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\int_{0}^{u_{c}(x,y)}\int_{0}^{v_{c}(x,y)}S(x^{\prime},y^{\prime})dx^{\prime}dy^{\prime}=xy.$
    |  | (1) |'
- en: 'However, methods for determining this transformation based on Eq. [1](#S3.E1
    "In III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing of High-Resolution
    Inputs with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey")
    are not efficient [[23](#bib.bib23)]. An alternative approach is to presume each
    pixel $(x^{\prime},y^{\prime})$ is pulling all other pixels with a force proportional
    to its saliency $S(x^{\prime},y^{\prime})$, which can be formulated as'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，基于公式[1](#S3.E1 "In III-A Non-Uniform Downsampling ‣ III Methods for Efficient
    Processing of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey")确定这种变换的方法并不高效[[23](#bib.bib23)]。另一种方法是假设每个像素$(x^{\prime},y^{\prime})$以与其显著性$S(x^{\prime},y^{\prime})$成正比的力拉动所有其他像素，这可以被公式化为'
- en: '|  | $\displaystyle u_{c}(x,y)=\frac{\sum_{x^{\prime},y^{\prime}}S(x^{\prime},y^{\prime})k((x,y),(x^{\prime},y^{\prime}))x^{\prime}}{\sum_{x^{\prime},y^{\prime}}S(x^{\prime},y^{\prime})k((x,y),(x^{\prime},y^{\prime}))},$
    |  | (2) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u_{c}(x,y)=\frac{\sum_{x^{\prime},y^{\prime}}S(x^{\prime},y^{\prime})k((x,y),(x^{\prime},y^{\prime}))x^{\prime}}{\sum_{x^{\prime},y^{\prime}}S(x^{\prime},y^{\prime})k((x,y),(x^{\prime},y^{\prime}))},$
    |  | (2) |'
- en: '|  | $\displaystyle v_{c}(x,y)=\frac{\sum_{x^{\prime},y^{\prime}}S(x^{\prime},y^{\prime})k((x,y),(x^{\prime},y^{\prime}))y^{\prime}}{\sum_{x^{\prime},y^{\prime}}S(x^{\prime},y^{\prime})k((x,y),(x^{\prime},y^{\prime}))},$
    |  | (3) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v_{c}(x,y)=\frac{\sum_{x^{\prime},y^{\prime}}S(x^{\prime},y^{\prime})k((x,y),(x^{\prime},y^{\prime}))y^{\prime}}{\sum_{x^{\prime},y^{\prime}}S(x^{\prime},y^{\prime})k((x,y),(x^{\prime},y^{\prime}))},$
    |  | (3) |'
- en: where $k((x,y),(x^{\prime},y^{\prime}))$ is a distance kernel, for instance,
    the Gaussian kernel. Using this formulation, salient areas will be sampled more
    since they attract more pixels. Moreover, based on this formulation, $u_{c}(\cdot)$
    and $v_{c}(\cdot)$ can be computed with simple convolutions. Therefore, this operation
    can be easily plugged into neural network architectures as a layer, and has the
    added benefit of preserving the differentiability which is a requirement for training
    neural networks with the backpropagation algorithm. The overall result is that
    the entire module including the saliency detection network and the task network
    can be trained end-to-end. The method in [[23](#bib.bib23)] uses this approach
    to improve the performance of gaze estimation as well as fine-grained classification,
    which is the task of differentiating between hard-to-distinguish objects such
    as different species of animals.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$k((x,y),(x^{\prime},y^{\prime}))$是距离核，例如高斯核。使用这个公式，显著区域将被更多地采样，因为它们吸引更多的像素。此外，基于这个公式，$u_{c}(\cdot)$和$v_{c}(\cdot)$可以通过简单的卷积计算。因此，这个操作可以轻松地插入到神经网络架构中作为一个层，并具有保持可微性的附加好处，这是使用反向传播算法训练神经网络的要求。最终的结果是，包括显著性检测网络和任务网络的整个模块可以端到端地训练。[[23](#bib.bib23)]中的方法使用这种方法来提高注视估计以及细粒度分类的性能，细粒度分类的任务是区分难以区分的对象，例如不同的动物物种。
- en: The method in [[72](#bib.bib72)] applies the idea of non-uniform downsampling
    to semantic segmentation. If the input image $I=I_{ij}$ has a size $H\times W$
    and must be downsampled to size $h\times w$, the first step is to generate ideal
    sampling tensors from ground truth (GT) labels based on
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[[72](#bib.bib72)]中的方法将非均匀下采样的思想应用于语义分割。如果输入图像$I=I_{ij}$的大小为$H\times W$，并且必须下采样到大小为$h\times
    w$，第一步是根据地面真实标签（GT）生成理想的采样张量，基于'
- en: '|  | $E(\phi)=\sum_{i,j}\&#124;\phi_{ij}-b(u_{ij})\&#124;^{2}+\lambda\sum_{&#124;i-i^{\prime}&#124;+&#124;j-j^{\prime}&#124;=1}\&#124;\phi_{ij}-\phi_{i^{\prime}j^{\prime}}\&#124;^{2},$
    |  | (4) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(\phi)=\sum_{i,j}\&#124;\phi_{ij}-b(u_{ij})\&#124;^{2}+\lambda\sum_{&#124;i-i^{\prime}&#124;+&#124;j-j^{\prime}&#124;=1}\&#124;\phi_{ij}-\phi_{i^{\prime}j^{\prime}}\&#124;^{2},$
    |  | (4) |'
- en: 'where $\phi\in[0,1]^{h\times w\times 2}$ is the sampling tensor to be determined,
    $E(\phi)$ is the (energy) cost function to minimize, $u\in[0,1]^{h\times w\times
    2}$ is the uniform downsampling tensor and $b(u_{ij})$ is the coordinates of the
    closest point to pixel $u_{ij}$ on semantic boundaries in the GT labels. Eq. [4](#S3.E4
    "In III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing of High-Resolution
    Inputs with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey")
    corresponds to a least squares problem with convex constraints that can be efficiently
    solved using a set of sparse linear equations. The first term in Eq. [4](#S3.E4
    "In III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing of High-Resolution
    Inputs with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey")
    ensures the sampling locations are close to the semantic boundaries, and the second
    term ensures that the distortion is not excessive by forcing the transformations
    of adjacent pixels to be similar. Eq. [4](#S3.E4 "In III-A Non-Uniform Downsampling
    ‣ III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey") is also subject to covering
    constraints that ensure the sampled locations cover the whole image. The contribution
    of the second term is controlled by a parameter $\lambda$ which is empirically
    set to 1\. The next step is to train a neural network to generate sampling tensors
    from input images. The images are then downsampled based on the output of this
    neural network and introduced to the task network. Finally, the segmentation output
    is upsampled to remove distortions and match the original resolution.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\phi\in[0,1]^{h\times w\times 2}$是待确定的采样张量，$E(\phi)$是需要最小化的（能量）成本函数，$u\in[0,1]^{h\times
    w\times 2}$是均匀下采样张量，$b(u_{ij})$是GT标签中像素$u_{ij}$上最近点的坐标。公式[4](#S3.E4 "在III-A 非均匀下采样
    ‣ III 高分辨率输入的深度学习高效处理方法 ‣ 高分辨率深度学习：综述")对应一个具有凸约束的最小二乘问题，可以通过一组稀疏线性方程高效解决。公式[4](#S3.E4
    "在III-A 非均匀下采样 ‣ III 高分辨率输入的深度学习高效处理方法 ‣ 高分辨率深度学习：综述")中的第一个项确保采样位置接近语义边界，第二个项则通过强制相邻像素的变换相似来确保失真不超过预期。公式[4](#S3.E4
    "在III-A 非均匀下采样 ‣ III 高分辨率输入的深度学习高效处理方法 ‣ 高分辨率深度学习：综述")还受到覆盖约束的影响，以确保采样位置覆盖整个图像。第二项的贡献由参数$\lambda$控制，经验上设置为1。接下来的步骤是训练一个神经网络，从输入图像生成采样张量。然后根据该神经网络的输出对图像进行下采样，并将其引入任务网络。最后，对分割输出进行上采样，以去除失真并匹配原始分辨率。
- en: Similarly, the method in [[73](#bib.bib73)] utilizes non-uniform downsampling
    for semantic segmentation. However, in contrast with the previous method, the
    saliency detector in this method is optimized based on the performance of semantic
    segmentation rather than external supervision signals. This method is similar
    to [[23](#bib.bib23)], however, applying a straightforward adaptation of [[23](#bib.bib23)]
    to semantic segmentation does not perform well. To improve the performance, an
    edge loss is added as a regularization term, which is calculated by using the
    mean squared error (MSE) between the deformation map $d$ obtained by the saliency
    detector and target deformation map $d_{t}$ calculated based on segmentation labels.
    To combat trivial solutions, the target deformation map has denser sampling around
    object boundaries and is formulated by $d_{t}=f_{\text{edge}}(f_{\text{gauss}}(Y_{lr}))$,
    where $Y_{lr}$ is the uniformly downsampled segmentation label, $f_{\text{edge}}$
    is an edge detection filter by convolution with a specific $3\times 3$ kernel,
    and $f_{\text{gauss}}$ is Gaussian blur with $\sigma=1$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，[[73](#bib.bib73)]中的方法利用非均匀下采样进行语义分割。然而，与前述方法不同的是，该方法中的显著性检测器是基于语义分割的性能而非外部监督信号进行优化的。该方法类似于[[23](#bib.bib23)]，但将[[23](#bib.bib23)]的简单适应应用于语义分割效果不佳。为了提高性能，添加了边缘损失作为正则化项，该损失通过计算显著性检测器获得的变形图$d$和基于分割标签计算的目标变形图$d_{t}$之间的均方误差（MSE）来获得。为了防止平凡解，目标变形图在物体边界周围具有更密集的采样，并由$d_{t}=f_{\text{edge}}(f_{\text{gauss}}(Y_{lr}))$构造，其中$Y_{lr}$是均匀下采样的分割标签，$f_{\text{edge}}$是通过特定的$3\times
    3$卷积核的边缘检测滤波器，$f_{\text{gauss}}$是$\sigma=1$的高斯模糊。
- en: 'Since the distortions caused by the customized grids defined in Eqs. [2](#S3.E2
    "In III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing of High-Resolution
    Inputs with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey")
    and [3](#S3.E3 "In III-A Non-Uniform Downsampling ‣ III Methods for Efficient
    Processing of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey") can be severe, the method in [[56](#bib.bib56)] introduces
    structured grids that can be combined with customized grids to obtain a more subtle
    spatial distortion effect for wireless capsule endoscopy (WCE) image classification.
    These structured grids ensure that pixels that were in the same row/column in
    the input image are also in the same row/column in the output image, and can be
    obtained by'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '由于自定义网格在公式 [2](#S3.E2 "In III-A Non-Uniform Downsampling ‣ III Methods for
    Efficient Processing of High-Resolution Inputs with Deep Learning ‣ Efficient
    High-Resolution Deep Learning: A Survey") 和 [3](#S3.E3 "In III-A Non-Uniform Downsampling
    ‣ III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey") 中定义的畸变可能很严重，方法 [[56](#bib.bib56)]
    引入了结构化网格，这些网格可以与自定义网格结合以获得更细腻的空间畸变效果，适用于无线胶囊内镜（WCE）图像分类。这些结构化网格确保输入图像中处于同一行/列的像素在输出图像中也处于同一行/列，并且可以通过以下方式获得'
- en: '|  | $\displaystyle u(x)=\frac{\sum_{x^{\prime}}S(x^{\prime})k(x,x^{\prime})x^{\prime}}{\sum_{x^{\prime}}S(x^{\prime})k(x,x^{\prime})},$
    |  | (5) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u(x)=\frac{\sum_{x^{\prime}}S(x^{\prime})k(x,x^{\prime})x^{\prime}}{\sum_{x^{\prime}}S(x^{\prime})k(x,x^{\prime})},$
    |  | (5) |'
- en: '|  | $\displaystyle v(y)=\frac{\sum_{y^{\prime}}S(y^{\prime})k(y,y^{\prime})x^{\prime}}{\sum_{y^{\prime}}S(y^{\prime})k(y,y^{\prime})},$
    |  | (6) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v(y)=\frac{\sum_{y^{\prime}}S(y^{\prime})k(y,y^{\prime})x^{\prime}}{\sum_{y^{\prime}}S(y^{\prime})k(y,y^{\prime})},$
    |  | (6) |'
- en: where $S(x)=\max_{y}S(x,y)$ and $S(y)=\max_{x}S(x,y)$. $u(x)$ and $v(y)$ are
    then copied and stacked to form $u_{s}(x,y)=u(x)$ and $v_{s}(x,y)=v(y)$. Finally,
    the combined deformation grids can be computed by
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$S(x)=\max_{y}S(x,y)$和$S(y)=\max_{x}S(x,y)$。然后将$u(x)$和$v(y)$复制并堆叠以形成$u_{s}(x,y)=u(x)$和$v_{s}(x,y)=v(y)$。最后，可以通过以下公式计算合成的变形网格
- en: '|  | $\displaystyle u(x,y)=\lambda u_{s}(x,y)+(1-\lambda)u_{c}(x,y),$ |  |
    (7) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u(x,y)=\lambda u_{s}(x,y)+(1-\lambda)u_{c}(x,y),$ |  |
    (7) |'
- en: '|  | $\displaystyle v(x,y)=\lambda v_{s}(x,y)+(1-\lambda)v_{c}(x,y),$ |  |
    (8) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v(x,y)=\lambda v_{s}(x,y)+(1-\lambda)v_{c}(x,y),$ |  |
    (8) |'
- en: where parameter $\lambda$ is empirically set to 0.5.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中参数$\lambda$根据经验设定为0.5。
- en: Similarly, FOVEA [[74](#bib.bib74)] discards custom grids and solely relies
    on structured grids for object detection in autonomous driving use cases. It also
    introduces anti-cropping regularization to combat cropping which may result in
    missing objects, by using reflect padding on the saliency map. In [[23](#bib.bib23)],
    the saliency detector is trained end-to-end along with the task network, however,
    as mentioned, finding saliency maps in object detection is more difficult. Therefore
    FOVEA uses intermediate supervision to train the saliency detection network.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，FOVEA [[74](#bib.bib74)] 摒弃了自定义网格，仅依赖于结构化网格来进行自动驾驶场景中的目标检测。它还引入了抗裁剪正则化，通过在显著图上使用反射填充来对抗可能导致遗漏目标的裁剪。在
    [[23](#bib.bib23)] 中，显著性检测器与任务网络端到端训练，但如前所述，目标检测中的显著图获取更为困难。因此，FOVEA使用中间监督来训练显著性检测网络。
- en: 'Even though the primary goal of the spatial transformer module in spatial transformer
    networks (STNs) [[75](#bib.bib75)] is to learn invariance to translation, scale,
    rotation and warping in order to improve performance, in the special case where
    the module is the first layer of the network, it can learn to crop the raw high-resolution
    input to a lower resolution and increase computational efficiency, thus it could
    be considered a form of NUD. Figure [7](#S3.F7 "Figure 7 ‣ III-A Non-Uniform Downsampling
    ‣ III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey") shows the architecture of
    the spatial transformer module, where the localization network determines the
    parameters $\theta$ for the transformation $\tau_{\theta}$ from input features
    $U$. $\tau_{\theta}(\cdot)$ can be a 2D affine transformation, a more constrained
    transformation such as'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管空间变换网络（STNs）[[75](#bib.bib75)]中空间变换模块的主要目标是学习对平移、缩放、旋转和形变的不变性以提高性能，但在模块作为网络的第一层的特殊情况下，它可以学习将原始高分辨率输入裁剪为较低分辨率，并提高计算效率，因此它可以被视为一种NUD形式。图[7](#S3.F7
    "Figure 7 ‣ III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing
    of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution Deep
    Learning: A Survey")展示了空间变换模块的架构，其中定位网络确定输入特征$U$的变换$\tau_{\theta}$的参数$\theta$。$\tau_{\theta}(\cdot)$可以是二维仿射变换，也可以是更受限制的变换，例如'
- en: '|  | $A_{\theta}=\begin{bmatrix}s&amp;0&amp;t_{x}\\ 0&amp;s&amp;t_{y}\end{bmatrix},$
    |  | (9) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{\theta}=\begin{bmatrix}s&amp;0&amp;t_{x}\\ 0&amp;s&amp;t_{y}\end{bmatrix},$
    |  | (9) |'
- en: which only allows cropping, translation and scaling, or a more general transformation
    such as plane projective transformation with 8 parameters, piecewise affine, thin
    plate spline [[76](#bib.bib76)], or any transformation as long as it is differentiable
    with respect to its parameters.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 仅允许裁剪、平移和缩放，或者更一般的变换，如具有8个参数的平面投影变换、分段仿射变换、薄板样条变换[[76](#bib.bib76)]，或者任何与其参数可微的变换。
- en: '![Refer to caption](img/8931022357f2a17c070874759b1ab9f3.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8931022357f2a17c070874759b1ab9f3.png)'
- en: 'Figure 7: Architecture of the spatial transformer module [[75](#bib.bib75)].'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '图7: 空间变换模块的架构 [[75](#bib.bib75)]。'
- en: 'SALISA [[77](#bib.bib77)] uses spatial transformer modules to perform non-uniform
    downsampling for object detection in high-resolution videos. In SALISA, the output
    of a video frame is used to determine the saliency map for the next frame. Figure
    [8](#S3.F8 "Figure 8 ‣ III-A Non-Uniform Downsampling ‣ III Methods for Efficient
    Processing of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey") shows this method, where the first frame is introduced
    to a high-performing detector without any downsampling. The detected objects are
    subsequently used to create a saliency map, which is then given to the resampling
    module. The resampling module contains a spatial transformer module with a thin
    plate spline transformation, where the localization network receives the saliency
    map as input. The downsampled image provided by the resampling module is then
    introduced to a lightweight detector. Since the lightweight detector detects objects
    in the warped image, the detected bounding boxes need to be transformed back into
    the original grid. Therefore an inverse transformation is applied before generating
    the saliency map. To prevent cascading errors, the method is reset to use the
    original high-resolution frame and high-performing detector every few frames.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'SALISA [[77](#bib.bib77)]使用空间变换模块在高分辨率视频中进行非均匀下采样以进行物体检测。在SALISA中，视频帧的输出用于确定下一帧的显著性图。图[8](#S3.F8
    "Figure 8 ‣ III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing
    of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution Deep
    Learning: A Survey")展示了这种方法，其中第一帧被引入高性能检测器而没有任何下采样。检测到的物体随后用于创建显著性图，然后传递给重采样模块。重采样模块包含一个具有薄板样条变换的空间变换模块，其中定位网络接收显著性图作为输入。重采样模块提供的下采样图像然后被引入轻量级检测器。由于轻量级检测器在变形图像中检测物体，因此检测到的边界框需要被转换回原始网格。因此，在生成显著性图之前应用了逆变换。为了防止级联错误，该方法每隔几帧重置为使用原始高分辨率帧和高性能检测器。'
- en: '![Refer to caption](img/fb07ebb97d7bd815370baae6248aee22.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fb07ebb97d7bd815370baae6248aee22.png)'
- en: 'Figure 8: Overview of SALISA [[77](#bib.bib77)]. The second frame is slightly
    different from the first frame (in this case, slightly rotated clockwise), therefore,
    the detection result obtained from the first frame can be used to estimate the
    saliency of objects in the second frame.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：SALISA概述[[77](#bib.bib77)]。第二帧与第一帧略有不同（在这种情况下，略微顺时针旋转），因此，第一帧获得的检测结果可以用来估计第二帧中物体的显著性。
- en: III-B Selective Zooming and Skipping
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 选择性缩放与跳过
- en: Selective zooming and skipping (SZS) methods take a more efficient approach
    to cutting into patches by only zooming into regions of the input image that are
    important. The zoom level may differ across different patches, and some patches
    may be entirely skipped. Reinforced Auto-Zoom Net (RAZN) [[78](#bib.bib78)] uses
    reinforcement learning to determine where to zoom in WSIs for the task of breast
    cancer segmentation. RAZN assumes the zoom-in action can be performed at most
    $m$ times and the zooming rate is a constant $r$. At each zoom level $i$, there
    is a different segmentation network $f_{\theta_{i}}$ and a different policy network
    $g_{\theta_{i}}$. Initially, policy network $g_{\theta_{0}}$ takes a cropped image
    $x_{0}\in\mathbb{R}^{H\times W\times 3}$ as input and determines whether to zoom-in
    or to break. If there is no need to zoom in, $x_{0}$ is given as input to segmentation
    network $f_{\theta_{0}}$ which produces the output, otherwise, a higher-resolution
    image $\hat{x}_{0}\in\mathbb{R}^{rH\times rW\times 3}$ is sampled from the same
    area and will be cut into $r^{2}$ patches of size $H\times W\times 3$. Each patch
    is then given to policy network $g_{\theta_{1}}$ and this process is recursively
    repeated until all policy networks break or the maximum zoom level is reached.
    RAZN achieves an improved performance over other state-of-the-art methods while
    reducing the inference time by a factor of $\sim$2\. Similarly, the methods in
    [[79](#bib.bib79)] and [[80](#bib.bib80)] use reinforcement learning for efficient
    object detection and aerial image classification, respectively.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性缩放和跳过（SZS）方法通过仅缩放输入图像中重要的区域，更有效地进行图像切分。不同的图像块可能有不同的缩放级别，有些图像块可能会被完全跳过。强化自动缩放网络（RAZN）[[78](#bib.bib78)]使用强化学习来确定在乳腺癌分割任务中应该在哪里缩放WSI。RAZN假设缩放动作最多可以执行$m$次，缩放速率为常数$r$。在每个缩放级别$i$，都有不同的分割网络$f_{\theta_{i}}$和不同的策略网络$g_{\theta_{i}}$。最初，策略网络$g_{\theta_{0}}$接收一个裁剪的图像$x_{0}\in\mathbb{R}^{H\times
    W\times 3}$作为输入，确定是否进行缩放。如果不需要缩放，则$x_{0}$作为输入传递给分割网络$f_{\theta_{0}}$以产生输出；否则，从相同区域中采样出更高分辨率的图像$\hat{x}_{0}\in\mathbb{R}^{rH\times
    rW\times 3}$，并将其切分为大小为$H\times W\times 3$的$r^{2}$个图像块。每个图像块然后传递给策略网络$g_{\theta_{1}}$，并且该过程递归重复，直到所有策略网络断开或达到最大缩放级别。RAZN在提高性能的同时减少了约$\sim$2倍的推理时间。类似地，[[79](#bib.bib79)]和[[80](#bib.bib80)]中的方法分别使用强化学习进行高效的目标检测和航空图像分类。
- en: Instead of reinforcement learning, the method in [[81](#bib.bib81)] uses a hierarchichal
    graph neural network to classify whether a mammogram (X-ray image of a breast)
    is normal/benign (contains a tumor that is not cancerous) or malignant (contains
    a tumor that is cancerous). At each zoom level $i$, the graph $G^{i}$ is defined
    by the adjacency matrix $A^{i}\in\mathbb{R}^{N_{i}\times N_{i}}$ where there is
    an edge between each zoomed-in patch and its original image. The feature matrix
    of the graph is defined as $X_{i}\in\mathbb{R}^{N_{i}\times D\times D}$, and the
    maximum zoom level is $R$. The features on the nodes are zoomed-in regions of
    the input image, resized to $D\times D$. A pre-trained CNN is used to extract
    feature vectors $H_{i}\in\mathbb{R}^{N_{i}\times H}$ from $X_{i}$. $\text{GAT}_{\text{node}}(\cdot)$
    is a graph attention network [[82](#bib.bib82)] used to classify whether to zoom
    in for each node. Therefore, the output of the $i$-th level in the hierarchical
    graph is
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与强化学习不同，[[81](#bib.bib81)]中的方法使用层次化图神经网络来分类乳腺X光图像是否正常/良性（包含非癌性肿瘤）或恶性（包含癌性肿瘤）。在每个缩放级别$i$，图$G^{i}$由邻接矩阵$A^{i}\in\mathbb{R}^{N_{i}\times
    N_{i}}$定义，其中每个放大区域与其原始图像之间存在一条边。图的特征矩阵定义为$X_{i}\in\mathbb{R}^{N_{i}\times D\times
    D}$，最大缩放级别为$R$。节点上的特征是输入图像的放大区域，调整为$D\times D$。使用预训练的CNN从$X_{i}$中提取特征向量$H_{i}\in\mathbb{R}^{N_{i}\times
    H}$。$\text{GAT}_{\text{node}}(\cdot)$是用于分类是否对每个节点进行放大的图注意力网络[[82](#bib.bib82)]。因此，层次图中第$i$级的输出是
- en: '|  | $P_{i}=\begin{cases}1,&amp;i=1,\\ \text{softmax}(\text{GAT}_{\text{node}}(A_{i},H_{i})),&amp;1<i<R,\end{cases}$
    |  | (10) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{i}=\begin{cases}1,&amp;i=1,\\ \text{softmax}(\text{GAT}_{\text{node}}(A_{i},H_{i})),&amp;1<i<R,\end{cases}$
    |  | (10) |'
- en: where $P_{i}\in\mathbb{R}^{N_{i}\times 2}$ represents the decision to zoom or
    not for each node of the $i$-th level. At the final zoom level $R$, another graph
    attention network $\text{GAT}_{\text{graph}}(\cdot)$ is used to perform the final
    classification for the entire mammogram based on $\hat{Y}=\text{softmax}(\text{GAT}_{\text{graph}}(A_{R},H_{R})W)$,
    where $W$ is a trainable weight matrix. The loss function contains both node losses
    and graph losses, with the zoom labels for nodes being obtained from lesion segmentation
    labels. This method achieves an accuracy comparable to the state-of-the-art, however,
    it is unclear how much it improves the inference speed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P_{i}\in\mathbb{R}^{N_{i}\times 2}$ 代表第 $i$ 层每个节点是否进行缩放的决策。在最终的缩放层 $R$，使用另一个图注意力网络
    $\text{GAT}_{\text{graph}}(\cdot)$ 对整个乳腺X光照片进行最终分类，基于 $\hat{Y}=\text{softmax}(\text{GAT}_{\text{graph}}(A_{R},H_{R})W)$，其中
    $W$ 是一个可训练的权重矩阵。损失函数包含节点损失和图损失，节点的缩放标签从病灶分割标签中获得。这种方法达到了与最先进技术相当的准确性，但尚不清楚它在推理速度上有多大提升。
- en: GigaDet [[83](#bib.bib83)] achieves near real-time object detection in gigapixel
    videos. At the core of GigaDet is the Patch Generation Network (PGN). PGN takes
    a uniformly downsampled image as input and outputs a dense regression map which
    counts the number of objects that are completely contained within the corresponding
    area in the image, referred to as the patch candidate. PGN is applied at different
    scales in order to obtain patch candidates of varying scales. The patch candidates
    selected by the PGN go through post-processing which includes non-maximum suppression
    (NMS), and are subsequently sorted based on their count. The top $K$ patch candidates
    are then selected to be processed by the Decorated Detector (DecDet) to detect
    objects. VGG [[84](#bib.bib84)] and YOLO [[85](#bib.bib85)] are used for the PGN
    and DecDet networks, respectively. Given gigapixel videos, GigaDet is capable
    of running 5 FPS on a single Nvidia 2080 Ti GPU, which is $50\times$ faster than
    Faster RCNN [[86](#bib.bib86)], yet obtains a comparable performance in terms
    of average precision.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: GigaDet [[83](#bib.bib83)] 实现了在千兆像素视频中近实时的物体检测。GigaDet 的核心是 Patch Generation
    Network (PGN)。PGN 以均匀下采样的图像为输入，输出一个密集回归图，该图计算图像中完全包含在相应区域内的物体数量，称为补丁候选。PGN 在不同的尺度下应用，以获得不同尺度的补丁候选。PGN
    选择的补丁候选经过后处理，包括非极大值抑制（NMS），然后根据其计数进行排序。前 $K$ 个补丁候选被选中，由 Decorated Detector (DecDet)
    处理以检测物体。VGG [[84](#bib.bib84)] 和 YOLO [[85](#bib.bib85)] 分别用于 PGN 和 DecDet 网络。给定千兆像素视频，GigaDet
    能够在单个 Nvidia 2080 Ti GPU 上运行 5 FPS，比 Faster RCNN [[86](#bib.bib86)] 快 $50\times$，且在平均精度方面性能相当。
- en: 'REMIX [[87](#bib.bib87)] detects pedestrians in high-resolution videos within
    a latency budget given by the user. The input frame is partitioned into several
    blocks, where more salient blocks are processed using a computationally expensive
    but accurate network whereas less salient blocks are processed using a computationally
    cheap network or even skipped, as shown in Figure [9](#S3.F9 "Figure 9 ‣ III-B
    Selective Zooming and Skipping ‣ III Methods for Efficient Processing of High-Resolution
    Inputs with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey").
    REMIX uses historical frames to determine the object distribution, and determines
    the optimal partition using a dynamic programming algorithm that takes into account
    the given latency budget, the estimated object distribution, as well as the accuracy
    and speed of available neural networks for object detection. REMIX achieves up
    to $8.1\times$ inference speedup with an accuracy comparable to the state-of-the-art
    methods.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'REMIX [[87](#bib.bib87)] 在用户给定的延迟预算内检测高分辨率视频中的行人。输入帧被分割成若干块，其中更显著的块使用计算量大但准确的网络进行处理，而较不显著的块则使用计算量小的网络处理甚至跳过，如图
    [9](#S3.F9 "Figure 9 ‣ III-B Selective Zooming and Skipping ‣ III Methods for
    Efficient Processing of High-Resolution Inputs with Deep Learning ‣ Efficient
    High-Resolution Deep Learning: A Survey") 所示。REMIX 使用历史帧来确定物体分布，并通过动态规划算法确定最佳分割，该算法考虑了给定的延迟预算、估计的物体分布以及可用神经网络的准确性和速度。REMIX
    实现了高达 $8.1\times$ 的推理加速，准确性与最先进的方法相当。'
- en: '![Refer to caption](img/76c9ffc47f5afd53e78e382902782446.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/76c9ffc47f5afd53e78e382902782446.png)'
- en: 'Figure 9: Partitioning in REMIX [[87](#bib.bib87)]. Some parts of the image
    are skipped, some processed by computationally cheap DNNs and some by computationally
    expensive DNNs.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：REMIX 中的分区 [[87](#bib.bib87)]。图像的某些部分被跳过，一些由计算开销较小的 DNN 处理，另一些由计算开销较大的 DNN
    处理。
- en: III-C Lightweight Scanner Networks
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 轻量级扫描网络
- en: Lightweight scanner networks (LSNs) are lightweight fully convolutional neural
    networks (FCNs) that efficiently scan the entire high-resolution input. To achieve
    a lightweight architecture, LSNs are typically designed and trained for very specific
    tasks. Moreover, as opposed to the cutting into patches approach, FCNs are inherently
    efficient in a sliding-window setting since they share the computation in overlapping
    regions [[88](#bib.bib88)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 轻量级扫描网络（LSNs）是轻量级的全卷积神经网络（FCNs），能够高效地扫描整个高分辨率输入。为了实现轻量级架构，LSNs 通常被设计和训练用于非常具体的任务。此外，与切割成补丁的方法不同，FCNs
    在滑动窗口设置中本质上高效，因为它们共享重叠区域中的计算 [[88](#bib.bib88)]。
- en: 'VGG-720p and VGG-1080p [[89](#bib.bib89), [90](#bib.bib90)] are LSNs capable
    of running in real-time on drones and provide heatmaps for input images of size
    1280$\times$720 and 1920$\times$1080 pixels, respectively, that specify whether
    or not there are people, faces, or bicycles at each location in the input image.
    Both models take patches of size 32$\times$32 or 64$\times$64 pixels as input.
    The architectures of VGG-720p and VGG-1080, shown in Tables [II](#S3.T2 "TABLE
    II ‣ III-C Lightweight Scanner Networks ‣ III Methods for Efficient Processing
    of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution Deep
    Learning: A Survey") and [III](#S3.T3 "TABLE III ‣ III-C Lightweight Scanner Networks
    ‣ III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey"), respectively, contain only
    5 convolutional layers with only 2 to 24 output channels. In contrast, the original
    VGG architectures have 11 to 19 layers with up to 512 output channels in some
    layers [[84](#bib.bib84)].'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 'VGG-720p 和 VGG-1080p [[89](#bib.bib89), [90](#bib.bib90)] 是能够在无人机上实时运行的轻量级扫描网络（LSNs），分别为输入图像尺寸为
    1280$\times$720 和 1920$\times$1080 像素的热图提供信息，这些热图指定了输入图像中的每个位置是否存在人、面孔或自行车。这两个模型都以
    32$\times$32 或 64$\times$64 像素的补丁作为输入。VGG-720p 和 VGG-1080 的架构分别如表 [II](#S3.T2
    "TABLE II ‣ III-C Lightweight Scanner Networks ‣ III Methods for Efficient Processing
    of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution Deep
    Learning: A Survey") 和 [III](#S3.T3 "TABLE III ‣ III-C Lightweight Scanner Networks
    ‣ III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey") 所示，仅包含 5 个卷积层，且输出通道仅为 2
    到 24 个。相比之下，原始 VGG 架构有 11 到 19 层，有些层的输出通道数高达 512 个 [[84](#bib.bib84)]。'
- en: 'TABLE II: Architecture of VGG-720p.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：VGG-720p 的架构。
- en: '| Layer | Kernel | Stride | Pad^† (X/Y)^∗ | Max Pool (X/Y) | Channels |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 核心 | 步幅 | 填充^†（X/Y）^∗ | 最大池化（X/Y） | 通道 |'
- en: '| conv1_1 | 3$\times$3 | 1/1 | 1/1 | - / - | 16 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| conv1_1 | 3$\times$3 | 1/1 | 1/1 | - / - | 16 |'
- en: '| conv1_2 | 3$\times$3 | 1/1 | 1/1 | ✓/ - | 16 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| conv1_2 | 3$\times$3 | 1/1 | 1/1 | ✓/ - | 16 |'
- en: '| conv2_1 | 3$\times$3 | 1/1 | 1/1 | - / - | 24 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| conv2_1 | 3$\times$3 | 1/1 | 1/1 | - / - | 24 |'
- en: '| conv2_2 | 3$\times$3 | 1/4 | 1/1 | ✓/ ✓ | 16 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| conv2_2 | 3$\times$3 | 1/4 | 1/1 | ✓/ ✓ | 16 |'
- en: '| conv_last | 8$\times$8 | 1/1 | 0/0 | - / - | 2 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| conv_last | 8$\times$8 | 1/1 | 0/0 | - / - | 2 |'
- en: '| ^†Zero padding |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ^†零填充 |'
- en: '| ^∗X and Y represent the horizontal and vertical axes |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ^∗X 和 Y 代表水平和垂直轴 |'
- en: 'TABLE III: Architecture of VGG-1080p.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：VGG-1080p 的架构。
- en: '| Layer | Kernel | Stride | Pad^† (X/Y)^∗ | Max Pool (X/Y) | Channels |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 核心 | 步幅 | 填充^†（X/Y）^∗ | 最大池化（X/Y） | 通道 |'
- en: '| conv1_1 | 3$\times$3 | 2/1 | 0/0 | - / - | 8 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| conv1_1 | 3$\times$3 | 2/1 | 0/0 | - / - | 8 |'
- en: '| conv1_2 | 3$\times$3 | 1/2 | 0/0 | ✓/ - | 8 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| conv1_2 | 3$\times$3 | 1/2 | 0/0 | ✓/ - | 8 |'
- en: '| conv2_1 | 3$\times$3 | 1/1 | 0/0 | - / - | 6 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| conv2_1 | 3$\times$3 | 1/1 | 0/0 | - / - | 6 |'
- en: '| conv2_2 | 3$\times$3 | 1/2 | 0/0 | - / - | 6 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| conv2_2 | 3$\times$3 | 1/2 | 0/0 | - / - | 6 |'
- en: '| conv_last | 8$\times$8 | 1/1 | 0/0 | - / - | 2 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| conv_last | 8$\times$8 | 1/1 | 0/0 | - / - | 2 |'
- en: '| ^†Zero padding |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ^†零填充 |'
- en: '| ^∗X and Y represent the horizontal and vertical axes |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ^∗X 和 Y 代表水平和垂直轴 |'
- en: Similarly, the study in [[91](#bib.bib91)] proposes an architecture with 6 convolutional
    layers for the same problem of generating a crowd heatmap from high-resolution
    images. The study in [[92](#bib.bib92)] proposes lightweight FCNs for face detection
    with 7 convolutional layers and 76K parameters, for facial parts detection (such
    as eyes, nose and mouth) with 4 convolutional layers and 20K parameters, and for
    combined face and parts detection with 9 convolutional layers and 101K parameters.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，[[91](#bib.bib91)]中的研究提出了一种具有6个卷积层的架构，用于从高分辨率图像生成拥挤热图。[[92](#bib.bib92)]中的研究提出了轻量级的FCN，用于面部检测，具有7个卷积层和76K参数，用于面部部位检测（如眼睛、鼻子和嘴巴）具有4个卷积层和20K参数，用于面部和部位的联合检测具有9个卷积层和101K参数。
- en: 'You only look twice (YOLT) [[9](#bib.bib9)] is a method that detects objects
    of different scales in DigitalGlobe satellite images which have a size of over
    250 megapixels. The architecture of YOLT is based on the YOLO architecture [[85](#bib.bib85)],
    however, it reduces the number of layers from the original 30 down to 22\. Furthermore,
    YOLT trains two separate models: one which processes images that correspond to
    areas of 200$\times$200m² for detecting relatively small objects such as cars,
    airplanes, boats and buildings; and another which processes images that correspond
    to areas of 2500$\times$2500m² for detecting large objects such as airports. YOLT
    has an inference speed of 32km²/min for the former model and 6000km²/min for the
    latter on an Nvidia Titan X GPU.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你只看两次（YOLT）[[9](#bib.bib9)]是一种在大小超过250兆像素的DigitalGlobe卫星图像中检测不同尺度物体的方法。YOLT的架构基于YOLO架构[[85](#bib.bib85)]，但将原始的30层减少到了22层。此外，YOLT训练了两个独立的模型：一个处理200$\times$200米²区域的图像，用于检测相对较小的物体，如汽车、飞机、船只和建筑物；另一个处理2500$\times$2500米²区域的图像，用于检测大型物体，如机场。在Nvidia
    Titan X GPU上，前者的推断速度为32平方公里/分钟，后者为6000平方公里/分钟。
- en: Fast ScanNet[[48](#bib.bib48)] converts VGG16 [[84](#bib.bib84)] to a fully
    convolutional network by replacing the last fully-connected layers in VGG16 with
    convolutional layers of kernel size 1$\times$1\. Fast ScanNet is applied to patches
    of size 2800$\times$2800 pixels, a size which is dictated by GPU memory limitations,
    taken from WSIs, which have $\sim$400 patches on average. It takes about one minute
    for Fast ScanNet to process a WSI on a workstation with 8$\times$Nvidia Titan
    X GPUs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Fast ScanNet[[48](#bib.bib48)]通过将VGG16 [[84](#bib.bib84)]的最后全连接层替换为1$\times$1卷积层，将其转换为全卷积网络。Fast
    ScanNet应用于大小为2800$\times$2800像素的补丁，这一大小由GPU内存限制决定，取自WSI，平均有$\sim$400个补丁。Fast ScanNet在配备8$\times$Nvidia
    Titan X GPU的工作站上处理一个WSI大约需要一分钟。
- en: 'ICNet [[34](#bib.bib34)] takes advantage of both the efficiency of processing
    lower resolutions and the accuracy of processing higher ones by uniformly downsampling
    the input image to two smaller scales, processing each scale separately, and fusing
    the result of processing lower resolutions with higher ones. Lower resolutions
    are processed with more convolution layers and higher resolutions with less, which
    makes the entire architecture efficient, as shown in Figure [10](#S3.F10 "Figure
    10 ‣ III-C Lightweight Scanner Networks ‣ III Methods for Efficient Processing
    of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution Deep
    Learning: A Survey"). In addition, some of the layers share weights in order to
    increase the efficiency. ICNet is able to perform semantic segmentation on 2048$\times$1024
    images at 30 frames per second with high accuracy on a Titan X GPU. Even though
    ICNet does not obtain state-of-the-art accuracy, it is $\sim 15\times$ faster
    than methods with similar performance.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ICNet [[34](#bib.bib34)]利用了处理低分辨率的效率和处理高分辨率的准确性，通过将输入图像均匀下采样到两个较小的尺度，分别处理每个尺度，并将低分辨率处理结果与高分辨率结果融合。低分辨率使用更多的卷积层处理，高分辨率使用较少的卷积层，这使得整个架构高效，如图[10](#S3.F10
    "图 10 ‣ III-C 轻量级扫描网络 ‣ III 高分辨率输入的深度学习高效处理方法 ‣ 高分辨率深度学习：综述")所示。此外，一些层共享权重以提高效率。ICNet能够以每秒30帧的速度在Titan
    X GPU上对2048$\times$1024的图像进行语义分割。尽管ICNet没有获得最先进的准确性，但其速度比类似性能的方法快$\sim 15\times$。
- en: '![Refer to caption](img/c8c6601781ec7cc234b5af57c32cdfa3.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8c6601781ec7cc234b5af57c32cdfa3.png)'
- en: 'Figure 10: ICNet architecture. CFF blocks perform the fusion operation and
    consist of convolution and upsample layers. CFF blocks get supervision signals
    using downsampled annotations during the training process.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：ICNet 架构。CFF 块执行融合操作，由卷积层和上采样层组成。CFF 块在训练过程中使用下采样标注作为监督信号。
- en: ESPNet [[93](#bib.bib93)] relies on efficient spatial pyramid (ESP) modules
    which reduce the amount of computation by decomposing standard convolutions with
    $n\times n$ kernels into two steps. The first step applies a 1$\times$1 convolution
    to project feature maps with dimension $N$ to feature maps with dimension $\frac{N}{K}$.
    The second step applies $K$ dilated convolutions with kernel size $n\times n$
    and dilation rates $2^{k-1},k\in\{1,\dots,K\}$ to the new feature maps simultaneously,
    and combines the results. Concatenating the outputs of dilated convolutions creates
    checkerboard artifacts, therefore, a simple solution is used where the outputs
    of dilated convolutions are hierarchically added to each other before concatenation.
    ESPNet can perform semantic segmentation on 2048$\times$1024 images at 54 frames
    per second with an accuracy comparable to the state-of-the-art.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ESPNet [[93](#bib.bib93)] 依赖于高效的空间金字塔（ESP）模块，通过将标准卷积的 $n\times n$ 核分解为两个步骤来减少计算量。第一步应用
    $1\times1$ 卷积，将维度为 $N$ 的特征图投影到维度为 $\frac{N}{K}$ 的特征图。第二步对新的特征图同时应用 $K$ 个膨胀卷积，卷积核大小为
    $n\times n$，膨胀率为 $2^{k-1},k\in\{1,\dots,K\}$，并将结果结合起来。由于膨胀卷积的输出在拼接时会产生棋盘状伪影，因此使用了一个简单的解决方案，即在拼接之前将膨胀卷积的输出层次相加。ESPNet
    能够以每秒 54 帧的速度对 2048$\times$1024 的图像进行语义分割，其准确性与最先进的技术相当。
- en: Neural architecture search (NAS) techniques can be used for designing better
    LSNs. Since LSNs need to be lightweight and contain few layers and parameters,
    the search space is relatively small, making NAS easier. HR-NAS [[94](#bib.bib94)]
    is one such method that searches for network architectures that can contain both
    convolutions and lightweight Transformers, and may have parallel branches. HR-NAS
    obtains state-of-the-art results in the trade-off between efficiency and accuracy
    in semantic segmentation, human pose estimation and 3D object detection tasks
    with high-resolution inputs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索（NAS）技术可用于设计更好的 LSN。由于 LSN 需要轻量化，且层数和参数较少，因此搜索空间相对较小，使得 NAS 更容易实现。HR-NAS
    [[94](#bib.bib94)] 是一种寻找可以同时包含卷积和轻量化 Transformers，并可能具有并行分支的网络架构的方法。HR-NAS 在高分辨率输入的语义分割、人类姿态估计和
    3D 目标检测任务中，在效率和准确性之间取得了最先进的结果。
- en: III-D Task-Oriented Input Compression
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 任务导向输入压缩
- en: Task-oriented input compression (TOIC) methods compress the high-resolution
    inputs into lightweight representations. These representations are then given
    to the task DNN as input instead of the high-resolution images or videos. The
    exact nature of the lightweight representations and the compression procedure
    varies from method to method and is often highly dependent on the underlying task.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 任务导向输入压缩（TOIC）方法将高分辨率输入压缩为轻量化表示。这些表示随后作为输入提供给任务 DNN，而不是高分辨率图像或视频。轻量化表示的具体性质和压缩过程因方法而异，并且通常高度依赖于基础任务。
- en: There is an important distinction between this approach and neural image compression
    methods such as SlimCAE [[95](#bib.bib95)]. The goal of neural image compression
    is to learn optimal compression algorithms for the task at hand, in order to reduce
    the size of stored or transmitted data. Therefore, the network that compresses
    and decompresses this data may be very large and inefficient. Moreover, neural
    image compression aims to reconstruct the input from the compressed representations,
    whereas TOIC does not reconstruct the input data and strives to extract compact
    representations that are suitable for the second part of the network which is
    responsible for performing the task.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与神经图像压缩方法（如 SlimCAE [[95](#bib.bib95)]）之间存在一个重要区别。神经图像压缩的目标是学习适用于当前任务的最优压缩算法，以减少存储或传输数据的大小。因此，用于压缩和解压缩数据的网络可能非常庞大且效率低下。此外，神经图像压缩旨在从压缩表示中重建输入，而
    TOIC 并不重建输入数据，而是致力于提取适合第二部分网络的紧凑表示，该网络负责执行任务。
- en: 'Slide Graph [[96](#bib.bib96)] recognizes the loss of visual context that comes
    with using the cutting into patches method, and fixes this issue by building and
    processing a compact graph representation of the cellular architecture in breast
    cancer WSIs in order to predict the status of human epidermal growth factor receptor
    2 (HER2) and progesterone receptor (PR), which are proteins that promote the growth
    of cancer cells. Slide Graph has four stages: The first stage uses a HoVer-Net
    [[97](#bib.bib97)], which is a CNN for segmentation and classification of cellular
    nuclei, trained on the PanNuke dataset [[98](#bib.bib98)] to extract features
    of the tissue cells. The second stage uses agglomerative clustering [[99](#bib.bib99)]
    to group neighboring nuclei to further reduce the computational cost. The third
    stage constructs a graph where each vertex corresponds to a cluster and contains
    features extracted in the previous stage. Graph edges are constructed based on
    Delauney triangulation where vertices are represented by the geometric center
    of their corresponding cluster, which results in a planar graph. In the final
    stage, HER2 and PR status predictions are obtained from the constructed graph
    using a graph convolutional network (GCN) [[100](#bib.bib100)]. Slide graph is
    more accurate than state-of-the-art methods and reduces the average inference
    time from 1.2 seconds of the baseline down to 0.4 milliseconds. However, these
    measurements do not include the graph construction phase. Therefore, the end-to-end
    improvement in efficiency obtained by Slide Graph is unclear.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Slide Graph [[96](#bib.bib96)] 识别到使用切片法时会丧失视觉上下文，并通过构建和处理乳腺癌 WSIs 中细胞结构的紧凑图表示来解决此问题，以预测人类表皮生长因子受体
    2 (HER2) 和孕酮受体 (PR) 的状态，这些是促进癌细胞生长的蛋白质。Slide Graph 有四个阶段：第一阶段使用 HoVer-Net [[97](#bib.bib97)]，这是一个用于细胞核分割和分类的
    CNN，经过 PanNuke 数据集 [[98](#bib.bib98)] 训练，以提取组织细胞的特征。第二阶段使用聚合聚类 [[99](#bib.bib99)]
    将相邻核分组，以进一步降低计算成本。第三阶段构建一个图，其中每个顶点对应于一个簇，并包含在先前阶段提取的特征。图的边基于 Delauney 三角化构建，其中顶点由其对应簇的几何中心表示，从而得到一个平面图。在最后阶段，HER2
    和 PR 状态预测是通过使用图卷积网络 (GCN) [[100](#bib.bib100)] 从构建的图中获得的。Slide Graph 比最先进的方法更准确，并将基准的平均推理时间从
    1.2 秒减少到 0.4 毫秒。然而，这些测量不包括图构建阶段。因此，Slide Graph 所获得的端到端效率提升尚不清楚。
- en: 'The method in [[101](#bib.bib101)], shown in Figure [11](#S3.F11 "Figure 11
    ‣ III-D Task-Oriented Input Compression ‣ III Methods for Efficient Processing
    of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution Deep
    Learning: A Survey"), compresses gigapixel histopathology WSIs down to a size
    that can be processed with a CNN on a single GPU. This compression is obtained
    by training an autoencoder (either VAE [[102](#bib.bib102)] or bidirectional GAN
    [[103](#bib.bib103)]) on image patches of size $P\times P\times 3$. The WSI image
    of size $M\times N\times 3$ is then cut into patches of the aforementioned size,
    and compressed embeddings of size $1\times 1\times C$ are obtained from the patches
    using the encoder part of the autoencoder. These embeddings are then concatenated
    to form a compressed image of size $\lceil\frac{M}{P}\rceil\times\lceil\frac{N}{P}\rceil\times
    C$, which can be given as input to the CNN. In experiments where $M=N=50,000$
    and $P=C=128$, the input size is reduced by a factor of $\sim$43.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 [[101](#bib.bib101)]，如图 [11](#S3.F11 "图 11 ‣ III-D 任务导向输入压缩 ‣ III 高分辨率输入的深度学习高效处理方法
    ‣ 高效高分辨率深度学习：综述") 所示，将 gigapixel 组织病理学 WSIs 压缩到可以用 CNN 在单个 GPU 上处理的大小。这种压缩是通过在大小为
    $P\times P\times 3$ 的图像块上训练自编码器（VAE [[102](#bib.bib102)] 或双向 GAN [[103](#bib.bib103)]）获得的。WSI
    图像的大小为 $M\times N\times 3$，然后被切成上述大小的块，使用自编码器的编码器部分从这些块中获得大小为 $1\times 1\times
    C$ 的压缩嵌入。这些嵌入随后被串联形成大小为 $\lceil\frac{M}{P}\rceil\times\lceil\frac{N}{P}\rceil\times
    C$ 的压缩图像，可以作为 CNN 的输入。在 $M=N=50,000$ 和 $P=C=128$ 的实验中，输入大小减少了约 43 倍。
- en: '![Refer to caption](img/ff1b382d162246f7fbb0370003f8cc66.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/ff1b382d162246f7fbb0370003f8cc66.png)'
- en: 'Figure 11: A method based on neural image compression for gigapixel histopathology
    images.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：一种基于神经图像压缩的 gigapixel 组织病理图像处理方法。
- en: MCAT [[104](#bib.bib104)] uses a combination of WSIs and genomics data for cancer
    survival outcome prediction. At the core of MCAT is the Genomic-Guided Co-Attention
    (GCA) layer which reduces the spatial complexity of processing WSIs. MCAT processes
    the input in data structures known as bags, which are unordered sets of objects
    of varying size without individual labels. MCAT constructs one bag ($H_{\text{bag}}$)
    from multiple WSIs in order to utilize the entire tissue microenvironment, and
    another bag ($G_{\text{bag}}$) from genomic features. $H_{\text{bag}}$ is constructed
    by cutting the WSIs into non-overlapping $256\times 256$ pixel patches and processing
    each patch with a ResNet50 CNN [[105](#bib.bib105)] pre-trained on the ImageNet
    dataset [[106](#bib.bib106)] to obtain $d_{k}$-dimensional feature embeddings.
    $G_{\text{bag}}$ is constructed by categorizing genes into N different sets based
    on similarity and applying a fully-connected (FC) layer to obtain genomic embeddings.
    GCA then takes these two bags as input and performs the co-attention operation
    by
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: MCAT [[104](#bib.bib104)] 结合了 WSIs 和基因组数据来预测癌症生存结果。MCAT 的核心是基因组引导的共同注意力（GCA）层，它减少了处理
    WSIs 的空间复杂性。MCAT 在数据结构中处理输入，这些结构称为袋，这些袋是不同大小且没有个别标签的无序对象集合。MCAT 从多个 WSIs 构建一个袋（$H_{\text{bag}}$），以利用整个组织微环境，并从基因组特征中构建另一个袋（$G_{\text{bag}}$）。$H_{\text{bag}}$
    通过将 WSIs 切割成非重叠的 $256\times 256$ 像素块，并用在 ImageNet 数据集 [[106](#bib.bib106)] 上预训练的
    ResNet50 CNN [[105](#bib.bib105)] 处理每个块，以获得 $d_{k}$ 维特征嵌入。$G_{\text{bag}}$ 通过根据相似性将基因分类为
    N 个不同的集合，并应用全连接（FC）层以获得基因组嵌入。然后 GCA 以这两个袋作为输入，执行共同注意力操作。
- en: '|  | $\displaystyle\text{CoAttn}_{G\rightarrow H}(G,H)$ | $\displaystyle=$
    | $\displaystyle\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$ |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{CoAttn}_{G\rightarrow H}(G,H)$ | $\displaystyle=$
    | $\displaystyle\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$ |  |'
- en: '|  |  | $\displaystyle=$ | $\displaystyle\text{softmax}\left(\frac{W_{q}GH^{T}W_{k}^{T}}{\sqrt{d_{k}}}\right)W_{v}H,$
    |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ | $\displaystyle\text{softmax}\left(\frac{W_{q}GH^{T}W_{k}^{T}}{\sqrt{d_{k}}}\right)W_{v}H,$
    |  |'
- en: 'where $Q=W_{q}G$ is the query matrix, $K=W_{k}H$ is the key matrix, $V=W_{v}H$
    is the value matrix, and $W_{q},W_{k},W_{v}\in\mathbb{R}^{d_{k}\times d_{k}}$
    are trainable weights. The output of this operation, as shown in Figure [12](#S3.F12
    "Figure 12 ‣ III-D Task-Oriented Input Compression ‣ III Methods for Efficient
    Processing of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey"), has a dimension of $N\times d_{k}$. Therefore, the
    subsequent self-attention layers in the MCAT network are quadratic with respect
    to $N$ instead of $M$. Since on average $M=15,231$ and $N=6$, this results in
    a massive reduction in complexity.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $Q=W_{q}G$ 是查询矩阵，$K=W_{k}H$ 是键矩阵，$V=W_{v}H$ 是值矩阵，$W_{q},W_{k},W_{v}\in\mathbb{R}^{d_{k}\times
    d_{k}}$ 是可训练权重。该操作的输出，如图 [12](#S3.F12 "Figure 12 ‣ III-D Task-Oriented Input Compression
    ‣ III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey") 所示，维度为 $N\times d_{k}$。因此，MCAT
    网络中的后续自注意力层相对于 $N$ 是二次的，而不是 $M$。由于平均 $M=15,231$ 和 $N=6$，这导致了复杂性的显著降低。'
- en: '![Refer to caption](img/51914aec7e3999de67b61c67bd7a8035.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/51914aec7e3999de67b61c67bd7a8035.png)'
- en: 'Figure 12: Genomic-Guided Co-Attention (GCA) layer.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：基因组引导的共同注意力（GCA）层。
- en: A subcategory of TOIC methods are frequency-domain DNNs, which convert input
    RGB pixels to frequency domain representations with the help of operations such
    as discrete cosine transform (DCT) or wavelet transform. The intuition behind
    this approach is that the first few layers in CNNs often learn filters that resemble
    such transforms. Therefore, not only are image representations more compact in
    the frequency domain, but also a lower number of layers is required for processing
    such representations.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: TOIC 方法的一个子类别是频域 DNNs，它们利用离散余弦变换（DCT）或小波变换等操作将输入 RGB 像素转换为频域表示。这种方法的直觉是 CNN
    的前几层通常学习类似于这些变换的滤波器。因此，图像表示在频域中更紧凑，而且处理这些表示所需的层数也更少。
- en: The method in [[107](#bib.bib107)] uses the DCT coefficients obtained in the
    middle of JPEG encoding as inputs to a modified ResNet50 CNN [[105](#bib.bib105)]
    for the image classification task. JPEG encoding consists of three stages. The
    first stage converts the input 3-channel 24-bit RGB image to the YCbCr color space
    by
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 [[107](#bib.bib107)] 使用在 JPEG 编码中间获得的 DCT 系数作为输入，传递给修改后的 ResNet50 CNN [[105](#bib.bib105)]
    进行图像分类任务。JPEG 编码包括三个阶段。第一阶段将输入的 3 通道 24 位 RGB 图像转换为 YCbCr 颜色空间。
- en: '|  | <math   alttext="\begin{bmatrix}Y\\ Cb\\'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{bmatrix}Y\\ Cb\\
- en: Cr\end{bmatrix}=\begin{bmatrix}0.299&amp;0.587&amp;0.114\\
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Cr\end{bmatrix}=\begin{bmatrix}0.299&0.587&0.114\\
- en: -0.168935&amp;-0.331665&amp;0.50059\\
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: -0.168935&-0.331665&0.50059\\
- en: 0.499813&amp;-0.418531&amp;-0.081282\end{bmatrix}\begin{bmatrix}R\\
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 0.499813&-0.418531&-0.081282\end{bmatrix}\begin{bmatrix}R\\
- en: G\\
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: G\\
- en: B\end{bmatrix}." display="block"><semantics ><mrow ><mrow  ><mrow ><mo  >[</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd ><mi  >Y</mi></mtd></mtr><mtr
    ><mtd ><mrow  ><mi >C</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >b</mi></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mi >C</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >r</mi></mrow></mtd></mtr></mtable><mo
    >]</mo></mrow><mo >=</mo><mrow  ><mrow ><mo  >[</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd ><mn >0.299</mn></mtd><mtd ><mn  >0.587</mn></mtd><mtd
    ><mn >0.114</mn></mtd></mtr><mtr ><mtd  ><mrow ><mo >−</mo><mn  >0.168935</mn></mrow></mtd><mtd
    ><mrow ><mo  >−</mo><mn >0.331665</mn></mrow></mtd><mtd ><mn >0.50059</mn></mtd></mtr><mtr
    ><mtd  ><mn >0.499813</mn></mtd><mtd ><mrow ><mo  >−</mo><mn >0.418531</mn></mrow></mtd><mtd
    ><mrow ><mo  >−</mo><mn >0.081282</mn></mrow></mtd></mtr></mtable><mo >]</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo  >[</mo><mtable displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd ><mi  >R</mi></mtd></mtr><mtr ><mtd ><mi  >G</mi></mtd></mtr><mtr
    ><mtd ><mi  >B</mi></mtd></mtr></mtable><mo >]</mo></mrow></mrow></mrow><mo lspace="0em"  >.</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><csymbol cd="latexml"  >matrix</csymbol><matrix
    ><matrixrow ><ci  >𝑌</ci></matrixrow><matrixrow ><apply ><ci  >𝐶</ci><ci >𝑏</ci></apply></matrixrow><matrixrow
    ><apply ><ci  >𝐶</ci><ci >𝑟</ci></apply></matrixrow></matrix></apply><apply ><apply
    ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><cn type="float"  >0.299</cn><cn
    type="float"  >0.587</cn><cn type="float"  >0.114</cn></matrixrow><matrixrow ><apply
    ><cn type="float" >0.168935</cn></apply><apply ><cn type="float" >0.331665</cn></apply><cn
    type="float" >0.50059</cn></matrixrow><matrixrow ><cn type="float"  >0.499813</cn><apply
    ><cn type="float" >0.418531</cn></apply><apply ><cn type="float" >0.081282</cn></apply></matrixrow></matrix></apply><apply
    ><csymbol cd="latexml" >matrix</csymbol><matrix  ><matrixrow ><ci >𝑅</ci></matrixrow><matrixrow
    ><ci  >𝐺</ci></matrixrow><matrixrow ><ci >𝐵</ci></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{bmatrix}Y\\ Cb\\ Cr\end{bmatrix}=\begin{bmatrix}0.299&0.587&0.114\\
    -0.168935&-0.331665&0.50059\\ 0.499813&-0.418531&-0.081282\end{bmatrix}\begin{bmatrix}R\\
    G\\ B\end{bmatrix}.</annotation></semantics></math> |  | (12) |
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{bmatrix}Y\\ Cb\\ Cr\end{bmatrix}=\begin{bmatrix}0.299&0.587&0.114\\ -0.168935&-0.331665&0.50059\\
    0.499813&-0.418531&-0.081282\end{bmatrix}\begin{bmatrix}R\\ G\\ B\end{bmatrix}。
- en: 'The luma component (Y) represents the brightness, and the chroma components
    (Cb and Cr) represent color. The resolution of chroma components is then reduced
    by a factor of 2 due to the fact that the human eye is less sensitive to fine
    color detail than fine brightness. Figure [13](#S3.F13 "Figure 13 ‣ III-D Task-Oriented
    Input Compression ‣ III Methods for Efficient Processing of High-Resolution Inputs
    with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey") shows
    an example image and its corresponding Y, Cb and Cr components. The second stage
    is a blockwise DCT, where each of the three components is partitioned into $8\times
    8$ blocks that undergo a 2D DCT. The amplitude values of the frequency domain
    are the input representations used by this method. The DCT representations of
    Cb and Cr are upsampled by a factor of two and concatenated with the DCT representation
    of Y before being given as input to the task DNN, as shown in Figure [14](#S3.F14
    "Figure 14 ‣ III-D Task-Oriented Input Compression ‣ III Methods for Efficient
    Processing of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey"). The rest of the JPEG encoding process contains the
    quantization of these representations as well as lossless compression techniques
    such as Huffman coding. However, this method uses the representations obtained
    before quantization and lossless compression.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 亮度分量（Y）表示亮度，而色度分量（Cb和Cr）表示颜色。由于人眼对细微颜色细节的敏感度低于对细微亮度的敏感度，因此色度分量的分辨率会减少一个2倍的因子。图[13](#S3.F13
    "图 13 ‣ III-D 任务导向的输入压缩 ‣ III 高分辨率输入的深度学习高效处理方法 ‣ 高分辨率深度学习：综述")显示了一个示例图像及其对应的Y、Cb和Cr分量。第二阶段是分块DCT，其中三个分量中的每一个都被划分为$8\times
    8$块，这些块经过2D DCT处理。频域的幅值是该方法使用的输入表示。Cb和Cr的DCT表示被上采样2倍，并与Y的DCT表示拼接，然后作为输入提供给任务DNN，如图[14](#S3.F14
    "图 14 ‣ III-D 任务导向的输入压缩 ‣ III 高分辨率输入的深度学习高效处理方法 ‣ 高分辨率深度学习：综述")所示。JPEG编码过程的其余部分包括对这些表示的量化以及霍夫曼编码等无损压缩技术。然而，该方法使用的是量化和无损压缩之前获得的表示。
- en: '| ![Refer to caption](img/18d06a2e29c78d677eba1e28778d579f.png) | ![Refer to
    caption](img/61cb171fad8f022218c6d892202cd15f.png) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/18d06a2e29c78d677eba1e28778d579f.png) | ![参考说明](img/61cb171fad8f022218c6d892202cd15f.png)
    |'
- en: '| (a) | (b) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| (a) | (b) |'
- en: '| ![Refer to caption](img/29e9a3eefa3a441e85d56b333f2ceb67.png) | ![Refer to
    caption](img/7b007c3b20c51332aba41f066ae8ae84.png) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/29e9a3eefa3a441e85d56b333f2ceb67.png) | ![参考说明](img/7b007c3b20c51332aba41f066ae8ae84.png)
    |'
- en: '| (c) | (d) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| (c) | (d) |'
- en: 'Figure 13: (a) Original color image, taken from the Shanghai Tech Part B dataset
    [[20](#bib.bib20)]; (b) luma component Y, which is essentially a grayscale version
    of the color image; (c) chroma component Cb; and (d) chroma component Cr.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: (a) 原始彩色图像，取自上海科技B部分数据集[[20](#bib.bib20)]; (b) 亮度分量Y，本质上是彩色图像的灰度版本; (c)
    色度分量Cb; (d) 色度分量Cr。'
- en: '![Refer to caption](img/72f528aa54ba8c7951ab8f0bf6de3ec5.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/72f528aa54ba8c7951ab8f0bf6de3ec5.png)'
- en: 'Figure 14: Initial stages of JPEG encoding, used by [[107](#bib.bib107)] to
    obtain frequency-domains representations of the RGB input.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: JPEG编码的初始阶段，由[[107](#bib.bib107)]用于获得RGB输入的频域表示。'
- en: With the help of these input representations, this method obtains DNNs that
    are both more accurate and up to $1.77\times$ faster than ResNet50\. Moreover,
    [[107](#bib.bib107)] includes experiments attempting to learn convolutions behaving
    like DCT, however, they find that this learned DCT transform leads to higher error
    compared to the conventional DCT transform.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些输入表示的帮助下，该方法获得的深度神经网络（DNN）比ResNet50更准确且速度快达$1.77\times$。此外，[[107](#bib.bib107)]包含了试图学习类似于DCT行为的卷积的实验，但他们发现学习到的DCT变换与传统的DCT变换相比导致了更高的错误率。
- en: The method in [[108](#bib.bib108)] uses the same idea for image classification
    and semantic segmentation tasks using ResNet50 and MobileNetV2 architectures.
    However, this method also prunes the 192 DCT channels with the help of a gating
    module that generates a binary decision for each channel. Furthermore, this study
    discovers that some channels are consistently pruned regardless of the particular
    task, and develops a static frequency channel selection scheme based on these
    results. This scheme prunes up to 87.5% of the channels with little accuracy drop,
    if any. The method in [[109](#bib.bib109)] uses the same approach for image classification,
    however, it uses several variants of discrete wavelet transform (DWT) instead
    of DCT. The advantage of DWT over DCT is that it can obtain a better compression
    ratio without loss of information, however, it is more computationally expensive
    [[110](#bib.bib110)]. Experiments show that using DWT instead of DCT can lead
    to higher accuracy, however, the impact of DWT on inference time is unclear.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[[108](#bib.bib108)] 中的方法在使用 ResNet50 和 MobileNetV2 架构进行图像分类和语义分割任务时采用了相同的思路。然而，该方法还通过一个生成每个通道的二进制决策的门控模块来修剪
    192 个 DCT 通道。此外，该研究发现某些通道在特定任务中始终被修剪，并基于这些结果开发了一种静态频率通道选择方案。如果有的话，该方案修剪了多达 87.5%
    的通道，且精度下降很小。[[109](#bib.bib109)] 中的方法对图像分类采用了相同的方法，但使用了几种离散小波变换 (DWT) 的变体而非 DCT。DWT
    相较于 DCT 的优势在于它可以在不丢失信息的情况下获得更好的压缩比，但计算开销更大 [[110](#bib.bib110)]。实验表明，使用 DWT 代替
    DCT 可以带来更高的准确性，但 DWT 对推理时间的影响尚不明确。'
- en: 'Finally, similar to images, DNNs can directly process the compressed representations
    obtained by video compression formats. MMNet [[111](#bib.bib111)] performs efficient
    object detection on H.264/MPEG-4 Part 10 compressed videos [[112](#bib.bib112)],
    one of the most commonly used video compression formats, by taking advantage of
    the motion information already embedded in the video compression format. It only
    runs the complete feature extractor DNN on few reference frames in the video and
    aggregates the visual information from the subsequent frames with the help of
    an LSTM [[113](#bib.bib113)]. H.264 has two types of frames: I-frames which contain
    a complete image, and P-frames, also known as delta frames, which store the offset
    to previous frames using motion vectors and residual errors. In MMNet, the extracted
    motion vectors and residual errors for each P-frame following an I-frame are passed
    on to the LSTM. MMNet is $3\times$ to $10\times$ faster than competing models
    with minor loss in accuracy.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，类似于图像，DNN 可以直接处理通过视频压缩格式获得的压缩表示。MMNet [[111](#bib.bib111)] 通过利用已经嵌入视频压缩格式中的运动信息，对
    H.264/MPEG-4 Part 10 压缩视频 [[112](#bib.bib112)] 执行高效的目标检测。它仅在视频中的少数参考帧上运行完整的特征提取器
    DNN，并利用 LSTM [[113](#bib.bib113)] 聚合后续帧的视觉信息。H.264 有两种帧类型：I-帧包含完整图像，而 P-帧，也称为增量帧，使用运动矢量和残差错误存储对前一帧的偏移。在
    MMNet 中，从 I-帧后提取的每个 P-帧的运动矢量和残差错误会传递给 LSTM。MMNet 的速度比竞争模型快 $3\times$ 到 $10\times$，且精度损失较小。
- en: III-E High-Resolution Vision Transformers
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 高分辨率视觉 Transformer
- en: As previously mentioned, the self-attention operation in Transformers has a
    high complexity that increases in a quadratic fashion with respect to the number
    of input tokens. This operation is formulated by
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Transformers 中的自注意力操作具有很高的复杂度，其复杂度随着输入标记数量的平方增加。该操作的公式为
- en: '|  | $Z=\mathit{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V,$ |  | (13)
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $Z=\mathit{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V,$ |  | (13)
    |'
- en: where query $Q=XW^{Q}\in\mathbb{R}^{n\times d_{q}}$, key $K=XW^{K}\in\mathbb{R}^{n\times
    d_{k}}$ and value $V=XW^{V}\in\mathbb{R}^{n\times d_{v}}$ are obtained from sequence
    of input tokens $X=(x_{1},\dots,x_{n})\in\mathbb{R}^{n\times d}$, and $W^{Q}$,
    $W^{K}$ and $W^{V}$ are learnable weight matrices. Due to this quadratic complexity,
    naive approaches, such as ViT [[6](#bib.bib6)], that create a long sequence of
    input tokens from a high-resolution image will lead to massive complexity. On
    the other hand, if $X$ contains few tokens, each input token represents a large
    area of the original image, leading to loss of detailed information that might
    be crucial to some applications.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其中查询 $Q=XW^{Q}\in\mathbb{R}^{n\times d_{q}}$、键 $K=XW^{K}\in\mathbb{R}^{n\times
    d_{k}}$ 和值 $V=XW^{V}\in\mathbb{R}^{n\times d_{v}}$ 是从输入 token 序列 $X=(x_{1},\dots,x_{n})\in\mathbb{R}^{n\times
    d}$ 中获得的，$W^{Q}$、$W^{K}$ 和 $W^{V}$ 是可学习的权重矩阵。由于这种二次复杂度，像 ViT [[6](#bib.bib6)]
    这样的朴素方法从高分辨率图像创建长序列的输入 token 会导致巨大的复杂度。另一方面，如果 $X$ 包含少量 token，则每个输入 token 代表原始图像的大面积，可能会丢失对某些应用至关重要的详细信息。
- en: Vision Longformer (ViL) [[114](#bib.bib114)] is a variant of Longformer [[115](#bib.bib115)]
    which has a linear complexity with respect to the number of input tokens, and
    is capable of processing high-resolution images. This linear complexity is achieved
    by adding $n_{g}$ global tokens, which include the classification token cls, that
    serve as global memory by attending to all input tokens. Input tokens are only
    allowed to attend to the global tokens as well as their neighbors within a 2D
    window. If the number of input tokens are $n_{l}$ and the 2D window size is $w$,
    then the memory complexity is $\mathcal{O}(n_{g}(n_{g}+n_{l})+n_{l}w^{2})$. When
    $n_{g}\ll n_{l}$, the complexity is significantly reduced from the original $n_{l}^{2}$
    in ViT. By using ViL in a multi-scale architecture, multi-scale Vision Longformer
    is able to obtain superior performance compared to the state-of-the-art in image
    classification, object detection and semantic segmentation while requiring less
    computation in terms of FLOPs in some cases.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Vision Longformer (ViL) [[114](#bib.bib114)] 是 Longformer [[115](#bib.bib115)]
    的一种变体，它在处理输入 token 数量时具有线性复杂度，并且能够处理高分辨率图像。这种线性复杂度是通过添加 $n_{g}$ 个全局 token 实现的，其中包括分类
    token cls，这些 token 通过关注所有输入 token 来作为全局记忆。输入 token 只能关注全局 token 以及其在 2D 窗口内的邻居。如果输入
    token 的数量为 $n_{l}$，而 2D 窗口大小为 $w$，那么记忆复杂度为 $\mathcal{O}(n_{g}(n_{g}+n_{l})+n_{l}w^{2})$。当
    $n_{g}\ll n_{l}$ 时，复杂度显著降低，相比于 ViT 原始的 $n_{l}^{2}$。通过在多尺度架构中使用 ViL，多尺度 Vision
    Longformer 在图像分类、目标检测和语义分割方面能够获得优于最先进技术的性能，同时在某些情况下需要更少的计算 FLOPs。
- en: 'High-Resolution Transformer (HRFormer) [[116](#bib.bib116)] reduces the computational
    complexity of self-attention by partitioning the input representations into non-overlapping
    patches, and performing the self-attention only within each patch. Figure [15](#S3.F15
    "Figure 15 ‣ III-E High-Resolution Vision Transformers ‣ III Methods for Efficient
    Processing of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey") shows the building block of HRFormer, which contains
    a depth-wise convolution that facilitates information exchange between patches.
    By utilizing this augmented self-attention in a multi-scale architecture, HRFormer
    obtains superior performance in human pose estimation and semantic segmentation
    with fewer parameters and FLOPs.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率 Transformer (HRFormer) [[116](#bib.bib116)] 通过将输入表示划分为不重叠的补丁，并仅在每个补丁内执行自注意力，从而降低了自注意力的计算复杂度。图
    [15](#S3.F15 "图 15 ‣ III-E 高分辨率视觉 Transformer ‣ III 高分辨率输入的深度学习高效处理方法 ‣ 高效高分辨率深度学习：综述")
    显示了 HRFormer 的构建块，其中包含一个深度卷积，用于促进补丁之间的信息交换。通过在多尺度架构中利用这种增强的自注意力，HRFormer 在人体姿态估计和语义分割中表现优异，同时参数和
    FLOPs 更少。
- en: '![Refer to caption](img/304509d1721335cb94225847629d35e3.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/304509d1721335cb94225847629d35e3.png)'
- en: 'Figure 15: HRFormer block. Multi-head self-attention (MHSA) is applied only
    within each patch. The patches are then concatenated and followed by a depth-wise
    (DW) convolution.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: HRFormer 块。多头自注意力 (MHSA) 仅在每个补丁内应用。然后将补丁连接起来，并进行深度卷积 (DW)。'
- en: 'Multi-Scale High-Resolution Vision Transformer (HRViT) [[117](#bib.bib117)]
    uses cross-shaped self-attention [[118](#bib.bib118)] and parameter sharing to
    decrease the computational cost of self-attention. Cross-shaped self-attention,
    shown in Figure [16](#S3.F16 "Figure 16 ‣ III-E High-Resolution Vision Transformers
    ‣ III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey"), splits the $K$ self-attention
    heads present in multi-head attention into two groups: $\{h_{1},\dots,h_{\frac{K}{2}}\}$
    and $\{h_{\frac{K}{2}+1},\dots,h_{K}\}$. These groups perform self-attention in
    horizontal and vertical strips in parallel. Strip width sw can be adjusted to
    achieve a trade-off between efficiency and performance. The linear projections
    for key and value tensors are shared in HRViT’s blocks to save in computation
    and parameters. In addition to efficient self-attention, HRViT employs a convolutional
    stem to reduce the spatial dimension of the input. HRViT achieves the best performance-efficiency
    trade-off compared to state-of-the-art models for semantic segmentation.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '多尺度高分辨率视觉变换器（HRViT）[[117](#bib.bib117)]使用交叉形状自注意力[[118](#bib.bib118)]和参数共享来降低自注意力的计算成本。交叉形状自注意力，如图[16](#S3.F16
    "Figure 16 ‣ III-E High-Resolution Vision Transformers ‣ III Methods for Efficient
    Processing of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey")所示，将多头注意力中的$K$个自注意力头分为两组：$\{h_{1},\dots,h_{\frac{K}{2}}\}$和$\{h_{\frac{K}{2}+1},\dots,h_{K}\}$。这些组在水平和垂直条带中并行执行自注意力。条带宽度sw可以调整，以在效率和性能之间实现权衡。HRViT的块中键和值张量的线性投影是共享的，以节省计算和参数。除了高效的自注意力，HRViT还采用卷积stem来减少输入的空间维度。与最先进的语义分割模型相比，HRViT在性能和效率之间实现了最佳的权衡。'
- en: '![Refer to caption](img/8b1dbccbedcda7359de8f96ce5150abe.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8b1dbccbedcda7359de8f96ce5150abe.png)'
- en: 'Figure 16: Cross-shaped self-attention.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：交叉形状自注意力。
- en: 'Instead of restricting self-attention to patches that are neighbors in the
    2D grid, Glance and Gaze Transformer (GG-Transformer) [[119](#bib.bib119)], shown
    in Figure [17](#S3.F17 "Figure 17 ‣ III-E High-Resolution Vision Transformers
    ‣ III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey"), performs the self-attention
    within dilated partitions. Since these dilations create holes in the receptive
    field, a parallel branch containing depth-wise convoluion is added to compensate
    for the local interactions with negligible cost. GG-Transformer achieves superior
    performance in image classification, object detection and semantic segmentation
    and reduces the parameters or FLOPs in some cases.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '与将自注意力限制在2D网格中相邻的补丁不同，Glance和Gaze Transformer（GG-Transformer）[[119](#bib.bib119)]，如图[17](#S3.F17
    "Figure 17 ‣ III-E High-Resolution Vision Transformers ‣ III Methods for Efficient
    Processing of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey")所示，在扩张分区内执行自注意力。由于这些扩张在感受野中创建了空洞，因此添加了一个包含深度卷积的并行分支，以以微不足道的成本弥补局部交互。GG-Transformer在图像分类、目标检测和语义分割方面表现优越，并在某些情况下减少了参数或FLOPs。'
- en: '![Refer to caption](img/c57a57d7371cc724e0cf1eacd361e5db.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c57a57d7371cc724e0cf1eacd361e5db.png)'
- en: 'Figure 17: GG-Transformer block.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：GG-Transformer块。
- en: 'Hierarchical Image Pyramid Transformer (HIPT) [[120](#bib.bib120)] processes
    gigapixel WSIs for the task of cancer subtyping and survival prediction. Since
    the input WSIs are as large as 150,000$\times$150,000 pixels, processing them
    with a normal ViT and small patch size, such as 16$\times$16, results in a massive
    number of parameters and computational cost requirements, and using large patch
    sizes such as 4096$\times$4096 pixels directly would result in loss of cellular
    information. Therefore, HIPT takes a hierarchical approach, shown in Figure [18](#S3.F18
    "Figure 18 ‣ III-E High-Resolution Vision Transformers ‣ III Methods for Efficient
    Processing of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey"), where an initial ViT processes patches of 16$\times$16
    in an area of size 256$\times$256 pixels. A second ViT then takes the aggregated
    tokens from the previous ViT and processes an area of size 4096$\times$4096 pixels.
    A final ViT takes the aggregated tokens from the second ViT and processes the
    entire image.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 层次化图像金字塔变换器（HIPT）[[120](#bib.bib120)]处理gigapixel WSI用于癌症亚型分类和生存预测任务。由于输入的WSI大到150,000$\times$150,000像素，用正常的ViT和小补丁大小（如16$\times$16）处理它们会导致大量的参数和计算成本，而使用如4096$\times$4096像素的大补丁大小直接处理会导致细胞信息的丢失。因此，HIPT采取了层次化的方法，如图[18](#S3.F18
    "图18 ‣ III-E 高分辨率视觉变换器 ‣ III 高分辨率输入的深度学习高效处理方法 ‣ 高效高分辨率深度学习：综述")所示，其中初始的ViT处理256$\times$256像素区域中的16$\times$16补丁。第二个ViT然后处理从第一个ViT聚合的tokens，并处理4096$\times$4096像素的区域。最终的ViT处理从第二个ViT聚合的tokens，并处理整个图像。
- en: '![Refer to caption](img/1b9b365f7589f61107d65a39290742e1.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1b9b365f7589f61107d65a39290742e1.png)'
- en: 'Figure 18: Hierarchical Image Pyramid Transformer (HIPT). The notation $\text{ViT}_{L}-l$
    means a Vision Transformer that operates on size $L\times L$ with patch size of
    $l\times l$. $\text{ViT}_{\text{WSI}}$ operates on the entire WSI.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：层次化图像金字塔变换器（HIPT）。符号$\text{ViT}_{L}-l$表示一个在大小为$L\times L$、补丁大小为$l\times
    l$的Vision Transformer。$\text{ViT}_{\text{WSI}}$操作整个WSI。
- en: IV High-Resolution Datasets
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 高分辨率数据集
- en: 'Table [IV](#S4.T4 "TABLE IV ‣ IV High-Resolution Datasets ‣ Efficient High-Resolution
    Deep Learning: A Survey") lists popular datasets used in the high-resolution deep
    learning literature and provides information about their attributes, such as which
    is the deep learning application they have been primarily used for, the number
    of images/videos in the dataset and their resolution, the type of available annotations,
    whether they specify training/validation/test set splits, the year of publication,
    and whether they are publicly available. It is important to note that studies
    reported in some papers create customized datasets. For instance, [[79](#bib.bib79)]
    constructs a dataset from YFCC100M [[121](#bib.bib121)]; [[89](#bib.bib89)] constructs
    datasets from AFLW [[122](#bib.bib122)], MTFL [[123](#bib.bib123)] and WIDER FACE
    [[124](#bib.bib124)]; and [[9](#bib.bib9)] constructs datasets from DigitalGlobe
    satellites, Planet satellites, and aerial platforms.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表[IV](#S4.T4 "表 IV ‣ IV 高分辨率数据集 ‣ 高效高分辨率深度学习：综述")列出了在高分辨率深度学习文献中使用的流行数据集，并提供了它们的属性信息，例如主要用于哪些深度学习应用、数据集中的图像/视频数量及其分辨率、可用的注释类型、是否指定了训练/验证/测试集划分、出版年份以及是否公开可用。需要注意的是，一些论文中的研究创建了自定义数据集。例如，[[79](#bib.bib79)]从YFCC100M[[121](#bib.bib121)]构建了数据集；[[89](#bib.bib89)]从AFLW[[122](#bib.bib122)]、MTFL[[123](#bib.bib123)]和WIDER
    FACE[[124](#bib.bib124)]构建了数据集；[[9](#bib.bib9)]从DigitalGlobe卫星、Planet卫星和航空平台构建了数据集。
- en: 'The Cancer Genome Atlas (TCGA) program is a collaboration between National
    Cancer Institute (NCI) and National Human Genome Research (NHGRI)¹¹1[https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga](https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga).
    Since 2006, TCGA has generated over 2.5 petabytes of publicly available data which
    has led to improvements in cancer diagnosis, treatment, and prevention. Among
    efficient high-resolution deep learning methods, the most widely used subset of
    this data is the breast invasive carcinoma (BRCA), which is outlined in Table
    [IV](#S4.T4 "TABLE IV ‣ IV High-Resolution Datasets ‣ Efficient High-Resolution
    Deep Learning: A Survey"). However, TCGA provides data for many other types of
    cancer, such as bladder urothelial carcinoma (BLCA), glioblastoma and lower grade
    glioma (GBMLGG), lung adenocarcinoma (LUAD), and uterine corpus endometrial carcinoma
    (UCEC). These are used in some studies, and have properties similar to that of
    BRCA.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '癌症基因组图谱（TCGA）计划是国家癌症研究所（NCI）和国家人类基因组研究所（NHGRI）¹¹1的合作项目[https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga](https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga)。自2006年以来，TCGA已生成超过2.5PB的公开数据，这些数据在癌症诊断、治疗和预防方面带来了改进。在高分辨率深度学习方法中，最广泛使用的数据子集是乳腺浸润癌（BRCA），其详细信息见表[IV](#S4.T4
    "TABLE IV ‣ IV High-Resolution Datasets ‣ Efficient High-Resolution Deep Learning:
    A Survey")。然而，TCGA还提供了许多其他类型的癌症数据，如膀胱尿路上皮癌（BLCA）、胶质母细胞瘤和低级别胶质瘤（GBMLGG）、肺腺癌（LUAD）和子宫内膜癌（UCEC）。这些数据在一些研究中被使用，且具有与BRCA类似的特征。'
- en: 'TABLE IV: List of Popular High-Resolution Datasets'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：流行高分辨率数据集列表
- en: '| Name | Applications | Resolution (Pixels) | # of Samples | Annotations |
    Splits | Year | Availability |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 应用 | 分辨率（像素） | 样本数量 | 标注 | 划分 | 年份 | 可用性 |'
- en: '| Supervisely Persons^‡ | Person Segmentation | 800$\times$1116 to 9933$\times$6622
    | 5711 images | Pixel Mask | None | 2018 | Public |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Supervisely Persons^‡ | 人物分割 | 800$\times$1116 到 9933$\times$6622 | 5711张图片
    | 像素掩膜 | 无 | 2018 | 公开 |'
- en: '| PANDA[[13](#bib.bib13)] | Person Detection | $>$25K$\times$14K | 555 frames^§
    | Person Bounding Box | None | 2020 | Upon Request |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| PANDA[[13](#bib.bib13)] | 人物检测 | $>$25K$\times$14K | 555帧^§ | 人体边界框 | 无 |
    2020 | 按需提供 |'
- en: '| UCF_CC_50[[125](#bib.bib125)] | Crowd Counting | 2888$\times$2101 on average
    | 50 images | Head Annotations^∗ | None | 2013 | Public |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| UCF_CC_50[[125](#bib.bib125)] | 人群计数 | 平均2888$\times$2101 | 50张图片 | 头部标注^∗
    | 无 | 2013 | 公开 |'
- en: '| Shanghai Tech Part A[[126](#bib.bib126)] | Crowd Counting | 868$\times$589
    | 482 images | Head Annotations | Train & Test | 2016 | Public |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 上海科技A部分[[126](#bib.bib126)] | 人群计数 | 868$\times$589 | 482张图片 | 头部标注 | 训练和测试
    | 2016 | 公开 |'
- en: '| Shanghai Tech Part B[[126](#bib.bib126)] | Crowd Counting | 1024$\times$768
    | 716 images | Head Annotations | Train & Test | 2016 | Public |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 上海科技B部分[[126](#bib.bib126)] | 人群计数 | 1024$\times$768 | 716张图片 | 头部标注 | 训练和测试
    | 2016 | 公开 |'
- en: '| UCF-QNRF[[127](#bib.bib127)] | Crowd Counting | 2902$\times$2013 on average
    | 1535 images | Head Annotations | Train & Test | 2018 | Public |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| UCF-QNRF[[127](#bib.bib127)] | 人群计数 | 平均2902$\times$2013 | 1535张图片 | 头部标注
    | 训练和测试 | 2018 | 公开 |'
- en: '| PANDA Crowd[[13](#bib.bib13)] | Crowd Counting | 25,151$\times$14,151 to
    26,908$\times$15,024 | 45 images | Person Bounding Box | None | 2020 | Upon Request
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| PANDA Crowd[[13](#bib.bib13)] | 人群计数 | 25,151$\times$14,151 到 26,908$\times$15,024
    | 45张图片 | 人体边界框 | 无 | 2020 | 按需提供 |'
- en: '| JHU-CROWD++[[128](#bib.bib128)] | Crowd Counting | 1430$\times$910 on average
    | 4372 images | Head Annotations | Train, Val & Test | 2020 | Public |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| JHU-CROWD++[[128](#bib.bib128)] | 人群计数 | 平均1430$\times$910 | 4372张图片 | 头部标注
    | 训练、验证和测试 | 2020 | 公开 |'
- en: '| NWPU-Crowd[[129](#bib.bib129)] | Crowd Counting | 3209$\times$2191 on average
    | 5109 images | Head Annotations | Train, Val & Test | 2020 | Public |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| NWPU-Crowd[[129](#bib.bib129)] | 人群计数 | 平均3209$\times$2191 | 5109张图片 | 头部标注
    | 训练、验证和测试 | 2020 | 公开 |'
- en: '| DISCO[[130](#bib.bib130)] | Audio-Visual Crowd Counting | 1920$\times$1080
    (Full HD) | 1935 images | Head Annotations | Train & Test | 2020 | Public |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| DISCO[[130](#bib.bib130)] | 音视频人群计数 | 1920$\times$1080（全高清） | 1935张图片 | 头部标注
    | 训练和测试 | 2020 | 公开 |'
- en: '| CityScapes[[131](#bib.bib131)] | Autonomous Driving | 2048$\times$1024 |
    5K images | Pixel Mask | Train, Val & Test | 2016 | Upon Request |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| CityScapes[[131](#bib.bib131)] | 自动驾驶 | 2048$\times$1024 | 5K张图片 | 像素掩膜 |
    训练、验证和测试 | 2016 | 按需提供 |'
- en: '| SYNTHIA-RAND[[132](#bib.bib132)] | Autonomous Driving | 1280$\times$720 (HD)
    | $\sim$13K images | Pixel Mask | Train & Test | 2016 | Public |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| SYNTHIA-RAND[[132](#bib.bib132)] | 自动驾驶 | 1280$\times$720（HD） | $\sim$13K张图片
    | 像素掩膜 | 训练和测试 | 2016 | 公开 |'
- en: '| ApolloScape[[133](#bib.bib133)] | Autonomous Driving | 3384$\times$2710 |
    $\sim$113K images | Pixel Mask | Train & Test | 2020 | Upon Request |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| ApolloScape[[133](#bib.bib133)] | 自动驾驶 | 3384$\times$2710 | $\sim$113K张图像
    | 像素掩模 | 训练与测试 | 2020 | 按要求提供 |'
- en: '| Argoverse-HD[[134](#bib.bib134)] | Autonomous Driving | 1920$\times$1200
    | 89 videos | Bounding Box | Train, Val & Test | 2020 | Public |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Argoverse-HD[[134](#bib.bib134)] | 自动驾驶 | 1920$\times$1200 | 89个视频 | 边界框
    | 训练、验证与测试 | 2020 | 公开 |'
- en: '| BDD100K[[135](#bib.bib135)] | Autonomous Driving | 1280$\times$720 (HD) |
    100K videos | Bounding Box | Train, Val & Test | 2020 | Upon Request |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| BDD100K[[135](#bib.bib135)] | 自动驾驶 | 1280$\times$720（HD） | 100K个视频 | 边界框
    | 训练、验证与测试 | 2020 | 按要求提供 |'
- en: '| PASCAL-Context[[136](#bib.bib136)] | Scene Understanding | 500$\times$375
    to 500$\times$500 | 10,103 images | Pixel Mask | Train & Test | 2014 | Public
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| PASCAL-Context[[136](#bib.bib136)] | 场景理解 | 500$\times$375至500$\times$500
    | 10,103张图像 | 像素掩模 | 训练与测试 | 2014 | 公开 |'
- en: '| ADE20K[[137](#bib.bib137)] | Scene Understanding | 683$\times$512 to 2100$\times$2100
    | 27,574 images | Pixel Mask | Train & Test | 2017 | Upon Request |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| ADE20K[[137](#bib.bib137)] | 场景理解 | 683$\times$512至2100$\times$2100 | 27,574张图像
    | 像素掩模 | 训练与测试 | 2017 | 按要求提供 |'
- en: '| COCO-Stuff 10K[[138](#bib.bib138)] | Scene Understanding | $\sim$640$\times$480
    | 10K images | Pixel Mask | Train & Test | 2018 | Public |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| COCO-Stuff 10K[[138](#bib.bib138)] | 场景理解 | $\sim$640$\times$480 | 10K张图像
    | 像素掩模 | 训练与测试 | 2018 | 公开 |'
- en: '| DeepGlobe[[139](#bib.bib139)] | Land Cover Classification | 2448$\times$2448
    | 1146 images | Pixel Mask | Train, Val & Test | 2018 | Public |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| DeepGlobe[[139](#bib.bib139)] | 地表覆盖分类 | 2448$\times$2448 | 1146张图像 | 像素掩模
    | 训练、验证与测试 | 2018 | 公开 |'
- en: '| Copernicus[[140](#bib.bib140)] | Land Cover Classification | 20,160$\times$20,160
    | 94 images | Pixel Mask | None | 2015-2019 | Public |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Copernicus[[140](#bib.bib140)] | 地表覆盖分类 | 20,160$\times$20,160 | 94张图像 |
    像素掩模 | 无 | 2015-2019 | 公开 |'
- en: '| fMoW[[141](#bib.bib141)] | Aerial Image Classification | up to 16,032$\times$14,840
    | 1,047,691 images | Classes | Train, Val & Test | 2018 | Public |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| fMoW[[141](#bib.bib141)] | 空中图像分类 | 高达16,032$\times$14,840 | 1,047,691张图像
    | 类别 | 训练、验证与测试 | 2018 | 公开 |'
- en: '| KID[[142](#bib.bib142)] | Capsule Endoscopy | 360$\times$360 | $\sim$2500
    frames | Pixel Mask | None | 2017 | Public (N/A) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| KID[[142](#bib.bib142)] | 胶囊内窥镜检查 | 360$\times$360 | $\sim$2500帧 | 像素掩模 |
    无 | 2017 | 公开（无） |'
- en: '| CAD-CAP[[143](#bib.bib143)] | Capsule Endoscopy | 576$\times$576 | 25,124
    frames | Pixel Mask | Train & Test | 2020 | Upon Request |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| CAD-CAP[[143](#bib.bib143)] | 胶囊内窥镜检查 | 576$\times$576 | 25,124帧 | 像素掩模 |
    训练与测试 | 2020 | 按要求提供 |'
- en: '| CAMELYON16[[124](#bib.bib124)] | Pathology | up to 200,000$\times$100,000
    | 400 images | Pixel Mask | Train & Test | 2016 | Public |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| CAMELYON16[[124](#bib.bib124)] | 病理学 | 高达200,000$\times$100,000 | 400张图像
    | 像素掩模 | 训练与测试 | 2016 | 公开 |'
- en: '| TUPAC16[[144](#bib.bib144)] | Pathology | $\sim$50,000$\times$50,000 | 821
    images | Proliferation Score^† | Train & Test | 2016 | Public |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| TUPAC16[[144](#bib.bib144)] | 病理学 | $\sim$50,000$\times$50,000 | 821张图像 |
    增殖评分^† | 训练与测试 | 2016 | 公开 |'
- en: '| BACH Part B[[145](#bib.bib145)] | Pathology | (39,980-62,952)$\times$(27,972-44,889)
    | 40 images | Pixel Mask | Train & Test | 2019 | Public |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| BACH Part B[[145](#bib.bib145)] | 病理学 | (39,980-62,952)$\times$(27,972-44,889)
    | 40张图像 | 像素掩模 | 训练与测试 | 2019 | 公开 |'
- en: '| TCGA-BRCA[[146](#bib.bib146)] | Pathology | up to 150,000$\times$100,000
    | 709 images | Classes | None | 2020 | Public |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| TCGA-BRCA[[146](#bib.bib146)] | 病理学 | 高达150,000$\times$100,000 | 709张图像 |
    类别 | 无 | 2020 | 公开 |'
- en: '| PCa-Histo[[73](#bib.bib73)] | Pathology | (1968±216)$\times$(9392±4794) |
    266 images | Pixel Mask | Train, Val & Test | 2021 | Private |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| PCa-Histo[[73](#bib.bib73)] | 病理学 | (1968±216)$\times$(9392±4794) | 266张图像
    | 像素掩模 | 训练、验证与测试 | 2021 | 私有 |'
- en: '| INbreast[[147](#bib.bib147)] | Breast Cancer Detection | 2560$\times$3328
    to 3328$\times$4084 | 410 images | Pixel Mask | Train & Test | 2012 | Public |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| INbreast[[147](#bib.bib147)] | 乳腺癌检测 | 2560$\times$3328至3328$\times$4084
    | 410张图像 | 像素掩模 | 训练与测试 | 2012 | 公开 |'
- en: '| UA-DETRAC[[148](#bib.bib148)] | Video Object Detection | 960$\times$540 |
    140K frames | Bounding Box | Train & Test | 2015 | Public |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| UA-DETRAC[[148](#bib.bib148)] | 视频目标检测 | 960$\times$540 | 140K帧 | 边界框 | 训练与测试
    | 2015 | 公开 |'
- en: '| ImageNet-VID[[149](#bib.bib149)] | Video Object Detection | 176$\times$132
    to 1280$\times$720 (HD) | 5354 videos | Bounding Box | Train, Val & Test | 2015
    | Public |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet-VID[[149](#bib.bib149)] | 视频目标检测 | 176$\times$132至1280$\times$720（HD）
    | 5354个视频 | 边界框 | 训练、验证与测试 | 2015 | 公开 |'
- en: '| FAIR1M[[150](#bib.bib150)] | Fine-Grained Object Detection | 600$\times$600
    to 10,000$\times$10,000 | 40,000 images | Bounding Box | Train & Test | 2021 |
    Public (N/A) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| FAIR1M[[150](#bib.bib150)] | 精细化目标检测 | 600$\times$600至10,000$\times$10,000
    | 40,000张图像 | 边界框 | 训练与测试 | 2021 | 公开（无） |'
- en: '| COCO[[151](#bib.bib151)] |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| COCO[[151](#bib.bib151)] |'
- en: '&#124; Object Detection &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目标检测 &#124;'
- en: '&#124; Human Pose Estimation &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人体姿态估计 &#124;'
- en: '| $\sim$640$\times$480 | $>$200K images |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| $\sim$640$\times$480 | $>$200K 图像 |'
- en: '&#124; Pixel Mask &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 像素掩码 &#124;'
- en: '&#124; Keypoints &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关键点 &#124;'
- en: '| Train, Val & Test | 2014 | Public |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 训练，验证与测试 | 2014 | 公开 |'
- en: '| ^§A frame is a single image in a sequence representing a video |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| ^§帧是表示视频序列中的单个图像 |'
- en: '| ^∗The locaion for the center of each human head in the image is specified
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| ^∗图像中每个人头部的中心位置已指定 |'
- en: '| ^†A measure of the number of cells in a tumor that are dividing |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| ^†肿瘤中分裂细胞的数量测量 |'
- en: '| ^‡[https://github.com/supervisely-ecosystem/persons](https://github.com/supervisely-ecosystem/persons)
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| ^‡[https://github.com/supervisely-ecosystem/persons](https://github.com/supervisely-ecosystem/persons)
    |'
- en: V Discussion and Open Issues
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 讨论与开放问题
- en: 'Each of the approaches introduced in Section [III](#S3 "III Methods for Efficient
    Processing of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution
    Deep Learning: A Survey") has its advantages and disadvantages and is useful in
    certain situations. NUD (Section [III-A](#S3.SS1 "III-A Non-Uniform Downsampling
    ‣ III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey")) works well in cases where
    the salient area is small compared to the entire image, and thus, it is possible
    to sample many pixels from such areas. This requirement is satisfied in gaze estimation
    or object detection problems. Our conjecture is that it would also work well in
    problems such as hand gesture detection and non-cropped facial expression recognition,
    although these tasks are not yet explored in the literature in combination with
    NUD. However, when the salient area is large, for instance densely populated scenes
    in visual crowd counting or a scene fully covered with objects in object detection,
    the quality gain obtained by sampling from salient areas will be negligible, and
    the result of NUD will be similar that of uniform downsampling [[77](#bib.bib77)].'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '每种在第[III](#S3 "III Methods for Efficient Processing of High-Resolution Inputs
    with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey")节中介绍的方法都有其优缺点，并在某些情况下非常有用。NUD（第[III-A](#S3.SS1
    "III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing of High-Resolution
    Inputs with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey")节）在显著区域相对于整个图像较小时表现良好，因此可以从这些区域中采样许多像素。这一要求在注视估计或目标检测问题中得到了满足。我们的猜测是，它在手势检测和未裁剪的面部表情识别等问题中也会表现良好，尽管这些任务尚未在文献中与NUD结合进行探索。然而，当显著区域较大时，例如在视觉人群计数中的密集场景或在目标检测中的完全覆盖物体的场景，通过从显著区域采样获得的质量提升将会微不足道，NUD的结果将类似于均匀下采样[[77](#bib.bib77)]。'
- en: 'Similarly, SZS methods (Section [III-B](#S3.SS2 "III-B Selective Zooming and
    Skipping ‣ III Methods for Efficient Processing of High-Resolution Inputs with
    Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey")) require the
    salient area to be small, otherwise they zoom everywhere and save little time
    and computation. This also means that the effectiveness of NUD and SZS methods
    may vary based on the specific input. For instance, the more people there are
    in an image processed for crowd counting, or the more tumors there are in cancer
    detection, the less efficient such methods will be, unless there are specific
    safeguards that prevent them from performing an enormous number of computations,
    such as GigaDet [[83](#bib.bib83)] which processes at most $K$ patch candidates.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '类似地，SZS方法（第[III-B](#S3.SS2 "III-B Selective Zooming and Skipping ‣ III Methods
    for Efficient Processing of High-Resolution Inputs with Deep Learning ‣ Efficient
    High-Resolution Deep Learning: A Survey")节）要求显著区域较小，否则它们会在所有地方进行缩放，从而节省的时间和计算量很少。这也意味着NUD和SZS方法的有效性可能会根据特定输入有所不同。例如，在处理人群计数的图像中，人数越多，或在癌症检测中肿瘤越多，这些方法的效率就会降低，除非有特定的保护措施防止它们进行大量计算，如GigaDet
    [[83](#bib.bib83)]，该方法最多处理$K$个补丁候选。'
- en: 'Furthermore, NUD methods are not effective when the resulting resolution is
    extremely smaller compared to the input resolution, for instance, when gigapixel
    inputs need to be resized down to HD, as this would result in highly distorted
    images which makes it difficult for the task DNN to perform well. Even when the
    gap between the two resolutions is not extremely large, NUD can lead to high distortions
    in some cases, for instance, it may completely distort and change the shape of
    the edges of a gastrointestinal lesion, making it difficult for the task network
    to detect useful features. This may reduce accuracy despite the fact that more
    pixels are sampled from salient areas. As explained in Section [III-A](#S3.SS1
    "III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing of High-Resolution
    Inputs with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey"),
    some methods try to mitigate the distortion by using structured grids. However,
    this may limit the benefits obtained by NUD.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，当结果分辨率相较于输入分辨率极其小，例如，当 gigapixel 输入需要缩小到 HD 时，NUD 方法效果不佳，因为这会导致图像严重失真，使得任务
    DNN 难以良好执行。即使两者之间的分辨率差距不是极大，NUD 在某些情况下也可能导致严重失真，例如，可能完全扭曲并改变胃肠道病变的边缘形状，使任务网络难以检测有用特征。尽管从显著区域采样了更多像素，但这可能会降低准确性。如第
    [III-A](#S3.SS1 "III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing
    of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution Deep
    Learning: A Survey") 节所解释，某些方法试图通过使用结构化网格来减轻失真。然而，这可能会限制 NUD 带来的好处。'
- en: 'In addition, since NUD is enlarging some parts of the image compared to uniform
    downsampling, some areas of the resulting image will be smaller than they would
    be with uniform downsampling. Thus, if the saliency map is not of high quality,
    unimportant areas will be enlarged and the ones important for the final task will
    shrink, resulting in accuracy loss. This is directly at odds with the requirement
    that the saliency detection method should be low-overhead, creating another trade-off
    that needs to be carefully balanced. Moreover, as explained in Section [III-A](#S3.SS1
    "III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing of High-Resolution
    Inputs with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey"),
    some variations of NUD require an external supervision signal or regularization
    term to train the saliency detection network, which can be difficult to design.
    In NUD or SZS methods that detect saliency in videos based on the results obtained
    from previous frames, such as SALISA [[77](#bib.bib77)] and REMIX [[87](#bib.bib87)],
    when the difference between subsequent frames is high, the method needs to be
    reset to processing the entire high-resolution image. When this occurs frequently,
    the obtained benefits are diminished.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，由于 NUD 在图像的某些部分进行放大，相较于均匀下采样，结果图像中的某些区域将比均匀下采样时小。因此，如果显著性图的质量不高，不重要的区域将被放大，而对于最终任务重要的区域将缩小，从而导致准确性下降。这与显著性检测方法应低开销的要求直接矛盾，创造了需要仔细平衡的另一个权衡。此外，如第
    [III-A](#S3.SS1 "III-A Non-Uniform Downsampling ‣ III Methods for Efficient Processing
    of High-Resolution Inputs with Deep Learning ‣ Efficient High-Resolution Deep
    Learning: A Survey") 节所解释，某些 NUD 的变体需要外部监督信号或正则化项来训练显著性检测网络，这可能很难设计。在基于前帧结果检测视频中的显著性如
    SALISA [[77](#bib.bib77)] 和 REMIX [[87](#bib.bib87)] 的 NUD 或 SZS 方法中，当后续帧之间的差异较大时，该方法需要重置为处理整个高分辨率图像。当这种情况频繁发生时，所获得的好处会减少。'
- en: 'As mentioned in Section [III-C](#S3.SS3 "III-C Lightweight Scanner Networks
    ‣ III Methods for Efficient Processing of High-Resolution Inputs with Deep Learning
    ‣ Efficient High-Resolution Deep Learning: A Survey"), LSNs need to designed,
    trained and well optimized for the specific problem at hand, which is not an easy
    task. Furthermore, since LSNs produce an output for each scanned area of the input,
    they are suitable for tasks where the output has the form of a map, such as dense
    classification or dense regression problems. Moreover, the scanning nature of
    LSNs means that all areas of the image are treated similarly, therefore, they
    are better suited for situations where there is no perspective and objects of
    the same type have the same size regardless of their location, such as WSIs and
    remote sensing, as opposed to surveillance and crowd counting where people close
    to the camera are larger than people far away.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如[III-C](#S3.SS3 "III-C 轻量级扫描网络 ‣ III 高分辨率输入的高效处理方法 ‣ 高效高分辨率深度学习：调查")部分所述，LSN需要针对具体问题进行设计、训练和优化，这不是一项容易的任务。此外，由于LSN为输入的每个扫描区域生成输出，因此它们适合于输出形式为地图的任务，例如密集分类或密集回归问题。此外，LSN的扫描特性意味着图像的所有区域被相似地处理，因此它们更适合于没有透视的情况，其中同一类型的物体大小相同，无论其位置如何，例如WSI和遥感，相较于监控和人群计数，其中靠近摄像头的人比远离摄像头的人更大。
- en: 'Since TOIC methods extract representations that are both compressed and suitable
    for the task at hand, they often need to be tailored to the specific problem,
    which requires high domain knowledge. Both Slide Graph [[96](#bib.bib96)] and
    MCAT [[104](#bib.bib104)] presented in Section [III-D](#S3.SS4 "III-D Task-Oriented
    Input Compression ‣ III Methods for Efficient Processing of High-Resolution Inputs
    with Deep Learning ‣ Efficient High-Resolution Deep Learning: A Survey") are based
    on domain knowledge about cellular structure of tissues and biological function
    of genes, respectively. Almost all frequency-domain DNNs try to preserve the architecture
    of the CNNs they are based on. However, since the interpretation of features in
    frequency-domain is different, and they have certain properties such as being
    non-negative, it might be better to customize the architectural elements for the
    frequency domain, as CS-Fnet [[152](#bib.bib152)] does.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TOIC方法提取的表示既压缩又适合具体任务，因此通常需要针对特定问题进行定制，这需要高度的领域知识。第[III-D](#S3.SS4 "III-D
    任务导向输入压缩 ‣ III 高分辨率输入的高效处理方法 ‣ 高效高分辨率深度学习：调查")部分介绍的Slide Graph [[96](#bib.bib96)]和MCAT
    [[104](#bib.bib104)]分别基于细胞组织的结构和基因的生物功能领域知识。几乎所有频域DNN都试图保留其所基于的CNN的架构。然而，由于频域中特征的解释不同，并且它们具有诸如非负性等特性，可能更好地为频域定制架构元素，如CS-Fnet
    [[152](#bib.bib152)]所做的那样。
- en: Most high-resolution Vision Transformer methods try to reduce the quadratic
    cost of self-attention to linear, and then compensate the accuracy loss by learning
    data transformations using convolutions. To keep the overhead of convolutions
    low, depth-wise convolution is typically used. Additionally, most high-resolution
    ViTs utilize a multi-scale architecture in order to capture features of various
    scales. High-resolution ViTs are more general purpose than other high-resolution
    deep learning methods and are often used for a large variety of tasks.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数高分辨率视觉变换器方法尝试将自注意力的二次成本降低到线性成本，然后通过使用卷积来弥补精度损失。为了保持卷积的开销较低，通常使用深度卷积。此外，大多数高分辨率视觉变换器利用多尺度架构以捕获各种尺度的特征。高分辨率视觉变换器比其他高分辨率深度学习方法更具通用性，通常用于各种任务。
- en: VI Conclusion and Outlook
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论与展望
- en: Processing high-resolution images and videos with deep learning is crucial in
    various domains of science and technology. However, few methods exist that address
    the computational challenges. Among existing methods, the trend of designing solutions
    specifically for the problem at hand is clearly visible. This can be an issue
    in tasks for which high-resolution datasets are not available. Similar to model
    compression approaches, both modifying existing methods and designing an efficient
    high-resolution method from scratch are viable approaches.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习处理高分辨率图像和视频在科学和技术的各个领域都至关重要。然而，解决计算挑战的方法仍然很少。在现有方法中，为特定问题设计解决方案的趋势非常明显。这在高分辨率数据集不可用的任务中可能会成为一个问题。类似于模型压缩方法，既可以修改现有方法，也可以从头开始设计一个高效的高分辨率方法。
- en: Efficient high-resolution deep learning is in its infancy and there is a lot
    of room for improvement. For instance, a number of attention-free MLP-based methods
    have been recently proposed as lightweight alternatives for Transformers [[153](#bib.bib153)],
    which try to mimic the global receptive field of Transformers without the self-attention
    mechanism. Exploiting such architectures for efficient processing of high-resolution
    inputs would be an interesting research direction. Furthermore, the multimodal
    co-attention in MCAT [[104](#bib.bib104)] can be applied to many other multimodal
    tasks, especially the ones with audio, vision and language modalities. Moreover,
    frequency-domain representations can be explored as inputs to ViTs, which can
    lead to more efficiency compared to frequency-domain CNNs. For instance, ViTs
    can take separate patches from DCT-Cb, DCT-Cr and DCT-Y components, bypassing
    the need to upsample DCT-Cb and DCT-Cr to match the dimensions of DCT-Y.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率深度学习尚处于起步阶段，仍有许多改进空间。例如，最近提出了一些无注意力机制的基于MLP的方法作为Transformer的轻量替代方案[[153](#bib.bib153)]，这些方法试图模拟Transformer的全局感受野，而不使用自注意力机制。利用这些架构进行高分辨率输入的高效处理将是一个有趣的研究方向。此外，MCAT中的多模态共同注意机制[[104](#bib.bib104)]可以应用于许多其他多模态任务，特别是涉及音频、视觉和语言模态的任务。此外，可以探索将频域表示作为ViTs的输入，这可能比频域CNN更高效。例如，ViTs可以从DCT-Cb、DCT-Cr和DCT-Y组件中提取单独的图块，从而绕过需要将DCT-Cb和DCT-Cr上采样以匹配DCT-Y的维度。
- en: The combination of efficient high-resolution deep learning with other efficient
    deep learning methods, such as model compression [[154](#bib.bib154)], dynamic
    inference [[155](#bib.bib155)], collaborative inference [[156](#bib.bib156)] and
    continual inference [[157](#bib.bib157)], is an unexplored area of research. For
    instance, if the saliency detection network is a lightweight version of the task
    network, NUD can be combined with early exiting, where the output of the saliency
    detection network would be a fast, but less accurate, early result. This is simple
    to implement in dense regression problems such as depth estimation and crowd counting,
    where the output of the task can be interpreted as a form of saliency.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的高分辨率深度学习与其他高效深度学习方法的结合，如模型压缩[[154](#bib.bib154)]、动态推断[[155](#bib.bib155)]、协作推断[[156](#bib.bib156)]和持续推断[[157](#bib.bib157)]，是一个尚未探索的研究领域。例如，如果显著性检测网络是任务网络的轻量级版本，NUD可以与早期退出结合使用，其中显著性检测网络的输出将是一个快速但较不准确的早期结果。这在密集回归问题中，如深度估计和人群计数中，简单易行，其中任务的输出可以被解释为一种显著性形式。
- en: Moreover, with the adoption of edge and cloud computing, transmission of high-resolution
    inputs to servers for processing is a real challenge. As a solution, efficient
    high-resolution deep learning methods can be combined with edge computing paradigms.
    For instance, the downsampled images in NUD and compressed representation in TOIC
    can be transmitted instead of the original inputs. This would be a form of split
    computing (also known as collaborative intelligence) [[158](#bib.bib158), [159](#bib.bib159)],
    where the initial portion of computation is performed on a resource-constrained
    end-device, and the compact intermediate representation is then transmitted to
    a server where the rest of the computation is carried out. A study using this
    idea for high-resolution images captured by drones is reported in [[160](#bib.bib160)].
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着边缘计算和云计算的采用，将高分辨率输入传输到服务器进行处理成为一个实际挑战。作为解决方案，可以将高效的高分辨率深度学习方法与边缘计算范式结合。例如，可以传输NUD中的下采样图像和TOIC中的压缩表示，而不是原始输入。这将是一种分割计算（也称为协作智能）的形式[[158](#bib.bib158),
    [159](#bib.bib159)]，其中初始计算部分在资源受限的终端设备上进行，然后将紧凑的中间表示传输到服务器，在服务器上进行剩余的计算。使用这一思想处理无人机捕获的高分辨率图像的研究报告见[[160](#bib.bib160)]。
- en: References
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] E. Thomson, M. Harfouche *et al.*, “Gigapixel behavioral and neural activity
    imaging with a novel multi-camera array microscope,” *bioRxiv*, 2021.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] E. Thomson, M. Harfouche *等人*, “使用新型多摄像头阵列显微镜进行吉格像素行为和神经活动成像，” *bioRxiv*，2021年。'
- en: '[2] X. Yuan, L. Fang *et al.*, “Multiscale gigapixel video: A cross resolution
    image matching and warping approach,” in *IEEE International Conference on Computational
    Photography*, 2017, pp. 1–9.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] X. Yuan, L. Fang *等人*, “多尺度吉格像素视频：一种跨分辨率图像匹配和变形方法，” 发表在 *IEEE国际计算摄影会议*，2017年，页码1–9。'
- en: '[3] R. Sargent, C. Bartley *et al.*, “Timelapse gigapan: Capturing, sharing,
    and exploring timelapse gigapixel imagery,” in *Fine International Conference
    on Gigapixel Imaging for Science*, 2010.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] R. Sargent, C. Bartley *等人*, “时光流转的吉格像素：捕捉、分享和探索时光流转的吉格像素图像，” 发表在 *国际吉格像素科学影像会议*，2010年。'
- en: '[4] N. Farahani, A. V. Parwani *et al.*, “Whole slide imaging in pathology:
    advantages, limitations, and emerging perspectives,” *Pathology and Laboratory
    Medicine International*, vol. 7, no. 23-33, p. 4321, 2015.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] N. Farahani, A. V. Parwani *等人*, “病理学中的全切片成像：优势、局限性和新兴视角，” *病理学与实验室医学国际*，第7卷，第23-33期，页码4321，2015年。'
- en: '[5] J. Chen, L. Wu *et al.*, “Deep learning-based model for detecting 2019
    novel coronavirus pneumonia on high-resolution computed tomography,” *Scientific
    Reports*, vol. 10, no. 1, p. 19196, 2020.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Chen, L. Wu *等人*, “基于深度学习的模型用于检测2019新型冠状病毒肺炎的高分辨率计算机断层扫描，” *科学报告*，第10卷，第1期，页码19196，2020年。'
- en: '[6] A. Dosovitskiy, L. Beyer *et al.*, “An image is worth 16x16 words: Transformers
    for image recognition at scale,” in *International Conference on Learning Representations*,
    2021\. [Online]. Available: [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Dosovitskiy, L. Beyer *等人*, “一张图像胜过16x16个词：用于大规模图像识别的变换器，” 发表在 *国际学习表征会议*，2021年。
    [在线]. 可用: [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy)'
- en: '[7] G. Gao, J. Gao *et al.*, “Cnn-based density estimation and crowd counting:
    A survey,” *arXiv preprint arXiv:2003.12783*, 2020.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] G. Gao, J. Gao *等人*, “基于CNN的密度估计和人群计数：综述，” *arXiv预印本 arXiv:2003.12783*，2020年。'
- en: '[8] J. van der Laak, G. Litjens, and F. Ciompi, “Deep learning in histopathology:
    the path to the clinic,” *Nature Medicine*, vol. 27, no. 5, pp. 775–784, 2021.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. van der Laak, G. Litjens, 和 F. Ciompi, “深度学习在组织病理学中的应用：通向临床的道路，” *自然医学*，第27卷，第5期，页码775–784，2021年。'
- en: '[9] A. Van Etten, “You only look twice: Rapid multi-scale object detection
    in satellite imagery,” *arXiv preprint arXiv:1805.09512*, 2018.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Van Etten, “你只看两次：卫星图像中的快速多尺度目标检测，” *arXiv预印本 arXiv:1805.09512*，2018年。'
- en: '[10] Y. Lecun, L. Bottou *et al.*, “Gradient-based learning applied to document
    recognition,” *Proceedings of the IEEE*, vol. 86, no. 11, pp. 2278–2324, 1998.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Lecun, L. Bottou *等人*, “基于梯度的学习应用于文档识别，” *IEEE汇刊*，第86卷，第11期，页码2278–2324，1998年。'
- en: '[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Proceedings of the 25th International
    Conference on Neural Information Processing Systems - Volume 1*, ser. NIPS’12.   Red
    Hook, NY, USA: Curran Associates Inc., 2012, p. 1097–1105.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行Imagenet分类，”
    发表在 *第25届国际神经信息处理系统会议 - 卷1*，系列NIPS’12。 纽约，USA：Curran Associates Inc.，2012年，页码1097–1105。'
- en: '[12] B. Ramachandra, M. J. Jones, and R. R. Vatsavai, “A survey of single-scene
    video anomaly detection,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 44, no. 5, pp. 2293–2312, 2022.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] B. Ramachandra, M. J. Jones, 和 R. R. Vatsavai, “单场景视频异常检测的综述，” *IEEE模式分析与机器智能汇刊*，第44卷，第5期，页码2293–2312，2022年。'
- en: '[13] X. Wang, X. Zhang *et al.*, “Panda: A gigapixel-level human-centric video
    dataset,” in *IEEE/CVF conference on computer vision and pattern recognition*,
    2020, pp. 3268–3278.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] X. Wang, X. Zhang *等人*, “Panda：一个千兆像素级人本视频数据集，” *IEEE/CVF计算机视觉与模式识别会议*，2020年，页码3268–3278。'
- en: '[14] Q. Song, C. Wang *et al.*, “To choose or to fuse? scale selection for
    crowd counting,” in *AAAI Conference on Artificial Intelligence*, vol. 35, no. 3,
    2021, pp. 2576–2583.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Q. Song, C. Wang *等人*, “选择还是融合？人群计数的尺度选择，” *AAAI人工智能会议*，第35卷，第3期，2021年，页码2576–2583。'
- en: '[15] Y. Tay, M. Dehghani *et al.*, “Efficient transformers: A survey,” *arXiv
    preprint arXiv:2009.06732*, 2020.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Tay, M. Dehghani *等人*, “高效的变换器：综述，” *arXiv预印本arXiv:2009.06732*，2020年。'
- en: '[16] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolutional
    neural networks,” in *International Conference on Machine Learning*, 2019, pp.
    6105–6114.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Tan 和 Q. Le, “Efficientnet：重新思考卷积神经网络的模型缩放，” *国际机器学习会议*，2019年，页码6105–6114。'
- en: '[17] M. Shoeybi, M. Patwary *et al.*, “Megatron-lm: Training multi-billion
    parameter language models using model parallelism,” *arXiv preprint arXiv:1909.08053*,
    2019.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] M. Shoeybi, M. Patwary *等人*, “Megatron-lm：使用模型并行训练多亿参数语言模型，” *arXiv预印本arXiv:1909.08053*，2019年。'
- en: '[18] L. Weng and G. Brockman. (2022) Techniques for training large neural networks.
    [Online]. Available: [https://openai.com/blog/techniques-for-training-large-neural-networks/](https://openai.com/blog/techniques-for-training-large-neural-networks/)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] L. Weng 和 G. Brockman. (2022) 大型神经网络训练技术。[在线]。可用：[https://openai.com/blog/techniques-for-training-large-neural-networks/](https://openai.com/blog/techniques-for-training-large-neural-networks/)'
- en: '[19] J. Du, X. Zhu *et al.*, “Model parallelism optimization for distributed
    inference via decoupled cnn structure,” *IEEE Transactions on Parallel and Distributed
    Systems*, vol. 32, no. 7, pp. 1665–1676, 2020.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Du, X. Zhu *等人*, “通过解耦CNN结构优化分布式推理中的模型并行，” *IEEE并行与分布式系统汇刊*，第32卷，第7期，页码1665–1676，2020年。'
- en: '[20] Y. Zhang, D. Zhou *et al.*, “Single-image crowd counting via multi-column
    convolutional neural network,” in *IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 589–597.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Zhang, D. Zhou *等人*, “通过多列卷积神经网络进行单图像人群计数，” *IEEE计算机视觉与模式识别会议*，2016年，页码589–597。'
- en: '[21] D. J. Brady, D. L. Marks *et al.*, “Petapixel photography and the limits
    of camera information capacity,” in *Computational Imaging XI*, 2013, pp. 87–93.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] D. J. Brady, D. L. Marks *等人*, “Petapixel摄影与相机信息容量的极限，” *计算成像XI*，2013年，页码87–93。'
- en: '[22] W. Lu, S. Graham *et al.*, “Capturing cellular topology in multi-gigapixel
    pathology images,” in *IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops*, 2020, pp. 260–261.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] W. Lu, S. Graham *等人*, “在多千兆像素病理图像中捕捉细胞拓扑，” *IEEE/CVF计算机视觉与模式识别会议工作坊*，2020年，页码260–261。'
- en: '[23] A. Recasens, P. Kellnhofer *et al.*, “Learning to zoom: a saliency-based
    sampling layer for neural networks,” in *European Conference on Computer Vision*,
    2018, pp. 51–66.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Recasens, P. Kellnhofer *等人*, “学习缩放：一种基于显著性的神经网络采样层，” *欧洲计算机视觉会议*，2018年，页码51–66。'
- en: '[24] G. Cheng, X. Xie *et al.*, “Remote sensing image scene classification
    meets deep learning: Challenges, methods, benchmarks, and opportunities,” *IEEE
    Journal of Selected Topics in Applied Earth Observations and Remote Sensing*,
    vol. 13, pp. 3735–3756, 2020.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] G. Cheng, X. Xie *等人*, “遥感图像场景分类与深度学习的碰撞：挑战、方法、基准和机会，” *IEEE应用地球观测与遥感期刊*，第13卷，页码3735–3756，2020年。'
- en: '[25] N. Dimitriou, O. Arandjelović, and P. D. Caie, “Deep learning for whole
    slide image analysis: An overview,” *Frontiers in Medicine*, vol. 6, 2019. [Online].
    Available: [https://www.frontiersin.org/articles/10.3389/fmed.2019.00264](https://www.frontiersin.org/articles/10.3389/fmed.2019.00264)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] N. Dimitriou, O. Arandjelović, 和 P. D. Caie, “全图像分析的深度学习：概述，” *医学前沿*，第6卷，2019年。[在线]。可用：[https://www.frontiersin.org/articles/10.3389/fmed.2019.00264](https://www.frontiersin.org/articles/10.3389/fmed.2019.00264)'
- en: '[26] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
    in *International Conference on Learning Representations*, 2019.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] I. Loshchilov 和 F. Hutter, “解耦权重衰减正则化，” *国际学习表示会议*，2019年。'
- en: '[27] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
    in *International Conference on Learning Representations*, 2015.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] D. P. Kingma 和 J. Ba, “Adam：一种随机优化方法”，发表于 *国际学习表征会议*，2015年。'
- en: '[28] C. L. Srinidhi, O. Ciga, and A. L. Martel, “Deep neural network models
    for computational histopathology: A survey,” *Medical Image Analysis*, vol. 67,
    p. 101813, 2021.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] C. L. Srinidhi, O. Ciga 和 A. L. Martel, “计算组织病理学的深度神经网络模型：综述”，*医学影像分析*，第67卷，文章编号101813，2021年。'
- en: '[29] J. D. Schuijf, J. A. Lima *et al.*, “Ct imaging with ultra-high-resolution:
    Opportunities for cardiovascular imaging in clinical practice,” *Journal of Cardiovascular
    Computed Tomography*, vol. 16, no. 5, pp. 388–396, 2022\. [Online]. Available:
    [https://www.sciencedirect.com/science/article/pii/S1934592522000235](https://www.sciencedirect.com/science/article/pii/S1934592522000235)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. D. Schuijf, J. A. Lima *等人*, “超高分辨率CT成像：临床实践中心血管成像的机遇”，*心血管计算机断层扫描杂志*，第16卷，第5期，第388–396页，2022年。
    [在线]. 可用： [https://www.sciencedirect.com/science/article/pii/S1934592522000235](https://www.sciencedirect.com/science/article/pii/S1934592522000235)'
- en: '[30] X. Zhang, L. Han *et al.*, “How well do deep learning-based methods for
    land cover classification and object detection perform on high resolution remote
    sensing imagery?” *Remote Sensing*, vol. 12, no. 3, 2020.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] X. Zhang, L. Han *等人*, “基于深度学习的方法在高分辨率遥感影像中的土地覆盖分类和物体检测表现如何？” *遥感*，第12卷，第3期，2020年。'
- en: '[31] H. Jiang, M. Peng *et al.*, “A survey on deep learning-based change detection
    from high-resolution remote sensing images,” *Remote Sensing*, vol. 14, no. 7,
    2022.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] H. Jiang, M. Peng *等人*, “基于深度学习的高分辨率遥感图像变化检测综述”，*遥感*，第14卷，第7期，2022年。'
- en: '[32] Z. Cai, Q. Fan *et al.*, “A unified multi-scale deep convolutional neural
    network for fast object detection,” in *Computer Vision – ECCV 2016*, B. Leibe,
    J. Matas *et al.*, Eds.   Cham: Springer International Publishing, 2016, pp. 354–370.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Z. Cai, Q. Fan *等人*, “一种统一的多尺度深度卷积神经网络用于快速物体检测”，发表于 *计算机视觉 – ECCV 2016*，B.
    Leibe, J. Matas *等人* 编，   Cham: Springer 国际出版，2016年，第354–370页。'
- en: '[33] J. Wang, K. Sun *et al.*, “Deep high-resolution representation learning
    for visual recognition,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 43, no. 10, pp. 3349–3364, 2021.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Wang, K. Sun *等人*, “用于视觉识别的深度高分辨率表示学习”，*IEEE 模式分析与机器智能汇刊*，第43卷，第10期，第3349–3364页，2021年。'
- en: '[34] H. Zhao, X. Qi *et al.*, “Icnet for real-time semantic segmentation on
    high-resolution images,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, September 2018.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] H. Zhao, X. Qi *等人*, “Icnet：用于高分辨率图像的实时语义分割”，发表于 *欧洲计算机视觉会议（ECCV）*，2018年9月。'
- en: '[35] U. Sajid, H. Sajid *et al.*, “Zoomcount: A zooming mechanism for crowd
    counting in static images,” *IEEE Transactions on Circuits and Systems for Video
    Technology*, vol. 30, no. 10, pp. 3499–3512, 2020.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] U. Sajid, H. Sajid *等人*, “Zoomcount：一种用于静态图像中的人群计数的缩放机制”，*IEEE 视讯技术电路与系统汇刊*，第30卷，第10期，第3499–3512页，2020年。'
- en: '[36] J. T. Zhou, L. Zhang *et al.*, “Locality-aware crowd counting,” *IEEE
    Transactions on Pattern Analysis and Machine Intelligence*, vol. 44, no. 7, pp.
    3602–3613, 2021.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. T. Zhou, L. Zhang *等人*, “局部性感知的人群计数”，*IEEE 模式分析与机器智能汇刊*，第44卷，第7期，第3602–3613页，2021年。'
- en: '[37] C. Liu, X. Weng, and Y. Mu, “Recurrent attentive zooming for joint crowd
    counting and precise localization,” in *IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2019.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] C. Liu, X. Weng 和 Y. Mu, “递归关注缩放用于联合人群计数和精确定位”，发表于 *IEEE/CVF 计算机视觉与模式识别会议*，2019年。'
- en: '[38] C. Xu, K. Qiu *et al.*, “Learn to scale: Generating multipolar normalized
    density maps for crowd counting,” in *IEEE/CVF International Conference on Computer
    Vision*, 2019.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] C. Xu, K. Qiu *等人*, “学习缩放：生成用于人群计数的多极化标准化密度图”，发表于 *IEEE/CVF 国际计算机视觉会议*，2019年。'
- en: '[39] D. J. Ho, D. V. Yarlagadda *et al.*, “Deep multi-magnification networks
    for multi-class breast cancer image segmentation,” *Computerized Medical Imaging
    and Graphics*, vol. 88, p. 101866, 2021.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] D. J. Ho, D. V. Yarlagadda *等人*, “深度多放大网络用于多类别乳腺癌图像分割”，*计算机化医学影像与图形*，第88卷，文章编号101866，2021年。'
- en: '[40] K. Wang, X. Zhang, and S. Huang, “KGZNet: knowledge-guided deep zoom neural
    networks for thoracic disease classification,” in *IEEE International Conference
    on Bioinformatics and Biomedicine (BIBM)*, 2019, pp. 1396–1401.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] K. Wang, X. Zhang 和 S. Huang, “KGZNet：用于胸部疾病分类的知识引导深度缩放神经网络”，发表于 *IEEE
    生物信息学与生物医学国际会议（BIBM）*，2019年，第1396–1401页。'
- en: '[41] S. Madec, X. Jin *et al.*, “Ear density estimation from high resolution
    rgb imagery using deep learning technique,” *Agricultural and Forest Meteorology*,
    vol. 264, pp. 225–234, 2019.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Madec, X. Jin *等*，“使用深度学习技术从高分辨率RGB图像中估算耳密度，” *农业与森林气象*，第264卷，页225–234，2019年。'
- en: '[42] Y. Xu, Z. Xie *et al.*, “Road extraction from high-resolution remote sensing
    imagery using deep learning,” *Remote Sensing*, vol. 10, no. 9, p. 1461, 2018.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Xu, Z. Xie *等*，“使用深度学习从高分辨率遥感图像中提取道路，” *遥感*，第10卷，第9期，页1461，2018年。'
- en: '[43] Y. Liu, K. Gadepalli *et al.*, “Detecting cancer metastases on gigapixel
    pathology images,” *arXiv preprint arXiv:1703.02442*, 2017.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Liu, K. Gadepalli *等*，“在千兆像素病理图像上检测癌症转移，” *arXiv 预印本 arXiv:1703.02442*，2017。'
- en: '[44] L. Wang, L. Ding *et al.*, “Automated identification of malignancy in
    whole-slide pathological images: identification of eyelid malignant melanoma in
    gigapixel pathological slides using deep learning,” *British Journal of Ophthalmology*,
    vol. 104, no. 3, pp. 318–323, 2020.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] L. Wang, L. Ding *等*，“全幻灯片病理图像中恶性肿瘤的自动识别：使用深度学习在千兆像素病理幻灯片中识别眼睑恶性黑色素瘤，”
    *英国眼科杂志*，第104卷，第3期，页318–323，2020年。'
- en: '[45] C. Xie, H. Muhammad *et al.*, “Beyond classification: Whole slide tissue
    histopathology analysis by end-to-end part learning,” in *Conference on Medical
    Imaging with Deep Learning*, 2020, pp. 843–856.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C. Xie, H. Muhammad *等*，“超越分类：通过端到端部分学习的全幻灯片组织病理分析，”在 *深度学习医学成像会议*，2020年，页843–856。'
- en: '[46] S. Cheng, S. Liu *et al.*, “Robust whole slide image analysis for cervical
    cancer screening using deep learning,” *Nature Communications*, vol. 12, no. 1,
    p. 5639, 2021.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] S. Cheng, S. Liu *等*，“使用深度学习进行宫颈癌筛查的鲁棒全幻灯片图像分析，” *自然通讯*，第12卷，第1期，页5639，2021年。'
- en: '[47] N. Z. Tsaku, S. C. Kosaraju *et al.*, “Texture-based deep learning for
    effective histopathological cancer image classification,” in *IEEE International
    Conference on Bioinformatics and Biomedicine (BIBM)*, 2019, pp. 973–977.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] N. Z. Tsaku, S. C. Kosaraju *等*，“基于纹理的深度学习用于有效的组织病理癌症图像分类，”在 *IEEE 国际生物信息学与生物医学会议（BIBM）*，2019年，页973–977。'
- en: '[48] H. Lin, H. Chen *et al.*, “Fast scannet: Fast and dense analysis of multi-gigapixel
    whole-slide images for cancer metastasis detection,” *IEEE Transactions on Medical
    Imaging*, vol. 38, no. 8, pp. 1948–1958, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] H. Lin, H. Chen *等*，“快速扫描：癌症转移检测的多千兆像素全幻灯片图像的快速和密集分析，” *IEEE 医学成像学报*，第38卷，第8期，页1948–1958，2019年。'
- en: '[49] Z. Lai, C. Wang *et al.*, “Joint semi-supervised and active learning for
    segmentation of gigapixel pathology images with cost-effective labeling,” in *IEEE/CVF
    International Conference on Computer Vision Workshops*, October 2021, pp. 591–600.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Z. Lai, C. Wang *等*，“具有成本效益标记的千兆像素病理图像分割的联合半监督和主动学习，”在 *IEEE/CVF 国际计算机视觉研讨会*，2021年10月，页591–600。'
- en: '[50] S. Javed, A. Mahmood *et al.*, “Deep multiresolution cellular communities
    for semantic segmentation of multi-gigapixel histology images,” in *IEEE/CVF International
    Conference on Computer Vision Workshops*, 2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Javed, A. Mahmood *等*，“用于多千兆像素组织学图像语义分割的深度多分辨率细胞社区，”在 *IEEE/CVF 国际计算机视觉研讨会*，2019年。'
- en: '[51] S. Yang, L. Jiang *et al.*, “Deep learning for detecting corona virus
    disease 2019 (covid-19) on high-resolution computed tomography: a pilot study,”
    *Annals of Translational Medicine*, vol. 8, no. 7, pp. 450–450, 2020.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] S. Yang, L. Jiang *等*，“用于检测2019冠状病毒病（COVID-19）的深度学习高分辨率计算机断层扫描：一项初步研究，”
    *翻译医学年鉴*，第8卷，第7期，页450–450，2020年。'
- en: '[52] M. Akagi, Y. Nakamura *et al.*, “Deep learning reconstruction improves
    image quality of abdominal ultra-high-resolution ct,” *European Radiology*, vol. 29,
    no. 11, pp. 6163–6171, 2019.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. Akagi, Y. Nakamura *等*，“深度学习重建改善腹部超高分辨率CT图像质量，” *欧洲放射学*，第29卷，第11期，页6163–6171，2019年。'
- en: '[53] A. Khadangi, T. Boudier, and V. Rajagopal, “EM-stellar: benchmarking deep
    learning for electron microscopy image segmentation,” *Bioinformatics*, vol. 37,
    no. 1, pp. 97–106, 2021.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] A. Khadangi, T. Boudier 和 V. Rajagopal，“EM-stellar：基准测试电子显微镜图像分割的深度学习，”
    *生物信息学*，第37卷，第1期，页97–106，2021年。'
- en: '[54] Y. Fukushima, Y. Fushimi *et al.*, “Evaluation of moyamoya disease in
    ct angiography using ultra-high-resolution computed tomography: Application of
    deep learning reconstruction,” *European Journal of Radiology*, vol. 151, p. 110294,
    2022\. [Online]. Available: [https://www.sciencedirect.com/science/article/pii/S0720048X22001449](https://www.sciencedirect.com/science/article/pii/S0720048X22001449)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Y. Fukushima, Y. Fushimi *等*，“使用超高分辨率计算机断层扫描评估 moyamoya 病：深度学习重建的应用”，*欧洲放射学杂志*，第151卷，页码110294，2022年。[在线]。可用：[https://www.sciencedirect.com/science/article/pii/S0720048X22001449](https://www.sciencedirect.com/science/article/pii/S0720048X22001449)'
- en: '[55] C. McLeavy, M. Chunara *et al.*, “The future of ct: deep learning reconstruction,”
    *Clinical Radiology*, vol. 76, no. 6, pp. 407–415, 2021\. [Online]. Available:
    [https://www.sciencedirect.com/science/article/pii/S0009926021000672](https://www.sciencedirect.com/science/article/pii/S0009926021000672)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] C. McLeavy, M. Chunara *等*，“CT 的未来：深度学习重建”，*临床放射学*，第76卷，第6期，页码407–415，2021年。[在线]。可用：[https://www.sciencedirect.com/science/article/pii/S0009926021000672](https://www.sciencedirect.com/science/article/pii/S0009926021000672)'
- en: '[56] X. Xing, Y. Yuan, and M. Q.-H. Meng, “Zoom in lesions for better diagnosis:
    Attention guided deformation network for wce image classification,” *IEEE Transactions
    on Medical Imaging*, vol. 39, no. 12, pp. 4047–4059, 2020.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] X. Xing, Y. Yuan 和 M. Q.-H. Meng，“放大病灶以提高诊断准确性：用于WCE图像分类的注意力引导变形网络”，*IEEE医学影像学报*，第39卷，第12期，页码4047–4059，2020年。'
- en: '[57] J. E. Ball, D. T. Anderson, and C. S. Chan Sr, “Comprehensive survey of
    deep learning in remote sensing: theories, tools, and challenges for the community,”
    *Journal of applied remote sensing*, vol. 11, no. 4, p. 042609, 2017.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] J. E. Ball, D. T. Anderson 和 C. S. Chan Sr，“深度学习在遥感中的综合调查：理论、工具和社区面临的挑战”，*应用遥感杂志*，第11卷，第4期，页码042609，2017年。'
- en: '[58] M. Vakalopoulou, K. Karantzalos *et al.*, “Building detection in very
    high resolution multispectral data with deep learning features,” in *IEEE International
    Geoscience and Remote Sensing Symposium*, 2015, pp. 1873–1876.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. Vakalopoulou, K. Karantzalos *等*，“利用深度学习特征在超高分辨率多光谱数据中进行建筑物检测”，发表于*IEEE国际地球科学与遥感研讨会*，2015年，页码1873–1876。'
- en: '[59] U. Alganci, M. Soydas, and E. Sertel, “Comparative research on deep learning
    approaches for airplane detection from very high-resolution satellite images,”
    *Remote Sensing*, vol. 12, no. 3, p. 458, 2020.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] U. Alganci, M. Soydas 和 E. Sertel，“从超高分辨率卫星图像中飞机检测的深度学习方法比较研究”，*遥感*，第12卷，第3期，页码458，2020年。'
- en: '[60] Z. M. Hamdi, M. Brandmeier, and C. Straub, “Forest damage assessment using
    deep learning on high resolution remote sensing data,” *Remote Sensing*, vol. 11,
    no. 17, p. 1976, 2019.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Z. M. Hamdi, M. Brandmeier 和 C. Straub，“利用深度学习对高分辨率遥感数据进行森林损伤评估”，*遥感*，第11卷，第17期，页码1976，2019年。'
- en: '[61] L. Ding, D. Lin *et al.*, “Looking outside the window: Wide-context transformer
    for the semantic segmentation of high-resolution remote sensing images,” *IEEE
    Transactions on Geoscience and Remote Sensing*, vol. 60, pp. 1–13, 2022.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] L. Ding, D. Lin *等*，“透过窗外看：用于高分辨率遥感图像语义分割的宽背景变换器”，*IEEE地球科学与遥感学报*，第60卷，页码1–13，2022年。'
- en: '[62] R. Zhao, Z. Shi, and Z. Zou, “High-resolution remote sensing image captioning
    based on structured attention,” *IEEE Transactions on Geoscience and Remote Sensing*,
    vol. 60, pp. 1–14, 2022.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] R. Zhao, Z. Shi 和 Z. Zou，“基于结构化注意力的高分辨率遥感图像描述”，*IEEE地球科学与遥感学报*，第60卷，页码1–14，2022年。'
- en: '[63] E. Rocha Rodrigues, I. Oliveira *et al.*, “Deepdownscale: A deep learning
    strategy for high-resolution weather forecast,” in *IEEE International Conference
    on e-Science*, 2018, pp. 415–422.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] E. Rocha Rodrigues, I. Oliveira *等*，“Deepdownscale: 一种用于高分辨率天气预报的深度学习策略”，发表于*IEEE国际电子科学会议*，2018年，页码415–422。'
- en: '[64] C. B. R. Ferreira, H. Pedrini *et al.*, “Where’s wally: A gigapixel image
    study for face recognition in crowds,” in *Advances in Visual Computing*, 2020,
    pp. 386–397.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] C. B. R. Ferreira, H. Pedrini *等*，“Wally 在哪里：用于人群中面部识别的千兆像素图像研究”，发表于*视觉计算进展*，2020年，页码386–397。'
- en: '[65] A. Specker, L. Moritz *et al.*, “Fast and lightweight online person search
    for large-scale surveillance systems,” in *IEEE/CVF Winter Conference on Applications
    of Computer Vision Workshops*, 2022, pp. 570–580.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] A. Specker, L. Moritz *等*，“大规模监控系统中的快速轻量级在线人员搜索”，发表于*IEEE/CVF计算机视觉应用冬季会议研讨会*，2022年，页码570–580。'
- en: '[66] M. Cormier, S. Wolf *et al.*, “Fast pedestrian detection for real-world
    crowded scenarios on embedded gpu,” in *IEEE International Conference on Smart
    Technologies*, 2021, pp. 40–44.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] M. Cormier, S. Wolf *等*，“在嵌入式 GPU 上进行现实世界拥挤场景的快速行人检测”，发表于*IEEE智能技术国际会议*，2021年，页码40–44。'
- en: '[67] L. Li, X. Guo *et al.*, “Region nms-based deep network for gigapixel level
    pedestrian detection with two-step cropping,” *Neurocomputing*, vol. 468, pp.
    482–491, 2022.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] L. Li, X. Guo *等*, “基于区域nms的深度网络用于千兆像素级行人检测与双步裁剪,” *Neurocomputing*, vol.
    468, pp. 482–491, 2022.'
- en: '[68] M. Aghaei, M. Bustreo *et al.*, “Single image human proxemics estimation
    for visual social distancing,” in *IEEE Winter Conference on Applications of Computer
    Vision*, 2021.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] M. Aghaei, M. Bustreo *等*, “用于视觉社交距离的单图像人类邻近估计,” 在 *IEEE Winter Conference
    on Applications of Computer Vision*, 2021.'
- en: '[69] I. Ahmed, M. Ahmad *et al.*, “A deep learning-based social distance monitoring
    framework for covid-19,” *Sustainable Cities and Society*, vol. 65, p. 102571,
    2021.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] I. Ahmed, M. Ahmad *等*, “基于深度学习的COVID-19社交距离监测框架,” *Sustainable Cities
    and Society*, vol. 65, p. 102571, 2021.'
- en: '[70] J. P. Horwath, D. N. Zakharov *et al.*, “Understanding important features
    of deep learning models for segmentation of high-resolution transmission electron
    microscopy images,” *NPJ Computational Materials*, vol. 6, no. 1, p. 108, 2020.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. P. Horwath, D. N. Zakharov *等*, “理解深度学习模型在高分辨率透射电子显微镜图像分割中的重要特征,” *NPJ
    Computational Materials*, vol. 6, no. 1, p. 108, 2020.'
- en: '[71] S. Lin, A. Ryabtsev *et al.*, “Real-time high-resolution background matting,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2021, pp. 8762–8771.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] S. Lin, A. Ryabtsev *等*, “实时高分辨率背景抠图,” 在 *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2021, pp.
    8762–8771.'
- en: '[72] D. Marin, Z. He *et al.*, “Efficient segmentation: Learning downsampling
    near semantic boundaries,” in *IEEE/CVF International Conference on Computer Vision*,
    2019, pp. 2131–2141.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] D. Marin, Z. He *等*, “高效分割：在语义边界附近学习下采样,” 在 *IEEE/CVF International Conference
    on Computer Vision*, 2019, pp. 2131–2141.'
- en: '[73] C. Jin, R. Tanno *et al.*, “Learning to downsample for segmentation of
    ultra-high resolution images,” *arXiv preprint arXiv:2109.11071*, 2021.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] C. Jin, R. Tanno *等*, “学习下采样以进行超高分辨率图像的分割,” *arXiv preprint arXiv:2109.11071*,
    2021.'
- en: '[74] C. Thavamani, M. Li *et al.*, “Fovea: Foveated image magnification for
    autonomous navigation,” in *IEEE/CVF International Conference on Computer Vision*,
    2021, pp. 15 539–15 548.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] C. Thavamani, M. Li *等*, “Fovea: 用于自主导航的视网膜图像放大,” 在 *IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 15 539–15 548.'
- en: '[75] M. Jaderberg, K. Simonyan *et al.*, “Spatial transformer networks,” *Advances
    in Neural Information Processing Systems*, vol. 28, 2015.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. Jaderberg, K. Simonyan *等*, “空间变换网络,” *Advances in Neural Information
    Processing Systems*, vol. 28, 2015.'
- en: '[76] J. Duchon, “Splines minimizing rotation-invariant semi-norms in sobolev
    spaces,” in *Constructive Theory of Functions of Several Variables*, 1977, pp.
    85–100.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] J. Duchon, “在Sobolev空间中最小化旋转不变半范数的样条函数,” 在 *Constructive Theory of Functions
    of Several Variables*, 1977, pp. 85–100.'
- en: '[77] B. E. Bejnordi, A. Habibian *et al.*, “Salisa: Saliency-based input sampling
    for efficient video object detection,” *arXiv preprint arXiv:2204.02397*, 2022.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] B. E. Bejnordi, A. Habibian *等*, “Salisa: 基于显著性的输入采样用于高效视频目标检测,” *arXiv
    preprint arXiv:2204.02397*, 2022.'
- en: '[78] N. Dong, M. Kampffmeyer *et al.*, “Reinforced auto-zoom net: towards accurate
    and fast breast cancer segmentation in whole-slide images,” in *Deep Learning
    in Medical Image Analysis and Multimodal Learning for Clinical Decision Support*,
    2018, pp. 317–325.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] N. Dong, M. Kampffmeyer *等*, “强化自动缩放网络：迈向精确且快速的全切片乳腺癌分割,” 在 *Deep Learning
    in Medical Image Analysis and Multimodal Learning for Clinical Decision Support*,
    2018, pp. 317–325.'
- en: '[79] M. Gao, R. Yu *et al.*, “Dynamic zoom-in network for fast object detection
    in large images,” in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] M. Gao, R. Yu *等*, “用于大图像快速目标检测的动态放大网络,” 在 *IEEE Conference on Computer
    Vision and Pattern Recognition*, 2018.'
- en: '[80] B. Uzkent and S. Ermon, “Learning when and where to zoom with deep reinforcement
    learning,” in *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] B. Uzkent 和 S. Ermon, “通过深度强化学习学习何时何地放大,” 在 *IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020.'
- en: '[81] H. Du, J. Feng, and M. Feng, “Zoom in to where it matters: a hierarchical
    graph based model for mammogram analysis,” *arXiv preprint arXiv:1912.07517*,
    2019.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] H. Du, J. Feng, 和 M. Feng, “放大重要区域：用于乳腺X光检查的分层图模型,” *arXiv preprint arXiv:1912.07517*,
    2019.'
- en: '[82] P. Velickovic, G. Cucurull *et al.*, “Graph attention networks,” in *International
    Conference on Learning Representations*, 2018.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] P. Velickovic, G. Cucurull *等*, “图注意力网络,” 在 *International Conference
    on Learning Representations*, 2018.'
- en: '[83] K. Chen, Z. Wang *et al.*, “Towards real-time object detection in gigapixel-level
    video,” *Neurocomputing*, vol. 477, pp. 14–24, 2022.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] K. Chen, Z. Wang *等*, “迈向千兆像素级视频中的实时目标检测,” *Neurocomputing*, vol. 477,
    pp. 14–24, 2022.'
- en: '[84] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] K. Simonyan 和 A. Zisserman，“用于大规模图像识别的非常深卷积网络，” *arXiv预印本 arXiv:1409.1556*，2014年。'
- en: '[85] J. Redmon, S. Divvala *et al.*, “You only look once: Unified, real-time
    object detection,” in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2016, pp. 779–788.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Redmon, S. Divvala *等*，“You only look once: 统一的实时目标检测，”发表于 *IEEE计算机视觉与模式识别会议*，2016年，第779–788页。'
- en: '[86] S. Ren, K. He *et al.*, “Faster r-cnn: Towards real-time object detection
    with region proposal networks,” *Advances in Neural Information Processing Systems*,
    vol. 28, 2015.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] S. Ren, K. He *等*，“Faster r-cnn: 面向实时目标检测的区域提议网络，” *神经信息处理系统进展*，第28卷，2015年。'
- en: '[87] S. Jiang, Z. Lin *et al.*, “Flexible high-resolution object detection
    on edge devices with tunable latency,” in *Annual International Conference on
    Mobile Computing and Networking*, 2021, p. 559–572.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] S. Jiang, Z. Lin *等*，“在边缘设备上具有可调延迟的灵活高分辨率目标检测，”发表于 *国际移动计算与网络年会*，2021年，第559–572页。'
- en: '[88] P. Sermanet, D. Eigen *et al.*, “Overfeat: Integrated recognition, localization
    and detection using convolutional networks,” *arXiv preprint arXiv:1312.6229*,
    2013.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] P. Sermanet, D. Eigen *等*，“Overfeat: 使用卷积网络进行集成识别、定位和检测，” *arXiv预印本 arXiv:1312.6229*，2013年。'
- en: '[89] M. Tzelepi and A. Tefas, “Class-specific discriminant regularization in
    real-time deep cnn models for binary classification problems,” *Neural Processing
    Letters*, vol. 51, no. 2, pp. 1989–2005, 2020.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] M. Tzelepi 和 A. Tefas，“实时深度CNN模型中的类别特定判别正则化用于二分类问题，” *神经处理信件*，第51卷，第2期，第1989–2005页，2020年。'
- en: '[90] M. Tzelepi and A. Tefas, “Improving the performance of lightweight cnns
    for binary classification using quadratic mutual information regularization,”
    *Pattern Recognition*, vol. 106, p. 107407, 2020.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. Tzelepi 和 A. Tefas，“通过二次互信息正则化提升轻量级CNN的二分类性能，” *模式识别*，第106卷，第107407页，2020年。'
- en: '[91] M. Tzelepi and A. Tefas, “Graph embedded convolutional neural networks
    in human crowd detection for drone flight safety,” *IEEE Transactions on Emerging
    Topics in Computational Intelligence*, vol. 5, no. 2, pp. 191–204, 2021.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. Tzelepi 和 A. Tefas，“图嵌入卷积神经网络在人群检测中的应用以提升无人机飞行安全，” *IEEE新兴计算智能主题期刊*，第5卷，第2期，第191–204页，2021年。'
- en: '[92] D. Triantafyllidou, P. Nousi, and A. Tefas, “Fast deep convolutional face
    detection in the wild exploiting hard sample mining,” *Big Data Research*, vol. 11,
    pp. 65–76, 2018.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] D. Triantafyllidou, P. Nousi, 和 A. Tefas，“利用困难样本挖掘在自然环境中进行快速深度卷积人脸检测，”
    *大数据研究*，第11卷，第65–76页，2018年。'
- en: '[93] S. Mehta, M. Rastegari *et al.*, “Espnet: Efficient spatial pyramid of
    dilated convolutions for semantic segmentation,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, September 2018.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. Mehta, M. Rastegari *等*，“Espnet: 用于语义分割的高效空间金字塔膨胀卷积，”发表于 *欧洲计算机视觉会议（ECCV）*，2018年9月。'
- en: '[94] M. Ding, X. Lian *et al.*, “Hr-nas: Searching efficient high-resolution
    neural architectures with lightweight transformers,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2021, pp.
    2982–2992.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] M. Ding, X. Lian *等*，“Hr-nas: 搜索高效高分辨率神经架构与轻量级变压器，”发表于 *IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2021年6月，第2982–2992页。'
- en: '[95] F. Yang, L. Herranz *et al.*, “Slimmable compressive autoencoders for
    practical neural image compression,” in *IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2021, pp. 4998–5007.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] F. Yang, L. Herranz *等*，“用于实际神经图像压缩的可缩放压缩自编码器，”发表于 *IEEE/CVF计算机视觉与模式识别大会*，2021年，第4998–5007页。'
- en: '[96] W. Lu, S. Graham *et al.*, “Capturing cellular topology in multi-gigapixel
    pathology images,” in *IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops*, 2020.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] W. Lu, S. Graham *等*，“捕捉多千兆像素病理图像中的细胞拓扑，”发表于 *IEEE/CVF计算机视觉与模式识别大会研讨会*，2020年。'
- en: '[97] S. Graham, Q. D. Vu *et al.*, “Hover-net: Simultaneous segmentation and
    classification of nuclei in multi-tissue histology images,” *Medical Image Analysis*,
    vol. 58, p. 101563, 2019.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] S. Graham, Q. D. Vu *等*，“Hover-net: 在多组织学图像中同时进行核的分割和分类，” *医学图像分析*，第58卷，第101563页，2019年。'
- en: '[98] J. Gamper, N. A. Koohbanani *et al.*, “Pannuke: an open pan-cancer histology
    dataset for nuclei instance segmentation and classification,” in *European Congress
    on Digital Pathology*, 2019, pp. 11–19.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. Gamper, N. A. Koohbanani *等*，“Pannuke: 一个用于核实例分割和分类的开放式全癌症组织学数据集，”发表于
    *欧洲数字病理学大会*，2019年，第11–19页。'
- en: '[99] D. Müllner, “Modern hierarchical, agglomerative clustering algorithms,”
    *arXiv preprint arXiv:1109.2378*, 2011.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] D. Müllner，“现代分层聚合聚类算法”，*arXiv预印本 arXiv:1109.2378*，2011年。'
- en: '[100] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *International Conference on Learning Representations*,
    2017.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] T. N. Kipf 和 M. Welling，“使用图卷积网络的半监督分类”，在*国际学习表示会议*，2017年。'
- en: '[101] D. Tellez, G. Litjens *et al.*, “Neural image compression for gigapixel
    histopathology image analysis,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 43, no. 2, pp. 567–578, 2021.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] D. Tellez, G. Litjens *等*，“用于大像素组织病理图像分析的神经图像压缩”，*IEEE模式分析与机器智能汇刊*，第43卷，第2期，页码567–578，2021年。'
- en: '[102] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv
    preprint arXiv:1312.6114*, 2013.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] D. P. Kingma 和 M. Welling，“自编码变分贝叶斯”，*arXiv预印本 arXiv:1312.6114*，2013年。'
- en: '[103] J. Donahue, P. Krähenbühl, and T. Darrell, “Adversarial feature learning,”
    *arXiv preprint arXiv:1605.09782*, 2016.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. Donahue, P. Krähenbühl, 和 T. Darrell，“对抗特征学习”，*arXiv预印本 arXiv:1605.09782*，2016年。'
- en: '[104] R. J. Chen, M. Y. Lu *et al.*, “Multimodal co-attention transformer for
    survival prediction in gigapixel whole slide images,” in *IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 4015–4025.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] R. J. Chen, M. Y. Lu *等*，“用于大像素全图图像生存预测的多模态共同注意力变换器”，发表于*IEEE/CVF国际计算机视觉会议*，2021年，页码4015–4025。'
- en: '[105] K. He, X. Zhang *et al.*, “Deep residual learning for image recognition,”
    in *IEEE Conference on Computer Vision and Pattern Recognition*, 2016, pp. 770–778.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] K. He, X. Zhang *等*，“用于图像识别的深度残差学习”，发表于*IEEE计算机视觉与模式识别会议*，2016年，页码770–778。'
- en: '[106] J. Deng, W. Dong *et al.*, “Imagenet: A large-scale hierarchical image
    database,” in *IEEE Conference on Computer Vision and Pattern Recognition*, 2009,
    pp. 248–255.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. Deng, W. Dong *等*，“ImageNet：大规模层次图像数据库”，在*IEEE计算机视觉与模式识别会议*，2009年，页码248–255。'
- en: '[107] L. Gueguen, A. Sergeev *et al.*, “Faster neural networks straight from
    jpeg,” *Advances in Neural Information Processing Systems*, vol. 31, 2018.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] L. Gueguen, A. Sergeev *等*，“从JPEG直接获取更快的神经网络”，*神经信息处理系统进展*，第31卷，2018年。'
- en: '[108] K. Xu, M. Qin *et al.*, “Learning in the frequency domain,” in *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 1740–1749.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] K. Xu, M. Qin *等*，“频域学习”，在*IEEE/CVF计算机视觉与模式识别会议*，2020年，页码1740–1749。'
- en: '[109] L. Wang and Y. Sun, “Image classification using convolutional neural
    network with wavelet domain inputs,” *IET Image Processing*, vol. 16, no. 8, pp.
    2037–2048, 2022.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] L. Wang 和 Y. Sun，“使用小波域输入的卷积神经网络进行图像分类”，*IET图像处理*，第16卷，第8期，页码2037–2048，2022年。'
- en: '[110] A. Katharotiya, S. Patel, and M. Goyani, “Comparative analysis between
    dct & dwt techniques of image compression,” *Journal of Information Engineering
    and Applications*, vol. 1, no. 2, pp. 9–17, 2011.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] A. Katharotiya, S. Patel, 和 M. Goyani，“图像压缩中DCT与DWT技术的比较分析”，*信息工程与应用期刊*，第1卷，第2期，页码9–17，2011年。'
- en: '[111] S. Wang, H. Lu, and Z. Deng, “Fast object detection in compressed video,”
    in *IEEE/CVF International Conference on Computer Vision*, 2019.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] S. Wang, H. Lu, 和 Z. Deng，“压缩视频中的快速目标检测”，发表于*IEEE/CVF国际计算机视觉会议*，2019年。'
- en: '[112] I. E. Richardson, *H. 264 and MPEG-4 video compression: video coding
    for next-generation multimedia*.   John Wiley & Sons, 2004.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] I. E. Richardson，*H. 264 和 MPEG-4 视频压缩：下一代多媒体视频编码*。约翰·威利父子公司，2004年。'
- en: '[113] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] S. Hochreiter 和 J. Schmidhuber，“长短期记忆”，*神经计算*，第9卷，第8期，页码1735–1780，1997年。'
- en: '[114] P. Zhang, X. Dai *et al.*, “Multi-scale vision longformer: A new vision
    transformer for high-resolution image encoding,” in *IEEE/CVF International Conference
    on Computer Vision*, 2021, pp. 2998–3008.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] P. Zhang, X. Dai *等*，“多尺度视觉长变换器：一种用于高分辨率图像编码的新型视觉变换器”，发表于*IEEE/CVF国际计算机视觉会议*，2021年，页码2998–3008。'
- en: '[115] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document
    transformer,” *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] I. Beltagy, M. E. Peters, 和 A. Cohan，“Longformer：长文档变换器”，*arXiv预印本 arXiv:2004.05150*，2020年。'
- en: '[116] Y. YUAN, R. Fu *et al.*, “Hrformer: High-resolution vision transformer
    for dense predict,” in *Advances in Neural Information Processing Systems*, vol. 34,
    2021, pp. 7281–7293.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Y. YUAN, R. Fu *等*，“Hrformer：高分辨率视觉变换器用于密集预测”，在*神经信息处理系统进展*，第34卷，2021年，页码7281–7293。'
- en: '[117] J. Gu, H. Kwon *et al.*, “Multi-scale high-resolution vision transformer
    for semantic segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2022, pp. 12 094–12 103.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] J. 顾, H. 权 *等人*, “用于语义分割的多尺度高分辨率视觉变换器”，发表于 *IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2022年，页码12 094–12 103。'
- en: '[118] X. Dong, J. Bao *et al.*, “Cswin transformer: A general vision transformer
    backbone with cross-shaped windows,” in *IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 12 124–12 134.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] X. 董, J. 包 *等人*, “Cswin变换器：一种具有交叉形窗口的通用视觉变换器骨干网”，发表于 *IEEE/CVF计算机视觉与模式识别会议*，2022年，页码12 124–12 134。'
- en: '[119] Q. Yu, Y. Xia *et al.*, “Glance-and-gaze vision transformer,” in *Advances
    in Neural Information Processing Systems*, vol. 34, 2021, pp. 12 992–13 003.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Q. 于, Y. 夏 *等人*, “一瞥一注视视觉变换器”，发表于 *神经信息处理系统进展*，第34卷，2021年，页码12 992–13 003。'
- en: '[120] R. J. Chen, C. Chen *et al.*, “Scaling vision transformers to gigapixel
    images via hierarchical self-supervised learning,” in *IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2022, pp. 16 144–16 155.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] R. J. 陈, C. 陈 *等人*, “通过层次自监督学习将视觉变换器扩展到千兆像素图像”，发表于 *IEEE/CVF计算机视觉与模式识别会议*，2022年，页码16 144–16 155。'
- en: '[121] B. Thomee, D. A. Shamma *et al.*, “Yfcc100m: The new data in multimedia
    research,” *Communications of the ACM*, vol. 59, no. 2, pp. 64–73, 2016.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] B. 托梅, D. A. 沙马 *等人*, “Yfcc100m：多媒体研究中的新数据”，*ACM通讯*，第59卷，第2期，页码64–73，2016年。'
- en: '[122] P. M. R. Martin Koestinger, Paul Wohlhart and H. Bischof, “Annotated
    Facial Landmarks in the Wild: A Large-scale, Real-world Database for Facial Landmark
    Localization,” in *IEEE International Workshop on Benchmarking Facial Image Analysis
    Technologies*, 2011.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] P. M. R. 马丁·科斯廷格, 保罗·沃尔哈特 和 H. 比肖夫，“野外标注的面部标志：一个大规模的现实世界面部标志定位数据库”，发表于
    *IEEE国际面部图像分析技术基准研讨会*，2011年。'
- en: '[123] Z. Zhang, P. Luo *et al.*, “Facial landmark detection by deep multi-task
    learning,” in *European Conference on Computer Vision*, 2014, pp. 94–108.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Z. 张, P. 罗 *等人*, “通过深度多任务学习进行面部标志检测”，发表于 *欧洲计算机视觉会议*，2014年，页码94–108。'
- en: '[124] S. Yang, P. Luo *et al.*, “Wider face: A face detection benchmark,” in
    *IEEE Conference on Computer Vision and Pattern Recognition*, 2016.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] S. 杨, P. 罗 *等人*, “Wider face：一个人脸检测基准”，发表于 *IEEE计算机视觉与模式识别会议*，2016年。'
- en: '[125] H. Idrees, I. Saleemi *et al.*, “Multi-source multi-scale counting in
    extremely dense crowd images,” in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2013, pp. 2547–2554.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] H. 伊德里斯, I. 萨利米 *等人*, “在极其密集的拥挤图像中进行多源多尺度计数”，发表于 *IEEE计算机视觉与模式识别会议*，2013年，页码2547–2554。'
- en: '[126] Y. Zhang, D. Zhou *et al.*, “Single-image crowd counting via multi-column
    convolutional neural network,” in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2016, pp. 589–597.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Y. 张, D. 周 *等人*, “通过多列卷积神经网络进行单图像人群计数”，发表于 *IEEE计算机视觉与模式识别会议*，2016年，页码589–597。'
- en: '[127] H. Idrees, M. Tayyab *et al.*, “Composition loss for counting, density
    map estimation and localization in dense crowds,” in *European conference on computer
    vision*, 2018, pp. 532–546.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] H. 伊德里斯, M. 塔耶布 *等人*, “用于计数、密度图估计和密集人群定位的组成损失”，发表于 *欧洲计算机视觉会议*，2018年，页码532–546。'
- en: '[128] V. A. Sindagi, R. Yasarla, and V. M. Patel, “Jhu-crowd++: Large-scale
    crowd counting dataset and a benchmark method,” *Technical Report*, 2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] V. A. 辛达吉, R. 雅萨拉 和 V. M. 帕特尔，“Jhu-crowd++：大规模人群计数数据集和基准方法”，*技术报告*，2020年。'
- en: '[129] Q. Wang, J. Gao *et al.*, “Nwpu-crowd: A large-scale benchmark for crowd
    counting and localization,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 43, no. 6, pp. 2141–2149, 2020.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Q. 王, J. 高 *等人*, “Nwpu-crowd：一个大规模的人群计数和定位基准”，*IEEE模式分析与机器智能学报*，第43卷，第6期，页码2141–2149，2020年。'
- en: '[130] D. Hu, L. Mou *et al.*, “Ambient sound helps: Audiovisual crowd counting
    in extreme conditions,” *arXiv preprint arXiv:2005.07097*, 2020.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] D. 胡, L. 牟 *等人*, “环境声音有帮助：极端条件下的视听人群计数”，*arXiv预印本 arXiv:2005.07097*，2020年。'
- en: '[131] M. Cordts, M. Omran *et al.*, “The cityscapes dataset for semantic urban
    scene understanding,” in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2016.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] M. 科尔茨, M. 奥姆兰 *等人*, “Cityscapes数据集用于语义城市场景理解”，发表于 *IEEE计算机视觉与模式识别会议*，2016年。'
- en: '[132] G. Ros, L. Sellart *et al.*, “The synthia dataset: A large collection
    of synthetic images for semantic segmentation of urban scenes,” in *IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] G. 罗斯, L. 塞拉特 *等人*, “Synthia数据集：用于城市场景语义分割的大型合成图像集合”，发表于 *IEEE计算机视觉与模式识别会议*，2016年。'
- en: '[133] X. Huang, P. Wang *et al.*, “The apolloscape open dataset for autonomous
    driving and its application,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 42, no. 10, pp. 2702–2719, 2020.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] X. Huang, P. Wang *等人*, “Apolloscape开放数据集及其在自动驾驶中的应用”，*IEEE模式分析与机器智能汇刊*，第42卷，第10期，页码2702–2719，2020年。'
- en: '[134] M. Li, Y.-X. Wang, and D. Ramanan, “Towards streaming perception,” in
    *European Conference on Computer Vision*, 2020, pp. 473–488.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] M. Li, Y.-X. Wang, 和 D. Ramanan, “迈向流媒体感知”，发表于 *欧洲计算机视觉大会*，2020年，页码473–488。'
- en: '[135] F. Yu, H. Chen *et al.*, “Bdd100k: A diverse driving dataset for heterogeneous
    multitask learning,” in *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] F. Yu, H. Chen *等人*, “Bdd100k: 一个多任务学习的多样化驾驶数据集”，发表于 *IEEE/CVF计算机视觉与模式识别大会*，2020年。'
- en: '[136] R. Mottaghi, X. Chen *et al.*, “The role of context for object detection
    and semantic segmentation in the wild,” in *IEEE Conference on Computer Vision
    and Pattern Recognition*, 2014.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] R. Mottaghi, X. Chen *等人*, “背景在野外物体检测和语义分割中的作用”，发表于 *IEEE计算机视觉与模式识别大会*，2014年。'
- en: '[137] B. Zhou, H. Zhao *et al.*, “Scene parsing through ade20k dataset,” in
    *IEEE Conference on Computer Vision and Pattern Recognition*, 2017, pp. 633–641.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] B. Zhou, H. Zhao *等人*, “通过ade20k数据集进行场景解析”，发表于 *IEEE计算机视觉与模式识别大会*，2017年，页码633–641。'
- en: '[138] H. Caesar, J. Uijlings, and V. Ferrari, “Coco-stuff: Thing and stuff
    classes in context,” in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 1209–1218.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] H. Caesar, J. Uijlings, 和 V. Ferrari, “Coco-stuff: 语境中的物体和物质类别”，发表于 *IEEE计算机视觉与模式识别大会*，2018年，页码1209–1218。'
- en: '[139] I. Demir, K. Koperski *et al.*, “Deepglobe 2018: A challenge to parse
    the earth through satellite images,” in *IEEE Conference on Computer Vision and
    Pattern Recognition Workshops*, 2018, pp. 172–181.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] I. Demir, K. Koperski *等人*, “Deepglobe 2018: 挑战通过卫星图像解析地球”，发表于 *IEEE计算机视觉与模式识别研讨会*，2018年，页码172–181。'
- en: '[140] M. Buchhorn, B. Smets *et al.*, “Copernicus global land service: Land
    cover 100m: collection 3: epoch 2019: Globe,” *Version V3\. 0.1)[Data set]*, 2020.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] M. Buchhorn, B. Smets *等人*, “Copernicus全球土地服务: 土地覆盖100m: 集合3: 纪元2019:
    全球”，*版本 V3.0.1*[数据集]，2020年。'
- en: '[141] G. Christie, N. Fendley *et al.*, “Functional map of the world,” in *IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] G. Christie, N. Fendley *等人*, “世界功能图谱”，发表于 *IEEE计算机视觉与模式识别大会*，2018年。'
- en: '[142] A. Koulaouzidis, D. Iakovidis *et al.*, “KID project: an internet-based
    digital video atlas of capsule endoscopy for research purposes,” *Endoscopy International
    Open*, vol. 5, no. 6, pp. E477–E483, 2017.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] A. Koulaouzidis, D. Iakovidis *等人*, “KID项目：一个基于互联网的胶囊内窥镜数字视频图谱用于研究目的”，*内窥镜国际开放*，第5卷，第6期，页码E477–E483，2017年。'
- en: '[143] R. Leenhardt, C. Li *et al.*, “CAD-CAP: a 25,000-image database serving
    the development of artificial intelligence for capsule endoscopy,” *Endosc Int
    Open*, vol. 8, no. 3, pp. E415–E420, 2020.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] R. Leenhardt, C. Li *等人*, “CAD-CAP: 一个25,000图像的数据库，服务于胶囊内窥镜人工智能的开发”，*Endosc
    Int Open*，第8卷，第3期，页码E415–E420，2020年。'
- en: '[144] M. Veta, Y. J. Heng *et al.*, “Predicting breast tumor proliferation
    from whole-slide images: The tupac16 challenge,” *Medical Image Analysis*, vol. 54,
    pp. 111–121, 2019.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] M. Veta, Y. J. Heng *等人*, “从全切片图像中预测乳腺肿瘤增殖：Tupac16挑战”，*医学图像分析*，第54卷，页码111–121，2019年。'
- en: '[145] G. Aresta, T. Araújo *et al.*, “Bach: Grand challenge on breast cancer
    histology images,” *Medical Image Analysis*, vol. 56, pp. 122–139, 2019.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] G. Aresta, T. Araújo *等人*, “Bach: 乳腺癌组织学图像的重大挑战”，*医学图像分析*，第56卷，页码122–139，2019年。'
- en: '[146] D. C. Koboldt, R. S. Fulton *et al.*, “Comprehensive molecular portraits
    of human breast tumours,” *Nature*, vol. 490, no. 7418, pp. 61–70, 2012.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] D. C. Koboldt, R. S. Fulton *等人*, “人类乳腺肿瘤的全面分子图谱”，*自然*，第490卷，第7418期，页码61–70，2012年。'
- en: '[147] I. C. Moreira, I. Amaral *et al.*, “INbreast,” *Academic Radiology*,
    vol. 19, no. 2, pp. 236–248, 2012.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] I. C. Moreira, I. Amaral *等人*, “INbreast”，*学术放射学*，第19卷，第2期，页码236–248，2012年。'
- en: '[148] L. Wen, D. Du *et al.*, “UA-DETRAC: A new benchmark and protocol for
    multi-object detection and tracking,” *Computer Vision and Image Understanding*,
    vol. 193, p. 102907, 2020.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] L. Wen, D. Du *等人*, “UA-DETRAC: 多目标检测和跟踪的新基准和协议”，*计算机视觉与图像理解*，第193卷，页码102907，2020年。'
- en: '[149] O. Russakovsky, J. Deng *et al.*, “Imagenet large scale visual recognition
    challenge,” *International Journal of Computer Vision*, vol. 115, no. 3, pp. 211–252,
    2015.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] O. Russakovsky, J. Deng *等人*, “Imagenet大规模视觉识别挑战”，*国际计算机视觉杂志*，第115卷，第3期，页码211–252，2015年。'
- en: '[150] X. Sun, P. Wang *et al.*, “Fair1m: A benchmark dataset for fine-grained
    object recognition in high-resolution remote sensing imagery,” *ISPRS Journal
    of Photogrammetry and Remote Sensing*, vol. 184, pp. 116–130, 2022. [Online].
    Available: [https://www.sciencedirect.com/science/article/pii/S0924271621003269](https://www.sciencedirect.com/science/article/pii/S0924271621003269)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] X. Sun, P. Wang *等人*, “Fair1m：一个用于高分辨率遥感图像中细粒度物体识别的基准数据集，” *ISPRS 摄影测量与遥感期刊*，第184卷，第116–130页，2022年。
    [在线]. 可用: [https://www.sciencedirect.com/science/article/pii/S0924271621003269](https://www.sciencedirect.com/science/article/pii/S0924271621003269)'
- en: '[151] T.-Y. Lin, M. Maire *et al.*, “Microsoft coco: Common objects in context,”
    in *European Conference on Computer Vision*, 2014, pp. 740–755.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] T.-Y. Lin, M. Maire *等人*, “Microsoft coco: 语境中的常见物体，” 在 *欧洲计算机视觉大会*，2014年，第740–755页。'
- en: '[152] R. Ma and Q. Hao, “Cs-fnet: A compressive sampling frequency neural network
    for simultaneous image compression and recognition,” in *IEEE International Conference
    on Multisensor Fusion and Integration for Intelligent Systems*, 2021, pp. 1–6.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] R. Ma 和 Q. Hao, “Cs-fnet：一种用于同时图像压缩和识别的压缩采样频率神经网络，” 在 *IEEE 国际多传感器融合与智能系统会议*，2021年，第1–6页。'
- en: '[153] M.-H. Guo, Z.-N. Liu *et al.*, “Can attention enable mlps to catch up
    with cnns?” *Computational Visual Media*, vol. 7, no. 3, pp. 283–288, 2021.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] M.-H. Guo, Z.-N. Liu *等人*, “注意力机制能否使MLPs赶上CNNs？” *计算视觉媒体*，第7卷，第3期，第283–288页，2021年。'
- en: '[154] Y. Cheng, D. Wang *et al.*, “Model compression and acceleration for deep
    neural networks: The principles, progress, and challenges,” *IEEE Signal Processing
    Magazine*, vol. 35, no. 1, pp. 126–136, 2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Y. Cheng, D. Wang *等人*, “深度神经网络的模型压缩与加速：原理、进展与挑战，” *IEEE 信号处理杂志*，第35卷，第1期，第126–136页，2018年。'
- en: '[155] Y. Han, G. Huang *et al.*, “Dynamic neural networks: A survey,” *arXiv
    preprint arXiv:2102.04906*, 2021.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Y. Han, G. Huang *等人*, “动态神经网络：综述，” *arXiv 预印本 arXiv:2102.04906*，2021年。'
- en: '[156] J. Carreira, V. Patraucean *et al.*, “Massively parallel video networks,”
    in *European Conference on Computer Vision*, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] J. Carreira, V. Patraucean *等人*, “大规模并行视频网络，” 在 *欧洲计算机视觉大会*，2018年。'
- en: '[157] L. Hedegaard and A. Iosifidis, “Continual inference: A library for efficient
    online inference with deep neural networks in pytorch,” *arXiv preprint: arXiv:2204.03418*,
    2022.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] L. Hedegaard 和 A. Iosifidis, “连续推理：一个用于高效在线推理的深度神经网络库，基于pytorch，” *arXiv
    预印本：arXiv:2204.03418*，2022年。'
- en: '[158] Y. Matsubara, M. Levorato, and F. Restuccia, “Split computing and early
    exiting for deep learning applications: Survey and research challenges,” *ACM
    Computing Surveys*, 2021.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Y. Matsubara, M. Levorato 和 F. Restuccia, “深度学习应用中的拆分计算和早期退出：综述与研究挑战，”
    *ACM Computing Surveys*，2021年。'
- en: '[159] A. Bakhtiarnia, N. Milošević *et al.*, “Dynamic split computing for efficient
    deep edge intelligence,” *arXiv preprint arXiv:2205.11269*, 2022.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] A. Bakhtiarnia, N. Milošević *等人*, “用于高效深度边缘智能的动态拆分计算，” *arXiv 预印本 arXiv:2205.11269*，2022年。'
- en: '[160] N. Boehrer, A. Gabriel *et al.*, “Onboard ROI selection for aerial surveillance
    using a high resolution, high framerate camera,” in *Mobile Multimedia/Image Processing,
    Security, and Applications*, vol. 11399, 2020, pp. 76 – 95.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] N. Boehrer, A. Gabriel *等人*, “使用高分辨率、高帧率相机进行空中监视的板载ROI选择，” 在 *移动多媒体/图像处理、安全与应用*，第11399卷，2020年，第76–95页。'
- en: Appendix A Data Sources
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据来源
- en: 'Data Sources and details for device camera resolutions are shown in Table [V](#A1.T5
    "TABLE V ‣ Appendix A Data Sources ‣ Efficient High-Resolution Deep Learning:
    A Survey").'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '数据来源和设备相机分辨率的详细信息见表 [V](#A1.T5 "TABLE V ‣ Appendix A Data Sources ‣ Efficient
    High-Resolution Deep Learning: A Survey")。'
- en: 'TABLE V: Details for device camera resolutions. All links were accessed at
    26 July 2022.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：设备相机分辨率的详细信息。所有链接均在2022年7月26日访问。
- en: '| Device Camera | Year | Resolution (MP) | Source |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 设备相机 | 年份 | 分辨率 (MP) | 来源 |'
- en: '| Apple iPhone Rear Camera | 2007 | 2 | [https://en.wikipedia.org/wiki/IPhone_(1st_generation)](https://en.wikipedia.org/wiki/IPhone_(1st_generation))
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 苹果 iPhone 后置摄像头 | 2007 | 2 | [https://en.wikipedia.org/wiki/IPhone_(1st_generation)](https://en.wikipedia.org/wiki/IPhone_(1st_generation))
    |'
- en: '|  | 2008 | 2 | [https://en.wikipedia.org/wiki/IPhone_3G](https://en.wikipedia.org/wiki/IPhone_3G)
    |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '|  | 2008 | 2 | [https://en.wikipedia.org/wiki/IPhone_3G](https://en.wikipedia.org/wiki/IPhone_3G)
    |'
- en: '|  | 2009 | 3 | [https://en.wikipedia.org/wiki/IPhone_3GS](https://en.wikipedia.org/wiki/IPhone_3GS)
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  | 2009 | 3 | [https://en.wikipedia.org/wiki/IPhone_3GS](https://en.wikipedia.org/wiki/IPhone_3GS)
    |'
- en: '|  | 2010 | 5 | [https://en.wikipedia.org/wiki/IPhone_4](https://en.wikipedia.org/wiki/IPhone_4)
    |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|  | 2010 | 5 | [https://en.wikipedia.org/wiki/IPhone_4](https://en.wikipedia.org/wiki/IPhone_4)
    |'
- en: '|  | 2011 | 8 | [https://en.wikipedia.org/wiki/IPhone_4S](https://en.wikipedia.org/wiki/IPhone_4S)
    |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | 2011 | 8 | [https://en.wikipedia.org/wiki/IPhone_4S](https://en.wikipedia.org/wiki/IPhone_4S)
    |'
- en: '|  | 2012 | 8 | [https://en.wikipedia.org/wiki/IPhone_5](https://en.wikipedia.org/wiki/IPhone_5)
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | 2012 | 8 | [https://en.wikipedia.org/wiki/IPhone_5](https://en.wikipedia.org/wiki/IPhone_5)
    |'
- en: '|  | 2013 | 8 | [https://en.wikipedia.org/wiki/IPhone_5S](https://en.wikipedia.org/wiki/IPhone_5S)
    |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | 2013 | 8 | [https://en.wikipedia.org/wiki/IPhone_5S](https://en.wikipedia.org/wiki/IPhone_5S)
    |'
- en: '|  | 2014 | 8 | [https://en.wikipedia.org/wiki/IPhone_6](https://en.wikipedia.org/wiki/IPhone_6)
    |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | 2014 | 8 | [https://en.wikipedia.org/wiki/IPhone_6](https://en.wikipedia.org/wiki/IPhone_6)
    |'
- en: '|  | 2015 | 12 | [https://en.wikipedia.org/wiki/IPhone_6S](https://en.wikipedia.org/wiki/IPhone_6S)
    |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | 2015 | 12 | [https://en.wikipedia.org/wiki/IPhone_6S](https://en.wikipedia.org/wiki/IPhone_6S)
    |'
- en: '|  | 2016 | 12.2 | [https://en.wikipedia.org/wiki/IPhone_SE_(1st_generation)](https://en.wikipedia.org/wiki/IPhone_SE_(1st_generation))
    |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | 12.2 | [https://en.wikipedia.org/wiki/IPhone_SE_(1st_generation)](https://en.wikipedia.org/wiki/IPhone_SE_(1st_generation))
    |'
- en: '|  | 2017 | 12 | [https://en.wikipedia.org/wiki/IPhone_X](https://en.wikipedia.org/wiki/IPhone_X)
    |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | 12 | [https://en.wikipedia.org/wiki/IPhone_X](https://en.wikipedia.org/wiki/IPhone_X)
    |'
- en: '|  | 2018 | 12 | [https://en.wikipedia.org/wiki/IPhone_XS](https://en.wikipedia.org/wiki/IPhone_XS)
    |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | 12 | [https://en.wikipedia.org/wiki/IPhone_XS](https://en.wikipedia.org/wiki/IPhone_XS)
    |'
- en: '|  | 2019 | 12 | [https://en.wikipedia.org/wiki/IPhone_11_Pro](https://en.wikipedia.org/wiki/IPhone_11_Pro)
    |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | 12 | [https://en.wikipedia.org/wiki/IPhone_11_Pro](https://en.wikipedia.org/wiki/IPhone_11_Pro)
    |'
- en: '|  | 2020 | 12 | [https://en.wikipedia.org/wiki/IPhone_12_Pro](https://en.wikipedia.org/wiki/IPhone_12_Pro)
    |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | 12 | [https://en.wikipedia.org/wiki/IPhone_12_Pro](https://en.wikipedia.org/wiki/IPhone_12_Pro)
    |'
- en: '|  | 2021 | 12 | [https://en.wikipedia.org/wiki/IPhone_13_Pro](https://en.wikipedia.org/wiki/IPhone_13_Pro)
    |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | 12 | [https://en.wikipedia.org/wiki/IPhone_13_Pro](https://en.wikipedia.org/wiki/IPhone_13_Pro)
    |'
- en: '|  | 2022 | 12 | [https://en.wikipedia.org/wiki/IPhone_SE_(3rd_generation)](https://en.wikipedia.org/wiki/IPhone_SE_(3rd_generation))
    |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | 12 | [https://en.wikipedia.org/wiki/IPhone_SE_(3rd_generation)](https://en.wikipedia.org/wiki/IPhone_SE_(3rd_generation))
    |'
- en: '| Samsung Galaxy S Rear Camera | 2010 | 5 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S](https://en.wikipedia.org/wiki/Samsung_Galaxy_S)
    |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| Samsung Galaxy S 后置摄像头 | 2010 | 5 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S](https://en.wikipedia.org/wiki/Samsung_Galaxy_S)
    |'
- en: '|  | 2011 | 8 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S_II](https://en.wikipedia.org/wiki/Samsung_Galaxy_S_II)
    |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  | 2011 | 8 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S_II](https://en.wikipedia.org/wiki/Samsung_Galaxy_S_II)
    |'
- en: '|  | 2012 | 8 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S_III](https://en.wikipedia.org/wiki/Samsung_Galaxy_S_III)
    |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  | 2012 | 8 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S_III](https://en.wikipedia.org/wiki/Samsung_Galaxy_S_III)
    |'
- en: '|  | 2013 | 13 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S4](https://en.wikipedia.org/wiki/Samsung_Galaxy_S4)
    |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  | 2013 | 13 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S4](https://en.wikipedia.org/wiki/Samsung_Galaxy_S4)
    |'
- en: '|  | 2014 | 16 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S5](https://en.wikipedia.org/wiki/Samsung_Galaxy_S5)
    |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | 2014 | 16 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S5](https://en.wikipedia.org/wiki/Samsung_Galaxy_S5)
    |'
- en: '|  | 2015 | 16 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S6](https://en.wikipedia.org/wiki/Samsung_Galaxy_S6)
    |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  | 2015 | 16 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S6](https://en.wikipedia.org/wiki/Samsung_Galaxy_S6)
    |'
- en: '|  | 2016 | 12 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S7](https://en.wikipedia.org/wiki/Samsung_Galaxy_S7)
    |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | 12 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S7](https://en.wikipedia.org/wiki/Samsung_Galaxy_S7)
    |'
- en: '|  | 2017 | 12 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S8](https://en.wikipedia.org/wiki/Samsung_Galaxy_S8)
    |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | 12 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S8](https://en.wikipedia.org/wiki/Samsung_Galaxy_S8)
    |'
- en: '|  | 2018 | 12 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S9](https://en.wikipedia.org/wiki/Samsung_Galaxy_S9)
    |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | 12 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S9](https://en.wikipedia.org/wiki/Samsung_Galaxy_S9)
    |'
- en: '|  | 2019 | 16 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S10](https://en.wikipedia.org/wiki/Samsung_Galaxy_S10)
    |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | 16 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S10](https://en.wikipedia.org/wiki/Samsung_Galaxy_S10)
    |'
- en: '|  | 2020 | 108 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S20](https://en.wikipedia.org/wiki/Samsung_Galaxy_S20)
    |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | 108 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S20](https://en.wikipedia.org/wiki/Samsung_Galaxy_S20)
    |'
- en: '|  | 2021 | 108 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S21](https://en.wikipedia.org/wiki/Samsung_Galaxy_S21)
    |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | 108 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S21](https://en.wikipedia.org/wiki/Samsung_Galaxy_S21)
    |'
- en: '|  | 2022 | 108 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S22](https://en.wikipedia.org/wiki/Samsung_Galaxy_S22)
    |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | 108 | [https://en.wikipedia.org/wiki/Samsung_Galaxy_S22](https://en.wikipedia.org/wiki/Samsung_Galaxy_S22)
    |'
- en: '| Microsoft HoloLens Camera | 2016 | 2.4 | [https://docs.microsoft.com/en-us/hololens/hololens1-hardware](https://docs.microsoft.com/en-us/hololens/hololens1-hardware)
    |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 微软HoloLens相机 | 2016 | 2.4 | [https://docs.microsoft.com/en-us/hololens/hololens1-hardware](https://docs.microsoft.com/en-us/hololens/hololens1-hardware)
    |'
- en: '|  | 2019 | 8 | [https://www.microsoft.com/en-us/hololens/hardware](https://www.microsoft.com/en-us/hololens/hardware)
    |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | 8 | [https://www.microsoft.com/en-us/hololens/hardware](https://www.microsoft.com/en-us/hololens/hardware)
    |'
- en: '| Raspberry Pi Camera | 2013 | 2.1 | [https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories](https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories)
    |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 树莓派相机 | 2013 | 2.1 | [https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories](https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories)
    |'
- en: '|  | 2016 | 8 | [https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories](https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories)
    |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | 8 | [https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories](https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories)
    |'
- en: '|  | 2020 | 12.3 | [https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories](https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories)
    |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | 12.3 | [https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories](https://en.wikipedia.org/wiki/Raspberry_Pi#Accessories)
    |'
- en: '| DJI Phantom Camera | 2012 | 12 | [https://en.wikipedia.org/wiki/GoPro#HERO3_(White/Silver/Black)](https://en.wikipedia.org/wiki/GoPro#HERO3_(White/Silver/Black))
    |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| DJI Phantom相机 | 2012 | 12 | [https://en.wikipedia.org/wiki/GoPro#HERO3_(White/Silver/Black)](https://en.wikipedia.org/wiki/GoPro#HERO3_(White/Silver/Black))
    |'
- en: '|  | 2013 | 14 | [https://www.dji.com/dk/phantom-2-vision](https://www.dji.com/dk/phantom-2-vision)
    |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | 2013 | 14 | [https://www.dji.com/dk/phantom-2-vision](https://www.dji.com/dk/phantom-2-vision)
    |'
- en: '|  | 2014 | 14 | [https://www.dji.com/dk/phantom-2-vision-plus](https://www.dji.com/dk/phantom-2-vision-plus)
    |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  | 2014 | 14 | [https://www.dji.com/dk/phantom-2-vision-plus](https://www.dji.com/dk/phantom-2-vision-plus)
    |'
- en: '|  | 2015 | 12.4 | [https://www.dji.com/dk/phantom-3-pro](https://www.dji.com/dk/phantom-3-pro)
    |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  | 2015 | 12.4 | [https://www.dji.com/dk/phantom-3-pro](https://www.dji.com/dk/phantom-3-pro)
    |'
- en: '|  | 2016 | 20 | [https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones](https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones)
    |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | 20 | [https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones](https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones)
    |'
- en: '|  | 2017 | 20 | [https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones](https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones)
    |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | 20 | [https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones](https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones)
    |'
- en: '|  | 2018 | 20 | [https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones](https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones)
    |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | 20 | [https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones](https://en.wikipedia.org/wiki/Phantom_(UAV)#Current_Phantom_drones)
    |'
