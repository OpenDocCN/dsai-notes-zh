- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:57:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2101.01993] A Survey of Deep RL and IL for Autonomous Driving Policy Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2101.01993](https://ar5iv.labs.arxiv.org/html/2101.01993)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep RL and IL for Autonomous Driving Policy Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Zeyu Zhu,  Huijing Zhao This work is supported in part by the National Natural
    Science Foundation of China under Grant (61973004). The authors are with the Key
    Laboratory of Machine Perception (MOE) and with the School of Electronics Engineering
    and Computer Science, Peking University, Beijing 100871, China. Correspondence:
    H.Zhao, zhaohj@pku.edu.cn.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Autonomous driving (AD) agents generate driving policies based on online perception
    results, which are obtained at multiple levels of abstraction, e.g., behavior
    planning, motion planning and control. Driving policies are crucial to the realization
    of safe, efficient and harmonious driving behaviors, where AD agents still face
    substantial challenges in complex scenarios. Due to their successful application
    in fields such as robotics and video games, the use of deep reinforcement learning
    (DRL) and deep imitation learning (DIL) techniques to derive AD policies have
    witnessed vast research efforts in recent years. This paper is a comprehensive
    survey of this body of work, which is conducted at three levels: First, a taxonomy
    of the literature studies is constructed from the system perspective, among which
    five modes of integration of DRL/DIL models into an AD architecture are identified.
    Second, the formulations of DRL/DIL models for conducting specified AD tasks are
    comprehensively reviewed, where various designs on the model state and action
    spaces and the reinforcement learning rewards are covered. Finally, an in-depth
    review is conducted on how the critical issues of AD applications regarding driving
    safety, interaction with other traffic participants and uncertainty of the environment
    are addressed by the DRL/DIL models. To the best of our knowledge, this is the
    first survey to focus on AD policy learning using DRL/DIL, which is addressed
    simultaneously from the system, task-driven and problem-driven perspectives. We
    share and discuss findings, which may lead to the investigation of various topics
    in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: deep reinforcement learning, deep imitation learning, autonomous driving policy
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autonoumous driving (AD) has received extensive attention in recent decades
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)] and could be
    a promising solution for improving road safety [[5](#bib.bib5)], traffic flow
    [[6](#bib.bib6)] and fuel economy [[7](#bib.bib7)], among other factors. A typical
    architecture of an AD system is illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"), which is
    composed of perception, planning and control modules. An AD agent generates driving
    policies based on online perception results, which are obtained at multiple levels
    of abstraction, e.g., behavior planning, motion planning and control. The earliest
    autonomous vehicles can be dated back to [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)].
    One milestone was the Defense Advanced Research Projects Agency (DARPA) Grand
    Challenges [[11](#bib.bib11), [12](#bib.bib12)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec6dd15ea92264e4b6e86b955c0d57ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Architecture of autonomous driving systems. A general abstraction
    that is based on [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent years have witnessed a huge boost in AD research, and many products
    and prototyping systems have been developed. Despite the fast development of the
    field, AD still faces substantial challenges in complex scenarios for the realization
    of safe, efficient and harmonious driving behaviors [[17](#bib.bib17), [18](#bib.bib18)].
    Reinforcement learning (RL) is a principled mathematical framework for solving
    sequential decision making problems [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)].
    Imitation learning (IL), which is closely related, refers to learning from expert
    demonstrations. However, the early methods of both were limited to relatively
    low-dimensional problems. The rise of deep learning (DL) techniques [[22](#bib.bib22),
    [23](#bib.bib23)] in recent years has provided powerful solutions to this problem
    through the appealing properties of deep neural networks (DNNs): function approximation
    and representation learning. DL techniques enable the scaling of RL/IL to previously
    intractable problems (e.g., high-dimensional state spaces), which have increased
    in popularity for complex locomotion [[24](#bib.bib24)], robotics [[25](#bib.bib25)]
    and autonomous driving [[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)]
    tasks. Unless otherwise stated, this survey focuses on Deep RL (DRL) and Deep
    IL (DIL).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A large variety of DRL/DIL models have been developed for learning AD policies,
    which are reviewed in this paper. Several surveys are relevant to this study.
    [[13](#bib.bib13), [15](#bib.bib15)] survey the motion planning and control methods
    of automated vehicles before the era of DL. [[29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33)] review general DRL/DIL methods
    without considering any particular applications. [[4](#bib.bib4)] addresses the
    deep learning techniques for AD with a focus on perception and control, while
    [[34](#bib.bib34)] addresses control only. [[35](#bib.bib35)] provides a taxonomy
    of AD tasks to which DRL models have been applied and highlights the key challenges.
    However, none of these studies answers the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How can DRL/DIL models be integrated into AD systems from the perspective of
    system architecture? How can they be formulated to accomplish specified AD tasks?
    How can methods be designed that address the challenging issues of AD, such as
    safety, interaction with other traffic participants, and uncertainty of the environment?
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0241f3bfb377a3397cc0339b9791d01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A taxonomy of the general methods of reinforcement learning (RL)
    and imitation learning (IL)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This study seeks answers to the above questions. To the best of our knowledge,
    this is the first survey to focus on AD policy learning using DRL/DIL, which is
    addressed from the system, task-driven and problem-driven perspectives. Our contributions
    are threefold:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A taxonomy of the literature is presented from the system perspective, from
    which five modes of integration of DRL/DIL models into an AD architecture are
    identified. The studies on each mode are reviewed, and the architectures are compared.
    It is found that the vast research efforts have focused mainly on exploring the
    potential of DRL/DIL in accomplishing AD tasks, while intensive studies are needed
    on the optimization of the architectures of DRL/DIL embedded systems toward real-world
    applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The formulations of DRL/DIL models for accomplishing specified AD tasks are
    comprehensively reviewed, where various designs on the model state and action
    spaces and the reinforcement learning rewards are covered. It is found that these
    formulations rely heavily on empirical designs, which are brute-force approaches
    and lack theoretic support. Changing the designs or tuning the parameters could
    result in substantially different driving policies, which may pose large challenges
    to the AD system’s stability and robustness in real-world deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The critical issues of AD applications that are addressed by the DRL/DIL models
    regarding driving safety, interaction with other traffic participants and uncertainty
    of the environment are comprehensively discussed. It is found that driving safety
    has been well studied. A typical strategy is to combine with traditional methods
    to ensure a DRL/DIL agent’s functional safety; however, striking a balance between
    optimal policies and hard constraints remains non-trivial. The studies on the
    latter two issues are still highly preliminary, in which the problems have been
    addressed from divergent perspectives and the studies have not been conducted
    systematically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remainder of this paper is organized as follows: Sections [II](#S2 "II
    Preliminaries of Reinforcement Learning ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning") and [III](#S3 "III Preliminaries of Imitation Learning
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning") briefly
    introduce (D)RL and (D)IL, respectively. Section [IV](#S4 "IV Architectures of
    DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning") reviews the research on DRL/DIL in AD from a system architecture
    perspective. Section [V](#S5 "V Task-Driven Methods ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning") reviews the task-driven methods and
    examines the formulations of DRL/DIL models for completing specified AD tasks.
    Section [VI](#S6 "VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning") reviews problem-driven methods, in which specified autonomous
    vehicle problems and challenges are addressed. Section [VII](#S7 "VII Discussion
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning") discusses
    the remaining challenges, and the conclusions of the survey are presented in Section
    [VIII](#S8 "VIII Conclusions ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: II Preliminaries of Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement learning (RL) is a principled mathematical framework that is based
    upon the paradigm of trial-and-error learning, where an agent interacts with its
    environment through a trade-off between exploitation and exploration [[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38)]. Markov decision processes (MDPs) are a mathematically
    idealized form of RL [[21](#bib.bib21)], which are represented as $(\cal S,\cal
    A,\cal P,\cal R,\gamma)$, where $\cal S$ and $\cal A$ denote the sets of states
    and actions, respectively, and $\cal P$$(s_{t+1}|s_{t},a_{t}):\cal S\times\cal
    S\times\cal A\rightarrow$ $[0,1]$ is the transition/dynamics function that maps
    state-action pairs onto a distribution of next-step states. The numerical immediate
    reward function $\cal R$$(s_{t},a_{t},s_{t+1}):\cal S\times\cal A\times\cal S\rightarrow\mathbb{R}$
    serves as a learning signal. A discount factor $\gamma\in[0,1]$ determines the
    present value of future rewards (lower values encourage more myopic behaviors).
    MDPs’ states satisfy the Markov property [[21](#bib.bib21)], namely, future states
    depend only on the immediately preceding states and actions. Partially observable
    MDPs (POMDPs) extend MDPs to problems in which access to fully observable Markov
    property states is not available. A POMDP has an observation set $\Omega$ and
    an observation function $\cal O$, where $\cal O$$(a_{t},s_{t+1},o_{t+1})=p(o_{t+1}|a_{t},s_{t+1})$
    is the probability of observing $o_{t+1}$ given that the agent has executed action
    $a_{t}$ and reached state $s_{t+1}$ [[39](#bib.bib39)]. For the theory and algorithms
    of POMDPs, we refer readers to [[40](#bib.bib40), [39](#bib.bib39)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The MDP agent selects an action $a_{t}\in\cal A$ at each time step $t$ based
    on the current state $s_{t}$, receives a numerical reward $r_{t+1}$ and visits
    a new state $s_{t+1}$. The generated sequence $\{s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...\}$
    is called a rollout or trajectory. The expected cumulative reward in the future,
    namely, the expected discounted return $G_{t}$ after time step $t$, is defined
    as [[21](#bib.bib21)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{t}\doteq r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+...=\sum_{k=0}^{T}\gamma^{k}r_{t+k+1}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $T$ is a finite value or $\infty$ for finite and infinite horizon problems,
    respectively. The policy $\pi(a|s)$ maps states to probabilities of selecting
    each possible action. The value function $v_{\pi}(s)$ is the expected return following
    $\pi$ from state $s$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle v_{\pi}(s)\doteq\mathbb{E}_{\pi}[G_{t}&#124;s_{t}=s]$ |  |
    (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Similarly, the action-value function $q_{\pi}(s,a)$ is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $q_{\pi}(s,a)\doteq\mathbb{E}_{\pi}[G_{t}&#124;s_{t}=s,a_{t}=a]$ |  |
    (3) |'
  prefs: []
  type: TYPE_TB
- en: 'which satisfies the recursive Bellman equation [[41](#bib.bib41)] :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $q_{\pi}(s_{t},a_{t})=\mathbb{E}_{s_{t+1}}[r_{t+1}+\gamma q_{\pi}(s_{t+1},\pi(s_{t+1}))]$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: The objective of RL is to identify the optimal policy that maximizes the expected
    return $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\pi}[G_{t}|s_{t}=s]$. The methods can
    be divided into three groups, as shown in Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning") (a).
  prefs: []
  type: TYPE_NORMAL
- en: II-B Value-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To solve a reinforcement learning problem, one can identify an optimal action-value
    function and recover the optimal policy from the learned state-action values.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q_{\pi^{*}}(s,a)=\max_{\pi}q_{\pi}(s,a)$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\pi^{*}}(s)=\arg\max_{a}q_{\pi^{*}}(s,a)$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'Q-learning [[42](#bib.bib42)] is one of the most popular methods, which estimates
    Q values through temporal difference (TD):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q_{\pi}(s_{t},a_{t})$ | $\displaystyle\leftarrow$ | $\displaystyle
    q_{\pi}(s_{t},a_{t})+\alpha(Y-q_{\pi}(s_{t},a_{t}))$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\displaystyle Y=r_{t+1}+\gamma\max\limits_{a_{t+1}\in\cal A}q_{\pi}(s_{t+1},a_{t+1})$
    is the temporal difference target and $\alpha$ is the learning rate. This can
    be regarded as a standard regression problem in which the error to be minimized
    is $Y-q_{\pi}(s_{t},a_{t})$. Q-learning is off-policy since it updates $q_{\pi}$
    based on experiences that are not necessarily generated by the derived policy,
    while SARSA [[43](#bib.bib43)] is an on-policy algorithm that uses the derived
    policy to generate experiences. Another distinction is that SARSA uses target
    $Y=r_{t+1}+\gamma q_{\pi}(s_{t+1},a_{t+1})$. In contrast to TD methods, Monte
    Carlo methods estimate the expected return through averaging the results of multiple
    rollouts, which can be applied to non-Markovian episodic environments. TD and
    Monte Carlo have been further combined in TD($\lambda$) [[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The early methods [[42](#bib.bib42), [43](#bib.bib43), [21](#bib.bib21)] of
    this group rely on tabular representations. A major problem is the “curse of dimensionality”
    [[44](#bib.bib44)], namely, an increase in the number of state features would
    result in exponential growth of the number of state-action pairs that must be
    stored. Recent methods use DNNs to approximate a parameterized value function
    $q(s,a;\omega)$, of which Deep Q-networks (DQNs) [[45](#bib.bib45)] are the most
    representative, which learn the values by minimizing the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\displaystyle J(\omega)$ | $\displaystyle=$ | $\displaystyle\mathbb{E}_{t}[(Y-q(s_{t},a_{t};\omega))^{2}]$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $Y=r_{t+1}+\gamma\max q(s_{t+1},a_{t+1};\omega^{-})$ is the target, $\omega^{-}$
    denotes the parameters of the target network, and $\omega$ can be learnt based
    on the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\omega\leftarrow\omega-\alpha\mathbb{E}_{t}[(Y-q(s_{t},a_{t};\omega))\nabla
    q(s_{t},a_{t};\omega)]$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: The major contributions of DQN are the introduction of the target network and
    experience replay. To avoid rapidly fluctuating Q values and stabilize the training,
    the target network is fixed for a specified number of iterations during the update
    of the primary Q-network and subsequently updated to match the primary Q-network.
    Moreover, experience replay [[46](#bib.bib46)], which maintains a memory that
    stores transitions $(s_{t},a_{t},s_{t+1},r_{t+1})$, increases the sample efficiency.
    A later study improves the uniform sample experience replay by introducing priority
    [[47](#bib.bib47)]. Continuous DQN (cDQN) [[48](#bib.bib48)] derives a continuous
    variant of DQN based on normalized advantage functions (NAFs). Double DQN (DDQN)
    [[49](#bib.bib49)] addresses the overestimation problem of DQN through the use
    of a double estimator. Dueling-DQN [[50](#bib.bib50)] introduces a dueling architecture
    in which both the value function and associated advantage function are estimated.
    QR-DQN [[51](#bib.bib51)] utilizes distributional reinforcement learning to learn
    the full value distribution rather than only the expectation.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Policy-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alternatively, one can directly search and optimize a parameterized policy
    $\pi_{\theta}$ to maximize the expected return:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta}J(\theta)=\max_{\theta}v_{\pi_{\theta}}(s)=\max_{\theta}\mathbb{E}_{\pi_{\theta}}[G_{t}&#124;s_{t}=s]$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\theta$ denotes the policy parameters, which can be optimized based
    on the policy gradient theorem [[21](#bib.bib21)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\displaystyle\nabla J(\theta)$ | $\displaystyle\propto$
    | $\displaystyle\sum_{s}\mu(s)\sum_{a}q_{\pi}(s,a)\nabla\pi_{\theta}(a&#124;s)$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{\pi}[\sum_{a}q_{\pi}(s_{t},a)\nabla\pi_{\theta}(a&#124;s_{t})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{\pi}[G_{t}\nabla\ln\pi_{\theta}(a_{t}&#124;s_{t})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mu(s)$ denotes the state visitation frequency. REINFORCE [[52](#bib.bib52)]
    is a straightforward Monte Carlo policy-based method that selects the discounted
    return $G_{t}$ following the policy $\pi_{\theta}$ to estimate the policy gradient
    in Eqn.[11](#S2.E11 "In II-C Policy-based Methods ‣ II Preliminaries of Reinforcement
    Learning ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning").
    The parameters are updated as follows [[21](#bib.bib21)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta\leftarrow\theta+\alpha G_{t}\nabla\ln\pi_{\theta}(a_{t}&#124;s_{t})$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: This update intuitively increases the log probability of actions that lead to
    higher returns. Since empirical returns are used, the resulting gradients suffer
    from high variances. A common technique for reducing the variance and accelerating
    the learning is to replace $G_{t}$ in Eqn. [11](#S2.E11 "In II-C Policy-based
    Methods ‣ II Preliminaries of Reinforcement Learning ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning") and [12](#S2.E12 "In II-C Policy-based
    Methods ‣ II Preliminaries of Reinforcement Learning ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning") by $G_{t}-b(s_{t})$ [[52](#bib.bib52),
    [21](#bib.bib21)], where $b(s_{t})$ is a baseline. Alternatively, $G_{t}$ can
    be replaced by the advantage function [[53](#bib.bib53), [54](#bib.bib54)] $A_{\pi_{\theta}}(s,a)=q_{\pi_{\theta}(s,a)}-v_{\pi_{\theta}}(s)$.
  prefs: []
  type: TYPE_NORMAL
- en: One problem of policy-based methods is poor gradient updates may result in newly
    updated policies that deviate wildly from previous policies, which may decrease
    the policy performance. Trust region policy optimization (TRPO) [[55](#bib.bib55)]
    solves this problem through optimization of a surrogate objective function, which
    guarantees the monotonic improvement of policy performance. Each policy gradient
    update is constrained by using a quadratic approximation of the Kullback-Leibler
    (KL) divergence between policies. Proximal policy optimization (PPO) [[56](#bib.bib56)]
    improved upon TRPO through the application of an adaptive penalty on the KL divergence
    and a heuristic clipped surrogate objective function. The requirement for only
    a first-order gradient also renders PPO easier to implement than TRPO.
  prefs: []
  type: TYPE_NORMAL
- en: II-D Actor-Critic Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Actor-critic methods have the advantages of both value-based and policy-based
    methods, where a neural network actor $\pi_{\theta}(a|s)$ selects actions and
    a neural network critic $q(s,a;\omega)$ or $v(s;\omega)$ estimates the values.
    The actor and critic are typically updated alternately according to Eqn. [11](#S2.E11
    "In II-C Policy-based Methods ‣ II Preliminaries of Reinforcement Learning ‣ A
    Survey of Deep RL and IL for Autonomous Driving Policy Learning") and Eqn. [8](#S2.E8
    "In II-B Value-based Methods ‣ II Preliminaries of Reinforcement Learning ‣ A
    Survey of Deep RL and IL for Autonomous Driving Policy Learning"), respectively.
    Deterministic policy gradient (DPG) [[57](#bib.bib57)] is an off-policy actor-critic
    algorithm that derives deterministic policies. Compared with stochastic policies,
    DPG only integrates over the state space and requires fewer samples in problems
    with large action spaces. Deep deterministic policy gradient (DDPG) [[24](#bib.bib24)]
    utilizes DNNs to operate on high-dimensional state spaces with experience replay
    and a separate actor-critic target network, which is similar to DQN. Exploitation
    of parallel computation is an alternative to experience replay. Asynchronous advantage
    actor-critic (A3C) [[58](#bib.bib58)] uses advantage estimates rather than discounted
    returns in the actor-critic framework and asynchronously updates policy and value
    networks on multiple parallel threads of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: The parallel independent environments stabilize the learning process and enable
    more exploration. Advantage actor critic (A2C) [[59](#bib.bib59)], which is the
    synchronous version of A3C, uses a single agent for simplicity or waits for each
    agent to finish its experience to collect multiple trajectories. Soft actor critic
    (SAC) [[60](#bib.bib60)] benefits from the addition of an entropy term to the
    reward function to encourage better exploration.
  prefs: []
  type: TYPE_NORMAL
- en: III Preliminaries of Imitation Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imitation learning possesses a simpler form and is based on learning from demonstrations
    (LfD) [[61](#bib.bib61)]. It is attractive for AD applications, where interaction
    with the real environment could be dangerous and vast amount of human driving
    data can be easily collected [[62](#bib.bib62)]. A demonstration dataset ${\cal
    D}=\{\xi_{i}\}_{i=0..N}$ contains a set of trajectories, where each trajectory
    $\xi_{i}=\{(s_{t}^{i},a_{t}^{i})\}_{t=0..T}$ is a sequence of state-action pairs,
    and action $a_{t}^{i}\in\cal A$ is taken by expert at state $s_{t}^{i}\in S\cal$
    under the guidance of an underlying policy $\pi_{E}$ of the expert [[32](#bib.bib32)].
    Using the collected dataset $\cal D$, a common optimization-based strategy of
    imitation learning is to learn a policy $\pi^{*}:\cal S\rightarrow\cal A$ that
    mimics the expert’s policy $\pi_{E}$ by satisfying [[33](#bib.bib33)]
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi^{*}=\arg\min_{\pi}\mathbb{D}(\pi_{E},\pi)$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{D}$ is a similarity measure between policies. The methods for
    solving the problem can be divided into three groups, as shown in Fig. [2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning") (b), which are reviewed below.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Behavior Clone
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Behavior clone (BC) formulates the problem as a supervised learning process
    with the objective of matching the learned policy $\pi_{\theta}$ to the expert
    policy $\pi_{E}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\mathbb{E}&#124;&#124;\pi_{\theta}-\pi_{E}&#124;&#124;_{2}$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 'which is typically realized by minimizing the L2-loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J(\theta)=\mathbb{E}_{(s,a)\sim\cal D}[(\pi_{\theta}(s)-a)^{2}]$ |  |
    (15) |'
  prefs: []
  type: TYPE_TB
- en: Early research on imitation learning can be dated back to the ALVINN system
    [[10](#bib.bib10)], which used a 3-layer neural network to perform road following
    based on front camera images. In the most recent decade, deep imitation learning
    (DIL) has been conducted using DNNs as policy function approximators and has realized
    success in end-to-end AD systems [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)].
    BC performs well for states that are covered by the training distribution but
    generalizes poorly to new states due to compounding errors in the actions, which
    is also referred to as covariate shift [[66](#bib.bib66), [67](#bib.bib67)]. To
    overcome this problem, Ross et al. [[68](#bib.bib68)] proposed DAgger, which improves
    upon supervised learning by using a primary policy to collect training examples
    while running a reference policy simultaneously. In each iteration, states that
    are visited by the primary policy are also sent to the reference policy to output
    expert actions, and the newly generated demonstrations are aggregated into the
    training dataset. SafeDAgger [[69](#bib.bib69)] extends on DAgger by introducing
    a safe policy that learns to predict the error that is made by a primary policy
    without querying the reference policy. In addition to dataset aggregation, data
    augmentation techniques such as the addition of random noise to the expert action
    have also been commonly used in DIL [[70](#bib.bib70), [71](#bib.bib71)].
  prefs: []
  type: TYPE_NORMAL
- en: III-C Inverse Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The inverse reinforcement learning problem, which was first formulized in the
    study of Ng et al. [[72](#bib.bib72)], is to identify a reward function $r_{\theta}$
    for which the expert behavior is optimal:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta}\mathbb{E}_{\pi_{E}}[G_{t}&#124;r_{\theta}]-\mathbb{E}_{\pi}[G_{t}&#124;r_{\theta}]$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'Early studies utilized linear function approximation of reward functions and
    identified the optimal reward via maximum margin approaches [[73](#bib.bib73),
    [74](#bib.bib74)]. By introducing the maximum entropy principle, Ziebart et al.
    [[75](#bib.bib75)] eliminated the reward ambiguity between demonstrations and
    expert policy where multiple rewards may explain the expert behavior. The reward
    function is learned through maximizing the posterior probability of observing
    expert trajectories:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J(\theta)=\mathbb{E}_{\xi_{i}\sim\cal D}[\log P(\xi_{i}&#124;r_{\theta})]$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: where the probability of a trajectory satisfies $P(\xi_{i}|r_{\theta})\propto\exp(r_{\theta}(\xi_{i}))$.
    Several studies have extended the reward functions to nonlinear formulations through
    Gaussian processes [[76](#bib.bib76)] or boosting [[77](#bib.bib77), [78](#bib.bib78)].
    However, the above methods generally operate on low-dimensional features. The
    use of rich and expressive function approximators, in the form of neural networks,
    has been proposed to learn reward functions directly on raw high-dimensional state
    representations [[79](#bib.bib79), [80](#bib.bib80)].
  prefs: []
  type: TYPE_NORMAL
- en: A problem that is encountered with IRL is that to evaluate the reward function,
    a forward reinforcement learning process must be conducted to obtain the corresponding
    policy, thereby rendering IRL inefficient and computationally expensive. Many
    early approaches require solving an MDP in the inner loop of each iterative optimization
    [[72](#bib.bib72), [20](#bib.bib20), [74](#bib.bib74), [75](#bib.bib75)]. Perfect
    knowledge of the system dynamics and an efficient offline solver are needed in
    these methods, thereby limiting their applications in complex real-world scenarios
    such as robotic control. Finn et al. [[80](#bib.bib80)] proposed guided cost learning
    (GCL), which handles unknown dynamics in high-dimensional complex systems and
    learns complex neural network cost functions through an efficient sample-based
    approximation.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Generative Adversarial Imitation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generative adversarial imitation learning (GAIL) [[81](#bib.bib81)] directly
    learns a policy from expert demonstrations while requiring neither the reward
    design in RL nor the expensive RL process in the inner loop of IRL. GAIL establishes
    an analogy between imitation learning and generative adversarial networks (GANs)
    [[82](#bib.bib82)]. The generator $\pi_{\theta}$ serves as a policy for imitating
    expert behavior by matching the state-action distribution of demonstrations, while
    the discriminator $D_{\omega}\in(0,1)$ serves as a surrogate reward function for
    measuring the similarity between generated and demonstration samples. The objective
    function of GAIL is formulated in the following min-max form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min\limits_{\pi_{\theta}}\max\limits_{D_{\omega}}{\mathbb{E}}_{\pi_{\theta}}[\log
    D_{\omega}(s,a)]+{\mathbb{E}}_{\pi_{E}}[\log(1-D_{\omega}(s,a))]-\lambda H(\pi_{\theta})$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'where $H(\pi)$ is a regularization entropy term. The generator and the discriminator
    are updated with the following gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\theta}J(\theta)={\mathbb{E}}_{\pi}[\nabla_{\theta}\log\pi_{\theta}(a&#124;s)Q(s,a)]-\lambda\nabla_{\theta}H(\pi_{\theta})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\nabla_{\omega}J(\omega)={\mathbb{E}}_{\pi}[\nabla_{\omega}\log
    D_{\omega}(s,a)]+{\mathbb{E}}_{\pi_{E}}[\nabla_{\omega}\log(1-D_{\omega}(s,a))]$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: Finn et al. [[83](#bib.bib83)] mathematically proved the connection among GANs,
    IRL and energy-based models. Fu et al.[[84](#bib.bib84)] proposed adversarial
    inverse reinforcement learning (AIRL) based on an adversarial reward learning
    formulation, which can recover reward functions that are robust to dynamics changes.
  prefs: []
  type: TYPE_NORMAL
- en: IV Architectures of DRL/DIL Integrated AD Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AD systems have been studied for decades [[13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)], which are commonly composed of modular pipelines,
    as illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A Survey of Deep
    RL and IL for Autonomous Driving Policy Learning"). How can DRL/DIL models be
    integrated into an AD system and collaborate with other modules? This section
    reviews the literature from the system architecture perspective, from which five
    modes are identified, as illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"). A classification of the studies in each mode is presented in
    Table [I](#S4.T1 "TABLE I ‣ IV Architectures of DRL/DIL Integrated AD Systems
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"), along with
    the exploited DRL/DIL methods, the upstream module for perception, the targeted
    AD tasks, and the advantages and disadvantages of the architectures, among other
    information. Below, we detail each mode of the architectures, which is followed
    by a comparison of the number of studies that correspond to each mode or to the
    use of DRL or DIL methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b5e6df58cd7d42d0eab57d3c807e38d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Integration modes of DRL/DIL models into AD architectures'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: A taxonomy of the literature by the integration modes of DRL or DIL
    models into AD architectures'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Advantages | Disadvantages | Studies | Methods | Perception¹
    | Tasks² | Remarks³ |'
  prefs: []
  type: TYPE_TB
- en: '| Mode 1. DRL/DIL Integrated Control | - Features a simple structure and adapts
    to various learning methods. | - Limited to specified tasks or skills. - Bypassing
    planning modules weakens the model’s interpretability and capability. | [[10](#bib.bib10)],
    [[85](#bib.bib85)], [[63](#bib.bib63)], [[64](#bib.bib64)], [[86](#bib.bib86)],
    [[87](#bib.bib87)], [[65](#bib.bib65)], [[88](#bib.bib88)] | BC | D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; road/lane following &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; urban driving &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; safety: [[69](#bib.bib69)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[69](#bib.bib69)], [[89](#bib.bib89)] | DAgger | D | road/lane following
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[90](#bib.bib90)], [[91](#bib.bib91)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[92](#bib.bib92)], [[93](#bib.bib93)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[94](#bib.bib94)], [[95](#bib.bib95)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NQL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DQN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| T |'
  prefs: []
  type: TYPE_TB
- en: '&#124; lane changing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; traffic merging &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; imminent events &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; intersection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; safety: [[90](#bib.bib90)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; interaction: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[95](#bib.bib95)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[96](#bib.bib96)], [[97](#bib.bib97)] | PPO | T |'
  prefs: []
  type: TYPE_TB
- en: '&#124; traffic merging &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; urban driving &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| [[28](#bib.bib28)], [[98](#bib.bib98)] | DDPG | D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; road/lane following &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; imminent events &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| [[99](#bib.bib99)], [[100](#bib.bib100)], [[101](#bib.bib101)], [[102](#bib.bib102)],
    [[103](#bib.bib103)] | DDPG | T |'
  prefs: []
  type: TYPE_TB
- en: '&#124; road/lane following &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; lane changing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; overtaking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; imminent events &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Mode 2. Extension of Mode 1 with High-level Command | - Considers both high-level
    planning and perception - Generates distinct control behaviors according to the
    high-level decisions | - Single model may not capture sufficiently diverse behaviors
    - Learning a model for each behavior increases the training cost and the demand
    for data | [[104](#bib.bib104)] | BC | D | urban driving |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[105](#bib.bib105)], [[70](#bib.bib70)], [[106](#bib.bib106)] | CIL | D
    | urban navigation |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[107](#bib.bib107)], [[108](#bib.bib108)] | UAIL | D | urban navigation
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; uncertainty: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[107](#bib.bib107)], [[108](#bib.bib108)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[109](#bib.bib109)] | DDPG | T | parking |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[26](#bib.bib26)] | DDPG | D | urban navigation |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[110](#bib.bib110)] | DDQN/TD3/SAC | T | roundabout |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mode 3. DRL/DIL Integrated Motion Planning | - Learn to imitate human trajectories
    - Efficient forward prediction process | - No guarantee on safety or feasibility
    | [[111](#bib.bib111)], [[71](#bib.bib71)] | BC | T | urban driving |'
  prefs: []
  type: TYPE_TB
- en: '&#124; safety: [[71](#bib.bib71)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[112](#bib.bib112)] | DAgger | T | highway driving |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[113](#bib.bib113)] | MaxEnt DIRL | D | urban driving |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[114](#bib.bib114)] | DDQN/DQfD | T | valet parking |'
  prefs: []
  type: TYPE_TB
- en: '&#124; safety: [[114](#bib.bib114)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[115](#bib.bib115)] | SAC | T | traffic merge |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mode 4. DRL/DIL Integrated Behavior Planning | - The DNNs need only plan
    high- level behavioral actions. | - Simple and few actions limit the control precision
    and diversity of driving styles. - Complicated and too many actions increase the
    training cost. | [[116](#bib.bib116)] | AIRL | T | lane change |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[117](#bib.bib117)] | IRL | D | lane change |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[118](#bib.bib118)], [[119](#bib.bib119)], [[120](#bib.bib120)], [[121](#bib.bib121)],
    [[122](#bib.bib122)], [[123](#bib.bib123)], [[124](#bib.bib124)], [[125](#bib.bib125)]
    | DQN | T |'
  prefs: []
  type: TYPE_TB
- en: '&#124; lane change &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; intersection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| [[126](#bib.bib126)], [[127](#bib.bib127)] | DQN | D | lane change |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[128](#bib.bib128)], [[129](#bib.bib129)] | Q-Learning | T | lane changing
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[130](#bib.bib130)], [[131](#bib.bib131)] | DDQN | T |'
  prefs: []
  type: TYPE_TB
- en: '&#124; lane changing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; urban driving &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| [[132](#bib.bib132)] | DQfD | T | lane changing |'
  prefs: []
  type: TYPE_TB
- en: '&#124; safety: [[132](#bib.bib132)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[133](#bib.bib133)] | QR-DQN | D | highway driving |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mode 5. DRL/DIL Integrated Hierarchical Planning and Control | - Simultaneously
    plan at various levels of abstraction. | - Hierarchical DNNs increase the training
    cost and decrease the convergence speed. | [[134](#bib.bib134)], [[135](#bib.bib135)]
    | DQN | T |'
  prefs: []
  type: TYPE_TB
- en: '&#124; cruise control &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; lane changing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| [[136](#bib.bib136)] | Hierarchical Policy gradient | T | traffic light passing
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[27](#bib.bib27)] | DDPG | D | lane changing |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[137](#bib.bib137)] | DDPG | T | intersection |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[138](#bib.bib138)] | DDPG | T | urban driving |  |'
  prefs: []
  type: TYPE_TB
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Type of upstream perception module: (D)eep learning method/(T)raditional method'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For detailed information about AD tasks, see Table [III](#S4.T3 "TABLE III ‣
    IV-C Mode 3\. DRL/DIL Integrated Motion Planning ‣ IV Architectures of DRL/DIL
    Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy
    Learning")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Studies that also address safety [VI-A](#S6.SS1 "VI-A Safety-enhanced DRL/DIL
    for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning"), interaction [VI-B](#S6.SS2 "VI-B Interaction-aware
    DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning") and uncertainty [VI-C](#S6.SS3 "VI-C Uncertainty-aware
    DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning") problems are labelled.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IV-A Mode 1\. DRL/DIL Integrated Control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many studies have applied DRL/DIL to control, which can be abstracted as the
    architecture of Mode 1 and is illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"). Bojarski et al. [[63](#bib.bib63)] proposed a well-known end-to-end
    self-driving control framework. They trained a nine-layer CNN by supervised learning
    to learn the steering policy without explicit manual decomposition of sequential
    modules. However, their model only adapts to lane keeping. An alternative option
    is to feed traditional perception results into the DNN control module. Tang et
    al. [[96](#bib.bib96)] proposed the use of environmental rasterization encoding,
    along with the relative distance, velocity and acceleration, as input to a two-branch
    neural network, which was trained via proximal policy optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Although Mode 1 features a simple structure and adapts to a large variety of
    learning methods, it is limited to specified tasks; thus, it has difficulty addressing
    scenarios in which multiple driving skills that are conditioned on various situations
    are needed. Moreover, bypassing and ignoring behavior planning or motion planning
    processes may weaken the model’s interpretability and performance in complex tasks
    (e.g., urban navigation).
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Mode 2\. Extension of Mode 1 with High-level Command
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated
    AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"),
    Mode 2 extends Mode 1 by considering the high-level planning output. The control
    module is composed of either a general model for all behaviors [[104](#bib.bib104),
    [110](#bib.bib110)] or a series of models for distinct behaviors [[105](#bib.bib105),
    [70](#bib.bib70), [26](#bib.bib26), [106](#bib.bib106), [108](#bib.bib108)]. Chen
    et al. [[110](#bib.bib110)] projected detected environment vehicles and the routing
    onto a bird-view road map, which served as the input of a policy network. Liang
    et al. [[26](#bib.bib26)] built on conditional imitation learning (CIL) [[70](#bib.bib70)]
    and proposed the branched actor network, as illustrated in Fig. [3](#S4.F3 "Figure
    3 ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning"). These methods learn several control
    submodules for distinct behaviors. A gating control command from high-level route
    planning and behavior planning modules is responsible for the selection of the
    corresponding control submodule.
  prefs: []
  type: TYPE_NORMAL
- en: Although Mode 2 mitigates the problems that are encountered with Mode 1, it
    has its own limitations. A general model may be not sufficient for capturing diverse
    behaviors. However, learning a model for each behavior increases the demand for
    training data. Moreover, Mode 2 may be not as computationally efficient as Mode
    1 since it requires high-level planning modules that are determined in advance
    to guide the control module.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Mode 3\. DRL/DIL Integrated Motion Planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mode 3 integrates DRL/DIL into the motion planning module, and its architecture
    is illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated
    AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning").
    Utilizing the planning output (e.g., routes and driving decisions) from high-level
    modules, along with the current perception output, DNNs are trained to predict
    future trajectories or paths. DIL models [[111](#bib.bib111), [112](#bib.bib112),
    [113](#bib.bib113), [71](#bib.bib71)] are the mainstream choices for implementing
    this architecture. As illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"), Sun et al. [[112](#bib.bib112)] proposed training a neural
    network that imitates long-term MPC via Sampled-DAgger, where the policy network’s
    input was from perception (obstacles, environment, and current states) and decision-making
    (driving decisions). Alternatively, Wulfmeier et al. [[113](#bib.bib113)] proposed
    projecting the LiDAR point cloud onto a grid map, which is sent to the DNN. The
    DNN is responsible for learning a cost function that guides the motion planning.
    The control part in Mode 3 typically utilizes traditional control techniques such
    as PID [[71](#bib.bib71)] or MPC [[112](#bib.bib112)].
  prefs: []
  type: TYPE_NORMAL
- en: One major disadvantage of Mode 3 is that although DNN planned trajectories can
    imitate human trajectories, their safety and feasibility cannot be guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Comparison of the literature by DRL/DIL integration modes'
  prefs: []
  type: TYPE_NORMAL
- en: '| Perception |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Control &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Modes 1&2) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Motion Planning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Mode 3) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Behavior Planning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Mode 4) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hierarchical P. & C. ¹ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Mode 5) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DRL | DIL | DRL | DIL | DRL | DIL | DRL | DIL |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional | 15 | 0 | 2 | 3 | 13 | 1 | 5 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| DNN | 3 | 16 | 0 | 1 | 3 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Subtotal | 18 (52.9%) | 16 (47.1%) | 2 (33.3%) | 4 (66.7%) | 16 (88.9%) |
    2 (11.1%) | 6 (100%) | 0 (0%) |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 34 (53.1%) | 6 (9.4%) | 18 (28.1%) | 6 (9.4%) |'
  prefs: []
  type: TYPE_TB
- en: '*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The values in this table are the numbers and percentages of papers in Table
    [I](#S4.T1 "TABLE I ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning") that belong to the
    corresponding categories.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abbreviation for “Hierarchical Planning and Control”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE III: A taxonomy of the literature by scenarios and AD tasks'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | AD Task | Description | Ref. |'
  prefs: []
  type: TYPE_TB
- en: '| DRL Methods | DIL Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Urban | Intersection |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Learn to drive through intersections (while interacting &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and negotiating with other traffic participants). &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DQN [[119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [90](#bib.bib90)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cDQN [[138](#bib.bib138)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DDPG [[137](#bib.bib137), [138](#bib.bib138)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| — |'
  prefs: []
  type: TYPE_TB
- en: '| Roundabout |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Learn to drive through roundabouts (while interacting &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and negotiating with other traffic participants). &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DDQN [[110](#bib.bib110)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TD3 [[110](#bib.bib110)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SAC [[110](#bib.bib110)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Horizon GAIL[[139](#bib.bib139)] |'
  prefs: []
  type: TYPE_TB
- en: '| Urban Navigation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Learn to drive in urban environments to reach specified &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; objectives by following global routes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| DDPG [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; BC [[104](#bib.bib104)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CIL [[105](#bib.bib105), [70](#bib.bib70), [106](#bib.bib106)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; UAIL [[107](#bib.bib107), [108](#bib.bib108)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Urban Driving | Learn to drive in urban environments without specified objectives.
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DDQN [[122](#bib.bib122)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PPO [[140](#bib.bib140)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Policy gradient [[136](#bib.bib136)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BC [[65](#bib.bib65), [111](#bib.bib111)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DIRL [[113](#bib.bib113)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Highway | Lane Change (LC) | Learn to decide and perform lane changes. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DQN [[118](#bib.bib118), [91](#bib.bib91)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DDQN [[141](#bib.bib141)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DDPG [[27](#bib.bib27)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Projection IRL [[117](#bib.bib117)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AIRL [[116](#bib.bib116)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Lane Keep (LK) | Learn to drive while maintaining the current lane. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DQN [[102](#bib.bib102)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DDPG [[28](#bib.bib28)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BC [[86](#bib.bib86), [87](#bib.bib87)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SafeDAgger [[69](#bib.bib69)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cruise Control | Learn a policy for adaptive cruise control. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; NQL [[92](#bib.bib92)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DQN [[134](#bib.bib134)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Policy gradient [[142](#bib.bib142)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Actor-critic [[143](#bib.bib143), [144](#bib.bib144)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| — |'
  prefs: []
  type: TYPE_TB
- en: '| Traffic Merging | Learn to merge into highways. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DQN [[93](#bib.bib93)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PPO [[96](#bib.bib96)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| — |'
  prefs: []
  type: TYPE_TB
- en: '| Highway Driving |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Learn a general policy for driving on a highway, which &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; may include multiple behaviors such as LC and LK. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DQN [[145](#bib.bib145), [126](#bib.bib126)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; QR-DQN [[133](#bib.bib133)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAIL [[146](#bib.bib146)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PS-GAIL [[147](#bib.bib147)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MaxEnt IRL [[148](#bib.bib148)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Others | Road Following | Learn to simply follow one road. | — |'
  prefs: []
  type: TYPE_TB
- en: '&#124; BC [[10](#bib.bib10), [85](#bib.bib85), [63](#bib.bib63), [64](#bib.bib64)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DAgger [[89](#bib.bib89)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Imminent Events | Learn to avoid or mitigate imminent events such as collisions.
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DQN [[94](#bib.bib94)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DDPG [[98](#bib.bib98)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RAIL [[149](#bib.bib149)] |'
  prefs: []
  type: TYPE_TB
- en: IV-D Mode 4\. DRL/DIL Integrated Behavior Planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many studies focus on integrating DRL/DIL into the behavior planning module
    and deriving high-level driving policies. The corresponding architecture is presented
    in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"), where DNNs
    decide behavioral actions and the subsequent motion planning and control modules
    typically utilize traditional methods. Many studies build upon DQN and its variants
    [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [126](#bib.bib126), [122](#bib.bib122), [124](#bib.bib124), [125](#bib.bib125),
    [123](#bib.bib123)]. As illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"), Yuan et al. [[126](#bib.bib126)] decomposed the action space
    into five typical behavioral decisions on a highway. Compared with DRL, studies
    that employ DIL to learn high-level policies are limited. A recent study by Wang
    et al. [[116](#bib.bib116)] proposed the use of augmented adversarial inverse
    reinforcement learning (AIRL) to learn human-like decision-making on highways,
    where the action space consists of all possible combinations of lateral and longitudinal
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In Mode 4, the design of behavioral actions is nontrivial, and one must balance
    the training cost and the diversity of driving styles. Simple and few behavioral
    actions limits the control precision and diversity of driving styles, whereas
    many and sophisticated actions increases the training cost.
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Mode 5\. DRL/DIL Integrated Hierarchical Planning and Control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE IV: Inputs of the DRL/DIL methods for AD tasks'
  prefs: []
  type: TYPE_NORMAL
- en: '| Information Source | Class | Inputs | Ref. |'
  prefs: []
  type: TYPE_TB
- en: '| DRL Methods | DIL Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Ego Vehicle |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Position Information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ego position | [[110](#bib.bib110), [120](#bib.bib120), [90](#bib.bib90),
    [136](#bib.bib136), [122](#bib.bib122), [93](#bib.bib93), [91](#bib.bib91)] |
    [[147](#bib.bib147), [146](#bib.bib146), [149](#bib.bib149), [111](#bib.bib111)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Heading Information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; heading angle, orientation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; steering, yaw, and yaw rate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[119](#bib.bib119), [121](#bib.bib121), [93](#bib.bib93), [91](#bib.bib91),
    [28](#bib.bib28), [138](#bib.bib138)] | [[147](#bib.bib147), [65](#bib.bib65),
    [64](#bib.bib64), [146](#bib.bib146), [149](#bib.bib149), [139](#bib.bib139)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Speed Information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| speed/velocity and acceleration |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[138](#bib.bib138), [118](#bib.bib118), [122](#bib.bib122), [91](#bib.bib91),
    [141](#bib.bib141), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [137](#bib.bib137), [90](#bib.bib90), [136](#bib.bib136), [26](#bib.bib26)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[93](#bib.bib93), [102](#bib.bib102), [28](#bib.bib28), [150](#bib.bib150),
    [94](#bib.bib94), [144](#bib.bib144), [134](#bib.bib134), [98](#bib.bib98)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[139](#bib.bib139), [117](#bib.bib117), [107](#bib.bib107), [108](#bib.bib108),
    [106](#bib.bib106), [105](#bib.bib105), [70](#bib.bib70), [65](#bib.bib65)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[147](#bib.bib147), [148](#bib.bib148), [146](#bib.bib146), [149](#bib.bib149),
    [89](#bib.bib89)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Road Environment | Pixel Data | camera RGB images | [[126](#bib.bib126),
    [145](#bib.bib145), [27](#bib.bib27), [133](#bib.bib133), [26](#bib.bib26), [28](#bib.bib28)]
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[69](#bib.bib69), [86](#bib.bib86), [87](#bib.bib87), [107](#bib.bib107),
    [108](#bib.bib108), [106](#bib.bib106), [105](#bib.bib105), [70](#bib.bib70),
    [65](#bib.bib65), [104](#bib.bib104)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[10](#bib.bib10), [19](#bib.bib19), [85](#bib.bib85), [89](#bib.bib89),
    [63](#bib.bib63), [64](#bib.bib64)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; semantically segmented images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[98](#bib.bib98)] | — |'
  prefs: []
  type: TYPE_TB
- en: '| 2D bird’s-eye-view images | [[96](#bib.bib96)] | — |'
  prefs: []
  type: TYPE_TB
- en: '| Point Data | LiDAR sensor readings | [[126](#bib.bib126), [145](#bib.bib145),
    [133](#bib.bib133)] | [[147](#bib.bib147), [146](#bib.bib146), [149](#bib.bib149),
    [139](#bib.bib139)] |'
  prefs: []
  type: TYPE_TB
- en: '| 2D LiDAR grid map | — | [[113](#bib.bib113)] |'
  prefs: []
  type: TYPE_TB
- en: '| Object Data |'
  prefs: []
  type: TYPE_TB
- en: '&#124; other road users’ information: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; relative speed, position, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and distance to ego &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[141](#bib.bib141), [118](#bib.bib118), [119](#bib.bib119), [110](#bib.bib110),
    [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [138](#bib.bib138)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[93](#bib.bib93), [134](#bib.bib134), [142](#bib.bib142), [94](#bib.bib94),
    [92](#bib.bib92), [144](#bib.bib144), [143](#bib.bib143), [96](#bib.bib96)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[147](#bib.bib147), [149](#bib.bib149), [116](#bib.bib116), [111](#bib.bib111)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; lane/road information: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ego vehicle’s distance to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; lane markings, road &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; curvature, and lane width &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[118](#bib.bib118), [134](#bib.bib134), [137](#bib.bib137), [122](#bib.bib122),
    [93](#bib.bib93), [91](#bib.bib91), [102](#bib.bib102), [141](#bib.bib141)] |
    [[147](#bib.bib147), [146](#bib.bib146), [149](#bib.bib149), [116](#bib.bib116)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Task |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Navigation Information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; navigational driving commands &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; or planned routes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[110](#bib.bib110), [26](#bib.bib26)] | [[106](#bib.bib106), [105](#bib.bib105),
    [70](#bib.bib70), [104](#bib.bib104), [111](#bib.bib111)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Destination Information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; destination position, distance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; or angle to destination &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| — | [[139](#bib.bib139)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Traffic Rule Information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; traffic lights’ state, speed &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; limit, and desired speed &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[136](#bib.bib136)] | [[148](#bib.bib148), [111](#bib.bib111)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Prior Knowledge &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Road Map |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 2D top-down road map images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[110](#bib.bib110)] | [[111](#bib.bib111)] | ![Refer to caption](img/f73b681db058067cf0b5dac103ddf3ca.png)![Refer
    to caption](img/b36837e52cf72dbe151ee1cc0565a120.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: The percentages of the literature that uses certain data as input.
    (a) Comparison by DRL/DIL methods. (b) Comparison by scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated
    AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"),
    Mode 5 blurs the lines between planning and control, where a single DNN outputs
    hierarchical actions [[27](#bib.bib27)] based on the parameterized action space
    [[151](#bib.bib151)] or hierarchical DNNs output actions at multiple levels [[136](#bib.bib136),
    [134](#bib.bib134), [135](#bib.bib135)]. Rezaee et al. [[134](#bib.bib134)] proposed
    an architecture, which is illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"), in which BP (behavior planning) is used to make high-level
    decisions regarding transitions between discrete states and MoP (motion planning)
    generates a target trajectory according to the decisions that are made via BP.
    Qiao et al. [[137](#bib.bib137)] built upon hierarchical MDP (HMDP) and realized
    their model through an options framework. In their implementation, the hierarchical
    options were modeled as high-level decisions (SlowForward or Forward). Based on
    high-level decisions and current observations, low-level control policies were
    derived.
  prefs: []
  type: TYPE_NORMAL
- en: Mode 5 simultaneously plans at multiple levels, and the low-level planning process
    considers high-level decisions. However, the use of hierarchical DNNs results
    in the increase of the training cost and potentially the decrease of the convergence
    speed since one poorly trained high-level DNN may mislead and disturb the learning
    process of low-level DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: IV-F Statistical Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A statistical comparison among modes in terms of the number of studies is presented
    in Table [II](#S4.T2 "TABLE II ‣ IV-C Mode 3\. DRL/DIL Integrated Motion Planning
    ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning"). Current studies on architectures
    are premature, unsystematic and unbalanced:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most studies focus on integrating DRL/DIL into control (Mode1&2), followed by
    behavior planning (Mode 4).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Mode 1&2, DRL methods mainly adopt traditional method-based perception,
    while DNN-based perception is preferred in DIL methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DRL seems to be more popular for high-level decision making (Mode 4&5), while
    DIL is chosen more frequently for low-level control (Mode 1&2).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Future studies may address the imbalance problem and identify potential new
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: V Task-Driven Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DRL/DIL studies in AD can be categorized according to their application scenarios
    and targeted tasks, as listed in Table [III](#S4.T3 "TABLE III ‣ IV-C Mode 3\.
    DRL/DIL Integrated Motion Planning ‣ IV Architectures of DRL/DIL Integrated AD
    Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning").
    These task-driven studies use DRL/DIL to solve specified AD tasks, where the formulations
    of DRL/DIL can be decomposed into several key components: 1) state space and input
    design, 2) action space and output design, and 3) reinforcement learning reward
    design.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Actions of the DRL/DIL methods for AD tasks'
  prefs: []
  type: TYPE_NORMAL
- en: '| Action Category | Subclass | Action Outputs | Ref |'
  prefs: []
  type: TYPE_TB
- en: '| DRL Methods | DIL Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Behavior-level Actions |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Acceleration &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Related &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| e.g., full brake, decelerate, continue, and accelerate | [[119](#bib.bib119),
    [121](#bib.bib121), [122](#bib.bib122), [142](#bib.bib142), [145](#bib.bib145),
    [126](#bib.bib126), [133](#bib.bib133)] | — |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lane Change &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Related &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; e.g., keep, LLC, and RLC; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; choice of lane change gaps &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[122](#bib.bib122), [141](#bib.bib141), [118](#bib.bib118), [145](#bib.bib145),
    [133](#bib.bib133)] | [[116](#bib.bib116)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Turn &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Related &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| e.g., straight, left-turn, right-turn, and stop | [[126](#bib.bib126)] |
    [[117](#bib.bib117), [65](#bib.bib65)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Interaction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Related &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| e.g., take/give way and follow a vehicle; wait/go | [[120](#bib.bib120),
    [121](#bib.bib121)] | — |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Trajectory-level &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Actions &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Planned &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Trajectory &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| future path 2D points | — | [[113](#bib.bib113), [111](#bib.bib111)] |'
  prefs: []
  type: TYPE_TB
- en: '| Control-level Actions | Lateral | discrete steer angles | [[19](#bib.bib19)]
    | [[10](#bib.bib10)] |'
  prefs: []
  type: TYPE_TB
- en: '| continuous steer | — | [[87](#bib.bib87), [86](#bib.bib86), [85](#bib.bib85),
    [63](#bib.bib63), [64](#bib.bib64)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; continuous angular speed &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; or yaw acceleration &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[91](#bib.bib91)] | [[65](#bib.bib65)] |'
  prefs: []
  type: TYPE_TB
- en: '| Longitudinal | discrete acceleration values | [[90](#bib.bib90), [94](#bib.bib94)]
    | — |'
  prefs: []
  type: TYPE_TB
- en: '| continuous acceleration | [[92](#bib.bib92), [143](#bib.bib143), [144](#bib.bib144)]
    | — |'
  prefs: []
  type: TYPE_TB
- en: '| continuous brake and throttle | [[150](#bib.bib150)] | — |'
  prefs: []
  type: TYPE_TB
- en: '| Simultaneous Lateral & Longitudinal |'
  prefs: []
  type: TYPE_TB
- en: '&#124; continuous steer/turn-rate and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; speed/acceleration/throttle &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[93](#bib.bib93), [110](#bib.bib110), [96](#bib.bib96), [28](#bib.bib28)]
    | [[70](#bib.bib70), [104](#bib.bib104), [106](#bib.bib106), [89](#bib.bib89),
    [107](#bib.bib107), [146](#bib.bib146), [147](#bib.bib147)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; continuous steer, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; acceleration/throttle and brake &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[102](#bib.bib102), [26](#bib.bib26), [98](#bib.bib98)] | [[105](#bib.bib105),
    [108](#bib.bib108)] |'
  prefs: []
  type: TYPE_TB
- en: '| continuous steer and binary brake decision | — | [[69](#bib.bib69)] |'
  prefs: []
  type: TYPE_TB
- en: '| Hierarchical Actions | Behavior & Control |'
  prefs: []
  type: TYPE_TB
- en: '&#124; e.g., behavior-level pass/stop, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; control-level acceleration, and steer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[138](#bib.bib138), [137](#bib.bib137), [136](#bib.bib136), [27](#bib.bib27)]
    | — |'
  prefs: []
  type: TYPE_TB
- en: '| Behavior & Trajectory |'
  prefs: []
  type: TYPE_TB
- en: '&#124; e.g., behavior-level maintenance, LLC, RLC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and trajectory-level path points &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[134](#bib.bib134)] | — |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: Rewards of the DRL methods for AD tasks'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Subclass | Description | Ref. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Safety | Avoid Collision | Impose penalties if a collision occurs |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[118](#bib.bib118), [26](#bib.bib26), [105](#bib.bib105), [119](#bib.bib119),
    [122](#bib.bib122), [116](#bib.bib116), [110](#bib.bib110), [120](#bib.bib120),
    [121](#bib.bib121), [137](#bib.bib137), [90](#bib.bib90), [138](#bib.bib138)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[94](#bib.bib94), [143](#bib.bib143), [149](#bib.bib149), [96](#bib.bib96),
    [69](#bib.bib69), [126](#bib.bib126), [145](#bib.bib145), [133](#bib.bib133),
    [98](#bib.bib98)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Time to Collision |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Impose penalties if the time to collision (TTC) is below &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; a safe threshold &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [122](#bib.bib122),
    [144](#bib.bib144)] |'
  prefs: []
  type: TYPE_TB
- en: '| Distance to Other Vehicles |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Impose penalties if this distance is shorter than a safe threshold &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[134](#bib.bib134), [142](#bib.bib142), [144](#bib.bib144), [93](#bib.bib93)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Number of Lane Changes |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Impose penalties if the number of lane changes is too &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; large or reward a smaller number of lane changes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[118](#bib.bib118), [134](#bib.bib134), [126](#bib.bib126), [145](#bib.bib145),
    [133](#bib.bib133), [141](#bib.bib141)] |'
  prefs: []
  type: TYPE_TB
- en: '| Out of Road | Impose penalties on driving out of road | [[118](#bib.bib118),
    [27](#bib.bib27), [102](#bib.bib102), [19](#bib.bib19), [96](#bib.bib96)] |'
  prefs: []
  type: TYPE_TB
- en: '| Efficiency | Speed |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Reward higher speed until the maximum speed limit is reached; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Impose penalties if the speed is lower than the minimum speed limit
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[138](#bib.bib138), [118](#bib.bib118), [141](#bib.bib141), [119](#bib.bib119),
    [122](#bib.bib122), [110](#bib.bib110), [136](#bib.bib136), [26](#bib.bib26),
    [105](#bib.bib105), [140](#bib.bib140)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[133](#bib.bib133), [126](#bib.bib126), [145](#bib.bib145), [134](#bib.bib134),
    [93](#bib.bib93), [102](#bib.bib102), [28](#bib.bib28), [96](#bib.bib96), [27](#bib.bib27),
    [150](#bib.bib150)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Success | Reward the agent if it finished the task successfully | [[118](#bib.bib118),
    [120](#bib.bib120), [121](#bib.bib121), [137](#bib.bib137), [90](#bib.bib90),
    [143](#bib.bib143), [116](#bib.bib116), [96](#bib.bib96), [138](#bib.bib138)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Number of Overtakes | Reward a higher number of overtakes for efficiency
    | [[126](#bib.bib126), [145](#bib.bib145), [27](#bib.bib27), [133](#bib.bib133)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Time |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Impose a negative reward in each step to encourage the agent finish
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the task faster or penalize the agent if the task cannot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; be finished within a time threshold &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[110](#bib.bib110), [121](#bib.bib121), [91](#bib.bib91), [137](#bib.bib137)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Distance to the Destination | Provide a larger reward the closer the agent
    is to the destination | [[137](#bib.bib137), [136](#bib.bib136), [105](#bib.bib105)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Comfort | Jerk |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Impose penalties if the longitudinal or lateral control &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; is too urgent &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[110](#bib.bib110), [120](#bib.bib120), [136](#bib.bib136), [93](#bib.bib93),
    [91](#bib.bib91), [150](#bib.bib150), [144](#bib.bib144), [149](#bib.bib149),
    [96](#bib.bib96), [138](#bib.bib138)] |'
  prefs: []
  type: TYPE_TB
- en: '| Traffic Rules | Lane Mark Invasion | Impose penalties if the agent invades
    the lane marks | [[26](#bib.bib26), [105](#bib.bib105), [149](#bib.bib149)] |'
  prefs: []
  type: TYPE_TB
- en: '| Distance to the Lane Centerlines |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Impose penalties if the agent deviates from the lane &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; centerlines or routing baselines &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[110](#bib.bib110), [140](#bib.bib140), [27](#bib.bib27), [19](#bib.bib19),
    [96](#bib.bib96), [138](#bib.bib138)] |'
  prefs: []
  type: TYPE_TB
- en: '| Wrong Lane |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Impose penalties if the agent is in the wrong lane, e.g., &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; staying in the left-turn lane if the assigned route is straight &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[122](#bib.bib122)] |'
  prefs: []
  type: TYPE_TB
- en: '| Blocking Traffic |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Impose penalties if the agent blocks the future paths of other &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; vehicles that have the right of way &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[137](#bib.bib137)] | ![Refer to caption](img/bac3a079625b66bd6f00830622a7d6c0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: A taxonomy of the literature on how driving safety is addressed by
    DRL/DIL models. (a)-(c) Three main methods with typical examples in literature.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1065a5f8a0878a9c4791f28c43898f1b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Reinforcement learning rewards for AD tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: V-A State Space and Input Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [IV](#S4.T4 "TABLE IV ‣ IV-E Mode 5\. DRL/DIL Integrated Hierarchical
    Planning and Control ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning") classifies commonly
    used inputs according to information source (ego vehicle/road environment/task/prior
    knowledge). A statistical comparison of the percentages of input classes that
    are used in DRL/DIL methods is presented in Fig. [4](#S4.F4 "Figure 4 ‣ IV-E Mode
    5\. DRL/DIL Integrated Hierarchical Planning and Control ‣ IV Architectures of
    DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"). Compared with the task and prior knowledge, the ego vehicle
    and road environment seem to be more popular information sources. In all input
    classes, the ego vehicle speed, pixel data (e.g., camera images) and object data
    (e.g., other road users’ relative speeds and positions) are the most commonly
    used. Another significant difference between DRL and DIL is that DRL models prefer
    object data while DIL models prefer pixel data. The selection of low-dimensional
    object data rather than high-dimensional pixel data as input for DRL models renders
    the problem more tractable and accelerates the training procedure. Fig. [4](#S4.F4
    "Figure 4 ‣ IV-E Mode 5\. DRL/DIL Integrated Hierarchical Planning and Control
    ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning") presents a statistical comparison
    of preferences for input classes in various scenarios. Input from the task (e.g.,
    goal positions) and prior knowledge (e.g., road maps) are mostly used in urban
    scenarios. Point data (e.g., LiDAR sensor readings) and object data are more commonly
    used in highway scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from deciding the choice of input, the application of a dynamic input
    size is an important problem since the number of cars or pedestrians in the ego’s
    vicinity varies over time. Unfortunately, standard DRL/DIL methods rely on inputs
    of fixed size. The use of occupancy grid maps as inputs of CNNs is a practical
    solution [[121](#bib.bib121), [119](#bib.bib119)]. However, this solution imposes
    a trade-off between computational workload and expressiveness. Low-resolution
    grids decrease the computational burden at the cost of being imprecise in their
    representation of the environment, whereas for high-resolution grids, most computations
    may be redundant due to sparsity of the grid maps. Furthermore, a grid map is
    still limited by its defined size, and agents outside this region is neglected.
    Alternatively, Everett et al. [[152](#bib.bib152)] proposed to leverage LSTMs’
    ability to encode a variable number of agents’ observations. Huegle et al. [[141](#bib.bib141)]
    suggested the use of deep sets [[153](#bib.bib153)] as a flexible and permutation-invariant
    architecture to handle dynamic input. Dynamic input remains an open topic for
    future studies.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Action Space and Output Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A self-driving DRL/DIL agent can plan at different levels of abstraction, namely,
    low-level control, high-level behavioral planning and trajectory planning, or
    even at multiple levels simultaneously. According to this, Table [V](#S5.T5 "TABLE
    V ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning") categorizes mainstream action spaces into four groups: behavior-level
    actions, trajectory-level actions, control-level actions and hierarchical actions.
    Behavior-level actions are usually designed according to specified tasks. For
    speed control, acceleration-related actions (e.g., full brake, decelerate, continue,
    and accelerate [[119](#bib.bib119)]) are commonly used. Lane change actions (e.g.,
    keep/LLC/LRC) and turn actions (e.g., turn left/right/go straight) are preferred
    in highway scenarios and urban scenarios, respectively. Trajectory-level actions
    refer to the planned or predicted trajectories/paths [[113](#bib.bib113), [111](#bib.bib111)],
    which are typically composed of future path 2D points. Control-level actions refer
    to low-level control commands (e.g., steer, acceleration, throttle, and brake),
    which are divided into three classes: lateral control, longitudinal control and
    simultaneous lateral and longitudinal control. Early studies focused mainly on
    discrete lateral [[10](#bib.bib10), [19](#bib.bib19)] or discrete longitudinal
    control [[143](#bib.bib143)], while continuous control was considered later [[91](#bib.bib91),
    [85](#bib.bib85), [63](#bib.bib63), [64](#bib.bib64)]. Continuous control is demonstrated
    in [[102](#bib.bib102)] to produce smoother trajectories than discrete control
    for lane keeping, which may make passengers feel more comfortable. Simultaneous
    lateral and longitudinal control has received wide attention, especially in urban
    scenarios [[105](#bib.bib105), [70](#bib.bib70), [106](#bib.bib106), [107](#bib.bib107),
    [108](#bib.bib108)]. Recently, hierarchical actions have attracted more attention
    [[138](#bib.bib138), [137](#bib.bib137), [136](#bib.bib136), [27](#bib.bib27),
    [134](#bib.bib134)], which provide higher robustness and interpretability.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C Reinforcement Learning Reward Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A major problem that limits RL’s real-world AD applications is the lack of underlying
    reward functions. Further, the ground truth reward, if it exists, may be multi-modal
    since human drivers change objectives according to the circumstances. To simplify
    the problem, current DRL models for AD tasks commonly formulate the reward function
    as a linear combination of factors, as presented in Fig. [6](#S5.F6 "Figure 6
    ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy
    Learning"). A large proportion of studies consider safety and efficiency. Reward
    terms that are used in the literature are listed in Table [VI](#S5.T6 "TABLE VI
    ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy
    Learning"). Collison and speed are the most common reward terms when considering
    safety and efficiency, respectively. However, empirically designed reward functions
    rely heavily on expert knowledge. It is difficult to balance rewards terms, which
    affects the trained policy performance. Recent studies on predictive reward and
    multi-reward RL may inspire future investigation. Hayashi et al. [[154](#bib.bib154)]
    proposed a predictive reward function that is based on the prediction error of
    a deep predictive network that models the transition of the surrounding environment.
    Their hypothesis is that the movement of surrounding vehicles becomes unpredictable
    when the ego vehicle performs an unnatural driving behavior. Yuan et al. [[126](#bib.bib126)]
    decomposed a single reward function into multi-reward functions to better represent
    multi-dimensional driving policies through a branched version of Q networks.
  prefs: []
  type: TYPE_NORMAL
- en: VI Problem-driven Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AD application has special requirements on factors such as driving safety, interaction
    with other traffic participants and uncertainty of the environment. This section
    reviews the literature from the problem-driven perspective with the objectives
    of determining how these critical issues are addressed by the DRL/DIL models and
    identifying the challenges that remain.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Safety-enhanced DRL/DIL for AD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/674208fa6d028791ca29586a448449aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A taxonomy of the literature on interaction-aware DRL/DIL models
    for AD. (a) Qi et al. [[155](#bib.bib155)], (b) Chen et al. [[156](#bib.bib156)]
    are examples of explicit and implicit interactive environment encoding, respectively.
    (c) Hu et al. [[157](#bib.bib157)] is an example of interactive learning strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although DRL/DIL can learn driving policies for complex high-dimensional problems,
    they only guarantee optimality of the learned policies in a statistical sense.
    However, in safety-critical AD systems, one failure (e.g., collision) would cause
    catastrophe. Below, we review representative methods for enhancing safety of DRL/DIL
    in the AD literature. Fig. [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning") categorizes the methods
    into three groups: (a) modified methods: methods that modify the original DRL/DIL
    algorithms, (b) combined methods: methods that combine DRL/DIL with traditional
    methods, and (c) hybrid methods: methods that integrate DRL/DIL into traditional
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A1 Modified Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As illustrated in Fig. [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning")(a), modified methods
    modify the standard DRL/DIL algorithms to enhance safety, typically by constraining
    the exploration space [[158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)].
    A safety model checker is introduced to identify the set of actions that satisfy
    the safety constraints at each state. This can be realized through several approaches,
    such as goal reachability [[90](#bib.bib90)][[161](#bib.bib161)], probabilistic
    prediction [[162](#bib.bib162)] and prior knowledge & constraints [[163](#bib.bib163)].
    Bouton et al. [[90](#bib.bib90)][[161](#bib.bib161)] use a probabilistic model
    checker, as illustrated in Fig. [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣
    A Survey of Deep RL and IL for Autonomous Driving Policy Learning")(a), to compute
    the probability of reaching the goal safely at each state-action pair. Then, safe
    actions are identified by applying a user-defined threshold on the probability.
    However, the proposed model checker requires a discretization of the state space
    and a full transition model. Alternatively, Isele et al. [[162](#bib.bib162)]
    proposed the use of probabilistic prediction to identify potentially dangerous
    actions that would cause collision, but the safety guarantee may not be sufficiently
    strong if the prediction is not accurate. Prior knowledge & constraints (e.g.,
    lane changes are disallowed if they will lead to small time gaps) are also exploited
    [[163](#bib.bib163), [125](#bib.bib125), [132](#bib.bib132)]. For DIL, Zhang et
    al. [[69](#bib.bib69)] proposed SafeDAgger, in which a safety policy is learned
    to predict the error made by a primary policy without querying the reference policy.
    If the safety policy determines that it is unsafe to let the primary policy drive,
    the reference policy will take over. One drawback is that the quality of the learned
    policy may be limited by that of the reference policy.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A2 Combined Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Various studies combine standard DRL/DIL with traditional rule-based methods
    to enhance safety. In contrast to the modified methods that are discussed above,
    combined methods don’t modify the learning process of standard DRL/DIL. As presented
    in Fig. [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning")(b), Chen et al. [[71](#bib.bib71)]
    proposed a framework in which DIL plans trajectories, while the rule-based tracking
    and safe set controller ensure safe control. Xiong et al. [[164](#bib.bib164)]
    proposed the linear combination of the control output from DDPG, artificial potential
    field and path tracking modules. According to Shalev-Shwartz et al. [[165](#bib.bib165)],
    hard constraints should be injected outside the learning framework. They decompose
    the double-merge problem into a composition of a learnable DRL policy and a trajectory
    planning module with non-learnable hard constraints. The learning part enables
    driving comfort, while the hard constraints guarantee safety.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A3 Hybrid Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hybrid methods integrate DRL/DIL into traditional heuristic search or POMDP
    planning methods. As presented in Fig. [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning")(c), Bernhard
    et al. [[114](#bib.bib114)] integrated experiences in the form of pretrained Q-values
    into Hybrid A^∗ as heuristics, thereby overcoming the statistical failure rate
    of DRL while still benefitting computationally from the learned policy. However,
    the experiments are limited to stationary environments. Pusse et al. [[166](#bib.bib166)]
    presented a hybrid solution that combines DRL and approximate POMDP planning for
    collision-free autonomous navigation in simulated critical traffic scenarios,
    which benefits from advantages of both methods.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Interaction-aware DRL/DIL for AD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interaction is one of the intrinsic characteristics of traffic environments.
    An intelligent agent should reason beforehand about the behaviors of other traffic
    participants to passively react or actively adjust its own policy to cooperate
    or compete with other agents. This section reviews the interaction modeling methods
    and two groups of interaction-aware DRL/DIL methods for AD, as presented in Fig.
    [7](#S6.F7 "Figure 7 ‣ VI-A Safety-enhanced DRL/DIL for AD ‣ VI Problem-driven
    Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning").
    One group of methods focus on interactive environment encoding, while the other
    focus on interactive learning strategies.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B1 Interaction Modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The simplest way to model interaction between multi-agents is to use a standard
    Markov decision process (MDP), where the other traffic participants are only treated
    as part of the environment [[156](#bib.bib156), [141](#bib.bib141)]. POMDP is
    another common interaction model [[95](#bib.bib95), [155](#bib.bib155), [167](#bib.bib167)],
    where the agent has limited sensing capabilities. A Markov game (MG) is also used
    for modeling interaction scenarios. According to whether agents have the same
    importance, methods can be categorized into three groups: 1) equal importance
    [[168](#bib.bib168), [157](#bib.bib157), [169](#bib.bib169)], 2) one vs. others
    [[170](#bib.bib170)], and 3) proactive-passive pair [[171](#bib.bib171)].'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B2 Interactive Environment Encoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Interactive encoding of the environment is a popular research direction. As
    presented in Fig. [7](#S6.F7 "Figure 7 ‣ VI-A Safety-enhanced DRL/DIL for AD ‣
    VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"), mainstream methods can be divided into two groups. One group
    of methods explicitly model other agents and utilize active reasoning about other
    agents in the algorithm workflow. POMDP is a common choice for these methods,
    where the intentions/cooperation levels of other agents are modeled as unobservable
    states that must be inferred. Qi et al. [[155](#bib.bib155)] proposed an intent-aware
    multi-agent planning framework, as presented in Fig. [7](#S6.F7 "Figure 7 ‣ VI-A
    Safety-enhanced DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep
    RL and IL for Autonomous Driving Policy Learning")(a), which decouples intent
    prediction, high-level reasoning and low-level planning. The maintained belief
    regarding other agents’ intents (objectives) was considered in the planning process.
    Bouton et al. [[95](#bib.bib95)] proposed a similar method that maintains a belief
    regarding the cooperation levels (e.g., the willingness to yield to the ego vehicle)
    of other drivers.
  prefs: []
  type: TYPE_NORMAL
- en: The other group of methods focuses on utilizing special neural network architectures
    to capture the interplay between agents by their relation or interaction representations.
    These methods are usually agnostic regarding the intentions of other agents. Jiang
    et al. [[167](#bib.bib167)] proposed graph convolutional reinforcement learning,
    in which the multi-agent environment is constructed as a graph. Agents are represented
    by nodes, and each node’s corresponding neighbors are determined by distance or
    other metrics. Then, the latent features that are produced by graph convolutional
    layers are exploited to learn cooperation. Similarly, Huegle et al. [[172](#bib.bib172)]
    built upon graph neural networks [[173](#bib.bib173)] and proposed the deep scene
    architecture for learning complex interaction-aware scene representations. Inspired
    by social pooling [[174](#bib.bib174), [175](#bib.bib175)] and attention models
    [[176](#bib.bib176), [177](#bib.bib177)], Chen et al. [[156](#bib.bib156)] proposed
    a socially attentive DRL method for interaction-aware robot navigation through
    a crowd. As illustrated in Fig. [7](#S6.F7 "Figure 7 ‣ VI-A Safety-enhanced DRL/DIL
    for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning")(b), they extracted pairwise features of interaction
    between the robot and each human and captured the interactions among humans via
    local maps. A self-attention mechanism was subsequently used to infer the relative
    importance of neighboring humans and aggregate interaction features.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B3 Interactive Learning Strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c82e71c44827897dea4c0e7574d879d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: A taxonomy of the literature on uncertainty-aware DRL/DIL models.
    (a) Tai et al. [[108](#bib.bib108)] addresses aleatoric uncertainty, while (b)
    Henaff et al. [[178](#bib.bib178)] addresses both aleatoric and epistemic uncertainties.'
  prefs: []
  type: TYPE_NORMAL
- en: Various learning strategies have been used to learn interactive policies. Curriculum
    learning has been used to learn interactive policies [[157](#bib.bib157)][[168](#bib.bib168)],
    which can decouple complex problems into simpler problems. As presented in Fig.
    [7](#S6.F7 "Figure 7 ‣ VI-A Safety-enhanced DRL/DIL for AD ‣ VI Problem-driven
    Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning")(c),
    Hu et al. [[157](#bib.bib157)] proposed an interaction-aware decision making approach
    that leverages curriculum learning. First, a decentralized critic for each agent
    is learned to generate distinct behaviors, where the agent does not react to other
    agents and only learns how to execute rational actions to complete its own task.
    Second, a centralized critic is learned to enable agents to interact with each
    other to realize joint success and maintain smooth traffic. One limitation of
    these methods is that new models must be learned if the number of agents increases.
    Based on dynamic coordination graph (DCG) [[179](#bib.bib179)], Chao et al. [[180](#bib.bib180)]
    proposed a strategic learning solution for coordinating multiple autonomous vehicles
    in highways. DCG was utilized to explicitly model the continuously changing coordination
    dependencies among vehicles. Another group of interaction-aware methods use game
    theory[[181](#bib.bib181)]. Game theory has already been applied in robotics tasks
    such as robust control [[182](#bib.bib182), [183](#bib.bib183)] and motion planning
    [[184](#bib.bib184), [185](#bib.bib185)]. Recent years have also witnessed the
    increasing application of game theory in interaction-aware AD policy learning
    [[170](#bib.bib170), [169](#bib.bib169), [171](#bib.bib171)]. Li et al. [[170](#bib.bib170)]
    proposed the combination of hierarchical reasoning game theory (i.e. “level-$k$”
    reasoning [[186](#bib.bib186)]) and reinforcement learning. Level-$k$ reasoning
    is used to model intelligent vehicles’ interactions in traffic, while RL evolves
    these interactions in a time-extended scenario. Ding et al. [[171](#bib.bib171)]
    introduced a proactive-passive game theoretical lane changing framework. The proactive
    vehicles learn to take actions to merge, while the passive vehicles learn to create
    merging space. Fisac et al. [[169](#bib.bib169)] proposed a novel game-theoretic
    real-time trajectory planning algorithm. The dynamic game is hierarchically decomposed
    into a long-horizon “strategic” game and a short-horizon “tactical” game. Furthermore,
    the long-horizon interaction game is solved to guide short-horizon planning, thereby
    implicitly extending the planning horizon and pushing the local trajectory optimization
    closer to global solutions. Apart from combining game theory and RL, solving an
    imitation learning problem under game-theoretic formalism is another approach.
    Sun et al. [[187](#bib.bib187)] proposed an interactive probabilistic prediction
    approach that was based on hierarchical inverse reinforcement learning (HIRL).
    They modeled the problem from the perspective of a two-agent game by explicitly
    considering the responses of one agent to the other. However, some of the current
    game-theoretic interaction-aware methods are limited by their two-vehicle settings
    and simulation experiments [[171](#bib.bib171), [169](#bib.bib169), [187](#bib.bib187)].
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Uncertainty-aware DRL/DIL for AD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before deployment of a learned model, it is important to determine what it
    does not understand and estimate the uncertainty of the decision making output.
    As presented in Fig. [8](#S6.F8 "Figure 8 ‣ VI-B3 Interactive Learning Strategy
    ‣ VI-B Interaction-aware DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning"), this section reviews
    the uncertainty-aware DRL/DIL methods for AD from three aspects: 1) AD and deep
    learning uncertainty, 2) uncertainty estimation methods, and 3) multi-modal driving
    behavior learning.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-C1 Autonomous Driving and Deep Learning Uncertainty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Autonomous driving has inherent uncertainty, while deep learning methods have
    deep learning uncertainty, and the two can intersect. The AD uncertainty can be
    categorized as:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic environment uncertainty [[188](#bib.bib188), [107](#bib.bib107), [189](#bib.bib189),
    [108](#bib.bib108), [190](#bib.bib190), [178](#bib.bib178)]. Stochastic and dynamic
    interactions among agents with distinct behaviors lead to intrinsic irreducible
    randomness and uncertainty in a traffic environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driving behavior uncertainty [[191](#bib.bib191)]. Human driving behavior is
    multi-modal and stochastic (e.g., a driver can either make a left lane change
    or a right lane change when he comes up behind a van that is moving at a crawl).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partial observability and sensor noise uncertainty [[192](#bib.bib192)]. In
    real-world scenarios, the AD agent usually has limited partial observability (e.g.,
    due to occlusion), and there is noise in the sensor observation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deriving from Bayesian deep learning approaches, Gal et al. [[193](#bib.bib193)]
    categorized deep learning uncertainty as aleatoric/data and epistemic/model uncertainties.
    Aleatoric uncertainty results from incomplete knowledge about the environment
    (e.g., partial observability and measurement noise), which can’t be reduced through
    access to more or even unlimited data but can be explicitly modeled. In contrast,
    epistemic uncertainty originates from an insufficient dataset and measures what
    our model doesn’t know, which can be eliminated with sufficient training data.
    We refer readers to [[194](#bib.bib194), [193](#bib.bib193)] for a deeper background
    on predictive uncertainty in deep neural networks. Although it is sometimes possible
    to use only aleatoric [[188](#bib.bib188), [189](#bib.bib189), [108](#bib.bib108)]
    or epistemic [[107](#bib.bib107), [190](#bib.bib190)] uncertainty to develop a
    reasonable model, the ideal approach would be to combine these two uncertainty
    estimates [[192](#bib.bib192), [191](#bib.bib191), [178](#bib.bib178)].
  prefs: []
  type: TYPE_NORMAL
- en: VI-C2 Uncertainty Estimation Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Aleatoric uncertainty is usually learned by using the heteroscedastic loss function
    [[194](#bib.bib194)]. The regression task and the loss are formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle[\tilde{\mathbf{y}},\tilde{\sigma}]=\mathbf{f}^{\theta}(\mathbf{x})$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathcal{L}(\theta)=\frac{1}{2}\frac{{\parallel\mathbf{y}-\tilde{\mathbf{y}}\parallel}^{2}}{\tilde{\sigma}^{2}}+\frac{1}{2}\log\tilde{\sigma}^{2}$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{x}$ denotes the input data and $\mathbf{y}$ and $\mathbf{\tilde{y}}$
    denote the regression ground truth and the prediction output, respectively. $\theta$
    denotes the model parameters, and $\tilde{\sigma}$ is another output of the model,
    which represents the standard variance of data $\mathbf{x}$ (the aleatoric uncertainty).
    The loss function can be interpreted as penalizing a large prediction error when
    the uncertainty is small and relaxing constraints on the prediction error when
    the uncertainty is large. In practice, the network predicts the log variance $\log\tilde{\sigma}^{2}$
    [[194](#bib.bib194)]. Tai et al. [[108](#bib.bib108)] proposed an end-to-end real-to-sim
    visual navigation deployment pipeline, as illustrated in Fig. [8](#S6.F8 "Figure
    8 ‣ VI-B3 Interactive Learning Strategy ‣ VI-B Interaction-aware DRL/DIL for AD
    ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning")(a). An uncertainty-aware IL policy is trained with the heteroscedastic
    loss and outputs actions, along with associated uncertainties. A similar technique
    is proposed by Lee et al. [[192](#bib.bib192)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The epistemic uncertainty is usually estimated via two popular methods: Monte
    Carlo (MC)-dropout [[195](#bib.bib195), [196](#bib.bib196)] and ensembles [[197](#bib.bib197),
    [198](#bib.bib198)]. These methods are similar in the sense that both apply probabilistic
    reasoning on the network weights. The variance of the model output serves as an
    estimate of the model uncertainty. However, multiple stochastic forward passes
    through dropout sampling may be time-consuming, while ensemble methods have higher
    training and storage costs. Kahn et al. [[190](#bib.bib190)] proposed an uncertainty-aware
    RL method that utilizes MC-dropout and bootstrapping [[199](#bib.bib199)], where
    the confidence for a specified obstacle is updated iteratively. Guided by the
    uncertainty cost, the agent behaves more carefully in unfamiliar scenarios in
    the early training phase. As presented in Fig. [8](#S6.F8 "Figure 8 ‣ VI-B3 Interactive
    Learning Strategy ‣ VI-B Interaction-aware DRL/DIL for AD ‣ VI Problem-driven
    Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning")(b),
    Henaff et al. [[178](#bib.bib178)] proposed training a driving policy by unrolling
    a learned dynamics model over multiple time steps while explicitly penalizing
    the original policy cost and an uncertainty cost that represents the divergence
    from the training dataset. Their method estimates both the aleatoric and epistemic
    uncertainties.'
  prefs: []
  type: TYPE_NORMAL
- en: The uncertainty estimation methods that are presented above depend mainly on
    sampling. A novel uncertainty estimation method that utilizes a mixture density
    network (MDN) was proposed by Choi et al. [[191](#bib.bib191)] for learning from
    complex and noisy human demonstrations. Since an MDN outputs the parameters for
    constructing a Gaussian mixture model (GMM), the total variance of the GMM can
    be calculated analytically, and the acquisition of uncertainty requires only a
    single forward pass. Distributional reinforcement learning [[200](#bib.bib200),
    [51](#bib.bib51), [201](#bib.bib201)] offers another approach for modelling the
    uncertainty that is associated with actions. It models the RL return $R$ as a
    random variable that is subject to the probability distribution $Z(r|s,a)$ and
    the Q-value as the expected return $Q(s,a)=\mathbb{E}_{r\sim Z(r|s,a)}[r]$. In
    the AD domain, Wang et al. [[188](#bib.bib188)] applied distributional DDPG to
    an energy management strategy (EMS) problem as a case study to evaluate the effects
    of estimating the uncertainty that is associated with various actions at various
    states. Bernhard [[189](#bib.bib189)] et al. presented a two-step approach for
    risk-sensitive behavior generation that combined offline distribution reinforcement
    learning with online risk assessment, which increased safety in intersection crossing
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C3 Multi-Modal Driving Behavior Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The inherent uncertainty in driving behavior results in multi-modal demonstrations.
    Many multi-modal imitation learning methods have been proposed. InfoGAIL [[202](#bib.bib202)]
    and Burn-InfoGAIL [[203](#bib.bib203)] infer latent/modal variables by maximizing
    the mutual information between latent variables and state-action pairs. VAE-GAIL
    [[204](#bib.bib204)] introduces a variational auto-encoder for inferring modal
    variables. However, due to the lack of labels in the demonstrations, these algorithms
    tend to distinguish latent labels without considering semantic information or
    the task context. Another direction focuses on labeled data in expert demonstrations.
    CGAIL [[205](#bib.bib205)] sends the modal labels directly to the generator and
    the discriminator. ACGAIL [[206](#bib.bib206)] introduces an auxiliary classifier
    for reconstructing the modal information, where the classifier cooperates with
    the discriminator to provide the adversarial loss to the generator. Nevertheless,
    the above methods mainly leverage random sampling of latent labels from a known
    prior distribution to distinguish multiple modalities. The trained models rely
    on manually specified labels to output actions; hence, they cannot select modes
    adaptively according to environmental scenarios. Recently, Fei et al. [[207](#bib.bib207)]
    proposed Triple-GAIL, which can learn adaptive skill selection and imitation jointly
    from expert demonstrations, and generated experiences by introducing an auxiliary
    skill selector.
  prefs: []
  type: TYPE_NORMAL
- en: VII Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although DRL and DIL attract significant amounts of interest in AD research,
    they remain far from ready for real-world applications, and challenges are faced
    at the architecture, task and algorithm levels. However, solutions remain largely
    underexplored. In this section, we discuss these challenges along with future
    investigation. DRL and DIL also have their own technical challenges; we refer
    readers to comprehensive discussions in [[29](#bib.bib29), [31](#bib.bib31), [33](#bib.bib33)].
  prefs: []
  type: TYPE_NORMAL
- en: VII-A System architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The success of modern AD systems depends on the meticulous design of architectures.
    The integration of DRL/DIL methods to collaborate with other modules and improve
    the performance of the system remains a substantial challenge. Studies have demonstrated
    various ways that DRL/DIL models could be integrated into an AD system. As illustrated
    Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"), some studies
    propose new AD architectures, e.g., Modes 1&2, where an entire pipeline from the
    input of the sensor/perception to the output of the vehicle’s actuators is covered.
    However, the traditional modules of sequential planning are missing in these new
    architectures, and driving policy is addressed at the control level only. Hence,
    these AD systems could adapt to only simple tasks, such as road following, that
    require neither the guidance of goal points nor switching of driving behaviors.
    The extension of these architectures to accomplish more complicated AD tasks remains
    a substantial challenge. Other studies utilize the traditional AD architectures,
    e.g., Modes 3&4, where DRL/DIL models are studied as substitutes for traditional
    modules to improve the performance in challenging scenarios. Mode 5 studies use
    both new and traditional architectures. Overall, the research effort until now
    has been focused more on exploring the potential of DRL/DIL in accomplishing AD
    tasks, whereas the design of the system architectures has yet to be intensively
    investigated.
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Formulation of driving tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Various DRL/DIL formulations have been established for accomplishing AD tasks.
    However, these formulations rely heavily on empirical designs. As reviewed in
    Section [V](#S5 "V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning"), the state space and input data are designed case by
    case, and ad-hoc reward functions are usually adopted with hand-tuned coefficients
    that balance the costs regarding safety, efficiency, comfort, and traffic rules,
    among other factors. Such designs are very brute-force approaches, which lack
    both theoretical proof and in-depth investigations. Changing the designs or tuning
    the parameters could result in substantially different driving policies. However,
    in real-world deployment, more attention should be paid to the following questions:
    What design could realize the most optimal driving policy? Could such a design
    adapt to various scenes? How can the boundary conditions of designs be identified?
    To answer these questions, rigorous studies with comparative experiments are needed.'
  prefs: []
  type: TYPE_NORMAL
- en: VII-C Safe driving policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AD applications have high requirements on safety, and guaranteeing the safety
    of a DRL/DIL integrated AD system is of substantial importance. Compared to traditional
    rule-based methods, DNN has been widely acknowledged as having poor interpretability.
    Its “black-box” nature renders difficult the prediction of when the agent may
    fail to generate a safe policy. Deep models for real-world AD applications must
    address unseen or rarely seen scenarios, which is difficult for DL methods as
    they optimize objectives at the level of expectations over specified instances.
    To solve this problem, a general strategy is to combine traditional methods to
    ensure a DRL/DIL agent’s functional safety. As reviewed in Fig. [5](#S5.F5 "Figure
    5 ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"), various methods have been proposed in the literature, where
    the problems are usually formulated as compositions of learned policies with hard
    constraints [[132](#bib.bib132), [125](#bib.bib125), [163](#bib.bib163)]. However,
    balancing between the learned optimal policy and the safety guarantee by hard
    constraints is non-trivial and requires intensive investigation in the future.
  prefs: []
  type: TYPE_NORMAL
- en: VII-D Interaction with traffic participants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The capability of human-like interaction is required of self-driving agents
    for sharing the roads with other traffic participants. As reviewed in Section
    [VI-B](#S6.SS2 "VI-B Interaction-aware DRL/DIL for AD ‣ VI Problem-driven Methods
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"), interaction-aware
    DRL/DIL is a rising topic, but the following problems remain: First, current studies
    attempt to solve the problem from various perspectives, and the systematic studies
    are needed. Second, few interaction-aware DIL methods are available, while interaction-aware
    DRL methods are limited to simplified scenarios that involve only a few agents.
    The combination of interaction-aware trajectory prediction methods [[208](#bib.bib208),
    [209](#bib.bib209), [210](#bib.bib210)] may be an open topic of potential value.
    Third, game theory and multi-agent reinforcement learning (MARL) [[31](#bib.bib31)]
    are highly correlated for interactive scenarios. MARL methods usually build on
    concepts of game theory (e.g., Markov games) to model the interaction process.
    Apart from DRL/DIL, methods are available for learning interactive policies through
    traditional game theory approaches, such as Nash equilibrium [[211](#bib.bib211)],
    level-$k$ reasoning [[212](#bib.bib212)] and game tree search [[213](#bib.bib213)].
    Exploiting POMDP planning to learn interactive polices is also a trend [[214](#bib.bib214),
    [215](#bib.bib215)]. These methods have satisfactory interpretability but are
    limited to simplified or coarse discretizations of the agents’ action space [[213](#bib.bib213),
    [215](#bib.bib215)]. Although the simplification reduces the computation burden,
    it tends to also lower the control precision. In the future, the combination of
    these methods and DRL/DIL may be promising.'
  prefs: []
  type: TYPE_NORMAL
- en: VII-E Uncertainty of the environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Decision-making under uncertainty has been studied for decades [[216](#bib.bib216),
    [217](#bib.bib217)]. Nevertheless, modeling the uncertainty in DRL/DIL formulations
    remains challenging, especially under complex uncertain traffic environments.
    Several problems have been identified in current research: First, most uncertainty-aware
    methods follow the style of deep learning predictive uncertainty [[193](#bib.bib193)]
    without a deeper investigation. Is computing the predictive uncertainty of DNNs
    sufficient for AD tasks? Second, can the computed uncertainty be effectively utilized
    to realize a better decision making policy? Various methods incorporate the uncertainty
    cost into the global cost functions [[190](#bib.bib190), [178](#bib.bib178)],
    while other methods utilize uncertainty to generate risk-sensitive behavior [[189](#bib.bib189)].
    Future efforts are needed to identify more promising applications. Third, human-driving
    behavior is uncertain, or multi-modal. However, DIL performs well for demonstrations
    from one expert rather than multiple experts [[218](#bib.bib218)]. A naive solution
    is to neglect the multi-modality and treat the demonstrations as if there is only
    one expert. The main side effect is that the model tends to learn an average policy
    rather than a multi-modal policy [[202](#bib.bib202)]. Thus, determining whether
    DRL/DIL learn effectively from noisy uncertain naturalistic driving data and generate
    multi-modal driving behavior according to various scenarios is meaningful.'
  prefs: []
  type: TYPE_NORMAL
- en: VII-F Validation and benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Validation and benchmarks are especially important for AD, but far from sufficient
    effort has been made regarding these aspects. First, comparison between DRL/DIL
    integrated architectures and traditional architectures is usually neglected in
    the literature, which is meaningful for identifying the quantitative performance
    gains and disadvantages of introducing DRL/DIL. Second, systematic comparison
    between DRL/DIL architectures is necessary. A technical barrier of the former
    two problems is the lack of a reasonable benchmark. High-fidelity simulators such
    as CARLA [[105](#bib.bib105)] may provide a virtual platform on which various
    architectures can be deployed and evaluated. Third, exhaustive validation of trained
    policies before deployment is of vital importance. However, validation is challenging.
    Real-world testing on vehicles has high costs in terms of time, finances and human
    labor and could be dangerous. Empirical validation through simulation can reduce
    the amount of required field testing and can be used as a first step for performance
    and safety evaluation. However, verification through simulation only ensures the
    performance in a statistical sense. Even small variations between the simulators
    and the real scenario can have drastic effects on the system behavior. Future
    studies are needed to identify practical, effective, low-risk and economical validation
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: VIII Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this study, a comprehensive survey is presented that focuses on autonomous
    driving policy learning using DRL/DIL, which is addressed simultaneously from
    the system, task-driven and problem-driven perspectives. The study is conducted
    at three levels: First, a taxonomy of the literature studies is presented from
    the system perspective, from which five modes of integration of DRL/DIL models
    into an AD architecture are identified. Second, the formulations of DRL/DIL models
    for accomplishing specified AD tasks are comprehensively reviewed, where various
    designs on the model state and action spaces and the reinforcement learning rewards
    are covered. Finally, an in-depth review is presented on how the critical issues
    of AD applications regarding driving safety, interaction with other traffic participants
    and uncertainty of the environment are addressed by the DRL/DIL models. The major
    findings are listed below, from which potential topics for future investigation
    are identified.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DRL/DIL attract significant amounts of interest in AD research. However, literature
    studies in this scope have focused more on exploring the potential of DRL/DIL
    in accomplishing AD tasks, whereas the design of the system architectures remains
    to be intensively investigated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many DRL/DIL models have been formulated for accomplishing AD tasks. However,
    these formulations rely heavily on empirical designs, which are brute-force approaches
    and lack both theoretical proof and in-depth investigations. In the real-world
    deployment of such models, substantial challenges in terms of stability and robustness
    may be encountered.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driving safety, which is the main issue in AD applications, has received the
    most attention in the literature. However, the studies on interaction with other
    traffic participants and the uncertainty of the environment remain highly preliminary,
    in which the problems have been addressed from divergent perspectives, and have
    not been conducted systematically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] C. Urmson and W. Whittaker, “Self-driving cars and the urban challenge,”
    *IEEE Intelligent Systems*, vol. 23, no. 2, pp. 66–68, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] S. Thrun, “Toward robotic cars,” *Communications of the ACM*, vol. 53,
    no. 4, pp. 99–106, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Eskandarian, *Handbook of intelligent vehicles*.   Springer, 2012, vol. 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S. M. Grigorescu, B. Trasnea, T. T. Cocias, and G. Macesanu, “A survey
    of deep learning techniques for autonomous driving,” *J. Field Robotics*, vol. 37,
    no. 3, pp. 362–386, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] W. H. Organization *et al.*, “Global status report on road safety 2018:
    Summary,” World Health Organization, Tech. Rep., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Talebpour and H. S. Mahmassani, “Influence of connected and autonomous
    vehicles on traffic flow stability and throughput,” *Transportation Research Part
    C: Emerging Technologies*, vol. 71, pp. 143–163, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] W. Payre, J. Cestac, and P. Delhomme, “Intention to use a fully automated
    car: Attitudes and a priori acceptability,” *Transportation research part F: traffic
    psychology and behaviour*, vol. 27, pp. 252–263, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] E. D. Dickmanns and A. Zapp, “Autonomous high speed road vehicle guidance
    by computer vision,” *IFAC Proceedings Volumes*, vol. 20, no. 5, pp. 221–226,
    1987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] C. Thorpe, M. H. Hebert, T. Kanade, and S. A. Shafer, “Vision and navigation
    for the carnegie-mellon navlab,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 10, no. 3, pp. 362–373, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural network,”
    in *Advances in Neural Information Processing Systems*, 1989, pp. 305–313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel,
    P. Fong, J. Gale, M. Halpenny, G. Hoffmann *et al.*, “Stanley: The robot that
    won the darpa grand challenge,” *Journal of field Robotics*, vol. 23, no. 9, pp.
    661–692, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] M. Buehler, K. Iagnemma, and S. Singh, *The DARPA urban challenge: autonomous
    vehicles in city traffic*.   springer, 2009, vol. 56.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] D. González, J. Pérez, V. Milanés, and F. Nashashibi, “A review of motion
    planning techniques for automated vehicles,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 17, no. 4, pp. 1135–1145, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] X. Li, Z. Sun, D. Cao, Z. He, and Q. Zhu, “Real-time trajectory planning
    for autonomous urban driving: Framework, algorithms, and verifications,” *IEEE/ASME
    Transactions on Mechatronics*, vol. 21, no. 2, pp. 740–753, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] B. Paden, M. Čáp, S. Z. Yong, D. Yershov, and E. Frazzoli, “A survey of
    motion planning and control techniques for self-driving urban vehicles,” *IEEE
    Transactions on Intelligent Vehicles*, vol. 1, no. 1, pp. 33–55, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. Ulbrich, A. Reschka, J. Rieken, S. Ernst, G. Bagschik, F. Dierkes,
    M. Nolte, and M. Maurer, “Towards a functional system architecture for automated
    vehicles,” *arXiv preprint arXiv:1703.08557*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] L. Li, K. Ota, and M. Dong, “Humanlike driving: Empirical decision-making
    system for autonomous vehicles,” *IEEE Trans. Veh. Technol.*, vol. 67, no. 8,
    pp. 6814–6823, 2018\. [Online]. Available: [https://doi.org/10.1109/TVT.2018.2822762](https://doi.org/10.1109/TVT.2018.2822762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] W. Schwarting, J. Alonso-Mora, and D. Rus, “Planning and decision-making
    for autonomous vehicles,” *Annual Review of Control, Robotics, and Autonomous
    Systems*, vol. 1, 05 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] G. Yu and I. K. Sethi, “Road-following with continuous learning,” in *the
    Intelligent Vehicles’ 95\. Symposium*.   IEEE, 1995, pp. 412–417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger,
    and E. Liang, “Autonomous inverted helicopter flight via reinforcement learning,”
    in *International Symposium on Experimental Robotics*, ser. Springer Tracts in
    Advanced Robotics, vol. 21.   Springer, 2004, pp. 363–372.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] R. S. Sutton and A. G. Barto, *Reinforcement learning: an introduction*.   MIT
    Press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Y. LeCun, Y. Bengio, and G. E. Hinton, “Deep learning,” *Nature*, vol.
    521, no. 7553, pp. 436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] I. J. Goodfellow, Y. Bengio, and A. C. Courville, *Deep Learning*, ser.
    Adaptive computation and machine learning.   MIT Press, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    in *International Conference on Learning Representations*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in *International Conference on Robotics and Automation*.   IEEE, 2017, pp. 3357–3364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] X. Liang, T. Wang, L. Yang, and E. Xing, “Cirl: Controllable imitative
    reinforcement learning for vision-based self-driving,” in *the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 584–599.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Chen, C. Dong, P. Palanisamy, P. Mudalige, K. Muelling, and J. M. Dolan,
    “Attention-based hierarchical deep reinforcement learning for lane change behaviors
    in autonomous driving,” in *IEEE Conference on Computer Vision and Pattern Recognition
    Workshops*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam,
    A. Bewley, and A. Shah, “Learning to drive in a day,” in *International Conference
    on Robotics and Automation*.   IEEE, 2019, pp. 8248–8254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
    reinforcement learning: A brief survey,” *IEEE Signal Processing Magazine*, vol. 34,
    no. 6, pp. 26–38, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Y. Li, “Deep reinforcement learning: An overview,” *CoRR*, vol. abs/1701.07274,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning
    for multi-agent systems: A review of challenges, solutions and applications,”
    *CoRR*, vol. abs/1812.11794, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne, “Imitation learning:
    A survey of learning methods,” *ACM Computing Surveys (CSUR)*, vol. 50, no. 2,
    pp. 1–35, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and J. Peters,
    “An algorithmic perspective on imitation learning,” *Found. Trends Robotics*,
    vol. 7, no. 1-2, pp. 1–179, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Kuutti, R. Bowden, Y. Jin, P. Barber, and S. Fallah, “A survey of deep
    learning applications to autonomous vehicle control,” *CoRR*, vol. abs/1912.10773,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. K.
    Yogamani, and P. Pérez, “Deep reinforcement learning for autonomous driving: A
    survey,” *CoRR*, vol. abs/2002.00444, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. B. Thrun, “Efficient exploration in reinforcement learning,” 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. Coggan, “Exploration and exploitation in reinforcement learning,” *Research
    supervised by Prof. Doina Precup, CRA-W DMP Project at McGill University*, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Z. Hong, T. Shann, S. Su, Y. Chang, T. Fu, and C. Lee, “Diversity-driven
    exploration strategy for deep reinforcement learning,” in *Advances in Neural
    Information Processing Systems*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] G. Shani, J. Pineau, and R. Kaplow, “A survey of point-based pomdp solvers,”
    *Autonomous Agents and Multi-Agent Systems*, vol. 27, no. 1, pp. 1–51, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] W. S. Lovejoy, “A survey of algorithmic methods for partially observed
    markov decision processes,” *Annals of Operations Research*, vol. 28, no. 1, pp.
    47–65, 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] R. Bellman and R. Kalaba, “On the role of dynamic programming in statistical
    communication theory,” *IRE Trans. Inf. Theory*, vol. 3, no. 3, pp. 197–203, 1957.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] C. J. C. H. Watkins and P. Dayan, “Technical note q-learning,” *Mach.
    Learn.*, vol. 8, pp. 279–292, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] G. A. Rummery and M. Niranjan, *On-line Q-learning using connectionist
    systems*.   University of Cambridge, Department of Engineering Cambridge, UK,
    1994, vol. 37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] R. Bellman, “Dynamic programming,” *Science*, vol. 153, no. 3731, pp.
    34–37, 1966.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski, and et al., “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, pp.
    529–533, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] L.-J. Lin, “Self-improving reactive agents based on reinforcement learning,
    planning and teaching,” *Machine learning*, vol. 8, no. 3-4, pp. 293–321, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” in *International Conference on Learning Representations*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] S. Gu, T. P. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep
    q-learning with model-based acceleration,” in *International Conference on Machine
    Learning*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] H. v. Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning,” in *the Thirtieth AAAI Conference on Artificial Intelligence*,
    2016, pp. 2094–2100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas,
    “Dueling network architectures for deep reinforcement learning,” in *International
    Conference on Machine Learning*, 2016, pp. 1995–2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos, “Distributional
    reinforcement learning with quantile regression,” in *AAAI Conference on Artificial
    Intelligence*, 2018, pp. 2892–2901.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Mach. Learn.*, vol. 8, pp. 229–256, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] L. C. Baird, “Reinforcement learning in continuous time: Advantage updating,”
    in *IEEE International Conference on Neural Networks*, vol. 4.   IEEE, 1994, pp.
    2448–2453.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel, “High-dimensional
    continuous control using generalized advantage estimation,” in *International
    Conference on Learning Representations*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” in *International Conference on Machine Learning*, 2015,
    pp. 1889–1897.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *CoRR*, vol. abs/1707.06347, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. A. Riedmiller,
    “Deterministic policy gradient algorithms,” in *International Conference on Machine
    Learning*, vol. 32, 2014, pp. 387–395.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International Conference on Machine Learning*, 2016, pp. 1928–1937.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] J. Wang, Z. Kurth-Nelson, H. Soyer, J. Z. Leibo, D. Tirumala, R. Munos,
    C. Blundell, D. Kumaran, and M. M. Botvinick, “Learning to reinforcement learn,”
    in *the 39th Annual Meeting of the Cognitive Science Society, CogSci 2017, London,
    UK, 16-29 July 2017*.   cognitivesciencesociety.org, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar,
    H. Zhu, A. Gupta, P. Abbeel, and S. Levine, “Soft actor-critic algorithms and
    applications,” *CoRR*, vol. abs/1812.05905, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] B. D. Argall, S. Chernova, M. M. Veloso, and B. Browning, “A survey of
    robot learning from demonstration,” *Robotics Auton. Syst.*, vol. 57, no. 5, pp.
    469–483, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] H. John and C. James, “Ngsim interstate 80 freeway dataset,” US Fedeal
    Highway Administration, FHWA-HRT-06-137, Washington, DC, USA, Tech. Rep., 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang *et al.*, “End to end learning for
    self-driving cars,” *arXiv preprint arXiv:1604.07316*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner, L. Jackel,
    and U. Muller, “Explaining how a deep neural network trained with end-to-end learning
    steers a car,” *arXiv preprint arXiv:1704.07911*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] H. Xu, Y. Gao, F. Yu, and T. Darrell, “End-to-end learning of driving
    models from large-scale video datasets,” in *IEEE conference on Computer Vision
    and Pattern Recognition*, 2017, pp. 2174–2182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] D. A. Pomerleau, “Efficient training of artificial neural networks for
    autonomous navigation,” *Neural computation*, vol. 3, no. 1, pp. 88–97, 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] S. Ross and D. Bagnell, “Efficient reductions for imitation learning,”
    in *International Conference on Artificial Intelligence and Statistics*, ser.
    JMLR Proceedings, vol. 9.   JMLR.org, 2010, pp. 661–668.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] S. Ross, G. J. Gordon, and D. Bagnell, “A reduction of imitation learning
    and structured prediction to no-regret online learning,” in *International Conference
    on Artificial Intelligence and Statistics*, ser. JMLR Proceedings, vol. 15, 2011,
    pp. 627–635.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. Zhang and K. Cho, “Query-efficient imitation learning for end-to-end
    autonomous driving,” *arXiv preprint arXiv:1605.06450*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] F. Codevilla, M. Miiller, A. López, V. Koltun, and A. Dosovitskiy, “End-to-end
    driving via conditional imitation learning,” in *International Conference on Robotics
    and Automation*.   IEEE, 2018, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J. Chen, B. Yuan, and M. Tomizuka, “Deep imitation learning for autonomous
    driving in generic urban scenarios with enhanced safety,” in *IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2019, pp. 2884–2890.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Y. Ng and S. J. Russell, “Algorithms for inverse reinforcement learning,”
    in *International Conference on Machine Learning*, 2000, pp. 663–670.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement
    learning,” in *International Conference on Machine Learning*, 2004, p. 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] N. D. Ratliff, J. A. Bagnell, and M. Zinkevich, “Maximum margin planning,”
    in *International Conference on Machine Learning*, vol. 148, 2006, pp. 729–736.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, “Maximum entropy
    inverse reinforcement learning,” in *Aaai*, vol. 8, 2008, pp. 1433–1438.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S. Levine, Z. Popovic, and V. Koltun, “Nonlinear inverse reinforcement
    learning with gaussian processes,” in *Advances in Neural Information Processing
    Systems*, 2011, pp. 19–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] N. D. Ratliff, D. M. Bradley, J. A. Bagnell, and J. E. Chestnutt, “Boosting
    structured prediction for imitation learning,” in *Advances in Neural Information
    Processing Systems*, 2006, pp. 1153–1160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] N. D. Ratliff, D. Silver, and J. A. Bagnell, “Learning to search: Functional
    gradient techniques for imitation learning,” *Auton. Robots*, vol. 27, no. 1,
    pp. 25–53, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] M. Wulfmeier, P. Ondruska, and I. Posner, “Maximum entropy deep inverse
    reinforcement learning,” *arXiv preprint arXiv:1507.04888*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] C. Finn, S. Levine, and P. Abbeel, “Guided cost learning: Deep inverse
    optimal control via policy optimization,” in *International Conference on Machine
    Learning*, 2016, pp. 49–58.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in *Advances
    in Neural Information Processing Systems*, 2016, pp. 4565–4573.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems*, 2014, pp. 2672–2680.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] C. Finn, P. F. Christiano, P. Abbeel, and S. Levine, “A connection between
    generative adversarial networks, inverse reinforcement learning, and energy-based
    models,” *CoRR*, vol. abs/1611.03852, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] J. Fu, K. Luo, and S. Levine, “Learning robust rewards with adversarial
    inverse reinforcement learning,” *CoRR*, vol. abs/1710.11248, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. L. Cun, “Off-road obstacle
    avoidance through end-to-end learning,” in *Advances in neural information processing
    systems*, 2006, pp. 739–746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] V. Rausch, A. Hansen, E. Solowjow, C. Liu, E. Kreuzer, and J. K. Hedrick,
    “Learning a deep neural net policy for end-to-end control of autonomous vehicles,”
    in *2017 American Control Conference (ACC)*.   IEEE, 2017, pp. 4914–4919.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] H. M. Eraqi, M. N. Moustafa, and J. Honer, “End-to-end deep learning for
    steering autonomous vehicles considering temporal dependencies,” *arXiv preprint
    arXiv:1710.03804*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] D. Wang, C. Devin, Q.-Z. Cai, F. Yu, and T. Darrell, “Deep object-centric
    policies for autonomous driving,” in *International Conference on Robotics and
    Automation*.   IEEE, 2019, pp. 8853–8859.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and B. Boots,
    “Agile autonomous driving using end-to-end deep imitation learning,” *arXiv preprint
    arXiv:1709.07174*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M. Bouton, A. Nakhaei, K. Fujimura, and M. J. Kochenderfer, “Safe reinforcement
    learning with scene decomposition for navigating complex urban environments,”
    in *Intelligent Vehicles Symposium*.   IEEE, 2019, pp. 1469–1476.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] P. Wang, C.-Y. Chan, and A. de La Fortelle, “A reinforcement learning
    based approach for automated lane change maneuvers,” in *Intelligent Vehicles
    Symposium*.   IEEE, 2018, pp. 1379–1384.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] X. Chen, Y. Zhai, C. Lu, J. Gong, and G. Wang, “A learning model for personalized
    adaptive cruise control,” in *Intelligent Vehicles Symposium*.   IEEE, 2017, pp.
    379–384.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] P. Wang and C.-Y. Chan, “Formulation of deep reinforcement learning architecture
    toward autonomous driving for on-ramp merge,” in *International Conference on
    Intelligent Transportation Systems*.   IEEE, 2017, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] H. Chae, C. M. Kang, B. Kim, J. Kim, C. C. Chung, and J. W. Choi, “Autonomous
    braking system via deep reinforcement learning,” in *International Conference
    on Intelligent Transportation Systems*.   IEEE, 2017, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] M. Bouton, A. Nakhaei, K. Fujimura, and M. J. Kochenderfer, “Cooperation-aware
    reinforcement learning for merging in dense traffic,” in *IEEE Intelligent Transportation
    Systems Conference*.   IEEE, 2019, pp. 3441–3447.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Y. Tang, “Towards learning multi-agent negotiations via self-play,” in
    *IEEE International Conference on Computer Vision Workshops*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] A. Folkers, M. Rick, and C. Büskens, “Controlling an autonomous vehicle
    with deep reinforcement learning,” in *Intelligent Vehicles Symposium*.   IEEE,
    2019, pp. 2025–2031.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] H. Porav and P. Newman, “Imminent collision mitigation with reinforcement
    learning and vision,” in *International Conference on Intelligent Transportation
    Systems*.   IEEE, 2018, pp. 958–964.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Wang, D. Jia, and X. Weng, “Deep reinforcement learning for autonomous
    driving,” *arXiv preprint arXiv:1811.11329*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] P. Wang, H. Li, and C.-Y. Chan, “Continuous control for automated lane
    change behavior based on deep deterministic policy gradient algorithm,” in *Intelligent
    Vehicles Symposium*.   IEEE, 2019, pp. 1454–1460.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] M. Kaushik, V. Prasad, K. M. Krishna, and B. Ravindran, “Overtaking maneuvers
    in simulated highway driving using deep reinforcement learning,” in *Intelligent
    Vehicles Symposium*.   IEEE, 2018, pp. 1885–1890.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “End-to-end deep reinforcement
    learning for lane keeping assist,” *arXiv preprint arXiv:1612.04340*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] R. Vasquez and B. Farooq, “Multi-objective autonomous braking system
    using naturalistic dataset,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE,
    2019, pp. 4348–4353.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Hecker, D. Dai, and L. Van Gool, “End-to-end learning of driving models
    with surround-view cameras and route planners,” in *the European Conference on
    Computer Vision (ECCV)*, 2018, pp. 435–453.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla:
    An open urban driving simulator,” *arXiv preprint arXiv:1711.03938*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] M. Abdou, H. Kamal, S. El-Tantawy, A. Abdelkhalek, O. Adel, K. Hamdy,
    and M. Abaas, “End-to-end deep conditional imitation learning for autonomous driving,”
    in *2019 31st International Conference on Microelectronics (ICM)*.   IEEE, 2019,
    pp. 346–350.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Y. Cui, D. Isele, S. Niekum, and K. Fujimura, “Uncertainty-aware data
    aggregation for deep imitation learning,” in *International Conference on Robotics
    and Automation*.   IEEE, 2019, pp. 761–767.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] L. Tai, P. Yun, Y. Chen, C. Liu, H. Ye, and M. Liu, “Visual-based autonomous
    driving deployment from a stochastic and uncertainty-aware perspective,” *arXiv
    preprint arXiv:1903.00821*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. Buechel and A. Knoll, “Deep reinforcement learning for predictive
    longitudinal control of automated vehicles,” in *International Conference on Intelligent
    Transportation Systems*.   IEEE, 2018, pp. 2391–2397.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] J. Chen, B. Yuan, and M. Tomizuka, “Model-free deep reinforcement learning
    for urban autonomous driving,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE,
    2019, pp. 2765–2771.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] M. Bansal, A. Krizhevsky, and A. Ogale, “Chauffeurnet: Learning to drive
    by imitating the best and synthesizing the worst,” *arXiv preprint arXiv:1812.03079*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] L. Sun, C. Peng, W. Zhan, and M. Tomizuka, “A fast integrated planning
    and control framework for autonomous driving via imitation learning,” in *Dynamic
    Systems and Control Conference*, vol. 51913.   American Society of Mechanical
    Engineers, 2018, p. V003T37A012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] M. Wulfmeier, D. Rao, D. Z. Wang, P. Ondruska, and I. Posner, “Large-scale
    cost function learning for path planning using deep inverse reinforcement learning,”
    *The International Journal of Robotics Research*, vol. 36, no. 10, pp. 1073–1087,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] J. Bernhard, R. Gieselmann, K. Esterle, and A. Knol, “Experience-based
    heuristic search: Robust motion planning with deep q-learning,” in *International
    Conference on Intelligent Transportation Systems*.   IEEE, 2018, pp. 3175–3182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] P. Hart, L. Rychly, and A. Knoll, “Lane-merging using policy-based reinforcement
    learning and post-optimization,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE,
    2019, pp. 3176–3181.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] P. Wang, D. Liu, J. Chen, H. Li, and C.-Y. Chan, “Human-like decision
    making for autonomous driving via adversarial inverse reinforcement learning,”
    *arXiv*, pp. arXiv–1911, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] S. Sharifzadeh, I. Chiotellis, R. Triebel, and D. Cremers, “Learning
    to drive using inverse reinforcement learning and deep q-networks,” *arXiv preprint
    arXiv:1612.03653*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] A. Alizadeh, M. Moghadam, Y. Bicer, N. K. Ure, U. Yavas, and C. Kurtulus,
    “Automated lane change decision making using deep reinforcement learning in dynamic
    and uncertain highway environment,” in *IEEE Intelligent Transportation Systems
    Conference*.   IEEE, 2019, pp. 1399–1404.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] N. Deshpande and A. Spalanzani, “Deep reinforcement learning based vehicle
    navigation amongst pedestrians using a grid-based state representation,” in *IEEE
    Intelligent Transportation Systems Conference*.   IEEE, 2019, pp. 2081–2086.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] T. Tram, I. Batkovic, M. Ali, and J. Sjöberg, “Learning when to drive
    in intersections by combining reinforcement learning and model predictive control,”
    in *International Conference on Intelligent Transportation Systems*.   IEEE, 2019,
    pp. 3263–3268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura, “Navigating
    occluded intersections with autonomous vehicles using deep reinforcement learning,”
    in *International Conference on Robotics and Automation*.   IEEE, 2018, pp. 2034–2039.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] C. Li and K. Czarnecki, “Urban driving with multi-objective deep reinforcement
    learning,” in *International Conference on Autonomous Agents and MultiAgent Systems
    (AAMAS)*.   International Foundation for Autonomous Agents and Multiagent Systems,
    2019, pp. 359–367.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] M. P. Ronecker and Y. Zhu, “Deep q-network based decision making for
    autonomous driving,” in *International Conference on Robotics and Automation Sciences*.   IEEE,
    2019, pp. 154–160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] P. Wolf, K. Kurzer, T. Wingert, F. Kuhnt, and J. M. Zollner, “Adaptive
    behavior generation for autonomous driving using deep reinforcement learning with
    compact semantic states,” in *Intelligent Vehicles Symposium*.   IEEE, 2018, pp.
    993–1000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] B. Mirchevska, C. Pek, M. Werling, M. Althoff, and J. Boedecker, “High-level
    decision making for safe and reasonable autonomous lane changing using reinforcement
    learning,” in *International Conference on Intelligent Transportation Systems*.   IEEE,
    2018, pp. 2156–2162.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] W. Yuan, M. Yang, Y. He, C. Wang, and B. Wang, “Multi-reward architecture
    based reinforcement learning for highway driving policies,” in *International
    Conference on Intelligent Transportation Systems*.   IEEE, 2019, pp. 3810–3815.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Lee and J. W. Choi, “May i cut into your lane?: A policy network to
    learn interactive lane change behavior for autonomous driving,” in *IEEE Intelligent
    Transportation Systems Conference*.   IEEE, 2019, pp. 4342–4347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] C. You, J. Lu, D. Filev, and P. Tsiotras, “Highway traffic modeling and
    decision making for autonomous vehicle using reinforcement learning,” in *Intelligent
    Vehicles Symposium*.   IEEE, 2018, pp. 1227–1232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] L. Wang, F. Ye, Y. Wang, J. Guo, I. Papamichail, M. Papageorgiou, S. Hu,
    and L. Zhang, “A q-learning foresighted approach to ego-efficient lane changes
    of connected and automated vehicles on freeways,” in *IEEE Intelligent Transportation
    Systems Conference*.   IEEE, 2019, pp. 1385–1392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] C.-J. Hoel, K. Wolff, and L. Laine, “Automated speed and lane change
    decision making using deep reinforcement learning,” in *International Conference
    on Intelligent Transportation Systems*.   IEEE, 2018, pp. 2148–2155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Y. Zhang, P. Sun, Y. Yin, L. Lin, and X. Wang, “Human-like autonomous
    vehicle speed control by deep reinforcement learning with double q-learning,”
    in *Intelligent Vehicles Symposium*.   IEEE, 2018, pp. 1251–1256.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] D. Liu, M. Brännstrom, A. Backhouse, and L. Svensson, “Learning faster
    to perform autonomous lane changes by constructing maneuvers from shielded semantic
    actions,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE, 2019,
    pp. 1838–1844.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] K. Min, H. Kim, and K. Huh, “Deep distributional reinforcement learning
    based high-level driving policy determination,” *IEEE Transactions on Intelligent
    Vehicles*, vol. 4, no. 3, pp. 416–424, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] K. Rezaee, P. Yadmellat, M. S. Nosrati, E. A. Abolfathi, M. Elmahgiubi,
    and J. Luo, “Multi-lane cruising using hierarchical planning and reinforcement
    learning,” in *International Conference on Intelligent Transportation Systems*.   IEEE,
    2019, pp. 1800–1806.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] T. Shi, P. Wang, X. Cheng, C.-Y. Chan, and D. Huang, “Driving decision
    and control for automated lane change behavior based on deep reinforcement learning,”
    in *IEEE Intelligent Transportation Systems Conference*.   IEEE, 2019, pp. 2895–2900.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] J. Chen, Z. Wang, and M. Tomizuka, “Deep hierarchical reinforcement learning
    for autonomous driving with distinct behaviors,” in *Intelligent Vehicles Symposium*.   IEEE,
    2018, pp. 1239–1244.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Z. Qiao, K. Muelling, J. Dolan, P. Palanisamy, and P. Mudalige, “Pomdp
    and hierarchical options mdp with continuous actions for autonomous driving at
    intersections,” in *International Conference on Intelligent Transportation Systems*.   IEEE,
    2018, pp. 2377–2382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] C. Paxton, V. Raman, G. D. Hager, and M. Kobilarov, “Combining neural
    networks and tree search for task and motion planning in challenging environments,”
    in *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2017, pp. 6059–6066.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] F. Behbahani, K. Shiarlis, X. Chen, V. Kurin, S. Kasewa, C. Stirbu, J. Gomes,
    S. Paul, F. A. Oliehoek, J. Messias *et al.*, “Learning from demonstration in
    the wild,” in *International Conference on Robotics and Automation*.   IEEE, 2019,
    pp. 775–781.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] L. Chen, Y. Chen, X. Yao, Y. Shan, and L. Chen, “An adaptive path tracking
    controller based on reinforcement learning with urban driving application,” in
    *Intelligent Vehicles Symposium*.   IEEE, 2019, pp. 2411–2416.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] M. Huegle, G. Kalweit, B. Mirchevska, M. Werling, and J. Boedecker, “Dynamic
    input for deep reinforcement learning in autonomous driving,” in *IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2019, pp. 7566–7573.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C. Desjardins and B. Chaib-Draa, “Cooperative adaptive cruise control:
    A reinforcement learning approach,” *IEEE Transactions on intelligent transportation
    systems*, vol. 12, no. 4, pp. 1248–1260, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] D. Zhao, B. Wang, and D. Liu, “A supervised actor–critic approach for
    adaptive cruise control,” *Soft Computing*, vol. 17, no. 11, pp. 2089–2099, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] D. Zhao, Z. Xia, and Q. Zhang, “Model-free optimal control based intelligent
    cruise control with hardware-in-the-loop demonstration [research frontier],” *IEEE
    Computational Intelligence Magazine*, vol. 12, no. 2, pp. 56–69, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] K. Min, H. Kim, and K. Huh, “Deep q learning based high level driving
    policy determination,” in *Intelligent Vehicles Symposium*.   IEEE, 2018, pp.
    226–231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] A. Kuefler, J. Morton, T. Wheeler, and M. Kochenderfer, “Imitating driver
    behavior with generative adversarial networks,” in *Intelligent Vehicles Symposium*.   IEEE,
    2017, pp. 204–211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] R. P. Bhattacharyya, D. J. Phillips, B. Wulfe, J. Morton, A. Kuefler,
    and M. J. Kochenderfer, “Multi-agent imitation learning for driving simulation,”
    in *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2018, pp. 1534–1539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] M. Kuderer, S. Gulati, and W. Burgard, “Learning driving styles for autonomous
    vehicles from demonstration,” in *International Conference on Robotics and Automation*.   IEEE,
    2015, pp. 2641–2646.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] R. P. Bhattacharyya, D. J. Phillips, C. Liu, J. K. Gupta, K. Driggs-Campbell,
    and M. J. Kochenderfer, “Simulating emergent properties of human driving behavior
    using multi-agent reward augmented imitation learning,” in *International Conference
    on Robotics and Automation*.   IEEE, 2019, pp. 789–795.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Z. Huang, X. Xu, H. He, J. Tan, and Z. Sun, “Parameterized batch reinforcement
    learning for longitudinal control of autonomous land vehicles,” *IEEE Transactions
    on Systems, Man, and Cybernetics: Systems*, vol. 49, no. 4, pp. 730–741, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] M. J. Hausknecht and P. Stone, “Deep reinforcement learning in parameterized
    action space,” in *International Conference on Learning Representations*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] M. Everett, Y. F. Chen, and J. P. How, “Motion planning among dynamic,
    decision-making agents with deep reinforcement learning,” in *IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*.   IEEE, 2018, pp. 3052–3059.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Póczos, R. Salakhutdinov, and
    A. J. Smola, “Deep sets,” in *Advances in Neural Information Processing Systems
    30*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] D. Hayashi, Y. Xu, T. Bando, and K. Takeda, “A predictive reward function
    for human-like driving based on a transition model of surrounding environment,”
    in *International Conference on Robotics and Automation*.   IEEE, 2019, pp. 7618–7624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] S. Qi and S.-C. Zhu, “Intent-aware multi-agent reinforcement learning,”
    in *International Conference on Robotics and Automation*.   IEEE, 2018, pp. 7533–7540.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] C. Chen, Y. Liu, S. Kreiss, and A. Alahi, “Crowd-robot interaction: Crowd-aware
    robot navigation with attention-based deep reinforcement learning,” in *International
    Conference on Robotics and Automation*.   IEEE, 2019, pp. 6015–6022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Y. Hu, A. Nakhaei, M. Tomizuka, and K. Fujimura, “Interaction-aware decision
    making with adaptive strategies under merging scenarios,” *arXiv preprint arXiv:1904.06025*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] J. García, Fern, and o Fernández, “A comprehensive survey on safe reinforcement
    learning,” *Journal of Machine Learning Research*, vol. 16, no. 42, pp. 1437–1480,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] N. Jansen, B. Könighofer, S. Junges, and R. Bloem, “Shielded decision-making
    in mdps,” *CoRR*, vol. abs/1807.06096, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] N. Fulton and A. Platzer, “Safe reinforcement learning via formal methods:
    Toward safe control through proof and learning,” in *the Thirty-Second AAAI Conference
    on Artificial Intelligence*, 2018, pp. 6485–6492.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] M. Bouton, J. Karlsson, A. Nakhaei, K. Fujimura, M. J. Kochenderfer,
    and J. Tumova, “Reinforcement learning with probabilistic guarantees for autonomous
    driving,” *arXiv preprint arXiv:1904.07189*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] D. Isele, A. Nakhaei, and K. Fujimura, “Safe reinforcement learning on
    autonomous vehicles,” in *IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS)*.   IEEE, 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] M. Mukadam, A. Cosgun, A. Nakhaei, and K. Fujimura, “Tactical decision
    making for lane changing with deep reinforcement learning,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] X. Xiong, J. Wang, F. Zhang, and K. Li, “Combining deep reinforcement
    learning and safety based control for autonomous driving,” *arXiv preprint arXiv:1612.00147*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement
    learning for autonomous driving,” *CoRR*, vol. abs/1610.03295, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] F. Pusse and M. Klusch, “Hybrid online pomdp planning and deep reinforcement
    learning for safer self-driving cars,” in *Intelligent Vehicles Symposium*.   IEEE,
    2019, pp. 1013–1020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. Jiang, C. Dun, T. Huang, and Z. Lu, “Graph convolutional reinforcement
    learning,” in *International Conference on Learning Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] A. Mohseni-Kabir, D. Isele, and K. Fujimura, “Interaction-aware multi-agent
    reinforcement learning for mobile agents with individual goals,” in *International
    Conference on Robotics and Automation*.   IEEE, 2019, pp. 3370–3376.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] J. F. Fisac, E. Bronstein, E. Stefansson, D. Sadigh, S. S. Sastry, and
    A. D. Dragan, “Hierarchical game-theoretic planning for autonomous vehicles,”
    in *International Conference on Robotics and Automation*.   IEEE, 2019, pp. 9590–9596.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] N. Li, D. W. Oyler, M. Zhang, Y. Yildiz, I. Kolmanovsky, and A. R. Girard,
    “Game theoretic modeling of driver and vehicle interactions for verification and
    validation of autonomous vehicle control systems,” *IEEE Transactions on control
    systems technology*, vol. 26, no. 5, pp. 1782–1797, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] G. Ding, S. Aghli, C. Heckman, and L. Chen, “Game-theoretic cooperative
    lane changing using data-driven models,” in *IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*.   IEEE, 2018, pp. 3640–3647.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] M. Huegle, G. Kalweit, M. Werling, and J. Boedecker, “Dynamic interaction-aware
    scene understanding for reinforcement learning in autonomous driving,” *arXiv
    preprint arXiv:1909.13582*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *International Conference on Learning Representations*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, F. Li, and S. Savarese,
    “Social LSTM: human trajectory prediction in crowded spaces,” in *IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 961–971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, “Social
    GAN: socially acceptable trajectories with generative adversarial networks,” in
    *IEEE Conference on Computer Vision and Pattern Recognition*, 2018, pp. 2255–2264.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] A. Vemula, K. Muelling, and J. Oh, “Social attention: Modeling attention
    in human crowds,” in *International Conference on Robotics and Automation*.   IEEE,
    2018, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Y. Hoshen, “VAIN: attentional multi-agent predictive modeling,” in *Advances
    in Neural Information Processing Systems*, 2017, pp. 2701–2711.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] M. Henaff, A. Canziani, and Y. LeCun, “Model-predictive policy learning
    with uncertainty regularization for driving in dense traffic,” in *International
    Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] C. Guestrin, M. G. Lagoudakis, and R. Parr, “Coordinated reinforcement
    learning,” in *International Conference on Machine Learning*, 2002, pp. 227–234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] C. Yu, X. Wang, X. Xu, M. Zhang, H. Ge, J. Ren, L. Sun, B. Chen, and
    G. Tan, “Distributed multiagent coordinated learning for autonomous driving in
    highways based on dynamic coordination graphs,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 21, no. 2, pp. 735–748, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] K. Leyton-Brown and Y. Shoham, *Essentials of Game Theory: A Concise
    Multidisciplinary Introduction*, ser. Synthesis Lectures on Artificial Intelligence
    and Machine Learning.   Morgan & Claypool Publishers, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] G. P. Papavassilopoulos and M. G. Safonov, “Robust control design via
    game theoretic methods,” in *IEEE Conference on Decision and Control*, 1989, pp.
    382–387 vol.1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] U. Branch, S. Ganebnyi, S. Kumkov, V. Patsko, and S. Pyatko, “Robust
    control in game problems with linear dynamics,” *International Journal of Mathematics,
    Game Theory and Algebra*, vol. 3, 01 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Hong Zhang, V. Kumar, and J. Ostrowski, “Motion planning with uncertainty,”
    in *International Conference on Robotics and Automation*, vol. 1, 1998, pp. 638–643
    vol.1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] M. Zhu, M. Otte, P. Chaudhari, and E. Frazzoli, “Game theoretic controller
    synthesis for multi-robot motion planning part i: Trajectory based algorithms,”
    in *International Conference on Robotics and Automation*, 2014, pp. 1646–1651.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] P. Wilson and D. Stahl, “On players’ models of other players: Theory
    and experimental evidence,” *Games and Economic Behavior*, vol. 10, pp. 218–254,
    07 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] L. Sun, W. Zhan, and M. Tomizuka, “Probabilistic prediction of interactive
    driving behavior via hierarchical inverse reinforcement learning,” in *International
    Conference on Intelligent Transportation Systems*.   IEEE, 2018, pp. 2111–2117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] P. Wang, Y. Li, S. Shekhar, and W. F. Northrop, “Uncertainty estimation
    with distributional reinforcement learning for applications in intelligent transportation
    systems: A case study,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE,
    2019, pp. 3822–3827.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] J. Bernhard, S. Pollok, and A. Knoll, “Addressing inherent uncertainty:
    Risk-sensitive behavior generation for automated driving using distributional
    reinforcement learning,” in *sium*.   IEEE, 2019, pp. 2148–2155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] G. Kahn, A. Villaflor, V. Pong, P. Abbeel, and S. Levine, “Uncertainty-aware
    reinforcement learning for collision avoidance,” *arXiv preprint arXiv:1702.01182*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] S. Choi, K. Lee, S. Lim, and S. Oh, “Uncertainty-aware learning from
    demonstration using mixture density networks with sampling-free variance modeling,”
    in *International Conference on Robotics and Automation*.   IEEE, 2018, pp. 6915–6922.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] K. Lee, K. Saigol, and E. A. Theodorou, “Safe end-to-end imitation learning
    for model predictive control,” *arXiv preprint arXiv:1803.10231*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Y. Gal, “Uncertainty in deep learning,” *University of Cambridge*, vol. 1,
    no. 3, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian deep
    learning for computer vision?” in *Advances in neural information processing systems*,
    2017, pp. 5574–5584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Y. Gal and Z. Ghahramani, “Bayesian convolutional neural networks with
    bernoulli approximate variational inference,” *CoRR*, vol. abs/1506.02158, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Y. Gal, R. Islam, and Z. Ghahramani, “Deep bayesian active learning with
    image data,” in *International Conference on Machine Learning*, 2017, pp. 1183–1192.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] T. G. Dietterich, “Ensemble methods in machine learning,” in *Multiple
    Classifier Systems, First International Workshop, MCS 2000, Cagliari, Italy, June
    21-23, 2000, Proceedings*, ser. Lecture Notes in Computer Science, vol. 1857.   Springer,
    2000, pp. 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable
    predictive uncertainty estimation using deep ensembles,” in *Advances in Neural
    Information Processing Systems 30*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] B. Efron and R. Tibshirani, *An Introduction to the Bootstrap*.   Springer,
    1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective
    on reinforcement learning,” in *International Conference on Machine Learning*,
    2017, pp. 449–458.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] W. Dabney, G. Ostrovski, D. Silver, and R. Munos, “Implicit quantile
    networks for distributional reinforcement learning,” in *International Conference
    on Machine Learning*, 2018, pp. 1104–1113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Y. Li, J. Song, and S. Ermon, “Infogail: Interpretable imitation learning
    from visual demonstrations,” in *Advances in Neural Information Processing Systems*,
    2017, pp. 3812–3822.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] A. Kuefler and M. J. Kochenderfer, “Burn-in demonstrations for multi-modal
    imitation learning,” *arXiv preprint arXiv:1710.05090*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Z. Wang, J. S. Merel, S. E. Reed, N. de Freitas, G. Wayne, and N. Heess,
    “Robust imitation of diverse behaviors,” in *Advances in Neural Information Processing
    Systems*, 2017, pp. 5320–5329.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] J. Merel, Y. Tassa, D. TB, S. Srinivasan, J. Lemmon, Z. Wang, G. Wayne,
    and N. Heess, “Learning human behaviors from motion capture by adversarial imitation,”
    *arXiv preprint arXiv:1707.02201*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] J. Lin and Z. Zhang, “Acgail: Imitation learning about multiple intentions
    with auxiliary classifier gans,” in *Pacific Rim International Conference on Artificial
    Intelligence*.   Springer, 2018, pp. 321–334.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] C. Fei, B. Wang, Y. Zhuang, Z. Zhang, J. Hao, H. Zhang, X. Ji, and W. Liu,
    “Triple-gail: A multi-modal imitation learning framework with generative adversarial
    nets,” in *the Twenty-Ninth International Joint Conference on Artificial Intelligence
    (IJCAI)*, 2020, pp. 2929–2935.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] E. Schmerling, K. Leung, W. Vollprecht, and M. Pavone, “Multimodal probabilistic
    model-based planning for human-robot interaction,” in *International Conference
    on Robotics and Automation*.   IEEE, 2018, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] J. Li, H. Ma, and M. Tomizuka, “Interaction-aware multi-agent tracking
    and probabilistic behavior prediction via adversarial learning,” in *International
    Conference on Robotics and Automation*.   IEEE, 2019, pp. 6658–6664.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] H. Ma, J. Li, W. Zhan, and M. Tomizuka, “Wasserstein generative learning
    with kinematic constraints for probabilistic interactive driving behavior prediction,”
    in *Intelligent Vehicles Symposium*.   IEEE, 2019, pp. 2477–2483.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] A. Turnwald, D. Althoff, D. Wollherr, and M. Buss, “Understanding human
    avoidance behavior: interaction-aware decision making based on game theory,” *International
    Journal of Social Robotics*, vol. 8, no. 2, pp. 331–351, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] R. Tian, S. Li, N. Li, I. Kolmanovsky, A. Girard, and Y. Yildiz, “Adaptive
    game-theoretic decision making for autonomous vehicle control at roundabouts,”
    in *IEEE Conference on Decision and Control (CDC)*.   IEEE, 2018, pp. 321–326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] D. Isele, “Interactive decision making for autonomous vehicles in dense
    traffic,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE, 2019,
    pp. 3981–3986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] H. Bai, S. Cai, N. Ye, D. Hsu, and W. S. Lee, “Intention-aware online
    pomdp planning for autonomous driving in a crowd,” in *International Conference
    on Robotics and Automation*.   IEEE, 2015, pp. 454–460.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] C. Hubmann, J. Schulz, G. Xu, D. Althoff, and C. Stiller, “A belief state
    planner for interactive merge maneuvers in congested traffic,” in *International
    Conference on Intelligent Transportation Systems*.   IEEE, 2018, pp. 1617–1624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] C. A. Holloway, *Decision making under uncertainty: models and choices*.   Prentice-Hall
    Englewood Cliffs, NJ, 1979, vol. 8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] M. J. Kochenderfer, *Decision making under uncertainty: theory and application*.   MIT
    press, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] R. Camacho and D. Michie, “Behavioral cloning A correction,” *AI Mag.*,
    vol. 16, no. 2, p. 92, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/9de2ba3603745830a3f7a5daffbb2ce2.png) | Zeyu Zhu
    received B.S. degree in computer science from Peking University, Beijing, China,
    in 2019, where he is currently pursuing the Ph.D. degree with the Key Laboratory
    of Machine Perception (MOE), Peking University. His research interests include
    intelligent vehicles, reinforcement learning, and machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/ef69948710785d2219365e9917766d29.png) | Huijing
    Zhao received B.S. degree in computer science from Peking University in 1991\.
    She obtained M.E. degree in 1996 and Ph.D. degree in 1999 in civil engineering
    from the University of Tokyo, Japan. From 1999 to 2007, she was a postdoctoral
    researcher and visiting associate professor at the Center for Space Information
    Science, University of Tokyo. In 2007, she joined Peking University as a tenure-track
    professor at the School of Electronics Engineering and Computer Science. She became
    an associate professor with tenure on 2013 and was promoted to full professor
    on 2020\. She has research interest in several areas in connection with intelligent
    vehicle and mobile robot, such as machine perception, behavior learning and motion
    planning, and she has special interests on the studies through real world data
    collection. |'
  prefs: []
  type: TYPE_TB
