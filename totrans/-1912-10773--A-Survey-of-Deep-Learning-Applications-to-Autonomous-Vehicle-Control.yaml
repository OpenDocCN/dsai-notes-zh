- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:03:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:03:33'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1912.10773] A Survey of Deep Learning Applications to Autonomous Vehicle Control'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1912.10773] 关于深度学习在自动驾驶车辆控制中的应用调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.10773](https://ar5iv.labs.arxiv.org/html/1912.10773)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1912.10773](https://ar5iv.labs.arxiv.org/html/1912.10773)
- en: A Survey of Deep Learning Applications to Autonomous Vehicle Control
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于深度学习在自动驾驶车辆控制中的应用调查
- en: 'Sampo Kuutti,  Richard Bowden,  Yaochu Jin,  Phil Barber,  and Saber Fallah
    This work was supported by the UK-EPSRC grant EP/R512217/1 and Jaguar Land Rover.Sampo
    Kuutti and Saber Fallah are with the Centre for Automotive Engineering, University
    of Surrey, Guildford, GU2 7XH, U.K. (e-mail: s.j.kuutti@surrey.ac.uk, s.fallah@surrey.ac.uk).Richard
    Bowden is with the Centre for Vision Speech and Signal Processing, University
    of Surrey, Guildford, GU2 7XH, U.K. (e-mail: r.bowden@surrey.ac.uk).Yaochu Jin
    is with the Department of Computer Science, University of Surrey, Guildford, GU2
    7XH, U.K. (e-mail: yaochu.jin@surrey.ac.uk).Phil Barber was with Jaguar Land Rover
    Limited (e-mail: pbarber2@jaguarlandrover.com).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Sampo Kuutti、**Richard Bowden**、Yaochu Jin、Phil Barber 和 **Saber Fallah**。这项工作得到了英国EPSRC资助（EP/R512217/1）和捷豹路虎的支持。Sampo
    Kuutti 和 Saber Fallah 目前在萨里大学汽车工程中心工作，地址：Guildford, GU2 7XH, U.K.（电子邮件：s.j.kuutti@surrey.ac.uk,
    s.fallah@surrey.ac.uk）。**Richard Bowden** 在萨里大学视觉、语音和信号处理中心工作，地址：Guildford, GU2
    7XH, U.K.（电子邮件：r.bowden@surrey.ac.uk）。Yaochu Jin 在萨里大学计算机科学系工作，地址：Guildford, GU2
    7XH, U.K.（电子邮件：yaochu.jin@surrey.ac.uk）。Phil Barber 曾在捷豹路虎有限公司工作（电子邮件：pbarber2@jaguarlandrover.com）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Designing a controller for autonomous vehicles capable of providing adequate
    performance in all driving scenarios is challenging due to the highly complex
    environment and inability to test the system in the wide variety of scenarios
    which it may encounter after deployment. However, deep learning methods have shown
    great promise in not only providing excellent performance for complex and non-linear
    control problems, but also in generalising previously learned rules to new scenarios.
    For these reasons, the use of deep learning for vehicle control is becoming increasingly
    popular. Although important advancements have been achieved in this field, these
    works have not been fully summarised. This paper surveys a wide range of research
    works reported in the literature which aim to control a vehicle through deep learning
    methods. Although there exists overlap between control and perception, the focus
    of this paper is on vehicle control, rather than the wider perception problem
    which includes tasks such as semantic segmentation and object detection. The paper
    identifies the strengths and limitations of available deep learning methods through
    comparative analysis and discusses the research challenges in terms of computation,
    architecture selection, goal specification, generalisation, verification and validation,
    as well as safety. Overall, this survey brings timely and topical information
    to a rapidly evolving field relevant to intelligent transportation systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个能够在所有驾驶场景中提供足够性能的自动驾驶车辆控制器是具有挑战性的，因为环境非常复杂且无法在部署后遇到的各种场景中进行测试。然而，深度学习方法在处理复杂和非线性控制问题方面表现出了极大的潜力，同时也能将之前学习的规则推广到新的场景。因此，深度学习在车辆控制中的应用越来越受到欢迎。尽管该领域已经取得了重要进展，但这些工作尚未得到充分总结。本文调查了文献中报告的各种研究工作，这些工作旨在通过深度学习方法控制车辆。虽然控制与感知之间存在重叠，但本文重点关注车辆控制，而不是包括语义分割和目标检测等任务的更广泛的感知问题。本文通过比较分析识别了现有深度学习方法的优缺点，并讨论了计算、架构选择、目标指定、推广、验证和验证以及安全等方面的研究挑战。总体而言，本调查为智能交通系统这一迅速发展的领域提供了及时而贴切的信息。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '索引词:'
- en: Machine learning, Neural networks, Intelligent control, Computer vision, Advanced
    driver assistance, Autonomous vehicles<svg   height="66.72" overflow="visible"
    version="1.1" width="604.52"><g transform="translate(0,66.72) matrix(1 0 0 -1
    0 0) translate(-122.74,0) translate(0,-14.11) matrix(1.0 0.0 0.0 1.0 127.35 42.63)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="595.3"
    height="57.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">©2019 IEEE.
    Personal use of this material is permitted. Permission from IEEE must be obtained
    for all other uses, in any current or future media, including reprinting/republishing
    this material for advertising or promotional purposes, creating new collective
    works, for resale or redistribution to servers or lists, or reuse of any copyrighted
    component of this work in other works.</foreignobject></g></svg>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习、神经网络、智能控制、计算机视觉、高级驾驶辅助、自动驾驶车辆<svg   height="66.72" overflow="visible" version="1.1"
    width="604.52"><g transform="translate(0,66.72) matrix(1 0 0 -1 0 0) translate(-122.74,0)
    translate(0,-14.11) matrix(1.0 0.0 0.0 1.0 127.35 42.63)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="595.3" height="57.5" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">©2019 IEEE. Personal use of this material is
    permitted. Permission from IEEE must be obtained for all other uses, in any current
    or future media, including reprinting/republishing this material for advertising
    or promotional purposes, creating new collective works, for resale or redistribution
    to servers or lists, or reuse of any copyrighted component of this work in other
    works.</foreignobject></g></svg>
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: In 2016, traffic accidents resulted in 37,000 fatalities in the United States
    [[1](#bib.bib1)] and 25,500 fatalities in the European Union [[2](#bib.bib2)].
    With the steady increase in the number of vehicles on the road, issues such as
    traffic congestion, pollution, and road safety are becoming critical issues [[3](#bib.bib3)].
    Autonomous vehicles have gained significant interest as solutions to these challenges
    [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]. For instance,
    90% of all car accidents are estimated to be caused by human errors, while only
    2% are caused by vehicle failures [[8](#bib.bib8)]. Further benefits from autonomous
    vehicles in terms of better fuel economy [[9](#bib.bib9), [10](#bib.bib10)], reduced
    pollution, car sharing [[11](#bib.bib11)], increased productivity, and improved
    traffic flow [[12](#bib.bib12)] have also been reported.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年，美国发生的交通事故导致了37,000人遇难[[1](#bib.bib1)]，欧盟则有25,500人遇难[[2](#bib.bib2)]。随着道路上车辆数量的稳定增加，交通拥堵、污染和道路安全等问题正变得越来越关键[[3](#bib.bib3)]。自动驾驶车辆因应对这些挑战而受到广泛关注[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]。例如，估计90%的车祸由人为错误引起，而只有2%是由车辆故障造成的[[8](#bib.bib8)]。自动驾驶车辆在燃油经济性[[9](#bib.bib9),
    [10](#bib.bib10)]、减少污染、车辆共享[[11](#bib.bib11)]、提高生产力和改善交通流量[[12](#bib.bib12)]等方面的进一步好处也已被报道。
- en: Some of the earliest autonomous vehicle projects were presented in 1980s by
    Carnegie Mellon University for driving in structured environments [[13](#bib.bib13)]
    and the University of Bundeswehr Munich for highway driving [[14](#bib.bib14)].
    Since then, projects such as DARPA Grand Challenges [[15](#bib.bib15), [16](#bib.bib16)]
    have continued to drive forward research in autonomous vehicles. Outside of academia,
    car manufacturers and tech companies have also carried out research to develop
    their own autonomous vehicles. This has led to multiple Advanced Driver Assistance
    Systems such as Adaptive Cruise Control (ACC), Lane Keeping Assistance, and Lane
    Departure Warning technologies, which provide modern vehicles with partial autonomy.
    These technologies not only increase the safety of modern vehicles and make driving
    easier but also pave the way for fully autonomous vehicles which do not require
    any human intervention.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最早的自动驾驶车辆项目由卡内基梅隆大学在1980年代展示，用于结构化环境的驾驶[[13](#bib.bib13)]，以及由联邦国防军大学慕尼黑校区展示，用于高速公路驾驶[[14](#bib.bib14)]。此后，诸如DARPA大奖挑战赛[[15](#bib.bib15),
    [16](#bib.bib16)]等项目继续推动自动驾驶车辆的研究。除了学术界，汽车制造商和科技公司也进行了研究，开发自己的自动驾驶车辆。这导致了多种高级驾驶辅助系统，如自适应巡航控制（ACC）、车道保持辅助和车道偏离警告技术，这些技术为现代车辆提供了部分自主性。这些技术不仅提高了现代车辆的安全性，使驾驶更轻松，还为完全自动驾驶车辆铺平了道路，这些车辆无需任何人工干预。
- en: Early autonomous vehicle systems were heavily reliant on accurate sensory data,
    utilising multi-sensor setups and expensive sensors such as LIDAR to provide accurate
    environment perception. Control of these autonomous vehicles was handled via rule-based
    controllers, where the parameters are set by the developers and hand-tuned after
    simulation and field testing [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)].
    The downside of this approach is the time intensive hand-tuning of parameters
    [[20](#bib.bib20)] and the difficulty of such rule-based controllers to generalise
    to new scenarios [[21](#bib.bib21)]. Also, the highly non-linear nature of driving
    means that control methods based on linearisation of the vehicle model or other
    algebraic analytical solutions are often infeasible or do not scale well [[22](#bib.bib22),
    [23](#bib.bib23)]. Recently, deep learning has gained attention due to the numerous
    state-of-the-art results it has achieved in fields such as image classification
    and speech recognition [[24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)].
    This has led to increasing use of deep learning in autonomous vehicle applications,
    including planning and decision making [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31)], perception [[32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)], as well as mapping and
    localisation [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)]. The performance
    of Convolutional Neural Networks (CNNs) with raw camera inputs has the potential
    to reduce the number of sensors used by autonomous vehicles. This has led to some
    organisations investigating autonomous vehicles without expensive sensors such
    as LIDAR, instead employing extensive use of deep learning for scene understanding,
    object recognition, semantic segmentation, and motion estimation. The strong results
    of deep learning in these perception problems have also sparked interest in using
    Deep Neural Networks (DNNs) to produce control actions in autonomous vehicles.
    Indeed, autonomous vehicle control often has a strong link to perception, as many
    techniques use CNNs to predict control actions based on images of the scene, without
    any separate perception module, thereby removing the separation between the perception
    and control layer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的自动驾驶系统高度依赖准确的传感数据，使用多传感器设置和昂贵的传感器，如激光雷达（LIDAR），以提供准确的环境感知。这些自动驾驶车辆的控制通过基于规则的控制器来处理，其中参数由开发者设置，并在仿真和实地测试后进行手动调优[[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)]。这种方法的缺点是参数手动调节非常耗时[[20](#bib.bib20)]，且这种基于规则的控制器难以对新场景进行泛化[[21](#bib.bib21)]。此外，驾驶的高度非线性特征意味着基于车辆模型线性化或其他代数分析解的控制方法通常不可行或扩展性差[[22](#bib.bib22),
    [23](#bib.bib23)]。最近，深度学习因其在图像分类和语音识别等领域取得的诸多前沿成果而受到关注[[24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26)]。这导致深度学习在自动驾驶应用中得到越来越多的使用，包括规划和决策[[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]，感知[[32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)]，以及地图绘制和定位[[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39)]。卷积神经网络（CNNs）在原始摄像头输入上的表现有可能减少自动驾驶车辆使用的传感器数量。这导致一些组织开始研究不使用昂贵传感器如激光雷达的自动驾驶车辆，而是广泛使用深度学习进行场景理解、物体识别、语义分割和运动估计。深度学习在这些感知问题上的强劲结果也引发了使用深度神经网络（DNNs）生成自动驾驶车辆控制动作的兴趣。实际上，自动驾驶车辆的控制往往与感知有着密切的联系，因为许多技术使用CNNs根据场景图像预测控制动作，而不需要独立的感知模块，从而消除了感知层与控制层之间的隔离。
- en: Deep learning offers several benefits for vehicle control. The ability to self-optimise
    its behaviour from data and adapt to new scenarios makes deep learning well suited
    to control problems in complex and dynamic environments [[40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42)]. Rather than having to tune each parameter iteratively while
    trying to maintain performance in all foreseeable scenarios, deep learning enables
    developers to describe the desired behaviour and teach the system to perform well
    and generalise to new environments through learning [[43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47)]. For these reasons, there
    has been significant interest in deep learning for autonomous vehicle control
    in recent years. There are a variety of different sensor configurations; whilst
    some researchers aim to control the vehicle with camera vision only, others utilise
    lower dimensional data from ranging sensors, and some use multi-sensor set ups.
    There are also some differences in terms of the control objective, some formulate
    the system as a high-level controller which provides, for example, desired acceleration,
    which is then realised through a low-level controller, often using classical control
    techniques. Others aim to learn driving end-to-end, mapping observations directly
    to low-level vehicle control interface commands. Although there has been a large
    variety of different approaches used to tackle autonomous vehicle control via
    deep learning, currently there is a lack of analysis and comparison between these
    different techniques. This manuscript aims to fill this gap in the literature,
    by reviewing the deep learning approaches to vehicle control and analysing their
    performance. Furthermore, the manuscript will evaluate the current state of the
    field, identify the main research challenges, and make recommendations for the
    direction of future research.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习为车辆控制提供了几个好处。深度学习能够从数据中自我优化行为并适应新场景，使其非常适合在复杂和动态环境中的控制问题[[40](#bib.bib40),
    [41](#bib.bib41), [42](#bib.bib42)]。与其在所有可预见的场景中尝试维护性能并逐步调整每个参数，深度学习使开发人员能够描述期望的行为，并通过学习教会系统在新环境中表现良好并进行泛化[[43](#bib.bib43),
    [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47)]。因此，近年来在自动驾驶车辆控制中对深度学习的兴趣显著增加。存在多种不同的传感器配置；一些研究者旨在仅通过摄像头视觉来控制车辆，而其他人则利用来自距离传感器的低维数据，还有一些使用多传感器配置。在控制目标方面也存在一些差异，有些将系统制定为高层控制器，例如提供期望加速度，然后通过低层控制器实现，通常使用经典控制技术。其他人则旨在端到端学习驾驶，将观察直接映射到低层车辆控制接口命令。尽管目前有多种不同的方法用于通过深度学习解决自动驾驶车辆控制问题，但目前缺乏对这些不同技术的分析和比较。本手稿旨在填补这一文献空白，通过回顾深度学习在车辆控制中的应用并分析其性能。此外，本手稿还将评估该领域的当前状态，识别主要研究挑战，并提出未来研究的方向建议。
- en: 'The remainder of this manuscript is structured as follows. Section II provides
    a brief introduction to deep learning methods and approaches relevant to autonomous
    vehicles. Section III discusses recent approaches to autonomous vehicle control
    using deep learning, which is broken into three categories: (A) lateral, (B) longitudinal,
    and (C) simultaneous lateral and longitudinal control. Section IV presents the
    main research challenges from the previous section’s discussion. Finally, Section
    V concludes the current state of the field and provides recommendations for the
    direction of future research.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本手稿的其余部分结构如下。第二部分简要介绍了与自动驾驶车辆相关的深度学习方法和方法。第三部分讨论了使用深度学习的自动驾驶车辆控制的最新方法，分为三类：(A)
    横向控制，(B) 纵向控制，以及 (C) 同时横向和纵向控制。第四部分呈现了前一部分讨论中的主要研究挑战。最后，第五部分总结了该领域的当前状态，并为未来研究方向提供建议。
- en: II Review of Deep Learning
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 深度学习综述
- en: In this section, we briefly introduce the deep learning techniques and approaches
    related to the works discussed in later sections. A brief summary on learning
    strategies, datasets, and tools for deep learning in autonomous vehicles is given.
    Since a full description on all deep learning algorithms used in autonomous vehicles
    would be out of the scope of this manuscript, we refer the interested reader to
    the insightful texts on this topic in [[48](#bib.bib48), [49](#bib.bib49), [46](#bib.bib46),
    [47](#bib.bib47), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要介绍了与后续章节讨论的工作相关的深度学习技术和方法。提供了关于学习策略、数据集和自动驾驶深度学习工具的简要总结。由于对所有在自动驾驶中使用的深度学习算法进行全面描述超出了本文的范围，我们建议感兴趣的读者查阅该主题的有见地的文献[[48](#bib.bib48),
    [49](#bib.bib49), [46](#bib.bib46), [47](#bib.bib47), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52)]。
- en: II-A Supervised Learning
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 监督学习
- en: In deep learning, the objective is to update the weights of a deep neural network
    during training, such that the model learns to represent a useful function for
    its task. There are numerous learning algorithms available, but most algorithms
    described in this manuscript can be classified as supervised or reinforcement
    learning. Supervised learning utilises labelled data, where an expert demonstrates
    performing the chosen task at hand. Each data point in the set includes an observation-action
    pair, which the neural network then learns to model. During training, the network
    approximates its own action for each observation, and compares the error to the
    labelled action by the expert. The advantage of supervised learning is speed of
    training convergence and no need to specify how the task should be performed.
    While the simplicity of the supervised approach is appealing, the approach has
    some disadvantages. Firstly, during training the network makes predictions on
    the control action in an offline framework, where the network’s predictions do
    not affect the states seen during training. However, once deployed, the network’s
    actions will affect future states, breaching the i.i.d. assumption made by most
    learning algorithms [[53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55)]. This
    leads to a distribution shift between training and operation, which can lead to
    the network making mistakes due to the unfamiliar state distributions seen during
    operation. Secondly, learning a behaviour from demonstration leaves the network
    susceptible to biases in the data set. For complex tasks, such as autonomous driving,
    the diversity of the data set should be ensured if the aim is to train a generalisable
    model which can drive in all different environments [[56](#bib.bib56), [57](#bib.bib57)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，目标是在训练过程中更新深度神经网络的权重，以使模型学会表示其任务的有用函数。虽然有许多学习算法可用，但本文描述的大多数算法可以归类为监督学习或强化学习。监督学习利用带标签的数据，其中专家演示执行所选择的任务。数据集中的每个数据点包括观察-动作对，神经网络学习对其进行建模。在训练过程中，网络为每个观察近似其自身的动作，并将误差与专家标记的动作进行比较。监督学习的优点在于训练收敛速度快，无需指定如何执行任务。虽然监督学习的方法简单，但也有一些缺点。首先，在训练过程中，网络在离线框架中对控制动作进行预测，其中网络的预测不会影响训练过程中看到的状态。然而，一旦部署，网络的动作将影响未来的状态，突破了大多数学习算法所做的i.i.d.假设[[53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55)]。这导致训练与操作之间的分布变化，可能导致网络因操作过程中遇到的不熟悉的状态分布而犯错。其次，从示范中学习行为使网络容易受到数据集偏差的影响。对于复杂任务，如自动驾驶，如果目标是训练一个能够在所有不同环境中驾驶的可泛化模型，则应确保数据集的多样性[[56](#bib.bib56),
    [57](#bib.bib57)]。
- en: II-B Reinforcement Learning
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 强化学习
- en: Reinforcement learning enables the model to learn to perform the task through
    trial and error. Reinforcement learning can be modelled as a Markov decision process,
    formally described as a tuple (S, A, P, R), where S denotes the state space, A
    represents the action space of possible actions, P denotes the state transition
    probability model, and R represents the reward function. At each time-step the
    agent observes a set of states $s_{t}$, takes an action $a_{t}$ from possible
    actions A, and then the environment transitions according to P. The agent then
    observes a new set of states $s_{t+1}$ and receives a reward $r_{t}$. The aim
    of the agent is to learn a policy $\pi(s_{t},a_{t})$ mapping observations to actions
    such that the accumulated rewards are maximised. Therefore, the agent can learn
    from its own actions through interactions with the environment and receives an
    estimate of its performance through the reward function. The advantage of this
    approach is that no labelled data sets are required and a behaviour which generalises
    well to new scenarios can be learned through reinforcement learning. The downside
    of reinforcement learning is its low sample efficiency [[58](#bib.bib58)], which
    means converging to an optimal policy can be slow, thereby requiring time-intensive
    simulations or costly real-world training [[50](#bib.bib50)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习使模型能够通过试错学习执行任务。强化学习可以被建模为一个马尔可夫决策过程，形式上描述为一个元组 (S, A, P, R)，其中 S 表示状态空间，A
    代表可能的动作空间，P 表示状态转移概率模型，R 代表奖励函数。在每个时间步，代理观察一组状态 $s_{t}$，从可能的动作 A 中选择一个动作 $a_{t}$，然后环境根据
    P 进行转移。代理接着观察到一组新的状态 $s_{t+1}$ 并获得奖励 $r_{t}$。代理的目标是学习一个策略 $\pi(s_{t},a_{t})$，将观察映射到动作，从而最大化累积奖励。因此，代理可以通过与环境的交互从自身行为中学习，并通过奖励函数获得其性能的估计。这种方法的优点是无需标记数据集，并且可以通过强化学习学习到在新场景中表现良好的行为。强化学习的缺点是其样本效率低
    [[58](#bib.bib58)]，这意味着收敛到最优策略可能较慢，从而需要时间密集的模拟或昂贵的现实世界训练 [[50](#bib.bib50)]。
- en: 'Reinforcement learning algorithms can be divided into three classes: value-based,
    policy gradient, and actor-critic algorithms [[59](#bib.bib59)]. Value-based algorithms
    (e.g. Q-learning [[60](#bib.bib60)]) estimate the value function $V(s)$, which
    represents the value (expected reward) of being in a given state. If the state
    transition dynamics P are known, the policy can choose actions which bring it
    to states such that the expected rewards are maximised. However, in most reinforcement
    learning settings the environment model is not known. Therefore, the state-action
    value or quality function $Q(s,a)$, which estimates the value of a given action
    in a given state, is used instead. The optimal policy is then found by greedily
    maximising the state-action value function $Q(s,a)$. The disadvantage of this
    approach is that there is no guarantee on the optimality of the learned policy
    [[61](#bib.bib61), [62](#bib.bib62)]. Policy gradient algorithms (e.g REINFORCE
    [[63](#bib.bib63)]) do not estimate a value function, but instead parametrise
    the policy and then update the parameters to maximise the expected rewards. This
    is done by constructing a loss function and estimating a gradient of the loss
    function with respect to the network parameters. During training, the network
    parameters are then updated in the direction of the policy gradient. The main
    disadvantage of this approach is the high variance in the estimated policy gradients
    [[64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66)]. The third class, actor-critic
    algorithms (e.g. A3C [[67](#bib.bib67)]), are hybrid methods which combine the
    use of a value function with a parametrised policy function. This creates a trade-off
    between the disadvantages of the high variance of policy gradients and the bias
    of value-based methods [[51](#bib.bib51), [68](#bib.bib68), [69](#bib.bib69)].
    Another separating factor between different reinforcement learning algorithms
    is the type of reward function used. The reward function used can be either sparse
    or dense. In a sparse reward function, the agent only receives a reward following
    specific events, such as success or failure in its task. The benefit of this approach
    is that the success (e.g. reaching a goal location) or failure (e.g. colliding
    with another object) is easy to define for most tasks. However, this can further
    exacerbate the sample complexity issue in reinforcement learning, since the agent
    would only receive a reward relatively rarely, resulting in slow convergence.
    On the other hand, in a dense reward function the agent is given a reward at every
    time-step based on the state it is in. This means that the agent receives a continuous
    learning signal, estimating how useful the chosen actions were in their respective
    states.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法可以分为三类：基于价值的方法、策略梯度方法和演员-评论家算法[[59](#bib.bib59)]。基于价值的方法（例如Q-learning[[60](#bib.bib60)]）估计价值函数$V(s)$，该函数表示在给定状态下的价值（期望回报）。如果状态转移动态P已知，则策略可以选择使其到达的状态，以最大化期望回报。然而，在大多数强化学习环境中，环境模型是未知的。因此，使用状态-动作价值或质量函数$Q(s,a)$来估计给定状态下给定动作的价值。然后通过贪婪地最大化状态-动作价值函数$Q(s,a)$来找到最优策略。这种方法的缺点是没有保证所学策略的最优性[[61](#bib.bib61),
    [62](#bib.bib62)]。策略梯度算法（例如REINFORCE[[63](#bib.bib63)]）不估计价值函数，而是对策略进行参数化，然后更新参数以最大化期望回报。这是通过构造损失函数并估计损失函数相对于网络参数的梯度来完成的。在训练过程中，网络参数沿着策略梯度的方向进行更新。这种方法的主要缺点是估计的策略梯度具有较高的方差[[64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66)]。第三类，演员-评论家算法（例如A3C[[67](#bib.bib67)]），是结合了价值函数和参数化策略函数的混合方法。这在策略梯度的高方差和基于价值方法的偏差之间创建了权衡[[51](#bib.bib51),
    [68](#bib.bib68), [69](#bib.bib69)]。不同强化学习算法之间的另一个区别因素是所使用的奖励函数类型。奖励函数可以是稀疏的或密集的。在稀疏奖励函数中，智能体仅在特定事件之后（如任务的成功或失败）获得奖励。这种方法的好处是成功（例如达到目标位置）或失败（例如与其他对象碰撞）对于大多数任务而言易于定义。然而，这可能进一步加剧强化学习中的样本复杂性问题，因为智能体获得奖励的频率相对较低，从而导致收敛缓慢。另一方面，在密集奖励函数中，智能体在每个时间步都根据其所处状态获得奖励。这意味着智能体接收到连续的学习信号，估计所选择动作在各自状态下的有效性。
- en: II-C Datasets and Tools for Deep Learning
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 深度学习的数据集和工具
- en: The rapid progress in the implementation of deep learning systems on autonomous
    vehicles has led to the availability of diverse deep learning data sets for autonomous
    driving and perception. Perhaps the most well known data set for autonomous driving
    is the KITTI benchmark suite [[70](#bib.bib70)], [[71](#bib.bib71)], which includes
    multiple data sets for evaluation of stereo vision, optical flow, scene flow,
    simultaneous localisation and mapping, object detection and tracking, road detection
    and semantic segmentation. Other useful data sets include the Waymo Open [[72](#bib.bib72)],
    Oxford Robotcar [[73](#bib.bib73)], ApolloScape [[74](#bib.bib74)], Udacity [[75](#bib.bib75)],
    ETH Pedestrian [[76](#bib.bib76)], and Caltech Pedestrian [[77](#bib.bib77)] data
    sets. For a more complete overview of available autonomous driving data sets,
    see the survey by Yin & Berger [[78](#bib.bib78)]. Besides public data sets, there
    are also a number of other tools available for the development of deep learning
    in autonomous vehicles. The current leading Artificial Intelligence (AI) platform
    for autonomous driving is the NVIDIA Drive PX2 [[79](#bib.bib79)], which provides
    two Tegra system-on-chips (SoC) and two Pascal graphics processors with dedicated
    memory and specialised support for DNN calculations. For more diverse tasks, the
    MobilEye EyeQ5 [[80](#bib.bib80)] provides four fully programmable accelerators,
    each optimised for a different family of machine learning algorithms. This diversity
    can be useful in systems where different families of deep learning algorithms
    have been used. On the other hand, Altera’s Cyclone V [[81](#bib.bib81)] SoC provides
    a driving solution optimised for sensor fusion. For a more in-depth review of
    autonomous driving hardware platforms, see the discussion by Liu et al. [[82](#bib.bib82)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统在自动驾驶车辆上的快速进展带来了各种深度学习数据集用于自动驾驶和感知。也许最著名的自动驾驶数据集是KITTI基准套件[[70](#bib.bib70)]、[[71](#bib.bib71)]，它包括多个用于立体视觉、光流、场景流、同步定位与地图构建、目标检测与跟踪、道路检测和语义分割的评估数据集。其他有用的数据集包括Waymo
    Open[[72](#bib.bib72)]、Oxford Robotcar[[73](#bib.bib73)]、ApolloScape[[74](#bib.bib74)]、Udacity[[75](#bib.bib75)]、ETH
    Pedestrian[[76](#bib.bib76)]和Caltech Pedestrian[[77](#bib.bib77)]数据集。有关可用自动驾驶数据集的更完整概述，请参见Yin
    & Berger的调查[[78](#bib.bib78)]。除了公共数据集外，还有许多其他工具可用于自动驾驶中的深度学习开发。目前领先的自动驾驶人工智能（AI）平台是NVIDIA
    Drive PX2[[79](#bib.bib79)]，它提供两个Tegra系统芯片（SoC）和两个Pascal图形处理器，配备专用内存和针对DNN计算的专门支持。对于更多样化的任务，MobilEye
    EyeQ5[[80](#bib.bib80)]提供四个完全可编程的加速器，每个加速器都针对不同类别的机器学习算法进行了优化。这种多样性在使用不同类别的深度学习算法的系统中可能会有所帮助。另一方面，Altera的Cyclone
    V[[81](#bib.bib81)] SoC提供了一种针对传感器融合优化的驱动解决方案。有关自动驾驶硬件平台的更深入评审，请参见Liu等人的讨论[[82](#bib.bib82)]。
- en: III Deep Learning Applications to Vehicle Control
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度学习在车辆控制中的应用
- en: The motion control of a vehicle can be broadly divided into two tasks; lateral
    motion of the vehicle is controlled by the steering of the vehicle, whilst longitudinal
    motion is controlled through manipulating the gas and brake pedals of the vehicle.
    Lateral control systems aim to control the vehicle’s position on the lane, as
    well as carry out other lateral actions such as lane changes or collision avoidance
    manoeuvres. In the deep learning domain, this is typically achieved by capturing
    the environment using the images from on-board cameras as the input to the neural
    network. Longitudinal control manages the acceleration of the vehicle such that
    it maintains the desirable velocity on the road, keeps a safe distance from the
    preceding vehicle, and avoids rear-end collisions. While lateral control is typically
    achieved through vision, the longitudinal control relies on measurements of relative
    velocity and distance to the preceding/following vehicles. This means that ranging
    sensors such as RADAR or LIDAR are more commonly used in longitudinal control
    systems. The majority of the current research projects have chosen to focus on
    only one of these actions, thereby simplifying the control problem. Moreover,
    both types of control systems have different challenges and differ in terms of
    implementation (e.g. sensor setups, test/use cases). For these reasons this section
    is split into three subsections, with the first two subsections discussing lateral
    and longitudinal control systems, independently, and the third subsection focusing
    on techniques which have attempted to combine both longitudinal and lateral control.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆的运动控制可以大致分为两个任务；车辆的横向运动由车辆的转向控制，而纵向运动则通过操作油门和刹车踏板来控制。横向控制系统旨在控制车辆在车道上的位置，以及执行其他横向操作，如变道或碰撞避免机动。在深度学习领域，这通常通过使用车载摄像头的图像来捕捉环境，并将其作为神经网络的输入来实现。纵向控制管理车辆的加速，以保持道路上的理想速度，保持与前方车辆的安全距离，并避免追尾碰撞。虽然横向控制通常通过视觉来实现，但纵向控制依赖于对前方/后方车辆的相对速度和距离的测量。这意味着像雷达（RADAR）或激光雷达（LIDAR）这样的测距传感器在纵向控制系统中更为常见。目前大多数研究项目选择只关注其中一个动作，从而简化了控制问题。此外，两种控制系统面临不同的挑战，并在实施方面有所不同（例如传感器设置、测试/使用案例）。因此，本节被分为三个子节，其中前两个子节分别讨论横向和纵向控制系统，第三个子节则关注尝试将纵向控制和横向控制相结合的技术。
- en: III-A Lateral Control Systems
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 横向控制系统
- en: One of the earliest applications of artificial neural networks to the vehicle
    control problem was the Autonomous Land Vehicle in a Neural Network (ALVINN) system
    by Pomerleau in 1989 which was first described in [[83](#bib.bib83)] and further
    extended in [[84](#bib.bib84)]. ALVINN utilised a feedforward neural network,
    with a 30x32-neuron input layer, one hidden layer with four neurons, and a 30-neuron
    output layer in which each neuron represents a possible discrete steering action.
    The system used the input from a camera together with the steering commands of
    the human driver as training data. To increase the amount of data and variety
    of scenarios available, the author employed data augmentation methods to increase
    the available training data without recording any additional footage; each image
    was shifted and rotated, so as to make the vehicle appear to be situated at a
    different part of the road laterally. Additionally, to avoid bias towards recent
    inputs (e.g. if a training session ends in a long right hand turn, the system
    could be biased to turn right more often) a buffering solution was used where
    previously encountered training patterns were retained in the buffer. The buffer
    contained 4 patterns of previous data at any time, which were periodically replaced
    such that the patterns in the buffer had no right or left bias on average. Both
    the image shifting as well as buffering solutions were shown to significantly
    improve the system performance. The system was trained on a 150m stretch of road,
    after which it was tested on a separate stretch of road at speeds ranging from
    5 to 55mph allowing steering without intervention for distances of up to 22 miles.
    The system was shown to be able to remain, on average, 1.6cm distance from the
    centre of the road compared to that of 4.0cm under human control. This demonstrated
    that neural networks can learn to steer a vehicle from recorded data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络在车辆控制问题中的最早应用之一是1989年由Pomerleau提出的自主陆地车辆神经网络（ALVINN）系统，最初在[[83](#bib.bib83)]中描述，并在[[84](#bib.bib84)]中进一步扩展。ALVINN使用了一个前馈神经网络，具有一个30x32神经元的输入层，一个包含四个神经元的隐藏层，以及一个30神经元的输出层，其中每个神经元代表一个可能的离散转向动作。该系统使用来自摄像头的输入数据以及人类驾驶员的转向命令作为训练数据。为了增加数据量和场景的多样性，作者采用了数据增强方法，在不录制额外视频的情况下增加了可用的训练数据；每张图像都被移动和旋转，以使车辆看起来位于道路的不同位置。此外，为了避免对最近输入的偏见（例如，如果训练会话以长时间的右转结束，系统可能会倾向于更频繁地右转），采用了缓冲解决方案，之前遇到的训练模式被保留在缓冲区。缓冲区随时包含4个以前的数据模式，这些模式会定期被替换，从而使缓冲区中的模式在平均水平上没有左右偏差。图像位移和缓冲解决方案都显著提高了系统性能。该系统在150米长的道路上进行了训练，随后在不同的道路上进行测试，速度范围从5到55英里每小时，允许在最长22英里的距离内无需干预地进行转向。该系统被证明能够在道路中心平均保持1.6厘米的距离，而人类控制下为4.0厘米。这表明神经网络可以通过记录的数据学习转向操作。
- en: The first to suggest reinforcement learning for vehicle steering was the work
    carried out by Yu [[85](#bib.bib85)]. Yu proposed a road following system based
    on Pomerleau’s work utilising reinforcement learning to design a controller. The
    advantage of which was the ability to learn from previous experiences to drive
    in new environments and continuously learn and improve its road following ability
    through online learning. Combining supervised learning and reinforcement learning,
    Moriarty et al. [[86](#bib.bib86)] developed a lane-selection strategy for a highway
    environment. The results showed that the vehicles with learned controllers managed
    to maintain speeds close to the desired speed and resulted in less lane-changes.
    Moreover, the learned control strategy resulted in better traffic flow than manually
    constructed controllers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个提出用于车辆转向的强化学习的是Yu [[85](#bib.bib85)]的工作。Yu提出了一种基于Pomerleau工作的道路跟随系统，利用强化学习设计控制器。其优点在于能够从以前的经验中学习，以适应新的环境，并通过在线学习不断提高其道路跟随能力。Moriarty等人[[86](#bib.bib86)]结合监督学习和强化学习，开发了一种高速公路环境下的车道选择策略。结果表明，使用学习控制器的车辆能够保持接近期望速度的行驶，并减少车道变换。此外，学习的控制策略比手动构建的控制器产生了更好的交通流。
- en: The neural networks utilised in the aforementioned early works are significantly
    smaller when compared to what is feasible with today’s technology [[87](#bib.bib87)].
    Indeed, while neural networks are hardly new, the research interest and adoption
    to various applications has exploded in recent years due to increased computing
    power, especially through parallel graphics processing units (GPUs) which can
    significantly reduce training time and improve performance. Moreover, the availability
    of large public data sets and hardware solutions optimised for deep learning have
    made training and validation of neural network systems easier. Overall, these
    recent advancements have enabled better performance through more complex systems
    with vastly increased amounts of training data and episodes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与今天的技术相比，前述早期工作的神经网络要小得多[[87](#bib.bib87)]。事实上，虽然神经网络并不新鲜，但由于计算能力的提升，特别是通过并行图形处理单元（GPU）显著缩短了训练时间并提高了性能，近年来的研究兴趣和应用扩展迅速。此外，大型公共数据集和针对深度学习优化的硬件解决方案的可用性，使得神经网络系统的训练和验证变得更加容易。总体而言，这些近期的进展使得通过更复杂的系统和大量增加的训练数据和训练轮次，实现了更好的性能。
- en: Utilising deeper models with CNNs, Muller et al. [[88](#bib.bib88)] trained
    a sub-scale radio controlled car to navigate off-road in the DARPA Autonomous
    VEhicle (DAVE) project. The model was trained with training data collected from
    two forward-facing cameras while a human was controlling the vehicle. Using a
    6-layer CNN, the model learned to navigate around obstacles when driving at speeds
    of 2m/s. Building on the approach of DAVE, NVIDIA utilised a CNN to create an
    end-to-end control system for steering of a vehicle through supervised learning
    [[87](#bib.bib87)]. The system is capable of self-optimising the system performance
    and detecting useful environmental features (e.g. detection of roads and lanes).
    The CNN used (see Fig. [1](#S3.F1 "Figure 1 ‣ III-A Lateral Control Systems ‣
    III Deep Learning Applications to Vehicle Control ‣ A Survey of Deep Learning
    Applications to Autonomous Vehicle Control")) can learn the steering policy without
    explicit manual decomposition of the environmental features, path planning, or
    control actions using a small amount of training data. The training data set consisted
    of recorded camera footage and steering signals from a human driven vehicle. The
    CNN consisted of 9 layers, including a normalisation layer, 5 convolutional layers
    and 3 fully connected layers, with a total of 27 million connections and 250,000
    parameters. This method achieved a 98% autonomy in initial testing and 100% autonomy
    during a 10-mile highway test, measured based on the number of interventions required
    over a given test time. However, it should be noted that this measure does not
    include lane changes or turns, and therefore only evaluates the system’s ability
    to stay in its current lane.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 利用深层模型和卷积神经网络（CNN），Muller 等人[[88](#bib.bib88)] 训练了一辆亚缩放的遥控车在DARPA自主车辆（DAVE）项目中进行越野导航。该模型使用了从两台前向摄像头收集的训练数据，期间由人工控制车辆。通过使用6层CNN，该模型在以2m/s的速度行驶时学习绕过障碍物。基于DAVE的方法，NVIDIA
    利用CNN创建了一个端到端的控制系统，用于通过监督学习来控制车辆的转向[[87](#bib.bib87)]。该系统能够自我优化系统性能并检测有用的环境特征（例如道路和车道的检测）。使用的CNN（见图[1](#S3.F1
    "图 1 ‣ III-A 横向控制系统 ‣ III 深度学习应用于车辆控制 ‣ 深度学习应用于自主车辆控制的调查")）能够在没有显式手动分解环境特征、路径规划或控制动作的情况下，使用少量训练数据学习转向策略。训练数据集包含记录的摄像机视频和来自人工驾驶车辆的转向信号。CNN包含9层，包括一个标准化层、5个卷积层和3个全连接层，总共有2700万个连接和250,000个参数。这种方法在初步测试中实现了98%的自主性，在10英里高速公路测试中实现了100%的自主性，这些都是基于在给定测试时间内所需的干预次数来衡量的。然而，需要注意的是，这一衡量标准不包括车道变更或转弯，因此仅评估系统在当前车道内保持行驶的能力。
- en: '![Refer to caption](img/f818c005ddad563a1b45621cce479bcf.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f818c005ddad563a1b45621cce479bcf.png)'
- en: 'Figure 1: Convolutional Neural Network utilised in the NVIDIA end-to-end steering
    system. (Figure recreated based on [[87](#bib.bib87)]).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：NVIDIA端到端转向系统中使用的卷积神经网络。（图基于[[87](#bib.bib87)]重绘）。
- en: A further example of supervised learning for steering of an autonomous vehicle
    is the work by Rausch et al. [[42](#bib.bib42)], where supervised learning was
    employed to create an end-to-end lateral vehicle controller. Rausch et al. utilised
    a CNN with four hidden layers, three convolutional layers and one fully connected
    layer. The training data was the steering angle and front-facing camera footage
    which was provided by a human steering a vehicle in a CarSim [[89](#bib.bib89)]
    simulation, with imaging captured at 12 frames per second (FPS) at a resolution
    of 1912x1036\. The data collection was collected from a 15-minute simulation run
    resulting in a total of 10,800 frames. Inappropriate frames caused by bad driving
    behaviour or graphic errors (e.g. due to a fault in the simulator) were removed
    from the training data manually. Then, the neural network was trained with three
    different optimisation algorithms to update the network weights, namely Stochastic
    Gradient Descent (SGD) [[90](#bib.bib90)], Adam [[91](#bib.bib91)], and Nesterov’s
    Accelerated Gradient (NAG) [[92](#bib.bib92)]. During training Adam resulted in
    the best loss convergence, while during the evaluation, the NAG trained network
    performed the best in terms of keeping the vehicle in the centre of the lane.
    Therefore, convergence of the loss function is not necessarily representative
    of a well-trained neural network. The neural networks were shown to learn good
    estimations of the human driver’s steering policy, however by comparing the steering
    angles, it could be seen that the steering signal of the neural networks included
    noisy behaviour. A potential reason is that the system estimates the required
    steering angle at each frame, with no context regarding previous states or actions.
    This results in the steering signals between subsequent time steps varying significantly
    from each other, causing noisy output. This could be resolved by utilising a RNN
    to provide memory of previous inputs and outputs for the system, giving it temporal
    context.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由Rausch等人提供的一个进一步的监督学习示例用于自主车辆的转向 [[42](#bib.bib42)]，其中监督学习被用来创建一个端到端的横向车辆控制器。Rausch等人利用了一个包含四层隐藏层、三层卷积层和一层全连接层的CNN。训练数据包括转向角度和由人类在CarSim
    [[89](#bib.bib89)]模拟中驾驶车辆时提供的前向摄像头画面，图像以每秒12帧（FPS）的速度捕获，分辨率为1912x1036。数据收集来自一次15分钟的模拟运行，共获得了10,800帧。由于驾驶行为不当或图形错误（例如，模拟器故障）导致的不适当帧被手动从训练数据中移除。然后，使用三种不同的优化算法来训练神经网络以更新网络权重，即随机梯度下降（SGD）
    [[90](#bib.bib90)]、Adam [[91](#bib.bib91)] 和 Nesterov加速梯度（NAG） [[92](#bib.bib92)]。在训练过程中，Adam的损失收敛效果最好，而在评估期间，NAG训练的网络在保持车辆在车道中心方面表现最佳。因此，损失函数的收敛性不一定代表网络的良好训练。神经网络显示出对人类驾驶员转向策略的良好估计，但通过比较转向角度，可以看到神经网络的转向信号包含噪声行为。一个可能的原因是系统在每帧上估计所需的转向角度，而没有考虑以前的状态或动作。这导致了随时间步骤变化的转向信号之间差异显著，从而产生噪声输出。通过使用RNN来提供对系统的先前输入和输出的记忆，从而为系统提供时间上下文，可以解决这一问题。
- en: Introducing temporal context to a deep learning steering model, Eraqi et al.
    [[93](#bib.bib93)] utilised a Convolutional Long Short-Term Memory Recurrent Neural
    Network (C-LSTM) to learn to steer a vehicle based on visual and dynamic temporal
    dependencies. The network was trained to predict steering angles based on image
    inputs, and then compared it to a simple CNN architecture used in [[94](#bib.bib94)].
    Experimental results showed improved accuracy and smoother steering variations
    when using the C-LSTM network. However, the model was only evaluated offline by
    comparing the predicted control action against ground truth, which does not necessarily
    give an accurate evaluation of driving quality [[95](#bib.bib95)]. Live testing,
    where the model can control the vehicle to test the learned driving behaviour,
    should be used instead.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为深度学习转向模型引入时间上下文，Eraqi等人 [[93](#bib.bib93)] 利用卷积长短期记忆递归神经网络（C-LSTM）来基于视觉和动态时间依赖关系学习车辆转向。该网络被训练以根据图像输入预测转向角度，然后将其与
    [[94](#bib.bib94)] 中使用的简单CNN架构进行比较。实验结果表明，使用C-LSTM网络可以提高准确性和转向变化的平滑性。然而，该模型仅通过将预测的控制动作与真实值进行离线比较来进行评估，这不一定能准确评估驾驶质量
    [[95](#bib.bib95)]。应改为使用实时测试，让模型控制车辆以测试学到的驾驶行为。
- en: There has also been lateral control techniques for lane change manoeuvrers presented.
    Wang et al. [[96](#bib.bib96)] used reinforcement learning to train an agent to
    execute lane change manoeuvrers using a Deep Q-Network (DQN). The network uses
    host vehicle speed, longitudinal acceleration, position, yaw angle, target lane,
    lane width and road curvature to provide a continuous value for the desired yaw
    acceleration. To ensure Q-learning could be used to output continuous action values,
    a modified Q-learning approach was used to support continuous action values, where
    the Q-function was a quadratic function approximated by three single hidden layer
    feedforward neural networks. The proposed approach was tested in a simulated highway
    environment, with preliminary results showing effective lane change manoeuvrers
    learned by the agent.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 还提出了用于车道变换操控的横向控制技术。Wang 等人 [[96](#bib.bib96)] 使用强化学习来训练一个代理，通过深度 Q 网络（DQN）执行车道变换操控。该网络使用主车速、纵向加速度、位置、偏航角、目标车道、车道宽度和道路曲率来提供期望的偏航加速度的连续值。为了确保
    Q 学习可以输出连续的动作值，采用了一种改进的 Q 学习方法来支持连续动作值，其中 Q 函数是由三个单隐层前馈神经网络逼近的二次函数。所提出的方法在模拟的高速公路环境中进行了测试，初步结果表明代理学习到了有效的车道变换操控。
- en: A summary of the research works covered in this section can be seen in Table
    [I](#S3.T1 "TABLE I ‣ III-A Lateral Control Systems ‣ III Deep Learning Applications
    to Vehicle Control ‣ A Survey of Deep Learning Applications to Autonomous Vehicle
    Control"). Due to the advancements mentioned previously, the recent trend has
    been to move to deeper models with increased amounts of training data. Recent
    works have also investigated introducing temporal cues into the learning model,
    but this suffers from instability in training. Moreover, many of the models developed
    so far have been trained and evaluated in relatively simple environments. For
    instance, most researchers have decided to focus on lateral control for a single
    task. For example in models trained for lane keeping no decision-making for e.g.
    lane changes or turns to different roads have been incorporated in these systems.
    This opens possible avenues for future research where multiple actions could be
    carried out by the same DNN. It should also be noted that the majority of these
    works were trained and evaluated in simulated environments, which further simplifies
    the task and would require further tests to validate their real world performance.
    Nevertheless, there have been important developments in this field and these results
    show great promise for the use of deep learning for autonomous vehicle control.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本节所涵盖的研究工作的总结见表 [I](#S3.T1 "表 I ‣ III-A 横向控制系统 ‣ III 深度学习在车辆控制中的应用 ‣ 深度学习在自动驾驶控制中的应用概述")。由于前述的进展，近期的趋势是转向更深层次的模型，并增加训练数据量。最近的工作还研究了将时间线索引入学习模型，但这会导致训练的不稳定。此外，目前开发的许多模型在相对简单的环境中进行了训练和评估。例如，大多数研究人员决定专注于单一任务的横向控制。例如，在用于车道保持的模型中，并未包含车道变换或转弯到不同道路的决策。这为未来的研究开辟了可能的途径，在这些研究中，同一
    DNN 可以执行多个动作。还应该注意的是，这些工作中的大多数是在模拟环境中训练和评估的，这进一步简化了任务，并需要进一步测试以验证其在现实世界中的性能。然而，这一领域已有重要发展，这些结果显示了深度学习在自动驾驶控制中应用的巨大潜力。
- en: 'TABLE I: A Comparison of Lateral Control Techniques.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：横向控制技术的比较。
- en: '| Ref. | Learning Strategy | Network | Inputs | Outputs | Pros | Cons | Experiments
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 学习策略 | 网络 | 输入 | 输出 | 优点 | 缺点 | 实验 |'
- en: '| [[83](#bib.bib83)], [[84](#bib.bib84)] | Supervised Learning | Feedforward
    network with 1 hidden layer | Camera image | Discretised steering angles | First
    promising results for neural network-based vehicle controllers | Simple network
    and discretised steering angle outputs degrade performance | Real & Simulation
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| [[83](#bib.bib83)], [[84](#bib.bib84)] | 监督学习 | 具有 1 个隐层的前馈网络 | 摄像头图像 | 离散化的转向角度
    | 神经网络基础的车辆控制器的初步有希望的结果 | 网络简单且离散化的转向角度输出降低了性能 | 实际 & 仿真 |'
- en: '| [[85](#bib.bib85)] | Reinforcement Learning | Feedforward network with 1
    hidden layer | Camera image | Discretised steering angles | Supports online learning
    | Simple network and discretised steering angle outputs degrade performance |
    Simulation |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| [[85](#bib.bib85)] | 强化学习 | 具有 1 个隐层的前馈网络 | 摄像头图像 | 离散化的转向角度 | 支持在线学习 | 网络简单且离散化的转向角度输出降低了性能
    | 仿真 |'
- en: '| [[88](#bib.bib88)] | Supervised Learning | 6-layer CNN | Camera images |
    Steering angle | Robust to environmental diversity | Large errors, trained and
    tested on a sub-scale vehicle model | Real world (sub-scale vehicle) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| [[88](#bib.bib88)] | 监督学习 | 6层CNN | 相机图像 | 转向角 | 对环境多样性具有鲁棒性 | 大误差，训练和测试在一个缩尺车辆模型上
    | 现实世界（缩尺车辆） |'
- en: '| [[87](#bib.bib87)] | Supervised Learning | 9-layer CNN | Camera image | Steering
    angle values | High level of autonomy during field tests | Only considers lane
    following, requires interventions by the driver | Real world & Simulated |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| [[87](#bib.bib87)] | 监督学习 | 9层CNN | 相机图像 | 转向角值 | 在实地测试中具有高水平的自主性 | 仅考虑车道保持，需要司机干预
    | 现实世界与仿真'
- en: '| [[42](#bib.bib42)] | Supervised Learning | 8-layer CNN | Camera image | Steering
    angle values | Learns from minimal training data | Noisy behaviour of the steering
    signal | Simulation |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| [[42](#bib.bib42)] | 监督学习 | 8层CNN | 相机图像 | 转向角值 | 从最少的训练数据中学习 | 转向信号的噪声行为
    | 仿真 |'
- en: '| [[93](#bib.bib93)] | Supervised Learning | C-LSTM | Camera image | Steering
    angle values | Considers temporal dependencies | RNNs can be difficult to train,
    lack of live testing | No live testing, tested on data set image examples only
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] | 监督学习 | C-LSTM | 相机图像 | 转向角值 | 考虑时间依赖性 | RNNs可能难以训练，缺乏实时测试
    | 无实时测试，仅在数据集图像示例上测试 |'
- en: '| [[96](#bib.bib96)] | Reinforcement Learning | 3 feedforward networks | Host
    vehicle states and road geometry | Vehicle yaw acceleration | Executes lane changes
    successfully | Limited testing or results, lack of comparison to other lane change
    algorithms | Simulation |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | 强化学习 | 3个前馈网络 | 主车状态和道路几何 | 车辆偏航加速度 | 成功执行车道变换 | 测试或结果有限，缺乏与其他车道变换算法的比较
    | 仿真 |'
- en: III-B Longitudinal Control Systems
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 纵向控制系统
- en: Machine learning methods have also shown promise in applications to vehicle
    longitudinal control, such as ACC design. The ACC can be described as an optimal
    tracking control problem for a complex nonlinear system [[97](#bib.bib97), [98](#bib.bib98)]
    and therefore is poorly suited to control systems based on linear vehicle models
    or other algebraic analytical solutions [[99](#bib.bib99)]. Such traditional control
    systems provide poor adaptability in complex environments and do not conform to
    the driver’s habits [[100](#bib.bib100)]. The strong nonlinear nature of the system
    makes it difficult to build a vehicle model without significant uncertainty, limiting
    the effectiveness of model-based solutions. However, neural networks have shown
    great potential for optimising nonlinear, high-dimensional control systems [[40](#bib.bib40),
    [41](#bib.bib41), [101](#bib.bib101), [102](#bib.bib102), [103](#bib.bib103),
    [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)]. For instance, reinforcement
    learning can learn an optimal control policy through interaction with the environment,
    without knowledge of the system model [[50](#bib.bib50)]. Furthermore, the strong
    adaptive capacity and model-free capability of reinforcement learning makes it
    an attractive solution for ACC design. In early works, Dai et al. [[107](#bib.bib107)]
    proposed a fuzzy reinforcement learning method for longitudinal control of an
    autonomous vehicle. The method combines a Q estimator network (QEN) with a Takagi-Sugeno-type
    Fuzzy Inference System (FIS). The QEN is used to estimate the optimal action value
    function whilst the FIS gets the control output based on the estimated action
    value function. The described approach was evaluated in a simulation of a car-following
    scenario where the lead vehicle varies its velocity over time with a maximum episode
    duration of 80s. The controller was shown to be able to successfully drive the
    vehicle without failing after 68 trials. However, the reward function of the proposed
    approach by Dai et al. is only based on the spacing between the lead and the following
    vehicle. The reward function is the key to a successful reinforcement learning
    approach as it is the means by which the developer indicates the desirability
    of being in any given state. Therefore, the reward function needs to accurately
    capture the task to be performed and the manner in which it should be completed.
    For longitudinal control, the reward function should motivate the agent to adopt
    a safe and efficient driving strategy. For these reasons, a reward function with
    only one parameter such as inter-vehicle spacing may not be sufficient in real-time
    applications.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习方法在车辆纵向控制应用中也显示出了潜力，例如自适应巡航控制（ACC）设计。ACC 可以被描述
- en: There are several works in which the use of multi-objective reward functions
    have been explored. For example, Desjardins & Chaib-Draa [[23](#bib.bib23)] used
    a multi-objective reward function based on time headway (distance in time from
    the lead vehicle) and time headway derivative. The agent was encouraged through
    the reward function to keep a 2s time headway to the lead vehicle, and the time
    headway derivative provided information regarding whether the vehicle is moving
    closer to or farther from the lead vehicle, and allowed it to adjust its driving
    strategy accordingly. Taking the time headway derivative into consideration in
    the reward function encourages the agent to choose actions which help it progress
    toward the desired state (ideal time headway). The authors used this reward function
    in a policy-gradient method for a Cooperative Adaptive Cruise Control (CACC) system.
    The neural network architecture chosen had two inputs, a single hidden layer of
    20 neurons, and an output layer with 3 discrete actions (brake, accelerate, do
    nothing). In the learning process, an average of over 2.2 million iterations were
    obtained over ten learning simulations. The chosen method was shown to be efficient
    in CACC, providing average time headway errors of 0.039s in an emergency braking
    scenario. While the magnitude of the time headway errors remain small, it should
    be noted that the velocity profile of the subject vehicle showed oscillatory behaviour.
    This would make the system uncomfortable for the passengers as well as pose a
    potential safety risk. Potential solutions for this could include utilising continuous
    action values, the use of RNNs, or negative rewards for changes in acceleration
    to help smooth the velocity profile of the vehicle. Similarly, Sun [[99](#bib.bib99)]
    proposed a CACC system based on rewards from time headway and time headway derivative
    in a Q-learning algorithm. This approach was shown to reduce the learning time
    of the neural network. Over one hundred learning simulations, the best performing
    policy (the policy which obtained the highest reward) was chosen for evaluation.
    The algorithm was evaluated in a simulation of a stop-and-go environment in which
    the lead vehicle accelerated and decelerated periodically. The agent was shown
    to provide adequate performance in a platoon scenario. However, whilst such multi-objective
    reward functions are an improvement over single objective reward functions such
    as the one proposed by Dai et al. [[107](#bib.bib107)], this reward function does
    not consider passenger comfort which could lead to harsh accelerations or decelerations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有几项研究探讨了多目标奖励函数的应用。例如，Desjardins & Chaib-Draa [[23](#bib.bib23)] 使用了一种基于时间头距（与前车的时间距离）和时间头距导数的多目标奖励函数。该奖励函数鼓励代理保持与前车的2秒时间头距，而时间头距导数提供了车辆与前车距离的变化信息，并允许车辆据此调整驾驶策略。在奖励函数中考虑时间头距导数鼓励代理选择有助于其向理想状态（理想时间头距）前进的行动。作者在合作自适应巡航控制（CACC）系统的策略梯度方法中使用了这一奖励函数。所选择的神经网络架构有两个输入，一个包含20个神经元的隐藏层，以及一个具有3个离散动作（刹车、加速、保持不变）的输出层。在学习过程中，获得了超过220万次迭代的平均值，跨越了十次学习模拟。所选方法在CACC中显示出高效性，在紧急制动场景中提供了0.039秒的平均时间头距误差。尽管时间头距误差的幅度仍然很小，但值得注意的是，受试车辆的速度曲线表现出振荡行为。这将使系统对乘客不舒服，并可能带来潜在的安全风险。可能的解决方案包括利用连续动作值、使用RNNs，或对加速变化施加负奖励，以帮助平滑车辆的速度曲线。类似地，Sun
    [[99](#bib.bib99)] 提出了一个基于时间头距和时间头距导数奖励的CACC系统的Q学习算法。该方法显示出减少神经网络学习时间的效果。在超过一百次学习模拟中，选择了表现最佳的策略（获得最高奖励的策略）进行评估。该算法在前车周期性加速和减速的停走环境模拟中进行了评估。结果表明，代理在车队场景中表现良好。然而，尽管这种多目标奖励函数比Dai等人提出的单目标奖励函数
    [[107](#bib.bib107)] 有所改进，但这一奖励函数未考虑乘客舒适度，这可能导致剧烈的加速或减速。
- en: Huang et al. [[108](#bib.bib108)] presented a Parameterised Batch Actor-Critic
    (PBAC) reinforcement learning algorithm for longitudinal control of autonomous
    vehicles based on actor-critic algorithms. A multi-objective reward function was
    designed to reward the algorithm for tracking precision and drive smoothness.
    The method was validated by field experiments on various driving environments
    (e.g. flat, slippery, sloping, etc.) and the results suggested the method can
    track time-varying speeds more precisely than traditional Proportion-Integration
    (PI) or Kernel-based Least Square Policy Iteration (KLSPI) controllers trained
    with reinforcement learning [[109](#bib.bib109), [110](#bib.bib110)]. This was
    due to lower sensitivity to noise of speeds and accelerations. Moreover, smooth
    driving was achieved using the proposed method. The addition of driving smoothness
    in the reward function makes these systems more comfortable for passengers. However,
    the method was evaluated in an environment without adjacent vehicles or other
    obstacles. This allowed the authors to not consider safety parameters in the reward
    function, which leaves the algorithm susceptible to crashes in environments with
    other vehicles present. Therefore, additional terms for safety would be required
    in the reward function to ensure safe behaviour of the autonomous vehicle.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 黄等人[[108](#bib.bib108)]提出了一种参数化批量演员-评论员（PBAC）强化学习算法，用于基于演员-评论员算法的自主车辆纵向控制。设计了一个多目标奖励函数，以奖励算法在跟踪精度和驾驶平稳性方面的表现。该方法通过在各种驾驶环境（如平坦、滑溜、坡度等）中的现场实验进行了验证，结果表明该方法比传统的比例-积分（PI）或基于核的最小二乘策略迭代（KLSPI）控制器在跟踪时间变化速度方面更为精确[[109](#bib.bib109)、[110](#bib.bib110)]。这是由于对速度和加速度的噪声具有较低的敏感性。此外，使用所提方法实现了平稳驾驶。奖励函数中加入驾驶平稳性使这些系统对乘客更为舒适。然而，该方法是在没有邻近车辆或其他障碍物的环境中进行评估的。这使得作者没有考虑奖励函数中的安全参数，这使得算法在存在其他车辆的环境中容易发生碰撞。因此，奖励函数中需要额外的安全项以确保自主车辆的安全行为。
- en: 'One such reward function was proposed by Chae et al. [[111](#bib.bib111)],
    who proposed an autonomous braking system for collision avoidance based on a DQN
    approach. The reward function balances two conflicting objectives: avoiding collision
    and getting out of high risk situations. To speed up convergence, a replay memory
    was used to store a number of episodes of which some are chosen randomly to help
    train the network. Additionally, a ’trauma memory’ of rare critical events (e.g.
    collision) was used to improve stability and make the agent more reliable. The
    system was evaluated in situations where the vehicle had to avoid collision with
    a pedestrian, using various Time-to-Collision (TTC) values with 10,000 tests for
    each TTC value. It was shown that for TTC values above 1.5s, collisions were avoided
    every time, whereas at 0.9s (lowest TTC value used) the collision rate was as
    high as 61.29%. Additionally, the system was evaluated in a test procedure specified
    by the Euro NCAP test protocol (CVFA and CVNA tests [[112](#bib.bib112)]) and
    the system passed these tests without collision. Therefore, the system was considered
    to exhibit desirable and consistent brake control behaviour. In addition, Chen
    et al. [[100](#bib.bib100)] presented a personalised ACC which can learn from
    human demonstration. The proposed algorithm is based on Q-learning with a reward
    function based on distance to the front vehicle, vehicle speed, and acceleration.
    The authors used a Q-learning algorithm based on a feedforward artificial neural
    network to estimate the Q-function and calculated the desired velocity, which
    is then converted to low-level control commands by a Proportional Integral Derivative
    (PID) controller. The neural network used to estimate the Q-function consists
    of an input layer with 5 nodes, a hidden layer with 3 nodes, and an output layer
    with 1 node which predicts the desired velocity. The performance of the system
    was evaluated based on comfort and driving smoothness in simulation with different
    velocities and desired inter-vehicle clearances. The system was shown to provide
    better performance when compared to traditional ACC approaches. Similarly, Zhao
    et al. [[113](#bib.bib113)] proposed a personalised ACC approach which considers
    safety, comfort, as well as personalised driving styles. The reward function considers
    the driver habits, passenger comfort, and safety in an effort to find a good tradeoff
    between safety and comfort. The proposed approach uses a Model-free Optimal Control
    (MFOC) algorithm based on an actor-critic neural network structure. By optimising
    the algorithm to drive in a more human-like fashion, the human driver is more
    likely to trust the system and continue using it. For this purpose, the network
    would also be capable of learning from the human driver when the cruise control
    feature was switched off to better tune its parameters and to adopt a driving
    strategy based on the owner’s driving habits. The proposed algorithm was tested
    in a simulation under various environments and was shown to perform better than
    PID and Linear Quadratic Regulator (LQR) based controllers. For instance, in an
    emergency braking test scenario shown in Fig. [2](#S3.F2 "Figure 2 ‣ III-B Longitudinal
    Control Systems ‣ III Deep Learning Applications to Vehicle Control ‣ A Survey
    of Deep Learning Applications to Autonomous Vehicle Control"), the MFOC maintained
    a safer clearance compared to PID, while the LQR failed the test by causing a
    rear end collision. However, while conforming to individual driving habits can
    be useful to ensure the user feels safe and comfortable in the car, strategies
    for mitigating the negative effects of learning bad driving habits should also
    be considered to ensure the long term reliability and safety of the system.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一种这样的奖励函数由 Chae 等人提出[[111](#bib.bib111)]，他们提出了一种基于 DQN 方法的自动刹车系统用于避免碰撞。该奖励函数平衡了两个相互冲突的目标：避免碰撞和脱离高风险情况。为了加快收敛速度，使用了回放记忆来存储多个情景，其中一些情景被随机选择以帮助训练网络。此外，使用了“创伤记忆”来记录稀有的关键事件（例如碰撞），以提高系统的稳定性并使代理更可靠。该系统在需要避免与行人碰撞的情况下进行了评估，使用了各种碰撞时间（TTC）值，并对每个
    TTC 值进行了 10,000 次测试。结果显示，对于 TTC 值大于 1.5 秒的情况，每次都能避免碰撞，而在 0.9 秒（使用的最低 TTC 值）时，碰撞率高达
    61.29%。此外，该系统还在 Euro NCAP 测试协议（CVFA 和 CVNA 测试[[112](#bib.bib112)]）规定的测试程序中进行了评估，结果系统通过了这些测试且未发生碰撞。因此，该系统被认为展现了理想且一致的刹车控制行为。此外，Chen
    等人[[100](#bib.bib100)] 提出了一个可以从人类示范中学习的个性化 ACC。该算法基于 Q 学习，奖励函数基于前车距离、车辆速度和加速度。作者使用了基于前馈人工神经网络的
    Q 学习算法来估计 Q 函数，并计算所需速度，然后由比例积分微分（PID）控制器将其转换为低级控制指令。用于估计 Q 函数的神经网络包括一个具有 5 个节点的输入层、一个具有
    3 个节点的隐藏层和一个具有 1 个节点的输出层，该输出层预测所需速度。该系统的性能基于舒适性和驾驶平稳性进行了仿真评估，结果显示与传统的 ACC 方法相比，表现更佳。类似地，Zhao
    等人[[113](#bib.bib113)] 提出了一个个性化的 ACC 方法，考虑了安全性、舒适性以及个性化的驾驶风格。奖励函数考虑了驾驶习惯、乘客舒适度和安全性，以寻找安全性和舒适性之间的良好权衡。该方法使用基于演员-评论家神经网络结构的无模型优化控制（MFOC）算法。通过优化算法以更像人类的方式驾驶，驾驶员更有可能信任系统并继续使用它。为此，网络还能够在巡航控制功能关闭时从人类驾驶员那里学习，以更好地调整其参数，并根据车主的驾驶习惯采用驾驶策略。该算法在各种环境下的仿真测试中表现优于
    PID 和线性二次调节器（LQR）控制器。例如，在图 [2](#S3.F2 "Figure 2 ‣ III-B Longitudinal Control Systems
    ‣ III Deep Learning Applications to Vehicle Control ‣ A Survey of Deep Learning
    Applications to Autonomous Vehicle Control") 所示的紧急制动测试场景中，MFOC 维持了比 PID 更安全的间隙，而
    LQR 由于引发了追尾碰撞而未通过测试。然而，尽管适应个人驾驶习惯有助于确保用户在车内感到安全和舒适，但还应考虑减轻学习不良驾驶习惯的负面影响，以确保系统的长期可靠性和安全性。
- en: '![Refer to caption](img/d5ae8df748135d7de5344b5c5e6c4933.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d5ae8df748135d7de5344b5c5e6c4933.png)'
- en: 'Figure 2: MFOC Controller compared to PID and LQR controllers in an emergency
    braking scenario. (a) Clearance between the lead and follower vehicle. (b) Velocity
    profiles of the lead vehicle and the three controllers [[113](#bib.bib113)].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在紧急刹车场景中，MFOC 控制器与 PID 和 LQR 控制器的对比。（a）前车和跟车之间的间隙。（b）前车和三种控制器的速度曲线[[113](#bib.bib113)]。
- en: Reinforcement learning has been shown to be an effective approach for vehicle
    longitudinal control systems as shown by the discussion above. However, the main
    drawback for reinforcement learning is the time-intensive training [[50](#bib.bib50),
    [114](#bib.bib114)]. In contrast, supervised learning methods simplify the learning
    process with the use of prior knowledge of the supervisor, but lack the level
    of adaption that makes reinforcement learning attractive to complex decision-making
    systems such as autonomous driving. For these reasons, there are multiple examples
    in the literature that combine reinforcement and supervised learning to exploit
    the advantages of both approaches; reinforcement learning allows for self-adaptation
    in new and complex environments whilst the prior knowledge of supervised learning
    speeds up the learning process. For example, Zhao et al. [[22](#bib.bib22), [115](#bib.bib115),
    [116](#bib.bib116)] introduced a supervised reinforcement learning algorithm for
    an ACC system. By utilising actor-critic methods, the authors propose a novel
    supervised actor-critic (SAC) learning scheme, which is then implemented with
    feed-forward neural networks into a hierarchical acceleration controller. The
    proposed approach was evaluated in a simulation for an emergency braking scenario.
    The network was trained for emergency braking in dry conditions, whilst it was
    evaluated in both dry and wet road conditions and results were compared to the
    performance of a PID controller. The simulation results demonstrated that the
    SAC algorithm has superior performance compared to that of the PID controller
    as well as a supervised learning based controller (without reinforcement learning),
    and can adapt to changing road conditions. This shows the benefits of combining
    supervised learning with reinforcement learning to leverage the combined advantages
    of both methods. Pre-training the network via supervised learning helps reduce
    the training time of reinforcement learning and improves the convergence of the
    algorithm, both of which are common problems in reinforcement learning algorithms.
    Meanwhile, by exploring different actions through trial and error, reinforcement
    learning improves the performance beyond what supervised learning can provide.
    Also, the authors stated that using an actor-critic network architecture was beneficial
    as the evaluation of actions by the critic boosts the system’s performance in
    critical scenarios such as emergency braking.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如上述讨论所示，强化学习已被证明是车辆纵向控制系统的一种有效方法。然而，强化学习的主要缺点是训练过程时间较长[[50](#bib.bib50), [114](#bib.bib114)]。相比之下，监督学习方法利用监督者的先验知识简化了学习过程，但缺乏使强化学习在复杂决策系统（如自动驾驶）中具有吸引力的适应能力。因此，文献中有多个例子结合了强化学习和监督学习，以发挥两者的优势；强化学习允许在新和复杂的环境中自我适应，而监督学习的先验知识则加速了学习过程。例如，赵等人[[22](#bib.bib22),
    [115](#bib.bib115), [116](#bib.bib116)] 为 ACC 系统引入了一种监督强化学习算法。通过利用演员-评论家方法，作者提出了一种新颖的监督演员-评论家（SAC）学习方案，并将其与前馈神经网络实现到层次加速控制器中。该方法在模拟紧急刹车场景中进行了评估。网络在干燥条件下进行了紧急刹车训练，并在干燥和湿滑路况下进行了评估，结果与
    PID 控制器的性能进行了比较。模拟结果表明，SAC 算法相比于 PID 控制器以及基于监督学习的控制器（没有强化学习）具有更优的性能，并且能够适应变化的道路条件。这显示了将监督学习与强化学习相结合的好处，从而利用了两种方法的综合优势。通过监督学习预训练网络有助于减少强化学习的训练时间，并改善算法的收敛性，这些都是强化学习算法中的常见问题。同时，通过试错探索不同的动作，强化学习提升了性能，超出了监督学习所能提供的水平。此外，作者指出，使用演员-评论家网络架构是有益的，因为评论家对动作的评估提升了系统在关键场景（如紧急刹车）中的性能。
- en: A summary of the longitudinal control methods can be seen in Table [II](#S3.T2
    "TABLE II ‣ III-B Longitudinal Control Systems ‣ III Deep Learning Applications
    to Vehicle Control ‣ A Survey of Deep Learning Applications to Autonomous Vehicle
    Control"). In contrast to lateral control systems, vision-based inputs are not
    generally used for longitudinal control. Instead sensor inputs from ranging sensors
    (e.g. RADAR, LIDAR) and host vehicle states are more commonly used. These lower
    dimensional inputs (e.g. time headway or relative distance) can then easily be
    used to define a reward function for reinforcement learning. The second major
    difference between lateral and longitudinal control algorithms is the choice of
    learning strategies. While lateral control techniques favour supervised learning
    techniques trained on labelled datasets, longitudinal control techniques favour
    reinforcement learning methods which learn through interaction with the environment.
    However, as seen in this section, the reward function in reinforcement learning
    needs to be carefully designed. Safety, performance, and comfort all need to be
    considered. Poorly designed reward functions result in poor performance or the
    model not converging. Another challenge with reinforcement learning algorithms
    is the trade-off between exploration and exploitation. During training, the agent
    must take random actions to explore the environment. However, to perform well
    in its task the agent should exploit its knowledge to find the optimal action.
    Example solutions for this are the $\epsilon$-greedy exploration policies and
    the Upper Confidence Bound (UCB) algorithm. $\epsilon$-greedy strategies choose
    a random action with a probability $\epsilon$, which decreases overtime as the
    agent learns its environment. On the other hand, UCB encourages exploration in
    states with high uncertainty, whilst exploitation is encouraged in regions with
    high confidence. Therefore, intrinsic motivation is implemented in the system,
    encouraging the agent to learn about its environment, whilst exploitation can
    be taken advantage of in states which have already been explored adequately [[51](#bib.bib51),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119)]. Other approaches
    have sought to use supervised learning as a pre-training step to get the advantages
    of both reinforcement and supervised learning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 长itudinal控制方法的总结可以在表[II](#S3.T2 "TABLE II ‣ III-B Longitudinal Control Systems
    ‣ III Deep Learning Applications to Vehicle Control ‣ A Survey of Deep Learning
    Applications to Autonomous Vehicle Control")中查看。与横向控制系统相比，纵向控制通常不使用基于视觉的输入。相反，更常用的是来自距离传感器（例如RADAR、LIDAR）和主车状态的传感器输入。这些低维输入（例如时间间隔或相对距离）可以很容易地用来定义强化学习的奖励函数。纵向和横向控制算法的第二个主要区别是学习策略的选择。虽然横向控制技术倾向于使用在标注数据集上训练的监督学习技术，但纵向控制技术则偏向于通过与环境互动进行学习的强化学习方法。然而，正如本节所见，强化学习中的奖励函数需要精心设计。安全性、性能和舒适性都需要考虑。不良设计的奖励函数会导致性能不佳或模型无法收敛。强化学习算法的另一个挑战是探索与利用之间的权衡。在训练过程中，代理必须采取随机动作以探索环境。然而，为了在任务中表现良好，代理应该利用其知识来找到最优动作。解决这个问题的例子包括$\epsilon$-贪婪探索策略和上置信界（UCB）算法。$\epsilon$-贪婪策略以概率$\epsilon$选择随机动作，这个概率会随着代理对环境的学习而逐渐降低。另一方面，UCB在不确定性较高的状态中鼓励探索，而在置信度较高的区域中鼓励利用。因此，系统中实现了内在动机，鼓励代理了解其环境，同时在已经充分探索的状态中可以利用已有知识[[51](#bib.bib51),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119)]。其他方法则试图利用监督学习作为预训练步骤，以获得强化学习和监督学习的双重优势。
- en: 'TABLE II: A Comparison of Longitudinal Control Techniques.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：纵向控制技术的比较。
- en: '| Ref. | Learning Strategy | Network | Inputs | Outputs | Pros | Cons | Experiments
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Ref. | Learning Strategy | Network | Inputs | Outputs | Pros | Cons | Experiments
    |'
- en: '| [[107](#bib.bib107)] | Fuzzy Reinforcement Learning | Feedforward network
    with 1 hidden layer | Relative distance, relative speed, previous control input
    | Throttle angle, brake torque | model-free, continuous action values | Single
    term reward function | Simulation |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| [[107](#bib.bib107)] | 模糊强化学习 | 带有1层隐藏层的前馈网络 | 相对距离、相对速度、前一控制输入 | 油门角度、刹车扭矩
    | 无模型、连续动作值 | 单项奖励函数 | 仿真 |'
- en: '| [[23](#bib.bib23)] | Reinforcement Learning | Feedforward network with 1
    hidden layer | Time headway, headway derivative | Accelerate, brake, or no-op
    | Maintains a safe distance | Oscillatory acceleration behaviour, no term for
    comfort in reward function | Simulation |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| [[23](#bib.bib23)] | 强化学习 | 具有1个隐藏层的前馈网络 | 时间间隔、时间间隔导数 | 加速、刹车或无操作 | 维持安全距离
    | 振荡加速行为，奖励函数中没有舒适度项 | 仿真 |'
- en: '| [[108](#bib.bib108)] | Reinforcement Learning | Actor-Critic Network with
    feedforward networks | Velocity, velocity tracking error, acceleration error,
    expected acceleration | Gas and brake commands | Learns from minimal training
    data | Noisy behaviour of the acceleration signal | Real world |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| [[108](#bib.bib108)] | 强化学习 | 使用前馈网络的演员-评论员网络 | 速度、速度跟踪误差、加速度误差、期望加速度 | 油门和刹车命令
    | 从最少的训练数据中学习 | 加速度信号的噪声行为 | 真实世界 |'
- en: '| [[111](#bib.bib111)] | Reinforcement Learning | Feedforward network with
    5 hidden layers | Vehicle velocity, relative position of the pedestrian for past
    5 time steps | Discretised deceleration actions | Reliably avoids collisions |
    Only considers collision avoidance with pedestrians, high rate of collision at
    low TTC | Simulation |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| [[111](#bib.bib111)] | 强化学习 | 具有5个隐藏层的前馈网络 | 车辆速度、过去5个时间步的行人相对位置 | 离散化减速动作
    | 可靠地避免碰撞 | 仅考虑与行人的碰撞规避，低TTC下碰撞率高 | 仿真 |'
- en: '| [[100](#bib.bib100)] | Reinforcement Learning | Feedforward network with
    1 hidden layer | Relative distance, relative velocity, relative acceleration (normalised)
    | Desired acceleration | Provides smooth driving styles, learns personal driving
    styles | No methods for preventing learning of bad habits from human drivers |
    Simulation |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| [[100](#bib.bib100)] | 强化学习 | 具有1个隐藏层的前馈网络 | 相对距离、相对速度、相对加速度（归一化） | 期望加速度
    | 提供平稳的驾驶风格，学习个人驾驶风格 | 没有防止学习驾驶员不良习惯的方法 | 仿真 |'
- en: '| [[113](#bib.bib113)] | Reinforcement Learning | Actor Critic Network with
    feedforward networks | Relative distance, host velocity, relative velocity, host
    acceleration | Desired acceleration | Performs well in a variety of scenarios,
    safety and comfort considered, learns personal driving styles | Adapting unsafe
    driver habits could degrade safety | Simulation |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bib113)] | 强化学习 | 使用前馈网络的演员-评论员网络 | 相对距离、主车速度、相对速度、主车加速度 | 期望加速度
    | 在多种场景下表现良好，考虑安全性和舒适度，学习个人驾驶风格 | 适应不安全的驾驶习惯可能会降低安全性 | 仿真 |'
- en: '| [[22](#bib.bib22)] | Supervised Reinforcement Learning | Actor-Critic Network
    with feedforward networks | Relative distance, relative velocity | Desired acceleration
    | Pre-training by supervised learning accelerates learning process and helps guarantee
    convergence, performs well in critical scenarios | Requires supervision to converge,
    driving comfort not considered | Simulation |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [[22](#bib.bib22)] | 监督强化学习 | 使用前馈网络的演员-评论员网络 | 相对距离、相对速度 | 期望加速度 | 通过监督学习进行预训练，加快学习过程并帮助保证收敛，在关键场景下表现良好
    | 需要监督才能收敛，未考虑驾驶舒适度 | 仿真 |'
- en: III-C Simultaneous Lateral & Longitudinal Control Systems
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 同时横向与纵向控制系统
- en: The previous sections demonstrated that DNNs can be trained for either longitudinal
    or lateral control of a vehicle. However, for autonomous driving, the vehicle
    must be able to control both steering and acceleration simultaneously. In early
    works towards full vehicle control through deep learning, Xia et al. [[120](#bib.bib120)]
    introduced an autonomous driving system based on Q-learning combined with learning
    from the experience of a professional driver. The reward value of the professional
    driver’s strategy and the Q-value learned through the Q-learning method were combined
    in the pre-training phase to improve the speed of convergence during training.
    A filtered experience replay stores a limited number of episodes and allows elimination
    of poor experimental rounds from memory, improving convergence on a control strategy.
    The proposed Deep Q-learning with filtered experiences (DQFE) approach was compared
    to a naive neural fitted Q-iteration (NFQ) [[121](#bib.bib121)] algorithm without
    pre-training by an experienced driver. During training, it was shown that the
    DQFE approach reduced the training time by 71.2% for the 300 training episodes.
    Moreover, during 50 tests on a competition track, the proposed approach completed
    the track 49 times, compared to only 33 with NFQ. Additionally, DQFE performed
    better in terms of mean distance from centre of the track. Therefore, the addition
    of filtered experience replay improved the speed of convergence as well as performance
    of the algorithm. Comparing two neural networks for lane keeping systems, Sallab
    et al. [[122](#bib.bib122)] investigated the effects of discretised and continuous
    actions. Two approaches, DQN and a Deep Deterministic Actor Critic (DDAC) algorithm,
    were evaluated in a TORCS simulator [[123](#bib.bib123)]. In the two networks
    developed by the authors, the DQN could only output discretised values (steer,
    gear, brake, and acceleration), while the DDAC supports continuous action values.
    The DDAC consisted of two networks; an Actor Network which is a neural network
    responsible for taking actions based on perceived states and the Critic Network
    which criticises the value of the action taken. The experimental results showed
    that the DQN algorithm suffered in performance due to the fact that it cannot
    support continuous actions or state spaces. The DQN algorithm is suitable for
    continuous (input) states, however it still requires discrete actions since it
    finds the action that maximises the action-value function. This would require
    an iterative process at every time step for continuous action spaces [[124](#bib.bib124)].
    As shown in Fig. [3](#S3.F3 "Figure 3 ‣ III-C Simultaneous Lateral & Longitudinal
    Control Systems ‣ III Deep Learning Applications to Vehicle Control ‣ A Survey
    of Deep Learning Applications to Autonomous Vehicle Control"), the ability to
    support continuous action values allowed the DDAC algorithm to follow curved tracks
    more smoothly and stay closer to the centre of the lane when compared to the DQN
    algorithm, thereby producing better performance for lane keeping.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的部分展示了DNN可以用于车辆的纵向或横向控制。然而，对于自动驾驶，车辆必须能够同时控制方向盘和加速。在早期通过深度学习实现全面车辆控制的工作中，Xia等人[[120](#bib.bib120)]引入了一种基于Q学习和专业司机经验的自动驾驶系统。在预训练阶段，将专业司机策略的奖励值与通过Q学习方法学到的Q值结合，以提高训练过程中的收敛速度。过滤后的经验回放存储有限数量的实验回合，并允许从记忆中排除表现不佳的实验回合，从而改善控制策略的收敛性。所提出的带过滤经验的深度Q学习（DQFE）方法与没有经过经验司机预训练的原始神经网络拟合Q迭代（NFQ）[[121](#bib.bib121)]算法进行了比较。在训练过程中，DQFE方法将300次训练回合的训练时间减少了71.2%。此外，在50次竞赛赛道测试中，所提出的方法完成了49次，相比之下NFQ只有33次。此外，DQFE在平均离赛道中心的距离方面表现更好。因此，添加过滤经验回放提高了收敛速度以及算法性能。对车道保持系统的两个神经网络进行比较时，Sallab等人[[122](#bib.bib122)]研究了离散和连续动作的效果。两种方法，DQN和深度确定性演员评论家（DDAC）算法，在TORCS模拟器[[123](#bib.bib123)]中进行了评估。在作者开发的两个网络中，DQN只能输出离散值（转向、齿轮、刹车和加速），而DDAC支持连续动作值。DDAC由两个网络组成；一个演员网络，该网络负责根据感知状态采取行动，和一个评论家网络，该网络评估所采取行动的价值。实验结果显示，DQN算法因无法支持连续动作或状态空间而表现不佳。DQN算法适用于连续（输入）状态，但仍需要离散动作，因为它找到最大化动作值函数的动作。这需要在每个时间步骤中对连续动作空间进行迭代处理[[124](#bib.bib124)]。如图[3](#S3.F3
    "Figure 3 ‣ III-C Simultaneous Lateral & Longitudinal Control Systems ‣ III Deep
    Learning Applications to Vehicle Control ‣ A Survey of Deep Learning Applications
    to Autonomous Vehicle Control")所示，支持连续动作值的能力使DDAC算法能够更平滑地跟随弯曲轨道，并在与DQN算法相比时更接近车道中心，从而在车道保持方面表现更好。
- en: '![Refer to caption](img/28ec6a7c4cdf654dbffb37132117d369.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/28ec6a7c4cdf654dbffb37132117d369.png)'
- en: 'Figure 3: The lane keeping performance of (a) the DQN with discretised outputs
    and (b) DDAC with continuous output values [[122](#bib.bib122)].'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：图 (a) 是具有离散化输出的 DQN 的车道保持性能，图 (b) 是具有连续输出值的 DDAC 的车道保持性能 [[122](#bib.bib122)]。
- en: Vision based vehicle control using CNNs has also been researched. For instance,
    Zhang et al. [[125](#bib.bib125)] proposed a supervised learning method, SafeDAgger,
    for training a CNN to drive in a TORCS simulation. The proposed method is based
    on the Dataset Aggregation (DAgger) imitation learning algorithm [[54](#bib.bib54)].
    In DAgger, the agent first learns a primary policy through traditional supervised
    learning, with the training set generated by a reference policy. Then, the algorithm
    iteratively generates new training examples through the learned policies, which
    are then labelled by the reference policy. The new expanded dataset can then be
    used to update the learned policy through supervised learning. This has the advantage
    that states which were not reached in the initial training set can be covered
    in the new extended training set. The primary policy is then iteratively fine-tuned
    using the new training set. Zhang et al. proposed an extension to this method,
    called SafeDAgger, where the system estimates (in any given state) whether the
    primary policy is likely to deviate from the reference policy. If the primary
    policy is likely to deviate by more than a specified threshold, the reference
    policy is used to drive the vehicle instead. The safety policy is estimated by
    a fully connected network where the input is the last convolutional layer’s activation.
    The authors used this method to train a CNN to predict a continuous steering wheel
    angle and a binary decision for braking (brake or do not brake). The authors then
    evaluated supervised learning, DAgger, and SafeDAgger by driving them on three
    test tracks, with up to three laps on each track. Out of the three algorithms
    evaluated, SafeDAgger was found to perform best in terms of the number of completed
    laps, number of collisions, and mean squared error of steering angles. In another
    work, Pan et al. [[126](#bib.bib126)] used DAgger-like imitation learning to learn
    to drive at high speeds autonomously, with continuous actions for both steering
    and acceleration. The reference policy for the dataset was obtained from a model
    predictive controller operated using expensive high resolution sensors, which
    the CNN then learned to imitate using only low cost camera sensors for observations.
    The technique was first tested in Robot Operation System (ROS) Gazebo [[127](#bib.bib127)]
    simulations, followed by a real-world 30m long dirt track with a 1/5-scale vehicle.
    The sub-scale vehicle successfully learned to drive at speeds up to 7.5m/s around
    the track. Instead of using direct vision for control, Wang et al. [[128](#bib.bib128)]
    demonstrated that DAgger can be used to train an object-centric policy, which
    uses salient objects in the image (e.g. vehicles, pedestrians) to output a control
    action. The trained control policy was tested in Grand Theft Auto V simulation,
    with a discrete control action (left, straight, right, fast, slow stop) which
    was then translated to a continuous control with a PID controller. The test results
    demonstrated improved performance with the object-centric policy compared to models
    without attention or those based on heuristic object selection. Vision based techniques
    have also been used to mitigate collisions by Porav & Newman [[129](#bib.bib129)],
    who built on the previous work by Chae et al. [[111](#bib.bib111)] by using a
    deep reinforcement learning algorithm for collision mitigation which can provide
    continuous control actions for both velocity and steering. The system uses a Variational
    AutoEncoder (VAE) coupled with an RNN to predict the movement of obstacles and
    learns a control policy with Deep Deterministic Policy Gradient (DDPG) to mitigate
    collisions in low TTC scenarios. The network used a semantically segmented image
    to predict continuous steering and deceleration actions. The proposed technique
    shows improvement over braking-only policies for TTC values between 0.5 and 1.5s,
    and up to 60% reduction in collision rates.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视觉的车辆控制使用CNN也得到了研究。例如，Zhang等人[[125](#bib.bib125)]提出了一种监督学习方法SafeDAgger，用于训练CNN在TORCS模拟中驾驶。该方法基于数据集聚合（DAgger）模仿学习算法[[54](#bib.bib54)]。在DAgger中，代理首先通过传统的监督学习学习一个初步策略，训练集由参考策略生成。然后，算法通过已学到的策略迭代生成新的训练示例，并由参考策略标记。扩展后的数据集可以用来通过监督学习更新已学到的策略。这有一个优点，即可以在新的扩展训练集中覆盖在初始训练集中未达到的状态。然后，使用新的训练集对初步策略进行迭代微调。Zhang等人提出了该方法的扩展，称为SafeDAgger，其中系统估计（在任何给定状态下）初步策略是否可能偏离参考策略。如果初步策略可能偏离超过指定阈值，则使用参考策略来驾驶车辆。安全策略由一个完全连接的网络估计，其中输入是最后卷积层的激活。作者使用这种方法训练CNN预测连续的方向盘角度和用于制动的二元决策（刹车或不刹车）。作者然后通过在三个测试赛道上进行驾驶来评估监督学习、DAgger和SafeDAgger，每条赛道最多三圈。在评估的三种算法中，SafeDAgger在完成圈数、碰撞次数和方向盘角度的均方误差方面表现最好。在另一项工作中，Pan等人[[126](#bib.bib126)]使用类似于DAgger的模仿学习来学习在高速下自主驾驶，提供连续的转向和加速动作。数据集的参考策略来自使用昂贵高分辨率传感器操作的模型预测控制器，CNN则学习用低成本相机传感器模仿这一策略。该技术首先在Robot
    Operation System (ROS) Gazebo [[127](#bib.bib127)]模拟中测试，然后在一个长30米的实际泥土赛道上测试，使用1/5比例的车辆。子比例车辆成功学会了以最高7.5m/s的速度驾驶绕过赛道。Wang等人[[128](#bib.bib128)]则展示了DAgger可以用来训练基于物体的策略，该策略利用图像中的显著物体（例如车辆、行人）来输出控制动作。经过训练的控制策略在Grand
    Theft Auto V模拟中进行测试，使用离散控制动作（左、直、右、快、慢、停车），然后用PID控制器转换为连续控制。测试结果显示，与没有注意力或基于启发式物体选择的模型相比，基于物体的策略表现有所改善。基于视觉的技术还被Porav
    & Newman[[129](#bib.bib129)]用于减少碰撞，他们在Chae等人[[111](#bib.bib111)]的前期工作基础上，使用深度强化学习算法进行碰撞缓解，该算法可以提供连续的速度和转向控制动作。该系统使用变分自编码器（VAE）结合RNN预测障碍物的移动，并使用深度确定性策略梯度（DDPG）学习控制策略，以在低TTC情境中缓解碰撞。网络使用语义分割图像来预测连续的转向和减速动作。提出的技术在TTC值介于0.5到1.5秒之间的情况下，相比仅使用制动的策略，表现出改善，并且碰撞率减少了多达60%。
- en: Inverse Reinforcement Learning (IRL) approaches have also been investigated
    in the context of control systems as a way to overcome the difficulty of defining
    an optimal reward function. IRL is a subset of reinforcement learning, in which
    the reward function is not specified, but the agent attempts to learn it from
    an expert’s demonstrations. In IRL, the agent assumes that the expert is completing
    the task by following an unknown reward function. It then estimates a reward function
    in which the demonstrators’ trajectory is the most likely one. This has the advantage
    that instead of requiring the developer to explicitly specify a reward function,
    they simply have to demonstrate the intended behaviour. This can be advantageous
    since in large and complex tasks, defining an adequate reward function to provide
    optimal agent behaviour can be both difficult and time consuming [[130](#bib.bib130)].
    IRL approaches have been shown to not only reduce the amount of time required
    for design and optimisation, but also improve the system performance by creating
    more robust reward functions. Abbeel & Ng [[131](#bib.bib131)] showed that when
    IRL was applied to a problem where the agent learned by observing an expert, the
    agent performed as well as the expert when evaluated with respect to the reward
    function used by the expert, even if the reward function derived from observations
    was not the expert’s true reward function. Moreover, it was shown that in a simplistic
    highway driving scenario with 5 different actions for lane selection available
    to the agent and multiple driving styles demonstrated, the IRL algorithm successfully
    learned to mimic the demonstrated driving behaviours. Further, Silver et al. [[21](#bib.bib21)]
    used an IRL algorithm based on Maximum Margin Planning [[132](#bib.bib132)] which
    was shown to be effective in a demonstration of an autonomous vehicle in unstructured
    terrain. The vehicle was shown to perform better than an agent based on traditional
    reinforcement learning with a hand-tuned reward function. Additionally, the IRL
    approach was shown to require significantly less time to design and optimise compared
    to the reinforcement learning agent. Kuderer et al. [[20](#bib.bib20)] proposed
    a vehicle controller that can learn individual driving styles from demonstration
    using IRL. The algorithm assumes that the demonstrator is driving in a way to
    maximise an unknown reward function. From this, the learning model estimates the
    weights in a linear reward function based on 9 features for driving. Initially,
    the weights were equally set and were then updated based on demonstrations of
    8 minutes per driver. After finding the driving policy, the chosen trajectories
    were compared to those observed from human drivers. The system was shown to learn
    drivers’ personal driving styles from minimal training data and performed adequately
    in simulated testing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 逆向强化学习（IRL）方法也在控制系统的背景下进行了研究，以克服定义最佳奖励函数的困难。IRL 是强化学习的一个子集，其中奖励函数并未明确指定，而是代理尝试从专家的示范中学习。在
    IRL 中，代理假设专家是通过遵循一个未知的奖励函数来完成任务的。然后，它估计一个奖励函数，使得示范者的轨迹是最可能的。这种方法的优点是，开发者无需明确指定奖励函数，只需展示期望的行为即可。这在大规模和复杂任务中尤其有利，因为定义一个足够好的奖励函数以提供最佳代理行为既困难又耗时[[130](#bib.bib130)]。研究表明，IRL
    方法不仅减少了设计和优化所需的时间，还通过创建更为稳健的奖励函数提高了系统性能。Abbeel 和 Ng [[131](#bib.bib131)] 证明，当
    IRL 应用于代理通过观察专家学习的问题时，代理在使用专家的奖励函数进行评估时表现与专家一样，即使从观察中得出的奖励函数并不是专家的真实奖励函数。此外，在一个简化的高速公路驾驶场景中，代理有
    5 种不同的车道选择动作和多种驾驶风格的示范，IRL 算法成功地学习了模仿示范的驾驶行为。此外，Silver 等人 [[21](#bib.bib21)] 使用了一种基于最大间隔规划的
    IRL 算法 [[132](#bib.bib132)]，该算法在非结构化地形中的自主车辆演示中表现出有效性。与传统的强化学习代理和手动调整的奖励函数相比，该车辆的表现更佳。此外，IRL
    方法在设计和优化上所需的时间明显少于强化学习代理。Kuderer 等人 [[20](#bib.bib20)] 提出了一个可以通过 IRL 从示范中学习个体驾驶风格的车辆控制器。该算法假设示范者的驾驶方式是为了最大化一个未知的奖励函数。基于此，学习模型估计了基于
    9 个驾驶特征的线性奖励函数中的权重。最初，这些权重被设置为相等，然后根据每位驾驶员的 8 分钟示范进行更新。在找到驾驶策略后，将所选轨迹与从人类驾驶员那里观察到的轨迹进行了比较。系统显示可以从最小的训练数据中学习驾驶员的个人驾驶风格，并在模拟测试中表现出色。
- en: Building on the IRL approaches, Wulfmeier et al. [[133](#bib.bib133)] proposed
    an IRL approach for deep learning. The proposed algorithm is based on the Maximum
    Entropy [[134](#bib.bib134)] model for a trajectory planner, and uses CNNs to
    infer the reward functions from expert demonstration. The approach was trained
    on a dataset collected over the course of one year with a total of 120km of driving
    a modified golfcart on walkways and cycle lanes. The input to the network was
    the LIDAR point cloud map, which was represented on a discretised grid map. The
    output of the network was a discrete set of actions. The proposed approach was
    demonstrated to work better than a manually constructed cost function. Moreover,
    the learned algorithm was shown to be more robust to sensor noise. This shows
    that the use of DNNs in an IRL algorithm for trajectory planning was beneficial
    overall. Therefore IRL techniques could be considered as a potential way to overcome
    the difficulties of designing an optimal reward function for driving.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 IRL 方法的基础上，Wulfmeier 等人[[133](#bib.bib133)] 提出了一个深度学习的 IRL 方法。该算法基于最大熵[[134](#bib.bib134)]
    模型，用于轨迹规划器，并使用 CNN 从专家演示中推断奖励函数。该方法在一个数据集上进行训练，该数据集在一年的时间内收集，共包括 120 公里在步道和自行车道上驾驶改装高尔夫球车。网络的输入是
    LIDAR 点云地图，表示在离散化的网格地图上。网络的输出是一组离散的动作。所提出的方法被证明比手动构建的成本函数效果更好。此外，所学算法在对抗传感器噪声方面表现得更为稳健。这表明在轨迹规划的
    IRL 算法中使用 DNN 是整体上有益的。因此，IRL 技术可以被视为克服驾驶最优奖励函数设计困难的一种潜在方式。
- en: However, there are some challenges for IRL approaches in practical applications.
    Firstly, there is no guarantee of optimality of the demonstrations. For example,
    in a driving demonstration, no human driver can carry out the driving tasks optimally
    every time. Therefore, the training data will include suboptimal demonstrations
    which will affect the final reward function constructed. There are some solutions
    to minimising the effect of suboptimal demonstrations; using multiple trajectories
    and averaging over multiple sets to find a reward function or removing the assumption
    of global optimality [[135](#bib.bib135)]. Secondly, reward ambiguity can lead
    to further problems in IRL approaches. Given expert demonstrations of driving
    strategies, there can be multiple reward functions that explain the expert’s behaviour.
    Therefore, an effective IRL algorithm must find a reward function that considers
    the expert’s trajectory optimal and rejects other possible trajectories. Thirdly,
    the reward function derived through IRL methods may not be safe, as noted by Abbeel
    et al. [[136](#bib.bib136)], who used IRL to operate an autonomous helicopter
    and had to manually tune the reward function for safety. Therefore, hand tuning
    of the derived reward function may be required to ensure safe behaviour. Lastly,
    the computational burden of IRL methods can be heavy since they often require
    iteratively solving reinforcement learning problems with each new reward function
    derived [[130](#bib.bib130)]. Nevertheless, in tasks where an adequately accurate
    reward function cannot be easily defined, IRL approaches can provide an effective
    solution.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，IRL 方法在实际应用中面临一些挑战。首先，演示的最优性没有保证。例如，在驾驶演示中，没有任何人类驾驶员能够每次都以最优方式完成驾驶任务。因此，训练数据将包括次优的演示，这会影响最终构建的奖励函数。为了最小化次优演示的影响，有一些解决方案；使用多个轨迹并对多个集合进行平均，以找到奖励函数，或去除全局最优性的假设[[135](#bib.bib135)]。其次，奖励模糊性可能导致
    IRL 方法出现进一步的问题。给定专家的驾驶策略演示，可能会有多个奖励函数来解释专家的行为。因此，一个有效的 IRL 算法必须找到一个将专家轨迹视为最优并排除其他可能轨迹的奖励函数。第三，通过
    IRL 方法推导出的奖励函数可能不安全，正如 Abbeel 等人所指出的[[136](#bib.bib136)]，他们使用 IRL 操作了一架自主直升机，并且必须手动调整奖励函数以确保安全。因此，可能需要手动调整推导出的奖励函数以确保安全行为。最后，IRL
    方法的计算负担可能很重，因为它们通常需要迭代地解决每个新奖励函数推导出的强化学习问题[[130](#bib.bib130)]。尽管如此，在无法轻易定义足够准确的奖励函数的任务中，IRL
    方法可以提供有效的解决方案。
- en: While the previously mentioned works in this section demonstrate that a DNN
    can be trained to drive a vehicle, training a vehicle to simply follow a road
    or keep in its lane without any outside context is not sufficient for deploying
    fully autonomous vehicles. Humans drive vehicles with the goal of arriving at
    our target destination, and learning to drive from camera images to imitate human
    driving behaviour is not enough to understand the full context behind the human
    driver’s action. For instance, it has been reported [[83](#bib.bib83)], that upon
    reaching a fork in the road end-to-end driving techniques tend to oscillate between
    the two possible driving directions. Not only is this impractical if our goal
    is to continue in the left direction, but can result in unsafe behaviour where
    the DNN oscillates between left and right but never picking either direction.
    Aiming to provide autonomous vehicles with contextual awareness, Hecker et al.
    [[137](#bib.bib137)] collected a data set with a 360-degree view from 8 cameras
    and a driver following a route plan. This data set was then used to train a DNN
    to predict steering wheel angle and velocities from example images and route plans
    in the data set. Qualitative testing was done to evaluate learning on instances
    from the data set, suggesting the model was learning to imitate the human driver,
    but no live testing was completed to validate performance. With a similar aim,
    Codevilla et al. [[138](#bib.bib138)] trained a supervised learning algorithm,
    which uses both images and a high-level navigational command for its driving policy.
    The network was trained through end-to-end supervised learning, conditioned by
    a high-level command which could be follow road, go straight, turn left, or turn
    right. The authors tested two network architectures which could take the navigational
    command into account; one where the command was an additional input to the network,
    and one where the network branched at the end into multiple sub-modules (feedforward
    layers), one for each possible command. The authors noted that the latter architecture
    performed better. The resulting network was initially tested in CARLA [[139](#bib.bib139)]
    simulation, followed by real-world testing on a 1/5-scale car. The resulting policy
    successfully learned to turn the correct way at intersections as commanded. The
    authors noted that data augmentation and noise injection during training was key
    to learning a robust control policy. This method was further extended in [[140](#bib.bib140)],
    by using an extra module for velocity prediction, which helps the network in some
    situations, such as when the vehicle is stopped at a traffic light, to predict
    the expected vehicle velocity from visual cues and prevent it from getting stuck
    when the vehicle comes to a full stop. Further improvements to the model were
    a deeper network architecture and a larger training set, which reduced the variance
    in training. A slightly different approach was explored using reinforcement learning
    by Paxton et al.[[141](#bib.bib141)] where the high-level command is provided
    by another DNN responsible for decision making. The system consisted a DDPG network
    for low-level control and a DQN for a stochastic high level policy subject to
    linear temporal logic constraints. The aim of the vehicle was to navigate a busy
    intersection, where some lanes had stopped vehicles so that the host vehicle had
    to successfully change lanes as well. The system was tested in 100 simulated intersections
    with and without stopped cars ahead, for a total of 200 tests. Without stopped
    cars the agent succeeded every time, whereas with stopped cars ahead, 3 collisions
    occurred.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本节中前面提到的工作展示了深度神经网络（DNN）可以训练来驾驶车辆，但仅仅训练车辆跟随道路或保持车道而没有任何外部上下文，并不足以部署完全自主的车辆。人类驾驶车辆的目标是到达目的地，仅通过摄像头图像来模仿人类驾驶行为不足以理解人类驾驶者行为背后的完整上下文。例如，有报道称[[83](#bib.bib83)]，当遇到岔路口时，端到端的驾驶技术往往在两个可能的行驶方向之间波动。如果我们的目标是继续向左行驶，这不仅不切实际，还可能导致不安全的行为，因为DNN在左右之间来回波动却不选择任何一个方向。为了给自主车辆提供上下文意识，Hecker等人[[137](#bib.bib137)]收集了一个包含来自8台摄像头的360度视图的数据集，并且数据集中的驾驶员遵循了一条路线计划。然后，使用该数据集训练DNN，从示例图像和路线计划中预测方向盘角度和速度。进行了定性测试以评估模型在数据集中的实例学习情况，表明模型正在学习模仿人类驾驶员，但没有完成实际测试来验证性能。具有类似目标的Codevilla等人[[138](#bib.bib138)]训练了一种监督学习算法，该算法使用图像和高级导航命令来制定其驾驶策略。网络通过端到端的监督学习进行训练，由高级命令（如跟随道路、直行、左转或右转）进行调节。作者测试了两种可以考虑导航命令的网络架构；一种是将命令作为网络的额外输入，另一种是在网络的末端分支成多个子模块（前馈层），每个可能的命令对应一个子模块。作者指出，后者的架构表现更好。结果网络最初在CARLA[[139](#bib.bib139)]模拟中进行了测试，然后在1/5比例的实际汽车上进行了测试。结果策略成功地学会了按照命令在交叉口处正确转向。作者指出，数据增强和训练期间的噪声注入是学习稳健控制策略的关键。该方法在[[140](#bib.bib140)]中得到了进一步扩展，通过使用一个额外的速度预测模块，帮助网络在某些情况下（如车辆在红灯前停下时）从视觉线索中预测预期车辆速度，并防止车辆在完全停下时卡住。对模型的进一步改进包括更深的网络架构和更大的训练集，这减少了训练中的方差。Paxton等人[[141](#bib.bib141)]探索了稍微不同的方法，使用强化学习，其中高级命令由另一个负责决策的DNN提供。该系统包括一个用于低级控制的DDPG网络和一个用于受线性时序逻辑约束的随机高级策略的DQN。车辆的目标是通过一个繁忙的交叉口，其中一些车道有停下的车辆，因此主车也必须成功换车道。该系统在100个有停下的汽车和没有停下汽车的模拟交叉口中进行了测试，总共进行了200次测试。没有停下的汽车时，代理每次都成功，而有停下的汽车时发生了3次碰撞。
- en: Moving away from end-to-end approaches, researchers at Waymo recently presented
    ChauffeurNet [[142](#bib.bib142)]. ChauffeurNet uses mid-to-mid learning to learn
    a driving policy, where the input is a pre-processed top-down view of the surrounding
    environment which represents useful features such as roadmap, traffic lights,
    a route plan to follow, dynamic objects, and past agent poses. The agent then
    processes these inputs through an RNN to provide a heading, speed, and waypoint,
    which are then achieved through a low-level controller. This had the advantage
    that pre-processed inputs could be obtained either from simulation or real-world
    data, which makes transferring driving policies from simulation to the real world
    easier [[143](#bib.bib143), [144](#bib.bib144)]. Furthermore, synthesising perturbations
    to model recoveries from incorrect lane positions or even scenarios such as collisions
    or driving off-road provides the model with robustness to errors and allows the
    model to learn to avoid such scenarios.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 摆脱端到端方法，Waymo的研究人员最近提出了ChauffeurNet [[142](#bib.bib142)]。ChauffeurNet使用中到中学习来学习驾驶策略，其中输入是经过预处理的自上而下的环境视图，代表了诸如路线图、交通信号灯、要跟随的路线计划、动态物体和过去的代理姿态等有用特征。然后，代理通过RNN处理这些输入，提供航向、速度和路径点，这些通过低级控制器实现。这种方法的优点在于，预处理的输入可以通过模拟或现实世界数据获得，这使得将驾驶策略从模拟环境转移到现实世界变得更加容易
    [[143](#bib.bib143), [144](#bib.bib144)]。此外，合成扰动来模拟从不正确车道位置的恢复，甚至是碰撞或偏离道路的场景，赋予了模型对错误的鲁棒性，并使模型能够学习避免这些场景。
- en: An overview of full vehicle control approaches can be seen in Table [III](#S3.T3
    "TABLE III ‣ III-C Simultaneous Lateral & Longitudinal Control Systems ‣ III Deep
    Learning Applications to Vehicle Control ‣ A Survey of Deep Learning Applications
    to Autonomous Vehicle Control"). Unlike previous sections, a variety of learning
    strategies have been utilised here, however supervised learning is still the preferred
    approach. An important note on the works where full vehicle control via neural
    networks is researched, is that robust and high performing models still seem out
    of reach. For instance, techniques which implement full vehicle control tend to
    have poorer performance on steering than techniques which only consider steering.
    This is explained by the significant increase in the complexity of the task which
    the neural network is trained to perform. For this reason, several of the works
    summarised in this section have been trained and evaluated in simplified simulated
    environments. While full vehicle control should be the end goal of autonomous
    vehicle control techniques, current approaches have yet to achieve adequate performance
    in complex and dynamic environments. Therefore future research is required to
    further improve the control performance of neural network-driven autonomous vehicles.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对整车控制方法的概述见表格[III](#S3.T3 "TABLE III ‣ III-C Simultaneous Lateral & Longitudinal
    Control Systems ‣ III Deep Learning Applications to Vehicle Control ‣ A Survey
    of Deep Learning Applications to Autonomous Vehicle Control")。与之前的部分不同，这里使用了多种学习策略，但监督学习仍然是首选方法。需要注意的是，在研究通过神经网络实现整车控制的工作中，稳健且高性能的模型仍然难以实现。例如，实现整车控制的技术在转向性能上通常比仅考虑转向的技术表现较差。这是由于任务复杂性的显著增加导致的，神经网络需要训练来应对这种复杂性。因此，本节总结的若干工作已在简化的模拟环境中进行训练和评估。虽然整车控制应是自动驾驶技术的终极目标，但当前的方法在复杂和动态环境中的表现仍不够理想。因此，未来的研究需要进一步提高神经网络驱动的自动驾驶车辆的控制性能。
- en: 'TABLE III: A Comparison of Full Vehicle Control Techniques.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 III：整车控制技术的比较。
- en: '| Ref. | Learning Strategy | Network | Inputs | Outputs | Pros | Cons | Experiments
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 学习策略 | 网络 | 输入 | 输出 | 优点 | 缺点 | 实验 |'
- en: '| [[120](#bib.bib120)] | Supervised Reinforcement Learning | Feedforward network
    with 2 hidden layers | Not mentioned | Steering, acceleration, braking | Fast
    training | Unstable (Can steer off the road) | Simulation |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| [[120](#bib.bib120)] | 监督强化学习 | 带有2层隐藏层的前馈网络 | 未提及 | 转向、加速、制动 | 训练快速 | 不稳定（可能偏离道路）
    | 模拟 |'
- en: '| [[122](#bib.bib122)] | Reinforcement Learning | Fully connected / Actor-Critic
    Network with feedforward networks | Position in lane, velocity | Steering, gear,
    brake, and acceleration values (discretised for DQN) | Continuous policy provides
    smooth steering | Simple simulation environment | Simulation |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| [[122](#bib.bib122)] | 强化学习 | 全连接 / Actor-Critic 网络与前馈网络 | 车道中的位置、速度 | 转向、档位、刹车和加速值（DQN
    的离散化） | 连续策略提供平滑转向 | 简单的仿真环境 | 仿真 |'
- en: '| [[125](#bib.bib125)] | Supervised Learning | CNN / Feedforward | Simulated
    camera image | Steering angle, binary braking decision | Estimates safety of the
    policy in any given state, DAgger provides robustness to compounding errors |
    Simple simulation environment, simplified longitudinal output | Simulation |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| [[125](#bib.bib125)] | 监督学习 | CNN / 前馈神经网络 | 模拟摄像头图像 | 转向角度、二进制刹车决策 | 评估在任何给定状态下策略的安全性，DAgger
    提供对复合错误的鲁棒性 | 简单的仿真环境，简化的纵向输出 | 仿真 |'
- en: '| [[126](#bib.bib126)] | Supervised Learning | CNN | Camera image | Steering
    and throttle | High speed driving, Learns to drive on low cost cameras, Robustness
    of DAgger to compounding errors | Trained only for elliptical race tracks with
    no other vehicles, Requires iteratively building the dataset with the reference
    policy | Real world (sub-scale vehicle) & Simulation |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [[126](#bib.bib126)] | 监督学习 | CNN | 摄像头图像 | 转向和油门 | 高速驾驶，学习在低成本摄像头上驾驶，DAgger
    对复合错误的鲁棒性 | 仅在椭圆形赛道上训练，没有其他车辆，需要与参考策略一起迭代构建数据集 | 现实世界（缩小比例车辆） & 仿真 |'
- en: '| [[128](#bib.bib128)] | Supervised Learning | CNN | Image | 9 discrete actions
    for motion | Object-centric policy provides attention to important objects | Highly
    simplified action space | Simulation |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| [[128](#bib.bib128)] | 监督学习 | CNN | 图像 | 9 个离散动作 | 面向对象的策略关注重要对象 | 高度简化的动作空间
    | 仿真 |'
- en: '| [[129](#bib.bib129)] | Reinforcement Learning | VAE-RNN | Semantically segmented
    image | Steering, acceleration | Improves collision rates over braking only policies
    | Only considers imminent collision scenarios | Simulation |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| [[129](#bib.bib129)] | 强化学习 | VAE-RNN | 语义分割图像 | 转向、加速 | 相比仅使用刹车策略，碰撞率有所改善
    | 仅考虑即将发生的碰撞场景 | 仿真 |'
- en: '| [[133](#bib.bib133)] | Inverse Reinforcement Learning | CNN | LIDAR point
    clouds on a grid map | Discrete motions | Robust to noise, avoids handcrafting
    of cost function | Increased computation burden of IRL, no guarantee of cost function
    optimality | No live testing |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| [[133](#bib.bib133)] | 逆向强化学习 | CNN | 网格地图上的 LIDAR 点云 | 离散动作 | 对噪声鲁棒，避免手工制作成本函数
    | IRL 的计算负担增加，没有成本函数最优性的保证 | 无实时测试 |'
- en: '| [[137](#bib.bib137)] | Supervised Learning | CNN | 360-degree view camera
    image, route plan | Steering angle, velocity | Takes route plan into account |
    Lack of live testing | No live testing, tested on data set image examples only
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| [[137](#bib.bib137)] | 监督学习 | CNN | 360度视角摄像头图像、路线规划 | 转向角度、速度 | 考虑了路线规划
    | 缺乏实时测试 | 无实时测试，仅在数据集图像示例上测试 |'
- en: '| [[138](#bib.bib138)] | Supervised Learning | CNN | Camera image, navigational
    command | Steering angle, acceleration | Takes navigational commands into account,
    generalises to new environments | Occasionally fails to take correct turn on first
    attempt | Real (sub-scale vehicle) & Simulation |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138)] | 监督学习 | CNN | 摄像头图像、导航指令 | 转向角度、加速 | 考虑导航指令，对新环境具有泛化能力
    | 偶尔在第一次尝试时未能正确转弯 | 现实（缩小比例车辆） & 仿真 |'
- en: '| [[141](#bib.bib141)] | Reinforcement Learning | Feedforward network with
    1 hidden layer | Host vehicle states, set of features for each nearby vehicle,
    vehicle position and priority in intersection | steering angle rate, acceleration
    | Considers decision making provided by another DNN | Large number inputs which
    would be difficult to extract in reality, Not collision free | Simulation |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| [[141](#bib.bib141)] | 强化学习 | 具有 1 层隐藏层的前馈网络 | 主车状态、每辆附近车辆的特征集、交叉口的车辆位置和优先级
    | 转向角速率、加速 | 考虑由另一个 DNN 提供的决策 | 输入数量庞大，现实中难以提取，不保证碰撞安全 | 仿真 |'
- en: '| [[142](#bib.bib142)] | Supervised Learning | CNN-RNN | Pre-processed top-down
    image of surroundings | Heading, velocity, waypoint | Ease of transfer from simulation
    to real world, robust to deviations from trajectory | Can output waypoints which
    make turns infeasible, can be over aggressive with other vehicles in new scenarios
    | Real world & Simulation |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| [[142](#bib.bib142)] | 监督学习 | CNN-RNN | 预处理的环境顶视图图像 | 方向、速度、路点 | 从仿真到现实世界的迁移简单，对轨迹偏差鲁棒
    | 可能输出使转弯不可行的路点，在新场景中可能对其他车辆过于激进 | 现实世界 & 仿真 |'
- en: IV Challenges
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 挑战
- en: The previous section discussed various examples of deep learning applied to
    vehicle controller design. While this shows that there is a significant amount
    of interest in the research of such systems, they are still far from ready for
    commercial application. There remains a number of challenges that must be overcome
    before learned autonomous vehicle technology is ready for widespread commercial
    use. This section is dedicated to discussing the technological challenges for
    deep learning based control of autonomous vehicles. It is worth remembering that
    besides these technological challenges, issues such as user acceptance, cost efficiency,
    machine ethics for artificial intelligence technologies, and lack of legislation/regulation
    for autonomous vehicles must also be addressed. However, the aim of this manuscript
    is to focus on deep learning based autonomous vehicle control methods and their
    technical challenges, therefore general and non-technological challenges for autonomous
    vehicles are out of the scope of this manuscript, for further reading on these
    topics, see [[145](#bib.bib145), [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148),
    [149](#bib.bib149), [150](#bib.bib150)].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节讨论了将深度学习应用于车辆控制器设计的各种例子。虽然这表明对这种系统的研究有相当大的兴趣，但它们仍远未准备好进行商业应用。在学习的自主驾驶技术准备好广泛商业使用之前，还存在许多挑战必须克服。本节致力于讨论基于深度学习的自主车辆控制的技术挑战。值得记住的是，除了这些技术挑战之外，还必须解决用户接受度、成本效益、人工智能技术的机器伦理以及自主车辆的立法/监管缺失等问题。然而，本手稿的目标是集中讨论基于深度学习的自主车辆控制方法及其技术挑战，因此，对自主车辆的一般性和非技术性挑战不在本手稿的范围之内，关于这些主题的进一步阅读，请参见[[145](#bib.bib145)、[146](#bib.bib146)、[147](#bib.bib147)、[148](#bib.bib148)、[149](#bib.bib149)、[150](#bib.bib150)]。
- en: IV-A Computation
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 计算
- en: The major drawback for deep learning methods is the large amount of data and
    time required for adequate training, especially for reinforcement learning methods.
    This can lead to long training periods which can cause delays and additional cost
    in the design of an autonomous vehicle. The common solution to reduce training
    data requirements or the time required for training is to combine reinforcement
    learning with supervised learning, which helps reduce the training time whilst
    still providing good adaptability. Nevertheless, for a fully autonomous vehicle,
    the amount of training data required to build a reliable and robust system can
    be vast. It is challenging to train a vehicle to drive in all possible scenarios
    that it could encounter in the real world due to the huge quantity of data that
    needs to be collected. There are several companies researching autonomous driving
    using machine learning and collaborating and sharing data would be the fastest
    route to move from experimental systems to commercial ones. However, this is unlikely
    as companies researching autonomous vehicles are not willing to share their resources
    due to fear of diluting their competitive advantage [[151](#bib.bib151)]. However,
    while increasing the amount of available data is useful to learn more complex
    behaviours, using larger data sets brings its own challenges, such as ensuring
    diversity of the data. If the amount of data used for training the model is increased,
    without ensuring variety in the data set, the risk of overfitting to the data
    set increases. For instance, Codevilla et al. [[140](#bib.bib140)] compared 4
    driving models trained with 2, 10, 50, and 100 hours of data, and it was shown
    that the model trained with 10 hours of driving data performed best in most scenarios.
    This is due to many of the instances in the training set being very similar, captured
    in typical driving conditions. As the data set size increases, rare driving scenarios
    (where the model is more likely to fail) are encountered increasingly rarely during
    training. Therefore, when generating large data sets, diversity in the data set
    must be ensured.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法的主要缺陷在于需要大量的数据和时间进行充分训练，特别是强化学习方法。这可能导致较长的训练周期，从而在设计自动驾驶车辆时造成延迟和额外成本。减少训练数据需求或训练时间的常见解决方案是将强化学习与监督学习结合，这有助于缩短训练时间，同时仍提供良好的适应性。然而，对于完全自动化的车辆，建立一个可靠且健壮的系统所需的训练数据量可能非常庞大。由于需要收集大量的数据，训练车辆在可能遇到的所有真实世界场景中行驶是一个挑战。有几个公司正在研究使用机器学习的自动驾驶，合作和数据共享将是从实验系统到商业系统的最快途径。然而，由于对稀释竞争优势的担忧，这种情况不太可能发生[[151](#bib.bib151)]。然而，虽然增加可用数据量有助于学习更复杂的行为，但使用更大的数据集也带来了自身的挑战，例如确保数据的多样性。如果用于训练模型的数据量增加，但没有确保数据集的多样性，模型过拟合数据集的风险也会增加。例如，Codevilla
    等[[140](#bib.bib140)]比较了使用2小时、10小时、50小时和100小时数据训练的4种驾驶模型，结果表明，在大多数场景中，使用10小时驾驶数据训练的模型表现最佳。这是因为训练集中许多实例非常相似，都是在典型驾驶条件下捕获的。随着数据集大小的增加，训练中遇到的稀有驾驶场景（模型更可能失败的情况）会越来越少。因此，在生成大数据集时，必须确保数据集的多样性。
- en: Further computational complexity is caused by the continuous states and actions
    in which the agent has to operate. As stated in the previous section, continuous
    action values are necessary for a deployable vehicle control system to have adequate
    performance. However, as the number of dimensions grows, the computational complexity
    grows exponentially [[152](#bib.bib152)], this is known as the Curse of Dimensionality
    [[153](#bib.bib153)]. In the high-dimensional problems of vehicle control, this
    has a significant effect on the computational complexity of any solution. Although
    discretisation of the system can reduce the complexity, as seen in previous examples,
    this can lead to degradation in system performance. Other solutions include using
    multiple learners to reduce learning time [[154](#bib.bib154), [155](#bib.bib155)],
    evolution strategies which are highly parallelisable [[156](#bib.bib156)], or
    removing unnecessary data from the training and system input data [[157](#bib.bib157)].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的计算复杂性源于代理需要操作的连续状态和动作。如前面章节所述，连续的动作值对于具有适当性能的可部署车辆控制系统是必要的。然而，随着维度数量的增加，计算复杂性呈指数增长[[152](#bib.bib152)]，这被称为维度诅咒[[153](#bib.bib153)]。在车辆控制的高维问题中，这对任何解决方案的计算复杂性产生了重大影响。尽管系统的离散化可以降低复杂性，但正如之前的例子所示，这可能会导致系统性能下降。其他解决方案包括使用多个学习者以减少学习时间[[154](#bib.bib154),
    [155](#bib.bib155)]，高度并行化的进化策略[[156](#bib.bib156)]，或从训练和系统输入数据中移除不必要的数据[[157](#bib.bib157)]。
- en: Overall, the high computational burden of DNNs is a challenge to not only the
    development and training of the networks but also the deployment of such systems
    in vehicles. The high computational overhead of the deep learning algorithms will
    require high computing capabilities on-board, driving up the system cost and power
    requirements, which must be kept in mind during the system design.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，深度神经网络（DNNs）的高计算负担不仅对网络的开发和训练构成挑战，还对这些系统在车辆中的部署提出了要求。深度学习算法的高计算开销将需要高计算能力的车载系统，从而推高系统成本和功耗，这在系统设计时必须考虑。
- en: IV-B Architectures
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 架构
- en: Another challenge with deep learning is selecting the architecture of the neural
    networks. There are no clear guidelines for ’good’ neural network architecture
    for a given task. For instance, in terms of size and number of layers, it has
    been shown that too few neurons will lead to a system with poor performance. However,
    too many neurons may overfit to the training data and therefore not generalise
    well. Also, given that additional neurons will lead to increased computational
    complexity, finding an optimal number of neurons would be of great benefit to
    deep learning methods [[158](#bib.bib158), [159](#bib.bib159)]. Other parameters
    can also have an effect on the performance, training, and convergence of the system.
    The fundamental architecture, training method, learning rate, loss function, batch
    size etc., all need to be decided upon and defined, which affect the performance
    of the agent. However, there are few methods for choosing these parameters, and
    often trial-and-error and heuristics are the only viable options for optimising
    each parameter due to the complexity of DNNs [[49](#bib.bib49)]. This is generally
    achieved by choosing a range of values for the hyperparameters in the neural network,
    and finding the best performing values. However, using such trial-and-error methods
    for exploring the hyperparameter space can be slow, given the amount of computation
    required for each training run.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的另一个挑战是选择神经网络的架构。对于特定任务，并没有明确的“良好”神经网络架构的指南。例如，在大小和层数方面，已经证明过少的神经元会导致系统性能不佳。然而，过多的神经元可能会对训练数据过拟合，从而无法很好地进行泛化。此外，考虑到额外的神经元会导致计算复杂性的增加，找到最佳的神经元数量将对深度学习方法大有裨益[[158](#bib.bib158),
    [159](#bib.bib159)]。其他参数也可能对系统的性能、训练和收敛产生影响。基本架构、训练方法、学习率、损失函数、批量大小等，都需要决定和定义，这些都会影响代理的性能。然而，选择这些参数的方法很少，通常由于深度神经网络的复杂性，试错法和启发式方法是优化每个参数的唯一可行选项[[49](#bib.bib49)]。这通常通过为神经网络的超参数选择一系列值，并找到最佳性能的值来实现。然而，鉴于每次训练运行所需的计算量，使用这种试错方法来探索超参数空间可能会很慢。
- en: Solutions to this challenge currently being researched include computerised
    ways of finding optimal values for these parameters, either by trialling across
    a range or using model-based methods to converge on the best values. There are
    several methods for changing the parameters over the chosen range, such as Coordinate
    Descent [[160](#bib.bib160)], Grid Search [[160](#bib.bib160), [161](#bib.bib161)],
    and Random Search [[162](#bib.bib162)]. Coordinate Descent keeps all hyperparameters
    except one fixed, and finds the best value for one parameter at a time. Grid Search
    optimises every parameter simultaneously, including the cross-product of all intervals.
    However, this vastly increases the computational expense by requiring a large
    number of neural network models to be trained and therefore is only suitable when
    the models can be trained quickly. Random Search often finds a good set of parameters
    faster than a Grid Search by sampling the chosen interval randomly [[162](#bib.bib162),
    [163](#bib.bib163)]. However, this has the disadvantage that the parameter space
    is often not covered completely, and some sample points can be very close to each
    other. These disadvantages can be solved by using quasi-random sequences [[164](#bib.bib164)].
    Alternatively, one can use model-based hyperparameter optimisation methods, such
    as Bayesian optimisation or tree-structured Parzen estimators, which tend to yield
    better results but are more time intensive [[164](#bib.bib164), [165](#bib.bib165),
    [166](#bib.bib166), [167](#bib.bib167)]. Other proposed approaches focus on automated
    hyperparameter tuning by eliminating undesirable regions of the hyperparameter
    search space in order to converge to optimal values [[168](#bib.bib168), [169](#bib.bib169)].
    Recent research has also explored neural architecture search methods which take
    hardware efficiency into account by incorporating the hardware feedback into the
    learning signal [[170](#bib.bib170), [171](#bib.bib171), [172](#bib.bib172), [173](#bib.bib173)].
    This has resulted in neural network architectures which are specialised for specific
    hardware platforms, and demonstrate a hardware efficiency benefit over non-specialised
    architectures. Such methods could also be extended to find efficient network architectures
    for vehicle on-board hardware platforms. It should be noted that automated neural
    architecture search is an active area of research, for further discussion on this
    topic we refer the reader to the survey by Elsken et al. [[174](#bib.bib174)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 目前正在研究的解决方案包括计算机化的方法来找到这些参数的最佳值，这些方法可以通过在一个范围内进行试验或使用基于模型的方法来收敛到最佳值。改变参数的方法有几种，例如坐标下降法
    [[160](#bib.bib160)]、网格搜索 [[160](#bib.bib160), [161](#bib.bib161)] 和随机搜索 [[162](#bib.bib162)]。坐标下降法保持所有超参数不变，仅对一个参数进行优化。网格搜索同时优化每个参数，包括所有区间的交叉产品。然而，这大大增加了计算开销，因为需要训练大量的神经网络模型，因此仅适用于模型能够快速训练的情况。随机搜索通过随机抽样选择区间，通常能比网格搜索更快地找到一组好的参数
    [[162](#bib.bib162), [163](#bib.bib163)]。然而，这有一个缺点，即参数空间通常未被完全覆盖，某些样本点可能非常接近。使用准随机序列
    [[164](#bib.bib164)] 可以解决这些缺点。或者，可以使用基于模型的超参数优化方法，如贝叶斯优化或树结构的帕尔岑估计器，这些方法通常能获得更好的结果，但更耗时
    [[164](#bib.bib164), [165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167)]。其他提出的方法集中于通过排除超参数搜索空间中的不希望区域来实现自动化超参数调整，以便收敛到最佳值
    [[168](#bib.bib168), [169](#bib.bib169)]。最近的研究还探索了神经架构搜索方法，这些方法通过将硬件反馈纳入学习信号来考虑硬件效率
    [[170](#bib.bib170), [171](#bib.bib171), [172](#bib.bib172), [173](#bib.bib173)]。这导致了专门针对特定硬件平台的神经网络架构，并展示了相对于非专用架构的硬件效率优势。这些方法也可以扩展到寻找适用于车载硬件平台的高效网络架构。需要注意的是，自动化神经架构搜索是一个活跃的研究领域，关于这一主题的进一步讨论请参见Elsken等人的综述
    [[174](#bib.bib174)]。
- en: While architecture selection is a general problem for many deep learning applications,
    a complex task such as autonomous driving also brings its own challenges. Currently,
    most end-to-end driving systems have been limited to smaller networks. This is
    due to the relatively small datasets used, which would cause deeper networks to
    overfit to the training data. However, as noted in [[140](#bib.bib140)], when
    large amounts of data are available, deeper architectures can reduce both bias
    and variance in training, resulting in more robust control policies. Further thought
    should be given to architectures specifically designed for autonomous driving,
    such as the conditional imitation learning model [[138](#bib.bib138)], where the
    network included a different final network layer for each high-level command used
    for driving. These challenges translate to mid-to-mid approaches as well, as the
    selection of high-level features represented in the input to the network must
    be chosen carefully. Future works investigating specialised network architectures
    for autonomous driving can therefore be expected.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然架构选择是许多深度学习应用中的普遍问题，但像自动驾驶这样复杂的任务也带来了自身的挑战。目前，大多数端到端驾驶系统都被限制在较小的网络中。这是因为使用的数据集相对较小，导致更深的网络会对训练数据过拟合。然而，如[[140](#bib.bib140)]所述，当大量数据可用时，更深的架构可以减少训练中的偏差和方差，从而产生更强健的控制策略。应进一步考虑专门为自动驾驶设计的架构，例如条件模仿学习模型[[138](#bib.bib138)]，该模型中网络为每个高层指令都包含一个不同的最终网络层。这些挑战同样适用于中间方法，因为网络输入中表示的高层特征的选择必须谨慎。可以预期，未来将会有针对自动驾驶的专门网络架构的研究。
- en: IV-C Goal Specification
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 目标规范
- en: Adequate goal specification is a challenge specific to reinforcement learning
    methods. One of the advantages of reinforcement learning is that the behaviour
    of the agent does not need to be specified implicitly as it would be in rule-based
    systems. Only the reward function, which can often be easier to define than the
    value function, and the control action (e.g. steering, acceleration, braking)
    need to be defined. However, the goal of reinforcement learning is to maximise
    the long term accumulated reward as defined by the reward function. Therefore,
    the desired behaviour of the agent must be accurately captured by the reward function,
    otherwise unexpected and undesired behaviour might occur. For instance, instead
    of using binary rewards for successful or unsuccessful completion of tasks, intermediate
    rewards can be used to guide the agent towards desired behaviour, this process
    in known as reward shaping [[175](#bib.bib175), [176](#bib.bib176)]. For example,
    Desjardins & Chaib-Draa [[23](#bib.bib23)] used the time headway derivative to
    reward the agent for actions that helped it move towards the ideal time headway
    state. Furthermore, for a complex task such as driving, a multi-objective reward
    function needs to consider different objectives which may conflict with each other.
    For example, for driving, these objectives may include maintaining a safe distance
    from other vehicles, staying close to the centre of the lane, avoiding pedestrians,
    not changing lanes too often, maintaining desired velocity, and avoiding harsh
    accelerations/braking. Hence, the reward function should not only consider all
    factors that affect the agent’s behaviour, but also the weight of these factors.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 足够的目标规范是强化学习方法中特有的挑战。强化学习的一个优势是，代理的行为不需要像规则系统中那样隐式指定。只需定义奖励函数（通常比价值函数更易定义）和控制动作（例如转向、加速、刹车）即可。然而，强化学习的目标是最大化长期累积的奖励，如奖励函数所定义。因此，代理的期望行为必须准确地通过奖励函数来捕获，否则可能会出现意外和不期望的行为。例如，可以使用中间奖励来引导代理朝向期望行为，而不是仅使用成功或失败的二进制奖励，这一过程被称为奖励塑形[[175](#bib.bib175),
    [176](#bib.bib176)]。例如，Desjardins & Chaib-Draa [[23](#bib.bib23)]使用了时间头距导数来奖励代理做出有助于它朝理想时间头距状态前进的动作。此外，对于像驾驶这样复杂的任务，多目标奖励函数需要考虑可能彼此冲突的不同目标。例如，对于驾驶，这些目标可能包括保持与其他车辆的安全距离、保持在车道中心、避免行人、不频繁变道、保持期望速度和避免急加速/急刹车。因此，奖励函数不仅应考虑所有影响代理行为的因素，还应考虑这些因素的权重。
- en: A further challenge for agents which control both lateral & longitudinal actions
    is the difficulty of defining a reward function when the agent must be able to
    perform multiple actions (steering, braking, and acceleration). In reinforcement
    learning, the agent uses the feedback from the reward function to improve its
    own performance. However, when the agent is carrying out multiple actions, it
    may not be clear which of the actions resulted in the given reward. For example,
    if the vehicle steers away from the road, the acceleration may not be at fault
    but a negative reward signal is sent to the agent. One solution to this is a Hybrid
    Reward Architecture [[177](#bib.bib177)], where the system uses a decomposed reward
    function and learns a separate value function for each component reward function.
    Alternatively, Shalev-Shwartz et al. [[178](#bib.bib178)] proposed a solution
    in which the reward function is decomposed into a high level decision making system,
    through which the agent learns to drive safely and make strategic decisions (e.g.
    which cars to overtake or give way to), and a low level reward function which
    helps the agent learn an optimal policy for different actions (e.g. overtaking,
    merging, decelerating etc.).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于同时控制横向和纵向动作的智能体来说，进一步的挑战在于定义奖励函数的困难，特别是当智能体需要执行多个动作（如转向、制动和加速）时。在强化学习中，智能体使用奖励函数的反馈来提升自身性能。然而，当智能体执行多个动作时，可能不清楚哪个动作导致了给定的奖励。例如，如果车辆偏离了道路，加速可能不是问题所在，但智能体却接收到负奖励信号。对此的一种解决方案是混合奖励架构
    [[177](#bib.bib177)]，该系统使用分解的奖励函数，并为每个组成部分奖励函数学习一个独立的价值函数。另一种解决方案是 Shalev-Shwartz
    等人 [[178](#bib.bib178)] 提出的方案，其中奖励函数被分解为一个高层决策系统，通过该系统，智能体学习安全驾驶和制定战略决策（例如，超车或让行），以及一个低层奖励函数，帮助智能体为不同的动作（如超车、合并、减速等）学习最优策略。
- en: The developer should also take care that the agent does not exploit the reward
    function in unexpected ways, resulting in unintended behaviour. This effect is
    also known as reward hacking. Reward hacking occurs when the agent finds an unanticipated
    way of exploiting the reward function to gain large rewards in a way which goes
    against the developers’ defined objective(s) for the agent. For example, a robot
    used in ball paddling with a reward function based on the distance between the
    ball and the desired highest point, may attempt to move the racket up and keep
    the ball resting on it [[152](#bib.bib152)]. Potential solutions to avoid reward
    hacking were proposed by Amodei, et al. [[179](#bib.bib179)] in the form of adversarial
    reward functions, model look-ahead, reward capping, multiple reward functions,
    and trip wires. Adversarial reward functions utilise a reward function which is
    its own agent, similar to generative adversarial networks. The reward function
    agent can then explore the environment, making it more robust to reward hacking.
    It could, for example, try to find instances where the system claims a high reward
    from its actions, while a human would label it as a low reward. On the other hand,
    model look-ahead gives a reward based on anticipated future states, instead of
    the present one. Reward capping is a simple solution to reward hacking, where
    a maximum value is imposed on the reward function, thereby preventing unexpected
    high reward scenarios. Multiple reward functions can also increase robustness
    to reward hacking, since multiple rewards can be more difficult to hack than a
    single one. Finally, trip wires are deliberately placed vulnerabilities in the
    system, where reward hacking is most likely to occur. These vulnerabilities are
    then monitored to alert the system if the agent is attempting to exploit its reward
    function. Another approach to solving these challenges in goal specification is
    using inverse reinforcement learning to extract a reward function from expert
    demonstrations of the task [[180](#bib.bib180), [181](#bib.bib181), [182](#bib.bib182),
    [183](#bib.bib183)].
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者还应注意，代理不应以意想不到的方式利用奖励函数，导致非预期行为。这种现象也被称为奖励黑客。奖励黑客发生在代理找到了一种未预料的方式来利用奖励函数，从而以违背开发者为代理定义的目标的方式获得大量奖励。例如，用于球拍击球的机器人，其奖励函数基于球与目标最高点之间的距离，可能会尝试将球拍向上移动并保持球在其上[[152](#bib.bib152)]。Amodei等人[[179](#bib.bib179)]提出了避免奖励黑客的潜在解决方案，包括对抗性奖励函数、模型预测、奖励上限、多个奖励函数和触发器。对抗性奖励函数利用一个作为自身代理的奖励函数，类似于生成对抗网络。奖励函数代理可以探索环境，使其对奖励黑客更具鲁棒性。例如，它可以尝试找到系统声称从其行动中获得高奖励的情况，而人类则将其标记为低奖励。另一方面，模型预测根据预期的未来状态给予奖励，而不是当前状态。奖励上限是一种简单的解决方案，对奖励函数施加最大值，从而防止意外的高奖励场景。多个奖励函数也可以增加对奖励黑客的鲁棒性，因为多个奖励比单一奖励更难以被黑客攻击。最后，触发器是在系统中故意设置的脆弱点，最有可能发生奖励黑客的地方。这些脆弱点会被监控，以提醒系统代理是否试图利用其奖励函数。另一种解决目标指定挑战的方法是使用逆向强化学习从专家演示任务中提取奖励函数[[180](#bib.bib180),
    [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183)]。
- en: IV-D Adaptability & Generalisation
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 适应性与泛化
- en: Another challenge for learned control systems is dealing with different environments
    with a scalable approach. For example, a driving strategy that is successful in
    an urban environment may not be optimal on a highway, since they are very different
    environments with different traffic flow patterns and safety issues. Similar issues
    arise with changing weather conditions, seasons, climates etc. A neural network’s
    ability to use what it has learned from previous experiences to operate in a completely
    new environment is referred to as generalisation. However, the problem with generalisation
    is that even if the system demonstrates good generalisation in one new environment,
    there is no guarantee it will generalise to other possible environments. Moreover,
    considering the complex operating environment of a vehicle, it is not possible
    to test the system in all scenarios. Therefore, building a deep learning system
    capable of generalising to such a vast variety of situations, as well as validating
    its generalisation capability poses major challenges. This is a challenge that
    must be overcome for deep learning driven autonomous vehicles to be deployable
    in the real world, as the vehicles must be able to cope with the various different
    environments it will be used in.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于学习型控制系统来说，另一个挑战是以可扩展的方式处理不同的环境。例如，在城市环境中成功的驾驶策略在高速公路上可能并不理想，因为这些环境在交通流模式和安全问题上非常不同。类似的问题也会出现在变化的天气条件、季节、气候等方面。神经网络利用从以往经验中学到的知识在全新环境中进行操作的能力称为**泛化**。然而，泛化的问题在于，即使系统在一个新的环境中表现出良好的泛化能力，也不能保证它能在其他可能的环境中同样泛化。此外，考虑到车辆复杂的操作环境，无法在所有场景中测试系统。因此，构建一个能够泛化到如此广泛情况的深度学习系统，以及验证其泛化能力，都是主要的挑战。这是深度学习驱动的自动驾驶车辆必须克服的挑战，因为这些车辆必须能够应对其将要使用的各种不同环境。
- en: 'Generally, to avoid poor generalisation in DNNs the training must be stopped
    before the DNN starts to overfit to the training data. Overfitting refers to creating
    a model that fits the training data too well, losing its ability to generalise
    to new data. Overfitting occurs when the network is trained with either insufficient
    amounts of training data or too many training episodes on the same training data.
    This results in the neural network memorising the training data, thereby losing
    generalisation. Unfortunately, there are no known methods of choosing the optimal
    stopping point in order to avoid overfitting [[184](#bib.bib184)]. However, it
    is possible to get some indication of the network’s generalisation capability
    by having three different data sets: training, validation, and test sets. The
    training and validation sets are used during training, but only the training set
    results are used to update the network weights [[185](#bib.bib185)]. The purpose
    of the validation set is to minimise overfitting, by monitoring the error in the
    validation data set. In this way, it will be ensured that changes which reduce
    the error on the training set also reduce the error on the validation set, thereby
    avoiding overfitting. If the accuracy of the validation set starts to decrease
    over the training iterations, then the network is starting to overfit and training
    should be stopped. In addition to stopping overfitting, a validation set can also
    be used to compare different network architectures (e.g. comparing two different
    networks with different numbers of hidden layers) to provide a measure of generalisation.
    Nevertheless, utilising the validation set simultaneously in the selection of
    the network and to terminate training can result in overfitting to the validation
    set. Therefore an additional independent set, known as testing set, is required
    for the evaluation of the network performance [[186](#bib.bib186)]. The testing
    set is only used to test the final network to confirm its performance and generalisation
    capabilities. The testing set must provide an unbiased evaluation of the network’s
    generalisation [[185](#bib.bib185)]. Therefore, it is crucial that the test set
    is not used to choose between different networks or network architectures.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: There are also techniques available for DNNs which aim to reduce the test error,
    although often at the cost of increased training error, known as regularisation
    techniques [[48](#bib.bib48)]. The basis of regularisation techniques is to introduce
    some constraints on the deep learning model, which either introduce prior knowledge
    into the model or promote simpler models in order to achieve better generalisation
    capability. There are a variety of regularisation techniques available to choose
    from. For instance, L1 and L2 regularisation techniques introduce a constraint
    on the model by including an additional term in the cost function of the learning
    model, which makes the network prefer smaller weights. The smaller weights in
    the network reduce the effect of individual inputs on its behaviour, which means
    that the effect of local noise is reduced and the network is more likely to learn
    trends across the whole data set [[49](#bib.bib49), [187](#bib.bib187)]. Similarly,
    imposing constraints on the network weights through weight clipping has also been
    shown to improve robustness [[188](#bib.bib188), [189](#bib.bib189)]. Another
    popular regularisation technique is dropout, which drops out some randomly selected
    neurons from training and only updates the remaining weights for the given training
    example. At each weight update, a different set of neurons is omitted, thereby
    preventing complex co-adaptions between neurons. This helps each neuron learn
    features which are important for the given task and therefore helps reduce overfitting
    [[190](#bib.bib190), [191](#bib.bib191)].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些针对 DNN 的技术，旨在减少测试误差，尽管通常以增加训练误差为代价，这些技术被称为正则化技术 [[48](#bib.bib48)]。正则化技术的基础是对深度学习模型施加一些约束，这些约束要么将先验知识引入模型，要么促进更简单的模型，以实现更好的泛化能力。有多种正则化技术可供选择。例如，L1
    和 L2 正则化技术通过在学习模型的成本函数中增加额外的项，对模型施加约束，使网络更倾向于较小的权重。网络中的较小权重减少了个体输入对其行为的影响，这意味着局部噪声的影响减少，网络更有可能学习整个数据集中的趋势
    [[49](#bib.bib49)、[187](#bib.bib187)]。同样，通过权重裁剪对网络权重施加约束也被证明能提高鲁棒性 [[188](#bib.bib188)、[189](#bib.bib189)]。另一个流行的正则化技术是
    dropout，它会随机丢弃一些神经元的训练，仅更新给定训练样本的其余权重。在每次权重更新时，都会丢弃一组不同的神经元，从而防止神经元之间复杂的共同适应。这有助于每个神经元学习对给定任务重要的特征，从而有助于减少过拟合
    [[190](#bib.bib190)、[191](#bib.bib191)]。
- en: IV-E Verification & Validation
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 验证与确认
- en: The testing of the system needs to be rigorous to validate the performance and
    safety of the system. However, the problem is that real-world testing can be expensive
    in terms of time, labour, and finances. Indeed, full-scale vehicle studies with
    multiple vehicles have typically been achieved through collaboration of government
    research projects with automotive manufacturers, such as Demo ’97 [[192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194)] or Demo 2000 [[195](#bib.bib195)]. Alternatively,
    simulation studies can reduce the amount field testing required, and can be used
    as a first step for performance and safety evaluation. Simulation studies are
    significantly cheaper, faster, more flexible, and can be used to set up situations
    not easily achieved in real life (e.g. crashes). Indeed, with the increasing accuracy
    and speed of simulation tools, simulation has become an increasingly dominant
    method of study in this field [[196](#bib.bib196)].
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的测试需要严谨，以验证系统的性能和安全性。然而，现实世界中的测试在时间、劳动力和财务方面可能非常昂贵。实际上，涉及多个车辆的全尺度车辆研究通常是通过政府研究项目与汽车制造商的合作来实现的，例如
    Demo ’97 [[192](#bib.bib192)、[193](#bib.bib193)、[194](#bib.bib194)] 或 Demo 2000
    [[195](#bib.bib195)]。另一方面，模拟研究可以减少所需的现场测试量，并可作为性能和安全评估的第一步。模拟研究显著更便宜、更快、更灵活，并且可以用来设置在现实生活中不易实现的情况（例如碰撞）。实际上，随着模拟工具准确性和速度的提高，模拟已成为该领域越来越主要的研究方法
    [[196](#bib.bib196)]。
- en: While simulation has multiple advantages, the model errors must be kept in consideration
    throughout the verification and validation process. This is especially critical
    for training, as training an agent in an imprecise model will result in a system
    that will not transfer to the real-world without significant modifications [[152](#bib.bib152),
    [197](#bib.bib197)]. Complex mechanical interactions, such as contacts and friction,
    are often difficult to model accurately. These small variations between the simulation
    model and the real-world can have drastic consequences on the system behaviour
    in the real world. In other words, the problem is the agent overfitting its policies
    to the simulation environment, and not transferring well to a real-world environment.
    For a system that can be evaluated and used in the real-world, training, as well
    as testing, in both simulation and field tests would be required [[198](#bib.bib198)].
    The large number of trials required for reinforcement learning algorithms to converge,
    makes them susceptible to this issue where simulation is used for training. However,
    recent studies in robot manipulation have shown effective transfer of learned
    policies from simulation to the real world [[199](#bib.bib199), [200](#bib.bib200),
    [201](#bib.bib201), [202](#bib.bib202)].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Validation of the model and simulation environment alone is not enough for autonomous
    vehicles, as the influence of the training data can be equal to that of the algorithm
    itself [[203](#bib.bib203)]. Therefore, there should be emphasis on validating
    the quality of the training set as well. Ensuring that the data set represents
    the desired operational environment adequately, and covers the potential states
    is important. For instance, data sets that are biased towards a certain action
    (e.g. turn left) or scenario (e.g. driving in daytime) can introduce harmful biases
    into the learning model. Therefore, data sets should be validated to understand
    if they contain potentially harmful biases or patterns that could lead to undesirable
    behaviour of the learned control policy [[56](#bib.bib56)].
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: A Summary of Research Challenges.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '| Challenge | Sub-challenges | Potential Solutions |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| Computation | • Computation requirements for deep learning • Large data sets
    for supervised learning • Curse of dimensionality in high dimensional problems
    • Simulation requirements for sample inefficient techniques | • Scalable model
    architectures • Sharing data sets • Improving sample efficiency in reinforcement
    learning • Parallelisable architectures for training |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Architectures | • Lack of clear rules for network architectures • Reliance
    on heuristics and trial-and-error | • Automated neural architecture search methods
    • Specialised architectures for autonomous driving |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| Goal Specification | • Well designed reward functions for complex tasks •
    Multi-objective reward functions • Reward hacking | • Reward shaping • Inverse
    reinforcement learning • Hybrid reward architectures |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 目标设定 | • 为复杂任务设计的良好奖励函数 • 多目标奖励函数 • 奖励劫持 | • 奖励塑造 • 逆向强化学习 • 混合奖励架构 |'
- en: '| Adaptability & Generalisation | • Wide variety of the operational environment
    • Overfitting to training data/environment | • Representative data sets and/or
    training environments • Effective use of regularisation techniques |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 适应性与泛化 | • 操作环境的广泛多样性 • 对训练数据/环境的过拟合 | • 代表性数据集和/或训练环境 • 有效的正则化技术使用 |'
- en: '| Verification & Validation | • Inability to test in all possible scenarios
    • High cost of field testing • Inaccuracies in simulation • Biases and gaps in
    data sets | • High fidelity simulations • Effective simulation to real world transfer
    • Validation of data set coverage |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 验证与确认 | • 无法在所有可能的场景中进行测试 • 现场测试的高成本 • 模拟中的不准确性 • 数据集中的偏差和漏洞 | • 高保真模拟 •
    有效的模拟到现实世界的转移 • 数据集覆盖范围的验证 |'
- en: '| Safety | • Complexity and opaqueness of DNNs • Safe training in the real
    world • Adversarial attacks | • Research into interpretability of DNNs • Fail
    safes and virtual safety cages • Human oversight • Improving model robustness
    to perturbations |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 安全 | • 深度神经网络的复杂性和不透明性 • 现实世界中的安全训练 • 对抗性攻击 | • 对深度神经网络可解释性的研究 • 失效保护和虚拟安全笼
    • 人类监督 • 提高模型对扰动的鲁棒性 |'
- en: IV-F Safety
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 安全
- en: In a safety-critical system, such as vehicle operation, a serious malfunction
    or failure could result in death or serious harm to people or property. Therefore,
    the safety of road users must be ensured before such systems are deployed commercially.
    However, ensuring functional safety in deep learning systems can be challenging.
    As the neural networks become more complex, the solutions they provide and how
    they come to those solutions becomes increasingly difficult to interpret [[204](#bib.bib204)].
    This is known as the black box problem. The opacity of these solutions is an obstacle
    to their implementation in safety-critical applications; while it is possible
    to show that these systems provide good performance in our validation environment,
    it is impossible to test these systems in all the possible environments they would
    encounter in the real world. Therefore, if we do not understand the way in which
    the system makes its decisions, ensuring it does not make unsafe decisions in
    new environments becomes increasingly difficult. It becomes even more challenging
    in online learning methods, since they change their policies during operation
    and therefore could potentially shift from safe policies to unsafe policies over
    time [[205](#bib.bib205), [206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [209](#bib.bib209)].
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全关键系统中，例如车辆操作，严重的故障或失效可能会导致人员或财产的死亡或严重伤害。因此，在这些系统商业部署之前，必须确保道路使用者的安全。然而，确保深度学习系统的功能安全可能具有挑战性。随着神经网络变得越来越复杂，它们提供的解决方案及其产生这些解决方案的方式变得越来越难以解释[[204](#bib.bib204)]。这被称为黑箱问题。这些解决方案的不透明性是它们在安全关键应用中实施的障碍；尽管可以证明这些系统在我们的验证环境中表现良好，但在现实世界中它们可能遇到的所有环境中进行测试是不可能的。因此，如果我们不了解系统做出决策的方式，在新环境中确保系统不会做出不安全的决策变得越来越困难。在在线学习方法中尤为具有挑战性，因为它们在操作过程中会改变其策略，因此随着时间的推移可能会从安全策略转变为不安全策略[[205](#bib.bib205),
    [206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209)]。
- en: Any autonomous vehicle system not only needs to drive safely, but it also needs
    to be capable of reacting in a safe manner to other vehicles or pedestrians acting
    unpredictably. It can be difficult to guarantee the safety for any vehicle controller
    if, for example, another driver is acting recklessly or a previously unseen pedestrian
    runs onto the road. Therefore, it would be useful to include unsafe and aggressive
    driving behaviours of other vehicles into the training data of the vehicle controller
    to enable it to learn how to deal with such situations. One option to improve
    reliability and safety in such situations is utilising a trauma memory [[111](#bib.bib111)]
    where rare negative events (e.g. collisions) are stored. These are then used in
    training to persistently remind the agent of these events and ensure it maintains
    safe behaviour.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 任何自动驾驶系统不仅需要安全驾驶，还需要能够以安全的方式对其他车辆或行人做出不可预测的反应。如果，例如，另一个司机行为鲁莽或一个之前未见的行人突然跑到路上，保证车辆控制器的安全可能会很困难。因此，将其他车辆的不安全和侵略性驾驶行为纳入车辆控制器的训练数据中，以使其学会如何处理这些情况，将是有用的。提高在这种情况下的可靠性和安全性的一种选择是利用创伤记忆
    [[111](#bib.bib111)]，在其中存储稀有的负面事件（例如碰撞）。然后在训练中使用这些事件，以持久地提醒代理这些事件，并确保其保持安全行为。
- en: Also, safety must be maintained during any training or testing in the real world.
    For instance, during early training of a reinforcement learning agent, the agent
    is more likely to use exploration than exploitation of past experiences, which
    means the agent will effectively be learning through trial and error. Therefore,
    care must be taken to ensure the exploration happens in a safe manner. This is
    especially true in any environment including other road users or pedestrians,
    since inappropriate actions chosen due to exploration could have disastrous results.
    Exploration poses safety challenges as the agent is encouraged to take random
    actions, which can lead to catastrophic events if not considered beforehand [[210](#bib.bib210),
    [211](#bib.bib211), [212](#bib.bib212), [213](#bib.bib213)]. Potential solutions
    include the use of demonstrations such as in IRL to provide examples of safe behaviour
    which could be used as a baseline policy, simulated exploration where exploration
    happens in a simulated environment, bounded exploration which limits exploration
    in state spaces which are considered unsafe, and human oversight although this
    is limited in scalability and not feasible in some real-time systems. The same
    holds true for any testing and evaluation of the system; until the system has
    been deemed to perform adequately and in a safe manner, all necessary precautions
    must be taken to ensure safety [[179](#bib.bib179)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何实际世界的训练或测试中，必须保持安全。例如，在强化学习代理的早期训练中，代理更可能进行探索而非利用过去的经验，这意味着代理将通过反复试验来学习。因此，必须确保探索过程是安全的。这在包括其他道路使用者或行人等环境中特别重要，因为由于探索而选择的不当行动可能会导致灾难性的后果。探索带来了安全挑战，因为代理被鼓励采取随机行动，如果没有事先考虑，这可能会导致严重事件。潜在的解决方案包括使用演示（如在
    IRL 中）提供安全行为的示例，这些示例可以用作基准策略、在模拟环境中进行模拟探索、限制探索在被认为不安全的状态空间中进行的有界探索，以及尽管在人类监督方面具有可扩展性有限且在某些实时系统中不可行。系统的任何测试和评估也是如此；在系统被认为表现适当且安全之前，必须采取所有必要的预防措施以确保安全
    [[210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212), [213](#bib.bib213)]。
- en: An approach for ensuring functional safety for deep learning based autonomous
    vehicles is suggested by Shalev-Shwartz et al. [[178](#bib.bib178)]. In the proposed
    system architecture, the policy function is decomposed into a learnable part and
    a non-learnable part. The learnable part is responsible for the comfort of driving
    and for making strategic decisions (e.g. which cars to overtake or give way to).
    This policy is learned from experience by maximising an expected reward from the
    reward function. On the other hand, the non-learnable policy is responsible for
    the safety by minimising a cost function with hard constraints (e.g. the vehicle
    is not allowed within a specified distance of other vehicles’ trajectories) to
    ensure functional safety. Alternatively, Xiong et al. [[214](#bib.bib214)] suggested
    a control structure which combines reinforcement learning based control with safety
    based control and path tracking. The aim is to combine a traditional control method
    with a reinforcement method to take advantage of the superior performance of deep
    learning systems whilst ensuring safety through traditional control theory. The
    path tracking element is included to ensure the vehicle stays on (or as close
    as is safe to) the centre of the lane. The reinforcement learning approach is
    based on the DDPG algorithm. Also, the safety based controller uses an Artificial
    Potential Field method [[215](#bib.bib215)] which models any obstacles with a
    repulsive force to steer the vehicle away from them. The final steering policy
    is then found by the weighted summation of the three models. The system was shown
    to keep a safe distance in a simulated environment where the vehicle had to drive
    along a curve with other vehicles nearby.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Shalev-Shwartz等人提出了一种确保深度学习基础的自动驾驶车辆功能安全的方法[[178](#bib.bib178)]。在所提议的系统架构中，策略函数被分解为可学习部分和不可学习部分。可学习部分负责驾驶的舒适性和制定战略决策（例如，超车或让行）。该策略通过最大化来自奖励函数的期望奖励来从经验中学习。另一方面，不可学习的策略通过最小化具有硬约束的成本函数（例如，车辆不得接近其他车辆轨迹的指定距离）来确保安全。或者，Xiong等人[[214](#bib.bib214)]建议了一种结合了基于强化学习的控制、基于安全的控制和路径跟踪的控制结构。其目的是将传统控制方法与强化方法相结合，利用深度学习系统的优越性能，同时通过传统控制理论确保安全。路径跟踪元素用于确保车辆保持在车道的中心（或尽可能接近）。强化学习方法基于DDPG算法。此外，基于安全的控制器使用人工势场方法[[215](#bib.bib215)]，通过排斥力来引导车辆远离障碍物。最终的转向策略是通过三种模型的加权求和来确定的。该系统在模拟环境中展示了在车辆必须沿曲线行驶且附近有其他车辆的情况下保持安全距离的能力。
- en: Furthermore, malicious inputs to deep learning systems have to be considered.
    It has been shown that visual classification DNN systems are vulnerable to adversarial
    examples, which are perturbed images that cause the DNNs to misclassify them with
    high confidence [[216](#bib.bib216), [217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219)],
    including misclassification of traffic signs [[220](#bib.bib220)]. DNNs have been
    shown to be vulnerable to printed adversarial examples in the real world [[221](#bib.bib221)]
    and even to 3D-printed physical adversarial examples [[222](#bib.bib222)], which
    suggests they are a threat to DNN applications in the real world. Moreover, the
    image modification of the adversarial examples have been shown to be subtle enough
    that a human eye does not notice the modification, making prevention of such malicious
    attacks difficult [[221](#bib.bib221)]. These types of weaknesses in DNNs could
    be exploited and pose a security concern for any technology using DNNs. Although
    defences against these attacks have been proposed [[223](#bib.bib223)], state-of-the-art
    attacks can by-pass defences and detection mechanisms.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还必须考虑到深度学习系统的恶意输入。研究表明，视觉分类DNN系统对对抗性样本（这些样本是扰动的图像，会使DNN高置信度地错误分类）易受攻击[[216](#bib.bib216),
    [217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219)]，包括交通标志的错误分类[[220](#bib.bib220)]。DNN系统已被证明在现实世界中对印刷的对抗性样本[[221](#bib.bib221)]，甚至对3D打印的物理对抗性样本[[222](#bib.bib222)]敏感，这表明它们对DNN应用构成威胁。此外，对抗性样本的图像修改已被证明足够微妙，以至于人眼无法察觉，从而使得防止这种恶意攻击变得困难[[221](#bib.bib221)]。这些DNN的弱点可能被利用，对任何使用DNN的技术构成安全隐患。尽管已经提出了防御这些攻击的方法[[223](#bib.bib223)]，但最先进的攻击仍然可以绕过防御和检测机制。
- en: V Concluding Remarks
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: 'In this manuscript, a survey of autonomous vehicle control approaches utilising
    deep learning was presented. The approaches were separated into three categories:
    lateral (steering), longitudinal (acceleration and braking), and simultaneous
    lateral and longitudinal control methods. The focus of this manuscript has been
    on the vehicle control techniques rather than perception, however there is some
    obvious overlap between them. It was shown that research interest in this field
    has grown significantly in recent years and is expected to continue to do so.
    The applications discussed in this paper show great promise for the application
    of deep learning to autonomous vehicle control. However, current deep learning
    based controller performance has significant room for improvement. Moreover, much
    of the current research is only limited to simulation. While testing in simulation
    is useful for feasibility studies and initial performance evaluations, extensive
    testing and training in the field will be required before these systems are ready
    for deployment.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: The main research challenges to deep learning based vehicle control were also
    discussed and can be seen summarised in Table [IV](#S4.T4 "TABLE IV ‣ IV-E Verification
    & Validation ‣ IV Challenges ‣ A Survey of Deep Learning Applications to Autonomous
    Vehicle Control"). Computation was identified as a challenge due to the large
    amount of data required to train deep learning models. Architectures were also
    identified as a challenge due to the difficulty of choosing the optimal network
    architecture for a given task. Goal specification is a challenge for reinforcement
    learning techniques due to the importance of designing a reward function which
    promotes the desired behaviour. Adaptability and generalisation is a challenge
    in the autonomous vehicle domain due to the highly complex nature of the operational
    environment. Verification and validation is a further challenge due to the high
    cost and time requirements of field tests and training. While simulation is an
    obvious solution to reduce the amount of physical field testing required, the
    use of simulation in training and testing has its own drawbacks. Safety was identified
    as a crucial challenge due to the safety critical nature of the autonomous vehicle
    domain. This is made more challenging due to the opaque nature of deep learning
    methods, making safety validation of these systems problematic.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, further research into interpretability of neural networks and functional
    safety validation methods for neural network-driven vehicles will be required.
    Before deep learning can be deployed on the road, some safety validation techniques
    will need to be found to address their opaqueness. Ensuring the safety of these
    deep neural networks is a major barrier preventing them from being used commercially.
    Furthermore, as noted by Salay et al. [[224](#bib.bib224)] in their analysis of
    the ISO26262 [[225](#bib.bib225)] standard, more than 40% of the required software
    techniques in the current version of the standard are incompatible with machine
    learning techniques, whilst the rest are either directly applicable or applicable
    if modified slightly. This reveals further need for these standards to be revised
    to address machine learning systems for autonomous vehicles [[226](#bib.bib226)].
    Other safety aspects which warrant further research include defences against adversarial
    attacks, as they currently present a significant safety problem for the use of
    DNNs in autonomous vehicles. Also, robustness to erroneous inputs from sensory
    data or communication failures must be investigated. There is currently a significant
    gap in the literature for investigation of fault tolerant systems. Further research
    into how deep learning control systems deal with issues such as communication
    failures, erroneous sensory inputs, input noise, or sensor failure would move
    the industry towards robust and safe solutions. Furthermore, while research into
    deep neural networks with both lateral and longitudinal vehicle control is still
    relatively sparse, there is significant on-going research in this area. Full vehicle
    control with deep neural networks is typically achieved in simple simulation scenarios
    and/or with discretised outputs. Much work can be done to improve the performance
    of the full vehicle control techniques as well. Techniques in Sections III-A and
    B show promising results for lateral and longitudinal control systems, and future
    work will be required to bridge these techniques into an autonomous vehicle system
    with strong performance in the more general case of combined lateral and longitudinal
    control. This will also include further experiments in the real world to validate
    the performance of the learned control policies. Other avenues for future research
    include learning driving manoeuvrers which are still typically achieved through
    classical control techniques, such as overtaking [[227](#bib.bib227), [228](#bib.bib228)]
    or merging [[229](#bib.bib229), [230](#bib.bib230)]. Further work will also be
    needed to design autonomous vehicles which can understand the rules of the road
    and the behaviour of other road users. Some on-going research was discussed where
    the deep neural network can take into account the intended route or target destination,
    but more research is needed to ensure these techniques can stop at stop signs
    and red lights, respect speed limits, or negotiate intersections and roundabouts
    with other vehicles.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] National Highway Traffic Safety Administration (NHTSA), “2016 Fatal Motor
    Vehicle Crashes: Overview,” 2017\. [Online]. Available: [https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812456](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812456)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] European Commission, “2016 road safety statistics: What is behind the figures?”
    2017\. [Online]. Available: [http://europa.eu/rapid/press-release_MEMO-17-675_en.htm](http://europa.eu/rapid/press-release_MEMO-17-675_en.htm)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] World Health Organization, “Global status report on road safety 2018,”
    2018\. [Online]. Available: [https://www.who.int/violence_injury_prevention/road_safety_status/2018/en/](https://www.who.int/violence_injury_prevention/road_safety_status/2018/en/)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] A. Eskandarian, *Handbook of intelligent vehicles*.   Springer, 2012, vol. 2.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] S. Thrun, “Toward robotic cars,” *Communications of the ACM*, vol. 53,
    no. 4, p. 99, 2010.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] C. Urmson and W. Whittaker, “Self-driving cars and the Urban challenge,”
    *IEEE Intelligent Systems*, vol. 23, no. 2, pp. 66–68, 2008.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] U. Montanaro, S. Dixit, S. Fallah, M. Dianati, A. Stevens, D. Oxtoby, and
    A. Mouzakitis, “Towards connected autonomous driving: review of use-cases,” *Vehicle
    System Dynamics*, pp. 1–36, 2018.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. Singh, “Critical reasons for crashes investigated in the National Motor
    Vehicle Crash Causation Survey,” *National Highway Traffic Safety Administration*,
    no. February, pp. 1–2, 2015.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] T. Luettel, M. Himmelsbach, and H. J. Wuensche, “Autonomous Ground Vehicles
    ―Concepts and a Path to the Future,” *Proceedings of the IEEE*, vol. 100, no.
    Special Centennial Issue, pp. 1831–1839, 2012.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] W. Payre, J. Cestac, and P. Delhomme, “Intention to use a fully automated
    car: Attitudes and a priori acceptability,” *Transportation Research Part F: Traffic
    Psychology and Behaviour*, vol. 27, no. PB, pp. 252–263, 2014.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] P. Ross, “Robot, you can drive my car,” *IEEE Spectrum*, vol. 51, no. 6,
    2014.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Department for Transport, “Research on the Impacts of Connected and Autonomous
    Vehicles (CAVs) on Traffic Flow: Summary Report,” 2017. [Online]. Available: [https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/530091/impacts-of-connected-and-autonomous-vehicles-on-traffic-flow-summary-report.pdf](https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/530091/impacts-of-connected-and-autonomous-vehicles-on-traffic-flow-summary-report.pdf)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] C. Thorpe, M. Herbert, T. Kanade, and S. Shafter, “Toward autonomous driving:
    the cmu navlab. ii. architecture and systems,” *IEEE expert*, vol. 6, no. 4, pp.
    44–52, 1991.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] E. D. Dickmanns and A. Zapp, “Autonomous high speed road vehicle guidance
    by computer vision1,” *IFAC Proceedings Volumes*, vol. 20, no. 5, pp. 221–226,
    1987.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel,
    P. Fong, J. Gale, M. Halpenny, G. Hoffmann *et al.*, “Stanley: The robot that
    won the darpa grand challenge,” *Journal of field Robotics*, vol. 23, no. 9, pp.
    661–692, 2006.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Buehler, K. Iagnemma, and S. Singh, *The DARPA urban challenge: autonomous
    vehicles in city traffic*.   springer, 2009, vol. 56.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] T. Le-Anh and M. De Koster, “A review of design and control of automated
    guided vehicle systems,” *European Journal of Operational Research*, vol. 171,
    no. 1, pp. 1–23, 2006.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] B. Paden, M. Čáp, S. Z. Yong, D. Yershov, and E. Frazzoli, “A survey of
    motion planning and control techniques for self-driving urban vehicles,” *IEEE
    Transactions on intelligent vehicles*, vol. 1, no. 1, pp. 33–55, 2016.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] M. Pasquier, C. Quek, and M. Toh, “Fuzzylot: a novel self-organising fuzzy-neural
    rule-based pilot system for automated vehicles,” *Neural networks*, vol. 14, no. 8,
    pp. 1099–1112, 2001.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] M. Kuderer, S. Gulati, and W. Burgard, “Learning driving styles for autonomous
    vehicles from demonstration,” *Proceedings - IEEE International Conference on
    Robotics and Automation*, vol. 2015-June, no. June, pp. 2641–2646, 2015.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] D. Silver, J. A. Bagnell, and A. Stentz, “Learning Autonomous Driving
    Styles and Maneuvers from Expert Demonstration,” in *Experimental Robotics*.   Springer,
    Heidelberg, 2013, pp. 371–386.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] D. Zhao, B. Wang, and D. Liu, “A supervised Actor-Critic approach for
    adaptive cruise control,” *Soft Computing*, vol. 17, no. 11, pp. 2089–2099, 2013.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] C. Desjardins and B. Chaib-draa, “Cooperative Adaptive Cruise Control:
    A Reinforcement Learning Approach,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 12, no. 4, pp. 1248–1260, 2011.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in neural information processing
    systems*, 2012, pp. 1097–1105.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *et al.*, “Deep neural networks for acoustic
    modeling in speech recognition: The shared views of four research groups,” *IEEE
    Signal Processing Magazine*, vol. 29, no. 6, pp. 82–97, 2012.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *Advances in neural information processing systems*,
    2014, pp. 3104–3112.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] W. Schwarting, J. Alonso-Mora, and D. Rus, “Planning and decision-making
    for autonomous vehicles,” *Annual Review of Control, Robotics, and Autonomous
    Systems*, no. 0, 2018.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] T. T. Mac, C. Copot, D. T. Tran, and R. De Keyser, “Heuristic approaches
    in robot path planning: A survey,” *Robotics and Autonomous Systems*, vol. 86,
    pp. 13–28, 2016.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. M. Veres, L. Molnar, N. K. Lincoln, and C. P. Morice, “Autonomous vehicle
    control systems—a review of decision making,” *Proceedings of the Institution
    of Mechanical Engineers, Part I: Journal of Systems and Control Engineering*,
    vol. 225, no. 2, pp. 155–195, 2011.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] L. Caltagirone, M. Bellone, L. Svensson, and M. Wahde, “Lidar-based driving
    path generation using fully convolutional neural networks,” in *Intelligent Transportation
    Systems (ITSC), 2017 IEEE 20th International Conference on*.   IEEE, 2017, pp.
    1–6.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Dixit, S. Fallah, U. Montanaro, M. Dianati, A. Stevens, F. Mccullough,
    and A. Mouzakitis, “Trajectory planning and tracking for autonomous overtaking:
    State-of-the-art and future prospects,” *Annual Reviews in Control*, 2018.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] H. Zhu, K.-V. Yuen, L. Mihaylova, and H. Leung, “Overview of environment
    perception for intelligent vehicles,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 18, no. 10, pp. 2584–2601, 2017.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] J. Van Brummelen, M. O’Brien, D. Gruyer, and H. Najjaran, “Autonomous
    vehicle perception: The technology of today and tomorrow,” *Transportation research
    part C: emerging technologies*, 2018.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J. Janai, F. Güney, A. Behl, and A. Geiger, “Computer vision for autonomous
    vehicles: Problems, datasets and state-of-the-art,” *arXiv preprint arXiv:1704.05519*,
    2017.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] R. Benenson, M. Omran, J. Hosang, and B. Schiele, “Ten years of pedestrian
    detection, what have we learned?” in *European Conference on Computer Vision*.   Springer,
    2014, pp. 613–627.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Zhang, R. Benenson, M. Omran, J. Hosang, and B. Schiele, “How far are
    we from solving pedestrian detection?” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 1259–1267.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, and
    M. J. Milford, “Visual place recognition: A survey,” *IEEE Transactions on Robotics*,
    vol. 32, no. 1, pp. 1–19, 2016.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] K. R. Konda and R. Memisevic, “Learning visual odometry with a convolutional
    network.” in *VISAPP (1)*, 2015, pp. 486–490.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Kuutti, S. Fallah, K. Katsaros, M. Dianati, F. Mccullough, and A. Mouzakitis,
    “A survey of the state-of-the-art localization techniques and their potentials
    for autonomous vehicle applications,” *IEEE Internet of Things Journal*, vol. 5,
    no. 2, pp. 829–846, 2018.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
    deep visuomotor policies,” *The Journal of Machine Learning Research*, vol. 17,
    no. 1, pp. 1334–1373, 2016.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen, “Learning hand-eye
    coordination for robotic grasping with large-scale data collection,” in *International
    Symposium on Experimental Robotics*.   Springer, 2016, pp. 173–184.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] V. Rausch, A. Hansen, E. Solowjow, C. Liu, E. Kreuzer, and J. K. Hedrick,
    “Learning a deep neural net policy for end-to-end control of autonomous vehicles,”
    in *2017 American Control Conference (ACC)*.   IEEE, 2017, pp. 4914–4919.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, p.
    529, 2015.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] I. Arel, D. C. Rose, and T. P. Karnowski, “Deep machine learning-a new
    frontier in artificial intelligence research [research frontier],” *IEEE computational
    intelligence magazine*, vol. 5, no. 4, pp. 13–18, 2010.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Tani, M. Ito, and Y. Sugita, “Self-organization of distributedly represented
    multiple behavior schemata in a mirror system: reviews of robot experiments using
    rnnpb,” *Neural Networks*, vol. 17, no. 8-9, pp. 1273–1289, 2004.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Lecun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    networks*, vol. 61, pp. 85–117, 2015.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] I. Goodfellow, Y. Bengio, and A. Courville, “Deep Learning,” *MIT Press*,
    2016\. [Online]. Available: [http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Nielsen, “Neural Networks and Deep Learning,” *Determination Press*,
    2015\. [Online]. Available: [http://neuralnetworksanddeeplearning.com/index.html](http://neuralnetworksanddeeplearning.com/index.html)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction*.   Cambridge,
    MA: MIT Press, 1998, vol. 9.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
    reinforcement learning: A brief survey,” *IEEE Signal Processing Magazine*, vol. 34,
    no. 6, pp. 26–38, 2017.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Y. Li, “Deep reinforcement learning: An overview,” *arXiv preprint arXiv:1701.07274*,
    2017.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] L. Bottou and O. Bousquet, “The tradeoffs of large scale learning,” in
    *Advances in neural information processing systems*, 2008, pp. 161–168.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
    and structured prediction to no-regret online learning,” in *Proceedings of the
    fourteenth international conference on artificial intelligence and statistics*,
    2011, pp. 627–635.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] P. de Haan, D. Jayaraman, and S. Levine, “Causal confusion in imitation
    learning,” *arXiv preprint arXiv:1905.11979*, 2019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Torralba, A. A. Efros *et al.*, “Unbiased look at dataset bias.” in
    *CVPR*, vol. 1, no. 2.   Citeseer, 2011, p. 7.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Gupta, A. Murali, D. P. Gandhi, and L. Pinto, “Robot learning in homes:
    Improving generalization and reducing dataset bias,” in *Advances in Neural Information
    Processing Systems*, 2018, pp. 9094–9104.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas,
    “Sample efficient actor-critic with experience replay,” *arXiv preprint arXiv:1611.01224*,
    2016.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] V. R. Konda and J. N. Tsitsiklis, “On actor-critic algorithms,” *SIAM
    journal on Control and Optimization*, vol. 42, no. 4, pp. 1143–1166, 2003.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] C. J. Watkins and P. Dayan, “Q-learning,” *Machine learning*, vol. 8,
    no. 3-4, pp. 279–292, 1992.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] G. J. Gordon, “Stable function approximation in dynamic programming,”
    in *Machine Learning Proceedings 1995*.   Elsevier, 1995, pp. 261–268.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. N. Tsitsiklis and B. Van Roy, “Feature-based methods for large scale
    dynamic programming,” *Machine Learning*, vol. 22, no. 1-3, pp. 59–94, 1996.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] R. J. Williams, *Reinforcement-learning connectionist systems*.   College
    of Computer Science, Northeastern University, 1987.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, “Policy gradient
    methods for reinforcement learning with function approximation,” in *Advances
    in neural information processing systems*, 2000, pp. 1057–1063.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] M. Riedmiller, J. Peters, and S. Schaal, “Evaluation of policy gradient
    methods and variants on the cart-pole benchmark,” in *Approximate Dynamic Programming
    and Reinforcement Learning, 2007\. ADPRL 2007\. IEEE International Symposium on*.   IEEE,
    2007, pp. 254–261.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” 2014.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International conference on machine learning*, 2016, pp. 1928–1937.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] I. Grondman, L. Busoniu, G. A. Lopes, and R. Babuska, “A survey of actor-critic
    reinforcement learning: Standard and natural policy gradients,” *IEEE Transactions
    on Systems, Man, and Cybernetics, Part C (Applications and Reviews)*, vol. 42,
    no. 6, pp. 1291–1307, 2012.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-dimensional
    continuous control using generalized advantage estimation,” *arXiv preprint arXiv:1506.02438*,
    2015.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The kitti dataset,” *International Journal of Robotics Research (IJRR)*, 2013.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *Computer Vision and Pattern Recognition
    (CVPR), 2012 IEEE Conference on*, 2012.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] “Waymo open dataset: An autonomous driving dataset,” 2019\. [Online].
    Available: [https://www.waymo.com/open](https://www.waymo.com/open)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year, 1000km: The
    Oxford RobotCar Dataset,” *The International Journal of Robotics Research (IJRR)*,
    vol. 36, no. 1, pp. 3–15, 2017\. [Online]. Available: [http://dx.doi.org/10.1177/0278364916679498](http://dx.doi.org/10.1177/0278364916679498)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin, and R. Yang,
    “The apolloscape dataset for autonomous driving,” *arXiv preprint arXiv:1803.06184*,
    2018.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Udacity Inc., “Udacity Self-driving Car Dataset,” 2018\. [Online]. Available:
    [https://github.com/udacity/self-driving-car](https://github.com/udacity/self-driving-car)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] A. Ess, B. Leibe, K. Schindler, , and L. van Gool, “A mobile vision system
    for robust multi-person tracking,” in *Computer Vision and Pattern Recognition
    (CVPR), 2008 IEEE Conference on*.   IEEE Press, June 2008.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] P. Dollár, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection:
    A benchmark,” in *Computer Vision and Pattern Recognition (CVPR), 2009 IEEE Conference
    on*.   IEEE, 2009, pp. 304–311.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] H. Yin and C. Berger, “When to use what data set for your self-driving
    car algorithm: An overview of publicly available driving datasets,” in *Intelligent
    Transportation Systems (ITSC), 2017 IEEE 20th International Conference on*.   IEEE,
    2017, pp. 1–8.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] NVIDIA Corporation, “Autonomous car development platform from NVIDIA DRIVE
    PX2,” 2018\. [Online]. Available: [https://www.nvidia.com/en-us/self-driving-cars/drive-platform/](https://www.nvidia.com/en-us/self-driving-cars/drive-platform/)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] MobilEye, “The Evolution of EyeQ,” 2018\. [Online]. Available: [https://www.mobileye.com/our-technology/evolution-eyeq-chip/](https://www.mobileye.com/our-technology/evolution-eyeq-chip/)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Intel Corporation, “Cyclone V - Overview,” 2018\. [Online]. Available:
    [https://www.altera.com/products/fpga/cyclone-series/cyclone-v/overview.html](https://www.altera.com/products/fpga/cyclone-series/cyclone-v/overview.html)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] S. Liu, J. Tang, Z. Zhang, and J.-L. Gaudiot, “Caad: Computer architecture
    for autonomous driving,” *arXiv preprint arXiv:1702.01894*, 2017.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural network,”
    *Advances in Neural Information Processing Systems 1*, pp. 305–313, 1989.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] D. Pomerleau, “Neural network vision for robot driving,” *Intelligent
    Unmanned Ground Vehicles*, pp. 1–22, 1997.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] G. Yu and I. K. Sethi, “Road-following with continuous learning,” in *Intelligent
    Vehicles ’95 Symposium., Proceedings of the*, Detroit, MI, 1995.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] D. E. Moriarty, S. Handley, and P. Langley, “Learning distributed strategies
    for traffic control,” *Proc. of the fifth International Conference of the Society
    for Adaptive Behavior*, no. May 1998, pp. 437–446, 1998.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba,
    “End to End Learning for Self-Driving Cars,” no. May, 2016. [Online]. Available:
    [http://arxiv.org/abs/1604.07316](http://arxiv.org/abs/1604.07316)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. L. Cun, “Off-road obstacle
    avoidance through end-to-end learning,” in *Advances in neural information processing
    systems*, 2006, pp. 739–746.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Mechanical Simulation Corporation, “CarSim.” [Online]. Available: [https://www.carsim.com](https://www.carsim.com)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] L. Bottou, “Large-scale machine learning with stochastic gradient descent,”
    in *Proceedings of COMPSTAT’2010*.   Springer, 2010, pp. 177–186.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
    *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] W. Su, S. Boyd, and E. Candes, “A differential equation for modeling nesterov’s
    accelerated gradient method: Theory and insights,” in *Advances in Neural Information
    Processing Systems*, 2014, pp. 2510–2518.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] H. M. Eraqi, M. N. Moustafa, and J. Honer, “End-to-end deep learning for
    steering autonomous vehicles considering temporal dependencies,” *arXiv preprint
    arXiv:1710.03804*, 2017.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] R. Rothe, R. Timofte, and L. Van Gool, “Dex: Deep expectation of apparent
    age from a single image,” in *Proceedings of the IEEE International Conference
    on Computer Vision Workshops*, 2015, pp. 10–15.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] F. Codevilla, A. M. López, V. Koltun, and A. Dosovitskiy, “On offline
    evaluation of vision-based driving models,” in *Proceedings of the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 236–251.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] P. Wang, C.-Y. Chan, and A. de La Fortelle, “A reinforcement learning
    based approach for automated lane change maneuvers,” in *2018 IEEE Intelligent
    Vehicles Symposium (IV)*.   IEEE, 2018, pp. 1379–1384.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] A. Vahidi and A. Eskandarian, “Research advances in intelligent collision
    avoidance and adaptive cruise control,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 4, no. 3, pp. 143–153, 2003.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] S. Moon, I. Moon, and K. Yi, “Design, tuning, and evaluation of a full-range
    adaptive cruise control system with collision avoidance,” *Control Engineering
    Practice*, vol. 17, no. 4, pp. 442–455, 2009.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Q. Sun, “Cooperative Adaptive Cruise Control Performance Analysis,” Ph.D.
    dissertation, Ecole Centrale de Lille, 2016.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Chen, Y. Zhai, C. Lu, J. Gong, and G. Wang, “A Learning Model for
    Personalized Adaptive Cruise Control,” in *Intelligent Vehicles Symposium (IV),
    2017 IEEE*, 2017, pp. 379–384.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] D. Wang and J. Huang, “Neural network-based adaptive dynamic surface
    control for a class of uncertain nonlinear systems in strict-feedback form,” *IEEE
    Transactions on Neural Networks*, vol. 16, no. 1, pp. 195–202, 2005.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. M. Polycarpou, “Stable adaptive neural control scheme for nonlinear
    systems,” *IEEE Transactions on Automatic Control*, vol. 41, no. 3, pp. 447–451,
    1996.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] R. Sanner and M. Mears, “Stable adaptive tracking of uncertainty systems
    using nonlinearly parameterized on-line approximators,” *IEEE Transactions on
    Neural Networks*, vol. 3, no. 6, pp. 837–863, 1992.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] D. Wang and J. Huang, “Adaptive neural network control for a class of
    uncertain nonlinear systems in pure-feedback form,” *Automatica*, vol. 38, no. 8,
    pp. 1365–1372, 2002.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] B. Ren, S. S. Ge, C.-Y. Su, and T. H. Lee, “Adaptive neural control for
    a class of uncertain nonlinear systems in pure-feedback form with hysteresis input,”
    *IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)*, vol. 39,
    no. 2, pp. 431–443, 2009.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] T. Zhang, S. S. Ge, and C. C. Hang, “Adaptive neural network control
    for strict-feedback nonlinear systems using backstepping design,” *Automatica*,
    vol. 36, no. 12, pp. 1835–1846, 2000.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] X. Dai, C.-K. Li, and A. B. Rad, “An approach to tune fuzzy controllers
    based on reinforcement learning for autonomous vehicle control,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 6, no. 3, pp. 285–293, 2005.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Z. Huang, X. Xu, H. He, J. Tan, and Z. Sun, “Parameterized Batch Reinforcement
    Learning for Longitudinal Control of Autonomous Land Vehicles,” *IEEE Transactions
    on Systems, Man, and Cybernetics: Systems*, pp. 1–12, 2017.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] X. Xu, D. Hu, and X. Lu, “Kernel-based least squares policy iteration
    for reinforcement learning,” *IEEE Transactions on Neural Networks*, vol. 18,
    no. 4, pp. 973–992, 2007.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] J. Wang, X. Xu, D. Liu, Z. Sun, and Q. Chen, “Self-learning cruise control
    using kernel-based least squares policy iteration,” *IEEE Transactions on Control
    Systems Technology*, vol. 22, no. 3, pp. 1078–1087, 2014.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] H. Chae, C. M. Kang, B. Kim, J. Kim, C. C. Chung, and J. W. Choi, “Autonomous
    braking system via deep reinforcement learning,” in *2017 IEEE 20th International
    Conference on Intelligent Transportation Systems (ITSC)*.   IEEE, 2017, pp. 1–6.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Euro NCAP, “European New Car Assessment Programme: Test Protocol - AEB
    VRU systems,” 2015.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] D. Zhao, Z. Xia, and Q. Zhang, “Model-free optimal control based intelligent
    cruise control with hardware-in-the-loop demonstration [research frontier],” *IEEE
    Computational Intelligence Magazine*, vol. 12, no. 2, pp. 56–69, 2017.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning:
    A survey,” *Journal of artificial intelligence research*, vol. 4, pp. 237–285,
    1996.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] D. Zhao, Z. Hu, Z. Xia, C. Alippi, Y. Zhu, and D. Wang, “Full-range adaptive
    cruise control based on supervised adaptive dynamic programming,” *Neurocomputing*,
    vol. 125, no. February, pp. 57–67, 2014.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] B. Wang, D. Zhao, C. Li, and Y. Dai, “Design and implementation of an
    adaptive cruise control system based on supervised actor-critic learning,” *2015
    5th International Conference on Information Science and Technology (ICIST)*, pp.
    243–248, 2015.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] T. L. Lai and H. Robbins, “Asymptotically efficient adaptive allocation
    rules,” *Advances in applied mathematics*, vol. 6, no. 1, pp. 4–22, 1985.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and
    R. Munos, “Unifying count-based exploration and intrinsic motivation,” in *Advances
    in Neural Information Processing Systems*, 2016, pp. 1471–1479.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] J. Schmidhuber, “A possibility for implementing curiosity and boredom
    in model-building neural controllers,” in *Proc. of the international conference
    on simulation of adaptive behavior: From animals to animats*, 1991, pp. 222–227.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] W. Xia, H. Li, and B. Li, “A control strategy of autonomous vehicles
    based on deep reinforcement learning,” in *Computational Intelligence and Design
    (ISCID), 2016 9th International Symposium on*, vol. 2.   IEEE, 2016, pp. 198–201.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M. Riedmiller, “Neural fitted q iteration–first experiences with a data
    efficient neural reinforcement learning method,” in *European Conference on Machine
    Learning*.   Springer, 2005, pp. 317–328.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “End-to-end deep reinforcement
    learning for lane keeping assist,” *arXiv preprint arXiv:1612.04340*, 2016.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] “The open racing car simulator.” [Online]. Available: [http://torcs.sourceforge.net/](http://torcs.sourceforge.net/)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *arXiv preprint arXiv:1509.02971*, 2015.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] J. Zhang and K. Cho, “Query-efficient imitation learning for end-to-end
    autonomous driving,” *arXiv preprint arXiv:1605.06450*, 2016.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and B. Boots,
    “Agile autonomous driving using end-to-end deep imitation learning,” *Proceedings
    of Robotics: Science and Systems. Pittsburgh, Pennsylvania*, 2018.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] N. Koenig and A. Howard, “Design and use paradigms for gazebo, an open-source
    multi-robot simulator,” in *2004 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)(IEEE Cat. No. 04CH37566)*, vol. 3.   IEEE, pp. 2149–2154.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] D. Wang, C. Devin, Q.-Z. Cai, F. Yu, and T. Darrell, “Deep object centric
    policies for autonomous driving,” *arXiv preprint arXiv:1811.05432*, 2018.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] H. Porav and P. Newman, “Imminent collision mitigation with reinforcement
    learning and vision,” in *2018 21st International Conference on Intelligent Transportation
    Systems (ITSC)*.   IEEE, 2018, pp. 958–964.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S. Zhifei and E. M. Joo, “A review of inverse reinforcement learning
    theory and recent advances,” *World Congress on Computational Intelligence*, pp.
    1–8, 2012.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement
    learning,” *Twenty-first international conference on Machine learning - ICML ’04*,
    p. 1, 2004.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, “Maximum margin planning,”
    in *Proceedings of the 23rd international conference on Machine learning - ICML
    ’06*, 2006, pp. 729–736.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] M. Wulfmeier, D. Rao, D. Z. Wang, P. Ondruska, and I. Posner, “Large-scale
    cost function learning for path planning using deep inverse reinforcement learning,”
    *International Journal of Robotics Research*, vol. 36, no. 10, pp. 1073–1087,
    2017.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey, “Maximum Entropy
    Inverse Reinforcement Learning.” *AAAI Conference on Artificial Intelligence*,
    pp. 1433–1438, 2008.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] S. Levine and V. Koltun, “Continuous Inverse Optimal Control with Locally
    Optimal Examples,” *International Conference on Machine Learning (ICML)*, pp.
    41–48, 2012.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] P. Abbeel, A. Coates, M. Quigley, and A. Y. Ng, “An application of reinforcement
    learning to aerobatic helicopter flight,” *Education*, vol. 19, p. 1, 2007.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] S. Hecker, D. Dai, and L. Van Gool, “End-to-end learning of driving models
    with surround-view cameras and route planners,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 435–453.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] F. Codevilla, M. Müller, A. López, V. Koltun, and A. Dosovitskiy, “End-to-end
    driving via conditional imitation learning,” in *2018 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2018, pp. 1–9.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA:
    An open urban driving simulator,” in *Proceedings of the 1st Annual Conference
    on Robot Learning*, 2017, pp. 1–16.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] F. Codevilla, E. Santana, A. M. López, and A. Gaidon, “Exploring the
    limitations of behavior cloning for autonomous driving,” *arXiv preprint arXiv:1904.08980*,
    2019.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] C. Paxton, V. Raman, G. D. Hager, and M. Kobilarov, “Combining neural
    networks and tree search for task and motion planning in challenging environments,”
    in *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2017, pp. 6059–6066.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] M. Bansal, A. Krizhevsky, and A. Ogale, “Chauffeurnet: Learning to drive
    by imitating the best and synthesizing the worst,” *arXiv preprint arXiv:1812.03079*,
    2018.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] X. Pan, Y. You, Z. Wang, and C. Lu, “Virtual to real reinforcement learning
    for autonomous driving,” *arXiv preprint arXiv:1704.03952*, 2017.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] M. Müller, A. Dosovitskiy, B. Ghanem, and V. Koltun, “Driving policy
    transfer via modularity and abstraction,” *arXiv preprint arXiv:1804.09364*, 2018.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] M. Maurer, J. C. Gerdes, B. Lenz, and H. Winner, *Autonomous Driving*.   Berlin:
    Springer, Heidelberg, 2016.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] European Commission, “Cooperative Intelligent Transportation Systems
    - Research Theme Analysis Report,” 2016\. [Online]. Available: [http://www.transport-research.info/sites/default/files/TRIP_C-ITS_Report.pdf](http://www.transport-research.info/sites/default/files/TRIP_C-ITS_Report.pdf)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] S. A. Bagloee, M. Tavana, M. Asadi, and T. Oliver, “Autonomous vehicles:
    challenges, opportunities, and future implications for transportation policies,”
    *Journal of Modern Transportation*, vol. 24, no. 4, pp. 284–303, 2016.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] HERE Technologies, “Consumer Acceptance of Autonomous Vehicles,” 2017.
    [Online]. Available: [https://here.com/file/13726/download?token=njs4ZwfW](https://here.com/file/13726/download?token=njs4ZwfW)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] L. Bosankic, “How consumers’ perception of autonomous cars will influence
    their adoption,” 2017\. [Online]. Available: [https://medium.com/@leo_pold_b/how-consumers-perception-of-autonomous-cars-will-influence-their-adoption-ba99e3f64e9a](https://medium.com/@leo_pold_b/how-consumers-perception-of-autonomous-cars-will-influence-their-adoption-ba99e3f64e9a)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] H. Abraham, B. Reimer, B. Seppelt, C. Fitzgerald, B. Mehler, and J. F.
    Coughlin, “Consumer Interest in Automation: Preliminary Observations Exploring
    a Year’s Change,” 2017\. [Online]. Available: [http://agelab.mit.edu/sites/default/files/MIT-NEMPAWhitePaperFINAL.pdf](http://agelab.mit.edu/sites/default/files/MIT-NEMPAWhitePaperFINAL.pdf)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] W. Knight, “An Ambitious Plan to Build a Self-Driving Borg,” 2016. [Online].
    Available: [https://www.technologyreview.com/s/602531/an-ambitious-plan-to-build-a-self-driving-borg/](https://www.technologyreview.com/s/602531/an-ambitious-plan-to-build-a-self-driving-borg/)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Kober, Jens J., Bagnell, Andrew, Peters, Jan, “Reinforcement Learning
    in Robotics: A Survey,” *International Journal of Robotics Research*, vol. 32,
    no. 11, pp. 1238–1274, 2013.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] R. Bellman, “Dynamic Programming,” *Science*, vol. 153, no. 3731, pp.
    34–37, 1966.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep q-learning
    with model-based acceleration,” in *International Conference on Machine Learning*,
    2016, pp. 2829–2838.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] J. T. Barron, D. S. Golland, and N. J. Hay, “Parallelizing reinforcement
    learning,” *UC Berkeley*, 2009.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, “Evolution strategies
    as a scalable alternative to reinforcement learning,” *arXiv preprint arXiv:1703.03864*,
    2017.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] S. Yang, W. Wang, C. Liu, W. Deng, and J. K. Hedrick, “Feature analysis
    and selection for training an end-to-end autonomous vehicle controller using deep
    learning approach,” in *2017 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE,
    2017, pp. 1033–1038.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Y. LeCun, “Generalization and network design strategies,” *Connectionism
    in perspective*, pp. 143–155, 1989.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] N. Morgan and H. Bourlard, “Generalization and Parameter Estimation in
    Feedforward Nets: Some Experiments,” *Advances in neural information processing
    systems*, pp. 630–637, 1989.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Y. Bengio, “Practical recommendations for gradient-based training of
    deep architectures,” in *Neural networks: Tricks of the trade*.   Springer, 2012,
    pp. 437–478.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller, “Efficient backprop,”
    in *Neural networks: Tricks of the trade*.   Springer, 1998, pp. 9–50.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] J. Bergstra and Y. Bengio, “Random Search for Hyper-Parameter Optimization,”
    *Journal of Machine Learning Research*, vol. 13, pp. 281–305, 2012.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] C. Raffel, “Neural Network Hyperparameters,” 2015\. [Online]. Available:
    [http://colinraffel.com/wiki/neural_network_hyperparameters](http://colinraffel.com/wiki/neural_network_hyperparameters)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Y. Sevchuk, “Hyperparameter optimization for Neural Networks,” 2016.
    [Online]. Available: [http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] J. Snoek, H. Larochelle, and R. P. Adams, “Practical Bayesian Optimization
    of Machine Learning Algorithms,” *Advances in Neural Information Processing Systems*,
    vol. 25, pp. 2960–2968, 2012.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] C. E. Rasmussen, “Gaussian processes in machine learning,” in *Advanced
    lectures on machine learning*.   Springer, 2004, pp. 63–71.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl, “Algorithms for Hyper-Parameter
    Optimization,” in *Advances in Neural Information Processing Systems (NIPS)*,
    2011, pp. 2546–2554.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] M. Kumar, G. E. Dahl, V. Vasudevan, and M. Norouzi, “Parallel architecture
    and hyperparameter search via successive halving and classification,” *arXiv preprint
    arXiv:1805.10255*, 2018.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] T. B. Hashimoto, S. Yadlowsky, and J. C. Duchi, “Derivative free optimization
    via repeated classification,” *arXiv preprint arXiv:1804.03761*, 2018.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] H. Cai, C. Gan, and S. Han, “Once for all: Train one network and specialize
    it for efficient deployment,” *arXiv preprint arXiv:1908.09791*, 2019.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V.
    Le, “Mnasnet: Platform-aware neural architecture search for mobile,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2019, pp.
    2820–2828.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] B. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, Y. Jia,
    and K. Keutzer, “Fbnet: Hardware-aware efficient convnet design via differentiable
    neural architecture search,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 10 734–10 742.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] F. Scheidegger, L. Benini, C. Bekas, and C. Malossi, “Constrained deep
    neural network architecture search for iot devices accounting hardware calibration,”
    *arXiv preprint arXiv:1909.10818*, 2019.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search:
    A survey,” *Journal of Machine Learning Research*, vol. 20, no. 55, pp. 1–21,
    2019.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward
    transformations : Theory and application to reward shaping,” *Sixteenth International
    Conference on Machine Learning*, vol. 3, pp. 278–287, 1999.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] A. D. Laud, “Theory and Application of Reward Shaping in Reinforcement
    Learning,” Ph.D. dissertation, University of Illinois, 2004.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] H. Van Seijen, M. Fatemi, J. Romoff, R. Laroche, T. Barnes, and J. Tsang,
    “Hybrid reward architecture for reinforcement learning,” in *Advances in Neural
    Information Processing Systems*, 2017, pp. 5392–5402.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement
    learning for autonomous driving,” *arXiv preprint arXiv:1610.03295*, 2016.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mané,
    “Concrete problems in ai safety,” *arXiv preprint arXiv:1606.06565*, 2016.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] S. Russell, “Learning agents for uncertain environments (extended abstract),”
    *Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT)*,
    pp. 101–103, 1998.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] J. Z. Kolter, P. Abbeel, and A. Y. Ng, “Hierarchical Apprenticeship Learning,
    with Application to Quadruped Locomotion,” *Science*, vol. 1, pp. 1–8, 2008.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] D. Silver, J. A. Bagnell, and A. Stentz, “Learning from demonstration
    for autonomous navigation in complex unstructured terrain,” in *International
    Journal of Robotics Research*, vol. 29, no. 12, 2010, pp. 1565–1592.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] N. Ratliff, J. A. Bagnell, and S. S. Srinivasa, “Imitation learning for
    locomotion and manipulation,” in *Proceedings of the 2007 7th IEEE-RAS International
    Conference on Humanoid Robots, HUMANOIDS 2007*, 2008, pp. 392–397.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] R. J. Schalkoff, *Artificial Neural Networks*.   New York: McGraw-Hill,
    1997.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] B. D. Ripley, *Pattern Recognition in Neural Networks*.   Cambridge:
    Cambridge University Press, 1996.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] C. M. Bishop, “Neural networks for pattern recognition,” *Journal of
    the American Statistical Association*, vol. 92, p. 482, 1995.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] A. Y. Ng, “Feature selection, l 1 vs. l 2 regularization, and rotational
    invariance,” in *Proceedings of the twenty-first international conference on Machine
    learning*.   ACM, 2004, p. 78.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] P. Merolla, R. Appuswamy, J. Arthur, S. K. Esser, and D. Modha, “Deep
    neural networks are robust to weight binarization and other non-linear distortions,”
    *arXiv preprint arXiv:1606.01981*, 2016.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] M. Courbariaux, Y. Bengio, and J.-P. David, “Binaryconnect: Training
    deep neural networks with binary weights during propagations,” in *Advances in
    neural information processing systems*, 2015, pp. 3123–3131.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov,
    “Improving neural networks by preventing co-adaptation of feature detectors,”
    *arXiv preprint arXiv:1207.0580*, 2012.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” *Journal
    of Machine Learning Research*, vol. 15, pp. 1929–1958, 2014.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] H. Raza and P. Ioannou, “Vehicle following control design for automated
    highway systems,” *IEEE Control Systems Magazine*, vol. 16, no. 6, pp. 43–60,
    1996.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] R. Rajamani, H. S. Tan, B. K. Law, and W. B. Zhang, “Demonstration of
    integrated longitudinal and lateral control for the operation of automated vehicles
    in platoons,” *IEEE Transactions on Control Systems Technology*, vol. 8, no. 4,
    pp. 695–708, 2000.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] C. Thorpe, T. Jochem, and D. Pomerleau, “The 1997 automated highway free
    agent demonstration,” in *Intelligent Transportation System, 1997\. ITSC’97.,
    IEEE Conference on*.   IEEE, 1997, pp. 496–501.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] S. Kato, S. Tsugawa, K. Tokuda, T. Matsui, and H. Fujii, “Vehicle Control
    Algorithms for Cooperative Driving with Automated Vehicles and Intervehicle Communications,”
    *IEEE Transactions on Intelligent Transportation Systems*, vol. 3, no. 3, pp.
    155–160, 2002.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] L. Ng, C. M. Clark, and J. P. Huissoon, “Reinforcement learning of adaptive
    longitudinal vehicle control for dynamic collaborative driving,” in *IEEE Intelligent
    Vehicles Symposium, Proceedings*, 2008, pp. 907–912.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] C. G. Atkeson, “Using Local Trajectory Optimizers To Speed Up Global
    Optimization In Dynamic Programming,” *Advances in Neural Information Processing
    Systems (NIPS),*, pp. 663–670, 1994.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, A. Sendonaris,
    G. Dulac-Arnold, I. Osband, J. Agapiou, J. Z. Leibo, and A. Gruslys, “Learning
    from demonstrations for real world reinforcement learning,” *arXiv preprint arXiv:1704.03732*,
    2017.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] P. Christiano, Z. Shah, I. Mordatch, J. Schneider, T. Blackwell, J. Tobin,
    P. Abbeel, and W. Zaremba, “Transfer from simulation to real world through learning
    deep inverse dynamics model,” *arXiv preprint arXiv:1610.03518*, 2016.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell,
    “Sim-to-real robot learning from pixels with progressive nets,” *arXiv preprint
    arXiv:1610.04286*, 2016.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] E. Tzeng, C. Devin, J. Hoffman, C. Finn, X. Peng, S. Levine, K. Saenko,
    and T. Darrell, “Towards adapting deep visuomotor representations from simulated
    to real environments,” *CoRR, abs/1511.07111*, 2015.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
    randomization for transferring deep neural networks from simulation to the real
    world,” in *Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International
    Conference on*.   IEEE, 2017, pp. 23–30.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] K. R. Varshney and H. Alemzadeh, “On the safety of machine learning:
    Cyber-physical systems, decision sciences, and data products,” *Big data*, vol. 5,
    no. 3, pp. 246–255, 2017.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] D. Castelvecchi, “Can we open the black box of ai?” *Nature News*, vol.
    538, no. 7623, p. 20, 2016.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] X. Zhang, M. Clark, K. Rattan, and J. Muse, “Controller verification
    in adaptive learning systems towards trusted autonomy,” in *Proceedings of the
    ACM/IEEE Sixth International Conference on Cyber-Physical Systems*.   ACM, 2015,
    pp. 31–40.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] M. Clark, X. Koutsoukos, J. Porter, R. Kumar, G. Pappas, O. Sokolsky,
    I. Lee, and L. Pike, “A study on run time assurance for complex cyber physical
    systems,” AIR FORCE RESEARCH LAB WRIGHT-PATTERSON AFB OH AEROSPACE SYSTEMS DIR,
    Tech. Rep., 2013.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] S. Jacklin, J. Schumann, P. Gupta, M. Richard, K. Guenther, and F. Soares,
    “Development of advanced verification and validation procedures and tools for
    the certification of learning systems in aerospace applications,” in *Infotech@
    Aerospace*, 2005, p. 6912.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] C. Wilkinson, J. Lynch, and R. Bharadwaj, *Final Report, Regulatory Considerations
    for Adaptive Systems*.   National Aeronautics and Space Administration, Langley
    Research Center, 2013.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] P. Van Wesel and A. E. Goodloe, “Challenges in the verification of reinforcement
    learning algorithms,” Technical report, NASA, Tech. Rep., 2017.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] J. G. Schneider, “Exploiting model uncertainty estimates for safe dynamic
    control learning,” in *Advances in neural information processing systems*, 1997,
    pp. 1047–1053.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] J. A. Bagnell, “Learning decisions: Robustness, uncertainty, and appoximation,”
    *Robotics Institute*, p. 78, 2004.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] M. Deisenroth and C. E. Rasmussen, “Pilco: A model-based and data-efficient
    approach to policy search,” in *Proceedings of the 28th International Conference
    on machine learning (ICML-11)*, 2011, pp. 465–472.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] T. M. Moldovan and P. Abbeel, “Safe exploration in markov decision processes,”
    *arXiv preprint arXiv:1205.4810*, 2012.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] X. Xiong, J. Wang, F. Zhang, and K. Li, “Combining deep reinforcement
    learning and safety based control for autonomous driving,” *arXiv preprint arXiv:1612.00147*,
    2016.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] S. Glaser, B. Vanholme, S. Mammar, D. Gruyer, and L. Nouveliere, “Maneuver-based
    trajectory planning for highly autonomous vehicles on real road with traffic and
    driver interaction,” *IEEE Transactions on Intelligent Transportation Systems*,
    vol. 11, no. 3, pp. 589–606, 2010.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” *arXiv preprint arXiv:1312.6199*,
    2013.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] A. Nguyen, J. Yosinski, and J. Clune, “Deep Neural Networks are Easily
    Fooled,” *Computer Vision and Pattern Recognition, 2015 IEEE Conference on*, pp.
    427–436, 2015.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] S. M. Moosavi Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
    and accurate method to fool deep neural networks,” in *Proceedings of 2016 IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, no. EPFL-CONF-218057,
    2016.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
    adversarial examples,” *arXiv preprint arXiv:1412.6572*, 2014.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, “Safety verification of
    deep neural networks,” in *Lecture Notes in Computer Science (including subseries
    Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)*,
    vol. 10426 LNCS, 2017, pp. 3–29.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the
    physical world,” *arXiv preprint arXiv:1607.02533*, 2016.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing Robust
    Adversarial Examples,” *arXiv preprint arXiv:1707.07397*, 2017.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] X. Yuan, P. He, Q. Zhu, and X. Li, “Adversarial examples: Attacks and
    defenses for deep learning,” *IEEE transactions on neural networks and learning
    systems*, 2019.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] R. Salay, R. Queiroz, and K. Czarnecki, “An analysis of iso 26262: Using
    machine learning safely in automotive software,” *arXiv preprint arXiv:1709.02435*,
    2017.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] International Organization for Standardization, “Iso 26262: Road vehicles-functional
    safety,” *International Standard ISO/FDIS*, 2011.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] F. Falcini, G. Lami, and A. M. Costanza, “Deep learning in automotive
    software,” *IEEE Software*, vol. 34, no. 3, pp. 56–63, 2017.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] S. Dixit, U. Montanaro, S. Fallah, M. Dianati, D. Oxtoby, T. Mizutani,
    and A. Mouzakitis, “Trajectory planning for autonomous high-speed overtaking using
    mpc with terminal set constraints,” in *2018 21st International Conference on
    Intelligent Transportation Systems (ITSC)*.   IEEE, 2018, pp. 1061–1068.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] S. Dixit, U. Montanaro, M. Dianati, D. Oxtoby, T. Mizutani, A. Mouzakitis,
    and S. Fallah, “Trajectory planning for autonomous high-speed overtaking in structured
    environments using robust mpc,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] K. Amezquita-Semprun, Y. C. Pradeep, P. C. Chen, W. Chen, and Z. Zhao,
    “Experimental evaluation of the stimuli-induced equilibrium point concept for
    automatic ramp merging systems,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2019.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] V. Milanés, J. Godoy, J. Villagrá, and J. Pérez, “Automated on-ramp merging
    system for congested traffic situations,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 12, no. 2, pp. 500–508, 2010.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/d9eee5377d105e87f65ed1efb9ff576b.png) | Sampo
    Kuutti received the MEng degree in mechanical engineering in 2017 from University
    of Surrey, Guildford, U.K., where he is currently pursuing the PhD degree in automotive
    engineering with the Connected Autonomous Vehicles Lab within the Centre for Automotive
    Engineering. His research interests include deep learning applied to autonomous
    vehicles, functional safety validation, and safety and interpretability in machine
    learning systems. |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/6732cb864b82ddcf9e672a08ccdef89e.png) | Richard
    Bowden is Professor of computer vision and machine learning at the University
    of Surrey where he leads the Cognitive Vision Group within the Centre for Vision,
    Speech and Signal Processing. His research centres on the use of computer vision
    to locate, track, and understand humans. He is an associate editor for the journals
    Image and Vision computing and IEEE TPAMI. In 2013 he was awarded a Royal Society
    Leverhulme Trust Senior Research Fellowship and is a fellow of the Higher Education
    Academy, a senior member of the IEEE and Fellow of the International Association
    of Pattern Recognition (IAPR). |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/7fc5185fa13474afc9f92cf405886b46.png) | Yaochu
    Jin is a Professor in Computational Intelligence, Department of Computer Science,
    University of Surrey, Guildford, U.K. His main research interests include data-driven
    surrogate-assisted evolutionary optimization, evolutionary learning, interpretable
    and secure machine learning, and evolutionary developmental systems. Dr Jin is
    the Editor-in-Chief of the IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS
    and Co-Editor-in-Chief of Complex & Intelligent Systems. He is an IEEE Distinguished
    Lecturer and IEEE Fellow. |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/68cc81afddad1ae2f509bbe2fa111a76.png) | Phil Barber
    was formerly Principal Technical Specialist in Capability Research at Jaguar Land
    Rover. For over 30 years in the automotive industry he has witnessed the introduction
    of computer controlled by-wire technology and been part of the debate over the
    safety issues involved in the implementation of real-time vehicle control. |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/ed74bd8a24f53e483322255dee869290.png) | Saber
    Fallah is Senior Lecturer (Associate Professor) in Vehicle and Mechatronic Systems
    at the University of Surrey and the Director of Connected Autonomous Vehicle Lab
    within the Centre for Automotive Engineering, where he leads several research
    activities funded by the UK and European governments (e.g. EPSRC, Innovate UK,
    H2020) in collaboration with major companies active in autonomous vehicle technologies.
    His research interests include reinforced deep learning, advanced control, optimisation
    and estimation and their applications to connected autonomous vehicles. |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
