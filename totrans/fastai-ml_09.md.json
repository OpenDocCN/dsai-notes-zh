["```py\n**from** **fastai.metrics** **import** *\n**from** **fastai.model** **import** *\n**from** **fastai.dataset** **import** *\n\n**import** **torch.nn** **as** **nn**net = nn.Sequential(\n    nn.Linear(28*28, 10),\n    nn.LogSoftmax()\n).cuda()\n```", "```py\nloss=nn.NLLLoss()\nmetrics=[accuracy]\n*opt=optim.Adam(net.parameters())*fit(net, md, n_epochs=5, crit=loss, opt=opt, metrics=metrics)\n```", "```py\n**def** get_weights(*dims): \n    **return** nn.Parameter(torch.randn(dims)/dims[0])\n**def** softmax(x): \n    **return** torch.exp(x)/(torch.exp(x).sum(dim=1)[:,**None**])\n\n**class** **LogReg**(nn.Module):\n    **def** __init__(self):\n        super().__init__()\n        self.l1_w = get_weights(28*28, 10)  *# Layer 1 weights*\n        self.l1_b = get_weights(10)         *# Layer 1 bias*\n\n    **def** forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = (x @ self.l1_w) + self.l1_b  *# Linear Layer*\n        x = torch.log(softmax(x)) *# Non-linear (LogSoftmax) Layer*\n        **return** x\n```", "```py\nnet2 = LogReg().cuda()\nopt=optim.Adam(net2.parameters())\n```", "```py\nfit(net2, md, n_epochs=1, crit=loss, opt=opt, metrics=metrics)\n```", "```py\ndl = iter(md.trn_dl)\n```", "```py\nxmb,ymb = next(dl)\n```", "```py\nxmb*-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n          ...             \u22f1             ...          \n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n[torch.FloatTensor of size 64x784 (GPU 0)]*\n```", "```py\nvxmb = Variable(xmb.cuda())\nvxmb*Variable containing:\n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n          ...             \u22f1             ...          \n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n-0.4245 -0.4245 -0.4245  ...  -0.4245 -0.4245 -0.4245\n[torch.cuda.FloatTensor of size 64x784 (GPU 0)]*\n```", "```py\n preds = net2(vxmb).exp(); preds[:3]Variable containing:\n\nColumns 0 to 5 \n 1.6740e-03  1.0416e-05  2.5454e-05  1.9119e-02  6.5026e-05  9.7470e-01\n 3.4048e-02  1.8530e-04  6.6637e-01  3.5073e-02  1.5283e-01  6.4995e-05\n 3.0505e-08  4.3947e-08  1.0115e-05  2.0978e-04  9.9374e-01  6.3731e-05\n\nColumns 6 to 9 \n 2.1126e-06  1.7638e-04  3.9351e-03  2.9154e-04\n 1.1891e-03  3.2172e-02  1.4597e-02  6.3474e-02\n 8.9568e-06  9.7507e-06  7.8676e-04  5.1684e-03\n[torch.cuda.FloatTensor of size 3x10 (GPU 0)]\n```", "```py\npreds = predict(net2, md.val_dl).argmax(1)\nplots(x_imgs[:8], titles=preds[:8])\n```", "```py\na = np.array([10, 6, -4])\nb = np.array([2, 8, 7]) a + b*array([12, 14,  3])*\n```", "```py\n(a < b).mean()*0.66666666666666663*\n```", "```py\na*array([10,  6, -4])*\n```", "```py\na > 0*array([ True,  True, False], dtype=bool)*\n```", "```py\na + 1*array([11,  7, -3])*\n```", "```py\nm = np.array([[1, 2, 3], [4,5,6], [7,8,9]]); marray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n```", "```py\n2*marray([[ 2,  4,  6],\n       [ 8, 10, 12],\n       [14, 16, 18]])\n```", "```py\nc = np.array([10,20,30]); c*array([10, 20, 30])*\n```", "```py\nm + c*array([[11, 22, 33],\n       [14, 25, 36],\n       [17, 28, 39]])*\n```", "```py\nnp.expand_dims(c,1).shape*(3, 1)*np.expand_dims(c,1)*array([[10],\n       [20],\n       [30]])*\n```", "```py\nm + np.expand_dims(c,1)*array([[11, 12, 13],\n       [24, 25, 26],\n       [37, 38, 39]])*\n```", "```py\nc[**None**]*array([[10, 20, 30]])*c[**None**].shape*(1, 3)*\n```", "```py\nc[:,**None**]*array([[10],\n       [20],\n       [30]])*c[:,**None**].shape*(3, 1)*\n```", "```py\nc[**None**,:,**None**].shape*(1, 3, 1)*\n```", "```py\nnp.broadcast_to(c, (3,3))*array([[10, 20, 30],\n       [10, 20, 30],\n       [10, 20, 30]])*\n```", "```py\nnp.broadcast_to(c, m.shape)*array([[10, 20, 30],\n       [10, 20, 30],\n       [10, 20, 30]])*\n```", "```py\nnp.broadcast_to(c[:,**None**], m.shape)*array([[10, 10, 10],\n       [20, 20, 20],\n       [30, 30, 30]])*\n```", "```py\nImage  (3d array): 256 x 256 x 3\nScale  (1d array):             3\nResult (3d array): 256 x 256 x 3\n```", "```py\nm, c*(array([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]]), array([10, 20, 30]))*\n```", "```py\nm @ c  *# np.matmul(m, c)**array([140, 320, 500])*\n```", "```py\nT(m) @ T(c)*140\n 320\n 500\n[torch.LongTensor of size 3]*\n```", "```py\nm * c*array([[ 10,  40,  90],\n       [ 40, 100, 180],\n       [ 70, 160, 270]])*\n```", "```py\n(m * c).sum(axis=1)*array([140, 320, 500])*\n```", "```py\n(m * n[:,0]).sum(axis=1)*array([140, 320, 500])*(m * n[:,1]).sum(axis=1)*array([ 25, 130, 235])*\n```", "```py\n**class** **LogReg**(nn.Module):\n    **def** __init__(self):\n        super().__init__()\n        self.l1_w = get_weights(28*28, 10)  *# Layer 1 weights*\n        self.l1_b = get_weights(10)         *# Layer 1 bias*\n\n    **def** forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = x @ self.l1_w + self.l1_b \n        **return** torch.log(softmax(x))\n```", "```py\nnet2 = LogReg().cuda()\nopt=optim.Adam(net2.parameters())\n\nfit(net2, md, n_epochs=1, crit=loss, opt=opt, metrics=metrics)[ 0\\.       0.31102  0.28004  0.92406]\n```", "```py\ndl = iter(md.trn_dl) *# Data loader*\n```", "```py\nxt, yt = next(dl)\ny_pred = net2(Variable(xt).cuda())\n```", "```py\nl = loss(y_pred, Variable(yt).cuda())\nprint(l)Variable containing:\n 2.4352\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\n```", "```py\n**for** t **in** range(100):\n  xt, yt = next(dl)\n  y_pred = net2(Variable(xt).cuda())\n  l = loss(y_pred, Variable(yt).cuda())\n\n  **if** t % 10 == 0:\n    accuracy = np.mean(to_np(y_pred).argmax(axis=1) == to_np(yt))\n    print(\"loss: \", l.data[0], \"**\\t** accuracy: \", accuracy)\n\n  optimizer.zero_grad()\n  l.backward()\n  optimizer.step()*loss:  2.2104923725128174 \t accuracy:  0.234375\nloss:  1.3094730377197266 \t accuracy:  0.625\nloss:  1.0296542644500732 \t accuracy:  0.78125\nloss:  0.8841525316238403 \t accuracy:  0.71875\nloss:  0.6643403768539429 \t accuracy:  0.8125\nloss:  0.5525785088539124 \t accuracy:  0.875\nloss:  0.43296846747398376 \t accuracy:  0.890625\nloss:  0.4388267695903778 \t accuracy:  0.90625\nloss:  0.39874207973480225 \t accuracy:  0.890625\nloss:  0.4848807752132416 \t accuracy:  0.875*\n```"]