- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:06:28'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1903.02026] Deep Learning in Medical Image Registration: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1903.02026](https://ar5iv.labs.arxiv.org/html/1903.02026)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '∎ ¹¹institutetext: G. Haskins, U. Kruger, P. Yan* ²²institutetext: Department
    of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180, USA'
  prefs: []
  type: TYPE_NORMAL
- en: Asterisk indicates corresponding author
  prefs: []
  type: TYPE_NORMAL
- en: 'Tel.: +1-518-276-4476'
  prefs: []
  type: TYPE_NORMAL
- en: '²²email: yanp2@rpi.edu'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning in Medical Image Registration: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Grant Haskins    Uwe Kruger    Pingkun Yan This work was partially supported
    by NIH/NIBIB under awards R21EB028001 and R01EB027898, and NIH/NCI under a Bench-to-Bedside
    award.This is a pre-print of an article published in Machine Vision and Applications.
    The final authenticated version is available online at: https://doi.org/10.1007/s00138-020-01060-x'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The establishment of image correspondence through robust image registration
    is critical to many clinical tasks such as image fusion, organ atlas creation,
    and tumor growth monitoring, and is a very challenging problem. Since the beginning
    of the recent deep learning renaissance, the medical imaging research community
    has developed deep learning based approaches and achieved the state-of-the-art
    in many applications, including image registration. The rapid adoption of deep
    learning for image registration applications over the past few years necessitates
    a comprehensive summary and outlook, which is the main scope of this survey. This
    requires placing a focus on the different research areas as well as highlighting
    challenges that practitioners face. This survey, therefore, outlines the evolution
    of deep learning based medical image registration in the context of both research
    challenges and relevant innovations in the past few years. Further, this survey
    highlights future research directions to show how this field may be possibly moved
    forward to the next level.
  prefs: []
  type: TYPE_NORMAL
- en: 1 INTRODUCTION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Image registration is the process of transforming different image datasets
    into one coordinate system with matched imaging contents, which has significant
    applications in medicine. Registration may be necessary when analyzing a pair
    of images that were acquired from different viewpoints, at different times, or
    using different sensors/modalities Hill et al. ([2001](#bib.bib41)); Zitova and
    Flusser ([2003](#bib.bib122)). Until recently, image registration was mostly performed
    manually by clinicians. However, many registration tasks can be quite challenging
    and the quality of manual alignments are highly dependent upon the expertise of
    the user, which can be clinically disadvantageous. To address the potential shortcomings
    of manual registration, automatic registration has been developed. Although other
    methods for automatic image registration have been extensively explored prior
    to (and during) the deep learning renaissance, deep learning has changed the landscape
    of image registration research Ambinder ([2005](#bib.bib4)). Ever since the success
    of AlexNet in the ImageNet challenge of 2012 Alom et al. ([2018](#bib.bib3)),
    deep learning has allowed for state-of-the-art performance in many computer vision
    tasks including, but not limited to: object detection Ren et al. ([2015](#bib.bib84)),
    feature extraction He et al. ([2016](#bib.bib37)), segmentation Ronneberger et al.
    ([2015](#bib.bib87)), image classification Alom et al. ([2018](#bib.bib3)), image
    denoising Yang et al. ([2018](#bib.bib112)), and image reconstruction Yao et al.
    ([2018](#bib.bib115)).'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, deep learning was successfully used to augment the performance of
    iterative, intensity based registration Cheng et al. ([2018](#bib.bib18)); Haskins
    et al. ([2019](#bib.bib36)); Simonovsky et al. ([2016](#bib.bib96)). Soon after
    this initial application, several groups investigated the intuitive application
    of reinforcement learning to registration Liao et al. ([2017](#bib.bib62)); Ma
    et al. ([2017](#bib.bib71)); Miao et al. ([2017](#bib.bib76)). Further, demand
    for faster registration methods later motivated the development of deep learning
    based one-step transformation estimation techniques and challenges associated
    with procuring/generating ground truth data have recently motivated many groups
    to develop unsupervised frameworks for one-step transformation estimation de Vos
    et al. ([2018](#bib.bib23)); Li and Fan ([2018](#bib.bib61)). One of the hurdles
    associated with this framework is the familiar challenge of image similarity quantification
    Heinrich et al. ([2012](#bib.bib38)); Viola and Wells III ([1997](#bib.bib106)).
    Recent efforts that use information theory based similarity metrics de Vos et al.
    ([2018](#bib.bib23)), segmentations of anatomical structures Hu et al. ([2018c](#bib.bib44)),
    and generative adversarial network like frameworks Fan et al. ([2018a](#bib.bib30))
    to address this challenge have shown promising results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 INTRODUCTION ‣ Deep Learning in Medical Image
    Registration: A Survey") shows the various categorizations of different deep learning
    based registration methods. On the other hand, Figure [2](#S1.F2 "Figure 2 ‣ 1
    INTRODUCTION ‣ Deep Learning in Medical Image Registration: A Survey") shows the
    observed growing interest in deep learning based registration methods according
    to the number of published papers in recent years. As the trends visualized in
    Figures [1](#S1.F1 "Figure 1 ‣ 1 INTRODUCTION ‣ Deep Learning in Medical Image
    Registration: A Survey") and [2](#S1.F2 "Figure 2 ‣ 1 INTRODUCTION ‣ Deep Learning
    in Medical Image Registration: A Survey") suggest, this field is moving very quickly
    to surmount the hurdles associated with deep learning based medical image registration
    and several groups have already enjoyed significant successes for their applications
    Hu et al. ([2018c](#bib.bib44)); Liu et al. ([2018](#bib.bib65)); Simonovsky et al.
    ([2016](#bib.bib96)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3d1baf673cb5d7443a073fe632ddc851.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of deep learning based medical image registration broken
    down by approach type. The popular research directions are written in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the purpose of this article is to comprehensively survey the field
    of deep learning based medical image registration, highlight common challenges
    that practitioners face, and discuss future research directions that may address
    these challenges. Deep learning belongs to a class of machine learning that uses
    neural networks with a large number of layers to learn representations of data
    Goodfellow et al. ([2016](#bib.bib34)); Schmidhuber ([2015](#bib.bib91)). When
    discussing neural networks it is important to provide insight into the different
    types of neural networks that can be used for various applications, the notable
    architectures that were recently invented to tackle engineering problems, and
    the variety of strategies that are used for training neural networks. Therefore,
    this deep learning introduction section is divided into three sections: Neural
    Network Types, Network Architectures, and Training Paradigms and Strategies. Note
    that there are many publicly available libraries that can be used to build the
    networks described in the section, for example TensorFlow Abadi et al. ([2016](#bib.bib1)),
    MXNet Chen et al. ([2015](#bib.bib16)), Keras Chollet et al. ([2015](#bib.bib20)),
    Caffe Jia et al. ([2014](#bib.bib49)), and PyTorch Paszke et al. ([2017](#bib.bib82)).
    Detailed discussion of deep learning based medical image analysis and various
    deep learning research directions is outside of the scope of this article. Comprehensive
    review articles that survey the application of deep learning to medical image
    analysis Lee et al. ([2017](#bib.bib59)); Litjens et al. ([2017](#bib.bib63)),
    reinforcement learning Kaelbling et al. ([1996](#bib.bib51)), and the application
    of GANs to medical image analysis Kazeminia et al. ([2018](#bib.bib52)) are recommended
    to the interested readers. In this article, the surveyed methods were divided
    into the following three categories: Deep Iterative Registration, Supervised Transformation
    Estimation, and Unsupervised Transformation Estimation. Following a discussion
    of the methods that belong to each of the aforementioned categories, future research
    directions and current trends are discussed in Section [5](#S5 "5 Research Trends
    and Future Directions ‣ Deep Learning in Medical Image Registration: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d96f7823b3483e2ae933548ac012638f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An overview of the number of deep learning based image registration
    works and deep learning based medical imaging works. The red line represents the
    trend line for medical imaging based approaches and the blue line represents the
    trend line for deep learning based medical image registration approaches. The
    dotted line represents extrapolation.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep Iterative Registration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 1: Deep Iterative Registration Methods Overview. RL denotes reinforcment
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Ref Learning Transform Modality ROI Model Eppenhof and Pluim ([2018b](#bib.bib29))
    Metric Deformable CT Thorax 9-layer CNN Blendowski and Heinrich ([2018](#bib.bib10))
    Metric Deformable CT Lung FCN Simonovsky et al. ([2016](#bib.bib96)) Metric Deformable
    MR Brain 5-layer CNN Wu et al. ([2013](#bib.bib109)) Metric Deformable MR Brain
    2-layer CAE Cheng et al. ([2018](#bib.bib18)) Metric Deformable CT/MR Head 5-layer
    DNN Sedghi et al. ([2018](#bib.bib92)) Metric Rigid MR/US Abdominal 5-layer CNN
    Haskins et al. ([2019](#bib.bib36)) Metric Rigid MR/US Prostate 14-layer CNN Matthew
    et al. ([2018](#bib.bib75)) Metric Rigid MR/US Fetal Brain LSTM/STN Krebs et al.
    ([2017](#bib.bib55)) RL Agent Deformable MR Prostate 8-layer CNN Liao et al. ([2017](#bib.bib62))
    RL Agent Rigid CT/CBCT Spine/ 8-layer CNN Cardiac Miao et al. ([2017](#bib.bib76))
    Multiple Rigid X-ray/CT Spine Dilated FCN RL Agents Ma et al. ([2017](#bib.bib71))
    RL Agent Rigid MR/CT Spine Dueling Network
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic intensity-based image registration requires both a metric that quantifies
    the similarity between a moving image and a fixed image and an optimization algorithm
    that updates the transformation parameters such that the similarity between the
    images is maximized. Prior to the deep learning renaissance, several manually
    crafted metrics were frequently used for such registration applications, including:
    sum of squared differences (SSD), cross-correlation (CC), mutual information (MI)
    Maes et al. ([1997](#bib.bib72)); Viola and Wells III ([1997](#bib.bib106)), normalized
    cross correlation (NCC), and normalized mutual information (NMI). Early applications
    of deep learning to medical image registration are direct extensions of the intensity-based
    registration framework Simonovsky et al. ([2016](#bib.bib96)); Wu et al. ([2013](#bib.bib109),
    [2016](#bib.bib110)). Several groups later used a reinforcement learning paradigm
    to iteratively estimate a transformation Krebs et al. ([2017](#bib.bib55)); Liao
    et al. ([2017](#bib.bib62)); Ma et al. ([2017](#bib.bib71)); Miao et al. ([2017](#bib.bib76))
    because this application is more consistent with how practitioners perform registration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A description of both types of methods is given in Table [1](#S2.T1 "Table
    1 ‣ 2 Deep Iterative Registration ‣ Deep Learning in Medical Image Registration:
    A Survey"). We will survey earlier methods that used deep similarity based registration
    in Section [2.1](#S2.SS1 "2.1 Deep Similarity based Registration ‣ 2 Deep Iterative
    Registration ‣ Deep Learning in Medical Image Registration: A Survey") and then
    some more recently developed methods that use deep reinforcement learning based
    registration in Section [2.2](#S2.SS2 "2.2 Reinforcement Learning based Registration
    ‣ 2 Deep Iterative Registration ‣ Deep Learning in Medical Image Registration:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: A visualization of the registration pipeline for works that use deep
    learning to quantify image similarity in an intensity-based registration framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Deep Similarity based Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, methods that use deep learning to learn a similarity metric
    are surveyed. This similarity metric is inserted into a classical intensity-based
    registration framework with a defined interpolation strategy, transformation model,
    and optimization algorithm. A visualization of this overall framework is given
    in Fig. [3](#S2.F3 "Figure 3 ‣ 2 Deep Iterative Registration ‣ Deep Learning in
    Medical Image Registration: A Survey"). The solid lines represent data flows that
    are required during training and testing, while the dashed lines represent data
    flows that are required only during training. Note that this is the case for the
    remainder of the figures in this article as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Overview of Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although manually crafted similarity metrics perform reasonably well in the
    unimodal registration case, deep learning has been used to learn superior metrics.
    This section will first discuss approaches that use deep learning to augment the
    performance of unimodal intensity based registration pipelines before multimodal
    registration.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1.1 Unimodal Registration
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Wu et al. Wu et al. ([2013](#bib.bib109), [2016](#bib.bib110)) were the first
    to use deep learning to obtain an application specific similarity metric for registration.
    They extracted the features that are used for unimodal, deformable registration
    of 3D brain MR volumes using a convolutional stacked autoencoder (CAE). They subsequently
    performed the registration using gradient descent to optimize the NCC of the two
    sets of features. This method outperformed diffeomorphic demons Vercauteren et al.
    ([2009](#bib.bib104)) and HAMMER Shen ([2007](#bib.bib94)) based registration
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Eppenhof et al. Eppenhof and Pluim ([2018b](#bib.bib29)) estimated
    registration error for the deformable registration of 3D thoracic CT scans (inhale-exhale)
    in an end-to-end capacity. They used a 3D CNN to estimate the error map for inputted
    inhale-exhale pairs of thoracic CT scans. Like the above method, only learned
    features were used in this work.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Blendowski et al. Blendowski and Heinrich ([2018](#bib.bib10)) proposed
    the combined use of both CNN-based descriptors and manually crafted MRF-based
    self-similarity descriptors for lung CT registration. Although the manually crafted
    descriptors outperformed the CNN-based descriptors, optimal performance was achieved
    using both sets of descriptors. This indicates that, in the unimodal registration
    case, deep learning may not outperform manually crafted methods. However, it can
    be used to obtain complementary information.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1.2 Multimodal Registration
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The advantages of the application of deep learning to intensity based registration
    are more obvious in the multimodal case, where manually crafted similarity metrics
    have had very little success.
  prefs: []
  type: TYPE_NORMAL
- en: Cheng et al. Cheng et al. ([2016](#bib.bib17), [2018](#bib.bib18)) recently
    used a stacked denoising autoencoder to learn a similarity metric that assesses
    the quality of the rigid alignment of CT and MR images. They showed that their
    metric outperformed NMI-optimization-based and local cross correlation (LCC)-optimization-based
    for their application.
  prefs: []
  type: TYPE_NORMAL
- en: In an effort to explicitly estimate image similarity in the multimodal case,
    Simonovsky et al. Simonovsky et al. ([2016](#bib.bib96)) used a CNN to learn the
    dissimilarity between aligned 3D T1 and T2 weighted brain MR volumes. Given this
    similarity metric, gradient descent was used in order to iteratively update the
    parameters that define a deformation field. This method was able to outperform
    MI-optimization-based registration and set the stage for deep intensity based
    multimodal registration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, Sedghi et al. Sedghi et al. ([2018](#bib.bib92)) performed the
    rigid registration of 3D US/MR (modalities with an even greater appearance difference
    than MR/CT) abdominal scans by using a 5-layer neural network to learn a similarity
    metric that is then optimized by Powell’s method. This approach also outperformed
    MI-optimization-based registration. Haskins et al. Haskins et al. ([2019](#bib.bib36))
    learned a similarity metric for multimodal rigid registration of MR and transrectal
    US (TRUS) volumes by using a CNN to predict target registration error (TRE). Instead
    of using a traditional optimizer like the above methods, they used an evolutionary
    algorithm to explore the solution space prior to using a traditional optimization
    algorithm because of the learned metric’s lack of convexity. This registration
    framework outperformed MIND-optimization-based Heinrich et al. ([2012](#bib.bib38))
    and MI-optimization-based registration. In stark contrast to the above methods,
    Wright et al. Matthew et al. ([2018](#bib.bib75)) used LSTM spatial co-transformer
    networks to iteratively register MR and US volumes group-wise. The recurrent spatial
    co-transformation occurred in three steps: image warping, residual parameter prediction,
    parameter composition. They demonstrated that their method is more capable of
    quantifying image similarity than a previous multimodal image similarity quantification
    method that uses self-similarity context descriptors Heinrich et al. ([2013](#bib.bib39)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Discussion and Assessment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recent works have confirmed the ability of neural networks to assess image similarity
    in multimodal medical image registration. The results achieved by the approaches
    described in this section demonstrate that deep learning can be successfully applied
    to challenging registration tasks. However, the findings from Blendowski and Heinrich
    ([2018](#bib.bib10)) suggest that learned image similarity metrics may be best
    suited to complement existing similarity metrics in the unimodal case. Further,
    it is difficult to use these iterative techniques for real time registration.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Reinforcement Learning based Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, methods that use reinforcement learning for their registration
    applications are surveyed. Here, a trained agent is used to perform the registration
    as opposed to a pre-defined optimization algorithm. A visualization of this framework
    is given in Fig. [4](#S2.F4 "Figure 4 ‣ 2.2 Reinforcement Learning based Registration
    ‣ 2 Deep Iterative Registration ‣ Deep Learning in Medical Image Registration:
    A Survey"). Reinforcement learning based registration typically involves a rigid
    transformation model. However, it is possible to use a deformable transformation
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8c8de298661cee539588f2a4f92854d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: A visualization of the registration pipeline for works that use deep
    reinforcement learning to implicitly quantify image similarity for image registration.
    Here, an agent learns to map states to actions based on rewards that it receives
    from the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Liao et al. Liao et al. ([2017](#bib.bib62)) were the first to use reinforcment
    learning based registration to perform the rigid registration of cardiac and abdominal
    3D CT images and cone-beam CT (CBCT) images. They used a greedy supervised approach
    for end-to-end training with an attention-driven hierarchical strategy. Their
    method outperformed MI based registration and semantic registration using probability
    maps.
  prefs: []
  type: TYPE_NORMAL
- en: Shortly after, Kai et al. Ma et al. ([2017](#bib.bib71)) used a reinforcement
    learning approach to perform the rigid registration of MR/CT chest volumes. This
    approach is derived from $Q$-learning and leverages contextual information to
    determine the depth of the projected images. The network used in this method is
    derived from the dueling network architecture Wang et al. ([2015](#bib.bib108)).
    Notably, this work also differentiates between terminal and non-terminal rewards.
    This method outperforms registration methods that are based on iterative closest
    points (ICP), landmarks, Hausdorff distance, Deep Q Networks, and the Dueling
    Network Wang et al. ([2015](#bib.bib108)).
  prefs: []
  type: TYPE_NORMAL
- en: Instead of training a single agent like the above methods, Miao et al. Miao
    et al. ([2017](#bib.bib76)) used a multi-agent system in a reinforcement learning
    paradigm to rigidly register X-Ray and CT images of the spine. They used an auto-attention
    mechanism to observe multiple regions and demonstrate the efficacy of a multi-agent
    system. They were able to significantly outperform registration approaches that
    used a state-of-the-art similarity metric given by De Silva et al. ([2016](#bib.bib22)).
  prefs: []
  type: TYPE_NORMAL
- en: As opposed to the above rigid registration based works, Krebs et al. Krebs et al.
    ([2017](#bib.bib55)) used a reinforcement learning based approach to perform the
    deformable registration of 2D and 3D prostate MR volumes. They used a low resolution
    deformation model for the registration and fuzzy action control to influence the
    stochastic action selection. The low resolution deformation model is necessary
    to restrict the dimensionality of the action space. This approach outperformed
    registration performed using the Elastix toolbox Klein et al. ([2010](#bib.bib53))
    and LCC-Demons Lorenzi et al. ([2013](#bib.bib69)) based registration techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c568e97148dc30faeac61f0188933073.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A visualization of supervised single step registration.'
  prefs: []
  type: TYPE_NORMAL
- en: The use of reinforcement learning is intuitive for medical image registration
    applications. One of the principle challenges for reinforcement learning based
    registration is the ability to handle high resolution deformation fields. There
    are no such challenges for rigid registration. Because of the intuitive nature
    and recency of these methods, we expect that such approaches will receive more
    attention from the research community in the next few years.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Supervised Transformation Estimation Methods. Gray rows use Diffeomorphisms.'
  prefs: []
  type: TYPE_NORMAL
- en: Ref Supervision Transform Modality ROI Model Yang et al. ([2016](#bib.bib114))
    Real Transforms Deformable MR Brain FCN Cao et al. ([2017](#bib.bib13)) Real Transforms
    Deformable MR Brain 9-layer CNN Lv et al. ([2018](#bib.bib70)) Real Transforms
    Deformable MR Abdominal CNN Rohé et al. ([2017](#bib.bib86)) Real Transforms Deformable
    MR Cardiac SVF-Net Sokooti et al. ([2017](#bib.bib99)) Synthetic Deformable CT
    Chest RegNet Transforms Eppenhof and Pluim ([2018a](#bib.bib28)) Synthetic Deformable
    CT Lung U-Net Transforms Uzunova et al. ([2017](#bib.bib103)) Synthetic Deformable
    MR Brain/ FlowNet Transforms Cardiac Ito and Ino ([2018](#bib.bib47)) Synthetic
    Deformable MR Brain GoogleNet Transforms Sun et al. ([2018](#bib.bib102)) Synthetic
    Deformable CT/US Liver DVFNet Transforms Yang ([2017](#bib.bib113)) Real + Synthetic
    Deformable MR Brain FCN Transforms Sloan et al. ([2018](#bib.bib97)) Synthetic
    Rigid MR Brain 6-layer CNN Transforms 10-layer FCN Salehi et al. ([2018](#bib.bib90))
    Synthetic Rigid MR Brain 11-layer CNN Transforms ResNet-18 Zheng et al. ([2018](#bib.bib119))
    Synthetic Rigid X-ray Bone 17-layer CNN Transforms PDA Module Miao et al. ([2016b](#bib.bib78))
    Synthetic Rigid X-ray/ Bone 6-layer CNN Transforms DDR Chee and Wu ([2018](#bib.bib15))
    Synthetic Rigid MR Brain AIRNet Transforms Hu et al. ([2018c](#bib.bib44)) Segmentations
    Deformable MR/US Prostate 30-layer FCN Hering et al. ([2018](#bib.bib40)) Segmentations
    + Deformable MR/US Prostate U-Net Similarity Metric GAN Hu et al. ([2018a](#bib.bib42))
    Segmentations + Deformable MR/US Prostate GAN Adversarial Loss Fan et al. ([2018b](#bib.bib31))
    Real Transforms + Deformable MR Brain U-Net Similarity Metric Yan et al. ([2018](#bib.bib111))
    Synthetic Rigid MR/US Prostate GAN Transforms + Adversarial Loss
  prefs: []
  type: TYPE_NORMAL
- en: 3 Supervised Transformation Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the early success of the previously described approaches, the transformation
    estimation in these methods is iterative, which can lead to slow registration.
    Haskins et al. ([2019](#bib.bib36)). This is especially true in the deformable
    registration case where the solution space is high dimensional Lee et al. ([2017](#bib.bib59)).
    This motivated the development of networks that could estimate the transformation
    that corresponds to optimal similarity in one step. However, fully supervised
    transformation estimation (the exclusive use of ground truth data to define the
    loss function) has several challenges that are highlighted in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'A visualization of supervised transformation estimation is given in Fig. [5](#S2.F5
    "Figure 5 ‣ 2.2 Reinforcement Learning based Registration ‣ 2 Deep Iterative Registration
    ‣ Deep Learning in Medical Image Registration: A Survey") and a description of
    notable works is given in Table [2](#S2.T2 "Table 2 ‣ 2.2 Reinforcement Learning
    based Registration ‣ 2 Deep Iterative Registration ‣ Deep Learning in Medical
    Image Registration: A Survey"). This section first discusses methods that use
    fully supervised approaches in Section [3.1](#S3.SS1 "3.1 Fully Supervised Transformation
    Estimation ‣ 3 Supervised Transformation Estimation ‣ Deep Learning in Medical
    Image Registration: A Survey") and then discusses methods that use dual/weakly
    supervised approaches in Section [3.2](#S3.SS2 "3.2 Dual/Weakly Supervised Transformation
    Estimation ‣ 3 Supervised Transformation Estimation ‣ Deep Learning in Medical
    Image Registration: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Fully Supervised Transformation Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, methods that used full supervision for single-step registration
    are surveyed. Using a neural network to perform registration as opposed to an
    iterative optimizer significantly speeds up the registration process.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Overview of works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several registration application require deformable transformation models that
    often prohibit the use of traditional convolutional neural networks because of
    the computational expense associated with using FC-layers to make predictions
    in highly dimensional solution spaces Krebs et al. ([2017](#bib.bib55)). Because
    the networks that are used to predict deformation fields are fully convolutional,
    the dimensionality of the solution space associated with a deformation field does
    not introduce additional computational constraints Yang et al. ([2016](#bib.bib114)).
    This section will first discuss approaches that use a rigid transformation model
    and then discuss approaches that use a deformable transformation model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1.1 Rigid Registration
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Miao et al. Miao et al. ([2016a](#bib.bib77), [b](#bib.bib78)) were the first
    to use deep learning to predict rigid transformation parameters. They used a CNN
    to predict the transformation matrix associated with the rigid registration of
    2D/3D X-ray attenuation maps and 2D X-ray images. Hierarchical regression is proposed
    in which the 6 transformation parameters are partitioned into 3 groups. Ground
    truth data was synthesized in this approach by transforming aligned data. This
    is the case for the next three approaches that are described as well. This approach
    outperformed MI, CC, and gradient correlation (GC)-optimization-based registration
    approaches with respect to both accuracy and computational efficiency. The improved
    computational efficiency is due to the use of a forward pass through a neural
    network instead of an optimization algorithm to perform the registration.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Chee et al. Chee and Wu ([2018](#bib.bib15)) used a CNN to predict
    the transformation parameters used to rigidly register 3D brain MR volumes. In
    their framework, affine image registration network (AIRNet), the MSE between the
    predicted and ground truth affine transforms is used to train the network. They
    were able to outperform MI-optimization-based registration for both the unimodal
    and multimodal cases.
  prefs: []
  type: TYPE_NORMAL
- en: That same year, Salehi et al. Salehi et al. ([2018](#bib.bib90)) used a deep
    residual regression network, a correction network, and a bivariant geodesic distance
    based loss function to rigidly register T1 and T2 weighted 3D fetal brain MRs
    for atlas construction. The use of the residual network to initially register
    the image volumes prior to the forward pass through the correction network allowed
    for an enhancement of the capture range of the registration. This approach was
    evaluated for both slice-to-volume registration and volume-to-volume registration.
    They validated the efficacy of their geodesic loss term and outperformed NCC-optimization-based
    registration.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Zheng et al. Zheng et al. ([2018](#bib.bib119)) proposed the integration
    of a pairwise domain adaptation module (PDA) into a pre-trained CNN that performs
    the rigid registration of pre-operative 3D X-Ray images and intraoperative 2D
    X-ray images using a limited amount of training data. Domain adaptation was used
    to address the discrepancy between synthetic data that was used to train the deep
    model and real data.
  prefs: []
  type: TYPE_NORMAL
- en: Sloan et al. Sloan et al. ([2018](#bib.bib97)) used a CNN is used to regress
    the rigid transformation parameters for the registration of T1 and T2 weighted
    brain MRs. Both unimodal and multimodal registration were investigated in this
    work. The parameters that constitute the convolutional layers that were used to
    extract low-level features in each image were only shared in the unimodal case.
    In the multimodal case, these parameters were learned separately. This approach
    also outperformed MI-optimization-based image registration.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1.2 Deformable Registration
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Unike the previous section, methods that use both real and synthesized ground
    truth labels will be discussed. Methods that use clinical/publicly available ground
    truth labels for training are discussed first. This ordering is reflective of
    the fact that simulating realistic deformable transformations is more difficult
    than simulating realistic rigid transformations.
  prefs: []
  type: TYPE_NORMAL
- en: First, Yang et al. Yang et al. ([2016](#bib.bib114)) predicted the deformation
    field with an FCN that is used to register 2D/3D intersubject brain MR volumes
    in a single step. A U-net like architecture Ronneberger et al. ([2015](#bib.bib87))
    was used in this approach. Further, they used large diffeomorphic metric mapping
    to provide a basis, used the initial momentum values of the pixels of the image
    volumes as the network input, and evolved these values to obtain the predicted
    deformation field. This method outperformed semi-coupled dictionary learning based
    registration Cao et al. ([2015](#bib.bib11)).
  prefs: []
  type: TYPE_NORMAL
- en: The following year, Rohe et al. Rohé et al. ([2017](#bib.bib86)) also used a
    U-net Ronneberger et al. ([2015](#bib.bib87)) inspired network to estimate the
    deformation field used to register 3D cardiac MR volumes. Mesh segmentations are
    used to compute the reference transformation for a given image pair and SSD between
    the prediction and ground truth is used as the loss function. This method outperformed
    LCC Demons based registration Lorenzi et al. ([2013](#bib.bib69)).
  prefs: []
  type: TYPE_NORMAL
- en: That same year, Cao et al. Cao et al. ([2017](#bib.bib13)) used a CNN to map
    input image patches of a pair of 3D brain MR volumes to their respective displacement
    vector. The totality of these displacement vectors for a given image constitutes
    the deformation field that is used to perform the registration. Additionally,
    they used the similarity between inputted image patches to guide the learning
    process. Further, they used equalized active-points guided sampling strategy that
    makes it so that patches with higher gradient magnitudes and displacement values
    are more likely to be sampled for training. This method outperforms SyN Avants
    et al. ([2008](#bib.bib6)) and Demons Vercauteren et al. ([2009](#bib.bib104))
    based registration methods.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Jun et al. Lv et al. ([2018](#bib.bib70)) used a CNN to perform the
    deformable registration of abdominal MR images to compensate for the deformation
    that is caused by respiration. This approach achieved registration results that
    are superior to those obtained using non-motion corrected registrations and local
    affine registration. Recently, unlike many of the other approaches discussed in
    this paper, Yang et al. Yang ([2017](#bib.bib113)) quantified the uncertainty
    associated with the deformable registration of 3D T1 and T2 weighted brain MRs
    using a low-rank Hessian approximation of the variational gaussian distribution
    of the transformation parameters. This method was evaulated on both real and synthetic
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Just as deep learning practitioners use random transformations to enhance the
    diversity of their dataset, Sokooti et al. Sokooti et al. ([2017](#bib.bib99))
    used random DVFs to augment their dataset. They used a multi-scale CNN to predict
    a deformation field. This deformation is used to perform intra-subject registration
    of 3D chest CT images. This method used late fusion as opposed to early fusion,
    in which the patches are concatenated and used as the input to the network. The
    performance of their method is competitive with B-Spline based registration Sokooti
    et al. ([2017](#bib.bib99)).
  prefs: []
  type: TYPE_NORMAL
- en: Such approaches have notable, but also limited ability to enhance the size and
    diversity of datasets. These limitations motivated the development of more sophisticated
    ground truth generation. The rest of the approaches described in this section
    use simulated ground truth data for their applications.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Eppenhof et al. Eppenhof and Pluim ([2018a](#bib.bib28)) used a
    3D CNN to perform the deformable registration of inhale-exhale 3D lung CT image
    volumes. A series of multi-scale, random transformations of aligned image pairs
    eliminate the need for manually annotated ground truth data while also maintaining
    realistic image appearance. Further, as is the case with other methods that generate
    ground truth data, the CNN can be trained using relatively few medical images
    in a supervised capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the above works, Uzunova et al. Uzunova et al. ([2017](#bib.bib103))
    generated ground truth data using statistical appearance models (SAMs). They used
    a CNN to estimate the deformation field for the registration of 2D brain MRs and
    2D cardiac MRs, and adapt FlowNet Dosovitskiy et al. ([2015](#bib.bib26)) for
    their application. They demonstrated that training FlowNet using SAM generated
    ground truth data resulted in superior performance to CNNs trained using either
    randomly generated ground truth data or ground truth data obtained using the registration
    method described in Ehrhardt et al. ([2015](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the other methods in this section that use random transformations or
    manually crafted methods to generate ground truth data, Ito et al. Ito and Ino
    ([2018](#bib.bib47)) used a CNN to learn plausible deformations for ground truth
    data generation. They evaluated their approach on the 3D brain MR volumes in the
    ADNI dataset and outperformed the MI-optimization-based approach proposed in Ikeda
    et al. ([2014](#bib.bib45)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Discussion and Assessment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Supervised transformation estimation has allowed for real time, robust registration
    across applications. However, such works are not without their limitations. Firstly,
    the quality of the registrations using this framework is dependent on the quality
    of the ground truth registrations. The quality of these labels is, of course,
    dependent upon the expertise of the practitioner. Furthermore, these labels are
    fairly difficult to obtain because there are relatively few individuals with the
    expertise necessary to perform such registrations. Transformations of training
    data and the generation of synthetic ground truth data can address such limitations.
    However, it is important to ensure that simulated data is sufficiently similar
    to clinical data. These challenges motivated the development of partially supervised/unsupervised
    approaches, which will be discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Dual/Weakly Supervised Transformation Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dual supervision refers to the use of both ground truth data and some metric
    that quantifies image similarity to train a model. On the other hand, weak supervision
    refers to using the overlap of segmentations of corresponding anatomical structures
    to design the loss function. This section will discuss the contributions of such
    works in Section [3.2.1](#S3.SS2.SSS1 "3.2.1 Overview of works ‣ 3.2 Dual/Weakly
    Supervised Transformation Estimation ‣ 3 Supervised Transformation Estimation
    ‣ Deep Learning in Medical Image Registration: A Survey") and then discuss the
    overall state of this research direction in Section [3.2.2](#S3.SS2.SSS2 "3.2.2
    Discussion and Assessment ‣ 3.2 Dual/Weakly Supervised Transformation Estimation
    ‣ 3 Supervised Transformation Estimation ‣ Deep Learning in Medical Image Registration:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Overview of works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad32c0c42ab62d93f3d21860cb88245f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: A visualization of deep single step registration where the agent
    is trained using dual supervision. The loss function is determined using both
    a metric that quantifies image similarity and ground truth data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, this section will discuss methods that use dual supervised and then
    will discuss methods that use weak supervision. Recently, Fan et al. Fan et al.
    ([2018b](#bib.bib31)) used hierarchical, dual-supervised learning to predicted
    the deformation field for 3D brain MR registration. They amend the traditional
    U-Net architecture Ronneberger et al. ([2015](#bib.bib87)) by using “gap-filling”
    (*i.e.*, inserting convolutional layers after the U-type ends or the architecture)
    and coarse-to-fine guidance. This approach leveraged both the similarity between
    the predicted and ground truth transformations, and the similarity between the
    warped and fixed images to train the network. The architecture detailed in this
    method outperformed the traditional U-Net architecture and the dual supervision
    strategy is verified by ablating the image similarity loss function term. A visualization
    of dual supervised transformation estimation is given in Fig. [6](#S3.F6 "Figure
    6 ‣ 3.2.1 Overview of works ‣ 3.2 Dual/Weakly Supervised Transformation Estimation
    ‣ 3 Supervised Transformation Estimation ‣ Deep Learning in Medical Image Registration:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, Yan et al. Yan et al. ([2018](#bib.bib111)) used a framework
    that is inspired by the GAN Goodfellow et al. ([2014](#bib.bib35)) to perform
    the rigid registration of 3D MR and TRUS volumes. In this work, the generator
    was trained to estimate a rigid transformation. While, the discriminator was trained
    to discern between images that were aligned using the ground truth transformations
    and images that were aligned using the predicted transformations. Both Euclidean
    distance to ground truth and an adversarial loss term are used to construct the
    loss function in this method. Note that the adversarial supervision strategy that
    was used in this approach is similar to the ones that are used in a number of
    unsupervised works that will be described in the next section. A visualization
    of adversarial transformation estimation is given in Fig. [7](#S3.F7 "Figure 7
    ‣ 3.2.1 Overview of works ‣ 3.2 Dual/Weakly Supervised Transformation Estimation
    ‣ 3 Supervised Transformation Estimation ‣ Deep Learning in Medical Image Registration:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/827955275448ceb55a3ad37a6b745d33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A visualization of an adversarial image registration framework. Here,
    the generator is trained using output from the discriminator. The discriminator
    takes the form of a learned metric here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the above methods that used dual supervision, Hu et al. Hu et al. ([2018b](#bib.bib43),
    [c](#bib.bib44)) recently used label similarity to train their network to perform
    MR-TRUS registration. In their initial work, they used two neural networks: local-net
    and global-net to estimate the global affine transformation with 12 degrees of
    freedom and the local dense deformation field respectively Hu et al. ([2018b](#bib.bib43)).
    The local-net uses the concatenation of the transformation of the moving image
    given by the global-net and the fixed image as its input. However, in their later
    work Hu et al. ([2018c](#bib.bib44)), they combine these networks in an end-to-end
    framework. This method outperformed NMI-optimization-based and NCC based registration.
    A visualization of weakly supervised transformation estimation is given in Fig. [8](#S3.F8
    "Figure 8 ‣ 3.2.1 Overview of works ‣ 3.2 Dual/Weakly Supervised Transformation
    Estimation ‣ 3 Supervised Transformation Estimation ‣ Deep Learning in Medical
    Image Registration: A Survey"). In another work, Hu et al. Hu et al. ([2018a](#bib.bib42))
    simultaneously maximized label similarity and minimized an adversarial loss term
    to predict the deformation for MR-TRUS registration. This regularization term
    forces the predicted transformation to result in the generation of a realistic
    image. Using the adversarial loss as a regularization term is likely to successfully
    force the transformation to be realistic given proper hyper parameter selection.
    The performance of this registration framework was inferior to the performance
    of their previous registration framework described above. However, they showed
    that adversarial regularization is superior to standard bending energy based regularization.
    Similar to the above method, Hering et al. Hering et al. ([2018](#bib.bib40))
    built upon the progress made with respect to both dual and weak supervision by
    introducing a label and similarity metric based loss function for cardiac motion
    tracking via the deformable registration of 2D cine-MR images. Both segmentation
    overlap and edge based normalized gradient fields distance were used to construct
    the loss function in this approach. Their method outperformed a multilevel registration
    approach similar to the one proposed in Rühaak et al. ([2013](#bib.bib88)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a14b8735627c5658b43ea872555b94a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: A visualization of deep single step registration where the agent
    is trained using label similarity (*i.e.* weak supervision). Manually annotated
    data (segmentations) are used to define the loss function used to train the network.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Discussion and Assessment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Direct transformation estimation marked a major breakthrough for deep learning
    based image registration. With full supervision, promising results have been obtained.
    However, at the same time, those techniques require a large amount of detailed
    annotated images for training. Partially/weakly supervised transformation estimation
    methods alleviated the limitations associated with the trustworthiness and expense
    of ground truth labels. However, they still require manually annotated data (*e.g.*
    ground truth and/or segmentations). On the other hand, weak supervision allows
    for similarity quantification in the multimodal case. Further, partial supervision
    allows for the aggregation of methods that can be used to assess the quality of
    a predicted registration. As a result, there is growing interest in these research
    areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Unsupervised Transformation Estimation Methods. Grays rows use Diffeomorphisms.'
  prefs: []
  type: TYPE_NORMAL
- en: Ref Loss Function Transform Modality ROI Model Jiang and Shackleford ([2018](#bib.bib50))
    SSD Deformable CT Chest Multi-scale CNN Ghosal and Ray ([2017](#bib.bib33)) UB
    SSD Deformable MR Brain 19-layer FCN Zhang ([2018](#bib.bib118)) MSD Deformable
    MR Brain ICNet Shu et al. ([2018](#bib.bib95)) MSE Deformable SEM Neurons 11-layer
    CNN Dalca et al. ([2018](#bib.bib21)) MSE Deformable MR Brain VoxelMorph Sheikhjafari
    et al. ([2018](#bib.bib93)) MSE Deformable MR Cardiac 8-layer Cine FCNet Kuang
    and Schmah ([2018](#bib.bib58)) CC Deformable MR Brain FAIM Li and Fan ([2018](#bib.bib61))
    NCC Deformable MR Brain 8-layer FCN Cao et al. ([2018](#bib.bib12)) NCC Deformable
    CT, MR Pelvis U-Net de Vos et al. ([2017](#bib.bib24)) NCC Deformable MR Cardiac
    DIRNet Cine de Vos et al. ([2018](#bib.bib23)) NCC Deformable MR Cardiac DLIR
    Cine Ferrante et al. ([2018](#bib.bib32)) NCC Deformable X-ray, MR Bone U-Net
    Cardiac STN Cine Sun and Zhang ([2018](#bib.bib101)) L2 Distance + Deformable
    MR, US Brain FCN Image Gradient Neylon et al. ([2017](#bib.bib81)) Predicted TRE
    Deformable CT Head/Neck FCN Fan et al. ([2018a](#bib.bib30)) BCE Deformable MR
    Brain GAN Mahapatra ([2018](#bib.bib73)) NMI + SSIM Deformable MR, FA/ Cardiac
    GAN + VGG Outputs Color fundus Retinal Mahapatra et al. ([2018](#bib.bib74)) NMI
    + SSIM + Deformable X-ray Bone GAN VGG Outputs + BCE Yoo et al. ([2017](#bib.bib117))
    MSE AE Output Deformable ssEM Neurons CAE STN Wu et al. ([2016](#bib.bib110))
    MSE Stacked Deformable MR Brain Stacked AE Outputs AE Wu et al. ([2013](#bib.bib109))
    NCC of Deformable MR Brain Stacked ISA Outputs ISA Krebs et al. ([2018a](#bib.bib56))
    Log Likelihood Deformable MR Brain cVAE STN Liu and Leung ([2017](#bib.bib67))
    SSD MIND + Deformable CT, MR Chest FCN PCANet Outputs Brain PCANet Kori et al.
    ([2018](#bib.bib54)) SSD VGG Rigid MR Brain CNN Outputs MLP
  prefs: []
  type: TYPE_NORMAL
- en: 4 Unsupervised Transformation Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite the success of the methods described in the previous sections, the
    difficult nature of the acquisition of reliable ground truth remains a significant
    hindrance Uzunova et al. ([2017](#bib.bib103)). This has motivated a number of
    different groups to explore unsupervised approaches de Vos et al. ([2017](#bib.bib24));
    Li and Fan ([2018](#bib.bib61)). One key innovation that has been useful to these
    works is the spatial transformer network (STN) Jaderberg et al. ([2015](#bib.bib48)).
    Several methods use an STN to perform the deformations associated with their registration
    applications Ferrante et al. ([2018](#bib.bib32)); Kuang and Schmah ([2018](#bib.bib58)).
    This section discusses unsupervised methods that utilize image similarity metrics
    (Section [4.1](#S4.SS1 "4.1 Similarity Metric based Unsupervised Transformation
    Estimation ‣ 4 Unsupervised Transformation Estimation ‣ Deep Learning in Medical
    Image Registration: A Survey")) and feature representations of image data (Section
    [4.2](#S4.SS2 "4.2 Feature based Unsupervised Transformation Estimation ‣ 4 Unsupervised
    Transformation Estimation ‣ Deep Learning in Medical Image Registration: A Survey"))
    to train their networks. A description of notable works is given in Table [3](#S3.T3
    "Table 3 ‣ 3.2.2 Discussion and Assessment ‣ 3.2 Dual/Weakly Supervised Transformation
    Estimation ‣ 3 Supervised Transformation Estimation ‣ Deep Learning in Medical
    Image Registration: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Similarity Metric based Unsupervised Transformation Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 Standard Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This section begins by discussing approaches that use a common similarity metric
    with common regularization strategies to define their loss functions. Later in
    the section, approaches that use more complex similarity metric based strategies
    are discussed. A visualization of standard similarity metric based transformation
    estimation is given in Fig. [9](#S4.F9 "Figure 9 ‣ 4.1.1 Standard Methods ‣ 4.1
    Similarity Metric based Unsupervised Transformation Estimation ‣ 4 Unsupervised
    Transformation Estimation ‣ Deep Learning in Medical Image Registration: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/40c688f94984086db1050b18fb48143d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: A visualization of deep single step registration where the network
    is trained using a metric that quantifies image similarity. Therefore, the approach
    is unsupervised.'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired to overcome the difficulty associated with obtaining ground truth data,
    Li et al. Li and Fan ([2017](#bib.bib60), [2018](#bib.bib61)) trained an FCN to
    perform deformable intersubject registration of 3D brain MR volumes using ”self-supervision.”
    NCC between the warped and fixed images and several common regularization terms
    (*e.g.* smoothing constraints) constitute the loss function in this method. Although
    many manually defined similarity metrics fail in the multimodal case (with the
    occasional exception of MI), they are often suitable for the unimodal case. The
    method detailed in this work outperforms Advanced Neuroimaging Tools (ANTs) based
    registration Avants et al. ([2011](#bib.bib7)) and the deep learning methods proposed
    by Sokooti et al. Sokooti et al. ([2017](#bib.bib99)) (discussed previously) and
    Yoo et al. Yoo et al. ([2017](#bib.bib117)) (discussed in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: Further, de Vos et al. de Vos et al. ([2017](#bib.bib24)) used NCC to train
    an FCN to perform the deformable registration of 4D cardiac cine MR volumes. A
    DVF is used in this method to deform the moving volume. Their method outperforms
    registration that is performed using the Elastix toolbox Klein et al. ([2010](#bib.bib53)).
  prefs: []
  type: TYPE_NORMAL
- en: In another work, de Vos et al. de Vos et al. ([2018](#bib.bib23)) use a multistage,
    multiscale approach to perform unimodal registration on several datasets. NCC
    and a bending-energy regularization term are used to train the networks that predict
    an affine transformation and subsequent coarse-to-fine deformations using a B-Spline
    transformation model. In addition to validating their multi-stage approach, they
    show that their method outperforms registration that is performed using the Elastix
    toolbox Klein et al. ([2010](#bib.bib53)) with and without bending energy.
  prefs: []
  type: TYPE_NORMAL
- en: The unsupervised deformable registration framework used by Ghosal et al. Ghosal
    and Ray ([2017](#bib.bib33)) minimizes the upper bound of the SSD (UB SSD) between
    the warped and fixed 3D brain MR images. The design of their network was inspired
    by the SKIP architecture Long et al. ([2015](#bib.bib68)). This method outperforms
    log-demons based registration.
  prefs: []
  type: TYPE_NORMAL
- en: Shu et al. Shu et al. ([2018](#bib.bib95)) used a coarse-to-fine, unsupervised
    deformable registration approach to register images of neurons that are acquired
    using a scanning electron microscope (SEM). The mean squared error (MSE) between
    the warped and fixed volumes is used as the loss function here. Their approach
    is competitive with and faster than the sift flow framework Liu et al. ([2011](#bib.bib64)).
  prefs: []
  type: TYPE_NORMAL
- en: Sheikhjafari et al. Sheikhjafari et al. ([2018](#bib.bib93)) used learned latent
    representations to perform the deformable registration of 2D cardiac cine MR volumes.
    Deformation fields are thus obtained by embedding. This latent representation
    is used as the input to a network that is composed of 8 fully connected layers
    to obtain the transformation. The sum of absolute errors (SAE) is used as the
    loss function. Here, the registration performance was seen to be influenced by
    the B-spline grid spacing. This method outperforms a moving mesh correspondence
    based method described in Punithakumar et al. ([2017](#bib.bib83)).
  prefs: []
  type: TYPE_NORMAL
- en: Stergios et al. Stergios et al. ([2018](#bib.bib100)) used a CNN to both linearly
    and locally register inhale-exhale pairs of lung MR volumes. Therefore, both the
    affine transformation and the deformation are jointly estimated. The loss function
    is composed of an MSE term and regularization terms. Their method outperforms
    several state-of-the-art methods that do not utilized ground truth data, including
    Demons Lorenzi et al. ([2013](#bib.bib69)), SyN Avants et al. ([2008](#bib.bib6)),
    and a deep learning based method that uses an MSE loss term. Further, the inclusion
    of the regularization terms is validated by an ablation study.
  prefs: []
  type: TYPE_NORMAL
- en: The successes of deep similarity metric based unsupervised registration motivated
    Neylon et al. Neylon et al. ([2017](#bib.bib81)) to use a neural network to learn
    the relationship between image similarity metric values and TRE when registering
    CT image volumes. This is done in order to robustly assess registration performance.
    The network was able to achieve subvoxel accuracy in 95% of cases. Similarly inspired,
    Balakrishnan et al. Balakrishnan et al. ([2018a](#bib.bib8), [b](#bib.bib9)) proposed
    a general framework for unsupervised image registration, which can be either unimodal
    or multimodal theoretically. The neural networks are trained using a selected,
    manually-defined image similarity metric (*e.g.* NCC, NMI, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: In a follow-up paper, Dalca et al. Dalca et al. ([2018](#bib.bib21)) casted
    deformation prediction as variational inference. Diffeomorphic integration is
    combined with a transformer layer to obtain a velocity field. Squaring and rescaling
    layers are used to integrate the velocity field to obtain the predicted deformation.
    MSE is used as the similarity metric that, along with a regularization term, define
    the loss function. Their method outperforms ANTs based registration Avants et al.
    ([2011](#bib.bib7)) and the deep learning based method described in Balakrishnan
    et al. ([2018a](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: Shortly after, Kuang et al. Kuang and Schmah ([2018](#bib.bib58)) used a CNN
    and STN inspired framework to perform the deformable registration of T1-weighted
    brain MR volumes. The loss function is composed of a NCC term and a regularization
    term. This method uses Inception modules, a low capacity model, and residual connections
    instead of skip connections. They compare their method with VoxelMorph (the method
    proposed by Balakrishnan et al., described above) Balakrishnan et al. ([2018b](#bib.bib9))
    and uTIlzReg GeoShoot Vialard et al. ([2012](#bib.bib105)) using the LBPA40 and
    Mindboggle 101 datasets and demonstrate superior performance with respect to both.
  prefs: []
  type: TYPE_NORMAL
- en: Building upon the progress made by the previously described metric-based approaches,
    Ferrante et al. Ferrante et al. ([2018](#bib.bib32)) used a transfer learning
    based approach to perform unimodal registration of both X-ray and cardiac cine
    images. In this work, the network is trained on data from a source domain using
    NCC as the primary loss function term and tested in a target domain. They used
    a U-net like architecture Ronneberger et al. ([2015](#bib.bib87)) and an STN Jaderberg
    et al. ([2015](#bib.bib48)) to perform the feature extraction and transformation
    estimation respectively. They demonstrated that transfer learning using either
    domain as the source or the target domain produces effective results. This method
    outperformed registration obtained using the Elastix toolbox Klein et al. ([2010](#bib.bib53))
    with parameters determined using grid search.
  prefs: []
  type: TYPE_NORMAL
- en: Although applying similarity metric based approaches to the multimodal case
    is difficult, Sun et al. Sun and Zhang ([2018](#bib.bib101)) proposed an unsupervised
    method for 3D MR/US brain registration that uses a 3D CNN that consists of a feature
    extractor and a deformation field generator. This network is trained using a similarity
    metric that incorporates both pixel intensity and gradient information. Further,
    both image intensity and gradient information are used as inputs into the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Extensions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cao et al. Cao et al. ([2018](#bib.bib12)) also applied similarity metric based
    training to the multimodal case. Specifically, they used intra-modality image
    similarity to supervise the multimodal deformable registration of 3D pelvic CT/MR
    volumes. The NCC between the moving image that is warped using the ground truth
    transformation and the moving image that is warped using the predicted transformation
    is used as the loss function. This work utilizes ”dual” supervision (*i.e.* the
    intra-modality supervision previously described is used for both the CT and the
    MR images). This is not to be confused with the dual supervision strategies described
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the limiting nature of the asymmetric transformations that typical
    unsupervised methods estimate, Zhang et al. Zhang ([2018](#bib.bib118)) used their
    network Inverse-Consistent Deep Network (ICNet)-to learn the symmetric diffeomorphic
    transformations for each of the brain MR volumes that are aligned into the same
    space. Different from other works that use standard regularization strategies,
    this work introduces an inverse-consistent regularization term and an anti-folding
    regularization term to ensure that a highly weighted smoothness constraint does
    not result in folding. Finally, the MSD between the two images allows this network
    to be trained in an unsupervised manner. This method outperformed SyN based registration
    Avants et al. ([2008](#bib.bib6)), Demons based registration Lorenzi et al. ([2013](#bib.bib69)),
    and several deep learning based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The next three approaches described in this section used a GAN for their applications.
    Unlike the GAN-based approaches described previously, these methods use neither
    ground truth data nor manually crafted segmentations. Mahapatra et al. Mahapatra
    ([2018](#bib.bib73)) used a GAN to implicitly learn the density function that
    represents the range of plausible deformations of cardiac cine images and multimodal
    retinal images (retinal colour fundus images and fluorescein angiography (FA)
    images). In addition to NMI, structual similarity index measure (SSIM), and a
    feature perceptual loss term (determined by the SSD between VGG outputs), the
    loss function is comprised of conditional and cyclic constraints, which are based
    on recent advances involving the implementation of adversarial frameworks. Their
    approach outperforms registration that is performed using the Elastix toolbox
    Klein et al. ([2010](#bib.bib53)) and the method proposed by de Vos et al. de Vos
    et al. ([2017](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: Further, Fan et al. Fan et al. ([2018a](#bib.bib30)) used a GAN to perform unsupervised
    deformable image registration of 3D brain MR volumes. Unlike most other unsupervised
    works that use a manually crafted similarity metric to determine the loss function
    and unlike the previous approach that used a GAN to ensure that the predicted
    deformation is realistic, this approach uses a discriminator to assess the quality
    of the alignment. This approach outperforms Diffeomorphic Demons and SyN registration
    on every dataset except for MGH10\. Further, the use of the discriminator for
    supervision of the registration network is superior to the use of ground truth
    data, SSD, and CC on all datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different from the hitherto previously described works (not just the GAN based
    ones), Mahapatra et al. Mahapatra et al. ([2018](#bib.bib74)) proposed simultaneous
    segmentation and registration of chest X-rays using a GAN framework. The network
    takes 3 inputs: reference image, floating image, and the segmentation mask of
    the reference image and outputs the segmentation mask of the transformed image,
    and the deformation field. Three discriminators are used to assess the quality
    of the generated outputs (deformation field, warped image, and segmentation) using
    cycle consistency and a dice metric. The generator is additionally trained using
    NMI, SSIM, and a feature perceptual loss term.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, instead of predicting a deformation field given a fixed parameterization
    as the other methods in this section do, Jiang et al. Jiang and Shackleford ([2018](#bib.bib50))
    used a CNN to learn an optimal parameterization of an image deformation using
    a multi-grid B-Spline method and L1-norm regularization. They use this approach
    to parameterize the deformable registration of 4D CT thoracic image volumes. Here,
    SSD is used as the similarity metric and L-BFGS-B is used as the optimizer. The
    convergence rate using the parameterized deformation model obtained using the
    proposed method is faster than the one obtained using a traditional L1-norm regularized
    multi-grid parameterization.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Discussion and Assessment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Image similarity based unsupervised image registration has received a lot of
    attention from the research community recently because it bypasses the need for
    expert labels of any kind. This means that the performance of the model will not
    depend on the expertise of the practitioner. Further, extensions of the original
    similarity metric based method that introduce more sophisticated similarity metrics
    (*e.g.* the discriminator of a GAN) and/or regularization strategies have yielded
    promising results. However, it is still difficult to quantify image similarity
    for multimodal registration applications. As a result, the scope of unsupervised,
    image similarity based works is largely confined to the unimodal case. Given that
    multimodal registration is often needed in many clinical applications, we expect
    to see more papers in the near future that will tackle this challenging problem.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Feature based Unsupervised Transformation Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, methods that use learned feature representations to train
    neural networks are surveyed. Like the methods surveyed in the previous section,
    the methods surveyed in this section do not require ground truth data. In this
    section, approaches that create unimodal registration pipelines are presented
    first. Then, an approach that tackles multimodal image registration is discussed.
    A visualization of featured based transformation estimation is given in Fig. [10](#S4.F10
    "Figure 10 ‣ 4.2.1 Unimodal Registration ‣ 4.2 Feature based Unsupervised Transformation
    Estimation ‣ 4 Unsupervised Transformation Estimation ‣ Deep Learning in Medical
    Image Registration: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Unimodal Registration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Yoo et al. Yoo et al. ([2017](#bib.bib117)) used an STN to register serial-section
    electron microscopy images (ssEMs). An autoencoder is trained to reconstruct fixed
    images and the L2 distance between reconstructed fixed images and corresponding
    warped moving images is used along with several regularization terms to construct
    the loss function. This approach outperforms the bUnwarpJ registration technique
    Arganda-Carreras et al. ([2006](#bib.bib5)) and the Elastic registration technique
    Saalfeld et al. ([2012](#bib.bib89)).
  prefs: []
  type: TYPE_NORMAL
- en: In the same year, Liu et al. Liu and Leung ([2017](#bib.bib67)) proposed a tensor
    based MIND method using a principle component analysis based network (PCANet)
    Chan et al. ([2015](#bib.bib14)) for both unimodal and multimodal registration.
    Both inhale-exhale pairs of thoracic CT volumes and multimodal pairs of brain
    MR images are used for experimental validation of this approach. MI and residual
    complexity (RC) based Myronenko and Song ([2010](#bib.bib79)), and the original
    MIND-based Heinrich et al. ([2012](#bib.bib38)) registration techniques were outperformed
    by the proposed method.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/32232b6d2203077015d00c48adab8874.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: A visualization of feature based unsupervised image registration.
    Here, a feature extractor is used to map inputted images to a feature space to
    facilitate the prediction of transformation parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Krebs et al. Krebs et al. ([2018a](#bib.bib56), [b](#bib.bib57)) performed the
    registration of 2D brain and cardiac MRs and bypassed the need for spatial regularization
    using a stochastic latent space learning approach. A conditional variational autoencoder
    Doersch ([2016](#bib.bib25)) is used to ensure that the parameter space follows
    a prescribed probability distribution. The negative log liklihood of the fixed
    image given the latent representation and the warped volume and KL divergence
    of the latent distribution from a prior distribution are used to define the loss
    function. This method outperforms the Demons technique Lorenzi et al. ([2013](#bib.bib69))
    and the deep learning method described in Balakrishnan et al. ([2018a](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Multimodal Registration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike all of the other methods described in this section, Kori et al. perform
    feature extraction and affine transformation parameter regression for the multimodal
    registration of 2-D T1 and T2 weighted brain MRs in an unsupervised capacity using
    pre-trained networks Kori et al. ([2018](#bib.bib54)). The images are binarized
    and then the Dice score between the moving and the fixed images is used as the
    cost function. As the appearance difference between these two modalities is not
    significant, the use of these pre-trained models can be reasonably effective.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Discussion and Assessment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Performing multimodal image registration in an unsupervised capacity is significantly
    more difficult than performing unimodal image registration because of the difficulty
    associated with using manually crafted similarity metrics to quantify the similarity
    between the two images, and generally using the unsupervised techniques described
    above to establish/detect voxel-to-voxel correspondence. The use of unsupervised
    learning to learn feature representations to determine an optimal transformation
    has generated significant interest from the research community recently. Along
    with the previously discussed unsupervised image registration method, we expect
    feature based unsupervised registration to continue to generate significant interest
    from the research community. Further, extension to the multimodal case (especially
    for applications that use image with significant appearance differences) is likely
    to be a prominent research focus in the next few years.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Research Trends and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we summarize the current research trends and future directions
    of deep learning in medical image registration. As we can see from Fig. [2](#S1.F2
    "Figure 2 ‣ 1 INTRODUCTION ‣ Deep Learning in Medical Image Registration: A Survey"),
    some research trends have emerged. First, deep learning based medical image registration
    seems to be following the observed trend for the general application of deep learning
    to medical image analysis. Second, unsupervised transformation estimation methods
    have been garnering more attention recently from the research community. Further,
    deep learning based methods consistently outperform traditional optimization based
    techniques Nazib et al. ([2018](#bib.bib80)). Based on the observed research trends,
    we speculate that the following research directions will receive more attention
    in the research community.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Deep Adversarial Image Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We further speculate that GANs will be used more frequently in deep learning
    based image registration in the next few years. As described above, GANs can serve
    several different purposes in deep learning based medical image registration:
    using a discriminator as a learned similarity metric, ensuring that predicted
    transformations are realistic, and using a GAN to perform image translation to
    transform a multimodal registration problem into a unimodal registration problem.'
  prefs: []
  type: TYPE_NORMAL
- en: GAN-like frameworks have been used in several works to directly train transformation
    predicting neural networks. Several recent works Fan et al. ([2018a](#bib.bib30));
    Yan et al. ([2018](#bib.bib111)) use a discriminator to discern between aligned
    and misaligned image pairs. Although the training paradigm borrows from an unsupervised
    training strategy, the discriminator requires pre-aligned image pairs. Therefore,
    it will have limited success in multimodal or challenging unimodal applications
    where it is difficult to register images. Because discriminators are trained to
    assign all misaligned image pairs the same label, they will likely be unable to
    model a spectrum of misalignments. Despite this limitation, the application of
    GANs to medical image registration are still quite promising and will be described
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Unconstrained deformation field prediction can result in warped moving images
    with unrealistic organ appearances. A common approach is to add the L2 norm of
    the predicted deformation field, its gradient, or its Laplacian to the loss function.
    However, the use of such regularization terms may limit the magnitude of the deformations
    that neural networks are able to predict. Therefore, Hu et al. Hu et al. ([2018a](#bib.bib42))
    explored the use of a GAN-like framework to produce realistic deformations. Constraining
    the deformation prediction using a discriminator results in superior performance
    relative to the use of L2 norm regularization in that work.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, GANs can be used to map medical images in a source domain (*e.g.* MR)
    to a target domain (*e.g.* CT) Choi et al. ([2018](#bib.bib19)); Isola et al.
    ([2017](#bib.bib46)); Liu et al. ([2017](#bib.bib66)); Yi et al. ([2017](#bib.bib116)),
    regardless of whether or not paired training data is available Zhu et al. ([2017](#bib.bib121)).
    This image appearance reduction technique would be advantageous because many unimodal
    unsupervised registration methods use similarity metrics that often fail in the
    multimodal case. If image translation is performed as a pre-processing step, then
    commonly used similarity metrics could be used to define the loss function of
    transformation predicting networks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Reinforcement Learning based Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We also project that reinforcement learning will also be more commonly used
    for medical image registration in the next few years because it is very intuitive
    and can mimic the manner in which physicians perform registration. It should be
    noted that there are some unique challenges associated with deep learning based
    medical image registration: including the dimensionality of the action space in
    the deformable registration case. However, we believe that such limitations are
    surmountable because there is already one proposed method that uses reinforcement
    learning based registration with a deformable transformation model Krebs et al.
    ([2017](#bib.bib55)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Raw Imaging Domain Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This article has focused on surveying methods performing registration using
    reconstructed images. However, we speculate that it is possible to incorporate
    reconstruction into an end-to-end deep learning based registration pipeline. In
    2016, Wang Wang ([2016](#bib.bib107)) postulated that deep neural networks could
    be used to perform image reconstruction. Further, several works Rivenson et al.
    ([2018](#bib.bib85)); Smith et al. ([2019](#bib.bib98)); Yao et al. ([2018](#bib.bib115));
    Zhu et al. ([2018](#bib.bib120)) recently demonstrated the ability of deep learning
    to map data points in the raw data domain to the reconstructed image domain. Therefore,
    it is reasonable to expect that registration pipelines that take raw data as input
    and output registered, reconstructed images can be developed within the next few
    years.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, the recent works that use deep learning to perform medical
    image registration have been examined. As each application has its own unique
    challenges, the creation of the deep learning based frameworks must be carefully
    designed. Many deep learning based medical image registration applications share
    similar challenges including the lack of a robust similarity metric for multimodal
    applications, in which there are significant image appearance differences and/or
    different fields of view (*e.g.* MR-TRUS registration) Haskins et al. ([2019](#bib.bib36)),
    the lack of availability of large datasets, the challenge associated with obtaining
    segmentations and ground truth registrations, and quantifying the uncertainty
    of a model’s prediction. Application-specific similarity metrics, patch-wise frameworks,
    unsupervised approaches, and variational autoencoder inspired registration frameworks
    are examples of popular solutions to these challenges. Furthermore, despite the
    sophistication of many of the methods discussed in this survey, resampling and
    interpolation are often not among the components of registration that are learned
    by the neural network. While researchers started to pay attention to this aspect
    Ali and Rittscher ([2019](#bib.bib2)), we expect more works to incorporate these
    components into their deep learning based methods as the field continues to mature.
    Recent successes have demonstrated the impact of the application of deep learning
    to medical image registration. This trend can be observed across medical imaging
    applications. Many future exciting works are sure to build on the recent progress
    that has been outlined in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abadi et al. (2016) Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
    J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. (2016). Tensorflow:
    a system for large-scale machine learning. In OSDI, volume 16, pages 265–283.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ali and Rittscher (2019) Ali, S. and Rittscher, J. (2019). Conv2warp: An unsupervised
    deformable image registration with continuous convolution and warping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alom et al. (2018) Alom, M. Z., Taha, T. M., Yakopcic, C., Westberg, S., Hasan,
    M., Van Esesn, B. C., Awwal, A. A. S., and Asari, V. K. (2018). The history began
    from alexnet: A comprehensive survey on deep learning approaches. arXiv preprint
    arXiv:1803.01164.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ambinder (2005) Ambinder, E. P. (2005). A history of the shift toward full computerization
    of medicine. Journal of oncology practice, 1(2):54–56.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arganda-Carreras et al. (2006) Arganda-Carreras, I., Sorzano, C. O., Marabini,
    R., Carazo, J. M., Ortiz-de Solorzano, C., and Kybic, J. (2006). Consistent and
    elastic registration of histological sections using vector-spline regularization.
    In International Workshop on Computer Vision Approaches to Medical Image Analysis,
    pages 85–95\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Avants et al. (2008) Avants, B. B., Epstein, C. L., Grossman, M., and Gee,
    J. C. (2008). Symmetric diffeomorphic image registration with cross-correlation:
    evaluating automated labeling of elderly and neurodegenerative brain. Medical
    image analysis, 12(1):26–41.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avants et al. (2011) Avants, B. B., Tustison, N. J., Song, G., Cook, P. A.,
    Klein, A., and Gee, J. C. (2011). A reproducible evaluation of ants similarity
    metric performance in brain image registration. Neuroimage, 54(3):2033–2044.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balakrishnan et al. (2018a) Balakrishnan, G., Zhao, A., Sabuncu, M. R., Guttag,
    J., and Dalca, A. V. (2018a). An unsupervised learning model for deformable medical
    image registration. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 9252–9260.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balakrishnan et al. (2018b) Balakrishnan, G., Zhao, A., Sabuncu, M. R., Guttag,
    J., and Dalca, A. V. (2018b). Voxelmorph: A learning framework for deformable
    medical image registration. arXiv preprint arXiv:1809.05231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blendowski and Heinrich (2018) Blendowski, M. and Heinrich, M. P. (2018). Combining
    mrf-based deformable registration and deep binary 3d-cnn descriptors for large
    lung motion estimation in copd patients. International journal of computer assisted
    radiology and surgery, pages 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2015) Cao, T., Singh, N., Jojic, V., and Niethammer, M. (2015).
    Semi-coupled dictionary learning for deformation prediction. In Biomedical Imaging
    (ISBI), 2015 IEEE 12th International Symposium on, pages 691–694\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2018) Cao, X., Yang, J., Wang, L., Xue, Z., Wang, Q., and Shen,
    D. (2018). Deep learning based inter-modality image registration supervised by
    intra-modality similarity. arXiv preprint arXiv:1804.10735.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2017) Cao, X., Yang, J., Zhang, J., Nie, D., Kim, M., Wang, Q.,
    and Shen, D. (2017). Deformable image registration based on similarity-steered
    cnn regression. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 300–308\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2015) Chan, T.-H., Jia, K., Gao, S., Lu, J., Zeng, Z., and Ma,
    Y. (2015). Pcanet: A simple deep learning baseline for image classification? IEEE
    Transactions on Image Processing, 24(12):5017–5032.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chee and Wu (2018) Chee, E. and Wu, J. (2018). Airnet: Self-supervised affine
    registration for 3d medical images using neural networks. arXiv preprint arXiv:1810.02583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2015) Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao,
    T., Xu, B., Zhang, C., and Zhang, Z. (2015). Mxnet: A flexible and efficient machine
    learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2016) Cheng, X., Zhang, L., and Zheng, Y. (2016). Deep similarity
    learning for multimodal medical images. In International conference on medical
    image computing and computer-assisted intervention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2018) Cheng, X., Zhang, L., and Zheng, Y. (2018). Deep similarity
    learning for multimodal medical images. Computer Methods in Biomechanics and Biomedical
    Engineering: Imaging & Visualization, 6(3):248–252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. (2018) Choi, Y., Choi, M., Kim, M., Ha, J.-W., Kim, S., and Choo,
    J. (2018). Stargan: Unified generative adversarial networks for multi-domain image-to-image
    translation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 8789–8797.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet et al. (2015) Chollet, F. et al. (2015). Keras.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalca et al. (2018) Dalca, A. V., Balakrishnan, G., Guttag, J., and Sabuncu,
    M. R. (2018). Unsupervised learning for fast probabilistic diffeomorphic registration.
    arXiv preprint arXiv:1805.04605.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De Silva et al. (2016) De Silva, T., Uneri, A., Ketcha, M., Reaungamornrat,
    S., Kleinszig, G., Vogt, S., Aygun, N., Lo, S., Wolinsky, J., and Siewerdsen,
    J. (2016). 3d–2d image registration for target localization in spine surgery:
    investigation of similarity metrics providing robustness to content mismatch.
    Physics in Medicine & Biology, 61(8):3009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Vos et al. (2018) de Vos, B. D., Berendsen, F. F., Viergever, M. A., Sokooti,
    H., Staring, M., and Išgum, I. (2018). A deep learning framework for unsupervised
    affine and deformable image registration. Medical Image Analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Vos et al. (2017) de Vos, B. D., Berendsen, F. F., Viergever, M. A., Staring,
    M., and Išgum, I. (2017). End-to-end unsupervised deformable image registration
    with a convolutional neural network. In Deep Learning in Medical Image Analysis
    and Multimodal Learning for Clinical Decision Support, pages 204–212\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doersch (2016) Doersch, C. (2016). Tutorial on variational autoencoders. arXiv
    preprint arXiv:1606.05908.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2015) Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P.,
    Hazirbas, C., Golkov, V., Van Der Smagt, P., Cremers, D., and Brox, T. (2015).
    Flownet: Learning optical flow with convolutional networks. In Proceedings of
    the IEEE International Conference on Computer Vision, pages 2758–2766.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ehrhardt et al. (2015) Ehrhardt, J., Schmidt-Richberg, A., Werner, R., and Handels,
    H. (2015). Variational registration. In Bildverarbeitung für die Medizin 2015,
    pages 209–214. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eppenhof and Pluim (2018a) Eppenhof, K. A. and Pluim, J. P. (2018a). Pulmonary
    ct registration through supervised learning with convolutional neural networks.
    IEEE transactions on medical imaging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eppenhof and Pluim (2018b) Eppenhof, K. A. J. and Pluim, J. P. (2018b). Error
    estimation of deformable image registration of pulmonary ct scans using convolutional
    neural networks. Journal of Medical Imaging, 5(2):024003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2018a) Fan, J., Cao, X., Xue, Z., Yap, P.-T., and Shen, D. (2018a).
    Adversarial similarity network for evaluating image alignment in deep learning
    based registration. In International Conference on Medical Image Computing and
    Computer-Assisted Intervention, pages 739–746\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2018b) Fan, J., Cao, X., Yap, P.-T., and Shen, D. (2018b). Birnet:
    Brain image registration using dual-supervised fully convolutional networks. arXiv
    preprint arXiv:1802.04692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferrante et al. (2018) Ferrante, E., Oktay, O., Glocker, B., and Milone, D. H.
    (2018). On the adaptability of unsupervised cnn-based deformable image registration
    to unseen image domains. In International Workshop on Machine Learning in Medical
    Imaging, pages 294–302\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghosal and Ray (2017) Ghosal, S. and Ray, N. (2017). Deep deformable registration:
    Enhancing accuracy by fully convolutional neural net. Pattern Recognition Letters,
    94:81–86.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2016) Goodfellow, I., Bengio, Y., Courville, A., and Bengio,
    Y. (2016). Deep learning, volume 1. MIT press Cambridge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative
    adversarial nets. In Advances in neural information processing systems, pages
    2672–2680.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haskins et al. (2019) Haskins, G., Kruecker, J., Kruger, U., Xu, S., Pinto,
    P. A., Wood, B. J., and Yan, P. (2019). Learning deep similarity metric for 3d
    mr-trus image registration. International Journal of Computer Assisted Radiology
    and Surgery, 14:417–425.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual
    learning for image recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinrich et al. (2012) Heinrich, M. P., Jenkinson, M., Bhushan, M., Matin,
    T., Gleeson, F. V., Brady, M., and Schnabel, J. A. (2012). Mind: Modality independent
    neighbourhood descriptor for multi-modal deformable registration. Medical image
    analysis, 16(7):1423–1435.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heinrich et al. (2013) Heinrich, M. P., Jenkinson, M., Papież, B. W., Brady,
    M., and Schnabel, J. A. (2013). Towards realtime multimodal fusion for image-guided
    interventions using self-similarities. In International conference on medical
    image computing and computer-assisted intervention, pages 187–194\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hering et al. (2018) Hering, A., Kuckertz, S., Heldmann, S., and Heinrich, M.
    (2018). Enhancing label-driven deep deformable image registration with local distance
    metrics for state-of-the-art cardiac motion tracking. arXiv preprint arXiv:1812.01859.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hill et al. (2001) Hill, D. L., Batchelor, P. G., Holden, M., and Hawkes, D. J.
    (2001). Medical image registration. Physics in medicine and biology, 46(3):R1–R45.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2018a) Hu, Y., Gibson, E., Ghavami, N., Bonmati, E., Moore, C. M.,
    Emberton, M., Vercauteren, T., Noble, J. A., and Barratt, D. C. (2018a). Adversarial
    deformation regularization for training image registration neural networks. arXiv
    preprint arXiv:1805.10665.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2018b) Hu, Y., Modat, M., Gibson, E., Ghavami, N., Bonmati, E., Moore,
    C. M., Emberton, M., Noble, J. A., Barratt, D. C., and Vercauteren, T. (2018b).
    Label-driven weakly-supervised learning for multimodal deformarle image registration.
    In Biomedical Imaging (ISBI 2018), 2018 IEEE 15th International Symposium on,
    pages 1070–1074\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2018c) Hu, Y., Modat, M., Gibson, E., Li, W., Ghavami, N., Bonmati,
    E., Wang, G., Bandula, S., Moore, C. M., Emberton, M., et al. (2018c). Weakly-supervised
    convolutional neural networks for multimodal image registration. Medical image
    analysis, 49:1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ikeda et al. (2014) Ikeda, K., Ino, F., and Hagihara, K. (2014). Efficient acceleration
    of mutual information computation for nonrigid registration using cuda. IEEE J.
    Biomedical and Health Informatics, 18(3):956–968.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isola et al. (2017) Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. (2017).
    Image-to-image translation with conditional adversarial networks. arXiv preprint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ito and Ino (2018) Ito, M. and Ino, F. (2018). An automated method for generating
    training sets for deep learning based image registration. In The 11th International
    Joint Conference on Biomedical Engineering Systems and Technologies - Volume 2:
    BIOIMAGING, pages 140–147. INSTICC, SciTePress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. (2015) Jaderberg, M., Simonyan, K., Zisserman, A., et al. (2015).
    Spatial transformer networks. In Advances in neural information processing systems,
    pages 2017–2025.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. (2014) Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J.,
    Girshick, R., Guadarrama, S., and Darrell, T. (2014). Caffe: Convolutional architecture
    for fast feature embedding. In Proceedings of the 22nd ACM international conference
    on Multimedia, pages 675–678\. ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang and Shackleford (2018) Jiang, P. and Shackleford, J. A. (2018). Cnn driven
    sparse multi-level b-spline image registration. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 9281–9289.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaelbling et al. (1996) Kaelbling, L. P., Littman, M. L., and Moore, A. W.
    (1996). Reinforcement learning: A survey. Journal of artificial intelligence research,
    4:237–285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kazeminia et al. (2018) Kazeminia, S., Baur, C., Kuijper, A., van Ginneken,
    B., Navab, N., Albarqouni, S., and Mukhopadhyay, A. (2018). Gans for medical image
    analysis. arXiv preprint arXiv:1809.06222.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klein et al. (2010) Klein, S., Staring, M., Murphy, K., Viergever, M. A., and
    Pluim, J. P. (2010). Elastix: a toolbox for intensity-based medical image registration.
    IEEE transactions on medical imaging, 29(1):196–205.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kori et al. (2018) Kori, A., Kumari, K., and Krishnamurthi, G. (2018). Zero
    shot learning for multi-modal real time image registration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krebs et al. (2017) Krebs, J., Mansi, T., Delingette, H., Zhang, L., Ghesu,
    F. C., Miao, S., Maier, A. K., Ayache, N., Liao, R., and Kamen, A. (2017). Robust
    non-rigid registration through agent-based action learning. In International Conference
    on Medical Image Computing and Computer-Assisted Intervention, pages 344–352\.
    Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krebs et al. (2018a) Krebs, J., Mansi, T., Mailhé, B., Ayache, N., and Delingette,
    H. (2018a). Learning structured deformations using diffeomorphic registration.
    arXiv preprint arXiv:1804.07172.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krebs et al. (2018b) Krebs, J., Mansi, T., Mailhé, B., Ayache, N., and Delingette,
    H. (2018b). Unsupervised probabilistic deformation modeling for robust diffeomorphic
    registration. In Deep Learning in Medical Image Analysis and Multimodal Learning
    for Clinical Decision Support, pages 101–109\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kuang and Schmah (2018) Kuang, D. and Schmah, T. (2018). Faim–a convnet method
    for unsupervised 3d medical image registration. arXiv preprint arXiv:1811.09243.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2017) Lee, J.-G., Jun, S., Cho, Y.-W., Lee, H., Kim, G. B., Seo,
    J. B., and Kim, N. (2017). Deep learning in medical imaging: general overview.
    Korean journal of radiology, 18(4):570–584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Fan (2017) Li, H. and Fan, Y. (2017). Non-rigid image registration using
    fully convolutional networks with deep self-supervision. arXiv preprint arXiv:1709.00799.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Fan (2018) Li, H. and Fan, Y. (2018). Non-rigid image registration using
    self-supervised fully convolutional networks without training data. In Biomedical
    Imaging (ISBI 2018), 2018 IEEE 15th International Symposium on, pages 1075–1078\.
    IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liao et al. (2017) Liao, R., Miao, S., de Tournemire, P., Grbic, S., Kamen,
    A., Mansi, T., and Comaniciu, D. (2017). An artificial agent for robust image
    registration. In AAAI, pages 4168–4175.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Litjens et al. (2017) Litjens, G., Kooi, T., Bejnordi, B. E., Setio, A. A. A.,
    Ciompi, F., Ghafoorian, M., van der Laak, J. A., Van Ginneken, B., and Sánchez,
    C. I. (2017). A survey on deep learning in medical image analysis. Medical image
    analysis, 42:60–88.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2011) Liu, C., Yuen, J., and Torralba, A. (2011). Sift flow: Dense
    correspondence across scenes and its applications. IEEE transactions on pattern
    analysis and machine intelligence, 33(5):978–994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018) Liu, J., Pan, Y., Li, M., Chen, Z., Tang, L., Lu, C., and
    Wang, J. (2018). Applications of deep learning to mri images: a survey. Big Data
    Mining and Analytics, 1(1):1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2017) Liu, M.-Y., Breuel, T., and Kautz, J. (2017). Unsupervised
    image-to-image translation networks. In Advances in Neural Information Processing
    Systems, pages 700–708.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Leung (2017) Liu, Q. and Leung, H. (2017). Tensor-based descriptor for
    image registration via unsupervised network. In Information Fusion (Fusion), 2017
    20th International Conference on, pages 1–7\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2015) Long, J., Shelhamer, E., and Darrell, T. (2015). Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431–3440.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lorenzi et al. (2013) Lorenzi, M., Ayache, N., Frisoni, G. B., Pennec, X.,
    (ADNI, A. D. N. I., et al. (2013). Lcc-demons: a robust and accurate symmetric
    diffeomorphic registration algorithm. NeuroImage, 81:470–483.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lv et al. (2018) Lv, J., Yang, M., Zhang, J., and Wang, X. (2018). Respiratory
    motion correction for free-breathing 3d abdominal mri using cnn-based image registration:
    a feasibility study. The British journal of radiology, 91(xxxx):20170788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2017) Ma, K., Wang, J., Singh, V., Tamersoy, B., Chang, Y.-J., Wimmer,
    A., and Chen, T. (2017). Multimodal image registration with deep context reinforcement
    learning. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 240–248\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maes et al. (1997) Maes, F., Collignon, A., Vandermeulen, D., Marchal, G., and
    Suetens, P. (1997). Multimodality image registration by maximization of mutual
    information. IEEE transactions on Medical Imaging, 16(2):187–198.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahapatra (2018) Mahapatra, D. (2018). Elastic registration of medical images
    with gans. arXiv preprint arXiv:1805.02369.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahapatra et al. (2018) Mahapatra, D., Ge, Z., Sedai, S., and Chakravorty, R.
    (2018). Joint registration and segmentation of xray images using generative adversarial
    networks. In International Workshop on Machine Learning in Medical Imaging, pages
    73–80\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matthew et al. (2018) Matthew, J., Hajnal, J. V., Rueckert, D., and Schnabel,
    J. A. (2018). Lstm spatial co-transformer networks for registration of 3d fetal
    us and mr brain images. In Data Driven Treatment Response Assessment and Preterm,
    Perinatal, and Paediatric Image Analysis, pages 149–159\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao et al. (2017) Miao, S., Piat, S., Fischer, P., Tuysuzoglu, A., Mewes, P.,
    Mansi, T., and Liao, R. (2017). Dilated fcn for multi-agent 2d/3d medical image
    registration. arXiv preprint arXiv:1712.01651.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao et al. (2016a) Miao, S., Wang, Z. J., and Liao, R. (2016a). A cnn regression
    approach for real-time 2d/3d registration. IEEE transactions on medical imaging,
    35(5):1352–1363.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao et al. (2016b) Miao, S., Wang, Z. J., Zheng, Y., and Liao, R. (2016b).
    Real-time 2d/3d registration via cnn regression. In Biomedical Imaging (ISBI),
    2016 IEEE 13th International Symposium on, pages 1430–1434\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Myronenko and Song (2010) Myronenko, A. and Song, X. (2010). Intensity-based
    image registration by minimizing residual complexity. IEEE transactions on medical
    imaging, 29(11):1882–1891.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nazib et al. (2018) Nazib, A., Fookes, C., and Perrin, D. (2018). A comparative
    analysis of registration tools: Traditional vs deep learning approach on high
    resolution tissue cleared data. arXiv preprint arXiv:1810.08315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neylon et al. (2017) Neylon, J., Min, Y., Low, D. A., and Santhanam, A. (2017).
    A neural network approach for fast, automated quantification of dir performance.
    Medical physics, 44(8):4126–4138.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paszke et al. (2017) Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang,
    E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. (2017). Automatic
    differentiation in pytorch. In NIPS-W.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Punithakumar et al. (2017) Punithakumar, K., Boulanger, P., and Noga, M. (2017).
    A gpu-accelerated deformable image registration algorithm with applications to
    right ventricular segmentation. IEEE Access, 5:20374–20382.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Ren, S., He, K., Girshick, R., and Sun, J. (2015). Faster
    r-cnn: Towards real-time object detection with region proposal networks. In Advances
    in neural information processing systems, pages 91–99.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rivenson et al. (2018) Rivenson, Y., Zhang, Y., Günaydın, H., Teng, D., and
    Ozcan, A. (2018). Phase recovery and holographic image reconstruction using deep
    learning in neural networks. Light: Science & Applications, 7(2):17141.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rohé et al. (2017) Rohé, M.-M., Datar, M., Heimann, T., Sermesant, M., and
    Pennec, X. (2017). Svf-net: Learning deformable image registration using shape
    matching. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 266–274\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. (2015).
    U-net: Convolutional networks for biomedical image segmentation. In International
    Conference on Medical image computing and computer-assisted intervention, pages
    234–241\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rühaak et al. (2013) Rühaak, J., Heldmann, S., Kipshagen, T., and Fischer,
    B. (2013). Highly accurate fast lung ct registration. In Medical Imaging 2013:
    Image Processing, volume 8669, page 86690Y. International Society for Optics and
    Photonics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saalfeld et al. (2012) Saalfeld, S., Fetter, R., Cardona, A., and Tomancak,
    P. (2012). Elastic volume reconstruction from series of ultra-thin microscopy
    sections. Nature methods, 9(7):717.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salehi et al. (2018) Salehi, S. S. M., Khan, S., Erdogmus, D., and Gholipour,
    A. (2018). Real-time deep registration with geodesic loss. arXiv preprint arXiv:1803.05982.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmidhuber (2015) Schmidhuber, J. (2015). Deep learning in neural networks:
    An overview. Neural networks, 61:85–117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sedghi et al. (2018) Sedghi, A., Luo, J., Mehrtash, A., Pieper, S., Tempany,
    C. M., Kapur, T., Mousavi, P., and Wells III, W. M. (2018). Semi-supervised deep
    metrics for image registration. arXiv preprint arXiv:1804.01565.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheikhjafari et al. (2018) Sheikhjafari, A., Noga, M., Punithakumar, K., and
    Ray, N. (2018). Unsupervised deformable image registration with fully connected
    generative neural network. In International conference on Medical Imaging with
    Deep Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen (2007) Shen, D. (2007). Image registration by local histogram matching.
    Pattern Recognition, 40(4):1161–1172.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shu et al. (2018) Shu, C., Chen, X., Xie, Q., and Han, H. (2018). An unsupervised
    network for fast microscopic image registration. In Medical Imaging 2018: Digital
    Pathology, volume 10581, page 105811D. International Society for Optics and Photonics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonovsky et al. (2016) Simonovsky, M., Gutiérrez-Becker, B., Mateus, D., Navab,
    N., and Komodakis, N. (2016). A deep metric for multimodal registration. In International
    Conference on Medical Image Computing and Computer-Assisted Intervention, pages
    10–18\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sloan et al. (2018) Sloan, J. M., Goatman, K. A., and Siebert, J. P. (2018).
    Learning rigid image registration - utilizing convolutional neural networks for
    medical image registration. In 11th International Joint Conference on Biomedical
    Engineering Systems and Technologies, pages 89–99\. SCITEPRESS-Science and Technology
    Publications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smith et al. (2019) Smith, J. T., Yao, R., Sinsuebphon, N., Rudkouskaya, A.,
    Un, N., Mazurkiewicz, J., Barroso, M., Yan, P., and Intes, X. (2019). Fast fit-free
    analysis of fluorescence lifetime imaging via deep learning. Proceedings of the
    National Academy of Sciences, 116(48):24019–24030.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sokooti et al. (2017) Sokooti, H., de Vos, B., Berendsen, F., Lelieveldt, B. P.,
    Išgum, I., and Staring, M. (2017). Nonrigid image registration using multi-scale
    3d convolutional neural networks. In International Conference on Medical Image
    Computing and Computer-Assisted Intervention, pages 232–239\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stergios et al. (2018) Stergios, C., Mihir, S., Maria, V., Guillaume, C., Marie-Pierre,
    R., Stavroula, M., and Nikos, P. (2018). Linear and deformable image registration
    with 3d convolutional neural networks. In Image Analysis for Moving Organ, Breast,
    and Thoracic Images, pages 13–22\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun and Zhang (2018) Sun, L. and Zhang, S. (2018). Deformable mri-ultrasound
    registration using 3d convolutional neural network. In Simulation, Image Processing,
    and Ultrasound Systems for Assisted Diagnosis and Navigation, pages 152–158\.
    Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2018) Sun, Y., Moelker, A., Niessen, W. J., and van Walsum, T. (2018).
    Towards robust ct-ultrasound registration using deep learning methods. In Understanding
    and Interpreting Machine Learning in Medical Image Computing Applications, pages
    43–51\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uzunova et al. (2017) Uzunova, H., Wilms, M., Handels, H., and Ehrhardt, J.
    (2017). Training cnns for image registration from few samples with model-based
    data augmentation. In International Conference on Medical Image Computing and
    Computer-Assisted Intervention, pages 223–231\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vercauteren et al. (2009) Vercauteren, T., Pennec, X., Perchant, A., and Ayache,
    N. (2009). Diffeomorphic demons: Efficient non-parametric image registration.
    NeuroImage, 45(1):S61–S72.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vialard et al. (2012) Vialard, F.-X., Risser, L., Rueckert, D., and Cotter,
    C. J. (2012). Diffeomorphic 3d image registration via geodesic shooting using
    an efficient adjoint calculation. International Journal of Computer Vision, 97(2):229–241.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viola and Wells III (1997) Viola, P. and Wells III, W. M. (1997). Alignment
    by maximization of mutual information. International journal of computer vision,
    24(2):137–154.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang (2016) Wang, G. (2016). A perspective on deep imaging. arXiv preprint arXiv:1609.04375.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2015) Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot,
    M., and De Freitas, N. (2015). Dueling network architectures for deep reinforcement
    learning. arXiv preprint arXiv:1511.06581.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2013) Wu, G., Kim, M., Wang, Q., Gao, Y., Liao, S., and Shen, D.
    (2013). Unsupervised deep feature learning for deformable registration of mr brain
    images. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 649–656\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2016) Wu, G., Kim, M., Wang, Q., Munsell, B. C., and Shen, D. (2016).
    Scalable high-performance image registration framework by unsupervised deep feature
    representations learning. IEEE Transactions on Biomedical Engineering, 63(7):1505–1516.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2018) Yan, P., Xu, S., Rastinehad, A. R., and Wood, B. J. (2018).
    Adversarial image registration with application for mr and trus image fusion.
    arXiv preprint arXiv:1804.11024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2018) Yang, Q., Yan, P., Zhang, Y., Yu, H., Shi, Y., Mou, X., Kalra,
    M. K., Zhang, Y., Sun, L., and Wang, G. (2018). Low dose ct image denoising using
    a generative adversarial network with wasserstein distance and perceptual loss.
    IEEE transactions on medical imaging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang (2017) Yang, X. (2017). Uncertainty Quantification, Image Synthesis and
    Deformation Prediction for Image Registration. PhD thesis, The University of North
    Carolina at Chapel Hill.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2016) Yang, X., Kwitt, R., and Niethammer, M. (2016). Fast predictive
    image registration. In Deep Learning and Data Labeling for Medical Applications,
    pages 48–57\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2018) Yao, R., Ochoa, M., Intes, X., and Yan, P. (2018). Deep compressive
    macroscopic fluorescence lifetime imaging. In Biomedical Imaging (ISBI 2018),
    2018 IEEE 15th International Symposium on, pages 908–911\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. (2017) Yi, Z., Zhang, H., Tan, P., and Gong, M. (2017). Dualgan:
    Unsupervised dual learning for image-to-image translation. arXiv preprint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yoo et al. (2017) Yoo, I., Hildebrand, D. G., Tobin, W. F., Lee, W.-C. A.,
    and Jeong, W.-K. (2017). ssemnet: Serial-section electron microscopy image registration
    using a spatial transformer network with learned features. In Deep Learning in
    Medical Image Analysis and Multimodal Learning for Clinical Decision Support,
    pages 249–257\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang (2018) Zhang, J. (2018). Inverse-consistent deep networks for unsupervised
    deformable image registration. arXiv preprint arXiv:1809.03443.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2018) Zheng, J., Miao, S., Wang, Z. J., and Liao, R. (2018). Pairwise
    domain adaptation module for cnn-based 2-d/3-d registration. Journal of Medical
    Imaging, 5(2):021204.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2018) Zhu, B., Liu, J. Z., Cauley, S. F., Rosen, B. R., and Rosen,
    M. S. (2018). Image reconstruction by domain-transform manifold learning. Nature,
    555(7697):487.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017) Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. (2017).
    Unpaired image-to-image translation using cycle-consistent adversarial networks.
    arXiv preprint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zitova and Flusser (2003) Zitova, B. and Flusser, J. (2003). Image registration
    methods: a survey. Image and vision computing, 21(11):977–1000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
