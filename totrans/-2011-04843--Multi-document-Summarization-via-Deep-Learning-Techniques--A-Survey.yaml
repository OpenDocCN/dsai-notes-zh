- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:58:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:58:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2011.04843] Multi-document Summarization via Deep Learning Techniques: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2011.04843] 通过深度学习技术的多文档摘要生成：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2011.04843](https://ar5iv.labs.arxiv.org/html/2011.04843)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2011.04843](https://ar5iv.labs.arxiv.org/html/2011.04843)
- en: 'Multi-document Summarization via Deep Learning Techniques: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过深度学习技术的多文档摘要生成：综述
- en: CONGBO MA The University of Adelaide [congbo.ma@adelaide.edu.au](mailto:congbo.ma@adelaide.edu.au)
    ,  WEI EMMA ZHANG The University of Adelaide [wei.e.zhang@adelaide.edu.au](mailto:wei.e.zhang@adelaide.edu.au)
    ,  MINGYU GUO The University of Adelaide [mingyu.guo@adelaide.edu.au](mailto:mingyu.guo@adelaide.edu.au)
    ,  HU WANG The University of Adelaide [hu.wang@adelaide.edu.au](mailto:hu.wang@adelaide.edu.au)
     and  QUAN Z. SHENG Macquarie University [michael.sheng@mq.edu.au](mailto:michael.sheng@mq.edu.au)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: CONGBO MA 阿德莱德大学 [congbo.ma@adelaide.edu.au](mailto:congbo.ma@adelaide.edu.au)，
    WEI EMMA ZHANG 阿德莱德大学 [wei.e.zhang@adelaide.edu.au](mailto:wei.e.zhang@adelaide.edu.au)，
    MINGYU GUO 阿德莱德大学 [mingyu.guo@adelaide.edu.au](mailto:mingyu.guo@adelaide.edu.au)，
    HU WANG 阿德莱德大学 [hu.wang@adelaide.edu.au](mailto:hu.wang@adelaide.edu.au) 和 QUAN
    Z. SHENG 麦考瑞大学 [michael.sheng@mq.edu.au](mailto:michael.sheng@mq.edu.au)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Multi-document summarization (MDS) is an effective tool for information aggregation
    that generates an informative and concise summary from a cluster of topic-related
    documents. Our survey, the first of its kind, systematically overviews the recent
    deep learning based MDS models. We propose a novel taxonomy to summarize the design
    strategies of neural networks and conduct a comprehensive summary of the state-of-the-art.
    We highlight the differences between various objective functions that are rarely
    discussed in the existing literature. Finally, we propose several future directions
    pertaining to this new and exciting field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 多文档摘要生成（MDS）是一种有效的信息聚合工具，它从一组相关主题的文档中生成一个信息丰富且简洁的摘要。我们的综述是首个此类综述，系统性地回顾了近期基于深度学习的MDS模型。我们提出了一种新的分类法来总结神经网络的设计策略，并对最先进技术进行了全面的总结。我们强调了现有文献中鲜有讨论的各种目标函数之间的差异。最后，我们提出了几个与这一新兴且令人兴奋的领域相关的未来研究方向。
- en: 'Multi-document summarization, Deep neural networks, Machine learning^†^†ccs:
    Computing methodologies Natural language processing^†^†ccs: Computing methodologies Machine
    learning algorithms^†^†ccs: Computing methodologies Information extraction'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '多文档摘要生成，深度神经网络，机器学习^†^†ccs: 计算方法 自然语言处理^†^†ccs: 计算方法 机器学习算法^†^†ccs: 计算方法 信息提取'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: In this era of rapidly advancing technology, the exponential increase of data
    availability makes analyzing and understanding text files a tedious, labor-intensive,
    and time-consuming task (Oussous et al., [2018](#bib.bib118); Hu et al., [2017](#bib.bib64)).
    The need to process this abundance of text data rapidly and efficiently calls
    for new, effective text summarization techniques. Text summarization is a key
    natural language processing (NLP) tasks that automatically converts a text, or
    a collection of texts within the same topic, into a concise summary that contains
    key semantic information which can be beneficial for many downstream applications
    such as creating news digests, search engine, and report generation (Paulus et al.,
    [2018](#bib.bib124)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术迅猛发展的时代，数据可用性的指数增长使得分析和理解文本文件成为一项繁琐、劳动密集且耗时的任务（Oussous et al., [2018](#bib.bib118);
    Hu et al., [2017](#bib.bib64)）。快速高效地处理这些大量文本数据的需求催生了新的有效文本摘要技术。文本摘要是一个关键的自然语言处理（NLP）任务，它自动将一段文本或同一主题的文本集合转换为包含关键信息的简洁摘要，这对许多下游应用（如新闻摘要生成、搜索引擎和报告生成）是非常有益的（Paulus
    et al., [2018](#bib.bib124)）。
- en: Text can be summarized from one or several documents, resulting in single document
    summarization (SDS) and multi-document summarization (MDS). While simpler to perform,
    SDS may not produce comprehensive summaries because it does not make good use
    of related, or more recent, documents. Conversely, MDS generates more comprehensive
    and accurate summaries from documents written at different times, covering different
    perspectives, but is accordingly more complicated as it tries to resolve potentially
    diverse and redundant information (Tas and Kiyani, [2007](#bib.bib147)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 文本可以从一个或多个文档中总结，产生单文档摘要（SDS）和多文档摘要（MDS）。虽然SDS更简单易行，但可能不会产生全面的摘要，因为它未能充分利用相关或更近期的文档。相反，MDS从不同时间撰写的文档中生成更全面和准确的摘要，涵盖了不同的观点，但相应地更复杂，因为它尝试解决可能存在的多样和冗余信息（Tas
    和 Kiyani，[2007](#bib.bib147)）。
- en: In addition, excessively long input documents often lead to model degradation
    (Jin et al., [2020](#bib.bib72)). It is challenging for models to retain the most
    critical contents of complex input sequences while generating a coherent, non-redundant,
    factual consistent and grammatically readable summary. Therefore, MDS requires
    models to have stronger capabilities for analyzing the input documents, identifying
    and merging consistent information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，过长的输入文档通常会导致模型性能下降（Jin 等，[2020](#bib.bib72)）。对于模型来说，保留复杂输入序列的最关键内容，同时生成连贯、不冗余、事实一致且语法可读的摘要是一项挑战。因此，MDS要求模型具备更强的能力来分析输入文档，识别和合并一致的信息。
- en: 'MDS enjoys a wide range of real-world applications, including summarization
    of news (Fabbri et al., [2019](#bib.bib44)), scientific publications (Yasunaga
    et al., [2019](#bib.bib167)), emails (Carenini et al., [2007](#bib.bib23); Zajic
    et al., [2008](#bib.bib171)), product reviews (Gerani et al., [2014](#bib.bib50)),
    medical documents (Afantenos et al., [2005](#bib.bib2)), lecture feedback (Luo
    and Litman, [2015](#bib.bib99); Luo et al., [2016](#bib.bib100)), software project
    activities (Alghamdi et al., [2020](#bib.bib3)), and Wikipedia articles generation
    (Liu et al., [2018](#bib.bib94)). Recently, MDS technology has also received a
    great amount of industry attention; an intelligent multilingual news reporter
    bot named Xiaomingbot (Xu et al., [2020a](#bib.bib161)) was developed for news
    generation, which can summarize multiple news sources into one article and translate
    it into multiple languages. Massive application requirements and rapidly growing
    online data have promoted the development of MDS. Existing methods using traditional
    algorithms are based on: term frequency-inverse document frequency (TF-IDF) (Radev
    et al., [2004](#bib.bib128); Baralis et al., [2012](#bib.bib11)), clustering (Goldstein
    et al., [2000](#bib.bib52); Wan and Yang, [2008](#bib.bib155)), graphs (Mani and
    Bloedorn, [1997](#bib.bib102); Wan and Yang, [2006](#bib.bib154)) and latent semantic
    analysis (Arora and Ravindran, [2008](#bib.bib8); Haghighi and Vanderwende, [2009](#bib.bib59)).
    Most of these works still generate summaries with manually crafted features (Mihalcea
    and Tarau, [2005](#bib.bib106); Wan and Yang, [2006](#bib.bib154)), such as sentence
    position features (Baxendale, [1958](#bib.bib12); Erkan and Radev, [2004](#bib.bib41)),
    sentence length features (Erkan and Radev, [2004](#bib.bib41)), proper noun features
    (Vodolazova et al., [2013](#bib.bib153)), cue-phrase features (Gupta and Lehal,
    [2010](#bib.bib58)), biased word features, sentence-to-sentence cohesion and sentence-to-centroid
    cohesion. Deep learning has gained enormous attention in recent years due to its
    success in various domains, for instance, computer vision (Krizhevsky et al.,
    [2012](#bib.bib79)), natural language processing (Devlin et al., [2014](#bib.bib36))
    and multi-modal learning (Wang et al., [2020b](#bib.bib157)). Both industry and
    academia have embraced deep learning to solve complex tasks due to its capability
    of capturing highly nonlinear relations of data. Moreover, deep learning based
    models reduce dependence on manual feature extraction and pre-knowledge in the
    field of linguistics, drastically improving the ease of engineering (Torfi et al.,
    [2020](#bib.bib148)). Therefore, deep learning based methods demonstrate outstanding
    performance in MDS tasks in most cases (Li et al., [2020a](#bib.bib91); Cao et al.,
    [2015b](#bib.bib21); Lu et al., [2020](#bib.bib98); Liu and Lapata, [2019](#bib.bib95);
    Lebanoff et al., [2019](#bib.bib83)). With recently dramatic improvements in computational
    power and the release of increasing numbers of public datasets, neural networks
    with deeper layers and more complex structures have been applied in MDS (Liu and
    Lapata, [2019](#bib.bib95); Li et al., [2017b](#bib.bib90)), accelerating the
    development of text summarization with more powerful and robust models. These
    tasks are attracting attention in the natural language processing community; the
    number of research publications on deep learning based MDS has increased rapidly
    over the last five years.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MDS 在现实世界中有广泛的应用，包括新闻摘要（Fabbri et al., [2019](#bib.bib44)）、科学出版物（Yasunaga et
    al., [2019](#bib.bib167)）、电子邮件（Carenini et al., [2007](#bib.bib23); Zajic et al.,
    [2008](#bib.bib171)）、产品评论（Gerani et al., [2014](#bib.bib50)）、医学文档（Afantenos et
    al., [2005](#bib.bib2)）、讲座反馈（Luo and Litman, [2015](#bib.bib99); Luo et al., [2016](#bib.bib100)）、软件项目活动（Alghamdi
    et al., [2020](#bib.bib3)）以及维基百科文章生成（Liu et al., [2018](#bib.bib94)）。最近，MDS 技术也受到了大量行业关注；一个名为
    Xiaomingbot 的智能多语言新闻报道机器人（Xu et al., [2020a](#bib.bib161)）被开发用于新闻生成，能够将多个新闻来源汇总为一篇文章并翻译成多种语言。大量的应用需求和快速增长的在线数据促进了
    MDS 的发展。现有使用传统算法的方法基于：词频-逆文档频率（TF-IDF）（Radev et al., [2004](#bib.bib128); Baralis
    et al., [2012](#bib.bib11)）、聚类（Goldstein et al., [2000](#bib.bib52); Wan and Yang,
    [2008](#bib.bib155)）、图（Mani and Bloedorn, [1997](#bib.bib102); Wan and Yang, [2006](#bib.bib154)）和潜在语义分析（Arora
    and Ravindran, [2008](#bib.bib8); Haghighi and Vanderwende, [2009](#bib.bib59)）。这些工作大多数仍生成具有人工特征（Mihalcea
    and Tarau, [2005](#bib.bib106); Wan and Yang, [2006](#bib.bib154)）的摘要，如句子位置特征（Baxendale,
    [1958](#bib.bib12); Erkan and Radev, [2004](#bib.bib41)）、句子长度特征（Erkan and Radev,
    [2004](#bib.bib41)）、专有名词特征（Vodolazova et al., [2013](#bib.bib153)）、提示短语特征（Gupta
    and Lehal, [2010](#bib.bib58)）、偏向词特征、句子间的连贯性和句子与中心点的连贯性。深度学习由于在计算机视觉（Krizhevsky
    et al., [2012](#bib.bib79)）、自然语言处理（Devlin et al., [2014](#bib.bib36)）和多模态学习（Wang
    et al., [2020b](#bib.bib157)）等各个领域取得的成功，近年来引起了极大的关注。由于深度学习能够捕捉数据的高度非线性关系，业界和学界都开始采用深度学习来解决复杂任务。此外，基于深度学习的模型减少了对手动特征提取和语言学领域先验知识的依赖，极大地提高了工程的便利性（Torfi
    et al., [2020](#bib.bib148)）。因此，基于深度学习的方法在大多数 MDS 任务中表现出色（Li et al., [2020a](#bib.bib91);
    Cao et al., [2015b](#bib.bib21); Lu et al., [2020](#bib.bib98); Liu and Lapata,
    [2019](#bib.bib95); Lebanoff et al., [2019](#bib.bib83)）。随着计算能力的剧增和越来越多公共数据集的发布，具有更深层次和更复杂结构的神经网络已应用于
    MDS（Liu and Lapata, [2019](#bib.bib95); Li et al., [2017b](#bib.bib90)），加速了更强大和更稳健的文本摘要模型的发展。这些任务在自然语言处理社区中引起了关注；基于深度学习的
    MDS 研究出版物数量在过去五年中迅速增加。
- en: '![Refer to caption](img/cbad3dcb98d0db2de34ab13259562446.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cbad3dcb98d0db2de34ab13259562446.png)'
- en: Figure 1\. Hierarchical Structure of This Survey.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 本次调查的层次结构。
- en: 'The prosperity of deep learning for summarization in both academia and industry
    requires a comprehensive review of current publications for researchers to better
    understand the process and research progress. However, most of the existing summarization
    survey papers are based on traditional algorithms instead of deep learning based
    methods or target general text summarization (Nenkova and McKeown, [2012](#bib.bib114);
    Haque et al., [2013](#bib.bib60); Ferreira et al., [2014](#bib.bib46); Shah and
    Jivani, [2016](#bib.bib139); El-Kassas et al., [2021](#bib.bib39)). We have therefore
    surveyed recent publications on deep learning methods for MDS that, to the best
    of our knowledge, is the first comprehensive survey of this field. This survey
    has been designed to classify neural based MDS techniques into diverse categories
    thoroughly and systematically. We also conduct a detailed discussion on the categorization
    and progress of these approaches to establish a clearer concept standing in the
    shoes of readers. We hope this survey provides a panorama for researchers, practitioners
    and educators to quickly understand and step into the field of deep learning based
    MDS. The key contributions of this survey are three-fold:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在学术界和工业界对摘要的繁荣需要对当前出版物进行全面回顾，以便研究人员更好地理解过程和研究进展。然而，大多数现有的摘要调查论文是基于传统算法而不是基于深度学习的方法，或者针对一般文本摘要（Nenkova
    和 McKeown，[2012](#bib.bib114)；Haque 等，[2013](#bib.bib60)；Ferreira 等，[2014](#bib.bib46)；Shah
    和 Jivani，[2016](#bib.bib139)；El-Kassas 等，[2021](#bib.bib39)）。因此，我们调查了有关深度学习方法的
    MDS 的最新出版物，据我们所知，这是该领域首个全面的调查。本次调查旨在将基于神经网络的 MDS 技术分类为不同的类别，并对这些方法的分类和进展进行详细讨论，以建立读者更清晰的概念。我们希望本次调查为研究人员、从业者和教育者提供一个全景，帮助他们快速了解并进入深度学习基于
    MDS 的领域。本次调查的关键贡献有三点：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a categorization scheme to organize current research and provide
    a comprehensive review for deep learning based MDS techniques, including deep
    learning based models, objective functions, benchmark datasets and evaluation
    metrics.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种分类方案，以组织当前的研究，并为基于深度学习的 MDS 技术提供全面的回顾，包括基于深度学习的模型、目标函数、基准数据集和评估指标。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review development movements and provide a systematic overview and summary
    of the state-of-the-art. We also summarize nine network design strategies based
    on our extensive studies of the current models.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了发展动态，提供了对最前沿技术的系统概述和总结。我们还总结了基于我们对当前模型的广泛研究的九种网络设计策略。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss the open issues of deep learning based multi-document summarization
    and identify the future research directions of this field. We also propose potential
    solutions for some discussed research directions.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了深度学习多文档摘要的开放问题，并确定了该领域的未来研究方向。我们还为一些讨论的研究方向提出了潜在的解决方案。
- en: Paper Selection. We used Google Scholar as the main search engine to select
    representative works from 2015 to 2021\. High-quality papers were selected from
    top NLP and AI journals and conferences, include ACL¹¹1Annual Meeting of the Association
    for Computational Linguistics., EMNLP²²2Empirical Methods in Natural Language
    Processing., COLING³³3International Conference on Computational Linguistics, NAACL⁴⁴4Annual
    Conference of the North American Chapter of the Association for Computational
    Linguistics., AAAI⁵⁵5AAAI Conference on Artificial Intelligence., ICML⁶⁶6International
    Conference on Machine Learning., ICLR⁷⁷7International Conference on Learning Representations
    and IJCAI⁸⁸8International Joint Conference on Artificial Intelligence.. The major
    keywords we used include multi-documentation summarization, summarization, extractive
    summarization, abstractive summarization, deep learning and neural networks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 论文选择。我们使用 Google Scholar 作为主要搜索引擎，从 2015 年到 2021 年选择了具有代表性的工作。我们从顶级 NLP 和 AI
    期刊及会议中选择了高质量的论文，包括 ACL¹¹1计算语言学协会年会，EMNLP²²2自然语言处理的经验方法，COLING³³3计算语言学国际会议，NAACL⁴⁴4北美计算语言学协会年会，AAAI⁵⁵5人工智能
    AAAI 会议，ICML⁶⁶6机器学习国际会议，ICLR⁷⁷7学习表征国际会议和 IJCAI⁸⁸8国际联合人工智能会议。我们使用的主要关键词包括多文档摘要、摘要、抽取式摘要、生成式摘要、深度学习和神经网络。
- en: 'Organization of the Survey. This survey will cover various aspects of recent
    advanced deep learning based works in MDS. Our proposed taxonomy categorizes the
    works from six aspects (Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")). To be more self-contained,
    in Section [2](#S2 "2\. From Single to Multi-document Summarization ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey"), we give the problem definition,
    the processing framework of text summarization, discuss similarities and differences
    of between SDS and MDS. Nine deep learning architecture design strategies, six
    deep learning based methods, and the variant tasks of MDS are presented in Section
    [3](#S3 "3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey"). Section [4](#S4 "4\. Objective
    Functions ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")
    summarizes objective functions that guide the model optimization process in the
    reviewed literature while evaluation metrics in Section [5](#S5 "5\. Evaluation
    metrics ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")
    help readers choose suitable indices to evaluate the effectiveness of a model.
    Section [6](#S6 "6\. Datasets ‣ Multi-document Summarization via Deep Learning
    Techniques: A Survey") summarizes standard and the variant MDS datasets. Finally,
    Section [7](#S7 "7\. Future research directions and open issues ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey") discusses future research
    directions for deep learning based MDS followed by conclusions in Section [8](#S8
    "8\. Conclusion ‣ Multi-document Summarization via Deep Learning Techniques: A
    Survey").'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 调查的组织结构。本调查将涵盖MDS中最近先进的深度学习工作的各个方面。我们提出的分类法从六个方面对这些工作进行分类（见图 [1](#S1.F1 "图 1
    ‣ 1\. 引言 ‣ 基于深度学习的多文档摘要技术：综述")）。为了更自成体系，在第 [2](#S2 "2\. 从单文档到多文档摘要 ‣ 基于深度学习的多文档摘要技术：综述")
    节中，我们给出问题定义、文本摘要的处理框架，讨论SDS和MDS之间的相似性和差异。在第 [3](#S3 "3\. 基于深度学习的多文档摘要方法 ‣ 基于深度学习的多文档摘要技术：综述")
    节中，介绍了九种深度学习架构设计策略、六种基于深度学习的方法以及MDS的变体任务。第 [4](#S4 "4\. 目标函数 ‣ 基于深度学习的多文档摘要技术：综述")
    节总结了指导模型优化过程的目标函数，而第 [5](#S5 "5\. 评估指标 ‣ 基于深度学习的多文档摘要技术：综述") 节中的评估指标帮助读者选择合适的指标来评估模型的有效性。第
    [6](#S6 "6\. 数据集 ‣ 基于深度学习的多文档摘要技术：综述") 节总结了标准和变体MDS数据集。最后，第 [7](#S7 "7\. 未来研究方向和未解问题
    ‣ 基于深度学习的多文档摘要技术：综述") 节讨论了基于深度学习的MDS的未来研究方向，并在第 [8](#S8 "8\. 结论 ‣ 基于深度学习的多文档摘要技术：综述")
    节中作出结论。
- en: '![Refer to caption](img/9bc5fa7d2ef048754fd1f844270e72b7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9bc5fa7d2ef048754fd1f844270e72b7.png)'
- en: Figure 2\. The Processing Framework of Text Summarization. Each of the highlighted
    steps (the one with triangle mark) indicates the differences between SDS and MDS.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 文本摘要的处理框架。每个突出显示的步骤（带有三角形标记的步骤）表示SDS和MDS之间的差异。
- en: 2\. From Single to Multi-document Summarization
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 从单文档到多文档摘要
- en: Before we dive into the details of existing deep learning based techniques,
    we start by defining SDS and MDS, and introducing the concepts used un both methods.
    The aim of MDS is to generate a concise and informative summary $Sum$ from a collection
    of documents $D$. $D$ denotes a cluster of topic-related documents $\left\{d_{i}\mid
    i\in[1,N]\right\}$, where $N$ is the number of documents. Each document $d_{i}$
    consists of $M_{d_{i}}$ sentences $\left\{s_{i,j}\mid j\in[1,M_{d_{i}}]\right\}$.
    $s_{i,j}$ refers to the $j$-th sentence in the $i$-th document. The standard summary
    $Ref$ is called the golden summary or reference summary. Currently, most golden
    summaries are written by experts. We keep this notation consistent throughout
    the article.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入现有基于深度学习的技术细节之前，我们首先定义SDS和MDS，并介绍这两种方法中使用的概念。MDS的目标是从一组文档$D$中生成一个简洁而信息丰富的摘要$Sum$。$D$表示一个主题相关文档的集合$\left\{d_{i}\mid
    i\in[1,N]\right\}$，其中$N$是文档的数量。每个文档$d_{i}$包含$M_{d_{i}}$个句子$\left\{s_{i,j}\mid
    j\in[1,M_{d_{i}}]\right\}$。$s_{i,j}$指的是第$i$个文档中的第$j$个句子。标准摘要$Ref$被称为黄金摘要或参考摘要。目前，大多数黄金摘要由专家编写。我们在整篇文章中保持这一符号的一致性。
- en: 'To give readers a clear understanding of the processing of deep learning based
    summarization tasks, we summarize and illustrate the processing framework as shown
    in Figure [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey"). The first step is preprocessing input
    document(s), such as segmenting sentences, tokenizing non-alphabetic characters,
    and removing punctuation (Shirwandkar and Kulkarni, [2018](#bib.bib141)). MDS
    models in particular need to select suitable concatenation methods to capture
    cross-document relations. Then, an appropriate deep learning based model is chosen
    to generate semantic-rich representation for downstream tasks. The next step is
    to fuse these various types of representation for later sentence selection or
    summary generation. Finally, document(s) are transformed into a concise and informative
    summary. Each of the highlighted steps in Figure [2](#S1.F2 "Figure 2 ‣ 1\. Introduction
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey") (indicated
    by triangles) indicates a difference between SDS and MDS. Based on this process,
    the research questions of MDS can be summarized as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '为了让读者清楚理解深度学习基于总结任务的处理过程，我们总结并阐述了如图[2](#S1.F2 "Figure 2 ‣ 1\. Introduction
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")所示的处理框架。第一步是预处理输入文档，如句子分割、非字母字符的标记化和标点符号的去除（Shirwandkar
    和 Kulkarni，[2018](#bib.bib141)）。尤其是MDS模型需要选择合适的连接方法来捕捉跨文档关系。接着，选择一个合适的深度学习模型来生成语义丰富的表示以用于下游任务。下一步是融合这些不同类型的表示以用于后续的句子选择或摘要生成。最后，将文档转化为简洁且信息丰富的摘要。图[2](#S1.F2
    "Figure 2 ‣ 1\. Introduction ‣ Multi-document Summarization via Deep Learning
    Techniques: A Survey")中每一个用三角形标出的步骤表示了SDS和MDS之间的差异。基于此过程，MDS的研究问题可以总结如下：'
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How to capture the cross-document relations and in-document relations from the
    input documents?
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何从输入文档中捕捉跨文档关系和文档内关系？
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compared to SDS, how to extract or generate salient information in a larger
    search space containing conflict, duplication and complementary information?
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相比于SDS，如何在包含冲突、重复和补充信息的大范围搜索空间中提取或生成显著信息？
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How to best fuse various representation from deep learning based models and
    external knowledge?
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何最佳地融合来自深度学习模型和外部知识的各种表示？
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How to comprehensively evaluate the performance of MDS models?
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何全面评估MDS模型的性能？
- en: The following sections provide a comprehensive analysis of the similarities
    and differences between SDS and MDS.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节提供了对SDS和MDS之间相似性和差异性的全面分析。
- en: '![Refer to caption](img/090b57d9028b1a6c46da70272153b233.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/090b57d9028b1a6c46da70272153b233.png)'
- en: Figure 3\. Summarization Construction Types for Text Summarization.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 文本摘要的总结构建类型。
- en: 2.1\. Similarities between SDS and MDS
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. SDS和MDS之间的相似性
- en: 'Existing SDS and MDS methods share the summarization construction types, learning
    strategies, evaluation indexes and objective functions. SDS and MDS both seek
    to compress the document(s) into a short and informative summary. Existing summarization
    methods can be grouped into abstractive summarization, extractive summarization
    and hybrid summarization (Figure [3](#S2.F3 "Figure 3 ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")).
    Extractive summarization methods select salient snippets from the source documents
    to creat informative summaries, and generally contain two major components: sentence
    ranking and sentence selection (Cao et al., [2015a](#bib.bib20); Nallapati et al.,
    [2017](#bib.bib110)). Abstractive summarization methods aim to present the main
    information of input documents by automatically generating summaries that are
    both succinct and coherent; this cluster of methods allows models to generate
    new words and sentences from a corpus pool (Paulus et al., [2018](#bib.bib124)).
    Hybrid models are proposed to combine the advantages of both extractive and abstractive
    methods to process the input texts. Research on summarization focuses on two learning
    strategies. One strategy seeks to enhance the generalization performance by improving
    the architecture design of the end-to-end models (Fabbri et al., [2019](#bib.bib44);
    Chu and Liu, [2019](#bib.bib31); Jin et al., [2020](#bib.bib72); Liu and Lapata,
    [2019](#bib.bib95)). The other leverages external knowledge or other auxiliary
    tasks to complement summary selection or generation (Cao et al., [2017](#bib.bib19);
    Li et al., [2020a](#bib.bib91)). Furthermore, both SDS and MDS aim to minimize
    the distance between machine-generated summary and golden summary. Therefore,
    SDS and MDS could share some indices to evaluate the performance of summarization
    models such as Recall-Oriented Understudy for Gisting Evaluation (ROUGE, see Section
    [5](#S5 "5\. Evaluation metrics ‣ Multi-document Summarization via Deep Learning
    Techniques: A Survey")), and objective functions to guide model optimization.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的SDS和MDS方法共享总结构建类型、学习策略、评估指标和目标函数。SDS和MDS都旨在将文档压缩成简短且信息丰富的摘要。现有的摘要方法可以分为抽象摘要、提取摘要和混合摘要（见图
    [3](#S2.F3 "Figure 3 ‣ 2\. From Single to Multi-document Summarization ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")）。提取摘要方法从源文档中选择显著的片段以创建信息丰富的摘要，通常包括两个主要组件：句子排名和句子选择（Cao等，[2015a](#bib.bib20)；Nallapati等，[2017](#bib.bib110)）。抽象摘要方法旨在通过自动生成简洁且连贯的摘要来呈现输入文档的主要信息；这一类方法允许模型从语料库中生成新的词汇和句子（Paulus等，[2018](#bib.bib124)）。混合模型则提出结合提取和抽象方法的优点来处理输入文本。摘要研究关注两种学习策略。一种策略通过改进端到端模型的架构设计来提高泛化性能（Fabbri等，[2019](#bib.bib44)；Chu和Liu，[2019](#bib.bib31)；Jin等，[2020](#bib.bib72)；Liu和Lapata，[2019](#bib.bib95)）。另一种策略则利用外部知识或其他辅助任务来补充摘要选择或生成（Cao等，[2017](#bib.bib19)；Li等，[2020a](#bib.bib91)）。此外，SDS和MDS都旨在最小化机器生成摘要与黄金摘要之间的距离。因此，SDS和MDS可以共享一些评估摘要模型性能的指标，如召回导向的学习评价（ROUGE，见第
    [5](#S5 "5\. Evaluation metrics ‣ Multi-document Summarization via Deep Learning
    Techniques: A Survey")节），以及用于指导模型优化的目标函数。'
- en: 2.2\. Differences between SDS and MDS
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. SDS与MDS的区别
- en: 'In the early stages of MDS, researchers directly applied SDS models to MDS
    (Mao et al., [2020](#bib.bib103)). However, a number of aspects in MDS that are
    different from SDS and these differences are also the breakthrough point for exploring
    the MDS models. We summarize the differences in the following five aspects:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDS的早期阶段，研究人员直接将SDS模型应用于MDS（Mao等，[2020](#bib.bib103)）。然而，MDS中存在许多与SDS不同的方面，这些差异也是探索MDS模型的突破点。我们总结了以下五个方面的差异：
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More diverse input document types;
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入文档类型更加多样化；
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Insufficient methods to capture cross-document relations;
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 捕捉跨文档关系的方法不足；
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: High redundancy and contradiction across input documents;
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入文档之间存在较高的冗余和矛盾。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Larger searching space but lack of sufficient training data;
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更大的搜索空间但缺乏足够的训练数据；
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lack of evaluation metrics specifically designed for MDS.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缺乏专门为MDS设计的评估指标。
- en: 'A defining different character between SDS and MDS is the number of input documents.
    The MDS tasks deal with multiple sources, of types that can be roughly divided
    into three groups:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SDS和MDS之间一个定义性不同的特点是输入文档的数量。MDS任务处理多个来源，通常可以大致分为三组：
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Many short sources, where each document is relatively short but the quantity
    of the input data is large. A typical example is product reviews summarization
    that aims to generate a short, informative summary from numerous individual reviews
    (Angelidis and Lapata, [2018](#bib.bib5)).
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 许多短文档，每个文档相对较短，但输入数据的数量很大。一个典型的例子是产品评论摘要，旨在从众多单独的评论中生成一个简短而信息丰富的摘要（Angelidis
    和 Lapata，[2018](#bib.bib5)）。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Few long sources. For example, generating a summary from a group of news articles
    (Fabbri et al., [2019](#bib.bib44)), or constructing a Wikipedia style article
    from several web articles (Liu et al., [2018](#bib.bib94)).
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 少量长文档。例如，从一组新闻文章中生成摘要（Fabbri 等，[2019](#bib.bib44)），或从几篇网络文章中构建维基百科风格的文章（Liu
    等，[2018](#bib.bib94)）。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hybrid sources containing one or few long documents with several to many shorter
    documents. For example, news article(s) with several readers’ comments to this
    news (Li et al., [2017a](#bib.bib89)), or a scientific summary from a long paper
    with several short corresponding citations (Yasunaga et al., [2019](#bib.bib167)).
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混合源包含一个或几个长文档和几个到许多较短的文档。例如，新闻文章及其多个读者评论（Li 等，[2017a](#bib.bib89)），或从一篇长论文中提取的科学总结和几个短的相关引用（Yasunaga
    等，[2019](#bib.bib167)）。
- en: 'As SDS only uses one input document, no additional processing is required to
    assess relationships between SDS inputs. By their very nature, the multiple input
    documents used in MDS are likely to contain more contradictory, redundant, and
    complementary information (Radev, [2000](#bib.bib127)). MDS models therefore require
    sophisticated algorithms to identify and cope with redundancy and contradictions
    across documents to ensure that the final summary is comprehensive. Detecting
    these relations across documents can bring benefits for MDS models. In the MDS
    tasks, there are two common methods to concatenate multiple input documents:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SDS只使用一个输入文档，因此无需额外处理来评估SDS输入之间的关系。就其本质而言，MDS中使用的多个输入文档可能包含更多矛盾、冗余和补充信息（Radev，[2000](#bib.bib127)）。因此，MDS模型需要复杂的算法来识别和处理文档之间的冗余和矛盾，以确保最终的摘要是全面的。检测这些文档间的关系可以为MDS模型带来好处。在MDS任务中，有两种常见的方法来连接多个输入文档：
- en: '![Refer to caption](img/2b57d1212642675d1e1a616bd9bb7d29.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2b57d1212642675d1e1a616bd9bb7d29.png)'
- en: Figure 4\. The Methods of Hierarchical Concatenation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 分层连接的方法。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Flat concatenation is a simple yet powerful concatenation method, where all
    input documents are spanned and processed as a flat sequence; to a certain extent,
    this method converts MDS to an SDS tasks. Inputting flat-concatenated documents
    requires models to have strong ability to process long sequences.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平坦连接是一种简单而强大的连接方法，其中所有输入文档被展开并作为一个平坦的序列处理；在一定程度上，这种方法将MDS任务转换为SDS任务。输入平坦连接的文档要求模型具备强大的处理长序列的能力。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hierarchical concatenation is able to preserve cross-document relations. However,
    many existing deep learning methods do not make full use of this hierarchical
    relationship (Wang et al., [2020a](#bib.bib156); Fabbri et al., [2019](#bib.bib44);
    Liu et al., [2018](#bib.bib94)). Taking advantage of hierarchical relations among
    documents instead of simply flat concatenating articles facilitates the MDS model
    to obtain representation with built-in hierarchical information, which in turn
    improves the effectiveness of the models. The input documents within a cluster
    describe a similar topic logically and semantically. Figure [4](#S2.F4 "Figure
    4 ‣ 2.2\. Differences between SDS and MDS ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")
    illustrates two representative methods of hierarchical concatenation. Existing
    hierarchical concatenation methods either perform document-level condensing in
    a cluster separately (Amplayo and Lapata, [2021](#bib.bib4)) or process documents
    in word/sentence-level inside document cluster (Nayeem et al., [2018](#bib.bib112);
    Antognini and Faltings, [2019](#bib.bib6); Wang et al., [2020a](#bib.bib156)).
    In Figure [4](#S2.F4 "Figure 4 ‣ 2.2\. Differences between SDS and MDS ‣ 2\. From
    Single to Multi-document Summarization ‣ Multi-document Summarization via Deep
    Learning Techniques: A Survey")(a), the extractive or abstractive summaries, or
    representation from the input documents are fused in the subsequent processes
    for final summary generation. The models using document-level concatenation methods
    are usually two-stage models. In Figure [4](#S2.F4 "Figure 4 ‣ 2.2\. Differences
    between SDS and MDS ‣ 2\. From Single to Multi-document Summarization ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(b), sentences in the documents
    can be replaced by words. For the word or sentence-level concatenation methods,
    clustering algorithms and graph-based techniques are the most commonly used methods.
    Clustering methods could help MDS models decrease redundancy and increase the
    information coverage for the generated summaries (Nayeem et al., [2018](#bib.bib112)).
    Sentence relation graph is able to model hierarchical relations among multi-documents
    as well (Antognini and Faltings, [2019](#bib.bib6); Yasunaga et al., [2019](#bib.bib167),
    [2017](#bib.bib168)). Most of the graph construction methods utilize sentences
    as vertexes and the edge between two sentences indicates their sentence-level
    relations (Antognini and Faltings, [2019](#bib.bib6)). Cosine similarity graph
    (Erkan and Radev, [2004](#bib.bib41)), discourse graph (Christensen et al., [2013](#bib.bib30);
    Yasunaga et al., [2017](#bib.bib168); Liu and Lapata, [2019](#bib.bib95)), semantic
    graph (Pasunuru et al., [2021b](#bib.bib123)) and heterogeneous graph (Wang et al.,
    [2020a](#bib.bib156)) can be used for building sentence graph structures. These
    graph structure could all serve as external knowledge to improve the performance
    of MDS models.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '层次化连接能够保持跨文档的关系。然而，许多现有的深度学习方法并未充分利用这种层次关系（Wang et al., [2020a](#bib.bib156);
    Fabbri et al., [2019](#bib.bib44); Liu et al., [2018](#bib.bib94)）。利用文档之间的层次关系而非简单的平坦连接文章，有助于MDS模型获得内置层次信息的表示，从而提高模型的有效性。一个集群中的输入文档在逻辑上和语义上描述了相似的主题。图[4](#S2.F4
    "Figure 4 ‣ 2.2\. Differences between SDS and MDS ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")展示了层次化连接的两个代表性方法。现有的层次化连接方法要么在集群中分别进行文档级的凝练（Amplayo
    and Lapata, [2021](#bib.bib4)），要么在文档集群内处理词语/句子级别的文档（Nayeem et al., [2018](#bib.bib112);
    Antognini and Faltings, [2019](#bib.bib6); Wang et al., [2020a](#bib.bib156)）。在图[4](#S2.F4
    "Figure 4 ‣ 2.2\. Differences between SDS and MDS ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")(a)中，来自输入文档的抽取式或生成式摘要，或表示会在后续过程中融合以生成最终摘要。使用文档级连接方法的模型通常是两阶段模型。在图[4](#S2.F4
    "Figure 4 ‣ 2.2\. Differences between SDS and MDS ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")(b)中，文档中的句子可以被词语替代。对于词语或句子级连接方法，聚类算法和基于图的技术是最常用的方法。聚类方法可以帮助MDS模型减少冗余，提高生成摘要的信息覆盖率（Nayeem
    et al., [2018](#bib.bib112)）。句子关系图能够建模多文档之间的层次关系（Antognini and Faltings, [2019](#bib.bib6);
    Yasunaga et al., [2019](#bib.bib167), [2017](#bib.bib168)）。大多数图构建方法利用句子作为顶点，两个句子之间的边表示它们的句子级关系（Antognini
    and Faltings, [2019](#bib.bib6)）。余弦相似度图（Erkan and Radev, [2004](#bib.bib41)）、话语图（Christensen
    et al., [2013](#bib.bib30); Yasunaga et al., [2017](#bib.bib168); Liu and Lapata,
    [2019](#bib.bib95)）、语义图（Pasunuru et al., [2021b](#bib.bib123)）和异构图（Wang et al.,
    [2020a](#bib.bib156)）可以用于构建句子图结构。这些图结构都可以作为外部知识来提高MDS模型的性能。'
- en: 'In addition to capture cross-document relation, hybrid summarization models
    can also be used to capture complex documents semantically, as well as to fuse
    disparate features that are more commonly adopted by MDS tasks. These models usually
    process data in two stages: extractive-abstractive and abstractive-abstractive
    (the right part of Figure [3](#S2.F3 "Figure 3 ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")).
    The two-stage models try to gather important information from source documents
    with extractive or abstractive methods at the first stage, to significantly reduce
    the length of documents. In the second stage, the processed texts are fed into
    an abstractive model to form final summaries (Amplayo and Lapata, [2021](#bib.bib4);
    Lebanoff et al., [2019](#bib.bib83); Liu et al., [2018](#bib.bib94); Liu and Lapata,
    [2019](#bib.bib95); Li et al., [2020a](#bib.bib91)).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '除了捕捉文档间的关系，混合摘要模型还可以用于语义上捕捉复杂文档，以及融合在MDS任务中更常见的不同特征。这些模型通常分两个阶段处理数据：提取式-抽象式和抽象式-抽象式（见图[3](#S2.F3
    "Figure 3 ‣ 2\. From Single to Multi-document Summarization ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")的右侧部分）。两个阶段的模型尝试在第一阶段通过提取式或抽象式方法从源文档中收集重要信息，从而显著减少文档的长度。在第二阶段，处理过的文本被输入到一个抽象式模型中以形成最终的摘要（Amplayo和Lapata，[2021](#bib.bib4)；Lebanoff等，[2019](#bib.bib83)；Liu等，[2018](#bib.bib94)；Liu和Lapata，[2019](#bib.bib95)；Li等，[2020a](#bib.bib91)）。'
- en: Furthermore, conflict, duplication, and complementarity among multiple source
    documents require MDS models to have stronger abilities to handle complex information.
    However, applying the SDS model directly on MDS tasks is difficult to handle much
    higher redundancy (Mao et al., [2020](#bib.bib103)). Therefore, the MDS models
    are required not only to generate coherent and complete summary but also more
    sophisticated algorithms to identify and cope with redundancy and contradictions
    across documents ensuring that the final summary should be complete in itself.
    MDS also involves larger searching spaces but has smaller-scale training data
    than SDS, which sets obstacles for deep learning based models to learn adequate
    representation (Mao et al., [2020](#bib.bib103)). In addition, there are no specific
    evaluation metrics designed for MDS; however, existing SDS evaluation metrics
    can not evaluate the relationship between the generated abstract and different
    input documents well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，多源文档之间的冲突、重复和互补性要求MDS模型具备更强的处理复杂信息的能力。然而，直接将SDS模型应用于MDS任务难以处理更高的冗余度（Mao等，[2020](#bib.bib103)）。因此，MDS模型不仅需要生成连贯且完整的摘要，还需要更复杂的算法来识别和应对文档间的冗余和矛盾，以确保最终的摘要本身是完整的。MDS还涉及更大的搜索空间，但其训练数据规模小于SDS，这给基于深度学习的模型学习充分的表示带来了障碍（Mao等，[2020](#bib.bib103)）。此外，目前没有专门针对MDS设计的评估指标；然而，现有的SDS评估指标无法有效评估生成的摘要与不同输入文档之间的关系。
- en: 3\. Deep Learning Based Multi-document Summarization Methods
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 基于深度学习的多文档摘要方法
- en: Deep neural network (DNN) models learn multiple levels of representation and
    abstraction from input data and can fit data in a variety of research fields,
    such as computer vision (Krizhevsky et al., [2012](#bib.bib79)) and natural language
    process (Devlin et al., [2014](#bib.bib36)). Deep learning algorithms replace
    manual feature engineering by learning distinctive features through back-propagation
    to minimize a given objective function. It is well known that linear solvable
    problems possess many advantages, such as being easily solved and having numerous
    theoretically proven supports; however, many NLP tasks are highly non-linear.
    As theoretically proven by Hornik et al. (Hornik et al., [1989](#bib.bib63)),
    neural networks can fit any given continuous function as a universal approximator.
    For the MDS tasks, DNNs also perform considerably better than traditional methods
    to effectively process large-scale documents and distill informative summaries
    due to their strong fitting abilities. In this section, we first introduce our
    novel taxonomy that generalizes nine neural network design strategies (Section
    3.1). We then present the state-of-the-art DNN based MDS models according to the
    main neural network architecture they adopt (Section 3.2 – 3.7), before finishing
    with a brief introduction to MDS variant tasks (Section 3.8).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Architecture Design Strategies
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Architecture design strategies play a critical role in deep learning based
    models, and many architectures have been applied to variants MDS tasks. Here,
    we have generalized the network architectures and summarize them into nine types
    based on how they generate or fuse semantic-rich and syntactic-rich representation
    to improve MDS model performance (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture
    Design Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")); these
    different architectures can also be used as basic structures or stacked on each
    other to obtain more diverse design strategies. In Figure [5](#S3.F5 "Figure 5
    ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning Based Multi-document
    Summarization Methods ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey"), deep neural models are in green boxes, and can be flexibly substituted
    with other backbone networks. The blue boxes indicate the neural embeddings processed
    by neural networks or heuristic-designed approaches, e.g., ”sentence/document”
    or ”other” representation. The explanation of each sub-figure is listed as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f45f5fb97a84c64cc6d1b42a3348142b.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Network Design Strategies.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naive Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies
    ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(a)). Multiple concatenated
    documents are input through DNN based models to extract features. Word-level,
    sentence-level or document-level representation is used to generate the downstream
    summary or select sentences. Naive networks represent the most naive model that
    lays the foundation for other strategies.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '朴素网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(a)）通过基于深度神经网络（DNN）的模型输入多个串联的文档以提取特征。使用词级、句级或文档级表示来生成下游摘要或选择句子。朴素网络代表了最简单的模型，为其他策略奠定了基础。'
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ensemble Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(b)). Ensemble based methods
    leverage multiple learning algorithms to obtain better performance than individual
    algorithms. To capture semantic-rich and syntactic-rich representation, Ensemble
    networks feed input documents to multiple paths with different network structures
    or operations. Later on, the representation from different networks is fused to
    enhance model expression capability. The majority vote or the average score can
    be used to determine the final output.'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '集成网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(b)）。集成方法利用多种学习算法来获得比单一算法更好的性能。为了捕捉语义丰富和句法丰富的表示，集成网络将输入文档馈送到具有不同网络结构或操作的多个路径中。随后，来自不同网络的表示被融合以增强模型的表达能力。多数投票或平均得分可以用于确定最终输出。'
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Auxiliary Task Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(c)) employ different tasks
    in the summarization models, where text classification, text reconstruction or
    other auxiliary tasks serve as complementary representation learners to obtain
    advanced features. Meanwhile, auxiliary task networks also provide researchers
    with a solution to use appropriate data from other tasks. In this strategy, parameters
    sharing scheme are used for jointly optimizing different tasks.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '辅助任务网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\.
    Deep Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(c)）在摘要模型中采用不同的任务，其中文本分类、文本重建或其他辅助任务作为补充表示学习器来获取高级特征。同时，辅助任务网络还为研究人员提供了一种利用其他任务中的适当数据的解决方案。在这种策略中，使用参数共享方案来共同优化不同的任务。'
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reconstruction Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(d)) optimize models from
    an unsupervised learning paradigm, which allows summarization models to overcome
    the limitation of insufficient annotated golden summaries. The use of such a paradigm
    enables generated summaries to be constrained in the natural language domain in
    a good manner.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '重建网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(d)）从无监督学习范式中优化模型，这使得摘要模型能够克服注释黄金摘要不足的限制。使用这种范式使生成的摘要在自然语言领域内得到良好约束。'
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fusion Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies
    ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(e)) fuse representation
    generated from neural networks and hand-crafted features. These hand-crafted features
    contain adequate prior knowledge that facilitates the optimization of summarization
    models.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '融合网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(e)）融合了神经网络生成的表示和手工特征。这些手工特征包含了足够的先验知识，有助于优化摘要模型。'
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Neural Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(f)). This strategy captures
    cross-document relations, crucial and beneficial for multi-document model training,
    by constructing graph structures based on the source documents, including word,
    sentence, or document-level information.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图神经网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(f)）。该策略通过基于源文档构建图结构，包括词汇、句子或文档级别的信息，从而捕捉文档间的关系，这对于多文档模型训练至关重要且有益。'
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Encoder-Decoder Structure (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture
    Design Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")(g)). The
    encoder embeds source documents into the hidden representation, i.e., word, sentence
    and document representation. This representation, containing compressed semantic
    and syntactic information, is passed to the decoder which processes the latent
    embeddings to synthesize local and global semantic/syntactic information to produce
    the final summaries.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '编码器-解码器结构（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\.
    Deep Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(g)）。编码器将源文档嵌入到隐藏表示中，即词汇、句子和文档表示。这种表示包含压缩的语义和句法信息，传递给解码器，解码器处理潜在的嵌入来综合局部和全局的语义/句法信息，以生成最终的摘要。'
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pre-trained Language Models (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture
    Design Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")(h)) obtain
    contextualized text representation by predicting words or phrases based on their
    context using large amounts of the corpus, which can be further fine-tuned for
    downstream task adaption (Dong et al., [2019](#bib.bib37)). The models can fine-tune
    with randomly initialized decoders in an end-to-end fashion since transfer learning
    can assist the model training process (Li et al., [2020a](#bib.bib91)).'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '预训练语言模型（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\.
    Deep Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(h)）通过使用大量语料库预测词汇或短语来获得上下文文本表示，这些模型可以进一步微调以适应下游任务（Dong
    et al., [2019](#bib.bib37)）。由于迁移学习可以帮助模型训练过程，这些模型可以通过随机初始化的解码器以端到端的方式进行微调（Li et
    al., [2020a](#bib.bib91)）。'
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hierarchical Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(i)). Multiple documents
    are concatenated as inputs to feed into the first DNN based model to capture low-level
    representation. Another DNN based model is cascaded to generate high-level representation
    based on the previous ones. The hierarchical networks empower the model with the
    ability to capture abstract-level and semantic-level features more efficiently.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '层次网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(i)）。多个文档被串联作为输入，输入到第一个基于DNN的模型中以捕捉低级表示。另一个基于DNN的模型级联生成基于前者的高级表示。层次网络使模型能够更高效地捕捉抽象级别和语义级别的特征。'
- en: 3.2\. Recurrent Neural Networks based Models
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 基于递归神经网络的模型
- en: 'Recurrent Neural Networks (RNNs) (Rumelhart et al., [1986](#bib.bib134)) excel
    in modeling sequential data by capturing sequential relations and syntactic/semantic
    information from word sequences. In RNN models, neurons are connected through
    hidden layers and unlike other neural network structures, the inputs of each RNN
    neuron come not only from the word or sentence embedding but also from the output
    of the previous hidden state. Despite being powerful, vanilla RNN models often
    encounter gradient explosion or vanishing issues, so a large number of RNN-variants
    have been proposed. The most prevalent ones are Long Short-Term Memory (LSTM)
    (Hochreiter and Schmidhuber, [1997](#bib.bib62)), Gated Recurrent Unit (GRU) (Chung
    et al., [2014](#bib.bib32)) and Bi-directional Long Short-Term Memory (Bi-LSTM)
    (Huang et al., [2015](#bib.bib65)). The DNN based Model in Figure [5](#S3.F5 "Figure
    5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning Based Multi-document
    Summarization Methods ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey") can be replaced with RNN based models to design models.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '循环神经网络（RNNs）（Rumelhart et al., [1986](#bib.bib134)）擅长通过捕捉序列关系和词序列中的句法/语义信息来建模顺序数据。在RNN模型中，神经元通过隐藏层连接，与其他神经网络结构不同，每个RNN神经元的输入不仅来自词语或句子嵌入，还来自前一个隐藏状态的输出。尽管功能强大，传统的RNN模型通常会遇到梯度爆炸或消失的问题，因此提出了大量的RNN变体。最常见的有长短期记忆（LSTM）（Hochreiter
    and Schmidhuber, [1997](#bib.bib62)）、门控递归单元（GRU）（Chung et al., [2014](#bib.bib32)）和双向长短期记忆（Bi-LSTM）（Huang
    et al., [2015](#bib.bib65)）。图[5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")中的基于DNN的模型可以被替换为基于RNN的模型进行设计。'
- en: RNN based models have been used in MDS tasks since 2015\. Cao et al. (Cao et al.,
    [2015a](#bib.bib20)) proposed an RNN-based model termed Ranking framework upon
    Recursive Neural Networks (R2N2), which leverages manually extracted words and
    sentence-level features as inputs. This model transfers the sentence ranking task
    into a hierarchical regression process, which measures the importance of sentences
    and constituents in the parsing tree. Zheng et al. (Zheng et al., [2019](#bib.bib182))
    used a hierarchical RNN structure to utilize the subtopic information by extracting
    not only sentence and document embeddings, but also topic embeddings. In this
    SubTopic-Driven Summarization (STDS) model, the readers’ comments are seen as
    auxiliary documents and the model employs soft clustering to incorporate comment
    and sentence representation for further obtaining subtopic representation. Arthur
    et al. (Bražinskas et al., [2019](#bib.bib16)) introduced a GRU-based encoder-decoder
    architecture to minimize the diversity of opinions reflecting the dominant views
    while generating multi-review summaries. Mao et al. (Mao et al., [2020](#bib.bib103))
    proposed a maximal margin relevance guided reinforcement learning framework (RL-MMR)
    to incorporate the advantages of neural sequence learning and statistical measures.
    The proposed soft attention for learning adequate representation allows more exploration
    of search space.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 自2015年以来，基于RNN的模型已被应用于MDS任务。Cao等人（Cao et al., [2015a](#bib.bib20)）提出了一种基于RNN的模型，称为基于递归神经网络的排序框架（R2N2），该模型利用手动提取的词语和句子级特征作为输入。该模型将句子排序任务转化为层次回归过程，以衡量句子和解析树中成分的重要性。Zheng等人（Zheng
    et al., [2019](#bib.bib182)）使用了一个层次RNN结构，通过提取句子和文档嵌入以及主题嵌入来利用子主题信息。在这一子主题驱动摘要（STDS）模型中，读者评论被视为辅助文档，模型采用软聚类将评论和句子表示结合起来，以进一步获取子主题表示。Arthur等人（Bražinskas
    et al., [2019](#bib.bib16)）介绍了一种基于GRU的编码器-解码器架构，用于在生成多评论摘要时最小化反映主流观点的意见多样性。Mao等人（Mao
    et al., [2020](#bib.bib103)）提出了一种最大边际相关引导的强化学习框架（RL-MMR），以结合神经序列学习和统计测量的优点。所提出的软注意力用于学习充分的表示，使搜索空间的探索更多。
- en: To leverage the advantage of hybrid summarization model, Reinald et al. (Amplayo
    and Lapata, [2021](#bib.bib4)) proposed a two-stage framework, viewing opinion
    summarization as an instance of multi-source transduction to distill salient information
    from source documents. The first stage of the model leverages a Bi-LSTM auto-encoder
    to learn word and document-level representation; the second stage fuses multi-source
    representation and generates an opinion summary with a simple LSTM decoder combined
    with a vanilla attention mechanism (Bahdanau et al., [2015](#bib.bib9)) and a
    copy mechanism (Vinyals et al., [2015](#bib.bib152)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用混合总结模型的优势，Reinald 等（Amplayo 和 Lapata， [2021](#bib.bib4)）提出了一个两阶段框架，将观点总结视为多源传导的一个实例，以从源文档中提取显著信息。模型的第一阶段利用
    Bi-LSTM 自编码器学习词级和文档级表示；第二阶段融合多源表示，并使用简单的 LSTM 解码器结合基本的注意机制（Bahdanau 等， [2015](#bib.bib9)）和复制机制（Vinyals
    等， [2015](#bib.bib152)）生成观点总结。
- en: Since paired MDS datasets are rare and hard to obtain, Li et al. (Li et al.,
    [2017b](#bib.bib90)) developed a RNN-based framework to extract salient information
    vectors from sentences in input documents in an unsupervised manner. Cascaded
    attention retains the most relevant embeddings to reconstruct the original input
    sentence vectors. During the reconstruction process, the proposed model leverages
    a sparsity constraint to penalize trivial information in the output vectors. Also,
    Chu et al. (Chu and Liu, [2019](#bib.bib31)) proposed an unsupervised end-to-end
    abstractive summarization architecture called MeanSum. This LSTM-based model formalizes
    product or business reviews summarization problem into two individual closed-loops.
    Inspired by MeanSum, Coavoux et al. (Coavoux et al., [2019](#bib.bib33)) used
    a two-layer standard LSTM to construct sentence representation for aspect-based
    multi-document abstractive summarization, and discovered that the clustering strategy
    empowers the model to reward review diversity and handle contradictory ones.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于配对 MDS 数据集稀缺且难以获得，Li 等（Li 等， [2017b](#bib.bib90)）开发了一个基于 RNN 的框架，以无监督的方式从输入文档中的句子中提取显著信息向量。级联注意机制保留了最相关的嵌入以重构原始输入句子向量。在重构过程中，所提出的模型利用稀疏性约束来惩罚输出向量中的琐碎信息。此外，Chu
    等（Chu 和 Liu， [2019](#bib.bib31)）提出了一种无监督的端到端抽象总结架构，称为 MeanSum。该基于 LSTM 的模型将产品或商业评论总结问题形式化为两个独立的闭环。受到
    MeanSum 的启发，Coavoux 等（Coavoux 等， [2019](#bib.bib33)）使用了一个两层标准 LSTM 来构建基于方面的多文档抽象总结的句子表示，并发现聚类策略使模型能够奖励评论的多样性并处理矛盾的评论。
- en: 3.3\. Convolutional Neural Networks Based Models
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 基于卷积神经网络的模型
- en: 'Convolutional neural networks (CNNs) (LeCun et al., [1998](#bib.bib85)) achieve
    excellent results in computer vision tasks. The convolution operation scans through
    the word/sentence embeddings and uses convolution kernels to extract important
    information from input data objects. Using a pooling operation at intervals can
    return simple to complex feature levels. CNNs have been proven to be effective
    for various NLP tasks in recent years (Kim, [2014](#bib.bib75); Dos Santos and
    Gatti, [2014](#bib.bib38)) as they can process natural language after sentence/word
    vectorization. Most of the CNN based MDS models use CNNs for semantic and syntactic
    feature representation. As with RNN, CNN-based models can also replace DNN-based
    models in network design strategies (Please refer to Figure [5](#S3.F5 "Figure
    5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning Based Multi-document
    Summarization Methods ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey")).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '卷积神经网络（CNNs）（LeCun 等， [1998](#bib.bib85)）在计算机视觉任务中取得了优异的结果。卷积操作扫描词语/句子嵌入，并使用卷积核从输入数据对象中提取重要信息。通过间歇性地使用池化操作，可以返回从简单到复杂的特征层次。近年来，CNN
    已被证明在各种 NLP 任务中有效（Kim， [2014](#bib.bib75)；Dos Santos 和 Gatti， [2014](#bib.bib38)），因为它们可以在句子/词向量化后处理自然语言。大多数基于
    CNN 的 MDS 模型使用 CNN 进行语义和句法特征表示。与 RNN 类似，基于 CNN 的模型也可以在网络设计策略中替代基于 DNN 的模型（请参阅图
    [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning
    Based Multi-document Summarization Methods ‣ Multi-document Summarization via
    Deep Learning Techniques: A Survey")）。'
- en: A simple way to use CNNs in MDS is by sliding multiple filters with different
    window sizes over the input documents for semantic representation. Cao et al.
    (Cao et al., [2015b](#bib.bib21)) proposed a hybrid CNN-based model PriorSum to
    capture latent document representation. The proposed representation learner slides
    over the input documents with filters of different window widths and two-layer
    max-over-time pooling operations (Collobert et al., [2011](#bib.bib34)) to fetch
    document-independent features that are more informative than using standard CNNs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDS中使用CNN的一种简单方法是通过在输入文档上滑动多个不同窗口大小的滤波器来进行语义表示。曹等（曹等，[2015b](#bib.bib21)）提出了一种基于混合CNN的模型PriorSum来捕捉潜在的文档表示。所提议的表示学习器在输入文档上滑动，使用不同窗口宽度的滤波器和两层最大时间池化操作（Collobert
    et al., [2011](#bib.bib34)）以提取比使用标准CNN更具信息性的文档独立特征。
- en: Similarly, HNet (Singh et al., [2018](#bib.bib142)) uses distinct CNN filters
    and max-over-time-pooling to generate salient feature representation for downstream
    processes. Cho et al. (Cho et al., [2019](#bib.bib29)) also used different filter
    sizes in DPP-combined model to extract low-level features. Yin et al. (Yin and
    Pei, [2015](#bib.bib169)) presented an unsupervised CNN-based model termed Novel
    Neural Language Model (NNLM) to extract sentence representation and diminish the
    redundancy of sentence selection. The NNLM framework contains only one convolution
    layer and one max-pooling layer, and both element-wise averaging sentence representation
    and context words representation are used to predict the next word. For aspect-based
    opinion summarization, Stefanos et al. (Angelidis and Lapata, [2018](#bib.bib5))
    leveraged a CNN based model to encode the product reviews which contain a set
    of segments for opinion polarity.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，HNet（Singh et al., [2018](#bib.bib142)）使用不同的CNN滤波器和最大时间池化来生成用于下游处理的显著特征表示。Cho等（Cho
    et al., [2019](#bib.bib29)）也在DPP组合模型中使用了不同的滤波器尺寸来提取低级特征。Yin等（Yin and Pei, [2015](#bib.bib169)）提出了一种无监督的基于CNN的模型，称为新颖神经语言模型（NNLM），用于提取句子表示并减少句子选择的冗余。NNLM框架仅包含一层卷积层和一层最大池化层，同时使用逐元素平均句子表示和上下文词表示来预测下一个词。对于基于方面的意见摘要，Stefanos等（Angelidis
    and Lapata, [2018](#bib.bib5)）利用基于CNN的模型来编码包含一组意见极性的产品评论。
- en: People with different background knowledge and understanding can produce different
    summaries of the same documents. To account for this variability, Zhang et al.
    (Zhang et al., [2016](#bib.bib179)) suggested a MV-CNN model that ensembles three
    individual models to incorporate multi-view learning and CNNs to improve the performance
    of MDS. In this work, three CNNs with dual-convolutional layers used multiple
    filters with different window sizes to extract distinct saliency scores of sentences.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 具有不同背景知识和理解的人可能会产生对同一文档的不同总结。为了考虑这种变异性，张等（张等，[2016](#bib.bib179)）提出了一种MV-CNN模型，该模型集成了三个独立模型，以结合多视角学习和CNN来提高MDS的性能。在这项工作中，三个具有双卷积层的CNN使用了不同窗口大小的多个滤波器，以提取句子的不同显著性分数。
- en: To overcome the MDS bottlenecks of insufficient training data, Cao et al. (Cao
    et al., [2017](#bib.bib19)) developed a TCSum model incorporating an auxiliary
    text classification sub-task into MDS to introduce more supervision signals. The
    text classification model uses a CNN descriptor to project documents onto the
    distributed representation, and to classify input documents into different categories.
    The summarization model shares the projected sentence embedding from the classification
    model, and the TCSum model then chooses the corresponding category based transformation
    matrices according to classification results to transform the sentence embedding
    into the summary embedding.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服训练数据不足的MDS瓶颈，曹等（曹等，[2017](#bib.bib19)）开发了一个TCSum模型，将一个辅助文本分类子任务纳入MDS中，以引入更多的监督信号。文本分类模型使用CNN描述符将文档映射到分布式表示，并将输入文档分类到不同的类别中。总结模型共享来自分类模型的投影句子嵌入，TCSum模型根据分类结果选择相应的类别转换矩阵，将句子嵌入转换为摘要嵌入。
- en: Unlike RNNs that support the processing of long time-serial signals, a naive
    CNN layer struggles to capture long-distance relations while processing sequential
    data due to the limitation of the fixed-sized convolutional kernels, each of which
    has a specific receptive field size. Nevertheless, CNN based models can increase
    their receptive fields through formation of hierarchical structures to calculate
    sequential data in a parallel manner. Because of this highly parallelizable characteristic,
    training of CNN-based summarization models is more efficient than for RNN-based
    models. However, summarizing lengthy input articles is still a challenging task
    for CNN based models because they are not skilled in modeling non-local relationships.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Graph Neural Networks Based Models
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'CNNs have been successfully applied to many computer vision tasks to extract
    distinguished image features from the Euclidean space, but struggle when processing
    non-Euclidean data. Natural language data consist of vocabularies and phrases
    with strong relations which can be better represented with graphs than with sequential
    orders. Graph neural networks (GNNs, Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture
    Design Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey") (f)) are
    composed of an ideal architecture for NLP since they can model strong relations
    between entities semantically and syntactically. Graph convolution networks (GCNs)
    and graph attention networks (GANs) are the most commonly adopted GNNs because
    of their efficiency and simplicity for integration with other neural networks.
    These models first build a relation graph based on input documents, where nodes
    can be words, sentences or documents, and edges capture the similarity among them.
    At the same time, input documents are fed into a DNN based model to generate embeddings
    at different levels. The GNNs are then built over the top to capture salient contextual
    information. Table [1](#S3.T1 "Table 1 ‣ 3.4\. Graph Neural Networks Based Models
    ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey") describes the current GNN
    based models used for MDS with details of nodes, edges, edge weights, and applied
    GNN methods.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Multi-document Summarization Models based on Graph Neural Networks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Nodes | Edges | Edge Weights | GNN Methods |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HeterDoc- &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SumGraph (Wang et al., [2020a](#bib.bib156)) &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; word, sentence, &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; document &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; word-sentence, &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; word-document &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '| TF-IDF |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '&#124; Graph Attention &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Networks &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Graph-based &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Neural MDS (Yasunaga et al., [2017](#bib.bib168)) &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '| sentence | sentence-sentence |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '&#124; Personalized &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Discourse Graph &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Graph Convolutional &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Networks &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '| SemSentSum (Antognini and Faltings, [2019](#bib.bib6)) | sentence | sentence-sentence
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '&#124; Cosine Similarity Graph &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Edge Removal Method &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Graph Convolutional &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Networks &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '| ScisummNet (Yasunaga et al., [2019](#bib.bib167)) | sentence | sentence-sentence
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '&#124; Cosine Similarity Graph &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Graph Convolutional &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Networks &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Yasunage et al. (Yasunaga et al., [2017](#bib.bib168)) developed a GCN based
    extractive model to capture the relations between sentences. This model first
    builds a sentence-based graph and then feeds the pre-processed data into a GCN
    (Kipf and Welling, [2017](#bib.bib76)) to capture sentence-wise related features.
    Defined by the model, each sentence is regarded as a node and the relation between
    each pair of sentences is defined as an edge. Inside each document cluster, the
    sentence relation graph can be generated through a cosine similarity graph (Erkan
    and Radev, [2004](#bib.bib41)), approximate discourse graph (Christensen et al.,
    [2013](#bib.bib30)), and the proposed personalized discourse graph. Both the sentence
    relation graph and sentence embeddings extracted by a sentence-level RNN are fed
    into GCN to produce the final sentence representation. With the help of a document-level
    GRU, the model generates cluster embeddings to fully aggregate features between
    sentences.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Antognini et al. (Antognini and Faltings, [2019](#bib.bib6)) proposed
    a GCN based model named SemSentSum that constructs a graph based on sentence relations.
    In contrast to Yasunage et al. (Yasunaga et al., [2017](#bib.bib168)), this work
    leverages external universal embeddings, pre-trained on the unrelated corpus,
    to construct a sentence semantic relation graph. Additionally, an edge removal
    method has been applied to deal with the sparse graph problems emphasizing high
    sentence similarities; if the weight of the edge is lower than a given threshold,
    the edge is removed. The sentence relation graph and sentence embeddings are fed
    into a GCN (Kipf and Welling, [2017](#bib.bib76)) to generate saliency estimation
    for extractive summaries.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Yasunage et al. (Yasunaga et al., [2019](#bib.bib167)) also designed a GCN based
    model for summarizing scientific papers. The proposed ScisummNet model uses not
    only the abstract of source scientific papers but also the relevant text from
    papers that cite the original source. The total number of citations is also incorporated
    in the model as an authority feature. A cosine similarity graph is applied to
    form the sentence relation graph, and GCNs are adopted to predict the sentence
    salience estimation from the sentence relation graph, authority scores and sentence
    embeddings.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Existing GNN based models focused mainly on the relationships between sentences,
    and do not fully consider the relationships between words, sentences, and documents.
    To fill this gap, Wang et al. (Wang et al., [2020a](#bib.bib156)) proposed a heterogeneous
    GAN based model, called HeterDoc-SUM Graph, that is specific for extractive MDS.
    This heterogeneous graph structure includes word, sentence, and document nodes,
    where sentence nodes and document nodes are connected according to the contained
    word nodes. Word nodes thus act as an intermediate bridge to connect the sentence
    and document nodes, and are used to better establish document-document, sentence-sentence
    and sentence-document relations. TF-IDF values are used to weight word-sentence
    and word-document edges, and the node representation of these three levels are
    passed into the graph attention networks for model update. In each iteration,
    bi-directional updating of both word-sentence and word-document relations are
    performed to better aggregate cross-level semantic knowledge.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Pointer-generator Networks Based Models
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pointer-generator (PG) networks (See et al., [2017](#bib.bib136)) are proposed
    to overcome the problems of factual errors and high redundancy in the summarization
    tasks. This network has been inspired by Pointer Network (Vinyals et al., [2015](#bib.bib152)),
    CopyNet (Gu et al., [2016](#bib.bib56)), forced-attention sentence compression
    (Miao and Blunsom, [2016](#bib.bib105)), and coverage mechanism from machine translation
    (Tu et al., [2016](#bib.bib149)). PG networks combine sequence-to-sequence (Seq2Seq)
    model and pointer networks to obtain a united probability distribution allowing
    vocabularies to be selected from source texts or generated by machines. Additionally,
    the coverage mechanism prevents PG networks from consistently choosing the same
    phrases.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'The Maximal Marginal Relevance (MMR) method is designed to select a set of
    salient sentences from source documents by considering both importance and redundancy
    indices (Carbonell and Goldstein, [1998](#bib.bib22)). The redundancy score controls
    sentence selection to minimize overlap with the existing summary. The MMR model
    adds a new sentence to the objective summary based on importance and redundancy
    scores until the summary length reaches a certain threshold. Inspired by MMR,
    Alexander et al. (Fabbri et al., [2019](#bib.bib44)) proposed an end-to-end Hierarchical
    MMR-Attention Pointer-generator (Hi-MAP) model to incorporate PG networks and
    MMR (Carbonell and Goldstein, [1998](#bib.bib22)) for abstractive MDS. The Hi-MAP
    model improves PG networks by modifying attention weights (multipling MMR scores
    by the original attention weights) to include better important sentences in, and
    filter redundant information from, the summary. Similarly, the MMR approach is
    implemented by PG-MMR model (Lebanoff et al., [2018](#bib.bib84)) to identify
    salient source sentences from multi-document inputs, albeit with a different method
    for calculating MMR scores from Hi-MAP; instead, ROUGE-L Recall and ROUGE-L Precision
    (Lin, [2004](#bib.bib92)) serve as evaluation metrics to calculate the importance
    and redundancy scores. To overcome the scarcity of MDS datasets, the PG-MMR model
    leverages a support vector regression model that is pre-trained on a SDS dataset
    to recognize the important contents. This support vector regression model also
    calculates the score of each input sentence by considering four factors: sentence
    length, sentence relative/absolute position, sentence-document similarities, and
    sentence quality obtained by a PG network. Sentences with the top-$K$ scores are
    fed into another PG network to generate a concise summary.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 3.6\. Transformer Based Models
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed, CNN based models are not as good at processing sequential data
    as RNN based models. However, RNN based models are not amenable to parallel computing,
    as the current states in RNN models highly depend on results from the previous
    steps. Additionally, RNNs struggle to process long sequences since former knowledge
    will fade away during the learning process. Adopting Transformer based architectures
    (Vaswani et al., [2017](#bib.bib151)) is one solution to solve these problems.
    The Transformer is based on the self-attention mechanism, has natural advantages
    for parallelization, and retains relative long-range dependencies. The Transformer
    model has achieved promising results in MDS tasks (Liu et al., [2018](#bib.bib94);
    Liu and Lapata, [2019](#bib.bib95); Li et al., [2020a](#bib.bib91); Jin et al.,
    [2020](#bib.bib72)) and can replace the DNN based Model in Figure [5](#S3.F5 "Figure
    5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning Based Multi-document
    Summarization Methods ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey"). Most of the Transformer based models follow an encoder-decoder structure.
    Transformer based models can be divided into flat Transformer, hierarchical Transformer,
    and pre-train language models.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '正如讨论的那样，基于CNN的模型在处理序列数据方面不如基于RNN的模型。然而，基于RNN的模型不适合并行计算，因为RNN模型中的当前状态高度依赖于前一步骤的结果。此外，由于在学习过程中前面的知识会逐渐消失，RNN在处理长序列时会遇到困难。采用基于Transformer的架构（Vaswani等人，[2017](#bib.bib151)）是一种解决这些问题的方案。Transformer基于自注意力机制，具有天然的并行化优势，并且保留了相对较长的依赖关系。Transformer模型在MDS任务中取得了有希望的结果（Liu等人，[2018](#bib.bib94)；Liu和Lapata，[2019](#bib.bib95)；Li等人，[2020a](#bib.bib91)；Jin等人，[2020](#bib.bib72)），并且可以替代图中的DNN基于模型[5](#S3.F5
    "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning Based Multi-document
    Summarization Methods ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey")。大多数基于Transformer的模型遵循编码器-解码器结构。基于Transformer的模型可以分为平坦Transformer、层次Transformer和预训练语言模型。'
- en: Flat Transformer. Liu et al. (Liu et al., [2018](#bib.bib94)) introduced Transformer
    to MDS tasks, aiming to generate a Wikipedia article from a given topic and set
    of references. The authors argue that the encoder-decoder based sequence transduction
    model cannot cope well with long input documents, so their model selects a series
    of top-$K$ tokens and feeds them into a Transformer based decoder-only sequence
    transduction model to generate Wikipedia articles. More specifically, the Transformer
    decoder-only architecture combines the result from the extractive stage and golden
    summary into a sentence for training. To obtain rich semantic representation from
    different granularity, Jin et al. (Jin et al., [2020](#bib.bib72)) proposed a
    Transformer based multi-granularity interaction network MGSum and unified extractive
    and abstractive MDS. Words, sentences and documents are considered as three granular
    levels of semantic unit connected by a granularity hierarchical relation graph.
    In the same granularity, a self-attention mechanism is used to capture the semantic
    relationships. Sentence granularity representation is employed in the extractive
    summarization, and word granularity representation is adapted to generate an abstractive
    summary. MGSum employs a fusion gate to integrate and update the semantic representation.
    Additionally, a spare attention mechanism is used to ensure the summary generator
    focus on important information. Brazinskas et al. (Brazinskas et al., [2020](#bib.bib17))
    created a precedent for few-shot learning for MDS that leverages a Transformer
    conditional language model and a plug-in network for both extractive and abstractive
    MDS to overcome rapid overfitting and poor generation problems resulting from
    naive fine-tuning of large parameter models.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**平坦 Transformer**。刘等人（Liu et al., [2018](#bib.bib94)）将 Transformer 引入 MDS
    任务，旨在从给定的主题和参考资料生成一篇 Wikipedia 文章。作者认为基于编码器-解码器的序列转换模型无法很好地处理长输入文档，因此他们的模型选择了一系列前$K$的
    tokens，并将它们输入到仅基于 Transformer 的解码器序列转换模型中，以生成 Wikipedia 文章。更具体地说，Transformer 仅解码器架构将从提取阶段得到的结果和黄金总结结合成一个句子进行训练。为了从不同粒度中获得丰富的语义表示，金等人（Jin
    et al., [2020](#bib.bib72)）提出了一种基于 Transformer 的多粒度交互网络 MGSum，并统一了提取式和抽象式 MDS。单词、句子和文档被视为由粒度层次关系图连接的三个语义单元粒度级别。在相同粒度下，使用自注意力机制来捕捉语义关系。句子粒度表示用于提取式总结，而单词粒度表示则用于生成抽象总结。MGSum
    使用融合门来整合和更新语义表示。此外，采用稀疏注意力机制来确保总结生成器关注重要信息。Brazinskas 等人（Brazinskas et al., [2020](#bib.bib17)）为
    MDS 的少样本学习创造了一个先例，该方法利用 Transformer 条件语言模型和插件网络进行提取式和抽象式 MDS，以克服因对大参数模型进行天真的微调而导致的快速过拟合和生成质量差的问题。'
- en: Hierarchical Transformer. To handle huge input documents, Yang et al. (Liu and
    Lapata, [2019](#bib.bib95)) proposed a two-stage Hierarchical Transformer (HT)
    model with an inter-paragraph and graph-informed attention mechanism that allows
    the model to encode multiple input documents hierarchically instead of by simple
    flat-concatenation. A logistic regression model is employed to select the top-$K$
    paragraphs, which are fed into a local Transformer layer to obtain contextual
    features. A global Transformer layer mixes the contextual information to model
    the dependencies of the selected paragraphs. To leverage graph structure to capture
    cross-document relations, Li et al. (Li et al., [2020a](#bib.bib91)) proposed
    an end-to-end Transformer based model GraphSum, based on the HT model. In the
    graph encoding layers, GraphSum extends the self-attention mechanism to the graph-informed
    self-attention mechanism, which incorporates the graph representation into the
    Transformer encoding process. Furthermore, the Gaussian function is applied to
    the graph representation matrix to control the intensity of the graph structure
    impact on the summarization model. The HT and GraphSum models are both based on
    the self-attention mechanism leading quadratic memory growth increases with the
    number of input sequences; to address this issue, Pasunuru et al. (Pasunuru et al.,
    [2021b](#bib.bib123)) modified the full self-attention with local and global attention
    mechanism (Beltagy et al., [2020](#bib.bib14)) to scale the memory linearly. Dual
    encoders are proposed for encoding truncated concatenated documents and linearized
    graph information from full documents.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 层次化变换器。为了处理大型输入文档，Yang 等（Liu 和 Lapata，[2019](#bib.bib95)）提出了一种两阶段的层次化变换器（HT）模型，该模型具有段落间和图信息驱动的注意机制，使得模型能够以层次化的方式对多个输入文档进行编码，而不是简单的平面拼接。采用逻辑回归模型来选择前$K$个段落，这些段落被输入到局部变换器层以获得上下文特征。全局变换器层混合上下文信息以建模所选段落的依赖关系。为了利用图结构捕捉跨文档关系，Li
    等（Li 等，[2020a](#bib.bib91)）提出了一种基于HT模型的端到端变换器模型GraphSum。在图编码层中，GraphSum 将自注意机制扩展为图信息驱动的自注意机制，将图表示融入变换器编码过程。此外，采用高斯函数对图表示矩阵进行处理，以控制图结构对摘要模型的影响强度。HT和GraphSum模型均基于自注意机制，因此随着输入序列数量的增加，内存需求呈平方增长；为了解决这一问题，Pasunuru
    等（Pasunuru 等，[2021b](#bib.bib123)）将全自注意力机制修改为局部和全局注意机制（Beltagy 等，[2020](#bib.bib14)），以实现内存线性扩展。提出了双重编码器用于编码截断的拼接文档和来自完整文档的线性图信息。
- en: Pre-trained language models (LMs). Pre-trained Transformers on large text corpora
    have shown great successes in downstream NLP tasks including text summarization.
    The pre-trained LMs can be trained on non-summarization or SDS datasets to overcome
    lack of MDS data (Zhang et al., [2020d](#bib.bib173); Li et al., [2020a](#bib.bib91);
    Pasunuru et al., [2021b](#bib.bib123)). Most pre-trained LMs such as BERT (Devlin
    et al., [2019](#bib.bib35)) and RoBERTa (Liu et al., [2019a](#bib.bib96)) can
    work well on short sequences. In hierarchical Transformer architecture, replacing
    the low-level Transformer (token-level) encoding layer with pre-trained LMs helps
    the model break through length limitations to perceive further information (Li
    et al., [2020a](#bib.bib91)). Inside a hierarchical Transformer architecture,
    the output vector of the ”[CLS]” token can be used as input for high-level Transformer
    models. To avoid the self-attention quadratic-memory increment when dealing with
    document-scale sequences, a Longformer based approach (Beltagy et al., [2020](#bib.bib14)),
    including local and global attention mechanisms, can be incorporated with pre-trained
    LMs to scale the memory linearly for MDS (Pasunuru et al., [2021b](#bib.bib123)).
    Another solution for computational issues can be borrowed from SDS is to use a
    multi-layer Transformer architecture to scale the length of documents allowing
    pre-trained LMs to encode a small block of text and the information can be shared
    among the blocks between two successive layers (Grail et al., [2021](#bib.bib54)).
    PEGASUS (Zhang et al., [2020d](#bib.bib173)) is a pre-trained Transformer-based
    encoder-decoder model with gap-sentences generation (GSG) specifically designed
    for abstractive summarization. GSG shows that masking whole sentences based on
    importance, instead of through random or lead selection, works well for downstream
    summarization tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 3.7\. Deep Hybrid Models
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many neural models can be integrated to formalize a more powerful and expressive
    model. In this section, we summarize the existing deep hybrid models that have
    proven to be effective for MDS.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: CNN + LSTM + Capsule networks. Cho et al. (Cho et al., [2019](#bib.bib29)) proposed
    a hybrid model based on the determinantal point processes for semantically measuring
    sentence similarities. A convolutional layer slides over the pairwise sentences
    with filters of different sizes to extract low-level features. Capsule networks
    (Sabour et al., [2017](#bib.bib135); Yang et al., [2018](#bib.bib163)) are employed
    to identify redundant information by transforming the spatial and orientational
    relationships for high-level representation. The authors also used LSTM to reconstruct
    pairwise sentences and add reconstruction loss to the final objective function.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: CNN + Bi-LSTM + Multi-layer Perceptron (MLP). Abhishek et al. (Singh et al.,
    [2018](#bib.bib142)) proposed an extractive MDS framework that considers document-dependent
    and document-independent information. In this model, a CNN with different filters
    captures phrase-level representation. Full binary trees formed with these salient
    representation are fed to the recommended Bi-LSTM tree indexer to enable better
    generalization abilities. A MLP with ReLU function is employed for leaf node transformation.
    More specifically, the Bi-LSTM tree indexer leverages the time serial power of
    LSTMs and the compositionality of recursive models to capture both semantic and
    compositional features.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: PG networks + Transformer. In generating a summary, it is necessary to consider
    the information fusion of multiple sentences, especially sentence pairs. Logan
    et al. (Lebanoff et al., [2019](#bib.bib83)) found the majority of summary sentences
    are generated by fusing one or two source sentences; so they proposed a two-stage
    summarization method that considers the semantic compatibility of sentence pairs.
    This method joint-scores single sentence and sentence pairs to filter representative
    from the original documents. Sentences or sentence pairs with high scores are
    then compressed and rewritten to generate a summary that leverages PG network.
    This paper uses a Transformer based model to encode both single sentence and sentence
    pairs indiscriminately to obtain the deep contextual representation of words and
    sequences.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Deep Learning based Methods. ”Ext”, ”Abs” and ”Hyd” mean extractive,
    abstractive and hybrid respectively; ”FC” and ”HC” represent Flat Concatenate,
    Hierarchical Concatenate respectively.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Works |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '&#124; Construction &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Types &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Document-level &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Relationship &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Comparison of &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DL based techniques &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '| Ext | Abs | Hyb | FC | HC | Pros and Cons |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| RNN | MeanSum (Chu and Liu, [2019](#bib.bib31)) |  | ✓ |  | ✓ |  | Pros:
    Can capture sequential relations and syntactic/semantic information from word
    sequences Cons: Not easy to parallel computing; Highly depending on results from
    the previous steps |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. (Zhang et al., [2018b](#bib.bib172)) |  | ✓ |  | ✓ |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| STDS (Zheng et al., [2019](#bib.bib182)) | ✓ |  |  |  | ✓ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| ParaFuse_doc (Nayeem et al., [2018](#bib.bib112)) |  | ✓ |  |  | ✓ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| R2N2 (Cao et al., [2015a](#bib.bib20)) | ✓ |  |  | ✓ |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| CondaSum (Amplayo and Lapata, [2021](#bib.bib4)) |  |  | ✓ |  | ✓ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| C-Attention (Li et al., [2017b](#bib.bib90)) |  | ✓ |  | ✓ |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| Wang et al.(Wang and Ling, [2016](#bib.bib158)) |  | ✓ |  | ✓ |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| RL-MMR (Mao et al., [2020](#bib.bib103)) | ✓ |  |  | ✓ |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| Coavoux et al.(Coavoux et al., [2019](#bib.bib33)) |  | ✓ |  | ✓ |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| CNN | MV-CNN (Zhang et al., [2016](#bib.bib179)) | ✓ |  |  | ✓ |  | Pros:
    Good parallel computing; Cons: Not good at processing sequential data |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| TCSum (Cao et al., [2017](#bib.bib19)) | ✓ |  |  | ✓ |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| CNNLM (Yin and Pei, [2015](#bib.bib169)) | ✓ |  |  | ✓ |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| PriorSum (Cao et al., [2015b](#bib.bib21)) | ✓ |  |  | ✓ |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| Angelidis et al.(Angelidis and Lapata, [2018](#bib.bib5)) | ✓ |  |  | ✓ |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| GNN | Yasunaga et al.(Yasunaga et al., [2017](#bib.bib168)) | ✓ |  |  |  |
    ✓ | Pros: Can capture cross-document and in-document relations Cons: Inefficient
    when dealing with large graphs |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| SemSentSum (Antognini and Faltings, [2019](#bib.bib6)) | ✓ |  |  |  | ✓ |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| Scisummnet (Yasunaga et al., [2019](#bib.bib167)) | ✓ |  |  |  | ✓ |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| HDSG (Wang et al., [2020a](#bib.bib156)) | ✓ |  |  |  | ✓ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| PG | PG-MMR (Lebanoff et al., [2018](#bib.bib84)) |  | ✓ |  | ✓ |  | Pros:
    Low redundancy Cons: Hard to train |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| Hi-MAP (Fabbri et al., [2019](#bib.bib44)) |  | ✓ |  | ✓ |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| Transformer | HT (Liu and Lapata, [2019](#bib.bib95)) |  | ✓ |  |  | ✓ |
    Pros: Good performance; Good parallel computing; Can capture cross-document and
    in-document relations Cons: Time-consuming; Problems with position encoding |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| MGSum (Jin et al., [2020](#bib.bib72)) | ✓ | ✓ |  |  | ✓ |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| FewSum (Brazinskas et al., [2020](#bib.bib17)) | ✓ | ✓ |  | ✓ |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| GraphSum (Li et al., [2020a](#bib.bib91)) |  | ✓ |  |  | ✓ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| Bart-Long (Pasunuru et al., [2021b](#bib.bib123)) |  | ✓ |  |  | ✓ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| WikiSum (Liu et al., [2018](#bib.bib94)) |  |  | ✓ | ✓ |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| Deep Hybid Model | Cho et al.(Cho et al., [2019](#bib.bib29)) | ✓ |  |  |
    ✓ |  | Pros: Combines the advantages of different DL models Cons: Computationally
    intensive |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| GT-SingPairMix (Lebanoff et al., [2019](#bib.bib83)) |  | ✓ |  | ✓ |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| HNet (Singh et al., [2018](#bib.bib142)) | ✓ |  |  | ✓ |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: 3.8\. The Variants of Multi-document Summarization
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we briefly introduce several MDS task variants to give researchers
    a comprehensive understanding of MDS. These tasks can be modeled as MDS problems
    and adopt the aforementioned deep learning techniques and neural network architectures.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Query-oriented MDS calls for a summary from a set of documents that answers
    a query. It tries to solve realistic query-oriented scenario problems and only
    summarizes important information that best answers the query in a logical order
    (Pasunuru et al., [2021a](#bib.bib122)). Specifically, query-oriented MDS combines
    the information retrieval and MDS techniques. The content that needs to be summarized
    is based on the given queries. Liu et al. (Liu and Lapata, [2019](#bib.bib95))
    incorporated the query by simply prepending the query to the top-ranked document
    during encoding. Pasunuru (Pasunuru et al., [2021a](#bib.bib122)) involved a query
    encoder and integrated query embedding into an MDS model, ranking the importance
    of documents for a given query.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Dialogue summarization aims to provide a succinct synopsis from multiple textual
    utterances of two or more participants, which could help quickly capture relevant
    information without having to listen to long and convoluted dialogues (Liu et al.,
    [2019b](#bib.bib93)). Dialogue summary covers several areas, including meetings
    (Zhu et al., [2020](#bib.bib187); Koay et al., [2020](#bib.bib78); Feng et al.,
    [2021](#bib.bib45)), email threads (Zhang et al., [2021](#bib.bib175)), medical
    dialogues (Song et al., [2020](#bib.bib143); Joshi et al., [2020](#bib.bib73);
    Enarvi et al., [2020](#bib.bib40)), customer service (Liu et al., [2019b](#bib.bib93))
    and media interviews (Zhu et al., [2021](#bib.bib186)). Challenges in dialogue
    summarization can be summarized into the following seven categories: informal
    language use, multiple participants, multiple turns, referral and coreference,
    repetition and interruption, negations and rhetorical questions, role and language
    change (Chen and Yang, [2020](#bib.bib25)). The flow of the dialogue would be
    neglected if MDS models are directly applied for dialogue summarization. Liu et
    al. (Liu et al., [2019b](#bib.bib93)) relied on human annotations to capture the
    logic of the dialogue. Wu et al. (Wu et al., [2021](#bib.bib159)) used summary
    sketch to identify the interaction between speakers and their corresponding textual
    utterances in each turn. Chen et al. (Chen and Yang, [2020](#bib.bib25)) proposed
    a multi-view sequence to sequence based encoder to extract dialogue structure
    and a multi-view decoder to incorporate different views to generate final summaries.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Stream summarization aims to summarize new documents in a continuously growing
    document stream, such as information from social media. Temporal summarization
    and real-time summarization (RTS)⁹⁹9http://trecrts.github.io/ can be seen as a
    form of stream document summarization. Stream summarization considers both historical
    dependencies and future uncertainty of the document stream. Yang et al. (Yang
    et al., [[n.d.]](#bib.bib162)) used deep reinforcement learning to solve the relevance,
    redundancy, and timeliness issues in steam summarization. Tan et al. (Tan et al.,
    [2017](#bib.bib146)) transformed the real time summarization task as a sequential
    decision making problem and used a LSTM layer and three fully connected neural
    network layers to maximize the long-term rewards.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 3.9\. Discussion
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we have reviewed the state-of-the-art works of deep learning
    based MDS models according to the neural networks applied. Table [2](#S3.T2 "Table
    2 ‣ 3.7\. Deep Hybrid Models ‣ 3\. Deep Learning Based Multi-document Summarization
    Methods ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")
    summarizes the reviewed works by considering the type of neural networks, construction
    types, and concatenation methods; and provides a high-level summary of their relative
    advantages and disadvantages. Transformer based models have been most commonly
    used in the last three years because they overcome the limitations of CNN’s fixed-size
    receptive field and RNN’s inability to parallel process. However, deep learning
    based MDS models face some challenges. Firstly, the complexity of deep learning
    based models and the data-driven deep learning systems do require more training
    data, with concomitant increased efforts in data labelling, and computing resources
    than non-deep learning based methods, which are not time efficient. Secondly,
    deep learning based methods lack linguistic knowledge that can serve as important
    roles in assisting deep learning based learners to have informative representation
    and better guide the summary generation. We believe that this is one possible
    reason that some non-deep learning based MDS methods sometimes show better performance
    than deep learning based methods (Lu et al., [2020](#bib.bib98); Cao et al., [2015b](#bib.bib21))
    as non-deep learning based methods pay more attention to linguistic information.
    We discuss this point in Section [7](#S7 "7\. Future research directions and open
    issues ‣ Multi-document Summarization via Deep Learning Techniques: A Survey").
    Further researches could also be based on techniques adopted in non-deep learning
    based MDS as reviewed in (Ferreira et al., [2014](#bib.bib46); Shah and Jivani,
    [2016](#bib.bib139); El-Kassas et al., [2021](#bib.bib39)).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Objective Functions
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will take a closer look at different objective functions
    adopted by various MDS models. In summarization models, objective functions play
    an important role by guiding the model to achieve specific purposes. To the best
    of our knowledge, we are the first to provide a comprehensive survey on different
    objectives of summarization tasks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Cross-Entropy Objective
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cross-entropy usually acts as an objective function to measure the distance
    between two distributions. Many existing MDS models adopt it to measure the difference
    between the distributions of generated summaries and the golden summaries (Cao
    et al., [2015a](#bib.bib20); Zhang et al., [2016](#bib.bib179); Wang et al., [2020a](#bib.bib156);
    Zhang et al., [2018b](#bib.bib172); Cho et al., [2019](#bib.bib29); Yasunaga et al.,
    [2019](#bib.bib167)). Formally, cross-entropy loss is defined as:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\small L_{CE}=-\sum_{i=1}\mathbf{y_{i}}\log(\mathbf{\hat{y}_{i}}),$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{y_{i}}$ is the target score from golden summaries and machine-generated
    summaries, and $\mathbf{\hat{y}_{i}}$ is the predicted estimation from the deep
    learning based models. Different from calculations in other tasks, such as text
    classification, in summarization tasks, $\mathbf{y_{i}}$ and $\mathbf{\hat{y}_{i}}$
    have several methods to calculate. $\mathbf{\hat{y}_{i}}$ usually is calculated
    by Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Please refer to
    Section [5](#S5 "5\. Evaluation metrics ‣ Multi-document Summarization via Deep
    Learning Techniques: A Survey")). For example, ROUGE-1(Antognini and Faltings,
    [2019](#bib.bib6)), ROUGE-2 (Liu and Lapata, [2019](#bib.bib95)) or the normalized
    average of ROUGE-1 and ROUGE-2 scores (Yasunaga et al., [2017](#bib.bib168)) could
    be adopted to compute the ground truth score between the selected sentences and
    golden summary.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Reconstructive Objective
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reconstructive objectives are used to train a distinctive representation learner
    by reconstructing the input vectors in an unsupervised learning manner. The objective
    function is defined as:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\small L_{Rec}=\left\&#124;\mathbf{x_{i}}-\phi^{\prime}(\phi(\mathbf{x_{i}};\theta);\theta^{\prime})\right\&#124;_{*},$
    |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{x_{i}}$ represents the input vector; $\phi$ and $\phi^{\prime}$
    represent the encoder and decoder with $\theta$ and $\theta^{\prime}$ as their
    parameters respectively, $||\cdot||_{*}$ represents norm (* stands for 0, 1, 2,
    …, infinity). $L_{Rec}$ is a measuring function to calculate the distance between
    source documents and their reconstructive outputs. Chu et al. (Chu and Liu, [2019](#bib.bib31))
    used a reconstructive loss to constrain the generated text into the natural language
    domain, reconstructing reviews in a token-by-token manner. Moreover, this paper
    also proposes a variant termed reconstruction cycle loss. By using the variant,
    the reviews are encoded into a latent space to further generate the summary, and
    the summary is then decoded to the reconstructed reviews to form another reconstructive
    closed-loop. An unsupervised learning loss was designed by Li et al. (Li et al.,
    [2017b](#bib.bib90)) to reconstruct the condensed output vectors to the original
    input sentence vectors with $L_{2}$ distance. This paper further constrains the
    condensed output vector with a $L_{1}$ regularizer to ensure sparsity. Similarly,
    Zheng et al. (Zheng et al., [2019](#bib.bib182)) adopted a bi-directional GRU
    encoder-decoder framework to reconstruct both news sentences and comment sentences
    in a word sequence manner. Liu et al. (Liu et al., [2018](#bib.bib94)) used reconstruction
    within the abstractive stage of a two-stage strategy to alleviate the problem
    introduced by long input documents. Both input and output sequences are concatenated
    to predict the next token to train the abstractive model. There are also some
    variants, such as leveraging the latent vectors of variational auto-encoder for
    reconstruction to capture better representation. Li et al. (Li et al., [2017a](#bib.bib89))
    introduced three individual reconstructive losses to consider both news reconstruction
    and comments reconstruction separately, along with a variational auto-encoder
    lower bound. Bravzinskas et al. (Bražinskas et al., [2019](#bib.bib16)) utilized
    a variational auto-encoder to generate the latent vectors of given reviews, where
    each review is reconstructed by the latent vectors combined with other reviews.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Redundancy Objective
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Redundancy is an important objective to minimize the overlap between semantic
    units in a machine-generated summary. By using this objective, models are encouraged
    to maximize information coverage. Formally,
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\small L_{Red}=Sim(\mathbf{x_{i}},\mathbf{x_{j}}),$ |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: where $Sim(\cdot)$ is the similarity function to measure the overlap between
    different $\mathbf{x_{i}}$ and $\mathbf{x_{j}}$, which can be phrases, sentences,
    topics or documents. The redundancy objective is often treated as an auxiliary
    objective combined with other loss functions. Li et al. (Li et al., [2017b](#bib.bib90))
    penalized phrase pairs with similar meanings to eliminate the redundancy. Nayeem
    et al. (Nayeem et al., [2018](#bib.bib112)) used the redundancy objective to avoid
    generating repetitive phrases, constraining a sentence to appear only once while
    maximizing the scores of important phrases. Zheng et al. (Zheng et al., [2019](#bib.bib182))
    adopted a redundancy loss function to measure overlaps between subtopics; intuitively,
    smaller overlaps between subtopics resulted in less redundancy in the output domain.
    Yin et al. (Yin and Pei, [2015](#bib.bib169)) proposed a redundancy objective
    to estimate the diversity between different sentences.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Max Margin Objective
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Max Margin Objectives (MMO) are also used to empower the MDS models to learn
    better representation. The objective function is formalized as:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\small L_{Margin}=\max\left(0,f(\mathbf{x_{i}};\theta)-f(\mathbf{x_{j}};\theta)+\gamma\right),$
    |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{x_{i}}$ and $\mathbf{x_{j}}$ represent the input vectors, $\theta$
    are parameters of the model function $f(\cdot)$, and $\gamma$ is the margin threshold.
    The MMO aims to force function $f(\mathbf{x_{i}};\theta)$ and function $f(\mathbf{x_{j}};\theta)$
    to be separated by a predefined margin $\gamma$. In Cao et al. (Cao et al., [2017](#bib.bib19)),
    a MMO is designed to constrain a pair of randomly sampled sentences with different
    salience scores – the one with higher score should be larger than the other one
    more than a marginal threshold. Two max margin losses are proposed in Zhong et
    al. (Zhong et al., [2020](#bib.bib183)): a margin-based triplet loss that encouraged
    the model to pull the golden summaries semantically closer to the original documents
    than to the machine-generated summaries; and a pair-wise margin loss based on
    a greater margin between paired candidates with more disparate ROUGE score rankings.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Multi-Task Objective
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Supervision signals from MDS objectives may not be strong enough for representation
    learners, so some works seek other supervision signals from multiple tasks. A
    general form is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\small L_{Mul}=L_{Summ}+L_{Other},$ |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: 'where $L_{Summ}$ is the loss function of MDS tasks, and $L_{Other}$ is the
    loss function of an auxiliary task. Angelidis et al. (Angelidis and Lapata, [2018](#bib.bib5))
    assumed that the aspect-relevant words not only provides a reasonable basis for
    model aspect reconstruction, but also a good indicator for product domain. Similarly,
    multi-task classification was introduced by Cao et al. (Cao et al., [2017](#bib.bib19)).
    Two models are maintained: text classification and text summarization models.
    In the first model, CNN is used to classify text categories and cross-entropy
    loss is used as the objective function. The summarization model and the text classification
    model share parameters and pooling operations, so are equivalent to the shared
    document vector representation. Coavoux et al. (Coavoux et al., [2019](#bib.bib33))
    jointly optimized the model from a language modeling objective and two other multi-task
    supervised classification losses, which are polarity loss and aspect loss.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 4.6\. Other Types of Objectives
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are many other types of objectives in addition to those mentioned above.
    Cao et al. (Cao et al., [2015b](#bib.bib21)) proposed using ROUGE-2 to calculate
    the sentence saliency scores and the model tries to estimate this saliency with
    linear regression. Yin et al. (Yin and Pei, [2015](#bib.bib169)) suggested summing
    the squares of the prestige vectors calculated by the PageRank algorithm to identify
    sentence importance. Zhang et al. (Zhang et al., [2016](#bib.bib179)) proposed
    an objective function by ensembling individual scores from multiple CNN models;
    besides the cross-entropy loss, a consensus objective is adopted to minimize disagreement
    between each pair of classifiers. Amplay et al. (Amplayo and Lapata, [2021](#bib.bib4))
    used two objectives in the abstract module: the first to optimize the generation
    probability distribution by maximizing the likelihood; and the second to constrain
    the model output to be close to its golden summary in the encoding space, as well
    as being distant from the random sampled negative summaries. Chu et al. (Chu and
    Liu, [2019](#bib.bib31)) designed a similarity objective that shares the encoder
    and decoder weights within the auto-encoder module, while in the summarization
    module, the average cosine distance indicates the similarity between the generated
    summary and the reviews. A variant similarity objective termed early cosine objective
    is further proposed to compute the similarity in a latent space which is the average
    of the cells states and hidden states to constrain the generated summaries semantically
    close to reviews.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 4.7\. Discussion
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In MDS, cross-entropy is the most commonly adopted objective function that bridges
    the predicted candidate summaries and the golden summaries by treating the golden
    summaries as strong supervision signals. However, adopting cross-entropy loss
    alone may not lead the model to achieve good performance since the supervisory
    signal for cross-entropy objective is not strong enough by itself to effectively
    learn good representation. Several other objectives can thus serve as complements,
    e.g., reconstruction objectives offer a view from the unsupervised learning perspective;
    the redundancy objective constrains models from generating redundant content;
    while max-margin objectives require a step-change improvements from previous versions.
    By using multiple objectives, model optimization could be conducted with the input
    documents themselves if the manual annotation is scarce. The models that adopt
    multi-task objectives explicitly define multiple auxiliary tasks to assist the
    main summarization task for better generalization, and provide various constraints
    from different angles that lead to better model optimization.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Evaluation metrics
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluation metrics are used to measure the effectiveness of a given method
    objectively, so well-defined evaluation metrics are crucial to MDS research. We
    classify the existing evaluation metrics in two categories and will discuss each
    category in detail: (1) ROUGE: the most commonly used evaluation metrics in the
    summarization community; and (2) other evaluation metrics that have not been widely
    used in MDS research to date.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. ROUGE
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, [2004](#bib.bib92))
    is a collection of evaluation indicators that is one of the most essential metrics
    for many natural language processing tasks, including machine translation and
    text summarization. ROUGE obtains prediction/ground-truth similarity scores through
    comparing automatically generated summaries with a set of corresponding human-written
    references. ROUGE has many variants to measure candidate abstracts in a variety
    of ways (Lin, [2004](#bib.bib92)). The most commonly used ones are ROUGE-N and
    ROUGE-L.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'ROUGE-N (ROUGE with n-gram co-occurrence statistics ) measures a n-gram recall
    between reference summaries and their corresponding candidate summaries (Lin,
    [2004](#bib.bib92)). Formally, ROUGE-N can be calculated as:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\small ROUGE\text{-}N=\frac{\sum_{Sum\in\{Ref\}}\sum_{gram_{n}\in
    Sum}Count_{match}(gram_{n})}{\sum_{Sum\in\{Ref\}}\sum_{gram_{n}\in Sum}Count(gram_{n})},$
    |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: where $Ref$ and $Sum$ are reference summary and machine-generated summary, $n$
    represents the length of n-gram, and $Count_{match}(gram_{n})$ represents the
    maximum number of n-grams in the reference summary and corresponding candidates.
    The numerator of ROUGE-N is the number of n-grams owned by both the reference
    and generated summary, while the denominator is the total number of n-grams occurring
    in the golden summary. The denominator could also be set to the number of candidate
    summary n-grams to measure precision; however, ROUGE-N mainly focuses on quantifying
    recall, so precision is not usually calculated. ROUGE-1 and ROUGE-2 are special
    cases of ROUGE-N that are usually chosen as best practices and represent the unigram
    and bigram, respectively.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'ROUGE-L (ROUGE with Longest Common Subsequence) adopts the longest common subsequence
    algorithm to count the longest matching vocabularies (Lin, [2004](#bib.bib92)).
    Formally, ROUGE-L is calculated using:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\small F_{lcs}=\frac{(1+\beta^{2})R_{lcs}P_{lcs}}{R_{lcs}+\beta^{2}P_{lcs}},$
    |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: where
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\small R_{lcs}=\frac{LCS(Ref,Sum)}{m},$ |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: and
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $\small P_{lcs}=\frac{LCS(Ref,Sum)}{n}.$ |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: where LCS($\cdot$) represents the longest common subsequence function. ROUGE-L
    is termed as LCS-based F-measure as it is obtained from LCS-Precision $P_{lcs}$
    and LCS-Recall $R_{lcs}$. $\beta$ is the balance factor between $R_{lcs}$ and
    $P_{lcs}$. It can be set by the fraction of $P_{lcs}$ and $R_{lcs}$; by setting
    $\beta$ to a big number, only $R_{lcs}$ is considered. The use of ROUGE-L enables
    measurement of the similarity of two text sequences at sentence-level. ROUGE-L
    also has the advantage of automatically deciding the n-gram without extra manual
    input, since the calculation of LCS empowers the model to count grams adaptively.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Other ROUGE Based Metrics. ROUGE-W (Lin, [2004](#bib.bib92)) is proposed to
    weight consecutive matches to better measure semantic similarities between two
    texts. ROUGE-S (Lin, [2004](#bib.bib92)) stands for ROUGE with Skip-bigram co-occurrence
    statistics that allows the bigram to skip arbitrary words. An extension of ROUGE-S,
    ROUGE-SU (Lin, [2004](#bib.bib92)) refers to ROUGE with Skip-bigram plus Unigram-based
    co-occurrence statistics and is able to be obtained from ROUGE-S by adding a begin-of-sentence
    token at the start of both references and candidates. ROUGE-WE (Ng and Abrecht,
    [2015](#bib.bib117)) is proposed to further extend ROUGE by measuring the pair-wise
    summary distances in word embeddings space. In recent years, more ROUGE-based
    evaluation models have been proposed to compare golden and machine-generated summaries,
    not just according to their literal similarity, but also considering semantic
    similarity (ShafieiBavani et al., [2018](#bib.bib138); Zhao et al., [2019](#bib.bib181);
    Zhang et al., [2020a](#bib.bib176)). In terms of the ROUGE metric for multiple
    golden summaries, the Jackknifing procedure (similar to K-fold validation) has
    been introduced (Lin, [2004](#bib.bib92)). The $M$ best scores are computed from
    sets composed of $M$-1 reference summaries and the final ROUGE-N is the average
    of $M$ scores. This procedure can also be applied to ROUGE-L, ROUGE-W and ROUGE-S.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Other Evaluation Metrics
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides ROUGE-based (Lin, [2004](#bib.bib92)) metrics, other evaluation metrics
    for MDS exist, but have received less attention than ROUGE. We hope this section
    will give researchers and practitioners a holistic view of alternative evaluation
    metrics in this field. Based on the mode of summaries matching, we divide the
    evaluation metrics into two groups: lexical matching metrics and semantic matching
    metrics.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Advantages and disadvantages of different evaluation metrics.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluation Metrics | Advantages | Disadvantages |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| Lexical Matching Metrics | ROUGE |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '&#124; •  Widely used &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Intuitive &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Easily computed &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Cannot measure texts &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     semantically &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Exact matching &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '| BLEU |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '&#124; •  Intuitive &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Easily computed &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  High correlations with &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     human judgments &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Cannot measure texts &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     semantically &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Cannot deal with &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     languages lacking word &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     boundaries &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '| Perplexity |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '&#124; •  Easily computed &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Intuitive &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Sensitive to certain &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     symbols and words &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '| Pyramid |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '&#124; •  High correlations with &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     human judgments &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Requires manually &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     extraction of units &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Bias results easily &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '| Responsiveness |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '&#124; •  Consider both content and &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     linguistic quality &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Can be calculated without &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     reference &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '| •  Not widely adopted |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Data &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Statistics &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Can measure the density &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     and coverage of summary &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Cannot measure texts &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     semantically &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '| Semantic Matching Metrics | MEREOR | •  Consider non-exact matching |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '&#124; •  Sensitive to length &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '| SUPERT |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '&#124; •  Can measuring texts semantic &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     similarity &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '| •  Not widely adopted |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Preferences &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; based Metric &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Does not depend on the &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     golden summaries &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Require human &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     annotations &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '| BERTScore |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '&#124; •  Semantically measure texts to &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     some extent &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Mimic human evaluation &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  High computational &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     demands &#124;'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '| MoverScore |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '&#124; •  Semantically measure texts to &#124;'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     some extent &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  More similar to human &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     evaluation by adopting earth &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     mover’s distance &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  High computational &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     demands &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '| Importance |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '&#124; •  Combining redundancy, &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     relevance and informativeness &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Theoretically supported &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Non-trivial for &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     implementation &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Human &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Evaluation &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Can accurately and &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     semantically measure texts &#124;'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Require human &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;     annotations &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Lexical Matching Metrics. BLEU (Papineni et al., [2002](#bib.bib120)) is a
    commonly used vocabulary-based evaluation metric that provides a precision-based
    evaluation indicator, as opposed to ROUGE that mainly focuses on recall. Perplexity
    (Jelinek et al., [1977](#bib.bib70)) is used to evaluate the quality of the language
    model by calculating the negative log probability of a word’s appearance. A low
    perplexity on a test dataset is a strong indicator of a summary’s high grammatical
    quality because it measures the probability of words appearing in sequences. Based
    on Pyramid (Nenkova et al., [2007](#bib.bib115)) calculation, the abstract sentences
    are manually divided into several Summarization Content Units (SCUs), each representing
    a core concept formed from a single word or phrase/sentence. After sorting SCUs
    in order of importance to form the Pyramid, the quality of automatic summarization
    is evaluated by calculating the number and importance of SCUs included in the
    document (Nenkova and Passonneau, [2004](#bib.bib116)). Intuitively, more important
    SCUs exist at higher levels of the pyramid. Although Pyramid shows strong correlation
    with human judgment, it requires professional annotations to match and evaluate
    SCUs in generated and golden summaries. Some recent works focus on the construction
    of Pyramid (Passonneau et al., [2013](#bib.bib121); Yang et al., [2016](#bib.bib164);
    Hirao et al., [2018](#bib.bib61); Gao et al., [2019](#bib.bib48); Shapira et al.,
    [2019](#bib.bib140)). Responsiveness (Louis and Nenkova, [2013](#bib.bib97)) measures
    content selection and linguistic quality of summaries by directly rating scores.
    Additionally, the assessments are calculated without reference to model summaries.
    Data Statistics (Grusky et al., [2018](#bib.bib55)) contain three evaluation metrics:
    extractive fragment coverage measures the novelty of generated summaries by calculating
    the percentage of words in the summary that are also present in source documents;
    extractive fragment density measures the average length of the extractive block
    to which each word in the summary belongs; and compression ratio compares the
    word numbers in the source documents and generated summary.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic Matching Metrics. METEOR (Metric for Evaluation of Translation with
    Explicit Ordering) (Banerjee and Lavie, [2005](#bib.bib10)) is an improvement
    to BLEU. The main idea behind METEOR is that while candidate summaries can be
    correct with similar meanings, they are not exactly matched with references. In
    such a case, WordNet^(10)^(10)10https://wordnet.princeton.edu/ is introduced to
    expand the synonym set, and the word form is also taken into account. SUPERT (Gao
    et al., [2020](#bib.bib49)) is an unsupervised MDS evaluation metric that measures
    the semantic similarity between the pseudo-reference summary and the machine-generated
    summary. SUPERT obviates the need for human annotations by not referring to golden
    summaries. Contextualized embeddings and soft token alignment techniques are leveraged
    to select salient information from the input documents to evaluate summary quality.
    Preferences based Metric (Zopf, [2018](#bib.bib189)) is a pairwise sentence preference-based
    evaluation model and it does not depend on the golden summaries. The underlying
    premise is to ask annotators about their pair-wise preferences rather than writing
    complex golden summaries, and are much easier and faster to obtain than traditional
    reference summary-based evaluation models. BERTScore (Zhang et al., [2020a](#bib.bib176))
    computes a similarity score for each token within the candidate sentence and the
    reference sentence. It measures the soft overlap of two texts’ BERT embeddings.
    MoverScore (Zhao et al., [2019](#bib.bib181)) adopts a distance to evaluate the
    agreement between two texts in the context of BERT and ELMo word embeddings. This
    proposed metric has a high correlation with human judgment of text quality by
    adopting earth mover’s distance. Importance (Peyrard, [2019](#bib.bib126)) is
    a simple but rigorous evaluation metric from the aspect of information theory.
    It is a final indicator calculated from the three aspects: Redundancy, Relevance,
    and Informativeness. A good summary should have low Redundancy and high Relevance
    and high Informativeness. The cluster of Human Evaluation is used to supplement
    automatic evaluation on relatively small instances. Annotators evaluate the quality
    of machine-generated summaries by rating Informativeness, Fluency, Conciseness,
    Readability, Relevance. Model ratings are usually computed by averaging the rating
    on all selected summary pairs.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Discussion
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We summarize the advantages and disadvantages of above-mentioned evaluation
    metrics in Table [3](#S5.T3 "Table 3 ‣ 5.2\. Other Evaluation Metrics ‣ 5\. Evaluation
    metrics ‣ Multi-document Summarization via Deep Learning Techniques: A Survey").
    Although there are many evaluation metrics for MDS, the indicators of the ROUGE
    series are generally accepted by the summarization community. Almost all the research
    works utilize ROUGE for evaluation, while other evaluation indicators are just
    for assistance currently. Among the ROUGE family, ROUGE-1, ROUGE-2 and ROUGE-L
    are the most commonly used evaluation metrics. In addition, there are plenty of
    existing evaluation metrics in other natural language processing tasks that could
    be potentially adjusted for MDS tasks, such as efficiency, effectiveness and coverage
    from information retrieval.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Datasets
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compared to SDS tasks, large-scale MDS datasets, which contain more general
    scenarios with many downstream tasks, are relatively scarce. In this section,
    we present our investigation on the 10 most representative datasets commonly used
    for MDS and its variant tasks.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'DUC & TAC. DUC^(11)^(11)11http://duc.nist.gov/ (Document Understanding Conference)
    provides official text summarization competitions each year from 2001-2007 to
    promote summarization research. DUC changed its name to Text Analysis Conference
    (TAC)^(12)^(12)12 http://www.nist.gov/tac/ in 2008\. Here, the DUC datasets refer
    to the data collected from 2001-2007; the TAC datasets refer to the dataset after
    2008\. Both DUC and TAC datasets are from the news domains, including various
    topics such as politics, natural disaster and biography. Nevertheless, as shown
    in Table [4](#S6.T4 "Table 4 ‣ 6\. Datasets ‣ Multi-document Summarization via
    Deep Learning Techniques: A Survey"), the DUC and TAC datasets provide small datasets
    for model evaluation that only include hundreds of news documents and human-annotated
    summaries. Of note, the first sentence in a news item is usually information-rich
    that renders bias in the news datasets, so it fails to reflect the structure of
    natural documents in daily lives. These two datasets are on a relatively small
    scale and not ideal for large-scale deep neural based MDS model training and evaluation.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Comparison of Different Datasets. In the table, “Ave”, “Summ”, “Len”,
    “bus”,“rev” and “#” represent average, summary, length, business, reviews and
    numbers respectively; “Docs” and “sents” mean documents and sentences respectively.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Cluster # | Document # | Summ # | Ave Summ Len | Topic |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| DUC01 | 30 | 309 docs | 60 summ | 100 words | News |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| DUC02 | 59 | 567 docs | 116 summ | 100 words | News |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| DUC03 | 30 | 298 docs | 120 summ | 100 words | News |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| DUC04 | 50 | 10 docs / cluster | 200 summ | 665 bytes | News |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| DUC05 | 50 | 25-50 docs / cluster | 140 summ | 250 words | News |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| DUC06 | 50 | 25 docs / cluster | 4 summ / cluster | 250 words | News |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| DUC07 | 45 | 25 docs / cluster | 4 summ / cluster | 250 words | News |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| TAC 2008 | 48 | 10 docs / cluster | 4 summ / cluster | 100 words | News |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| TAC 2009 | 44 | 10 docs / cluster | 4 summ / cluster | 100 words | News |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| TAC 2010 | 46 | 10 docs / cluster | 4 summ / cluster | 100 words | News |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| TAC 2011 | 44 | 10 docs / cluster | 4 summ / cluster | 100 words | News |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| OPOSUM | 60 | 600 rev | 1 summ / cluster | 100 words |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '&#124; Amazon &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reviews &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '| WikiSum | - |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '&#124; train / val / test &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1579360 / 38144 / 38205 &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 summ / cluster | 139.4 tokens/ summ | Wikipedia |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| Multi-News | - |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '&#124; train / val / test &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 44972 / 5622 / 5622 &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2-10 docs / cluster &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 summ / cluster |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '&#124; 263.66 words / summ &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 9.97 sents / summ &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 262 tokens / summ &#124;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '| News |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| Opinosis | 51 | 6457 rev | 5 summ / cluster | - |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '&#124; Site &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reviews &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rotten &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Tomatoes &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '| 3731 | 99.8 rev / cluster | 1 summ / cluster | 19.6 tokens / summ |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '&#124; Movie &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reviews &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '| Yelp | - |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '&#124; train / val / test &#124;'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; bus: 10695 / 1337 / 1337 &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; rev: 1038184 / 129856 / 129840 &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '| - | - |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: '&#124; Customer &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reviews &#124;'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '| Scisumm | 1000 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '&#124; 21 - 928 cites / paper &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 15 sents / refer &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 summ / cluster | 151 words |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '&#124; Science &#124;'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paper &#124;'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '| WCEP | 10200 | 235 docs / cluster | 1 summ / cluster | 32 words | Wikipedia
    |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: '| Multi-XScience | - |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: '&#124; train / val / test &#124;'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 30369 / 5066 / 5093 &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 summ / cluster |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '&#124; 116.44 words / summ &#124;'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Science &#124;'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paper &#124;'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: OPOSUM. OPOSUM (Angelidis and Lapata, [2018](#bib.bib5)) collects multiple reviews
    of six product domains from Amazon. This dataset not only contains multiple reviews
    and corresponding summaries but also products’ domain and polarity information.
    The latter information could be used as the auxiliary supervision signals.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: WikiSum. WikiSum (Liu et al., [2018](#bib.bib94)) targets abstractive MDS. For
    a specific Wikipedia theme, the documents cited in Wikipedia articles or the top-10
    Google search results (using the Wikipedia theme as query) are seen as the source
    documents. Golden summaries are the real Wikipedia articles. However, some of
    the URLs are not available and can be identical to each other in parts. To remedy
    these problems, Liu et al. (Liu and Lapata, [2019](#bib.bib95)) cleaned the dataset
    and deleted duplicated examples, so here we report statistical results from (Liu
    and Lapata, [2019](#bib.bib95)).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Multi-News. Multi-News (Fabbri et al., [2019](#bib.bib44)) is a relatively large-scale
    dataset in the news domain; the articles and human-written summaries are all from
    the Web^(13)^(13)13http://newser.com. This dataset includes 56,216 article-summary
    pairs and contain trace-back links to the original documents. Moreover, the authors
    compared the Multi-News dataset with prior datasets in terms of coverage, density,
    and compression, revealing that this dataset has various arrangement styles of
    sequences.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: Opinosis. The Opinosis dataset (Ganesan et al., [2010](#bib.bib47)) contains
    reviews of 51 topic clusters collected from TripAdvisor^(14)^(14)14https://www.tripadvisor.com/,
    Amazon^(15)^(15)15https://www.amazon.com.au/, and Edmunds^(16)^(16)16https://www.edmunds.com/.
    For each topic, approximately 100 sentences on average are provided and the reviews
    are fetched from different sources. For each cluster, five professional written
    golden summaries are provided for the model training and evaluation.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Rotten Tomatoes. The Rotten Tomatoes dataset (Wang and Ling, [2016](#bib.bib158))
    consists of the collected reviews of 3,731 movies from the Rotten Tomato website^(17)^(17)17http://rottentomatoes.com.
    The reviews contain both professional critics and user comments. For each movie,
    a one-sentence summary is created by professional editors.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Yelp. Chu et al. (Chu and Liu, [2019](#bib.bib31)) proposed a dataset named
    Yelp based on the Yelp Dataset Challenge. This dataset includes multiple customer
    reviews with five-star ratings. The authors provided 100 manual-written summaries
    for model evaluation using Amazon Mechanical Turk (AMT), within which every eight
    input reviews are summarized into one golden summary.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Scisumm. Scisumm dataset (Yasunaga et al., [2019](#bib.bib167)) is a large,
    manually annotated corpus for scientific document summarization. The input documents
    are a scientific publication, called the reference paper, and multiple sentences
    from the literature that cite this reference paper. In the SciSumm dataset, the
    1,000 most cited papers from the ACL Anthology Network (Radev et al., [2013](#bib.bib129))
    are treated as reference papers, and an average 15 citation sentences are provided
    after cleaning. For each cluster, one golden summary is created by five NLP-based
    Ph.D. students or equivalent professionals.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: WCEP. The Wikipedia Current Events Portal dataset (WCEP) (Ghalandari et al.,
    [2020](#bib.bib51)) contains human-written summaries of recent news events. Similar
    articles are provided by searching similar articles from Common Crawl News dataset^(18)^(18)18https://commoncrawl.org/2016/10/news-dataset-available/
    to extend the inputs to obtain large-scale news articles. Overall, the WCEP dataset
    has good alignment with the real-world industrial use cases.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Multi-XScience. The source data of Multi-XScience (Lu et al., [2020](#bib.bib98))
    are from Arxiv and Microsoft academic graphs and this dataset is suitable for
    abstractive MDS. Multi-XScience contains fewer positional and extractive biases
    than WikiSum and Multi-News datasets, so the drawback of obtaining higher scores
    from a copy sentence at a certain position can be partially avoided.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Datasets for MDS Variants. The representative query-oriented MDS datasets are
    Debatepedia (Nema et al., [2017](#bib.bib113)), AQUAMUSE (Kulkarni et al., [2020](#bib.bib80)),
    and QBSUM (Zhao et al., [2021](#bib.bib180)). The representative dialogue summarization
    datasets are DIALOGSUM (Chen et al., [2021](#bib.bib26)), AMI (Carletta et al.,
    [2005](#bib.bib24)), MEDIASUM (Zhu et al., [2021](#bib.bib186)), and QMSum (Zhong
    et al., [2021](#bib.bib184)). RTS is a track at the Text Retrieval Conference
    (TREC) which provides several RTS datasets^(19)^(19)19http://trecrts.github.io/.
    Tweet Contextualization track (Bellot et al., [2016](#bib.bib13)) (2012-2014)
    is derived from the INEX 2011 Question Answering Track, that focuses on more NLP-oriented
    tasks and moves to MDS.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion. Table [4](#S6.T4 "Table 4 ‣ 6\. Datasets ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey") compares 20 MDS datasets based on the
    numbers of clusters and documents; the number and the average length of summaries;
    and the field to which the dataset belongs. Currently, the main areas covered
    by the MDS datasets are news (60$\%$), scientific papers (10$\%$) and Wikipedia
    (10$\%$). In early development of the MDS tasks, most studies were performed on
    the DUC and TAC datasets. However, the size of these datasets is relatively small,
    and thus not highly suitable for training deep neural network models. Datasets
    on news articles are also common, but the structure of news articles (highly compressed
    information in the first paragraph or first sentence of each paragraph) can cause
    positional and extractive biases during training. In recent years, large-scale
    datasets such as WikiSum and Multi-News datasets have been developed and used
    by researchers to meet training requirements, reflecting the rising trend of data-driven
    approaches.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Future research directions and open issues
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although existing works have established a solid foundation for MDS it is a
    relatively understudied field compared with SDS and other NLP topics. Summarizing
    on multi-modal data, medical records, codes, project activities and MDS combining
    with Internet of Things (Zhang et al., [2020c](#bib.bib178)) have still received
    less attention. Actually, MDS techniques are beneficial for a variety of practical
    applications, including generating Wikipedia articles, summarizing news, scientific
    papers, and product reviews, and individuals, industries have a huge demand for
    compressing multiple related documents into high-quality summaries. This section
    outlines several prospective research directions and open issues that we believe
    are critical to resolve in order to advance the field.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Capturing Cross-document Relations for MDS
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, many MDS models still center on simple concatenation of input documents
    into a flat sequence, ignoring cross-document relations. Unlike SDS, MDS input
    documents may contain redundant, complementary, or contradictory information (Radev,
    [2000](#bib.bib127)). Discovering cross-document relations, which can assist models
    to extract salient information, improve the coherence and reduce redundancy of
    summaries(Li et al., [2020a](#bib.bib91)). Research on capturing cross-document
    relations has begun to gain momentum in the past two years; one of the most widely
    studied topics is graphical models, which can easily be combined with deep learning
    based models such as graph neural networks and Transformer models. Several existing
    works indicate the efficacy of graph-based deep learning models in capturing semantic-rich
    and syntactic-rich representation and generating high-quality summaries (Wang
    et al., [2020a](#bib.bib156); Yasunaga et al., [2019](#bib.bib167); Li et al.,
    [2020a](#bib.bib91); Yasunaga et al., [2017](#bib.bib168)). To this end, a promising
    and important direction would be to design a better mechanism to introduce different
    graph structures (Christensen et al., [2013](#bib.bib30)) or linguistic knowledge
    (Bing et al., [2015](#bib.bib15); Ma et al., [2021](#bib.bib101)), possibly into
    the attention mechanism in deep learning based models, to capture cross-document
    relations and to facilitate summarization.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Creating More High-quality Datasets for MDS
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Benchmark datasets allow researchers to train, evaluate and compare the capabilities
    of different models on the same stage. High-quality datasets are critical to develop
    MDS tasks. DUC and TAC, the most common datasets used for MDS tasks, have a relatively
    small number of samples so are not very suitable for training DNN models. In recent
    years, some large datasets have been proposed, including WikiSum (Liu et al.,
    [2018](#bib.bib94)), Multi-News (Fabbri et al., [2019](#bib.bib44)), and WCEP
    (Ghalandari et al., [2020](#bib.bib51)), but more efforts are still needed. Datasets
    with documents of rich diversity, with minimal positional and extractive biases
    are desperately required to promote and accelerate MDS research, as are datasets
    for other applications such as summarization of medical records or dialogue (Molenaar
    et al., [2020](#bib.bib109)), email (Ulrich et al., [2008](#bib.bib150); Zajic
    et al., [2008](#bib.bib171)), code (Rodeghero et al., [2014](#bib.bib132); McBurney
    and McMillan, [2014](#bib.bib104)), software project activities (Alghamdi et al.,
    [2020](#bib.bib3)), legal documents (Kanapala et al., [2019](#bib.bib74)), and
    multi-modal data (Li et al., [2020b](#bib.bib86)). The development of large-scale
    cross-task datasets will facilitate multi-task learning (Xu et al., [2020b](#bib.bib160)).
    However, the datasets of MDS combining with text classification, question answering,
    or other language tasks have seldom been proposed in the MDS research community,
    but these datasets are essential and widely employed in industrial applications.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. Improving Evaluation Metrics for MDS
  id: totrans-457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To our best knowledge, there are no evaluation metrics specifically designed
    for MDS models – SDS and MDS models share the same evaluation metrics. New MDS
    evaluation metrics should be able to: (1) evaluating the relations between the
    different input documents in the generated summary; (2) measuring to what extent
    the redundancy in input documents is reduced; and (3) judging whether the contradictory
    information across documents is reasonably handled. A good evaluation indicator
    is able to reflect the true performance of an MDS model and guide design of improved
    models. However, current evaluation metrics (Fabbri et al., [2021](#bib.bib43))
    still have several obvious defects. For example, despite the effectiveness of
    commonly used ROUGE metrics, they struggle to accurately measure the semantic
    similarity between a golden and generated summary because ROUGE-based evaluation
    metrics only consider vocabulary-level distances; as such, even if a ROUGE score
    improves, it does not necessarily mean that the summary is of a higher quality
    and so is not ideal for model training. Recently, some works extend ROUGE along
    with WordNet (ShafieiBavani et al., [2018](#bib.bib138)) or pre-trained LMs (Zhang
    et al., [2020a](#bib.bib176)) to alleviate these drawbacks. It is challenging
    to propose evaluation indicators that can reflect the true quality of generated
    summaries comprehensively and as semantically as human raters. Another frontline
    challenge for evaluation metrics research is unsupervised evaluation, being explored
    by a number of recent studies (Sun and Nenkova, [2019](#bib.bib144); Gao et al.,
    [2020](#bib.bib49)).'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 7.4\. Reinforcement Learning for MDS
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement learning (Mnih et al., [2016](#bib.bib108)) is a cluster of algorithms
    based on dynamic programming according to the Bellman Equation to deal with sequential
    decision problems, where state transition dynamics of the environment are provided
    in advance. Several existing works (Paulus et al., [2018](#bib.bib124); Narayan
    et al., [2018](#bib.bib111); Yao et al., [2018](#bib.bib166)) model the document
    summarization task as a sequential decision problem and adopt reinforcement learning
    to tackle the task. Although deep reinforcement learning for SDS has made great
    progress, we still face challenges to adapt existing SDS models to MDS, as the
    latter suffer from a large state, action space, and problems with high redundancy
    and contradiction (Mao et al., [2020](#bib.bib103)). Additionally, current summarization
    methods are based on model-free reinforcement learning algorithms, in which the
    model is not aware of environment dynamics but continuously explores the environment
    through simple trial-and-error strategies, so they inevitably suffer from low
    sampling efficiencies. Nevertheless, the model-based approaches can leverage data
    more efficiently since they update models upon the prior to the environment. In
    this case, data-efficient reinforcement learning for MDS could potentially be
    explored in the future.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: 7.5\. Pre-trained Language Models for MDS
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many NLP tasks, the limited labeled corpora are not adequate to train semantic-rich
    word vectors. Using large-scale, unlabeled, task-agnostic corpora for pre-training
    can enhance the generalization ability of models and accelerate convergence of
    networks (Peters et al., [2018](#bib.bib125); Mikolov et al., [2013](#bib.bib107)).
    At present, pre-trained LMs have led to successes in many deep learning based
    NLP tasks. Among the reviewed papers (Zhong et al., [2020](#bib.bib183); Lebanoff
    et al., [2019](#bib.bib83); Li et al., [2020a](#bib.bib91)), multiple works adopt
    pre-trained LMs for MDS and achieve promising improvements. Applying pre-trained
    LMs such as BERT (Devlin et al., [2019](#bib.bib35)), GPT-2 (Radford et al., [2019](#bib.bib130)),
    GPT-3 (Brown et al., [2020](#bib.bib18)), XLNet (Yang et al., [2019](#bib.bib165)),
    ALBERT (Lan et al., [2020](#bib.bib82)), or T5 (Raffel et al., [2020](#bib.bib131)),
    and fine-tuning them on a variety of downstream tasks allows the model to achieve
    faster convergence speed and can improve model performance. MDS requires the model
    to have a strong ability to process long sequences. It is promising to explore
    powerful LMs specifically targeting long sequence input characteristics and avoiding
    quadratic memory growth for self-attention mechanism, such as Longformer (Beltagy
    et al., [2020](#bib.bib14)), REFORMER (Kitaev et al., [2020](#bib.bib77)), or
    Big Bird (Zaheer et al., [2020](#bib.bib170)) with pre-trained models. Also, tailor-designed
    pre-trained LMs for summarization have not been well-explored, e.g., using gap
    sentences generation is more suitable than using masked language model (Zhang
    et al., [2020d](#bib.bib173)). Most MDS methods focus on combining pre-trained
    LMs in encoder and, as for capturing cross-document relations, applying them in
    decoder is also a worthwhile direction for research (Pasunuru et al., [2021b](#bib.bib123)).
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: 7.6\. Creating Explainable Deep Learning Model for MDS
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning models can be regarded as black boxes with high non-linearity;
    it is extremely challenging to understand the detailed transformation inside them.
    However, an explainable model can reveal how it generates candidate summaries
    – to distinguish whether the model has learned the distribution of generating
    condensed and coherent summaries from multiple documents without bias – and is
    thus crucial for model building. Recently, a large amount of researches into explainable
    models (Zhang et al., [2018a](#bib.bib174); Rudin, [2019](#bib.bib133)) have proposed
    easing the non-interpretable concern of deep neural networks, within which model
    attention plays an especially important role in model interpretation (Zhou et al.,
    [2016](#bib.bib185); Serrano and Smith, [2019](#bib.bib137)). While explainable
    methods have been intensively researched in NLP (Kumar and Talukdar, [2020](#bib.bib81);
    Jain et al., [2020](#bib.bib66)), studies into explainable MDS models are relatively
    scarce and would benefit from future development.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: 7.7\. Adversarial Attack and Defense for MDS
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial examples are strategically modified samples that aim to fool deep
    neural networks based models. An adversarial example is created via the worst-case
    perturbation of the input to which a robust DNN model would still assign correct
    labels, while a vulnerable DNN model would have high confidence in the wrong prediction.
    The idea of using adversarial examples to examine the robustness of a DNN model
    originated from research in Computer Vision (Szegedy et al., [2014](#bib.bib145))
    and was introduced in NLP by Jia et al. (Jia and Liang, [2017](#bib.bib71)). An
    essential purpose for generating adversarial examples for neural networks is to
    utilize these adversarial examples to enhance the model’s robustness (Goodfellow
    et al., [2015](#bib.bib53)). Therefore, research on adversarial examples not only
    helps identify and apply a robust model but also helps to build robust models
    for different tasks. Following the pioneering work proposed by Jia et al. (Jia
    and Liang, [2017](#bib.bib71)), many attack methods have been proposed to address
    this problem in NLP applications (Zhang et al., [2020b](#bib.bib177)) with limited
    research for MDS (Cheng et al., [2020](#bib.bib28)). It is worth filling this
    gap by exploring existing and developing new, adversarial attacks on the state-of-the-art
    DNN-based MDS models.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: 7.8\. Multi-modality for MDS
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Existing multi-modal summarization is based on non-deep learning techniques
    (Li et al., [2017c](#bib.bib87); Jangra et al., [2021](#bib.bib69), [2020a](#bib.bib67),
    [2020b](#bib.bib68)), leaving a huge opportunity to exploit deep learning techniques
    for this task. Multi-modal learning has led to successes in many deep learning
    tasks, such as Visual Language Navigation (Wang et al., [2020b](#bib.bib157))
    and Visual Question Answering (Antol et al., [2015](#bib.bib7)). Combining MDS
    with multi-modality has a range of applications:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'text + image: generating summaries with pictures and texts for documents with
    pictures. This kind of multi-modal summary can improve the satisfaction of users
    (Zhu et al., [2018](#bib.bib188));'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'text + video: based on the video and its subtitles, generating a concise text
    summary that describes the main context of video (Palaskar et al., [2019](#bib.bib119)).
    Movie synopsis is one application;'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'text + audio: generating short summaries of audio files that people could quickly
    preview without actually listening to the entire audio recording (Erol et al.,
    [2003](#bib.bib42)).'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deep learning is well-suited for multi-modal tasks (Guo et al., [2019](#bib.bib57)),
    as it is able to effectively capture highly nonlinear relationships between images,
    text or video data. Existing MDS models target at dealing with textual data only.
    Involving richer modalities based on textual data requires models to embrace larger
    capacity to handle these multi-modal data. The big models such as UNITER (Chen
    et al., [2020](#bib.bib27)), VisualBERT (Li et al., [2019](#bib.bib88)) deserve
    more attention in multi-modality MDS tasks. However, at present, there is little
    multi-modal research work based on MDS; this is a promising, but largely under-explored,
    area where more studies are expected.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusion
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we have presented the first comprehensive review of the most
    notable works to date on deep learning based multi-document summarization (MDS).
    We propose a taxonomy for organizing and clustering existing publications and
    devise the network design strategies based on the state-of-the-art methods. We
    also provide an overview of the existing multi-document objective functions, evaluation
    metrics and datasets, and discuss some of the most pressing open problems and
    promising future extensions in MDS research. We hope this survey provides readers
    with a comprehensive understanding of the key aspects of the MDS tasks, clarifies
    the most notable advances, and sheds light on future studies.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Afantenos et al. (2005) Stergos Afantenos, Vangelis Karkaletsis, and Panagiotis
    Stamatopoulos. 2005. Summarization from Medical Documents: A Survey. *Artificial
    Intelligence in Medicine* 33, 2, 157–177.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alghamdi et al. (2020) Mahfouth Alghamdi, Christoph Treude, and Markus Wagner.
    2020. Human-Like Summaries from Heterogeneous and Time-Windowed Software Development
    Artefacts. In *Proceedings of the 6th International Conference of Parallel Problem
    Solving from Nature (PPSN 2020)*. Leiden, The Netherlands, 329–342.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amplayo and Lapata (2021) Reinald Kim Amplayo and Mirella Lapata. 2021. Informative
    and Controllable Opinion Summarization. In *Proceedings of the 16th Conference
    of the European Chapter of the Association for Computational Linguistics: Main
    Volume (EACL 2021)*. Online, 2662–2672.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Angelidis and Lapata (2018) Stefanos Angelidis and Mirella Lapata. 2018. Summarizing
    Opinions: Aspect Extraction Meets Sentiment Prediction and They are Both Weakly
    Supervised. In *Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing (EMNLP 2018)*. Brussels, Belgium, 3675–3686.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Antognini and Faltings (2019) Diego Antognini and Boi Faltings. 2019. Learning
    to Create Sentence Semantic Relation Graphs for Multi-Document Summarization.
    In *Proceedings of the 2nd Workshop on New Frontiers in Summarization (EMNLP 2019)*.
    Hongkong, China.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual
    Question Answering. In *Proceedings of the 2015 IEEE International Conference
    on Computer Vision (ICCV 2015)*. Santiago, Chile, 2425–2433.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arora and Ravindran (2008) Rachit Arora and Balaraman Ravindran. 2008. Latent
    Dirichlet Allocation and Singular Value Decomposition based Multi-document Summarization.
    In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining
    (ICDM 2008)*. Pisa, Italy, 713–718.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.
    Neural Machine Translation by Jointly Learning to Align and Translate. In *Proceedings
    of the 3rd International Conference on Learning Representations (ICLR 2015)*.
    San Diego, CA, United States.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
    An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.
    In *Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures
    for Machine Translation and/or Summarization (ACL 2005)*. 65–72.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baralis et al. (2012) Elena Baralis, Luca Cagliero, Saima Jabeen, and Alessandro
    Fiori. 2012. Multi-document Summarization Exploiting Frequent Itemsets. In *Proceedings
    of the 27th Annual ACM Symposium on Applied Computing (SAC2012)*. Riva, Italy,
    782–786.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baxendale (1958) Phyllis B Baxendale. 1958. Machine-made Index for Technical
    Literature - An Experiment. *IBM Journal of Research and Development* 2, 4, 354–361.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bellot et al. (2016) Patrice Bellot, Véronique Moriceau, Josiane Mothe, Eric
    SanJuan, and Xavier Tannier. 2016. INEX Tweet Contextualization task: Evaluation,
    results and lesson learned. *Information Processing and Management.* 52, 5 (2016),
    801–819.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
    Longformer: The Long-document Transformer. *arXiv preprint arXiv:2004.05150*.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bing et al. (2015) Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo, and Rebecca J.
    Passonneau. 2015. Abstractive Multi-Document Summarization via Phrase Selection
    and Merging. In *Proceedings of the 53rd Annual Meeting of the Association for
    Computational Linguistics and the 7th International Joint Conference on Natural
    Language Processing of the Asian Federation of Natural Language Processing (ACL
    2015)*. Beijing, China, 1587–1597.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bražinskas et al. (2019) Arthur Bražinskas, Mirella Lapata, and Ivan Titov.
    2019. Unsupervised Multi-Document Opinion Summarization as Copycat-Review Generation.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL 2020)*. Online.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brazinskas et al. (2020) Arthur Brazinskas, Mirella Lapata, and Ivan Titov.
    2020. Few-Shot Learning for Opinion Summarization. In *Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)*.
    Online, 4119–4135.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020. Language Models Are Few-shot Learners. In *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020 (NeurIPS 2020)*. Online.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2017) Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. 2017. Improving
    Multi-document Summarization via Text Classification. In *Proceedings of the 31st
    AAAI Conference on Artificial Intelligence (AAAI 2017)*. San Francisco, United
    States, 3053–3059.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2015a) Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and Ming Zhou.
    2015a. Ranking with Recursive Neural Networks and its Application to Multi-document
    Summarization. In *Proceedings of the 29th AAAI Conference on Artificial Intelligence
    (AAAI 2015)*. Austin, United States, 2153–2159.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2015b) Ziqiang Cao, Furu Wei, Sujian Li, Wenjie Li, Ming Zhou, and
    Houfeng Wang. 2015b. Learning Summary Prior Representation for Extractive Summarization.
    In *Proceedings of the 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing
    (ACL 2015)*. Beijing, China, 829–833.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carbonell and Goldstein (1998) Jaime G. Carbonell and Jade Goldstein. 1998.
    The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing
    Summaries. In *Proceedings of the 21st Annual International Conference on Research
    and Development in Information Retrieval (SIGIR 1998)*. Melbourne, Australia,
    335–336.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carenini et al. (2007) Giuseppe Carenini, Raymond T. Ng, and Xiaodong Zhou.
    2007. Summarizing Email Conversations with Clue Words. In *Proceedings of the
    16th International Conference on World Wide Web (WWW 2007)*. Banff, Canada, 91–100.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carletta et al. (2005) Jean Carletta, Simone Ashby, Sebastien Bourban, Mike
    Flynn, Maël Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel
    Kraaij, Melissa Kronenthal, Guillaume Lathoud, Mike Lincoln, Agnes Lisowska, Iain
    McCowan, Wilfried Post, Dennis Reidsma, and Pierre Wellner. 2005. The AMI Meeting
    Corpus: A Pre-announcement. In *Machine Learning for Multimodal Interaction, Second
    International Workshop (MLMI 2005)*. Edinburgh, UK, 28–39.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Yang (2020) Jiaao Chen and Diyi Yang. 2020. Multi-View Sequence-to-Sequence
    Models with Conversational Structure for Abstractive Dialogue Summarization. In
    *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2020)*. Online, 4106–4118.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. 2021.
    DialogSumm: A Real-Life Scenario Dialogue Summarization Dataset. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics (ACL
    2021)*. Online.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal
    Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal Image-text
    Representation Learning. In *European conference on computer vision*. Springer,
    104–120.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2020) Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and
    Cho-Jui Hsieh. 2020. Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence
    Models with Adversarial Examples. In *Proceedings of the 34th AAAI Conference
    on Artificial Intelligence (AAAI 2020)*. New York, NY, United States, 3601–3608.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2019) Sangwoo Cho, Logan Lebanoff, Hassan Foroosh, and Fei Liu.
    2019. Improving the Similarity Measure of Determinantal Point Processes for Extractive
    Multi-Document Summarization. In *Proceedings of the 57th Conference of the Association
    for Computational Linguistics (ACL 2019)*. Florence, Italy, 1027–1038.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Christensen et al. (2013) Janara Christensen, Stephen Soderland, Oren Etzioni,
    et al. 2013. Towards Coherent Multi-document Summarization. In *Proceedings of
    the 2013 conference of the North American chapter of the association for computational
    linguistics: Human language technologies*. 1163–1173.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chu and Liu (2019) Eric Chu and Peter J. Liu. 2019. MeanSum: A Neural Model
    for Unsupervised Multi-Document Abstractive Summarization. In *Proceedings of
    the 36th International Conference on Machine Learning (ICML 2019)*. Long Beach,
    United States, 1223–1232.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2014) Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua
    Bengio. 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
    Modeling. *arXiv preprint arXiv:1412.3555*.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coavoux et al. (2019) Maximin Coavoux, Hady Elsahar, and Matthias Gallé. 2019.
    Unsupervised Aspect-Based Multi-Document Abstractive Summarization. In *Proceedings
    of the 2nd Workshop on New Frontiers in Summarization*. Hong Kong, China, 42–47.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collobert et al. (2011) Ronan Collobert, Jason Weston, Léon Bottou, Michael
    Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing
    (Almost) from Scratch. *Journal of Machine Learning Research* 12, ARTICLE, 2493–2537.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies
    (NAACL-HLT 2019)*. Minneapolis, United States, 4171–4186.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Devlin et al. (2014) Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar,
    Richard Schwartz, and John Makhoul. 2014. Fast and Robust Neural Network Joint
    Models for Statistical Machine Translation. In *Proceedings of the 52nd Annual
    Meeting of the Association for Computational Linguistics (ACL 2014)*. Baltimore,
    United States, 1370–1380.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,
    Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified Language Model
    Pre-training for Natural Language Understanding and Generation. In *Advances in
    Neural Information Processing Systems 32: Annual Conference on Neural Information
    Processing Systems 2019 (NeurIPS 2019)*. 13042–13054.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dos Santos and Gatti (2014) Cicero Dos Santos and Maira Gatti. 2014. Deep Convolutional
    Neural Networks for Sentiment Analysis of Short Texts. In *Proceedings of the
    International Conference on Computational Linguistics (COLING 2014)*. Dublin,
    Ireland, 69–78.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'El-Kassas et al. (2021) Wafaa S. El-Kassas, Cherif R. Salama, Ahmed A. Rafea,
    and Hoda K. Mohamed. 2021. Automatic Text Summarization: A Comprehensive Survey.
    *Expert Systems with Applications* 165 (2021), 113679.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enarvi et al. (2020) Seppo Enarvi, Marilisa Amoia, Miguel Del-Agua Teba, Brian
    Delaney, Frank Diehl, Stefan Hahn, Kristina Harris, Liam McGrath, Yue Pan, Joel
    Pinto, et al. 2020. Generating Medical Reports from Patient-doctor Conversations
    Using Sequence-to-sequence Models. In *Proceedings of the first workshop on natural
    language processing for medical conversations*.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Erkan and Radev (2004) Günes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based
    Lexical Centrality as Salience in Text Summarization. *Journal of Artificial Intelligence
    Research* 22, 457–479.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erol et al. (2003) Berna Erol, Dar-Shyang Lee, and Jonathan J. Hull. 2003. Multimodal
    Summarization of Meeting Recordings. In *Proceedings of the 2003 IEEE International
    Conference on Multimedia and Expo (ICME 2003)*. Baltimore, United States, 25–28.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fabbri et al. (2021) Alexander R Fabbri, Wojciech Kryściński, Bryan McCann,
    Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating
    Summarization Evaluation. *Transactions of the Association for Computational Linguistics*
    9, 391–409.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fabbri et al. (2019) Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and
    Dragomir R. Radev. 2019. Multi-News: A Large-Scale Multi-Document Summarization
    Dataset and Abstractive Hierarchical Model. In *Proceedings of the 57th Conference
    of the Association for Computational Linguistics (ACL 2019)*. Florence, Italy,
    1074–1084.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2021) Xiachong Feng, Xiaocheng Feng, Bing Qin, Xinwei Geng, and
    Ting Liu. 2021. Dialogue Discourse-Aware Graph Convolutional Networks for Abstractive
    Meeting Summarization. In *Proceedings of the 30th International Joint Conference
    on Artificial Intelligence (IJCAI 2021)*.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferreira et al. (2014) Rafael Ferreira, Luciano de Souza Cabral, Frederico Freitas,
    Rafael Dueire Lins, Gabriel de França Silva, Steven J Simske, and Luciano Favaro.
    2014. A Multi-document Summarization System based on Statistics and Linguistic
    Treatment. *Expert Systems with Applications* 41, 13, 5780–5787.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ganesan et al. (2010) Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. 2010.
    Opinosis: A Graph Based Approach to Abstractive Summarization of Highly Redundant
    Opinions. In *Proceedings of the 23rd International Conference on Computational
    Linguistics (COLING 2010)*. Beijing, China, 340–348.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2019) Yanjun Gao, Chen Sun, and Rebecca J Passonneau. 2019. Automated
    Pyramid Summarization Evaluation. In *Proceedings of the 23rd Conference on Computational
    Natural Language Learning (CoNLL 2019)*. Hong Kong, China, 404–418.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2020) Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: Towards
    New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL 2020)*. Online, 1347–1354.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gerani et al. (2014) Shima Gerani, Yashar Mehdad, Giuseppe Carenini, Raymond
    Ng, and Bita Nejat. 2014. Abstractive Summarization of Product Reviews Using Discourse
    Structure. In *Proceedings of the 2014 Conference on Empirical Methods in Natural
    Language Processing (EMNLP 2014)*. Doha, Qatar, 1602–1613.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghalandari et al. (2020) Demian Gholipour Ghalandari, Chris Hokamp, Nghia The
    Pham, John Glover, and Georgiana Ifrim. 2020. A Large-Scale Multi-Document Summarization
    Dataset from the Wikipedia Current Events Portal. In *Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics (ACL 2020)*. Online,
    1302–1308.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goldstein et al. (2000) Jade Goldstein, Vibhu O Mittal, Jaime G Carbonell,
    and Mark Kantrowitz. 2000. Multi-document Summarization by Sentence Extraction.
    In *Proceedings of the Conference of the North American Chapter of the Association
    for Computational Linguistics: Applied Natural Language Processing Conference
    (NAACL-ANLP 2000)*. Seattle, United States, 91–98.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2015) Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
    2015. Explaining and Harnessing Adversarial Examples. In *Proceedings of the 3rd
    International Conference on Learning Representations (ICLR 2015)*. San Diego,
    CA, United States,.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grail et al. (2021) Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing
    BERT-based Transformer Architectures for Long Document Summarization. In *Proceedings
    of the 16th Conference of the European Chapter of the Association for Computational
    Linguistics: Main Volume (EACL2021)*. Online, 1792–1810.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grusky et al. (2018) Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom:
    A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies. In *Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (NAACL-HLT 2018)*. New Orleans, USA,
    708–719.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2016) Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K. Li. 2016.
    Incorporating Copying Mechanism in Sequence-to-Sequence Learning. In *Proceedings
    of the 54th Annual Meeting of the Association for Computational Linguistics (ACL
    2016)*. Berlin, Germany.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2019) Wenzhong Guo, Jianwen Wang, and Shiping Wang. 2019. Deep
    Multimodal Representation Learning: A Survey. *IEEE Access* 7 (2019), 63373–63394.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta and Lehal (2010) Vishal Gupta and Gurpreet Singh Lehal. 2010. A Survey
    of Text Summarization Extractive Techniques. *Journal of Emerging Technologies
    in Web Intelligence* 2, 3, 258–268.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haghighi and Vanderwende (2009) Aria Haghighi and Lucy Vanderwende. 2009. Exploring
    Content Models for Multi-document Summarization. In *Proceedings of the 2009 Annual
    Conference of the North American Chapter of the Association for Computational
    Linguistics (HLT-NAACL 2009)*. Boulder, United States, 362–370.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haque et al. (2013) Majharul Haque, Suraiya Pervin, Zerina Begum, et al. 2013.
    Literature Review of Automatic Multiple Documents Text Summarization. *International
    Journal of Innovation and Applied Studies* 3, 1, 121–129.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hirao et al. (2018) Tsutomu Hirao, Hidetaka Kamigaito, and Masaaki Nagata. 2018.
    Automatic Pyramid Evaluation Exploiting Edu-based Extractive Reference Summaries.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2018)*. Brussels, Belgium, 4177–4186.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long Short-term Memory. *Neural Computation* 9, 8, 1735–1780.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hornik et al. (1989) Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989.
    Multilayer feedforward networks are universal approximators. *Neural networks*
    2, 5 (1989), 359–366.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2017) Ya-Han Hu, Yen-Liang Chen, and Hui-Ling Chou. 2017. Opinion
    Mining from Online Hotel Reviews–A Text Summarization Approach. *Information Processing
    & Management* 53, 2, 436–449.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2015) Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF
    Models for Sequence Tagging. *arXiv preprint arXiv:1508.01991*.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. (2020) Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and Byron C.
    Wallace. 2020. Learning to Faithfully Rationalize by Construction. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics (ACL
    2020)*. Online, 4459–4473.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jangra et al. (2020a) Anubhav Jangra, Adam Jatowt, Mohammad Hasanuzzaman, and
    Sriparna Saha. 2020a. Text-image-video Summary Generation Using Joint Integer
    Linear Programming. *Advances in Information Retrieval* 12036 (2020), 190.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jangra et al. (2020b) Anubhav Jangra, Sriparna Saha, Adam Jatowt, and Mohammad
    Hasanuzzaman. 2020b. Multi-modal Summary Generation Using Multi-objective Optimization.
    In *Proceedings of the 43rd International ACM SIGIR Conference on Research and
    Development in Information Retrieval (SIGIR 2020)*. 1745–1748.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jangra et al. (2021) Anubhav Jangra, Sriparna Saha, Adam Jatowt, and Mohammed
    Hasanuzzaman. 2021. Multi-Modal Supplementary-Complementary Summarization using
    Multi-Objective Optimization. In *Proceedings of the 44th International ACM SIGIR
    Conference on Research and Development in Information Retrieval (SIGIR2021)*.
    Online, 818–828.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jelinek et al. (1977) Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K
    Baker. 1977. Perplexity - A Measure of the Difficulty of Speech Recognition Tasks.
    *The Journal of the Acoustical Society of America* 62, S1, S63–S63.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia and Liang (2017) Robin Jia and Percy Liang. 2017. Adversarial Examples for
    Evaluating Reading Comprehension Systems. In *Proceedings of the 2017 Conference
    on Empirical Methods in Natural Language Processing (EMNLP 2017)*. Copenhagen,
    Denmark, 2021–2031.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2020) Hanqi Jin, Tianming Wang, and Xiaojun Wan. 2020. Multi-Granularity
    Interaction Network for Extractive and Abstractive Multi-Document Summarization.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL 2020)*. Online, 6244–6254.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2020) Anirudh Joshi, Namit Katariya, Xavier Amatriain, and Anitha
    Kannan. 2020. Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting
    Local Structures. In *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing: Findings, (EMNLP2020)*. Online, 3755–3763.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kanapala et al. (2019) Ambedkar Kanapala, Sukomal Pal, and Rajendra Pamula.
    2019. Text Summarization from Legal Documents: A Survey. *Artificial Intelligence
    Review* 51, 3, 371–402.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim (2014) Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2014)*. Doha, Qatar, 1746–1751.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised
    Classification with Graph Convolutional Networks. In *Proceedings of the 5th International
    Conference on Learning Representations (ICLR 2017)*. Toulon, France.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020.
    Reformer: The Efficient Transformer. In *Proceedings of the 8th International
    Conference on Learning Representations (ICLR 2020)*. Addis Ababa, Ethiopia.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koay et al. (2020) Jia Jin Koay, Alexander Roustai, Xiaojin Dai, Dillon Burns,
    Alec Kerrigan, and Fei Liu. 2020. How Domain Terminology Affects Meeting Summarization
    Performance. In *Proceedings of the 28th International Conference on Computational
    Linguistics (COLING 2020)*. Online, 5689–5695.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet Classification with Deep Convolutional Neural Networks. In *Proceedings
    of the 26th Annual Conference on Neural Information Processing Systems (NIPS 2012)*.
    Lake Tahoe, United States, 1106–1114.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kulkarni et al. (2020) Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and
    Eugene Ie. 2020. AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document
    Summarization. *CoRR* abs/2010.12694 (2020).'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumar and Talukdar (2020) Sawan Kumar and Partha P. Talukdar. 2020. NILE :
    Natural Language Inference with Faithful Natural Language Explanations. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics (ACL
    2020)*. Online, 8730–8742.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised
    Learning of Language Representations. In *Proceedings of the 8th International
    Conference on Learning Representations (ICLR 2020)*. Addis Ababa, Ethiopia.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lebanoff et al. (2019) Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon
    Kim, Seokhwan Kim, Walter Chang, and Fei Liu. 2019. Scoring Sentence Singletons
    and Pairs for Abstractive Summarization. In *Proceedings of the 57th Conference
    of the Association for Computational Linguistics (ACL 2019)*. Florence, Italy,
    2175–2189.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lebanoff et al. (2018) Logan Lebanoff, Kaiqiang Song, and Fei Liu. 2018. Adapting
    the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2018)*. Brussels, Belgium, 4131–4141.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based Learning Applied to Document Recognition. *Proc. IEEE* 86,
    11, 2278–2324.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020b) Haoran Li, Peng Yuan, Song Xu, Youzheng Wu, Xiaodong He, and
    Bowen Zhou. 2020b. Aspect-Aware Multimodal Summarization for Chinese E-Commerce
    Products. In *The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI2020)*.
    New York, USA, 8188–8195.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017c) Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, and Chengqing
    Zong. 2017c. Multi-modal Summarization for Asynchronous Collection of Text, Image,
    Audio and Video. In *Proceedings of the 2017 Conference on Empirical Methods in
    Natural Language Processing (EMNLP2017)*. Copenhagen, Denmark, 1092–1102.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and
    Kai-Wei Chang. 2019. Visualbert: A Simple and Performant Baseline for Vision and
    Language. *arXiv preprint arXiv:1908.03557* (2019).'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017a) Piji Li, Lidong Bing, and Wai Lam. 2017a. Reader-Aware Multi-Document
    Summarization: An Enhanced Model and The First Dataset. In *Proceedings of the
    Workshop on New Frontiers in Summarization (NFiS@EMNLP 2017)*. Copenhagen, Denmark,
    91–99.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017b) Piji Li, Wai Lam, Lidong Bing, Weiwei Guo, and Hang Li. 2017b.
    Cascaded Attention based Unsupervised Information Distillation for Compressive
    Summarization. In *Proceedings of the 2017 Conference on Empirical Methods in
    Natural Language Processing (EMNLP 2017)*. Copenhagen, Denmark, 2081–2090.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020a) Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, and
    Junping Du. 2020a. Leveraging Graph to Improve Abstractive Multi-Document Summarization.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL 2020)*. Online, 6232–6243.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Chin-Yew Lin. 2004. Rouge: A Package for Automatic Evaluation of
    Summaries. In *Text Summarization Branches Out*. 74–81.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019b) Chunyi Liu, Peng Wang, Jiang Xu, Zang Li, and Jieping Ye.
    2019b. Automatic Dialogue Summary Generation for Customer Service. In *Proceedings
    of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining, (KDD 2019)*. Anchorage, USA, 1957–1965.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan
    Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating Wikipedia by Summarizing
    Long Sequences. In *Proceedings of the 6th International Conference on Learning
    Representations (ICLR 2018)*. Vancouver,Canada.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Lapata (2019) Yang Liu and Mirella Lapata. 2019. Hierarchical Transformers
    for Multi-Document Summarization. In *Proceedings of the 57th Conference of the
    Association for Computational Linguistics (ACL 2019)*. Florence, Italy, 5070–5081.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019a.
    Roberta: A Robustly Optimized Bert Pretraining Approach. *arXiv preprint arXiv:1907.11692*.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Louis and Nenkova (2013) Annie Louis and Ani Nenkova. 2013. Automatically Assessing
    Machine Summary Content Without A Gold Standard. *Computational Linguistics* 39,
    2, 267–300.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2020) Yao Lu, Yue Dong, and Laurent Charlin. 2020. Multi-XScience:
    A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2020)*. Online, 8068–8074.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo and Litman (2015) Wencan Luo and Diane Litman. 2015. Summarizing Student
    Responses to Reflection Prompts. In *Proceedings of the 2015 Conference on Empirical
    Methods in Natural Language Processing (EMNLP 2015)*. Lisbon, Portugal, 1955–1960.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2016) Wencan Luo, Fei Liu, Zitao Liu, and Diane J. Litman. 2016.
    Automatic Summarization of Student Course Feedback. In *Proceedings of the 2016
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (NAACL-HLT 2016)*. San Diego California,
    United States, 80–85.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2021) Congbo Ma, Wei Emma Zhang, Hu Wang, Shubham Gupta, and Mingyu
    Guo. 2021. Incorporating Linguistic Knowledge for Abstractive Multi-document Summarization.
    *arXiv preprint arXiv:2109.11199* (2021).
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mani and Bloedorn (1997) Inderjeet Mani and Eric Bloedorn. 1997. Multi-Document
    Summarization by Graph Search and Matching. In *Proceedings of the Fourteenth
    National Conference on Artificial Intelligence and Ninth Innovative Applications
    of Artificial Intelligence Conference (AAAI 1997)*. Providence, United States,
    622–628.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao et al. (2020) Yuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, and Jiawei Han.
    2020. Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement
    Learning. In *Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing (EMNLP 2020)*. Online, 1737–1751.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McBurney and McMillan (2014) Paul W McBurney and Collin McMillan. 2014. Automatic
    Documentation Generation via Source Code Summarization of Method Context. In *Proceedings
    of the 22nd International Conference on Program Comprehension (ICPC 2014)*. Hyderabad,
    India, 279–290.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miao and Blunsom (2016) Yishu Miao and Phil Blunsom. 2016. Language as a Latent
    Variable: Discrete Generative Models for Sentence Compression. In *Proceedings
    of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP
    2016)*. Austin, United States, 319–328.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mihalcea and Tarau (2005) Rada Mihalcea and Paul Tarau. 2005. A Language Independent
    Algorithm for Single and Multiple Document Summarization. In *Proceedings of the
    2nd International Joint Conference, Companion Volume to the Proceedings of Conference
    including Posters/Demos and Tutorial Abstracts (IJCNLP 2005)*. Jeju Island, Republic
    of Korea.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. Distributed Representations of Words and Phrases and Their
    Compositionality. In *Proceedings of the 27th Annual Conference on Neural Information
    Processing Systems (NIPS 2013)*. Lake Tahoe, United States, 3111–3119.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2016) Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
    Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016.
    Asynchronous Methods for Deep Reinforcement Learning. In *Proceedings of the 33nd
    International Conference on Machine Learning (ICML2016)*. New York City, United
    States, 1928–1937.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Molenaar et al. (2020) Sabine Molenaar, Lientje Maas, Verónica Burriel, Fabiano
    Dalpiaz, and Sjaak Brinkkemper. 2020. Medical Dialogue Summarization for Automated
    Reporting in Healthcare. In *Proceedings of the International Conference on Advanced
    Information Systems Engineering (CAiSE Workshops 2020)*. Grenoble, France, 76–88.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nallapati et al. (2017) Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
    SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization
    of Documents. In *Proceedings of the Thirty-First AAAI Conference on Artificial
    Intelligence (AAAI 2017)*. San Francisco, United States, 3075–3081.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018.
    Ranking Sentences for Extractive Summarization with Reinforcement Learning. In
    *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018)*.
    New Orleans, United States, 1747–1759.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nayeem et al. (2018) Mir Tafseer Nayeem, Tanvir Ahmed Fuad, and Yllias Chali.
    2018. Abstractive Unsupervised Multi-Document Summarization using Paraphrastic
    Sentence Fusion. In *Proceedings of the 27th International Conference on Computational
    Linguistics (COLING 2018)*. Santa Fe, United States, 1191–1204.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nema et al. (2017) Preksha Nema, Mitesh M. Khapra, Anirban Laha, and Balaraman
    Ravindran. 2017. Diversity driven attention model for query-based abstractive
    summarization. In *Proceedings of the 55th Annual Meeting of the Association for
    Computational Linguistics, (ACL 2017)*. Vancouver, Canada, 1063–1072.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nenkova and McKeown (2012) Ani Nenkova and Kathleen R. McKeown. 2012. A Survey
    of Text Summarization Techniques. In *Mining Text Data*.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nenkova et al. (2007) Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown.
    2007. The Pyramid Method: Incorporating Human Content Selection Variation in Summarization
    Evaluation. *ACM Transactions on Speech and Language Processing* 4, 2, 4–es.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nenkova and Passonneau (2004) Ani Nenkova and Rebecca J Passonneau. 2004. Evaluating
    Content Selection in Summarization: The Pyramid Method. In *Proceedings of the
    Human Language Technology Conference of the North American Chapter of the Association
    for Computational Linguistics (HLT-NAACL 2004)*. Boston, United States, 145–152.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ng and Abrecht (2015) Jun-Ping Ng and Viktoria Abrecht. 2015. Better Summarization
    Evaluation with Word Embeddings for ROUGE. In *Proceedings of the 2015 Conference
    on Empirical Methods in Natural Language Processing (EMNLP 2015)*. Lisbon, Portugal,
    1925–1930.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oussous et al. (2018) Ahmed Oussous, Fatima-Zahra Benjelloun, Ayoub Ait Lahcen,
    and Samir Belfkih. 2018. Big Data Technologies: A Survey. *Journal of King Saud
    University-Computer and Information Sciences* 30, 4, 431–448.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Palaskar et al. (2019) Shruti Palaskar, Jindrich Libovický, Spandana Gella,
    and Florian Metze. 2019. Multimodal Abstractive Summarization for How2 Videos.
    In *Proceedings of the 57th Conference of the Association for Computational Linguistics
    (ACL 2019)*. Florence, Italy, 6587–6596.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In
    *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics
    (ACL2002)*. Philadelphia, United States, 311–318.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passonneau et al. (2013) Rebecca J Passonneau, Emily Chen, Weiwei Guo, and Dolores
    Perin. 2013. Automated Pyramid Scoring of Summaries Using Distributional Semantics.
    In *Proceedings of the 51st Annual Meeting of the Association for Computational
    Linguistics (ACL 2013)*. Sofia, Bulgaria, 143–147.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pasunuru et al. (2021a) Ramakanth Pasunuru, Asli Celikyilmaz, Michel Galley,
    Chenyan Xiong, Yizhe Zhang, Mohit Bansal, and Jianfeng Gao. 2021a. Data Augmentation
    for Abstractive Query-Focused Multi-Document Summarization. In *Proceedings of
    the AAAI Conference on Artificial Intelligence (AAAI 2021)*. Online, 13666–13674.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pasunuru et al. (2021b) Ramakanth Pasunuru, Mengwen Liu, Mohit Bansal, Sujith
    Ravi, and Markus Dreyer. 2021b. Efficiently Summarizing Text and Graph Encodings
    of Multi-Document Clusters. In *Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies (NAACL-HLT 2021)*. Online, 4768–4779.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paulus et al. (2018) Romain Paulus, Caiming Xiong, and Richard Socher. 2018.
    A Deep Reinforced Model for Abstractive Summarization. In *Proceedings of the
    6th International Conference on Learning Representations (ICLR 2018)*. Vancouver,
    Canada.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized
    Word Representations. In *Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies
    (NAACL-HLT 2018)*. New Orleans, United States, 2227–2237.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peyrard (2019) Maxime Peyrard. 2019. A Simple Theoretical Model of Importance
    for Summarization. In *Proceedings of the 57th Conference of the Association for
    Computational Linguistics (ACL 2019)*. Association for Computational Linguistics,
    Florence, Italy, 1059–1073.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radev (2000) Dragomir R. Radev. 2000. A Common Theory of Information Fusion
    from Multiple Text Sources Step One: Cross-Document Structure. In *Proceedings
    of the Workshop of the 1st Annual Meeting of the Special Interest Group on Discourse
    and Dialogue (SIGDIAL 2000)*. Hong Kong, China, 74–83.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radev et al. (2004) Dragomir R Radev, Hongyan Jing, Małgorzata Styś, and Daniel
    Tam. 2004. Centroid-based Summarization of Multiple Documents. *Information Processing
    & Management* 40, 6, 919–938.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radev et al. (2013) Dragomir R. Radev, Pradeep Muthukrishnan, Vahed Qazvinian,
    and Amjad Abu-Jbara. 2013. The ACL Anthology Network Corpus. *Lang. Resour. Evaluation*
    47, 4, 919–944.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.
    *OpenAI Blog* 1, 8, 9.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *Journal
    of Machine Learning Research* 21, 140:1–140:67.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rodeghero et al. (2014) Paige Rodeghero, Collin McMillan, Paul W McBurney, Nigel
    Bosch, and Sidney D’Mello. 2014. Improving Automated Source Code Summarization
    via An Eye-tracking Study of Programmers. In *Proceedings of the 36th International
    Conference on Software Engineering (ICSE 2014)*. Hyderabad, India, 390–401.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rudin (2019) Cynthia Rudin. 2019. Stop Explaining Black Box Machine Learning
    Models for High Stakes Decisions and Use Interpretable Models Instead. *Nature
    Machine Intelligence* 1, 5, 206–215.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning Representations by Back-propagating Errors. *Nature* 323, 6088,
    533–536.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sabour et al. (2017) Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017.
    Dynamic Routing Between Capsules. In *Proceedings of the 2017 Annual Conference
    on Neural Information Processing Systems(NIPS 2017)*. Long Beach, United States,
    3856–3866.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. 2017.
    Get To The Point: Summarization with Pointer-Generator Networks. In *Proceedings
    of the 55th Annual Meeting of the Association for Computational Linguistics (ACL
    2017)*. Vancouver, Canada, 1073–1083.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serrano and Smith (2019) Sofia Serrano and Noah A. Smith. 2019. Is Attention
    Interpretable?. In *Proceedings of the 57th Conference of the Association for
    Computational Linguistics (ACL 2019)*. Florence, Italy, 2931–2951.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ShafieiBavani et al. (2018) Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond
    Wong, and Fang Chen. 2018. A Graph-theoretic Summary Evaluation for Rouge. In
    *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2018)*. 762–767.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shah and Jivani (2016) Chintan Shah and Anjali Jivani. 2016. Literature Study
    on Multi-document Text Summarization Techniques. In *Proceedings of the International
    Conference on Smart Trends for Information Technology and Computer Communications*.
    442–451.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shapira et al. (2019) Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth
    Pasunuru, Mohit Bansal, Yael Amsterdamer, and Ido Dagan. 2019. Crowdsourcing Lightweight
    Pyramids for Manual Summary Evaluation. In *Proceedings of the Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies (NAACL-HLT 2019)*. Minneapolis, United States, 682–687.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shirwandkar and Kulkarni (2018) Nikhil S Shirwandkar and Samidha Kulkarni. 2018.
    Extractive Text Summarization Using Deep Learning. In *Proceedings of the 2018
    Fourth International Conference on Computing Communication Control and Automation
    (ICCUBEA 2018)*. Pune, India.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. (2018) Abhishek Kumar Singh, Manish Gupta, and Vasudeva Varma.
    2018. Unity in Diversity: Learning Distributed Heterogeneous Sentence Representation
    for Extractive Summarization. In *Proceedings of the 32nd AAAI Conference on Artificial
    Intelligence (AAAI 2018)*. New Orleans, United States, 5473–5480.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020) Yan Song, Yuanhe Tian, Nan Wang, and Fei Xia. 2020. Summarizing
    Medical Conversations via Identifying Important Utterances. In *Proceedings of
    the 28th International Conference on Computational Linguistics (COLING2020)*.
    Online, 717–729.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun and Nenkova (2019) Simeng Sun and Ani Nenkova. 2019. The Feasibility of
    Embedding Based Automatic Evaluation for Single Document Summarization. In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP
    2019)*. Hong Kong, China, 1216–1221.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing Properties
    of Neural Networks. In *Proceedings of the 2nd International Conference on Learning
    Representations (ICLR 2014)*. Banff, Canada.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2017) Haihui Tan, Ziyu Lu, and Wenjie Li. 2017. Neural Network based
    Reinforcement Learning for Real-time Pushing on Text Stream. In *Proceedings of
    the 40th International ACM SIGIR Conference on Research and Development in Information
    Retrieval (SIGIR2017)*. Tokyo, Japan, 913–916.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tas and Kiyani (2007) Oguzhan Tas and Farzad Kiyani. 2007. A Survey Automatic
    Text Summarization. *PressAcademia Procedia* 5, 1, 205–213.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Torfi et al. (2020) Amirsina Torfi, Rouzbeh A. Shirvani, Yaser Keneshloo, Nader
    Tavaf, and Edward A. Fox. 2020. Natural Language Processing Advancements By Deep
    Learning: A Survey. *CoRR* abs/2003.01200 (2020). [https://arxiv.org/abs/2003.01200](https://arxiv.org/abs/2003.01200)'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tu et al. (2016) Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang
    Li. 2016. Modeling Coverage for Neural Machine Translation. In *Proceedings of
    the 54th Annual Meeting of the Association for Computational Linguistics (ACL
    2016)*. Berlin, Germany.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ulrich et al. (2008) Jan Ulrich, Gabriel Murray, and Giuseppe Carenini. 2008.
    A Publicly Available Annotated Corpus for Supervised Email Summarization. In *Proceedings
    of the Twenty-Third AAAI Conference on Artificial Intelligence in Enhanced Messaging
    Workshop (AAAI 2008)*. Chicago, United States.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    Is All You Need. In *Proceedings of the Annual Conference on Neural Information
    Processing Systems (NIPS 2017)*. Long Beach, United States, 5998–6008.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vinyals et al. (2015) Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015.
    Pointer Networks. In *Proceedings of the 2015 Annual Conference on Neural Information
    Processing Systems (NIPS 2015)*. Montreal, Canada, 2692–2700.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vodolazova et al. (2013) Tatiana Vodolazova, Elena Lloret, Rafael Muñoz, and
    Manuel Palomar. 2013. Extractive Text Summarization: Can We Use the Same Techniques
    for Any Text?. In *Proceedings of the 18th International Conference on Applications
    of Natural Language to Information Systems (NLDB 2013)*. Salford, UK, 164–175.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan and Yang (2006) Xiaojun Wan and Jianwu Yang. 2006. Improved Affinity Graph
    based Multi-document Summarization. In *Proceedings of the Human Language Technology
    Conference of the North American Chapter of the Association of Computational Linguistics
    (NAACL 2006)*. New York,United States, 336–347.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan and Yang (2008) Xiaojun Wan and Jianwu Yang. 2008. Multi-document Summarization
    Using Cluster-based Link Analysis. In *Proceedings of the 31st International ACM
    SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008)*.
    Singapore, 299–306.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, and
    Xuanjing Huang. 2020a. Heterogeneous Graph Neural Networks for Extractive Document
    Summarization. In *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics (ACL 2020)*. Online, 6209–6219.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Hu Wang, Qi Wu, and Chunhua Shen. 2020b. Soft Expert Reward
    Learning for Vision-and-Language Navigation. In *Proceedings of the 16th European
    Conference on Computer Vision (ECCV 2020)*. Online, 126–141.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Ling (2016) Lu Wang and Wang Ling. 2016. Neural Network-Based Abstract
    Generation for Opinions and Arguments. In *Proceedings of the 2016 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (HLT-NAACL 2016)*. San Diego California, United States,
    47–57.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021) Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp,
    and Caiming Xiong. 2021. Controllable Abstractive Dialogue Summarization with
    Sketch Supervision. In *Findings of the Association for Computational Linguistics:
    (ACL/IJCNLP 2021)*. Online, 5108–5122.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020b) Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu, and Chenliang
    Li. 2020b. MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question
    Answering and Summarization. In *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics (ACL 2020)*. Online, 3586–3596.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020a) Runxin Xu, Jun Cao, Mingxuan Wang, Jiaze Chen, Hao Zhou,
    Ying Zeng, Yuping Wang, Li Chen, Xiang Yin, Xijin Zhang, Songcheng Jiang, Yuxuan
    Wang, and Lei Li. 2020a. Xiaomingbot: A Multilingual Robot News Reporter. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics: System
    Demonstrations (ACL 2020)*. Online, 1–8.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. ([n.d.]) Min Yang, Chengming Li, Fei Sun, Zhou Zhao, Ying Shen,
    and Chenglin Wu. [n.d.]. Be Relevant, Non-Redundant, and Timely: Deep Reinforcement
    Learning for Real-Time Event Summarization. In *The Thirty-Fourth AAAI Conference
    on Artificial Intelligence (AAAI2020)*. New York, USA, 9410–9417.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2018) Min Yang, Wei Zhao, Jianbo Ye, Zeyang Lei, Zhou Zhao, and
    Soufei Zhang. 2018. Investigating Capsule Networks with Dynamic Routing for Text
    Classification. In *Proceedings of the 2018 Conference on Empirical Methods in
    Natural Language Processing (EMNLP 2018)*. Brussels, Belgium, 3110–3119.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2016) Qian Yang, Rebecca J Passonneau, and Gerard De Melo. 2016.
    PEAK: Pyramid Evaluation via Automated Knowledge Extraction. In *Proceedings of
    the Thirtieth AAAI Conference on Artificial Intelligence (AAAI 2016)*. Phoenix,
    Arizona, 2673–2680.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
    Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized Autoregressive Pretraining
    for Language Understanding. In *Proceedings of the Annual Conference on Neural
    Information Processing System (NIPS 2019)*. Vancouver, Canada, 5754–5764.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2018) Kaichun Yao, Libo Zhang, Tiejian Luo, and Yanjun Wu. 2018.
    Deep Reinforcement Learning for Extractive Document Summarization. *Neurocomputing*
    284, 52–62.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yasunaga et al. (2019) Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R
    Fabbri, Irene Li, Dan Friedman, and Dragomir R Radev. 2019. Scisummnet: A Large
    Annotated Corpus and Content-impact Models for Scientific Paper Summarization
    with Citation Networks. In *Proceedings of the AAAI Conference on Artificial Intelligence
    (AAAI 2019)*. Honolulu, United States, 7386–7393.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yasunaga et al. (2017) Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush
    Pareek, Krishnan Srinivasan, and Dragomir R. Radev. 2017. Graph-based Neural Multi-Document
    Summarization. In *Proceedings of the 21st Conference on Computational Natural
    Language Learning (CoNLL 2017)*. Vancouver, Canada, 452–462.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin and Pei (2015) Wenpeng Yin and Yulong Pei. 2015. Optimizing Sentence Modeling
    and Selection for Document Summarization. In *Proceedings of the 24th International
    Joint Conference on Artificial Intelligence (IJCAI 2015)*. Buenos Aires, Argentina,
    1383–1389.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, and Amr Ahmed. 2020. Big Bird: Transformers for Longer Sequences. In
    *Advances in Neural Information Processing Systems 33: Annual Conference on Neural
    Information Processing Systems 2020 (NeurIPS 2020)*. Online.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zajic et al. (2008) David M Zajic, Bonnie J Dorr, and Jimmy Lin. 2008. Single-document
    and Multi-document Summarization Techniques for Email Threads Using Sentence Compression.
    *Information Processing & Management* 44, 4, 1600–1610.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018b) Jianmin Zhang, Jiwei Tan, and Xiaojun Wan. 2018b. Adapting
    Neural Single-document Summarization Model for Abstractive Multi-document Summarization:
    A Pilot Study. In *Proceedings of the 11th International Conference on Natural
    Language Generation (INLG 2018)*. Tilburg, Netherlands, 381–390.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020d) Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J.
    Liu. 2020d. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive
    Summarization. In *Proceedings of the 37th International Conference on Machine
    Learning (ICML2020)*. Online, 11328–11339.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018a) Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. 2018a.
    Interpretable Convolutional Neural Networks. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR2018)*. Salt Lake City, United
    States, 8827–8836.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021) Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, and Mohit
    Bansal. 2021. EmailSum: Abstractive Email Thread Summarization. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP
    2021)*. Online, 6895–6909.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020a) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020a. BERTScore: Evaluating Text Generation with BERT. In *Proceedings
    of the 8th International Conference on Learning Representations (ICLR 2020)*.
    Addis Ababa, Ethiopia.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Wei Emma Zhang, Quan Z. Sheng, Ahoud Abdulrahmn F. Alhazmi,
    and Chenliang Li. 2020b. Adversarial Attacks on Deep-learning Models in Natural
    Language Processing: A Survey. *ACM Transactions on Intelligent Systems and Technology*
    11, 3, 24:1–24:41.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020c) Wei Emma Zhang, Quan Z. Sheng, Adnan Mahmood, Dai Hoang
    Tran, Munazza Zaib, Salma Abdalla Hamad, Abdulwahab Aljubairy, Ahoud Abdulrahmn F.
    Alhazmi, Subhash Sagar, and Congbo Ma. 2020c. The 10 Research Topics in the Internet
    of Things. In *6th IEEE International Conference on Collaboration and Internet
    Computing (CIC 2020)*. Atlanta, USA, 34–43.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2016) Yong Zhang, Meng Joo Er, Rui Zhao, and Mahardhika Pratama.
    2016. Multiview Convolutional Neural Networks for Multidocument Extractive Summarization.
    *IEEE Transactions on Cybernetics* 47, 10, 3230–3242.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021) Mingjun Zhao, Shengli Yan, Bang Liu, Xinwang Zhong, Qian
    Hao, Haolan Chen, Di Niu, Bowei Long, and Weidong Guo. 2021. QBSUM: A Large-scale
    Query-based Document Summarization Dataset from real-world applications. *Comput.
    Speech Lang.* 66 (2021), 101166.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2019) Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M.
    Meyer, and Steffen Eger. 2019. MoverScore: Text Generation Evaluating with Contextualized
    Embeddings and Earth Mover Distance. In *Proceedings of the Conference on Empirical
    Methods in Natural Language and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP 2019)*. Hong Kong, China, 563–578.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2019) Xin Zheng, Aixin Sun, Jing Li, and Karthik Muthuswamy. 2019.
    Subtopic-driven Multi-Document Summarization. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019)*. Hong Kong,
    China, 3151–3160.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. (2020) Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng
    Qiu, and Xuanjing Huang. 2020. Extractive Summarization as Text Matching. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics (ACL
    2020)*. Online, 6197–6208.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma,
    Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and
    Dragomir R. Radev. 2021. QMSum: A New Benchmark for Query-based Multi-domain Meeting
    Summarization. In *Proceedings of the 2021 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    (NAACL-HLT 2021)*. Online, 5905–5921.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2016) Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. 2016. Learning Deep Features for Discriminative Localization.
    In *Proceedings of the IEEE conference on Computer Vision and Pattern Recognition
    (CVPR 2016)*. Las Vegas, United States, 2921–2929.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2021) Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021.
    MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization. In
    *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, (NAACL-HLT 2021)*.
    Online, 5927–5934.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020) Chenguang Zhu, Ruochen Xu, Michael Zeng, and Xuedong Huang.
    2020. A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain
    Pretraining. In *Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing: Findings (EMNLP 2020)*. Online, 194–203.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2018) Junnan Zhu, Haoran Li, Tianshang Liu, Yu Zhou, Jiajun Zhang,
    and Chengqing Zong. 2018. MSMO: Multimodal Summarization with Multimodal Output.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2018)*. Brussels, Belgium, 4154–4164.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zopf (2018) Markus Zopf. 2018. Estimating Summary Quality with Pairwise Preferences.
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018)*.
    New Orleans, United States, 1687–1696.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
