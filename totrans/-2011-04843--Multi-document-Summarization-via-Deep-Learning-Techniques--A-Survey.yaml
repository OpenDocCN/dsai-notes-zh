- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:58:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:58:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2011.04843] Multi-document Summarization via Deep Learning Techniques: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2011.04843] 通过深度学习技术的多文档摘要生成：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2011.04843](https://ar5iv.labs.arxiv.org/html/2011.04843)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2011.04843](https://ar5iv.labs.arxiv.org/html/2011.04843)
- en: 'Multi-document Summarization via Deep Learning Techniques: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过深度学习技术的多文档摘要生成：综述
- en: CONGBO MA The University of Adelaide [congbo.ma@adelaide.edu.au](mailto:congbo.ma@adelaide.edu.au)
    ,  WEI EMMA ZHANG The University of Adelaide [wei.e.zhang@adelaide.edu.au](mailto:wei.e.zhang@adelaide.edu.au)
    ,  MINGYU GUO The University of Adelaide [mingyu.guo@adelaide.edu.au](mailto:mingyu.guo@adelaide.edu.au)
    ,  HU WANG The University of Adelaide [hu.wang@adelaide.edu.au](mailto:hu.wang@adelaide.edu.au)
     and  QUAN Z. SHENG Macquarie University [michael.sheng@mq.edu.au](mailto:michael.sheng@mq.edu.au)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: CONGBO MA 阿德莱德大学 [congbo.ma@adelaide.edu.au](mailto:congbo.ma@adelaide.edu.au)，
    WEI EMMA ZHANG 阿德莱德大学 [wei.e.zhang@adelaide.edu.au](mailto:wei.e.zhang@adelaide.edu.au)，
    MINGYU GUO 阿德莱德大学 [mingyu.guo@adelaide.edu.au](mailto:mingyu.guo@adelaide.edu.au)，
    HU WANG 阿德莱德大学 [hu.wang@adelaide.edu.au](mailto:hu.wang@adelaide.edu.au) 和 QUAN
    Z. SHENG 麦考瑞大学 [michael.sheng@mq.edu.au](mailto:michael.sheng@mq.edu.au)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Multi-document summarization (MDS) is an effective tool for information aggregation
    that generates an informative and concise summary from a cluster of topic-related
    documents. Our survey, the first of its kind, systematically overviews the recent
    deep learning based MDS models. We propose a novel taxonomy to summarize the design
    strategies of neural networks and conduct a comprehensive summary of the state-of-the-art.
    We highlight the differences between various objective functions that are rarely
    discussed in the existing literature. Finally, we propose several future directions
    pertaining to this new and exciting field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 多文档摘要生成（MDS）是一种有效的信息聚合工具，它从一组相关主题的文档中生成一个信息丰富且简洁的摘要。我们的综述是首个此类综述，系统性地回顾了近期基于深度学习的MDS模型。我们提出了一种新的分类法来总结神经网络的设计策略，并对最先进技术进行了全面的总结。我们强调了现有文献中鲜有讨论的各种目标函数之间的差异。最后，我们提出了几个与这一新兴且令人兴奋的领域相关的未来研究方向。
- en: 'Multi-document summarization, Deep neural networks, Machine learning^†^†ccs:
    Computing methodologies Natural language processing^†^†ccs: Computing methodologies Machine
    learning algorithms^†^†ccs: Computing methodologies Information extraction'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '多文档摘要生成，深度神经网络，机器学习^†^†ccs: 计算方法 自然语言处理^†^†ccs: 计算方法 机器学习算法^†^†ccs: 计算方法 信息提取'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: In this era of rapidly advancing technology, the exponential increase of data
    availability makes analyzing and understanding text files a tedious, labor-intensive,
    and time-consuming task (Oussous et al., [2018](#bib.bib118); Hu et al., [2017](#bib.bib64)).
    The need to process this abundance of text data rapidly and efficiently calls
    for new, effective text summarization techniques. Text summarization is a key
    natural language processing (NLP) tasks that automatically converts a text, or
    a collection of texts within the same topic, into a concise summary that contains
    key semantic information which can be beneficial for many downstream applications
    such as creating news digests, search engine, and report generation (Paulus et al.,
    [2018](#bib.bib124)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术迅猛发展的时代，数据可用性的指数增长使得分析和理解文本文件成为一项繁琐、劳动密集且耗时的任务（Oussous et al., [2018](#bib.bib118);
    Hu et al., [2017](#bib.bib64)）。快速高效地处理这些大量文本数据的需求催生了新的有效文本摘要技术。文本摘要是一个关键的自然语言处理（NLP）任务，它自动将一段文本或同一主题的文本集合转换为包含关键信息的简洁摘要，这对许多下游应用（如新闻摘要生成、搜索引擎和报告生成）是非常有益的（Paulus
    et al., [2018](#bib.bib124)）。
- en: Text can be summarized from one or several documents, resulting in single document
    summarization (SDS) and multi-document summarization (MDS). While simpler to perform,
    SDS may not produce comprehensive summaries because it does not make good use
    of related, or more recent, documents. Conversely, MDS generates more comprehensive
    and accurate summaries from documents written at different times, covering different
    perspectives, but is accordingly more complicated as it tries to resolve potentially
    diverse and redundant information (Tas and Kiyani, [2007](#bib.bib147)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 文本可以从一个或多个文档中总结，产生单文档摘要（SDS）和多文档摘要（MDS）。虽然SDS更简单易行，但可能不会产生全面的摘要，因为它未能充分利用相关或更近期的文档。相反，MDS从不同时间撰写的文档中生成更全面和准确的摘要，涵盖了不同的观点，但相应地更复杂，因为它尝试解决可能存在的多样和冗余信息（Tas
    和 Kiyani，[2007](#bib.bib147)）。
- en: In addition, excessively long input documents often lead to model degradation
    (Jin et al., [2020](#bib.bib72)). It is challenging for models to retain the most
    critical contents of complex input sequences while generating a coherent, non-redundant,
    factual consistent and grammatically readable summary. Therefore, MDS requires
    models to have stronger capabilities for analyzing the input documents, identifying
    and merging consistent information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，过长的输入文档通常会导致模型性能下降（Jin 等，[2020](#bib.bib72)）。对于模型来说，保留复杂输入序列的最关键内容，同时生成连贯、不冗余、事实一致且语法可读的摘要是一项挑战。因此，MDS要求模型具备更强的能力来分析输入文档，识别和合并一致的信息。
- en: 'MDS enjoys a wide range of real-world applications, including summarization
    of news (Fabbri et al., [2019](#bib.bib44)), scientific publications (Yasunaga
    et al., [2019](#bib.bib167)), emails (Carenini et al., [2007](#bib.bib23); Zajic
    et al., [2008](#bib.bib171)), product reviews (Gerani et al., [2014](#bib.bib50)),
    medical documents (Afantenos et al., [2005](#bib.bib2)), lecture feedback (Luo
    and Litman, [2015](#bib.bib99); Luo et al., [2016](#bib.bib100)), software project
    activities (Alghamdi et al., [2020](#bib.bib3)), and Wikipedia articles generation
    (Liu et al., [2018](#bib.bib94)). Recently, MDS technology has also received a
    great amount of industry attention; an intelligent multilingual news reporter
    bot named Xiaomingbot (Xu et al., [2020a](#bib.bib161)) was developed for news
    generation, which can summarize multiple news sources into one article and translate
    it into multiple languages. Massive application requirements and rapidly growing
    online data have promoted the development of MDS. Existing methods using traditional
    algorithms are based on: term frequency-inverse document frequency (TF-IDF) (Radev
    et al., [2004](#bib.bib128); Baralis et al., [2012](#bib.bib11)), clustering (Goldstein
    et al., [2000](#bib.bib52); Wan and Yang, [2008](#bib.bib155)), graphs (Mani and
    Bloedorn, [1997](#bib.bib102); Wan and Yang, [2006](#bib.bib154)) and latent semantic
    analysis (Arora and Ravindran, [2008](#bib.bib8); Haghighi and Vanderwende, [2009](#bib.bib59)).
    Most of these works still generate summaries with manually crafted features (Mihalcea
    and Tarau, [2005](#bib.bib106); Wan and Yang, [2006](#bib.bib154)), such as sentence
    position features (Baxendale, [1958](#bib.bib12); Erkan and Radev, [2004](#bib.bib41)),
    sentence length features (Erkan and Radev, [2004](#bib.bib41)), proper noun features
    (Vodolazova et al., [2013](#bib.bib153)), cue-phrase features (Gupta and Lehal,
    [2010](#bib.bib58)), biased word features, sentence-to-sentence cohesion and sentence-to-centroid
    cohesion. Deep learning has gained enormous attention in recent years due to its
    success in various domains, for instance, computer vision (Krizhevsky et al.,
    [2012](#bib.bib79)), natural language processing (Devlin et al., [2014](#bib.bib36))
    and multi-modal learning (Wang et al., [2020b](#bib.bib157)). Both industry and
    academia have embraced deep learning to solve complex tasks due to its capability
    of capturing highly nonlinear relations of data. Moreover, deep learning based
    models reduce dependence on manual feature extraction and pre-knowledge in the
    field of linguistics, drastically improving the ease of engineering (Torfi et al.,
    [2020](#bib.bib148)). Therefore, deep learning based methods demonstrate outstanding
    performance in MDS tasks in most cases (Li et al., [2020a](#bib.bib91); Cao et al.,
    [2015b](#bib.bib21); Lu et al., [2020](#bib.bib98); Liu and Lapata, [2019](#bib.bib95);
    Lebanoff et al., [2019](#bib.bib83)). With recently dramatic improvements in computational
    power and the release of increasing numbers of public datasets, neural networks
    with deeper layers and more complex structures have been applied in MDS (Liu and
    Lapata, [2019](#bib.bib95); Li et al., [2017b](#bib.bib90)), accelerating the
    development of text summarization with more powerful and robust models. These
    tasks are attracting attention in the natural language processing community; the
    number of research publications on deep learning based MDS has increased rapidly
    over the last five years.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MDS 在现实世界中有广泛的应用，包括新闻摘要（Fabbri et al., [2019](#bib.bib44)）、科学出版物（Yasunaga et
    al., [2019](#bib.bib167)）、电子邮件（Carenini et al., [2007](#bib.bib23); Zajic et al.,
    [2008](#bib.bib171)）、产品评论（Gerani et al., [2014](#bib.bib50)）、医学文档（Afantenos et
    al., [2005](#bib.bib2)）、讲座反馈（Luo and Litman, [2015](#bib.bib99); Luo et al., [2016](#bib.bib100)）、软件项目活动（Alghamdi
    et al., [2020](#bib.bib3)）以及维基百科文章生成（Liu et al., [2018](#bib.bib94)）。最近，MDS 技术也受到了大量行业关注；一个名为
    Xiaomingbot 的智能多语言新闻报道机器人（Xu et al., [2020a](#bib.bib161)）被开发用于新闻生成，能够将多个新闻来源汇总为一篇文章并翻译成多种语言。大量的应用需求和快速增长的在线数据促进了
    MDS 的发展。现有使用传统算法的方法基于：词频-逆文档频率（TF-IDF）（Radev et al., [2004](#bib.bib128); Baralis
    et al., [2012](#bib.bib11)）、聚类（Goldstein et al., [2000](#bib.bib52); Wan and Yang,
    [2008](#bib.bib155)）、图（Mani and Bloedorn, [1997](#bib.bib102); Wan and Yang, [2006](#bib.bib154)）和潜在语义分析（Arora
    and Ravindran, [2008](#bib.bib8); Haghighi and Vanderwende, [2009](#bib.bib59)）。这些工作大多数仍生成具有人工特征（Mihalcea
    and Tarau, [2005](#bib.bib106); Wan and Yang, [2006](#bib.bib154)）的摘要，如句子位置特征（Baxendale,
    [1958](#bib.bib12); Erkan and Radev, [2004](#bib.bib41)）、句子长度特征（Erkan and Radev,
    [2004](#bib.bib41)）、专有名词特征（Vodolazova et al., [2013](#bib.bib153)）、提示短语特征（Gupta
    and Lehal, [2010](#bib.bib58)）、偏向词特征、句子间的连贯性和句子与中心点的连贯性。深度学习由于在计算机视觉（Krizhevsky
    et al., [2012](#bib.bib79)）、自然语言处理（Devlin et al., [2014](#bib.bib36)）和多模态学习（Wang
    et al., [2020b](#bib.bib157)）等各个领域取得的成功，近年来引起了极大的关注。由于深度学习能够捕捉数据的高度非线性关系，业界和学界都开始采用深度学习来解决复杂任务。此外，基于深度学习的模型减少了对手动特征提取和语言学领域先验知识的依赖，极大地提高了工程的便利性（Torfi
    et al., [2020](#bib.bib148)）。因此，基于深度学习的方法在大多数 MDS 任务中表现出色（Li et al., [2020a](#bib.bib91);
    Cao et al., [2015b](#bib.bib21); Lu et al., [2020](#bib.bib98); Liu and Lapata,
    [2019](#bib.bib95); Lebanoff et al., [2019](#bib.bib83)）。随着计算能力的剧增和越来越多公共数据集的发布，具有更深层次和更复杂结构的神经网络已应用于
    MDS（Liu and Lapata, [2019](#bib.bib95); Li et al., [2017b](#bib.bib90)），加速了更强大和更稳健的文本摘要模型的发展。这些任务在自然语言处理社区中引起了关注；基于深度学习的
    MDS 研究出版物数量在过去五年中迅速增加。
- en: '![Refer to caption](img/cbad3dcb98d0db2de34ab13259562446.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cbad3dcb98d0db2de34ab13259562446.png)'
- en: Figure 1\. Hierarchical Structure of This Survey.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 本次调查的层次结构。
- en: 'The prosperity of deep learning for summarization in both academia and industry
    requires a comprehensive review of current publications for researchers to better
    understand the process and research progress. However, most of the existing summarization
    survey papers are based on traditional algorithms instead of deep learning based
    methods or target general text summarization (Nenkova and McKeown, [2012](#bib.bib114);
    Haque et al., [2013](#bib.bib60); Ferreira et al., [2014](#bib.bib46); Shah and
    Jivani, [2016](#bib.bib139); El-Kassas et al., [2021](#bib.bib39)). We have therefore
    surveyed recent publications on deep learning methods for MDS that, to the best
    of our knowledge, is the first comprehensive survey of this field. This survey
    has been designed to classify neural based MDS techniques into diverse categories
    thoroughly and systematically. We also conduct a detailed discussion on the categorization
    and progress of these approaches to establish a clearer concept standing in the
    shoes of readers. We hope this survey provides a panorama for researchers, practitioners
    and educators to quickly understand and step into the field of deep learning based
    MDS. The key contributions of this survey are three-fold:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在学术界和工业界对摘要的繁荣需要对当前出版物进行全面回顾，以便研究人员更好地理解过程和研究进展。然而，大多数现有的摘要调查论文是基于传统算法而不是基于深度学习的方法，或者针对一般文本摘要（Nenkova
    和 McKeown，[2012](#bib.bib114)；Haque 等，[2013](#bib.bib60)；Ferreira 等，[2014](#bib.bib46)；Shah
    和 Jivani，[2016](#bib.bib139)；El-Kassas 等，[2021](#bib.bib39)）。因此，我们调查了有关深度学习方法的
    MDS 的最新出版物，据我们所知，这是该领域首个全面的调查。本次调查旨在将基于神经网络的 MDS 技术分类为不同的类别，并对这些方法的分类和进展进行详细讨论，以建立读者更清晰的概念。我们希望本次调查为研究人员、从业者和教育者提供一个全景，帮助他们快速了解并进入深度学习基于
    MDS 的领域。本次调查的关键贡献有三点：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a categorization scheme to organize current research and provide
    a comprehensive review for deep learning based MDS techniques, including deep
    learning based models, objective functions, benchmark datasets and evaluation
    metrics.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种分类方案，以组织当前的研究，并为基于深度学习的 MDS 技术提供全面的回顾，包括基于深度学习的模型、目标函数、基准数据集和评估指标。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review development movements and provide a systematic overview and summary
    of the state-of-the-art. We also summarize nine network design strategies based
    on our extensive studies of the current models.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了发展动态，提供了对最前沿技术的系统概述和总结。我们还总结了基于我们对当前模型的广泛研究的九种网络设计策略。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss the open issues of deep learning based multi-document summarization
    and identify the future research directions of this field. We also propose potential
    solutions for some discussed research directions.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了深度学习多文档摘要的开放问题，并确定了该领域的未来研究方向。我们还为一些讨论的研究方向提出了潜在的解决方案。
- en: Paper Selection. We used Google Scholar as the main search engine to select
    representative works from 2015 to 2021\. High-quality papers were selected from
    top NLP and AI journals and conferences, include ACL¹¹1Annual Meeting of the Association
    for Computational Linguistics., EMNLP²²2Empirical Methods in Natural Language
    Processing., COLING³³3International Conference on Computational Linguistics, NAACL⁴⁴4Annual
    Conference of the North American Chapter of the Association for Computational
    Linguistics., AAAI⁵⁵5AAAI Conference on Artificial Intelligence., ICML⁶⁶6International
    Conference on Machine Learning., ICLR⁷⁷7International Conference on Learning Representations
    and IJCAI⁸⁸8International Joint Conference on Artificial Intelligence.. The major
    keywords we used include multi-documentation summarization, summarization, extractive
    summarization, abstractive summarization, deep learning and neural networks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 论文选择。我们使用 Google Scholar 作为主要搜索引擎，从 2015 年到 2021 年选择了具有代表性的工作。我们从顶级 NLP 和 AI
    期刊及会议中选择了高质量的论文，包括 ACL¹¹1计算语言学协会年会，EMNLP²²2自然语言处理的经验方法，COLING³³3计算语言学国际会议，NAACL⁴⁴4北美计算语言学协会年会，AAAI⁵⁵5人工智能
    AAAI 会议，ICML⁶⁶6机器学习国际会议，ICLR⁷⁷7学习表征国际会议和 IJCAI⁸⁸8国际联合人工智能会议。我们使用的主要关键词包括多文档摘要、摘要、抽取式摘要、生成式摘要、深度学习和神经网络。
- en: 'Organization of the Survey. This survey will cover various aspects of recent
    advanced deep learning based works in MDS. Our proposed taxonomy categorizes the
    works from six aspects (Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")). To be more self-contained,
    in Section [2](#S2 "2\. From Single to Multi-document Summarization ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey"), we give the problem definition,
    the processing framework of text summarization, discuss similarities and differences
    of between SDS and MDS. Nine deep learning architecture design strategies, six
    deep learning based methods, and the variant tasks of MDS are presented in Section
    [3](#S3 "3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey"). Section [4](#S4 "4\. Objective
    Functions ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")
    summarizes objective functions that guide the model optimization process in the
    reviewed literature while evaluation metrics in Section [5](#S5 "5\. Evaluation
    metrics ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")
    help readers choose suitable indices to evaluate the effectiveness of a model.
    Section [6](#S6 "6\. Datasets ‣ Multi-document Summarization via Deep Learning
    Techniques: A Survey") summarizes standard and the variant MDS datasets. Finally,
    Section [7](#S7 "7\. Future research directions and open issues ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey") discusses future research
    directions for deep learning based MDS followed by conclusions in Section [8](#S8
    "8\. Conclusion ‣ Multi-document Summarization via Deep Learning Techniques: A
    Survey").'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 调查的组织结构。本调查将涵盖MDS中最近先进的深度学习工作的各个方面。我们提出的分类法从六个方面对这些工作进行分类（见图 [1](#S1.F1 "图 1
    ‣ 1\. 引言 ‣ 基于深度学习的多文档摘要技术：综述")）。为了更自成体系，在第 [2](#S2 "2\. 从单文档到多文档摘要 ‣ 基于深度学习的多文档摘要技术：综述")
    节中，我们给出问题定义、文本摘要的处理框架，讨论SDS和MDS之间的相似性和差异。在第 [3](#S3 "3\. 基于深度学习的多文档摘要方法 ‣ 基于深度学习的多文档摘要技术：综述")
    节中，介绍了九种深度学习架构设计策略、六种基于深度学习的方法以及MDS的变体任务。第 [4](#S4 "4\. 目标函数 ‣ 基于深度学习的多文档摘要技术：综述")
    节总结了指导模型优化过程的目标函数，而第 [5](#S5 "5\. 评估指标 ‣ 基于深度学习的多文档摘要技术：综述") 节中的评估指标帮助读者选择合适的指标来评估模型的有效性。第
    [6](#S6 "6\. 数据集 ‣ 基于深度学习的多文档摘要技术：综述") 节总结了标准和变体MDS数据集。最后，第 [7](#S7 "7\. 未来研究方向和未解问题
    ‣ 基于深度学习的多文档摘要技术：综述") 节讨论了基于深度学习的MDS的未来研究方向，并在第 [8](#S8 "8\. 结论 ‣ 基于深度学习的多文档摘要技术：综述")
    节中作出结论。
- en: '![Refer to caption](img/9bc5fa7d2ef048754fd1f844270e72b7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9bc5fa7d2ef048754fd1f844270e72b7.png)'
- en: Figure 2\. The Processing Framework of Text Summarization. Each of the highlighted
    steps (the one with triangle mark) indicates the differences between SDS and MDS.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 文本摘要的处理框架。每个突出显示的步骤（带有三角形标记的步骤）表示SDS和MDS之间的差异。
- en: 2\. From Single to Multi-document Summarization
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 从单文档到多文档摘要
- en: Before we dive into the details of existing deep learning based techniques,
    we start by defining SDS and MDS, and introducing the concepts used un both methods.
    The aim of MDS is to generate a concise and informative summary $Sum$ from a collection
    of documents $D$. $D$ denotes a cluster of topic-related documents $\left\{d_{i}\mid
    i\in[1,N]\right\}$, where $N$ is the number of documents. Each document $d_{i}$
    consists of $M_{d_{i}}$ sentences $\left\{s_{i,j}\mid j\in[1,M_{d_{i}}]\right\}$.
    $s_{i,j}$ refers to the $j$-th sentence in the $i$-th document. The standard summary
    $Ref$ is called the golden summary or reference summary. Currently, most golden
    summaries are written by experts. We keep this notation consistent throughout
    the article.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入现有基于深度学习的技术细节之前，我们首先定义SDS和MDS，并介绍这两种方法中使用的概念。MDS的目标是从一组文档$D$中生成一个简洁而信息丰富的摘要$Sum$。$D$表示一个主题相关文档的集合$\left\{d_{i}\mid
    i\in[1,N]\right\}$，其中$N$是文档的数量。每个文档$d_{i}$包含$M_{d_{i}}$个句子$\left\{s_{i,j}\mid
    j\in[1,M_{d_{i}}]\right\}$。$s_{i,j}$指的是第$i$个文档中的第$j$个句子。标准摘要$Ref$被称为黄金摘要或参考摘要。目前，大多数黄金摘要由专家编写。我们在整篇文章中保持这一符号的一致性。
- en: 'To give readers a clear understanding of the processing of deep learning based
    summarization tasks, we summarize and illustrate the processing framework as shown
    in Figure [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey"). The first step is preprocessing input
    document(s), such as segmenting sentences, tokenizing non-alphabetic characters,
    and removing punctuation (Shirwandkar and Kulkarni, [2018](#bib.bib141)). MDS
    models in particular need to select suitable concatenation methods to capture
    cross-document relations. Then, an appropriate deep learning based model is chosen
    to generate semantic-rich representation for downstream tasks. The next step is
    to fuse these various types of representation for later sentence selection or
    summary generation. Finally, document(s) are transformed into a concise and informative
    summary. Each of the highlighted steps in Figure [2](#S1.F2 "Figure 2 ‣ 1\. Introduction
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey") (indicated
    by triangles) indicates a difference between SDS and MDS. Based on this process,
    the research questions of MDS can be summarized as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '为了让读者清楚理解深度学习基于总结任务的处理过程，我们总结并阐述了如图[2](#S1.F2 "Figure 2 ‣ 1\. Introduction
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")所示的处理框架。第一步是预处理输入文档，如句子分割、非字母字符的标记化和标点符号的去除（Shirwandkar
    和 Kulkarni，[2018](#bib.bib141)）。尤其是MDS模型需要选择合适的连接方法来捕捉跨文档关系。接着，选择一个合适的深度学习模型来生成语义丰富的表示以用于下游任务。下一步是融合这些不同类型的表示以用于后续的句子选择或摘要生成。最后，将文档转化为简洁且信息丰富的摘要。图[2](#S1.F2
    "Figure 2 ‣ 1\. Introduction ‣ Multi-document Summarization via Deep Learning
    Techniques: A Survey")中每一个用三角形标出的步骤表示了SDS和MDS之间的差异。基于此过程，MDS的研究问题可以总结如下：'
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How to capture the cross-document relations and in-document relations from the
    input documents?
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何从输入文档中捕捉跨文档关系和文档内关系？
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compared to SDS, how to extract or generate salient information in a larger
    search space containing conflict, duplication and complementary information?
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相比于SDS，如何在包含冲突、重复和补充信息的大范围搜索空间中提取或生成显著信息？
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How to best fuse various representation from deep learning based models and
    external knowledge?
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何最佳地融合来自深度学习模型和外部知识的各种表示？
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How to comprehensively evaluate the performance of MDS models?
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何全面评估MDS模型的性能？
- en: The following sections provide a comprehensive analysis of the similarities
    and differences between SDS and MDS.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节提供了对SDS和MDS之间相似性和差异性的全面分析。
- en: '![Refer to caption](img/090b57d9028b1a6c46da70272153b233.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/090b57d9028b1a6c46da70272153b233.png)'
- en: Figure 3\. Summarization Construction Types for Text Summarization.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 文本摘要的总结构建类型。
- en: 2.1\. Similarities between SDS and MDS
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. SDS和MDS之间的相似性
- en: 'Existing SDS and MDS methods share the summarization construction types, learning
    strategies, evaluation indexes and objective functions. SDS and MDS both seek
    to compress the document(s) into a short and informative summary. Existing summarization
    methods can be grouped into abstractive summarization, extractive summarization
    and hybrid summarization (Figure [3](#S2.F3 "Figure 3 ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")).
    Extractive summarization methods select salient snippets from the source documents
    to creat informative summaries, and generally contain two major components: sentence
    ranking and sentence selection (Cao et al., [2015a](#bib.bib20); Nallapati et al.,
    [2017](#bib.bib110)). Abstractive summarization methods aim to present the main
    information of input documents by automatically generating summaries that are
    both succinct and coherent; this cluster of methods allows models to generate
    new words and sentences from a corpus pool (Paulus et al., [2018](#bib.bib124)).
    Hybrid models are proposed to combine the advantages of both extractive and abstractive
    methods to process the input texts. Research on summarization focuses on two learning
    strategies. One strategy seeks to enhance the generalization performance by improving
    the architecture design of the end-to-end models (Fabbri et al., [2019](#bib.bib44);
    Chu and Liu, [2019](#bib.bib31); Jin et al., [2020](#bib.bib72); Liu and Lapata,
    [2019](#bib.bib95)). The other leverages external knowledge or other auxiliary
    tasks to complement summary selection or generation (Cao et al., [2017](#bib.bib19);
    Li et al., [2020a](#bib.bib91)). Furthermore, both SDS and MDS aim to minimize
    the distance between machine-generated summary and golden summary. Therefore,
    SDS and MDS could share some indices to evaluate the performance of summarization
    models such as Recall-Oriented Understudy for Gisting Evaluation (ROUGE, see Section
    [5](#S5 "5\. Evaluation metrics ‣ Multi-document Summarization via Deep Learning
    Techniques: A Survey")), and objective functions to guide model optimization.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的SDS和MDS方法共享总结构建类型、学习策略、评估指标和目标函数。SDS和MDS都旨在将文档压缩成简短且信息丰富的摘要。现有的摘要方法可以分为抽象摘要、提取摘要和混合摘要（见图
    [3](#S2.F3 "Figure 3 ‣ 2\. From Single to Multi-document Summarization ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")）。提取摘要方法从源文档中选择显著的片段以创建信息丰富的摘要，通常包括两个主要组件：句子排名和句子选择（Cao等，[2015a](#bib.bib20)；Nallapati等，[2017](#bib.bib110)）。抽象摘要方法旨在通过自动生成简洁且连贯的摘要来呈现输入文档的主要信息；这一类方法允许模型从语料库中生成新的词汇和句子（Paulus等，[2018](#bib.bib124)）。混合模型则提出结合提取和抽象方法的优点来处理输入文本。摘要研究关注两种学习策略。一种策略通过改进端到端模型的架构设计来提高泛化性能（Fabbri等，[2019](#bib.bib44)；Chu和Liu，[2019](#bib.bib31)；Jin等，[2020](#bib.bib72)；Liu和Lapata，[2019](#bib.bib95)）。另一种策略则利用外部知识或其他辅助任务来补充摘要选择或生成（Cao等，[2017](#bib.bib19)；Li等，[2020a](#bib.bib91)）。此外，SDS和MDS都旨在最小化机器生成摘要与黄金摘要之间的距离。因此，SDS和MDS可以共享一些评估摘要模型性能的指标，如召回导向的学习评价（ROUGE，见第
    [5](#S5 "5\. Evaluation metrics ‣ Multi-document Summarization via Deep Learning
    Techniques: A Survey")节），以及用于指导模型优化的目标函数。'
- en: 2.2\. Differences between SDS and MDS
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. SDS与MDS的区别
- en: 'In the early stages of MDS, researchers directly applied SDS models to MDS
    (Mao et al., [2020](#bib.bib103)). However, a number of aspects in MDS that are
    different from SDS and these differences are also the breakthrough point for exploring
    the MDS models. We summarize the differences in the following five aspects:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDS的早期阶段，研究人员直接将SDS模型应用于MDS（Mao等，[2020](#bib.bib103)）。然而，MDS中存在许多与SDS不同的方面，这些差异也是探索MDS模型的突破点。我们总结了以下五个方面的差异：
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More diverse input document types;
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入文档类型更加多样化；
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Insufficient methods to capture cross-document relations;
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 捕捉跨文档关系的方法不足；
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: High redundancy and contradiction across input documents;
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入文档之间存在较高的冗余和矛盾。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Larger searching space but lack of sufficient training data;
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更大的搜索空间但缺乏足够的训练数据；
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lack of evaluation metrics specifically designed for MDS.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缺乏专门为MDS设计的评估指标。
- en: 'A defining different character between SDS and MDS is the number of input documents.
    The MDS tasks deal with multiple sources, of types that can be roughly divided
    into three groups:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SDS和MDS之间一个定义性不同的特点是输入文档的数量。MDS任务处理多个来源，通常可以大致分为三组：
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Many short sources, where each document is relatively short but the quantity
    of the input data is large. A typical example is product reviews summarization
    that aims to generate a short, informative summary from numerous individual reviews
    (Angelidis and Lapata, [2018](#bib.bib5)).
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 许多短文档，每个文档相对较短，但输入数据的数量很大。一个典型的例子是产品评论摘要，旨在从众多单独的评论中生成一个简短而信息丰富的摘要（Angelidis
    和 Lapata，[2018](#bib.bib5)）。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Few long sources. For example, generating a summary from a group of news articles
    (Fabbri et al., [2019](#bib.bib44)), or constructing a Wikipedia style article
    from several web articles (Liu et al., [2018](#bib.bib94)).
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 少量长文档。例如，从一组新闻文章中生成摘要（Fabbri 等，[2019](#bib.bib44)），或从几篇网络文章中构建维基百科风格的文章（Liu
    等，[2018](#bib.bib94)）。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hybrid sources containing one or few long documents with several to many shorter
    documents. For example, news article(s) with several readers’ comments to this
    news (Li et al., [2017a](#bib.bib89)), or a scientific summary from a long paper
    with several short corresponding citations (Yasunaga et al., [2019](#bib.bib167)).
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混合源包含一个或几个长文档和几个到许多较短的文档。例如，新闻文章及其多个读者评论（Li 等，[2017a](#bib.bib89)），或从一篇长论文中提取的科学总结和几个短的相关引用（Yasunaga
    等，[2019](#bib.bib167)）。
- en: 'As SDS only uses one input document, no additional processing is required to
    assess relationships between SDS inputs. By their very nature, the multiple input
    documents used in MDS are likely to contain more contradictory, redundant, and
    complementary information (Radev, [2000](#bib.bib127)). MDS models therefore require
    sophisticated algorithms to identify and cope with redundancy and contradictions
    across documents to ensure that the final summary is comprehensive. Detecting
    these relations across documents can bring benefits for MDS models. In the MDS
    tasks, there are two common methods to concatenate multiple input documents:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SDS只使用一个输入文档，因此无需额外处理来评估SDS输入之间的关系。就其本质而言，MDS中使用的多个输入文档可能包含更多矛盾、冗余和补充信息（Radev，[2000](#bib.bib127)）。因此，MDS模型需要复杂的算法来识别和处理文档之间的冗余和矛盾，以确保最终的摘要是全面的。检测这些文档间的关系可以为MDS模型带来好处。在MDS任务中，有两种常见的方法来连接多个输入文档：
- en: '![Refer to caption](img/2b57d1212642675d1e1a616bd9bb7d29.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2b57d1212642675d1e1a616bd9bb7d29.png)'
- en: Figure 4\. The Methods of Hierarchical Concatenation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 分层连接的方法。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Flat concatenation is a simple yet powerful concatenation method, where all
    input documents are spanned and processed as a flat sequence; to a certain extent,
    this method converts MDS to an SDS tasks. Inputting flat-concatenated documents
    requires models to have strong ability to process long sequences.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平坦连接是一种简单而强大的连接方法，其中所有输入文档被展开并作为一个平坦的序列处理；在一定程度上，这种方法将MDS任务转换为SDS任务。输入平坦连接的文档要求模型具备强大的处理长序列的能力。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hierarchical concatenation is able to preserve cross-document relations. However,
    many existing deep learning methods do not make full use of this hierarchical
    relationship (Wang et al., [2020a](#bib.bib156); Fabbri et al., [2019](#bib.bib44);
    Liu et al., [2018](#bib.bib94)). Taking advantage of hierarchical relations among
    documents instead of simply flat concatenating articles facilitates the MDS model
    to obtain representation with built-in hierarchical information, which in turn
    improves the effectiveness of the models. The input documents within a cluster
    describe a similar topic logically and semantically. Figure [4](#S2.F4 "Figure
    4 ‣ 2.2\. Differences between SDS and MDS ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")
    illustrates two representative methods of hierarchical concatenation. Existing
    hierarchical concatenation methods either perform document-level condensing in
    a cluster separately (Amplayo and Lapata, [2021](#bib.bib4)) or process documents
    in word/sentence-level inside document cluster (Nayeem et al., [2018](#bib.bib112);
    Antognini and Faltings, [2019](#bib.bib6); Wang et al., [2020a](#bib.bib156)).
    In Figure [4](#S2.F4 "Figure 4 ‣ 2.2\. Differences between SDS and MDS ‣ 2\. From
    Single to Multi-document Summarization ‣ Multi-document Summarization via Deep
    Learning Techniques: A Survey")(a), the extractive or abstractive summaries, or
    representation from the input documents are fused in the subsequent processes
    for final summary generation. The models using document-level concatenation methods
    are usually two-stage models. In Figure [4](#S2.F4 "Figure 4 ‣ 2.2\. Differences
    between SDS and MDS ‣ 2\. From Single to Multi-document Summarization ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(b), sentences in the documents
    can be replaced by words. For the word or sentence-level concatenation methods,
    clustering algorithms and graph-based techniques are the most commonly used methods.
    Clustering methods could help MDS models decrease redundancy and increase the
    information coverage for the generated summaries (Nayeem et al., [2018](#bib.bib112)).
    Sentence relation graph is able to model hierarchical relations among multi-documents
    as well (Antognini and Faltings, [2019](#bib.bib6); Yasunaga et al., [2019](#bib.bib167),
    [2017](#bib.bib168)). Most of the graph construction methods utilize sentences
    as vertexes and the edge between two sentences indicates their sentence-level
    relations (Antognini and Faltings, [2019](#bib.bib6)). Cosine similarity graph
    (Erkan and Radev, [2004](#bib.bib41)), discourse graph (Christensen et al., [2013](#bib.bib30);
    Yasunaga et al., [2017](#bib.bib168); Liu and Lapata, [2019](#bib.bib95)), semantic
    graph (Pasunuru et al., [2021b](#bib.bib123)) and heterogeneous graph (Wang et al.,
    [2020a](#bib.bib156)) can be used for building sentence graph structures. These
    graph structure could all serve as external knowledge to improve the performance
    of MDS models.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '层次化连接能够保持跨文档的关系。然而，许多现有的深度学习方法并未充分利用这种层次关系（Wang et al., [2020a](#bib.bib156);
    Fabbri et al., [2019](#bib.bib44); Liu et al., [2018](#bib.bib94)）。利用文档之间的层次关系而非简单的平坦连接文章，有助于MDS模型获得内置层次信息的表示，从而提高模型的有效性。一个集群中的输入文档在逻辑上和语义上描述了相似的主题。图[4](#S2.F4
    "Figure 4 ‣ 2.2\. Differences between SDS and MDS ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")展示了层次化连接的两个代表性方法。现有的层次化连接方法要么在集群中分别进行文档级的凝练（Amplayo
    and Lapata, [2021](#bib.bib4)），要么在文档集群内处理词语/句子级别的文档（Nayeem et al., [2018](#bib.bib112);
    Antognini and Faltings, [2019](#bib.bib6); Wang et al., [2020a](#bib.bib156)）。在图[4](#S2.F4
    "Figure 4 ‣ 2.2\. Differences between SDS and MDS ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")(a)中，来自输入文档的抽取式或生成式摘要，或表示会在后续过程中融合以生成最终摘要。使用文档级连接方法的模型通常是两阶段模型。在图[4](#S2.F4
    "Figure 4 ‣ 2.2\. Differences between SDS and MDS ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")(b)中，文档中的句子可以被词语替代。对于词语或句子级连接方法，聚类算法和基于图的技术是最常用的方法。聚类方法可以帮助MDS模型减少冗余，提高生成摘要的信息覆盖率（Nayeem
    et al., [2018](#bib.bib112)）。句子关系图能够建模多文档之间的层次关系（Antognini and Faltings, [2019](#bib.bib6);
    Yasunaga et al., [2019](#bib.bib167), [2017](#bib.bib168)）。大多数图构建方法利用句子作为顶点，两个句子之间的边表示它们的句子级关系（Antognini
    and Faltings, [2019](#bib.bib6)）。余弦相似度图（Erkan and Radev, [2004](#bib.bib41)）、话语图（Christensen
    et al., [2013](#bib.bib30); Yasunaga et al., [2017](#bib.bib168); Liu and Lapata,
    [2019](#bib.bib95)）、语义图（Pasunuru et al., [2021b](#bib.bib123)）和异构图（Wang et al.,
    [2020a](#bib.bib156)）可以用于构建句子图结构。这些图结构都可以作为外部知识来提高MDS模型的性能。'
- en: 'In addition to capture cross-document relation, hybrid summarization models
    can also be used to capture complex documents semantically, as well as to fuse
    disparate features that are more commonly adopted by MDS tasks. These models usually
    process data in two stages: extractive-abstractive and abstractive-abstractive
    (the right part of Figure [3](#S2.F3 "Figure 3 ‣ 2\. From Single to Multi-document
    Summarization ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")).
    The two-stage models try to gather important information from source documents
    with extractive or abstractive methods at the first stage, to significantly reduce
    the length of documents. In the second stage, the processed texts are fed into
    an abstractive model to form final summaries (Amplayo and Lapata, [2021](#bib.bib4);
    Lebanoff et al., [2019](#bib.bib83); Liu et al., [2018](#bib.bib94); Liu and Lapata,
    [2019](#bib.bib95); Li et al., [2020a](#bib.bib91)).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '除了捕捉文档间的关系，混合摘要模型还可以用于语义上捕捉复杂文档，以及融合在MDS任务中更常见的不同特征。这些模型通常分两个阶段处理数据：提取式-抽象式和抽象式-抽象式（见图[3](#S2.F3
    "Figure 3 ‣ 2\. From Single to Multi-document Summarization ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")的右侧部分）。两个阶段的模型尝试在第一阶段通过提取式或抽象式方法从源文档中收集重要信息，从而显著减少文档的长度。在第二阶段，处理过的文本被输入到一个抽象式模型中以形成最终的摘要（Amplayo和Lapata，[2021](#bib.bib4)；Lebanoff等，[2019](#bib.bib83)；Liu等，[2018](#bib.bib94)；Liu和Lapata，[2019](#bib.bib95)；Li等，[2020a](#bib.bib91)）。'
- en: Furthermore, conflict, duplication, and complementarity among multiple source
    documents require MDS models to have stronger abilities to handle complex information.
    However, applying the SDS model directly on MDS tasks is difficult to handle much
    higher redundancy (Mao et al., [2020](#bib.bib103)). Therefore, the MDS models
    are required not only to generate coherent and complete summary but also more
    sophisticated algorithms to identify and cope with redundancy and contradictions
    across documents ensuring that the final summary should be complete in itself.
    MDS also involves larger searching spaces but has smaller-scale training data
    than SDS, which sets obstacles for deep learning based models to learn adequate
    representation (Mao et al., [2020](#bib.bib103)). In addition, there are no specific
    evaluation metrics designed for MDS; however, existing SDS evaluation metrics
    can not evaluate the relationship between the generated abstract and different
    input documents well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，多源文档之间的冲突、重复和互补性要求MDS模型具备更强的处理复杂信息的能力。然而，直接将SDS模型应用于MDS任务难以处理更高的冗余度（Mao等，[2020](#bib.bib103)）。因此，MDS模型不仅需要生成连贯且完整的摘要，还需要更复杂的算法来识别和应对文档间的冗余和矛盾，以确保最终的摘要本身是完整的。MDS还涉及更大的搜索空间，但其训练数据规模小于SDS，这给基于深度学习的模型学习充分的表示带来了障碍（Mao等，[2020](#bib.bib103)）。此外，目前没有专门针对MDS设计的评估指标；然而，现有的SDS评估指标无法有效评估生成的摘要与不同输入文档之间的关系。
- en: 3\. Deep Learning Based Multi-document Summarization Methods
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 基于深度学习的多文档摘要方法
- en: Deep neural network (DNN) models learn multiple levels of representation and
    abstraction from input data and can fit data in a variety of research fields,
    such as computer vision (Krizhevsky et al., [2012](#bib.bib79)) and natural language
    process (Devlin et al., [2014](#bib.bib36)). Deep learning algorithms replace
    manual feature engineering by learning distinctive features through back-propagation
    to minimize a given objective function. It is well known that linear solvable
    problems possess many advantages, such as being easily solved and having numerous
    theoretically proven supports; however, many NLP tasks are highly non-linear.
    As theoretically proven by Hornik et al. (Hornik et al., [1989](#bib.bib63)),
    neural networks can fit any given continuous function as a universal approximator.
    For the MDS tasks, DNNs also perform considerably better than traditional methods
    to effectively process large-scale documents and distill informative summaries
    due to their strong fitting abilities. In this section, we first introduce our
    novel taxonomy that generalizes nine neural network design strategies (Section
    3.1). We then present the state-of-the-art DNN based MDS models according to the
    main neural network architecture they adopt (Section 3.2 – 3.7), before finishing
    with a brief introduction to MDS variant tasks (Section 3.8).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）模型从输入数据中学习多层次的表示和抽象，并可以适应各种研究领域的数据，如计算机视觉（Krizhevsky 等， [2012](#bib.bib79)）和自然语言处理（Devlin
    等， [2014](#bib.bib36)）。深度学习算法通过反向传播学习独特特征，以最小化给定的目标函数，从而替代了手动特征工程。众所周知，线性可解问题具有许多优点，如容易解决和具有众多理论支持；然而，许多
    NLP 任务是高度非线性的。正如 Hornik 等人（Hornik 等， [1989](#bib.bib63)）理论证明的那样，神经网络可以作为通用近似器拟合任何给定的连续函数。对于
    MDS 任务，DNN 由于其强大的拟合能力，也比传统方法表现更好，能有效处理大规模文档并提炼出有用的摘要。在本节中，我们首先介绍我们新颖的分类法，该分类法概括了九种神经网络设计策略（第
    3.1 节）。然后，我们根据它们采用的主要神经网络架构介绍最先进的基于 DNN 的 MDS 模型（第 3.2 节至第 3.7 节），最后简要介绍 MDS 变体任务（第
    3.8 节）。
- en: 3.1\. Architecture Design Strategies
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 架构设计策略
- en: 'Architecture design strategies play a critical role in deep learning based
    models, and many architectures have been applied to variants MDS tasks. Here,
    we have generalized the network architectures and summarize them into nine types
    based on how they generate or fuse semantic-rich and syntactic-rich representation
    to improve MDS model performance (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture
    Design Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")); these
    different architectures can also be used as basic structures or stacked on each
    other to obtain more diverse design strategies. In Figure [5](#S3.F5 "Figure 5
    ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning Based Multi-document
    Summarization Methods ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey"), deep neural models are in green boxes, and can be flexibly substituted
    with other backbone networks. The blue boxes indicate the neural embeddings processed
    by neural networks or heuristic-designed approaches, e.g., ”sentence/document”
    or ”other” representation. The explanation of each sub-figure is listed as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 架构设计策略在基于深度学习的模型中发挥了关键作用，许多架构已被应用于各种 MDS 任务。在这里，我们对网络架构进行了概括，并根据它们如何生成或融合语义丰富和句法丰富的表示来总结为九种类型，以提高
    MDS 模型的性能（图 [5](#S3.F5 "图 5 ‣ 3.1\. 架构设计策略 ‣ 3\. 基于深度学习的多文档摘要方法 ‣ 基于深度学习技术的多文档摘要：综述")）；这些不同的架构也可以作为基本结构或相互堆叠，以获得更多样化的设计策略。在图
    [5](#S3.F5 "图 5 ‣ 3.1\. 架构设计策略 ‣ 3\. 基于深度学习的多文档摘要方法 ‣ 基于深度学习技术的多文档摘要：综述") 中，深度神经模型以绿色框表示，可以灵活地用其他主干网络替代。蓝色框表示由神经网络或启发式设计方法处理的神经嵌入，例如，"句子/文档"或"其他"表示。每个子图的解释如下：
- en: '![Refer to caption](img/f45f5fb97a84c64cc6d1b42a3348142b.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f45f5fb97a84c64cc6d1b42a3348142b.png)'
- en: Figure 5\. Network Design Strategies.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 网络设计策略。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Naive Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies
    ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(a)). Multiple concatenated
    documents are input through DNN based models to extract features. Word-level,
    sentence-level or document-level representation is used to generate the downstream
    summary or select sentences. Naive networks represent the most naive model that
    lays the foundation for other strategies.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '朴素网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(a)）通过基于深度神经网络（DNN）的模型输入多个串联的文档以提取特征。使用词级、句级或文档级表示来生成下游摘要或选择句子。朴素网络代表了最简单的模型，为其他策略奠定了基础。'
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ensemble Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(b)). Ensemble based methods
    leverage multiple learning algorithms to obtain better performance than individual
    algorithms. To capture semantic-rich and syntactic-rich representation, Ensemble
    networks feed input documents to multiple paths with different network structures
    or operations. Later on, the representation from different networks is fused to
    enhance model expression capability. The majority vote or the average score can
    be used to determine the final output.'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '集成网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(b)）。集成方法利用多种学习算法来获得比单一算法更好的性能。为了捕捉语义丰富和句法丰富的表示，集成网络将输入文档馈送到具有不同网络结构或操作的多个路径中。随后，来自不同网络的表示被融合以增强模型的表达能力。多数投票或平均得分可以用于确定最终输出。'
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Auxiliary Task Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(c)) employ different tasks
    in the summarization models, where text classification, text reconstruction or
    other auxiliary tasks serve as complementary representation learners to obtain
    advanced features. Meanwhile, auxiliary task networks also provide researchers
    with a solution to use appropriate data from other tasks. In this strategy, parameters
    sharing scheme are used for jointly optimizing different tasks.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '辅助任务网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\.
    Deep Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(c)）在摘要模型中采用不同的任务，其中文本分类、文本重建或其他辅助任务作为补充表示学习器来获取高级特征。同时，辅助任务网络还为研究人员提供了一种利用其他任务中的适当数据的解决方案。在这种策略中，使用参数共享方案来共同优化不同的任务。'
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reconstruction Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(d)) optimize models from
    an unsupervised learning paradigm, which allows summarization models to overcome
    the limitation of insufficient annotated golden summaries. The use of such a paradigm
    enables generated summaries to be constrained in the natural language domain in
    a good manner.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '重建网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(d)）从无监督学习范式中优化模型，这使得摘要模型能够克服注释黄金摘要不足的限制。使用这种范式使生成的摘要在自然语言领域内得到良好约束。'
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fusion Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies
    ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(e)) fuse representation
    generated from neural networks and hand-crafted features. These hand-crafted features
    contain adequate prior knowledge that facilitates the optimization of summarization
    models.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '融合网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(e)）融合了神经网络生成的表示和手工特征。这些手工特征包含了足够的先验知识，有助于优化摘要模型。'
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Neural Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(f)). This strategy captures
    cross-document relations, crucial and beneficial for multi-document model training,
    by constructing graph structures based on the source documents, including word,
    sentence, or document-level information.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图神经网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(f)）。该策略通过基于源文档构建图结构，包括词汇、句子或文档级别的信息，从而捕捉文档间的关系，这对于多文档模型训练至关重要且有益。'
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Encoder-Decoder Structure (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture
    Design Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")(g)). The
    encoder embeds source documents into the hidden representation, i.e., word, sentence
    and document representation. This representation, containing compressed semantic
    and syntactic information, is passed to the decoder which processes the latent
    embeddings to synthesize local and global semantic/syntactic information to produce
    the final summaries.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '编码器-解码器结构（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\.
    Deep Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(g)）。编码器将源文档嵌入到隐藏表示中，即词汇、句子和文档表示。这种表示包含压缩的语义和句法信息，传递给解码器，解码器处理潜在的嵌入来综合局部和全局的语义/句法信息，以生成最终的摘要。'
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pre-trained Language Models (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture
    Design Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")(h)) obtain
    contextualized text representation by predicting words or phrases based on their
    context using large amounts of the corpus, which can be further fine-tuned for
    downstream task adaption (Dong et al., [2019](#bib.bib37)). The models can fine-tune
    with randomly initialized decoders in an end-to-end fashion since transfer learning
    can assist the model training process (Li et al., [2020a](#bib.bib91)).'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '预训练语言模型（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\.
    Deep Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(h)）通过使用大量语料库预测词汇或短语来获得上下文文本表示，这些模型可以进一步微调以适应下游任务（Dong
    et al., [2019](#bib.bib37)）。由于迁移学习可以帮助模型训练过程，这些模型可以通过随机初始化的解码器以端到端的方式进行微调（Li et
    al., [2020a](#bib.bib91)）。'
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hierarchical Networks (Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")(i)). Multiple documents
    are concatenated as inputs to feed into the first DNN based model to capture low-level
    representation. Another DNN based model is cascaded to generate high-level representation
    based on the previous ones. The hierarchical networks empower the model with the
    ability to capture abstract-level and semantic-level features more efficiently.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '层次网络（图 [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep
    Learning Based Multi-document Summarization Methods ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey")(i)）。多个文档被串联作为输入，输入到第一个基于DNN的模型中以捕捉低级表示。另一个基于DNN的模型级联生成基于前者的高级表示。层次网络使模型能够更高效地捕捉抽象级别和语义级别的特征。'
- en: 3.2\. Recurrent Neural Networks based Models
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 基于递归神经网络的模型
- en: 'Recurrent Neural Networks (RNNs) (Rumelhart et al., [1986](#bib.bib134)) excel
    in modeling sequential data by capturing sequential relations and syntactic/semantic
    information from word sequences. In RNN models, neurons are connected through
    hidden layers and unlike other neural network structures, the inputs of each RNN
    neuron come not only from the word or sentence embedding but also from the output
    of the previous hidden state. Despite being powerful, vanilla RNN models often
    encounter gradient explosion or vanishing issues, so a large number of RNN-variants
    have been proposed. The most prevalent ones are Long Short-Term Memory (LSTM)
    (Hochreiter and Schmidhuber, [1997](#bib.bib62)), Gated Recurrent Unit (GRU) (Chung
    et al., [2014](#bib.bib32)) and Bi-directional Long Short-Term Memory (Bi-LSTM)
    (Huang et al., [2015](#bib.bib65)). The DNN based Model in Figure [5](#S3.F5 "Figure
    5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning Based Multi-document
    Summarization Methods ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey") can be replaced with RNN based models to design models.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '循环神经网络（RNNs）（Rumelhart et al., [1986](#bib.bib134)）擅长通过捕捉序列关系和词序列中的句法/语义信息来建模顺序数据。在RNN模型中，神经元通过隐藏层连接，与其他神经网络结构不同，每个RNN神经元的输入不仅来自词语或句子嵌入，还来自前一个隐藏状态的输出。尽管功能强大，传统的RNN模型通常会遇到梯度爆炸或消失的问题，因此提出了大量的RNN变体。最常见的有长短期记忆（LSTM）（Hochreiter
    and Schmidhuber, [1997](#bib.bib62)）、门控递归单元（GRU）（Chung et al., [2014](#bib.bib32)）和双向长短期记忆（Bi-LSTM）（Huang
    et al., [2015](#bib.bib65)）。图[5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design
    Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey")中的基于DNN的模型可以被替换为基于RNN的模型进行设计。'
- en: RNN based models have been used in MDS tasks since 2015\. Cao et al. (Cao et al.,
    [2015a](#bib.bib20)) proposed an RNN-based model termed Ranking framework upon
    Recursive Neural Networks (R2N2), which leverages manually extracted words and
    sentence-level features as inputs. This model transfers the sentence ranking task
    into a hierarchical regression process, which measures the importance of sentences
    and constituents in the parsing tree. Zheng et al. (Zheng et al., [2019](#bib.bib182))
    used a hierarchical RNN structure to utilize the subtopic information by extracting
    not only sentence and document embeddings, but also topic embeddings. In this
    SubTopic-Driven Summarization (STDS) model, the readers’ comments are seen as
    auxiliary documents and the model employs soft clustering to incorporate comment
    and sentence representation for further obtaining subtopic representation. Arthur
    et al. (Bražinskas et al., [2019](#bib.bib16)) introduced a GRU-based encoder-decoder
    architecture to minimize the diversity of opinions reflecting the dominant views
    while generating multi-review summaries. Mao et al. (Mao et al., [2020](#bib.bib103))
    proposed a maximal margin relevance guided reinforcement learning framework (RL-MMR)
    to incorporate the advantages of neural sequence learning and statistical measures.
    The proposed soft attention for learning adequate representation allows more exploration
    of search space.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 自2015年以来，基于RNN的模型已被应用于MDS任务。Cao等人（Cao et al., [2015a](#bib.bib20)）提出了一种基于RNN的模型，称为基于递归神经网络的排序框架（R2N2），该模型利用手动提取的词语和句子级特征作为输入。该模型将句子排序任务转化为层次回归过程，以衡量句子和解析树中成分的重要性。Zheng等人（Zheng
    et al., [2019](#bib.bib182)）使用了一个层次RNN结构，通过提取句子和文档嵌入以及主题嵌入来利用子主题信息。在这一子主题驱动摘要（STDS）模型中，读者评论被视为辅助文档，模型采用软聚类将评论和句子表示结合起来，以进一步获取子主题表示。Arthur等人（Bražinskas
    et al., [2019](#bib.bib16)）介绍了一种基于GRU的编码器-解码器架构，用于在生成多评论摘要时最小化反映主流观点的意见多样性。Mao等人（Mao
    et al., [2020](#bib.bib103)）提出了一种最大边际相关引导的强化学习框架（RL-MMR），以结合神经序列学习和统计测量的优点。所提出的软注意力用于学习充分的表示，使搜索空间的探索更多。
- en: To leverage the advantage of hybrid summarization model, Reinald et al. (Amplayo
    and Lapata, [2021](#bib.bib4)) proposed a two-stage framework, viewing opinion
    summarization as an instance of multi-source transduction to distill salient information
    from source documents. The first stage of the model leverages a Bi-LSTM auto-encoder
    to learn word and document-level representation; the second stage fuses multi-source
    representation and generates an opinion summary with a simple LSTM decoder combined
    with a vanilla attention mechanism (Bahdanau et al., [2015](#bib.bib9)) and a
    copy mechanism (Vinyals et al., [2015](#bib.bib152)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用混合总结模型的优势，Reinald 等（Amplayo 和 Lapata， [2021](#bib.bib4)）提出了一个两阶段框架，将观点总结视为多源传导的一个实例，以从源文档中提取显著信息。模型的第一阶段利用
    Bi-LSTM 自编码器学习词级和文档级表示；第二阶段融合多源表示，并使用简单的 LSTM 解码器结合基本的注意机制（Bahdanau 等， [2015](#bib.bib9)）和复制机制（Vinyals
    等， [2015](#bib.bib152)）生成观点总结。
- en: Since paired MDS datasets are rare and hard to obtain, Li et al. (Li et al.,
    [2017b](#bib.bib90)) developed a RNN-based framework to extract salient information
    vectors from sentences in input documents in an unsupervised manner. Cascaded
    attention retains the most relevant embeddings to reconstruct the original input
    sentence vectors. During the reconstruction process, the proposed model leverages
    a sparsity constraint to penalize trivial information in the output vectors. Also,
    Chu et al. (Chu and Liu, [2019](#bib.bib31)) proposed an unsupervised end-to-end
    abstractive summarization architecture called MeanSum. This LSTM-based model formalizes
    product or business reviews summarization problem into two individual closed-loops.
    Inspired by MeanSum, Coavoux et al. (Coavoux et al., [2019](#bib.bib33)) used
    a two-layer standard LSTM to construct sentence representation for aspect-based
    multi-document abstractive summarization, and discovered that the clustering strategy
    empowers the model to reward review diversity and handle contradictory ones.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于配对 MDS 数据集稀缺且难以获得，Li 等（Li 等， [2017b](#bib.bib90)）开发了一个基于 RNN 的框架，以无监督的方式从输入文档中的句子中提取显著信息向量。级联注意机制保留了最相关的嵌入以重构原始输入句子向量。在重构过程中，所提出的模型利用稀疏性约束来惩罚输出向量中的琐碎信息。此外，Chu
    等（Chu 和 Liu， [2019](#bib.bib31)）提出了一种无监督的端到端抽象总结架构，称为 MeanSum。该基于 LSTM 的模型将产品或商业评论总结问题形式化为两个独立的闭环。受到
    MeanSum 的启发，Coavoux 等（Coavoux 等， [2019](#bib.bib33)）使用了一个两层标准 LSTM 来构建基于方面的多文档抽象总结的句子表示，并发现聚类策略使模型能够奖励评论的多样性并处理矛盾的评论。
- en: 3.3\. Convolutional Neural Networks Based Models
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 基于卷积神经网络的模型
- en: 'Convolutional neural networks (CNNs) (LeCun et al., [1998](#bib.bib85)) achieve
    excellent results in computer vision tasks. The convolution operation scans through
    the word/sentence embeddings and uses convolution kernels to extract important
    information from input data objects. Using a pooling operation at intervals can
    return simple to complex feature levels. CNNs have been proven to be effective
    for various NLP tasks in recent years (Kim, [2014](#bib.bib75); Dos Santos and
    Gatti, [2014](#bib.bib38)) as they can process natural language after sentence/word
    vectorization. Most of the CNN based MDS models use CNNs for semantic and syntactic
    feature representation. As with RNN, CNN-based models can also replace DNN-based
    models in network design strategies (Please refer to Figure [5](#S3.F5 "Figure
    5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning Based Multi-document
    Summarization Methods ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey")).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '卷积神经网络（CNNs）（LeCun 等， [1998](#bib.bib85)）在计算机视觉任务中取得了优异的结果。卷积操作扫描词语/句子嵌入，并使用卷积核从输入数据对象中提取重要信息。通过间歇性地使用池化操作，可以返回从简单到复杂的特征层次。近年来，CNN
    已被证明在各种 NLP 任务中有效（Kim， [2014](#bib.bib75)；Dos Santos 和 Gatti， [2014](#bib.bib38)），因为它们可以在句子/词向量化后处理自然语言。大多数基于
    CNN 的 MDS 模型使用 CNN 进行语义和句法特征表示。与 RNN 类似，基于 CNN 的模型也可以在网络设计策略中替代基于 DNN 的模型（请参阅图
    [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning
    Based Multi-document Summarization Methods ‣ Multi-document Summarization via
    Deep Learning Techniques: A Survey")）。'
- en: A simple way to use CNNs in MDS is by sliding multiple filters with different
    window sizes over the input documents for semantic representation. Cao et al.
    (Cao et al., [2015b](#bib.bib21)) proposed a hybrid CNN-based model PriorSum to
    capture latent document representation. The proposed representation learner slides
    over the input documents with filters of different window widths and two-layer
    max-over-time pooling operations (Collobert et al., [2011](#bib.bib34)) to fetch
    document-independent features that are more informative than using standard CNNs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDS中使用CNN的一种简单方法是通过在输入文档上滑动多个不同窗口大小的滤波器来进行语义表示。曹等（曹等，[2015b](#bib.bib21)）提出了一种基于混合CNN的模型PriorSum来捕捉潜在的文档表示。所提议的表示学习器在输入文档上滑动，使用不同窗口宽度的滤波器和两层最大时间池化操作（Collobert
    et al., [2011](#bib.bib34)）以提取比使用标准CNN更具信息性的文档独立特征。
- en: Similarly, HNet (Singh et al., [2018](#bib.bib142)) uses distinct CNN filters
    and max-over-time-pooling to generate salient feature representation for downstream
    processes. Cho et al. (Cho et al., [2019](#bib.bib29)) also used different filter
    sizes in DPP-combined model to extract low-level features. Yin et al. (Yin and
    Pei, [2015](#bib.bib169)) presented an unsupervised CNN-based model termed Novel
    Neural Language Model (NNLM) to extract sentence representation and diminish the
    redundancy of sentence selection. The NNLM framework contains only one convolution
    layer and one max-pooling layer, and both element-wise averaging sentence representation
    and context words representation are used to predict the next word. For aspect-based
    opinion summarization, Stefanos et al. (Angelidis and Lapata, [2018](#bib.bib5))
    leveraged a CNN based model to encode the product reviews which contain a set
    of segments for opinion polarity.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，HNet（Singh et al., [2018](#bib.bib142)）使用不同的CNN滤波器和最大时间池化来生成用于下游处理的显著特征表示。Cho等（Cho
    et al., [2019](#bib.bib29)）也在DPP组合模型中使用了不同的滤波器尺寸来提取低级特征。Yin等（Yin and Pei, [2015](#bib.bib169)）提出了一种无监督的基于CNN的模型，称为新颖神经语言模型（NNLM），用于提取句子表示并减少句子选择的冗余。NNLM框架仅包含一层卷积层和一层最大池化层，同时使用逐元素平均句子表示和上下文词表示来预测下一个词。对于基于方面的意见摘要，Stefanos等（Angelidis
    and Lapata, [2018](#bib.bib5)）利用基于CNN的模型来编码包含一组意见极性的产品评论。
- en: People with different background knowledge and understanding can produce different
    summaries of the same documents. To account for this variability, Zhang et al.
    (Zhang et al., [2016](#bib.bib179)) suggested a MV-CNN model that ensembles three
    individual models to incorporate multi-view learning and CNNs to improve the performance
    of MDS. In this work, three CNNs with dual-convolutional layers used multiple
    filters with different window sizes to extract distinct saliency scores of sentences.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 具有不同背景知识和理解的人可能会产生对同一文档的不同总结。为了考虑这种变异性，张等（张等，[2016](#bib.bib179)）提出了一种MV-CNN模型，该模型集成了三个独立模型，以结合多视角学习和CNN来提高MDS的性能。在这项工作中，三个具有双卷积层的CNN使用了不同窗口大小的多个滤波器，以提取句子的不同显著性分数。
- en: To overcome the MDS bottlenecks of insufficient training data, Cao et al. (Cao
    et al., [2017](#bib.bib19)) developed a TCSum model incorporating an auxiliary
    text classification sub-task into MDS to introduce more supervision signals. The
    text classification model uses a CNN descriptor to project documents onto the
    distributed representation, and to classify input documents into different categories.
    The summarization model shares the projected sentence embedding from the classification
    model, and the TCSum model then chooses the corresponding category based transformation
    matrices according to classification results to transform the sentence embedding
    into the summary embedding.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服训练数据不足的MDS瓶颈，曹等（曹等，[2017](#bib.bib19)）开发了一个TCSum模型，将一个辅助文本分类子任务纳入MDS中，以引入更多的监督信号。文本分类模型使用CNN描述符将文档映射到分布式表示，并将输入文档分类到不同的类别中。总结模型共享来自分类模型的投影句子嵌入，TCSum模型根据分类结果选择相应的类别转换矩阵，将句子嵌入转换为摘要嵌入。
- en: Unlike RNNs that support the processing of long time-serial signals, a naive
    CNN layer struggles to capture long-distance relations while processing sequential
    data due to the limitation of the fixed-sized convolutional kernels, each of which
    has a specific receptive field size. Nevertheless, CNN based models can increase
    their receptive fields through formation of hierarchical structures to calculate
    sequential data in a parallel manner. Because of this highly parallelizable characteristic,
    training of CNN-based summarization models is more efficient than for RNN-based
    models. However, summarizing lengthy input articles is still a challenging task
    for CNN based models because they are not skilled in modeling non-local relationships.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 与支持处理长时间序列信号的 RNN 不同，天真的 CNN 层由于固定大小的卷积核的限制，难以在处理序列数据时捕捉长距离关系，因为每个卷积核都有特定的感受野大小。然而，基于
    CNN 的模型可以通过形成层级结构来增加其感受野，从而以并行的方式计算序列数据。由于这种高度可并行化的特性，基于 CNN 的总结模型的训练效率比基于 RNN
    的模型更高。然而，对于 CNN 基础的模型来说，总结长篇输入文章仍然是一个具有挑战性的任务，因为它们不擅长建模非局部关系。
- en: 3.4\. Graph Neural Networks Based Models
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 基于图神经网络的模型
- en: 'CNNs have been successfully applied to many computer vision tasks to extract
    distinguished image features from the Euclidean space, but struggle when processing
    non-Euclidean data. Natural language data consist of vocabularies and phrases
    with strong relations which can be better represented with graphs than with sequential
    orders. Graph neural networks (GNNs, Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Architecture
    Design Strategies ‣ 3\. Deep Learning Based Multi-document Summarization Methods
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey") (f)) are
    composed of an ideal architecture for NLP since they can model strong relations
    between entities semantically and syntactically. Graph convolution networks (GCNs)
    and graph attention networks (GANs) are the most commonly adopted GNNs because
    of their efficiency and simplicity for integration with other neural networks.
    These models first build a relation graph based on input documents, where nodes
    can be words, sentences or documents, and edges capture the similarity among them.
    At the same time, input documents are fed into a DNN based model to generate embeddings
    at different levels. The GNNs are then built over the top to capture salient contextual
    information. Table [1](#S3.T1 "Table 1 ‣ 3.4\. Graph Neural Networks Based Models
    ‣ 3\. Deep Learning Based Multi-document Summarization Methods ‣ Multi-document
    Summarization via Deep Learning Techniques: A Survey") describes the current GNN
    based models used for MDS with details of nodes, edges, edge weights, and applied
    GNN methods.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 已成功应用于许多计算机视觉任务，从欧几里得空间中提取出显著的图像特征，但在处理非欧几里得数据时表现不佳。自然语言数据由具有强关系的词汇和短语组成，这些关系用图比用序列顺序表示更好。图神经网络（GNNs，图
    [5](#S3.F5 "图 5 ‣ 3.1\. 架构设计策略 ‣ 3\. 基于深度学习的多文档总结方法 ‣ 基于深度学习技术的多文档总结：综述") (f)）构成了理想的
    NLP 架构，因为它们可以在语义和句法上建模实体之间的强关系。图卷积网络（GCNs）和图注意力网络（GANs）是最常用的 GNN，因为它们在与其他神经网络的集成中高效且简单。这些模型首先基于输入文档构建关系图，其中节点可以是单词、句子或文档，边则捕捉它们之间的相似性。同时，输入文档被输入到基于
    DNN 的模型中，以生成不同层次的嵌入。然后，在其顶部构建 GNN，以捕捉显著的上下文信息。表 [1](#S3.T1 "表 1 ‣ 3.4\. 基于图神经网络的模型
    ‣ 3\. 基于深度学习的多文档总结方法 ‣ 基于深度学习技术的多文档总结：综述") 描述了当前用于 MDS 的 GNN 基础模型，包括节点、边、边权重和应用的
    GNN 方法的详细信息。
- en: Table 1\. Multi-document Summarization Models based on Graph Neural Networks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 基于图神经网络的多文档总结模型。
- en: '| Models | Nodes | Edges | Edge Weights | GNN Methods |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 节点 | 边 | 边权重 | GNN 方法 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HeterDoc- &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HeterDoc- &#124;'
- en: '&#124; SumGraph (Wang et al., [2020a](#bib.bib156)) &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SumGraph (Wang et al., [2020a](#bib.bib156)) &#124;'
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; word, sentence, &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单词，句子，&#124;'
- en: '&#124; document &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文档 &#124;'
- en: '|'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; word-sentence, &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单词-句子，&#124;'
- en: '&#124; word-document &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单词-文档 &#124;'
- en: '| TF-IDF |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| TF-IDF |'
- en: '&#124; Graph Attention &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图注意力 &#124;'
- en: '&#124; Networks &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络 &#124;'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Graph-based &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于图的 &#124;'
- en: '&#124; Neural MDS (Yasunaga et al., [2017](#bib.bib168)) &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 神经 MDS (Yasunaga et al., [2017](#bib.bib168)) &#124;'
- en: '| sentence | sentence-sentence |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 句子 | 句子-句子 |'
- en: '&#124; Personalized &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 个性化 &#124;'
- en: '&#124; Discourse Graph &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 话语图 &#124;'
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Graph Convolutional &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图卷积 &#124;'
- en: '&#124; Networks &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络 &#124;'
- en: '|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SemSentSum (Antognini and Faltings, [2019](#bib.bib6)) | sentence | sentence-sentence
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| SemSentSum (Antognini and Faltings, [2019](#bib.bib6)) | 句子 | 句子-句子 |'
- en: '&#124; Cosine Similarity Graph &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 余弦相似度图 &#124;'
- en: '&#124; Edge Removal Method &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边缘移除方法 &#124;'
- en: '|'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Graph Convolutional &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图卷积 &#124;'
- en: '&#124; Networks &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络 &#124;'
- en: '|'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ScisummNet (Yasunaga et al., [2019](#bib.bib167)) | sentence | sentence-sentence
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| ScisummNet (Yasunaga et al., [2019](#bib.bib167)) | 句子 | 句子-句子 |'
- en: '&#124; Cosine Similarity Graph &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 余弦相似度图 &#124;'
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Graph Convolutional &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图卷积 &#124;'
- en: '&#124; Networks &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络 &#124;'
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Yasunage et al. (Yasunaga et al., [2017](#bib.bib168)) developed a GCN based
    extractive model to capture the relations between sentences. This model first
    builds a sentence-based graph and then feeds the pre-processed data into a GCN
    (Kipf and Welling, [2017](#bib.bib76)) to capture sentence-wise related features.
    Defined by the model, each sentence is regarded as a node and the relation between
    each pair of sentences is defined as an edge. Inside each document cluster, the
    sentence relation graph can be generated through a cosine similarity graph (Erkan
    and Radev, [2004](#bib.bib41)), approximate discourse graph (Christensen et al.,
    [2013](#bib.bib30)), and the proposed personalized discourse graph. Both the sentence
    relation graph and sentence embeddings extracted by a sentence-level RNN are fed
    into GCN to produce the final sentence representation. With the help of a document-level
    GRU, the model generates cluster embeddings to fully aggregate features between
    sentences.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Yasunage 等人（Yasunaga et al.，[2017](#bib.bib168)）开发了一种基于 GCN 的提取式模型，以捕捉句子之间的关系。该模型首先构建一个基于句子的图，然后将预处理后的数据输入到
    GCN（Kipf and Welling，[2017](#bib.bib76)）中，以捕捉句子级相关特征。模型定义了每个句子作为一个节点，并且每对句子之间的关系定义为一条边。在每个文档集群中，句子关系图可以通过余弦相似度图（Erkan
    and Radev，[2004](#bib.bib41)）、近似话语图（Christensen et al., [2013](#bib.bib30)）和提出的个性化话语图生成。通过句子级
    RNN 提取的句子关系图和句子嵌入被输入到 GCN 中，以生成最终的句子表示。在文档级 GRU 的帮助下，该模型生成集群嵌入，以完全聚合句子之间的特征。
- en: Similarly, Antognini et al. (Antognini and Faltings, [2019](#bib.bib6)) proposed
    a GCN based model named SemSentSum that constructs a graph based on sentence relations.
    In contrast to Yasunage et al. (Yasunaga et al., [2017](#bib.bib168)), this work
    leverages external universal embeddings, pre-trained on the unrelated corpus,
    to construct a sentence semantic relation graph. Additionally, an edge removal
    method has been applied to deal with the sparse graph problems emphasizing high
    sentence similarities; if the weight of the edge is lower than a given threshold,
    the edge is removed. The sentence relation graph and sentence embeddings are fed
    into a GCN (Kipf and Welling, [2017](#bib.bib76)) to generate saliency estimation
    for extractive summaries.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Antognini 等人（Antognini and Faltings，[2019](#bib.bib6)）提出了一种基于 GCN 的模型，名为
    SemSentSum，该模型基于句子关系构建图。与 Yasunage 等人（Yasunaga et al.，[2017](#bib.bib168)）的工作相比，该模型利用外部通用嵌入，这些嵌入是在不相关的语料库上预训练的，用于构建句子语义关系图。此外，还应用了一种边缘移除方法来处理稀疏图问题，重点强调句子相似度高的情况；如果边的权重低于给定阈值，则移除该边。句子关系图和句子嵌入被输入到
    GCN（Kipf and Welling，[2017](#bib.bib76)）中，以生成提取式摘要的显著性估计。
- en: Yasunage et al. (Yasunaga et al., [2019](#bib.bib167)) also designed a GCN based
    model for summarizing scientific papers. The proposed ScisummNet model uses not
    only the abstract of source scientific papers but also the relevant text from
    papers that cite the original source. The total number of citations is also incorporated
    in the model as an authority feature. A cosine similarity graph is applied to
    form the sentence relation graph, and GCNs are adopted to predict the sentence
    salience estimation from the sentence relation graph, authority scores and sentence
    embeddings.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Yasunage 等人（Yasunaga et al.，[2019](#bib.bib167)）还设计了一种用于总结科学论文的基于 GCN 的模型。提出的
    ScisummNet 模型不仅使用源科学论文的摘要，还使用引用原始来源的论文中的相关文本。总引用次数也作为权威特征纳入模型中。余弦相似度图被应用于形成句子关系图，GCN
    被用于从句子关系图、权威分数和句子嵌入中预测句子的显著性估计。
- en: Existing GNN based models focused mainly on the relationships between sentences,
    and do not fully consider the relationships between words, sentences, and documents.
    To fill this gap, Wang et al. (Wang et al., [2020a](#bib.bib156)) proposed a heterogeneous
    GAN based model, called HeterDoc-SUM Graph, that is specific for extractive MDS.
    This heterogeneous graph structure includes word, sentence, and document nodes,
    where sentence nodes and document nodes are connected according to the contained
    word nodes. Word nodes thus act as an intermediate bridge to connect the sentence
    and document nodes, and are used to better establish document-document, sentence-sentence
    and sentence-document relations. TF-IDF values are used to weight word-sentence
    and word-document edges, and the node representation of these three levels are
    passed into the graph attention networks for model update. In each iteration,
    bi-directional updating of both word-sentence and word-document relations are
    performed to better aggregate cross-level semantic knowledge.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的基于图神经网络（GNN）的模型主要关注句子之间的关系，并未充分考虑单词、句子和文档之间的关系。为填补这一空白，王等人（Wang et al., [2020a](#bib.bib156)）提出了一种异构GAN模型，称为HeterDoc-SUM
    Graph，专门用于提取式多文档摘要（MDS）。这种异构图结构包括单词、句子和文档节点，其中句子节点和文档节点根据包含的单词节点进行连接。单词节点因此充当连接句子节点和文档节点的中介桥梁，并用于更好地建立文档-文档、句子-句子和句子-文档关系。TF-IDF值被用来加权单词-句子和单词-文档的边，这三个层次的节点表示被传递到图注意力网络中进行模型更新。在每次迭代中，进行单词-句子和单词-文档关系的双向更新，以更好地聚合跨层次的语义知识。
- en: 3.5\. Pointer-generator Networks Based Models
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5. 指针生成网络模型
- en: Pointer-generator (PG) networks (See et al., [2017](#bib.bib136)) are proposed
    to overcome the problems of factual errors and high redundancy in the summarization
    tasks. This network has been inspired by Pointer Network (Vinyals et al., [2015](#bib.bib152)),
    CopyNet (Gu et al., [2016](#bib.bib56)), forced-attention sentence compression
    (Miao and Blunsom, [2016](#bib.bib105)), and coverage mechanism from machine translation
    (Tu et al., [2016](#bib.bib149)). PG networks combine sequence-to-sequence (Seq2Seq)
    model and pointer networks to obtain a united probability distribution allowing
    vocabularies to be selected from source texts or generated by machines. Additionally,
    the coverage mechanism prevents PG networks from consistently choosing the same
    phrases.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 指针生成（PG）网络（See et al., [2017](#bib.bib136)）被提出以克服摘要任务中的事实错误和高冗余问题。这种网络的灵感来自于指针网络（Vinyals
    et al., [2015](#bib.bib152)）、CopyNet（Gu et al., [2016](#bib.bib56)）、强制注意力句子压缩（Miao
    and Blunsom, [2016](#bib.bib105)）以及机器翻译中的覆盖机制（Tu et al., [2016](#bib.bib149)）。PG网络结合了序列到序列（Seq2Seq）模型和指针网络，获得了一个统一的概率分布，允许从源文本中选择词汇或由机器生成。此外，覆盖机制防止PG网络始终选择相同的短语。
- en: 'The Maximal Marginal Relevance (MMR) method is designed to select a set of
    salient sentences from source documents by considering both importance and redundancy
    indices (Carbonell and Goldstein, [1998](#bib.bib22)). The redundancy score controls
    sentence selection to minimize overlap with the existing summary. The MMR model
    adds a new sentence to the objective summary based on importance and redundancy
    scores until the summary length reaches a certain threshold. Inspired by MMR,
    Alexander et al. (Fabbri et al., [2019](#bib.bib44)) proposed an end-to-end Hierarchical
    MMR-Attention Pointer-generator (Hi-MAP) model to incorporate PG networks and
    MMR (Carbonell and Goldstein, [1998](#bib.bib22)) for abstractive MDS. The Hi-MAP
    model improves PG networks by modifying attention weights (multipling MMR scores
    by the original attention weights) to include better important sentences in, and
    filter redundant information from, the summary. Similarly, the MMR approach is
    implemented by PG-MMR model (Lebanoff et al., [2018](#bib.bib84)) to identify
    salient source sentences from multi-document inputs, albeit with a different method
    for calculating MMR scores from Hi-MAP; instead, ROUGE-L Recall and ROUGE-L Precision
    (Lin, [2004](#bib.bib92)) serve as evaluation metrics to calculate the importance
    and redundancy scores. To overcome the scarcity of MDS datasets, the PG-MMR model
    leverages a support vector regression model that is pre-trained on a SDS dataset
    to recognize the important contents. This support vector regression model also
    calculates the score of each input sentence by considering four factors: sentence
    length, sentence relative/absolute position, sentence-document similarities, and
    sentence quality obtained by a PG network. Sentences with the top-$K$ scores are
    fed into another PG network to generate a concise summary.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最大边际相关性（MMR）方法旨在通过考虑重要性和冗余指数来从源文档中选择一组突出的句子（Carbonell 和 Goldstein，[1998](#bib.bib22)）。冗余分数控制句子选择，以最小化与现有摘要的重叠。MMR
    模型根据重要性和冗余分数将新句子添加到目标摘要中，直到摘要长度达到一定阈值。受到 MMR 启发，Alexander 等人（Fabbri 等，[2019](#bib.bib44)）提出了一种端到端的层次化
    MMR-注意力指针生成器（Hi-MAP）模型，以结合 PG 网络和 MMR（Carbonell 和 Goldstein，[1998](#bib.bib22)）用于抽象
    MDS。Hi-MAP 模型通过修改注意力权重（将 MMR 分数与原始注意力权重相乘）来改进 PG 网络，以将更重要的句子纳入摘要中，并过滤冗余信息。类似地，MMR
    方法通过 PG-MMR 模型（Lebanoff 等，[2018](#bib.bib84)）实现，以从多文档输入中识别突出的源句子，尽管与 Hi-MAP 计算
    MMR 分数的方法不同；而是使用 ROUGE-L 召回率和 ROUGE-L 精确率（Lin，[2004](#bib.bib92)）作为评价指标来计算重要性和冗余分数。为了克服
    MDS 数据集的稀缺性，PG-MMR 模型利用在 SDS 数据集上预训练的支持向量回归模型来识别重要内容。该支持向量回归模型还通过考虑四个因素来计算每个输入句子的分数：句子长度、句子相对/绝对位置、句子与文档的相似性，以及通过
    PG 网络获得的句子质量。分数排名前 $K$ 的句子将输入另一个 PG 网络，以生成简洁的摘要。
- en: 3.6\. Transformer Based Models
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. 基于 Transformer 的模型
- en: 'As discussed, CNN based models are not as good at processing sequential data
    as RNN based models. However, RNN based models are not amenable to parallel computing,
    as the current states in RNN models highly depend on results from the previous
    steps. Additionally, RNNs struggle to process long sequences since former knowledge
    will fade away during the learning process. Adopting Transformer based architectures
    (Vaswani et al., [2017](#bib.bib151)) is one solution to solve these problems.
    The Transformer is based on the self-attention mechanism, has natural advantages
    for parallelization, and retains relative long-range dependencies. The Transformer
    model has achieved promising results in MDS tasks (Liu et al., [2018](#bib.bib94);
    Liu and Lapata, [2019](#bib.bib95); Li et al., [2020a](#bib.bib91); Jin et al.,
    [2020](#bib.bib72)) and can replace the DNN based Model in Figure [5](#S3.F5 "Figure
    5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning Based Multi-document
    Summarization Methods ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey"). Most of the Transformer based models follow an encoder-decoder structure.
    Transformer based models can be divided into flat Transformer, hierarchical Transformer,
    and pre-train language models.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '正如讨论的那样，基于CNN的模型在处理序列数据方面不如基于RNN的模型。然而，基于RNN的模型不适合并行计算，因为RNN模型中的当前状态高度依赖于前一步骤的结果。此外，由于在学习过程中前面的知识会逐渐消失，RNN在处理长序列时会遇到困难。采用基于Transformer的架构（Vaswani等人，[2017](#bib.bib151)）是一种解决这些问题的方案。Transformer基于自注意力机制，具有天然的并行化优势，并且保留了相对较长的依赖关系。Transformer模型在MDS任务中取得了有希望的结果（Liu等人，[2018](#bib.bib94)；Liu和Lapata，[2019](#bib.bib95)；Li等人，[2020a](#bib.bib91)；Jin等人，[2020](#bib.bib72)），并且可以替代图中的DNN基于模型[5](#S3.F5
    "Figure 5 ‣ 3.1\. Architecture Design Strategies ‣ 3\. Deep Learning Based Multi-document
    Summarization Methods ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey")。大多数基于Transformer的模型遵循编码器-解码器结构。基于Transformer的模型可以分为平坦Transformer、层次Transformer和预训练语言模型。'
- en: Flat Transformer. Liu et al. (Liu et al., [2018](#bib.bib94)) introduced Transformer
    to MDS tasks, aiming to generate a Wikipedia article from a given topic and set
    of references. The authors argue that the encoder-decoder based sequence transduction
    model cannot cope well with long input documents, so their model selects a series
    of top-$K$ tokens and feeds them into a Transformer based decoder-only sequence
    transduction model to generate Wikipedia articles. More specifically, the Transformer
    decoder-only architecture combines the result from the extractive stage and golden
    summary into a sentence for training. To obtain rich semantic representation from
    different granularity, Jin et al. (Jin et al., [2020](#bib.bib72)) proposed a
    Transformer based multi-granularity interaction network MGSum and unified extractive
    and abstractive MDS. Words, sentences and documents are considered as three granular
    levels of semantic unit connected by a granularity hierarchical relation graph.
    In the same granularity, a self-attention mechanism is used to capture the semantic
    relationships. Sentence granularity representation is employed in the extractive
    summarization, and word granularity representation is adapted to generate an abstractive
    summary. MGSum employs a fusion gate to integrate and update the semantic representation.
    Additionally, a spare attention mechanism is used to ensure the summary generator
    focus on important information. Brazinskas et al. (Brazinskas et al., [2020](#bib.bib17))
    created a precedent for few-shot learning for MDS that leverages a Transformer
    conditional language model and a plug-in network for both extractive and abstractive
    MDS to overcome rapid overfitting and poor generation problems resulting from
    naive fine-tuning of large parameter models.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**平坦 Transformer**。刘等人（Liu et al., [2018](#bib.bib94)）将 Transformer 引入 MDS
    任务，旨在从给定的主题和参考资料生成一篇 Wikipedia 文章。作者认为基于编码器-解码器的序列转换模型无法很好地处理长输入文档，因此他们的模型选择了一系列前$K$的
    tokens，并将它们输入到仅基于 Transformer 的解码器序列转换模型中，以生成 Wikipedia 文章。更具体地说，Transformer 仅解码器架构将从提取阶段得到的结果和黄金总结结合成一个句子进行训练。为了从不同粒度中获得丰富的语义表示，金等人（Jin
    et al., [2020](#bib.bib72)）提出了一种基于 Transformer 的多粒度交互网络 MGSum，并统一了提取式和抽象式 MDS。单词、句子和文档被视为由粒度层次关系图连接的三个语义单元粒度级别。在相同粒度下，使用自注意力机制来捕捉语义关系。句子粒度表示用于提取式总结，而单词粒度表示则用于生成抽象总结。MGSum
    使用融合门来整合和更新语义表示。此外，采用稀疏注意力机制来确保总结生成器关注重要信息。Brazinskas 等人（Brazinskas et al., [2020](#bib.bib17)）为
    MDS 的少样本学习创造了一个先例，该方法利用 Transformer 条件语言模型和插件网络进行提取式和抽象式 MDS，以克服因对大参数模型进行天真的微调而导致的快速过拟合和生成质量差的问题。'
- en: Hierarchical Transformer. To handle huge input documents, Yang et al. (Liu and
    Lapata, [2019](#bib.bib95)) proposed a two-stage Hierarchical Transformer (HT)
    model with an inter-paragraph and graph-informed attention mechanism that allows
    the model to encode multiple input documents hierarchically instead of by simple
    flat-concatenation. A logistic regression model is employed to select the top-$K$
    paragraphs, which are fed into a local Transformer layer to obtain contextual
    features. A global Transformer layer mixes the contextual information to model
    the dependencies of the selected paragraphs. To leverage graph structure to capture
    cross-document relations, Li et al. (Li et al., [2020a](#bib.bib91)) proposed
    an end-to-end Transformer based model GraphSum, based on the HT model. In the
    graph encoding layers, GraphSum extends the self-attention mechanism to the graph-informed
    self-attention mechanism, which incorporates the graph representation into the
    Transformer encoding process. Furthermore, the Gaussian function is applied to
    the graph representation matrix to control the intensity of the graph structure
    impact on the summarization model. The HT and GraphSum models are both based on
    the self-attention mechanism leading quadratic memory growth increases with the
    number of input sequences; to address this issue, Pasunuru et al. (Pasunuru et al.,
    [2021b](#bib.bib123)) modified the full self-attention with local and global attention
    mechanism (Beltagy et al., [2020](#bib.bib14)) to scale the memory linearly. Dual
    encoders are proposed for encoding truncated concatenated documents and linearized
    graph information from full documents.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 层次化变换器。为了处理大型输入文档，Yang 等（Liu 和 Lapata，[2019](#bib.bib95)）提出了一种两阶段的层次化变换器（HT）模型，该模型具有段落间和图信息驱动的注意机制，使得模型能够以层次化的方式对多个输入文档进行编码，而不是简单的平面拼接。采用逻辑回归模型来选择前$K$个段落，这些段落被输入到局部变换器层以获得上下文特征。全局变换器层混合上下文信息以建模所选段落的依赖关系。为了利用图结构捕捉跨文档关系，Li
    等（Li 等，[2020a](#bib.bib91)）提出了一种基于HT模型的端到端变换器模型GraphSum。在图编码层中，GraphSum 将自注意机制扩展为图信息驱动的自注意机制，将图表示融入变换器编码过程。此外，采用高斯函数对图表示矩阵进行处理，以控制图结构对摘要模型的影响强度。HT和GraphSum模型均基于自注意机制，因此随着输入序列数量的增加，内存需求呈平方增长；为了解决这一问题，Pasunuru
    等（Pasunuru 等，[2021b](#bib.bib123)）将全自注意力机制修改为局部和全局注意机制（Beltagy 等，[2020](#bib.bib14)），以实现内存线性扩展。提出了双重编码器用于编码截断的拼接文档和来自完整文档的线性图信息。
- en: Pre-trained language models (LMs). Pre-trained Transformers on large text corpora
    have shown great successes in downstream NLP tasks including text summarization.
    The pre-trained LMs can be trained on non-summarization or SDS datasets to overcome
    lack of MDS data (Zhang et al., [2020d](#bib.bib173); Li et al., [2020a](#bib.bib91);
    Pasunuru et al., [2021b](#bib.bib123)). Most pre-trained LMs such as BERT (Devlin
    et al., [2019](#bib.bib35)) and RoBERTa (Liu et al., [2019a](#bib.bib96)) can
    work well on short sequences. In hierarchical Transformer architecture, replacing
    the low-level Transformer (token-level) encoding layer with pre-trained LMs helps
    the model break through length limitations to perceive further information (Li
    et al., [2020a](#bib.bib91)). Inside a hierarchical Transformer architecture,
    the output vector of the ”[CLS]” token can be used as input for high-level Transformer
    models. To avoid the self-attention quadratic-memory increment when dealing with
    document-scale sequences, a Longformer based approach (Beltagy et al., [2020](#bib.bib14)),
    including local and global attention mechanisms, can be incorporated with pre-trained
    LMs to scale the memory linearly for MDS (Pasunuru et al., [2021b](#bib.bib123)).
    Another solution for computational issues can be borrowed from SDS is to use a
    multi-layer Transformer architecture to scale the length of documents allowing
    pre-trained LMs to encode a small block of text and the information can be shared
    among the blocks between two successive layers (Grail et al., [2021](#bib.bib54)).
    PEGASUS (Zhang et al., [2020d](#bib.bib173)) is a pre-trained Transformer-based
    encoder-decoder model with gap-sentences generation (GSG) specifically designed
    for abstractive summarization. GSG shows that masking whole sentences based on
    importance, instead of through random or lead selection, works well for downstream
    summarization tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练语言模型（LMs）。在大型文本语料库上预训练的变换器在下游自然语言处理任务中取得了巨大成功，包括文本摘要。预训练的 LMs 可以在非摘要或 SDS
    数据集上进行训练，以克服 MDS 数据的缺乏（Zhang et al., [2020d](#bib.bib173); Li et al., [2020a](#bib.bib91);
    Pasunuru et al., [2021b](#bib.bib123)）。大多数预训练 LMs，如 BERT（Devlin et al., [2019](#bib.bib35)）和
    RoBERTa（Liu et al., [2019a](#bib.bib96)），在短序列上表现良好。在层次化变换器架构中，用预训练的 LMs 替换低层次变换器（标记级别）编码层有助于模型突破长度限制，感知更多信息（Li
    et al., [2020a](#bib.bib91)）。在层次化变换器架构中，“[CLS]” 标记的输出向量可以用作高层变换器模型的输入。为了避免在处理文档级序列时自注意力的二次内存增加，可以采用基于
    Longformer 的方法（Beltagy et al., [2020](#bib.bib14)），包括局部和全局注意机制，可以与预训练的 LMs 结合，线性扩展
    MDS 的内存（Pasunuru et al., [2021b](#bib.bib123)）。另一个解决计算问题的方案可以借鉴 SDS 是使用多层变换器架构来扩展文档长度，使预训练的
    LMs 能够编码一小块文本，信息可以在两个连续层之间的块之间共享（Grail et al., [2021](#bib.bib54)）。PEGASUS（Zhang
    et al., [2020d](#bib.bib173)）是一个基于变换器的编码器-解码器模型，具有生成间隙句子（GSG），专为抽象摘要设计。GSG 表明，基于重要性掩盖整个句子，而不是通过随机或前导选择，对下游摘要任务效果良好。
- en: 3.7\. Deep Hybrid Models
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7\. 深度混合模型
- en: Many neural models can be integrated to formalize a more powerful and expressive
    model. In this section, we summarize the existing deep hybrid models that have
    proven to be effective for MDS.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 许多神经模型可以被整合以形成更强大和更具表现力的模型。在这一部分，我们总结了现有的深度混合模型，这些模型已被证明对 MDS 有效。
- en: CNN + LSTM + Capsule networks. Cho et al. (Cho et al., [2019](#bib.bib29)) proposed
    a hybrid model based on the determinantal point processes for semantically measuring
    sentence similarities. A convolutional layer slides over the pairwise sentences
    with filters of different sizes to extract low-level features. Capsule networks
    (Sabour et al., [2017](#bib.bib135); Yang et al., [2018](#bib.bib163)) are employed
    to identify redundant information by transforming the spatial and orientational
    relationships for high-level representation. The authors also used LSTM to reconstruct
    pairwise sentences and add reconstruction loss to the final objective function.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: CNN + LSTM + 胶囊网络。Cho et al.（Cho et al., [2019](#bib.bib29)）提出了一种基于行列式点过程的混合模型，用于语义测量句子相似度。卷积层使用不同大小的滤波器在成对的句子上滑动，以提取低级特征。胶囊网络（Sabour
    et al., [2017](#bib.bib135); Yang et al., [2018](#bib.bib163)）用于通过转换空间和方向关系来识别冗余信息，以进行高级表示。作者还使用
    LSTM 重新构建成对的句子，并将重构损失添加到最终目标函数中。
- en: CNN + Bi-LSTM + Multi-layer Perceptron (MLP). Abhishek et al. (Singh et al.,
    [2018](#bib.bib142)) proposed an extractive MDS framework that considers document-dependent
    and document-independent information. In this model, a CNN with different filters
    captures phrase-level representation. Full binary trees formed with these salient
    representation are fed to the recommended Bi-LSTM tree indexer to enable better
    generalization abilities. A MLP with ReLU function is employed for leaf node transformation.
    More specifically, the Bi-LSTM tree indexer leverages the time serial power of
    LSTMs and the compositionality of recursive models to capture both semantic and
    compositional features.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: CNN + Bi-LSTM + 多层感知机（MLP）。Abhishek et al. (Singh et al., [2018](#bib.bib142))
    提出了一个提取式MDS框架，考虑了文档相关和文档无关的信息。在该模型中，一个具有不同过滤器的CNN捕捉短语级表示。利用这些显著表示形成的完整二叉树被输入到推荐的Bi-LSTM树索引器中，以实现更好的泛化能力。使用ReLU函数的MLP用于叶节点转换。更具体地说，Bi-LSTM树索引器利用LSTM的时间序列能力和递归模型的组成性来捕捉语义和组成特征。
- en: PG networks + Transformer. In generating a summary, it is necessary to consider
    the information fusion of multiple sentences, especially sentence pairs. Logan
    et al. (Lebanoff et al., [2019](#bib.bib83)) found the majority of summary sentences
    are generated by fusing one or two source sentences; so they proposed a two-stage
    summarization method that considers the semantic compatibility of sentence pairs.
    This method joint-scores single sentence and sentence pairs to filter representative
    from the original documents. Sentences or sentence pairs with high scores are
    then compressed and rewritten to generate a summary that leverages PG network.
    This paper uses a Transformer based model to encode both single sentence and sentence
    pairs indiscriminately to obtain the deep contextual representation of words and
    sequences.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: PG网络 + Transformer。在生成摘要时，需要考虑多句子，特别是句子对的信息融合。Logan et al. (Lebanoff et al.,
    [2019](#bib.bib83)) 发现大多数摘要句子是通过融合一个或两个源句子生成的；因此，他们提出了一种考虑句子对语义兼容性的两阶段摘要方法。这种方法对单句和句子对进行联合打分，以从原始文档中筛选出代表性内容。得分高的句子或句子对随后被压缩和改写，以生成利用PG网络的摘要。本文使用基于Transformer的模型对单句和句子对进行不加区分的编码，以获得词语和序列的深层上下文表示。
- en: Table 2\. Deep Learning based Methods. ”Ext”, ”Abs” and ”Hyd” mean extractive,
    abstractive and hybrid respectively; ”FC” and ”HC” represent Flat Concatenate,
    Hierarchical Concatenate respectively.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 基于深度学习的方法。“Ext”，“Abs”和“Hyd”分别表示提取式、抽象式和混合式；“FC”和“HC”分别代表平坦拼接、层次拼接。
- en: '| Methods | Works |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 工作 |'
- en: '&#124; Construction &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 构建 &#124;'
- en: '&#124; Types &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类型 &#124;'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Document-level &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文档级别 &#124;'
- en: '&#124; Relationship &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关系 &#124;'
- en: '|'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Comparison of &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 比较 &#124;'
- en: '&#124; DL based techniques &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于DL的技术 &#124;'
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Ext | Abs | Hyb | FC | HC | Pros and Cons |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Ext | Abs | Hyb | FC | HC | 优缺点 |'
- en: '| RNN | MeanSum (Chu and Liu, [2019](#bib.bib31)) |  | ✓ |  | ✓ |  | Pros:
    Can capture sequential relations and syntactic/semantic information from word
    sequences Cons: Not easy to parallel computing; Highly depending on results from
    the previous steps |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| RNN | MeanSum (Chu and Liu, [2019](#bib.bib31)) |  | ✓ |  | ✓ |  | 优点：能够捕捉词序列中的顺序关系和语法/语义信息
    缺点：不容易进行并行计算；高度依赖于前一步的结果 |'
- en: '| Zhang et al. (Zhang et al., [2018b](#bib.bib172)) |  | ✓ |  | ✓ |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. (Zhang et al., [2018b](#bib.bib172)) |  | ✓ |  | ✓ |  |'
- en: '| STDS (Zheng et al., [2019](#bib.bib182)) | ✓ |  |  |  | ✓ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| STDS (Zheng et al., [2019](#bib.bib182)) | ✓ |  |  |  | ✓ |'
- en: '| ParaFuse_doc (Nayeem et al., [2018](#bib.bib112)) |  | ✓ |  |  | ✓ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| ParaFuse_doc (Nayeem et al., [2018](#bib.bib112)) |  | ✓ |  |  | ✓ |'
- en: '| R2N2 (Cao et al., [2015a](#bib.bib20)) | ✓ |  |  | ✓ |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| R2N2 (Cao et al., [2015a](#bib.bib20)) | ✓ |  |  | ✓ |  |'
- en: '| CondaSum (Amplayo and Lapata, [2021](#bib.bib4)) |  |  | ✓ |  | ✓ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| CondaSum (Amplayo and Lapata, [2021](#bib.bib4)) |  |  | ✓ |  | ✓ |'
- en: '| C-Attention (Li et al., [2017b](#bib.bib90)) |  | ✓ |  | ✓ |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| C-Attention (Li et al., [2017b](#bib.bib90)) |  | ✓ |  | ✓ |  |'
- en: '| Wang et al.(Wang and Ling, [2016](#bib.bib158)) |  | ✓ |  | ✓ |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. (Wang and Ling, [2016](#bib.bib158)) |  | ✓ |  | ✓ |  |'
- en: '| RL-MMR (Mao et al., [2020](#bib.bib103)) | ✓ |  |  | ✓ |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| RL-MMR (Mao et al., [2020](#bib.bib103)) | ✓ |  |  | ✓ |  |'
- en: '| Coavoux et al.(Coavoux et al., [2019](#bib.bib33)) |  | ✓ |  | ✓ |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Coavoux et al. (Coavoux et al., [2019](#bib.bib33)) |  | ✓ |  | ✓ |  |'
- en: '| CNN | MV-CNN (Zhang et al., [2016](#bib.bib179)) | ✓ |  |  | ✓ |  | Pros:
    Good parallel computing; Cons: Not good at processing sequential data |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| CNN | MV-CNN (Zhang et al., [2016](#bib.bib179)) | ✓ |  |  | ✓ |  | 优点：良好的并行计算；缺点：处理序列数据的能力不足
    |'
- en: '| TCSum (Cao et al., [2017](#bib.bib19)) | ✓ |  |  | ✓ |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| TCSum (Cao et al., [2017](#bib.bib19)) | ✓ |  |  | ✓ |  |'
- en: '| CNNLM (Yin and Pei, [2015](#bib.bib169)) | ✓ |  |  | ✓ |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| CNNLM (Yin and Pei, [2015](#bib.bib169)) | ✓ |  |  | ✓ |  |'
- en: '| PriorSum (Cao et al., [2015b](#bib.bib21)) | ✓ |  |  | ✓ |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| PriorSum (Cao et al., [2015b](#bib.bib21)) | ✓ |  |  | ✓ |  |'
- en: '| Angelidis et al.(Angelidis and Lapata, [2018](#bib.bib5)) | ✓ |  |  | ✓ |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Angelidis et al.(Angelidis and Lapata, [2018](#bib.bib5)) | ✓ |  |  | ✓ |  |'
- en: '| GNN | Yasunaga et al.(Yasunaga et al., [2017](#bib.bib168)) | ✓ |  |  |  |
    ✓ | Pros: Can capture cross-document and in-document relations Cons: Inefficient
    when dealing with large graphs |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| GNN | Yasunaga et al.(Yasunaga et al., [2017](#bib.bib168)) | ✓ |  |  |  |
    ✓ | 优点：可以捕捉跨文档和文档内的关系 缺点：处理大图时效率低 |'
- en: '| SemSentSum (Antognini and Faltings, [2019](#bib.bib6)) | ✓ |  |  |  | ✓ |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| SemSentSum (Antognini and Faltings, [2019](#bib.bib6)) | ✓ |  |  |  | ✓ |'
- en: '| Scisummnet (Yasunaga et al., [2019](#bib.bib167)) | ✓ |  |  |  | ✓ |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| Scisummnet (Yasunaga et al., [2019](#bib.bib167)) | ✓ |  |  |  | ✓ |'
- en: '| HDSG (Wang et al., [2020a](#bib.bib156)) | ✓ |  |  |  | ✓ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| HDSG (Wang et al., [2020a](#bib.bib156)) | ✓ |  |  |  | ✓ |'
- en: '| PG | PG-MMR (Lebanoff et al., [2018](#bib.bib84)) |  | ✓ |  | ✓ |  | Pros:
    Low redundancy Cons: Hard to train |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| PG | PG-MMR (Lebanoff et al., [2018](#bib.bib84)) |  | ✓ |  | ✓ |  | 优点：低冗余
    缺点：训练困难 |'
- en: '| Hi-MAP (Fabbri et al., [2019](#bib.bib44)) |  | ✓ |  | ✓ |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Hi-MAP (Fabbri et al., [2019](#bib.bib44)) |  | ✓ |  | ✓ |  |'
- en: '| Transformer | HT (Liu and Lapata, [2019](#bib.bib95)) |  | ✓ |  |  | ✓ |
    Pros: Good performance; Good parallel computing; Can capture cross-document and
    in-document relations Cons: Time-consuming; Problems with position encoding |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | HT (Liu and Lapata, [2019](#bib.bib95)) |  | ✓ |  |  | ✓ |
    优点：性能良好；良好的并行计算；可以捕捉跨文档和文档内的关系 缺点：耗时；位置编码存在问题 |'
- en: '| MGSum (Jin et al., [2020](#bib.bib72)) | ✓ | ✓ |  |  | ✓ |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| MGSum (Jin et al., [2020](#bib.bib72)) | ✓ | ✓ |  |  | ✓ |'
- en: '| FewSum (Brazinskas et al., [2020](#bib.bib17)) | ✓ | ✓ |  | ✓ |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| FewSum (Brazinskas et al., [2020](#bib.bib17)) | ✓ | ✓ |  | ✓ |  |'
- en: '| GraphSum (Li et al., [2020a](#bib.bib91)) |  | ✓ |  |  | ✓ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| GraphSum (Li et al., [2020a](#bib.bib91)) |  | ✓ |  |  | ✓ |'
- en: '| Bart-Long (Pasunuru et al., [2021b](#bib.bib123)) |  | ✓ |  |  | ✓ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Bart-Long (Pasunuru et al., [2021b](#bib.bib123)) |  | ✓ |  |  | ✓ |'
- en: '| WikiSum (Liu et al., [2018](#bib.bib94)) |  |  | ✓ | ✓ |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| WikiSum (Liu et al., [2018](#bib.bib94)) |  |  | ✓ | ✓ |  |'
- en: '| Deep Hybid Model | Cho et al.(Cho et al., [2019](#bib.bib29)) | ✓ |  |  |
    ✓ |  | Pros: Combines the advantages of different DL models Cons: Computationally
    intensive |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Deep Hybid Model | Cho et al.(Cho et al., [2019](#bib.bib29)) | ✓ |  |  |
    ✓ |  | 优点：结合了不同DL模型的优点 缺点：计算密集型 |'
- en: '| GT-SingPairMix (Lebanoff et al., [2019](#bib.bib83)) |  | ✓ |  | ✓ |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| GT-SingPairMix (Lebanoff et al., [2019](#bib.bib83)) |  | ✓ |  | ✓ |  |'
- en: '| HNet (Singh et al., [2018](#bib.bib142)) | ✓ |  |  | ✓ |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| HNet (Singh et al., [2018](#bib.bib142)) | ✓ |  |  | ✓ |  |'
- en: 3.8\. The Variants of Multi-document Summarization
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8\. 多文档摘要的变体
- en: In this section, we briefly introduce several MDS task variants to give researchers
    a comprehensive understanding of MDS. These tasks can be modeled as MDS problems
    and adopt the aforementioned deep learning techniques and neural network architectures.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要介绍了几种MDS任务变体，以便研究人员对MDS有一个全面的了解。这些任务可以被建模为MDS问题，并采用上述深度学习技术和神经网络架构。
- en: Query-oriented MDS calls for a summary from a set of documents that answers
    a query. It tries to solve realistic query-oriented scenario problems and only
    summarizes important information that best answers the query in a logical order
    (Pasunuru et al., [2021a](#bib.bib122)). Specifically, query-oriented MDS combines
    the information retrieval and MDS techniques. The content that needs to be summarized
    is based on the given queries. Liu et al. (Liu and Lapata, [2019](#bib.bib95))
    incorporated the query by simply prepending the query to the top-ranked document
    during encoding. Pasunuru (Pasunuru et al., [2021a](#bib.bib122)) involved a query
    encoder and integrated query embedding into an MDS model, ranking the importance
    of documents for a given query.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 查询导向MDS要求从一组文档中生成一个回答查询的摘要。它试图解决现实中的查询导向场景问题，仅总结最能回答查询的重要信息，并以逻辑顺序进行总结（Pasunuru
    et al., [2021a](#bib.bib122)）。具体来说，查询导向MDS结合了信息检索和MDS技术。需要总结的内容基于给定的查询。Liu et
    al. (Liu and Lapata, [2019](#bib.bib95)) 通过在编码时将查询简单地添加到排名最高的文档上来结合查询。Pasunuru
    (Pasunuru et al., [2021a](#bib.bib122)) 引入了一个查询编码器，并将查询嵌入集成到MDS模型中，为给定的查询排名文档的重要性。
- en: 'Dialogue summarization aims to provide a succinct synopsis from multiple textual
    utterances of two or more participants, which could help quickly capture relevant
    information without having to listen to long and convoluted dialogues (Liu et al.,
    [2019b](#bib.bib93)). Dialogue summary covers several areas, including meetings
    (Zhu et al., [2020](#bib.bib187); Koay et al., [2020](#bib.bib78); Feng et al.,
    [2021](#bib.bib45)), email threads (Zhang et al., [2021](#bib.bib175)), medical
    dialogues (Song et al., [2020](#bib.bib143); Joshi et al., [2020](#bib.bib73);
    Enarvi et al., [2020](#bib.bib40)), customer service (Liu et al., [2019b](#bib.bib93))
    and media interviews (Zhu et al., [2021](#bib.bib186)). Challenges in dialogue
    summarization can be summarized into the following seven categories: informal
    language use, multiple participants, multiple turns, referral and coreference,
    repetition and interruption, negations and rhetorical questions, role and language
    change (Chen and Yang, [2020](#bib.bib25)). The flow of the dialogue would be
    neglected if MDS models are directly applied for dialogue summarization. Liu et
    al. (Liu et al., [2019b](#bib.bib93)) relied on human annotations to capture the
    logic of the dialogue. Wu et al. (Wu et al., [2021](#bib.bib159)) used summary
    sketch to identify the interaction between speakers and their corresponding textual
    utterances in each turn. Chen et al. (Chen and Yang, [2020](#bib.bib25)) proposed
    a multi-view sequence to sequence based encoder to extract dialogue structure
    and a multi-view decoder to incorporate different views to generate final summaries.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对话摘要旨在从两个或更多参与者的多个文本发言中提供简明的概述，这可以帮助快速捕捉相关信息，而无需听长时间复杂的对话（Liu et al., [2019b](#bib.bib93)）。对话摘要涵盖多个领域，包括会议（Zhu
    et al., [2020](#bib.bib187)；Koay et al., [2020](#bib.bib78)；Feng et al., [2021](#bib.bib45)）、电子邮件线程（Zhang
    et al., [2021](#bib.bib175)）、医疗对话（Song et al., [2020](#bib.bib143)；Joshi et al.,
    [2020](#bib.bib73)；Enarvi et al., [2020](#bib.bib40)）、客户服务（Liu et al., [2019b](#bib.bib93)）和媒体采访（Zhu
    et al., [2021](#bib.bib186)）。对话摘要中的挑战可以总结为以下七个类别：非正式语言使用、多参与者、多轮对话、引用和共指、重复和打断、否定和修辞问题、角色和语言变化（Chen
    and Yang, [2020](#bib.bib25)）。如果直接将MDS模型应用于对话摘要，将忽视对话的流程。刘等（Liu et al., [2019b](#bib.bib93)）依赖人工标注来捕捉对话的逻辑。吴等（Wu
    et al., [2021](#bib.bib159)）使用摘要草图来识别发言者之间的互动及其在每轮中的对应文本发言。陈等（Chen and Yang, [2020](#bib.bib25)）提出了一个多视角的序列到序列编码器，以提取对话结构，并使用多视角解码器整合不同视角生成最终摘要。
- en: Stream summarization aims to summarize new documents in a continuously growing
    document stream, such as information from social media. Temporal summarization
    and real-time summarization (RTS)⁹⁹9http://trecrts.github.io/ can be seen as a
    form of stream document summarization. Stream summarization considers both historical
    dependencies and future uncertainty of the document stream. Yang et al. (Yang
    et al., [[n.d.]](#bib.bib162)) used deep reinforcement learning to solve the relevance,
    redundancy, and timeliness issues in steam summarization. Tan et al. (Tan et al.,
    [2017](#bib.bib146)) transformed the real time summarization task as a sequential
    decision making problem and used a LSTM layer and three fully connected neural
    network layers to maximize the long-term rewards.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 流摘要旨在总结在持续增长的文档流中出现的新文档，例如社交媒体中的信息。时间摘要和实时摘要（RTS）⁹⁹9http://trecrts.github.io/
    可以看作是一种流文档摘要形式。流摘要考虑了文档流的历史依赖性和未来不确定性。杨等（Yang et al., [[n.d.]](#bib.bib162)）使用深度强化学习来解决流摘要中的相关性、冗余性和时效性问题。谭等（Tan
    et al., [2017](#bib.bib146)）将实时摘要任务转化为一个序列决策问题，并使用了一个LSTM层和三个全连接神经网络层以最大化长期奖励。
- en: 3.9\. Discussion
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.9\. 讨论
- en: 'In this section, we have reviewed the state-of-the-art works of deep learning
    based MDS models according to the neural networks applied. Table [2](#S3.T2 "Table
    2 ‣ 3.7\. Deep Hybrid Models ‣ 3\. Deep Learning Based Multi-document Summarization
    Methods ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")
    summarizes the reviewed works by considering the type of neural networks, construction
    types, and concatenation methods; and provides a high-level summary of their relative
    advantages and disadvantages. Transformer based models have been most commonly
    used in the last three years because they overcome the limitations of CNN’s fixed-size
    receptive field and RNN’s inability to parallel process. However, deep learning
    based MDS models face some challenges. Firstly, the complexity of deep learning
    based models and the data-driven deep learning systems do require more training
    data, with concomitant increased efforts in data labelling, and computing resources
    than non-deep learning based methods, which are not time efficient. Secondly,
    deep learning based methods lack linguistic knowledge that can serve as important
    roles in assisting deep learning based learners to have informative representation
    and better guide the summary generation. We believe that this is one possible
    reason that some non-deep learning based MDS methods sometimes show better performance
    than deep learning based methods (Lu et al., [2020](#bib.bib98); Cao et al., [2015b](#bib.bib21))
    as non-deep learning based methods pay more attention to linguistic information.
    We discuss this point in Section [7](#S7 "7\. Future research directions and open
    issues ‣ Multi-document Summarization via Deep Learning Techniques: A Survey").
    Further researches could also be based on techniques adopted in non-deep learning
    based MDS as reviewed in (Ferreira et al., [2014](#bib.bib46); Shah and Jivani,
    [2016](#bib.bib139); El-Kassas et al., [2021](#bib.bib39)).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经回顾了根据所应用的神经网络技术的最先进深度学习基础MDS模型。表[2](#S3.T2 "表 2 ‣ 3.7\. 深度混合模型 ‣ 3\.
    基于深度学习的多文档摘要方法 ‣ 基于深度学习技术的多文档摘要：综述")总结了通过考虑神经网络的类型、构建类型和拼接方法来审查的工作，并提供了它们相对优缺点的高级总结。基于变换器的模型在过去三年中最为常用，因为它们克服了CNN固定大小感受野和RNN无法并行处理的局限性。然而，基于深度学习的MDS模型面临一些挑战。首先，深度学习基础模型和数据驱动的深度学习系统确实需要更多的训练数据，相应地增加了数据标注和计算资源的投入，而非深度学习基础的方法则不够高效。其次，深度学习基础的方法缺乏可以在帮助深度学习基础学习者提供信息性表示和更好地指导摘要生成中发挥重要作用的语言知识。我们认为这是一些非深度学习基础的MDS方法有时表现优于深度学习基础的方法的一个可能原因（Lu
    et al., [2020](#bib.bib98); Cao et al., [2015b](#bib.bib21)），因为非深度学习基础的方法更关注语言信息。我们在第[7](#S7
    "7\. 未来研究方向和开放问题 ‣ 基于深度学习的多文档摘要：综述")节中讨论了这一点。进一步的研究也可以基于非深度学习基础的MDS中采用的技术，如（Ferreira
    et al., [2014](#bib.bib46); Shah 和 Jivani, [2016](#bib.bib139); El-Kassas et al.,
    [2021](#bib.bib39)）中审查的内容。
- en: 4\. Objective Functions
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 目标函数
- en: In this section, we will take a closer look at different objective functions
    adopted by various MDS models. In summarization models, objective functions play
    an important role by guiding the model to achieve specific purposes. To the best
    of our knowledge, we are the first to provide a comprehensive survey on different
    objectives of summarization tasks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更详细地考察不同MDS模型所采用的目标函数。在摘要模型中，目标函数通过指导模型实现特定目标发挥着重要作用。据我们所知，我们是首次提供关于摘要任务不同目标的综合综述。
- en: 4.1\. Cross-Entropy Objective
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 交叉熵目标
- en: 'Cross-entropy usually acts as an objective function to measure the distance
    between two distributions. Many existing MDS models adopt it to measure the difference
    between the distributions of generated summaries and the golden summaries (Cao
    et al., [2015a](#bib.bib20); Zhang et al., [2016](#bib.bib179); Wang et al., [2020a](#bib.bib156);
    Zhang et al., [2018b](#bib.bib172); Cho et al., [2019](#bib.bib29); Yasunaga et al.,
    [2019](#bib.bib167)). Formally, cross-entropy loss is defined as:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵通常作为目标函数来测量两个分布之间的距离。许多现有的MDS模型采用它来测量生成摘要和黄金摘要之间的差异（Cao et al., [2015a](#bib.bib20);
    Zhang et al., [2016](#bib.bib179); Wang et al., [2020a](#bib.bib156); Zhang et
    al., [2018b](#bib.bib172); Cho et al., [2019](#bib.bib29); Yasunaga et al., [2019](#bib.bib167)）。正式地，交叉熵损失定义为：
- en: '| (1) |  | $\small L_{CE}=-\sum_{i=1}\mathbf{y_{i}}\log(\mathbf{\hat{y}_{i}}),$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\small L_{CE}=-\sum_{i=1}\mathbf{y_{i}}\log(\mathbf{\hat{y}_{i}}),$
    |  |'
- en: 'where $\mathbf{y_{i}}$ is the target score from golden summaries and machine-generated
    summaries, and $\mathbf{\hat{y}_{i}}$ is the predicted estimation from the deep
    learning based models. Different from calculations in other tasks, such as text
    classification, in summarization tasks, $\mathbf{y_{i}}$ and $\mathbf{\hat{y}_{i}}$
    have several methods to calculate. $\mathbf{\hat{y}_{i}}$ usually is calculated
    by Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Please refer to
    Section [5](#S5 "5\. Evaluation metrics ‣ Multi-document Summarization via Deep
    Learning Techniques: A Survey")). For example, ROUGE-1(Antognini and Faltings,
    [2019](#bib.bib6)), ROUGE-2 (Liu and Lapata, [2019](#bib.bib95)) or the normalized
    average of ROUGE-1 and ROUGE-2 scores (Yasunaga et al., [2017](#bib.bib168)) could
    be adopted to compute the ground truth score between the selected sentences and
    golden summary.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{y_{i}}$ 是来自黄金摘要和机器生成摘要的目标分数，而 $\mathbf{\hat{y}_{i}}$ 是基于深度学习模型的预测估计。与其他任务的计算不同，例如文本分类，在摘要任务中，$\mathbf{y_{i}}$
    和 $\mathbf{\hat{y}_{i}}$ 具有几种计算方法。$\mathbf{\hat{y}_{i}}$ 通常通过面向召回的替代评估（ROUGE）计算（请参见第
    [5](#S5 "5\. 评估指标 ‣ 基于深度学习技术的多文档摘要：调查") 节）。例如，ROUGE-1（Antognini 和 Faltings，[2019](#bib.bib6)）、ROUGE-2（Liu
    和 Lapata，[2019](#bib.bib95)）或 ROUGE-1 和 ROUGE-2 分数的标准化平均值（Yasunaga 等，[2017](#bib.bib168)）可以用来计算选定句子与黄金摘要之间的真实分数。
- en: 4.2\. Reconstructive Objective
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 重建目标
- en: 'Reconstructive objectives are used to train a distinctive representation learner
    by reconstructing the input vectors in an unsupervised learning manner. The objective
    function is defined as:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 重建目标用于通过以无监督学习的方式重建输入向量来训练一种独特的表示学习模型。目标函数定义为：
- en: '| (2) |  | $\small L_{Rec}=\left\&#124;\mathbf{x_{i}}-\phi^{\prime}(\phi(\mathbf{x_{i}};\theta);\theta^{\prime})\right\&#124;_{*},$
    |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\small L_{Rec}=\left\&#124;\mathbf{x_{i}}-\phi^{\prime}(\phi(\mathbf{x_{i}};\theta);\theta^{\prime})\right\&#124;_{*},$
    |  |'
- en: where $\mathbf{x_{i}}$ represents the input vector; $\phi$ and $\phi^{\prime}$
    represent the encoder and decoder with $\theta$ and $\theta^{\prime}$ as their
    parameters respectively, $||\cdot||_{*}$ represents norm (* stands for 0, 1, 2,
    …, infinity). $L_{Rec}$ is a measuring function to calculate the distance between
    source documents and their reconstructive outputs. Chu et al. (Chu and Liu, [2019](#bib.bib31))
    used a reconstructive loss to constrain the generated text into the natural language
    domain, reconstructing reviews in a token-by-token manner. Moreover, this paper
    also proposes a variant termed reconstruction cycle loss. By using the variant,
    the reviews are encoded into a latent space to further generate the summary, and
    the summary is then decoded to the reconstructed reviews to form another reconstructive
    closed-loop. An unsupervised learning loss was designed by Li et al. (Li et al.,
    [2017b](#bib.bib90)) to reconstruct the condensed output vectors to the original
    input sentence vectors with $L_{2}$ distance. This paper further constrains the
    condensed output vector with a $L_{1}$ regularizer to ensure sparsity. Similarly,
    Zheng et al. (Zheng et al., [2019](#bib.bib182)) adopted a bi-directional GRU
    encoder-decoder framework to reconstruct both news sentences and comment sentences
    in a word sequence manner. Liu et al. (Liu et al., [2018](#bib.bib94)) used reconstruction
    within the abstractive stage of a two-stage strategy to alleviate the problem
    introduced by long input documents. Both input and output sequences are concatenated
    to predict the next token to train the abstractive model. There are also some
    variants, such as leveraging the latent vectors of variational auto-encoder for
    reconstruction to capture better representation. Li et al. (Li et al., [2017a](#bib.bib89))
    introduced three individual reconstructive losses to consider both news reconstruction
    and comments reconstruction separately, along with a variational auto-encoder
    lower bound. Bravzinskas et al. (Bražinskas et al., [2019](#bib.bib16)) utilized
    a variational auto-encoder to generate the latent vectors of given reviews, where
    each review is reconstructed by the latent vectors combined with other reviews.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{x_{i}}$ 表示输入向量；$\phi$ 和 $\phi^{\prime}$ 分别表示编码器和解码器，其参数分别为 $\theta$
    和 $\theta^{\prime}$，$||\cdot||_{*}$ 表示范数（* 表示 0、1、2、……、无穷大）。 $L_{Rec}$ 是一个测量函数，用于计算源文档与其重建输出之间的距离。Chu
    等人（Chu 和 Liu，[2019](#bib.bib31)）使用重建损失将生成的文本约束到自然语言领域，以逐词的方式重建评论。此外，本文还提出了一种称为重建循环损失的变体。通过使用该变体，评论被编码到潜在空间中以进一步生成摘要，然后摘要被解码为重建的评论，形成另一个重建闭环。Li
    等人（Li 等，[2017b](#bib.bib90)）设计了一种无监督学习损失，以 $L_{2}$ 距离将压缩输出向量重建为原始输入句子向量。本文进一步使用
    $L_{1}$ 正则化器对压缩输出向量施加约束，以确保稀疏性。类似地，Zheng 等人（Zheng 等，[2019](#bib.bib182)）采用了双向
    GRU 编码器-解码器框架，以词序列的方式重建新闻句子和评论句子。Liu 等人（Liu 等，[2018](#bib.bib94)）在两阶段策略的抽象阶段中使用重建，以缓解长输入文档引入的问题。输入和输出序列被连接以预测下一个
    token 来训练抽象模型。还有一些变体，例如利用变分自编码器的潜在向量进行重建，以捕捉更好的表示。Li 等人（Li 等，[2017a](#bib.bib89)）引入了三个独立的重建损失，以分别考虑新闻重建和评论重建，并结合变分自编码器下界。Bravzinskas
    等人（Bražinskas 等，[2019](#bib.bib16)）利用变分自编码器生成给定评论的潜在向量，其中每个评论由潜在向量与其他评论组合进行重建。
- en: 4.3\. Redundancy Objective
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 冗余目标
- en: Redundancy is an important objective to minimize the overlap between semantic
    units in a machine-generated summary. By using this objective, models are encouraged
    to maximize information coverage. Formally,
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余是最小化机器生成摘要中语义单元重叠的重要目标。通过使用这个目标，模型被鼓励最大化信息覆盖。正式地，
- en: '| (3) |  | $\small L_{Red}=Sim(\mathbf{x_{i}},\mathbf{x_{j}}),$ |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\small L_{Red}=Sim(\mathbf{x_{i}},\mathbf{x_{j}}),$ |  |'
- en: where $Sim(\cdot)$ is the similarity function to measure the overlap between
    different $\mathbf{x_{i}}$ and $\mathbf{x_{j}}$, which can be phrases, sentences,
    topics or documents. The redundancy objective is often treated as an auxiliary
    objective combined with other loss functions. Li et al. (Li et al., [2017b](#bib.bib90))
    penalized phrase pairs with similar meanings to eliminate the redundancy. Nayeem
    et al. (Nayeem et al., [2018](#bib.bib112)) used the redundancy objective to avoid
    generating repetitive phrases, constraining a sentence to appear only once while
    maximizing the scores of important phrases. Zheng et al. (Zheng et al., [2019](#bib.bib182))
    adopted a redundancy loss function to measure overlaps between subtopics; intuitively,
    smaller overlaps between subtopics resulted in less redundancy in the output domain.
    Yin et al. (Yin and Pei, [2015](#bib.bib169)) proposed a redundancy objective
    to estimate the diversity between different sentences.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Sim(\cdot)$ 是测量不同 $\mathbf{x_{i}}$ 和 $\mathbf{x_{j}}$ 之间重叠的相似性函数，这些 $\mathbf{x_{i}}$
    和 $\mathbf{x_{j}}$ 可以是短语、句子、主题或文档。冗余目标通常被视为与其他损失函数结合的辅助目标。Li 等人（Li et al., [2017b](#bib.bib90)）惩罚具有相似意义的短语对，以消除冗余。Nayeem
    等人（Nayeem et al., [2018](#bib.bib112)）使用冗余目标来避免生成重复的短语，将句子限制为仅出现一次，同时最大化重要短语的得分。Zheng
    等人（Zheng et al., [2019](#bib.bib182)）采用冗余损失函数来测量子主题之间的重叠；直观上，子主题之间的重叠越小，输出领域中的冗余越少。Yin
    等人（Yin and Pei, [2015](#bib.bib169)）提出了一种冗余目标来估计不同句子之间的多样性。
- en: 4.4\. Max Margin Objective
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 最大间隔目标
- en: 'Max Margin Objectives (MMO) are also used to empower the MDS models to learn
    better representation. The objective function is formalized as:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最大间隔目标（MMO）也用于提升 MDS 模型的表示能力。目标函数形式化为：
- en: '| (4) |  | $\small L_{Margin}=\max\left(0,f(\mathbf{x_{i}};\theta)-f(\mathbf{x_{j}};\theta)+\gamma\right),$
    |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\small L_{Margin}=\max\left(0,f(\mathbf{x_{i}};\theta)-f(\mathbf{x_{j}};\theta)+\gamma\right),$
    |  |'
- en: 'where $\mathbf{x_{i}}$ and $\mathbf{x_{j}}$ represent the input vectors, $\theta$
    are parameters of the model function $f(\cdot)$, and $\gamma$ is the margin threshold.
    The MMO aims to force function $f(\mathbf{x_{i}};\theta)$ and function $f(\mathbf{x_{j}};\theta)$
    to be separated by a predefined margin $\gamma$. In Cao et al. (Cao et al., [2017](#bib.bib19)),
    a MMO is designed to constrain a pair of randomly sampled sentences with different
    salience scores – the one with higher score should be larger than the other one
    more than a marginal threshold. Two max margin losses are proposed in Zhong et
    al. (Zhong et al., [2020](#bib.bib183)): a margin-based triplet loss that encouraged
    the model to pull the golden summaries semantically closer to the original documents
    than to the machine-generated summaries; and a pair-wise margin loss based on
    a greater margin between paired candidates with more disparate ROUGE score rankings.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{x_{i}}$ 和 $\mathbf{x_{j}}$ 代表输入向量，$\theta$ 是模型函数 $f(\cdot)$ 的参数，$\gamma$
    是间隔阈值。MMO 的目标是强制函数 $f(\mathbf{x_{i}};\theta)$ 和函数 $f(\mathbf{x_{j}};\theta)$ 通过预定义的间隔
    $\gamma$ 分隔开。在 Cao 等人（Cao et al., [2017](#bib.bib19)）的研究中，设计了一个 MMO 来约束一对随机采样的句子，其显著性分数不同——分数较高的那个应大于另一个，超过一个边际阈值。在
    Zhong 等人（Zhong et al., [2020](#bib.bib183)）的研究中，提出了两种最大间隔损失：一种基于间隔的三元组损失，鼓励模型将黄金摘要在语义上拉近到原始文档，而不是机器生成的摘要；另一种是基于成对候选的较大间隔的损失，具有更离散的
    ROUGE 分数排名。
- en: 4.5\. Multi-Task Objective
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 多任务目标
- en: 'Supervision signals from MDS objectives may not be strong enough for representation
    learners, so some works seek other supervision signals from multiple tasks. A
    general form is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 MDS 目标的监督信号可能对表示学习者不够强，因此一些研究寻找来自多任务的其他监督信号。一般形式如下：
- en: '| (5) |  | $\small L_{Mul}=L_{Summ}+L_{Other},$ |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\small L_{Mul}=L_{Summ}+L_{Other},$ |  |'
- en: 'where $L_{Summ}$ is the loss function of MDS tasks, and $L_{Other}$ is the
    loss function of an auxiliary task. Angelidis et al. (Angelidis and Lapata, [2018](#bib.bib5))
    assumed that the aspect-relevant words not only provides a reasonable basis for
    model aspect reconstruction, but also a good indicator for product domain. Similarly,
    multi-task classification was introduced by Cao et al. (Cao et al., [2017](#bib.bib19)).
    Two models are maintained: text classification and text summarization models.
    In the first model, CNN is used to classify text categories and cross-entropy
    loss is used as the objective function. The summarization model and the text classification
    model share parameters and pooling operations, so are equivalent to the shared
    document vector representation. Coavoux et al. (Coavoux et al., [2019](#bib.bib33))
    jointly optimized the model from a language modeling objective and two other multi-task
    supervised classification losses, which are polarity loss and aspect loss.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{Summ}$ 是 MDS 任务的损失函数，$L_{Other}$ 是辅助任务的损失函数。Angelidis 等人（Angelidis 和
    Lapata, [2018](#bib.bib5)）假设，相关的词汇不仅为模型的方面重建提供了合理的基础，而且是产品领域的良好指标。同样，Cao 等人（Cao
    et al., [2017](#bib.bib19)）引入了多任务分类。维持两个模型：文本分类模型和文本摘要模型。在第一个模型中，使用 CNN 来分类文本类别，并使用交叉熵损失作为目标函数。摘要模型和文本分类模型共享参数和池化操作，因此相当于共享的文档向量表示。Coavoux
    等人（Coavoux et al., [2019](#bib.bib33)）从语言建模目标和另外两个多任务监督分类损失（极性损失和方面损失）共同优化模型。
- en: 4.6\. Other Types of Objectives
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6. 其他类型的目标
- en: 'There are many other types of objectives in addition to those mentioned above.
    Cao et al. (Cao et al., [2015b](#bib.bib21)) proposed using ROUGE-2 to calculate
    the sentence saliency scores and the model tries to estimate this saliency with
    linear regression. Yin et al. (Yin and Pei, [2015](#bib.bib169)) suggested summing
    the squares of the prestige vectors calculated by the PageRank algorithm to identify
    sentence importance. Zhang et al. (Zhang et al., [2016](#bib.bib179)) proposed
    an objective function by ensembling individual scores from multiple CNN models;
    besides the cross-entropy loss, a consensus objective is adopted to minimize disagreement
    between each pair of classifiers. Amplay et al. (Amplayo and Lapata, [2021](#bib.bib4))
    used two objectives in the abstract module: the first to optimize the generation
    probability distribution by maximizing the likelihood; and the second to constrain
    the model output to be close to its golden summary in the encoding space, as well
    as being distant from the random sampled negative summaries. Chu et al. (Chu and
    Liu, [2019](#bib.bib31)) designed a similarity objective that shares the encoder
    and decoder weights within the auto-encoder module, while in the summarization
    module, the average cosine distance indicates the similarity between the generated
    summary and the reviews. A variant similarity objective termed early cosine objective
    is further proposed to compute the similarity in a latent space which is the average
    of the cells states and hidden states to constrain the generated summaries semantically
    close to reviews.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述提到的目标，还有许多其他类型的目标。Cao 等人（Cao et al., [2015b](#bib.bib21)）提出使用 ROUGE-2 计算句子重要性评分，并通过线性回归来估计这一重要性。Yin
    等人（Yin 和 Pei, [2015](#bib.bib169)）建议通过 PageRank 算法计算的声望向量的平方和来确定句子的重要性。Zhang 等人（Zhang
    et al., [2016](#bib.bib179)）通过结合多个 CNN 模型的单独评分提出了一个目标函数；除了交叉熵损失，还采用了一个共识目标以最小化每对分类器之间的不一致性。Amplay
    等人（Amplayo 和 Lapata, [2021](#bib.bib4)）在摘要模块中使用了两个目标：第一个通过最大化似然来优化生成概率分布；第二个约束模型输出在编码空间中接近其黄金摘要，并且远离随机抽样的负摘要。Chu
    等人（Chu 和 Liu, [2019](#bib.bib31)）设计了一个相似性目标，该目标在自编码模块中共享编码器和解码器的权重，而在摘要模块中，平均余弦距离表示生成摘要与评论之间的相似性。进一步提出了一种变体相似性目标，称为早期余弦目标，以计算潜在空间中的相似性，即单元状态和隐藏状态的平均值，以约束生成的摘要在语义上接近评论。
- en: 4.7\. Discussion
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7. 讨论
- en: In MDS, cross-entropy is the most commonly adopted objective function that bridges
    the predicted candidate summaries and the golden summaries by treating the golden
    summaries as strong supervision signals. However, adopting cross-entropy loss
    alone may not lead the model to achieve good performance since the supervisory
    signal for cross-entropy objective is not strong enough by itself to effectively
    learn good representation. Several other objectives can thus serve as complements,
    e.g., reconstruction objectives offer a view from the unsupervised learning perspective;
    the redundancy objective constrains models from generating redundant content;
    while max-margin objectives require a step-change improvements from previous versions.
    By using multiple objectives, model optimization could be conducted with the input
    documents themselves if the manual annotation is scarce. The models that adopt
    multi-task objectives explicitly define multiple auxiliary tasks to assist the
    main summarization task for better generalization, and provide various constraints
    from different angles that lead to better model optimization.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDS中，交叉熵是最常用的目标函数，通过将黄金摘要视为强监督信号来桥接预测候选摘要和黄金摘要。然而，单独采用交叉熵损失可能无法使模型获得良好的性能，因为交叉熵目标的监督信号本身并不足够强大，不能有效地学习良好的表示。因此，其他一些目标可以作为补充，例如，重建目标提供了一种无监督学习的视角；冗余目标约束模型生成冗余内容；而最大边际目标要求从以前的版本中进行步骤性的改进。通过使用多种目标，如果手动注释稀缺，模型优化可以使用输入文档本身。采用多任务目标的模型显式地定义多个辅助任务，以帮助主要摘要任务实现更好的泛化，并从不同角度提供各种约束，从而实现更好的模型优化。
- en: 5\. Evaluation metrics
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 评价指标
- en: 'Evaluation metrics are used to measure the effectiveness of a given method
    objectively, so well-defined evaluation metrics are crucial to MDS research. We
    classify the existing evaluation metrics in two categories and will discuss each
    category in detail: (1) ROUGE: the most commonly used evaluation metrics in the
    summarization community; and (2) other evaluation metrics that have not been widely
    used in MDS research to date.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 评价指标用于客观地衡量给定方法的有效性，因此明确定义的评价指标对MDS研究至关重要。我们将现有的评价指标分为两类，并将详细讨论每一类：（1）ROUGE：在摘要社区中最常用的评价指标；（2）其他在MDS研究中尚未广泛使用的评价指标。
- en: 5.1\. ROUGE
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. ROUGE
- en: Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, [2004](#bib.bib92))
    is a collection of evaluation indicators that is one of the most essential metrics
    for many natural language processing tasks, including machine translation and
    text summarization. ROUGE obtains prediction/ground-truth similarity scores through
    comparing automatically generated summaries with a set of corresponding human-written
    references. ROUGE has many variants to measure candidate abstracts in a variety
    of ways (Lin, [2004](#bib.bib92)). The most commonly used ones are ROUGE-N and
    ROUGE-L.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 面向回忆的概括评价（ROUGE）（Lin, [2004](#bib.bib92)）是一组评价指标，是许多自然语言处理任务中最重要的指标之一，包括机器翻译和文本摘要。ROUGE通过将自动生成的摘要与一组对应的人工撰写的参考摘要进行比较，从而获得预测/真实值的相似性评分。ROUGE有许多变体，用于以不同方式衡量候选摘要（Lin,
    [2004](#bib.bib92)）。最常用的有ROUGE-N和ROUGE-L。
- en: 'ROUGE-N (ROUGE with n-gram co-occurrence statistics ) measures a n-gram recall
    between reference summaries and their corresponding candidate summaries (Lin,
    [2004](#bib.bib92)). Formally, ROUGE-N can be calculated as:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE-N（带有n-gram共现统计的ROUGE）衡量参考摘要与其对应的候选摘要之间的n-gram召回（Lin, [2004](#bib.bib92)）。形式上，ROUGE-N可以计算为：
- en: '| (6) |  | $\small ROUGE\text{-}N=\frac{\sum_{Sum\in\{Ref\}}\sum_{gram_{n}\in
    Sum}Count_{match}(gram_{n})}{\sum_{Sum\in\{Ref\}}\sum_{gram_{n}\in Sum}Count(gram_{n})},$
    |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\small ROUGE\text{-}N=\frac{\sum_{Sum\in\{Ref\}}\sum_{gram_{n}\in
    Sum}Count_{match}(gram_{n})}{\sum_{Sum\in\{Ref\}}\sum_{gram_{n}\in Sum}Count(gram_{n})},$
    |  |'
- en: where $Ref$ and $Sum$ are reference summary and machine-generated summary, $n$
    represents the length of n-gram, and $Count_{match}(gram_{n})$ represents the
    maximum number of n-grams in the reference summary and corresponding candidates.
    The numerator of ROUGE-N is the number of n-grams owned by both the reference
    and generated summary, while the denominator is the total number of n-grams occurring
    in the golden summary. The denominator could also be set to the number of candidate
    summary n-grams to measure precision; however, ROUGE-N mainly focuses on quantifying
    recall, so precision is not usually calculated. ROUGE-1 and ROUGE-2 are special
    cases of ROUGE-N that are usually chosen as best practices and represent the unigram
    and bigram, respectively.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Ref$ 和 $Sum$ 分别是参考摘要和机器生成摘要，$n$ 代表 n-gram 的长度，$Count_{match}(gram_{n})$
    代表参考摘要和对应候选摘要中的 n-gram 的最大数量。ROUGE-N 的分子是参考摘要和生成摘要中共有的 n-gram 数量，而分母是黄金摘要中出现的
    n-gram 的总数。分母也可以设置为候选摘要 n-grams 的数量以测量精度；然而，ROUGE-N 主要关注量化召回率，因此通常不计算精度。ROUGE-1
    和 ROUGE-2 是 ROUGE-N 的特例，通常被作为最佳实践，分别代表 unigram 和 bigram。
- en: 'ROUGE-L (ROUGE with Longest Common Subsequence) adopts the longest common subsequence
    algorithm to count the longest matching vocabularies (Lin, [2004](#bib.bib92)).
    Formally, ROUGE-L is calculated using:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE-L（ROUGE 与最长公共子序列）采用最长公共子序列算法来计算最长匹配的词汇（Lin，[2004](#bib.bib92)）。形式上，ROUGE-L
    使用以下公式计算：
- en: '| (7) |  | $\small F_{lcs}=\frac{(1+\beta^{2})R_{lcs}P_{lcs}}{R_{lcs}+\beta^{2}P_{lcs}},$
    |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\small F_{lcs}=\frac{(1+\beta^{2})R_{lcs}P_{lcs}}{R_{lcs}+\beta^{2}P_{lcs}},$
    |  |'
- en: where
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '| (8) |  | $\small R_{lcs}=\frac{LCS(Ref,Sum)}{m},$ |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\small R_{lcs}=\frac{LCS(Ref,Sum)}{m},$ |  |'
- en: and
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '| (9) |  | $\small P_{lcs}=\frac{LCS(Ref,Sum)}{n}.$ |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\small P_{lcs}=\frac{LCS(Ref,Sum)}{n}.$ |  |'
- en: where LCS($\cdot$) represents the longest common subsequence function. ROUGE-L
    is termed as LCS-based F-measure as it is obtained from LCS-Precision $P_{lcs}$
    and LCS-Recall $R_{lcs}$. $\beta$ is the balance factor between $R_{lcs}$ and
    $P_{lcs}$. It can be set by the fraction of $P_{lcs}$ and $R_{lcs}$; by setting
    $\beta$ to a big number, only $R_{lcs}$ is considered. The use of ROUGE-L enables
    measurement of the similarity of two text sequences at sentence-level. ROUGE-L
    also has the advantage of automatically deciding the n-gram without extra manual
    input, since the calculation of LCS empowers the model to count grams adaptively.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 LCS($\cdot$) 表示最长公共子序列函数。ROUGE-L 被称为基于 LCS 的 F 测量，因为它是由 LCS-精度 $P_{lcs}$
    和 LCS-召回率 $R_{lcs}$ 得到的。$\beta$ 是 $R_{lcs}$ 和 $P_{lcs}$ 之间的平衡因子。通过设置 $\beta$ 为一个大数字，只考虑
    $R_{lcs}$。使用 ROUGE-L 可以在句子级别上衡量两个文本序列的相似度。ROUGE-L 还具有自动决定 n-gram 的优点，无需额外的手动输入，因为
    LCS 的计算使模型能够自适应地计数 grams。
- en: Other ROUGE Based Metrics. ROUGE-W (Lin, [2004](#bib.bib92)) is proposed to
    weight consecutive matches to better measure semantic similarities between two
    texts. ROUGE-S (Lin, [2004](#bib.bib92)) stands for ROUGE with Skip-bigram co-occurrence
    statistics that allows the bigram to skip arbitrary words. An extension of ROUGE-S,
    ROUGE-SU (Lin, [2004](#bib.bib92)) refers to ROUGE with Skip-bigram plus Unigram-based
    co-occurrence statistics and is able to be obtained from ROUGE-S by adding a begin-of-sentence
    token at the start of both references and candidates. ROUGE-WE (Ng and Abrecht,
    [2015](#bib.bib117)) is proposed to further extend ROUGE by measuring the pair-wise
    summary distances in word embeddings space. In recent years, more ROUGE-based
    evaluation models have been proposed to compare golden and machine-generated summaries,
    not just according to their literal similarity, but also considering semantic
    similarity (ShafieiBavani et al., [2018](#bib.bib138); Zhao et al., [2019](#bib.bib181);
    Zhang et al., [2020a](#bib.bib176)). In terms of the ROUGE metric for multiple
    golden summaries, the Jackknifing procedure (similar to K-fold validation) has
    been introduced (Lin, [2004](#bib.bib92)). The $M$ best scores are computed from
    sets composed of $M$-1 reference summaries and the final ROUGE-N is the average
    of $M$ scores. This procedure can also be applied to ROUGE-L, ROUGE-W and ROUGE-S.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 其他基于ROUGE的指标。ROUGE-W（Lin，[2004](#bib.bib92)）提出了加权连续匹配，以更好地测量两个文本之间的语义相似性。ROUGE-S（Lin，[2004](#bib.bib92)）代表了ROUGE带有跳跃二元组共现统计，这允许二元组跳过任意词。ROUGE-S的扩展，ROUGE-SU（Lin，[2004](#bib.bib92)）指的是带有跳跃二元组和基于单字的共现统计的ROUGE，并且可以通过在所有参考文献和候选文本的开头添加一个句子开始标记来从ROUGE-S获得。ROUGE-WE（Ng和Abrecht，[2015](#bib.bib117)）提出了进一步扩展ROUGE的方法，通过测量词嵌入空间中的配对摘要距离。近年来，更多的基于ROUGE的评估模型被提出，不仅仅根据字面相似性来比较黄金和机器生成的摘要，还考虑语义相似性（ShafieiBavani
    et al., [2018](#bib.bib138)；Zhao et al., [2019](#bib.bib181)；Zhang et al., [2020a](#bib.bib176)）。在多个黄金摘要的ROUGE指标方面，引入了Jackknifing程序（类似于K折验证）（Lin，[2004](#bib.bib92)）。$M$个最佳得分是从由$M$-1个参考摘要组成的集合中计算得出的，最终的ROUGE-N是$M$个得分的平均值。此程序也可以应用于ROUGE-L、ROUGE-W和ROUGE-S。
- en: 5.2\. Other Evaluation Metrics
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 其他评估指标
- en: 'Besides ROUGE-based (Lin, [2004](#bib.bib92)) metrics, other evaluation metrics
    for MDS exist, but have received less attention than ROUGE. We hope this section
    will give researchers and practitioners a holistic view of alternative evaluation
    metrics in this field. Based on the mode of summaries matching, we divide the
    evaluation metrics into two groups: lexical matching metrics and semantic matching
    metrics.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于ROUGE的（Lin，[2004](#bib.bib92)）指标，还有其他MDS的评估指标，但它们比ROUGE受到的关注较少。我们希望这一部分能为研究人员和从业者提供关于该领域替代评估指标的全面视角。根据摘要匹配的模式，我们将评估指标分为两组：词汇匹配指标和语义匹配指标。
- en: Table 3\. Advantages and disadvantages of different evaluation metrics.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 不同评估指标的优缺点。
- en: '| Evaluation Metrics | Advantages | Disadvantages |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 评估指标 | 优点 | 缺点 |'
- en: '| --- | --- | --- |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Lexical Matching Metrics | ROUGE |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 词汇匹配指标 | ROUGE |'
- en: '&#124; •  Widely used &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 广泛使用的&#124;'
- en: '&#124; •  Intuitive &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 直观的&#124;'
- en: '&#124; •  Easily computed &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 易于计算&#124;'
- en: '|'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Cannot measure texts &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 无法测量文本&#124;'
- en: '&#124;     semantically &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     语义上的&#124;'
- en: '&#124; •  Exact matching &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 精确匹配&#124;'
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| BLEU |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| BLEU |'
- en: '&#124; •  Intuitive &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 直观的&#124;'
- en: '&#124; •  Easily computed &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 易于计算&#124;'
- en: '&#124; •  High correlations with &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 高度相关于&#124;'
- en: '&#124;     human judgments &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     人工判断&#124;'
- en: '|'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Cannot measure texts &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 无法测量文本&#124;'
- en: '&#124;     semantically &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     语义上的&#124;'
- en: '&#124; •  Cannot deal with &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 无法处理&#124;'
- en: '&#124;     languages lacking word &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     缺乏词汇的语言&#124;'
- en: '&#124;     boundaries &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     边界&#124;'
- en: '|'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Perplexity |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度 |'
- en: '&#124; •  Easily computed &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 易于计算&#124;'
- en: '&#124; •  Intuitive &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 直观的&#124;'
- en: '|'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Sensitive to certain &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 对某些&#124;'
- en: '&#124;     symbols and words &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     符号和词汇&#124;'
- en: '|'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Pyramid |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 金字塔 |'
- en: '&#124; •  High correlations with &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 高度相关于&#124;'
- en: '&#124;     human judgments &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     人工判断&#124;'
- en: '|'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Requires manually &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 需要手动&#124;'
- en: '&#124;     extraction of units &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     单元的提取&#124;'
- en: '&#124; •  Bias results easily &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 容易产生偏差结果&#124;'
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Responsiveness |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 响应性 |'
- en: '&#124; •  Consider both content and &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 需要考虑内容和&#124;'
- en: '&#124;     linguistic quality &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     语言质量&#124;'
- en: '&#124; •  Can be calculated without &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  可以在不依赖于 &#124;'
- en: '&#124;     reference &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     参考 &#124;'
- en: '| •  Not widely adopted |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| •  使用不广泛 |'
- en: '|'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Data &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据 &#124;'
- en: '&#124; Statistics &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 统计 &#124;'
- en: '|'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Can measure the density &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  可以测量密度 &#124;'
- en: '&#124;     and coverage of summary &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     以及摘要的覆盖范围 &#124;'
- en: '|'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Cannot measure texts &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  不能测量文本 &#124;'
- en: '&#124;     semantically &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     语义上 &#124;'
- en: '|'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Semantic Matching Metrics | MEREOR | •  Consider non-exact matching |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 语义匹配度量 | MEREOR | •  考虑非精确匹配 |'
- en: '&#124; •  Sensitive to length &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  对长度敏感 &#124;'
- en: '|'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SUPERT |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| SUPERT |'
- en: '&#124; •  Can measuring texts semantic &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  可以测量文本的语义 &#124;'
- en: '&#124;     similarity &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     相似度 &#124;'
- en: '| •  Not widely adopted |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| •  使用不广泛 |'
- en: '|'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Preferences &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 偏好 &#124;'
- en: '&#124; based Metric &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于度量 &#124;'
- en: '|'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Does not depend on the &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  不依赖于 &#124;'
- en: '&#124;     golden summaries &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     黄金摘要 &#124;'
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Require human &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  需要人工 &#124;'
- en: '&#124;     annotations &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     注释 &#124;'
- en: '|'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| BERTScore |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| BERTScore |'
- en: '&#124; •  Semantically measure texts to &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  从语义上测量文本 &#124;'
- en: '&#124;     some extent &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     某种程度上 &#124;'
- en: '&#124; •  Mimic human evaluation &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  模拟人类评估 &#124;'
- en: '|'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  High computational &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  高计算成本 &#124;'
- en: '&#124;     demands &#124;'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     需求 &#124;'
- en: '|'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MoverScore |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| MoverScore |'
- en: '&#124; •  Semantically measure texts to &#124;'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  从语义上测量文本 &#124;'
- en: '&#124;     some extent &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     某种程度上 &#124;'
- en: '&#124; •  More similar to human &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  更类似于人类 &#124;'
- en: '&#124;     evaluation by adopting earth &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     通过采用地球 &#124;'
- en: '&#124;     mover’s distance &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     mover’s 距离 &#124;'
- en: '|'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  High computational &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  高计算成本 &#124;'
- en: '&#124;     demands &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     需求 &#124;'
- en: '|'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Importance |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 重要性 |'
- en: '&#124; •  Combining redundancy, &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  结合冗余 &#124;'
- en: '&#124;     relevance and informativeness &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     相关性和信息量 &#124;'
- en: '&#124; •  Theoretically supported &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  理论上支持 &#124;'
- en: '|'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Non-trivial for &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  对 &#124;'
- en: '&#124;     implementation &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     实施 &#124;'
- en: '|'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Human &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人工 &#124;'
- en: '&#124; Evaluation &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估 &#124;'
- en: '|'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Can accurately and &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  可以准确测量 &#124;'
- en: '&#124;     semantically measure texts &#124;'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     从语义上测量文本 &#124;'
- en: '|'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Require human &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  需要人工 &#124;'
- en: '&#124;     annotations &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     注释 &#124;'
- en: '|'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Lexical Matching Metrics. BLEU (Papineni et al., [2002](#bib.bib120)) is a
    commonly used vocabulary-based evaluation metric that provides a precision-based
    evaluation indicator, as opposed to ROUGE that mainly focuses on recall. Perplexity
    (Jelinek et al., [1977](#bib.bib70)) is used to evaluate the quality of the language
    model by calculating the negative log probability of a word’s appearance. A low
    perplexity on a test dataset is a strong indicator of a summary’s high grammatical
    quality because it measures the probability of words appearing in sequences. Based
    on Pyramid (Nenkova et al., [2007](#bib.bib115)) calculation, the abstract sentences
    are manually divided into several Summarization Content Units (SCUs), each representing
    a core concept formed from a single word or phrase/sentence. After sorting SCUs
    in order of importance to form the Pyramid, the quality of automatic summarization
    is evaluated by calculating the number and importance of SCUs included in the
    document (Nenkova and Passonneau, [2004](#bib.bib116)). Intuitively, more important
    SCUs exist at higher levels of the pyramid. Although Pyramid shows strong correlation
    with human judgment, it requires professional annotations to match and evaluate
    SCUs in generated and golden summaries. Some recent works focus on the construction
    of Pyramid (Passonneau et al., [2013](#bib.bib121); Yang et al., [2016](#bib.bib164);
    Hirao et al., [2018](#bib.bib61); Gao et al., [2019](#bib.bib48); Shapira et al.,
    [2019](#bib.bib140)). Responsiveness (Louis and Nenkova, [2013](#bib.bib97)) measures
    content selection and linguistic quality of summaries by directly rating scores.
    Additionally, the assessments are calculated without reference to model summaries.
    Data Statistics (Grusky et al., [2018](#bib.bib55)) contain three evaluation metrics:
    extractive fragment coverage measures the novelty of generated summaries by calculating
    the percentage of words in the summary that are also present in source documents;
    extractive fragment density measures the average length of the extractive block
    to which each word in the summary belongs; and compression ratio compares the
    word numbers in the source documents and generated summary.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇匹配指标。**BLEU**（Papineni et al., [2002](#bib.bib120)）是一个常用的基于词汇的评估指标，它提供了基于精确度的评价标准，而ROUGE则主要关注召回率。**Perplexity**（Jelinek
    et al., [1977](#bib.bib70)）用于通过计算单词出现的负对数概率来评估语言模型的质量。测试数据集上的低困惑度是摘要语法质量高的强指示，因为它测量了单词在序列中出现的概率。基于**Pyramid**（Nenkova
    et al., [2007](#bib.bib115)）的计算，将摘要句子手动划分为若干个总结内容单元（SCUs），每个SCU代表一个核心概念，形成自单词或短语/句子。将SCUs按重要性排序形成金字塔后，通过计算文档中包含的SCUs的数量和重要性来评估自动摘要的质量（Nenkova
    和 Passonneau, [2004](#bib.bib116)）。直观地说，更重要的SCUs存在于金字塔的更高层级。尽管Pyramid与人工判断具有较强的相关性，但它需要专业的标注来匹配和评估生成的摘要与标准摘要中的SCUs。一些最近的研究集中在Pyramid的构建上（Passonneau
    et al., [2013](#bib.bib121)；Yang et al., [2016](#bib.bib164)；Hirao et al., [2018](#bib.bib61)；Gao
    et al., [2019](#bib.bib48)；Shapira et al., [2019](#bib.bib140)）。**Responsiveness**（Louis
    和 Nenkova, [2013](#bib.bib97)）通过直接评分来衡量摘要的内容选择和语言质量。此外，评估是计算时不参考模型摘要的。**Data Statistics**（Grusky
    et al., [2018](#bib.bib55)）包含三个评估指标：**extractive fragment coverage** 衡量生成摘要的新颖性，通过计算摘要中出现且也存在于源文档中的单词百分比；**extractive
    fragment density** 衡量每个单词所属的提取块的平均长度；**compression ratio** 比较源文档和生成摘要中的单词数量。
- en: 'Semantic Matching Metrics. METEOR (Metric for Evaluation of Translation with
    Explicit Ordering) (Banerjee and Lavie, [2005](#bib.bib10)) is an improvement
    to BLEU. The main idea behind METEOR is that while candidate summaries can be
    correct with similar meanings, they are not exactly matched with references. In
    such a case, WordNet^(10)^(10)10https://wordnet.princeton.edu/ is introduced to
    expand the synonym set, and the word form is also taken into account. SUPERT (Gao
    et al., [2020](#bib.bib49)) is an unsupervised MDS evaluation metric that measures
    the semantic similarity between the pseudo-reference summary and the machine-generated
    summary. SUPERT obviates the need for human annotations by not referring to golden
    summaries. Contextualized embeddings and soft token alignment techniques are leveraged
    to select salient information from the input documents to evaluate summary quality.
    Preferences based Metric (Zopf, [2018](#bib.bib189)) is a pairwise sentence preference-based
    evaluation model and it does not depend on the golden summaries. The underlying
    premise is to ask annotators about their pair-wise preferences rather than writing
    complex golden summaries, and are much easier and faster to obtain than traditional
    reference summary-based evaluation models. BERTScore (Zhang et al., [2020a](#bib.bib176))
    computes a similarity score for each token within the candidate sentence and the
    reference sentence. It measures the soft overlap of two texts’ BERT embeddings.
    MoverScore (Zhao et al., [2019](#bib.bib181)) adopts a distance to evaluate the
    agreement between two texts in the context of BERT and ELMo word embeddings. This
    proposed metric has a high correlation with human judgment of text quality by
    adopting earth mover’s distance. Importance (Peyrard, [2019](#bib.bib126)) is
    a simple but rigorous evaluation metric from the aspect of information theory.
    It is a final indicator calculated from the three aspects: Redundancy, Relevance,
    and Informativeness. A good summary should have low Redundancy and high Relevance
    and high Informativeness. The cluster of Human Evaluation is used to supplement
    automatic evaluation on relatively small instances. Annotators evaluate the quality
    of machine-generated summaries by rating Informativeness, Fluency, Conciseness,
    Readability, Relevance. Model ratings are usually computed by averaging the rating
    on all selected summary pairs.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 语义匹配指标。METEOR（Metric for Evaluation of Translation with Explicit Ordering）（Banerjee
    和 Lavie，[2005](#bib.bib10)）是对BLEU的改进。METEOR的主要思想是，虽然候选摘要的意思可能正确，但与参考文献并不完全匹配。在这种情况下，引入了WordNet^(10)^(10)10https://wordnet.princeton.edu/
    来扩展同义词集，并且还考虑了词形变化。SUPERT（Gao 等，[2020](#bib.bib49)）是一个无监督的MDS评估指标，用于测量伪参考摘要与机器生成摘要之间的语义相似度。SUPERT通过不参考黄金摘要而省去了人工注释的需要。利用上下文化的嵌入和软令牌对齐技术，从输入文档中选择显著信息以评估摘要质量。基于偏好的指标（Zopf，[2018](#bib.bib189)）是一种基于成对句子偏好的评估模型，它不依赖于黄金摘要。其基本前提是询问注释者他们的成对偏好，而不是编写复杂的黄金摘要，这比传统的参考摘要评估模型更容易且更快获得。BERTScore（Zhang
    等，[2020a](#bib.bib176)）计算候选句子和参考句子中每个词的相似度得分。它测量两个文本BERT嵌入的软重叠。MoverScore（Zhao
    等，[2019](#bib.bib181)）采用距离来评估BERT和ELMo词嵌入背景下两个文本之间的一致性。这种提出的指标通过采用地球搬运工距离与人类文本质量判断有很高的相关性。重要性（Peyrard，[2019](#bib.bib126)）是一个简单但严谨的信息论角度的评估指标。它是从三个方面计算的最终指标：冗余度、相关性和信息量。一个好的摘要应该具有低冗余度、高相关性和高信息量。人工评估的聚集用于补充相对较小实例上的自动评估。注释者通过对信息量、流畅性、简洁性、可读性、相关性进行评分来评估机器生成的摘要质量。模型评分通常通过对所有选择的摘要对的评分进行平均计算。
- en: 5.3\. Discussion
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3. 讨论
- en: 'We summarize the advantages and disadvantages of above-mentioned evaluation
    metrics in Table [3](#S5.T3 "Table 3 ‣ 5.2\. Other Evaluation Metrics ‣ 5\. Evaluation
    metrics ‣ Multi-document Summarization via Deep Learning Techniques: A Survey").
    Although there are many evaluation metrics for MDS, the indicators of the ROUGE
    series are generally accepted by the summarization community. Almost all the research
    works utilize ROUGE for evaluation, while other evaluation indicators are just
    for assistance currently. Among the ROUGE family, ROUGE-1, ROUGE-2 and ROUGE-L
    are the most commonly used evaluation metrics. In addition, there are plenty of
    existing evaluation metrics in other natural language processing tasks that could
    be potentially adjusted for MDS tasks, such as efficiency, effectiveness and coverage
    from information retrieval.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[3](#S5.T3 "Table 3 ‣ 5.2\. Other Evaluation Metrics ‣ 5\. Evaluation metrics
    ‣ Multi-document Summarization via Deep Learning Techniques: A Survey")中总结了上述评估指标的优缺点。虽然针对MDS的评估指标有很多，但ROUGE系列的指标通常被总结社区接受。几乎所有的研究都使用ROUGE进行评估，而其他评估指标目前只是辅助使用。在ROUGE系列中，ROUGE-1、ROUGE-2和ROUGE-L是最常用的评估指标。此外，还有许多现有的自然语言处理任务中的评估指标可能被调整用于MDS任务，例如来自信息检索的效率、有效性和覆盖范围。'
- en: 6\. Datasets
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 数据集
- en: Compared to SDS tasks, large-scale MDS datasets, which contain more general
    scenarios with many downstream tasks, are relatively scarce. In this section,
    we present our investigation on the 10 most representative datasets commonly used
    for MDS and its variant tasks.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 与SDS任务相比，大规模MDS数据集（包含更多通用场景和许多下游任务）相对较少。在这一部分，我们展示了对10个最具代表性的数据集的调查，这些数据集通常用于MDS及其变体任务。
- en: 'DUC & TAC. DUC^(11)^(11)11http://duc.nist.gov/ (Document Understanding Conference)
    provides official text summarization competitions each year from 2001-2007 to
    promote summarization research. DUC changed its name to Text Analysis Conference
    (TAC)^(12)^(12)12 http://www.nist.gov/tac/ in 2008\. Here, the DUC datasets refer
    to the data collected from 2001-2007; the TAC datasets refer to the dataset after
    2008\. Both DUC and TAC datasets are from the news domains, including various
    topics such as politics, natural disaster and biography. Nevertheless, as shown
    in Table [4](#S6.T4 "Table 4 ‣ 6\. Datasets ‣ Multi-document Summarization via
    Deep Learning Techniques: A Survey"), the DUC and TAC datasets provide small datasets
    for model evaluation that only include hundreds of news documents and human-annotated
    summaries. Of note, the first sentence in a news item is usually information-rich
    that renders bias in the news datasets, so it fails to reflect the structure of
    natural documents in daily lives. These two datasets are on a relatively small
    scale and not ideal for large-scale deep neural based MDS model training and evaluation.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 'DUC与TAC。DUC^(11)^(11)11http://duc.nist.gov/（文档理解会议）每年从2001年至2007年提供官方文本摘要竞赛，以促进摘要研究。DUC于2008年更名为文本分析会议（TAC）^(12)^(12)12
    http://www.nist.gov/tac/。在这里，DUC数据集指的是从2001年至2007年收集的数据；TAC数据集指的是2008年后的数据集。DUC和TAC数据集都来自新闻领域，涵盖政治、自然灾害和传记等各种主题。然而，如表[4](#S6.T4
    "Table 4 ‣ 6\. Datasets ‣ Multi-document Summarization via Deep Learning Techniques:
    A Survey")所示，DUC和TAC数据集提供的小规模数据集仅包含数百篇新闻文档和人工标注的摘要。值得注意的是，新闻项的第一句话通常信息丰富，这在新闻数据集中造成了偏差，因此无法反映日常生活中自然文档的结构。这两个数据集规模较小，不适合大规模深度神经网络MDS模型的训练和评估。'
- en: Table 4\. Comparison of Different Datasets. In the table, “Ave”, “Summ”, “Len”,
    “bus”,“rev” and “#” represent average, summary, length, business, reviews and
    numbers respectively; “Docs” and “sents” mean documents and sentences respectively.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 表4\. 不同数据集的比较。表中，“Ave”，“Summ”，“Len”，“bus”，“rev”和“#”分别表示平均值、总结、长度、商业、评论和数量；“Docs”和“sents”分别表示文档和句子。
- en: '| Datasets | Cluster # | Document # | Summ # | Ave Summ Len | Topic |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Datasets | Cluster # | Document # | Summ # | Ave Summ Len | Topic |'
- en: '| DUC01 | 30 | 309 docs | 60 summ | 100 words | News |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| DUC01 | 30 | 309 docs | 60 summ | 100 words | News |'
- en: '| DUC02 | 59 | 567 docs | 116 summ | 100 words | News |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| DUC02 | 59 | 567 docs | 116 summ | 100 words | News |'
- en: '| DUC03 | 30 | 298 docs | 120 summ | 100 words | News |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| DUC03 | 30 | 298 docs | 120 summ | 100 words | News |'
- en: '| DUC04 | 50 | 10 docs / cluster | 200 summ | 665 bytes | News |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| DUC04 | 50 | 10 docs / cluster | 200 summ | 665 bytes | News |'
- en: '| DUC05 | 50 | 25-50 docs / cluster | 140 summ | 250 words | News |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| DUC05 | 50 | 25-50 docs / cluster | 140 summ | 250 words | News |'
- en: '| DUC06 | 50 | 25 docs / cluster | 4 summ / cluster | 250 words | News |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| DUC06 | 50 | 25 docs / cluster | 4 summ / cluster | 250 words | News |'
- en: '| DUC07 | 45 | 25 docs / cluster | 4 summ / cluster | 250 words | News |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| DUC07 | 45 | 25 docs / cluster | 4 summ / cluster | 250 words | 新闻 |'
- en: '| TAC 2008 | 48 | 10 docs / cluster | 4 summ / cluster | 100 words | News |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| TAC 2008 | 48 | 10 docs / cluster | 4 summ / cluster | 100 words | 新闻 |'
- en: '| TAC 2009 | 44 | 10 docs / cluster | 4 summ / cluster | 100 words | News |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| TAC 2009 | 44 | 10 docs / cluster | 4 summ / cluster | 100 words | 新闻 |'
- en: '| TAC 2010 | 46 | 10 docs / cluster | 4 summ / cluster | 100 words | News |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| TAC 2010 | 46 | 10 docs / cluster | 4 summ / cluster | 100 words | 新闻 |'
- en: '| TAC 2011 | 44 | 10 docs / cluster | 4 summ / cluster | 100 words | News |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| TAC 2011 | 44 | 10 docs / cluster | 4 summ / cluster | 100 words | 新闻 |'
- en: '| OPOSUM | 60 | 600 rev | 1 summ / cluster | 100 words |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| OPOSUM | 60 | 600 评论 | 1 summ / cluster | 100 words |'
- en: '&#124; Amazon &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 亚马逊 &#124;'
- en: '&#124; reviews &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评论 &#124;'
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| WikiSum | - |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| WikiSum | - |'
- en: '&#124; train / val / test &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 / 验证 / 测试 &#124;'
- en: '&#124; 1579360 / 38144 / 38205 &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1579360 / 38144 / 38205 &#124;'
- en: '| 1 summ / cluster | 139.4 tokens/ summ | Wikipedia |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 1 summ / cluster | 139.4 tokens/ summ | 维基百科 |'
- en: '| Multi-News | - |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| Multi-News | - |'
- en: '&#124; train / val / test &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 / 验证 / 测试 &#124;'
- en: '&#124; 44972 / 5622 / 5622 &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 44972 / 5622 / 5622 &#124;'
- en: '&#124; 2-10 docs / cluster &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2-10 docs / cluster &#124;'
- en: '| 1 summ / cluster |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 1 summ / cluster |'
- en: '&#124; 263.66 words / summ &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 263.66 words / summ &#124;'
- en: '&#124; 9.97 sents / summ &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 9.97 sents / summ &#124;'
- en: '&#124; 262 tokens / summ &#124;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 262 tokens / summ &#124;'
- en: '| News |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 新闻 |'
- en: '| Opinosis | 51 | 6457 rev | 5 summ / cluster | - |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| Opinosis | 51 | 6457 评论 | 5 summ / cluster | - |'
- en: '&#124; Site &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 站点 &#124;'
- en: '&#124; reviews &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评论 &#124;'
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Rotten &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Rotten &#124;'
- en: '&#124; Tomatoes &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 西红柿 &#124;'
- en: '| 3731 | 99.8 rev / cluster | 1 summ / cluster | 19.6 tokens / summ |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 3731 | 99.8 评论 / cluster | 1 summ / cluster | 19.6 tokens / summ |'
- en: '&#124; Movie &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 电影 &#124;'
- en: '&#124; reviews &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评论 &#124;'
- en: '|'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Yelp | - |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| Yelp | - |'
- en: '&#124; train / val / test &#124;'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 / 验证 / 测试 &#124;'
- en: '&#124; bus: 10695 / 1337 / 1337 &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; bus: 10695 / 1337 / 1337 &#124;'
- en: '&#124; rev: 1038184 / 129856 / 129840 &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评论: 1038184 / 129856 / 129840 &#124;'
- en: '| - | - |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| - | - |'
- en: '&#124; Customer &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 客户 &#124;'
- en: '&#124; reviews &#124;'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评论 &#124;'
- en: '|'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Scisumm | 1000 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| Scisumm | 1000 |'
- en: '&#124; 21 - 928 cites / paper &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 21 - 928 cites / paper &#124;'
- en: '&#124; 15 sents / refer &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 15 sents / refer &#124;'
- en: '| 1 summ / cluster | 151 words |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 1 summ / cluster | 151 words |'
- en: '&#124; Science &#124;'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 科学 &#124;'
- en: '&#124; Paper &#124;'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 论文 &#124;'
- en: '|'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| WCEP | 10200 | 235 docs / cluster | 1 summ / cluster | 32 words | Wikipedia
    |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| WCEP | 10200 | 235 docs / cluster | 1 summ / cluster | 32 words | 维基百科 |'
- en: '| Multi-XScience | - |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| Multi-XScience | - |'
- en: '&#124; train / val / test &#124;'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 / 验证 / 测试 &#124;'
- en: '&#124; 30369 / 5066 / 5093 &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 30369 / 5066 / 5093 &#124;'
- en: '| 1 summ / cluster |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 1 summ / cluster |'
- en: '&#124; 116.44 words / summ &#124;'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 116.44 words / summ &#124;'
- en: '|'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Science &#124;'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 科学 &#124;'
- en: '&#124; Paper &#124;'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 论文 &#124;'
- en: '|'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: OPOSUM. OPOSUM (Angelidis and Lapata, [2018](#bib.bib5)) collects multiple reviews
    of six product domains from Amazon. This dataset not only contains multiple reviews
    and corresponding summaries but also products’ domain and polarity information.
    The latter information could be used as the auxiliary supervision signals.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: OPOSUM。OPOSUM（Angelidis和Lapata，[2018](#bib.bib5)）收集了亚马逊上六个产品领域的多个评论。该数据集不仅包含多个评论和相应的摘要，还包含产品领域和极性信息。后者的信息可用作辅助监督信号。
- en: WikiSum. WikiSum (Liu et al., [2018](#bib.bib94)) targets abstractive MDS. For
    a specific Wikipedia theme, the documents cited in Wikipedia articles or the top-10
    Google search results (using the Wikipedia theme as query) are seen as the source
    documents. Golden summaries are the real Wikipedia articles. However, some of
    the URLs are not available and can be identical to each other in parts. To remedy
    these problems, Liu et al. (Liu and Lapata, [2019](#bib.bib95)) cleaned the dataset
    and deleted duplicated examples, so here we report statistical results from (Liu
    and Lapata, [2019](#bib.bib95)).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: WikiSum。WikiSum（刘等， [2018](#bib.bib94)）旨在进行抽象化MDS。对于特定的维基百科主题，维基百科文章中引用的文献或使用维基百科主题作为查询的前10个Google搜索结果被视为源文档。黄金摘要是真实的维基百科文章。然而，一些网址不可用，并且在部分内容上可能重复。为了解决这些问题，刘等（刘和Lapata，[2019](#bib.bib95)）清理了数据集并删除了重复的例子，因此我们报告了（刘和Lapata，[2019](#bib.bib95)）中的统计结果。
- en: Multi-News. Multi-News (Fabbri et al., [2019](#bib.bib44)) is a relatively large-scale
    dataset in the news domain; the articles and human-written summaries are all from
    the Web^(13)^(13)13http://newser.com. This dataset includes 56,216 article-summary
    pairs and contain trace-back links to the original documents. Moreover, the authors
    compared the Multi-News dataset with prior datasets in terms of coverage, density,
    and compression, revealing that this dataset has various arrangement styles of
    sequences.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: Multi-News。Multi-News（Fabbri 等，[2019](#bib.bib44)）是新闻领域中一个相对大规模的数据集；这些文章和人工编写的摘要都来自
    Web^(13)^(13)13http://newser.com。该数据集包含 56,216 对文章-摘要，并包含追溯到原始文档的链接。此外，作者在覆盖范围、密度和压缩方面将
    Multi-News 数据集与先前的数据集进行了比较，揭示了该数据集具有各种序列排列风格。
- en: Opinosis. The Opinosis dataset (Ganesan et al., [2010](#bib.bib47)) contains
    reviews of 51 topic clusters collected from TripAdvisor^(14)^(14)14https://www.tripadvisor.com/,
    Amazon^(15)^(15)15https://www.amazon.com.au/, and Edmunds^(16)^(16)16https://www.edmunds.com/.
    For each topic, approximately 100 sentences on average are provided and the reviews
    are fetched from different sources. For each cluster, five professional written
    golden summaries are provided for the model training and evaluation.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: Opinosis。Opinosis 数据集（Ganesan 等，[2010](#bib.bib47)）包含了从 TripAdvisor^(14)^(14)14https://www.tripadvisor.com/、Amazon^(15)^(15)15https://www.amazon.com.au/
    和 Edmunds^(16)^(16)16https://www.edmunds.com/ 收集的 51 个主题集群的评论。每个主题平均提供约 100 个句子，评论来自不同的来源。对于每个集群，提供了五个专业编写的黄金摘要，用于模型训练和评估。
- en: Rotten Tomatoes. The Rotten Tomatoes dataset (Wang and Ling, [2016](#bib.bib158))
    consists of the collected reviews of 3,731 movies from the Rotten Tomato website^(17)^(17)17http://rottentomatoes.com.
    The reviews contain both professional critics and user comments. For each movie,
    a one-sentence summary is created by professional editors.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: Rotten Tomatoes。Rotten Tomatoes 数据集（Wang 和 Ling，[2016](#bib.bib158)）包含了来自 Rotten
    Tomato 网站^(17)^(17)17http://rottentomatoes.com 的 3,731 部电影的评论。评论包括专业评论和用户评论。对于每部电影，专业编辑创建了一个一句话的总结。
- en: Yelp. Chu et al. (Chu and Liu, [2019](#bib.bib31)) proposed a dataset named
    Yelp based on the Yelp Dataset Challenge. This dataset includes multiple customer
    reviews with five-star ratings. The authors provided 100 manual-written summaries
    for model evaluation using Amazon Mechanical Turk (AMT), within which every eight
    input reviews are summarized into one golden summary.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: Yelp。Chu 等人（Chu 和 Liu，[2019](#bib.bib31)）提出了一个基于 Yelp 数据集挑战的数据集，名为 Yelp。该数据集包括多个带有五星评级的客户评价。作者使用
    Amazon Mechanical Turk（AMT）提供了 100 个人工编写的摘要用于模型评估，每八个输入评论被总结成一个黄金摘要。
- en: Scisumm. Scisumm dataset (Yasunaga et al., [2019](#bib.bib167)) is a large,
    manually annotated corpus for scientific document summarization. The input documents
    are a scientific publication, called the reference paper, and multiple sentences
    from the literature that cite this reference paper. In the SciSumm dataset, the
    1,000 most cited papers from the ACL Anthology Network (Radev et al., [2013](#bib.bib129))
    are treated as reference papers, and an average 15 citation sentences are provided
    after cleaning. For each cluster, one golden summary is created by five NLP-based
    Ph.D. students or equivalent professionals.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: Scisumm。Scisumm 数据集（Yasunaga 等，[2019](#bib.bib167)）是一个大型的人工注释的科学文献总结语料库。输入文档是科学出版物，即参考论文，以及来自文献中引用该参考论文的多个句子。在
    SciSumm 数据集中，来自 ACL Anthology Network（Radev 等，[2013](#bib.bib129)）的 1,000 篇最常被引用的论文被视为参考论文，经过清理后提供了平均
    15 个引用句子。对于每个集群，由五位基于 NLP 的博士生或等效专业人员创建一个黄金摘要。
- en: WCEP. The Wikipedia Current Events Portal dataset (WCEP) (Ghalandari et al.,
    [2020](#bib.bib51)) contains human-written summaries of recent news events. Similar
    articles are provided by searching similar articles from Common Crawl News dataset^(18)^(18)18https://commoncrawl.org/2016/10/news-dataset-available/
    to extend the inputs to obtain large-scale news articles. Overall, the WCEP dataset
    has good alignment with the real-world industrial use cases.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: WCEP。维基百科时事门户数据集（WCEP）（Ghalandari 等，[2020](#bib.bib51)）包含了由人工编写的最近新闻事件的摘要。类似的文章可以通过搜索来自
    Common Crawl News 数据集的类似文章来获得，以扩展输入，获得大规模新闻文章。总体而言，WCEP 数据集与实际的工业用例对齐良好。
- en: Multi-XScience. The source data of Multi-XScience (Lu et al., [2020](#bib.bib98))
    are from Arxiv and Microsoft academic graphs and this dataset is suitable for
    abstractive MDS. Multi-XScience contains fewer positional and extractive biases
    than WikiSum and Multi-News datasets, so the drawback of obtaining higher scores
    from a copy sentence at a certain position can be partially avoided.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: Multi-XScience。Multi-XScience的数据源（Lu et al., [2020](#bib.bib98)）来自Arxiv和Microsoft学术图谱，这个数据集适合用于抽象MDS。与WikiSum和Multi-News数据集相比，Multi-XScience包含的位置信息和提取偏差较少，因此可以在一定程度上避免从某一位置的复制句子中获得更高分数的缺点。
- en: Datasets for MDS Variants. The representative query-oriented MDS datasets are
    Debatepedia (Nema et al., [2017](#bib.bib113)), AQUAMUSE (Kulkarni et al., [2020](#bib.bib80)),
    and QBSUM (Zhao et al., [2021](#bib.bib180)). The representative dialogue summarization
    datasets are DIALOGSUM (Chen et al., [2021](#bib.bib26)), AMI (Carletta et al.,
    [2005](#bib.bib24)), MEDIASUM (Zhu et al., [2021](#bib.bib186)), and QMSum (Zhong
    et al., [2021](#bib.bib184)). RTS is a track at the Text Retrieval Conference
    (TREC) which provides several RTS datasets^(19)^(19)19http://trecrts.github.io/.
    Tweet Contextualization track (Bellot et al., [2016](#bib.bib13)) (2012-2014)
    is derived from the INEX 2011 Question Answering Track, that focuses on more NLP-oriented
    tasks and moves to MDS.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: MDS变体的数据集。代表性的查询导向MDS数据集有Debatepedia（Nema et al., [2017](#bib.bib113)）、AQUAMUSE（Kulkarni
    et al., [2020](#bib.bib80)）和QBSUM（Zhao et al., [2021](#bib.bib180)）。代表性的对话摘要数据集有DIALOGSUM（Chen
    et al., [2021](#bib.bib26)）、AMI（Carletta et al., [2005](#bib.bib24)）、MEDIASUM（Zhu
    et al., [2021](#bib.bib186)）和QMSum（Zhong et al., [2021](#bib.bib184)）。RTS是文本检索会议（TREC）的一个赛道，提供了几个RTS数据集^(19)^(19)19http://trecrts.github.io/。Tweet
    Contextualization赛道（Bellot et al., [2016](#bib.bib13)）（2012-2014）源自INEX 2011问题回答赛道，专注于更多的自然语言处理任务并转向MDS。
- en: 'Discussion. Table [4](#S6.T4 "Table 4 ‣ 6\. Datasets ‣ Multi-document Summarization
    via Deep Learning Techniques: A Survey") compares 20 MDS datasets based on the
    numbers of clusters and documents; the number and the average length of summaries;
    and the field to which the dataset belongs. Currently, the main areas covered
    by the MDS datasets are news (60$\%$), scientific papers (10$\%$) and Wikipedia
    (10$\%$). In early development of the MDS tasks, most studies were performed on
    the DUC and TAC datasets. However, the size of these datasets is relatively small,
    and thus not highly suitable for training deep neural network models. Datasets
    on news articles are also common, but the structure of news articles (highly compressed
    information in the first paragraph or first sentence of each paragraph) can cause
    positional and extractive biases during training. In recent years, large-scale
    datasets such as WikiSum and Multi-News datasets have been developed and used
    by researchers to meet training requirements, reflecting the rising trend of data-driven
    approaches.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '讨论。表格[4](#S6.T4 "Table 4 ‣ 6\. Datasets ‣ Multi-document Summarization via
    Deep Learning Techniques: A Survey") 比较了20个多文档摘要（MDS）数据集，这些数据集基于簇和文档的数量、摘要的数量和平均长度，以及数据集所属的领域。目前，MDS数据集主要覆盖的领域是新闻（60%）、科学论文（10%）和维基百科（10%）。在MDS任务的早期阶段，大多数研究是在DUC和TAC数据集上进行的。然而，这些数据集的规模相对较小，因此不太适合用于训练深度神经网络模型。新闻文章数据集也很常见，但新闻文章的结构（每段的第一段或第一句话中高度压缩的信息）可能在训练过程中造成位置和提取偏差。近年来，像WikiSum和Multi-News这样的规模较大的数据集已经被开发并被研究人员使用，以满足训练要求，反映了数据驱动方法的上升趋势。'
- en: 7\. Future research directions and open issues
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 未来研究方向和未解决的问题
- en: Although existing works have established a solid foundation for MDS it is a
    relatively understudied field compared with SDS and other NLP topics. Summarizing
    on multi-modal data, medical records, codes, project activities and MDS combining
    with Internet of Things (Zhang et al., [2020c](#bib.bib178)) have still received
    less attention. Actually, MDS techniques are beneficial for a variety of practical
    applications, including generating Wikipedia articles, summarizing news, scientific
    papers, and product reviews, and individuals, industries have a huge demand for
    compressing multiple related documents into high-quality summaries. This section
    outlines several prospective research directions and open issues that we believe
    are critical to resolve in order to advance the field.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有工作为多文档摘要奠定了坚实的基础，但与单文档摘要和其他自然语言处理（NLP）主题相比，它仍是一个相对较少研究的领域。对于多模态数据、医疗记录、代码、项目活动和多文档摘要与物联网的结合（Zhang
    et al.，[2020c](#bib.bib178)）的研究仍然较少。实际上，多文档摘要技术对各种实际应用非常有利，包括生成维基百科文章、总结新闻、科学论文和产品评论，个人和行业对将多个相关文档压缩成高质量摘要的需求巨大。本节概述了我们认为关键的几个前景研究方向和待解决的问题，以推动该领域的发展。
- en: 7.1\. Capturing Cross-document Relations for MDS
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 捕捉多文档摘要的文档间关系
- en: Currently, many MDS models still center on simple concatenation of input documents
    into a flat sequence, ignoring cross-document relations. Unlike SDS, MDS input
    documents may contain redundant, complementary, or contradictory information (Radev,
    [2000](#bib.bib127)). Discovering cross-document relations, which can assist models
    to extract salient information, improve the coherence and reduce redundancy of
    summaries(Li et al., [2020a](#bib.bib91)). Research on capturing cross-document
    relations has begun to gain momentum in the past two years; one of the most widely
    studied topics is graphical models, which can easily be combined with deep learning
    based models such as graph neural networks and Transformer models. Several existing
    works indicate the efficacy of graph-based deep learning models in capturing semantic-rich
    and syntactic-rich representation and generating high-quality summaries (Wang
    et al., [2020a](#bib.bib156); Yasunaga et al., [2019](#bib.bib167); Li et al.,
    [2020a](#bib.bib91); Yasunaga et al., [2017](#bib.bib168)). To this end, a promising
    and important direction would be to design a better mechanism to introduce different
    graph structures (Christensen et al., [2013](#bib.bib30)) or linguistic knowledge
    (Bing et al., [2015](#bib.bib15); Ma et al., [2021](#bib.bib101)), possibly into
    the attention mechanism in deep learning based models, to capture cross-document
    relations and to facilitate summarization.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，许多多文档摘要（MDS）模型仍然以简单地将输入文档串联成一个平坦的序列为中心，忽略了文档间的关系。与单文档摘要（SDS）不同，多文档摘要的输入文档可能包含冗余、互补或矛盾的信息（Radev，[2000](#bib.bib127)）。发现文档间的关系可以帮助模型提取重要信息，提高摘要的连贯性并减少冗余（Li
    et al.，[2020a](#bib.bib91)）。在过去两年中，捕捉文档间关系的研究开始获得动力；其中一个最广泛研究的主题是图形模型，它们可以很容易地与基于深度学习的模型（如图神经网络和Transformer模型）结合。几项现有的工作表明，基于图的深度学习模型在捕捉语义丰富和句法丰富的表示以及生成高质量摘要方面具有良好的效果（Wang
    et al.，[2020a](#bib.bib156)；Yasunaga et al.，[2019](#bib.bib167)；Li et al.，[2020a](#bib.bib91)；Yasunaga
    et al.，[2017](#bib.bib168)）。为此，一个有前途且重要的方向是设计更好的机制，将不同的图结构（Christensen et al.，[2013](#bib.bib30)）或语言知识（Bing
    et al.，[2015](#bib.bib15)；Ma et al.，[2021](#bib.bib101)）引入深度学习模型中的注意力机制，以捕捉文档间的关系并促进摘要生成。
- en: 7.2\. Creating More High-quality Datasets for MDS
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 为多文档摘要创建更多高质量的数据集
- en: Benchmark datasets allow researchers to train, evaluate and compare the capabilities
    of different models on the same stage. High-quality datasets are critical to develop
    MDS tasks. DUC and TAC, the most common datasets used for MDS tasks, have a relatively
    small number of samples so are not very suitable for training DNN models. In recent
    years, some large datasets have been proposed, including WikiSum (Liu et al.,
    [2018](#bib.bib94)), Multi-News (Fabbri et al., [2019](#bib.bib44)), and WCEP
    (Ghalandari et al., [2020](#bib.bib51)), but more efforts are still needed. Datasets
    with documents of rich diversity, with minimal positional and extractive biases
    are desperately required to promote and accelerate MDS research, as are datasets
    for other applications such as summarization of medical records or dialogue (Molenaar
    et al., [2020](#bib.bib109)), email (Ulrich et al., [2008](#bib.bib150); Zajic
    et al., [2008](#bib.bib171)), code (Rodeghero et al., [2014](#bib.bib132); McBurney
    and McMillan, [2014](#bib.bib104)), software project activities (Alghamdi et al.,
    [2020](#bib.bib3)), legal documents (Kanapala et al., [2019](#bib.bib74)), and
    multi-modal data (Li et al., [2020b](#bib.bib86)). The development of large-scale
    cross-task datasets will facilitate multi-task learning (Xu et al., [2020b](#bib.bib160)).
    However, the datasets of MDS combining with text classification, question answering,
    or other language tasks have seldom been proposed in the MDS research community,
    but these datasets are essential and widely employed in industrial applications.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 基准数据集允许研究人员在相同的阶段上训练、评估和比较不同模型的能力。高质量的数据集对开发MDS任务至关重要。DUC和TAC是最常用的MDS任务数据集，但由于样本数量相对较少，因此不太适合训练DNN模型。近年来，已经提出了一些大型数据集，包括WikiSum（Liu
    et al., [2018](#bib.bib94)）、Multi-News（Fabbri et al., [2019](#bib.bib44)）和WCEP（Ghalandari
    et al., [2020](#bib.bib51)），但仍需更多努力。为了促进和加速MDS研究，迫切需要具有丰富多样性的文档数据集，并且要尽量减少位置和抽取偏差，此外，还需要用于其他应用的数据集，如医疗记录总结或对话（Molenaar
    et al., [2020](#bib.bib109)）、电子邮件（Ulrich et al., [2008](#bib.bib150)；Zajic et
    al., [2008](#bib.bib171)）、代码（Rodeghero et al., [2014](#bib.bib132)；McBurney and
    McMillan, [2014](#bib.bib104)）、软件项目活动（Alghamdi et al., [2020](#bib.bib3)）、法律文件（Kanapala
    et al., [2019](#bib.bib74)）以及多模态数据（Li et al., [2020b](#bib.bib86)）。开发大规模跨任务数据集将促进多任务学习（Xu
    et al., [2020b](#bib.bib160)）。然而，MDS与文本分类、问答或其他语言任务结合的数据集在MDS研究社区中很少被提出，但这些数据集在工业应用中至关重要且被广泛使用。
- en: 7.3\. Improving Evaluation Metrics for MDS
  id: totrans-457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3. 改进MDS评估指标
- en: 'To our best knowledge, there are no evaluation metrics specifically designed
    for MDS models – SDS and MDS models share the same evaluation metrics. New MDS
    evaluation metrics should be able to: (1) evaluating the relations between the
    different input documents in the generated summary; (2) measuring to what extent
    the redundancy in input documents is reduced; and (3) judging whether the contradictory
    information across documents is reasonably handled. A good evaluation indicator
    is able to reflect the true performance of an MDS model and guide design of improved
    models. However, current evaluation metrics (Fabbri et al., [2021](#bib.bib43))
    still have several obvious defects. For example, despite the effectiveness of
    commonly used ROUGE metrics, they struggle to accurately measure the semantic
    similarity between a golden and generated summary because ROUGE-based evaluation
    metrics only consider vocabulary-level distances; as such, even if a ROUGE score
    improves, it does not necessarily mean that the summary is of a higher quality
    and so is not ideal for model training. Recently, some works extend ROUGE along
    with WordNet (ShafieiBavani et al., [2018](#bib.bib138)) or pre-trained LMs (Zhang
    et al., [2020a](#bib.bib176)) to alleviate these drawbacks. It is challenging
    to propose evaluation indicators that can reflect the true quality of generated
    summaries comprehensively and as semantically as human raters. Another frontline
    challenge for evaluation metrics research is unsupervised evaluation, being explored
    by a number of recent studies (Sun and Nenkova, [2019](#bib.bib144); Gao et al.,
    [2020](#bib.bib49)).'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，目前还没有专门为多文档摘要（MDS）模型设计的评估指标——单文档摘要（SDS）和MDS模型共享相同的评估指标。新的MDS评估指标应能够：(1)
    评估生成摘要中不同输入文档之间的关系；(2) 衡量输入文档中的冗余程度是否降低；以及(3) 判断跨文档的矛盾信息是否得到合理处理。一个好的评估指标能够反映MDS模型的真实表现，并指导改进模型的设计。然而，当前的评估指标（Fabbri
    et al., [2021](#bib.bib43)）仍然存在几个明显的缺陷。例如，尽管ROUGE指标被广泛使用，但它们在准确测量黄金摘要与生成摘要之间的语义相似性时仍面临困难，因为ROUGE基于词汇层面的距离来进行评估；因此，即使ROUGE分数有所提高，也不一定意味着摘要质量更高，因此不适合用于模型训练。最近，一些研究扩展了ROUGE，结合了WordNet（ShafieiBavani
    et al., [2018](#bib.bib138)）或预训练语言模型（Zhang et al., [2020a](#bib.bib176)）来缓解这些缺陷。提出能够全面且语义上反映生成摘要真实质量的评估指标是一个挑战。另一个前沿挑战是无监督评估，最近有一些研究在探索这一领域（Sun
    and Nenkova, [2019](#bib.bib144)；Gao et al., [2020](#bib.bib49)）。
- en: 7.4\. Reinforcement Learning for MDS
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 强化学习在多文档摘要中的应用
- en: Reinforcement learning (Mnih et al., [2016](#bib.bib108)) is a cluster of algorithms
    based on dynamic programming according to the Bellman Equation to deal with sequential
    decision problems, where state transition dynamics of the environment are provided
    in advance. Several existing works (Paulus et al., [2018](#bib.bib124); Narayan
    et al., [2018](#bib.bib111); Yao et al., [2018](#bib.bib166)) model the document
    summarization task as a sequential decision problem and adopt reinforcement learning
    to tackle the task. Although deep reinforcement learning for SDS has made great
    progress, we still face challenges to adapt existing SDS models to MDS, as the
    latter suffer from a large state, action space, and problems with high redundancy
    and contradiction (Mao et al., [2020](#bib.bib103)). Additionally, current summarization
    methods are based on model-free reinforcement learning algorithms, in which the
    model is not aware of environment dynamics but continuously explores the environment
    through simple trial-and-error strategies, so they inevitably suffer from low
    sampling efficiencies. Nevertheless, the model-based approaches can leverage data
    more efficiently since they update models upon the prior to the environment. In
    this case, data-efficient reinforcement learning for MDS could potentially be
    explored in the future.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（Mnih et al., [2016](#bib.bib108)）是一类基于动态规划的算法，依据贝尔曼方程来处理序列决策问题，其中环境的状态转移动态是事先提供的。现有的一些工作（Paulus
    et al., [2018](#bib.bib124)；Narayan et al., [2018](#bib.bib111)；Yao et al., [2018](#bib.bib166)）将文档摘要任务建模为序列决策问题，并采用强化学习来处理这个任务。尽管深度强化学习在SDS（序列决策系统）方面取得了巨大进展，但我们仍面临将现有SDS模型适应到MDS（多文档摘要系统）的挑战，因为后者在状态和动作空间上都很庞大，并且存在高度冗余和矛盾的问题（Mao
    et al., [2020](#bib.bib103)）。此外，目前的摘要方法基于无模型强化学习算法，这些算法在不知道环境动态的情况下，通过简单的试错策略不断探索环境，因此不可避免地存在低采样效率的问题。然而，基于模型的方法可以更高效地利用数据，因为它们在环境先验基础上更新模型。在这种情况下，未来可能会探索用于MDS的数据高效强化学习。
- en: 7.5\. Pre-trained Language Models for MDS
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5\. 预训练语言模型在MDS中的应用
- en: In many NLP tasks, the limited labeled corpora are not adequate to train semantic-rich
    word vectors. Using large-scale, unlabeled, task-agnostic corpora for pre-training
    can enhance the generalization ability of models and accelerate convergence of
    networks (Peters et al., [2018](#bib.bib125); Mikolov et al., [2013](#bib.bib107)).
    At present, pre-trained LMs have led to successes in many deep learning based
    NLP tasks. Among the reviewed papers (Zhong et al., [2020](#bib.bib183); Lebanoff
    et al., [2019](#bib.bib83); Li et al., [2020a](#bib.bib91)), multiple works adopt
    pre-trained LMs for MDS and achieve promising improvements. Applying pre-trained
    LMs such as BERT (Devlin et al., [2019](#bib.bib35)), GPT-2 (Radford et al., [2019](#bib.bib130)),
    GPT-3 (Brown et al., [2020](#bib.bib18)), XLNet (Yang et al., [2019](#bib.bib165)),
    ALBERT (Lan et al., [2020](#bib.bib82)), or T5 (Raffel et al., [2020](#bib.bib131)),
    and fine-tuning them on a variety of downstream tasks allows the model to achieve
    faster convergence speed and can improve model performance. MDS requires the model
    to have a strong ability to process long sequences. It is promising to explore
    powerful LMs specifically targeting long sequence input characteristics and avoiding
    quadratic memory growth for self-attention mechanism, such as Longformer (Beltagy
    et al., [2020](#bib.bib14)), REFORMER (Kitaev et al., [2020](#bib.bib77)), or
    Big Bird (Zaheer et al., [2020](#bib.bib170)) with pre-trained models. Also, tailor-designed
    pre-trained LMs for summarization have not been well-explored, e.g., using gap
    sentences generation is more suitable than using masked language model (Zhang
    et al., [2020d](#bib.bib173)). Most MDS methods focus on combining pre-trained
    LMs in encoder and, as for capturing cross-document relations, applying them in
    decoder is also a worthwhile direction for research (Pasunuru et al., [2021b](#bib.bib123)).
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多自然语言处理（NLP）任务中，有限的标注语料库不足以训练出语义丰富的词向量。利用大规模的、未标注的、任务无关的语料库进行预训练可以增强模型的泛化能力，并加速网络的收敛（Peters
    et al., [2018](#bib.bib125)；Mikolov et al., [2013](#bib.bib107)）。目前，预训练语言模型（LM）在许多基于深度学习的NLP任务中取得了成功。在所评审的论文中（Zhong
    et al., [2020](#bib.bib183)；Lebanoff et al., [2019](#bib.bib83)；Li et al., [2020a](#bib.bib91)），多个研究采用了预训练LM用于多文档摘要（MDS），并取得了令人满意的改进。应用诸如BERT（Devlin
    et al., [2019](#bib.bib35)）、GPT-2（Radford et al., [2019](#bib.bib130)）、GPT-3（Brown
    et al., [2020](#bib.bib18)）、XLNet（Yang et al., [2019](#bib.bib165)）、ALBERT（Lan
    et al., [2020](#bib.bib82)）或T5（Raffel et al., [2020](#bib.bib131)）等预训练LM，并在各种下游任务上进行微调，可以使模型实现更快的收敛速度，并提高模型性能。MDS需要模型具备处理长序列的强大能力。探索专门针对长序列输入特征的强大LM，并避免自注意力机制的平方级内存增长，例如Longformer（Beltagy
    et al., [2020](#bib.bib14)）、REFORMER（Kitaev et al., [2020](#bib.bib77)）或Big Bird（Zaheer
    et al., [2020](#bib.bib170)）与预训练模型结合使用，是一个有前景的方向。此外，专门为摘要任务设计的预训练LM尚未得到充分探索，例如，使用生成缺失句子的模型比使用掩码语言模型更合适（Zhang
    et al., [2020d](#bib.bib173)）。大多数MDS方法集中于在编码器中结合预训练LM，对于捕捉跨文档关系，将其应用于解码器也是一个值得研究的方向（Pasunuru
    et al., [2021b](#bib.bib123)）。
- en: 7.6\. Creating Explainable Deep Learning Model for MDS
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6. 创建可解释的深度学习模型用于多文档摘要
- en: Deep learning models can be regarded as black boxes with high non-linearity;
    it is extremely challenging to understand the detailed transformation inside them.
    However, an explainable model can reveal how it generates candidate summaries
    – to distinguish whether the model has learned the distribution of generating
    condensed and coherent summaries from multiple documents without bias – and is
    thus crucial for model building. Recently, a large amount of researches into explainable
    models (Zhang et al., [2018a](#bib.bib174); Rudin, [2019](#bib.bib133)) have proposed
    easing the non-interpretable concern of deep neural networks, within which model
    attention plays an especially important role in model interpretation (Zhou et al.,
    [2016](#bib.bib185); Serrano and Smith, [2019](#bib.bib137)). While explainable
    methods have been intensively researched in NLP (Kumar and Talukdar, [2020](#bib.bib81);
    Jain et al., [2020](#bib.bib66)), studies into explainable MDS models are relatively
    scarce and would benefit from future development.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型可以被视为具有高度非线性的黑箱；理解其内部的详细转换极具挑战。然而，一个可解释的模型可以揭示其如何生成候选摘要——区分模型是否从多个文档中学到了生成浓缩和连贯摘要的分布，而没有偏差——因此对模型构建至关重要。最近，大量的研究集中在可解释模型上（Zhang等，[2018a](#bib.bib174)；Rudin，[2019](#bib.bib133)），提出了缓解深度神经网络不可解释性的问题，其中模型注意力在模型解释中发挥了特别重要的作用（Zhou等，[2016](#bib.bib185)；Serrano和Smith，[2019](#bib.bib137)）。虽然可解释方法在NLP中得到了广泛研究（Kumar和Talukdar，[2020](#bib.bib81)；Jain等，[2020](#bib.bib66)），但对可解释MDS模型的研究相对稀缺，并且未来的发展将受益于这些研究。
- en: 7.7\. Adversarial Attack and Defense for MDS
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7\. MDS的对抗攻击与防御
- en: Adversarial examples are strategically modified samples that aim to fool deep
    neural networks based models. An adversarial example is created via the worst-case
    perturbation of the input to which a robust DNN model would still assign correct
    labels, while a vulnerable DNN model would have high confidence in the wrong prediction.
    The idea of using adversarial examples to examine the robustness of a DNN model
    originated from research in Computer Vision (Szegedy et al., [2014](#bib.bib145))
    and was introduced in NLP by Jia et al. (Jia and Liang, [2017](#bib.bib71)). An
    essential purpose for generating adversarial examples for neural networks is to
    utilize these adversarial examples to enhance the model’s robustness (Goodfellow
    et al., [2015](#bib.bib53)). Therefore, research on adversarial examples not only
    helps identify and apply a robust model but also helps to build robust models
    for different tasks. Following the pioneering work proposed by Jia et al. (Jia
    and Liang, [2017](#bib.bib71)), many attack methods have been proposed to address
    this problem in NLP applications (Zhang et al., [2020b](#bib.bib177)) with limited
    research for MDS (Cheng et al., [2020](#bib.bib28)). It is worth filling this
    gap by exploring existing and developing new, adversarial attacks on the state-of-the-art
    DNN-based MDS models.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本是经过策略性修改的样本，旨在欺骗基于深度神经网络的模型。通过对输入进行最坏情况下的扰动来创建对抗样本，强大的DNN模型仍能分配正确的标签，而脆弱的DNN模型则对错误的预测充满信心。利用对抗样本来检查DNN模型的鲁棒性的想法起源于计算机视觉研究（Szegedy等，[2014](#bib.bib145)），并由Jia等（Jia和Liang，[2017](#bib.bib71)）引入了NLP。生成对抗样本的一个重要目的是利用这些对抗样本来增强模型的鲁棒性（Goodfellow等，[2015](#bib.bib53)）。因此，对对抗样本的研究不仅有助于识别和应用鲁棒模型，还帮助为不同任务构建鲁棒模型。继Jia等（Jia和Liang，[2017](#bib.bib71)）提出的开创性工作之后，许多攻击方法已被提出以解决NLP应用中的这个问题（Zhang等，[2020b](#bib.bib177)），但对MDS的研究仍有限（Cheng等，[2020](#bib.bib28)）。值得通过探索现有的和开发新的对抗攻击来填补这一空白，以应对最先进的基于DNN的MDS模型。
- en: 7.8\. Multi-modality for MDS
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.8\. MDS的多模态
- en: 'Existing multi-modal summarization is based on non-deep learning techniques
    (Li et al., [2017c](#bib.bib87); Jangra et al., [2021](#bib.bib69), [2020a](#bib.bib67),
    [2020b](#bib.bib68)), leaving a huge opportunity to exploit deep learning techniques
    for this task. Multi-modal learning has led to successes in many deep learning
    tasks, such as Visual Language Navigation (Wang et al., [2020b](#bib.bib157))
    and Visual Question Answering (Antol et al., [2015](#bib.bib7)). Combining MDS
    with multi-modality has a range of applications:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的多模态摘要基于非深度学习技术（Li等， [2017c](#bib.bib87)；Jangra等，[2021](#bib.bib69)，[2020a](#bib.bib67)，[2020b](#bib.bib68)），这为利用深度学习技术完成此任务留出了巨大的机会。多模态学习在许多深度学习任务中取得了成功，如视觉语言导航（Wang等，[2020b](#bib.bib157)）和视觉问答（Antol等，[2015](#bib.bib7)）。将MDS与多模态结合具有广泛的应用：
- en: •
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'text + image: generating summaries with pictures and texts for documents with
    pictures. This kind of multi-modal summary can improve the satisfaction of users
    (Zhu et al., [2018](#bib.bib188));'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'text + image: 为包含图片的文档生成带有图片和文本的摘要。这种多模态摘要可以提高用户的满意度（Zhu等，[2018](#bib.bib188)）；'
- en: •
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'text + video: based on the video and its subtitles, generating a concise text
    summary that describes the main context of video (Palaskar et al., [2019](#bib.bib119)).
    Movie synopsis is one application;'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'text + video: 基于视频及其字幕，生成一个简洁的文本摘要，描述视频的主要内容（Palaskar等，[2019](#bib.bib119)）。电影梗概是一个应用场景；'
- en: •
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'text + audio: generating short summaries of audio files that people could quickly
    preview without actually listening to the entire audio recording (Erol et al.,
    [2003](#bib.bib42)).'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'text + audio: 生成音频文件的简短摘要，使人们能够快速预览而无需实际听完整个音频录音（Erol等，[2003](#bib.bib42)）。'
- en: Deep learning is well-suited for multi-modal tasks (Guo et al., [2019](#bib.bib57)),
    as it is able to effectively capture highly nonlinear relationships between images,
    text or video data. Existing MDS models target at dealing with textual data only.
    Involving richer modalities based on textual data requires models to embrace larger
    capacity to handle these multi-modal data. The big models such as UNITER (Chen
    et al., [2020](#bib.bib27)), VisualBERT (Li et al., [2019](#bib.bib88)) deserve
    more attention in multi-modality MDS tasks. However, at present, there is little
    multi-modal research work based on MDS; this is a promising, but largely under-explored,
    area where more studies are expected.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习非常适合多模态任务（Guo等，[2019](#bib.bib57)），因为它能够有效捕捉图像、文本或视频数据之间的高度非线性关系。现有的MDS模型主要针对处理文本数据。涉及基于文本数据的更丰富模态需要模型具备更大的容量来处理这些多模态数据。像UNITER（Chen等，[2020](#bib.bib27)）、VisualBERT（Li等，[2019](#bib.bib88)）这样的大模型在多模态MDS任务中值得更多关注。然而，目前基于MDS的多模态研究很少；这是一个有前景但尚未被充分探索的领域，期待有更多研究。
- en: 8\. Conclusion
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: In this article, we have presented the first comprehensive review of the most
    notable works to date on deep learning based multi-document summarization (MDS).
    We propose a taxonomy for organizing and clustering existing publications and
    devise the network design strategies based on the state-of-the-art methods. We
    also provide an overview of the existing multi-document objective functions, evaluation
    metrics and datasets, and discuss some of the most pressing open problems and
    promising future extensions in MDS research. We hope this survey provides readers
    with a comprehensive understanding of the key aspects of the MDS tasks, clarifies
    the most notable advances, and sheds light on future studies.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们首次全面回顾了迄今为止在深度学习基础的多文档摘要（MDS）方面最重要的研究成果。我们提出了一种用于组织和聚类现有文献的分类法，并基于最先进的方法制定了网络设计策略。我们还提供了现有多文档目标函数、评估指标和数据集的概述，并讨论了一些最紧迫的开放问题以及MDS研究中有前景的未来扩展。我们希望这项调查能够为读者提供对MDS任务关键方面的全面理解，阐明最显著的进展，并为未来的研究提供启示。
- en: References
  id: totrans-478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Afantenos et al. (2005) Stergos Afantenos, Vangelis Karkaletsis, and Panagiotis
    Stamatopoulos. 2005. Summarization from Medical Documents: A Survey. *Artificial
    Intelligence in Medicine* 33, 2, 157–177.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Afantenos等（2005）Stergos Afantenos、Vangelis Karkaletsis 和 Panagiotis Stamatopoulos。2005。来自医疗文档的摘要：一项调查。*医学中的人工智能*
    33, 2, 157–177。
- en: Alghamdi et al. (2020) Mahfouth Alghamdi, Christoph Treude, and Markus Wagner.
    2020. Human-Like Summaries from Heterogeneous and Time-Windowed Software Development
    Artefacts. In *Proceedings of the 6th International Conference of Parallel Problem
    Solving from Nature (PPSN 2020)*. Leiden, The Netherlands, 329–342.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alghamdi等（2020）Mahfouth Alghamdi、Christoph Treude 和 Markus Wagner。2020。来自异构和时间窗口软件开发文档的人类化摘要。在*第6届自然并行问题解决国际会议（PPSN
    2020）*论文集中。荷兰莱顿，329–342。
- en: 'Amplayo and Lapata (2021) Reinald Kim Amplayo and Mirella Lapata. 2021. Informative
    and Controllable Opinion Summarization. In *Proceedings of the 16th Conference
    of the European Chapter of the Association for Computational Linguistics: Main
    Volume (EACL 2021)*. Online, 2662–2672.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amplayo和Lapata（2021）Reinald Kim Amplayo 和 Mirella Lapata。2021。信息性和可控的意见摘要。在*第16届欧洲计算语言学协会年会：主卷（EACL
    2021）*论文集中。在线，2662–2672。
- en: 'Angelidis and Lapata (2018) Stefanos Angelidis and Mirella Lapata. 2018. Summarizing
    Opinions: Aspect Extraction Meets Sentiment Prediction and They are Both Weakly
    Supervised. In *Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing (EMNLP 2018)*. Brussels, Belgium, 3675–3686.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Angelidis 和 Lapata（2018）Stefanos Angelidis 和 Mirella Lapata。2018年。总结观点：方面提取遇上情感预测且二者均为弱监督。在
    *2018年自然语言处理经验方法会议（EMNLP 2018）* 论文集中。比利时布鲁塞尔，3675–3686。
- en: Antognini and Faltings (2019) Diego Antognini and Boi Faltings. 2019. Learning
    to Create Sentence Semantic Relation Graphs for Multi-Document Summarization.
    In *Proceedings of the 2nd Workshop on New Frontiers in Summarization (EMNLP 2019)*.
    Hongkong, China.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antognini 和 Faltings（2019）Diego Antognini 和 Boi Faltings。2019年。学习创建句子语义关系图以进行多文档摘要。在
    *第2届摘要新前沿研讨会（EMNLP 2019）* 论文集中。中国香港。
- en: 'Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual
    Question Answering. In *Proceedings of the 2015 IEEE International Conference
    on Computer Vision (ICCV 2015)*. Santiago, Chile, 2425–2433.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antol 等人（2015）Stanislaw Antol、Aishwarya Agrawal、Jiasen Lu、Margaret Mitchell、Dhruv
    Batra、C. Lawrence Zitnick 和 Devi Parikh。2015年。VQA：视觉问答。在 *2015年IEEE计算机视觉国际会议（ICCV
    2015）* 论文集中。智利圣地亚哥，2425–2433。
- en: Arora and Ravindran (2008) Rachit Arora and Balaraman Ravindran. 2008. Latent
    Dirichlet Allocation and Singular Value Decomposition based Multi-document Summarization.
    In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining
    (ICDM 2008)*. Pisa, Italy, 713–718.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora 和 Ravindran（2008）Rachit Arora 和 Balaraman Ravindran。2008年。基于潜在狄利克雷分配和奇异值分解的多文档摘要。在
    *2008年第八届IEEE数据挖掘国际会议（ICDM 2008）* 论文集中。意大利比萨，713–718。
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.
    Neural Machine Translation by Jointly Learning to Align and Translate. In *Proceedings
    of the 3rd International Conference on Learning Representations (ICLR 2015)*.
    San Diego, CA, United States.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等人（2015）Dzmitry Bahdanau、Kyunghyun Cho 和 Yoshua Bengio。2015年。通过联合学习对齐和翻译的神经机器翻译。在
    *第3届国际学习表征会议（ICLR 2015）* 论文集中。美国加州圣地亚哥。
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
    An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.
    In *Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures
    for Machine Translation and/or Summarization (ACL 2005)*. 65–72.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banerjee 和 Lavie（2005）Satanjeev Banerjee 和 Alon Lavie。2005年。METEOR：一种与人类判断改进相关性的机器翻译自动评估指标。在
    *机器翻译和/或摘要内在和外在评估度量研讨会（ACL 2005）* 论文集中。65–72。
- en: Baralis et al. (2012) Elena Baralis, Luca Cagliero, Saima Jabeen, and Alessandro
    Fiori. 2012. Multi-document Summarization Exploiting Frequent Itemsets. In *Proceedings
    of the 27th Annual ACM Symposium on Applied Computing (SAC2012)*. Riva, Italy,
    782–786.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baralis 等人（2012）Elena Baralis、Luca Cagliero、Saima Jabeen 和 Alessandro Fiori。2012年。利用频繁项集的多文档摘要。在
    *第27届年度ACM应用计算研讨会（SAC2012）* 论文集中。意大利里瓦，782–786。
- en: Baxendale (1958) Phyllis B Baxendale. 1958. Machine-made Index for Technical
    Literature - An Experiment. *IBM Journal of Research and Development* 2, 4, 354–361.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baxendale（1958）Phyllis B Baxendale。1958年。技术文献的机器制作索引 - 一次实验。*IBM研究与开发期刊* 2,
    4, 354–361。
- en: 'Bellot et al. (2016) Patrice Bellot, Véronique Moriceau, Josiane Mothe, Eric
    SanJuan, and Xavier Tannier. 2016. INEX Tweet Contextualization task: Evaluation,
    results and lesson learned. *Information Processing and Management.* 52, 5 (2016),
    801–819.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellot 等人（2016）Patrice Bellot、Véronique Moriceau、Josiane Mothe、Eric SanJuan
    和 Xavier Tannier。2016年。INEX推文上下文化任务：评估、结果和经验教训。*信息处理与管理* 52, 5 (2016), 801–819。
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
    Longformer: The Long-document Transformer. *arXiv preprint arXiv:2004.05150*.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltagy 等人（2020）Iz Beltagy、Matthew E Peters 和 Arman Cohan。2020年。Longformer：长文档变换器。*arXiv预印本
    arXiv:2004.05150*。
- en: Bing et al. (2015) Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo, and Rebecca J.
    Passonneau. 2015. Abstractive Multi-Document Summarization via Phrase Selection
    and Merging. In *Proceedings of the 53rd Annual Meeting of the Association for
    Computational Linguistics and the 7th International Joint Conference on Natural
    Language Processing of the Asian Federation of Natural Language Processing (ACL
    2015)*. Beijing, China, 1587–1597.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bing 等人（2015）Lidong Bing、Piji Li、Yi Liao、Wai Lam、Weiwei Guo 和 Rebecca J. Passonneau。2015年。通过短语选择和合并进行抽象多文档摘要。在
    *第53届年度计算语言学协会年会暨第七届亚洲自然语言处理联合会议（ACL 2015）* 论文集中。中国北京，1587–1597。
- en: Bražinskas et al. (2019) Arthur Bražinskas, Mirella Lapata, and Ivan Titov.
    2019. Unsupervised Multi-Document Opinion Summarization as Copycat-Review Generation.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL 2020)*. Online.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bražinskas et al. (2019) Arthur Bražinskas、Mirella Lapata 和 Ivan Titov. 2019.
    无监督多文档观点摘要作为仿效评论生成。在*第58届计算语言学协会年会论文集（ACL 2020）*。在线。
- en: Brazinskas et al. (2020) Arthur Brazinskas, Mirella Lapata, and Ivan Titov.
    2020. Few-Shot Learning for Opinion Summarization. In *Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)*.
    Online, 4119–4135.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brazinskas et al. (2020) Arthur Brazinskas、Mirella Lapata 和 Ivan Titov. 2020.
    用于观点摘要的少样本学习。在*2020年自然语言处理经验方法会议论文集（EMNLP 2020）*。在线，4119–4135。
- en: 'Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020. Language Models Are Few-shot Learners. In *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020 (NeurIPS 2020)*. Online.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom B Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared
    Kaplan、Prafulla Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda
    Askell 等. 2020. 语言模型是少样本学习者。在*神经信息处理系统进展33：2020年神经信息处理系统年会论文集（NeurIPS 2020）*。在线。
- en: Cao et al. (2017) Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. 2017. Improving
    Multi-document Summarization via Text Classification. In *Proceedings of the 31st
    AAAI Conference on Artificial Intelligence (AAAI 2017)*. San Francisco, United
    States, 3053–3059.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2017) Ziqiang Cao、Wenjie Li、Sujian Li 和 Furu Wei. 2017. 通过文本分类改进多文档摘要。在*第31届人工智能AAAI会议论文集（AAAI
    2017）*。美国旧金山，3053–3059。
- en: Cao et al. (2015a) Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and Ming Zhou.
    2015a. Ranking with Recursive Neural Networks and its Application to Multi-document
    Summarization. In *Proceedings of the 29th AAAI Conference on Artificial Intelligence
    (AAAI 2015)*. Austin, United States, 2153–2159.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2015a) Ziqiang Cao、Furu Wei、Li Dong、Sujian Li 和 Ming Zhou. 2015a.
    使用递归神经网络进行排名及其在多文档摘要中的应用。在*第29届人工智能AAAI会议论文集（AAAI 2015）*。美国奥斯汀，2153–2159。
- en: Cao et al. (2015b) Ziqiang Cao, Furu Wei, Sujian Li, Wenjie Li, Ming Zhou, and
    Houfeng Wang. 2015b. Learning Summary Prior Representation for Extractive Summarization.
    In *Proceedings of the 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing
    (ACL 2015)*. Beijing, China, 829–833.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2015b) Ziqiang Cao、Furu Wei、Sujian Li、Wenjie Li、Ming Zhou 和 Houfeng
    Wang. 2015b. 学习摘要先验表示用于提取式摘要。在*第53届计算语言学协会年会及第7届国际自然语言处理联合会议论文集（ACL 2015）*。中国北京，829–833。
- en: Carbonell and Goldstein (1998) Jaime G. Carbonell and Jade Goldstein. 1998.
    The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing
    Summaries. In *Proceedings of the 21st Annual International Conference on Research
    and Development in Information Retrieval (SIGIR 1998)*. Melbourne, Australia,
    335–336.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carbonell and Goldstein (1998) Jaime G. Carbonell 和 Jade Goldstein. 1998. 使用MMR，基于多样性的重新排序用于重新排序文档和生成摘要。在*第21届国际信息检索研究与发展会议论文集（SIGIR
    1998）*。澳大利亚墨尔本，335–336。
- en: Carenini et al. (2007) Giuseppe Carenini, Raymond T. Ng, and Xiaodong Zhou.
    2007. Summarizing Email Conversations with Clue Words. In *Proceedings of the
    16th International Conference on World Wide Web (WWW 2007)*. Banff, Canada, 91–100.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carenini et al. (2007) Giuseppe Carenini、Raymond T. Ng 和 Xiaodong Zhou. 2007.
    使用线索词总结电子邮件对话。在*第16届全球信息网络会议论文集（WWW 2007）*。加拿大班夫，91–100。
- en: 'Carletta et al. (2005) Jean Carletta, Simone Ashby, Sebastien Bourban, Mike
    Flynn, Maël Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel
    Kraaij, Melissa Kronenthal, Guillaume Lathoud, Mike Lincoln, Agnes Lisowska, Iain
    McCowan, Wilfried Post, Dennis Reidsma, and Pierre Wellner. 2005. The AMI Meeting
    Corpus: A Pre-announcement. In *Machine Learning for Multimodal Interaction, Second
    International Workshop (MLMI 2005)*. Edinburgh, UK, 28–39.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carletta et al. (2005) Jean Carletta、Simone Ashby、Sebastien Bourban、Mike Flynn、Maël
    Guillemot、Thomas Hain、Jaroslav Kadlec、Vasilis Karaiskos、Wessel Kraaij、Melissa
    Kronenthal、Guillaume Lathoud、Mike Lincoln、Agnes Lisowska、Iain McCowan、Wilfried
    Post、Dennis Reidsma 和 Pierre Wellner. 2005. AMI会议语料库：预告。在*多模态交互机器学习，第二届国际研讨会（MLMI
    2005）*。英国爱丁堡，28–39。
- en: Chen and Yang (2020) Jiaao Chen and Diyi Yang. 2020. Multi-View Sequence-to-Sequence
    Models with Conversational Structure for Abstractive Dialogue Summarization. In
    *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2020)*. Online, 4106–4118.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Yang（2020）Jiaao Chen 和 Diyi Yang. 2020. 多视角序列到序列模型与对话结构用于抽象对话总结。在 *2020
    年自然语言处理实证方法会议（EMNLP 2020）论文集*。在线，4106–4118。
- en: 'Chen et al. (2021) Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. 2021.
    DialogSumm: A Real-Life Scenario Dialogue Summarization Dataset. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics (ACL
    2021)*. Online.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2021）Yulong Chen、Yang Liu、Liang Chen 和 Yue Zhang。2021. DialogSumm: 一种现实场景对话总结数据集。在
    *第 59 届计算语言学协会年会（ACL 2021）论文集*。在线。'
- en: 'Chen et al. (2020) Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal
    Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal Image-text
    Representation Learning. In *European conference on computer vision*. Springer,
    104–120.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2020）Yen-Chun Chen、Linjie Li、Licheng Yu、Ahmed El Kholy、Faisal Ahmed、Zhe
    Gan、Yu Cheng 和 Jingjing Liu。2020. Uniter: 通用图像-文本表示学习。在 *欧洲计算机视觉会议*。Springer，104–120。'
- en: 'Cheng et al. (2020) Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and
    Cho-Jui Hsieh. 2020. Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence
    Models with Adversarial Examples. In *Proceedings of the 34th AAAI Conference
    on Artificial Intelligence (AAAI 2020)*. New York, NY, United States, 3601–3608.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等（2020）Minhao Cheng、Jinfeng Yi、Pin-Yu Chen、Huan Zhang 和 Cho-Jui Hsieh。2020.
    Seq2Sick: 评估序列到序列模型在对抗样本下的鲁棒性。在 *第 34 届 AAAI 人工智能会议（AAAI 2020）论文集*。美国纽约，3601–3608。'
- en: Cho et al. (2019) Sangwoo Cho, Logan Lebanoff, Hassan Foroosh, and Fei Liu.
    2019. Improving the Similarity Measure of Determinantal Point Processes for Extractive
    Multi-Document Summarization. In *Proceedings of the 57th Conference of the Association
    for Computational Linguistics (ACL 2019)*. Florence, Italy, 1027–1038.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等（2019）Sangwoo Cho、Logan Lebanoff、Hassan Foroosh 和 Fei Liu。2019. 改进抽取式多文档总结的决定性点过程相似度度量。在
    *第 57 届计算语言学协会会议（ACL 2019）论文集*。意大利佛罗伦萨，1027–1038。
- en: 'Christensen et al. (2013) Janara Christensen, Stephen Soderland, Oren Etzioni,
    et al. 2013. Towards Coherent Multi-document Summarization. In *Proceedings of
    the 2013 conference of the North American chapter of the association for computational
    linguistics: Human language technologies*. 1163–1173.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christensen 等（2013）Janara Christensen、Stephen Soderland、Oren Etzioni 等。2013.
    朝着连贯的多文档总结迈进。在 *2013 年北美计算语言学协会：人类语言技术会议论文集*。1163–1173。
- en: 'Chu and Liu (2019) Eric Chu and Peter J. Liu. 2019. MeanSum: A Neural Model
    for Unsupervised Multi-Document Abstractive Summarization. In *Proceedings of
    the 36th International Conference on Machine Learning (ICML 2019)*. Long Beach,
    United States, 1223–1232.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chu 和 Liu（2019）Eric Chu 和 Peter J. Liu. 2019. MeanSum: 一种用于无监督多文档抽象总结的神经模型。在
    *第 36 届国际机器学习会议（ICML 2019）论文集*。美国长滩，1223–1232。'
- en: Chung et al. (2014) Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua
    Bengio. 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
    Modeling. *arXiv preprint arXiv:1412.3555*.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等（2014）Junyoung Chung、Çaglar Gülçehre、KyungHyun Cho 和 Yoshua Bengio。2014.
    门控递归神经网络在序列建模中的实证评估。*arXiv 预印本 arXiv:1412.3555*。
- en: Coavoux et al. (2019) Maximin Coavoux, Hady Elsahar, and Matthias Gallé. 2019.
    Unsupervised Aspect-Based Multi-Document Abstractive Summarization. In *Proceedings
    of the 2nd Workshop on New Frontiers in Summarization*. Hong Kong, China, 42–47.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coavoux 等（2019）Maximin Coavoux、Hady Elsahar 和 Matthias Gallé。2019. 无监督的基于方面的多文档抽象总结。在
    *第 2 届总结新前沿研讨会论文集*。中国香港，42–47。
- en: Collobert et al. (2011) Ronan Collobert, Jason Weston, Léon Bottou, Michael
    Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing
    (Almost) from Scratch. *Journal of Machine Learning Research* 12, ARTICLE, 2493–2537.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collobert 等（2011）Ronan Collobert、Jason Weston、Léon Bottou、Michael Karlen、Koray
    Kavukcuoglu 和 Pavel Kuksa。2011. 从零开始的自然语言处理。*机器学习研究期刊* 12，ARTICLE，2493–2537。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies
    (NAACL-HLT 2019)*. Minneapolis, United States, 4171–4186.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等人（2019）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2019年。BERT:
    用于语言理解的深度双向变换器的预训练。载于 *2019年北美计算语言学协会年会：人类语言技术（NAACL-HLT 2019）论文集*。美国明尼阿波利斯，4171–4186。'
- en: Devlin et al. (2014) Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar,
    Richard Schwartz, and John Makhoul. 2014. Fast and Robust Neural Network Joint
    Models for Statistical Machine Translation. In *Proceedings of the 52nd Annual
    Meeting of the Association for Computational Linguistics (ACL 2014)*. Baltimore,
    United States, 1370–1380.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等人（2014）Jacob Devlin、Rabih Zbib、Zhongqiang Huang、Thomas Lamar、Richard
    Schwartz 和 John Makhoul。2014年。快速而稳健的神经网络联合模型用于统计机器翻译。载于 *第52届计算语言学协会年会（ACL 2014）论文集*。美国巴尔的摩，1370–1380。
- en: 'Dong et al. (2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,
    Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified Language Model
    Pre-training for Natural Language Understanding and Generation. In *Advances in
    Neural Information Processing Systems 32: Annual Conference on Neural Information
    Processing Systems 2019 (NeurIPS 2019)*. 13042–13054.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人（2019）Li Dong、Nan Yang、Wenhui Wang、Furu Wei、Xiaodong Liu、Yu Wang、Jianfeng
    Gao、Ming Zhou 和 Hsiao-Wuen Hon。2019年。统一语言模型预训练用于自然语言理解和生成。载于 *神经信息处理系统进展 32：2019年神经信息处理系统年会（NeurIPS
    2019）*。13042–13054。
- en: Dos Santos and Gatti (2014) Cicero Dos Santos and Maira Gatti. 2014. Deep Convolutional
    Neural Networks for Sentiment Analysis of Short Texts. In *Proceedings of the
    International Conference on Computational Linguistics (COLING 2014)*. Dublin,
    Ireland, 69–78.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dos Santos 和 Gatti（2014）Cicero Dos Santos 和 Maira Gatti。2014年。用于短文本情感分析的深度卷积神经网络。载于
    *国际计算语言学会议论文集（COLING 2014）*。爱尔兰都柏林，69–78。
- en: 'El-Kassas et al. (2021) Wafaa S. El-Kassas, Cherif R. Salama, Ahmed A. Rafea,
    and Hoda K. Mohamed. 2021. Automatic Text Summarization: A Comprehensive Survey.
    *Expert Systems with Applications* 165 (2021), 113679.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: El-Kassas 等人（2021）Wafaa S. El-Kassas、Cherif R. Salama、Ahmed A. Rafea 和 Hoda
    K. Mohamed。2021年。自动文本摘要：全面调查。*应用专家系统* 165（2021），113679。
- en: Enarvi et al. (2020) Seppo Enarvi, Marilisa Amoia, Miguel Del-Agua Teba, Brian
    Delaney, Frank Diehl, Stefan Hahn, Kristina Harris, Liam McGrath, Yue Pan, Joel
    Pinto, et al. 2020. Generating Medical Reports from Patient-doctor Conversations
    Using Sequence-to-sequence Models. In *Proceedings of the first workshop on natural
    language processing for medical conversations*.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Enarvi 等人（2020）Seppo Enarvi、Marilisa Amoia、Miguel Del-Agua Teba、Brian Delaney、Frank
    Diehl、Stefan Hahn、Kristina Harris、Liam McGrath、Yue Pan、Joel Pinto 等人。2020年。从患者-医生对话生成医疗报告的序列到序列模型。载于
    *首届医疗对话自然语言处理研讨会论文集*。
- en: 'Erkan and Radev (2004) Günes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based
    Lexical Centrality as Salience in Text Summarization. *Journal of Artificial Intelligence
    Research* 22, 457–479.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erkan 和 Radev（2004）Günes Erkan 和 Dragomir R Radev。2004年。Lexrank：基于图的词汇中心性在文本摘要中的显著性。*人工智能研究期刊*
    22, 457–479。
- en: Erol et al. (2003) Berna Erol, Dar-Shyang Lee, and Jonathan J. Hull. 2003. Multimodal
    Summarization of Meeting Recordings. In *Proceedings of the 2003 IEEE International
    Conference on Multimedia and Expo (ICME 2003)*. Baltimore, United States, 25–28.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erol 等人（2003）Berna Erol、Dar-Shyang Lee 和 Jonathan J. Hull。2003年。会议记录的多模态摘要。载于
    *2003年IEEE国际多媒体与博览会（ICME 2003）论文集*。美国巴尔的摩，25–28。
- en: 'Fabbri et al. (2021) Alexander R Fabbri, Wojciech Kryściński, Bryan McCann,
    Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating
    Summarization Evaluation. *Transactions of the Association for Computational Linguistics*
    9, 391–409.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fabbri 等人（2021）Alexander R Fabbri、Wojciech Kryściński、Bryan McCann、Caiming
    Xiong、Richard Socher 和 Dragomir Radev。2021年。Summeval: 重新评估摘要评估。*计算语言学协会交易* 9,
    391–409。'
- en: 'Fabbri et al. (2019) Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and
    Dragomir R. Radev. 2019. Multi-News: A Large-Scale Multi-Document Summarization
    Dataset and Abstractive Hierarchical Model. In *Proceedings of the 57th Conference
    of the Association for Computational Linguistics (ACL 2019)*. Florence, Italy,
    1074–1084.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fabbri 等人（2019）Alexander R. Fabbri、Irene Li、Tianwei She、Suyi Li 和 Dragomir
    R. Radev。2019年。Multi-News: 大规模多文档摘要数据集和抽象层次模型。载于 *第57届计算语言学协会年会（ACL 2019）论文集*。意大利佛罗伦萨，1074–1084。'
- en: Feng et al. (2021) Xiachong Feng, Xiaocheng Feng, Bing Qin, Xinwei Geng, and
    Ting Liu. 2021. Dialogue Discourse-Aware Graph Convolutional Networks for Abstractive
    Meeting Summarization. In *Proceedings of the 30th International Joint Conference
    on Artificial Intelligence (IJCAI 2021)*.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人（2021）夏冲·冯、肖成·冯、冰沁·秦、鑫伟·耿和婷刘。2021。对话语篇感知图卷积网络用于抽象会议摘要。见于 *第30届国际人工智能联合会议（IJCAI
    2021）论文集*。
- en: Ferreira et al. (2014) Rafael Ferreira, Luciano de Souza Cabral, Frederico Freitas,
    Rafael Dueire Lins, Gabriel de França Silva, Steven J Simske, and Luciano Favaro.
    2014. A Multi-document Summarization System based on Statistics and Linguistic
    Treatment. *Expert Systems with Applications* 41, 13, 5780–5787.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira 等人（2014）拉斐尔·费雷拉、卢西亚诺·德·索萨·卡布拉尔、弗雷德里科·弗雷塔斯、拉斐尔·杜伊雷·林斯、加布里埃尔·德·弗朗卡·席尔瓦、史蒂文·J·辛斯克和卢西亚诺·法瓦罗。2014。基于统计和语言处理的多文档摘要系统。*专家系统与应用*
    41, 13, 5780–5787。
- en: 'Ganesan et al. (2010) Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. 2010.
    Opinosis: A Graph Based Approach to Abstractive Summarization of Highly Redundant
    Opinions. In *Proceedings of the 23rd International Conference on Computational
    Linguistics (COLING 2010)*. Beijing, China, 340–348.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ganesan 等人（2010）卡维塔·加内桑、郑翔·翟和贾伟·韩。2010。Opinosis：一种基于图的抽象总结高度冗余意见的方法。见于 *第23届国际计算语言学会议（COLING
    2010）论文集*。中国北京，340–348。
- en: Gao et al. (2019) Yanjun Gao, Chen Sun, and Rebecca J Passonneau. 2019. Automated
    Pyramid Summarization Evaluation. In *Proceedings of the 23rd Conference on Computational
    Natural Language Learning (CoNLL 2019)*. Hong Kong, China, 404–418.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人（2019）严军高、陈孙和丽贝卡·J·帕索纽。2019。自动化金字塔摘要评估。见于 *第23届计算自然语言学习会议（CoNLL 2019）论文集*。中国香港，404–418。
- en: 'Gao et al. (2020) Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: Towards
    New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL 2020)*. Online, 1347–1354.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等人（2020）杨高、魏赵和斯特芬·埃格尔。2020。SUPERT: 向无监督评估指标的新前沿迈进。见于 *第58届计算语言学协会年会（ACL
    2020）论文集*。在线，1347–1354。'
- en: Gerani et al. (2014) Shima Gerani, Yashar Mehdad, Giuseppe Carenini, Raymond
    Ng, and Bita Nejat. 2014. Abstractive Summarization of Product Reviews Using Discourse
    Structure. In *Proceedings of the 2014 Conference on Empirical Methods in Natural
    Language Processing (EMNLP 2014)*. Doha, Qatar, 1602–1613.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerani 等人（2014）希玛·杰拉尼、亚夏·梅赫达德、朱塞佩·卡雷尼、雷蒙德·吴和比塔·内贾特。2014。使用语篇结构的产品评论抽象摘要。见于 *2014年自然语言处理经验方法会议（EMNLP
    2014）论文集*。卡塔尔多哈，1602–1613。
- en: Ghalandari et al. (2020) Demian Gholipour Ghalandari, Chris Hokamp, Nghia The
    Pham, John Glover, and Georgiana Ifrim. 2020. A Large-Scale Multi-Document Summarization
    Dataset from the Wikipedia Current Events Portal. In *Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics (ACL 2020)*. Online,
    1302–1308.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghalandari 等人（2020）德米安·戈利普尔·贾兰达里、克里斯·霍坎普、阮·塔·范、约翰·格洛弗和乔治安娜·伊弗里姆。2020。来自维基百科时事门户的大规模多文档摘要数据集。见于
    *第58届计算语言学协会年会（ACL 2020）论文集*。在线，1302–1308。
- en: 'Goldstein et al. (2000) Jade Goldstein, Vibhu O Mittal, Jaime G Carbonell,
    and Mark Kantrowitz. 2000. Multi-document Summarization by Sentence Extraction.
    In *Proceedings of the Conference of the North American Chapter of the Association
    for Computational Linguistics: Applied Natural Language Processing Conference
    (NAACL-ANLP 2000)*. Seattle, United States, 91–98.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldstein 等人（2000）杰德·戈尔斯坦、维布胡·O·米塔尔、哈梅·G·卡博内尔和马克·坎特罗维茨。2000。通过句子提取进行多文档摘要。见于
    *北美计算语言学协会应用自然语言处理会议（NAACL-ANLP 2000）论文集*。美国西雅图，91–98。
- en: Goodfellow et al. (2015) Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
    2015. Explaining and Harnessing Adversarial Examples. In *Proceedings of the 3rd
    International Conference on Learning Representations (ICLR 2015)*. San Diego,
    CA, United States,.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人（2015）伊恩·J·古德费洛、乔纳森·什伦斯和克里斯蒂安·塞格迪。2015。解释和利用对抗性样本。见于 *第3届国际学习表征会议（ICLR
    2015）论文集*。美国加州圣地亚哥。
- en: 'Grail et al. (2021) Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing
    BERT-based Transformer Architectures for Long Document Summarization. In *Proceedings
    of the 16th Conference of the European Chapter of the Association for Computational
    Linguistics: Main Volume (EACL2021)*. Online, 1792–1810.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grail et al. (2021) Quentin Grail, Julien Perez 和 Eric Gaussier. 2021. 全球化基于BERT的变换器架构用于长文档摘要.
    收录于 *第16届欧洲计算语言学协会年会论文集：主卷（EACL2021）*. 在线, 1792–1810.
- en: 'Grusky et al. (2018) Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom:
    A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies. In *Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (NAACL-HLT 2018)*. New Orleans, USA,
    708–719.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grusky et al. (2018) Max Grusky, Mor Naaman 和 Yoav Artzi. 2018. 新闻室：一个包含130万篇摘要的数据集，具有多样的提取策略.
    收录于 *2018年北美计算语言学协会年会论文集：人类语言技术（NAACL-HLT 2018）*. 美国新奥尔良, 708–719.
- en: Gu et al. (2016) Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K. Li. 2016.
    Incorporating Copying Mechanism in Sequence-to-Sequence Learning. In *Proceedings
    of the 54th Annual Meeting of the Association for Computational Linguistics (ACL
    2016)*. Berlin, Germany.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2016) Jiatao Gu, Zhengdong Lu, Hang Li 和 Victor O. K. Li. 2016. 在序列到序列学习中融入复制机制.
    收录于 *第54届计算语言学协会年会论文集（ACL 2016）*. 德国柏林.
- en: 'Guo et al. (2019) Wenzhong Guo, Jianwen Wang, and Shiping Wang. 2019. Deep
    Multimodal Representation Learning: A Survey. *IEEE Access* 7 (2019), 63373–63394.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2019) Wenzhong Guo, Jianwen Wang 和 Shiping Wang. 2019. 深度多模态表示学习：综述.
    *IEEE Access* 7 (2019), 63373–63394.
- en: Gupta and Lehal (2010) Vishal Gupta and Gurpreet Singh Lehal. 2010. A Survey
    of Text Summarization Extractive Techniques. *Journal of Emerging Technologies
    in Web Intelligence* 2, 3, 258–268.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta and Lehal (2010) Vishal Gupta 和 Gurpreet Singh Lehal. 2010. 文本摘要提取技术的调查.
    *网络智能新兴技术期刊* 2, 3, 258–268.
- en: Haghighi and Vanderwende (2009) Aria Haghighi and Lucy Vanderwende. 2009. Exploring
    Content Models for Multi-document Summarization. In *Proceedings of the 2009 Annual
    Conference of the North American Chapter of the Association for Computational
    Linguistics (HLT-NAACL 2009)*. Boulder, United States, 362–370.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haghighi and Vanderwende (2009) Aria Haghighi 和 Lucy Vanderwende. 2009. 探索多文档摘要的内容模型.
    收录于 *2009年北美计算语言学协会年会论文集（HLT-NAACL 2009）*. 美国博尔德, 362–370.
- en: Haque et al. (2013) Majharul Haque, Suraiya Pervin, Zerina Begum, et al. 2013.
    Literature Review of Automatic Multiple Documents Text Summarization. *International
    Journal of Innovation and Applied Studies* 3, 1, 121–129.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haque et al. (2013) Majharul Haque, Suraiya Pervin, Zerina Begum 等. 2013. 自动多文档文本摘要的文献综述.
    *国际创新与应用研究期刊* 3, 1, 121–129.
- en: Hirao et al. (2018) Tsutomu Hirao, Hidetaka Kamigaito, and Masaaki Nagata. 2018.
    Automatic Pyramid Evaluation Exploiting Edu-based Extractive Reference Summaries.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2018)*. Brussels, Belgium, 4177–4186.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirao et al. (2018) Tsutomu Hirao, Hidetaka Kamigaito 和 Masaaki Nagata. 2018.
    利用基于教育的提取参考摘要进行自动金字塔评价. 收录于 *2018年自然语言处理经验方法会议论文集（EMNLP 2018）*. 比利时布鲁塞尔, 4177–4186.
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long Short-term Memory. *Neural Computation* 9, 8, 1735–1780.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter and Schmidhuber (1997) Sepp Hochreiter 和 Jürgen Schmidhuber. 1997.
    长短期记忆. *神经计算* 9, 8, 1735–1780.
- en: Hornik et al. (1989) Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989.
    Multilayer feedforward networks are universal approximators. *Neural networks*
    2, 5 (1989), 359–366.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hornik et al. (1989) Kurt Hornik, Maxwell Stinchcombe 和 Halbert White. 1989.
    多层前馈网络是通用逼近器. *神经网络* 2, 5 (1989), 359–366.
- en: Hu et al. (2017) Ya-Han Hu, Yen-Liang Chen, and Hui-Ling Chou. 2017. Opinion
    Mining from Online Hotel Reviews–A Text Summarization Approach. *Information Processing
    & Management* 53, 2, 436–449.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2017) Ya-Han Hu, Yen-Liang Chen 和 Hui-Ling Chou. 2017. 从在线酒店评论中进行意见挖掘——一种文本摘要方法.
    *信息处理与管理* 53, 2, 436–449.
- en: Huang et al. (2015) Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF
    Models for Sequence Tagging. *arXiv preprint arXiv:1508.01991*.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2015) Zhiheng Huang, Wei Xu 和 Kai Yu. 2015. 双向LSTM-CRF模型用于序列标注.
    *arXiv预印本 arXiv:1508.01991*.
- en: Jain et al. (2020) Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and Byron C.
    Wallace. 2020. Learning to Faithfully Rationalize by Construction. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics (ACL
    2020)*. Online, 4459–4473.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等（2020）Sarthak Jain、Sarah Wiegreffe、Yuval Pinter 和 Byron C. Wallace。2020。通过构建学习忠实的推理。见于*第58届计算语言学协会年会论文集（ACL
    2020）*。在线，4459–4473。
- en: Jangra et al. (2020a) Anubhav Jangra, Adam Jatowt, Mohammad Hasanuzzaman, and
    Sriparna Saha. 2020a. Text-image-video Summary Generation Using Joint Integer
    Linear Programming. *Advances in Information Retrieval* 12036 (2020), 190.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jangra 等（2020a）Anubhav Jangra、Adam Jatowt、Mohammad Hasanuzzaman 和 Sriparna Saha。2020a。使用联合整数线性规划生成文本-图像-视频摘要。*信息检索进展*
    12036（2020），190。
- en: Jangra et al. (2020b) Anubhav Jangra, Sriparna Saha, Adam Jatowt, and Mohammad
    Hasanuzzaman. 2020b. Multi-modal Summary Generation Using Multi-objective Optimization.
    In *Proceedings of the 43rd International ACM SIGIR Conference on Research and
    Development in Information Retrieval (SIGIR 2020)*. 1745–1748.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jangra 等（2020b）Anubhav Jangra、Sriparna Saha、Adam Jatowt 和 Mohammad Hasanuzzaman。2020b。使用多目标优化的多模态摘要生成。见于*第43届国际ACM
    SIGIR信息检索研究与发展会议论文集（SIGIR 2020）*。1745–1748。
- en: Jangra et al. (2021) Anubhav Jangra, Sriparna Saha, Adam Jatowt, and Mohammed
    Hasanuzzaman. 2021. Multi-Modal Supplementary-Complementary Summarization using
    Multi-Objective Optimization. In *Proceedings of the 44th International ACM SIGIR
    Conference on Research and Development in Information Retrieval (SIGIR2021)*.
    Online, 818–828.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jangra 等（2021）Anubhav Jangra、Sriparna Saha、Adam Jatowt 和 Mohammed Hasanuzzaman。2021。使用多目标优化的多模态补充-互补摘要。见于*第44届国际ACM
    SIGIR信息检索研究与发展会议论文集（SIGIR2021）*。在线，818–828。
- en: Jelinek et al. (1977) Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K
    Baker. 1977. Perplexity - A Measure of the Difficulty of Speech Recognition Tasks.
    *The Journal of the Acoustical Society of America* 62, S1, S63–S63.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jelinek 等（1977）Fred Jelinek、Robert L Mercer、Lalit R Bahl 和 James K Baker。1977。困惑度
    - 语音识别任务难度的衡量标准。*美国声学学会期刊* 62, S1, S63–S63。
- en: Jia and Liang (2017) Robin Jia and Percy Liang. 2017. Adversarial Examples for
    Evaluating Reading Comprehension Systems. In *Proceedings of the 2017 Conference
    on Empirical Methods in Natural Language Processing (EMNLP 2017)*. Copenhagen,
    Denmark, 2021–2031.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 和 Liang（2017）Robin Jia 和 Percy Liang。2017。用于评估阅读理解系统的对抗样本。见于*2017年自然语言处理实证方法会议论文集（EMNLP
    2017）*。哥本哈根，丹麦，2021–2031。
- en: Jin et al. (2020) Hanqi Jin, Tianming Wang, and Xiaojun Wan. 2020. Multi-Granularity
    Interaction Network for Extractive and Abstractive Multi-Document Summarization.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL 2020)*. Online, 6244–6254.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等（2020）Hanqi Jin、Tianming Wang 和 Xiaojun Wan。2020。用于抽取式和生成式多文档摘要的多粒度交互网络。见于*第58届计算语言学协会年会论文集（ACL
    2020）*。在线，6244–6254。
- en: 'Joshi et al. (2020) Anirudh Joshi, Namit Katariya, Xavier Amatriain, and Anitha
    Kannan. 2020. Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting
    Local Structures. In *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing: Findings, (EMNLP2020)*. Online, 3755–3763.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi 等（2020）Anirudh Joshi、Namit Katariya、Xavier Amatriain 和 Anitha Kannan。2020。Dr.
    Summarize：通过利用局部结构对医疗对话进行全球总结。见于*2020年自然语言处理实证方法会议：发现论文集（EMNLP2020）*。在线，3755–3763。
- en: 'Kanapala et al. (2019) Ambedkar Kanapala, Sukomal Pal, and Rajendra Pamula.
    2019. Text Summarization from Legal Documents: A Survey. *Artificial Intelligence
    Review* 51, 3, 371–402.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanapala 等（2019）Ambedkar Kanapala、Sukomal Pal 和 Rajendra Pamula。2019。从法律文件中提取摘要：一项调查。*人工智能评论*
    51, 3, 371–402。
- en: Kim (2014) Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2014)*. Doha, Qatar, 1746–1751.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim（2014）Yoon Kim。2014。用于句子分类的卷积神经网络。见于*2014年自然语言处理实证方法会议论文集（EMNLP 2014）*。多哈，卡塔尔，1746–1751。
- en: Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised
    Classification with Graph Convolutional Networks. In *Proceedings of the 5th International
    Conference on Learning Representations (ICLR 2017)*. Toulon, France.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf 和 Welling（2017）Thomas N. Kipf 和 Max Welling。2017。带图卷积网络的半监督分类。见于*第五届国际学习表示会议论文集（ICLR
    2017）*。图卢兹，法国。
- en: 'Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020.
    Reformer: The Efficient Transformer. In *Proceedings of the 8th International
    Conference on Learning Representations (ICLR 2020)*. Addis Ababa, Ethiopia.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitaev 等人（2020）Nikita Kitaev, Lukasz Kaiser, 和 Anselm Levskaya。2020。《Reformer：高效的
    Transformer》。发表于*第八届国际学习表征会议（ICLR 2020）*。亚的斯亚贝巴，埃塞俄比亚。
- en: Koay et al. (2020) Jia Jin Koay, Alexander Roustai, Xiaojin Dai, Dillon Burns,
    Alec Kerrigan, and Fei Liu. 2020. How Domain Terminology Affects Meeting Summarization
    Performance. In *Proceedings of the 28th International Conference on Computational
    Linguistics (COLING 2020)*. Online, 5689–5695.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koay 等人（2020）Jia Jin Koay, Alexander Roustai, Xiaojin Dai, Dillon Burns, Alec
    Kerrigan, 和 Fei Liu。2020。《领域术语如何影响会议总结的表现》。发表于*第28届国际计算语言学会议（COLING 2020）*。线上，5689–5695。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet Classification with Deep Convolutional Neural Networks. In *Proceedings
    of the 26th Annual Conference on Neural Information Processing Systems (NIPS 2012)*.
    Lake Tahoe, United States, 1106–1114.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等人（2012）Alex Krizhevsky, Ilya Sutskever, 和 Geoffrey E Hinton。2012。《利用深度卷积神经网络进行
    Imagenet 分类》。发表于*第26届神经信息处理系统年会（NIPS 2012）*。太浩湖，美国，1106–1114。
- en: 'Kulkarni et al. (2020) Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and
    Eugene Ie. 2020. AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document
    Summarization. *CoRR* abs/2010.12694 (2020).'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni 等人（2020）Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, 和 Eugene
    Ie。2020。《AQuaMuSe：自动生成基于查询的多文档总结数据集》。*CoRR* abs/2010.12694 (2020)。
- en: 'Kumar and Talukdar (2020) Sawan Kumar and Partha P. Talukdar. 2020. NILE :
    Natural Language Inference with Faithful Natural Language Explanations. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics (ACL
    2020)*. Online, 8730–8742.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 和 Talukdar（2020）Sawan Kumar 和 Partha P. Talukdar。2020。《NILE：具有忠实自然语言解释的自然语言推理》。发表于*第58届计算语言学协会年会（ACL
    2020）*。线上，8730–8742。
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised
    Learning of Language Representations. In *Proceedings of the 8th International
    Conference on Learning Representations (ICLR 2020)*. Addis Ababa, Ethiopia.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 等人（2020）Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
    Sharma, 和 Radu Soricut。2020。《ALBERT：一种轻量级 BERT 用于自监督语言表示学习》。发表于*第八届国际学习表征会议（ICLR
    2020）*。亚的斯亚贝巴，埃塞俄比亚。
- en: Lebanoff et al. (2019) Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon
    Kim, Seokhwan Kim, Walter Chang, and Fei Liu. 2019. Scoring Sentence Singletons
    and Pairs for Abstractive Summarization. In *Proceedings of the 57th Conference
    of the Association for Computational Linguistics (ACL 2019)*. Florence, Italy,
    2175–2189.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lebanoff 等人（2019）Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon
    Kim, Seokhwan Kim, Walter Chang, 和 Fei Liu。2019。《对抽象总结进行句子单体和对的评分》。发表于*第57届计算语言学协会会议（ACL
    2019）*。佛罗伦萨，意大利，2175–2189。
- en: Lebanoff et al. (2018) Logan Lebanoff, Kaiqiang Song, and Fei Liu. 2018. Adapting
    the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2018)*. Brussels, Belgium, 4131–4141.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lebanoff 等人（2018）Logan Lebanoff, Kaiqiang Song, 和 Fei Liu。2018。《将神经编码器-解码器框架从单文档总结适应到多文档总结》。发表于*2018
    年自然语言处理实证方法会议（EMNLP 2018）*。布鲁塞尔，比利时，4131–4141。
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based Learning Applied to Document Recognition. *Proc. IEEE* 86,
    11, 2278–2324.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人（1998）Yann LeCun, Léon Bottou, Yoshua Bengio, 和 Patrick Haffner。1998。《基于梯度的学习应用于文档识别》。*Proc.
    IEEE* 86, 11, 2278–2324。
- en: Li et al. (2020b) Haoran Li, Peng Yuan, Song Xu, Youzheng Wu, Xiaodong He, and
    Bowen Zhou. 2020b. Aspect-Aware Multimodal Summarization for Chinese E-Commerce
    Products. In *The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI2020)*.
    New York, USA, 8188–8195.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020b）Haoran Li, Peng Yuan, Song Xu, Youzheng Wu, Xiaodong He, 和 Bowen
    Zhou。2020b。《面向中文电子商务产品的面向方面的多模态总结》。发表于*第34届 AAAI 人工智能大会（AAAI 2020）*。纽约，美国，8188–8195。
- en: Li et al. (2017c) Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, and Chengqing
    Zong. 2017c. Multi-modal Summarization for Asynchronous Collection of Text, Image,
    Audio and Video. In *Proceedings of the 2017 Conference on Empirical Methods in
    Natural Language Processing (EMNLP2017)*. Copenhagen, Denmark, 1092–1102.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2017c) 郝然·李、瞿楠·朱、丛·马、贾俊·张和程庆·宗。2017c。异步集合的文本、图像、音频和视频的多模态摘要。在 *2017年自然语言处理实证方法会议（EMNLP2017）论文集*。丹麦哥本哈根，1092–1102。
- en: 'Li et al. (2019) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and
    Kai-Wei Chang. 2019. Visualbert: A Simple and Performant Baseline for Vision and
    Language. *arXiv preprint arXiv:1908.03557* (2019).'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2019) 刘年·哈罗德·李、马克·亚茨卡、达·尹、朱惠和凯·魏·张。2019。Visualbert：视觉和语言的简单而高效的基线。*arXiv
    预印本 arXiv:1908.03557* (2019)。
- en: 'Li et al. (2017a) Piji Li, Lidong Bing, and Wai Lam. 2017a. Reader-Aware Multi-Document
    Summarization: An Enhanced Model and The First Dataset. In *Proceedings of the
    Workshop on New Frontiers in Summarization (NFiS@EMNLP 2017)*. Copenhagen, Denmark,
    91–99.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2017a) 皮吉·李、李东·冰和韦·拉姆。2017a。面向阅读者的多文档摘要：一种增强模型及第一个数据集。在 *新前沿摘要工作坊（NFiS@EMNLP
    2017）论文集*。丹麦哥本哈根，91–99。
- en: Li et al. (2017b) Piji Li, Wai Lam, Lidong Bing, Weiwei Guo, and Hang Li. 2017b.
    Cascaded Attention based Unsupervised Information Distillation for Compressive
    Summarization. In *Proceedings of the 2017 Conference on Empirical Methods in
    Natural Language Processing (EMNLP 2017)*. Copenhagen, Denmark, 2081–2090.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2017b) 皮吉·李、韦·拉姆、李东·冰、魏巍·郭和杭·李。2017b。基于级联注意力的无监督信息提取用于压缩摘要。在 *2017年自然语言处理实证方法会议（EMNLP
    2017）论文集*。丹麦哥本哈根，2081–2090。
- en: Li et al. (2020a) Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, and
    Junping Du. 2020a. Leveraging Graph to Improve Abstractive Multi-Document Summarization.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL 2020)*. Online, 6232–6243.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2020a) 韦·李、辛燕·肖、贾晨·刘、华·吴、海峰·王和俊平·杜。2020a。利用图提升抽象多文档摘要。在 *第58届计算语言学协会年会（ACL
    2020）论文集*。在线，6232–6243。
- en: 'Lin (2004) Chin-Yew Lin. 2004. Rouge: A Package for Automatic Evaluation of
    Summaries. In *Text Summarization Branches Out*. 74–81.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin (2004) 林震曜。2004。Rouge：用于自动评估摘要的工具包。在 *文本摘要的分支扩展*。74–81。
- en: Liu et al. (2019b) Chunyi Liu, Peng Wang, Jiang Xu, Zang Li, and Jieping Ye.
    2019b. Automatic Dialogue Summary Generation for Customer Service. In *Proceedings
    of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining, (KDD 2019)*. Anchorage, USA, 1957–1965.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2019b) 春怡·刘、彭·王、江·徐、藏·李和杰平·叶。2019b。用于客户服务的自动对话摘要生成。在 *第25届 ACM SIGKDD
    国际知识发现与数据挖掘大会（KDD 2019）论文集*。美国安克雷奇，1957–1965。
- en: Liu et al. (2018) Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan
    Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating Wikipedia by Summarizing
    Long Sequences. In *Proceedings of the 6th International Conference on Learning
    Representations (ICLR 2018)*. Vancouver,Canada.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2018) 彼得·J·刘、穆罕默德·萨勒、艾蒂安·波特、本·古德里奇、瑞安·塞帕西、卢卡斯·凯瑟和诺姆·沙泽尔。2018。通过总结长序列生成维基百科。在
    *第6届国际学习表征会议（ICLR 2018）论文集*。加拿大温哥华。
- en: Liu and Lapata (2019) Yang Liu and Mirella Lapata. 2019. Hierarchical Transformers
    for Multi-Document Summarization. In *Proceedings of the 57th Conference of the
    Association for Computational Linguistics (ACL 2019)*. Florence, Italy, 5070–5081.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 和 Lapata (2019) 杨刘和米雷拉·拉帕塔。2019。层次化 Transformers 用于多文档摘要。在 *第57届计算语言学协会年会（ACL
    2019）论文集*。意大利佛罗伦萨，5070–5081。
- en: 'Liu et al. (2019a) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019a.
    Roberta: A Robustly Optimized Bert Pretraining Approach. *arXiv preprint arXiv:1907.11692*.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2019a) 银汉·刘、迈尔·奥特、纳曼·戈亚尔、景飞·杜、曼达尔·乔希、丹琪·陈、奥默·莱维、迈克·刘易斯、卢克·泽特尔莫耶和韦塞林·斯托亚诺夫。2019a。Roberta：一种稳健优化的
    Bert 预训练方法。*arXiv 预印本 arXiv:1907.11692*。
- en: Louis and Nenkova (2013) Annie Louis and Ani Nenkova. 2013. Automatically Assessing
    Machine Summary Content Without A Gold Standard. *Computational Linguistics* 39,
    2, 267–300.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louis 和 Nenkova (2013) 安妮·路易斯和阿尼·嫩科娃。2013。自动评估机器摘要内容而无需黄金标准。*计算语言学* 39，2，267–300。
- en: 'Lu et al. (2020) Yao Lu, Yue Dong, and Laurent Charlin. 2020. Multi-XScience:
    A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2020)*. Online, 8068–8074.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2020）Yao Lu、Yue Dong 和 Laurent Charlin。2020。《Multi-XScience：用于极端多文档摘要的大规模数据集》。见于
    *2020年自然语言处理实证方法会议论文集（EMNLP 2020）*。在线，8068–8074。
- en: Luo and Litman (2015) Wencan Luo and Diane Litman. 2015. Summarizing Student
    Responses to Reflection Prompts. In *Proceedings of the 2015 Conference on Empirical
    Methods in Natural Language Processing (EMNLP 2015)*. Lisbon, Portugal, 1955–1960.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 和 Litman（2015）Wencan Luo 和 Diane Litman。2015。《总结学生对反思提示的回应》。见于 *2015年自然语言处理实证方法会议论文集（EMNLP
    2015）*。葡萄牙里斯本，1955–1960。
- en: 'Luo et al. (2016) Wencan Luo, Fei Liu, Zitao Liu, and Diane J. Litman. 2016.
    Automatic Summarization of Student Course Feedback. In *Proceedings of the 2016
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (NAACL-HLT 2016)*. San Diego California,
    United States, 80–85.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等（2016）Wencan Luo、Fei Liu、Zitao Liu 和 Diane J. Litman。2016。《学生课程反馈的自动摘要》。见于
    *2016年北美计算语言学协会人类语言技术会议论文集（NAACL-HLT 2016）*。美国加利福尼亚州圣地亚哥，80–85。
- en: Ma et al. (2021) Congbo Ma, Wei Emma Zhang, Hu Wang, Shubham Gupta, and Mingyu
    Guo. 2021. Incorporating Linguistic Knowledge for Abstractive Multi-document Summarization.
    *arXiv preprint arXiv:2109.11199* (2021).
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2021）Congbo Ma、Wei Emma Zhang、Hu Wang、Shubham Gupta 和 Mingyu Guo。2021。《将语言知识纳入抽象多文档摘要》。*arXiv
    预印本 arXiv:2109.11199*（2021年）。
- en: Mani and Bloedorn (1997) Inderjeet Mani and Eric Bloedorn. 1997. Multi-Document
    Summarization by Graph Search and Matching. In *Proceedings of the Fourteenth
    National Conference on Artificial Intelligence and Ninth Innovative Applications
    of Artificial Intelligence Conference (AAAI 1997)*. Providence, United States,
    622–628.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mani 和 Bloedorn（1997）Inderjeet Mani 和 Eric Bloedorn。1997。《通过图搜索和匹配进行多文档摘要》。见于
    *第十四届人工智能全国会议和第九届人工智能创新应用会议论文集（AAAI 1997）*。美国罗德岛州普罗维登斯，622–628。
- en: Mao et al. (2020) Yuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, and Jiawei Han.
    2020. Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement
    Learning. In *Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing (EMNLP 2020)*. Online, 1737–1751.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等（2020）Yuning Mao、Yanru Qu、Yiqing Xie、Xiang Ren 和 Jiawei Han。2020。《通过最大边际相关性引导的强化学习进行多文档摘要》。见于
    *2020年自然语言处理实证方法会议论文集（EMNLP 2020）*。在线，1737–1751。
- en: McBurney and McMillan (2014) Paul W McBurney and Collin McMillan. 2014. Automatic
    Documentation Generation via Source Code Summarization of Method Context. In *Proceedings
    of the 22nd International Conference on Program Comprehension (ICPC 2014)*. Hyderabad,
    India, 279–290.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McBurney 和 McMillan（2014）Paul W McBurney 和 Collin McMillan。2014。《通过源代码总结方法上下文进行自动文档生成》。见于
    *第22届国际程序理解会议论文集（ICPC 2014）*。印度海得拉巴，279–290。
- en: 'Miao and Blunsom (2016) Yishu Miao and Phil Blunsom. 2016. Language as a Latent
    Variable: Discrete Generative Models for Sentence Compression. In *Proceedings
    of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP
    2016)*. Austin, United States, 319–328.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao 和 Blunsom（2016）Yishu Miao 和 Phil Blunsom。2016。《语言作为潜变量：句子压缩的离散生成模型》。见于
    *2016年自然语言处理实证方法会议论文集（EMNLP 2016）*。美国奥斯汀，319–328。
- en: Mihalcea and Tarau (2005) Rada Mihalcea and Paul Tarau. 2005. A Language Independent
    Algorithm for Single and Multiple Document Summarization. In *Proceedings of the
    2nd International Joint Conference, Companion Volume to the Proceedings of Conference
    including Posters/Demos and Tutorial Abstracts (IJCNLP 2005)*. Jeju Island, Republic
    of Korea.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihalcea 和 Tarau（2005）Rada Mihalcea 和 Paul Tarau。2005。《一种语言无关的单文档和多文档摘要算法》。见于
    *第2届国际联合会议，会议附录（包括海报/演示和教程摘要）（IJCNLP 2005）*。韩国济州岛。
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. Distributed Representations of Words and Phrases and Their
    Compositionality. In *Proceedings of the 27th Annual Conference on Neural Information
    Processing Systems (NIPS 2013)*. Lake Tahoe, United States, 3111–3119.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等（2013）Tomas Mikolov、Ilya Sutskever、Kai Chen、Greg S Corrado 和 Jeff Dean。2013。《词语和短语的分布式表示及其组合性》。见于
    *第27届神经信息处理系统年会论文集（NIPS 2013）*。美国内华达州湖塔霍，3111–3119。
- en: Mnih et al. (2016) Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
    Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016.
    Asynchronous Methods for Deep Reinforcement Learning. In *Proceedings of the 33nd
    International Conference on Machine Learning (ICML2016)*. New York City, United
    States, 1928–1937.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 (2016) Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves,
    Timothy Lillicrap, Tim Harley, David Silver, 和 Koray Kavukcuoglu. 2016. 深度强化学习的异步方法。发表于
    *第33届国际机器学习大会（ICML 2016）论文集*。美国纽约，1928–1937。
- en: Molenaar et al. (2020) Sabine Molenaar, Lientje Maas, Verónica Burriel, Fabiano
    Dalpiaz, and Sjaak Brinkkemper. 2020. Medical Dialogue Summarization for Automated
    Reporting in Healthcare. In *Proceedings of the International Conference on Advanced
    Information Systems Engineering (CAiSE Workshops 2020)*. Grenoble, France, 76–88.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molenaar 等 (2020) Sabine Molenaar, Lientje Maas, Verónica Burriel, Fabiano Dalpiaz,
    和 Sjaak Brinkkemper. 2020. 医疗对话摘要用于自动化医疗报告。发表于 *国际先进信息系统工程会议（CAiSE Workshops 2020）论文集*。法国格勒诺布尔，76–88。
- en: 'Nallapati et al. (2017) Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
    SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization
    of Documents. In *Proceedings of the Thirty-First AAAI Conference on Artificial
    Intelligence (AAAI 2017)*. San Francisco, United States, 3075–3081.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nallapati 等 (2017) Ramesh Nallapati, Feifei Zhai, 和 Bowen Zhou. 2017. SummaRuNNer:
    基于递归神经网络的文档提取式摘要序列模型。发表于 *第31届美国人工智能协会会议（AAAI 2017）论文集*。美国旧金山，3075–3081。'
- en: 'Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018.
    Ranking Sentences for Extractive Summarization with Reinforcement Learning. In
    *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018)*.
    New Orleans, United States, 1747–1759.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan 等 (2018) Shashi Narayan, Shay B. Cohen, 和 Mirella Lapata. 2018. 使用强化学习对提取式摘要进行句子排名。发表于
    *2018年北美计算语言学协会：人类语言技术会议（NAACL-HLT 2018）论文集*。美国新奥尔良，1747–1759。
- en: Nayeem et al. (2018) Mir Tafseer Nayeem, Tanvir Ahmed Fuad, and Yllias Chali.
    2018. Abstractive Unsupervised Multi-Document Summarization using Paraphrastic
    Sentence Fusion. In *Proceedings of the 27th International Conference on Computational
    Linguistics (COLING 2018)*. Santa Fe, United States, 1191–1204.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nayeem 等 (2018) Mir Tafseer Nayeem, Tanvir Ahmed Fuad, 和 Yllias Chali. 2018.
    使用释义句子融合的抽象无监督多文档摘要。发表于 *第27届计算语言学国际会议（COLING 2018）论文集*。美国圣菲，1191–1204。
- en: Nema et al. (2017) Preksha Nema, Mitesh M. Khapra, Anirban Laha, and Balaraman
    Ravindran. 2017. Diversity driven attention model for query-based abstractive
    summarization. In *Proceedings of the 55th Annual Meeting of the Association for
    Computational Linguistics, (ACL 2017)*. Vancouver, Canada, 1063–1072.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nema 等 (2017) Preksha Nema, Mitesh M. Khapra, Anirban Laha, 和 Balaraman Ravindran.
    2017. 基于多样性的注意力模型用于查询驱动的抽象摘要。发表于 *第55届计算语言学协会年会（ACL 2017）论文集*。加拿大温哥华，1063–1072。
- en: Nenkova and McKeown (2012) Ani Nenkova and Kathleen R. McKeown. 2012. A Survey
    of Text Summarization Techniques. In *Mining Text Data*.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nenkova 和 McKeown (2012) Ani Nenkova 和 Kathleen R. McKeown. 2012. 文本摘要技术综述。发表于
    *挖掘文本数据*。
- en: 'Nenkova et al. (2007) Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown.
    2007. The Pyramid Method: Incorporating Human Content Selection Variation in Summarization
    Evaluation. *ACM Transactions on Speech and Language Processing* 4, 2, 4–es.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nenkova 等 (2007) Ani Nenkova, Rebecca Passonneau, 和 Kathleen McKeown. 2007.
    金字塔方法：在摘要评估中纳入人为内容选择的变化。*ACM 语音与语言处理学报* 4, 2, 4–es。
- en: 'Nenkova and Passonneau (2004) Ani Nenkova and Rebecca J Passonneau. 2004. Evaluating
    Content Selection in Summarization: The Pyramid Method. In *Proceedings of the
    Human Language Technology Conference of the North American Chapter of the Association
    for Computational Linguistics (HLT-NAACL 2004)*. Boston, United States, 145–152.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nenkova 和 Passonneau (2004) Ani Nenkova 和 Rebecca J Passonneau. 2004. 摘要生成中的内容选择评估：金字塔方法。发表于
    *北美计算语言学协会人类语言技术会议（HLT-NAACL 2004）论文集*。美国波士顿，145–152。
- en: Ng and Abrecht (2015) Jun-Ping Ng and Viktoria Abrecht. 2015. Better Summarization
    Evaluation with Word Embeddings for ROUGE. In *Proceedings of the 2015 Conference
    on Empirical Methods in Natural Language Processing (EMNLP 2015)*. Lisbon, Portugal,
    1925–1930.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 和 Abrecht (2015) Jun-Ping Ng 和 Viktoria Abrecht. 2015. 使用词嵌入改进 ROUGE 摘要评估。发表于
    *2015年自然语言处理经验方法会议（EMNLP 2015）论文集*。葡萄牙里斯本，1925–1930。
- en: 'Oussous et al. (2018) Ahmed Oussous, Fatima-Zahra Benjelloun, Ayoub Ait Lahcen,
    and Samir Belfkih. 2018. Big Data Technologies: A Survey. *Journal of King Saud
    University-Computer and Information Sciences* 30, 4, 431–448.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oussous et al. (2018) Ahmed Oussous, Fatima-Zahra Benjelloun, Ayoub Ait Lahcen,
    和 Samir Belfkih. 2018. 大数据技术：调查。*国王沙特大学计算机与信息科学期刊* 30, 4, 431–448。
- en: Palaskar et al. (2019) Shruti Palaskar, Jindrich Libovický, Spandana Gella,
    and Florian Metze. 2019. Multimodal Abstractive Summarization for How2 Videos.
    In *Proceedings of the 57th Conference of the Association for Computational Linguistics
    (ACL 2019)*. Florence, Italy, 6587–6596.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Palaskar et al. (2019) Shruti Palaskar, Jindrich Libovický, Spandana Gella,
    和 Florian Metze. 2019. How2 视频的多模态抽象总结。载于*第57届计算语言学协会会议论文集 (ACL 2019)*。意大利佛罗伦萨，6587–6596。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In
    *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics
    (ACL2002)*. Philadelphia, United States, 311–318.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing
    Zhu. 2002. BLEU：一种自动评估机器翻译的方法。载于*第40届计算语言学协会年会论文集 (ACL2002)*。美国费城，311–318。
- en: Passonneau et al. (2013) Rebecca J Passonneau, Emily Chen, Weiwei Guo, and Dolores
    Perin. 2013. Automated Pyramid Scoring of Summaries Using Distributional Semantics.
    In *Proceedings of the 51st Annual Meeting of the Association for Computational
    Linguistics (ACL 2013)*. Sofia, Bulgaria, 143–147.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Passonneau et al. (2013) Rebecca J Passonneau, Emily Chen, Weiwei Guo, 和 Dolores
    Perin. 2013. 使用分布语义的自动化金字塔评分。载于*第51届计算语言学协会年会论文集 (ACL 2013)*。保加利亚索非亚，143–147。
- en: Pasunuru et al. (2021a) Ramakanth Pasunuru, Asli Celikyilmaz, Michel Galley,
    Chenyan Xiong, Yizhe Zhang, Mohit Bansal, and Jianfeng Gao. 2021a. Data Augmentation
    for Abstractive Query-Focused Multi-Document Summarization. In *Proceedings of
    the AAAI Conference on Artificial Intelligence (AAAI 2021)*. Online, 13666–13674.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pasunuru et al. (2021a) Ramakanth Pasunuru, Asli Celikyilmaz, Michel Galley,
    Chenyan Xiong, Yizhe Zhang, Mohit Bansal, 和 Jianfeng Gao. 2021a. 针对抽象查询聚焦的多文档总结的数据增强。载于*AAAI人工智能会议论文集
    (AAAI 2021)*。在线，13666–13674。
- en: 'Pasunuru et al. (2021b) Ramakanth Pasunuru, Mengwen Liu, Mohit Bansal, Sujith
    Ravi, and Markus Dreyer. 2021b. Efficiently Summarizing Text and Graph Encodings
    of Multi-Document Clusters. In *Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies (NAACL-HLT 2021)*. Online, 4768–4779.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pasunuru et al. (2021b) Ramakanth Pasunuru, Mengwen Liu, Mohit Bansal, Sujith
    Ravi, 和 Markus Dreyer. 2021b. 高效总结文本和图形编码的多文档集群。载于*2021年北美计算语言学协会人类语言技术会议论文集 (NAACL-HLT
    2021)*。在线，4768–4779。
- en: Paulus et al. (2018) Romain Paulus, Caiming Xiong, and Richard Socher. 2018.
    A Deep Reinforced Model for Abstractive Summarization. In *Proceedings of the
    6th International Conference on Learning Representations (ICLR 2018)*. Vancouver,
    Canada.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paulus et al. (2018) Romain Paulus, Caiming Xiong, 和 Richard Socher. 2018. 用于抽象总结的深度强化模型。载于*第6届国际学习表征会议论文集
    (ICLR 2018)*。加拿大温哥华。
- en: 'Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized
    Word Representations. In *Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies
    (NAACL-HLT 2018)*. New Orleans, United States, 2227–2237.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, 和 Luke Zettlemoyer. 2018. 深度上下文化词表示。载于*2018年北美计算语言学协会人类语言技术会议论文集
    (NAACL-HLT 2018)*。美国新奥尔良，2227–2237。
- en: Peyrard (2019) Maxime Peyrard. 2019. A Simple Theoretical Model of Importance
    for Summarization. In *Proceedings of the 57th Conference of the Association for
    Computational Linguistics (ACL 2019)*. Association for Computational Linguistics,
    Florence, Italy, 1059–1073.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peyrard (2019) Maxime Peyrard. 2019. 一个对摘要重要的简单理论模型。载于*第57届计算语言学协会会议论文集 (ACL
    2019)*。计算语言学协会，意大利佛罗伦萨，1059–1073。
- en: 'Radev (2000) Dragomir R. Radev. 2000. A Common Theory of Information Fusion
    from Multiple Text Sources Step One: Cross-Document Structure. In *Proceedings
    of the Workshop of the 1st Annual Meeting of the Special Interest Group on Discourse
    and Dialogue (SIGDIAL 2000)*. Hong Kong, China, 74–83.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radev (2000) Dragomir R. Radev. 2000. 多文本源信息融合的共同理论 第一步：跨文档结构。载于*第1届年会特殊兴趣小组话语与对话研讨会论文集
    (SIGDIAL 2000)*。中国香港，74–83。
- en: Radev et al. (2004) Dragomir R Radev, Hongyan Jing, Małgorzata Styś, and Daniel
    Tam. 2004. Centroid-based Summarization of Multiple Documents. *Information Processing
    & Management* 40, 6, 919–938.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radev et al. (2004) Dragomir R Radev, Hongyan Jing, Małgorzata Styś, and Daniel
    Tam. 2004. 基于中心的多文档摘要。*信息处理与管理* 40, 6, 919–938。
- en: Radev et al. (2013) Dragomir R. Radev, Pradeep Muthukrishnan, Vahed Qazvinian,
    and Amjad Abu-Jbara. 2013. The ACL Anthology Network Corpus. *Lang. Resour. Evaluation*
    47, 4, 919–944.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radev et al. (2013) Dragomir R. Radev, Pradeep Muthukrishnan, Vahed Qazvinian,
    and Amjad Abu-Jbara. 2013. ACL文献网络语料库。*语言资源与评估* 47, 4, 919–944。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.
    *OpenAI Blog* 1, 8, 9.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. 语言模型是无监督的多任务学习者。*OpenAI 博客* 1, 8, 9。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *Journal
    of Machine Learning Research* 21, 140:1–140:67.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. 通过统一的文本到文本变换器探索迁移学习的极限。*机器学习研究杂志*
    21, 140:1–140:67。
- en: Rodeghero et al. (2014) Paige Rodeghero, Collin McMillan, Paul W McBurney, Nigel
    Bosch, and Sidney D’Mello. 2014. Improving Automated Source Code Summarization
    via An Eye-tracking Study of Programmers. In *Proceedings of the 36th International
    Conference on Software Engineering (ICSE 2014)*. Hyderabad, India, 390–401.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rodeghero et al. (2014) Paige Rodeghero, Collin McMillan, Paul W McBurney, Nigel
    Bosch, and Sidney D’Mello. 2014. 通过程序员眼动追踪研究改进自动源代码摘要。载于 *第36届国际软件工程大会论文集（ICSE
    2014）*。印度海得拉巴，390–401。
- en: Rudin (2019) Cynthia Rudin. 2019. Stop Explaining Black Box Machine Learning
    Models for High Stakes Decisions and Use Interpretable Models Instead. *Nature
    Machine Intelligence* 1, 5, 206–215.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rudin (2019) Cynthia Rudin. 2019. 停止解释用于高风险决策的黑箱机器学习模型，改用可解释模型。*自然机器智能* 1, 5,
    206–215。
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning Representations by Back-propagating Errors. *Nature* 323, 6088,
    533–536.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. 通过反向传播误差学习表示。*自然* 323, 6088, 533–536。
- en: Sabour et al. (2017) Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017.
    Dynamic Routing Between Capsules. In *Proceedings of the 2017 Annual Conference
    on Neural Information Processing Systems(NIPS 2017)*. Long Beach, United States,
    3856–3866.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabour et al. (2017) Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017.
    胶囊之间的动态路由。载于 *2017年神经信息处理系统年会论文集（NIPS 2017）*。美国长滩，3856–3866。
- en: 'See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. 2017.
    Get To The Point: Summarization with Pointer-Generator Networks. In *Proceedings
    of the 55th Annual Meeting of the Association for Computational Linguistics (ACL
    2017)*. Vancouver, Canada, 1073–1083.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. 2017.
    直达要点：使用指针-生成网络进行摘要。载于 *第55届计算语言学协会年会论文集（ACL 2017）*。加拿大温哥华，1073–1083。
- en: Serrano and Smith (2019) Sofia Serrano and Noah A. Smith. 2019. Is Attention
    Interpretable?. In *Proceedings of the 57th Conference of the Association for
    Computational Linguistics (ACL 2019)*. Florence, Italy, 2931–2951.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serrano and Smith (2019) Sofia Serrano and Noah A. Smith. 2019. 注意力是否可解释？载于
    *第57届计算语言学协会会议论文集（ACL 2019）*。意大利佛罗伦萨，2931–2951。
- en: ShafieiBavani et al. (2018) Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond
    Wong, and Fang Chen. 2018. A Graph-theoretic Summary Evaluation for Rouge. In
    *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2018)*. 762–767.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ShafieiBavani et al. (2018) Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond
    Wong, and Fang Chen. 2018. 一种基于图论的Rouge摘要评估方法。载于 *2018年自然语言处理经验方法会议论文集（EMNLP 2018）*。762–767。
- en: Shah and Jivani (2016) Chintan Shah and Anjali Jivani. 2016. Literature Study
    on Multi-document Text Summarization Techniques. In *Proceedings of the International
    Conference on Smart Trends for Information Technology and Computer Communications*.
    442–451.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shah and Jivani (2016) Chintan Shah and Anjali Jivani. 2016. 多文档文本摘要技术的文献综述。载于
    *国际智能趋势信息技术与计算机通信会议论文集*。442–451。
- en: 'Shapira et al. (2019) Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth
    Pasunuru, Mohit Bansal, Yael Amsterdamer, and Ido Dagan. 2019. Crowdsourcing Lightweight
    Pyramids for Manual Summary Evaluation. In *Proceedings of the Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies (NAACL-HLT 2019)*. Minneapolis, United States, 682–687.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shapira 等人（2019）Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth Pasunuru,
    Mohit Bansal, Yael Amsterdamer, 和 Ido Dagan. 2019. 众包轻量级金字塔用于手动摘要评估。发表于 *北美计算语言学协会：人类语言技术会议（NAACL-HLT
    2019）论文集*。美国明尼阿波利斯，682–687。
- en: Shirwandkar and Kulkarni (2018) Nikhil S Shirwandkar and Samidha Kulkarni. 2018.
    Extractive Text Summarization Using Deep Learning. In *Proceedings of the 2018
    Fourth International Conference on Computing Communication Control and Automation
    (ICCUBEA 2018)*. Pune, India.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shirwandkar 和 Kulkarni（2018）Nikhil S Shirwandkar 和 Samidha Kulkarni. 2018. 使用深度学习的提取式文本摘要。发表于
    *2018年第四届国际计算通信控制与自动化会议（ICCUBEA 2018）论文集*。印度浦那。
- en: 'Singh et al. (2018) Abhishek Kumar Singh, Manish Gupta, and Vasudeva Varma.
    2018. Unity in Diversity: Learning Distributed Heterogeneous Sentence Representation
    for Extractive Summarization. In *Proceedings of the 32nd AAAI Conference on Artificial
    Intelligence (AAAI 2018)*. New Orleans, United States, 5473–5480.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人（2018）Abhishek Kumar Singh, Manish Gupta, 和 Vasudeva Varma. 2018. 多样性中的统一：用于提取式摘要的分布式异构句子表示学习。发表于
    *第32届美国人工智能协会年会（AAAI 2018）论文集*。美国新奥尔良，5473–5480。
- en: Song et al. (2020) Yan Song, Yuanhe Tian, Nan Wang, and Fei Xia. 2020. Summarizing
    Medical Conversations via Identifying Important Utterances. In *Proceedings of
    the 28th International Conference on Computational Linguistics (COLING2020)*.
    Online, 717–729.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人（2020）Yan Song, Yuanhe Tian, Nan Wang, 和 Fei Xia. 2020. 通过识别重要发言总结医疗对话。发表于
    *第28届国际计算语言学会议（COLING2020）论文集*。在线，717–729。
- en: Sun and Nenkova (2019) Simeng Sun and Ani Nenkova. 2019. The Feasibility of
    Embedding Based Automatic Evaluation for Single Document Summarization. In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP
    2019)*. Hong Kong, China, 1216–1221.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 和 Nenkova（2019）Simeng Sun 和 Ani Nenkova. 2019. 嵌入式自动评估单文档摘要的可行性。发表于 *2019年自然语言处理经验方法会议与第9届国际联合自然语言处理会议（EMNLP-IJCNLP
    2019）论文集*。中国香港，1216–1221。
- en: Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing Properties
    of Neural Networks. In *Proceedings of the 2nd International Conference on Learning
    Representations (ICLR 2014)*. Banff, Canada.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等人（2014）Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
    Dumitru Erhan, Ian J. Goodfellow, 和 Rob Fergus. 2014. 神经网络的有趣属性。发表于 *第2届国际学习表征会议（ICLR
    2014）论文集*。加拿大班夫。
- en: Tan et al. (2017) Haihui Tan, Ziyu Lu, and Wenjie Li. 2017. Neural Network based
    Reinforcement Learning for Real-time Pushing on Text Stream. In *Proceedings of
    the 40th International ACM SIGIR Conference on Research and Development in Information
    Retrieval (SIGIR2017)*. Tokyo, Japan, 913–916.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等人（2017）Haihui Tan, Ziyu Lu, 和 Wenjie Li. 2017. 基于神经网络的实时文本流推送的强化学习。发表于
    *第40届国际ACM SIGIR信息检索研究与开发会议（SIGIR2017）论文集*。日本东京，913–916。
- en: Tas and Kiyani (2007) Oguzhan Tas and Farzad Kiyani. 2007. A Survey Automatic
    Text Summarization. *PressAcademia Procedia* 5, 1, 205–213.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tas 和 Kiyani（2007）Oguzhan Tas 和 Farzad Kiyani. 2007. 自动文本摘要综述。*PressAcademia
    Procedia* 5, 1, 205–213。
- en: 'Torfi et al. (2020) Amirsina Torfi, Rouzbeh A. Shirvani, Yaser Keneshloo, Nader
    Tavaf, and Edward A. Fox. 2020. Natural Language Processing Advancements By Deep
    Learning: A Survey. *CoRR* abs/2003.01200 (2020). [https://arxiv.org/abs/2003.01200](https://arxiv.org/abs/2003.01200)'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torfi 等人（2020）Amirsina Torfi, Rouzbeh A. Shirvani, Yaser Keneshloo, Nader Tavaf,
    和 Edward A. Fox. 2020. 深度学习推动的自然语言处理进展：综述。*CoRR* abs/2003.01200 (2020)。 [https://arxiv.org/abs/2003.01200](https://arxiv.org/abs/2003.01200)
- en: Tu et al. (2016) Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang
    Li. 2016. Modeling Coverage for Neural Machine Translation. In *Proceedings of
    the 54th Annual Meeting of the Association for Computational Linguistics (ACL
    2016)*. Berlin, Germany.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 等人（2016）Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, 和 Hang Li. 2016.
    神经机器翻译中的覆盖建模。发表于 *第54届计算语言学协会年会（ACL 2016）论文集*。德国柏林。
- en: Ulrich et al. (2008) Jan Ulrich, Gabriel Murray, and Giuseppe Carenini. 2008.
    A Publicly Available Annotated Corpus for Supervised Email Summarization. In *Proceedings
    of the Twenty-Third AAAI Conference on Artificial Intelligence in Enhanced Messaging
    Workshop (AAAI 2008)*. Chicago, United States.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ulrich et al. (2008) 简·乌尔里希、加布里埃尔·穆雷和朱塞佩·卡雷尼。2008。**公开可用的带注释的监督邮件摘要语料库**。发表于*第23届美国人工智能协会会议增强消息研讨会（AAAI
    2008）*。美国芝加哥。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    Is All You Need. In *Proceedings of the Annual Conference on Neural Information
    Processing Systems (NIPS 2017)*. Long Beach, United States, 5998–6008.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) 阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔玛、雅各布·乌斯科雷特、利昂·琼斯、艾丹·N·戈麦斯、卢卡斯·凯泽和伊利亚·波洛苏欣。2017。**注意力机制是你所需要的一切**。发表于*年度神经信息处理系统会议（NIPS
    2017）论文集*。美国长滩，5998–6008。
- en: Vinyals et al. (2015) Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015.
    Pointer Networks. In *Proceedings of the 2015 Annual Conference on Neural Information
    Processing Systems (NIPS 2015)*. Montreal, Canada, 2692–2700.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals et al. (2015) 奥里奥尔·维尼亚尔斯、梅尔·福图纳托和纳夫迪普·贾特利。2015。**指针网络**。发表于*2015年度神经信息处理系统会议（NIPS
    2015）论文集*。加拿大蒙特利尔，2692–2700。
- en: 'Vodolazova et al. (2013) Tatiana Vodolazova, Elena Lloret, Rafael Muñoz, and
    Manuel Palomar. 2013. Extractive Text Summarization: Can We Use the Same Techniques
    for Any Text?. In *Proceedings of the 18th International Conference on Applications
    of Natural Language to Information Systems (NLDB 2013)*. Salford, UK, 164–175.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vodolazova et al. (2013) 塔季扬娜·沃多拉佐娃、埃琳娜·利奥雷特、拉斐尔·穆尼奥斯和曼努埃尔·帕洛马尔。2013。**抽取式文本摘要：我们能否对任何文本使用相同的技术？**。发表于*第18届自然语言应用于信息系统国际会议（NLDB
    2013）论文集*。英国索尔福德，164–175。
- en: Wan and Yang (2006) Xiaojun Wan and Jianwu Yang. 2006. Improved Affinity Graph
    based Multi-document Summarization. In *Proceedings of the Human Language Technology
    Conference of the North American Chapter of the Association of Computational Linguistics
    (NAACL 2006)*. New York,United States, 336–347.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan and Yang (2006) 万晓军和杨建武。2006。**改进的基于亲和图的多文档摘要**。发表于*北美计算语言学协会人类语言技术会议（NAACL
    2006）*。美国纽约，336–347。
- en: Wan and Yang (2008) Xiaojun Wan and Jianwu Yang. 2008. Multi-document Summarization
    Using Cluster-based Link Analysis. In *Proceedings of the 31st International ACM
    SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008)*.
    Singapore, 299–306.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan and Yang (2008) 万晓军和杨建武。2008。**基于集群的链接分析的多文档摘要**。发表于*第31届国际ACM SIGIR信息检索研究与发展会议（SIGIR
    2008）论文集*。新加坡，299–306。
- en: Wang et al. (2020a) Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, and
    Xuanjing Huang. 2020a. Heterogeneous Graph Neural Networks for Extractive Document
    Summarization. In *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics (ACL 2020)*. Online, 6209–6219.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020a) 汪丹青、刘鹏飞、郑怡宁、邱希鹏和黄璇晶。2020a。**用于抽取式文档摘要的异质图神经网络**。发表于*第58届计算语言学协会年会（ACL
    2020）论文集*。在线，6209–6219。
- en: Wang et al. (2020b) Hu Wang, Qi Wu, and Chunhua Shen. 2020b. Soft Expert Reward
    Learning for Vision-and-Language Navigation. In *Proceedings of the 16th European
    Conference on Computer Vision (ECCV 2020)*. Online, 126–141.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020b) 华王、吴奇和沈春华。2020b。**视觉与语言导航的软专家奖励学习**。发表于*第16届欧洲计算机视觉会议（ECCV
    2020）论文集*。在线，126–141。
- en: 'Wang and Ling (2016) Lu Wang and Wang Ling. 2016. Neural Network-Based Abstract
    Generation for Opinions and Arguments. In *Proceedings of the 2016 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (HLT-NAACL 2016)*. San Diego California, United States,
    47–57.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Ling (2016) 王璐和林望。2016。**基于神经网络的意见和论点摘要生成**。发表于*2016年北美计算语言学协会会议：人类语言技术（HLT-NAACL
    2016）*。美国加州圣地亚哥，47–57。
- en: 'Wu et al. (2021) Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp,
    and Caiming Xiong. 2021. Controllable Abstractive Dialogue Summarization with
    Sketch Supervision. In *Findings of the Association for Computational Linguistics:
    (ACL/IJCNLP 2021)*. Online, 5108–5122.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2021) 吴建生、刘林青、刘文浩、庞图斯·斯特内托普和熊采名。2021。**具有草图监督的可控抽象对话摘要**。发表于*计算语言学协会会议成果：（ACL/IJCNLP
    2021）*。在线，5108–5122。
- en: 'Xu et al. (2020b) Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu, and Chenliang
    Li. 2020b. MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question
    Answering and Summarization. In *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics (ACL 2020)*. Online, 3586–3596.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 (2020b) Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu, 和 Chenliang Li.
    2020b. MATINF: 一个联合标注的大规模分类、问答和摘要数据集。发表于 *第58届计算语言学协会年会 (ACL 2020)*。在线, 3586–3596。'
- en: 'Xu et al. (2020a) Runxin Xu, Jun Cao, Mingxuan Wang, Jiaze Chen, Hao Zhou,
    Ying Zeng, Yuping Wang, Li Chen, Xiang Yin, Xijin Zhang, Songcheng Jiang, Yuxuan
    Wang, and Lei Li. 2020a. Xiaomingbot: A Multilingual Robot News Reporter. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics: System
    Demonstrations (ACL 2020)*. Online, 1–8.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 (2020a) Runxin Xu, Jun Cao, Mingxuan Wang, Jiaze Chen, Hao Zhou, Ying
    Zeng, Yuping Wang, Li Chen, Xiang Yin, Xijin Zhang, Songcheng Jiang, Yuxuan Wang,
    和 Lei Li. 2020a. Xiaomingbot: 一个多语言机器人新闻记者。发表于 *第58届计算语言学协会年会: 系统展示 (ACL 2020)*。在线,
    1–8。'
- en: 'Yang et al. ([n.d.]) Min Yang, Chengming Li, Fei Sun, Zhou Zhao, Ying Shen,
    and Chenglin Wu. [n.d.]. Be Relevant, Non-Redundant, and Timely: Deep Reinforcement
    Learning for Real-Time Event Summarization. In *The Thirty-Fourth AAAI Conference
    on Artificial Intelligence (AAAI2020)*. New York, USA, 9410–9417.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 ([n.d.]) Min Yang, Chengming Li, Fei Sun, Zhou Zhao, Ying Shen, 和 Chenglin
    Wu. [n.d.]. 相关、非冗余、及时: 用于实时事件摘要的深度强化学习。发表于 *第34届AAAI人工智能会议 (AAAI2020)*。美国纽约, 9410–9417。'
- en: Yang et al. (2018) Min Yang, Wei Zhao, Jianbo Ye, Zeyang Lei, Zhou Zhao, and
    Soufei Zhang. 2018. Investigating Capsule Networks with Dynamic Routing for Text
    Classification. In *Proceedings of the 2018 Conference on Empirical Methods in
    Natural Language Processing (EMNLP 2018)*. Brussels, Belgium, 3110–3119.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2018) Min Yang, Wei Zhao, Jianbo Ye, Zeyang Lei, Zhou Zhao, 和 Soufei
    Zhang. 2018. 研究用于文本分类的动态路由胶囊网络。发表于 *2018年自然语言处理经验方法会议 (EMNLP 2018)*。比利时布鲁塞尔, 3110–3119。
- en: 'Yang et al. (2016) Qian Yang, Rebecca J Passonneau, and Gerard De Melo. 2016.
    PEAK: Pyramid Evaluation via Automated Knowledge Extraction. In *Proceedings of
    the Thirtieth AAAI Conference on Artificial Intelligence (AAAI 2016)*. Phoenix,
    Arizona, 2673–2680.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 (2016) Qian Yang, Rebecca J Passonneau, 和 Gerard De Melo. 2016. PEAK:
    通过自动知识提取的金字塔评估方法。发表于 *第30届人工智能AAAI会议 (AAAI 2016)*。美国亚利桑那州凤凰城, 2673–2680。'
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
    Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized Autoregressive Pretraining
    for Language Understanding. In *Proceedings of the Annual Conference on Neural
    Information Processing System (NIPS 2019)*. Vancouver, Canada, 5754–5764.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ
    R Salakhutdinov, 和 Quoc V Le. 2019. Xlnet: 一种用于语言理解的广义自回归预训练方法。发表于 *第33届神经信息处理系统年会
    (NIPS 2019)*。加拿大温哥华, 5754–5764。'
- en: Yao et al. (2018) Kaichun Yao, Libo Zhang, Tiejian Luo, and Yanjun Wu. 2018.
    Deep Reinforcement Learning for Extractive Document Summarization. *Neurocomputing*
    284, 52–62.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 (2018) Kaichun Yao, Libo Zhang, Tiejian Luo, 和 Yanjun Wu. 2018. 深度强化学习用于提取式文档摘要。*神经计算*
    284, 52–62。
- en: 'Yasunaga et al. (2019) Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R
    Fabbri, Irene Li, Dan Friedman, and Dragomir R Radev. 2019. Scisummnet: A Large
    Annotated Corpus and Content-impact Models for Scientific Paper Summarization
    with Citation Networks. In *Proceedings of the AAAI Conference on Artificial Intelligence
    (AAAI 2019)*. Honolulu, United States, 7386–7393.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yasunaga 等人 (2019) Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R
    Fabbri, Irene Li, Dan Friedman, 和 Dragomir R Radev. 2019. Scisummnet: 一个大型标注语料库和用于科学论文摘要的内容影响模型与引用网络。发表于
    *AAAI人工智能会议 (AAAI 2019)*。美国檀香山, 7386–7393。'
- en: Yasunaga et al. (2017) Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush
    Pareek, Krishnan Srinivasan, and Dragomir R. Radev. 2017. Graph-based Neural Multi-Document
    Summarization. In *Proceedings of the 21st Conference on Computational Natural
    Language Learning (CoNLL 2017)*. Vancouver, Canada, 452–462.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yasunaga 等人 (2017) Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek,
    Krishnan Srinivasan, 和 Dragomir R. Radev. 2017. 基于图的神经多文档摘要。发表于 *第21届计算自然语言学习会议
    (CoNLL 2017)*。加拿大温哥华, 452–462。
- en: Yin and Pei (2015) Wenpeng Yin and Yulong Pei. 2015. Optimizing Sentence Modeling
    and Selection for Document Summarization. In *Proceedings of the 24th International
    Joint Conference on Artificial Intelligence (IJCAI 2015)*. Buenos Aires, Argentina,
    1383–1389.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 和 Pei (2015) Wenpeng Yin 和 Yulong Pei. 2015. 优化句子建模和选择以进行文档摘要。发表于 *第24届国际人工智能联合会议
    (IJCAI 2015)*。阿根廷布宜诺斯艾利斯, 1383–1389。
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, and Amr Ahmed. 2020. Big Bird: Transformers for Longer Sequences. In
    *Advances in Neural Information Processing Systems 33: Annual Conference on Neural
    Information Processing Systems 2020 (NeurIPS 2020)*. Online.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaheer 等（2020）Manzil Zaheer、Guru Guruganesh、Kumar Avinava Dubey、Joshua Ainslie、Chris
    Alberti、Santiago Ontañón、Philip Pham、Anirudh Ravula、Qifan Wang、Li Yang 和 Amr Ahmed。2020。《Big
    Bird：用于更长序列的 Transformer》。发表于 *神经信息处理系统年会 33（NeurIPS 2020）*。在线。
- en: Zajic et al. (2008) David M Zajic, Bonnie J Dorr, and Jimmy Lin. 2008. Single-document
    and Multi-document Summarization Techniques for Email Threads Using Sentence Compression.
    *Information Processing & Management* 44, 4, 1600–1610.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zajic 等（2008）David M Zajic、Bonnie J Dorr 和 Jimmy Lin。2008。《使用句子压缩的单文档和多文档摘要技术应用于电子邮件线程》。*信息处理与管理*
    44, 4, 1600–1610。
- en: 'Zhang et al. (2018b) Jianmin Zhang, Jiwei Tan, and Xiaojun Wan. 2018b. Adapting
    Neural Single-document Summarization Model for Abstractive Multi-document Summarization:
    A Pilot Study. In *Proceedings of the 11th International Conference on Natural
    Language Generation (INLG 2018)*. Tilburg, Netherlands, 381–390.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2018b）Jianmin Zhang、Jiwei Tan 和 Xiaojun Wan。2018b。《为抽象多文档摘要调整神经单文档摘要模型：一项初步研究》。发表于
    *第十一届国际自然语言生成会议（INLG 2018）*。荷兰蒂尔堡，381–390。
- en: 'Zhang et al. (2020d) Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J.
    Liu. 2020d. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive
    Summarization. In *Proceedings of the 37th International Conference on Machine
    Learning (ICML2020)*. Online, 11328–11339.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020d）Jingqing Zhang、Yao Zhao、Mohammad Saleh 和 Peter J. Liu。2020d。《PEGASUS：使用提取的间隙句子进行抽象摘要的预训练》。发表于
    *第37届国际机器学习会议（ICML 2020）*。在线，11328–11339。
- en: Zhang et al. (2018a) Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. 2018a.
    Interpretable Convolutional Neural Networks. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR2018)*. Salt Lake City, United
    States, 8827–8836.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2018a）Quanshi Zhang、Ying Nian Wu 和 Song-Chun Zhu。2018a。《可解释的卷积神经网络》。发表于
    *IEEE 计算机视觉与模式识别会议（CVPR 2018）*。美国盐湖城，8827–8836。
- en: 'Zhang et al. (2021) Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, and Mohit
    Bansal. 2021. EmailSum: Abstractive Email Thread Summarization. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP
    2021)*. Online, 6895–6909.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2021）Shiyue Zhang、Asli Celikyilmaz、Jianfeng Gao 和 Mohit Bansal。2021。《EmailSum：抽象电子邮件线程摘要》。发表于
    *第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（ACL/IJCNLP 2021）*。在线，6895–6909。
- en: 'Zhang et al. (2020a) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020a. BERTScore: Evaluating Text Generation with BERT. In *Proceedings
    of the 8th International Conference on Learning Representations (ICLR 2020)*.
    Addis Ababa, Ethiopia.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020a）Tianyi Zhang、Varsha Kishore、Felix Wu、Kilian Q. Weinberger 和 Yoav
    Artzi。2020a。《BERTScore：使用 BERT 评估文本生成》。发表于 *第八届国际学习表征会议（ICLR 2020）*。埃塞俄比亚亚的斯亚贝巴。
- en: 'Zhang et al. (2020b) Wei Emma Zhang, Quan Z. Sheng, Ahoud Abdulrahmn F. Alhazmi,
    and Chenliang Li. 2020b. Adversarial Attacks on Deep-learning Models in Natural
    Language Processing: A Survey. *ACM Transactions on Intelligent Systems and Technology*
    11, 3, 24:1–24:41.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020b）Wei Emma Zhang、Quan Z. Sheng、Ahoud Abdulrahmn F. Alhazmi 和 Chenliang
    Li。2020b。《自然语言处理中的深度学习模型的对抗攻击：综述》。*ACM 智能系统与技术期刊* 11, 3, 24:1–24:41。
- en: Zhang et al. (2020c) Wei Emma Zhang, Quan Z. Sheng, Adnan Mahmood, Dai Hoang
    Tran, Munazza Zaib, Salma Abdalla Hamad, Abdulwahab Aljubairy, Ahoud Abdulrahmn F.
    Alhazmi, Subhash Sagar, and Congbo Ma. 2020c. The 10 Research Topics in the Internet
    of Things. In *6th IEEE International Conference on Collaboration and Internet
    Computing (CIC 2020)*. Atlanta, USA, 34–43.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020c）Wei Emma Zhang、Quan Z. Sheng、Adnan Mahmood、Dai Hoang Tran、Munazza
    Zaib、Salma Abdalla Hamad、Abdulwahab Aljubairy、Ahoud Abdulrahmn F. Alhazmi、Subhash
    Sagar 和 Congbo Ma。2020c。《物联网中的 10 个研究主题》。发表于 *第六届 IEEE 国际协作与互联网计算会议（CIC 2020）*。美国亚特兰大，34–43。
- en: Zhang et al. (2016) Yong Zhang, Meng Joo Er, Rui Zhao, and Mahardhika Pratama.
    2016. Multiview Convolutional Neural Networks for Multidocument Extractive Summarization.
    *IEEE Transactions on Cybernetics* 47, 10, 3230–3242.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2016）Yong Zhang、Meng Joo Er、Rui Zhao 和 Mahardhika Pratama。2016。《用于多文档提取摘要的多视图卷积神经网络》。*IEEE
    网络系统学报* 47, 10, 3230–3242。
- en: 'Zhao et al. (2021) Mingjun Zhao, Shengli Yan, Bang Liu, Xinwang Zhong, Qian
    Hao, Haolan Chen, Di Niu, Bowei Long, and Weidong Guo. 2021. QBSUM: A Large-scale
    Query-based Document Summarization Dataset from real-world applications. *Comput.
    Speech Lang.* 66 (2021), 101166.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等人（2021） Mingjun Zhao、Shengli Yan、Bang Liu、Xinwang Zhong、Qian Hao、Haolan
    Chen、Di Niu、Bowei Long 和 Weidong Guo。2021年。QBSUM: 一个大规模的基于查询的文档摘要数据集，来自实际应用。*Comput.
    Speech Lang.* 66（2021），101166。'
- en: 'Zhao et al. (2019) Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M.
    Meyer, and Steffen Eger. 2019. MoverScore: Text Generation Evaluating with Contextualized
    Embeddings and Earth Mover Distance. In *Proceedings of the Conference on Empirical
    Methods in Natural Language and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP 2019)*. Hong Kong, China, 563–578.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等人（2019） Wei Zhao、Maxime Peyrard、Fei Liu、Yang Gao、Christian M. Meyer 和
    Steffen Eger。2019年。MoverScore: 通过上下文化嵌入和地球移动距离评估文本生成。在 *自然语言处理经验方法会议和第9届国际自然语言处理联合会议（EMNLP-IJCNLP
    2019）*。中国香港，563–578。'
- en: Zheng et al. (2019) Xin Zheng, Aixin Sun, Jing Li, and Karthik Muthuswamy. 2019.
    Subtopic-driven Multi-Document Summarization. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019)*. Hong Kong,
    China, 3151–3160.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2019） Xin Zheng、Aixin Sun、Jing Li 和 Karthik Muthuswamy。2019年。基于子主题的多文档摘要。在
    *2019年自然语言处理经验方法会议和第9届国际自然语言处理联合会议（EMNLP-IJCNLP 2019）*。中国香港，3151–3160。
- en: Zhong et al. (2020) Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng
    Qiu, and Xuanjing Huang. 2020. Extractive Summarization as Text Matching. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics (ACL
    2020)*. Online, 6197–6208.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等人（2020） Ming Zhong、Pengfei Liu、Yiran Chen、Danqing Wang、Xipeng Qiu 和 Xuanjing
    Huang。2020年。提取式摘要作为文本匹配。在 *第58届计算语言学协会年会（ACL 2020）会议论文集*。在线，6197–6208。
- en: 'Zhong et al. (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma,
    Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and
    Dragomir R. Radev. 2021. QMSum: A New Benchmark for Query-based Multi-domain Meeting
    Summarization. In *Proceedings of the 2021 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    (NAACL-HLT 2021)*. Online, 5905–5921.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等人（2021） Ming Zhong、Da Yin、Tao Yu、Ahmad Zaidi、Mutethia Mutuma、Rahul Jha、Ahmed
    Hassan Awadallah、Asli Celikyilmaz、Yang Liu、Xipeng Qiu 和 Dragomir R. Radev。2021年。QMSum:
    基于查询的多领域会议摘要的新基准。在 *2021年计算语言学协会北美分会：人类语言技术会议（NAACL-HLT 2021）*。在线，5905–5921。'
- en: Zhou et al. (2016) Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. 2016. Learning Deep Features for Discriminative Localization.
    In *Proceedings of the IEEE conference on Computer Vision and Pattern Recognition
    (CVPR 2016)*. Las Vegas, United States, 2921–2929.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2016） Bolei Zhou、Aditya Khosla、Agata Lapedriza、Aude Oliva 和 Antonio
    Torralba。2016年。学习用于判别定位的深度特征。在 *IEEE计算机视觉与模式识别会议（CVPR 2016）*。美国拉斯维加斯，2921–2929。
- en: 'Zhu et al. (2021) Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021.
    MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization. In
    *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, (NAACL-HLT 2021)*.
    Online, 5927–5934.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人（2021） Chenguang Zhu、Yang Liu、Jie Mei 和 Michael Zeng。2021年。MediaSum:
    一个大规模的媒体访谈数据集，用于对话摘要。在 *2021年计算语言学协会北美分会：人类语言技术会议（NAACL-HLT 2021）*。在线，5927–5934。'
- en: 'Zhu et al. (2020) Chenguang Zhu, Ruochen Xu, Michael Zeng, and Xuedong Huang.
    2020. A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain
    Pretraining. In *Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing: Findings (EMNLP 2020)*. Online, 194–203.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2020） Chenguang Zhu、Ruochen Xu、Michael Zeng 和 Xuedong Huang。2020年。用于抽象会议摘要的分层网络与跨领域预训练。在
    *2020年自然语言处理经验方法会议：发现（EMNLP 2020）*。在线，194–203。
- en: 'Zhu et al. (2018) Junnan Zhu, Haoran Li, Tianshang Liu, Yu Zhou, Jiajun Zhang,
    and Chengqing Zong. 2018. MSMO: Multimodal Summarization with Multimodal Output.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2018)*. Brussels, Belgium, 4154–4164.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人（2018） Junnan Zhu、Haoran Li、Tianshang Liu、Yu Zhou、Jiajun Zhang 和 Chengqing
    Zong。2018年。MSMO: 具有多模态输出的多模态摘要。在 *2018年自然语言处理经验方法会议（EMNLP 2018）*。比利时布鲁塞尔，4154–4164。'
- en: 'Zopf (2018) Markus Zopf. 2018. Estimating Summary Quality with Pairwise Preferences.
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018)*.
    New Orleans, United States, 1687–1696.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zopf（2018）马克斯·佐普。2018年。在*2018年北美计算语言学协会人类语言技术会议论文集（NAACL-HLT 2018）*中估计摘要质量的配对偏好。新奥尔良，美国，1687–1696。
