- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:40:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:40:32
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2303.11332] Deep Learning for Video-based Person Re-Identification: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2303.11332] 基于视频的人物重识别的深度学习：一项综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2303.11332](https://ar5iv.labs.arxiv.org/html/2303.11332)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2303.11332](https://ar5iv.labs.arxiv.org/html/2303.11332)
- en: 'Deep Learning for Video-based Person Re-Identification: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于视频的人物重识别的深度学习：一项综述
- en: Khawar \snmIslam [khawar512@gmail.com](mailto:khawar512@gmail.com) floppydisk.ai,
    Karachi, Pakistan(1 May 2013; 13 May 2013; K. Islam)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Khawar \snmIslam [khawar512@gmail.com](mailto:khawar512@gmail.com) floppydisk.ai,
    卡拉奇, 巴基斯坦（2013年5月1日；2013年5月13日；K. Islam）
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Video-based person re-identification (video re-ID) has lately fascinated growing
    attention due to its broad practical applications in various areas, such as surveillance,
    smart city, and public safety. Nevertheless, video re-ID is quite difficult and
    is an ongoing stage due to numerous uncertain challenges such as viewpoint, occlusion,
    pose variation, and uncertain video sequence, etc. In the last couple of years,
    deep learning on video re-ID has continuously achieved surprising results on public
    datasets, with various approaches being developed to handle diverse problems in
    video re-ID. Compared to image-based re-ID, video re-ID is much more challenging
    and complex. To encourage future research and challenges, this first comprehensive
    paper introduces a review of up-to-date advancements in deep learning approaches
    for video re-ID. It broadly covers three important aspects, including brief video
    re-ID methods with their limitations, major milestones with technical challenges,
    and architectural design. It offers comparative performance analysis on various
    available datasets, guidance to improve video re-ID with valuable thoughts, and
    exciting research directions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视频的人物重识别（视频重识别）最近因其在监控、智能城市和公共安全等领域的广泛实际应用而引起了越来越多的关注。然而，视频重识别非常困难且仍在进行中，因为面临许多不确定的挑战，如视角、遮挡、姿态变化和不确定的视频序列等。在过去几年中，视频重识别的深度学习在公共数据集上持续取得了惊人的成果，开发了各种方法来处理视频重识别中的不同问题。与基于图像的重识别相比，视频重识别更具挑战性和复杂性。为了鼓励未来的研究和挑战，这篇综合性的论文首次介绍了关于视频重识别的深度学习方法的最新进展。它广泛涵盖了三个重要方面，包括简要的视频重识别方法及其局限性、主要里程碑及技术挑战和架构设计。它提供了对各种现有数据集的性能比较分析，对如何改善视频重识别提供了宝贵的建议，并展望了令人兴奋的研究方向。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'Image Compression, Video Compression, HEVC, JPEG, Entropy Models^†^†journal:
    Computer Vision and Image Understanding\finalform'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图像压缩、视频压缩、HEVC、JPEG、熵模型^†^†期刊：计算机视觉与图像理解\最终版
- en: 10 May 2013 \availableonline15 May 2013
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2013年5月10日 \在线获取2013年5月15日
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the endless efforts of computer vision and deep learning researchers, deep
    learning has accomplished exceptional success in person re-ID. In a few years,
    deep learning shows remarkable results in video re-ID and gives new birth to surveillance
    systems. With the rapid improvement in multimedia technology, video re-ID has
    gained much more attention in academia and the industrial sector over the last
    ten years Zheng et al. ([2016b](#bib.bib98)); Nambiar et al. ([2019](#bib.bib55));
    Islam ([2020](#bib.bib26)). The dominant reason for video re-ID popularity is
    to provide a wide range of services for public safety such as tracking each person
    with a unique ID, preventing crimes, behavior analysis, forensic investigation,
    etc. Almasawa et al. ([2019](#bib.bib2)). In intelligent video surveillance applications,
    video re-ID is defined as recognizing an individual person through various non-overlapping
    cameras from the huge number of gallery images Chen et al. ([2020a](#bib.bib9)).
    It is one of the intriguing computer vision problems that are present among inter-camera
    variance challenges such as background clutter, occlusion, viewpoint, illumination
    changes, human pose variation and etc.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算机视觉和深度学习研究人员的不懈努力，深度学习在人员再识别方面取得了卓越的成功。近年来，深度学习在视频再识别方面表现出显著成果，并为监控系统带来了新的突破。随着多媒体技术的快速发展，视频再识别在过去十年中在学术界和工业界获得了更多关注
    Zheng 等人 ([2016b](#bib.bib98))；Nambiar 等人 ([2019](#bib.bib55))；Islam ([2020](#bib.bib26))。视频再识别流行的主要原因是为公共安全提供广泛的服务，如使用唯一
    ID 跟踪每个人、预防犯罪、行为分析、法医调查等 Almasawa 等人 ([2019](#bib.bib2))。在智能视频监控应用中，视频再识别被定义为通过各种非重叠的摄像头识别个体
    Chen 等人 ([2020a](#bib.bib9))。这是一个有趣的计算机视觉问题，存在于摄像头间变异挑战中，如背景杂乱、遮挡、视角、光照变化、人类姿态变化等。
- en: 'Table 1: Comparison between existing survey papers and our survey paper. Our
    survey paper mainly focuses on video re-ID.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：现有综述论文与我们综述论文的比较。我们的综述论文主要关注视频再识别。
- en: '|   Survey | Focus | Major Contribution | Video re-ID | Publication |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|   综述 | 重点 | 主要贡献 | 视频再识别 | 发表 |'
- en: '| Mazzon et al. ([2012](#bib.bib53)) | Crowd |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| Mazzon 等人 ([2012](#bib.bib53)) | 人群 |'
- en: '&#124; Drawbacks of existing approaches &#124;'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 现有方法的缺陷 &#124;'
- en: '&#124; Proposed simple knowledge-based method &#124;'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了简单的基于知识的方法 &#124;'
- en: '| Partial | PRL |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | PRL |'
- en: '| Satta ([2013](#bib.bib59)) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Satta ([2013](#bib.bib59)) |'
- en: '&#124; Appearance &#124;'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '&#124; Descriptors &#124;'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 描述符 &#124;'
- en: '|'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Covers public datasets with current evaluation &#124;'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 涵盖了当前评估的公共数据集 &#124;'
- en: '&#124; Raised open and closed set re-ID scenarios &#124;'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了开放和闭合集再识别场景 &#124;'
- en: '| Partial | arXiv |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | arXiv |'
- en: '| Gala and Shah ([2014](#bib.bib17)) | Open-Closed |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Gala 和 Shah ([2014](#bib.bib17)) | 开放-闭合 |'
- en: '&#124; Highlighted public datasets with current evaluation &#124;'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 突出了当前评估的公共数据集 &#124;'
- en: '&#124; Raised open and closed set re-ID scenarios &#124;'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了开放和闭合集再识别场景 &#124;'
- en: '| Partial | IVC |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | IVC |'
- en: '| Zheng et al. ([2016b](#bib.bib98)) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 郑等人 ([2016b](#bib.bib98)) |'
- en: '&#124; Image &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; & Video &#124;'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; & 视频 &#124;'
- en: '|'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Discussed history and relationship of person re-ID &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 讨论了人员再识别的历史和关系 &#124;'
- en: '&#124; Hand-crafted and DL methods are reviewed &#124;'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 综述了手工制作和深度学习方法 &#124;'
- en: '| Partial | arXiv |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | arXiv |'
- en: '| Lavi et al. ([2018](#bib.bib35)) | re-ID |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Lavi 等人 ([2018](#bib.bib35)) | 再识别 |'
- en: '&#124; Survey on deep neural networks techniques &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度神经网络技术综述 &#124;'
- en: '&#124; Covers loss function and data augmentation &#124;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 涵盖了损失函数和数据增强 &#124;'
- en: '| X | arXiv |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| X | arXiv |'
- en: '| Wang et al. ([2018a](#bib.bib67)) | re-ID |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([2018a](#bib.bib67)) | 再识别 |'
- en: '&#124; Traditional methods and architectural perspectives &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 传统方法和架构视角 &#124;'
- en: '&#124; CNN, RNN and GAN for person Re-ID &#124;'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN、RNN 和 GAN 用于人员再识别 &#124;'
- en: '| X | CAAI-TIT |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| X | CAAI-TIT |'
- en: '| Wu et al. ([2019](#bib.bib75)) | Image |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 吴等人 ([2019](#bib.bib75)) | 图像 |'
- en: '&#124; Survey of SOTA methods with feature designing &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 具有特征设计的最先进方法综述 &#124;'
- en: '&#124; Several results on ResNet and Inception &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关于 ResNet 和 Inception 的若干结果 &#124;'
- en: '| P |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| P |'
- en: '&#124; Neuro- &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 神经- &#124;'
- en: '&#124; computing &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算 &#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Masson et al. ([2019](#bib.bib52)) | Image |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Masson 等人 ([2019](#bib.bib52)) | 图像 |'
- en: '&#124; Extensively covered pruning methods, strategies &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 广泛涵盖了剪枝方法和策略 &#124;'
- en: '&#124; Performance evaluation on different datasets &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在不同数据集上的性能评估 &#124;'
- en: '| X | JIVP |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| X | JIVP |'
- en: '| Nambiar et al. ([2019](#bib.bib55)) | Gait |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Nambiar 等人 ([2019](#bib.bib55)) | 步态 |'
- en: '&#124; Covered bio-metric details, pose analysis &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 涵盖了生物特征细节、姿势分析 &#124;'
- en: '&#124; Datasets and Multi-dimensional gait &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集和多维步态 &#124;'
- en: '| X | ACM-CS |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| X | ACM-CS |'
- en: '| Leng et al. ([2019](#bib.bib36)) | Open-world |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Leng 等人 ([2019](#bib.bib36)) | 开放世界 |'
- en: '&#124; Generalized open-world re-ID &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 泛化开放世界再识别 &#124;'
- en: '&#124; Specific application driven re-ID &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特定应用驱动的再识别 &#124;'
- en: '| P | IEEE-TCSVT |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| P | IEEE-TCSVT |'
- en: '| Wang et al. ([2019b](#bib.bib74)) | Heterogeneous |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([2019b](#bib.bib74)) | 异构 |'
- en: '&#124; Focused on heterogeneous re-ID &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 专注于异构再识别 &#124;'
- en: '&#124; Problem of inter-modality discrepancies &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨模态差异问题 &#124;'
- en: '| Partial | IJCAI |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | IJCAI |'
- en: '| Wang et al. ([2020](#bib.bib66)) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([2020](#bib.bib66)) |'
- en: '&#124; Image &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; & Video &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; & 视频 &#124;'
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Extensive review of previous Re-ID methods &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 广泛回顾了以前的再识别方法 &#124;'
- en: '&#124; Briefly discussed CNN, RNN and GAN &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 简要讨论了 CNN、RNN 和 GAN &#124;'
- en: '| X | IEEE-Access |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| X | IEEE-Access |'
- en: '| Ye et al. ([2021](#bib.bib88)) | Image&Video |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Ye 等人 ([2021](#bib.bib88)) | 图像&视频 |'
- en: '&#124; Discussed closed-world and open-world re-ID &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 讨论了封闭世界和开放世界的再识别 &#124;'
- en: '&#124; Baseline for single-/cross-modality re-ID &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单模态/跨模态再识别的基线 &#124;'
- en: '| Partial | IEEE-PAMI |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | IEEE-PAMI |'
- en: '| Xiangtan et al. ([2021](#bib.bib79)) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Xiangtan 等人 ([2021](#bib.bib79)) |'
- en: '&#124; Text & &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文本 & &#124;'
- en: '&#124; Image &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Extensively reviewed person search methods &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 广泛评审了人员搜索方法 &#124;'
- en: '&#124; Feature learning and identity-driven methods &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征学习和身份驱动方法 &#124;'
- en: '| X | IJCAI |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| X | IJCAI |'
- en: '| Lin et al. ([2021](#bib.bib41)) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Lin 等人 ([2021](#bib.bib41)) |'
- en: '&#124; Image & Video &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 & 视频 &#124;'
- en: '|'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Extensively covered unsupervised methods &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 广泛覆盖了无监督方法 &#124;'
- en: '&#124; Discussion about dataset and evaluation &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 讨论数据集和评估 &#124;'
- en: '&#124; Peformance analysis and metrics &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 性能分析和度量 &#124;'
- en: '| Partial | arXiV |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | arXiV |'
- en: '| Ours | Video |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 视频 |'
- en: '&#124; Briefly discuss video re-ID methods &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 简要讨论了视频再识别方法 &#124;'
- en: '&#124; Discuss unique architectures, loss functions &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 讨论独特的架构和损失函数 &#124;'
- en: '&#124; Performance analysis of current methods &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 当前方法的性能分析 &#124;'
- en: '| Full | CVIU |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 完整 | CVIU |'
- en: '|    |  |  |  |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|    |  |  |  |  |'
- en: Video re-ID is an extended way of image-based person re-ID. Rather than comparing
    image pairs, pairs of video sequences are provided to the re-ID algorithm. The
    essential and important task of the video re-ID algorithm is to obtain temporal
    features from video sequences. Compare with image-based information, videos naturally
    comprise more information and evidence than individual images. Lately, numerous
    methods have been developed for video re-ID Zhou et al. ([2017](#bib.bib99));
    Zhang et al. ([2017](#bib.bib93)). Most existing approaches emphasize extracting
    spatial and temporal features present in a video and then applying the re-ID algorithm
    to obtained features. In general, taking a video from different surveillance cameras
    like CCTV from different outside places. Then, detect persons in a video sequence
    and create a bounding box on it. Due to the high volume of data, it is difficult
    to draw manually bounding boxes and annotate each person’s image for training.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 视频再识别是一种基于图像的人员再识别的扩展方式。与比较图像对不同，视频再识别算法提供的是视频序列对。视频再识别算法的核心任务是从视频序列中获取时间特征。与基于图像的信息相比，视频自然包含比单独图像更多的信息和证据。最近，针对视频再识别的方法已经得到大量发展，Zhou
    等人 ([2017](#bib.bib99)); Zhang 等人 ([2017](#bib.bib93))。大多数现有方法强调从视频中提取空间和时间特征，然后将再识别算法应用于获得的特征。通常，视频来自不同监控摄像头，如不同场所的
    CCTV。然后，在视频序列中检测人员并创建边界框。由于数据量庞大，手动绘制边界框并为训练标注每个人的图像是困难的。
- en: Different studies Ren et al. ([2015](#bib.bib58)); Li et al. ([2017](#bib.bib37));
    Lan et al. ([2018](#bib.bib34)) trained detectors to detect persons in a video
    sequence. Next, training a new re-ID model on highly noisy data based on previously
    annotated data. At last, giving query (probe) person image to re-ID model to find
    query person in a large set of candidate gallery Ye et al. ([2021](#bib.bib88)).
    The main role of video re-ID is to extract spatiotemporal features from video
    sequences. Some previous studies directly utilized person re-ID methods for images
    with some extension and applied for video. These approaches extract spatiotemporal
    information from each image independently by utilizing a recurrent neural network,
    feature aggregation function, and different pooling operations to obtain a frame-level
    information (e.g. appearance) representation. These above-mentioned techniques
    view different video frames with equal importance when needed frame-level features.
    However, these approaches extract abstract-level global features from the human
    body, while ignoring several local visual cues from a body such as a gait, hairs
    and etc.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的研究如Ren等人（[2015](#bib.bib58)）、Li等人（[2017](#bib.bib37)）、Lan等人（[2018](#bib.bib34)）训练了检测器以在视频序列中检测人物。接下来，在基于先前注释数据的高度噪声数据上训练一个新的re-ID模型。最后，将查询（探测）人物图像输入到re-ID模型中，以在大量候选库中找到查询人物Ye等人（[2021](#bib.bib88)）。视频re-ID的主要作用是从视频序列中提取时空特征。一些先前的研究直接利用了针对图像的人物re-ID方法，并进行了扩展以应用于视频。这些方法通过利用递归神经网络、特征聚合函数和不同的池化操作，从每个图像中独立提取时空信息（例如外观）表示。这些上述技术在需要帧级特征时，将不同的视频帧视为同等重要。然而，这些方法从人体中提取抽象级别的全局特征，同时忽略了身体的多个局部视觉线索，如步态、头发等。
- en: Person re-ID in videos taken by multiple non-overlapping cameras which is a
    more practical implementation than images and achieves growing research trends
    Wang et al. ([2014](#bib.bib69)); Zhou et al. ([2017](#bib.bib99)). In practical
    terms, videos captured from surveillance cameras with the involvement of pedestrians
    are the actual videos for person re-ID because these videos contain useful abundant
    information and spatial temporal features of a pedestrian that includes different
    human poses with diverse view angles. Nevertheless, recognizing discriminative
    portions of pedestrians against noisy data and extracting their features is an
    intriguing vision problem that is complicated for matching persons. Several video
    re-ID methods McLaughlin et al. ([2017](#bib.bib54)); Zhang et al. ([2017](#bib.bib93))
    utilize CNN and RNN networks to extract spatio-temporal features from images and
    employ a pooling strategy to aggregate them. However, following these procedures,
    the task of matching persons becomes more sensitive when there are some noisy
    samples in data due to cluttered background or occlusion. While comparing two
    images of a person, each frame contributes equally to the matching task. For instance,
    if two persons are occupied with the same occluded object, the same appearance
    on occluded objects gives a false positive result in person re-ID.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个非重叠摄像头拍摄的视频中的人物re-ID，比图像中的实现更具实用性，并且研究趋势不断增长Wang等人（[2014](#bib.bib69)）；Zhou等人（[2017](#bib.bib99)）。在实际操作中，涉及行人的监控摄像头拍摄的视频是人物re-ID的实际视频，因为这些视频包含了有用的丰富信息和行人的时空特征，包括不同的人体姿势和多样的视角。然而，识别在噪声数据中区分度高的行人的部分并提取其特征是一个引人入胜的视觉问题，这使得人物匹配变得复杂。几种视频re-ID方法McLaughlin等人（[2017](#bib.bib54)）；Zhang等人（[2017](#bib.bib93)）利用CNN和RNN网络从图像中提取时空特征，并采用池化策略进行聚合。然而，遵循这些程序后，当数据中存在一些噪声样本时，由于背景杂乱或遮挡，匹配人物的任务变得更加敏感。在比较两张人物图像时，每一帧对匹配任务的贡献都是相同的。例如，如果两个人被相同的遮挡物占据，则遮挡物上的相同外观会导致人物re-ID中的假阳性结果。
- en: '![Refer to caption](img/753c774e53465e44104622f96dba0bf3.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/753c774e53465e44104622f96dba0bf3.png)'
- en: 'Figure 1: Timeline of the top-performing methods for video re-ID task.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：视频re-ID任务的顶尖方法时间线。
- en: 1.1 Contribution of this survey paper
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 本文的贡献
- en: 'Most of the researchers focused on and surveyed traditional re-ID methods.
    Several survey papers covered conventional techniques including feature learning
    and distance learning, and some of them broadly covered deep learning techniques
    for re-ID. As far as our deep analysis, there is no survey paper discussing the
    recent video re-ID methods, novel loss functions, architectural designs, and approaches
    for video re-ID perspective. In this paper, we discuss comprehensive recent methods
    published in top-tier conferences and journals. In a nutshell, the contributions
    discussed in this survey paper are summarized as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数研究人员集中于调查传统的 Re-ID 方法。几篇调查论文涵盖了包括特征学习和距离学习在内的传统技术，其中一些广泛覆盖了 Re-ID 的深度学习技术。根据我们的深入分析，目前尚无调查论文讨论最新的视频
    Re-ID 方法、创新损失函数、架构设计和视频 Re-ID 视角的方法。本文讨论了在顶级会议和期刊上发表的全面的最新方法。总之，本调查论文中讨论的贡献总结如下：
- en: '1.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: To the best of our knowledge, this is the very first review paper to extensively
    cover deep learning methods for video re-ID instead of all types of person re-ID
    compared with recent existing surveys Ye et al. ([2021](#bib.bib88)); Wu et al.
    ([2019](#bib.bib75)); Almasawa et al. ([2019](#bib.bib2)).
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一篇广泛覆盖视频 Re-ID 深度学习方法的综述论文，而不是所有类型的人员 Re-ID，相较于最近的现有调查 Ye et al. ([2021](#bib.bib88));
    Wu et al. ([2019](#bib.bib75)); Almasawa et al. ([2019](#bib.bib2)).
- en: '2.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We comprehensively cover deep learning techniques for video re-ID from multiple
    aspects, including global appearance methods, local part alignment methods, attention
    methods, graph methods, and transformer methods.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从多个方面全面覆盖了视频 Re-ID 的深度学习技术，包括全局外观方法、局部部件对齐方法、注意力方法、图方法和变换器方法。
- en: '3.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: This survey paper broadly covers architectural designs, novel loss functions,
    existing work, and the rapid progress of deep learning for video re-ID. Thus,
    it gives the readers to overlook the entire video re-ID work.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查论文广泛涵盖了视频 Re-ID 的架构设计、创新损失函数、现有工作以及深度学习的快速进展。因此，它让读者能够俯瞰整个视频 Re-ID 的工作。
- en: '4.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Extensive comparison of top-ranking results on the benchmark datasets is performed.
    The development of video Re-ID and the challenges affecting video Re-ID systems
    are discussed, and a brief review and future discussion are given.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对基准数据集上的顶级结果进行了广泛比较。讨论了视频 Re-ID 的发展及其挑战，并提供了简要的回顾和未来讨论。
- en: 1.2 Review of existing survey papers
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 现有调查论文的回顾
- en: 'Our recent survey offers a comprehensive and in-depth review of video re-ID
    and is distinct from previous and existing surveys and studies, as we broadly
    include the particular area of intelligent video surveillance and its practical
    usage. A detailed literature of previous survey articles aiming at person re-ID
    and its deep learning-based approaches is present, and some of them focused on
    open-world and close-world re-ID. Still, as far as we know, there is no previous
    paper that deeply focuses on video-based person re-ID from a practical point of
    view. We classify the previous and existing work of literature on person re-ID
    into five major categories named re-ID in the crowd, DL for re-ID, appearance
    descriptor for re-ID, opened the world, and closed re-ID. The comprehensive application
    scenario, relevant contributions, and special considered factors about past survey
    papers are described in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep Learning
    for Video-based Person Re-Identification: A Survey"). The influential work on
    person re-ID applications is mentioned in Zheng et al. ([2016b](#bib.bib98));
    Almasawa et al. ([2019](#bib.bib2)).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '我们最近的调查提供了对视频 Re-ID 的全面而深入的回顾，与之前和现有的调查和研究不同，因为我们广泛地包含了智能视频监控及其实际应用领域。提供了针对人员
    Re-ID 及其基于深度学习方法的前期调查文章的详细文献，其中一些集中于开放世界和封闭世界的 Re-ID。尽管如此，据我们所知，尚无以前的论文从实际角度深入关注基于视频的人员
    Re-ID。我们将人员 Re-ID 的前期和现有文献工作分为五大类：人群中的 Re-ID、用于 Re-ID 的深度学习、Re-ID 的外观描述符、开放世界和封闭
    Re-ID。关于过去调查论文的综合应用场景、相关贡献和特别考虑因素在表 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep
    Learning for Video-based Person Re-Identification: A Survey")中描述。人员 Re-ID 应用中的有影响力工作提到
    Zheng et al. ([2016b](#bib.bib98)); Almasawa et al. ([2019](#bib.bib2)).'
- en: Mazzon et al. ([2012](#bib.bib53)) presented a state-of-the-art (SOTA) re-ID
    framework for crowd applications and implementation of the practical framework
    in crowded scenes where people’s movement captured from body appearance and comprehensively
    covered the discussion about person re-ID applications in terms of appearance
    features (color, texture, shape), association (distance, learning, optim) and
    calibration (color, spatial-temp). Similarly, Riccardo Satta ([2013](#bib.bib59)),
    provided a comprehensive overview of appearance descriptors and challenging issues
    i.e., illumination changes, partial occlusion, changes in color response and pose
    and viewpoint variations. Furthermore, they covered global and local features
    with some “other cues”. Furthermore, the author in Gala and Shah ([2014](#bib.bib17))
    broadly discussed person re-ID problems, especially challenges related to the
    system level and component level. The authors discussed possible re-ID scenarios
    with comprehensively covered public datasets, evaluation metrics, and current
    work in re-ID.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Mazzon 等人 ([2012](#bib.bib53)) 提出了一个最先进的 (SOTA) 再识别框架，用于人群应用及在拥挤场景中的实际框架实现，其中通过身体外观捕捉到的人员移动被综合讨论了基于外观特征（颜色、纹理、形状）、关联（距离、学习、优化）和校准（颜色、空间-时间）的人再识别应用。类似地，Riccardo
    Satta ([2013](#bib.bib59)) 提供了对外观描述符和挑战性问题的全面概述，例如光照变化、部分遮挡、颜色响应变化以及姿势和视角变化。此外，他们还讨论了全局和局部特征以及一些“其他线索”。此外，Gala
    和 Shah ([2014](#bib.bib17)) 的作者广泛讨论了人再识别问题，特别是系统级和组件级的挑战。作者讨论了可能的再识别场景，并全面覆盖了公共数据集、评估指标以及当前的再识别工作。
- en: One of the major and remarkable surveys Zheng et al. ([2016b](#bib.bib98)) focused
    on each aspect of re-ID and connected with instance retrieval and image classification.
    A brief milestone and technical perspectives of image/video-based re-ID with hand-crafted
    methods were discussed. Traditional methods for person re-ID Wang et al. ([2018a](#bib.bib67))
    were highlighted with further extended deep learning approaches such as CNN, RNN,
    and GAN to achieve person re-ID task and covered advantages and disadvantages.
    Similarly, the author in Lavi et al. ([2018](#bib.bib35)) briefly discussed person
    re-ID from a video surveillance perspective and covered specific novel re-ID loss
    functions. The authors provided detailed re-ID approaches and divided them into
    i.e., recognized, verified deep model, distance learning metrics, feature learning,
    video-based person re-ID models, and data augmentation models. Further, they also
    conducted some experiments on the base model with several people re-ID methods
    Wu et al. ([2019](#bib.bib75)). In Masson et al. ([2019](#bib.bib52)), a detailed
    analysis of pruning techniques for compressing re-ID models is presented. To strengthen
    person the work, the authors further experimented with different pruning techniques
    and applied them to the deep siamese neural network. Their finding shows that
    pruning methods substantially decrease the number of parameters without decreasing
    accuracy.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一项重要且显著的综述 Zheng 等人 ([2016b](#bib.bib98)) 重点关注了再识别的各个方面，并与实例检索和图像分类进行了关联。讨论了基于图像/视频的再识别的简要里程碑和技术视角，包括手工制作的方法。传统的再识别方法由
    Wang 等人 ([2018a](#bib.bib67)) 突出了，并进一步扩展了深度学习方法，如 CNN、RNN 和 GAN，以实现人再识别任务，并涵盖了优缺点。类似地，Lavi
    等人 ([2018](#bib.bib35)) 从视频监控的角度简要讨论了人再识别，并覆盖了具体的新型再识别损失函数。作者提供了详细的再识别方法，并将其分为：识别、验证深度模型、距离学习指标、特征学习、基于视频的人再识别模型和数据增强模型。此外，他们还在基础模型上进行了一些实验，涉及多种人再识别方法
    Wu 等人 ([2019](#bib.bib75))。在 Masson 等人 ([2019](#bib.bib52)) 的研究中，提出了用于压缩再识别模型的剪枝技术的详细分析。为了增强工作，作者进一步实验了不同的剪枝技术，并将其应用于深度孪生神经网络。他们的发现表明，剪枝方法显著减少了参数数量而不降低准确性。
- en: Different from previous surveys, a gait-based Nambiar et al. ([2019](#bib.bib55))
    person re-ID has been discussed and highlighted various biometric regions in the
    human body i.e., hard biometrics including face identity, fingerprint, DNA, eye
    retina, and palmprint Islam et al. ([2021](#bib.bib27), [2022](#bib.bib28)). Similarly,
    soft biometrics are related to body measurement, eye color, gait, and hair/beard/mustache.
    Particularly, the authors in Almasawa et al. ([2019](#bib.bib2)) briefly discussed
    traditional and deep learning-based popular architectures and categories into
    image and video re-ID. Additionally, they compared to rank 1 results with SOTA
    methods and highlighted important challenges with future direction. Mostly person
    re-ID systems designed for closed-world settings, in Leng et al. ([2019](#bib.bib36)),
    the authors focused on open-world settings and discussed new trends of person
    re-ID in that area. They analyzed inconsistencies between open and closed-world
    applications and briefly discussed data-driven methods. Several specific surveys
    Mazzon et al. ([2012](#bib.bib53)); Nambiar et al. ([2019](#bib.bib55)); Wang
    et al. ([2019b](#bib.bib74)) presented a depth literature review of some particular
    areas like heterogeneous re-ID Wang et al. ([2019b](#bib.bib74)), the author studied
    the concept of Hetero re-ID. They provided a comprehensive literature review in
    infrared images, low-resolution images, text, and sketches. Afterward, the authors
    analyzed various datasets with evaluation metrics by giving future insights and
    providing new challenges areas in Hetero re-ID.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的综述不同，Nambiar 等人 ([2019](#bib.bib55)) 讨论了基于步态的人再识别，并突出了人体各种生物特征区域，例如，包括面部身份、指纹、DNA、眼睛视网膜和掌纹的硬生物特征
    Islam 等人 ([2021](#bib.bib27), [2022](#bib.bib28))。类似地，软生物特征与身体测量、眼睛颜色、步态以及头发/胡须/小胡子有关。特别是，Almasawa
    等人 ([2019](#bib.bib2)) 简要讨论了传统的和基于深度学习的流行架构，并将其分类为图像和视频再识别。此外，他们将排名第1的结果与 SOTA
    方法进行了比较，并强调了重要挑战及未来方向。大多数为封闭世界设置设计的人再识别系统，在 Leng 等人 ([2019](#bib.bib36)) 中，作者专注于开放世界设置，并讨论了该领域人再识别的新趋势。他们分析了开放世界和封闭世界应用之间的不一致性，并简要讨论了数据驱动的方法。一些具体的综述
    Mazzon 等人 ([2012](#bib.bib53)); Nambiar 等人 ([2019](#bib.bib55)); Wang 等人 ([2019b](#bib.bib74))
    提供了关于一些特定领域的深度文献综述，如异质再识别 Wang 等人 ([2019b](#bib.bib74))，作者研究了异质再识别的概念。他们在红外图像、低分辨率图像、文本和草图方面提供了全面的文献综述。随后，作者通过提供未来见解和提出异质再识别中的新挑战领域，分析了各种数据集及其评估指标。
- en: 'Recently, the author Ye et al. ([2021](#bib.bib88)) conducted an extensive
    literature review of deep learning-based re-ID. Instead of focusing on an overview,
    they briefly covered limitations and advantages. The new AGW baseline is designed
    with a novel evaluation metric (mINP) for single and cross-modality re-ID tasks.
    However, the above survey papers of all presented covered person re-ID surveys
    do not focus on recent methods of VID re-ID and their solutions for intelligent
    video surveillance and practical applications. Precisely, we cover recent novel
    loss functions designed for video re-ID, architectural design, brief technical
    aspects of significant papers, and broadly discuss performance analysis with the
    most frequent datasets used for video-based re-ID. Several popular methods are
    illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for
    Video-based Person Re-Identification: A Survey")'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，作者 Ye 等人 ([2021](#bib.bib88)) 对基于深度学习的再识别进行了广泛的文献综述。他们没有集中于概述，而是简要地覆盖了局限性和优点。新的
    AGW 基线设计了一个新颖的评估指标（mINP），用于单一和跨模态再识别任务。然而，以上综述论文中所有呈现的涵盖人再识别的综述并未关注 VID 再识别的最新方法及其在智能视频监控和实际应用中的解决方案。具体来说，我们涵盖了为视频再识别设计的最新损失函数、架构设计、重要论文的简要技术方面，并广泛讨论了与视频基础再识别最常用的数据集相关的性能分析。一些流行的方法在图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Video-based Person Re-Identification:
    A Survey") 中进行了说明。'
- en: 2 Video re-ID Methods
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 视频再识别方法
- en: 'This section discusses the feature representation learning approaches for video
    re-ID. We divide it into five main categories: a) Global Appearance Methods ([subsection 2.1](#S2.SS1
    "2.1 Global Appearance Methods ‣ 2 Video re-ID Methods ‣ Deep Learning for Video-based
    Person Re-Identification: A Survey")) b) Local Part Alignments Methods ([subsection 2.2](#S2.SS2
    "2.2 Local Part Alignments Methods ‣ 2 Video re-ID Methods ‣ Deep Learning for
    Video-based Person Re-Identification: A Survey")) c) Attention Methods ([subsection 2.3](#S2.SS3
    "2.3 Attention Methods ‣ 2 Video re-ID Methods ‣ Deep Learning for Video-based
    Person Re-Identification: A Survey")) d) Graphs Methods ([subsection 2.4](#S2.SS4
    "2.4 Graph Methods ‣ 2 Video re-ID Methods ‣ Deep Learning for Video-based Person
    Re-Identification: A Survey")) and e) Transformers Methods ([subsection 2.5](#S2.SS5
    "2.5 Transformer Methods ‣ 2 Video re-ID Methods ‣ Deep Learning for Video-based
    Person Re-Identification: A Survey")).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '本节讨论了视频再识别的特征表示学习方法。我们将其分为五大类：a) 全球外观方法 ([subsection 2.1](#S2.SS1 "2.1 Global
    Appearance Methods ‣ 2 Video re-ID Methods ‣ Deep Learning for Video-based Person
    Re-Identification: A Survey")) b) 局部部件对齐方法 ([subsection 2.2](#S2.SS2 "2.2 Local
    Part Alignments Methods ‣ 2 Video re-ID Methods ‣ Deep Learning for Video-based
    Person Re-Identification: A Survey")) c) 注意力方法 ([subsection 2.3](#S2.SS3 "2.3
    Attention Methods ‣ 2 Video re-ID Methods ‣ Deep Learning for Video-based Person
    Re-Identification: A Survey")) d) 图方法 ([subsection 2.4](#S2.SS4 "2.4 Graph Methods
    ‣ 2 Video re-ID Methods ‣ Deep Learning for Video-based Person Re-Identification:
    A Survey")) 和 e) 变换器方法 ([subsection 2.5](#S2.SS5 "2.5 Transformer Methods ‣ 2
    Video re-ID Methods ‣ Deep Learning for Video-based Person Re-Identification:
    A Survey"))。'
- en: 2.1 Global Appearance Methods
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 全球外观方法
- en: This class of methods extracts a single feature vector from a person’s image
    without any supplementary information. Since person re-ID is originally applied
    for person retrieval problems Zhang et al. ([2020a](#bib.bib91)), learning global
    feature is often ignored in previous studies when incorporating existing DL approaches
    into the video re-ID domain. As a pioneering work, Niall et al. ([2016](#bib.bib56))
    introduces the first Recurrent Deep Neural Network (RDNN) architecture based on
    pooling and re-currency mechanism to combine all time-step data into a single
    feature vector.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这类方法从人的图像中提取单一特征向量，而无需任何补充信息。由于人再识别最初应用于人检索问题 Zhang 等 ([2020a](#bib.bib91))，在将现有的深度学习方法应用于视频再识别领域时，全球特征学习通常被忽视。作为开创性工作，Niall
    等 ([2016](#bib.bib56)) 介绍了基于池化和递归机制的首个递归深度神经网络（RDNN）架构，将所有时间步数据结合成一个特征向量。
- en: To compare different temporal modeling methods, Gao and Nevatia ([2018](#bib.bib18))
    comprehensively study 3D ConvNET, RNN, temporal pooling, and temporal attention
    by fixed baseline architecture trained with triplet and softmax cross-entropy
    losses. Fu et al. ([2019](#bib.bib16)) address large-scale video re-ID problem
    by introducing their Spatial Temporal Attention (STA) method. Rather than extracting
    direct frame-level clues by using average pooling, a 2D ST map is used to measure
    clip-level feature representation without any additional clues. Generally, features
    extracted from a single frame contain a lot of noise, illumination, occlusion,
    and different postures. This results in the loss of discriminative information
    (e.g., appearance and motion). Refining Recurrent Unit (RRU) Liu et al. ([2019b](#bib.bib50))
    recovers the missing parts with the help of motion context and appearance from
    the previous frame.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较不同的时间建模方法，Gao 和 Nevatia ([2018](#bib.bib18)) 对 3D ConvNET、RNN、时间池化和时间注意力进行了综合研究，采用了使用三元组和软最大交叉熵损失训练的固定基线架构。Fu
    等 ([2019](#bib.bib16)) 通过引入他们的空间时间注意力（STA）方法解决了大规模视频再识别问题。他们使用 2D ST 图来测量片段级别的特征表示，而不是通过平均池化直接提取帧级线索。通常，单帧提取的特征包含大量噪声、光照、遮挡和不同的姿势。这导致了判别信息的丢失（例如外观和运动）。Refining
    Recurrent Unit (RRU) Liu 等 ([2019b](#bib.bib50)) 利用前一帧的运动上下文和外观来恢复缺失的部分。
- en: Another popular solution is to explicitly handle alignment problem corruption
    using occluded regions. Li et al. ([2018](#bib.bib40)) employs a unique diversity
    regularization expression formulated on Hellinger distance to verify the SA models
    which do not find similar body parts. Zhao et al. ([2019](#bib.bib95)) propose
    an attribute-based technique for feature re-weighting frame and disentanglement.
    Single frame features are divided into different categories of sub-features, and
    each category defines a specific semantic attribute. A two-stream network Song
    et al. ([2019](#bib.bib63)) that jointly handle detailed and holistic features
    utilize an attention approach to extract feature at the global level. Another
    network captures local features from the video and enhances the discriminative
    ST features by combining these two features.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的解决方案是通过使用遮挡区域显式处理对齐问题的损坏。Li等人([2018](#bib.bib40))采用了一种基于Hellinger距离的独特多样性正则化表达式来验证SA模型，这些模型未能找到相似的身体部位。Zhao等人([2019](#bib.bib95))提出了一种基于属性的技术用于特征重加权和解缠结。单帧特征被分为不同类别的子特征，每个类别定义一个特定的语义属性。Song等人([2019](#bib.bib63))的双流网络联合处理细节和整体特征，利用注意力方法在全局层面提取特征。另一个网络从视频中捕获局部特征，并通过结合这两种特征来增强判别性ST特征。
- en: Different from Zhang et al. ([2020b](#bib.bib94)), a Global-guided Reciprocal
    Learning (GRL) framework Liu et al. ([2021d](#bib.bib48)) extracts fine-grained
    information in an image sequence. Based on local and global features, Global-guided
    Correlation Estimation (GCE) module generates feature correlation maps, locating
    low and high correlation regions to identify similar persons. Further, to handle
    multiple memory units and enhance temporal features, Temporal Reciprocal Learning
    (TRL) is constructed to gather specific clues. Li et al. ([2021](#bib.bib39))
    improve the global appearance by jointly investigating global and local region
    alignments by considering inter-frame relations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与Zhang等人([2020b](#bib.bib94))不同，Liu等人([2021d](#bib.bib48))的全球引导递归学习（GRL）框架提取图像序列中的细粒度信息。基于局部和全局特征，全球引导相关性估计（GCE）模块生成特征相关性图，定位低和高相关区域以识别相似的人。进一步地，为了处理多个记忆单元并增强时间特征，构建了时间递归学习（TRL）以收集特定线索。Li等人([2021](#bib.bib39))通过考虑帧间关系改进了全球外观，通过共同研究全局和局部区域对齐来提高效果。
- en: 2.2 Local Part Alignments Methods
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 局部部分对齐方法
- en: These methods extract local part/region that effectively prevents misalignment
    with other frames in a tracklet. Considering the persistent body structure with
    the combination of inconsistent body parts in terms of appearance, they are new
    to each other. The goal is to distinguish personal images based on visual similarity.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法提取局部部分/区域，有效防止与轨迹中的其他帧对齐错误。考虑到外观上不一致的身体部位与持久身体结构的组合，它们彼此之间是新的。目标是基于视觉相似性区分个人图像。
- en: To preserve structural relationship details, the Structural Relationship Learning
    (SRL) Bao et al. ([2019](#bib.bib5)) is proposed to extract structural relations
    in a refined and efficient way. SRL helps convolutional features to make the relation
    useful between regions and GCN. GCN allows learning the feature representations
    of the hidden layers which encode node features and local structural information
    of graph. Another popular solution is Spatial-Temporal Completion network (STCnet)
    Hou et al. ([2019](#bib.bib24)), a method explicitly handles partial occlusion
    by recovering the occluded part appearance. Region-based Quality Estimation Network
    (RQEN) Song et al. ([2018b](#bib.bib62)) designs an end-to-end training technique
    with gradient and learns the partial quality of each person image and aggregates
    complementary partial details of video frames in a sequence.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持结构关系的细节，Bao等人提出的结构关系学习（SRL）([2019](#bib.bib5))以一种精细而高效的方式提取结构关系。SRL帮助卷积特征使区域和GCN之间的关系变得有用。GCN允许学习隐藏层的特征表示，这些特征表示编码节点特征和图的局部结构信息。另一种流行的解决方案是空间-时间补全网络（STCnet）Hou等人([2019](#bib.bib24))，这是一种通过恢复遮挡部分的外观来显式处理部分遮挡的方法。基于区域的质量估计网络（RQEN）Song等人([2018b](#bib.bib62))设计了一种端到端训练技术，通过梯度学习每个人图像的部分质量，并在序列中聚合视频帧的互补部分细节。
- en: '![Refer to caption](img/98e07c274c01e5a25da09d00e3eee04c.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/98e07c274c01e5a25da09d00e3eee04c.png)'
- en: 'Figure 2: Illustration of simple graph each line connected with two vertices.
    In a hypergraph, each edge is connected with more than two vertices. In a multi-granularity
    graph, each node models specific spatial granularity, and each hypergraph is connected
    with multiple nodes.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：简单图的说明，每条线连接两个顶点。在超图中，每个边连接多个顶点。在多粒度图中，每个节点建模特定的空间粒度，每个超图连接多个节点。
- en: Different from previous methods, they utilize erasing techniques to penalize
    regularized terms during network training to prevent over-fitting. Hou et al.
    ([2020](#bib.bib23)) capture complementary affinities from video frames using
    an erasing strategy during training and testing. Based on the activated parts
    of previous frames, this approach erases the regions of each frame which ensures
    the frame concentrate on a new human part. To extract fine-grained cues, Multi-Granularity
    Reference aided Attentive Feature Aggregation (MG-RAFA) is proposed in Zhang et al.
    ([2020b](#bib.bib94)) to jointly handle Spatio-temporal features. Semantic hierarchy
    is considered for each node/position from a global point of view. For the position
    of each feature, local affinities are utilized with reference to feature nodes
    which provide the global structural and appearance information to support different
    weights to local features. Li et al. ([2021](#bib.bib39)) considers a holistic
    feature for visual similarity of video frames while focusing on the quality that
    allows the recovery of misaligned parts.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于之前的方法，他们利用擦除技术来惩罚网络训练中的正则化项，以防止过拟合。Hou 等人 ([2020](#bib.bib23)) 在训练和测试过程中通过擦除策略从视频帧中捕捉补充性相似度。基于先前帧的激活部分，这种方法擦除每帧的区域，确保帧集中在一个新的人体部位上。为了提取细粒度线索，Zhang
    等人 ([2020b](#bib.bib94)) 提出了 Multi-Granularity Reference aided Attentive Feature
    Aggregation (MG-RAFA) 方法，以共同处理时空特征。每个节点/位置的语义层次从全局角度考虑。对于每个特征的位置，利用本地相似度以参考特征节点，这提供了全局结构和外观信息，以支持本地特征的不同权重。Li
    等人 ([2021](#bib.bib39)) 通过关注允许恢复未对齐部分的质量，考虑了视频帧的整体特征以进行视觉相似度分析。
- en: 2.3 Attention Methods
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 注意力方法
- en: These methods usually ignore dissimilar pixels in training and prediction, employing
    similar pixels to make computational-friendly networks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法通常忽略训练和预测中的不同像素，使用相似像素以使网络计算更友好。
- en: Song et al. ([2018a](#bib.bib61)) introduce a mask-guided network, where binary
    masks are used to coexist with corresponding person images to decrease background
    clutter. Similar to the prior work, Subramaniam et al. ([2019](#bib.bib64)), CO-Segmentation
    approaches have shown remarkable improvements in video re-ID over different baselines
    by integrating a Cosegmentation-based Attention (COSAM) Subramaniam et al. ([2021](#bib.bib65))
    block among different layers in CNN networks. These CO-segmentation methods are
    able to extract unique features between person images and use them for channel
    and spatial-wise attention. In a different work in video re-ID, Chen et al. ([2019a](#bib.bib8))
    learn spatial-temporal features and calculate an attention score map to specify
    the quality of different components of a person.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Song 等人 ([2018a](#bib.bib61)) 引入了一种掩码引导网络，其中二进制掩码与对应的人员图像共存，以减少背景杂乱。类似于前期工作，Subramaniam
    等人 ([2019](#bib.bib64))，CO-Segmentation 方法通过在 CNN 网络的不同层中集成基于 Cosegmentation 的
    Attention (COSAM) Subramaniam 等人 ([2021](#bib.bib65)) 块，在不同基准上显著改进了视频重新识别。这些 CO-segmentation
    方法能够提取人物图像之间的独特特征，并用于通道和空间方向的注意力。在视频重新识别的不同工作中，Chen 等人 ([2019a](#bib.bib8)) 学习了时空特征并计算了注意力分数图，以指定人物不同组件的质量。
- en: In real-world applications, the motion patterns of humans are the dominant part
    of re-ID. The Flow Guided-Attention network Kiran et al. ([2021](#bib.bib32))
    is designed to fuse images and sequence of optical flow using CNN feature extractor
    which allows encoding of temporal data among spatial appearance information. The
    Flow Guided-Attention depends on joint SA between optical flow and features to
    take out unique features among them. Additionally, an approach to aggregate features
    is proposed for longer input streams for improved representation of video sequences.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，人类的运动模式是重新识别的主要部分。Flow Guided-Attention 网络 Kiran 等人 ([2021](#bib.bib32))
    旨在通过 CNN 特征提取器融合图像和光流序列，从而编码空间外观信息中的时间数据。Flow Guided-Attention 依赖于光流和特征之间的联合 SA，以提取它们之间的独特特征。此外，为了改善视频序列的表示，提出了一种聚合特征的方法，适用于更长的输入流。
- en: Several studies focus on multi-grained and multi-attention approaches to concentrate
    on important parts of the human body. Hu et al. ([2020](#bib.bib25)) introduce
    Concentrated Multi-grained Multi-Attention Network (CMMANet), multi-attention
    blocks are proposed to obtain multi-grained details by processing intermediate
    multi-scale features. Moreover, multiple-attention sub-modules in multi-attention
    blocks can automatically discover multiple discriminative regions in the frame
    sequence. Relevant to multi-branch networks, Hou et al. ([2021](#bib.bib22)) propose
    an innovative and computational-friendly video re-ID network that differs from
    the existing frameworks. Bilateral Complementary Network (BiCnet) preserves spatial
    features from the original image and down-sampling approach to broaden receptive
    fields and Temporal Kernel Selection (TKS) module captures the temporal relationship
    of videos. Different from previous studies, Chen et al. ([2020a](#bib.bib9)) introduces
    an end-to-end 3D framework to capture salient features of pedestrians in spatial-temporal
    domains. In this framework, salient 3D bins are selected with the help of two-stream
    networks and an RNN model to extract motion and appearance information.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究集中于多粒度和多注意力方法，以关注人体的重要部分。Hu等人 ([2020](#bib.bib25)) 介绍了集中多粒度多注意力网络（CMMANet），提出了多注意力块，通过处理中间的多尺度特征来获取多粒度细节。此外，多注意力块中的多个注意力子模块可以自动发现帧序列中的多个区分性区域。与多分支网络相关，Hou等人
    ([2021](#bib.bib22)) 提出了一个创新且计算友好的视频重新识别网络，该网络与现有框架有所不同。双边互补网络（BiCnet）保留了原始图像的空间特征和下采样方法，以扩展接收场，时间卷积选择（TKS）模块捕捉视频的时间关系。与以往研究不同，Chen等人
    ([2020a](#bib.bib9)) 介绍了一个端到端的3D框架，以捕捉空间-时间领域中行人的显著特征。在这个框架中，通过双流网络和RNN模型的帮助，选择了显著的3D桶，以提取运动和外观信息。
- en: 2.4 Graph Methods
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 图方法
- en: After the remarkable success of the CNN model Krizhevsky et al. ([2012](#bib.bib33))
    in image understanding and reconstruction, academic and industrial researchers
    have focused on developing convolutional approaches for graph data. Recently,
    researchers combine re-ID methods with graph models and explore Video re-ID Yan
    et al. ([2016](#bib.bib81)). Cheng et al. ([2018](#bib.bib13)) develop a training
    network that jointly handles conventional triplet and contrastive losses through
    a joint laplacian form that can take complete benefit of ranking data and relationships
    between training samples. In Shen et al. ([2018](#bib.bib60)), a novel unsupervised
    algorithm is formulated, which maps the ranking mechanism in the person re-ID
    method. Then, the formulation procedure is extended to be able to utilize ranking
    results from multiple algorithms. Only matching scores produced by different algorithms
    can lead to consensus results. The key role of the person re-ID task is to efficiently
    calculate the visual resemblance among person images. Still, ongoing person re-ID
    methods usually calculate the similarity of different image pairs (investigated)
    and candidate lists separately whereas neglecting the association knowledge among
    various query-candidate pairs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在Krizhevsky等人 ([2012](#bib.bib33)) 的CNN模型在图像理解和重建方面取得显著成功之后，学术界和工业界的研究人员开始集中于为图数据开发卷积方法。最近，研究人员将重新识别方法与图模型结合，并探索视频重新识别
    Yan等人 ([2016](#bib.bib81))。 Cheng等人 ([2018](#bib.bib13)) 开发了一个训练网络，通过联合拉普拉斯形式共同处理传统三元组和对比损失，从而充分利用排名数据和训练样本之间的关系。在Shen等人
    ([2018](#bib.bib60)) 中，提出了一种新颖的无监督算法，该算法映射了人物重新识别方法中的排名机制。然后，将这一公式过程扩展到能够利用来自多个算法的排名结果。只有不同算法生成的匹配分数才能得出一致的结果。人物重新识别任务的关键作用是有效地计算人物图像之间的视觉相似性。然而，当前的人物重新识别方法通常分别计算不同图像对（已调查）和候选列表的相似性，同时忽视了各种查询-候选对之间的关联知识。
- en: To solve the above problems, Similarity-Guided Graph Neural Network (SGGNN)
    Chen et al. ([2018b](#bib.bib7)) propose to generate a graph to illustrate the
    pairwise associations between query and candidate pairs (nodes) and utilize these
    associations to provide up-to-date query candidate correlation features extracted
    from the image in an end-to-end manner. Most re-ID approaches emphasize local
    features for similarity matching. Chen et al. ([2018b](#bib.bib7)) combine multiple
    person images to estimate the association between local relation and global relation
    in their Conditional Random Field (CRF). The benefit of this model is to learn
    local resemblance metrics from image pairs whereas considering the dependencies
    of all images in a collection, shaping group similarities. Yan et al. ([2019](#bib.bib83))
    put more effort into person re-ID and employ context details. They first develop
    a contextual module called the instance expansion part, which emphasizes on relative
    attention part to find and purify beneficial context detail in the scene. One
    of the innovative works Wu et al. ([2020](#bib.bib77)) for video re-ID is graph-based
    adaptive representation. Existing studies ignore part-based features, which contain
    temporal and spatial information. This approach allows the association between
    contextual information and relevant regional features such as feature affinity
    and poses alignment connection, to propose an adaptive structure-aware contagiousness
    graph. Liu et al. ([2021b](#bib.bib44)) present Correlation and Topology Learning
    (CTL) method which generates robust and distinguish features. It captures features
    at multi-granularity levels and overcomes posing appearance problems.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，陈等人（[2018b](#bib.bib7)）提出了相似性引导图神经网络（SGGNN），该方法生成一个图来说明查询和候选对（节点）之间的配对关联，并利用这些关联以端到端的方式提供从图像中提取的最新查询候选相关特征。大多数重新识别方法强调局部特征以进行相似性匹配。陈等人（[2018b](#bib.bib7)）在他们的条件随机场（CRF）中结合了多个人物图像，以估计局部关系和全局关系之间的关联。该模型的好处在于从图像对中学习局部相似度度量，同时考虑到集合中所有图像的依赖关系，塑造组间相似性。颜等人（[2019](#bib.bib83)）在人物重新识别方面投入了更多的努力，并利用了上下文细节。他们首先开发了一个称为实例扩展部分的上下文模块，重点关注相对注意部分，以查找和净化场景中的有益上下文细节。吴等人（[2020](#bib.bib77)）在视频重新识别方面的创新工作之一是基于图的自适应表示。现有研究忽略了部分特征，这些特征包含时间和空间信息。该方法允许上下文信息与相关区域特征（如特征亲和性和姿态对齐连接）之间的关联，提出了一种自适应结构感知的传染图。刘等人（[2021b](#bib.bib44)）提出了相关性与拓扑学习（CTL）方法，该方法生成了鲁棒和区分性的特征。它在多粒度层次上捕捉特征，并克服了姿态外观问题。
- en: 'Lately, hyper GNNs have attracted a lot of attention and achieved dominant
    results in various computer vision research fields such as person re-ID Shen et al.
    ([2018](#bib.bib60)), action recognition Wang and Gupta ([2018](#bib.bib70)) and
    image recognition Chen et al. ([2019b](#bib.bib12)). These hypergraph algorithms
    develop pairwise relationships on the basis of object interest. In general, a
    hypergraph is a graph in which edges independently work and can join any considerable
    number of vertices. The illustration of hypergraph as shown in Fig. [2](#S2.F2
    "Figure 2 ‣ 2.2 Local Part Alignments Methods ‣ 2 Video re-ID Methods ‣ Deep Learning
    for Video-based Person Re-Identification: A Survey") (b) Conversely, as represented
    in Fig. [2](#S2.F2 "Figure 2 ‣ 2.2 Local Part Alignments Methods ‣ 2 Video re-ID
    Methods ‣ Deep Learning for Video-based Person Re-Identification: A Survey") (a)
    where an edge exactly links with two vertices in a simple graph. In MG hypergraph,
    as represented in Fig. [2](#S2.F2 "Figure 2 ‣ 2.2 Local Part Alignments Methods
    ‣ 2 Video re-ID Methods ‣ Deep Learning for Video-based Person Re-Identification:
    A Survey") (d) hypergraphs with distinct spatio granularities are built utilizing
    numerous stages of features like body part throughout the video frames. In every
    hypergraph stage, novel temporal granularities are taken by hyperedges which connect
    a type of nodes in a graph such as body part features around separate temporal
    scales. The first Multi-Granular Hypergraph (MGH) Yan et al. ([2020](#bib.bib82))
    hypergraph and innovative mutual information loss function are proposed to overcome
    the image retrieval problem. The MGH approach clearly supports multi-granular
    ST information from the frame sequence. Then, they propose an attention procedure
    to combine features presenting at the node level to obtain better discriminative
    graph representations. Remarkably, the proposed approach achieves 90% rank-1 accuracy
    which is amongst the highest accuracy on the MARS dataset. Label estimation in
    graph matching is closely related to person re-ID problems in unsupervised learning.
    Ye et al. ([2019](#bib.bib87)) present an unsupervised Dynamic Graph Matching
    (DGM) video re-ID approach to predict labels. This technique iterates the update
    process by utilizing a discriminative metric and correspondingly updated labels.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，超图神经网络（hyper GNNs）受到了广泛关注，并在各种计算机视觉研究领域中取得了显著成果，如行人重识别 Shen 等人 ([2018](#bib.bib60))、动作识别
    Wang 和 Gupta ([2018](#bib.bib70)) 和图像识别 Chen 等人 ([2019b](#bib.bib12))。这些超图算法在对象兴趣的基础上发展了成对关系。一般来说，超图是一个图，其中的边独立工作，并可以连接任意数量的顶点。如图
    [2](#S2.F2 "Figure 2 ‣ 2.2 Local Part Alignments Methods ‣ 2 Video re-ID Methods
    ‣ Deep Learning for Video-based Person Re-Identification: A Survey") (b) 所示，超图的示意图。而在简单图中，如图
    [2](#S2.F2 "Figure 2 ‣ 2.2 Local Part Alignments Methods ‣ 2 Video re-ID Methods
    ‣ Deep Learning for Video-based Person Re-Identification: A Survey") (a) 所示，一条边正好连接两个顶点。在多粒度超图（MG
    hypergraph）中，如图 [2](#S2.F2 "Figure 2 ‣ 2.2 Local Part Alignments Methods ‣ 2 Video
    re-ID Methods ‣ Deep Learning for Video-based Person Re-Identification: A Survey")
    (d) 所示，通过利用视频帧中的多个特征阶段（如身体部位）构建具有不同空间粒度的超图。在每个超图阶段，新颖的时间粒度由超边确定，这些超边连接图中的一种节点类型，例如在不同时间尺度上的身体部位特征。首个多粒度超图（MGH）
    Yan 等人 ([2020](#bib.bib82)) 和创新的互信息损失函数被提出以解决图像检索问题。MGH 方法明确支持来自帧序列的多粒度时空信息。随后，他们提出了一种注意力机制，将节点级别的特征进行组合，以获得更好的判别图表示。值得注意的是，该方法在
    MARS 数据集上达到了 90% 的 rank-1 准确率，这在该数据集中属于最高准确率之一。图匹配中的标签估计与无监督学习中的行人重识别问题密切相关。Ye
    等人 ([2019](#bib.bib87)) 提出了一个无监督的动态图匹配（DGM）视频重识别方法来预测标签。该技术通过利用判别度量和相应更新的标签来迭代更新过程。'
- en: 2.5 Transformer Methods
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 Transformer 方法
- en: Recently, transformer shows a great interest in the computer vision field, and
    self-attention-based methods are proposed to solve visual problems. Inspired by
    recent development, Zhang et al. ([2021b](#bib.bib92)) put forward the first step,
    propose first SpatioTemporal transformer (STT) and synthesize pre-training data
    strategy to reduce over-fitting for video re-ID task. In their network, the global
    module enables supplement to utilize the relation among patches from frames. To
    extract comprehensive features from videos, Liu et al. ([2021c](#bib.bib47)) further
    explore transformers and introduce Trigeminal Transformers (TMT) with robust novel
    feature extractor that jointly transform raw videos into S, T, and ST domains.
    To capture fine-grained features and aggregate in multi-view features, a self-view
    transformer is proposed to enhance single-view features and a cross-view transformer
    is used to combine multiple features. A Duplex SpatioTemporal Filtering Network
    (DSFN) Zheng et al. ([2021](#bib.bib96)) architecture is designed to extract static
    and dynamic data from frame sequences for video re-ID. To enhance the capability
    of kernels, sparse-orthogonal constraints are developed to broaden the difference
    in temporal features. To collaborate with a group of kernels, they add additional
    channels to assist and extract ST clues from distinct features. A Hybrid Dense
    Interaction Learning (DenseIL) framework is presented in He et al. ([2021](#bib.bib21))
    which utilizes both CNN and Attention mechanism for video re-ID. DenseIL consists
    of a CNN-based encoder which is responsible to extract efficient discriminative
    spatial features and a DI-based decoder densely modeling the ST inherent interaction
    among frames.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，transformer 在计算机视觉领域引起了极大关注，基于自注意力的方法被提出以解决视觉问题。受近期发展的启发，张等人 ([2021b](#bib.bib92))
    提出了第一步，提出了第一个时空 transformer (STT) 并合成预训练数据策略，以减少视频 re-ID 任务的过拟合。在他们的网络中，全球模块能够补充利用帧中块之间的关系。为了从视频中提取综合特征，刘等人
    ([2021c](#bib.bib47)) 进一步探索了 transformers，并引入了三叉神经网络 (TMT)，具有稳健的新型特征提取器，将原始视频共同转换为
    S、T 和 ST 域。为了捕获细粒度特征并在多视角特征中聚合，提出了一种自视角 transformer，以增强单视角特征，并使用交叉视角 transformer
    来结合多个特征。郑等人 ([2021](#bib.bib96)) 设计了双重时空滤波网络 (DSFN) 架构，用于从帧序列中提取静态和动态数据以进行视频 re-ID。为了增强内核的能力，开发了稀疏正交约束以扩展时间特征中的差异。为了与一组内核协作，他们添加了额外的通道以协助并从不同特征中提取
    ST 线索。何等人 ([2021](#bib.bib21)) 提出了一个混合密集交互学习 (DenseIL) 框架，利用 CNN 和注意力机制进行视频 re-ID。DenseIL
    包括一个基于 CNN 的编码器，负责提取有效的判别空间特征，以及一个基于 DI 的解码器，密集建模帧之间的 ST 固有交互。
- en: 3 Novel Architectures
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 种新型架构
- en: Different from existing architectures, Jiang et al. ([2021](#bib.bib30)) propose
    a novel design to handle misalignment problems in video re-ID. Self-Separated
    network (SSN) provides an effective approach to deal with temporal and spatial
    variation of a person’s body parts. SSN derives a two-round classification approach,
    leading to better training in pixel-wise and aggregated features. The improved
    Coarse-to-Fine Axial Attention Network (CF-AAN) Liu et al. ([2021a](#bib.bib42))
    is designed with the help of Link and re-Detect block which can align noisy tracklist
    on the image level. This module not only decreases computational costs but also
    achieves promising results. Various video re-ID methods are still suffering from
    pose changes and personal misalignment problems. To handle misalignment, Zhang
    et al. ([2021a](#bib.bib90)) propose the Reference-Aided Part-Aligned (RAPA) that
    focuses on different parts of the body and disentangles the discriminative features.
    Reference Feature Learning (RFL) pose-based module is provided to capture uniform
    standards for alignment. Aligning the body parts in intra-video, relations, and
    attention-based Part Feature Disentangling (PFD) blocks are designed to locate
    and match body parts through frames.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有架构不同，Jiang 等人 ([2021](#bib.bib30)) 提出了一个新颖的设计来处理视频重新识别中的对齐问题。自分离网络 (SSN)
    提供了一种有效的方法来处理人物身体部位的时间和空间变化。SSN 引出了一个两轮分类方法，从而在像素级和聚合特征上获得更好的训练。改进的粗到细轴向注意力网络
    (CF-AAN) Liu 等人 ([2021a](#bib.bib42)) 在 Link 和 re-Detect 模块的帮助下设计，可以在图像级对齐噪声跟踪列表。该模块不仅降低了计算成本，还取得了令人满意的结果。各种视频重新识别方法仍然受到姿态变化和个人对齐问题的困扰。为了解决对齐问题，Zhang
    等人 ([2021a](#bib.bib90)) 提出了参考辅助部位对齐 (RAPA) 方法，该方法关注身体的不同部位并解开判别特征。提供了基于参考特征学习
    (RFL) 的姿态模块来捕获对齐的统一标准。设计了对齐身体部位的内部视频、关系和基于注意力的部位特征解耦 (PFD) 模块，通过帧来定位和匹配身体部位。
- en: 'Table 2: Training configuration of novel architectures. LR denotes learning
    rate and L represents loss'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：新型架构的训练配置。LR 表示学习率，L 代表损失
- en: '|  Reference and Venue | Method | Extractor | L. Function | LR | Optimizer
    | Epochs |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  参考和场地 | 方法 | 提取器 | 损失函数 | 学习率 | 优化器 | 训练轮次 |'
- en: '| Wang et al. ([2014](#bib.bib69)) [ECCV] | DVR | HOG3D | Hinge | — | — | —
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([2014](#bib.bib69)) [ECCV] | DVR | HOG3D | Hinge | — | — | — |'
- en: '| Karanam et al. ([2015](#bib.bib31)) [CVPR] | SRID | Schmid, Gabor filters
    | — | — | — | — |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Karanam 等人 ([2015](#bib.bib31)) [CVPR] | SRID | Schmid, Gabor 滤波器 | — | —
    | — | — |'
- en: '| Liu et al. ([2015](#bib.bib46)) [ICCV] | STFV3D | Fisher Vector | — | — |
    — | — |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 ([2015](#bib.bib46)) [ICCV] | STFV3D | Fisher Vector | — | — | — |
    — |'
- en: '| Wu et al. ([2016](#bib.bib76)) [ARXIV] | Deep RCN | — | — | — | — | — |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等人 ([2016](#bib.bib76)) [ARXIV] | Deep RCN | — | — | — | — | — |'
- en: '| You et al. ([2016](#bib.bib89)) [CVPR] | TDL |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| You 等人 ([2016](#bib.bib89)) [CVPR] | TDL |'
- en: '&#124; HOG3D, Color &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HOG3D, Color &#124;'
- en: '&#124; Histograms, LBP &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Histograms, LBP &#124;'
- en: '| Hinge | — | — | — |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Hinge | — | — | — |'
- en: '| Chen et al. ([2016](#bib.bib11)) [IEEE-SRL] | OFEI | LBP | — | — | — | —
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 ([2016](#bib.bib11)) [IEEE-SRL] | OFEI | LBP | — | — | — | — |'
- en: '| Chen et al. ([2016](#bib.bib11)) [ECCV] | RFA-Net | LBP, HSV, Lab | Softmax
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 ([2016](#bib.bib11)) [ECCV] | RFA-Net | LBP, HSV, Lab | Softmax |'
- en: '&#124; 0.001 to &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.001 到 &#124;'
- en: '&#124; 0.0001 &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.0001 &#124;'
- en: '| — | 400 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| — | 400 |'
- en: '| Niall et al. ([2016](#bib.bib56)) [CVPR] | CNN and RNN | Cross Entropy |
    — | 0.001 | SGD | 500 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Niall 等人 ([2016](#bib.bib56)) [CVPR] | CNN 和 RNN | 交叉熵 | — | 0.001 | SGD
    | 500 |'
- en: '| Zhou et al. ([2017](#bib.bib99)) [CVPR] | JS-TRNN | TAM and SRM | Triplet
    | — | — | — |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Zhou 等人 ([2017](#bib.bib99)) [CVPR] | JS-TRNN | TAM 和 SRM | Triplet | — |
    — | — |'
- en: '| Liu et al. ([2017](#bib.bib49)) [CVPR] | QAN | — | Softmax and Triplet |
    — | — | — |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 ([2017](#bib.bib49)) [CVPR] | QAN | — | Softmax 和 Triplet | — | —
    | — |'
- en: '| Xu et al. ([2017](#bib.bib80)) [ICCV] | ASTPN | — | CE and Hinge | 0.001
    | SGD | 700 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Xu 等人 ([2017](#bib.bib80)) [ICCV] | ASTPN | — | CE 和 Hinge | 0.001 | SGD
    | 700 |'
- en: '| Chung et al. ([2017](#bib.bib14)) [ICCV] | 2SCNN | CNN and RNN | Softmax
    | 0.001 | SGD | 1000 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Chung 等人 ([2017](#bib.bib14)) [ICCV] | 2SCNN | CNN 和 RNN | Softmax | 0.001
    | SGD | 1000 |'
- en: '| Gao et al. ([2021](#bib.bib19)) [ACM_MM] | CMA | CNN+RNN | Softmax | 0.001
    | SGD | 800 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Gao 等人 ([2021](#bib.bib19)) [ACM_MM] | CMA | CNN+RNN | Softmax | 0.001 |
    SGD | 800 |'
- en: '|   |  |  |  |  |  |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |'
- en: Most video re-ID methods focus on the important region of the image, therefore,
    these methods can easily lose out on fine-grained hints in image sequences. Different
    from previous studies, the novel GRL Liu et al. ([2021d](#bib.bib48)) framework
    is introduced along with reciprocal learning and correlation estimation. The GCE
    module creates the feature maps of local and global features that helps to locate
    the low regions and high regions to identify a similar person. Then, a novel TRL
    approach is introduced to improve the high-correlation semantic information. Gu
    et al. ([2020](#bib.bib20)) propose Appearance Preserving 3D Convolution (AP3D)
    and Appearance-Preserving Module (APM), which align neighborhood feature maps
    in pixel-level. 3D ConvNets model temporal information on the basis of preserving
    the quality of visual appearance. It may be easier to aggregate AP3D with current
    3DConNet by substituting prior 3D-Conv filters to AP3Ds. In video re-ID, personal
    attributes and visual appearance are key to matching identities, and both features
    significantly contribute to the tracking of pedestrians. Novel TALNet Liu et al.
    ([2020](#bib.bib45)) is proposed to focus on attribute-temporal learning by constructing
    a branch network with the help of SA and temporal-semantic context.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数视频重识别方法专注于图像的重要区域，因此，这些方法很容易忽略图像序列中的细粒度线索。不同于以往的研究，新颖的GRL Liu等人（[2021d](#bib.bib48)）框架引入了互惠学习和相关性估计。GCE模块创建了局部和全局特征的特征图，帮助定位低区域和高区域，以识别相似的人。随后，提出了一种新颖的TRL方法来提高高相关性的语义信息。Gu等人（[2020](#bib.bib20)）提出了外观保留的3D卷积（AP3D）和外观保留模块（APM），在像素级对齐邻域特征图。3D
    ConvNets在保持视觉外观质量的基础上建模时间信息。通过用AP3Ds替代先前的3D-Conv滤波器，将AP3D与当前的3DConNet聚合可能更容易。在视频重识别中，个人属性和视觉外观是匹配身份的关键，这两种特征显著有助于行人的跟踪。新颖的TALNet
    Liu等人（[2020](#bib.bib45)）被提出，重点关注属性-时间学习，通过构建一个分支网络，并借助SA和时间-语义上下文。
- en: 4 Loss Functions
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 损失函数
- en: Loss function plays a major and crucial role in discriminating the learned features.
    In general, the softmax loss separates the learned features rather than discriminates.
    The main goal of designing a person re-ID loss function is to enhance representation
    with an efficiency loss. We highlight several of the most influential loss functions
    for video re-ID.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数在区分学习到的特征中起着重要而关键的作用。一般来说，softmax损失分隔学习到的特征，而不是进行区分。设计人脸重识别损失函数的主要目标是通过效率损失来增强表示能力。我们强调了几种对视频重识别最具影响力的损失函数。
- en: 4.1 Attention and CL Loss
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 注意力和CL损失
- en: Pathak et al. ([2020](#bib.bib57)) introduce CL centers online soft mining loss
    which utilizes center vectors from center loss as class label vector representations
    to crop out those frames that contain higher noise because it contains high variance
    compared to the original classifier weights. Additionally, they penalize the model
    by giving maximum attention scores to those frames that have randomly deleted
    patches. Those random erased frames are labeled as $1$ otherwise $0$ and N is
    the number of total frames.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Pathak等人（[2020](#bib.bib57)）引入了CL中心在线软挖掘损失，它利用来自中心损失的中心向量作为类别标签向量表示，以剪切出那些包含更高噪声的帧，因为这些帧与原始分类器权重相比具有更高的方差。此外，他们通过对那些随机删除补丁的帧给予最高的注意力分数来惩罚模型。这些随机擦除的帧标记为$1$，否则为$0$，N为总帧数。
- en: '|  | $\displaystyle\mathrm{AL}=$ | $\displaystyle\frac{1}{N}\sum_{i=1}^{N}\operatorname{label}(i)*\text{
    Attention }_{score}(i)$ |  | (1) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{AL}=$ | $\displaystyle\frac{1}{N}\sum_{i=1}^{N}\operatorname{label}(i)*\text{
    Attention }_{score}(i)$ |  | (1) |'
- en: 4.2 Weighted Triple-Sequence Loss (WTSL)
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 加权三重序列损失（WTSL）
- en: Jiang et al. ([2020](#bib.bib29)) explicitly encode frame-based image level
    information into video level features that can decrease the effect of outlier
    frame. Intra-class distance in WTSL makes similar videos closer and inter-class
    distance pushes dissimilar videos further apart.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Jiang等人（[2020](#bib.bib29)）明确地将基于帧的图像级信息编码到视频级特征中，从而减少离群帧的影响。WTSL中的类内距离使得相似视频更接近，而类间距离则将不相似的视频推得更远。
- en: '|  | $\displaystyle\mathrm{L}_{WTSL}=$ | $\displaystyle\sum_{i=1}^{N}\left[\left\&#124;\mathrm{~{}F}_{a}^{i}-\mathrm{F}_{p}^{i}\right\&#124;_{2}^{2}\right.\left.-\left\&#124;\mathrm{F}_{a}^{i}-\mathrm{F}_{n}^{i}\right\&#124;_{2}^{2}+\alpha\right]_{2}$
    |  | (2) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{L}_{WTSL}=$ | $\displaystyle\sum_{i=1}^{N}\left[\left\&#124;\mathrm{~{}F}_{a}^{i}-\mathrm{F}_{p}^{i}\right\&#124;_{2}^{2}\right.\left.-\left\&#124;\mathrm{F}_{a}^{i}-\mathrm{F}_{n}^{i}\right\&#124;_{2}^{2}+\alpha\right]_{2}$
    |  | (2) |'
- en: where $\alpha$ represents margin, N is the number of triple-sequences and P
    represents person ID. The F[a] is a closer feature to its own class centroid and
    far away from other class centroids.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Symbolic Triplet Loss (STL)
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Aruna Kumar et al. ([2020](#bib.bib3)) propose STL which utilizes the Wasserstein
    metric to overcome the representation problem which allows obtaining the distance
    between feature vectors that are symbolic.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D_{w}\left(\psi_{i},\psi_{j}\right)=\sum_{m=1}^{M}\sum_{t=1}^{T}\psi_{im}{}^{-1}(t)-\psi_{jm}{}^{-1}(t)$
    |  | (3) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: where $\psi_{i}$ and $\psi_{j}$ denote the distributions of multi-dimensional
    feature vectors at the i^(th) and j^(th). ${\psi_{i}}^{-1}(t)$ is the quantile
    function and M is the feature of each video.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Weighted Contrastive Loss (WCL)
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Wang et al. ([2019a](#bib.bib71)) construct WCL by the combination of traditional
    contrastive loss. The purpose of this loss function is to allocate an appropriate
    weight for every proper image pair.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{WCL}(N)=\frac{1}{2}\frac{\sum_{\left(x_{i},x_{j}\right)\in N}w_{ij}^{-}\max\left(0,\alpha-d_{ij}\right)^{2}}{\sum_{\left(x_{i},x_{j}\right)\in
    N}w_{ij}^{-}}$ |  | (4) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle L_{WCL}(P,N)=(1-\lambda)L_{WCL}(P)+\lambda L_{WCL}(N)$
    |  | (5) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: where hyperparameter $\lambda$ handles the contribution of both positive and
    negative sets towards final value of contrastive loss.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Performance analysis of top-performing approaches on Duke, iLIDS,
    and Mars datasets. “NL” represents a non-local block.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '|   |  | MARS | DukeV | iLIDS |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| Method | Backbone | mAP | R-1 | mAP | R-1 | R-1 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| STAN[(CVPR’18)] | Res-50 | 65.8 | 82.3 | $\times$ | $\times$ | 80.2 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| Snippet[(CVPR’18)] | Res-50 | 76.1 | 86.3 | $\times$ | $\times$ | 85.4 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| STA[(AAAI’19)] | Res-50 | 80.8 | 86.3 | 94.9 | 96.2 | $\times$ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| ADFD[(CVPR’19)] | Res-50 | 78.2 | 87.0 | $\times$ | $\times$ | 86.3 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| VRSTC[(CVPR’19)] | Res-50 | 82.3 | 88.5 | 93.5 | 95.0 | 83.4 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| GLTR[(ICCV’19)] | Res-50 | 78.5 | 87.0 | 93.7 | 96.3 | 86.0 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| COSAM[(ICCV’19)] | SERes-50 | 79.9 | 84.9 | 94.1 | 95.4 | 79.6 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| STE-NVAN[(BMVC’19)] | Res-50-NL | 81.2 | 88.9 | 93.5 | 95.2 | $\times$ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| MG-RAFA[(CVPR’20)] | Res-50 | 85.9 | 88.8 | $\times$ | $\times$ | 88.6 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| MGH[(CVPR’20)] | Res-50-NL | 85.8 | 90.0 | $\times$ | $\times$ | 85.6 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| STGCN[(CVPR’20)] | Res-50 | 83.7 | 90.0 | 95.7 | 97.3 | $\times$ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| TCLNet[(ECCV’20)] | Res-50-TCL | 85.1 | 89.8 | 96.2 | 96.9 | 86.6 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| AP3D[(ECCV’20)] | AP3D | 85.1 | 90.1 | 95.6 | 96.3 | 86.7 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| AFA[(ECCV’20)] | Res-50 | 82.9 | 90.2 | 95.4 | 97.2 | 88.5 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| HMN[(TCSVT’21)] | Res-50 | 88.8 | 89 | 95.1 | 96.2 | $\times$ |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| SANet[(TCSVT’21)] | Res-50 | 86.0 | 91.2 | 96.7 | 97.7 | $\times$ |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| DPRAM[(TIP’21)] | Res-50 | 83.0 | 89.0 | 95.6 | 97.1 | $\times$ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| PSTA[(ICCV’21)] | Res-50 | 85.8 | 91.5 | 97.4 | 98.3 | 91.5 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| STRF[(ICCV’21)] | Res-50 | 86.1 | 90.3 | 96.4 | 97.4 | 89.3 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| DenseIL[(ICCV’21)] | Res-50 | 87.0 | 90.8 | 97.1 | 97.6 | 92.0 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| STMN[(ICCV’21)] | Res-50 | 83.7 | 89.9 | 94.6 | 96.7 | 80.6 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| GRL[(ICCV’21)] | Res-50 | 84.8 | 91.0 | $\times$ | $\times$ | 90.4 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| TMT[(arXiv’21)] | Res-50 | 85.8 | 91.2 | $\times$ | $\times$ | 91.3 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: 4.5 Triplet Loss
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chen et al. ([2019a](#bib.bib8)) design triplet loss to conserve ranking relationship
    among videos of pedestrian triplets. In triplet loss, the distance between feature
    pairs belonging to similar classes decreases, while the distance between feature
    pairs of different classes increases.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle L_{tri}=\sum_{i,j,k\in\Omega}\left[d_{g}(i,j)-d_{g}(i,k)+m_{g}\right]_{+}$
    |  | (6) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{i,j,k\in\Omega}\lambda\left[d_{l}(i,j)-d_{l}(i,k)+m_{l}\right]_{+}$
    |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: where $m_{g}$ and $m_{l}$ represent thresholds margin to restrict the distance
    gap between positive and negative samples and $[x]^{+}$ is the max function max$(0,x)$.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Regressive Pairwise Loss (RPL)
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Liu et al. ([2018](#bib.bib51)) develop RPL to improve pairwise similarity by
    combining all positive sets in one single subspace. It helps with the soft margin
    between positive sets and is harder than the general triplet loss.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}L_{p}\left(x_{i},x_{j},y\right)=y\cdot\max\left\{d\left(x_{i},x_{j}\right)-\log(\alpha),0\right\}\\
    \quad+(1-y)\cdot\max\left\{\alpha-d\left(x_{i},x_{j}\right),0\right\}\end{gathered}$
    |  | (7) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: where $y$ denotes label whether $x_{i}$ and $x_{j}$ are similar people. If a
    person is from the same identity it is represented as $1$ otherwise $0$. When
    y = 0, RPL pushes samples far away from each other beyond margin $\alpha$. When
    y = 1, RPL pulls the samples together within distances no more than log($\alpha$).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 5 Datasets and Metrics
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first describe the statistics of benchmark datasets that are frequently used
    for evaluating video re-ID methods. Secondly, we broadly review the performance
    of previous superior methods in chronological order. Lastly, we analyze results
    based on several major factors for video re-ID.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Training and Testing Datasets
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since video re-ID is a real-world problem and closer to a video surveillance
    scenario. During past years, various demanding datasets have been constructed
    for video re-ID: MARS Zheng et al. ([2016a](#bib.bib97)), DukeMTMC-VID Wu et al.
    ([2018](#bib.bib78)) and iLIDS-VIDWang et al. ([2014](#bib.bib69)), these three
    datasets are commonly used for training and evaluation, because of the large number
    of track-lets and pedestrian identities.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 MARS
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dataset is constructed based on six synchronized CCTV cameras. It comprises
    $1,261$ pedestrians with different varieties of images (poor image quality, poses,
    colors, and illuminations) captured by two cameras. It is extremely difficult
    to match pedestrian images because it contains $3,248$ distractors to make the
    dataset more real-world.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 DukeMTMC-VID
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is a subgroup of the DukeMTMC dataset which purely consists of 8 cameras
    with high-resolution images. It is one of the large-scale datasets where pedestrian
    images are cropped using manual hand-drawn bounding boxes. Overall, it comprises
    $702$ identities, $16,522$ training images $17,661$ gallery images, and $2,228$
    probe images.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 它是DukeMTMC数据集的一个子集，完全由8个高分辨率图像的摄像头组成。它是一个大型数据集，其中行人图像是使用人工绘制的边界框裁剪的。总体而言，它包括$702$个身份，$16,522$张训练图像，$17,661$张图库图像，以及$2,228$张探测图像。
    |
- en: 5.1.3 iLIDS-VID
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 iLIDS-VID |
- en: It is one of the challenging datasets which contains $300$ pedestrians captured
    by two CCTV cameras in public. Due to the public images, it contains lighting,
    viewpoint changes, different similarities, background clutter, and occlusions.
    It consists of a $600$ sequence of images of $300$ diverse individual images.
    Each sequence of the pedestrian images has a range length of $23$ to $192$ and
    the number of frames is $73$.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个具有挑战性的数据集，包含$300$名行人，这些行人由两个公共CCTV摄像头捕获。由于是公共图像，它包含了光照、视角变化、不同的相似性、背景杂乱和遮挡。它由$300$个不同个体图像的$600$个图像序列组成。每个行人图像序列的长度范围为$23$到$192$，帧数为$73$。
    |
- en: 5.2 Evaluation Protocol
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 评估协议 |
- en: The are two standard evaluation protocols for evaluating video re-ID methods
    which are mAP and CMC. CMC is the probability of top top-K correct matches in
    a retrieval list. Another evaluation metric is mAP, which measures the average
    retrieval accuracy with multiple GT.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种标准评估协议用于评估视频重新识别方法，即mAP和CMC。CMC是检索列表中前K个正确匹配的概率。另一个评估指标是mAP，它测量具有多个GT的平均检索准确率。
    |
- en: 'Table 4: A comparison of 20 approaches of Video based person re-identification
    published in top conferences, sorted by year'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：按年份排序的在顶级会议上发布的20种基于视频的人物重新识别方法的比较 |
- en: '| Approach | Conference | Year | Main contributions |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Approach | Conference | Year | 主要贡献 |'
- en: '| CN | CVPR | 2010 | Extracted color names from video sequences and then used
    the color names to perform person re-identification. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| CN | CVPR | 2010 | 从视频序列中提取颜色名称，然后使用这些颜色名称进行人物重新识别。 |'
- en: '| HoG | ICCV | 2011 | Extracted HoG features from video sequences and then
    used the HoG features to perform person re-identification. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| HoG | ICCV | 2011 | 从视频序列中提取HoG特征，然后使用这些HoG特征进行人物重新识别。 |'
- en: '| FV | CVPR | 2012 | Extracted visual features from video sequences and then
    aggregated the features into a Fisher vector representation, and then used the
    Fisher vector representation to perform person re-identification. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| FV | CVPR | 2012 | 从视频序列中提取视觉特征，然后将这些特征聚合到Fisher向量表示中，再使用Fisher向量表示进行人物重新识别。
    |'
- en: '| BoW | ICCV | 2013 | Extracted visual features from video sequences and then
    clustered the features into a bag-of-words representation, and then used the bag-of-words
    representation to perform person re-identification. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| BoW | ICCV | 2013 | 从视频序列中提取视觉特征，然后将这些特征聚类成词袋表示，再使用词袋表示进行人物重新识别。 |'
- en: '| LCRF | CVPR | 2014 | Proposed a locally enhanced correlation filter (LCRF)
    that learns to track discriminative regions in video sequences, and then uses
    the tracked regions to perform person re-identification. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| LCRF | CVPR | 2014 | 提出了一个局部增强相关滤波器（LCRF），它学习跟踪视频序列中的区分区域，然后使用跟踪到的区域进行人物重新识别。
    |'
- en: '| Siamese Network | CVPR | 2015 | Proposed a Siamese network that learns to
    match pairs of video sequences, and then uses the learned matching function to
    perform person re-identification. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Siamese Network | CVPR | 2015 | 提出了一个Siamese网络，学习匹配视频序列对，然后使用学习到的匹配函数进行人物重新识别。
    |'
- en: '| 3D CNN-based | ECCV | 2016 | Employed a 3D convolutional neural network to
    learn spatio-temporal features from video sequences, and then used the learned
    features to perform person re-identification. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 3D CNN-based | ECCV | 2016 | 使用3D卷积神经网络从视频序列中学习时空特征，然后使用学习到的特征进行人物重新识别。 |'
- en: '| LSTM-based | CVPR | 2016 | Employed a long short-term memory (LSTM) network
    to learn temporal dependencies between frames in a video sequence, and then used
    the learned dependencies to perform person re-identification. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| LSTM-based | CVPR | 2016 | 使用长短期记忆（LSTM）网络学习视频序列中帧之间的时间依赖关系，然后使用学习到的依赖关系进行人物重新识别。
    |'
- en: '| RNN-based | ICCV | 2016 | Employed a recurrent neural network to learn temporal
    dependencies between frames in a video sequence, and then used the learned dependencies
    to perform person re-identification. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| RNN-based | ICCV | 2016 | 使用递归神经网络学习视频序列中帧之间的时间依赖关系，然后利用学习到的依赖关系进行人物重识别。
    |'
- en: '| SVDNet | CVPR | 2017 | Proposed a spectral video descriptor network that
    learns to represent video sequences as a collection of spectral features, and
    then uses these features to perform person re-identification. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| SVDNet | CVPR | 2017 | 提出了一个光谱视频描述符网络，该网络学习将视频序列表示为光谱特征集合，然后使用这些特征进行人物重识别。
    |'
- en: '| ST-GCN | ECCV | 2018 | Constructed a spatial-temporal graph to capture the
    relationships between frames in a video sequence, and then used graph convolutional
    networks to learn discriminative features from the graph. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| ST-GCN | ECCV | 2018 | 构建了一个时空图来捕捉视频序列中帧之间的关系，然后使用图卷积网络从图中学习判别特征。 |'
- en: '| PTGAN | CVPR | 2018 | Proposed a person transferable GAN that learns to generate
    discriminative features for video-based person re-identification by transferring
    knowledge from a source person to a target person. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| PTGAN | CVPR | 2018 | 提出了一个可迁移的生成对抗网络（GAN），通过将源人物的知识迁移到目标人物，学习生成用于视频基础的人物重识别的判别性特征。
    |'
- en: '| GLTR | CVPR | 2018 | Proposed a novel geometric locally transient representation
    that captures both global and local temporal information for video-based person
    re-identification. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| GLTR | CVPR | 2018 | 提出了一个新颖的几何局部瞬态表示方法，用于捕捉视频中的全球和局部时间信息，以进行基于视频的人物重识别。
    |'
- en: '| Top-push | ICCV | 2017 | Introduced a top-push video-based person re-identification
    framework that learns to push the feature representations of the same person closer
    together and the feature representations of different people further apart. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Top-push | ICCV | 2017 | 引入了一个基于视频的人物重识别框架，该框架学习将相同人物的特征表示推得更近，将不同人物的特征表示推得更远。
    |'
- en: '| COSAM | CVPR | 2019 | Introduced a convolutional Siamese attention model
    that learns to attend to discriminative spatial and temporal regions in video
    sequences. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| COSAM | CVPR | 2019 | 引入了一个卷积Siamese注意力模型，该模型学习关注视频序列中的判别性时空区域。'
- en: '| STMN | CVPR | 2020 | Proposed a novel spatial and temporal memory network
    to learn discriminative and robust features for video-based person re-identification.
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| STMN | CVPR | 2020 | 提出了一个新颖的时空记忆网络，以学习用于视频基础的人物重识别的判别性和鲁棒性特征。 |'
- en: 6 Analysis and Future Direction
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 分析与未来方向
- en: 'We broadly review the top-performing methods from video re-ID perspectives.
    We mostly focus on the work published in $2018$ till now. Specifically, we include
    STAN Li et al. ([2018](#bib.bib40)), Snippet Chen et al. ([2018a](#bib.bib6)),
    STA Fu et al. ([2019](#bib.bib16)), ADFD Zhao et al. ([2019](#bib.bib95)), VRSTC
    Hou et al. ([2019](#bib.bib24)), GLTR Li et al. ([2019](#bib.bib38)), COSAM Subramaniam
    et al. ([2019](#bib.bib64)), STE-NVAN Liu et al. ([2019a](#bib.bib43)), MG-RAFA
    Zhang et al. ([2020b](#bib.bib94)), MGH Yan et al. ([2020](#bib.bib82)), STGCN
    Yang et al. ([2020](#bib.bib84)), TCLNet Hou et al. ([2020](#bib.bib23)), AP3D
    Gu et al. ([2020](#bib.bib20)), AFA Chen et al. ([2020b](#bib.bib10)), PSTAWang
    et al. ([2021a](#bib.bib72)) DenseIL He et al. ([2021](#bib.bib21)), STMN Eom
    et al. ([2021](#bib.bib15)), STRF Aich et al. ([2021](#bib.bib1)), SANet Bai et al.
    ([2021](#bib.bib4)), DPRAM Yang et al. ([2021](#bib.bib85)), HMN Wang et al. ([2021b](#bib.bib73)),
    GRL Liu et al. ([2021d](#bib.bib48)), and TMT Liu et al. ([2021c](#bib.bib47)).
    We summarize the video re-ID results on three widely used benchmark datasets.
    Table. [3](#S4.T3 "Table 3 ‣ 4.4 Weighted Contrastive Loss (WCL) ‣ 4 Loss Functions
    ‣ Deep Learning for Video-based Person Re-Identification: A Survey") highlights
    the backbone, mAP and R-1 results, and methods.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, with the recent development of self-attention-based methods, several
    video re-ID methods have obtained higher mAP and top-1 accuracy (Liu et al. ([2021c](#bib.bib47))
    91.2%) on the widely used MARS dataset. Especially, DenseIL He et al. ([2021](#bib.bib21))
    achieves the highest mAP of 87.0% but rank-1 accuracy is 90.8% which is slightly
    lower than TMTLiu et al. ([2021c](#bib.bib47)) on MARS dataset. The advantage
    of the DenseIL He et al. ([2021](#bib.bib21)) method is to simultaneously use
    CNN and attention-based architecture to efficiently encode spatial information
    into discriminative features. Those methods focus on long-range relationships
    and specific part-level information on an input signal. Various popular methods
    separately learn weights and spatial-temporal features Hou et al. ([2019](#bib.bib24),
    [2020](#bib.bib23)). Another observation in Zhang et al. ([2021b](#bib.bib92))
    illustrates that capturing and aggregating pedestrian cues is spatial-temporal
    while ignoring discrepancies including background areas, viewpoint, and occlusions.
    However, in a real-world scenario, the visual data contains a lot of diverse modalities
    such as recording information, camera ID, etc. Most studies focus on visual similarity
    by matching probe images into gallery images. Thus, it neglects textual information
    which is not a good idea. Proposing a new method that extracts visual-textual
    information at the same time would be helpful in a real-world environment and
    it will also help to provide more accurate results.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, annotating new datasets with accurate labels on different CCTV cameras
    is an expensive and laborious task. In most cases, annotated data are wrong-labeled
    due to various factors such as person visibility, background clutter, and noise
    issues in images. Several researchers focus on unsupervised methods Ye et al.
    ([2018](#bib.bib86), [2019](#bib.bib87)) and active learning approaches Wang et al.
    ([2018b](#bib.bib68)) to alleviate the annotation problem. Still, the accuracy
    of unsupervised video re-ID methods degrades significantly compare to supervised
    video re-ID methods. In the future, introducing a unique video re-ID method that
    facilitates clustering and label assignment will be considered to improve existing
    unsupervised methods. Further, designing a specific data augmentation policy in
    a re-ID search space can easily increase the overall performance for all re-ID
    methods.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the accuracy on three challenging datasets reaches a difficult state,
    where the performance gap is less than 1% accuracy like PSTA Wang et al. ([2021a](#bib.bib72))
    and DenseIL He et al. ([2021](#bib.bib21)) on the DukeVID dataset. As a result,
    it is still difficult to select the best superior method. On iLIDS, the rank-1
    performance of PSTA Wang et al. ([2021a](#bib.bib72)) is 91.5% and TMT Liu et al.
    ([2021c](#bib.bib47)) is 91.3%. However, most video re-ID architectures are complex
    in terms of the number of parameters for learning invariant feature representations
    on combined datasets. Meanwhile, re-ID methods use metric learning techniques
    like euclidean distance to calculate feature similarity which is time-consuming
    and slow retrieval and not applicable in real-world applications. How to design
    a new strategy to replace metric learning strategies still needs more research.
    Thus, further exploration of video re-ID approaches remains an interesting area
    for future research.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents a comprehensive review of global appearance, local part
    alignment methods, graph learning, attention, and transformer model in video re-ID.
    We provide specific loss functions with mathematical representation to help new
    researchers to use them instead of using straightforward common loss functions
    for video re-ID. Finally, we highlight widely and frequently used datasets for
    evaluating video re-ID techniques and analyze the performance of different methods
    and provide future research direction.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Aich et al. (2021) Aich, A., Zheng, M., Karanam, S., Chen, T., Roy-Chowdhury,
    A.K., Wu, Z., 2021. Spatio-temporal representation factorization for video-based
    person re-identification, in: International Conference on Computer Vision (ICCV).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Almasawa et al. (2019) Almasawa, M.O., Elrefaei, L.A., Moria, K., 2019. A survey
    on deep learning-based person re-identification systems. IEEE Access .
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aruna Kumar et al. (2020) Aruna Kumar, S., Yaghoubi, E., Proença, H., 2020.
    A symbolic temporal pooling method for video-based person re-identification. arXiv
    .
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2021) Bai, S., Ma, B., Chang, H., Huang, R., Shan, S., Chen, X.,
    2021. Sanet: Statistic attention network for video-based person re-identification.
    IEEE Transactions on Circuits and Systems for Video Technology .'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao et al. (2019) Bao, L., Ma, B., Chang, H., Chen, X., 2019. Preserving structural
    relationships for person re-identification, in: ICMEW, IEEE. pp. 120–125.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018a) Chen, D., Li, H., Xiao, T., Yi, S., Wang, X., 2018a. Video
    person re-identification with competitive snippet-similarity aggregation and co-attentive
    snippet embedding, in: IEEE / CVF Computer Vision and Pattern Recognition Conference.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018b) Chen, D., Xu, D., Li, H., Sebe, N., Wang, X., 2018b. Group
    consistent similarity learning via deep crf for person re-identification, in:
    IEEE / CVF Computer Vision and Pattern Recognition Conference.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019a) Chen, G., Lu, J., Yang, M., Zhou, J., 2019a. Spatial-temporal
    attention-aware learning for video-based person re-identification. IEEE Transactions
    on Image Processing .
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Chen, G., Lu, J., Yang, M., Zhou, J., 2020a. Learning recurrent
    3d attention for video-based person re-identification. IEEE IEEE Transactions
    on Image Processing .
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020b) Chen, G., Rao, Y., Lu, J., Zhou, J., 2020b. Temporal coherence
    or temporal motion: Which is more critical for video-based person re-identification?,
    in: European Conference on Computer Vision.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016) Chen, J., Wang, Y., Tang, Y.Y., 2016. Person re-identification
    by exploiting spatio-temporal cues and multi-view metric learning. IEEE Signal
    Processing Letters 23, 998–1002.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019b) Chen, Z.M., Wei, X.S., Wang, P., Guo, Y., 2019b. Multi-label
    image recognition with graph convolutional networks, in: IEEE / CVF Computer Vision
    and Pattern Recognition Conference.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2018) Cheng, D., Gong, Y., Chang, X., Shi, W., Hauptmann, A.,
    Zheng, N., 2018. Deep feature learning via structured graph laplacian embedding
    for person re-identification. PR .
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chung et al. (2017) Chung, D., Tahboub, K., Delp, E.J., 2017. A two stream
    siamese convolutional neural network for person re-identification, in: Proceedings
    of the IEEE international conference on computer vision, pp. 1983–1991.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eom et al. (2021) Eom, C., Lee, G., Lee, J., Ham, B., 2021. Video-based person
    re-identification with spatial and temporal memory networks, in: International
    Conference on Computer Vision (ICCV).'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2019) Fu, Y., Wang, X., Wei, Y., Huang, T., 2019. Sta: Spatial-temporal
    attention for large-scale video-based person re-identification, in: Association
    for the Advancement of Artificial Intelligence.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gala and Shah (2014) Gala, A., Shah, S.K., 2014. A survey of approaches and
    trends in person re-identification. Image and vision computing 32, 270–286.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao and Nevatia (2018) Gao, J., Nevatia, R., 2018. Revisiting temporal modeling
    for video-based person reid. British Machine Vision Conference (BMVC) .
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2021) Gao, Z., Shao, Y., Guan, W., Liu, M., Cheng, Z., Chen, S.,
    2021. A novel patch convolutional neural network for view-based 3d model retrieval,
    in: Proceedings of the 29th ACM International Conference on Multimedia, pp. 2699–2707.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2020) Gu, X., Chang, H., Ma, B., Zhang, H., Chen, X., 2020. Appearance-preserving
    3d convolution for video-based person re-identification, in: European Conference
    on Computer Vision.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2021) He, T., Jin, X., Shen, X., Huang, J., Chen, Z., Hua, X.S.,
    2021. Dense interaction learning for video-based person re-identification. International
    Conference on Computer Vision (ICCV) .
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. (2021) Hou, R., Chang, H., Ma, B., Huang, R., Shan, S., 2021. Bicnet-tks:
    Learning efficient spatial-temporal representation for video person re-identification.
    IEEE / CVF Computer Vision and Pattern Recognition Conference .'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. (2020) Hou, R., Chang, H., Ma, B., Shan, S., Chen, X., 2020. Temporal
    complementary learning for video person re-identification, in: European Conference
    on Computer Vision.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. (2019) Hou, R., Ma, B., Chang, H., Gu, X., Shan, S., Chen, X., 2019.
    Vrstc: Occlusion-free video person re-identification, in: IEEE / CVF Computer
    Vision and Pattern Recognition Conference.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2020) Hu, P., Liu, J., Huang, R., 2020. Concentrated multi-grained
    multi-attention network for video based person re-identification. arXiv .
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Islam (2020) Islam, K., 2020. Person search: New paradigm of person re-identification:
    A survey and outlook of recent works. Image and Vision Computing 101, 103970.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Islam et al. (2021) Islam, K., Lee, S., Han, D., Moon, H., 2021. Face recognition
    using shallow age-invariant data, in: 2021 36th International Conference on Image
    and Vision Computing New Zealand (IVCNZ), IEEE. pp. 1–6.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Islam et al. (2022) Islam, K., Zaheer, M.Z., Mahmood, A., 2022. Face pyramid
    vision transformer-supplementary .
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2020) Jiang, M., Leng, B., Song, G., Meng, Z., 2020. Weighted
    triple-sequence loss for video-based person re-identification. Neurocomputing
    .
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2021) Jiang, X., Qiao, Y., Yan, J., Li, Q., Zheng, W., Chen,
    D., 2021. Ssn3d: Self-separated network to align parts for 3d convolution in video
    person re-identification, in: Association for the Advancement of Artificial Intelligence.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karanam et al. (2015) Karanam, S., Li, Y., Radke, R.J., 2015. Sparse re-id:
    Block sparsity for person re-identification, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition workshops, pp. 33–40.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kiran et al. (2021) Kiran, M., Bhuiyan, A., Blais-Morin, L.A., Javan, M., Ayed,
    I.B., Granger, E., 2021. A flow-guided mutual attention network for video-based
    person re-identification.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012.
    Imagenet classification with deep convolutional neural networks. Neural Information
    Processing Systems .
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2018) Lan, W., Dang, J., Wang, Y., Wang, S., 2018. Pedestrian detection
    based on yolo network model, in: ICMA.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lavi et al. (2018) Lavi, B., Serj, M.F., Ullah, I., 2018. Survey on deep learning
    techniques for person re-identification task. arXiv .
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leng et al. (2019) Leng, Q., Ye, M., Tian, Q., 2019. A survey of open-world
    person re-identification. IEEE Transactions on Circuits and Systems for Video
    Technology .
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017) Li, J., Liang, X., Shen, S., Xu, T., Feng, J., Yan, S., 2017.
    Scale-aware fast r-cnn for pedestrian detection. TM .
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019) Li, J., Wang, J., Tian, Q., Gao, W., Zhang, S., 2019. Global-local
    temporal representations for video person re-identification, in: International
    Conference on Computer Vision (ICCV).'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021) Li, Q., Huang, J., Gong, S., 2021. Local-global associative
    frame assemble in video re-id. British Machine Vision Conference (BMVC) .
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Li, S., Bak, S., Carr, P., Wang, X., 2018. Diversity regularized
    spatiotemporal attention for video-based person re-identification, in: IEEE /
    CVF Computer Vision and Pattern Recognition Conference.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2021) Lin, X., Ren, P., Yeh, C.H., Yao, L., Song, A., Chang, X.,
    2021. Unsupervised person re-identification: A systematic survey of challenges
    and solutions. arXiv preprint arXiv:2109.06057 .'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021a) Liu, C.T., Chen, J.C., Chen, C.S., Chien, S.Y., 2021a. Video-based
    person re-identification without bells and whistles. arXiv .
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019a) Liu, C.T., Wu, C.W., Wang, Y.C.F., Chien, S.Y., 2019a. Spatially
    and temporally efficient non-local attention network for video-based person re-identification.
    British Machine Vision Conference (BMVC) .
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021b) Liu, J., Zha, Z.J., Wu, W., Zheng, K., Sun, Q., 2021b. Spatial-temporal
    correlation and topology learning for person re-identification in videos. arXiv
    .
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Liu, J., Zhu, X., Zha, Z.J., 2020. Temporal attribute-appearance
    learning network for video-based person re-identification. arXiv .
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2015) Liu, K., Ma, B., Zhang, W., Huang, R., 2015. A spatio-temporal
    appearance representation for viceo-based pedestrian re-identification, in: Proceedings
    of the IEEE international conference on computer vision, pp. 3810–3818.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021c) Liu, X., Zhang, P., Yu, C., Lu, H., Qian, X., Yang, X.,
    2021c. A video is worth three views: Trigeminal transformers for video-based person
    re-identification. arXiv .'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021d) Liu, X., Zhang, P., Yu, C., Lu, H., Yang, X., 2021d. Watching
    you: Global-guided reciprocal learning for video-based person re-identification,
    in: International Conference on Computer Vision (ICCV).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2017) Liu, Y., Yan, J., Ouyang, W., 2017. Quality aware network
    for set to set recognition, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, pp. 5790–5799.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Liu, Y., Yuan, Z., Zhou, W., Li, H., 2019b. Spatial and
    temporal mutual promotion for video-based person re-identification, in: Association
    for the Advancement of Artificial Intelligence.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Liu, Z., Wang, Y., Li, A., 2018. Hierarchical integration
    of rich features for video-based person re-identification. IEEE Transactions on
    Circuits and Systems for Video Technology .
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masson et al. (2019) Masson, H., Bhuiyan, A., Nguyen-Meidine, L.T., Javan, M.,
    Siva, P., Ayed, I.B., Granger, E., 2019. A survey of pruning methods for efficient
    person re-identification across domains. arXiv .
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mazzon et al. (2012) Mazzon, R., Tahir, S.F., Cavallaro, A., 2012. Person re-identification
    in crowd. Pattern Recognition Letters 33, 1828–1837.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McLaughlin et al. (2017) McLaughlin, N., del Rincon, J.M., Miller, P., 2017.
    Video person re-identification for wide area tracking based on recurrent neural
    networks. IEEE Transactions on Circuits and Systems for Video Technology .
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nambiar et al. (2019) Nambiar, A., Bernardino, A., Nascimento, J.C., 2019.
    Gait-based person re-identification: A survey. ACM CSUR .'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niall et al. (2016) Niall, M., Del Rincon, J.M., Miller, P., 2016. Recurrent
    convolutional network for video-based person re-identification, in: IEEE / CVF
    Computer Vision and Pattern Recognition Conference.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pathak et al. (2020) Pathak, P., Eshratifar, A.E., Gormish, M., 2020. Video
    person re-id: Fantastic techniques and where to find them (student abstract),
    in: Association for the Advancement of Artificial Intelligence.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn:
    Towards real-time object detection with region proposal networks. arXiv .'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Satta (2013) Satta, R., 2013. Appearance descriptors for person re-identification:
    a comprehensive review. arXiv preprint arXiv:1307.5748 .'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2018) Shen, Y., Li, H., Yi, S., Chen, D., Wang, X., 2018. Person
    re-identification with deep similarity-guided graph neural network, in: European
    Conference on Computer Vision.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2018a) Song, C., Huang, Y., Ouyang, W., Wang, L., 2018a. Mask-guided
    contrastive attention model for person re-identification, in: IEEE / CVF Computer
    Vision and Pattern Recognition Conference.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2018b) Song, G., Leng, B., Liu, Y., Hetang, C., Cai, S., 2018b.
    Region-based quality estimation network for large-scale person re-identification,
    in: Association for the Advancement of Artificial Intelligence.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2019) Song, W., Wu, Y., Zheng, J., Chen, C., Liu, F., 2019. Extended
    global–local representation learning for video person re-identification. Access
    .
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Subramaniam et al. (2019) Subramaniam, A., Nambiar, A., Mittal, A., 2019. Co-segmentation
    inspired attention networks for video-based person re-identification, in: International
    Conference on Computer Vision (ICCV).'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subramaniam et al. (2021) Subramaniam, A., Vaidya, J., Ameen, M.A.M., Nambiar,
    A., Mittal, A., 2021. Co-segmentation inspired attention module for video-based
    computer vision tasks .
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Wang, H., Du, H., Zhao, Y., Yan, J., 2020. A comprehensive
    overview of person re-identification approaches. Ieee Access 8, 45556–45583.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018a) Wang, K., Wang, H., Liu, M., Xing, X., Han, T., 2018a. Survey
    on person re-identification based on deep learning. CAAI Transactions on Intelligence
    Technology 3, 219–227.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018b) Wang, M., Lai, B., Jin, Z., Gong, X., Huang, J., Hua, X.,
    2018b. Deep active learning for video-based person re-identification. arXiv .
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2014) Wang, T., Gong, S., Zhu, X., Wang, S., 2014. Person re-identification
    by video ranking, in: European Conference on Computer Vision.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Gupta (2018) Wang, X., Gupta, A., 2018. Videos as space-time region
    graphs, in: European Conference on Computer Vision.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Wang, X., Hua, Y., Kodirov, E., Hu, G., Robertson, N.M.,
    2019a. Deep metric learning by online soft mining and class-aware attention, in:
    Association for the Advancement of Artificial Intelligence.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021a) Wang, Y., Zhang, P., Gao, S., Geng, X., Lu, H., Wang, D.,
    2021a. Pyramid spatial-temporal aggregation for video-based person re-identification,
    in: International Conference on Computer Vision (ICCV).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021b) Wang, Z., He, L., Tu, X., Zhao, J., Gao, X., Shen, S., Feng,
    J., 2021b. Robust video-based person re-identification by hierarchical mining.
    IEEE Transactions on Circuits and Systems for Video Technology .
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019b) Wang, Z., Wang, Z., Zheng, Y., Wu, Y., Zeng, W., Satoh,
    S., 2019b. Beyond intra-modality: A survey of heterogeneous person re-identification.
    arXiv .'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) Wu, D., Zheng, S.J., Zhang, X.P., Yuan, C.A., Cheng, F., Zhao,
    Y., Lin, Y.J., Zhao, Z.Q., Jiang, Y.L., Huang, D.S., 2019. Deep learning-based
    methods for person re-identification: A comprehensive review. Neurocomputing 337,
    354–371.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2016) Wu, L., Shen, C., Hengel, A.v.d., 2016. Deep recurrent convolutional
    networks for video-based person re-identification: An end-to-end approach. arXiv
    preprint arXiv:1606.01609 .'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Wu, Y., Bourahla, O.E.F., Li, X., Wu, F., Tian, Q., Zhou, X.,
    2020. Adaptive graph representation learning for video person re-identification.
    IEEE Transactions on Image Processing 29, 8821–8830.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2018) Wu, Y., Lin, Y., Dong, X., Yan, Y., Ouyang, W., Yang, Y.,
    2018. Exploit the unknown gradually: One-shot video-based person re-identification
    by stepwise learning, in: IEEE / CVF Computer Vision and Pattern Recognition Conference.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiangtan et al. (2021) Xiangtan, L., Ren, P., Xiao, Y., Chang, X., Hauptmann,
    A., 2021. Person search challenges and solutions: A survey. arXiv preprint arXiv:2105.01605
    .'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2017) Xu, S., Cheng, Y., Gu, K., Yang, Y., Chang, S., Zhou, P.,
    2017. Jointly attentive spatial-temporal pooling networks for video-based person
    re-identification, in: Proceedings of the IEEE international conference on computer
    vision, pp. 4733–4742.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2016) Yan, Y., Ni, B., Song, Z., Ma, C., Yan, Y., Yang, X., 2016.
    Person re-identification via recurrent feature aggregation, in: European Conference
    on Computer Vision.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2020) Yan, Y., Qin, J., Chen, J., Liu, L., Zhu, F., Tai, Y., Shao,
    L., 2020. Learning multi-granular hypergraphs for video-based person re-identification,
    in: IEEE / CVF Computer Vision and Pattern Recognition Conference.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2019) Yan, Y., Zhang, Q., Ni, B., Zhang, W., Xu, M., Yang, X.,
    2019. Learning context graph for person search, in: IEEE / CVF Computer Vision
    and Pattern Recognition Conference.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020) Yang, J., Zheng, W.S., Yang, Q., Chen, Y.C., Tian, Q., 2020.
    Spatial-temporal graph convolutional network for video-based person re-identification,
    in: IEEE / CVF Computer Vision and Pattern Recognition Conference.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2021) Yang, X., Liu, L., Wang, N., Gao, X., 2021. A two-stream
    dynamic pyramid representation model for video-based person re-identification.
    IEEE Transactions on Image Processing .
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2018) Ye, M., Lan, X., Yuen, P.C., 2018. Robust anchor embedding
    for unsupervised video person re-identification in the wild, in: European Conference
    on Computer Vision, pp. 170–186.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2019) Ye, M., Li, J., Ma, A.J., Zheng, L., Yuen, P.C., 2019. Dynamic
    graph co-matching for unsupervised video-based person re-identification. IEEE
    Transactions on Image Processing 28, 2976–2990.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2021) Ye, M., Shen, J., Lin, G., Xiang, T., Shao, L., Hoi, S.C.,
    2021. Deep learning for person re-identification: A survey and outlook. IEEE Transactions
    on Pattern Analysis and Machine Intelligence .'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. (2016) You, J., Wu, A., Li, X., Zheng, W.S., 2016. Top-push video-based
    person re-identification, in: Proceedings of the ieee conference on computer vision
    and pattern recognition, pp. 1345–1353.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021a) Zhang, G., Chen, Y., Dai, Y., Zheng, Y., Wu, Y., 2021a.
    Reference-aided part-aligned feature disentangling for video person re-identification.
    ICME .
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020a) Zhang, L., Shi, Z., Zhou, J.T., Cheng, M.M., Liu, Y.,
    Bian, J.W., Zeng, Z., Shen, C., 2020a. Ordered or orderless: A revisit for video
    based person re-identification. IEEE TPAMI .'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021b) Zhang, T., Wei, L., Xie, L., Zhuang, Z., Zhang, Y., Li,
    B., Tian, Q., 2021b. Spatiotemporal transformer for video-based person re-identification.
    arXiv .
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017) Zhang, W., Yu, X., He, X., 2017. Learning bidirectional
    temporal cues for video-based person re-identification. IEEE Transactions on Circuits
    and Systems for Video Technology .
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Zhang, Z., Lan, C., Zeng, W., Chen, Z., 2020b. Multi-granularity
    reference-aided attentive feature aggregation for video-based person re-identification,
    in: IEEE / CVF Computer Vision and Pattern Recognition Conference.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020b) Zhang, Z., Lan, C., Zeng, W., Chen, Z., 2020b. 《多粒度参考辅助注意特征聚合用于视频基础人员重识别》，发表于：IEEE
    / CVF 计算机视觉与模式识别会议。
- en: 'Zhao et al. (2019) Zhao, Y., Shen, X., Jin, Z., Lu, H., Hua, X.s., 2019. Attribute-driven
    feature disentangling and temporal aggregation for video person re-identification,
    in: IEEE / CVF Computer Vision and Pattern Recognition Conference.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2019) Zhao, Y., Shen, X., Jin, Z., Lu, H., Hua, X.s., 2019. 《基于属性驱动的特征解耦和时间聚合用于视频人员重识别》，发表于：IEEE
    / CVF 计算机视觉与模式识别会议。
- en: 'Zheng et al. (2021) Zheng, C., Wei, P., Zheng, N., 2021. A duplex spatiotemporal
    filtering network for video-based person re-identification, in: ICPR.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2021) Zheng, C., Wei, P., Zheng, N., 2021. 《用于视频基础人员重识别的双重时空过滤网络》，发表于：国际模式识别大会（ICPR）。
- en: 'Zheng et al. (2016a) Zheng, L., Bie, Z., Sun, Y., Wang, J., Su, C., Wang, S.,
    Tian, Q., 2016a. Mars: A video benchmark for large-scale person re-identification,
    in: European Conference on Computer Vision.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2016a) Zheng, L., Bie, Z., Sun, Y., Wang, J., Su, C., Wang, S.,
    Tian, Q., 2016a. 《火星：大规模人员重识别的视频基准》，发表于：欧洲计算机视觉会议。
- en: 'Zheng et al. (2016b) Zheng, L., Yang, Y., Hauptmann, A.G., 2016b. Person re-identification:
    Past, present and future. arXiv .'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2016b) Zheng, L., Yang, Y., Hauptmann, A.G., 2016b. 《人员重识别：过去、现在与未来》。arXiv.
- en: 'Zhou et al. (2017) Zhou, Z., Huang, Y., Wang, W., Wang, L., Tan, T., 2017.
    See the forest for the trees: Joint spatial and temporal recurrent neural networks
    for video-based person re-identification, in: IEEE / CVF Computer Vision and Pattern
    Recognition Conference.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2017) Zhou, Z., Huang, Y., Wang, W., Wang, L., Tan, T., 2017. 《看树木却见森林：用于视频基础人员重识别的联合空间和时间递归神经网络》，发表于：IEEE
    / CVF 计算机视觉与模式识别会议。
