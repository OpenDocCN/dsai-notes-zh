- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:32:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2405.00334] A Survey on Deep Active Learning: Recent Advances and New Frontiers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.00334](https://ar5iv.labs.arxiv.org/html/2405.00334)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A Survey on Deep Active Learning: Recent Advances and New Frontiers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dongyuan Li, Zhen Wang, Yankai Chen, Renhe Jiang, Weiping Ding, , Manabu Okumura
    Dongyuan Li, Zhen Wang, Manabu Okumura are with the Institute of Innovative Research,
    School of Information and Communication Engineering, Tokyo Institute of Technology,
    Tokyo 152-8550, Japan. ({lidy,wzh}@lr.pi.titech.ac.jp, oku@pi.titech.ac.jp). (D.
    Li and Z. Wang contributed equally to this work). Yankai Chen is with the School
    of Computer Science and Engineering, The Chinese University of Hong Kong, (email:
    ykchen@cse.cuhk.edu.hk). Renhe Jiang is with the Center for Spatial Information
    Science, The University of Tokyo. Tokyo, Japan. (email: jiangrh@csis.u-tokyo.ac.jp).
    Weiping Ding is with the School of Information Science and Technology, Nantong
    University, Nantong 226019, China (e-mail: dwp9988@163.com) Corresponding authors:
    Weiping Ding and Manabu Okumura.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Active learning seeks to achieve strong performance with fewer training samples.
    It does this by iteratively asking an oracle to label new selected samples in
    a human-in-the-loop manner. This technique has gained increasing popularity due
    to its broad applicability, yet its survey papers, especially for deep learning-based
    active learning (DAL), remain scarce. Therefore, we conduct an advanced and comprehensive
    survey on DAL. We first introduce reviewed paper collection and filtering. Second,
    we formally define the DAL task and summarize the most influential baselines and
    widely used datasets. Third, we systematically provide a taxonomy of DAL methods
    from five perspectives, including annotation types, query strategies, deep model
    architectures, learning paradigms, and training processes, and objectively analyze
    their strengths and weaknesses. Then, we comprehensively summarize main applications
    of DAL in Natural Language Processing (NLP), Computer Vision (CV), and Data Mining
    (DM), etc. Finally, we discuss challenges and perspectives after a detailed analysis
    of current studies. This work aims to serve as a useful and quick guide for researchers
    in overcoming difficulties in DAL. We hope that this survey will spur further
    progress in this burgeoning field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Active learning, Deep learning, Natural language processing, Computer vision,
    Uncertainty quantification, Sequential optimal design, Adaptive sampling.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The remarkable success of deep learning relies heavily on large-scale datasets
    with human-annotated labels [[1](#bib.bib1)]. However, continually labeling large-scale
    datasets is an extremely time-consuming, expensive, and laborious task, which
    tends to become a bottleneck for deep learning with limited labeled data. To tackle
    this issue, Deep Active Learning (DAL) recently exhibits great potential. As Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers") shows, DAL models are first trained on an initial training
    dataset. Then, query strategies can be iteratively applied to select the most
    informative and representative samples from a large pool of unlabeled data. Finally,
    an oracle labels the selected samples and adds them to the training dataset for
    retraining or fine-tuning of the DAL models. DAL aims to achieve competitive performance
    while reducing annotation costs within a reasonable time [[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)]. Benefiting from the strong representation capabilities of various
    neural networks, such as Graph Neural Networks (GNNs) [[5](#bib.bib5)], Convolutional
    Neural Networks (CNNs) [[6](#bib.bib6)], and Transformers [[7](#bib.bib7)], as
    well as leveraging prior knowledge from pre-trained models like Contrastive Language-Image
    Pre-Training (CLIP) [[8](#bib.bib8)] and Generative Pre-trained Transformer (GPT) [[9](#bib.bib9)],
    DAL has made significant advances.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cfda53036970606f7f5c0be7de8fed75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The general pipeline in deep active learning.'
  prefs: []
  type: TYPE_NORMAL
- en: As a methodology for selecting or generating a subset of training data in data-centric
    AI, DAL is closely related to learning settings and practical techniques, including
    curriculum learning [[10](#bib.bib10)], transfer learning [[11](#bib.bib11)],
    data augmentation or pruning [[12](#bib.bib12), [13](#bib.bib13)], and dataset
    distillation [[14](#bib.bib14)]. The commonality of these methods is to train
    or fine-tune a model using a small number of samples, aiming to remove noise and
    redundancy while improving training efficiency without decreasing models’ performance
    on downstream tasks. However, one primary difference from DAL is that these approaches
    have full access to all labels when selecting, distilling, or generating training
    subsets. DAL defaults to that all data should be unlabeled during the training
    subset selection process, making it better suited for real-world scenarios where
    labels are initially unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize DAL methodologies, recent efforts have focused on specific tasks
    such as text classification [[15](#bib.bib15)] and image analysis [[16](#bib.bib16),
    [17](#bib.bib17)], specific domains like NLP [[18](#bib.bib18)] and CV [[19](#bib.bib19),
    [20](#bib.bib20)], or reproducing mainstream baselines [[21](#bib.bib21), [22](#bib.bib22)].
    As for most early survey work, one common inadequacy is that they may not have
    enough discussion of recent advances [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)],
    or lack summarization of emerging learning paradigms (contrastive learning etc.)
    and challenges [[26](#bib.bib26), [27](#bib.bib27)], especially in light of rapidly
    developing deep learning techniques (e.g., Fine-tune on pre-trained models). To
    assist researchers in reviewing, summarizing, and planning for future exploration,
    we provide a comprehensive review encompassing the latest advancements and insights
    in the field. While some survey papers focus on stream-based DAL [[28](#bib.bib28)],
    this paper concentrates on pool-based DAL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we first introduce our strategy for collecting reviewed papers
    and explain our criteria for selecting them in Section [II](#S2 "II Paper Collection
    and Filtering ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers").
    Then, we give a specific formal definition for DAL in Section [III-A](#S3.SS1
    "III-A Notations & Definition ‣ III Deep Active Learning ‣ A Survey on Deep Active
    Learning: Recent Advances and New Frontiers"), and chronologically summarize the
    most influential DAL baselines and the widely used datasets in Section [III-C](#S3.SS3
    "III-C Important DAL Baselines and Datasets ‣ III Deep Active Learning ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers"). As Fig. [2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers") shows, in Section [IV](#S4 "IV Taxonomy of DAL ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers"), we develop a high-level
    taxonomy to provide a broad overview of this field, categorizing previous studies
    from five perspectives. In Section [IV-A](#S4.SS1 "IV-A Annotation Type ‣ IV Taxonomy
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"),
    we classify the annotation types into hard, soft, hybrid, explanatory, and random/multi-agent
    annotations, and give a detailed introduction to each annotation type. In Section [IV-B](#S4.SS2
    "IV-B Query Strategy ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), we summarize query strategies into five distinct
    categories, including uncertainty-based, representative-based, influence-based,
    Bayesian-based and their hybrid methods, and analyze the strengths and weaknesses
    of each query type. As for deep model architectures, in Section [IV-C](#S4.SS3
    "IV-C Model Architecture ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), they are mainly categorized into Recurrent
    Neural Networks (RNNs), CNNs, GNNs, and Pre-trained methods. We discuss the benefits
    and drawbacks of each type of architecture. In Section [IV-D](#S4.SS4 "IV-D Learning
    Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers"), we are pleased to discover that various emerging learning
    paradigms, such as Curriculum Learning and Continual Learning, have shown promising
    results when combined with DAL. For each learning paradigm, we provide a detailed
    description of its definition and how to integrate it with DAL. Finally, in Section [IV-E](#S4.SS5
    "IV-E Training Process ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), three different training processes, including
    traditional training, curriculum learning-based training, and pre-training & fine-tuning
    will be introduced with typical examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/269d839502ba34ad8965f623d603e311.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Taxonomy for deep active learning methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bb6194866d5be41b040418ca169db074.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Emerging challenges in deep active learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Section [V](#S5 "V Applications of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), we comprehensively show some domains in which
    DAL methods have been successfully applied, including NLP, CV, DM, etc. As depicted
    in Fig. [3](#S1.F3 "Figure 3 ‣ I Introduction ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), despite the remarkable progress in DAL, this
    rapidly developing field is still fraught with several crucial emerging challenges.
    In Section [VI](#S6 "VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active
    Learning: Recent Advances and New Frontiers"), we analyze the causes and opportunities
    of each challenge, which can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline-related: inefficient & costly human annotation, insufficient research
    on stopping strategies, and cold-start;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Task-related: difficulty in cross-domain transfer, unstable performance, and
    lack of scalability and generalizability;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dataset-related: outlier data & oracles, data scarcity & imbalance, and class
    distribution mismatch.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, after organizing and summarizing the current DAL-related research,
    we have four intriguing findings that we would like to share with the readers:
    (1) As shown in Section [IV-E](#S4.SS5 "IV-E Training Process ‣ IV Taxonomy of
    DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"), DAL
    has great potential as a sample selection strategy to apply few-shot or one-shot
    setting for large-scale pre-trained models with billions of parameters [[29](#bib.bib29),
    [30](#bib.bib30)]. Furthermore, as discussed in Section [III-C](#S3.SS3 "III-C
    Important DAL Baselines and Datasets ‣ III Deep Active Learning ‣ A Survey on
    Deep Active Learning: Recent Advances and New Frontiers"), many studies have shown
    that using only 10$\sim$20% labeled samples for fine-tuning the pre-trained language
    models with billions of parameters can yield even better performance and be 5$\sim$10
    times more efficient than training with a full labeled dataset [[31](#bib.bib31),
    [32](#bib.bib32)]. (2) Intuitively, having more high-quality samples can promote
    model performance for some tasks. Thus, as shown in Section [IV-D](#S4.SS4 "IV-D
    Learning Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers"), many works integrate DAL with semi-supervised strategies,
    allowing to obtain more high-quality labeled samples without increasing the need
    for human labor. However, as discussed in Section [VI-C](#S6.SS3 "VI-C Dataset-related
    Issues ‣ VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), semi-supervised methods are highly sensitive
    to outliers and error labels, easily fueling a vicious cycle, i.e., models continue
    to label samples with wrong pseudo-labels. How to effectively integrate DAL with
    semi-supervised strategies, using human-labeled true signals to guide semi-supervised
    annotation and avoid the mislabel circular, remains an open and challenging issue
    waiting to be solved. (3) From the detailed analysis of Scalability & Generalizability
    in Section [VI-B](#S6.SS2 "VI-B Task-related Issues ‣ VI Challenges & Opportunities
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"),
    although DAL has achieved great success in classification tasks, comparing various
    DAL methods to choose the optimal one for a given task remains time-intensive
    and unrealistic in practice. Thus, there is an urgent need for a universal framework
    that is friendly to various downstream tasks. (4) By summarizing DAL applications
    for NLP in Section [V-A](#S5.SS1 "V-A Applications in Natural Language Processing
    ‣ V Applications of DAL ‣ A Survey on Deep Active Learning: Recent Advances and
    New Frontiers"), we find only a few DAL studies focused on generative tasks. Generative
    tasks, such as summarization and question answering, urgently require more attention
    and research compared to classification tasks. This is because generating informative
    objects, such as annotations, is more difficult and time-consuming. Defining the
    most meaningful samples for generation tasks and explaining why those samples
    play an important role are two core problems that need to be solved. We hope that
    future research can promote the development of DAL for generation tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the main contributions of this paper are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the latest comprehensive and systematic survey paper on DAL to help
    researchers review, summarize, and look forward to the future about DAL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the novel DAL texonomy, we detail the explanations and discussions
    of the methodology, ranging from annotation types, query strategies, deep model
    architectures, learning paradigms, and training processes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difficult challenges in DAL are presented from multiple perspectives. By
    a detailed analysis of challenges and current studies, we discuss possible advanced
    solutions for them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GitHub repository¹¹1[https://github.com/Clearloveyuan/Awesome-Active-Learning](https://github.com/Clearloveyuan/Awesome-Active-Learning)
    is available with the most up-to-date DAL techniques, including papers, code,
    and datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Remaining part of this survey is organized as follows. Section [II](#S2 "II
    Paper Collection and Filtering ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers") shows the collection of DAL papers. Section [III](#S3 "III
    Deep Active Learning ‣ A Survey on Deep Active Learning: Recent Advances and New
    Frontiers") introduces important DAL baselines and datasets. Section [IV](#S4
    "IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New
    Frontiers") details the taxonomy of DAL methods. Section [V](#S5 "V Applications
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")
    reviews DAL-related applications. Section [VI](#S6 "VI Challenges & Opportunities
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")
    introduces DAL challenges and opportunities. Section [VII](#S7 "VII Conclusion
    ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers") ends this
    article with the conclusions.'
  prefs: []
  type: TYPE_NORMAL
- en: II Paper Collection and Filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first determine relevant keywords used to search articles and create an
    initial keyword list, as shown in Fig. [4](#S2.F4 "Figure 4 ‣ II Paper Collection
    and Filtering ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers").
    We perform searches across multiple databases using all possible 3-keyword combinations
    from defined keyword groups, such as “Active Learning”, “Machine Learning”, and
    “Open-set”. The databases searched include Google Scholar, Scopus, Semantic Scholar,
    and Web of Science. We limit the number of papers collected per query to 200,
    and the publication date ranges from January 2013 to March 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1ec6b438efc3f31738b9b42ed42770d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Keywords and publication trend on DAL.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We collect a total of 10,000 research papers from various sources and obtain
    3,967 unique papers after removing any duplicates. Fig. [4](#S2.F4 "Figure 4 ‣
    II Paper Collection and Filtering ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers") shows the trend of these articles over time, revealing a growing
    interest in the topic we are investigating. To ensure the relevance of the collected
    articles to DAL, we conduct a detailed manual inspection of their abstracts. As
    a result, we identify 1,273 articles that are considered interesting and pertinent
    for our study. Based on the collected materials, we employ these keywords to perform
    a final filtering process and also consider the reputation of conferences or journals
    in which the papers were published, as well as their impact. This approach further
    refines our dataset, resulting in 405 articles that are selected for systematic
    analysis, and 220 articles are finally summarized and discussed, focusing on their
    key findings and contributions. This rigorous analysis ensures that the articles
    are relevant and provide valuable insight into the field of DAL.'
  prefs: []
  type: TYPE_NORMAL
- en: III Deep Active Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first introduce the basic notation and definition of DAL
    and then discuss the most important DAL baselines based on their relevance and
    chronological order.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 DAL procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Unlabeled Data $\mathcal{D}_{\textbf{pool}}$ Parameter: Batch Size $b$,
    Iteration Times $T$, Query Function $\alpha$ Output: The final trained model $\mathcal{M}$'
  prefs: []
  type: TYPE_NORMAL
- en: 1:  $\mathcal{Q}_{0}\leftarrow$ Initialization sampling from $\mathcal{D}_{\textbf{pool}}$
    where $|\mathcal{Q}_{0}|=b$;2:  $\mathcal{D}_{\textbf{train}}^{0}\leftarrow\mathcal{Q}_{0}$
    [Initialization of training dataset];3:  $\mathcal{M}_{0}\leftarrow$ Train $\mathcal{M}_{0}$
    on $\mathcal{D}_{\textbf{train}}^{0}$;4:  while not stop-criterion( ) $\&amp;$
    i $\leq$  $T$ do5:     $\mathcal{Q}_{i}$  $\leftarrow$  $\alpha(\mathcal{M}_{i-1},\mathcal{D}_{\textbf{pool}}^{i-1},b$)
      [Annotating $b$ samples];6:     $\mathcal{D}_{\textbf{train}}^{i}$  =  $\mathcal{D}_{\textbf{train}}^{i-1}$  $\cup$  $\mathcal{Q}_{i}$;
    $\mathcal{D}_{\textbf{pool}}^{i}\leftarrow\mathcal{D}_{\textbf{pool}}^{i-1}\backslash\mathcal{Q}_{i}$;7:     $\mathcal{M}_{i}\leftarrow$
    Train $\mathcal{M}_{i-1}$ on $\mathcal{D}_{\textbf{train}}^{i}$;8:  end while
  prefs: []
  type: TYPE_NORMAL
- en: III-A Notations & Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We focus on pool-based DAL methods since most DAL methods belong to this category.
    Pool-based DAL methods iteratively select the most informative samples from a
    large pool of unlabeled datasets until either the base model reaches a certain
    level of performance or a pre-defined budget is exhausted. As shown in Algorithm [1](#alg1
    "Algorithm 1 ‣ III Deep Active Learning ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers"), we use a classification task as an example for illustration,
    while other tasks follow the typical definition of their task domains. Given an
    initial labeled training dataset $\mathcal{D}_{\textbf{train}}=\{\bm{x}_{i},y_{i}\}_{i=1}^{m}$
    and a large-scale pool of unlabeled data $\mathcal{D}_{\textbf{pool}}=\{\bm{x}_{i}\}_{i=1}^{n}$,
    where m$\ll$n, $\bm{x}_{i}$ represents the feature vector of the $i$-th sample,
    and $y_{i}\in\{0,1\}$ is the class label for binary classification (or $y_{i}\in\{1,\dots,k\}$
    for multi-label classification), the DAL procedure is carried out in $T$ iterations.
    In the $i$-th iteration, a batch of samples $\mathcal{Q}^{i}$ with batch size
    $b$ is selected from $\mathcal{D}_{\textbf{pool}}^{i-1}$ on the basis of the base
    model $\mathcal{M}$ and an acquisition function $\alpha(\,)$. These samples $\mathcal{Q}^{i}$
    are then labeled by an oracle and added to the $i$-th training dataset $\mathcal{D}^{i}_{\textbf{train}}$,
    with which the model $\mathcal{M}$ is then re-trained. DAL terminates when the
    labeled budget $Q$ is exhausted or the desired performance of the model is reached.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Comparisons between Traditional and Deep AL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The differences between traditional and Deep AL mainly lie in the following
    two aspects: (1) most traditional AL methods use fixed pre-processed features
    to calculate uncertainty/representativeness. In deep learning tasks, feature representations
    are jointly learned with Deep Neural Networks (DNNs). Therefore, feature representations
    dynamically change during DAL processes, and thus pairwise distances/similarities
    used by representativeness-based measures need to be re-computed in every stage.
    In contrast, for traditional AL with classical ML tasks, these pairwise terms
    should be pre-computed [[22](#bib.bib22)]. (2) DAL can leverage advanced large-scale
    pre-trained language models to achieve comparable performance in few-shot or one-shot
    settings. In contrast, traditional AL methods with few-shot or one-shot settings
    may not meet the minimum requirements for the number of training samples needed
    to achieve comparable performance [[30](#bib.bib30), [33](#bib.bib33)]. On the
    other hand, the most similar aspect between traditional and deep AL methods is
    their utilization of a small number of the most informative samples to train models,
    thereby improving efficiency and reducing reliance on labeled samples.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Important DAL Baselines and Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most important baselines for DAL are carefully categorized in Table [I](#S3.T1
    "TABLE I ‣ III-C Important DAL Baselines and Datasets ‣ III Deep Active Learning
    ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers") from six
    perspectives to provide readers with a complete understanding of the development
    of DAL and the identification of the most relevant works. These influential studies
    have achieved breakthroughs in designing new DAL methods, tackling novel tasks,
    or integrating with emerging learning paradigms. They have been published in influential
    international conferences or high-quality journals in machine learning, CV, NLP,
    etc., and have been highly cited with more than 100 total citations or more than
    10 citations per year.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Detailed taxonomy of important Deep Active Learning baselines. Refer
    to Section [IV](#S4 "IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers") for a detailed explanation of each category. Any
    Types in Query Strategy means the proposed frameworks can be combined with any
    types of DAL query strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Query Strategy | Architecture | Learning Paradigm | Annotation |
    Training | Tasks |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BCBA [2016] [[34](#bib.bib34)] | Bayesian | CNNs | Traditional | Hard | Traditional
    | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| DBAL [2017] [[35](#bib.bib35)] | Bayesian | CNNs | Semi-supervised Learning
    | Hard | Traditional | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| CEAL [2017] [[36](#bib.bib36)] | Uncertainty | CNNs | Curriculum Learning
    | Hybrid | Curriculum | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| ESNN [2017] [[37](#bib.bib37)] | Uncertainty | BNNs | Adversarial Learning
    | Hard | Traditional | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| PAL [2017] [[38](#bib.bib38)] | Uncertainty | BNNs | Reinforcement Learning
    | Hard | Traditional | Named Entity Recognition |'
  prefs: []
  type: TYPE_TB
- en: '| LAL [2017] [[39](#bib.bib39)] | Influence | Random Forest | Traditional |
    Hard | Traditional | Regression Tasks |'
  prefs: []
  type: TYPE_TB
- en: '| GAAL [2017] [[40](#bib.bib40)] | Uncertainty | GNNs | Adversarial Learning
    | Hard | Traditional | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| CoreSet [2018] [[41](#bib.bib41)] | Representative | CNNs | Semi-supervised
    Learning | Hard | Traditional | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| DFAL [2018] [[42](#bib.bib42)] | Uncertainty | CNNs | Adversarial Training
    | Hard | Traditional | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| ASM [2019] [[43](#bib.bib43)] | Uncertainty | CNNs | Curriculum Learning
    | Hybrid | Curriculum | Objective Detection |'
  prefs: []
  type: TYPE_TB
- en: '| MIAL [2019] [[44](#bib.bib44)] | Representative | SVM | Traditional | Hard
    | Traditional | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| BatchBALD [2019] [[45](#bib.bib45)] | Uncertainty | BNNs | Traditional |
    Hard | Traditional | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| DRAL [2019] [[46](#bib.bib46)] | Uncertainty | CNNs | Reinforcement Learning
    | Hard | Pre+FT | Person Re-Identification |'
  prefs: []
  type: TYPE_TB
- en: '| DLER [2019] [[47](#bib.bib47)] | Uncertainty | PLMs | Transfer Learning |
    Hard | Pre+FT | Entity Resolution |'
  prefs: []
  type: TYPE_TB
- en: '| BGADL [2019] [[48](#bib.bib48)] | Hybrid | BNNs | Semi-supervised Learning
    | Hard | Traditional | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| VAAL [2019] [[49](#bib.bib49)] | Representative | VAE | Adversarial Learning
    | Hard | Traditional | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| AADA [2020] [[50](#bib.bib50)] | Hybrid | CNNs | Transfer Learning | Hard
    | Pre+FT | Object Detection |'
  prefs: []
  type: TYPE_TB
- en: '| CSAL [2020] [[51](#bib.bib51)] | Hybrid | CNNs | Traditional | Hard | Pre+FT
    | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| SRAAL [2020] [[52](#bib.bib52)] | Uncertainty | CNNs | Adversarial Learning
    | Hard | Pre+FT | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| ALPS [2020] [[31](#bib.bib31)] | Uncertainty | PLMs | Traditional | Hard
    | Pre+FT | Cold-start Issue |'
  prefs: []
  type: TYPE_TB
- en: '| Ein-Dor et al. [2020] [[53](#bib.bib53)] | Any Types | PLMs | Traditional
    | Hard | Pre+FT | Text Classification |'
  prefs: []
  type: TYPE_TB
- en: '| TOD [2021] [[54](#bib.bib54)] | Uncertainty | CNNs | Traditional | Hard |
    Pre+FT | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster-Margin [2021] [[55](#bib.bib55)] | Representative | CNNs | Traditional
    | Hard | Pre+FT | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| LADA [2021] [[56](#bib.bib56)] | Uncertainty | CNNs | Semi-supervised Learning
    | Hard | Traditional | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| TA-VAAL [2021] [[57](#bib.bib57)] | Influence | VAE | Adversarial Learning
    | Hard | Pre+FT | Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Karamcheti et al. [2021] [[58](#bib.bib58)] | Hybrid | PLMs | Traditional
    | Hard | Pre+FT | Visual Question Answering |'
  prefs: []
  type: TYPE_TB
- en: '| MAML [2022] [[59](#bib.bib59)] | Any Types | PLMs | Meta Learning | Hard
    | Pre+FT | Text Classification |'
  prefs: []
  type: TYPE_TB
- en: '| BATL [2022] [[32](#bib.bib32)] | Hybrid | PLMs | Traditional | Hard | Pre+FT
    | Text Classification |'
  prefs: []
  type: TYPE_TB
- en: '| TYROGUE [2022] [[60](#bib.bib60)] | Hybrid | PLMs | Traditional | Hard |
    Pre+FT | Text Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Schroder et al. [2022] [[61](#bib.bib61)] | Uncertainty | PLMs | Traditional
    | Hard | Pre+FT | Text Classification |'
  prefs: []
  type: TYPE_TB
- en: BCBA [[34](#bib.bib34)] pioneers the combination of AL with Bayesian neural
    networks (BNNs), using Monte Carlo dropout for a variational Bayesian approximation
    to apply for image classification. Based on this, DBAL [[35](#bib.bib35)] proposes
    an uncertainty-based query strategy for high-dimensional image classification.
    To expand number of labeled samples without increasing human labors, CEAL [[36](#bib.bib36)]
    combines DAL with semi-supervised strategies by assigning pseudo-labels to high-confidence
    samples while requesting annotations for the most uncertain samples. Relying on
    a single query strategy may lead to errors. Thus, ESNN [[37](#bib.bib37)] uses
    a deep ensemble of DNNs to measure sample uncertainty from multiple aspects and
    achieves good robustness for unbalanced datasets. However, the aforementioned
    methods are criticized for being less effective for batch DAL [[45](#bib.bib45)].
    To address this issue, CoreSet [[41](#bib.bib41)] selects informative batches
    that cover the whole data distribution and BatchBALD [[45](#bib.bib45)] uses mutual
    information to identify the most informative batches. And Cluster-Margin [[55](#bib.bib55)]
    aims to select informative and diverse mini batches to improve accuracy and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: To better help DAL adjust to different tasks, reinforcement learning provides
    detailed rewards for dynamically controlling query strategies. For example, PAL [[38](#bib.bib38)]
    learns a deep reinforcement learning-based Q-network as an adaptive policy to
    select data samples for labeling. Similarly, DRAL [[46](#bib.bib46)] uses a reinforcement
    learning framework to dynamically adjust the acquisition function via rewards
    to obtain high-quality queries. UCBVI [[62](#bib.bib62)] provides a new modification
    to the Q-network formulation for reward-free exploration, significantly reducing
    query complexity. However, reinforcement learning requires a large amount of training
    data and human-designed rewards, which is difficult for many real-world applications.
    To address this issue, meta learning and transfer learning have become main solutions.
    LAL [[39](#bib.bib39)] trains a regressor to learn optimal query strategies for
    downstream tasks. MAML [[59](#bib.bib59)] combines meta learning and DAL by initializing
    an active learner with meta-learned parameters obtained through meta-training
    on tasks similar to the target task during DAL. DLER [[47](#bib.bib47)] designs
    an architecture to learn a transferable model from a high-resource setting to
    a low-resource one, allowing DAL to select a few informative samples based on
    the knowledge of the source domain. AADA [[50](#bib.bib50)] jointly considers
    domain alignment, uncertainty, and diversity for sample selection.
  prefs: []
  type: TYPE_NORMAL
- en: To enlarge the labeled training dataset for DNNs without incurring additional
    human labor costs, semi-supervised, semi-supervised, and self-supervised DAL methods
    have been proposed. MIAL [[44](#bib.bib44)] pioneers semi-supervised DAL using
    cluster-based strategies to measure sample informativeness. ASM [[43](#bib.bib43)]
    collaborates with self-learning and DAL, designing a selector function to selectively
    and seamlessly determine the confidence of the samples, where high-confidence
    samples are labeled by a pseudo-labeling module, and low-confidence samples are
    labeled by humans. CSAL [[51](#bib.bib51)] first uses semi-supervised learning
    to distill information from unlabeled data during the training stage and then
    uses consistency-based sample selection for DAL. TOD [[54](#bib.bib54)] leverages
    a novel unlabeled data sampling strategy for data annotation in conjunction with
    a semi-supervised training scheme to improve the performance of the task model
    with unlabeled data. Recently, data augmentation has expanded to become a deep
    neural model that generates virtual instances to help expand training datasets.
    GAAL [[40](#bib.bib40)] introduces a generative adversarial network to the DAL
    query method to generate informative samples to train the model. BGADL [[48](#bib.bib48)]
    expands GAAL and combines generative adversarial DAL with Bayesian data augmentation
    to generate diverse and informative samples. DFAL [[42](#bib.bib42)] uses adversarial
    DAL to select samples close to the decision boundary as the most informative samples
    for DAL. VAAL [[49](#bib.bib49)] learns a latent space using a variational autoencoder
    (VAE) to generate new informative samples and trains an adversarial network to
    discriminate labeled and unlabeled data. Inspired by these works, TA-VAAL [[57](#bib.bib57)]
    incorporates a learning loss prediction module and a task ranker to enable task-aware
    sample selection. SRAAL [[52](#bib.bib52)] proposes a relabel adversarial model
    that aims to obtain the most informative unlabeled samples. LADA [[56](#bib.bib56)]
    anticipates data augmentation impact by scoring both real and virtually augmented
    instances, allowing training in informative labeled and augmented data.
  prefs: []
  type: TYPE_NORMAL
- en: Large-scale pre-trained language models (PLMs) achieve great success and become
    a milestone in artificial intelligence. Due to sophisticated pre-training objectives
    and huge model parameters, large-scale PLMs effectively captures knowledge from
    massive labeled and unlabeled data. DAL also ushers in a new paradigm by leveraging
    the prior knowledge in PLMs to enable few-shot or zero-shot learning for many
    downstream tasks. ALPS [[31](#bib.bib31)] extracts knowledge from PLMs to select
    the first batch of data using masked language modeling loss, which successfully
    solves the cold-start problem of DAL. Ein-Dor et al. [[53](#bib.bib53)] use multiple
    DAL methods to select samples for fine-tuning in BERT-based text classification.
    It achieves comparable or higher performance than fine-tuning on full datasets
    only with 10%$\sim$20% labeled samples. Karamcheti et al. [[58](#bib.bib58)] use
    DAL to identify and remove noisy data, select balanced samples to fine-tune PLMs,
    and achieve better performance in visual question-answering. BATL [[32](#bib.bib32)]
    is a task-independent batch acquisition method on a PLMs with triplet loss to
    determine hard samples, which have similar features but difficult to identify
    labels in an unlabeled data pool. TYROGUE [[60](#bib.bib60)] designs an interactive
    DAL framework to flexibly select samples to fine-tune PLMs for multiple low-resource
    tasks. Schroder et al. [[61](#bib.bib61)] extend the PLMs using available unlabeled
    data for greater adaptability and introduce effective fine-tuning for the robustness
    of DAL in low-resource and high-resource settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [II](#S3.T2 "TABLE II ‣ III-C Important DAL Baselines and
    Datasets ‣ III Deep Active Learning ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers"), we also conclude the most widely used datasets in
    DAL including images, text, and audio.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Widely used DAL dataset information.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Size | Domain | Tasks |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MNIST [[6](#bib.bib6)] | 70,000 | Images | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-10 [[63](#bib.bib63)] | 60,000 | Images | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| SVHN [[64](#bib.bib64)] | 600,000 | Images | Classification, Localization
    |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet [[65](#bib.bib65)] | 1.2M | Images | Classification, Detection |'
  prefs: []
  type: TYPE_TB
- en: '| MSCOCO [[66](#bib.bib66)] | 123,287 | Images | Object detection |'
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes [[67](#bib.bib67)] | 5,000 | Images | Semantic segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Caltech-101 [[68](#bib.bib68)] | 9,000 | Images | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| SST [[69](#bib.bib69)] | 11,855 | Text | Sentiment analysis |'
  prefs: []
  type: TYPE_TB
- en: '| TREC [[70](#bib.bib70)] | 5,952 | Text | Question answering |'
  prefs: []
  type: TYPE_TB
- en: '| SNLI [[71](#bib.bib71)] | 570,000 | Text | Natural language inference |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB [[72](#bib.bib72)] | 50,000 | Text | Sentiment analysis |'
  prefs: []
  type: TYPE_TB
- en: '| AGNews [[73](#bib.bib73)] | 31,900 | Text | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| PubMed [[74](#bib.bib74)] | 19,717 | Text | Document classification |'
  prefs: []
  type: TYPE_TB
- en: '| YouTube-8M [[75](#bib.bib75)] | 237,000 | Audio | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| MIMIC-III [[76](#bib.bib76)] | 112,000 | Medical | Healthcare analytic |'
  prefs: []
  type: TYPE_TB
- en: IV Taxonomy of DAL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IV-A Annotation Type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Hard annotations provide one or multiple discrete categorical labels independently
    for each sample. For example, Citovsky et al. [[55](#bib.bib55)] annotate each
    image with a specific label such as “balloon” or “strawberry” for an image classification
    task. Wiechman et al. [[77](#bib.bib77)] design an online annotation system to
    assign multiple labels to long documents based on their sentiments, topics, and
    spam/non-spam status.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Soft annotations allow continuous and subjective labels for samples. For instance,
    ReDAL [[78](#bib.bib78)] annotate continuous 2D region labels for 3D point clouds
    in semantic segmentation. Kothawade et al. [[79](#bib.bib79)] use mutual information
    as an auxiliary metric to select annotation regions in images for autonomous vehicles.
    Xie et al. [[80](#bib.bib80)] propose a region-based approach to automatically
    query a small subset of image regions to label while maximizing segmentation performance.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid annotations combine automatic pseudo-labels of high-confidence predictions
    with human labeling of low-confidence samples in an iterative self-paced manner [[43](#bib.bib43)].
    For example, Wang et al. [[36](#bib.bib36)] propose a complementary sample selection
    strategy to progressively choose the most informative samples, pseudo-labeling
    high-confidence predictions for training. Yu et al. [[81](#bib.bib81)] jointly
    use the expertise of different annotation groups, inter-relations between workers,
    and label correlations within groups. By weighting groups, they reduce the impact
    of low-quality workers and calculate reliable consensus labels.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Explanatory annotations provide a hard or soft label along with an explanation
    for each annotation. For example, Schroder et al. [[82](#bib.bib82)] use topic-related
    annotations for environmental texts. Similarly, Yan et al.[[83](#bib.bib83)] annotate
    the text and list keywords as evidence of the accuracy of the label. Unlike the
    above methods, Zhou et al. [[84](#bib.bib84)] annotate samples by minimizing correlations
    between tasks and provide explainable medical knowledge to distinguish selected
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Random/multi-agent annotations use multiple independent pseudo-annotators to
    randomly label new unlabeled samples without human input [[85](#bib.bib85)]. For
    example, Gong et al. [[86](#bib.bib86)] use an agent team to collaboratively select
    informative images for annotation based on the decisions from the other agents.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Query Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Uncertainty-based methods aim to select the most ambiguous samples according
    to model predictions. Given an input $\bm{x}_{i}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Entropy}(\bm{x}_{i})=\mathop{\arg\max}\limits_{\bm{x}_{i}}(\sum_{j}P(\hat{y}_{j}&#124;\bm{x}_{i})\log
    P(\hat{y}_{j}&#124;\bm{x}_{i})),$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $P(\hat{y}_{i}|\bm{x}_{i})$ represents the likelihood that $\bm{x}_{i}$
    is classified into the $i$-th class [[87](#bib.bib87)]. Uncertainty-based methods
    focus on designing various score functions to measure sample uncertainty and informativeness,
    including predictive entropy [[87](#bib.bib87)], least confidence [[88](#bib.bib88)],
    highest estimated dual variables [[89](#bib.bib89)], mutual information between
    model posterior and predictions [[79](#bib.bib79)]. Some strategies check samples
    near the decision boundary as the most uncertain ones [[90](#bib.bib90)], such
    as instances close to the hyperplane [[44](#bib.bib44)] or close to the margin [[91](#bib.bib91)].
    Others combine multiple query strategies, forming a query-by-committee [[92](#bib.bib92)]
    or disagreement-based [[93](#bib.bib93)] DAL strategy to decrease errors made
    by a single query strategy. With the development of adversarial learning, instead
    of selecting samples from unlabeled datasets, models tend to generate the most
    informative and uncertain synthetic samples to expand the training dataset [[48](#bib.bib48)].
  prefs: []
  type: TYPE_NORMAL
- en: 'However, they have some common drawbacks: (1) redundant samples, as uncertain
    points, are continually selected yet in short of coverage; (2) simply focusing
    on a single sample lacks robustness to outliers; (3) these task-specific designs
    exhibit limited generalizability.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Representative-based methods aim to sample the most prototypical data points
    that effectively cover the distribution of the entire feature space. Existing
    methods can be categorized into density-based and diversity-based approaches.
    Density-based methods prefer to select samples that can represent all unlabeled
    samples. They use clustering methods to select cluster centers [[94](#bib.bib94)]
    as the most representative samples or select samples that can maximize probability
    coverage of the whole feature space of unlabeled datasets [[41](#bib.bib41)].
    For example, Kim et al. [[95](#bib.bib95)] design the density awareness coreset
    approach to estimate sample densities and preferentially select diverse points
    from sparse regions. Given the input $\bm{x}_{i}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Density}(\bm{x}_{i})=\frac{1}{k}\sum_{j\in\mathcal{N}(\bm{x}_{i},k)}\&#124;\bm{x}_{i}-\bm{x}_{j}\&#124;_{2}^{2},$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{N}(\bm{x}_{i},k)$ represents the $k$-nearest neighbors of $\bm{x}_{i}$ [[95](#bib.bib95)].
    Coleman et al. [[96](#bib.bib96)] and Gudovskiy et al. [[97](#bib.bib97)] achieve
    efficiency by only considering nearest neighbors rather than all data or matching
    feature densities with self-supervised methods. Diversity-based methods prefer
    to select samples that are different from the labeled samples. They use context-sensitive
    methods [[98](#bib.bib98)] that take into account the distance between a sample
    and its surrounding labeled samples to enrich the diversity of the labeled dataset.
    BMAL [[99](#bib.bib99)] performs DAL for the image labeling problem, where diversity
    is measured by the KL-divergence of the class probabilities distribution of similar
    neighboring instances, formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\text{Divergence}(\bm{x}_{i},\bm{x}_{j})=\sum_{j}P(\hat{y}_{j}&#124;\bm{x}_{i})-P(\hat{y}_{j}&#124;\bm{x}_{j})\log\frac{P(\hat{y}_{j}&#124;\bm{x}_{i})}{P(\hat{y}_{j}&#124;\bm{x}_{j})}.$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Other diversity-based methods tend to train a model, such as adversarial networks [[57](#bib.bib57)],
    contrastive networks [[100](#bib.bib100)], hierarchical clustering [[44](#bib.bib44)],
    and pre-trained models [[53](#bib.bib53)], to help discriminate labeled and unlabeled
    sets and select the most different unlabeled samples. For example, Li et al. [[101](#bib.bib101)]
    explicitly learn a non-linear embedding to select representative samples. Parvaneh
    et al. [[102](#bib.bib102)] explore neighborhoods around unlabeled data by interpolating
    features with labeled points. Li et al. [[103](#bib.bib103)] propose an acquisition
    function that measures mutual information between a batch of queries to encourage
    diversity. To further increase label efficiency, Citovsky et al. [[55](#bib.bib55)]
    use hierarchical clustering to diversify batches, requiring only 40% of the labels
    to achieve the same target performance. However, since they use ResNet-101 as
    their backbone, which contains only 170 MB parameters, more than 20% labeled samples
    are required for fine-tuning the model.
  prefs: []
  type: TYPE_NORMAL
- en: However, the aforementioned representative-based methods, which solely focus
    on sampling diverse samples, are always insensitive to samples that are close
    to the decision boundary (excluding hybrid methods that jointly consider representative
    and uncertainty), despite the fact that such samples are probably more important
    to the prediction model, as suggested by Zhao et al. [[104](#bib.bib104)]. In
    addition, representative-based methods work well for a small sample of data and
    classifiers with a small number of classes since their computational complexity
    is almost quadratic with respect to data size [[55](#bib.bib55)].
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Influence-based methods aim to select samples that will have the greatest impact
    on the performance of the target model. These techniques can be categorized into
    three main groups. (1) The first group is directly measuring the expected impact
    on the modal through metrics such as gradient norm [[105](#bib.bib105)], query
    complexity [[106](#bib.bib106)], kernel approximation [[107](#bib.bib107)], KL
    divergence [[97](#bib.bib97)], change of loss function [[108](#bib.bib108)], or
    model parameters [[54](#bib.bib54)], and expected error reduction (EER) [[109](#bib.bib109)].
    Specifically, EER can be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\text{EER}(\bm{x}_{i})=\mathbb{E}_{\bm{x}_{s}}\{\mathbb{E}_{y_{i}&#124;\bm{x}_{i}}[\max_{y_{s}}p(y_{s}&#124;\bm{x}_{s},\bm{x}_{i},y_{i})]-\max_{y_{s}}p(y_{s}&#124;\bm{x}_{s})\},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{x}_{s}$ refers to the labeled sample. (2) The second group is incorporating
    different learning policies, such as reinforcement learning and imitation learning,
    to select samples based on reward signals or demonstrated actions. Despite the
    promising advantages, this requires significant additional training [[110](#bib.bib110)].
    For example, Wertz et al. [[111](#bib.bib111)] propose reinforced DAL, a reinforcement
    learning policy that uses multiple elements of the data and the task to dynamically
    pick the most useful unlabeled subset during the DAL process; (3) The last group
    is training a separate model to estimate the impact on the target model [[89](#bib.bib89)].
    For example, Peng et al. [[14](#bib.bib14)] propose a knowledge distillation framework
    to evaluate the impact of samples based on the knowledge learned by the student
    model. Elenter et al. [[89](#bib.bib89)] use the dual variables of the original
    model to measure the impact on the target model.
  prefs: []
  type: TYPE_NORMAL
- en: However, despite recent advances, influence-based DAL remains challenging. Directly
    measuring model changes or incorporating new learning policies always requires
    huge time and space costs, and training a new model will over-rely on its accuracy
    and often lead to unstable results.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian methods aim to minimize classification errors and improve model beliefs
    by leveraging Bayes’ rule. Most studies have treated Bayesian models (e.g., Gaussian
    process [[109](#bib.bib109)], BNNs [[35](#bib.bib35)], Bayesian probabilistic
    ensemble [[112](#bib.bib112)]) as uncertainty-based methods, using them to estimate
    the informativeness of the sample. However, Bayesian DAL is better viewed as its
    own distinct system, with methods that select batches by directly measuring impact
    on the target model, such as BatchBALD [[45](#bib.bib45)] and Causal-BALD [[113](#bib.bib113)].
    For example, we define a Bayesian model with model parameters $\bm{w}\sim p(\bm{w}|\mathcal{D}_{\text{train}})$,
    and BALD can be defined to estimate the mutual information between the model predictions
    and the model parameters, formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\mathbb{I}(y;\bm{w}&#124;\bm{x},\mathcal{D}_{\text{train}})=\mathbb{H}(y&#124;\bm{x},\mathcal{D}_{\text{train}})-\mathbb{E}_{p(\bm{w}&#124;\mathcal{D}_{\text{train}})}[\mathbb{H}(y&#124;\bm{x},\bm{w},\mathcal{D}_{\text{train}})],$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{H}$ represents the entropy and $\mathbb{E}$ is the expectation.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to standard DNNs, the aforementioned Bayesian DAL methods, which leverage
    the advantages of probabilistic graphical theory [[35](#bib.bib35)], can often
    provide reasonable explanations for why these samples should be selected [[45](#bib.bib45)].
    However, they often require extensive accurate prior knowledge and tend to underperform
    deep learning models in representation learning and fitting capacity.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid methods aim to take advantage of the above multiple query strategies
    and to achieve a trade-off among them. Hybrid methods can be further categorized
    according to interaction patterns. Serial-form hybrids apply criteria sequentially
    within an DAL cycle, filtering out non-informative samples until the batch is
    filled [[55](#bib.bib55)]. Criteria-selection hybrids use only one query strategy
    in one DAL iteration, in which they select the best query strategy or network
    architecture with the highest criterion. For example, DUAL [[114](#bib.bib114)]
    switches between density-based and uncertainty-based selectors to choose the best
    criterion for each DAL cycle. Unlike DUAL, iNAS [[115](#bib.bib115)] searches
    a restricted candidate set to find the optimal model architecture incrementally
    in each DAL iteration. Parallel-form hybrids use multi-objective optimization
    methods or a weighted sum to merge multiple query criteria into one for sample
    selection. For example, Gu et al. [[2](#bib.bib2)] efficiently acquire batches
    with discriminative and representative samples by proposing procedures to update
    labeled and unlabeled sets, based on path-following optimization techniques. Citovsky
    et al. [[55](#bib.bib55)] jointly optimize the uncertainty and diversity criteria
    in batch mode using multi-objective acquisition functions. TOD [[54](#bib.bib54)]
    selects samples with high model uncertainty and outputs discrepancy through a
    weighted combination of both metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid methods combine the advantages of different query strategies. However,
    determining the most effective combinations and trade-offs between criteria is
    time consuming and still remains open for further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Model Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Traditional Machine Learning architectures, such as Forest [[39](#bib.bib39)]
    and Support Vector Machine (SVM) [[44](#bib.bib44)], are statistical-based models
    that do not use neural networks. And they attract great attention in the early
    stage of the DAL development.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Neural Networks (BNNs) combine neural networks with Bayesian inference,
    quantifying the uncertainty introduced by the models in terms of outputs and weights
    to explain the trustworthiness of the prediction [[116](#bib.bib116)]. Many studies
    propose DAL strategies based on BNNs, aiming to improve efficiency and explainability
    in samples selection [[38](#bib.bib38), [45](#bib.bib45)].
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs) [[117](#bib.bib117)] use their reasoning from
    previous experiences to predict upcoming events and are able to learn features
    with long-term dependencies. They have been widely used for sequential data such
    as text and audio. DAL is seldom combined with RNNs since they require large-scale
    labeled datasets for training. Some special tasks that easily recognizable patterns,
    such as malicious word detection on social networks [[118](#bib.bib118)], can
    be solved with DAL.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional Neural Networks (CNNs) [[6](#bib.bib6)] are feedforward neural
    networks that can extract features from data with convolution structures and have
    been widely used for image processing with three advantages: local connections,
    weight sharing, and down-sampling dimensionality reduction. DAL can be effectively
    combined with CNNs since Sener et al. [[41](#bib.bib41)] proved that a subset
    of samples (coreset) can geometrically characterize all features of the entire
    image set and can be selected by minimizing a rigorous bound. Following their
    study, more studies have been conducted [[49](#bib.bib49), [55](#bib.bib55)].'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Graph Neural Networks (GNNs) [[5](#bib.bib5)] learn node representations by
    aggregating neighborhood information and achieve great success in various tasks,
    such as node classification. However, effectively handling graph data with dense
    interconnections between samples using limited labeled data remains an open challenge [[119](#bib.bib119)].
    DAL can help address this by selectively querying labels for the most informative
    samples and executing only one training epoch to reduce the annotation cost for
    various types of graphs, such as homogeneous graphs [[120](#bib.bib120)], heterogeneous
    graphs [[121](#bib.bib121)] and attribute graphs [[122](#bib.bib122)].
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoders (VAEs) is a class of neural network architecture designed
    with an encoder-decoder framework [[123](#bib.bib123)]. It aims to capture the
    underlying data distribution and learn to generate samples that closely resemble
    the input data. VAEs-based DAL methods usually generate samples to fool discriminators
    in an adversarial training manner, thus improving discriminators’ ability to select
    the most challenging-to-distinguish samples for training DAL models [[49](#bib.bib49),
    [57](#bib.bib57)].
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Trained Language Models (PLMs), based on Transformers, utilize multi-head
    self-attention to capture long-term dependencies. By pre-training on large unlabeled
    corpora, PLMs embed substantial general knowledge and transfer to downstream tasks,
    enabling state-of-the-art (SOTA) performance [[30](#bib.bib30)]. For example,
    Seo et al. [[32](#bib.bib32)] identify the most informative samples for a given
    task, focusing on PLMs fine-tuning, to learn salient patterns with minimal annotation
    cost. The combination of pre-training rich knowledge foundation and DAL’s sample-efficient
    tuning unlocks PLMs ’s further potential for many applications.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Learning Paradigm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional Learning Paradigm, as illustrated in Algorithm [1](#alg1 "Algorithm
    1 ‣ III Deep Active Learning ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers"), iteratively queries and labels samples to train the models
    in a vanilla supervised learning manner, without incorporating any advanced learning
    paradigms [[34](#bib.bib34), [32](#bib.bib32)].'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised Learning, also known as weakly-supervised learning, aims to
    jointly use real-labeled samples and pseudo-labeled samples to train the models.
    Current DAL methods are designed with various efficient strategies to obtain pseudo-labels
    for unlabeled samples. For instance, DBAL [[35](#bib.bib35)] and CoreSet [[41](#bib.bib41)]
    first predict pseudo-labels using their models and then calculate samples’ confidence
    scores to judge whether these pseudo-labels should be trusted or not. On the other
    hand, LADA [[56](#bib.bib56)] and BGADL [[48](#bib.bib48)] propose new data augmentation
    methods to create more samples based on original labeled samples, using their
    original real-labeled samples as pseudo-labels. These studies effectively reduce
    human-labors and achieve comparable performance compared with traditional supervised
    learning using larger labeled samples.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrastive Learning improves feature representation by pulling similar instances
    closer together while pushing dissimilar instances apart [[124](#bib.bib124)].
    Contrastive methods extract discriminative features, such as semantics [[100](#bib.bib100)]
    and distinctiveness [[57](#bib.bib57)], to estimate the sample uncertainty during
    acquisition. For example, as shown in Fig. [5](#S4.F5 "Figure 5 ‣ IV-D Learning
    Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers"), Du et al. [[125](#bib.bib125)] extract both semantic and
    distinctive features with contrastive learning and then combine them in a query
    strategy to choose the most informative unlabeled samples with matched categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fee57d12174b6a89cec10254ad6984c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An example for contrastive learning based query strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversarial Learning enables a model to train fully differentiable by solving
    minimax optimization problems [[49](#bib.bib49)]. This approach can be used as
    a generative query technique for DAL. For example, DAL can be combined with generative
    adversarial network, which consist of a generator and a discriminator, where the
    DAL model acts as the discriminator and the generator explores the distribution
    of unlabeled data to generate the most informative and uncertain synthetic samples
    for training [[57](#bib.bib57)]. Li et al. [[122](#bib.bib122)] propose SEAL,
    as shown in Fig. [6](#S4.F6 "Figure 6 ‣ IV-D Learning Paradigm ‣ IV Taxonomy of
    DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers") which
    consists of two adversarial components. The graph embedding network encodes all
    nodes into a shared space, with the intention of making the discriminator treat
    all nodes as labeled. Additionally, a semi-supervised discriminator is used to
    differentiate unlabeled nodes from labeled ones. The divergence score of the discriminator
    is used as an informativeness measure to actively select the most informative
    node for labeling. The two components form a loop to mutually improve DAL.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e7aa6b8b48493cfb226739f7a7d9804d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An example for contrastive learning based query strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Meta Learning enables DNNs to leverage the knowledge acquired from multiple
    tasks, represented in the network with their weights, to adapt faster to new tasks.
    Meta learning can provide an acquisition function for DAL [[39](#bib.bib39), [126](#bib.bib126)]
    or favorable model initialization during DAL by controlling the transfer of knowledge
    from multiple source tasks. For example, Shao et al. [[127](#bib.bib127)] propose
    Learning-to-Sample, where a boosting model and sampling model dynamically learn
    from each other and iteratively improve performance. Zhu et al. [[59](#bib.bib59)]
    combine both paradigms by initializing an active learner with meta-learned parameters
    via meta-training on tasks similar to the target task.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning involves an agent that can interact with its environment
    and learn to alter its behavior in response to received rewards [[119](#bib.bib119)].
    Given that almost all DAL methods use heuristic acquisition functions with limited
    effectiveness, Reinforcement learning frames DAL as a reinforcement learning problem
    to explicitly optimize an acquisition policy. In the DAL with reinforcement learning
    setup, an autonomous agent (acquisition selector) controlled by a deep learning
    algorithm that observes a state $s_{t}$ from its environment (predictor) at time
    $t$. It takes an action $a_{t}$ to maximize the reward $r_{t}$ (prediction accuracy),
    where $a_{t}$ decides whether to query unlabeled samples [[62](#bib.bib62)].
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Curriculum Learning mimic human and animal learning processes, where the training
    progresses gradually from simple to complex samples. This provides a natural way
    to exploit labeled data for robust learning [[10](#bib.bib10), [128](#bib.bib128)].
    Specifically, curriculum learning uses a predefined learning constraint to incrementally
    incorporate additional labeled samples during training. Curriculum Learning introduces
    a weighted loss on all labeled samples, acting as a general regularizer over the
    sample weights. For example, Wang et al. [[129](#bib.bib129)] use a pseudo-labels
    strategy which iteratively assigns pseudo-labels to unlabeled samples with high
    prediction confidence.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Continual Learning is developed for constraints on task-based settings, where
    the model continuously learns a sequence of tasks one at a time, where all data
    for the current task are labeled and available in increments. However, real-world
    systems do not have the luxury of large labeled datasets for each new task. To
    address this issue, Mundt et al. [[130](#bib.bib130)] present a detailed analysis
    of continual learning-based DAL and out-of-distribution detection works. They
    suggest a unified perspective with open-set recognition as a natural interface
    between continual learning and DAL. Ayub et al. [[30](#bib.bib30)] develop a method
    that allows a continue learning agent to continually learn new object classes
    from a few labeled examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7282bff6a6c4f9dba102afbb6b725718.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An example for transfer learning based query strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Transfer Learning extracts knowledge from one or more source tasks and applies
    it to a target task. It has two broad categories: transductive and inductive.
    While transductive methods adapt models learned from a labeled source domain to
    a different unlabeled target domain with the same task, inductive methods ensure
    that the domains of source and target are the same but tasks are different. DAL
    with transfer learning can better enhance each other’s performance by selecting
    the best target samples with a distribution similar to the source domain [[50](#bib.bib50)].
    In addition, transfer learning can minimize the number of annotation labels needed
    and provide auxiliary information for DAL acquisition functions. For example,
    as shown in Fig. [7](#S4.F7 "Figure 7 ‣ IV-D Learning Paradigm ‣ IV Taxonomy of
    DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"), Xie
    et al. [[87](#bib.bib87)] propose an energy-based active domain adaptation that
    balances domain representation and uncertainty when selecting target data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d3ea36d6f9539651b6f7ca0e2e1ca2f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: An example for imitation learning [[131](#bib.bib131)].'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Imitation Learning provides SOTA results in many structured prediction tasks
    by learning near-optimal search policies [[92](#bib.bib92)]. Such methods assume
    access to an expert during training that can provide the optimal action in any
    queried state, essentially asking “what would you do here?” and learning to mimic
    that choice. For example, Bullard et al. [[132](#bib.bib132)] use imitation learning
    to allow an agent in a constrained environment to concurrently reason about both
    its internal learning goals and externally impose environmental constraints within
    its objective function. Löffler et al. [[131](#bib.bib131)] propose an imitation
    learning scheme (IALE) that mimics the selection of the best-performing expert
    heuristic at each stage of the learning cycle in a batch-mode setting. As shown
    in Fig. [8](#S4.F8 "Figure 8 ‣ IV-D Learning Paradigm ‣ IV Taxonomy of DAL ‣ A
    Survey on Deep Active Learning: Recent Advances and New Frontiers"), IALE can
    well imitate the Entropy-based and CoreSet-based methods and thus obtain better
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Multi-task Learning (MTL) focuses on formulating methods to maintain performance
    across multiple tasks rather than a single task. Multi-task DAL (MTAL) methods
    combine multiple individual task-related query strategies into a single unified
    approach and jointly optimize the unified one. In contrast to single-task query
    settings, where the uncertainty of a single selected task classifier is used to
    query unlabeled samples, in MTAL the uncertainty of an instance is determined
    by the uncertainties from classifiers across all tasks. For example, Ikhwantri
    et al. [[133](#bib.bib133)] propose an MTAL framework for semantic role labeling
    with entity recognition as an auxiliary task. This alleviated data needs and leverages
    entity information to aid role labeling. Their experiments show that MTAL can
    outperform single-task DAL and standard MTL, using 12% less training data than
    passive learning. Zhou et al. [[84](#bib.bib84)] propose a Multi-Task Adversarial
    DAL framework, where adversarial learning maintains the effectiveness of the MTL
    and DAL modules. A task discriminator eliminates irregular task-specific features,
    while a diversity discriminator exploits heterogeneity between samples to satisfy
    diversity constraints.
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Training Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Traditional Training first trains a model on an initialized training dataset
    and then selects unlabeled samples to annotate based on the predictions of the
    current model. The newly annotated samples are added to the training set for re-training
    the model in the next iteration [[134](#bib.bib134)]. This iterative process continues,
    with the model parameters randomly re-initialized before each epoch of re-training [[36](#bib.bib36)],
    until either the sample budget or number of DAL iterations is reached.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Curriculum Learning Training gradually progresses from easy to complex samples,
    mimicking human and animal learning processes. This provides a natural and iterative
    way to exploit labeled data for robust learning. For example, Tang et al. [[135](#bib.bib135)]
    propose a self-paced DAL approach that jointly considers the value and difficulty
    of a sample. It queries samples from easy to hard to minimize annotation cost.
    Wang et al. [[43](#bib.bib43)] show that curriculum learning alone improves the
    accuracy of the object detection by 3.6%, while the combination of curriculum
    learning and DAL improve the accuracy by 4.3%.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training & Fine-tuning (Pre+FT) have become a primary training process with
    the development of large-scale PLMs [[58](#bib.bib58)]. It leverages the rich
    prior knowledge in PLMs to solve different downstream tasks. DAL attracts attention
    as a sample selection strategy for fine-tuning with only 10%$\sim$20% of labeled
    data achieving competitive performance compared to full data fine-tuning [[32](#bib.bib32)].
    DAL iteratively selects and annotates batches of informative samples to fine-tune
    the PLMs for the downstream task. This satisfies task-specific needs, while also
    enabling a few-shot learning [[30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Illustration of DAL-related applications in main fields, including
    classic methods with their advantages and disadvantages.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Areas | Applications | Classic Methods | Advantages | Disadvantages |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| NLP | Text Classification | generate samples for training [[83](#bib.bib83),
    [136](#bib.bib136)]. | make the selection process efficient. | high time consumption,
    unstable performance. |'
  prefs: []
  type: TYPE_TB
- en: '| uncertainty sampling [[61](#bib.bib61)]. | high efficiency and performance.
    | vulnerable to outliers, unstable performance. |'
  prefs: []
  type: TYPE_TB
- en: '| use pre-trained language models [[137](#bib.bib137)]. | easily adapt to new
    datasets. | vulnerable to outliers and imbalanced datasets. |'
  prefs: []
  type: TYPE_TB
- en: '| Text Summarization | PLMs with Monte Carlo dropout [[138](#bib.bib138)].
    | efficient and effectiveness. | vulnerable to outliers, unstable performance.
    |'
  prefs: []
  type: TYPE_TB
- en: '| diverse sampling [[139](#bib.bib139)]. | remove outliers and diverse sampling.
    | vulnerable to document embeddings. |'
  prefs: []
  type: TYPE_TB
- en: '| Question Answering | DataMap [[58](#bib.bib58)]. | eliminate outliers and
    improve accuracy. | high time consumption, lack of generalizability. |'
  prefs: []
  type: TYPE_TB
- en: '| interactive query strategy [[140](#bib.bib140)]. | efficiently minimize costly
    data annotations. | wait for human reaction, need expert knowledge. |'
  prefs: []
  type: TYPE_TB
- en: '| Information Extraction | label identical subsequences [[141](#bib.bib141)]
    | high efficiency and effectiveness. | lack of generalizability, cold-start. |'
  prefs: []
  type: TYPE_TB
- en: '| label most novel words [[142](#bib.bib142)]. | high efficiency and effectiveness.
    | unstable performance, cold-start. |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic Parsing | hyperparameter selection [[143](#bib.bib143)]. | reduce
    data annotation. | high time consumption, lack of generalizability. |'
  prefs: []
  type: TYPE_TB
- en: '| hybrid query strategies [[144](#bib.bib144)]. | select the most semantically
    varied samples. | vulnerable to outliers, lack of scalability. |'
  prefs: []
  type: TYPE_TB
- en: '| CV | Image Captioning | semantic adversarial DAL [[145](#bib.bib145)] | overcome
    scarcity of labeled data. | difficulty in cross-domain transfer, cold-start. |'
  prefs: []
  type: TYPE_TB
- en: '| domain transfer learning [[146](#bib.bib146)]. | transfer knowledge from
    high-resource. | vulnerable to outliers, data scarcity. |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic Segmentation | uncertainty-based DAL [[147](#bib.bib147)]. | high
    efficiency and effectiveness. | unstable performance, easily select outliers.
    |'
  prefs: []
  type: TYPE_TB
- en: '| region-based selection [[148](#bib.bib148), [80](#bib.bib80)]. | balance
    between label efforts and effect. | vulnerable to outliers, imbalance datasets.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection | hybrid selection [[149](#bib.bib149), [43](#bib.bib43)].
    | avoid noisy samples and outliers. | data scaricity, unstable performance. |'
  prefs: []
  type: TYPE_TB
- en: '| instance uncertainty learning [[150](#bib.bib150)]. | suppress noisy instances.
    | unstable performance, lack of scalability. |'
  prefs: []
  type: TYPE_TB
- en: '| Pose Estimation | traditional DAL strategy [[151](#bib.bib151), [152](#bib.bib152)].
    | effectiveness, easy to apply. | vulnerable to outliers, cold-start. |'
  prefs: []
  type: TYPE_TB
- en: '| meta learning [[86](#bib.bib86)]. | can learn an optimal sampling policy.
    | vulnerable to outliers and imbalance datasets. |'
  prefs: []
  type: TYPE_TB
- en: '| Target Tracking | multi-frame collaboration [[153](#bib.bib153)] | eliminate
    background noise, ensure diversity. | unstable performance, lack of scalability.
    |'
  prefs: []
  type: TYPE_TB
- en: '| multi-target object tracking [[154](#bib.bib154)]. | high efficient and effectiveness.
    | high time consumption, cold-start |'
  prefs: []
  type: TYPE_TB
- en: '| Person Re-identification | human-in-the-loop [[46](#bib.bib46)]. | improve
    model performance. | high time consumption, lack of generalizability. |'
  prefs: []
  type: TYPE_TB
- en: '| incremental annotation [[155](#bib.bib155)]. | select diverse samples without
    redundancy. | vulnerable to outliers, cold-start. |'
  prefs: []
  type: TYPE_TB
- en: '| DM | Node Classification | semi-supervised adversatial DAL [[122](#bib.bib122)].
    | better performance gains. | unstable performance, cold-start. |'
  prefs: []
  type: TYPE_TB
- en: '| graph policy network [[120](#bib.bib120)]. | stable performance. | single
    sample selection costs much time. |'
  prefs: []
  type: TYPE_TB
- en: '| Link Prediction | multi-view DAL [[156](#bib.bib156)]. | query informative
    samples from multi-view. | lack of scalability and generalizability. |'
  prefs: []
  type: TYPE_TB
- en: '| transfer learning DAL [[157](#bib.bib157)]. | easily apply to new datasets.
    | unstable performance, cold-start. |'
  prefs: []
  type: TYPE_TB
- en: '| Community Detection | topic-based [[158](#bib.bib158)]. | reducing the unreliable
    dataset. | high time consumption, unstable performance. |'
  prefs: []
  type: TYPE_TB
- en: '| geometric block model [[159](#bib.bib159)]. | efficient and effectiveness.
    | unstable performance, cold-start. |'
  prefs: []
  type: TYPE_TB
- en: V Applications of DAL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As shown in Table [III](#S4.T3 "TABLE III ‣ IV-E Training Process ‣ IV Taxonomy
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"),
    the integration of DL and AL is leading to an increasing application of AL methods
    in various domains of life, ranging from agricultural development [[82](#bib.bib82)]
    to industrial revitalization [[82](#bib.bib82)], and from artificial intelligence [[137](#bib.bib137)]
    to biomedical fields [[160](#bib.bib160)]. In this section, we aim to provide
    a systematic and detailed overview of existing DAL-related work from a broad application
    perspective.'
  prefs: []
  type: TYPE_NORMAL
- en: V-A Applications in Natural Language Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the emergence of large-scale language models, NLP has achieved great success
    using computers to help understand intricate languages. However, fine-tuning these
    language models requires a substantial amount of data, computation resources,
    and time. DAL provides a strategy for searching high-quality small and high-quality
    samples to help fine-tune the model and save resources. In the following, we introduce
    some of the most influential DAL methods in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Text Classification aims to classify large-scale text with particular labels
    such as topic or sentiment. Researchers propose several methods to efficiently
    select informative samples for training. For example, Yan et al. [[83](#bib.bib83)]
    generate the most informative examples for training, efficiently skipping the
    sample selection process. They approximate the generated example with a few summary
    words, which significantly reduces the labeling cost for annotators, as they only
    need to read a few words instead of a long document. Tan et al. [[136](#bib.bib136)]
    develop the Bayesian estimate of mean proper scores (BEMPS) framework for DAL,
    which allows the calculation of scores such as logarithmic probability to better
    help select informative and uncertainty samples. Experiments demonstrate that
    BEMPS is more effective than baselines in various text classification datasets.
    On the other hand, Schroder et al. [[61](#bib.bib61)] use transformers for uncertainty-based
    sample selection. Interestingly, they achieve comparable performance in widely
    used text classification datasets while training in less than 20% of the labeled
    data, which demonstrates their ability to utilize limited labeled data. In another
    study, Jelenic et al. [[137](#bib.bib137)] conduct an initial empirical study
    to investigate the transferability of the DAL by using PLMs . They find DAL can
    effectively adapt to new datasets with pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Abstractive Text Summarization (ATS) aims to compress a document into a brief,
    informative and readable summary that retains the key information of the original
    document. However, constructing human-annotated datasets is a time-consuming and
    costly endeavor. DAL are explored to reduce the amount of annotation needed while
    achieving a certain level of ATS performance. For example, Gidiotis et al. [[138](#bib.bib138)]
    address the issue from a Bayesian view and study uncertainty estimation for SOTA
    text summarization models. They augment the pre-trained summarization models with
    Monte Carlo dropout, forming the corresponding variational Bayesian PLMs models.
    By generating multiple summaries from these models, they approximate Bayesian
    inference and estimate the summarization uncertainty. Experiments on multiple
    benchmark datasets consistently demonstrate their improved summarization performance
    with higher Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores.
    Unlike the above method, as Fig. [9](#S5.F9 "Figure 9 ‣ V-A Applications in Natural
    Language Processing ‣ V Applications of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers") (a) shows, Tsvigun et al. [[139](#bib.bib139)]
    propose an alternative query strategy for ATS based on diversity principles. This
    strategy, known as in-domain diversity sampling, involves selecting instances
    that are dissimilar from annotated documents, but similar to the core documents
    of the domain. Given limited annotation budget, they can improve model performance
    and consistency scores.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a0b1867758bd079636d8088a56907083.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: An example for samples selection of ATS and Datamap.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Question Answering involves answering questions about images or passages of
    text [[161](#bib.bib161)]. However, current models require large-scale training
    data to achieve high performance. DAL methods, such as Datamap [[58](#bib.bib58)]
    and hierarchical dialog policies [[140](#bib.bib140)], are designed to maximize
    performance with minimal labeling effort. Specifically, in Fig. [9](#S5.F9 "Figure
    9 ‣ V-A Applications in Natural Language Processing ‣ V Applications of DAL ‣
    A Survey on Deep Active Learning: Recent Advances and New Frontiers") (b), DataMap [[58](#bib.bib58)]
    is able to detect and eliminate outlier examples from the unlabeled set, resulting
    in a significant increase in model accuracy with fewer labeled examples. Padmakumar
    et al. [[140](#bib.bib140)] develop a joint policy for clarification and DAL in
    an interactive image retrieval task. Asking users for clarification while querying
    new examples improves the model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Information Extraction refers to many NLP tasks, including named entity recognition,
    keyword extraction, word segmentation, etc. Manual annotation of large-scale sequences
    is time consuming, expensive, and thus difficult to realize. To address this,
    Brantley et al. [[92](#bib.bib92)] design a new DAL annotation manner. They use
    a noisy heuristic labeling function to provide initial low-quality labels, train
    a classifier to decide whether to trust these labels, and annotate the most uncertain
    samples with trustable labels. Their model achieves high efficiency and effectiveness
    on many information extraction tasks. Similarly, Radmard et al. [[141](#bib.bib141)]
    focus on improving the efficiency of DAL for naming entity recognition by querying
    subsequences within each sentence and propagating labels to unseen identical subsequences
    in the dataset. They demonstrate that the DAL strategy requires only 20% of the
    dataset to achieve the same results as training on the full dataset. Hua et al. [[142](#bib.bib142)]
    propose two model-independent acquisition strategies for identifying and understanding
    the structure of argumentative discourse, achieving competitive results with fewer
    computations The former selects samples with the most novel words for labeling,
    while the latter seeks to identify more relation links by matching any of the
    18 prominent discourse markers from a manual.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic Parsing aims to convert a natural language utterance to a logical
    form: a machine-understandable representation of its meaning [[162](#bib.bib162)].
    DAL can help reduce data requirements and improve efficiency for semantic parsing.
    For example, Duong et al. [[143](#bib.bib143)] design a simple hyperparameter
    selection technique for DAL to accelerate data annotation. Experiments show that
    their method significantly reduces the need for data annotation and improves the
    model’s performance on semantic parsing. Li et al. [[163](#bib.bib163)] also design
    a hyperparameter tuning module to reduce the additional annotation cost. In addition,
    they design a novel query strategy that prioritizes examples with various logical
    form structures and more lexical choices, which further improve the performance
    for semantic parsing. Cohen et al. [[144](#bib.bib144)] propose a novel DAL method
    with two new annotation manners, called HAT. Experiments show that HAT can pick
    out the most semantically varied and illustrative utterances, leading to the highest
    possible gains in parser performance.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B Applications in Computer Vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the remarkable success of CNNs and Vision Transformers, a valuable insight
    has been gained that more labeled image datasets can promote to obtain better
    performance of the task. However, as the amount of data increases, training DNNs
    becomes time and resource consuming. Additionally, even if the number of data
    increases, the presence of noise often leads to limited performance improvement.
    DAL can effectively reduce noise and time consumption in many CV tasks. Hereafter,
    we provide detailed information on specific tasks and their improvements achieved
    with DAL in CV.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Classification aims to accurately classify images based on the provided
    labels for many specific fields such as remote sensing [[16](#bib.bib16)], medical
    imaging [[164](#bib.bib164)] and face recognition [[129](#bib.bib129)]. We list
    the most successful DAL methods for image classification in Section [III-C](#S3.SS3
    "III-C Important DAL Baselines and Datasets ‣ III Deep Active Learning ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers"), such as BCBA, DBAL
    and CEAL, which can be referred to for more detailed information.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Image Captioning aims to automatically generate descriptive text about the content
    of an image. Achieving high-quality captioning requires large-scale datasets with
    diverse images. Unfortunately, creating such a dataset is time-consuming and costly.
    To tackle this issue, Zhang et al. [[145](#bib.bib145)] devise a novel adversarial
    DAL model, which uses visual and textual information to select the most representative
    samples to optimize the performance of image captioning. Experiments show that
    they overcome the limitations of labeled data scarcity and improve the practicality
    and effectiveness of image captioning. In a similar vein, Cheikh et al. [[146](#bib.bib146)]
    introduce a knowledge-transferable DAL framework for low-resorce datasets. They
    take advantage of existing datasets, translating their captions into Arabic, and
    train the model with translated caption datasets as prior knowledge for low-resource
    ArabicFlickr1K datasets (which contain only 1,095 images). Their model achieves
    the Bilingual Evaluation Understudy (BLEU) score of 47%, serving as compelling
    evidence for the effectiveness of their approach.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Segmentation aims to understand images at the pixel level, serving
    as the basis for various applications, including autonomous driving [[80](#bib.bib80)]
    and robot manipulation [[30](#bib.bib30)]. However, training segmentation models
    requires an extensive amount of data with pixel-wise annotations, a process that
    is burdensome and prohibitively expensive [[78](#bib.bib78)]. To solve this challenge,
    Konyushkova et al. [[147](#bib.bib147)] propose an uncertainty-based DAL method
    with geometric priors to expedite and simplify the annotation process for image
    segmentation. Experiments show that their method can be applied to both background-foreground
    and multi-class segmentation tasks. Qiao et al. [[148](#bib.bib148)] introduce
    a collaborative panoptic regional DAL framework for partial annotated semantic
    segmentation. By incorporating semantic-agnostic panoptic matching and region-based
    selection and extension, their model strikes a balance between labeling efforts
    and performance. Similarly, Xie et al. [[80](#bib.bib80)] propose an automated
    region-based DAL approach for semantic segmentation considering the spatial adjacency
    of image regions and the confidence in prediction. Experiments show that they
    can use a small number of labeled image regions while maximizing segmentation
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Object Detection is transformed into a region classification task by generating
    candidate regions of objects from the input image. Features are typically extracted
    from candidate object regions using CNNs and classifiers are subsequently employed
    for the final detection. DAL can reduce labeled data to better fit numerous parameters
    of CNN. Wu et al. [[149](#bib.bib149)] propose a novel hybrid query strategy that
    jointly considers uncertainty and diversity. Extensive experiments are conducted
    on two object detection datasets that effectively demonstrate the superiority
    and effectiveness of their model. Wang et al. [[43](#bib.bib43)] introduce active
    sample mining with switchable selection criteria to incrementally train robust
    object detectors using unlabeled or partially labeled samples, avoiding the influence
    of noisy samples and outliers. The effectiveness of the model is demonstrated
    through extensive experiments on publicly available object detection benchmarks.
    Yuan et al. [[150](#bib.bib150)] define an instance uncertainty learning module
    that takes advantage of the discrepancy of two adversarial instance classifiers
    trained in the labeled set to predict the instance uncertainty of the unlabeled
    set. With iterative instance uncertainty learning and re-weighting, they suppress
    noisy instances, bridging the gap between instance and image-level uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Pose Estimation aims to localize the positions of specific key points in images,
    which has a wide range of applications, such as augmented reality, translation
    of sign language, and human-robot interaction. Obtaining pose annotations can
    be extremely expensive and laborious. To address this issue, Caramalau et al.[[151](#bib.bib151)]
    propose distribution-based methods for the selection of diverse and representative
    samples. Experiments demonstrate their high efficiency and effectiveness for pose
    estimation. Similarly, Shukla et al.[[152](#bib.bib152)] use an uncertainty-based
    query strategy and annotate samples with the lowest confidence scores and further
    improve the performance with fewer labeled samples. Gong et al. [[86](#bib.bib86)]
    design a novel meta agent teaming DAL (MATAL) framework to actively select and
    label informative images for effective learning. MATAL formulates the sample selection
    procedure as a Markov Decision Process and learns an optimal sampling policy that
    effectively maximizes the performance of the pose estimator.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Target Tracking aims to accurately track targets in images, which can be applied
    for numerous applications, including video surveillance, autonomous vehicles,
    etc. Using DAL can better help train neural networks with limited labeled samples
    for target tracking. Yuan et al. [[153](#bib.bib153)] present a new DAL sequence
    selection method in a multi-frame collaboration way for target tracking. To ensure
    the diversity of selected sequences, they measure samples’ similarity by their
    temporal relation between multiple frames in each video, and they use a nearest
    neighbor discriminator to select the representative samples. Experiments show
    that their method can eliminate background noise and improve efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Person Re-identification (Re-ID) aims to match a specific pedestrian using different
    cameras, which is an essential task for public security. Previous efforts mainly
    concentrate on enhancing the performance of Re-ID models, relying on large labeled
    datasets. However, these efforts often overlook data redundancy issues that can
    arise in constructing Re-ID datasets. To address data redundancy in Re-ID datasets,
    Liu et al. [[46](#bib.bib46)] propose an alternative human-in-the-loop model based
    on reinforce learning. In their method, a human annotator provides binary feedback
    to fine-tune a pre-trained CNNs Re-ID model. Extensive experiments prove the superiority
    of their method compared to existing unsupervised, transfer learning, and DAL
    models. On the other hand, Xu et al. [[155](#bib.bib155)] focus on learning from
    scratch with incremental labeling through human annotators and model feedback.
    They combine DAL with an incremental annotation process to select informative
    and diverse samples without redundancy from an unlabeled set in each iteration.
    These samples are then labeled by human annotators to further improve the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e0075b3d81eed3c098921a691985b262.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The framework of Graph Policy Network [[120](#bib.bib120)].'
  prefs: []
  type: TYPE_NORMAL
- en: V-C Applications in Graph Data Mining and Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are substantial increase in content-rich network from various domains,
    such as social networks, citation networks, and financial networks. Graphs have
    emerged as a powerful tool for representing and discovering knowledge, with nodes
    representing instances characterized by rich content features and edges denoting
    relationships or interactions between nodes.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Node Classification is to predict the labels of unlabeled nodes in a partially
    labeled network. GNNs rely heavily on a sufficient number of labeled nodes, which
    is costly and time-consuming. To address this problem, many graph-based DAL methods
    are proposed. For example, ICA-based methods [[165](#bib.bib165)] leverage label
    dependence among neighboring nodes to select diverse samples for node classification,
    while AGE [[166](#bib.bib166)] and ANRMAB [[167](#bib.bib167)] integrate GCNs
    with three traditional DAL query strategies and achieve good performance on many
    node classification datasets. As Fig. [10](#S5.F10 "Figure 10 ‣ V-B Applications
    in Computer Vision ‣ V Applications of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers") shows, Hu et al. [[120](#bib.bib120)] present
    a graph policy network for transferable DAL on graphs, which formalizes DAL on
    graphs as a Markov decision process and learns the optimal query strategy with
    reinforce learning. The state is defined based on the current graph status, and
    the action is to select a node for annotation at each query step. The reward is
    defined as the performance gain of the GNNs trained with the selected nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Link Prediction aims to predict missing or potential links between nodes in
    a given network. It involves using existing connections or relationships to infer
    the likelihood of forming new connections. In the context of link prediction,
    the challenge arises from the limited availability of existing link information
    between nodes in a network. DAL can help alleviate this issue, for example, DALAUP [[168](#bib.bib168)]
    uses neural networks to obtain vector representations of user pairs and utilizes
    multiple query strategies to select informative user pairs for labeling and model
    training, achieving superior performance compared to existing methods. Cai et
    al.[[156](#bib.bib156)] design a multi-view DAL method that reduces the annotation
    cost by selectively querying metadata for the most informative examples, using
    a mapping function from the visual view to the text view. They demonstrate that
    multi-view DAL can use richer information to help improve performance than using
    single view. Zhao et al.[[157](#bib.bib157)] propose a DAL-based transfer learning
    framework for link prediction in recommender systems, which iteratively selects
    entities from source systems for target systems using uncertainty-based criteria.
    Experiments show that their method successfully improves efficiency and effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Community Detection aims to accurately partition nodes into distinct classes
    based on the topological structure of the networks. However, in many practical
    scenarios, unsupervised methods struggle to achieve the exact community. To solve
    this issue, Gupta et al. [[158](#bib.bib158)] propose community trolling, a DAL-based
    method for topic-based community detection. Their method selects relevant samples
    from polluted big data, reducing the unreliable dataset to a reliable one for
    studying communities. Chien et al. [[159](#bib.bib159)] propose a novel DAL method
    for geometric community detection. They first remove many cross-cluster edges
    while preserving intra-cluster connectivity to avoid noise. Then, they interactively
    query the label of one node for each disjoint component to recover the underlying
    clusters. Experiments show that they can achieve SOTA performance in community
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Other Selected Interesting Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: 'Engineering Systems. DAL methods exhibit remarkable performance in computationally
    demanding engineering systems by significantly reducing running time and computational
    costs. For example, Yue et al. [[169](#bib.bib169)] introduce two novel DAL algorithms:
    the variance-based weighted AL and the D-optimal weighted AL, designed specifically
    for Gaussian processes with uncertainties. Numerical studies demonstrate the effectiveness
    of their approach, notably improving predictive modeling for automatic shape control
    of composite fuselage structures. In another vein, Lee et al. [[170](#bib.bib170)]
    optimize their DAL acquisition function by jointly considering safe variance reduction
    and safe region expansion tasks, aiming to minimize failures without explicit
    knowledge of failure regions. This approach is tailored for real systems with
    uncertain failure conditions, as demonstrated in the predictive modeling of composite
    fuselage deformation, achieving zero failures by considering the composite failure
    criterion. Furthermore, Lee et al. [[171](#bib.bib171)] introduce a partitioned
    DAL method, comprising two systematic steps: global searching for uncertain design
    spaces and local searching using local Gaussian processes. They apply their method
    to aerospace manufacturing and materials science, achieving superior performance
    in prediction accuracy and computational efficiency compared to benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Personalized Medical Treatment explores how patient health is affected by taking
    a drug and how user questions are answered by search recommendation [[172](#bib.bib172)].
    Although modern methods can achieve impressive performance, they need a significant
    amount of labeled data. To solve this issue, Deng et al. [[160](#bib.bib160)]
    propose the use of DAL to recruit patients and assign treatments that reduce the
    uncertainty of an Individual Treatment Effect model. Sundin et al. [[173](#bib.bib173)]
    propose to use a Gaussian process to model the individual treatment effect and
    use the expected information gain over the S-type error rate, defined as the error
    in predicting the sign of the conditional average treatment effect, as their acquisition
    function. Jesson et al. [[113](#bib.bib113)] develop epistemic uncertainty-aware
    methods for DAL of personalized treatment effects from high-dimensional observational
    data. In contrast to previous work that only uses information gain as the acquisition
    objective, they propose Causal-BALD because they consider both information gain
    and overlap between the treatment and control groups. Li et al. [[174](#bib.bib174)]
    used DAL to help people by recognizing their emotion.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Summary of various challenges and opportunities.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Challenge Types | Challenges | Opportunities |'
  prefs: []
  type: TYPE_TB
- en: '| Pipeline-related Issues | Inefficient & Costly human annotation | servers,
    workers and annotators share information [[134](#bib.bib134)]. |'
  prefs: []
  type: TYPE_TB
- en: '| self-supervised pseudo-labels to reduce human efforts [[36](#bib.bib36),
    [85](#bib.bib85)]. |'
  prefs: []
  type: TYPE_TB
- en: '| incorporate additional knowledge to reduce expert knowledge [[94](#bib.bib94),
    [148](#bib.bib148)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Insufficient research on stopping strategies | the confidence among the selected
    samples does not increase [[175](#bib.bib175)]. |'
  prefs: []
  type: TYPE_TB
- en: '| stop when all instances lie between two contour lines [[176](#bib.bib176)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| upper bound in expected generalization errors as stopping criterion [[177](#bib.bib177)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cold-start | use pre-trained embeddings [[31](#bib.bib31), [178](#bib.bib178)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| design initial queries [[179](#bib.bib179), [180](#bib.bib180)]. |'
  prefs: []
  type: TYPE_TB
- en: '| use diverse sampling [[181](#bib.bib181), [182](#bib.bib182)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Tasks-related Issues | Difficulty in cross-domain transfer | select samples
    in regions of joint disagreement between models [[84](#bib.bib84), [183](#bib.bib183),
    [154](#bib.bib154)]. |'
  prefs: []
  type: TYPE_TB
- en: '| source and target domain distribution matching [[110](#bib.bib110), [184](#bib.bib184)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| transferable DAL policies between the source and target graphs [[120](#bib.bib120)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Unstable performance | avoid DAL’s sensitivity to the initial labeled set [[176](#bib.bib176),
    [185](#bib.bib185), [94](#bib.bib94), [31](#bib.bib31), [53](#bib.bib53)]. |'
  prefs: []
  type: TYPE_TB
- en: '| use distribution information to improve model’s robustness [[186](#bib.bib186),
    [187](#bib.bib187), [187](#bib.bib187)]. |'
  prefs: []
  type: TYPE_TB
- en: '| use pre-trained language model [[61](#bib.bib61), [188](#bib.bib188)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Lack of scalability & generalizability | hybrid strategies for sample selection [[60](#bib.bib60),
    [104](#bib.bib104)]. |'
  prefs: []
  type: TYPE_TB
- en: '| nearest-neighbor classifiers [[189](#bib.bib189)]. |'
  prefs: []
  type: TYPE_TB
- en: '| combining annotation and counterfactual sample construction [[190](#bib.bib190),
    [191](#bib.bib191)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Datasets-related Issues | Outlier Data & Noisy Oracles | find the best balance
    between purity and informativeness [[126](#bib.bib126), [89](#bib.bib89)]. |'
  prefs: []
  type: TYPE_TB
- en: '| knowledge distillation [[14](#bib.bib14)]. |'
  prefs: []
  type: TYPE_TB
- en: '| relabeling frameworks for correst oracle labels [[81](#bib.bib81), [192](#bib.bib192),
    [193](#bib.bib193)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Data Scarcity & Imbalance | data augmentation and large PLMs  [[12](#bib.bib12),
    [32](#bib.bib32), [97](#bib.bib97)]. |'
  prefs: []
  type: TYPE_TB
- en: '| cost-sensitive learning [[176](#bib.bib176), [194](#bib.bib194)]. |'
  prefs: []
  type: TYPE_TB
- en: '| design new query strategies for imbalanced datasets [[195](#bib.bib195),
    [196](#bib.bib196), [197](#bib.bib197)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Class distribution mismatch | new DAL query strategy [[125](#bib.bib125),
    [198](#bib.bib198)]. |'
  prefs: []
  type: TYPE_TB
- en: '| new DAL framework [[199](#bib.bib199)]. |'
  prefs: []
  type: TYPE_TB
- en: '| incoporate additional detector [[200](#bib.bib200)]. |'
  prefs: []
  type: TYPE_TB
- en: VI Challenges & Opportunities of DAL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As Table [IV](#S5.T4 "TABLE IV ‣ V-D Other Selected Interesting Applications
    ‣ V Applications of DAL ‣ A Survey on Deep Active Learning: Recent Advances and
    New Frontiers") shows, hereafter, we summarize the challenges and the corresponding
    potential solutions and opportunities.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Pipeline-related Issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Inefficient & Costly Human Annotation. DAL assumes that human annotators are
    readily available to label new samples once they are required. However, this assumption
    may not hold in some real-world applications. Human annotators can get tired or
    need breaks, forcing the DAL process to be suspended until they reappear. Moreover,
    human annotation is time-consuming and needs expert knowledge, resulting in long
    waits before models can be re-trained with new labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eaf8035ee5f74fa126428fb051a2ceb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The framework for efficiently annotation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve efficiency, DAL methods incorporate additional techniques to reduce
    human annotation. Wang et al. [[36](#bib.bib36)] use self-supervised learning
    by adding pseudo-labels with high confidence to help reduce human effort and improve
    the performance of the model. Go one step further, Yang et al. [[85](#bib.bib85)]
    introduce multiple pseudo-annotators that provide labels for unlabeled samples,
    achieving good performance without requiring human expert knowledge. On the other
    hand, as shown in Fig. [11](#S6.F11 "Figure 11 ‣ VI-A Pipeline-related Issues
    ‣ VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers"), Huang et al. [[134](#bib.bib134)] propose a new
    annotation strategy to allow servers, workers, and annotators to cooperate efficiently
    for sharing candidate queries and annotations. Experiments show that their model
    can avoid annotation noise and save much time for re-checking annotations. To
    further reduce expert knowledge, others tend to reduce the search scope in each
    iteration to improve efficiency. For example, Yang et al. [[94](#bib.bib94)] restrict
    candidate samples to their nearest neighbors of the labeled set rather than scanning
    all data.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Insufficient Research on Stopping Strategies. Few studies are designed for stopping
    strategies of DAL methods [[196](#bib.bib196)]. However, stopping strategies are
    essential for DAL because they reduce the amount of human labor by limiting the
    number of samples that need to be labeled and prevent the inclusion of noisy and
    redundant samples, which can negatively affect the performance of DAL models.
  prefs: []
  type: TYPE_NORMAL
- en: McDonald et al. [[175](#bib.bib175)] design two novel stopping strategies for
    DAL methods in the document classification task. The first strategy measures the
    overall confidence of the classifiers in correctly classifying the remaining unlabeled
    documents. It assumes that when the classifier’s mean confidence level for the
    remaining documents stabilizes, the model stops the DAL process, since its effectiveness
    would no longer improve. The second strategy measures the confidence of the classifiers
    among the selected documents to be reviewed. It assumes that when the classifier’s
    confidence stops increasing for these documents, it has reached its maximal confidence
    and stops the DAL process. Benefiting from the idea of the margin exhaustion criterion,
    Yu et al. [[176](#bib.bib176)] identify two corresponding contour lines in the
    instance space and assume that the DAL process can only be stopped when all instances
    lying between these two contour lines have been labeled. They achieve good performance
    in many classification tasks. Based on the Bayesian theory, Ishibashi et al. [[177](#bib.bib177)]
    derive a novel upper bound for the difference in expected generalization errors
    before and after obtaining new training data. They then combine this upper bound
    with a statistical test to derive a stopping criterion for DAL and significantly
    improve efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Cold-start. Most DAL methods fail to improve over random selection when the
    annotation budget is very small, a phenomenon sometimes term as “cold-start” [[179](#bib.bib179)].
    Uncertainty sampling has been shown to be inherently unsuitable for low budgets,
    possibly explaining the cold-start phenomenon [[201](#bib.bib201)]. Low budgets
    can be seen in many applications, especially those that require an expert tagger
    whose time is expensive. If we want to expand deep learning to new domains, overcoming
    the cold-start problem is an ever-important task.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e7396d4ecc1e6860c507f61e8bc7cb2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: An example for cold-start data selection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To relieve the cold-start issue, Yuan et al. [[31](#bib.bib31)] use pre-trained
    embeddings on unsupervised tasks, decreasing budget dependency while remaining
    faithful to uncertainty sampling. Similarly, Yu et al. [[178](#bib.bib178)] try
    to use pre-trained knowledge from PLMs to avoid cold-start. They select few shot
    samples to fine-tune large-scale PLM, achieve SOTA performance in six datasets,
    and improve the efficiency of labeling over existing baselines by 3.2%–6.9% on
    average. On the other hand, in Fig. [12](#S6.F12 "Figure 12 ‣ VI-A Pipeline-related
    Issues ‣ VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers") (a-b), Yehuda et al. [[180](#bib.bib180)]
    develop a new DAL initialization strategy to solve the cold-start issue for low-budget
    image classification, which significantly outperforms CoreSet initialization in
    the low-budget regime. They also theoretically analyze different DAL strategies
    in embedding spaces and improve performance on both low- and high-budget scenes.
    In Fig. [12](#S6.F12 "Figure 12 ‣ VI-A Pipeline-related Issues ‣ VI Challenges
    & Opportunities of DAL ‣ A Survey on Deep Active Learning: Recent Advances and
    New Frontiers") (c), Cao et al. [[181](#bib.bib181)] apply the informative sampling
    policy on the $\gamma$ tube to solve the cold-start sampling problem. Mahmood
    et al. [[182](#bib.bib182)] query a diverse set of examples with minimal Wasserstein
    distance from unlabeled data. They report a significant performance boost in the
    low-budget regime.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Task-related Issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Difficulty in Cross-domain Transfer. We discuss two difficulties of cross-domain
    transfer in DAL. First, machine learning systems are always deployed on various
    devices with the same labeled dataset. However, DAL is often model-dependent and
    not directly transferable, i.e., data queried for one model may be less effective
    for another [[183](#bib.bib183)]; Second, transfer learning biases DAL to select
    samples that match the distribution of the source domain to the target domain,
    leading to sampling bias and the high cost of transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/257444e710c80f632886dcfef31526ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Multi-task learning transfer knowledge from sources [[84](#bib.bib84)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'To benefit multiple target models, some methods aim to select samples in joint
    disagreement regions across models [[183](#bib.bib183)], adopt multi-agent reinforcement
    learning for optimal selection [[154](#bib.bib154)], or leverage multi-task learning
    to transfer common knowledge from the source domain as shown in Fig. [13](#S6.F13
    "Figure 13 ‣ VI-B Task-related Issues ‣ VI Challenges & Opportunities of DAL ‣
    A Survey on Deep Active Learning: Recent Advances and New Frontiers"). To avoid
    sampling bias, Farquhar et al. [[184](#bib.bib184)] apply corrective weighting
    using an unbiased risk estimator to maintain the target distribution during pool-based
    sampling. Trang et al. [[110](#bib.bib110)] introduce a heuristic query strategy
    that matches the distribution of the source domain while retrieving valuable target
    samples. Hu et al. [[120](#bib.bib120)] learn transferable DAL policies on labeled
    source graphs that generalize selection to unlabeled target graphs. Experiments
    show that the above methods can achieve excellent performance and transferability.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Unstable Performance. DAL methods always have unstable performance, i.e., results
    for the same method vary significantly with different initialized seeds [[108](#bib.bib108)].
    Two primary reasons can explain this instability. First, the DAL methods are sensitive
    to the initial labeled dataset. The initial selected samples have a great influence
    on the eventual outcome of the current approaches. With insufficient initial labeling,
    subsequent DAL cycles become highly biased, resulting in poor selection. Second,
    current DAL methods always separate active learning and deep learning methods
    into two separate processes, easily leading to sub-optimal and unstable performance [[202](#bib.bib202)].
  prefs: []
  type: TYPE_NORMAL
- en: To solve DAL’s sensitivity to the initialization, current methods always use
    diverse sampling and pre-trained models. Yu et al. [[176](#bib.bib176)] adopt
    hierarchical clustering to select 10% samples near each clustering center as representative
    samples. Their new initialization greatly helps stabilize the performance. Zlabinger
    et al. [[185](#bib.bib185)] take into account both diversity and polarization
    to effectively select initial samples for DAL methods that further stabilize the
    performance of the DAL process. Yang et al. [[94](#bib.bib94)] select initial
    samples by evaluating the total distance between the unlabeled samples and the
    initial samples, showing that the same distance between them can result in better
    and stable performance. On the other hand, Yuan et al. [[31](#bib.bib31)] incorporate
    language information as prior knowledge to help learn node representations and
    use clustering methods to select the initial data. Similarly, Ein-Dor [[53](#bib.bib53)]
    uses BERT to learn the representations of the input sentences and uses a hybrid
    query strategy to select the most uncertain and diverse samples as the initialized
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a593789a38e0ff64fc47d18cdb0d5d90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Stable performance of TrustAL [[186](#bib.bib186)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'To bridge the gap between AL and deep learning models, Kwak et al. [[186](#bib.bib186)]
    introduce Trustworthy AL (TrustAL), a label-efficient DAL framework by transferring
    distilled knowledge from deep learning models to the data selection process. As
    Fig. [14](#S6.F14 "Figure 14 ‣ VI-B Task-related Issues ‣ VI Challenges & Opportunities
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")
    shows, they jointly optimize knowledge distillation and DAL to obtain a more consistent
    and reliable performance compared to the two best performing baselines on three
    benchmarks. Similarly, Ma et al. [[187](#bib.bib187)] learn nonlinear embeddings
    to map inputs into a latent space and introduce a selection block to choose representative
    samples in the learned latent space to achieve stable performance. Margatina et
    al. [[61](#bib.bib61)] extend the PLMs to continually pre-train on available unlabeled
    data to tailor it to the task-specific domain, where they can benefit from both
    labeled and unlabeled data at each DAL iteration. Their experiments show considerable
    enhancements in data efficiency and stability compared to the standard fine-tuning
    approach, emphasizing the importance of a suitable training strategy in DAL. Mamooler
    et al. [[188](#bib.bib188)] try to combine DAL with PLMs in the legal domain,
    where they use unlabeled data in three stages: training the model to adjust it
    to the downstream task, using knowledge distillation to direct the embeddings
    to a semantically meaningful space, and identifying the initial set.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Lack of Scalability & Generalizability. Current DAL methods lack scalability,
    as they always require significant modifications to neural network architectures
    for adapting to different query strategies. Another issue with current methods
    is their heavy reliance on DAL’s weight parameters, while the parameters may not
    be generalizable to different datasets. Users are required to prepare additional
    labeled samples as a validation set to tune parameters by cross-validation, which
    contradicts the goal of minimizing the need for labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In response to the above issues, Maekawa et al. [[60](#bib.bib60)] introduce
    a novel DAL method, called TYROGUE, that uses a hybrid query strategy to improve
    model generalization and reduce labeling costs. As Figure [15](#S6.F15 "Figure
    15 ‣ VI-B Task-related Issues ‣ VI Challenges & Opportunities of DAL ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers") shows, uncertainty-based
    methods tend to acquire similar data points from a specific area within an iteration,
    diversity-based methods tend to acquire data points similar to the samples acquired
    in previous iterations, and TYROGUE balances diversity and uncertainty by acquiring
    samples that are diverse and also closer to the model decision boundary. RMQCAL [[104](#bib.bib104)]
    is a novel scalable DAL method, which allows for any number and type of query
    criteria, eliminates the need for empirical parameters, and makes the trade-offs
    between the query criteria self-adaptive. On the other hand, Wan et al. [[189](#bib.bib189)]
    propose an embedded network of nearest-neighbor classifiers to enhance the generalization
    ability of models trained in labeled and unlabeled sub-spaces in a simple but
    effective manner. Deng et al. [[190](#bib.bib190)] focus on combining sample annotation
    and counterfactual sample construction in the DAL procedure to enhance the model’s
    out-of-distribution generalization. Wang et al. [[191](#bib.bib191)] introduce
    a new training manner to improve model’s generalizability and show a strong positive
    correlation between convergence speed and generalization performance under ultra-wide
    conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c50f0e8e43a75dd106c16fab143624d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: TYROGUE can select better samples than baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Dataset-related Issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Outlier Data & Noisy Oracles. DAL methods tend to acquire outliers since models
    always assign high uncertainty scores to outliers. Outliers can damage a model’s
    learning ability and fuel a vicious cycle in which DAL methods continue to select
    them [[43](#bib.bib43)]. Identifying and removing outliers has become an important
    direction in improving DAL performance and robustness. On the other hand, classic
    DAL methods assume that annotators have high labeling accuracy. However, in real-world
    settings, sample difficulty and annotator expertise can significantly affect the
    quality and accuracy of annotation, which may further degrade model performance.
  prefs: []
  type: TYPE_NORMAL
- en: To remove outliers, Park et al. [[126](#bib.bib126)] propose MQ-Net to adaptively
    find the best balance between purity and informativeness of samples, filtering
    out noisy open-set data. Elenter et al. [[89](#bib.bib89)] introduce a new query
    strategy based on Lagrangian duality to select diverse samples, efficiently removing
    redundant data. Other studies [[14](#bib.bib14)] use knowledge distillation to
    compress useful knowledge into a small model, effectively identifying and removing
    outliers. To make high-quality annotations, AMCC [[81](#bib.bib81)] measures worker
    annotations considering both their commonality and individuality to reduce the
    impact of unreliable workers and improve effectiveness. Zhao et al. [[192](#bib.bib192)]
    actively select samples that are relabeled multiple times through crowd-sourcing
    majority voting. EMMA [[193](#bib.bib193)] relabels samples to remove noisy annotations
    by analyzing the stimulus based on model memory retention and greedy heuristics.
    BALT [[203](#bib.bib203)] improves human expertise during labeling to improve
    relabel quality and significantly improve model performance. Zlabinger [[185](#bib.bib185)]
    trains human annotators on a set of pre-labeled samples to improve the quality
    of annotations. Huang et al. [[134](#bib.bib134)] propose a multi-server, multi-worker
    framework for DAL, where servers and workers cooperate to select diverse samples
    and improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Data Scarcity & Imbalance. Data scarcity poses two critical challenges. First,
    datasets are difficult to collect and annotate [[204](#bib.bib204)]; Second, DAL
    methods have the common underlying assumption that all classes are equal, while
    some classes have more samples than others (skewed class distribution [[176](#bib.bib176)])
    or some classes may be more difficult to learn than others, leading to sampling
    bias in the acquisition process [[205](#bib.bib205)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/816d4aff56093cbcce0599b564af2deb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: An example of imbalanced sampling [[195](#bib.bib195)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'For scarce datasets, Chen et al. [[12](#bib.bib12)] used data augmentation
    to generate diverse samples to expand training data. Other studies used PLMs as
    prior knowledge and fine-tuned them to reduce the required labeled samples [[32](#bib.bib32)].
    For difficult annotations, Gudovskiy et al. [[97](#bib.bib97)] introduce several
    novel self-supervised pseudo-labels estimators to correct acquisition bias by
    minimizing the distribution shift between unlabeled data and weakly labeled validation
    data. To mitigate the classes imbalance, Yu et al. [[176](#bib.bib176)] are the
    first to use cost-sensitive learning. They choose the extreme weighted learning
    machine as the base learner to select samples based on the class imbalance ratio,
    class overlap, and small disjunction. They investigate why DAL can be impacted
    by a skewed instance distribution and improve DAL performance on imbalanced datasets.
    Choi et al. [[194](#bib.bib194)] solve the issue of data imbalance by considering
    the probability of mislabeling a class, the probability of the data given a predicted
    class, and the prior probability of the abundance of a predicted class, during
    querying samples of DAL. Experiments show that they can significantly enhance
    the ability of existing DAL methods to handle unbalanced datasets. As shown in
    Fig. [16](#S6.F16 "Figure 16 ‣ VI-C Dataset-related Issues ‣ VI Challenges & Opportunities
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"),
    Zhao et al. [[195](#bib.bib195)] propose an alternate query strategy by using
    the medial distribution to find a compromise between importance weighting and
    class-balanced sampling. Experiments show that their model can be easily combined
    with various DAL methods and successfully select balanced samples in imbalanced
    datasets. Hartford et al. [[196](#bib.bib196)] present an exemplar guided DAL
    method that shows strong empirical performance under extremely skewed label distributions
    by using exemplar embedding. Zhang et al. [[197](#bib.bib197)] propose a graph-based
    DAL method that applies a more sophisticated version of uncertainty sampling.
    Their strategy can select more evenly distributed examples for labeling than standard
    uncertainty sampling.'
  prefs: []
  type: TYPE_NORMAL
- en: \adfhalfrightarrowhead
  prefs: []
  type: TYPE_NORMAL
- en: Class Distribution Mismatch. DAL methods assume that the labeled and unlabeled
    data are drawn from the same class distribution, which means that the categories
    of both datasets are identical [[200](#bib.bib200)]. However, in real-world scenarios,
    unlabeled data often come from uncontrolled sources, and a large portion of the
    examples may belong to unknown classes. For example, when crawling images for
    binary image classification using keywords like “dog” and “cat,” over 50% of the
    images in the unlabeled dataset are irrelevant to the task (e.g., “deer,” “horse”).
    Annotating these irrelevant images will lead to a waste of annotation budget as
    they are unnecessary for training the desired classifier. Despite this challenge,
    existing DAL systems tend to select these irrelevant images for annotation, as
    they contain more uncertain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this issue, As shown in Fig. [17](#S6.F17 "Figure 17 ‣ VI-C Dataset-related
    Issues ‣ VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers") (a), He et al. [[198](#bib.bib198)] propose
    the energy discrepancy to measure the density distribution between the seen and
    unseen classes. Then, they propose an iterative optimization strategy to facilitate
    the teacher-student distillation network to avoid selecting samples from unseen
    classes. Furthermore, Tang et al. [[199](#bib.bib199)] propose a dual DAL framework
    that simultaneously performs model search and data selection. Their framework
    effectively addressed the issue of distribution mismatch and significantly improves
    model performance. In Fig. [17](#S6.F17 "Figure 17 ‣ VI-C Dataset-related Issues
    ‣ VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers") (b), Ning et al. [[200](#bib.bib200)] introduce a
    detector-classifier DAL framework, where the detector filters unknown classes
    using Gaussian Mixture Models and the classifier selects uncertain in-distribution
    samples for retraining. By actively acquiring purer in-distribution query sets,
    this framework improves the model generalization on class distribution mismatch.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c7d3b4df84d29b8c0ce1741cc691290e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Methods for solving class distribution mismatch.'
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the advantages of DAL, such as high efficiency, good effectiveness, and
    strong robustness, DAL has been deployed in both research and industry projects.
    This article provides a comprehensive survey on DAL, including its collection,
    definition, influential baselines and datasets, taxonomy, applications, challenges,
    and some inspiring prospects. First, we discuss the collection and filtering of
    DAL papers to ensure their high-quality. Second, we give the definition of DAL
    tasks, and present its basic pipeline, influential baselines, and widely used
    datasets. Third, we present our taxonomy for DAL methods from several perspectives
    and discuss their strengths and weaknesses. From them, we obtain some guidelines
    for selecting different query strategies, deep model architectures, and learning
    paradigms to apply for different tasks. In addition, different annotation strategies
    can significantly reduce manual labor while also bringing certain drawbacks. In
    terms of training process, curriculum learning training and Pre+FT can better
    adapt to the current era of large language models. Fourth, we discuss some typical
    applications of DAL. Other than the commonly used and popular DAL methods used
    for CV tasks, we also introduce the carefully designed DAL method for NLP, DM,
    etc. Finally, even though DAL has many benefits, we reckon that they can be refined
    further in terms of pipeline, tasks, and datasets. Specifically, there are many
    problems that DAL is hard to handle, such as inefficient human annotation, difficulty
    in cross-domain transfer, unstable performance, lack of scalability, data imbalance,
    and class distribution mismatch. We share DAL-related resources on Github. We
    hope that this work will be a quick guide for researchers and motivate them to
    solve important problems in the DAL domain.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, “A comprehensive
    survey on graph neural networks,” *IEEE Trans. Neural Networks Learn. Syst.*,
    vol. 32, no. 1, pp. 4–24, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] B. Gu, Z. Zhai, C. Deng, and H. Huang, “Efficient active learning by querying
    discriminative and representative samples and fully exploiting unlabeled data,”
    *IEEE Trans. Neural Networks Learn. Syst.*, vol. 32, no. 9, pp. 4111–4122, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] X. Cao and I. Tsang, “Shattering distribution for active learning,” *IEEE
    Trans. Neural Networks Learn. Syst.*, vol. 33, no. 1, pp. 215–228, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S. Liu, S. Xue, J. Wu, C. Zhou, J. Yang, Z. Li, and J. Cao, “Online active
    learning for drifting data streams,” *IEEE Trans. Neural Networks Learn. Syst.*,
    vol. 34, no. 1, pp. 186–200, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] T. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional
    networks,” in *Proc. of of ICLR*, 2017\. [Online]. Available: [https://openreview.net/forum?id=SJU4ayYgl](https://openreview.net/forum?id=SJU4ayYgl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” *Proceedings of the IEEE*, vol. 86, no. 11,
    pp. 2278–2324, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Vaswani, N. Shazeer *et al.*, “Attention is all you need,” in *Proc.
    of NeurIPS*, 2017, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Radford, J. W. Kim *et al.*, “Learning transferable visual models from
    natural language supervision,” in *Proc. of ICML*, 2021, pp. 8748–8763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] OpenAI, “GPT-4 technical report,” *CoRR*, vol. abs/2303.08774, 2023. [Online].
    Available: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *Proc. of ICML*, 2009, pp. 41–48.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] L. Shao, F. Zhu, and X. Li, “Transfer learning for visual categorization:
    A survey,” *IEEE Trans. Neural Networks Learn. Syst.*, vol. 26, no. 5, pp. 1019–1034,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Z. Chen, J. Zhang *et al.*, “When active learning meets implicit semantic
    data augmentation,” in *Proc. of ECCV*, 2022, pp. 56–72.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. Yang, Z. Xie *et al.*, “Dataset pruning: Reducing training data by
    examining generalization influence,” in *Proc. of ICLR*, 2023. [Online]. Available:
    [https://openreview.net/forum?id=4wZiAXD29TQ](https://openreview.net/forum?id=4wZiAXD29TQ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] F. Peng, C. Wang *et al.*, “Active learning for lane detection: A knowledge
    distillation approach,” in *Proc. of ICCV*, 2021, pp. 15 152–15 161.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Z. Zhang, E. Strubell, and E. H. Hovy, “A survey of active learning for
    natural language processing,” in *Proc. of EMNLP*, 2022, pp. 6166–6190.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] D. Tuia, M. Volpi, L. Copa, M. F. Kanevski, and J. Muñoz-Marí, “A survey
    of active learning algorithms for supervised remote sensing image classification,”
    *CoRR*, 2021\. [Online]. Available: [https://arxiv.org/abs/2104.07784](https://arxiv.org/abs/2104.07784)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] H. Wang, Q. Jin *et al.*, “A comprehensive survey on deep active learning
    and its applications in medical image analysis,” *CoRR*, 2023. [Online]. Available:
    [https://doi.org/10.48550/arXiv.2310.14230](https://doi.org/10.48550/arXiv.2310.14230)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] H. Hadian and H. Sameti, “Active learning in noisy conditions for spoken
    language understanding,” in *Proc. of ACL*, 2014, pp. 1081–1090.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S. Budd, E. Robinson *et al.*, “A survey on active learning and human-in-the-loop
    deep learning for medical image analysis,” *Medical Image Anal.*, vol. 71, pp.
    102 062–102 082, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. Takezoe, X. Liu *et al.*, “Deep active learning for computer vision
    past and future,” *CoRR*, 2022\. [Online]. Available: [https://arxiv.org/abs/2211.14819](https://arxiv.org/abs/2211.14819)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] X. Zhan, H. Liu *et al.*, “A comparative survey: Benchmarking for pool-based
    active learning,” in *Proc. of IJCAI*, 2021, pp. 4679–4686.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X. Zhan, Q. Wang *et al.*, “A comparative survey of deep active learning,”
    *CoRR*, 2022\. [Online]. Available: [https://arxiv.org/abs/2203.13450](https://arxiv.org/abs/2203.13450)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Y. Fu, X. Zhu, and B. Li, “A survey on instance selection for active learning,”
    *Knowl. Inf. Syst.*, vol. 35, no. 2, pp. 249–283, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] C. Aggarwal, X. Kong *et al.*, “Active learning: A survey,” in *Data Classification*,
    2014\. [Online]. Available: [https://charuaggarwal.net/active-survey.pdf](https://charuaggarwal.net/active-survey.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Tharwat and W. Schenck, “A survey on active learning: State-of-the-art,
    practical challenges and research directions,” *Mathematics*, vol. 11, no. 4,
    2023\. [Online]. Available: [www.mdpi.com/2227-7390/11/4/820](www.mdpi.com/2227-7390/11/4/820)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] P. Ren, Y. Xiao *et al.*, “A survey of deep active learning,” *ACM Comput.
    Surv.*, vol. 54, no. 9, pp. 1–40, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] P. Liu, L. Wang *et al.*, “A survey on active deep learning: From model
    driven to data driven,” *ACM Comput. Surv.*, vol. 54(10), pp. 1–34, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] D. Cacciarelli and M. Kulahci, “Active learning for data streams: a survey,”
    *Mach. Learn.*, vol. 113, no. 1, pp. 185–239, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] K. Margatina, L. Barrault, and N. Aletras, “On the importance of effectively
    adapting pretrained language models for active learning,” in *Proceedings of ACL*,
    2022, pp. 825–836.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] A. Ayub and C. Fendley, “Few-shot continual active learning by a robot,”
    in *Prof. NeurIPS*, vol. 35, 2022, pp. 30 612–30 624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] M. Yuan, H.-T. Lin, and J. Boyd-Graber, “Cold-start active learning through
    self-supervised language modeling,” in *Proc. of EMNLP*, 2020, p. 7935–7948.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. Seo, D. Kim, Y. Ahn, and K. Lee, “Active learning on pre-trained language
    model with task-independent triplet loss,” in *Proc. of AAAI*, 2022, pp. 11 276–11 284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] K. Margatina, T. Schick, N. Aletras, and J. Dwivedi-Yu, “Active learning
    principles for in-context learning with large language models,” in *Findings of
    EMNLP*, 2023, pp. 5011–5034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Y. Gal and Z. Ghahramani, “Bayesian convolutional neural networks with
    bernoulli approximate variational inference,” *Proc. of ICLR*, 2016. [Online].
    Available: [https://arxiv.org/abs/1506.02158](https://arxiv.org/abs/1506.02158)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Gal, R. Islam *et al.*, “Deep bayesian active learning with image data,”
    in *Proc. of ICML*, 2017, pp. 1183–1192.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] K. Wang, D. Zhang *et al.*, “Cost-effective active learning for deep image
    classification,” *IEEE Trans. Circuits Syst. Video Technol.*, vol. 27, no. 12,
    pp. 2591–2600, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] L. Balaji, A. Pritzel *et al.*, “Simple and scalable predictive uncertainty
    estimation using deep ensembles,” in *Proc. of NeurIPS*, 2017\. [Online]. Available:
    [https://arxiv.org/abs/1612.01474](https://arxiv.org/abs/1612.01474)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] M. Fang, Y. Li, and T. Cohn, “Learning how to active learn: A deep reinforcement
    learning approach,” in *Proc. of EMNLP*, 2017, pp. 595–605.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] K. Konyushkova, R. Sznitman, and P. Fua, “Learning active learning from
    data,” in *Proc. of NeurIPS*, 2017\. [Online]. Available: [https://arxiv.org/abs/1703.03365](https://arxiv.org/abs/1703.03365)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Zhu and J. Bento, “Generative adversarial active learning,” *CoRR*,
    2017\. [Online]. Available: [https://arxiv.org/abs/1702.07956](https://arxiv.org/abs/1702.07956)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] O. Sener and S. Savarese, “Active learning for convolutional neural networks:
    A core-set approach,” in *Proc. of ICLR*, 2018\. [Online]. Available: [https://openreview.net/forum?id=H1aIuk-RW](https://openreview.net/forum?id=H1aIuk-RW)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. Ducoffe and F. Precioso, “Adversarial active learning for deep networks:
    a margin based approach,” *CoRR*, 2018\. [Online]. Available: [https://arxiv.org/abs/1802.09841](https://arxiv.org/abs/1802.09841)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] K. Wang, L. Lin, X. Yan, Z. Chen, D. Zhang, and L. Zhang, “Cost-effective
    object detection: Active sample mining with switchable selection criteria,” *IEEE
    Trans. Neural Networks Learn. Syst.*, vol. 30, no. 3, pp. 834–850, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] M. Carbonneau, E. Granger, and G. Gagnon, “Bag-level aggregation for multi-instance
    active learning in instance classification problems,” *IEEE Trans. Neural Networks
    Learn. Syst.*, vol. 30, no. 5, pp. 1441–1451, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] A. Kirsch, J. Amersfoort, and Y. Gal, “Batchbald: Efficient and diverse
    batch acquisition for deep bayesian active learning,” in *Proc. of NeurIPS*, 2019\.
    [Online]. Available: [https://arxiv.org/abs/1906.08158](https://arxiv.org/abs/1906.08158)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Z. Liu, J. Wang, S. Gong, H. Lu, and D. Tao, “Deep reinforcement active
    learning for human-in-the-loop person re-identification,” in *Proc. of ICCV*,
    2019, pp. 6122–6131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] J. Kasai, K. Qian *et al.*, “Low-resource deep entity resolution with
    transfer and active learning,” in *Proc. of ACL*, 2019, pp. 5851–5861.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] T. Tran, T. Do *et al.*, “Bayesian generative active deep learning,” in
    *Proc. of ICML*, 2019, pp. 6295–6304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] S. Sinha, S. Ebrahimi, and T. Darrell, “Variational adversarial active
    learning,” in *Proc. of ICCV*, 2019, pp. 5972–5981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J.-C. Su, Y.-H. Tsai, K. Sohn, B. Liu *et al.*, “Active adversarial domain
    adaptation,” in *Proc. of WACV*, 2020, pp. 739–748.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] M. Gao, Z. Zhang *et al.*, “Consistency-based semi-supervised active learning:
    Towards minimizing labeling cost,” in *Proc. of ECCV*, 2020, pp. 510–526.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] B. Zhang, L. Li *et al.*, “State-relabeling adversarial active learning,”
    in *Proc. of CVPR*, 2020, pp. 8756–8765.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] L. Dor, A. Halfon *et al.*, “Active learning for bert an empirical study,”
    in *Proc. of EMNLP*, 2020, pp. 7949–7962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Huang, T. Wang *et al.*, “Semi-supervised active learning with temporal
    output discrepancy,” in *Proc. of ICCV*, 2021, pp. 3447–3456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] G. Citovsky, G. DeSalvo, C. Gentile, L. Karydas, A. Rajagopalan, A. Rostamizadeh,
    and S. Kumar, “Batch active learning at scale,” in *Proc. of NeurIPS*, 2021, pp.
    11 933–11 944.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Y. Kim, K. Song, J. Jang, and I. Moon, “LADA: look-ahead data acquisition
    via augmentation for deep active learning,” in *Proc. of NeurIPS*, 2021, pp. 22 919–22 930.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] K. Kim, D. Park *et al.*, “Task-aware variational adversarial active learning,”
    in *Proc. of CVPR*, 2021, pp. 8166–8175.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] K. Siddharth, K. Ranjay *et al.*, “Mind your outliers! investigating the
    negative impact of outliers on active learning for visual question answering,”
    in *Proc. of ACL*, 2021, pp. 7265–7281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Z. Zhue, V. Yadav *et al.*, “Few-shot initializing of active learner via
    meta-learning,” in *Proc. of EMNLP*, 2022, pp. 1117–1133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] S. Maekawa, D. Zhang *et al.*, “Low-resource interactive active labeling
    for fine-tuning language models,” in *Proc. of EMNLP*, 2022, pp. 3230–3242.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] C. Schröder, A. Niekler, and M. Potthast, “Revisiting uncertainty-based
    query strategies for active learning with transformers,” in *Findings of ACL*,
    2022, pp. 2194–2203.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] P. Menard, O. Domingues *et al.*, “Fast active learning for pure exploration
    in reinforcement learning,” in *Proc. of ICML*, 2021, pp. 7599–7608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] A. Krizhevsky, G. Hinton *et al.*, “Learning multiple layers of features
    from tiny images,” 2009\. [Online]. Available: [https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf](https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Netzer, T. Wang, A. Coates *et al.*, “Reading digits in natural images
    with unsupervised feature learning,” 2011\. [Online]. Available: [http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] J. Deng, W. Dong *et al.*, “Imagenet: A large-scale hierarchical image
    database,” in *Proc. of CVPR*, 2009, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] T.-Y. Lin, M. Maire *et al.*, “Microsoft coco: Common objects in context,”
    in *Proc. of ECCV*, 2014, pp. 740–755.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] M. Cordts, M. Omran *et al.*, “The cityscapes dataset for semantic urban
    scene understanding,” in *Proc. of CVPR*, 2016, pp. 3213–3223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] L. Fei-Fei, R. Fergus, and P. Perona, “Learning generative visual models
    from few training examples: An incremental bayesian approach tested on 101 object
    categories,” in *Proc. of CVPR*, 2004, pp. 178–178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] R. Socher, A. Perelygin *et al.*, “Recursive deep models for semantic
    compositionality over a sentiment treebank,” in *Proc. of EMNLP*, 2013, pp. 1631–1642.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] X. Li and D. Roth, “Learning question classifiers,” in *Proc. of COLING*,
    2002, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning, “A large annotated
    corpus for learning natural language inference,” in *Proc. of EMNLP*, 2015, pp.
    632–642.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Maas, R. E. Daly *et al.*, “Learning word vectors for sentiment analysis,”
    in *Proc. of ACL*, 2011, pp. 142–150.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional networks
    for text classification,” *Proc. of NeurIPS*, 2015\. [Online]. Available: [https://arxiv.org/abs/1509.01626](https://arxiv.org/abs/1509.01626)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] P. Sen, G. Namata *et al.*, “Collective classification in network data,”
    *AI magazine*, vol. 29, no. 3, pp. 93–93, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S. Abu-El-Haija, N. Kothari *et al.*, “Youtube-8m: A large-scale video
    classification benchmark,” *arXiv*, 2016\. [Online]. Available: [https://arxiv.org/abs/1609.08675](https://arxiv.org/abs/1609.08675)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] A. E. Johnson, T. J. Pollard *et al.*, “Mimic-iii, a freely accessible
    critical care database,” *Scientific data*, vol. 3, no. 1, pp. 1–9, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] M. Wiechmann, S. Yimam *et al.*, “Activeanno: General-purpose document-level
    annotation tool with active learning integration,” in *Proc. of NAACL*, 2021,
    pp. 99–105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] T. Wu, Y. Liu *et al.*, “Redal: Region-based and diversity-aware active
    learning for point cloud semantic segmentation,” in *Proc. of ICCV*, 2021, pp.
    15 510–15 519.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] S. Kothawade, S. Ghosh *et al.*, “Targeted active learning for object
    detection with rare classes and slices using submodular mutual information,” in
    *Proc. of ECCV*, 2022, pp. 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] B. Xie, L. Yuan *et al.*, “Towards fewer annotations: Active learning
    via region impurity and prediction uncertainty for domain adaptive semantic segmentation,”
    in *Proc. of CVPR*, 2022, pp. 8068–8078.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] G. Yu, J. Tu, J. Wang, C. Domeniconi, and X. Zhang, “Active multilabel
    crowd consensus,” *IEEE Trans. Neural Networks Learn. Syst.*, vol. 32, no. 4,
    pp. 1448–1459, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] C. Schröder, K. Bürgl *et al.*, “Supporting land reuse of former open
    pit mining sites using text classification and active learning,” in *Proc. of
    ACL*, 2021, pp. 4141–4152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Yan, S. Huang *et al.*, “Active learning with query generation for
    cost-effective text classification,” in *Proc. of AAAI*, 2020, pp. 6583–6590.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] B. Zhou, X. Cai *et al.*, “MTAAL: multi-task adversarial active learning
    for medical named entity recognition and normalization,” in *Proc. of AAAI*, 2021,
    pp. 14 586–14 593.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Y. Yang and M. Loog, “Single shot active learning using pseudo annotators,”
    *Pattern Recognit.*, vol. 89, pp. 22–31, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Gong, Z. Fan *et al.*, “Meta agent teaming active learning for pose
    estimation,” in *Proc. of CVPR*, 2022, pp. 11 069–11 079.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] B. Xie, L. Yuan *et al.*, “Active learning for domain adaptation: An energy-based
    approach,” in *Proc. of AAAI*, 2022, pp. 8708–8716.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] D. Wang and Y. Shang, “A new active labeling method for deep learning,”
    in *Proc. of IJCNN*, 2014, pp. 112–119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] J. Elenter, N. Naderializadeh, and A. Ribeiro, “A lagrangian duality approach
    to active learning,” in *Proc. of NeurIPS*, 2022, pp. 37 575–37 589.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] W. Li, G. Dasarathy, K. N. Ramamurthy, and V. Berisha, “Finding the homology
    of decision boundaries with active learning,” in *Proc. of NeurIPS*, 2020, pp.
    8355–8365.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] D. Roth, K. Small *et al.*, “Margin-based active learning for structured
    output spaces,” in *Proc. of ECML*, 2006, pp. 413–424.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] K. Brantley, H. D. III, and A. Sharaf, “Active imitation learning with
    noisy guidance,” in *Proc. of ACL*, 2020, pp. 2093–2105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] S. Yan, K. Chaudhuri, and T. Javidi, “The label complexity of active learning
    from observational data,” in *Proc. of NeurIPS*, 2019, pp. 1808–1817.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Y. Yang and M. Loog, “To actively initialize active learning,” *Pattern
    Recognit.*, vol. 131, p. 108836, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Y. Kim and B. Shin, “In defense of core-set: A density-aware core-set
    selection for active learning,” in *Proc. of SIGKDD*, 2022, p. 804–812.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] C. Coleman, E. Chou *et al.*, “Similarity search for efficient active
    learning and search of rare concepts,” in *Proc. of AAAI*, 2022, pp. 6402–6410.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] D. A. Gudovskiy, A. Hodgkinson, T. Yamaguchi, and S. Tsukizawa, “Deep
    active learning for biased datasets via fisher kernel self-supervision,” in *Proc.
    of CVPR*, 2020, pp. 9041–9049.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Hasan, S. Paul, A. I. Mourikis, and A. K. Roy-Chowdhury, “Context-aware
    query selection for active learning in event recognition,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 42, no. 3, pp. 554–567, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Chakraborty, V. N. Balasubramanian, Q. Sun, S. Panchanathan, and J. Ye,
    “Active batch selection via convex relaxations with guaranteed solution bounds,”
    *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 37, no. 10, pp. 1945–1958, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Q. Jin, M. Yuan *et al.*, “One-shot active learning for image segmentation
    via contrastive learning and diversity-based sampling,” *Knowl. Based Syst.*,
    vol. 241, p. 108278, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] C. Li, H. Ma, Z. Kang, Y. Yuan, X. Zhang, and G. Wang, “On deep unsupervised
    active learning,” in *Proc. of IJCAI*, 2020, pp. 2626–2632.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] A. Parvaneh, E. Abbasnejad, D. Teney, R. Haffari, A. van den Hengel,
    and J. Q. Shi, “Active learning by feature mixing,” in *Proc. of CVPR*, 2022,
    pp. 12 227–12 236.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Li, J. M. Phillips, X. Yu, R. M. Kirby, and S. Zhe, “Batch multi-fidelity
    active learning with budget constraints,” in *Proc. of NeurIPS*, 2022, pp. 995–1007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Zhao, Z. Shi, J. Zhang, D. Chen, and L. Gu, “A novel active learning
    framework for classification: Using weighted rank aggregation to achieve multiple
    query criteria,” *Pattern Recognit.*, vol. 93, pp. 581–602, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] T. Wang, X. Li *et al.*, “Boosting active learning via improving test
    performance,” in *Proc. of AAAI*, 2022, pp. 8566–8574.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] M. Thiessen and T. Gärtner, “Active learning of convex halfspaces on
    graphs,” in *Proc. of NeurIPS*, 2021, pp. 23 413–23 425.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] M. A. Mohamadi, W. Bae, and D. J. Sutherland, “Making look-ahead active
    learning strategies feasible with neural tangent kernels,” in *Prof. NeurIPS*,
    vol. 35, 2022, pp. 12 542–12 553.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] D. Yoo, I. Kweon *et al.*, “Learning loss for active learning,” in *Proc.
    of CVPR*, 2019, pp. 93–102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] G. Zhao, E. R. Dougherty *et al.*, “Efficient active learning for gaussian
    process classification by error reduction,” in *Proc. of NeurIPS*, 2021, pp. 9734–9746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] T. Vu, M. Liu *et al.*, “Learning how to active learn by dreaming,” in
    *Proc. of ACL*, 2019, p. 4091–4101.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] L. Wertz, J. Bogojeska, K. Mirylenka, and J. Kuhn, “Reinforced active
    learning for low-resource, domain-specific, multi-label text classification,”
    in *Findings of ACL*, 2023, pp. 10 959–10 977.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] W. Beluch, T. Genewein *et al.*, “The power of ensembles for active learning
    in image classification,” in *Proc. of CVPR*, 2018, pp. 9368–9377.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Jesson, P. Tigas *et al.*, “Causal-bald: Deep bayesian active learning
    of outcomes to infer treatment-effects from observational data,” in *Proc. of
    NeurIPS*, 2021, pp. 30 465–30 478.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] P. Donmez, J. Carbonell *et al.*, “Dual strategy active learning,” in
    *Proc. of ECML*, 2007, pp. 116–127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Y. Geifman and R. El-Yaniv, “Deep active learning with a neural architecture
    search,” in *Proc. of NeurIPS*, 2019, pp. 5974–5984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] P. Izmailov, S. Vikram, M. D. Hoffman, and A. G. Wilson, “What are bayesian
    neural network posteriors really like?” in *Proc. of ICML*, 2021, pp. 4629–4640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] D. E. Rumelhart *et al.*, “Learning representations by back-propagating
    errors,” *Nature*, vol. 323, no. 6088, pp. 533–536, 1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] X. Zeng, S. Garg *et al.*, “Empirical evaluation of active learning techniques
    for neural MT,” in *Proc. of EMNLP*, 2019, pp. 84–93.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] A. Amirinezhad, S. Salehkaleybar, and M. Hashemi, “Active learning of
    causal structures with deep reinforcement learning,” *Neural Networks*, vol. 154,
    pp. 22–30, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] S. Hu, Z. Xiong, M. Qu, X. Yuan, M. Côté, Z. Liu, and J. Tang, “Graph
    policy network for transferable active learning on graphs,” in *Proc. of NeurIPS*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Y. Ren, B. Wang, J. Zhang, and Y. Chang, “Adversarial active learning
    based heterogeneous graph neural network for fake news detection,” in *Proc. of
    ICDM*.   IEEE, 2020, pp. 452–461.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Y. Li, J. Yin, and L. Chen, “SEAL: semisupervised adversarial active
    learning on attributed graphs,” *IEEE Trans. Neural Networks Learn. Syst.*, vol. 32,
    no. 7, pp. 3136–3147, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in *Proc.
    of ICLR*, 2014\. [Online]. Available: [https://openreview.net/forum?id=33X9fd2-9FyZd](https://openreview.net/forum?id=33X9fd2-9FyZd)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] S. Albelwi, “Survey on self-supervised learning: Auxiliary pretext tasks
    and contrastive learning methods in imaging,” *Entropy*, vol. 24, no. 4, p. 551,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] P. Du, H. Chen, S. Zhao, S. Chai, H. Chen, and C. Li, “Contrastive active
    learning under class distribution mismatch,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 45, no. 4, pp. 4260–4273, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] D. Park, Y. Shin, J. Bang, Y. Lee, H. Song, and J. Lee, “Meta-query-net:
    Resolving purity-informativeness dilemma in open-set active learning,” in *Proc.
    of NeurIPS*, 2022, pp. 31 416–31 429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Shao, Q. Wang, and F. Liu, “Learning to sample: An active learning
    framework,” in *Proc. of ICDM*, 2019, pp. 538–547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] L. Jiang, D. Meng, Q. Zhao, S. Shan, and A. G. Hauptmann, “Self-paced
    curriculum learning,” in *Proc. of AAAI*, 2015, pp. 2694–2700.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] L. Lin, K. Wang, D. Meng, W. Zuo, and L. Zhang, “Active self-paced learning
    for cost-effective and progressive face identification,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 40, no. 1, pp. 7–19, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] M. Mundt, Y. Hong, I. Pliushch, and V. Ramesh, “Forgotten lessons and
    the bridge to active and open world learning,” *Neural Networks*, vol. 160, pp.
    306–336, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] C. Löffler and C. Mutschler, “IALE: imitating active learner ensembles,”
    *J. Mach. Learn. Res.*, vol. 23, pp. 107:1–107:29, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] K. Bullard, Y. Schroecker, and S. Chernova, “Active learning within constrained
    environments through imitation of an expert questioner,” in *Proc. of IJCAI*,
    2019, pp. 2045–2052.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] F. Ikhwantri, S. Louvan *et al.*, “Multi-task active learning for neural
    semantic role labeling on low resource conversational corpus,” in *Proc. of the
    Workshop on DLA for Low-Resource*, 2018, pp. 43–50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] S. Huang, C. Zong *et al.*, “Asynchronous active learning with distributed
    label querying,” in *Proc. of IJCAI*, 2021, pp. 2570–2576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Y. Tang and S. Huang, “Self-paced active learning: Query the right thing
    at the right time,” in *Proc. of AAAI*, 2019, pp. 5117–5124.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] W. Tan, L. Du, and W. L. Buntine, “Diversity enhanced active learning
    with strictly proper scoring rules,” in *Proc. of NeurIPS*, 2021, pp. 10 906–10 918.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] F. Jelenic, J. Jukic, N. Drobac, and J. Snajder, “On dataset transferability
    in active learning for transformers,” in *Findings of ACL*, 2023, pp. 2282–2295.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Gidiotis and G. Tsoumakas, “Should we trust this summary? bayesian
    abstractive summarization to the rescue,” in *Proc. of ACL*, 2022, pp. 4119–4131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] A. Tsvigun, I. Lysenko *et al.*, “Active learning for abstractive text
    summarization,” in *Proc. of EMNLP*, 2022, pp. 5128–5152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] A. Padmakumar and R. J. Mooney, “Dialog policy learning for joint clarification
    and active learning queries,” in *Proc. of AAAI*, 2021, pp. 13 604–13 612.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] P. Radmard, Y. Fathullah, and A. Lipani, “Subsequence based deep active
    learning for named entity recognition,” in *Proc. of ACL*, 2021, pp. 4310–4321.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] X. Hua and L. Wang, “Efficient argument structure extraction with transfer
    learning and active learning,” in *Findings of ACL*, 2022, pp. 423–437.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] L. Duong, H. Afshar, D. Estival, G. Pink, P. R. Cohen, and M. Johnson,
    “Active learning for deep semantic parsing,” in *Proc. of ACL*, 2018, pp. 43–48.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Z. Li, L. Qu, P. R. Cohen, R. Tumuluri, and G. Haffari, “The best of
    both worlds: Combining human and machine translations for multilingual semantic
    parsing with active learning,” in *Proc. of ACL*, 2023, pp. 9511–9528.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] B. Zhang, L. Li, L. Su, S. Wang, J. Deng, Z. Zha, and Q. Huang, “Structural
    semantic adversarial active learning for image captioning,” in *Proc. of MM*.   ACM,
    2020, pp. 1112–1121.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] M. Cheikh and M. Zrigui, “Active learning based framework for image captioning
    corpus creation,” in *Proc. of LION*, vol. 12096, 2020, pp. 128–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] K. Konyushkova, R. Sznitman, and P. Fua, “Geometry in active learning
    for binary and multi-class image segmentation,” *Comput. Vis. Image Underst.*,
    vol. 182, pp. 1–16, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Y. Qiao, J. Zhu *et al.*, “CPRAL: collaborative panoptic-regional active
    learning for semantic segmentation,” in *Proc. of AAAI*, 2022, pp. 2108–2116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] J. Wu, J. Chen, and D. Huang, “Entropy-based active learning for object
    detection with progressive diversity constraint,” in *Proc. of CVPR*, 2022, pp.
    9387–9396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] T. Yuan, F. Wan, M. Fu, J. Liu, S. Xu, X. Ji, and Q. Ye, “Multiple instance
    active learning for object detection,” in *Proc. of CVPR*, 2021, pp. 5330–5339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] R. Caramalau, B. Bhattarai, and T. Kim, “Active learning for bayesian
    3d hand pose estimation,” in *Proc. of WACV*, 2021, pp. 3418–3427.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] M. Shukla and S. Ahmed, “A mathematical analysis of learning loss for
    active learning in regression,” in *Proc. of CVPR*, 2021, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] D. Yuan, X. Chang, Q. Liu, D. Wang, and Z. He, “Active learning for deep
    visual tracking,” *CoRR*, 2021\. [Online]. Available: [https://arxiv.org/abs/2110.13259](https://arxiv.org/abs/2110.13259)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Z. Chen, J. Zhao *et al.*, “Multi-target active object tracking with
    monte carlo tree search and target motion modeling,” *CoRR*, 2022. [Online]. Available:
    [https://arxiv.org/abs/2205.03555?context=cs.LG](https://arxiv.org/abs/2205.03555?context=cs.LG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] X. Xu, L. Liu, X. Zhang, W. Guan, and R. Hu, “Rethinking data collection
    for person re-identification: active redundancy reduction,” *Pattern Recognit.*,
    vol. 113, p. 107827, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] J. Cai, J. Tang, Q. Chen, Y. Hu, X. Wang, and S. Huang, “Multi-view active
    learning for video recommendation,” in *Proc. of IJCAI*, 2019, pp. 2053–2059.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] L. Zhao, S. J. Pan, and Q. Yang, “A unified framework of active transfer
    learning for cross-system recommendation,” *Artif. Intell.*, vol. 245, pp. 38–55,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] P. Gupta, R. Jindal, and A. Sharma, “Community trolling: An active learning
    approach for topic based community detection in big data,” *J. Grid Comput.*,
    vol. 16, no. 4, pp. 553–567, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] E. Chien, A. M. Tulino, and J. Llorca, “Active learning in the geometric
    block model,” in *Proc. of AAAI*, 2020, pp. 3641–3648.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] K. Deng, J. Pineau, and S. A. Murphy, “Active learning for personalizing
    treatment,” in *Proc. of ADPRL*.   IEEE, 2011, pp. 32–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] K. Jedoui, R. Krishna, M. S. Bernstein, and L. Fei-Fei, “Deep bayesian
    active learning for multiple correct outputs,” *CoRR*, 2019\. [Online]. Available:
    [https://arxiv.org/abs/1912.01119](https://arxiv.org/abs/1912.01119)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] M. Moradshahi, V. Tsai, G. Campagna, and M. Lam, “Contextual semantic
    parsing for multilingual task-oriented dialogues,” in *Proc. of EACL*, 2023, pp.
    902–915.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Z. Li and G. Haffari, “Active learning for multilingual semantic parser,”
    in *Findings of EACL*, 2023, pp. 621–627.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Z. Zhou, J. Shin *et al.*, “Fine-tuning convolutional neural networks
    for biomedical image analysis: Actively and incrementally,” in *Proc. of CVPR*,
    2017, pp. 7340–7351.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] M. Bilgic, L. Mihalkova, and L. Getoor, “Active learning for networked
    data,” in *Proc. of ICML*, 2010, pp. 79–86.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] H. Cai, V. W. Zheng, and K. C. Chang, “Active learning for graph embedding,”
    *CoRR*, 2017\. [Online]. Available: [https://arxiv.org/abs/1705.05085](https://arxiv.org/abs/1705.05085)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] L. Gao, H. Yang *et al.*, “Active discriminative network representation
    learning,” in *Proc. of IJCAI*, 2018, pp. 2142–2148.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] A. Cheng, C. Zhou *et al.*, “Deep active learning for anchor user prediction,”
    in *Proc. of IJCAI*, 2019, pp. 2151–2157.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] X. Yue, Y. Wen, J. H. Hunt, and J. Shi, “Active learning for gaussian
    process considering uncertainties with application to shape control of composite
    fuselage,” *IEEE Trans Autom. Sci. Eng.*, vol. 18, no. 1, pp. 36–46, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] C. Lee, X. Wang, J. Wu, and X. Yue, “Failure-averse active learning for
    physics-constrained systems,” *IEEE Trans Autom. Sci. Eng.*, vol. 20, no. 4, pp.
    2215–2226, 2023\. [Online]. Available: [https://doi.org/10.1109/TASE.2022.3213827](https://doi.org/10.1109/TASE.2022.3213827)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] C. Lee, K. Wang, J. Wu, W. Cai, and X. Yue, “Partitioned active learning
    for heterogeneous systems,” *J. Comput. Inf. Sci. Eng.*, vol. 23, no. 4, 2023\.
    [Online]. Available: [https://doi.org/10.1115/1.4056567](https://doi.org/10.1115/1.4056567)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] A. Rahman, “Algorithms of oppression: How search engines reinforce racism,”
    *New Media Soc.*, vol. 22, no. 3, pp. 308–310, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] I. Sundin, P. Schulam *et al.*, “Active learning for decision-making
    from imbalanced observational data,” in *Proc. of ICML*, 2019, pp. 6046–6055.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] D. Li, Y. Wang, K. Funakoshi, and M. Okumura, “After: Active learning
    based fine-tuning framework for speech emotion recognition,” in *Proc. of IEEE-ASRU*,
    2023, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] G. McDonald, C. Macdonald, and I. Ounis, “Active learning stopping strategies
    for technology-assisted sensitivity review,” in *Proc. of SIGIR*, 2020, pp. 2053–2056.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] H. Yu, X. Yang *et al.*, “Active learning from imbalanced data: A solution
    of online weighted extreme learning machine,” *IEEE Trans. Neural Networks Learn.
    Syst.*, vol. 30, no. 4, pp. 1088–1103, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] H. Ishibashi and H. Hino, “Stopping criterion for active learning based
    on deterministic generalization bounds,” in *Proc. of AISTATS*, 2020, pp. 386–397.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Y. Yu, R. Zhang *et al.*, “Cold-start data selection for better few-shot
    language model fine-tuning: A prompt-based uncertainty propagation approach,”
    in *Proc. of ACL*, 2023, pp. 2499–2521.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] L. Chen, Y. Bai *et al.*, “Making your first choice: To address cold
    start problem in vision active learning,” *CoRR*, 2022\. [Online]. Available:
    [https://arxiv.org/abs/2210.02442](https://arxiv.org/abs/2210.02442)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] O. Yehuda and A. Dekel, “Active learning through a covering lens,” in
    *Proc. of NeurIPS*, vol. 35, 2022, pp. 22 354–22 367.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] X. Cao, I. W. Tsang, and J. Xu, “Cold-start active sampling via $\gamma$-tube,”
    *IEEE Trans. Cybern.*, vol. 52, no. 7, pp. 6034–6045, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] R. Mahmood, S. Fidler *et al.*, “Low-budget active learning via wasserstein
    distance: An integer programming approach,” in *Proc. of ICLR*, 2022\. [Online].
    Available: [https://arxiv.org/abs/2106.02968](https://arxiv.org/abs/2106.02968)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Y. Tang and S. Huang, “Active learning for multiple target models,” in
    *Proc. of NeurIPS*, 2022, pp. 38 424–38 435.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] S. Farquhar, Y. Gal *et al.*, “On statistical bias in active learning:
    How and when to fix it,” in *Proc. of ICLR*, 2021\. [Online]. Available: [https://openreview.net/forum?id=JiYq3eqTKY](https://openreview.net/forum?id=JiYq3eqTKY)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] M. Zlabinger, “Efficient and effective text-annotation through active
    learning,” in *Proc. of SIGIR*, 2019, p. 1456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] B. Kwak, Y. Kim *et al.*, “Trustal: Trustworthy active learning using
    knowledge distillation,” in *Proc. of AAAI*, 2022, pp. 7263–7271.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] H. Ma, C. Li, X. Shi, Y. Yuan, and G. Wang, “Deep unsupervised active
    learning on learnable graphs,” *CoRR*, 2021\. [Online]. Available: [https://arxiv.org/abs/2111.04286](https://arxiv.org/abs/2111.04286)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] S. Mamooler, R. Lebret, S. Massonnet, and K. Aberer, “An efficient active
    learning pipeline for legal text classification,” in *Proc. of NLLP*, 2022, pp.
    345–358.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] F. Wan, T. Yuan *et al.*, “Nearest neighbor classifier embedded network
    for active learning,” in *Proc. of AAAI*, 2021, pp. 10 041–10 048.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] X. Deng, W. Wang, F. Feng, H. Zhang, X. He, and Y. Liao, “Counterfactual
    active learning for out-of-distribution generalization,” in *Proc. of ACL*, 2023,
    pp. 11 362–11 377.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] H. Wang, W. Huang, Z. Wu, H. Tong, A. Margenot, and J. He, “Deep active
    learning by leveraging training dynamics,” in *Proc. of NeurIPS*, 2022, pp. 25 171–25 184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] L. Zhao, G. Sukthankar *et al.*, “Incremental relabeling for active learning
    with noisy crowdsourced annotations,” in *Proc. of PASSAT*, 2011, pp. 728–733.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Z. Ashari, H. Ghasemzadeh *et al.*, “Mindful active learning,” in *Proc.
    of IJCAI*, 2019, pp. 2265–2271.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] J. Choi, K. Yi *et al.*, “Vab-al: Incorporating class imbalance and difficulty
    with variational bayes for active learning,” in *Proc. of CVPR*, 2021, pp. 6749–6758.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] E. Zhao, A. Liu *et al.*, “Active learning under label shift,” in *Proc.
    of AISTATS*, 2021, pp. 3412–3420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] J. S. Hartford, K. Leyton-Brown, H. Raviv, D. Padnos, S. Lev, and B. Lenz,
    “Exemplar guided active learning,” in *Proc. of NeurIPS*, 2020. [Online]. Available:
    [https://arxiv.org/abs/2011.01285](https://arxiv.org/abs/2011.01285)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] J. Zhang, J. Katz-Samuels, and R. D. Nowak, “GALAXY: graph-based active
    learning at the extreme,” in *Proc. of ICML*, 2022, pp. 26 223–26 238.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] R. He, Z. Han, X. Lu, and Y. Yin, “Safe-student for safe deep semi-supervised
    learning with unseen-class unlabeled data,” in *Proc. of CVPR*, 2022, pp. 14 565–14 574.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Y. Tang and S. Huang, “Dual active learning for both model and data selection,”
    in *Proc. of IJCAI*, 2021, pp. 3052–3058.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] K. Ning, X. Zhao *et al.*, “Active learning for open-set annotation,”
    in *Proc. of CVPR*, 2022, pp. 41–49.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] G. Hacohen, A. Dekel, and D. Weinshall, “Active learning on a budget:
    Opposite strategies suit high and low budgets,” in *Proc. of ICML*, vol. 162,
    2022, pp. 8175–8195.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] M. Mosbach, M. Andriushchenko, and D. Klakow, “On the stability of fine-tuning
    bert: Misconceptions, explanations, and strong baselines,” in *Proc. of ICLR*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] F. Tang, “Bidirectional active learning with gold-instance-based human
    training,” in *Proc. of IJCAI*, 2019, pp. 5989–5996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] S. Kothawade, N. Beck *et al.*, “Submodular information measures based
    active learning in realistic scenarios,” in *Proc. of NeurIPS*, 2021, pp. 18 685–18 697.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] S. Ertekin, J. Huang, and C. L. Giles, “Active learning for class imbalance
    problem,” in *Proc. of SIGIR*.   ACM, 2007, pp. 823–824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/8fcae6e99057f47c2f8ddd7af8f90ca7.png) | Dongyuan
    Li received his bachelor’s degree in computer science from Dalian University of
    Technology in 2018\. He received his Master’s degree from the School of Computer
    Science and Technology, Xidian University in 2021. He currently is a third-year
    Ph.D. candidate in the School of Information and Communication Engineering, Tokyo
    Institute of Technology. His research interests include natural language processing,
    machine learning, data mining, social network analyses, and bio-informatics. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/e1ceb8896b305b8f2f0571cd3cf3b34c.png) | Zhen Wang
    received his bachelor’s degree in computer science from Beihang University in
    2018\. He received his Master’s degree from the Faculty of Electrical Engineering,
    Mathematics, and Computer Science, Delft University of Technology. He is currently
    pursuing his Ph.D. degree in the School of Information and Communication Engineering,
    Tokyo Institute of Technology. His research interests include natural language
    processing, computer vision, and multimodal learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2024b71a13f4e6434d7bfcf81be0e0e4.png) | Yankai
    Chen currently is a fourth year Ph.D. candidate in Department of Computer Science
    and Engineering, The Chinese University of Hong Kong. He received the B.S. degree
    from Nanjing University in 2016 and the M.S. degree from the University of Hong
    Kong in 2018. His research interests include data mining and applied machine learning
    and for database management and information retrieval. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/bb6d074cfe6594e103b3ab394b1c28ef.png) | Renhe
    Jiang is a lecturer at Center for Spatial Information Science, The University
    of Tokyo. He received his B.E. degree in Software Engineering from Dalian University
    of Technology in 2012, M.S. degree in Information Science from Nagoya University
    in 2015, and Ph.D. degree in Civil Engineering from The University of Tokyo in
    2019\. From 2019 to 2022, he was an assistant professor at Information Technology
    Center, The University of Tokyo. His research interests include spatiotemporal
    data mining, multivariate time series, graph neural networks. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2884a746a5e60108c6d49dcb4818def4.png) | Weiping
    Ding (M’16-SM’19) received the Ph.D. degree in Computer Science, Nanjing University
    of Aeronautics and Astronautics, Nanjing, China, in 2013\. In 2016, He was a Visiting
    Scholar at National University of Singapore, Singapore. From 2017 to 2018, he
    was a Visiting Professor at University of Technology Sydney, Australia. He is
    a Full Professor with the School of Information Science and Technology, Nantong
    University, Nantong, China, and also the supervisor of Ph.D postgraduate by the
    Faculty of Data Science at City University of Macau, China. His main research
    directions involve DNNs, multimodal machine learning, and medical images analysis.
    He has published over 250 articles, including over 110 IEEE Transactions papers.
    His nineteen authored/co-authored papers have been selected as ESI Highly Cited
    Papers. He has co-authored four books. He has holds 28 approved invention patents,
    including two U.S. patents and one Australian patent. He serves as an Associate
    Editor/Editorial Board member of IEEE Transactions on Neural Networks and Learning
    Systems, IEEE Transactions on Fuzzy Systems, IEEE Transactions on Intelligent
    Transportation Systems, IEEE Transactions on Intelligent Vehicles, IEEE Transactions
    on Artificial Intelligence, Information Fusion, Information Sciences, Neurocomputing,
    Applied Soft Computing. He is the Leading Guest Editor of Special Issues in several
    prestigious journals, including IEEE Transactions on Evolutionary Computation,
    IEEE Transactions on Fuzzy Systems, and Information Fusion. He is the Co-Editor-in-Chief
    of both Journal of Artificial Intelligence and Systems and Journal of Artificial
    Intelligence Advances. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/df18044ab7bcc0a33398f3cfad282142.png) | Manabu
    Okumura was born in 1962\. He received B.E., M.E. and Dr. Eng. from Tokyo Institute
    of Technology in 1984, 1986 and 1989, respectively. He was an assistant professor
    at the Department of Computer Science, Tokyo Institute of Technology from 1989
    to 1992, and an associate professor at the School of Information Science, Japan
    Advanced Institute of Science and Technology from 1992 to 2000\. He was also a
    visiting associate professor at the Department of Computer Science, University
    of Toronto from 1997 to 1998\. From 2000, he was an associate professor at Precision
    and Intelligence Laboratory, Tokyo Institute of Technology, and he is currently
    a professor at Institute of Innovative Research, Tokyo Institute of Technology.
    His current research interests include natural language processing, especially
    text summarization, computer assisted language learning, and text data mining.
    |'
  prefs: []
  type: TYPE_TB
